,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,9,a1y3z2,arxiv.org,[R] Training ResNet-50 on ImageNet in 35 Epochs using Second-order Optimization Method for Large Mini-batch,https://www.reddit.com/r/MachineLearning/comments/a1y3z2/r_training_resnet50_on_imagenet_in_35_epochs/,FlyingOctopus0,1543623764,,4,1,False,default,,,,,
1,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,10,a1yhd7,self.MachineLearning,[R] A Closer Look at Deep Policy Gradients,https://www.reddit.com/r/MachineLearning/comments/a1yhd7/r_a_closer_look_at_deep_policy_gradients/,andrew_ilyas,1543626687,"Hi r/MachineLearning! A few weeks ago we published the paper ""[Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?](https://arxiv.org/abs/1811.02553)"" This week we published two blog posts (out of an eventual three) that summarize some of our paper:

1.  Part 1 ([http://gradsci.org/policy\_gradients\_pt1](http://gradsci.org/policy_gradients_pt1)) is an introduction to deep policy gradient methods and an analysis on the optimizations used. 
2. Part 2 ([http://gradsci.org/policy\_gradients\_pt2](http://gradsci.org/policy_gradients_pt2)) is on the quality of gradient estimates, and on the role of the value network in training.

Let us know if you have any questions!",5,1,False,self,,,,,
2,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,11,a1z7qg,self.MachineLearning,"[D] If one had create a neural network to read, understand and assign marks for students exam papers, which type of network should one go with to get better results and why?",https://www.reddit.com/r/MachineLearning/comments/a1z7qg/d_if_one_had_create_a_neural_network_to_read/,MildMile,1543632527,,4,1,False,self,,,,,
3,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,13,a2041u,self.MachineLearning,[R] An Off-policy Policy Gradient Theorem Using Emphatic Weightings,https://www.reddit.com/r/MachineLearning/comments/a2041u/r_an_offpolicy_policy_gradient_theorem_using/,EhsanEI,1543640247,"In this NeurIPS (NIPS) paper, we

* point out a discrepancy between common off-policy policy gradient methods (like OffPAC and DPG) and the true gradient,
* show an example where training with these methods can reduce the performance,
* offer a fix.

[https://arxiv.org/abs/1811.09013](https://arxiv.org/abs/1811.09013)",0,1,False,self,,,,,
4,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,14,a209tp,self.MachineLearning,A Beginner Mathematics Book For Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/a209tp/a_beginner_mathematics_book_for_machine_learning/,Sudhi10,1543641511,[removed],0,1,False,self,,,,,
5,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,14,a20ita,self.MachineLearning,The Next Generation Cognitive Security Operations Center,https://www.reddit.com/r/MachineLearning/comments/a20ita/the_next_generation_cognitive_security_operations/,wessax,1543643728,"A Security Operations Center (SOC) can be defined as an organized and highly skilled team that uses advanced computer forensics tools to prevent, detect and respond to cybersecurity incidents of an organization. The fundamental aspects of an effective SOC is related to the ability to examine and analyze the vast number of data flows and to correlate several other types of events from a cybersecurity perception. The supervision and categorization of network flow is an essential process not only for the scheduling, management, and regulation of the networks services, but also for attacks identification and for the consequent forensics investigations. A serious potential disadvantage of the traditional software solutions used today for computer network monitoring, and specifically for the instances of effective categorization of the encrypted or obfuscated network flow, which enforces the rebuilding of messages packets in sophisticated underlying protocols, is the requirements of computational resources. In addition, an additional significant inability of these software packages is they create high false positive rates because they are deprived of accurate predicting mechanisms. For all the reasons above, in most cases, the traditional software fails completely to recognize unidentified vulnerabilities and zero-day exploitations. This paper proposes a novel intelligence driven Network Flow Forensics Framework (NF3) which uses low utilization of computing power and resources, for the Next Generation Cognitive Computing SOC (NGC2SOC) that rely solely on advanced fully automated intelligence methods. It is an effective and accurate Ensemble Machine Learning forensics tool to Network Traffic Analysis, Demystification of Malware Traffic and Encrypted Traffic Identification.View Full-Text
",0,1,False,self,,,,,
6,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,17,a21d0q,self.MachineLearning,What are the must read papers for a beginner in the field of Machine Learning and Artificial Intelligence? [Discussion],https://www.reddit.com/r/MachineLearning/comments/a21d0q/what_are_the_must_read_papers_for_a_beginner_in/,RogerCray,1543652416,,67,1,False,self,,,,,
7,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,17,a21exh,self.MachineLearning,Is anyone working on a C++ to Python transpiler?,https://www.reddit.com/r/MachineLearning/comments/a21exh/is_anyone_working_on_a_c_to_python_transpiler/,ishandutta2007,1543653034,[removed],0,1,False,self,,,,,
8,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,18,a21svs,self.MachineLearning,"Comparing Decision trees, Random forests and Decision jungles",https://www.reddit.com/r/MachineLearning/comments/a21svs/comparing_decision_trees_random_forests_and/,ciphermatrix,1543657591,[removed],0,1,False,self,,,,,
9,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,18,a21uc8,self.MachineLearning,30day wolfram mathmatica free trial.,https://www.reddit.com/r/MachineLearning/comments/a21uc8/30day_wolfram_mathmatica_free_trial/,DaysofstaticUK,1543658068,[removed],0,1,False,self,,,,,
10,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,20,a22bk7,blog.echen.me,Moving Beyond CTR: Better Recommendations Through Human Evaluation,https://www.reddit.com/r/MachineLearning/comments/a22bk7/moving_beyond_ctr_better_recommendations_through/,boredcodeeva,1543663443,,0,1,False,default,,,,,
11,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,21,a22kk0,self.MachineLearning,[R] MetaGAN: An Adversarial Approach to Few-Shot Learning,https://www.reddit.com/r/MachineLearning/comments/a22kk0/r_metagan_an_adversarial_approach_to_fewshot/,ndha1995,1543666298,"Found this interesting paper among NIPS 2018 papers: [MetaGAN: An Adversarial Approach to Few-Shot Learning](http://papers.nips.cc/paper/7504-metagan-an-adversarial-approach-to-few-shot-learning)

Abstract:

&gt;In this paper, we propose a conceptually simple and general framework called MetaGAN for few-shot learning problems. Most state-of-the-art few-shot classification models can be integrated with MetaGAN in a principled and straightforward way. By introducing an adversarial generator conditioned on tasks, we augment vanilla few-shot classification models with the ability to discriminate between real and fake data. We argue that this GAN-based approach can help few-shot classifiers to learn sharper decision boundary, which could generalize better. We show that with our MetaGAN framework, we can extend supervised few-shot learning models to naturally cope with unsupervised data. Different from previous work in semi-supervised few-shot learning, our algorithms can deal with semi-supervision at both sample-level and task-level. We give theoretical justifications of the strength of MetaGAN, and validate the effectiveness of MetaGAN on challenging few-shot image classification benchmarks.",1,1,False,self,,,,,
12,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,21,a22qjk,self.MachineLearning,Why is Eye Tracking really important for Market Research?,https://www.reddit.com/r/MachineLearning/comments/a22qjk/why_is_eye_tracking_really_important_for_market/,ayushvinny,1543668132," 

*****95% of the purchase decisions happen in the subconscious***

[Eye Tracking](https://blog.karna.ai/why-is-eye-tracking-important-for-market-research-9ee0b2acdccf)

Yes, you read it right! 95% of the purchase decisions happen in our sub-consciousaccording to [research](https://hbswk.hbs.edu/item/the-subconscious-mind-of-the-consumer-and-how-to-reach-it) performed by Harvard Business School professor Gerald Zaltman. For the uninitiated, or someone new to the consumer insights industry, this is a revelation. Indeed, when I think of it, I cannot give a reasonable explanation to why I chose to drink orange juice over apple juice at a party last night. Both the options were in front of me, some neurons fired in my brain and my hand just steered towards the orange juice.

This research from Prof. Gerald also made me think that how companies can understand whats happening in consumers subconscious in order to derive insights. And it is important to figure this out because asking consumers (surveys, interviews etc) can only yield conscious reasons behind their actions, which explains only 5% of the decisions as per Prof. Gerald. People usually decide in their subconscious and then give a logical explanation according to their conscious brain",0,1,False,self,,,,,
13,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,22,a22xnz,self.MachineLearning,"Machine Learning has been defined as the ""Field of study that gives computers the ability to learn without being explicitly programmed."" Learn Machine Learning By Building Projects",https://www.reddit.com/r/MachineLearning/comments/a22xnz/machine_learning_has_been_defined_as_the_field_of/,boredcodeeva,1543670147,[removed],0,1,False,self,,,,,
14,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,22,a22z2s,self.MachineLearning,Laptop suggestions,https://www.reddit.com/r/MachineLearning/comments/a22z2s/laptop_suggestions/,philmifsud97,1543670523,[removed],0,1,False,self,,,,,
15,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,22,a232fb,self.MachineLearning,Searching for project comparing social media websites (e.g. Facebook and Twitter),https://www.reddit.com/r/MachineLearning/comments/a232fb/searching_for_project_comparing_social_media/,rohitr7,1543671412,[removed],0,1,False,self,,,,,
16,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,23,a23hza,self.MachineLearning,people who bought this also bought this question,https://www.reddit.com/r/MachineLearning/comments/a23hza/people_who_bought_this_also_bought_this_question/,fnatanoy,1543675113,[removed],0,1,False,self,,,,,
17,MachineLearning,t5_2r3gv,2018-12-1,2018,12,1,23,a23n1p,youtu.be,Can a machine brain cure disease?,https://www.reddit.com/r/MachineLearning/comments/a23n1p/can_a_machine_brain_cure_disease/,chelsea_bear,1543676275,,0,1,False,default,,,,,
18,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,0,a23wad,self.MachineLearning,What is the chance of a paper with average rating of 6/10 to be accepted by ICLR as conference paper(from previous years)?,https://www.reddit.com/r/MachineLearning/comments/a23wad/what_is_the_chance_of_a_paper_with_average_rating/,Periplokos,1543678261,,0,1,False,self,,,,,
19,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,1,a24b7z,technologyreview.com,Inside the world of AI that forges beautiful art and terrifying deepfakes,https://www.reddit.com/r/MachineLearning/comments/a24b7z/inside_the_world_of_ai_that_forges_beautiful_art/,chaoticflipflops,1543681276,,0,1,False,default,,,,,
20,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,1,a24ed1,self.MachineLearning,[D] Does ICLR have a code of conduct...,https://www.reddit.com/r/MachineLearning/comments/a24ed1/d_does_iclr_have_a_code_of_conduct/,alexmlamb,1543681918,"... for public comments on openreview?  

I feel like the public comments have some good effects overall, but I wonder if they've ever written down what types of public comments are appropriate or inappropriate?  There are some things that seem obviously out of line to me, like paying/asking someone to write a positive public comment - or writing a negative comment out of spite.  But are you supposed to not make public comments if they're related to your own work?  If they're related to a paper you have under review?  

I think that it would be helpful to have some better rules and guidelines in place for this.  One simple change that I would appreciate would be restricting the timetable for public comments, so that they always come in some time before the authors have a chance to respond to them (before reviews are posted, for example).  ",6,1,False,self,,,,,
21,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,2,a255oj,self.MachineLearning,Recommended machine learning projects for beginners (or websites that give machine learning projects for beginners)?,https://www.reddit.com/r/MachineLearning/comments/a255oj/recommended_machine_learning_projects_for/,SenseDeletion,1543687046,[removed],0,1,False,self,,,,,
22,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,3,a25ese,self.MachineLearning,ELI5: How do Sigmoid functions operate in Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/a25ese/eli5_how_do_sigmoid_functions_operate_in_neural/,Psclly,1543688758,[removed],0,1,False,self,,,,,
23,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,3,a25f4c,self.MachineLearning,What is the best resource for learning ML been for you?,https://www.reddit.com/r/MachineLearning/comments/a25f4c/what_is_the_best_resource_for_learning_ml_been/,PorterG2003,1543688811,For all beginners trying to learn ML.,0,1,False,self,,,,,
24,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,3,a25n1z,techenga.ge,Meet Norman AI; the American Psycho of Digital World - TechEngage,https://www.reddit.com/r/MachineLearning/comments/a25n1z/meet_norman_ai_the_american_psycho_of_digital/,jinnahsequaid,1543690283,,0,1,False,default,,,,,
25,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,3,a25owq,self.MachineLearning,[P] Training a Goal-Oriented Chatbot with Deep Reinforcement Learning in Python,https://www.reddit.com/r/MachineLearning/comments/a25owq/p_training_a_goaloriented_chatbot_with_deep/,DL_IRL,1543690638,"Here is a tutorial series I made on Medium on coding a goal-oriented chatbot with DRL in python from scratch

* [Introduction and overview of training](https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-i-introduction-and-dce3af21d383)
* [DQN Agent with Keras](https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-ii-dqn-agent-f84122cc995c)
* [Dialogue state tracking](https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iii-dialogue-state-d29c2828ce2a)
* [User simulator and error model controller](https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-iv-user-simulator-and-a0efd3829364)
* [Future research](https://medium.com/@maxbrenner110/training-a-goal-oriented-chatbot-with-deep-reinforcement-learning-part-v-running-the-agent-and-63d8cd27d1d)

The code repo is on [github](https://github.com/maxbren/GO-Bot-DRL)!",2,1,False,self,,,,,
26,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,4,a25vho,self.MachineLearning,[D] Recommendation engines - beyond common tutorials,https://www.reddit.com/r/MachineLearning/comments/a25vho/d_recommendation_engines_beyond_common_tutorials/,baahalex,1543691846,"I've been starting to read about recommendation engines in general but so far I think there's something missing from the usual tutorials (imdb, goodreads, movielens datasets), or maybe I'm missing it. I'm hoping some redditor can clarify this a bit.  
How does a recommendation engine handle the inflow of new data? For example, let's say you train an engine on data from 500 users and 100.000 .. songs? Ok, songs it is. Assuming there are 1000 new songs appearing in the database per day, and 50 new users with data per day, how does an engine handle that?  
Do you need to retrain the engine every day or so, or even more often? Such that it can recommend also songs from the 1000 new daily ones? From what I've seen in tutorials, all you need to do is to provide the data of a user (can be a user that the engine's never seen before), and the engine returns the ids/titles of items that it was trained on. But if the items change in time, how does the engine know the id of an item it's never seen before?   
Or is there a way to feed in the data of the user but also a set of new song data, and the engine gives you recommendations from the set it was provided with? This, to me, seems like a more natural way, but somehow I've never seen this mentioned in any of the tutorials I've read. I guess this would mean you express the user data and the song data in the same vector space, and then take some form of a distance -&gt; those are the recommendations for this user.  
Or is there another way?

&amp;#x200B;",14,1,False,self,,,,,
27,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,4,a25zq6,self.MachineLearning,HELP ME install Seaborn!,https://www.reddit.com/r/MachineLearning/comments/a25zq6/help_me_install_seaborn/,taqueria_on_the_moon,1543692680,[removed],0,1,False,self,,,,,
28,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,4,a2616z,self.MachineLearning,[D] What are the differences between the probabilistic programming and neural network approaches to machine learning?,https://www.reddit.com/r/MachineLearning/comments/a2616z/d_what_are_the_differences_between_the/,singular_icecream,1543692956,"Bear with me, I'm still learning so I'm not sure if my usage of these terms is exactly correct.

I recently saw a presentation at my university about probabilistic programming that was about computing Bayesian posteriors in Python with simple guess and check function. The presenter showed us some solutions to tasks that were too complicated to solve by programming explicitly. 

I'm not quite sure if neural network approaches to machine learning falls into the category of 'probabilistic programming', and I suppose the two are very much nonexclusive and used together. However, I am sort of interested in the various ways that probabilistic programming can address the difficulties in the neural net approach to machine learning. In particular, I would like to know:

* What types of problems is probabilistic programming more suited to as opposed to neural networks and vice-versa?
* What are the current issues facing probabilistic programming and how do they differ from the problems with the neural network approach?
* Probabilistic programming seems to not be a very popular enterprise. Is there a large drawback to probabilistic programming that makes it less appealing to use as opposed to neural nets?",17,1,False,self,,,,,
29,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,6,a26rhc,self.MachineLearning,Examples of interesting toy datasets suitable for ML research,https://www.reddit.com/r/MachineLearning/comments/a26rhc/examples_of_interesting_toy_datasets_suitable_for/,lume_,1543698010,[removed],0,1,False,self,,,,,
30,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,6,a26wmo,self.MachineLearning,[P] Tic Tac Toe - Creating Unbeatable AI with Minimax Algorithm,https://www.reddit.com/r/MachineLearning/comments/a26wmo/p_tic_tac_toe_creating_unbeatable_ai_with_minimax/,g_surma,1543698943,"&amp;#x200B;

https://i.redd.it/ewwqvdztfq121.png

[medium](https://towardsdatascience.com/tic-tac-toe-creating-unbeatable-ai-with-minimax-algorithm-8af9e52c1e7d)

In todays article, I am going to show you how to create an unbeatable AI agent that plays the classic **Tic Tac Toe** game. You will learn the concept of the **Minimax** algorithm that is widely and successfully used across the fields like **Artificial Intelligence**, **Economics**, **Game Theory**, **Statistics** or even **Philosophy**.",1,1,False,https://b.thumbs.redditmedia.com/VVNkntwcRwQJvgoE0uOCfMAYOtFWX3tbNgNfVqHQWQU.jpg,,,,,
31,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,6,a270i4,self.MachineLearning,"We're doing startup name generators now? Here's mine from last year, with code available.",https://www.reddit.com/r/MachineLearning/comments/a270i4/were_doing_startup_name_generators_now_heres_mine/,mwlon,1543699675,[removed],0,1,False,self,,,,,
32,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,7,a27po0,self.MachineLearning,Reading material for model-based deep RL,https://www.reddit.com/r/MachineLearning/comments/a27po0/reading_material_for_modelbased_deep_rl/,WalkingCook1e,1543704470,[removed],0,1,False,self,,,,,
33,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,8,a27tkk,arxiv.org,[R] Diverse Image Synthesis from Semantic Layouts via Conditional IMLE,https://www.reddit.com/r/MachineLearning/comments/a27tkk/r_diverse_image_synthesis_from_semantic_layouts/,HigherTopoi,1543705244,,0,1,False,default,,,,,
34,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,8,a27z74,self.MachineLearning,Kernel Trick,https://www.reddit.com/r/MachineLearning/comments/a27z74/kernel_trick/,magu01,1543706321,[removed],0,1,False,self,,,,,
35,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,8,a281bv,self.MachineLearning,[D] Is it possible to self-learn ML without going to grad school?,https://www.reddit.com/r/MachineLearning/comments/a281bv/d_is_it_possible_to_selflearn_ml_without_going_to/,coder9795,1543706747,"Hello all. I am a CS student who gonna graduate this December. I already got a job offer as a Software Engineer at a tech company (I do full-stack development). This semester, I took a ML course at my college and felt very interested in it. I have worked on gender classification problem and a hair segmentation one, using Keras. However, I understand that my knowledge about ML is just a beginner because right now I used already-built model on tutorials I found online. As a result, I plan to self-learn ML after work. For that reason, I have few questions I want to ask:

1. Is it possible to self-learn ML without going to grad school (and actually build efficient model)? I read [this post](https://www.fast.ai/2018/08/27/grad-school/) on [Fast.ai](https://Fast.ai) that the engineer here said that you don't really need a PhD to apply ML in your products.
2. If it is, how's your study guide look like? Is self-learn good enough to build a good model to use in production for your companies? 
3. Is Kaggle competition a good way to improve your ML skill? My professor used Kaggle to host 2 in-class competitions and I felt it is a good way to learn.

Thanks a lot.",28,1,False,self,,,,,
36,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,8,a284no,youtube.com,Bitcoin Price Prediction | Using Advanced Models (2018),https://www.reddit.com/r/MachineLearning/comments/a284no/bitcoin_price_prediction_using_advanced_models/,Piterst,1543707414,,0,1,False,https://b.thumbs.redditmedia.com/QP25HB5HuYfEDUUgpXQoG73GVHgWytD3VqdefdBq5is.jpg,,,,,
37,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,8,a2870z,self.MachineLearning,Save a tensorflow.js model as json in node,https://www.reddit.com/r/MachineLearning/comments/a2870z/save_a_tensorflowjs_model_as_json_in_node/,CSS_Programmer,1543707901,"I am trying to save a tensorflow.js model as a json file but it's not working.

    var exampleModel = {input:1,name:'model'} fs.writeFileSync('model.json', JSON.stringify(model,null,2)) //=&gt; '""{\""input\"":1, \""name\"": \""model\""}""' 

It's not in json format. I wasted a lot of time trying to manually create a json but that didn't work either and I will have to redo it by copying it to a normal object or something.

I already asked stack overflow but no one knows.",0,1,False,self,,,,,
38,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,9,a28fu6,self.MachineLearning,What math do I need to know to do Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/a28fu6/what_math_do_i_need_to_know_to_do_machine_learning/,snowleopardkitten34,1543709717,[removed],0,1,False,self,,,,,
39,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,9,a28nux,youtube.com,[P] Darknet Yolo Algorithm Applied to a Home Movie,https://www.reddit.com/r/MachineLearning/comments/a28nux/p_darknet_yolo_algorithm_applied_to_a_home_movie/,dustball,1543711339,,0,1,False,https://a.thumbs.redditmedia.com/fJozU4z_NLUiWy-C_JXLkus6crlVPG2EDXiqBA28UD0.jpg,,,,,
40,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,9,a28s4p,self.MachineLearning,Big Gan on art dataset,https://www.reddit.com/r/MachineLearning/comments/a28s4p/big_gan_on_art_dataset/,artgan2,1543712212,[removed],0,1,False,self,,,,,
41,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,10,a28xge,self.MachineLearning,[D] Big Gan on Art Dataset,https://www.reddit.com/r/MachineLearning/comments/a28xge/d_big_gan_on_art_dataset/,artgan2,1543713306,"Hi, I been lurking here for a little while and gotten really interesting in results from progressive gan and big gan. I have tried out the demo colab notebook showing results with big gan that I was wondering could be applied to a art dataset like wikiart or [https://bam-dataset.org/](https://bam-dataset.org/). As I am a beginner I have not idea how to go about doing this. This is the colab notebook I found for big gan [https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan\_generation\_with\_tf\_hub.ipynb](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb)

Ideally I would want to try it on art from [https://www.deviantart.com/](https://www.deviantart.com/) or [https://www.artstation.com/](https://www.artstation.com/) with digital paintings but would need a scrapper first for that and the data is not labelled properly?

Thanks",17,1,False,self,,,,,
42,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,11,a29b54,self.MachineLearning,Can I predict stock splits using a RNN?,https://www.reddit.com/r/MachineLearning/comments/a29b54/can_i_predict_stock_splits_using_a_rnn/,dwlsalmeida,1543716315,[removed],0,1,False,self,,,,,
43,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,13,a2a4j5,self.MachineLearning,"[P] We're doing startup name generators now? Here's mine from last year, with code available.",https://www.reddit.com/r/MachineLearning/comments/a2a4j5/p_were_doing_startup_name_generators_now_heres/,mwlon,1543723269,"I scraped AngelList for startup names and trained a simple RNN to generate them so that I could effectively play [ArXiv vs. SnarXiv](http://snarxiv.org/vs-arxiv/) but with startup names:

[http://graphallthethings.com/posts/startups](http://graphallthethings.com/posts/startups)

Code for scraping, training, and generating here:

[https://github.com/mwlon/rnn-name-generator](https://github.com/mwlon/rnn-name-generator)

In the post, I also look at the ""most startup"" name of all.",2,1,False,self,,,,,
44,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,13,a2a5j2,self.MachineLearning,Machine learning question,https://www.reddit.com/r/MachineLearning/comments/a2a5j2/machine_learning_question/,hyona91,1543723516,[removed],0,1,False,self,,,,,
45,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,13,a2a62h,tomstechnicalblog.blogspot.com,Is Deep Learning Already Hitting it's Limitations?,https://www.reddit.com/r/MachineLearning/comments/a2a62h/is_deep_learning_already_hitting_its_limitations/,RacerRex9727,1543723633,,0,1,False,default,,,,,
46,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,13,a2aij7,youtube.com,"Live recording, of an Artificial Neural Network Written from scratch [By Jordan Bennett]",https://www.reddit.com/r/MachineLearning/comments/a2aij7/live_recording_of_an_artificial_neural_network/,ProgrammingGodJordan,1543726787,,0,1,False,https://b.thumbs.redditmedia.com/y3tlHq_C6vCwk5Ss70t-CtxVkgIskOJJF66gENUue4M.jpg,,,,,
47,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,16,a2bcy8,self.MachineLearning,[Project]AI vs Me. WHO.WILL.WIN?,https://www.reddit.com/r/MachineLearning/comments/a2bcy8/projectai_vs_me_whowillwin/,Madamin_Z,1543735413,"Hi guys! Today I want to show you how I used a genetic algorithm to teach AI to play against me.

Link: [Click Me!](https://www.youtube.com/watch?v=vxloGgn9Fj0&amp;list=PLpSMHMRlawwHAnFltguX1SouT4W2aCbKX)",1,1,False,self,,,,,
48,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,17,a2bpry,self.MachineLearning,AMA NeurIPS 2018 Workshop on Causal Learning,https://www.reddit.com/r/MachineLearning/comments/a2bpry/ama_neurips_2018_workshop_on_causal_learning/,heinzedeml,1543739879,[removed],32,1,False,self,,,,,
49,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,18,a2bvea,self.MachineLearning,audio to video feature mapping using neural network,https://www.reddit.com/r/MachineLearning/comments/a2bvea/audio_to_video_feature_mapping_using_neural/,ChanForChampKD,1543741839,[removed],0,1,False,self,,,,,
50,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,18,a2c0wi,self.MachineLearning,large sparse matrix or small dense matrix?,https://www.reddit.com/r/MachineLearning/comments/a2c0wi/large_sparse_matrix_or_small_dense_matrix/,grantli,1543743773,[removed],0,1,False,self,,,,,
51,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,18,a2c340,self.MachineLearning,[D] people who bought this also bought this question,https://www.reddit.com/r/MachineLearning/comments/a2c340/d_people_who_bought_this_also_bought_this_question/,fnatanoy,1543744570,"Many recommendation systems use collaborative filtering using matrix factorization  or something similar...

&amp;#x200B;

But what should I do if the data I have is only if a user **did buy** something but not if he saw it and decided not to buy it.

The utility matrix is either 1's or unknown-

&amp;#x200B;

|user\\movie|movie 1|movie 2|movie 3|
|:-|:-|:-|:-|
|**user 1**|1|?|?|
|**user 2**|?|?|1|
|**user 3**|1|?|?|

&amp;#x200B;

It is different from a rating matrix where I know a user saw the movie and didn't like it -

&amp;#x200B;

|user\\movie|movie 1|movie 2|movie 3|
|:-|:-|:-|:-|
|**user 1**|1|2|?|
|**user 2**|?|3|5|
|**user 3**|4|?|?|

&amp;#x200B;

Or if a user saw an item and **decided** **not** to buy it buy it (binary matrix) -

&amp;#x200B;

|user\\item|item 1|item 2|item 3|
|:-|:-|:-|:-|
|**user 1**|1|0|?|
|**user 2**|?|0|1|
|**user 3**|1|?|?|

&amp;#x200B;

thanks

&amp;#x200B;",6,1,False,self,,,,,
52,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,19,a2c87m,self.MachineLearning,[P] Deep learning Go bot tournament,https://www.reddit.com/r/MachineLearning/comments/a2c87m/p_deep_learning_go_bot_tournament/,maxpumperla,1543746278,"Manning has just announced a Go bot tournament on OGS:

[http://deals.manning.com/go-comp/](http://deals.manning.com/go-comp/)

&amp;#x200B;

The idea is to enter a ladder tournament with a bot account and rank up by playing against other bots. Your bot will be hosted on AWS. What methodology to use to power your bot is your own choice, but advanced deep learning techniques will certainly help at some point.

&amp;#x200B;

Disclaimer: I'm one of the authors of the book this tournament is based on. The book was written by us, the text on the above landing page was not. ",1,1,False,self,,,,,
53,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,20,a2cgd8,self.MachineLearning,Possibility or hardwork that should be done for a weak mathematics student to learn ML or Sub Feilds,https://www.reddit.com/r/MachineLearning/comments/a2cgd8/possibility_or_hardwork_that_should_be_done_for_a/,Dewanik_Koirala,1543749154,[removed],0,1,False,self,,,,,
54,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,20,a2cjck,self.MachineLearning,Odometry with deep learning,https://www.reddit.com/r/MachineLearning/comments/a2cjck/odometry_with_deep_learning/,sparab97,1543750183,[removed],0,1,False,self,,,,,
55,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,20,a2cogi,self.MachineLearning,What can model can I apply with a dataset like this? (First column is the class),https://www.reddit.com/r/MachineLearning/comments/a2cogi/what_can_model_can_i_apply_with_a_dataset_like/,macoit18,1543751927,[removed],0,1,False,https://b.thumbs.redditmedia.com/qTEdgDYb8nqm10BStq_JO_mQ7oZl5xlf6KpMyvUlfPs.jpg,,,,,
56,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,21,a2cuuf,arxiv.org,[1810.09538] Pyro: Deep Universal Probabilistic Programming,https://www.reddit.com/r/MachineLearning/comments/a2cuuf/181009538_pyro_deep_universal_probabilistic/,ndha1995,1543753956,,19,1,False,default,,,,,
57,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,22,a2d1hf,self.MachineLearning,Robustness in machine learning,https://www.reddit.com/r/MachineLearning/comments/a2d1hf/robustness_in_machine_learning/,CrisprCookie,1543755919,[removed],0,1,False,self,,,,,
58,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,22,a2d7r1,/r/MachineLearning/comments/a2d7r1/what_is_with_my_screen_and_how_do_i_fix_it/,What is with my screen and how do i fix it,https://www.reddit.com/r/MachineLearning/comments/a2d7r1/what_is_with_my_screen_and_how_do_i_fix_it/,furno_04,1543757555,,0,1,False,default,,,,,
59,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,22,a2ddnf,self.MachineLearning,Detecting manipulations in images,https://www.reddit.com/r/MachineLearning/comments/a2ddnf/detecting_manipulations_in_images/,cannotchangeit,1543759028,"Does anyone know what the noise domain mentioned in this paper is?  
What I got is it just the output of applying the SRM kernels on the input image, Is that right?",0,1,False,self,,,,,
60,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,23,a2dhfc,techgrabyte.com,"Google's Machine Learning Model can decode the songs sung by whales in deep sea. The machine learning system analyzes 170,000 hours of acoustic data.",https://www.reddit.com/r/MachineLearning/comments/a2dhfc/googles_machine_learning_model_can_decode_the/,navin49,1543759849,,0,1,False,https://b.thumbs.redditmedia.com/A9ddbPcOpYM2M6idIZz0GN_NAEHQmtTzfX78i9pbjnk.jpg,,,,,
61,MachineLearning,t5_2r3gv,2018-12-2,2018,12,2,23,a2dmf9,self.MachineLearning,Create an AI that predict lottery and you never be poor,https://www.reddit.com/r/MachineLearning/comments/a2dmf9/create_an_ai_that_predict_lottery_and_you_never/,navin49,1543760970,[removed],0,1,False,self,,,,,
62,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,0,a2e0xm,self.MachineLearning,Training neural networks with distant data points?,https://www.reddit.com/r/MachineLearning/comments/a2e0xm/training_neural_networks_with_distant_data_points/,fkrd13,1543764120,"I did some experiments on a single perceptron (using a SGD optimizer with lr = 0.1, a MSE loss function and 100 training epochs. The function for the perceptron should be y = 3x + 0):

If the training set is [(1,3), (2,6), (3,9), (4,12), (5,15), (6, 18)], after 100 training epochs, the loss would be 0.0023.

If the training set is [(1,3), (2,6), (3,9), (4,12), (5,15), (-1, -3)] or [(1,3), (2,6), (3,9), (4,12), (5,15), (-10, -30)], the losses after 100 training epochs would be even smaller using the given parameters.

However, if the training set is [(1,3), (2,6), (3,9), (4,12), (5,15), (-100, -300)], using any lr, from 0.01, 0.001, 0.0001, 0.00001 etc wouldn't work. The (-100, -300) is a valid data point, but in this case it prevents the function of the perceptron to converge. I wonder if in general, distant data points or data points with different signs wouldn't work too well for neural networks?",0,1,False,self,,,,,
63,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,2,a2f2ey,self.MachineLearning,[P] Sharing Your TensorBoard Dashboard Online,https://www.reddit.com/r/MachineLearning/comments/a2f2ey/p_sharing_your_tensorboard_dashboard_online/,agost_biro,1543771245,"Hi,

I've long been frustrated with the hassle involved in sharing TensorBoard dashboards with others, so I've built a tool where you can upload your log directory and get a link to a secret dashboard with your data. It's kind of like pastebin, but for log directories. I hope that this tool will be useful for fellow machine learners, so I'd be very grateful for all feedback.

This is the link to the service: [https://boards.aughie.org/](https://boards.aughie.org/)

And a short tutorial: [https://medium.com/@org.aughie/how-to-share-your-tensorboard-online-4ed7c2d3941c](https://medium.com/@org.aughie/how-to-share-your-tensorboard-online-4ed7c2d3941c)

Best,

Agost",18,1,False,self,,,,,
64,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,4,a2gk8k,self.MachineLearning,[P] mazelab 1.0: A customizable framework to create maze and gridworld environments.,https://www.reddit.com/r/MachineLearning/comments/a2gk8k/p_mazelab_10_a_customizable_framework_to_create/,modernrl,1543780404,"[https://github.com/zuoxingdong/mazelab](https://github.com/zuoxingdong/mazelab)

&amp;#x200B;

Release 1.0:

\- Modular design

\- Gym-compatible API + Monitor for video generation

\- Build-in Dijkstra solver

\- User can easily defines color pattern for rendering

\- User can easily define different kinds of objects

\- User can easily define different available actions

&amp;#x200B;

https://i.redd.it/wehfcbm76x121.gif

&amp;#x200B;",2,1,False,self,,,,,
65,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,6,a2hhqm,self.MachineLearning,Are Modular Neural Networks an interesting avenue for further research?,https://www.reddit.com/r/MachineLearning/comments/a2hhqm/are_modular_neural_networks_an_interesting_avenue/,harshsikka123,1543785992,[removed],0,1,False,self,,,,,
66,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,8,a2if5e,self.MachineLearning,[D] Regression Decision Tree from Scratch,https://www.reddit.com/r/MachineLearning/comments/a2if5e/d_regression_decision_tree_from_scratch/,amstell,1543791884,"I'm looking for an implementation of a Regression Tree from scratch and have only been able to find classification trees. 

Does anyone have a simple implementation of a regression decision tree with sample data?",2,1,False,self,,,,,
67,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,8,a2ift8,medium.com,Introduction to Genetic Algorithm,https://www.reddit.com/r/MachineLearning/comments/a2ift8/introduction_to_genetic_algorithm/,Fewthp,1543792007,,0,1,False,default,,,,,
68,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,8,a2ijf9,self.MachineLearning,tflearn error: Shape must be rank 1 but is rank 2 for 'strided_slice'. How should I change to correct rank?,https://www.reddit.com/r/MachineLearning/comments/a2ijf9/tflearn_error_shape_must_be_rank_1_but_is_rank_2/,Fab_root,1543792702,[removed],0,1,False,self,,,,,
69,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,8,a2ijm4,deepmind.com,[R] AlphaFold: Using AI for scientific discovery | DeepMind,https://www.reddit.com/r/MachineLearning/comments/a2ijm4/r_alphafold_using_ai_for_scientific_discovery/,Kaixhin,1543792741,,0,1,False,default,,,,,
70,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,8,a2ip5z,self.MachineLearning,[D] tflearn error: Shape must be rank 1 but is rank 2 for 'strided_slice'. How should I change to correct rank?,https://www.reddit.com/r/MachineLearning/comments/a2ip5z/d_tflearn_error_shape_must_be_rank_1_but_is_rank/,Fab_root,1543793799,"Hi there, new to reddit so if I'm doing something wrong please inform me :-)

I'm struggling to complete a project which needs to be finished fairly soon. I've put a lot of work into it but I've been struggling at this bit for quite a while.

I am attempting to make a CRNN (Convolutional Recurrent Neural Network), using tflearn, which inputs 17 grayscale images of 28 by 28.

The network is as follows:

    def rnn_conv_lstm_model(width, height, sequence_length):
       network = tflearn.input_data(shape=[None, sequence_length, height, width, 1])
       network = tflearn.reshape(network, [-1, height, width, 1])
       network = tflearn.conv_2d(network, 32, 3, activation='relu')
       network = tflearn.max_pool_2d(network, 2)
       network = tflearn.conv_2d(network, 64, 3, activation='relu')
       network = tflearn.conv_2d(network, 64, 3, activation='relu')
       network = tflearn.max_pool_2d(network, 2)
       network = tflearn.reshape(network, [-1, sequence_length, int((height/2**2)*(width/2**2)*64)])
       network = tflearn.lstm(network, 256)
       network = tflearn.fully_connected(network, 2, activation='sigmoid')
       network = tflearn.regression(network, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.001, name=""output1"")
       return network

This network is then fed into:

    get_input_data()
    # train the model
    model = tflearn.DNN(rnn_conv_lstm_model(28, 28, 17), tensorboard_verbose=0)
    model.fit(X_input_data, Y_labels, show_metric=True, batch_size=None, snapshot_step=None, n_epoch=1)

The 2 errors I get are ""Exception in thread Thread-8"" and are as follows:

    tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 2 for 'strided_slice' (op: 'StridedSlice') with input shapes: [1,2], [1,17], [1,17], [1].
    During handling of the above exception, another exception occurred:
    ValueError: Shape must be rank 1 but is rank 2 for 'strided_slice' (op: 'StridedSlice') with input shapes: [1,2], [1,17], [1,17], [1].

From looking around and researching, I'm fairly sure that rank means number of dimensions. As a result, as each input shape in the error above starts with a 1, I would assume that it should be possible to reshape these to a shape of rank 1 relatively easily. Having said this, I'm not sure how to, where to, and even not sure if this idea is correct at all.

I have looked at the shape of my X\_inputs and Y\_label before they are fed into the network, and they are as follows:

    X_inputs shape:
    (17, 1)
    X_inputs:
    [[&lt;tf.Tensor 'Squeeze_35:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_26:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_30:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_27:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_31:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_28:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_29:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_34:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_36:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze_37:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze:0' shape=(28, 28, 1) dtype=int32&gt;]
     [&lt;tf.Tensor 'Squeeze:0' shape=(28, 28, 1) dtype=int32&gt;]]
    
    
    Y_label shape:
    (1, 2)
    Y_label:
    Tensor(""one_hot_2:0"", shape=(1, 2), dtype=float32)

The data is obtained through this function:

    def get_input_data():
       global X_input_data
       X_input_data = generate_images()
       X_input_data = np.array(X_input_data)
       X_input_data = X_input_data.reshape(17, 1)
       #X_input_data = X_input_data.reshape(-1, 17, 1)  -- if using this, error declares input shapes to be [1,2], [1,1], [1,1], [1]
    
       global Y_labels
       Y_labels = y

I've been stuck on this for a while and now need to complete the CRNN quite soon. I would like it to run on more epochs than 1 of course, but I am just trying to get it to run at all at the moment.

If someone could help me with resolving this, that would be very much appreciated. If you would like some more information/code then I'm sure I could provide it.

Thanks in advance",2,1,False,self,,,,,
71,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,8,a2iqxs,self.MachineLearning,What is Deep Probabilistic Programming used for?,https://www.reddit.com/r/MachineLearning/comments/a2iqxs/what_is_deep_probabilistic_programming_used_for/,c0cky_,1543794145,[removed],0,1,False,self,,,,,
72,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,8,a2irb6,self.MachineLearning,[D] What is Deep Probabilistic Programming used for?,https://www.reddit.com/r/MachineLearning/comments/a2irb6/d_what_is_deep_probabilistic_programming_used_for/,c0cky_,1543794211,"I've seen the post recently on the Pyro paper and checked it out a while back. I think one major piece that's missing for me (someone who has limited knowledge) is:

*what this type of modeling is useful for?*

If anyone has any resources or explanations it would be greatly appreciated",24,1,False,self,,,,,
73,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,9,a2iylb,self.MachineLearning,[R] NeurIPS 2018: Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding,https://www.reddit.com/r/MachineLearning/comments/a2iylb/r_neurips_2018_sparse_attentive_backtracking/,karuma10,1543795634,"Found this paper pretty interesting, [link](http://papers.nips.cc/paper/7991-sparse-attentive-backtracking-temporal-credit-assignment-through-reminding) here, really strong results.",1,1,False,self,,,,,
74,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,9,a2j3zk,self.MachineLearning,[Python / OpenCV] How to use extracted features for image classification?,https://www.reddit.com/r/MachineLearning/comments/a2j3zk/python_opencv_how_to_use_extracted_features_for/,sencinitas,1543796637,[removed],0,1,False,self,,,,,
75,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,10,a2jr1g,self.MachineLearning,[P] Time Series modeling with multiple independent time series,https://www.reddit.com/r/MachineLearning/comments/a2jr1g/p_time_series_modeling_with_multiple_independent/,MarSizzle,1543801158,"Hi there! I'll describe my problem:

I have multiple independent time series (say, 300 individual series), each of them has the same number of observed features per time step (say, 500 features), but each time series has its own number of time steps (i.e. the duration of the individual series is not constant over all series) that ranges from 100-400 time steps.

As a motivating example, say I have 300 songs. For every song, imagine that I have a feature extractor that will return 500 features for every 0.5 seconds of song play. For every 0.5 seconds, imagine a rating y that I'd like to predict where y is in {0, 100}. The songs can be of different duration (for instance, a song might be 30 seconds long, or it might be 3 minutes long).

My challenge has been to find and apply a model that would allow me to predict y at time step t given some arbitrary number of previous time steps and given any song in the dataset.

What I've done so far:

Attempted to train a Keras LSTM by using Keras' fit\_generator with steps\_per\_epoch = 1, epochs to 15000, and training generator randomly yielding one song's X\_train and y\_train (where X\_train is of shape (numTrainingExamples, 1, 500) and y is of shape (numTrainingExamples,), where numTrainingExamples is 2/3 \* the duration of the individual song).",2,1,False,self,,,,,
76,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,10,a2jvm1,deepmind.com,AlphaFold: Using AI for scientific discovery | DeepMind,https://www.reddit.com/r/MachineLearning/comments/a2jvm1/alphafold_using_ai_for_scientific_discovery/,Surextra,1543802092,,0,1,False,default,,,,,
77,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,10,a2jvwc,self.MachineLearning,[R] TDLS: Neural Image Caption Generation with Visual Attention,https://www.reddit.com/r/MachineLearning/comments/a2jvwc/r_tdls_neural_image_caption_generation_with/,tdls_to,1543802149,"**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**

This paper uses an attention mechanism to caption images and SOTA performance on BLEU and METEOR metrics. 

**Algorithm review:** [https://youtu.be/ENVGHs3yw7k](https://youtu.be/ENVGHs3yw7k)

**Discussions:** [https://youtu.be/u\_Mdp\_3RVRA](https://youtu.be/u_Mdp_3RVRA)

**Paper Reference:** [http://proceedings.mlr.press/v37/xuc15.pdf](http://proceedings.mlr.press/v37/xuc15.pdf)

&amp;#x200B;

**Discussion Points:**

Some of the points that we thought were important about the paper:

* Very strong work and journal paper, cited over 2300 times!
* First version appeared in ICML 2015 then the final version appeared in PMLR 2016
* Authors original code is in Theano but there are different implementations using tensorflow available on the web, check it out 
* This work is mainly based on Karpathy, Andrej and Li, Fei-Fei. Deep visual-semantic alignments for generating image descriptions. CVPR 2015. , but that work used an object detection approach instead of attention 
* There is some criticism against BLEU so the authors used METEOR as another metric. Soft and hard attention outperform the other approaches. Interestingly, soft attention obtains a better result with METEOR. However, the difference in real-world application is negligible
* Authors didnt provide a clue which attention mechanism is superior, and why?
* Length of the caption is a tricky issue while training, due to number of times that the LSTM should be run. To remedy this, the authors used a dictionary for storing captions of equal length

&amp;#x200B;

**Paper Abstract:**

Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.

&amp;#x200B;

**Paper Conclusions:** 

We propose an attention based approach that gives state of the art performance on three benchmark datasets using the BLEU and METEOR metric. We also show how the learned attention can be exploited to give more interpretability into the models generation process, and demonstrate that the learned alignments correspond very well to human intuition. We hope that the results of this paper will encourage future work in using visual attention. We also expect that the modularity of the encoder-decoder approach combined with attention to have useful applications in other domains",0,1,False,self,,,,,
78,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,10,a2jwv2,self.MachineLearning,What form of machine learning should I use?,https://www.reddit.com/r/MachineLearning/comments/a2jwv2/what_form_of_machine_learning_should_i_use/,Jamboc731,1543802346,"I am a university student making games, I'm trying to make a machine learning AI that learns how to search for and kill the player. Does anyone know what kind of machine learning I should be looking into? I started looking at neural networks using reinforcement learning or unsupervised learning but I don't know if either are what I am looking for.",0,1,False,self,,,,,
79,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,10,a2jx3v,deepmind.com,AlphaFold: Using AI for scientific discovery | DeepMind,https://www.reddit.com/r/MachineLearning/comments/a2jx3v/alphafold_using_ai_for_scientific_discovery/,scionaura,1543802392,,0,1,False,https://b.thumbs.redditmedia.com/7vWtZUocbaWaYv27OIiDNxapl9Cq8ITjO8uJl3pmkew.jpg,,,,,
80,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,11,a2k3sr,self.MachineLearning,[R] TDLS - Classics: Visualizing Data using t-SNE,https://www.reddit.com/r/MachineLearning/comments/a2k3sr/r_tdls_classics_visualizing_data_using_tsne/,tdls_to,1543803726,"**Visualizing Data using t-SNE**

we reviewed the original t-SNE paper, compared it to some of the other techniques, and discussed some of the practicalities of using it.

**Algorithm Review:** [https://youtu.be/PwDPIVHn8T0](https://youtu.be/PwDPIVHn8T0)

**Paper Discussions:** [https://youtu.be/ocE-oeFVkmY](https://youtu.be/ocE-oeFVkmY)

**Paper Reference:** [http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html)

&amp;#x200B;

**Discussion Points:**

we talked about some of the weaknesses of tSNE

* Can only be used to reduce to d = 2/3 dimensions. Hence cannot be used more generally due to heavy tails of t-distributions which may comprise large probability mass in higher D and thus loss of local embedding efficiency.
* Since the cost function is non-convex, the optimization parameter choice influences the local minima search and its a stochastic search so no guarantee you will reach global minima. However, using the prescribed list of hyper-parameters- its shown that seemingly different datasets can be visualized effectively.
* Global order is not completely kept, so the location of the clusters on the map doesn't mean anything 

**Paper abstract:** 

We present a new technique called ""t-SNE"" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.

&amp;#x200B;

**Paper conclusions:**

The paper presents a new technique for the visualization of similarity data that is capable of retaining the local structure of the data while also revealing some important global structure (such as clusters at multiple scales). Both the computational and the memory complexity of t-SNE are O(n\^2), but we present a landmark approach that makes it possible to successfully visualize large real-world data sets with limited computational demands. Our experiments on a variety of data sets show that t-SNE outperforms existing state-of-the-art techniques for visualizing a variety of real-world data sets. Matlab implementations of both the normal and the random walk version of t-SNE are available for download at http://ticc.uvt.nl/lvdrmaaten/tsne. In future work we plan to investigate the optimization of the number of degrees of freedom of the Student-t distribution used in t-SNE. This may be helpful for dimensionality reduction when the low-dimensional representation has many dimensions. We will also investigate the extension of t-SNE to models in which each high-dimensional datapoint is modeled by several low-dimensional map points as in Cook et al. (2007). Also, we aim to develop a parametric version of t-SNE that allows for generalization to held-out test data by using the t-SNE objective function to train a multilayer neural network that provides an explicit mapping to the low-dimensional space.",0,1,False,self,,,,,
81,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,11,a2kbq1,self.MachineLearning,[D] Straw Poll: How many of you are actively working on AGI?,https://www.reddit.com/r/MachineLearning/comments/a2kbq1/d_straw_poll_how_many_of_you_are_actively_working/,2Punx2Furious,1543805305,[removed],2,1,False,self,,,,,
82,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,11,a2kbws,self.MachineLearning,Using AI for Scientific discovery (DeepMind),https://www.reddit.com/r/MachineLearning/comments/a2kbws/using_ai_for_scientific_discovery_deepmind/,vector_machines,1543805344,[removed],0,1,False,self,,,,,
83,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,12,a2kq90,harald.co,NeurIPS 2018 Day 0: Expo,https://www.reddit.com/r/MachineLearning/comments/a2kq90/neurips_2018_day_0_expo/,hcarlens,1543808196,,0,1,False,default,,,,,
84,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,12,a2kvnm,self.MachineLearning,ML Stock/Number predictor,https://www.reddit.com/r/MachineLearning/comments/a2kvnm/ml_stocknumber_predictor/,Zman98789,1543809309,"I want to make a bot that takes in tons of stock prices and when I give it an indivisul stocks price history it gives me what it thinks the stocks price will be in the future. I'm not sure whare to start to make something like this but I looked at tensorflow and tensors and all that are very overwhelming to me.

P.s. I'm a beginer :).

Small side note the actual data I'm feading this isnt stock prices but it fluxuates and acts very similerly to stock prices",0,1,False,self,,,,,
85,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,13,a2l2a1,self.MachineLearning,AlphaFold: Predicting protein folding structure with deep learning from DeepMind,https://www.reddit.com/r/MachineLearning/comments/a2l2a1/alphafold_predicting_protein_folding_structure/,zerghunter,1543810724,[removed],0,1,False,self,,,,,
86,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,13,a2lb1v,self.MachineLearning,[D] Do all GANs have the ability to smoothly interpolate between latent variables?,https://www.reddit.com/r/MachineLearning/comments/a2lb1v/d_do_all_gans_have_the_ability_to_smoothly/,zeec123,1543812643,"This is a beginner question. In most GAN papers, there are images which are generated by interpolating between two latent variables. I was wondering if this is always possible.

Mathematically, the generator must be a continuous function to obey this property. Is this the case for most GANs?

For example:
[Generative Visual Manipulation on the Natural Image Manifold]
(https://arxiv.org/pdf/1609.03552.pdf)

If the answer is yes, what is the difference to algorithms like [InfoGAN](https://arxiv.org/abs/1606.03657)?",3,1,False,self,,,,,
87,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,14,a2lnjt,jalammar.github.io,"The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)",https://www.reddit.com/r/MachineLearning/comments/a2lnjt/the_illustrated_bert_elmo_and_co_how_nlp_cracked/,nortab,1543815387,,1,1,False,default,,,,,
88,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,15,a2luz7,self.MachineLearning,[D] Why do GANs produce images that are both very realistic and very bizarre?,https://www.reddit.com/r/MachineLearning/comments/a2luz7/d_why_do_gans_produce_images_that_are_both_very/,zergling103,1543817105,"With the release of BigGAN we've all probably seen some pretty amazing images being shared. The discriminator is able to guide the generator to the point of perfection with respect to texture, shading, lighting and even artistic effects like depth of field blur. But you'd think for it to capable of learning such a fine level of detail, it'd have figured out that dogs don't have two noses, that all the wheels of a car in should be the same size, and that foreground objects shouldn't be occluded by background objects.

It appears that doing more of the same on a larger scale (e.g. more training data, bigger networks, longer training time, etc.) isn't going to solve this issue, and that it'd most likely lead to GANs improving its ability to refine local features like texture even further with diminishing returns. Rather, some new approach (perhaps attention? An architecture that explicitly enforces 3D coherence?) perhaps is what is needed to close the gap.  


What have GAN researchers discovered as to why GANs seem to have this shortcoming and what ways they can be overcome?",11,1,False,self,,,,,
89,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,15,a2m1dh,self.MachineLearning,[R] What is subspace discriminant?,https://www.reddit.com/r/MachineLearning/comments/a2m1dh/r_what_is_subspace_discriminant/,OldManufacturer,1543818616,Saw this as an option in matlab and getting pretty good results. Just wondering if anyone knew more about it since I cant find much online. Thank you in advance.,3,1,False,self,,,,,
90,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,15,a2m4cx,self.MachineLearning,[D] Has there been any research/interest in Modular Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/a2m4cx/d_has_there_been_any_researchinterest_in_modular/,harshsikka123,1543819365,"Modular/Multiple Neural networks (MNNs) revolve around training smaller, independent networks that can feed into each other or another higher network [https://en.wikipedia.org/wiki/Modular\_neural\_network](https://en.wikipedia.org/wiki/Modular_neural_network)

In principle, the hierarchical organization could allow us to make sense of more complex problem spaces and reach a higher functionality, but it seems difficult to find examples of concrete research done in the past regarding this. I've found a few sources:

[https://www.teco.edu/\~albrecht/neuro/html/node32.html](https://www.teco.edu/~albrecht/neuro/html/node32.html)

[https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&amp;isAllowed=y](https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&amp;isAllowed=y)

A few concrete questions I have:

* *Are there any tasks where MNNs have shown better performance than large single nets?*
* *Could MNNs be used for multimodal classification, i.e. train each net on a fundamentally different type of data, (text vs image) and feed forward to a higher level intermediary that operates on all the outputs?*
* *From a software engineering perspective, aren't these more fault tolerant and easily isolatable on a distributed system?*
* *Has there been any work into dynamically adapting the topologies of subnetworks using a process like Neural Architecture Search?*
* *Generally, are MNNs practical in any way?*

Apologies if these questions seem naive, I've just come into ML and more broadly CS from a biology/neuroscience background and am captivated by the potential interplay.

I really appreciate you taking the time and lending your insight!",11,1,False,self,,,,,
91,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,15,a2m78x,self.MachineLearning,Deduplication of Records in a dataset,https://www.reddit.com/r/MachineLearning/comments/a2m78x/deduplication_of_records_in_a_dataset/,saikrishna_dataguy1,1543820081,[removed],0,1,False,self,,,,,
92,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,16,a2m9l9,youtube.com,Machine Learning vs Deep Learning vs Artificial Intelligence - My doubts have been cleared after long time,https://www.reddit.com/r/MachineLearning/comments/a2m9l9/machine_learning_vs_deep_learning_vs_artificial/,MainBuilder,1543820671,,0,1,False,https://b.thumbs.redditmedia.com/I3Q4cqatKbwqm-RrRN9rOQcCOQVhAzr4sN_iLCyuZng.jpg,,,,,
93,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,16,a2mav3,apsense.com,5 Best Skills to Get a Machine Learning Job!,https://www.reddit.com/r/MachineLearning/comments/a2mav3/5_best_skills_to_get_a_machine_learning_job/,smadrid056,1543820986,,0,1,False,default,,,,,
94,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,16,a2mlnu,self.MachineLearning,[P] AI for Social Good : Mumbai Slum Segmentation and Mapping,https://www.reddit.com/r/MachineLearning/comments/a2mlnu/p_ai_for_social_good_mumbai_slum_segmentation_and/,cbsudux,1543823859,"Hey guys! I currently work in Mumbai and the city is pretty popular for its slums. Ive interacted with slum dwellers of the major slums and I quickly realised the situation was much worse than I had thought. I learnt that a staggering 62% of Mumbai live in slums. I felt very strongly about this and I wanted to do something that would help them actively in the long run (5-10 years) and has the potential for large scale slum improvement.

&amp;#x200B;

So we built this : [https://github.com/cbsudux/Mumbai-slum-segmentation](https://github.com/cbsudux/Mumbai-slum-segmentation)

&amp;#x200B;

We also published our work in the NeurIPS (NIPS) 2018 ML4D workshop. (Paper link : [https://arxiv.org/abs/1811.07896](https://arxiv.org/abs/1811.07896))",13,1,False,self,,,,,
95,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,17,a2mvbj,weirdgeek.com,Applying Linear Regression to Boston Housing Dataset,https://www.reddit.com/r/MachineLearning/comments/a2mvbj/applying_linear_regression_to_boston_housing/,WeirdGeekDotCom,1543826621,,0,1,False,https://b.thumbs.redditmedia.com/siLrdcEMhpfcqn5DZ5sKTXG0XhdIU7B13kJae9OG5Mc.jpg,,,,,
96,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,17,a2mwhk,youtube.com,"Ben Goertzel from SingularityNET will be on Joe Rogan's podcast tomorrow, 4th of December, 1pm PST.",https://www.reddit.com/r/MachineLearning/comments/a2mwhk/ben_goertzel_from_singularitynet_will_be_on_joe/,crypto_monkey,1543826960,,0,1,False,https://b.thumbs.redditmedia.com/TuFPdGu20fxWnyuZXwTx6QfPfLHg6boJwKevZ6aM-Sw.jpg,,,,,
97,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,18,a2n1r7,self.MachineLearning,"Global Machine Learning Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/a2n1r7/global_machine_learning_market_size_outlook/,srikanthenvision,1543828507,"The machine learning holds the highest CAGR of 44.86% during the forecast period 2019-2025.
Download a Sample Report at:- https://www.envisioninteligence.com/industry-report/global-machine-learning-market/?utm_source=reddit-srikanth",0,1,False,self,,,,,
98,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,18,a2n3j1,letsdiskuss.com,Choosing between SAS R and Python for Big Data Solution,https://www.reddit.com/r/MachineLearning/comments/a2n3j1/choosing_between_sas_r_and_python_for_big_data/,rritika,1543829023,,0,1,False,default,,,,,
99,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,18,a2n9ge,self.MachineLearning,[D] Papers / solutions for ML model maintenance in production,https://www.reddit.com/r/MachineLearning/comments/a2n9ge/d_papers_solutions_for_ml_model_maintenance_in/,pp314159,1543830736,"I'm looking for articles or ready solutions (preferably open source) for proper ML model maintenance in production. I'm looking for techniques that can:
 - monitor concept drift
 - provide feedback loop
 - decides when to retrain the model based on feedback

Do you know approaches for making decision when to retrain the model based on feedback? ",2,1,False,self,,,,,
100,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,20,a2nofx,medium.com,Forecasting the USD - Mongolian Tugrik Exchange Rate with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a2nofx/forecasting_the_usd_mongolian_tugrik_exchange/,robertritz,1543835035,,0,1,False,https://b.thumbs.redditmedia.com/AgvFikQGpXdFhiXzgF7LsNvibkXQEO0ZT2svI9DzKQQ.jpg,,,,,
101,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,20,a2nw3o,self.MachineLearning,Good books on kalman filters,https://www.reddit.com/r/MachineLearning/comments/a2nw3o/good_books_on_kalman_filters/,mufflonicus,1543837159,[removed],0,1,False,self,,,,,
102,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,20,a2nwww,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a2nwww/global_machine_learning_market_size_outlook/,santhosh2443,1543837391,,0,1,False,https://b.thumbs.redditmedia.com/fo6g_7PDCiGjJ625DgwOMLe4aiAFYMxsFPSzW0GStfo.jpg,,,,,
103,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,20,a2o054,self.MachineLearning,A query on ML modeling,https://www.reddit.com/r/MachineLearning/comments/a2o054/a_query_on_ml_modeling/,pvramnath3,1543838275,"Hi, I am relatively new to ML and am trying to solve a ML problem. The details are given below.

The sensor data (5 features) are collected at different point of timing. The event recorder has also collected the event (1 event) at the regular time intervals. There is no direct correlation between sensor data and event recorder data except for time of collection, which also does not match exactly. This is the train data.

The test data contains the sensor data collected at the different time intervals and the corresponding event recorder data has to be learnt with the time of event collection.

I just need a help as how i can formulate the relationship between the sensor data and the event recorder data with the time of collection. Will NN help here ? Or any other mechanisms to do this ?

Pls. do let me know.

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
104,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,21,a2o1ft,self.MachineLearning,[D] Does many-to-one neural network imply ill-conditioned?,https://www.reddit.com/r/MachineLearning/comments/a2o1ft/d_does_manytoone_neural_network_imply/,phizaz,1543838598,"I have read some introduction to ill-conditioned matrices, and it is still a very new idea to me. (see [https://www.quora.com/What-does-it-mean-to-have-a-poorly-conditioned-Hessian-matrix](https://www.quora.com/What-does-it-mean-to-have-a-poorly-conditioned-Hessian-matrix))

I wonder though if I want to approximate a many-to-one function with a neural network.

By the sole fact of being many-to-one, does this already imply that there is an ill-conditioned matrix involved? ",3,1,False,self,,,,,
105,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,21,a2oaiy,self.MachineLearning,[R] AlphaFold: Using AI for scientific discovery,https://www.reddit.com/r/MachineLearning/comments/a2oaiy/r_alphafold_using_ai_for_scientific_discovery/,P4TR10T_TR41T0R,1543840861,"DeepMind recently released a new blog post about their system called AlphaFold, whose aim is to predict 3D models of proteins. I guess they will soon present their paper, since at the moment the article is light on the details.

&amp;#x200B;

[https://deepmind.com/blog/alphafold/](https://deepmind.com/blog/alphafold/)

HN: [https://news.ycombinator.com/item?id=18587612](https://news.ycombinator.com/item?id=18587612)",52,1,False,self,,,,,
106,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,22,a2ohig,self.MachineLearning,[D] B-LTSM vs B-ConvLTSM,https://www.reddit.com/r/MachineLearning/comments/a2ohig/d_bltsm_vs_bconvltsm/,Doomro22,1543842513,"Hi,

I am trying to compare two architectures, the bidirectional LSTM and the bidirectional ConvLTSM as in [https://arxiv.org/abs/1506.04214](https://arxiv.org/abs/1506.04214)

I expected the ConvLSTM architecture to perform better than the classical LSTM as in the paper but the difference is quite significant in favor of the LTSM. I am trying to come up with a good explanation. The input of my networks are spectrograms.

Here is the code I used for both in Keras (that I believe to be correct):

`input_layer = Input(shape=(None, 80))`

`x = Reshape((-1, 1, 1, 80))(input_layer)`

`x = Bidirectional(ConvLSTM2D(32, 5, 1, padding='same', data_format='channels_last', dilation_rate=1, return_sequences=True, dropout=0.05))(x)`

`x = Reshape((-1, x._keras_shape[4]))(x)`

`output_layer = TimeDistributed(Dense(config.CLASSES, activation='sigmoid'))(x)`

`model = Model(input_layer, output_layer)`

and 

`model = Sequential()`

`model.add(Bidirectional(LSTM(150, dropout=0.05, return_sequences=True), input_shape=(None, 80)))`

`model.add(TimeDistributed(Dense(config.CLASSES, activation='sigmoid')))`

&amp;#x200B;

I also did a hyper-parameter optimization with *hyperas* to come up with a good configuration.

Did I do a mistake or miss a point somewhere? Do some of you have a good insight on the reason for this unexpected result?",7,1,False,self,,,,,
107,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,22,a2oizi,self.MachineLearning,[R] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations,https://www.reddit.com/r/MachineLearning/comments/a2oizi/r_challenging_common_assumptions_in_the/,konasj,1543842844,"Pretty nice and comprehensive paper [1]. Key results (at least for me):

- The is no disentanglement in unsupervised learning without inductive priors on data and model
- Extensive evaluation of common approaches for disentanglement shows disentangling does not improve sample efficiency

Actually, I like this paper as it is a good piece of science: a lot of empirical evaluation and evidence to prove your claims, a nice theoretic insight, punching hard evidence/theory holes in fluffy folklore about unsupervised modeling :-)

[1] https://arxiv.org/pdf/1811.12359v1.pdf
",9,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,22,a2oojg,julialang.org,Building a Language and Compiler for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a2oojg/building_a_language_and_compiler_for_machine/,one_more_minute,1543844068,,0,1,False,default,,,,,
109,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,22,a2op0r,self.MachineLearning,[D] HDD vs SSD vs M.2 NVME for machine learning,https://www.reddit.com/r/MachineLearning/comments/a2op0r/d_hdd_vs_ssd_vs_m2_nvme_for_machine_learning/,crytoy,1543844174,"For Training Resnet50 and vgg16, would using an HDD really slow-down the training process?

Which data solution can fully saturate the GPU ""rtx 2070"" without being too expensive?

&amp;#x200B;",8,1,False,self,,,,,
110,MachineLearning,t5_2r3gv,2018-12-3,2018,12,3,22,a2osev,julialang.org,[R] Building a Language and Compiler for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a2osev/r_building_a_language_and_compiler_for_machine/,one_more_minute,1543844918,,0,1,False,default,,,,,
111,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,0,a2pgbi,i.redd.it,ResNet50 looking at rotating moles (first 5 channels 8x8 activations last layer),https://www.reddit.com/r/MachineLearning/comments/a2pgbi/resnet50_looking_at_rotating_moles_first_5/,ragulpr,1543849597,,1,1,False,default,,,,,
112,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,0,a2pv7o,self.MachineLearning,"Problem: MultiModal net with 2 types of features (image and text) overfits in one type, doesn't use the other",https://www.reddit.com/r/MachineLearning/comments/a2pv7o/problem_multimodal_net_with_2_types_of_features/,gombru,1543852372,"Hello,

I'm working on a classification problem where I have features from two differents modalities (image and text, extracted by a CNN and a RNN) per each sample. Both features are discriminatory and not redundant. However, when I combine them I don't get better results than when using a single modality:

&amp;#x200B;

&amp;#x200B;

![img](1t279ic543221)

\- Image and textual information are not redundant

\- To combine features for Ca,b I've tried basic concatenation and more complex techniques, but none seem to work.

\- Ca classifies correctly arround 60% of the samples that Cb classifies wrong. 

\-When training (using ADAM) Ca,b overfits fast on Textual features. The resulting model has similar behaviour as Cb, and similar performance when only Textual features are inputed.

\- I'm using a pretrained CNN as feature extractor and building my MultiModal model on top of it, merging Textual features in a high level layer. 

\- I have tried adding a strong dropout on Textual features and the model does not overfit until advanced epochs, but I still get que same result.  


Do you have any idea on how can I build a model able to exploit both feature modalities? Where can I find literature about this?  
",0,1,False,self,,,,,
113,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,0,a2pwfk,self.MachineLearning,IJCAI Application Papers?,https://www.reddit.com/r/MachineLearning/comments/a2pwfk/ijcai_application_papers/,skywang329,1543852587,[removed],0,1,False,self,,,,,
114,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,1,a2pyxh,machinelearning.apple.com,[R] Optimizing Siri on HomePod in FarField Settings,https://www.reddit.com/r/MachineLearning/comments/a2pyxh/r_optimizing_siri_on_homepod_in_farfield_settings/,Rexall_Pharmacy,1543853005,,0,1,False,https://b.thumbs.redditmedia.com/baV5mF4pWxiNJyHqSutnWHk2-sjj4ryXohps7a8NP0w.jpg,,,,,
115,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,1,a2pzrk,self.MachineLearning,[Meta] There should be a live megathread for #NeurIPS2018,https://www.reddit.com/r/MachineLearning/comments/a2pzrk/meta_there_should_be_a_live_megathread_for/,leonoel,1543853140,[removed],0,1,False,self,,,,,
116,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,1,a2q20f,self.MachineLearning,"[P] Blogpost: a review of self-supervised learning, part 1",https://www.reddit.com/r/MachineLearning/comments/a2q20f/p_blogpost_a_review_of_selfsupervised_learning/,shgidigo,1543853524,[removed],0,1,False,self,,,,,
117,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,1,a2q4oe,weirdgeek.com,Applying Linear Regression to Boston Housing Dataset,https://www.reddit.com/r/MachineLearning/comments/a2q4oe/applying_linear_regression_to_boston_housing/,WeirdGeekDotCom,1543854004,,0,1,False,default,,,,,
118,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,1,a2q593,medium.com,Google AI Music Project Magenta Drops Beats Like Humans,https://www.reddit.com/r/MachineLearning/comments/a2q593/google_ai_music_project_magenta_drops_beats_like/,gwen0927,1543854103,,0,1,False,default,,,,,
119,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,1,a2qere,bigstep.com,Recording and Jupyter Notebook for the Data Science Webinar on Recommender Systems - From Simple to Complex,https://www.reddit.com/r/MachineLearning/comments/a2qere/recording_and_jupyter_notebook_for_the_data/,supercake53,1543855731,,0,1,False,https://b.thumbs.redditmedia.com/s2rBfwoOJmhw4nUelMpUu2DLHnKPlwSmaXmycV-ttHo.jpg,,,,,
120,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,1,a2qieb,self.MachineLearning,Codecademy launched Natural Language Processing!,https://www.reddit.com/r/MachineLearning/comments/a2qieb/codecademy_launched_natural_language_processing/,sonnynomnom,1543856376,[removed],0,1,False,self,,,,,
121,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,2,a2qkks,self.MachineLearning,How to predict the headcount of people in building?,https://www.reddit.com/r/MachineLearning/comments/a2qkks/how_to_predict_the_headcount_of_people_in_building/,ollipekkav,1543856733,[removed],0,1,False,self,,,,,
122,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,2,a2qmsu,tech.trivago.com,"Teardown, Rebuild: Migrating from Hive to PySpark",https://www.reddit.com/r/MachineLearning/comments/a2qmsu/teardown_rebuild_migrating_from_hive_to_pyspark/,mre__,1543857101,,0,1,False,default,,,,,
123,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,2,a2qsgi,twitch.tv,Slightly optimized ML monsters [stream],https://www.reddit.com/r/MachineLearning/comments/a2qsgi/slightly_optimized_ml_monsters_stream/,spiderpai,1543858073,,0,1,False,default,,,,,
124,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,2,a2qtfj,self.MachineLearning,[D] Upgrading a cheap desktop for playing around with CNNs -- How much RAM? Best upgrade path?,https://www.reddit.com/r/MachineLearning/comments/a2qtfj/d_upgrading_a_cheap_desktop_for_playing_around/,ZombieLincoln666,1543858270,"I'm self-learning ML and CNNs, with my interest in applications for medical imaging and also generative art stuff. This is not for production or professional use, just for myself. I'm wondering if there are immediate upgrades that I will need, and if this PC is worth upgrading later when used 1080ti or 2080ti prices have decreased.

I have a pre-built HP desktop that I bought when GPU prices were ridiculous last year during the cryptomining craze, and it was a very good deal at the time (~$600). I just bought a used gtx 1070 along with a new PSU for it.

So the specs right now are:

* Micro-ITX HP motherboard, H170 chipset (pretty limited -- only supports up to 16 gb ram, and up to an i7-7700k CPU)

* CPU: i5-7400 (4 cores, 3.0Ghz)

* RAM: 2x4 = 8 gb DDR4-2400 (PC4-19200 MB/s)

* GPU: Zotac GTX 1070 mini w/ 8gb VRAM (will replace the 1060 3gb that came with it)

* 500gb crucial mx500 SATA SSD -- Windows 10

* 1tb 7400 rpm HDD -- Ubuntu 18.04 

* PSU: EVGA G2 550W (will replace the 300 w PSU that came with it)

The case is tiny (which I like), but it's doesn't fit anything other than small form factor GPUs, at least without removing the one optical drive, although it that is definitely an option. I'm adding a 140 mm intake fan on the top to help cool the new GPU. 

&gt;&gt;&gt; Will I need to upgrade to 16gb of ram?

How well will this setup work? ",6,1,False,self,,,,,
125,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,3,a2r6sx,self.MachineLearning,Feasibility of pursuing masters in Machine Learning [Discussion],https://www.reddit.com/r/MachineLearning/comments/a2r6sx/feasibility_of_pursuing_masters_in_machine/,anantvindal,1543860374,"I will graduate in 2020 so I spend quite some time online looking at Universities. But considering the amount of courses available, Machine Learning as a skill might become as common as knowing any other programming language. So is it still feasible to do a masters in Machine Learning or would it be better for me to do some other specialisation whilst not losing my grip on Machine Learning. I'm just a beginner in ML though, looking for colleges outside India.",10,1,False,self,,,,,
126,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,3,a2r84j,self.MachineLearning,Building a a customizable text generation model utilizing LSTM network,https://www.reddit.com/r/MachineLearning/comments/a2r84j/building_a_a_customizable_text_generation_model/,notoriousjoeski,1543860596,[removed],0,1,False,self,,,,,
127,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,3,a2rd5y,self.MachineLearning,Machine learning in a practical application scenario: Free online tool for text summarization and named-entity recognition,https://www.reddit.com/r/MachineLearning/comments/a2rd5y/machine_learning_in_a_practical_application/,developFFM,1543861438,[removed],0,1,False,self,,,,,
128,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,3,a2rg70,codingame.com,Handwritten Digit Recognition (MNIST) Using scikit-learn,https://www.reddit.com/r/MachineLearning/comments/a2rg70/handwritten_digit_recognition_mnist_using/,maxime81,1543861951,,0,1,False,default,,,,,
129,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,3,a2rhor,self.MachineLearning,Machine learning in a practical application scenario: free online tool for text summarization and named-entity recognition [Project],https://www.reddit.com/r/MachineLearning/comments/a2rhor/machine_learning_in_a_practical_application/,developFFM,1543862200,"How to video: [https://youtu.be/jksjwE0cpfQ](https://youtu.be/jksjwE0cpfQ)

Online tool: [http://playground.aisoma.de/summarizer/?lang=us](http://playground.aisoma.de/summarizer/?lang=us)

Blog post: [https://www.aisoma.de/nlp-in-practice-a-web-app-demo-for-text-summarization-and-named-entity-recognition/](https://www.aisoma.de/nlp-in-practice-a-web-app-demo-for-text-summarization-and-named-entity-recognition/)",6,1,False,self,,,,,
130,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,3,a2rodv,rise.cs.berkeley.edu,An Open Source Tool for Scaling Multi-Agent Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a2rodv/an_open_source_tool_for_scaling_multiagent/,rayspear,1543863300,,0,1,False,default,,,,,
131,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,4,a2rwvh,self.MachineLearning,Building customizable text generation models utilizing rnns,https://www.reddit.com/r/MachineLearning/comments/a2rwvh/building_customizable_text_generation_models/,notoriousjoeski,1543864694,[removed],0,1,False,self,,,,,
132,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,5,a2sf5o,franz.media,Advent of Data Science - The RANSACRegressor,https://www.reddit.com/r/MachineLearning/comments/a2sf5o/advent_of_data_science_the_ransacregressor/,gadgetarian_me,1543867726,,0,1,False,https://b.thumbs.redditmedia.com/IEH5BB4qFFdspaqqwlpuB4MhVTKOCcGuPoHDy8k9_5o.jpg,,,,,
133,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,5,a2sgv7,self.MachineLearning,"[P] Chapter 8, Advanced Practice, of the Hundred-Page Machine Learning Book is out",https://www.reddit.com/r/MachineLearning/comments/a2sgv7/p_chapter_8_advanced_practice_of_the_hundredpage/,RudyWurlitzer,1543868019,"The eighth chapter of The Hundred Page Machine Learning Book, ""Advanced Practice"", is published on the [book's website](http://themlbook.com/). Enjoy your reading!

And as always, please send me your comments/corrections. The easiest ways to do that is by annotating the PDF file (no special software needed) or by leaving the comment directly in Dropbox.",0,1,False,self,,,,,
134,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,5,a2sisi,self.MachineLearning,Neuroevolution in practice (or other ML algorithms with non-differentiable loss functions),https://www.reddit.com/r/MachineLearning/comments/a2sisi/neuroevolution_in_practice_or_other_ml_algorithms/,kalabele,1543868339,[removed],0,1,False,self,,,,,
135,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,6,a2t0j1,animalbehaviorandcognition.org,"Rats Make Friends With Robots, and Rescue Them When Stuck (2018).",https://www.reddit.com/r/MachineLearning/comments/a2t0j1/rats_make_friends_with_robots_and_rescue_them/,CL20,1543871204,,0,1,False,default,,,,,
136,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,6,a2t51h,jalammar.github.io,"The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)",https://www.reddit.com/r/MachineLearning/comments/a2t51h/the_illustrated_bert_elmo_and_co_how_nlp_cracked/,wavelander,1543871921,,0,1,False,default,,,,,
137,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,6,a2t5sk,nature.com,[R] Dimensionality reduction for visualizing single-cell data using UMAP,https://www.reddit.com/r/MachineLearning/comments/a2t5sk/r_dimensionality_reduction_for_visualizing/,bjallan,1543872042,,0,1,False,default,,,,,
138,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,6,a2tbfb,self.MachineLearning,How would I go about highlighting the difference between a base image and a slightly different version of that image.,https://www.reddit.com/r/MachineLearning/comments/a2tbfb/how_would_i_go_about_highlighting_the_difference/,demoem,1543872894,"**Preface:** My experience with ML is pretty limited, I've done ~70% of Andrew Ng's course other than that I have no experience. I also have no experience with DL.

**Problem:**
I have a PCB with LEDs on it and I want to be able to distinguish which LEDs are on or off compared to some base image. How would I go about doing this?

I've thought about using a multi-class NN to be able to classify each LED as a number (0-20) . Then I would be able to return the values depending on which LEDs were on. I could then compare the returned values to the expected value.

I'm not sure if this is the best approach because I do know CNNs are supposed to be better for computer vision and picture related problems so I'm just looking for some insight and help.

Thanks in advance.
",0,1,False,self,,,,,
139,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,6,a2tgno,github.com,tensorflow/ranking is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/a2tgno/tensorflowranking_is_a_new_github_repo_by/,sjoerdapp,1543873696,,0,1,False,default,,,,,
140,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,6,a2tkx9,machinelearning.apple.com,[R] Optimizing Siri on HomePod in FarField Settings - Apple,https://www.reddit.com/r/MachineLearning/comments/a2tkx9/r_optimizing_siri_on_homepod_in_farfield/,bobchennan,1543874358,,0,1,False,https://b.thumbs.redditmedia.com/baV5mF4pWxiNJyHqSutnWHk2-sjj4ryXohps7a8NP0w.jpg,,,,,
141,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,7,a2tnz0,self.MachineLearning,[R] Learning to Predict Depth on the Mobile Phones - How Google uses neural networks for their Portrait Mode,https://www.reddit.com/r/MachineLearning/comments/a2tnz0/r_learning_to_predict_depth_on_the_mobile_phones/,ranihorev,1543874828,"Google recently released a blog post that explains how they generate trained a convolutional neural network to improve their portrait mode on Pixel 3. 

[https://ai.googleblog.com/2018/11/learning-to-predict-depth-on-pixel-3.html](https://ai.googleblog.com/2018/11/learning-to-predict-depth-on-pixel-3.html)

They built a custom ""Frankenphone"" rig that combines five phones and allows them to generate high-quality depth estimation from photos.  ",22,1,False,self,,,,,
142,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,7,a2to58,machinelearning.apple.com,[R] Optimizing Siri on HomePod in FarField Settings - Apple,https://www.reddit.com/r/MachineLearning/comments/a2to58/r_optimizing_siri_on_homepod_in_farfield_settings/,bobchennan,1543874855,,0,1,False,default,,,,,
143,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,8,a2ub38,ayasdi.com,Answering DeepMind's question about the origin of graphs for use in graph networks,https://www.reddit.com/r/MachineLearning/comments/a2ub38/answering_deepminds_question_about_the_origin_of/,jtsymonds,1543878351,,0,1,False,https://b.thumbs.redditmedia.com/GexcFVmHOyz22HusN97767ClXtUGSspHWA-dhN4S2Jw.jpg,,,,,
144,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,8,a2ufgt,self.MachineLearning,MS in Machine Learning with a BS in Computer Engineering,https://www.reddit.com/r/MachineLearning/comments/a2ufgt/ms_in_machine_learning_with_a_bs_in_computer/,SanderzFor3,1543879077,Rising college studnet here. Is it feasible to major in computer engineering (with CS heavier electives) and go on to master in machine learning? I'd like to have the background in embedded systems and design. Or should I switch to CS? I've heard a lot of different education paths for those who want to work on AI and ML. Thanks.,0,1,False,self,,,,,
145,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,8,a2ulml,self.MachineLearning,Artificial Intelligence: Adaption and Acceptance Questionnaire,https://www.reddit.com/r/MachineLearning/comments/a2ulml/artificial_intelligence_adaption_and_acceptance/,kylajaycee,1543880115,[removed],0,1,False,self,,,,,
146,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,9,a2v8mj,medium.com,[N] NeurIPS 2018 Best Papers Announced,https://www.reddit.com/r/MachineLearning/comments/a2v8mj/n_neurips_2018_best_papers_announced/,gwen0927,1543884390,,0,1,False,default,,,,,
147,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,10,a2vhrd,self.MachineLearning,Training Error and Validation error,https://www.reddit.com/r/MachineLearning/comments/a2vhrd/training_error_and_validation_error/,veejarAmrev,1543886158,"If training and validation error while training neural network are almost equal, does this always mean that model is underfitting? What could be other possibility here, if any?",0,1,False,self,,,,,
148,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,10,a2vurn,self.MachineLearning,Bio and Machine leaning project ideas,https://www.reddit.com/r/MachineLearning/comments/a2vurn/bio_and_machine_leaning_project_ideas/,sidnand,1543888663,Hey for my biology 11 class I want to combine my love for machine learning and biology. Does anyone have an ideas for simple but cool machine learning and biology projects that I could complete in a 1 week? ,0,1,False,self,,,,,
149,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,11,a2w7re,self.MachineLearning,Is it possible for Machine Learning and AI to change the future?,https://www.reddit.com/r/MachineLearning/comments/a2w7re/is_it_possible_for_machine_learning_and_ai_to/,Ruchi89,1543891130," Technology is readily outpacing the innovation and market trend, as more and more people keep talking about how Machine Learning could impact the lives of many. Particularly when **automation technologies** such as **Machine Learning** and **AI** will play an important role in our day to day lives, the effect it will have in our workspace is now a major concern.

Should we be worried, when there will be things like driverless cars hitting the road few years down the line? Its still hard to say for sure exactly how and what those changes will look like before such vehicles are even on the roads. 

 Even the thought of it freaks us out, the biggest fears still concern us as to how likely are we going to rely on technology with regards to our safety concerns is still a question we need to ponder upon.

 Artificial Intelligence has already taken that step of predicting the future, most of the Doctors are already using it to predict when a patient is most likely to have a heart attack or a stroke. There are many decisions in our lives which require a good forecast and AI are almost good at predicting those, but due to technology advances, we still seem to lack the confidence in AI predictions.

 With all these sorts of societal benefits, its up to us to decide what we want them to do. Algorithms arent going anywhere, and I think we should all agree that theyre only going to get more powerful day by day unless the academics, technologists and the stakeholders do not determine a process to hold the algorithms and the tech companies back, were all at risk.

What can we do today in order to integrate them without disrupting out societal ethics?

&amp;#x200B;",0,1,False,self,,,,,
150,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,11,a2wahz,self.MachineLearning,What is the best regression model for finding the function of weights for target outputs?,https://www.reddit.com/r/MachineLearning/comments/a2wahz/what_is_the_best_regression_model_for_finding_the/,probably_likely_mayb,1543891647,[removed],0,1,False,self,,,,,
151,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,11,a2wdmf,self.MachineLearning,[Project] What approach would you take for image ranking by emotional intensity?,https://www.reddit.com/r/MachineLearning/comments/a2wdmf/project_what_approach_would_you_take_for_image/,probably_likely_mayb,1543892245,"I have images that I've classified into different emotions like happy, sad, neutral, disgusted where the output is a percentage of confidence in each emotion adding up to 1.0.

My goal is to find the images who have the most intense emotions out of a group of input images by finding which weights need to be applied to each emotion percentage. The tricky part is that my classifier is trained better for some emotions than others and often has happy at 99% for an inferior image than say, disgusted at 62%.

If I were to take a new set of training data of extreme emotion vs very little emotion, is there a regression approach (or anything else that's more apt that I'm missing) for finding the function of what weights need to be applied to the emotion percentages to get an ideal score for ranking my input images by emotional intensity?",8,1,False,self,,,,,
152,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,12,a2wo7z,self.MachineLearning,Announcing Clusterone SaaS,https://www.reddit.com/r/MachineLearning/comments/a2wo7z/announcing_clusterone_saas/,mhejrati,1543894270,[removed],0,1,False,self,,,,,
153,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,12,a2wt1n,self.MachineLearning,[D] State of the art in facial emotion recognition,https://www.reddit.com/r/MachineLearning/comments/a2wt1n/d_state_of_the_art_in_facial_emotion_recognition/,nobodykid23,1543895241,"I'm currently researching model in recognizing facial emotion used to evaluate generated emotion sampled from my GAN model.

Does anyone have recommendation on state-of-the-art model specified for (facial) emotion recognition? Especially the ones resulted in probability of each emotion classes",2,1,False,self,,,,,
154,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,14,a2xrqw,self.MachineLearning,[R] Optimizing Siri on HomePod in FarField Settings,https://www.reddit.com/r/MachineLearning/comments/a2xrqw/r_optimizing_siri_on_homepod_in_farfield_settings/,ranihorev,1543902176,"An article by Apple describing how the use machine learning to improve HomePods signal processing system to remove echo, background noise and separate sound sources.
 
https://machinelearning.apple.com/2018/12/03/optimizing-siri-on-homepod-in-far-field-settings.html",0,1,False,self,,,,,
155,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,15,a2y03g,apachesparkcertification.wordpress.com,Introduction to Apache Spark: Big Data Analytics Simplified,https://www.reddit.com/r/MachineLearning/comments/a2y03g/introduction_to_apache_spark_big_data_analytics/,rritika,1543904123,,0,1,False,default,,,,,
156,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,15,a2y4v0,self.MachineLearning,Minibatch discrimination implementation,https://www.reddit.com/r/MachineLearning/comments/a2y4v0/minibatch_discrimination_implementation/,-beubeu-,1543905291,[removed],0,1,False,self,,,,,
157,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,15,a2y7ez,self.MachineLearning,[D] Minibatch Discrimination - implementation,https://www.reddit.com/r/MachineLearning/comments/a2y7ez/d_minibatch_discrimination_implementation/,-beubeu-,1543905926," 

I am currently trying to implement minibatch discrimination, as described here: [https://arxiv.org/pdf/1606.03498.pdf](https://arxiv.org/pdf/1606.03498.pdf)(section 3.2).

Does anyone know what is the best way to define the dimensions of the transformation tensor T, and also the best way to initialize its elements?

Also, looking at the Keras and Chainer implementations ([here](https://github.com/keras-team/keras/pull/3677/commits/979a00df42a9ed15c5f182d7ffce05700158562a) and [here](https://github.com/musyoku/minibatch_discrimination/blob/master/minibatch_discrimination.py), respectively) I see that the L1-norm is always calculated by taking the transpose of the matrix M. I could not understand why this is done. Could someone clarify? I suspect the answer is related to some simple linear algebra rule.

I would also be happy if anyone that has used this technique in the past could comment on the results obtained when compared to using no minibatch discrimination.",4,1,False,self,,,,,
158,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,16,a2yk08,technews1.quora.com,Why Its Essential For Businesses To Have Chatbots,https://www.reddit.com/r/MachineLearning/comments/a2yk08/why_its_essential_for_businesses_to_have_chatbots/,tech-info,1543909173,,0,1,False,default,,,,,
159,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,16,a2ykfz,self.MachineLearning,object tracking,https://www.reddit.com/r/MachineLearning/comments/a2ykfz/object_tracking/,medhav0000,1543909288,[removed],0,1,False,self,,,,,
160,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,18,a2z20f,self.MachineLearning,Resizer - A Quick Python shell script to resize and reduce the size of images,https://www.reddit.com/r/MachineLearning/comments/a2z20f/resizer_a_quick_python_shell_script_to_resize_and/,codezoned,1543914470,[removed],0,1,False,self,,,,,
161,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,18,a2z8ad,zachpfeffer.com,Transcript of Ali Rahimi NIPS 2017 Test-of-Time Award Presentation Spe | Home | Zach's Blog,https://www.reddit.com/r/MachineLearning/comments/a2z8ad/transcript_of_ali_rahimi_nips_2017_testoftime/,ZachPfeffer,1543916354,,0,1,False,https://a.thumbs.redditmedia.com/ojBrQstiAOLAkIaEtP8m5OZhibHTIeRIn3h-Dscxw10.jpg,,,,,
162,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,19,a2zff8,youtu.be,The First Interactive AI Rendered Virtual World,https://www.reddit.com/r/MachineLearning/comments/a2zff8/the_first_interactive_ai_rendered_virtual_world/,cmillionaire9,1543918404,,0,1,False,https://a.thumbs.redditmedia.com/ejwGNFhXcT5jax9SF8CdTdeiRvHW3hhNqPGB2lrqeX0.jpg,,,,,
163,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,21,a30cuw,arxiv.org,[1812.00334] Image Score: How to Select Useful Samples,https://www.reddit.com/r/MachineLearning/comments/a30cuw/181200334_image_score_how_to_select_useful_samples/,ihaphleas,1543927486,,6,1,False,default,,,,,
164,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,22,a30rbj,crate.io,"Machine Learning and CrateDB, Part Two: Getting Started With Jupyter",https://www.reddit.com/r/MachineLearning/comments/a30rbj/machine_learning_and_cratedb_part_two_getting/,nachrieb,1543930830,,0,1,False,default,,,,,
165,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,22,a30tm3,soundcloud.com,[N] Interview with Isabel Drost-Fromm co-founder of Apache Mahout and member of the Apache Software Foundation,https://www.reddit.com/r/MachineLearning/comments/a30tm3/n_interview_with_isabel_drostfromm_cofounder_of/,newthinkingevents,1543931354,,0,1,False,default,,,,,
166,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,23,a311oq,iwm.fraunhofer.de,Materials data space for additive manufacturing - creating digital twins of materials,https://www.reddit.com/r/MachineLearning/comments/a311oq/materials_data_space_for_additive_manufacturing/,Erik_Feder,1543933044,,0,1,False,default,,,,,
167,MachineLearning,t5_2r3gv,2018-12-4,2018,12,4,23,a316mj,ai.googleblog.com,The NeurIPS 2018 Test of Time Award: The Trade-Offs of Large Scale Learning,https://www.reddit.com/r/MachineLearning/comments/a316mj/the_neurips_2018_test_of_time_award_the_tradeoffs/,sjoerdapp,1543934063,,0,1,False,default,,,,,
168,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,0,a31eep,self.MachineLearning,[D] NeurIPS (prev. NIPS) papers selection?,https://www.reddit.com/r/MachineLearning/comments/a31eep/d_neurips_prev_nips_papers_selection/,sizaka,1543935628,"NeurIPS (prev. NIPS) has just started and it is not possible for everybody to attend it. That is why I have written this papers selection:  
[https://blog.sicara.com/nips-neurips-papers-selection-28efd4d73189](https://blog.sicara.com/nips-neurips-papers-selection-28efd4d73189)  
Do you like the format? Is it useful? I would really like some feedback on this.

By the way, what are the biggest breakthroughs for you this year?",29,1,False,self,,,,,
169,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,0,a31tk3,arxiv.org,[R] QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a31tk3/r_qmix_monotonic_value_function_factorisation_for/,ewanlee,1543938470,,0,1,False,default,,,,,
170,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,1,a31xmb,sisu.ai,What to do with Big Data? Making ML useful is a platform problem,https://www.reddit.com/r/MachineLearning/comments/a31xmb/what_to_do_with_big_data_making_ml_useful_is_a/,alexeyr,1543939238,,0,1,False,default,,,,,
171,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,1,a31xwi,sisu.ai,[Discussion] What to do with Big Data? Making ML useful is a platform problem,https://www.reddit.com/r/MachineLearning/comments/a31xwi/discussion_what_to_do_with_big_data_making_ml/,alexeyr,1543939282,,0,1,False,default,,,,,
172,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,1,a320tc,thenextweb.com,Intel's AI strategy for 2019 goes beyond chips,https://www.reddit.com/r/MachineLearning/comments/a320tc/intels_ai_strategy_for_2019_goes_beyond_chips/,Fatherthinger,1543939776,,0,1,False,default,,,,,
173,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,1,a326yv,self.MachineLearning,[P] Lambdo - feature engineering and machine learning: together at last!,https://www.reddit.com/r/MachineLearning/comments/a326yv/p_lambdo_feature_engineering_and_machine_learning/,asavinov,1543940862,"**Code/description/examples: https://github.com/asavinov/lambdo** 

Lambdo [0] is an open source workflow engine written in Python which simplifies machine learning by combining in one analysis pipeline: 

* *Feature engineering and machine learning:* Lambdo does not distinguish them and treats them as data transformations

* *Model training and prediction:* both feature definitions and ML models can be trained as part of one workflow 

* *Table population and column evaluation:* workflow consists of nodes of these two types. This makes it similar to Bistro [1] 

Lambdo is intended for the following use cases: 

* Numerous derived features with parameters derived from the data

* Regular re-training is required by using the same features as those to be used during prediction

* Time series analysis because it is where the quality of derived features is especially important

* Customization via user-defined (Python) functions 

[0] https://github.com/asavinov/lambdo - Lambdo 

[1] https://github.com/asavinov/bistro - Bistro 
",0,1,False,self,,,,,
174,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,1,a327hq,self.MachineLearning,[D] How to ELI5 a colleague about model calibration,https://www.reddit.com/r/MachineLearning/comments/a327hq/d_how_to_eli5_a_colleague_about_model_calibration/,marksteve4,1543940952,I am trying to ELI5 a group of audiences about model calibration. what it is and what are the basic approaches. Any good material or suggestion for this?,3,1,False,self,,,,,
175,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,1,a32cgr,youtube.com,[D] Extending Pandas with Custom Types - Will Ayd,https://www.reddit.com/r/MachineLearning/comments/a32cgr/d_extending_pandas_with_custom_types_will_ayd/,_quanttrader_,1543941826,,0,1,False,default,,,,,
176,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,1,a32cjk,medium.com,AWS isnt just the Future of the Cloud  UtopiaPress  Medium,https://www.reddit.com/r/MachineLearning/comments/a32cjk/aws_isnt_just_the_future_of_the_cloud_utopiapress/,BackgroundResult,1543941842,,0,1,False,default,,,,,
177,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32i4s,visionary.life,Decentralized Computing is what machine learning needs. The Most Promising Projects,https://www.reddit.com/r/MachineLearning/comments/a32i4s/decentralized_computing_is_what_machine_learning/,LukahnLSD,1543942818,,0,1,False,https://b.thumbs.redditmedia.com/ux9ul3FU5QfGoVimQqEiURp_UYEPJ0JBK2wtPo1hGMQ.jpg,,,,,
178,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32ji0,self.MachineLearning,[Uni Assignment] Automatic Navigation of miniature vehicle Through Image Recognition,https://www.reddit.com/r/MachineLearning/comments/a32ji0/uni_assignment_automatic_navigation_of_miniature/,CancerRaccoon,1543943027,[removed],0,1,False,self,,,,,
179,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32k40,self.MachineLearning,Multidimensional scaling for paired comparisons,https://www.reddit.com/r/MachineLearning/comments/a32k40/multidimensional_scaling_for_paired_comparisons/,mlagunas,1543943126,[removed],0,1,False,self,,,,,
180,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32kf8,pipeline.ai,Does anyone tried pipeline.ai? What's your opinion?,https://www.reddit.com/r/MachineLearning/comments/a32kf8/does_anyone_tried_pipelineai_whats_your_opinion/,pp314159,1543943177,,0,1,False,default,,,,,
181,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32lrg,self.MachineLearning,How do I assign output nodes to the correct labels?,https://www.reddit.com/r/MachineLearning/comments/a32lrg/how_do_i_assign_output_nodes_to_the_correct_labels/,Hathadar,1543943413,[removed],0,1,False,self,,,,,
182,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32mj0,machinelearningplus.com,[D] Topic modeling visualization  How to present the results of LDA models?,https://www.reddit.com/r/MachineLearning/comments/a32mj0/d_topic_modeling_visualization_how_to_present_the/,selva86,1543943546,,0,1,False,default,,,,,
183,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32ne1,kdnuggets.com,[D] Beyond One-Hot: an exploration of categorical variables,https://www.reddit.com/r/MachineLearning/comments/a32ne1/d_beyond_onehot_an_exploration_of_categorical/,_quanttrader_,1543943690,,0,1,False,default,,,,,
184,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32og3,roamanalytics.com,[D] Are categorical variables getting lost in your random forests?,https://www.reddit.com/r/MachineLearning/comments/a32og3/d_are_categorical_variables_getting_lost_in_your/,_quanttrader_,1543943869,,0,1,False,default,,,,,
185,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32qjc,self.MachineLearning,Finetuning Embedding for out of vocabulary words,https://www.reddit.com/r/MachineLearning/comments/a32qjc/finetuning_embedding_for_out_of_vocabulary_words/,omaramin,1543944237,[removed],0,1,False,self,,,,,
186,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32qjz,github.com,[D] manifoldai/merf: Mixed Effects Random Forest,https://www.reddit.com/r/MachineLearning/comments/a32qjz/d_manifoldaimerf_mixed_effects_random_forest/,_quanttrader_,1543944240,,0,1,False,default,,,,,
187,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a32r0z,developer.amazon.com,Deep-learning method makes Alexa a better conversationalist,https://www.reddit.com/r/MachineLearning/comments/a32r0z/deeplearning_method_makes_alexa_a_better/,rweckel,1543944317,,0,1,False,default,,,,,
188,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,2,a332do,self.MachineLearning,Giving away demo of real time neural network calibration (NN passing/learning over data) + genetic algos + other tools,https://www.reddit.com/r/MachineLearning/comments/a332do/giving_away_demo_of_real_time_neural_network/,futurismos,1543946238,[removed],1,1,False,self,,,,,
189,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,3,a335xr,self.MachineLearning,Giving away demo of real time neural network calibration (NN passing/learning over data) + genetic algos + other tools,https://www.reddit.com/r/MachineLearning/comments/a335xr/giving_away_demo_of_real_time_neural_network/,Nick-Andres,1543946835,[removed],0,1,False,self,,,,,
190,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,3,a33aec,self.MachineLearning,[D] Giving away demo of real time neural network calibration (NN passing/learning over data) + genetic algos + other tools,https://www.reddit.com/r/MachineLearning/comments/a33aec/d_giving_away_demo_of_real_time_neural_network/,futurismos,1543947779," 

## pm me to give you 100% free demo, example of time series machine learning tools and more:

&amp;#x200B;

[https://www.youtube.com/watch?v=7WpOVMg6G4U&amp;t=2s](https://www.youtube.com/watch?v=7WpOVMg6G4U&amp;t=2s)",0,1,False,self,,,,,
191,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,3,a33dqg,self.MachineLearning,What is the state-of-the-art multimodal technique for sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/a33dqg/what_is_the_stateoftheart_multimodal_technique/,ZER_0_NE,1543948334,[removed],0,1,False,self,,,,,
192,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,3,a33drb,self.MachineLearning,Does anyone here with a PhD in a non-ML field work in ML?,https://www.reddit.com/r/MachineLearning/comments/a33drb/does_anyone_here_with_a_phd_in_a_nonml_field_work/,orioncygnus1,1543948337,[removed],0,1,False,self,,,,,
193,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,3,a33h2w,self.MachineLearning,Help with a school project :),https://www.reddit.com/r/MachineLearning/comments/a33h2w/help_with_a_school_project/,sleepy3005,1543948932,[removed],0,1,False,self,,,,,
194,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,3,a33j3n,arxiv.org,Rethinking ImageNet Pre-training,https://www.reddit.com/r/MachineLearning/comments/a33j3n/rethinking_imagenet_pretraining/,isameer,1543949267,,1,1,False,default,,,,,
195,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,3,a33lkl,medium.com,AI Brush: New GAN Tool Paints Worlds,https://www.reddit.com/r/MachineLearning/comments/a33lkl/ai_brush_new_gan_tool_paints_worlds/,Yuqing7,1543949691,,0,1,False,https://b.thumbs.redditmedia.com/8tA1-JbnHK0GKg09wavnMohJHFy6OHYQdyVPwQiRVOk.jpg,,,,,
196,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a33tq5,self.MachineLearning,Quality inspection using machine learning in a manufacturing line,https://www.reddit.com/r/MachineLearning/comments/a33tq5/quality_inspection_using_machine_learning_in_a/,ro_mittal,1543951068,[removed],0,1,False,self,,,,,
197,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a33u2s,ibm.com,"""AI Fairness 360, A Step Towards Trusted AI"" from IBM Research",https://www.reddit.com/r/MachineLearning/comments/a33u2s/ai_fairness_360_a_step_towards_trusted_ai_from/,jsalsman,1543951128,,0,1,False,default,,,,,
198,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a33x3t,self.MachineLearning,[D] Keras Embedding and input_dim,https://www.reddit.com/r/MachineLearning/comments/a33x3t/d_keras_embedding_and_input_dim/,hadaev,1543951657,"Docs says

&gt;input\_dim: int &gt; 0. Size of the vocabulary, i.e. maximum integer index + 1.

But i also want masking, and it says

&gt;**mask\_zero**: Whether or not the input value 0 is a special ""padding""     value that should be masked out.     This is useful when using [recurrent layers](https://keras.io/layers/recurrent/)which may take variable length input.  If this is True then all subsequent layers     in the model need to support masking or an exception will be raised.     If mask\_zero is set to True, as a consequence, index 0 cannot be     used in the vocabulary (input\_dim should equal size of     vocabulary + 1). 

So, do i need maximum integer index + 1 or +2?",4,1,False,self,,,,,
199,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a33yo7,self.MachineLearning,"[N] ""AI Fairness 360, A Step Towards Trusted AI"" from IBM Research on eliminating bias from training data",https://www.reddit.com/r/MachineLearning/comments/a33yo7/n_ai_fairness_360_a_step_towards_trusted_ai_from/,jsalsman,1543951910,"I recently attended a panel at the Reimagine Education 2018 conference where the IBM representative mentioned this recent work about eliminating bias from training data: https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/

It looks good to me, but I would like to learn others' opinions on it, e.g. how it stacks up to [""An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics"" by Lpez *et al* (2013.)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.413.1919&amp;rep=rep1&amp;type=pdf)",5,1,False,self,,,,,
200,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a33z4n,self.MachineLearning,[R] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses,https://www.reddit.com/r/MachineLearning/comments/a33z4n/r_decoupling_direction_and_norm_for_efficient/,entarko,1543951993,[removed],0,1,False,self,,,,,
201,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a341to,self.MachineLearning,Best Online MOOC to learn Pytorch?,https://www.reddit.com/r/MachineLearning/comments/a341to/best_online_mooc_to_learn_pytorch/,edidamanish,1543952441,[removed],0,1,False,self,,,,,
202,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a342wj,arxiv.org,[R] Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses,https://www.reddit.com/r/MachineLearning/comments/a342wj/r_decoupling_direction_and_norm_for_efficient/,entarko,1543952634,,9,1,False,default,,,,,
203,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,4,a343vu,self.MachineLearning,Which of these math courses will be better for a ML career?,https://www.reddit.com/r/MachineLearning/comments/a343vu/which_of_these_math_courses_will_be_better_for_a/,2020Tokyo,1543952789,[removed],0,1,False,self,,,,,
204,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,5,a34a26,self.MachineLearning,Training Neural Networks with a small amount of data,https://www.reddit.com/r/MachineLearning/comments/a34a26/training_neural_networks_with_a_small_amount_of/,TzeenchRulesOverAll,1543953857,[removed],0,1,False,self,,,,,
205,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,5,a34bdu,self.MachineLearning,[D] Program To Dynamically Hypothesizing Rules And Track Data. What's It Called?,https://www.reddit.com/r/MachineLearning/comments/a34bdu/d_program_to_dynamically_hypothesizing_rules_and/,meteoraln,1543954086,"I'm a programmer without a background in machine learning. I'm trying to do some research but I don't know the proper terms.

&amp;#x200B;

I'm imagining a dataset of z = f(x, y), and the purpose of the program is to guess what the relationship is. So it hypothesizes some rules in possibly a random way. Like, z = x + y? z = 2x? z = x / y?

&amp;#x200B;

Then, it would apply the rules to the dataset and see how well the rules do, measured by the size of the errors. Rules with large errors would be thrown out, and rules with smaller errors would be ""improved"" upon, by making small modifications.

&amp;#x200B;

It would be like a genetic algorithm whose output is a set of rules rather than a datapoint. Anyone know what this might be called or know of any papers written on it?",12,1,False,self,,,,,
206,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,5,a34ivm,self.MachineLearning,[R] Part-time PhD in Data Science,https://www.reddit.com/r/MachineLearning/comments/a34ivm/r_parttime_phd_in_data_science/,__Julia,1543955355,"I'm working as a research engineer and I would like to do a part-time PhD in order to move on in my career. Based on the nature of my work, I co-authored papers with my colleagues, and due to my exposure to research field, I have an understanding on how to perform research which is part of most PhD program. Quitting my job (enjoy doing my work), and joining a PhD program is not an option for me. I am based in EU, and I would like to do a PhD in Data Science (remotely or Part-time). In case you have experience doing it, please share some insights, or contacts of remote-friendly groups. ",10,1,False,self,,,,,
207,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,5,a34ner,self.MachineLearning,[P] Can you tell if these faces are real or GAN-generated?,https://www.reddit.com/r/MachineLearning/comments/a34ner/p_can_you_tell_if_these_faces_are_real_or/,aveni0,1543956141,"http://nikola.mit.edu

Hi! We are a pair of students at MIT trying to measure how well humans can differentiate between real and (current state-of-the-art) GAN-generated faces, for a class project. We're concerned with GAN-generated images' potential for fake news and ads, and we believe it would be good to measure empirically how often people get fooled by these pictures under different image exposure times. 

[The quiz takes 5-10 minutes,](http://nikola.mit.edu) and we could really use the data! We'll post overall results at the end of the week.",162,1,False,self,,,,,
208,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,5,a34owc,self.MachineLearning,Machine Learning for Mental Health Assessment.,https://www.reddit.com/r/MachineLearning/comments/a34owc/machine_learning_for_mental_health_assessment/,embelishsomething,1543956393,"I've had this idea of having a voice recorder that can automatically sort statements made by a client/patient/person during conversation into a mental-measurement like the SIB-R, MMPI, Woodcock-Johnson V, etc. 

My google fu is subpar. I've been trying to teach myself how to use Rstudio (data frames and plots is all I can do atm). Hopefully someone has an idea of a direction, or some software I don't know about?",0,1,False,self,,,,,
209,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,5,a34sct,self.MachineLearning,[P] A general-purpose framework for machine learning applied to predicting customer churn,https://www.reddit.com/r/MachineLearning/comments/a34sct/p_a_generalpurpose_framework_for_machine_learning/,Kmax12,1543956997,"A four-part series on building a customer churn model . This project used only open source tools including: [Pandas](http://pandas.pydata.org), [Featuretools](https://featuretools.com), and [scikit-learn](http://scikit-learn.org). 

&amp;#x200B;

Here are the parts:

1. [Overview: A General-Purpose Framework for Machine Learning](https://blog.featurelabs.com/how-to-create-value-with-machine-learning/)
2. [Prediction Engineering: How to Set Up Your Machine Learning Problem](https://blog.featurelabs.com/prediction-engineering-churn/)
3. [Feature Engineering: What Powers Machine Learning](https://towardsdatascience.com/feature-engineering-what-powers-machine-learning-93ab191bcc2d)
4. [Modeling: Teaching an Algorithm to Make Predictions](https://blog.featurelabs.com/modeling-churn/)",2,1,False,self,,,,,
210,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,6,a34tyl,self.MachineLearning,Trying to find my first job in machine learning.Need help.,https://www.reddit.com/r/MachineLearning/comments/a34tyl/trying_to_find_my_first_job_in_machine/,gooddev25,1543957269,"I'm 31.I have a BA in IT and MA in psychology. I was always unsure of what i want to do.then i saw an application for coding bootcamp in mobile dev(Android/ios) applied for it and got accepted.  
Coding was fun and i liked it.

But after graduating from this 3 month bootcamp , I realized i don't really like to work on mobile.

This change of mind happened when in the last month,one of our teachers shared some really cool videos on machine learning.

I was hooked. I finally knew what i wanted to do.

I really want to find a job/internship in AI/ML but i don't know how to do that?  
right now i'm learning python in udacity. what other things should i learn?  
what is your suggestion. how can i increase my chance of getting a job in AI/ML!  
is there any volunteering opportunities in Toronto regarding AI/ML ?

&amp;#x200B;",0,1,False,self,,,,,
211,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,6,a35cny,self.MachineLearning,Total Noob - Binary Classification Data Sets: Good Samples,https://www.reddit.com/r/MachineLearning/comments/a35cny/total_noob_binary_classification_data_sets_good/,Rkneeacl,1543960436,[removed],0,1,False,self,,,,,
212,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,7,a35lec,reddit.com,20 YouTube channels for AI &amp; data science,https://www.reddit.com/r/MachineLearning/comments/a35lec/20_youtube_channels_for_ai_data_science/,skj8,1543961925,,0,1,False,default,,,,,
213,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,7,a35p1n,medium.com,Insilico Medicine Announces MOSES Benchmark Platform for Molecular Generation,https://www.reddit.com/r/MachineLearning/comments/a35p1n/insilico_medicine_announces_moses_benchmark/,Yuqing7,1543962585,,0,1,False,default,,,,,
214,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,7,a35sqn,youtu.be,"[P] Creating NN agents that can find ""apples"" in two minutes",https://www.reddit.com/r/MachineLearning/comments/a35sqn/p_creating_nn_agents_that_can_find_apples_in_two/,FredrikNoren,1543963225,,1,1,False,default,,,,,
215,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,8,a36eib,self.MachineLearning,Explained: GPipe  Training Giant Neural Nets using Pipeline Parallelism,https://www.reddit.com/r/MachineLearning/comments/a36eib/explained_gpipe_training_giant_neural_nets_using/,tldrtldreverything,1543967159,"Hey everyone,
Google recently released GPipe, an interesting paper which presents a technique to efficiently use very large (&gt;500M neurons) neural networks by paralleling them across many GPUs. I wrote a short summary, also based on conversations with one of the paper writers. I hope you like it! https://lyrn.ai/2018/11/30/gpipe-training-giant-neural-nets-using-pipeline-parallelism/",0,1,False,self,,,,,
216,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,9,a36lr8,i.redd.it,DUE DILIGENCES V,https://www.reddit.com/r/MachineLearning/comments/a36lr8/due_diligences_v/,JamurGerloff,1543968547,,0,1,False,https://b.thumbs.redditmedia.com/Ob5lcLqry-Lyp3hcJ-532WJNRwf73417QVuDBOi1OSc.jpg,,,,,
217,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,9,a36p1i,self.MachineLearning,[P] This web app makes n-gram models of tweets from a twitter account and makes new tweets sounding like them,https://www.reddit.com/r/MachineLearning/comments/a36p1i/p_this_web_app_makes_ngram_models_of_tweets_from/,ParrrotBot,1543969131,"Link: [https://www.pleaselet.me/tweetas](https://www.pleaselet.me/tweetas)

&amp;#x200B;

[An example tweet as @elonmusk](https://i.redd.it/032xyz01rc221.png)

&amp;#x200B;

The app can also make word cloud of words most frequently used by atwitter account (i.e. uni-grams):

&amp;#x200B;

*Processing img 411qcg2upc221...*

&amp;#x200B;

And can also show the table of all n-gram probabilities in the model.",0,1,False,self,,,,,
218,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,9,a36zd8,self.MachineLearning,[R] Explained: GPipe  Training Giant Neural Nets using Pipeline Parallelism,https://www.reddit.com/r/MachineLearning/comments/a36zd8/r_explained_gpipe_training_giant_neural_nets/,tldrtldreverything,1543971095,"Hey everyone, Google recently released GPipe, an interesting paper which presents a technique to efficiently use very large (&gt;500M neurons) neural networks by paralleling them across many GPUs. I wrote a short summary, also based on conversations with one of the paper writers. I hope you like it! https://lyrn.ai/2018/11/30/gpipe-training-giant-neural-nets-using-pipeline-parallelism/",0,1,False,self,,,,,
219,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,10,a37h08,self.MachineLearning,"[D] Advice for engineer working on his Masters, trying to break in ML field",https://www.reddit.com/r/MachineLearning/comments/a37h08/d_advice_for_engineer_working_on_his_masters/,eemamedo,1543974564,"I would really appreciate any advice.

A little bit of background: I am currently a Master of Science, working on my thesis at one of the best universities in Canada. The program I am in is power and energy branch of electrical engineering, which as you might understand is very little to do with ML. However, after taking a ML class in summer 18, I realized that this is something I can see myself doing for quite some time. As the result, I came up with the topic for my thesis that would combine ML and DL fields along with the power branch of EE: predictive maintenance. I was able to get a grant that allowed me to get raw dataset that I would need to analyze. I have been working quite hard to get my hands dirty but will really appreciate if someone can give me some more suggestions on what I can work on during the remaining time of my program (9 months) to make myself a stronger candidate.

What have I done so far: Kaggle project (the competition got closed a week later but I still learned a lot), statistics (shaum's outline and occasionally, look at Elements of statistical learning for more insight), linear algebra (covered in undergraduate), theoretical foundations of ML (what is regression, what is decision tree, what is Lasso), python with packages

What I plan to do: working with dirty dataset (clean it, etc... Required for my thesis); 2 more kaggle projects (one more for practice and one more to actually compete).",13,1,False,self,,,,,
220,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,11,a37mr7,self.MachineLearning,[D] How do you handle large fan in / fan out ratios for the inputs to Dense layers at the end of CNNs,https://www.reddit.com/r/MachineLearning/comments/a37mr7/d_how_do_you_handle_large_fan_in_fan_out_ratios/,idg101,1543975696,"I often have to use binary classifiers with images in a convolution neural network form.  As the layers increase in my model, so do the number of filters.  After all the convolutional filters, i flatten and then pass the output to Dense layer (fully connected) which outputs a 0 or 1 indicating the class. Its not uncommon for me to have a fan in fan out ratio to that Dense layer of 100:1.  I am looking for papers that discuss different approaches to address this issue.  Here is what I have seen so far:

&amp;#x200B;

1. Use dropout between the flatten layer and the dense. Cons are you have to tune the dropout.

2. Instead of a flatten layer, use something like global average pooling or global max pooling.  Are there any good papers which analyze the differences in these two?  Cons are if you last layer has 64 filters, you still have a 64:1 fan in fan out ratio and likely will need something like dropout (see item 1).

3. Use a stepped down approach composed of several fully connected layers.  I have never been able to get this to work well.  It seems to make me overfit even quicker with multiple layers than with a single.  ",12,1,False,self,,,,,
221,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,11,a37u45,youtube.com,[D] Attacking Clustered Data with a Mixed Effects Random Forests Model in Python - Sourav Dey,https://www.reddit.com/r/MachineLearning/comments/a37u45/d_attacking_clustered_data_with_a_mixed_effects/,_quanttrader_,1543977154,,0,1,False,https://b.thumbs.redditmedia.com/Lc1WOiJ9RYCNpeAsHGozSYkqrOyB8xOKRAwusGZL6Ig.jpg,,,,,
222,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,13,a38ohx,self.MachineLearning,[D] Hobbyist build for ML fun,https://www.reddit.com/r/MachineLearning/comments/a38ohx/d_hobbyist_build_for_ml_fun/,moar_qwibqwib,1543983292,"Hello /r/MachineLearning,

&amp;#x200B;

I'm an ex-machine learner trying to get back into it.  In Grad school I focused mostly on smaller NNs and self-organizing networks.  Being limited by my build, I couldn't do much more.  Now that I have a steady income, I want to do some hobby work.  My goal is to get into kaggling and a few side projects.  No plans to do big image sets.  This is my first build, and I'm worried I'm over thinking and over building.

&amp;#x200B;

 [PCPartPicker part list](https://pcpartpicker.com/list/wZdxjy) / [Price breakdown by merchant](https://pcpartpicker.com/list/wZdxjy/by_merchant/)

|Type|Item|Price|
|:-|:-|:-|
|**CPU**|[Intel - Core i7-8700K 3.7 GHz 6-Core Processor](https://pcpartpicker.com/product/sxDzK8/intel-core-i7-8700k-37ghz-6-core-processor-bx80684i78700k)|$369.89 @ OutletPC|
|**CPU Cooler**|[be quiet! - Dark Rock Pro 4 50.5 CFM CPU Cooler](https://pcpartpicker.com/product/F3gzK8/be-quiet-dark-rock-pro-4-505-cfm-cpu-cooler-bk022)|$84.99 @ SuperBiiz|
|**Motherboard**|[Asus - PRIME Z370-A II ATX LGA1151 Motherboard](https://pcpartpicker.com/product/H43H99/asus-prime-z370-a-ii-atx-lga1151-motherboard-prime-z370-a-ii)|$170.98 @ SuperBiiz|
|**Memory**|[G.Skill - Ripjaws V Series 32 GB (2 x 16 GB) DDR4-3200 Memory](https://pcpartpicker.com/product/kXbkcf/gskill-memory-f43200c16d32gvk)|$249.89 @ OutletPC|
|**Storage**|[Samsung - 860 Evo 1 TB M.2-2280 Solid State Drive](https://pcpartpicker.com/product/wd97YJ/samsung-860-evo-1tb-m2-2280-solid-state-drive-mz-n6e1t0bw)|$147.99 @ Amazon|
|**Video Card**|[EVGA - GeForce GTX 1080 Ti 11 GB FTW3 GAMING iCX Video Card](https://pcpartpicker.com/product/KBtWGX/evga-geforce-gtx-1080-ti-11gb-ftw-gaming-icx-video-card-11g-p4-6696-kr)|$804.98 @ Newegg Business|
|**Case**|[Cooler Master - MasterCase MC500M ATX Mid Tower Case](https://pcpartpicker.com/product/tnGxFT/cooler-master-mastercase-mc500m-atx-mid-tower-case-mcm-m500m-kg5n-s00)|$158.98 @ Newegg|
|**Power Supply**|[Corsair - 750 W 80+ Platinum Certified Fully-Modular ATX Power Supply](https://pcpartpicker.com/product/BJFPxr/corsair-power-supply-cp9020072)|$109.89 @ OutletPC|
|**Optical Drive**|[LG - WH14NS40 Blu-Ray/DVD/CD Writer](https://pcpartpicker.com/product/z2dqqs/lg-optical-drive-wh14ns40)|$59.89 @ OutletPC|
|*Prices include shipping, taxes, rebates, and discounts*|||
|Total (before mail-in rebates)|$2197.48||
|Mail-in rebates|\-$40.00||
|**Total**|**$2157.48**||
|Generated by [PCPartPicker](https://pcpartpicker.com) 2018-12-04 23:06 EST-0500|||

&amp;#x200B;",6,1,False,self,,,,,
223,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,13,a38qtf,self.MachineLearning,Learning Uncertainty from Training Data,https://www.reddit.com/r/MachineLearning/comments/a38qtf/learning_uncertainty_from_training_data/,sherlock_1695,1543983797,I am trying to create a network which can learn the uncertainty in training data. I am creating an ideal data and then corrupting it with noise. I wanna know if there is any other approach other than Bayesian for doing such data? ,0,1,False,self,,,,,
224,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,13,a38shg,self.MachineLearning,Python or R?,https://www.reddit.com/r/MachineLearning/comments/a38shg/python_or_r/,engineheat,1543984170,"I kind of know both, but I want to master one. I'm thinking Python since it's more general than R and has many packages available. I mean, Python with Panda, Numpy, ScikitLearn and so on can do anything R can do right?

So why R?

Thanks",0,1,False,self,,,,,
225,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,13,a38tc0,self.MachineLearning,Compiled summaries of NIPS'18 papers,https://www.reddit.com/r/MachineLearning/comments/a38tc0/compiled_summaries_of_nips18_papers/,moon5moon5moon,1543984349,[removed],0,1,False,self,,,,,
226,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,13,a38v3h,self.MachineLearning,Averaging the predictions of different Object detection models in an ensemble,https://www.reddit.com/r/MachineLearning/comments/a38v3h/averaging_the_predictions_of_different_object/,deeplearning2018,1543984749,"In this paper (https://arxiv.org/abs/1809.03193?fbclid=IwAR1p0MHK_GIbdYCJJ05oKcxGWTkRAVM47VLCIfcWMsH2tOKigdWoKDQfa20) the authors propose a technique to get better inference results by using ensembles, I know examples of ensembles in the classification task but have anyone seen and preferably point to a code example of doing inference of object detection using ensemble voting?",0,1,False,self,,,,,
227,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,13,a38vrn,self.MachineLearning,RasPi3 for motion detection and speed-calculation?,https://www.reddit.com/r/MachineLearning/comments/a38vrn/raspi3_for_motion_detection_and_speedcalculation/,Phischstaebchen,1543984898,[removed],0,1,False,self,,,,,
228,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,14,a39395,self.MachineLearning,[D] NeuRIPS to be hosted in Canada for 3 consecutive years,https://www.reddit.com/r/MachineLearning/comments/a39395/d_neurips_to_be_hosted_in_canada_for_3/,milaworld,1543986446,"In the NeurIPS 2018 handout, there is a photo that mentioned the conference will be held in Vancouver for both 2019 and 2020. Since the conference is held in Montral this year, that means NeurIPS will be help held for three years straight in the same country.

Not that I have any problems with Canada, but wouldn't it be a good idea to diversity the distribution of countries that hosts a conference like NeurIPS? Many PhD students outside of North America will find it difficult to secure a visa for most of their PhD to attend the most important conference.

Are ML conferences supposed to be more like the Olympics, or the Superbowl?
",93,1,False,self,,,,,
229,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,14,a398g0,themartec.com,Robotic Process Automation: The Future of Business Transformation,https://www.reddit.com/r/MachineLearning/comments/a398g0/robotic_process_automation_the_future_of_business/,OpportunityXCost,1543987582,,0,1,False,https://b.thumbs.redditmedia.com/oKlYu1guONRlgmEZtRogTNXee7_rxIUq7rZABhevXKw.jpg,,,,,
230,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,14,a39glv,self.MachineLearning,Asking professors for research positions,https://www.reddit.com/r/MachineLearning/comments/a39glv/asking_professors_for_research_positions/,stormyjan2601,1543989458,[removed],1,1,False,self,,,,,
231,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,15,a39oeg,facebook.com,[R] NeurIPS talk on non-neural computation in biological organisms,https://www.reddit.com/r/MachineLearning/comments/a39oeg/r_neurips_talk_on_nonneural_computation_in/,amatuni,1543991272,,0,1,False,default,,,,,
232,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,15,a39oep,self.MachineLearning,[P] TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks,https://www.reddit.com/r/MachineLearning/comments/a39oep/p_tensorflow_estimators_managing_simplicity_vs/,BatmantoshReturns,1543991273,"https://arxiv.org/abs/1708.02637

This a paper Google put out about their Estimator API. I read it because there is some weird stuff going on with the estimators give errors when certain tensors or operations are in the `model_fn`. For example, `sampled_softmax_loss` does not seem to work when using Estimator. Some details here

https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&amp;utm_source=footer#!topic/discuss/Qx9JcHK6qbA

https://stackoverflow.com/questions/53626302/tensorflow-estimator-api-causes-crash-at-sampled-softmax-loss-and-nce-loss-f

https://github.com/tensorflow/tensorflow/issues/4026

https://github.com/tensorflow/tensorflow/issues/8042

Anyways, the paper didn't give any insight to my issue, but was an interesting read into software eng practices regarding TF. Some selected highlights. 

&gt; e user congures an Estimator by passing a callback, the
&gt; model fn, to the constructor. When one of its methods is called,
&gt; Estimator creates a TensorFlow graph, sets up the input pipeline
&gt; specied by the user in the arguments to the method (see Sec. 3.2),
&gt; and then calls the model fn with appropriate arguments to generate
&gt; the graph representing the model. e Estimator class itself
&gt; contains the necessary code to run a training or evaluation loop, to
&gt; predict using a trained model, or to export a prediction model for
&gt; use in production



&gt; Estimator hides some TensorFlow concepts, such as Graph and
&gt; Session, from the user. e Estimator constructor also receives a
&gt; conguration object called RunConfig which communicates everything
&gt; that this Estimator needs to know about the environment in which the model will be run: how many workers are available,
how oen to save intermediate checkpoints, etc.


&gt; To ensure encapsulation, Estimator creates a new graph, and
&gt; possibly restores from checkpoint, every time a method is called.
&gt; Rebuilding the graph is expensive, and it could be cached to make
&gt; it more economical to run, say, evaluate or predict in a loop.
&gt; However, we found it very useful to explicitly recreate the graph,
&gt; trading o performance for clarity. Even if we did not rebuild
&gt; the graph, writing such loops is highly suboptimal in terms of
&gt; performance. Making this cost very visible discourages users from
&gt; accidentally writing badly performing code



&gt; Specifying inputs with input fn. e methods train, evaluate,
&gt; and predict all take an input function, which is expected to produce
&gt; two dictionaries: one containing Tensors with inputs (features),
&gt; and one containing Tensors with labels. Whenever a method
&gt; of Estimator is called, a new graph is created, the input fn passed
&gt; as an argument to the method call is called to produce the input
&gt; pipeline of the Estimator, and then the model fn is called with
&gt; the appropriate mode argument to build the actual model graph.



&gt; Specifying the model with model fn. We chose to congure
&gt; Estimator with a single callback, the model fn, which returns ops
&gt; for training, evaluation, or prediction, depending on which graph
&gt; is being requested (which method of Estimator is being called).
&gt; For example, if the train method is called, model fn will be called
&gt; with an argument mode=TRAIN, which the user can then use to
&gt; build a custom graph in the knowledge that it is going to be used
&gt; for training



",0,1,False,self,,,,,
233,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,15,a39pef,self.MachineLearning,[R] TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks,https://www.reddit.com/r/MachineLearning/comments/a39pef/r_tensorflow_estimators_managing_simplicity_vs/,BatmantoshReturns,1543991523,"https://arxiv.org/abs/1708.02637

This a paper Google put out about their Estimator API. I read it because there is some weird stuff going on with the estimators give errors when certain tensors or operations are in the `model_fn`. For example, `sampled_softmax_loss` does not seem to work when using Estimator. Some details here

https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&amp;utm_source=footer#!topic/discuss/Qx9JcHK6qbA

https://stackoverflow.com/questions/53626302/tensorflow-estimator-api-causes-crash-at-sampled-softmax-loss-and-nce-loss-f

https://github.com/tensorflow/tensorflow/issues/4026

https://github.com/tensorflow/tensorflow/issues/8042

Anyways, the paper didn't give any insight to my issue, but was an interesting read into software eng practices regarding TF. Some selected highlights. 

&gt; e user congures an Estimator by passing a callback, the
&gt; model fn, to the constructor. When one of its methods is called,
&gt; Estimator creates a TensorFlow graph, sets up the input pipeline
&gt; specied by the user in the arguments to the method (see Sec. 3.2),
&gt; and then calls the model fn with appropriate arguments to generate
&gt; the graph representing the model. e Estimator class itself
&gt; contains the necessary code to run a training or evaluation loop, to
&gt; predict using a trained model, or to export a prediction model for
&gt; use in production

.

&gt; Estimator hides some TensorFlow concepts, such as Graph and
&gt; Session, from the user. e Estimator constructor also receives a
&gt; conguration object called RunConfig which communicates everything
&gt; that this Estimator needs to know about the environment in which the model will be run: how many workers are available,
how oen to save intermediate checkpoints, etc.

.

&gt; To ensure encapsulation, Estimator creates a new graph, and
&gt; possibly restores from checkpoint, every time a method is called.
&gt; Rebuilding the graph is expensive, and it could be cached to make
&gt; it more economical to run, say, evaluate or predict in a loop.
&gt; However, we found it very useful to explicitly recreate the graph,
&gt; trading o performance for clarity. Even if we did not rebuild
&gt; the graph, writing such loops is highly suboptimal in terms of
&gt; performance. Making this cost very visible discourages users from
&gt; accidentally writing badly performing code

.

&gt; Specifying inputs with input fn. e methods train, evaluate,
&gt; and predict all take an input function, which is expected to produce
&gt; two dictionaries: one containing Tensors with inputs (features),
&gt; and one containing Tensors with labels. Whenever a method
&gt; of Estimator is called, a new graph is created, the input fn passed
&gt; as an argument to the method call is called to produce the input
&gt; pipeline of the Estimator, and then the model fn is called with
&gt; the appropriate mode argument to build the actual model graph.

.

&gt; Specifying the model with model fn. We chose to congure
&gt; Estimator with a single callback, the model fn, which returns ops
&gt; for training, evaluation, or prediction, depending on which graph
&gt; is being requested (which method of Estimator is being called).
&gt; For example, if the train method is called, model fn will be called
&gt; with an argument mode=TRAIN, which the user can then use to
&gt; build a custom graph in the knowledge that it is going to be used
&gt; for training



",11,1,False,self,,,,,
234,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,15,a39vil,self.MachineLearning,Multi label classification with large dataset,https://www.reddit.com/r/MachineLearning/comments/a39vil/multi_label_classification_with_large_dataset/,deepak2401,1543993016,[removed],0,1,False,self,,,,,
235,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,15,a39vnv,arxiv.org,[R] Gaussian AutoEncoder  latent variables directly approaching CDF of multivariate Gaussian for radii and distances,https://www.reddit.com/r/MachineLearning/comments/a39vnv/r_gaussian_autoencoder_latent_variables_directly/,jarekduda,1543993048,,1,1,False,default,,,,,
236,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,16,a3a1xy,arxiv.org,[R] ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,https://www.reddit.com/r/MachineLearning/comments/a3a1xy/r_proxylessnas_direct_neural_architecture_search/,HigherTopoi,1543994628,,7,1,False,default,,,,,
237,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,16,a3a35c,github.com,Compact prediction trees for fast sequence prediction,https://www.reddit.com/r/MachineLearning/comments/a3a35c/compact_prediction_trees_for_fast_sequence/,ashubham,1543994956,,0,1,False,default,,,,,
238,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,16,a3a3oq,self.MachineLearning,How to test the reliability of a Time Series Machine Learning model?,https://www.reddit.com/r/MachineLearning/comments/a3a3oq/how_to_test_the_reliability_of_a_time_series/,rodrigonader,1543995109,"Recently I came up with a Machine Learning model that would predict for some (only a few) stocks if they are going to reach a certain value in the next 30 minutes with 80% PRECISION (notice that I seek precision here, not recall, neither accuracy). The problem is that the model is really stupid: it's a simple Logistic Regression with ONLY 2 variables - price and volume. I tested the null hypothesis of all predictions being TRUE, and it gives me only 54% precision. I just reached this precision because I've tested the model with all possible stocks in the market, and a couple of them seem to fit the model well.

&amp;#x200B;

Model Information:

X: Price and Volume

Y: If in the next 30 minutes asset goes up by 0.1% (True or False)

&amp;#x200B;

Training data - about 100k datapoints (minutes)

Testing data - about 30k datapoints (minutes)

&amp;#x200B;

I checked every possible flaw, but it seem that the model is really predicting well with only those variables. If it works only for some few stocks (symbols) and seem not to be very intelligent at all, how can I know if I can use it or not? Could it perform that good just by luck? And is there any statistical test to make and verify its reliability?",0,1,False,self,,,,,
239,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,17,a3aaij,self.MachineLearning,What is local feature learning versus global feature learning in terms of AlignedReID paper context?,https://www.reddit.com/r/MachineLearning/comments/a3aaij/what_is_local_feature_learning_versus_global/,voqtuyen,1543996974,[removed],0,1,False,self,,,,,
240,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,17,a3ablb,self.MachineLearning,"[P] GANfield: Something, Something GAT Pun",https://www.reddit.com/r/MachineLearning/comments/a3ablb/p_ganfield_something_something_gat_pun/,vdalv,1543997268,"I recently tried to generate some images of Garfield.

Here's a [quick sample](https://i.imgur.com/p3CAHLA.jpg) of the results. You can find my writeup [here](https://vdalv.github.io/2018/12/04/ganfield.html).

Also, [teaser](https://i.imgur.com/15gk2xv.jpg).",45,1,False,self,,,,,
241,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,17,a3ah8g,self.MachineLearning,[D] How do ML packages like TensorFlow/PyTorch handle gradient descent?,https://www.reddit.com/r/MachineLearning/comments/a3ah8g/d_how_do_ml_packages_like_tensorflowpytorch/,Rainymood_XI,1543998956,"So, kinda newbie question here. But I imagine that if you can configure such a large amount of different models that gradient descent has to happen in some automatic way in which you don't have to explicitely calculate the derivative symbolically, is there a name for this or could someone point me to the code that does this? I'm highly intrigued because I would love to write my own backprop algo (which I've done already for a 2-layer MLP/ANN) but I'd like to make the backprop algo more general. ",16,1,False,self,,,,,
242,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,17,a3ai6z,self.MachineLearning,How to detect a stalled object in a video,https://www.reddit.com/r/MachineLearning/comments/a3ai6z/how_to_detect_a_stalled_object_in_a_video/,Ahmad401,1543999249,[removed],0,1,False,self,,,,,
243,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,17,a3algr,alibabacloud.com,Simplifying Marketing with AI: Introducing Alimama's New Ad Tech,https://www.reddit.com/r/MachineLearning/comments/a3algr/simplifying_marketing_with_ai_introducing/,Jen_Cl,1544000287,,0,1,False,default,,,,,
244,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,18,a3am5u,getrevue.co,Awesome Computer Science - Issue #13,https://www.reddit.com/r/MachineLearning/comments/a3am5u/awesome_computer_science_issue_13/,programming-innovate,1544000494,,0,1,False,https://b.thumbs.redditmedia.com/yuYWc139KAwssAhacuMuppBLTB0p02t8YdOffLFeO3M.jpg,,,,,
245,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,18,a3am7j,self.MachineLearning,"Recommendations for books on ""Statistics for Machine Learning"" and Linear Algebra.",https://www.reddit.com/r/MachineLearning/comments/a3am7j/recommendations_for_books_on_statistics_for/,JAGGI_JATT,1544000513,[removed],0,1,False,self,,,,,
246,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,18,a3ammb,self.MachineLearning,[D] Changing the density of the priorboxes for SSD object detection?,https://www.reddit.com/r/MachineLearning/comments/a3ammb/d_changing_the_density_of_the_priorboxes_for_ssd/,RebeccaFx,1544000643,"I had an idea, that we could change the density of the prior boxes based on the distribution of the centroids of the GT boxes for certain datasets.

For example we could have boxes at every grid cell at the center of the image and have fewer boxes on the top and bottom parts because we would like to detect objects on the horizon.

Is this something I should investigate or not? I am raising this question because I have not found anything about this topic, everyone uses the default ""settings"". Maybe they only change the priorbox stride.",1,1,False,self,,,,,
247,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,18,a3ao0t,gruaportico.cl,Gra Prtico Y Polipasto - AICRANE Gras,https://www.reddit.com/r/MachineLearning/comments/a3ao0t/gra_prtico_y_polipasto_aicrane_gras/,Aicranegruaportico,1544001083,,0,1,False,default,,,,,
248,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,18,a3awph,self.MachineLearning,How does Microsoft OneNote's formula recognition work?,https://www.reddit.com/r/MachineLearning/comments/a3awph/how_does_microsoft_onenotes_formula_recognition/,mcMdy,1544003706,[removed],0,1,False,self,,,,,
249,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,18,a3awu5,self.MachineLearning,"[D] If downsampling majority class due to imbalanced classes, do we standardize test data using a scaler that is fit on training data before or after downsampling?",https://www.reddit.com/r/MachineLearning/comments/a3awu5/d_if_downsampling_majority_class_due_to/,rossthebern,1544003739,"So I'm working on a project in which I need to heavily downsample the majority class in my training data. I can't seem to find a good answer for this anywhere on the internet. It seems like a scaler that is fit on heavily imbalanced training data would be way different than one fit on the training data after balancing the classes with downsampling. So, should test data be standardized using the mean and standard deviation of the downsampled training data, or of the training data before downsampling? ",14,1,False,self,,,,,
250,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,19,a3az73,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a3az73/global_machine_learning_market_size_outlook/,hemaei,1544004395,[removed],0,1,False,self,,,,,
251,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,19,a3b2pr,youtube.com,why we add Bias in neural network . [Hindi] eng sub,https://www.reddit.com/r/MachineLearning/comments/a3b2pr/why_we_add_bias_in_neural_network_hindi_eng_sub/,Neel_kamal_sahu,1544005355,,0,1,False,https://b.thumbs.redditmedia.com/esoyyagrDMwGgcLRQ7kkPagtgHpSFSHz5XzQPiyUPIY.jpg,,,,,
252,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,19,a3b6ue,theverge.com,[N] Nvidia has created the first real-time game demo using AI-generated graphics,https://www.reddit.com/r/MachineLearning/comments/a3b6ue/n_nvidia_has_created_the_first_realtime_game_demo/,hooba_stank_,1544006578,,0,1,False,https://b.thumbs.redditmedia.com/QKCiJiJ0I-O-wQRw41OXUWqgtYDw5URvIIQy84orlQw.jpg,,,,,
253,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,19,a3b89z,self.MachineLearning,[N] Nvidia has created the first real-time game demo using AI-generated graphics,https://www.reddit.com/r/MachineLearning/comments/a3b89z/n_nvidia_has_created_the_first_realtime_game_demo/,hooba_stank_,1544006991,"Nothing too novel under the covers, but it works in real time on single GPU.

https://www.theverge.com/2018/12/3/18121198/ai-generated-video-game-graphics-nvidia-driving-demo-neurips",30,1,False,self,,,,,
254,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,20,a3bbo2,youtu.be,Atanasoff - The Father of the Computer (2014) [720p] - Most people know who invented the telephone. The light bulb. The airplane. But what about the first computer? Atanasoff: Father of the Computer tells the story of the lone inventor who fought all his life to be recognized as the originator...,https://www.reddit.com/r/MachineLearning/comments/a3bbo2/atanasoff_the_father_of_the_computer_2014_720p/,cardust,1544007975,,0,1,False,https://b.thumbs.redditmedia.com/MyY8wa_qAt508P8iyAnKqFpm39VY2MSDcOHHIUJPy2k.jpg,,,,,
255,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,20,a3be4d,self.MachineLearning,"AIOps Platform Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a3be4d/aiops_platform_market_size_outlook_trends_and/,hemaei,1544008650,[removed],0,1,False,self,,,,,
256,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,20,a3bgyb,stats.stackexchange.com,Why doesn't deep learning work as well in regression as in classification?,https://www.reddit.com/r/MachineLearning/comments/a3bgyb/why_doesnt_deep_learning_work_as_well_in/,Lopelh,1544009433,,0,1,False,default,,,,,
257,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,21,a3bojg,self.MachineLearning,Neural style transfer: improvement ideas,https://www.reddit.com/r/MachineLearning/comments/a3bojg/neural_style_transfer_improvement_ideas/,nolimitex,1544011456,[removed],0,1,False,self,,,,,
258,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,21,a3bs59,self.MachineLearning,"What does ""second-order pooling"" means?",https://www.reddit.com/r/MachineLearning/comments/a3bs59/what_does_secondorder_pooling_means/,albert1905,1544012320,[removed],0,1,False,self,,,,,
259,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,21,a3bufn,self.MachineLearning,Code2vec : learning from source code,https://www.reddit.com/r/MachineLearning/comments/a3bufn/code2vec_learning_from_source_code/,yazriel0,1544012880,[removed],0,1,False,self,,,,,
260,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,21,a3bw9k,self.MachineLearning,Why mini batch when,https://www.reddit.com/r/MachineLearning/comments/a3bw9k/why_mini_batch_when/,ChesterAiGo,1544013325,[removed],0,1,False,self,,,,,
261,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3c34a,arxiv.org,[R] GANsfer Learning: Combining labelled and unlabelled data for GAN based data augmentation,https://www.reddit.com/r/MachineLearning/comments/a3c34a/r_gansfer_learning_combining_labelled_and/,i-like-big-gans,1544014902,,0,1,False,default,,,,,
262,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3c3a4,papers.nips.cc,[R] BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,https://www.reddit.com/r/MachineLearning/comments/a3c3a4/r_bingan_learning_compact_binary_descriptors_with/,i-like-big-gans,1544014935,,0,1,False,default,,,,,
263,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3c6g9,papers.nips.cc,[R] BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,https://www.reddit.com/r/MachineLearning/comments/a3c6g9/r_bingan_learning_compact_binary_descriptors_with/,i-like-big-gans,1544015660,,0,1,False,default,,,,,
264,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3c9lk,self.MachineLearning,[D] self organizing tree algorithm (SOTA) in matlab,https://www.reddit.com/r/MachineLearning/comments/a3c9lk/d_self_organizing_tree_algorithm_sota_in_matlab/,MashV,1544016371," 

Hello guys, does someone know how to implement a SOTA(self organizing tree algorithm) algorithm in matlab? Or maybe you know any tool that can help implement it?

Thank you for your attention and your response.",3,1,False,self,,,,,
265,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3c9pz,papers.nips.cc,[R] BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,https://www.reddit.com/r/MachineLearning/comments/a3c9pz/r_bingan_learning_compact_binary_descriptors_with/,i-like-big-gans,1544016392,,0,1,False,default,,,,,
266,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3cadc,arxiv.org,[R] BinGAN: Learning Compact Binary Descriptors with a Regularized GAN,https://www.reddit.com/r/MachineLearning/comments/a3cadc/r_bingan_learning_compact_binary_descriptors_with/,i-like-big-gans,1544016538,,6,1,False,default,,,,,
267,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3cfxc,youtube.com,Data Science In 5 Minutes - Grt Info,https://www.reddit.com/r/MachineLearning/comments/a3cfxc/data_science_in_5_minutes_grt_info/,MainBuilder,1544017766,,0,1,False,https://b.thumbs.redditmedia.com/MLZPd2AUz4g6j2XeGbz9v_fdFLwwvJdy_VB67EE4aiQ.jpg,,,,,
268,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,22,a3cgp6,dimensionless.in,8 Data Science Projects to Build your Portfolio,https://www.reddit.com/r/MachineLearning/comments/a3cgp6/8_data_science_projects_to_build_your_portfolio/,divya2018,1544017940,,0,1,False,https://b.thumbs.redditmedia.com/4Fk7GnaKiC8ZAnIXO4htc_PhJGCRlxKKcAs7wbyVEMM.jpg,,,,,
269,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3cmiv,techgrabyte.com,"$60 million dollar scam happened in cryptocurrency, This machine learning model predicts cryptocurrency scam even before they happen",https://www.reddit.com/r/MachineLearning/comments/a3cmiv/60_million_dollar_scam_happened_in_cryptocurrency/,navin49,1544019157,,0,1,False,default,,,,,
270,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3cmvm,self.MachineLearning,Why Minibatch when you can train on entire dataset?,https://www.reddit.com/r/MachineLearning/comments/a3cmvm/why_minibatch_when_you_can_train_on_entire_dataset/,ChesterAiGo,1544019241,"Hi!

&amp;#x200B;

A really simple idea just popped up for better training of NNs and although it looks really naive, I couldn't find any problems with it so wonder if anyone can help.

&amp;#x200B;

Let's denote a dataset = {S1, S2, S3 ... } where S1 \~ Sn are samples like (Xi, Yi) (doesn't really matter what they are).

&amp;#x200B;

Usually what SGD does is like: for S in dataset: NN.update(S). Minibatch improves it a bit by accumulating the gradients from batch first then divide it by the batchsize.

&amp;#x200B;

Another thing is, theoretically, the greater the batch size, the more stable the training and the faster the converge is. So my question is, why don't people just accumulate all the gradients first (to somewhere with greater storage space instead of on GPU) and divide it by the number of dataset so that at least the randomness introduced by how the minibatch is extracted can be eliminated. Also technically it should accelerate the convergence.

&amp;#x200B;

Would this idea work?",0,1,False,self,,,,,
271,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3cnoq,medium.com,[P] Deep Learning based magnifying glass: Super scale your low-res images.,https://www.reddit.com/r/MachineLearning/comments/a3cnoq/p_deep_learning_based_magnifying_glass_super/,cfrancesco,1544019411,,1,1,False,default,,,,,
272,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3cohx,self.MachineLearning,Non-Bayesian methods for learning Uncertainty in Training Data,https://www.reddit.com/r/MachineLearning/comments/a3cohx/nonbayesian_methods_for_learning_uncertainty_in/,sherlock_1695,1544019568,[removed],0,1,False,self,,,,,
273,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3cq9q,self.MachineLearning,[N] Hot off the Press! Links to Computer Vision News of December - with codes!,https://www.reddit.com/r/MachineLearning/comments/a3cq9q/n_hot_off_the_press_links_to_computer_vision_news/,Gletta,1544019924,"Here is the December 2018 issue of Computer Vision News, the magazine of the algorithm community published by RSIP Vision: 38 pages full of articles on computer science, computer scientists and their work.

Enjoy new Deep Learning and Artificial Intelligence researches and reviews. Free subscription at page 30.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2018December/) 

[PDF version](https://www.rsipvision.com/computer-vision-news-2018-december-pdf/)

Enjoy!

&amp;#x200B;

https://i.redd.it/gpg0w2edyg221.jpg",0,1,False,self,,,,,
274,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3cs25,self.MachineLearning,{Q} Non-Bayesian methods for learning Uncertainty in Training Data in LSTMs,https://www.reddit.com/r/MachineLearning/comments/a3cs25/q_nonbayesian_methods_for_learning_uncertainty_in/,sherlock_1695,1544020279,"I am trying to create a network which can learn the uncertainty in training data.  The aim is to learn the uncertainty inherent in data. I am creating artificial data in which noise is injected to account for uncertainty. Data is sequential images so I am using the LSTM network to learn this. 

Is there any other approach, apart from Bayesian, to learn such information. I know of ensemble techniques but they are usually expensive for LSTM network. ",0,1,False,self,,,,,
275,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3ctdt,self.MachineLearning,[D] What is a good online ML course for non data scientists?,https://www.reddit.com/r/MachineLearning/comments/a3ctdt/d_what_is_a_good_online_ml_course_for_non_data/,DanielBaldielocks,1544020543,"To clarify.  I am leaving my current job where I have function as the teams subject matter expert on all things mathematics, statistics, predictive analytics, and machine learning.  Everybody else on the team is highly proficient technically when it comes to general IT subjects and programming but lack experience in data science topics like math and statistics.

So, to help ease the transition I am trying to find a good online course to suggest to my manager for the other team members.  To clarify they are using Splunk as a data analytics platform and an area of concern for me is that they are going to be utilizing some automated machine learning tools but lack the training on how to recognize if the models created are appropriate.  So I was hoping to find some educational materials to help them with that.  Ideally it would cover the basics of model fitting and analysis but wouldn't go too deep into the underlying math.

Any help would be greatly appreciated.

Thanks",13,1,False,self,,,,,
276,MachineLearning,t5_2r3gv,2018-12-5,2018,12,5,23,a3cy2s,self.MachineLearning,[Q] Non-Bayesian methods for learning Uncertainty in Training Data in LSTMs,https://www.reddit.com/r/MachineLearning/comments/a3cy2s/q_nonbayesian_methods_for_learning_uncertainty_in/,sherlock_1695,1544021462,[removed],0,1,False,self,,,,,
277,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,0,a3d531,self.MachineLearning,"ICLR 2019: stats, trends, and best papers.",https://www.reddit.com/r/MachineLearning/comments/a3d531/iclr_2019_stats_trends_and_best_papers/,russellsparadox101,1544022772,[removed],0,1,False,https://b.thumbs.redditmedia.com/jPiG88XxEeO0vgVXCaRsx-c4Zd5qxOCp1chjTwswcDI.jpg,,,,,
278,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,0,a3d6zq,codecampanion.blogspot.com,Strassens Algorithm - Explained,https://www.reddit.com/r/MachineLearning/comments/a3d6zq/strassens_algorithm_explained/,AshishKhuraishy,1544023123,,0,1,False,default,,,,,
279,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,0,a3d984,self.MachineLearning,How to train your MAML blog post,https://www.reddit.com/r/MachineLearning/comments/a3d984/how_to_train_your_maml_blog_post/,AntreasAntoniou,1544023542,[removed],0,1,False,self,,,,,
280,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,0,a3d9zz,self.MachineLearning,[R] How to train your MAML blog post,https://www.reddit.com/r/MachineLearning/comments/a3d9zz/r_how_to_train_your_maml_blog_post/,AntreasAntoniou,1544023698,"Dear r/machinelearning friends,

Some time ago I posted on /r/MachineLearning my latest paper on ""How to train your MAML"". However, I felt that a more informal means of communicating about meta-learning in general and my work in particular was required. That way I could use simpler language and explain everything step by step, such that a wider audience can understand meta-learning in general, and get an example of it, in the form of this work. So I decided to write a blog post. https://www.bayeswatch.com/2018/11/30/HTYM/. Any feedback would be very appreciated!

P.S. My code that implements MAML and MAML++ is the only implementation that I could find in pytorch that actually replicates the original paper's results. Furthermore, instead of using the hardcoded way of doing inner loop step optimization, I abstract the updates down to the layer level, thus allowing one to build arbitrary architectures that can be used in the inner loop optimization out of the box. Feel free to post questions or suggestions using github.

Previous reddit post: https://old.reddit.com/r/MachineLearning/comments/9r79hd/r_how_to_train_your_maml/

Relevant tweet: https://twitter.com/_AntreasAntonio/status/1070331747760566272
Paper: https://arxiv.org/abs/1810.09502

Code: https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch",5,1,False,self,,,,,
281,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,0,a3dah9,arxiv.org,[1811.12222v2] ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving,https://www.reddit.com/r/MachineLearning/comments/a3dah9/181112222v2_apollocar3d_a_large_3d_car_instance/,ihaphleas,1544023791,,2,1,False,default,,,,,
282,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,0,a3di23,self.MachineLearning,"Simple Questions Thread December 05, 2018",https://www.reddit.com/r/MachineLearning/comments/a3di23/simple_questions_thread_december_05_2018/,AutoModerator,1544025170,[removed],0,1,False,self,,,,,
283,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,1,a3drk6,self.MachineLearning,Must read papers for Optical Character Recognition?,https://www.reddit.com/r/MachineLearning/comments/a3drk6/must_read_papers_for_optical_character_recognition/,GGEverything,1544026816,[removed],0,1,False,self,,,,,
284,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,1,a3dw01,self.MachineLearning,Interpreting xreg in time series forecast,https://www.reddit.com/r/MachineLearning/comments/a3dw01/interpreting_xreg_in_time_series_forecast/,seanwarmstrong,1544027567,[removed],0,1,False,self,,,,,
285,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,2,a3e5oi,ibm.com,"Think 2019 | Feb 12-15 | IBM Flagship Technology Conference | AI, Blockchain, Cloud and more.",https://www.reddit.com/r/MachineLearning/comments/a3e5oi/think_2019_feb_1215_ibm_flagship_technology/,5thandmelody,1544029210,,0,1,False,https://b.thumbs.redditmedia.com/VqSBiHlYQCJGYovzHh5sFde4bvtx3JU7MRgx9Ns-Pjs.jpg,,,,,
286,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,2,a3edmp,self.MachineLearning,[D] is calibration and training the same in machine learning context?,https://www.reddit.com/r/MachineLearning/comments/a3edmp/d_is_calibration_and_training_the_same_in_machine/,marksteve4,1544030493,I don't see any  difference between these to terms,7,1,False,self,,,,,
287,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,2,a3egjg,self.MachineLearning,Filtr.pub: Finding signals in noisy AI Research,https://www.reddit.com/r/MachineLearning/comments/a3egjg/filtrpub_finding_signals_in_noisy_ai_research/,jeetmehta,1544030982,[removed],0,1,False,self,,,,,
288,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,2,a3eq4h,medium.com,[N] Vancouver Named NeurIPS 2019 &amp; 2020 Host as Visa Issues Continue to Plague the AI Conference,https://www.reddit.com/r/MachineLearning/comments/a3eq4h/n_vancouver_named_neurips_2019_2020_host_as_visa/,gwen0927,1544032581,,0,1,False,default,,,,,
289,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,3,a3eti7,ai.googleblog.com,TF-Ranking: A Scalable TensorFlow Library for Learning-to-Rank,https://www.reddit.com/r/MachineLearning/comments/a3eti7/tfranking_a_scalable_tensorflow_library_for/,sjoerdapp,1544033131,,0,1,False,default,,,,,
290,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,3,a3f10t,self.MachineLearning,[R] Fashionable Modelling with Flux,https://www.reddit.com/r/MachineLearning/comments/a3f10t/r_fashionable_modelling_with_flux/,tensorflower,1544034413,"https://arxiv.org/abs/1811.01457

Anyone in this community using Flux? I've recently been rewriting some simple implementations of my projects in Flux and I'm quite impressed with how nice/non-ugly I can make parts that look very ugly in other frameworks. ",15,1,False,self,,,,,
291,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,3,a3f3mi,weirdgeek.com,Dimension Reduction with Principal Component Analysis (PCA),https://www.reddit.com/r/MachineLearning/comments/a3f3mi/dimension_reduction_with_principal_component/,WeirdGeekDotCom,1544034856,,0,1,False,default,,,,,
292,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,3,a3f735,self.MachineLearning,Creating Your First Machine Learning Classifier with Sklearn,https://www.reddit.com/r/MachineLearning/comments/a3f735/creating_your_first_machine_learning_classifier/,andrea_manero,1544035457,[removed],0,1,False,self,,,,,
293,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,3,a3f9i9,self.MachineLearning,"Applied Machine Learning Days 2019 - Jeff Dean and more speakers from top unis, startups, Google, Facebook, Microsoft, Alibaba",https://www.reddit.com/r/MachineLearning/comments/a3f9i9/applied_machine_learning_days_2019_jeff_dean_and/,adammathias,1544035872,"Applied Machine Learning Days is a quality machine learning event held in Lausanne every January.  2018 had participation from most top machine learning orgs including folks who created PyTorch and ImageNet.

In 2019 there will be participants like Jeff Dean from Google, Facebook, Microsoft and Alibaba, top unis and interesting startups like Nabla, and new focused tracks on application areas like language, finance and cities.

Especially for those of us more on the engineering and/or startup side, this is a much more usable format than the major academic research conferences, which are mostly theoretical and increasingly overloaded.

So it's a great chance for efficient meaningful interaction with top machine learning people from around the world in continental Europe and at a relatively calm time of year.

This is the last week for early-bird tickets.

Full disclosure: I am organising [the Language track](https://www.appliedmldays.org/tracks/8), but I make no profit from this, quite the opposite, I just find it an intriguing event.",0,1,False,self,,,,,
294,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,4,a3fg5c,self.MachineLearning,Improving convergence using example dropout,https://www.reddit.com/r/MachineLearning/comments/a3fg5c/improving_convergence_using_example_dropout/,Drag0nDr0p,1544037000,[removed],0,1,False,self,,,,,
295,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,5,a3gca0,medium.com,Chinese Publisher Introduces AI Textbooks For Preschoolers,https://www.reddit.com/r/MachineLearning/comments/a3gca0/chinese_publisher_introduces_ai_textbooks_for/,Yuqing7,1544042427,,0,1,False,https://b.thumbs.redditmedia.com/FtU23H8iiObkpCiiDaf0fVojlYt8-8msJkDfXpEIDtQ.jpg,,,,,
296,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,5,a3gdec,self.MachineLearning,[Help] Fantasy Football AI (Python),https://www.reddit.com/r/MachineLearning/comments/a3gdec/help_fantasy_football_ai_python/,sameer097k,1544042634,"Hi, my AI class has a final project, I decided to predict fantasy football points as my project. I'm done with what you would call a pre alpha version. 

What do I have?
Have two models for predicting so far but they aren't great at predicting by any measurement. My loss function is returning 28.3 or something after 10k epochs if that makes any sense to you.
My first model takes in tensor (player_offense_rank, player_avgFantasyPoints, weather, opp_defense_rank) and returns a tensor that contains a float predicted fantasy points acquired that game value.
Second model takes in tensor (player fantasy avg, over under for the game, vegas betting line (so minus x if favored and plus x if not favored)

As you can see they are all float values because the model im using only takes floats. So things like player names, positions, team names, and the like are hard to represent.

I need help deciding on a model format that will give me the best predictions. What should my input be? I'm scraping data from https://www.pro-football-reference.com 
And placing it into csv files to manipulate and store. 

Also the positions value in the site is kinda shabby if you have worked with it do you have any tips?

If you have any other questions or what my code just comment below.

Thanks, any help appreciated.

Tldr; creating fantasy football AI in Python. Current models are really wrong, need help deciding on what i should feed my model using data from (https://www.pro-football-reference.com). ",0,1,False,self,,,,,
297,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,6,a3gqkw,self.MachineLearning,AI Platforms subreddit?,https://www.reddit.com/r/MachineLearning/comments/a3gqkw/ai_platforms_subreddit/,ellie6939,1544044889,"Is there a subreddit dedicated to AI platforms - pros/cons, applications, performance benchmarks etc? Or is here the best place to discuss those things?",0,1,False,self,,,,,
298,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,6,a3grk1,youtube.com,Lleg DataBeers a Hermosillo: Samuel Noriega.,https://www.reddit.com/r/MachineLearning/comments/a3grk1/lleg_databeers_a_hermosillo_samuel_noriega/,shugert,1544045055,,0,1,False,https://b.thumbs.redditmedia.com/PmIFj8vqsJsDuLK8sK4w07BDCEigv-EpoZuKc-q546o.jpg,,,,,
299,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,6,a3h1xm,self.MachineLearning,"$100,000 in Google Cloud credits for research",https://www.reddit.com/r/MachineLearning/comments/a3h1xm/100000_in_google_cloud_credits_for_research/,iliketorun1,1544046832,"Hey /r/MachineLearning,

&amp;#x200B;

I run a company called [Pioneer](https://pioneer.app). We're trying to scalably identify and nurture the ambitious outsiders of the world. We make a game where you can apply with any type of project: research in AI, physics, chemistry, a company or even music. Winners are selected monthly. 

We're giving out $100,000 in Google Cloud credits in our December Tournament, along with a round-trip ticket to Silicon Valley, mentorship from experts like Stephen Wolfram and more.

&amp;#x200B;

Applications close on Sunday. Read more here: [https://pioneer.app/blog/holiday-tournament/](https://pioneer.app/blog/holiday-tournament/). 

\-Daniel

&amp;#x200B;

P.S. This isn't a scam, BTW. Read about me here: [https://twitter.com/danielgross](https://twitter.com/danielgross). ",0,1,False,self,,,,,
300,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,7,a3h8un,self.MachineLearning,Looking to predict speed from dashcam video. Critique my approach?,https://www.reddit.com/r/MachineLearning/comments/a3h8un/looking_to_predict_speed_from_dashcam_video/,UnfazedButDazed,1544048010,[removed],0,1,False,self,,,,,
301,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,8,a3hx1o,julialang.org,Building a Language and Compiler for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a3hx1o/building_a_language_and_compiler_for_machine/,Bdamkin54,1544052226,,0,1,False,default,,,,,
302,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,10,a3ixdi,self.MachineLearning,What kind of questions are asked in OpenAI internship interviews?,https://www.reddit.com/r/MachineLearning/comments/a3ixdi/what_kind_of_questions_are_asked_in_openai/,elitalobo1995,1544058918,[removed],0,1,False,self,,,,,
303,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,11,a3jf6z,self.MachineLearning,Data Sciences motivation / career change,https://www.reddit.com/r/MachineLearning/comments/a3jf6z/data_sciences_motivation_career_change/,Guizmolabs,1544062438,[removed],0,1,False,self,,,,,
304,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,11,a3jngz,self.MachineLearning,Can you help my friend realize the threat that is AI through machine learning?,https://www.reddit.com/r/MachineLearning/comments/a3jngz/can_you_help_my_friend_realize_the_threat_that_is/,ayypex,1544064073,[removed],0,1,False,self,,,,,
305,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,11,a3jqzv,self.MachineLearning,Question for Caffe: How to set up input layers in an outdated version of Caffe?,https://www.reddit.com/r/MachineLearning/comments/a3jqzv/question_for_caffe_how_to_set_up_input_layers_in/,Arsenal591,1544064760,"For some reasons, I have to use an outdated version of Caffe, thus have to work on old fashioned ""\*.prototxt"" file format. So when defining my own network, at the beginning of my prototxt, I have to write something like this: 

 

    layer {     
        name: 'input-data'     
        type: 'Python'     
        top: 'data'     
        top: 'gt_boxes'     
        python_param {         
            module: 'roi_data_layer.layer'         
            layer: 'RoIDataLayer'         
            param_str: ""'num_classes': 2""     
        }     
        include {         
            phase: TRAIN     
        } 
    } 
    input: ""data"" 
    input_shape {     
        dim: 1     
        dim: 3    
        dim: 224     
        dim: 224 
    }

When I start to train,  a ""caffe Top blob 'data' produced by multiple sources"" is raised, indicating that both layer with a name ""data"" is generated. However, I'd like to make the second one only avaiable in TEST phase, rather than in TRAIN phase.

In up-to-date version of Caffe, there is a ""Input"" layer type, so I could just set phase like other layer types. But in this outdated version, it seems that there is not ""Input"" layer type.

So, what should I do now?  Or is there any way that can work around this issue? ",0,1,False,self,,,,,
306,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,12,a3jwj9,self.MachineLearning,Can you train deep recurrent neural network layer by layer?,https://www.reddit.com/r/MachineLearning/comments/a3jwj9/can_you_train_deep_recurrent_neural_network_layer/,qudcjf7928,1544065892,[removed],0,1,False,https://a.thumbs.redditmedia.com/Emg0WkvIM42mabAqC7YVqRUKpNr17t63WEcSj37b8m8.jpg,,,,,
307,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,12,a3jxtf,youtube.com,[Video] DeepVoxels: Learning Persistent 3D Feature Embeddings,https://www.reddit.com/r/MachineLearning/comments/a3jxtf/video_deepvoxels_learning_persistent_3d_feature/,TwoUpper,1544066152,,0,1,False,default,,,,,
308,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,12,a3jyzu,self.MachineLearning,Adam optimizer on water analytics.,https://www.reddit.com/r/MachineLearning/comments/a3jyzu/adam_optimizer_on_water_analytics/,Prof_Montgomery,1544066388,[removed],0,1,False,self,,,,,
309,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,12,a3k85c,self.MachineLearning,[R] Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon,https://www.reddit.com/r/MachineLearning/comments/a3k85c/r_machine_learning_for_combinatorial_optimization/,sorayah44,1544068284,"https://arxiv.org/abs/1811.06128

This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art methodologies involve algorithmic decisions that either require too much computing time or are not mathematically well defined. Thus, machine learning looks like a promising candidate to effectively deal with those decisions. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.

Authors: Yoshua Bengio, Andrea Lodi, Antoine Prouvost",7,1,False,self,,,,,
310,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,13,a3kiq5,arxiv.org,[R] Progressive Neural Architecture Search,https://www.reddit.com/r/MachineLearning/comments/a3kiq5/r_progressive_neural_architecture_search/,SixHampton,1544070523,,2,1,False,default,,,,,
311,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,13,a3kkcm,self.MachineLearning,[D] Probabilistic Graphical Models,https://www.reddit.com/r/MachineLearning/comments/a3kkcm/d_probabilistic_graphical_models/,RudyWurlitzer,1544070877,"Do you know examples of practical problems solved using probabilistic graphical models? I mean the theory of which is given in [this book](https://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193/) in more than 1200 pages.  If you do, please explain what those problems are and why the traditional machine learning paradigm doesn't work in this case. Thank you!",13,1,False,self,,,,,
312,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,14,a3kto1,self.MachineLearning,Help,https://www.reddit.com/r/MachineLearning/comments/a3kto1/help/,siddugunner,1544072975,[removed],0,1,False,self,,,,,
313,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,14,a3kw9f,/r/MachineLearning/comments/a3kw9f/7_ways_ai_will_transform_hearhcare/,7 ways AI will transform hearhcare,https://www.reddit.com/r/MachineLearning/comments/a3kw9f/7_ways_ai_will_transform_hearhcare/,navin49,1544073571,,0,1,False,default,,,,,
314,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,14,a3kymi,self.MachineLearning,Is it a good idea to start a consulting firm that works on machine learning?,https://www.reddit.com/r/MachineLearning/comments/a3kymi/is_it_a_good_idea_to_start_a_consulting_firm_that/,cloudrobo,1544074121,[removed],0,1,False,self,,,,,
315,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,14,a3l0vh,issuu.com,5 Best Machine Learning Applications,https://www.reddit.com/r/MachineLearning/comments/a3l0vh/5_best_machine_learning_applications/,smadrid056,1544074630,,0,1,False,default,,,,,
316,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,14,a3l5v9,self.MachineLearning,Anyone interested in Xilinx UltraScale+ KU3P FPGA accelerator at $400?,https://www.reddit.com/r/MachineLearning/comments/a3l5v9/anyone_interested_in_xilinx_ultrascale_ku3p_fpga/,rappidminerx,1544075806,[removed],1,1,False,self,,,,,
317,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,15,a3lbr1,self.MachineLearning,[D] What is the state-of-the-art multimodal technique for sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/a3lbr1/d_what_is_the_stateoftheart_multimodal_technique/,ZER_0_NE,1544077196,,4,1,False,self,,,,,
318,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,15,a3lin7,self.MachineLearning,[D] How does the image captioning technique works? Is there a way to generate sentiment-based captions from images with text(read memes)?,https://www.reddit.com/r/MachineLearning/comments/a3lin7/d_how_does_the_image_captioning_technique_works/,ZER_0_NE,1544078957,,2,1,False,self,,,,,
319,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,16,a3lr2r,codecampanion.blogspot.com,Dinics algorithm for Maximum Flow,https://www.reddit.com/r/MachineLearning/comments/a3lr2r/dinics_algorithm_for_maximum_flow/,AshishKhuraishy,1544081088,,0,1,False,https://b.thumbs.redditmedia.com/diZazT5F-AGtn7uJqLX6uezHxSVhwsrMI2OQng5V5IQ.jpg,,,,,
320,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,16,a3ls91,slideshare.net,5 Excellent Machine Learning Applications,https://www.reddit.com/r/MachineLearning/comments/a3ls91/5_excellent_machine_learning_applications/,smadrid056,1544081414,,0,1,False,default,,,,,
321,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,16,a3luo5,self.MachineLearning,Time Series,https://www.reddit.com/r/MachineLearning/comments/a3luo5/time_series/,Anu2008,1544082070,[removed],0,1,False,self,,,,,
322,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,16,a3lx0u,youtube.com,YouTube,https://www.reddit.com/r/MachineLearning/comments/a3lx0u/youtube/,augustinedukett,1544082738,,0,1,False,https://b.thumbs.redditmedia.com/1ywAhtOmP13eIGnLRBHeMqvZ9PGoTIZfY1wYhISQAnk.jpg,,,,,
323,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,17,a3lzm0,arxiv.org,[R] Neural Rejuvenation: Improving Deep Network Training by Enhancing Computational Resource Utilization,https://www.reddit.com/r/MachineLearning/comments/a3lzm0/r_neural_rejuvenation_improving_deep_network/,HigherTopoi,1544083466,,1,1,False,default,,,,,
324,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,17,a3lzsq,arxiv.org,[R] A Retrieve-and-Edit Framework for Predicting Structured Outputs,https://www.reddit.com/r/MachineLearning/comments/a3lzsq/r_a_retrieveandedit_framework_for_predicting/,HigherTopoi,1544083513,,0,1,False,default,,,,,
325,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,17,a3m7mi,arxiv.org,[R] Quasi-hyperbolic momentum and Adam for deep learning,https://www.reddit.com/r/MachineLearning/comments/a3m7mi/r_quasihyperbolic_momentum_and_adam_for_deep/,abstractcontrol,1544085805,,0,1,False,default,,,,,
326,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,17,a3maco,self.MachineLearning,Benchmarking transformer pytorch v tensorflow--pytorch 2x faster?,https://www.reddit.com/r/MachineLearning/comments/a3maco/benchmarking_transformer_pytorch_v/,ObjectivePlankton,1544086633,[removed],0,1,False,self,,,,,
327,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,18,a3md4v,youtube.com,machine learning .p16 -Activation Function. [Hindi],https://www.reddit.com/r/MachineLearning/comments/a3md4v/machine_learning_p16_activation_function_hindi/,Neel_kamal_sahu,1544087446,,0,1,False,default,,,,,
328,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,18,a3mhpv,self.MachineLearning,"Data Science Course,Data Analytics Course,Machine Learning Courses Pune,Mumbai",https://www.reddit.com/r/MachineLearning/comments/a3mhpv/data_science_coursedata_analytics_coursemachine/,Tejas-Sonawane,1544088785,[removed],0,1,False,self,,,,,
329,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,18,a3mirn,self.MachineLearning,Deep Learning Vs Machine Learning And Its Affect On Jobs,https://www.reddit.com/r/MachineLearning/comments/a3mirn/deep_learning_vs_machine_learning_and_its_affect/,andrea_manero,1544089082,[removed],0,1,False,self,,,,,
330,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,19,a3momz,self.MachineLearning,[D] Benchmarking transformer pytorch v tensorflow--pytorch 2x faster? N,https://www.reddit.com/r/MachineLearning/comments/a3momz/d_benchmarking_transformer_pytorch_v/,ObjectivePlankton,1544090834,"\[repost because didn't tag title correctly the first time\]

 

Not trying to turn this into a pytorch v tf flame war, but would appreciate any insight collective wisdom might provide here. I'd love for the answer to just be that we're doing things wrong.

We're working with variants of the Transformer model and are trying to push training throughput performance. The reference tf implementation seems significantly slower.

[https://arxiv.org/abs/1806.00187](https://arxiv.org/abs/1806.00187), from FB, reports a throughput of 54k tokens/s on 8x V100 on their reimplementation (and then pushes to &gt;200k tokens/s with mixed precision and various tweaks, although we are not focused on that quite yet). The reference implementation is available at [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq), and we verified that it does what it says it does.

The same paper then reports the original Tensorflow Transformer implementation (citing their paper) at 25k tokens/s, on 8x P100 (reference implementation at [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)).

The latter tf is less than half speed of the former pytorch--but this is of course apples:oranges, as it is 8xP100 versus 8xV100. We then rang the Tensorflow implementation on 8xV100...and got throughput numbers only slightly higher than 25k tokens/s.

We did our best to ensure the hparams across the two implementations were as close as possible (as did the FB authors of the cited arxiv paper). We also played with various versus of CUDA, tf, and pytorch, but did not see any major performance movement.

**Net, it looks like the pytorch implementation is &gt;2x faster than the tf implementation**. Performance and benchmarking is a black art, but would love any insight--issues we might be missing, is this result totally unsurprising (there is a lot of anecdotal stuff out there about pytorch potentially being faster, but 2x faster on one of Google's flagship research models is somewhat disturbing), etc.

CONTEXT: training throughput has become a big issue for us, for reasons of both cost and wall clock (researcher) time, and we're exploring trying to get a step up via mixed precision, etc.; the FB paper is the best resource on doing this with transformer that we've found. We're fairly heavily invested to-date in both tensorflow and tensor2tensor, so are loathe to move off tf, but if pytorch gives large performance boosts (for us, probably closer to 8x than 2x, since pytorch fairseq has working mixed precision support, and tf still has incredibly anemic, half-baked support), it may sadly (for us) be worth it.

As something of an additional sanity check on what we've seen, others using tf ([https://github.com/NVIDIA/OpenSeq2Seq/issues/250](https://github.com/NVIDIA/OpenSeq2Seq/issues/250)) have had challenges matching pytorch fairseq performance (on separate, non-t2t codebases). From looking over the pytorch transformer code base, there doesn't appear to be anything deeply custom going on, so this feels like things should be behaving better.

Again, not trying to start a tensorflow-is-terrible holy war, but am trying to gain insight into this large performance gap--and, ideally, anything we can do to ameliorate it, or feedback that we should give up and move on.",13,1,False,self,,,,,
331,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,20,a3n2nd,self.MachineLearning,[D] Expressing k classes with size log(k) output?,https://www.reddit.com/r/MachineLearning/comments/a3n2nd/d_expressing_k_classes_with_size_logk_output/,seann999,1544094853,"Is there a method to express k classes with log(k) neurons? The classes semantics can be organized in a tree structure (e.g. cat, tiger, wolf, and dog can be a tree that first splits by feline/canine and then domesticated/wild. Or an even closer example to what Im trying to do would be predicting a number through digits.). Ive read about hierarchical softmax, but that still seems to require k-1 neurons.",10,1,False,self,,,,,
332,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,20,a3n4t0,medium.com,[D] The deepest problem with deep learning,https://www.reddit.com/r/MachineLearning/comments/a3n4t0/d_the_deepest_problem_with_deep_learning/,alexeyr,1544095456,,0,1,False,default,,,,,
333,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,20,a3n83c,self.MachineLearning,High definition Image to Image translation ideas?,https://www.reddit.com/r/MachineLearning/comments/a3n83c/high_definition_image_to_image_translation_ideas/,ashblue21,1544096364,[removed],0,1,False,self,,,,,
334,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,20,a3n89y,nature.com,[R] Machine learning predicts individual cancer patient responses to therapeutic drugs with high accuracy,https://www.reddit.com/r/MachineLearning/comments/a3n89y/r_machine_learning_predicts_individual_cancer/,born_to_engineer,1544096415,,0,1,False,default,,,,,
335,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,20,a3n92z,self.MachineLearning,Can't Classify An Example,https://www.reddit.com/r/MachineLearning/comments/a3n92z/cant_classify_an_example/,axaqel,1544096634,[removed],0,1,False,self,,,,,
336,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,21,a3nnme,self.MachineLearning,Is there any type of pooling that allows CNNs to work on variable-size audio inputs?,https://www.reddit.com/r/MachineLearning/comments/a3nnme/is_there_any_type_of_pooling_that_allows_cnns_to/,adelredjimi,1544101075,[removed],0,1,False,self,,,,,
337,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,22,a3np23,self.MachineLearning,Regression Tree,https://www.reddit.com/r/MachineLearning/comments/a3np23/regression_tree/,machinescientist,1544101375,"Can someone provide a python code on how to build a regression tree for this data set? It would be much appreciated! :)

&amp;#x200B;

np.random.seed(777)

train\_x = np.linspace(0,5, 100)

train\_y = train\_x\*\*2 + np.random.randn(len(train\_x))

plt.plot(train\_x, train\_y, '.')",0,1,False,self,,,,,
338,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,22,a3ntdv,self.MachineLearning,[D] Multi agent systems research,https://www.reddit.com/r/MachineLearning/comments/a3ntdv/d_multi_agent_systems_research/,Maplernothaxor,1544102306,"Besides OpenAI, are there many other groups who are tackling this problem? What are some must read papers for someone trying to familiarise themselves with the area?

Thanks",6,1,False,self,,,,,
339,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,22,a3nvf8,self.MachineLearning,PyCM 1.6 released : New machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/a3nvf8/pycm_16_released_new_machine_learning_library_for/,sepandhaghighi,1544102747,"PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters.	PyCM is the swiss-army knife of confusion matrices, targeted mainly at data scientists that need a broad array of metrics for predictive models and an accurate evaluation of large variety of classifiers. 

&amp;#x200B;

Version 1.6 changelog :

* AUC Value Interpretation (AUCI) added
* Example-6 added (Unbalanced data)
* Anaconda cloud package added
* overall\_param and class\_param arguments added to stat, save\_stat and save\_html methods
* class\_param argument added to save\_csv method
* \_ removed from overall statistics names
* README modified
* Document modified

Repository : [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

Website : [http://pycm.shaghighi.ir/](http://pycm.shaghighi.ir/)

Document :  [http://pycm.shaghighi.ir/doc/](http://pycm.shaghighi.ir/doc/)",0,1,False,self,,,,,
340,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,23,a3o71f,davidsbatista.net,"An overview of some recent (2015~2016) methods to performer sequence labelling in NLP context: NER, pos-tagging, chunking, etc.",https://www.reddit.com/r/MachineLearning/comments/a3o71f/an_overview_of_some_recent_20152016_methods_to/,fulltime_philosopher,1544105172,,0,1,False,default,,,,,
341,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,23,a3oe32,self.MachineLearning,"The Future of Commerce, Customer Service and Personalization | Blake Morgan",https://www.reddit.com/r/MachineLearning/comments/a3oe32/the_future_of_commerce_customer_service_and/,The_Syndicate_VC,1544106646,[removed],0,1,False,self,,,,,
342,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,23,a3ofor,self.MachineLearning,[D] Depth First Learning Fellowship: $4000 grants to build ML curricula,https://www.reddit.com/r/MachineLearning/comments/a3ofor/d_depth_first_learning_fellowship_4000_grants_to/,suryabhupa,1544106960,"Hi! Were researchers from NYU, FAIR, Google Brain, and and DeepMind. We designed Depth First Learning, a pedagogy for diving deep into ML by carefully tailoring a curriculum around a particular ML paper or concept and leading small, focused discussion groups. So far, weve created guides for InfoGAN, TRPO, AlphaGoZero, and DeepStack.

Since our launch, weve received very positive feedback from students and researchers. Now, we want to run new, online classes around the world.

Thanks to the generosity of Jane Street, we will provide 4 fellows with a $4000 grant each to build a 6 week curriculum and run weekly on-line discussions.

If youd like to lead a class about an important paper in machine learning, please visit http://fellowship.depthfirstlearning.com to apply. We look forward to hearing from you!

We understand that the process of curating a meaningful curriculum with reading materials, practice problems, and instructive discussion points can be very rewarding, but also time-consuming and difficult. We wanted to make sure that the people compiling the content understood that their efforts were well worth their time, and we hope that this fellowship program will enable more guides to get on board.",24,1,False,self,,,,,
343,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,23,a3oi1s,i.redd.it,Could anyone describe the Y-axis of this graph?,https://www.reddit.com/r/MachineLearning/comments/a3oi1s/could_anyone_describe_the_yaxis_of_this_graph/,Virre14,1544107441,,0,1,False,default,,,,,
344,MachineLearning,t5_2r3gv,2018-12-6,2018,12,6,23,a3okff,self.MachineLearning,Best Unconditional ImageNet Generations Sor Far: SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING,https://www.reddit.com/r/MachineLearning/comments/a3okff/best_unconditional_imagenet_generations_sor_far/,downtownslim,1544107905,[removed],0,1,False,self,,,,,
345,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,0,a3oscv,self.MachineLearning,[R] Best Unconditional ImageNet Generations Sor Far: SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING,https://www.reddit.com/r/MachineLearning/comments/a3oscv/r_best_unconditional_imagenet_generations_sor_far/,downtownslim,1544109399,[removed],0,1,False,self,,,,,
346,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,0,a3osi1,twitter.com,Protest NIPS Professor has death wish against academic colleague,https://www.reddit.com/r/MachineLearning/comments/a3osi1/protest_nips_professor_has_death_wish_against/,joeldick,1544109424,,1,1,False,default,,,,,
347,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,0,a3osj0,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Dec. 6, 2018",https://www.reddit.com/r/MachineLearning/comments/a3osj0/n_weekly_machine_learning_opensource_roundup_dec/,stkim1,1544109429,,0,1,False,default,,,,,
348,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,0,a3oto4,nastel.com,Using AIOps,https://www.reddit.com/r/MachineLearning/comments/a3oto4/using_aiops/,davidliff,1544109645,,0,1,False,default,,,,,
349,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,0,a3ow08,medium.com,[R] The Data Science Workflow,https://www.reddit.com/r/MachineLearning/comments/a3ow08/r_the_data_science_workflow/,AnYvia,1544110086,,0,1,False,default,,,,,
350,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,0,a3p1rv,self.MachineLearning,What is the best/right approach to evaluate the performance in this case?,https://www.reddit.com/r/MachineLearning/comments/a3p1rv/what_is_the_bestright_approach_to_evaluate_the/,Hansi_klein,1544111118,[removed],0,1,False,self,,,,,
351,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3pcfq,xconomy.com,Cambridge Startup Forge AI Raises $11M to Crack Unstructured Data for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a3pcfq/cambridge_startup_forge_ai_raises_11m_to_crack/,jenniferlum,1544112936,,0,1,False,default,,,,,
352,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3pdnm,nature.com,[R] Why rankings of biomedical image analysis competitions should be interpreted with care,https://www.reddit.com/r/MachineLearning/comments/a3pdnm/r_why_rankings_of_biomedical_image_analysis/,TropicalAudio,1544113151,,1,1,False,default,,,,,
353,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3phcx,self.MachineLearning,Neural network optimization by matching prediction error with true error,https://www.reddit.com/r/MachineLearning/comments/a3phcx/neural_network_optimization_by_matching/,daved_it,1544113765,[removed],1,1,False,self,,,,,
354,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3pin6,stateoftheart.ai,State of the art AI,https://www.reddit.com/r/MachineLearning/comments/a3pin6/state_of_the_art_ai/,wavelander,1544113973,,0,1,False,default,,,,,
355,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3pjp5,self.MachineLearning,[D] Tools for research papers reviewing,https://www.reddit.com/r/MachineLearning/comments/a3pjp5/d_tools_for_research_papers_reviewing/,Gibepad,1544114145,"Alright, here's the deal.
I'm an undergrad student (soon to be a bachelor) and my main goal of life right now is finishing my CompSci course and applying for a Masters degree.

I'm in the process of writing my first research paper, my advisor wants to publish my research in Elsevier, the thing is, I don't think my English is advanced enough to write this alone (yeah, he's reviewing it, but one a month, tops).

So, anyone knows any tools (pref. free) witch could bump my english game into ""research papers"" standarts?",1,1,False,self,,,,,
356,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3pkgq,self.MachineLearning,[R] Enhanced Network Embedding with Text Information (ICPR 2018),https://www.reddit.com/r/MachineLearning/comments/a3pkgq/r_enhanced_network_embedding_with_text/,benitorosenberg,1544114269,"&amp;#x200B;

https://i.redd.it/s0u85pjxqo221.png

Paper: [https://github.com/benedekrozemberczki/TENE/blob/master/tene\_paper.pdf](https://github.com/benedekrozemberczki/TENE/blob/master/tene_paper.pdf)

Python: [https://github.com/benedekrozemberczki/TENE](https://github.com/benedekrozemberczki/TENE)

&amp;#x200B;

ABSTRACT:

Network embedding aims at learning the low dimensional and continuous vector representation for each node in networks, which is useful in many real applications. While most existing network embedding methods only focus on the network structure, the rich text information associated with nodes, which is often closely related to network structure, is widely neglected. Thus, how to effectively incorporate text information into network embedding is a problem worth studying. To solve the problem, we propose a Text Enhanced Network Embedding (TENE) method under the framework of non-negative matrix factorization to integrate network structure and text information together. We explore the consistent relationship between node representations and text cluster structure to make the network embedding more informative and discriminative. TENE learns the representations of nodes under the guidance of both proximity matrix which captures the network structure and text cluster membership matrix derived from clustering for text information. We evaluate the quality of network embedding on the task of multi-class classification of nodes. Experimental results on all three real-world datasets show the superior performance of TENE compared with baselines. ",0,1,False,self,,,,,
357,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3pl6z,self.MachineLearning,[R] Neural Network optimization by matching predicted error with true error,https://www.reddit.com/r/MachineLearning/comments/a3pl6z/r_neural_network_optimization_by_matching/,daved_it,1544114382,"Hello all,

This is my first post in machine learning.  I've been thinking  about a way to prevent neural networks from overfitting.  This is probably been done before, but I wasn't able to find any literature on it.  I'm looking for feedback and drawbacks that this approach might have.

One of the challenges of neural networks is avoiding overfitting.  Say we have an independent set of known values with a known measurement error.  We are trying to build a model between the known values and a dataset of physical measurements corresponding to them.  We know that the dataset has the same or less measurement error as the known values.  Along with cross-validation and the standard battery of tests, **could we optimize the neural network so that the predictive error of the model is the same as the error in the measurements in the known values?**",4,1,False,self,,,,,
358,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,1,a3ppvh,blog.openai.com,[R] Quantifying Generalization in Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a3ppvh/r_quantifying_generalization_in_reinforcement/,Kaixhin,1544115142,,1,1,False,default,,,,,
359,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,2,a3pwkj,self.MachineLearning,Statistical Learning overlaps with Analytics Edge,https://www.reddit.com/r/MachineLearning/comments/a3pwkj/statistical_learning_overlaps_with_analytics_edge/,x-w-j,1544116251,"So I am prepared to Andrew Ng's ML on coursera and another most cited complimentary course is 

[https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/)

and as I go through the syllabus, it seems it overlaps to the most [Analytics Edge](https://www.edx.org/course/the-analytics-edge) and I am wondering did anyone did both the statistics course and would love to get some thought on the overlap and worth doing them again. I did the analytics edge in Spring 16

&amp;#x200B;

Thanks",0,1,False,self,,,,,
360,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,2,a3q0gt,self.MachineLearning,[R] Best Unconditional ImageNet Generations Sor Far: SUBSCALE PIXEL NETWORKS AND MULTIDIMENSIONAL UPSCALING,https://www.reddit.com/r/MachineLearning/comments/a3q0gt/r_best_unconditional_imagenet_generations_sor_far/,downtownslim,1544116897,"https://twitter.com/NalKalchbrenner/status/1070669203680755712
&gt; More compute, better architectures and novel robust orderings have fueled tremendous progress for AR image models. SPNs are our latest installment. Looking forward to samples from 2020!

paper: http://arxiv.org/abs/1812.01608
open reviews (scores 9,10,7): http://tinyurl.com/ycn3j3cj",0,1,False,self,,,,,
361,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,2,a3q53w,self.MachineLearning,[R] PyCM 1.6 released: New machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/a3q53w/r_pycm_16_released_new_machine_learning_library/,sepandhaghighi,1544117677,"PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters. PyCM is the swiss-army knife of confusion matrices, targeted mainly at data scientists that need a broad array of metrics for predictive models and an accurate evaluation of large variety of classifiers.

&amp;#x200B;

Version 1.6 changelog :

 

* AUC Value Interpretation (AUCI) added
* Example-6 added
* Anaconda cloud package added
* overall\_param and class\_param arguments added to stat, save\_stat and save\_html methods
* class\_param argument added to save\_csv method
* \_ removed from overall statistics names
* README modified
* Document modified

Repository : [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

Website : [http://pycm.shaghighi.ir/](http://pycm.shaghighi.ir/)

Document : [http://pycm.shaghighi.ir/doc/](http://pycm.shaghighi.ir/doc/)

Paper Link : [http://joss.theoj.org/papers/10.21105/joss.00729](http://joss.theoj.org/papers/10.21105/joss.00729)",4,1,False,self,,,,,
362,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,3,a3qfjj,self.MachineLearning,Ensemble of neural networks?,https://www.reddit.com/r/MachineLearning/comments/a3qfjj/ensemble_of_neural_networks/,aashwin93,1544119383,[removed],0,1,False,self,,,,,
363,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,3,a3qms6,self.MachineLearning,Tried to do some project after long.Find it really hard now.Advice needed.,https://www.reddit.com/r/MachineLearning/comments/a3qms6/tried_to_do_some_project_after_longfind_it_really/,BySNiP,1544120566,[removed],0,1,False,self,,,,,
364,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,3,a3qp87,medium.com,How to recognize fake AI-generated images,https://www.reddit.com/r/MachineLearning/comments/a3qp87/how_to_recognize_fake_aigenerated_images/,kcimc,1544120980,,0,1,False,https://b.thumbs.redditmedia.com/DvSzuA9G90usj1B48XojIb26ASjk_oEOJpmRZceQDck.jpg,,,,,
365,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,3,a3qui0,youtube.com,Perfect example of how to apply #SyntheticData dealing with situations when you have a very limited presence of some material type of events in your dataset (which is a pretty much widespread issue across all industries) hashtag,https://www.reddit.com/r/MachineLearning/comments/a3qui0/perfect_example_of_how_to_apply_syntheticdata/,PashaDatsiuk,1544121814,,0,1,False,default,,,,,
366,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3r0s8,science.sciencemag.org,"[N] Show Reddit: AlphaZero Science paper, A general reinforcement learning algorithm that masters chess, shogi and Go through self-play",https://www.reddit.com/r/MachineLearning/comments/a3r0s8/n_show_reddit_alphazero_science_paper_a_general/,Mononofu,1544122817,,1,1,False,default,,,,,
367,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3r3gj,self.MachineLearning,[D] How to recognize GAN-generated faces,https://www.reddit.com/r/MachineLearning/comments/a3r3gj/d_how_to_recognize_gangenerated_faces/,kcimc,1544123247,"https://medium.com/@kcimc/how-to-recognize-fake-ai-generated-images-4d1f6f9a2842

I saw this post [Can you tell if these faces are real or GAN-generated?](https://www.reddit.com/r/MachineLearning/comments/a34ner/p_can_you_tell_if_these_faces_are_real_or/) and wanted to compile some tips for how to distinguish GAN-generated faces. There are a few small things I left out, but I wanted to know if anyone recognized any other big discrepancies.",4,1,False,self,,,,,
368,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3r62u,self.MachineLearning,User Experience Study with Google Cloud,https://www.reddit.com/r/MachineLearning/comments/a3r62u/user_experience_study_with_google_cloud/,gauravkarnataki,1544123675,[removed],0,1,False,self,,,,,
369,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3r7ex,deepmind.com,"[R] AlphaZero: Shedding new light on the grand games of chess, shogi and Go",https://www.reddit.com/r/MachineLearning/comments/a3r7ex/r_alphazero_shedding_new_light_on_the_grand_games/,Kaixhin,1544123892,,0,1,False,https://b.thumbs.redditmedia.com/UeSBdEPo3AI9OWjajTUt0USF_7i-4qE6iZ4AGB20WxE.jpg,,,,,
370,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3rabk,arxiv.org,[R] Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling,https://www.reddit.com/r/MachineLearning/comments/a3rabk/r_generating_high_fidelity_images_with_subscale/,HigherTopoi,1544124368,,10,1,False,default,,,,,
371,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3radd,self.MachineLearning,how long does 1G tweets take in data pre-processing by jupyter? How do I know it's runing?,https://www.reddit.com/r/MachineLearning/comments/a3radd/how_long_does_1g_tweets_take_in_data/,RioChenRio,1544124375,[removed],0,1,False,https://b.thumbs.redditmedia.com/op0mJrJvpRNL8BLzKhLct1a_3PtmZToU9JXLQifuN6U.jpg,,,,,
372,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3rcbj,deepmind.com,"[R] ""AlphaZero: Shedding new light on the grand games of chess, shogi and Go"" [DM releases followup paper on AlphaZero, +100 shogi games, +100 chess games, and video discussion]",https://www.reddit.com/r/MachineLearning/comments/a3rcbj/r_alphazero_shedding_new_light_on_the_grand_games/,gwern,1544124689,,0,1,False,https://b.thumbs.redditmedia.com/UeSBdEPo3AI9OWjajTUt0USF_7i-4qE6iZ4AGB20WxE.jpg,,,,,
373,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3req3,self.MachineLearning,[D] Handling liability issues wrt large datasets obtained online?,https://www.reddit.com/r/MachineLearning/comments/a3req3/d_handling_liability_issues_wrt_large_datasets/,30201102,1544125082,"It's impossible to personally check terabyte+ sized datasets for illegal files (copyrighted materials, banned materials, etc). Even if a dataset comes from a verified source, there are no guarantees it contains what it claims to, so there are always inherent risks. Government entities clearly have methods to detect such downloads, but for obvious reasons it's unlikely they will ever be disclosed. Is there existing documentation or legislation on how such issues should be handled?",3,1,False,self,,,,,
374,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,4,a3ril2,self.MachineLearning,[D] Is Reptile necessarily about finding a good initialization?,https://www.reddit.com/r/MachineLearning/comments/a3ril2/d_is_reptile_necessarily_about_finding_a_good/,abstractcontrol,1544125711,"[Reptile](https://arxiv.org/abs/1803.02999) represents itself very strongly as a first order alternative to MAML, but since it has the hyperlearning rate which interpolates between old and new weights unlike MAML which takes discrete steps, I am wondering whether its true potential has been overlooked. Since with Reptile it becomes possible to smoothly tilt the update towards the new weights, I am wondering if by setting a high hyperlearning rate - something like 10% old weights and 90% new weights one would get the best of the both worlds. Namely, no separation between metatraining and training which is a weakness of MAML, and high capacity for transfer learning and resistance to catastrophic forgetting.

This is just a though and I could very well be wrong. I am just wondering whether this has been investigated?",13,1,False,self,,,,,
375,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,5,a3rqwp,languagelog.ldc.upenn.edu,Language Log  Deep learning stumbles again,https://www.reddit.com/r/MachineLearning/comments/a3rqwp/language_log_deep_learning_stumbles_again/,outofusernams,1544127054,,1,1,False,default,,,,,
376,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,5,a3rvtv,self.MachineLearning,[D] What are essential Python ML libraries?,https://www.reddit.com/r/MachineLearning/comments/a3rvtv/d_what_are_essential_python_ml_libraries/,LastCell7,1544127859,,0,1,False,self,,,,,
377,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,5,a3rvu4,self.MachineLearning,[N] ARTificial - a visual art exhibit - Montral 6-7 Dc 2018 - Espace POP,https://www.reddit.com/r/MachineLearning/comments/a3rvu4/n_artificial_a_visual_art_exhibit_montral_67_dc/,artificialmtl,1544127860,"Hello again !  


So after a successful call for contributions (thanks Reddit!), we are glad to announce that ARTificial is currently open to the public at its Espace POP location.

&amp;#x200B;

If you are in Montreal, you should definitely visit - we're in the Mile End area, close to the corner of Park Avenue and Saint-Viateur.

&amp;#x200B;

For more information including a list of the artists presenting work, jump to our website: [artificial.st](https://artificial.st) or on social media: @artificialmtl.

&amp;#x200B;

See you soon !",1,1,False,self,,,,,
378,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,5,a3rx9o,self.MachineLearning,[D] Why is filter grouping hardly ever used in convolution layers?,https://www.reddit.com/r/MachineLearning/comments/a3rx9o/d_why_is_filter_grouping_hardly_ever_used_in/,KimTaylorC,1544128104, All popular frameworks support the groups parameter in convolution layers. But you can hardly see it being used anywhere. Why is it like that? ,12,1,False,self,,,,,
379,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,5,a3s08h,self.MachineLearning,[N] OpenAI CoinRun env: Quantifying Generalization in Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a3s08h/n_openai_coinrun_env_quantifying_generalization/,modernrl,1544128587,[removed],0,1,False,self,,,,,
380,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,6,a3s8q2,self.MachineLearning,[N] Standardizing on Keras: Guidance on High-level APIs in TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/a3s8q2/n_standardizing_on_keras_guidance_on_highlevel/,KimTaylorC,1544130012,"[https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)

""TensorFlow is preparing for the release of version 2.0.  In this article, we want to preview the direction TensorFlows  high-level APIs are heading, and answer some frequently asked questions.""",0,1,False,self,,,,,
381,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,6,a3sagw,rossum.ai,Peeking into the Neural Network Black Box,https://www.reddit.com/r/MachineLearning/comments/a3sagw/peeking_into_the_neural_network_black_box/,thonic,1544130299,,0,1,False,default,,,,,
382,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,6,a3sf54,self.MachineLearning,"[D] For those how are involving in hiring and interviewing candidates for ML positions, what are some resources for developing interviews for ML positions ?",https://www.reddit.com/r/MachineLearning/comments/a3sf54/d_for_those_how_are_involving_in_hiring_and/,AdditionalWay,1544131059,"Initially I was going to ask for Tensorflow developers specifically but why not open it up to all positions related to ML?

I see lots of resources for handling ML interviews, but how about for conducting ML interviews? How do we know the candidate will be able to perform to the position? What are ways to gain insights to a candidate's competence? ",12,1,False,self,,,,,
383,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,6,a3sv2o,self.MachineLearning,"[R] AlphaZero: Shedding new light on the grand games of chess, shogi and Go",https://www.reddit.com/r/MachineLearning/comments/a3sv2o/r_alphazero_shedding_new_light_on_the_grand_games/,P4TR10T_TR41T0R,1544133597,"A new blog post by DeepMind ([link here](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/)), regarding the paper published on Science ([link](http://science.sciencemag.org/content/362/6419/1140)). It provides a new examination of the AlphaZero algorithm. Here is another [popscience article](https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/mb) written by P.E. Ross for IEEE.",19,1,False,self,,,,,
384,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,9,a3ueu9,youtu.be,Musician Experimenting with Googles Magenta Studio Plug-ins to apply Machine Learning to Music Composition,https://www.reddit.com/r/MachineLearning/comments/a3ueu9/musician_experimenting_with_googles_magenta/,dtizzlenizzle,1544143303,,0,1,False,default,,,,,
385,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,11,a3v4h9,self.MachineLearning,How can I create this?,https://www.reddit.com/r/MachineLearning/comments/a3v4h9/how_can_i_create_this/,WarAndGeese,1544148182,[removed],0,1,False,self,,,,,
386,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,11,a3vkpv,self.MachineLearning,What will happen in unbalance training with replacement sampling?,https://www.reddit.com/r/MachineLearning/comments/a3vkpv/what_will_happen_in_unbalance_training_with/,simon_nada425,1544151314,[removed],0,1,False,self,,,,,
387,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,12,a3vok8,youtube.com,Open AI Challenge Tanzania Winners: Anthropocene Labs,https://www.reddit.com/r/MachineLearning/comments/a3vok8/open_ai_challenge_tanzania_winners_anthropocene/,imitationcheese,1544152035,,0,1,False,https://b.thumbs.redditmedia.com/EiYauDgJfqg0A9rKX2yvLbMWy0qt8LcBExMp5AdzmKs.jpg,,,,,
388,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,12,a3vpi0,self.MachineLearning,What will happen in unbalance training with replacement sampling?,https://www.reddit.com/r/MachineLearning/comments/a3vpi0/what_will_happen_in_unbalance_training_with/,simon_nada425,1544152209,[removed],0,1,False,self,,,,,
389,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,12,a3vy12,self.MachineLearning,How to properly split data set for training and testing,https://www.reddit.com/r/MachineLearning/comments/a3vy12/how_to_properly_split_data_set_for_training_and/,jordanl12345,1544153909,[removed],0,1,False,self,,,,,
390,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,12,a3vydh,self.MachineLearning,What are the various machine learning classification algorithms in sklearn that I could try for multivariate classification?,https://www.reddit.com/r/MachineLearning/comments/a3vydh/what_are_the_various_machine_learning/,FixMyEnglish,1544153977,[removed],0,1,False,self,,,,,
391,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,12,a3w0fn,manaliescort.in,luxury | Manali Escorts | 7807700147 | Call Girl in Manali,https://www.reddit.com/r/MachineLearning/comments/a3w0fn/luxury_manali_escorts_7807700147_call_girl_in/,christinyrbraug,1544154385,,0,1,False,default,,,,,
392,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,12,a3w3qa,self.MachineLearning,How do you know when you're done with a dataset,https://www.reddit.com/r/MachineLearning/comments/a3w3qa/how_do_you_know_when_youre_done_with_a_dataset/,BrokenWineGlass,1544155049,[removed],0,1,False,self,,,,,
393,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,13,a3wfj6,self.MachineLearning,Automatic Tensor shape evaluation for numpy and pytorch,https://www.reddit.com/r/MachineLearning/comments/a3wfj6/automatic_tensor_shape_evaluation_for_numpy_and/,tobyclh,1544157372,"[https://github.com/shiba6v/shape\_commentator](https://github.com/shiba6v/shape_commentator)

A plugin (that I am no mean affiliated with) that automatically output shapes of numpy and pytorch tensor. 

Seems extremely useful for prototyping.

There is another [writeup in Japanese](http://shiba6v.hatenablog.com/entry/shape_commentator_release) on this plugin but I guess you can simply dive in even if you don't read it.",0,1,False,self,,,,,
394,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,13,a3wh4s,self.MachineLearning,[D] Is association rule learning still a thing?,https://www.reddit.com/r/MachineLearning/comments/a3wh4s/d_is_association_rule_learning_still_a_thing/,RudyWurlitzer,1544157686,"The algorithms of association rule learning (or mining) like Apriori and FP Growth, used to find patterns in buyers' transactions, are they still a thing or they were replaced by recommender systems (collaborative filtering) or something else?",3,1,False,self,,,,,
395,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,14,a3wr2a,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a3wr2a/global_machine_learning_market_size_outlook/,nhemaei,1544159699,"The machine learning holds the highest CAGR of 44.86% during the forecast period 2019-2025. 

&amp;#x200B;

Request a sample @ https://www.envisioninteligence.com/industry-report/global-machine-learning-market/?utm\_source=reddit-hema ",0,1,False,self,,,,,
396,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,14,a3wtns,self.MachineLearning,Searching for More things like this...,https://www.reddit.com/r/MachineLearning/comments/a3wtns/searching_for_more_things_like_this/,Ryusho,1544160254,[removed],0,1,False,self,,,,,
397,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,14,a3wwox,self.MachineLearning,Questions on Caffe2,https://www.reddit.com/r/MachineLearning/comments/a3wwox/questions_on_caffe2/,AgnosticIsaac,1544160909,[removed],0,1,False,self,,,,,
398,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,15,a3x9i7,self.MachineLearning,Do you recommend me jumping to DeepLeaning.ai,https://www.reddit.com/r/MachineLearning/comments/a3x9i7/do_you_recommend_me_jumping_to_deepleaningai/,H4zardd,1544163875,"Hi! I completed 1-4 weeks of ML course at Coursera by Andrew Ng, doing all the Octave exercises. Before going to week 5 where he starts going deep on Neural Networks, should I jump to [DeepLearning.ai](https://DeepLearning.ai)? His new course? Because so far I have a bit of knowledge about Python and ML but I have no clue about implementing my own solutions in Jupyter Notebook.",0,1,False,self,,,,,
399,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,15,a3xgqh,stateoftheart.ai,easily consultable repository for state of the art ML tasks - StateOfTheArt.ai,https://www.reddit.com/r/MachineLearning/comments/a3xgqh/easily_consultable_repository_for_state_of_the/,gitamar,1544165671,,0,1,False,default,,,,,
400,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,15,a3xhdi,self.MachineLearning,Transfer learning in NLP,https://www.reddit.com/r/MachineLearning/comments/a3xhdi/transfer_learning_in_nlp/,textMinier,1544165828,[removed],0,1,False,self,,,,,
401,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,16,a3xl1c,youtube.com,AI Atelier using Internet Image Montage and Interactive Style Transfer,https://www.reddit.com/r/MachineLearning/comments/a3xl1c/ai_atelier_using_internet_image_montage_and/,kh22l22,1544166723,,0,1,False,default,,,,,
402,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,16,a3xpav,youtu.be,AI Atelier using Internet Image Montage and Interactive Style Transfer,https://www.reddit.com/r/MachineLearning/comments/a3xpav/ai_atelier_using_internet_image_montage_and/,kh22l22,1544167833,,0,1,False,https://b.thumbs.redditmedia.com/ms2kQ4qVlfReONdCCI0DGKxAOrjheoGj-rrPruwb5FQ.jpg,,,,,
403,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,16,a3xsog,gengo.ai,How distribution shift defines the limits of machine learning: Interview with Professor Zachary Lipton,https://www.reddit.com/r/MachineLearning/comments/a3xsog/how_distribution_shift_defines_the_limits_of/,reimmoriks,1544168777,,0,1,False,default,,,,,
404,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,16,a3xve6,medium.com,Phone cameras are going through a great revolution!!! - How Cameras Are Learning To See,https://www.reddit.com/r/MachineLearning/comments/a3xve6/phone_cameras_are_going_through_a_great/,UmbalaNetwork,1544169568,,0,1,False,https://a.thumbs.redditmedia.com/IiBHX-YCU84EqU2-BB3GjhQFWEtcC2FkqptEUEJhkU0.jpg,,,,,
405,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,17,a3xyfj,self.MachineLearning,Why python-based third party modules have so many bugs?,https://www.reddit.com/r/MachineLearning/comments/a3xyfj/why_pythonbased_third_party_modules_have_so_many/,Llewyn_Wan,1544170477,[removed],0,1,False,self,,,,,
406,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,17,a3xzpg,self.MachineLearning,[P] Test,https://www.reddit.com/r/MachineLearning/comments/a3xzpg/p_test/,kh22l22,1544170887,test,0,1,False,self,,,,,
407,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,17,a3y0b1,self.MachineLearning,[P] A.I. Atelier using Internet Image Montage and Interactive Style Transfer,https://www.reddit.com/r/MachineLearning/comments/a3y0b1/p_ai_atelier_using_internet_image_montage_and/,kh22l22,1544171083,[AI Atelier Demo](https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=79s),0,1,False,self,,,,,
408,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,17,a3y0p5,youtube.com,[P] A.I. Atelier using internet image montage and interactive style transfer,https://www.reddit.com/r/MachineLearning/comments/a3y0p5/p_ai_atelier_using_internet_image_montage_and/,kh22l22,1544171208,,0,1,False,https://b.thumbs.redditmedia.com/hFZNr_dRS7bMUEJajbnHoYlWTZbs1XTLOK1unkCtx5g.jpg,,,,,
409,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,17,a3y10r,self.MachineLearning,[P] test,https://www.reddit.com/r/MachineLearning/comments/a3y10r/p_test/,kh22l22,1544171319,test,0,1,False,self,,,,,
410,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,17,a3y1p3,self.MachineLearning,[P] A.I Atelier,https://www.reddit.com/r/MachineLearning/comments/a3y1p3/p_ai_atelier/,kh22l22,1544171521,"using Internet Image Montage and Interactive Style Transfer

[https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=79s](https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=79s)",0,1,False,self,,,,,
411,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,17,a3y5ep,self.MachineLearning,[R] Making Classification Competitive for Deep Metric Learning,https://www.reddit.com/r/MachineLearning/comments/a3y5ep/r_making_classification_competitive_for_deep/,melgor89,1544172682,"I would say that this time come finally with this paper:

[https://arxiv.org/abs/1811.12649](https://arxiv.org/abs/1811.12649)

It describe methods (ex. Angular Loss) which was created for Face-Recognition used for general Metric-Learning or Image-Retrieval. For me  it was such obvious that this Face-Recognition and Image-Retrieval are exactly the same task. In fact, in meanwhile I'm creating a library which could be used for general 'Metric-Learning' stuff (but now my pace is really low as my son came into this world).   


In general, it is very interesting that the Normalized Softmax (which is softmax where input and weights are normalized using L2) in even better than any triplet/proxy method without any drawbacks ( maybe just we need to subsample classification layer if more than 100k classes). This make all process a lot easier, no need of specific sampling etc. And in PyTorch it is like 2-lines of code compared to normal classification learning.

&amp;#x200B;

Do you have ever used Normalized-SoftMax?

What do you think about idea of using Cosine-Product in Linear Layers (cosine because we multiply normalized vectors)?

Would you like to have a general Metric-Learning library for all tasks (I hope the authors would release the code, then every thing would be ready)

&amp;#x200B;

&amp;#x200B;",10,1,False,self,,,,,
412,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,18,a3y93n,self.MachineLearning,[N] Standardizing on Keras: Guidance on High-level APIs in TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/a3y93n/n_standardizing_on_keras_guidance_on_highlevel/,omoindrot,1544173817,"An update from the TensorFlow team about the high-level APIs in TF 2.0.

**TL;DR:** everyone should move to `tf.keras` (even people using `tf.estimator`).",41,1,False,self,,,,,
413,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,18,a3yidm,eleks.com,How Augmented Analytics can automate resource-intensive data analysis tasks.,https://www.reddit.com/r/MachineLearning/comments/a3yidm/how_augmented_analytics_can_automate/,Victor_Stakh,1544176550,,0,1,False,default,,,,,
414,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,18,a3yix2,i.redd.it,Artificial Intelligence Online Training!,https://www.reddit.com/r/MachineLearning/comments/a3yix2/artificial_intelligence_online_training/,ZenithTrainings,1544176708,,0,1,False,default,,,,,
415,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,19,a3ykzf,self.MachineLearning,[R] The Illustrated BERT and ELMo (How NLP Cracked Transfer Learning),https://www.reddit.com/r/MachineLearning/comments/a3ykzf/r_the_illustrated_bert_and_elmo_how_nlp_cracked/,nortab,1544177282," 

This is my best attempt visually explaining BERT, ELMo, and the OpenAI transformer. 



This can be thought of as a follow-up to my earlier post [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/), as BERT builds up on top of the Transformer architecture.

&amp;#x200B;

All feedback is appreciated. 

&amp;#x200B;

[https://jalammar.github.io/illustrated-bert/](https://jalammar.github.io/illustrated-bert/)",20,1,False,self,,,,,
416,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,19,a3yn2y,self.MachineLearning,How many people on your data science team are actually training/developing models?,https://www.reddit.com/r/MachineLearning/comments/a3yn2y/how_many_people_on_your_data_science_team_are/,ai_yoda,1544177870,,0,1,False,self,,,,,
417,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,19,a3yqd7,self.MachineLearning,Digital Disruptor | The Digital Enterprise,https://www.reddit.com/r/MachineLearning/comments/a3yqd7/digital_disruptor_the_digital_enterprise/,thedigitalenterprise,1544178836,[removed],0,1,False,self,,,,,
418,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,19,a3yskf,linkedin.com,Best AI and Machine Learning Tools For Developers:,https://www.reddit.com/r/MachineLearning/comments/a3yskf/best_ai_and_machine_learning_tools_for_developers/,tech-info,1544179494,,0,1,False,default,,,,,
419,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,19,a3yskw,self.MachineLearning,Amazon selects its workforce,https://www.reddit.com/r/MachineLearning/comments/a3yskw/amazon_selects_its_workforce/,Kevin_Clever,1544179500,[removed],0,1,False,self,,,,,
420,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,19,a3ytbh,self.MachineLearning,"On a team working on machine learning model development/training, how is the team leader involved in the process on a daily basis?",https://www.reddit.com/r/MachineLearning/comments/a3ytbh/on_a_team_working_on_machine_learning_model/,ai_yoda,1544179696,[removed],0,1,False,self,,,,,
421,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,21,a3z938,youtu.be,[P] Neural network visualization in Pixling World,https://www.reddit.com/r/MachineLearning/comments/a3z938/p_neural_network_visualization_in_pixling_world/,FredrikNoren,1544184151,,0,1,False,https://b.thumbs.redditmedia.com/SMAoF_Xpj8ByZu5bq8g6W9f3oJLpN-wTFkMdSfP_hzA.jpg,,,,,
422,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,21,a3zb35,self.MachineLearning,"How to approach training a network to detect fast, small and featureless object in a video sequence ?",https://www.reddit.com/r/MachineLearning/comments/a3zb35/how_to_approach_training_a_network_to_detect_fast/,RavlaAlvar,1544184651,[removed],0,1,False,self,,,,,
423,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,21,a3zke1,youtu.be,GOTO 2018  On the Road to Artificial General Intelligence  Danny Lange,https://www.reddit.com/r/MachineLearning/comments/a3zke1/goto_2018_on_the_road_to_artificial_general/,rick-rebel,1544186983,,0,1,False,https://b.thumbs.redditmedia.com/uVj7x3ZGuXWU0lpN7I23UUTB6eO1QgbEDufclmJF-vA.jpg,,,,,
424,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,22,a3znb6,self.MachineLearning,[D] Is it possible to use object detection on obscured/blurred images?,https://www.reddit.com/r/MachineLearning/comments/a3znb6/d_is_it_possible_to_use_object_detection_on/,Nimda_lel,1544187686,"Hello /r/MachineLearning,

&amp;#x200B;

I am new to ML and I was assigned a task to make a decent Object Detection model to work on Google Vision Kit ([https://aiyprojects.withgoogle.com/vision](https://aiyprojects.withgoogle.com/vision)), but it seems really hard to find a decent object detection model that would run on that Device and produce decent results (I am only trying to detect people).

&amp;#x200B;

As a fallback plan I thought I could use AWS ML Optimized machines. It is all fine except the fact that the Pictures might be confidental, i.e. I want to detect only how many people are on the image rather than seeing the people themselves.

&amp;#x200B;

I was wondering, is it possible to apply some kind of filter on the image and still use ML to detect just the number of people on the image?

  
In theory that should be possible, but does anybody have any example/tutorial/post or something that might be related?

&amp;#x200B;

NOTE: I prefer using Tensorflow and I am also open to ideas about optimized Object Detection Models that only detect people.",6,1,False,self,,,,,
425,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,22,a3zq49,github.com,I just stumbled upon this AWESOME machine learning cheat sheet series. Thank you Stanford!,https://www.reddit.com/r/MachineLearning/comments/a3zq49/i_just_stumbled_upon_this_awesome_machine/,code_x_7777,1544188263,,0,1,False,default,,,,,
426,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,22,a3zwfq,self.LanguageTechnology,An implementation guide to Word2Vec using NumPy and Google Sheets,https://www.reddit.com/r/MachineLearning/comments/a3zwfq/an_implementation_guide_to_word2vec_using_numpy/,rainboiboi,1544189588,,0,1,False,https://b.thumbs.redditmedia.com/jbwALWuNF7Y5_GoMFcqwmb0cxLIpW2YG6GfAxPvkAhU.jpg,,,,,
427,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,22,a3zx08,facebook.com,DECEMBER DEAL The Complete SQL Bootcamp DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/a3zx08/december_deal_the_complete_sql_bootcamp_discount/,TieshaJauregui,1544189706,,0,1,False,default,,,,,
428,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,23,a408ef,facebook.com,DECEMBER DEAL Data Science and Machine Learning Bootcamp with R DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/a408ef/december_deal_data_science_and_machine_learning/,LizzieRoux,1544191981,,0,1,False,default,,,,,
429,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,23,a40kuv,self.MachineLearning,Machine learning without a lot of data,https://www.reddit.com/r/MachineLearning/comments/a40kuv/machine_learning_without_a_lot_of_data/,angelinux74,1544194387,"Hello, I am searching some good starting point to study how to apply machine learning to small dataset. Could you please provide some reference? Thanks.",0,1,False,self,,,,,
430,MachineLearning,t5_2r3gv,2018-12-7,2018,12,7,23,a40lj3,blog.floydhub.com,Spinning Up a Pong AI With Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a40lj3/spinning_up_a_pong_ai_with_deep_reinforcement/,pirate7777777,1544194517,,0,1,False,https://a.thumbs.redditmedia.com/n1YI2B-4RAkLZoZIlr0cbM-B9G8mpKicH71xYa9w-58.jpg,,,,,
431,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,0,a40n2h,self.MachineLearning,Pretrained Version of EnhanceNet?,https://www.reddit.com/r/MachineLearning/comments/a40n2h/pretrained_version_of_enhancenet/,DieEeneGast,1544194817,[removed],0,1,False,self,,,,,
432,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,0,a40t7d,self.MachineLearning,GPU for time-series training rather than images training?,https://www.reddit.com/r/MachineLearning/comments/a40t7d/gpu_for_timeseries_training_rather_than_images/,shallow_share,1544195988,"I am building a computer on a budget and I'm not sure that I need a GTX 1060 / GTX 1070 / GTX 1080 because most people who recommend them are doing training on images. Those GPU's are pricey ($250 - $800) and if I need them I want to be sure that I'm not buying it when something cheaper would suffice. I will be doing training on time-series with like 50-100 features and like 10 million rows, while the neural-network itself will be like 10 layers deep at most. What GPU specs should I be looking for? 

* Bandwith? 
* Memory? 
* Clock Speed? 
* Something else? ",0,1,False,self,,,,,
433,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,0,a40vdt,self.MachineLearning,[N] Community-driven Website for Current State-of-the-arts in several fields of AI. Contribute and help it grow!,https://www.reddit.com/r/MachineLearning/comments/a40vdt/n_communitydriven_website_for_current/,ZER_0_NE,1544196415,[removed],0,1,False,self,,,,,
434,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,0,a40ygo,codecampanion.blogspot.com,Genetic Algorithm - Explained | Applications &amp; Example,https://www.reddit.com/r/MachineLearning/comments/a40ygo/genetic_algorithm_explained_applications_example/,AshishKhuraishy,1544196971,,0,1,False,https://b.thumbs.redditmedia.com/giTULeytEnrmrND_Aqf_4BnlXTNbbygS85Nvk10VWJo.jpg,,,,,
435,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,0,a40zug,self.MachineLearning,"From your experience in working on data science projects, how important is model quality/accuracy? How often getting 1% improvement is actually worth it?",https://www.reddit.com/r/MachineLearning/comments/a40zug/from_your_experience_in_working_on_data_science/,ai_yoda,1544197238,[removed],0,1,False,self,,,,,
436,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,0,a411qf,ai.googleblog.com,Adding Diversity to Images with Open Images Extended,https://www.reddit.com/r/MachineLearning/comments/a411qf/adding_diversity_to_images_with_open_images/,sjoerdapp,1544197562,,0,1,False,default,,,,,
437,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,1,a41bjk,self.MachineLearning,"[D] What goes onto your ""Best of"" list from r/MachineLearning for 2018",https://www.reddit.com/r/MachineLearning/comments/a41bjk/d_what_goes_onto_your_best_of_list_from/,jamesonatfritz,1544199300,"It's not over yet and we've still got a bunch of really cool stuff coming out of NeurIPS, but we went through some of the top posts so far in 2018 and there were so many cool projects I had forgotten about.

&amp;#x200B;

\[Best of Machine Learning in 2018: Reddit Edition\]([https://heartbeat.fritz.ai/best-of-machine-learning-in-2018-reddit-edition-7f517dfd0bc3](https://heartbeat.fritz.ai/best-of-machine-learning-in-2018-reddit-edition-7f517dfd0bc3))

&amp;#x200B;

What's your vote for best project this year?",6,1,False,self,,,,,
438,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,1,a41kj9,self.MachineLearning,Does any of you couldn't attend NeurIPS because of visa issues?,https://www.reddit.com/r/MachineLearning/comments/a41kj9/does_any_of_you_couldnt_attend_neurips_because_of/,pikachuchameleon,1544200870,"I am a graduate student who was supposed to attend the NeurIPS conference this year. Unfortunately, the Canadian visa process has been extremely slow and they haven't even returned my  passport yet. Even after numerous calls and emails, there has been no cooperation so far. For a conference that big, I feel that there should be a mechanism to make the whole visa process student friendly. Especially, many international students in USA have been affected this year as they just changed the visa offices in NY and LA on Nov. 1 and the whole process has been a horrible mess. I can't even imagine how hard it might be for students outside USA.

 So I am wondering if there is anyone out there who couldn't attend because of similar issues? I am also wondering if I can ask for a refund of the conference registration fee since this issue is common to a lot of us.

Also, to avoid similar issues in the future, I feel that if some top ML researchers take a stand and convey it to the relevant authorities about these visa issues while holding the conference in the next few years, it would be a great step towards making it student friendly. Right now, even getting a registration ticket is a big deal. On top of that, the Canadian visa authorities wanted it to make it even harder for anyone to attend. The whole process is so discouraging!",0,1,False,self,,,,,
439,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,2,a41s1z,self.MachineLearning,RL- Saturating problem - Support needed,https://www.reddit.com/r/MachineLearning/comments/a41s1z/rl_saturating_problem_support_needed/,Jandevries101,1544202152,[removed],0,1,False,self,,,,,
440,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,2,a41t3k,self.MachineLearning,How many machine learning experiments (retraining models doesn't count) does your team run in a month?,https://www.reddit.com/r/MachineLearning/comments/a41t3k/how_many_machine_learning_experiments_retraining/,ai_yoda,1544202315,,0,1,False,self,,,,,
441,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,2,a42c1f,self.MachineLearning,Low Ranking Approximation,https://www.reddit.com/r/MachineLearning/comments/a42c1f/low_ranking_approximation/,machinescientist,1544205591,"Hi there everyone!

I'm given 200 data points and 20 features, and I'm asked to do a low ranking approximation and reduce the number of feature to 10.  Then do PCA and reduce the number of features from 10 to 5.

Can anyone tell me the steps on how to do it?

Thanks! :)",0,1,False,self,,,,,
442,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,3,a42da5,self.MachineLearning,"As a data scientist, how much time on average do you spend on tasks that are not directly related to improving machine learning models?",https://www.reddit.com/r/MachineLearning/comments/a42da5/as_a_data_scientist_how_much_time_on_average_do/,ai_yoda,1544205797,[removed],0,1,False,self,,,,,
443,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,3,a42lrj,mlwhiz.com,[D] Feature building techniques for your next ML project,https://www.reddit.com/r/MachineLearning/comments/a42lrj/d_feature_building_techniques_for_your_next_ml/,mlwhiz,1544207278,,0,1,False,default,,,,,
444,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,3,a42mer,self.MachineLearning,Could you give me a paper on the high-performance Named Entity Recognition?,https://www.reddit.com/r/MachineLearning/comments/a42mer/could_you_give_me_a_paper_on_the_highperformance/,thisisiron,1544207393,[removed],0,1,False,self,,,,,
445,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,3,a42qms,self.MachineLearning,Best tutorial Sequence-to-Sequence learning.,https://www.reddit.com/r/MachineLearning/comments/a42qms/best_tutorial_sequencetosequence_learning/,vivekkarn,1544208136,[removed],0,1,False,self,,,,,
446,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,4,a4376h,github.com,Pytorch 1.0 released,https://www.reddit.com/r/MachineLearning/comments/a4376h/pytorch_10_released/,nicoulaj,1544211038,,0,1,False,https://b.thumbs.redditmedia.com/GRx3y3JbOXC6mDB24S10ngNlCzwkbqzG-Fp50IeaOjs.jpg,,,,,
447,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,4,a43ain,self.MachineLearning,Pytorch 1.0.0 released,https://www.reddit.com/r/MachineLearning/comments/a43ain/pytorch_100_released/,nicoulaj,1544211647,[removed],0,1,False,self,,,,,
448,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,4,a43ek0,self.MachineLearning,[P] trickster: Library and experiments for attacking machine learning in discrete domains,https://www.reddit.com/r/MachineLearning/comments/a43ek0/p_trickster_library_and_experiments_for_attacking/,hidden-markov,1544212355,"Github: https://github.com/spring-epfl/trickster
Docs: https://trickster-lib.readthedocs.io
Guide: https://trickster-lib.readthedocs.io/en/latest/guide.html
Paper: https://arxiv.org/abs/1810.10939

trickster allows to run the following kind of attack. Starting with a given correctly classified example, apply a sequence of semantics-preserving transformations to it until the changed version of the example causes a missclassification. This allows to only consider valid transformations when the space is constrained: text, hand-crafted features.",0,1,False,self,,,,,
449,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,4,a43gla,self.MachineLearning,[P] MLG - Visual Machine Learning Graph of all arXiv papers and researchers,https://www.reddit.com/r/MachineLearning/comments/a43gla/p_mlg_visual_machine_learning_graph_of_all_arxiv/,ranihorev,1544212713,"Hey everyone!

Ive been working recently on a project that Id love to share with you:

[https://arxiv.lyrn.ai/network](https://arxiv.lyrn.ai/network)

MLG is a visual representation of ML researchers and papers from arXiv from the last year (for now). Each node in the graph is an authors and the edges represent co-authorship of papers. The purpose of the project was to find the most interesting papers (and authors) in order to summarize them but I thought some people might find it useful as well.

MLG allows you to:

1. Search for papers or authors.
2. Filter by topics (NLP, Vision, etc).
3. Focus on a specific author and explore his/her neighbors gradually.

Id love to get ideas for features and use cases. The code can be found [here](https://github.com/ranihorev/arxiv-network-graph).

Currently in progress:

1. Improving the rendering performance.
2. Adding references as another layer of the graph.",26,1,False,self,,,,,
450,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,5,a43khg,code.fb.com,"[N] PyTorch developer ecosystem expands, 1.0 stable release now available",https://www.reddit.com/r/MachineLearning/comments/a43khg/n_pytorch_developer_ecosystem_expands_10_stable/,dayanruben,1544213372,,0,1,False,default,,,,,
451,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,6,a443fo,self.MachineLearning,[N] PyTorch v1.0 stable release,https://www.reddit.com/r/MachineLearning/comments/a443fo/n_pytorch_v10_stable_release/,crypto_ha,1544216753,"[JIT Compiler, Faster Distributed, C++ Frontend](https://github.com/pytorch/pytorch/releases/tag/v1.0.0) (github.com)

[PyTorch developer ecosystem expands, 1.0 stable release now available](https://code.fb.com/ai-research/pytorch-developer-ecosystem-expands-1-0-stable-release) (code.fb.com)",84,1,False,self,,,,,
452,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,6,a443nu,github.com,PyTorch 1.0 Released,https://www.reddit.com/r/MachineLearning/comments/a443nu/pytorch_10_released/,Deepblue129,1544216791,,0,1,False,default,,,,,
453,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,6,a4441b,code.fb.com,PyTorch 1.0 Release - Blog Post,https://www.reddit.com/r/MachineLearning/comments/a4441b/pytorch_10_release_blog_post/,Deepblue129,1544216854,,0,1,False,default,,,,,
454,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,6,a444vr,code.fb.com,"PyTorch 1.0 Stable Released, at the NeurIPS!",https://www.reddit.com/r/MachineLearning/comments/a444vr/pytorch_10_stable_released_at_the_neurips/,Deepblue129,1544217006,,0,1,False,https://a.thumbs.redditmedia.com/-MrWwUx_b_mvDnpjcRRcFROIjGDyaB3br7Juot2cME4.jpg,,,,,
455,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,6,a445rx,code.fb.com,"[N] PyTorch 1.0 Stable Released, at the NeurIPS!",https://www.reddit.com/r/MachineLearning/comments/a445rx/n_pytorch_10_stable_released_at_the_neurips/,Deepblue129,1544217158,,0,1,False,https://a.thumbs.redditmedia.com/-MrWwUx_b_mvDnpjcRRcFROIjGDyaB3br7Juot2cME4.jpg,,,,,
456,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,6,a4499e,medium.com,"[P] Deep Graph Library, a Python Package For Graph Neural Networks",https://www.reddit.com/r/MachineLearning/comments/a4499e/p_deep_graph_library_a_python_package_for_graph/,gwen0927,1544217794,,0,1,False,default,,,,,
457,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,6,a44i1v,self.MachineLearning,First AI Blog Post,https://www.reddit.com/r/MachineLearning/comments/a44i1v/first_ai_blog_post/,Daniyal9538,1544219375,[removed],0,1,False,self,,,,,
458,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,7,a452wh,arxiv.org,[R] Towards Accurate Generative Models of Video: A New Metric &amp; Challenges,https://www.reddit.com/r/MachineLearning/comments/a452wh/r_towards_accurate_generative_models_of_video_a/,downtownslim,1544223139,,0,1,False,default,,,,,
459,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,7,a4544c,self.MachineLearning,GpuHub. Mass providing GPU-power for machine learning,https://www.reddit.com/r/MachineLearning/comments/a4544c/gpuhub_mass_providing_gpupower_for_machine/,gpuhub,1544223365,[removed],0,1,False,self,,,,,
460,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,8,a457p2,self.MachineLearning,[D] GpuHub. Mass providing GPU-power for machine learning,https://www.reddit.com/r/MachineLearning/comments/a457p2/d_gpuhub_mass_providing_gpupower_for_machine/,gpuhub,1544224007,"[https://www.youtube.com/watch?v=LgfNKd-cWMI](https://www.youtube.com/watch?v=LgfNKd-cWMI)

GpuHub introduces schema, which allow to consume ML significantly cheaper.  


Article, describing details

[https://telegra.ph/GpuHub-Cybercortex-12-07](https://telegra.ph/GpuHub-Cybercortex-12-07)",0,1,False,self,,,,,
461,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,8,a45b4f,self.MachineLearning,"predict weather by dqn model, it is possible ?",https://www.reddit.com/r/MachineLearning/comments/a45b4f/predict_weather_by_dqn_model_it_is_possible/,asda43asdf23423,1544224663,"how to set  'action' ?     only one choice,  no others ?",0,1,False,self,,,,,
462,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,11,a46qat,self.MachineLearning,AWS Machine Learning Specialty Certification Help,https://www.reddit.com/r/MachineLearning/comments/a46qat/aws_machine_learning_specialty_certification_help/,solomonline,1544235674,[removed],0,1,False,self,,,,,
463,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,11,a46t4k,github.com,"JAX: GPU-backed NumPy with differentiation, JIT compilation and auto-vectorization",https://www.reddit.com/r/MachineLearning/comments/a46t4k/jax_gpubacked_numpy_with_differentiation_jit/,alexbwww,1544236339,,1,1,False,default,,,,,
464,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,11,a46vx3,/r/MachineLearning/comments/a46vx3/gradient_descent/,Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/a46vx3/gradient_descent/,Bypasser8,1544237004,,0,1,False,default,,,,,
465,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,13,a47iyv,developer.nvidia.com,[N] NVIDIA Tesla Deep Learning Product Performance MXNet vs Pytorch vs Tensorflow,https://www.reddit.com/r/MachineLearning/comments/a47iyv/n_nvidia_tesla_deep_learning_product_performance/,thomasdlt,1544242551,,0,1,False,default,,,,,
466,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,14,a481lb,self.MachineLearning,"[D] Now that cryptocurrencies have fallen, where can I (or should I) buy cheap GPU's?",https://www.reddit.com/r/MachineLearning/comments/a481lb/d_now_that_cryptocurrencies_have_fallen_where_can/,warproxxx,1544247190,"The price of crypto currencies have plunged recently and I would believe the price of GPU has fallen too in the secondary market. 

Should I buy GPU at this time? Are GPU's used for mining inferior for some reason (damage)? Where can I buy such cheap GPU's?",27,1,False,self,,,,,
467,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,14,a483qo,self.MachineLearning,"Machine Vision Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/a483qo/machine_vision_market_size_outlook_trends_and/,srikavyaa,1544247736,[removed],0,1,False,self,,,,,
468,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,15,a4886k,self.MachineLearning,Pyro 0.3.0 Was Released,https://www.reddit.com/r/MachineLearning/comments/a4886k/pyro_030_was_released/,AnyPolicy,1544248911,[removed],0,1,False,self,,,,,
469,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,15,a48juv,self.MachineLearning,Still relevant - Hands-On Machine Learning with Scikit-Learn &amp; Tensorflow first edition?,https://www.reddit.com/r/MachineLearning/comments/a48juv/still_relevant_handson_machine_learning_with/,daredevildas,1544251986,[removed],0,1,False,self,,,,,
470,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,16,a48npg,self.MachineLearning,reinforcement learning in nlp,https://www.reddit.com/r/MachineLearning/comments/a48npg/reinforcement_learning_in_nlp/,avinashmadasu,1544253055,[removed],0,1,False,self,,,,,
471,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,16,a48pq5,self.MachineLearning,"Global Machine Learning Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/a48pq5/global_machine_learning_market_size_outlook/,Ganeshsing,1544253656,[removed],0,1,False,self,,,,,
472,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,17,a48x66,self.MachineLearning,[R] Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,https://www.reddit.com/r/MachineLearning/comments/a48x66/r_rigorous_agent_evaluation_an_adversarial/,anan-yak,1544256010,[removed],0,1,False,self,,,,,
473,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,19,a49mcb,self.MachineLearning,Complete Guide on what is the best way and how to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/a49mcb/complete_guide_on_what_is_the_best_way_and_how_to/,docksonpaul,1544264013,[removed],0,1,False,self,,,,,
474,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,19,a49nuh,evernote.com,Complete Guide on what is the best way and how to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/a49nuh/complete_guide_on_what_is_the_best_way_and_how_to/,Amitagarwal7021,1544264494,,0,1,False,default,,,,,
475,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,19,a49sh8,solutionfactory.in,Dive Deep into Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a49sh8/dive_deep_into_deep_learning/,Ajaygawde,1544265982,,0,1,False,default,,,,,
476,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,20,a49w9o,self.MachineLearning,Complete Guide on what is the best way and How to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/a49w9o/complete_guide_on_what_is_the_best_way_and_how_to/,Amitagarwal7021,1544267160,[removed],0,1,False,self,,,,,
477,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,21,a4ah04,arxiv.org,[R] Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models,https://www.reddit.com/r/MachineLearning/comments/a4ah04/r_molecular_sets_moses_a_benchmarking_platform/,zhebrak,1544273453,,2,1,False,default,,,,,
478,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,22,a4apf0,self.MachineLearning,hey guys I want to share a personal python (machine Leaning) youtube channel,https://www.reddit.com/r/MachineLearning/comments/a4apf0/hey_guys_i_want_to_share_a_personal_python/,adarsh_adg,1544275764,"&amp;#x200B;

I was working in a small personal project, a community where we can help each other regarding machine learning in python, leave me a valuable feedback if you like it [https://youtu.be/v9jXnT7tvbs](https://youtu.be/v9jXnT7tvbs)",0,1,False,self,,,,,
479,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,22,a4aur4,self.MachineLearning,Cristmas gift,https://www.reddit.com/r/MachineLearning/comments/a4aur4/cristmas_gift/,travis1bickle,1544277115,What is a good book about machine learning focusing on recurrent neural networks? It is for an experience scientist. ,0,1,False,self,,,,,
480,MachineLearning,t5_2r3gv,2018-12-8,2018,12,8,23,a4b4vt,self.MachineLearning,[Q] is there a reason to use groupnorm if your batch size is sufficient?,https://www.reddit.com/r/MachineLearning/comments/a4b4vt/q_is_there_a_reason_to_use_groupnorm_if_your/,torviche,1544279430,[removed],0,1,False,self,,,,,
481,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,1,a4cbkc,self.MachineLearning,"beginner in ML, need advice",https://www.reddit.com/r/MachineLearning/comments/a4cbkc/beginner_in_ml_need_advice/,iambibekp,1544288200,[removed],0,1,False,self,,,,,
482,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,2,a4cnpi,self.MachineLearning,"If you were going to redo your project, what would you do differently?",https://www.reddit.com/r/MachineLearning/comments/a4cnpi/if_you_were_going_to_redo_your_project_what_would/,hazard02,1544290502,[removed],0,1,False,self,,,,,
483,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,2,a4cpe4,self.MachineLearning,[P] 90s Pop Lyrics Generator - an exploration of LSTMs,https://www.reddit.com/r/MachineLearning/comments/a4cpe4/p_90s_pop_lyrics_generator_an_exploration_of_lstms/,Laboratory_one,1544290816,"I did a project to study LSTMs a bit ago. Its purpose is to Generate 90s pop lyrics. 

I wanted to see how I could build a sequence-to-sequence model and determine the quality of data required. I initially thought that a word-level model would be the best way to go, but for me, a character-level model worked far better. It also turns out that I didn't need as much data as I suspected. I'd like to retrained this with a cleaner dataset in the future.

What's your experience with similar projects? Any suggestions on how this could be improved?

The code is [here](https://github.com/PeterChauYEG/90s_pop_songs) if anyone's interested. 

http://labone.tech/90s-pop-lyrics-generator/",11,1,False,self,,,,,
484,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,2,a4cqrc,self.MachineLearning,[D] eXtreme MultiLabel Classification in less than 5 minutes,https://www.reddit.com/r/MachineLearning/comments/a4cqrc/d_extreme_multilabel_classification_in_less_than/,therhappy,1544291071,"Hi Reddit!

I've posted this video online about a year ago. It's a quick and comprehensive explication of XML classification techniques as proposed in [https://arxiv.org/abs/1704.03718](https://arxiv.org/abs/1704.03718). The description includes a link to a notebook that implements these methods.

As the video happened to gather a few views I realized it could actually be useful to some people, so I'm posting it here. I would be glad to get returns on how to improve the explanations, would I do others videos of the same genre.

Good day to you!",2,1,False,self,,,,,
485,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,4,a4dgc9,self.MachineLearning,Need Twitter login info for API,https://www.reddit.com/r/MachineLearning/comments/a4dgc9/need_twitter_login_info_for_api/,Trrwwa,1544295810,[removed],0,1,False,self,,,,,
486,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,4,a4dieb,self.MachineLearning,[D] Evaluate success of banner ads - what approach and variables to use?,https://www.reddit.com/r/MachineLearning/comments/a4dieb/d_evaluate_success_of_banner_ads_what_approach/,7OceansMan,1544296166,"I found plenty of companies marketing that they use ""AI"" to create the best advertising campaigns yet they only mention using machine learning to post and target, never to actually evaluate click rate of particular banner ad on a specific customer group.

For example: a banner ad for fb with image of current employees smiling and holding a paper saying ""join us"" targeted at group of people between 20-30 years who are interested in customer support - and now this program would evaluate what the click rate is going to be for this specific post for this specific target group.

My idea to solve this issue would be to identify the **visual variables** and to look for them in any ad:

\- Color (what is most used, how many different once are used,  etc.)

\- What things/people/animals are in the image (ice creams, cats, Spiderman, etc.)

\- Layout (what space is used for each of those things)

\- Texts (what does it say, is it funny, serious, etc.)

\- what else?

Then I would create samples of ads with different visual variables and present them to the target audience (20-30yo) with interest in HR and find out which ads would do the best (the click rate).

Then train the program on those findings so that any imputed ad could be evaluated with click rate as an outcome.

Lastly evaluate the results on a control group by presenting them the ads as with group 1 but also another ad, which would be evaluated by the program as similar in click rate to test the effectiveness of the program.

Now thanks for reading all of this. What other variables do you think would be necessary, would there be other variables related to the target audience (like education, style of humour...) and would this in general work, or in other words, can be a funny creative image broken down into visual variables as the whole is more than sum of its parts?

Thx

P.S. Facebook always suggests the probable click rate when ad is submitted so if anyone knows how that works I would love to know.",0,1,False,self,,,,,
487,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,4,a4djcl,arxiv.org,[R] Deep Learning for Classical Japanese Literature,https://www.reddit.com/r/MachineLearning/comments/a4djcl/r_deep_learning_for_classical_japanese_literature/,baylearn,1544296330,,9,1,False,default,,,,,
488,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,4,a4dxna,arxiv.org,[R] Bag of Tricks for Image Classification with Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a4dxna/r_bag_of_tricks_for_image_classification_with/,ewanlee,1544298931,,25,1,False,default,,,,,
489,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,5,a4eb7x,self.MachineLearning,[D] Camera/Lens type invariance for image classification/object detection/semseg?,https://www.reddit.com/r/MachineLearning/comments/a4eb7x/d_cameralens_type_invariance_for_image/,gabegabe6,1544301444,"Dear all,
I was wondering if you know about any research on this kind of invariance (if we can call it that). I think this could be really important when we collect images from different sources, or when we fit a model to a certain type then in real world problems we need to use a different camera type.
I would love to hear your thoughts on this subject and also any paper on the subject is welcome!",4,1,False,self,,,,,
490,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,5,a4ebxv,self.MachineLearning,I made a Machine Learning Discord Server,https://www.reddit.com/r/MachineLearning/comments/a4ebxv/i_made_a_machine_learning_discord_server/,CzoKc,1544301582,[removed],0,1,False,self,,,,,
491,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,5,a4egbv,self.MachineLearning,Beginner question: from the math to actual code,https://www.reddit.com/r/MachineLearning/comments/a4egbv/beginner_question_from_the_math_to_actual_code/,swamprat09,1544302431,[removed],0,1,False,self,,,,,
492,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,6,a4eic0,self.MachineLearning,"[D] G Brain, FAIR Residency",https://www.reddit.com/r/MachineLearning/comments/a4eic0/d_g_brain_fair_residency/,nigeria1741,1544302808,"Not sure if this is the right place for this. However, I was directed here after posting in /r/cscareerquestions 

Got an offer for an internship in either Google Brain or FAIR (FB AI Research). Im an undergraduate passionate about ML/AI. However, instead going for a full time engineering role at G or FB. I want to try to get into the Google Brain Residency or FAIR residency. Does anyone know if I can get into this residency as an undergrad? I don't have any research experience but I worked on an applied ML team at G or FB and as I said above I will be working at FAIR or Google Brain as an intern soon.

My idea right now is since I will be internal to Brain or FAIR soon I can talk to manager/team members/respective individuals and instead of getting evaluated for a SWE offer, ask to get evaluated for FAIR (obviously not a traditional thing to do) so I am trying to get more ideas.

Anybody have any ideas on this?

Cheers",6,1,False,self,,,,,
493,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,6,a4ephk,self.MachineLearning,[R] Any papers on the time required to train a neural network?,https://www.reddit.com/r/MachineLearning/comments/a4ephk/r_any_papers_on_the_time_required_to_train_a/,koen_C,1544304040,As far as I know the time required to train a neural network typically scales with the number of trainable parameters. However I can't find any detailing this.,2,1,False,self,,,,,
494,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,7,a4fi0k,self.MachineLearning,Attempt to reproduce BERT but degenerate layers are everywhere.,https://www.reddit.com/r/MachineLearning/comments/a4fi0k/attempt_to_reproduce_bert_but_degenerate_layers/,minsheng,1544309449,[removed],0,1,False,self,,,,,
495,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,8,a4fpkr,youtube.com,Python: Divide by Zero (NEW CUTTING EDGE),https://www.reddit.com/r/MachineLearning/comments/a4fpkr/python_divide_by_zero_new_cutting_edge/,rylanpfowers,1544310920,,0,1,False,https://a.thumbs.redditmedia.com/3zf0VKAD3H3VI0eqWHWi7X-S7TFCoIRHY-cm4g9LMu8.jpg,,,,,
496,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,8,a4fupj,self.MachineLearning,[D] Best way to use ML to translate CAD files,https://www.reddit.com/r/MachineLearning/comments/a4fupj/d_best_way_to_use_ml_to_translate_cad_files/,carsonpoole,1544311975,"So the short of it is that there is very few products (and the ones around are very shitty)  to do something that is used in pretty much every medium to heavy industry. Woodworking, sign-making, metalwork, hell Boeing does it, etc... all would use this kind of product, and there's basically no competition. At the MVP level, it would just have to translate a DXF file (vector CAD file from basically any vector design program) to what's called gCode, which is essentially the instructions for a router/laser-cutter/waterjet cutter/etc to make the actual product you've designed. There're very few products offered for doing this and its a huge market.

&amp;#x200B;

Here's an excerpt from a DXF file (only like 5% of it)

&amp;#x200B;

`CIRCLE`

 `8`

`ROUTE_T_5DRILL_`

 `10`

`7.0`

 `20`

`10.0`

 `30`

`0.0`

 `39`

`-19.0`

 `40`

`2.5`

 `210`

 `0` 

 `220`

 `0` 

 `230`

 `1` 

 `0`

&amp;#x200B;

And here's the corresponding part of the gCode file:

`N0042 G00 X7.0 Y10.0`

`N0043 G01 Z-19.0 F3810.0`

&amp;#x200B;

If you look, you'll see matching measurements. The issue is the DXF file has a ton of irrelevant details not needed, and the gCode file gets things from the DXF that I can't seem to figure out the logic.

&amp;#x200B;

I'm guessing some kind of Seq2Seq should be able to figure all this out, and I've tried one model using Attention but it couldn't seem to figure out much in 35 epochs, and that took forever. My bigger issue is that it uses a ""dictionary"" and a finite set of vocab, but since some of the data is a infinite set of numbers, a finite vocab won't really help much, and the model can't figure out that ""12"" is closer to ""12.000001""  than ""VERTEX"" is to ""POLYLINE"" when it's just a dictionary.

&amp;#x200B;

So what I'm asking is simply for advice as far as how to go about translating between these two languages using ML.

&amp;#x200B;",7,1,False,self,,,,,
497,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,8,a4fwnb,scitepress.org,[R] Applied ML in Neurosience - A Brain-Computer Interface system for the Cybathlon: The Bionic Olympics,https://www.reddit.com/r/MachineLearning/comments/a4fwnb/r_applied_ml_in_neurosience_a_braincomputer/,edugonzalezp,1544312383,,0,1,False,default,,,,,
498,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,9,a4gi4g,self.MachineLearning,[P] Predicting the Owner of an Ethereum Address,https://www.reddit.com/r/MachineLearning/comments/a4gi4g/p_predicting_the_owner_of_an_ethereum_address/,TWP21,1544316916,"https://towardsdatascience.com/clustering-ethereum-addresses-18aeca61919d

Hi SlashML - I built a clustering algorithm to characterize the Ethereum address space. Feedback encouraged!",1,1,False,self,,,,,
499,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,10,a4gr58,self.MachineLearning,Publishing in a journal from outside of academia,https://www.reddit.com/r/MachineLearning/comments/a4gr58/publishing_in_a_journal_from_outside_of_academia/,swagbitcoinmoney,1544318868,[removed],0,1,False,self,,,,,
500,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,10,a4grfe,self.MachineLearning,[D] Publishing in a journal from outside of academia,https://www.reddit.com/r/MachineLearning/comments/a4grfe/d_publishing_in_a_journal_from_outside_of_academia/,swagbitcoinmoney,1544318931,"I recently finished a research project in the field of deep learning / robotics and would like to publish it. The issue I am facing is that I am outside of the academic field but would still like to publish this in an actual journal / conference (not just arXiv), both to share my results with the world and also to add to my resume. Are there any journals that are ""easier"" to get published in (less stringent requirements for publication) but still recognized as legitimate journals?
",15,1,False,self,,,,,
501,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,10,a4gteq,github.com,[P] Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models,https://www.reddit.com/r/MachineLearning/comments/a4gteq/p_molecular_sets_moses_a_benchmarking_platform/,zhebrak,1544319371,,0,1,False,default,,,,,
502,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,11,a4h0he,self.MachineLearning,What system could be used to train a sparse tree topology convolutional RBM to compute all possible combos of the iota lambda (which is a minimalist general computing function) recursing into the tree and returning upward by RBM association? Theres a bandwidth and data duplication issue,https://www.reddit.com/r/MachineLearning/comments/a4h0he/what_system_could_be_used_to_train_a_sparse_tree/,BenRayfield,1544320953,[removed],0,1,False,self,,,,,
503,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,12,a4hh2f,openreview.net,"""Exploration by random distillation"", Burda et al 2018 {OA} [predicting outputs of a random network to define novelty; SOTA on _Montezuma's Revenge_]",https://www.reddit.com/r/MachineLearning/comments/a4hh2f/exploration_by_random_distillation_burda_et_al/,gwern,1544324645,,0,1,False,https://a.thumbs.redditmedia.com/O5xSqPxXQan0eq4XIj_39G9lsyFLtyg3D_81hYNOIr4.jpg,,,,,
504,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,13,a4i3dc,techgrabyte.com,This machine learning model will help you to make for money http://techgrabyte.com/bank-ai-save-money-manage-finances/,https://www.reddit.com/r/MachineLearning/comments/a4i3dc/this_machine_learning_model_will_help_you_to_make/,navin49,1544330072,,0,1,False,default,,,,,
505,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,14,a4ihvd,self.MachineLearning,[D] How to do text generation from a small dataset,https://www.reddit.com/r/MachineLearning/comments/a4ihvd/d_how_to_do_text_generation_from_a_small_dataset/,invertedpassion,1544333924,"Hello,

Ive been trying to generate quotes similar to quotes in a dataset. My dataset is small: 5200 sentences. I know LSTMs are perfect for this task if I had a large dataset. When I tried character level RNN on this small dataset, I didnt get good results.

Does anyone know what I should be exploring for this problem? The ideal end result would be text generation thats similar to these 5200 sentences.",6,1,False,self,,,,,
506,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,18,a4jlyk,locandapiazzaparlamentoroma.com,Bed and Breakfast Roma | Locanda Piazza del Parlamento,https://www.reddit.com/r/MachineLearning/comments/a4jlyk/bed_and_breakfast_roma_locanda_piazza_del/,lynelledrohanby,1544346110,,0,1,False,default,,,,,
507,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,18,a4juv2,self.MachineLearning,Is indRNN applicable on the REMC datasets for Histone Modification data. link given below,https://www.reddit.com/r/MachineLearning/comments/a4juv2/is_indrnn_applicable_on_the_remc_datasets_for/,SajadMulk,1544349150,[removed],0,1,False,self,,,,,
508,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,19,a4k5jt,self.MachineLearning,Customer Store Recommendation Problem,https://www.reddit.com/r/MachineLearning/comments/a4k5jt/customer_store_recommendation_problem/,cp_four,1544352828,[removed],0,1,False,self,,,,,
509,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,19,a4k5zc,codecab.blogspot.com,ChatBots | Everything You Need to Know | Chatbot Revolution is Coming,https://www.reddit.com/r/MachineLearning/comments/a4k5zc/chatbots_everything_you_need_to_know_chatbot/,evan_brown,1544352975,,0,1,False,https://b.thumbs.redditmedia.com/aZO-0yLGtJQxffU9Mf9kyk_WBq7LAARgpuqsalu5dXo.jpg,,,,,
510,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,20,a4k7l2,self.MachineLearning,[D] How to approach Customer Store Recommendation Problem,https://www.reddit.com/r/MachineLearning/comments/a4k7l2/d_how_to_approach_customer_store_recommendation/,cp_four,1544353483,"3 files: User profile, store profile, transaction histroy.

I have user profiles of 100k customers and store profiles of 35 shops. Also, a transactions set that has all the purchases made by customers at any store (550k transactions).

Some new stores are opened, store profile is given (similar to other 35 shops), no transactions. The objective is to determine whether an existing customer will shop in each of the new stores.

I am trying to do this with recommendation system with item cold start. I am also looking into implicit feedback recommendation system, however, I m very new to this and cannot figure out how to use all these features and data together.

I need suggestion on how to approach this problem, or where to get started.

p.s. Excuse my naivety, I am also new to Reddit.",21,1,False,self,,,,,
511,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,20,a4kg5k,bryanlohjy.gitlab.io,[R] SpaceSheets: Interactive Latent Space Exploration through a Spreadsheet Interface,https://www.reddit.com/r/MachineLearning/comments/a4kg5k/r_spacesheets_interactive_latent_space/,ofirpress,1544356350,,1,1,False,default,,,,,
512,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,21,a4kjve,self.MachineLearning,Did anyone mastered tf.data?,https://www.reddit.com/r/MachineLearning/comments/a4kjve/did_anyone_mastered_tfdata/,Holyzone,1544357474,[removed],0,1,False,self,,,,,
513,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,22,a4l572,i.redd.it,Projetos de ETE V,https://www.reddit.com/r/MachineLearning/comments/a4l572/projetos_de_ete_v/,JamurGerloff,1544363967,,0,1,False,https://b.thumbs.redditmedia.com/ltyRP3oTDbcJC0wg3Wegt7FFA6Z3l9ih7oQZXTmcofE.jpg,,,,,
514,MachineLearning,t5_2r3gv,2018-12-9,2018,12,9,23,a4l9qw,self.MachineLearning,Looking for help with starting ML for use with analysis of sensor data from previously built infrastructure...,https://www.reddit.com/r/MachineLearning/comments/a4l9qw/looking_for_help_with_starting_ml_for_use_with/,liskl,1544365178,[removed],0,1,False,self,,,,,
515,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,0,a4lsir,self.MachineLearning,Liftering mfcc's,https://www.reddit.com/r/MachineLearning/comments/a4lsir/liftering_mfccs/,GideonRT,1544369955,[removed],0,1,False,self,,,,,
516,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,1,a4m53f,self.MachineLearning,Dataset structure,https://www.reddit.com/r/MachineLearning/comments/a4m53f/dataset_structure/,J0zif,1544372850,[removed],0,1,False,self,,,,,
517,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,1,a4mdta,self.MachineLearning,[D] Is there any way to tell which subnetworks in a neural network are most important for making a specific class prediction?,https://www.reddit.com/r/MachineLearning/comments/a4mdta/d_is_there_any_way_to_tell_which_subnetworks_in_a/,lrningcode,1544374508,Let's say you have an MNIST classifier with an output vector of length 10 with each index corresponding to each possible class. Is there any way to find out which subnetworks/weights in the classifier are most important for maintaining the prediction accuracy for a specific class?,27,1,False,self,,,,,
518,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,3,a4n0j4,self.MachineLearning,Would the Intel AI compute stick be a suitable tool to learn AI?,https://www.reddit.com/r/MachineLearning/comments/a4n0j4/would_the_intel_ai_compute_stick_be_a_suitable/,wishinghorse,1544378424,[removed],0,1,False,self,,,,,
519,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,3,a4n9vh,self.MachineLearning,Distributed vs. disentangled representation,https://www.reddit.com/r/MachineLearning/comments/a4n9vh/distributed_vs_disentangled_representation/,neuranus,1544380078,[removed],0,1,False,self,,,,,
520,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,3,a4ndd7,self.MachineLearning,[D] Distributed vs. disentangled representation,https://www.reddit.com/r/MachineLearning/comments/a4ndd7/d_distributed_vs_disentangled_representation/,neuranus,1544380707,"These two seem like completely opposite concepts, however I rarely see any debate around it. Is my understanding flawed?",4,1,False,self,,,,,
521,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,4,a4ny6d,self.MachineLearning,On the verge of quitting,https://www.reddit.com/r/MachineLearning/comments/a4ny6d/on_the_verge_of_quitting/,tornado01,1544384392,[removed],0,1,False,self,,,,,
522,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,5,a4o6ab,self.MachineLearning,Classification of User Comments into Social Media Groups with Opposing Views,https://www.reddit.com/r/MachineLearning/comments/a4o6ab/classification_of_user_comments_into_social_media/,Epokhe,1544385793,[removed],0,1,False,self,,,,,
523,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,5,a4o9wc,brainforce.launchaco.com,Deploy AI model at scale with confidence on BrainForce,https://www.reddit.com/r/MachineLearning/comments/a4o9wc/deploy_ai_model_at_scale_with_confidence_on/,theoszymk,1544386437,,0,1,False,default,,,,,
524,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,5,a4oe7m,self.MachineLearning,[P] Classification of User Comments into Social Media Groups with Opposing Views,https://www.reddit.com/r/MachineLearning/comments/a4oe7m/p_classification_of_user_comments_into_social/,Epokhe,1544387188,"Previous year, we worked on a comment classification project for our deep learning class. We wanted to share the results here.

Summary: We gathered political pages of two parties(AKP-CHP for Turkey, Republican-Democrat for US) from Facebook and collected people's comments in those pages. Comments were labeled with the political party their parent page supported. Tested out different architectures(Char-level LSTM/GRU, Word-level Conv). Models reached 80~% accuracy. This page prediction task is different than directly predicting the political affinity from a comment, we didn't have the data that provided direct mapping from comments to political affinity. However, it still managed to extract information about subtle cues related to political parties. The detailed results are shared for Turkish parties in the report but they could be generated for US parties with the code on github(datasets are in the repo).

Here is the [code](https://github.com/obe-dl/social-media-comment-classification) and [project report](https://github.com/obe-dl/social-media-comment-classification/raw/master/report.pdf).
",1,1,False,self,,,,,
525,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,5,a4onjk,self.MachineLearning,[D] Project Management Books for Research Projects,https://www.reddit.com/r/MachineLearning/comments/a4onjk/d_project_management_books_for_research_projects/,risig_sag,1544388833,"There are a many great books on project management for software engineering.

But which books would you recommend for running ML research projects?

What science managers are reading at DeepMind, OpenAI etc.?",5,1,False,self,,,,,
526,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,5,a4oo9c,moalquraishi.wordpress.com,[D] 'AlphaFold @ CASP13: What just happened?',https://www.reddit.com/r/MachineLearning/comments/a4oo9c/d_alphafold_casp13_what_just_happened/,gwern,1544388949,,0,1,False,default,,,,,
527,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,6,a4opot,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 52,https://www.reddit.com/r/MachineLearning/comments/a4opot/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1544389205,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/ClydeMachine: [the OpenAI blog post on Iterated Amplification](https://blog.openai.com/amplifying-ai-training/)

/u/wassname: [""Optimizing Agent Behavior over Long Time Scales by Transporting Value""](https://arxiv.org/abs/1810.06721)

/u/ndha1995: [1703.00573\] Generalization and Equilibrium in Generative Adversarial Nets (GANs)](https://arxiv.org/abs/1703.00573)

Besides that, there are no rules, have fun.",11,1,False,self,,,,,
528,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,6,a4oq16,self.MachineLearning,I want to get started,https://www.reddit.com/r/MachineLearning/comments/a4oq16/i_want_to_get_started/,SavingsClassic,1544389261,[removed],0,1,False,self,,,,,
529,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,6,a4oyx8,self.MachineLearning,[D] Is bad clusterability a sign for bad predictability?,https://www.reddit.com/r/MachineLearning/comments/a4oyx8/d_is_bad_clusterability_a_sign_for_bad/,aeppelsaeft,1544390855,"When I am trying dimensionality reduction (UMAP) on a dataset with circa 100 features and then visualize the results, then I am seeing 8-10 distinct clusters. Let's say we color the data points w.r.t. some target variable but the colors are all over the place, none of the clusters are made up of mostly one color. 

Does this imply that the target variable also cannot be predicted well by any classifier? If so, is that a general rule? Are there exceptions? Or does bad clusterability have nothing to do with bad predictability?

Thanks  for your help!",10,1,False,self,,,,,
530,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,6,a4p3in,medium.com,"[P] I get a lot of questions about the autopilot repository I made a few years back, so I finally made a Medium post explaining my process",https://www.reddit.com/r/MachineLearning/comments/a4p3in/p_i_get_a_lot_of_questions_about_the_autopilot/,Weihua99,1544391749,,0,1,False,default,,,,,
531,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,7,a4prc0,openias.org,[P] Does Sweden Have Rule Of Law?,https://www.reddit.com/r/MachineLearning/comments/a4prc0/p_does_sweden_have_rule_of_law/,bjornsing,1544396273,,0,1,False,default,,,,,
532,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,8,a4ps3h,openias.org,[P] Does Sweden Have Rule Of Law?,https://www.reddit.com/r/MachineLearning/comments/a4ps3h/p_does_sweden_have_rule_of_law/,bjornsing,1544396427,,0,1,False,https://a.thumbs.redditmedia.com/RFBAjgAJJ5PHecbLtIb2ZQ1RK_0pewI0RPjnTzNudd0.jpg,,,,,
533,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,9,a4qh82,self.MachineLearning,Holykell WETEX 2018 In Dubai,https://www.reddit.com/r/MachineLearning/comments/a4qh82/holykell_wetex_2018_in_dubai/,Holykell,1544401614,[removed],0,1,False,self,,,,,
534,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,9,a4qii3,self.MachineLearning,Are classes in computer architecture and circuits in any way relevant to furthering my understanding of ML?,https://www.reddit.com/r/MachineLearning/comments/a4qii3/are_classes_in_computer_architecture_and_circuits/,BlackStallion101,1544401886,[removed],0,1,False,self,,,,,
535,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,9,a4qp8n,self.MachineLearning,can you help me to solve this problem?,https://www.reddit.com/r/MachineLearning/comments/a4qp8n/can_you_help_me_to_solve_this_problem/,mohamedwittiadou,1544403318,[removed],0,1,False,self,,,,,
536,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,10,a4qtbo,self.MachineLearning,What kind of questions are asked in OpenAI interviews for internships?,https://www.reddit.com/r/MachineLearning/comments/a4qtbo/what_kind_of_questions_are_asked_in_openai/,elita1151995,1544404190,,0,1,False,self,,,,,
537,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,12,a4s0py,self.MachineLearning,[D] TFrecord in tensorflow,https://www.reddit.com/r/MachineLearning/comments/a4s0py/d_tfrecord_in_tensorflow/,marksteve4,1544413517,"I read this article about tfrecord which gives a good example of tfrecord usage. But it does not touch why should we use tfrecord and what the pros and cons of the alternative. Any thought on this topic?

&amp;#x200B;

[https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564](https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564)",10,1,False,self,,,,,
538,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,13,a4s9ev,techgrabyte.com,"Warning, AI has start manipulating we humans and that too in a untouchable way, This storytelling AI fools 3 out 5 humans with its writing",https://www.reddit.com/r/MachineLearning/comments/a4s9ev/warning_ai_has_start_manipulating_we_humans_and/,navin49,1544415402,,0,1,False,default,,,,,
539,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,13,a4sf7t,167.114.87.240,ID Demo | Agen Judi Bola | Bandar Bola | Agen Sbobet - Clickbet88,https://www.reddit.com/r/MachineLearning/comments/a4sf7t/id_demo_agen_judi_bola_bandar_bola_agen_sbobet/,sanorawuduskeyr,1544416714,,0,1,False,default,,,,,
540,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,16,a4tg5r,self.MachineLearning,[D] Why is the process of discovering/managing papers so bad?,https://www.reddit.com/r/MachineLearning/comments/a4tg5r/d_why_is_the_process_of_discoveringmanaging/,lpatks,1544425866,"This is my current work flow:
- Find new papers via arxiv-sanity and twitter, and occasionally reddit or anywhere else.
- If the paper looks relevant, add to both arxiv-sanity (which is necessary to continue getting good suggestions) and Mendeley (where I actually read papers).
- When I subsequently have a more thorough read, I might add a tag or put them in a folder in Mendeley. I have not found this that useful, possibly because I work in an interdisciplinary area, so many papers are not that easily categorised.

Keeping two libraries is a pain. Mendeley is just not that nice to use. If I don't check twitter for X days, there is no easy way for me to see what I missed. Arixv-sanity works well, but only for papers that appear on arxiv (which is fine for ML, but not for other areas). **Is there a better way?**

Other things I have tried for the actual management side of things:
1) Zotero - basically the same as Mendeley, but with that a worse UI.
2) Papers3 - I quite like this, but it doesn't work on linux, and has some really odd bugs that have been around for ages. Also you can't have shared folders.
3) Just keeping a dropbox folder full of papers. This is honestly only marginally worse than Mendeley/Zotero.",61,1,False,self,,,,,
541,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,16,a4tkqm,arxiv.org,[R] Arbitrary Style Transfer with Style-Attentional Networks,https://www.reddit.com/r/MachineLearning/comments/a4tkqm/r_arbitrary_style_transfer_with_styleattentional/,kh22l22,1544427140,,11,1,False,default,,,,,
542,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,16,a4tkri,self.MachineLearning,Applications of Neural Network,https://www.reddit.com/r/MachineLearning/comments/a4tkri/applications_of_neural_network/,Nishujanu1209,1544427145,[removed],0,1,False,self,,,,,
543,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,16,a4tmde,github.com,[P] Automatic Deep Extreme Cut,https://www.reddit.com/r/MachineLearning/comments/a4tmde/p_automatic_deep_extreme_cut/,kh22l22,1544427610,,0,1,False,default,,,,,
544,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,16,a4tnev,self.MachineLearning,What ways are there to penalize the certain outputs of NN classifier?,https://www.reddit.com/r/MachineLearning/comments/a4tnev/what_ways_are_there_to_penalize_the_certain/,TanktopSamurai,1544427919,[removed],0,1,False,self,,,,,
545,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,16,a4tpie,self.MachineLearning,[P] Automatic Deep Extreme-Cut,https://www.reddit.com/r/MachineLearning/comments/a4tpie/p_automatic_deep_extremecut/,kh22l22,1544428546,[removed],0,1,False,self,,,,,
546,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,17,a4trku,self.MachineLearning,[P] Implementing automatic deep extreme cut,https://www.reddit.com/r/MachineLearning/comments/a4trku/p_implementing_automatic_deep_extreme_cut/,kh22l22,1544429136,"[https://github.com/kh22l22/Auto-DEXTR-Pytorch](https://github.com/kh22l22/Auto-DEXTR-Pytorch)

This project provides a pytorch implementation of an automatic deep extreme cut algorithm using canny edge detection and NLFD (Non-Local Deep Features for Salient Object Detection-CVPR2017).

&amp;#x200B;

&amp;#x200B;",6,1,False,self,,,,,
547,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,17,a4txoa,jalammar.github.io,"The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)",https://www.reddit.com/r/MachineLearning/comments/a4txoa/the_illustrated_bert_elmo_and_co_how_nlp_cracked/,steccami,1544430986,,0,1,False,default,,,,,
548,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,17,a4u0qa,self.MachineLearning,Finding optimal weights for ensemble learning with neural network,https://www.reddit.com/r/MachineLearning/comments/a4u0qa/finding_optimal_weights_for_ensemble_learning/,mrdanibudapest,1544431915,[removed],0,1,False,self,,,,,
549,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,17,a4u1rj,reddit.com,The Ugears Tower Windmill !!!,https://www.reddit.com/r/MachineLearning/comments/a4u1rj/the_ugears_tower_windmill/,UgearsIndia,1544432246,,0,1,False,default,,,,,
550,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,18,a4u4i9,self.MachineLearning,[N] Fully paid Internship in Software Development for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a4u4i9/n_fully_paid_internship_in_software_development/,UnhappyElderberry,1544433043,"Pattern Recognition and Image Analysis Group at University of Muenster offers an internship at our project 'Barista - A Graphical User Interface for Designing and Training Neural Networks'. We are offering an opportunity for a 3-months stay in Germany starting around May 2019. The stay will be funded by German Academic Exchange Service (DAAD). The applicant must be enrolled in an undergraduate program at a United States, Canadian, British or Irish university/college for at least two years.

If you are interested in Deep Learning and have some experience in Software Development using python, we are happy to receive your application. You can find more information on Barista at [http://barista.uni-muenster.de](http://barista.uni-muenster.de) and on the application portal of the DAAD: [https://www.daad.de/rise/en/rise-germany/find-an-internship/application-portal/](https://www.daad.de/rise/en/rise-germany/find-an-internship/application-portal/). Our Reference code is Muenster\_CS\_3607.",9,1,False,self,,,,,
551,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,18,a4u6zq,self.MachineLearning,"[D] ""The strategy that delivers the best returns, it turns out, is to divide the funding equally among all researchers.""",https://www.reddit.com/r/MachineLearning/comments/a4u6zq/d_the_strategy_that_delivers_the_best_returns_it/,phobrain,1544433800,[removed],1,1,False,self,,,,,
552,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,18,a4u8e5,arxiv.org,[R] Image-to-image translation for cross-domain disentanglement,https://www.reddit.com/r/MachineLearning/comments/a4u8e5/r_imagetoimage_translation_for_crossdomain/,i-like-big-gans,1544434252,,3,1,False,default,,,,,
553,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,19,a4umgz,self.MachineLearning,Is there anyone from India attending Machine Learning Summer School 2019?,https://www.reddit.com/r/MachineLearning/comments/a4umgz/is_there_anyone_from_india_attending_machine/,yashiiit,1544438649,[removed],0,1,False,self,,,,,
554,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,20,a4uwn6,github.com,"800,000+ handwritten digits",https://www.reddit.com/r/MachineLearning/comments/a4uwn6/800000_handwritten_digits/,kensanata,1544441624,,0,1,False,default,,,,,
555,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,20,a4uyko,self.MachineLearning,Implementation of Linear Regression From Scratch (With Mathematical Explanation),https://www.reddit.com/r/MachineLearning/comments/a4uyko/implementation_of_linear_regression_from_scratch/,SarvasvKulpati,1544442170,[removed],0,1,False,self,,,,,
556,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,20,a4uzt9,multisoftvirtualacademy.com,Machine Learning Online Courses Are The Best Choice for Both Students &amp; Professionals - MVA Blog,https://www.reddit.com/r/MachineLearning/comments/a4uzt9/machine_learning_online_courses_are_the_best/,multisoftmva0,1544442538,,0,1,False,default,,,,,
557,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,21,a4v269,github.com,"800,000+ handwritten digits",https://www.reddit.com/r/MachineLearning/comments/a4v269/800000_handwritten_digits/,kensanata,1544443213,,0,1,False,default,,,,,
558,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,21,a4vaml,self.MachineLearning,"[R] TDLS - Classics: SMOTE, Synthetic Minority Over-sampling Technique",https://www.reddit.com/r/MachineLearning/comments/a4vaml/r_tdls_classics_smote_synthetic_minority/,tdls_to,1544445407,"**Review of the algorithm:** [https://youtu.be/I-sMbGFNAUM](https://youtu.be/I-sMbGFNAUM)

**Discussion about the paper:** [https://youtu.be/wBnpY-3Wc-Y](https://youtu.be/wBnpY-3Wc-Y)

**Paper reference:** [https://arxiv.org/pdf/1106.1813.pdf](https://www.youtube.com/redirect?q=https%3A%2F%2Farxiv.org%2Fpdf%2F1106.1813.pdf&amp;event=video_description&amp;v=wBnpY-3Wc-Y&amp;redir_token=tD9mfTuiLBN6z2ojVFL5-iBgcMR8MTU0NDUzMTM3NUAxNTQ0NDQ0OTc1)

we recently reviewed the original SMOTE paper, which is an oversampling technique that uses KNN to generate new samples (in fact interpolates with a random probability within the existing dataset). It can improve the accuracy of classifiers for minority class, in principle. A few points about the technique:

* Combination of SMOTE + under-sampling generally performs better than only under-sampling
   * Less over-fitting -&gt; more general
* Provides better results than adjusting model parameters for some algorithms (e.g. loss ratio in Ripper; class priors in Naive Bayes)
* Provides more minority class samples from which to learn
* Generally provides similar results to over- or under-sampling with challenging decision boundaries

Some final thoughts that we had about the paper:

* Are there any areas that SMOTE works particularly good?
* Is this a popular idea, but outdated?
   * Adaptive Synthetic Sampling (He et al 2008)
   * Generative Adversarial Networks (e.g. Douzas &amp; Bacao 2018)
* Is this a good inspiration for generating synthetic data?

**Paper abstract:** 

An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of normal examples with only a small percentage of abnormal or interesting examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy

&amp;#x200B;",7,1,False,self,,,,,
559,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,21,a4vczo,self.MachineLearning,[D] What would be the best way to classify sequences of spectrograms?,https://www.reddit.com/r/MachineLearning/comments/a4vczo/d_what_would_be_the_best_way_to_classify/,sebmensink,1544446021,"As part of my research in the field of neuroscience I have over 700,000 spectrograms that depict ultrasonic mouse vocalisations in time and frequency domains. The spectrograms have come from a collection of 5 minute audio recording of various mice strains and I thought it would be an interesting project to try and predict the strain of mice based on the sequence of the spectrogram images. My thoughts are to use a mixture of a CNN and a LSTM. Ive used both separately before with Keras, and know that they have been combined before. However I am unsure on how to arrange the images into a sequence, any help would be very much appreciated.
",7,1,False,self,,,,,
560,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,21,a4vdyw,self.MachineLearning,[R] TDLS: Automated Vulnerability Detection in Source Code Using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a4vdyw/r_tdls_automated_vulnerability_detection_in/,tdls_to,1544446269,"**Algorithm review:** [https://youtu.be/4AAGZ\_H5vMA](https://youtu.be/4AAGZ_H5vMA)

**Paper discussion:** [https://youtu.be/TvZCT-qHU4Q](https://youtu.be/TvZCT-qHU4Q)

**Paper reference:** [https://arxiv.org/abs/1807.04320](https://www.youtube.com/redirect?v=TvZCT-qHU4Q&amp;redir_token=5T5W-s6PybTUyxB2GXbGXGsVuxJ8MTU0NDUzMjEzMEAxNTQ0NDQ1NzMw&amp;event=video_description&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F1807.04320)

We reviewed this recent paper on using representation learning on source code for the purpose of detecting code vulnerabilities. After some preprocessing (going through a lexer) they use CNN or RNN architectures to learn embedding which they use for vulnerability classification.

**Discussion points:** 

* No concrete analysis is provided to support claims about the lexer used in the paper.
* Each contribution should be tested against a hypothesis: authors make assumptions to build lexer without any validation backed by data
* For GitHub, Debian existing static analyzers were used to generate labels
* Study on SATE IV show low precision/recall for these analyzers at vulnerability detection
   * Thus, first experiment basically shows how well the CNN can approximate the result of the combined output of the open source static analyzers
* Typically, positive results from the methods which do not appear in the golden set are manually inspected to ensure correct labeling
* Neither humans, nor OS static analyzers have perfect accuracy
* Whether or not a method can lead to discovery of new as yet unknown vulnerabilities is a good test of the quality of analyzers.
   * In the absence of such an analysis, it is really hard to know what the true performance is",1,1,False,self,,,,,
561,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,22,a4vhyi,recast.ai,"If you want to learn more about Convolution Neural Networks, we listed blog posts, studies and research papers you should read.",https://www.reddit.com/r/MachineLearning/comments/a4vhyi/if_you_want_to_learn_more_about_convolution/,Recast-AI,1544447210,,0,1,False,default,,,,,
562,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,22,a4vjxq,self.reinforcementlearning,improving tic tac toe model,https://www.reddit.com/r/MachineLearning/comments/a4vjxq/improving_tic_tac_toe_model/,promach,1544447670,,0,1,False,default,,,,,
563,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,22,a4vl00,recast.ai,"If you want to learn more about Convolution Neural Networks, we listed blog posts, studies and research papers you should read.",https://www.reddit.com/r/MachineLearning/comments/a4vl00/if_you_want_to_learn_more_about_convolution/,Recast-AI,1544447917,,0,1,False,default,,,,,
564,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,23,a4w8jn,medium.com,Implementing ResNet with MXNET Gluon and Comet.ml for image classification,https://www.reddit.com/r/MachineLearning/comments/a4w8jn/implementing_resnet_with_mxnet_gluon_and_cometml/,ceceshao1,1544453141,,0,1,False,default,,,,,
565,MachineLearning,t5_2r3gv,2018-12-10,2018,12,10,23,a4wcaj,nautil.us,[D] Why robot brains need symbols,https://www.reddit.com/r/MachineLearning/comments/a4wcaj/d_why_robot_brains_need_symbols/,sinshallah,1544453918,,0,1,False,default,,,,,
566,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,0,a4wgb3,self.MachineLearning,[D] Best AI news in November?,https://www.reddit.com/r/MachineLearning/comments/a4wgb3/d_best_ai_news_in_november/,sizaka,1544454691,"I recently wrote an article to help anyone catch up with the latest news from the AI world. 

[https://blog.sicara.com/11-2018-best-ai-new-articles-this-month-a219efa105ba-8cf1a554e161](https://blog.sicara.com/11-2018-best-ai-new-articles-this-month-a219efa105ba-8cf1a554e161)

Anything I missed? I would really like some feedback on this as it is quite hard to keep up with the field even when you're actively monitoring it.

&amp;#x200B;",19,1,False,self,,,,,
567,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,0,a4wm60,youtube.com,Applications of AI,https://www.reddit.com/r/MachineLearning/comments/a4wm60/applications_of_ai/,RevolutionaryONEE,1544455842,,0,1,False,default,,,,,
568,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,0,a4wpkm,arxiv.org,"[R] Updated and expanded UMAP paper (algorithm descriptions, more explanation, more experiments)",https://www.reddit.com/r/MachineLearning/comments/a4wpkm/r_updated_and_expanded_umap_paper_algorithm/,lmcinnes,1544456494,,3,1,False,default,,,,,
569,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,1,a4wxf6,i.redd.it,Projeto Contra Incndio V,https://www.reddit.com/r/MachineLearning/comments/a4wxf6/projeto_contra_incndio_v/,JamurGerloff,1544457979,,0,1,False,https://a.thumbs.redditmedia.com/kIh3-OdnsS7Th03Dq-mQmRYNtM7iu85mj5GuV96IlN4.jpg,,,,,
570,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,1,a4wxoq,self.MachineLearning,[D] Can AI Atelier become a new paradigm to create art?,https://www.reddit.com/r/MachineLearning/comments/a4wxoq/d_can_ai_atelier_become_a_new_paradigm_to_create/,kh22l22,1544458032,"AI Atelier is a creative tool by which creators and AI collaborate to create artworks. AI Atelier is a combination of internet image montage and interactive style transfer. Internet image montage allows the creator to freely determine the content and composition of his / her desired image, thereby generating a base image. Interactive style transfer uses a style palette and a style brush to apply a pixel-wise style transfer to a base image, so that unlike the existing style transfer algorithm, you can create a myriad of style images from the same photo. It is still incomplete, but soon it will be added to the mix of style palettes and the ability to control the strength of the style.

Can AI Atelier become a new paradigm to create art?

AI Atelier Demo: [https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=408s](https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=408s)

The first AI artwork using AI Atelier: [https://www.youtube.com/watch?v=19VD5\_Dgx1o](https://www.youtube.com/watch?v=19VD5_Dgx1o)

Interactive Style Transfer using AI Atelier: [https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=39s](https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=39s)",0,1,False,self,,,,,
571,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,1,a4x1yx,medium.com,[P] Implementing ResNet with MXNET Gluon and Comet.ml for image classification,https://www.reddit.com/r/MachineLearning/comments/a4x1yx/p_implementing_resnet_with_mxnet_gluon_and/,gidime,1544458792,,2,1,False,https://b.thumbs.redditmedia.com/_g3NLe-DaqjtSjaGJc498F-jlwdhWCrmLWcBWdFniZY.jpg,,,,,
572,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,1,a4xdrq,github.com,[D] Visualize Python code execution (line-by-line) in Jupyter Notebook cells.,https://www.reddit.com/r/MachineLearning/comments/a4xdrq/d_visualize_python_code_execution_linebyline_in/,_quanttrader_,1544460946,,0,1,False,default,,,,,
573,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,2,a4xfz0,self.MachineLearning,[D] Can A.I. Atelier become a new paradigm to create arts?,https://www.reddit.com/r/MachineLearning/comments/a4xfz0/d_can_ai_atelier_become_a_new_paradigm_to_create/,kh22l22,1544461328," 

A.I. Atelier is a creative tool by which creators and AI collaborate to create artworks. A.I. Atelier is a combination of internet image montage and interactive style transfer. Internet image montage allows the creator to freely determine the content and composition of his / her desired image, thereby generating a base image. Interactive style transfer uses a style palette and a style brush to apply a pixel-wise style transfer to a base image, so that unlike the existing style transfer algorithm, you can create a myriad of style images from the same photo. It is still incomplete, but soon it will be added to the mix of style palettes and the ability to control the strength of the style.

Can A.I. Atelier become a new paradigm to create arts?

A.I. Atelier Demo: [https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=408s](https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=408s)

The first AI artwork using A.I. Atelier: [https://www.youtube.com/watch?v=19VD5\_Dgx1o](https://www.youtube.com/watch?v=19VD5_Dgx1o)

Interactive Style Transfer using A.I. Atelier: [https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=39s](https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=39s)",0,1,False,self,,,,,
574,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,2,a4xhqw,self.MachineLearning,[D] Can A.I. Atelier become a new paradigm to create arts?,https://www.reddit.com/r/MachineLearning/comments/a4xhqw/d_can_ai_atelier_become_a_new_paradigm_to_create/,kh22l22,1544461620,"A.I. Atelier is a creative tool by which creators and AI collaborate to create artworks. A.I. Atelier is a combination of internet image montage and interactive style transfer. Internet image montage allows the creator to freely determine the content and composition of his / her desired image, thereby generating a base image. Interactive style transfer uses a style palette and a style brush to apply a pixel-wise style transfer to a base image, so that unlike the existing style transfer algorithm, you can create a myriad of style images from the same photo. It is still incomplete, but soon it will be added to the mix of style palettes and the ability to control the strength of the style.

Can A.I. Atelier become a new paradigm to create arts?

A.I. Atelier Demo: [https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=408s](https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=408s)

The first AI artwork using A.I. Atelier: [https://www.youtube.com/watch?v=19VD5\_Dgx1o](https://www.youtube.com/watch?v=19VD5_Dgx1o)

Interactive Style Transfer using A.I. Atelier: [https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=39s](https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=39s)",0,1,False,self,,,,,
575,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,2,a4xi07,/r/MachineLearning/comments/a4xi07/machine_learning/,Machine... Learning?,https://www.reddit.com/r/MachineLearning/comments/a4xi07/machine_learning/,Rainymood_XI,1544461662,,0,1,False,https://b.thumbs.redditmedia.com/94g8rIjVPK7T_m3u_7hZLkvdtz7jdcPqlmhKwudkkYo.jpg,,,,,
576,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,2,a4xjby,self.MachineLearning,[D] Can A.I. Atelier become a new paradigm to create arts?,https://www.reddit.com/r/MachineLearning/comments/a4xjby/d_can_ai_atelier_become_a_new_paradigm_to_create/,kh22l22,1544461901,"A.I. Atelier is a creative tool by which creators and AI collaborate to create artworks. A.I. Atelier is a combination of an internet image montage and an interactive style transfer. Internet image montage allows the creator to freely determine the content and composition of his / her desired image, thereby generating a base image. Interactive style transfer uses style palettes and style brushes to apply a pixel-wise style transfer to a base image, so that unlike the existing style transfer algorithm, you can create a myriad of style images from the same photo. It is still incomplete, but soon it will be added to the mix of style palettes and the ability to control the strength of the style.

Can A.I. Atelier become a new paradigm to create arts?

A.I. Atelier Demo: [https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=425s](https://www.youtube.com/watch?v=fsOokweoWf8&amp;t=425s)

The first artwork using A.I. Atelier: [https://www.youtube.com/watch?v=19VD5\_Dgx1o&amp;t=40s](https://www.youtube.com/watch?v=19VD5_Dgx1o&amp;t=40s)

Interactive Style Transfer using A.I. Atelier:[https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=1s](https://www.youtube.com/watch?v=G-wZ7DurlMo&amp;t=1s)",1,1,False,self,,,,,
577,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,2,a4xk2t,self.MachineLearning,[P] The unsupervised learning chapter of THPMLB is out,https://www.reddit.com/r/MachineLearning/comments/a4xk2t/p_the_unsupervised_learning_chapter_of_thpmlb_is/,RudyWurlitzer,1544462029,"The draft of Chapter 9 ""Unsupervised Learning"" of my book is [now online](http://themlbook.com). It covers the following topics: density estimation, clustering, dimensionality reduction, and outlier detection.

Since it's a draft, it's not perfect. If you read it and see an opportunity to improve the text, please let me know! The names of the most active contributors will be mentioned in the book.",0,1,False,self,,,,,
578,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,2,a4xw1v,self.MachineLearning,[P] Help with model configuration,https://www.reddit.com/r/MachineLearning/comments/a4xw1v/p_help_with_model_configuration/,computer_crisps,1544464124,"
TL;DR: How do I configure a stock price predicting regression model?

Im fairly new at machine learning and Im stuck at solving a stock-price prediction regression model.

My approach is to collect stock data (~1M entries) that includes several popular indicators, a few derived indicators and a couple of indicators I developed myself. Theres 84 features and the target is the following days percentual change. Ive randomized normalized and thoroughly checked the data.
Since its not a time series dataset and Im just getting started with ML, I followed the steps of a house price prediction model I found online and settled for a supervised regression model. Ive tried several Sklearn and Keras configurations so far.

My main problem is that despite the various adjustments in optimizers (Keras), network shapes and depths, learning rates, activation functions, etc., I cant get the model to drop below a ~5% mean absolute error. Worse still, the results are often very similar after changing several parameters, and the learning curves look flat, so Ive gone clueless on what to change.
Im also concerned about the fact that some models simply assign an arbitrary value to all predictions, usually close to zero, so that zero-hovering targets (change percentages) are, on average, close enough.

Am I hitting the limit for what my data can offer or am I missing something? Where would you start?
Am I being incredibly naf for trying to predict stock market prices with my limited ML knowledge?

[Code screenshots [here](https://imgur.com/a/T6Lw7TA)]

Thanks (a lot!) in advance.",3,1,False,self,,,,,
579,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,3,a4y5mp,self.MachineLearning,[P] MobileNet Object classifier Demo in the browser,https://www.reddit.com/r/MachineLearning/comments/a4y5mp/p_mobilenet_object_classifier_demo_in_the_browser/,GamerMinion,1544465786,"Hello everyone,

I recently built a quick demo for a MobileNet-based object classifier in the browser and I want to share it here.

https://pyrestone.ai/projects/tfjs_mobilenet/

*Note:* If you are on a mobile device, please enable WiFi to avoid data charges.

If you encounter any problems with the site, please send me a message.",2,1,False,self,,,,,
580,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,3,a4y84w,self.MachineLearning,10 Great Articles about Stochastic Processes and Related Topics,https://www.reddit.com/r/MachineLearning/comments/a4y84w/10_great_articles_about_stochastic_processes_and/,andrea_manero,1544466224,[removed],0,1,False,self,,,,,
581,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,3,a4ybti,self.MachineLearning,[R] Why Deep Learning Works: Implicit Self-Regularization in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a4ybti/r_why_deep_learning_works_implicit/,downtownslim,1544466849,"Short Summary:
&gt; DNN optimization with larger batch sizes leads to less-well implicitly-regularized models, and it provides an explanation for the generalization gap phenomena.


https://www.youtube.com/watch?v=bd19nrIYlTc

Information Page: https://simons.berkeley.edu/talks/9-24-mahoney-deep-learning",4,1,False,self,,,,,
582,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,3,a4yimk,medium.com,10 AI Failures in 2018,https://www.reddit.com/r/MachineLearning/comments/a4yimk/10_ai_failures_in_2018/,gwen0927,1544468016,,0,1,False,https://a.thumbs.redditmedia.com/-jQu3hDA2sCh_gZuwunRULDIk0HO562y93ZOQKHZvd8.jpg,,,,,
583,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,4,a4yme7,self.MachineLearning,How to take more than one features as input and predict one value or multiple value using LSTM?,https://www.reddit.com/r/MachineLearning/comments/a4yme7/how_to_take_more_than_one_features_as_input_and/,samsadsajid,1544468652,[removed],0,1,False,self,,,,,
584,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,4,a4yp70,sites.google.com,[R] From Waymo Research - ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst,https://www.reddit.com/r/MachineLearning/comments/a4yp70/r_from_waymo_research_chauffeurnet_learning_to/,ceceshao1,1544469128,,0,1,False,default,,,,,
585,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,4,a4yqgf,ai.stanford.edu,CariGANs: Unpaired Photo-to-Caricature Translation,https://www.reddit.com/r/MachineLearning/comments/a4yqgf/carigans_unpaired_phototocaricature_translation/,worldwide__master,1544469344,,0,1,False,default,,,,,
586,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,4,a4ywrh,self.MachineLearning,[R] Applied Federated Learning: Improving Google Keyboard Query Suggestions,https://www.reddit.com/r/MachineLearning/comments/a4ywrh/r_applied_federated_learning_improving_google/,ranihorev,1544470428,"Google released a new paper on how they trained the query suggestions in their keyboard. They described the model architecture and the challenges of building a large-scale federated learning algorithm (to keep the user privacy).

[https://arxiv.org/pdf/1812.02903v1.pdf](https://arxiv.org/pdf/1812.02903v1.pdf)

&amp;#x200B;",0,1,False,self,,,,,
587,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,5,a4zkri,self.MachineLearning,Machine Learning meets quantum computing,https://www.reddit.com/r/MachineLearning/comments/a4zkri/machine_learning_meets_quantum_computing/,TheLolguy11,1544474678,[removed],0,1,False,self,,,,,
588,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,6,a5041p,self.MachineLearning,What are your thoughts on the Deep Learning and Machine Learning books from Manning Publications?,https://www.reddit.com/r/MachineLearning/comments/a5041p/what_are_your_thoughts_on_the_deep_learning_and/,UnfazedButDazed,1544477964,[removed],0,1,False,self,,,,,
589,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,6,a506yr,self.MachineLearning,Memory limit for Neural Style Transfer using VGG-19 model?,https://www.reddit.com/r/MachineLearning/comments/a506yr/memory_limit_for_neural_style_transfer_using/,wandadars,1544478486,[removed],1,1,False,self,,,,,
590,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,6,a507ae,self.MachineLearning,AI fellowship application invitation,https://www.reddit.com/r/MachineLearning/comments/a507ae/ai_fellowship_application_invitation/,VinayUPrabhu,1544478547,[removed],0,1,False,self,,,,,
591,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,7,a50i7i,self.MachineLearning,[D] Is there an activation function which would combine the effects of ELUs and leaky ReLUs / PReLUs?,https://www.reddit.com/r/MachineLearning/comments/a50i7i/d_is_there_an_activation_function_which_would/,tsskyx,1544480545,"In short, the leaky ReLU function is `y = x` when `x &gt;= 0` and `y = 0.01x` when `x &lt; 0`. The ELU function is the same for `x &gt;= 0` but for `x &lt; 0`, it takes the form of `y = a(e^x-1)`.

The one good thing about leaky ReLU is that its gradient never vanishes. For ELU however, the gradient tends towards 0 as x gets smaller and smaller. But then again, ELU has been shown to perform better.

What if we were to combine the two? I.e., do `y = a(e^x-1)+0.01x {x &lt; 0}`, or even better, introduce an extra parameter ""b"" for the slope of the linear term from the PReLU activation function: `y = a(e^x-1)+bx {x &lt; 0}`.

My question is, has such an activation function been implemented somewhere yet? If yes, what was it named and what were the results?",9,1,False,self,,,,,
592,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,8,a50v57,self.MachineLearning,[R] Survey | Academic Research on Data activities in business (3 mins max with 10 questions),https://www.reddit.com/r/MachineLearning/comments/a50v57/r_survey_academic_research_on_data_activities_in/,CalmLetterhead,1544482946," Hey redditors !!

I am working on an academic research and looking for collecting insights from the data scientist community. I am interested to learn how organizations performing their data activities, including, preparation, analysis and quality assessment - 

[https://docs.google.com/forms/d/e/1FAIpQLSejvjuLSnxiJzX0Rwo1d4LpNgp03KvtCVKHuAc7vDOx\_DtEmw/viewform](https://docs.google.com/forms/d/e/1FAIpQLSejvjuLSnxiJzX0Rwo1d4LpNgp03KvtCVKHuAc7vDOx_DtEmw/viewform)

Any help is greatly appreciated!",0,1,False,self,,,,,
593,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,8,a50x5i,self.MachineLearning,"Inexpensive deep learning hardware on the cloud via Shadow (GTX 1080 or equivalent, 12 GB RAMm 256 GB storage and 1Gbps internet) for $15 a month",https://www.reddit.com/r/MachineLearning/comments/a50x5i/inexpensive_deep_learning_hardware_on_the_cloud/,LethophobicKarma,1544483326,"Shadow is a cloud gaming platform which offers a subscription service. Right now, the rate has been slashed from $35 to $25 a month. With the coupon LTT, you can get another $10 off, taking it down to $15.",1,1,False,self,,,,,
594,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,8,a50yq5,mrl.snu.ac.kr,Aerobatics Control of Flying Creatures via Self-Regulated Learning,https://www.reddit.com/r/MachineLearning/comments/a50yq5/aerobatics_control_of_flying_creatures_via/,ai_is_matrix_mult,1544483641,,0,1,False,default,,,,,
595,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,8,a510ak,self.MachineLearning,"[D] Inexpensive deep learning hardware on the cloud via Shadow (GTX 1080 or equivalent, 12 GB RAM, 256 GB storage and 1Gbps internet) for $15 a month",https://www.reddit.com/r/MachineLearning/comments/a510ak/d_inexpensive_deep_learning_hardware_on_the_cloud/,LethophobicKarma,1544483927,"Shadow is a cloud gaming platform which offers a subscription service. Right now, the rate has been slashed from $35 to $25 a month. With the coupon LTT, you can get another $10 off, taking it down to $15.  
Its not available everywhere. You can check if it is available to you [here](https://shop.shadow.tech/usen).
",2,1,False,self,,,,,
596,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,9,a51j4a,arxiv.org,[R] ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst,https://www.reddit.com/r/MachineLearning/comments/a51j4a/r_chauffeurnet_learning_to_drive_by_imitating_the/,ploutone,1544487518,,6,1,False,default,,,,,
597,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,9,a51k2m,ai.googleblog.com,Providing Gender-Specific Translations in Google Translate,https://www.reddit.com/r/MachineLearning/comments/a51k2m/providing_genderspecific_translations_in_google/,sjoerdapp,1544487729,,0,1,False,default,,,,,
598,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,9,a51oxb,self.MachineLearning,Symbiotic Learning,https://www.reddit.com/r/MachineLearning/comments/a51oxb/symbiotic_learning/,mrinalsourav,1544488733,[removed],1,1,False,self,,,,,
599,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,9,a51s1m,gengo.ai,AI predictions for 2019,https://www.reddit.com/r/MachineLearning/comments/a51s1m/ai_predictions_for_2019/,reimmoriks,1544489378,,0,1,False,https://b.thumbs.redditmedia.com/xZuZ-ZeVrlPmbuQ9l__bFJ4Ao_H3EsgiVHE81YAEHxA.jpg,,,,,
600,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,11,a52k0p,github.com,practicalAI notebooks - A practical approach to learning machine learning,https://www.reddit.com/r/MachineLearning/comments/a52k0p/practicalai_notebooks_a_practical_approach_to/,peterkuharvarduk,1544495187,,0,1,False,default,,,,,
601,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,11,a52nfy,self.MachineLearning,[D] Does ML and Geology match? here's a $1m competition.,https://www.reddit.com/r/MachineLearning/comments/a52nfy/d_does_ml_and_geology_match_heres_a_1m_competition/,huabamane,1544495878,"Just came across this competition, but I'm having a hard time getting my head around it. I'm neither an expert in ML, nor Geology, but my understanding is that you'd need to have a lot of data points to train a ML model. Given that geology is a somewhat inaccurate science (based on a lot of theories of how things take shape) means the set of instructions (especially for finding new types of deposits) will be ambiguous, which would make it hard to build a model. On top of that, a ML approach would need a lot of successful targets to train on. Given that there aren't huge numbers of successful discoveries, how would someone have enough targets to build a good model? 

Do you guys think this could be approached from the ML side only, or would it always require a strong geological background?",11,1,False,self,,,,,
602,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,11,a52odr,self.MachineLearning,[D] Help with generating entire vector font set given one or a few characters,https://www.reddit.com/r/MachineLearning/comments/a52odr/d_help_with_generating_entire_vector_font_set/,carsonpoole,1544496072,"So my end goal is to be able to give some kind of model one or two vector characters and have the model generate the entire font set. I'm assuming this should be possible, as we all can kind of do this in our head when we see some billboard or poster, and the data should be pretty easy to find--there're hundreds of online font sites with free fonts and there's Google web fonts as well.

&amp;#x200B;

I need help with pointing me in the right direction. I could see this as a use for GANs or VAEs or maybe something else. I'm still relatively new to this kind of thing so I want to use it as a learning experience. General thoughts and advice/help would be greatly appreciated! Thanks",9,1,False,self,,,,,
603,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,11,a52pf4,self.MachineLearning,"Which ML Algorithm to use, if dependent variable has more categorical data (like 9 categories in dependent variable)?",https://www.reddit.com/r/MachineLearning/comments/a52pf4/which_ml_algorithm_to_use_if_dependent_variable/,Varunkrishna,1544496292,"Can anyone give some suggestions, about how to proceed with this kind of problem - ML Newbie Request...!!! ",0,1,False,self,,,,,
604,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,11,a52pjw,self.MachineLearning,[Request] Funding &amp; Collaboration Marketplace for Computer Science Research,https://www.reddit.com/r/MachineLearning/comments/a52pjw/request_funding_collaboration_marketplace_for/,JordanCampbellNebula,1544496321,"Hi all,

&amp;#x200B;

I'm building [Frontiers](https://frontiers.astro.codes) \- a place where computer scientists can meet potential funders and collaborators (think Patreon / Angel List, but specifically for computer science research).

&amp;#x200B;

I'm looking to talk to anyone that is currently doing any form of research and is looking for money and / or collaborators. Any backgrounds, any topics, any levels etc.

&amp;#x200B;

Thank you in advance for your time!

&amp;#x200B;

Survey: [https://goo.gl/forms/dNlxSSOchxryvI303](https://goo.gl/forms/dNlxSSOchxryvI303)

Email: [jordan@astro.codes](mailto:jordan@astro.codes)

&amp;#x200B;",0,1,False,self,,,,,
605,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,12,a539gj,self.MachineLearning,[P] Tensorflow Replacement? Google Releases Jax Library,https://www.reddit.com/r/MachineLearning/comments/a539gj/p_tensorflow_replacement_google_releases_jax/,MassivePellfish,1544500433,"Looks like they decided to go to a *much* simpler framework which is basically numpy with autograd.
&gt; Whats new is that JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs.

Link: https://github.com/google/jax",11,1,False,self,,,,,
606,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,14,a53z0t,self.datascience,Prescriptive Maintenance for Manufacturing Industry using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a53z0t/prescriptive_maintenance_for_manufacturing/,parthadeka,1544506277,,0,1,False,https://b.thumbs.redditmedia.com/SQq6ikAXb-7AZWhkkyzaIiWPcG0CkY6auHAGYc03pfk.jpg,,,,,
607,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,15,a54cul,self.MachineLearning,A question about deep learning,https://www.reddit.com/r/MachineLearning/comments/a54cul/a_question_about_deep_learning/,jacob8015,1544509770,[removed],0,1,False,self,,,,,
608,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,16,a54jf1,self.MachineLearning,What does an AI camera do?,https://www.reddit.com/r/MachineLearning/comments/a54jf1/what_does_an_ai_camera_do/,chhab798,1544511634,[removed],0,1,False,self,,,,,
609,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,16,a54kp8,github.com,PyTorch to Keras deep neural network model converter,https://www.reddit.com/r/MachineLearning/comments/a54kp8/pytorch_to_keras_deep_neural_network_model/,nerox8664,1544511981,,0,1,False,default,,,,,
610,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,16,a54l72,self.MachineLearning,Playing Pong with 3 hidden states,https://www.reddit.com/r/MachineLearning/comments/a54l72/playing_pong_with_3_hidden_states/,HeavyStatus4,1544512121,[removed],0,1,False,self,,,,,
611,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,16,a54qoc,self.MachineLearning,Are there good suggestions for drawing an illustration in a paper?,https://www.reddit.com/r/MachineLearning/comments/a54qoc/are_there_good_suggestions_for_drawing_an/,Luolc,1544513717,"Hi there!

&amp;#x200B;

I am an undergrad and have already published several papers on AI conferences. I found that drawing the illustration of models is a big challenge for me. There are many good examples in existing papers, such as Attention is All You Need [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf). The illustration in it is not complicated, but very pretty. But I don't know how to draw an illustration like this. What software should I use? What skills should I learn?

&amp;#x200B;

Thanks!",0,1,False,self,,,,,
612,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,17,a550s9,ello.co,"Machine Vision Market Share A new research report by imarc group, global machine vision market share expected to reach ...",https://www.reddit.com/r/MachineLearning/comments/a550s9/machine_vision_market_share_a_new_research_report/,liveSmith,1544516796,,0,1,False,default,,,,,
613,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,17,a555h3,self.MachineLearning,How good is the University of South Florida (USA) for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/a555h3/how_good_is_the_university_of_south_florida_usa/,Kyak787,1544518258,"I was hoping that someone with sufficient knowledge about Machine Learning could look into USF's computer science Master's program and let me know how well it would prepare me for employment in Machine Learning?

I have family there that I could live with, and I'm in state, so going there would be extraordinarily cheap compared to any other university.

Thank you.",0,1,False,self,,,,,
614,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,17,a555zr,arxiv.org,[R] Neuromodulated Learning in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a555zr/r_neuromodulated_learning_in_deep_neural_networks/,d9w,1544518418,,0,1,False,default,,,,,
615,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,17,a555zv,self.MachineLearning,"Simple Tensorflow implementation of NVIDIA "" Partial Convolution based Padding""",https://www.reddit.com/r/MachineLearning/comments/a555zv/simple_tensorflow_implementation_of_nvidia/,taki0112,1544518418,[removed],1,1,False,https://b.thumbs.redditmedia.com/TB3JyYxWyACmpdA714pAGT3AA0af-N7MlpvnK5HrzpA.jpg,,,,,
616,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,17,a5570k,self.MachineLearning,"[P] Simple Tensorflow implementation of NVIDIA ""Partial Convolution based Padding""",https://www.reddit.com/r/MachineLearning/comments/a5570k/p_simple_tensorflow_implementation_of_nvidia/,taki0112,1544518748,"&amp;#x200B;

![img](lm505aco5m321)",25,1,False,self,,,,,
617,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,18,a557yy,towardsdatascience.com,Convert Your ML Models to a fully abstracted drag-and-drop Blocks,https://www.reddit.com/r/MachineLearning/comments/a557yy/convert_your_ml_models_to_a_fully_abstracted/,ibrahimessam,1544519040,,0,1,False,default,,,,,
618,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,19,a55k54,self.MachineLearning,Trouble implementing a kNN Image classifier,https://www.reddit.com/r/MachineLearning/comments/a55k54/trouble_implementing_a_knn_image_classifier/,shyam_sundar19,1544522805,[removed],0,1,False,self,,,,,
619,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,19,a55kn0,self.MachineLearning,Is it learning the prior distribution of a variational autoencoder (VAE) or any other graphical model dangerous?,https://www.reddit.com/r/MachineLearning/comments/a55kn0/is_it_learning_the_prior_distribution_of_a/,jmaronasm,1544522951,[removed],2,1,False,self,,,,,
620,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,19,a55mf4,self.MachineLearning,My First Neural Network,https://www.reddit.com/r/MachineLearning/comments/a55mf4/my_first_neural_network/,nexriz,1544523461,[removed],0,1,False,self,,,,,
621,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,19,a55o5r,self.MachineLearning,Using BERT for text summarization (and other seq2seq tasks),https://www.reddit.com/r/MachineLearning/comments/a55o5r/using_bert_for_text_summarization_and_other/,MLFencer,1544523999,[removed],0,1,False,self,,,,,
622,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a55wg3,youtube.com,[D] Ellen Friedman  Beyond the Algorithm: What Makes Machine Learning Work? (Berlin Buzzwords 2018),https://www.reddit.com/r/MachineLearning/comments/a55wg3/d_ellen_friedman_beyond_the_algorithm_what_makes/,newthinkingevents,1544526562,,0,1,False,default,,,,,
623,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a55xd0,docs.google.com,Frontiers in Natural Language Processing: Expert Responses,https://www.reddit.com/r/MachineLearning/comments/a55xd0/frontiers_in_natural_language_processing_expert/,Isinlor,1544526813,,1,1,False,default,,,,,
624,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a5615i,self.MachineLearning,"Measure, Manifold, Learning, and Optimization: A Theory Of Neural Networks",https://www.reddit.com/r/MachineLearning/comments/a5615i/measure_manifold_learning_and_optimization_a/,shawnLeeZX,1544527969,[removed],0,1,False,self,,,,,
625,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a561or,developer.nvidia.com,Benchmark from NVidia Shows MXNet outperforming Pytorch and Tensorflow,https://www.reddit.com/r/MachineLearning/comments/a561or/benchmark_from_nvidia_shows_mxnet_outperforming/,parfamz,1544528136,,0,1,False,default,,,,,
626,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a5640f,github.com,PracticalAI PyTorch Notebooks: A practical approach to learning machine learning,https://www.reddit.com/r/MachineLearning/comments/a5640f/practicalai_pytorch_notebooks_a_practical/,peterkuharvarduk,1544528793,,0,1,False,https://b.thumbs.redditmedia.com/7-E3CVWTRFh6Rf-raG51hsz2mKDQSDuPu39JFLTCHVM.jpg,,,,,
627,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a5653l,self.MachineLearning,[P] awesome sentence embedding: A curated list of pretrained sentence embedding models,https://www.reddit.com/r/MachineLearning/comments/a5653l/p_awesome_sentence_embedding_a_curated_list_of/,Separius12,1544529106,"Hi guys,

I've compiled a pretty huge and up to date list of all word embeddings and sentence embeddings with pretrained models, you can access it [here](https://github.com/Separius/awesome-sentence-embedding).

please let me know if something's missing or there is a typo or anything else.

Thanks! :))",11,1,False,self,,,,,
628,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a565q2,self.MachineLearning,Pretrained models for nlp tasks,https://www.reddit.com/r/MachineLearning/comments/a565q2/pretrained_models_for_nlp_tasks/,textMinier,1544529280,[removed],0,1,False,self,,,,,
629,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,20,a5663k,i.redd.it,"Zo, Microsoft's LSTM chatbot",https://www.reddit.com/r/MachineLearning/comments/a5663k/zo_microsofts_lstm_chatbot/,Aerosherm,1544529384,,0,1,False,https://b.thumbs.redditmedia.com/QsxgX-AOGDw5MF2qrpNitLi4Jc2Fb_3sgMpIxbsEPQo.jpg,,,,,
630,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,21,a56gex,self.MachineLearning,Whats the Difference between probabilistic programming such as pyro and Deep belief networks?,https://www.reddit.com/r/MachineLearning/comments/a56gex/whats_the_difference_between_probabilistic/,Apsylem,1544531998,[removed],0,1,False,self,,,,,
631,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,21,a56i14,self.MachineLearning,Generating Data from a given data set in Python,https://www.reddit.com/r/MachineLearning/comments/a56i14/generating_data_from_a_given_data_set_in_python/,White94,1544532420,[removed],0,1,False,self,,,,,
632,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,22,a56olg,self.MachineLearning,Are bidirectional lstm susceptible to correlated input data?,https://www.reddit.com/r/MachineLearning/comments/a56olg/are_bidirectional_lstm_susceptible_to_correlated/,aziz_22,1544533984,"Working on speech-recognition tasks.

I have processed my input files using two techniques (Filter-bank and MFCC) and the results showed me that my model is generalizing better when the input features are processed using MFCC.

Going back to the literature, I have read that it is better to use Mel-scaled filter banks if the machine learning algorithm is not susceptible to highly correlated input and  use MFCCs if the machine learning algorithm is susceptible to correlated input.

&amp;#x200B;

To be honest, I can't confirm that a seq2seq model could be susceptible to correlated input data but in the other hand I can't understand why this is the case?

&amp;#x200B;

In attachment you find both my training loss and my validation loss using MFCC/FBANk.

The model is the Listen-attend-and-spell.

&amp;#x200B;

&amp;#x200B;

![img](3od5f72ven321)

![img](t5azc82ven321)",0,1,False,self,,,,,
633,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,22,a56paa,dimensionless.in,Machine Learning (ML) Essentials,https://www.reddit.com/r/MachineLearning/comments/a56paa/machine_learning_ml_essentials/,divya2018,1544534143,,0,1,False,default,,,,,
634,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,22,a56uka,self.MachineLearning,"[R] BERT, Pretranied Deep Bidirectional Transformers for Language Understanding",https://www.reddit.com/r/MachineLearning/comments/a56uka/r_bert_pretranied_deep_bidirectional_transformers/,tdls_to,1544535328,"**Algorithm Review:** [https://youtu.be/BhlOGGzC0Q0](https://youtu.be/BhlOGGzC0Q0)

**Paper Discussion:** [https://youtu.be/rMQMHA-uv\_E](https://youtu.be/rMQMHA-uv_E)

**Paper Reference:** [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)

We recently reviewed the BERT paper where the authors use the encoder part of a transformer mechanism to obtain representations that are ""generalizable"" for several tasks. In summary:

* BERT is a strong pre-trained language model that uses bidirectional transformers
   * Trained on two novel language modelling tasks
* BERT may be fine-tuned to beat many SOTA results on various NLP tasks

**Some of our discussion points:**

1. Could you combine this with a stacked, pre-trained transformer decoder for machine translation? (Does this make sense?)
2. Will we need to train models from scratch in the future if this is really generalizable?
3. In ablation studies between BERT and OpenAI GPT, the authors never examine the effect of adding the Wikipedia corpus to the training data. In their ""LTR &amp; No NSP"" ablation study,  they acknowledge that the training data is different, but don't seem to consider this significant. How well would OpenAI GPT do with this additional training data, and how well would BERT do without it?
4.  Some of their justifications seem to lie on very slight numerical differences. For example they state that 1M pretraining steps was necessary to achieve higher accuracy and that BERT\_base achieves almost 1% additional accuracy on MNLI when trained on 1M steps compared to 500k steps. First of all, from the graph the difference looks less than ""almost 1%"", and the difference is even less significant between 600k and 1M steps. Similarly when they are comparing using the model in a feature-based approach they find that the best feature aggregation method achieves only 0.3 less F1 than the model that fine-tunes parameters on a downstream task. Do these results still validate the efficacy of extensive training steps and fine tuning?
5. In the paper they mask randomly 15% of the tokens and from those 10% are replaced with random word or a synonym. What is the importance of that step? Does leaving it out results in a small performance decrease or in complete failure? Is this only to create negative examples?
6. They showed the model can be used to various NLP problems. Would it also work on other sequence based problems that are not necessarily NLP. For example on customer transaction data?

**Paper Abstract:**

We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. 

BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%

&amp;#x200B;",1,1,False,self,,,,,
635,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,23,a57asv,self.MachineLearning,I'd like to know your opinion about my popular science article.,https://www.reddit.com/r/MachineLearning/comments/a57asv/id_like_to_know_your_opinion_about_my_popular/,molchevsky,1544538898,"Hi Guys. I'd like to know your opinion. 

I wrote this article to show in a simple and exciting form how the Markov chains algorithm works. I believe it could be useful for explanation of this subject and capabilities of such algorithms for people who are not IT professionals. If you find it good enough I could write something like this about other AI algorithms like neural networks, genetic algorithms and so on. If you have any ideas how to improve it I'd like to read them.

[https://medium.com/@molchevsky/harry-potter-and-the-markov-chains-44429527b5d1](https://medium.com/@molchevsky/harry-potter-and-the-markov-chains-44429527b5d1)

&amp;#x200B;",0,1,False,self,,,,,
636,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,23,a57dr0,self.MachineLearning,[D] I'd like to know your opinion about my popular science article.,https://www.reddit.com/r/MachineLearning/comments/a57dr0/d_id_like_to_know_your_opinion_about_my_popular/,molchevsky,1544539518,"Hi Guys. I'd like to know your opinion. 

I wrote this article to show in a simple and exciting form how the Markov chains algorithm works. I believe it could be useful for explanation of this subject and capabilities of such algorithms for people who are not IT professionals. If you find it good enough I could write something like this about other AI algorithms like neural networks, genetic algorithms and so on. If you have any ideas how to improve it I'd like to read them.

[https://medium.com/@molchevsky/harry-potter-and-the-markov-chains-44429527b5d1](https://medium.com/@molchevsky/harry-potter-and-the-markov-chains-44429527b5d1)

&amp;#x200B;",2,1,False,self,,,,,
637,MachineLearning,t5_2r3gv,2018-12-11,2018,12,11,23,a57dvk,self.MachineLearning,What are the minimal (or recommended) hardware requirements for machine learning? Also wich OS is recommended?,https://www.reddit.com/r/MachineLearning/comments/a57dvk/what_are_the_minimal_or_recommended_hardware/,ArtificialReddit,1544539541,[removed],0,1,False,self,,,,,
638,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,0,a57mr8,self.MachineLearning,Have there been any attempts to use a GAN to analyze a spectrogram and generate audio?,https://www.reddit.com/r/MachineLearning/comments/a57mr8/have_there_been_any_attempts_to_use_a_gan_to/,tacit1000000,1544541288,[removed],0,1,False,https://b.thumbs.redditmedia.com/N1bf91TLcuRXYXVp_kzyfL2aJNtTLtNiBxYoGXkekgg.jpg,,,,,
639,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,0,a57qo6,self.MachineLearning,Neural network calculation,https://www.reddit.com/r/MachineLearning/comments/a57qo6/neural_network_calculation/,jinxasils,1544542019,"Dont know if this is the right forum but I come across a neural network question that I dont understand how to calculate.

https://i.redd.it/0gj5tknr2o321.png

1. How to calculate the output of H1 and H2.

2. How to calculate the output of Node 1.

&amp;#x200B;

Thank you.",0,1,False,https://b.thumbs.redditmedia.com/Qikh0Zkkw4akz4pLIAJUe6FeMrJqjsp9mD8GqrxqEQA.jpg,,,,,
640,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,0,a57sqy,self.MachineLearning,[D] Article series on neural networks,https://www.reddit.com/r/MachineLearning/comments/a57sqy/d_article_series_on_neural_networks/,p_bogdan,1544542389,"Hello!

I am starting a series of [blog posts on neural networks](http://penkovsky.com/post/neural-networks/) in Haskell. I would appreciate any feedback from both beginners and more experienced folks. Also I would be very curios to know which practical problems are you trying to solve with neural networks. Perhaps, I could provide some illustrations.

Thanks",5,1,False,self,,,,,
641,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,0,a57tjf,techgrabyte.com,"Meet world's first AI driven wheelchair from Intel And Hoobox Robotics, which you can control just by your facial expression",https://www.reddit.com/r/MachineLearning/comments/a57tjf/meet_worlds_first_ai_driven_wheelchair_from_intel/,navin49,1544542539,,0,1,False,default,,,,,
642,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,0,a57u3e,github.com,GitHub - GokuMohandas/practicalAI: A practical approach to learning machine learning.,https://www.reddit.com/r/MachineLearning/comments/a57u3e/github_gokumohandaspracticalai_a_practical/,peterkuharvarduk,1544542639,,0,1,False,https://b.thumbs.redditmedia.com/7-E3CVWTRFh6Rf-raG51hsz2mKDQSDuPu39JFLTCHVM.jpg,,,,,
643,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,0,a57vs5,arxiv.org,[R] The effects of negative adaptation in Model-Agnostic Meta-Learning: the adaptation in an algorithm like MAML can significantly decrease the performance of an agent in a meta-reinforcement learning setting,https://www.reddit.com/r/MachineLearning/comments/a57vs5/r_the_effects_of_negative_adaptation_in/,downtownslim,1544542952,,0,1,False,default,,,,,
644,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,0,a580eb,amitray.com,7 Core Qubit Technologies for Quantum Computing - Amit Ray,https://www.reddit.com/r/MachineLearning/comments/a580eb/7_core_qubit_technologies_for_quantum_computing/,audreykelly1,1544543814,,1,1,False,default,,,,,
645,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,1,a5868e,self.MachineLearning,Scalable IoT ML Platform with Apache Kafka + Deep Learning + MQTT,https://www.reddit.com/r/MachineLearning/comments/a5868e/scalable_iot_ml_platform_with_apache_kafka_deep/,andrea_manero,1544544870,[removed],0,1,False,self,,,,,
646,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,1,a589dq,self.MachineLearning,Cleaning up video,https://www.reddit.com/r/MachineLearning/comments/a589dq/cleaning_up_video/,SofaKingTired,1544545434,[removed],0,1,False,https://a.thumbs.redditmedia.com/nQcniOkLPyxwqFKrmDGGenOqGaIaMK2Uv_i8bTG5hw0.jpg,,,,,
647,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,1,a58fts,towardsdatascience.com,A gentle journey from linear regression to neural networks,https://www.reddit.com/r/MachineLearning/comments/a58fts/a_gentle_journey_from_linear_regression_to_neural/,joseph_rocca,1544546549,,0,1,False,default,,,,,
648,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,1,a58l7n,self.MachineLearning,Machine Learning Jump Start: Watch the first chapter for FREE,https://www.reddit.com/r/MachineLearning/comments/a58l7n/machine_learning_jump_start_watch_the_first/,LordTribual,1544547495,[removed],0,1,False,self,,,,,
649,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a58m94,i.redd.it,GERENCIAMENTO DE CRISES V,https://www.reddit.com/r/MachineLearning/comments/a58m94/gerenciamento_de_crises_v/,JamurGerloff,1544547682,,0,1,False,https://b.thumbs.redditmedia.com/Sa_1R9YbC06Ciqwew3codG-Ag8WSMDtqHMdgkXcjbso.jpg,,,,,
650,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a58o62,self.MachineLearning,[D] Character Level NLP models,https://www.reddit.com/r/MachineLearning/comments/a58o62/d_character_level_nlp_models/,TalkingJellyFish,1544547986,"Hi, 

We ([LightTag](https://www.lighttag.io))recently did an internal review of character level models and their alternatives, which turned into a [blog post](https://www.lighttag.io/blog/character-level-NLP/). 

The tl;dr is character level models help with large vocabularies and reduce the expense of pre/cotraining tasks but lose the semantics of word embeddings. Their are a few alternatives that address either of these two dimensions but not one that hits both. ",2,1,False,self,,,,,
651,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a58ui6,self.MachineLearning,Applying Machine Learning on a Maintenance Department,https://www.reddit.com/r/MachineLearning/comments/a58ui6/applying_machine_learning_on_a_maintenance/,askright,1544549079,[removed],0,1,False,self,,,,,
652,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a58y6r,self.MachineLearning,Applying Machine Learning to Maintenance,https://www.reddit.com/r/MachineLearning/comments/a58y6r/applying_machine_learning_to_maintenance/,work_weld,1544549725,[removed],0,1,False,self,,,,,
653,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a58z38,self.MachineLearning,#1 Trending Repo on Github is an ML notebook repo,https://www.reddit.com/r/MachineLearning/comments/a58z38/1_trending_repo_on_github_is_an_ml_notebook_repo/,peterkuharvarduk,1544549880,"[https://github.com/trending](https://github.com/trending)

When was the last time this happened? Theano and TensorFlow initial release maybe?",1,1,False,self,,,,,
654,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a58zvi,self.artificial,[R] Fast Algorithm for Category Construction,https://www.reddit.com/r/MachineLearning/comments/a58zvi/r_fast_algorithm_for_category_construction/,Feynmanfan85,1544550015,,0,1,False,https://b.thumbs.redditmedia.com/jommKnY1gZyNvKaBSMeQ69xs92NdBcXgDPWNDnc-AhU.jpg,,,,,
655,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a590ms,self.MachineLearning,[P] matrixprofile-ts: a Python package for the Matrix Profile algorithm,https://www.reddit.com/r/MachineLearning/comments/a590ms/p_matrixprofilets_a_python_package_for_the_matrix/,cypa11,1544550146,"Hi all,

   My team at Target has open-sourced a [Python implementation](https://github.com/target/matrixprofile-ts) of Matrix Profile called matrixprofile-ts, complete with availability on [pip](https://pypi.org/project/matrixprofile-ts/). u/eamonnkeogh and others have discussed Matrix Profile previously, but it's a highly-performant, robust method of identifying patterns and anomalies present in time-series data. We've written a [blog post](https://tech.target.com/2018/12/11/matrix-profile.html) that describes the algorithm in more detail, but we'd love to get some community feedback. Right now our implementation includes STAMP, STAMP-sampling, STAMPi and the annotation vector. We've done additional work on STOMP-GPU that we hope to release shortly. We've been able to analyze \~20 years' worth of data (5 minute samples) in under 20 seconds with STOMP-GPU, so we're particularly exited for that addition. Try it out and pass along your thoughts!",2,1,False,self,,,,,
656,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a591gs,self.MachineLearning,[D] #1 Trending Repo on Github is an ML notebook repo,https://www.reddit.com/r/MachineLearning/comments/a591gs/d_1_trending_repo_on_github_is_an_ml_notebook_repo/,peterkuharvarduk,1544550295,"[https://github.com/trending](https://github.com/trending) 

When was the last time this happened? Theano and TensorFlow initial release maybe?",3,1,False,self,,,,,
657,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,2,a594p8,self.MachineLearning,[R] Fast Algorithm for Category Construction,https://www.reddit.com/r/MachineLearning/comments/a594p8/r_fast_algorithm_for_category_construction/,Feynmanfan85,1544550845,"In a previous post entitled, ""Image Recognition with No Prior Information"", I introduced an algorithm that can identify structure in random images with no prior information by making use of assumptions rooted in information theory. As part of that algorithm, I showed how we can use the Shannon entropy of a matrix to create meaningful, objective categorizations ex ante, without any prior information about the data we're categorizing. In this post, I'll present a generalized algorithm that can take in an arbitrary data set, and quickly construct meaningful, intuitively correct partitions of the data set, again with no prior information.

Though I am still conducting research on the run-time of the algorithm, the algorithm generally takes only a few seconds to run on an ordinary commercial laptop, and I believe the worst case complexity of the algorithm to be O(log(n)n^2 + n^2 + n), where n is the number of data points in the data set.

*The Distribution of Information*

Consider the vector of integers x = [1 2 3 5 10 11 15 21]. In order to store, convey, or operate on x, wed have to represent x in some form, and as a result, by its nature, x has some intrinsic information content. In this case, x consists of a series of 8 integers, all less than 2^5 = 32, so we could, for example, represent the entire vector as a series of 8, 5-bit binary numbers, using a total of 40 bits...

The rest of the article, together with the code, and data, is available here:

https://www.researchgate.net/project/Information-Theory-16

Thoughts are welcomed!",1,1,False,self,,,,,
658,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,3,a598mh,self.MachineLearning,[D] Dream GPU configuration for under $3000,https://www.reddit.com/r/MachineLearning/comments/a598mh/d_dream_gpu_configuration_for_under_3000/,smashMaster3000,1544551505,Hey yall Ive recently garnered enough money to buy a new (or a few) gpus and I want to optimize my training time for my models. Is it worth looking at multi-gpu setups? Should I be looking for lots of memory? Do tensorcores mean anything? If you have any insight that could help me please post!! (I develop in keras and tensorflow if that means anything),69,1,False,self,,,,,
659,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,3,a59pvp,blog.talla.com,How Smart is AI? Machine Comprehension at Talla,https://www.reddit.com/r/MachineLearning/comments/a59pvp/how_smart_is_ai_machine_comprehension_at_talla/,Rob,1544554463,,0,1,False,https://a.thumbs.redditmedia.com/QK1_pVZuBen3q5sRTWtulWZbk7qmKjqkWdS58KvMhG0.jpg,,,,,
660,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,4,a5a6dw,self.MachineLearning,Anyone here an OpenAI Winter 2019 Fellow?,https://www.reddit.com/r/MachineLearning/comments/a5a6dw/anyone_here_an_openai_winter_2019_fellow/,HipsterToofer,1544557288,Just wondering whether they're done hiring.,0,1,False,self,,,,,
661,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,5,a5agpb,medium.com,Chinese AI Unicorn Megvii Targets $500 Million in New Funding,https://www.reddit.com/r/MachineLearning/comments/a5agpb/chinese_ai_unicorn_megvii_targets_500_million_in/,Yuqing7,1544559055,,0,1,False,default,,,,,
662,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,5,a5aq4n,self.MachineLearning,[D] Can bad data in your training set be helpful?,https://www.reddit.com/r/MachineLearning/comments/a5aq4n/d_can_bad_data_in_your_training_set_be_helpful/,decimated_napkin,1544560676,"Something very peculiar has happened to me while training a one-dimensional CNN. I have a pretty small amount of electrode data that I'm using to train a classifier to detect whenever a certain waveform appears. In my original setup, I have 240 records in the training set and 60 in the test set. The best model has a val loss of .15 and 98.3% accuracy. When looking at the data more carefully I noticed that some of my training data was very noisy. I then decided to remove it and try again, so my training set now only has 200 records. The best model now only has a val loss of .3 and accuracy of 95%! My theory is that since I'm not doing any dropout, the bad data is actually acting as the lone regularization term and making the model better. Has anyone encountered anything like this?",25,1,False,self,,,,,
663,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,6,a5az0h,self.MachineLearning,Text analysis: Is it possible to detect future use of words within the sentence until end?,https://www.reddit.com/r/MachineLearning/comments/a5az0h/text_analysis_is_it_possible_to_detect_future_use/,Iwillgetasoda,1544562200,"I've read about RNN and LTSM networks, which they are good at predicting the next word. I would like train a network to find out the most likely words which would be used some random time until the end of the sentence. Are those networks good enough or is there any other model should I read about? Thank you for hints.",0,1,False,self,,,,,
664,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,6,a5b8u0,medium.com,2018 in Review: 10 AI Failures,https://www.reddit.com/r/MachineLearning/comments/a5b8u0/2018_in_review_10_ai_failures/,trcytony,1544563870,,0,1,False,default,,,,,
665,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,7,a5bk9w,self.MachineLearning,How do you stop replicating and start innovating,https://www.reddit.com/r/MachineLearning/comments/a5bk9w/how_do_you_stop_replicating_and_start_innovating/,iDemandEuph0ria,1544565877,[removed],0,1,False,self,,,,,
666,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,7,a5bq7p,ai.googleblog.com,Grasp2Vec: Learning Object Representations from Self-Supervised Grasping,https://www.reddit.com/r/MachineLearning/comments/a5bq7p/grasp2vec_learning_object_representations_from/,sjoerdapp,1544566938,,0,1,False,https://b.thumbs.redditmedia.com/UnyrIYfpK2LnAd-zkMumNq3Ei-wIYurLTY0oZXTtvUo.jpg,,,,,
667,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,7,a5bzd9,medium.com,GANs Demystified  What the hell do they learn? [Paper Summary],https://www.reddit.com/r/MachineLearning/comments/a5bzd9/gans_demystified_what_the_hell_do_they_learn/,idiot_panda,1544568579,,0,1,False,default,,,,,
668,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,8,a5ce5z,benjamin.computer,Using PyTorch to find the orientation of a torus,https://www.reddit.com/r/MachineLearning/comments/a5ce5z/using_pytorch_to_find_the_orientation_of_a_torus/,onidaito,1544571299,,0,1,False,default,,,,,
669,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,9,a5cub2,self.MachineLearning,Facial recognition: Its time for action,https://www.reddit.com/r/MachineLearning/comments/a5cub2/facial_recognition_its_time_for_action/,shehackspurple,1544574522,"Facial recognition: Its time for action. 

[https://blogs.microsoft.com/on-the-issues/2018/12/06/facial-recognition-its-time-for-action/](https://blogs.microsoft.com/on-the-issues/2018/12/06/facial-recognition-its-time-for-action/?WT.mc_id=AIBlog-Reddit-tajanca)

Does anyone have any thoughts on this?  I found it puter interesting.  They talk about; opportunities, problems, bias, discrimination, privacy,democratic freedoms, human rights, laws and regulations.  

&amp;#x200B;",0,1,False,self,,,,,
670,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,10,a5d67e,self.MachineLearning,Looking for general direction,https://www.reddit.com/r/MachineLearning/comments/a5d67e/looking_for_general_direction/,orangeatom,1544576854,[removed],0,1,False,self,,,,,
671,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,10,a5d7ga,arxiv.org,[R] [1812.03473v1] Comixify: Transform video into a comics,https://www.reddit.com/r/MachineLearning/comments/a5d7ga/r_181203473v1_comixify_transform_video_into_a/,hanyuqn,1544577106,,6,1,False,default,,,,,
672,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,10,a5dl4c,chinadaily.com.cn,AI textbooks for Chinese kindergarteners released,https://www.reddit.com/r/MachineLearning/comments/a5dl4c/ai_textbooks_for_chinese_kindergarteners_released/,sercosan,1544579889,,0,1,False,default,,,,,
673,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,11,a5dqor,self.MachineLearning,[D] Multinomial Logistic Regression : Softmax Equivalent on training sets?,https://www.reddit.com/r/MachineLearning/comments/a5dqor/d_multinomial_logistic_regression_softmax/,evolutionblues,1544581044,"Hello, 

I am aware of use cases where multinomial logistic regression is used to predict multiclass targets.

Classic example:

[https://en.wikipedia.org/wiki/Iris\_flower\_data\_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)

However, I am interested in others experience where the training data itself mimics a softmax output.

For example, in the link above, if the training dataset instead looked like:

&amp;#x200B;

\[Feature 1, Feature 2... Feature N\] \[P(I. setosa) = 0.5, P(I. versicolor) = 0.4, P(I. virginica) = 0.1\]

Here for a given featureset, the label is the probability for each class instead of a single value.

Does any one have any pointers on dealing with these kind of inputs, and how they can be represented as inputs for a multinomial logistic regression model?

Thanks.",5,1,False,self,,,,,
674,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,12,a5efvn,self.MachineLearning,Generalizing deep learning model,https://www.reddit.com/r/MachineLearning/comments/a5efvn/generalizing_deep_learning_model/,skbhagat40,1544586324,[removed],0,1,False,self,,,,,
675,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,13,a5elgr,self.MachineLearning,"[N] Preferred Networks announces MN-Core, a processor for deep learning.",https://www.reddit.com/r/MachineLearning/comments/a5elgr/n_preferred_networks_announces_mncore_a_processor/,milaworld,1544587519,"From the announcement [page](https://projects.preferred.jp/mn-core/en/):

Preferred Networks (PFN) has developed the Chainer open-source deep learning framework and has been working to build large-scale clusters that support its research and development activities with the aim of applying deep learning technology in the real world.

To promote this initiative further, PFN is developing MN-Core, a processor dedicated to the acceleration of deep learning research.

The MN-Core chip is optimized for the training phase in deep learning. Unlike a general-purpose chip, it delivers excellent processing performance by having only limited functionalities. As well as focusing on minimal functionalities, PFNs proprietary MN-Core has a dedicated circuit for performing matrix operations, a required process in deep learning, to make deep learning much faster.

Nowadays, performance per watt is becoming increasingly important when developing a processor mainly because of the cooling capacity reaching its limit. MN-Core is expected to achieve 1TFLOPS/W in a half-precision floating-point format, a top-class performance per watt in the world.

https://projects.preferred.jp/mn-core/en/",18,1,False,self,,,,,
676,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,13,a5ex3h,self.MachineLearning,[D] Can Gaussian Mixture Models used to learn a trajectory?,https://www.reddit.com/r/MachineLearning/comments/a5ex3h/d_can_gaussian_mixture_models_used_to_learn_a/,mackie__m,1544590207,I'm trying to understand whether GMM a trajectory (an ordered sequence of vectors). I'm trying to learn the motion of a person for a very small data set (&lt; 100 steps) and was wondering what could be a viable approach.,0,1,False,self,,,,,
677,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,14,a5f0wp,self.MachineLearning,[D] What is a simple approach to learn and sample a trajectory of a small data size?,https://www.reddit.com/r/MachineLearning/comments/a5f0wp/d_what_is_a_simple_approach_to_learn_and_sample_a/,mackie__m,1544591072,I'm trying to understand whether a GMM (or a similar non-deep method) can be used to learn and sample a trajectory (an ordered sequence of vectors). I'm trying to learn the motion of a person for a very small data set (&lt; 100 steps) and was wondering what could be a viable approach. It's not obvious if it can be used. Some steps of how would be appreciated.,6,1,False,self,,,,,
678,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,14,a5fe86,pythonmachinelearning.pro,How to Use Machine Learning to Show Predictions in Augmented Reality  Part 1,https://www.reddit.com/r/MachineLearning/comments/a5fe86/how_to_use_machine_learning_to_show_predictions/,Gecko13Z,1544594207,,0,1,False,default,,,,,
679,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,15,a5fm27,self.MachineLearning,What is the most common method when training using Reinforced learning to play games?,https://www.reddit.com/r/MachineLearning/comments/a5fm27/what_is_the_most_common_method_when_training/,zevzev,1544596175,[removed],0,1,False,self,,,,,
680,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,15,a5frfa,india4world.com,AI Using Python: Popular Combinations that You Can Deploy in AI/Machine Learning Projects,https://www.reddit.com/r/MachineLearning/comments/a5frfa/ai_using_python_popular_combinations_that_you_can/,SunilAhujaa,1544597556,,0,1,False,default,,,,,
681,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,16,a5fv5f,self.MachineLearning,[P] Ideas for improving Pix2Pix GAN training for generating short dancing clips?,https://www.reddit.com/r/MachineLearning/comments/a5fv5f/p_ideas_for_improving_pix2pix_gan_training_for/,to4life,1544598526,"I'm working on using the below pix2pix library, with pose extraction, to create a video of a person dancing like an expert dancer. The inspiration is the ""Everybody Dance Now"" research, whose own code doesn't seem to be released. 

https://github.com/affinelayer/pix2pix-tensorflow
https://carolineec.github.io/everybody_dance_now/ 

After taking a short sample video of someone moving about randomly for 30 seconds, similar to as instructed in the work, I have a sample video below. I trained the pix2pix network as-is for about 10 hours (~300 or so epochs) on an Amazon P2 (single GPU, Tesla K80) machine. 

My results so far: 
https://imgur.com/a/0Wg82yH

As you can see, it's not terrible and generally catches the pose and puts it onto the target subject decently. The target subject training vid is pretty good quality recorded from a smartphone on plain background, and is 30 seconds of the target moving about in different ways, so I think the training material is good. 

What I'm looking for are tips, ideas, strategies for improving the GAN training. I thought given that I'm doing a particular type of task, maybe there's a smart training/tuning strategy. Also I can definitely try training for more time/on a better machine, but I'd like to make this project scale a bit better so I can dance videos for others. 

You can check out pix2pix linked above, I haven't trained it and have just been using the default settings and setup. I'm still reading through and understanding the original paper behind it: https://arxiv.org/abs/1611.07004 

Anyways, thanks for any help, and if/when we get it working better, I'd be happy to make some dance videos for you guys, and release some code of the modified version! =)

",9,1,False,self,,,,,
682,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,16,a5fvbr,oodlestech.blogspot.com,5 Best Artificial Intelligence Tools,https://www.reddit.com/r/MachineLearning/comments/a5fvbr/5_best_artificial_intelligence_tools/,tech-info,1544598572,,0,1,False,default,,,,,
683,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,16,a5g1qw,self.MachineLearning,Do You Ever Imagine The Role Of AI In The Future Of Cyber Security ?,https://www.reddit.com/r/MachineLearning/comments/a5g1qw/do_you_ever_imagine_the_role_of_ai_in_the_future/,Linda_Wilsonjv,1544600379,[removed],0,1,False,self,,,,,
684,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,16,a5g2lj,self.MachineLearning,A gentle journey from linear regression to neural networks,https://www.reddit.com/r/MachineLearning/comments/a5g2lj/a_gentle_journey_from_linear_regression_to_neural/,joseph_rocca,1544600621,[removed],0,1,False,self,,,,,
685,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,16,a5g4uo,self.MachineLearning,[D] A gentle journey from linear regression to neural networks,https://www.reddit.com/r/MachineLearning/comments/a5g4uo/d_a_gentle_journey_from_linear_regression_to/,joseph_rocca,1544601252,"Hi Reddit,

I would like to share my last Medium article about Machine Learning, Deep Learning and Neural Networks. This is a soft introduction and overview of many concepts ranging from very basic to more advanced.

Hoping it could be helpful/insightful ! Good reading ! :)

[https://towardsdatascience.com/a-gentle-journey-from-linear-regression-to-neural-networks-68881590760e](https://towardsdatascience.com/a-gentle-journey-from-linear-regression-to-neural-networks-68881590760e)",0,1,False,self,,,,,
686,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,17,a5g6wa,self.MachineLearning,Top Examples of Why Data Science is Not Just .fit().predict(),https://www.reddit.com/r/MachineLearning/comments/a5g6wa/top_examples_of_why_data_science_is_not_just/,yonatanhadar,1544601806,[removed],0,1,False,self,,,,,
687,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,17,a5gck1,self.MachineLearning,Real-time machine learning,https://www.reddit.com/r/MachineLearning/comments/a5gck1/realtime_machine_learning/,truespartan3,1544603503,[removed],0,1,False,self,,,,,
688,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,17,a5gh3x,self.MachineLearning,"Time of day as response, bin as categorical, keep as time, or discretize?",https://www.reddit.com/r/MachineLearning/comments/a5gh3x/time_of_day_as_response_bin_as_categorical_keep/,thrownaway1190,1544604972,[removed],0,1,False,self,,,,,
689,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,18,a5ghsz,self.MachineLearning,[P] Real-Time Machine Learning Research,https://www.reddit.com/r/MachineLearning/comments/a5ghsz/p_realtime_machine_learning_research/,truespartan3,1544605200,"Hey guys!

I am going to do a project involving real-time learning and was wondering if you had some mandatory articles or search words for this subject. I've been doing some initial background research but the subject seems sparse. ",12,1,False,self,,,,,
690,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,18,a5gqkx,self.MachineLearning,YouTube Channel Digital Marketing,https://www.reddit.com/r/MachineLearning/comments/a5gqkx/youtube_channel_digital_marketing/,digitalmarketingDE,1544607814,[removed],0,1,False,self,,,,,
691,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,18,a5gr1q,self.MachineLearning,[R] Adversarial Framing for Image and Video Classification,https://www.reddit.com/r/MachineLearning/comments/a5gr1q/r_adversarial_framing_for_image_and_video/,hare_man,1544607965,[removed],0,1,False,self,,,,,
692,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,19,a5gyet,ciolook.com,Machine Learning is not only for Tech Giant | CIO Look,https://www.reddit.com/r/MachineLearning/comments/a5gyet/machine_learning_is_not_only_for_tech_giant_cio/,ciolook,1544610105,,0,1,False,default,,,,,
693,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,19,a5gzse,soco.ps,"""[Discussion]"" A How-To-Guide to Utilize Distributed System for Big Data!",https://www.reddit.com/r/MachineLearning/comments/a5gzse/discussion_a_howtoguide_to_utilize_distributed/,ReasonablyHank,1544610498,,0,1,False,default,,,,,
694,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,19,a5h46i,soco.ps,A How-To-Guide to Utilize Distributed System for Big Data!,https://www.reddit.com/r/MachineLearning/comments/a5h46i/a_howtoguide_to_utilize_distributed_system_for/,ReasonablyHank,1544611832,,0,1,False,default,,,,,
695,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,20,a5h8ex,hybridize.ai,[R] Coconditional Autoencoding Adversarial Networks for Chinese Font Feature Learning,https://www.reddit.com/r/MachineLearning/comments/a5h8ex/r_coconditional_autoencoding_adversarial_networks/,Surf4kyle,1544613054,,0,1,False,default,,,,,
696,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,20,a5h98e,hackernoon.com,NeurIPS 2018 Recap by Forge.AI,https://www.reddit.com/r/MachineLearning/comments/a5h98e/neurips_2018_recap_by_forgeai/,jenniferlum,1544613285,,0,1,False,https://a.thumbs.redditmedia.com/WVInAu2VUxjk8KRNaTJWAB5VkXC9dSA9yPhQ4c9nES4.jpg,,,,,
697,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,20,a5hf7v,self.MachineLearning,[R] Adversarial Framing for Image and Video Classification,https://www.reddit.com/r/MachineLearning/comments/a5hf7v/r_adversarial_framing_for_image_and_video/,kzolna,1544615025,"https://youtu.be/PrU9R6eFNTs  
Hi, check out our paper on adversarial attacks for video. Above is the link to YT video, here is the paper: https://arxiv.org/pdf/1812.04599.pdf and the code: https://github.com/zajaczajac/adv\_framing .",4,1,False,self,,,,,
698,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,21,a5hs63,self.MachineLearning,Love for Machine Learning start with this guys series,https://www.reddit.com/r/MachineLearning/comments/a5hs63/love_for_machine_learning_start_with_this_guys/,kuchbhi12345,1544618346,[removed],0,1,False,self,,,,,
699,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,22,a5hyfx,self.MachineLearning,How to calculate gradients when using operations that's not differentiable?,https://www.reddit.com/r/MachineLearning/comments/a5hyfx/how_to_calculate_gradients_when_using_operations/,jtfidje,1544619865,[removed],0,1,False,self,,,,,
700,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,22,a5i639,self.MachineLearning,Looking for an Image Segmentation Architecture with Inception Blocks,https://www.reddit.com/r/MachineLearning/comments/a5i639/looking_for_an_image_segmentation_architecture/,SpinatGemuese,1544621589,[removed],0,1,False,self,,,,,
701,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,22,a5i9wk,self.MachineLearning,[P] Text classification with recurrent models. Adding noise to gradients?,https://www.reddit.com/r/MachineLearning/comments/a5i9wk/p_text_classification_with_recurrent_models/,the_3bodyproblem,1544622449,"Hey there!

&amp;#x200B;

I am working on a text classification problem for a while now. The architecture has grown to be a few layers deep of stacked LSTMs, increasingly improving the final classification score.  The problem is that all the models I have tried so far easily overfit the dataset. After a small number of epochs, validation accuracy decreases when training accuracy increases. (The dataset is not small, with around 1M pairs, but with a strong imbalance towards negative samples \~88%).

This behaviour appears even for very small models (thin and shallow). Since augmenting text data is not easy, I have been trying other methods. One of the things that catched my attention was this paper:

[https://openreview.net/pdf?id=rkjZ2Pcxe](https://openreview.net/pdf?id=rkjZ2Pcxe)

&amp;#x200B;

They claim that adding noise to the gradients is helpful. I have not yet seen this gain. Anyway, do you know other related schemes that can improve regularization of sequential models where very little augmentation can be done?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",12,1,False,self,,,,,
702,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,23,a5if93,youtube.com,AlphaZero documentary (2017),https://www.reddit.com/r/MachineLearning/comments/a5if93/alphazero_documentary_2017/,rajarsheem,1544623591,,0,1,False,default,,,,,
703,MachineLearning,t5_2r3gv,2018-12-12,2018,12,12,23,a5im7k,self.MachineLearning,[D] Why are there no image segmentation architectures using inception blocks?,https://www.reddit.com/r/MachineLearning/comments/a5im7k/d_why_are_there_no_image_segmentation/,SpinatGemuese,1544625010,"Hi Reddit

&amp;#x200B;

I've been trying to find papers using inception blocks in their architecture for image segmentation, but I have not been able to find any. Why is that?",9,1,False,self,,,,,
704,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,0,a5jbxz,self.MachineLearning,"Simple Questions Thread December 12, 2018",https://www.reddit.com/r/MachineLearning/comments/a5jbxz/simple_questions_thread_december_12_2018/,AutoModerator,1544629963,[removed],0,1,False,self,,,,,
705,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,1,a5jrlv,twitter.com,"[Discussion] hardmaru on Twitter: After spending a day meeting with a dozen researchers Vector Inst. and undergrads UofT (some already have published 2 conf papers at NIPS, ICML etc), I realized the bar is much higher now than when I started. If I started from scratch today, I don't think I'd get in",https://www.reddit.com/r/MachineLearning/comments/a5jrlv/discussion_hardmaru_on_twitter_after_spending_a/,GuiltyResearcher,1544632701,,2,1,False,default,,,,,
706,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,1,a5jsg9,tryolabs.com,How we built a stand-in robot for remote workers using IoT and computer vision,https://www.reddit.com/r/MachineLearning/comments/a5jsg9/how_we_built_a_standin_robot_for_remote_workers/,minmidinosaur,1544632846,,0,1,False,https://b.thumbs.redditmedia.com/0MTDg70Osdg2ie3MH_SsMv35colzCXZi3uhxhSuwEKk.jpg,,,,,
707,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,1,a5jthe,self.MachineLearning,[P] How we built a stand-in robot for remote workers using IoT and computer vision,https://www.reddit.com/r/MachineLearning/comments/a5jthe/p_how_we_built_a_standin_robot_for_remote_workers/,minmidinosaur,1544633031,"[How we built a stand-in robot for remote workers using IoT and computer vision](https://tryolabs.com/blog/hackathon-robot-remote-work-iot-computer-vision/)

https://i.redd.it/beiibxfglv321.jpg",0,1,False,https://b.thumbs.redditmedia.com/kyQE-BR-DKfJOZXWCtGWn_u-YtWFq2fX8ULE-KqhmjQ.jpg,,,,,
708,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,2,a5jz0p,self.MachineLearning,[R] Signed Graph Convolutional Network (ICDM2018),https://www.reddit.com/r/MachineLearning/comments/a5jz0p/r_signed_graph_convolutional_network_icdm2018/,benitorosenberg,1544634036,"&amp;#x200B;

https://i.redd.it/ox7mk3xaov321.jpg

Paper: [https://github.com/benedekrozemberczki/SGCN/blob/master/sgcn.pdf](https://github.com/benedekrozemberczki/SGCN/blob/master/sgcn.pdf)

Python: [https://github.com/benedekrozemberczki/SGCN/](https://github.com/benedekrozemberczki/SGCN/)

ABSTRACT:

Due to the fact much of today's data can be represented as graphs, there  has been a demand for generalizing neural network models for graph  data. One recent direction that has shown fruitful results, and  therefore growing interest, is the usage of graph convolutional neural  networks (GCNs). They have been shown to provide a significant  improvement on a wide range of tasks in network analysis, one of which  being node representation learning. The task of learning low-dimensional  node representations has shown to increase performance on a plethora of  other tasks from link prediction and node classification, to community  detection and visualization. Simultaneously, signed networks (or graphs  having both positive and negative links) have become ubiquitous with the  growing popularity of social media. However, since previous GCN models  have primarily focused on unsigned networks (or graphs consisting of  only positive links), it is unclear how they could be applied to signed  networks due to the challenges presented by negative links. The primary  challenges are based on negative links having not only a different  semantic meaning as compared to positive links, but their principles are  inherently different and they form complex relations with positive  links. Therefore we propose a dedicated and principled effort that  utilizes balance theory to correctly aggregate and propagate the  information across layers of a signed GCN model. We perform empirical  experiments comparing our proposed signed GCN against state-of-the-art  baselines for learning node representations in signed networks. More  specifically, our experiments are performed on four real-world datasets  for the classical link sign prediction problem that is commonly used as  the benchmark for signed network embeddings algorithms.

&amp;#x200B;",2,1,False,https://b.thumbs.redditmedia.com/7aQtJ12YkXbPX_5ckJTWlRSX_hMxot1AuwhWsSImsLQ.jpg,,,,,
709,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,2,a5k0s5,appliedmachinelearning.blog,Facial Emotion Detection From Real Time Videos,https://www.reddit.com/r/MachineLearning/comments/a5k0s5/facial_emotion_detection_from_real_time_videos/,Abhijeet3922,1544634322,,0,1,False,https://b.thumbs.redditmedia.com/GaZeZuqfb73G80soG0fO6Dxj9GdLzIg2bW2BaEsh22M.jpg,,,,,
710,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,2,a5k4md,self.MachineLearning,distributed pytorch,https://www.reddit.com/r/MachineLearning/comments/a5k4md/distributed_pytorch/,ice109,1544634965,does anyone know any good writeups for distributed pytorch? i.e. on a small cluster? https://pytorch.org/tutorials/intermediate/dist_tuto.html is out of date (because e.g. tcp backend is deprecated) and beyond that i can't quite figure out from it how to get a model trained (i.e. the weights) across a cluster,0,1,False,self,,,,,
711,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,2,a5kbs1,self.MachineLearning,[D] 2018: How relevant is Breiman's The Two Cultures paper?,https://www.reddit.com/r/MachineLearning/comments/a5kbs1/d_2018_how_relevant_is_breimans_the_two_cultures/,crypto_ha,1544636201,"[Statistical Modeling: The Two Cultures](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726) (Leo Breiman, 2001)

Is this paper still relevant? Have data modeling and algorithmic modeling begun to fuse and complement each other?",9,1,False,self,,,,,
712,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,3,a5knj5,self.MachineLearning,Understanding the Adoption Curve of VR and Mixed Reality | Sam Watts,https://www.reddit.com/r/MachineLearning/comments/a5knj5/understanding_the_adoption_curve_of_vr_and_mixed/,The_Syndicate_VC,1544638250,[removed],0,1,False,self,,,,,
713,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,3,a5kx7k,self.MachineLearning,Accelerate your Data Analysis and Presentation Skills with the PwC Approach Specialization,https://www.reddit.com/r/MachineLearning/comments/a5kx7k/accelerate_your_data_analysis_and_presentation/,internetdigitalentre,1544639910,[removed],0,1,False,https://b.thumbs.redditmedia.com/n64a33g6tDAefrahLs5Tbf_gz9ifQx8ICD1UITGyZXM.jpg,,,,,
714,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,3,a5kyj3,self.MachineLearning,Why machine learning / automations?,https://www.reddit.com/r/MachineLearning/comments/a5kyj3/why_machine_learning_automations/,adarigirishkumar,1544640143,"I love the ideas and concepts of automation, machine learning ,apps do multiple activities ,but why should we go for these when we have many people to do the work .
Some one pls help find reasons to back automation, iot, machine learning and such technologies.",0,1,False,self,,,,,
715,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,3,a5l1z3,github.com,How to Grow Software Architecture out of Jupyter Notebooks (as seen on HN),https://www.reddit.com/r/MachineLearning/comments/a5l1z3/how_to_grow_software_architecture_out_of_jupyter/,GChe,1544640734,,0,1,False,https://b.thumbs.redditmedia.com/i3MopAMgQTqQS4Z_wHbGYE3oyrGun2p0-m7Mlk0-lRg.jpg,,,,,
716,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,4,a5li7p,medium.com,Using Machine Learning to Synthesize Peptides,https://www.reddit.com/r/MachineLearning/comments/a5li7p/using_machine_learning_to_synthesize_peptides/,Yuqing7,1544643519,,0,1,False,https://b.thumbs.redditmedia.com/9Pew6nZcHmORatIRBWNspaz6NKHf1SseohzoIC5S9hQ.jpg,,,,,
717,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,5,a5lwzo,self.MachineLearning,[D] Journals for machine learning research?,https://www.reddit.com/r/MachineLearning/comments/a5lwzo/d_journals_for_machine_learning_research/,TheJCBand,1544646139,It seems like most of the top ML papers are either from conferences or self-published.  Are there any reputable journals for machine learning?  I'm aware that the field has championed open-access and collectively rallied against journals that require a subscription to view articles.,24,1,False,self,,,,,
718,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,5,a5m4iv,self.MachineLearning,[Discussion] Help!,https://www.reddit.com/r/MachineLearning/comments/a5m4iv/discussion_help/,vasishtsudhanva,1544647451,"&amp;#x200B;

https://i.redd.it/1obe0mc9rw321.png

I was implementing FIND S algorithm in python using numpy and pandas. (Code given above in the picture)  
The dataset I have given for this program is as follows:

sunny, warm, normal, strong, warm, same, yes  
sunny, warm, high, strong, warm, same, yes  
rainy, cold, high, strong, warm, change, no  
sunny, warm, high, strong, cool, change, yes

This is a dataset regarding the event 'play tennis'.

Now my question is - in the maximally specific set (output in the program), shouldn't the output be 

sunny, warm, ?, strong, ?, ? 

&amp;#x200B;

Because for both normal and high in the third column in the dataset, there is a positive (yes) outcome

  
",1,1,False,https://a.thumbs.redditmedia.com/522vXdbs_KCRCOcofz7R5hrMe3PCQh7vV2yQsAb9YZ8.jpg,,,,,
719,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,5,a5m8gt,self.MachineLearning,Which would be your favourite topic on an AI Conference ?,https://www.reddit.com/r/MachineLearning/comments/a5m8gt/which_would_be_your_favourite_topic_on_an_ai/,kritya947,1544648163,"We are organising a brand new conference in Budapest for March, and I would really appreciate your opinion about the speakers and topics. We already have speakers from *Google* , *Uber and IBM* with different topics, but we would like to bring more hot topics so we are curious your ideas.

[https://reinforceconf.com/](https://reinforceconf.com/)",0,1,False,self,,,,,
720,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,6,a5mclp,cloud.google.com,Now you can train TensorFlow machine learning models faster and at lower cost on Cloud TPU Pods,https://www.reddit.com/r/MachineLearning/comments/a5mclp/now_you_can_train_tensorflow_machine_learning/,x2342,1544648853,,0,1,False,default,,,,,
721,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,6,a5mczj,twitter.com,The 2018 AI Index Annual Report is now out! Lots of amazing data collected by @indexingai led by @yshoham. See the rise of AI,https://www.reddit.com/r/MachineLearning/comments/a5mczj/the_2018_ai_index_annual_report_is_now_out_lots/,AdditionalWay,1544648922,,0,1,False,default,,,,,
722,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,6,a5min6,self.MachineLearning,What about original CNN - cellular neural network?,https://www.reddit.com/r/MachineLearning/comments/a5min6/what_about_original_cnn_cellular_neural_network/,pkonowrocki,1544649932,[removed],0,1,False,self,,,,,
723,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,6,a5mmdg,self.MachineLearning,[D] What about the original CNN - cellular neural network?,https://www.reddit.com/r/MachineLearning/comments/a5mmdg/d_what_about_the_original_cnn_cellular_neural/,pkonowrocki,1544650573,"First of all, I'm new to this society, so I'd like to say hello to everyone. :)
But getting to the main reason Im posting. Right now I'm writing my thesis on mobile robot path planning, and as I was gathering different papers on path finding, I found this really fascinating concept of Cellular Neural Networks (http://www.scholarpedia.org/article/Cellular_neural_network). I think that its really clever concept, similar to cellular automata - it's simple but gives some really good effects. And as I was reading more about CNN (give that title back ConvNet!), I was wondering, have you ever came upon CNN in real life applications? Did you ever use it? Or is it completely dead subject of Machine Learning? Or can you even call it a subject of ML? 
Id really love to hear more from someone more experienced, casue Im pretty much a beginner :)",14,1,False,self,,,,,
724,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,7,a5mwfe,mlperf.org,MLPerf results are out,https://www.reddit.com/r/MachineLearning/comments/a5mwfe/mlperf_results_are_out/,gdiamos,1544652318,,0,1,False,https://a.thumbs.redditmedia.com/Dz0XDhVDV77l-wYLVr__uggIkN5Ychx-erKYqrykME8.jpg,,,,,
725,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,7,a5mzhu,self.MachineLearning,[D] Multi-Language embeddings (Word2Vec),https://www.reddit.com/r/MachineLearning/comments/a5mzhu/d_multilanguage_embeddings_word2vec/,__Julia,1544652869,"We are collecting data about ancient languages and we would like to train a classifier to identify the language. I was using normal classifier using n-grams, I tried also using LSTM/CNN for that purpose. However, one thing that I was investigating is using word2vec as it is unsupervised  and helps to retain relevant information about a corpus while having relatively low dimensionality.
What I didn't see commonly used in literature is training Multi Language in one. 

Let's say that we have three old African languages where there some words that are common, and other words that belong solely to one language. Should we train every language separately ? If not, I am trying to understand why this is not common ?

For the sake of simplicity, you can think of latin languages, they share some words like `action`, `information`, `Attention`, `Communication`, .. are common words between English and French. ",8,1,False,self,,,,,
726,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,7,a5n2u6,arxiv.org,[R] [1812.03973] Bayesian Layers: A Module for Neural Network Uncertainty,https://www.reddit.com/r/MachineLearning/comments/a5n2u6/r_181203973_bayesian_layers_a_module_for_neural/,steven2358,1544653488,,23,1,False,default,,,,,
727,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,8,a5neuv,self.MachineLearning,[D] You've heard of gif or gif. What about GAN or GAN?,https://www.reddit.com/r/MachineLearning/comments/a5neuv/d_youve_heard_of_gif_or_gif_what_about_gan_or_gan/,manicman1999,1544655659,"I've always been a believer that it's pronounced ""gif"" with the hard G sound, because the word the G stands for is graphic. I've also heard many people pronounce GANs with a hard G, but I would disagree. GAN should be pronounced ""jan"" since the first word (generative) has the soft-sounding g (the one that sounds like a J). Thank you for your time.",13,1,False,self,,,,,
728,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,9,a5o9qy,self.MachineLearning,Can anyone reference where to learn to Use XGBOOST,https://www.reddit.com/r/MachineLearning/comments/a5o9qy/can_anyone_reference_where_to_learn_to_use_xgboost/,gauchoezm,1544661665,[removed],0,1,False,self,,,,,
729,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,9,a5od1q,self.MachineLearning,What happens when we change the underlying model in Federated Learning?,https://www.reddit.com/r/MachineLearning/comments/a5od1q/what_happens_when_we_change_the_underlying_model/,randomdistillation,1544662347,[removed],0,1,False,self,,,,,
730,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,10,a5ogoh,youtube.com,Yankang 2000L-5layers blow molding machine running in Pakistan,https://www.reddit.com/r/MachineLearning/comments/a5ogoh/yankang_2000l5layers_blow_molding_machine_running/,miyawang12138,1544663075,,0,1,False,default,,,,,
731,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,10,a5ogxq,self.MachineLearning,I'm making a machine learning &amp; data science student org at my university. What would be good potential resources?,https://www.reddit.com/r/MachineLearning/comments/a5ogxq/im_making_a_machine_learning_data_science_student/,DataScienceUTA,1544663127,[removed],0,1,False,self,,,,,
732,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,10,a5oizo,self.MachineLearning,ML and auto to generate Narratives from data,https://www.reddit.com/r/MachineLearning/comments/a5oizo/ml_and_auto_to_generate_narratives_from_data/,duyth,1544663548,[removed],0,1,False,self,,,,,
733,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,10,a5ojvh,self.MachineLearning,[D] I'm making a machine learning &amp; data science student org at my university. What would be good potential resources?,https://www.reddit.com/r/MachineLearning/comments/a5ojvh/d_im_making_a_machine_learning_data_science/,DataScienceUTA,1544663729,"The org is going to have graduate and undergraduate students.

The goal is the organization is a bit more broad than a typical org; we are going to allow all majors and focus on basic and applied research, as well as industry applications. Our university focuses heavily in DS and ML (we even got labs in Psychology making Neural Network models since the 1980's).

What resources would you suggest?",8,1,False,self,,,,,
734,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,13,a5pwvv,self.MachineLearning,"No help on /r/learnmachinelearning, Where can I find CNN and RNN problems?",https://www.reddit.com/r/MachineLearning/comments/a5pwvv/no_help_on_rlearnmachinelearning_where_can_i_find/,GeneralShoulder,1544673679,[removed],0,1,False,self,,,,,
735,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,14,a5qh2b,mlperf.org,MLPerf v0.5 Results,https://www.reddit.com/r/MachineLearning/comments/a5qh2b/mlperf_v05_results/,m0d8ye,1544678063,,0,1,False,https://a.thumbs.redditmedia.com/Dz0XDhVDV77l-wYLVr__uggIkN5Ychx-erKYqrykME8.jpg,,,,,
736,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,14,a5qhy0,arxiv.org,Check this out!,https://www.reddit.com/r/MachineLearning/comments/a5qhy0/check_this_out/,Shobu711,1544678257,,0,1,False,default,,,,,
737,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,14,a5qkx0,self.MachineLearning,Help for starting machine learning as research,https://www.reddit.com/r/MachineLearning/comments/a5qkx0/help_for_starting_machine_learning_as_research/,durgesh2018,1544678982,[removed],0,1,False,self,,,,,
738,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,14,a5qnek,self.MachineLearning,"Global Machine Learning Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/a5qnek/global_machine_learning_market_size_outlook/,Ganeshsing,1544679593,[removed],0,1,False,self,,,,,
739,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,14,a5qs1a,self.MachineLearning,Filling Machines  An Overview,https://www.reddit.com/r/MachineLearning/comments/a5qs1a/filling_machines_an_overview/,matconmaticsg,1544680758,[removed],0,1,False,self,,,,,
740,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,15,a5qzqu,self.MachineLearning,Word Embedding capable of handling Top and bottom word dependency,https://www.reddit.com/r/MachineLearning/comments/a5qzqu/word_embedding_capable_of_handling_top_and_bottom/,thak123,1544682653,Hi has anyone come across any word-embedding algo which takes into consideration the not only the left and right word but also the top and the bottom word for scoring the representation,0,1,False,self,,,,,
741,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,16,a5rcmx,self.MachineLearning,Creating a Filipino/Cebuano Text-to-Speech Program for Education Purposes,https://www.reddit.com/r/MachineLearning/comments/a5rcmx/creating_a_filipinocebuano_texttospeech_program/,subalzero,1544686013," **Hello, everyone! My team and I are planning to make a Speech Synthesizer for Filipino or Cebuano as a startup for Technopreneurship101 course. Its purpose will be to help our fellow blind Filipino citizens (and possibly educators). We would like to know and discuss about your interest on other benefits of TTS. We would also like to know about your opinions if this will be marketable and if you would like us to continue our work.** ",0,1,False,self,,,,,
742,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,17,a5rkz9,self.MachineLearning,Interesting chat group,https://www.reddit.com/r/MachineLearning/comments/a5rkz9/interesting_chat_group/,ashutosj,1544688431,[removed],0,1,False,self,,,,,
743,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,17,a5rlsv,self.MachineLearning,I work in a Sheet Metal facility and I have a couple questions about the Salvagnini P2Xe.,https://www.reddit.com/r/MachineLearning/comments/a5rlsv/i_work_in_a_sheet_metal_facility_and_i_have_a/,GummieLindsays,1544688671,[removed],0,1,False,self,,,,,
744,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,17,a5rm5y,mlperf.org,"MLPerf v0.5 results announced comparing top ML hardware for ML training (Google, NVIDIA, Intel)",https://www.reddit.com/r/MachineLearning/comments/a5rm5y/mlperf_v05_results_announced_comparing_top_ml/,gfursin,1544688771,,0,1,False,default,,,,,
745,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,17,a5romc,openpr.com,"Turbomolecular Pumps Market Expected to Reach $1,529 Million by 2025 | Top Key Players are Agilent, Atlas Copco, Busch, Ebara Technologies, KYKY Technology, Osaka Vacuum, Welch",https://www.reddit.com/r/MachineLearning/comments/a5romc/turbomolecular_pumps_market_expected_to_reach/,Swati_Shinde,1544689541,,0,1,False,default,,,,,
746,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,17,a5rq9h,self.MachineLearning,can anyone tell me an interesting project to do on supervised learning?,https://www.reddit.com/r/MachineLearning/comments/a5rq9h/can_anyone_tell_me_an_interesting_project_to_do/,chakie09877,1544690054,[removed],0,1,False,self,,,,,
747,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,18,a5s1ho,ai.googleblog.com, Improving the Effectiveness of Diabetic Retinopathy Models,https://www.reddit.com/r/MachineLearning/comments/a5s1ho/improving_the_effectiveness_of_diabetic/,sjoerdapp,1544693534,,0,1,False,https://a.thumbs.redditmedia.com/ug8RqocbF7Z-PeKURqW66gawqHmLghJbgfrdcURuB64.jpg,,,,,
748,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,18,a5s5vz,mlperf.org,"[N] MLPerf v0.5 results announced comparing top ML hardware for ML training (Google, NVIDIA, Intel)",https://www.reddit.com/r/MachineLearning/comments/a5s5vz/n_mlperf_v05_results_announced_comparing_top_ml/,gfursin,1544694885,,0,1,False,https://b.thumbs.redditmedia.com/fUtx6EiqCV6dVpksc2hc2ezejv3vgOzoEyjbNkobLVE.jpg,,,,,
749,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,19,a5s8pv,self.MachineLearning,[R] A bags of tricks which may improve deep learning models [Slides],https://www.reddit.com/r/MachineLearning/comments/a5s8pv/r_a_bags_of_tricks_which_may_improve_deep/,kmkolasinski,1544695732,"Hi, few days ago nice guys from Amazon posted an arxiv paper about bags of tricks they used to improve accuracy of ImageNet classification task. With those simple modifications they were able to increase accuracy from \~75% to \~79%, which can be considered rather as a large improvement. Here is the link to that [post](https://www.reddit.com/r/MachineLearning/comments/a4dxna/r_bag_of_tricks_for_image_classification_with/). In the same time, I was slowly preparing my slides for a talk on exactly the same topic. However, my slides cover a little more content and you may find this interesting. 

Link to slides:  [https://github.com/kmkolasinski/deep-learning-notes/tree/master/seminars/2018-12-Improving-DL-with-tricks](https://github.com/kmkolasinski/deep-learning-notes/tree/master/seminars/2018-12-Improving-DL-with-tricks)

My slides cover following topics:

* new learning rate schedulers,
* large mini-batch training,
* AdamW and importance of weight decay,
* ZeroInit,
* Label smoothing and mixup,
* etc",18,1,False,self,,,,,
750,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,19,a5s9b3,self.MachineLearning,Learning word splittings of Scriptio Continua languages,https://www.reddit.com/r/MachineLearning/comments/a5s9b3/learning_word_splittings_of_scriptio_continua/,crvineeth97,1544695898,Hello. Is there a machine learning model that can learn the words that are present in a scriptio continua sentence? Scriptio Continua languages are ones where there are no spaces or any form of punctuation between words of a sentence. For eg. Chinese or Japanese.,0,1,False,self,,,,,
751,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,19,a5sa5u,controlgmc.com,AUTOMATIC PACKING MACHINES AND FILLING LINES,https://www.reddit.com/r/MachineLearning/comments/a5sa5u/automatic_packing_machines_and_filling_lines/,controlgmc,1544696152,,0,1,False,default,,,,,
752,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,19,a5sc8v,self.MachineLearning,[D] Word splittings of Scriptio Continua languages,https://www.reddit.com/r/MachineLearning/comments/a5sc8v/d_word_splittings_of_scriptio_continua_languages/,crvineeth97,1544696792,"Is there a machine learning model which can learn the words present in the sentence of a Scriptio Continua language?

A Scriptio Continua language is one in which there are no spaces or more generally any sort of punctuation between the words of a sentence.

Some examples of such languages include Chinese and Japanese",4,1,False,self,,,,,
753,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,20,a5slwp,self.MachineLearning,Is there any work on biological or physical methods for solving neural network with numerical optimization?,https://www.reddit.com/r/MachineLearning/comments/a5slwp/is_there_any_work_on_biological_or_physical/,misssprite,1544699662,"I came across a paper in which sort of circuit is design to solve a neural unit leveraging physical law, automatically?

I'm not sure whether I understood that paper but failed to find it again.",0,1,False,self,,,,,
754,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,20,a5sr3n,self.MachineLearning,Is there any work on biological or physical methods for solving neural network without numerical optimization?,https://www.reddit.com/r/MachineLearning/comments/a5sr3n/is_there_any_work_on_biological_or_physical/,misssprite,1544701110,[removed],0,1,False,self,,,,,
755,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,20,a5stjb,self.MachineLearning,Introducing Ragged Tensors,https://www.reddit.com/r/MachineLearning/comments/a5stjb/introducing_ragged_tensors/,sayantandas30011998,1544701785,[removed],0,1,False,self,,,,,
756,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,21,a5sx4n,technologyreview.com,A radical new neural network design could overcome big challenges in AI,https://www.reddit.com/r/MachineLearning/comments/a5sx4n/a_radical_new_neural_network_design_could/,mavenchist,1544702727,,0,1,False,https://a.thumbs.redditmedia.com/-J0Kg4J_-IfOZt4WIeNs4MreXwFkTXhkuVZOTkcgs00.jpg,,,,,
757,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,21,a5t0cx,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Dec. 13, 2018",https://www.reddit.com/r/MachineLearning/comments/a5t0cx/n_weekly_machine_learning_opensource_roundup_dec/,stkim1,1544703545,,0,1,False,default,,,,,
758,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,21,a5t14c,self.MachineLearning,Matrix as one feature in a matrix of features [help],https://www.reddit.com/r/MachineLearning/comments/a5t14c/matrix_as_one_feature_in_a_matrix_of_features_help/,Merlin_Aliese,1544703734,[removed],0,1,False,self,,,,,
759,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,21,a5t1px,technologyreview.com,[N] A radical new neural network design could overcome big challenges in AI,https://www.reddit.com/r/MachineLearning/comments/a5t1px/n_a_radical_new_neural_network_design_could/,mavenchist,1544703891,,0,1,False,https://a.thumbs.redditmedia.com/-J0Kg4J_-IfOZt4WIeNs4MreXwFkTXhkuVZOTkcgs00.jpg,,,,,
760,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,21,a5t4vg,self.MachineLearning,Where was I wrong?,https://www.reddit.com/r/MachineLearning/comments/a5t4vg/where_was_i_wrong/,ahsr0x,1544704655,[removed],0,1,False,self,,,,,
761,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,21,a5t7wu,self.MachineLearning,"[D] Stuart Russel: ""Probabilistic programming and AI""",https://www.reddit.com/r/MachineLearning/comments/a5t7wu/d_stuart_russel_probabilistic_programming_and_ai/,abstractcontrol,1544705437,"Video (54m): https://youtu.be/JzBrp5LnNCo

This video is the most interesting of the bunch from the PROBPROG conference in my view. Here is a short transcript of a part where a shot was fired at deep learning. It is a juicy piece of meat to chew on.

https://youtu.be/JzBrp5LnNCo?t=655

---

...And so you are throwing a way a massive amount of generalization capability if you do not have this kind of expressive power.

And I believe, though this is not a particularly well explored area of computational learning theory that, you could also think, not so much what is the expressive power of the language, but how fast is it to evaluate the function? If it is a linear time function, that your hypothesis applied to an example is linear time then that means that if you are trying to learn a function from a family that is NP-Hard, you are going to in many cases need an exponentially large representation for the function and therefore again you are going to need a vast number of examples.

So when someone says (dismissive wave of hand) neural nets are really fast, and you are trying to do inference with Bayes nets and that's really slow. I am not going to even talk to you because you can't run the Bayes net on the entire web at once...then don't feel bad. The fact that your Bayes net is taking a long time is a good sign. At least it has a chance of forming a concise representation of the function that you are trying to learn. And I think the same argument in spades extends to things that are much more expressive than Bayes nets. So I do not think it is a scale the more complex your language the larger the time complexity of inference the worse things get. In fact it may be that the better things get.

So I suspect that the deep learning community may be repeating or is already repeating historical mistake that happened in AI. This is Bad Old Fashioned AI (in the 1960s). This is preformal (laughter), this is before we even sort of understood how to do first order inference and all that stuff, but people showed ""Wow, with GPS, General Problem Solver we could produce three, four step plans so it is just a matter of scaling up, right? You know, computers will deal with this, they're effectively infinitely fast.""

This is before we had the theory of NP-Hardess and all that stuff, so they were completely ignoring computational intractability. They eventually called this the combinatorial explosion. And then they tried other ways of building AI systems. We are doing the same thing now except with sample intractability. We are looking at approaches to problems that are going to require an exponentially or super-exponentially many examples. The universe is just not big enough to contain that much data for some of these problems.",9,1,False,self,,,,,
762,MachineLearning,t5_2r3gv,2018-12-13,2018,12,13,22,a5tkxn,github.com,[N] skorch version 0.5.0 - high-level scikit-learn wrapper for PyTorch 1.0,https://www.reddit.com/r/MachineLearning/comments/a5tkxn/n_skorch_version_050_highlevel_scikitlearn/,ottonemo,1544708435,,1,1,False,default,,,,,
763,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,0,a5u9ch,self.MachineLearning,Document Classification Approach,https://www.reddit.com/r/MachineLearning/comments/a5u9ch/document_classification_approach/,Namlich,1544713514,"Hi ML Reddit,  


I was simply curious, as im about to work on an document classification task. How would you approach it? Would you use take input as an image, text or both? Not many papers seem to take the route of images.   


My task is similar to RVL-CDIP dataset, though im able to extract text if necessary. My data will be limited though, and right now my problem is that even basic CNNs will overfit on the RVL-CDIP dataset.  


So my basic question for now is, would you go with images or text? ",0,1,False,self,,,,,
764,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,0,a5ub8b,bbc.com,The key to cracking long-dead languages?,https://www.reddit.com/r/MachineLearning/comments/a5ub8b/the_key_to_cracking_longdead_languages/,roy-m-kim,1544713851,,0,1,False,https://b.thumbs.redditmedia.com/5Cw33fI1VEE3sTAEoD5OkKuJI-ho8h8GEFQgoibx4vo.jpg,,,,,
765,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,0,a5ukut,self.MachineLearning,How Analytics and Data Science Accelerates AI/ Machine Learning Adoption,https://www.reddit.com/r/MachineLearning/comments/a5ukut/how_analytics_and_data_science_accelerates_ai/,JackWillls,1544715665,[removed],0,1,False,self,,,,,
766,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,0,a5ulne,aisoma.de,[D] Importance of Unsupervised Learning in data preprocessing,https://www.reddit.com/r/MachineLearning/comments/a5ulne/d_importance_of_unsupervised_learning_in_data/,seemingly_omniscient,1544715818,,0,1,False,default,,,,,
767,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,0,a5umpr,self.MachineLearning,Technical Test for ML Interns?,https://www.reddit.com/r/MachineLearning/comments/a5umpr/technical_test_for_ml_interns/,xanc17,1544716006,[removed],0,1,False,self,,,,,
768,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,1,a5v346,self.MachineLearning,[R] Learning Latent Dynamics for Planning from Pixels,https://www.reddit.com/r/MachineLearning/comments/a5v346/r_learning_latent_dynamics_for_planning_from/,danijar,1544718955,"Paper: [https://arxiv.org/abs/1811.04551](https://arxiv.org/abs/1811.04551)

Policies: [https://youtu.be/TeUceNCmq34](https://youtu.be/TeUceNCmq34)

https://i.redd.it/y2jn8pouo2421.jpg",14,1,False,https://b.thumbs.redditmedia.com/cDkaYVMl9sc8n6_xV5qjIGHUVn9saVZSsk7NphL8E0k.jpg,,,,,
769,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,1,a5v6ra,aisoma.de,[D] Importance of Unsupervised Learning in data preprocessing,https://www.reddit.com/r/MachineLearning/comments/a5v6ra/d_importance_of_unsupervised_learning_in_data/,seemingly_omniscient,1544719615,,0,1,False,default,,,,,
770,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,2,a5vn31,self.MachineLearning,[R] Doubly Bayesian Optimization,https://www.reddit.com/r/MachineLearning/comments/a5vn31/r_doubly_bayesian_optimization/,boltzBrain,1544722491,"Preprint: [https://arxiv.org/abs/1812.04562](https://arxiv.org/abs/1812.04562)

&amp;#x200B;

**Abstract:** Bayesian optimization (BO) is a powerful method for optimizing complex black-box functions that are costly to evaluate directly. Although useful out of the box, complexities arise when the domain exhibits non-smooth structure, noise, or greater than five dimensions. Extending BO for these issues is non-trivial, which is why we suggest casting BO methods into the probabilistic programming paradigm. These systems (PPS) enable users to encode model structure and naturally reason about uncertainties, which can be leveraged towards improved BO methods. Here we present a probabilistic domain-specific language where BO is native, showing this probabilistic approach to optimization is more naturally expressed in a PPS. We show results on standard optimization benchmarks, and, more importantly, demonstrate that the framework enables the user to more readily use advanced techniques such as unscented BO and noisy expected improvement.",1,1,False,self,,,,,
771,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,3,a5vz3s,bbc.com,How AI could help us with ancient languages like Sumerian,https://www.reddit.com/r/MachineLearning/comments/a5vz3s/how_ai_could_help_us_with_ancient_languages_like/,j_orshman,1544724573,,0,1,False,https://b.thumbs.redditmedia.com/5Cw33fI1VEE3sTAEoD5OkKuJI-ho8h8GEFQgoibx4vo.jpg,,,,,
772,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,3,a5w6mb,moalquraishi.wordpress.com,AlphaFold @ CASP13: What just happened?,https://www.reddit.com/r/MachineLearning/comments/a5w6mb/alphafold_casp13_what_just_happened/,alexeyr,1544725889,,0,1,False,default,,,,,
773,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,3,a5w6sx,moalquraishi.wordpress.com,[D] AlphaFold @ CASP13: What just happened?,https://www.reddit.com/r/MachineLearning/comments/a5w6sx/d_alphafold_casp13_what_just_happened/,alexeyr,1544725921,,0,1,False,https://b.thumbs.redditmedia.com/30MvAnGSMnBMeNtGqmrtNoJGAYu6K1HkuYRApR5juGk.jpg,,,,,
774,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,3,a5wctv,self.MachineLearning,Publish papers or blogs for novel algorithms?,https://www.reddit.com/r/MachineLearning/comments/a5wctv/publish_papers_or_blogs_for_novel_algorithms/,brandinho77,1544726949,[removed],0,1,False,self,,,,,
775,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,3,a5wg6z,i.redd.it,Anlise de Pay Back V,https://www.reddit.com/r/MachineLearning/comments/a5wg6z/anlise_de_pay_back_v/,JamurGerloff,1544727552,,0,1,False,https://a.thumbs.redditmedia.com/ftu-dyw8Xc7tLzd2sNG2wr8pPkwRsBQtQCv2bpbnDg8.jpg,,,,,
776,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,4,a5wrz4,youtube.com,"Wildly impressive GAN results from NVIDIA - ""A Style Based Generator Architecture for Generative Adversarial Networks""",https://www.reddit.com/r/MachineLearning/comments/a5wrz4/wildly_impressive_gan_results_from_nvidia_a_style/,a_draganov,1544729603,,0,1,False,default,,,,,
777,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,4,a5wt9p,arxiv.org,[R] A Style-Based Generator Architecture for Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/a5wt9p/r_a_stylebased_generator_architecture_for/,vwvwvvwwvvvwvwwv,1544729828,,0,1,False,default,,,,,
778,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,4,a5wti9,arxiv.org,[R] [1812.04948] A Style-Based Generator Architecture for Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/a5wti9/r_181204948_a_stylebased_generator_architecture/,vwvwvvwwvvvwvwwv,1544729867,,44,1,False,default,,,,,
779,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,5,a5x30a,self.MachineLearning,"Webinar: Learning AI for Hackers  A Visual Primer (Dec 20, 2018)",https://www.reddit.com/r/MachineLearning/comments/a5x30a/webinar_learning_ai_for_hackers_a_visual_primer/,ddonzal,1544731491,[removed],0,1,False,https://b.thumbs.redditmedia.com/uP1Kv3de_lCKQrrqSM1BsSW6VxJHu_T_ujNE2nPqiIQ.jpg,,,,,
780,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,5,a5xgnl,nbviewer.jupyter.org,Interactive Jupyter/Python demo of Linear Regression right in your browser: example of predicting country happiness rank by Economy GDP and Happiness rate,https://www.reddit.com/r/MachineLearning/comments/a5xgnl/interactive_jupyterpython_demo_of_linear/,trekhleb,1544733828,,0,1,False,default,,,,,
781,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,6,a5xnhj,self.MachineLearning,Training ResNet-34 with custom dataset,https://www.reddit.com/r/MachineLearning/comments/a5xnhj/training_resnet34_with_custom_dataset/,quack015,1544734985,[removed],0,1,False,self,,,,,
782,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,6,a5xx0e,self.MachineLearning,Any online tools to enhance tiny blurry images?,https://www.reddit.com/r/MachineLearning/comments/a5xx0e/any_online_tools_to_enhance_tiny_blurry_images/,D3Pixel,1544736637,[removed],0,1,False,self,,,,,
783,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,6,a5y6oq,self.MachineLearning,Bare necessities to run your python agent in Unity3D!,https://www.reddit.com/r/MachineLearning/comments/a5y6oq/bare_necessities_to_run_your_python_agent_in/,taylerallen6,1544738317,[removed],0,1,False,self,,,,,
784,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,7,a5yl9v,self.MachineLearning,Games to add AI to,https://www.reddit.com/r/MachineLearning/comments/a5yl9v/games_to_add_ai_to/,TheRealPikzel,1544740889,[removed],0,1,False,self,,,,,
785,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,7,a5yq60,medium.com,[N] 2018 AI Index Report Released,https://www.reddit.com/r/MachineLearning/comments/a5yq60/n_2018_ai_index_report_released/,gwen0927,1544741783,,0,1,False,https://b.thumbs.redditmedia.com/E4Ha9RPnoBynrZK6a-cGEDua5Hn2BGsuiCK9AabQrpU.jpg,,,,,
786,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,8,a5ysm9,self.MachineLearning,Ask: can you help me find neural network image refining and clarifications?,https://www.reddit.com/r/MachineLearning/comments/a5ysm9/ask_can_you_help_me_find_neural_network_image/,VDBDEV,1544742230,[removed],0,1,False,self,,,,,
787,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,8,a5z4u1,self.MachineLearning,"Okay, I'm lost - Advice",https://www.reddit.com/r/MachineLearning/comments/a5z4u1/okay_im_lost_advice/,alceluiz,1544744543,[removed],0,1,False,self,,,,,
788,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,8,a5z57v,self.MachineLearning,any tutorials on GPUs?,https://www.reddit.com/r/MachineLearning/comments/a5z57v/any_tutorials_on_gpus/,Prof_Montgomery,1544744615,[removed],0,1,False,self,,,,,
789,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,8,a5z81d,self.MachineLearning,Survey | Academic Research on Data activities in business (3 mins max with 10 questions),https://www.reddit.com/r/MachineLearning/comments/a5z81d/survey_academic_research_on_data_activities_in/,brianlamo74,1544745156,[removed],0,1,False,self,,,,,
790,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,8,a5z877,self.MachineLearning,[D] Re-create Reduced Network After Pruning,https://www.reddit.com/r/MachineLearning/comments/a5z877/d_recreate_reduced_network_after_pruning/,sunrisetofu,1544745188,"After we applying one of the many pruning methods for our network, and say we obtain a mask indicating pruned neurons, how do we recreate reduced network? How would this differ for dense layers, conv layers and lstm/gru layers? ",0,1,False,self,,,,,
791,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,9,a5zq43,self.MachineLearning,Salt your seeds! Unexpected brilliance in tensorflow/distributions.,https://www.reddit.com/r/MachineLearning/comments/a5zq43/salt_your_seeds_unexpected_brilliance_in/,ragulpr,1544748645,[removed],0,1,False,self,,,,,
792,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,10,a602p5,self.MachineLearning,I am looking for projects or papers where machine learning is applied to complex energy supply systems with big action spaces.,https://www.reddit.com/r/MachineLearning/comments/a602p5/i_am_looking_for_projects_or_papers_where_machine/,314nuts,1544751231,[removed],0,1,False,self,,,,,
793,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,10,a604mb,self.MachineLearning,[P] Music Transformer: Generating Music with Long-Term Structure,https://www.reddit.com/r/MachineLearning/comments/a604mb/p_music_transformer_generating_music_with/,baylearn,1544751618,"New [blog post](https://magenta.tensorflow.org/music-transformer) from google brain about their recent [music transformer paper](https://arxiv.org/abs/1809.04281).

https://magenta.tensorflow.org/music-transformer",45,1,False,self,,,,,
794,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,11,a60amf,self.MachineLearning,Virtual Assistant Chatbot,https://www.reddit.com/r/MachineLearning/comments/a60amf/virtual_assistant_chatbot/,DG_Capital,1544752859,[removed],0,1,False,self,,,,,
795,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,11,a60kb9,self.MachineLearning,How to save the DQN model weight?,https://www.reddit.com/r/MachineLearning/comments/a60kb9/how_to_save_the_dqn_model_weight/,newbornking87,1544754916,[removed],0,1,False,self,,,,,
796,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,11,a60lul,self.MachineLearning,[P] Tackling Kuzushiji-MNIST - A new MNIST alternative for Deep Learning Tasks,https://www.reddit.com/r/MachineLearning/comments/a60lul/p_tackling_kuzushijimnist_a_new_mnist_alternative/,ranihorev,1544755260,"Hey!   
Recently, three datasets of Kuzushiji, an old Japanese script, were released, with one of them being Kuzushiji-MNIST, an alternative to the legendary MNIST. Kuzushiji was used for over 1000 years but almost no one knows how to read it today, while millions of documents in Japan are available only in this format.   
I built a VGG-ResNet ensemble model and reached state-of-the-art results (98.9%) on Kuzushiji-MNIST which is pretty cool :)  
I also used Capsule Networks for the first time but the results werent better (97.9%).   
The code can be found [here](https://github.com/ranihorev/Kuzushiji_MNIST) and I also wrote a short [summary](https://www.lyrn.ai/2018/12/13/kuzushiji-mnist-japanese-literature-alternative-dataset-for-deep-learning-tasks/) and added a bit of background on the Japanese language (which is completely different from latin languages).   


The paper: [https://arxiv.org/abs/1812.01718](https://arxiv.org/abs/1812.01718)

The dataset: [https://github.com/rois-codh/kmnist](https://github.com/rois-codh/kmnist)

  
Any suggestions for improvement would be great!",0,1,False,self,,,,,
797,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,11,a60m9u,magenta.tensorflow.org,"[R] ""Music Transformer: Generating Music with Long-Term Structure"" [Huang et al 2018, updated w/discussion &amp; demos]",https://www.reddit.com/r/MachineLearning/comments/a60m9u/r_music_transformer_generating_music_with/,gwern,1544755348,,0,1,False,https://b.thumbs.redditmedia.com/rPls8IAlbaDr4QMQEG2NHvHReOw6fZZq7suX0WrEk_o.jpg,,,,,
798,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,13,a61dur,self.MachineLearning,Conditional grouping using ML,https://www.reddit.com/r/MachineLearning/comments/a61dur/conditional_grouping_using_ml/,koolsatyam,1544761182,[removed],0,1,False,self,,,,,
799,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,14,a61uk7,self.MachineLearning,Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects,https://www.reddit.com/r/MachineLearning/comments/a61uk7/strike_with_a_pose_neural_networks_are_easily/,hungry_for_knowledge,1544764978,[removed],0,1,False,https://b.thumbs.redditmedia.com/ZJlaSV_k25kDolb26AtN7HyEfDABzQGe0-AmeKKZAvY.jpg,,,,,
800,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,14,a61yiw,self.MachineLearning,New model to learn corrections.,https://www.reddit.com/r/MachineLearning/comments/a61yiw/new_model_to_learn_corrections/,yashkumaratri,1544765918,[removed],0,1,False,self,,,,,
801,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,14,a61ys8,self.MachineLearning,Log analyzer to predict errors of application,https://www.reddit.com/r/MachineLearning/comments/a61ys8/log_analyzer_to_predict_errors_of_application/,khanshakeeb,1544765978,[removed],0,1,False,self,,,,,
802,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,15,a625jx,self.MachineLearning,Learning to learn human corrected model predictions,https://www.reddit.com/r/MachineLearning/comments/a625jx/learning_to_learn_human_corrected_model/,yashkumaratri,1544767632,[removed],0,1,False,self,,,,,
803,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,17,a633c5,self.MachineLearning,Holykell Diesel fuel tank level sensor application,https://www.reddit.com/r/MachineLearning/comments/a633c5/holykell_diesel_fuel_tank_level_sensor_application/,Holykell,1544776836,[removed],0,1,False,self,,,,,
804,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,17,a635mi,self.MachineLearning,Good TreeRNN/TreeLSTM libraries?,https://www.reddit.com/r/MachineLearning/comments/a635mi/good_treernntreelstm_libraries/,bayareasurfer,1544777531,[removed],0,1,False,self,,,,,
805,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,18,a63e1g,journals.plos.org,PLOS Medicine: Advancing the beneficial use of machine learning in health care and medicine: Toward a community understanding,https://www.reddit.com/r/MachineLearning/comments/a63e1g/plos_medicine_advancing_the_beneficial_use_of/,eleitl,1544780027,,1,1,False,default,,,,,
806,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,18,a63f7s,ezeetest.app,9 Applications of Artificial Intelligence in Education,https://www.reddit.com/r/MachineLearning/comments/a63f7s/9_applications_of_artificial_intelligence_in/,vermarajan,1544780406,,0,1,False,default,,,,,
807,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,19,a63l8o,dorianbrown.github.io,A Little Math on Logistic Regression  Dorian Brown  Finding signal and escaping the noise,https://www.reddit.com/r/MachineLearning/comments/a63l8o/a_little_math_on_logistic_regression_dorian_brown/,ballzoffury,1544782202,,0,1,False,default,,,,,
808,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,19,a63rq9,self.MachineLearning,[N] skorch version 0.5.0 - high-level scikit-learn wrapper for PyTorch 1.0,https://www.reddit.com/r/MachineLearning/comments/a63rq9/n_skorch_version_050_highlevel_scikitlearn/,ottonemo,1544784234,"This release is a more regular release to let people know that we indeed support PyTorch 1.0 and quite a few convenience fixes regarding checkpointing, model loading and parameter setting.

The most significant new feature is the automatic generation of command line interface code. Now you can write your model, document the parameters as docstrings and skorch will automatically generate command line parameters for you, making it possible to do things like this without repeating yourself:

    python train.py --lr 0.02 --batch_size=32 --module__hidden_units=1024

This also supports pipelines so you can experiment/grid search your feature extraction as well!
Read more about this [here](https://github.com/dnouri/skorch/tree/master/examples/cli).

If you want to get started right now you can explore most of our example notebooks on Google Colab, for example the [Advanced Usage](https://colab.research.google.com/github/dnouri/skorch/blob/master/notebooks/Advanced_Usage.ipynb) notebook.

If you have questions or suggestions feel free to let us know (here, on our [issue tracker](https://github.com/dnouri/skorch/issues) or on [stackoverflow](https://stackoverflow.com/questions/tagged/skorch)).",4,1,False,self,,,,,
809,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,21,a649ck,arxiv.org,500+ Times Faster Than Deep Learning (A Case Study Exploring Faster Methods for Text Mining StackOverflow),https://www.reddit.com/r/MachineLearning/comments/a649ck/500_times_faster_than_deep_learning_a_case_study/,DiogenicOrder,1544789255,,10,1,False,default,,,,,
810,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,21,a64b9t,self.MachineLearning,Best way to get large number of files to cloud?,https://www.reddit.com/r/MachineLearning/comments/a64b9t/best_way_to_get_large_number_of_files_to_cloud/,aashwin93,1544789774,[removed],0,1,False,self,,,,,
811,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,21,a64e1y,self.MachineLearning,Which GAN do you recommend to use for semi-aligned images?,https://www.reddit.com/r/MachineLearning/comments/a64e1y/which_gan_do_you_recommend_to_use_for_semialigned/,nyquist_karma,1544790482,[removed],0,1,False,self,,,,,
812,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,22,a64re5,github.com,Here is a 'bare-bones' approach to using python machine learning with Unity3D. This tutorial show only the necessities needed to use your own python agents in a Unity built environment.,https://www.reddit.com/r/MachineLearning/comments/a64re5/here_is_a_barebones_approach_to_using_python/,taylerallen6,1544793663,,0,1,False,https://a.thumbs.redditmedia.com/OH4_01tRIbIGJKBRTphwbWXWuZU1NS3KRXJAs9OVQk0.jpg,,,,,
813,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,22,a64x01,youtube.com,Wanna Kicks  Inspirational AR,https://www.reddit.com/r/MachineLearning/comments/a64x01/wanna_kicks_inspirational_ar/,darya_sesitskaya,1544794940,,0,1,False,https://b.thumbs.redditmedia.com/RjG7nlTJjWua54vnb_EikSa5Zyiy-dg8PLylytVwcUw.jpg,,,,,
814,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,23,a658e4,self.MachineLearning,What are your predictions for the field of Machine Learning in 2019?,https://www.reddit.com/r/MachineLearning/comments/a658e4/what_are_your_predictions_for_the_field_of/,wreckurd,1544797441,[removed],0,1,False,self,,,,,
815,MachineLearning,t5_2r3gv,2018-12-14,2018,12,14,23,a65f5i,self.MachineLearning,What is the usage space for a user in Google Colab in a single day?,https://www.reddit.com/r/MachineLearning/comments/a65f5i/what_is_the_usage_space_for_a_user_in_google/,vignesh_md,1544798872,[removed],0,1,False,self,,,,,
816,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,0,a65jvd,self.MachineLearning,Prediction in Random forest,https://www.reddit.com/r/MachineLearning/comments/a65jvd/prediction_in_random_forest/,NLP_RL,1544799820,[removed],0,1,False,self,,,,,
817,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,0,a65v5r,arxiv.org,Neural Ordinary Differential Equations [PDF],https://www.reddit.com/r/MachineLearning/comments/a65v5r/neural_ordinary_differential_equations_pdf/,yogthos,1544801992,,103,1,False,default,,,,,
818,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,2,a66m5p,self.MachineLearning,MLaaS - What do you think about them?,https://www.reddit.com/r/MachineLearning/comments/a66m5p/mlaas_what_do_you_think_about_them/,_FedEx,1544806871,[removed],0,1,False,self,,,,,
819,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,2,a66pr4,self.MachineLearning,[Python] XGBoost Algorithm Question,https://www.reddit.com/r/MachineLearning/comments/a66pr4/python_xgboost_algorithm_question/,DerrayProductions,1544807500,[removed],0,1,False,self,,,,,
820,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,2,a66x4h,blog.openai.com,How to choose the best batch size for your deep network? (new work from OpenAI),https://www.reddit.com/r/MachineLearning/comments/a66x4h/how_to_choose_the_best_batch_size_for_your_deep/,TallCitizen,1544808838,,0,1,False,default,,,,,
821,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,3,a67ctb,github.com,Pytext: A natural language modeling framework based on PyTorch,https://www.reddit.com/r/MachineLearning/comments/a67ctb/pytext_a_natural_language_modeling_framework/,QuirkySpiceBush,1544811573,,0,1,False,default,,,,,
822,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,3,a67egd,self.MachineLearning,[P] PyData/Sparse Webinar.,https://www.reddit.com/r/MachineLearning/comments/a67egd/p_pydatasparse_webinar/,hameerabbasi,1544811869,"Hello everyone! I'm the developer of a library called PyData/Sparse \[[Docs](http://sparse.pydata.org/)|[Source](https://github.com/pydata/sparse)\] that allows you to work with sparse arrays. It builds on top of NumPy and Numba.

&amp;#x200B;

It mimics NumPy, much like Dask and CuPy.

&amp;#x200B;

If you want to learn more, I'm going to be giving a webinar on it on December 19. You can register [here](https://app.livestorm.co/quansight/).",3,1,False,self,,,,,
823,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,3,a67gcj,self.MachineLearning,The Dangers of AI and Safety of Mutually Assured Destruction | George Dvorsky,https://www.reddit.com/r/MachineLearning/comments/a67gcj/the_dangers_of_ai_and_safety_of_mutually_assured/,The_Syndicate_VC,1544812230,[removed],0,1,False,self,,,,,
824,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,3,a67h91,self.MachineLearning,"Sigmoid saturating, but why? Assistance needed.",https://www.reddit.com/r/MachineLearning/comments/a67h91/sigmoid_saturating_but_why_assistance_needed/,Jandevries101,1544812392,[removed],0,1,False,self,,,,,
825,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,3,a67lrl,self.MachineLearning,[P] AdaptiLab: Online interactive course for coding deep neural networks-Seeking feedback!,https://www.reddit.com/r/MachineLearning/comments/a67lrl/p_adaptilab_online_interactive_course_for_coding/,CyberCorgi,1544813198,[removed],0,1,False,self,,,,,
826,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,3,a67p3c,github.com,AI Face recognition plugin for shinobi(CCTV) on a cheep TV Box,https://www.reddit.com/r/MachineLearning/comments/a67p3c/ai_face_recognition_plugin_for_shinobicctv_on_a/,solderzzc,1544813764,,0,1,False,default,,,,,
827,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,4,a67rs3,self.MachineLearning,[R] Kuzushiji-MNIST  Japanese Literature Alternative Dataset for Deep Learning Tasks ( + my SOTA implementation),https://www.reddit.com/r/MachineLearning/comments/a67rs3/r_kuzushijimnist_japanese_literature_alternative/,ranihorev,1544814244,"Hey!

Recently, three datasets of Kuzushiji, an old Japanese script, were released, with one of them being Kuzushiji-MNIST, an alternative to the legendary MNIST. Kuzushiji was used for over 1000 years but almost no one knows how to read it today, while millions of documents in Japan are available only in this format. 

I built a VGG-ResNet ensemble model and reached state-of-the-art results (98.9%) on Kuzushiji-MNIST which is pretty cool :)

I also used Capsule Networks for the first time but the results werent better (97.9%).

The code can be found [here](https://github.com/ranihorev/Kuzushiji_MNIST) and I also wrote a short [summary](https://www.lyrn.ai/2018/12/13/kuzushiji-mnist-japanese-literature-alternative-dataset-for-deep-learning-tasks/) and added a bit of background on the Japanese language (which is completely different from latin languages).

The paper: [https://arxiv.org/abs/1812.01718](https://arxiv.org/abs/1812.01718)

The dataset: [https://github.com/rois-codh/kmnist](https://github.com/rois-codh/kmnist)

Any suggestions for improvement would be great!

https://i.redd.it/c6rv08o9ka421.png",7,1,False,https://b.thumbs.redditmedia.com/cmIgl4M96w1Qz_4BZCXsTWsFrl8yKDClKtmL3M3BE_M.jpg,,,,,
828,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,4,a67vl4,self.datascience,Algorithm Suggestion,https://www.reddit.com/r/MachineLearning/comments/a67vl4/algorithm_suggestion/,tettusud,1544814925,,0,1,False,https://b.thumbs.redditmedia.com/ewN2z-aItAB9f34kAytqxoI2Qc4uGrJYFospCyJFLyM.jpg,,,,,
829,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,4,a67yqp,self.MachineLearning,Why do we need multiple nodes in a hidden layer of a neural net?,https://www.reddit.com/r/MachineLearning/comments/a67yqp/why_do_we_need_multiple_nodes_in_a_hidden_layer/,Aneervan,1544815515,[removed],0,1,False,self,,,,,
830,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,4,a683tc,self.MachineLearning,[R] Towards Robust Deep Neural Networks with BANG,https://www.reddit.com/r/MachineLearning/comments/a683tc/r_towards_robust_deep_neural_networks_with_bang/,idg101,1544816451,"Machine learning models, including state-of-the-art deep neural networks, are vulnerable to small perturbations that cause unexpected classification errors. This unexpected lack of robustness raises fundamental questions about their generalization properties and poses a serious concern for practical deployments. As such perturbations can remain imperceptible - the formed adversarial examples demonstrate an inherent inconsistency between vulnerable machine learning models and human perception - some prior work casts this problem as a security issue. Despite the significance of the discovered instabilities and ensuing research, their cause is not well understood and no effective method has been developed to address the problem. In this paper, we present a novel theory to explain why this unpleasant phenomenon exists in deep neural networks. Based on that theory, we introduce a simple, efficient, and effective training approach, Batch Adjusted Network Gradients (BANG), which significantly improves the robustness of machine learning models. While the BANG technique does not rely on any form of data augmentation or the utilization of adversarial images for training, the resultant classifiers are more resistant to adversarial perturbations while maintaining or even enhancing the overall classification performance. 

&amp;#x200B;

[https://arxiv.org/abs/1612.00138](https://arxiv.org/abs/1612.00138)",2,1,False,self,,,,,
831,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,5,a68ibn,self.MachineLearning,Selling compute power for 1 month for a third of IBM Clouds prices,https://www.reddit.com/r/MachineLearning/comments/a68ibn/selling_compute_power_for_1_month_for_a_third_of/,travisbreedy,1544819116,[removed],0,1,False,self,,,,,
832,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,5,a68mlv,medium.com,A Peek Inside Andrew Ngs AI Transformation Playbook,https://www.reddit.com/r/MachineLearning/comments/a68mlv/a_peek_inside_andrew_ngs_ai_transformation/,Yuqing7,1544819912,,0,1,False,https://b.thumbs.redditmedia.com/j5q6ZQG4EMhG4Y0dhBdkBdCvWEMIRPF74vq54SUvm8U.jpg,,,,,
833,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,5,a68p0u,self.MachineLearning,A beginner who would like to call an expert,https://www.reddit.com/r/MachineLearning/comments/a68p0u/a_beginner_who_would_like_to_call_an_expert/,MKUltraLover69,1544820382,[removed],0,1,False,self,,,,,
834,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,6,a68urf,github.com,"[N] PyText, a natural language modeling framework based on PyTorch, released by Facebook Research",https://www.reddit.com/r/MachineLearning/comments/a68urf/n_pytext_a_natural_language_modeling_framework/,Hyperparticles,1544821463,,1,1,False,default,,,,,
835,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,6,a68wjv,self.MachineLearning,Challenge: Outsmart Tumblr's censorbot,https://www.reddit.com/r/MachineLearning/comments/a68wjv/challenge_outsmart_tumblrs_censorbot/,styrofoammicrophone,1544821805,[removed],0,1,False,self,,,,,
836,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,6,a68zx4,self.MachineLearning,[Project] Challenge: Outsmart Tumblr's censorbot,https://www.reddit.com/r/MachineLearning/comments/a68zx4/project_challenge_outsmart_tumblrs_censorbot/,styrofoammicrophone,1544822438,"Tumblr's new NSFW policy has upset a lot of people.

According to the following post:

http://clish.tumblr.com/post/181113103950/once-i-encountered-the-funny-story-of-an-ai-image

User crazy-pages says that the bot they are using is an open source, ""one-layer, unidirectional, bicategory, tag-trained, neural network.""

Source: https://github.com/yahoo/open_nsfw

They also suggest that a counter-neural network could be created that ""puts an imperceptible (to humans) layer of patterned static over arbitrary images"" and is trained by ""having it bot-post static-ed images to Tumblr and [be reinforced] based on whether the images are labeled nsfw or sfw.""

Would anyone be up to the challenge of creating the above described network, and then a basic program which uses the network to turn nsfw images into ""sfw""?",9,1,False,self,,,,,
837,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,6,a6957t,self.MachineLearning,Sony machine learning animation patent,https://www.reddit.com/r/MachineLearning/comments/a6957t/sony_machine_learning_animation_patent/,PuzzledProgrammer3,1544823433,[removed],0,1,False,self,,,,,
838,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,6,a698fg,self.MachineLearning,[N] Sony machine learning animation patent,https://www.reddit.com/r/MachineLearning/comments/a698fg/n_sony_machine_learning_animation_patent/,PuzzledProgrammer3,1544824032,"Can someone explain this, what exactly is it doing?

""The patent filing also includes the machine-learning component of the *Spider-Verse* animation process, which streamlines the process as an automated function that predicts the position of lines on the next frame. The extrapolated lines streamline the process and give animators an advantage for the fine-tuning the lines.""

Source:[https://deadline.com/2018/12/sony-gets-inventive-seeks-patents-for-spider-man-into-the-spider-verse-animation-tech-1202518373/](https://deadline.com/2018/12/sony-gets-inventive-seeks-patents-for-spider-man-into-the-spider-verse-animation-tech-1202518373/)

Another article covering it

""But the biggest innovation had to do with line drawing, with Sony writing new software to allow two-dimensional drawing over the CG character models. Then we had to write tools to create rigs out of those lines to be used by the animators, Dimian said. The geometry generated from those lines made it more immersive. For the lines that are not as expressive (on the hands or the chin over the neck), we wrote software for machine learning to automate that process for the rest of the drawings.

Source: [https://www.indiewire.com/2018/12/spider-man-into-the-spider-verse-breaking-the-visual-rules-of-animation-1202027410/](https://www.indiewire.com/2018/12/spider-man-into-the-spider-verse-breaking-the-visual-rules-of-animation-1202027410/)",0,1,False,self,,,,,
839,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,6,a698nv,arxiv.org,[R] [1812.02788] Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs,https://www.reddit.com/r/MachineLearning/comments/a698nv/r_181202788_beyond_imitation_zeroshot_task/,steven2358,1544824078,,1,1,False,default,,,,,
840,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,7,a69ml4,i.redd.it,Efluentes Complexos V,https://www.reddit.com/r/MachineLearning/comments/a69ml4/efluentes_complexos_v/,JamurGerloff,1544826721,,0,1,False,https://b.thumbs.redditmedia.com/4wj-QGBANUgYFzXN6lUbmC7nxq18nPNxd3UM0b8SMOI.jpg,,,,,
841,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,7,a69tl3,arxiv.org,[R] A Style-Based Generator Architecture for Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/a69tl3/r_a_stylebased_generator_architecture_for/,jpedrofs,1544828106,,0,1,False,default,,,,,
842,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,8,a69ze2,self.MachineLearning,Cross Validation inability in time-series testing,https://www.reddit.com/r/MachineLearning/comments/a69ze2/cross_validation_inability_in_timeseries_testing/,bitsofshit,1544829281,[removed],0,1,False,self,,,,,
843,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,8,a69zmq,self.MachineLearning,[P] Building a pipeline to Map the Red Planet,https://www.reddit.com/r/MachineLearning/comments/a69zmq/p_building_a_pipeline_to_map_the_red_planet/,wronk17,1544829333,"I wanted to plug some of [recent ML work](https://medium.com/devseed/mapping-mars-with-ai-590ec1c9cb11) to map Mars with help from a couple planetary scientists. We're working to map craters on Mars using the YOLOv3 object detection model (specifically, [AlexeyAB's fork](https://github.com/AlexeyAB/darknet)). We're hoping to provide better/more data for rover and human landings as well as help answer questions about geologic processes on Mars.

&amp;#x200B;

[Mars 2020 rover mission landing ellipse \(blue\) with detected craters \(green\).](https://i.redd.it/wcrpac3wsb421.gif)

For context, the current best planet-wide crater dataset relied on multi-year tracing effort. Obviously, we'll have bring humans into the loop to confirm our model's less-confident predictions, but it should be possible to substantially shorten the planetary mapping timeline.

&amp;#x200B;

We haven't ran the full planet yet but hope to as soon as we work out an actual funding source. Besides posting for shameless self-promotion, I'm hoping to get help on a couple questions:

1. Are there any good guidelines on a max input image size for YOLO? And what parameters affect that? I imagine the number of image bands, number of classes, and feature complexity might be involved, but I haven't been able to find anything concrete. All the advice I've seen suggests slicing up large images with windowed reads and putting the puzzle pieces back together after prediction (which is what we've been doing so far).
2. Does anyone know if there are any iterative improvements on YOLO coming down the pipe? 
3. Does anyone who has actually ported a YOLO model to TF or PyTorch have tips on doing so? I've tried [one implementation](https://github.com/ayooshkathuria/pytorch-yolo-v3) and didn't have good luck. It might have just been a case of user-error, but wanted to see if anyone has had success before trying out more options.",4,1,False,https://a.thumbs.redditmedia.com/jeAXFyXTRcn-9BKKBXY3Rm9vzhcZ_dIIJDaupQlxzl8.jpg,,,,,
844,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,8,a6a7cv,/r/MachineLearning/comments/a6a7cv/ai_learns_to_avoid_obstacles_using_neuroevolution/,AI learns to avoid obstacles using neuro-evolution,https://www.reddit.com/r/MachineLearning/comments/a6a7cv/ai_learns_to_avoid_obstacles_using_neuroevolution/,PrestonW_,1544830955,,1,1,False,https://b.thumbs.redditmedia.com/7wFg8w3TIuX-vRXW-vPwgwwoU4k_GzXiy3ARslmSN8I.jpg,,,,,
845,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,9,a6adzh,self.MachineLearning,How to prepare for research engineer interview,https://www.reddit.com/r/MachineLearning/comments/a6adzh/how_to_prepare_for_research_engineer_interview/,VirtualEfficiency,1544832405,[removed],0,1,False,self,,,,,
846,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,9,a6afoa,/r/MachineLearning/comments/a6afoa/p_ai_learns_to_avoid_obstacles_using/,[P] AI learns to avoid obstacles using neuro-evolution,https://www.reddit.com/r/MachineLearning/comments/a6afoa/p_ai_learns_to_avoid_obstacles_using/,PrestonW_,1544832736,,2,1,False,default,,,,,
847,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,9,a6ahem,self.MachineLearning,[D] How to prepare for a Research Engineer Interview,https://www.reddit.com/r/MachineLearning/comments/a6ahem/d_how_to_prepare_for_a_research_engineer_interview/,VirtualEfficiency,1544833108,I am an engineer with experience in production machine learning and software engineering. I want to apply to Research Engineer position at Google. What's the best strategy to prepare to it?,15,1,False,self,,,,,
848,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,9,a6arg9,self.MachineLearning,Feature Denoising for Improving Adversarial Robustness,https://www.reddit.com/r/MachineLearning/comments/a6arg9/feature_denoising_for_improving_adversarial/,cihang-xie,1544835268,"Please check our new adversarial defense paper [https://arxiv.org/pdf/1812.03411.pdf](https://arxiv.org/pdf/1812.03411.pdf). We mainly show two things:

(1) adversarial perturbations at pixel level produce noise at feature levels

(2) denoising at feature levels can improve adversarial robustness",0,1,False,self,,,,,
849,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,10,a6axv1,github.com,PyTorch on TPUs!!,https://www.reddit.com/r/MachineLearning/comments/a6axv1/pytorch_on_tpus/,it-is-a-good-day,1544836693,,0,1,False,default,,,,,
850,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,10,a6ayib,blog.openai.com,"[R] OpenAI: 'Announcing the first finding from our ""Science of AI"" effort: a simple statistical metric that lets us predict the parallelizability of neural network training on a wide range of tasks'",https://www.reddit.com/r/MachineLearning/comments/a6ayib/r_openai_announcing_the_first_finding_from_our/,Z3F,1544836843,,0,1,False,https://b.thumbs.redditmedia.com/OQ5Yf49cfk3hU2yi-eNlG0cAptW5tdH9U1e_YerypAs.jpg,,,,,
851,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,10,a6b8i1,code.fb.com,[P] Open-sourcing PyText for faster NLP development,https://www.reddit.com/r/MachineLearning/comments/a6b8i1/p_opensourcing_pytext_for_faster_nlp_development/,beltsazar,1544839192,,0,1,False,default,,,,,
852,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,11,a6b910,self.MachineLearning,[D] Question about using Binary Cross Entropy for Multi-label Loss - does it capture label dependencies in Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/a6b910/d_question_about_using_binary_cross_entropy_for/,newperson77777777,1544839306,"For multi-label binary cross entropy loss, does it capture label dependencies? By multi-label binary cross entropy loss, I mean the sum of binary cross entropy losses for each label summed over all the examples in the dataset (I am doing this with relation to neural networks).

When looking to derive the conditional likelihood for example i for Probability function P, I have the expression below:

P(y1, ... yk | X, theta), where yj is a binary value indicating the presence of label j, k is the number of labels, X is the input data, and theta is the parameters for a probability function P (in this case, our neural network) that models prediction of the labels given the input data.  


If you assume that the labels are conditionally independent given X and theta, then you have that the above expression is equivalent to:  


Product of (P(yj | X, theta) for j =1,..., k.  


From there you can derive the log likelihood and the expression for multi-label binary cross entropy. However, is this a reasonable assumption? For example, if you had two sets of identical labels, the joint probability for both labels would be equivalent to the probability of a single label. This does not seem like it's captured here. However, intuitively, for a neural network if you just have several sigmoid outputs, then, other than the last fully connected layer, all the parameters/activations are shared for label predictions so intuitively it seems to capture the label dependencies. How is this addressed within the use of the loss function?   


&amp;#x200B;",3,1,False,self,,,,,
853,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,11,a6bbtr,self.MachineLearning,[D] Best way to get binary class labels in a sparse multiclass classification task?,https://www.reddit.com/r/MachineLearning/comments/a6bbtr/d_best_way_to_get_binary_class_labels_in_a_sparse/,dominik_schmidt,1544839941,"I'm training a neural network to classify images where each image has on average 17 of 2000 possible labels. (Some images only have one or two labels, some have 20-30). I'm using sigmoid\_cross\_entropy as loss which works well but I'm unsure how to convert the network output to binary class labels (like `[0, 0, 1, 0, 0, 1, 1]`) so I can compute the accuracy and get the networks results.

I tried applying `round(sigmoid(predictions))` but since only a few of the 2000 labels are ever correct the networks output mostly looks like this: `[-2.86759, -5.62045, -4.2197, -3.3005, -4.7657]` which is all zeros after applying sigmoid and round. What's the best way to do this?",2,1,False,self,,,,,
854,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,12,a6bws1,self.MachineLearning,[D] I was playing around with SSD Face Detection and a smartphone IP camera. This is what happenned...,https://www.reddit.com/r/MachineLearning/comments/a6bws1/d_i_was_playing_around_with_ssd_face_detection/,HotVector,1544844829,"I was playing around with face detection using a Caffe model and cv2.dnn, and I thought I would try using with with the IP camera app on my phone. Just for funsies, I opened up a page of Donald Trump pictures. Here's a video of what happened. I kept tilting the phone back and forth from Donald Trump's picture to the detected frame, so that is why it got really redshifted. Just thought I would share it, didn't know where else I could share it...

![video](asif8u4w2d421 ""Video of Donald Trump with face detection"")",2,1,False,self,,,,,
855,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,12,a6c12x,self.MachineLearning,[R] Feature Denoising for Improving Adversarial Robustness,https://www.reddit.com/r/MachineLearning/comments/a6c12x/r_feature_denoising_for_improving_adversarial/,cihang-xie,1544845868,"Please check our new adversarial defense paper https://arxiv.org/pdf/1812.03411.pdf. We mainly show two things:

(1) adversarial perturbations at pixel level produce noise at feature levels

(2) denoising at feature levels can improve adversarial robustness",10,1,False,self,,,,,
856,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,13,a6c7uk,self.MachineLearning,"[D] why logarithmic loss, logistic loss and cross-entropy loss the same thing?",https://www.reddit.com/r/MachineLearning/comments/a6c7uk/d_why_logarithmic_loss_logistic_loss_and/,marksteve4,1544847465,the nomenclature really confuses me,1,1,False,self,,,,,
857,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,13,a6c9k3,self.MachineLearning,How do you deploy Machine Learning in production?,https://www.reddit.com/r/MachineLearning/comments/a6c9k3/how_do_you_deploy_machine_learning_in_production/,mln00b13,1544847892,[removed],0,1,False,self,,,,,
858,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,13,a6camy,xkcd.com,xkcd 2085: arXiv - Relevant to discussions on this subreddit from earlier this year.,https://www.reddit.com/r/MachineLearning/comments/a6camy/xkcd_2085_arxiv_relevant_to_discussions_on_this/,ClydeMachine,1544848178,,0,1,False,default,,,,,
859,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,13,a6cbzm,self.MachineLearning,[D] What is the best ML paper you read in 2018 and why?,https://www.reddit.com/r/MachineLearning/comments/a6cbzm/d_what_is_the_best_ml_paper_you_read_in_2018_and/,omniscientclown,1544848498,"Enjoyed this thread last year, so I am making a one for this year. ",96,1,False,self,,,,,
860,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,13,a6cgup,youtu.be,"[R] Reproducible, Reusable, and Robust Reinforcement Learning - NeurIPS 2018 Invited Talk",https://www.reddit.com/r/MachineLearning/comments/a6cgup/r_reproducible_reusable_and_robust_reinforcement/,pienuthome,1544849753,,0,1,False,default,,,,,
861,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,14,a6cmlv,bair.berkeley.edu,[R] Soft Actor CriticDeep Reinforcement Learning with Real-World Robots,https://www.reddit.com/r/MachineLearning/comments/a6cmlv/r_soft_actor_criticdeep_reinforcement_learning/,SkiddyX,1544851193,,3,1,False,https://b.thumbs.redditmedia.com/Us2tyr4GGw7GfAqr7aak6ciqdFUiycrpekCuOl59t2A.jpg,,,,,
862,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,14,a6col4,youtu.be,"[R] Reproducible, Reusable, and Robust Reinforcement Learning - NeurIPS 2018 Invited Talk",https://www.reddit.com/r/MachineLearning/comments/a6col4/r_reproducible_reusable_and_robust_reinforcement/,pienuthome,1544851717,,0,1,False,https://a.thumbs.redditmedia.com/h1glg92f_wyexN6ZpzF7YT3uXKd7Ryuq4grwJsCMjs4.jpg,,,,,
863,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,15,a6d12t,self.MachineLearning,Advice on receipt classification,https://www.reddit.com/r/MachineLearning/comments/a6d12t/advice_on_receipt_classification/,HouseLTN,1544855059,[removed],0,1,False,self,,,,,
864,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,15,a6d25c,self.MachineLearning,ICLR decisions/camera ready date,https://www.reddit.com/r/MachineLearning/comments/a6d25c/iclr_decisionscamera_ready_date/,eleos_phobos,1544855375,[removed],0,1,False,self,,,,,
865,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,16,a6dae6,self.MachineLearning,First timer trying to build a ML/DL PC build. How is this build?,https://www.reddit.com/r/MachineLearning/comments/a6dae6/first_timer_trying_to_build_a_mldl_pc_build_how/,valcroft,1544857746,"Hello! I came up with this build for getting into deep learning and playing some steam games. I thought that this would be a better alternative than keep on using Paperspace when I don't need too much power.  And just run my stuff there when I'll need to do something big in the future. For context, I own a Macbook Pro 13 inch 2015 as my daily driver, for work and school. I don't own a desktop PC yet.

How is this budget-wise and capability-wise?

&amp;#x200B;

**Power Supply**: Seasonic M12II-520 EVO

**Chassis**:	Deep cool Tesseract Mid-Tower

**SSD:** SanDisk Plus 240GB

**Hard Drive:**  WD Caviar Blue 1TB

**RAM:** GSKILL RIPJAWS V 16GB DUAL DDR4 3200MHZ (F4-3200C16D-16GVKB)

**Processor:** AMD Ryzen 5 2600

**Motherboard:** ASROCK AB350 PRO4

**GPU:**  GIGABYTE GTX 1060 WINFORCE OC 6GB GV-N1060WF2OC-6GD

&amp;#x200B;

If in USD, this is going to cost me about $918. Am not from the US.

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
866,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,19,a6e7fb,self.MachineLearning,Are there any methods to determine whats important in an image?,https://www.reddit.com/r/MachineLearning/comments/a6e7fb/are_there_any_methods_to_determine_whats/,shamoons,1544868837,[removed],0,1,False,self,,,,,
867,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,19,a6eaf3,venturebeat.com,VentureBeat: AI Weekly: This machine learning report is required reading.,https://www.reddit.com/r/MachineLearning/comments/a6eaf3/venturebeat_ai_weekly_this_machine_learning/,redbiteX1,1544869887,,0,1,False,default,,,,,
868,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,20,a6ei28,gitxiv.com,GitXiv: Collaborative Open Computer Science,https://www.reddit.com/r/MachineLearning/comments/a6ei28/gitxiv_collaborative_open_computer_science/,donutloop,1544872615,,0,1,False,default,,,,,
869,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,21,a6erpw,self.MachineLearning,[Project] The TL;DR Challenge,https://www.reddit.com/r/MachineLearning/comments/a6erpw/project_the_tldr_challenge/,codemetro,1544875959,"&amp;#x200B;

 We invite you to participate in [**The TL;DR Challenge**](https://tldr.webis.de/) \-  given a post, generate its TL;DR!

We have mined almost **3 Million posts** from our beloved Reddit containing tl;drs to make the dataset for this challenge! So basically, **not** another boring dataset full of News articles ;)

How to participate ?

1. Register at [tldr.webis.de](https://tldr.webis.de) and download the training data.
2. Develop and train a summarization model on our Webis-TLDR-17 dataset.
3. Deploy the trained model on a virtual machine we assign to you.
4. Use [tira.io](https://www.tira.io/) to self-evaluate the deployed model on the test set.
5. Briefly tell us about your approach in a paper.

Here are two examples of what we generated using a base Seq2Seq model :

**OP - 1**

just talked to catholic priest on a wedding . guys , have no mercy , please . the shit they preach is a mind killer for our civilization . religious people are fucking dumb , dumb fucks and they spread shit all over the place . i fucking spit on catholics . my whole family is religious and i can ' t have intellectual conversation on any subject with them . we can not tolerate any of this shit any longer . i ' m picking hitchens and the rest of dawkins for my holidays and i ' m going militant then . hoooly shit ! sorry for rant but the level of bullshit they can get over with is killing me .

**Original TL;DR**

religion = mental retardation .

**Generated TL;DR**

religion is fucking stupid .

&amp;#x200B;

**OP - 2**

I finished Path of Daggers earlier this month and took a break to read I Am Pilgrim ( I HIGHLY recommend ) and I am now ready to start my journey into book 9 . One thing , I ' ve forgotten a few plot threads . I can ' t search for them as possible spoilers , so what do I need to know going in ? Oh , and SPOILERS for those not up to here and please , no spoilers for me . I love this series to much for it to be ruined . I know Faile has been taken but I ' m not sure about the other main characters whereabouts and smaller character plots .

**Original TL;DR**

where are the characters at and what are they hoping to do ? People refreshing my memory would be greatly appreciated .

**Generated TL;DR**

What do I need to know to start my journey into book 9 ?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

We hope you find this dataset useful. Looking forward for your participation :)

&amp;#x200B;",31,1,False,self,,,,,
870,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,21,a6exd8,self.MachineLearning,Face similarity project : Does two face images belong to the same person?,https://www.reddit.com/r/MachineLearning/comments/a6exd8/face_similarity_project_does_two_face_images/,csharp_ai,1544877797,[removed],0,1,False,self,,,,,
871,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,22,a6f9sj,self.MachineLearning,When to use projected gradient descent vs lagrangian methods?,https://www.reddit.com/r/MachineLearning/comments/a6f9sj/when_to_use_projected_gradient_descent_vs/,collegemathtutor,1544881255,[removed],0,1,False,self,,,,,
872,MachineLearning,t5_2r3gv,2018-12-15,2018,12,15,23,a6flej,self.MachineLearning,[Discussion] Free online course on Probabilistic Programming,https://www.reddit.com/r/MachineLearning/comments/a6flej/discussion_free_online_course_on_probabilistic/,springcoil,1544884309,[Learn Probabilistic Programming](https://mailchi.mp/4940464d8e53/probprogrammer) with this free online course I put together. It'll take you through how to build Bayesian models from scratch.,5,1,False,self,,,,,
873,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,0,a6fxaz,self.MachineLearning,[D] When to use projected gradient descent vs lagrangian methods?,https://www.reddit.com/r/MachineLearning/comments/a6fxaz/d_when_to_use_projected_gradient_descent_vs/,collegemathtutor,1544887107,"Projected gradient descent (PGD) tries to solve an contrained optimization problem by first taking a normal gradient descent (GD) step, and then mapping the result of this to the feasible set, i.e. takes a step to modify this result to make the constraint satisfied.

AFAIK, even lagrangian methods deal with the same setting, but solving a hybrid loss involving the optimization objective as well as the constraint, albeit in practical cases choosing to multipliers by hand.

I think I am missing some subtle difference between the two methods. What are the conditions in which either is to be used?
",9,1,False,self,,,,,
874,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,0,a6fzvl,self.MachineLearning,Adaboost explanation please,https://www.reddit.com/r/MachineLearning/comments/a6fzvl/adaboost_explanation_please/,Crossfire_dcr,1544887682,[removed],0,1,False,self,,,,,
875,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,0,a6g308,self.MachineLearning,Facebook prophet- Time series analysis,https://www.reddit.com/r/MachineLearning/comments/a6g308/facebook_prophet_time_series_analysis/,Deepak_ram,1544888382,[removed],0,1,False,self,,,,,
876,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,1,a6g9qd,therichestreviews.com,Python Machine Learning Sebastian Raschka PDF|Python Machine Learning PDF,https://www.reddit.com/r/MachineLearning/comments/a6g9qd/python_machine_learning_sebastian_raschka/,marinating,1544889821,,0,1,False,spoiler,,,,,
877,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,1,a6gf32,medium.com, AI Diary #5,https://www.reddit.com/r/MachineLearning/comments/a6gf32/ai_diary_5/,omarsar,1544890920,,0,1,False,default,,,,,
878,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,1,a6gh8d,self.MachineLearning,How to submit my person MNIST-like image to TensorFlow? I trained on the MNIST dataset and would like to try my own handwritten digit on the neural network. Having problems with the format of the image data in TensorFlow :(,https://www.reddit.com/r/MachineLearning/comments/a6gh8d/how_to_submit_my_person_mnistlike_image_to/,cudaeducation,1544891360,"I know that you can train your TensorFlow network on the part of the MNIST data, then test it on another part of the MNIST data.  That's wonderful!  But what about trying to submit an image of your own handwritten digit to the TensorFlow neural network and see if it can ""read"" the number correctly?!?!

&amp;#x200B;

Please tell me that I'm not the only person on planet earth who has thought of doing this!  It really is a simple matter, but I'm having all kinds of grief getting TensorFlow to read my image properly and have the pixel data etc. arranged in a way that gels with what you would expect from an MNIST image.

&amp;#x200B;

I sincerely hope I'm not the only person that has tried to do this.",0,1,False,self,,,,,
879,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,2,a6gv6h,wired.com,Wired's interview with Geoffrey Hinton,https://www.reddit.com/r/MachineLearning/comments/a6gv6h/wireds_interview_with_geoffrey_hinton/,IndiaNgineer,1544894067,,0,1,False,https://b.thumbs.redditmedia.com/UhGTT2viX2YYAjgbtU07ahiDKtZ1nFIBJLvZeiMHwSA.jpg,,,,,
880,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,2,a6gvrr,youtu.be,[Project] Computer Vision: Real-time Object Detection &amp; Classification with Deep Learning on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/a6gvrr/project_computer_vision_realtime_object_detection/,seemingly_omniscient,1544894182,,1,1,False,default,,,,,
881,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,3,a6he1k,therichestreviews.com,Windows 10 for dummies PDF [Direct Download Link],https://www.reddit.com/r/MachineLearning/comments/a6he1k/windows_10_for_dummies_pdf_direct_download_link/,marinating,1544897712,,0,1,False,default,,,,,
882,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,3,a6heph,self.MachineLearning,"Given advances in AI and Machine learning, what is the possibility of there being pilotless commercial aircraft in the next 4-5 decades?",https://www.reddit.com/r/MachineLearning/comments/a6heph/given_advances_in_ai_and_machine_learning_what_is/,stratosfeerick,1544897826,[removed],0,1,False,self,,,,,
883,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,3,a6hkau,self.MachineLearning,[D] Improving the performance of GA on training this NN?,https://www.reddit.com/r/MachineLearning/comments/a6hkau/d_improving_the_performance_of_ga_on_training/,patniemeyer,1544898937,"The example below trains a three layer auto encoder on MNIST using GA.  It sort of works but compared to back propagation is ridiculously slow and tends to find a local minima with a blurred image covering the region of all of the digits.  If I limit it to just two images it will eventually start to differentiate them, but very slowly.

&amp;#x200B;

The only improvements Ive made so far that helped are:

1. Doing crossover and mutation per-layer rather than all in one giant bag.  This helps give equal time to the bottleneck and small layers

2. Making sure that there are a variety of mutation rates down to single element mutations, so that it doesnt get stuck easily.

&amp;#x200B;

As for raw performance - I cant find a good way to parallelize this in Python because the genes are so large that the (lame) process pool execution which sends data over IPC is slower than serial at this point.  I am sure I can find some way around this if needed but for the moment Im just wondering about the basic GA architecture and if Im missing something.

&amp;#x200B;

Is this just a particularly bad domain for GA due to the large number of parameters?  Any suggestions appreciated.  Thanks.

&amp;#x200B;

[https://github.com/patniemeyer/ga-autoencoder/blob/master/autoencoder-evolve.py](https://github.com/patniemeyer/ga-autoencoder/blob/master/autoencoder-evolve.py)

&amp;#x200B;

&amp;#x200B;",9,1,False,self,,,,,
884,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,3,a6ho78,oodlestechnologies.com,Blockchain and Machine Learning To Predict Consumer Behavior,https://www.reddit.com/r/MachineLearning/comments/a6ho78/blockchain_and_machine_learning_to_predict/,tech-info,1544899678,,0,1,False,default,,,,,
885,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,3,a6hpa4,github.com,Here I built a simple neural network with a relu activation function to classify the mnist dataset. It's accuracy is 96% on the training data and 93% on the test data. Take a look!,https://www.reddit.com/r/MachineLearning/comments/a6hpa4/here_i_built_a_simple_neural_network_with_a_relu/,taylerallen6,1544899884,,0,1,False,https://b.thumbs.redditmedia.com/qBrPtYZ8-64Wwegdv-q3lEiuciA7WtX3KUGC_Fst2dg.jpg,,,,,
886,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,4,a6i24h,self.MachineLearning,[D] NeurIPS 2019 T-shirt ideas? (for fun),https://www.reddit.com/r/MachineLearning/comments/a6i24h/d_neurips_2019_tshirt_ideas_for_fun/,daddydickie,1544902389,"I have a couple of ideas:

1. NeurIPS' new neuro-systems
2. Make NeurIPS NIPS again.

&amp;#x200B;",0,1,False,self,,,,,
887,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,5,a6imuk,self.MachineLearning,An algorithm using backprob to solve a single state environment?,https://www.reddit.com/r/MachineLearning/comments/a6imuk/an_algorithm_using_backprob_to_solve_a_single/,TheLetterS1x,1544906409,[removed],0,1,False,self,,,,,
888,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,5,a6ir49,self.MachineLearning,[D] An algorithm using backprob to solve a single state environment?,https://www.reddit.com/r/MachineLearning/comments/a6ir49/d_an_algorithm_using_backprob_to_solve_a_single/,TheLetterS1x,1544907214,"Reinforcement learning algorithms are optimized using a Markov decision process. This would not be possible in an environment with only one state, for example, an agent whose goal is to design a house.

Are there any approaches to this problem that uses backpropagation? I am aware Evolutionary algorithms could solve this.",3,1,False,self,,,,,
889,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,6,a6j26o,self.MachineLearning,"[D] What are you 'stuck' on these days? Papers, concepts, coding, anything ML related etc.",https://www.reddit.com/r/MachineLearning/comments/a6j26o/d_what_are_you_stuck_on_these_days_papers/,AdditionalWay,1544909309,"Me: Converting models to Keras in order to use colab's free TPUs. 

",108,1,False,self,,,,,
890,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,9,a6kduu,self.MachineLearning,[D] What's the open source state of the art in human pose tracking?,https://www.reddit.com/r/MachineLearning/comments/a6kduu/d_whats_the_open_source_state_of_the_art_in_human/,Pafnouti,1544919202,"[DensePose](https://github.com/facebookresearch/DensePose) and [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) are quite good at pose detection, but they work on a frame by frame basis, and I can see that on videos the predictions can be a bit inconsistent temporaly.  
Is there some opensource code that uses tracking across frames to improve the detection?",13,1,False,self,,,,,
891,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,10,a6ktkt,youtube.com,"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier for anyone reading this. Hope you guys found it helpful!",https://www.reddit.com/r/MachineLearning/comments/a6ktkt/for_me_one_of_the_main_barriers_to_the_world_of/,antaloaalonso,1544922623,,0,1,False,default,,,,,
892,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,10,a6l0tp,self.MachineLearning,Does domain randomization work if target environment changes?,https://www.reddit.com/r/MachineLearning/comments/a6l0tp/does_domain_randomization_work_if_target/,DeepFi,1544924314,[removed],0,1,False,self,,,,,
893,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,11,a6lliz,self.MachineLearning,Is YOLO a good choice for real-time defect detection on images?,https://www.reddit.com/r/MachineLearning/comments/a6lliz/is_yolo_a_good_choice_for_realtime_defect/,niki_niki123,1544929068,[removed],0,1,False,self,,,,,
894,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,13,a6m3rs,self.MachineLearning,PC Build Critique for ML Research,https://www.reddit.com/r/MachineLearning/comments/a6m3rs/pc_build_critique_for_ml_research/,peep186,1544933464,[removed],0,1,False,self,,,,,
895,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,13,a6mbk8,self.MachineLearning,[D] Why does SAGAN use a matrix multiply with a transposed feature map?,https://www.reddit.com/r/MachineLearning/comments/a6mbk8/d_why_does_sagan_use_a_matrix_multiply_with_a/,VariousDistribution,1544935451,"I was wondering why, specifically, the SAGAN paper uses the architecture it does. There doesn't appear to be much justification as to the exact architecture or ablation testing of other models. The matrix multiplication of the feature maps feels especially weird, as there's no obvious connection. Any ideas where I can find detailed information on this? ",10,1,False,self,,,,,
896,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,15,a6mv19,analyticsinsight.net,Machine Learning and Mobile Technology: Amalgamation for the Future | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/a6mv19/machine_learning_and_mobile_technology/,analyticsinsight,1544940775,,0,1,False,default,,,,,
897,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,15,a6n2o6,youtube.com,[D] Why is Mean Squared Error used?,https://www.reddit.com/r/MachineLearning/comments/a6n2o6/d_why_is_mean_squared_error_used/,VCubingX,1544943138,,0,1,False,default,,,,,
898,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,16,a6n79q,self.MachineLearning,How to integrate ML with Embedded systems ?,https://www.reddit.com/r/MachineLearning/comments/a6n79q/how_to_integrate_ml_with_embedded_systems/,karker97,1544944619,[removed],0,1,False,self,,,,,
899,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,16,a6n8s8,youtu.be,[D] Okay now I see lift machine can be colling by celing fan,https://www.reddit.com/r/MachineLearning/comments/a6n8s8/d_okay_now_i_see_lift_machine_can_be_colling_by/,tutorial_for_all,1544945123,,0,1,False,https://b.thumbs.redditmedia.com/AcNZv2Yh_YcZ1q-zWWb3ans7IFoEbUNkV3G01xmlGSs.jpg,,,,,
900,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,16,a6nck7,updegree.org,Data Science Masterclass - 4 Projects + 8 Case Studies + Interview Preparation (Coupon code : AYUSS11),https://www.reddit.com/r/MachineLearning/comments/a6nck7/data_science_masterclass_4_projects_8_case/,patidarayush11,1544946415,,0,1,False,default,,,,,
901,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,17,a6ngd0,self.datascience,DNN in python optimization.,https://www.reddit.com/r/MachineLearning/comments/a6ngd0/dnn_in_python_optimization/,vinaybk8,1544947728,,0,1,False,default,,,,,
902,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,19,a6nzzj,comixify.ii.pw.edu.pl,[D] Comixify: Converting a video into a comics,https://www.reddit.com/r/MachineLearning/comments/a6nzzj/d_comixify_converting_a_video_into_a_comics/,sksq9,1544954877,,0,1,False,default,,,,,
903,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,20,a6ocbn,self.MachineLearning,Have the academic requirements for a career in ML reduced in the UK job market? Is a masters degree required?,https://www.reddit.com/r/MachineLearning/comments/a6ocbn/have_the_academic_requirements_for_a_career_in_ml/,Aaraeus,1544959298,[removed],0,1,False,self,,,,,
904,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,21,a6otco,self.MachineLearning,HOW CAN I GET THE DISTRIBUTION FOR LOTTO GENERATED NUMBERS,https://www.reddit.com/r/MachineLearning/comments/a6otco/how_can_i_get_the_distribution_for_lotto/,AJIBODE,1544965067,[removed],0,1,False,self,,,,,
905,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,23,a6p9xn,self.MachineLearning,hand written characters/MNIST,https://www.reddit.com/r/MachineLearning/comments/a6p9xn/hand_written_charactersmnist/,alpha_so,1544969550,[removed],0,1,False,self,,,,,
906,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,23,a6pe0h,self.MachineLearning,What is the state of the art paper for face re-identification?,https://www.reddit.com/r/MachineLearning/comments/a6pe0h/what_is_the_state_of_the_art_paper_for_face/,Thmsvicente,1544970587,[removed],0,1,False,self,,,,,
907,MachineLearning,t5_2r3gv,2018-12-16,2018,12,16,23,a6piux,ibtimes.sg,Scientists claim Machine Learning can help to predict infection risk from hospital bug,https://www.reddit.com/r/MachineLearning/comments/a6piux/scientists_claim_machine_learning_can_help_to/,BhaswatiGuha19,1544971739,,0,1,False,default,,,,,
908,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,1,a6q42c,self.MachineLearning,[D] Whats the current state of the art CNN architecture? nesNe,https://www.reddit.com/r/MachineLearning/comments/a6q42c/d_whats_the_current_state_of_the_art_cnn/,Tollpatsch93,1544976167,"Hello Guys,

Im a bit confused about what is the current state of the art architecture? Some sites cite ResNeXt (https://arxiv.org/abs/1611.05431), some DenseNet(https://arxiv.org/abs/1608.06993).

As far as i know is ResNeXt kind of a combination of inception and resnet and DenseNet a whole new architecture.
Which one would you cite as state of the art and why this one and not the other? 

Because its from 2017/2016 im absoluty not sure if there is not a other ""better"" architecture.

Thanks in advance 

",9,1,False,self,,,,,
909,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,1,a6qbgn,youtu.be,Robots were allowed to kill people. Machine learning,https://www.reddit.com/r/MachineLearning/comments/a6qbgn/robots_were_allowed_to_kill_people_machine/,cmillionaire9,1544977552,,0,1,False,https://b.thumbs.redditmedia.com/bUDYcn6yor-tulzgntK_WUYq0dwsI57OhTuapYta_yQ.jpg,,,,,
910,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,2,a6qpu6,self.MachineLearning,ML Applications,https://www.reddit.com/r/MachineLearning/comments/a6qpu6/ml_applications/,rocco20,1544980177,"Hi, I'm currently doing a few courses on the subject but I am losing motivation because I have no direction on what to work towards. What are some interesting applications I can work towards that have not been done yet. Something that can be done by one person.",0,1,False,self,,,,,
911,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,2,a6quhc,self.MachineLearning,Reinforcement Learning Tutorial With Demo,https://www.reddit.com/r/MachineLearning/comments/a6quhc/reinforcement_learning_tutorial_with_demo/,obsezer,1544981011,[removed],0,1,False,self,,,,,
912,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,2,a6qzek,web.mit.edu,[R] REINFORCEMENT LEARNING AND OPTIMAL CONTROL by Dimitri P. Bertsekas,https://www.reddit.com/r/MachineLearning/comments/a6qzek/r_reinforcement_learning_and_optimal_control_by/,tigerneil,1544981874,,9,1,False,default,,,,,
913,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,3,a6rjh2,self.MachineLearning,[D] Which deep learning advancements transfer to vanilla MLPs?,https://www.reddit.com/r/MachineLearning/comments/a6rjh2/d_which_deep_learning_advancements_transfer_to/,joshuacpeterson,1544985522,"Any papers tracking this? I have a large tabular dataset where MLPs are beating out XGB and RF (after extensive gridding) and would like to push the result further. Does Mixup work with well with MLPs? Residual connections? BatchNormalization seems to make things worse. For ensembling, how does one maximize MLP diversity?",9,1,False,self,,,,,
914,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,3,a6rlck,self.MachineLearning,[D] Are you doing distributed training?,https://www.reddit.com/r/MachineLearning/comments/a6rlck/d_are_you_doing_distributed_training/,yvrdeeplearning,1544985876,"A question for the ML community are you doing distributed training -- multiple CPU/GPU on single host, multiple machines (each with multiple GPUs) etc? Curious to hear about your setup and use-case.

If you're not, it'd also be interested to hear why not e.g. not enough training data to make it worthwhile, poor framework support etc.",27,1,False,self,,,,,
915,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,4,a6rtvr,self.MachineLearning,Semantics of the word [AI],https://www.reddit.com/r/MachineLearning/comments/a6rtvr/semantics_of_the_word_ai/,__olamilekan__,1544987445,[removed],0,1,False,self,,,,,
916,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,5,a6sc7w,self.MachineLearning,[D] Pieter Abbeel: Deep Reinforcement Learning (podcast conversation),https://www.reddit.com/r/MachineLearning/comments/a6sc7w/d_pieter_abbeel_deep_reinforcement_learning/,UltraMarathonMan,1544990777,"Here's my conversation with Pieter Abbeel, one of the top researchers in the world working on how to make robots understand and interact with the world around them, especially through deep reinforcement learning.

Podcast audio version: [https://lexfridman.com/pieter-abbeel](https://lexfridman.com/pieter-abbeel)

Video version: 

[https://www.youtube.com/watch?v=l-mYLq6eZPY](https://www.youtube.com/watch?v=l-mYLq6eZPY)

&amp;#x200B;",4,1,False,self,,,,,
917,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,5,a6sd47,self.MachineLearning,"[R] Reproducible, Reusable, and Robust Reinforcement Learning - NeurIPS 2018",https://www.reddit.com/r/MachineLearning/comments/a6sd47/r_reproducible_reusable_and_robust_reinforcement/,pienuthome,1544990944,"Joelle Pineau's very nice critique of contemporary reinforcement learning research. Some key highlights:

\- Suggestions on improving reproducibility and robustness of reinforcement learning methods

\- Suggestions on injecting natural noise into ""artificial"" RL benchmarks (Atari games)

\- Do research that matters. Batch learning is often times more realistic than tabular rasa exploration",0,1,False,self,,,,,
918,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,5,a6sg9u,youtu.be,"[R] Reproducible, Reusable, and Robust Reinforcement Learning - NeurIPS 2018",https://www.reddit.com/r/MachineLearning/comments/a6sg9u/r_reproducible_reusable_and_robust_reinforcement/,pienuthome,1544991530,,1,1,False,https://a.thumbs.redditmedia.com/h1glg92f_wyexN6ZpzF7YT3uXKd7Ryuq4grwJsCMjs4.jpg,,,,,
919,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,6,a6svwb,self.MachineLearning,[D] Sequential decision making in simple games.,https://www.reddit.com/r/MachineLearning/comments/a6svwb/d_sequential_decision_making_in_simple_games/,chubbyspartn,1544994329,"Sequential decision making has been interesting me a lot recently, I am attempting to understand it better and was wondering if anyone on this sub could point me in the right direction.  


The game I am using to learn sequential decision making is Gomoku (19x19 tick tack toe where you need 5 in a row to win). My eventual hope was to develop some type of bot that could eventually play some type of real time 2d 1v1 game (think asteroids, but w/o the asertiods and its just two ships 1v1ing)

&amp;#x200B;

 I have a decent understanding of supervised learning, but supervised learning does not seem to translate well into sequences of actions where the value of an action cannot be known until some terminal state is reached, and even at that terminal state we only know the value of the sequence, not necessarily the value of each action in the sequence. I studied a bit of [Q-learning](https://en.wikipedia.org/wiki/Q-learning), but this method also doesnt seem well suited to even a simple game like gomoku, since there are so many unique states of the board and at each state there is also a large number of actions available. Similarly with approximate Q-learning, even though the state space is reduced it is still very large, and the learning agent will only be as good as the feature functions I am able to design.   


Are there any methods of active learning that would be well suited to these types of games?",6,1,False,self,,,,,
920,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,6,a6t6bm,self.MachineLearning,I play a mmorpg on steam and am wondering if it would be possible to make a bot that plays the game and uses machine learning. Also how hard this would be.,https://www.reddit.com/r/MachineLearning/comments/a6t6bm/i_play_a_mmorpg_on_steam_and_am_wondering_if_it/,TheKillerRabbit1,1544996240,[removed],0,1,False,self,,,,,
921,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,6,a6tclk,self.MachineLearning,Naive Bayes-like text classification with non-binary term-document matrix?,https://www.reddit.com/r/MachineLearning/comments/a6tclk/naive_bayeslike_text_classification_with/,mosskin-woast,1544997448,[removed],0,1,False,self,,,,,
922,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,7,a6tjeb,youtu.be,[P] Computer Vision: Real-time Object Detection &amp; Classification with Deep Learning on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/a6tjeb/p_computer_vision_realtime_object_detection/,seemingly_omniscient,1544998738,,0,1,False,https://b.thumbs.redditmedia.com/RJCSh4XanNFAcdp3IF-Gu_m-K5Puj27cCvNCBfow_mA.jpg,,,,,
923,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,7,a6tsbk,self.MachineLearning,how to add tanh to one embedding layer in keras,https://www.reddit.com/r/MachineLearning/comments/a6tsbk/how_to_add_tanh_to_one_embedding_layer_in_keras/,ejiido,1545000445,[removed],0,1,False,self,,,,,
924,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,7,a6tv4d,self.MachineLearning,"[D] In general, how representative is this sub of the overall ML literature?",https://www.reddit.com/r/MachineLearning/comments/a6tv4d/d_in_general_how_representative_is_this_sub_of/,mintman777,1545001037,"I'm an undergrad that's been subbed to this subreddit for a few months now, and I've noticed that most of the papers posted on here are related to deep learning/neural networks in some way, with the occasional PPL paper in there as well.  I'm curious if this is what the field is actually like right now in academia.  Are there still notable people doing work on stuff like ""traditional"" ML (SVMs, random forests, etc), or more theoretical stuff such as learning theory, or is most of the research today focused on deep learning?  I ask this because I'm interested in Bayesian Nonparametrics, but I don't see papers related to that topic posted much to this subreddit.",11,1,False,self,,,,,
925,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,8,a6u436,self.MachineLearning,[D] How to read machine learning conference paper effectively?,https://www.reddit.com/r/MachineLearning/comments/a6u436/d_how_to_read_machine_learning_conference_paper/,zcwang0702,1545002824,Any suggestions? Thanks in advance.,16,1,False,self,,,,,
926,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,8,a6u69q,self.MachineLearning,Machine Learning App,https://www.reddit.com/r/MachineLearning/comments/a6u69q/machine_learning_app/,puffycloud98,1545003255,[removed],0,1,False,self,,,,,
927,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,8,a6u6z4,self.MachineLearning,"[R] Struct2Depth - Predicting object depth in dynamic environments (robots, autonomous cars)",https://www.reddit.com/r/MachineLearning/comments/a6u6z4/r_struct2depth_predicting_object_depth_in_dynamic/,tldrtldreverything,1545003406,"Hey,
I recently encountered a new Google Brain paper about depth prediction. It's a pretty cool subject that's relevant to important technologies like drones, autonomous cars, etc. This is my summary of the paper, I'm happy to get feedback and answer any questions you may have.
https://www.lyrn.ai/2018/12/12/struct2depth-predicting-object-depth-in-dynamic-environments/",4,1,False,self,,,,,
928,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,8,a6u82a,self.MachineLearning,"We created a free, fully automatic tool to remove the background of a person image: remove.bg",https://www.reddit.com/r/MachineLearning/comments/a6u82a/we_created_a_free_fully_automatic_tool_to_remove/,Tuxa13,1545003630,[removed],0,1,False,self,,,,,
929,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,8,a6u9br,self.MachineLearning,How do you stress test a trained algorithm?,https://www.reddit.com/r/MachineLearning/comments/a6u9br/how_do_you_stress_test_a_trained_algorithm/,Drgoldsz22,1545003886,[removed],0,1,False,self,,,,,
930,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,8,a6udcs,self.MachineLearning,Notation for neural networks,https://www.reddit.com/r/MachineLearning/comments/a6udcs/notation_for_neural_networks/,bharat0to,1545004719,[removed],0,1,False,self,,,,,
931,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,9,a6uiee,self.MachineLearning,Trouble understanding related work for the ECCV'18 paper : https://arxiv.org/pdf/1808.06032.pdf,https://www.reddit.com/r/MachineLearning/comments/a6uiee/trouble_understanding_related_work_for_the_eccv18/,spoiltForChoice,1545005751,[removed],0,1,False,self,,,,,
932,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,10,a6uyvs,self.MachineLearning,[P] Image Processing For Real Estate Photography,https://www.reddit.com/r/MachineLearning/comments/a6uyvs/p_image_processing_for_real_estate_photography/,carsonpoole,1545009088,"In real estate photography, good photographers will use high dollar flashes to get accurate colors, and mix that shot with an ambient light shot to get natural shadows. This is a very common process called ""flambient"" photography. I decided to see how well a CycleGAN could do with translating shots from ambient -&gt; flambient. I also used this on HDR photos, which are notorious in the REPhotography community as being for beginners and producing very inaccurate colors and lighting. 

&amp;#x200B;

I did 200 epochs with a decaying LR and around 250 total photos to train on.

&amp;#x200B;

Well, without further ado, here's the results!

&amp;#x200B;

# HDR -&gt; Flash (ish)

https://i.redd.it/z55pmh5anq421.png

# More examples

&amp;#x200B;

https://i.redd.it/pja23xnknq421.png

https://i.redd.it/ic5h56oknq421.png

https://i.redd.it/beu30ynknq421.png

&amp;#x200B;

&amp;#x200B;",27,1,False,self,,,,,
933,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,11,a6vkiy,self.MachineLearning,[D] Survey | Academic Research on Data activities in business (3 mins max with 10 questions),https://www.reddit.com/r/MachineLearning/comments/a6vkiy/d_survey_academic_research_on_data_activities_in/,CalmLetterhead,1545013673,"Hey redditors !!

&amp;#x200B;

I am working on an academic research and looking for collecting insights from the data scientist community. I am interested to learn how organizations performing their data activities, including, preparation, analysis and quality assessment -

&amp;#x200B;

[https://docs.google.com/forms/d/e/1FAIpQLSejvjuLSnxiJzX0Rwo1d4LpNgp03KvtCVKHuAc7vDOx\_DtEmw/viewform](https://docs.google.com/forms/d/e/1FAIpQLSejvjuLSnxiJzX0Rwo1d4LpNgp03KvtCVKHuAc7vDOx_DtEmw/viewform)

&amp;#x200B;

Any help is greatly appreciated!",1,1,False,self,,,,,
934,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,12,a6vwmu,self.MachineLearning,[D] Idea: iOS device as portable external GPU for deep learning.,https://www.reddit.com/r/MachineLearning/comments/a6vwmu/d_idea_ios_device_as_portable_external_gpu_for/,RavlaAlvar,1545016239,"GPU of iOS device has getting increasingly power, but we don't really have much application that can utilise such computation aside from gaming. 

&amp;#x200B;

I am not talking on device training that current coreML enabled but, something like a tensorflow backend that run on iOS device that is plugged to a computer. 

&amp;#x200B;

If this is possible somehow, it would further lower that entry barrier of deep learning. ",10,1,False,self,,,,,
935,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,14,a6wq76,self.MachineLearning,[D] Confusions regarding RetinaNet,https://www.reddit.com/r/MachineLearning/comments/a6wq76/d_confusions_regarding_retinanet/,zenggyu,1545022898,"I have been studying RetinaNet recently. I read the original paper and some related ones and wrote a post sharing what I have learnt: [http://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/](http://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/). However, I still have some confusions, which I also pointed out in the post. Can anyone please enlighten me?

&amp;#x200B;

Confusion #1

As indicated by the paper, an anchor box is assigned to background if its IoU with any ground-truth is below 0.4. In this case, what should be the corresponding classification target label (assuming there's K classes)?

I know that SSD has a background class (which makes K+1 classes in total), while YOLO predicts an confidence score indicating whether there is an object in the box (not background) or not (background) in addition to the K class probabilities. While I didn't find any statements in the paper indicating RetinaNet includes a background class, I did see this statement: ""..., we only decode box predictions from ..., after thresholding detector confidence at 0.05"", which seems to indicate that there's a prediction for confidence score. However, where does this score come from (since the classification subnet only outputs K numbers indicating the probability of K classes)?

If RetinaNet defines target labels differently from SSD or YOLO, I would assume that the target is a length-K vector with all 0s entries and no 1s. However, in this case how does the focal loss (see definition below) will punish an anchor if it is a false negative?

&amp;#x200B;

https://i.redd.it/dgztvy7air421.png

where

https://i.redd.it/kl4xiaiynr421.png

&amp;#x200B;

Confusion #2

Is equation (2)-(5) in my post correct?

&amp;#x200B;

Many thanks in advance and feel free to point out any other misunderstandings in the post!

&amp;#x200B;",6,1,False,https://b.thumbs.redditmedia.com/OGdtwEJtRcJuF7hSSyRqbJ1eF5SmRftQ_2qyML7CKSQ.jpg,,,,,
936,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,14,a6wr2c,cetpainfotech.com,Is It Time To Talk More About Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/a6wr2c/is_it_time_to_talk_more_about_machine_learning/,Divya123divya,1545023100,,0,1,False,default,,,,,
937,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,15,a6xbam,self.MachineLearning,Future Ethics of AI within Cyber Security!,https://www.reddit.com/r/MachineLearning/comments/a6xbam/future_ethics_of_ai_within_cyber_security/,baskaran08,1545028114,[removed],0,1,False,self,,,,,
938,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,15,a6xcn5,self.MachineLearning,Learn Machine Learning Technologies in Jaipur Ajmer Kota,https://www.reddit.com/r/MachineLearning/comments/a6xcn5/learn_machine_learning_technologies_in_jaipur/,digitalsamyak,1545028443,"&amp;#x200B;

*Processing img quhbuor29s421...*

Dont let the digital supply chain scare you. Big data, IoT, cloud, AI, drones and deep learning are just ways to improve the supply chain.

&amp;#x200B;

Join #Machine\_Learning\_Course today with Samyak IT Solutions Pvt Ltd and boost your career.

&amp;#x200B;

Call Now: +91-9772271081

Visit Page: [http://bit.ly/2Ez8ESH](http://bit.ly/2Ez8ESH)

&amp;#x200B;

\#machinelearning #artificialintelligence",0,1,False,self,,,,,
939,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,16,a6xpfl,youronlinecourses.net,Machine Learning with Java and Weka Course - 100% OFF,https://www.reddit.com/r/MachineLearning/comments/a6xpfl/machine_learning_with_java_and_weka_course_100_off/,Masawdah,1545031933,,0,1,False,default,,,,,
940,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,16,a6xt2a,self.MachineLearning,Innovative Industrial Sectors Leveraging Machine Learning Skills for 2019,https://www.reddit.com/r/MachineLearning/comments/a6xt2a/innovative_industrial_sectors_leveraging_machine/,SunilAhujaa,1545032987,[removed],0,1,False,self,,,,,
941,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,16,a6xttq,self.MachineLearning,Unified Game-Theoretic Approach to MARL implementation,https://www.reddit.com/r/MachineLearning/comments/a6xttq/unified_gametheoretic_approach_to_marl/,abhijeetg12,1545033212,[removed],0,1,False,self,,,,,
942,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,17,a6y69p,self.MachineLearning,"We created a free, fully automatic tool to remove the background of a person image: remove.bg",https://www.reddit.com/r/MachineLearning/comments/a6y69p/we_created_a_free_fully_automatic_tool_to_remove/,Tuxa13,1545037154,[removed],0,1,False,self,,,,,
943,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,18,a6yfgm,self.MachineLearning,Uncertainty of target in multi-label image classification,https://www.reddit.com/r/MachineLearning/comments/a6yfgm/uncertainty_of_target_in_multilabel_image/,TerrorBlizzard,1545039946,[removed],0,1,False,self,,,,,
944,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,18,a6yfnh,self.MachineLearning,Speaker recognition database,https://www.reddit.com/r/MachineLearning/comments/a6yfnh/speaker_recognition_database/,deveid,1545040003,I am trying to find a free data-set for a speaker recognition system.COuld be an api. Where can I get one?,0,1,False,self,,,,,
945,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,18,a6yht5,devblogs.nvidia.com,[P] How CatBoost Enables Fast Gradient Boosting on Decision Trees Using GPUs,https://www.reddit.com/r/MachineLearning/comments/a6yht5/p_how_catboost_enables_fast_gradient_boosting_on/,s0ulmate,1545040695,,0,1,False,default,,,,,
946,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,19,a6yp70,self.MachineLearning,Should I use batch norm when i use batch normed pretrained model??,https://www.reddit.com/r/MachineLearning/comments/a6yp70/should_i_use_batch_norm_when_i_use_batch_normed/,sjh9020,1545042878,[removed],0,1,False,self,,,,,
947,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,20,a6yx3h,self.MachineLearning,Need some directions,https://www.reddit.com/r/MachineLearning/comments/a6yx3h/need_some_directions/,dfnathan6,1545045028,[removed],0,1,False,self,,,,,
948,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,20,a6yxmg,self.MachineLearning,is there a function like softmax function but also gives negative values?,https://www.reddit.com/r/MachineLearning/comments/a6yxmg/is_there_a_function_like_softmax_function_but/,qudcjf7928,1545045184,[removed],0,1,False,self,,,,,
949,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,21,a6zdsj,hackernoon.com,How to Tackle Overfitting and other issues in Machine Learning with tests/ solutions,https://www.reddit.com/r/MachineLearning/comments/a6zdsj/how_to_tackle_overfitting_and_other_issues_in/,ravensdraven,1545049584,,0,1,False,https://a.thumbs.redditmedia.com/o8FO4HNP-riCKtPJIY8lY1ceTwIZFLsDr_Hr7qpAEV4.jpg,,,,,
950,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,21,a6zgn1,self.MachineLearning,Job Interview,https://www.reddit.com/r/MachineLearning/comments/a6zgn1/job_interview/,parzev,1545050334,[removed],0,1,False,self,,,,,
951,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,21,a6zgwa,arxiv.org,[1812.05687] Ablation of a Robot's Brain: Neural Networks Under a Knife,https://www.reddit.com/r/MachineLearning/comments/a6zgwa/181205687_ablation_of_a_robots_brain_neural/,ihaphleas,1545050398,,5,1,False,default,,,,,
952,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,21,a6zh31,arxiv.org,[1812.05720] Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem,https://www.reddit.com/r/MachineLearning/comments/a6zh31/181205720_why_relu_networks_yield_highconfidence/,ihaphleas,1545050452,,44,1,False,default,,,,,
953,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,21,a6zj7h,self.MachineLearning,[D] Looking for someone working in ML for an interview,https://www.reddit.com/r/MachineLearning/comments/a6zj7h/d_looking_for_someone_working_in_ml_for_an/,parzev,1545050965,"Hi, i'm a Computer Science student looking for someone who could answer some questions (textually) about working in the ML field, it's for an University assignment.

Please comment and I will private message you the questions, no need to write much, only a few words will be more then enough!

Thank you",2,1,False,self,,,,,
954,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,22,a6zyfp,self.MachineLearning,[D] Does the reparameterization gradient require the output as probability rather than the output probability?,https://www.reddit.com/r/MachineLearning/comments/a6zyfp/d_does_the_reparameterization_gradient_require/,abstractcontrol,1545054533,"I am trying to wrap my mind around probabilistic programs and before I can understand [variational inference](https://youtu.be/ogdv_6dbvVQ?t=3160) there is something I need to make sure. I've never been aware of this before, but now I am dimly aware that there is a difference between probability being an output of a deep net - such as when using the softmax or the sigmoid activation, and well, the stuff you could find in a [probabilistic programming book](http://probmods.org/).

I am wondering how it would be possible to take the gradient of a trace of a probabilistic program even if all the individual operations are differentiable?

Even though a probabilistic program is a distribution and might be continuous and supposedly differentiable, the sampled values are not differentiable and neither is the probability of the sampled value. Factor statements are almost like a cost function, but not quite.

Is there some trick I am missing, or is WebPPL just using the score function estimator in order to do variational inference under the hood?",5,1,False,self,,,,,
955,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,23,a703wn,arxiv.org,[1812.05866] Evolutionary Neural Architecture Search for Image Restoration,https://www.reddit.com/r/MachineLearning/comments/a703wn/181205866_evolutionary_neural_architecture_search/,gerardvanwyk,1545055716,,4,1,False,default,,,,,
956,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,23,a708w4,self.MachineLearning,[N] Announcing the Zero Ressource Speech Challenge 2019: TTS without T,https://www.reddit.com/r/MachineLearning/comments/a708w4/n_announcing_the_zero_ressource_speech_challenge/,edupoux,1545056801,"This new iteration of the **Zero Ressource Speech Challenge,** which is submitted to *Interspeech 2019* is now **open**.

The aim is to build a **speech synthesizer without any text or phonetic  labels** (hence Text to Speech without T). It takes inspiration from young  infants who learn to talk before they learn to read or write. Here, the  task is to discover a *pseudo-text* (a sub-word symbolic  representation internal to the machine), from raw speech, without any  labels, and to use these discovered units to resynthesize new utterances  in a target voice.

Deadline for submission of contribution is March 15, 2019. Details and registration at [**http://www.zerospeech.com/2019**](http://www.zerospeech.com/2019).  
",1,1,False,self,,,,,
957,MachineLearning,t5_2r3gv,2018-12-17,2018,12,17,23,a70cgy,theappsolutions.com,The Definitive Guide to Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/a70cgy/the_definitive_guide_to_pattern_recognition/,bil-sabab,1545057566,,0,1,False,default,,,,,
958,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,0,a70jse,github.com,PyGBM: Gradient Boosting Machines in Python with numba,https://www.reddit.com/r/MachineLearning/comments/a70jse/pygbm_gradient_boosting_machines_in_python_with/,Niourf,1545059081,,1,1,False,default,,,,,
959,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,0,a70swe,self.MachineLearning,[D] Roadmap of publishing a paper (scientific result),https://www.reddit.com/r/MachineLearning/comments/a70swe/d_roadmap_of_publishing_a_paper_scientific_result/,gabegabe6,1545060897,"Dear all,

Unfortunately I do not have any experience with publishing, citation, etc... The little I know is from the papers I read on *arxiv* and when I cited works in my thesis and other projects.

I have a few interesting experiments/ideas with results, which I would love to share with the community (yes, I looked around for similar works before asking these questions and starting writing papers).

My questions:
- Where can I publish a paper I wrote?
- Is it a problem that I wrote the paper alone? (As I know it does not really matter, but change my mind, if I am not right)
- Do I need to be affiliated with a Uni? (I am just finishing Masters - MS)
    - I think this is not a prerequisite, but then why I can't have a verified email on Google-Scholar? (It says I need to use my university email or work email)
- Where should I register to follow the ""life of the publications""? Google Scholar and other sites...
- Is there anything I should be worried about? Could you give me a few tips on the process as most of you have much more experience then I do?

Thank you very much for the help in advance!",23,1,False,self,,,,,
960,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,0,a70v0h,arxiv.org,[R] Learning to Reason with Third-Order Tensor Products,https://www.reddit.com/r/MachineLearning/comments/a70v0h/r_learning_to_reason_with_thirdorder_tensor/,Ash3nBlue,1545061310,,4,1,False,default,,,,,
961,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,0,a70xq0,reddit.com,[N] Predictive analytics for promotion and price optimization,https://www.reddit.com/r/MachineLearning/comments/a70xq0/n_predictive_analytics_for_promotion_and_price/,ikatsov,1545061825,,0,1,False,default,,,,,
962,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,1,a710yp,blog.griddynamics.com,[N] Predictive analytics for promotion and price optimization,https://www.reddit.com/r/MachineLearning/comments/a710yp/n_predictive_analytics_for_promotion_and_price/,ikatsov,1545062429,,0,1,False,https://a.thumbs.redditmedia.com/JJXnmxrFaXqPUe6oH5x9gWv6I5GI-791g4N-2I_NiF4.jpg,,,,,
963,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,1,a712b6,endtoend.ai,[News] RL Weekly 1: Soft Actor-Critic Code Release; Text-based RL Competition; Learning with Training Wheels,https://www.reddit.com/r/MachineLearning/comments/a712b6/news_rl_weekly_1_soft_actorcritic_code_release/,seungjaeryanlee,1545062664,,0,1,False,https://b.thumbs.redditmedia.com/QFMUaWX1WEK7kj1I5MzIOFL9Sbc6FIJ1Y_ffcnrXjDQ.jpg,,,,,
964,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,1,a71hfz,self.MachineLearning,"Can ""mining only"" GPU such as the P106-100 be used for machine learning?",https://www.reddit.com/r/MachineLearning/comments/a71hfz/can_mining_only_gpu_such_as_the_p106100_be_used/,MasterScrat,1545065431,[removed],0,1,False,self,,,,,
965,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,1,a71hw8,self.MachineLearning,"[D] Can ""mining only"" GPUs such as the P106-100 be used for machine learning?",https://www.reddit.com/r/MachineLearning/comments/a71hw8/d_can_mining_only_gpus_such_as_the_p106100_be/,MasterScrat,1545065515,"I'm seeing a lot of such cards second hand at good prices: https://www.gigabyte.com/Graphics-Card/NVIDIA-P106-100

Would they be equivalent to GTX 1060 6gb? (minus the video output of course)",29,1,False,self,,,,,
966,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,1,a71k93,self.MachineLearning,Transfer Learning - SpongeBob SquarePants Character Recogniser,https://www.reddit.com/r/MachineLearning/comments/a71k93/transfer_learning_spongebob_squarepants_character/,Laboratory_one,1545065940,[removed],0,1,False,self,,,,,
967,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,2,a71qmp,self.MachineLearning,[D] ADMM for Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/a71qmp/d_admm_for_neural_networks/,dundermifflined,1545067069,"The goal of *alternating direction method of multipliers (*ADMM) is to solve convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. 

* Is it possible to develop an analogous architecture where the goal is to break a large neural network into smaller ones, each of which can run on low-power or embedded devices?
* Can we employ a neural network to solve ADMM?

&amp;#x200B;

If yes, can you suggest me some relevant reading material. Thanks in anticipation.",12,1,False,self,,,,,
968,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,2,a71rs2,i.redd.it,How can this be done?,https://www.reddit.com/r/MachineLearning/comments/a71rs2/how_can_this_be_done/,Andy-SPD,1545067266,,0,1,False,https://a.thumbs.redditmedia.com/P4Qy1TOQWbT-thMBqHdpAlUb67RZKgoj-fPrOHKJmg8.jpg,,,,,
969,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,2,a720g0,youtu.be,"Amazon CEO Jeff Bezos: Tell someone a 100 years ago, when all were farmers, that we would have in 2018 massage therapist . Don't worry about AI - It's worth watching the entire video, even if it's longer than the usual video - but the link is to the right timing for the AI conversation (35:20)",https://www.reddit.com/r/MachineLearning/comments/a720g0/amazon_ceo_jeff_bezos_tell_someone_a_100_years/,gheghici,1545068794,,0,1,False,https://b.thumbs.redditmedia.com/2AgxJRsCSRfAtxxPwz-S_zTSzwEkwn5XVSPx95W3C7w.jpg,,,,,
970,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,3,a72c7u,blog.acolyer.org,"[R] ""Applied machine learning at Facebook: a datacenter infrastructure perspective"", Hazelwood et al 2018",https://www.reddit.com/r/MachineLearning/comments/a72c7u/r_applied_machine_learning_at_facebook_a/,gwern,1545070827,,0,1,False,default,,,,,
971,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,3,a72cf0,self.MachineLearning,[BEGINNER TUTORIAL] Build a lane detector,https://www.reddit.com/r/MachineLearning/comments/a72cf0/beginner_tutorial_build_a_lane_detector/,affinitive2,1545070866,"Step-by-step beginners tutorial on building a lane detector.

[https://medium.com/@chuanenlin/tutorial-build-a-lane-detector-679fd8953132](https://medium.com/@chuanenlin/tutorial-build-a-lane-detector-679fd8953132)

Feel free to check it out and kindly smash the clap button if you found it useful!",1,1,False,self,,,,,
972,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,3,a72m0u,self.MachineLearning,[P] The Hundred-Page Machine Learning Book manuscript is complete,https://www.reddit.com/r/MachineLearning/comments/a72m0u/p_the_hundredpage_machine_learning_book/,RudyWurlitzer,1545072541,"The drafts of the two final chapters of The Hundred-Page Machine Learning Book [are now online](http://themlbook.com). They consider metric learning, learning to rank, learning to recommend (including factorization machines and denoising autoencoders), and word embeddings.

The book is now complete and I'm so happy about that! I will make an announcement in a couple of weeks when the book will be available for purchase on Amazon. Subscribe to the mailing list to not miss anything.

Enjoy the reading and please let me know if you find any opportunity for improvement of the manuscript.",30,1,False,self,,,,,
973,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,4,a72siy,self.MachineLearning,[P] Codeq NLP API - A robust set of tools for intelligent state of the art text understanding,https://www.reddit.com/r/MachineLearning/comments/a72siy/p_codeq_nlp_api_a_robust_set_of_tools_for/,codeqcourier,1545073646,Happy Monday r/MachineLearning! Wanted to pop by and let ya'll know we just launched [https://api.codeq.com/](https://api.codeq.com/). We would love to have your feedback on our project!,0,1,False,self,,,,,
974,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,4,a72wvw,self.MachineLearning,How can I make machine-learning sex machine?,https://www.reddit.com/r/MachineLearning/comments/a72wvw/how_can_i_make_machinelearning_sex_machine/,cringe_master_5000,1545074411,"Hi, everyone. I am from the ""adult industry"".

I want to make a robotic hand that wanks when a wank is being wanked in the video. It has to be realistic. There are two machine learning parts to this project. The realistic stroking movements and analyzing each frame of the video to determine when a wank is being wanked. How do I do this?",0,1,False,self,,,,,
975,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,4,a72zfd,medium.com,[P] Machine Learning Experiment Management Cheat Sheet,https://www.reddit.com/r/MachineLearning/comments/a72zfd/p_machine_learning_experiment_management_cheat/,ceceshao1,1545074845,,0,1,False,default,,,,,
976,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,4,a7314p,ai.googleblog.com,Exploring Quantum Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a7314p/exploring_quantum_neural_networks/,sjoerdapp,1545075127,,0,1,False,default,,,,,
977,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,4,a731mi,magenta.tensorflow.org,[N] Google AI  Music Transformer: Generating Music with Long-Term Structure,https://www.reddit.com/r/MachineLearning/comments/a731mi/n_google_ai_music_transformer_generating_music/,ceceshao1,1545075213,,0,1,False,https://b.thumbs.redditmedia.com/rPls8IAlbaDr4QMQEG2NHvHReOw6fZZq7suX0WrEk_o.jpg,,,,,
978,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,5,a73hgf,self.MachineLearning,[D] Using seq2seq models for generating time series.,https://www.reddit.com/r/MachineLearning/comments/a73hgf/d_using_seq2seq_models_for_generating_time_series/,TheRedSphinx,1545077952,"I originally posted this to r/MLQuestions but it didn't receive much traction there. If this is an inappropriate place to post this, please let me know and I will delete it. 

&amp;#x200B;

I've seen a few papers (most recently [this one](https://arxiv.org/pdf/1809.04281.pdf)) that use a seq2seq model for generating time series data. They usually include a table with average (negative-)log-likelihood (NLL) values computed, with comparisons to other models. However, I feel I don't quite understand the exact framework of the problem. Let's suppose we look at a single sample, say x\_1, ... , x\_T.

1. Are we trying to train the network to solve the problem: ""Given x\_1, ... , x\_k, output a high probability of the next element being x\_{k+1}"".

2.   If so, should I then take this single sample and turn it into T samples of the form (past\_i, x\_i) where past\_i = \[x\_1, ... , x\_{i-1}\] during the pre-processing? Here I'm thinking of past\_i as the input variable, and x\_i as the target variable.

3.  Suppose 1) and 2) are correct, when people report the average NLL values, are they computing for each (past\_i, x\_i) example then averaging (amounting to computing just the NLL value for the whole sequence) or is there no averaging and just a division by the batch size (in this case, 1)?

4. Assuming 2) is correct, should I be taking the gradient steps at the end of the sample (i.e. when I evaluate the point (past\_T, x\_T), or multiple times as the model traverses the time series, e.g. compute gradients when the model tries to predict  and so on? Presumably if the sequences are very long, I guess some choosing a window size to compute gradient becomes a hyperparameter?

5.  How do we actually use this to generate sequences? Normally for things like VAE, we're allowed to just sample randomly from the latent space and just decode that sample. In this setting, I can't imagine that randomly sampling one time step would be that useful, but at the same time, wouldn't generating a few time steps be as difficult as the original problem? Do we just start with a few time steps that we know are ""sensible"" and then see what the network does from there?

&amp;#x200B;

6. Related to 5., suppose we choose some primer sequence of length T and we predict the (T+1)th entry. Do we now continue decoding or do we ""start again"" with a new sequence of length T+1 given by our old sequence with the new entry appended to it at the end, and feed it through the encoder and then through the decoder to produce the next element of our sequence?

Thanks!",0,1,False,self,,,,,
979,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,5,a73ny2,self.MachineLearning,Assortment planning - Arimax feeding into an optimization function,https://www.reddit.com/r/MachineLearning/comments/a73ny2/assortment_planning_arimax_feeding_into_an/,trojanpun,1545079093,[removed],0,1,False,self,,,,,
980,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,6,a742ml,self.MachineLearning,[P] Real-time Object Detection &amp; Classification (50ms) with Deep Learning on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/a742ml/p_realtime_object_detection_classification_50ms/,seemingly_omniscient,1545081622," Case Study: Real-time object detection and classification with DeepLearning on the Raspberry Pi 3 B+

without connecting to any external web/cloud services Our approach:



1. Object detection

HoughCircle Detection (OpenCV)



1. Object classification

A specially optimized deep learning network is used, which is also performant on computers with limited resources without GPU and can nevertheless achieve relatively high accuracies. Only a small training and test data set are available for the eight different euro coins: 1281 photos for training, 707 for testing. Since the training data set is minimal, so-called transfer learning is used. A pre-trained deep learning network is used, which was trained on the ImageNet training dataset (approx. 1.2 million images from 1000 categories). To make the classification more robust against rotations, brightness, contrast etc., the training images were additionally rotated randomly, and brightness and contrast were changed (data augmentation).



Achieved validation accuracy: 93.3%\*



Test accuracy achieved: 93.4%\*



Runtime for classification: 50ms

&amp;#x200B;

[Demo Video](https://youtu.be/XVvfcj_F_uc)",1,1,False,self,,,,,
981,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,7,a74l0v,self.MachineLearning,[D] How to start writing academic papers?,https://www.reddit.com/r/MachineLearning/comments/a74l0v/d_how_to_start_writing_academic_papers/,ArtisticHamster,1545084833,"I am  studying machine learning. I want to start career in academic machine learning, so I need to write papers. Unfortunately, I don't have a lot of people who can help me write. There're people who can help me writing academic papers, but not machine learning papers. So, I have several questions:

* Which conferences should I submit to, to have a real probability of acceptance?
* Are there any tips on how to write and what's expected from me?",9,1,False,self,,,,,
982,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,7,a74m36,self.MachineLearning,Path to become expert with AI and applying it?,https://www.reddit.com/r/MachineLearning/comments/a74m36/path_to_become_expert_with_ai_and_applying_it/,datarainfall,1545085026,[removed],0,1,False,self,,,,,
983,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,7,a74spl,self.MachineLearning,App Similar to FaceApp,https://www.reddit.com/r/MachineLearning/comments/a74spl/app_similar_to_faceapp/,throwaway201084,1545086214,[removed],0,1,False,self,,,,,
984,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,8,a755ah,developer.ibm.com,"IBM is giving away vouchers for a month of their ""Advanced Data Science with IBM"" specialization on Coursera",https://www.reddit.com/r/MachineLearning/comments/a755ah/ibm_is_giving_away_vouchers_for_a_month_of_their/,etylback,1545088553,,1,1,False,https://b.thumbs.redditmedia.com/qMq4qFg8E6Rdxl8PK34KQ9XlE8y7gV4ce1rMKenQKfU.jpg,,,,,
985,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,8,a758wi,self.MachineLearning,[P] A GAMEBOY supercomputer,https://www.reddit.com/r/MachineLearning/comments/a758wi/p_a_gameboy_supercomputer/,hardmaru,1545089260,"Kami Rocki built an FPGA-based emulator for gameboy games that he uses for reinforcement learning research.

At a total of slightly over 1 billion frames per second it is arguably the fastest 8-bit game console cluster in the world.

https://towardsdatascience.com/a-gameboy-supercomputer-33a6955a79a4",5,1,False,self,,,,,
986,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,8,a75h0a,self.MachineLearning,Extracting Excerpts from short text articles,https://www.reddit.com/r/MachineLearning/comments/a75h0a/extracting_excerpts_from_short_text_articles/,ashish_arma,1545090770,[removed],0,1,False,self,,,,,
987,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,9,a75kbu,self.MachineLearning,What is exactly logdet divergence ?,https://www.reddit.com/r/MachineLearning/comments/a75kbu/what_is_exactly_logdet_divergence/,AlgueRythme,1545091413,[removed],0,1,False,self,,,,,
988,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,10,a76fdl,self.MachineLearning,"[R] Unsupervised learning generates stunning images now, what about supervised learning without labels?",https://www.reddit.com/r/MachineLearning/comments/a76fdl/r_unsupervised_learning_generates_stunning_images/,rantana,1545097716,"Wasn't the original goal of all this unsupervised learning to learn on datasets without labels? 

Now that we have all these stunning generated images:

https://www.youtube.com/watch?v=kSLJriaOumA&amp;feature=youtu.be

https://twitter.com/NalKalchbrenner/status/1070669203680755712

Why isn't there any progress in doing classification without labels?",6,1,False,self,,,,,
989,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,12,a77abw,self.MachineLearning,[D] Neural Networks as Ordinary Differential Equations,https://www.reddit.com/r/MachineLearning/comments/a77abw/d_neural_networks_as_ordinary_differential/,baylearn,1545104128,"A blog post about the Neural ODE paper discussed [earlier](https://old.reddit.com/r/MachineLearning/comments/a65v5r/neural_ordinary_differential_equations_pdf/):

https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/",40,1,False,self,,,,,
990,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,12,a77dcm,self.MachineLearning,"Learning from multiple, related time series...",https://www.reddit.com/r/MachineLearning/comments/a77dcm/learning_from_multiple_related_time_series/,longestPath,1545104760,[removed],0,1,False,self,,,,,
991,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,13,a77hja,self.datascience,Machine Learning Knowledge Graph,https://www.reddit.com/r/MachineLearning/comments/a77hja/machine_learning_knowledge_graph/,leandromineti,1545105652,,0,1,False,https://b.thumbs.redditmedia.com/BpVBBVdABGpS6Om02DbuHcheBPWZImPeCyl9AG8_2dY.jpg,,,,,
992,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,13,a77kbi,self.datascience,[P] Machine Learning Knowledge Graph,https://www.reddit.com/r/MachineLearning/comments/a77kbi/p_machine_learning_knowledge_graph/,leandromineti,1545106253,,0,1,False,https://b.thumbs.redditmedia.com/BpVBBVdABGpS6Om02DbuHcheBPWZImPeCyl9AG8_2dY.jpg,,,,,
993,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,13,a77qnl,self.MachineLearning,"Two papers on ""Residual Reinforcement/Policy Learning""",https://www.reddit.com/r/MachineLearning/comments/a77qnl/two_papers_on_residual_reinforcementpolicy/,galaxstar,1545107618,[removed],0,1,False,self,,,,,
994,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,13,a77ryc,self.MachineLearning,Two papers on Residual Reinforcement/Policy Learning,https://www.reddit.com/r/MachineLearning/comments/a77ryc/two_papers_on_residual_reinforcementpolicy/,tomssilver,1545107900,[removed],0,1,False,self,,,,,
995,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,13,a77tcy,self.MachineLearning,[R] Two papers on Residual Reinforcement/Policy Learning,https://www.reddit.com/r/MachineLearning/comments/a77tcy/r_two_papers_on_residual_reinforcementpolicy/,galaxstar,1545108219,"From MIT: [https://k-r-allen.github.io/residual-policy-learning/](https://k-r-allen.github.io/residual-policy-learning/)

From Berkeley/Siemens/Hamburg: [https://residualrl.github.io/](https://residualrl.github.io/)",4,1,False,self,,,,,
996,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,13,a77xlf,remove.bg,This Background Removal Tool on r/webdev,https://www.reddit.com/r/MachineLearning/comments/a77xlf/this_background_removal_tool_on_rwebdev/,DRdefective,1545109194,,0,1,False,https://b.thumbs.redditmedia.com/wDZ4HIZ3QqNifhlXbHUu85XR8KZz3hKFz6bG7F1MEpo.jpg,,,,,
997,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,14,a787ze,i.redd.it,Types of Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/a787ze/types_of_machine_learning_algorithms/,corpnce,1545111655,,0,1,False,default,,,,,
998,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,14,a78buo,self.MachineLearning,Machine Learning Bangalore | Machine Learning Course Bangalore,https://www.reddit.com/r/MachineLearning/comments/a78buo/machine_learning_bangalore_machine_learning/,vaidehisoftware,1545112600,[removed],0,1,False,self,,,,,
999,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,14,a78c78,self.MachineLearning,[R] Master Thesis on Bayesian CNN using Variational Inference and Application using PyTorch,https://www.reddit.com/r/MachineLearning/comments/a78c78/r_master_thesis_on_bayesian_cnn_using_variational/,KumarShridhar,1545112688,"Read the complete thesis here:  [https://github.com/kumar-shridhar/Master-Thesis-BayesianCNN/](https://github.com/kumar-shridhar/Master-Thesis-BayesianCNN/blob/master/README.md)

&amp;#x200B;

Code available here: [https://github.com/kumar-shridhar/PyTorch-BayesianCNN](https://github.com/kumar-shridhar/PyTorch-BayesianCNN)

&amp;#x200B;

The thesis involves proposal of a Bayesian convolutional neural network with Variational Inference and then shows the way to do the uncertainty estimate with every prediction. Model pruning is done to reduce the number of parameters and then it is finally applied to computer vision tasks of Image Recognition, Image Super-Resolution and GANs. 

&amp;#x200B;

Let me know what you guys think about it. Recently similar things have been implemented in Tensorflow : [https://github.com/tensorflow/probability/blob/master/tensorflow\_probability/python/layers/conv\_variational.py#L414](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/layers/conv_variational.py#L414)

&amp;#x200B;

&amp;#x200B;",7,1,False,self,,,,,
1000,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,15,a78l8i,mvasiaonline.com,A LOOK AT THE PROGRESSION OF MACHINE VISION TECHNOLOGY OVER THE LAST THREE YEARS,https://www.reddit.com/r/MachineLearning/comments/a78l8i/a_look_at_the_progression_of_machine_vision/,MvasiaInfomatrix,1545114924,,0,1,False,default,,,,,
1001,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,15,a78loa,self.MachineLearning,[D] Cross Entropy loss not decreasing and gradient norm increasing. What can be the cause?,https://www.reddit.com/r/MachineLearning/comments/a78loa/d_cross_entropy_loss_not_decreasing_and_gradient/,dchatterjee172,1545115036,"I am training a model with transformer encoders as building blocks. After a certain point, the model loss does not decrease that much but the global norm of the gradients increases. I am using a very low learning rate, with linear decay. And I am clipping gradients also. Any suggestions?  
I think this may be happening because of ill-conditioned hessian but not sure.  
I have attached the graphs, Orange is simple SGD and Blue is ADAM.  
And I used truncated random normal to initialize the weights.

&amp;#x200B;

https://i.redd.it/vabzp0jqez421.png

&amp;#x200B;",6,1,False,https://a.thumbs.redditmedia.com/1FKEN4JO6MmcpX1RawnNyDAo_06-J6RPOQkFhKwtBw0.jpg,,,,,
1002,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,15,a78olj,blogs.unity3d.com,"ML-Agents v0.6, Scriptable Object Brains, Demonstration Recorder",https://www.reddit.com/r/MachineLearning/comments/a78olj/mlagents_v06_scriptable_object_brains/,leonchenzhy,1545115784,,0,1,False,https://a.thumbs.redditmedia.com/yjan9kMuPxM8dBYhhUSW9VsbYZJs2G53JAnZmEiKkh8.jpg,,,,,
1003,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,18,a79hnl,euler16.github.io,Image Captioning in Browser using Tensorflow.js,https://www.reddit.com/r/MachineLearning/comments/a79hnl/image_captioning_in_browser_using_tensorflowjs/,euler_16,1545124265,,0,1,False,default,,,,,
1004,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,19,a79uq1,arxiv.org,[1812.06369] Provable limitations of deep learning,https://www.reddit.com/r/MachineLearning/comments/a79uq1/181206369_provable_limitations_of_deep_learning/,ihaphleas,1545128091,,3,1,False,default,,,,,
1005,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,19,a79xbk,self.MachineLearning,LISK sidechain Project's Founder Reassures Us on AI Danger and Market Flash Crashes!,https://www.reddit.com/r/MachineLearning/comments/a79xbk/lisk_sidechain_projects_founder_reassures_us_on/,John_Muck,1545128847,[removed],0,1,False,self,,,,,
1006,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,19,a79yux,self.MachineLearning,Andrews ML course,https://www.reddit.com/r/MachineLearning/comments/a79yux/andrews_ml_course/,snip3r77,1545129292,[removed],0,1,False,self,,,,,
1007,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,20,a7a9bu,cc.gatech.edu,Explainable Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/a7a9bu/explainable_artificial_intelligence/,steccami,1545132511,,4,1,False,default,,,,,
1008,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,20,a7a9n6,self.MachineLearning,Visualize Gradient Descent steps in Linear Regression,https://www.reddit.com/r/MachineLearning/comments/a7a9n6/visualize_gradient_descent_steps_in_linear/,fedetask,1545132604,[removed],0,1,False,self,,,,,
1009,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,20,a7a9xj,paperswithcode.com,"Research Papers with Code for Artificial Intelligence, Deep learning, Machine Learning, and Neural Networks",https://www.reddit.com/r/MachineLearning/comments/a7a9xj/research_papers_with_code_for_artificial/,rohan36,1545132676,,0,1,False,default,,,,,
1010,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,20,a7adyg,self.MachineLearning,What is the story with these cheap cards? (GeFrce-GTX-1080-Ti),https://www.reddit.com/r/MachineLearning/comments/a7adyg/what_is_the_story_with_these_cheap_cards/,bsdmike,1545133810,[removed],0,1,False,self,,,,,
1011,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,20,a7afzi,github.com,LiDAR data processing (pull requests are welcome),https://www.reddit.com/r/MachineLearning/comments/a7afzi/lidar_data_processing_pull_requests_are_welcome/,dronecub,1545134390,,0,1,False,https://b.thumbs.redditmedia.com/R87BWXxlMET0xB0NQhjK0tsYR1xdmM5IeKSQ2042dOw.jpg,,,,,
1012,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,21,a7ak5x,leandromineti.github.io,[P] Machine Learning Knowledge Graph,https://www.reddit.com/r/MachineLearning/comments/a7ak5x/p_machine_learning_knowledge_graph/,leandromineti,1545135431,,0,1,False,default,,,,,
1013,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,22,a7avaw,self.MachineLearning,Kambria Fueling The Robotics Economy,https://www.reddit.com/r/MachineLearning/comments/a7avaw/kambria_fueling_the_robotics_economy/,HappyCompetition,1545138194,[removed],0,1,False,https://b.thumbs.redditmedia.com/LXYrnRd_2H0ch4KUDPVoi-KiLULCoRXL4FXx7RrnQ8o.jpg,,,,,
1014,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,22,a7axik,link.medium.com,Learn the building blocks of CNN in just 3 minutes.,https://www.reddit.com/r/MachineLearning/comments/a7axik/learn_the_building_blocks_of_cnn_in_just_3_minutes/,aniketmaurya,1545138701,,0,1,False,default,,,,,
1015,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,22,a7b29b,self.MachineLearning,Multi label classification,https://www.reddit.com/r/MachineLearning/comments/a7b29b/multi_label_classification/,sanchit2843,1545139780,[removed],0,1,False,self,,,,,
1016,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,22,a7b2q7,blog.newey.me,An introduction to gradient boosting,https://www.reddit.com/r/MachineLearning/comments/a7b2q7/an_introduction_to_gradient_boosting/,rijncur,1545139887,,0,1,False,default,,,,,
1017,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,22,a7b335,hackernoon.com,Methods to Tackle Common Problems with Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/a7b335/methods_to_tackle_common_problems_with_machine/,ravensdraven,1545139969,,0,1,False,default,,,,,
1018,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,23,a7bel4,self.MachineLearning,[P] PyCM 1.7 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/a7bel4/p_pycm_17_released_machine_learning_library_for/,sepandhaghighi,1545142473,"[https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

Changelog :

* Gini Index (GI) added
* Example-7 added
* `pycm_profile.py` added
* `class_name` argument added to `stat`,`save_stat`,`save_csv` and `save_html`  methods
* `overall_param` and `class_param` arguments empty list bug fixed
* `matrix_params_calc`, `matrix_params_from_table` and `vector_filter` functions optimized
* `overall_MCC_calc`, `CEN_misclassification_calc` and `convex_combination` functions optimized
* Document modified",0,1,False,self,,,,,
1019,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,23,a7bgfs,doi.org,Explainable machine-learning predictions for the prevention of hypoxaemia during surgery,https://www.reddit.com/r/MachineLearning/comments/a7bgfs/explainable_machinelearning_predictions_for_the/,everydAI,1545142880,,1,1,False,default,,,,,
1020,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,23,a7bko0,self.MachineLearning,[Project] Malaria Classification on Blood Cells images,https://www.reddit.com/r/MachineLearning/comments/a7bko0/project_malaria_classification_on_blood_cells/,usernameisafarce,1545143772,"This is a weekend \[project\]([https://www.kaggle.com/freakeisch/anopheles-project-malaria-classification](https://www.kaggle.com/freakeisch/anopheles-project-malaria-classification)) I did for learning Deep Learning and Convolutional neural network.

The classifier can get a blood cell image and classify for malaria infection. On validation reached \~\_\_95.5%\_\_ Accuracy.

It is a binary classification which makes it perfect for beginners to study and perfect for the internet community to try and solve by making the classifier even more accurate. Everybody come help us Classify Malaria for the better of everything!

&amp;#x200B;",5,1,False,self,,,,,
1021,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,23,a7bl80,rkevingibson.github.io,Neural networks as Ordinary Differential Equations,https://www.reddit.com/r/MachineLearning/comments/a7bl80/neural_networks_as_ordinary_differential_equations/,j_orshman,1545143891,,0,1,False,https://b.thumbs.redditmedia.com/HKg2CSfp90NZ3TO1FOrR0ih5yGlbQ-M5yaaXYHratOE.jpg,,,,,
1022,MachineLearning,t5_2r3gv,2018-12-18,2018,12,18,23,a7bqb7,self.MachineLearning,[D] Pytorch parallelism,https://www.reddit.com/r/MachineLearning/comments/a7bqb7/d_pytorch_parallelism/,ewanlee,1545144923,"Hello everyone, I recently encountered a problem with pytorch parallelism. Since I have less contact with parallel programming, the problem may be very simple.

I have been doing some multi-agent reinforcement learning experiments recently. Now there are n independent agents. Each agent has its own independent parameters and model, but they can access a shared replay buffer. I want each model to be trained on the GPU. What should I do if I want to train in parallel?

Thank you all :P",3,1,False,self,,,,,
1023,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,0,a7bx3g,youtu.be,Nvidia learned to make realistic faces,https://www.reddit.com/r/MachineLearning/comments/a7bx3g/nvidia_learned_to_make_realistic_faces/,cmillionaire9,1545146221,,0,1,False,default,,,,,
1024,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,0,a7c3wx,deeplearning.mit.edu,[N] Announcing MIT Deep Learning courses,https://www.reddit.com/r/MachineLearning/comments/a7c3wx/n_announcing_mit_deep_learning_courses/,UltraMarathonMan,1545147508,,29,1,False,default,,,,,
1025,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,0,a7c71q,medium.com,MLPerf Announces First Results; Microsoft &amp; Facebook Get On Board,https://www.reddit.com/r/MachineLearning/comments/a7c71q/mlperf_announces_first_results_microsoft_facebook/,Yuqing7,1545148101,,0,1,False,default,,,,,
1026,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,0,a7c83b,self.MachineLearning,"Setting up our Rasa/NLU on docker container, error/help?",https://www.reddit.com/r/MachineLearning/comments/a7c83b/setting_up_our_rasanlu_on_docker_container/,SettySatt,1545148293,[removed],0,1,False,self,,,,,
1027,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,0,a7cabc,medium.com,2018 in Review: 10 AI Quotes,https://www.reddit.com/r/MachineLearning/comments/a7cabc/2018_in_review_10_ai_quotes/,gwen0927,1545148705,,0,1,False,https://b.thumbs.redditmedia.com/FdlEa80_shma6FBfRWNBXemXNIBIt8pppFpWQHEEfhQ.jpg,,,,,
1028,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,0,a7cacs,arxiv.org,"[R] Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",https://www.reddit.com/r/MachineLearning/comments/a7cacs/r_stochastic_gradient_descent_performs/,abstractcontrol,1545148713,,7,1,False,default,,,,,
1029,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,1,a7crjj,self.MachineLearning,[D] Do any of you use semi-supervised for a real problem (not artificially constructed)?,https://www.reddit.com/r/MachineLearning/comments/a7crjj/d_do_any_of_you_use_semisupervised_for_a_real/,alexmlamb,1545151736,"Out of curiosity, do any of you use semi-supervised learning in a real setting: i.e. where you actually don't have enough labeled data?  I'm referring to cases where this restriction of labeled data is not artificially introduced to make SSL useful.  

&amp;#x200B;

I'd be curious to learn more about how this effects the choice of methods.  For example, my understanding is that consistency based methods are not very good when the unlabeled and labeled data follow slightly different distributions.  ",9,1,False,self,,,,,
1030,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,1,a7cs1j,satishmpandey.tech,Matplotlib Basics for beginners,https://www.reddit.com/r/MachineLearning/comments/a7cs1j/matplotlib_basics_for_beginners/,satishmpandey,1545151829,,0,1,False,default,,,,,
1031,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,1,a7cs57,assuredpharmacy.co.uk,AI Created Christmas Song,https://www.reddit.com/r/MachineLearning/comments/a7cs57/ai_created_christmas_song/,D3G3N3RAT3,1545151844,,0,1,False,default,,,,,
1032,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,2,a7cwvh,self.MachineLearning,curve fitting vs regression,https://www.reddit.com/r/MachineLearning/comments/a7cwvh/curve_fitting_vs_regression/,anissdjellal,1545152688,[removed],0,1,False,self,,,,,
1033,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,2,a7czot,determined.ai,Whats the deal with Neural Architecture Search?,https://www.reddit.com/r/MachineLearning/comments/a7czot/whats_the_deal_with_neural_architecture_search/,yoavz,1545153163,,0,1,False,default,,,,,
1034,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,2,a7d0vm,youtube.com,AI in China - a must watch youtube video,https://www.reddit.com/r/MachineLearning/comments/a7d0vm/ai_in_china_a_must_watch_youtube_video/,rickspick,1545153365,,0,1,False,https://b.thumbs.redditmedia.com/G2zbdbwYB668Eku9t30dj4IXV4k-g_wkyAdWp61bMsk.jpg,,,,,
1035,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,2,a7d4mg,lanl.gov,Machine learning-detected signal predicts time to earthquake,https://www.reddit.com/r/MachineLearning/comments/a7d4mg/machine_learningdetected_signal_predicts_time_to/,greenprius,1545154025,,0,1,False,default,,,,,
1036,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,2,a7dbzd,self.MachineLearning,[D] Subreddit for Machine Learning Projects,https://www.reddit.com/r/MachineLearning/comments/a7dbzd/d_subreddit_for_machine_learning_projects/,manicman1999,1545155278,"I was wondering if you guys thought it would be a good idea to create a new subreddit with the sole goal of finding and sharing projects involving machine learning. It could be a great place to gain inspiration for creative projects, or a place to show off and get feed back on projects you've made yourself. ",15,1,False,self,,,,,
1037,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,2,a7dd91,self.MachineLearning,Does it make sense to use Lasso for feature selection then using those features to train a NN?,https://www.reddit.com/r/MachineLearning/comments/a7dd91/does_it_make_sense_to_use_lasso_for_feature/,sadboijoy,1545155514,[removed],0,1,False,self,,,,,
1038,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,3,a7dhr4,self.MachineLearning,W2Vec Comparsions,https://www.reddit.com/r/MachineLearning/comments/a7dhr4/w2vec_comparsions/,nagabv,1545156261,[removed],0,1,False,self,,,,,
1039,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,3,a7dtbr,medium.com,Alibaba AI Detects Pig Pregnancies,https://www.reddit.com/r/MachineLearning/comments/a7dtbr/alibaba_ai_detects_pig_pregnancies/,Yuqing7,1545158252,,0,1,False,default,,,,,
1040,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,3,a7dw0n,blog.griddynamics.com,"Algorithmic Pricing, Part I: The Risks and Opportunities",https://www.reddit.com/r/MachineLearning/comments/a7dw0n/algorithmic_pricing_part_i_the_risks_and/,ikatsov,1545158741,,0,1,False,default,,,,,
1041,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,3,a7dxb9,youtu.be,I wrote a Python pet detector camera program that watches the door and sends me a text if my cat or dog wants to be let inside! It runs on a Raspberry Pi and uses the TensorFlow MobileNet-SSD model for object detection.,https://www.reddit.com/r/MachineLearning/comments/a7dxb9/i_wrote_a_python_pet_detector_camera_program_that/,Taxi-guy,1545158968,,0,1,False,https://b.thumbs.redditmedia.com/1AqlXLbJZSfQx4HG3Ki9XQybHbs3rut_ygX-frPz_iw.jpg,,,,,
1042,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,3,a7e097,self.MachineLearning,Collecting images for a dataset,https://www.reddit.com/r/MachineLearning/comments/a7e097/collecting_images_for_a_dataset/,CSGOvelocity,1545159501,[removed],0,1,False,self,,,,,
1043,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,4,a7e4on,blog.griddynamics.com,"[N] Algorithmic Pricing, Part I: The Risks and Opportunities",https://www.reddit.com/r/MachineLearning/comments/a7e4on/n_algorithmic_pricing_part_i_the_risks_and/,ikatsov,1545160232,,0,1,False,https://b.thumbs.redditmedia.com/zewsA4ukVsH7X6pAUg9euf3hwrgf_85rxeO4a5wAQPg.jpg,,,,,
1044,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,4,a7e8sj,imgur.com,vast.ai RTX 2080 deployment.,https://www.reddit.com/r/MachineLearning/comments/a7e8sj/vastai_rtx_2080_deployment/,coopsindahouse,1545160937,,4,1,False,default,,,,,
1045,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,4,a7eamz,axel.org,[D] Humans vs Robots: The Difference Between AI and AGI,https://www.reddit.com/r/MachineLearning/comments/a7eamz/d_humans_vs_robots_the_difference_between_ai_and/,Amigoly,1545161252,,0,1,False,https://b.thumbs.redditmedia.com/3SljwrbZU2MU87oKRLpoThA5h09_P6M38qpd-vbFX3I.jpg,,,,,
1046,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,4,a7ecct,ai.googleblog.com,Google AI Princeton: Current and Future Research,https://www.reddit.com/r/MachineLearning/comments/a7ecct/google_ai_princeton_current_and_future_research/,sjoerdapp,1545161540,,0,1,False,default,,,,,
1047,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,4,a7egt8,self.MachineLearning,[D] What's a good way of getting started with Variational Inference?,https://www.reddit.com/r/MachineLearning/comments/a7egt8/d_whats_a_good_way_of_getting_started_with/,progfu,1545162312,"
*Note: I'm reposting this from /r/LML since it got zero response there. I guess VI is not really something discussed over there. I know this sub is mainly for discussion and research topics, but VI feels like something a lot of people say they ""do"", but not many are showing ""how"".*

---

I've been reading about VI for quite a while, and I feel that I understand it from the theoretical point of view. That is deriving ELBO and showing that maximizing ELBO minimizes the KL between our two approximate q and the target p densities, using mean field with exponential family q_i's to make the computations easier, etc.

The problem is, I feel kind of stuck actually applying this. For DL it is quite clear by now what is the ""hello world"" and how a person should generally progress when learning, that is doing a simple DNN on MNIST, then moving onto CNNs, RNNs, while trying lots of different tricks like L2 regularization, dropout, layer/batch normalization, etc. I'd also say that people probably agree that doing a DNN in pure Python at least once is beneficial, and then give advice on say using Autograd/Pytorch to get a better feel for the gradients, and stuff like that.

Is there such thing for VI? What are some of the things one should do while getting started? What are some small projects to do or github projects to look at?

Pyro and TF Probability both seem quite hard to begin with, and PyMC3 on the other hand feels quite detached from the DL world.

I've tried following the Pyro guide, but either I'm missing some more background knowledge, or it's just not meant as a guide for people transitioning from other existing VI toolkits, but a lot of the APIs and naming of things and the number of things one has to do seem pretty arbitrary and I don't see that many parallels to what I'm reading in theory.

My initial thought process of how VI works is:

- create a probabilistic model (or really just define the generative process for the join)
- condition on some stuff, out comes somehow a ""posterior"" that I want
- define the parametric family that'll be optimized over
- ""train"" and get an approximation for the posterior

But looking at Pyro, there seems to be many more things available and many different APIs.

I don't even know if Pyro is the right thing to start with. I don't really have an end goal, I'm mainly looking for understanding and the ability to play with all the things I'm reading about in Bishop/Murphy/papers.

I am however interested in seeing how Bayesian methods can be applied in deep learning, so I'd like to get to that eventually, but I don't mind doing the hard work of implementing some stuff myself in pure Python. The problem is I just don't know ""what"" to begin with.

Thanks for any tips &amp; resources!",44,1,False,self,,,,,
1048,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,4,a7ehde,self.MachineLearning,[D] Need help with identifying a research topic for my Master's Thesis,https://www.reddit.com/r/MachineLearning/comments/a7ehde/d_need_help_with_identifying_a_research_topic_for/,sonicmachine,1545162399,"I am starting work on my Master's thesis in few weeks. However, I don't know what I need to work on. The feeling is akin to seeking to buy a car but having no idea on which car to buy. I really need some quality advice in this regard. My thesis adviser has no background in AI/ML research. If you're wondering why work with such an adviser? My University does not have a good catalogue of advisers to pick from. I've picked best from the lot based on ability to support a student. 

For finding a research topic I have started reading research papers from the AI/ML community. There has been no progress. Can someone please help me in understanding what needs to be done in identifying a research topic? The following questions boggle my mind. Is there a good list of research topics to pick from that I fail to find? Is my adviser supposed to give me a research topic to work on? How does it work?

For some researchers out there, topics are devised out of personal interest. I'm sorry, I do not have much experience in the field to know my niche. Having given it a serious thought, I like the work done in computer vision. Regardless, I continue to struggle in finding a research topic which can fulfill the requirements of a Master's thesis.

I wish to bring about a sincere discussion regarding the process of realizing search topics in order for me to get detailed information about it. Please guide me Reddit.
",7,1,False,self,,,,,
1049,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,5,a7epj7,self.MachineLearning,[D] Survey : Understanding the relocation preferences and challenges faced by AI talents,https://www.reddit.com/r/MachineLearning/comments/a7epj7/d_survey_understanding_the_relocation_preferences/,Consilience_OY,1545163835,"Do you currently work in or are you a student in data science, machine learning, AI or a closely related field?

If yes, were trying to understand the reasons behind AI talents choice of work, (re)location and the challenges they face. If you feel that companies or cities do not seem to understand what attracts you to (or repels you from) a job/location, this is your chance to offer some solid statistics on your preferences.

We would really appreciate it if you filled out our survey (\~10-15 minutes): 

[https://docs.google.com/forms/d/1qsl0Y0z\_tO4PSuoIJr7NXwAoEAvP\_kluEhcxFzhzZP0/viewform?edit\_requested=true](https://docs.google.com/forms/d/1qsl0Y0z_tO4PSuoIJr7NXwAoEAvP_kluEhcxFzhzZP0/viewform?edit_requested=true) 

Thanks a lot!",5,1,False,self,,,,,
1050,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,5,a7esyk,self.MachineLearning,[D] What's the fastest and most efficient method for summarizing text?,https://www.reddit.com/r/MachineLearning/comments/a7esyk/d_whats_the_fastest_and_most_efficient_method_for/,carsonpoole,1545164438,"There are lots of github repos for summarizing text, but many involve huge model files and take an eternity to run. Is there a quicker way to do this and or is there a repo you know of that solves the huge file and lots of RAM problem?

Thanks",7,1,False,self,,,,,
1051,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,5,a7eugg,self.MachineLearning,Good books for machine learning / neural networks,https://www.reddit.com/r/MachineLearning/comments/a7eugg/good_books_for_machine_learning_neural_networks/,aleexxp,1545164693,[removed],0,1,False,self,,,,,
1052,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,5,a7eyt1,self.MachineLearning,[D] Autoencoder with reflection/rotation symmetry?,https://www.reddit.com/r/MachineLearning/comments/a7eyt1/d_autoencoder_with_reflectionrotation_symmetry/,deepcleansingguffaw,1545165473,"I'm planning to use an autoencoder (probably k-sparse) to learn a representation of board game states. The game (Thud) has reflection and (discrete) rotation symmetry though, and I'd like the representation to express that (ie states that differ only by symmetry transformation should have nearby encodings).

I'm thinking that I will be able to do this by, for each item in the training set, simultaneously training a random symmetry of the item, then specifying a loss function on the encoding that moves the learned representations of the two toward each other. In support of this, the loss function for the decoded output will try all 8 symmetries and use the one that most closely matches.

My questions are: Will this do what I'm expecting? And is there a simpler/better way to accomplish it?",11,1,False,self,,,,,
1053,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,5,a7f3o0,paperswithcode.com,[R] Papers with Code : the latest in machine learning,https://www.reddit.com/r/MachineLearning/comments/a7f3o0/r_papers_with_code_the_latest_in_machine_learning/,seemingly_omniscient,1545166326,,0,1,False,https://b.thumbs.redditmedia.com/rANQf9nGIUUTf2R5hJI5KaKPQWgAedLzC5dRbLhDwls.jpg,,,,,
1054,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,6,a7fl3d,tvm.ai,[P] Automating Generation of Low Precision Deep Learning Operators,https://www.reddit.com/r/MachineLearning/comments/a7fl3d/p_automating_generation_of_low_precision_deep/,crowwork,1545169271,,0,1,False,default,,,,,
1055,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,6,a7fn0k,self.MachineLearning,Modify Facial Features,https://www.reddit.com/r/MachineLearning/comments/a7fn0k/modify_facial_features/,throwaway201084,1545169610,[removed],0,1,False,self,,,,,
1056,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,6,a7fot7,self.MachineLearning,Compression in Deep Neural Networks is Comprehensive!,https://www.reddit.com/r/MachineLearning/comments/a7fot7/compression_in_deep_neural_networks_is/,mrtnoshad,1545169937,[removed],0,1,False,self,,,,,
1057,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,6,a7fre7,opendatascience.com,10 Tips to Get Started with Kaggle,https://www.reddit.com/r/MachineLearning/comments/a7fre7/10_tips_to_get_started_with_kaggle/,OpenDataSciCon,1545170397,,0,1,False,default,,,,,
1058,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,7,a7fudn,self.MachineLearning,Pytorchic BERT - clean implementation of google BERT model,https://www.reddit.com/r/MachineLearning/comments/a7fudn/pytorchic_bert_clean_implementation_of_google/,benjamin_dhlee,1545170910,[removed],0,1,False,self,,,,,
1059,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,7,a7fxvu,self.MachineLearning,Any tutorials on pipeline for evaluating machine learning models?,https://www.reddit.com/r/MachineLearning/comments/a7fxvu/any_tutorials_on_pipeline_for_evaluating_machine/,po-handz,1545171508,[removed],0,1,False,self,,,,,
1060,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,7,a7g7v2,arxiv.org,[R] Latent Dirichlet Allocation in Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/a7g7v2/r_latent_dirichlet_allocation_in_generative/,i-like-big-gans,1545173258,,3,1,False,default,,,,,
1061,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,8,a7gczg,self.MachineLearning,Making predictions?,https://www.reddit.com/r/MachineLearning/comments/a7gczg/making_predictions/,rocco20,1545174208,[removed],0,1,False,self,,,,,
1062,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,8,a7gnn0,luxedo.github.io,Two Neurons Worm,https://www.reddit.com/r/MachineLearning/comments/a7gnn0/two_neurons_worm/,ArmlessJohn404,1545176241,,0,1,False,default,,,,,
1063,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,9,a7h829,self.MachineLearning,Tutorial for Anomaly Detection - Azure ML?,https://www.reddit.com/r/MachineLearning/comments/a7h829/tutorial_for_anomaly_detection_azure_ml/,orangeatom,1545180137,[removed],0,1,False,self,,,,,
1064,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,9,a7hao4,arxiv.org,[R] Temporal Analysis of Entity Relatedness and its Evolution using Wikipedia and DBpedia,https://www.reddit.com/r/MachineLearning/comments/a7hao4/r_temporal_analysis_of_entity_relatedness_and_its/,ranihorev,1545180639,,1,1,False,default,,,,,
1065,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,10,a7hjsm,self.MachineLearning,Cause of neural net NaN error tied to learning rate,https://www.reddit.com/r/MachineLearning/comments/a7hjsm/cause_of_neural_net_nan_error_tied_to_learning/,Netcat2,1545182409,[removed],0,1,False,self,,,,,
1066,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,11,a7hzt8,self.MachineLearning,Udacity's Intro to ML vs Courseera's 'ML by Andrew NG' ?,https://www.reddit.com/r/MachineLearning/comments/a7hzt8/udacitys_intro_to_ml_vs_courseeras_ml_by_andrew_ng/,endeavour23,1545185672,[removed],0,1,False,self,,,,,
1067,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,11,a7i0ci,self.MachineLearning,"If convolution is commutative, why do we stack 3x3 convolutions with relu in the middle?",https://www.reddit.com/r/MachineLearning/comments/a7i0ci/if_convolution_is_commutative_why_do_we_stack_3x3/,idg101,1545185777,[removed],0,1,False,self,,,,,
1068,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,11,a7i8qb,self.MachineLearning,"Should I wait until I get exertise on ML, or go for a job?",https://www.reddit.com/r/MachineLearning/comments/a7i8qb/should_i_wait_until_i_get_exertise_on_ml_or_go/,endeavour23,1545187470,[removed],0,1,False,self,,,,,
1069,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,11,a7ibxg,self.MachineLearning,Is this error surface correct?,https://www.reddit.com/r/MachineLearning/comments/a7ibxg/is_this_error_surface_correct/,shebbbb,1545188120,[removed],0,1,False,self,,,,,
1070,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,11,a7ic6g,self.MachineLearning,[R] TDLS: Human-level control through deep reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/a7ic6g/r_tdls_humanlevel_control_through_deep/,tdls_to,1545188169,"**Algorithm Review:** [https://youtu.be/teDuLk3cIeI](https://youtu.be/teDuLk3cIeI)

**Paper Discussion:** [https://youtu.be/ugjjjtuVshY](https://youtu.be/ugjjjtuVshY)

**Paper Reference:** [https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)

&amp;#x200B;

We recently reviewed the original deep Q-learning paper from Deepmind. This paper...

* Combined Q-learning with a conv net
* Able to use high-dimensional input
* Added techniques to stabilize learning:
   * Experience replay
   * Second network
* Result: generalizable agent
   * Learns in various different tasks (=Atari games)
   * Beats human performance in 29 of the 49 games

We ended with some discussion points:

* Some games are more manageable for the agent than others. Why?
* What other tasks can this agent be applied to?
   * Anything in your line of work?
   * Would the architecture need to be tweaked?
* The agent starts with no clue about the environment it works in. How can this knowledge be used?
* Reward is highest game score. What other reward functions can be used here?
* 49 games = 49 trained agents. What can be done to generalize these agents?

&amp;#x200B;",1,1,False,self,,,,,
1071,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,12,a7igje,self.MachineLearning,[R] TDLS: Automated Deep Learning: Joint Neural Architecture and Hyperparameter Search,https://www.reddit.com/r/MachineLearning/comments/a7igje/r_tdls_automated_deep_learning_joint_neural/,tdls_to,1545189063,"**Algorithm Review:** [https://youtu.be/MtuADH7kVeQ](https://youtu.be/MtuADH7kVeQ)

**Paper Discussion:** [https://youtu.be/ghtM-DILSj0](https://youtu.be/ghtM-DILSj0)

**Paper Reference:** [https://arxiv.org/abs/1807.06906](https://arxiv.org/abs/1807.06906)

&amp;#x200B;

We have reviewed this recent paper on simultaneous architecture and hyperparameter search. The paper shows...

* How to use a recent combination (Falkner et al., 2018) of Bayesian optimization (Shahriari et al., 2016) and Hyperband (Li et al., 2017) to perform efficient joint neural architecture and hyperparameter search. 
* That weak correlation between performance after short and long training budgets, which potentially affects both architecture and hyperparameter choices when optimized with these two budgets, and show how to avoid this effect by incrementally increasing the budget during the optimization process. 
* That for a limited training runtime of 3 hours they can achieve competitive performance on CIFAR-10 if we optimize the hyperparameters and architecture jointly

One major discussion point that we were left with was:

* Neural Architecture Search (NAS) and BOHB classifier was applied to a particular task in this paper, and how could this be applied to various data types and data sets (Images \[CIFAR-10\], NLP \[Text data or text to speech and vice versa\], Voice \[IVR systems\], Medical Images \[Pathology classification of X-Ray images\])?",0,1,False,self,,,,,
1072,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,12,a7ijsb,arxiv.org,[R][Neural ODE] FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models,https://www.reddit.com/r/MachineLearning/comments/a7ijsb/rneural_ode_ffjord_freeform_continuous_dynamics/,downtownslim,1545189738,,9,1,False,default,,,,,
1073,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,13,a7iv7z,self.MachineLearning,[D] What is the SOTA for 3D object detection?,https://www.reddit.com/r/MachineLearning/comments/a7iv7z/d_what_is_the_sota_for_3d_object_detection/,akanimax,1545192227,"I am looking for the state of the art for 3D object detection. Specifically, architecture that is able to provide multiple detections in a given RGB-D image similar to YOLO in 2D. Currently referring the Frustrum-pointnet paper, but I wonder if more sophisticated techniques are available. Work with code and weights preferred. Thank you.",3,1,False,self,,,,,
1074,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,14,a7jng5,soumyadip1995.blogspot.com,My new blog.,https://www.reddit.com/r/MachineLearning/comments/a7jng5/my_new_blog/,Soumyadip1995,1545198855,,0,1,False,default,,,,,
1075,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,15,a7jq0h,soumyadip1995.blogspot.com,"In this blog, I talk about topics on ML.",https://www.reddit.com/r/MachineLearning/comments/a7jq0h/in_this_blog_i_talk_about_topics_on_ml/,Soumyadip1995,1545199477,,0,1,False,default,,,,,
1076,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,15,a7jraw,openpr.com,"Auger Drilling Market unique analytical investigation, Size and Forecast to 2025 by Top Key Players; MARL Technologies, Barbco, Liebherr Group, MARL Technologies, Terex, Little Beaver and Agromaster Agricultural Machinery.",https://www.reddit.com/r/MachineLearning/comments/a7jraw/auger_drilling_market_unique_analytical/,stephenb65,1545199806,,0,1,False,default,,,,,
1077,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,15,a7jrr2,self.MachineLearning,Tensorflow Multiple Graphs,https://www.reddit.com/r/MachineLearning/comments/a7jrr2/tensorflow_multiple_graphs/,8222Tamil,1545199917,[removed],0,1,False,self,,,,,
1078,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,15,a7jx60,self.MachineLearning,[R] A Full Hardware Guide to Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a7jx60/r_a_full_hardware_guide_to_deep_learning/,downtownslim,1545201310,"&gt; For good cost/performance, I generally recommend an RTX 2070 or an RTX 2080 Ti. If you use these cards you should use 16-bit models. Otherwise, GTX 1070, GTX 1080, GTX 1070 Ti, and GTX 1080 Ti from eBay are fair choices and you can use these GPUs with 32-bit (but not 16-bit).


http://timdettmers.com/2018/12/16/deep-learning-hardware-guide/",25,1,False,self,,,,,
1079,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,16,a7kcx4,self.MachineLearning,"[D] Hey folks of r/MachineLearning, can we please share paper abstracts in the post description when sharing research paper?",https://www.reddit.com/r/MachineLearning/comments/a7kcx4/d_hey_folks_of_rmachinelearning_can_we_please/,seriously_curious_,1545205648,,15,1,False,self,,,,,
1080,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,17,a7khsa,self.MachineLearning,Starting in Machine Learning...,https://www.reddit.com/r/MachineLearning/comments/a7khsa/starting_in_machine_learning/,JoseChovi,1545207068,"Hi all!  


Just started in **Machine Learning** with some frameworks like *Apache Spark* or *WEKA* but don't know if I am in the right way.

&amp;#x200B;

In my project I have differents CSV files with *19 attributes*:

* 7 Attributes are **String**
* 1 Attribute is **Array of Doubles**
* 11 Attributes are **Float**

So am I right using WEKA or Spark? I can't go on with my project because I don't know how to work in WEKA with arrays or in Spark with arrays and strings.

&amp;#x200B;

What framework would you recommend me? better if I can use it with Java.

&amp;#x200B;

Thank you guys!",0,1,False,self,,,,,
1081,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,17,a7kliq,self.MachineLearning,Laptop for ML: Apple Macbook Pro 13 or Dell XPS 15,https://www.reddit.com/r/MachineLearning/comments/a7kliq/laptop_for_ml_apple_macbook_pro_13_or_dell_xps_15/,duchungvu,1545208184,"Hi guys! I'm considering between 3 laptops Apple MacBook Pro 13 and Dell XPS 15:  


Apple MacBook Pro 13 Dual-core:

\- 2.3GHz dual-core Intel Core i5, Turbo Boost up to 3.6GHz, with 64MB of eDRAM 

\- 8GB of 2133MHz LPDDR3 onboard memory

\- 128GB PCIe-based onboard SSD

\- Intel Iris Plus Graphics 640 

\- Price: $1099

&amp;#x200B;

Apple MacBook Pro 13 Quad-core:

\- 2.3GHz quad-core Intel Core i5, Turbo Boost up to 3.8GHz, with 128MB of eDRAM 

\- 8GB of 2133MHz LPDDR3 onboard memory

\- 256GB SSD

\- Intel Iris Plus Graphics 655 

\- Price: $1529

&amp;#x200B;

Dell XPS 15:

\-  8th Generation Intel Core i7-8750H Processor (9MB Cache, up to 4.1 GHz, 6 Cores)

\- 16GB 2x8GB DDR4-2666MHz

\-  M.2 256GB 2280 PCIe Solid State Drive 

\- NVIDIA GeForce GTX 1050Ti with 4GB GDDR5

\- Price: $1379

&amp;#x200B;

So yeah, I am considering 2 options. First is getting a more powerful laptop (XPS 15) so that I can run CUDA on my laptop and stuff... However, I feel like to train a ML model, a laptop is not enough. But is it enough for educational purpose? Second is getting a low spec laptop (Mac Pro Dual-Core) and spend rest of money renting cloud. The second Mac Pro is kinda expensive but it's MacOS and I'm kinda want to try MacOS.  


I've never used MacOS or Linux, but I am willing to change to either of them. However, I don't know which OS is better for ML. Can you guys help me determine which option I should go for? Thanks :)",0,1,False,self,,,,,
1082,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,17,a7koau,youtu.be,Nvidia learned to make realistic faces,https://www.reddit.com/r/MachineLearning/comments/a7koau/nvidia_learned_to_make_realistic_faces/,skj8,1545209074,,0,1,False,https://a.thumbs.redditmedia.com/jw1X8r8F26DDtafurmTgM24_b8dnpXkVkxXmq4xB5h8.jpg,,,,,
1083,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,17,a7kqeg,cnlasercutter.com,"Laser Cutting Machine, Laser Engraving Machine, Laser Marking Machine, Laser Welding Machine",https://www.reddit.com/r/MachineLearning/comments/a7kqeg/laser_cutting_machine_laser_engraving_machine/,lisarwilliams01,1545209725,,0,1,False,default,,,,,
1084,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,19,a7l5gk,self.MachineLearning,Mei App uses Artificial Intelligence to decipher the meaning behind text messages: https://www.compelo.com/text-interpretation-app/,https://www.reddit.com/r/MachineLearning/comments/a7l5gk/mei_app_uses_artificial_intelligence_to_decipher/,finer_life,1545214215,Sometimes it can be confusing to decipher the tone of text messages especially if you don't know the person well. This app helps you interpret text messages,0,1,False,self,,,,,
1085,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,19,a7la71,self.MachineLearning,[Discussion] ELU is more computation efficient then Leaky RELU?,https://www.reddit.com/r/MachineLearning/comments/a7la71/discussion_elu_is_more_computation_efficient_then/,nobody_tw,1545215628,"I'm building a image enhancement network for resource-constrained devices. The network is based on U-net, implement with tensorflow. Using a 1080Ti with a 512x512 input image, **ELU gives a result of 0.036 sec/image,** in contrast, **Leaky RELU is slower with 0.039 sec/image**. I also tried with CPU, Leaky RELU is still 10% slower. It's pretty strange to me since the function of Leaky RELU is mostly linear and is much simpler, it should be faster and more efficient. Maybe its cause by the optimization strategy in the CUDA? What did i miss?

&amp;#x200B;",6,1,False,self,,,,,
1086,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,19,a7lb7v,self.MachineLearning,First complete online course on Generative Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/a7lb7v/first_complete_online_course_on_generative/,martinmusiol14,1545215958,[removed],0,1,False,self,,,,,
1087,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,19,a7lbkp,/r/MachineLearning/comments/a7lbkp/stepwise_regression/,Step-wise Regression,https://www.reddit.com/r/MachineLearning/comments/a7lbkp/stepwise_regression/,ath_ank,1545216068,,0,1,False,default,,,,,
1088,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,19,a7ldap,self.MachineLearning,[D] How to identify the correct manifold(s) for an optimization problem,https://www.reddit.com/r/MachineLearning/comments/a7ldap/d_how_to_identify_the_correct_manifolds_for_an/,anonymousTestPoster,1545216601,"I don't have much experience in topology, but I am interested to know if: 


  Given a particular problem and associated cost function, how would one deduce what kind of manifold this problem lies on.


I ask this because as far as I know standard gradient descent algorithms implicitly assume we are working on a Euclidean manifold ... but how would I be able to know whether or not I am working on a Euclidean manifold on a problem-by-problem basis? 

For example if we can prove that the parameter space for a particular problem lies on a Riemannian manifold, then the natural gradient may prove more fruitful to compute than the standard gradient.



Any help, key-words, pointers in the right direction would be super helpful. What are the key steps to identify this.",6,1,False,self,,,,,
1089,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,19,a7leua,arxiv.org,[1812.07019] Malthusian Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a7leua/181207019_malthusian_reinforcement_learning/,ihaphleas,1545217073,,1,1,False,default,,,,,
1090,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,20,a7lfh1,arxiv.org,[1812.07179] Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving,https://www.reddit.com/r/MachineLearning/comments/a7lfh1/181207179_pseudolidar_from_visual_depth/,ihaphleas,1545217247,,6,1,False,default,,,,,
1091,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,20,a7lga0,youtube.com,"Dr. Joelle Pineau - Reproducible, Reusable, and Robust Reinforcement Learning - NeurIPS 2018",https://www.reddit.com/r/MachineLearning/comments/a7lga0/dr_joelle_pineau_reproducible_reusable_and_robust/,pienuthome,1545217466,,0,1,False,default,,,,,
1092,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,20,a7lhjt,learntek.org,Machine Learning is the future: Heres everything you must know,https://www.reddit.com/r/MachineLearning/comments/a7lhjt/machine_learning_is_the_future_heres_everything/,jahnavi1209,1545217822,,0,1,False,default,,,,,
1093,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,20,a7lid2,arxiv.org,[1810.10340] A Case for Object Compositionality in Deep Generative Models of Images,https://www.reddit.com/r/MachineLearning/comments/a7lid2/181010340_a_case_for_object_compositionality_in/,meta_learner,1545218060,,1,1,False,default,,,,,
1094,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,20,a7lrcl,self.MachineLearning,Boom Vangs Market Checkout the Unexpected Future 2025,https://www.reddit.com/r/MachineLearning/comments/a7lrcl/boom_vangs_market_checkout_the_unexpected_future/,Nikita828,1545220528," Boom Vangs Market report contains a forecast of the next 5 years, starting 2018 and ending 2023 with a host of metrics like supply-demand ratio, Boom Vangs market frequency, dominant players of Boom Vangs market, driving factors, restraints, and challenges. This comprehensive Boom Vangs Market research report includes a brief on these trends that can help the businesses operating in the industry to understand the market and strategize for their business expansion accordingly. The research report analyses the market size, industry share, growth, key segments, CAGR and key drivers.  
Boom Vangs Market research report delivers a plan regarding the expansion of supply and demand of the generated products and offering services compared with the key market players  **BAMAR, Cariboni, Forespar, Garhauer Marine, Hall Spars &amp; rigging, Holt, Hood Yacht Systems, Nautos, Navtec, Nemo Industrie, OCEAN YACHT SYSTEMS, Reckmann, Schaefer, Sea Sure, Selden Mast AB, Sparcraft, Z-Spars** of the Boom Vangs market globally.  
[**Download Sample PDF Here**](https://qyresearch.us/report/global-boom-vangs-market-2018/372122/#requestForSample)  


**Product Insights of Boom Vangs Market:**

Hydraulic, Rigid, Reverse Thrust, Pneumatic

&amp;#x200B;

**Application Insights of Boom Vangs Market:**

Professional Sports, Amateur Leisure

&amp;#x200B;

Geographically, this report studies the top producers and consumers, focuses on product capacity, production, value, consumption, market share and growth opportunity in these key regions, covering  
North America, Europe China Japan, Southeast Asia, India  
A deep investigation of the Boom Vangs market depends on global patterns, which have been recently incorporated into the study, is also included in the report. In addition, the market report through deep analysis provides statistical estimations on the upcoming force of the market. The dynamic foundation of the global market is based on the calculation of product supply in different markets, their revenues, capability, and a chain of production.  
New vendors in the market are facing tough competition from established international vendors as they struggle with technological innovations, reliability and quality issues. The report will answer questions about the current market developments and the scope of competition, opportunity cost and more.  
[**Any Query?? Ask Here**](https://qyresearch.us/report/global-boom-vangs-market-2018/372122/#inquiry)  
The global Boom Vangs market is also estimated through the production efficiency and production cost, average demand and supply of the products at the global level, and the income generated by the item. Various logical methods and tools, for example, asset returns, probability, and SWOT analysis have been utilized in the report to represent an entire review of the global Boom Vangs market.  
This exhaustive Boom Vangs report are designed via a proprietary research methodology and are available for key industries. The Boom Vangs Market Intelligence Data expert team accepts questions as well, so you can contact them on the official website, and you can order a custom report for expanding your business. Companies who purchase and use this report are totally profited with the inferences delivered in it. Apart from this, the report also provides in-depth analysis of Boom Vangs sale as well as the factors that influence the customers as well as enterprises towards this technique. ",0,1,False,self,,,,,
1095,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,21,a7lytd,arxiv.org,[R] Three Mechanisms of Weight Decay Regularization,https://www.reddit.com/r/MachineLearning/comments/a7lytd/r_three_mechanisms_of_weight_decay_regularization/,abstractcontrol,1545222433,,2,1,False,default,,,,,
1096,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,22,a7m7p9,self.MachineLearning,[Discussion] Handling missing data between historical and api data,https://www.reddit.com/r/MachineLearning/comments/a7m7p9/discussion_handling_missing_data_between/,ohwhyme1987,1545224545,"Say I have two datasets: one is a large dataset with all historical data and another where the data is tiny (sometimes has 1-5 rows). I have managed to impute the mean with the historical dataset but I cannot do the same with the api dataset because sometimes there are columns that are completely empty so sklearn's Imputer will drop it because it cannot get the mean. Thus I cannot predict because the model expects a certain number of columns but the Imputer dropped the empty ones because the number of rows is so low in the api dataset.

&amp;#x200B;

Do I delete the problematic columns, merge the api data to the historical data and then impute (would be a lot slower since the data is huge), or somehow transfer the imputed mean data to the api data, and would that even work? Would that mean I have to stick with ensemble algos or something that deals with missing data better? Atm I am trying to use stochastic gradient descent. Thanks.",0,1,False,self,,,,,
1097,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,22,a7m8y3,self.MachineLearning,[P] arxiv_abstract_bot,https://www.reddit.com/r/MachineLearning/comments/a7m8y3/p_arxiv_abstract_bot/,arXiv_abstract_bot,1545224818,"hey, saw someone mention recently that I was missed... I'm hosted on a lab computer that gets messed with fairly often, so in case I go down permanently in the future, or if you'd like to run something similar on other subreddits, here's the code:

    import praw
    import requests
    import bs4
    import html2text
    import time


    USERNAME = 'arXiv_abstract_bot'
    PASSWORD = '********'
    CLIENT_ID = '*******'
    SECRET = '********'

    r = praw.Reddit(
        username=USERNAME,
        password=PASSWORD,
        client_id=CLIENT_ID,
        client_secret=SECRET,
        user_agent='linux:arXiv_abstract_bot:0.2 (by /u/arXiv_abstract_bot'
        )

    subreddit = r.subreddit('machinelearning')

    alreadydone = set()

    def scrape_arxiv(url):
        r = requests.get(url)
        soup = bs4.BeautifulSoup(r.text)
        abstract = soup.select('.abstract')[0]
        abstract = html2text.html2text(abstract.decode()).replace('\n', ' ')

        authors = soup.select('.authors')[0]
        authors = html2text.html2text(authors.decode()).replace('\n', ' ')
        authors = authors.replace('(/', '(http://arxiv.org/')

        title = soup.select('.title')[0]
        title =  html2text.html2text(title.decode()).replace('\n', ' ')[2:]

        abs_link = u'[Landing page]({})'.format(url)
        pdf_url = url.replace('/abs/', '/pdf/')
        pdf_link = u'[PDF link]({})'.format(pdf_url)
        links = u'{}  {}'.format(pdf_link, abs_link)
        response = '\n\n'.join([title, authors, abstract, links]) 
        return response


    def comment():
        print(time.asctime(), ""searching"")
        try:
            all_posts = subreddit.new(limit=100)
            for post in all_posts:
                if 'arxiv.org' in post.url:
                    if post.id in alreadydone:
                        continue
                    for comment in post.comments:
                        if str(comment.author) == USERNAME:
                            break
                    else:
                        landing_url = post.url
                        if '.pdf' in landing_url:
                            landing_url = post.url.replace('.pdf', '')
                            landing_url = landing_url.replace('/pdf/', '/abs/')

                        response = scrape_arxiv(landing_url)
                        post.reply(response)
                        alreadydone.add(post.id)
                        print(landing_url, response)
                        time.sleep(10)
        except Exception as error:
            print(error)


    if __name__ == ""__main__"":
        while True:
            comment()
            time.sleep(30)


",7,1,False,self,,,,,
1098,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,22,a7matd,arxiv.org,CFUN: Combining Faster R-CNN and U-net Network for Efficient Whole Heart Segmentation,https://www.reddit.com/r/MachineLearning/comments/a7matd/cfun_combining_faster_rcnn_and_unet_network_for/,Jul8234,1545225228,,1,1,False,default,,,,,
1099,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,23,a7mul2,self.MachineLearning,Parallel programming in ML,https://www.reddit.com/r/MachineLearning/comments/a7mul2/parallel_programming_in_ml/,white_noise212,1545229543,[removed],0,1,False,self,,,,,
1100,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,23,a7n0k1,self.MachineLearning,Earn 40%* More As A Software Developer,https://www.reddit.com/r/MachineLearning/comments/a7n0k1/earn_40_more_as_a_software_developer/,denisehasler,1545230774,[removed],0,1,False,self,,,,,
1101,MachineLearning,t5_2r3gv,2018-12-19,2018,12,19,23,a7n4qf,self.MachineLearning,[P] cuda-bootstrap.com - Install CUDA,https://www.reddit.com/r/MachineLearning/comments/a7n4qf/p_cudabootstrapcom_install_cuda/,kendrick__,1545231593,"[Admins delete if not allowed]

Hello /r/machinelearning, one of the biggest pain points for me while doing machine learning was installing CUDA for the DL libraries. So, I wrote a little webapp to streamline that specific process and thought I would share it with the community - https://cuda-bootstrap.com

Any feedback/suggestions are welcomed.",23,1,False,self,,,,,
1102,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,0,a7n9qy,youtu.be,GOTO 2018  (Deep) Learning to Fly  Krzysztof Kudrynski &amp; Blazej Kubiak,https://www.reddit.com/r/MachineLearning/comments/a7n9qy/goto_2018_deep_learning_to_fly_krzysztof/,rick-rebel,1545232526,,0,1,False,https://b.thumbs.redditmedia.com/gfIYxrNwLL7BwbM7jWhEFV_KBXlN12oPdgcAf6KXszY.jpg,,,,,
1103,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,0,a7n9wu,self.MachineLearning,[D] Whatever happened to distributional RL?,https://www.reddit.com/r/MachineLearning/comments/a7n9wu/d_whatever_happened_to_distributional_rl/,TheRedSphinx,1545232557,"I know there was a lot of interest in this like a year ago, but I only see a few papers from the a quick google search, with very few citations. Did it not just pan out or did it just fall out of favor?",9,1,False,self,,,,,
1104,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,0,a7nbao,self.MachineLearning,Google Colab Dark Theme HOWTO,https://www.reddit.com/r/MachineLearning/comments/a7nbao/google_colab_dark_theme_howto/,fstbm,1545232817,[removed],0,1,False,self,,,,,
1105,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,0,a7nlp2,self.MachineLearning,"Simple Questions Thread December 19, 2018",https://www.reddit.com/r/MachineLearning/comments/a7nlp2/simple_questions_thread_december_19_2018/,AutoModerator,1545234761,[removed],0,1,False,self,,,,,
1106,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,0,a7nn6p,self.MachineLearning,[P] Generating new ideas for machine learning projects through machine learning,https://www.reddit.com/r/MachineLearning/comments/a7nn6p/p_generating_new_ideas_for_machine_learning/,invertedpassion,1545235034,"Hello everyone,

Two weeks ago, I had asked on this subreddit how to do text generation from a small corpus. I got [many helpful responses](https://www.reddit.com/r/MachineLearning/comments/a4ihvd/d_how_to_do_text_generation_from_a_small_dataset/).

&amp;#x200B;

This helped me finish a fun project where **I generated new machine learning ideas** based on a small corpus of 2.5k machine learning ideas I collected from the Internet. The key idea that helped me was to use a pre-trained language model if the dataset is small. I wrote about my project in this article: [Generating New Ideas for Machine Learning Projects Through MachineLearning](https://towardsdatascience.com/generating-new-ideas-for-machine-learning-projects-through-machine-learning-ce3fee50ec2). It uses PyTorch v1 and FastAI v1.

&amp;#x200B;

Hope you like the project! I welcome any feedback for improvement.

&amp;#x200B;",5,1,False,self,,,,,
1107,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,1,a7npqn,medium.com,[P] NeurIPS 2018 Through the Eyes of First-Timers,https://www.reddit.com/r/MachineLearning/comments/a7npqn/p_neurips_2018_through_the_eyes_of_firsttimers/,gwen0927,1545235492,,0,1,False,default,,,,,
1108,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,1,a7ntl2,oreilly.com,Deep automation in machine learning,https://www.reddit.com/r/MachineLearning/comments/a7ntl2/deep_automation_in_machine_learning/,gradientflow,1545236204,,0,1,False,default,,,,,
1109,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,1,a7nv34,self.MachineLearning,[D} Energy Based Models still a trend?,https://www.reddit.com/r/MachineLearning/comments/a7nv34/d_energy_based_models_still_a_trend/,mackie__m,1545236471,[removed],0,1,False,self,,,,,
1110,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,2,a7od98,medium.com,[P] Reinforcement Learning from Scratch: Designing and Solving a Task All Within a Python Notebook,https://www.reddit.com/r/MachineLearning/comments/a7od98/p_reinforcement_learning_from_scratch_designing/,osbornep,1545239659,,1,1,False,https://b.thumbs.redditmedia.com/5tHYHYB0qEv9UExqMfMRHD9e1L3jDnTX8T0uP4mBCDw.jpg,,,,,
1111,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,2,a7ojfm,arxiv.org,[R] Uncertainty in Neural Networks: Bayesian Ensembling,https://www.reddit.com/r/MachineLearning/comments/a7ojfm/r_uncertainty_in_neural_networks_bayesian/,abstractcontrol,1545240740,,9,1,False,default,,,,,
1112,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,2,a7oomf,tryolabs.com,[D] The major advancements in Deep Learning in 2018,https://www.reddit.com/r/MachineLearning/comments/a7oomf/d_the_major_advancements_in_deep_learning_in_2018/,minmidinosaur,1545241672,,0,1,False,default,,,,,
1113,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,2,a7oqdu,fermatslibrary.com,"A Chrome extension that enhances arXiv papers. Get direct links to references, BibTeX extraction and comments on all arXiv papers",https://www.reddit.com/r/MachineLearning/comments/a7oqdu/a_chrome_extension_that_enhances_arxiv_papers_get/,boxedmilkyway,1545241981,,0,1,False,https://b.thumbs.redditmedia.com/6LeV0vmNIO6z-76PUqkeMGIF1yMDTUPublSGb2yigck.jpg,,,,,
1114,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,3,a7pa81,tryolabs.com,[D] The major advancements in Deep Learning in 2018,https://www.reddit.com/r/MachineLearning/comments/a7pa81/d_the_major_advancements_in_deep_learning_in_2018/,thesameoldstories,1545245476,,0,1,False,default,,,,,
1115,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,4,a7pm0b,self.MachineLearning,What's your take on Attention/Transformers/Bert replacing RNNs/LSTMs/GRUs ?,https://www.reddit.com/r/MachineLearning/comments/a7pm0b/whats_your_take_on_attentiontransformersbert/,BatmantoshReturns,1545247484,[removed],0,1,False,self,,,,,
1116,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,4,a7pmck,self.MachineLearning,[D] What's your take on Attention/Transformers/Bert replacing RNNs/LSTMs/GRUs ?,https://www.reddit.com/r/MachineLearning/comments/a7pmck/d_whats_your_take_on_attentiontransformersbert/,BatmantoshReturns,1545247544,,32,1,False,self,,,,,
1117,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,4,a7pszv,self.MachineLearning,I bought 6 1080tis for ML. What's the best / most economical way to use them all together for deep learning?,https://www.reddit.com/r/MachineLearning/comments/a7pszv/i_bought_6_1080tis_for_ml_whats_the_best_most/,FR_STARMER,1545248712,[removed],0,1,False,self,,,,,
1118,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,4,a7pugl,self.MachineLearning,AWS Mechanical Turk Object Annotation Price,https://www.reddit.com/r/MachineLearning/comments/a7pugl/aws_mechanical_turk_object_annotation_price/,salihkaragoz,1545248964,[removed],0,1,False,self,,,,,
1119,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,5,a7qbsr,self.MachineLearning,[D] I find ML research papers hard to read. What would be a better format?,https://www.reddit.com/r/MachineLearning/comments/a7qbsr/d_i_find_ml_research_papers_hard_to_read_what/,ranihorev,1545251963,"Hey, 

Ive been reading research papers on Machine and Deep Learning for years, and I still find them hard to read. A few reasons I can think of:

1. The abstract is unreadable, sometimes because of over complicated language and sometimes requires knowledge from the paper itself. 
2. Many papers assume a lot of previous knowledge and they reference to another paper without giving enough context. Jumping back and forth to referenced papers makes it hard to follow the paper (I have to read it at least twice). 
3. Most papers dive straight into the details without providing an overview of the model.

Im generalizing of course and there are excellent papers that are also easy to read, but they are rare. Most researchers Ive met are super smart people (way more than me) but it doesnt mean that they are the most talented writers.

I was wondering if its just me or not :)

Im also thinking of writing summaries of papers and Id love to know what would be the best format in your opinion? 

When you want to stay updated on new developments, are you more interested in the background, the model, the results, use cases? Something else?",24,1,False,self,,,,,
1120,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,5,a7qgx1,self.MachineLearning,"Did studying machine learning change the way you view human/animal intelligence, seeing as how neural networks have a similar structure to brains?",https://www.reddit.com/r/MachineLearning/comments/a7qgx1/did_studying_machine_learning_change_the_way_you/,BatBast,1545252869,[removed],0,1,False,self,,,,,
1121,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,6,a7qjdp,youtube.com,[P] I made a Raspberry Pi pet detector camera that watches the door and sends me a text if my cat or dog wants to be let inside. It uses the TensorFlow SSDLite-MobileNet model for object detection and achieves a decent framerate.,https://www.reddit.com/r/MachineLearning/comments/a7qjdp/p_i_made_a_raspberry_pi_pet_detector_camera_that/,Taxi-guy,1545253299,,0,1,False,default,,,,,
1122,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,6,a7qutb,bmj.com,"On a lighter note, lets try not to have more papers like this",https://www.reddit.com/r/MachineLearning/comments/a7qutb/on_a_lighter_note_lets_try_not_to_have_more/,DeepDreamNet,1545255329,,0,1,False,default,,,,,
1123,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,7,a7rcd6,arstechnica.com,[D] How computers got shockingly good at recognizing images - Article,https://www.reddit.com/r/MachineLearning/comments/a7rcd6/d_how_computers_got_shockingly_good_at/,Quazaka,1545258486,,0,1,False,https://b.thumbs.redditmedia.com/0S5xiuiAbYhB2Ko8SWRcpA4rk_xEH7LPjzNwoN0d0AE.jpg,,,,,
1124,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,7,a7rdp6,self.MachineLearning,Code for [Video Object Detection with an Aligned Spatial-Temporal Memory],https://www.reddit.com/r/MachineLearning/comments/a7rdp6/code_for_video_object_detection_with_an_aligned/,fanyix,1545258731,Code for \[Video Object Detection with an Aligned Spatial-Temporal Memory\] is now available at: [https://github.com/fanyix/STMN](https://github.com/fanyix/STMN),0,1,False,self,,,,,
1125,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,7,a7rg2e,arstechnica.com,How computers got shockingly good at recognizing images,https://www.reddit.com/r/MachineLearning/comments/a7rg2e/how_computers_got_shockingly_good_at_recognizing/,Quazaka,1545259166,,0,1,False,https://b.thumbs.redditmedia.com/0S5xiuiAbYhB2Ko8SWRcpA4rk_xEH7LPjzNwoN0d0AE.jpg,,,,,
1126,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,8,a7rwsx,arxiv.org,[R] Bayesian Optimization in AlphaGo (to improve its win-rate from 50% to 66.5% in self-play games),https://www.reddit.com/r/MachineLearning/comments/a7rwsx/r_bayesian_optimization_in_alphago_to_improve_its/,ranihorev,1545262280,,27,1,False,default,,,,,
1127,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,8,a7rzzf,self.MachineLearning,Need help with Text classification in Keras R,https://www.reddit.com/r/MachineLearning/comments/a7rzzf/need_help_with_text_classification_in_keras_r/,rohan36,1545262909,[removed],0,1,False,self,,,,,
1128,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,11,a7t9ka,self.MachineLearning,[D] could deep learning style transfer be used to generate episodes of shows that have gone off the air?,https://www.reddit.com/r/MachineLearning/comments/a7t9ka/d_could_deep_learning_style_transfer_be_used_to/,squakmix,1545271863,"It would be wacky, but I'm imagining a system that uses footage from a long-running show along with transcripts of the show and could take input in the form of a description of events and output synthesized video footage of the events. I'm imagining an animated show like King of the Hill as the dataset with input like ""Hank joins his friends in the alley and grabs a beer. He says 'Yep'"".

Is this feasible right now? I have to imagine someone is working on this and I'd appreciate a point in the right direction. ",0,1,False,self,,,,,
1129,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,12,a7txdi,self.MachineLearning,Suggestions for network architecture for ASL recognition,https://www.reddit.com/r/MachineLearning/comments/a7txdi/suggestions_for_network_architecture_for_asl/,Withered_Shadow,1545276924,[removed],0,1,False,self,,,,,
1130,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,12,a7tzph,self.MachineLearning,The Lego blocks of Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/a7tzph/the_lego_blocks_of_deep_learning/,Gear5th,1545277426,[removed],0,1,False,self,,,,,
1131,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,12,a7u0jl,self.MachineLearning,[D] The Lego blocks of Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/a7u0jl/d_the_lego_blocks_of_deep_learning/,Gear5th,1545277608,"A lot of Deep Learning research stems from taking basic building blocks discovered so far, and combining them in interesting ways.

Although treating DL as Lego building without understanding the underlying principles will only get a researcher so far, it is still beneficial to have a list of the available Lego blocks for people trying to apply DL, or people looking for small but non-trivial ML projects.


So, what are the known lego blocks of Deep Learning?

I will start with some:

- **Convolutional NNs:** exploit the inherent structure present in real world images to significantly reduce the number of parameters needed (compared to a fully connected NN)

- **Dropout:** A powerful regulariser. Can be thought of as having an ensemble of models, where the different models in the ensemble share some weights.

- **Attention / Memory:** Allows models to selectively focus on parts of the input while generating part of the output.

Let's try to place as little constraints on the range of answer as possible.",4,1,False,self,,,,,
1132,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,14,a7ur80,arxiv.org,[R] Credibility evaluation of tax declarations - how to predict conditional probability from a large number of mostly discrete variables?,https://www.reddit.com/r/MachineLearning/comments/a7ur80/r_credibility_evaluation_of_tax_declarations_how/,jarekduda,1545283653,,16,1,False,default,,,,,
1133,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,15,a7uzlh,self.MachineLearning,[R]A Concise And Insightful Summary Of Concrete Mixer Pumps,https://www.reddit.com/r/MachineLearning/comments/a7uzlh/ra_concise_and_insightful_summary_of_concrete/,aimixgrupo,1545285665,"Throughout my time in the industry, Ive noticed that a lot of different managers and executives neglect to check out boosting the efficiency from the concrete mixer pumps that they can utilize. It is a great shame because they pumps can really enhance the profits seen by a company if they are found in the very best fashion. ONe of the biggest ways to improve profit margins regarding concrete mixer pumps is to formulate better strategies about the shipping logistics that can be used for these pumps. This information([Esta informacin](https://aimixgrupo.com.mx)) will provide a brief and insightful review of concrete mixer pumps.

&amp;#x200B;

[Concrete Mixer Pump](https://i.redd.it/8otwuc9hhd521.jpg)

As stated, concrete mixer pumps certainly are a very vital section of the operations of the huge range of different companies in the marketplace. Ive seen a lot of companies make heavy consumption of these pumps without putting any resources towards formulating new means of by using these pumps to better profits. One of the biggest regions of expenses that occur from utilizing concrete mixer pumps is by shipping logistics. Simply because these pumps really need to be replaced often times all through the year, and while they are replaced, foreign manufacturers are usually contracted to provide the newest pumps.

&amp;#x200B;

Getting foreign manufacturers and retailers to provide the latest concrete mixer pumps which can be needed for operations means that plenty of expense every year could be accumulated through shipping fees. The shipping logistics that happen to be needed for these pumps( [para estas bombas](https://aimixgrupo.com.mx/bomba-mezcladora-de-concreto/) ) are often quite expensive, specially when originating from foreign retailers and manufacturers. Many individuals in the industry, We have noticed, are unaware of how quick shipping costs can add up. Lots of people simply see that foreign companies can have better prices for such pumps, thus immediately contract with one of these businesses without properly considering the high shipping costs that have to be endured by using the services of such companies.

&amp;#x200B;

Hence, once i begun to realize the amount of an impact shipping logistic costs had about the company i used to manage, I immediately did start to make some changes. I knew that I would still have to replace concrete mixer pumps frequently all year round. However, I also knew that we could choose better suppliers and manufacturers that would be able to significantly reduce the total costs I will have to endure to be able to have these pumps delivered within a convenient and trustworthy fashion. Hence, I made the decision to accomplish a lot of research regarding concrete mixer pump suppliers that have been as near the plant I found myself managing as is possible. By contracting with one of these businesses, I surely could immediately lower the shipping expenses experienced around.

&amp;#x200B;

Hence, from the new implementations that we had developed, I was able to reduce the overall costs( [costos generales](https://es.wikipedia.org/wiki/Gastos_generales) ) that had been connected with maintaining and replacing concrete mixer pumps each and every year. Needless to say, buying pumps from local manufacturers and retailers was a lot more expensive than buying from foreign companies. However, if you included the shipping costs together with the prices being asked by foreign companies, it had been clear that this cheaper option was to buy concrete mixer pumps locally.",0,1,False,https://b.thumbs.redditmedia.com/F5vpsa9v1mVUwQh7fVj26CDImIT5sA0obxKil5Zvs1M.jpg,,,,,
1134,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,15,a7v0fy,oculus.com,Introducing DeepFocus: The AI Rendering System Powering Half Dome,https://www.reddit.com/r/MachineLearning/comments/a7v0fy/introducing_deepfocus_the_ai_rendering_system/,Marha01,1545285867,,0,1,False,default,,,,,
1135,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,16,a7vknj,self.MachineLearning,What are good list of papers to read for a beginner in deep learning? [D],https://www.reddit.com/r/MachineLearning/comments/a7vknj/what_are_good_list_of_papers_to_read_for_a/,mhachem-reddit,1545291080,,1,1,False,self,,,,,
1136,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,18,a7wbl3,hackernoon.com,[P]  Advanced annotation tools in Deep Learning: training data for computer vision with Supervisely,https://www.reddit.com/r/MachineLearning/comments/a7wbl3/p_advanced_annotation_tools_in_deep_learning/,tdionis,1545299032,,0,1,False,https://b.thumbs.redditmedia.com/m8w7sIshXvjrzVb5tdB5o9Nx1oybCmOJcZ1INxtBf_k.jpg,,,,,
1137,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,19,a7wfy0,self.MachineLearning,[D] suprising input focus in neural networks: any references?,https://www.reddit.com/r/MachineLearning/comments/a7wfy0/d_suprising_input_focus_in_neural_networks_any/,FilippoC,1545300320,"Hi,

&amp;#x200B;

I heard many stories about suprising results when inspecting which part of an input image is the more important for the output.  
One of them is about image classification, where for one dataset someone found that the important feature for being classified to the ""boat"" class is not the boat itself but the water around it.

&amp;#x200B;

Does anyone have any references for this stories? Maybe in NLP too?

I failed to find any paper talking about that...

&amp;#x200B;

Thanks!  
",3,1,False,self,,,,,
1138,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,19,a7wok9,medium.com,NeurIPS 2018 Highlights (Part 1),https://www.reddit.com/r/MachineLearning/comments/a7wok9/neurips_2018_highlights_part_1/,omarsar,1545302520,,0,1,False,default,,,,,
1139,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,19,a7wrjx,self.mlpapers,Reading group for ML papers,https://www.reddit.com/r/MachineLearning/comments/a7wrjx/reading_group_for_ml_papers/,rosnikv,1545303310,,0,1,False,default,,,,,
1140,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,19,a7wrns,self.MachineLearning,Scalable clustering with minimum size constraint,https://www.reddit.com/r/MachineLearning/comments/a7wrns/scalable_clustering_with_minimum_size_constraint/,creiser,1545303341,[removed],0,1,False,self,,,,,
1141,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,21,a7xb9i,self.MachineLearning,[D] Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a7xb9i/d_recurrent_neural_networks/,safe_lel,1545307886,I am new to ML(5 months) and have decided to go forth with the concept of RNNs. Can anyone suggest me some good study material I can get online for free? ,4,1,False,self,,,,,
1142,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,22,a7y1m1,self.MachineLearning,The Future Ethics of AI in Cyber security,https://www.reddit.com/r/MachineLearning/comments/a7y1m1/the_future_ethics_of_ai_in_cyber_security/,wisecurity,1545313423,[removed],0,1,False,self,,,,,
1143,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,22,a7y37w,self.MachineLearning,[D] How do we formalize the notions of generalization and memorization?,https://www.reddit.com/r/MachineLearning/comments/a7y37w/d_how_do_we_formalize_the_notions_of/,olaconquistador,1545313740,"Are there any works looking to formalize these notions? what does it mean if a sample has been memorized, vs good performance on it as a result of good generalization across samples. Can we test for the presence/absence these phenomena, or quantify their effects?",6,1,False,self,,,,,
1144,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,23,a7yfjm,self.MachineLearning,FREE EARNING,https://www.reddit.com/r/MachineLearning/comments/a7yfjm/free_earning/,harjitbaj,1545316071,[removed],0,1,False,self,,,,,
1145,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,23,a7yjkv,self.MachineLearning,[D] What do you feel were the biggest advancements in DL for this year?,https://www.reddit.com/r/MachineLearning/comments/a7yjkv/d_what_do_you_feel_were_the_biggest_advancements/,GrammarBordaberry,1545316833,"I came across [this](https://tryolabs.com/blog/2018/12/19/major-advancements-deep-learning-2018/) blogpost about this year in deep learning, and was wondering what your stands on the subject were.

Not just individual papers, but what actual advances in the technology do you feel has significantly evolved over this year?

Thanks. :)",50,1,False,self,,,,,
1146,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,23,a7yld3,self.MachineLearning,[Project] Best way to classify pages in a PDF?,https://www.reddit.com/r/MachineLearning/comments/a7yld3/project_best_way_to_classify_pages_in_a_pdf/,PlaneReflection,1545317203,"I had an idea for a project at work and wanted to get your input. I'd get a PDF of 1000+ pages related to a mortgage and we'd parse out each page depending on what it is. For example, we'd place appraisals in one folder names ""Appraisal,"" loan applications in the ""Loan Application"" folder and so on. With that being said, I'd like to build a machine learning tool that does the following:

1. Check known truth classified folders to see if there's any changes. If there are, update datadrames.
2. Drag and drop unclassified PDF in.
3. Rotate page until it's ""up."" Some pages are side ways, upside down and etc.
4. OCR to convert page by page into a dataframe.
5. Compare each dataframe with the knowns truth documents.
6. Extract that page until the document end (for example, a PDF can have 10 pages) and rename the file to match the known truth foldername appended by loan number. 

The known truth classified folders would contain files that we know belong in there. Again, all loan applications would be in the ""Loan Application"" folder, and no appraisals will be in this folder.

With that being said, I've noticed difficulties with working with PDFs and OCR. Can someone point me to tools/libraries that could help to tackle these two difficulties? 

The more important question, what would be the best way of classifying these? It seems like building a dictionary would be too time consuming on my part and take too long to parse out. Perhaps building a model for each data type based on words might be faster and easier (e.g. counting the number of times there's ""comparable"" or known ratios against other words). I was hoping there would be a function that takes the known truth dataframes and unclassified dataframe as an input, compare the two, and produce some score to indicate the similarity.

Thanks in advance for the help!",0,1,False,self,,,,,
1147,MachineLearning,t5_2r3gv,2018-12-20,2018,12,20,23,a7yp7x,self.MachineLearning,Anyone going to Stellenbosch to attend the Machine Learning Summer School 2019?,https://www.reddit.com/r/MachineLearning/comments/a7yp7x/anyone_going_to_stellenbosch_to_attend_the/,yashiiit,1545317964,[removed],0,1,False,self,,,,,
1148,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,0,a7yps9,self.MachineLearning,Training on Imperfect Data,https://www.reddit.com/r/MachineLearning/comments/a7yps9/training_on_imperfect_data/,jammy_say,1545318067,[removed],0,1,False,self,,,,,
1149,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,0,a7z0ra,self.MachineLearning,[D] Derivation of expected sufficient statistics of GDD,https://www.reddit.com/r/MachineLearning/comments/a7z0ra/d_derivation_of_expected_sufficient_statistics_of/,cuenta4384,1545319988,"I am trying to learn Bayesian modeling, and I am analyzing [this paper](https://spectrum.library.concordia.ca/978906/1/Bakhtiari_Bouguila.pdf) that uses LDA with the Generalized Dirichlet distribution (GDD) instead of Dirichlet distribution. 

  
In [the paper](https://spectrum.library.concordia.ca/978906/1/Bakhtiari_Bouguila.pdf), the author expresses the GDD in the exponential family (equation 23) and calculate the expectation of the sufficient statistics which is equal to the derivation of the log normalization factor w.r.t. the natural parameters (equation 24 and 25). However, I don't understand the derivation of equation 24.

&amp;#x200B;

The GDD has \\alpha and \\beta as natural parameters. For instance, when deriving the log normalization factor w.r.t. \\beta\_l, the resulting expression is equation 25. So, in order to get equation 24, we need to derive w.r.t. \\alpha\_l. This is the part that I am confused because I don't understand where **\\digamma(\\beta\_l)** comes. What am I missing here? Can somebody help me? 

&amp;#x200B;",0,1,False,self,,,,,
1150,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,1,a7zohf,analyticsvidhya.com,Top Highlights from the Amazing Machine Learning Tutorials Presented at NeurIPS (NIPS) 2018,https://www.reddit.com/r/MachineLearning/comments/a7zohf/top_highlights_from_the_amazing_machine_learning/,ZER_0_NE,1545323918,,0,1,False,default,,,,,
1151,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,1,a7zp5n,self.MachineLearning,What is your workflow for reviewing papers and/or keeping up with literature?,https://www.reddit.com/r/MachineLearning/comments/a7zp5n/what_is_your_workflow_for_reviewing_papers_andor/,se4u,1545324022,[removed],0,1,False,self,,,,,
1152,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,1,a7zqya,self.MachineLearning,[D] Remote jobs in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a7zqya/d_remote_jobs_in_deep_learning/,tensor_x,1545324300,"Hey guys,

&amp;#x200B;

Do any of you have experience with remote Deep Learning jobs? I am currently looking for remote opportunities and I'd greatly appreciate it if you could tell me how you guys got them. 

&amp;#x200B;

PS : I'd be happy to send my CV over if any of you have an opening! (I have strong industrial and research experience   
 \- Published in NeurIPS)

&amp;#x200B;",0,1,False,self,,,,,
1153,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,1,a7zrx7,self.MachineLearning,[Project] Thesis Field/Topic Recommendations,https://www.reddit.com/r/MachineLearning/comments/a7zrx7/project_thesis_fieldtopic_recommendations/,Noahk97,1545324443,"Dear Reddit MachineLearning,

I am a third-year Economics and Computer Science student, whereas I am going to hopefully graduate next year (enrolled in a three-year bachelor program).  
However, I am still rather undecided on what I should write my thesis about.

I would consider myself to be on an intermediate level, as I have performed through Univeristy 4 projects in which we've used both supervised an unsupervised algorithms (We've had Machine Learning as course in the 4th semester).  
Thus, my main question is if someone could just refer me some fields/topics in which I could joyfully deepen my studies.  
Best regards and than you in advance!",0,1,False,self,,,,,
1154,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,1,a7zsc0,self.MachineLearning,Get this amazing book for free: Machine Learning Yearning by Andrew Y. Ng,https://www.reddit.com/r/MachineLearning/comments/a7zsc0/get_this_amazing_book_for_free_machine_learning/,navin49,1545324513,[removed],0,1,False,self,,,,,
1155,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,2,a807hy,self.MachineLearning,[D] How long does it typically take to comprehend a NIPS paper?,https://www.reddit.com/r/MachineLearning/comments/a807hy/d_how_long_does_it_typically_take_to_comprehend_a/,laituan245,1545326841,"For a paper that has a lot of math, it typically takes me &gt; 2 hours to fully comprehend the paper (i.e., methods, experiment setups, results, ...). I was wondering how long would it take for other people?",9,1,False,self,,,,,
1156,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,2,a80els,blog.griddynamics.com,"[N] Algorithmic Pricing, Part II: AI and Pricing Strategy",https://www.reddit.com/r/MachineLearning/comments/a80els/n_algorithmic_pricing_part_ii_ai_and_pricing/,ikatsov,1545327982,,0,1,False,default,,,,,
1157,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,2,a80j4r,self.MachineLearning,How we should prepare an existing custom image dataset for machine learning?,https://www.reddit.com/r/MachineLearning/comments/a80j4r/how_we_should_prepare_an_existing_custom_image/,palash89,1545328719," 

I'm a newbie in machine learning. 

I'm using  custom image dataset to train a model using keras. Then I came to know about data augmentation. I know that how data augmentation helps to reduce overfitting. Though I  still have some doubts about collecting image data. Im hoping if you guys can assist me on the following which Im facing right now.

1. Should I take only square (1:1 ratio) images as this have a better chance for data augmentation? Also, I have seen in some [codes](https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d) where it takes input\_shape with same **width** &amp; **height**.  which means if I keep rectangle images in dataset, it will hamper the  aspect ratio while processing those images. Am I right? Cant I use free  form images (both square &amp; rectangle images) in a dataset?
2. Does images with **black** or **white** background have better **accuracy rate**? But this approach goes against collecting natural imagesright?
3. Can I use a image which consists of multiple same object inside a dataset (i.e, a  bucket full of apples image along with single apple images to train a apple dataset)?

Any information regarding those will be helpful. Thanks in advance.",0,1,False,self,,,,,
1158,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,3,a80sb2,github.com,Github check out my new github for machine learning models and algorithm with solutions,https://www.reddit.com/r/MachineLearning/comments/a80sb2/github_check_out_my_new_github_for_machine/,pavankp1,1545330141,,0,1,False,default,,,,,
1159,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,3,a80yk9,self.MachineLearning,[D] Porting arbitrary style transfer to the browser,https://www.reddit.com/r/MachineLearning/comments/a80yk9/d_porting_arbitrary_style_transfer_to_the_browser/,Reiinakano,1545331157,"Hi Reddit! A few weeks ago, I shared my browser demo for arbitrary style transfer. https://github.com/reiinakano/arbitrary-image-stylization-tfjs

Today, I'm happy to announce it's made its way into magenta-js as an easy-to-use library. https://github.com/tensorflow/magenta-js/tree/master/image

I also wrote a blog post about the whole process. Check it out here: https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/
",0,1,False,self,,,,,
1160,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,3,a8108h,self.MachineLearning,[P] Porting arbitrary style transfer to the browser,https://www.reddit.com/r/MachineLearning/comments/a8108h/p_porting_arbitrary_style_transfer_to_the_browser/,Reiinakano,1545331425,"Hi Reddit! A few weeks ago, I shared my browser demo for arbitrary style transfer.

Today, I'm happy to announce it's made its way into magenta-js as an easy-to-use library.

I wrote a blog post about the whole process. Check it out here:https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/

Magenta lib: https://github.com/tensorflow/magenta-js/tree/master/image

My old demo: https://github.com/reiinakano/arbitrary-image-stylization-tfjs
",1,1,False,self,,,,,
1161,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,4,a816p1,reddit.com,Hello! Programming noob needs help with Keras,https://www.reddit.com/r/MachineLearning/comments/a816p1/hello_programming_noob_needs_help_with_keras/,dijete_u_vremenu,1545332480,,0,1,False,default,,,,,
1162,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,4,a81b4m,medium.com,Top NodeJS Libraries and Tools For Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a81b4m/top_nodejs_libraries_and_tools_for_machine/,simplicius_,1545333177,,0,1,False,https://b.thumbs.redditmedia.com/lzkBF9xhK_fwF3EaiMl_FlDOAamn3Z5SSDyMunmByhw.jpg,,,,,
1163,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,4,a81i6g,ai.googleblog.com,Top Shot on Pixel 3,https://www.reddit.com/r/MachineLearning/comments/a81i6g/top_shot_on_pixel_3/,sjoerdapp,1545334327,,0,1,False,default,,,,,
1164,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,4,a81rie,medium.com,What is machine intelligence &amp; how can we measure it?,https://www.reddit.com/r/MachineLearning/comments/a81rie/what_is_machine_intelligence_how_can_we_measure_it/,prnvb,1545335828,,0,1,False,https://b.thumbs.redditmedia.com/80Qe41ySUnMFZFvjIiSzJ1eYAYOAxl_UzPNSBSsSBGI.jpg,,,,,
1165,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,5,a81xgf,self.MachineLearning,How feasible is it to create useful ML algorithms and sell them to businesses?,https://www.reddit.com/r/MachineLearning/comments/a81xgf/how_feasible_is_it_to_create_useful_ml_algorithms/,negnatz,1545336819,Very interested in machine learning and am starting to learn a lot of ML as of recently. I am a Comp Sci and Econ double major undergrad and want to know how feasible would it be/what are the steps to selling useful ML to small/medium sized businesses? Is there any profit to be made by doing this? Or would it be more beneficial to create useful ML products/services?,0,1,False,self,,,,,
1166,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,5,a827l5,self.MachineLearning,Resources and questions on (sequential) generative models,https://www.reddit.com/r/MachineLearning/comments/a827l5/resources_and_questions_on_sequential_generative/,mostly_rnd_questions,1545338460,"Hello, do you know any good resources (tutorials, papers, lectures) on generative models? I am mainly interested in sequential generative models (like the ones used for video prediction [Stochastic Adversarial Video Prediction](https://arxiv.org/abs/1804.01523)). Is there any formal theory behind them or any resources on practical applications? Also what is the connection to the Vision and Memory module proposed in [World Models](https://arxiv.org/pdf/1803.10122.pdf)? If the authors had used a Convolutional LSTM do you think they would have gotten the same results? Finally if you wanted to model all possible states given the current state (get the probability distribution of p(s'|s) instead of p(s'|s,a)) would the removal of the action as the rnn input suffice? ",0,1,False,self,,,,,
1167,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,5,a82dcl,self.LanguageTechnology,Lingua: written natural language detection for both long and short text,https://www.reddit.com/r/MachineLearning/comments/a82dcl/lingua_written_natural_language_detection_for/,pemistahl,1545339364,,0,1,False,default,,,,,
1168,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,6,a82o6r,self.MachineLearning,Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters,https://www.reddit.com/r/MachineLearning/comments/a82o6r/deconstructing_bert_distilling_6_patterns_from/,vig612,1545341083,"This is my attempt to gain a better intuition of BERT's attention layers through a visualization tool I adapted to BERT. In the process I noticed some interesting patterns.

&amp;#x200B;

Any feedback is appreciated.

&amp;#x200B;

[https://medium.com/@JesseVig/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77](https://medium.com/@JesseVig/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1169,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,6,a82ph8,self.MachineLearning,Can someone explain Conditional Batch Normalization?,https://www.reddit.com/r/MachineLearning/comments/a82ph8/can_someone_explain_conditional_batch/,uofT_B,1545341292,[removed],0,1,False,self,,,,,
1170,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,6,a82qok,medium.com,[N] Ex-Google AI Chief Joins Apples Executive Team,https://www.reddit.com/r/MachineLearning/comments/a82qok/n_exgoogle_ai_chief_joins_apples_executive_team/,gwen0927,1545341491,,0,1,False,https://b.thumbs.redditmedia.com/po4aj2KJKVCcoSZ-nHJHzTsYGU20cnTOmllYOi7csFo.jpg,,,,,
1171,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,7,a832bk,self.MachineLearning,[D] Resources and questions on (sequential) generative models,https://www.reddit.com/r/MachineLearning/comments/a832bk/d_resources_and_questions_on_sequential/,mostly_rnd_questions,1545343371,"## Hello,  do you know any good resources (tutorials, papers, lectures) on  generative models? I am mainly interested in sequential generative  models (like the ones used for video prediction [Stochastic Adversarial Video Prediction](https://arxiv.org/abs/1804.01523)).  Is there any formal theory behind them or any resources on practical  applications? Also what is the connection to the Vision and Memory  module proposed in [World Models](https://arxiv.org/pdf/1803.10122.pdf)?  If the authors had used a Convolutional LSTM do you think they would  have gotten the same results? Finally if you wanted to model all  possible states given the current state (get the probability  distribution of p(s'|s) instead of p(s'|s,a)) would the removal of the  action as the RNN input suffice?",0,1,False,self,,,,,
1172,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,7,a83fpx,self.MachineLearning,[R] What is the status about the capsule networks?,https://www.reddit.com/r/MachineLearning/comments/a83fpx/r_what_is_the_status_about_the_capsule_networks/,regularized,1545345557,What are the recent development about the capsule networks? Is there any significant improvement?,30,1,False,self,,,,,
1173,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,8,a83w7g,self.MachineLearning,ML &amp; DL tutorials for noobs,https://www.reddit.com/r/MachineLearning/comments/a83w7g/ml_dl_tutorials_for_noobs/,AndreiD2017,1545348315,[removed],0,1,False,self,,,,,
1174,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,8,a83zls,self.MachineLearning,Are there any machine learning things to increase resolution of audio files?,https://www.reddit.com/r/MachineLearning/comments/a83zls/are_there_any_machine_learning_things_to_increase/,deama15,1545348891,[removed],0,1,False,self,,,,,
1175,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,9,a84lzy,cnn.com,The very important reason these tiny robots are taking pictures of cats,https://www.reddit.com/r/MachineLearning/comments/a84lzy/the_very_important_reason_these_tiny_robots_are/,unsung_unshift,1545353019,,0,1,False,default,,,,,
1176,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a84rkg,self.MachineLearning,Running BigGAN locally,https://www.reddit.com/r/MachineLearning/comments/a84rkg/running_biggan_locally/,witzowitz,1545354167,[removed],0,1,False,self,,,,,
1177,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a84s92,youtube.com,potato cutting machine for chips good thinness shape video show,https://www.reddit.com/r/MachineLearning/comments/a84s92/potato_cutting_machine_for_chips_good_thinness/,fryingmachine,1545354306,,1,1,False,default,,,,,
1178,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a84trq,self.MachineLearning,Why are confidence intervals generally not reported with precision and recall in machine learning publications?,https://www.reddit.com/r/MachineLearning/comments/a84trq/why_are_confidence_intervals_generally_not/,yoganium,1545354621,"Hello all,

&amp;#x200B;

I am a statistician who has recently been asked to be a reviewer on a machine learning paper. One of my comments was that their models calculated precision and recall without reporting the 95% confidence intervals or any form of the margin of error. Their response to my comment was that the confidence intervals are not normally represented in machine learning works (they then went on to cite a Journal of biomedical informatics paper).

&amp;#x200B;

I was wondering if/why this is the case? I feel like having a better understanding of the margin of error around a prediction estimate would give valuable insight into the validity of the model being proposed. When looking on JMLR, ICML, and NIPS most model predictions include at least some form of margin or error in the model estimates.

&amp;#x200B;",0,1,False,self,,,,,
1179,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a84wgc,self.MachineLearning,FAIR releases Nevergrad: An open source tool for derivative-free optimization,https://www.reddit.com/r/MachineLearning/comments/a84wgc/fair_releases_nevergrad_an_open_source_tool_for/,downtownslim,1545355203,"Nevergrad offers an extensive collection of algorithms that do not require gradient computation and presents them in a standard ask-and-tell Python framework. It also includes testing and evaluation tools.

The library includes a wide range of optimizers, such as:

* Differential evolution.
* Sequential quadratic programming.
* FastGA.
* Covariance matrix adaptation.
* Population control methods for noise management.
* Particle swarm optimization.

https://code.fb.com/ai-research/nevergrad/",0,1,False,self,,,,,
1180,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a84wwj,self.MachineLearning,[R] Facebook AI releases Nevergrad: An open source tool for derivative-free optimization,https://www.reddit.com/r/MachineLearning/comments/a84wwj/r_facebook_ai_releases_nevergrad_an_open_source/,downtownslim,1545355296,"Nevergrad offers an extensive collection of algorithms that do not require gradient computation and presents them in a standard ask-and-tell Python framework. It also includes testing and evaluation tools.

The library includes a wide range of optimizers, such as:

* Differential evolution.
* Sequential quadratic programming.
* FastGA.
* Covariance matrix adaptation.
* Population control methods for noise management.
* Particle swarm optimization.


https://code.fb.com/ai-research/nevergrad/",35,1,False,self,,,,,
1181,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a8517r,self.MachineLearning,How to contribute to open-source machine learning libraries?,https://www.reddit.com/r/MachineLearning/comments/a8517r/how_to_contribute_to_opensource_machine_learning/,etmhpe,1545356231,[removed],0,1,False,self,,,,,
1182,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a851jc,self.MachineLearning,Ideas that fizzled out,https://www.reddit.com/r/MachineLearning/comments/a851jc/ideas_that_fizzled_out/,AnvaMiba,1545356297,[removed],0,1,False,self,,,,,
1183,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a85329,github.com,A visual attempt to understand how one-neuron resnet can linearly separate data that a DNN with hidden layers having 2 neurons cannot.,https://www.reddit.com/r/MachineLearning/comments/a85329/a_visual_attempt_to_understand_how_oneneuron/,nivter,1545356617,,0,1,False,default,,,,,
1184,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a854xg,openreview.net,ICLR2019 Results are out,https://www.reddit.com/r/MachineLearning/comments/a854xg/iclr2019_results_are_out/,ajmooch,1545357031,,0,1,False,default,,,,,
1185,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a855g9,openreview.net,[D] ICLR 2019 Results are out,https://www.reddit.com/r/MachineLearning/comments/a855g9/d_iclr_2019_results_are_out/,ajmooch,1545357150,,35,1,False,default,,,,,
1186,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,10,a856oe,self.MachineLearning,[D] Pytorch 1.0 deployment pipeline,https://www.reddit.com/r/MachineLearning/comments/a856oe/d_pytorch_10_deployment_pipeline/,sunrisetofu,1545357413,"Given Pytorch 1.0 update and its support for hybrid front end, onnx support and c++ support. I'm curious as to the pipelines everyone is using to deploy their trained pytorch models in production? ",8,1,False,self,,,,,
1187,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,11,a85erw,self.MachineLearning,[Project] - Trucking question,https://www.reddit.com/r/MachineLearning/comments/a85erw/project_trucking_question/,AppKale,1545359172,"Hi. I am trying to think through a potential problem. I am interested in diesel trucks. What you need to know: 

&amp;#x200B;

Diesel vehicles have a lot more power than what comes stock. 

People buy software (manually written, not a SaaS but a USB stick install kind of deal) for their truck to get more power and MPG. 

&amp;#x200B;

I can't seem to find any data or insight but I am imagining there would be a way to: 

&amp;#x200B;

make that software automated (ie. I know exactly what is going on in the programs, its hard to do but not impossible). 

&amp;#x200B;

The software is adjusting how the ECU on the truck is running the timing/fuel injection/turbo/etc to provide more air flow and fuel. That means more power + better MPG. 

&amp;#x200B;

So, with this said, does anyone know of any data sources where I could access ob2 data from diesel vehicles? Or really any vehicles. I could start with whatever I could get and try to make an algorithm work. 

&amp;#x200B;

I've done some beginner stuff but this is a lot harder of a problem I'm trying to learn (while solving)",5,1,False,self,,,,,
1188,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,11,a85han,openreview.net,[R] Composing Complex Skills by Learning Transition Policies with Proximity Reward Induction,https://www.reddit.com/r/MachineLearning/comments/a85han/r_composing_complex_skills_by_learning_transition/,edwardthegreat2,1545359718,,2,1,False,default,,,,,
1189,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,12,a85zod,self.MachineLearning,[R] The LambdaLoss Framework for Ranking Metric Optimization,https://www.reddit.com/r/MachineLearning/comments/a85zod/r_the_lambdaloss_framework_for_ranking_metric/,fishscaleyu,1545363773,"[https://dl.acm.org/citation.cfm?id=3271784](https://dl.acm.org/citation.cfm?id=3271784)

&amp;#x200B;

Abstract:

How to optimize ranking metrics such as Normalized Discounted Cumulative Gain (NDCG) is an important but challenging problem, because ranking metrics are either flat or discontinuous everywhere, which makes them hard to be optimized directly. Among existing approaches, LambdaRank is a novel algorithm that incorporates ranking metrics into its learning procedure. Though empirically effective, it still lacks theoretical justification. For example, the underlying loss that LambdaRank optimizes for remains unknown until now. Due to this, there is no principled way to advance the LambdaRank algorithm further. In this paper, we present LambdaLoss, a probabilistic framework for ranking metric optimization. We show that LambdaRank is a special configuration with a well-defined loss in the LambdaLoss framework, and thus provide theoretical justification for it. More importantly, the LambdaLoss framework allows us to define metric-driven loss functions that have clear connection to different ranking metrics. We show a few cases in this paper and evaluate them on three publicly available data sets. Experimental results show that our metric-driven loss functions can significantly improve the state-of-the-art learning-to-rank algorithms.",0,1,False,self,,,,,
1190,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,13,a86izs,self.MachineLearning,sound analyzer for automating quality checks?,https://www.reddit.com/r/MachineLearning/comments/a86izs/sound_analyzer_for_automating_quality_checks/,engineheat,1545368302,[removed],0,1,False,self,,,,,
1191,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,14,a86juq,self.MachineLearning,Will I be able to get a job or internship in machine learning with a BA in computer science?,https://www.reddit.com/r/MachineLearning/comments/a86juq/will_i_be_able_to_get_a_job_or_internship_in/,negnatz,1545368496,[removed],0,1,False,self,,,,,
1192,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,14,a86lxu,self.MachineLearning,[P] Machine Learning Experiments Organizer,https://www.reddit.com/r/MachineLearning/comments/a86lxu/p_machine_learning_experiments_organizer/,mlvpj,1545368930,"This library lets you organize TensorFlow machine learning experiments (mostly reinforcement learning). I've recently made a few updates to help create custom charts based on TensorBoard summaries and to add headers to python source codes from experiment results to better organize experiments.

Github Repo: [https://github.com/vpj/lab](https://github.com/vpj/lab)

Getting Started: [http://blog.varunajayasiri.com/ml/lab/lab\_getting\_started.html](http://blog.varunajayasiri.com/ml/lab/lab_getting_started.html)

Custom charts with TensorBoard summaries: [https://github.com/vpj/lab/blob/master/sample\_analytics.ipynb](https://github.com/vpj/lab/blob/master/sample_analytics.ipynb)",0,1,False,self,,,,,
1193,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,15,a871t1,arxiv.org,[R] Deep Paper Gestalt,https://www.reddit.com/r/MachineLearning/comments/a871t1/r_deep_paper_gestalt/,hardmaru,1545372429,,13,1,False,default,,,,,
1194,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,15,a87cgc,ataspinar.com,A guide for using the Wavelet Transform for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a87cgc/a_guide_for_using_the_wavelet_transform_for/,ataspinar,1545374877,,0,1,False,default,,,,,
1195,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,16,a87hi3,self.MachineLearning,How could I approach testing if two product titles reference the same product?,https://www.reddit.com/r/MachineLearning/comments/a87hi3/how_could_i_approach_testing_if_two_product/,caseyscompass,1545376121,[removed],0,1,False,self,,,,,
1196,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,17,a87vbe,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a87vbe/global_machine_learning_market_size_outlook/,uday_03,1545379706,[removed],0,1,False,self,,,,,
1197,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,17,a87wx4,generativeai.net,First complete online course on Generative Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/a87wx4/first_complete_online_course_on_generative/,martinmusiol14,1545380165,,0,1,False,default,,,,,
1198,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,17,a87z75,gengo.ai,How machine learning solutions are transforming financial services: Interview with Data Scientist Iain Brown,https://www.reddit.com/r/MachineLearning/comments/a87z75/how_machine_learning_solutions_are_transforming/,reimmoriks,1545380801,,0,1,False,default,,,,,
1199,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,17,a881qt,analyticsinsight.net,Indian Learners Gravitated Towards Machine Learning Courses On E-learning Platform in 2018 | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/a881qt/indian_learners_gravitated_towards_machine/,analyticsinsight,1545381521,,0,1,False,default,,,,,
1200,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,17,a882hi,openreview.net,Learning to Make Analogies by Contrasting Abstract Relational Structure,https://www.reddit.com/r/MachineLearning/comments/a882hi/learning_to_make_analogies_by_contrasting/,circuithunter,1545381743,,0,1,False,default,,,,,
1201,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,17,a882m7,openreview.net,[R] Learning to Make Analogies by Contrasting Abstract Relational Structure,https://www.reddit.com/r/MachineLearning/comments/a882m7/r_learning_to_make_analogies_by_contrasting/,circuithunter,1545381786,,1,1,False,https://a.thumbs.redditmedia.com/O5xSqPxXQan0eq4XIj_39G9lsyFLtyg3D_81hYNOIr4.jpg,,,,,
1202,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,17,a884b5,sinxloud.com,#4 Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a884b5/4_machine_learning/,skj8,1545382280,,0,1,False,https://a.thumbs.redditmedia.com/czyDJinq8amKCmt0Ud7YdaZExb9ixrdPDygat3WK-94.jpg,,,,,
1203,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,18,a888s9,arxiv.org,Deep Metric Transfer for Label Propagation with Limited Annotated Data,https://www.reddit.com/r/MachineLearning/comments/a888s9/deep_metric_transfer_for_label_propagation_with/,binliu,1545383495,,1,1,False,default,,,,,
1204,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,18,a8894p,self.MachineLearning,Hands-on Deep Learning Course,https://www.reddit.com/r/MachineLearning/comments/a8894p/handson_deep_learning_course/,DowntownDark,1545383591,[removed],0,1,False,self,,,,,
1205,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,18,a889u9,self.MachineLearning,Hi.If anyone is interested in a 1080ti https://www.ebay.com/itm/13700772033,https://www.reddit.com/r/MachineLearning/comments/a889u9/hiif_anyone_is_interested_in_a_1080ti/,Brend0Nn,1545383783,Hi.If anyone is interested in a 1080ti [https://www.ebay.com/itm/13700772033](https://www.ebay.com/itm/13700772033),0,1,False,self,,,,,
1206,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,19,a88k24,programmablebusiness.com,Factors that affect RPA implementation timeframes,https://www.reddit.com/r/MachineLearning/comments/a88k24/factors_that_affect_rpa_implementation_timeframes/,Imaginea123,1545386525,,0,1,False,default,,,,,
1207,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,19,a88o2g,programmablebusiness.com,Whats your policy on Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/a88o2g/whats_your_policy_on_machine_learning/,Imaginea123,1545387599,,0,1,False,default,,,,,
1208,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,19,a88tn0,self.MachineLearning,Looking for graph tool library on Alpine linux,https://www.reddit.com/r/MachineLearning/comments/a88tn0/looking_for_graph_tool_library_on_alpine_linux/,Nasgartam,1545389070,[removed],0,1,False,self,,,,,
1209,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,19,a88vf9,self.MachineLearning,How to make my way in ML,https://www.reddit.com/r/MachineLearning/comments/a88vf9/how_to_make_my_way_in_ml/,Katacleczema,1545389553,[removed],0,1,False,self,,,,,
1210,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,20,a88yiz,self.MachineLearning,Clustering discrete and continuous features using DBSCAN - plots are.... strange...,https://www.reddit.com/r/MachineLearning/comments/a88yiz/clustering_discrete_and_continuous_features_using/,donrondadon,1545390389,[removed],0,1,False,self,,,,,
1211,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,20,a8905p,github.com, Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained,https://www.reddit.com/r/MachineLearning/comments/a8905p/python_examples_of_popular_machine_learning/,trekhleb,1545390811,,0,1,False,default,,,,,
1212,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,20,a8951w,arxiv.org,[R] Deep Metric Transfer for Label Propagation with Limited Annotated Data,https://www.reddit.com/r/MachineLearning/comments/a8951w/r_deep_metric_transfer_for_label_propagation_with/,binliu,1545392061,,5,1,False,default,,,,,
1213,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,21,a89bdk,self.MachineLearning,AWS Mechanical Turk Object Annotation Price,https://www.reddit.com/r/MachineLearning/comments/a89bdk/aws_mechanical_turk_object_annotation_price/,salihkaragoz,1545393620,[removed],0,1,False,self,,,,,
1214,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,21,a89g0t,i.redd.it,Perito Regulador de Sinistro V,https://www.reddit.com/r/MachineLearning/comments/a89g0t/perito_regulador_de_sinistro_v/,JamurGerloff,1545394728,,0,1,False,https://b.thumbs.redditmedia.com/cAllcbmRJfuIJ6j6fTSI1BEo1MdRs_lib_W829qIFBs.jpg,,,,,
1215,MachineLearning,t5_2r3gv,2018-12-21,2018,12,21,21,a89lc4,self.MachineLearning,Future Ethics Of AI Within Cyber Security!,https://www.reddit.com/r/MachineLearning/comments/a89lc4/future_ethics_of_ai_within_cyber_security/,cwsecurity,1545395985,[removed],0,1,False,self,,,,,
1216,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,0,a8ax0c,self.MachineLearning,[D] Did anyone here trained tacotron from nvidia repo? Need help,https://www.reddit.com/r/MachineLearning/comments/a8ax0c/d_did_anyone_here_trained_tacotron_from_nvidia/,hadaev,1545405358,"Repo im talking about [https://github.com/NVIDIA/tacotron2](https://github.com/NVIDIA/tacotron2)

So, I'm very impressed by the new waveglov net and trying to train it for Russian dataset.

But, can't get tacotron part working.

Maybe I do wrong something? Maybe some common tips? One of the invidia devs suggest to remove  silence beginning and end of wavs, it helps a bit, but not much.

My issues on github for more context if my problems are interesting to someone [https://github.com/NVIDIA/tacotron2/issues/111](https://github.com/NVIDIA/tacotron2/issues/111)",0,1,False,self,,,,,
1217,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,0,a8b5g1,self.MachineLearning,Best Way to start Machine learning.,https://www.reddit.com/r/MachineLearning/comments/a8b5g1/best_way_to_start_machine_learning/,1994_shashank,1545406837,[removed],0,1,False,self,,,,,
1218,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,0,a8baj6,self.MachineLearning,[P]Project Malaria Detection using Keras,https://www.reddit.com/r/MachineLearning/comments/a8baj6/pproject_malaria_detection_using_keras/,iArunava,1545407690,"For the last 2 weeks, I have been working on a Malaria Detection using Keras and deployed the model for production!

And finally, the project is complete!!

\*Do Star the repo, if you like the project\*: [https://github.com/iArunava/Malaria-Detection-using-Keras](https://github.com/iArunava/Malaria-Detection-using-Keras)

Video: [https://youtu.be/rJY3lNYKkn0](https://youtu.be/rJY3lNYKkn0)

&amp;#x200B;

References:

1. \[PyImageSearch - Deep Learning and Medical Image Analysis with Keras\]([https://www.pyimagesearch.com/2018/12/03/deep-learning-and-medical-image-analysis-with-keras/](https://www.pyimagesearch.com/2018/12/03/deep-learning-and-medical-image-analysis-with-keras/))",5,1,False,self,,,,,
1219,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,1,a8bugt,commentvulture.blogspot.com,AlphaZero would draw against Carlsen,https://www.reddit.com/r/MachineLearning/comments/a8bugt/alphazero_would_draw_against_carlsen/,lawrence_revell,1545411052,,0,1,False,https://a.thumbs.redditmedia.com/ZZ-0ftFU194PH1Ir0kFneXuN1kJzDcwYNJFvMOhqJ30.jpg,,,,,
1220,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,1,a8bw9x,self.MachineLearning,[D] Scalable Bayesian Inference - NeurIPS 2018,https://www.reddit.com/r/MachineLearning/comments/a8bw9x/d_scalable_bayesian_inference_neurips_2018/,abstractcontrol,1545411355,"Video: https://youtu.be/0HXpnG_WnlI

Overall, David Dunson came off rather optimistic on the prospects of scaling up MCMC. Here is an interesting tidbit about brain regions that is rather long into the talk.

https://youtu.be/0HXpnG_WnlI?t=5899
""Highly-creative individuals have significantly &gt; inter-hemispheric connections"" (slide)
""...It was an interesting side comment, we had a press release on this result and this left brain/right brain hypothesis, this whole thing where left brain people are more mathematical and the right brain people are more creative or whatever, the guy who came up with this - he is deceased now, but his son contacted us as said, well actually his dad ended up not believing the right brain/left brain thing at all and what he ended up believing was much more consistent with our thing which was - it is not right brain/left brain it is more interconnected brains that are more creative. The left brain/right brain thing is just bogus.""

https://youtu.be/0HXpnG_WnlI?t=3387
""In standard Bayesian inference it is assumed that the model is correct."" (slide)
""Small violations in the assumptions **sometimes** have large impacts, particularly in large datasets."" (slide)

Apart from scalability he talked a lot about the need to improve robustness for large datasets. I was not aware that this is a problem so I am highlighting this part. I'd be interested to know whether the same applies for deep learning.

Currently, I am interested in ensembling and there was a tad on this.

https://youtu.be/0HXpnG_WnlI?t=3136
""WASP/PIE is **much** faster than MCMC &amp; highly accurate"" (slide)

https://arxiv.org/abs/1605.04029
Simple, Scalable and Accurate Posterior Interval Estimation

This is the paper which he compared to MCMC.

https://arxiv.org/abs/1112.5016
A Scalable Bootstrap for Massive Data

This is the 'bag of little bootstraps' paper that was mentioned in the talk.",7,1,False,self,,,,,
1221,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,2,a8c0cw,self.MachineLearning,[R] ICLR Controversial Rejection: DeepMind shows SOTA Results on LipReading,https://www.reddit.com/r/MachineLearning/comments/a8c0cw/r_iclr_controversial_rejection_deepmind_shows/,NiceMolegaroo,1545412019,"From the metareviewer:

&gt;This paper describes the development of a large-scale continuous visual speech recognition (lipreading) system, including an audiovisual processing pipeline that is used to extract stabilized videos of lips and corresponding phone sequences from YouTube videos, a deep network architecture trained with CTC loss that maps video sequences to sequences of distributions over phones, and an FST-based decoder that produces word sequences from the phone score sequences. A performance evaluation shows that the proposed system outperforms other models described in the literature, as well as professional lipreaders. A number of ablation experiments compare the performance of the proposed architecture to the previously proposed LipNet and ""Watch, Attend, and Spell"" architectures, explore the performance differences caused by using phone- or character-based CTC models, and some variations on the proposed architecture. This paper was extremely controversial and received a robust discussion between the authors and reviewers, with the primary point of contention being the suitability of the paper for ICLR. All reviewers agree that the quality of the work in the paper is excellent and that the reported results are impressive, but there was strong disagreement on whether or not this was sufficient for an ICLR paper. One reviewer thought so, while the other two reviewers argued that this is insufficient, and that to appear in ICLR the paper either (1) should have focused more on the preparation of the dataset, included public release of the data so other researchers could build on the work, and put forth the V2P model as a (very) strong baseline for the task; or (2) done a more in-depth exploration of the representation learning aspects of the work by comparing phoneme and viseme units and providing more (admittedly costly) ablation experiments to shed more light on what aspects of the V2P architecture lead to the reported improvements in performance. The AC finds the arguments of the two negative reviewers to be persuasive. It is quite clear at this point that many supervised classification tasks (even structured classification tasks like lipreading) can be effectively tackled by a combination of a sufficiently flexible learning architecture and collection of a massive, annotated dataset, and the modeling techniques used in this paper are not new, per se, even if their application to lipreading is. Moreover, if the dataset is not publicly available, it is impossible for anyone else to build on this work. The paper, as it currently stands, would be appropriate in a more applications-oriented venue.

&amp;#x200B;

This paper shows state of the art results on the largest existing visual speech recognition dataset! This rejection seems ridiculous.

[https://openreview.net/forum?id=HJxpDiC5tX](https://openreview.net/forum?id=HJxpDiC5tX)",9,1,False,self,,,,,
1222,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,2,a8cbf8,medium.com,Overview of Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a8cbf8/overview_of_reinforcement_learning/,beluis3d,1545413801,,0,1,False,https://b.thumbs.redditmedia.com/uccbjf6238F9PQTGWScgJU7MoKFLURR12p_mhfA3H2Q.jpg,,,,,
1223,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,2,a8cd28,openreview.net,[R] Learning To Simulate,https://www.reddit.com/r/MachineLearning/comments/a8cd28/r_learning_to_simulate/,StrawberryNumberNine,1545414069,,0,1,False,default,,,,,
1224,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,2,a8cg5h,self.MachineLearning,We're building a machine learning team in Chicago!,https://www.reddit.com/r/MachineLearning/comments/a8cg5h/were_building_a_machine_learning_team_in_chicago/,dianeeee92,1545414561,[removed],0,1,False,self,,,,,
1225,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,3,a8cl5b,self.MachineLearning,Someone using AMD GPUs and ROCm?,https://www.reddit.com/r/MachineLearning/comments/a8cl5b/someone_using_amd_gpus_and_rocm/,SparkMonkay,1545415378,[removed],0,1,False,self,,,,,
1226,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,3,a8cro0,twitter.com,"A Twitter thread, showing some ICLR 2019 acceptance statistic",https://www.reddit.com/r/MachineLearning/comments/a8cro0/a_twitter_thread_showing_some_iclr_2019/,ChocoMoi,1545416415,,0,1,False,default,,,,,
1227,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,3,a8cxad,self.MachineLearning,[N] Biomedical Knowledge Discovery in Networks Through Language/Graphical Models and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a8cxad/n_biomedical_knowledge_discovery_in_networks/,vstuart,1545417304,"For those interested, I have prepared a ""Technical Review"" of subject areas of interest to me with an overall emphasis on natural language processing, machine learning, and graph signal processing (and a particular interest, where appropriate, on molecular genetics/genomics/biomedical interests).

[Biomedical Knowledge Discovery in Networks Through Language/Graphical Models and Machine Learning](https://persagen.com/resources/biokdd-review.html)

  * [Natural Language Processing](https://persagen.com/resources/biokdd-review-nlp.html)
  * [Natural Language Understanding](https://persagen.com/resources/biokdd-review-nlu.html)
  * [Addressing Information Overload](https://persagen.com/resources/biokdd-review-information_overload.html)
  * [Knowledge Graphs](https://persagen.com/resources/biokdd-review-knowledge_graphs.html)
  * [Transfer and Multitask Learning](https://persagen.com/resources/biokdd-review-transfer_learning.html)
  * [Explainable (Interpretable) Models](https://persagen.com/resources/biokdd-review-interpretable_models.html)
  * [In silico Modeling](https://persagen.com/resources/biokdd-review-in_silico_modeling.html)
  * [Biomedical Applications of Machine Learning](https://persagen.com/resources/biokdd-review-bio_ml.html)

... each with embedded content, which is additionally complemented by the following associated items.

  * [Glossary](https://persagen.com/resources/glossary.html)
  * Related [Resources](https://persagen.com/resources.html)
    * [Dataset Descriptions](https://persagen.com/resources/dataset_descriptions.html)
    * [Graph Signal Processing](https://persagen.com/resources/graph_signal_processing.html)
    * ...
",0,1,False,self,,,,,
1228,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,3,a8d0up,self.MachineLearning,arXiv:1812.07697 utterly eviscerates too-good-to-be-true EEG mind reading results,https://www.reddit.com/r/MachineLearning/comments/a8d0up/arxiv181207697_utterly_eviscerates/,singularineet,1545417890,[removed],1,1,False,self,,,,,
1229,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,4,a8d782,self.MachineLearning,[Discussion] What prior work is there on detecting if a figure is made from Excel or PowerPoint?,https://www.reddit.com/r/MachineLearning/comments/a8d782/discussion_what_prior_work_is_there_on_detecting/,cosmic_dozen,1545418915,"Given enough trained data, it should be possible to identify figures that are made with MS Excel or Powerpoint vs those made from Illustrator, LaTeX, ... etc. This is especially true if we limit the classification to figures made with the default font, coloring, and sizing of Excel. I tried searching for this, but unfortunately this is a niche topic and search results are dominated by other uses of Excel. ",7,1,False,self,,,,,
1230,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,4,a8dp57,tech.internal.adroll.com,Questioning Second-Price Auctions in Ad Tech,https://www.reddit.com/r/MachineLearning/comments/a8dp57/questioning_secondprice_auctions_in_ad_tech/,mbw1131,1545421871,,0,1,False,default,,,,,
1231,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,5,a8dzz9,self.bobross,[P] CycleGAN Between Landscape Images and Bob Ross Paintings,https://www.reddit.com/r/MachineLearning/comments/a8dzz9/p_cyclegan_between_landscape_images_and_bob_ross/,manicman1999,1545423714,,0,1,False,https://b.thumbs.redditmedia.com/PKIbuUs77YUS9kJnXwirlNS3YN8WWimcgrAZ_9vlzQY.jpg,,,,,
1232,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,5,a8e3ty,tech.adroll.com,Questioning Second-Price Auctions in Ad Tech,https://www.reddit.com/r/MachineLearning/comments/a8e3ty/questioning_secondprice_auctions_in_ad_tech/,mbw1131,1545424456,,0,1,False,default,,,,,
1233,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,5,a8e582,medium.com,[N] Math Is Forever - Talk with NeurIPS 2018 Best Paper Team,https://www.reddit.com/r/MachineLearning/comments/a8e582/n_math_is_forever_talk_with_neurips_2018_best/,gwen0927,1545424708,,0,1,False,https://b.thumbs.redditmedia.com/0HCjtfIwSPPvDLc4YpQK5gd7TBVf6eSRccPd0dqS5oY.jpg,,,,,
1234,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,5,a8e5em,github.com,tensorflow/privacy is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/a8e5em/tensorflowprivacy_is_a_new_github_repo_by/,sjoerdapp,1545424747,,0,1,False,default,,,,,
1235,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,7,a8f0i8,self.MachineLearning,Is it just me or this professor has no idea what Machine Learning is?,https://www.reddit.com/r/MachineLearning/comments/a8f0i8/is_it_just_me_or_this_professor_has_no_idea_what/,starrymonkey,1545430536,[removed],0,1,False,self,,,,,
1236,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,8,a8fh5x,phys.org,Bullshit or new computing era?,https://www.reddit.com/r/MachineLearning/comments/a8fh5x/bullshit_or_new_computing_era/,darkolorin,1545433799,,0,1,False,default,,,,,
1237,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,8,a8fm34,self.MachineLearning,Information Theory of Deep Learning - Explained,https://www.reddit.com/r/MachineLearning/comments/a8fm34/information_theory_of_deep_learning_explained/,adityashrm21,1545434841,[removed],0,1,False,self,,,,,
1238,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,14,a8ibbv,kdnuggets.com,"Deep Learning Development with Google Colab, TensorFlow, Keras &amp; PyTorch",https://www.reddit.com/r/MachineLearning/comments/a8ibbv/deep_learning_development_with_google_colab/,bexeloj,1545457333,,0,1,False,default,,,,,
1239,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,14,a8idgr,self.MachineLearning,Which conference to attend?,https://www.reddit.com/r/MachineLearning/comments/a8idgr/which_conference_to_attend/,CatGoesWooof,1545457900,[removed],0,1,False,self,,,,,
1240,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,15,a8ii6a,self.MachineLearning,ML Conference's,https://www.reddit.com/r/MachineLearning/comments/a8ii6a/ml_conferences/,anwarshaikh078,1545459148,[removed],0,1,False,self,,,,,
1241,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,15,a8iky3,code.fb.com,"[R] Open sourcing wav2letter++, the fastest state-of-the-art speech system, and flashlight, an ML library going native (FAIR)",https://www.reddit.com/r/MachineLearning/comments/a8iky3/r_open_sourcing_wav2letter_the_fastest/,circuithunter,1545459928,,0,1,False,https://b.thumbs.redditmedia.com/ZzCVTJ1g0hgTIqU_7q9xI76qcOKoiEBhOY-pZDSYuos.jpg,,,,,
1242,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,15,a8iou9,arxiv.org,WEST: Word Encoded Sequence Transducers,https://www.reddit.com/r/MachineLearning/comments/a8iou9/west_word_encoded_sequence_transducers/,zlli0520,1545461059,,1,1,False,default,,,,,
1243,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,16,a8itwf,arxiv.org,WEST: Word Encoded Sequence Transducers,https://www.reddit.com/r/MachineLearning/comments/a8itwf/west_word_encoded_sequence_transducers/,zlli0520,1545462575,,1,1,False,default,,,,,
1244,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,16,a8iubf,self.MachineLearning,tf.map_fn() doesn't work as expected,https://www.reddit.com/r/MachineLearning/comments/a8iubf/tfmap_fn_doesnt_work_as_expected/,ImaginaryAnon,1545462703,[removed],0,1,False,self,,,,,
1245,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,16,a8ix54,self.MachineLearning,[D] Why do we use batches in training?,https://www.reddit.com/r/MachineLearning/comments/a8ix54/d_why_do_we_use_batches_in_training/,darkconfidantislife,1545463604,"So I understand that batches are used to increase parallelism and thus improve utilization in GPUs, but there's significant \*intra-layer\* parallelism to the order of 50 million + withing layers themselves (e.g. see ResNet-50), so why not parallelize across say filters and activation map depth instead of across batch inputs? 

&amp;#x200B;

This has the benefits of reducing batch size meaning maintaining high speeds reducing the needs for oodles of memory and hyperparameter turning and clever parallel large batch training techniques like LARS and so forth.",23,1,False,self,,,,,
1246,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,16,a8ixd4,arxiv.org,[R] WEST: Word Encoded Sequence Transducers,https://www.reddit.com/r/MachineLearning/comments/a8ixd4/r_west_word_encoded_sequence_transducers/,zlli0520,1545463677,,1,1,False,default,,,,,
1247,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,16,a8ixw7,youtube.com,automatic pomegranate deseeding machine india,https://www.reddit.com/r/MachineLearning/comments/a8ixw7/automatic_pomegranate_deseeding_machine_india/,gelgoogmachine,1545463841,,1,1,False,default,,,,,
1248,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,16,a8j0i6,self.MachineLearning,Neural Network: One Output Node vs Three?,https://www.reddit.com/r/MachineLearning/comments/a8j0i6/neural_network_one_output_node_vs_three/,rockitman12,1545464687,"I'm working on my first NN; the kind that you'd see in any YouTube tutorial. Basic stuff.

For this first one, I wrote a simple game for it to play. Within the game, there are three possible moves at any time: Up, Down, Hold (i.e. do nothing).

For the output layer, there are two obvious options for this particular case:

1. Three Output Nodes

 For this option, each action as its own node, with an output range of 0.0-1.0 (where 0 is 0% confidence in taking that action, and 1 is 100%). Ideally there would only be three possible output states:

 1) 1, 0, 0  (i.e. Up)
 2) 0, 1, 0  (i.e. Down)
 3) 0, 0, 1  (i.e. Hold)",0,1,False,self,,,,,
1249,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,16,a8j373,self.MachineLearning,Neural Network: Three Output Nodes vs One?,https://www.reddit.com/r/MachineLearning/comments/a8j373/neural_network_three_output_nodes_vs_one/,rockitman12,1545465554,[removed],0,1,False,self,,,,,
1250,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,17,a8j3jt,analyticsinsight.net,Machine Learning in Light of Gdels Incompleteness Theorems | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/a8j3jt/machine_learning_in_light_of_gdels/,analyticsinsight,1545465645,,0,1,False,default,,,,,
1251,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,17,a8j3q0,self.MachineLearning,"[D] How do you keep track of all the updates in your field? Research, techniques, and programming. Very time consuming to keep up. Considering hiring some tutors.",https://www.reddit.com/r/MachineLearning/comments/a8j3q0/d_how_do_you_keep_track_of_all_the_updates_in/,AdditionalWay,1545465693,"This is a quickly evolving field, and I feel that I spend a lot of time keeping up. I am considering finding some paid help to keep up with it all, to cut down on the time it takes to keep up. 

How do you all keep up? ",43,1,False,self,,,,,
1252,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,17,a8jdmt,i.redd.it,"This is definitely not as impressive as all the amazing threads on this subreddit, but I trained an LSTM network to predict the next data point based on 50 previous data points. Every frame is another epoch of training!",https://www.reddit.com/r/MachineLearning/comments/a8jdmt/this_is_definitely_not_as_impressive_as_all_the/,pocketMAD,1545468920,,0,1,False,default,,,,,
1253,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,18,a8jf0h,self.MachineLearning,[D] Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a8jf0h/d_learning_machine_learning/,safe_lel,1545469375,"Is there any free course out there which takes a pragmatic approach towards explaining the concepts of ML. I tried to finish the course offered by Andrew NG on coursera but I personally felt he didn't thoroughly explain the concepts. 
",10,1,False,self,,,,,
1254,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,18,a8jp01,self.MachineLearning,Video Compression KMeans,https://www.reddit.com/r/MachineLearning/comments/a8jp01/video_compression_kmeans/,stringsaeed,1545472770,[removed],0,1,False,self,,,,,
1255,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,19,a8jvxv,self.MachineLearning,Unable to replicate results with Deep Q Learning in Atari !,https://www.reddit.com/r/MachineLearning/comments/a8jvxv/unable_to_replicate_results_with_deep_q_learning/,Anotherhuman_yeah,1545474982,[removed],0,1,False,self,,,,,
1256,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,20,a8k0ea,amp.theguardian.com,[N] Alexa's advice to 'kill your foster parents' fuels concern over Amazon Echo,https://www.reddit.com/r/MachineLearning/comments/a8k0ea/n_alexas_advice_to_kill_your_foster_parents_fuels/,smokeycheesedip,1545476489,,1,1,False,https://b.thumbs.redditmedia.com/VG-ZLh8bRG1IS3A87K_IXD-WR8hwkooaVyFD1Ds0roU.jpg,,,,,
1257,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,20,a8k4jg,self.MachineLearning,[p] Would it be possible to teach an AI to learn beat saber?,https://www.reddit.com/r/MachineLearning/comments/a8k4jg/p_would_it_be_possible_to_teach_an_ai_to_learn/,teeshapie,1545477866,"I am wanting to try to get an AI machine to learn to play beat saber, is it possible given the game is in virtual reality? 

If, this is possible would you have to train it to not get a game over and reach the end of the song ",4,1,False,self,,,,,
1258,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,21,a8kirh,self.MachineLearning,[D] GAN Failure,https://www.reddit.com/r/MachineLearning/comments/a8kirh/d_gan_failure/,NaxAlpha,1545482415,"Hi guys,

I started my quest for GANs recently. I followed a medium article to build a standalone script for training a GAN. Here is code from original article: [https://github.com/diegoalejogm/gans/blob/master/1.%20Vanilla%20GAN%20PyTorch.ipynb](https://github.com/diegoalejogm/gans/blob/master/1.%20Vanilla%20GAN%20PyTorch.ipynb)

I executed notebook and got promising results in a few iterations (shown below):

https://i.redd.it/u35jxk0fqt521.png

I built my own script as close to original code as possible. At start my result seems promising but after a few iterations my script always diverges xD. Following is result after a few thousand iterations:

&amp;#x200B;

https://i.redd.it/j1kopdr3rt521.png

Just looking for someone who can help me figure out what am I missing:

[https://gist.github.com/NaxAlpha/241f3430fdc1424c84d3f49e52c1f896](https://gist.github.com/NaxAlpha/241f3430fdc1424c84d3f49e52c1f896)

&amp;#x200B;

&amp;#x200B;",0,1,False,https://b.thumbs.redditmedia.com/aIIfgMpI3bcwZCTm2XR9wFtfQLSNiVdEdvuPa5oEfTI.jpg,,,,,
1259,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,22,a8kzs3,towardsdatascience.com,What problems can we solve using AI - An Article,https://www.reddit.com/r/MachineLearning/comments/a8kzs3/what_problems_can_we_solve_using_ai_an_article/,malgamves,1545487086,,0,1,False,default,,,,,
1260,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,23,a8l6jd,github.com,"Recently discovered ignite, what does everyone else think?",https://www.reddit.com/r/MachineLearning/comments/a8l6jd/recently_discovered_ignite_what_does_everyone/,HenryJia,1545488783,,0,1,False,https://b.thumbs.redditmedia.com/Zp2M6cw6R02VDKm3EkabqWBDrP80FU9AcgHst8uwOWY.jpg,,,,,
1261,MachineLearning,t5_2r3gv,2018-12-22,2018,12,22,23,a8l71u,self.MachineLearning,[D] VAE versus WAE/SWAE/CWAE/GAE - advantages of (unnecessary) adding distortion in nondeterministic encoder?,https://www.reddit.com/r/MachineLearning/comments/a8l71u/d_vae_versus_waeswaecwaegae_advantages_of/,jarekduda,1545488909,"There are these two basic philosophies of generative autoencoders:

 1. Original VAE using nondeterministic encoder - randomly choosing latent variable from some Gaussian distribution,

 2. More recent approaches using standard **deterministic encoder-decoder** AE, with **additional optimization to make latent variable sample distribution close to Gaussian distribution**, for example: [WAE](https://arxiv.org/pdf/1711.01558) using Wasserstein metric, [SWAE](https://arxiv.org/pdf/1804.01947) (projected Wasserstein), [CWAE](https://arxiv.org/pdf/1805.09235) (projected KDE L2 norm), [GAE](https://arxiv.org/pdf/1811.04751) (optimize agreement of CDF for radii and distances).

The former philosophy, beside being more complex, artificially adds random distortion (blur) in the encoding-decoding process, what seems unwanted property (?) and is completely unnecessary - is easily avoided in the later philosophy.

However, VAE still seems the default choice (?) - is it only a matter of popularity inertia, or does it have some advantages at least in some tasks?",19,1,False,self,,,,,
1262,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,0,a8lfqn,arxiv.org,[R] DeepMind + German Cancer Research Center: A Probabilistic U-Net for Segmentation of Ambiguous Images,https://www.reddit.com/r/MachineLearning/comments/a8lfqn/r_deepmind_german_cancer_research_center_a/,YesWeGAN,1545490925,,4,1,False,default,,,,,
1263,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,1,a8m6k7,self.MachineLearning,[R] Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/a8m6k7/r_transfer_learning_for_related_reinforcement/,Shani_Gamrian,1545496588,"Check out my paper with Yoav Goldberg (ICLR rejected with 7,7,4): [https://arxiv.org/abs/1806.07377](https://arxiv.org/abs/1806.07377)

**We show that:** 

*  RL-over-pixels really overfits, agents fail completely with small visual changes. 
*  Fine-tuning a trained agent is harder than training from scratch.

**We propose:** 

*  Do transfer learning by analogy: train an unaligned GAN to map between old and new domain, and feeding the agent with the GAN's mapping.  This is much more sample efficient than training from scratch or fine-tuning.
*  A more realistic metric evaluation for unaligned GANs.

**We also have two cool videos:**

Transferring between the original and modified versions of Breakout - [https://youtu.be/4mnkzYyXMn4](https://youtu.be/4mnkzYyXMn4)

Transferring between level 1 and levels 2,3,4 of a car racing game called Road Fighter - [https://youtu.be/khtS-VjpOEA](https://youtu.be/khtS-VjpOEA)

&amp;#x200B;",2,1,False,self,,,,,
1264,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,1,a8maa7,self.MachineLearning,Maths for ML?,https://www.reddit.com/r/MachineLearning/comments/a8maa7/maths_for_ml/,victoriasecretagent,1545497344,[removed],0,1,False,self,,,,,
1265,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,2,a8mfqx,self.MachineLearning,"Why only gaussian mixture models are there, why not Pareto, Beta , etc mixture models?",https://www.reddit.com/r/MachineLearning/comments/a8mfqx/why_only_gaussian_mixture_models_are_there_why/,yash_8141,1545498383,[removed],0,1,False,self,,,,,
1266,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,2,a8mm67,self.MachineLearning,Implementing Deep Painterly Harmonization in Keras,https://www.reddit.com/r/MachineLearning/comments/a8mm67/implementing_deep_painterly_harmonization_in_keras/,aluminumshirts,1545499644,[removed],0,1,False,self,,,,,
1267,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,2,a8mmp2,self.MachineLearning,Simple Deep Q for Doom with Weights,https://www.reddit.com/r/MachineLearning/comments/a8mmp2/simple_deep_q_for_doom_with_weights/,syrios12,1545499751,[removed],0,1,False,self,,,,,
1268,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,2,a8mpuc,self.MachineLearning,[P] RESULTS - Identifying real vs. GAN-generated faces,https://www.reddit.com/r/MachineLearning/comments/a8mpuc/p_results_identifying_real_vs_gangenerated_faces/,aveni0,1545500381,"[Original post](https://www.reddit.com/r/MachineLearning/comments/a34ner/p_can_you_tell_if_these_faces_are_real_or/)

Take the test for yourself! [http://nikola.mit.edu](http://nikola.mit.edu)

Imgur album with results: [https://imgur.com/a/LUR3opq](https://imgur.com/a/LUR3opq)

**tl;dr On average, users misclassify GAN faces as real \~30% of the time, even given 5 seconds to view the image.**

&amp;#x200B;

Hey! Thanks to everyone who took our [online test](http://nikola.mit.edu) to see how well people can identify real vs. GAN-generated faces. Our goal was to measure how often GAN faces fool people today, and to inform the public of the current potential for *automatically-generated* fake news.

We had an amazing turnout from this subreddit with over 6500 responses! Here are the overall results we saw [along with plots](https://imgur.com/a/LUR3opq), as well as some possible issues in our experimental design:

&amp;#x200B;

When asked to classify randomly-ordered fake and real images...

1. **Users' average accuracy** drops from \~68% to \~54% as image exposure time is reduced from 5000ms to 250ms. Random guessing would give an accuracy of 50%.
2. **Users' average false-positive rate** (how often fake images are classified as real) increases from \~30% to \~50% as exposure time is reduced from 5000ms to 250ms.
3. **Experts perform better than non-experts,** especially when eyes are blacked out. This might be because people familiar with GANs can detect artifacts in the background/hair/ears while non-experts can't.
4. Among the experts, **men perform better than women at long exposure times** (&gt;=1000ms) but we also have a large sample imbalance and the gap is much smaller in Experiment 2 (eyes blacked out).
5. It seems that **blacking out eyes from the image does not impact experts' accuracy**, and only affects non-experts. However, due to the fixed ordering of the experiments (see below), it's hard to be confident in comparing Experiment 1 vs Experiment 2.

&amp;#x200B;

For all of our analyses, we assume we're estimating a Bernoulli variable shared by the population and that each user response is an IID event.

&amp;#x200B;

**Experimental Design issues**

* The order of the experiments was fixed
   * Experiment 1 (eyes visible) was always before Experiment 2 (eyes blacked out)
   * The image exposure time was always in the order 5000ms -&gt; 2000ms -&gt; 1000ms -&gt; 500ms -&gt; 250ms
   * There is a visible increase in accuracy from (Exp1, 5000ms) -&gt; (Exp1, 2000ms) and from (Exp1, 5000ms -&gt; Exp2, 5000ms), probably because users were exposed to more images over time and got partial feedback :(
* We didn't explicitly ask users if they were experts/non-experts
   * Non-experts were classmates and friends reached out to prior to posting on Reddit
   * Experts were those who saw our post through r/MachineLearning, and were assumed to be more familiar with GANs
* Sample imbalance
   * 5871 male vs 825 female users among experts
* Some real faces are [famous](http://nikola.mit.edu:5000/images/real/real-41.jpg) [people](http://nikola.mit.edu:5000/images/real-blur/real-blur-63.jpg), and easy to recognize
   * We tried our best to remove the obvious ones but clearly we don't know our celebrities ;P

&amp;#x200B;

**Future work/Shout-outs**

We've updated the online test to address the flaws described above.

Concidentially, just a week after we posted this experiment, the same team from NVIDIA released an *even better* GAN for generating faces ([Video](https://stylegan.xyz/video), [Paper](https://stylegan.xyz/paper)). Perhaps in the future we can repeat this test with StyleGAN images and see how much harder it is :)

If you want tips on how to recognize artifacts in GAN faces, check out [this blog post](https://medium.com/@kcimc/how-to-recognize-fake-ai-generated-images-4d1f6f9a2842).

&amp;#x200B;

\~\~ Thanks again for helping us with our class project, and we hope you had fun! \~\~",14,1,False,self,,,,,
1269,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,3,a8mz9k,self.MachineLearning,Im new to ML and programming but want to learn.,https://www.reddit.com/r/MachineLearning/comments/a8mz9k/im_new_to_ml_and_programming_but_want_to_learn/,TacoCrafter1,1545502190,[removed],0,1,False,self,,,,,
1270,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,4,a8ngy5,self.MachineLearning,[D] AISTATS 2019 notifications are out,https://www.reddit.com/r/MachineLearning/comments/a8ngy5/d_aistats_2019_notifications_are_out/,bronzestick,1545505672,Just received mine. Good luck everyone!,18,1,False,self,,,,,
1271,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,4,a8nj03,self.MachineLearning,Suggestion for new macbook pro 2018 model.,https://www.reddit.com/r/MachineLearning/comments/a8nj03/suggestion_for_new_macbook_pro_2018_model/,iqmpast,1545506063,"I am thinking of buying the new macbook pro version 2018 with the specs of i5 8th gen(quad core), 8gb ram, and 256 gb ssd. Please tell me whether it is good for machine learning models?? or i should go for an egpu??i fyes, then please suggest some..

&amp;#x200B;",0,1,False,self,,,,,
1272,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,4,a8nnt2,self.MachineLearning,"best way to create a training set for character detection in clothing, should I crop pics to just #/letters?",https://www.reddit.com/r/MachineLearning/comments/a8nnt2/best_way_to_create_a_training_set_for_character/,_thankunext,1545507020,[removed],0,1,False,self,,,,,
1273,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,4,a8nqn3,openreview.net,[R][ICLR Oral] Pay Less Attention with Lightweight and Dynamic Convolutions,https://www.reddit.com/r/MachineLearning/comments/a8nqn3/riclr_oral_pay_less_attention_with_lightweight/,downtownslim,1545507571,,3,1,False,default,,,,,
1274,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,5,a8o7zx,self.MachineLearning,Under the Hood with Reinforcement Learning  Understanding Basic RL Models,https://www.reddit.com/r/MachineLearning/comments/a8o7zx/under_the_hood_with_reinforcement_learning/,andrea_manero,1545510987,[removed],0,1,False,self,,,,,
1275,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,6,a8ohle,self.MachineLearning,Convert regular Video to greyscale 32x32 footage.,https://www.reddit.com/r/MachineLearning/comments/a8ohle/convert_regular_video_to_greyscale_32x32_footage/,SuperJMan64,1545512854,[removed],0,1,False,self,,,,,
1276,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,7,a8p0l8,arxiv.org,[P] Training on the test set? An analysis of Spampinato et al. [31],https://www.reddit.com/r/MachineLearning/comments/a8p0l8/p_training_on_the_test_set_an_analysis_of/,schmidhubernet,1545516568,,82,1,False,default,,,,,
1277,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,9,a8q0y8,medium.com,Make NeurIPS Neural Again  Johannes Rieke  Medium,https://www.reddit.com/r/MachineLearning/comments/a8q0y8/make_neurips_neural_again_johannes_rieke_medium/,jrieke,1545524079,,0,1,False,default,,,,,
1278,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,9,a8q8az,self.MachineLearning,Few questions and my experience with MeanShift and Active Directory logs - Scikit Learn (python),https://www.reddit.com/r/MachineLearning/comments/a8q8az/few_questions_and_my_experience_with_meanshift/,Phoenix_Rageburn,1545525625,[removed],0,1,False,https://b.thumbs.redditmedia.com/9XOsJwKwotyj-LMA2R-HNLjssPW6HGyvSTI2Er8_9Xo.jpg,,,,,
1279,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,10,a8qfa0,self.MachineLearning,[Course Review/ Feedback] Udactiy NanoDegree in NLP,https://www.reddit.com/r/MachineLearning/comments/a8qfa0/course_review_feedback_udactiy_nanodegree_in_nlp/,aB9s,1545527142,[removed],0,1,False,self,,,,,
1280,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,10,a8qoxo,self.MachineLearning,Tendency to underforecast for sparse data (keras/ConvNet),https://www.reddit.com/r/MachineLearning/comments/a8qoxo/tendency_to_underforecast_for_sparse_data/,dakpanWTS,1545529410,[removed],0,1,False,self,,,,,
1281,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,10,a8qqdo,link.medium.com,"EMNLP18: Cookie Monsters, Blackboxes, and Document-Level NMT",https://www.reddit.com/r/MachineLearning/comments/a8qqdo/emnlp18_cookie_monsters_blackboxes_and/,Thmsrey,1545529747,,0,1,False,default,,,,,
1282,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,13,a8rxr7,self.MachineLearning,"Creating a Denoising Autoencoder, and some features are larger than others",https://www.reddit.com/r/MachineLearning/comments/a8rxr7/creating_a_denoising_autoencoder_and_some/,coolguy985,1545540331,[removed],0,1,False,self,,,,,
1283,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,14,a8sahg,self.MachineLearning,How does GAN in text to image conversion create image ?,https://www.reddit.com/r/MachineLearning/comments/a8sahg/how_does_gan_in_text_to_image_conversion_create/,Bishwa12,1545543783,[removed],0,1,False,self,,,,,
1284,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,16,a8suj4,self.MachineLearning,How does GAN work in text to image conversion ?,https://www.reddit.com/r/MachineLearning/comments/a8suj4/how_does_gan_work_in_text_to_image_conversion/,Bishwa12,1545549732,[removed],0,1,False,self,,,,,
1285,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,16,a8t03r,self.MachineLearning,Ethics in ML,https://www.reddit.com/r/MachineLearning/comments/a8t03r/ethics_in_ml/,deartiste,1545551657,Does this community explore all aspects of ML (such as ethics) or primarily research/development? Thanks!,0,1,False,self,,,,,
1286,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,17,a8t6tc,self.MachineLearning,Tracking and Predicting Vessel Movements in Time Series,https://www.reddit.com/r/MachineLearning/comments/a8t6tc/tracking_and_predicting_vessel_movements_in_time/,Khaled_Arja,1545553980,"  

I have a little project to do and I am having a little difficulty to draw the plan before starting.

I have a database consisting of: **Vessel name, date time, latitude, longitude**, it's all about tracking the vessel movement in the space-time.

the data: [https://www.dropbox.com/s/85a6nj3v19jw9b6/test.png?dl=0](https://www.dropbox.com/s/85a6nj3v19jw9b6/test.png?dl=0)

In that case of having a time series, which model you would apply for the problem in order to predict the vessel's next destination? the moment when the vessel enters/exits a port?

Should I use ARIMA models instead of machine learning algorithms?

and last question: if I don't have the ports names, how can I manually define the ports based on the positions observation?

I would love to have some discussions and argue to see the different point of views, and thank you!",0,1,False,self,,,,,
1287,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,17,a8t82z,self.MachineLearning,Can RNNs be better hypernetworks?,https://www.reddit.com/r/MachineLearning/comments/a8t82z/can_rnns_be_better_hypernetworks/,botperson,1545554464,"I've been working on a hypernetwork that outputs a neural network that takes in 25088 values as inputs, has a 512 neuron hidden layer, and has a 1 neuron output.

&amp;#x200B;

The generation of the parameters for the last layer is simple - a hypernetwork only has to output 513 values (512 weights and 1 bias). All it requires is 2 hidden layers and 513 neurons with linear activations as outputs. The hypernetwork's input size is also 25088, like the network it outputs.

&amp;#x200B;

The problem is when the second-to-last layer (or the hidden layer) has to be generated by the hypernetwork. The second layer will have 25088  512 + 512 parameters to be outputted, which is 12,845,568 parameters - almost 13 million.

&amp;#x200B;

This is absolutely enormous. What I ended up doing was dividing these 13 million parameters into 1000 different parts. I would then make 1000 separate neural networks to handle each of these 1000 parts. So, one neural network only had to output around 13,000 values.

&amp;#x200B;

This worked, but training 1000 neural networks on 100 training samples (each training sample having 25088 values as an input and one-thousandth of a neural network as an output) took one week to train and took up 1.5 Terabytes of storage. I wish to repeat the process because the hypernetwork technique was extremely effective in a problem that I was attempting to solve.

&amp;#x200B;

More recently, I read [a blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy where he talks about how RNNs can be used for non-temporal, and even non-sequential (in some sense) data. I'm not too experienced with RNNs. I am hoping that I could use 1 relational neural network that outputs 25089 values. These values are the weights and the bias of one of the 512 neurons of the hidden layer. The RNN could then switch to the second neuron and output those values. After that, it could repeat the process for all the 512 neurons. Each batch of outputs is a member of a 'sequence' of sorts. Instead of dividing the task amongst many networks, can one RNN be used to generate the values for each of the neurons one at a time?

&amp;#x200B;

Again, I am not very experienced with RNNs and if my question is too vague, I'll clarify it.",0,1,False,self,,,,,
1288,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,18,a8th4o,self.MachineLearning,[D] What are best papers regarding GANs for 2018 you read?,https://www.reddit.com/r/MachineLearning/comments/a8th4o/d_what_are_best_papers_regarding_gans_for_2018/,mhachem-reddit,1545557734,,22,1,False,self,,,,,
1289,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,18,a8tjt5,self.MachineLearning,Leasing of online computer services for running neural networks,https://www.reddit.com/r/MachineLearning/comments/a8tjt5/leasing_of_online_computer_services_for_running/,littlefiredragon,1545558713,[removed],0,1,False,self,,,,,
1290,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,18,a8tjx2,self.MachineLearning,Deep Learning: Classifying Unknown Classes &amp; Extracting Their Features,https://www.reddit.com/r/MachineLearning/comments/a8tjx2/deep_learning_classifying_unknown_classes/,Julian_Kewoski,1545558753,[removed],0,1,False,self,,,,,
1291,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,19,a8topp,self.MachineLearning,ML based attendance system,https://www.reddit.com/r/MachineLearning/comments/a8topp/ml_based_attendance_system/,zaerrc,1545560354,[removed],0,1,False,self,,,,,
1292,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,21,a8u7tz,smadrid056.tumblr.com,Basic Concepts of Machine Learning!,https://www.reddit.com/r/MachineLearning/comments/a8u7tz/basic_concepts_of_machine_learning/,smadrid056,1545567050,,0,1,False,default,,,,,
1293,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,22,a8uk39,codecampanion.blogspot.com,Intro to Keras U-Net - Nuclei in divergent images to advance medical discovery,https://www.reddit.com/r/MachineLearning/comments/a8uk39/intro_to_keras_unet_nuclei_in_divergent_images_to/,AshishKhuraishy,1545570815,,0,1,False,default,,,,,
1294,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,22,a8usyq,self.MachineLearning,Analyzing r/MachineLearning 2018 posts with Graphext unsupervised NLP algorithms,https://www.reddit.com/r/MachineLearning/comments/a8usyq/analyzing_rmachinelearning_2018_posts_with/,victorianoi,1545573245,[removed],0,1,False,self,,,,,
1295,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,23,a8uvuq,medium.freecodecamp.org,Want to learn neural networks? Heres a free Brain.js course! Merry Christmas!,https://www.reddit.com/r/MachineLearning/comments/a8uvuq/want_to_learn_neural_networks_heres_a_free/,mrborgen86,1545573949,,0,1,False,https://b.thumbs.redditmedia.com/LWZsNrX_cmipzXohQQvEfo46f4fGhzKMgPQGCXKNkIs.jpg,,,,,
1296,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,23,a8uxl2,self.MachineLearning,Analyzing r/MachineLearning 2018 posts with Graphext unsupervised NLP algorithms,https://www.reddit.com/r/MachineLearning/comments/a8uxl2/analyzing_rmachinelearning_2018_posts_with/,victorianoi,1545574386,[removed],0,1,False,self,,,,,
1297,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,23,a8v04l,self.MachineLearning,Analyzing r/MachineLearning 2018 posts with Graphext unsupervised NLP algorithms,https://www.reddit.com/r/MachineLearning/comments/a8v04l/analyzing_rmachinelearning_2018_posts_with/,victorianoi,1545575011,[removed],0,1,False,https://b.thumbs.redditmedia.com/jAo4-IDbpolhO7qGwN7HLWuxR1AoztHuGV6dNqb0K3A.jpg,,,,,
1298,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,23,a8v5k1,self.MachineLearning,[Project] [P] Analyzing r/MachineLearning 2018 posts with Graphext unsupervised NLP algorithms,https://www.reddit.com/r/MachineLearning/comments/a8v5k1/project_p_analyzing_rmachinelearning_2018_posts/,victorianoi,1545576320,"&amp;#x200B;

[2509 posts from r\/MachineLearning in 2018 clustered with Graphext ](https://i.redd.it/hrwpz413i1621.jpg)

We at **Graphext** ( [@graphext](https://twitter.com/graphext) ) use word2vec + dimensionality reduction + network algorithms to cluster all type of data, from text to images to numerical and categorical vectors to spot unsupervised patterns, among many other things in data science. I scraped all the posts in [r/MachineLearning](https://www.reddit.com/r/MachineLearning) from [@slashml](https://twitter.com/slashml) uploaded to Graphext and found all these different narratives in the community.

Each node is a post, we connect similar posts talking about similar things, then we form a network, calculate clusters with Louvain algorithm, and find what terms characterize them more. The size of the nodes is the number of retweets they got.

Another day I would like to compare the evolution of each topic over the years to see what things are becoming more trending lately :)",0,1,False,self,,,,,
1299,MachineLearning,t5_2r3gv,2018-12-23,2018,12,23,23,a8v8ya,self.MachineLearning,[Project] [P] Analyzing r/MachineLearning 2018 posts with Graphext unsupervised NLP algorithms,https://www.reddit.com/r/MachineLearning/comments/a8v8ya/project_p_analyzing_rmachinelearning_2018_posts/,victorianoi,1545577107,"&amp;#x200B;

[2509 posts from r\/MachineLearning in 2018 clustered with Graphext](https://i.redd.it/94jirwc3j1621.jpg)

We at **Graphext** ( [@graphext](https://twitter.com/graphext) ) use word2vec + dimensionality reduction + network algorithms to cluster all type of data, from text to images to numerical and categorical vectors to spot unsupervised patterns, among many other things in data science. I scraped all the posts in [r/MachineLearning](https://www.reddit.com/r/MachineLearning) from [@slashml](https://twitter.com/slashml) uploaded to Graphext and found all these different narratives in the community.

Each node is a post, we connect similar posts talking about similar things, then we form a network, calculate clusters with Louvain algorithm, and find what terms characterize them more. The size of the nodes is the number of retweets they got.

Another day I would like to compare the evolution of each topic over the years to see what things are becoming more trending lately :)",17,1,False,self,,,,,
1300,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,0,a8vaqb,timdettmers.com,A Full Hardware Guide to Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a8vaqb/a_full_hardware_guide_to_deep_learning/,etca2z,1545577507,,0,1,False,https://a.thumbs.redditmedia.com/QFyw55f3lLd8bZN7gTsKZNsjfyzJl-x4rmgQkuOOlZ0.jpg,,,,,
1301,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,0,a8vewo,self.MachineLearning,[D] Comparing word similarities from different corpora,https://www.reddit.com/r/MachineLearning/comments/a8vewo/d_comparing_word_similarities_from_different/,ElPresidente408,1545578448,"I'm interested if anyone has seen additional research around using word embeddings to measure the similarities of words in different corpora. For example, we could have two sets of embeddings trained in separate spaces (ie. conservative-leaning vs. liberal-leaning texts). By comparing the relationships we could possibly see words that have a different than expected representation. One example I found comes from [this paper](http://www.aclweb.org/anthology/P15-2108), but it's the only similar example I could find.

From my understanding of the paper, they learn a linear remapping function from one space to another using the top 1,000 words. Presumably these tend to be ""neutral"" words like pronouns, articles, stop words, etc.  and help define a ground truth. This is different than training jointly on the two spaces where a word like ""immigration"" in a jointly learned embedding space would have different neighbors than if it were learned in two separate spaces (using my politics example). 

My end goal really is to quantifiably find words where there is a significant divergence in context/usage. Curious what research anyone may have come across.",6,1,False,self,,,,,
1302,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,2,a8wdye,self.MachineLearning,What are best papers regarding segmentation for 2018 you read ?,https://www.reddit.com/r/MachineLearning/comments/a8wdye/what_are_best_papers_regarding_segmentation_for/,muneeb2405,1545585489,[removed],0,1,False,self,,,,,
1303,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,2,a8whac,returnbyte.com,Free Online tool to remove the background of any photos using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a8whac/free_online_tool_to_remove_the_background_of_any/,febinmathew,1545586118,,0,1,False,default,,,,,
1304,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,2,a8whi9,self.MachineLearning,"[D] Conversation with Juergen Schmidhuber on Godel Machines, Meta-Learning, and LSTMs",https://www.reddit.com/r/MachineLearning/comments/a8whi9/d_conversation_with_juergen_schmidhuber_on_godel/,UltraMarathonMan,1545586158,"This is a conversation with Juergen Schmidhuber, co-creator of long short-term memory networks (LSTMs) which are used in billions of devices today for speech recognition, translation, and much more. Over 30 years, he has proposed a lot of interesting, out-of-the-box ideas in artificial intelligence including a formal theory of creativity.

Podcast version: [https://lexfridman.com/juergen-schmidhuber](https://lexfridman.com/juergen-schmidhuber)

YouTube video: [https://www.youtube.com/watch?v=3FIo6evmweo](https://www.youtube.com/watch?v=3FIo6evmweo)",25,1,False,self,,,,,
1305,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,2,a8wqes,self.MachineLearning,Best day of my life &lt;3,https://www.reddit.com/r/MachineLearning/comments/a8wqes/best_day_of_my_life_3/,Akainu18448,1545587820,[removed],0,1,False,self,,,,,
1306,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,3,a8wrmb,self.MachineLearning,[D] Handling condition inputs in Conditional GANs,https://www.reddit.com/r/MachineLearning/comments/a8wrmb/d_handling_condition_inputs_in_conditional_gans/,Natsu6767,1545588051,"What are some ways to feed the conditions to the discriminator in a Conditional GAN? The method described in the original paper, [""Conditional Generative Adversarial Nets""](https://arxiv.org/abs/1411.1784) is for MLPs and is basically just concatenating the conditions with the flattened input image. 

However, something like that won't work if using only convolutional layers in your network (like in a DCGAN). One method to feed the conditions, in this case, would be to tile (repeat multiple times) the condition vector to the same dimensions as that of the input image and then concatenate it with the image along the channel dimension. For example, say the condition is a **10-d vector** and the images are **3 x 64 x 64**. You would turn the condition to a tensor of dimension **10 x 64 x 64**  and on concatenating the input to the discriminator will be **13 x 64 x 64**.

However, I believe that this tiling method to transform the condition to a tensor isn't good and is sort of just a ""hack"". Has anyone come across an interesting way to input the conditions?

&amp;#x200B;

*P.S. I think the problem is only with giving the input to the discriminator. For the generator, just concatenating with the noise vector (which is also just a single dimensional tensor like the conditions) seems to be fine.*",5,1,False,self,,,,,
1307,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,3,a8wve6,self.MachineLearning,What Machine Learning framework do YOU recommend?,https://www.reddit.com/r/MachineLearning/comments/a8wve6/what_machine_learning_framework_do_you_recommend/,PlymouthPolyHecknic,1545588746,[removed],0,1,False,self,,,,,
1308,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,4,a8xgjl,youtube.com,"MIT AI: Godel Machines, Meta-Learning, and LSTMs (Juergen Schmidhuber)",https://www.reddit.com/r/MachineLearning/comments/a8xgjl/mit_ai_godel_machines_metalearning_and_lstms/,Mynameis__--__,1545592918,,0,1,False,default,,,,,
1309,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,4,a8xjh0,self.MachineLearning,[D] I'm tired of reading results-oriented papers. What are good papers or sources of papers more focused on the emerging theory of machine learning?,https://www.reddit.com/r/MachineLearning/comments/a8xjh0/d_im_tired_of_reading_resultsoriented_papers_what/,maroonedscientist,1545593492,"As the title says, so many papers I read now seem to be demonstrating a result, with relatively little discussion about the network design decisions. I'm interested to find papers more focused on the theory of network design, network operation, training methods, dataset requirements, provability, network pruning, etc... 

Any advice on specific papers or sources of papers that would meet these needs?",150,1,False,self,,,,,
1310,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,5,a8xwd5,github.com,PHP has a New Machine Learning Library,https://www.reddit.com/r/MachineLearning/comments/a8xwd5/php_has_a_new_machine_learning_library/,andrewdalpino,1545595973,,0,1,False,https://b.thumbs.redditmedia.com/by7lubrvHOF6cdfQJN_OvEy4QBffjyJrAMjnhfO1tKQ.jpg,,,,,
1311,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,5,a8xzgm,github.com,[News] PHP has a New Machine Learning Library,https://www.reddit.com/r/MachineLearning/comments/a8xzgm/news_php_has_a_new_machine_learning_library/,andrewdalpino,1545596576,,0,1,False,https://b.thumbs.redditmedia.com/by7lubrvHOF6cdfQJN_OvEy4QBffjyJrAMjnhfO1tKQ.jpg,,,,,
1312,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,5,a8y7fe,self.MachineLearning,[D] Fleshing out Stanley's ultra-compact compressed representation of large neural networks.,https://www.reddit.com/r/MachineLearning/comments/a8y7fe/d_fleshing_out_stanleys_ultracompact_compressed/,moschles,1545598141,"A recent publication appeared authored by Kenneth Stanley, Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman,  and Jeff Clune.   Titled : *Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning*

While the paper reports almost completely on a Genetic Algorithm,  it also reports something which should be of great interest to anyone who uses neural networks in Machine Learning.

Only described in the index, the representation does not store the weights of the network at all, but instead stores an ordered sequence of  RNG seeds,  each of which are 28bits wide.  The authors claimed this method could be used to represent networks with approximately 4 million weights, after de-compression and decoding.   In several places, the compression is given as 10,000:1 

A portion of the paper is given :

https://i.imgur.com/ZkNfX0D.png

It is not clear from this description how new networks with slight mutations can be created from old ones.  This would have to be done eventually since the paper describes searching nearby an existing vector of weights in a ""tight cluster"".   There would have to be slight backtracking to an earlier form, reproduced and then mutated again with a new tau RNG seed near the end portion of the cascade of mutations.  (In other words,)  it is not clear how this decoding method could reliably produce children who are ""very near"" but  slightly different than a parent network. 

Your thoughts? 
",8,1,False,self,,,,,
1313,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,5,a8y8fj,self.MachineLearning,Statistics foundations,https://www.reddit.com/r/MachineLearning/comments/a8y8fj/statistics_foundations/,diego-user,1545598341,[removed],0,1,False,self,,,,,
1314,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,6,a8yaro,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 53,https://www.reddit.com/r/MachineLearning/comments/a8yaro/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1545598806,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/blackbearx3: [Variational Learning of Inducing Variables in Sparse Gaussian Processes](http://proceedings.mlr.press/v5/titsias09a.html)

/u/wassname: [Non-Delusional Q-Learning and Value-Iteration](https://papers.nips.cc/paper/8200-non-delusional-q-learning-and-value-iteration.pdf)

Besides that, there are no rules, have fun.",18,1,False,self,,,,,
1315,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,6,a8ysf4,self.MachineLearning,My GPU is chugging fine while i post this,https://www.reddit.com/r/MachineLearning/comments/a8ysf4/my_gpu_is_chugging_fine_while_i_post_this/,MLCrazyDude,1545602318,[removed],0,1,False,https://b.thumbs.redditmedia.com/YVSSh3SDQ_I2CaAd6VedJ4QbGtcgDjJWN2gcz_0grMw.jpg,,,,,
1316,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,9,a903y2,self.MachineLearning,What's the technique behind salesforce Einstein Voice? How to find related paper? Name entity?,https://www.reddit.com/r/MachineLearning/comments/a903y2/whats_the_technique_behind_salesforce_einstein/,ejiido,1545612262,https://www.salesforceben.com/introducing-einstein-voice-now-were-really-talking-ai/,0,1,False,self,,,,,
1317,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,10,a90fz3,link.medium.com,My first Data Science Interview : 3 problem statements to ponder on!,https://www.reddit.com/r/MachineLearning/comments/a90fz3/my_first_data_science_interview_3_problem/,harveenchadha,1545614929,,0,1,False,default,,,,,
1318,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,11,a90u3t,self.MachineLearning,[D] What makes a good text-to-speech dataset?,https://www.reddit.com/r/MachineLearning/comments/a90u3t/d_what_makes_a_good_texttospeech_dataset/,Kohomology,1545618096,"Subtitle: What causes the ""speaking into a fan"" warble in Text-To-Speech models? 

I'm training a TTS model (https://github.com/Kyubyong/dc_tts) on a medium-quality custom dataset (speech from TV news), and a professionally-recorded dataset. 

The results from training on the TV news dataset have the distinctive ""speaking into a fan"" warble that you can hear in samples from many open-source models.

Examples of the warbling I'm talking about:
https://soundcloud.com/kyubyong-park/sets/tacotron_lj_200k

and to a lesser extent:
https://keithito.github.io/audio-samples/


I can't seem to get rid of the warbling when training on the TV news data, but the warbling is unnoticeable when I train on the professionally-recorded data. So I'm leaning towards assuming that this is a problem with the training data that seems to be present in many open datasets as well (the two samples posted above were trained on the LJ speech dataset). But just listening to the ""bad"" and ""good"" datasets by ear, I can't tell what the problem is.

Hence my question:

What subtle problems can there be in a text-to-speech dataset? I am particularly interested in the cause of the warbling mentioned above, but would be glad to learn about unrelated problems as well.

",3,1,False,self,,,,,
1319,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,11,a912zz,self.MachineLearning,Consuming Papers in Audio Format,https://www.reddit.com/r/MachineLearning/comments/a912zz/consuming_papers_in_audio_format/,jfishersolutions,1545620177,[removed],0,1,False,self,,,,,
1320,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,13,a91t98,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a91t98/global_machine_learning_market_size_outlook/,hema8ent,1545626285,[removed],0,1,False,self,,,,,
1321,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,15,a92dos,self.MachineLearning,[Question] Information theory + AI,https://www.reddit.com/r/MachineLearning/comments/a92dos/question_information_theory_ai/,kazemiro,1545631232,[removed],0,1,False,self,,,,,
1322,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,17,a93dxf,chitramachineries.com,PASTE PREPERATION VESSEL Lab Model | Chitra Machineries | Pharmaceuticals Industries | Food Industries | Cosmetic Industries,https://www.reddit.com/r/MachineLearning/comments/a93dxf/paste_preperation_vessel_lab_model_chitra/,chitramachineries,1545641335,,0,1,False,default,,,,,
1323,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,18,a93ha7,learntek.org,Machine Learning and Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/a93ha7/machine_learning_and_pattern_recognition/,jahnavi1209,1545642371,,0,1,False,default,,,,,
1324,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,18,a93ils,self.MachineLearning,LSTM-RNN Tutorial with Stock Prices Prediction Code Example,https://www.reddit.com/r/MachineLearning/comments/a93ils/lstmrnn_tutorial_with_stock_prices_prediction/,obsezer,1545642792,[removed],0,1,False,self,,,,,
1325,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,19,a93z0u,self.MachineLearning,State of Hebbian Learning Research,https://www.reddit.com/r/MachineLearning/comments/a93z0u/state_of_hebbian_learning_research/,avrock123,1545647874,[removed],0,1,False,self,,,,,
1326,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,20,a94647,twitter.com,Difference chaining the learning rate will do on the accuracy.,https://www.reddit.com/r/MachineLearning/comments/a94647/difference_chaining_the_learning_rate_will_do_on/,Tolure,1545650032,,0,1,False,default,,,,,
1327,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,20,a9467c,github.com,[P] Ergonomic ndarrays and deep learning in a compiled language with char-rnn example,https://www.reddit.com/r/MachineLearning/comments/a9467c/p_ergonomic_ndarrays_and_deep_learning_in_a/,Karyo_Ten,1545650060,,1,1,False,https://b.thumbs.redditmedia.com/jEjuY2Ch9WDFts8dGUxXN28rhquXlGLFvYlJZXhAq_A.jpg,,,,,
1328,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,20,a9472z,i.redd.it,Remote controlled demolition robot,https://www.reddit.com/r/MachineLearning/comments/a9472z/remote_controlled_demolition_robot/,staypositiveworkhard,1545650342,,0,1,False,default,,,,,
1329,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,20,a949qu,twitter.com,Difference changing the learning rate will do on the accuracy,https://www.reddit.com/r/MachineLearning/comments/a949qu/difference_changing_the_learning_rate_will_do_on/,Tolure,1545651129,,0,1,False,https://a.thumbs.redditmedia.com/FFydGMaUo7N3d3PopUBSZuIAPe5ajaUF0c1sjpg12D8.jpg,,,,,
1330,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,22,a94zhg,medium.com,Exciting news! Text Classification with State of the Art NLP Library  Flair,https://www.reddit.com/r/MachineLearning/comments/a94zhg/exciting_news_text_classification_with_state_of/,thelole,1545658187,,0,1,False,default,,,,,
1331,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,23,a957vm,gengo.ai,AI and machine learning conferences in 2019,https://www.reddit.com/r/MachineLearning/comments/a957vm/ai_and_machine_learning_conferences_in_2019/,reimmoriks,1545660261,,0,1,False,default,,,,,
1332,MachineLearning,t5_2r3gv,2018-12-24,2018,12,24,23,a95i8j,self.MachineLearning,"Do you know guys why tensorflow gives me this error ""module 'tensorflow' has no attribute 'lite'""?",https://www.reddit.com/r/MachineLearning/comments/a95i8j/do_you_know_guys_why_tensorflow_gives_me_this/,Vinceeeent,1545662640,"I'm trying to convert my trained model in android using tensorflow lite but while trying to convert my model it gives me this error "" module 'tensorflow' has no attribute 'lite'  """,0,1,False,self,,,,,
1333,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,1,a966wh,self.MachineLearning,Hands-on AI Agents Developer Guide with code on X-mas sale! ($5),https://www.reddit.com/r/MachineLearning/comments/a966wh/handson_ai_agents_developer_guide_with_code_on/,ai_energy,1545667810,"[Hands-on AI Agents Developer Guide with code on X-mas sale! ($5)](https://www.packtpub.com/big-data-and-business-intelligence/hands-intelligent-agents-openai-gym?utm_source=Praveen%20Palanisamy&amp;utm_medium=referral&amp;utm_campaign=9781788836579)

""If you are someone wanting to get a head start in this direction of building intelligent agents to solve problems and you are looking for a structured yet concise and hands-on approach to follow, you will enjoy this book and the [code repository](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym). 

&amp;#x200B;

![img](4yh89wvf19621)

The chapters in this book and the accompanying code repository are aimed at being simple to understand and easy to follow. While simple language is used everywhere possible to describe the algorithms, the core theoretical concepts including the mathematical equations are laid out with brief and intuitive explanations as they are essential for understanding the code implementation and for further modifications and tailoring by the readers.

&amp;#x200B;

The book takes the readers through the step-by-step process of building intelligent agent algorithms using deep reinforcement learning starting from implementation of the building blocks for configuring, training, logging, visualizing, testing and monitoring the agent. **The book walks the reader through agent implementations in PyTorch to solve a variety of tasks and problems including: classical AI problems and console games like the Atari games, and complex problems like autonomous driving in the CARLA driving simulator**. In the closing chapters, the book provides an overview of the latest learning environments and learning algorithms along with pointers to more resources that will help the readers to take their hands-on skills to the next level. The[ code repository](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/) for the book on GitHub provides all the necessary code, scripts and instructions to follow through the book successfully.

## Learning outcome

* Get introduced to intelligent agents and learning environments
* Understand the basics of Reinforcement Learning (RL) &amp; deep RL
* Get started with OpenAI gym &amp; PyTorch for deep RL
* Implement building blocks to configure, train, log, visualize &amp; test agents
* Implement deep Q learning agent to solve discrete optimal control tasks
* Learn to create custom learning environments for real-world problems
* Implement deep actor-critic agent to drive a car autonomously in CARLA
* Resources to take your intelligent agent development skills to next level

**Available here:** [Hands-on AI Agents Developer Guide with code on X-mas sale! ($5)](https://www.packtpub.com/big-data-and-business-intelligence/hands-intelligent-agents-openai-gym?utm_source=Praveen%20Palanisamy&amp;utm_medium=referral&amp;utm_campaign=9781788836579)

""",0,1,False,self,,,,,
1334,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,2,a96mn7,self.MachineLearning,DirtyQuick UI for specific scenarios,https://www.reddit.com/r/MachineLearning/comments/a96mn7/dirtyquick_ui_for_specific_scenarios/,paarulakan,1545670858,[removed],0,1,False,self,,,,,
1335,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,2,a96uoy,self.MachineLearning,Coming up with a neural network design for a research problem,https://www.reddit.com/r/MachineLearning/comments/a96uoy/coming_up_with_a_neural_network_design_for_a/,ajaysub110,1545672379,"I (along with a partner) recently started off with a machine learning (more deep learning) project on medical image detection and classification. We decided to target the classification problem first. After the initial data exploration, we went through some papers that discussed work on very similar research problems. After getting an idea of the current progress, benchmarks and model architectures currently being used in the field, we decided to run a few pretrained networks on the data just to obtain a starting point for our further work.
So, since it is an image classification problem, we ran the data through some common CNN based networks such as Alexnet, inception v3 and densenet, of which densenet produced relatively good results.
After obtaining these results, were in a dilemma as to how to proceed with improving our results. Would be great to receive some advice from someone who has some experience in working on such projects.
Thanks in advance ",0,1,False,self,,,,,
1336,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,2,a96vft,self.MachineLearning,"Anyone interested in hosting deep learning project locally, have a look at my recent post",https://www.reddit.com/r/MachineLearning/comments/a96vft/anyone_interested_in_hosting_deep_learning/,ashutosj,1545672523,[removed],0,1,False,self,,,,,
1337,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,2,a970no,self.MachineLearning,Chief AI Officer coming to hospitals near you,https://www.reddit.com/r/MachineLearning/comments/a970no/chief_ai_officer_coming_to_hospitals_near_you/,dcn20002,1545673514,[removed],0,1,False,self,,,,,
1338,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,2,a974di,reddit.com,Reuso de Resduos V,https://www.reddit.com/r/MachineLearning/comments/a974di/reuso_de_resduos_v/,JamurGerloff,1545674228,,0,1,False,default,,,,,
1339,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,4,a97rl3,self.MachineLearning,[P] Learning to play Tetris with MCTS and TD learning,https://www.reddit.com/r/MachineLearning/comments/a97rl3/p_learning_to_play_tetris_with_mcts_and_td/,b0red1337,1545678554,"Hi all,

This is my attempt to solve the Tetris environment using MCTS and TD learning inspired by AlphaGo Zero.

At 400 games of self-plays, the agent surpassed the vanilla MCTS (random policy rollouts) which has an average of 7 lines cleared per game.

At 800 games of self-plays, the agent was able to clear more than 1300 lines in a single game which to the best of my knowledge is the best result without using any heuristics. (I couldn't find many papers on the topic, please let me know if I'm wrong.)

Here is a video showing the agent in action

[https://www.youtube.com/watch?v=EALo2GfZuYU](https://www.youtube.com/watch?v=EALo2GfZuYU)

Here is the git repo in case you are interested

[https://github.com/hrpan/tetris\_mcts](https://github.com/hrpan/tetris_mcts)

&amp;#x200B;

Let me know what you think!",35,1,False,self,,,,,
1340,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,4,a97sl9,self.MachineLearning,Waypointing for a pure maths graduate interested in ML theory?,https://www.reddit.com/r/MachineLearning/comments/a97sl9/waypointing_for_a_pure_maths_graduate_interested/,SigmaB,1545678739,[removed],0,1,False,self,,,,,
1341,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,5,a988rd,self.MachineLearning,What preferences when Renting GPU or other Dedicated Servers?,https://www.reddit.com/r/MachineLearning/comments/a988rd/what_preferences_when_renting_gpu_or_other/,yar_hetzner,1545681841,[removed],0,1,False,self,,,,,
1342,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,6,a98vfl,self.MachineLearning,Deep Quantile Regression on the Christmas Tree Dataset,https://www.reddit.com/r/MachineLearning/comments/a98vfl/deep_quantile_regression_on_the_christmas_tree/,ig248,1545686324,[removed],0,1,False,self,,,,,
1343,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,6,a98xav,self.MachineLearning,Particle Swarm Optimization Tutorial in Python,https://www.reddit.com/r/MachineLearning/comments/a98xav/particle_swarm_optimization_tutorial_in_python/,IranNeto,1545686705,[removed],0,1,False,self,,,,,
1344,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,8,a99wb8,self.MachineLearning,[P] My list of known code repos,https://www.reddit.com/r/MachineLearning/comments/a99wb8/p_my_list_of_known_code_repos/,superaromatic,1545694077,[removed],0,1,False,self,,,,,
1345,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,8,a99zw5,shiftedup.com,Yet another article about your Machine Learning career,https://www.reddit.com/r/MachineLearning/comments/a99zw5/yet_another_article_about_your_machine_learning/,svpino,1545694850,,0,1,False,default,,,,,
1346,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,8,a9a2b3,self.MachineLearning,[P] My list of known code repos,https://www.reddit.com/r/MachineLearning/comments/a9a2b3/p_my_list_of_known_code_repos/,superaromatic,1545695385,[removed],0,1,False,self,,,,,
1347,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,9,a9a8ea,self.MachineLearning,[D] My list of known code repos,https://www.reddit.com/r/MachineLearning/comments/a9a8ea/d_my_list_of_known_code_repos/,superaromatic,1545696697,[removed],0,1,False,self,,,,,
1348,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,9,a9aeli,self.MachineLearning,Best byte-sized educational video about deepfakes?,https://www.reddit.com/r/MachineLearning/comments/a9aeli/best_bytesized_educational_video_about_deepfakes/,fontkiller,1545698064,[removed],0,1,False,self,,,,,
1349,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,9,a9aes4,self.MachineLearning,[D] Defenses against GAN generated images for Image Forensics and forgery detection,https://www.reddit.com/r/MachineLearning/comments/a9aes4/d_defenses_against_gan_generated_images_for_image/,TheShadow29,1545698111,"I am looking for papers in the field of forgery detection. Like if a part of the image is replaced by a GAN generated input then how to detect that part? Or if the whole image is generated by a GAN how to classify it as a synthetic image?

The papers I have found are: 
1. Buster Net for copy move forge detection: http://openaccess.thecvf.com/content_ECCV_2018/papers/Rex_Yue_Wu_BusterNet_Detecting_Copy-Move_ECCV_2018_paper.pdf

2. Face Tamper Detection: https://arxiv.org/pdf/1803.11276.pdf

Wanted to know if anyone else is aware of any other papers.",4,1,False,self,,,,,
1350,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,9,a9ajc6,self.MachineLearning,"[D] How often do you implement ML's algorithms ""by hand"" opposed to using a programming library?",https://www.reddit.com/r/MachineLearning/comments/a9ajc6/d_how_often_do_you_implement_mls_algorithms_by/,riot-nerf-red-buff,1545699195,,82,1,False,self,,,,,
1351,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,12,a9bqly,self.MachineLearning,[R] Deciphering the performance of different ML algorithms on a given feature space,https://www.reddit.com/r/MachineLearning/comments/a9bqly/r_deciphering_the_performance_of_different_ml/,OldManufacturer,1545710054,"My question is this: is it possible to generate some metrics that would effectively explain why ML algorithm A (SVM for example) would perform better than B for a given feature space? I know that they each have their strengths and weaknesses. But what if we dont know the exact nature of our data set? Is there some way to look at an unknown feature space and narrow down our list of competitive ML algorithms without actually testing each one (and thus dealing with a computationally expensive experiment)?

I would guess that variance in each feature would be an important piece of information, but could it be used to assume that one algorithm would perform better than the rest?",2,1,False,self,,,,,
1352,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,13,a9c4n4,self.MachineLearning,[P] Running regression on both binary and continuous variables?,https://www.reddit.com/r/MachineLearning/comments/a9c4n4/p_running_regression_on_both_binary_and/,dcn20002,1545713662,"I am currently doing a project to write a learning algorithm to predict user behavior based on several factors. Most of which are text which I use bag of words to detect the frequency and recode them into 1 or 0. For the rest, they are continuous variable (age, etc). 

&amp;#x200B;

I want to figure which is the correct regression model to use. I am not an expert at statistics and would need to brush up on this topic. I wonder any of you have an advice on this?",4,1,False,self,,,,,
1353,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,15,a9cthh,self.MachineLearning,Make your Career with Machine Learning Training in Pune,https://www.reddit.com/r/MachineLearning/comments/a9cthh/make_your_career_with_machine_learning_training/,techethans,1545720331,[removed],0,1,False,self,,,,,
1354,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,17,a9ddu8,self.MachineLearning,[P] Holiday card generator using Sketch-RNN and Style Transfer,https://www.reddit.com/r/MachineLearning/comments/a9ddu8/p_holiday_card_generator_using_sketchrnn_and/,ljvmiranda,1545726708,"Hi all, 

For the holidays, we made a holiday card generator to give to our workmates and friends, thought I might share this here too! 

https://stories.thinkingmachin.es/ai-art-holiday-cards/

Given an input string, it looks for the nearest Quick, Draw! class and draws it using Sketch-RNN, then we applied style transfer to the output image.

We also open-sourced our pipeline here:

https://github.com/thinkingmachines/christmAIs

Feel free to check that out!

Happy Holidays!",4,1,False,self,,,,,
1355,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,19,a9dxn2,github.com,Learn backprop the hard way: build your neural network from scratch using Numpy only,https://www.reddit.com/r/MachineLearning/comments/a9dxn2/learn_backprop_the_hard_way_build_your_neural/,ahmedbesbes,1545733483,,0,1,False,https://b.thumbs.redditmedia.com/PRRLkRXS5Qob2mLxoLNTMSoAzgcfafh55zXueynyLRY.jpg,,,,,
1356,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,21,a9eg5k,self.MachineLearning,How to become a machine learning engineer,https://www.reddit.com/r/MachineLearning/comments/a9eg5k/how_to_become_a_machine_learning_engineer/,Aditya_H,1545739738,[removed],0,1,False,self,,,,,
1357,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,21,a9eim0,self.MachineLearning,[D] Computer Vision in browser,https://www.reddit.com/r/MachineLearning/comments/a9eim0/d_computer_vision_in_browser/,xmxman,1545740614,"Im working on a couple of projects which requires computer vision for web on a client side essentially is object detection from webcam stream. I found couple libraries https://gammacv.com, https://inspirit.github.io/jsfeat which can help to build things similar to OpenCV, also TensorFlow.js is now available for browser but it is maybe heavy to use because of model size and performance. It is a very interesting to me, does anyone have an experience to use CV for a web in production",54,1,False,self,,,,,
1358,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,21,a9eors,self.MachineLearning,Neural network representation,https://www.reddit.com/r/MachineLearning/comments/a9eors/neural_network_representation/,paarulakan,1545742627,[removed],0,1,False,self,,,,,
1359,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,22,a9euuu,self.MachineLearning,How to start with NLP,https://www.reddit.com/r/MachineLearning/comments/a9euuu/how_to_start_with_nlp/,gjp16,1545744405,[removed],0,1,False,self,,,,,
1360,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,22,a9exmg,self.MachineLearning,[P] Real Time Genetic Algorithm applied to Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a9exmg/p_real_time_genetic_algorithm_applied_to_neural/,RKuurstra,1545745215,"Hellow everyone,

&amp;#x200B;

I'm a long time stalker of this subreddit. I have no background in this field ( not work or even relevant degrees ), but I really love it! I'm a videogame programmer, and in my free time I always ""try stuff"" in this field to hone my skills as such.

So I have several projects, and recently I opened few of them to the public!

&amp;#x200B;

In [this project](https://gitlab.com/kuurstra.renato/NeuralNetworksStatic/wikis/Genetic-algorithm-and-Neural-Networks-in-real-time) I was trying to apply Genetic Algorithm approach to train neural networks in real time. The link is on a small wiki page of the site of the library project. The main goal for me was trying to learn how GA works ( or don't!:p) with neural networks and learn how to exploit ( or don't!:p )  it in a video game environment.

&amp;#x200B;

The [other project](https://gitlab.com/kuurstra.renato/NNetworkTest/wikis/Neural-networks-using-Unreal-Engine-4.15) use this library in two small ""gyms"". One of such is the classic ""drive the car on the circuit"" problem, which is the most succesfull one. The framework is Unreal Engine 4.15, and I made use of both c++ and blueprints.

&amp;#x200B;

Beside hoping that some of you big guys find this at least a bit interesting I have a few questions:

&amp;#x200B;

1)The only paper I found doing something similar ( very similar :D ) is. It took me a LOT to find. The only way seem to read all the papers and memorizing ""what's going on"". But as a amateur doing this in my free time this is extremely difficult! Any hint?

2) I'm currently unemployed, do you think these projects are too amateurish to be added to my resume or they could make some sort of good impression in a possible employer ( most likely video game )?

3) What should I focus more on my future projects? Should the code be more readable? More optimize? I try to make the code as readable as possible, but since the scope of these projects are very small I also tend to try and go for ""shortcuts"" as much as possible. I see this in a lot of research projects too, should I keep it like this?  
4) What you would add to the wikis? 

&amp;#x200B;

Thanks a lot if you have read all of this \^\_\^",14,1,False,self,,,,,
1361,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,22,a9eyw5,self.MachineLearning,shhhh...decoding,https://www.reddit.com/r/MachineLearning/comments/a9eyw5/shhhhdecoding/,allthhatnonsense,1545745580,[removed],0,1,False,self,,,,,
1362,MachineLearning,t5_2r3gv,2018-12-25,2018,12,25,23,a9f7p7,self.MachineLearning,DeepXmas: AI knows if you are naughty or nice,https://www.reddit.com/r/MachineLearning/comments/a9f7p7/deepxmas_ai_knows_if_you_are_naughty_or_nice/,mormon_data_geek,1545747938,[removed],0,1,False,https://b.thumbs.redditmedia.com/xmNRxj0IWbF_ngZyydTCqIZ9skLv2oi84wZeWQVPJ6o.jpg,,,,,
1363,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,0,a9fgz3,self.MachineLearning,[D] Replicating deep video portraits,https://www.reddit.com/r/MachineLearning/comments/a9fgz3/d_replicating_deep_video_portraits/,hanyuqn,1545750336,"I am trying to find an open source project that can achieve similar results to Deep Video Portraits (https://www.youtube.com/watch?v=qc5P2bvfl44) where facial expressions and body movements are transferred to a target actor. A few things I've looked at:

* The AVATAR model in deepfacelab is able to transfer facial expressions (see https://www.youtube.com/watch?v=3M0E4QnWMqA) but it doesn't seem like the result could be put back into the original video so the body is still visible. There is this comment https://github.com/iperov/DeepFaceLab/is...-448825013 saying it could be done but I very much doubt it would work.
* Recycle-GAN (https://www.youtube.com/watch?v=EU4BvhtEuG0) in their example has major artefacts
* Face2Face (https://www.youtube.com/watch?v=ohmajJTcpNk) is implemented in a third-party repo here https://github.com/datitran/face2face-demo but the example (https://github.com/datitran/face2face-de...xample.gif) is of very poor quality

Any suggestions welcome.",2,1,False,self,,,,,
1364,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,0,a9fi2e,self.MachineLearning,[P] Automatic Cancer Screening with Region-based CNN,https://www.reddit.com/r/MachineLearning/comments/a9fi2e/p_automatic_cancer_screening_with_regionbased_cnn/,whria78,1545750604,"Hello, I am Han Seung Seog from I Dermatology clinic.

&amp;#x200B;

The automatic screening system consists of 1) **blob-detector**, 2) **fine-image-selector**, and  3) **malignancy-classifier**.

&amp;#x200B;

1. The blob-detector is trained with py-faster-RCNN (model = VGG-16) and approximately 25,000 nodular disorder images + 100,000 ImageNet images.
2. The fine-image-selector is trained with NVCaffe (model = SE-ResNext-50, SE-ResNet-50) and over 500,000 images (clinical images - 400,000; ImageNet images - 100,000)
3. The malignancy-classifier (Model Dermatology; version 20181225; The old (20180623) version of model dermatology is currently available at [http://modelderm.com](http://modelderm.com)) is trained with NVCaffe (model = SENet, SE-ResNet-50) and over 270,000 images (220,000 - clinical diagnosis; 50,000 - generated by RCNN technology and diagnosis had been tagged by dermatology).

&amp;#x200B;

&amp;#x200B;

The screenshot was created by testing our algorithm with 8 internet images which was downloaded  via search engine (image.google.com) .

1) before

2) after blob-detector

3) after fine-image-selector

4) after malignancy classifier

&amp;#x200B;

Although the AI had been shown dermatologist-level performance in several studies, the most important problem in skin cancer screening is false positive. We are working to reduce the false positive by generating a lot of skin blobs images and training CNNs with those images.

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1365,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,0,a9fii0,i.redd.it,[P] Automatic Skin Cancer Screening with Region-based CNN,https://www.reddit.com/r/MachineLearning/comments/a9fii0/p_automatic_skin_cancer_screening_with/,whria78,1545750705,,1,1,False,default,,,,,
1366,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,0,a9fkog,self.MachineLearning,Why do we use non-linear activation functions? What's the purpose of adding multiple layers?,https://www.reddit.com/r/MachineLearning/comments/a9fkog/why_do_we_use_nonlinear_activation_functions/,Yahiabouda,1545751222,[removed],0,1,False,self,,,,,
1367,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,0,a9fmzq,self.MachineLearning,[P] Automatic Skin Cancer Screening with Region-based CNN,https://www.reddit.com/r/MachineLearning/comments/a9fmzq/p_automatic_skin_cancer_screening_with/,whria78,1545751760,"Hello, I am Han Seung Seog from I Dermatology clinic.

&amp;#x200B;

The automatic screening system consists of 1) **blob-detector**, 2) **fine-image-selector**, and  3) **malignancy-classifier**.

&amp;#x200B;

1. The blob-detector was trained with py-faster-RCNN (model = VGG-16) and approximately 25,000 nodular disorder images ([https://www.ncbi.nlm.nih.gov/pubmed/29428356](https://www.ncbi.nlm.nih.gov/pubmed/29428356)) + 100,000 ImageNet images.
2. The fine-image-selector was trained with NVCaffe (model = SE-ResNext-50, SE-ResNet-50) and over 500,000 images (clinical images - 400,000; ImageNet images - 100,000)
3. The malignancy-classifier (Model Dermatology; version 20181225; The old (20180623) version of model dermatology is currently available at [http://modelderm.com](http://modelderm.com)) was trained with NVCaffe (model = SENet, SE-ResNet-50) and over 230,000 images (220,000 - clinical diagnosis; over 10,000 - generated by RCNN technology and the diagnosis had been tagged by dermatologist from image findings).

&amp;#x200B;

**Screenshot :** [**https://i.redd.it/0d1ittotwf621.jpg**](https://i.redd.it/0d1ittotwf621.jpg)

&amp;#x200B;

The screenshot was created by testing our algorithm with 8 internet images which was downloaded  via search engine (image.google.com) .

1. before
2. after blob-detector
3. after fine-image-selector
4. after malignancy classifier

&amp;#x200B;

Although the AI had been shown dermatologist-level performance in several studies, the most difficult problem to be solved in skin cancer screening is false positive problem. We are working to reduce the false positive by generating a lot of skin blobs images and training the algorithm with those images.

&amp;#x200B;

&amp;#x200B;",18,1,False,self,,,,,
1368,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,2,a9geci,self.MachineLearning,"Which is the [Q] Fastest (Given a passage and a question, how long will it take to find the span on a certain hardware) and smallest (With respect to number of parameters) model trained for SQuaD task?",https://www.reddit.com/r/MachineLearning/comments/a9geci/which_is_the_q_fastest_given_a_passage_and_a/,dchatterjee172,1545757696,[removed],0,1,False,self,,,,,
1369,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,2,a9gent,self.MachineLearning,What are the possible ways design a deep learning model with a very few training images?,https://www.reddit.com/r/MachineLearning/comments/a9gent/what_are_the_possible_ways_design_a_deep_learning/,Ahmad401,1545757752,[removed],0,1,False,self,,,,,
1370,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,2,a9ghnq,self.MachineLearning,"[Q] Which is the fastest (Given a passage and a question, how long will it take to find the span on a certain hardware) and smallest (With respect to number of parameters) model trained for SQuaD task?",https://www.reddit.com/r/MachineLearning/comments/a9ghnq/q_which_is_the_fastest_given_a_passage_and_a/,dchatterjee172,1545758353,,0,1,False,self,,,,,
1371,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,2,a9gsw6,self.MachineLearning,AI vs Deep Learning vs Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a9gsw6/ai_vs_deep_learning_vs_machine_learning/,andrea_manero,1545760620,http://www.datasciencecentral.com/profiles/blogs/6448529:BlogPost:459267,0,1,False,self,,,,,
1372,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,4,a9hej5,self.MachineLearning,Question about CNN Data Input,https://www.reddit.com/r/MachineLearning/comments/a9hej5/question_about_cnn_data_input/,onenerdbird,1545765053,[removed],0,1,False,self,,,,,
1373,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,5,a9htnd,self.MachineLearning,"""[P]"" Need ideas for undergrad final year project",https://www.reddit.com/r/MachineLearning/comments/a9htnd/p_need_ideas_for_undergrad_final_year_project/,madhavgoyal98,1545768195,"I'm looking for some innovative project ideas that can be done using machine learning/deep learning.

The project has to be completed in 6 months.",13,1,False,self,,,,,
1374,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,5,a9hy55,self.MachineLearning,"[P] Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors &lt;- Make pictures, no GAN!",https://www.reddit.com/r/MachineLearning/comments/a9hy55/p_nonadversarial_image_synthesis_with_generative/,neuralPr0cess0r,1545769153,"Non-Adversarial Image Synthesis with Generative Latent Nearest Neighbors

[https://arxiv.org/abs/1812.08985v1](https://arxiv.org/abs/1812.08985v1)

&amp;#x200B;

A very interesting paper by Facebook. In this paper they describe a method to create an image-generator that has many of the same qualities of a 'standard' GAN generator but without the hassle of actually training a GAN. This means the training is more stable, less chance of mode collapse, faster to train ... etc 

They achieve this by first projecting their dataset into a low-dimensional space. This low-dimensional space is then used as input into a generator. The generator maps each projected-low-dimensional point into its complementary image. The image quality is then judges by a VGG-perceptual metric. (how similar is the generated image in VGG-feature space to the original) Finally, they learn a layer that maps noise vectors into good spots within the previously projected low-D space.

&amp;#x200B;

&amp;#x200B;",16,1,False,self,,,,,
1375,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,6,a9ij16,self.MachineLearning,"which model is the best of weather prediction, gru, lstm, another .......?",https://www.reddit.com/r/MachineLearning/comments/a9ij16/which_model_is_the_best_of_weather_prediction_gru/,asda43asdf23423,1545773679,[removed],0,1,False,self,,,,,
1376,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,7,a9iv5w,self.MachineLearning,YOLO implementation in python,https://www.reddit.com/r/MachineLearning/comments/a9iv5w/yolo_implementation_in_python/,H3t0N,1545776344,[removed],0,1,False,self,,,,,
1377,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,10,a9k4y4,self.MachineLearning,[D] Using Binary Cross Entropy Loss after Softmax for Multi-class Classification,https://www.reddit.com/r/MachineLearning/comments/a9k4y4/d_using_binary_cross_entropy_loss_after_softmax/,alexmlamb,1545786982,"Hello, 

Sometimes, when I've done multi-class classification, I've used the binary cross entropy on all of the labels, but after the softmax.  

Putting aside the question of whether this is ideal - it seems to yield a different loss from doing categorical cross entropy after the softmax.  

I did some math on both of the losses and they seem different in a way that doesn't yield an obvious mathematical correspondence.  Both push up the value on the ""true"" class, but they have different normalization terms (pushing down pre-softmax on non-selected classes).  

I ran this in Pytorch: 

    &gt;&gt;&gt; p = torch.Tensor([0.9, 0.05, 0.02, 0.03])
    &gt;&gt;&gt; nll(p.unsqueeze(0),torch.LongTensor([0]))
    tensor(-0.9000)
    &gt;&gt;&gt; nll(p.unsqueeze(0),torch.LongTensor([1]))
    tensor(-0.0500)
    &gt;&gt;&gt; nll(p.unsqueeze(0),torch.LongTensor([2]))
    tensor(-0.0200)
    &gt;&gt;&gt; nll(p.unsqueeze(0),torch.LongTensor([3]))
    tensor(-0.0300)
    &gt;&gt;&gt; y = torch.Tensor([0.0, 1.0, 0.0, 0.0])
    &gt;&gt;&gt; bce(p,torch.Tensor([1.0, 0.0, 0.0, 0.0]))
    tensor(0.0518)
    &gt;&gt;&gt; bce(p,torch.Tensor([0.0, 1.0, 0.0, 0.0]))
    tensor(1.3372)
    &gt;&gt;&gt; bce(p,torch.Tensor([0.0, 0.0, 1.0, 0.0]))
    tensor(1.5741)
    &gt;&gt;&gt; bce(p,torch.Tensor([0.0, 0.0, 0.0, 1.0]))
    tensor(1.4702)
    

Both give the same rank-order, but the values are rather different.  

Do you know if there are any derivations or math comparing these two losses?  Or maybe a better intuition on why one is better than the other?  ",7,1,False,self,,,,,
1378,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,12,a9kuwr,self.MachineLearning,Data-mining of public facebook pages?,https://www.reddit.com/r/MachineLearning/comments/a9kuwr/datamining_of_public_facebook_pages/,Phischstaebchen,1545793325,[removed],0,1,False,self,,,,,
1379,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,12,a9l3kx,self.MachineLearning,[Q] Do deep learning models behave linear in high dimensional space?,https://www.reddit.com/r/MachineLearning/comments/a9l3kx/q_do_deep_learning_models_behave_linear_in_high/,finallyifoundvalidUN,1545795424,[removed],0,1,False,self,,,,,
1380,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,12,a9l52l,self.MachineLearning,[D] do deep learning models behave linear in high dimensional space?,https://www.reddit.com/r/MachineLearning/comments/a9l52l/d_do_deep_learning_models_behave_linear_in_high/,finallyifoundvalidUN,1545795784,"In the following paper the author says ""
Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples""

https://arxiv.org/abs/1412.6572",9,1,False,self,,,,,
1381,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,13,a9lkbt,youtube.com,How Neural Networks Work- Simply Explained,https://www.reddit.com/r/MachineLearning/comments/a9lkbt/how_neural_networks_work_simply_explained/,ailearn12,1545799485,,0,1,False,https://b.thumbs.redditmedia.com/lAiMzjTyr2KqqhRGgJUVMzQdwU2_M_RzNgdeOAGU2eY.jpg,,,,,
1382,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,13,a9lm1p,self.MachineLearning,[Help] Binary Krill Herd for Feature Selection in Mathlab,https://www.reddit.com/r/MachineLearning/comments/a9lm1p/help_binary_krill_herd_for_feature_selection_in/,ReginaMeis,1545799898,"My machinelearning teacher wants me to write down ""Binary Krill Herd Feature Selection"" matlab code. There is no source code on internet that i can use. There might be useful stuff but im at the zero level on matlab. Only code that i have found about ""Krill Herd"" is this -&gt; [https://www.mathworks.com/matlabcentral/fileexchange/55486-krill-herd-algorithm](https://www.mathworks.com/matlabcentral/fileexchange/55486-krill-herd-algorithm) and this is classification. Related documentation is here -&gt; [https://drive.google.com/file/d/1Wf82G5C1s40fGjV2W89z41J8XWZHHn8A/view?usp=sharing](https://drive.google.com/file/d/1Wf82G5C1s40fGjV2W89z41J8XWZHHn8A/view?usp=sharing)My teacher wants feature selection of that code and she insists that its easiest thing to do. I am trying to workaround for two weeks. Couldn't do anything at all. This project is for final. I haven't slept for about 56 hours. I don't want to study for another year at my university.

 ",0,1,False,self,,,,,
1383,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,15,a9m6l6,youtube.com,The 7 Steps of Machine Learning (AI Adventures),https://www.reddit.com/r/MachineLearning/comments/a9m6l6/the_7_steps_of_machine_learning_ai_adventures/,josephd,1545804896,,0,1,False,default,,,,,
1384,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,15,a9me0n,self.MachineLearning,Was 2018 the year of PyTorch?,https://www.reddit.com/r/MachineLearning/comments/a9me0n/was_2018_the_year_of_pytorch/,resumSOTA,1545806938,[removed],0,1,False,self,,,,,
1385,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,15,a9mexa,linkedin.com,"Abhinav IT Solutions Pvt Ltd on LinkedIn: ""AI applications are used in one or the other form today in education for student engagement and assistance in administration. Have a broad outlook of AI applications in Education.",https://www.reddit.com/r/MachineLearning/comments/a9mexa/abhinav_it_solutions_pvt_ltd_on_linkedin_ai/,vermarajan,1545807205,,0,1,False,default,,,,,
1386,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,15,a9mg39,self.MachineLearning,[D] Was 2018 the year of PyTorch?,https://www.reddit.com/r/MachineLearning/comments/a9mg39/d_was_2018_the_year_of_pytorch/,resumSOTA,1545807539,"With [PyTorch 1.0](https://developers.facebook.com/blog/post/2018/05/02/announcing-pytorch-1.0-for-research-production/), here are some of the patterns I've seen.

*  in top PyTorch [GitHub Repositories](https://github.com/topics/pytorch) (amazing packages and tutorials)
*  in companies adopting PyTorch (Facebook, Tesla, Nvidia, Apple, AllenAI, parts of Microsoft and Amazon)
* in top universities adopting PyTorch
*  in PyTorch code releases at ML conferences (ICML, ICLR, NeuroIPS, UAI, EMNLP, etc.)
*  in PyTorch courses (Udacity, Cousera, fastai, Siraj, etc.)
*  in PyTorch books (O'Reilly, Packt, etc.)
*  in AI twitter icons using PyTorch ([Andrej Karpathy](https://twitter.com/karpathy/status/868178954032513024), [Jeremy Howard](https://twitter.com/jeremyphoward/status/906293226235035648), etc.)

Will PyTorch have an even bigger impact in 2019?",0,1,False,self,,,,,
1387,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,16,a9mhm3,self.MachineLearning,Computer Vision - RL: Learning Acrobatics by Watching YouTube,https://www.reddit.com/r/MachineLearning/comments/a9mhm3/computer_vision_rl_learning_acrobatics_by/,obsezer,1545807969,[removed],0,1,False,self,,,,,
1388,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,16,a9mi5y,iamchandreshk.me,New release of ML.NET 0.7 (Machine Learning .NET),https://www.reddit.com/r/MachineLearning/comments/a9mi5y/new_release_of_mlnet_07_machine_learning_net/,iamchandreshk,1545808123,,0,1,False,default,,,,,
1389,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,18,a9n4uf,self.MachineLearning,Can you guys please direct me towards good resources to learn how to implement generative adversarial networks?,https://www.reddit.com/r/MachineLearning/comments/a9n4uf/can_you_guys_please_direct_me_towards_good/,fhrkingbradley,1545815419,[removed],0,1,False,self,,,,,
1390,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,19,a9ne4q,self.MachineLearning,LDA2Vec implementation,https://www.reddit.com/r/MachineLearning/comments/a9ne4q/lda2vec_implementation/,luffy352,1545818467,[removed],0,1,False,self,,,,,
1391,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,19,a9ngly,self.MachineLearning,[P] LDA2Vec implementation,https://www.reddit.com/r/MachineLearning/comments/a9ngly/p_lda2vec_implementation/,luffy352,1545819199,"Hello,

I would like to test my list of documents on LDA2Vec in order to be able to compare it with c\_v and u\_mass measures of LDA gensim and mallet.

But I can't find any working LDA2Vec implementation . I've tried [https://github.com/cemoody/lda2vec](https://github.com/cemoody/lda2vec) but it has been 3 years since it's not updated and there are many broken dependencies.

Do you know any working implementation?

 Thank you!

&amp;#x200B;",1,1,False,self,,,,,
1392,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,19,a9nmeo,self.MachineLearning,A radical new neural network design could overcome big challenges in AI,https://www.reddit.com/r/MachineLearning/comments/a9nmeo/a_radical_new_neural_network_design_could/,savanmorya,1545821067,[removed],0,1,False,self,,,,,
1393,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,19,a9nn5m,self.MachineLearning,[P] Neuroevolution-Bots,https://www.reddit.com/r/MachineLearning/comments/a9nn5m/p_neuroevolutionbots/,baylearn,1545821316,"A browser based [environment](https://mishig25.github.io/neuroevolution-robots/) where humanoids are trained to walk through Neuroevolution of Augmenting Topologies ([NEAT](http://nn.cs.utexas.edu/downloads/papers/stanley.gecco02_1.pdf)).

Web Demo: https://mishig25.github.io/neuroevolution-robots/


Tools used:

- [TensorFlow.js](https://js.tensorflow.org)
- [Neataptic.js](https://github.com/wagenaartje/neataptic)
- [Planck.js](http://piqnt.com/planck.js/) (a Box2D rewrite)

https://mishig25.github.io/neuroevolution-robots/",7,1,False,self,,,,,
1394,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,20,a9npkn,visualpath.in,DevOps Online Training in Hyderabad,https://www.reddit.com/r/MachineLearning/comments/a9npkn/devops_online_training_in_hyderabad/,vepambattuchandu111,1545822099,,0,1,False,default,,,,,
1395,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,20,a9nqje,self.MachineLearning,Hexagonal Shape Data Set,https://www.reddit.com/r/MachineLearning/comments/a9nqje/hexagonal_shape_data_set/,akifakkaya,1545822381,Hi everybody. I am searching hexagonal shape data set but i cant find any result. I need minimum 200 hexagonal shape.,0,1,False,self,,,,,
1396,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,20,a9nxqk,self.MachineLearning,How You Think Machine Learning And artificial Intelligent Will Change ?,https://www.reddit.com/r/MachineLearning/comments/a9nxqk/how_you_think_machine_learning_and_artificial/,iamrealadvait,1545824619,[removed],0,1,False,self,,,,,
1397,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,21,a9o9yf,self.MachineLearning,Sklearn MLP Classifier,https://www.reddit.com/r/MachineLearning/comments/a9o9yf/sklearn_mlp_classifier/,ee1160694,1545828162,[removed],0,1,False,self,,,,,
1398,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,21,a9oara,martechcube.com,Customer Data Platform  Filling The Bridges Between A Marketer and A Customer,https://www.reddit.com/r/MachineLearning/comments/a9oara/customer_data_platform_filling_the_bridges/,hrtechcube,1545828367,,0,1,False,default,,,,,
1399,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,21,a9oblj,self.MachineLearning,"[D] What would you like to know about your own Facebook data? (Data viz, NLP)",https://www.reddit.com/r/MachineLearning/comments/a9oblj/d_what_would_you_like_to_know_about_your_own/,Anogio94,1545828614,"Hi!
Since the GDPR laws in Europe came into effect, it is now possible to download your complete Facebook data as JSON files (messages, comments, connexion ips, friends...)
Many of us have been using Facebook for years, so it's usually a large dataset with lots of opportunities for some cool data viz (or even NLP?) :) 
I am currently working on a script that would allow anyone to perform analytics on their own data and build nice graphs. 
I've got most of the boilerplate down and I would really appreciate some inputs on what kind of analytics you guys would find interesting!

A map showing connexion locations? An activity graph ? Friends network? Sentiment analysis ? 
I know there is quite a lot of fun stuff to do and I would like to get my priorities right

It's far from ready for now, but I'll make sure to share the repo in the future if anyone is interested :)

PS: I was sure that this would already exist, and I did some looking around and could not find anything. If you know about a similar project do let me know! ",12,1,False,self,,,,,
1400,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,21,a9ocbv,self.MachineLearning,[P] BERT implementation using wikipedia,https://www.reddit.com/r/MachineLearning/comments/a9ocbv/p_bert_implementation_using_wikipedia/,thelara32,1545828826,"**What i wanna do:**

a) Develop a Q&amp;A chatbot to my university

&amp;#x200B;

**What i thought:**  


1) Make a pre-trained BERT ([https://github.com/google-research/bert](https://github.com/google-research/bert) / [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)) model using wikipedia on my main language (portuguese).

2) Fine tune using some questions

3) make it available in a web app.

&amp;#x200B;

I'm not sure if i'm following the correct path, so, i need some guidance. That's the way to achiev what i want? Are there better ways?",2,1,False,self,,,,,
1401,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,23,a9oylh,self.MachineLearning,[D] Single epoch reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/a9oylh/d_single_epoch_reinforcement_learning/,Laser_Plasma,1545834456,"I was thinking of an equivalent to one-shot/few-shot learning in SL, but for RL. Is anyone familiar with any research around this theme? I'm thinking about agents that preferably manage to learn something useful even before finishing an epoch.",5,1,False,self,,,,,
1402,MachineLearning,t5_2r3gv,2018-12-26,2018,12,26,23,a9p57g,youtube.com,Machine Learning Algorithms Part 5: Random Forest Classification In Python,https://www.reddit.com/r/MachineLearning/comments/a9p57g/machine_learning_algorithms_part_5_random_forest/,corymaklin,1545835974,,0,1,False,default,,,,,
1403,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,0,a9p7yr,learnaionlineatoz.blogspot.com,What makes AI bots think better than humans? Simple explanation,https://www.reddit.com/r/MachineLearning/comments/a9p7yr/what_makes_ai_bots_think_better_than_humans/,Updownvizze,1545836564,,0,1,False,default,,,,,
1404,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,0,a9p8gp,self.MachineLearning,Text-to-speech at the end of 2018,https://www.reddit.com/r/MachineLearning/comments/a9p8gp/texttospeech_at_the_end_of_2018/,anagapetyan,1545836671,"Hi,

I became interested in text-to-speech. 

What old and state of the art papers and articles are worth reading?",0,1,False,self,,,,,
1405,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,0,a9pk4u,self.MachineLearning,"Bio Inspired Robotics, Drones on Battlefield and Building Realworld C3PO? | Auke Ijspeert",https://www.reddit.com/r/MachineLearning/comments/a9pk4u/bio_inspired_robotics_drones_on_battlefield_and/,The_Syndicate_VC,1545839118,[removed],0,1,False,self,,,,,
1406,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,0,a9pmc8,self.MachineLearning,"Simple Questions Thread December 26, 2018",https://www.reddit.com/r/MachineLearning/comments/a9pmc8/simple_questions_thread_december_26_2018/,AutoModerator,1545839567,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",0,1,False,self,,,,,
1407,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,1,a9pr2m,self.MachineLearning,From Data Analysis to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a9pr2m/from_data_analysis_to_machine_learning/,andrea_manero,1545840499,[removed],0,1,False,self,,,,,
1408,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,2,a9qkns,self.MachineLearning,[R] SlowFast - Dual-mode CNN for Video Understanding,https://www.reddit.com/r/MachineLearning/comments/a9qkns/r_slowfast_dualmode_cnn_for_video_understanding/,tldrtldreverything,1545846247,"Hey, I published a summary of a new paper from FAIR called SlowFast. The paper details a cool technique which allows to understand what's happening in images by using two neural networks - a 'slow' CNN that recognizes the fine content of the image and a 'fast' CNN that recognizes swift changes in the content. Full summary here: https://lyrn.ai/2018/12/21/slowfast-dual-mode-cnn-for-video-understanding/",11,1,False,self,,,,,
1409,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,2,a9qlnx,openreview.net,[R][ICLR Oral] Learning Unsupervised Learning Rules,https://www.reddit.com/r/MachineLearning/comments/a9qlnx/riclr_oral_learning_unsupervised_learning_rules/,downtownslim,1545846442,,0,1,False,default,,,,,
1410,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,3,a9r7o2,youtube.com,"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting",https://www.reddit.com/r/MachineLearning/comments/a9r7o2/this_video_goes_over_a_model_that_predicts_the/,antaloaalonso,1545850526,,0,1,False,https://b.thumbs.redditmedia.com/5jjO2lkQVL7ColtqLT4ZeJoZvNLKXRVa9BgFVt-_50Y.jpg,,,,,
1411,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,4,a9rc0y,i.redd.it,Back propagation explained intuitively by andew ng,https://www.reddit.com/r/MachineLearning/comments/a9rc0y/back_propagation_explained_intuitively_by_andew_ng/,heidiki01,1545851350,,0,1,False,https://b.thumbs.redditmedia.com/r-sDl5NesfYcu3a00BZKOZullcN3xaPuud0sYRcGFHA.jpg,,,,,
1412,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,4,a9ronj,self.MachineLearning,Best places to read about ML research papers?,https://www.reddit.com/r/MachineLearning/comments/a9ronj/best_places_to_read_about_ml_research_papers/,uwwasteman,1545853706,[removed],0,1,False,self,,,,,
1413,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,5,a9sakq,self.MachineLearning,How Helpful are Publications in Getting Good Jobs in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/a9sakq/how_helpful_are_publications_in_getting_good_jobs/,TrainingStorm,1545857758,Does Having 2-4 Publications in Machine Learning research areas make a significant difference in the number of job offers you get when looking for a new job? ,0,1,False,self,,,,,
1414,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,7,a9su90,self.MachineLearning,[P] A knowledge extractor from text,https://www.reddit.com/r/MachineLearning/comments/a9su90/p_a_knowledge_extractor_from_text/,ak96,1545861654,"I am trying to build a model which can extract information from a pdf/html file and use it for answering questions which are asked later. So, for example, I have a page on a football club which has information about its captain, manager, owner etc. and I want to extract it and store it for later retrieval upon request. You guys have any suggestions on how to do it? Or any papers that you can point me to? 

(Similar to Question Answering model using RNNs with bi-directional LSTMs/GRUs)",7,1,False,self,,,,,
1415,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,7,a9sytu,self.MachineLearning,I don't understand some of the math behind backpropagation,https://www.reddit.com/r/MachineLearning/comments/a9sytu/i_dont_understand_some_of_the_math_behind/,Yahiabouda,1545862591,[removed],0,1,False,self,,,,,
1416,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,7,a9t0dy,self.MachineLearning,Make Some Money,https://www.reddit.com/r/MachineLearning/comments/a9t0dy/make_some_money/,agm400,1545862909,[removed],0,1,False,self,,,,,
1417,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,9,a9u7t9,self.MachineLearning,"[D] Vote for Best of Machine Learning for 2018 -Categories include papers, talks, videos, applications, projects, reddit comments, Cross Validated - Stack Exchange posts, innovations, tools, and projects.",https://www.reddit.com/r/MachineLearning/comments/a9u7t9/d_vote_for_best_of_machine_learning_for_2018/,BatmantoshReturns,1545871743,"Last year's thread 

https://www.reddit.com/r/MachineLearning/comments/7nrzhn/d_results_from_best_of_machine_learning_2017/

Vote for:

Best paper

Best innovation 

Best application

Best video

Best youtube channel

Best blog post

Best blog overall

Best course (released in 2018)

Best book (released in 2018)

Best reddit comment/post

Best Cross Validated - Stack Exchange posts

Best project

Best new tool

Anything else?",1,1,False,self,,,,,
1418,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,10,a9ub6s,lambdalabs.com,Titan RTX - TensorFlow Benchmarks for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a9ub6s/titan_rtx_tensorflow_benchmarks_for_deep_learning/,mippie_moe,1545872462,,0,1,False,default,,,,,
1419,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,11,a9ut5z,self.MachineLearning,[Summar] First Kaggle Competition Experience,https://www.reddit.com/r/MachineLearning/comments/a9ut5z/summar_first_kaggle_competition_experience/,init__27,1545876227,[removed],0,1,False,self,,,,,
1420,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,11,a9utm5,self.MachineLearning,Depth First Learning Fellowship: $4000 grants to build ML curricula,https://www.reddit.com/r/MachineLearning/comments/a9utm5/depth_first_learning_fellowship_4000_grants_to/,cinjon,1545876324,"Hi all! We posted this earlier and are bringing it back just in case folks missed it in the pre-holiday rush. We're actively looking now for fellows so please don't hesitate if you're interested.

Were researchers from NYU, FAIR, Google Brain, and and DeepMind. Last year, we designed Depth First Learning, a pedagogy for diving deep into ML by carefully tailoring a curriculum around a particular ML paper or concept and leading small, focused discussion groups. So far, weve created guides for [InfoGAN](http://www.depthfirstlearning.com/2018/InfoGAN), [TRPO](http://www.depthfirstlearning.com/2018/TRPO), [AlphaGoZero](http://www.depthfirstlearning.com/2018/AlphaGoZero), and [DeepStack](http://www.depthfirstlearning.com/2018/DeepStack).

Since our launch, weve received very positive feedback from students and researchers. Now, we want to run new, online classes around the world. Thanks to the generosity of Jane Street, we will provide four fellows each with a $4000 grant to build a six-week curriculum and run weekly on-line discussions.

If youd like to lead a class about an important paper in machine learning, please visit [http://fellowship.depthfirstlearning.com](http://fellowship.depthfirstlearning.com/) to apply. We look forward to hearing from you!",0,1,False,self,,,,,
1421,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,11,a9v5d6,github.com,"Simple-Sh-DataScience: A collection of Bash scripts to install data science Tool, Lib and application",https://www.reddit.com/r/MachineLearning/comments/a9v5d6/simpleshdatascience_a_collection_of_bash_scripts/,nullbyte91,1545878870,,0,1,False,https://a.thumbs.redditmedia.com/ds1ns-7PqzSd4CjHQHcCcK5jS4MWzMd3GEmbCh0tVD4.jpg,,,,,
1422,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,12,a9vh36,self.MachineLearning,[R] [ICLR poster] Improving MMD-GAN Training with Repulsive Loss Function,https://www.reddit.com/r/MachineLearning/comments/a9vh36/r_iclr_poster_improving_mmdgan_training_with/,Richard_wth,1545881374,"**TL;DR**: Rearranging the terms in maximum mean discrepancy yields a much better loss function for the discriminator of generative adversarial nets

**Keywords**:

Spectral normalization, repulsive loss, bounded RBF kernel

**Abstract**:

Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.

**Links**

OpenReview link: [https://openreview.net/forum?id=HygjqjR9Km](https://openreview.net/forum?id=HygjqjR9Km)

arXiv link: [https://arxiv.org/abs/1812.09916](https://arxiv.org/abs/1812.09916)

Code link: [https://github.com/richardwth/MMD-GAN](https://github.com/richardwth/MMD-GAN)

\---------------------------------------------

Good day, mate!

I am the first author of this GAN paper that proposes a new loss function, a slightly modified spectral normalization method, and a bounded RBF kernel. The methods works very well with DCGAN architecture. 

Please take a few minutes to read the paper. Any comments are welcomed and I am happy to answer any questions in this extremely hot Christmas (yes I am in Australia and we stand upside down). ",2,1,False,self,,,,,
1423,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,12,a9vp1h,analyticsvidhya.com,25 best ML projects in 2018,https://www.reddit.com/r/MachineLearning/comments/a9vp1h/25_best_ml_projects_in_2018/,_guru007,1545883148,,0,1,False,default,,,,,
1424,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,13,a9w22t,thegradient.pub,Playing a game of GANstruction,https://www.reddit.com/r/MachineLearning/comments/a9w22t/playing_a_game_of_ganstruction/,worldwide__master,1545886309,,0,1,False,default,,,,,
1425,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,14,a9wemp,techblog.rakuten.co.jp,A beginner's guide on how to win a Deep Learning challenge,https://www.reddit.com/r/MachineLearning/comments/a9wemp/a_beginners_guide_on_how_to_win_a_deep_learning/,alvations,1545889236,,1,1,False,default,,,,,
1426,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,15,a9wk44,self.MachineLearning,"what are some of the best research labs that do work on deep reinforcement learning applied to robotics specifically to the field of bionics, and also who are the field experts?",https://www.reddit.com/r/MachineLearning/comments/a9wk44/what_are_some_of_the_best_research_labs_that_do/,dhruv2129,1545890555,I planning to apply for graduate course related to field of robotics and DRL or PhD. ,0,1,False,self,,,,,
1427,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,15,a9wkhe,self.MachineLearning,[P] Super SloMo: A CNN to convert any video to a slomo video.,https://www.reddit.com/r/MachineLearning/comments/a9wkhe/p_super_slomo_a_cnn_to_convert_any_video_to_a/,atplwl,1545890642,"I have implemented the paper [Super SloMo](https://people.cs.umass.edu/~hzjiang/projects/superslomo/) by Jiang et al. in PyTorch.  Super SloMo allows you to interpolate any number of frames between two reference frames. You can convert a 30 fps video to 240 fps or even 1000 fps video. 

 [GitHub Repo](https://github.com/avinashpaliwal/Super-SloMo) 

&amp;#x200B;",72,1,False,self,,,,,
1428,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,15,a9wnv0,self.MachineLearning,[D] State of Hebbian Learning Research,https://www.reddit.com/r/MachineLearning/comments/a9wnv0/d_state_of_hebbian_learning_research/,avrock123,1545891452,"Current deep learning is based off of backprop, aka a global tweaking of an algorithm via propagation of an error signal. However I've heard that biological networks make updates via a local learning rule, which I interpret as an algo that is only provided the states of a neuron's immediate stimuli to decide how to tweak that neuron's weights. A local learning rule would also make sense considering brain circuitry consists of a huge proportion of feedback connections, and (classic) backprop only works on DAGs. Couple questions:

\- How are 'weights' represented in neurons and by what mechanism are they tweaked?

\- Is this local learning rule narrative even correct? Any clear evidence?

\- What is the state of research regarding hebbian/local learning rules, why haven't they gotten traction? I was also specifically interested in research concerned w/ finding algorithms to discover an optimal local rule for a task (a hebbian meta-learner if that makes sense).

I'd love pointers to any resources/research, especially since I don't know where to start trying to understand these systems. I've studied basic ML theory and am caught up w/ deep learning, but want to better understand the foundational ideas of learning that people have come up with in the past.

\* I use 'hebbian' and 'local' interchangeably, correct me if there is a distinction between the two \*",16,1,False,self,,,,,
1429,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,16,a9wz72,self.MachineLearning,[P] TorchGAN: A Framework based on Pytorch for modeling GANs,https://www.reddit.com/r/MachineLearning/comments/a9wz72/p_torchgan_a_framework_based_on_pytorch_for/,avikpal,1545894323,"We have been working on this project for the past few months. Our attempt has been to develop an easy mechanism to combine various techniques from different GAN papers so as to allow rapid experimentation. Also, we have tried not to compromise with customizability on the part of the user. We would love to get some community feedback regarding this project.

&amp;#x200B;

Link - [https://github.com/torchgan/torchgan/](https://github.com/torchgan/torchgan/)",10,1,False,self,,,,,
1430,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,16,a9x07x,self.MachineLearning,[D] One-model ensemble-like inference like Monte-Carlo Dropout?,https://www.reddit.com/r/MachineLearning/comments/a9x07x/d_onemodel_ensemblelike_inference_like_montecarlo/,tsauri,1545894577,Are there more papers related to this method?,1,1,False,self,,,,,
1431,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,16,a9x60z,blog.azuratech.in,How is AI helping small businesses grow? Very insightful,https://www.reddit.com/r/MachineLearning/comments/a9x60z/how_is_ai_helping_small_businesses_grow_very/,chiragbabbar,1545896105,,0,1,False,default,,,,,
1432,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,16,a9xabt,pin.it,How to learn machine learning (05 easy steps) (Infographics),https://www.reddit.com/r/MachineLearning/comments/a9xabt/how_to_learn_machine_learning_05_easy_steps/,davidreed7021,1545897323,,0,1,False,default,,,,,
1433,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,18,a9xqwr,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a9xqwr/global_machine_learning_market_size_outlook/,enviint,1545902288,"The machine learning holds the highest CAGR of 44.86% during the forecast period 2019-2025. 

&amp;#x200B;

Request a sample @  https://www.envisioninteligence.com/industry-report/global-machine-learning-market/?utm\_source=reddit-hema ",0,1,False,self,,,,,
1434,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,18,a9xv1k,self.MachineLearning,[R] Upgrading to Nvidia RTX cards: Does anything break with FP16 training?,https://www.reddit.com/r/MachineLearning/comments/a9xv1k/r_upgrading_to_nvidia_rtx_cards_does_anything/,rantana,1545903570,"We're considering a cluster upgrade to the new Nvidia RTX cards which have a substantial performance boost when moving to FP16.

Has anyone had issues going from FP32 to FP16 training for their models? Where would most issues crop up?",5,1,False,self,,,,,
1435,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,19,a9y2r1,self.MachineLearning,Is it unethical to pursue research in artificial intelligence considering future threats?,https://www.reddit.com/r/MachineLearning/comments/a9y2r1/is_it_unethical_to_pursue_research_in_artificial/,cipher1202,1545905865,[removed],0,1,False,self,,,,,
1436,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,19,a9y358,github.com,GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint,https://www.reddit.com/r/MachineLearning/comments/a9y358/ganqp_a_novel_gan_framework_without_gradient/,rahulbhalley,1545905982,,0,1,False,default,,,,,
1437,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,19,a9y54n,github.com,GAN-QP: GAN Without Gradient Vanishing and Lipschitz Constraint,https://www.reddit.com/r/MachineLearning/comments/a9y54n/ganqp_gan_without_gradient_vanishing_and/,rahulbhalley,1545906580,,0,1,False,https://b.thumbs.redditmedia.com/cU1hInM7MScJRWQh97MZdGMpdeNKuVNv_RIV7it0Wyc.jpg,,,,,
1438,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,19,a9y9pg,self.MachineLearning,Adaptive threshold setting for parametric anomaly detection system applied to time series data,https://www.reddit.com/r/MachineLearning/comments/a9y9pg/adaptive_threshold_setting_for_parametric_anomaly/,deepakprabakar,1545907933,[removed],0,1,False,self,,,,,
1439,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,20,a9ykws,self.MachineLearning,Andrew Ng's Machine Learning course python implementation,https://www.reddit.com/r/MachineLearning/comments/a9ykws/andrew_ngs_machine_learning_course_python/,Beneblau,1545911206,[removed],0,1,False,self,,,,,
1440,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,20,a9ymml,self.MachineLearning,"'Future Hotel' opens in China, guests check-in by scanning faces!",https://www.reddit.com/r/MachineLearning/comments/a9ymml/future_hotel_opens_in_china_guests_checkin_by/,Azuratech,1545911659," Billionaire Jack Ma-led Alibaba has opened a hi-tech 'Future Hotel' in China named FlyZoo, that lets guests check-in at the hotel and access their rooms by scanning their faces. Guests can also control the lights, television and curtains in the room via Alibaba's voice-activated digital assistant. The hotel also employs robots to serve food, cocktails and coffee. 

&amp;#x200B;

*Processing img yhv6x9gf7t621...*

 Hotel bookings and check-out can also be done with a few clicks on mobile through an app.  

""The AI-based solution can help customers save time and relieve hotel employees from repetitive work,"" said Wang Qun, CEO of FlyZoo Hotel.

Wang said the new AI system will help to improve the management efficiency of the hotel, by reducing more than half of the labour force.

For fiscal year ended March 2018, the company reported revenues of USD 39.9 billion.

The hotel is the latest example of Chinese tech companies' foray into traditional industries such as the hospitality sector, the report said.

E-commerce giant JD.com announced in October its strategy to put smart home and electronic devices sold on its platform into hotels, in an effort to boost online sales.

In July, Baidu teamed up with Intercontinental Hotels Group in Beijing to allow guests to use its voice-controlled assistant to adjust room temperature and order room service at ease.

Before that, social media giant Tencent introduced QQfamily, a similar tech solution for hotel operators, in the southern city of Zhuhai last year.

""We want to install a 'smart brain' for hotels,"" said Wang.

""In the future, we will continue to make hotels smarter and more automated, as well as create more customized experiences for consumers,"" Wang added.

&amp;#x200B;",0,1,False,self,,,,,
1441,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,20,a9ymny,self.MachineLearning,Where ML is going in 2019?,https://www.reddit.com/r/MachineLearning/comments/a9ymny/where_ml_is_going_in_2019/,Andrey_Khakhariev,1545911671,[removed],0,1,False,self,,,,,
1442,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,21,a9yp6c,self.MachineLearning,[D] Where ML is going in 2019?,https://www.reddit.com/r/MachineLearning/comments/a9yp6c/d_where_ml_is_going_in_2019/,Andrey_Khakhariev,1545912295,"Any thoughts on major tech &amp; business trends? Any breakthroughs around the corner?

Also, what are you going to learn in 2019 to strengthen your ML skills?",45,1,False,self,,,,,
1443,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,21,a9yq5e,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Dec. 27, 2018",https://www.reddit.com/r/MachineLearning/comments/a9yq5e/n_weekly_machine_learning_opensource_roundup_dec/,stkim1,1545912556,,0,1,False,default,,,,,
1444,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,21,a9ysek,self.MachineLearning,"Of late there are lots of tools and platforms for machine learning. Have you come across any good material (books, videos, tutorials) on how best to use these tools in day to day ML development?",https://www.reddit.com/r/MachineLearning/comments/a9ysek/of_late_there_are_lots_of_tools_and_platforms_for/,abdush,1545913159,[removed],0,1,False,self,,,,,
1445,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,21,a9ywtc,youtube.com,"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educative",https://www.reddit.com/r/MachineLearning/comments/a9ywtc/this_video_goes_over_a_model_that_predicts_the/,antaloaalonso,1545914396,,0,1,False,default,,,,,
1446,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,21,a9yzaa,youtube.com,"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educative",https://www.reddit.com/r/MachineLearning/comments/a9yzaa/this_video_goes_over_a_model_that_predicts_the/,antaloaalonso,1545915068,,0,1,False,default,,,,,
1447,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,21,a9z0ma,madridsoftwaretrainings.com,Why Machine Learning with Python is the Best Career Option Nowadays?,https://www.reddit.com/r/MachineLearning/comments/a9z0ma/why_machine_learning_with_python_is_the_best/,rahuldas120,1545915417,,0,1,False,https://b.thumbs.redditmedia.com/Tivr1QM9Yl8C1FvC3PJImiW6GHD4KtC4XiA6duGqVyU.jpg,,,,,
1448,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,21,a9z0zj,activewizards.com,Comparison of the Top Speech Processing APIs,https://www.reddit.com/r/MachineLearning/comments/a9z0zj/comparison_of_the_top_speech_processing_apis/,viktoriia_shulga,1545915524,,0,1,False,default,,,,,
1449,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,22,a9z24s,gengo.ai,12 Best AI &amp; machine learning articles of 2018,https://www.reddit.com/r/MachineLearning/comments/a9z24s/12_best_ai_machine_learning_articles_of_2018/,reimmoriks,1545915813,,0,1,False,https://b.thumbs.redditmedia.com/a30lohV9-R84LxIIyhEdHmDfRaZzgQOQv06flsBLpRg.jpg,,,,,
1450,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,22,a9z3j4,self.MachineLearning,Publicly available histopathology image datasets for Breast cancer classification task.,https://www.reddit.com/r/MachineLearning/comments/a9z3j4/publicly_available_histopathology_image_datasets/,Arvo007,1545916157,[removed],0,1,False,self,,,,,
1451,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,22,a9z5nc,arxiv.org,[R] Can VAEs Generate Novel Examples?,https://www.reddit.com/r/MachineLearning/comments/a9z5nc/r_can_vaes_generate_novel_examples/,IborkedyourGPU,1545916691,,19,1,False,default,,,,,
1452,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,22,a9ze40,self.neuralnetworks,Comparing trained neural network models,https://www.reddit.com/r/MachineLearning/comments/a9ze40/comparing_trained_neural_network_models/,DeutschlandMehl,1545918807,,1,1,False,default,,,,,
1453,MachineLearning,t5_2r3gv,2018-12-27,2018,12,27,23,a9zp7q,self.MachineLearning,[Discussion] Have there been any networks that tries to analyze images in the same way as a CNN without being given the underlying structure of a image?,https://www.reddit.com/r/MachineLearning/comments/a9zp7q/discussion_have_there_been_any_networks_that/,mrconter1,1545921384,"Have there been any work that tries to analyze data in the same ways as a CNN but doesn't make assumptions about the data. A CNN is designed to take advantage of the 2D structure of a image. In other words, are there any networks that can learn the underlying data structure of a image and make predictions based on that? Being given only an array of color values.",3,1,False,self,,,,,
1454,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,0,aa044c,self.MachineLearning,[D] What are some modern Generative Models that allow for sampling as well as calculating the probability of generated samples,https://www.reddit.com/r/MachineLearning/comments/aa044c/d_what_are_some_modern_generative_models_that/,collegemathtutor,1545924596,"Hi, I am a bit green in the field of generative models, and with all the GAN hype going around, I was curious what models can do what GANs cannot - get the probability of the generated samples. 

The only models I know like this are GMMs, but what are the recent developments in these type of models where we can get the probability as well as the samples (on complex data)? ",6,1,False,self,,,,,
1455,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,0,aa0502,self.MachineLearning,[D] Which tool has saved the most time for you while developing your ML engine?,https://www.reddit.com/r/MachineLearning/comments/aa0502/d_which_tool_has_saved_the_most_time_for_you/,abdush,1545924771,,12,1,False,self,,,,,
1456,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,0,aa0c55,self.MachineLearning,[D] Impact of inclusion or exclusion of bias on the convergence of policy gradients,https://www.reddit.com/r/MachineLearning/comments/aa0c55/d_impact_of_inclusion_or_exclusion_of_bias_on_the/,invertedpassion,1545926208,"Hello,

I've been trying to implement policy gradients in Pytorch based on [this tutorial](http://karpathy.github.io/2016/05/31/rl/). I decided to write my own version using convolutional layers on PyTorch, and not simply copy the code. 

I was struggling to get my Pong to work, so I tried a simpler problem first (CartPole). I roughly followed this tutorial for [CartPole](https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf) but wrote my code from scratch. My version wasn't learning (convergence wasn't happening). I was struggling to see where I was going wrong. After doing a line by line comparison of my code with the tutorial, I realized the only major difference was that there were no bias values for the neural network in the tutorial. The author had initialized network weights only:

&amp;#x200B;

\&gt; self.l1 = nn.Linear(self.state\_space, 128, bias=False)

&amp;#x200B;

I made a similar change in my code (made bias=False in all my layers) and it started training really well. Then I did the same in my code for Pong agent and it started training really well. Then I realized that even the tutorial for Pong didn't have any biases in neural network, only weights.

&amp;#x200B;

I'm really surprised how big of an impact removal of 'bias' had on network's convergence. Anyone here has an intuition for why should that be? This was not at all obvious to me and I spent full two days trying to figure out where I was going wrong.

&amp;#x200B;

On a similar note, are there any best practices for debugging deep networks? I'm new at this and I'm not sure if I hadn't found tutorials, I would have figured out what was wrong with my model.",6,1,False,self,,,,,
1457,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,0,aa0cz0,deeplearningio.com,What Is Apriori Algorithm? | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aa0cz0/what_is_apriori_algorithm_deep_learning_io/,mrcgllr,1545926379,,0,1,False,https://b.thumbs.redditmedia.com/rmtX16pP2d3jBbcV__x9gtbwHduSgGfRskloSmPa4Js.jpg,,,,,
1458,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,1,aa0d5s,self.MachineLearning,[D] State of Bayesian Deep Learning for (end of) 2018?,https://www.reddit.com/r/MachineLearning/comments/aa0d5s/d_state_of_bayesian_deep_learning_for_end_of_2018/,tsauri,1545926416,"So far I see that last year Bayesian ML is pool of largely questionable theories   
previous discussion.  
[https://www.reddit.com/r/MachineLearning/comments/7bm4b2/d\_what\_is\_the\_current\_state\_of\_dropout\_as/](https://www.reddit.com/r/MachineLearning/comments/7bm4b2/d_what_is_the_current_state_of_dropout_as/)  


Any new useful papers that actually legit, no some shady bogus math involved?",5,1,False,self,,,,,
1459,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,1,aa0e0y,self.MachineLearning,[D] Depth First Learning Fellowship: $4000 grants to build ML curricula,https://www.reddit.com/r/MachineLearning/comments/aa0e0y/d_depth_first_learning_fellowship_4000_grants_to/,cinjon,1545926584,"Hi all! We posted this earlier and are bringing it back just in case folks missed it in the pre-holiday rush. We're actively looking now for fellows so please don't hesitate if you're interested.

Were researchers from NYU, FAIR, Google Brain, and and DeepMind. Last year, we designed Depth First Learning, a pedagogy for diving deep into ML by carefully tailoring a curriculum around a particular ML paper or concept and leading small, focused discussion groups. So far, weve created guides for [InfoGAN](http://www.depthfirstlearning.com/2018/InfoGAN), [TRPO](http://www.depthfirstlearning.com/2018/TRPO), [AlphaGoZero](http://www.depthfirstlearning.com/2018/AlphaGoZero), and [DeepStack](http://www.depthfirstlearning.com/2018/DeepStack).

Since our launch, weve received very positive feedback from students and researchers. Now, we want to run new, online classes around the world. Thanks to the generosity of Jane Street, we will provide four fellows each with a $4000 grant to build a six-week curriculum and run weekly on-line discussions.

If youd like to lead a class about an important paper in machine learning, please visit [http://fellowship.depthfirstlearning.com](http://fellowship.depthfirstlearning.com/) to apply. We look forward to hearing from you!",6,1,False,self,,,,,
1460,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,1,aa0ema,self.MachineLearning,Starting a DL/RL research lab in India,https://www.reddit.com/r/MachineLearning/comments/aa0ema/starting_a_dlrl_research_lab_in_india/,drkolenklow,1545926686,[removed],0,1,False,self,,,,,
1461,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,1,aa0kuk,youtube.com,Why Not Just: Think of AGI Like a Corporation?,https://www.reddit.com/r/MachineLearning/comments/aa0kuk/why_not_just_think_of_agi_like_a_corporation/,Mynameis__--__,1545927885,,0,1,False,default,,,,,
1462,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,1,aa0sf4,self.MachineLearning,Neural net to alter facial features,https://www.reddit.com/r/MachineLearning/comments/aa0sf4/neural_net_to_alter_facial_features/,throwaway201084,1545929313,How can I create a neural net to alter facial features? Creating a smaller nose or a wider mouth for instance. I've read about face arithmetic on DCGAN but I'm not sure how to apply it. At what point do the vectors get added/subtracted?,0,1,False,self,,,,,
1463,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,2,aa189d,self.MachineLearning,[P] Transfer Learning - SpongeBob SquarePants Character Recogniser,https://www.reddit.com/r/MachineLearning/comments/aa189d/p_transfer_learning_spongebob_squarepants/,Laboratory_one,1545932186,"[Project Blog Post](http://labone.tech/transfer-learning/)

I'm rather disillusioned by the current applications of ML. There seems to be a distinct lack of applications on important topics /s. 

I give you... a SpongeBob SquarePants character recogniser!!! Finally, we can know who we are looking at :). 

I used transfer learning of the VGG19 to build this. I hope you enjoy this tongue-in-cheek toy project.",14,1,False,self,,,,,
1464,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,2,aa1avj,deeplearningio.com,Deep Learning Mobile Application Development  Part 1: Conversion of Keras Trained Model to TensorFlow | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aa1avj/deep_learning_mobile_application_development_part/,mrcgllr,1545932648,,0,1,False,https://b.thumbs.redditmedia.com/chtjjb6n7G3JSuIysBkp_8UwGtIuaj9um5EEQBs9xdg.jpg,,,,,
1465,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,2,aa1d0t,self.MachineLearning,[Question] Covariate shift,https://www.reddit.com/r/MachineLearning/comments/aa1d0t/question_covariate_shift/,cipst,1545933019,[removed],0,1,False,self,,,,,
1466,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,3,aa1ymn,self.MachineLearning,compute output uncertainty in DL - a VAE-like linear regression,https://www.reddit.com/r/MachineLearning/comments/aa1ymn/compute_output_uncertainty_in_dl_a_vaelike_linear/,jmlbeau,1545936753,[removed],0,1,False,self,,,,,
1467,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,4,aa28ni,self.MachineLearning,Looking for experiments/application areas where DBNs outperform VAEs and/or GANs,https://www.reddit.com/r/MachineLearning/comments/aa28ni/looking_for_experimentsapplication_areas_where/,se4u,1545938502,[removed],0,1,False,self,,,,,
1468,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,4,aa2bw6,self.MachineLearning,[P] Machine Learning Web App,https://www.reddit.com/r/MachineLearning/comments/aa2bw6/p_machine_learning_web_app/,aiproject3939,1545939078,"Hey all,

I'm a front-end web developer looking for some partners to start a small side project with a significant component involving AI / Machine Learning. This is a long shot - but if you fit the description and have a few hours to spend with me on something cool, please PM me! Not too comfortable discussing project in public.
",2,1,False,self,,,,,
1469,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,5,aa2puy,self.MachineLearning,High School Interning,https://www.reddit.com/r/MachineLearning/comments/aa2puy/high_school_interning/,Northern_Reaches,1545941491,[removed],0,1,False,self,,,,,
1470,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,5,aa2rxj,self.MachineLearning,[question] do convolutional layers themselves get trained or are they more like a mathematical transform?,https://www.reddit.com/r/MachineLearning/comments/aa2rxj/question_do_convolutional_layers_themselves_get/,slugsnot,1545941857,,0,1,False,self,,,,,
1471,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,5,aa2u4v,self.datascience,Got any data-sciency questions you would like answered?,https://www.reddit.com/r/MachineLearning/comments/aa2u4v/got_any_datasciency_questions_you_would_like/,tommos0,1545942254,,0,1,False,default,,,,,
1472,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,5,aa34bk,self.MachineLearning,[P] Conditional AnimeGAN - Generating Anime faces conditioned on eyes and hair color.,https://www.reddit.com/r/MachineLearning/comments/aa34bk/p_conditional_animegan_generating_anime_faces/,Natsu6767,1545944115,"Recently I have been reading about Generative Adversarial Networks (GANs) and find them really fascinating. So after going through many papers, articles and mathematical derivations, I decided to practically implement one for myself. Instead of using one of the ""standard"" datasets (MNIST, CIFAR-10, CelebA, etc.), I decided to play around with a dataset on Anime character faces. One of the most fun and entertaining projects I've done so far!

&amp;#x200B;

Link: [https://github.com/Natsu6767/Conditional-AnimeGAN](https://github.com/Natsu6767/Conditional-AnimeGAN)

&amp;#x200B;

Now, coding up a GAN architecture is pretty simple. The hard part is training it; more precisely finding good hyperparameters. To add to this the large training time and progress becomes slow and stressful. However, after finally managing to train the model, here are some things I learned:

1. Use Leaky ReLU for the generator as well. Many papers point out using Leaky ReLU for the discriminator and just use plain ReLU for the generator. However, using a Leaky ReLU in both places gives better results.
2. While training a cGAN add to the loss of the discriminator a term corresponding to using real images with fake/wrong labels (conditions). The discriminator should classify this form of the input as ""fake"". This enforces the network to make use of the conditions and not just simply ignore them.
3. If the discriminator is doing too well using weight clipping for the weights of the discriminator helps. I experimented with this and got slightly better results but finding the right cut-off can be difficult. In the end, I decided not to use it.

***It's crazy to see a machine drawing better than you by itself!***

&amp;#x200B;

*(The generated images aren't the prettiest and the conditions sometimes don't seem to work, however, it's still fun to see the generated images. One problem with the dataset I used was that half of the images didn't have labels and there were about 33K images only.)*",0,1,False,self,,,,,
1473,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,6,aa36z2,medium.com,[D] Can AI Judge a Paper on Appearance Alone?,https://www.reddit.com/r/MachineLearning/comments/aa36z2/d_can_ai_judge_a_paper_on_appearance_alone/,gwen0927,1545944593,,0,1,False,default,,,,,
1474,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,6,aa3b5o,self.MachineLearning,[P] Conditional AnimeGAN - Generating Anime faces conditioned on eye and hair color.,https://www.reddit.com/r/MachineLearning/comments/aa3b5o/p_conditional_animegan_generating_anime_faces/,Natsu6767,1545945359,"Recently I have been reading about Generative Adversarial Networks (GANs) and find them really fascinating. So after going through many papers, articles and mathematical derivations, I decided to practically implement one for myself. Instead of using one of the ""standard"" datasets (MNIST, CIFAR-10, CelebA, etc.), I decided to play around with a dataset on Anime character faces. One of the most fun and entertaining projects I've done so far!

Link: [https://github.com/Natsu6767/Conditional-AnimeGAN](https://github.com/Natsu6767/Conditional-AnimeGAN)

Now, coding up a GAN architecture is pretty simple. The hard part is training it; more precisely finding good hyperparameters. To add to this the large training time and progress becomes slow and stressful. However, after finally managing to train the model, here are some things I learned:

1. Use Leaky ReLU for the generator as well. Many papers point out using Leaky ReLU for the discriminator and just use plain ReLU for the generator. However, using a Leaky ReLU in both places gives better results.
2. While training a cGAN add to the loss of the discriminator a term corresponding to using real images with fake/wrong labels (conditions). The discriminator should classify this form of the input as ""fake"". This enforces the network to make use of the conditions and not just simply ignore them.
3. If the discriminator is doing too well using weight clipping for the weights of the discriminator helps. I experimented with this and got slightly better results but finding the right cut-off can be difficult. In the end, I decided not to use it.

***It's crazy to see a machine drawing better than you by itself!***

&amp;#x200B;

*(The generated images aren't the prettiest and the conditions sometimes don't seem to work, however, it's still fun to see the generated images. One problem with the dataset I used was that half of the images didn't have labels and there were about 33K images only.)*",22,1,False,self,,,,,
1475,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,6,aa3dg0,youtube.com,LinearRegression Machine Learning Chapter 4 Real Time Signals Edjio,https://www.reddit.com/r/MachineLearning/comments/aa3dg0/linearregression_machine_learning_chapter_4_real/,rajeevdoi,1545945775,,0,1,False,default,,,,,
1476,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,6,aa3jq6,self.MachineLearning,Custom built image classification system - looking to hire someone,https://www.reddit.com/r/MachineLearning/comments/aa3jq6/custom_built_image_classification_system_looking/,smackey,1545946927,[removed],0,1,False,self,,,,,
1477,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,8,aa4lb0,lifehfcquincy.com,L.I.F.E Health &amp;amp; Fitness Center - Your One Stop Shop for Fitness!,https://www.reddit.com/r/MachineLearning/comments/aa4lb0/life_health_amp_fitness_center_your_one_stop_shop/,callienmflamenc,1545953946,,0,1,False,default,,,,,
1478,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,9,aa52op,self.MachineLearning,[P] Computer Vision: Object Detection and Segmentation with Mask R-CNN,https://www.reddit.com/r/MachineLearning/comments/aa52op/p_computer_vision_object_detection_and/,seemingly_omniscient,1545957294,"Object Detection and image segmentation with Mask R-CNN and COCO dataset. Source video clips are shot in Frankfurt am Main, Germany. Hope you like it.

&amp;#x200B;

[Demo Video](https://youtu.be/akK5ui-vel0)",3,1,False,self,,,,,
1479,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,9,aa54ob,arxiv.org,[R] Training Deep Capsule Networks,https://www.reddit.com/r/MachineLearning/comments/aa54ob/r_training_deep_capsule_networks/,Turing__Incomplete,1545957690,,6,1,False,default,,,,,
1480,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,10,aa5oiw,self.MachineLearning,[D] Inter-stream vs intra-stream parallelization,https://www.reddit.com/r/MachineLearning/comments/aa5oiw/d_interstream_vs_intrastream_parallelization/,throwohhaimark2,1545961764,"I was reading a [paper](https://arxiv.org/pdf/1503.02852.pdf) about parallelizing RNNs. Their claim seemed to be that traditionally inter-stream parallelization (training multiple copies of an RNN at the same time) is more tractable since the RNN graph has cycles in it. They go on to describe a method of converting the graph into a DAG so they can do intra-stream parallelization on it more easily.

My question: don't already GPUs automatically give a substantial speedup in the intra-stream parallelization sense, since GPUs can do the matrix multiplications *within* each node much faster? I get that the authors' approach to intra-stream parallelization is different, but they make it seem like this naive form of parallelization doesn't exist. Or am I misunderstanding something?",3,1,False,self,,,,,
1481,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,11,aa5rim,self.MachineLearning,[D] How do you deal with getting stuck and learning new features for Tensorflow and Pytorch besides StackOverflow? Is there some sort of paid help we could look into?,https://www.reddit.com/r/MachineLearning/comments/aa5rim/d_how_do_you_deal_with_getting_stuck_and_learning/,AdditionalWay,1545962417,"I've noticed that a lot of hours are being burned navigating through Tensorflow. Stackoverflow is great, but sometimes takes a while to get a response, sometimes we get none at all. 

Is there some sort of paid service we could look into? Express solutions and 1-one-1 explanations, answering any questions. Something like that would be well worth the money, if it meant saving a ton of time on this aspects of ML. ",0,1,False,self,,,,,
1482,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,11,aa619e,self.MachineLearning,Advice for doing research as an undergrad?,https://www.reddit.com/r/MachineLearning/comments/aa619e/advice_for_doing_research_as_an_undergrad/,EveningAlgae,1545964473,"Hi,

&amp;#x200B;

I've recently got the opportunity to do reinforcement learning projects with a prof at my school. I'm pretty excited and I've been refreshing myself going through Sutton and Barto throughout the holidays. I am also implementing stuff I've read in Sutton and Barto in OpenAI's Gym, and I'm making a Github repo containing all of it, along with some tex'd up notes.

Once you have this base of knowledge as an undergrad, what would you advise him to do to make themselves as useful as possible to their advisor? What kind of stuff can you try to do to make yourself less of a burden on whoever's mentoring you? What are the major dos and don'ts when it comes to making progress on research in this field, if any, that are specific to ML?

I'd really appreciate any advice any of you would have to offer; my goal is to hopefully push our knowledge of RL a tiny bit, culminating in a paper, but really I'd be happy just working with the prof for a long time, learning as much as I can about the field.",0,1,False,self,,,,,
1483,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,12,aa6ogu,youtube.com,"Sparlab is an open-source project that allows fighting gamers to create their own bots to train against, experiment with different combinations of input, and more. How can machine learning improve the user's training experience? What do you guys think about this tool? (Github link in the comments)",https://www.reddit.com/r/MachineLearning/comments/aa6ogu/sparlab_is_an_opensource_project_that_allows/,realjohnward,1545969373,,1,1,False,https://b.thumbs.redditmedia.com/k3uMu_E-JFppihB1kI2GNEwW4x3wkehjw8tJ1QiEZTg.jpg,,,,,
1484,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,13,aa6rfc,youtube.com,Sparlab is an open-source project that allows fighting gamers to create their own bots to train against and experiment with various combinations of input. What do you guys think? How can machine learning be applied to improve the user's training experience? (Github link in comments),https://www.reddit.com/r/MachineLearning/comments/aa6rfc/sparlab_is_an_opensource_project_that_allows/,realjohnward,1545970020,,1,1,False,https://b.thumbs.redditmedia.com/k3uMu_E-JFppihB1kI2GNEwW4x3wkehjw8tJ1QiEZTg.jpg,,,,,
1485,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,14,aa797w,link.medium.com,I am writing blogs on reinforcement learning give it a look.,https://www.reddit.com/r/MachineLearning/comments/aa797w/i_am_writing_blogs_on_reinforcement_learning_give/,sanchit2843,1545973972,,0,1,False,default,,,,,
1486,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,14,aa7hrm,nature.com,"[Research]Machine learning provides extensive insight into the global determinants of enzyme turnover numbers and improve the understanding of the kinetom and thus, the quantitative proteome of E.coli.",https://www.reddit.com/r/MachineLearning/comments/aa7hrm/researchmachine_learning_provides_extensive/,MistWeaver80,1545976018,,0,1,False,default,,,,,
1487,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,14,aa7kg5,github.com,My own documentation on how to train PJReddies YOLOv3 to detect specific objects using Google's OpenImagesV4,https://www.reddit.com/r/MachineLearning/comments/aa7kg5/my_own_documentation_on_how_to_train_pjreddies/,Oswald_Hydrabot,1545976655,,0,1,False,https://b.thumbs.redditmedia.com/KYc6btMVtwvvAryMfBy7LYyLJqPwl4N2vx-zr1-I9ds.jpg,,,,,
1488,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,15,aa7phs,self.MachineLearning,Salary question: Am I being low-balled? [D],https://www.reddit.com/r/MachineLearning/comments/aa7phs/salary_question_am_i_being_lowballed_d/,RainyScraper,1545977830,"Im evaluating an offer from a major tech company in Seattle for a data science role. Its for a position in a research group with a heavy focus on neural nets/deep learning. I have a PhD and several years post-PhD experience as an academic. The job sounds extremely interesting,  it the base salary and sign-on bonus are much lower than I have seen on Glassdoor, Paysa, and a number of online forums (Reddit and teamblind).

Anyone in data science at one of the big players have any insight about negotiating offers? I feel kind of in the dark, and dont want to insult them by telling them that their offer is insulting.

I dont have any other offers in-hand right now, but I have three very promising interviews lined up for mid-January, after the holidays. ",191,1,False,self,,,,,
1489,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,15,aa7z3n,self.MachineLearning,"LSTM and RNN Tutorial with Demo (with Stock/Bitcoin Time Series Prediction, Sentiment Analysis, Music Generation)",https://www.reddit.com/r/MachineLearning/comments/aa7z3n/lstm_and_rnn_tutorial_with_demo_with_stockbitcoin/,obsezer,1545980178,"I updated tutorial. This tutorial  summarizes all of them. In this tutorial, RNN Cell, RNN Forward and    Backward Pass, LSTM Cell, LSTM Forward Pass, Sample LSTM Project:    Prediction of Stock Prices Using LSTM network, Sample LSTM Project:    Sentiment Analysis, Sample LSTM Project: Music Generation. It will    continue to be updated over time.

[https://github.com/omerbsezer/LSTM\_RNN\_Tutorials\_with\_Demo](https://github.com/omerbsezer/LSTM_RNN_Tutorials_with_Demo)

Extra:  Reinforcement Learning (RL) &amp; Deep RL Tutorial With Sample Codes/Demo

[https://github.com/omerbsezer/Reinforcement\_learning\_tutorial\_with\_demo](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo)",0,1,False,self,,,,,
1490,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,16,aa89kt,envisioninteligence.com,"Machine Learning as a Service (Mlaas) Market  Size, Outlook, Trends and Forecasts (2018  2024)",https://www.reddit.com/r/MachineLearning/comments/aa89kt/machine_learning_as_a_service_mlaas_market_size/,srikanthenvision,1545982933,,0,1,False,default,,,,,
1491,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,16,aa8b7j,self.MachineLearning,Anyone looking to join my Kaggle team for the Humpback Whale Competition?,https://www.reddit.com/r/MachineLearning/comments/aa8b7j/anyone_looking_to_join_my_kaggle_team_for_the/,that_one_ai_nerd,1545983421,[removed],0,1,False,self,,,,,
1492,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,17,aa8hlz,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/aa8hlz/global_machine_learning_market_size_outlook/,env8985,1545985266,[removed],0,1,False,self,,,,,
1493,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,17,aa8jgj,marktechpost.com,Machine Learning can be used to Predict Peak Medical Emergency Times,https://www.reddit.com/r/MachineLearning/comments/aa8jgj/machine_learning_can_be_used_to_predict_peak/,asifrazzaq1988,1545985850,,0,1,False,default,,,,,
1494,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,18,aa8rzg,openpr.com,"Currency Sorter Market by Type, Product Type, Technology, Industry Vertical, Leading Players, Application and Forecast Till 2019-2023",https://www.reddit.com/r/MachineLearning/comments/aa8rzg/currency_sorter_market_by_type_product_type/,Swati_Shinde,1545988402,,0,1,False,default,,,,,
1495,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,19,aa92bf,self.MachineLearning,What approach should I use to group similar looking textures?,https://www.reddit.com/r/MachineLearning/comments/aa92bf/what_approach_should_i_use_to_group_similar/,prLone,1545991526,"I have bunch of textures of tiles and carpets like the ones below. I would like to group them together on the basis of how similar they look. The two types of similarity I am looking for are colors and the pattern on them. I have tried using k-means and GMM but they have been not so effective. Can anyone please guide me to what should I look towards to accomplish this. FYI I have 1000s of such images without any labeling to take supervised learning into consideration. Any help would be appreciated. Thanks!

&amp;#x200B;

*Processing img ms2msh3hsz621...*

*Processing img kdrhzu3hsz621...*

*Processing img xt1yau3hsz621...*

*Processing img q2z5vt3hsz621...*",0,1,False,self,,,,,
1496,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,19,aa956y,self.MachineLearning,Bill Vorhies Retrospective: Part 5,https://www.reddit.com/r/MachineLearning/comments/aa956y/bill_vorhies_retrospective_part_5/,andrea_manero,1545992354,[removed],0,1,False,self,,,,,
1497,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,19,aa96zt,medium.com,NLP 2018 Highlights (PDF Report),https://www.reddit.com/r/MachineLearning/comments/aa96zt/nlp_2018_highlights_pdf_report/,omarsar,1545992883,,0,1,False,https://a.thumbs.redditmedia.com/D3EHcwddKGgx9bLeggS77CiPepNtYjvOkbjVy32nDQ4.jpg,,,,,
1498,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,19,aa98is,greyatom.com,"I came across this structured study plan for Data Science. Every week they send an email with Jupyter Notebooks with concepts,practice assignments, book recommendations, blogs etc. Personally, I use it as a reminder that I need to get back to work and stop slacking off.",https://www.reddit.com/r/MachineLearning/comments/aa98is/i_came_across_this_structured_study_plan_for_data/,iampa1,1545993336,,0,1,False,https://b.thumbs.redditmedia.com/6BHvhD-MYXpaBWV3kkr-xfEXUIoQiO2lpZRED8wmhcw.jpg,,,,,
1499,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,20,aa9n2c,self.MachineLearning,Neural Network in C-Sharp problems,https://www.reddit.com/r/MachineLearning/comments/aa9n2c/neural_network_in_csharp_problems/,Realseppy,1545997459,[removed],0,1,False,self,,,,,
1500,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,21,aa9rji,greyatom.com,Join study plan... Study material for Machine learning delivered weekly for 26 weeks,https://www.reddit.com/r/MachineLearning/comments/aa9rji/join_study_plan_study_material_for_machine/,mshilotri,1545998675,,0,1,False,https://b.thumbs.redditmedia.com/6BHvhD-MYXpaBWV3kkr-xfEXUIoQiO2lpZRED8wmhcw.jpg,,,,,
1501,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,22,aaa6vb,self.MachineLearning,Anyone worked/working on Face Anti-spoofing?,https://www.reddit.com/r/MachineLearning/comments/aaa6vb/anyone_workedworking_on_face_antispoofing/,denny_arjun,1546002696,[removed],0,1,False,self,,,,,
1502,MachineLearning,t5_2r3gv,2018-12-28,2018,12,28,23,aaakxb,i.redd.it,Tecnologia 4.0 e PMBOK V,https://www.reddit.com/r/MachineLearning/comments/aaakxb/tecnologia_40_e_pmbok_v/,JamurGerloff,1546006003,,0,1,False,https://b.thumbs.redditmedia.com/UCSEAC7_gwoqEZ3eSs24d1S7kYxcOf6WKxq5njo7nYM.jpg,,,,,
1503,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,0,aab9gv,self.MachineLearning,A question regarding implementation,https://www.reddit.com/r/MachineLearning/comments/aab9gv/a_question_regarding_implementation/,alias_is,1546011205,[removed],0,1,False,self,,,,,
1504,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,0,aabesg,self.MachineLearning,How Would You Build An Automated Grocery Store ?,https://www.reddit.com/r/MachineLearning/comments/aabesg/how_would_you_build_an_automated_grocery_store/,DiscoDiggy,1546012259,[removed],0,1,False,self,,,,,
1505,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,0,aabg3g,self.MachineLearning,[R][ICLR] Hierarchical interpretations for neural network predictions,https://www.reddit.com/r/MachineLearning/comments/aabg3g/riclr_hierarchical_interpretations_for_neural/,shi223,1546012511,"[https://openreview.net/forum?id=SkEqro0ctQ](https://openreview.net/forum?id=SkEqro0ctQ)

**Abstract:** Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.

**TL;DR:** We introduce and validate hierarchical local interpretations, the first technique to automatically search for and display important interactions for individual predictions made by LSTMs and CNNs.

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/st2y9nbdj1721.png",0,1,False,https://a.thumbs.redditmedia.com/bA-cN2Ot3k6-3A7eCyjdZ8c1XjTUgaUxEEfkxgqHbU0.jpg,,,,,
1506,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,1,aabji0,self.MachineLearning,[R][ICLR] Hierarchical interpretations for neural network predictions,https://www.reddit.com/r/MachineLearning/comments/aabji0/riclr_hierarchical_interpretations_for_neural/,shi223,1546013132,"[https://openreview.net/forum?id=SkEqro0ctQ](https://openreview.net/forum?id=SkEqro0ctQ)

**Abstract:** Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.

**TL;DR:** We introduce and validate hierarchical local interpretations, the first technique to automatically search for and display important interactions for individual predictions made by LSTMs and CNNs.",0,1,False,self,,,,,
1507,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,1,aabn3k,self.MachineLearning,If you were to chose ONE university to do your PhD,https://www.reddit.com/r/MachineLearning/comments/aabn3k/if_you_were_to_chose_one_university_to_do_your_phd/,Naeph,1546013794,[removed],1,1,False,self,,,,,
1508,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,1,aac0nq,self.MachineLearning,[D] If you were to choose ONE university to pursue a Phd,https://www.reddit.com/r/MachineLearning/comments/aac0nq/d_if_you_were_to_choose_one_university_to_pursue/,Naeph,1546016331,"Which one would you chose and why ?

Stanford ?
CMU ?
MIT ? 
NYU ?
Toronto ?
Oxford ?

Regarding the quality and the quantity of the publications, the prestige of the researchers, the market value of the fresh graduates, the fellowhips, the proximity with best labs, and so on.
",14,1,False,self,,,,,
1509,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,2,aac1vd,self.MachineLearning,[D] Question about which ML algorithms to learn,https://www.reddit.com/r/MachineLearning/comments/aac1vd/d_question_about_which_ml_algorithms_to_learn/,Pawnbrake,1546016536,"I have an employee who is to spend the next month or two learning machine learning algorithms for general use.  I wanted to ask the community: which algorithms would be best (assuming he's starting from scratch, but he has a strong background in calculus and linear algebra with basic statistics knowledge)?

My idea was to start with linear and logistic regressions, move to clustering, touch on neural networks and build a simple MNIST predictor, then go to random forests, svms, and not sure what's next.  Any suggestions?",12,1,False,self,,,,,
1510,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,2,aacdbk,self.MachineLearning,[R] A Style-Based Generator Architecture for GANs - Generating and Tuning Realistic Artificial Faces,https://www.reddit.com/r/MachineLearning/comments/aacdbk/r_a_stylebased_generator_architecture_for_gans/,ranihorev,1546018595,"Hey, I wrote a summary of NVIDIA's new paper, A Style-Based Generator Architecture for Generative Adversarial Networks. The paper presents a new technique to generate high-quality images of faces (and cars and bedrooms...), but even more importantly, to control different aspects of the image from face shape to hair color. It also shows how to combine multiple images (of faces) into one image and how to pick which features come from which image. 

[https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)

&amp;#x200B;

It's a fairly complicated topic and I'd love to get you feedback on it. 

&amp;#x200B;",1,1,False,self,,,,,
1511,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,2,aacile,self.MachineLearning,"Different types of approaches in machine learning apart from classification, clustering,regression and heuristic",https://www.reddit.com/r/MachineLearning/comments/aacile/different_types_of_approaches_in_machine_learning/,Ankeetshk,1546019555,[removed],0,1,False,self,,,,,
1512,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,2,aackhh,sunps.blogspot.com,"Top 10 companies using Machine Learning, Its commonly believed that smaller startups are generally more dynamic and more innovative than larger, established market leaders.",https://www.reddit.com/r/MachineLearning/comments/aackhh/top_10_companies_using_machine_learning_its/,sunps,1546019888,,0,1,False,default,,,,,
1513,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,3,aacwq0,twitter.com,Toren [Online Game Code] is 75% OFF,https://www.reddit.com/r/MachineLearning/comments/aacwq0/toren_online_game_code_is_75_off/,JosefinaHand34,1546022030,,0,1,False,https://b.thumbs.redditmedia.com/p-UWVy69OMDZ63rE5ptjyJ2wwaKJEW9WnM99Rwe5y-Y.jpg,,,,,
1514,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,4,aadhf5,self.MachineLearning,[D] Are the problems with training GANs surmountable?,https://www.reddit.com/r/MachineLearning/comments/aadhf5/d_are_the_problems_with_training_gans_surmountable/,abstractcontrol,1546025683,"Reading [this article](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b) on why is it so hard to train GANs made me wonder about this. The fact that cost functions might not converge in a minimax game with gradient descent seems especially bad for GANs. That having said, I am not that familiar with them or follow the research on them are so I am asking here.

Will GANs or AEs (variational or otherwise) come out on top in the long term? If so, why?",10,1,False,self,,,,,
1515,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,4,aadk71,self.MachineLearning,Google/Nvidia Paper About Machines Writing Most Code?,https://www.reddit.com/r/MachineLearning/comments/aadk71/googlenvidia_paper_about_machines_writing_most/,ShallowLearner10101,1546026164,"Listened to Steve Jurvetson on the Tim Ferriss podcast talk about how most code at Google today is machine generated via neural networks:

&amp;#x200B;

[https://tim.blog/2018/06/26/the-tim-ferriss-show-transcripts-steve-jurvetson/](https://tim.blog/2018/06/26/the-tim-ferriss-show-transcripts-steve-jurvetson/)

&amp;#x200B;

He referenced a paper by Google and a summary by Nvidia about this - does anyone have a link?",0,1,False,self,,,,,
1516,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,5,aadq5f,medium.com,Programmatically access model versions - Comet.mls Python API Client,https://www.reddit.com/r/MachineLearning/comments/aadq5f/programmatically_access_model_versions_cometmls/,ceceshao1,1546027209,,0,1,False,https://b.thumbs.redditmedia.com/oJjInenuc1Af0kjmGfzkBcAtYDil-obEZPOaUwvkcaE.jpg,,,,,
1517,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,5,aadz68,self.MachineLearning,What after MIT 6.S191,https://www.reddit.com/r/MachineLearning/comments/aadz68/what_after_mit_6s191/,x-w-j,1546028791,[removed],0,1,False,self,,,,,
1518,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,6,aaeeqb,self.MachineLearning,[D] What after MIT 6.S191,https://www.reddit.com/r/MachineLearning/comments/aaeeqb/d_what_after_mit_6s191/,x-w-j,1546031560,"So what after the [MIT 6.S191](https://www.youtube.com/watch?v=NVH8EYPHi30&amp;index=3&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI)

I am in the middle of course now and the first lecture was really excellent on providing the basics and all the other lectures so far is very superficial to some extent -- yes, it does delve on some math but the lab are practically so far have been filling some filler codes. I would whole  heartedly recommend the MIT 6.S191 to anyone, incase if someone is a very beginner.

So if I want to dive deep in this domain, what would be the next stop?

I have read rave reviews on [Ian's book](http://www.deeplearningbook.org/), would that be considered as a comprehensive book if I want to build DIY models on DL? or any others? I haven't completed the ML in Coursera by Andrew Ng, indeed this (MIT YT CL) has been so far the first DL course wherever I have some foray into Analytics in Analytics Edge at MITX three years back but a lot has been update in the past three so I would love to build deep in DL as this particular interests me on a problem I am deeply involved in my career.",5,1,False,self,,,,,
1519,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,6,aaemlu,deeplearningio.com,Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aaemlu/deep_learning_io/,mrcgllr,1546032969,,0,1,False,default,,,,,
1520,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,7,aaew6n,self.MachineLearning,2019The Year of Further Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/aaew6n/2019the_year_of_further_machine_learning/,LityxIQ,1546034723,[removed],0,1,False,self,,,,,
1521,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,8,aafdse,self.MachineLearning,"[D] Conversation with Tuomas Sandholm, co-creator of Libratus, first AI system to beat top human players at heads-up poker",https://www.reddit.com/r/MachineLearning/comments/aafdse/d_conversation_with_tuomas_sandholm_cocreator_of/,UltraMarathonMan,1546038044,"Here's my conversation with Tuomas Sandholm, co-creator of Libratus, which is the first AI system to beat top human players at the game of Heads-Up No-Limit Texas Hold'em.

Podcast audio: [https://lexfridman.com/tuomas-sandholm](https://lexfridman.com/tuomas-sandholm)

Podcast video: [https://www.youtube.com/watch?v=b7bStIQovcY](https://www.youtube.com/watch?v=b7bStIQovcY)

Tuomas and team are using ML now, but I think it's illuminating just how little ML was used in the original version of Libratus that beat the human poker pros.",2,1,False,self,,,,,
1522,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,8,aafsua,self.MachineLearning,Issues with retinanet in PyTorch and TF ObjDet API.,https://www.reddit.com/r/MachineLearning/comments/aafsua/issues_with_retinanet_in_pytorch_and_tf_objdet_api/,taman_,1546040942,"Hi, I m using this implementation of retinanet in Pytorch ([https://github.com/kuangliu/pytorch-retinanet](https://github.com/kuangliu/pytorch-retinanet)) to detect dense small objects. After adjusting the anchor boxes generator to my use case this works great.

I want to put this model into production by loading it into an Amazon Lambda function with PyTorch CPU 0.4.0.

I get it working but the inference time is dead slow (&gt;15 minutes on max resources) for one image and I have no clue as to why.

I trained the same dataset with retinanet from Tensorflow object detection API. Loaded that as well into a AWS Lambda function and inference takes a few seconds. Why the extreme difference in inference times? 

I don't have any preference between PyTorch or TF (whatever gets the job done) but I can't seem to get the anchor boxes right in TF. 

It gives me a maximum of \~25 objects no matter what I change (aspect\_ratio, scale, etc) in the config file and the TF OD source code is quite complex to go through.

So I m basically stuck between a great model that is unworkable for inference and a shitty model that works great for inference. Anyone any ideas how to speed up pytorch in AWS lambda or how to configure TF OD retinanet anchorboxes outside the config file? 



&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1523,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,9,aag5n4,self.MachineLearning,Crypto Noobs /Junior Devs/Amateurs Working together to build a D.A.O.,https://www.reddit.com/r/MachineLearning/comments/aag5n4/crypto_noobs_junior_devsamateurs_working_together/,McNattyDread,1546043525,"Kaleidoscope is a Decentralized autonomous organization being built &amp; developed by crypt noobs /junior devs who are trying to take our ish seriously &amp; grow our skills. We are doing our best to make the most out of this bear market . Here is our website if interested in learning more. We also teach skills if you dont have any.

[https://kld.life/](https://kld.life/)

Mahalo.",0,1,False,self,,,,,
1524,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,10,aaguou,lambdalabs.com,First Titan RTX benchmarks for Machine Learning -- Titan RTX / V100 / 2080 Ti / 1080 Ti / Titan V / Titan Xp -- TensorFlow Performance,https://www.reddit.com/r/MachineLearning/comments/aaguou/first_titan_rtx_benchmarks_for_machine_learning/,ai_painter,1546048781,,0,1,False,https://b.thumbs.redditmedia.com/Uy8BpjLLigRO43cZmdSX3utdRZxxRMC4CHd18HFMI-w.jpg,,,,,
1525,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,11,aah01r,self.MachineLearning,[D] First Titan RTX benchmarks for Machine Learning -- Titan RTX / V100 / 2080 Ti / 1080 Ti / Titan V / Titan Xp -- TensorFlow Performance,https://www.reddit.com/r/MachineLearning/comments/aah01r/d_first_titan_rtx_benchmarks_for_machine_learning/,ai_painter,1546049927,"Lambda Labs just posted some nice TensorFlow benchmarks for various GPUs:

[Best GPU for Machine Learning: Titan RTX vs. Tesla V100 vs. 2080 Ti vs. 1080 Ti vs. Titan V vs. Titan Xp](https://lambdalabs.com/blog/titan-rtx-tensorflow-benchmarks/)

**The following GPUs are benchmarked:**

* Titan RTX !!
* RTX 2080 Ti
* GTX 1080 Ti
* Tesla V100
* Titan V
* Titan Xp

The 2080 Ti seems by far the best GPU in terms of price/performance (unless you need more than 11 GB of GPU memory).

**With FP32, doing some quick math using their raw data, the 2080 Ti is...**

* \~5% slower than the Titan RTX and 50% of the Titan RTX's price
* \~20% slower than the Tesla V100 and 15% of the V100's price!!
* \~2% slower than the Titan V and 40% of the Titan V's price.
* \~40% faster than the 1080 Ti and basically the same price as the 1080 Ti (on Amazon)
* \~24% faster than the Titan Xp and the same price as the Titan Xp

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",49,1,False,self,,,,,
1526,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,11,aah0nt,medium.com,Machine Learning Top 50 Articles for the Past Year (v.2019),https://www.reddit.com/r/MachineLearning/comments/aah0nt/machine_learning_top_50_articles_for_the_past/,kumeralex,1546050062,,0,1,False,https://b.thumbs.redditmedia.com/6YaqeEafvnbOEXAxL7jhV1ePNYrLn9wA7ss2sxcg8AM.jpg,,,,,
1527,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,11,aah1vb,medium.com,[P] Machine Learning Top 50 Articles for the Past Year (v.2019),https://www.reddit.com/r/MachineLearning/comments/aah1vb/p_machine_learning_top_50_articles_for_the_past/,kumeralex,1546050318,,0,1,False,default,,,,,
1528,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,11,aah343,medium.com,[R] Machine Learning Top 50 Articles for the Past Year (v.2019),https://www.reddit.com/r/MachineLearning/comments/aah343/r_machine_learning_top_50_articles_for_the_past/,itsawesomeday,1546050585,,0,1,False,https://b.thumbs.redditmedia.com/6YaqeEafvnbOEXAxL7jhV1ePNYrLn9wA7ss2sxcg8AM.jpg,,,,,
1529,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,13,aahuqd,self.MachineLearning,[D] What statistical methods or machine learning models can be used to predict next year's revenue?,https://www.reddit.com/r/MachineLearning/comments/aahuqd/d_what_statistical_methods_or_machine_learning/,zenggyu,1546056604,"Dear community,

I work in a telecommunication company and am about to take part in a project that aims to predict the revenue of next year. Currently, the prediction is given by a custom formula which seems really arbitrary and there is no prove for its validity. I am new to this field and thus is not sure which model is more suitable for this task.

We have user-level data covering the type of service he/she uses, usage history, bills of every month, etc.; as well as service-level data covering many features of the services (price, etc.). For each user, we want to predict his/her bill for \*\*every month\*\* in next year.

So, what statistical methods or machine learning models can be used?  Is time-series analysis appropriate? Any suggestion is appreciated!",16,1,False,self,,,,,
1530,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,13,aai5pc,self.MachineLearning,[P] ONNX.js : Javascript library for running ONNX models on browsers and on Node.js,https://www.reddit.com/r/MachineLearning/comments/aai5pc/p_onnxjs_javascript_library_for_running_onnx/,holy_ash,1546059074,"[Github](https://github.com/Microsoft/onnxjs)

The benchmark on GPU looks very good in comparison with tf.js and keras.js

[Benchmark from their github repo](https://i.redd.it/o8pp4305d5721.png)",4,1,False,https://b.thumbs.redditmedia.com/8ti6ddi2_qyaAjB5sK-UL_nZqMDaNBZo6s4Y9cIsmPc.jpg,,,,,
1531,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,14,aaifbo,twitter.com,Lords of Football Super Training [Online Game Code] is 75% OFF,https://www.reddit.com/r/MachineLearning/comments/aaifbo/lords_of_football_super_training_online_game_code/,BridieLindgren65,1546061194,,0,1,False,https://b.thumbs.redditmedia.com/u-Iecrvt9eV8HxJRT59Js9dR9tUrLiZfylC_Xk7Pwuo.jpg,,,,,
1532,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,15,aaip0i,self.MachineLearning,"Few thoughts on, How not to fail with a ML MOOC and a few answers to: Why is Machine Learning Hard? I know the basics, which framework should I use? Do I need to know Math?",https://www.reddit.com/r/MachineLearning/comments/aaip0i/few_thoughts_on_how_not_to_fail_with_a_ml_mooc/,init__27,1546063514,[removed],0,1,False,self,,,,,
1533,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,18,aajrs6,self.MachineLearning,Learn Computer Vision,https://www.reddit.com/r/MachineLearning/comments/aajrs6/learn_computer_vision/,bluepanda1219,1546074362,[removed],0,1,False,self,,,,,
1534,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,20,aakeez,self.MachineLearning,I have been assigned a project to detect abnormal human behaviour using CCTV footage but i am a not a total but noob in this field. Was looking for some direction. Is this the right place?,https://www.reddit.com/r/MachineLearning/comments/aakeez/i_have_been_assigned_a_project_to_detect_abnormal/,rockangator,1546081369,"Little did i come across researching about it that i could maybe use the tensorflow object detection api or YOLO to detect humans and then classify different action but i dont know clearly if this is the approach i should go for, or is it efficient enough.

For the time being i want to focus on detecting abnormal activities(anything else than walking or sitting(for the time)) performed by a single human in a closed environment(rooms, office spaces, classrooms).

I have a 7700HQ, GTX1060(notebook), will be using python.

Any help? I welcome help with getting the tensorflow gpu installation too.",0,1,False,self,,,,,
1535,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,21,aakpn0,self.MachineLearning,"20 Cheat Sheets: Python, ML, Data Science, R, and More",https://www.reddit.com/r/MachineLearning/comments/aakpn0/20_cheat_sheets_python_ml_data_science_r_and_more/,andrea_manero,1546084956,http://www.datasciencecentral.com/profiles/blogs/20-cheat-sheets-python-ml-data-science,0,1,False,self,,,,,
1536,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,22,aal2uj,twitter.com,[D] What are your thoughts on this tweet? PhD vs MOOCs,https://www.reddit.com/r/MachineLearning/comments/aal2uj/d_what_are_your_thoughts_on_this_tweet_phd_vs/,Frawzey,1546088809,,2,1,False,default,,,,,
1537,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,22,aal9sa,self.MachineLearning,Stochastic Graphlet Embedding,https://www.reddit.com/r/MachineLearning/comments/aal9sa/stochastic_graphlet_embedding/,AnjanDutta,1546090696,[removed],0,1,False,https://a.thumbs.redditmedia.com/Gqh8ilNuAe1e6-_C35hneQdk9gIRBQU11BsFgF8vh54.jpg,,,,,
1538,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,22,aalel6,medium.com,Mapping All of the Trees with Machine Learning [P],https://www.reddit.com/r/MachineLearning/comments/aalel6/mapping_all_of_the_trees_with_machine_learning_p/,gds506,1546091961,,0,1,False,default,,,,,
1539,MachineLearning,t5_2r3gv,2018-12-29,2018,12,29,23,aalgh6,youtube.com,"What's the best way to get into ML? Well, if you're looking to get your feet wet ASAP, you should consider learning APIs like Keras as opposed to backends like Tensorflow and Theano. Here's a good video that explains exactly what Keras is",https://www.reddit.com/r/MachineLearning/comments/aalgh6/whats_the_best_way_to_get_into_ml_well_if_youre/,antaloaalonso,1546092415,,0,1,False,https://b.thumbs.redditmedia.com/VeOOWnBsA49wJCtCan9jcEGRYYS_UxVWwAmiJ07YSPM.jpg,,,,,
1540,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,0,aalxje,blockdelta.io,5 Best Open-source Frameworks for Creating Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/aalxje/5_best_opensource_frameworks_for_creating_machine/,BlockDelta,1546096470,,0,1,False,default,,,,,
1541,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,0,aam4nz,edyoda.com,FREE step by step course on Machine Learning using Google's Tensorflow with projects.,https://www.reddit.com/r/MachineLearning/comments/aam4nz/free_step_by_step_course_on_machine_learning/,iamarmaan,1546097997,,0,1,False,default,,,,,
1542,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,1,aamavn,self.MachineLearning,[ICLR 2019] Biologically-Plausible Learning Algorithms Can Scale to Large Datasets,https://www.reddit.com/r/MachineLearning/comments/aamavn/iclr_2019_biologicallyplausible_learning/,chenhonglin,1546099335,[removed],0,1,False,self,,,,,
1543,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,1,aamdjs,self.MachineLearning,[Research] Stochastic Graphlet Embedding,https://www.reddit.com/r/MachineLearning/comments/aamdjs/research_stochastic_graphlet_embedding/,AnjanDutta,1546099890,"Joint work of Dr. Anjan Dutta (CVC, UAB, Barcelona) and Dr. Hichem Sahbi (LIP6, UPMC, Paris) on ""Stochastic Graphlet Embedding"" is published in IEEE TNNLS. An efficient and robust graph embedding technique that encode the distribution of increasing sized graphlets with stochastic graph parsing and hashing: [https://ieeexplore.ieee.org/document/8587135](https://ieeexplore.ieee.org/document/8587135).

[Overview of Stochastic Graphlet Embedding](https://i.redd.it/39ztho42q8721.jpg)",0,1,False,https://b.thumbs.redditmedia.com/sgAQxdJ5cnUhNdHCq6SCA6mPoA9_bNJTTxA0-hvkxbw.jpg,,,,,
1544,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,1,aamdqd,self.MachineLearning,[R][ICLR2019] Biologically-Plausible Learning Algorithms Can Scale to Large Datasets,https://www.reddit.com/r/MachineLearning/comments/aamdqd/riclr2019_biologicallyplausible_learning/,chenhonglin,1546099933,"[https://openreview.net/forum?id=SygvZ209F7](https://openreview.net/forum?id=SygvZ209F7)

**Abstract:** The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this weight transport problem (Grossberg, 1987), two biologically-plausible algorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax BPs weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) finds that although feedback alignment (FA) and some variants of target-propagation (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry (SS) algorithm (Liao et al., 2016), which differs from both BP and FA in that the feedback and feedforward weights do not share magnitudes but share signs. We examined the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures (ResNet-18 and AlexNet for ImageNet; RetinaNet for MS COCO). Surprisingly, networks trained with sign-symmetry can attain classification performance approaching that of BP-trained networks. These results complement the study by Bartunov et al. (2018) and establish a new benchmark for future biologically-plausible learning algorithms on more difficult datasets and more complex architectures.

**Keywords:** biologically plausible learning algorithm, ImageNet, sign-symmetry, feedback alignment

**TL;DR:** Biologically plausible learning algorithms, particularly sign-symmetry, work well on ImageNet",3,1,False,self,,,,,
1545,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,1,aamf5p,self.MachineLearning,[R] Stochastic Graphlet Embedding,https://www.reddit.com/r/MachineLearning/comments/aamf5p/r_stochastic_graphlet_embedding/,AnjanDutta,1546100226,"Joint work of Dr. Anjan Dutta (CVC, UAB, Barcelona) and Dr. Hichem Sahbi (LIP6, UPMC, Paris) on ""Stochastic Graphlet Embedding"" is published in IEEE TNNLS. An efficient and robust graph embedding technique that encode the distribution of increasing sized graphlets with stochastic graph parsing and hashing: [https://ieeexplore.ieee.org/document/8587135](https://ieeexplore.ieee.org/document/8587135).

&amp;#x200B;

[Overview of Stochastic Graphlet Embedding](https://i.redd.it/861eyhc4s8721.jpg)",6,1,False,self,,,,,
1546,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,1,aamh4n,github.com,[P] Simpler human-readable labels for ImageNet,https://www.reddit.com/r/MachineLearning/comments/aamh4n/p_simpler_humanreadable_labels_for_imagenet/,anishathalye,1546100631,,1,1,False,default,,,,,
1547,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,1,aamjkl,self.MachineLearning,[P] Simpler human-readable labels for ImageNet,https://www.reddit.com/r/MachineLearning/comments/aamjkl/p_simpler_humanreadable_labels_for_imagenet/,anishathalye,1546101116,"Thought I'd share this just in case it's useful to anyone else.

For a project I'm working on, I needed human-readable ImageNet labels, but I wasn't happy with the ""official"" ImageNet labels. A lot of them aren't particularly understandable, and displaying entire synsets is confusing. And in many situations, choosing the first synonym from the synset gives a confusing result. I couldn't find simplified labels online, so I spent a day going through all the ImageNet classes and tried to come up with simple, human-readable labels. There's a bit more detail on the process in the GitHub repo readme, in case anyone is interested.

I have no idea if this will be relevant to anyone else, but I thought I'd share just in case (and save someone else the hours of manual labor required to do this).

Link: https://github.com/anishathalye/imagenet-simple-labels",5,1,False,self,,,,,
1548,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,1,aampk6,self.MachineLearning,[R] Explained: A Style-Based Generator Architecture for GANs - Generating and Tuning Realistic Artificial Faces,https://www.reddit.com/r/MachineLearning/comments/aampk6/r_explained_a_stylebased_generator_architecture/,ranihorev,1546102303,"Hey, 

I wrote a summary of NVIDIAs Style-Based GANs - A new architecture for generating artificial images of faces, and more importantly, controlling different aspects of the image from face shape to hair color. It's based on the ProGAN architecture which generates that image gradually from low-res to high-res, but it uses the input vector in a completely new way (as a style generator). 

The paper also presents a technique to combine multiple images (of faces) into one image by picking which features come from which image. Their video demonstrations are really cool.

[https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)

A few notes:

1. In my opinion, it resembles to genes and traits - A random vector (DNA-like) is used to generate an image; Multiple features of the image (traits) are affected by multiple elements of the vector (many-to-many); The challenge is changing a single feature without affecting others.
2. The authors of the paper were really responsive and helped with any question I had. Much appreciated!
3. It's a fairly complicated topic and I'd love to get you feedback on it.",17,1,False,self,,,,,
1549,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,2,aamvjv,openreview.net,[R][ICLR19] Complement Objective Training,https://www.reddit.com/r/MachineLearning/comments/aamvjv/riclr19_complement_objective_training/,henry8527,1546103442,,0,1,False,default,,,,,
1550,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,2,aamyv4,self.MachineLearning,[R][ICLR2019]Complement Objective Training,https://www.reddit.com/r/MachineLearning/comments/aamyv4/riclr2019complement_objective_training/,henry8527,1546104083,"https://openreview.net/forum?id=HyM7AiA5YX

Abstract: Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to adversarial attacks.
Keywords: optimization, entropy, image recognition, natural language understanding, adversarial attacks, deep learning
TL;DR: We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.",0,1,False,self,,,,,
1551,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,2,aan2i5,self.MachineLearning,[R][ICLR2019] Complement Objective Training,https://www.reddit.com/r/MachineLearning/comments/aan2i5/riclr2019_complement_objective_training/,henry8527,1546104765,"**Abstract:** Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to adversarial attacks. 

**Keywords:** optimization, entropy, image recognition, natural language understanding, adversarial attacks, deep learning

**TL;DR:** We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.

Github: [https://github.com/henry8527/COT](https://github.com/henry8527/COT)",2,1,False,self,,,,,
1552,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,2,aan3ef,ntt123.github.io,My demo (and colab notebook) on relational network network with Sort-of-CLEVR dataset,https://www.reddit.com/r/MachineLearning/comments/aan3ef/my_demo_and_colab_notebook_on_relational_network/,xcodevn,1546104944,,0,1,False,default,,,,,
1553,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,2,aan51q,self.MachineLearning,Must read deep learning papers for a beginner?,https://www.reddit.com/r/MachineLearning/comments/aan51q/must_read_deep_learning_papers_for_a_beginner/,dklvch,1546105266,I want to get a deeper understanding of core dl concepts and architectures (especially CV). ,0,1,False,self,,,,,
1554,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,2,aan6gi,ntt123.github.io,My demo (and colab notebook) on relational network with Sort-of-CLEVR dataset,https://www.reddit.com/r/MachineLearning/comments/aan6gi/my_demo_and_colab_notebook_on_relational_network/,xcodevn,1546105542,,0,1,False,default,,,,,
1555,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,3,aangiv,self.MachineLearning,[R][ICLR] Complement Objective Training,https://www.reddit.com/r/MachineLearning/comments/aangiv/riclr_complement_objective_training/,henry8527,1546107388,[removed],0,1,False,self,,,,,
1556,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,3,aani3u,self.MachineLearning,LSTM with combined features,https://www.reddit.com/r/MachineLearning/comments/aani3u/lstm_with_combined_features/,nlpredditproject,1546107671,[removed],0,1,False,self,,,,,
1557,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,3,aankk4,self.MachineLearning,Inventing TV on the Internet a Tad Bit Too Early | Scott Klososky,https://www.reddit.com/r/MachineLearning/comments/aankk4/inventing_tv_on_the_internet_a_tad_bit_too_early/,The_Syndicate_VC,1546108123,[removed],0,1,False,self,,,,,
1558,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,4,aany7s,self.MachineLearning,Launch your big data and visualization career with Tableau Desktop 10 Certification Training,https://www.reddit.com/r/MachineLearning/comments/aany7s/launch_your_big_data_and_visualization_career/,internetdigitalentre,1546110555,[removed],0,1,False,self,,,,,
1559,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,4,aao4vt,self.MachineLearning,GAN: generator makes black images,https://www.reddit.com/r/MachineLearning/comments/aao4vt/gan_generator_makes_black_images/,TripinPanda,1546111784,[removed],0,1,False,self,,,,,
1560,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,5,aaohwn,deeplearningio.com,What is Intersection over Union (IoU) and Mean Average Precision (MAP)? | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aaohwn/what_is_intersection_over_union_iou_and_mean/,mrcgllr,1546114177,,0,1,False,default,,,,,
1561,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,5,aaonkh,self.MachineLearning,[P] Program and Slides for TVM and Deep Learning Compiler Conference,https://www.reddit.com/r/MachineLearning/comments/aaonkh/p_program_and_slides_for_tvm_and_deep_learning/,crowwork,1546115233,"TVM is an open-source deep learning compiler stack for CPUs, GPUs, and specialized accelerators. It aims to close the gap between the productivity-focused deep learning frameworks, and the performance- or efficiency-oriented hardware backends.  The conference covers discuss recent advances in frameworks, compilers, systems and architecture support, security, training, and hardware acceleration.

&amp;#x200B;

Link: [https://sampl.cs.washington.edu/tvmconf/#about-tvmconf](https://sampl.cs.washington.edu/tvmconf/#about-tvmconf)",0,1,False,self,,,,,
1562,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,6,aap696,self.MachineLearning,[D] Training for unexplainability: keeping the net unaware of what it is doing,https://www.reddit.com/r/MachineLearning/comments/aap696/d_training_for_unexplainability_keeping_the_net/,phobrain,1546118624,[removed],0,1,False,self,,,,,
1563,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,7,aapjs3,self.MachineLearning,New scientist joining the field.,https://www.reddit.com/r/MachineLearning/comments/aapjs3/new_scientist_joining_the_field/,ParanoidPar,1546121114,[removed],0,1,False,self,,,,,
1564,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,7,aaplcn,self.MachineLearning,[D] visualize raw data from keras.io,https://www.reddit.com/r/MachineLearning/comments/aaplcn/d_visualize_raw_data_from_kerasio/,kkbrennm,1546121412,"Hello everyone,

I have been reading deep learning with python by  Fracois Chollet and am having a hard time understanding the examples since i can't visualize the datasets. Whenever I open the python scripts for the given dataset that i am working with, all I see is a link to where the dataset is being downloaded from. How do I open these datasets to visualize the raw data? The datasets from all of these examples are the ones included by default in the [keras.io](https://keras.io) library, 

&amp;#x200B;

Thanks for all the help!",2,1,False,self,,,,,
1565,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,7,aappsm,self.MachineLearning,Multi agent/Cooperative agents and vehicles,https://www.reddit.com/r/MachineLearning/comments/aappsm/multi_agentcooperative_agents_and_vehicles/,Maplernothaxor,1546122256,[removed],0,1,False,self,,,,,
1566,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,7,aapv8y,self.MachineLearning,How can I learn machine learning on my own?,https://www.reddit.com/r/MachineLearning/comments/aapv8y/how_can_i_learn_machine_learning_on_my_own/,moicel,1546123301,[removed],0,1,False,self,,,,,
1567,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,7,aaq08m,self.MachineLearning,"Going back to the basics: exploring OR, AND, XOR perceptron style models with Keras",https://www.reddit.com/r/MachineLearning/comments/aaq08m/going_back_to_the_basics_exploring_or_and_xor/,chrisfilo,1546124221,[removed],0,1,False,https://b.thumbs.redditmedia.com/Z91LOqfkQmOaoCspXIBkMX5h1CSb_6zyqjxNEkIqkuA.jpg,,,,,
1568,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,8,aaq4qa,self.MachineLearning,[D] Does anyone have experience with outputting a mixture of continuous and binary actions with a reinforcement learning algorithm like DDPG?,https://www.reddit.com/r/MachineLearning/comments/aaq4qa/d_does_anyone_have_experience_with_outputting_a/,iamiamwhoami,1546125062,"Im training a reinforcement learning algorithm to place orders on an exchange. The way Im imagining it my policy will output four variables: buy_price, buy_size, do_nothing, cancel_orders. The first two will be continuous variables that will specify the price and how much of a product to buy. The second two will be binary variables. If do_nothing=1 it will do nothing otherwise it will place an order. If cancel_all=1 it will cancel all existing orders otherwise it will place an order. I know ddpg is supposed to be for continuous action spaces but I cant see any problems with this approach. Does anyone have experience with doing something like this or sees anything obviously wrong?",2,1,False,self,,,,,
1569,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,8,aaq5b2,self.MachineLearning,[D] Where does one get inexpensive and high powered compute for personal use?,https://www.reddit.com/r/MachineLearning/comments/aaq5b2/d_where_does_one_get_inexpensive_and_high_powered/,Radiatin,1546125164,"I thought this might be an interesting discussion. I do data science during the day but Ive been working on several capstone projects in my personal time with promising results. Ive had access to systems with 4000 cores at work for one project and other powerful systems for others but my home system is a pitiful in comparison, boasting just 8 cores and a single 1080 GTX.

Im currently running a personal project which requires 18 hours to compute each iteration and has required dozens of revisions so far, and Im really struggling dealing with such long wait times.

Whats everyone else here using for high powered computing for their personal projects on a personal budget? Im not exactly a hardware guy, and a friend recommended setting up an Azure environment with 4 K80s which they use for their own machine learning projects. At $600 per month of continuous use this seems to be a decent way to go versus a ~$4000 upgrade and $100 in continuous monthly power use considering I wont be using it constantly.

Im curious if anyone has better alternatives, options, or opinions on buying hardware versus paying for a personal environment and which to chose. Im a bit concerned about getting a bunch of nice hardware, putting large amounts of effort into getting it running, and then having to sell it for pennies on the dollar in a year.

Any insight would be appreciated.",63,1,False,self,,,,,
1570,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,8,aaqb7b,reddit.com,Would having a mathematically described function that approximates the function encoded in a neural network help researchers understand how a neural network gets its output?,https://www.reddit.com/r/MachineLearning/comments/aaqb7b/would_having_a_mathematically_described_function/,FIREATWlLL,1546126280,,0,1,False,default,,,,,
1571,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,11,aas09w,self.MachineLearning,MOOC(s) that teach ML using Python,https://www.reddit.com/r/MachineLearning/comments/aas09w/moocs_that_teach_ml_using_python/,PeleMaradona,1546138530,[removed],0,1,False,self,,,,,
1572,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,12,aasgsf,self.MachineLearning,Adam Optimization vs SGD optimization,https://www.reddit.com/r/MachineLearning/comments/aasgsf/adam_optimization_vs_sgd_optimization/,abhijeetg12,1546142148,"This might help you visualize how the decision boundaries evolve for the two optimization algorithms.

(Both algorithms run for 100 epochs over all the data points)

*Processing video pl5a4jmh8c721...*

*Processing video obe28rci8c721...*",0,1,False,self,,,,,
1573,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,13,aaskgu,self.MachineLearning,Free Access to HPC GPU Compute for Anyone Interested,https://www.reddit.com/r/MachineLearning/comments/aaskgu/free_access_to_hpc_gpu_compute_for_anyone/,johanseom,1546142967,[removed],0,1,False,self,,,,,
1574,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,13,aaskll,self.MachineLearning,Import file in python,https://www.reddit.com/r/MachineLearning/comments/aaskll/import_file_in_python/,ank_itsharma,1546142997,[removed],0,1,False,self,,,,,
1575,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,14,aat3mr,luckytoilet.wordpress.com,Deep Learning for NLP: SpaCy vs PyTorch vs AllenNLP,https://www.reddit.com/r/MachineLearning/comments/aat3mr/deep_learning_for_nlp_spacy_vs_pytorch_vs_allennlp/,lucky94,1546147306,,0,1,False,default,,,,,
1576,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,14,aat88v,self.MachineLearning,Accelerated Proximal Policy Optimization [Research],https://www.reddit.com/r/MachineLearning/comments/aat88v/accelerated_proximal_policy_optimization_research/,brandinho77,1546148413,"Hey everyone, 

&amp;#x200B;

I'm working on a new algorithm that builds upon Proximal Policy Optimization and Nesterov's Accelerated Gradient. It's still a work in progress, but I figured I'd try something new and share a progress update for anyone who wants to contribute to the research. Below is the link to my blog post:

&amp;#x200B;

[https://brandinho.github.io/APPO/](https://brandinho.github.io/APPO/)

&amp;#x200B;

It's still in rough condition, so I welcome any feedback that you have!

&amp;#x200B;

Thanks a lot :)",1,1,False,self,,,,,
1577,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,15,aatlkk,self.MachineLearning,How to query a black box,https://www.reddit.com/r/MachineLearning/comments/aatlkk/how_to_query_a_black_box/,tabidots,1546151731,[removed],0,1,False,self,,,,,
1578,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,16,aatw58,self.MachineLearning,[R][ICLR] Complement Objective Training,https://www.reddit.com/r/MachineLearning/comments/aatw58/riclr_complement_objective_training/,henry8527,1546154524,"[https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=HyM7AiA5YX](https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=HyM7AiA5YX)

**Abstract:** Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to adversarial attacks. 

**Keywords:** optimization, entropy, image recognition, natural language understanding, adversarial attacks, deep learning

**TL;DR:** We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.",3,1,False,self,,,,,
1579,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,17,aaub8l,self.MachineLearning,Resources for Emotion Recognition from Speech,https://www.reddit.com/r/MachineLearning/comments/aaub8l/resources_for_emotion_recognition_from_speech/,YaegerKnight,1546159125,[removed],0,1,False,self,,,,,
1580,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,18,aaui43,self.MachineLearning,/////,https://www.reddit.com/r/MachineLearning/comments/aaui43/_/,allthhatnonsense,1546161280,https://m.youtube.com/watch?v=6ZktNItwexo,0,1,False,self,,,,,
1581,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,18,aaumjf,self.MachineLearning,Sudden drop in loss while training a model and stuck at the same loss and accuracy for the last 5 epochs,https://www.reddit.com/r/MachineLearning/comments/aaumjf/sudden_drop_in_loss_while_training_a_model_and/,thepixelatedguy,1546162702,[removed],1,1,False,self,,,,,
1582,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,20,aav63x,self.MachineLearning,Best Course for NLP,https://www.reddit.com/r/MachineLearning/comments/aav63x/best_course_for_nlp/,Shrey_Dixit,1546169050,[removed],0,1,False,self,,,,,
1583,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,20,aav8s0,self.MachineLearning,[R] Can someone suggest me good problems for research in machine learning? Application based project will be preferred. I hope the community can help.,https://www.reddit.com/r/MachineLearning/comments/aav8s0/r_can_someone_suggest_me_good_problems_for/,iambestest,1546169918,I have done basic course in ML in my college and also know a bit about Deep Learning. ,7,1,False,self,,,,,
1584,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,20,aavbea,self.MachineLearning,Codifier - An Artificial Intelligence C Programmer,https://www.reddit.com/r/MachineLearning/comments/aavbea/codifier_an_artificial_intelligence_c_programmer/,sauloqf,1546170765,[removed],0,1,False,self,,,,,
1585,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,21,aavgj3,twitter.com,&gt;Observer_ [Online Game Code] is 55% OFF,https://www.reddit.com/r/MachineLearning/comments/aavgj3/observer_online_game_code_is_55_off/,KristyLedner42,1546172340,,0,1,False,https://a.thumbs.redditmedia.com/NvYaVKGl-MOEvPYxSY3e3NLkLH1VH8qJ_AEB2a_uEl4.jpg,,,,,
1586,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,22,aavr7d,self.MachineLearning,since im surrounded by corrupt judges,https://www.reddit.com/r/MachineLearning/comments/aavr7d/since_im_surrounded_by_corrupt_judges/,allthhatnonsense,1546175426,[removed],1,1,False,self,,,,,
1587,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,22,aavrt0,self.MachineLearning,Efficient adversarial attack based on Trust Region for Pytorch model,https://www.reddit.com/r/MachineLearning/comments/aavrt0/efficient_adversarial_attack_based_on_trust/,zheweiyao,1546175581,[removed],0,1,False,self,,,,,
1588,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,22,aavw1u,linkedin.com,[D] 10 things I hated about Artificial Intelligence in 2018,https://www.reddit.com/r/MachineLearning/comments/aavw1u/d_10_things_i_hated_about_artificial_intelligence/,jos_pol,1546176709,,0,1,False,default,,,,,
1589,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,22,aavxv8,self.MachineLearning,Adam Optimization decision boundaries,https://www.reddit.com/r/MachineLearning/comments/aavxv8/adam_optimization_decision_boundaries/,abhijeetg12,1546177155,"## This video helps you visualize how the decision boundaries evolve for the Adam optimization algorithms for a simple classification neural network.

![video](y2gub3yp4f721)",0,1,False,self,,,,,
1590,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,23,aaw4m4,self.MachineLearning,[D] Making autoencoders care about small details?,https://www.reddit.com/r/MachineLearning/comments/aaw4m4/d_making_autoencoders_care_about_small_details/,Chroteus,1546178847,"I am training an AE to reconstruct game frames from Breakout.

The model is able to learn to reconstruct the paddle and after overfitting really badly, it's able to reconstruct the ball only half of the time.


However, there are issues:
1) Ball takes up a small space, so not a lot of loss is incurred if the model isn't  able to reconstruct the ball.

I've tried hiding part of the image behind a mask and just rewarding the model for how much it could reconstruct the hidden part, expecting that in this case, small features like the ball would take up a bigger chunk of what model is trying to reconstruct, and hence incur a bigger loss for not reconstructing the ball.
It didn't work, sadly.


2) The model is too sensitive to any kind of regularization. 

b-VAE? The model just ignores the latent code and decoder simply outputs average of all frames.

I also tried InfoVAE with similar results.


So, what I wanted to ask: Is my approach wrong from the beginning? Or are there techniques to alleviate this issue that I am not aware of?


",22,1,False,self,,,,,
1591,MachineLearning,t5_2r3gv,2018-12-30,2018,12,30,23,aaw5fv,self.MachineLearning,Worlds First AI News Anchor Which Is Likely To Replace Human Anchors,https://www.reddit.com/r/MachineLearning/comments/aaw5fv/worlds_first_ai_news_anchor_which_is_likely_to/,navin49,1546179051,[removed],0,1,False,https://b.thumbs.redditmedia.com/tiMbW5MitpLuju9Dvz8Ee6joJ1JnNVSPIPiEBeOjICc.jpg,,,,,
1592,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,0,aawv1p,self.MachineLearning,[P] Pro_Gan_PyTorch package 2.0 release,https://www.reddit.com/r/MachineLearning/comments/aawv1p/p_pro_gan_pytorch_package_20_release/,akanimax,1546184745,"Hello all,

&amp;#x200B;

I am releasing the version 2.0 of my package named pro-gan-pth. \[Progressive GAN implementation in PyTorch\]

This version has the following new features:

1.) Full Support for Multi-GPU training

2.) All the new loss functions added to the package for experimentation

including the RelativisticAverageHingeGAN

3.) Support for Conditional Progressive GAN (uses the projection mechanism from Miyato et. al. 2018)

4.) Code simplification and refactoring for easier understanding (research related purposes).

&amp;#x200B;

Special thanks to Erik Nijkamp ([https://www.linkedin.com/in/enijkamp/](https://www.linkedin.com/in/enijkamp/)) \[Ph.D student at UCLA\] for using my package for training the baseline on the CelebA-HQ model for his research.

&amp;#x200B;

*Processing gif fgdjkr7tpf721...*

Full Quality Video at -&gt; [https://www.youtube.com/watch?v=lzTm6Lq76Mo](https://www.youtube.com/watch?v=lzTm6Lq76Mo) \[**Best viewed in 4K 60fps**\]

&amp;#x200B;

The package is now very easy to use. Mere two lines of code of creating and training a Pro-GAN model. 

I have tried to make it easy to use for Researchers as well as Coders / Enthusiasts alike.

Please feel free to use this package for research as well as projects on your choice of datasets.

&amp;#x200B;

Thanks!

&amp;#x200B;

Best regards,

akanimax",3,1,False,self,,,,,
1593,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,1,aax4nh,self.MachineLearning,[D] Using dilated convolutions for streaming measurements at inference,https://www.reddit.com/r/MachineLearning/comments/aax4nh/d_using_dilated_convolutions_for_streaming/,dluther93,1546186614,"Im very familiar with dilated convolutions and their implantation, but I am perplexed by my use case.

I have a multiple input / output problem where inputs are measurements and outputs are of a higher dimension. At training time, the dilation make since because I can input the entire measurement sequence and it is future blinded.

However, at inference time, I will only get 1 measurement at a time. I want it to operate in real time where I get an output for each measurement. 

How would. Accomplish this? Do I create a padded input array of 0s and then append the incoming measurement so that the data being input is always the same dimensionality?

Ive read numerous articles on wavenet etc and they all seem to generate arbitrary output with no specified input.

TLDR; how do I perform inference with dilated convolutions with streaming measurement data?
",5,1,False,self,,,,,
1594,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,2,aaxxur,openreview.net,[R][ICLR] Backpropamine: training self-modifying neural networks with...,https://www.reddit.com/r/MachineLearning/comments/aaxxur/riclr_backpropamine_training_selfmodifying_neural/,downtownslim,1546192021,,0,1,False,default,,,,,
1595,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,2,aay1ds,openreview.net,[R][ICLR] Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity,https://www.reddit.com/r/MachineLearning/comments/aay1ds/riclr_backpropamine_training_selfmodifying_neural/,downtownslim,1546192655,,8,1,False,https://a.thumbs.redditmedia.com/O5xSqPxXQan0eq4XIj_39G9lsyFLtyg3D_81hYNOIr4.jpg,,,,,
1596,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,4,aayvo2,self.MachineLearning,[D] What's your favorite logger?,https://www.reddit.com/r/MachineLearning/comments/aayvo2/d_whats_your_favorite_logger/,mateja,1546198071,"What's everyone favorite logger for use with ML training? 

I'd like to be able to capture metadata associated with the resulting model somehow, including the net architecture, the name of the training dataset, number of epochs it was trained to, loss at each epoch, output class labels, etc. Embarrassingly, I am still capturing this information in a note-taking app and writing down the filename of the model.

In the ideal case, I would love to be able to throw a trained model along with its metadata into a database or a table, which I could sort or query.

What is everyone else using for this?",10,1,False,self,,,,,
1597,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,4,aaywia,self.MachineLearning,[D] Suggested setup for local real-time CNN implementation?,https://www.reddit.com/r/MachineLearning/comments/aaywia/d_suggested_setup_for_local_realtime_cnn/,zannyfamily,1546198221,"Hi /r/MachineLearning! I'm thinking about implementing a network for automatic segmentation of images from video feed. I'm pretty sure I'd be able to train a CNN and get the model, however I wonder what would be the best approach to making real-time setup for automatic segmentation prediction? Having no experience with that, I'm thinking it would require a local setup, but does anyone know if it is feasible to have simple GPU, say GTX 1080, to run it in real time? If yes how many frames per second do you think it would be able to process? I guess the image resolution I need to process is around 500x500 and the images are RGB. Cheers!",2,1,False,self,,,,,
1598,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,5,aazelt,self.MachineLearning,Image Transformer in Pytorch,https://www.reddit.com/r/MachineLearning/comments/aazelt/image_transformer_in_pytorch/,yusuf_const,1546201388,[removed],0,1,False,self,,,,,
1599,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,5,aazpcv,youtube.com,Computer Vision - RL: Learning Acrobatics by Watching YouTube,https://www.reddit.com/r/MachineLearning/comments/aazpcv/computer_vision_rl_learning_acrobatics_by/,obsezer,1546203308,,0,1,False,https://b.thumbs.redditmedia.com/Brfky9u3BxN1ZACjhoXTVvKltKzV4GOR-LjxTe78O0s.jpg,,,,,
1600,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,6,aazw49,i.redd.it,How people achieved 100% accuracy on Imagenet?,https://www.reddit.com/r/MachineLearning/comments/aazw49/how_people_achieved_100_accuracy_on_imagenet/,kushaj,1546204465,,0,1,False,https://b.thumbs.redditmedia.com/8Ht0xAxPWE-zHRJtoHQe-AbOTuk0t2oIcKftEN_jwFI.jpg,,,,,
1601,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,6,aazzlh,self.MachineLearning,[D] How people achieved 100% accuracy on Imagenet?,https://www.reddit.com/r/MachineLearning/comments/aazzlh/d_how_people_achieved_100_accuracy_on_imagenet/,kushaj,1546205066,Checking onto [Imagenet Object Localization Challenge](https://www.kaggle.com/c/imagenet-object-localization-challenge) leaderboard on Kaggle we can see some people have a score of 0.0000. Can someone please explain how is this possible? Am I missing something here?,5,1,False,self,,,,,
1602,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,7,ab0fny,self.MachineLearning,"Computer Science Video lectures from recent CMU, MIT, UcBerkeley, Stanford?",https://www.reddit.com/r/MachineLearning/comments/ab0fny/computer_science_video_lectures_from_recent_cmu/,ankitshah009,1546207923,[removed],0,1,False,self,,,,,
1603,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,7,ab0hiv,self.MachineLearning,[R] Sim2Real  Using Simulation to Train Real-Life Grasping Robots,https://www.reddit.com/r/MachineLearning/comments/ab0hiv/r_sim2real_using_simulation_to_train_reallife/,tldrtldreverything,1546208256,"Hey, I published a summary of RCAN, a new robotics paper from X/Google/Deepmind which achieves state-of-the-art results in robotic grasping with very little training data. The main idea is that instead of training the grasping robot on full-resolution images of object grasping you train it on a simplified version (canonical style) of the grasps. Personally, I think it's one of the most interesting papers of 2018 and it's also cool that it combines GANs, Reinforcement Learning, and Computer Vision. Full summary here: https://www.lyrn.ai/2018/12/30/sim2real-using-simulation-to-train-real-life-grasping-robots/",5,1,False,self,,,,,
1604,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,8,ab15o1,msiam.github.io,Summary of Recent Img2Img Translation Methods,https://www.reddit.com/r/MachineLearning/comments/ab15o1/summary_of_recent_img2img_translation_methods/,mennasiam,1546212675,,0,1,False,default,,,,,
1605,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,10,ab20sf,arxiv.org,[Research] Meta Learning for Few-shot Keyword Spotting,https://www.reddit.com/r/MachineLearning/comments/ab20sf/research_meta_learning_for_fewshot_keyword/,I_ai_AI,1546218453,,2,1,False,default,,,,,
1606,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,10,ab29ru,self.MachineLearning,TMTrackNN  generating TrackMania tracks with neural networks,https://www.reddit.com/r/MachineLearning/comments/ab29ru/tmtracknn_generating_trackmania_tracks_with/,donadigo,1546220154,[removed],0,1,False,self,,,,,
1607,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,10,ab2don,github.com,[P] Imbalanced dataset sampler for oversampling low frequent classes and undersampling high frequent ones.,https://www.reddit.com/r/MachineLearning/comments/ab2don/p_imbalanced_dataset_sampler_for_oversampling_low/,ufoym,1546220921,,0,1,False,https://b.thumbs.redditmedia.com/MkUOgMWREbIv262fb7zuz9G3IbPG6sYcLo98CWYTOeI.jpg,,,,,
1608,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,11,ab2kd3,self.MachineLearning,[P] TMTrackNN  generating TrackMania tracks with neural networks,https://www.reddit.com/r/MachineLearning/comments/ab2kd3/p_tmtracknn_generating_trackmania_tracks_with/,donadigo,1546222264," 

Hello!  First time posting here, I wanted to share my first ML project  involving LSTM's on an absolutely non-serious task of generating tracks  for a racing game called TrackMania (yes, it is quite weird).

I  would say, don't expect anything great or SOTA results for a 18yr old  guy that just started experimenting with ML, but I hope you'll find this  at least somewhat interesting...

Medium post: [https://medium.com/@donadigos159/tmtracknn-generating-trackmania-tracks-with-neural-networks-146db058e7cb](https://medium.com/@donadigos159/tmtracknn-generating-trackmania-tracks-with-neural-networks-146db058e7cb)

GitHub: [https://github.com/donadigo/tmtracknn](https://github.com/donadigo/tmtracknn)",15,1,False,self,,,,,
1609,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,14,ab4207,inst.eecs.berkeley.edu,"UC Berkeley and Berkeley AI Research published all materials of CS 188: Introduction to Artificial Intelligence, Fall 2018",https://www.reddit.com/r/MachineLearning/comments/ab4207/uc_berkeley_and_berkeley_ai_research_published/,dronecub,1546233422,,63,1,False,https://b.thumbs.redditmedia.com/dDf34dtPdBNb2yMRrsJ_fBAuo2M7Ri9PnHf7GuCMSlE.jpg,,,,,
1610,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,14,ab420e,self.MachineLearning,Newbie in need of guidance,https://www.reddit.com/r/MachineLearning/comments/ab420e/newbie_in_need_of_guidance/,npangarang,1546233423,[removed],0,1,False,self,,,,,
1611,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,14,ab458n,youtu.be,Neural Network + Genetic Algorithm = AI master of Google's Dino run Game,https://www.reddit.com/r/MachineLearning/comments/ab458n/neural_network_genetic_algorithm_ai_master_of/,onil_gova,1546234155,,0,1,False,https://b.thumbs.redditmedia.com/FvL6cJQ_KMmnRC4jmZVC-99IkPYmEJ-vxxDAFGwck0Q.jpg,,,,,
1612,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,14,ab45cs,krazytech.com,Is R Really that Awesome for Machine Learning? - Krazytech,https://www.reddit.com/r/MachineLearning/comments/ab45cs/is_r_really_that_awesome_for_machine_learning/,ravibandakkanavar,1546234182,,0,1,False,default,,,,,
1613,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,15,ab4e10,washingtonpost.com,[D] Fake-porn videos are being weaponized to harass and humiliate women: Everybody is a potential target,https://www.reddit.com/r/MachineLearning/comments/ab4e10/d_fakeporn_videos_are_being_weaponized_to_harass/,sugarhilldt2,1546236120,,0,1,False,default,,,,,
1614,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,15,ab4nmo,self.MachineLearning,[D] Any info on Microsoft NLP model winning over BERT?,https://www.reddit.com/r/MachineLearning/comments/ab4nmo/d_any_info_on_microsoft_nlp_model_winning_over/,Stillexploring-,1546238391,"On GLUE benchmark ( [https://gluebenchmark.com/leaderboard](https://gluebenchmark.com/leaderboard) ),

it seems a team from Microsoft submitted a model slightly better than BERT in almost all tasks.

Anyone have some info to share?

&amp;#x200B;

Not sure about which tag to use.",6,1,False,self,,,,,
1615,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,16,ab4t22,imgur.com,"In-air wheel interface, inspired by iPod wheel",https://www.reddit.com/r/MachineLearning/comments/ab4t22/inair_wheel_interface_inspired_by_ipod_wheel/,ooopoooooooooo,1546239747,,1,1,False,https://a.thumbs.redditmedia.com/CdFDKVyvuMuKwZ_ZXKKciConJt3gRSSdliQ5_kcuO24.jpg,,,,,
1616,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,16,ab55me,self.MachineLearning,"[discussion] As a practitioner, do you read papers? If so, how many per month?",https://www.reddit.com/r/MachineLearning/comments/ab55me/discussion_as_a_practitioner_do_you_read_papers/,urlwolf,1546243085,"The field is moving so fast that it's hard to keep up even for academics. There's plenty of research happening at big Tech companies, so at least some people there must be keeping up with literature. Then, there's the long tail of companies that for sure don't do research, but benefit from implementing stuff that is fresh out of the oven to get a leg up over the competition. And finally, there's the 'barely working data science team' that doesn't need anything beyond using what already comes in a library and tweaking parameters. I just wonder what the proportions are. ",10,1,False,self,,,,,
1617,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,18,ab5qob,qwentic.com,5 of the Best Chatbot Solutions for Ecommerce | Ecommerce Bot,https://www.reddit.com/r/MachineLearning/comments/ab5qob/5_of_the_best_chatbot_solutions_for_ecommerce/,Qwentic,1546249368,,0,1,False,default,,,,,
1618,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,20,ab69qa,self.MachineLearning,Intuition behind RMSLE,https://www.reddit.com/r/MachineLearning/comments/ab69qa/intuition_behind_rmsle/,rock321987,1546254964,"What is the intuition behind using **Root Mean Square Logarithmic Error (RMSLE)** instead of **Root Mean Square Error (RMSE)**?

Suppose I am predicting income of a person, and I get a **RMSLE** of **0.14,** what does it really mean? Can I convert this to  **e****^(0.14)** and say the income of a person has an error of **1.15027379886 (e****^(0.14)****)** ?

The thing is **RMSLE** is not as widely discussed metric as **RMSE** except in Kaggle and few other places. So any explanation   
behind intuition of using it would be helpful!!",0,1,False,self,,,,,
1619,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,20,ab6b55,self.MachineLearning,Information about Neural style transfer for text writing styles,https://www.reddit.com/r/MachineLearning/comments/ab6b55/information_about_neural_style_transfer_for_text/,Dark_Messiah,1546255376,[removed],1,1,False,self,,,,,
1620,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,20,ab6ii8,self.MachineLearning,Unsupervised learning benchmarks?,https://www.reddit.com/r/MachineLearning/comments/ab6ii8/unsupervised_learning_benchmarks/,question_5040,1546257528,"I'm looking for unsupervised equivalents of image classification benchmarks, i.e. entropy over dataset benchmarks. Are there any competitions where the lowest entropy is the goal?",0,1,False,self,,,,,
1621,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,21,ab6j1f,self.MachineLearning,Eye-opening Machine Learning Book: the Feynman book for machine learning,https://www.reddit.com/r/MachineLearning/comments/ab6j1f/eyeopening_machine_learning_book_the_feynman_book/,fortea92,1546257680,[removed],1,1,False,self,,,,,
1622,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,21,ab6wlk,self.MachineLearning,[D] How to use Forward and Backward chaining for regression problems?,https://www.reddit.com/r/MachineLearning/comments/ab6wlk/d_how_to_use_forward_and_backward_chaining_for/,saswata64900,1546261151,"I have a regression problem where I have to do the following things

1) I have to predict the independent variable from dependent variables.(FORWARD CHAINING)

2) And also I need to say what values of the dependent variable might have caused a particular value of the independent variable(BACKWARD CHAINING).

&amp;#x200B;

If somebody has any resources or code for implementing Forward and Backward chaining for regression problems , PLEASE HELP.

&amp;#x200B;

THANKS IN ADVANCE!!",1,1,False,self,,,,,
1623,MachineLearning,t5_2r3gv,2018-12-31,2018,12,31,22,ab7bav,self.MachineLearning,Neural Network Distillation,https://www.reddit.com/r/MachineLearning/comments/ab7bav/neural_network_distillation/,makml,1546264578,[removed],0,1,False,self,,,,,
