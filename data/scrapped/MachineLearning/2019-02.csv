,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,9,alw2mn,self.MachineLearning,Anyone know of an api or model for classifying a comment to a post as either in agreement or disagreement?,https://www.reddit.com/r/MachineLearning/comments/alw2mn/anyone_know_of_an_api_or_model_for_classifying_a/,backprop88,1548980639,[removed],0,1,False,self,,,,,
1,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,9,alwcpk,self.MachineLearning,"sc2 deepmind questions, casual observer",https://www.reddit.com/r/MachineLearning/comments/alwcpk/sc2_deepmind_questions_casual_observer/,bestminipc,1548982356,[removed],0,1,False,self,,,,,
2,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,10,alwkwr,self.MachineLearning,"[Discussion] sc2 deepmind questions, casual observer",https://www.reddit.com/r/MachineLearning/comments/alwkwr/discussion_sc2_deepmind_questions_casual_observer/,bestminipc,1548983826,"looking for plain english, non-technical explanations

&amp;#x200B;

seems that the ml was not fully 'self-learned' and the process was speeded up by giving it 'game strategies'

reddit.com/r/MachineLearning/comments/ajgzoc/we\_are\_oriol\_vinyals\_and\_david\_silver\_from/eevk92z/

&amp;#x200B;

1. **even a ml is 'self-learned' or partially 'self-learned' (w/e 'self-learned' means to a computer), then exactly in full details does a ml actually 'self-learned' through w/e are the common ways a ml 'self-learns'?**

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

'We found that naive implementations of self-play often tended to get stuck in specific strategies or forget how to defeat previous strategies.'

reddit.com/r/MachineLearning/comments/ajgzoc/we\_are\_oriol\_vinyals\_and\_david\_silver\_from/eext6wg/

&amp;#x200B;

2. **how is that even possible that a ml can forget things, it's a computer, it shouldnt be possible for it to forget anything**

&amp;#x200B;

&amp;#x200B;

it seems that not all 'ml building methods' can be used due to the limited time, and the computing time it actually takes to do this ml stuff is alot, so not all  'ml building methods'  can be tested / tried / done

reddit.com/r/MachineLearning/comments/ajgzoc/we\_are\_oriol\_vinyals\_and\_david\_silver\_from/eevk92z/

&amp;#x200B;

in the 6th game the sc2 player won vs the ml, just like in the 4th alphago game

in both cases they won cos the ml didnt know what to do (ml didnt have the prior knowledge)

&amp;#x200B;

**3. how it even possible, given many human-equivalent  years of computing time, that the ml (even tho they saw games where players would a) do drops or b) do drop harass) not have learned from the experience?**

maybe this relates to the 'forgetting' part above

&amp;#x200B;

&amp;#x200B;

looking for plain english, non-technical explanations

&amp;#x200B;

if a person knows the answers but does not have the ability to communciate well or does not have that skillset, then umm...  feel free to link to the answers, and maybe someone that has both skillset would be able to translate w/e technical things is being said in a way where the answers are clear and that most humans can understand

&amp;#x200B;

&amp;#x200B;",3,1,False,self,,,,,
3,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,12,alxjmb,arxiv.org,[R] The Evolved Transformer,https://www.reddit.com/r/MachineLearning/comments/alxjmb/r_the_evolved_transformer/,SixHampton,1548990218,,7,1,False,default,,,,,
4,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,12,alxpz1,analyticsinsight.net,Understanding the Treasures Behind Automated Machine Learning | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/alxpz1/understanding_the_treasures_behind_automated/,analyticsinsight,1548991415,,0,1,False,default,,,,,
5,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,12,alxqso,self.MachineLearning,Good Master's Program in Machine learning with scholarship,https://www.reddit.com/r/MachineLearning/comments/alxqso/good_masters_program_in_machine_learning_with/,PyWarrior,1548991560,[removed],0,1,False,self,,,,,
6,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,12,alxryp,medium.com,"How Search Engines Work - Informative and concise article, explained it better than my prof. [D]",https://www.reddit.com/r/MachineLearning/comments/alxryp/how_search_engines_work_informative_and_concise/,BeBetterThanEver,1548991775,,0,1,False,https://b.thumbs.redditmedia.com/VfzS4z7o_QwlhpntLsou5a4u6pCOvrHaDAtQRPatAig.jpg,,,,,
7,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,12,alxvki,self.MachineLearning,Build a research career in Machine learning,https://www.reddit.com/r/MachineLearning/comments/alxvki/build_a_research_career_in_machine_learning/,PyWarrior,1548992425,[removed],0,1,False,self,,,,,
8,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,13,aly3ir,patents.google.com,[N] Google patent: Object detection using neural network systems,https://www.reddit.com/r/MachineLearning/comments/aly3ir/n_google_patent_object_detection_using_neural/,RickMcCoy,1548993892,,1,1,False,default,,,,,
9,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,13,alyaqd,self.MachineLearning,How to force neural network weights to have a normal distribution,https://www.reddit.com/r/MachineLearning/comments/alyaqd/how_to_force_neural_network_weights_to_have_a/,achellaris,1548995352,[removed],0,1,False,self,,,,,
10,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,14,alylkm,doi.org,Spike-timing-dependent ensemble encoding by non-classically responsive cortical neurons,https://www.reddit.com/r/MachineLearning/comments/alylkm/spiketimingdependent_ensemble_encoding_by/,ss_alien_9,1548997572,,1,1,False,default,,,,,
11,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,15,alz7sy,self.MachineLearning,Math requirements for ML within Computer Graphics?,https://www.reddit.com/r/MachineLearning/comments/alz7sy/math_requirements_for_ml_within_computer_graphics/,dive819,1549002421,[removed],0,1,False,self,,,,,
12,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,15,alza00,self.MachineLearning,Keras playing punchout. Open to contributions,https://www.reddit.com/r/MachineLearning/comments/alza00/keras_playing_punchout_open_to_contributions/,mkm_dz,1549002946,[removed],0,1,False,self,,,,,
13,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,15,alzack,self.MachineLearning,[D] How to construct the graph from raw text (dependency tree agnostic)?,https://www.reddit.com/r/MachineLearning/comments/alzack/d_how_to_construct_the_graph_from_raw_text/,speedcell4,1549003024,"the input sentences are just the raw sentences, we don't know their dependency tree structures. then how can we construct the graph for utilizing graph neural network? here are some of my ideas

1. [http://aclweb.org/anthology/P18-1030](http://aclweb.org/anthology/P18-1030), similar to this paper, we connect each word with its previous word, next word and the global node
2. we can connect these words which occur more than one time with all of its other occurrences
3. if we have part-of-speech annotations, we can connect the words which share the same POS tags.

any other idea about this topic? thanks",1,1,False,self,,,,,
14,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,16,alzi75,self.MachineLearning,What should I study?,https://www.reddit.com/r/MachineLearning/comments/alzi75/what_should_i_study/,palszz,1549004892,[removed],0,1,False,self,,,,,
15,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,16,alzm1c,medium.com,Summary: Prioritized Experience Replay  Arxiv Bytes  Medium,https://www.reddit.com/r/MachineLearning/comments/alzm1c/summary_prioritized_experience_replay_arxiv_bytes/,CartPole,1549005873,,0,1,False,default,,,,,
16,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,18,am07ek,self.MachineLearning,Machine Learning Fakes,https://www.reddit.com/r/MachineLearning/comments/am07ek/machine_learning_fakes/,ed-den,1549011766,"Look at this clowns talking about disruption. Textbook Definition of Building Castles in the Air

[https://www.youtube.com/watch?v=vMLLJwdVEmY](https://www.youtube.com/watch?v=vMLLJwdVEmY&amp;feature=youtu.be)",0,1,False,self,,,,,
17,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,18,am0994,self.MachineLearning,"[D] Improving SGD convergence by more sophisticated modelling, e.g. tracing multiple promising directions, or estimating distance from minimum?",https://www.reddit.com/r/MachineLearning/comments/am0994/d_improving_sgd_convergence_by_more_sophisticated/,jarekduda,1549012287,"Slow convergence is often seen as the main difficulty of deep learning - gradients used in [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) only average to the real gradient of objective function, and standard gradient descent has already own difficulties like oscillating in narrow valley.

So **I wanted to propose a discussion if it might be worth to perform some more sophisticated e.g. statistical analysis of obtained sequence of approximated gradients - increase cost of their processing, hopefully to compensate it with reduced number of steps. What kind of analysis?** The current methods are relatively simple - are there also considered more sophisticated ones? Is it worth investigating?

A universal framework might be trying to model local behavior of objective function with some simple parametrization, update this model accordingly to calculated gradients, then shift the main parameters accordingly to this model. 

For example trying to model only a few most promising eigendirections of Hessian ( https://arxiv.org/pdf/1901.11457 ), we could

*  instead of focusing only on a single direction e.g. in momentum/ADAM/... methods, we could use these approximated gradients to simultaneously model behavior in multiple most promising directions (imagine their PCA...),

* thanks to modelling with parabola, we know its minimum - can estimate distance to minimum to adapt step size.

Is it worth to investigate more sophisticated (and costly) analysis of SGD series of calculated gradients?

Beside above two potential improvement opportunities, what other hints might be worth to have in mind while designing such analysis?

Any new promising/interesting directions/approaches for improving SGD convergence?",12,1,False,self,,,,,
18,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,18,am0ags,self.MachineLearning,What is a controversial opinion that you have?,https://www.reddit.com/r/MachineLearning/comments/am0ags/what_is_a_controversial_opinion_that_you_have/,sugarhilldt2,1549012626,[removed],0,1,False,self,,,,,
19,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,18,am0aou,blockdelta.io,Robotics Revolution: Man vs Machine - Case Study on Japan,https://www.reddit.com/r/MachineLearning/comments/am0aou/robotics_revolution_man_vs_machine_case_study_on/,BlockDelta,1549012684,,0,1,False,https://b.thumbs.redditmedia.com/xXgETThQmYRWT_OjYnHOGI7uMaRnh8X-_zE5MkCUNkI.jpg,,,,,
20,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,18,am0fdt,self.MachineLearning,Connectionist Temporal Classification Loss for Astroturfing Detection,https://www.reddit.com/r/MachineLearning/comments/am0fdt/connectionist_temporal_classification_loss_for/,ruanchaves,1549014052,[removed],0,1,False,self,,,,,
21,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,18,am0iuu,self.MachineLearning,[D] Multi-task binary classification dataset with high number of tasks (for online meta-learning),https://www.reddit.com/r/MachineLearning/comments/am0iuu/d_multitask_binary_classification_dataset_with/,rikkajounin,1549015025,"Hi everyone,

I am currently working in online meta-learning in a very simple setting: learning the initialization of a linear model. In the online setting, the initialization is learned iteratively with one update after the linear model is learned on a single task. The final initialization is then evaluated on a set of evaluation tasks.

&amp;#x200B;

This settings and the method i'm working on suggest empirical evaluation on datasets with an high number of tasks. Looking [at this survey on multi-task learning](https://arxiv.org/abs/1707.08114) i found nice datasets for regression problems (&gt; 100 tasks) but for binary classification the dataset with the highest number of tasks contains just 35.

&amp;#x200B;

What do you think a nice dataset for binary classification could be in this setting? I was thinking at least 100 tasks and not too difficult due to the simplicity of the model. Images might be not a good fit, but maybe using already learned features they could be. For example i was thinking about turning CIFAR-100 into a 100 one vs all tasks dataset. Do you know any other datsets that fit this description?

&amp;#x200B;

Thanks in advance",0,1,False,self,,,,,
22,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,19,am0sya,self.MachineLearning,My Degree Thesis: Set Expansion on challenging case (only 1 seed) with word embeddings,https://www.reddit.com/r/MachineLearning/comments/am0sya/my_degree_thesis_set_expansion_on_challenging/,uccollab,1549017827,[removed],0,1,False,self,,,,,
23,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,20,am12do,research.google.com,"Seedbank is a registry and search engine for Colab(oratory) notebooks for and around machine learning for rapid exploration and learning. You can browse the site and use, experiment, fork Colab notebooks. Thank you Google for this.",https://www.reddit.com/r/MachineLearning/comments/am12do/seedbank_is_a_registry_and_search_engine_for/,ddb1995,1549020295,,0,1,False,default,,,,,
24,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,21,am1dns,self.MachineLearning,What template should we use for KDD 2019?,https://www.reddit.com/r/MachineLearning/comments/am1dns/what_template_should_we_use_for_kdd_2019/,FlyingQuokka,1549023008,[removed],0,1,False,self,,,,,
25,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,22,am1yeq,self.MachineLearning,[P] Browse State-of-the-Art Papers with Code,https://www.reddit.com/r/MachineLearning/comments/am1yeq/p_browse_stateoftheart_papers_with_code/,rstoj,1549027438,"[https://paperswithcode.com/sota](https://paperswithcode.com/sota)

Hi all,

Weve just released the latest version of Papers With Code. As part of this weve extracted 950+ unique ML tasks, 500+ evaluation tables (with state of the art results) and 8500+ papers with code. Weve also open-sourced the entire dataset.

Everything on the site is editable and versioned. Weve found the tasks and state-of-the-art data really informative to discover and compare research - and even found some research gems that we didnt know about before. Feel free to join us in annotating and discussing papers!

Let us know your thoughts.

Thanks!

Robert",74,1,False,self,,,,,
26,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,22,am223r,self.MachineLearning,Is there an 'updated version' of (N)PMI,https://www.reddit.com/r/MachineLearning/comments/am223r/is_there_an_updated_version_of_npmi/,babuunn,1549028169,[removed],0,1,False,self,,,,,
27,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,22,am27y7,self.MachineLearning,[D] How do gradient boosted decision trees work in terms of sequential learning?,https://www.reddit.com/r/MachineLearning/comments/am27y7/d_how_do_gradient_boosted_decision_trees_work_in/,justfindinganswers,1549029320,"I understand the methodology behind gradient boosting - an ensemble method in which one weak leaner after the next tries to correct the problems of the error of the previous model - I understand completely how the AdaBoost method works by increasing the weights of those missclassified - however i'm a little confused on the Gradient Boosting side of things - specifically this: 

If we create a gradient boosted binary classification decision tree  - what has the next sequential model been trained on? has it been trained on the loss of the previous tree in terms of missclassification of binary outcomes - because alot of information out there says the model is trained on the residual error - and i understand this from the standpoint of regression as it will train the next model on the RMSE or whatever is used - so is the next model trained on the binary:logistic error in the case of a binary classifier??

&amp;#x200B;

Bit confused on this one!

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
28,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,23,am2ai7,self.MachineLearning,"[D] Our last podcast episode is about DeepMind's AI playing Starcraft 2, feedback would be nice",https://www.reddit.com/r/MachineLearning/comments/am2ai7/d_our_last_podcast_episode_is_about_deepminds_ai/,Eysenor,1549029784,"As title, we are discussing the recent alphastar performances but we are not AI experts and somewhat decent at knowing starcraft 2 (following the pro scene, not much at playing at all).

Would be nice to have some feedback from the community here about the AI side of the discussion. 

Also I would really like if some expert in DeepMind or deep learning in general would like to come to the podcast as guest to teach us about the topic. We have an idea of what the science behind is but a deeper insight would be really nice.

&amp;#x200B;

[https://www.podbean.com/media/share/pb-ahjnt-a66c43](https://www.podbean.com/media/share/pb-ahjnt-a66c43)

PS hopefully this is not a bad post in here.",4,1,False,self,,,,,
29,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,23,am2jjt,self.MachineLearning,Free Webinar on the Next Generation Data Lake designed for exploratory Data Science,https://www.reddit.com/r/MachineLearning/comments/am2jjt/free_webinar_on_the_next_generation_data_lake/,supercake53,1549031383,[removed],0,1,False,self,,,,,
30,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,23,am2pyl,self.MachineLearning,Support case escalation dataset?,https://www.reddit.com/r/MachineLearning/comments/am2pyl/support_case_escalation_dataset/,humminghero,1549032484,"Hello all,

Are there any open datasets which can be used to predict if a customer will be escalating a support ticket or not.

I would like to solve this problem similar to customer churn problem.

Thank you in advance. ",0,1,False,self,,,,,
31,MachineLearning,t5_2r3gv,2019-2-1,2019,2,1,23,am2t2f,/r/MachineLearning/comments/am2t2f/pytorchopencv_my_implementation_of_quickdraw_an/,[Pytorch+OpenCV] My implementation of QuickDraw - an online game developed by Google (Source code: https://github.com/vietnguyen91/QuickDraw),https://www.reddit.com/r/MachineLearning/comments/am2t2f/pytorchopencv_my_implementation_of_quickdraw_an/,1991viet,1549033026,,1,1,False,default,,,,,
32,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,0,am2wd6,self.MachineLearning,Qualifications for machine learning research engineer,https://www.reddit.com/r/MachineLearning/comments/am2wd6/qualifications_for_machine_learning_research/,0x4a616b6f62,1549033584,[removed],0,1,False,self,,,,,
33,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,0,am2xiy,self.MachineLearning,Software is eating the world (The Verge),https://www.reddit.com/r/MachineLearning/comments/am2xiy/software_is_eating_the_world_the_verge/,init__27,1549033766,[removed],0,1,False,self,,,,,
34,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,0,am38zw,heartbeat.fritz.ai,Exploring the MobileNet Models in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/am38zw/exploring_the_mobilenet_models_in_tensorflow/,the-dagger,1549035601,,0,1,False,default,,,,,
35,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,1,am3o3s,self.MachineLearning,Salary expectations for ML Engineer at Startup in New York,https://www.reddit.com/r/MachineLearning/comments/am3o3s/salary_expectations_for_ml_engineer_at_startup_in/,chavacado,1549037933,"I feel the normal sources (Glassdoor, indeed, etc) have averaged that are not extremely helpful. The range, especially anything with deep learning, is quit wide. 

With an MS and experience with some years experience in data science it seems you could reasonably expect &gt; 165k at a profitable startup?",0,1,False,self,,,,,
36,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,1,am43gr,stanmed.stanford.edu,Capturing the brains learning and recall motor in silicon,https://www.reddit.com/r/MachineLearning/comments/am43gr/capturing_the_brains_learning_and_recall_motor_in/,planet9z,1549040225,,2,1,False,default,,,,,
37,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,2,am47oi,self.MachineLearning,What is L1 and L2 reguralization?,https://www.reddit.com/r/MachineLearning/comments/am47oi/what_is_l1_and_l2_reguralization/,kirankumar1221,1549040851,[removed],0,1,False,self,,,,,
38,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,2,am48v5,wired.com,The Worlds Fastest Supercomputer Breaks an AI Record,https://www.reddit.com/r/MachineLearning/comments/am48v5/the_worlds_fastest_supercomputer_breaks_an_ai/,OakFace,1549041029,,0,1,False,default,,,,,
39,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,3,am540i,self.MachineLearning,[N] https://www.forbes.com/sites/bernardmarr/2019/02/01/how-artificial-intelligence-is-used-to-make-beer/#afc42bd70cf4,https://www.reddit.com/r/MachineLearning/comments/am540i/n/,Davlucmac,1549045648,"Forbes is reporting on creating beer recipes with ""AI"" - a simple application, but it's an ML/AI application not related to self-driving cards that seems to have caught people's attention.",3,1,False,self,,,,,
40,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,3,am54y2,self.MachineLearning,Intuition behind Follow The Regularized Leader and its influence on perturbing the dataset?,https://www.reddit.com/r/MachineLearning/comments/am54y2/intuition_behind_follow_the_regularized_leader/,Fender6969,1549045801,[removed],0,1,False,self,,,,,
41,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,3,am54y5,self.MachineLearning,Intuition behind Follow The Regularized Leader and its influence on perturbing the dataset?,https://www.reddit.com/r/MachineLearning/comments/am54y5/intuition_behind_follow_the_regularized_leader/,Fender6969,1549045802,[removed],0,1,False,self,,,,,
42,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,4,am5i0y,self.MachineLearning,I am new to ML. I am interesting in training my own model. The idea is to have a model that can detect skin. What kind of images I have to provide to the training model?,https://www.reddit.com/r/MachineLearning/comments/am5i0y/i_am_new_to_ml_i_am_interesting_in_training_my/,CoolAppz,1549047766,[removed],0,1,False,self,,,,,
43,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,4,am5ndb,self.MachineLearning,[D] Best R package for deep learning work?,https://www.reddit.com/r/MachineLearning/comments/am5ndb/d_best_r_package_for_deep_learning_work/,LegacyAngel,1549048576,"Hi,

My new project requires me to use R for various reasons. I am looking into implementing a variation of CycleGANs, so I have been looking for packages that give me ample flexibility. 

I am decent at pytorch and tensorflow (much prefer pytorch), so I am looking for something that has a similar paradigm.

I found the following and here are my thoughts. Looking for something better, or which one of the below to pick:

1. ruta --- uses keras underneath, seems nice but limited options for losses, and some stuff is buggy
2. mxnet -- never used mxnet before, but the documentation makes this seem most flexible, even if cumbersome to work in
3. h2o --- used this before for python, didn't like it. worth looking into? don't like the jvm requirements and how rigid it is (handholdy)
4. nnet --- seems immature",10,1,False,self,,,,,
44,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,5,am66ne,self.MachineLearning,[P] PCA  Calculating the original variable: Y(n+1),https://www.reddit.com/r/MachineLearning/comments/am66ne/p_pca_calculating_the_original_variable_yn1/,Unlistedd,1549051579,"After i am done with my PCA, getting a simple structure of factor loadings by solving for the eigenequation and rotating the factor loadings matrix, etc  i am left with the equation: Y = (1) \* **(1) + (2) \* **(2) +  + . How am i suppose to calculate Y(n+1)? All i can gather from my PCA is the number of necessary factor loadings needed to calculate Y. Now, the quesiton remains, where- and how do i obtain the numbers of (n+1) and **(n+1)? Am i suppose to use a BaumWelch algorithm given that (n+1) and **(n+1) are unknown/hidden? In that case, how would that work/be implemented into my model?",1,1,False,self,,,,,
45,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,5,am6a5q,self.MachineLearning,[D] Tensorflow Eager Wrapper?,https://www.reddit.com/r/MachineLearning/comments/am6a5q/d_tensorflow_eager_wrapper/,eukaryote31,1549052155,"Would it be possible to create a wrapper around tensorflow eager to expose a pytorch-like API to allow for minimal-effort porting of existing code? This would be extremely helpful for ROCm, as pytorch ROCm support is currently far worse than tensorflow. ",4,1,False,self,,,,,
46,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,5,am6ipn,self.MachineLearning,"[P] Hypergraph, a new paradigm for hyper-parameters definition and optimization.",https://www.reddit.com/r/MachineLearning/comments/am6ipn/p_hypergraph_a_new_paradigm_for_hyperparameters/,aljabr0,1549053528,"**Hypergraph** is an open source library originally  developed to provide a high level of abstraction when developing machine  learning and deep neural networks.

The key concept of this library is the graph, a structure composed by  interconnected nodes. The connections between nodes play the crucial part, these can be  optimized through meta-heurisitic optimisation algorithms (e.g. genetic  algorithms). The result is a network of nodes composed by ""moving parts"" which can be  somehow altered by global optimization algorithms.

The purpose of the project is purely experimental and we are willing to accept any contribution and comment.

[https://github.com/aljabr0/hypergraph](https://github.com/aljabr0/hypergraph)",2,1,False,self,,,,,
47,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,5,am6jn6,self.MachineLearning,[R] Identifying and Correcting Label Bias in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/am6jn6/r_identifying_and_correcting_label_bias_in/,tldrtldreverything,1549053686,"Hey, I published a summary of a new paper from Google with a very interesting approach to reducing bias in ML algorithms.  The main idea is that you can look at a biased dataset as an unbiased dataset that has gone through unwanted manipulation, and then train your ML algorithm based on the unbiased dataset you uncovered. Using some simple and elegant mathematical tricks, the researchers were able to apply this technique to achieve excellent results in un-biasing datasets. Full summary here: https://www.lyrn.ai/2019/01/29/identifying-and-correcting-label-bias-in-machine-learning/",0,1,False,self,,,,,
48,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,6,am6yjv,nature.com,Machine-learning algorithms able to predict what word a Twitter user is most likely to write next based on their social network about as often as researchers could when based on users' own previous tweets and interactions.,https://www.reddit.com/r/MachineLearning/comments/am6yjv/machinelearning_algorithms_able_to_predict_what/,Science_Podcast,1549056120,,1,1,False,default,,,,,
49,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,6,am6z73,self.deeplearning,Deep Learning Journal Club,https://www.reddit.com/r/MachineLearning/comments/am6z73/deep_learning_journal_club/,kal138,1549056226,,0,1,False,default,,,,,
50,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,6,am75g7,youtu.be,Coding a Tensorflow Clone in C++,https://www.reddit.com/r/MachineLearning/comments/am75g7/coding_a_tensorflow_clone_in_c/,karanchahal1996,1549057285,,0,1,False,https://b.thumbs.redditmedia.com/8ItKKC_BbGY7MfnLwkjVHWTQ6vnHlVtxMowsdMaCwOY.jpg,,,,,
51,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,6,am76v7,arxiv.org,[1901.11390] MONet: Unsupervised Scene Decomposition and Representation,https://www.reddit.com/r/MachineLearning/comments/am76v7/190111390_monet_unsupervised_scene_decomposition/,azhag,1549057517,,4,1,False,default,,,,,
52,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,7,am7c5z,self.MachineLearning,How to adopt transfer learning in sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/am7c5z/how_to_adopt_transfer_learning_in_sentiment/,CathyQian,1549058406,[removed],0,1,False,self,,,,,
53,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,7,am7x1y,self.MachineLearning,[D] What kind of Machine Learning Model/Neural Network would be used to predict the second part of a pair of images after being trained?,https://www.reddit.com/r/MachineLearning/comments/am7x1y/d_what_kind_of_machine_learning_modelneural/,antondan,1549061955,"Hello, Noobie here. So lets say I have lots of pairs of images like the one below. Where A is a low resolution drawn sprite of a person with his hands down and B with his hands up. I want to use all those pairs of images in order to train a machine learning program, then I want to give it a new image A and it producing image B. What do I need to look into?  


Thanks in advance.  


&amp;#x200B;

https://i.redd.it/yy207v2se1e21.png",8,1,False,https://b.thumbs.redditmedia.com/bm1urXXBkwVYhPA7Ie9twYyes_viTs27bVpzMLys6Ag.jpg,,,,,
54,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,8,am84sy,self.MachineLearning,is it possible to train a model so it can teach you something in the future ?,https://www.reddit.com/r/MachineLearning/comments/am84sy/is_it_possible_to_train_a_model_so_it_can_teach/,charara007,1549063297,[removed],0,1,False,self,,,,,
55,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,9,am8mw6,self.MachineLearning,[D] Importance of BatchNorm in Attention papers,https://www.reddit.com/r/MachineLearning/comments/am8mw6/d_importance_of_batchnorm_in_attention_papers/,WillingCucumber,1549066560,"Hi all,

All the attention papers generally use dot product to evaluate the similarity. 

Considering self attention paper, where we have a query map and a key map, which are generated from the same feature map using different 1X1 convolutions. How important is BatchNorm post such convolutions ?

&amp;#x200B;

Reason I am asking this is:  
If my features at pixel location 1,2,3 are \[2,2\], \[2,2\], and \[-2,40\], then according to dot product 1 and 3 are more correlated compared to 1 and 2.

&amp;#x200B;

Thanks !! ",4,1,False,self,,,,,
56,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,9,am8vdf,self.MachineLearning,[D] Investing 6500$ in a Computer Vision Rack,https://www.reddit.com/r/MachineLearning/comments/am8vdf/d_investing_6500_in_a_computer_vision_rack/,nicoj10,1549068213,"Hi,

I know this topic has been discussed at several points. But here I would like to ask for your opinion on two a little more specific topics:

1. Since AMD started to build good processors again, would you consider a Threadripper 2990X for example? Not for training of course but for preprocessing mainly. A cheaper alternative would be a I9-9900k for example. 
2. Would you consider the Titan RTX even though 2x RTX 2080 TI (via NVLINK) would probably still be faster (would it?). If I save money on the GPU I could also maybe fit in a RTX Titan and a RTX 2080 TI. 

What I am planning to do is train multiple parallel Resnet50/Resnet152 and have them combined as an ensemble model.

I am going to run this on 500.000 Images so a lot of preprocessing will be necessary before I can even start. Therefore I assume that I'll need a lot of VRAM. 

&amp;#x200B;

The current setup is:

&amp;#x200B;

*Processing img x44zrkqdx1e21...*

What do you think about these two issues?",13,1,False,self,,,,,
57,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,12,amaa8u,self.MachineLearning,[D]KDD conference question: How prestigious are Applied data science track papers compared to Research track papers?,https://www.reddit.com/r/MachineLearning/comments/amaa8u/dkdd_conference_question_how_prestigious_are/,hitman_reborn_no1,1549078285,,3,1,False,self,,,,,
58,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,12,amahrm,analyticsvidhya.com,A Comprehensive Learning Path for Deep Learning in 2019,https://www.reddit.com/r/MachineLearning/comments/amahrm/a_comprehensive_learning_path_for_deep_learning/,hiren_p,1549079861,,0,1,False,default,,,,,
59,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,13,amar5l,i.redd.it,Button from the CU (Buffaloes) NEXT conference last night. Just add Data #JustAddData,https://www.reddit.com/r/MachineLearning/comments/amar5l/button_from_the_cu_buffaloes_next_conference_last/,bbateman2011,1549081882,,0,1,False,default,,,,,
60,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,13,amatez,self.MachineLearning,A machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/amatez/a_machine_learning_game_ive_been_working_on/,twm7,1549082385,"Wasn't sure where to post this as I'm still working on it but wanted to put it out there to get any useful feedback or thoughts from the experts. It's basically a game similar to 20 Questions (or Animal, Vegetable, Mineral) that attempts to ask you questions to work out an object you are thinking about. You can think of everyday items (animals, household objects, food, quite a bit of other stuff etc) and it has 30 questions to try and guess the item. I've been working on it for a while but not sure what to do next so interested to hear anyone's thoughts...

The link for anyone that wants to try it out is [incredicat.com](http://www.incredicat.com/)

Thanks in advance!",0,1,False,self,,,,,
61,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,14,amb34a,self.MachineLearning,What are some ML methods/techniques for creating art other than Neural Style Transfer?,https://www.reddit.com/r/MachineLearning/comments/amb34a/what_are_some_ml_methodstechniques_for_creating/,ihavenoyukata,1549084578,[removed],0,1,False,self,,,,,
62,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,15,ambnma,self.MachineLearning,What is the ideal (male and female) face according to a beauty ranking algorithm?,https://www.reddit.com/r/MachineLearning/comments/ambnma/what_is_the_ideal_male_and_female_face_according/,tylersnard,1549089595,[removed],0,1,False,self,,,,,
63,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,15,ambnv2,self.MachineLearning,Papers with code for NLP,https://www.reddit.com/r/MachineLearning/comments/ambnv2/papers_with_code_for_nlp/,speedcell4,1549089662,"I found an awesome collection which ranks the CV papers by its repo stars

[https://github.com/zziz/pwc](https://github.com/zziz/pwc)

&amp;#x200B;

I am just wondering if there are some similar repositories for NLP? 

if no, is anyone interested in building one with me?",0,1,False,self,,,,,
64,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,17,amcbkt,i.redd.it,Check this,https://www.reddit.com/r/MachineLearning/comments/amcbkt/check_this/,MarkPetter,1549096544,,0,1,False,https://b.thumbs.redditmedia.com/nkRechsGNlw0VKynbV-F58n9daS37CUnvtASGtnkVGg.jpg,,,,,
65,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,18,amcjzp,self.MachineLearning,Linear Regression From Scratch With Python  Sigmoid  Medium,https://www.reddit.com/r/MachineLearning/comments/amcjzp/linear_regression_from_scratch_with_python/,SarvasvKulpati,1549099132,[removed],0,1,False,self,,,,,
66,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,18,amcm0u,apenwarr.ca,[D] Forget privacy: you're terrible at targeting anyway,https://www.reddit.com/r/MachineLearning/comments/amcm0u/d_forget_privacy_youre_terrible_at_targeting/,alexeyr,1549099773,,0,1,False,default,,,,,
67,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,19,amcucd,i.redd.it,Help!!! Confused regarding the representation of these linear equations on graph.(Taken from Mathematics for machine learning book),https://www.reddit.com/r/MachineLearning/comments/amcucd/help_confused_regarding_the_representation_of/,suchachin,1549102319,,0,1,False,https://b.thumbs.redditmedia.com/9EWnTM4nrBvsFLcJT-q-uMN2jZWNLY_PZyxRwuCRxFw.jpg,,,,,
68,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,19,amd13i,self.MachineLearning,stock price predict next day,https://www.reddit.com/r/MachineLearning/comments/amd13i/stock_price_predict_next_day/,GoBacksIn,1549104309,[removed],1,1,False,self,,,,,
69,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,19,amd334,self.MachineLearning,Following Machine Learning papers,https://www.reddit.com/r/MachineLearning/comments/amd334/following_machine_learning_papers/,kjaisingh,1549104901,[removed],0,1,False,self,,,,,
70,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,19,amd35p,youtu.be,Big new holland teaching the ways,https://www.reddit.com/r/MachineLearning/comments/amd35p/big_new_holland_teaching_the_ways/,my_little_donkey,1549104921,,0,1,False,https://b.thumbs.redditmedia.com/2vwbXGMju0JBijLDWGOrSZFaTr2MS6gBtT-JBXaafjw.jpg,,,,,
71,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,20,amd9iq,github.com,Amazing Machine Learning Open Source for the Past Year (v.2019),https://www.reddit.com/r/MachineLearning/comments/amd9iq/amazing_machine_learning_open_source_for_the_past/,ccGardnerr,1549106731,,0,1,False,default,,,,,
72,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,20,amd9qp,github.com,[P] Amazing Machine Learning Open Source for the Past Year (v.2019),https://www.reddit.com/r/MachineLearning/comments/amd9qp/p_amazing_machine_learning_open_source_for_the/,ccGardnerr,1549106797,,0,1,False,https://b.thumbs.redditmedia.com/EZ43HYHbJPN_s4sw6rQE6eVK1XhH0yGYg6u4G4lAjfU.jpg,,,,,
73,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,20,amd9sj,medium.com,Linear Regression From Scratch With Python  Sigmoid  Medium,https://www.reddit.com/r/MachineLearning/comments/amd9sj/linear_regression_from_scratch_with_python/,SarvasvKulpati,1549106811,,1,1,False,https://b.thumbs.redditmedia.com/i68nSeT4RIrgaj0xFqwTQiwTphx6I3Ofs2YTqy9SeUM.jpg,,,,,
74,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,20,amdanr,self.MachineLearning,[P] Amazing Machine Learning Open Source (v.2019),https://www.reddit.com/r/MachineLearning/comments/amdanr/p_amazing_machine_learning_open_source_v2019/,ccGardnerr,1549107063,[removed],0,1,False,self,,,,,
75,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,21,amdm2t,youtube.com,[R] Basics of the Transformer Model (Google) explained with examples,https://www.reddit.com/r/MachineLearning/comments/amdm2t/r_basics_of_the_transformer_model_google/,DemiourgosD,1549110142,,0,1,False,https://b.thumbs.redditmedia.com/bpC0V78resjfNps21TpQ1Zmzh8BtWB1Sn9cZgpXVXnE.jpg,,,,,
76,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,23,amedjh,arxiv.org,"[1901.06955v1] Deep Neural Network Approximation for Custom Hardware: Where We've Been, Where We're Going",https://www.reddit.com/r/MachineLearning/comments/amedjh/190106955v1_deep_neural_network_approximation_for/,mcianster,1549116680,,1,1,False,default,,,,,
77,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,23,amedkk,self.MachineLearning,[D] Random Forest questions,https://www.reddit.com/r/MachineLearning/comments/amedkk/d_random_forest_questions/,qwerty123000,1549116686,"Preface: am a newbie, need to get familiar with methods for analytics at work.

Question 1: how does random forest voting across trees work? Let's say there are a million trees. Is the overall prediction the simple unweighted average of all million trees' predictions?

Question 2: I understand the random forest is something of a black box. But is it possible for the black box to scan all the trees and recommend a few individual trees as those that are ""closest"" to the results of the entire forest, i.e., would have been pretty good single decision trees in terms of fit? In many business settings I'd rather have something with slightly worse predictive power but easier to explain which variables were used. Or is this pointless and i should just use a regular decision tree if I need to be able to explain it since I wouldn't be at the mercy of random variable selection?

Thanks",12,1,False,self,,,,,
78,MachineLearning,t5_2r3gv,2019-2-2,2019,2,2,23,amera3,self.MachineLearning,[D] Two kinds of ML models,https://www.reddit.com/r/MachineLearning/comments/amera3/d_two_kinds_of_ml_models/,anderl1980,1549119503,There are two types of models which influence the system and which do not. Unfortunately I forgot what they are called.,9,1,False,self,,,,,
79,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,0,ametjb,self.MachineLearning,LSTM for air pollution prediction,https://www.reddit.com/r/MachineLearning/comments/ametjb/lstm_for_air_pollution_prediction/,starzmustdie,1549119902,[removed],0,1,False,https://b.thumbs.redditmedia.com/AKhFDZ0vOhYWpinmwvSgpYm4IWHnT-Iz_Ygzu1V1eVw.jpg,,,,,
80,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,0,ameur3,self.MachineLearning,[D] DeepFake Ransomware - What happens when harassment gets automated at scale?,https://www.reddit.com/r/MachineLearning/comments/ameur3/d_deepfake_ransomware_what_happens_when/,paubric,1549120129,"[https://medium.com/@paul\_andrei/deepfake-ransomware-oaas-part-1-b6d98c305cd9](https://medium.com/@paul_andrei/deepfake-ransomware-oaas-part-1-b6d98c305cd9)

""\[...\] is a type of malicious software that automatically generates fake video which shows the victim performing an incriminatory or intimate action and threatens to distribute it unless a ransom ispaid.""",32,1,False,self,,,,,
81,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,0,amf689,self.MachineLearning,Artificial Intelligence Career Path | Step-by-Step Guide,https://www.reddit.com/r/MachineLearning/comments/amf689/artificial_intelligence_career_path_stepbystep/,m_irfu,1549122204,[removed],0,1,False,self,,,,,
82,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,0,amf97e,self.MachineLearning,Hello There. I created nlp-tutoral repository who is studying NLP(Natural Language Processing) using TensorFlow and Pytorch,https://www.reddit.com/r/MachineLearning/comments/amf97e/hello_there_i_created_nlptutoral_repository_who/,nlkey2022,1549122703,"Hello. This is my first post in reddit.

I created nlp-tutoral repository who is studying NLP(Natural Language Processing) using TensorFlow and Pytorch  inspired by other example code.

You can see NNLM which is first language model, baseline model such as RNN, LSTM, TextCNN, Word2Vec in NLP. Also You can more easily learn NLP model, training steps as  implemented Only ONE file (\*.py) from seq2seq, attention, bi-LSTM attenton, Transformer(self-attention), to BERT model.

I implemented with Pytorch, Tensorflow both.

[https://github.com/graykode/nlp-tutorial](https://github.com/graykode/nlp-tutorial?fbclid=IwAR3_DRc1-fcWQjumjDTM6e8xCpwZ09kYcXKOLBaS3AC2zbVeGvXKJOz4voo) This is my repository",0,1,False,self,,,,,
83,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,0,amfakl,self.MachineLearning,"Is Reinforcement Learning used in actual, engineering applications in industry?",https://www.reddit.com/r/MachineLearning/comments/amfakl/is_reinforcement_learning_used_in_actual/,QuietPragmatism,1549122937,[removed],0,1,False,self,,,,,
84,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfc57,self.MachineLearning,What should I use or where should I look for?,https://www.reddit.com/r/MachineLearning/comments/amfc57/what_should_i_use_or_where_should_i_look_for/,ar3106,1549123210,[removed],0,1,False,self,,,,,
85,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfinl,self.MachineLearning,[Project] nlp-tutoral repository who is studying NLP(Natural Language Processing) using TensorFlow and Pytorch,https://www.reddit.com/r/MachineLearning/comments/amfinl/project_nlptutoral_repository_who_is_studying/,nlkey2022,1549124335," 

Hello. This is my first post in reddit

I created nlp-tutoral repository who is studying NLP(Natural Language Processing) using TensorFlow and Pytorch inspired by other example code.

You can see NNLM which is first language model, baseline model such as RNN, LSTM, TextCNN, Word2Vec in NLP. Also You can more easily learn NLP model, training steps as implemented Only ONE file (\*.py) from seq2seq, attention, bi-LSTM attenton, Transformer(self-attention), to BERT model.  
I implemented with Pytorch, Tensorflow both.

[https://github.com/graykode/nlp-tutorial](https://github.com/graykode/nlp-tutorial?fbclid=IwAR3_DRc1-fcWQjumjDTM6e8xCpwZ09kYcXKOLBaS3AC2zbVeGvXKJOz4voo) This is my repository",24,1,False,self,,,,,
86,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfizh,heartbeat.fritz.ai,A curated list of AI &amp; machine learning newsletters,https://www.reddit.com/r/MachineLearning/comments/amfizh/a_curated_list_of_ai_machine_learning_newsletters/,zsajjad,1549124395,,0,1,False,default,,,,,
87,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfjlu,trumporstaff.com,[P] Made an app that uses machine learning to predict if a tweet is written by Trump or a staffer,https://www.reddit.com/r/MachineLearning/comments/amfjlu/p_made_an_app_that_uses_machine_learning_to/,Rockefellow,1549124501,,0,1,False,default,,,,,
88,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfkat,david-abel.github.io,[R] AAAI 2019 Notes,https://www.reddit.com/r/MachineLearning/comments/amfkat/r_aaai_2019_notes/,pdxdabel,1549124626,,1,1,False,default,,,,,
89,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfn1l,self.MachineLearning,[D] How to get multiple GPU free access online?,https://www.reddit.com/r/MachineLearning/comments/amfn1l/d_how_to_get_multiple_gpu_free_access_online/,rahulbhalley,1549125089,"I am doing research in deep learning and require multiple GPUs (like 8 GPUs). And am using PyTorch 1.0 for it. And may require it for ~7 days. One way for free GPU I know is Kaggle or Google Colab (Tesla K80). But I am finding 8 GPUs in parallel cuz network is too big. 

Please let me know if its possible in the comments. ",16,1,False,self,,,,,
90,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfrwe,heartbeat.fritz.ai,"Build, Save, and Host Your First Machine Learning Model Using Flask and Heroku",https://www.reddit.com/r/MachineLearning/comments/amfrwe/build_save_and_host_your_first_machine_learning/,mwitiderrick,1549125905,,0,1,False,default,,,,,
91,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,1,amfw8v,self.MachineLearning,Help in ML project ideas,https://www.reddit.com/r/MachineLearning/comments/amfw8v/help_in_ml_project_ideas/,the_aris,1549126627,I have to complete a project in my final year based on machine learning. I am looking for various ideas/topics on which I can work in order to complete my project. Can anyone help me regarding the same. Thanks in advance!,0,1,False,self,,,,,
92,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,2,amg4og,self.MachineLearning,Amazon Pinpoint,https://www.reddit.com/r/MachineLearning/comments/amg4og/amazon_pinpoint/,AWSTrainingInHouston,1549128025,[removed],0,1,False,self,,,,,
93,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,2,amgc3s,youtube.com,A Quick Exploration of the Image of Artificial Intelligence In Pop Culture,https://www.reddit.com/r/MachineLearning/comments/amgc3s/a_quick_exploration_of_the_image_of_artificial/,nix_zu_verlian,1549129251,,0,1,False,default,,,,,
94,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,3,amgs5x,self.MachineLearning,"[D] Besides work embedding architectures (ie Word2Vec, GloVe, etc), are there any other ML architectures where only a small fraction of the total weights are trained at every step?",https://www.reddit.com/r/MachineLearning/comments/amgs5x/d_besides_work_embedding_architectures_ie/,BatmantoshReturns,1549131910,"I'm about to submit a Tensorflow feature request to allow weights to be stored as binary files and only loaded into memory when trained, kind of like tfrecords/datasetAPI, which would greatly increase the number of weights in any model where only a small percentage of the weights are trained per step. 

I'm doing a literature survey to find as many cases where such a feature would be beneficial, so that the TF developers can access what sort of priority to give such a feature. 

So far I found PathNet and Extreme learning machines and it's derivatives. This feature may also simplify training and increase efficiency of other types of training where weights are being turned off, like dropout and pruning. ",9,1,False,self,,,,,
95,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,3,amgva5,youtube.com,"A journey through neural networks - Self Driving, simulations and education",https://www.reddit.com/r/MachineLearning/comments/amgva5/a_journey_through_neural_networks_self_driving/,DevTechRetopall,1549132421,,0,1,False,https://b.thumbs.redditmedia.com/g5pij9nYL7r_Yq75c0Xi1kzujQ4qfSGoD0dnObmYP5s.jpg,,,,,
96,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,3,amh0ms,mlmed.org,Chester the AI Radiology Assistant - a free open source web delivered locally computed tool to diagnosis chest xrays in the browser using neural networks at global scale (NOT FOR MEDICAL USE),https://www.reddit.com/r/MachineLearning/comments/amh0ms/chester_the_ai_radiology_assistant_a_free_open/,ieee8023,1549133317,,0,1,False,default,,,,,
97,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,3,amh1ew,self.MachineLearning,[P] (noob) Questions regarding VGGNet16's features extraction,https://www.reddit.com/r/MachineLearning/comments/amh1ew/p_noob_questions_regarding_vggnet16s_features/,mourad1081,1549133433,"Hello, I am currently programming an action detection algorithm for videos. In the paper I've read, they started by extracting features of each frame of the videos with VGG16. Then, they fed that to a bidirectional LSTM that will learn the actions. Here are my questions:

1. Why is it good to extract features and not immediately feed the LSTM with the frames of the videos instead of the extracted features?

2. In my code, VGG16 generated a 300 Mo file despite the mp4 video's size was only 8 Mo. Is it a normal behaviour that I have a *much* bigger file than the original video?

Thank you!
",1,1,False,self,,,,,
98,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,4,amh76s,medium.com,3 Ways Blockchain will Unleash the Full Potential of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/amh76s/3_ways_blockchain_will_unleash_the_full_potential/,PAAlmasi,1549134394,,0,1,False,https://b.thumbs.redditmedia.com/Nwt-0kWg9QXMnvPJKWGIca584EArYI1ylg6BvkiVXhc.jpg,,,,,
99,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,4,amhbl5,self.MachineLearning,I'm looking into the possibility of doing some kind of signal classification from EEG data,https://www.reddit.com/r/MachineLearning/comments/amhbl5/im_looking_into_the_possibility_of_doing_some/,techtrum,1549135130,[removed],0,1,False,self,,,,,
100,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,5,amhtkp,self.MachineLearning,Can you solve the dual support vector regression problem with Lagrange multipliers?,https://www.reddit.com/r/MachineLearning/comments/amhtkp/can_you_solve_the_dual_support_vector_regression/,davidboja4,1549138060,[removed],0,1,False,https://b.thumbs.redditmedia.com/9qOZpPd3uxTxZ3Jj8TKYKmhGPTLmCt2b7zfN66gu9Ig.jpg,,,,,
101,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,6,amimtf,self.MachineLearning,Accelerate your career growth - From Data to Insights with Google Cloud Platform Specialization,https://www.reddit.com/r/MachineLearning/comments/amimtf/accelerate_your_career_growth_from_data_to/,internetdigitalentre,1549143041,[removed],0,1,False,https://b.thumbs.redditmedia.com/ipYTQGfPNtBIaKy3xhCIsfRlDxSmj0-iLN646xOuu7I.jpg,,,,,
102,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,7,amj0vv,openreview.net,"[R] Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet ""the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies""",https://www.reddit.com/r/MachineLearning/comments/amj0vv/r_approximating_cnns_with_bagoflocalfeatures/,downtownslim,1549145386,,0,1,False,default,,,,,
103,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,7,amj60h,self.MachineLearning,[D] What are some modern machine learning approaches for Time Series Analysis/Forecasting?,https://www.reddit.com/r/MachineLearning/comments/amj60h/d_what_are_some_modern_machine_learning/,Fender6969,1549146253,"I am newer to time series, and have played a small role on a team where we used ARIMA/ARIMAX methods for forecasting. We are assigned to do some research on modern machine learning methods to do Time Series (TS). 

From what Ive gathered, is Deep Learning seems to be good. The issue with deep learning is that if we lose interpretability, this makes ours superiors very nervous. So we want to preferably maintain as much as interpretability as possible. 

Any recommendations would be great!",49,1,False,self,,,,,
104,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,7,amj7nt,self.MachineLearning,Can Deep Learning continue to perfect itself? Or as it improves other areas weaken (explanation below),https://www.reddit.com/r/MachineLearning/comments/amj7nt/can_deep_learning_continue_to_perfect_itself_or/,kalavala93,1549146538,[removed],0,1,False,self,,,,,
105,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,8,amjiyj,self.MachineLearning,"[D] Growing collection of Deep Learning, Machine Learning, Reinforcement Learning lectures",https://www.reddit.com/r/MachineLearning/comments/amjiyj/d_growing_collection_of_deep_learning_machine/,kmario23,1549148500,"Hello everyone,  
  I have collected a list of freely available courses on *Machine Learning, Deep Learning, Reinforcement Learning, Natural Language Processing, Computer Vision, Probabilistic Graphical Models, Machine Learning Fundamentals, and Deep Learning boot camps or summer schools*. 

The complete list is available here: [deep learning drizzle](https://github.com/kmario23/deep-learning-drizzle)

Feel free to share it with your friends, colleagues, or anyone who would be interested in learning ML independently. Also, please make yourself comfortable in forking or starring the repo as you'd like.

Also, if you have some suggestions, please leave a comment here or raise an issue in the git repo.

GitHub repo: [deep learning drizzle]([deep learning drizzle](https://github.com/kmario23/deep-learning-drizzle)

I wish you all a nice weekend!",20,1,False,self,,,,,
106,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,9,amk9k2,self.MachineLearning,"If you could start a Ph.D. In Machine Learning today, what would your research interest be?",https://www.reddit.com/r/MachineLearning/comments/amk9k2/if_you_could_start_a_phd_in_machine_learning/,causalai,1549153310,"Actually, It was three years ago questions as below

[https://www.reddit.com/r/MachineLearning/comments/38sub5/if\_you\_could\_start\_a\_phd\_in\_machine\_learning/](https://www.reddit.com/r/MachineLearning/comments/38sub5/if_you_could_start_a_phd_in_machine_learning/)

However, I wonder the current issue of that question :D",0,1,False,self,,,,,
107,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,11,aml68d,self.MachineLearning,Have any papers looked into models for denoising/processing video data using ConvLSTM models?,https://www.reddit.com/r/MachineLearning/comments/aml68d/have_any_papers_looked_into_models_for/,kds_medphys,1549159731,[removed],0,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,12,amlnvw,youtube.com,Build a Tensorflow Clone in C++,https://www.reddit.com/r/MachineLearning/comments/amlnvw/build_a_tensorflow_clone_in_c/,karanchahal1996,1549163373,,0,1,False,https://b.thumbs.redditmedia.com/9cgkQQ5bnMDMakWtR99WTaauHHdFl3sqwubm9zuPABM.jpg,,,,,
109,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,12,amlwgn,sociable.co,[N] Keeping Prometheus Out of Machine Learning Systems - The Sociable,https://www.reddit.com/r/MachineLearning/comments/amlwgn/n_keeping_prometheus_out_of_machine_learning/,hinchlt,1549165178,,0,1,False,default,,,,,
110,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,13,amm6x2,analyticsinsight.net,Top 10 Machine Learning Programming Languages | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/amm6x2/top_10_machine_learning_programming_languages/,analyticsinsight,1549167398,,0,1,False,default,,,,,
111,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,13,amm8c7,self.MachineLearning,[D] Recommend algorithms for predicting race outcome between 1 and 8 position.,https://www.reddit.com/r/MachineLearning/comments/amm8c7/d_recommend_algorithms_for_predicting_race/,sikocan,1549167716,"I've put together dataset of race data with results and attempted using Amazons machine learning platform and based on my dataset, it's recommending linear regression model and predictions based on 4000 races with 34 columns of data, it's scoring poorly and I'm not getting any better with more data.

Is there an algorithm you can recommend? Or is there a place I can post the request and pay someone to write python code which I can then supply data in CSV format and does predictions on different models?

Thanks in advance",18,1,False,self,,,,,
112,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,13,ammdvk,self.MachineLearning,What's the most common algorithmic approach for predicting probability among known non-binary results?,https://www.reddit.com/r/MachineLearning/comments/ammdvk/whats_the_most_common_algorithmic_approach_for/,soojcho,1549168959,[removed],0,1,False,self,,,,,
113,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,14,ammvkn,self.MachineLearning,Feeding data to the Capsule Networks,https://www.reddit.com/r/MachineLearning/comments/ammvkn/feeding_data_to_the_capsule_networks/,Raman070,1549172875,[removed],0,1,False,https://b.thumbs.redditmedia.com/uwvRqiziYSBAV-9z-CDAdiUYMZB3yYWAK6J1msQsoyw.jpg,,,,,
114,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,16,amnef4,self.MachineLearning,Automated Hyperparameter Optimization for Reinforcement Learning Models,https://www.reddit.com/r/MachineLearning/comments/amnef4/automated_hyperparameter_optimization_for/,slavakurilyak,1549177775,[removed],0,1,False,self,,,,,
115,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,16,amngli,self.MachineLearning,Meta-Metric for Overfitting of Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/amngli/metametric_for_overfitting_of_machine_learning/,slavakurilyak,1549178397,[removed],0,1,False,self,,,,,
116,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,16,amnhy2,self.MachineLearning,Should I correct the reviewer for some unreliable measurement (GPU memory usage) he/she is asking me to do?,https://www.reddit.com/r/MachineLearning/comments/amnhy2/should_i_correct_the_reviewer_for_some_unreliable/,laoreja001,1549178794,[removed],1,1,False,self,,,,,
117,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,16,amnjju,lilianweng.github.io,Generalized Language Models,https://www.reddit.com/r/MachineLearning/comments/amnjju/generalized_language_models/,kartayyar,1549179282,,0,1,False,default,,,,,
118,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,18,amoae7,self.MachineLearning,Final year project selection,https://www.reddit.com/r/MachineLearning/comments/amoae7/final_year_project_selection/,spacesinghh,1549187804,[removed],0,1,False,self,,,,,
119,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,19,amoazy,self.MachineLearning,Mathematical sound foundation [Graduate level],https://www.reddit.com/r/MachineLearning/comments/amoazy/mathematical_sound_foundation_graduate_level/,ProfessionalFinger5,1549188007,[removed],0,1,False,self,,,,,
120,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,19,amokxz,self.MachineLearning,"What are some good lectures on data understanding, data analysis, data exploration?",https://www.reddit.com/r/MachineLearning/comments/amokxz/what_are_some_good_lectures_on_data_understanding/,rverdelli,1549191063,[removed],1,1,False,self,,,,,
121,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,19,amommu,hackernoon.com,[P] Generating Alpha from Information Arbitrage in the Financial Markets using NLP Datasets: ,https://www.reddit.com/r/MachineLearning/comments/amommu/p_generating_alpha_from_information_arbitrage_in/,BattleStarDrive,1549191596,,0,1,False,default,,,,,
122,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,20,amonyz,self.MachineLearning,[D] Sound Mathematical Foundation for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/amonyz/d_sound_mathematical_foundation_for_machine/,ProfessionalFinger5,1549191975,"I'm starting to specialize in Machine Learning and would like to discuss the way forward. My exposure has been minimal so far; I did implement a CNN to do basic image recognition from the MNIST set in C and am currently trying to make the kernel run in parallel on CUDA.

I'm a CS undergraduate, and on the mathematical side completed Linear Algebra, Real Analysis, Probability &amp; Statistics, various introductions to topics from Discrete Mathematics (Combinatorics, Graph- &amp; Number Theory, Abstract Algebra, Formal Logic), and a bunch of Algorithm courses that notably also included Numerical Analysis, some Optimization, Convex Geometry and some Game Theory topics.

I'm considering taking Mathematical Optimization, Foundations of Statistics and Information Theory. The various Machine Learning courses are very thorough by reputation, and consist of a mix between rigorous theory and programming projects where we implement the methods from scratch. There's also a Statistical Learning Theory course, for which I am very excited. Additionally, I'm not forgetting to take courses on (Big) Data (Analysis / Mining / Retrieval), etc.

Does this line up sound good? Are there other fundamental topics to consider? I don't necessarily aim to do a PhD, but I want to learn what's necessary to ""understand"" what's going on and to be able to create new stuff and build upon latest research. Consequently, that also includes the necessary tools to learn the Graduate Level Math that might creep up in Machine Learning in the present and future.",0,1,False,self,,,,,
123,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,20,amoyss,self.MachineLearning,How to train Multi label classification data sing bayesian neural network,https://www.reddit.com/r/MachineLearning/comments/amoyss/how_to_train_multi_label_classification_data_sing/,jsaini94,1549195165,[removed],1,1,False,https://a.thumbs.redditmedia.com/w6VJnfmVUnjFX966Vv8A3LSWIdjHYlI6D3loZpxGQu8.jpg,,,,,
124,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,21,amp63q,self.MachineLearning,"[D] How do i ""push"" features down into the lower layers of a NN so i can use them as pre-learning",https://www.reddit.com/r/MachineLearning/comments/amp63q/d_how_do_i_push_features_down_into_the_lower/,yazriel0,1549197101,"With images and CNNs, it is convenient to thrown away the upper fully connected layers, and use the lower convolutions as features for pre-learning.

My networks are similar to res-net. Any tips or directions to try ?

The domain is completely simulated, graph-type inputs, with lots of samples and search, so training is expensive but ""easy"" to stabilize,

",9,1,False,self,,,,,
125,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,22,ampe4g,self.MachineLearning,Reinforcement Learning Frameworks,https://www.reddit.com/r/MachineLearning/comments/ampe4g/reinforcement_learning_frameworks/,lipsterge,1549199190,[removed],0,1,False,self,,,,,
126,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,22,ampi1l,codeingschool.com,Machine Learning Based Diabetes Prediction Software with voice control Full Tutorial:,https://www.reddit.com/r/MachineLearning/comments/ampi1l/machine_learning_based_diabetes_prediction/,subhamroy021,1549200142,,0,1,True,nsfw,,,,,
127,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,22,ampqon,self.MachineLearning,"ML workshop @ Brown, lectures available online",https://www.reddit.com/r/MachineLearning/comments/ampqon/ml_workshop_brown_lectures_available_online/,rickkava,1549202177,"ICERM at Brown University held a workshop on ML theory and applications in the computational sciences last week. Videos / slides from all the lectures are available [here (scroll down for videos)](https://icerm.brown.edu/events/ht19-1-sml/). Lots of interesting stuff there, maybe useful for somebody on here. They seem to have all the previous workshops online as well.",0,1,False,self,,,,,
128,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,23,amq240,self.MachineLearning,Is there any free machine learning cloud provider where I can work online?,https://www.reddit.com/r/MachineLearning/comments/amq240/is_there_any_free_machine_learning_cloud_provider/,codefrk,1549204613,[removed],0,1,False,self,,,,,
129,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,23,amq3dz,towardsdatascience.com,Data Science Has Become Too Vague  Towards Data Science,https://www.reddit.com/r/MachineLearning/comments/amq3dz/data_science_has_become_too_vague_towards_data/,RacerRex9727,1549204885,,0,1,False,default,,,,,
130,MachineLearning,t5_2r3gv,2019-2-3,2019,2,3,23,amq7k2,self.MachineLearning,[D] Baum Welch,https://www.reddit.com/r/MachineLearning/comments/amq7k2/d_baum_welch/,Unlistedd,1549205725,"Could you use Baum-Welch algorithm to identify a variable, **not** in terms of: Identify the proabaility of  happening, but rather: Identify the value of  given that  is the most likely outcome?",1,1,False,self,,,,,
131,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,0,amqk25,self.MachineLearning,[D] How to properly implement gradient penalty with non-saturating GAN loss?,https://www.reddit.com/r/MachineLearning/comments/amqk25/d_how_to_properly_implement_gradient_penalty_with/,Yggdrasil524,1549208112,"Quick question/sanity check:

I want to implement the loss function (equation 10) from [this paper](https://arxiv.org/pdf/1710.08446.pdf). The notation is slightly unclear to me.

For WGAN-GP, generator and discriminator losses (L_d, L_g) are defined:

gradient_penalty(y, x) = (l2_norm(dy / dx) - 1) ^ 2

L_g = -D(G(z))

L_d = D(x) - D(G(z)) + gradient_penalty(D(x_hat), x_hat)

where D is the unconstrained output of the discriminator function, G is the generator function, z is the latents, x is real images, and x_hat is a mixture of real and generated images

The non-saturating loss is defined:

L_g = -log(sigmoid(D(G(z)))

L_d = -log(sigmoid(D(G(z))) - log(sigmoid(1 - D(G(z)))

Now if I want to add a gradient penalty to the non-saturating loss should it be

L_d += gradient_penalty(D(x_hat), x_hat)

or

L_d += gradient_penalty(sigmoid(D(x_hat)), x_hat)

The reason I'm confused is that the paper uses the same notation for 
the Wasserstein discriminator, which outputs an unconstrained number, and the standard discriminator, which outputs a probability from 0 to 1.",9,1,False,self,,,,,
132,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,1,amqu2u,self.LanguageTechnology,"Multi-lingual sentence segmentation with single model for English, French, Italian.",https://www.reddit.com/r/MachineLearning/comments/amqu2u/multilingual_sentence_segmentation_with_single/,winchester6788,1549209924,,0,1,False,default,,,,,
133,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,1,amr75d,self.MachineLearning,Machine Learning Signals,https://www.reddit.com/r/MachineLearning/comments/amr75d/machine_learning_signals/,wootnoob,1549212102,[removed],0,1,False,self,,,,,
134,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,2,amrfn7,self.MachineLearning,Question about a technique in ML?,https://www.reddit.com/r/MachineLearning/comments/amrfn7/question_about_a_technique_in_ml/,theMusicalGamer88,1549213499,[removed],0,1,False,self,,,,,
135,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,2,amrmvn,self.MachineLearning,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/amrmvn/machine_learning/,Nasser_Simpson,1549214641," **Any recommendation? I have started Andrew Ng machine learning course in the middle now. Which is the best after this, Kaggle nano degree on udacity or Washington specialization on coursera** ",0,1,False,self,,,,,
136,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,2,amrtsl,self.MachineLearning,how to build a neural network such that its output has more variability?,https://www.reddit.com/r/MachineLearning/comments/amrtsl/how_to_build_a_neural_network_such_that_its/,qudcjf7928,1549215732,"Given the input, and given the fact that there can only be 1 output node,

no matter what neural network architecture I use

(residual  neural network, or regular MLP with batch normalization....etc, input  data normalization....etc), the output value is typically around -0.05  (plus or minus of about) 0.003

Because  the output value is so small in magnitude, it's near impossible to  train the system. I tried with just a linear activation for the output  node, but still the same result.",0,1,False,self,,,,,
137,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,2,amrtwl,self.MachineLearning,Question: How does the number of output classes affect inference time?,https://www.reddit.com/r/MachineLearning/comments/amrtwl/question_how_does_the_number_of_output_classes/,teling,1549215749,[removed],1,1,False,self,,,,,
138,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,3,ams1ds,self.MachineLearning,How do i create a dataset for convolution neural networks,https://www.reddit.com/r/MachineLearning/comments/ams1ds/how_do_i_create_a_dataset_for_convolution_neural/,06darknebula,1549216938,[removed],0,1,False,self,,,,,
139,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,3,ams4k2,self.MachineLearning,Website to keep up with recently published (and interesting) papers?,https://www.reddit.com/r/MachineLearning/comments/ams4k2/website_to_keep_up_with_recently_published_and/,coltar13,1549217428,[removed],0,1,False,self,,,,,
140,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,3,ams88b,arxiv.org,[1901.11503] Paper explaining why random search is a competitive alternative for Model-Free RL,https://www.reddit.com/r/MachineLearning/comments/ams88b/190111503_paper_explaining_why_random_search_is_a/,bronzestick,1549218021,,5,1,False,default,,,,,
141,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,4,amsv9q,self.MachineLearning,Real time instance segmentation with a web camera - fps atrocious [HELP],https://www.reddit.com/r/MachineLearning/comments/amsv9q/real_time_instance_segmentation_with_a_web_camera/,arialblack14,1549221562,"## Hello people,

I am very new to this machine/deep learning stuff so I would appreciate any help.I managed to have real time instance segmentation using this [article](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46) (it is using MaskRCNN).

The thing is I barely get 1-2 fps and that of course is unacceptable.Has anyone managed to get around this?

How does this [app](https://play.google.com/store/apps/details?id=com.cyberlink.youcammakeup&amp;hl=el) get to be so fast at this?Any thoughts/suggestions??

Thank you in advance!  


Sorry if this is the wrong sub for this. I posted this on r/learnmachinelearning but got no answers.",0,1,False,self,,,,,
142,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,5,amtl0f,youtube.com,Creating Neural Networks from Scratch,https://www.reddit.com/r/MachineLearning/comments/amtl0f/creating_neural_networks_from_scratch/,AIlearner678,1549225494,,0,1,False,default,,,,,
143,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,5,amtotn,self.MachineLearning,Any recommendation,https://www.reddit.com/r/MachineLearning/comments/amtotn/any_recommendation/,supla99,1549226080,[removed],0,1,False,self,,,,,
144,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,6,amub81,self.MachineLearning,Create Twitter WordCloud with just 40 lines of RCode,https://www.reddit.com/r/MachineLearning/comments/amub81/create_twitter_wordcloud_with_just_40_lines_of/,andrea_manero,1549229586,[removed],0,1,False,self,,,,,
145,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,7,amum6n,self.MachineLearning,Difference between tacotron and tacotron2,https://www.reddit.com/r/MachineLearning/comments/amum6n/difference_between_tacotron_and_tacotron2/,adi214,1549231302,,0,1,False,self,,,,,
146,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,9,amvxc5,i.redd.it,[D] Not saying which team it picked until after the game but..... here are my super bowl predictions from my model.,https://www.reddit.com/r/MachineLearning/comments/amvxc5/d_not_saying_which_team_it_picked_until_after_the/,existeverywhere,1549239647,,0,1,False,default,,,,,
147,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,9,amvxrt,self.MachineLearning,[D] Generalized Language Models (Overview),https://www.reddit.com/r/MachineLearning/comments/amvxrt/d_generalized_language_models_overview/,hardmaru,1549239728,"*We will discuss the models on learning contextualized word vectors, as well as the new trend in large unsupervised pre-trained language models which have achieved amazing SOTA results on a variety of language tasks.*

link to post: https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html

",1,1,False,self,,,,,
148,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,9,amw5tz,self.MachineLearning,Serverless Deployments of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/amw5tz/serverless_deployments_of_deep_learning/,slavakurilyak,1549241264,[removed],0,1,False,self,,,,,
149,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,9,amw8i0,arxiv.org,[R] Fixup Initialization: Residual Learning Without Normalization (They train 10K layer networks w/o BatchNorm),https://www.reddit.com/r/MachineLearning/comments/amw8i0/r_fixup_initialization_residual_learning_without/,wei_jok,1549241801,,12,1,False,default,,,,,
150,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,10,amwnch,self.MachineLearning,Automated Hyperparameter Optimization for Reinforcement Learning Models,https://www.reddit.com/r/MachineLearning/comments/amwnch/automated_hyperparameter_optimization_for/,slavakurilyak,1549244591,[removed],0,1,False,self,,,,,
151,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,14,amyawj,arxiv.org,[R] Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation,https://www.reddit.com/r/MachineLearning/comments/amyawj/r_understanding_impacts_of_highorder_loss/,singlasahil14,1549256679,,1,1,False,default,,,,,
152,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,15,amyqpc,self.MachineLearning,How to learn Mathematics for machine learning to Read research paper as well as writing research paper?,https://www.reddit.com/r/MachineLearning/comments/amyqpc/how_to_learn_mathematics_for_machine_learning_to/,sdwlo123,1549260043,[removed],0,1,False,self,,,,,
153,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,15,amys0p,self.MachineLearning,[R] Theories of Error Back-Propagation in the Brain,https://www.reddit.com/r/MachineLearning/comments/amys0p/r_theories_of_error_backpropagation_in_the_brain/,baylearn,1549260335,"*Can neurons do backprop? A [review](https://doi.org/10.1016/j.tics.2018.12.005) of how neural circuits could approximate the error back-propagation algorithm. The dynamics and plasticity of the models can be described within a common framework of energy minimisation.*

article (open access): https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30012-9

[pdf](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930012-9)
",5,1,False,self,,,,,
154,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,16,amzan3,self.MachineLearning,Finding Rare Event from a CSV file and adding values to new column in that CSV.,https://www.reddit.com/r/MachineLearning/comments/amzan3/finding_rare_event_from_a_csv_file_and_adding/,satyasashi,1549264725,[removed],0,1,False,self,,,,,
155,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,17,amzrdq,self.MachineLearning,[P] Lecture on Information and Intelligence in a view of Digital Physics,https://www.reddit.com/r/MachineLearning/comments/amzrdq/p_lecture_on_information_and_intelligence_in_a/,hiconcep,1549269245,"**This is a lecture in Korean with English slides and subtitle.**

It's a channel of lectures on information and intelligence. The first lecture introduces a broader interpretation of information and a different perspective of our universe, life and humans. 

[Information and Intelligence: The way of new thinking about our universe.](https://www.youtube.com/watch?v=K2F9jOf1nxE)",0,1,False,self,,,,,
156,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,18,amzyzd,self.MachineLearning,"What is your preferred framework for tuning hyperparameters [skorch, hypersearch,Ray, Other] ?",https://www.reddit.com/r/MachineLearning/comments/amzyzd/what_is_your_preferred_framework_for_tuning/,pigdogsheep,1549271534,[removed],0,1,False,self,,,,,
157,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,18,an070m,towardsdatascience.com,[P] Making Programming Quicker and Easier with Keyboard Macros,https://www.reddit.com/r/MachineLearning/comments/an070m/p_making_programming_quicker_and_easier_with/,osbornep,1549273849,,0,1,False,default,,,,,
158,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,19,an09n8,self.MachineLearning,[R] TuckER: Tensor Factorization for Knowledge Graph Completion,https://www.reddit.com/r/MachineLearning/comments/an09n8/r_tucker_tensor_factorization_for_knowledge_graph/,ibalazevic,1549274616,"&amp;#x200B;

https://i.redd.it/gicg4kh0zie21.png

&amp;#x200B;

**Paper:** [**https://arxiv.org/abs/1901.09590**](https://arxiv.org/abs/1901.09590)

**PyTorch Code:** [**https://github.com/ibalazevic/TuckER**](https://github.com/ibalazevic/TuckER)

&amp;#x200B;

**Key contributions:**

* proposing TuckER, a relatively simple **linear** model for **link prediction** in knowledge graphs that achieves **state-of-the-art results** across all standard datasets;
* proving that TuckER is **fully expressive** and deriving a bound on the entity and relation embedding dimensionality for full expressiveness which is several orders of magnitude lower than the bound of previous state-of-the-art models ComplEx and SimplE; and
* showing that TuckER **subsumes several previously proposed tensor factorization approaches** to link prediction, i.e. that RESCAL, DistMult, ComplEx and SimplE are all special cases of our model.

**Abstract:**

Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively simple but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms all previous state-of-the-art models across standard link prediction datasets. We prove that TuckER is a fully expressive model, deriving the bound on its entity and relation embedding dimensionality for full expressiveness which is several orders of magnitude smaller than the bound of previous state-of-the-art models ComplEx and SimplE. We further show that several previously introduced linear models can be viewed as special cases of TuckER.",10,1,False,https://a.thumbs.redditmedia.com/GPf40MEKYkmSvPKgxOF8a82ZBbdI9Bfhb4fe60xPWC8.jpg,,,,,
159,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,19,an0bjx,arxiv.org,[1902.00423] Do we train on test data? Purging CIFAR of near-duplicates,https://www.reddit.com/r/MachineLearning/comments/an0bjx/190200423_do_we_train_on_test_data_purging_cifar/,ihaphleas,1549275133,,18,1,False,default,,,,,
160,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,19,an0eh0,self.MachineLearning,Newbie at Tensorflow ML Related stuff,https://www.reddit.com/r/MachineLearning/comments/an0eh0/newbie_at_tensorflow_ml_related_stuff/,MagicElyas,1549275882,[removed],0,1,False,self,,,,,
161,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,19,an0gbh,self.MachineLearning,Artificial intelligence ideas that might turn into a product.,https://www.reddit.com/r/MachineLearning/comments/an0gbh/artificial_intelligence_ideas_that_might_turn/,YazanKhatib,1549276374,[removed],0,1,False,self,,,,,
162,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0m9o,self.MachineLearning,Optimization Problem Matrix with a non-linear constraint,https://www.reddit.com/r/MachineLearning/comments/an0m9o/optimization_problem_matrix_with_a_nonlinear/,pacopenco,1549278049,[removed],0,1,False,self,,,,,
163,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0nwy,self.MachineLearning,Recommended data-set of human faces,https://www.reddit.com/r/MachineLearning/comments/an0nwy/recommended_dataset_of_human_faces/,Shmulik123,1549278497,[removed],0,1,False,self,,,,,
164,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0p58,self.MachineLearning,Optimization problem matrix with non-linear objective,https://www.reddit.com/r/MachineLearning/comments/an0p58/optimization_problem_matrix_with_nonlinear/,pacopenco,1549278821,[removed],0,1,False,self,,,,,
165,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0pky,self.MachineLearning,What machine learning or other techniques could be used in online tourism?,https://www.reddit.com/r/MachineLearning/comments/an0pky/what_machine_learning_or_other_techniques_could/,D4nnyzke,1549278939,[removed],0,1,False,self,,,,,
166,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0poa,self.MachineLearning,Interview with Twice kaggle GrandMaster and Data Scientist at h2oai : (SRK) Sudalai Rajkumar,https://www.reddit.com/r/MachineLearning/comments/an0poa/interview_with_twice_kaggle_grandmaster_and_data/,init__27,1549278961,[https://hackernoon.com/interview-with-twice-kaggle-grandmaster-and-data-scientist-at-h20-ai-sudalai-rajkumar-cd952ef0c522](https://hackernoon.com/interview-with-twice-kaggle-grandmaster-and-data-scientist-at-h20-ai-sudalai-rajkumar-cd952ef0c522),0,1,False,self,,,,,
167,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0rwy,self.MachineLearning,[D] What machine learning or other techniques could be used in online tourism?,https://www.reddit.com/r/MachineLearning/comments/an0rwy/d_what_machine_learning_or_other_techniques_could/,D4nnyzke,1549279547,"Hi  all!

In this semester I Will do my complimentary internship for an  online travel agency, mainly for The flight sales divison. My job will  be that I should collect/order the data sources and after that I should  give tips/solutions for them to how to use data efficently. This can  include visualization, machine learning or literally anatyhing. My boss  said that I should search for international best practices too. Could  you recommend some ideas or sites where i can search for these ?

Thank you for your kindly responses in advance and have a great day y'all! :)",3,1,False,self,,,,,
168,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0si9,arxiv.org,[D] Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,https://www.reddit.com/r/MachineLearning/comments/an0si9/d_flow_improving_flowbased_generative_models_with/,PuzzledForm,1549279697,,1,1,False,default,,,,,
169,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0tmr,self.MachineLearning,[P] Deblur image with WGAN and tensorflow,https://www.reddit.com/r/MachineLearning/comments/an0tmr/p_deblur_image_with_wgan_and_tensorflow/,doyuplee,1549279970,"**DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks \[paper\](**[**https://arxiv.org/abs/1711.07064**](https://arxiv.org/abs/1711.07064)**)**

In this paper, WGAN is trained to make blurred image sharp.

Also, there is various implementations (pytorch, keras), but recently new tensorflow implementation is available.

Click \[link\]([https://github.com/LeeDoYup/DeblurGAN-tf](https://github.com/LeeDoYup/DeblurGAN-tf))",1,1,False,self,,,,,
170,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0vyd,smarten.com,Self-Serve Data Prep is Key to Data Democratization!,https://www.reddit.com/r/MachineLearning/comments/an0vyd/selfserve_data_prep_is_key_to_data_democratization/,ElegantMicroWebIndia,1549280604,,0,1,False,default,,,,,
171,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0wj2,self.MachineLearning,Novice wanting to generate text using RNN,https://www.reddit.com/r/MachineLearning/comments/an0wj2/novice_wanting_to_generate_text_using_rnn/,pmolloy,1549280750,"Pardon my ignorance.

 I want so badly to set up a RNN thing into which I can just dump a lot of text and watch it generate back nonsense. I have Windows 10. I installed Python and pip and followed instructions for installing tensorflow for this purpose but the command prompt doesn't seem to recognize anything.

Please help me. Thank you.",0,1,False,self,,,,,
172,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,20,an0ylt,self.MachineLearning,Supervised learning online-regression with 1 output,https://www.reddit.com/r/MachineLearning/comments/an0ylt/supervised_learning_onlineregression_with_1_output/,kombinot,1549281276,[removed],0,1,False,self,,,,,
173,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,22,an1g14,arxiv.org,[R] Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation,https://www.reddit.com/r/MachineLearning/comments/an1g14/r_understanding_impacts_of_highorder_loss/,singlasahil14,1549285255,,2,1,False,default,,,,,
174,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,22,an1lr8,self.MachineLearning,"[D] Is CNN pre-training useful, even if the target images are vastly different?",https://www.reddit.com/r/MachineLearning/comments/an1lr8/d_is_cnn_pretraining_useful_even_if_the_target/,Seiteshyru,1549286464,"The State of the Art for CNN seems to be pretraining (e.g. on ImageNet) and then fine-tuning for a specific task. From my understanding this is assumed to work because the CNN picks up generic features, that are sufficiently abstract to work on many objects.  

What I'd like to diskuss now is: Does this behaviour translate even to images, such as spectograms, that have a significantly different structure from the training images used for pretraining.  

I did not find (m)any papers that directly address this issue, though 'Rethinking ImageNet Pre-training' from He et al. might stirr up some future research in that direction.

&amp;#x200B;

I would assume that such pre-training would probably reduce the overall convergence time, but might produce lower quality results than a comparable network trained from scratch - but then again: I'm by no means an expert.",17,1,False,self,,,,,
175,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,22,an1np7,self.MachineLearning,#Amazon Simple Email Service (Amazon SES),https://www.reddit.com/r/MachineLearning/comments/an1np7/amazon_simple_email_service_amazon_ses/,AWSTrainingInHouston,1549286881,[removed],0,1,False,self,,,,,
176,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,22,an1qzy,self.MachineLearning,[D] Room presence detection FPR/FNR,https://www.reddit.com/r/MachineLearning/comments/an1qzy/d_room_presence_detection_fprfnr/,Cock-tail,1549287567,"Suppose there is an office space, with some ""focus rooms"" here and there, all with glass walls, so you can see through them. Also, there is a system which allows you to see the entire office map, with a red/green overlay on the focus rooms, which inform of their respective occupancy. Which one is worse? 

1. Falsely reporting that the room is occupied
2. Falsely reporting that the room is unoccupied?

I can't make up my mind. ",3,1,False,self,,,,,
177,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,22,an1rjq,ziffity.com,The CIOs Quick Guide To Taking The Machine Learning Dive,https://www.reddit.com/r/MachineLearning/comments/an1rjq/the_cios_quick_guide_to_taking_the_machine/,Ziffty,1549287683,,0,1,False,default,,,,,
178,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,23,an205f,self.MachineLearning,"Why do Batch Norm, why not optiimze explicity for white (~N(0,1)) output?",https://www.reddit.com/r/MachineLearning/comments/an205f/why_do_batch_norm_why_not_optiimze_explicity_for/,idg101,1549289413,[removed],0,1,False,self,,,,,
179,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,23,an21cm,self.MachineLearning,Deep learning audio,https://www.reddit.com/r/MachineLearning/comments/an21cm/deep_learning_audio/,Dudelove994,1549289641,"Hi all, 

So I've been fascinated with machine learning for some time now and especially in conjunction with sound/audio. My current aim is to implement googles tensorflow/magenta model to input raw audio to output new sounds although I'm fairly new to the deep learning process. Fundamentally, what would I need to explore to get this project on the go, is the magenta framework enough to get the job done or do I need to include further algorithms?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
180,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,23,an258q,self.MachineLearning,Understanding ML for predicting crime,https://www.reddit.com/r/MachineLearning/comments/an258q/understanding_ml_for_predicting_crime/,understandML,1549290404,"Hi, I am looking for someone to discuss Machine Learning with me.

I am trying to make a list of the exact differences and commonalities between traditional statistical predictions that have been around for decades vs. machine learning predictions.

&amp;#x200B;

Specifically in the context of **parole decisions,** I am trying to understand what are the  differences between the traditional statistical risk assessments to **predict future criminal behaviour** and predictions of future criminal behaviour based on ML.

&amp;#x200B;

What criticism of traditional statistical risk assessments can just be transferred to ML predictions, and what are really new criticism of the ML procedure?

&amp;#x200B;

At this point I only have:

**commonalities:** 

\- both statistical risk assessment and ML are a-theoretical and only give correlation no explanation;

\- the only thing we can know about a statistical model or the (more interpretable kinds of) ML algorithms is the weight a certain variable (e.g. age, number of prior arrest) played for the outcome prediction

\- pseudo objectivity: bias of the training data can be perpetuated through predictions

\- statistical and ML predictions don't give a truly individual assessment of risk; only give a group risks assesment (how many people out of a group of people with the same features as the defendant have recidivism in the past/in the training data?)

&amp;#x200B;

**differences:**

\- with statistical risk assessment the human has to determine in advance which variables (e.g. age, number of prior arrests) they want to correlate with criminal behaviour, while ML can discover new crime related variables, the human hasn't considered before

\- greater volume of data can be analysed with ML as training data

\- as a consequence new kinds of data sources could be used as training data in prediction, e.g. open source social media data; leading to hunger for more data and an approach of ""all data are crime-related data""

\- more crime related variables can be correlated in a model and make it (hopefully) more precise that way (the statistical risk assent tool VRAG-R for example only considers 12 variables, ML could considers many more)

\- inherent non-transparency of ML algorithms: the correlation of neural networks ML are not transparent to humans; while decision tree ML is more transparent to humans, the more variables are considered and the more complex a decision tree grows, the harder it is for humans so comprehend the correlations.

&amp;#x200B;

Can you correct and complete the list? (I don't have computer science background, so please be gentle with the expert terms).

&amp;#x200B;

Thanks so much! 

&amp;#x200B;",0,1,False,self,,,,,
181,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,23,an26va,openreview.net,[R] A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks,https://www.reddit.com/r/MachineLearning/comments/an26va/r_a_convergence_analysis_of_gradient_descent_for/,dimber-damber,1549290723,,2,1,False,default,,,,,
182,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,23,an27il,self.MachineLearning,[D] Should I get a conversion degree in CS to do PhD in machine learning?,https://www.reddit.com/r/MachineLearning/comments/an27il/d_should_i_get_a_conversion_degree_in_cs_to_do/,babedos,1549290845,"I'm aspired to become an ML researcher and most of the time a PhD degree is required to do so. I have a BSc in mathematics but have little to no CS education, although I learned python on my own and did my undergrad thesis related to neural networks. The thing is, most CS PhD programs require you to have a CS background (or at least courses in CS).

I looked for potential MSc programs and found that there are conversion MSc programs in CS that require no CS background (most of them are in UK). Should I go for those programs and then go for a PhD in CS? Or should I go for ML / data science programs that don't require CS background?",14,1,False,self,,,,,
183,MachineLearning,t5_2r3gv,2019-2-4,2019,2,4,23,an28sp,self.MachineLearning,Deep Learning project ideas on GPU,https://www.reddit.com/r/MachineLearning/comments/an28sp/deep_learning_project_ideas_on_gpu/,CArchGuy,1549291089,[removed],0,1,False,self,,,,,
184,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,0,an2hwp,actuia.com,"[N] Latest News from the French AI ecosystem (February 4, 2019)",https://www.reddit.com/r/MachineLearning/comments/an2hwp/n_latest_news_from_the_french_ai_ecosystem/,actuia,1549292768,,0,1,False,https://b.thumbs.redditmedia.com/TbeZZSw0WReWsWJer79wRMqw-ZlNnNHHjgX0ALRTlkw.jpg,,,,,
185,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,0,an2jta,thecuriousdev.org,Understanding Convolution,https://www.reddit.com/r/MachineLearning/comments/an2jta/understanding_convolution/,soniewins,1549293094,,0,1,False,default,,,,,
186,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,0,an2lfb,self.MachineLearning,Recommendations for distance ML / AI grad programs?,https://www.reddit.com/r/MachineLearning/comments/an2lfb/recommendations_for_distance_ml_ai_grad_programs/,irulenot,1549293375,[removed],0,1,False,self,,,,,
187,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,0,an2o9g,github.com,Learn how Neural Networks Work - implement them from Scratch.,https://www.reddit.com/r/MachineLearning/comments/an2o9g/learn_how_neural_networks_work_implement_them/,ktessera,1549293868,,0,1,False,default,,,,,
188,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,0,an2w8k,york.qualtrics.com,User perception of AI-generated 3D rendered regions,https://www.reddit.com/r/MachineLearning/comments/an2w8k/user_perception_of_aigenerated_3d_rendered_regions/,rjsYork,1549295258,,0,1,False,default,,,,,
189,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,0,an2yce,self.MachineLearning,How can i implement the HindSight Experience Replay to my DDPG network,https://www.reddit.com/r/MachineLearning/comments/an2yce/how_can_i_implement_the_hindsight_experience/,Jandevries101,1549295614," 

Hi,

&amp;#x200B;

So i got this ddpg network and i just stumbled uppon the Hindsight experience replay. i currently use normal experience replay and that looks something like this:

&amp;#x200B;

       def store_transition(self, s, a, r, s_):         transition = np.hstack((s, a, [r], s_))         index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory         self.memory[index, :] = transition         with open('Training_Memory', 'wb') as f:             pickle.dump(self.memory, f)          self.pointer += 1 

&amp;#x200B;

How can i implement the HindSight Experience Replay to my network, i tried, however since i have a hard time implementing this with the sparse tutorials on the internet, i am left stuck.

&amp;#x200B;

If anyone can help out, thanks

&amp;#x200B;

Jan",0,1,False,self,,,,,
190,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,1,an324i,arxiv.org,Effect of Various Regularizers on Model Complexities of Neural Networks in Presence of Input Noise,https://www.reddit.com/r/MachineLearning/comments/an324i/effect_of_various_regularizers_on_model/,mayankiitkgp10,1549296219,,2,1,False,default,,,,,
191,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,1,an3cqz,self.MachineLearning,[P] Mushroom: Reinforcement Learning library,https://www.reddit.com/r/MachineLearning/comments/an3cqz/p_mushroom_reinforcement_learning_library/,carloderamo,1549297932,"Hi everyone,

few months ago I posted here to let you know about a RL Python library, called Mushroom, I'm working on for my PhD. It has the purpose to simplify the developing of RL experiments and it is highly customizable to let user add and test their own algorithms. It uses Gym and Pytorch (one can also use Tensorflow or other libraries transparently) to exploit already developed powerful libraries and just to focus on RL aspects. Recently, we added several policy gradient and actor-critic algorithms, such as the recent DPG. I'd really appreciate if you give a look to Mushroom and consider it for doing your research, or simply to give me a feedback on it. Here are the GitHub link: [https://github.com/AIRLab-POLIMI/mushroom](https://github.com/AIRLab-POLIMI/mushroom) and the doc link:[https://mushroomrl.readthedocs.io/en/latest/](https://mushroomrl.readthedocs.io/en/latest/).

Cheers!",22,1,False,self,,,,,
192,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,1,an3g0f,youtu.be,"Programmer working on a homebrew AI that's 8-but, Text user interface, with only two color display. Simplicity at it's finest.",https://www.reddit.com/r/MachineLearning/comments/an3g0f/programmer_working_on_a_homebrew_ai_thats_8but/,NAIDBTPROG,1549298443,,0,1,False,default,,,,,
193,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,1,an3h6c,self.MachineLearning,[P] Making deep neural networks paint to understand how they work,https://www.reddit.com/r/MachineLearning/comments/an3h6c/p_making_deep_neural_networks_paint_to_understand/,invertedpassion,1549298630,"Hello,

I wrote a tutorial on generating abstract art using Compositional Pattern Producing Networks. In the process of generating images, I also explore various settings (different number of hidden layers, different number of neurons in hidden layers, impact of activation functions, etc.) to gain insights into what happens inside deep neural networks.

The code is in PyTorch (and pretty short - less than 100 lines of code).  It generates images such as:

*Processing img al34snd5yke21...*

Check out the write up here: [Making deep neural networks paint to understand how theywork](https://towardsdatascience.com/making-deep-neural-networks-paint-to-understand-how-they-work-4be0901582ee). You can access the Jupyter notebook with [code from my repository](https://github.com/paraschopra/abstract-art-neural-network).

&amp;#x200B;

Hope you like the project. Feedback appreciated :)",0,1,False,self,,,,,
194,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,1,an3kwz,self.MachineLearning,[P] Making deep neural networks paint to understand how they work,https://www.reddit.com/r/MachineLearning/comments/an3kwz/p_making_deep_neural_networks_paint_to_understand/,invertedpassion,1549299228,"Hello,

I wrote a tutorial on **generating abstract art using compositional pattern producing networks**. In the process of generating images, I explore various things like the impact of additional hidden layers, behavior of activation functions, and impact of more neurons per layer to gain insights into how deep neural networks work. 

The code is short (&lt;100 lines) and is written in PyTorch. Following is an example of an image generated:

*Processing img a3yuwdt50le21...*

Check out the tutorial here: [Making deep neural networks paint to understand how theywork](https://towardsdatascience.com/making-deep-neural-networks-paint-to-understand-how-they-work-4be0901582ee) and access the [code here](https://github.com/paraschopra/abstract-art-neural-network).

Hope you like the project. Feedback welcome :)",18,1,False,https://b.thumbs.redditmedia.com/Beu2H5836gRyLVJYjBJvw58oeZBM9O4yV5jMckh9apg.jpg,,,,,
195,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,2,an3nrw,self.MachineLearning,Filter for text documents to check relevance,https://www.reddit.com/r/MachineLearning/comments/an3nrw/filter_for_text_documents_to_check_relevance/,Tarific2003,1549299669,[removed],0,1,False,self,,,,,
196,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,2,an3q5p,arxiv.org,[R] Learning to Make Analogies by Contrasting Abstract Relational Structure,https://www.reddit.com/r/MachineLearning/comments/an3q5p/r_learning_to_make_analogies_by_contrasting/,downtownslim,1549300022,,1,1,False,default,,,,,
197,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,2,an3qpx,arxiv.org,[R] Evolving intrinsic motivations for altruistic behavior,https://www.reddit.com/r/MachineLearning/comments/an3qpx/r_evolving_intrinsic_motivations_for_altruistic/,downtownslim,1549300110,,2,1,False,default,,,,,
198,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,2,an3smq,arxiv.org,[R] A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms,https://www.reddit.com/r/MachineLearning/comments/an3smq/r_a_metatransfer_objective_for_learning_to/,bay_der,1549300414,,4,1,False,default,,,,,
199,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,2,an3wx3,youtube.com,[D] A Bluffer's Guide to Dimension Reduction - Leland McInnes,https://www.reddit.com/r/MachineLearning/comments/an3wx3/d_a_bluffers_guide_to_dimension_reduction_leland/,_quanttrader_,1549301101,,0,1,False,https://b.thumbs.redditmedia.com/Ls_qcpdm95O78iJo3yYhJ4Iu82uPv-D8cUflEi14Pwo.jpg,,,,,
200,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,2,an47ti,self.MachineLearning,China Racing To Lead Machine Learning AI Industry| $15.7 Trillion USD Value By 2030,https://www.reddit.com/r/MachineLearning/comments/an47ti/china_racing_to_lead_machine_learning_ai_industry/,getrich_or_diemining,1549302759,[removed],0,1,False,self,,,,,
201,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4av7,self.MachineLearning,"I am a novice, How can I get my data from hardware to a DB?",https://www.reddit.com/r/MachineLearning/comments/an4av7/i_am_a_novice_how_can_i_get_my_data_from_hardware/,MukaiGuy,1549303231,"Help Please! 

&amp;#x200B;

I am trying to build datasets to analyze from hardware sensors. I have a background in Archaeology and I am trying to develop better predictive models based on some principles used in the academic world and apply them to consumer products. So for example, if I have an environmental sensor built using a Ras-PI-3  that is monitoring air quality, light levels, sound levels, etc. How do I get it to read into a database (such as Postgres, et al) that I can use for my application?

&amp;#x200B;

Also, I would like to pull my health data from my apple watch I know I would have to build an application to talk and pull the data in swift. 

&amp;#x200B;

Any info and or tips are welcome.

&amp;#x200B;

Thank you!

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
202,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4ayh,medium.com,MIT Deep Learning Basics: Introduction and Overview with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/an4ayh/mit_deep_learning_basics_introduction_and/,dayanruben,1549303242,,0,1,False,https://a.thumbs.redditmedia.com/8UUb5iTIBj47_4Sn0WluJMPOuc5IIfmPdQ0HGTnn5R0.jpg,,,,,
203,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4eeo,medium.com,[D] Deep Learning Basics with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/an4eeo/d_deep_learning_basics_with_tensorflow/,UltraMarathonMan,1549303746,,0,1,False,default,,,,,
204,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4gl1,self.MachineLearning,What is the best coding language for machine learning.,https://www.reddit.com/r/MachineLearning/comments/an4gl1/what_is_the_best_coding_language_for_machine/,Duel-shock514,1549304088,[removed],0,1,False,self,,,,,
205,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4ita,blogs.microsoft.com,"This Week in Machine Learning Podcast: How AI can truly help society, from the environment, medicine, accessibility, to humanitarian assistance",https://www.reddit.com/r/MachineLearning/comments/an4ita/this_week_in_machine_learning_podcast_how_ai_can/,myinnerbanjo,1549304424,,0,1,False,default,,,,,
206,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4ked,self.MachineLearning,Need dataset of webpage screenshots,https://www.reddit.com/r/MachineLearning/comments/an4ked/need_dataset_of_webpage_screenshots/,pranitkothari,1549304668,[removed],0,1,False,self,,,,,
207,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4ktm,self.MachineLearning,I'm a broke convicted felon about to be released from federal prison who is looking for advice on machine learning bootcamp.,https://www.reddit.com/r/MachineLearning/comments/an4ktm/im_a_broke_convicted_felon_about_to_be_released/,NiversaCo,1549304734,"I've taught myself linear algebra.  I've read that multivariate calculus and statistics&amp; probabilities is also a plus.  But I'm really looking for a scholarship program or something of that nature that could assist me on this journey.

I'm hoping that one of you may know of any programs offering people in my situation a way to get where I want to be.  After I'm released from my prison, I will be on home confinement, so I'll literally have 12+ hours a day to dedicate to whatever I get into.  

Thanks",0,1,False,self,,,,,
208,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4s79,self.MachineLearning,Learning the model in a Kalman Filter using machine learning.,https://www.reddit.com/r/MachineLearning/comments/an4s79/learning_the_model_in_a_kalman_filter_using/,ReinforcementBoi,1549305845,"Hi all,   


I was wondering if it was possible to learn the state transition probabilities (dynamics model), which is used in the update step in a kalman filter using some machine learning technique ? Do you know of any papers / references that have achieved this before?   


Thanks !",0,1,False,self,,,,,
209,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,3,an4w6m,github.com,tensorflow/tfx is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/an4w6m/tensorflowtfx_is_a_new_github_repo_by_tensorflow/,sjoerdapp,1549306449,,0,1,False,default,,,,,
210,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,4,an5eqx,bookengine.colorsleep.com,[P] Search within all the Project Gutenberg books with BERT,https://www.reddit.com/r/MachineLearning/comments/an5eqx/p_search_within_all_the_project_gutenberg_books/,hollowayaegis,1549309397,,1,1,False,default,,,,,
211,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,4,an5kqo,self.MachineLearning,Dialog | Chatbot UI,https://www.reddit.com/r/MachineLearning/comments/an5kqo/dialog_chatbot_ui/,mac_cumhaill,1549310297,"I have a seq2seq question answer model trained. The whole system is set up in Python to accept an input, feature extract an vectorize, pass to the model and return an output. 

&amp;#x200B;

I'm looking to wrap the interface in something pretty, I know some work has been done in django with React.js. I'm wondering does anyone have any experience of these systems? 

&amp;#x200B;

I'm \_not\_ looking for something like [Wit.ai](https://Wit.ai) or dialogflow since I already have a trained model. ",0,1,False,self,,,,,
212,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,5,an600p,self.MachineLearning,[P] Anyone could provide me a good -&gt; Heart Disease Identifer Paper,https://www.reddit.com/r/MachineLearning/comments/an600p/p_anyone_could_provide_me_a_good_heart_disease/,MarkPetter,1549312534,"So We are having some projects in my school, so anyone has a paper on Heart Disease Indentifier. ",2,1,False,self,,,,,
213,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,5,an64p2,medium.com,Shannon.AIs Glyce Masters 13 Chinese NLP Tasks,https://www.reddit.com/r/MachineLearning/comments/an64p2/shannonais_glyce_masters_13_chinese_nlp_tasks/,Yuqing7,1549313240,,0,1,False,default,,,,,
214,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,6,an6b35,self.MachineLearning,"PyTorch Tutorials for creating RNN, GRU and LSTM from scratch?",https://www.reddit.com/r/MachineLearning/comments/an6b35/pytorch_tutorials_for_creating_rnn_gru_and_lstm/,buildmeapcnyc,1549314218,"Hi,

as the title suggests, I was wondering if are there any tutorials to create RNNs, GRUs or LSTMs ""from scratch"" in PyTorch, i.e. using only the basic building blocks available in PyTorch? That would help my understanding a lot.

I managed to create an RNN by using two linear layers that are separated from one another and the results seem to tie out with toy RNN examples, but ideally I'd like to have a proper resource that guides me through that. Especially when it comes to LSTM I think I'll face challenges",0,1,False,self,,,,,
215,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,6,an6bnu,self.MachineLearning,[Discussion] Unsupervised Learning with Random Forest Classifier,https://www.reddit.com/r/MachineLearning/comments/an6bnu/discussion_unsupervised_learning_with_random/,Penguin474,1549314304,"So, I've read that it's possible to use RFC (a tool normally used for supervised ML) to do unsupervised learning. In this [paper](https://www.researchgate.net/profile/Andy_Liaw/publication/228451484_Classification_and_Regression_by_RandomForest/links/53fb24cc0cf20a45497047ab/Classification-and-Regression-by-RandomForest.pdf), the authors hint at creating a ""dummy"" dataset by sampling from the distribution of the original dataset, and then using RFC to differentiate between the two. 

I'm struggling to see how this can be useful. What I really want to do is find classes of data within the original dataset (similar to how other unsupervised methods work -- kmeans, DBSCAN, etc.)

Is this worth looking into or am I wasting time?",5,1,False,self,,,,,
216,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,6,an6hjp,self.MachineLearning,What is the ideal amount of data for a NN,https://www.reddit.com/r/MachineLearning/comments/an6hjp/what_is_the_ideal_amount_of_data_for_a_nn/,ItayElf,1549315182,[removed],0,1,False,self,,,,,
217,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,6,an6son,voicetechpodcast.com,"[D] Machine Learning Signals - Christopher Oates, audEERING - Voice Tech Podcast ep.020",https://www.reddit.com/r/MachineLearning/comments/an6son/d_machine_learning_signals_christopher_oates/,wootnoob,1549316847,,0,1,False,default,,,,,
218,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,7,an6yir,self.rust,"""[P]"" Crosspost of the HorseShoe calculus crate announcement for Rust",https://www.reddit.com/r/MachineLearning/comments/an6yir/p_crosspost_of_the_horseshoe_calculus_crate/,ClothBoithepuppet,1549317746,,0,1,False,https://b.thumbs.redditmedia.com/J7MUtpE_QxOw7JHsWoWfhw6s1sL76EssVp1rY50YWKI.jpg,,,,,
219,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,7,an76nq,self.MachineLearning,The threat is not AI. It is the way we are doing it.,https://www.reddit.com/r/MachineLearning/comments/an76nq/the_threat_is_not_ai_it_is_the_way_we_are_doing_it/,satori02,1549319052,"So, after taking a quarter off to work on a couple of personal projects, I recently decided to come back to the job market. Excitedly, I received the agenda for an on premises series of interviews with a certain potential employer. Three things stood out to me: first, along with one hour of data science interview by a software engineer, my schedule included a one hour of a software engineering interview by two other software engineers. Second, there was also one hour allocated to past projects to be discussed with the CTO.  The third was that the first 2 interviews I mentioned were to be conducted both by software engineers. Plus a CTO for the third interview.

First, yes, data science is the intersection of many disciplines and software engineering is by all means one very important such discipline. But, so is statistics, scientific methods, etc. And, what was the objective of a different software engineering interview that the data science interview would not cover? I was applying for a data science position anyway. I have been coding for over 20 years in any kind of languages that to most under 30 are not even familiar sounding, but a software engineer I am not. I am a scientist. Not an engineer. I wanted to be a scientist when I would grow up. And thats what I did (i.e. became a scientist; still working on the growing up piece).

Second, I was reminded one result I had obtained while I was recruiting data scientists for one of my teams in the past. I used one of the online testing platforms to set up a set of evaluation exercises. The usual suspects were there: a bit of SQL, more ML, a couple of easy arithmetic (i dont think they even qualified as math) questions. One of these questions was: if I used 234 digits to label the employees of my company using consecutive natural numbers - as in 1, 2, 3,  - how any employees do I have? Oh my. I got the most complex document classification code in these evaluations; but I also got python code  trying to solve my arithmetic question. In the best of cases it looked like the candidate did try to do some estimate using pen and paper, but it was usually wrong and/or referred to as estimate. The questions meanwhile had a nice exact solution derivable in less than a minute with pen and paper. A tiny fraction of candidates got this one right - and all with a standard education background which i wont share because I dont want to come across as blowing my own horn.

So, do you think I could have a conversation with my data science software engineer interviewer about the pros and cons of the parameter prior that is used in LASSO? Experience makes me say that 99% of the times I couldnt. Experiences like this make me want to ask to ...interview my interviewers It is an inter-view anyway.  They want to know if I know what they think I should. Shouldnt I as well make sure these people know the things they should? In any event,  from experience I can  tell you that its more likely that my interview would be about whether I should be ending SQL statements with a ; or not. And what is the name of the python package that does xyz, rather than what assumptions underlie a LASSO regression...

And then we are worried about the future of AI. We talk about setting guidelines for responsible use of AI, etc.  In our more pessimistic we are thinking of  a potential tech singularity. Of AI dominating the world and turning us into slaves even! Yes, I agree, we should totally be worried - or, anyway, be at least proactive and risk averse with a better safe than sorry attitude.

But nothing comes from nothing (sure, sure, we can entertain quantum vacuum fluctuations but lets just say that there are 99 problems with data science but this aint one - for the time being or the near future at least; trust me, I have a PhD in Cosmology). So, its not gonna be one fateful night that AI will decide to take over Its gonna be many years of us, humans, treating all of it as a practitioner's game only: if you know how to stitch code together thats it. That makes you a machine learning expert, a data scientist, whatever you wanna call it Well, the ability to stitch code and calling it science is one of the biggest AI related threats. That and that we are data rich and theory poor - but Ill touch upon this and the third interview I mentioned at the beginning of this write up in pieces to follow. 

If you think about it, both these problems are manifestations of the same thing: a practitioners **only** attitude. Knowing from practice/experience how to use a hammer does not mean you are using it correctly.  ",0,1,False,self,,,,,
220,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,7,an7b29,bookengine.colorsleep.com,[Project] Search within all the Project Gutenberg books with BERT,https://www.reddit.com/r/MachineLearning/comments/an7b29/project_search_within_all_the_project_gutenberg/,hollowayaegis,1549319801,,1,1,False,default,,,,,
221,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,8,an7jz7,self.MachineLearning,[R] Hotels-50K: A Global Hotel Recognition Dataset,https://www.reddit.com/r/MachineLearning/comments/an7jz7/r_hotels50k_a_global_hotel_recognition_dataset/,abby621,1549321338,"&amp;#x200B;

*Processing img js4itys7tme21...*

For the last several years, my lab has worked on approaches to hotel recognition, with the goal of building global scale image search systems to help human trafficking investigators locate what hotels victims of human trafficking are being photographed in. Our efforts included the creation of a smartphone application, called TraffickCam, which has been used by over 150,000 travelers to collect imagery that is more similar to investigative images than images that can be found on travel websites, and a global scale image search approach trained on this data to human trafficking investigators at the National Center for Missing and Exploited Children.

To support further advancement in this important and challenging problem domain, we released the Hotels-50K dataset at AAAI this past week.

**Abstract**: ""Recognizing a hotel from an image of a hotel room is important for human trafficking investigations.  Images directly link victims to places and can help verify where victims have been trafficked, and where their traffickers might move them or others in the future.  Recognizing the hotel from images is challenging because of low image quality, uncommon camera perspectives, large occlusions (often the victim), and the similarity of objects (e.g., furniture, art, bedding) across different hotel rooms.

To support efforts towards this hotel recognition task, we have curated a dataset of over 1 million annotated hotel room images from 50,000 hotels. These images include professionally captured photographs from travel websites and crowd-sourced images from a mobile application, which are more similar to the types of images analyzed in real-world investigations.  We present a baseline approach based on a standard network architecture and a collection of data-augmentation approaches tuned to this problem domain.""

**Paper**: [https://www.aaai.org/Papers/AAAI/2019/AAAI-StylianouA.3453.pdf](https://www.aaai.org/Papers/AAAI/2019/AAAI-StylianouA.3453.pdf) 

**Code and dataset available at**: [https://github.com/GWUvision/Hotels-50K](https://github.com/GWUvision/Hotels-50K)",11,1,False,self,,,,,
222,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,8,an7mco,kite.com,Practical Machine Learning with Python and Keras,https://www.reddit.com/r/MachineLearning/comments/an7mco/practical_machine_learning_with_python_and_keras/,brendanmcd96,1549321762,,0,1,False,https://b.thumbs.redditmedia.com/XKKsxAeWjmGbevBXl1nDBBHf4Fu9mTI7ERGsvDa0A7Q.jpg,,,,,
223,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,8,an7qzw,self.MachineLearning,[D] Are there any good references discussing the effects of tokenization on downstream tasks for English language?,https://www.reddit.com/r/MachineLearning/comments/an7qzw/d_are_there_any_good_references_discussing_the/,swaroopgrs,1549322580,,0,1,False,self,,,,,
224,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,8,an7rls,self.MachineLearning,[D] Job loss due to AI  How bad is it going to be?,https://www.reddit.com/r/MachineLearning/comments/an7rls/d_job_loss_due_to_ai_how_bad_is_it_going_to_be/,milaworld,1549322692,"Article from [Skynet Today](https://www.skynettoday.com/editorials/ai-automation-job-loss):

*Recent studies suggest the impact of AI on jobs in the near future will not be significantly more disruptive than the impact of automation in the past*

https://www.skynettoday.com/editorials/ai-automation-job-loss",4,1,False,self,,,,,
225,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,8,an800h,self.MachineLearning,Best resources for ML engineering interviews?,https://www.reddit.com/r/MachineLearning/comments/an800h/best_resources_for_ml_engineering_interviews/,scpdstudent,1549324139,[removed],0,1,False,self,,,,,
226,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,8,an8151,self.MachineLearning,Machine Learning In Node.js With TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/an8151/machine_learning_in_nodejs_with_tensorflowjs/,Feliciamyn,1549324340,[removed],0,1,False,self,,,,,
227,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,8,an817v,self.MachineLearning,Question about machine learning problem.,https://www.reddit.com/r/MachineLearning/comments/an817v/question_about_machine_learning_problem/,Rufashaw,1549324354,[removed],0,1,False,self,,,,,
228,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,9,an8axn,reddit.com,Ignorance of Statistics in Data Science will eventually Inspire Dread for your ML projects ...,https://www.reddit.com/r/MachineLearning/comments/an8axn/ignorance_of_statistics_in_data_science_will/,skj8,1549326113,,0,1,False,default,,,,,
229,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,9,an8bjp,github.com,[R] Flickr-Faces-HQ Dataset (FFHQ),https://www.reddit.com/r/MachineLearning/comments/an8bjp/r_flickrfaceshq_dataset_ffhq/,galvatron,1549326234,,0,1,False,https://b.thumbs.redditmedia.com/Yp8YVqwFdLim2C4BShnCmMPY3z-JZHhsfh6hA-CQjyU.jpg,,,,,
230,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,9,an8g06,self.MachineLearning,Flickr-Faces-HQ dataset released,https://www.reddit.com/r/MachineLearning/comments/an8g06/flickrfaceshq_dataset_released/,d0_0d,1549327006,"Flickr-Faces-HQ (FFHQ) is out now. ""... 70,000 high-quality PNG images at 10241024 resolution and contains considerable variation in terms of age, ethnicity and image background."" The dataset used for the Style-GAN paper. [https://github.com/NVlabs/ffhq-dataset ](https://t.co/7ieYcC0N4F)",0,1,False,self,,,,,
231,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,9,an8gr3,sinxloud.com,This peice on Cats and the Ignorance of Statistics in Data Science and ML inspires dread,https://www.reddit.com/r/MachineLearning/comments/an8gr3/this_peice_on_cats_and_the_ignorance_of/,skj8,1549327146,,0,1,False,default,,,,,
232,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,10,an90ej,self.MachineLearning,Looking for freelance Machine learning experts who also enjoy teaching others,https://www.reddit.com/r/MachineLearning/comments/an90ej/looking_for_freelance_machine_learning_experts/,kyledevyay,1549330710,[removed],0,1,False,self,,,,,
233,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,11,an9h23,arxiv.org,[R] Robustness Certificates Against Adversarial Examples for ReLU Networks,https://www.reddit.com/r/MachineLearning/comments/an9h23/r_robustness_certificates_against_adversarial/,singlasahil14,1549333787,,1,1,False,default,,,,,
234,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,11,an9oeo,amazon.in,Hands on machine learning with scikitlearn,https://www.reddit.com/r/MachineLearning/comments/an9oeo/hands_on_machine_learning_with_scikitlearn/,santoshv99,1549335184,,0,1,False,default,,,,,
235,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,13,anadq8,self.MachineLearning,Website for Summary of ML papers ?,https://www.reddit.com/r/MachineLearning/comments/anadq8/website_for_summary_of_ml_papers/,ashutosj,1549340045,[removed],0,1,False,self,,,,,
236,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,13,analq0,self.MachineLearning,Machine Learning Bangalore | Machine Learning Course Bangalore,https://www.reddit.com/r/MachineLearning/comments/analq0/machine_learning_bangalore_machine_learning/,vikramreddy14321,1549341662,[removed],0,1,False,self,,,,,
237,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,14,anb2y2,youtube.com,TDLS classic papers presentation: Large-scale Parallel Collaborative Filtering for the Netflix Prize (w/ ALS algorithm),https://www.reddit.com/r/MachineLearning/comments/anb2y2/tdls_classic_papers_presentation_largescale/,tdls_to,1549345242,,0,1,False,https://b.thumbs.redditmedia.com/qzPl12r3UVQoEu0X22KzIsvmlcZSN-HFhSozs0YBHMw.jpg,,,,,
238,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,15,anbbcc,self.MachineLearning,[P] A2C not working in OpenAi Pendulum,https://www.reddit.com/r/MachineLearning/comments/anbbcc/p_a2c_not_working_in_openai_pendulum/,premepopulation,1549347160,"I've been spending weeks trying to get an actor-critic reinforcement learning model to work with the [*OpenAi Pendulum*](https://github.com/openai/gym/wiki/Pendulum-v0) environment, but I haven't been able to solve it, **yet**. The critic (value) model is predicting the value well and its loss is low. The actor, however, is predicting actions all over the place with it's mean (mu) and variance (sigma) totally not aligned with what they should be. If I limit the mean using a tanh activation then the sigma will keep going up towards infinity. I've tried different activation functions, initializers, and hyper-parameters, but nothing seems to work. I've read the [DeepMind A2C paper](https://deepmind.com/research/publications/sample-efficient-actor-critic-experience-replay/) along with countless other sources and they seem to be doing the same thing I'm doing, yet, I can't get the A2C to work on the pendulum environment. Anyways, I'm still learning and if anyone could check out my code and give some pointers/advice, it would be greatly appreciated. Thanks. 

&amp;#x200B;

    import gym
    import numpy as np
    import tensorflow as tf
    import math
    import keras
    import random
    
    class pendulum:
        def __init__(self, sess, env, state_size, action_size):
            self.env = env
            self.num_states = state_size
            self.num_actions = action_size
            self.action_space_min = -2.0
            self.action_space_max = 2.0
            self.sess = sess
            self.epsilon = 1.0
            self.epsilon_decay = .99
            self.return_actor_loss = 0.0
            self.return_critic_loss = 0.0
            self.memory = []
            self.gamma = .95
            self.mu = 0
            self.sigma = 0
    
            self.actor_and_value_model()
            init = tf.global_variables_initializer()
            self.sess.run(init)
        
        def actor_and_value_model(self):
            #State input
            self.model_state_input = tf.placeholder(shape=[None, self.num_states], dtype=tf.float32)
            #Advantage input
            self.actual_advantage_label = tf.placeholder(shape=[None, 1], dtype=tf.float32)
        
    
            #Hidden layer 1 (using relu so we don't have negative outputs.)
            self.model_hiddenlayer1_weights = tf.Variable(tf.random_normal([self.num_states, 64]))
            self.model_hiddenlayer1_bias = tf.Variable(tf.random_normal([64]))
            self.model_hiddenlayer1_output = tf.matmul(self.model_state_input, self.model_hiddenlayer1_weights) + self.model_hiddenlayer1_bias
            self.model_hiddenlayer1_output = tf.nn.relu(self.model_hiddenlayer1_output)
    
            #Hidden layer 2
            self.model_hiddenlayer2_weights = tf.Variable(tf.random_normal([64, 32]))
            self.model_hiddenlayer2_bias = tf.Variable(tf.random_normal([32]))
            self.model_hiddenlayer2_output = tf.matmul(self.model_hiddenlayer1_output, self.model_hiddenlayer2_weights) + self.model_hiddenlayer2_bias
            self.model_hiddenlayer2_output = tf.nn.relu(self.model_hiddenlayer2_output)
    
    
    
            #Predict sigma (softplus so we  don't have negative sigma)
            self.sigma_weights = tf.Variable(tf.random_normal([32, 1]))
            self.sigma_bias = tf.Variable(tf.random_normal([1]))
            self.sigma_output = tf.matmul(self.model_hiddenlayer2_output, self.sigma_weights) + self.sigma_bias
            self.sigma_output = tf.nn.softplus(self.sigma_output) + .1
    
            #Predict MU (tanh to restrict the domain between -2 and 2)
            self.mu_weights = tf.Variable(tf.random_normal([32, 1]))
            self.mu_bias = tf.Variable(tf.random_normal([1]))
            self.mu_output = tf.matmul(self.model_hiddenlayer2_output, self.mu_weights) + self.mu_bias
            self.mu_output = keras.activations.linear(self.mu_output)
            # self.mu_output = tf.nn.tanh(self.mu_output + 1e-5) * 2 #Doesn't work, just causes sigma to grow rapidly. 
    
            #Predict value
            self.q_weights = tf.Variable(tf.random_normal([32, 1]))
            self.q_bias = tf.Variable(tf.random_normal([1]))
            self.q_output = tf.matmul(self.model_hiddenlayer2_output, self.q_weights) + self.q_bias
            self.q_output = keras.activations.linear(self.q_output)
    
            #Create a normal distribution of the actions and the likelyhood they will return a big advantage
            self.normal_distribution = tf.distributions.Normal(self.mu_output, self.sigma_output)
            
            #Sample a action from the distribution,
            self.action = self.normal_distribution.sample(1)
            self.action = tf.clip_by_value(self.action, self.env.action_space.low[0], self.env.action_space.high[0])
    
            #Gradient ascent loss to maxamize the reward. E.g (-log(.01) * 200 = 400), (-log(.99) * 200 = 0.8)
            self.actor_loss = -tf.log(self.normal_distribution.prob(self.action) + 1e-5) * self.actual_advantage_label 
            self.actor_train_model=tf.train.AdamOptimizer(learning_rate=0.0005).minimize(self.actor_loss)
            
            #MSE to minimize value loss
            self.critic_loss=tf.losses.mean_squared_error(self.q_output, self.actual_advantage_label)
            self.critic_train_model = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.critic_loss)
    
    
        def predict_action(self, state):
            if self.mu != 0:
                self.epsilon *= self.epsilon_decay
            if self.epsilon &lt; 0.1:
                self.epsilon = 0.2
                self.epsilon_decay = 1
            if (np.random.random() &lt; self.epsilon):
                action = env.action_space.sample()
            else:
                action = self.sess.run(self.action, feed_dict={self.model_state_input: state})[0]
            return action
    
        def predict_value(self, state):
            q_value = self.sess.run(self.q_output, feed_dict={self.model_state_input: state})
            return q_value
    
        
        def train_model(self, state, advantage):
            mu, sigma, advantage, _, _, actor_loss, critic_loss = self.sess.run([self.mu_output, self.sigma_output, self.actual_advantage_label,self.actor_train_model, self.critic_train_model, self.actor_loss, self.critic_loss], feed_dict={self.model_state_input: state, self.actual_advantage_label: advantage})
            self.return_actor_loss = actor_loss
            self.return_critic_loss = critic_loss
            self.mu = mu
            self.sigma = sigma
    
        def get_actor_loss(self):
            return self.return_actor_loss
    
        def get_critic_loss(self):
            return self.return_critic_loss
        
        def get_mu(self):
            return self.mu
        def get_sigma(self):
            return self.sigma
    
    
        def experience_replay(self):
            #Learn from memory
            if len(self.memory) &lt; 250:
                return
            del self.memory[np.random.randint(0,250)]
            batch = random.sample(self.memory, 5)
    
            #Train from a random batch of memory
            for state, action, reward, new_state, done in batch:
                target = reward
                new_state = np.array(new_state).reshape((1, self.num_states))
                
                #Get advantage of taking action over the average reward we get.
                if not done:
                    target = (reward + (self.gamma * self.predict_value(new_state)[0][0])) - self.predict_value(state)[0][0]
                target = np.array(target).reshape((1, 1))
                #Train model to solve enviroment
                self.train_model(state, target)
    
    
    
    
    env = gym.make(""Pendulum-v0"") # ""Pendulum-v0"" Or ""MountainCarContinuous-v0""
    sess = tf.Session()
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.shape[0]
    A2C = pendulum(sess, env, state_size, action_size)
    
    while True:
        state = env.reset()
        for i in range(1000): 
            env.render()
            state = np.array(state).reshape((1, state_size))
            action = A2C.predict_action(state)
            new_state, reward, done, _ = env.step(action)
            A2C.memory.append([state,action,reward,new_state,done])
            A2C.experience_replay()
            state = new_state
            if done:
                break
                
    
        print(""actor loss = "", A2C.get_actor_loss(), ""critic loss = "", A2C.get_critic_loss(), ""mu = "", A2C.get_mu(), ""sigma = "", A2C.get_sigma())
    
    
    
    
    
    

&amp;#x200B;",20,1,False,self,,,,,
239,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbocn,streamanalytix.com,IoT Analytics - Streaming Data Analytics for IoT with Apache Spark,https://www.reddit.com/r/MachineLearning/comments/anbocn/iot_analytics_streaming_data_analytics_for_iot/,Emma-Thompson,1549350311,,0,1,False,default,,,,,
240,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbood,self.MachineLearning,[D] Unpopular opinion: I prefer graph execution over eager mode in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/anbood/d_unpopular_opinion_i_prefer_graph_execution_over/,carlthome,1549350399,"Coming from compsci, I know static analysis and compile steps are required for managing code complexity, especially for repos with more than one developer. [This is a good read.](http://www.haskellforall.com/2016/04/worst-practices-should-be-hard.html)

TensorFlow's graph API is thus one of the few semi-comfortable areas for me in Python-land (where typically, mistakes default to being detected at runtime), acting like a compiler for vectorized expressions. This was what I always wanted for my MATLAB scripting in university!

In the back of my mind I hoped the immense popularity of TensorFlow could turn Python developers to enjoy static code analysis more so that lazy languages like Haskell would seem less foreign eventually.

After getting repeatedly burnt by scripts crashing late into their run, and finally finding pure functional languages truly intuitive and reminiscent of regular algebra, I think it's disconcerting that _eager mode_ is overtaking the language agnostic graph/session API. Unfortunately I think the move towards an imperative Python frontend will introduce even more brittle and hard to grok code, and I wish TensorFlow had made a harder push towards a pure functional library for numerical computing.

Maybe Swift for TensorFlow is the future?",40,1,False,self,,,,,
241,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbrn1,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/anbrn1/global_machine_learning_market_size_outlook/,vardhan1020,1549351172,,0,1,False,default,,,,,
242,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbtpk,self.MachineLearning,Is Instagram the best source for collecting Data Sets,https://www.reddit.com/r/MachineLearning/comments/anbtpk/is_instagram_the_best_source_for_collecting_data/,MarkPetter,1549351729,[removed],0,1,False,self,,,,,
243,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbu4e,darrequipment.com,Forklift Repair and Troubleshooting Tips,https://www.reddit.com/r/MachineLearning/comments/anbu4e/forklift_repair_and_troubleshooting_tips/,darrequipment,1549351838,,0,1,False,default,,,,,
244,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbw72,self.MachineLearning,Find similar faces on social media.,https://www.reddit.com/r/MachineLearning/comments/anbw72/find_similar_faces_on_social_media/,Anthony1985,1549352430,"I been wondering if there is a tool / website that lets you find similar faces to social media, moreover on Instagram.

&amp;#x200B;

I heard that there was actually a website that would let you search for faces on the Russian facebook called VK, but seems like they went private.",0,1,False,self,,,,,
245,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbze2,self.MachineLearning,[R] Flickr-Faces-HQ Dataset (FFHQ),https://www.reddit.com/r/MachineLearning/comments/anbze2/r_flickrfaceshq_dataset_ffhq/,milaworld,1549353356,"[Flickr-Faces-HQ (FFHQ)](https://github.com/NVlabs/ffhq-dataset) is a high-quality image dataset of human faces, originally created as a benchmark for generative adversarial networks (GAN), and used in the [StyleGAN](http://stylegan.xyz/paper) paper.

The dataset consists of 70,000 high-quality PNG images at 1024&amp;times;1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.

https://github.com/NVlabs/ffhq-dataset
",12,1,False,self,,,,,
246,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,16,anbzzd,arxiv.org,[R] Training Deep Neural Networks with 8-bit Floating Point Numbers,https://www.reddit.com/r/MachineLearning/comments/anbzzd/r_training_deep_neural_networks_with_8bit/,modeless,1549353533,,4,1,False,default,,,,,
247,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,17,anc17t,self.MachineLearning,Machine learning papers summary ?,https://www.reddit.com/r/MachineLearning/comments/anc17t/machine_learning_papers_summary/,ashutosj,1549353883,[removed],0,1,False,self,,,,,
248,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,17,anc1c0,self.MachineLearning,[Project] 50x Faster Tsetlin Machine in CUDA with IMDB Demo,https://www.reddit.com/r/MachineLearning/comments/anc1c0/project_50x_faster_tsetlin_machine_in_cuda_with/,olegranmo,1549353914,"Leveraging bitwise operators and parallel GPU computation, I have managed to make the Tsetlin Machine approx. 50 times faster. One epoch on IMDB, updating 50 million Tsetlin Automata, can be carried out in approx. 20 secs on a Tesla V100.  [https://github.com/cair/fast-tsetlin-machine-in-cuda-with-imdb-demo](https://github.com/cair/fast-tsetlin-machine-in-cuda-with-imdb-demo)",7,1,False,self,,,,,
249,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,18,ancg4d,arxiv.org,[R][1811.00525] On the Geometry of Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/ancg4d/r181100525_on_the_geometry_of_adversarial_examples/,IborkedyourGPU,1549358358,,2,1,False,default,,,,,
250,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,19,ancpw6,self.MachineLearning,"[Project] Multi-lingual un-punctuated sentence segmentation with BiLSTM+CRF for English, French, Italian.",https://www.reddit.com/r/MachineLearning/comments/ancpw6/project_multilingual_unpunctuated_sentence/,winchester6788,1549361285,"Hi all, I have trained and released https://github.com/bedapudi6788/deepsegment for French, Italian and English.

For this, I use vector alignment. The model performs excellently even for unpunctuated text. The english demo can be seen at bpraneeth.com/projects .

The pre-trained models are available at https://github.com/bedapudi6788/DeepSegment-Models .

The methodology followed is explained in the post https://medium.com/@praneethbedapudi/deepsegment-2-0-multilingual-text-segmentation-with-vector-alignment-fd76ce62194f",0,1,False,self,,,,,
251,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,19,ancrvd,theappsolutions.com,Recommender Systems and Their Impact on Customer Experience,https://www.reddit.com/r/MachineLearning/comments/ancrvd/recommender_systems_and_their_impact_on_customer/,lady_monsoon,1549361825,,0,1,False,default,,,,,
252,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,19,ancvjs,self.MachineLearning,[R] SimGNN: A Neural Network Approach to Fast Graph Similarity Computation (WSDM 2019),https://www.reddit.com/r/MachineLearning/comments/ancvjs/r_simgnn_a_neural_network_approach_to_fast_graph/,benitorosenberg,1549362853,"&amp;#x200B;

*Processing img qgohvu0k9qe21...*

**Paper:** [http://web.cs.ucla.edu/\~yzsun/papers/2019\_WSDM\_SimGNN.pdf](http://web.cs.ucla.edu/~yzsun/papers/2019_WSDM_SimGNN.pdf)

**PyTorch implementation:** [https://github.com/benedekrozemberczki/SimGNN](https://github.com/benedekrozemberczki/SimGNN)

**Tensorflow implementation:** [https://github.com/yunshengb/SimGNN](https://github.com/yunshengb/SimGNN)

**Abstract:**

Graph similarity search is among the most important graph-based  applications, e.g. finding the chemical compounds that are most similar  to a query compound. Graph similarity/distance computation, such as  Graph Edit Distance (GED) and Maximum Common Subgraph (MCS), is the core  operation of graph similarity search and many other applications, but  very costly to compute in practice. Inspired by the recent success of  neural network approaches to several graph applications, such as node or  graph classification, we propose a novel neural network based approach  to address this classic yet challenging graph problem, aiming to  alleviate the computational burden while preserving a good performance.  The proposed approach, called SimGNN, combines two strategies. First, we  design a learnable embedding function that maps every graph into an  embedding vector, which provides a global summary of a graph. A novel  attention mechanism is proposed to emphasize the important nodes with  respect to a specific similarity metric. Second, we design a pairwise  node comparison method to sup plement the graph-level embeddings with  fine-grained node-level information. Our model achieves better  generalization on unseen graphs, and in the worst case runs in quadratic  time with respect to the number of nodes in two graphs. Taking GED  computation as an example, experimental results on three real graph  datasets demonstrate the effectiveness and efficiency of our approach.  Specifically, our model achieves smaller error rate and great time  reduction compared against a series of baselines, including several  approximation algorithms on GED computation, and many existing graph  neural network based models. Our study suggests SimGNN provides a new  direction for future research on graph similarity computation and graph  similarity search.

&amp;#x200B;",4,1,False,self,,,,,
253,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,19,ancx0z,self.MachineLearning,A look over Machine learning and jobs associated with it,https://www.reddit.com/r/MachineLearning/comments/ancx0z/a_look_over_machine_learning_and_jobs_associated/,Summerinternship,1549363271,[removed],0,1,False,self,,,,,
254,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,19,ancxis,smarten.com,Advanced Analytics with Natural Language Processing (NLP)!,https://www.reddit.com/r/MachineLearning/comments/ancxis/advanced_analytics_with_natural_language/,ElegantMicroWebIndia,1549363402,,0,1,False,default,,,,,
255,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,19,ancxqe,self.MachineLearning,Machine Learning in JavaScript with TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/ancxqe/machine_learning_in_javascript_with_tensorflowjs/,JJohnson0x,1549363462,[removed],0,1,False,self,,,,,
256,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,and3gc,self.MachineLearning,[D] What Machine Learning papers do you think are very important to the field but also very hard to understand?,https://www.reddit.com/r/MachineLearning/comments/and3gc/d_what_machine_learning_papers_do_you_think_are/,freechoice,1549365019,,11,1,False,self,,,,,
257,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,and3w7,arxiv.org,[1902.00717] De-Health: All Your Online Health Information Are Belong to Us,https://www.reddit.com/r/MachineLearning/comments/and3w7/190200717_dehealth_all_your_online_health/,ihaphleas,1549365135,,1,1,False,default,,,,,
258,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,and4es,arxiv.org,[1902.00744] Asymmetric Valleys: Beyond Sharp and Flat Local Minima,https://www.reddit.com/r/MachineLearning/comments/and4es/190200744_asymmetric_valleys_beyond_sharp_and/,ihaphleas,1549365283,,1,1,False,default,,,,,
259,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,and4qr,arxiv.org,[1902.00648] Numerical Integration Method for Training Neural Network,https://www.reddit.com/r/MachineLearning/comments/and4qr/190200648_numerical_integration_method_for/,ihaphleas,1549365378,,8,1,False,default,,,,,
260,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,and87m,self.MachineLearning,ML Coursera Andrew Ng. Assigment's token,https://www.reddit.com/r/MachineLearning/comments/and87m/ml_coursera_andrew_ng_assigments_token/,miscmate,1549366309,[removed],0,1,False,self,,,,,
261,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,andb1z,activewizards.com,Top 10 Technology Trends of 2019,https://www.reddit.com/r/MachineLearning/comments/andb1z/top_10_technology_trends_of_2019/,viktoriia_shulga,1549367053,,0,1,False,default,,,,,
262,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,andcbm,self.MachineLearning,Variance Accounted For - some help needed,https://www.reddit.com/r/MachineLearning/comments/andcbm/variance_accounted_for_some_help_needed/,thetarunbs,1549367377,[removed],0,1,False,self,,,,,
263,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,20,anddyc,self.MachineLearning,In Mould Labelling Market Set for Rapid Growth and Trend by 2024,https://www.reddit.com/r/MachineLearning/comments/anddyc/in_mould_labelling_market_set_for_rapid_growth/,Shivs7,1549367802,[removed],0,1,False,self,,,,,
264,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,21,andun5,aimachinelearninganddatascience.wordpress.com,Important Types of Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/andun5/important_types_of_machine_learning_algorithms/,fullstackanalytics1,1549371541,,0,1,False,default,,,,,
265,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,22,andwqv,self.MachineLearning,How to Get into ML Research,https://www.reddit.com/r/MachineLearning/comments/andwqv/how_to_get_into_ml_research/,MyBloodyEscapePlan,1549371972,"Hi everyone. I'm relatively new to ML and DS as a whole, but I've decided that I want to pursue a PhD in ML. I can code in Python already, and I just started Andrew Ng's course. 

What should be my steps after that?

What books should I read?

How do I know when I am ready to start implementing algorithms from research papers?

I plan to start my masters in stats next Spring. Should I focus more on theory or application right now?

Any advice you guys have would be great. Thank you!",0,1,False,self,,,,,
266,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,22,andyg2,youtu.be,AI &amp; Computer Vision: Object Detection and Segmentation with Mask R-CNN,https://www.reddit.com/r/MachineLearning/comments/andyg2/ai_computer_vision_object_detection_and/,developFFM,1549372312,,2,1,False,https://b.thumbs.redditmedia.com/76M6Q5elSq2aFkUGQ_2Eu5csUtFQFM07Q_TKDShb0Cs.jpg,,,,,
267,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,22,ane3yp,self.MachineLearning,Can ML help with my business problem and where do I start if so?,https://www.reddit.com/r/MachineLearning/comments/ane3yp/can_ml_help_with_my_business_problem_and_where_do/,bustyLaserCannon,1549373454,[removed],0,1,False,self,,,,,
268,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,22,ane666,arxiv.org,[1902.00249] ProteinNet: a standardized data set for machine learning of protein structure,https://www.reddit.com/r/MachineLearning/comments/ane666/190200249_proteinnet_a_standardized_data_set_for/,SuperFX,1549373896,,1,1,False,default,,,,,
269,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,22,ane84d,self.MachineLearning,[N] NeuRIPS 2019 deadline announces; 4500 reviewers needed,https://www.reddit.com/r/MachineLearning/comments/ane84d/n_neurips_2019_deadline_announces_4500_reviewers/,olBaa,1549374282,"https://medium.com/@NeurIPSConf/starting-preparations-for-the-neurips-2019-review-process-d104ec708045

Overall increase by 50\% across all categories of reviewer food chain. Do you think this model is sustainable with so many people reviewing?",0,1,False,self,,,,,
270,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,22,ane8a6,self.MachineLearning,[N] NeuRIPS 2019 announced; 4500 reviewers needed,https://www.reddit.com/r/MachineLearning/comments/ane8a6/n_neurips_2019_announced_4500_reviewers_needed/,olBaa,1549374312,"https://medium.com/@NeurIPSConf/starting-preparations-for-the-neurips-2019-review-process-d104ec708045

Overall increase by 50\% across all categories of reviewer food chain. Do you think this model is sustainable with so many people reviewing?",15,1,False,self,,,,,
271,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,23,anee0e,self.MachineLearning,[D] Are evolutionary algorithms really the way to go for optimization?,https://www.reddit.com/r/MachineLearning/comments/anee0e/d_are_evolutionary_algorithms_really_the_way_to/,harshitsikchi,1549375460,"I have been working on improving the sample efficiency of RL algorithms. There has been some work on neuro-evolution, in general, optimization in parameter space. But this doesn't feel intuitive. It seems like making the problem harder by searching in a higher dimensional space.   


Consider a simple experiment, you are playing MountainCar-v0 for the first time. Your initial strategy will be to go right(as that is where the goal is). Now you execute that. But now you realize the car cannot reach the goal(as the velocity slides to zero over the hill). Now you have a different strategy. You go left and then move right.   


In the above experiment, isn't it clear that humans tend to think in terms of actions? and this, in turn, trains our mind to perform the current task better next time(more gradient like). ",26,1,False,self,,,,,
272,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,23,anekxg,self.MachineLearning,What books would you recommend which also covers advanced topics in good detail to a beginner?,https://www.reddit.com/r/MachineLearning/comments/anekxg/what_books_would_you_recommend_which_also_covers/,yatharthkaushik,1549376775,[removed],0,1,False,self,,,,,
273,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,23,aneqv7,self.MachineLearning,Solar data set and exploring machine learning projects,https://www.reddit.com/r/MachineLearning/comments/aneqv7/solar_data_set_and_exploring_machine_learning/,Monina306,1549377906,[removed],0,1,False,self,,,,,
274,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,23,aner1c,github.com,[N] Nvidia StyleGAN code released,https://www.reddit.com/r/MachineLearning/comments/aner1c/n_nvidia_stylegan_code_released/,joshuacpeterson,1549377933,,0,1,False,https://b.thumbs.redditmedia.com/69M9R5l9kP03Y3bLBJEXPDF7cTvAYsrkr-fvt2A4euY.jpg,,,,,
275,MachineLearning,t5_2r3gv,2019-2-5,2019,2,5,23,anernc,self.MachineLearning,Implementing a ML model in a web app,https://www.reddit.com/r/MachineLearning/comments/anernc/implementing_a_ml_model_in_a_web_app/,ghaaawd,1549378046,[removed],0,1,False,self,,,,,
276,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,0,anfgwc,self.MachineLearning,[R] Facebook has released code and pretrained models for Cross-lingual Language Model Pretraining,https://www.reddit.com/r/MachineLearning/comments/anfgwc/r_facebook_has_released_code_and_pretrained/,Thomjazz,1549382364,"It's an impressive piece of work that get state-of-the-art results on Cross-lingual Natural Language Inference and unsupervised Machine Translation (even out-performing supervised Machine Translation in some settings).

The model is a cross-lingual implementation of BERT.

PyTorch implementation and pre-trained models: [https://github.com/facebookresearch/XLM](https://github.com/facebookresearch/XLM)

Paper: [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291)",22,1,False,self,,,,,
277,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,1,anfj9m,medium.com,Papers With Code Adds State-of-the-Art Features,https://www.reddit.com/r/MachineLearning/comments/anfj9m/papers_with_code_adds_stateoftheart_features/,Yuqing7,1549382732,,0,1,False,https://b.thumbs.redditmedia.com/UFXG5WCGKyUEmW5dg3fBxioLI0xb6v8t7OAwO9mh_sw.jpg,,,,,
278,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,1,anfpoe,i.redd.it,What does the symbol mean?,https://www.reddit.com/r/MachineLearning/comments/anfpoe/what_does_the_symbol_mean/,proggramy,1549383770,,0,1,False,https://b.thumbs.redditmedia.com/WCafCuPasEyjQqOvB-tcIFY7E-nm-MGft2B2RtRKV_w.jpg,,,,,
279,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,1,ang2b6,self.MachineLearning,Is a Stochastic Block Model considered a machine learning algorithm?,https://www.reddit.com/r/MachineLearning/comments/ang2b6/is_a_stochastic_block_model_considered_a_machine/,conorosully1,1549385786,,0,1,False,self,,,,,
280,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,2,angbuw,self.MachineLearning,Need help with student assessment dataset,https://www.reddit.com/r/MachineLearning/comments/angbuw/need_help_with_student_assessment_dataset/,Nusherjk,1549387265,[removed],0,1,False,self,,,,,
281,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,2,angnzo,self.MachineLearning,Nvidia GPU for dell Laptop for deep learning/machine learning?,https://www.reddit.com/r/MachineLearning/comments/angnzo/nvidia_gpu_for_dell_laptop_for_deep/,bandalorian,1549389155,[removed],0,1,False,self,,,,,
282,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,3,angzxl,producthunt.com,LanguageBot for translating between languages inside Slack (Like google translate but in Slack),https://www.reddit.com/r/MachineLearning/comments/angzxl/languagebot_for_translating_between_languages/,arklabco,1549390983,,0,1,False,default,,,,,
283,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,3,anh7li,self.MachineLearning,[D] Baum-Welch HMM and numeric predictions,https://www.reddit.com/r/MachineLearning/comments/anh7li/d_baumwelch_hmm_and_numeric_predictions/,Unlistedd,1549392183,"Is it possible to build a Baum-Welch, (*HMM* \- *hidden markov model*), that identifies/estimates numeric values based on a set of statistical states?

&amp;#x200B;

2D time-value diagrams: (diagram 1, 2 &amp; 3)

|                                   |                                   |      

|                                    |                                 |                  

|                                  |                                  |      

|\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_          |\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_         |\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Suppose we have a diagram for every factor loading: \[1, 2, 3\], respective values are plotted on a time-value diagram. The -values are numeric as we use them for our equation: Y, Y = 1 \* C1 + 2 \* C2 + B3 \* C3 + . We want to predict these -values so that we can solve for the next Y. Is it possible to build a transition diagram based on numeric states or how would you go about solving this problem?  
",0,1,False,self,,,,,
284,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,3,anhbt9,youtu.be,Can a machine brain cure disease?,https://www.reddit.com/r/MachineLearning/comments/anhbt9/can_a_machine_brain_cure_disease/,sophylondon,1549392845,,0,1,False,https://b.thumbs.redditmedia.com/9hiJ0Y_sAAI8hJBlTOqqqrmOyTfn27xrfyvjYExG0Ug.jpg,,,,,
285,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,3,anhbuc,self.MachineLearning,"starcraft ai: generalizing to new maps, and the connectedness predicate?",https://www.reddit.com/r/MachineLearning/comments/anhbuc/starcraft_ai_generalizing_to_new_maps_and_the/,boyobo,1549392851,"One of the limitations of the AlphaStar that Deepmind showed us a few weeks ago was that it was only capable of playing on the map that it had been trained on. 

An interesting goal would be to train an AI that could adapt to a new map. One possible problem to solve is the following:

**Problem:** Train an AI that can be competitive even when it plays on a map that it hasn't played before. The AI given a 2d rendering of the [new map](https://www.google.com/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiHmNC7n6XgAhUAJjQIHZXtCREQjRx6BAgBEAU&amp;url=https%3A%2F%2Fstarcraft.fandom.com%2Fwiki%2FTempest_(map)&amp;psig=AOvVaw3rfXsuB9BFn784i8GsniEL&amp;ust=1549478525814886)  as an input, which it can use to plan its strategy.


It seems to me that any AI that can do this must, at some level, be able to solve the connectedness predicate or problems of a similar style. For instance it needs to be able to tell which parts of the map are islands (which is exactly the connectedness predicate). It might also be useful to estimate how quickly units can get from point a to point b, this would be useful in, say, base trade scenarios. 

However we know from Minksy and Pappert that these sorts of problems cannot be solved efficiently by feed forward neural networks. But obviously AlphaStar is not just a feedforward neural network.

My question is: if AlphaStar could be made to play on new maps, what parts of the AlphaStar architecture could potentially be responsible for solving this 'connectedness' subproblem? Does a RNN with LTSM suffice? Pointers to relevant research would be appreciated. 
",0,1,False,self,,,,,
286,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,4,anhtiy,self.MachineLearning,Image scraping service for deep learning,https://www.reddit.com/r/MachineLearning/comments/anhtiy/image_scraping_service_for_deep_learning/,gtruck,1549395618,[removed],0,1,False,self,,,,,
287,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,4,anhwdw,self.MachineLearning,Classification of Image Data by Support Vector Machine,https://www.reddit.com/r/MachineLearning/comments/anhwdw/classification_of_image_data_by_support_vector/,CodeMK,1549396048,"Hi, I'm looking for objects from an application area to train an SVM. I've been thinking for some time which objects are suitable, because I would not use images from fish or butterflies.

Does anybody have an interesting idea? Above all, objects would be in demand, which differ particularly because of their geometry.

&amp;#x200B;

Thanks in advance for every tip.",0,1,False,self,,,,,
288,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,4,anhzkq,self.MachineLearning,[Project] NVIDIA's StyleGAN code released,https://www.reddit.com/r/MachineLearning/comments/anhzkq/project_nvidias_stylegan_code_released/,thomash,1549396563,"The code to the paper *A Style-Based Generator Architecture for Generative Adversarial Networks* has just been released. The results, high-res images that look more authentic than previously generated images, caught the attention of the machine learning community at the end of last year but the code was only just released.

**Official Tensorflow implementation**: [https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)

**Paper:** [http://stylegan.xyz/paper](http://stylegan.xyz/paper)

&amp;#x200B;",8,1,False,self,,,,,
289,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,5,ani9it,self.MachineLearning,[Question] Are there any free online services that let you upload a data set then it can run different algorithms on it for you?,https://www.reddit.com/r/MachineLearning/comments/ani9it/question_are_there_any_free_online_services_that/,aatomator,1549398109,[removed],0,1,False,self,,,,,
290,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,6,anipya,medium.com,David vs Goliath: Clarifai CEO Matt Zeiler Takes On the Tech Giants,https://www.reddit.com/r/MachineLearning/comments/anipya/david_vs_goliath_clarifai_ceo_matt_zeiler_takes/,Yuqing7,1549400857,,0,1,False,default,,,,,
291,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,6,anir4z,self.MachineLearning,[D] Did I depict Deep learning here correctly?,https://www.reddit.com/r/MachineLearning/comments/anir4z/d_did_i_depict_deep_learning_here_correctly/,CrazyCar09,1549401041,"I am writing a research paper on deep learning. I am only a high school student and my teacher is clueless about machine learning. She asked for me to describe all of the specialty words. I don't know if I used this analogy correctly. I made it up in my head so it may be very far off. Side note: if anyone has some credible sources on the future of deep learning could you comment them? I haven't really found great sources. 

&amp;#x200B;

 Jeff Dean mentions a neural net. Neural net is short for the term neural network, a computer diagram that allows for deep learning to take place. This neural network looks like a morphed tree branch. Each branch has a set of twigs. These twigs are called the hidden layers. The term deep learning. Is a categorization of machine learning, they differ because there are many of these twigs, hidden layers. These twigs can form another twig, or layer, if needed. The final set of twigs will produce an output. These outputs gives the neural network its actions.  ",7,1,False,self,,,,,
292,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,6,anix8y,youtube.com,Explaining the Self-attention / Transformer,https://www.reddit.com/r/MachineLearning/comments/anix8y/explaining_the_selfattention_transformer/,DemiourgosD,1549401997,,0,1,False,default,,,,,
293,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,6,anj245,youtube.com,[R] Explaining the Self-attention / Transformer model,https://www.reddit.com/r/MachineLearning/comments/anj245/r_explaining_the_selfattention_transformer_model/,DemiourgosD,1549402736,,0,1,False,https://b.thumbs.redditmedia.com/bpC0V78resjfNps21TpQ1Zmzh8BtWB1Sn9cZgpXVXnE.jpg,,,,,
294,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,7,anjeal,arxiv.org,The Hanabi Challenge: A New Frontier for AI Research,https://www.reddit.com/r/MachineLearning/comments/anjeal/the_hanabi_challenge_a_new_frontier_for_ai/,j_orshman,1549404625,,32,1,False,default,,,,,
295,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,7,anjiqn,self.MachineLearning,What's the best way to add punctuation to a large corpus of text?,https://www.reddit.com/r/MachineLearning/comments/anjiqn/whats_the_best_way_to_add_punctuation_to_a_large/,cuddle_cuddle,1549405339,[removed],0,1,False,self,,,,,
296,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,8,ank83s,self.MachineLearning,[Project] Search within all the Project Gutenberg books with BERT,https://www.reddit.com/r/MachineLearning/comments/ank83s/project_search_within_all_the_project_gutenberg/,hollowayaegis,1549409479,"I think that a lot of interesting content is hidden in books and that a keywords search is not good enough to find this content. So I built something to search through all the paragraphs with natural language with BERT.

All the paragraphs in all the books have been fed to BERT Base Uncased and the outputs of the second to last layer averaged to get a single vector. The input query is also fed to BERT and then the nearest neighbors of this query among the vectors of all the paragraphs are what are shown as results.

Queries that are full sentences will work best! It's hosted here: [https://bookengine.colorsleep.com](https://bookengine.colorsleep.com). Feedback welcome and appreciated! I think it gives a good intuition of what close neighbors in Bert-output-space look like.",11,1,False,self,,,,,
297,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,8,anka20,self.MachineLearning,Machine Learning with C++ - Classification with Shark-ML,https://www.reddit.com/r/MachineLearning/comments/anka20/machine_learning_with_c_classification_with/,andrea_manero,1549409818,https://www.datasciencecentral.com/profiles/blogs/machine-learning-with-c-classification-with-shark-ml/,0,1,False,self,,,,,
298,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,8,anketk,self.MachineLearning,"[D] starcraft ai: generalizing to new maps, and the connectedness predicate?",https://www.reddit.com/r/MachineLearning/comments/anketk/d_starcraft_ai_generalizing_to_new_maps_and_the/,boyobo,1549410657,"One of the limitations of the AlphaStar that Deepmind showed us a few weeks ago was that it was only capable of playing on the map that it had been trained on. 

An interesting goal would be to train an AI that could adapt to a new map. One possible problem to solve is the following:

**Problem:** Train an AI that can be competitive even when it plays on a map that it hasn't played before. The AI given a 2d rendering of the [new map](https://www.google.com/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiHmNC7n6XgAhUAJjQIHZXtCREQjRx6BAgBEAU&amp;url=https%3A%2F%2Fstarcraft.fandom.com%2Fwiki%2FTempest_(map)&amp;psig=AOvVaw3rfXsuB9BFn784i8GsniEL&amp;ust=1549478525814886)  as an input, which it can use to plan its strategy.


It seems to me that any AI that can do this must, at some level, be able to solve the connectedness predicate or problems of a similar style. For instance it needs to be able to tell which parts of the map are islands (which is exactly the connectedness predicate). It might also be useful to estimate how quickly units can get from point a to point b, this would be useful in, say, base trade scenarios. 

However we know from Minksy and Pappert that these sorts of problems cannot be solved efficiently by feed forward neural networks. But obviously AlphaStar is not just a feedforward neural network.

My question is: if AlphaStar could be made to play on new maps, what parts of the AlphaStar architecture could potentially be responsible for solving this 'connectedness' subproblem? Does a RNN with LTSM suffice? Pointers to relevant research would be appreciated. 
",13,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,10,anlf19,self.MachineLearning,[P] McCulloch &amp; Pitts Neural Net Simulator,https://www.reddit.com/r/MachineLearning/comments/anlf19/p_mcculloch_pitts_neural_net_simulator/,justinmeiners,1549416996,"This project is more about the history of machine learning than any current research. In 1943 McCulloch &amp; Pitts created a model of computation based on neurons, rather than electronic logic gates. Their neurons were designed to reason about the kinds of calculations that brains could do, even though they were much more simple then real biological neurons.

I read Computation Finite and Infinite, by Marvin Minsky (1967), who is also the author of Percpetrons. In this book he uses this neural net model to explore theory of computation and finite state machines.  I thought neural nets were fascinating, and was surprised I had never heard about anything like them, besides machine learning.

I created an open source simulator so that people can learn about this history and play around with them: 

https://justinmeiners.github.io/neural-nets-sim/",0,1,False,self,,,,,
300,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,10,anlfyl,reddit.com,[R] TDLS classic papers presentation: Large-scale Parallel Collaborative Filtering for the Netflix Prize (w/ ALS algorithm),https://www.reddit.com/r/MachineLearning/comments/anlfyl/r_tdls_classic_papers_presentation_largescale/,tdls_to,1549417170,,0,1,False,default,,,,,
301,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,10,anlgqw,self.MachineLearning,[R] TDLS classic papers presentation: Large-scale Parallel Collaborative Filtering for the Netflix Prize (w/ ALS algorithm),https://www.reddit.com/r/MachineLearning/comments/anlgqw/r_tdls_classic_papers_presentation_largescale/,tdls_to,1549417302,"Part 1: [https://www.youtube.com/watch?v=lRBZzSWUkUI](https://www.youtube.com/watch?v=lRBZzSWUkUI)

Part 2: [https://www.youtube.com/watch?v=6fB0McQFSHQ](https://www.youtube.com/watch?v=6fB0McQFSHQ)

Paper: [https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E6%8E%A8%E8%8D%90/papers/Large-scale%20Parallel%20Collaborative%20Filtering%20the%20Netflix%20Prize.pdf](https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E6%8E%A8%E8%8D%90/papers/Large-scale%20Parallel%20Collaborative%20Filtering%20the%20Netflix%20Prize.pdf)",0,1,False,self,,,,,
302,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,10,anllwx,self.MachineLearning,What to use: Keras or tf.keras?,https://www.reddit.com/r/MachineLearning/comments/anllwx/what_to_use_keras_or_tfkeras/,idg101,1549418224,[removed],0,1,False,self,,,,,
303,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,11,anlo4k,arxiv.org,[R] AlphaStar: An Evolutionary Computation Perspective,https://www.reddit.com/r/MachineLearning/comments/anlo4k/r_alphastar_an_evolutionary_computation/,baylearn,1549418626,,4,1,False,default,,,,,
304,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,12,anmby8,self.MachineLearning,Newbie Developing an app which needs machine learning need help with it,https://www.reddit.com/r/MachineLearning/comments/anmby8/newbie_developing_an_app_which_needs_machine/,mysterio26,1549422955,[removed],0,1,False,self,,,,,
305,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,12,anmgtg,self.MachineLearning,[D] Voice Cloning/Conversion?,https://www.reddit.com/r/MachineLearning/comments/anmgtg/d_voice_cloningconversion/,nixylvarie,1549423842,"I am new to machine learning, but familiar with Python - as my first foray into this subject, I want to attempt to create a voice modulator which alters pitch, volume, and timbre to match a pre-recorded voice (represented as a vector) while leaving timelike features like prosody unaltered. This is probably a difficult problem, and I dont plan on doing this right away, but that is my end goal. How should I get started, and what beginner things should I look into before attempting this?

Please downvote me if this is a stupid question that gets asked a lot. Sorry.",1,1,False,self,,,,,
306,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,12,anmkgy,self.MachineLearning,"[D] Keras or tf.keras, that is the question",https://www.reddit.com/r/MachineLearning/comments/anmkgy/d_keras_or_tfkeras_that_is_the_question/,idg101,1549424519,"I am getting very frustrated with keras and the documentation seems to  be lacking when you really need to dig into something.  

Is switching to  tf.keras the way to go? I know with tf.keras you can get eager execution  now.  

Which version of keras is best to use? ",11,1,False,self,,,,,
307,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,13,anmuhb,self.MachineLearning,Suggest me a laptop,https://www.reddit.com/r/MachineLearning/comments/anmuhb/suggest_me_a_laptop/,A_man_p,1549426340,[removed],0,1,False,self,,,,,
308,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,13,ann3i9,self.MachineLearning,How can we recreate the neural net described in this video with TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/ann3i9/how_can_we_recreate_the_neural_net_described_in/,jamesbjoyce,1549428035,[https://www.youtube.com/watch?v=QpH-YPin01k](https://www.youtube.com/watch?v=QpH-YPin01k),0,1,False,self,,,,,
309,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,13,ann7dp,self.MachineLearning,Is this a good course selection for an undergrad wishing to do AI/ML research? Any suggestions?,https://www.reddit.com/r/MachineLearning/comments/ann7dp/is_this_a_good_course_selection_for_an_undergrad/,bobby891a,1549428753,[removed],0,1,False,self,,,,,
310,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,14,annj91,self.MachineLearning,[D] Are you ready to change the world? | Umbra AI Recruitment,https://www.reddit.com/r/MachineLearning/comments/annj91/d_are_you_ready_to_change_the_world_umbra_ai/,danielhanchen,1549431047,"Hey everyone! I've started on a new AI startup named Umbra AI. My aim is to create the world's first public simulation of Planet Earth to solve crime, poverty, congestion to name a few.

I'm currently looking for more people to join, and if you want to change the world like me, please do a short quiz @  [Are you ready to change the world?](https://daniel3112.typeform.com/to/K84Qu0) . I'll be waiting :) Thanks all!

(By the way, I want to know your breadth not depth of knowledge, and you have infinite tries, infinite time to do this!)

&amp;#x200B;

https://i.redd.it/0fxuabvyvve21.png

By the way, I can be reached @ [danielhanchen@gmail.com](mailto:danielhanchen@gmail.com) if you have questions or stuff to talk about. Thanks all!",0,1,False,https://b.thumbs.redditmedia.com/a2kt8nNt1BkKjZ2XgK2sKPWrZzhJn8wJs0fPLbf0lOI.jpg,,,,,
311,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,15,annwnl,self.MachineLearning,"Interview with Kaggle (RSNA Pneumonia Detection Challenge winner) Expert, Radiologist: Dr. Alexandre Cadrin-Chenevert",https://www.reddit.com/r/MachineLearning/comments/annwnl/interview_with_kaggle_rsna_pneumonia_detection/,init__27,1549433987,[removed],0,1,False,self,,,,,
312,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,15,ano30h,self.MachineLearning,Need help to extract entities from Resume,https://www.reddit.com/r/MachineLearning/comments/ano30h/need_help_to_extract_entities_from_resume/,omkarpathak27,1549435568,[removed],0,1,False,self,,,,,
313,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,16,ano6hu,self.MachineLearning,LSTM model not showing output and giving me error,https://www.reddit.com/r/MachineLearning/comments/ano6hu/lstm_model_not_showing_output_and_giving_me_error/,madh46,1549436444,"I'm a newbie in machine learning and all this stuff. I did some classification problems and I was just trying out RNN, particularly a lstm model which would predict heart disease. I used this [https://www.kaggle.com/ronitf/heart-disease-uci](https://www.kaggle.com/ronitf/heart-disease-uci) dataset

&amp;#x200B;

    import tensorflow as tf
    import pandas as pd
    import numpy as np
    from tensorflow.contrib import rnn
    from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
    
    def preProcess():
    
        data = pd.read_csv('heart.csv')
    
        features = data.iloc[:, 0:13]
        labels = data.iloc[:, -1]
    
        return features, labels
    
    def get_batches(x, y, batch_size=100):
        n_batches = len(x) // batch_size
        x, y = x[:n_batches * batch_size], y[:n_batches * batch_size]
        for ii in range(0, len(x), batch_size):
            yield x[ii:ii + batch_size], y[ii:ii + batch_size]
    
    
    def train_test_neural_network():
    
        features, labels, = preProcess()
    
        split_frac1 = 0.8
    
        idx1 = int(len(features) * split_frac1)
        train_x, test_x = features[:idx1], features[idx1:]
        train_y, test_y = labels[:idx1], labels[idx1:]
    
    
        epochs = 8
        n_classes = 1
        n_units = 200
        n_features = 13
        batch_size = 35
    
        # Create the graph object
        graph = tf.Graph()
        # Add nodes to the graph
        with graph.as_default():
    
            # shape of place holder when training (&lt;batch size&gt;, 30)
            xplaceholder = tf.placeholder('float', [None, n_features])
            # shape of place holder when training (&lt;batch size&gt;,)
            yplaceholder = tf.placeholder('float')
    
            # giving the weights and biases random values
            layer = {'weights': tf.Variable(tf.random_normal([n_units, n_classes])),
                     'bias': tf.Variable(tf.random_normal([n_classes]))}
    
            x = tf.split(xplaceholder, n_features, 0)
    
            lstm_cell = rnn.BasicLSTMCell(n_units)
    
            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)
    
            output = tf.matmul(outputs[-1], layer['weights']) + layer['bias']
    
            logit = tf.reshape(output, [-1])
    
            cost, final_state = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=yplaceholder))
    
            optimizer = tf.train.AdamOptimizer().minimize(cost)
    
            predictions = tf.round(tf.nn.sigmoid(logit))
    
        with tf.Session(graph=graph) as sess:
    
            tf.set_random_seed(1)
    
            sess.run(tf.global_variables_initializer())
    
            iteration = 1
            for e in range(epochs):
                for ii, (x, y) in enumerate(get_batches(np.array(train_x), np.array(train_y), batch_size), 1):
    
                    feed = {xplaceholder: x,
                            yplaceholder: y,
                            }
    
                    loss, states, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)
    
                    if iteration % 5 == 0:
                        print(""Epoch: {}/{}"".format(e, epochs),
                              ""Iteration: {}"".format(iteration),
                              ""Train loss: {:.3f}"".format(loss))
                    iteration += 1
    
        # -----------------testing test set-----------------------------------------
        print(""starting testing set"")
        prediction_val = []
        y_val = []
        with tf.Session(graph=graph) as sess:
    
            tf.set_random_seed(1)
    
            for ii, (x, y) in enumerate(get_batches(np.array(test_x), np.array(test_y), batch_size), 1):
    
                feed = {xplaceholder: x,
                        yplaceholder: y,
                        }
    
                prediction = sess.run(predictions, feed_dict=feed)
                prediction = prediction.astype(int)
    
                for i in range(len(prediction)):
                    prediction_val.append(prediction[i][0])
                    y_val.append(y[i])
    
            accuracy = accuracy_score(y_val, prediction_val)
            f1 = f1_score(y_val, prediction_val, average='macro')
            recall = recall_score(y_true=y_val, y_pred=prediction_val, average='macro')
            precision = precision_score(y_val, prediction_val, average='macro')
    
            print(""-----------------testing validation set-----------------------------------------"")
            print(""Test accuracy: {:.3f}"".format(accuracy))
            print(""F1 Score: {:.3f}"".format(f1))
            print(""Recall: {:.3f}"".format(recall))
            print(""Precision: {:.3f}"".format(precision))
    
    if __name__ == '__main__':
        train_test_neural_network()

 The above code is giving me this error: 

&amp;#x200B;

     WARNING:tensorflow:From &lt;ipython-input-1-467b0d09f413&gt;:63: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
    Instructions for updating:
    This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').
    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    &lt;ipython-input-1-467b0d09f413&gt; in &lt;module&gt;()
        130 
        131 if __name__ == '__main__':
    --&gt; 132     train_test_neural_network()
    
    &lt;ipython-input-1-467b0d09f413&gt; in train_test_neural_network()
         69         logit = tf.reshape(output, [-1])
         70 
    ---&gt; 71         cost, final_state = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=yplaceholder))
         72 
         73         optimizer = tf.train.AdamOptimizer().minimize(cost)
    
    ~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __iter__(self)
        457     if not context.executing_eagerly():
        458       raise TypeError(
    --&gt; 459           ""Tensor objects are only iterable when eager execution is ""
        460           ""enabled. To iterate over this tensor use tf.map_fn."")
        461     shape = self._shape_tuple()
    
    TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.

 I can't figure out what am I doing wrong here? Why am I getting this error? How should I fix it? ",0,1,False,self,,,,,
314,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,16,ano6ua,self.MachineLearning,Is this ICCV Paper Cheating or Do I Have a Misunderstanding?,https://www.reddit.com/r/MachineLearning/comments/ano6ua/is_this_iccv_paper_cheating_or_do_i_have_a/,Yeepong_Lau_2021,1549436523,[removed],0,1,False,https://b.thumbs.redditmedia.com/GXFCT6dtPiRPSOqV4IW_Znw2-q8CzAW6EyUtpqtY7MI.jpg,,,,,
315,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,16,anoe9x,self.MachineLearning,Upcoming series of carefully tailored videos and playlists for learning Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/anoe9x/upcoming_series_of_carefully_tailored_videos_and/,rmalhotra651,1549438413,[removed],0,1,False,self,,,,,
316,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,17,anokwx,self.MachineLearning,How can I fully utilise the GPU when training my model in FloyHub?,https://www.reddit.com/r/MachineLearning/comments/anokwx/how_can_i_fully_utilise_the_gpu_when_training_my/,dasgauranga2,1549440203,[removed],0,1,False,self,,,,,
317,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,17,anouag,rubikscode.net,Introduction to CycleGAN,https://www.reddit.com/r/MachineLearning/comments/anouag/introduction_to_cyclegan/,RubiksCodeNMZ,1549442874,,0,1,False,default,,,,,
318,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,17,anovev,self.MachineLearning,Anyone heard from MILA MSc/PhD application for Fall 2019?,https://www.reddit.com/r/MachineLearning/comments/anovev/anyone_heard_from_mila_mscphd_application_for/,karanchahal1996,1549443211,[removed],0,1,False,self,,,,,
319,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,18,anoylp,self.MachineLearning,What's the purpose of variance found in linear regression,https://www.reddit.com/r/MachineLearning/comments/anoylp/whats_the_purpose_of_variance_found_in_linear/,fedetask,1549444150,[removed],0,1,False,self,,,,,
320,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,18,anoz4p,dataespresso.com,Tutorial: Stress detection with wearable devices and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/anoz4p/tutorial_stress_detection_with_wearable_devices/,FrokenFrosk,1549444290,,0,1,False,default,,,,,
321,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,18,anp0ic,self.MachineLearning,How valid is it to change learning rate proportional to gradients while training a classifier?,https://www.reddit.com/r/MachineLearning/comments/anp0ic/how_valid_is_it_to_change_learning_rate/,nik38,1549444717,[removed],0,1,False,self,,,,,
322,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,18,anp605,self.MachineLearning,Need help: Prevent ddos attack using machine learning.,https://www.reddit.com/r/MachineLearning/comments/anp605/need_help_prevent_ddos_attack_using_machine/,harsh52,1549446323,"Hello everyone,
I am a CSE student and interested in machine learning and cyber security stuff.
I want to design a system which can prevent ddos attack, I had search on Google but not getting enough resources(only research paper).
I am really interested in this project.

I need some resources to complete this project like (git hub code), research paper etc.
Please help.",0,1,False,self,,,,,
323,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,19,anpafd,self.MachineLearning,Object detection without object recognition,https://www.reddit.com/r/MachineLearning/comments/anpafd/object_detection_without_object_recognition/,Nowado,1549447535,[removed],0,1,False,self,,,,,
324,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,19,anpc2i,self.MachineLearning,"Machine Learning as a Service (Mlaas) Market  Size, Outlook, Trends and Forecasts (2018  2024)",https://www.reddit.com/r/MachineLearning/comments/anpc2i/machine_learning_as_a_service_mlaas_market_size/,Objective_Concept,1549447994,[removed],0,1,False,self,,,,,
325,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,19,anpei8,self.MachineLearning,I want to get into ML and also build a new PC. Can ATI cards run ML applications or do I need an Nvidia GPU?,https://www.reddit.com/r/MachineLearning/comments/anpei8/i_want_to_get_into_ml_and_also_build_a_new_pc_can/,QQOQ,1549448685,[removed],0,1,False,self,,,,,
326,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,19,anpk5b,self.MachineLearning,Swine Flu Dataset?,https://www.reddit.com/r/MachineLearning/comments/anpk5b/swine_flu_dataset/,shreykhetrapal,1549450266,[removed],0,1,False,self,,,,,
327,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,19,anplj0,self.MachineLearning,[Project] tsalib: a library for tensor shape annotation and transformations (no more shape woes!),https://www.reddit.com/r/MachineLearning/comments/anplj0/project_tsalib_a_library_for_tensor_shape/,ekshaks,1549450672,"Tracking, transforming and ensuring validity of tensor shapes (and data pipelines) is a source of major pain when dealing with deep learning code. **tsalib** allows you to define and use **named dimensions** to

* track shapes with  shape annotations (`x: 'btd'` using optional type annotations in Python)
* debug with shape assertions,   (`assert x.shape == (B,T,D)`)
* and write (sequence of) shorthand, readable shape transformations. (`warp(x, 'btd -&gt; bdt -&gt; b,d//2,t*2`)

**repository**: [https://github.com/ofnote/tsalib/](https://github.com/ofnote/tsalib/)

&amp;#x200B;

Several tsalib-enhanced model examples are provided. In particular, the repository contains a fully-annotated and transformed [BERT model](https://github.com/ofnote/tsalib/tree/master/models/bert) based on the original tensorflow [code](https://github.com/google-research/bert). In our opinion, the enhanced model is more succinct and readable than the original, both due to shape annotations and shorthand transformations.

&amp;#x200B;

\---

The idea of named dimension is not new. Several proposals have been made -- see references [here](https://github.com/ofnote/tsalib#references). Unfortunately, the popular tensor libraries -- numpy, pytorch, tensorflow, ... -- do not yet support working with named dimensions natively.

`tsalib` bridges this gap by enabling you to **plugin** named shapes into your existing code:

* incorporates design decisions from multiple proposals
* is designed to work with arbitrary backends (currently tested with numpy, pytorch and tensorflow)
* is loosely-coupled to different backends, with backend-independent and dependent parts.
* designed for progressive adoption.

Would be glad to get feedback and contributions from all practitioners.

&amp;#x200B;",6,1,False,self,,,,,
328,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,20,anpvjy,arxiv.org,TF-Replicator Distributed Machine Learning for Researchers,https://www.reddit.com/r/MachineLearning/comments/anpvjy/tfreplicator_distributed_machine_learning_for/,amrit_za,1549453270,,2,1,False,default,,,,,
329,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,21,anqad5,artnome.com,"Artists use GANs to create ""infinite skulls"" for Paris gallery",https://www.reddit.com/r/MachineLearning/comments/anqad5/artists_use_gans_to_create_infinite_skulls_for/,hoopism,1549456615,,0,1,False,default,,,,,
330,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,21,anqais,self.learnprogramming,PSA: Many of Berkeley's courses have lectures and materials free online,https://www.reddit.com/r/MachineLearning/comments/anqais/psa_many_of_berkeleys_courses_have_lectures_and/,oodetola,1549456648,,0,1,False,default,,,,,
331,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,21,anqe2e,self.MachineLearning,Tensorflow - anaconda,https://www.reddit.com/r/MachineLearning/comments/anqe2e/tensorflow_anaconda/,Dankious_memeious1,1549457449,[removed],0,1,False,self,,,,,
332,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,21,anqeap,self.MachineLearning,"[N] Awesome papers, new tools, AI news and research reviews [with codes!] on Computer Vision News. Links for free reading!",https://www.reddit.com/r/MachineLearning/comments/anqeap/n_awesome_papers_new_tools_ai_news_and_research/,Gletta,1549457505,"Hot off the Press! Here are the links to the February 2019 issue of **Computer Vision News**, the magazine of the algorithm community published by **RSIP Vision**: many articles about **Artificial Intelligence** and **Autonomous Driving**. Free subscription on page 40.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019February/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-february-pdf/)

Enjoy!

https://i.redd.it/7fy0drfy2ye21.jpg",0,1,False,self,,,,,
333,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,22,anqown,self.MachineLearning,Anyone using ROCm 2.0 for Tensorflow with AMD GPUs? Is the setup straight forward and the performance ok?,https://www.reddit.com/r/MachineLearning/comments/anqown/anyone_using_rocm_20_for_tensorflow_with_amd_gpus/,QQOQ,1549459698,[removed],0,1,False,self,,,,,
334,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,22,anqr70,self.MachineLearning,Help with pattern recognition,https://www.reddit.com/r/MachineLearning/comments/anqr70/help_with_pattern_recognition/,ExternalStimuli,1549460165,[removed],0,1,False,self,,,,,
335,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,22,anqtvv,self.MachineLearning,[D] Speeding up non-linear dimensionality reduction,https://www.reddit.com/r/MachineLearning/comments/anqtvv/d_speeding_up_nonlinear_dimensionality_reduction/,kalabele,1549460694,"I am experimenting with Isomap, Kernel PCA, LLE and so on. Unfortunately, they are all very slow on my data set since it has several thousand features. Now I am wonderig if I could first apply PCA or ICA to reduce the number of columns to about a tenth (which still explains most of the variance) and then apply the non-linear dimensionality reduction. Is there any drawback to this approach? Are there other alternatives I should consider?

",5,1,False,self,,,,,
336,MachineLearning,t5_2r3gv,2019-2-6,2019,2,6,23,anrams,self.MachineLearning,[D] Sharing my personal resource list for deep learning comprehension,https://www.reddit.com/r/MachineLearning/comments/anrams/d_sharing_my_personal_resource_list_for_deep/,mavenchist,1549463834,"Since I always like to have some theoretical knowledge (often shallow) of modern techniques, I complied this list of (free) courses, textbooks and references for an educational approach to deep learning and neural nets.

* [Deep Learning (CS 1470)](http://cs.brown.edu/courses/cs1470/index.html)
* [Deep Learning Book](https://www.deeplearningbook.org/) [\[GitHub\]](https://github.com/janishar/mit-deep-learning-book-pdf) [\[tutorial\]](http://www.iro.umontreal.ca/~bengioy/talks/lisbon-mlss-19juillet2015.pdf) [\[videos\]](https://www.youtube.com/channel/UCF9O8Vj-FEbRDA5DcDGz-Pg/videos)
* [Dive into Deep Learning](https://d2l.ai/) [\[GitHub\]](https://github.com/d2l-ai/d2l-en) [\[pdf\]](https://en.d2l.ai/d2l-en.pdf) [\[STAT 157\]](http://courses.d2l.ai/berkeley-stat-157/index.html)
* [Neural Network Design](http://hagan.okstate.edu/nnd.html) [\[pdf\]](http://hagan.okstate.edu/NNDesign.pdf)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) [\[GitHub\]](https://github.com/mnielsen/neural-networks-and-deep-learning) [\[pdf\]](http://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf) [\[solutions\]](https://github.com/reachtarunhere/nndl/blob/master/2016-11-22-ch1-sigmoid-2.md)
* [Theories of Deep Learning (STATS 385)](https://stats385.github.io/) [\[videos\]](https://www.researchgate.net/project/Theories-of-Deep-Learning)
* [Theoretical Principles for Deep Learning (IFT 6085)](http://mitliagkas.github.io/ift6085-dl-theory-class-2019/)

Do with it, as you will. Any new books/updates that I'm missing here? ",26,1,False,self,,,,,
337,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,0,anrnxb,self.MachineLearning,Please help me understand deep learning,https://www.reddit.com/r/MachineLearning/comments/anrnxb/please_help_me_understand_deep_learning/,knownsuq,1549466160,[removed],0,1,False,self,,,,,
338,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,0,anrv9c,self.MachineLearning,Can I install dlib on DigitalOcean with 1GB of RAM?,https://www.reddit.com/r/MachineLearning/comments/anrv9c/can_i_install_dlib_on_digitalocean_with_1gb_of_ram/,rotttten,1549467368,[removed],0,1,False,self,,,,,
339,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,0,ans1mj,self.MachineLearning,"Simple Questions Thread February 06, 2019",https://www.reddit.com/r/MachineLearning/comments/ans1mj/simple_questions_thread_february_06_2019/,AutoModerator,1549468371,[removed],0,1,False,self,,,,,
340,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,1,ansat6,self.MachineLearning,Use Python to analyze a photo on cell phone.,https://www.reddit.com/r/MachineLearning/comments/ansat6/use_python_to_analyze_a_photo_on_cell_phone/,rinntintin1,1549469766,Hello! I am trying to design an free app to make my job easier.  Can you tell me if you think my idea is possible.  User will take a picture of a form.  App will analyze about 20 blocks.  After analyzing the photo it will tell you if you forgot to fill a block out or sign the form.  Thank you!,0,1,False,self,,,,,
341,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,1,ansem5,self.MachineLearning,[D] Advances in GANs,https://www.reddit.com/r/MachineLearning/comments/ansem5/d_advances_in_gans/,thatbrguy_,1549470363,"Hello everyone. I created a blog post on ""Advances in Generative Adversarial Networks"". The article aims to summarize some of the important concepts and improvements made in the field of GANs. The article is by no means exhaustive, but it covers a good amount of the salient concepts in this field. 

&amp;#x200B;

You can give it a read here: [https://medium.com/beyondminds/advances-in-generative-adversarial-networks-7bad57028032?source=friends\_link&amp;sk=9f420a2c96c228a3f6d1a9f52b382109](https://medium.com/beyondminds/advances-in-generative-adversarial-networks-7bad57028032?source=friends_link&amp;sk=9f420a2c96c228a3f6d1a9f52b382109)

&amp;#x200B;

I would love to hear your feedback and suggestions!",0,1,False,self,,,,,
342,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,1,ansio0,self.MachineLearning,Understanding Neural ODE's,https://www.reddit.com/r/MachineLearning/comments/ansio0/understanding_neural_odes/,begooboi,1549470983,[removed],0,1,False,self,,,,,
343,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,1,ansjut,self.MachineLearning,What's your best Podcast idea?,https://www.reddit.com/r/MachineLearning/comments/ansjut/whats_your_best_podcast_idea/,Doctor_who1,1549471164,[removed],0,1,False,self,,,,,
344,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,2,ansxcq,self.MachineLearning,[D] Understanding Neural ODE's,https://www.reddit.com/r/MachineLearning/comments/ansxcq/d_understanding_neural_odes/,begooboi,1549473157,"This article is written by Jonty Sinai and he is explaining the fundementals of neural ode and how its different from current artificial neural network.

https://jontysinai.github.io/jekyll/update/2019/01/18/understanding-neural-odes.html",14,1,False,self,,,,,
345,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,2,ansynk,self.MachineLearning,Are there any sources for pre-trained 3D models?,https://www.reddit.com/r/MachineLearning/comments/ansynk/are_there_any_sources_for_pretrained_3d_models/,jashshah27,1549473349,[removed],0,1,False,self,,,,,
346,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,2,ant4gt,youtube.com,DeepMinds AlphaStar Beats Humans 10-0 (or 1),https://www.reddit.com/r/MachineLearning/comments/ant4gt/deepminds_alphastar_beats_humans_100_or_1/,SpaceRustem,1549474222,,0,1,False,https://b.thumbs.redditmedia.com/4dA-r0Y-gRgq224Va16fjcYEF3ZgHPulZI0DvNGspxU.jpg,,,,,
347,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,2,ant9os,youtube.com,[D] DeepMinds AlphaStar Beats Humans 10-0 (or 1),https://www.reddit.com/r/MachineLearning/comments/ant9os/d_deepminds_alphastar_beats_humans_100_or_1/,SpaceRustem,1549475017,,0,1,False,https://b.thumbs.redditmedia.com/4dA-r0Y-gRgq224Va16fjcYEF3ZgHPulZI0DvNGspxU.jpg,,,,,
348,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,2,antc40,self.MachineLearning,[D] Why is robustness to adversarial attacks relevant?,https://www.reddit.com/r/MachineLearning/comments/antc40/d_why_is_robustness_to_adversarial_attacks/,Seerdecker,1549475382,"I see many papers discussing defenses against adversarial attacks. Yet the test error is far from zero on most practical datasets where that could come useful, e.g. ImageNet, SPAM/ad classification. It seems to me that a system cannot be made robust if it fails in the non-adversarial case in the first place. What am I missing?",40,1,False,self,,,,,
349,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,3,antm1v,scientificworldinfo.com,[D] SAM- A Self-learning assistance system for efficient processes,https://www.reddit.com/r/MachineLearning/comments/antm1v/d_sam_a_selflearning_assistance_system_for/,mahtabalam93,1549476850,,0,1,False,default,,,,,
350,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,3,antmse,self.MachineLearning,Cooking Dataset,https://www.reddit.com/r/MachineLearning/comments/antmse/cooking_dataset/,SrinathNM,1549476962,[removed],0,1,False,self,,,,,
351,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,3,antoz1,coderecipe.ai,"I created a diagram to share how to use AWS Rekognition, does it look useful to you? What else do you hope to see?",https://www.reddit.com/r/MachineLearning/comments/antoz1/i_created_a_diagram_to_share_how_to_use_aws/,codingrecipe,1549477290,,0,1,False,default,,,,,
352,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,3,antrmv,self.MachineLearning,[P] Activity Classification for watchOS: Part 2  Skafos.ai  Medium,https://www.reddit.com/r/MachineLearning/comments/antrmv/p_activity_classification_for_watchos_part_2/,Davlucmac,1549477695,"Part 2 in our journey to develop an ML-driven Apple Watch app. Would love your thoughts as we progress through this project!

[https://medium.com/metis-machine/activity-classification-for-watchos-part-2-1011ee5e75d5](https://medium.com/metis-machine/activity-classification-for-watchos-part-2-1011ee5e75d5)

*Note: Digging into Apple's Turicreate framework, I was surprised the activity classification example model included data from a competing smartphone platform. This article is aimed at developing an app for the same platform.*",1,1,False,self,,,,,
353,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anu6ga,opensource.googleblog.com,Dopamine 2.0: providing more flexibility in reinforcement learning research,https://www.reddit.com/r/MachineLearning/comments/anu6ga/dopamine_20_providing_more_flexibility_in/,whereistimbo,1549479880,,0,1,False,default,,,,,
354,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anubv4,self.MachineLearning,"[D] To those of you interested in learning about Machine Learning, what is preventing you from getting started?",https://www.reddit.com/r/MachineLearning/comments/anubv4/d_to_those_of_you_interested_in_learning_about/,hoekrb,1549480693,,25,1,False,self,,,,,
355,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anuf8d,self.MachineLearning,[Discussion]Machine learning suits vs. regular model building,https://www.reddit.com/r/MachineLearning/comments/anuf8d/discussionmachine_learning_suits_vs_regular_model/,Coffeino,1549481213,"I have heard a lot about rising popularity of machine learning 'plug and play' program suits such as IBM Watson and Aspentech Mtell. I am used to building my own models and identifying the best candidate fit for each new dataset, these developers claim they can do all this automatically. Do any of you have any hands-on experience with the performance of these models and the strengths and weaknesses compared to building your own? Thanks!",0,1,False,self,,,,,
356,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anuj33,self.MachineLearning,PPO actor-critic style,https://www.reddit.com/r/MachineLearning/comments/anuj33/ppo_actorcritic_style/,Ali7422626,1549481775,[removed],0,1,False,self,,,,,
357,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anujct,self.MachineLearning,[D] ML Model building vs. ML software suits,https://www.reddit.com/r/MachineLearning/comments/anujct/d_ml_model_building_vs_ml_software_suits/,Coffeino,1549481817,"I have heard a lot about rising popularity of machine learning 'plug and play' program suits such as IBM Watson and Aspentech Mtell. I am used to building my own models and identifying the best candidate fit for each new dataset, these developers claim they can do all this automatically. Do any of you have any hands-on experience with the performance of these models and the strengths and weaknesses compared to building your own?",0,1,False,self,,,,,
358,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anujhu,youtu.be,How to Become a Data Scientist in 2019,https://www.reddit.com/r/MachineLearning/comments/anujhu/how_to_become_a_data_scientist_in_2019/,sarasotadude,1549481837,,0,1,False,https://b.thumbs.redditmedia.com/GMAAJWYb2MV_tujFXPjEtd9xeoRPYABtZExXaBjwhic.jpg,,,,,
359,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anungs,self.MachineLearning,"Noob question, where do you find a bunch of images to train your algorithms?",https://www.reddit.com/r/MachineLearning/comments/anungs/noob_question_where_do_you_find_a_bunch_of_images/,b42thomas,1549482406,[removed],0,1,False,self,,,,,
360,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,4,anur20,self.MachineLearning,"Hi,",https://www.reddit.com/r/MachineLearning/comments/anur20/hi/,batmanparam,1549482927,[removed],0,1,False,self,,,,,
361,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,5,anv3m2,self.MachineLearning,[D] Supervised learning and HMM,https://www.reddit.com/r/MachineLearning/comments/anv3m2/d_supervised_learning_and_hmm/,Unlistedd,1549484849,"We start of with a dataset that describes a variable: Y.

There are too many variables, we use PCA to find the most important variables.

\-*PCA* is *supervised learning.*

\-*Supervised learning* uses *labeled data*.

Is it possible to input/use this *labeled data* in a *Baum-Welch*, *hidden markov model*, to observe and thereby estimate the *most probable* value for the *next* *data point* in our dataset?

I have so far only seen Baum-Welch algorithms that uses *unlabeled data* as its *states*, is it possible to instead use *labeled data*? If so, *how,* and by what *means*?",1,1,False,self,,,,,
362,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,6,anvmbj,coderecipe.ai,"[Discussion] I created a diagram to share how to use AWS Rekognition, does it look useful to you? What else do you hope to see?",https://www.reddit.com/r/MachineLearning/comments/anvmbj/discussion_i_created_a_diagram_to_share_how_to/,codingrecipe,1549487637,,0,1,False,default,,,,,
363,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,6,anvnjc,self.MachineLearning,PPO actor-critic style,https://www.reddit.com/r/MachineLearning/comments/anvnjc/ppo_actorcritic_style/,Ali7422626,1549487816,[removed],0,1,False,self,,,,,
364,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,6,anvo16,self.MachineLearning,Research topic,https://www.reddit.com/r/MachineLearning/comments/anvo16/research_topic/,Jerry-K7,1549487891,[removed],0,1,False,self,,,,,
365,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,6,anvsg1,self.MachineLearning,[D] What is the current state-of-the-art (as of February 2019) approachs for text classification?,https://www.reddit.com/r/MachineLearning/comments/anvsg1/d_what_is_the_current_stateoftheart_as_of/,MoBizziness,1549488516,"I've been on the search for new architectures to experiment with for a project I'm working on.

I was wondering what the current state of the art implementations are in this domain? 

Currently I've been experimenting with using neural networks primarily, using ELMO embeddings for character, word &amp; sentence level embeddings with stuff like attention networks, CNNs where the different channels are the different levels of token vectors (character/word/sentence etc.).

TLDR: What are the state-of-the-art architectures &amp; techniques for text classification?",9,1,False,self,,,,,
366,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,6,anvwgb,medium.com,On Compilers: First TVM and Deep Learning Conference,https://www.reddit.com/r/MachineLearning/comments/anvwgb/on_compilers_first_tvm_and_deep_learning/,Yuqing7,1549489090,,0,1,False,https://b.thumbs.redditmedia.com/auydtcQuyiyaxlSqbC5nZwTBXqZpSWn6Erv9qyY2sFA.jpg,,,,,
367,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,7,anwbe8,vas3k.com,Machine Learning for Everyone,https://www.reddit.com/r/MachineLearning/comments/anwbe8/machine_learning_for_everyone/,unab0mber,1549491281,,0,1,False,default,,,,,
368,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,7,anwixx,github.com,Why is the accuracy not improving at all (only a constant zero),https://www.reddit.com/r/MachineLearning/comments/anwixx/why_is_the_accuracy_not_improving_at_all_only_a/,jweir136,1549492370,,0,1,False,default,,,,,
369,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,7,anwscs,self.MachineLearning,[Project] Bunny Breed Resnet Classifier,https://www.reddit.com/r/MachineLearning/comments/anwscs/project_bunny_breed_resnet_classifier/,elemark,1549493788,[removed],0,1,False,self,,,,,
370,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,8,anxbzi,self.MachineLearning,"[D} Why do Batch Norm, why not optimize explicit for white (~N(0,1)) output?",https://www.reddit.com/r/MachineLearning/comments/anxbzi/d_why_do_batch_norm_why_not_optimize_explicit_for/,idg101,1549496913, Why do we do batch norm?  Why not simply do a statistical regularization which penalizes non-white outputs? ,0,1,False,self,,,,,
371,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,9,anxg4y,self.MachineLearning,[D] Alternatives to cuda on nvidia GPUs?,https://www.reddit.com/r/MachineLearning/comments/anxg4y/d_alternatives_to_cuda_on_nvidia_gpus/,observerc,1549497627,"So, installing cuda is an horrible PITA. I have succeeded to install it but only with much struggle. The documentation is out of date for some combination of OS/graphical-card requiring manual tweaks with are not always obvious. And in most cases requires rather specific things like loading a specific kernel version or installing some old unsuported version of a library.

&amp;#x200B;

I haven't tried them, but plaidml and rocm appear to be much simpler to install. Do these generally work with nvidia cards? Let's take the nvidia 1080 as an example? Can I easily get rocm (hiptensorflow) or plaidml to work with it? Or will it require a bunch of hacks/tweaks?

&amp;#x200B;

Are there other alternatives that are easy to install for nvidia GPUs? I would like something that I could get to work in a couple of minutes on a fresh linux server (no desktop environment) install.

&amp;#x200B;

Performance within the same order of maginitude would be acceptable... say 70% of cuda or so.

&amp;#x200B;

Thank you",8,1,False,self,,,,,
372,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,9,anxm9u,self.MachineLearning,"[D] Why do Batch Norm, why not optimize explicitly for white (~N(0,1)) output?",https://www.reddit.com/r/MachineLearning/comments/anxm9u/d_why_do_batch_norm_why_not_optimize_explicitly/,idg101,1549498641, Why do we do batch norm?  Why not simply do a statistical regularization which penalizes non-white outputs? ,31,1,False,self,,,,,
373,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,9,anxnwd,self.MachineLearning,"If you were an NFL General Manager, what data would resonate with you most?",https://www.reddit.com/r/MachineLearning/comments/anxnwd/if_you_were_an_nfl_general_manager_what_data/,menodialogues,1549498947,,0,1,False,self,,,,,
374,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,10,any68m,self.MachineLearning,StyleGAN in reverse -- image to latent vector. Is it possible?,https://www.reddit.com/r/MachineLearning/comments/any68m/stylegan_in_reverse_image_to_latent_vector_is_it/,VerTiGo_Etrex,1549502158,[removed],0,1,False,self,,,,,
375,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,11,anyz2m,i.redd.it,Could have been an amazing Easter Egg for NLP Folks from Google(Gboard),https://www.reddit.com/r/MachineLearning/comments/anyz2m/could_have_been_an_amazing_easter_egg_for_nlp/,saikpr,1549507383,,0,1,False,default,,,,,
376,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,12,anz63d,arxiv.org,[R] Robustness Certificates Against Adversarial Examples for ReLU Networks,https://www.reddit.com/r/MachineLearning/comments/anz63d/r_robustness_certificates_against_adversarial/,singlasahil14,1549508626,,1,1,False,default,,,,,
377,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,12,anzgpq,self.MachineLearning,Titan RTX vs Two 2080 ti Deep Learning,https://www.reddit.com/r/MachineLearning/comments/anzgpq/titan_rtx_vs_two_2080_ti_deep_learning/,Aklenar,1549510552,[removed],0,1,False,self,,,,,
378,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,12,anzi1t,self.MachineLearning,"[D] StyleGAN, but in reverse. Is it possible?",https://www.reddit.com/r/MachineLearning/comments/anzi1t/d_stylegan_but_in_reverse_is_it_possible/,VerTiGo_Etrex,1549510803,"I discovered https://github.com/NVlabs/stylegan today, and was wondering if it's possible to train a network that outputs the optimal StyleGAN vector given an image as input.

Disclaimer: I did some work for Snap on boosted decision trees for ad and lens ranking, but my background is in infrastructure and app dev, and I haven't ever really touched CNNs. Looking to learn more, hence my question.

It seems that one could train a network similar to the way CycleGANs or autoencoders are trained. Slap a CNN on the front whose outputs encode down to a 512 length vector (the input size of StyleGAN,) and then feed that vector into StyleGAN to get an output image. The network would minimize the distance between the input image, and the generated StyleGAN image, and in doing so, would learn the weights required to convert an image into a StyleGAN encoding.

This would allow us to do fun things like interpolate between two input faces, or blend the traits of multiple faces into one face.

Producing training examples would be trivial. Simply feed a random vector into StyleGAN, and use the output as the training image.

If this seems feasible, please tell me. I would love to take a crack at it.",13,1,False,self,,,,,
379,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,13,anzqvt,self.MachineLearning,Hierarchical clustering evaluation,https://www.reddit.com/r/MachineLearning/comments/anzqvt/hierarchical_clustering_evaluation/,br_anonbinary,1549512494,[removed],0,1,False,self,,,,,
380,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,13,anzxc9,self.MachineLearning,[D] Hierarchical clustering evaluation,https://www.reddit.com/r/MachineLearning/comments/anzxc9/d_hierarchical_clustering_evaluation/,br_anonbinary,1549513733,"In my research I need to organize a dataset hierarchically (groups and subgroups) a set of objects. Each object has a feature vector.

&amp;#x200B;

I ran a hierarchical clustering (eg bisecking k-means) in the dataset and now I need to evaluate the quality.

&amp;#x200B;

My first idea is to use a strategy based on ""clustering stability"":

&amp;#x200B;

1. Split the dataset randomly into two subsets A and B
2. H1 = hierarchical\_clustering (A)
3. Use H1 as a ""training set"" for a hierarchical classifier and test on B. Use hierarchical\_clustering (H2) as a ""ground truth"" to calculate F1-Measure of the hierarchical classifier result.
4. Repeat 1 to 3 to get the average F1-Measure.

&amp;#x200B;

It is still a relative clustering validation strategy.

Will high values of F1-Score necessarily indicate a high quality of hierarchical clustering?",5,1,False,self,,,,,
381,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,13,ao03z7,arxiv.org,[R] Ambitious Data Science Can Be Painless,https://www.reddit.com/r/MachineLearning/comments/ao03z7/r_ambitious_data_science_can_be_painless/,milaworld,1549515049,,3,1,False,default,,,,,
382,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,14,ao0kjj,linkedin.com,"Blockchain, AI and Big Data Bringing Next Tech Revolution",https://www.reddit.com/r/MachineLearning/comments/ao0kjj/blockchain_ai_and_big_data_bringing_next_tech/,Anubhav-Singh,1549518376,,0,1,False,https://b.thumbs.redditmedia.com/0A2_fD_pZLrY9-CZAc3FariYUueienIg-c1Mhgz64vM.jpg,,,,,
383,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,15,ao140i,self.MachineLearning,Industrial IoT Solutions for centrifugal pump problems,https://www.reddit.com/r/MachineLearning/comments/ao140i/industrial_iot_solutions_for_centrifugal_pump/,gowitek-,1549522793,[removed],0,1,False,self,,,,,
384,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,16,ao18yi,self.MachineLearning,How to implement a disease prediction system based on the symptoms inputted?,https://www.reddit.com/r/MachineLearning/comments/ao18yi/how_to_implement_a_disease_prediction_system/,tnarkiv17,1549524002,[removed],0,1,False,self,,,,,
385,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,16,ao1cib,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/ao1cib/global_machine_learning_market_size_outlook/,SingleMoney7,1549524907,[removed],0,1,False,self,,,,,
386,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,16,ao1e4b,self.MachineLearning,How to implement Replay Buffers for Reinforcement Learning when most experiences give zero reward?,https://www.reddit.com/r/MachineLearning/comments/ao1e4b/how_to_implement_replay_buffers_for_reinforcement/,qudcjf7928,1549525334,[removed],0,1,False,self,,,,,
387,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,18,ao2023,self.MachineLearning,[P] Gen Studio,https://www.reddit.com/r/MachineLearning/comments/ao2023/p_gen_studio/,chisai_mikan,1549531470,"https://gen.studio

A new project from the Metropolitan Museum of Art. It's basically a GAN that has been trained to create artworks from the Met's collection. Main idea is for each object at the MET, we can:

- find a corresponding object in GAN latent space 
- traverse latent space between objects to discover artworks that could have been made, but never were

A writeup on the MET [blog](https://www.metmuseum.org/blogs/now-at-the-met/2019/artificial-intelligence-machine-learning-art-authorship)

Interactive project site: https://gen.studio
",1,1,False,self,,,,,
388,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,18,ao23cp,self.MachineLearning,[P] How to use BERT in Kaggle Competitions - A tutorial on fine-tuning and model adaptations,https://www.reddit.com/r/MachineLearning/comments/ao23cp/p_how_to_use_bert_in_kaggle_competitions_a/,Thomjazz,1549532348,"A step-by-step tutorial on how to adapt and finetune BERT for a Kaggle Challenge classification task: The [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).

This post covers pretty much everything from data processing to model modifications with code examples for each part.

Results are in the top-10% of this $35.000 competition with a single model modified in 30 lines of code.

Link: [https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)",18,1,False,self,,,,,
389,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,18,ao24qz,self.MachineLearning,How to build AutoML from scratch  Alexander Mamaev  Medium,https://www.reddit.com/r/MachineLearning/comments/ao24qz/how_to_build_automl_from_scratch_alexander_mamaev/,alxmamaev,1549532723,[removed],0,1,False,self,,,,,
390,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,19,ao290h,realtimecrm.co.uk,[D] How we went from humans to machine learning to make the impossible into reality,https://www.reddit.com/r/MachineLearning/comments/ao290h/d_how_we_went_from_humans_to_machine_learning_to/,Mattrt123,1549533824,,0,1,False,default,,,,,
391,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,19,ao2b13,blog.ntrlab.com,Roadshow! Meet NT Robotics and its amazing fully autonomous indoor Drone.,https://www.reddit.com/r/MachineLearning/comments/ao2b13/roadshow_meet_nt_robotics_and_its_amazing_fully/,Batareika_1,1549534334,,0,1,False,https://b.thumbs.redditmedia.com/GAupLB4pj1TdQEXIhJlZ8eDOY3BotX3J1fvVWujzngc.jpg,,,,,
392,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,19,ao2gq6,self.MachineLearning,machine learning training in bangalore,https://www.reddit.com/r/MachineLearning/comments/ao2gq6/machine_learning_training_in_bangalore/,archanaangel,1549535860,[removed],1,1,False,self,,,,,
393,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,19,ao2hm2,self.artificial,Making the most of your genome with Genetic Chatbot? Are we there yet?,https://www.reddit.com/r/MachineLearning/comments/ao2hm2/making_the_most_of_your_genome_with_genetic/,mygenefax,1549536085,,0,1,False,default,,,,,
394,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,19,ao2jeh,self.MachineLearning,[D] How to make use of part-of-speech tags?,https://www.reddit.com/r/MachineLearning/comments/ao2jeh/d_how_to_make_use_of_partofspeech_tags/,speedcell4,1549536547,"the par-to-speech tags are annotated in my corpus for every sentence every word, how can I make use of that information for the downstream task, i.e. text classification, sequential tagging? My ideas are here,

1. just randomly initialize another embedding matrix for POS tags, and concatenate these vectors with the origin word vectors. $\[w\_i, p\_i\]$
2. if the tag set of POS is small, then randomly initialize a linear layer for each POS, and feed words to their corresponding layers. $W\_{p\_{w\_i}} w\_i + b\_{p\_{w\_i}}$",9,1,False,self,,,,,
395,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,19,ao2l8s,clomedia.com,Machine Learning to enhance the learning experience of professionals,https://www.reddit.com/r/MachineLearning/comments/ao2l8s/machine_learning_to_enhance_the_learning/,gaurav0120,1549537035,,0,1,False,default,,,,,
396,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao2nbc,linkedin.com,Can My Business Achieve Optimal Analytics Without Hiring Dozens of Data Scientists?,https://www.reddit.com/r/MachineLearning/comments/ao2nbc/can_my_business_achieve_optimal_analytics_without/,ElegantMicroWebIndia,1549537554,,0,1,False,default,,,,,
397,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao2rmu,arxiv.org,[1902.02095] Space Navigator: a Tool for the Optimization of Collision Avoidance Maneuvers,https://www.reddit.com/r/MachineLearning/comments/ao2rmu/190202095_space_navigator_a_tool_for_the/,ihaphleas,1549538620,,1,1,False,default,,,,,
398,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao2so2,arxiv.org,[1902.02322] Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?,https://www.reddit.com/r/MachineLearning/comments/ao2so2/190202322_is_ami_attacks_meet_interpretability/,ihaphleas,1549538887,,9,1,False,default,,,,,
399,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao2szd,arxiv.org,[1902.01941] Market Manipulation of Bitcoin: Evidence from Mining the Mt. Gox Transaction Network,https://www.reddit.com/r/MachineLearning/comments/ao2szd/190201941_market_manipulation_of_bitcoin_evidence/,ihaphleas,1549538964,,1,1,False,default,,,,,
400,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao2us6,arxiv.org,[1902.01894] A Generalized Framework for Population Based Training,https://www.reddit.com/r/MachineLearning/comments/ao2us6/190201894_a_generalized_framework_for_population/,ihaphleas,1549539386,,1,1,False,default,,,,,
401,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao2uvj,arxiv.org,[1902.01909] Adaptive Stress Testing for Autonomous Vehicles,https://www.reddit.com/r/MachineLearning/comments/ao2uvj/190201909_adaptive_stress_testing_for_autonomous/,ihaphleas,1549539411,,1,1,False,default,,,,,
402,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao300m,prakhartechviz.blogspot.com,"[D] Faster pandas, even on your laptop",https://www.reddit.com/r/MachineLearning/comments/ao300m/d_faster_pandas_even_on_your_laptop/,_quanttrader_,1549540617,,0,1,False,default,,,,,
403,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,20,ao308v,self.MachineLearning,What are the best websites and blogs for following up the state of the art ML research?,https://www.reddit.com/r/MachineLearning/comments/ao308v/what_are_the_best_websites_and_blogs_for/,van_ozy,1549540677,[removed],0,1,False,self,,,,,
404,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,21,ao31yb,arxiv.org,"[1902.01917] Same, Same But Different - Recovering Neural Network Quantization Error Through Weight Factorization",https://www.reddit.com/r/MachineLearning/comments/ao31yb/190201917_same_same_but_different_recovering/,DruishDude,1549541022,,1,1,False,default,,,,,
405,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,21,ao380a,self.MachineLearning,What is the latest scope of research in applying Data Science for genomics/proteonomics and drug discovery?,https://www.reddit.com/r/MachineLearning/comments/ao380a/what_is_the_latest_scope_of_research_in_applying/,mischief_23,1549542400,[removed],0,1,False,self,,,,,
406,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,21,ao3f4v,self.MachineLearning,Do you know the music like this about car learning and deep learning and artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/ao3f4v/do_you_know_the_music_like_this_about_car/,Doctor_who1,1549543975,[removed],0,1,False,self,,,,,
407,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,22,ao3hwr,self.MachineLearning,[P] Question about document classification with unspecified number of images,https://www.reddit.com/r/MachineLearning/comments/ao3hwr/p_question_about_document_classification_with/,Pawnbrake,1549544525,"There are plenty of algorithms for [document classification](https://en.wikipedia.org/wiki/Document_classification).  Likewise, there are plenty of good image classification algorithms, and there is a clear standout with [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) outperforming nearly anything else.   
 However, I am working on a task that requires a hybrid of the two fields, and I am unsure how to merge the two.  

&amp;#x200B;

For my project, I am classifying documents.  I have 300 or so categories, and a document may belong to multiple categories at once (no more than 5).  These documents contain text and an unspecified number of images - each document has a different number of images, and some may have zero.  In order to have high accuracy in document classification, I am required to process these images.  For the purposes of my project, assume that it is trivial to parse out the images and text from the documents.  

&amp;#x200B;

Is there any research on how to use unspecified number of images as well as text in document classification?  I spent some time researching and I couldn't find anything too useful.  My project is experimental so I'm interested in trying a variety of algorithms, so I would try something computationally intensive such as using perhaps a hybrid of [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) and CNN, and other computationally simple algorithms.  ",3,1,False,self,,,,,
408,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,22,ao3jg0,self.MachineLearning,"Searching for resources to practice ML, beginner level",https://www.reddit.com/r/MachineLearning/comments/ao3jg0/searching_for_resources_to_practice_ml_beginner/,hungarywolf,1549544814,[removed],0,1,False,self,,,,,
409,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,22,ao3o7w,self.MachineLearning,[D] Best AI news in January?,https://www.reddit.com/r/MachineLearning/comments/ao3o7w/d_best_ai_news_in_january/,antmoreau,1549545748,"I wrote an article to help anyone catch up with the latest Machine Learning news.

[https://blog.sicara.com/01-2019-best-ai-new-articles-this-month-8e2113fbd17b](https://blog.sicara.com/01-2019-best-ai-new-articles-this-month-8e2113fbd17b)

Anything I missed? I would really like some feedback on this as it is quite hard to keep up with the field even when you're actively monitoring it.",1,1,False,self,,,,,
410,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,22,ao3qwg,self.MachineLearning,[P] Warcraft 3 Cinematic (Human Ending) enhanced using neural networks V1,https://www.reddit.com/r/MachineLearning/comments/ao3qwg/p_warcraft_3_cinematic_human_ending_enhanced/,DagothHertil,1549546273,"Video with NN enhancment: [https://youtu.be/1IlHcVM\_IHw](https://youtu.be/1IlHcVM_IHw)

Video with side-dy-side comparison (new on the left, old on the right): [https://youtu.be/2Dfj30rvAV0](https://youtu.be/2Dfj30rvAV0) 

Video with old and new cinematic at the same time (new on the top, old on the bottom): [https://youtu.be/UjJlKUa5GRI](https://youtu.be/UjJlKUa5GRI)

For lovers of the russian language here is the enhanced video with russian voiceover: [https://youtu.be/05HojCkXX1I](https://youtu.be/05HojCkXX1I)

(please view in 1080p60fps)

&amp;#x200B;

So I've being working on this project for quite some time now, my aim is to improve the video quality of old low resolution low fps videos using compurter vision and neural networks. For now the full list of stuff I use:

OpenCV and C++,

[https://github.com/tensorlayer/srgan](https://github.com/tensorlayer/srgan)

[https://github.com/sniklaus/pytorch-s](https://github.com/sniklaus/pytorch-s)...

[https://github.com/jiangsutx/SRN-Deblur](https://github.com/jiangsutx/SRN-Deblur)

&amp;#x200B;

This is the first version and there is still a lot to improve (like the random jitters on the right and overall blurriness of the video). As the first step I am going to move from SRN-Deblur to DeblurGAN implementation.

&amp;#x200B;

Hope you enjoy what you see :) ",11,1,False,self,,,,,
411,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao41pm,self.MachineLearning,Workstation for ML and scientic computing,https://www.reddit.com/r/MachineLearning/comments/ao41pm/workstation_for_ml_and_scientic_computing/,Aixas123,1549548294,"Hey folks,

I have been working for quite some time now on ML and different kinds of scientific computing. I have been working on huge clusters and things, but I wanted to build my own small workstation. Yet, I built a lot of pcs for friends, not a single server and thus do not know what to aim for.

Do any of you have some experience with that and can tell me how it worked out? And is a server better for medium sized problems than a normal PC? I really don't know. Please send nu.. Help :) ",0,1,False,self,,,,,
412,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao42us,self.MachineLearning,"[D] What statistical methods exist to compare the performance of different classifiers (e.g., Friedman-Nemenyi test)?",https://www.reddit.com/r/MachineLearning/comments/ao42us/d_what_statistical_methods_exist_to_compare_the/,carryonbag78,1549548503,"Not talking about AUROC, precision, recall, or F1 score here, but methods to COMPARE these measures across classifiers, an example being the Friedman-Nemenyi test. What other methods are there, and when/how should they be used? ",3,1,False,self,,,,,
413,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao45wz,youtube.com,Engines forced to play romantically | Stockfish reacts brilliantly to Le...,https://www.reddit.com/r/MachineLearning/comments/ao45wz/engines_forced_to_play_romantically_stockfish/,kingscrusher-youtube,1549549034,,0,1,False,default,,,,,
414,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao46oo,get.valohai.com,Webinar: Synthetic Data Generation in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ao46oo/webinar_synthetic_data_generation_in_machine/,Valohai,1549549174,,0,1,False,default,,,,,
415,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao46rm,self.MachineLearning,Classifying unbalanced data,https://www.reddit.com/r/MachineLearning/comments/ao46rm/classifying_unbalanced_data/,badmrsynth,1549549190,[removed],0,1,False,self,,,,,
416,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao4ac8,self.MachineLearning,"Top 5 podcasts for data science, machine learning, statistics and AI",https://www.reddit.com/r/MachineLearning/comments/ao4ac8/top_5_podcasts_for_data_science_machine_learning/,Mayalittlepony,1549549835,[removed],0,1,False,self,,,,,
417,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao4b4e,self.MachineLearning,[News] Stockfish top Alpha-Beta Engine shows dominance still tactically vs top Neural Network Leela Chess,https://www.reddit.com/r/MachineLearning/comments/ao4b4e/news_stockfish_top_alphabeta_engine_shows/,kingscrusher-youtube,1549549973,[removed],0,1,False,self,,,,,
418,MachineLearning,t5_2r3gv,2019-2-7,2019,2,7,23,ao4g5o,self.MachineLearning,Stockfish top Alpha-Beta Engine shows dominance still tactically vs top Neural Network Leela Chess,https://www.reddit.com/r/MachineLearning/comments/ao4g5o/stockfish_top_alphabeta_engine_shows_dominance/,kingscrusher-youtube,1549550885,[removed],0,1,False,self,,,,,
419,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,1,ao5e8m,self.MachineLearning,[R][1902.00577] Robustness of Generalized Learning Vector Quantization Models against Adversarial Attacks,https://www.reddit.com/r/MachineLearning/comments/ao5e8m/r190200577_robustness_of_generalized_learning/,LarsHoldijk,1549556433,"For the application of machine learning techniques in security critical applications, such as autonomous driving, the potential threat of adversarial attacks is a very serious issue. Luckily, a lot of research is currently being done on the existence of these attacks and in improving the robustness of models against them. The research in this area has so far primarily focused on neural networks, with a few exceptions of course. We however believe that studying adversarial attacks and model robustness in the context of other machine learning methods, such as Learning Vector Quantization, could be a valuable source of knowledge. 

For this reason, we have performed a first evaluation of the robustness of Generalized Learning Vector Quantization models against adversarial attacks. We would love to get your feedback on our evaluation and welcome you to ask some questions.

We would love to get your feedback on our work and answer questions.

**Abstract:**
Adversarial attacks and the development of (deep) neural networks robust against them are currently two widely researched topics. The robustness of Learning Vector Quantization (LVQ) models against adversarial attacks has however not yet been studied to the same extend. We therefore present an extensive evaluation of three LVQ models: Generalized LVQ, Generalized Matrix LVQ and Generalized Tangent LVQ. The evaluation suggests that both Generalized LVQ and Generalized Tangent LVQ have a high base robustness, on par with the current state-of-the-art in robust neural network methods. In contrast to this, Generalized Matrix LVQ shows a high susceptibility to adversarial attacks, scoring consistently behind all other models. Additionally, our numerical evaluation indicates that increasing the number of prototypes per class improves the robustness of the models.

**Arxiv-link:**
https://arxiv.org/abs/1902.00577

**Summarizing Twitter thread:**
https://twitter.com/HoldijkLars/status/1093475731345207296",0,1,False,self,,,,,
420,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,1,ao5gis,heartbeat.fritz.ai,Using coremltools to Convert a Keras model to Core ML for iOS,https://www.reddit.com/r/MachineLearning/comments/ao5gis/using_coremltools_to_convert_a_keras_model_to/,gonzalezcgg11,1549556805,,0,1,False,default,,,,,
421,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,1,ao5gzi,self.MachineLearning,16 months to prepare for a Masters application in ML: what to do?,https://www.reddit.com/r/MachineLearning/comments/ao5gzi/16_months_to_prepare_for_a_masters_application_in/,GlassSculpture,1549556886,"Hi all,

(A big thank you in advance for any advice).

I wanted to ask for some advice in how I can best use the next 16 months or so to (full-time) prepare myself for an application to a MSc in Machine Learning (to start in the Fall of 2020). First some background. I'm 27 and graduated in 2013 with a MSc in Economics from a (I think) top ranking university, with good grades. Since then my work has been fairly commercial in nature and although I have done a lot of rough Python programming, I haven't done anything really academic in nature.

I've always been interested in the intersection of computer science and statistics and having read about machine learning for me it sounds like what I want to do with my life. I want to study the MSc to either go into industry after it or try to continue to a PhD if I enjoy research.

Now for the advice part: I'm aware that my profile and academic knowledge probably aren't quite good enough to get into a good masters program right now. I have no formal computer science background and an academic black hole in my CV. I live in London and the CSML program at UCL would be one of my top choices, but I'd also be keen on applying to similar programmes in the US. I've saved up enough money to be able to dedicate my time to preparing myself for a MSc application and was wondering what the best use of my time might be. Some things I'd thought of:

* Self-study the OSSU computer science curriculum ( [https://github.com/ossu/computer-science/blob/dev/README.md](https://github.com/ossu/computer-science/blob/dev/README.md) )
* Self-study some math textbooks
* Complete a project and put it on Git
* Do some sort of 12 month course online via distance learning as preparation (maybe in Computer Science)

I think a formal course via distance learning would look better than self-study but I can't seem to find a suitable course that isn't a Masters degree in itself.

Any suggestions would be very much appreciated!

&amp;#x200B;",0,1,False,self,,,,,
422,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,1,ao5jfn,self.MachineLearning,[R] Classification with Costly Features using Deep Reinforcement Learning | AAAI19,https://www.reddit.com/r/MachineLearning/comments/ao5jfn/r_classification_with_costly_features_using_deep/,jaromiru,1549557267,"Practical use of RL is still scarce, hence I'd like to point you to my recent research. You can read the [blog post](https://jaromiru.com/2019/02/07/hands-on-classification-with-costly-features/) or jump on the [paper](https://jaromiru.com/media/about/aaai19_cwcf_paper.pdf) itself.

&gt;**Abstract:** We study a classification problem where each feature can be acquired for a cost and the goal is to optimize a trade-off between the expected classification error and the feature cost. We revisit a former approach that has framed the problem as a sequential decision-making problem and solved it by Q-learning with a linear approximation, where individual actions are either requests for feature values or terminate the episode by providing a classification decision. On a set of eight problems, we demonstrate that by replacing the linear approximation with neural networks the approach becomes comparable to the state-of-the-art algorithms developed specifically for this problem. The approach is flexible, as it can be improved with any new reinforcement learning enhancement, it allows inclusion of pre-trained high-performance classifier, and unlike prior art, its performance is robust across all evaluated datasets.",0,1,False,self,,,,,
423,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,1,ao5l35,v.redd.it,We should put this kid in charge of Ethics at a self driving car firm,https://www.reddit.com/r/MachineLearning/comments/ao5l35/we_should_put_this_kid_in_charge_of_ethics_at_a/,its-trivial,1549557511,,0,1,False,https://b.thumbs.redditmedia.com/dY9PfrJLUxhkH6O9Gs7LWEu171eqNRaCyITb2QZEJno.jpg,,,,,
424,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,2,ao5ytf,medium.com,Uber AIs Go-Explore Tackles Hard-Exploration Problems,https://www.reddit.com/r/MachineLearning/comments/ao5ytf/uber_ais_goexplore_tackles_hardexploration/,Yuqing7,1549559570,,0,1,False,default,,,,,
425,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,2,ao5z34,self.MachineLearning,Where does Inference Latency matter the most?,https://www.reddit.com/r/MachineLearning/comments/ao5z34/where_does_inference_latency_matter_the_most/,sehaji1869,1549559607,[removed],0,1,False,self,,,,,
426,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,2,ao5zci,self.MachineLearning,[P] Distributing on-device models based on hardware capabilities.,https://www.reddit.com/r/MachineLearning/comments/ao5zci/p_distributing_ondevice_models_based_on_hardware/,jamesonatfritz,1549559645,"Deploying models for on-device inference can be difficult given the fragmentation in hardware and huge performance disparities. An iPhone 6 might be 20x slower than an iPhone XS. I've seen a posters ask how to manage model distribution in these environments. We've developed tools to help ensure the right model gets on the right device.

[Here's an example for how you'd do this in an iOS app.](https://heartbeat.fritz.ai/distributing-on-device-machine-learning-models-with-tags-and-metadata-ccae40f5c059)

A summary of how it works:

1. Create multiple versions of your model trading off between on-device speed and accuracy. For example, MobileNet back bones are parameterized with a width parameter that lets you adjust the number of parameters in the model.
2. Multiple versions of models are uploaded to our service and tagged based on their size, performance, or accuracy. Think ""small"", ""medium"", ""large"".
3. In your on-device application code, the client looks at the device it's running on, the available processing power, and decides which size model to use.
4. Models are then pulled down to the client and loaded dynamically.

I'd love any feedback or to hear about other tools people use to solve this problem.

&amp;#x200B;",0,1,False,self,,,,,
427,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,2,ao5zwx,self.MachineLearning,[D] Changes in neural network weights with respect to affine transformations of input,https://www.reddit.com/r/MachineLearning/comments/ao5zwx/d_changes_in_neural_network_weights_with_respect/,Minimum_Zucchini,1549559734,"I'm curios to learn more about this and if it has been properly studied and if so, what some theoretical results are. I wasn't able to find papers but probably because I'm using the wrong terminology so please bear with me.

An example, let's say we build a CNN image classifier of cats vs. dogs. After we are done training, lets say I pass in a transformed image from the training set into the neural network. If we take 'feature space' to mean the weights on the final fully connected layer, how close are the weights in the feature space between the original image and the affinely transformed image? Do affine transformations of inputs cause unpredictable drastic changes in the 'feature space' or is there some sort of relationship we can establish? 


The reason I'm asking is because I read that one way of increasing data set sizes and generalization power of a neural network (at least for image classifiers) is to apply transformations to your existing data (scaling, zooming, rotating, etc). This makes sense intuitively. For humans we usually only have to see one or two examples of something before we can recognize it in any arbitrary orientation, scale etc. Of course there are potentially an infinite number of affine transformations one can apply to their data set, but this would increase training time significantly. But if we already have some idea of how these transformations will affect the neural network weights, then maybe we can somehow skip the training on the augmented data set..? I don't know.


I'm currently a masters student and my work is more so in applied machine learning but I'd like to have a discussion about this and/or if someone can point me to relevant research papers that would be great!",10,1,False,self,,,,,
428,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,2,ao69yh,self.MachineLearning,[D] Neural net vs SUR in time series,https://www.reddit.com/r/MachineLearning/comments/ao69yh/d_neural_net_vs_sur_in_time_series/,QuadraticCowboy,1549561285,"I cant see a difference in a neural net vs Seemingly Un-Related (SUR) time series.  If the training matrix is optimally designed and cleaned, you are effectively working with the same vector space.  

The only difference is how you model the system and train it.  SUR is a huge pain in the ass to clean and train, NN is fast.  

Conversely, NN is a black box, but SUR is extremely transparent.  Needs a source, but dont economists, the fed, and traditional hedge funds prefer transparency and use those models, but quant funds and tech firms prefer NN?",4,1,False,self,,,,,
429,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,3,ao6jmh,self.MachineLearning,Feasible to build a recommendation engine for a website?,https://www.reddit.com/r/MachineLearning/comments/ao6jmh/feasible_to_build_a_recommendation_engine_for_a/,monkeyunited,1549562755,"I apologize if this is not the place for such question. I'm looking for directions on building a recommendation engine for someone else's website.

I buy Chinese literature regularly from a website that ships internationally. However the website is very rudimentary and takes forever to look for books in the same category or even by the same author.

Is there a way I can:

1. scrape their entire stock of books (how?)
2. label them myself
3. build a recommendation engine (and it can be very simple such as recommend best seller in same category, or even simply same author)

As a bonus, maybe present the prototype to the website so at least some form of recommendation is available on the website (which I can then use). Is this a feasible project and where can I find step-by-step guide for this? ",0,1,False,self,,,,,
430,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,3,ao6mth,hackernoon.com,"Superalgos, Part One: The Trading Singularity  Hacker Noon - One day in the future, a trading intelligence capable of outperforming every other entity at the markets will emerge. Both humans and current algorithms will be surpassed by Superalgos.",https://www.reddit.com/r/MachineLearning/comments/ao6mth/superalgos_part_one_the_trading_singularity/,cienciaslocas,1549563222,,0,1,False,https://a.thumbs.redditmedia.com/4VyXimmV3N3WONggsiUR7jLqczW7LuEk_EfLIfZvbw8.jpg,,,,,
431,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,3,ao6wlm,self.MachineLearning,On Politically Charged Social Issues in Our Maths(Or a response to Gendered Pronoun Resolution): From my Kaggle post on the same subject,https://www.reddit.com/r/MachineLearning/comments/ao6wlm/on_politically_charged_social_issues_in_our/,hisairnessag3,1549564670,"Is this really the type of competition that is getting promoted to general Kagglers? While I would consider myself center-left, this feels very divisive(I received an email promoting this comp. in particular).

The competition(seen here: [https://www.kaggle.com/c/gendered-pronoun-resolution),](https://www.kaggle.com/c/gendered-pronoun-resolution),) while pure intentioned, is advertising optimization for a problem that is *not yet solved*. Namely, how to integrate people who do not resonate with either binary gender. To push the idea that an arbitrary number of pronouns should apply to the general population *is not tenable*.

I urge Kaggle to keep contentious social issues out of our math and out of our data, especially when it is based on a rocky presupposition. I thus urge teams to boycott this competition.

Thank you, I hope to keep an open conversation about this issue.  


Additional rambling for reddit: 

  
The Greeks and other ancient cultures(and more recent) had this solved long ago. Assuming an answer for a problem, without scientifically deriving said answer, then optimizing based on the presumption has rarely served anyone well. ",0,1,False,self,,,,,
432,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,3,ao6zbx,self.MachineLearning,what is fuzzy support vector machine?,https://www.reddit.com/r/MachineLearning/comments/ao6zbx/what_is_fuzzy_support_vector_machine/,hoda_fakharzade,1549565078,[removed],0,1,False,self,,,,,
433,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,4,ao7jx1,jesusverduzco.soy,"Excellent post in Spanish about, Hire a data analyst or your business won't grow",https://www.reddit.com/r/MachineLearning/comments/ao7jx1/excellent_post_in_spanish_about_hire_a_data/,Alexp9108,1549568115,,0,1,False,default,,,,,
434,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,5,ao80t1,amarkets.com,Forex broker,https://www.reddit.com/r/MachineLearning/comments/ao80t1/forex_broker/,ephrembroomhead,1549570512,,0,1,False,default,,,,,
435,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,5,ao8107,actuia.com,"[D] Interview with Luc Julia : ""Artificial Intelligence doesn't exist",https://www.reddit.com/r/MachineLearning/comments/ao8107/d_interview_with_luc_julia_artificial/,actuia,1549570539,,0,1,False,https://b.thumbs.redditmedia.com/DCwZhdAtyghPxOTVmHSLPB2c70xmgLi3R1f79FnCYKY.jpg,,,,,
436,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,5,ao81d0,self.MachineLearning,[P] HD Video Style Transfer Using Star Wars Posters,https://www.reddit.com/r/MachineLearning/comments/ao81d0/p_hd_video_style_transfer_using_star_wars_posters/,dh27182,1549570592,"Style Transfer is nothing new under the sun, most of you probably know how that works. Yet I haven't seen it used in ""pop culture"" much to this date (apart from like 5s of Kirsten Stewart's [short movie](https://arxiv.org/abs/1701.04928)).

In collaboration with a band called Exit Empire, we created an official music video for one of their songs using style transfer. Now, the model is pretty straight forward, but the challenges arise when performing on video where you care about the final quality, e.g. find a balance so that consecutive frames are similar, inference on full HD video and rendering in lossless quality. I did a little write-up on the challenges we've encountered in anyone's interested: [https://danielhavir.github.io/notes/hd-style-transfer/](https://danielhavir.github.io/notes/hd-style-transfer/#video-challenges)",4,1,False,self,,,,,
437,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,5,ao85q7,actuia.com,"[D] Interview with Luc Julia : ""Artificial Intelligence doesn't exist""",https://www.reddit.com/r/MachineLearning/comments/ao85q7/d_interview_with_luc_julia_artificial/,actuia,1549571239,,0,1,False,https://b.thumbs.redditmedia.com/DCwZhdAtyghPxOTVmHSLPB2c70xmgLi3R1f79FnCYKY.jpg,,,,,
438,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,5,ao8fg7,producthunt.com,An AI that you can video chat with.,https://www.reddit.com/r/MachineLearning/comments/ao8fg7/an_ai_that_you_can_video_chat_with/,julien_c,1549572643,,0,1,False,default,,,,,
439,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,6,ao8p3a,self.MachineLearning,[P] A (weird) experimental AI project that let you Facetime an AI,https://www.reddit.com/r/MachineLearning/comments/ao8p3a/p_a_weird_experimental_ai_project_that_let_you/,Thomjazz,1549574041,"Voice-boloss is an experimental AI project that let you Facetime an AI (yes)

You can try it here: [https://www.producthunt.com/posts/voice-boloss](https://www.producthunt.com/posts/voice-boloss)

&amp;#x200B;

It's developed by the Huggingface team which has open-sourced/published many parts of the underlying tech, mostly involving rather advanced NLP tech. Here is a quick summary of the tech:

* The Natural Understanding pipeline is based on
   * a [Hierarchical Multi-Task Approach for Learning Embeddings from Semantic Tasks](https://arxiv.org/abs/1811.06031) (published in AAAI 2019), code open-sourced here: [https://github.com/huggingface/hmtl](https://github.com/huggingface/hmtl) and demo here: [https://huggingface.co/hmtl/](https://huggingface.co/hmtl/),
   * a [very Fast](https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced) [Coreference Resolution module](https://medium.com/huggingface/how-to-train-a-neural-coreference-model-neuralcoref-2-7bb30c1abdfe) (details in the links) using Neural Networks, code open-sourced here: [https://github.com/huggingface/neuralcoref](https://github.com/huggingface/neuralcoref), and
   * Sentence and words embeddings computed using Google AI's BERT model in production using the plug-in PyTorch implementation open-sourced here: [https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)
* The Natural Language Generation part involves several parallel systems, some being similar the work that won the automatic metrics track of the ConvAI2 challenge last month at NeurIPS. See this paper of last week (the code should come soon) for more information: [https://arxiv.org/abs/1901.08149](https://arxiv.org/abs/1901.08149)

Hope you like this experiment!",2,1,False,self,,,,,
440,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,6,ao94i2,arxiv.org,[1806.05236] Manifold Mixup: Better Representations by Interpolating Hidden States (New Version),https://www.reddit.com/r/MachineLearning/comments/ao94i2/180605236_manifold_mixup_better_representations/,alexmlamb,1549576358,,23,1,False,default,,,,,
441,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,6,ao96ec,self.MachineLearning,[D] 16 months to prepare for a Masters application in ML: what to do?,https://www.reddit.com/r/MachineLearning/comments/ao96ec/d_16_months_to_prepare_for_a_masters_application/,GlassSculpture,1549576656," Hi all,

(A big thank you in advance for any advice).

I wanted to ask for some advice in how I can best use the next 16 months or so to (full-time) prepare myself for an application to a MSc in Machine Learning (to start in the Fall of 2020). First some background. I'm 27 and graduated in 2013 with a MSc in Economics from a (I think) top ranking university, with good grades. Since then my work has been fairly commercial in nature and although I have done a lot of rough Python programming, I haven't done anything really academic in nature.

I've always been interested in the intersection of computer science and statistics and having read about machine learning for me it sounds like what I want to do with my life. I want to study the MSc to either go into industry after it or try to continue to a PhD if I enjoy research.

Now for the advice part: I'm aware that my profile and academic knowledge probably aren't quite good enough to get into a good masters program right now. I have no formal computer science background and an academic black hole in my CV. I live in London and the CSML program at UCL would be one of my top choices, but I'd also be keen on applying to similar programmes in the US. I've saved up enough money to be able to dedicate my time to preparing myself for a MSc application and was wondering what the best use of my time might be. Some things I'd thought of:

* Self-study the OSSU computer science curriculum ( [https://github.com/ossu/computer-science/blob/dev/README.md](https://github.com/ossu/computer-science/blob/dev/README.md) )
* Self-study some math textbooks
* Complete a project and put it on Git
* Do some sort of 12 month course online via distance learning as preparation (maybe in Computer Science)

I think a formal course via distance learning would look better than self-study but I can't seem to find a suitable course that isn't a Masters degree in itself.

Any suggestions would be very much appreciated!",52,1,False,self,,,,,
442,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,7,ao9h1e,vas3k.com,"[P] Machine Learning for Everyone. In simple words. With real-world examples. Yes, again.",https://www.reddit.com/r/MachineLearning/comments/ao9h1e/p_machine_learning_for_everyone_in_simple_words/,sneks,1549578311,,0,1,False,default,,,,,
443,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,7,ao9p83,self.MachineLearning,"[D] Whats harder for ML: driving a car, or playing StarCraft and Dota?",https://www.reddit.com/r/MachineLearning/comments/ao9p83/d_whats_harder_for_ml_driving_a_car_or_playing/,strangecosmos,1549579641,"Given the recent success of DeepMinds [AlphaStar](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/) at learning StarCraft, and [OpenAI Fives](https://blog.openai.com/the-international-2018-results/) success with Dota in the summer, I have become intensely curious about this question: 

_Is it harder for a neural network to learn to drive a car through imitation learning and/or reinforcement learning than to play StarCraft or Dota at an expert level?_

What do yall think? Its clearly easier for a human to get a drivers license to become an expert StarCraft or Dota player (i.e. top 1% of players globally), but the question is whats easier for a neural network to learn. ",12,1,False,self,,,,,
444,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,7,ao9pbe,self.MachineLearning,Best deep learning technique to identify player position on a minimap?,https://www.reddit.com/r/MachineLearning/comments/ao9pbe/best_deep_learning_technique_to_identify_player/,abhi91,1549579657,I am trying to build a model that can consume the minimap information from a stream of a game like Fortnite or Apex Legends and identify where in the world map the player currently is. Anyone have any advice on which techniques to use? ,0,1,False,self,,,,,
445,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,8,aoa8d6,kaggle.com,"What can I implement with this, my first kaggle exercise? Looking for your suggestions.",https://www.reddit.com/r/MachineLearning/comments/aoa8d6/what_can_i_implement_with_this_my_first_kaggle/,gecicihesap17,1549582798,,0,1,False,default,,,,,
446,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,9,aoagba,self.MachineLearning,[R] PROVEN: Certifying Robustness of Neural Networks with a Probabilistic Approach,https://www.reddit.com/r/MachineLearning/comments/aoagba/r_proven_certifying_robustness_of_neural_networks/,PresidentOfTacoTown,1549584142,"[Tsui-Wei Weng](https://arxiv.org/search/cs?searchtype=author&amp;query=Weng%2C+T), [Pin-Yu Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+P), [Lam M. Nguyen](https://arxiv.org/search/cs?searchtype=author&amp;query=Nguyen%2C+L+M), [Mark S. Squillante](https://arxiv.org/search/cs?searchtype=author&amp;query=Squillante%2C+M+S), [Ivan Oseledets](https://arxiv.org/search/cs?searchtype=author&amp;query=Oseledets%2C+I), [Luca Daniel](https://arxiv.org/search/cs?searchtype=author&amp;query=Daniel%2C+L)

&gt;With deep neural networks providing state-of-the-art machine learning models for numerous machine learning tasks, quantifying the robustness of these models has become an important area of research. However, most of the research literature merely focuses on the *worst-case* setting where the input of the neural network is perturbed with noises that are constrained within an p ball; and several algorithms have been proposed to compute certified lower bounds of minimum adversarial distortion based on such worst-case analysis. In this paper, we address these limitations and extend the approach to a *probabilistic* setting where the additive noises can follow a given distributional characterization. We propose a novel probabilistic framework PROVEN to **PRO**babilistically **VE**rify **N**eural networks with statistical guarantees -- i.e., PROVEN certifies the probability that the classifier's top-1 prediction cannot be altered under any constrained p norm perturbation to a given input. Importantly, we show that it is possible to derive closed-form probabilistic certificates based on current state-of-the-art neural network robustness verification frameworks. Hence, the probabilistic certificates provided by PROVEN come naturally and with almost no overhead when obtaining the worst-case certified lower bounds from existing methods such as Fast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR neural network models demonstrate our probabilistic approach can achieve up to around 75% improvement in the robustness certification with at least a 99.99% confidence compared with the worst-case robustness certificate delivered by CROWN.

[https://arxiv.org/pdf/1812.08329.pdf](https://arxiv.org/pdf/1812.08329.pdf)

&amp;#x200B;

Very interested in this paper, struggling to interpret the improvement they present. Would love some help understanding the work.",0,1,False,self,,,,,
447,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,10,aobdpu,self.MachineLearning,[D] Generative Adversarial Network producing same fake samples,https://www.reddit.com/r/MachineLearning/comments/aobdpu/d_generative_adversarial_network_producing_same/,jmarsha5,1549590134,Hey ya'll when a GAN is generating the exact same fake samples what could be the issue? We have tried changing the learning rate but we get the same results. We also tried different layers. Any ideas on how to go about figuring out what needs to be changed?,16,1,False,self,,,,,
448,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,11,aobv41,youtube.com,Forensic Video Analytic Software,https://www.reddit.com/r/MachineLearning/comments/aobv41/forensic_video_analytic_software/,antonjeran1234,1549593349,,0,1,False,https://b.thumbs.redditmedia.com/mGPtPs1EZsyuh5jmNapaq8pKDSgpf0Uthm_CU-O_D-o.jpg,,,,,
449,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,12,aocid2,self.MachineLearning,[R] TDLS paper presentation &amp; discussion: Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer (accepted by ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/aocid2/r_tdls_paper_presentation_discussion/,tdls_to,1549597794,"Recording: [https://youtu.be/Tu3FqCD7-BY?t=718](https://youtu.be/Tu3FqCD7-BY?t=718)

Paper: [https://openreview.net/forum?id=S1fQSiCcYm](https://openreview.net/forum?id=S1fQSiCcYm)

Takeaways:

* The paper provided in-depth perspective on interpolation in autoencoders.
* It proposed Adversarially Constrained Autoencoder Interpolation (ACAI).
* Proposed a synthetic benchmark and showed that ACAI substantially outperformed common autoencoder models.
* It studied the effect of improved interpolation on downstream tasks.
* It showed that ACAI led to improved performance for feature learning.

Discussion points:

* Does the technique help in getting better random samples? If the proposed regularizer is applied to a VAE, does it help in getting better random samples by decoding z?
* How does ACAI benefit models with non visual results?
* How would ACAI extend to more complicated data sets rather than toy models?
* Can similar technique be applied to VAE?",0,1,False,self,,,,,
450,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,13,aod0h5,arxiv.org,[R] Neural Inverse Knitting: From Images to Manufacturing Instructions,https://www.reddit.com/r/MachineLearning/comments/aod0h5/r_neural_inverse_knitting_from_images_to/,hardmaru,1549601447,,4,1,False,default,,,,,
451,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,13,aod0lq,arxiv.org,This paper might be interesting! Threshold Phasor Associative Memory.,https://www.reddit.com/r/MachineLearning/comments/aod0lq/this_paper_might_be_interesting_threshold_phasor/,mustafafayez,1549601469,,3,1,False,default,,,,,
452,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,14,aodgkd,self.MachineLearning,[D] Metric for imbalanced data,https://www.reddit.com/r/MachineLearning/comments/aodgkd/d_metric_for_imbalanced_data/,amil123123,1549604807,"Hey all,

I have a data of binary classes which is hugely imbalanced ( large number of -ve class ). I want an error metric for the model, more specifically a single number. I am training a Logistic Regression model trained with class weights ( class\_weight parameter in sklearn). I have searched for metrics like f1-score, MCC , AUC-ROC, AUC-PC however I cannot find sources which discuss the tradeoff of each metric - like F1-score does not take into account TN which might give biased results etc.

I really want to know the tradeoffs of using the above-mentioned metrics.

Thanks!",16,1,False,self,,,,,
453,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,15,aodmvg,self.MachineLearning,"Global Machine Learning Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/aodmvg/global_machine_learning_market_size_outlook/,vardhan1020,1549606177,[removed],0,1,False,self,,,,,
454,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,15,aodngd,self.MachineLearning,Whats best program you recommend for online Master in Machine learning or analytics,https://www.reddit.com/r/MachineLearning/comments/aodngd/whats_best_program_you_recommend_for_online/,yycglad,1549606312,[removed],0,1,False,self,,,,,
455,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,15,aodpek,self.MachineLearning,[N] University of Toronto is offering a course on Quantum Machine Learning through edX!,https://www.reddit.com/r/MachineLearning/comments/aodpek/n_university_of_toronto_is_offering_a_course_on/,Turing__Incomplete,1549606753,"The course has just started a few days ago: [https://www.edx.org/course/quantum-machine-learning](https://www.edx.org/course/quantum-machine-learning)

&gt;By the end of this course, you will be able to:  
 Distinguish between quantum computing paradigms relevant for machine learning  
 Assess expectations for quantum devices on various time scales  
 Identify opportunities in machine learning for using quantum resources  
 Implement learning algorithms on quantum computers in Python",48,1,False,self,,,,,
456,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,15,aodz3t,arxiv.org,[1812.04877] Quantum Statistical Inference,https://www.reddit.com/r/MachineLearning/comments/aodz3t/181204877_quantum_statistical_inference/,ndha1995,1549608997,,1,1,False,default,,,,,
457,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,17,aoegl1,nytimes.com,El legado de Alan Turing va ms all de la informtica,https://www.reddit.com/r/MachineLearning/comments/aoegl1/el_legado_de_alan_turing_va_ms_all_de_la/,noabandones,1549613623,,0,1,False,default,,,,,
458,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,17,aoeog5,self.MachineLearning,[D] Changing padding values for CNNs,https://www.reddit.com/r/MachineLearning/comments/aoeog5/d_changing_padding_values_for_cnns/,data-soup,1549615969,"Hi guys, I posted a question about padding values on [stack overflow](https://datascience.stackexchange.com/questions/45027/changing-padding-values-in-keras) and didn't get much attention so I'll try it here.

What is the influence of changing the padding value with its borders, I might miss vocabulary because I can't find many papers about this alternative.

In Keras: the actual behavior of the `SAME` padding (stride=6, width=5):

```
            pad|                                        |pad
   inputs:      0 |1  2  3  4  5  6  7  8  9  10 11 12 13|0  0
               |________________|
                              |_________________|
                                             |________________|
```

What about, for instance, repeating the border for circular inputs (like 360 images)? Like so:

```
             pad|                                      |pad
   inputs:   13 |1  2  3  4  5  6  7  8  9  10 11 12 13| 1 2
             |________________|
                            |_________________|
                                           |________________|

```

If you have any resources on it It'll be very much appreciated.",10,1,False,self,,,,,
459,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,17,aoeoug,self.MachineLearning,Volentix - the future generation eco-system,https://www.reddit.com/r/MachineLearning/comments/aoeoug/volentix_the_future_generation_ecosystem/,VolentixLab,1549616089,[removed],0,1,False,self,,,,,
460,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,18,aoet2n,hakunamatata.in,How Artificial Intelligence is Transforming the Manufacturing Industry,https://www.reddit.com/r/MachineLearning/comments/aoet2n/how_artificial_intelligence_is_transforming_the/,alba941,1549617305,,0,1,False,default,,,,,
461,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,18,aof1zq,medium.com,"Evolutionary Trading Algorithms  Imagine a tiny algorithm like a cell alive with its own DNA (source code), and genes (parameters). Imagine millions of cells forming tissues, organs, and a whole organism resulting from all sorts of algorithms. Now, try to visualize such AI trading at the markets.",https://www.reddit.com/r/MachineLearning/comments/aof1zq/evolutionary_trading_algorithms_imagine_a_tiny/,cienciaslocas,1549619860,,0,1,False,https://b.thumbs.redditmedia.com/A29-b7wz9iZLgEXSGyxbjlP44Kway1ccU44D380pltw.jpg,,,,,
462,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,18,aof29d,theappsolutions.com,Conversational Interfaces,https://www.reddit.com/r/MachineLearning/comments/aof29d/conversational_interfaces/,lady_monsoon,1549619942,,0,1,False,https://a.thumbs.redditmedia.com/siPexYGTBFoksta80qT1b7RLRBZAWFrncHqLrJAGom4.jpg,,,,,
463,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,19,aofazo,seldon.io,[P] Outlier Detection with Seldon,https://www.reddit.com/r/MachineLearning/comments/aofazo/p_outlier_detection_with_seldon/,ahousley,1549622281,,0,1,False,default,,,,,
464,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,19,aofbz4,dunebook.com,12 Best Machine Learning Projects In 2019,https://www.reddit.com/r/MachineLearning/comments/aofbz4/12_best_machine_learning_projects_in_2019/,deven_rathore,1549622547,,0,1,False,default,,,,,
465,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,20,aofiou,machinelearningsummerinternship.blogspot.com,Summer Internship 2019 for Computer Science,https://www.reddit.com/r/MachineLearning/comments/aofiou/summer_internship_2019_for_computer_science/,Summerinternship,1549624370,,0,1,False,default,,,,,
466,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,20,aofixl,self.MachineLearning,Amplifying the Imitation Effect for Reinforcement Learning of UCAVs Mission Execution,https://www.reddit.com/r/MachineLearning/comments/aofixl/amplifying_the_imitation_effect_for_reinforcement/,bluediary8,1549624433,[removed],0,1,False,self,,,,,
467,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,20,aofmm5,self.MachineLearning,Looking for collaborators to port stylegan to pytorch,https://www.reddit.com/r/MachineLearning/comments/aofmm5/looking_for_collaborators_to_port_stylegan_to/,DecentMakeover,1549625345,[removed],0,1,False,self,,,,,
468,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,21,aofzda,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/aofzda/global_machine_learning_market_size_outlook/,performadorb,1549628341,[removed],0,1,False,self,,,,,
469,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,21,aog4ab,/r/MachineLearning/comments/aog4ab/check_out_this_realtime_humanbackground/,"Check out this Real-time Human&amp;Background separating app, the power of AI is super amazing!",https://www.reddit.com/r/MachineLearning/comments/aog4ab/check_out_this_realtime_humanbackground/,shallwayjp,1549629444,,0,1,False,https://b.thumbs.redditmedia.com/MTu5ClmdI9tr5JG2x9p2abr-IOpGy_tq7tYXluC3IBI.jpg,,,,,
470,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,22,aogb8l,ai-benchmark.com,"[N] AI rush: Snapdragon 855, MediaTek P90, Kirin 980 or Exynos 9820  who rules the game?",https://www.reddit.com/r/MachineLearning/comments/aogb8l/n_ai_rush_snapdragon_855_mediatek_p90_kirin_980/,aiff22,1549630978,,0,1,False,https://b.thumbs.redditmedia.com/hWO98vxz4esuLCakPZ74jtzvuFJVI7YU09GLM5Gwp4k.jpg,,,,,
471,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,22,aogf4b,self.MachineLearning,Exciting opportunities to join True AI,https://www.reddit.com/r/MachineLearning/comments/aogf4b/exciting_opportunities_to_join_true_ai/,Juliadoncheva,1549631823,"Hello everyone,

Excited about working in a rapidly growing tech startup? Want to be part of the next wave of AI-driven innovation in conversational interfaces?

If you answered yes to the above, you could be just what were looking for.

We currently have 2 exciting opportunities to join us:

Full stack software engineer:  
[https://trueai.workable.com/j/3A4D8FAC29](https://trueai.workable.com/j/3A4D8FAC29?fbclid=IwAR3mLsPQg-OgQ7P3wBhNykwC-XcGaQw0fpCXSoIEix23zRsqmK4weZJqvAQ)

Backend software engineer:  
[https://trueai.workable.com/j/0513556D41](https://trueai.workable.com/j/0513556D41?fbclid=IwAR1WTPa440xLrMKcdxoQanS9lG3ysBRm-jm4hXFB19N6VMLBfHSBzllo3BM)

To apply, please email [apply@trueai.io](mailto:apply@trueai.io). In your email:

Include your CV / link to your professional profile  
Let us know your ideal start date  
Confirm that you would not require us to arrange a visa and on what grounds (i.e. EU citizenship, student visa)  
Include a link to your portfolio / relevant examples of past work  
Let us know why you are interested in the job

Looking forward to hearing from you! :)

Best regards,  
Julia",0,1,False,self,,,,,
472,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,22,aogij7,youtube.com,Glimpse of Mr. Vimal Daga on News Channel @Pilani,https://www.reddit.com/r/MachineLearning/comments/aogij7/glimpse_of_mr_vimal_daga_on_news_channel_pilani/,Summerinternship,1549632553,,0,1,False,default,,,,,
473,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,22,aogpth,self.MachineLearning,Are there online services to recognize given features in images?,https://www.reddit.com/r/MachineLearning/comments/aogpth/are_there_online_services_to_recognize_given/,xxdarkie,1549634052,[removed],0,1,False,self,,,,,
474,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,23,aogyvb,self.MachineLearning,Why do none of the state-of-the-art results feature ensembles?,https://www.reddit.com/r/MachineLearning/comments/aogyvb/why_do_none_of_the_stateoftheart_results_feature/,drd13,1549635753,[removed],0,1,False,self,,,,,
475,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,23,aoh59y,self.MachineLearning,I just had a dream where I escaped from the police by rolling down a big hill and yelling GRADIENT DESCENT,https://www.reddit.com/r/MachineLearning/comments/aoh59y/i_just_had_a_dream_where_i_escaped_from_the/,Zheng261,1549636987,[removed],0,1,False,self,,,,,
476,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,23,aoh7h5,blog.mousephenotype.org,Using machine learning to identify mammalian gene function,https://www.reddit.com/r/MachineLearning/comments/aoh7h5/using_machine_learning_to_identify_mammalian_gene/,MousePhenotyping,1549637283,,0,1,False,https://b.thumbs.redditmedia.com/DtxlXCT9SmJkeo8QYG_6cVJJy64lCg6mUjtS_D1MVGs.jpg,,,,,
477,MachineLearning,t5_2r3gv,2019-2-8,2019,2,8,23,aoh8gw,self.MachineLearning,Are there any aerospace engineers working in ML in the tech industry on here?,https://www.reddit.com/r/MachineLearning/comments/aoh8gw/are_there_any_aerospace_engineers_working_in_ml/,orioncygnus1,1549637454,[removed],0,1,False,self,,,,,
478,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,0,aohsc2,blockdelta.io,Artificial Intelligence Benefits and Challenges - A Double-Edged Promise.,https://www.reddit.com/r/MachineLearning/comments/aohsc2/artificial_intelligence_benefits_and_challenges_a/,BlockDelta,1549640878,,0,1,False,default,,,,,
479,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,1,aoi4gp,self.MachineLearning,code to test quick image segmentation,https://www.reddit.com/r/MachineLearning/comments/aoi4gp/code_to_test_quick_image_segmentation/,deluded_soul,1549642857,[removed],0,1,False,self,,,,,
480,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,1,aoi5sw,iconary.allenai.org,Draw and Guess with AllenAI,https://www.reddit.com/r/MachineLearning/comments/aoi5sw/draw_and_guess_with_allenai/,GantMan,1549643070,,0,1,False,https://b.thumbs.redditmedia.com/F-T2nAjMt-x-Pkf5HIKxgvGEsTRDdhwA0RmyA7hsgQI.jpg,,,,,
481,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,1,aoi8gy,self.MachineLearning,[D] Why do we need special methods to evaluate the performance of a GAN? Can't we just do that from the discriminator network's loss?,https://www.reddit.com/r/MachineLearning/comments/aoi8gy/d_why_do_we_need_special_methods_to_evaluate_the/,Eoncarry,1549643499,"I came across inception score in a [youtube video](https://www.youtube.com/watch?v=1ct_P3IZow0) and was wondering why can't we just check the quality of an image from the predicted probability of the discriminator network (i.e., if the discriminator network outputs a high probability for my generated image, then my GAN is doing a good job). Why do we need seperate methods to evaluate the performance?

Thanks in advance!",9,1,False,self,,,,,
482,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,1,aoih04,self.MachineLearning,Linear Regression &amp; Gradient Descent from Scratch with Visualization,https://www.reddit.com/r/MachineLearning/comments/aoih04/linear_regression_gradient_descent_from_scratch/,theshubhamgoel,1549644840,[removed],0,1,False,self,,,,,
483,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,2,aoiyj9,self.MachineLearning,Care to speculate? Are Protein Fragments or Entire Protein Sequences useful when classifying via Machine Learning techniques?,https://www.reddit.com/r/MachineLearning/comments/aoiyj9/care_to_speculate_are_protein_fragments_or_entire/,mockrun,1549647620,"Greetings,

I  have a question that I am investigating/researching but in the   meantime would like to gather feedback regarding. Would you care to   speculate or hypothesize?

Situation:

I  would like to classify a set of proteins as either belonging to a   group or not using machine learning techniques. Pretty straight forward   so far.  I have downloaded proteins from Uniprot, for example,  protein-X  vs Not protein-X. As one would expect, among the protein  sequences many  fragments (length &lt; 50AA) are also present in the  results.

Question:

Would  you be inclined be to remove (OR not remove) the protein-X  fragments  (length &lt; 50AA) from the super-set of proteins? Do the  protein-X  fragments represent the category of proteins being  investigating or  not?

I would appreciate your insights,",0,1,False,self,,,,,
484,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,2,aoj0bh,self.MachineLearning,Crop Diseases Image Database,https://www.reddit.com/r/MachineLearning/comments/aoj0bh/crop_diseases_image_database/,ExitioGriffiths,1549647892,[removed],0,1,False,self,,,,,
485,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,3,aoj7zn,producthunt.com,Just Launched: Text to speech using familiar voices trained with Microsoft Custom Speech,https://www.reddit.com/r/MachineLearning/comments/aoj7zn/just_launched_text_to_speech_using_familiar/,drakeb,1549649076,,0,1,False,https://b.thumbs.redditmedia.com/jtywtQiYHRujyUwm9H744vCtNn8fy0w4dAaomfU7aik.jpg,,,,,
486,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,3,aojh3y,medium.com,Facebook Boosts Cross-Lingual Language Model Pretraining Performance,https://www.reddit.com/r/MachineLearning/comments/aojh3y/facebook_boosts_crosslingual_language_model/,Yuqing7,1549650535,,0,1,False,default,,,,,
487,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,3,aojkoz,self.MachineLearning,Keras Neural Network Accuracy stuck at 45%,https://www.reddit.com/r/MachineLearning/comments/aojkoz/keras_neural_network_accuracy_stuck_at_45/,TeaAndSauce,1549651111,"Hello everyone,

I am doing a project at university and I am training a neural network using transfer learning with the VGG16 model. I am using Keras and Tensorflow, I have been testing a lot of things but nothing seems to get me past an accuracy of 45% so I have come here as a final resort. I am new to the world of machine learning and so there is still a lot I have yet to learn. Here's the code:

    image_input = Input(shape=(224, 224, 3))
    model = VGG16(input_tensor=image_input, include_top=True, weights=""imagenet"")
    
    # Store connected layers to reconnect later
    fc1 = model.layers[-3]
    fc2 = model.layers[-2]
    output = model.layers[-1]

    # Initialise Dropout Layers with a dropout rate of 50%
    dropout1 = Dropout(0.5)
    dropout2 = Dropout(0.5)

    # Reconnect layers that were stored
    x = dropout1(fc1.output)
    x = fc2(x)
    x = dropout2(x)
    last_layer = output(x)

    # Adjust model output layer for 3 classifications
    model = Model(input=model.input, output=last_layer)
    last_layer = model.get_layer(""dropout_2"").output
    out = Dense(3, activation=""softmax"", name=""output"")(last_layer)
    model = Model(input=model.input, output=out)

    # Compile out adapted model
    model.compile(loss=""categorical_crossentropy"", \
            optimizer=""adadelta"", metrics=[""accuracy""])
    
    for layer in model.layers[:-1]:
        layer.trainable = False

    model.fit(
                x_train,
                y_train,
                batch_size=64,
                epochs=200,
                validation_data=(x_test, y_test),
                shuffle=True
                )

I have been stuck on trying to solve this for days. There are 3 classifications:
* Spiral (1600 images)
* Elliptical (1600 images)
* Irregular (332 images)

These are galaxy classifications and as you can see, I don't have many irregular galaxy images. When training the neural network, the accuracy does not go over 0.45 and I am wondering if it has something to do with the small amount of images for the irregular galaxies. 

Here is the neural network structure:

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
==================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544 
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
output (Dense)               (None, 3)                 12291     
==================================================
Total params: 134,272,835
Trainable params: 134,272,835
Non-trainable params: 0
_________________________________________________________________

Are there any suggestions that someone could give me to help me get better accuracy results?

Thank you",0,1,False,self,,,,,
488,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,3,aojr1p,self.MachineLearning,I want to label images of Coffee Cups for an Object Detection model I am training. Best tools/platform?,https://www.reddit.com/r/MachineLearning/comments/aojr1p/i_want_to_label_images_of_coffee_cups_for_an/,heybluez,1549652139,[removed],0,1,False,self,,,,,
489,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,4,aojta0,self.MachineLearning,[D] Weighted Cosine Similarity,https://www.reddit.com/r/MachineLearning/comments/aojta0/d_weighted_cosine_similarity/,abinmn619,1549652504,"Hi everyone, I am new to ML and I am having some problems in understanding some concepts. I am trying to write an algorithm to find the similarity between two users.

Say I have m users with n properties for each user. Each property will have different weights associated with them. Greater the weight, greater the importance when predicting the final similarity. Can I achieve this using cosine similarity? If so what should be the formula to include the weights?

Any help is appreciated.",8,1,False,self,,,,,
490,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,4,aojtis,self.MachineLearning,Machine learning for video compression.,https://www.reddit.com/r/MachineLearning/comments/aojtis/machine_learning_for_video_compression/,sasksean,1549652539,[removed],0,1,False,self,,,,,
491,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,5,aokgim,self.MachineLearning,Machine Learning problems,https://www.reddit.com/r/MachineLearning/comments/aokgim/machine_learning_problems/,m_mbahaa,1549656244,[removed],0,1,False,self,,,,,
492,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,5,aokidf,medium.com,Google Brain Research Scientist Quoc Le on AutoML and More,https://www.reddit.com/r/MachineLearning/comments/aokidf/google_brain_research_scientist_quoc_le_on_automl/,Yuqing7,1549656538,,0,1,False,https://b.thumbs.redditmedia.com/1rf6nNXpHMfkewV_nxwAmILuj0BJCv5NHvU9zIo6Otg.jpg,,,,,
493,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,5,aokwx0,youtube.com,Why can't we build a brain out of circuits?,https://www.reddit.com/r/MachineLearning/comments/aokwx0/why_cant_we_build_a_brain_out_of_circuits/,420CARLSAGAN420,1549658854,,0,1,False,https://b.thumbs.redditmedia.com/5caUH3EDv67DOvDaKHuoSXtwjL5Wjs_cmpkUBrbnsiM.jpg,,,,,
494,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,6,aolgzu,self.MachineLearning,Hi! Can anyone recommend me any github project which I can contribute to to get my hands dirty?,https://www.reddit.com/r/MachineLearning/comments/aolgzu/hi_can_anyone_recommend_me_any_github_project/,holanacho,1549662176,[removed],0,1,False,self,,,,,
495,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,7,aom4db,self.MachineLearning,[D] Assistance with designing a NLP model to tackle a problem in Competitive Debate,https://www.reddit.com/r/MachineLearning/comments/aom4db/d_assistance_with_designing_a_nlp_model_to_tackle/,Best_Mord_Brazil,1549666111,"Hi everyone 

I am trying to solve a unique NLP problem. 

In the USA Competitive Debate community, there is high demand for any tool that could do something like ""word level extractive summarisation"" 

In the competative debate community, we use evidence - formatted like [this](https://thedebateguru.weebly.com/uploads/4/9/1/7/49179595/_1074768.png): 

My goal is to create model that can underline debate evidence for me. 

Now, competitive debate is kind of word. People general do a thing called [""Spreading""](https://www.youtube.com/watch?v=0FPsEwWT6K0) which means that complete grammatical accuracy isn't as important as it might be in other domains. That means that ""almost correct"" is good enough for me. 

I've created a dataset from all publicly available evidence listed on [""Open Evidence""](https://openev.debatecoaches.org/) which engs up being a total of 300,000 pieces of evidence.

I've written parsing code to pull the text out and tag each word with either a ""und"" or ""non"" indicating if each word is underlined (appears in the summary)  My text file (in CONLL2003 format) ends up being 1.5 Gb, and it takes over 100GB of RAM to load this dataset into memory for processing with state of the art PoS tagging tools like [Flair](https://github.com/zalandoresearch/flair). I'm trying to use the amount of data that will fit into my GPU (about 60mb) along with Glove word embeddings, but I do not think that this is efficient or will lead to good results. 

I have several questions given my situation: 


1. Is PoS tagging even the right choice for this? Should I experiment with seq-seq models? Is there some other type of model that would be more efficient for my problem? 

2. Do previous ""word level extractive"" summarization techniques exist? That is, methods that will only use the source words in a document. I can find plenty of sentence level extractive techniques, but nothing at the word level :( 

3. What can I do to mitigate my RAM and VRAM problem? I have a RTX 2080ti at home, but even that is far too small for the kinds of models that I want to build. Ideally, I'd use ever technique at my disposal to improve my models accuracy (Bidirectional LSTM with CRF, Attention, BERT/Elmo/Flair/Glove embeddings, etc) - but many of these techniques are not memory friendly. Are there good CPU bound techniques - or GPU frameworks that support iterative training of datasets that don't fit in memory? 

4. If a GPU solution cannot be found - than would it be useful for me to do something like try to create my own custom word vectors out of my own corpus using something like [umap](https://github.com/lmcinnes/umap) combined with random forests to try to predict which words get underlined? Do CPU based techniques give good performance? Can I use pre-trained word embeddings?

5. My current ideas do not add additional information, such as the ""tagline"" (a short summary read outloud by the debater before they read the summarized evidence). How can I add this information into my model to make my underliner more effective? 

I will provide more information as is needed. Soon I will open source my dataset and my Flair code for training a PoS tagger on it, but I cannot take advantage of all of my data without a super-computer if my current assumptions are correct. What do I do? ",0,1,False,self,,,,,
496,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,8,aomedr,self.MachineLearning,Resource recommendations for applying Learning theory,https://www.reddit.com/r/MachineLearning/comments/aomedr/resource_recommendations_for_applying_learning/,its-trivial,1549667807,[removed],0,1,False,self,,,,,
497,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,8,aomjql,self.MachineLearning,Looking for someone to do a commission,https://www.reddit.com/r/MachineLearning/comments/aomjql/looking_for_someone_to_do_a_commission/,goodoldshane,1549668765,[removed],0,1,False,self,,,,,
498,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,12,aoom2a,self.MachineLearning,User- image recommender system by using image CNN feature,https://www.reddit.com/r/MachineLearning/comments/aoom2a/user_image_recommender_system_by_using_image_cnn/,kevinaiworld,1549683595," There are a lot of user-item recommender systems by using user's and item's context infotmation.

Are there any popular user-image recommender system papers that incorporate image CNN feature? because it is very easy to extract CNN feature from image. In item, such as movie or music, which the features is not easy to obtain.

My question:

Any popular papers talk about user-image recommender system using CNN feature?

PS: It may be inappropriate to ask this question in StackOverflow, Any suggest places should I ask this kind of problem?

Thanks",0,1,False,self,,,,,
499,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,13,aoovxy,self.MachineLearning,Good places to do masters in machine learning/data science,https://www.reddit.com/r/MachineLearning/comments/aoovxy/good_places_to_do_masters_in_machine_learningdata/,xuhu55,1549685727,[removed],0,1,False,self,,,,,
500,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,13,aooxf5,self.MachineLearning,Hobbyist environment setup,https://www.reddit.com/r/MachineLearning/comments/aooxf5/hobbyist_environment_setup/,Shoddy_Researcher,1549686068,[removed],0,1,False,self,,,,,
501,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,14,aop82i,self.MachineLearning,Has VAE applied to any industrial recommendation system?,https://www.reddit.com/r/MachineLearning/comments/aop82i/has_vae_applied_to_any_industrial_recommendation/,xwzded07,1549688457,[removed],0,1,False,self,,,,,
502,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,14,aopjtj,arxiv.org,[R] A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks,https://www.reddit.com/r/MachineLearning/comments/aopjtj/r_a_generalization_theory_of_gradient_descent_for/,ladycrab,1549691219,,13,1,False,default,,,,,
503,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,14,aoplti,self.MachineLearning,Question Regarding the Power of Individual Data Points,https://www.reddit.com/r/MachineLearning/comments/aoplti/question_regarding_the_power_of_individual_data/,spot4992,1549691738,[removed],0,1,False,self,,,,,
504,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,15,aoptfo,self.MachineLearning,Get to know the Benefits of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aoptfo/get_to_know_the_benefits_of_machine_learning/,zenraysofficial,1549693614," 

 Machine Learning is a field of Artificial Intelligence. It allows software applications to become more accurate. The basic thing of Machine Learning is to build algorithms that receives data inputto predictoutput using statistical analysis. It requires searching data for patterns.

&amp;#x200B;

**Benefits of Machine Learning**

1. Data mining is the process of examining several databases toanalyze data and to provideinformation.Machine learning provides actual assumptions that can be used to support decisions.
2. Machine learning system system willrefer newly acquired data to do betterconsumption patterns.
3. Machine learning can automate task

**Machine Learning is applied in**

1. Fraud Detection
2. Web Search Results
3. Self-driving Google Car
4. Online recommendation engines
5. New pricing models
6. Prediction of equipment failures
7. Real-time ads on Web pages
8. Email spam filtering
9. Pattern &amp; Image recognition
10. Text based sentimental analysis
11. Network intrusion detection
12. Credit scoring
13. Next best offers

If you are Interested to learn Machine learning, have a look at course content

&amp;#x200B;

*Processing img f2iokz10lhf21...*

&amp;#x200B;",0,1,False,self,,,,,
505,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,15,aopxut,self.MachineLearning,I want to write a program that can answer trivia questions. where do I start?,https://www.reddit.com/r/MachineLearning/comments/aopxut/i_want_to_write_a_program_that_can_answer_trivia/,RedBostonian,1549694770,[removed],0,1,False,self,,,,,
506,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,15,aoq0jb,self.MachineLearning,LANL Earthquake Prediction Challenge,https://www.reddit.com/r/MachineLearning/comments/aoq0jb/lanl_earthquake_prediction_challenge/,sayantandas30011998,1549695456,[removed],0,1,False,self,,,,,
507,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,16,aoq8mm,self.MachineLearning,Google's AI semantic segmentation for Iphone,https://www.reddit.com/r/MachineLearning/comments/aoq8mm/googles_ai_semantic_segmentation_for_iphone/,gulzainali,1549697705,[removed],0,1,False,self,,,,,
508,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,16,aoqbu6,youtube.com,E-Degree On Artificial Intelligence And Machine Learning | Kickstarter |...,https://www.reddit.com/r/MachineLearning/comments/aoqbu6/edegree_on_artificial_intelligence_and_machine/,KiranKiller,1549698637,,0,1,False,default,,,,,
509,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,18,aoqsac,self.MachineLearning,[R] DeepSets: Modeling Permutation Invariance,https://www.reddit.com/r/MachineLearning/comments/aoqsac/r_deepsets_modeling_permutation_invariance/,geshuni,1549703623,"My friend wrote a blog post together with Ferenc Huszar about their new paper on permutation invariant neural networks. Check it out here: [https://www.inference.vc/deepsets-modeling-permutation-invariance/?fbclid=IwAR2XQQrjzGaE1ge\_\_al0dZUIdgsK0Eb-en1JjHlIfmsXgu\_STzpUyeyNMS0](https://www.inference.vc/deepsets-modeling-permutation-invariance/?fbclid=IwAR2XQQrjzGaE1ge__al0dZUIdgsK0Eb-en1JjHlIfmsXgu_STzpUyeyNMS0)

&amp;#x200B;

Hope you like it!",8,1,False,self,,,,,
510,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,18,aoqw3y,self.MachineLearning,[D]Preparation of ground truth vectors for YOLOv3,https://www.reddit.com/r/MachineLearning/comments/aoqw3y/dpreparation_of_ground_truth_vectors_for_yolov3/,Atom_101,1549704788,"I'm trying to implement a training module for YOLOv3, from scratch in keras+tensorflow. I'm having a hard time figuring out how to prepare the target vectors from bounding box coordinates. 

The network divides the input image into grid cells and for each cell it predicts an objectness and 4 terms (tx,ty,th,tw) for the bounding box. The actual box coordinates are obtained as follows:

    bx = sigmoid (tx) + cx
    by = sigmoid(ty) + cy
    bh = ph.e^(th)
    bw = pw.e^(tw)

Where ph,pw are anchor box priors and cx,cy are position of grid cell. Am I supposed to 

a) use the inverse of these functions to calculate tx,ty,th,tw for each image?(maybe it's because my code isn't very optimised but this is taking quite a while to compute) 

OR

b) calculate actual box coordinates(in pixels) for each image and compute the loss using ground truth box coordinates? If yes, then how am I to find cross entropy box loss using pixel values?

Also, I have used the darknet repo by GitHub user AlexeyAB. It takes the coordinates as the ratio of pixel values of ground truth to the total height/width of the image. How does it calculate the target vectors?",4,1,False,self,,,,,
511,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,19,aor3he,self.MachineLearning,"The Definition of ""Online handwriting recognition""",https://www.reddit.com/r/MachineLearning/comments/aor3he/the_definition_of_online_handwriting_recognition/,SerMabrouk,1549707032,"I'm a little bit confused but does ""Online"" just mean that it is real-time? Or does it have to use the x,y coordinates of the pen for inference?",0,1,False,self,,,,,
512,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,19,aorc8b,arxiv.org,[R] ColorNet: Investigating the importance of color spaces for image classification,https://www.reddit.com/r/MachineLearning/comments/aorc8b/r_colornet_investigating_the_importance_of_color/,xternalz,1549709499,,5,1,False,default,,,,,
513,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,20,aorije,self.MachineLearning,"AWS Certification demonstrates technical expertise, helps you advance your career, and helps employers find skilled cloud professionals.",https://www.reddit.com/r/MachineLearning/comments/aorije/aws_certification_demonstrates_technical/,AWSTrainingInHouston,1549711256,[removed],0,1,False,self,,,,,
514,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,20,aoriz2,self.MachineLearning,Intelligenza Artificiale Citt,https://www.reddit.com/r/MachineLearning/comments/aoriz2/intelligenza_artificiale_citt/,diego-user,1549711374,[removed],0,1,False,self,,,,,
515,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,20,aorjkg,self.MachineLearning,Black Swans Meetups: Get to Know Other Data-Scientists!,https://www.reddit.com/r/MachineLearning/comments/aorjkg/black_swans_meetups_get_to_know_other/,jdyr1729,1549711543,[removed],0,1,False,self,,,,,
516,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,20,aorlvj,self.MachineLearning,Black Swans Meetups: Get to Know Other Data-Scientists!,https://www.reddit.com/r/MachineLearning/comments/aorlvj/black_swans_meetups_get_to_know_other/,jdyr1729,1549712178,[removed],0,1,False,self,,,,,
517,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,20,aorn7d,self.MachineLearning,[D] Forecasting stock movement prices based on multivariate historical data,https://www.reddit.com/r/MachineLearning/comments/aorn7d/d_forecasting_stock_movement_prices_based_on/,let2make,1549712558,"Hello all

I have been looking into what is the best way to forecast an actual stock price or it's potential movements based on historical data of the target stock and other related stocks which influence the target variable. My time series data is multivariate. I am looking to forecast up to 2 weeks of potential movements.

&amp;nbsp;

An example of the dataset (not real representation):

&amp;nbsp;
 
Date | Target Stock | Stock 1 | Stock 2 | Stock 3 | Stock 4 | Stock 5 | Stock 6
----|------------|-------|-------|-------|-------|-------|-------
2012-01-01 | 121.53 | 10.53 | 53.51 | 432.15 | 2692.36 | 89.01 | 77.74
2012-01-02 | 131.30 | 12.75 | 54.21 | 429.35 | 2717.53 | 89.31 | 81.07
2012-01-03 | 132.43 | 12.73 | 59.25 | 427.92 | 2643.48 | 90.28 | 81.26
... | ... | ... | ... | ... | ... | ... | ...
2017-01-01 | 222.34 | 33.25 | 140.32 | 353.12 | 2633.36 | 96.65 | 90.74

&amp;nbsp;

Forecasting result:

&amp;nbsp;

Date | Target Stock
----|------------
2017-01-02 | Forecast 1
2017-01-03 | Forecast 2
2017-01-04 | Forecast 3
... | ...
2017-01-15 | Forecast 14

&amp;nbsp;

I obviously don't expect amazing results, but it would be useful to at least forecast what would be the general movement direction of a stock for the next 2 weeks. I am looking at what is the best approach and what models to use for the forecast (ex. LSTM, VAR) 

",8,1,False,self,,,,,
518,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,21,aos10f,self.MachineLearning,ML / Deep Learning on Windows,https://www.reddit.com/r/MachineLearning/comments/aos10f/ml_deep_learning_on_windows/,killver,1549716224,[removed],0,1,False,self,,,,,
519,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,21,aos28w,self.MachineLearning,Black Swans Meetups: Get to Know Other Data-Scientists!,https://www.reddit.com/r/MachineLearning/comments/aos28w/black_swans_meetups_get_to_know_other/,jdyr1729,1549716550,[removed],0,1,False,self,,,,,
520,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,22,aosafo,youtube.com,Genius Pawn break mastery | Leela reacts beautifully vs Stockfish in Fre...,https://www.reddit.com/r/MachineLearning/comments/aosafo/genius_pawn_break_mastery_leela_reacts/,kingscrusher-youtube,1549718653,,0,1,False,https://b.thumbs.redditmedia.com/broXTu0wumflGpMK10-34B1Api8TACXSo34y0ndvuJw.jpg,,,,,
521,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,22,aosau4,github.com,[P] Homoiconic Graph Machine: A Unified Graph Theoretic Model for Narrow Artificial Intelligence and Artificial General Intelligence,https://www.reddit.com/r/MachineLearning/comments/aosau4/p_homoiconic_graph_machine_a_unified_graph/,wengchunkn,1549718745,,0,1,False,default,,,,,
522,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,22,aosj0u,self.MachineLearning,Where can I get a Cricket (sports) data?,https://www.reddit.com/r/MachineLearning/comments/aosj0u/where_can_i_get_a_cricket_sports_data/,Hopefully_awesome,1549720662,[removed],0,1,False,self,,,,,
523,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,23,aosp9n,arxiv.org,Don't Unroll Adjoint: Differentiating SSA-Form Programs,https://www.reddit.com/r/MachineLearning/comments/aosp9n/dont_unroll_adjoint_differentiating_ssaform/,skariel,1549721987,,1,1,False,default,,,,,
524,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,23,aosuzz,codeingschool.com,Tracking Bird Migration Using Python 3: Source Code &amp; Tutorial:,https://www.reddit.com/r/MachineLearning/comments/aosuzz/tracking_bird_migration_using_python_3_source/,subhamroy021,1549723244,,0,1,True,nsfw,,,,,
525,MachineLearning,t5_2r3gv,2019-2-9,2019,2,9,23,aosz5u,self.MachineLearning,How good is R to work with deep learning? (and some simpler machine learning methods),https://www.reddit.com/r/MachineLearning/comments/aosz5u/how_good_is_r_to_work_with_deep_learning_and_some/,jamal_gui,1549724107,[removed],0,1,False,self,,,,,
526,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,0,aot7xg,self.MachineLearning,Deep Learning Libraries by Language,https://www.reddit.com/r/MachineLearning/comments/aot7xg/deep_learning_libraries_by_language/,andrea_manero,1549725853,[removed],0,1,False,self,,,,,
527,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,0,aotakz,medium.com,Photonic quantum neural networks - XanaduAI,https://www.reddit.com/r/MachineLearning/comments/aotakz/photonic_quantum_neural_networks_xanaduai/,MoBizziness,1549726381,,0,1,False,default,,,,,
528,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,0,aotcih,medium.com,[R] Photonic quantum neural networks - processes information stored in quantum states of light for machine learning applications - XanaduAI,https://www.reddit.com/r/MachineLearning/comments/aotcih/r_photonic_quantum_neural_networks_processes/,MoBizziness,1549726752,,0,1,False,https://b.thumbs.redditmedia.com/DLoTn0DF8QMxjGNoRgUqMoZuLkH_Q2f6tG71dROysWA.jpg,,,,,
529,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,0,aotcrx,medium.com,[R] Photonic quantum neural networks - Processing information stored in quantum states of light for machine learning applications - XanaduAI,https://www.reddit.com/r/MachineLearning/comments/aotcrx/r_photonic_quantum_neural_networks_processing/,MoBizziness,1549726800,,0,1,False,default,,,,,
530,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,0,aotdoi,medium.com,[R] Photonic quantum neural networks - Processing information stored in quantum states of light for machine learning applications - XanaduAI,https://www.reddit.com/r/MachineLearning/comments/aotdoi/r_photonic_quantum_neural_networks_processing/,MoBizziness,1549726975,,0,1,False,https://b.thumbs.redditmedia.com/DLoTn0DF8QMxjGNoRgUqMoZuLkH_Q2f6tG71dROysWA.jpg,,,,,
531,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,1,aotjdu,medium.com,Building a Big Data Machine Learning Spark Application for Flight Delay Prediction,https://www.reddit.com/r/MachineLearning/comments/aotjdu/building_a_big_data_machine_learning_spark/,pedrodc23,1549728036,,0,1,False,https://b.thumbs.redditmedia.com/LWPPP9SCyKkcmGPJg0phCETiOIwcMWubnlmztBegqOM.jpg,,,,,
532,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,2,aou6nt,self.MachineLearning,How to create my own text-to-speech network model?,https://www.reddit.com/r/MachineLearning/comments/aou6nt/how_to_create_my_own_texttospeech_network_model/,tarlanahad,1549732053,[removed],0,1,False,self,,,,,
533,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,2,aouauc,self.MachineLearning,"How to create my own text-to-speech network model? [Deep Learning],[Neural Network]",https://www.reddit.com/r/MachineLearning/comments/aouauc/how_to_create_my_own_texttospeech_network_model/,tarlanahad,1549732769,[removed],0,1,False,self,,,,,
534,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aoutqa,ai.googleblog.com,any coreML implementation of this readily available?,https://www.reddit.com/r/MachineLearning/comments/aoutqa/any_coreml_implementation_of_this_readily/,gulzainali,1549735807,,0,1,False,default,,,,,
535,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aouwe3,self.MachineLearning,[D] Can you give examples of Bayesian statistics applied to computer vision to great effect?,https://www.reddit.com/r/MachineLearning/comments/aouwe3/d_can_you_give_examples_of_bayesian_statistics/,llrful,1549736249,,6,1,False,self,,,,,
536,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aov0ln,thegradient.pub,The Limitations of Deep Learning for Vision and How We Might Fix Them,https://www.reddit.com/r/MachineLearning/comments/aov0ln/the_limitations_of_deep_learning_for_vision_and/,hughbzhang,1549736948,,0,1,False,default,,,,,
537,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aov4pk,thegradient.pub,[D] The Limitations of Deep Learning for Vision and How We Might Fix Them,https://www.reddit.com/r/MachineLearning/comments/aov4pk/d_the_limitations_of_deep_learning_for_vision_and/,hughbzhang,1549737577,,0,1,False,https://b.thumbs.redditmedia.com/kMS2qNO7rc3MlJnUD1qqrSkGakhUWjOuvSYSqtfgDTI.jpg,,,,,
538,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aov63v,self.MachineLearning,Reinforcement Learning Project Ideas,https://www.reddit.com/r/MachineLearning/comments/aov63v/reinforcement_learning_project_ideas/,QuietLie,1549737795,[removed],0,1,False,self,,,,,
539,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aov8sb,self.MachineLearning,No Bullshit Guide to Install Tensorflow GPU on Ubuntu 18.04/18.10.,https://www.reddit.com/r/MachineLearning/comments/aov8sb/no_bullshit_guide_to_install_tensorflow_gpu_on/,rednafi,1549738211,[removed],0,1,False,self,,,,,
540,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aov8x7,github.com,[R] Tensorflow Lingvo speech toolkit,https://www.reddit.com/r/MachineLearning/comments/aov8x7/r_tensorflow_lingvo_speech_toolkit/,tsauri,1549738231,,0,1,False,default,,,,,
541,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,3,aova4j,self.MachineLearning,[D] The Limitations of Deep Learning for Vision and How We Might Fix Them,https://www.reddit.com/r/MachineLearning/comments/aova4j/d_the_limitations_of_deep_learning_for_vision_and/,hughbzhang,1549738415,Newest article from the Gradient discussion what deep learning can and can't do. [https://thegradient.pub/the-limitations-of-visual-deep-learning-and-how-we-might-fix-them/](https://thegradient.pub/the-limitations-of-visual-deep-learning-and-how-we-might-fix-them/),16,1,False,self,,,,,
542,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,4,aovcu7,self.MachineLearning,Machine Learning for Better Creative,https://www.reddit.com/r/MachineLearning/comments/aovcu7/machine_learning_for_better_creative/,NowTecc,1549738835,[removed],0,1,False,self,,,,,
543,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,4,aovdk4,self.MachineLearning,ML position interviews at tech companies,https://www.reddit.com/r/MachineLearning/comments/aovdk4/ml_position_interviews_at_tech_companies/,cmanthp2,1549738946,[removed],0,1,False,self,,,,,
544,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,4,aovlg0,self.MachineLearning,ai-designlab,https://www.reddit.com/r/MachineLearning/comments/aovlg0/aidesignlab/,ai-designlab,1549740168," 

# ai-designlab

Greetings. I am an architectural designer selling the domain I've owned for the last several years, called ai-designlab. It was previously are Architecture + Interior Design Lab website. However, I have moved all my work as an architectural and interior designer and environmental sustainability consultant to a different web address (edennie). If ai-designlab does not sell, I will develop a new site for artificial intelligence at the domain to explore ideas, increase traffic, and resell. Please find me through eBay or here if you would like to make an offer on ai-designlab. Thank you. - EDennie, domain owner

[http://ebay.us/3vaaPO?cmpnId=5338273189](http://ebay.us/3vaaPO?cmpnId=5338273189)

AI HAS UNPRECEDENTED POTENTIAL. COMPANIES WILL THRIVE OR WITHER DEPENDING ON WHETHER THEY REALIZE THIS POTENTIAL. - RICH KARLGAARD, Publisher &amp; Futurist, Forbes Media

&amp;#x200B;",0,1,False,self,,,,,
545,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,4,aovm8u,self.MachineLearning,This Canadian Genius Created Modern AI,https://www.reddit.com/r/MachineLearning/comments/aovm8u/this_canadian_genius_created_modern_ai/,devishulk,1549740300,[removed],0,1,False,self,,,,,
546,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,6,aowvfl,self.MachineLearning,Advice Needed. Is there a machine learning algorithm to identify handwriting of possible two different people?,https://www.reddit.com/r/MachineLearning/comments/aowvfl/advice_needed_is_there_a_machine_learning/,samanshrestharay,1549747191,"Hello everyone, 
I am trying to study Roman Graffiti and apply machine learning to those writings. My professor thinks that a single person is responsible for various ancient graffiti however she is not sure and no way to prove it with the information she has. But they could be writings from different person as well. 
In your opinion, what is the best way to tackle this problem?
Thank you. ",0,1,False,self,,,,,
547,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,7,aoxa0i,self.MachineLearning,"facial recognition , i really need an advice from you guys",https://www.reddit.com/r/MachineLearning/comments/aoxa0i/facial_recognition_i_really_need_an_advice_from/,smiloutchaa,1549749639,"Hello every one ,  i am a  student of embedded systems , and i want to do a project for my degree and i have only 3 months!!!

This project is a smart mirror that contains facial recognition and give an information to the one who is watching it (for example his emails for today or something like that but my professor asked me to do for it something useful that can makes it able be a project for little company one day, because i gave her bad ideas and she says that they are not really good and make people give their money for it ) ,  and actually its a mirror that contains a screen with raspberrypie  behind it . Now what i know is the html/css/js  java/ raspberry (but not that good) part,  i don't know anything about machine learning for facial recognition, and i have only 3 months to do this project , do you guys really advice me  to go ahead in that amount of time ? if yes, i am very thankful if you give me some good  resources to learn only what i need ( the target)

Ps: I am really sorry for that english.  .",0,1,False,self,,,,,
548,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,8,aoxwvh,self.MachineLearning,[R] New Model of AI Applied to UCI Datasets,https://www.reddit.com/r/MachineLearning/comments/aoxwvh/r_new_model_of_ai_applied_to_uci_datasets/,Feynmanfan85,1549753458,"I've applied the new, polynomial time model of artificial intelligence that I've developed to four well-known datasets from the UCI Machine Learning Repository. For each of the four classification problems, the categorizations and predictions generated by the algorithms were generated on an unsupervised basis. Over the four classification problems, the categorization algorithm had an average success rate of 92.833%, where success is measured by the percentage of categories that are consistent with the hidden classification data. Over the four classification problems, the prediction algorithm had an average success rate of 93.497%, where success is measured by the percentage of predictions that are consistent with the hidden classification data. All of the code necessary to run these algorithms, and apply them to the training data, is available on my researchgate homepage.

https://www.researchgate.net/publication/330988443_A_New_Model_of_Artificial_Intelligence_Application_to_Data_I",5,1,False,self,,,,,
549,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,8,aoy0oz,self.MachineLearning,[D] Hidden markov model,https://www.reddit.com/r/MachineLearning/comments/aoy0oz/d_hidden_markov_model/,Unlistedd,1549754131,"Baum welch is used to predict the **probabilities** of a particular outcome (state).

What type of algorithm/math is used to predict the **measure** of a particular outcome (state) generated by the our baum welch algo?",5,1,False,self,,,,,
550,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,8,aoy2op,self.MachineLearning,"Adversary, Attractor, Astonishment : Subverting traps in Human vs AI games",https://www.reddit.com/r/MachineLearning/comments/aoy2op/adversary_attractor_astonishment_subverting_traps/,evanthebouncy,1549754476,[removed],0,1,False,self,,,,,
551,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,8,aoy635,self.MachineLearning,Deep Learning: Titan RTX vs Two 2080 ti,https://www.reddit.com/r/MachineLearning/comments/aoy635/deep_learning_titan_rtx_vs_two_2080_ti/,Aklenar,1549755082,[removed],0,1,False,self,,,,,
552,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,9,aoyhgd,self.MachineLearning,"[D] Adversary, Attractor, Astonishment : Subverting traps in Human vs AI games",https://www.reddit.com/r/MachineLearning/comments/aoyhgd/d_adversary_attractor_astonishment_subverting/,evanthebouncy,1549757145,"Hi,

&amp;#x200B;

I am an AI PhD student at MIT and I just wrote an (anti-hype) article discussing human vs AI in games. Specifically, I discuss under which circumstances is it hopeless for a human to win, and under which circumstances do we have a good shot. This written in context of the recent AlphaStar agent that learns to play Starcraft 2. Link here: [https://medium.com/@evanthebouncy/adversary-attractor-astonishment-cea801d761](https://medium.com/@evanthebouncy/adversary-attractor-astonishment-cea801d761)

&amp;#x200B;

It should be a good read ! !

&amp;#x200B;

I can answer any questions here as well, I do not work for DeepMind so I can be more frank in my answers, but at the same times these answers will largely be speculative as I do not work directly on the project.",11,1,False,self,,,,,
553,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,9,aoyo9u,self.MachineLearning,Advice for writing research paper,https://www.reddit.com/r/MachineLearning/comments/aoyo9u/advice_for_writing_research_paper/,CSGOvelocity,1549758347,"Hello, I am currently writing a paper which involves using the YOLO (You Only Look Once) model for object detection.

A part of the framework requires real-time detection and another part of it does not.

So should I compare YOLO to the other real time system (SSD) or will a comparison with Fast R-CNN is good enough.

I am asking this as SSD (have not trained it on my dataset, saying this from data on websites) clearly beats YOLO in terms of FPS so it will look like I chose the slower architecture. So it will look like I chose the worse architecture so I won't be able to justify my use of YOLO.

I used YOLO as the APIs have much better documentation. Training an SSD turned out to be a pain because of lack of documentation for TensorFlow's API.

Please ask for any further details that are required. Thanks.",0,1,False,self,,,,,
554,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,9,aoyo9w,self.MachineLearning,[D] Question: Denormalize Mean Squared Error Output on Normalized Dataset?,https://www.reddit.com/r/MachineLearning/comments/aoyo9w/d_question_denormalize_mean_squared_error_output/,Ayruai,1549758348,"Say I had a deep neural network, but the inputs to the neural network are normalized such that they fall within -1 and 1. I train my neural network with loss function mean squared error; Do I report the value of the denormalized MSE value?",2,1,False,self,,,,,
555,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,9,aoywtw,self.MachineLearning,Model to remove applause or background music from audio?,https://www.reddit.com/r/MachineLearning/comments/aoywtw/model_to_remove_applause_or_background_music_from/,hanyuqn,1549759943,[removed],0,1,False,self,,,,,
556,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,9,aoyxfh,self.MachineLearning,[D] Which machine learning algorithm would you use to to generate a subject line to get attention on /r/MachineLearning ?,https://www.reddit.com/r/MachineLearning/comments/aoyxfh/d_which_machine_learning_algorithm_would_you_use/,wengchunkn,1549760053,,5,1,False,self,,,,,
557,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,10,aoz3n4,github.com,Can a Perceptron play a simple game?,https://www.reddit.com/r/MachineLearning/comments/aoz3n4/can_a_perceptron_play_a_simple_game/,atum47,1549761218,,0,1,False,default,,,,,
558,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,10,aoz4b3,self.MachineLearning,[D] Do you know of research specifically using hyperbolic arcsine as an activation function?,https://www.reddit.com/r/MachineLearning/comments/aoz4b3/d_do_you_know_of_research_specifically_using/,SetOfAllSubsets,1549761349,"Some of the advantages of the usual sigmoid activation functions tanh and logistic are that they can't blow up and they have continuous bounded derivatives. Their disadvantages are that they saturate easily, the have vanishing gradients on both sides, and the logistic function isn't 0-centered. 

The half-linear functions  ReLU and softplus on the other hand don't have a near-constant gradient on the right side, which can make training faster, but they can blow up easily.  ReLU has no derivative at 0 and softplus can't reach zero.

It seems like using asinh(x) = log(x + sqrt\[1 + x\^2\]) could be a middleground between the bounded sigmoids and half-linear functions. Asinh(x) has continuous bounded derivatives and asinh(x)\~=x near x=0. The gradient asymptotically approaches 0, meaning very large weight values will be less sensitive to change, but it approaches 0 it much slower than the gradient of tanh meaning it won't vanish so quickly. It is unbounded on both sides meaning it won't saturate, but it's bounded by log(x) instead of x so it won't blow up as easily. One disadvantage is that it may take more time to calculate than the others (especially  ReLU ). Another possible disadvantage is that it's anti-symmetric like tanh instead of asymmetric like ReLU. 

I haven't been able to find any specific references to it being used or studied in comparison to the others. Does anyone know of something that uses/discusses an activation function with similar properties? SReLU and SiLU are the closest things I've found.",15,1,False,self,,,,,
559,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,10,aozgpi,github.com,NSFW Detector Model - Repo,https://www.reddit.com/r/MachineLearning/comments/aozgpi/nsfw_detector_model_repo/,GantMan,1549763850,,0,1,True,nsfw,,,,,
560,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,11,aozksk,blog.varunajayasiri.com,Standalone PPO implementation in PyTorch,https://www.reddit.com/r/MachineLearning/comments/aozksk/standalone_ppo_implementation_in_pytorch/,mlvpj,1549764653,,0,1,False,default,,,,,
561,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,11,aozrqc,self.MachineLearning,[D] Feature normalization is usually done but what about label normalization in the case of multi-task regression?,https://www.reddit.com/r/MachineLearning/comments/aozrqc/d_feature_normalization_is_usually_done_but_what/,grandkz,1549766066,"Feature normalization is usually a must before training a model. However, I have read a few sources (and stackexchange answers) that says that label normalization is not required and might in fact be bad. However, what if we have a multi-task regression problem, for example with 3 tasks a, b, and c, with usual values of about 0.01, 500, 10. 

&amp;#x200B;

Should the labels be normalized such that a, b, and c are all within 0 to 1? This is because

1) It will help during the training process as it allows a, b, and c to have equal importance when updating the weights and bias? Or does the difference in magnitude not matter at all during training?

2) It will help in computing a fairer measure of how good the model is when evaluating on the test data set? Since b is so much larger than c, the overall mean square error is dominated by b. So if the labels are normalized, it will make a, b, and c all have equal importance, allowing a better metric to optimize if one were to perform a hyper-parameter optimization of the model? Or does it not matter again?

&amp;#x200B;

Thank you!

&amp;#x200B;

&amp;#x200B;",4,1,False,self,,,,,
562,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,12,ap0fg2,self.MachineLearning,[D] Are machine learning algorithms heading toward dead ends?,https://www.reddit.com/r/MachineLearning/comments/ap0fg2/d_are_machine_learning_algorithms_heading_toward/,wengchunkn,1549771002,"What are the promising fields in artificial intelligence or machine learning?

What are the dead ends that you would definitely avoid?

What are your predictions for AI and ML within the next 5 years?

What breakthrough do you think is needed to achieve Artificial General Intelligence?",3,1,False,self,,,,,
563,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,13,ap0n1t,self.MachineLearning,"Is my GAN doing ""Catastrophic forgetting""?",https://www.reddit.com/r/MachineLearning/comments/ap0n1t/is_my_gan_doing_catastrophic_forgetting/,AlexGuilBot,1549772642,[removed],0,1,False,self,,,,,
564,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,13,ap0p1p,youtube.com,[P] Micromachine.AI: Teach an AI how to drive in C# (YouTube video),https://www.reddit.com/r/MachineLearning/comments/ap0p1p/p_micromachineai_teach_an_ai_how_to_drive_in_c/,cbovar,1549773087,,0,1,False,default,,,,,
565,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,13,ap0v51,self.MachineLearning,"Deep Learning for High-Resolution Land Cover Mapping - Nebojsa Jojic, Microsoft Research",https://www.reddit.com/r/MachineLearning/comments/ap0v51/deep_learning_for_highresolution_land_cover/,samlanka,1549774467,[removed],0,1,False,self,,,,,
566,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,14,ap0xt5,youtube.com,"Deep Learning for High-Resolution Land Cover Mapping - Nebojsa Jojic, Microsoft Research",https://www.reddit.com/r/MachineLearning/comments/ap0xt5/deep_learning_for_highresolution_land_cover/,samlanka,1549775072,,0,1,False,https://b.thumbs.redditmedia.com/odNqE6bmAHZ6g0blX-g_G3ewOiowDR-ETmz0zgdO8ms.jpg,,,,,
567,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,14,ap0y4n,self.MachineLearning,Are there any apps that scan the internet for a similar face from uploading a photo?,https://www.reddit.com/r/MachineLearning/comments/ap0y4n/are_there_any_apps_that_scan_the_internet_for_a/,JasonJost,1549775139,[removed],0,1,False,self,,,,,
568,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,14,ap12iw,self.MachineLearning,[P] A machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/ap12iw/p_a_machine_learning_game_ive_been_working_on/,twm7,1549776181,"Wasn't sure where to post this as I'm still working on it but wanted to put it out there to get any useful feedback or thoughts from the experts. It's basically a game similar to 20 Questions (or Animal, Vegetable, Mineral) that attempts to ask you questions to work out an object you are thinking about. You can think of everyday items (animals, household objects, food, quite a bit of other stuff etc) and it has 30 questions to try and guess the item. I've been working on it for a while but not sure what to do next so interested to hear anyone's thoughts...

The link for anyone that wants to try it out is [incredicat.com](https://incredicat.com/)

Thanks in advance!",31,1,False,self,,,,,
569,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,15,ap1cpl,self.MachineLearning,[D] Best way to store labels for every frame in a video?,https://www.reddit.com/r/MachineLearning/comments/ap1cpl/d_best_way_to_store_labels_for_every_frame_in_a/,murrdpirate,1549778671,"I will soon have thousands of fully-labeled video clips. Each frame will have a corresponding label representing the bounding boxes of objects. The clips are 2 min long and 60 fps.

Is there a common approach to storing this kind of data for machine learning? There are a couple of options I'm considering:

1. Just extract all the frames from each clip and save them as jpgs. Have filenames match a corresponding label file. The downside of this approach is the sheer size of the data, easily several TB. I guess HDs are cheap, but it'd make sharing the data with others a bit challenging.
2. Keep the clips in mp4 format. Embed the label inside the mp4 as a subtitle stream (or embed the name of the label files). This would reduce the size by an order of magnitude, but I'm not sure if I can guarantee how well the subtitle stream matches the video frames.",10,1,False,self,,,,,
570,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,15,ap1g1c,self.MachineLearning,Ddos detection using ML,https://www.reddit.com/r/MachineLearning/comments/ap1g1c/ddos_detection_using_ml/,harsh52,1549779516,[removed],0,1,False,self,,,,,
571,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,16,ap1y18,self.MachineLearning,Emulator for Road signals,https://www.reddit.com/r/MachineLearning/comments/ap1y18/emulator_for_road_signals/,caldenrodrigues,1549784631,[removed],0,1,False,self,,,,,
572,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,17,ap25bm,i.redd.it,[R] Beholder-GAN: Generation and Beautification of Facial Images with Conditioning on Their Beauty Level https://arxiv.org/abs/1902.02593,https://www.reddit.com/r/MachineLearning/comments/ap25bm/r_beholdergan_generation_and_beautification_of/,FSMer,1549786924,,0,1,False,default,,,,,
573,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,20,ap35od,self.MachineLearning,[D] Neural Networks Formalization,https://www.reddit.com/r/MachineLearning/comments/ap35od/d_neural_networks_formalization/,freechoice,1549798265,"I am looking for a mathematical formalization of a neural networks. Could you please recommend any papers/books that deals with this problem? Ideally if they deal also with definitions of RNNs, CNNs, etc. Of course it does not have to be one resource for all of those.",5,1,False,self,,,,,
574,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,21,ap3df7,self.MachineLearning,Need help with predicting stock out and Safety Stocks - Inventory Management,https://www.reddit.com/r/MachineLearning/comments/ap3df7/need_help_with_predicting_stock_out_and_safety/,Shiva_Ved,1549800531," 

Hi All,

I'm working on a case study where I have to come up with a solution to minimize the stock outs in a distribution centre. Has anyone worked in supply chain related problems? it would be of great help if I can get some guidance.",0,1,False,self,,,,,
575,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,21,ap3jfk,self.MachineLearning,Lambda Deep Learning Discord &amp; Deep Learning Forum,https://www.reddit.com/r/MachineLearning/comments/ap3jfk/lambda_deep_learning_discord_deep_learning_forum/,That_Actuator,1549802180,[removed],0,1,False,self,,,,,
576,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,22,ap3z3w,self.MachineLearning,[D] Any universities/research groups/researchers that focus on AutoML?,https://www.reddit.com/r/MachineLearning/comments/ap3z3w/d_any_universitiesresearch_groupsresearchers_that/,zjplab,1549806110,I am first year master student in CS and  wish to continue to obtain a PhD degree especially in  Automatic Machine Learning area.  I know Frank Hutter in Uni Freiburg and his famous auto-sklearn. What about other groups/universities that do similar area of research that I can potentially join later?,6,1,False,self,,,,,
577,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,22,ap40i6,github.com,"""Machine Learning from scratch!""",https://www.reddit.com/r/MachineLearning/comments/ap40i6/machine_learning_from_scratch/,dronecub,1549806421,,0,1,False,default,,,,,
578,MachineLearning,t5_2r3gv,2019-2-10,2019,2,10,23,ap4knu,self.MachineLearning,[D] Are we expected to solve hard programming challenges to work in ML/DL industry?,https://www.reddit.com/r/MachineLearning/comments/ap4knu/d_are_we_expected_to_solve_hard_programming/,tw01010100,1549810646,"Lots of tech companies hire people based on programming challenges. People [quit their job](https://medium.freecodecamp.org/why-i-studied-full-time-for-8-months-for-a-google-interview-cc662ce9bb13) to dedicate months preparing for an interview. [Cracking the Code Interview](https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/0984782850) is the #1 best-selling book in programming and software development. Even if a person has a strong experience in developing real-world software, they [may fail an interview](https://twitter.com/mxcl/status/608682016205344768) because of a CS/coding question. 

I understand that there is a correlation between people that can solve these challenges and people that can do real-world software engineering, but I believe that this correlation is not as strong as companies believe. People may just overfit to these challenges and get a job without getting their hands dirty in real software development.

Enough of my rant.

I work as a ML/CV software engineer in the same company for five years. I did not have a single interview in the meantime. I'm considering applying to another job and I'm worried about the coding challenge interviews. I can produce well-written and maintainable code. I can understand, discuss, and implement recent advances in computer vision. But give me a simple data-structure interview challenge and I will struggle to solve it without looking at some references.

Am I expect to solve hard programming challenges during interviews for ML/CV positions?",203,1,False,self,,,,,
579,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,0,ap4lxf,medium.com,"[D] A Gentle Introduction to Graph Neural Network (Basics, DeepWalk, and GraphSage)",https://www.reddit.com/r/MachineLearning/comments/ap4lxf/d_a_gentle_introduction_to_graph_neural_network/,steeveHuang,1549810885,,0,1,False,default,,,,,
580,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,0,ap4o1s,self.singularity,"""Man in the Mirror"" Experiment: Bootstrapping AI with SAIGON Kotlin OpenCV",https://www.reddit.com/r/MachineLearning/comments/ap4o1s/man_in_the_mirror_experiment_bootstrapping_ai/,wengchunkn,1549811283,,0,1,False,default,,,,,
581,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,0,ap523h,self.MachineLearning,"Is there a word for ML classification methods that can find a complex nonlinear decision boundary in the training data (e.g., neural nets, classification trees) that differentiates them from methods that can find nonlinear decision boundaries, just not complex ones (e.g., logistic regression)?",https://www.reddit.com/r/MachineLearning/comments/ap523h/is_there_a_word_for_ml_classification_methods/,synysterbates,1549813771,[removed],0,1,False,self,,,,,
582,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,0,ap53fx,self.MachineLearning,Do I have any chance?,https://www.reddit.com/r/MachineLearning/comments/ap53fx/do_i_have_any_chance/,peeerooo,1549814001,[removed],0,1,False,self,,,,,
583,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,1,ap5bce,self.MachineLearning,Harnessing Machine Learning in Payments,https://www.reddit.com/r/MachineLearning/comments/ap5bce/harnessing_machine_learning_in_payments/,andrea_manero,1549815346,[removed],0,1,False,self,,,,,
584,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,1,ap5eeh,self.MachineLearning,Proximal Policy Optimization with Beta distribution - Value goes to infinity,https://www.reddit.com/r/MachineLearning/comments/ap5eeh/proximal_policy_optimization_with_beta/,unieye,1549815860,[removed],0,1,False,self,,,,,
585,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,2,ap5xjt,self.MachineLearning,"I have come up with an activation function, if you would like to try it you're welcome to [P/D]",https://www.reddit.com/r/MachineLearning/comments/ap5xjt/i_have_come_up_with_an_activation_function_if_you/,Foulgaz3,1549818938,"I'm still working on testing it on different projects, however, I was able fit on regular functions such as x^2 -5 and 5^x with only two neurons


It is a modification between PReLU and swish

So, the function is defined by:

f(x) = XSB

Where

X = x
S = sigmoid(x)
B = b * exp(-x) + 1

b is updated during training, however, I suppose you could make it a constant to use it like Leaky ReLU.

I fit it to the normal functions with the simple model y = a_1 * f(a_2 * x + b_2) + b_1, and I trained it on 20 datapoints, x =-10...10. The final error for y = x^2 at x = 100 was .013.

Obviously the function is  pretty expensive, but considering I was not able to get the same level of accuracy with 200 neurons and 50 data 
points using ReLU, I think it might be worthwhile to try it.

I'm going to be trying to do some more scientific studying and experimenting with the function, but thought I should share it so I could get some feedback on what to try or watch out for from those with more experience than I.

What are your thoughts on it and do you have any recommendations? Is this really a new function or is it a sign of my negligence? What should be it's name?

",0,1,False,self,,,,,
586,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,2,ap6231,youtu.be,"TIL that Geoff Hinton have been carrying a back problem that makes him work in a standing posture for the last 12 years, he also commute from place to place by walking.",https://www.reddit.com/r/MachineLearning/comments/ap6231/til_that_geoff_hinton_have_been_carrying_a_back/,whiletruelearn,1549819638,,0,1,False,https://b.thumbs.redditmedia.com/lKiCVrvqqdksySmZuiFsVbUAJPCScZ96yGTSrsh9s0I.jpg,,,,,
587,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,2,ap66gq,self.MachineLearning,"[P] A new activation function. It's expensive, but seemingly pretty powerful",https://www.reddit.com/r/MachineLearning/comments/ap66gq/p_a_new_activation_function_its_expensive_but/,Foulgaz3,1549820339,"I'm still working on testing it on different projects, however, I was able fit on regular functions such as x^2 -5 or 5^x with only two neurons

It is a mix between PReLU and Swish, derived using LERP with the sigmoid function as alpha

So, the function is defined by:

f(x) = XSB

Where

X = x
S = sigmoid(x)
B = b * exp(-x) + 1

b is updated during training, however, I suppose you could make it a constant to use it like Leaky ReLU.

I fit it to the normal functions with the simple model y = a_1 * f(a_2 * x + b_2) + b_1, and I trained it on 20 datapoints, x =-10...10. The final error for y = x^2 at x = 15 was .013

Obviously the function is  pretty expensive, but considering I was not able to get the same level of accuracy with 200 neurons and 50 data 
points using ReLU, I think it might be worthwhile to try it.

I'm going to be trying to do some more scientific studying and experimenting with the function, but thought I should share it so I could get some feedback on what to try or watch out for from those with more experience than I.

What are your thoughts on it and do you have any recommendations? Is this really a new function or is it a sign of my negligence? What should be it's name?

",30,1,False,self,,,,,
588,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,2,ap6dsq,self.MachineLearning,RELU vs ResNets,https://www.reddit.com/r/MachineLearning/comments/ap6dsq/relu_vs_resnets/,DarkJPMC,1549821517,"I'm fairly new to Machine Learning, but have a decent understanding of the concepts.

&amp;#x200B;

Now, both RELU-type functions and ResNets combat the vanishing gradient problem. My question is, why use one instead of the other and why does DeepMind use both in AlphaZero? Isn't it redundant? Sorry if this has been answered already I can't seem to find a definite answer to this.

&amp;#x200B;

Thanks",0,1,False,self,,,,,
589,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,3,ap70ol,self.MachineLearning,Where are the backend C++ Algorithm Implementations in TensorFlow Source Code?,https://www.reddit.com/r/MachineLearning/comments/ap70ol/where_are_the_backend_c_algorithm_implementations/,mariebks,1549825153,[removed],0,1,False,self,,,,,
590,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,4,ap75ui,self.MachineLearning,What's the best programming language for ML?,https://www.reddit.com/r/MachineLearning/comments/ap75ui/whats_the_best_programming_language_for_ml/,HealthyAtomBomb,1549825941,[removed],0,1,False,self,,,,,
591,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,4,ap79w1,self.MachineLearning,"Implemented vanilla ANN, what's next ?",https://www.reddit.com/r/MachineLearning/comments/ap79w1/implemented_vanilla_ann_whats_next/,WERE_CAT,1549826549,[removed],0,1,False,self,,,,,
592,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,4,ap7d37,github.com,PyTorch implementation of a character based Convolutional Neural Network for text,https://www.reddit.com/r/MachineLearning/comments/ap7d37/pytorch_implementation_of_a_character_based/,ahmedbesbes,1549827050,,0,1,False,https://b.thumbs.redditmedia.com/2Nsg6jfwlvB1tnRtAZ9X930pIVQ9SCJ92JveZT7zQII.jpg,,,,,
593,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,4,ap7enx,self.MachineLearning,[D] Career advise for Masters Student Specializing in ML,https://www.reddit.com/r/MachineLearning/comments/ap7enx/d_career_advise_for_masters_student_specializing/,sgondala2,1549827278,"I would be starting my Masters in CS from Georgia Tech this fall. The main reason for joining Masters is to learn ML and transition into applied ML roles. What should I do to optimize my learning and improve my chances of landing good jobs once I finish Masters? Would I need a thesis/projects/relevant coursework?

Also, ML is a vast and fast-moving field. How to keep myself relevant/updated with the latest advances as a student?",7,1,False,self,,,,,
594,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,4,ap7jz8,self.MachineLearning,[D] Looking for work on deep learning Question Answer systems that utilize knowledge graphs/common sense,https://www.reddit.com/r/MachineLearning/comments/ap7jz8/d_looking_for_work_on_deep_learning_question/,aDutchofMuch,1549828045,"Recently I've been looking for work that utilizes knowledge graphs (such as Open Mind Common Sense) and deep learning for query parsing, link inference, and and answer construction to build general question answer systems. So far, I really haven't found much, beyond some simple query parsers like the one at https://medium.com/octavian-ai/answering-english-questions-using-knowledge-graphs-and-sequence-translation-2acbaa35a21d

I've recently seen TuckER posted here https://arxiv.org/pdf/1901.09590.pdf, which was for link inference, but this isn't quite the type of work I'm looking for. Anyone have any pointers?",5,1,False,self,,,,,
595,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,5,ap7riw,self.MachineLearning,Sklearn vs Tensorflow vs Keras vs Pytorch,https://www.reddit.com/r/MachineLearning/comments/ap7riw/sklearn_vs_tensorflow_vs_keras_vs_pytorch/,DakotaFelspar,1549829149,[removed],0,1,False,self,,,,,
596,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,6,ap8ctk,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 56,https://www.reddit.com/r/MachineLearning/comments/ap8ctk/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1549832406,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

Besides that, there are no rules, have fun.",3,1,False,self,,,,,
597,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,6,ap8l0d,i.redd.it,My implementation of 3 NLP models for text classification in Pytorch and Tensorflow,https://www.reddit.com/r/MachineLearning/comments/ap8l0d/my_implementation_of_3_nlp_models_for_text/,1991viet,1549833676,,1,1,False,default,,,,,
598,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,6,ap8l7l,self.MachineLearning,Public Anaphora/Coreference datasets specific to Novels/Fiction/Literature?,https://www.reddit.com/r/MachineLearning/comments/ap8l7l/public_anaphoracoreference_datasets_specific_to/,Skeptic95,1549833705,[removed],0,1,False,self,,,,,
599,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,6,ap8tef,self.MachineLearning,[D] How do you deal with timed data challenges?,https://www.reddit.com/r/MachineLearning/comments/ap8tef/d_how_do_you_deal_with_timed_data_challenges/,sadboijoy,1549835023,"I recently completed a timed 3 hour coding challenge with a company.

The problem was that most of my analysis was shit. I spent maybe a good hour and half familiarizing myself with the data i.e. making distribution charts, doing "".describe()"", etc. I basically wasted a good chunk of my time trying to figure out if I could include more features, remove irrelevant observations ,etc. The dataset itself was also tricky so I was thinking of doing some other loss function i.e. the median was zero when the variable spanned \[0, inf\]. I was thinking of maybe trying out robust regression for this problem.

Before I started modeling, I then wanted to test if the assumptions were valid to employ regression. All of this was in one big ass lazy notebook.

In the middle of all of this, I noticed I only had like 20 min. left and I didn't write anything down. I tried to clean up my notebook and made some random comments here and there. I did some garbage logistic regression and submitted it just as time was up. It's frustrating since I feel like I wasted a good majority of my time just thinking about the data. I'm 100% sure, I didn't pass.

What do you guys do?

What was the rookie mistake I made?

I NEVER want this to happen again.",8,1,False,self,,,,,
600,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,7,ap9100,self.MachineLearning,Transformation for Dependent variable with exponential distribution [D],https://www.reddit.com/r/MachineLearning/comments/ap9100/transformation_for_dependent_variable_with/,harsh_sahu,1549836235,"Hi Guys,

I am working on a problem where dependent variable is such that, around 70% of the values are '0' and the rest spreads exponentially reaching a maximum of around '50k'. I am using XgBoost and already getting decent performance but in need to improve it. I have already tried log, logit and boxcox transformation but they aren't working. Is there anything additional that can be done? Any transformation or some other preprocessing technique that i can try?
Any help would be highly appreciated :) ",0,1,False,self,,,,,
601,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,7,ap97sh,medium.com,Future of fragrance: Can AI disrupt an industry that is at the intersection of both art and,https://www.reddit.com/r/MachineLearning/comments/ap97sh/future_of_fragrance_can_ai_disrupt_an_industry/,CypherGray,1549837316,,0,1,False,default,,,,,
602,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,7,ap982e,self.MachineLearning,What are the top movies about artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/ap982e/what_are_the_top_movies_about_artificial/,Doctor_who1,1549837362,[removed],0,1,False,self,,,,,
603,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,8,ap9w73,self.MachineLearning,Solution to find patterns in time series data?,https://www.reddit.com/r/MachineLearning/comments/ap9w73/solution_to_find_patterns_in_time_series_data/,ExtremeEducation,1549841415,[removed],0,1,False,self,,,,,
604,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,8,apa4zf,youtu.be,Alarmingly Cute Home Robots you can Hold in your Hand Right Now!,https://www.reddit.com/r/MachineLearning/comments/apa4zf/alarmingly_cute_home_robots_you_can_hold_in_your/,nix_zu_verlian,1549842971,,0,1,False,https://b.thumbs.redditmedia.com/b9Aj0jGmI2fSjPRTgAjpo24ErUgT5igEtTvLsn0y4aQ.jpg,,,,,
605,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,9,apaaht,youtu.be,How machine can learn making sequential decisions...check this video tutorial on Reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/apaaht/how_machine_can_learn_making_sequential/,Riturajkaushik,1549843959,,0,1,False,https://a.thumbs.redditmedia.com/lbKBpUdtdFKEgysL1Pi4XCwKNc2ZsHpPROuo5GdrHM8.jpg,,,,,
606,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,9,apabbx,self.MachineLearning,Are there any non convolutional deep models?,https://www.reddit.com/r/MachineLearning/comments/apabbx/are_there_any_non_convolutional_deep_models/,tornado28,1549844094,"Specifically non convolutional networks that are more than say 10 layers deep and achieve state of the art performance on some task. Or does the phrase ""deep learning"" really apply best only to convolutional networks?",0,1,False,self,,,,,
607,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,10,apar68,self.MachineLearning,[D] Thoughts on the BagNet Paper,https://www.reddit.com/r/MachineLearning/comments/apar68/d_thoughts_on_the_bagnet_paper/,hardmaru,1549846980,"*Some thoughts on the interesting [BagNet](https://openreview.net/forum?id=SkfMWhAqYQ) paper (accepted at ICLR 2019) currently being circulated around the Machine Learning Twitter Community.*

https://blog.evjang.com/2019/02/bagnet.html
",18,1,False,self,,,,,
608,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,10,apau7n,self.MachineLearning,"[P] Introducing Silas: a fast, accurate and dependable binary classifier",https://www.reddit.com/r/MachineLearning/comments/apau7n/p_introducing_silas_a_fast_accurate_and/,dev-depintel,1549847539,"Hi fellow data enthusiasts,

&amp;#x200B;

For a few months now, we've been working on a new binary classifier based on ensemble decision trees with a strong emphasis on dependability.  Today, we are proud to present to you our new classification tool -- Silas, now available on Windows, Mac and Linux for free! 

&amp;#x200B;

Fast and accurate, Silas provides you with the ability to formally verify properties of its predictive models as well as the ability to enforce them during learning.

&amp;#x200B;

We invite all data enthusiasts with structured datasets to try it out! We greatly appreciate all your feedback! 

&amp;#x200B;

[Download](https://www.depintel.com/silas_download.html)

[Documentation](https://www.depintel.com/documentation/_build/html/index.html)

[Get started](https://www.depintel.com/documentation/_build/html/tutorials/basic.html)",19,1,False,self,,,,,
609,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,11,apboby,self.MachineLearning,"[Discussion] Long work hours becoming normal and glorified, dangers and ways to counteract this",https://www.reddit.com/r/MachineLearning/comments/apboby/discussion_long_work_hours_becoming_normal_and/,GeekMonolith,1549853352,I've had this subject on my mind for quite a while and want to hear more people's perspective on this. ,33,1,False,self,,,,,
610,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,12,apbsfv,self.MachineLearning,RecSys Challenge 2018 Dataset,https://www.reddit.com/r/MachineLearning/comments/apbsfv/recsys_challenge_2018_dataset/,Shoddy_Researcher,1549854131,[removed],0,1,False,self,,,,,
611,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,12,apc3fe,mlcourse.ai,Open Machine Learning Course by Bunch of Kaggle Grandmasters,https://www.reddit.com/r/MachineLearning/comments/apc3fe/open_machine_learning_course_by_bunch_of_kaggle/,rohan36,1549856286,,0,1,False,default,,,,,
612,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,12,apc6ee,self.MachineLearning,SWEs in Machine Learning Interviews,https://www.reddit.com/r/MachineLearning/comments/apc6ee/swes_in_machine_learning_interviews/,spoiltForChoice,1549856864,[removed],0,1,False,self,,,,,
613,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,13,apcddt,self.MachineLearning,Hey I'm trying to make my own neural network and it produces the exact same output irrespective of the input,https://www.reddit.com/r/MachineLearning/comments/apcddt/hey_im_trying_to_make_my_own_neural_network_and/,afterburners_engaged,1549858243,"So as I've said im making a NN and it works for simple problems like XOR and AND, But ive been trying to make it recognise numbers. By plotting them onto a white background and then propagating it through the network everytime I run this this generates the exact same output irrespective of the input. Do you have any idea as to why this could be happening?

PS: The MNIST data set produces the same issue 

You can find my code here:[https://github.com/arunkmani/project](https://github.com/arunkmani/project)",0,1,False,self,,,,,
614,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,14,apcvre,openreview.net,[R] [D] Transferring Knowledge to Smaller Network With Class Distance Loss,https://www.reddit.com/r/MachineLearning/comments/apcvre/r_d_transferring_knowledge_to_smaller_network/,PK_thundr,1549861959,,0,1,False,default,,,,,
615,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,15,apdbtt,self.MachineLearning,Data exploration (Microsoft SQL),https://www.reddit.com/r/MachineLearning/comments/apdbtt/data_exploration_microsoft_sql/,romnac,1549865392,[removed],0,1,False,self,,,,,
616,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,15,apdk1y,arxiv.org,[R] Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning,https://www.reddit.com/r/MachineLearning/comments/apdk1y/r_dropout_is_a_special_case_of_the_stochastic/,_bearface,1549867293,,17,1,False,default,,,,,
617,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,16,apdpar,self.MachineLearning,[D] Trump will launch American AI Initiative with an executive order,https://www.reddit.com/r/MachineLearning/comments/apdpar/d_trump_will_launch_american_ai_initiative_with/,baylearn,1549868554,"Regardless of your politics, this news may be relevant to American labs and research institutions working on machine learning.

MIT Tech Review article: [Trump has a plan to keep America first in artificial intelligence](https://www.technologyreview.com/s/612926/trump-will-sign-an-executive-order-to-put-america-first-in-artificial-intelligence/).

WIRED: [Trump's plan to keep America first in AI](https://www.wired.com/story/trumps-plan-keep-america-first-ai/)
",141,1,False,self,,,,,
618,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,16,apdsfy,self.MachineLearning,Caffe2 Conv3D- input layout,https://www.reddit.com/r/MachineLearning/comments/apdsfy/caffe2_conv3d_input_layout/,albert1905,1549869350,[removed],0,1,False,self,,,,,
619,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,16,apdsmu,self.MachineLearning,[P] Deep Learning on Healthcare Lecture Series,https://www.reddit.com/r/MachineLearning/comments/apdsmu/p_deep_learning_on_healthcare_lecture_series/,hiconcep,1549869394,"Ive picked up my first english lecture theme on deep learning on healthcare. Actually, this lecture was given to Google, Sloan School at MIT, Center for Data Science at NYU and Massachusetts General Hospital. And also I spoke at Nvidia GTC Korea last year on this themes. This is more like MBA style lecture based on my experiences especially on my investment portfolio companies. Ill try to explain what is the reality and myths when you guys try to do something using deep learning on healthcare. This lecture will be divided into several clips as a series lecture.

&amp;#x200B;

[Deep Learning on Healthcare (1)](https://www.youtube.com/watch?v=k0LacC4hyY8&amp;index=23&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee)",2,1,False,self,,,,,
620,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,16,apdsny,self.MachineLearning,Can someone help me with installing LIBSVM in Octave for Mac,https://www.reddit.com/r/MachineLearning/comments/apdsny/can_someone_help_me_with_installing_libsvm_in/,shreykhetrapal,1549869403,[removed],0,1,False,self,,,,,
621,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,17,apeasa,ai-benchmark.com,"TensorFlow Lite performance: Snapdragon 855, MediaTek P90, Kirin 980 or Exynos 9820 - which AI chip is faster?",https://www.reddit.com/r/MachineLearning/comments/apeasa/tensorflow_lite_performance_snapdragon_855/,aiff22,1549874552,,0,1,False,https://b.thumbs.redditmedia.com/hWO98vxz4esuLCakPZ74jtzvuFJVI7YU09GLM5Gwp4k.jpg,,,,,
622,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,17,apedlq,github.com,"Drench yourself in Deep Learning, Reinforcement Learning, Machine Learning, Computer Vision, and NLP by learning from these exciting lectures!!",https://www.reddit.com/r/MachineLearning/comments/apedlq/drench_yourself_in_deep_learning_reinforcement/,rohan36,1549875436,,0,1,False,https://b.thumbs.redditmedia.com/TgRgEVjTZpmtcS7Dzg2tiOKv2QM3_Mw2w3agznnHPBc.jpg,,,,,
623,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,18,apelru,forbes.com,5 Artificial Intelligence Trends To Watch Out For In 2019,https://www.reddit.com/r/MachineLearning/comments/apelru/5_artificial_intelligence_trends_to_watch_out_for/,kirankarthick2019,1549877735,,0,1,False,default,,,,,
624,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,18,apem23,self.MachineLearning,"[N] TensorFlow Lite performance: Snapdragon 855, MediaTek P90, Kirin 980 or Exynos 9820 - which AI chip is faster?",https://www.reddit.com/r/MachineLearning/comments/apem23/n_tensorflow_lite_performance_snapdragon_855/,aiff22,1549877811,"AI Benchmark presented the results of testing float and quantized performance of all recently released mobile chipsets with AI accelerators:

[http://ai-benchmark.com/ranking\_processors.html](http://ai-benchmark.com/ranking_processors.html)

Here is a description of the tests:

[http://ai-benchmark.com/tests.html](http://ai-benchmark.com/tests.html)

And here are the findings:

[http://ai-benchmark.com/news.html](http://ai-benchmark.com/news.html)

A brief summary of the results:

* Snapdragon 855 and MediaTek P90  -  comparable 6-15X acceleration for float and quantized networks
* HiSilicon Kirin 980  -  the same acceleration for float networks, somewhat smaller - for quantized models
* Snapdragon 845  -  10% slower float performance than in SDM855, 20-60% slower int speed
* HiSilicon Kirin 970  -  30-50% slower than Kirin 980
* Google Pixel Visual Core  -  due to its insufficient performance Google decided not to release any SDK for developers, so cannot be used by anybody except Google
* Samsung Exynos 9810  -  the same situation as with Google: no SDK or specs were released, all computations on CPU
* Samsung Exynos 9820  -  official results will come soon, the results of prototypes are yet mixed",4,1,False,self,,,,,
625,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,19,apevon,arxiv.org,[1902.02947] Understanding the One-Pixel Attack: Propagation Maps and Locality Analysis,https://www.reddit.com/r/MachineLearning/comments/apevon/190202947_understanding_the_onepixel_attack/,ihaphleas,1549880341,,2,1,False,default,,,,,
626,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,19,apf0tb,self.MachineLearning,Buy Briquetting Machine In USA,https://www.reddit.com/r/MachineLearning/comments/apf0tb/buy_briquetting_machine_in_usa/,ecostan1,1549881719,[removed],2,1,False,self,,,,,
627,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,20,apf6d9,/r/MachineLearning/comments/apf6d9/facial_recognition_app_built_an_opencv_model_to/,Facial Recognition App - Built an OpenCV model to do face detection and classification at Optisol with my interns. Testing and having fun with it here,https://www.reddit.com/r/MachineLearning/comments/apf6d9/facial_recognition_app_built_an_opencv_model_to/,Optisoldatalabs,1549883129,,0,1,False,https://b.thumbs.redditmedia.com/uVdjMAkUquq1-BEZf5AyY3Wgov5Jdb_lbwB4jZDyAtc.jpg,,,,,
628,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,20,apfacn,blockdelta.io,Advancements in AI Revolutionizing the Future,https://www.reddit.com/r/MachineLearning/comments/apfacn/advancements_in_ai_revolutionizing_the_future/,BlockDelta,1549884057,,0,1,False,https://b.thumbs.redditmedia.com/4lZEOR1k6zRqXM-CRi-IrIQGCNhBDMNEfz09DfqUAwQ.jpg,,,,,
629,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,21,apfl8v,github.com,[P] Kitkopt: Simple bayesian optimiser for black box functions implemented in python.,https://www.reddit.com/r/MachineLearning/comments/apfl8v/p_kitkopt_simple_bayesian_optimiser_for_black_box/,party-horse,1549886605,,0,1,False,default,,,,,
630,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,21,apfobj,i.redd.it,Fits here?,https://www.reddit.com/r/MachineLearning/comments/apfobj/fits_here/,UnlikelyYesterday,1549887289,,0,1,False,default,,,,,
631,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,21,apfq78,youtube.com,SingularityNET (AGI) Beta Platform Open For The Public Today,https://www.reddit.com/r/MachineLearning/comments/apfq78/singularitynet_agi_beta_platform_open_for_the/,lisamatney,1549887677,,0,1,False,default,,,,,
632,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,21,apfu5c,youtube.com,Tools to Scale Your Production Machine Learning,https://www.reddit.com/r/MachineLearning/comments/apfu5c/tools_to_scale_your_production_machine_learning/,NuEd_Fernandes,1549888542,,0,1,False,https://b.thumbs.redditmedia.com/7Ppo26inBEItDpWV-TatE3O8rlhVCgYOW0WsJLVN7gs.jpg,,,,,
633,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,21,apfysw,wired.com,Google and Microsoft Warn That AI May Do Dumb Things,https://www.reddit.com/r/MachineLearning/comments/apfysw/google_and_microsoft_warn_that_ai_may_do_dumb/,Ejil23,1549889557,,0,1,False,https://b.thumbs.redditmedia.com/7mcrvA-O3yara2hx3lOv7JnmLe5vNMZhBNI11CDwMNc.jpg,,,,,
634,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,21,apfzh8,self.MachineLearning,What is the current best technique for speaker-independent voice conversion?,https://www.reddit.com/r/MachineLearning/comments/apfzh8/what_is_the_current_best_technique_for/,heikir11,1549889693,Looking for recent papers which focus on multimodal speech -&gt; speech conversion where the input is unseen voice and output either pretrained or novel voice(conversion does not require training a new network).,1,1,False,self,,,,,
635,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,21,apg0l9,i.redd.it,Just AI things!,https://www.reddit.com/r/MachineLearning/comments/apg0l9/just_ai_things/,harrshjain,1549889949,,0,1,False,https://b.thumbs.redditmedia.com/F7MKy5ekz-Jke_3JuW_Owwn6rbuEwrJnAkQgVcvOBSI.jpg,,,,,
636,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,22,apg17o,self.MachineLearning,Youngsters,https://www.reddit.com/r/MachineLearning/comments/apg17o/youngsters/,chessbucket,1549890082,[removed],0,1,False,self,,,,,
637,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,22,apg3t1,towardsdatascience.com,Generating Simpsons cartoons via GAN,https://www.reddit.com/r/MachineLearning/comments/apg3t1/generating_simpsons_cartoons_via_gan/,logan-diamond,1549890620,,0,1,False,default,,,,,
638,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,22,apg5f6,self.MachineLearning,[P] V100 server on-prem vs p3 instance total cost of ownership comparison,https://www.reddit.com/r/MachineLearning/comments/apg5f6/p_v100_server_onprem_vs_p3_instance_total_cost_of/,sabalaba,1549890941,"Lambda just finished up our Total Cost of Ownership (TCO) analysis of an 8 x V100 Server with NVLink and an AWS p3dn.24xlarge instance.

TL;DR

The V100 Server:

- Outperforms p3dn.24xlarge on all tested deep learning tasks.
- Is 2.6% faster than AWS for FP32 training with TensorFlow.
- Is 3.2% faster than AWS for FP16 training with TensorFlow.
- Has a Total Cost of Ownership (TCO) that's $69,441 less than a p3dn.24xlarge 3-year contract with partial upfront payment. Our TCO includes energy, hiring a part-time system administrator, and co-location costs. In addition, you still get value from the system after three years, the AWS instance.

Full results here:
https://lambdalabs.com/blog/8-v100-server-on-prem-vs-p3-instance-tco-analysis-cost-comparison/

Hope you find this useful, we're very happy to share these results with the community.",6,1,False,self,,,,,
639,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,22,apg8pr,self.MachineLearning,The best resource for learning neural networks,https://www.reddit.com/r/MachineLearning/comments/apg8pr/the_best_resource_for_learning_neural_networks/,fgafarov,1549891605,"[http://learn-neural-networks.com/](http://learn-neural-networks.com/)  Here you can find a lot of tutorials about neural networks, mashine learning, convolutional neural networks, lstm networks,keras.",0,1,False,self,,,,,
640,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,22,apgacq,self.MachineLearning,[R] [ICLR 2019] Unsupervised Adversarial Image Reconstruction,https://www.reddit.com/r/MachineLearning/comments/apgacq/r_iclr_2019_unsupervised_adversarial_image/,ChocoMoi,1549891929,"Open review: [https://openreview.net/forum?id=BJg4Z3RqF7](https://openreview.net/forum?id=BJg4Z3RqF7)

&amp;#x200B;

**Abstract:** We address the problem of recovering an underlying signal from lossy, inaccurate observations in an unsupervised setting. Typically, we consider situations where there is little to no background knowledge on the structure of the underlying signal, no access to signal-measurement pairs, nor even unpaired signal-measurement data. The only available information is provided by the observations and the measurement process statistics. We cast the problem as finding the \\textit{maximum a posteriori} estimate of the signal given each measurement, and propose a general framework for the reconstruction problem. We use a formulation of generative adversarial networks, where the generator takes as input a corrupted observation in order to produce realistic reconstructions, and add a penalty term tying the reconstruction to the associated observation. We evaluate our reconstructions on several image datasets with different types of corruptions. The proposed approach yields better results than alternative baselines, and comparable performance with model variants trained with additional supervision.",0,1,False,self,,,,,
641,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,22,apgfkc,phys.org,High Harmonic Generation at quantum level,https://www.reddit.com/r/MachineLearning/comments/apgfkc/high_harmonic_generation_at_quantum_level/,Ghosthaze1,1549892939,,0,1,False,default,,,,,
642,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,23,apglej,self.MachineLearning,[P] Beating the bookmaker with machine learning,https://www.reddit.com/r/MachineLearning/comments/apglej/p_beating_the_bookmaker_with_machine_learning/,richard-bartels,1549894055,"Bookmaker odds are such that the bookmaker guarantees themselves a net profit over time assuming that they assessed each player or teams win probabilities correctly. One could use machine learning to attempt to beat the bookmaker, however, one is unlikely to outperform the bookmaker significantly. For example, using binary-cross entropy to assess win probabilities in a head-to-head game will give you good accuracy in predicting the outcome of a game, but not good enough to beat the bookmaker.

We performed an exercise where instead of binary-cross entropy we use a custom loss function, that also accounts for the bookmaker odds in order to check if a particular game offers fair odds. This way we optimise on identifying faults in the bookmaker odds rather than on predicting the outcome of games as good as possible.

The full blog post can be found here:[https://medium.com/vantageai/beating-the-bookies-with-machine-learning-7b429a0b5980](https://medium.com/vantageai/beating-the-bookies-with-machine-learning-7b429a0b5980)",4,1,False,self,,,,,
643,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,23,apgmav,cmarix.com,Leverage The Hidden Potent Of Machine Learning For Mobile Apps,https://www.reddit.com/r/MachineLearning/comments/apgmav/leverage_the_hidden_potent_of_machine_learning/,Jennygordon,1549894223,,0,1,False,default,,,,,
644,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,23,apgt7g,self.MachineLearning,[N] whatever message,https://www.reddit.com/r/MachineLearning/comments/apgt7g/n_whatever_message/,uaaaaaaaa,1549895534,You can leave whatever you want for the artificial mind to read. New subreddit has been created.. r/readmeai,0,1,False,self,,,,,
645,MachineLearning,t5_2r3gv,2019-2-11,2019,2,11,23,aph1a0,youtube.com,Machine Learning Models with TensorFlow Using Amazon SageMaker - AWS Onl...,https://www.reddit.com/r/MachineLearning/comments/aph1a0/machine_learning_models_with_tensorflow_using/,NuEd_Fernandes,1549897075,,0,1,False,default,,,,,
646,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,0,aph24r,self.MachineLearning,[R] TDLS paper walkthrough: Learning Functional Casual Models with GANs (CGNN),https://www.reddit.com/r/MachineLearning/comments/aph24r/r_tdls_paper_walkthrough_learning_functional/,settinghead,1549897240,[removed],0,1,False,self,,,,,
647,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,0,aph2rr,self.MachineLearning,[R] TDLS paper walkthrough: Learning Functional Casual Models with GANs (CGNN),https://www.reddit.com/r/MachineLearning/comments/aph2rr/r_tdls_paper_walkthrough_learning_functional/,tdls_to,1549897347,"## Recording: [Part 1: lgorithm &amp; model walkthrough](https://www.youtube.com/watch?v=xVjX9293XkY), [Part 2: discussions](https://www.youtube.com/watch?v=Ox4nCGqQoqM)

Paper: [https://arxiv.org/abs/1709.05321](https://arxiv.org/abs/1709.05321)

Slides: [https://tdls.a-i.science/static/slides/20190121\_ChristopherAlert.pdf](https://tdls.a-i.science/static/slides/20190121_ChristopherAlert.pdf)

**Key takeaways:**

* Pros:  

   * CGNN can learn the structure of causal relationships between observed variables
   * Robust performance on real data or given a noisy skeleton of dependencies between variables
   * Provides a generative model to simulate interventions on one or more variables in a system and evaluate their impact
* Cons:  

   * Models highly sensitive to n\_h , the # neurons in each hidden layer in the causal mechanisms f\_i
   * Graph searching algorithm is time expensive and does not parallelize

Discussion points (part 2):

* Is it possible to have a better causal graph searching algorithm in score-based methods, e.g.,gradient free optimisation algorithms (simulated annealing and genetic algorithm) or Bayesian optimisation for better sampling?
* Are there any well-known graph structures that allow you to parallelize the searching algorithm by searching over subgraphs and optimizing local MMD\_k score?

Presenter: [Christopher Alert](https://www.linkedin.com/in/chrisalert/), data scientist from Bell Canada

Facilitator: [Masoud Hashemi](https://www.linkedin.com/in/masoud-hashemi-2b151635/) from RBC

For more details visit [https://tdls.a-i.science/#/events/2019-01-21](https://tdls.a-i.science/#/events/2019-01-21)",1,1,False,self,,,,,
648,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,0,aph7ps,deepsense.ai,"[N] AlphaStar beats human champions, robots learn to grasp and a Finnish way to make AI a commodity",https://www.reddit.com/r/MachineLearning/comments/aph7ps/n_alphastar_beats_human_champions_robots_learn_to/,AnnaKow,1549898192,,0,1,False,default,,,,,
649,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,0,aphbmk,self.MachineLearning,"It's ""easier"" to program AI to win at chess than to solve a CAPTCHA. What's this problem called?",https://www.reddit.com/r/MachineLearning/comments/aphbmk/its_easier_to_program_ai_to_win_at_chess_than_to/,0927351885333020,1549898884,[removed],0,1,False,self,,,,,
650,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,0,aphhfs,self.MachineLearning,Neural Net for time series stock/future predictions only using Keras and 1-minute data?,https://www.reddit.com/r/MachineLearning/comments/aphhfs/neural_net_for_time_series_stockfuture/,jamesbjoyce,1549899885,[removed],0,1,False,self,,,,,
651,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,0,aphkun,sigarch.org,[N] Challenges to reproduce computer architecture research: deep learning example,https://www.reddit.com/r/MachineLearning/comments/aphkun/n_challenges_to_reproduce_computer_architecture/,mllosab,1549900433,,0,1,False,https://b.thumbs.redditmedia.com/Qzht_mhoPtx74ndsaLaQTWvpHMo8CvkWW0rPhOYzPoQ.jpg,,,,,
652,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,1,aphvz0,self.MachineLearning,[CLS] token in Google BERT?,https://www.reddit.com/r/MachineLearning/comments/aphvz0/cls_token_in_google_bert/,fiddlewin,1549902229,"I've been playing around with the BERT model for a while and I'm trying to understand the necessity of the \[CLS\] token.

&amp;#x200B;

For Phase 1 of the pre-training (MLM), it doesn't seem necessary and for classification tasks a additional layer can be thrown in in training.

My understanding is that it is set up to be compatible with Phase 2 of the pre-training (next sentence) so that \[CLS\] can be used as an output without changing the overall structure of the network and hence Phase 1 and 2 and be integrated more seamlessly.

&amp;#x200B;

However, if all what I want is a classification task, do I really need Phase 2 (maybe results even better without Phase 2 or maybe I can get good results with less heads/layers?)? If that's the case, maybe I can also remove \[CLS\]?

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
653,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,1,aphyys,self.MachineLearning,Evaluating Models Against Heavily Trained Models,https://www.reddit.com/r/MachineLearning/comments/aphyys/evaluating_models_against_heavily_trained_models/,Pangurbon,1549902707,[removed],0,1,False,self,,,,,
654,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,1,aphz3c,self.MachineLearning,Resources or ways to extract fact triplet from sentence?,https://www.reddit.com/r/MachineLearning/comments/aphz3c/resources_or_ways_to_extract_fact_triplet_from/,yash_8141,1549902727,[removed],1,1,False,self,,,,,
655,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,1,api76r,self.MachineLearning,[D] Will Transformers replace Capsules ?,https://www.reddit.com/r/MachineLearning/comments/api76r/d_will_transformers_replace_capsules/,Jean-Porte,1549903993,"Hi, is anyone working on adapting transformers to computer vision ?  
\-It seems possible to have a 2D positional encoding

\-The token embeddings can be replaced by a CNN

If this was implemented, wouldn't a CNN-transformer be able to deal with all problems that Hinton attempts to solve with capsules ?",3,1,False,self,,,,,
656,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,1,api7vy,intellipaat-youtube.clickable.cards,Introduction to Python - Intellipaat Python basics tutorial is your stepping stone to a successful career!,https://www.reddit.com/r/MachineLearning/comments/api7vy/introduction_to_python_intellipaat_python_basics/,anonymous12_,1549904105,,0,1,False,https://b.thumbs.redditmedia.com/gIYYvI9yODo-Xid7LPN0jO2IO75Xg3kQAgXZjjlxEhM.jpg,,,,,
657,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,2,apiilq,github.com,"Wego: Word2Vec, GloVe, and Lexvec in Go!",https://www.reddit.com/r/MachineLearning/comments/apiilq/wego_word2vec_glove_and_lexvec_in_go/,aqny,1549905724,,0,1,False,https://b.thumbs.redditmedia.com/GpT5JvE_KVkzlSZR6M5vqQGd05y_iQDR0_ePk9TZx2Q.jpg,,,,,
658,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,2,apikxe,self.MachineLearning,"[N] Uber's Ludwig for ""code-free, deep learning"" development",https://www.reddit.com/r/MachineLearning/comments/apikxe/n_ubers_ludwig_for_codefree_deep_learning/,amw5gster,1549906070,"Article [https://eng.uber.com/introducing-ludwig/](https://eng.uber.com/introducing-ludwig/)

and project homepage [https://uber.github.io/ludwig/](https://uber.github.io/ludwig/)

&amp;#x200B;",17,1,False,self,,,,,
659,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,2,apisdq,github.com,"[P] Wego: Word2Vec, GloVe, and Lexvec in Go!",https://www.reddit.com/r/MachineLearning/comments/apisdq/p_wego_word2vec_glove_and_lexvec_in_go/,aqny,1549907178,,0,1,False,https://b.thumbs.redditmedia.com/GpT5JvE_KVkzlSZR6M5vqQGd05y_iQDR0_ePk9TZx2Q.jpg,,,,,
660,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,3,apiyhk,dragan.rocks,"Deep Learning in Clojure from Scratch to GPU,- Part 0 : Why Bother?",https://www.reddit.com/r/MachineLearning/comments/apiyhk/deep_learning_in_clojure_from_scratch_to_gpu_part/,dragandj,1549908088,,0,1,False,https://b.thumbs.redditmedia.com/lVotfLoPs2sleRDkCBOcKp7dC08lGbsugfsJZHlCWoI.jpg,,,,,
661,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,3,apjcsr,self.MachineLearning,Using Featureplot command in 'caret' package for R,https://www.reddit.com/r/MachineLearning/comments/apjcsr/using_featureplot_command_in_caret_package_for_r/,mockrun,1549910207,[removed],0,1,False,self,,,,,
662,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,3,apje3v,self.MachineLearning,[D] Is here some pretrained network for JPEG artifact removal,https://www.reddit.com/r/MachineLearning/comments/apje3v/d_is_here_some_pretrained_network_for_jpeg/,[deleted],1549910407,[deleted],0,1,False,default,,,,,
663,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,3,apjede,ai.googleblog.com,Using Global Localization to Improve Navigation,https://www.reddit.com/r/MachineLearning/comments/apjede/using_global_localization_to_improve_navigation/,sjoerdapp,1549910448,,0,1,False,https://b.thumbs.redditmedia.com/2AE3XgHvuWgxKkrVym6J5zChayteGuIMIeJAAa2IU7Y.jpg,,,,,
664,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,3,apjf3f,self.MachineLearning,[D] Is here some pretrained network for JPEG artifact removal?,https://www.reddit.com/r/MachineLearning/comments/apjf3f/d_is_here_some_pretrained_network_for_jpeg/,hadaev,1549910555," I itried to google it, but find only repos without pretrained weights.",6,1,False,self,,,,,
665,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,4,apjs56,self.MachineLearning,[D] Any suggestion or research paper to extract facts triplet from the sentence?,https://www.reddit.com/r/MachineLearning/comments/apjs56/d_any_suggestion_or_research_paper_to_extract/,yash_8141,1549912500,"for example:  
Michael a speaker of forest ministry said Africa was committed to protecting the tigers which live in Madagascar.

the output should be:  
(tiger ,live,Madagascar) (Africa,protect,tiger )",7,2,False,self,,,,,
666,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,4,apk11l,gwern.net,"[R] ""Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence"", Liang et al 2019",https://www.reddit.com/r/MachineLearning/comments/apk11l/r_evaluation_and_accurate_diagnoses_of_pediatric/,gwern,1549913832,,0,1,False,default,,,,,
667,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,4,apk1ni,blog.samibadawi.com,"ML and Data in AWS, Azure and GCP",https://www.reddit.com/r/MachineLearning/comments/apk1ni/ml_and_data_in_aws_azure_and_gcp/,type-tinker,1549913928,,0,1,False,https://b.thumbs.redditmedia.com/UsMoBv3jq1dCRzX_LMVUis8hEqbq7o4qkCK2bENnm8Y.jpg,,,,,
668,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,4,apk64x,self.MachineLearning,Interested in joining an EdTech company and improving the way students learn?,https://www.reddit.com/r/MachineLearning/comments/apk64x/interested_in_joining_an_edtech_company_and/,fay-jai,1549914615,[removed],0,1,False,self,,,,,
669,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,5,apkdgu,unity3d.com,The Obstacle Tower Challenge is live!,https://www.reddit.com/r/MachineLearning/comments/apkdgu/the_obstacle_tower_challenge_is_live/,leonchenzhy,1549915725,,0,1,False,default,,,,,
670,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,6,apll4u,medium.com,[R] Understanding Building Blocks of ULMFIT (Universal Language Model Fine-tuning),https://www.reddit.com/r/MachineLearning/comments/apll4u/r_understanding_building_blocks_of_ulmfit/,risig_sag,1549922237,,0,1,False,https://b.thumbs.redditmedia.com/J1m6a-TUcStYfaH17sYHWTeBaEaoTVOxYeyeZ_SOmoM.jpg,,,,,
671,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,7,apm7fx,self.MachineLearning,Some questions about AGI research?,https://www.reddit.com/r/MachineLearning/comments/apm7fx/some_questions_about_agi_research/,Dovageris,1549925739,[removed],0,1,False,self,,,,,
672,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,8,apmo9i,adaptilab.com,"Quick explanation of DeepScale's SqueezeNet model: ""Conquering ImageNet with a &lt;1MB model""",https://www.reddit.com/r/MachineLearning/comments/apmo9i/quick_explanation_of_deepscales_squeezenet_model/,CyberCorgi,1549928494,,0,1,False,https://b.thumbs.redditmedia.com/0PUEQa7qrzl-JNr4jd8tpOYKCdzUvq97xgrQtY_ziaY.jpg,,,,,
673,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,9,apn79b,auai.org,[D] Towards Flatter Loss Surface via Nonmonotonic Learning Rate Scheduling,https://www.reddit.com/r/MachineLearning/comments/apn79b/d_towards_flatter_loss_surface_via_nonmonotonic/,FlyingQuokka,1549931740,,1,1,False,default,,,,,
674,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,9,apnah0,github.com,tensorflow/swift-apis is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/apnah0/tensorflowswiftapis_is_a_new_github_repo_by/,sjoerdapp,1549932318,,0,1,False,default,,,,,
675,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,11,apo9qh,self.MachineLearning,"[P] EuclidesDB v.0.2.0 released, libtorch 1.0.1 and Faiss index support",https://www.reddit.com/r/MachineLearning/comments/apo9qh/p_euclidesdb_v020_released_libtorch_101_and_faiss/,perone,1549938565,"For those interested, I just released EuclidesDB v.0.2.0, full [changelog here](https://euclidesdb.readthedocs.io/en/latest/changelog.html#release-v-0-2-0) and [documentation here](https://euclidesdb.readthedocs.io/en/latest/index.html).

EuclidesDB is a multi-model machine learning feature database that is tightly coupled with PyTorch and provides a backend for including and querying data on the model feature space. Some features of EuclidesDB are listed below:

* Written in C++ for performance;
* Uses protobuf for data serialization;
* Uses gRPC for communication;
* LevelDB integration for database serialization;
* Many indexing methods implemented (Annoy, Faiss, etc);
* Tight PyTorch integration through libtorch;
* Easy integration for new custom fine-tuned models;
* Easy client language binding generation;
* Free and open-source with permissive license;
",4,1,False,self,,,,,
676,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,11,apogbz,self.MachineLearning,what is the details of adaboost for regression?,https://www.reddit.com/r/MachineLearning/comments/apogbz/what_is_the_details_of_adaboost_for_regression/,frank_cao,1549939753,"Now I want to code for adaboost method and I have completed it for classifier, but not for regression.

I have search much for regression adaboost but not find material for explaining its details.",0,1,False,self,,,,,
677,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,12,apovve,youtube.com,"[R] Lecture of ""Adversarial Examples Are a Natural Consequence of Test Error in Noise""",https://www.reddit.com/r/MachineLearning/comments/apovve/r_lecture_of_adversarial_examples_are_a_natural/,howdygoop,1549942547,,0,1,False,https://b.thumbs.redditmedia.com/A7fLEo9tIG6lwKHa6-MWCaWDapvLyo4KuvWki6ET52Y.jpg,,,,,
678,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,12,app3l4,self.MachineLearning,[R] Lecture of Adversarial Examples Are a Natural Consequence of Test Error in Noise,https://www.reddit.com/r/MachineLearning/comments/app3l4/r_lecture_of_adversarial_examples_are_a_natural/,howdygoop,1549943944,"Lecture: [https://youtu.be/ozt-yu72GpQ](https://youtu.be/ozt-yu72GpQ)

Paper: [https://arxiv.org/abs/1901.10513](https://arxiv.org/abs/1901.10513)

Course: [https://berkeley-deep-learning.github.io/cs294-131-s19/](https://berkeley-deep-learning.github.io/cs294-131-s19/)",0,1,False,self,,,,,
679,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,13,appe07,self.MachineLearning,Any predictive/analysis ideas on web application logs?,https://www.reddit.com/r/MachineLearning/comments/appe07/any_predictiveanalysis_ideas_on_web_application/,rupreetg,1549945925,[removed],0,1,False,self,,,,,
680,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,13,appha0,self.MachineLearning,How to troubleshoot Press Brake Bending Problems?,https://www.reddit.com/r/MachineLearning/comments/appha0/how_to_troubleshoot_press_brake_bending_problems/,CNCPressBrakeChina,1549946568,[removed],0,1,False,self,,,,,
681,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,14,appv7e,self.MachineLearning,[D] Is there any research that models human intuition?,https://www.reddit.com/r/MachineLearning/comments/appv7e/d_is_there_any_research_that_models_human/,enzlbtyn,1549949242,"After reading about [attention](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/) and how it is modelled (loosely) by how we pay attention to objects, I am curious to know if anyone has done any work in modelling human intuition? 

For instance, if we define 'human intuition' as a heuristic function for search, e.g. to prove a mathematical statement is true. From what I've seen, it seems there is not much done?",9,1,False,self,,,,,
682,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,14,appy4e,self.MachineLearning,[D] Have anyone tried the Quantum ML course?,https://www.reddit.com/r/MachineLearning/comments/appy4e/d_have_anyone_tried_the_quantum_ml_course/,crypto_ha,1549949818,A few days ago someone posted information on the [Quantum ML course](https://www.edx.org/course/quantum-machine-learning) by Uni. of Toronto. Did anyone try enrolling in the course and want to share some initial reviews?,2,1,False,self,,,,,
683,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,14,appzd4,self.MachineLearning,Need advice,https://www.reddit.com/r/MachineLearning/comments/appzd4/need_advice/,droidexpress,1549950070,[removed],0,1,False,self,,,,,
684,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,14,appzs9,self.MachineLearning,Simple PyTorch implementaion of Neural Machine Translation(NMT) for beginners,https://www.reddit.com/r/MachineLearning/comments/appzs9/simple_pytorch_implementaion_of_neural_machine/,lyeoni,1549950149,[removed],0,1,False,https://b.thumbs.redditmedia.com/8-gXW1NSXGLC1tUKSfoeABGDsgtExwbkH3rnN67DUfU.jpg,,,,,
685,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,14,apq46q,medium.com,[R] Reaching a deeper understanding of conversational AI through deep learning,https://www.reddit.com/r/MachineLearning/comments/apq46q/r_reaching_a_deeper_understanding_of/,abhisvnit,1549951045,,0,1,False,https://a.thumbs.redditmedia.com/GO6sEB1dwcnAclj8ggBvTF6FijvahJOMfUWlpH4X-I4.jpg,,,,,
686,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,15,apq4xu,self.MachineLearning,[P] StyleGAN on Anime Faces,https://www.reddit.com/r/MachineLearning/comments/apq4xu/p_stylegan_on_anime_faces/,wei_jok,1549951207,"Some people have started training [StyleGAN](https://arxiv.org/abs/1812.04948) ([code](https://github.com/NVlabs/stylegan)) on anime datasets, and obtained some pretty cool results

https://twitter.com/_Ryobot/status/1095160640241651712

/u/gwern provided models for StyleGAN trained on anime faces if anyone would like to have a play with them:

https://twitter.com/gwern/status/1095131651246575616

I think he used the [Danbooru2018](https://www.gwern.net/Danbooru2018) that he made available last year.
",11,1,False,self,,,,,
687,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,15,apq614,self.MachineLearning,"[Project] for beginners, simple PyTorch implementaion of Neural Machine Translation(NMT)",https://www.reddit.com/r/MachineLearning/comments/apq614/project_for_beginners_simple_pytorch/,lyeoni,1549951439,"Hello :) This is my first post.

Most machine translation implementations are too complicated (especially to me.), so i implemented it for some who want simple, like me.

This repo contains a simple source code for neural machine translation based on sequence-to-sequence network. And I tested(also, evaluated) following models sequentially:

* Baseline(base): Simple Sequence to Sequence model
* Reverse: Apply Bi-directional LSTM to the encoder part
* Embeddings: Apply Fasttext word embeddings (300D)
* Attention: Apply attention mechanisms to the decoder part

I hope that this repo can be a good solution for people who dont't want unnecessarily many features, and be of help to make your own neural-machine translator.

[https://github.com/lyeoni/nlp-tutorial/tree/master/neural-machine-translation](https://github.com/lyeoni/nlp-tutorial/tree/master/neural-machine-translation)",3,1,False,self,,,,,
688,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,15,apqkww,aarkstore.com,"Machine Learning as a Service Market size, trends, growth and Regional Forecast 2018-2025",https://www.reddit.com/r/MachineLearning/comments/apqkww/machine_learning_as_a_service_market_size_trends/,aarksneha11223,1549954611,,0,1,False,default,,,,,
689,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,16,apqox5,self.MachineLearning,Looking for a recommendation for a package or library for keyword extraction [C++/Python],https://www.reddit.com/r/MachineLearning/comments/apqox5/looking_for_a_recommendation_for_a_package_or/,thenewstampede,1549955471,[removed],0,1,False,self,,,,,
690,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,16,apqs8i,self.MachineLearning,[D] Style GAN - Generating styles which are correlated over different scales,https://www.reddit.com/r/MachineLearning/comments/apqs8i/d_style_gan_generating_styles_which_are/,WillingCucumber,1549956199,"I have a doubt regarding recent paper: StyleGAN. The paper uses different mean/variance normalization parameters at different layers. I am confused that when we normalize an intermediate layer, it will change the mean and variance of the input to 0,1 respectively  and then adjust the feature map using gamma and beta parameters.. Thus, when different styles are used in a coarse to fine model, won't the model forget the style at higher level and  style of the generated image  would only posses  style of the last layer features only ??

Please help me resolve my doubt. Thanks !!",0,1,False,self,,,,,
691,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,17,apr8gk,self.MachineLearning,Filmmaker creating graphic novel describing how the internet will wake up.,https://www.reddit.com/r/MachineLearning/comments/apr8gk/filmmaker_creating_graphic_novel_describing_how/,Jackson_Filmmaker,1549960216,[removed],0,1,False,self,,,,,
692,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,17,aprajg,rubikscode.net,Implementing CycleGAN Using Python,https://www.reddit.com/r/MachineLearning/comments/aprajg/implementing_cyclegan_using_python/,RubiksCodeNMZ,1549960780,,0,1,False,https://b.thumbs.redditmedia.com/Sq6UZAdu6loRgyWIg4xQVy8gCRWlNKYwANGIrAtCGww.jpg,,,,,
693,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,17,apre9h,self.MachineLearning,How can we efficiently manage the huge dataset in multiple servers?,https://www.reddit.com/r/MachineLearning/comments/apre9h/how_can_we_efficiently_manage_the_huge_dataset_in/,kaisper,1549961808,[removed],0,1,False,self,,,,,
694,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,18,aprie2,self.MachineLearning,VTX Tokens - The Key To Success,https://www.reddit.com/r/MachineLearning/comments/aprie2/vtx_tokens_the_key_to_success/,VolentixLab,1549962857,[removed],1,1,False,self,,,,,
695,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,18,aprlyz,medium.com,Predicting Functional Water Pumps in Tanzania using Random Forests and Logistic Regression in Python,https://www.reddit.com/r/MachineLearning/comments/aprlyz/predicting_functional_water_pumps_in_tanzania/,Fewthp,1549963797,,0,1,False,https://b.thumbs.redditmedia.com/kv6V2_HWnzTiBVv4wEthCED2nrKYG6cOxgkePCH14PE.jpg,,,,,
696,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,18,aprr86,arxiv.org,[R] Extreme value loss - generative models for the support of a distribution,https://www.reddit.com/r/MachineLearning/comments/aprr86/r_extreme_value_loss_generative_models_for_the/,NichG,1549965131,,1,1,False,default,,,,,
697,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,19,aps2wn,arxiv.org,[1902.04043] The StarCraft Multi-Agent Challenge,https://www.reddit.com/r/MachineLearning/comments/aps2wn/190204043_the_starcraft_multiagent_challenge/,ihaphleas,1549968037,,1,1,False,default,,,,,
698,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,19,aps3vi,arxiv.org,[R] Optimal Kronecker-Sum Approximation of Real Time Recurrent Learning,https://www.reddit.com/r/MachineLearning/comments/aps3vi/r_optimal_kroneckersum_approximation_of_real_time/,asierm,1549968276,,1,1,False,default,,,,,
699,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,20,apsa2b,self.MachineLearning,[D] Best practices for handling large training data,https://www.reddit.com/r/MachineLearning/comments/apsa2b/d_best_practices_for_handling_large_training_data/,pastaking,1549969780,"I'm currently using tfrecord with TensorFlow. My training data is ballooning to 180GB+ of .tfrecord files. (I work with speech data). Was wondering how does everyone deal with big training data in your organization, eg. How do you store it? How do you use it for training? How to you compress data / save space?",2,1,False,self,,,,,
700,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,20,apshmd,self.MachineLearning,[D] Networks and layer for time-series data,https://www.reddit.com/r/MachineLearning/comments/apshmd/d_networks_and_layer_for_timeseries_data/,mortengryning,1549971658,"I have been looking into using LSTM's for predicting time-series data. I have played with different networks, both on my own artificial training data (where I know complex pattern exist) and real data, but in general I don't find their performance satisfactory.  


I have tried basic LSTM networks like simple LSTM followed by dense layers, encoder/decoder network, LSTM network with CNN layers and basic ANN. I have tried to provide different timesteps, to normalize/not normalize the data, to look at differences between timesteps with different lags, and to only look if the values on the next timestep increased or decreased. 

I have been looking for articles/research papers/sources on this subject, but many are behind paywalls, so I don't know which to get.  


1) What are some good research papers on predicting time-series data using neural network that you would recommend?  


2) In some cases, I have inputs from different sources (up to 20). Do studies exist on how to best get this data into the network? Do I create multiple inputs and concatenate them, or is there a smarter way?",1,1,False,self,,,,,
701,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,21,apsqfi,self.MachineLearning,Struggling with deciding how to build a new PC for machine learning [Discussion],https://www.reddit.com/r/MachineLearning/comments/apsqfi/struggling_with_deciding_how_to_build_a_new_pc/,Moondra2017,1549973671,"Hi guys, I am struggling to put togethera build and I figured since most of you are very experienced maybe I could get some advice.

&amp;#x200B;

&amp;#x200B;

[https://pcpartpicker.com/list/cyxXFt](https://pcpartpicker.com/list/cyxXFt)

&amp;#x200B;

&amp;#x200B;

My aim is to work with larger datasets (million plus images), prototype main models as fact as I can. I am still a hobbyist and dont have a set budget yet. I want something that is middle end in terms of performanace.

\#GPU

Currently I am looking at  RTX 2070 (X2) for GPUs, as from what I have read that seems to be the best bang for the buck.It seems 2070 allows FP16 operations (Is this a software thing that was purposelly unlocked just for this graphics card?), which leads to faster operations and essentialy double the batch size (?) due to lower memory footprint.

Should I get 2070x 2 or should I get one 1080TI / one 2070 instead?

&amp;#x200B;

\#CPU

As for CPU I am looking for something with a lot of cores as I want to do a lot of data preprocessing on millions of images, so I was looking at the amd-threadripper 1920, which seems to be the best bang for the buck. I feel like I may encounter lots of CPU intensive tasks, so I want to have at least 12 cores.

&amp;#x200B;

\#RAM

As for RAM, I will look at either 64gigs to 128 gigs, though I feel 64 should be enough to load smaller datasets fully into RAM.

&amp;#x200B;

\#MOTHERBOARD

As for motherboard, I have no idea.

For this build,  I cant decided if I should get a board that holds only two graphic cards slots (not sure if there are any), or should I look for one with 4. However all motherboards for Ryzen gen 1 seem to  start at $250  according to PCPartsPicker, which is a bit surprising.

I was expecting to get a decent board for $150.

&amp;#x200B;

I was looking into this one as it seems to on the cheaper side:

[https://pcpartpicker.com/product/kjmxFT/asrock-x399-taichi-atx-tr4-motherboard-x399-taichi](https://pcpartpicker.com/product/kjmxFT/asrock-x399-taichi-atx-tr4-motherboard-x399-taichi)?

&amp;#x200B;

&amp;#x200B;

As for the rest, I think I am ok.

&amp;#x200B;

Thank you very much. 

&amp;#x200B;",2,1,False,self,,,,,
702,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,21,apstfo,medium.com,[P] Serving article comments using reinforcement learning of a neural net,https://www.reddit.com/r/MachineLearning/comments/apstfo/p_serving_article_comments_using_reinforcement/,RealJon,1549974380,,0,1,False,default,,,,,
703,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,21,apsyq3,self.MachineLearning,[P] Sharing my TensorFlow build repository,https://www.reddit.com/r/MachineLearning/comments/apsyq3/p_sharing_my_tensorflow_build_repository/,davex32,1549975529,"[https://github.com/davidenunes/tensorflow-wheels](https://github.com/davidenunes/tensorflow-wheels)  
Hi, everyone. I created this repository to post various TensorFlow builds I create and use over time. I use Arch Linux so I tend to be on the latest version of CUDA. I found myself building TF any time a new version comes out to match the library versions I have installed, include support for SSE AVX, test XLA etc.  


Building TF from source can be a pain in some machines so I decided to share my builds from now on. If anyone needs a particular build, you can open an issue, I am but one person but I'll try to help if I can.   


cheers",0,1,False,self,,,,,
704,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,22,apt6no,self.MachineLearning,Interviews with Machine Learning Heroes (Kaggle GM(s) + AI Researchers + Practitioners),https://www.reddit.com/r/MachineLearning/comments/apt6no/interviews_with_machine_learning_heroes_kaggle/,init__27,1549977128,"Hi, 

During the past few weeks, I have tried to interview various ""Machine Learning Heroes"". Here is the index to all of the interviews:

[https://www.kaggle.com/general/76241#post448008](https://www.kaggle.com/general/76241#post448008)",0,1,False,self,,,,,
705,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,22,apt765,self.MachineLearning,[D] Some of the upcoming applications of machine learning and artificial intelligence in 2019,https://www.reddit.com/r/MachineLearning/comments/apt765/d_some_of_the_upcoming_applications_of_machine/,pr33tish,1549977225,"*Some thoughts and compilation of upcoming applications of ML and AI that we can witness this year.*

[*https://www.promptcloud.com/upcoming-applications-of-machine-learning-and-artificial-intelligence-in-2019/*](https://www.promptcloud.com/upcoming-applications-of-machine-learning-and-artificial-intelligence-in-2019/)",0,1,False,self,,,,,
706,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,22,apt8m9,hodlerxl.com,SingularityNET (AGI) DApp And AI Marketplace Platform Roadmap -,https://www.reddit.com/r/MachineLearning/comments/apt8m9/singularitynet_agi_dapp_and_ai_marketplace/,tokyomarui,1549977504,,0,1,False,https://b.thumbs.redditmedia.com/VO0z-twHSA9soKa16TRfqrda6M5IHwAEpPoh74VcnrQ.jpg,,,,,
707,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,22,apta8s,self.MachineLearning,"Interview with the Creator of DeOldify: A project that uses DL to ""colorise"" B&amp;W Images",https://www.reddit.com/r/MachineLearning/comments/apta8s/interview_with_the_creator_of_deoldify_a_project/,init__27,1549977837,[removed],0,1,False,self,,,,,
708,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,22,apth7w,github.com,Spektral: deep learning on graphs with Keras.,https://www.reddit.com/r/MachineLearning/comments/apth7w/spektral_deep_learning_on_graphs_with_keras/,EdmondRR,1549979161,,0,1,False,default,,,,,
709,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,22,aptkp7,self.MachineLearning,[P] Image annotation python package for converting and visualizing different formats,https://www.reddit.com/r/MachineLearning/comments/aptkp7/p_image_annotation_python_package_for_converting/,InfoPaste,1549979819,"Hello all,

I've recently been working on creating my own image dataset for training object detection models. I've struggled to find a library the implements useful functions, so I decided to create my own called [imantics](https://github.com/jsbroks/imantics). Currently it allows to you export, create and visualize image annotations from binary masks, COCO, YOLO or VOC formats.

It's a work in progress and I'm looking for anyone would like the contribute.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
710,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptn3s,self.MachineLearning,[P] How to build an API for a machine learning model using Flask,https://www.reddit.com/r/MachineLearning/comments/aptn3s/p_how_to_build_an_api_for_a_machine_learning/,elftim,1549980253,"Recently I have discovered the Python micro web framework Flask and used it a couple times since. I have written a blogpost about it. For prototyping and making your model accessible in your organisation I think it's a great an easy way to do so.

[Link](https://medium.com/vantageai/how-to-build-an-api-for-a-machine-learning-model-in-5-minutes-using-flask-eb72d8cb4504)",4,1,False,self,,,,,
711,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,apto23,self.MachineLearning,Simple and robust automatic image caption generation?,https://www.reddit.com/r/MachineLearning/comments/apto23/simple_and_robust_automatic_image_caption/,YourWelcomeOrMine,1549980433,"I would like to use different word embeddings (ELMo, BERT, etc.) for image caption generation. However, I'm more concerned with the structure of the language it produces, rather than how accurate the caption is. As such, I was going to test it with very simple images that are easy for the caption generation system. Does some sort of baseline like this exist?",0,1,False,self,,,,,
712,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptr4k,self.MachineLearning,Flair,https://www.reddit.com/r/MachineLearning/comments/aptr4k/flair/,Amphagory,1549980984,[removed],0,1,False,self,,,,,
713,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptrhv,self.MachineLearning,Help a noob?,https://www.reddit.com/r/MachineLearning/comments/aptrhv/help_a_noob/,tryxter7,1549981051,"Hey guys. I'm new to ML and started learning about neural nets for a college course. I'm trying to understand Keras' Sequential model and the process of building Dense layers. Can anyone suggest a great starting point for this? 
I'd prefer it to be resources other than the official documentation.
Many thanks :)",0,1,False,self,,,,,
714,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptrwi,self.MachineLearning,Master in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aptrwi/master_in_machine_learning/,fedetask,1549981127,[removed],0,1,False,self,,,,,
715,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptt4x,self.MachineLearning,[D] Simple and robust automatic image caption generation?,https://www.reddit.com/r/MachineLearning/comments/aptt4x/d_simple_and_robust_automatic_image_caption/,YourWelcomeOrMine,1549981346,"I would like to use different word embeddings (ELMo, BERT, etc.) for image caption generation. However, I'm more concerned with the structure of the language it produces, rather than how accurate the caption is. As such, I was going to test it with very simple images that are easy for the caption generation system. Does some sort of baseline like this exist?",0,1,False,self,,,,,
716,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptubk,arxiv.org,"[Discussion] [1902.03477] The Omniglot Challenge: A 3-Year Progress Report by Lake, Salakhutdinov and Tenenbaum; A nice review paper which can also be seen as an excellent critique of both deep learning methods and deep learning research",https://www.reddit.com/r/MachineLearning/comments/aptubk/discussion_190203477_the_omniglot_challenge_a/,astonished_crofty,1549981547,,19,1,False,default,,,,,
717,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptwxm,self.MachineLearning,"[D] What are the main differences between the word embeddings of ELMo, BERT, Word2vec, and GloVe?",https://www.reddit.com/r/MachineLearning/comments/aptwxm/d_what_are_the_main_differences_between_the_word/,YourWelcomeOrMine,1549982009,"Focusing more on linguistic aspects, rather than engineerings aspects, what are the significant differences between the embeddings of the following systems? If there are any significant systems I've left off, please add them as well:

* ELMo
* BERT
* Word2vec
* GloVe",13,1,False,self,,,,,
718,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,aptxkn,github.com,"[R] BLOCK (AAAI 2019), with a multimodal fusion library for deep learning models",https://www.reddit.com/r/MachineLearning/comments/aptxkn/r_block_aaai_2019_with_a_multimodal_fusion/,Tamazy,1549982134,,0,1,False,default,,,,,
719,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,apu1ac,self.MachineLearning,Tractor Market Insights and Global Industry Forecast to 2023,https://www.reddit.com/r/MachineLearning/comments/apu1ac/tractor_market_insights_and_global_industry/,parag_bir,1549982808,[removed],1,1,False,self,,,,,
720,MachineLearning,t5_2r3gv,2019-2-12,2019,2,12,23,apu3yn,self.MachineLearning,Adding background music to novels,https://www.reddit.com/r/MachineLearning/comments/apu3yn/adding_background_music_to_novels/,aklagoo,1549983277,[removed],0,1,False,self,,,,,
721,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,0,apuf4l,self.MachineLearning,What is Python? Why Python? Why has it become so popular in so little time? Do you also have these questions in your mind? Do watch this video to learn more about this awesome programming language.,https://www.reddit.com/r/MachineLearning/comments/apuf4l/what_is_python_why_python_why_has_it_become_so/,yashica_,1549985166,[removed],0,1,False,self,,,,,
722,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,0,apuin5,self.MachineLearning,Machine Learning with fuzzywuzzy,https://www.reddit.com/r/MachineLearning/comments/apuin5/machine_learning_with_fuzzywuzzy/,Annie10969,1549985745,[removed],0,1,False,self,,,,,
723,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,0,apup6r,self.MachineLearning,[D] Waymo machine learning at scale (MIT Self-Driving Cars series),https://www.reddit.com/r/MachineLearning/comments/apup6r/d_waymo_machine_learning_at_scale_mit_selfdriving/,UltraMarathonMan,1549986763,"New talk from Drago Anguelov (Waymo) as part of MIT Self-Driving Cars lecture series. Waymo is doing a lot of exciting work on machine learning at scale, plus achieving the engineering feat of driving over 10 million miles autonomously. [https://www.youtube.com/watch?v=Q0nGo2-y0xY](https://www.youtube.com/watch?v=Q0nGo2-y0xY) 

https://i.redd.it/8axr56lis5g21.png

A lot of exciting topics in ML covered from reinforcement learning for simulation to AutoML.",0,1,False,https://b.thumbs.redditmedia.com/9kVQ3vzxr0bFeA9bKgW4yH8EC-pgz4XX8Z9mMlcQmAc.jpg,,,,,
724,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,0,apuq9s,self.MachineLearning,Data Annotation made easy with Google Forms,https://www.reddit.com/r/MachineLearning/comments/apuq9s/data_annotation_made_easy_with_google_forms/,prakhar21,1549986937,[removed],0,1,False,self,,,,,
725,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,1,apv0wb,blockdelta.io,Setting Rythm in Algorithm,https://www.reddit.com/r/MachineLearning/comments/apv0wb/setting_rythm_in_algorithm/,BlockDelta,1549988555,,0,1,False,default,,,,,
726,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,1,apv2u7,self.MachineLearning,[D] Suggestions for graduate programs in applied ML,https://www.reddit.com/r/MachineLearning/comments/apv2u7/d_suggestions_for_graduate_programs_in_applied_ml/,magellan91,1549988847,"Hi everyone, I'm a health physics undergrad looking to apply for grad school (MSc, then probably PhD) by the end of this year. 

My interests are ML applications in either health physics or satellite imagery/GIS. 
Which schools/labs (in the US, Canada, Europe, or Australia) can you recommend? 

I figured this would make my search for compatible research labs much easier. Thanks in advance!",0,1,False,self,,,,,
727,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,1,apv31c,i.redd.it,Source inshorts.,https://www.reddit.com/r/MachineLearning/comments/apv31c/source_inshorts/,UnlikelyYesterday,1549988878,,0,1,False,default,,,,,
728,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,1,apv5ua,self.MachineLearning,Features for Music Mastering Algorithm,https://www.reddit.com/r/MachineLearning/comments/apv5ua/features_for_music_mastering_algorithm/,cvantass,1549989274,"Hi, does anyone here have any experience with ML for audio engineering? I am about to embark on my first real project, which is to create an automated music mastering program. The idea is to take the user's mix and a reference track that they want their mix to sound like, extract the features from the reference track, learn how important each feature is to that specific reference mix, then apply the same weights to the features of the user's mix and adjust each feature/parameter accordingly. The one part that I am struggling with more than anything else is deciding which features to use for the algorithm. I found an audio analysis toolkit on GitHub that works well, so I'm good on gathering the data I need for this project, but I am wondering if anyone has any suggestions for what would make some of the best/most effective features for music mastering? Maybe even someone who has done this before? Would MFCC be useful or nah? (also what the heck really even is MFCC?) 

A little background: I am a composer turned audio engineer and only recently this year started on machine learning, so this is more for my learning benefit than for creating a marketable product, but I'm hoping it will be useful for at least my own projects. I will also take any advice on implementation since there are probably better ways to do this, or links to similar open source projects. Thanks everyone!",0,1,False,self,,,,,
729,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,1,apv6j8,self.MachineLearning,"[D] Stat mech /""energy"" based machine learning?",https://www.reddit.com/r/MachineLearning/comments/apv6j8/d_stat_mech_energy_based_machine_learning/,IDoCompNeuro,1549989376,"Most of the posts on this subreddit pertain to deep Neural nets, either feedforward or lstm-like recurrent nets. 

I've recently become interested in more energy based models like Boltzmann machines, helmholtz machines, etc. that are described in terms of statistical physics (entropy, free energy, etc). These type of models seem to have been popular in the 90s, but I see very little about them now. For example, they're rarely discussed on this subreddit. Is there a good reason they've gone out of fashion? Are there interesting directions in which they're being applied and studied academically?

Finally, can anyone recommend a source for reading more about these energy based models (for lack of a better term)? I have a basic background in information theory, but no background in Stat mech and I have a lot of trouble getting an intuition for what I read about Boltzmann and helmholtz machines and the like. It's frustrating because I feel like I was able to understand the math and concepts behind ffwd deep nets with ease, but just can't seem to get the knack of this stat mech way of looking at things!

",8,1,False,self,,,,,
730,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,1,apvauc,self.MachineLearning,"Understanding ""MONet: Unsupervised Scene Decomposition and Representation"" paper.",https://www.reddit.com/r/MachineLearning/comments/apvauc/understanding_monet_unsupervised_scene/,ruderr,1549989981,[removed],0,1,False,self,,,,,
731,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apvilm,self.MachineLearning,What is the best way to predict the country market rating using ML?,https://www.reddit.com/r/MachineLearning/comments/apvilm/what_is_the_best_way_to_predict_the_country/,pmitra0811,1549991133,[removed],0,1,False,self,,,,,
732,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apvjtf,youtube.com,IBM Project Debater,https://www.reddit.com/r/MachineLearning/comments/apvjtf/ibm_project_debater/,CountFrolic,1549991309,,0,1,False,https://b.thumbs.redditmedia.com/vWrhyEOFgzdk0nc5fTfOW-FyKZ-g2Yv4M6BNk1iwuuY.jpg,,,,,
733,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apvo2f,self.MachineLearning,[D] What is the best way to predict the country market rating using ML?,https://www.reddit.com/r/MachineLearning/comments/apvo2f/d_what_is_the_best_way_to_predict_the_country/,pmitra0811,1549991933,"This is the problem statement I was given, I am looking for the answer for the context in the problem statement:

""To utilize heuristics &amp; predictive AI to dynamically map graphically multiple parameters using mix of internet scouring and syndicated databases Solution The AI will utilize dynamic database to update Country market rating on a select cycle for various pharma formulations products.""
",0,1,False,self,,,,,
734,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apvrkn,self.MachineLearning,Time to train a CNN+LTSM,https://www.reddit.com/r/MachineLearning/comments/apvrkn/time_to_train_a_cnnltsm/,MangoGodXOXO,1549992443,[removed],0,1,False,self,,,,,
735,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apvw19,self.MachineLearning,automatic keyframe detection,https://www.reddit.com/r/MachineLearning/comments/apvw19/automatic_keyframe_detection/,dirtyharry2,1549993069,[removed],0,1,False,self,,,,,
736,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apvxx2,self.MachineLearning,Machine learning used in pediatric diagnosis,https://www.reddit.com/r/MachineLearning/comments/apvxx2/machine_learning_used_in_pediatric_diagnosis/,kaiyuanmifen,1549993336,"I read this paper today :[https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/s41591-018-0335-9.pdf](https://www-nature-com.ezp-prod1.hul.harvard.edu/articles/s41591-018-0335-9.pdf)

Which was published in the high-profile nature medicine journal 

Outstanding point 

&amp;#x200B;

1. Large scale 

 2. the method tries to mimic hypotheticodeductive reasoning in medicine, therefore, fitting in existing working flow in hospitals

&amp;#x200B;

3. Include multiple steps which are understandable by clinicians 

&amp;#x200B;

3. Prior knowledge and data-driven model were combined 

&amp;#x200B;

4. NLP pipeline was used to extract clinically relevant features 

&amp;#x200B;

5.clinical chart manually labelled by senior physicians were used as label for information extraction model

&amp;#x200B;

6. Multi-step logistic regression was used for diagnosis based on anatomic division ,mimicing traditional frame used by physicinas 

&amp;#x200B;

7. the gold standard for diagnosis is initial diagnoses by examining physician

&amp;#x200B;

8. the models are interpretable 

&amp;#x200B;

9. The performance of the methods were compared with diagnosis from 20 physicians 

&amp;#x200B;",0,1,False,self,,,,,
737,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apvxxd,i.imgur.com,Math + Algorithms = Machine Learning,https://www.reddit.com/r/MachineLearning/comments/apvxxd/math_algorithms_machine_learning/,renishb,1549993336,,0,1,False,https://b.thumbs.redditmedia.com/ZGf2uHb_OpOApBYIBwoZmKz50X0aDqzrsG1Soicq9NE.jpg,,,,,
738,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apw27r,self.MachineLearning,A few questions about machine learning and brain processes,https://www.reddit.com/r/MachineLearning/comments/apw27r/a_few_questions_about_machine_learning_and_brain/,holographicman,1549993944,[removed],0,1,False,self,,,,,
739,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,2,apw4cn,self.MachineLearning,[P] Introduction to timeseries features - getting started on a DrivenData competition,https://www.reddit.com/r/MachineLearning/comments/apw4cn/p_introduction_to_timeseries_features_getting/,dat-um,1549994254,"An introduction to creating features from a timeseries to use for machine learning:

[http://drivendata.co/blog/rinse-over-run-benchmark/](http://drivendata.co/blog/rinse-over-run-benchmark/)",0,1,False,self,,,,,
740,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,3,apwd1s,self.MachineLearning,How to plot a graph like this?,https://www.reddit.com/r/MachineLearning/comments/apwd1s/how_to_plot_a_graph_like_this/,madh46,1549995517,[removed],0,1,False,https://b.thumbs.redditmedia.com/8hTHcWcwqtIFnxTi9NZd3lHsj9i-lXOBN51PZ50Y6lU.jpg,,,,,
741,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,3,apwk7r,self.MachineLearning,Invisibility Cloak using OpenCV,https://www.reddit.com/r/MachineLearning/comments/apwk7r/invisibility_cloak_using_opencv/,spmallick,1549996555,"Remember the good old days of Harry Potter?   
Well at LearnOpenCV, we can't provide you with a Philosopher's stone (at least not in the near future) but we can definitely teach you how to make your own invisibility cloak!  
[https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/](https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.learnopencv.com%2Finvisibility-cloak-using-color-detection-and-segmentation-with-opencv%2F%3Ffbclid%3DIwAR1WwI-K20Qi5fh8G3FUULkQqEMi4GVik97dD0IPHk_iznKh0mjRnwiJTyc&amp;h=AT1JzfTBH6t9tF_wrDhERMiWkkPU1opRcXSYkFZ4yRnEzAG4MkP9gDuwlfPokYO9kIslcugOL-FcZzMdZKyPrWtXTj_o9WdPxhzI1HkUMZjefGxNfltHw-EJMbzi6KERN0MipQ)  
All you need is a basic idea of colour detection and segmentation. That, and a red cloth and you are all set! We have shared the code in Python and C++.   
[https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Fspmallick%2Flearnopencv%2Ftree%2Fmaster%2FInvisibilityCloak%3Ffbclid%3DIwAR1WwI-K20Qi5fh8G3FUULkQqEMi4GVik97dD0IPHk_iznKh0mjRnwiJTyc&amp;h=AT1JzfTBH6t9tF_wrDhERMiWkkPU1opRcXSYkFZ4yRnEzAG4MkP9gDuwlfPokYO9kIslcugOL-FcZzMdZKyPrWtXTj_o9WdPxhzI1HkUMZjefGxNfltHw-EJMbzi6KERN0MipQ)  
Please mention reviews and what you want us to work on next, in the comments!  
Invisibility Cloak using OpenCV 

![video](342m7ujtl6g21)",0,1,False,self,,,,,
742,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,3,apwm0q,vmls-book.stanford.edu,"Free online Linear Algebra book from Stanford: Introduction to Applied Linear Algebra  Vectors, Matrices, and Least Squares",https://www.reddit.com/r/MachineLearning/comments/apwm0q/free_online_linear_algebra_book_from_stanford/,danielcar,1549996839,,41,1,False,default,,,,,
743,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,3,apwm3c,medium.com,IBM AI Debater Vs Human Champ,https://www.reddit.com/r/MachineLearning/comments/apwm3c/ibm_ai_debater_vs_human_champ/,gwen0927,1549996851,,0,1,False,https://b.thumbs.redditmedia.com/5oFOxYbQGi1Kr03wHY70LTso_u_I2fsMABcA075QSOo.jpg,,,,,
744,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,3,apwm6c,self.MachineLearning,[R] A Paper I Wrote - An Approach to Track Reading Progression Using Eye-Gaze Fixation Points,https://www.reddit.com/r/MachineLearning/comments/apwm6c/r_a_paper_i_wrote_an_approach_to_track_reading/,eng_steve,1549996863,"Hey all,

Here's an arXiv link [https://arxiv.org/abs/1902.03322](https://arxiv.org/abs/1902.03322) to a recent paper of mine while it undergoes the peer review process for the journal I've submitted to. 

I'll paste the abstract below. In a nutshell we use Hidden Markov Models to detect the line of text which is being read by an individual at a given point in time. This is old research relative to what we've done since this paper was concluded, but I think that it's definitely relevant to pay homage to the more classical ML techniques that are often overlooked in today's deep learning-centric climate (deep learning is great though).

A point of discussion I'd like to put on the table: I'd love to try applying a LSTM network to this problem in order to see how that fares compared to the HMM, does anyone have any good resources to follow for implementing one in Python? My guess is that the HMM will outperform since the amount of training data available is limited, and it's in our best interests to keep the complexity as low as possible. Still, it would be a fun side project.

Also - I always love connecting with others in the field. Feel free to reach out through LinkedIn: [https://www.linkedin.com/feed/](https://www.linkedin.com/feed/) 

&amp;#x200B;

ABSTRACT: In this paper, we consider the problem of tracking the eyegaze of individuals while they engage in reading. Particularly, we develop ways to accurately track the line being read by an individual using commercially available eye tracking devices. Such an approach will enable futuristic functionalities such as comprehension evaluation, interest level detection, and user-assisting applications like hands-free navigation and automatic scrolling. Existing commercial eye trackers provide an estimated location of the eye-gaze fixations every few milliseconds. However, this estimated data is found to be very noisy. As such, commercial eyetrackers are unable to accurately track lines while reading. In this paper we propose several statistical models to bridge the commercial gaze tracker outputs and eye-gaze patterns while reading. We then employ hidden Markov models to parametrize these statistical models and to accurately detect the line being read. The proposed approach is shown to yield an improvement of over 20% in line detection accuracy. ",2,1,False,self,,,,,
745,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,4,apxd51,self.MachineLearning,Ideas for Reinforcement Learning project in robotics ?,https://www.reddit.com/r/MachineLearning/comments/apxd51/ideas_for_reinforcement_learning_project_in/,void_gear,1550000797,[removed],0,1,False,self,,,,,
746,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,4,apxfvt,elderresearch.com,linktext You are submitting a link. The key to a successful submission is interesting content and a descriptive title. url https://www.elderresearch.com/company/resource-center/webinars/machine-learning-ai-opportunities-iiot x WEBINAR: AI/ML Opportunities for the Industrial Internet of Things,https://www.reddit.com/r/MachineLearning/comments/apxfvt/linktext_you_are_submitting_a_link_the_key_to_a/,ElderResearch,1550001192,,0,1,False,default,,,,,
747,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,5,apxn3z,medium.com,DeepFashion2: A Versatile Benchmark for Fashion Image Understanding,https://www.reddit.com/r/MachineLearning/comments/apxn3z/deepfashion2_a_versatile_benchmark_for_fashion/,Yuqing7,1550002267,,0,1,False,https://b.thumbs.redditmedia.com/SoAItUoNV9V9Gdap6y3eJMFQ-3KVVR2gd2TFmabendk.jpg,,,,,
748,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,5,apxtoe,self.MachineLearning,[R] Understanding building blocks of ULMFIT (Universal Language Model Fine-tuning for Text Classification),https://www.reddit.com/r/MachineLearning/comments/apxtoe/r_understanding_building_blocks_of_ulmfit/,risig_sag,1550003270,"What is an AWD-LSTM? How Dropout is used everywhere? What is a QRNN and why might it be better?

By [Kerem Turgutlu](https://twitter.com/KeremTurgutlu/status/1094487988040417280)

&amp;#x200B;

[https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b](https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b)",0,1,False,self,,,,,
749,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,5,apxxgz,deadletter.io,Thoughts on this?,https://www.reddit.com/r/MachineLearning/comments/apxxgz/thoughts_on_this/,Smayy12,1550003865,,0,1,False,default,,,,,
750,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,5,apy5ey,self.MachineLearning,NEW SUBREDDIT FOR GOOGLE COLABORATORY,https://www.reddit.com/r/MachineLearning/comments/apy5ey/new_subreddit_for_google_colaboratory/,Atralb,1550005000,[removed],0,1,False,self,,,,,
751,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,6,apy7qb,self.MachineLearning,Data labeling services,https://www.reddit.com/r/MachineLearning/comments/apy7qb/data_labeling_services/,violintendencies,1550005339,[removed],0,1,False,self,,,,,
752,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,6,apyklp,self.MachineLearning,Mendeley group for ML DL papers reading,https://www.reddit.com/r/MachineLearning/comments/apyklp/mendeley_group_for_ml_dl_papers_reading/,alexchauncy,1550007190,[removed],0,1,False,self,,,,,
753,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,6,apyl8b,pcmag.com,IBM Uses Machine Learning to Lower Bottling Costs,https://www.reddit.com/r/MachineLearning/comments/apyl8b/ibm_uses_machine_learning_to_lower_bottling_costs/,edxsocial,1550007281,,0,1,False,default,,,,,
754,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,6,apylbt,self.MachineLearning,[Recommendation] Macbook pro 2018 or Dell Xps 15 for Deep Learning ?,https://www.reddit.com/r/MachineLearning/comments/apylbt/recommendation_macbook_pro_2018_or_dell_xps_15/,sinashish,1550007296,[removed],0,1,False,self,,,,,
755,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,7,apz0o8,self.MachineLearning,[Discussion] Machine Learning techniques for uneven series of events,https://www.reddit.com/r/MachineLearning/comments/apz0o8/discussion_machine_learning_techniques_for_uneven/,Tetlanesh,1550009490," 

I don't know the correct terminology and so long everything I typed into google lead me to some version of time series modeling where all time series had the same number of points in a given time window. So i'll try to explicitly describe the problem and hopefully You can help me out with proper terminology.

&amp;#x200B;

My case have a lot of discrete events for each customer. Some customers produce tons of events every day, some may produce an event once a month or even less often (and of course there is a distribution of everything in between).

&amp;#x200B;

Those events can happen on short time scale and at the same time there can be a huge gap between events - You can have a 100 events within a minute than few days of nothing than one event than few months of nothing (this example is rather extreme).

&amp;#x200B;

Usually response variable we are predicting is something that encapsulates longer trends of interaction with our products - like propensity to buy new product.

&amp;#x200B;

We are currently aggregating those events into monthly/yearly/quarterly/etc features + combine with all other features we have about customers and do most ML projects on resulting tabular dataset. Things like predicting some properties of customer, churn prediction, propensity to buy, customer segmentation, etc...

&amp;#x200B;

With every new project there is quite an overhead on researching and creating new aggregates based on those events because every time we tackle new problem new features emerge from them that where either considered not related or was not present in the portion of the dataset related to given project.

&amp;#x200B;

We are looking into a ways to use raw data of those events instead of aggregates - we can assign a fixed size vector of ""raw features"" (things we can describe about the event, who, what, where, how, etc) to each event and the goal is to let NN's to learn the right temporal combinations of those features for a given problem.

&amp;#x200B;

One of the approaches to building ML system on data of varying length is to use RNN's. We could feed stream of those vectors ordered by the order in which they appeared for a time window of some size and model our target based on those.

Are there alternative approaches? Is the RNN approach even suitable for this type of problem?

&amp;#x200B;

What kind of terminology best describes this kind of problem that I can use to further my research into the topic?",6,1,False,self,,,,,
756,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,7,apzc2d,i.redd.it,"Most popular ""learn..."" subreddits [OC]",https://www.reddit.com/r/MachineLearning/comments/apzc2d/most_popular_learn_subreddits_oc/,Simonyeee,1550011247,,0,1,False,https://a.thumbs.redditmedia.com/l27vITrSsobwXSsMSDh67oyzdirwjOTqYtzD6l5fB68.jpg,,,,,
757,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,8,aq03qu,self.MachineLearning,What is a good NN architecture for optimizing for a particular outcome given previous training data?,https://www.reddit.com/r/MachineLearning/comments/aq03qu/what_is_a_good_nn_architecture_for_optimizing_for/,joiemoie,1550015754,"I am working on a current hobby project. I have kept my day by day log of powerlifting training for the past 1.5 years, with every set and rep scheme used in the workout. Given this training data, I would like to optimize for the reward of maximizing my powerlifting total, which may be either a delayed reward or a future reward. The output of the network would therefore be a workout that I should do to optimize for this reward. However, sometimes, I may want to increase my squat, and sometimes, I may want to increase my deadlift. Therefore, rewards may change over time.

Training is very time sensitive, which would suggest that an RNN structure would work well for this. However, RNN is primarily used for forecasting. However, reinforcement learning techniques are generally used to optimize for a reward. How can I use reinforcement learning to optimize for the reward, where the only known information is my previous workouts and whether I can choose to workout on that day or not?",0,1,False,self,,,,,
758,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,9,aq06si,self.MachineLearning,Free Databricks-Snowflake Learning Events - The Data Mastery Tour,https://www.reddit.com/r/MachineLearning/comments/aq06si/free_databrickssnowflake_learning_events_the_data/,TinyParadox,1550016293,[removed],0,1,False,self,,,,,
759,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,9,aq07et,self.MachineLearning,Developing image memory using unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/aq07et/developing_image_memory_using_unsupervised/,joiemoie,1550016403,[removed],0,1,False,self,,,,,
760,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,9,aq0g1a,self.MachineLearning,[D] What Is Differentiable Programming?,https://www.reddit.com/r/MachineLearning/comments/aq0g1a/d_what_is_differentiable_programming/,chisai_mikan,1550017872,"Blog post on differentiable programming:

*The idea of differentiable programming is coming up a lot in the machine learning world. To many, its not clear if this term reflects a real shift in how researchers think about machine learning, or is just (another) rebranding of deep learning. This post clarifies what new things differentiable programming (or DP) brings to the machine learning table.

Most importantly, differentiable programming is actually a shift opposite from the direction taken by deep learning; from increasingly heavily parameterised models to simpler ones that take more advantage of problem structure.*

https://fluxml.ai/2019/02/07/what-is-differentiable-programming.html
",12,1,False,self,,,,,
761,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,10,aq0txw,github.com,m2cgen - simple way to generate native code (Java/Python/C) from trained ML models (Scikit-learn/XGBoost/LightGBM),https://www.reddit.com/r/MachineLearning/comments/aq0txw/m2cgen_simple_way_to_generate_native_code/,s0ck_r4w,1550020301,,0,1,False,default,,,,,
762,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,10,aq0yca,github.com,StyleGAN Encoder - real-images to latent representation,https://www.reddit.com/r/MachineLearning/comments/aq0yca/stylegan_encoder_realimages_to_latent/,_puzer_,1550021079,,0,1,False,default,,,,,
763,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,10,aq0yk9,vimeo.com,Pattern and texture generated with style transfer,https://www.reddit.com/r/MachineLearning/comments/aq0yk9/pattern_and_texture_generated_with_style_transfer/,kpimmel,1550021123,,0,1,False,default,,,,,
764,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,11,aq1e49,self.MachineLearning,[D] Doubt in understanding Disentangled representations in Beta-VAE,https://www.reddit.com/r/MachineLearning/comments/aq1e49/d_doubt_in_understanding_disentangled/,WillingCucumber,1550023946,"Hi all,

The paper ""Understanding disentangling in beta-VAE"" explains why the latent dimensions would represent the key factors for reconstructing the data.

I am confused about the paragraph which explains why latent factors would be aligned with the generative factors of the data reconstruction. What I mean that the latent factor can be at a certain angle to the true generative factor, contribution of the latent factor to a particular generative factor might not be its full capacity.

&amp;#x200B;

The paper explains this using the diagonal co-variance matrix. Please can someone explain me this ?

Thanks !!

Referring to page 5 last paragraph in paper: [https://arxiv.org/pdf/1804.03599.pdf](https://arxiv.org/pdf/1804.03599.pdf)

Thanks !!

&amp;#x200B;",5,1,False,self,,,,,
765,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,11,aq1lup,youtu.be,CNC Press Brake WE67K 100T 3200mm 8+1Axis Operation Video,https://www.reddit.com/r/MachineLearning/comments/aq1lup/cnc_press_brake_we67k_100t_3200mm_81axis/,CNCPressBrakeChina,1550025347,,0,1,False,https://b.thumbs.redditmedia.com/lkFt4xANfSKgBE1gTd6yHTH0PPO8m-tfcGkKMt5vW5M.jpg,,,,,
766,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,11,aq1pkr,self.MachineLearning,"I'm screwing around with R's swirl, and I'm bored. I want to get to natural language processing. I heard R is great with ML. What's your advice?",https://www.reddit.com/r/MachineLearning/comments/aq1pkr/im_screwing_around_with_rs_swirl_and_im_bored_i/,IntrepidDust,1550026016,[removed],0,1,False,self,,,,,
767,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,12,aq1zlz,self.MachineLearning,Did I explain Deep Learning here correctly?,https://www.reddit.com/r/MachineLearning/comments/aq1zlz/did_i_explain_deep_learning_here_correctly/,CrazyCar09,1550027841,"I am typing a research paper for my high school lit class. i am trying to be thorough in my explanations but I feel like it is still really complicated to understand. I also might have misrepresented how to weight in layers works.

&amp;#x200B;

PARAGRAPH: 

&amp;#x200B;

 

Jeff Dean, a PhD graduate from University of Washington, draws a picture of deep learning, stating, When you hear the term deep learning, just think of a large deep neural net. Deep refers to the number of layers typically and so this is kind of the popular term thats been adopted in the press. I think of them as deep neural networks generally. Neural net is short for the term neural network, a computer diagram that allows for deep learning to take place. This diagram is inspired by a human brain. At the start of the network is an input. This input leads into a set of layers called hidden layers. These layers are where the change occurs. From the hidden layer, an output is produced. This all functions like a human brain. A human brain is composed of neurons, which are composed of dendrites and one axon. When one dendrite is prompted, it will trigger the axon. This axon might trigger another separate neuron and this continues until an output is given (Brownlee). Using advanced algorithms, as well as the chain rule, computers can create their own outputs. These algorithms give each input a weight; generally this weight is around 1 for small networks, a large network might be given a number in the 100s to produce a more accurate output. Once the input leaves, it is transferred to a hidden layer. Based on the programs algorithm and function, this newly arrived input will be given a new weight, if it is very close to the desired output then the weight remains close to the original weight. The farther from the output, the lower the weight. This process repeats for the amount of hidden layers there are (Cosmos). After it gets to the final stage of hidden layers, the highest number is chosen to be the output. This process leads into the first implementations of deep learning.",0,1,False,self,,,,,
768,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,12,aq21jg,self.MachineLearning,Passing in new data to a model after training (TrainTestSplit),https://www.reddit.com/r/MachineLearning/comments/aq21jg/passing_in_new_data_to_a_model_after_training/,run_jmc_619,1550028198,[removed],1,1,False,self,,,,,
769,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,13,aq2dxo,self.MachineLearning,"Feature extraction in R-CNN, how do we assist classes to warped proposals ?",https://www.reddit.com/r/MachineLearning/comments/aq2dxo/feature_extraction_in_rcnn_how_do_we_assist/,ashutosj,1550030414,[removed],0,1,False,self,,,,,
770,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,13,aq2vk6,self.MachineLearning,My LightGBM Algorithm is giving me really inaccurate values,https://www.reddit.com/r/MachineLearning/comments/aq2vk6/my_lightgbm_algorithm_is_giving_me_really/,DerrayProductions,1550033823,[removed],0,1,False,self,,,,,
771,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,14,aq2wnh,self.MachineLearning,Where to start?,https://www.reddit.com/r/MachineLearning/comments/aq2wnh/where_to_start/,ansonplusc,1550034029,[removed],0,1,False,self,,,,,
772,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,14,aq37zn,self.MachineLearning,Newbie looking for guidance for simple text categorization,https://www.reddit.com/r/MachineLearning/comments/aq37zn/newbie_looking_for_guidance_for_simple_text/,pernunz,1550036253,[removed],0,1,False,self,,,,,
773,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,15,aq3myl,self.MachineLearning,NLP,https://www.reddit.com/r/MachineLearning/comments/aq3myl/nlp/,saketsaurav,1550039427,[removed],0,1,False,self,,,,,
774,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,15,aq3rvn,self.MachineLearning,Machine Learning In Node.js With TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/aq3rvn/machine_learning_in_nodejs_with_tensorflowjs/,WarrenR7,1550040512,[removed],0,1,False,self,,,,,
775,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,17,aq4h37,self.MachineLearning,Machine learning to detect Breast cancer? Is it used in healthcare?,https://www.reddit.com/r/MachineLearning/comments/aq4h37/machine_learning_to_detect_breast_cancer_is_it/,xXguitarsenXx,1550046739,[removed],0,1,False,self,,,,,
776,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,17,aq4hqt,dev.thegeeknews.net,Machine Learning  Building a Pet Detector in 30 minutes using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/aq4hqt/machine_learning_building_a_pet_detector_in_30/,MarkSzv,1550046940,,0,1,False,default,,,,,
777,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,18,aq4t9r,self.MachineLearning,StyleGAN Encoder - from real images to latent representation,https://www.reddit.com/r/MachineLearning/comments/aq4t9r/stylegan_encoder_from_real_images_to_latent/,___mlm___,1550049933,[removed],0,1,False,self,,,,,
778,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,18,aq4xrh,i.redd.it,Shrink Wrapping and Shrink Tunnels Machine Manufacturers,https://www.reddit.com/r/MachineLearning/comments/aq4xrh/shrink_wrapping_and_shrink_tunnels_machine/,compak03,1550051124,,0,1,False,https://b.thumbs.redditmedia.com/2XB-y6ImEL8YWC2NZQ7hjWSAnmVLmAvmTk899efvLQI.jpg,,,,,
779,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,18,aq50ij,arxiv.org,[1902.04522] ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero,https://www.reddit.com/r/MachineLearning/comments/aq50ij/190204522_elf_opengo_an_analysis_and_open/,ihaphleas,1550051820,,6,1,False,default,,,,,
780,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,19,aq52xj,self.MachineLearning,[P] NSFW images URLs collection for scrapping,https://www.reddit.com/r/MachineLearning/comments/aq52xj/p_nsfw_images_urls_collection_for_scrapping/,ebazarov,1550052403,"**Project that provide lists of URLs that will help you download NSFW images**, this set can be used in building big enough dataset to train robust NSFM classification model.

This work was inspired by [nsfw_data_scrapper](https://github.com/alexkimxyz/nsfw_data_scrapper) and for downloading images suggested to use scripts from the scrapper.

**Some stats**
You will find different txt files each of them contains list of URLs, here some stats for this set:

* **159** different categories
* in total **1 589 331** URLs
* after downloading and cleaning it's possible to have ~ **500GB** or in other words ~ **1 300 000** of NSFW images",42,1,True,nsfw,,,,,
781,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,20,aq5gff,cogitotech.com,Measure Quality While Training the Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/aq5gff/measure_quality_while_training_the_machine/,trainingdata,1550055854,,0,1,False,default,,,,,
782,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,20,aq5mn2,self.MachineLearning,What if you are living in a computer simulatiob,https://www.reddit.com/r/MachineLearning/comments/aq5mn2/what_if_you_are_living_in_a_computer_simulatiob/,gokucopkake,1550057294,[removed],0,1,False,self,,,,,
783,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,21,aq5w8y,self.MachineLearning,Leverage The Hidden Potent Of Machine Learning For Mobile Apps,https://www.reddit.com/r/MachineLearning/comments/aq5w8y/leverage_the_hidden_potent_of_machine_learning/,Jerry_filyo,1550059495,[removed],0,1,False,self,,,,,
784,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,21,aq620f,self.MachineLearning,Synthetic Bots - The Ultimate Guide To Being Human,https://www.reddit.com/r/MachineLearning/comments/aq620f/synthetic_bots_the_ultimate_guide_to_being_human/,paubric,1550060710,[removed],0,1,False,self,,,,,
785,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,21,aq64j8,self.MachineLearning,The best article about artificial intelligence in plain language for ordinary people,https://www.reddit.com/r/MachineLearning/comments/aq64j8/the_best_article_about_artificial_intelligence_in/,Doctor_who1,1550061242,[removed],0,1,False,self,,,,,
786,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,22,aq6jxf,self.MachineLearning,[P] StyleGAN Encoder - from real images to latent representation,https://www.reddit.com/r/MachineLearning/comments/aq6jxf/p_stylegan_encoder_from_real_images_to_latent/,___mlm___,1550064284,"I made a implementation of encoder for StyleGAN which can transform a real image to latent representation of generator. Then this representation can be moved along some direction in latent space, e.g. ""smiling direction"" and transformed back into images by generator.

Link: https://github.com/Puzer/stylegan

Notebook with more examples https://github.com/Puzer/stylegan/blob/master/Play_with_latent_directions.ipynb",49,1,False,self,,,,,
787,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,23,aq6yep,self.MachineLearning,How to pick the best images to improve generalization?,https://www.reddit.com/r/MachineLearning/comments/aq6yep/how_to_pick_the_best_images_to_improve/,mateja,1550067035,[removed],0,1,False,self,,,,,
788,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,23,aq704p,self.MachineLearning,[D] Advice on Implementing Hybrid Bayesian Networks in Python?,https://www.reddit.com/r/MachineLearning/comments/aq704p/d_advice_on_implementing_hybrid_bayesian_networks/,Davveeee,1550067331,"I am trying to implement a hybrid Bayesian network (discrete and continuous variables) in Python.

I've checked out a lot of the available tools (pomegranate, pgmpy, libpgm, pymc3) but none of them seem to satisfy my requirements out of the box:

* Support for discrete and continuous (any distribution) variables 
* Parameter learning from data
* Ability to do inference

Now my question is if anyone here has experience with implementing Bayesian networks and could share some advice?",7,1,False,self,,,,,
789,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,23,aq75aa,self.MachineLearning,[D] Does anyone here use knowledge distillation in their projects?,https://www.reddit.com/r/MachineLearning/comments/aq75aa/d_does_anyone_here_use_knowledge_distillation_in/,-Lousy,1550068269,"I started trying out knowledge distillation after I saw[ an article](https://imrsv.ai/blog/2019/2/8/a-case-for-knowledge-distillation) on it a few days ago, and it seems to work much better than I expected. Though most of my models have been relatively simple thus far I think knowledge distillation could still work really well for larger models.

I was wondering if anyone here had put the concept to much use before and whether or not you had any tips or tricks to share about using it?",9,1,False,self,,,,,
790,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,23,aq76r0,self.MachineLearning,New Simple Neural Network Package In Python,https://www.reddit.com/r/MachineLearning/comments/aq76r0/new_simple_neural_network_package_in_python/,lupeboy13,1550068530,[removed],0,1,False,self,,,,,
791,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,23,aq7923,self.MachineLearning,C5.0 Decision Tree implementation in Python?,https://www.reddit.com/r/MachineLearning/comments/aq7923/c50_decision_tree_implementation_in_python/,trngoon,1550068947,[removed],0,1,False,self,,,,,
792,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,23,aq79la,github.com,Skeleton Based Action Recognition Papers and Notes (Any Comments and/or Contributions are highly aprreciated),https://www.reddit.com/r/MachineLearning/comments/aq79la/skeleton_based_action_recognition_papers_and/,cagbal,1550069044,,0,1,False,default,,,,,
793,MachineLearning,t5_2r3gv,2019-2-13,2019,2,13,23,aq7cuh,self.MachineLearning,How AI and machine learning can transform agriculture and help tackle hunger,https://www.reddit.com/r/MachineLearning/comments/aq7cuh/how_ai_and_machine_learning_can_transform/,Charlie_Ensor,1550069625,[removed],0,1,False,self,,,,,
794,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7g1e,self.MachineLearning,"[P] Cool Fashion + AI Papers and Resources (datasets, companies, events, ...)",https://www.reddit.com/r/MachineLearning/comments/aq7g1e/p_cool_fashion_ai_papers_and_resources_datasets/,lzhbrian,1550070169,"[https://github.com/lzhbrian/Cool-Fashion-Papers](https://github.com/lzhbrian/Cool-Fashion-Papers)

Hi all, I am organizing a curated list of Fashion + AI papers and resources (datasets, companies, events, ...) to track the progress of technologies. Any advice or suggestions would be very much appreciated.

I really look forward to some killer apps of AI in Fashion Design in the near future!

thanks",5,1,False,self,,,,,
795,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7kor,crate.io,Machine Learning and CrateDB: Experiment Design &amp; Linear Regression,https://www.reddit.com/r/MachineLearning/comments/aq7kor/machine_learning_and_cratedb_experiment_design/,nachrieb,1550070946,,0,1,False,https://b.thumbs.redditmedia.com/kAdwkeMa_A7EkWstJCpd98b7AO_5mFv1WDtjDXcoCYw.jpg,,,,,
796,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7m6s,heartbeat.fritz.ai,Binary Classification using Keras in R  Heartbeat,https://www.reddit.com/r/MachineLearning/comments/aq7m6s/binary_classification_using_keras_in_r_heartbeat/,mwitiderrick,1550071194,,0,1,False,default,,,,,
797,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7qwn,self.reinforcementlearning,[D] Why do we need Nash averaging for AlphaStar (and not AZ etc) ?,https://www.reddit.com/r/MachineLearning/comments/aq7qwn/d_why_do_we_need_nash_averaging_for_alphastar_and/,so_tiredso_tired,1550071975,,0,1,False,default,,,,,
798,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7uol,self.MachineLearning,DIGITAL TWINS AND PREDICTIVE MAINTENANCE: THE WINNING COMBO?,https://www.reddit.com/r/MachineLearning/comments/aq7uol/digital_twins_and_predictive_maintenance_the/,alexvolpiMW75,1550072589," In 2002, during his speech at the University of Michigan, Dr. Michael Grieves unveiled for the first time his theory of Digital Twin. Behind this rather obscure term lies the idea of developing a digital copy of a physical system based on its own data. It will take more than ten years and the advent of Industry 4.0 for organizations to have the necessary technological resources to achieve this vision.  Click the link to learn more! ",0,1,False,self,,,,,
799,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7ycf,self.MachineLearning,"Simple Questions Thread February 13, 2019",https://www.reddit.com/r/MachineLearning/comments/aq7ycf/simple_questions_thread_february_13_2019/,AutoModerator,1550073166,[removed],0,1,False,self,,,,,
800,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7yxw,github.com,[P] Parallel and Lazy evaluation data pipeline library for NLP in Python,https://www.reddit.com/r/MachineLearning/comments/aq7yxw/p_parallel_and_lazy_evaluation_data_pipeline/,yasufumy,1550073271,,0,1,False,default,,,,,
801,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,0,aq7zio,self.MachineLearning,[D] How to pick the best images to improve generalization?,https://www.reddit.com/r/MachineLearning/comments/aq7zio/d_how_to_pick_the_best_images_to_improve/,mateja,1550073363,"Say you're working on an object detection problem and you have a pre-trained model, a labeled test set, and a large, unlabeled set of images. You run the trained model with the labeled test set and you get back a set of baseline accuracy scores for each of the classes.

Now you want to deliberately pick a subset of images from your unlabeled set to label by human and then train such that it will lead to the largest increase in accuracy scores across your output classes. Let's say that you are guaranteed that all of the images in your unlabeled set contain at least one instance in the output classes.

One way of doing this might be to run the whole unlabeled set through the trained model, sort by the scores, and then label-and-train the images with the lowest scores. Would this necessarily lead to better generalization?

How would you quantify the potential for information in an image to ""teach"" a model something new?

Is there any way of measuring how dissimilar a image is from those kinds that the model has already learned to recognize?

What references can you think of that might help with this problem?",3,1,False,self,,,,,
802,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,1,aq8cl8,self.MachineLearning,Which tornado has the most cheese? Highlights of the Applied Machine Learning Days,https://www.reddit.com/r/MachineLearning/comments/aq8cl8/which_tornado_has_the_most_cheese_highlights_of/,AljoSt,1550075363,[removed],0,1,False,self,,,,,
803,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,1,aq8dim,self.MachineLearning,[D] What questions do you ask yourselves or your clients before starting a new ML project?,https://www.reddit.com/r/MachineLearning/comments/aq8dim/d_what_questions_do_you_ask_yourselves_or_your/,tryo_labs,1550075503,"Hi everyone,

We are currently releasing [some of the questions we make to our clients before kicking off a new project](https://tryolabs.com/blog/2019/02/13/11-questions-to-ask-before-starting-a-successful-machine-learning-project/) to maximize the chances of success.

What are your ideas on the subject? Have you had a ML project fail spectacularly because you didn't ask the right questions beforehand?",7,1,False,self,,,,,
804,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,1,aq8dqq,medium.com,Grok Neural Networks &amp; Backpropagation by re-inventing them  a hackers guide with Python code,https://www.reddit.com/r/MachineLearning/comments/aq8dqq/grok_neural_networks_backpropagation_by/,neuronq,1550075540,,0,1,False,default,,,,,
805,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,2,aq98vr,self.MachineLearning,Autoencoder not converging to zero.,https://www.reddit.com/r/MachineLearning/comments/aq98vr/autoencoder_not_converging_to_zero/,neanderthal_math,1550080050,[removed],0,1,False,self,,,,,
806,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,2,aq9bhl,self.MachineLearning,NLP - Ideas needed around understanding why some words are more prone to be used in conversations than others?,https://www.reddit.com/r/MachineLearning/comments/aq9bhl/nlp_ideas_needed_around_understanding_why_some/,achyutjoshi,1550080434,[removed],0,1,False,self,,,,,
807,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,3,aq9fbm,petlew.com,[P] Basics of Extreme Learning Machines with notebook included,https://www.reddit.com/r/MachineLearning/comments/aq9fbm/p_basics_of_extreme_learning_machines_with/,freechoice,1550081007,,1,1,False,https://b.thumbs.redditmedia.com/4yvHfFVyt0bBPmsHkjt-egSUzrl-vYnYvASnY3FrDbI.jpg,,,,,
808,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,3,aq9s0l,medium.com,Ubers Code-Free Ludwig: Deep Learning for Dummies?,https://www.reddit.com/r/MachineLearning/comments/aq9s0l/ubers_codefree_ludwig_deep_learning_for_dummies/,Yuqing7,1550082869,,0,1,False,https://b.thumbs.redditmedia.com/6WkfeCNCZ3MGjQAANQAezYWd7cZc9cRLNeQyfMIDbZk.jpg,,,,,
809,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,4,aqac5i,self.MachineLearning,"Do the majority of people in ML positions have a CS degree, or a non-CS degree?",https://www.reddit.com/r/MachineLearning/comments/aqac5i/do_the_majority_of_people_in_ml_positions_have_a/,cmanthp2,1550085824,"Do the majority of people in ML positions have a CS degree, or a non-CS (STEM) degree?",0,1,False,self,,,,,
810,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,4,aqahq2,topbots.com,Easy-to-read summaries of major reinforcement learning papers of 2018,https://www.reddit.com/r/MachineLearning/comments/aqahq2/easytoread_summaries_of_major_reinforcement/,drrobobot,1550086612,,0,1,False,default,,,,,
811,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,4,aqap5s,self.MachineLearning,How Does License Plate Recognition Work in Real-time?,https://www.reddit.com/r/MachineLearning/comments/aqap5s/how_does_license_plate_recognition_work_in/,techsavvynerd91,1550087682,[removed],0,1,False,self,,,,,
812,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,4,aqar7l,self.MachineLearning,[D] music interpretation?,https://www.reddit.com/r/MachineLearning/comments/aqar7l/d_music_interpretation/,XSSpants,1550087983,"Is there any software you can feed an audio file into and have it spit out a numerical representation of each note?

Bonus if it can differentiate instruments/synths/tonalities and ignore voices, even if the differential is vague.",1,1,False,self,,,,,
813,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,5,aqatyu,l7.curtisnorthcutt.com,"I built Lambda's $12,500 multi-GPU deep learning rig for $6200. Now you can, too",https://www.reddit.com/r/MachineLearning/comments/aqatyu/i_built_lambdas_12500_multigpu_deep_learning_rig/,cgnorthcutt,1550088388,,1,1,False,default,,,,,
814,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,5,aqau3f,self.MachineLearning,"What is more applicable to machine learning (specifically deep neural networks), linear algebra or calculus 3?",https://www.reddit.com/r/MachineLearning/comments/aqau3f/what_is_more_applicable_to_machine_learning/,MDeanW,1550088405,[removed],0,1,False,self,,,,,
815,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,5,aqbaky,arxiv.org,[R] Invertible Residual Networks,https://www.reddit.com/r/MachineLearning/comments/aqbaky/r_invertible_residual_networks/,anantzoid,1550090874,,25,1,False,default,,,,,
816,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,6,aqbgb9,self.MachineLearning,[Research] [Project] My very eloquent mother just sat upon nine people - Mnemonic Device Generator Survey,https://www.reddit.com/r/MachineLearning/comments/aqbgb9/research_project_my_very_eloquent_mother_just_sat/,ISUCreativityLab,1550091735,"Hello ML Community,

I'm a Computer Science graduate student working on a computational creativity project for generating mnemonic devices using machine learning.What's a mnemonic? Say you want to memorize the order of the planets: Mercury, Venus, etc. A mnemonic for remembering this order is: ""My very eloquent mother just sat upon nine people"". The survey invites you to evaluate some computer generated mnemonics andshould take about 5-10 minutes. Thanks everyone.

Survey Link - https://isu.co1.qualtrics.com/jfe/form/SV\_bvBcX8fK2Rop7W5

Mods, if this isn't allowed, I apologize and would be happy to oblige by removing it. ",13,1,False,self,,,,,
817,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,6,aqbjjm,eng.uber.com,"Uber Introduces Ludwig, a Code-Free Deep Learning Toolbox",https://www.reddit.com/r/MachineLearning/comments/aqbjjm/uber_introduces_ludwig_a_codefree_deep_learning/,slavakurilyak,1550092191,,1,1,False,https://b.thumbs.redditmedia.com/X4DHqPmJu8-_Ka_S5in8XSjns74oUkZwUQXQr3zF1tQ.jpg,,,,,
818,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,6,aqbt3w,self.MachineLearning,Social Media-based Recommender System,https://www.reddit.com/r/MachineLearning/comments/aqbt3w/social_mediabased_recommender_system/,kuomi,1550093622,[removed],0,1,False,self,,,,,
819,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,7,aqci8a,self.MachineLearning,[D] Train seq2seq autoencoders,https://www.reddit.com/r/MachineLearning/comments/aqci8a/d_train_seq2seq_autoencoders/,nanptr,1550097368,"Hi everyone! These days I'm playing with seq2seq models trained as denosing autoencoders. 
The goal of these tests is to experiment the effect of pre-train the encoder with this unsupervised criterion an then fine-tune it on the supervised task (dropping the decoder and connecting the pre-trained encoder to the supervised model).
The reason of this test is to try to assest if I can improve the performance of the supervised task, since I have few labelled examples and a lot of unlabeled data to exploit (I'm working on textual data datasets). 
Some initial tests seems to shows good improvement on the supervised task but I've some doubts about what is the the right way to train a denoising autoencoder. 

Currently I simply ""stop"" the training of the autoencoder after N epochs (on some huge datasets after 3 epochs I get a ""good"" crossentropy loss, but on other small dataset I need 20 epochs to reach same performance) and I'm searching way to define a good strategy to automatically stop training since I want to apply this pipeline on multiple datasets.
I've tried to monitor the loss on a validation set during training (in order to apply Early Stopping) but unfortunately the data has a lot of different words (e.g: id, specific name, rare words, ...) so the loss on the validation may quickly increase ""simply"" because the autoencoder doesn't see this data during training and for this reason is not easy to define a ""good"" validation set. 
What are the options to train an autoencoder?

     - is the right way to monitor the reconstruction loss on a validation set? 
     - should I monitor only the training loss (using all the data for training) and stops when it doesn't improve? 
     - should I monitor different metrics instead (e.g: BLEU of noised data vs reconstructed)? 

I've read many papers about autoencoder on texts but many of them simply says ""we trained the autoencoder"" without specifying details about epochs definition. 

Thank you, any comment paper/blog reference is really appreciated :)",2,1,False,self,,,,,
820,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,8,aqcvw3,github.com,tensorflow/examples is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/aqcvw3/tensorflowexamples_is_a_new_github_repo_by/,sjoerdapp,1550099428,,0,1,False,default,,,,,
821,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,8,aqcwua,self.MachineLearning,Onsite for Google AI residency 2019?,https://www.reddit.com/r/MachineLearning/comments/aqcwua/onsite_for_google_ai_residency_2019/,nhg2020,1550099569,,2,1,False,self,,,,,
822,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,8,aqczt6,self.MachineLearning,Suggestions for training platforms for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/aqczt6/suggestions_for_training_platforms_for_deep/,owlesh,1550100046,"Hi

I'm new to reinforcement learning and I am trying to make my own implementations of algorithms such as DQN, A3C, A2C, PPO and testing them on some Atari games(Pong, Breakout, Space Invaders, etc.) using OpenAI gym. I made a DQN implementation and I trained it on FloydHub. It's very convenient and beginner friendly but also expensive and there is not much flexibility (as we cannot configure the kind of CPU and cores to use with the Tesla K80 GPU which I would need for A2C and A3C among other things). I tried looking at AWS and Google Cloud but they look too complicated to setup to begin with. Any suggestions? Any links for tutorials on setup step by step would be also be appreciated.",0,1,False,self,,,,,
823,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,10,aqef71,self.MachineLearning,Consumer loyalty within the banking industry,https://www.reddit.com/r/MachineLearning/comments/aqef71/consumer_loyalty_within_the_banking_industry/,javk18,1550108873,[removed],0,1,False,self,,,,,
824,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,10,aqeii9,self.MachineLearning,[Research] [Project] Consumer loyalty within the banking industry - survey,https://www.reddit.com/r/MachineLearning/comments/aqeii9/research_project_consumer_loyalty_within_the/,javk18,1550109470,"Hi Guys

I am Currently conducting a research study for my dissertation and am struggling for respondents.I was wondering if the good people of Reddit could spare a few minutes to fill it in:

[https://uwsbusinessschool.qualtrics.com/jfe/form/SV\_3NMOtwqI75xzulv](https://uwsbusinessschool.qualtrics.com/jfe/form/SV_3NMOtwqI75xzulv)

any help would be appreciated

thanks",0,1,False,self,,,,,
825,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,12,aqfeas,self.MachineLearning,[D] removing duplicates?,https://www.reddit.com/r/MachineLearning/comments/aqfeas/d_removing_duplicates/,GantMan,1550115162,Ive used Gemini on a Mac and CCleaner on a Windows machine. Both left duplicates behind. What do you use to find and clean dupes?,0,1,False,self,,,,,
826,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,12,aqfl70,self.MachineLearning,[D] Neural Networks With Second Derivative Zero A.E.,https://www.reddit.com/r/MachineLearning/comments/aqfl70/d_neural_networks_with_second_derivative_zero_ae/,you-get-an-upvote,1550116471,"It's possible this is considered a trivial observation and that nobody has bothered to tell me, but I thought I'd note something I discovered today -- for posterity if nothing else:

If your activation function and error function have second derivatives of zero (almost everywhere) then the second derivative of your error with respect to its parameters is zero (almost everywhere).

Let f(x) be your model (i.e. a neural network) and consider loss(f(x), y).  Then, using D\[a,b\] = da/db, we have

    D[loss,x] = D[loss,f] * D[f,x]

and

    D^2[loss,x] = D^2[loss,f] * D[f,x] + D[loss,f] D^2[f,x]

so when

    (D^2[loss,f] = 0) and (D^2[f,x] = 0)   implies   D^2[loss,x] = 0

The first term is D\^2\[loss,f\] and is just the second derivative of the loss function.  We can force it to be zero by (e.g.) using L1 loss or Hinge loss.

The second term is D\^2\[f,x\] which is the second derivative of your model's prediction with respect to some parameter x.

Since a neural network is just an alternating series of linear functions (i.e. functions that merely scale f, f', f'', etc. by a constant factor) and nonlinear activations, if the second derivative of the nonlinear activations is zero almost everywhere, D\^2\[f,x\] must be zero almost everywhere too.  Here is a helpful example (to show I'm not just making the Calculus up).  Let R be some activation function such that R'' = 0 a.e. (e.g. ReLU)

    let f = a * R(b * R(c * x))
    D^2[f,c] = (a * b * x) * [ R'(b * R(c * x)) * R'(c * x) ]

Both components of the term between brackets \[...\] has a second derivative of zero, hence the entire expression does as well (you can perform the product rule if you'd like).  If we added more layers there would be more components but all of these components' second derivatives would still be zero, so the derivative of \[...\] would still be zero.  (It's important to note that if *any* activation function has a non-zero second derivative, then this theorem is ruined).

The most common activation with a zero second derivative is ReLU, but any piecewise linear function suffices (e.g. leaky ReLU).

This seems particularly interesting given the idea that ""small second derivatives imply good generalization"".  At least on the face of it, this seems to be nonsense!  It's easy to create neural networks with ReLU activations and trained with L1 loss that over-fit tremendously, despite having a second derivative of zero!

Still we should be careful not to claim too much.  While the second derivative is technically zero a.e., in practice the number of output nodes and the size of the batch will mean the first derivative jumps a lot!  Sure in the limit as h-&gt;0, (f'(x+h) - f'(x)) / h = 0, but it is likely the case that there are a large number of discontinuities within the ball around x, even for very small h.",14,1,False,self,,,,,
827,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,13,aqfnke,github.com,"MPPI implementation of the paper ""Information Theoretic MPC for Model-Based Reinforcement Learning"" for OpenAI gym pendulum",https://www.reddit.com/r/MachineLearning/comments/aqfnke/mppi_implementation_of_the_paper_information/,whiletrue2,1550116924,,0,1,False,default,,,,,
828,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,13,aqfqno,self.MachineLearning,What is the ML job outlook like for those that will be finishing their undergrad in 2-3 years?,https://www.reddit.com/r/MachineLearning/comments/aqfqno/what_is_the_ml_job_outlook_like_for_those_that/,l0__0I,1550117546,[removed],0,1,False,self,,,,,
829,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,13,aqfro4,self.MachineLearning,[D] PHD Machine Learning Research Topics,https://www.reddit.com/r/MachineLearning/comments/aqfro4/d_phd_machine_learning_research_topics/,legacy-of-x,1550117750,"I am a PhD student who recently switched advisor to focus on ML. This topic intrigues me a lot and I have already worked a little with this. Even though I have thought of a few projects, I dont think they would be good enough to get a PhD. If anyone has any ideas or advice, it would be greatly appreciated. ",11,1,False,self,,,,,
830,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,13,aqftk3,arxiv.org,[R] Certified Adversarial Robustness via Randomized Smoothing,https://www.reddit.com/r/MachineLearning/comments/aqftk3/r_certified_adversarial_robustness_via_randomized/,downtownslim,1550118129,,15,1,False,default,,,,,
831,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,14,aqgbh1,arxiv.org,[R] Lyapunov-based approach to safe reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/aqgbh1/r_lyapunovbased_approach_to_safe_reinforcement/,JughHackman69,1550121715,,1,1,False,default,,,,,
832,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,14,aqggth,self.MachineLearning,Research in Active Learning,https://www.reddit.com/r/MachineLearning/comments/aqggth/research_in_active_learning/,_1427_,1550122830,"I'm currently looking for research directions. Recently, I come across active learning, a problem in which we need to come up with smart strategies to query the most useful &amp; informative examples to train our model, so that the model still achieves powerful capability but with as few labeled data as possible. I find this quite interesting and think it would have important application towards problems that have expensive annotation cost (e.g. medical problems).

&amp;#x200B;

However, I find that active learning is not being ""actively"" researched right now (very few papers about it). I guess this is because there isn't a good benchmark to evaluate the performance of an active learning method (most of the recent work in active learning experiment on easy datasets such as MNIST, SVHN). Furthermore, most active learning methods have huge computation time (e.g. finding the most informative example requires looping over all examples in the training set, finding the example that results in the greatest model parameters change also requires taking an expectation over all possible classes of that example).

&amp;#x200B;

What do people think about active learning? Is it a good research direction to dive into right now, given that I am a student and just started doing research? Thank you.",0,1,False,self,,,,,
833,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,14,aqggw9,self.MachineLearning,SeqGans Output numbers,https://www.reddit.com/r/MachineLearning/comments/aqggw9/seqgans_output_numbers/,jmarsha5,1550122847,Hey guys have a quick question about SeqGANs for language generation. I pass in text to the SeqGAN but get varying numbers instead of words. I'm guessing these numbers map to the words but how do I access that mapping.  It wasn't made to clear in this github repo here --&gt;[https://github.com/bhushan23/Transformer-SeqGAN-PyTorch](https://github.com/bhushan23/Transformer-SeqGAN-PyTorch),0,1,False,self,,,,,
834,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,15,aqgmok,youtu.be,Video: Hacking Machine Learning with Amy Boyd!,https://www.reddit.com/r/MachineLearning/comments/aqgmok/video_hacking_machine_learning_with_amy_boyd/,shehackspurple,1550124088,,1,1,False,https://b.thumbs.redditmedia.com/uA2vkgfEEoGOSAJZo2qYQBqHgK3qg1WdStRpR-BMvfw.jpg,,,,,
835,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,15,aqh0ly,self.MachineLearning,Web crawler for Youtube Advertisements,https://www.reddit.com/r/MachineLearning/comments/aqh0ly/web_crawler_for_youtube_advertisements/,raijinraijuu,1550127200,[removed],0,1,False,self,,,,,
836,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,16,aqh36d,self.MachineLearning,[Research] Web Crawler for Youtube Advertisements,https://www.reddit.com/r/MachineLearning/comments/aqh36d/research_web_crawler_for_youtube_advertisements/,raijinraijuu,1550127798,"I want to retrieve the videos, descriptions and possibly the captions of youtube advertisements to train a classifier. I couldn't find any ""smart"" way of doing this. Any suggestions are welcome.",4,1,False,self,,,,,
837,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,16,aqhb55,self.MachineLearning,[R] TDLS: Tensor Field Networks: Rotation- and translation-equivariant neural networks for 3D point clouds,https://www.reddit.com/r/MachineLearning/comments/aqhb55/r_tdls_tensor_field_networks_rotation_and/,tdls_to,1550129684,"* Video: [https://www.youtube.com/watch?v=COgEuIAnhtk](https://www.youtube.com/watch?v=COgEuIAnhtk)
* Slides: [https://tdls.a-i.science/static/slides/20190211\_PengCheng\_ChrisDryden.pdf](https://tdls.a-i.science/static/slides/20190211_PengCheng_ChrisDryden.pdf)
* paper: [https://arxiv.org/abs/1802.08219](https://arxiv.org/abs/1802.08219)
* Event page: [https://tdls.a-i.science/events/2019-02-11/](https://tdls.a-i.science/events/2019-02-11/)

# Motivation

In this session we will discuss the relationship between data augmentation, invariant/equivariant features and the abstract concept of convolution layers, specifically, how this abstraction can be extended to devise concrete neural network architectures that are robust to diverse data and augmentation types (all of which are published after 2016). We will focus on the latest of the series, designed to handle spatial graph data augmentable by 3d translations &amp; rotations. In the end, we will showcase it's applications in molecule analysis and autonomous flight. We will focus on the latest of the series, designed to handle spatial graph data augmentable by 3d translations &amp; rotations. In the end, we will showcase it's applications in molecule analysis and autonomous flight.

# Discussion points

* Paper did not go in depth about the information stored in the points
   * More applications are possible. Would the chemical dataset work with chemical properties.
* Can it be applied to more traditional 3d-image sets?
   * Eg: Autonomous Driving
* Can we incorporate in this architecture other types of symmetries?
   * Ex: Mirror Symmetries, R-L Enantiomers
* Can this be applied to Neural-ODEs?

[Subscribe to TDLS YouTube Channel](https://www.youtube.com/c/TorontoDeepLearningSeries?view_as=subscriber&amp;sub_confirmation=1)",0,1,False,self,,,,,
838,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,17,aqhle3,self.MachineLearning,Seq2Seq vs vanilla LSTM,https://www.reddit.com/r/MachineLearning/comments/aqhle3/seq2seq_vs_vanilla_lstm/,shivam2298,1550132366,[removed],0,1,False,self,,,,,
839,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,18,aqhvjx,medium.com,Neural Networks seem to follow a puzzlingly simple strategy to classify images,https://www.reddit.com/r/MachineLearning/comments/aqhvjx/neural_networks_seem_to_follow_a_puzzlingly/,ThisIs_MyName,1550135118,,0,1,False,https://b.thumbs.redditmedia.com/u78YN4TMXqS3Jmg9gfwRhCsA_FZNfazvgHOAZAjwewA.jpg,,,,,
840,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,18,aqhzdq,sciencedaily.com,Smarter brains run on sparsely connected neurons,https://www.reddit.com/r/MachineLearning/comments/aqhzdq/smarter_brains_run_on_sparsely_connected_neurons/,deftware,1550136131,,0,1,False,default,,,,,
841,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,18,aqi3sw,self.MachineLearning,[R] Neural Networks seem to follow a puzzlingly simple strategy to classify images,https://www.reddit.com/r/MachineLearning/comments/aqi3sw/r_neural_networks_seem_to_follow_a_puzzlingly/,skariel,1550137328,"&amp;#x200B;

[This Reasearch](https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f) shows many similarities between deep convolutional image classification networks and good old ""bag of features"". This gives some really interesting insight into the inner workings of current CNNs",65,1,False,self,,,,,
842,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,18,aqi5nc,youtu.be,"""Machine Learning on Source Code"" with Francesc Campoy (47min talk from GOTO Copenhagen 2018)",https://www.reddit.com/r/MachineLearning/comments/aqi5nc/machine_learning_on_source_code_with_francesc/,goto-con,1550137851,,0,1,False,default,,,,,
843,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,18,aqi76p,medium.com,"""PyTorch: Zero to GANs"" - A series of coding-focused tutorials on Deep Learning with PyTorch",https://www.reddit.com/r/MachineLearning/comments/aqi76p/pytorch_zero_to_gans_a_series_of_codingfocused/,aakashns,1550138252,,1,1,False,default,,,,,
844,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,19,aqicif,self.MachineLearning,"""Machine Learning on Source Code with"" Francesc Campoy",https://www.reddit.com/r/MachineLearning/comments/aqicif/machine_learning_on_source_code_with_francesc/,goto-con,1550139600,[removed],0,1,False,self,,,,,
845,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,19,aqig1n,self.MachineLearning,good practical introduction to tensorflow?,https://www.reddit.com/r/MachineLearning/comments/aqig1n/good_practical_introduction_to_tensorflow/,user48392,1550140572,[removed],0,1,False,self,,,,,
846,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,19,aqiin0,self.MachineLearning,Montezuma's Revenge with OpenAI Baseline's,https://www.reddit.com/r/MachineLearning/comments/aqiin0/montezumas_revenge_with_openai_baselines/,alpha_ma,1550141272,[removed],0,1,False,self,,,,,
847,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,20,aqipxy,self.MachineLearning,Contextual Chatbot Powered By Artificial Intelligence (AI) and Its Use Cases,https://www.reddit.com/r/MachineLearning/comments/aqipxy/contextual_chatbot_powered_by_artificial/,vijay2208,1550143113,[removed],0,1,False,self,,,,,
848,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,20,aqiq39,self.MachineLearning,State of the art mask segmentation,https://www.reddit.com/r/MachineLearning/comments/aqiq39/state_of_the_art_mask_segmentation/,nyquist_karma,1550143155,"Hi,

I would like to train a network using images and their corresponding segmentation masks. The goal is to input an image and then predict the corresponding segmentation mask. Do you have any ideas on which is the state-of-the-art in doing that?",0,1,False,self,,,,,
849,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,20,aqir4e,self.MachineLearning,[Project] : Multi-modal emtion recognition,https://www.reddit.com/r/MachineLearning/comments/aqir4e/project_multimodal_emtion_recognition/,omrane8,1550143408,"Hello ML community,

So I'm starting a new graduation project about emotion recognition from video recordings.

Any suggestions about papers or resources to start with?

Thank you so much.",3,1,False,self,,,,,
850,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,20,aqiy0e,self.MachineLearning,[N] 1st International Workshop on Reservoir Computing at ICANN 2019,https://www.reddit.com/r/MachineLearning/comments/aqiy0e/n_1st_international_workshop_on_reservoir/,scardax88,1550145074,"Hi everyone, we are accepting submissions for the 1st International Workshop on Reservoir Computing that we are organizing at ICANN 2019, as part of the activities of the IEEE task force on RC. We already have a planned keynote speech from Dr. Maass, and selected papers will be extended for a special issue we are planning.
Link: https://sites.google.com/view/reservoir-computing-workshop",0,1,False,self,,,,,
851,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,21,aqj20f,self.MachineLearning,automatic gradients vs. analytical gradients?,https://www.reddit.com/r/MachineLearning/comments/aqj20f/automatic_gradients_vs_analytical_gradients/,jeyhounmarks,1550145929,[removed],0,1,False,self,,,,,
852,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,21,aqj80j,self.MachineLearning,Machine Learning Hello World using Python,https://www.reddit.com/r/MachineLearning/comments/aqj80j/machine_learning_hello_world_using_python/,MCAL_Training,1550147208,[removed],0,1,False,self,,,,,
853,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,21,aqj8ra,medium.com,[P] Mastering Fast Gradient Boosting on Google Colaboratory with free GPU,https://www.reddit.com/r/MachineLearning/comments/aqj8ra/p_mastering_fast_gradient_boosting_on_google/,s0ulmate,1550147364,,0,1,False,default,,,,,
854,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,22,aqjmyg,self.MachineLearning,"[News] ""One does not simply put Machine Learning into Production"" with Henrik Brink (37min talk from GOTO Copenhagen)",https://www.reddit.com/r/MachineLearning/comments/aqjmyg/news_one_does_not_simply_put_machine_learning/,goto-con,1550150160,"* [Video](https://youtu.be/JKxIiSfWtjI)
* [Sldies](https://gotocph.com/2017/sessions/150)

ABSTRACT  
When deciding to infuse existing products with machine-learning smarts, or building ML-first products, there are multiple challenges to be aware of.

First, you and your organization need to understand important dimensions -- accuracy, cost, maintainability, interpretability -- and trade-offs between them. Second, several technical challenges present themselves when deploying data science experiments into production environments.

I will share some lessons learned while building ML products serving billions of predictions to live customers -- and hopefully provide some take-aways for anyone in the audience looking to indeed put machine learning into production.

&amp;#x200B;",0,1,False,self,,,,,
855,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,22,aqjt4y,self.MachineLearning,Data ingestion from http APIs to kafka to generate dataset for ML project,https://www.reddit.com/r/MachineLearning/comments/aqjt4y/data_ingestion_from_http_apis_to_kafka_to/,ghaaawd,1550151363,[removed],0,1,False,self,,,,,
856,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,22,aqjycx,eduonix.com,Anyone taken this course? What are your thoughts? Is it good?,https://www.reddit.com/r/MachineLearning/comments/aqjycx/anyone_taken_this_course_what_are_your_thoughts/,_thekinginthenorth,1550152321,,0,1,False,default,,,,,
857,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,23,aqk3gx,self.MachineLearning,Fair Forecasting Timeseries with Wave Transforms,https://www.reddit.com/r/MachineLearning/comments/aqk3gx/fair_forecasting_timeseries_with_wave_transforms/,fasdfsdfasdag,1550153220,[removed],0,1,False,https://b.thumbs.redditmedia.com/kNR_mz1a3nS0As9JF049b5mkVztZV4HG6D8EM3pREcs.jpg,,,,,
858,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,23,aqkg7j,i.redd.it,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aqkg7j/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550155451,,1,1,False,default,,,,,
859,MachineLearning,t5_2r3gv,2019-2-14,2019,2,14,23,aqklea,self.MachineLearning,[D] Any opinions about BigML platform for ML?,https://www.reddit.com/r/MachineLearning/comments/aqklea/d_any_opinions_about_bigml_platform_for_ml/,x-3cutioner,1550156332,"My company is currently busy with some implementation projects of Machine learning.

What are the drawbacks of using such platforms as BigML? 
What is the difference between these platforms and the librarys such as Tenserflow? 
If you have some experience with BigML is it any good?

",1,1,False,self,,,,,
860,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqknzt,self.MachineLearning,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aqknzt/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550156761,"&amp;#x200B;

https://i.redd.it/4ijx7wobujg21.png",1,1,False,https://b.thumbs.redditmedia.com/czbCDsHKaeQsHZOnXo1ZoMke__bZxd8fRRfW9voNu7Y.jpg,,,,,
861,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqkqb5,self.MachineLearning,"Can I learn Machine Learning if I am in class 8th? What topics in mathematics are compulsory? If I will be able to, please give me some free resources on internet.",https://www.reddit.com/r/MachineLearning/comments/aqkqb5/can_i_learn_machine_learning_if_i_am_in_class_8th/,Ashutoshkv,1550157130,[removed],0,1,False,self,,,,,
862,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqkrc6,i.redd.it,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aqkrc6/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550157288,,0,1,False,default,,,,,
863,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqksei,self.MachineLearning,Does a machine learning AI develop transferable skills?,https://www.reddit.com/r/MachineLearning/comments/aqksei/does_a_machine_learning_ai_develop_transferable/,Simon_Drake,1550157450,[removed],0,1,False,self,,,,,
864,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqkv7p,i.redd.it,[R]MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aqkv7p/rmiso_mutual_information_loss_with_stochastic/,the-fire-fist,1550157925,,0,1,False,default,,,,,
865,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqkviy,self.MachineLearning,Things you can do while waiting for your models to converge,https://www.reddit.com/r/MachineLearning/comments/aqkviy/things_you_can_do_while_waiting_for_your_models/,Mayalittlepony,1550157971,[removed],2,1,False,self,,,,,
866,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqkwbv,i.redd.it,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aqkwbv/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550158105,,0,1,False,default,,,,,
867,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqkwth,i.redd.it,[R] Neural Galaxy Image Generation,https://www.reddit.com/r/MachineLearning/comments/aqkwth/r_neural_galaxy_image_generation/,kooro1,1550158192,,0,1,False,default,,,,,
868,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aqky99,i.redd.it,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aqky99/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550158418,,0,1,False,https://b.thumbs.redditmedia.com/2Jxn60UTaiPNdsrxwMPHAAaApiB-7spTi7a4U8lgWZw.jpg,,,,,
869,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aql0sb,i.redd.it,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aql0sb/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550158834,,0,1,False,default,,,,,
870,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aql3jk,self.MachineLearning,Does anyone have the book deep learning for natural language processing by Jason Brownlee?,https://www.reddit.com/r/MachineLearning/comments/aql3jk/does_anyone_have_the_book_deep_learning_for/,afterburners_engaged,1550159276,[removed],0,1,False,self,,,,,
871,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aql42m,imgur.com,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aql42m/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550159360,,0,1,False,default,,,,,
872,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,0,aql7zc,self.MachineLearning,[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/aql7zc/r_miso_mutual_information_loss_with_stochastic/,the-fire-fist,1550159992,"&amp;#x200B;

[Results of multimodal image-to-image translation on multiple datasets](https://i.redd.it/qc38ogz33kg21.png)

arXiv : [https://arxiv.org/abs/1902.03938](https://arxiv.org/abs/1902.03938)",7,1,False,self,,,,,
873,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,1,aqlcjv,arxiv.org,[R] Microsoft research won the 2018 text adventure game AI competition and describes their agent in a new paper,https://www.reddit.com/r/MachineLearning/comments/aqlcjv/r_microsoft_research_won_the_2018_text_adventure/,jinpanZe,1550160685,,2,1,False,default,,,,,
874,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,1,aqlffh,self.MachineLearning,Can you train a Deep Neural Net to segment an organ with a single annotated slice? Check out my and Abhijit Guha Roy recent work on few-shot learning for segmenting volumetric medical scans,https://www.reddit.com/r/MachineLearning/comments/aqlffh/can_you_train_a_deep_neural_net_to_segment_an/,shayansiddiqui,1550161133,I am super excited to bring to your attention one of our recent work on few-shot segmentation. The results of this research are amazing in my opinion and I think it could be a basis for other significant future works. Please feel free to share your views.,0,1,False,self,,,,,
875,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,1,aqlfjv,arxiv.org,[R] Mad Max: Affine Spline Insights into Deep Learning,https://www.reddit.com/r/MachineLearning/comments/aqlfjv/r_mad_max_affine_spline_insights_into_deep/,IborkedyourGPU,1550161154,,1,1,False,default,,,,,
876,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,1,aqlij4,arxiv.org,[R] Analysing and Improving Representations with the Soft Nearest Neighbor Loss,https://www.reddit.com/r/MachineLearning/comments/aqlij4/r_analysing_and_improving_representations_with/,ryanbuck_,1550161633,,2,1,False,default,,,,,
877,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,1,aqlmxv,self.MachineLearning,How to start learning Machine learning?,https://www.reddit.com/r/MachineLearning/comments/aqlmxv/how_to_start_learning_machine_learning/,ahmadjaved97,1550162341,[removed],0,1,False,self,,,,,
878,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,1,aqlsx0,self.MachineLearning,90% Accuracy on Kaggle's Digit Recognizer with C++ Neural Network from Scratch - https://www.youtube.com/playlist?list=PLitWaJK2QKeHXUmaSPzMNPjSkhFJlsIDy,https://www.reddit.com/r/MachineLearning/comments/aqlsx0/90_accuracy_on_kaggles_digit_recognizer_with_c/,pesheto,1550163240,[removed],0,1,False,self,,,,,
879,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,2,aqlzde,self.MachineLearning,[R] OpenAI: Better Language Models and Their Implications,https://www.reddit.com/r/MachineLearning/comments/aqlzde/r_openai_better_language_models_and_their/,jinpanZe,1550164193,"https://blog.openai.com/better-language-models/

""Weve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization  all without task-specific training.""

Interestingly,

""Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a muchsmaller modelfor researchers to experiment with, as well as atechnical paper.""",143,1,False,self,,,,,
880,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,2,aqm29r,blog.openai.com,[R] Better Language Models and Their Implications,https://www.reddit.com/r/MachineLearning/comments/aqm29r/r_better_language_models_and_their_implications/,evc123,1550164621,,2,1,False,default,,,,,
881,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,2,aqm2av,medium.com,[N] OpenAI Guards Its ML Model Code &amp; Data to Thwart Malicious Usage,https://www.reddit.com/r/MachineLearning/comments/aqm2av/n_openai_guards_its_ml_model_code_data_to_thwart/,gwen0927,1550164627,,0,1,False,default,,,,,
882,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,3,aqmm1i,self.MachineLearning,#Amazon Simple Storage Service (Amazon S3),https://www.reddit.com/r/MachineLearning/comments/aqmm1i/amazon_simple_storage_service_amazon_s3/,AWSTrainingInHouston,1550167606,[removed],0,1,False,self,,,,,
883,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,3,aqmntq,self.MachineLearning,Recommendation system with ratings and user data,https://www.reddit.com/r/MachineLearning/comments/aqmntq/recommendation_system_with_ratings_and_user_data/,marctorsoc,1550167868,[removed],0,1,False,self,,,,,
884,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,3,aqmp3a,self.MachineLearning,[R] XLM  Enhancing BERT for Cross-lingual Language Model,https://www.reddit.com/r/MachineLearning/comments/aqmp3a/r_xlm_enhancing_bert_for_crosslingual_language/,ranihorev,1550168050,"I wrote a summary of a new model by FAIR that takes BERT a step forward for cross-lingual language models and achieves SOTA results in cross-lingual classification and translation tasks. 

The paper includes 3 interesting concepts: 

1. Training BERT with multi-lingual BPE tokens instead of words to increase shared vocabulary.  
2. Training BERT with dual-language input to learn cross-language context. 
3. Initializing translation model embeddings with pretrained BERT to improve Back-Translation.

Summary: [https://www.lyrn.ai/2019/02/11/xlm-cross-lingual-language-model/](https://www.lyrn.ai/2019/02/11/xlm-cross-lingual-language-model/)

Code: [https://github.com/facebookresearch/XLM/](https://github.com/facebookresearch/XLM/)

&amp;#x200B;

I'd love to hear your thoughts about the model",5,1,False,self,,,,,
885,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,3,aqn644,self.MachineLearning,GPU while travelling,https://www.reddit.com/r/MachineLearning/comments/aqn644/gpu_while_travelling/,Maplernothaxor,1550170513,[removed],0,1,False,self,,,,,
886,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,3,aqn6us,self.MachineLearning,[D] Cloud GPU options,https://www.reddit.com/r/MachineLearning/comments/aqn6us/d_cloud_gpu_options/,Maplernothaxor,1550170614,"I will be travelling for the next few months and wont have access to a GPU. I have never used any cloud-based GPU solutions before.

What free options do I have that can integrate well with my local files/repos?",16,1,False,self,,,,,
887,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,4,aqn85s,self.MachineLearning,Careers in #ML related to sales and business strategy?,https://www.reddit.com/r/MachineLearning/comments/aqn85s/careers_in_ml_related_to_sales_and_business/,apensity,1550170808,"While retraining myself in ML and Data Analysis, I am looking to enter into a slightly different career than I have now in SaaS sales. I have over 20 years in sales related to tech and finance, but ML has me entranced! I'd like to develop my new direction into working with startups and ML to drive business success. I'm just looking for some ideas, possibly some companies you might know of, titles of positions related to my background and interest in ML and Business success, etc. Also, if you have any suggestions on good pages, SM accounts, forums, etc. I should follow, not for the programming and technical aspects, but the overall usage of ML for business, I would be appreciative. ",0,1,False,self,,,,,
888,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,4,aqnrb4,medium.com,Federated Learning: The Future of Distributed Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aqnrb4/federated_learning_the_future_of_distributed/,gwen0927,1550173597,,0,1,False,default,,,,,
889,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,5,aqnznh,self.MachineLearning,[D] BagNet results are being wildly misinterpreted. ImageNet is just easy to solve with bag of features.,https://www.reddit.com/r/MachineLearning/comments/aqnznh/d_bagnet_results_are_being_wildly_misinterpreted/,AlivePomegranate,1550174873,"[https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f](https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f)

The amount of people using this as an indictment of neural networks and ignoring the ImageNet aspect is embarrassing. To be fair, the paper and blog post provides a certain narrative for results which \*\*are not\*\* conclusive at all. 

This title:

""Neural Networks seem to follow a puzzlingly simple strategy to classifyimages"" is very deceptive.

&amp;#x200B;

They perform experiments on ImageNet and ConvNets together, therefore their results cannot disentangle which aspect is causing a 'bag of features' to be learned. This is basic scientific method. If you want to make a conclusion specifically about neural networks, you have to show an experiment that removes ImageNet as a factor.

Luckily, the large body of research on GANs provides a counter example that clearly counters this paper's message. We can say with reasonable confidence that Bag of Features is all you need to perform well on ImageNet. Therefore, ImageNet just turns out to be a much easier task to solve than people previously thought.",21,1,False,self,,,,,
890,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,5,aqoc4y,self.MachineLearning,[D] How to handle variable inputs in machine learning? (Number of players in a card game),https://www.reddit.com/r/MachineLearning/comments/aqoc4y/d_how_to_handle_variable_inputs_in_machine/,Zealousideal_Grab,1550176724,"As per the title, I am creating a model to support an AI I am making for my final year project at university. I was wondering if there is an efficient way to handle a variable number of players in a game such as poker.  

There is a finite number of cards so I know it would be possible to create a mapping of [card, card, bet, card, card, bet] and then have [card, card, card, card, card], using null/zero values if a player is not in the game, but this would be inefficient to learn from, fails to highlight the players cards and fails to take into account factors like re-raising (which would also have a variable number of inputs).


The project is for next year so I have a significant amount of time to implement this so any pointers in the right direction would be greatly appreciated, thanks! 
",6,1,False,self,,,,,
891,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,5,aqojdb,stackoverflow.com,"What is meant by ""stationarity of statistics"" in the context of CNNs?",https://www.reddit.com/r/MachineLearning/comments/aqojdb/what_is_meant_by_stationarity_of_statistics_in/,pepitolander,1550177814,,1,1,False,default,,,,,
892,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,6,aqot9s,self.MachineLearning,"Can the model of this persondoesnotexist be applied for subsets like ""models""",https://www.reddit.com/r/MachineLearning/comments/aqot9s/can_the_model_of_this_persondoesnotexist_be/,Nicestory112,1550179299,[removed],0,1,False,self,,,,,
893,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,6,aqovhz,self.MachineLearning,[Discussion] Should I release my MNIST model or keep it closed source fearing malicious use?,https://www.reddit.com/r/MachineLearning/comments/aqovhz/discussion_should_i_release_my_mnist_model_or/,astonished_crofty,1550179641,Today I trained a 23064 layer ResNet and it got 99.6% accuracy on MNIST. I would love to share the model but I fear it being used maliciously. What if it is used to read documents by the Russians? What are your thoughts?,177,1,False,self,,,,,
894,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,10,aqr5sb,self.MachineLearning,[D] Thoughts on Andrew Yang? He's running a U.S. 2020 Presidential Candidate running on the fear that AI will destroy jobs.,https://www.reddit.com/r/MachineLearning/comments/aqr5sb/d_thoughts_on_andrew_yang_hes_running_a_us_2020/,tensorluffy,1550193317,"&gt;Im Andrew Yang, and Im running for President as a Democrat in 2020 because I fear for thefuture of our country. New technologies  robots, software, artificial intelligence  have already destroyedmore than 4 million US jobs, and in the next 5-10 years, they will eliminate millions more. A third of all American workers are at risk of permanent unemployment. And this time, the jobs will not come back.

Do you think that our current AI technologies (e.g., autonomous cars, CNN's for radiology, and Google Duplex) really change our economy in the next 5-10 years? ",14,1,False,self,,,,,
895,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,10,aqr7s6,self.MachineLearning,Prosody representation?,https://www.reddit.com/r/MachineLearning/comments/aqr7s6/prosody_representation/,synysterbates,1550193661,[removed],0,1,False,self,,,,,
896,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,10,aqrhj9,blog.floydhub.com,Controlling robotic arm with deep reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/aqrhj9/controlling_robotic_arm_with_deep_reinforcement/,adamnemecek,1550195475,,0,1,False,https://b.thumbs.redditmedia.com/vs3jtfXh7YKf-CvJdi2b0cKcSfz2jbS1Kq0umuLqw6I.jpg,,,,,
897,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,11,aqrtw6,creospiders.com,How Machine Learning Works ?,https://www.reddit.com/r/MachineLearning/comments/aqrtw6/how_machine_learning_works/,Pranav2719,1550197827,,0,1,False,default,,,,,
898,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,12,aqs3wb,amazon.com,coffee press,https://www.reddit.com/r/MachineLearning/comments/aqs3wb/coffee_press/,lanitavumahaxay,1550199735,,0,1,False,https://b.thumbs.redditmedia.com/pZM90O6Yj-OjR5kJuT_7Yb1KcFhJq-8GgxRC2hf20TQ.jpg,,,,,
899,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,13,aqt130,self.MachineLearning,[D] What is the best implementation/approach of Byte pair encoding?,https://www.reddit.com/r/MachineLearning/comments/aqt130/d_what_is_the_best_implementationapproach_of_byte/,DeepLearningDownundr,1550206485,Many deep learning NLP papers utilise BPE. I have seen a number of implementations. Looking for some ideas around which are considered the best? - or if they are all the same,4,1,False,self,,,,,
900,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,14,aqt3nt,self.MachineLearning,best practice/methods for facial marker tracking via ML,https://www.reddit.com/r/MachineLearning/comments/aqt3nt/best_practicemethods_for_facial_marker_tracking/,svenip,1550207000," 

Hi,

i am investigating if i can adopt ML to predict markers on faces. to  describe the problem better: a very typical method for facial  performance capture is to put markers on actors faces and later on track  their positions frame by frame inside the image dimension. so let's say  the actor has 20 markers and their position is in 2D space. i do have a  large dataset with manual tracked markers and their corresponding  images. Right now i'm trying to see what approach is the best to take  here. at it's simplest i already tried a convolutional net where the  output is a flattened array for the marker positions. this got me  relative close but i need much more accuracy. right now i'm looking into  autoencoders as an alternative but haven't fully implemented anything  yet.

are there other known networks which might be best suited for this task?",0,1,False,self,,,,,
901,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,14,aqt8rt,self.MachineLearning,Fake news machine.,https://www.reddit.com/r/MachineLearning/comments/aqt8rt/fake_news_machine/,Jackson_Filmmaker,1550208049,[removed],0,1,False,self,,,,,
902,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,14,aqt9gf,blockdelta.io,Artificial Intelligence Disrupts and Transforms key Industry Verticals,https://www.reddit.com/r/MachineLearning/comments/aqt9gf/artificial_intelligence_disrupts_and_transforms/,BlockDelta,1550208194,,0,1,False,default,,,,,
903,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,15,aqtv61,self.MachineLearning,How to convert .npy to .jpeg?,https://www.reddit.com/r/MachineLearning/comments/aqtv61/how_to_convert_npy_to_jpeg/,sourya155,1550213084,[removed],0,1,False,self,,,,,
904,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,15,aqtyd4,jigsawacademy.com,Machine Learning and AI Bootcamp,https://www.reddit.com/r/MachineLearning/comments/aqtyd4/machine_learning_and_ai_bootcamp/,fullstackanalytics1,1550213846,,0,1,False,default,,,,,
905,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,17,aquftr,github.com,I implemented a MLP in JavaScript,https://www.reddit.com/r/MachineLearning/comments/aquftr/i_implemented_a_mlp_in_javascript/,atum47,1550218260,,0,1,False,default,,,,,
906,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,17,aquje4,self.MachineLearning,[P] machine learning and games,https://www.reddit.com/r/MachineLearning/comments/aquje4/p_machine_learning_and_games/,atum47,1550219196,"Recently I uploaded a experiment: can a Perceptron play a simple game?

https://github.com/victorqribeiro/carGamePerceptron

I did this to prove that you can do interesting things with simple code. Now I just finished my MLP, so I can do more robust things. I'm thinking about something with image recognition. Anyways, here's the link in case you want to see the code.

https://github.com/victorqribeiro/mlp",2,1,False,self,,,,,
907,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,17,aqukpi,self.MachineLearning,Need some clarification with Deep Belief Networks,https://www.reddit.com/r/MachineLearning/comments/aqukpi/need_some_clarification_with_deep_belief_networks/,sudheer2015,1550219579,[removed],0,1,False,self,,,,,
908,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,17,aqun05,self.MachineLearning,Developing a grammar corrector with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aqun05/developing_a_grammar_corrector_with_machine/,fedetask,1550220228,[removed],0,1,False,self,,,,,
909,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,18,aqv1l2,self.MachineLearning,"[N] ""Machine Learning with TensorFlow and Google Cloud"" with Vijay Reddy (53min talk from GOTO Copenhagen)",https://www.reddit.com/r/MachineLearning/comments/aqv1l2/n_machine_learning_with_tensorflow_and_google/,goto-con,1550224205,"* [Video](https://youtu.be/ZuMdaXNR9Mk)
* [Slides](https://gotocph.com/2017/sessions/249)

&amp;#x200B;

ABSTRACT

What is TensorFlow, why is it so popular, and how can you leverage it to build Machine Learning applications?

We will walk through an end-to-end example including data ingestion, training, and prediction on a real dataset using a neural network. This talk will also cover how to use Google Cloud to supercharge your training and prediction as well as to remove pain from your development and operational workflows.",0,1,False,self,,,,,
910,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,19,aqvajg,self.MachineLearning,Doubt: Is Depth wise Convolution similar to MapReduce?,https://www.reddit.com/r/MachineLearning/comments/aqvajg/doubt_is_depth_wise_convolution_similar_to/,gapten-the-captain,1550226571,[removed],0,1,False,self,,,,,
911,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,19,aqvgll,self.MachineLearning,"[D] Building Deep Learning enabled PC for 2019 (currently without GPU, but enough room for upgrades)",https://www.reddit.com/r/MachineLearning/comments/aqvgll/d_building_deep_learning_enabled_pc_for_2019/,thebluebloo,1550228232,"DISCLAIMER: This is my first proper reddit post and also my first PC build.

&amp;#x200B;

\[PCPartPicker part list\]([https://pcpartpicker.com/list/bFG4TB](https://pcpartpicker.com/list/bFG4TB)) / \[Price breakdown by merchant\]([https://pcpartpicker.com/list/bFG4TB/by\_merchant/](https://pcpartpicker.com/list/bFG4TB/by_merchant/))

&amp;#x200B;

Type|Item|Price

:----|:----|:----

\*\*CPU\*\* | \[Intel - Core i7-9700K 3.6 GHz 8-Core Processor\]([https://pcpartpicker.com/product/WtyV3C/intel-core-i7-9700k-36ghz-8-core-processor-bx80684i79700k](https://pcpartpicker.com/product/WtyV3C/intel-core-i7-9700k-36ghz-8-core-processor-bx80684i79700k)) | $409.99 @ Amazon 

\*\*CPU Cooler\*\* | \[Cooler Master - MasterLiquid ML240L RGB 66.7 CFM Liquid CPU Cooler\]([https://pcpartpicker.com/product/RcdFf7/cooler-master-masterliquid-ml240l-rgb-667-cfm-liquid-cpu-cooler-mlw-d24m-a20pc-r1](https://pcpartpicker.com/product/RcdFf7/cooler-master-masterliquid-ml240l-rgb-667-cfm-liquid-cpu-cooler-mlw-d24m-a20pc-r1)) | $69.99 @ B&amp;H 

\*\*Motherboard\*\* | \[Gigabyte - Z390 AORUS PRO WIFI ATX LGA1151 Motherboard\]([https://pcpartpicker.com/product/N6gzK8/gigabyte-z390-aorus-pro-wifi-atx-lga1151-motherboard-z390-aorus-pro-wifi](https://pcpartpicker.com/product/N6gzK8/gigabyte-z390-aorus-pro-wifi-atx-lga1151-motherboard-z390-aorus-pro-wifi)) | $202.98 @ SuperBiiz 

\*\*Memory\*\* | \[G.Skill - Aegis 16 GB (2 x 8 GB) DDR4-3000 Memory\]([https://pcpartpicker.com/product/FNprxr/gskill-aegis-16gb-2-x-8gb-ddr4-3000-memory-f43000c16d16gisb](https://pcpartpicker.com/product/FNprxr/gskill-aegis-16gb-2-x-8gb-ddr4-3000-memory-f43000c16d16gisb)) | $94.99 @ Newegg 

\*\*Storage\*\* | \[Samsung - 960 EVO 500 GB M.2-2280 Solid State Drive\]([https://pcpartpicker.com/product/Ykbkcf/samsung-960-evo-500gb-m2-2280-solid-state-drive-mz-v6e500](https://pcpartpicker.com/product/Ykbkcf/samsung-960-evo-500gb-m2-2280-solid-state-drive-mz-v6e500)) | $139.89 @ OutletPC 

\*\*Case\*\* | \[Cooler Master - MasterBox Pro 5 RGB ATX Mid Tower Case\]([https://pcpartpicker.com/product/hWnG3C/cooler-master-masterbox-pro-5-rgb-atx-mid-tower-case-mcy-b5p2-kwgn-01](https://pcpartpicker.com/product/hWnG3C/cooler-master-masterbox-pro-5-rgb-atx-mid-tower-case-mcy-b5p2-kwgn-01)) | $69.99 @ B&amp;H 

\*\*Power Supply\*\* | \[EVGA - SuperNOVA G2 650 W 80+ Gold Certified Fully-Modular ATX Power Supply\]([https://pcpartpicker.com/product/9q4NnQ/evga-power-supply-220g20650y1](https://pcpartpicker.com/product/9q4NnQ/evga-power-supply-220g20650y1)) | $89.99 @ Amazon 

 | \*Prices include shipping, taxes, rebates, and discounts\* |

 | Total (before mail-in rebates) | $1127.82

 | Mail-in rebates | -$50.00

 | \*\*Total\*\* | \*\*$1077.82\*\*

 

I am trying to build a Deep Learning enabled PC currently for \~1500 USD budget but with enough room for upgrades (with a RTX 20 series GPU later and more storage and RAM, if required). 

# Use case

My main current goal is to set up this barebone PC and get started with ML projects in my spare time. I enjoy reading a lot of articles and want to try out various ML algorithms, participate in Kaggle competitions, build my own mini-ML projects. I use python and my current DL framework of choice is PyTorch. Avid [fast.ai](https://fast.ai) fan and aspire to complete their recent courses on this machine.

For certain projects where a GPU might significantly be helpful, I have GTX 4 GB M series GPU on my other machine which I can use for quick results sparingly. Other option, for now, is using Azure based solution, if I really need it. My main reference article for part choice inspiration: [http://timdettmers.com/2018/12/16/deep-learning-hardware-guide/](http://timdettmers.com/2018/12/16/deep-learning-hardware-guide/)

I do like to play PC games and have been playing third person open world games like Batman (all parts), Witcher 3 etc. I mostly play them on my other machine with manageable graphics. With a GPU card added, this PC will be quite a powerful machine and will be able to handle recent games as well. I don't care about 1440p gaming. Mentioning this here because this might influence some part choice preferences, like an overclock enabled CPU and supporting motherboard, dual GPU setups with one GPU for ML while other for gaming/ML.

&amp;#x200B;

I need help in knowing if my choices are validated against my use case scenario or if I need to tweak my current build.

# CPU

&amp;#x200B;

This is the most confusing part of the build for me right now.

**Why not AMD?** 

* The main reason for not using Ryzen 2700X is the lack of onboard graphics. Since initially, I won't be getting a GPU, this is not an option for me.
* Another main reason is Intel processors are better suited for numpy library with Ryzen not performing well (at least till previous year, as mentioned in a [post](https://www.reddit.com/r/Amd/comments/80uwp0/amd_ryzen_openblas_and_numpy/)). 
* Intel processor is rated at 95W TDP which is the same for i9 models. Threadripper some models have \~165W TDP which is too much, if I want to keep the machine running for a few days, at least.

I find that i5-9600k is a lot cheaper than i7 model, but have lesser number of cores. Is that a major problem for ML algorithms?

&amp;#x200B;

**Why not older generation CPU?**

* Because the recent 9th generation chips have hardware fixes for Spectre/Meltdown: [https://www.digitaltrends.com/computing/intel-9-series-cpu-spectre/](https://www.digitaltrends.com/computing/intel-9-series-cpu-spectre/)
* Also, apart from lower prices, I don't know (yet) any strong case for buying an older generation chip. Any insights are welcomed.

# Motherboard

&amp;#x200B;

The main concern is that the motherboard should have enough room for two GPUs for later upgrades. If opting for RTX 20 series models, they take up two expansion slots. So the PCIe slots should have placement such that both GPU can fit together with enough breathing room between them.

Reason for choosing Z390 AORUS PRO WIFI is 12 phase VRM. I am not an expert (building PC for the first time) but with the review videos, I saw that it's a good thing to have for Intel K-series processors if later on, I want to overclock. Disabling of SATA ports due to addition M.2 is not a problem for me.

&amp;#x200B;

==============================================

&amp;#x200B;

With the current build, I am fine with just a 512 GB M.2 SSD which will get me started for most small learning projects and tutorials. Again, having a motherboard with enough room for later upgrades is most crucial for my use case.

&amp;#x200B;

I also don't know much about RAM. I literally picked the first best looking choice for RAM (with decent speed), Cabinet (keeping in mind that it was mid-tower) and PSU (fully modular, Gold rated).

I did read that PSU should support enough PCIe points if interested in dual graphics. This I am not sure with my current PSU choice.

&amp;#x200B;

As is evident, my main concern is to get started ASAP with good components with enough breathing room for later upgrades. I plan to get an RTX 2070 later on.",21,1,False,self,,,,,
912,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,21,aqvxvt,self.MachineLearning,[N] Dive into Kaggle competitions with mlcourse.ai,https://www.reddit.com/r/MachineLearning/comments/aqvxvt/n_dive_into_kaggle_competitions_with_mlcourseai/,festline,1550232603,"&amp;#x200B;

https://i.redd.it/rx9bv0h01qg21.jpg

  
[mlcourse.ai](https://mlcourse.ai/) is an open and free ML course led by OpenDataScience, or [ods.ai](https://ods.ai/), community known firstly for its top Kagglers. The course launches twice a year, and I've already described the course last time: [post1](https://www.reddit.com/r/MachineLearning/comments/9elgl1/n_mlcourseai_open_machine_learning_course_by/), [post2](https://www.reddit.com/r/MachineLearning/comments/9nner0/n_join_mlcourseai_open_and_free_machine_learning/). A new session has launched on February 11th, and this time it's a bit different: it focuses on **Kaggle Inclass competitions**. There'll be 4 of them during the 10-week course. 

[mlcourse.ai](https://mlcourse.ai) is a MOOC, some 9k participants have already taken part. This time we decided to address Kaggle: got consulted by Kaggle grandmasters, shared course content as a [Kaggle Dataset](https://www.kaggle.com/kashnitsky/mlcourse) with [Kernels](https://www.kaggle.com/kashnitsky/mlcourse/kernels?sortBy=voteCount&amp;group=everyone&amp;pageSize=20&amp;datasetId=32132). The activity [is supported](https://twitter.com/antgoldbloom/status/1094652527738417152) by Kaggle's CEO Anthony Goldbloom. 

If you ever wanted to step onto a long Kaggle path, this course will be a perfect start. Visit [mlcourse.ai](https://mlcourse.ai) main page to join (you can do it at any point before the end of the course, April 26th). Two competitions are already there, and the first deadline is February 24th, high time to join.  

&amp;#x200B;

&amp;#x200B;",5,1,False,https://b.thumbs.redditmedia.com/YdBrOIEbLnF11M7DpBGCi_TrV7ZXfIVeqyIqNYPwLrE.jpg,,,,,
913,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,21,aqvzfd,self.MachineLearning,[P] A curated list of deep learning image classification papers and codes,https://www.reddit.com/r/MachineLearning/comments/aqvzfd/p_a_curated_list_of_deep_learning_image/,372995411,1550232943,"Recently, I just created a project that curated a list of deep learning image lcassification papers and codes, if anyone were intrested, here is the github repository:[https://github.com/weiaicunzai/awesome-image-classification](https://github.com/weiaicunzai/awesome-image-classification) , welcome to create a pull request.

&amp;#x200B;

I don't know if I am allowed to post my github repository here, If I am violating the rules, please let me know.",2,1,False,self,,,,,
914,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwcyx,self.MachineLearning,[Discussion] OpenAI should now change their name to ClosedAI,https://www.reddit.com/r/MachineLearning/comments/aqwcyx/discussion_openai_should_now_change_their_name_to/,SirLordDragon,1550235879,It's the only way to complete the hype wave.,238,1,False,self,,,,,
915,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwf7e,self.MachineLearning,A closer look at optimization landscape of deep over-parameterized neural networks reveals insights into why gradient descent succeeded,https://www.reddit.com/r/MachineLearning/comments/aqwf7e/a_closer_look_at_optimization_landscape_of_deep/,alibabass,1550236335,[removed],0,1,False,self,,,,,
916,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwk3u,self.MachineLearning,why my post now showing up,https://www.reddit.com/r/MachineLearning/comments/aqwk3u/why_my_post_now_showing_up/,alexchan123,1550237358,[removed],0,1,False,self,,,,,
917,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwlju,i.redd.it,"This is not a photoshop project, this is image generation using google deepdream artificial neural network which uses inception net, this basically was started as a project to understand how convolution neural network works.",https://www.reddit.com/r/MachineLearning/comments/aqwlju/this_is_not_a_photoshop_project_this_is_image/,FoCDoT,1550237651,,0,1,False,default,,,,,
918,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwn9w,self.MachineLearning,A closer look at optimization landscape of deep over-parameterized neural networks reveals insights into why gradient descent succeeded,https://www.reddit.com/r/MachineLearning/comments/aqwn9w/a_closer_look_at_optimization_landscape_of_deep/,alibabass,1550237995,"[https://arxiv.org/abs/1901.07417](https://arxiv.org/abs/1901.07417)

This work shows that the loss function of deep over-parameterized neural networks have connected sublevel sets, and thus the optimization landscape has very simple structure. There are a few immediate consequences:

1. there are bad local valleys that gradient descent may get stuck

2. there is a continuous descent path from any starting point to a global minimum

3. all global minima are path-connected ",0,1,False,self,,,,,
919,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwpcn,self.MachineLearning,[P] Deep Learning experiments in AWS,https://www.reddit.com/r/MachineLearning/comments/aqwpcn/p_deep_learning_experiments_in_aws/,alexkimxyz,1550238405,"If you thought about buying a PC for your deep learning experiments, this is something I think worth trying out. Any feedback will much appreciated.",0,1,False,self,,,,,
920,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwpkg,alexkimxyz.github.io,[P] Deep Learning experiments in AWS,https://www.reddit.com/r/MachineLearning/comments/aqwpkg/p_deep_learning_experiments_in_aws/,alexkimxyz,1550238451,,1,1,False,default,,,,,
921,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwrcx,self.MachineLearning,ML Requirement in Recommendation System?,https://www.reddit.com/r/MachineLearning/comments/aqwrcx/ml_requirement_in_recommendation_system/,HecrouxIdiot,1550238806,[removed],0,1,False,self,,,,,
922,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwswo,self.MachineLearning,[R] New insights on the simple structure of the loss function reveals why gradient descent can optimize overparameterized neural networks,https://www.reddit.com/r/MachineLearning/comments/aqwswo/r_new_insights_on_the_simple_structure_of_the/,alibabass,1550239119,"[https://arxiv.org/abs/1901.07417](https://arxiv.org/abs/1901.07417)

This work shows that the loss function of deep over-parameterized neural networks have connected sublevel sets, and so the optimization landscape has very simple structure. There are several immediate consequences:

1. the loss surface has no bad local valleys where gradient descent might get stuck
2. there is a continuous descent path from any starting point to a global minimum
3. all global minima are path-connected",9,1,False,self,,,,,
923,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,22,aqwt25,self.MachineLearning,What are some good references to learn machine learning with Julia language?,https://www.reddit.com/r/MachineLearning/comments/aqwt25/what_are_some_good_references_to_learn_machine/,SuccessfulLeadership,1550239149,[removed],0,1,False,self,,,,,
924,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,23,aqwuiu,self.MachineLearning,ML Requirement in Recommendation System,https://www.reddit.com/r/MachineLearning/comments/aqwuiu/ml_requirement_in_recommendation_system/,rayrivessinada,1550239413,[removed],0,1,False,self,,,,,
925,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,23,aqwzq1,self.MachineLearning,[Discussion] Whats the largest number of layers youve ever put into a neural network and why?,https://www.reddit.com/r/MachineLearning/comments/aqwzq1/discussion_whats_the_largest_number_of_layers/,LukeArrigoni,1550240360,"I saw the shit post about the 20k layers and it made me laugh but also made me wonder how deep your models actually are. 

In true statistician form, if you wouldnt mind sharing your mean layers for a type of network and the max it would be awesome to know where my production models fall.",11,1,False,self,,,,,
926,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,23,aqx0hm,self.MachineLearning,"[P] RobustSTL: Time Series Decomposition into Trend, Seasonality, and Remainder",https://www.reddit.com/r/MachineLearning/comments/aqx0hm/p_robuststl_time_series_decomposition_into_trend/,doyuplee,1550240494,"There was time series decomposition study In AAAI 2019

Seasonality and Trend extraction is important to forecast or detect anomalies in time series sample!

  
Title: RobustSTL: A Robust Seasonal-Trend Decomposition Algorithm for Long Time Series

\- Arxiv: [https://arxiv.org/abs/1812.01767](https://arxiv.org/abs/1812.01767) 

The codes are not available, but I implemented it !

\- Implementation: [https://github.com/LeeDoYup/RobustSTL](https://github.com/LeeDoYup/RobustSTL) 

\- Results on Synthetic Sample

[Original sample \(left\) &amp; Trend, Seasonality, and Remainder decomposition \(right\)](https://i.redd.it/gyyld284rqg21.png)

&amp;#x200B;",1,1,False,self,,,,,
927,MachineLearning,t5_2r3gv,2019-2-15,2019,2,15,23,aqxcu6,self.MachineLearning,How much does it worth to retrain model on previously predicted data?,https://www.reddit.com/r/MachineLearning/comments/aqxcu6/how_much_does_it_worth_to_retrain_model_on/,AnkBurov,1550242698,[removed],0,1,False,self,,,,,
928,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,0,aqxi84,self.MachineLearning,Intro to Reinforcement learning with small project,https://www.reddit.com/r/MachineLearning/comments/aqxi84/intro_to_reinforcement_learning_with_small_project/,loser-two-point-o,1550243589,[removed],0,1,False,self,,,,,
929,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,0,aqxk9h,/r/MachineLearning/comments/aqxk9h/i_found_this_amazing_course_of_aritificial/,I found this amazing course of aritificial intelligence for Product mangers. What do you guys think is it helpful? This site have more content hope you like it: http://bit.ly/AI_fr_PM,https://www.reddit.com/r/MachineLearning/comments/aqxk9h/i_found_this_amazing_course_of_aritificial/,Simrit-gupta,1550243922,,0,1,False,default,,,,,
930,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,0,aqxlpb,self.MachineLearning,"[R] Modern Neural Networks (with conv, (self-)attention, batchnorm, LSTM, etc) become Gaussian Processes when randomly initialized, as their widths grow to infinity.",https://www.reddit.com/r/MachineLearning/comments/aqxlpb/r_modern_neural_networks_with_conv_selfattention/,jinpanZe,1550244166,"Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation

[https://arxiv.org/abs/1902.04760](https://arxiv.org/abs/1902.04760)

&amp;#x200B;

This is proved using a new ""tensor program"" framework. Neural network computations are expressed in tensor programs, and a general scaling limit result is proved for all tensor programs.

&amp;#x200B;

An intriguing possibility here is to build a compiler that compiles Tensorflow graphs or pytorch tapes to ""tensor programs"" and then convert any neural network to its limiting GP automatically.

&amp;#x200B;

More discussions [here](https://twitter.com/TheGregYang/status/1096141395297890304)

[tensor program specification](https://i.redd.it/mzojo54vzqg21.png)",3,1,False,self,,,,,
931,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,0,aqxrlu,self.MachineLearning,[Question] Is there any material or resources on tracking specific object with image recognition?,https://www.reddit.com/r/MachineLearning/comments/aqxrlu/question_is_there_any_material_or_resources_on/,b42thomas,1550245138,[removed],0,1,False,self,,,,,
932,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,0,aqxsdd,self.MachineLearning,Has anyone done a study on capacity of various NN architectures?,https://www.reddit.com/r/MachineLearning/comments/aqxsdd/has_anyone_done_a_study_on_capacity_of_various_nn/,achaiah777,1550245262,"Now that we've got a number of different NN architectures (e.g. Resnet, Inception, Densenet, Nasnet etc) have there been any studies as to how well they perform when you keep increasing the number of distinct classes? So instead of 1000 classes from imagenet, what if we train them with 10K classes? 100K? Which networks perform better as you keep adding classes and do they have a breaking point beyond which they stop meaningfully distinguish between classes or is it just a gradual reduction in accuracy?

I recall seeing a Resnet trained on 10K classes in one paper but other than that I haven't it being researched much in literature.",0,1,False,self,,,,,
933,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,0,aqxurc,github.com,[R] disentanglement_lib is an open-source library for research on learning disentangled representations,https://www.reddit.com/r/MachineLearning/comments/aqxurc/r_disentanglement_lib_is_an_opensource_library/,ykilcher,1550245648,,0,1,False,default,,,,,
934,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqy14b,self.MachineLearning,What is a low-rank assumption?,https://www.reddit.com/r/MachineLearning/comments/aqy14b/what_is_a_lowrank_assumption/,justAHairyMeatBag,1550246647,[removed],0,1,False,self,,,,,
935,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqy4hq,self.MachineLearning,[R] Do ImageNet Classifiers Generalize to ImageNet?,https://www.reddit.com/r/MachineLearning/comments/aqy4hq/r_do_imagenet_classifiers_generalize_to_imagenet/,jinpanZe,1550247167,"[http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf](http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf)

&amp;#x200B;

Ben Recht et al. followed up on their CIFAR10 work by reproducing the Imagenet test set and evaluating Imagenet models on it. An accuracy drop of 11-14% is observed.

https://i.redd.it/517jvasuarg21.png

&amp;#x200B;",6,1,False,self,,,,,
936,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqy5ar,creospiders.com,How Machine Learning Works ? - Data Rules the Kingdom,https://www.reddit.com/r/MachineLearning/comments/aqy5ar/how_machine_learning_works_data_rules_the_kingdom/,Pranav2719,1550247295,,0,1,False,default,,,,,
937,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqyabc,self.MachineLearning,This tutorial is for people who want to start learning python or have confusion regarding Python. This tutorial has a series of videos explaining python and its implementation.,https://www.reddit.com/r/MachineLearning/comments/aqyabc/this_tutorial_is_for_people_who_want_to_start/,yashica_,1550248075,[removed],0,1,False,self,,,,,
938,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqyitf,creospiders.com,How Machine Learning Works - Mathematics every where,https://www.reddit.com/r/MachineLearning/comments/aqyitf/how_machine_learning_works_mathematics_every_where/,Pranav2719,1550249428,,0,1,False,default,,,,,
939,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqyj4a,self.MachineLearning,This tutorial is for people who want to start learning python or have confusion regarding Python. This tutorial has a series of videos explaining python and its implementation.,https://www.reddit.com/r/MachineLearning/comments/aqyj4a/this_tutorial_is_for_people_who_want_to_start/,yashica_,1550249476,[removed],0,1,False,self,,,,,
940,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqyjjs,syncedreview.com,AI Hasn't Found Its Isaac Newton: Gary Marcus on Deep Learning Defects &amp; 'Frenemy' Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/aqyjjs/ai_hasnt_found_its_isaac_newton_gary_marcus_on/,Yuqing7,1550249538,,0,1,False,https://a.thumbs.redditmedia.com/ROFSablDcGrdQ8E1Wy1uum5aNka_AH0svMBfkvuohD4.jpg,,,,,
941,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqyk4u,dlrlsummerschool.ca,Deep Learning and Reinforcement Learning Summer School 2019,https://www.reddit.com/r/MachineLearning/comments/aqyk4u/deep_learning_and_reinforcement_learning_summer/,camlinke,1550249629,,1,1,False,default,,,,,
942,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,1,aqymc3,self.MachineLearning,I have a problem with matplotlib in Anaconda and Jupyter notebook.,https://www.reddit.com/r/MachineLearning/comments/aqymc3/i_have_a_problem_with_matplotlib_in_anaconda_and/,afloareirazvan,1550249967,[removed],0,1,False,https://b.thumbs.redditmedia.com/u_14_UklK2YclPNEhWlhSJQtTQuWsoGT2kyjgPAGmAQ.jpg,,,,,
943,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,2,aqynds,i.redd.it,Cool edits using deep learning,https://www.reddit.com/r/MachineLearning/comments/aqynds/cool_edits_using_deep_learning/,FoCDoT,1550250117,,2,1,False,default,,,,,
944,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,2,aqyoko,self.MachineLearning,What Machine Learning can do for your business and products.,https://www.reddit.com/r/MachineLearning/comments/aqyoko/what_machine_learning_can_do_for_your_business/,Sophia77Wright,1550250299,[removed],0,1,False,self,,,,,
945,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,2,aqyx3y,self.MachineLearning,Generated content copyrights/license,https://www.reddit.com/r/MachineLearning/comments/aqyx3y/generated_content_copyrightslicense/,epwik,1550251641,[removed],0,1,False,self,,,,,
946,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,2,aqz833,self.MachineLearning,[D] Generated content copyrights/license,https://www.reddit.com/r/MachineLearning/comments/aqz833/d_generated_content_copyrightslicense/,epwik,1550253375,"Can someone explain my rights to use generated material, for example, if using styleGAN which have Attribution-NonCommercial 4.0 International license (link to it [https://github.com/NVlabs/stylegan/blob/master/LICENSE.txt](https://github.com/NVlabs/stylegan/blob/master/LICENSE.txt)), and for both content and style I use my own images, can I use the results as it would be my own images, or they fall under the same license as the code? ",6,1,False,self,,,,,
947,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,3,aqzbof,self.MachineLearning,Would it be feasible to train a neural net to identify untagged spoilers on reddit?,https://www.reddit.com/r/MachineLearning/comments/aqzbof/would_it_be_feasible_to_train_a_neural_net_to/,ShamelessC,1550253919,[removed],0,1,False,spoiler,,,,,
948,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,3,aqzf23,ai.googleblog.com,Introducing PlaNet: A Deep Planning Network for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/aqzf23/introducing_planet_a_deep_planning_network_for/,sjoerdapp,1550254445,,0,1,False,default,,,,,
949,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,3,aqzjv1,self.MachineLearning,[D] Open Alternative Reddit Scraper Inspired by OpenAI's New Language Dataset,https://www.reddit.com/r/MachineLearning/comments/aqzjv1/d_open_alternative_reddit_scraper_inspired_by/,joshuacpeterson,1550255180,"With the intention that this post not overlap with others debating OpenAI's release policy, is anyone interested in collaborating on developing such an open alternative? The resulting datasets could be released on Academic Torrents.",27,1,False,self,,,,,
950,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,3,aqzl1b,reddit.com,"[R] PlaNet: data-efficient, model-based RL",https://www.reddit.com/r/MachineLearning/comments/aqzl1b/r_planet_dataefficient_modelbased_rl/,valdanylchuk,1550255365,,0,1,False,default,,,,,
951,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,3,aqzll1,self.MachineLearning,[R] PlaNet: A Deep Planning Network for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/aqzll1/r_planet_a_deep_planning_network_for/,jinpanZe,1550255446,"https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html?m=1

The authors present theDeep Planning Network (PlaNet)agent, which learns a world model from image inputs only and successfully leverages it for planning.",18,1,False,self,,,,,
952,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,3,aqzvg7,theguardian.com,"New AI fake text generator may be too dangerous to release, say creators",https://www.reddit.com/r/MachineLearning/comments/aqzvg7/new_ai_fake_text_generator_may_be_too_dangerous/,Captainmanic,1550256966,,0,1,False,https://b.thumbs.redditmedia.com/q3usbyxNdI8Vl2OEBtD7jFc1Pvz-qNlSMFR_yAEThFE.jpg,,,,,
953,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,3,aqzvno,self.MachineLearning,is cosine similarity and dot product referring to the same thing?,https://www.reddit.com/r/MachineLearning/comments/aqzvno/is_cosine_similarity_and_dot_product_referring_to/,marksteve4,1550256996,[removed],0,1,False,self,,,,,
954,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,4,ar092t,self.MachineLearning,Best Practices for Dataset Construction,https://www.reddit.com/r/MachineLearning/comments/ar092t/best_practices_for_dataset_construction/,rajivpoc,1550258996,[removed],0,1,False,self,,,,,
955,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,5,ar0req,self.MachineLearning,[D] Instability on toy problems,https://www.reddit.com/r/MachineLearning/comments/ar0req/d_instability_on_toy_problems/,csbotos,1550261851,"I tried producing a 16x16 black image with a random 4x4 square appearing on it to test the coverage of the randomness of different GANs and a basic VAE model.

Surprisingly, all the PyTorch example code have failed:

[https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)  
[https://github.com/pytorch/examples/tree/master/dcgan](https://github.com/pytorch/examples/tree/master/dcgan)

[https://github.com/pytorch/examples/tree/master/vae](https://github.com/pytorch/examples/tree/master/vae)

&amp;#x200B;

I have also tried more nuanced versions of GAN training methods, like WGAN, Spectral Normalization, TTUR without success. Needless to say, I tried to reduce the number of parameters, increase/decrease the batch size (which helped a bit for the VAE) but this is ridiculous.

I started to wonder, how people are making HQ images and argue which model is the best when tinkering around with their baselines are only working for that specific dataset? Am I missing something? ",0,1,False,self,,,,,
956,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,5,ar0vvl,self.MachineLearning,[D] Instability of image generation models on a very simple problem.,https://www.reddit.com/r/MachineLearning/comments/ar0vvl/d_instability_of_image_generation_models_on_a/,csbotos,1550262569,"* Objective: I synthesized a dataset of 16x16 black images with a single random 4x4 white square appearing on each of them.
* Reason: test the coverage of the uniform distribution of the appearances using different GANs and a basic VAE model
* Outcome: surprisingly, all the PyTorch example code have failed:

[https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)[https://github.com/pytorch/examples/tree/master/dcgan](https://github.com/pytorch/examples/tree/master/dcgan)

[https://github.com/pytorch/examples/tree/master/vae](https://github.com/pytorch/examples/tree/master/vae)

&amp;#x200B;

I have also tried more nuanced versions of GAN training methods, like WGAN, Spectral Normalization, TTUR without success. I trained them for 50k iterations, I tried to reduce the number of parameters, increase/decrease the batch size (which helped a bit for the VAE) but this is ridiculous.

I started to wonder, how people are making HQ images and argue which model is the best when tinkering around with their baselines are only working for that specific dataset (e.g. celebA-HQ, ImageNet, LSUN...)? 

Am I missing something?",3,1,False,self,,,,,
957,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,5,ar0w6h,creospiders.com,Machine Learning - Types of Learning - Explained,https://www.reddit.com/r/MachineLearning/comments/ar0w6h/machine_learning_types_of_learning_explained/,Pranav2719,1550262616,,0,1,False,default,,,,,
958,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,5,ar16mj,towardsdatascience.com,Audio AI: isolating vocals from stereo music using Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/ar16mj/audio_ai_isolating_vocals_from_stereo_music_using/,roy-m-kim,1550264293,,0,1,False,default,,,,,
959,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,6,ar19bl,self.MachineLearning,[R] BagNet  Solving ImageNet with a Simple Bag-of-features Model,https://www.reddit.com/r/MachineLearning/comments/ar19bl/r_bagnet_solving_imagenet_with_a_simple/,tldrtldreverything,1550264698,"Hey, I published a summary of the new BagNet paper, which shows that you can get surprisingly good results with feature-based models that don't require deep learning. In its strongest mode, their model achieved 87.6% in Top-5 Validation Performance on ImageNet, close to the VGG-16 result and way ahead of the legendary 2012 AlexNet model. Happy to get your thoughts. Full summary here: https://www.lyrn.ai/2019/01/29/identifying-and-correcting-label-bias-in-machine-learning/",13,1,False,self,,,,,
960,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,6,ar1max,self.MachineLearning,[Discussion] How much non-ml prepping does a researcher need for amazon applied ml scientist interviews?,https://www.reddit.com/r/MachineLearning/comments/ar1max/discussion_how_much_nonml_prepping_does_a/,kardiapal,1550266761,"I failed getting into my dream labs for ML PhD, so currently plan to spend a bit of time in industry. I am fairly confident in ML, math and statistics side of things with 2  ICML papers and an overall stellar academic background, however I am worried about non-ml related questions and generally things that are not necessary for day-to-day ML research. If you've gone through this what would you recommend?

Any help appreciated, I haven't done an industry interview since freshman year ...",13,1,False,self,,,,,
961,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,6,ar1si7,self.MachineLearning,early authors or seminal papers regarding the development of AI,https://www.reddit.com/r/MachineLearning/comments/ar1si7/early_authors_or_seminal_papers_regarding_the/,boskle,1550267775,[removed],0,1,False,self,,,,,
962,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,7,ar1u95,youtube.com,"For any of you guys interested in reinforcement learning and data analysis, this project may be interesting to you. Sparlab is a desktop tool that helps fighting gamers train more effectively than what's currently offered in games. Any thoughts? Ideas?",https://www.reddit.com/r/MachineLearning/comments/ar1u95/for_any_of_you_guys_interested_in_reinforcement/,realjohnward,1550268056,,1,1,False,default,,,,,
963,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,7,ar1ukr,github.com,Ever wondered how to use your trained sklearn/xgboost/lightgbm models in production? We developed a simple library which turns your models into native code (Python/C/Java),https://www.reddit.com/r/MachineLearning/comments/ar1ukr/ever_wondered_how_to_use_your_trained/,krinart,1550268104,,0,1,False,default,,,,,
964,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,7,ar2egw,self.MachineLearning,[D] what do you think about the applications of machine learning in discoveries?,https://www.reddit.com/r/MachineLearning/comments/ar2egw/d_what_do_you_think_about_the_applications_of/,tdls_to,1550271472,"I saw this article recently which is very interesting essentially claiming that we need to include more machune learning in scientific discovery. what do people think about it?

https://www.technologyreview.com/s/612898/ai-is-reinventing-the-way-we-invent/",7,1,False,self,,,,,
965,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,8,ar2vni,self.MachineLearning,"[D] Machine Learning, Deep Learning, and Reinforcement Learning for Vision, Robotics, and NLP",https://www.reddit.com/r/MachineLearning/comments/ar2vni/d_machine_learning_deep_learning_and/,kmario23,1550274480,"Dear all,
  For those who are enthusiastic in developing an in-depth understanding of deep learning, machine learning, reinforcement learning, DL for computer vision/NLP, optimization etc.,  I have put together an almost comprehensive list of freely available online lectures.

It can be found in my repo: [Deep-Learning-Drizzle](https://github.com/kmario23/deep-learning-drizzle)

Please feel free to fork, star, and share it with anyone who might be interested in this!

Happy Weekend!",1,1,False,self,,,,,
966,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,9,ar38ti,self.MachineLearning,[D] What are your research habits?,https://www.reddit.com/r/MachineLearning/comments/ar38ti/d_what_are_your_research_habits/,c0cky_,1550276822,"I've noticed that there are vast amounts of more papers being submitted on many different interesting things in the field. I'm really curious how other researchers are dealing with it and how often they read all these new papers.

&amp;#x200B;

I'm putting together a REALLY quick survey that looks at the habits of researchers, I'm then going to release the results of that survey for everyone to look at.

&amp;#x200B;

If you're interested check it out here: [https://bit.ly/2V1tDlg](https://bit.ly/2V1tDlg)",0,1,False,self,,,,,
967,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,9,ar3b9n,i.redd.it,"Here is my pytorch implementation of the model described in the paper DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs (https://arxiv.org/pdf/1606.00915.pdf) Source code: https://github.com/vietnguyen91/Deeplab-pytorch",https://www.reddit.com/r/MachineLearning/comments/ar3b9n/here_is_my_pytorch_implementation_of_the_model/,1991viet,1550277279,,1,1,False,default,,,,,
968,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,9,ar3j7k,self.MachineLearning,Curiosity Learning in games like Minecraft,https://www.reddit.com/r/MachineLearning/comments/ar3j7k/curiosity_learning_in_games_like_minecraft/,yrrah1,1550278797,[removed],0,1,False,self,,,,,
969,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,10,ar3pm2,self.MachineLearning,How do we avoid SOTA-hacking?,https://www.reddit.com/r/MachineLearning/comments/ar3pm2/how_do_we_avoid_sotahacking/,my_gpu_is_bigger,1550280006,[removed],0,1,False,self,,,,,
970,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,10,ar3tc2,self.MachineLearning,[D] How do we avoid SOTA-hacking?,https://www.reddit.com/r/MachineLearning/comments/ar3tc2/d_how_do_we_avoid_sotahacking/,my_gpu_is_bigger,1550280731,"The OpenAI blog post reports that they achieve ""state-of-the-art performance on many language modeling benchmarks"", but they compare a model pretrained on additional data to models trained only on the provided dataset. Reporting this as beating the SOTA on these datasets doesn't make sense. As mentioned on Twitter, the results they state as the existing SOTA can be beaten trivially:  
[https://twitter.com/BFelbo/status/1096310277312634882](https://twitter.com/BFelbo/status/1096310277312634882)  
[https://twitter.com/seb\_ruder/status/1096335334969933829](https://twitter.com/seb_ruder/status/1096335334969933829)

&amp;#x200B;

Claiming SOTA results in this way makes it harder to publish papers focused on sample-efficiency and to compare results across papers. The OpenAI blog post / paper is just the latest of many papers claiming SOTA results with a comparison that doesn't make sense. How do we as a community avoid this kind of SOTA-hacking?",22,73,False,self,,,,,
971,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,13,ar5bgq,self.MachineLearning,[D] using AI to verify the reproducability of social sciences,https://www.reddit.com/r/MachineLearning/comments/ar5bgq/d_using_ai_to_verify_the_reproducability_of/,tdls_to,1550291586,"I feel like this should be done first on the reproducability of ml algos used to check the reproducability of the social science results
https://www.wired.com/story/darpa-wants-to-solve-sciences-replication-crisis-with-robots/",0,5,False,self,,,,,
972,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,14,ar5j0x,blog.gigaspaces.com,Operationalizing your learning models is key to uncovering those insights at Super Speed,https://www.reddit.com/r/MachineLearning/comments/ar5j0x/operationalizing_your_learning_models_is_key_to/,AliG68,1550293231,,0,1,False,default,,,,,
973,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,14,ar5l77,arxiv.org,New Multi-Task Deep Neural Network for Natural Language Understanding published by Microsoft,https://www.reddit.com/r/MachineLearning/comments/ar5l77/new_multitask_deep_neural_network_for_natural/,MonstarGaming,1550293700,,9,51,False,default,,,,,
974,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,14,ar5p4r,steemit.com,One month of free access to Google Cloud Specializations,https://www.reddit.com/r/MachineLearning/comments/ar5p4r/one_month_of_free_access_to_google_cloud/,poonddetatte,1550294573,,0,1,False,default,,,,,
975,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,15,ar62xx,self.MachineLearning,What is the point of using RNN in DQN?,https://www.reddit.com/r/MachineLearning/comments/ar62xx/what_is_the_point_of_using_rnn_in_dqn/,Username201999,1550297658,[removed],0,1,False,self,,,,,
976,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,15,ar6d7z,georgedatascience.com,Read how Amazon India Using Artificial Intelligence and Machine Learning to Develop Their Business,https://www.reddit.com/r/MachineLearning/comments/ar6d7z/read_how_amazon_india_using_artificial/,georgedatascience,1550300173,,0,1,False,https://a.thumbs.redditmedia.com/8VCTHQkv8ay_aCzNB2yTAvgd8aimz7g5wtpdl-IQOc4.jpg,,,,,
977,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,17,ar6ucu,self.MachineLearning,Best Machine Learning Certification Courses in Kolkata and Bangalore,https://www.reddit.com/r/MachineLearning/comments/ar6ucu/best_machine_learning_certification_courses_in/,praxisedu,1550304750,"Praxis offers the [**best machine learning certification courses in Kolkata and Bangalore**](http://praxis.ac.in/machine-learning-certification-courses-in-kolkata-and-bangalore/). Praxis is the best institute for machine learning courses in India. Machine learning has now become one of the fastest growing and lucrative careers in the new millennium. Because of constantly evolving computing technologies, machine learning is not a thing of the past. With large and complex data being generated every moment, it has become imperative for the companies to analyze the data and compute them to find some insight that can prove to be profitable for them. Praxis is the best institute for machine learning in India with the best team of expert faculties. Praxis provides certification courses in machine learning at Bangalore and Kolkata campus in India.

![img](iucxtzbb1wg21)",0,1,False,self,,,,,
978,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,18,ar79qu,self.MachineLearning,Architectures for LIDAR Data,https://www.reddit.com/r/MachineLearning/comments/ar79qu/architectures_for_lidar_data/,rondodude2000,1550309186,[removed],0,1,False,self,,,,,
979,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,18,ar7gem,medium.com,Mathematics for machine learning from Imperial college London,https://www.reddit.com/r/MachineLearning/comments/ar7gem/mathematics_for_machine_learning_from_imperial/,frenchdic,1550311148,,0,1,False,default,,,,,
980,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,19,ar7lhn,self.MachineLearning,How to make an object detector that runs in unity?,https://www.reddit.com/r/MachineLearning/comments/ar7lhn/how_to_make_an_object_detector_that_runs_in_unity/,Amanpradhan1,1550312523,[removed],0,1,False,self,,,,,
981,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,19,ar7sz6,self.MachineLearning,Mxnet Vs rest of the world :\,https://www.reddit.com/r/MachineLearning/comments/ar7sz6/mxnet_vs_rest_of_the_world/,mouryarishik,1550314607,[removed],0,1,False,self,,,,,
982,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,20,ar7zlx,self.MachineLearning,Concepts for a balanced society with machines,https://www.reddit.com/r/MachineLearning/comments/ar7zlx/concepts_for_a_balanced_society_with_machines/,best1400au,1550316405,[removed],0,1,False,self,,,,,
983,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,20,ar86sz,self.MachineLearning,Easily setup jupyter for deep learning at up to 90% off !!,https://www.reddit.com/r/MachineLearning/comments/ar86sz/easily_setup_jupyter_for_deep_learning_at_up_to/,sdhnshu,1550318385,[removed],0,1,False,self,,,,,
984,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,21,ar8dei,medium.com,Summary: World Models  Arxiv Bytes  Medium,https://www.reddit.com/r/MachineLearning/comments/ar8dei/summary_world_models_arxiv_bytes_medium/,CartPole,1550320018,,0,1,False,https://b.thumbs.redditmedia.com/MbEMBCliawONqlhALPeAsZgvOMVmMLoq3JC3X2qy3os.jpg,,,,,
985,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,22,ar8rjz,self.MachineLearning,so I learned that there is no software that can ANALYZE and INTERPRET ECG or ElectroCardioGraphy? is machine learning not capable such an exact thing that human eye can!?,https://www.reddit.com/r/MachineLearning/comments/ar8rjz/so_i_learned_that_there_is_no_software_that_can/,martin80k,1550323626,[removed],0,1,False,self,,,,,
986,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,22,ar8uzy,self.MachineLearning,[P] Step by step guide to Tensorflow,https://www.reddit.com/r/MachineLearning/comments/ar8uzy/p_step_by_step_guide_to_tensorflow/,jaleyhd,1550324437,"Glad to share ""***Tensorflow Hands-on Tutorial***"".  I have also included ""*Building Neural Networks from scratch*"" along with the theory to make it more comprehensive. Will also be updating the course with  Tensorflow 2.0 with eager execution. 

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

**Course Link :** [https://www.edyoda.com/course/1429](https://www.edyoda.com/course/1429). 

&amp;#x200B;

https://i.redd.it/ndmi0qnuoxg21.jpg

Sharing the TOC of the course

&amp;#x200B;

1. **Tensor flow Fundamentals :**
   1.  Graphs and Session
   2. Operations and Tensors
   3. Placeholders and Constants
   4. Matrix Multiplication in Tensorflow.
   5. Executing Tensors
   6. Variables in Tensorflow
2. **Understanding Gradients**
   1. Comprehensive understanding of Gradients
   2. Finding the Gradients in Tensorflow
   3. Understanding math behind Line fitting
   4. Coding Linear Regression in Tensorflow
   5. Understanding Gradient Descent Algorithm
3. **Visualizing Model and Realtime Plotting**
   1. Tensorboard Visualization
   2. Variable Scope : Making Tensorboard visualization better
   3. Plotting real-time loss in Tensorflow pt-1
   4. Merging summary in Tensorflow
   5. Hyper-parameter Tuning: Plotting loss curve for different learning rate
4. **Understanding Neural Networks and general Training paradigm**
   1. Training a model : A gentle Introduction
   2. Simplified explanation of Cross Entropy Loss
   3. Getting started with Neural Networks
   4. Neural Network with 2 layers from scratch
   5. Matrix view of Multilayer Perceptron
5. **Creating Neural Networks from scratch via Tensorflow**
   1. Understanding different Optimizers and Loss in  Tensorflow 
   2. Creating the Tensorflow model
   3. Building Neural Networks from scratch : Course Finale

&amp;#x200B;

I am open to suggestions. Let me know any other Deep Learning topic that you find difficult to understand and want me to work on. 

&amp;#x200B;

Cheers,

&amp;#x200B;",22,117,False,https://b.thumbs.redditmedia.com/cO24T5mEpBapli4-W7t1ZScIADhOxFcKzmNIoZlqquw.jpg,,,,,
987,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,22,ar8zxx,self.MachineLearning,[P] Multi-class classification in pure Python.,https://www.reddit.com/r/MachineLearning/comments/ar8zxx/p_multiclass_classification_in_pure_python/,rezaofdegreesix,1550325544,"In this post (drafted in Jupyter notebooks and prettied up for the web) I go through mathematical derivation + Python implementation of OvA multi-class classification. Here's the link: [https://jermwatt.github.io/machine\_learning\_refined/notes/7\_Linear\_multiclass\_classification/7\_2\_OvA.html](https://jermwatt.github.io/machine_learning_refined/notes/7_Linear_multiclass_classification/7_2_OvA.html)  

Pull the repo, take a peek under the hood, tweak the models, and have fun!  

https://i.redd.it/yn1v5y5gnxg21.png",9,27,False,self,,,,,
988,MachineLearning,t5_2r3gv,2019-2-16,2019,2,16,23,ar9h1b,self.MachineLearning,[P] Automatic image enhance first attempt,https://www.reddit.com/r/MachineLearning/comments/ar9h1b/p_automatic_image_enhance_first_attempt/,funkmasterplex,1550329050,"Here's some outputs from the test set:

https://i.imgur.com/Go66srR.png

The network structure is a GAN, with a unet style autoencoder as the generator, and a normal discriminator based on DCGAN.

For my training set, I took a few thousand images from CelebA, and removed the ones that were faded/over saturated/black and white/bad skin tones (too green or red usually).

For the test set, I picked some of those images which were rejected from the training set.

During training, the input to the autoencoder is a mangled version of the training data, realtime augmentation with random brightness shifts, contrast shifts, separate colour channel shifts, some converted to grayscale, some HSV channel shifts, some gaussian noise, some blurring, some random boxes drawn over the images, some sobel edge detection mixed in.  The output to the autoencoder is the original image.

I used Adam optimisers with low momentum, inverted smooth labels to the discriminator (0=real, 0.9=fake).  For the loss of the joined model, I used mae for the reconstruction of the image, and binary cross entropy on the discriminator output, weighted at 0.98/0.02 respectively.  The disciminator is just Conv/LeakyReLU layers, the generator is the same but with instance normalisation layers and skip connections.

Training was about half a day on an NVIDIA 960.  I am pretty pleased with these first results.",20,92,False,self,,,,,
989,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,0,ar9p7z,self.MachineLearning,What thesis topic is the best for 2019?,https://www.reddit.com/r/MachineLearning/comments/ar9p7z/what_thesis_topic_is_the_best_for_2019/,oscar1518,1550330609,[removed],0,1,False,self,,,,,
990,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,0,ar9sn4,github.com,[P] Keras model filter pruning package - prune convolutional filters,https://www.reddit.com/r/MachineLearning/comments/ar9sn4/p_keras_model_filter_pruning_package_prune/,gabegabe6,1550331274,,0,1,False,default,,,,,
991,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,0,ar9su4,self.MachineLearning,Real-time object detection in the cloud,https://www.reddit.com/r/MachineLearning/comments/ar9su4/realtime_object_detection_in_the_cloud/,Alien0006,1550331308,[removed],0,1,False,self,,,,,
992,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,0,ar9xta,youtube.com,[N] Get started with Google Colaboratory coding TensorFlow (TensorFlow YouTube channel),https://www.reddit.com/r/MachineLearning/comments/ar9xta/n_get_started_with_google_colaboratory_coding/,asuagar,1550332226,,0,1,False,https://a.thumbs.redditmedia.com/ewVD79IzC7bmbACXMWlQ8SEwuCSOu90gQCDw0NV6Eo4.jpg,,,,,
993,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,0,ara0xz,self.MachineLearning,New in machine learning,https://www.reddit.com/r/MachineLearning/comments/ara0xz/new_in_machine_learning/,droidexpress,1550332791,[removed],0,1,False,self,,,,,
994,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,1,araab4,medium.com,Yoshua Bengio On AI Priors and Challenges: the current AI system is still very stupid,https://www.reddit.com/r/MachineLearning/comments/araab4/yoshua_bengio_on_ai_priors_and_challenges_the/,Yuqing7,1550334438,,0,1,False,https://b.thumbs.redditmedia.com/ehU4MSHfpvRNbEDDzkEfRrBhHMrZEQQPD_25n6RNRhg.jpg,,,,,
995,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,1,arab8l,self.MachineLearning,Pupil area identification?,https://www.reddit.com/r/MachineLearning/comments/arab8l/pupil_area_identification/,Tuffplay,1550334592,[removed],0,1,False,self,,,,,
996,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,1,aracn8,self.MachineLearning,Can self-taught machine learning enthusiast get job as machine learning scientists?,https://www.reddit.com/r/MachineLearning/comments/aracn8/can_selftaught_machine_learning_enthusiast_get/,massman2468,1550334838,[removed],0,1,False,self,,,,,
997,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbepk,self.MachineLearning,GANs and Black People,https://www.reddit.com/r/MachineLearning/comments/arbepk/gans_and_black_people/,KevMuriithi,1550341168,[removed],0,1,False,self,,,,,
998,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbffd,self.MachineLearning,[P] Simple Tensorflow Cookbook for easy-to-use,https://www.reddit.com/r/MachineLearning/comments/arbffd/p_simple_tensorflow_cookbook_for_easytouse/,taki0112,1550341288,[removed],0,1,False,self,,,,,
999,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbfka,self.MachineLearning,How to land an AI internship?,https://www.reddit.com/r/MachineLearning/comments/arbfka/how_to_land_an_ai_internship/,josbahena,1550341311,[removed],0,1,False,self,,,,,
1000,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbh75,i.redd.it,[D] This person does not exist. I went to thispersondoesnotexist.com and this creepy image showed up. The person in the left corner gives me nightmare.,https://www.reddit.com/r/MachineLearning/comments/arbh75/d_this_person_does_not_exist_i_went_to/,sinashish,1550341582,,0,1,False,default,,,,,
1001,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbjuh,self.MachineLearning,[Project] Simple Tensorflow Cookbook for easy-to-use,https://www.reddit.com/r/MachineLearning/comments/arbjuh/project_simple_tensorflow_cookbook_for_easytouse/,taki0112,1550342018,[removed],0,1,False,https://b.thumbs.redditmedia.com/LAV5wikK2iAyqZV7d1xHSOk6NS9d_d1TmBqD47Pr17Q.jpg,,,,,
1002,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arblrm,self.MachineLearning,[Project] Simple Tensorflow Cookbook for easy-to-use,https://www.reddit.com/r/MachineLearning/comments/arblrm/project_simple_tensorflow_cookbook_for_easytouse/,taki0112,1550342331,[removed],0,1,False,https://b.thumbs.redditmedia.com/Cr7s_xV5QcrHYTiRSfgmVZ-7zmnxpZR1axmXU7lohYI.jpg,,,,,
1003,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbm2y,self.MachineLearning,What is the current state of the art in high-dimensional variable selection in low SNR regimes?,https://www.reddit.com/r/MachineLearning/comments/arbm2y/what_is_the_current_state_of_the_art_in/,hftquant,1550342382,[removed],0,1,False,self,,,,,
1004,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbmoh,self.MachineLearning,[P] Simple Tensorflow Cookbook for easy-to-use,https://www.reddit.com/r/MachineLearning/comments/arbmoh/p_simple_tensorflow_cookbook_for_easytouse/,taki0112,1550342481,[removed],0,1,False,https://b.thumbs.redditmedia.com/j7XEOPn4gNVr8_zTLFjD2ibDHFybsnoTHYOQIEdRd8o.jpg,,,,,
1005,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbo5k,self.MachineLearning,[Project] Simple Tensorflow Cookbook for easy-to-use,https://www.reddit.com/r/MachineLearning/comments/arbo5k/project_simple_tensorflow_cookbook_for_easytouse/,taki0112,1550342726,[removed],0,1,False,self,,,,,
1006,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,3,arbor8,self.MachineLearning,[Project] Simple Tensorflow Cookbook for easy-to-use,https://www.reddit.com/r/MachineLearning/comments/arbor8/project_simple_tensorflow_cookbook_for_easytouse/,taki0112,1550342820,[removed],0,1,False,https://a.thumbs.redditmedia.com/LMp0fbRpL7l5QYv6RqfgsxhWjG018iYPFGW2VMHHX50.jpg,,,,,
1007,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,4,arbw17,self.MachineLearning,[Project] Simple Tensorflow Cookbook for easy-to-use,https://www.reddit.com/r/MachineLearning/comments/arbw17/project_simple_tensorflow_cookbook_for_easytouse/,taki0112,1550344015,"&amp;#x200B;

[by taki0112 \(Junho Kim\)](https://i.redd.it/zoqdcsn5bzg21.png)

Hello.

&amp;#x200B;

I made a cookbook for easy use of the tensorflow code.Just copy &amp; paste!

&amp;#x200B;

It's not here, but if there are things you use often, please add them!

Your pull requests and issues are always welcome.

&amp;#x200B;

github :[https://github.com/taki0112/Tensorflow-Cookbook](https://github.com/taki0112/Tensorflow-Cookbook)

&amp;#x200B;

Thank you.",0,1,False,https://b.thumbs.redditmedia.com/8gHypc9zE5MDjkxHD-fIe5X1SoXLwbxQGSDego9ulOI.jpg,,,,,
1008,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,4,arcb19,thenextweb.com,3 Types of AI Fears,https://www.reddit.com/r/MachineLearning/comments/arcb19/3_types_of_ai_fears/,GantMan,1550346569,,0,1,False,default,,,,,
1009,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,5,archl0,youtube.com,I Used ML to Get a Drone to Fly (07:01 - 07:38),https://www.reddit.com/r/MachineLearning/comments/archl0/i_used_ml_to_get_a_drone_to_fly_0701_0738/,brandonhotdog,1550347648,,1,1,False,https://a.thumbs.redditmedia.com/mAJyDWn2uKWpvvAQHEzPbK8Kv6OAHMvmtaiXz2GPtT0.jpg,,,,,
1010,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,5,arci6y,self.MachineLearning,Learning by few examples,https://www.reddit.com/r/MachineLearning/comments/arci6y/learning_by_few_examples/,ricksanchezyouall,1550347750,How're humans able to learn from a single or very few examples but machines still need a large amount of data to do so? Is it something the way our brain is wired or we follow a different learning process?,0,1,False,self,,,,,
1011,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,5,arcjme,self.MachineLearning,"GPT2's answer to 'the ultimate question of life, the universe and everything' is not 42",https://www.reddit.com/r/MachineLearning/comments/arcjme/gpt2s_answer_to_the_ultimate_question_of_life_the/,barmageddon,1550347988,"&amp;#x200B;

*Processing img hypttuhumzg21...*",0,1,False,self,,,,,
1012,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,5,arck9f,arxiv.org,[1901.09024] Diversity-Sensitive Conditional Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/arck9f/190109024_diversitysensitive_conditional/,QuantMountain,1550348091,,2,1,False,default,,,,,
1013,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,6,ard0lu,self.MachineLearning,Is building an ML model mostly trial and error? Or I am I missing something?,https://www.reddit.com/r/MachineLearning/comments/ard0lu/is_building_an_ml_model_mostly_trial_and_error_or/,freebobbyand6ix9ine,1550350895,[removed],0,1,False,self,,,,,
1014,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,6,ardexj,self.MachineLearning,The isolated islands of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ardexj/the_isolated_islands_of_machine_learning/,m_mbahaa,1550353338,"There are tons of frameworks, many models and almost no standards. Is it only me that feels like machine learning is a bunch of isolated islands?",0,1,False,self,,,,,
1015,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,7,are0pm,medium.com,"AI for algorithmic trading: rethinking bars, labeling, and stationarity",https://www.reddit.com/r/MachineLearning/comments/are0pm/ai_for_algorithmic_trading_rethinking_bars/,rachnogstyle,1550357065,,0,1,False,default,,,,,
1016,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,7,are44v,self.MachineLearning,[D] Currently enrolled in a masters degree in Computer Science and am looking for an internship over the summer - care to help me make my decision about which internship to pick?,https://www.reddit.com/r/MachineLearning/comments/are44v/d_currently_enrolled_in_a_masters_degree_in/,The_Grey_Wolf,1550357654,"Hello guys,

I'm posting this here since I think this is the best subreddit to post to, so you can help me make a decision which I'm a bit fuzzy about making.

**My background**

Ever since I read *Superintelligence* by Nick Bostorm I decided that I want to learn more about AI safety. To do that, I decided that I will spend some time building the fundamentals - learning about machine learning - and then contribute to AI safety. *To this very day, I haven't had an internship, except for a very brief period of freelance work.*

**My future plans**

Since eventually I want to do AI safety research, I will probably do a PhD in Machine Learning. But, before I do a PhD (in between my masters degree and my PhD) I would like to get a taste for the Machine Learning landscape. I would prefer to do an internship for some research scientist position, but I feel that I am not yet able enough to do substantial amount of research on my own.

**Internships this summer - the most optimal for my position?**

So, with the above being said, I am faced with (essentially) 3 choices in applying for internships:

1. Research scientist internships - this way I could test out if I actually like doing research, but I don't know if companies exist out there that would take me, a Computer Science major, and show me the research process and guide me
2. Research engineer internships - balance between a research scientist and a ""regular"" software engineer; I read something about this on this subreddit and people had divergent opinions, but the taste I was left with was not to really do this unless it is somewhere like DeepMind
3. ""Classical"" software engineering roles - could be in some major companies, minor local companies etc.

*If you were me, what would you do?* Again, my long-term goal being eventually becoming a research scientist, but keeping in mind I haven't had extensive programming experience in the industry.

Thank you in advance!",4,0,False,self,,,,,
1017,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,8,are8na,nber.org,Machine Learning and Health Care call for papers by NBER,https://www.reddit.com/r/MachineLearning/comments/are8na/machine_learning_and_health_care_call_for_papers/,serghiou,1550358408,,0,1,False,default,,,,,
1018,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,8,are8uq,self.MachineLearning,[R] How do Convolutional Layers Work in Keras? - Comprehensive Stack Overflow answer w/ language agnostic explanation,https://www.reddit.com/r/MachineLearning/comments/are8uq/r_how_do_convolutional_layers_work_in_keras/,Zolmeci,1550358446,[removed],0,1,False,self,,,,,
1019,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,9,arf87j,flancia.org,"About unsupervised language models, or unicorns in South America",https://www.reddit.com/r/MachineLearning/comments/arf87j/about_unsupervised_language_models_or_unicorns_in/,flancian,1550364646,,0,1,False,default,,,,,
1020,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,10,arfc2z,self.MachineLearning,"ML Internship in 3 months, how can I prepare?",https://www.reddit.com/r/MachineLearning/comments/arfc2z/ml_internship_in_3_months_how_can_i_prepare/,lulugirl17,1550365351,[removed],0,1,False,self,,,,,
1021,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,10,arfclz,self.MachineLearning,"[D] What are the fundamental limitations of popular machine learning methods, even with infinite data and compute?",https://www.reddit.com/r/MachineLearning/comments/arfclz/d_what_are_the_fundamental_limitations_of_popular/,yashchandak,1550365453,"Looking forward several years ahead, it will be relatively even easier to gather  even larger datasets and bigger clusters of GPUs...
What in your opinion would still pose a fundamental problem, given the conventional methods we use today?

-----------
Some obvious examples that I know of:
[1] The problem of understanding causality - [Ferenc provides a brilliant blogpost on it](https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/) 
[2] Language understanding - [Enlightening Norvig-Chomsky debate]( http://daselab.cs.wright.edu/nesy/NeSy13/norvig.pdf)
[3] Predicitng the next digit of pi - [though there is a succinct underlying structure to generate it, it is considered impossible to predict the next one given infinite data about the previous numbers](https://www.askamathematician.com/2009/11/since-pi-is-infinite-can-i-draw-any-random-number-sequence-and-be-certain-that-it-exists-somewhere-in-the-digits-of-pi/)

 ",45,39,False,self,,,,,
1022,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,10,arfg1h,self.MachineLearning,[P] Real-time image recognition of key presses,https://www.reddit.com/r/MachineLearning/comments/arfg1h/p_realtime_image_recognition_of_key_presses/,Top_Hat_Tomato,1550366081,"My first little implementation of tensorflow that I was able to get running real-time.

Nothing too special, but my hope is that I'll be able to get it able to recognize the entire keyboard instead of just whether the ""a"" key is pressed (which is the current implementation).",0,1,False,self,,,,,
1023,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,10,arfju2,/r/MachineLearning/comments/arfju2/p_realtime_image_recognition_of_key_presses/,[P] Real-time image recognition of key presses,https://www.reddit.com/r/MachineLearning/comments/arfju2/p_realtime_image_recognition_of_key_presses/,Top_Hat_Tomato,1550366796,,1,3,False,https://b.thumbs.redditmedia.com/cZNu4-001_Yl9FXZFgJG9BxYV0c4klFrnfdusieyjtU.jpg,,,,,
1024,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,11,arg0oy,self.MachineLearning,'Jacobian factor' mentioned in Bishop's PRML,https://www.reddit.com/r/MachineLearning/comments/arg0oy/jacobian_factor_mentioned_in_bishops_prml/,randomicly,1550370008,[removed],0,1,False,self,,,,,
1025,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,11,arg0vm,self.MachineLearning,[P] Simple Tensorflow collection of easy-to-use codes,https://www.reddit.com/r/MachineLearning/comments/arg0vm/p_simple_tensorflow_collection_of_easytouse_codes/,taki0112,1550370044,"&amp;#x200B;

[by taki0112 \(Junho Kim\)](https://i.redd.it/6ihziuz4g1h21.png)

Hello.

I made a cookbook for easy use of the tensorflow code.

Just copy &amp; paste!

&amp;#x200B;

It's not here, but if there are things you use often, please add them!

Your pull requests and issues are always welcome.

&amp;#x200B;

github :[https://github.com/taki0112/Tensorflow-Cookbook](https://github.com/taki0112/Tensorflow-Cookbook)

&amp;#x200B;

Also, I am going to add code for tf-Eager mode !

Thank you.",12,239,False,https://b.thumbs.redditmedia.com/t-htqp01R8i3atn-9vtlWnppk6zdd1YhMII7Jpp6tHk.jpg,,,,,
1026,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,11,arg4tc,i.redd.it,Trump AI upscaled from 446x299.,https://www.reddit.com/r/MachineLearning/comments/arg4tc/trump_ai_upscaled_from_446x299/,mariois64,1550370799,,0,1,False,default,,,,,
1027,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,13,arh0cb,self.MachineLearning,Deep Learning/Reinforcement Learning idea I want to do,https://www.reddit.com/r/MachineLearning/comments/arh0cb/deep_learningreinforcement_learning_idea_i_want/,WebzWorld2,1550377130,"So, me and my friends have spent a few months working on a business plan for our business. We need someone that knows how to code AI in python to make a bot that utilizes deep learning/reinforcement learning . If anyone is interested in  being a co-founder to the company and creating something super unique and disruptive to the whole investment industry, message me :) I'll fill you in on everything. It's really exciting!",0,1,False,self,,,,,
1028,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,13,arh0ms,self.MachineLearning,"Tips to implement industry standard machine learning algorithms like YOLO, DeepLab etc.",https://www.reddit.com/r/MachineLearning/comments/arh0ms/tips_to_implement_industry_standard_machine/,perceptron333,1550377186,[removed],0,1,False,self,,,,,
1029,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,13,arhb11,reddit.com,Machine learning causing science crisis,https://www.reddit.com/r/MachineLearning/comments/arhb11/machine_learning_causing_science_crisis/,lorenzo_fabbri92,1550379399,,0,1,False,default,,,,,
1030,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,14,arhqry,self.computervision,Marathon Bib Identification and Recognition,https://www.reddit.com/r/MachineLearning/comments/arhqry/marathon_bib_identification_and_recognition/,kapilvarshney,1550382785,,0,1,False,https://b.thumbs.redditmedia.com/rgVhV7UbiA9hSAn3j0X7IbMOT6YsLvP-T7Th7qe7QNA.jpg,,,,,
1031,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,15,ari020,self.MachineLearning,Are there papers with time series prediction in AIOps?,https://www.reddit.com/r/MachineLearning/comments/ari020/are_there_papers_with_time_series_prediction_in/,RexKing6,1550384913,[removed],0,1,False,self,,,,,
1032,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,16,aric7k,blog.openai.com,"After training the language model with Reddit content, OpenAI sees GPT-2 as a threat to the world",https://www.reddit.com/r/MachineLearning/comments/aric7k/after_training_the_language_model_with_reddit/,parsprototo,1550388119,,0,1,False,https://b.thumbs.redditmedia.com/36Nfhults6JkdnEGCxO5pzP0lruripIL3momzVGNOZA.jpg,,,,,
1033,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,16,arihji,self.MachineLearning,Should I start learning now?,https://www.reddit.com/r/MachineLearning/comments/arihji/should_i_start_learning_now/,Cyper_Angel,1550389662,[removed],0,1,False,self,,,,,
1034,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,17,arioy9,self.MachineLearning,colabs for stylegan encoder,https://www.reddit.com/r/MachineLearning/comments/arioy9/colabs_for_stylegan_encoder/,eyaler,1550391810,[removed],0,1,False,self,,,,,
1035,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,17,aripqf,youtube.com,SingularityNET Beta | Object + Action Recognition Example,https://www.reddit.com/r/MachineLearning/comments/aripqf/singularitynet_beta_object_action_recognition/,getrich_or_diemining,1550392045,,0,1,False,https://b.thumbs.redditmedia.com/zhhdXTbjJIFPCvV6S-QTfwEotYA-VdLhaLsGxByRaSM.jpg,,,,,
1036,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,17,arisvn,self.MachineLearning,"Id like to hear, or read, a discussion between OpenAI members, other NLP practitioners, and Cybersecurity practitioners discuss the realities of gpt-2s potential threats or overblown worries.",https://www.reddit.com/r/MachineLearning/comments/arisvn/id_like_to_hear_or_read_a_discussion_between/,WrongTechnician,1550392959,,0,1,False,self,,,,,
1037,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,17,ariu1w,self.MachineLearning,[D] How to add a new class to object identifier ?,https://www.reddit.com/r/MachineLearning/comments/ariu1w/d_how_to_add_a_new_class_to_object_identifier/,SaltyStackSmasher,1550393300,"Hi,

&amp;#x200B;

Summary (if you don't have time to read everything):

I have a fully trained model capable of identifying and localizing(drawing bounding boxes) `k` classes of objects. Now I want to add another class to the network, Is there any sleek way of doing this other than completely throwing the current network and retraining it again with `k + 1` classes ? If not, why ?

&amp;#x200B;

I've been training an object localization model for a week now(yet to test its results) While it is still under training, I wanted to know if there is any way to add a new object to this dataset. The dataset I'm using is a collected set of cat images and their bounding boxes. Now after a month, I'll be getting dog images and their bounding boxes too. My project manager also requested me to  identify dog images as a bonus.

&amp;#x200B;

Is there any elegant way of doing this other than training the network from scratch with dogs + cats data ? I think this would be close to 'face verification' task(one-to-many matches) but I didn't find much useful resources online. Can anyone shed some light on the approach that will be applicable in my case ?? I believe one-shot learning problem won't apply here because I should be able to add multiple classes to current object localization network. I also believe that there was something called as 'Online Learning' problem that resembles with my current problem, however searching for those keywords yields none other than websites like coursera, udemy, edX (because they're online educational platforms and search engines kinda confuse it with online learning)",10,5,False,self,,,,,
1038,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,18,arj01w,twitter.com,Thread on OpenAI's decision not to release the full GPT2 model by Joshua Achiam on Twitter,https://www.reddit.com/r/MachineLearning/comments/arj01w/thread_on_openais_decision_not_to_release_the/,Spugpow,1550395051,,0,1,False,default,,,,,
1039,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,18,arj3r5,medium.com,Track and organize fastai experimentation process in Neptune,https://www.reddit.com/r/MachineLearning/comments/arj3r5/track_and_organize_fastai_experimentation_process/,ai_yoda,1550396068,,0,1,False,default,,,,,
1040,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,19,arjbk6,self.MachineLearning,Looking for a place to read papers,https://www.reddit.com/r/MachineLearning/comments/arjbk6/looking_for_a_place_to_read_papers/,mahav2000,1550398360,What is the best place to start reading papers related to machine learning? I am a beginner and I have worked with all the basic algorithms and have also worked with CNN's for image processing. I want to get into research but I have no idea where I can find papers that match my level of understanding to read and develop my knowledge.,0,1,False,self,,,,,
1041,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,19,arjfi3,blockdelta.io,Could Artificial Intelligence Lead to Communism?,https://www.reddit.com/r/MachineLearning/comments/arjfi3/could_artificial_intelligence_lead_to_communism/,BlockDelta,1550399463,,0,1,False,https://a.thumbs.redditmedia.com/De1lp5D0hTQDjTcgFPPzOOzlCLW9Ps1uQYQ3y_AUjK0.jpg,,,,,
1042,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,20,arjm65,self.MachineLearning,One Shot Facial Recognition for Blacklist Security?,https://www.reddit.com/r/MachineLearning/comments/arjm65/one_shot_facial_recognition_for_blacklist_security/,The_Austinator,1550401416,[removed],0,1,False,self,,,,,
1043,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,20,arjnhc,self.MachineLearning,[D] A Quick Start for Matrix Calculus,https://www.reddit.com/r/MachineLearning/comments/arjnhc/d_a_quick_start_for_matrix_calculus/,LynnHoHZL,1550401790,I write a page [https://github.com/LynnHo/Matrix-Calculus](https://github.com/LynnHo/Matrix-Calculus) of a quick start for deriving most matrix derivatives (listed in [wiki](https://en.wikipedia.org/wiki/Matrix_calculus)).  Welcome to discuss.,17,28,False,self,,,,,
1044,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,20,arjo68,self.MachineLearning,Key Factors in The Successful Use of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/arjo68/key_factors_in_the_successful_use_of_machine/,andrea_manero,1550401989,[removed],0,1,False,self,,,,,
1045,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,20,arjsre,self.MachineLearning,How to query a domain-specific knowledge graph using natural language for question answering?,https://www.reddit.com/r/MachineLearning/comments/arjsre/how_to_query_a_domainspecific_knowledge_graph/,rahul1214,1550403334,[removed],0,1,False,self,,,,,
1046,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,20,arju6w,github.com,PyTorch implementation of GOTURN single object tracker,https://www.reddit.com/r/MachineLearning/comments/arju6w/pytorch_implementation_of_goturn_single_object/,amoudgl,1550403695,,0,1,False,default,,,,,
1047,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,20,arjvpw,self.MachineLearning,[Project] [Discussion] How to query a domain-specific knowledge graph using natural language for question answering?,https://www.reddit.com/r/MachineLearning/comments/arjvpw/project_discussion_how_to_query_a_domainspecific/,rahul1214,1550404146,"We have extracted entities and relations from medical data to map them to a knowledge graph (It's currently in Neo4j, but we're open to suggestions). We want to build a QA system on top of this knowledge graph (KG). **How do we convert natural language queries to a language that can query the KG?** (A few papers we read used SPARQL to query huge knowledge bases such as freebase etc., but since we're using our own knowledge graph, we are not sure how to proceed).

Also on a tangential note, will using graph embeddings help us in QA over using a graph database like Neo4J?

Any help would be much appreciated, thanks! (I had posted this on r/MLQuestions earlier but did not receive any replies) ",7,7,False,self,,,,,
1048,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,21,arka2z,self.MachineLearning,Deep Learning applied to software testing,https://www.reddit.com/r/MachineLearning/comments/arka2z/deep_learning_applied_to_software_testing/,clarissa_ar,1550407865,[removed],0,1,False,default,,,,,
1049,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,22,arkd7y,self.MachineLearning,[D] Which is the best method for non-linear number mapping,https://www.reddit.com/r/MachineLearning/comments/arkd7y/d_which_is_the_best_method_for_nonlinear_number/,jason-jo,1550408614,"Hello all

I am working with a mapping problem.  I have several hundred million numbers that I need to map to a value representing  the number.  So from what I've understood so far reading tutorials and playing around with Python and R, I have two choices: either I can do a multi class mapping or some kind of regression.  The problem with multi class mapping is that I have thousands of classes, so I think some kind of regression might be easier but the problem is that the mapping isn't remotely linear.  At least, if there is some pattern I've not been able to identify it from the graph.  Here is a sampling of the data:

3251, 28686

3253, 28686

3254, 28686

3257, 28686

3258, 28686

3260, 28686

3271, 28674

3275, 28674

3277, 28674

3278, 28674

3283, 28686

3285, 28686

3286, 28686

3289, 28686

3290, 28686

3292, 28686

3299, 28686

3301, 28686

3302, 28686

3305, 28686

3306, 28686

3308, 28686

3313, 32782

3314, 32782

Can this be done by Polynomial regression?  Something else?  Or does it have to be modelled with some kind of multi class solution?",46,5,False,self,,,,,
1050,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,22,arkom1,self.MachineLearning,How protect my models in cloud?,https://www.reddit.com/r/MachineLearning/comments/arkom1/how_protect_my_models_in_cloud/,fabioibaf,1550411276,[removed],0,1,False,self,,,,,
1051,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,22,arkqxb,self.MachineLearning,How to increase accuracy of MP Neuron Model?,https://www.reddit.com/r/MachineLearning/comments/arkqxb/how_to_increase_accuracy_of_mp_neuron_model/,arohanajit232,1550411802,[removed],0,1,False,self,,,,,
1052,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,23,arksa1,i.redd.it,Maybe I should get data on Girls,https://www.reddit.com/r/MachineLearning/comments/arksa1/maybe_i_should_get_data_on_girls/,SSSSSS78,1550412095,,0,1,False,https://b.thumbs.redditmedia.com/_O815B_nfYpo2F3cV8lzTkYG8mdNsVgdrM1hs368RlU.jpg,,,,,
1053,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,23,arky4c,self.MachineLearning,[P] How can I improve my current CNN project? It is a simple binary classification but I am having some trouble.,https://www.reddit.com/r/MachineLearning/comments/arky4c/p_how_can_i_improve_my_current_cnn_project_it_is/,Al7123,1550413294,"Hello, I'm quite new to this, so take it easy with me.

Inspired by [Pepito the cat](www.twitter.com/pepitothecat/) I decided to make a cnn to detect when my dog goes to the porch to bark at home.

I have now 2000 pictures of the porch without him and 2000 with him and about 1000 of just him (cutout).

I thought it was very simple since the camera is still and he is a black dog. I've got some good results even with changing light and shadows during the day, but I realized that it is detecting black blobs on the image and not his contour/edges. I mean, if put anything black in front of the camera It will trigger, not only just him.

What can I do to improve my neural network?",17,1,False,self,,,,,
1054,MachineLearning,t5_2r3gv,2019-2-17,2019,2,17,23,arl83h,self.MachineLearning,[P] Check out my Deep Reinforcement Learning mini-library for Keras,https://www.reddit.com/r/MachineLearning/comments/arl83h/p_check_out_my_deep_reinforcement_learning/,csxeba,1550415385,"Hi guys, check out my Deep RL library, [trickster](https://github.com/csxeba/trickster).

It is intended to be as clean as possible, it's existence is the side-effect of my learning :) Keras is the intended ANN backend for the library and a Gym-like interface is expected from the environments.

The following algos are implemented:

* DQN
* Double DQN
* REINFORCE
* A2C (aka. VPG with baseline)
* PPO

PPO is not perfect at the moment, I intend to check it against OpenAI spinning up, but the others are converging for smaller problems relatively quickly.

The library is of course much worse than mainstream RL libraries like anyrl and keras-rl, baselines, etc. It is capable of handling multiple environments at once, but it is not (yet?) parallelized.

Any kind of feedback is welcome, feel free to file issues, PRs and comments!",1,33,False,self,,,,,
1055,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,0,arlad7,self.MachineLearning,3D Face Reconstruction using Position Map Regression,https://www.reddit.com/r/MachineLearning/comments/arlad7/3d_face_reconstruction_using_position_map/,Cipher10,1550415835,[removed],0,1,False,self,,,,,
1056,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,0,arlfui,self.MachineLearning,python output help?,https://www.reddit.com/r/MachineLearning/comments/arlfui/python_output_help/,helpnhelper,1550416871,[removed],0,1,False,self,,,,,
1057,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,0,arlhij,d4mucfpksywv.cloudfront.net,[R] Language Models are Unsupervised Multitask Learners,https://www.reddit.com/r/MachineLearning/comments/arlhij/r_language_models_are_unsupervised_multitask/,dimber-damber,1550417162,,0,1,False,default,,,,,
1058,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,0,arllrq,i.redd.it,How to find the bias in three conditions?,https://www.reddit.com/r/MachineLearning/comments/arllrq/how_to_find_the_bias_in_three_conditions/,bruce1923,1550417948,,0,1,False,https://b.thumbs.redditmedia.com/2ihZndVfU8dq6zc5ojXaGClyBKmhe343VJ9JSzEJ2_A.jpg,,,,,
1059,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,0,arlq1d,self.MachineLearning,[D] Did weight decay fall out of favor for regularizing NNs?,https://www.reddit.com/r/MachineLearning/comments/arlq1d/d_did_weight_decay_fall_out_of_favor_for/,puchosluda,1550418692,"At least this is my impression based on it no being used anymore in newer papers. But how come? By contrast to other techniques it actually has a very solid theoretical foundation.

&amp;#x200B;",34,109,False,self,,,,,
1060,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,1,arm1pe,self.MachineLearning,[Discussion] What is the latest research on SGD as MCMC and its uses?,https://www.reddit.com/r/MachineLearning/comments/arm1pe/discussion_what_is_the_latest_research_on_sgd_as/,astonished_crofty,1550420704,"As it says on the tin. As far as I am aware, how neural networks generalize is still a mystery. Since we cannot be better than Bayesian, it makes sense to treat SGD as a Bayesian process and try to solve the problem from that end. I have seen some slides, colleagues mentioning this concept. But I do not know what is the latest understanding of this link in the field. Can anyone experience shed some light?",17,12,False,self,,,,,
1061,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,1,arm4bt,go.morioh.net,Machine Learning in JavaScript with TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/arm4bt/machine_learning_in_javascript_with_tensorflowjs/,AbbeyEg,1550421144,,0,1,False,default,,,,,
1062,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,1,arm4k1,towardsdatascience.com,The complete beginners guide to data cleaning and preprocessing,https://www.reddit.com/r/MachineLearning/comments/arm4k1/the_complete_beginners_guide_to_data_cleaning_and/,tastingeverything,1550421186,,0,1,False,default,,,,,
1063,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,1,armbp6,self.MachineLearning,How to compute the inverse of mapping X-&gt;Y using machine learning methods?,https://www.reddit.com/r/MachineLearning/comments/armbp6/how_to_compute_the_inverse_of_mapping_xy_using/,dancingpig2011,1550422381,[removed],0,1,False,self,,,,,
1064,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,2,armhby,self.MachineLearning,How to compute the inverse of mapping X-&gt;Y using machine learning methods?,https://www.reddit.com/r/MachineLearning/comments/armhby/how_to_compute_the_inverse_of_mapping_xy_using/,dancingpig2011,1550423286," Hello, denote X features with size n, Y the corresponding property. It is not hard to find their correlation by computing the mapping X-&gt;Y using regression methods on a large size of training data. My question is how we find the inverse of the mapping? Specifically, given the desired property, how to compute its corresponding features? Thanks! ",6,2,False,self,,,,,
1065,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,2,armltp,self.MachineLearning,[D] Managerial books on ML,https://www.reddit.com/r/MachineLearning/comments/armltp/d_managerial_books_on_ml/,Seen_Unseen,1550423997,"I feel a tiny introduction is required on the situation. I own a fairly large company doing online sales in a highly competitive market (not US/EU). And it's hard to escape seemingly that everyone is in AI/ML which makes me want to understand more about it as well being able to move our company towards applying there where we see fit. I imagine we could achieve a lot more with ML for SEO/SEM through the vast databases we have build among others.

The issue though is that I'm not a programmer, I have a team of guys who currently run various platforms and develop according to our needs. Opposed to what many do, I opted to develop according to our needs and stay away from existing platforms due to my high distrust of local suppliers.

What books you guys would suggest me to pick up to understand better where we could go. I finished a while ago The master algorithm but what else gives?",6,3,False,self,,,,,
1066,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,2,armq2w,self.MachineLearning,Python code dump for beginners to copy/paste?,https://www.reddit.com/r/MachineLearning/comments/armq2w/python_code_dump_for_beginners_to_copypaste/,peratrom,1550424669,[removed],0,1,False,self,,,,,
1067,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,2,armthm,markgacoka.com,Could AI be used to predict stock prices? [Answer and Tutorial],https://www.reddit.com/r/MachineLearning/comments/armthm/could_ai_be_used_to_predict_stock_prices_answer/,SilentDifficulty,1550425185,,1,1,False,default,,,,,
1068,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,2,arn1b9,self.MachineLearning,a question regarding a typical regression problem,https://www.reddit.com/r/MachineLearning/comments/arn1b9/a_question_regarding_a_typical_regression_problem/,C00255207,1550426377,[removed],0,1,False,self,,,,,
1069,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,3,arngfq,self.MachineLearning,Machine Learning in PHP with Rubix ML Beta (Examples and Tutorials included),https://www.reddit.com/r/MachineLearning/comments/arngfq/machine_learning_in_php_with_rubix_ml_beta/,andrewdalpino,1550428669,[removed],0,1,False,self,,,,,
1070,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,3,arnkl9,self.MachineLearning,[D] Machine Learning in PHP w/ Rubix ML Beta (Examples and Tutorials included),https://www.reddit.com/r/MachineLearning/comments/arnkl9/d_machine_learning_in_php_w_rubix_ml_beta/,andrewdalpino,1550429312," Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning), OP and author here,

&amp;#x200B;

I wanted to see if we could get a discussion going with the community about my project [Rubix ML for PHP](https://github.com/RubixML) that I released into beta

I realize the *PHP* engineer among us is fewer and far between, however, I figured that we could discuss anything on topic

Feel free to ask me about the library, why I wrote it, or anything you're curious about

&amp;#x200B;

Here's a link to the repo

[https://github.com/RubixML](https://github.com/RubixML)

&amp;#x200B;

Thanks",0,1,False,self,,,,,
1071,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,3,arnl1c,self.MachineLearning,Interesting output from OpenAI's gpt2,https://www.reddit.com/r/MachineLearning/comments/arnl1c/interesting_output_from_openais_gpt2/,vin101,1550429381,[removed],0,1,False,self,,,,,
1072,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,4,arnpm0,self.MachineLearning,How does Wittgenstein support a ML approach to natural language processing?,https://www.reddit.com/r/MachineLearning/comments/arnpm0/how_does_wittgenstein_support_a_ml_approach_to/,James_Representi,1550430081,"In an article in Quartz the author says:

&gt; This connection is a representation of Wittgensteins notion of language. In *Philosophical Investigations,* published posthumously in 1953, the philosopher argued that there are no standard, fixed meanings to words; instead, their meanings lie in their use. 

&amp;#x200B;

She then goes on to say that Wittgenstein's discussions support word2vec. The article's title states:  

# ""Google Translate is a manifestation of Wittgensteins theory of language""

&amp;#x200B;

So, my question is, to what specifically in Wittgenstein's ""Philosophical Investigations"" (I'm assuming) are they referring?

&amp;#x200B;

A link to the article:

[https://qz.com/1549212/google-translate-is-a-manifestation-of-wittgensteins-theory-of-language/](https://qz.com/1549212/google-translate-is-a-manifestation-of-wittgensteins-theory-of-language/)",0,1,False,self,,,,,
1073,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,4,arnq16,self.MachineLearning,[D] Machine Learning in PHP (Examples and Tutorials included),https://www.reddit.com/r/MachineLearning/comments/arnq16/d_machine_learning_in_php_examples_and_tutorials/,andrewdalpino,1550430144,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning), OP and author here,

&amp;#x200B;

I wanted to see if we could get a discussion going with the community about my project [Rubix ML for PHP](https://github.com/RubixML) that I released into beta

I realize the *PHP* engineer among us is fewer and far between, however, I figured that we could discuss anything on topic

Feel free to ask me about the library, why I wrote it, or anything you're curious about

&amp;#x200B;

Here's a link to the repo

[https://github.com/RubixML](https://github.com/RubixML)

&amp;#x200B;

You can check out our discussion in r/PHP here

[https://www.reddit.com/r/PHP/comments/ap7ctl/machine\_learning\_in\_php\_with\_examples\_tutorials/](https://www.reddit.com/r/PHP/comments/ap7ctl/machine_learning_in_php_with_examples_tutorials/)

&amp;#x200B;

Thanks",0,2,False,self,,,,,
1074,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,4,arnuf7,self.MachineLearning,[Discussion] How does Wittgenstein support a ML approach to natural language processing?,https://www.reddit.com/r/MachineLearning/comments/arnuf7/discussion_how_does_wittgenstein_support_a_ml/,James_Representi,1550430809," In an article in [Quartz](https://qz.com/1549212/google-translate-is-a-manifestation-of-wittgensteins-theory-of-language/) the author says:

&gt;This connection is a representation of Wittgensteins notion of language. In *Philosophical Investigations,* published posthumously in 1953, the philosopher argued that there are no standard, fixed meanings to words; instead, their meanings lie in their use.

She then goes on to say that Wittgenstein's discussions support word2vec. The article's title states:

# ""Google Translate is a manifestation of Wittgensteins theory of language""

&amp;#x200B;

So, my question is, to what specifically in Wittgenstein's ""Philosophical Investigations"" (I'm assuming) are they referring?

&amp;#x200B;",27,8,False,self,,,,,
1075,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,4,arnwg3,engineersays.com,types of lathe machine,https://www.reddit.com/r/MachineLearning/comments/arnwg3/types_of_lathe_machine/,sanatariq,1550431115,,0,1,False,default,,,,,
1076,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,4,aro0td,self.MachineLearning,[D] A blog post introducing variational inference,https://www.reddit.com/r/MachineLearning/comments/aro0td/d_a_blog_post_introducing_variational_inference/,HomogeneousSpace,1550431769,"Hello r/ml, recently I wrote a post giving a mathematical introduction and derivations of various models of variational inference.

&amp;#x200B;

**Abstract**

In this post I give an introduction to variational inference, which is about maximising the evidence lower bound (ELBO).

I use a top-down approach, starting with the KL divergence and the  ELBO, to lay the mathematical framework of all the models in this post.

Then I define mixture models and the EM algorithm, with Gaussian  mixture model (GMM), probabilistic latent semantic analysis (pLSA) the  hidden markov model (HMM) as examples.

After that I present the fully Bayesian version of EM, also known as  mean field approximation (MFA), and apply it to fully Bayesian mixture  models, with fully Bayesian GMM (also known as variational GMM), latent  Dirichlet allocation (LDA) and Dirichlet process mixture model (DPMM) as  examples.

Then I explain stochastic variational inference, a modification of EM and MFA to improve efficiency.

Finally I talk about autoencoding variational Bayes (AEVB), a  Monte-Carlo + neural network approach to raising the ELBO, exemplified  by the variational autoencoder (VAE). I also show its fully Bayesian  version.

&amp;#x200B;

**Link:**

[https://ypei.me/posts/2019-02-14-raise-your-elbo.html](https://ypei.me/posts/2019-02-14-raise-your-elbo.html)

&amp;#x200B;

All feedback is welcome.",10,70,False,self,,,,,
1077,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,5,aroevs,nassma.um6p.ma,Summer School in ML for (African but not only) Phd students and engineers.,https://www.reddit.com/r/MachineLearning/comments/aroevs/summer_school_in_ml_for_african_but_not_only_phd/,saadoune2018,1550433890,,0,1,False,default,,,,,
1078,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,5,arofym,engineersays.com,Drill Machine and Types of Drill Machine. Fully Explained,https://www.reddit.com/r/MachineLearning/comments/arofym/drill_machine_and_types_of_drill_machine_fully/,sanatariq,1550434051,,0,1,False,default,,,,,
1079,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,5,arombr,self.MachineLearning,import gpt 2 into google colab,https://www.reddit.com/r/MachineLearning/comments/arombr/import_gpt_2_into_google_colab/,loopy_fun,1550435049,[removed],0,1,False,self,,,,,
1080,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,5,aroosf,stats.stackexchange.com,Why are rewards scaled when using Reinforcement Learning (RL) algorithms in practice?,https://www.reddit.com/r/MachineLearning/comments/aroosf/why_are_rewards_scaled_when_using_reinforcement/,real_pinocchio,1550435439,,0,1,False,https://b.thumbs.redditmedia.com/PtW3SQLl6psrb3_YUQty_1U1q7wTb2sW6fo4ckThZ2o.jpg,,,,,
1081,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,5,aros63,self.MachineLearning,[D] Training a model based on a Dataset from Kaggel,https://www.reddit.com/r/MachineLearning/comments/aros63/d_training_a_model_based_on_a_dataset_from_kaggel/,githugs,1550435976,"Hi , I'm new to AI   
I have found a really nice dataset in Kaggel that contains a bunch of csv files and pictures of paintings and info about their authors ,   
and i want to know how i can make use of it , as in make a model , train it and test it . (any good tutorials , or articles on this type of data sets ect ...)  
here is the link to the data set :  
[https://www.kaggle.com/c/painter-by-numbers/data](https://www.kaggle.com/c/painter-by-numbers/data)  


any help is much appreciated  !! ",0,0,False,self,,,,,
1082,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,6,arp608,self.MachineLearning,Counting number of students in a classroom using existing technology (cameras / wifi etc).,https://www.reddit.com/r/MachineLearning/comments/arp608/counting_number_of_students_in_a_classroom_using/,python_J,1550438119,[removed],0,1,False,self,,,,,
1083,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,6,arp63r,nassma-ml.org,flair:News 2019 Summer School in ML for (African but not only) Phd students and engineers.,https://www.reddit.com/r/MachineLearning/comments/arp63r/flairnews_2019_summer_school_in_ml_for_african/,saadoune2018,1550438134,,0,1,False,default,,,,,
1084,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,6,arpg7p,self.MachineLearning,Google AI Residency Program Onsite Interview Questions?,https://www.reddit.com/r/MachineLearning/comments/arpg7p/google_ai_residency_program_onsite_interview/,kaafar,1550439716,[removed],0,1,False,self,,,,,
1085,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,6,arplfc,self.MachineLearning,I have multiple csv files how do I get them into one so that it can be used in a machine learning model,https://www.reddit.com/r/MachineLearning/comments/arplfc/i_have_multiple_csv_files_how_do_i_get_them_into/,travin1998,1550440514,[removed],1,1,False,self,,,,,
1086,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,7,arppd2,self.MachineLearning,"pomegranate v0.11 released, now with neural probabilistic models",https://www.reddit.com/r/MachineLearning/comments/arppd2/pomegranate_v011_released_now_with_neural/,ants_rock,1550441143,"Howdy all

I just released pomegranate v0.11. There are some bug fixes, speed improvements, and significantly expanded tutorials. The most significant additions are:

* NetworkX v2.0 support
* Custom distribution support (https://bit.ly/2rjWjJx)
* Neural probabilistic models (https://bit.ly/2QQcMDH)

The ability to merge probabilistic models with neural networks is the biggest addition. These are probabilistic models that, rather than having probability distributions (or mixtures) as the emissions, have a single neural network that takes in the observations and outputs class probabilities. You can read more about it in the tutorial above.

I had two use-cases of neural probabilistic models in mind when I added support for them. The first is that when you don't know what probability distribution to use, a neural network can learn the appropriate transformation without having to specify a distribution in advance. The second is when you'd like to extend a pre-trained neural network to sequences, such as extending image classification to videos. Because HMMs don't need labels, they can learn how to extend pre-trained models to sequences without need for labeled training data. 

The pomegranate repository is here: https://github.com/jmschrei/pomegranate

The tutorial repository is here: https://github.com/jmschrei/pomegranate/tree/master/tutorials

You can get pomegranate using `pip install pomegranate` or by building it from source. It's written mostly in Cython so you may need a working C compiler if you're building from source.

I'd love to get any feedback you may have!",0,1,False,self,,,,,
1087,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,7,arppwm,self.MachineLearning,[P] pomegranate v0.11 released: now with neural probabilistic models,https://www.reddit.com/r/MachineLearning/comments/arppwm/p_pomegranate_v011_released_now_with_neural/,ants_rock,1550441236,"Howdy all

I just released pomegranate v0.11. There are some bug fixes, speed improvements, and significantly expanded tutorials. The most significant additions are:

* NetworkX v2.0 support
* Custom distribution support (https://bit.ly/2rjWjJx)
* Neural probabilistic models (https://bit.ly/2QQcMDH)

The ability to merge probabilistic models with neural networks is the biggest addition. These are probabilistic models that, rather than having probability distributions (or mixtures) as the emissions, have a single neural network that takes in the observations and outputs class probabilities. You can read more about it in the tutorial above.

I had two use-cases of neural probabilistic models in mind when I added support for them. The first is that when you don't know what probability distribution to use, a neural network can learn the appropriate transformation without having to specify a distribution in advance. The second is when you'd like to extend a pre-trained neural network to sequences, such as extending image classification to videos. Because HMMs don't need labels, they can learn how to extend pre-trained models to sequences without need for labeled training data. 

The pomegranate repository is here: https://github.com/jmschrei/pomegranate

The tutorial repository is here: https://github.com/jmschrei/pomegranate/tree/master/tutorials

You can get pomegranate using `pip install pomegranate` or by building it from source. It's written mostly in Cython so you may need a working C compiler if you're building from source.

I'd love to get any feedback you may have!",0,1,False,self,,,,,
1088,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,7,arpsjq,self.MachineLearning,Alternatives to scikit-learn,https://www.reddit.com/r/MachineLearning/comments/arpsjq/alternatives_to_scikitlearn/,valentincalomme,1550441670,[removed],0,1,False,self,,,,,
1089,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,7,arpysz,self.MachineLearning,[D] OpenAI Not Releasing Fully Trained Model - Is it just an exercise in delaying the inevitable.,https://www.reddit.com/r/MachineLearning/comments/arpysz/d_openai_not_releasing_fully_trained_model_is_it/,JamieMcLachlan,1550442720,OpenAI has gone against their standard practice of releasing a fully trained model and instead has released a smaller model for experimentation. They have stated that it is out of fear of it being utilized by people with malicious intent. Do people think that eventually technology like this will end up in the hands of people with malicious intent and if so is this just an exercise in delaying the inevitable?  ,147,225,False,self,,,,,
1090,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,7,arq7oy,self.MachineLearning,[R] What are the earliest papers to illustrate neural networks for machine learning?,https://www.reddit.com/r/MachineLearning/comments/arq7oy/r_what_are_the_earliest_papers_to_illustrate/,ExilePrime,1550444199,,4,2,False,self,,,,,
1091,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,8,arqf1l,self.MachineLearning,OpenAIs gpt-2 running ask it anything I guess,https://www.reddit.com/r/MachineLearning/comments/arqf1l/openais_gpt2_running_ask_it_anything_i_guess/,Leader_of_Internet,1550445450,[removed],0,1,False,self,,,,,
1092,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,8,arqf5t,digitaltrends.com,Statistician raises red flag about reliability of machine learning techniques,https://www.reddit.com/r/MachineLearning/comments/arqf5t/statistician_raises_red_flag_about_reliability_of/,lengau,1550445474,,0,1,False,default,,,,,
1093,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,8,arqlf0,youtube.com,Get free GPU for training with Google Colab - Detailed Tutorial,https://www.reddit.com/r/MachineLearning/comments/arqlf0/get_free_gpu_for_training_with_google_colab/,Additional_Proof,1550446581,,0,1,False,default,,,,,
1094,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,9,arqtq0,felonyferry.com,Felony Ferry,https://www.reddit.com/r/MachineLearning/comments/arqtq0/felony_ferry/,reneeddweinstei,1550448068,,0,1,False,https://b.thumbs.redditmedia.com/iVZU5QC7g5bz-z7hQDuJMxUW5YoFmVPAJCxKQjjwGXM.jpg,,,,,
1095,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,9,arqzre,self.MachineLearning,[D] What preliminary papers that I need before I dive into invertible ResNets?,https://www.reddit.com/r/MachineLearning/comments/arqzre/d_what_preliminary_papers_that_i_need_before_i/,akaberto,1550449138,,3,10,False,self,,,,,
1096,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,10,arrkyh,amazon.com,shopping cart covers for baby,https://www.reddit.com/r/MachineLearning/comments/arrkyh/shopping_cart_covers_for_baby/,sharondapearypr,1550453053,,0,1,False,default,,,,,
1097,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,10,arrn36,self.MachineLearning,[P] OpenWebText - Open clone of the GPT-2 WebText dataset,https://www.reddit.com/r/MachineLearning/comments/arrn36/p_openwebtext_open_clone_of_the_gpt2_webtext/,eukaryote31,1550453448,"This project is a clone of the GPT-2 WebText dataset as outlined in the [OpenAI paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). While the script is still WIP, it should work reasonably well. Be warned--this script could take a long time to run. Contributions to the filtering heuristics would be very welcome. Once I finish downloading all the URLs, I might add a tgz of the download URL list to save a few hours off the process. 

&amp;#x200B;

GitHub: [https://github.com/eukaryote31/openwebtext](https://github.com/eukaryote31/openwebtext)

&amp;#x200B;

Have fun!",14,48,False,self,,,,,
1098,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,10,arrrgc,self.MachineLearning,"How are online weights separated from the target weights? ""[Project]""",https://www.reddit.com/r/MachineLearning/comments/arrrgc/how_are_online_weights_separated_from_the_target/,premepopulation,1550454290,"I'm creating a *DDPG* neural network in *tensorflow* that involves using a target value and a target action for stable learning, and the common way of doing this is to have code like the below:

`# Actor Network`  
 `self.inputs, self.out, self.scaled_out = self.create_actor_network()`  
 `self.network_params = tf.trainable_variables()`  
 `# Target Network`  
 `self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()`  
 `self.target_network_params = tf.trainable_variables()[len(self.network_params):]`

&amp;#x200B;

My question is, how does the target model know to use the target weights? I'm still learning so I may have misused some terms. 

&amp;#x200B;",3,2,False,self,,,,,
1099,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,10,arru7f,self.MachineLearning,Support vector Machines conceptual question: Minimum memory needed to store parameters for linear and non-linear kernels?,https://www.reddit.com/r/MachineLearning/comments/arru7f/support_vector_machines_conceptual_question/,stats_nerd21,1550454807," 

I saw a question asking what is the minimum possible memory needed to store parameters for a SVM classification model that outputs a boolean variable (so a binary classifier), where each data-point has N features.

I am guessing the minimum memory will correspond to simply the number of weight parameters that are trained/stored in the SVM model and used for predictions. I understand the dimensions of the weight parameters will differ based on what kind of kernel is being used. Here is what I think is the answer to the questions is for different models:

1) Linear kernel- the minimum memory needed here should be for storing (N+2) weight parameters, similar to that of a linear regression model (N parameters for N features + 1 intercept parameter) + 1 regularizer (C) parameter

2) RBF/Gaussian kernel- the minimum memory needed here should be for storing (N+3) weight parameters: N weights for N features, 1 intercept parameter, 1 regularizer parameter, and 1 gamma parameter that decides the spread of Gaussian distribution

3) Complex- non-linear Kernel:  
For a kernel with a decision boundary like this ([https://imgur.com/NM6NFgS](https://imgur.com/NM6NFgS)) I am guessing the minimum number of weight parameters would be &gt;2N+1, and depend on how complex of a kernel I am creating.

&amp;#x200B;

Is my reasoning through this answer correct? Please let me know if it's not. Also, if I am being dumb and missing the whole point of the question and if there's some other obvious way to minimize memory, please do let me know about that as well. Thank you",0,1,False,self,,,,,
1100,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,11,ars1uj,sites.google.com,[N] CS294-158 Deep Unsupervised Learning Spring 2018,https://www.reddit.com/r/MachineLearning/comments/ars1uj/n_cs294158_deep_unsupervised_learning_spring_2018/,CartPole,1550456196,,0,1,False,default,,,,,
1101,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,11,arsf1y,self.MachineLearning,"Try to merge algorithms of random forest, adaboost and GBDT together into one method",https://www.reddit.com/r/MachineLearning/comments/arsf1y/try_to_merge_algorithms_of_random_forest_adaboost/,frank_cao,1550458646,"Nowadays I am trying to merge random forest, adaboost and GBDT together;

I have pushed my code about this onto github: [https://github.com/frank0532/merge\_randomForest\_adaboost\_GBDT](https://github.com/frank0532/merge_randomForest_adaboost_GBDT)

I tried by two methods: simplified one and completed one; 

The former seems to run well but the later has some problems on 'adaboost' stage;

 

1. Simplified:

i.	Create random forest which includes n\_1 big Trees;

ii.	Each big Tree in random forest is created by n\_2 trees which are combined by GBDT;

iii.	Each tree in GBDT is trained by sample\_weight according to Adaboost;

Note: this method seems to run well until now;

2. Completed:

i.	Create random forest which includes n\_1 big Trees;

ii.	Each big Tree in random forest is created by n\_2 big Trees which are combined by Adaboost;

iii.	Each big Tree in Adaboost is created by n\_3 trees which are combined by GBDT;

iv.	Each tree in GBDT is trained according to GBDT completely;

Note: this method seems to have some problems in Adaboost stage.",0,1,False,self,,,,,
1102,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,12,arsrqb,digitaltrends.com,[N] Statistician raises red flag about reliability of machine learning techniques,https://www.reddit.com/r/MachineLearning/comments/arsrqb/n_statistician_raises_red_flag_about_reliability/,simpleconjugate,1550461065,,0,1,False,default,,,,,
1103,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,12,arsx3c,self.MachineLearning,Self taught Software Engineer looking to get into Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/arsx3c/self_taught_software_engineer_looking_to_get_into/,ballinhuge,1550462024,[removed],0,1,False,self,,,,,
1104,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,13,art63i,self.MachineLearning,Help with pocketsphinx-python updates for SpeechRecognition project,https://www.reddit.com/r/MachineLearning/comments/art63i/help_with_pocketsphinxpython_updates_for/,thCRITICAL,1550463690,[removed],0,1,False,self,,,,,
1105,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,13,artcd0,9spl.com,ML KIT &amp; Its Feature Highlights,https://www.reddit.com/r/MachineLearning/comments/artcd0/ml_kit_its_feature_highlights/,jamielannisters,1550464927,,0,1,False,default,,,,,
1106,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,13,artg5v,self.MachineLearning,[D] Looking for papers on the structure of artificial neural networks,https://www.reddit.com/r/MachineLearning/comments/artg5v/d_looking_for_papers_on_the_structure_of/,Pseudoabdul,1550465674,"For my thesis this year, I've planned to investigate the performance comparisons between different structures of ANNs. While nothing is set in stone yet, I plan to be investigating feed forward networks. 

I've been reading about hyper-parameters such width, depth, activation functions, biases and so on. What I have yet to encounter is networks that don't fit these confines. 

For example, a network with one less neuron in each hidden layer than in the last, or a network with non-homogeneous activation functions. 

I'm yet to encounter anything about these sorts of alternate layouts, although I'm unsure which terminology to search. 

Any help would be much appreciated. ",10,0,False,self,,,,,
1107,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,13,arthcp,self.MachineLearning,Model to predict depression symptoms using twitter,https://www.reddit.com/r/MachineLearning/comments/arthcp/model_to_predict_depression_symptoms_using_twitter/,vinicius978,1550465911,[removed],0,1,False,self,,,,,
1108,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,15,aru3ip,self.MachineLearning,Using ML to accelerate Material Science,https://www.reddit.com/r/MachineLearning/comments/aru3ip/using_ml_to_accelerate_material_science/,brereddit,1550470275,[removed],0,1,False,self,,,,,
1109,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,16,arukpk,reddit.com,paper clip making machine cost,https://www.reddit.com/r/MachineLearning/comments/arukpk/paper_clip_making_machine_cost/,candidstationery,1550473976,,0,1,False,default,,,,,
1110,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,16,arul4d,self.MachineLearning,[D] Any subsequent work on Neural Turing Machine (NTM and DNC) ?!,https://www.reddit.com/r/MachineLearning/comments/arul4d/d_any_subsequent_work_on_neural_turing_machine/,so_tiredso_tired,1550474068,I recall the learning rate was very slow. I wonder if there has been subsequent work.,8,31,False,self,,,,,
1111,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,16,aruodx,self.MachineLearning,North African Summer School in ML (NASSMA),https://www.reddit.com/r/MachineLearning/comments/aruodx/north_african_summer_school_in_ml_nassma/,saadoune2018,1550474819,"Hi all!

UM6P university in Morocco is organizing a first edition of the North African Summer School in ML (NASSMA). It is intended for academics, PhD students, engineers, with the possibility to present a poster. There are also (limited) possibilities for scholarships.

It is an opportunity for networking, engaging collaborations and of course learning about ML from very interesting Lecturers.
",0,2,False,self,,,,,
1112,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,16,arupne,self.MachineLearning,Need a piece of advice about IT sales,https://www.reddit.com/r/MachineLearning/comments/arupne/need_a_piece_of_advice_about_it_sales/,nattie985,1550475090,[removed],0,1,False,self,,,,,
1113,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,16,aruusx,self.MachineLearning,OMG THESE MACHINES ARE EPIC,https://www.reddit.com/r/MachineLearning/comments/aruusx/omg_these_machines_are_epic/,Junaidshah2900,1550476351,[removed],0,1,False,self,,,,,
1114,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,17,arv22u,self.MachineLearning,[P] Clustering Pollock,https://www.reddit.com/r/MachineLearning/comments/arv22u/p_clustering_pollock/,travellingsalesman2,1550478146,"Hi all,

I applied kmeans clustering to some of the Pollock's paintings. The idea was to track the artist's usage of #colors through the years. Here's the outcome!

I had really good fun in mixing computer science and art. I used Python with the standard data science stack (pandas, numpy, scikitlearn) plus opencv. echarts for the visualizations at the end of the article .

Let me know what you think!

https://medium.com/@andrea.ialenti/clustering-pollock-1ec24c9cf447",31,113,False,self,,,,,
1115,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,17,arv5zd,rubikscode.net,Ultimate Guide to Machine Learning with ML.NET,https://www.reddit.com/r/MachineLearning/comments/arv5zd/ultimate_guide_to_machine_learning_with_mlnet/,RubiksCodeNMZ,1550479151,,0,1,False,https://a.thumbs.redditmedia.com/XxZYWcJ2UR7nfd2SI0yeNs0K42to6y7K6Pgzgdq70p0.jpg,,,,,
1116,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,18,arvcfv,stationerymachine.com,wax crayons making machine,https://www.reddit.com/r/MachineLearning/comments/arvcfv/wax_crayons_making_machine/,candidstationery,1550480790,,0,0,False,https://b.thumbs.redditmedia.com/sNjiVexZ7gJqmxun3wSnWMX_m05-oQD_oJoxdwztfaI.jpg,,,,,
1117,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,18,arvhxi,eleks.com,The Dos and Don'ts of Machine Learning in Finance,https://www.reddit.com/r/MachineLearning/comments/arvhxi/the_dos_and_donts_of_machine_learning_in_finance/,Victor_Stakh,1550482123,,0,1,False,default,,,,,
1118,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,18,arvjgb,self.MachineLearning,How to approach node/graph classification in an event?,https://www.reddit.com/r/MachineLearning/comments/arvjgb/how_to_approach_nodegraph_classification_in_an/,SousVent,1550482462,[removed],0,1,False,self,,,,,
1119,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,18,arvmjo,self.MachineLearning,Gtx 1050 ti being faster than 1060 (upgrade problem),https://www.reddit.com/r/MachineLearning/comments/arvmjo/gtx_1050_ti_being_faster_than_1060_upgrade_problem/,ASHu21998,1550483205,[removed],0,1,False,self,,,,,
1120,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,19,arvq4r,self.MachineLearning,Highway Layers Vs. Residual Connections,https://www.reddit.com/r/MachineLearning/comments/arvq4r/highway_layers_vs_residual_connections/,rahulbhalley,1550484100,[removed],0,1,False,self,,,,,
1121,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,19,arvrer,analytixlabs.co.in,Best Online Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/arvrer/best_online_machine_learning_course/,SunilAhujaa,1550484410,,0,1,False,default,,,,,
1122,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,19,arvsz2,gg.gg,Loving woman looking for well-groomed man with a good heart and humor,https://www.reddit.com/r/MachineLearning/comments/arvsz2/loving_woman_looking_for_wellgroomed_man_with_a/,Ewers123,1550484772,,0,1,False,default,,,,,
1123,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,20,arw5hx,georgedatascience.com,Google Launches Reinforcement Learning Frameworks to Train AI Models,https://www.reddit.com/r/MachineLearning/comments/arw5hx/google_launches_reinforcement_learning_frameworks/,georgedatascience,1550487899,,0,1,False,https://b.thumbs.redditmedia.com/pDr-ddO8V4uoa_M2Eob5T0QljjTXY3asoPF3h6gjISg.jpg,,,,,
1124,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,20,arwebb,self.MachineLearning,Do you have an AI startup? We've got great news!,https://www.reddit.com/r/MachineLearning/comments/arwebb/do_you_have_an_ai_startup_weve_got_great_news/,cryptimi,1550489953,[removed],0,1,False,self,,,,,
1125,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,20,arwi5g,self.MachineLearning,learnml.online - GPU-accelerated computing Beta testing.,https://www.reddit.com/r/MachineLearning/comments/arwi5g/learnmlonline_gpuaccelerated_computing_beta/,learnml-online,1550490870,[removed],6,0,False,self,,,,,
1126,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,20,arwit6,self.MachineLearning,Faster ML training,https://www.reddit.com/r/MachineLearning/comments/arwit6/faster_ml_training/,inaccel,1550491028,[removed],0,1,False,https://a.thumbs.redditmedia.com/BkFOyBFQvz7VlbkYhjfirET_HWAT8blTbMF_CGCCC98.jpg,,,,,
1127,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,21,arwkym,nflsport10.tk,Stephen Hawking: Visionary physicist dies aged 76,https://www.reddit.com/r/MachineLearning/comments/arwkym/stephen_hawking_visionary_physicist_dies_aged_76/,fitnees,1550491475,,0,1,False,https://b.thumbs.redditmedia.com/1voJdH2J9cj2DfnSGY6rAJP4XJt_B-fW1CGpJsWv31M.jpg,,,,,
1128,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,21,arwn68,self.MachineLearning,Beginner trying to learn ML,https://www.reddit.com/r/MachineLearning/comments/arwn68/beginner_trying_to_learn_ml/,iAmRutWIZ,1550491987,[removed],0,1,False,self,,,,,
1129,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,21,arwxzt,self.MachineLearning,Strong laptop for AI and support linux out of the box,https://www.reddit.com/r/MachineLearning/comments/arwxzt/strong_laptop_for_ai_and_support_linux_out_of_the/,hesham_khalil,1550494389,[removed],0,1,False,self,,,,,
1130,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,22,arx2hu,self.MachineLearning,How should I transition from pure mathematics to machine learning engineering?,https://www.reddit.com/r/MachineLearning/comments/arx2hu/how_should_i_transition_from_pure_mathematics_to/,LtComDippy,1550495329,[removed],0,1,False,self,,,,,
1131,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,22,arx5my,t.me,Telegram group for data science related articles,https://www.reddit.com/r/MachineLearning/comments/arx5my/telegram_group_for_data_science_related_articles/,blackbird9820,1550495960,,0,1,False,default,,,,,
1132,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,22,arxbbm,self.MachineLearning,"Datasets for sports matches(NBA, football, cricket and hockey)",https://www.reddit.com/r/MachineLearning/comments/arxbbm/datasets_for_sports_matchesnba_football_cricket/,pranav_rs,1550497106,Does anyone know where I can get data for sports matches? Preferably in .csv . ,0,1,False,self,,,,,
1133,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,22,arxfje,self.MachineLearning,[P] Public APIs for cyrillic detection in image,https://www.reddit.com/r/MachineLearning/comments/arxfje/p_public_apis_for_cyrillic_detection_in_image/,MiksLus,1550497951,"Hi, I am working on a project that would work with images containing russian text and was wondering if there are any publicly available image recognition APIs that support extraction of cyrillic text.

I started to research AWS's Amazon Rekognition API but stumbled upon the fact that it tries to convert cyrillic to latin and that is not what I need in this project.",2,1,False,self,,,,,
1134,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,22,arxg84,engineersays.com,Drill Machine and Types of Drill Machine. Fully Explained,https://www.reddit.com/r/MachineLearning/comments/arxg84/drill_machine_and_types_of_drill_machine_fully/,sanatariq,1550498082,,0,1,False,default,,,,,
1135,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,23,arxibf,self.MachineLearning,Regarding Andrew Ng's course and Octave,https://www.reddit.com/r/MachineLearning/comments/arxibf/regarding_andrew_ngs_course_and_octave/,5arg,1550498491,[removed],0,1,False,self,,,,,
1136,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,23,arxr06,medium.com,How to Create a Sentiment Analyzer with Text Classification  Python (AI),https://www.reddit.com/r/MachineLearning/comments/arxr06/how_to_create_a_sentiment_analyzer_with_text/,Fewthp,1550500116,,0,1,False,https://b.thumbs.redditmedia.com/FNXTiOv9hQURn1spDnXlCLV0w90eJbsSIXP3mxTPpEk.jpg,,,,,
1137,MachineLearning,t5_2r3gv,2019-2-18,2019,2,18,23,ary0bq,self.MachineLearning,Self-attention - why 3 matrices for creating Q K V,https://www.reddit.com/r/MachineLearning/comments/ary0bq/selfattention_why_3_matrices_for_creating_q_k_v/,smolendawid,1550501865,[removed],0,1,False,self,,,,,
1138,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,ary4h9,self.MachineLearning,Resources to learn Deep learning and neural networks?,https://www.reddit.com/r/MachineLearning/comments/ary4h9/resources_to_learn_deep_learning_and_neural/,Sanchez_C_137,1550502594,[removed],0,1,False,self,,,,,
1139,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,arybyh,self.MachineLearning,Question about double submissions to non-archival workshops,https://www.reddit.com/r/MachineLearning/comments/arybyh/question_about_double_submissions_to_nonarchival/,fuqmebaby,1550503875,[removed],0,1,False,self,,,,,
1140,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,arydmc,self.MachineLearning,Starter project for TensorFlow.js in TypeScript for Node.js,https://www.reddit.com/r/MachineLearning/comments/arydmc/starter_project_for_tensorflowjs_in_typescript/,type-tinker,1550504133,[removed],0,1,False,self,,,,,
1141,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,aryggi,self.MachineLearning,How to choose the convenient dataset for a chatbot,https://www.reddit.com/r/MachineLearning/comments/aryggi/how_to_choose_the_convenient_dataset_for_a_chatbot/,Haithamha,1550504589,[removed],0,1,False,self,,,,,
1142,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,aryiku,self.MachineLearning,Creating a TTS (synthetic voice) based on voice-recordings from many different people,https://www.reddit.com/r/MachineLearning/comments/aryiku/creating_a_tts_synthetic_voice_based_on/,yolandasquatpump,1550504928,[removed],0,1,False,self,,,,,
1143,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,aryjif,self.MachineLearning,[D] Alternatives to Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/aryjif/d_alternatives_to_scikitlearn/,valentincalomme,1550505088,"Whilst there are many well-respected deep learning libraries (Pytorch, Chainer, Tensorflow, Caffe2, etc.) competing with each other, scikit-learn seems to be the undisputed champion when it comes to classical machine learning. Obviously, scikit-learn has its qualities, it offers a wide array of implementations and is widely used and supported. However, I definitely know it isn't perfect and I don't want to be using it blindly when there might be better alternatives out there. From my research, I've only come across statsmodel as a potential alternative though it doesn't cover as many algorithms as scikit-learn.

&amp;#x200B;

Are there alternatives to scikit-learn that I should know about? And perhaps more interestingly, what parts of scikit-learn should be challenged? I for one am not a big fan of the way they implemented their linear models and would love to see a cleaner alternative.",106,158,False,self,,,,,
1144,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,arykge,self.AskStatistics,Computing P(error) for Naive Bayes Classifier,https://www.reddit.com/r/MachineLearning/comments/arykge/computing_perror_for_naive_bayes_classifier/,ConfusedNoobie,1550505246,,0,1,False,default,,,,,
1145,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,0,aryl7d,self.MachineLearning,Online Incremental Learning,https://www.reddit.com/r/MachineLearning/comments/aryl7d/online_incremental_learning/,eray_ee,1550505369,[removed],0,2,False,self,,,,,
1146,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,1,arynmj,self.MachineLearning,My ann reduces error by making the weights and outputs really small.,https://www.reddit.com/r/MachineLearning/comments/arynmj/my_ann_reduces_error_by_making_the_weights_and/,Inline_6ix,1550505746,"I've been making my network from scratch and I wont post the code yet because its quite long and convoluted but basically all my outputs decrease over time and become roughly the same value. For example, they end up looking like \[0.12, 0.13, 0.122, 0.121, 0.123, 0.199\] for any input. Is this indicative of a specific problem? or could it be anything. 

&amp;#x200B;

My data set is very small, my prof only gave us about 80 data points.  I've tried different numbers of hidden layers and they mostly gave similar results but what I'm using right now is 9 - 5 - 6. (Input - hidden - output). I've also tried different learning rates and momentums. Right now I think my momentum is 1 and LR is 0.1.",0,1,False,self,,,,,
1147,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,1,arz6jo,guessthatshit.com,Can you identify AI generated faces?,https://www.reddit.com/r/MachineLearning/comments/arz6jo/can_you_identify_ai_generated_faces/,Shoru,1550508641,,0,1,False,default,,,,,
1148,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,1,arz87s,self.MachineLearning,[D] Are you able to submit to non-archival workshops already published work?,https://www.reddit.com/r/MachineLearning/comments/arz87s/d_are_you_able_to_submit_to_nonarchival_workshops/,fuqmebaby,1550508896,"From what I understand, being accepted to a non-archival workshop means that you're still able to submit a similar paper to another venue. But what about if you already have a publication at an obscure conference, are you allowed to submit again to a much larger non-archival workshop?",1,2,False,self,,,,,
1149,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,2,arzctd,guessthatshit.com,Can you identify AI generated faces?,https://www.reddit.com/r/MachineLearning/comments/arzctd/can_you_identify_ai_generated_faces/,Shoru,1550509577,,0,1,False,https://b.thumbs.redditmedia.com/sgGcTj6FtnEH6eT8Lr7bhu9lBdWGrh1qDxQ6n5vrdxw.jpg,,,,,
1150,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,2,arzdcv,self.MachineLearning,how do i move my q_target= to the mainloop in DDPG?,https://www.reddit.com/r/MachineLearning/comments/arzdcv/how_do_i_move_my_q_target_to_the_mainloop_in_ddpg/,Jandevries101,1550509655,[removed],0,1,False,self,,,,,
1151,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,2,arzdia,self.MachineLearning,I don't know where to start with reading others codes,https://www.reddit.com/r/MachineLearning/comments/arzdia/i_dont_know_where_to_start_with_reading_others/,0oneTwo3Four,1550509679,[removed],0,1,False,self,,,,,
1152,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,2,arzdxg,self.MachineLearning,[P] Where is the natural gradient in variational inference libraries?,https://www.reddit.com/r/MachineLearning/comments/arzdxg/p_where_is_the_natural_gradient_in_variational/,trivialfis,1550509742,"I started reading about bayesian models learning, specifically [stochastic variational inference](http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf) ,  and other related papers like [stochastic variational inference for hidden markov models](https://papers.nips.cc/paper/5560-stochastic-variational-inference-for-hidden-markov-models.pdf) .  They both uses natural gradient descent with exponential distributions to do optimization.   But when I tried to find general implementation about SVI, I searched repositories including Pyro, Pymc3, adward and didn't find any mention of words like ""natural parameter"", ""sufficient statistic"".  How does these general probabilistic programming libraries implement SVI without calculating natural parameters / sufficient statistic from exponential distributions?",4,7,False,self,,,,,
1153,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,2,arziln,self.MachineLearning,How to create a tool using python nltk to detect abuse of women on twitter?,https://www.reddit.com/r/MachineLearning/comments/arziln/how_to_create_a_tool_using_python_nltk_to_detect/,code_bot,1550510445,[removed],0,1,False,self,,,,,
1154,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,2,arzwqu,self.MachineLearning,[D] Using machine learning to identify idea zero,https://www.reddit.com/r/MachineLearning/comments/arzwqu/d_using_machine_learning_to_identify_idea_zero/,imthenachoman,1550512576,"I got to thinking about the idea of original thought. Even though the human race is young, we still have quite a bit of documented, and digitized, history including thoughts, ideas, books, movies, etc...

Often times one idea leads to another which leads to another. And if you had the information you could probably trace an idea back to idea zero. Take the 4th installment of a movie that resulted from a spin-off of another series -- idea zero would be the very first movie that started the chain reaction. Better yet, maybe we could trace idea zero back to a book or something else that, through a series of events, lead to the first movie.

I thought this might be something great for machine learning. Feeding it all the data we can to track down idea zero. Feed it books, movie scripts, etc. and let it digest the data to see if it can identify a pattern.

Curious what the community thinks of this idea?

&amp;#x200B;

Full disclosure: I am no where in/near the ML field -- I am more a customer of tools that use ML to solve cyber-security problems.",8,0,False,self,,,,,
1155,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,3,as0543,self.MachineLearning,Collaborative Filtering Android,https://www.reddit.com/r/MachineLearning/comments/as0543/collaborative_filtering_android/,arjybarji,1550513798,[removed],0,1,False,self,,,,,
1156,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,3,as05ym,self.MachineLearning,Github for ML / ImageNet wiki: MLDHub.com,https://www.reddit.com/r/MachineLearning/comments/as05ym/github_for_ml_imagenet_wiki_mldhubcom/,SteveCoast_,1550513916,"I've been working on a community site where anyone can upload and tag images in a wiki-like way, and then ML models are automatically built, that anyone can download - [mldhub.com](https://mldhub.com)

&amp;#x200B;

It's kind of clunky right now but works. It's just tagging, no segmentation yet. And it's just images not text or audio. The roadmap includes things like:

&amp;#x200B;

* auto-suggesting tags
* Caffe, CNTK and other model generation / download
* Surfacing low confidence images to users to fix
* Better navigation
* Better uploading experience

&amp;#x200B;

The idea is to lower the barrier substantially so anyone can make a model and pull it in to their project, and, let people work on the long tail - to make models for all kinds of random things.

&amp;#x200B;

Feedback welcome!",0,1,False,self,,,,,
1157,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,3,as0903,self.MachineLearning,Need help for my concept in supervised learning,https://www.reddit.com/r/MachineLearning/comments/as0903/need_help_for_my_concept_in_supervised_learning/,MLUser2018,1550514357,[removed],0,1,False,self,,,,,
1158,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,3,as0bpz,machinelearningplus.com,[P] [New Tutorial] ARIMA Model  Simplified Guide to Time Series Forecasting in Python (with Full Code),https://www.reddit.com/r/MachineLearning/comments/as0bpz/p_new_tutorial_arima_model_simplified_guide_to/,selva86,1550514734,,0,1,False,default,,,,,
1159,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,3,as0ktp,self.MachineLearning,[D] Thoughts for my concept in supervised learning,https://www.reddit.com/r/MachineLearning/comments/as0ktp/d_thoughts_for_my_concept_in_supervised_learning/,MLUser2018,1550516069,"Hello,

&amp;#x200B;

I am trying to apply supervised learning on data like the following, with a target ""Revenue"" and the five labels ""Very high"", ""High"", ""Middle"", ""Low"" and ""Very low"".

&amp;#x200B;

https://i.redd.it/f0vhu6gqidh21.png

I am using the scikit-learn library and could not get a higher accuracy than 0.46 with the estimator KNeighborsClassifier(). The end goal will be a prediction for a revenue with the above-noted features, but with higher accuracy. I have applied a label encoding, removed outliers and tried different classifiers, unfortunately without success. I would be thankful for any hints.

&amp;#x200B;

Yours sincerelly",12,0,False,https://b.thumbs.redditmedia.com/t6hPz4JlUZIIeFqp-gatXrjmE-yEyx4Ohqc7I9fGgbY.jpg,,,,,
1160,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,4,as0sek,farmingfirst.org,How AI and machine learning can transform agriculture and help tackle hunger,https://www.reddit.com/r/MachineLearning/comments/as0sek/how_ai_and_machine_learning_can_transform/,Charlie_Ensor,1550517164,,0,1,False,default,,,,,
1161,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,4,as1882,[p] https,A Complete Introduction to Reinforcement Learning and the Effect of Varying Parameters,https://www.reddit.com/r/MachineLearning/comments/as1882/a_complete_introduction_to_reinforcement_learning/,osbornep,1550519496,,0,1,False,default,,,,,
1162,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,4,as18po,kaggle.com,[P] A Complete Introduction to Reinforcement Learning and the Impact of Varying Parameters,https://www.reddit.com/r/MachineLearning/comments/as18po/p_a_complete_introduction_to_reinforcement/,osbornep,1550519569,,0,1,False,default,,,,,
1163,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,4,as1ai0,self.MachineLearning,Conditional Generative Adversarial Networks - Further conditions like pixel values,https://www.reddit.com/r/MachineLearning/comments/as1ai0/conditional_generative_adversarial_networks/,Fjjkordsfv,1550519835,[removed],0,1,False,self,,,,,
1164,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,4,as1ano,self.MachineLearning,[P] Looking Deeply into a Keras LSTM,https://www.reddit.com/r/MachineLearning/comments/as1ano/p_looking_deeply_into_a_keras_lstm/,bigschott,1550519858,"I'm working on a classification project of English language novels. I am working with a corpus of 50 novels which have a total of 4689007 words, 69230 of which are unique.

I carried out feature engineering and supervised classification with Decision Trees and an SVM and got pretty good performance, so I wanted to try my hand at unsupervised learning using keras in Python. I figured the robustness of my corpus would lend itself well to unsupervised learning.

Now, an LSTM is perfectly capable of correctly classifying each of my novels into my two sets of interest: ""lyrical"" and ""detective"" fictions, respectively. But I am not all that interested in the LSTM just making that binary choice. I want to be able to look inside the network at what specific \*words\* drive the classification of each novel. Is there a way to do this?

I tried following along with this stackOverflow post:

[https://stackoverflow.com/questions/51477977/highlighting-important-words-in-a-sentence-using-deep-learning](https://stackoverflow.com/questions/51477977/highlighting-important-words-in-a-sentence-using-deep-learning)

But the output I'm getting isn't the importance for each ""word"" in my texts, rather (what I can only assume is) an importance level for each text itself.

i.e. when I run model.predict(text\_1, text\_2) I'm handed a number back for each text.  It's interesting that text\_1 is considered to be more or less lyrical than text\_2, sure. But I'm really after the words themselves that drive that decision. 

I would to hear any ideas about how to make this happen -- for example, is there a call I can make while the network is training to write every word's ""importance"" rating into a file?

Here is my work thusfar - 

[https://github.com/timschott/dmp/blob/master/Python\_Scripts/LSTM/LSTM.py](https://github.com/timschott/dmp/blob/master/Python_Scripts/LSTM/LSTM.py)

Feel free to tool around in my code. At the moment I'm only using a few books for the sake of my computer's processor; if I can get the correct output working on a small scale I can send my code onto my school's high performance computing cluster to properly carry this out.",9,9,False,self,,,,,
1165,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,5,as1o7f,stackchief.com,Promises or Observables?,https://www.reddit.com/r/MachineLearning/comments/as1o7f/promises_or_observables/,stackchief,1550521840,,0,0,False,https://b.thumbs.redditmedia.com/qAYjhCPohfQgL69r8KJ88ffiwk0xoZeP7HvMmT0IEJA.jpg,,,,,
1166,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as217x,self.MachineLearning,"[D] Are there any neural networks that turn your drawn 2D stick figure poses into actual 2D character poses, or something similar?",https://www.reddit.com/r/MachineLearning/comments/as217x/d_are_there_any_neural_networks_that_turn_your/,PaleInsect,1550523788,"Ideal workflow: You design a detailed character.

You then draw stick figures in various poses. The program responds by inserting the detailed character in those drawn poses, filling in the blanks via neural networking/machine learning.

Does something like this exist?",4,4,False,self,,,,,
1167,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as25az,self.MachineLearning,"Oriol Vinyals at Boston University, AlphaStar: Mastering the Real-Time Strategy Game StarCraft II",https://www.reddit.com/r/MachineLearning/comments/as25az/oriol_vinyals_at_boston_university_alphastar/,MICommunity,1550524402,[removed],0,1,False,self,,,,,
1168,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as26af,self.MachineLearning,Can Amazon Sagemaker be used to scale out and train millions of classifier models?,https://www.reddit.com/r/MachineLearning/comments/as26af/can_amazon_sagemaker_be_used_to_scale_out_and/,031val,1550524551,[removed],0,1,False,self,,,,,
1169,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as2ci5,self.MachineLearning,"Oriol Vinyals at Boston University, AlphaStar: Mastering the Real-Time Strategy Game StarCraft II",https://www.reddit.com/r/MachineLearning/comments/as2ci5/oriol_vinyals_at_boston_university_alphastar/,MICommunity,1550525474,"[BU Artificial Intelligence Research](https://www.bu.edu/hic/air/) will be hosting [Oriol Vinyals](https://www.reddit.com/user/OriolVinyals).

Date: March 11th, 2019 11 AMLocation: Boston University (specific building and room TBA. I'll update this post when I get more info.)

TITLE:AlphaStar: Mastering the Real-Time Strategy Game StarCraft II

ABSTRACT: Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a grand challenge for AI research.In this talk, I will introduce our StarCraft II program AlphaStar, the first Artificial Intelligence to defeat a top professional player. In a series of test matches held on 19 December, AlphaStar decisively beat Team Liquids Grzegorz ""MaNa"" Komincz, one of the worlds strongest professional StarCraft players, 5-1, following a successful benchmark match against his team-mate Dario TLO Wnsch. The matches took place under professional match conditions on a competitive ladder map and without any game restrictions.

BIO: Oriol Vinyals is a Sr Staff Research Scientist at Google DeepMind, working in Deep Learningand Artificial Intelligence. Prior to joining DeepMind, Oriol was part of the Google Brain team. He holds a Ph.D. in EECS from the University of California, Berkeley and is a recipient of the 2016 MIT TR35 innovator award. His research has been featured multiple times at the New York Times, BBC, etc., and his articles have been cited over 36000 times. Some of his contributions are used inGoogle Translate, Text-To-Speech, and Speech recognition, serving billions of queries every day, and he was the lead researcher of the AlphaStar project, creating an agent that defeated a top professional at the game of StarCraft. At DeepMind he continues working on his areas of interest, which include artificial intelligence, with particular emphasis on machine learning, deep learning and reinforcement learning.

Come thru!",0,1,False,self,,,,,
1170,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as2e4o,github.com,A face recognition in browser with JavaScript and neural networks.,https://www.reddit.com/r/MachineLearning/comments/as2e4o/a_face_recognition_in_browser_with_javascript_and/,atum47,1550525714,,0,1,False,https://b.thumbs.redditmedia.com/6KlOuvw0jqSKuSjFlh8qn7FTiJ0ShxByVdXl8aqyNms.jpg,,,,,
1171,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as2hxz,self.MachineLearning,Oriol Vinyals AlphaStar: Mastering the Real-Time Strategy Game StarCraft II at Boston University,https://www.reddit.com/r/MachineLearning/comments/as2hxz/oriol_vinyals_alphastar_mastering_the_realtime/,MICommunity,1550526309,[removed],0,1,False,self,,,,,
1172,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as2i1e,github.com,[P] A face recognition in browser with JavaScript and neural networks.,https://www.reddit.com/r/MachineLearning/comments/as2i1e/p_a_face_recognition_in_browser_with_javascript/,atum47,1550526323,,0,1,False,https://b.thumbs.redditmedia.com/6KlOuvw0jqSKuSjFlh8qn7FTiJ0ShxByVdXl8aqyNms.jpg,,,,,
1173,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,6,as2i9t,self.MachineLearning,Making Art with AI,https://www.reddit.com/r/MachineLearning/comments/as2i9t/making_art_with_ai/,aidocumentarian,1550526364,[removed],0,1,False,self,,,,,
1174,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,7,as2xd4,self.MachineLearning,[Problem] Calculate the weight vector and the bias after a single gradient descent update given the learning rate and gradients and return the new weights and bias,https://www.reddit.com/r/MachineLearning/comments/as2xd4/problem_calculate_the_weight_vector_and_the_bias/,itsmegeorge,1550528693,[removed],0,1,False,self,,,,,
1175,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,7,as2yhj,self.MachineLearning,[R] On Evaluating Adversarial Robustness,https://www.reddit.com/r/MachineLearning/comments/as2yhj/r_on_evaluating_adversarial_robustness/,anishathalye,1550528868,"A few of us (Nicholas Carlini, me, Nicholas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry) got together to summarize the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses against adversarial examples. The resulting document, hosted on GitHub, is meant to be a living document to which we hope many will contribute their ideas and suggestions. We hope that it will be useful to those designing their own defenses, to those reviewing defense papers and to those just wondering what goes into a defense evaluation.

**Abstract**: Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.

We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.

**Link**: https://github.com/evaluating-adversarial-robustness/adv-eval-paper/blob/master/paper.pdf

(arXiv version will be available shortly; we'll add a comment when it's available)",12,60,False,self,,,,,
1176,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,7,as325b,self.MachineLearning,New ML framework for Swift,https://www.reddit.com/r/MachineLearning/comments/as325b/new_ml_framework_for_swift/,sylvan_m,1550529444,[removed],0,1,False,self,,,,,
1177,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,8,as3k2y,self.MachineLearning,[D] Youtube's recommendation system is curating content for pedophiles.,https://www.reddit.com/r/MachineLearning/comments/as3k2y/d_youtubes_recommendation_system_is_curating/,antiquechrono,1550532405,"I'm not going to post the video as I don't know if it breaks the subs rules but it is NSFW, NSFL, and deeply disturbing.

If you missed the post on the front page this morning Youtube's recommendation algorithm has linked together thousands of videos that pedophiles are sharing of children. Most of the videos themselves are simply children being children but they are being posted with the express purpose of sexualizing them. The comments are sexual and tend to link to specific timestamps of the kids in revealing poses. Most troubling is that youtube is basically acting as a social network for pedophiles to find each other, communicate, and share links to actual illegal content. Some of the videos are posted innocently by the children themselves and the pedophiles are trying to encourage them to post more explicit videos.

I'd like to start a discussion on big tech companies refusing to take responsibility for the disastrous side effects of employing machine learning in the search of profit. Who should be held responsible here if anyone? I for one am getting sick of tech companies that make billions basically shrugging and saying there's nothing they can do about it when they have the resources and the talent to deal with the problem.

The thing that's the most infuriating is that we all know nothing is going to come from this. All the sick sexual videos targeted at children are still on the platform from the elsagate debacle and I'm sure the pedo ring on youtube will still exist a year from now as well. There's simply no monetary gain in fighting this crap. I'd love to know how much Google spent on Alpha Star and how much they spend removing illegal content on their platforms. What gets me is that Google's own algorithms are identifying and linking this content together but they mysteriously can't do anything about it.

I'm really not sure what the solution here is but I know that it's not inaction and apathy. 

* Should we start licensing engineers and holding them accountable? 
* Should we pass laws that hold these platforms accountable? 
* Should journals blacklist research from unethical companies? 
* If you are a researcher or an engineer at one of these companies how would you feel if something you worked on was used to exploit children or otherwise cause harm?",2,0,False,self,,,,,
1178,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,8,as3qyb,self.MachineLearning,Stereo image classification &amp; deep learning,https://www.reddit.com/r/MachineLearning/comments/as3qyb/stereo_image_classification_deep_learning/,kur054ki,1550533519,"Hello guys , just been wondering how can I combine stereo vision and convolutional neural networks for classification problems ?",0,1,False,self,,,,,
1179,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,8,as3r44,self.MachineLearning,[N] $1M Unearthed - Explorer challenge - Machine Learning and Geology,https://www.reddit.com/r/MachineLearning/comments/as3r44/n_1m_unearthed_explorer_challenge_machine/,wassname,1550533542,"There's [been some interest](https://old.reddit.com/r/MachineLearning/comments/a52nfy/d_does_ml_and_geology_match_heres_a_1m_competition/) in the Explorer Challenge which is a 1 million dollar competition combining machine learning and geology to come up with the best prospect.

A [funny video](https://youtu.be/fN1FeCZ75Hc) and [detailed slides](https://drive.google.com/open?id=1x7qsfqXItF5A-kNwjgXFbAdnmplxea1G) have been released. I was also at the presentation in Perth, Australia so feel free to clarify things with me if the slides aren't clear.

This is an interesting competition because geology seems like it could be disrupted by the application of ML, but it's also challenging because of the large amount of contextual and qualitative data that goes into making decisions.

",13,52,False,self,,,,,
1180,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,9,as4ckb,self.MachineLearning,[P] Analyzing Recurrent Neural Networks (RNNs) Using Polymer Dynamics Theory,https://www.reddit.com/r/MachineLearning/comments/as4ckb/p_analyzing_recurrent_neural_networks_rnns_using/,hagy,1550537272,"Hi all,

&amp;#x200B;

I'm learning RNN theory and as a project, I tried to better understand the dynamics of LSTM elements when applied to input strings by relating the dynamics to concepts I'm more familiar with in chemical nonequilibrium statistical mechanics. Identified some interesting behavior in terms of the relatively smaller impact of terminal \`&lt;pad&gt;\` characters on the element dynamics versus other characters which cause large changes in the element values. Details in the linked blog post.

&amp;#x200B;

Assume this behavior is well known, but I wasn't able to find a publication that demonstrates this behavior. Would appreciate learning about prior related work that I should be citing. Would also appreciate any critique of the work.

&amp;#x200B;

Thanks!

[https://medium.com/@matthagy/analyzing-recurrent-neural-networks-rnns-using-chemical-dynamics-theory-d777a182bd6d](https://medium.com/@matthagy/analyzing-recurrent-neural-networks-rnns-using-chemical-dynamics-theory-d777a182bd6d)",14,96,False,self,,,,,
1181,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,10,as4gwv,self.MachineLearning,"[D] Is ""AI &amp; Big Data Expo Europe"" interesting for researchers?",https://www.reddit.com/r/MachineLearning/comments/as4gwv/d_is_ai_big_data_expo_europe_interesting_for/,michalgregor,1550538024,"Hi, has anybody been to the ""AI &amp; Big Data Expo Europe"" before? [https://www.ai-expo.net/europe/](https://www.ai-expo.net/europe/)

I was wondering whether this would be interesting from a researchers' perspective  given that vast majority of the speakers seem to be from the industry. What is the intended audience? Is it worth going, do you think?",2,0,False,self,,,,,
1182,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,10,as4jeq,self.MachineLearning,What projects at Google/Facebook are trying to develop a new programming language for Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/as4jeq/what_projects_at_googlefacebook_are_trying_to/,QinLu,1550538463,"On the article https://venturebeat.com/2019/02/18/facebooks-chief-ai-scientist-deep-learning-may-need-a-new-programming-language/, Yan LeCun says:

There are several projects at Google, Facebook, and other places to kind of design such a compiled language that can be efficient for deep learning, but its not clear at all that the community will follow, because people just want to use Python,

What are these projects?",0,1,False,self,,,,,
1183,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,10,as4jik,self.MachineLearning,Is this a viable ML project?,https://www.reddit.com/r/MachineLearning/comments/as4jik/is_this_a_viable_ml_project/,scun1995,1550538487,[removed],0,1,False,self,,,,,
1184,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,10,as4na7,self.MachineLearning,"Oriol Vinyals Talk at Boston University, AlphaStar: Mastering the Real-Time Strategy Game StarCraft II",https://www.reddit.com/r/MachineLearning/comments/as4na7/oriol_vinyals_talk_at_boston_university_alphastar/,ch3njus,1550539074,[removed],0,1,False,self,,,,,
1185,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,10,as4qto,twitter.com,Oriol Vinyals presenting AlphaStar at Boston University AIR,https://www.reddit.com/r/MachineLearning/comments/as4qto/oriol_vinyals_presenting_alphastar_at_boston/,ch3njus,1550539713,,0,1,False,default,,,,,
1186,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,10,as4v22,self.MachineLearning,How to optimize my device's computing capabilities for deep learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/as4v22/how_to_optimize_my_devices_computing_capabilities/,daitranskku,1550540497,"Dear,

&amp;#x200B;

How to optimize my device's computing capabilities for deep learning algorithms?

Can anyone recommend some methods???

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/772g38zpifh21.png

https://i.redd.it/z3dvozupifh21.png

https://i.redd.it/q24nh0vpifh21.png

https://i.redd.it/q7tfx1vpifh21.png",0,1,False,self,,,,,
1187,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,12,as61q9,self.MachineLearning,[D] Best GPU that doesn't require external power,https://www.reddit.com/r/MachineLearning/comments/as61q9/d_best_gpu_that_doesnt_require_external_power/,ggelango,1550548122,Is there a x16 Pcie 3.0 GPU that doesn't require any external power. Preferably an Nvidia GPU. I am intending on passing through this GPU into a Proxmox VM running Tensorflow.,3,0,False,self,,,,,
1188,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,12,as626p,self.MachineLearning,[P] Can I get a peer review of this paragraph in my essay?,https://www.reddit.com/r/MachineLearning/comments/as626p/p_can_i_get_a_peer_review_of_this_paragraph_in_my/,CrazyCar09,1550548207,"I am new to deep learning and want to make sure I am representing it correctly and explaining it well. Thanks a million!

&amp;#x200B;

 Jeff Dean, a PhD graduate from University of Washington, draws a picture of deep learning, stating, When you hear the term deep learning, just think of a large deep neural net. Deep refers to the number of layers typically and so this is kind of the popular term thats been adopted in the press. I think of them as deep neural networks generally. Neural net is short for the term neural network, a computer diagram that allows for deep learning to take place. This diagram is inspired by a human brain. At the start of the network is an input. This input leads into a set of layers called hidden layers. These layers are where the change occurs. From the hidden layer, an output is produced. This all functions like a human brain. A human brain is composed of neurons, which are composed of dendrites and one axon. When one dendrite is prompted, it will trigger the axon. This axon might trigger another separate neuron and this continues until an output is given (Brownlee). Using advanced algorithms, as well as the chain rule, computers can create their own outputs. These algorithms give each input a weight; generally this weight is around 1 for small networks, a large network might be given a number in the 100s to produce a more accurate output. Once the input leaves, it is transferred to a hidden layer. Based on the programs algorithm and function, this newly arrived input will be given a new weight, if it is very close to the desired output then the weight remains close to the original weight. The farther from the output, the lower the weight. This process repeats for the amount of hidden layers there are (Cosmos). After it gets to the final stage of hidden layers, the highest number is chosen to be the output. This process leads into the first implementations of deep learning. ",5,0,False,self,,,,,
1189,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,13,as68sl,arxiv.org,"[D] Code Request: GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations If someone has the code, please provide the link",https://www.reddit.com/r/MachineLearning/comments/as68sl/d_code_request_glomo_unsupervisedly_learned/,schrodingershit,1550549461,,4,26,False,default,,,,,
1190,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,13,as6cj0,self.MachineLearning,How important is a Mac laptop for deep / machine learning?,https://www.reddit.com/r/MachineLearning/comments/as6cj0/how_important_is_a_mac_laptop_for_deep_machine/,tritonEYE,1550550209,[removed],0,1,False,self,,,,,
1191,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,13,as6n7k,self.MachineLearning,[P] 14 million URLs for OpenWebText (OpenAI WebText Clone),https://www.reddit.com/r/MachineLearning/comments/as6n7k/p_14_million_urls_for_openwebtext_openai_webtext/,joshuacpeterson,1550552328,[removed],19,44,False,self,,,,,
1192,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,14,as6nn9,self.MachineLearning,[P] A PyTorch implementation of Neural Machine Translation (NMT): Along with intrinsic/extrinsic evaluation,https://www.reddit.com/r/MachineLearning/comments/as6nn9/p_a_pytorch_implementation_of_neural_machine/,lyeoni,1550552414,"Hello :)

&amp;#x200B;

The official PyTorch tutorial provides the end-to-end translation example, and the exercise part at the end of official document encourage us to try a variety of different methods ourselves. 

e.g. adopt pre-trained word representaion, deeper and compledx model, and check the presence of the attention.

&amp;#x200B;

So, I implemented and tested what the PyTorch official document gave us as an assignment. In addition, I did extrinsic evaluation by estimating BLEU, as well as intrinsic evaluation by translating.

&amp;#x200B;

I archived all I have implemented for neural machine translation in below repo.

I hope that this repo can be of help to follow PyTorch website's translation tutorial.

&amp;#x200B;

Github: [https://github.com/lyeoni/nlp-tutorial/tree/master/neural-machine-translation](https://github.com/lyeoni/nlp-tutorial/tree/master/neural-machine-translation)

&amp;#x200B;",0,1,False,self,,,,,
1193,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,14,as6t43,self.MachineLearning,[D] Synchronous SGD with Momentum?,https://www.reddit.com/r/MachineLearning/comments/as6t43/d_synchronous_sgd_with_momentum/,idlemang,1550553514,"Hi, often I have seen that SGD can apply Momentum? I am wondering if Synchronous SGD can apply Momentum as well? More importantly, is there any theoretical results related to this?  If this question is dumb... please forgive me about me new to ML...thanks a lot...",2,3,False,self,,,,,
1194,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,15,as794p,i.redd.it,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/as794p/global_machine_learning_market_size_outlook/,vardhan1020,1550556831,,0,1,False,default,,,,,
1195,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,15,as7j0e,slideserve.com,ML KIT &amp; Its Feature Highlights,https://www.reddit.com/r/MachineLearning/comments/as7j0e/ml_kit_its_feature_highlights/,jamielannisters,1550559000,,0,1,False,default,,,,,
1196,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,15,as7l2q,self.MachineLearning,Can you use 3 CNNs to attain better accuracy?,https://www.reddit.com/r/MachineLearning/comments/as7l2q/can_you_use_3_cnns_to_attain_better_accuracy/,Umtiza,1550559478,[removed],0,1,False,self,,,,,
1197,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,17,as88mf,arxiv.org,[R] Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity,https://www.reddit.com/r/MachineLearning/comments/as88mf/r_learning_to_control_selfassembling_morphologies/,hardmaru,1550565409,,2,8,False,default,,,,,
1198,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,17,as88xp,self.MachineLearning,[P] This AirBnB does not exist,https://www.reddit.com/r/MachineLearning/comments/as88xp/p_this_airbnb_does_not_exist/,invertedpassion,1550565487,"Similar to thispersondoesnotexist.com, I just came across this project by someone: [https://thisairbnbdoesnotexist.com/](https://thisairbnbdoesnotexist.com/)",52,240,False,self,,,,,
1199,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,20,as986q,self.MachineLearning,"[P] Speech Synthesis based on a multi-person corpus (Non-singular gender, age and geographic origin)",https://www.reddit.com/r/MachineLearning/comments/as986q/p_speech_synthesis_based_on_a_multiperson_corpus/,yolandasquatpump,1550574208,"Hey!

&amp;#x200B;

As a part of a project, we want to do experiments with synthetic voices where these do not have a singular geographic origin, body, age or gender. We have our own data-set, but I thought of during initial experiments with VCTK and build a voice using Tacotron2 or something similar. Does anyone know if a similar project has been done? Where the physical body that we imagine connected to a voice is intentionally ambiguous. Or other projects where TTS has be trained on a multi-person corpus? Additionally, does anyone know of any caveats or potential problems in terms of this approach? Maybe there could be ways of working with transfer-learning that could be beneficial.

Thanks!",8,5,False,self,,,,,
1200,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,20,as9aqs,kaggle.com,[P] Reinforcement Learning from Scratch: Applying Model-free Methods and Evaluating Parameters in Detail,https://www.reddit.com/r/MachineLearning/comments/as9aqs/p_reinforcement_learning_from_scratch_applying/,osbornep,1550574792,,0,1,False,default,,,,,
1201,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,20,as9cww,self.MachineLearning,viral dna,https://www.reddit.com/r/MachineLearning/comments/as9cww/viral_dna/,information-seeker,1550575288,[removed],0,1,False,self,,,,,
1202,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,20,as9h5l,self.MachineLearning,Running a collaborative filtering algorithm on a server for use in an android application,https://www.reddit.com/r/MachineLearning/comments/as9h5l/running_a_collaborative_filtering_algorithm_on_a/,arjybarji,1550576260,[removed],0,1,False,self,,,,,
1203,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,20,as9jm8,github.com,"[P] awesome-doesnotexist Repository, for all the existing hype around existing generators",https://www.reddit.com/r/MachineLearning/comments/as9jm8/p_awesomedoesnotexist_repository_for_all_the/,paubric,1550576803,,0,1,False,default,,,,,
1204,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,20,as9kd7,self.MachineLearning,[P] Pytorch-based classification experiments template (github),https://www.reddit.com/r/MachineLearning/comments/as9kd7/p_pytorchbased_classification_experiments/,learning-luke,1550576973,"In the spirit of cooperation and sharing, I've made my ""pytorch experiments template"" repo neater and public: 

[https://github.com/learning-luke/pytorch-experiments-template](https://github.com/learning-luke/pytorch-experiments-template)

I find it pretty useful to have a working base from which to experiment with classification style deep learning problems and, after many renditions, this feels pretty smooth. I usually start a new project as a new git repo and then copy repo's contents into it (minus license and readme, that is).

There's a notebook in there that plots things pretty-like, and also combines seeded results to give error bars. I've really tried to make this as bare-bones as I could, yet still have the functionality that I find myself exploring for many experiments.

Enjoy.",1,1,False,self,,,,,
1205,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,21,as9tsh,self.MachineLearning,Need some advice for bounding box regression.,https://www.reddit.com/r/MachineLearning/comments/as9tsh/need_some_advice_for_bounding_box_regression/,mischief_23,1550578885,[removed],0,1,False,self,,,,,
1206,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,21,as9v2y,self.MachineLearning,How valid is it to change learning rate proportional to gradients while training a classifier?,https://www.reddit.com/r/MachineLearning/comments/as9v2y/how_valid_is_it_to_change_learning_rate/,nik38,1550579154,[removed],0,1,False,self,,,,,
1207,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,21,asa1z0,self.MachineLearning,Knowledge distillation from BERT Large to a BERT base model architecture?,https://www.reddit.com/r/MachineLearning/comments/asa1z0/knowledge_distillation_from_bert_large_to_a_bert/,yzyy,1550580528,[removed],1,1,False,self,,,,,
1208,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,22,asaat5,self.MachineLearning,[D] On IJCAI-19 Submissions,https://www.reddit.com/r/MachineLearning/comments/asaat5/d_on_ijcai19_submissions/,redlow0992,1550582213,"I was wondering if someone that submitted a paper to IJCAI last year can help out. With more than 20 hours remaining to the deadline, our paper got an ID of high 4000s. Was it like this last year too?

Even for ICML the number of submissions was not this high. Is this normal? ",11,12,False,self,,,,,
1209,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,22,asagb2,self.MachineLearning,[Discussion] Attention mechanism on time series data,https://www.reddit.com/r/MachineLearning/comments/asagb2/discussion_attention_mechanism_on_time_series_data/,thtonmoy,1550583247,I am new to the field and I did not have much success searching. I would appreciate if anyone could point to some interesting works on usage of attention on time series data.  ,3,2,False,self,,,,,
1210,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,22,asaged,self.MachineLearning,[D] Which neural net optimizer for a dataset with highly random outcome,https://www.reddit.com/r/MachineLearning/comments/asaged/d_which_neural_net_optimizer_for_a_dataset_with/,Destruct1,1550583262,"Based on the course reinforcement learning by David Silva I tried to create an AI for a card game. The objective of the game is to maximize the average amount of points each game over a series of games. The points are only scored at the very end of each game. Most of silvas method work well but I have a problem with the randomness.

&amp;#x200B;

The training data created by self-play is highly random. For the same features x the y value can vary between 0 and 30. y depends both on the objective game state (x at the start or the middle) AND on the random card draws AND on decisions in the later part of the game. The real value would be the average of many games with the same starting point.

Additionally the ""true"" y value changes over time. As the neural net learns to play better in the later part of the game the evaluation of the middle game position changes. At the start of training nearly all early game positions are worth close to 0. Later on they get to 6-45 (depending on the cards already played).

&amp;#x200B;

If I use the adam or adagrad optimizer the learning starts well. The average value per game climbs fast. But after a while the strategy is just swinging back and forth. Depending if the last couple of games were sucessful with strategy A/B/C the neural net bounces between them and never settles into a stable middle.

&amp;#x200B;

I see three solutions:

a) Change something technical. Examples are the amount of games played before creating the training set or changing the learning rate (although both adam and adagrad discourage this)

&amp;#x200B;

b) Use a different optimizer. I looked into different optimizer but they seem like black magic to me. What optimizer would you recommend?

&amp;#x200B;

c) Program a search ahead or average value. I could play the same game with the same starting cards a 100 times and average the outcome of these 100 games. This would flatten the randomness. But this approach feels unprincipled. Instead of playing 100 games and then taking the average it seems better to give the net all 100 examples with wildly different outcomes so that it can draw the right conclusions itsself.",0,0,False,self,,,,,
1211,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,22,asaogm,venturebeat.com,Deep learning may need a new programming language,https://www.reddit.com/r/MachineLearning/comments/asaogm/deep_learning_may_need_a_new_programming_language/,j_orshman,1550584736,,0,1,False,https://b.thumbs.redditmedia.com/sHQInUUmoF82KuwGjaOVYngjdD8OlLtypeZZcgNVSEI.jpg,,,,,
1212,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,23,asavsi,self.MachineLearning,RL tutorial without all of the indices?,https://www.reddit.com/r/MachineLearning/comments/asavsi/rl_tutorial_without_all_of_the_indices/,ice109,1550585996,[removed],0,1,False,self,,,,,
1213,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,23,asb0ld,self.MachineLearning,Hekp needed on Machine Learning Pipelines in production,https://www.reddit.com/r/MachineLearning/comments/asb0ld/hekp_needed_on_machine_learning_pipelines_in/,isaurav,1550586829,"Hi guys,

I am preparing for my first Data Scientist interview next month and would like to understand how Machine Learning Models are actually deployed on production. 

I have pretty comfortable with Predictive Analytics and Modeling part but have no idea how these models are actually used in production environment.  Is it used with Kafka, Flask API, or something else?

I googled about various Machine learning pipelines but couldn't find any relevant demo project.

If you guys have worked or cone across any such pipeline, please let me know.

Thanks for your help! ",0,1,False,self,,,,,
1214,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,23,asb4el,self.MachineLearning,Help needed on Machine Learning Pipelines in production,https://www.reddit.com/r/MachineLearning/comments/asb4el/help_needed_on_machine_learning_pipelines_in/,isaurav,1550587492,[removed],0,1,False,self,,,,,
1215,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,23,asb5vq,self.MachineLearning,Need help with the selection of the ML algorithm for my project,https://www.reddit.com/r/MachineLearning/comments/asb5vq/need_help_with_the_selection_of_the_ml_algorithm/,Troied,1550587756,[removed],0,1,False,self,,,,,
1216,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,23,asb61g,self.MachineLearning,[P] 8-bit VAE: MusicVAE on NES Music.,https://www.reddit.com/r/MachineLearning/comments/asb61g/p_8bit_vae_musicvae_on_nes_music/,TheRedSphinx,1550587782,"Some time back, I stumbled upon Magenta's awesome work, MusicVAE([https://magenta.tensorflow.org/music-vae](https://magenta.tensorflow.org/music-vae)) and thought was pretty neat. Then, I stumbled upon the NES Music Database ([https://github.com/chrisdonahue/nesmdb](https://github.com/chrisdonahue/nesmdb)) from Chris Donahue and I thought, ""yo, this is awesome! What if we used this with MusicVAE???"" At the time, I didn't have enough of a background or time to really pursue this project, but I finally found some time lately to delve into it again.

&amp;#x200B;

 It took some data wrangling and messing around, but I finally got decent results. Notice that the original samples which are interpolated are somewhere between 0-2 seconds long. Despite this, the results sound surprisingly cohesive...or maybe that's just my bias haha. Have a listen!

&amp;#x200B;

[https://soundcloud.com/xavier-garcia-958359339/sample-b](https://soundcloud.com/xavier-garcia-958359339/sample-b)

&amp;#x200B;

[https://soundcloud.com/xavier-garcia-958359339/sample-a](https://soundcloud.com/xavier-garcia-958359339/sample-a)

&amp;#x200B;

I used the same first sample in both songs, but chose different ending samples. You can find the code at [https://github.com/xgarcia238/8bit-VAE](https://github.com/xgarcia238/8bit-VAE). Feedback and suggestions are welcome!

&amp;#x200B;

&amp;#x200B;",1,21,False,self,,,,,
1217,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,23,asb7p7,self.MachineLearning,[Project] Available pre-trained neural translator models?,https://www.reddit.com/r/MachineLearning/comments/asb7p7/project_available_pretrained_neural_translator/,soepjongen,1550588066," 

For my current project I would like to play around with the internal representations learned by a neural machine translator (English to whatever language). However, a pre-trained model for python is hard to come by. Training one myself is not really an option, since my hardware available and time is limited.

Does someone have any suggestions?",3,0,False,self,,,,,
1218,MachineLearning,t5_2r3gv,2019-2-19,2019,2,19,23,asb93t,self.MachineLearning,The Power of AI Generated Stories,https://www.reddit.com/r/MachineLearning/comments/asb93t/the_power_of_ai_generated_stories/,00hello,1550588305,[removed],0,0,False,self,,,,,
1219,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,0,asbdrj,self.MachineLearning,Guide on Genetic Algorithms,https://www.reddit.com/r/MachineLearning/comments/asbdrj/guide_on_genetic_algorithms/,rish-16,1550589041,"Hey,

If you're interested in Reinforcement Learning, specifically Genetic Algorithms, go check out this post:

[https://medium.com/sigmoid/https-medium-com-rishabh-anand-on-the-origin-of-genetic-algorithms-fc927d2e11e0](https://medium.com/sigmoid/https-medium-com-rishabh-anand-on-the-origin-of-genetic-algorithms-fc927d2e11e0)

Sure to get you up and going with coding your own GA algorithms to complete interesting tasks",0,1,False,self,,,,,
1220,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,0,asbi9d,theappsolutions.com,The Definitive Guide to Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/asbi9d/the_definitive_guide_to_pattern_recognition/,lady_monsoon,1550589779,,0,1,False,default,,,,,
1221,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,0,asbmt5,marti.ai,[R] Hong Kong Machine Learning Meetup Season 1 Episode 7,https://www.reddit.com/r/MachineLearning/comments/asbmt5/r_hong_kong_machine_learning_meetup_season_1/,gau_mar,1550590498,,0,1,False,https://b.thumbs.redditmedia.com/6WAmsBK_AIItkSCtOGiVf2LfzRpXQVtOgb7toez1_zg.jpg,,,,,
1222,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,1,asc0y1,self.MachineLearning,Anyone else working on GPT2?,https://www.reddit.com/r/MachineLearning/comments/asc0y1/anyone_else_working_on_gpt2/,ASPNetthrow,1550592689,[removed],0,1,False,self,,,,,
1223,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,1,asc2kd,self.MachineLearning,Advanced deep learning program with Python (Design advice),https://www.reddit.com/r/MachineLearning/comments/asc2kd/advanced_deep_learning_program_with_python_design/,Matty5565,1550592940,[removed],0,1,False,self,,,,,
1224,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,1,asc3eq,github.com,[P] Huskarl: Deep Reinforcement Learning Framework (for TensorFlow 2.0),https://www.reddit.com/r/MachineLearning/comments/asc3eq/p_huskarl_deep_reinforcement_learning_framework/,danaugrs,1550593067,,0,1,False,default,,,,,
1225,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,1,asc971,self.MachineLearning,[P] A PyTorch implementation of Neural Machine Translation (NMT): Along with intrinsic/extrinsic evaluation.,https://www.reddit.com/r/MachineLearning/comments/asc971/p_a_pytorch_implementation_of_neural_machine/,lyeoni,1550593949,"Hello :)

&amp;#x200B;

The official PyTorch tutorial provides the end-to-end translation example, and the exercise part at the end of official document encourage us to try a variety of different methods ourselves. (e.g. adopt pre-trained word representaion, deeper and compledx model, and check the presence of the attention.)

&amp;#x200B;

So, I implemented and tested what the PyTorch official document gave us as an assignment. In addition, I did extrinsic evaluation by estimating BLEU, as well as intrinsic evaluation by translating.

&amp;#x200B;

I archived all I have implemented for neural machine translation in below repo. I hope that this repo can be of help to follow PyTorch website's translation tutorial.

Github: [https://github.com/lyeoni/nlp-tutorial/tree/master/neural-machine-translation](https://github.com/lyeoni/nlp-tutorial/tree/master/neural-machine-translation)",0,0,False,self,,,,,
1226,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,ascknp,self.MachineLearning,[R] TDLS - A Style-Based Generator Architecture for Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/ascknp/r_tdls_a_stylebased_generator_architecture_for/,tdls_to,1550595678,"* Part 1, algorithmic review: [https://www.youtube.com/watch?v=SPI5uGCnxlc](https://www.youtube.com/watch?v=SPI5uGCnxlc)
* Part 2, results &amp; discussions:[https://www.youtube.com/watch?v=\_bh3U9HB-kg](https://www.youtube.com/watch?v=_bh3U9HB-kg)

  
Slides:[https://tdls.a-i.science/static/slides/20190124\_DiegoCantor.pdf](https://tdls.a-i.science/static/slides/20190124_DiegoCantor.pdf)  


Contributions:

1. Significant improvement over traditional GAN generators architecture
2. Separation of high-level attributes from stochastic effects
3. Does not generate new images from scratch but rather through a smart combination of styles that are embedded in sample images (latent codes)

Discussions:

* Though it became apparent later when the source code was released, at the time of discussion there was some confusion around where the source style image came from in the style mixing section. Some of the group speculated whether latent Z was used as input for source style images.

For more details, visit the event page at [https://tdls.a-i.science/events/2019-01-24/](https://tdls.a-i.science/events/2019-01-24/).",7,14,False,self,,,,,
1227,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,ascns9,thegradient.pub,[D] Dear OpenAI: Please Open Source Your Language Model,https://www.reddit.com/r/MachineLearning/comments/ascns9/d_dear_openai_please_open_source_your_language/,hughbzhang,1550596104,,0,1,False,default,,,,,
1228,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,ascp7z,self.MachineLearning,[D] Dear OpenAI: Please Open Source Your Language Model,https://www.reddit.com/r/MachineLearning/comments/ascp7z/d_dear_openai_please_open_source_your_language/,hughbzhang,1550596314,"Newest Gradient perspective on OpenAI not open sourcing the GPT-2. [https://thegradient.pub/openai-please-open-source-your-language-model/](https://thegradient.pub/openai-please-open-source-your-language-model/)

&amp;#x200B;

Since I'm the author on this one, happy to respond to any comments / criticism of the article!",74,155,False,self,,,,,
1229,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,ascw2k,self.MachineLearning,[P] Huskarl: Parallelized Deep Reinforcement Learning Framework (for TensorFlow 2.0),https://www.reddit.com/r/MachineLearning/comments/ascw2k/p_huskarl_parallelized_deep_reinforcement/,danaugrs,1550597304,"https://i.redd.it/spvwnelz5kh21.png

Hi guys, I'm working on an open-source deep reinforcement learning framework called [Huskarl](https://github.com/danaugrs/huskarl).

It's based on TensorFlow 2.0 which should come out very soon.

Huskarl makes it easy to parallelize (or not) multi-environment training, which is important for efficient training of on-policy algorithms such as A2C and PPO which benefit from multiple sources of experience.

[OpenAI Gym](https://gym.openai.com/) environments are supported out-of-the-box. There are plans to support [Unity3D environments](https://unity3d.com/machine-learning/) and the goal is to ultimately support generalized multi-agent environments with any number of ""presences"" per agent (e.g. a simple self-play environment would have 1 agent and 2 presences of that same agent).

I'm very satisfied with the architecture so far.

Algorithms implemented to date:

* Deep Q-Learning Network (DQN)
* Multi-step DQN
* Double DQN
* Dueling Architecture DQN
* Advantage Actor-Critic (A2C)

Algorithms planned: 

* Deep Deterministic Policy Gradient (DDPG)
* Proximal Policy Optimization (PPO)
* Prioritized Experience Replay
* Curiosity-Driven Exploration

Let me know what you think! Also let me know any other algorithms you would like to see implemented.",5,27,False,https://b.thumbs.redditmedia.com/laJbhHfxkmW56k4GNOJgAlp-XZD86CA6TedT4t0GzxU.jpg,,,,,
1230,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,asczd5,github.com,Tensorflow2.0 - Examples,https://www.reddit.com/r/MachineLearning/comments/asczd5/tensorflow20_examples/,thibo73800,1550597751,,0,1,False,default,,,,,
1231,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,asd0cc,jclinepi.com,[R] Systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models,https://www.reddit.com/r/MachineLearning/comments/asd0cc/r_systematic_review_shows_no_performance_benefit/,ossicones,1550597884,,0,1,False,default,,,,,
1232,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,asd1es,self.MachineLearning,CALLING ALL MACHINE LEARNING COURSE BETA TESTERS,https://www.reddit.com/r/MachineLearning/comments/asd1es/calling_all_machine_learning_course_beta_testers/,Ltarshis,1550598037,[removed],0,1,False,self,,,,,
1233,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,2,asd4it,self.MachineLearning,[ReInforcement Learning] Are there any alternatives to greedy-epsilon?,https://www.reddit.com/r/MachineLearning/comments/asd4it/reinforcement_learning_are_there_any_alternatives/,burnsca,1550598471,"when using a standard decay rate with greedy epsilon it is perfectly linear so that you explore early and exploit later.  This seems like it could create opportunities for inefficiencies, especially given Experience replay.  There is probably an earlier point at which an RL agent would be better rewarded by exploiting but the greedy epsilon rate still trends toward exploring.  I am curious to know if any have tested alternative options, what where they, how did they perform, how did you measure the increased optimization?  TIA!",0,1,False,self,,,,,
1234,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,3,asdu8g,self.MachineLearning,Network Analysis for Fake News Detection,https://www.reddit.com/r/MachineLearning/comments/asdu8g/network_analysis_for_fake_news_detection/,_brian_,1550602126,[removed],0,1,False,self,,,,,
1235,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,4,asdzjv,futurism.com,I guess the tables are turning.,https://www.reddit.com/r/MachineLearning/comments/asdzjv/i_guess_the_tables_are_turning/,asiamnesis,1550602868,,0,1,False,default,,,,,
1236,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,4,ase3jm,self.MachineLearning,[P] Can I use a model trained in TensorflowJS in Tensorflow for python?,https://www.reddit.com/r/MachineLearning/comments/ase3jm/p_can_i_use_a_model_trained_in_tensorflowjs_in/,G3ru1a,1550603436,"Hi, I'm  working on a project where I have to recognize emotions from a video feedand I have trained a model in tensorflow js (because I'm  not that good with python) and now I need to run the model in python where I get and process the video feed. Is there any way to do that? Online i just found how to run models from python to js but not the other way around. Thanks.",1,1,False,self,,,,,
1237,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,4,ase4dr,self.MachineLearning,When to use pretrained embedding vs when to train them from scratch.,https://www.reddit.com/r/MachineLearning/comments/ase4dr/when_to_use_pretrained_embedding_vs_when_to_train/,acobobby,1550603551,"I am not able to find a good discussion about this question (neither on this subreddit, nor on the web), so feel free to link me some relevant sources.

In your experience when it is convenient to use pretrained embeddings (like Glove, FastText...)?  
When it is instead more convenient to train an Embedding layer from scratch?

&amp;#x200B;

I think that for a very general task, in which very little assumption on the data distribution can be made, a pretrained embedding is the way to go, as it is trained on large corpora and obtaining better results on a general environment would require a great effort.

If the task is domain-specific maybe to train embeddings from scratch would result in more accurate relationships between words than the pretrained ones, as the latter does not assume any particular main ""topic"".

&amp;#x200B;

Do you want to add something more? Do you disagree with these observations? Let's discuss!  
",0,1,False,self,,,,,
1238,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,4,ase4ez,self.MachineLearning,[P] Network Analysis for Fake News Detection,https://www.reddit.com/r/MachineLearning/comments/ase4ez/p_network_analysis_for_fake_news_detection/,_brian_,1550603555,"Hi yall! I recently completed a project using twitter ego network analysis and machine learning to detect fake news.

Here is an article where i walkthrough my analysis: [https://towardsdatascience.com/ego-network-analysis-for-the-detection-of-fake-news-da6b2dfc7c7e](https://towardsdatascience.com/ego-network-analysis-for-the-detection-of-fake-news-da6b2dfc7c7e)

Ultimately, my results had a lot more to do with the word embeddings from profile descriptions than the network features, but very interested to hear your thoughts about using network analysis for this sort of thing",2,13,False,self,,,,,
1239,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,4,ase4sh,self.MachineLearning,Categorization of tax entries,https://www.reddit.com/r/MachineLearning/comments/ase4sh/categorization_of_tax_entries/,vreten,1550603610,"Is there a ML function that I can use in sheets to categorize my bank entries?  

So for example text 
Debit Card Purchase - LYFT RIDE WED 1PM 85521234 CA
Debit Card Purchase - DREAMHOST DH FEE COM CA

For the Lyft entry it would categorize it as ""Travel"". 
For the Dreamhost entry it would categorize it as ""Travel"".
And perhaps a score on the confidence.  

I have everything in sheets but can use excel, python etc. 

This is quite taxing to go through this manually. 


",0,1,False,self,,,,,
1240,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,aseo3u,self.MachineLearning,(D) Training strategies for LSTM,https://www.reddit.com/r/MachineLearning/comments/aseo3u/d_training_strategies_for_lstm/,Fluxing_Capacitor,1550606457,[removed],0,1,False,self,,,,,
1241,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,asepp9,self.MachineLearning,How to approach the sequential decision-making problem with sub-actions with RL?,https://www.reddit.com/r/MachineLearning/comments/asepp9/how_to_approach_the_sequential_decisionmaking/,wqinc,1550606693," 

I have been trying to solve a sequential decision-making problem that involves taking ""sub-actions"". I would like to share my experience and really appreciate any suggestions / insights.

Suppose we model it as a Traveling salesman problem (TSP) that has 5 nodes (however, there's no need to go back to the starting node), and at each node, we also have to decide what to do over 3 possible sub-actions.

For example, one possible path is:

(Node 2, Action 0)  (Node 4, Action 2)  (Node 3, Action 1)  (Node 1, Action 0)  (Node 0, Action 2)

We cannot get intermediate rewards, and a final reward is returned at the completion of the sequence. 

Methods I have tried:

1. Genetic algorithm (GA): this method works well overall, but not that attractive as a piece of research work
2. Monte Carlo Tree Search (MCTS): I implemented the vanilla version of MCTS and set the random rollout to 10k, it surpasses the GA, but with a tradeoff of 10X computation time
3. I went on to evaluate the cross-entropy method (CEM) by modeling the sequence generation as a Markov chain and sub-action as an independent Bernoulli distribution, but the results are rather poor

Several future directions I can think of now:

* Speed up the MCTS, perhaps by pruning or parallel processing
* Methods like SeqGAN, but was wondering how to deal with the sub-actions (how to generate tuples)
* Try TD approach by assigning 0-reward for the intermediate states and only evaluate the final state

Would really love to hear your opinions and advice! 

Cheers!",0,1,False,self,,,,,
1242,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,ases19,self.MachineLearning,[D] What is the difference?,https://www.reddit.com/r/MachineLearning/comments/ases19/d_what_is_the_difference/,Unlistedd,1550607025,"What is the difference between a nonparametric Bayesian model and a hidden markov model?

Is the hidden markov model an extanded version of the nonparametric Bayesian model? Or are they not related at all?",0,0,False,self,,,,,
1243,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,aseylm,spreadshare.co,3+ Machine Learning Courses to Consider in 2019 - Stream on SpreadShare,https://www.reddit.com/r/MachineLearning/comments/aseylm/3_machine_learning_courses_to_consider_in_2019/,skj8,1550608001,,0,1,False,default,,,,,
1244,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,asf481,self.MachineLearning,MADDPG with RNN?,https://www.reddit.com/r/MachineLearning/comments/asf481/maddpg_with_rnn/,MuhahaSmile,1550608836,[removed],0,1,False,self,,,,,
1245,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,asf65r,self.MachineLearning,Extracting text from few areas on product label,https://www.reddit.com/r/MachineLearning/comments/asf65r/extracting_text_from_few_areas_on_product_label/,cloudares,1550609115,"Hi, I'm trying to achieve algorithm that will extract text from few areas(marked with red color) on label(similar to attached image) and QR code on a single photo.  I'm struggling with choosing and executing good enough solution to accomplish it. I was wondering about labeling images with marked boxes around my area of intersts and then put it into some CNN.  I've encounter some interesting paper on text spotting([https://arxiv.org/pdf/1810.12738v1.pdf](https://arxiv.org/pdf/1810.12738v1.pdf)) but I'm not sure if it would be good solution for my problem. 

I will be more than thankful for any help and suggestions. Thanks in advance :)",0,1,False,self,,,,,
1246,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,asf91q,self.MachineLearning,[D] REDDIT: A one word reason why I support OpenAIs GPT-2 decision,https://www.reddit.com/r/MachineLearning/comments/asf91q/d_reddit_a_one_word_reason_why_i_support_openais/,VinayUPrabhu,1550609528,"Link: [https://medium.com/@VinayPrabhu/reddit-a-one-word-reason-why-i-support-openais-gpt-2-decision-56b82912443c](https://medium.com/@VinayPrabhu/reddit-a-one-word-reason-why-i-support-openais-gpt-2-decision-56b82912443c)

I am the author and welcome comments / criticisms alike.",8,0,False,self,,,,,
1247,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,asf9gs,self.MachineLearning,Publishing paper with no affiliation?,https://www.reddit.com/r/MachineLearning/comments/asf9gs/publishing_paper_with_no_affiliation/,chkips,1550609586,[removed],0,1,False,self,,,,,
1248,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,5,asfam5,self.MachineLearning,[D] Automatic differentiation is easy,https://www.reddit.com/r/MachineLearning/comments/asfam5/d_automatic_differentiation_is_easy/,p_bogdan,1550609768,An introductory post about AD [penkovsky.com/post/neural-networks-3/](https://penkovsky.com/post/neural-networks-3/),0,0,False,self,,,,,
1249,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,6,asfnsi,arxiv.org,[R] Perturbative GAN: GAN with Perturbation Layers,https://www.reddit.com/r/MachineLearning/comments/asfnsi/r_perturbative_gan_gan_with_perturbation_layers/,FirstTimeResearcher,1550611693,,0,1,False,default,,,,,
1250,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,6,asfp1r,self.MachineLearning,How close are we to speech models that are virtually indifferentiable from human voice?,https://www.reddit.com/r/MachineLearning/comments/asfp1r/how_close_are_we_to_speech_models_that_are/,actualsnek,1550611890,[removed],0,1,False,self,,,,,
1251,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,6,asfunt,lorenlugosch.github.io,An introduction to sequence-to-sequence learning,https://www.reddit.com/r/MachineLearning/comments/asfunt/an_introduction_to_sequencetosequence_learning/,m_nemo_syne,1550612724,,1,1,False,default,,,,,
1252,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,7,asg9ia,syncedreview.com,Facebook Open-Sources Improved Go Bot and Huge Game Library,https://www.reddit.com/r/MachineLearning/comments/asg9ia/facebook_opensources_improved_go_bot_and_huge/,Yuqing7,1550614965,,0,1,False,https://b.thumbs.redditmedia.com/I85Mx2O8oIQw-3m-9yhatnRGt6oA9EmCO01ac1c9wMk.jpg,,,,,
1253,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,7,asgaq4,self.MachineLearning,Huge difference between training/testing accuracies,https://www.reddit.com/r/MachineLearning/comments/asgaq4/huge_difference_between_trainingtesting_accuracies/,agonq,1550615150,"I'm working in a kind of a sentiment classification (binary) task. Using google's pre-trained word2vec vectors for the embedding layer (tried other word vectors as well) and 2 Convolutional layers for the model (both appended by MaxPooling and Dropout layers). On top of that, there's a fully-connected (Dense) layer and the network ends with an output layer. Will attach a diagram of my model:

&amp;#x200B;

&amp;#x200B;

[My model](https://i.redd.it/qhf679icplh21.png)

&amp;#x200B;

The model is performing very well in the training set (600,000 articles with a 50/50 distribution of labels) with an accuracy of \~93%, but performing really poor on the testing set (150,000 articles with a 50/50 distribution of labels) - accuracy: \~58%. This seems like an overfitting issue, however I've appended Dropout layers after each of my regular layers.

Another interesting fact: I combined my training and testing sets together (appended the testing set to the training set), shuffled them randomly, and then re-splitted them to training/testing in a 80-20 ratio (using sklearn's train\_test\_split  
 method), and the accuracy is much more balanced after training the model with these ""artificial"" datasets: they're both around 93%.

Also, while investigating this issue, reduced my vocabulary to only 500 and 1000 most common words (in order to make the model more general) and trained my model with these, but the accuracies didn't change.

Anyone has any idea what the issue might be with such a difference in the training/testing accuracies?",0,1,False,self,,,,,
1254,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,7,asgbh8,self.MachineLearning,[P] Training a CNN to identify certain features in scene. But what if at inference scene is totally different?,https://www.reddit.com/r/MachineLearning/comments/asgbh8/p_training_a_cnn_to_identify_certain_features_in/,vcxzzxcv,1550615265,"I'm looking for techniques that tell the neural network to ""not give any positive output"" for the case where the scene the video camera is looking at is completely different from the training data. 

For reference, I am making a project to use video to locate defects in pieces of wood. The network will run in real time on a video camera / laptop (at low-ish resolutions). The input is a 320x240x3 image, output is also 320x240x3, except it is meant to be a heatmap showing where the network thinks the defects are located. 

Example: 

https://i.imgur.com/NHIov7K.jpg

Basically I have hundreds of images like this created manually using a labelling program. The network will be trained to take the left image and generate the right image, so we have a heatmap of where wood defects will be. 

But what happens when the camera is moved about and looks at something that's not wood or not at all represented in the training set? In this case I'd like the output to be zero / all black... 

What are some training techniques, data augmentation techniques, etc, I could use to achieve this? 

One idea I had was to use an autoencoder on the images and use the autoencoded feature vector to create the output heatmap. I thought maybe this would mean that if the network sees something it's never seen before, it can't autoencode it properly, so output will be garbage (zeros hopefully). Probably not the best approach but it's one idea I had. Just don't want false positives. 

Appreciate any and all ideas, cheers =) ",6,0,False,self,,,,,
1255,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,7,asgj2p,i.redd.it,[D] Gradient Descent was invented by Louis Augustin Cauchy in 1847.,https://www.reddit.com/r/MachineLearning/comments/asgj2p/d_gradient_descent_was_invented_by_louis_augustin/,saadmrb,1550616462,,0,1,False,https://b.thumbs.redditmedia.com/dY9lu370O6-a6H88fA8u53E_jEiaDNXjhDuAZhxfiWM.jpg,,,,,
1256,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,7,asgj5t,i.redd.it,[D] Gradient Descent was invented by Louis Augustin Cauchy in 1847.,https://www.reddit.com/r/MachineLearning/comments/asgj5t/d_gradient_descent_was_invented_by_louis_augustin/,saadmrb,1550616478,,0,1,False,default,,,,,
1257,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,7,asglwd,self.MachineLearning,Are Vapnik's books a good introduction to statistical learning theory?,https://www.reddit.com/r/MachineLearning/comments/asglwd/are_vapniks_books_a_good_introduction_to/,jurniss,1550616920,[removed],0,1,False,self,,,,,
1258,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,8,asgtmp,self.MachineLearning,[D] Has anyone done a study on capacity of various NN architectures?,https://www.reddit.com/r/MachineLearning/comments/asgtmp/d_has_anyone_done_a_study_on_capacity_of_various/,achaiah777,1550618139,"Now that we've got a number of different NN architectures (e.g. Resnet, Inception, Densenet, Nasnet etc) have there been any studies as to how well they perform when you keep increasing the number of distinct classes? So instead of 1000 classes from imagenet, what if we train them with 10K classes? 100K? Which networks perform better as you keep adding classes and do they have a breaking point beyond which they stop meaningfully distinguish between classes or is it just a gradual reduction in accuracy?

I recall seeing a Resnet trained on 10K classes in one paper but other than that I haven't seen it being researched much in literature.",6,36,False,self,,,,,
1259,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,9,ashsfe,self.MachineLearning,why it seems better when GBDT uses square loss than cross entropy loss in the task of 0-1 classifying?,https://www.reddit.com/r/MachineLearning/comments/ashsfe/why_it_seems_better_when_gbdt_uses_square_loss/,frank_cao,1550623942,[removed],0,1,False,self,,,,,
1260,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,9,ashu7r,self.MachineLearning,"[D] Assuming excellent knowledge of both frameworks, what are things you can only do in Tensorflow and not in Pytorch?",https://www.reddit.com/r/MachineLearning/comments/ashu7r/d_assuming_excellent_knowledge_of_both_frameworks/,Valiox,1550624242,"I love Pytorch, but I assume it is likely that I will be asked if I know how to use Tensorflow in my first job involving deep learning.

Tensorflow definitely has a lower level feel than most deep learning libraries. My impression is that you can achieve more complex things but with less fancy abstractions. Is that generally true? What would be a practical example of something you cannot do in PyTorch (or that would be ""ugly"") but that you could do fine in Tensorflow?",68,69,False,self,,,,,
1261,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,10,asifrt,arxiv.org,[1902.06720] Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/asifrt/190206720_wide_neural_networks_of_any_depth/,jinpanZe,1550627871,,39,227,False,default,,,,,
1262,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,11,asipxr,self.MachineLearning,BigML. Oscar 2000-2018 Prediction dataset help.,https://www.reddit.com/r/MachineLearning/comments/asipxr/bigml_oscar_20002018_prediction_dataset_help/,Bulky_Astronaut,1550629535,[removed],0,1,False,self,,,,,
1263,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,12,asjfmm,self.MachineLearning,How do data scientists test the claimed accuracy of a model?,https://www.reddit.com/r/MachineLearning/comments/asjfmm/how_do_data_scientists_test_the_claimed_accuracy/,Idindunuffin69,1550633927,[removed],0,1,False,self,,,,,
1264,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,13,ask3tw,self.MachineLearning,"For a beginner to ML/AI, which is a better library: PyTorch or Tensorflow (or other libraries)",https://www.reddit.com/r/MachineLearning/comments/ask3tw/for_a_beginner_to_mlai_which_is_a_better_library/,Fengax,1550638219,[removed],0,1,False,self,,,,,
1265,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,14,ask7e2,self.MachineLearning,[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color,https://www.reddit.com/r/MachineLearning/comments/ask7e2/r_scfegan_face_editing_generative_adversarial/,run_youngjoo,1550638878,"SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color

arXiv: [https://arxiv.org/abs/1902.06838](https://arxiv.org/abs/1902.06838)

github: [https://github.com/JoYoungjoo/SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN)

Abstract

We present a novel image editing system that generates images as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.",0,1,False,self,,,,,
1266,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,14,askgkc,self.MachineLearning,[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color,https://www.reddit.com/r/MachineLearning/comments/askgkc/r_scfegan_face_editing_generative_adversarial/,run_youngjoo,1550640541,"&amp;#x200B;

https://i.redd.it/6f9fyvabonh21.jpg

&amp;#x200B;

**SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color**

arXiv: [https://arxiv.org/abs/1902.06838](https://arxiv.org/abs/1902.06838)

github: [https://github.com/JoYoungjoo/SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN)

Abstract

We present a novel image editing system that generates images as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.",1,1,False,https://b.thumbs.redditmedia.com/sVk5CWvcrKX-3tD1zkpemuGyvWQ__eunM0LuNnmY28A.jpg,,,,,
1267,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,14,askk0b,i.redd.it,[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color,https://www.reddit.com/r/MachineLearning/comments/askk0b/r_scfegan_face_editing_generative_adversarial/,run_youngjoo,1550641168,,2,1,False,default,,,,,
1268,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,14,askpim,youtu.be,"[D] Earlier this evening, Sam of This Week in Machine Learning &amp; AI hosted a panel to discuss the controversial recent release of the OpenAI GPT-2 Language Model. Great format for this discussion.",https://www.reddit.com/r/MachineLearning/comments/askpim/d_earlier_this_evening_sam_of_this_week_in/,Fatman_Johnson,1550642211,,0,1,False,https://a.thumbs.redditmedia.com/T_0oX-FHcDRg2_FsZx0gIf6PA8tn04UWm4rRW2xwUh4.jpg,,,,,
1269,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,15,askrcp,self.MachineLearning,[D] AI Safety Needs Social Scientists,https://www.reddit.com/r/MachineLearning/comments/askrcp/d_ai_safety_needs_social_scientists/,wei_jok,1550642572,"[AI Safety Needs Social Scientists](https://distill.pub/2019/safety-needs-social-scientists/)

A distill.pub article by Geoffrey Irving and Amanda Askell (both affiliated with OpenAI)

*Properly aligning advanced AI systems with human values will require resolving many uncertainties related to the psychology of human rationality, emotion, and biases. These can only be resolved empirically through experimentationif we want to train AI to do what humans want, we need to study humans.*

https://distill.pub/2019/safety-needs-social-scientists/",1,6,False,self,,,,,
1270,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,15,askvqn,i.redd.it,Artifial Intelligence in Brand Marketing,https://www.reddit.com/r/MachineLearning/comments/askvqn/artifial_intelligence_in_brand_marketing/,hiwilliam31,1550643418,,0,1,False,default,,,,,
1271,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,15,askzkj,self.MachineLearning,[P]Use two simultaneous input for my AI.,https://www.reddit.com/r/MachineLearning/comments/askzkj/puse_two_simultaneous_input_for_my_ai/,Kriegher2005,1550644187,"I want to train my ml algorithm with my heart beat and pulse rate. The data are in different format. How do input the data and make sure that they are corresponding? I mean, that point X in time of pulse rate and X in heart beat are occurring at the same time.",0,1,False,self,,,,,
1272,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,15,asl2ow,self.MachineLearning,Lorentz Embeddings Implementation,https://www.reddit.com/r/MachineLearning/comments/asl2ow/lorentz_embeddings_implementation/,arjoonn,1550644800,[removed],0,1,False,self,,,,,
1273,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,15,asl5av,self.MachineLearning,[D] Taking 2 simultaneous input for AI,https://www.reddit.com/r/MachineLearning/comments/asl5av/d_taking_2_simultaneous_input_for_ai/,Kriegher2005,1550645318,"I want to train my ml algorithm with my heart beat and pulse rate. The data are in different format. How do input the data and make sure that they are corresponding? I mean, that point X in time of pulse rate and X in heart beat are occurring at the same time.

-

As the pulse rate and heart beat happen simultaneously, I want the ml to learn how one would affect the other.",4,0,False,self,,,,,
1274,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,16,aslagc,self.Amd,Radeon VII Tensorflow Deep Learning results - Huge improvement from Vega FE,https://www.reddit.com/r/MachineLearning/comments/aslagc/radeon_vii_tensorflow_deep_learning_results_huge/,eleitl,1550646329,,0,1,False,default,,,,,
1275,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,16,aslcxe,self.MachineLearning,Deep learning hardware recommendations for startup .,https://www.reddit.com/r/MachineLearning/comments/aslcxe/deep_learning_hardware_recommendations_for_startup/,gachiemchiep,1550646848,[removed],0,1,False,self,,,,,
1276,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,16,aslgt2,self.MachineLearning,"[D] I am implementating code gpt-2 with Pytorch, but i want to help tensorflow checkpoint file to pytorch",https://www.reddit.com/r/MachineLearning/comments/aslgt2/d_i_am_implementating_code_gpt2_with_pytorch_but/,nlkey2022,1550647682,"I am implementating code gpt-2 with Pytorch, But in original gpt-2 repository, they save model to checkpoint file,

so I want to load checkpoint in tensorflow file to Pytorch.

How can I do? I think there are not good way.. when i googling.. :(

[https://github.com/graykode/gpt-2-pytorch](https://github.com/graykode/gpt-2-pytorch)",11,6,False,self,,,,,
1277,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,16,aslm9m,self.MachineLearning,[D] Using Visual and Audio Data for training,https://www.reddit.com/r/MachineLearning/comments/aslm9m/d_using_visual_and_audio_data_for_training/,Kriegher2005,1550648870,"I want to train a ml algorithm using data from my camera and microphone. The data will be occurring simultaneously.

-

In a way I want my data to hear a sound and make a visual representation of the thing causing the sound and to give a sound after getting the image of an object.",2,0,False,self,,,,,
1278,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,17,aslwqp,on.morioh.net,Top 10 Algorithms for Machine Learning Newbies,https://www.reddit.com/r/MachineLearning/comments/aslwqp/top_10_algorithms_for_machine_learning_newbies/,GokuJP123,1550651209,,0,1,False,default,,,,,
1279,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,17,aslwre,self.MachineLearning,[N] Machine Learning on Source Code with Francesc Campoy (48min talk from GOTO Copenhagen 2018),https://www.reddit.com/r/MachineLearning/comments/aslwre/n_machine_learning_on_source_code_with_francesc/,goto-con,1550651215,"* [Video](https://youtu.be/-lMVH3DtMFQ)
* [Slides](https://gotocph.com/2018/sessions/628)

&amp;#x200B;

**ABSTRACT**

source{d} is building the open-source components to enable large-scale code analysis and machine learning on source code.

Their powerful tools can ingest all of the worlds public git repositories turning code into ASTs ready for machine learning and other analyses, all exposed through a flexible and friendly API.

Francesc Campoy, VP of Developer Relations at [source{d}](https://sourced.tech/), will show you how to run machine learning on source code with a series of live demos.",0,1,False,self,,,,,
1280,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,17,aslxbx,theappsolutions.com,[Discussion]: Conversational Interfaces - The Future of UI,https://www.reddit.com/r/MachineLearning/comments/aslxbx/discussion_conversational_interfaces_the_future/,lady_monsoon,1550651345,,0,1,False,default,,,,,
1281,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,17,asm0p4,arxiv.org,[1902.06789] Seven Myths in Machine Learning Research,https://www.reddit.com/r/MachineLearning/comments/asm0p4/190206789_seven_myths_in_machine_learning_research/,statmlsn,1550652128,,8,0,False,default,,,,,
1282,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,19,asmme3,self.MachineLearning,Gaussian Process,https://www.reddit.com/r/MachineLearning/comments/asmme3/gaussian_process/,philipsolomonsee,1550657032,[removed],0,1,False,self,,,,,
1283,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,19,asmrdq,self.MachineLearning,[D] Has anyone worked with insurance claims data before?,https://www.reddit.com/r/MachineLearning/comments/asmrdq/d_has_anyone_worked_with_insurance_claims_data/,bananakiu,1550658145,I'm working on a project (semi-confidential) that involves disease prediction using claims data. Do you guys have any insights on how to go about feature engineering with claims data?,5,3,False,self,,,,,
1284,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,19,asmv8n,self.MachineLearning,[P] Extracting text from different areas on label,https://www.reddit.com/r/MachineLearning/comments/asmv8n/p_extracting_text_from_different_areas_on_label/,cloudares,1550659028,"Hi, I'm trying to achieve algorithm that will extract text from few areas(marked with red color) on label(similar to attached image) on a single photo taken with mobile camera so label may be in a little bit different position. I'm struggling with choosing and executing good enough solution to accomplish it. I was wondering about labeling images with marked boxes around my area of intersts and then put it into some CNN. I've encounter some interesting paper on text spotting([https://arxiv.org/pdf/1810.12738v1.pdf](https://arxiv.org/pdf/1810.12738v1.pdf)) but I'm not sure if it would be good solution for my problem.

I will be more than thankful for any help and suggestions. Thanks in advance :)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/lq8f05qqbph21.png",0,1,False,https://b.thumbs.redditmedia.com/VEj5XREYMxkOcNWGGcOvNAwXj08m1eX4w65R7T7y-cI.jpg,,,,,
1285,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,19,asmxb9,towardsdatascience.com,[D] Introduction to gradient boosting on decision trees with CatBoost,https://www.reddit.com/r/MachineLearning/comments/asmxb9/d_introduction_to_gradient_boosting_on_decision/,s0ulmate,1550659504,,0,3,False,default,,,,,
1286,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,19,asmxew,docs.google.com,[D] I'm doing research on the experience of training AI algorithms. If you could fill out this 2 minute survey I would really appreciate it!,https://www.reddit.com/r/MachineLearning/comments/asmxew/d_im_doing_research_on_the_experience_of_training/,potatoborn,1550659528,,0,2,False,default,,,,,
1287,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,19,asn0fx,github.com,PYBO: Natural Language Processing for Tibetan Language,https://www.reddit.com/r/MachineLearning/comments/asn0fx/pybo_natural_language_processing_for_tibetan/,mikkokotila,1550660217,,0,1,False,default,,,,,
1288,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,19,asn141,self.MachineLearning,Margin loss Capsule networks,https://www.reddit.com/r/MachineLearning/comments/asn141/margin_loss_capsule_networks/,PyWarrior,1550660373,[removed],0,1,False,self,,,,,
1289,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,20,asn2t8,ark-invest.com,On the Road to Full Autonomy With Elon Musk  FYI Podcast,https://www.reddit.com/r/MachineLearning/comments/asn2t8/on_the_road_to_full_autonomy_with_elon_musk_fyi/,mlvpj,1550660712,,0,1,False,https://b.thumbs.redditmedia.com/6KL-cRRGQxof_e-Grpeu-yj6tZJZ5Omukj37ZCpygYw.jpg,,,,,
1290,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,20,asn684,theappsolutions.com,[Discussion]: Conversational Interfaces - The Future of UI,https://www.reddit.com/r/MachineLearning/comments/asn684/discussion_conversational_interfaces_the_future/,lady_monsoon,1550661442,,0,1,False,https://a.thumbs.redditmedia.com/siPexYGTBFoksta80qT1b7RLRBZAWFrncHqLrJAGom4.jpg,,,,,
1291,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,20,asn7w8,self.MachineLearning,[R] Combining Neural Networks with Personalized PageRank for Classification on Graphs (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/asn7w8/r_combining_neural_networks_with_personalized/,benitorosenberg,1550661799,"&amp;#x200B;

https://i.redd.it/lnqa0lgrjph21.jpg

**Paper:** [https://arxiv.org/abs/1810.05997](https://arxiv.org/abs/1810.05997)

**OpenReview:** [**https://openreview.net/forum?id=H1gL-2A9Ym**](https://openreview.net/forum?id=H1gL-2A9Ym)

**PyTorch implementation:** [https://github.com/benedekrozemberczki/APPNP/](https://github.com/benedekrozemberczki/APPNP/)

**Abstract:**

Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood cannot be easily extended. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct personalized propagation of neural predictions (PPNP) and its approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification on multiple graphs in the most thorough study done so far for GCN-like models.",2,28,False,https://b.thumbs.redditmedia.com/y4vKdOX7VeyI0meoxNFrdaeql3vj-y6y13J7T89mSLQ.jpg,,,,,
1292,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,20,asngew,self.MachineLearning,Open call for participation in Data Science StackExchange,https://www.reddit.com/r/MachineLearning/comments/asngew/open_call_for_participation_in_data_science/,dantek88,1550663543,"Hi everyone. I would like to make an open call for participation in the Data Science StackExchange.

[https://datascience.stackexchange.com/](https://datascience.stackexchange.com/)

I have no affiliation with it. This is not an affiliate link. Those of you that are familiar with the stackoverflow, you can understand what a big asset is for the developers to have such a community. If we participate in Data Science StackExchange, we can do the same!

What any StackExchange needs:

1. Votes! Votes! Votes! Votes are the mean to moderate and improve the content of the site.
2. Answers. At the moment, DS has \~4700 unanswered questions. We are a community of 38.000 members. We just need to answer 0.12 questions each of us :P
3. Valid questions. If you have an awesome question (even if you know the answer), put it there. The content is the king. Another user will find it the next day and will really appreciate that someone got his back",0,1,False,self,,,,,
1293,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,21,asnxy1,self.MachineLearning,Looking for basic computer vision papers to replicate,https://www.reddit.com/r/MachineLearning/comments/asnxy1/looking_for_basic_computer_vision_papers_to/,Wide_Procedure,1550666899,[removed],0,1,False,self,,,,,
1294,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,21,asnyg7,self.MachineLearning,[D] Question about the use of the prediction problem,https://www.reddit.com/r/MachineLearning/comments/asnyg7/d_question_about_the_use_of_the_prediction_problem/,bogdan461993,1550666996,"Hi everyone! I am new to reinforcement learning and I have a question regarding the use of prediction. When is it useful, since we are interested in general in obtaining an improved policy over time? Can't we just apply the control problem all the time? ",6,1,False,self,,,,,
1295,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,22,aso8k7,self.MachineLearning,Help me (learn about you)!,https://www.reddit.com/r/MachineLearning/comments/aso8k7/help_me_learn_about_you/,SmoothMarx,1550668792,[removed],0,1,False,self,,,,,
1296,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,22,aso9b6,self.MachineLearning,[question] Brain.js get user input and answer,https://www.reddit.com/r/MachineLearning/comments/aso9b6/question_brainjs_get_user_input_and_answer/,BernardoPiedade,1550668927,[removed],0,1,False,self,,,,,
1297,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,22,aso9jz,crate.io,"CrateDB, Machine Learning, and Hydroelectric Power: Part One",https://www.reddit.com/r/MachineLearning/comments/aso9jz/cratedb_machine_learning_and_hydroelectric_power/,nachrieb,1550668968,,0,3,False,https://a.thumbs.redditmedia.com/yaGqeGUY1E24ufVpyXzHZB1L4sRJWA6hpB26MHie7B4.jpg,,,,,
1298,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,22,asocj3,self.MachineLearning,Book: Java Deep Learning Essentials,https://www.reddit.com/r/MachineLearning/comments/asocj3/book_java_deep_learning_essentials/,andrea_manero,1550669474,[removed],0,1,False,self,,,,,
1299,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,22,asofr6,self.MachineLearning,[P] A Python to Scala transpiler using neural machine translation (NMT),https://www.reddit.com/r/MachineLearning/comments/asofr6/p_a_python_to_scala_transpiler_using_neural/,hagy,1550670024,"Hi all,

&amp;#x200B;

I wanted to learn more about neural machine translation (NMT) so I created a small project to translate programming expression in the Python language into Scala. Details in [this blog post](https://medium.com/@matthagy/a-python-to-scala-transpiler-using-neural-machine-translation-nmt-90d4d02afa70). [The code is on GitHub](https://github.com/matthagy/nmt_python_scala_transpiler).

&amp;#x200B;

I recognize challenges in dealing with the recursive structure of complex, nested expressions. Is anyone aware of NMT methods that are designed to capture such recursive patterns? Currently, I'm using a LMST fed into an attention decoder.

&amp;#x200B;

At present, the methods can only translate 56.5% of expressions correctly. Any ideas for how I could improve performance? Here are some I have:

* Train the model for longer (currently trained for about 90 epochs in \~8 hours on a GPU)
* Increase the size of the training data. Currently using 320,000 observations.
* Increase the size of the LSTM layer
* Increase the size of the attention decoder layer
* Experiment with different NMT network architectures

Thanks for any help! I'd also appreciate any critique of the work and blog post. Trying to get better at NMT methods and writing.",1,3,False,self,,,,,
1300,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,22,asohr4,self.MachineLearning,Is Ivector Dynamic,https://www.reddit.com/r/MachineLearning/comments/asohr4/is_ivector_dynamic/,deveid,1550670354,"How true is this, i-vectors produced by one system are incompatible with those generated by a different system?

 I have created a function that does extraction of ivectors and does linear discriminant analysis from sklearn  and model and now I have another function that collects my voice in real time, it extracts the ivector and it attempts to predict who's speaking from the saved model. But it predicts the wrong person each time. Now during the time of training, I split the saved audio sample to train/test and it predicts the test data well with a 87% accuracy 1.00 precision,1.00 recall and 1.00 f1-score. but doesn't do well in real time data
",0,1,False,self,,,,,
1301,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,22,asoi7v,self.MachineLearning,"None of my posts appear in ""new""",https://www.reddit.com/r/MachineLearning/comments/asoi7v/none_of_my_posts_appear_in_new/,mlvpj,1550670433,[removed],0,1,False,self,,,,,
1302,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,23,asouel,self.MachineLearning,"Reinforcement Learning Summer SChOOL in 1-12 July 2019, Lille, France : Apply now !",https://www.reddit.com/r/MachineLearning/comments/asouel/reinforcement_learning_summer_school_in_112_july/,mseurin,1550672423,[removed],1,2,False,self,,,,,
1303,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,23,asovg1,self.MachineLearning,Doing background research,https://www.reddit.com/r/MachineLearning/comments/asovg1/doing_background_research/,ch3njus,1550672591,[removed],0,1,False,self,,,,,
1304,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,23,asp0d2,self.MachineLearning,"[N] Applications to Reinforcement Learning Summer SCOOL in 1-12 July 2019, Lille, France just opened!",https://www.reddit.com/r/MachineLearning/comments/asp0d2/n_applications_to_reinforcement_learning_summer/,mseurin,1550673369,"SequeL presents: Applications to Reinforcement Learning Summer SCOOL in 1-12 July 2019, Lille, France just opened!  [https://rlss.inria.fr/application/](https://rlss.inria.fr/application/)  Check out the list of confirmed speakers at: [https://rlss.inria.fr/speakers/](https://rlss.inria.fr/speakers/)",1,19,False,self,,,,,
1305,MachineLearning,t5_2r3gv,2019-2-20,2019,2,20,23,asp0qb,self.MachineLearning,Automate Blog/Forum Commenting with AI (Open AI),https://www.reddit.com/r/MachineLearning/comments/asp0qb/automate_blogforum_commenting_with_ai_open_ai/,sagarbasak,1550673427,[removed],1,1,False,self,,,,,
1306,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspc2k,nsfwjs.com,NSFW JS - TensorflowJS Model based on nsfw_data_scapper,https://www.reddit.com/r/MachineLearning/comments/aspc2k/nsfw_js_tensorflowjs_model_based_on_nsfw_data/,GantMan,1550675092,,0,1,True,nsfw,,,,,
1307,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspi12,blog.varunajayasiri.com,[P] Lab: Organize Machine Learning Experiments,https://www.reddit.com/r/MachineLearning/comments/aspi12/p_lab_organize_machine_learning_experiments/,mlvpj,1550675962,,0,1,False,default,,,,,
1308,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspjvx,self.MachineLearning,New Similarity Methods for Unsupervised Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aspjvx/new_similarity_methods_for_unsupervised_machine/,ramonserrallonga,1550676242,"**1. Introduction**

Data science is changing the rules of the game for decision making. Artificial intelligence is living its golden years where abundance of data, cheap computing capacity, and devoted talent depicts an unstoppable intelligence assisted life for humans. While it is common to hear about AI advice on health or financial investments, the same in business strategy is not so common. Maybe it is just a matter of time that AI learns how to handle data to support decision-making on business strategy, but it could also be that there is a lack of theoretical framework for it to build on. Following the competitive dynamics approach proposed in the article [*Strategizing with Competitive Asymmetry*](https://www.linkedin.com/pulse/strategizing-competitive-asymmetry-ramon-serrallonga/), a quantitative model was built to bridge this gap between business strategy and data science. In this article, I will outline an experiment that compares competitors' data arranged in vectors using this framework. The outcome was an alternative similarity measure, Projection Similarity, as accurate as Cosine Similarity but with asymmetric similarity.

**2. Using Cosine Similarity**

The competitive dynamics model used in [*Strategizing with Competitive Asymmetry*](https://www.linkedin.com/pulse/strategizing-competitive-asymmetry-ramon-serrallonga/) has two dimensions, Market Commonality and Resource Similarity, and the possible combinations are:

&amp;#x200B;

https://i.redd.it/0ohcaebfqqh21.png

*Source: Competitor Analysis and Interfirm Rivalry: Toward a Theoretical Integration,* [*Ming-Jer Chen*](https://www.linkedin.com/in/ming-jer-chen-0577289/)*, Academy of Management Review, 1996, Vol. 21, No. 1, 100-134.*

Under this approach, companies were characterized with one vector for each dimension including several determinant traits of their markets and their resources. Cosine Similarity was initially used to compare the vectors pairwise, but two problems arose.

First, Cosine Similarity is symmetric. The similarity of vector A with respect to vector B is the same as the one of vector B with respect of vector A. Cosine Similarity fails to represent competitive asymmetry.

Second, the similarities were very high. In a two-by-two matrix like Image 1 above, the intuitive threshold to classify a data point as high or low is 50%. Above 50% there are more odds that the two data points compared are similar than they are not. They are classified as ""high"". And vice versa - low if below 50%. With Cosine Similarity even companies radically different had similarities above 50%. If there is a training data set to find which is the optimal threshold, rather than at 50%, this problem is solvable. In this case falls Market Commonality where the industry and the countries where a company operates are known. But, for unsupervised classification, the fact that the optimal threshold falls at the intuitive 50% has a significant impact on the accuracy of the classification. This is the case of Resource Similarity where the skills of a company are neither easily nor publicly known.

**3. Using Projection Similarity**

An alternative method to compare the vectors was used in order to have asymmetric similarity. The projection similarity of vector A in relation to vector B was calculated as follows:

1. Calculate the orthogonal projection of vector A over vector B

2. Divide the norm of the orthogonal projection by the norm of B, which will give the relative value of the norm of the orthogonal projection in relation to B

3. Subtract 1 to the resulting value of step 2 and take the absolute value (to take advantage of the symmetric distributions)

&amp;#x200B;

https://i.redd.it/l0nkwp9iqqh21.png

This difference statistic can be used as Z value of a standard normal distribution to get the Standard Projection Similarity by multiplying the area of the cumulative distribution function from - to -Z by 2:

&amp;#x200B;

https://i.redd.it/q22udbhlqqh21.png

It could also be used as the exponent of a logistic function to get the Logistic Projection Similarity:

&amp;#x200B;

https://i.redd.it/nnp5mzsmqqh21.png

In the next section, we will examine the validity and accuracy of this alternative method.

**4. Calculations**

At this phase, 5 different measures were calculated: Cosine Similarity, Standard Projection Similarity, Logistic Projection Similarity, Cosine Similarity multiplied by Standard Projection Similarity, and Cosine Similarity multiplied by Logistic Projection Similarity. Each dimension had its unique data set, one for Market Commonality and one for Resource Similarity. The criterion to decide if two companies were similar was set by industry: if two companies are in the same industry, their similarity should be high in any of the dimensions; otherwise, low. The positive outcomes for both data sets were 32%, and the negative outcomes 68%. The chosen criterion carried the implicit assumption that companies in the same industry can differ but not a lot, either in one dimension or the other one. The performance of each method as a function of the threshold value were the following:

**Market Commonality**

&amp;#x200B;

https://i.redd.it/9gdiqp7pqqh21.png

&amp;#x200B;

https://i.redd.it/vwcxv5hqqqh21.png

&amp;#x200B;

https://i.redd.it/1e62dx1sqqh21.png

**Resource Similarity**

&amp;#x200B;

https://i.redd.it/znalaztuqqh21.png

*Processing img ycl7zvsuqqh21...*

*Processing img o1n7y0tuqqh21...*

&amp;#x200B;

*Processing img 1ekjzr2yqqh21...*

**5. Conclusions**

The two challenges of using Cosine Similarity were the presence of symmetric similarity and the optimal threshold value far from the intuitive 50%.

For the former, the experiment showed that the method of Cosine Similarity multiplied by Logistic Projection Similarity can successfully deliver the best asymmetric similarity with a high accuracy at par with Cosine Similarity.

For the latter, the optimal threshold of Cosine  Logistic (60%) was 5% below the Cosine one (65%) for Market Commonality, and 10% below for Resource Similarity (75% and 85% respectively). But those values were still far from 50%. So, even if there was an improvement, the challenge for unsupervised classification remained.

Following closely, the suboptimal Cosine  Standard method delivered a pair of optimal thresholds of 55% for Market Commonality and 70% for Resource Similarity. Even a bit better.",0,1,False,self,,,,,
1309,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspkwh,self.MachineLearning,AI-Based Real-Time Face Authorization System,https://www.reddit.com/r/MachineLearning/comments/aspkwh/aibased_realtime_face_authorization_system/,koloko_ko,1550676387,"In recent years, thanks to the rapid growth of technology, an era of process automation began and influenced many areas of our lives. For instance, its easy to notice that the number of self-service offices in the supermarkets or on the train stations significantly increased and waiting time in the queues has shrunk.

However, these are not only areas in which technology can be applied to improve peoples everyday life. Therefore, we decided in Daftcode to start a new project and create a solution for an automatic authorization system based on real-time face recognition and document scanning. But you will askwhy? Do you remember the last time that you were waiting in the long queue to the registration desk at the conference or in the medical center? Thanks to our solution, the time for your turn could be reduced, because the processing capacity of registration would be increased.

&amp;#x200B;

Read the whole article  [https://blog.daftcode.pl/ai-based-real-time-face-authorization-system-d405a1c42caf](https://blog.daftcode.pl/ai-based-real-time-face-authorization-system-d405a1c42caf)",0,1,False,self,,,,,
1310,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspmas,self.MachineLearning,[R] Bandit Swarm Networks,https://www.reddit.com/r/MachineLearning/comments/aspmas/r_bandit_swarm_networks/,CireNeikual,1550676581,"Hello,

I have been experimenting with a new online/incremental learning technique that I thought I should share. As far as I know it is novel despite being incredibly simple (maybe I am wrong though). It's called Bandit Swarm Networks, and is capable of performing reinforcement learning on any network with any activations, discretization, sparsity, and architecture. If a genetic algorithm can train it, so can this, but incrementally and with a single agent.

[Link to the blog post](https://twistedkeyboardsoftware.com/?p=147)

Feedback welcomed!",19,106,False,self,,,,,
1311,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspmn6,self.MachineLearning,Definition of ML: Trying to make sense of Pan and Yang's article: A survey of transfer learning,https://www.reddit.com/r/MachineLearning/comments/aspmn6/definition_of_ml_trying_to_make_sense_of_pan_and/,FrimannKjerulf,1550676629,[removed],0,1,False,self,,,,,
1312,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspv5e,self.MachineLearning,[P] AI-Based Real-Time Face Authorization System,https://www.reddit.com/r/MachineLearning/comments/aspv5e/p_aibased_realtime_face_authorization_system/,koloko_ko,1550677861,"In recent years, thanks to the rapid growth of technology, an era of process automation began and influenced many areas of our lives. For instance, its easy to notice that the number of self-service offices in the supermarkets or on the train stations significantly increased and waiting time in the queues has shrunk.

However, these are not only areas in which technology can be applied to improve peoples everyday life. Therefore, we decided in Daftcode to start a new project and create a solution for an automatic authorization system based on real-time face recognition and document scanning. But you will askwhy? Do you remember the last time that you were waiting in the long queue to the registration desk at the conference or in the medical center? Thanks to our solution, the time for your turn could be reduced, because the processing capacity of registration would be increased.

&amp;#x200B;

Read the whole article  [https://blog.daftcode.pl/ai-based-real-time-face-authorization-system-d405a1c42caf](https://blog.daftcode.pl/ai-based-real-time-face-authorization-system-d405a1c42caf)",0,5,False,self,,,,,
1313,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspv8t,self.MachineLearning,U.S. government shutdown: we analyzed Twitter conversation clusters with NLP &amp; co-word association,https://www.reddit.com/r/MachineLearning/comments/aspv8t/us_government_shutdown_we_analyzed_twitter/,Nexalogy,1550677874,[removed],0,1,False,self,,,,,
1314,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspvpb,self.MachineLearning,How do i select a label,https://www.reddit.com/r/MachineLearning/comments/aspvpb/how_do_i_select_a_label/,ighoyotaben,1550677940,[removed],0,1,False,self,,,,,
1315,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspvup,self.MachineLearning,"Simple Questions Thread February 20, 2019",https://www.reddit.com/r/MachineLearning/comments/aspvup/simple_questions_thread_february_20_2019/,AutoModerator,1550677964,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",0,1,False,self,,,,,
1316,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,0,aspx8x,self.MachineLearning,[D] Methods for hyperparameter tuning with LightGBM?,https://www.reddit.com/r/MachineLearning/comments/aspx8x/d_methods_for_hyperparameter_tuning_with_lightgbm/,Fender6969,1550678156,"New to LightGBM have always used XgBoost in the past. I want to give LightGBM a shot but am struggling with how to do the hyperparameter tuning and feed a grid of parameters into something like GridSearchCV (Python) and call the .best_params_ to have the GridSearchCV give me the optimal hyperparameters. 

Is there anything like this for LightsGBM? From my understanding, lgb.cv is to evaluate performance of model building procedure rather than to do hyperparameter. ",17,3,False,self,,,,,
1317,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,1,aspzvv,self.MachineLearning,[P] Library to organize machine learning experiments,https://www.reddit.com/r/MachineLearning/comments/aspzvv/p_library_to_organize_machine_learning_experiments/,mlvpj,1550678525,"I created this library to help me organize machine learning experiments. It has tools for logging, TensorBoard integration, custom visualizations of TensorBoard summaries, etc.

[Intro](http://blog.varunajayasiri.com/ml/lab/lab_getting_started.html)

[Github](https://github.com/vpj/lab)

It can add the results to the source file as a comment. This is something I've found very useful lately to keep track of the experiments. Let me know your views. Also if someone wants to use it, I suggest you go through the code (it's quite small and documented), because I do breaking changes frequently.",4,1,False,self,,,,,
1318,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,1,asqm57,self.MachineLearning,[P] Face and document authorization system // code + blog post,https://www.reddit.com/r/MachineLearning/comments/asqm57/p_face_and_document_authorization_system_code/,_djab_,1550681628,"This is a blog post (with repository) about project and created open-source solution for authorization based on video of a face and document scan.

[https://medium.com/asap-report/a-deeper-insight-into-face-authorization-system-f3e8007aa7c6?fbclid](https://medium.com/asap-report/a-deeper-insight-into-face-authorization-system-f3e8007aa7c6?fbclid)",0,1,False,self,,,,,
1319,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,2,asqpw7,thecleverest.com,Hot or Not for GAN generated faces,https://www.reddit.com/r/MachineLearning/comments/asqpw7/hot_or_not_for_gan_generated_faces/,solomania9,1550682144,,0,1,False,default,,,,,
1320,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,2,asr4mq,self.MachineLearning,"[P] Practical use cases of NLP: Text Classification, Sentiment Analysis",https://www.reddit.com/r/MachineLearning/comments/asr4mq/p_practical_use_cases_of_nlp_text_classification/,lyeoni,1550684168," A step-by-step tutorial on how to implement and adapt to the simple real-word NLP task.

* news-category-classifcation:  This repo contains a simple source code for text-classification based on TextCNN. Corpus is **Huffpost** news category dataset in English. Most open sources are a bit difficult to study &amp; make text-classification model for beginners. So, I hope that this repo can be a good solution for people who want to have their own text-classification model. 
* movie-rating-classification:  This repo contains a simple source code for text-classification based on TextCNN. Corpus is **movie review** dataset in the Korean language. Most open sources are a bit difficult to study &amp; make text-classification model for beginners. So, I hope that this repo can be a good solution for people who want to have their own text-classification model. 

Link : [https://github.com/lyeoni/nlp-tutorial](https://github.com/lyeoni/nlp-tutorial)",3,6,False,self,,,,,
1321,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,2,asrdbk,self.MachineLearning,Research Ideas,https://www.reddit.com/r/MachineLearning/comments/asrdbk/research_ideas/,deep_into_ml,1550685361,"Looking for ideas to work in CNN: Maybe some Training/New architectural modifications in CNN. Or maybe into Gradient Optimization algorithms. Suggestions, anything, Something that someone wanted to try?

Look forward to new IDEAS!!! :)",0,1,False,self,,,,,
1322,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,3,asrvff,mlwhiz.com,What my first Silver Medal taught me about Text Classification and Kaggle in general?,https://www.reddit.com/r/MachineLearning/comments/asrvff/what_my_first_silver_medal_taught_me_about_text/,kiser_soze,1550687754,,0,1,False,default,,,,,
1323,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,3,ass0tx,self.MachineLearning,Quick Question about ML Courses/Book,https://www.reddit.com/r/MachineLearning/comments/ass0tx/quick_question_about_ml_coursesbook/,Slaught3rr,1550688473,"Hey,

I am currently trying to get into machine learning and I've checked the wiki so I know what courses are generally recommended here. I like a hands on approach so right now I am going through ""Introduction to Machine Learning with Python"" by Andreas Mueller. I am enjoying the book but I am also worried that I am not getting a deeper level of explanation on the various algorithms since the book just shows how the algorithm works and why/when they are used.

I've also started Andrew Ng's ML course on Coursera but I am not a big fan of it because it uses octave and I'd prefer to focus on just python right now and perfect my skills.

I was just looking for opinions on if its a good idea to just continue going through ""Introduction to Machine Learning with Python"" by Andreas Mueller first or if I should choose another online course to go through.

Thank you for taking the time to read this!

",0,1,False,self,,,,,
1324,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,3,ass42s,github.com,tensorflow/ngraph is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/ass42s/tensorflowngraph_is_a_new_github_repo_by/,sjoerdapp,1550688917,,0,1,False,default,,,,,
1325,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,4,assey6,axel.org,Is Machine Learning and Artificial Intelligence Compromising Our Privacy?,https://www.reddit.com/r/MachineLearning/comments/assey6/is_machine_learning_and_artificial_intelligence/,BenStoAmigo,1550690364,,0,1,False,default,,,,,
1326,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,4,assjkd,axel.org,[D] Is Machine Learning and Artificial Intelligence Compromising Our Privacy?,https://www.reddit.com/r/MachineLearning/comments/assjkd/d_is_machine_learning_and_artificial_intelligence/,BenStoAmigo,1550690977,,0,1,False,default,,,,,
1327,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,5,astf1c,self.MachineLearning,[D] An alternative exact example to XOR using wide Neural Networks,https://www.reddit.com/r/MachineLearning/comments/astf1c/d_an_alternative_exact_example_to_xor_using_wide/,kayzaks,1550695182,"It is pretty obvious that this was inspired from the discussion on the arXiv paper on wide neural networks in the other thread. I've been sitting on a whole bunch of figures and equations that I put together a while back for which I finally found the perfect occasion.

&amp;#x200B;

I put it in blog format so its easier to read with the figures/equations: [https://medium.com/@m1o1d1/writing-down-the-weights-and-biases-of-a-universal-neural-network-without-training-it-5d184bab128](https://medium.com/@m1o1d1/writing-down-the-weights-and-biases-of-a-universal-neural-network-without-training-it-5d184bab128)

&amp;#x200B;

This is meant as an additional example to the XOR-problem that is used in pretty much every textbook, but with the added bonus of being able to approximate any n-to-m dimensional function. And like with the XOR example, where one can write down the weights and biases directly, the one I present is also able to do that.

&amp;#x200B;

I think it would make a cool exercise in a textbook to approximate a function (say sin(x) from -2\\pi to 2\\pi) using this method, to show that there are more ways to doing things than just running an optimizer. Of course its not practical, but I would have loved this as a student to gain some more insight. Further I think it helps in understanding the universal approximation theorem a bit better by seeing an actual easy-to-follow example. What do you guys think?",0,4,False,self,,,,,
1328,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,6,astqjl,self.MachineLearning,"Call for Papers (CfP) for the Workshop on ""Structure and Priors in Reinforcement Learning"" (SPiRL) at ICLR 2019",https://www.reddit.com/r/MachineLearning/comments/astqjl/call_for_papers_cfp_for_the_workshop_on_structure/,1abhigupta,1550696693,[removed],0,1,False,self,,,,,
1329,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,6,asu3ve,self.MachineLearning,"What do you think about a project on machine learning monitoring for physical stores analysis, like a real world google analyzer",https://www.reddit.com/r/MachineLearning/comments/asu3ve/what_do_you_think_about_a_project_on_machine/,mcc111000011011,1550698499,[removed],0,1,False,self,,,,,
1330,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,6,asu805,self.MachineLearning,What is the state of art in anomaly detection?,https://www.reddit.com/r/MachineLearning/comments/asu805/what_is_the_state_of_art_in_anomaly_detection/,Invisible_Nothing,1550699054,"Hello everyone, I would like to know what are the state of art on anomaly detection and if there is a good resource to become aware of novelty on that field?",0,1,False,self,,,,,
1331,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,7,asufb2,self.MachineLearning,"What do you think about a project on machine learning monitoring for physical stores analysis, like a real world google analyzer",https://www.reddit.com/r/MachineLearning/comments/asufb2/what_do_you_think_about_a_project_on_machine/,culo2015,1550700061,[removed],0,1,False,self,,,,,
1332,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,7,asuk4z,yolyzer.com,"What do you think about a project on machine learning monitoring for physical stores analysis, like a real world google analyzer . I'm still in the initial phase, and started to call stores trying to sell.",https://www.reddit.com/r/MachineLearning/comments/asuk4z/what_do_you_think_about_a_project_on_machine/,culo2015,1550700690,,0,1,False,default,,,,,
1333,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,8,asv5x2,self.MachineLearning,"EliNoob: Vision, Data Augmentation, and some sort of semantic-ish cartoonify-ish layers that can be shared and reused?",https://www.reddit.com/r/MachineLearning/comments/asv5x2/elinoob_vision_data_augmentation_and_some_sort_of/,firesalamander,1550703667,[removed],0,1,False,self,,,,,
1334,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,8,asv7an,self.MachineLearning,Keras: How to reshape data for use with LSTM in keras to predict sine wave values?,https://www.reddit.com/r/MachineLearning/comments/asv7an/keras_how_to_reshape_data_for_use_with_lstm_in/,ShittyUkePlayer,1550703853,[removed],0,1,False,self,,,,,
1335,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,8,asvoah,i.redd.it,[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color,https://www.reddit.com/r/MachineLearning/comments/asvoah/r_scfegan_face_editing_generative_adversarial/,run_youngjoo,1550706292,,0,1,False,default,,,,,
1336,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,9,asvxgc,i.redd.it,[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color,https://www.reddit.com/r/MachineLearning/comments/asvxgc/r_scfegan_face_editing_generative_adversarial/,run_youngjoo,1550707626,,1,1,False,default,,,,,
1337,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,9,asw1to,/r/MachineLearning/comments/asw1to/a_few_friends_of_mine_are_working_on_an_app_that/,"A few friends of mine are working on an app that uses Deep Learning to move the face on an image/video/painting the way you move it, in real time. Amazed me, so I thought I would share.",https://www.reddit.com/r/MachineLearning/comments/asw1to/a_few_friends_of_mine_are_working_on_an_app_that/,Rick_grin,1550708212,,0,1,False,https://a.thumbs.redditmedia.com/ZbKyITldiua7cbGnO4HJ1Uw9yX4lDMY_4sxOi6gCfd4.jpg,,,,,
1338,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,9,asw6nr,self.MachineLearning,[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color,https://www.reddit.com/r/MachineLearning/comments/asw6nr/r_scfegan_face_editing_generative_adversarial/,run_youngjoo,1550708926,"&amp;#x200B;

https://i.redd.it/pja1y12wfth21.jpg

SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color

arXiv: [https://arxiv.org/abs/1902.06838](https://arxiv.org/abs/1902.06838)

code: [https://github.com/JoYoungjoo/SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN)

&amp;#x200B;

Abstract

We present a novel image editing system that generates images as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.",41,208,False,self,,,,,
1339,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,10,aswvt7,self.MachineLearning,Creating training data/images for a binary hand gesture classifier?,https://www.reddit.com/r/MachineLearning/comments/aswvt7/creating_training_dataimages_for_a_binary_hand/,bandalorian,1550712733,[removed],0,1,False,self,,,,,
1340,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,10,aswx8k,self.MachineLearning,[P] Is ML an appropriate approach for modeling my environmental data?,https://www.reddit.com/r/MachineLearning/comments/aswx8k/p_is_ml_an_appropriate_approach_for_modeling_my/,zoombackcameraa,1550712942,"Hello! Environmental scientist here considering wading into ML for my first time. I have 300+ distinct variables (such as species abundances, temperature, etc.) recorded almost every week for 60+ years. Id like to be able to predict the abundance of some species given a set of inputs. 

1. Is this an appropriate project for ML?
2. If yes, what would a rough project plan for executing this look like? 

Any advice or feedback you could provide would be greatly appreciate!",6,19,False,self,,,,,
1341,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,10,aswzsw,self.MachineLearning,"If you had 2-3 weeks to prepare for a Machine Learning interview, how would you structure the prep?",https://www.reddit.com/r/MachineLearning/comments/aswzsw/if_you_had_23_weeks_to_prepare_for_a_machine/,spoiltForChoice,1550713339,[removed],0,1,False,self,,,,,
1342,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,10,asx1lg,github.com,kristery/Awesome-Imitation-Learning,https://www.reddit.com/r/MachineLearning/comments/asx1lg/kristeryawesomeimitationlearning/,Kristery,1550713624,,0,1,False,default,,,,,
1343,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,11,asxlx9,self.MachineLearning,Pytorch vs Tensorflow for research,https://www.reddit.com/r/MachineLearning/comments/asxlx9/pytorch_vs_tensorflow_for_research/,zestNotJest,1550716795,[removed],0,1,False,self,,,,,
1344,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,12,asy1fn,vinacool.vn,Mch bn cch bo qun thc phm ng lnh ti ngon v lu nht,https://www.reddit.com/r/MachineLearning/comments/asy1fn/mch_bn_cch_bo_qun_thc_phm_ng_lnh_ti/,Vinacool,1550719266,,0,1,False,https://b.thumbs.redditmedia.com/cUO8zFaeGBD5wwPSMnvCrX5Pj3fuyIOdDmZOjFh-kTk.jpg,,,,,
1345,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,13,asyxx4,self.MachineLearning,NEAT noise screen representation: what exactly is it and how do i do it?,https://www.reddit.com/r/MachineLearning/comments/asyxx4/neat_noise_screen_representation_what_exactly_is/,nrmxndal,1550724669,[removed],0,1,False,self,,,,,
1346,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,14,asz8wv,self.MachineLearning,Subscribe Google AI Blog into email,https://www.reddit.com/r/MachineLearning/comments/asz8wv/subscribe_google_ai_blog_into_email/,Mrikapa,1550726571,"&amp;#x200B;

How to subscribe Google AI Blog into email ? 

&amp;#x200B;

[https://ai.googleblog.com/](https://ai.googleblog.com/)

&amp;#x200B;

Any one tried it or getting the posts automatically delivered to your email id ?",0,1,False,self,,,,,
1347,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,14,asz9hj,i.redd.it,Live Streaming App Development,https://www.reddit.com/r/MachineLearning/comments/asz9hj/live_streaming_app_development/,hiwilliam31,1550726669,,0,1,False,default,,,,,
1348,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,14,aszbv0,self.MachineLearning,Gender &amp; Age Classification using OpenCV Deep Learning ( C++/Python ),https://www.reddit.com/r/MachineLearning/comments/aszbv0/gender_age_classification_using_opencv_deep/,spmallick,1550727100,"Ever wondered what a person's real age was? Or have you seen a baby and been really confused if it is a boy or a girl? Well, guess what! LearnOpenCV has a new blog post and it reveals how you can easily guess age and gender using OpenCV Deep Learning

[https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/](https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/)

*Processing video s9upjvhayuh21...*

We'll be using Convolutional Neural Network (CNN) architecture, and focus on honing the Age Prediction Model.   
Like, tag your friends and follow us for more of such exciting stuff! Mention reviews and what you want us to work on next, in the comments!",0,1,False,self,,,,,
1349,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,15,at02v1,self.MachineLearning,[R] Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing,https://www.reddit.com/r/MachineLearning/comments/at02v1/r_semantic_relation_classification_via/,roomylee,1550732201,"&amp;#x200B;

[Bidirectional LSTM with Entity-aware Attention and Latent Entity Typing](https://i.redd.it/ad96v9ezcvh21.png)

arXiv: [https://arxiv.org/abs/1901.08163](https://arxiv.org/abs/1901.08163)

code: [https://github.com/roomylee/entity-aware-relation-classification](https://github.com/roomylee/entity-aware-relation-classification)

&amp;#x200B;

Abstract

Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing (NLP). Most previous models for relation classification rely on the high-level lexical and syntactic features obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET. Experimental results on the SemEval-2010 Task 8, one of the most popular relation classification task, demonstrate that our model outperforms existing state-of-the-art models without any high-level features.",0,7,False,self,,,,,
1350,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,16,at09dw,128.84.21.199,[R] World Discovery Models,https://www.reddit.com/r/MachineLearning/comments/at09dw/r_world_discovery_models/,ewanlee,1550733563,,0,1,False,default,,,,,
1351,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,16,at0a3h,arxiv.org,[R] World Discovery Models,https://www.reddit.com/r/MachineLearning/comments/at0a3h/r_world_discovery_models/,ewanlee,1550733719,,8,11,False,default,,,,,
1352,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,16,at0cdm,compakk.blogspot.com,Compak Shrink Wrapping and Tunnels Machine Manufacturers,https://www.reddit.com/r/MachineLearning/comments/at0cdm/compak_shrink_wrapping_and_tunnels_machine/,compak03,1550734181,,0,1,False,default,,,,,
1353,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,16,at0d9r,openreview.net,[R] Recurrent Experience Replay in Distributed Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/at0d9r/r_recurrent_experience_replay_in_distributed/,ewanlee,1550734366,,2,9,False,https://a.thumbs.redditmedia.com/O5xSqPxXQan0eq4XIj_39G9lsyFLtyg3D_81hYNOIr4.jpg,,,,,
1354,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,16,at0elb,self.MachineLearning,Working with Google Colab GPU/TPU,https://www.reddit.com/r/MachineLearning/comments/at0elb/working_with_google_colab_gputpu/,delpotroswrist,1550734639,[removed],0,1,False,self,,,,,
1355,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,17,at0ydo,self.MachineLearning,How to train general RNNs ?,https://www.reddit.com/r/MachineLearning/comments/at0ydo/how_to_train_general_rnns/,rootmolloch,1550739228,[removed],0,1,False,self,,,,,
1356,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,18,at14kx,youtube.com,Real-Time Vehicle Classification,https://www.reddit.com/r/MachineLearning/comments/at14kx/realtime_vehicle_classification/,johnwillsons,1550740678,,0,1,False,https://a.thumbs.redditmedia.com/69N-yIWb1GIW55rhyv2NorxqIE_lUWe7qgj3YDAaOu8.jpg,,,,,
1357,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,18,at1egz,self.MachineLearning,[D] Difference between AUC PR vs AUC ROC,https://www.reddit.com/r/MachineLearning/comments/at1egz/d_difference_between_auc_pr_vs_auc_roc/,amil123123,1550742919,"Hey all,
I tried reading a lot on AUC PR VS AUC Roc however I am still not able to intuitively understand in which case to use what?
Would be really glad if someone could give an example explaining the tradeoff.
Thanks in advance!!",14,19,False,self,,,,,
1358,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,19,at1g5c,self.MachineLearning,Dataset needed for Medical Annotation System.,https://www.reddit.com/r/MachineLearning/comments/at1g5c/dataset_needed_for_medical_annotation_system/,keyser_kint,1550743309,[removed],0,1,False,self,,,,,
1359,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,19,at1lin,self.MachineLearning,[R] Saliency Tubes: Visual Explanations for Spatio-Temporal Convolutions,https://www.reddit.com/r/MachineLearning/comments/at1lin/r_saliency_tubes_visual_explanations_for/,Alex_Stergiou,1550744524,"arxiv paper: [https://arxiv.org/abs/1902.01078](https://arxiv.org/abs/1902.01078)
github repo: [https://github.com/alexandrosstergiou/Saliency-Tubes-Visual-Explanations-for-Spatio-Temporal-Convolutions](https://github.com/alexandrosstergiou/Saliency-Tubes-Visual-Explanations-for-Spatio-Temporal-Convolutions)
youtube video: [https://youtu.be/JANUqoMc3es](https://youtu.be/JANUqoMc3es)

Abstract
------
Deep learning approaches have been established as the main methodology for video classification and recognition. Recently, 3-dimensional convolutions have been used to achieve state-of-the-art performance in many challenging video datasets. Because of the high level of complexity of these methods, as the convolution operations are also extended to additional dimension in order to extract features from them as well, providing a visualization for the signals that the network interpret as informative, is a challenging task. An effective notion of understanding the network's inner-workings would be to isolate the spatio-temporal regions on the video that the network finds most informative. We propose a method called Saliency Tubes which demonstrate the foremost points and regions in both frame level and over time that are found to be the main focus points of the network. We demonstrate our findings on widely used datasets for third-person and egocentric action classification and enhance the set of methods and visualizations that improve 3D Convolutional Neural Networks (CNNs) intelligibility.",0,4,False,self,,,,,
1360,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,19,at1myl,smarten.com,White Paper - Organizational Readiness Is Crucial to the Success of an Advanced Analytics Initiative,https://www.reddit.com/r/MachineLearning/comments/at1myl/white_paper_organizational_readiness_is_crucial/,ElegantMicroWebIndia,1550744859,,0,1,False,default,,,,,
1361,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,20,at1wo0,self.MachineLearning,Looking for Exercise/Example Datasets for Bayesian Models,https://www.reddit.com/r/MachineLearning/comments/at1wo0/looking_for_exerciseexample_datasets_for_bayesian/,UncertainFuture80,1550747078,[removed],0,1,False,self,,,,,
1362,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,20,at217h,self.MachineLearning,Need a topic to present on,https://www.reddit.com/r/MachineLearning/comments/at217h/need_a_topic_to_present_on/,oneofchaos,1550748090,[removed],0,1,False,self,,,,,
1363,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,20,at27vh,/r/MachineLearning/comments/at27vh/my_implementation_of_yolo_you_only_look_once_ver/,My implementation of YOLO - You only look once (ver 2) for object detection tasks. Source code: https://github.com/vietnguyen91/Yolo-v2-pytorch,https://www.reddit.com/r/MachineLearning/comments/at27vh/my_implementation_of_yolo_you_only_look_once_ver/,1991viet,1550749540,,1,1,False,default,,,,,
1364,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,20,at2c3c,self.MachineLearning,Source code for SKlearn Gaussian Naive Bayes Classifier?,https://www.reddit.com/r/MachineLearning/comments/at2c3c/source_code_for_sklearn_gaussian_naive_bayes/,DWscrub,1550750392,[removed],0,1,False,self,,,,,
1365,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2dp5,self.MachineLearning,How to differentiate the topics from topic modeling algorithm as an emerging topic or not?,https://www.reddit.com/r/MachineLearning/comments/at2dp5/how_to_differentiate_the_topics_from_topic/,pinksii,1550750649,[removed],0,1,False,self,,,,,
1366,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2erm,self.MachineLearning,[D] Up-to-date deep learning classification courses,https://www.reddit.com/r/MachineLearning/comments/at2erm/d_uptodate_deep_learning_classification_courses/,supermanstream,1550750846,"Hey, so as a bit of background, I work as a ML engineer in a lab. I work on a project right now which involves binary classification of highly imbalanced data. While I am able to get decent results with RandomForestClassifier, I feel like I can achieve much more with neural networks, simply due to the fact that the dataset is huge (\~10 mil. rows) and highly dimensional (\~200 columns). I feel like RandomForestClassifier is not complex enough to ""understand"" the data in the ML way.

&amp;#x200B;

So, I started to look into neural networks. I feel like I have a solid understanding of them in terms of basics of how they work. However, one thing I don't get about neural networks is what architecture to choose for my project. I was looking for courses online that teach deep learning from scratch, looking to find relevant architecture discussion in them, but the courses mostly describe different applications, such as image classification, or text mining, and while this is definitely fun, I do not think that it is suitable for me. So, my question is:

&amp;#x200B;

TL,DR: Is there a good up-to-date course that you can advise that goes over using different neural network architectures for classification problems of tabular data?",10,4,False,self,,,,,
1367,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2ig1,self.MachineLearning,[P] Annotated Proximal Policy Optimization (PPO) implementation in PyTorch,https://www.reddit.com/r/MachineLearning/comments/at2ig1/p_annotated_proximal_policy_optimization_ppo/,mlvpj,1550751576,"Here's a code of PPO reinforcement learning algorithm in PyTorch. I implemented this to try PyTorch.

[http://blog.varunajayasiri.com/ml/ppo\_pytorch.html](http://blog.varunajayasiri.com/ml/ppo_pytorch.html)

The code is annotated with maths from the papers and lectures.",4,15,False,self,,,,,
1368,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2ik8,self.MachineLearning,How to do systematic experimenting,https://www.reddit.com/r/MachineLearning/comments/at2ik8/how_to_do_systematic_experimenting/,croxcrocodile,1550751604,[removed],0,1,False,self,,,,,
1369,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2j94,self.MachineLearning,Help for uni assignment,https://www.reddit.com/r/MachineLearning/comments/at2j94/help_for_uni_assignment/,mayank010698,1550751743," The given dataset consists of some coarse classes. Each class has some fine classes, each of which in turn consists of some images of the respective coarse and fine class. The task of this assignment is: given some test images, you have to correctly classify it into its correct coarse and fine grained class.    


what could be a good approach",0,1,False,self,,,,,
1370,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2kp6,self.MachineLearning,Intel MKL complementary or exclusively as GPU replacement (RTX 2070-80 build CPU choice),https://www.reddit.com/r/MachineLearning/comments/at2kp6/intel_mkl_complementary_or_exclusively_as_gpu/,Cantrill1758,1550752036,"Hi,

&amp;#x200B;

Thinking about a ML (ML, *not only deep*) dedicated build with an RTX 2070 or -80, and can't grasp the exact weight I should give to the fact that Intel CPUs have Intel MKL (Math Kernel Library) dedicated instructions. Am I supposed to ignore this MKL advantage (and possibly just buy a Ryzen 7 2700X - cheaper price/thread ratio, but no MKL like library) since I plan on using my GPU for training models ?

&amp;#x200B;

At the same time, I also must admit that, though having understood lots of cores (i7-Ryzen 2700x) is a good thing for preprocessing, I'm not sure how stupid (or not?) it would be to just buy an i5 9XXX with the RTX, since it doesn't look like it will be a bottleneck source.

&amp;#x200B;

Thanks

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1371,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2lhj,self.MachineLearning,[D] Deep learning summer schools 2019,https://www.reddit.com/r/MachineLearning/comments/at2lhj/d_deep_learning_summer_schools_2019/,davinci1913,1550752189,"Hi, it's summer school application time, and I am looking for suggestions to where to apply. I'm based in Europe, but would consider schools anywhere in the world. Experiences from previous participants are highly appreciated as well. 

Personally I am very interested in the [Deep|Bayes](http://deepbayes.ru) summer school in Moscow (currently spending my most of my time learning about BDL and VI), but I am afraid my chances of acceptance are slim considering that I'm a masters student with no publications to show to. **Deadline: 15 April**

I also had a look at the [DLRL](https://dlrlsummerschool.ca) summer school in Edmonton, which looks super interesting as well, but it seem to be targeted at even higher level students and researchers, i.e., grad (Ph.D.?) students and postdocs. My only hope of getting in there is that the acceptance procedure is stochastic (!) according to their FAQ page, with applicants with a strong track record having a higher probability of acceptance. **Deadline: 22 February**

&amp;#x200B;",39,72,False,self,,,,,
1372,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2oyq,self.MachineLearning,HELP,https://www.reddit.com/r/MachineLearning/comments/at2oyq/help/,bojac_k,1550752821,[removed],0,1,False,self,,,,,
1373,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,21,at2sq8,self.MachineLearning,Fast.AI course. Any good?,https://www.reddit.com/r/MachineLearning/comments/at2sq8/fastai_course_any_good/,iamMess,1550753511,[removed],0,1,False,self,,,,,
1374,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,23,at3jve,self.Amd,Radeon VII Tensorflow Deep Learning results - Huge improvement from Vega FE,https://www.reddit.com/r/MachineLearning/comments/at3jve/radeon_vii_tensorflow_deep_learning_results_huge/,SandboChang,1550758124,,0,1,False,default,,,,,
1375,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,23,at3n8a,i.redd.it,Can I just be a normal person please?,https://www.reddit.com/r/MachineLearning/comments/at3n8a/can_i_just_be_a_normal_person_please/,GeekMonolith,1550758671,,0,1,False,https://a.thumbs.redditmedia.com/-UgRNvbZ-VrTV8p8LQ7MxC-N1uwaUhsXa512fYICZ80.jpg,,,,,
1376,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,23,at3oa9,i.redd.it,[D] Can I just be a normal person please?,https://www.reddit.com/r/MachineLearning/comments/at3oa9/d_can_i_just_be_a_normal_person_please/,GeekMonolith,1550758853,,0,1,False,default,,,,,
1377,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,23,at3tf2,theappsolutions.com,Machine Learning Algorithms: 4 Types You Should Know,https://www.reddit.com/r/MachineLearning/comments/at3tf2/machine_learning_algorithms_4_types_you_should/,lady_monsoon,1550759716,,0,1,False,default,,,,,
1378,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,23,at3yt1,self.MachineLearning,Good Projects To Help You Learn More About ML?,https://www.reddit.com/r/MachineLearning/comments/at3yt1/good_projects_to_help_you_learn_more_about_ml/,LegalCrow,1550760567,"I've been watching YouTube videos for a couple weeks now and trying to understand the theory behind linear regression, classifiers, neural networks, etc.

I want to do a project applies the theory and help me learn. I want something simple which will hopefully include a tutorial so I can just follow along and get used to the technology.

I have heard about Pandas, TensorFlow and so many other things but I don't know how to use any of them. Can you link me to any project tutorials which can help me build something so I can learn?",0,1,False,self,,,,,
1379,MachineLearning,t5_2r3gv,2019-2-21,2019,2,21,23,at4096,self.MachineLearning,[D] Dataset size for image-to-image algorithms?,https://www.reddit.com/r/MachineLearning/comments/at4096/d_dataset_size_for_imagetoimage_algorithms/,lzhbrian,1550760802,"Training dataset size seems to have critical impact for training image synthesizing models like BigGAN, StyleGAN.
For example, if you have 1,000 or 10,000 or 100,000 images for training, the quality of the synthesized results can really differ.

Does this also apply for image-to-image algorithms (e.g. CycleGAN, pix2pix) ?
In pix2pix, size of the datasets used differs a lot, however, the small datasets also provide decent results.

| Dataset | size |
| ------- | ---- |
| Architectural labels -&gt; photo | 400 |
| Maps -&gt; aerial photograph | 1096 |
| Edges -&gt; shoes | 50,000 |
| Edge -&gt; Handbags | 137,000 |

Thanks.
",1,1,False,self,,,,,
1380,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,0,at46cc,self.MachineLearning,[D] How do you do pruning?,https://www.reddit.com/r/MachineLearning/comments/at46cc/d_how_do_you_do_pruning/,CaminantErrante,1550761740,"Hi everybody, so i wanted to learn to do pruning in deep neural network (specifically on Tensorflow) , but the only thing i have found is the library of tensorflow, that it relies in applying masks to the different operations. but it doesn't have a lot of documentation, 

So i wanted to ask which method have you used, for example, i tried to modify the cpkt files that have the weights of the network, but i haven't found a correct way to do it.",11,8,False,self,,,,,
1381,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,0,at476g,self.MachineLearning,Is AI-Art really Art?,https://www.reddit.com/r/MachineLearning/comments/at476g/is_aiart_really_art/,esteimle,1550761870,[removed],0,1,False,https://b.thumbs.redditmedia.com/PNuOrMjdeYtxPCSeAd3_8piFM-4SDx4NyC6wrkqjbHg.jpg,,,,,
1382,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,0,at4967,self.MachineLearning,[D] Meaning of GAN Losses,https://www.reddit.com/r/MachineLearning/comments/at4967/d_meaning_of_gan_losses/,mackie__m,1550762172,"I'm trying to get into GANs, and started playing around with pix2pix, and did some training. While the results look interesting visually, I'm trying to understand the numerical meaning of the GAN loss numbers. I see these numbers as output of the training, what does it intuitively mean? Smaller is better cause it's a loss or is it? Does the L1 loss need to go to zero, does the Adversarial loss need to go to zero? Asking here about G_*, and D_* numbers.

(epoch: 86, iters: 4200, time: 0.096, data: 0.001) G_GAN: 8.314 G_L1: 3.214 D_real: 0.001 D_fake: 0.001
(epoch: 86, iters: 4300, time: 0.396, data: 0.001) G_GAN: 6.475 G_L1: 3.402 D_real: 0.000 D_fake: 0.018
(epoch: 86, iters: 4400, time: 0.095, data: 0.001) G_GAN: 6.951 G_L1: 3.892 D_real: 0.000 D_fake: 0.008
(epoch: 86, iters: 4500, time: 0.096, data: 0.002) G_GAN: 7.276 G_L1: 3.367 D_real: 0.001 D_fake: 0.003
(epoch: 86, iters: 4600, time: 0.099, data: 0.001) G_GAN: 9.714 G_L1: 3.350 D_real: 0.005 D_fake: 0.001
(epoch: 86, iters: 4700, time: 0.454, data: 0.002) G_GAN: 8.952 G_L1: 3.676 D_real: 0.000 D_fake: 0.001
(epoch: 86, iters: 4800, time: 0.096, data: 0.002) G_GAN: 10.616 G_L1: 5.906 D_real: 0.000 D_fake: 0.000 ",8,5,False,self,,,,,
1383,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,0,at4c98,i.redd.it,[D] Setting unrealistic and unreasonable expectations for data science and ML experts. Your thoughts?,https://www.reddit.com/r/MachineLearning/comments/at4c98/d_setting_unrealistic_and_unreasonable/,GeekMonolith,1550762644,,0,1,False,default,,,,,
1384,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,1,at4zrg,i.redd.it,[D] Setting unreasonable bars for ML and data science experts. Is this true?,https://www.reddit.com/r/MachineLearning/comments/at4zrg/d_setting_unreasonable_bars_for_ml_and_data/,LaMasterShredder,1550766047,,0,1,False,https://b.thumbs.redditmedia.com/wb5Ln6G9d__ELf8SNlRL83BjD3wx7kYNrfnDubtP25c.jpg,,,,,
1385,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,1,at505y,self.MachineLearning,Do you think Kai Fu Lee is sincere in his compassion for people and their jobs? Or do you think he is planting a counter argument for the work that he has dedicated his life to? https://www.endwellproject.org/kai-fu-lee-phd-double-exposure-2/,https://www.reddit.com/r/MachineLearning/comments/at505y/do_you_think_kai_fu_lee_is_sincere_in_his/,twoguysandagpu,1550766105,[removed],0,1,False,self,,,,,
1386,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,1,at56yk,medium.com,[P] NAS-Generated Model Achieves SOTA In Super-Resolution,https://www.reddit.com/r/MachineLearning/comments/at56yk/p_nasgenerated_model_achieves_sota_in/,gwen0927,1550767029,,0,1,False,default,,,,,
1387,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,1,at5c6y,self.MachineLearning,What network should I select ??,https://www.reddit.com/r/MachineLearning/comments/at5c6y/what_network_should_i_select/,hnkz_hnkz,1550767727,[removed],0,1,False,self,,,,,
1388,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,2,at5iau,self.MachineLearning,[D] Does learning control theory helps to understand machine learning?,https://www.reddit.com/r/MachineLearning/comments/at5iau/d_does_learning_control_theory_helps_to/,tsauri,1550768555,"Anyone here think that control theory is actually useful for ML research?  
Control engineering folks seems to use many of the same maths ML people are using.  
Only that they do mostly online, low-dimensional (&lt;10) stuff but have strong theoretical guarantees and strive to get convexity whenever possible.

There are recurring themes in both fields but few are cross-pollinating the research, to name a few:

[Reinforcement learning is direct adaptive optimal control](https://ieeexplore.ieee.org/document/126844)  
[Model Predictive Control vs Reinforcement Learning](https://www.sciencedirect.com/science/article/pii/S2405896317311941)  
[Imitation Learning is somewhat System Identification](https://arxiv.org/abs/1806.04642)  
[Is Regression a kind of System Identificaiton](https://cs.stackexchange.com/questions/10134/machine-learning-vs-system-identification)  
Thinking back, Richard Bellman is a control theorist, as the guy behind HJB equation and whatever Bellman optimality these RL people are talking.",42,190,False,self,,,,,
1389,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,2,at5n4z,self.MachineLearning,How does it know that features belong to one subject?,https://www.reddit.com/r/MachineLearning/comments/at5n4z/how_does_it_know_that_features_belong_to_one/,airoscar,1550769217,[removed],0,1,False,self,,,,,
1390,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,2,at5wzj,self.MachineLearning,Structuring Legal Documents with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/at5wzj/structuring_legal_documents_with_deep_learning/,arnaudmiribel,1550770558,[removed],0,1,False,self,,,,,
1391,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,2,at60ep,prathamikshaikshikkhabar.blogspot.com,  :-:    -   -    :       ,https://www.reddit.com/r/MachineLearning/comments/at60ep/___________/,gop_15279,1550771017,,0,1,False,default,,,,,
1392,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,2,at67fv,self.MachineLearning,[D] Why not sample the posterior directly in Relevance Vector Machines (i.e. sparse Bayesian regression)?,https://www.reddit.com/r/MachineLearning/comments/at67fv/d_why_not_sample_the_posterior_directly_in/,soutioirsim,1550771961,"I have been trying to apply Relevance Vector Machines (RVMs) to some data in Python using pymc3. RVMs are sparse Bayesian regression models, which uses a hierarchical/multi-level model with individual hyperpriors on each regression weight. 

However, in the following papers by Mike Tipping

\- [Original paper.](http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf)

\- [Fast marginal-likelihood maximisation for RVM.](https://www.researchgate.net/profile/Anita_Faul/publication/2851008_Fast_Marginal_Likelihood_Maximisation_for_Sparse_Bayesian_Models/links/00b4952106168617be000000/Fast-Marginal-Likelihood-Maximisation-for-Sparse-Bayesian-Models.pdf)

an approximation is used such that the hyperprior parameters are estimated using type-II maximum likelihood. I don't quite understand this. The author claims they use this approximation because the original problem is analytically intractable.

...Why not just sample the posterior using NUTS or Metropolis? Once you have all the priors and hyperpriors setup, it seems pretty straightforward to do the sampling in pymc3. I feel like I'm missing something.

Is sampling the posterior too computationally intense?",1,3,False,self,,,,,
1393,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,3,at68db,self.MachineLearning,Recommendations for Synthetic Data Comparisons,https://www.reddit.com/r/MachineLearning/comments/at68db/recommendations_for_synthetic_data_comparisons/,ChemEngandTripHop,1550772082,[removed],0,1,False,self,,,,,
1394,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,3,at69p5,l7.curtisnorthcutt.com,Benchmarking all 34 Keras-TensorFlow and PyTorch pre-trained models (100% reproducible code),https://www.reddit.com/r/MachineLearning/comments/at69p5/benchmarking_all_34_kerastensorflow_and_pytorch/,cgnorthcutt,1550772257,,1,1,False,default,,,,,
1395,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,3,at6b0b,self.MachineLearning,Have Capsule Networks become the state of the art method on any ML task since their inception?,https://www.reddit.com/r/MachineLearning/comments/at6b0b/have_capsule_networks_become_the_state_of_the_art/,TheMoskowitz,1550772441,[removed],0,1,False,self,,,,,
1396,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,3,at6ckd,self.MachineLearning,[P] Quick Text-Generator with GPT2 on Pytorch,https://www.reddit.com/r/MachineLearning/comments/at6ckd/p_quick_textgenerator_with_gpt2_on_pytorch/,nlkey2022,1550772650,"Hello!

I referred code from openAi/gpt-2, huggingface/pytorch-pretrained-BERT and made simple Text-Generator with GPT-2 on Pytorch.

It is not training code, just testing Text-Generator with GPT-2 more simply code.

Thanks

[https://github.com/graykode/gpt-2-Pytorch](https://github.com/graykode/gpt-2-Pytorch)",6,13,False,self,,,,,
1397,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,3,at6dfs,protoexpress.com,AI and Machine Learning: Similar and Yet Different,https://www.reddit.com/r/MachineLearning/comments/at6dfs/ai_and_machine_learning_similar_and_yet_different/,LucySierraCircuits,1550772767,,0,1,False,https://b.thumbs.redditmedia.com/yvBa9FuIFkFy0xcDCssIs3SJdE0nZgX6fO0wCIhS-aw.jpg,,,,,
1398,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,3,at6is7,arxiv.org,[P] Random Search and Reproducibility for Neural Architecture Search,https://www.reddit.com/r/MachineLearning/comments/at6is7/p_random_search_and_reproducibility_for_neural/,yoavz,1550773497,,1,5,False,default,,,,,
1399,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,3,at6jtj,self.MachineLearning,[P] Tutorial: Generating Synthetic Data for Image Segmentation with Unity and PyTorch/fastai,https://www.reddit.com/r/MachineLearning/comments/at6jtj/p_tutorial_generating_synthetic_data_for_image/,stratospark,1550773638,"&amp;#x200B;

https://i.redd.it/hy5fbbajsyh21.jpg

Hey all, I've spent the last few months playing with domain randomization and synthetic data generation in Unity. I found it to be a great asset for machine learning explorations and wanted to spread awareness to the larger community. Since I'm assuming most ML developers haven't dabbled with game engines, my tutorial tries to walk them through the more idiosyncratic features of the Unity workflow.

In future lessons, I'd like to tackle randomized lighting, HDR textures, camera angles and lens characteristics, as well as importing 3d models and randomizing blendshapes. I'd also like to cover more realistic datasets like humans or cars. Or maybe the community has additional ideas?

Appreciate any feedback.

writeup: [https://blog.stratospark.com/generating-synthetic-data-image-segmentation-unity-pytorch-fastai.html](https://blog.stratospark.com/generating-synthetic-data-image-segmentation-unity-pytorch-fastai.html)

video tutorial: [https://youtu.be/P4CCMvtUohA](https://youtu.be/P4CCMvtUohA)

github repo: [https://github.com/stratospark/UnityImageSynthesisTutorial1](https://github.com/stratospark/UnityImageSynthesisTutorial1)",5,20,False,https://b.thumbs.redditmedia.com/ryrQkAf5kXaq6IWHV3mwGKs-VBRaCANuGfvUgmZK8ng.jpg,,,,,
1400,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,4,at72wk,self.MachineLearning,Why use Batch Normalization and where?,https://www.reddit.com/r/MachineLearning/comments/at72wk/why_use_batch_normalization_and_where/,Jandevries101,1550776283,"Hi guys,

&amp;#x200B;

So i've been working with ML lately and my network keeps on saturating (leaky relu&gt;tanh&gt;softmax) and i got reccomended to use Batch normalization, so i've read some things about batch norm on the internet, however i don't see any real downsides listed on the internet of using batch normalization? I mean every good thing has his cons right? 

&amp;#x200B;

Besides that where do you use batch normalization? My state is normalized, so the input layer is not needed, i was guessing in between the first and second layer, else the output layer?

&amp;#x200B;

Thanks for the explaination in advance,

&amp;#x200B;

Jan",0,1,False,self,,,,,
1401,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,4,at73o0,ai.stanford.edu,[D] In Favor of Developing Ethical Best Practices in AI Research,https://www.reddit.com/r/MachineLearning/comments/at73o0/d_in_favor_of_developing_ethical_best_practices/,regalalgorithm,1550776389,,10,36,False,default,,,,,
1402,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,4,at7a7y,blog.gigaspaces.com,"With the plethora of data sources, running AI and Machine Learning models needs in-memory processing to analyze billions of records, delivering super fast insights. Latency cost Amazon millions in lost sales. Were talking milliseconds.",https://www.reddit.com/r/MachineLearning/comments/at7a7y/with_the_plethora_of_data_sources_running_ai_and/,AliG68,1550777305,,0,1,False,https://b.thumbs.redditmedia.com/jEtVxK7sa8vHBrO4Hb7Ld9yddWNeio3QbvzhGiR0W_s.jpg,,,,,
1403,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,4,at7b8m,self.MachineLearning,"[D] Semantic segmentation for methane leak detection, does it make sense?",https://www.reddit.com/r/MachineLearning/comments/at7b8m/d_semantic_segmentation_for_methane_leak/,Horror_Counter,1550777446,"We're starting to apply AI in a heavy industry context and ideas have been brainstormed. One idea was to use ConvNets to detect methane leaks, by looking at thermal cameras images (actually, I think it would be thermal cameras _videos_, but one could start by looking at images sampled every now &amp; then). An image from one of these camera could look like this:

http://www.hazardexonthenet.net/article/107539/Massive-gas-leak-from-California-underground-storage-reservoir-causes-1-800-families-to-relocate.aspx

and you would likely look for red-plumey-thingies, which should be hot gas escaping from a storage site, or a well, etc.

https://www.eurekalert.org/pub_releases/2018-12/uov-nsf122018.php

Do you think the idea could make sense? These images are very different from the usual images on which one trains ConvNets (in particular, I think Fully Convolutional Networks could be used for this task), so I'm not sure how much help pretrained models would be, or to put it in another way, how much retraining a pretrained model would need, before getting a decent validation loss.

Now, I was wondering if there could be a way to at least estimate the size of the problem. In other words, if one could give some numbers for the dynamic range and the resolution of these thermograms, would there be any way to very roughly estimate the order of magnitude of the size of the dataset needed to build such a model? Or should one go the other way round, and say, given that I'm going to use this model (say, FCN-16), which has a certain capacity, how many images do I need to train it to a  certain accuracy? Is there any way to get some kind of estimate, or the only way is to try and see? Am I missing something obvious?   ",9,5,False,self,,,,,
1404,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,4,at7f8a,self.MachineLearning,[D] Political implications of AI text generators (OpenAI GPT-2),https://www.reddit.com/r/MachineLearning/comments/at7f8a/d_political_implications_of_ai_text_generators/,kirasolo,1550778000,"
[Justin Murphy](https://twitter.com/jmrphy), a political scientist who runs a blog/podcast covering topics at the intersection of media and technology, has released [a video discussing some potential implications of GP-2 and future models](https://www.youtube.com/watch?v=2hpB_H_QMRk). His focus is on not on super-intelligent text generation AI, but a more plausible proximal future where such models attain summarization and essay-writing skills surpassing ""average"" human coherency.",0,1,False,self,,,,,
1405,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,4,at7ltg,medium.com,"Exploring virtual worlds with reinforcement learning. InThePocket consists of multiple teams, among which a team focussing on AR and another on AI. The first is working with Unity, while the latter knows all about TensorFlow. Recently they joined forces to learn an Agent navigate our virtual office.",https://www.reddit.com/r/MachineLearning/comments/at7ltg/exploring_virtual_worlds_with_reinforcement/,khelsens,1550778908,,0,2,False,default,,,,,
1406,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,5,at7v5l,kanoki.org,How to Automate Customer Support : A Machine Learning Approach,https://www.reddit.com/r/MachineLearning/comments/at7v5l/how_to_automate_customer_support_a_machine/,min2bro,1550780207,,0,1,False,default,,,,,
1407,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,5,at8802,self.MachineLearning,[D] Prestige of interdisciplinary journals for ML,https://www.reddit.com/r/MachineLearning/comments/at8802/d_prestige_of_interdisciplinary_journals_for_ml/,doctorjuice,1550782008,"My work is interdisciplinary at the intersection of machine learning and computational neuroscience. For this reason, I am wondering if journals such Frontiers in Computational Neuroscience, PNAS, etc. may be a better fit.

&amp;#x200B;

How does the ML community generally view the prestige of such papers? Does it compare to JMLR, ICML, NeurIPS, etc.? Would an ML researcher prefer to publish is these top tier general journals or would it be more appealing to publish in, say, ICML? I would imagine that the top conferences give you more exposure.

&amp;#x200B;

Thanks for any input.",7,2,False,self,,,,,
1408,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,5,at88u7,self.MachineLearning,"I'm from India and Should I do 1 month academic internship by STMI, NUS in Big Data Analytics using Artificial Neural Networks at S$3990?",https://www.reddit.com/r/MachineLearning/comments/at88u7/im_from_india_and_should_i_do_1_month_academic/,Rkpandey123,1550782121,[removed],0,1,False,self,,,,,
1409,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,6,at8o9w,self.MachineLearning,[D] ML Research for GPS signals-specifically multipath mitigation?,https://www.reddit.com/r/MachineLearning/comments/at8o9w/d_ml_research_for_gps_signalsspecifically/,FluffdaddyFluff,1550784281,"Does anyone know of any papers that have utilized GPS signal data with ML applications? I know that Kalman filters are worshipped in the GPS world, but I'd like to see if ML can be utilized to classify multipath signals",0,1,False,self,,,,,
1410,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,6,at8rdg,realclearscience.com,100-Year-Old Ideas About Geometry Are Reshaping Big Data,https://www.reddit.com/r/MachineLearning/comments/at8rdg/100yearold_ideas_about_geometry_are_reshaping_big/,rieslingatkos,1550784726,,0,1,False,https://b.thumbs.redditmedia.com/zKp3aP2uMSZ5lpbUA79zRJ4Koy3bQ0C5lU8mlMCucwk.jpg,,,,,
1411,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,6,at9329,self.MachineLearning,"Oriol Vinyals Talk at BU, AlphaStar: Mastering the Real-Time Strategy Game StarCraft II",https://www.reddit.com/r/MachineLearning/comments/at9329/oriol_vinyals_talk_at_bu_alphastar_mastering_the/,ch3njust1n,1550786376," 

Date/Time: March 11th 11 AM

Location: Boston University (room TBA)

ABSTRACT: Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a grand challenge for AI research.

In this talk, I will introduce our StarCraft II program AlphaStar, the first Artificial Intelligence to defeat a top professional player. In a series of test matches held on 19 December, AlphaStar decisively beat Team Liquids Grzegorz ""MaNa"" Komincz, one of the worlds strongest professional StarCraft players, 5-1, following a successful benchmark match against his team-mate Dario TLO Wnsch. The matches took place under professional match conditions on a competitive ladder map and without any game restrictions.",0,1,False,self,,,,,
1412,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,7,at943j,self.MachineLearning,Health Data Science/ML Masters Degree,https://www.reddit.com/r/MachineLearning/comments/at943j/health_data_scienceml_masters_degree/,MeanMantaray,1550786520,[removed],0,1,False,self,,,,,
1413,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,7,at99lh,self.MachineLearning,[R] Oriol Vinyals AlphaStar: Mastering the Real-Time Strategy Game StarCraft II Talk at Boston University,https://www.reddit.com/r/MachineLearning/comments/at99lh/r_oriol_vinyals_alphastar_mastering_the_realtime/,MICInc,1550787317,"Date/Time: March 11th, 11 AM  
Location: Boston University (Building and Room TBA)  


ABSTRACT: Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a grand challenge for AI research.

This talk will introduce the StarCraft II program AlphaStar, the first Artificial Intelligence to defeat a top professional player. In a series of test matches held on 19 December, AlphaStar decisively beat Team Liquids Grzegorz ""MaNa"" Komincz, one of the worlds strongest professional StarCraft players, 5-1, following a successful benchmark match against his team-mate Dario TLO Wnsch. The matches took place under professional match conditions on a competitive ladder map and without any game restrictions.

&amp;#x200B;

BIO: Oriol Vinyals is a Sr. Staff Research Scientist at Google DeepMind, working in Deep Learning  and Artificial Intelligence. Prior to joining DeepMind, Oriol was part of the Google Brain team. He holds a Ph.D. in EECS from the University of California, Berkeley and is a recipient of the 2016 MIT TR35 innovator award. His research has been featured multiple times at the New York Times, BBC, etc., and his articles have been cited over 36000 times. Some of his contributions are used in Google Translate, Text-To-Speech, and Speech recognition, serving billions of queries every day, and he was the lead researcher of the AlphaStar project, creating an agent that defeated a top professional at the game of StarCraft. At DeepMind he continues working on his areas of interest, which include artificial intelligence, with particular emphasis on machine learning, deep learning and reinforcement learning.",15,23,False,self,,,,,
1414,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,7,at9hmh,self.MachineLearning,[Project] Hierarchical Mult-task Learning NLP model using PyTorch and AllenNLP library,https://www.reddit.com/r/MachineLearning/comments/at9hmh/project_hierarchical_multtask_learning_nlp_model/,ceceshao1,1550788512,"Based on PyTorch and AllenNLP library, this is a super cool model that *combines four fundamental NLP/NLU tasks* (Named Entity Recognition, Entity Mention Detection, Relation Extraction, and Coreference Resolution). By training on these four tasks together, the model can learn learn richer representations.

&amp;#x200B;

Seems like awesome work from the HuggingFace team and wondering if anyone has tried it? 

&amp;#x200B;

\[Excerpt from [the blog post](https://medium.com/huggingface/beating-the-state-of-the-art-in-nlp-with-hmtl-b4e1d5c3faf) introducing the model, code, and paper\]

&gt;""Traditionally, a specific model was independently trained for each of these NLP tasks (Named-Entity Recognition, Entity Mention Detection, Relation Extraction, Coreference Resolution).  
&gt;  
&gt;In the case of HMTL, all these results are given by a **single model**, in a **single forward pass**!  
&gt;  
&gt;But Multi-Task Learning is more than just a way to reduce the computation by using a single model instead of several models.  
&gt;  
&gt;Multi-Task Learning (MTL) can be used to encourage the model to learn embeddings that can be shared between different tasks. One fundamental motivation behind Multi-Task Learning is that related (or loosely related) tasks can **benefit** from each other by inducing richer representations.""

&amp;#x200B;

Online demo - [https://huggingface.co/hmtl/](https://huggingface.co/hmtl/)

arXiv paper - [https://arxiv.org/abs/1811.06031](https://arxiv.org/abs/1811.06031)

Github repo - [https://github.com/huggingface/hmtl](https://github.com/huggingface/hmtl)

&amp;#x200B;

Screenshot from the online demo  

https://i.redd.it/uwmwyo5400i21.png",0,11,False,self,,,,,
1415,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,8,at9ufi,self.MachineLearning,"[R] Call for Papers (CfP) for the Workshop on ""Structure and Priors in Reinforcement Learning"" (SPiRL) at ICLR 2019",https://www.reddit.com/r/MachineLearning/comments/at9ufi/r_call_for_papers_cfp_for_the_workshop_on/,1abhigupta,1550790447,"We're excited to share the call for papers for our upcoming workshop on structure and priors in reinforcement learning at ICLR 2019. The submission deadline for a 5-page extended abstract is 3/7.

Best, SPiRL Co-organizers

Workshop on Structure &amp; Priors in Reinforcement Learning (SPiRL) at ICLR 2019 Monday, May 6th, 09:00 AM  06:00 PM CST Room R4, Ernest N. Morial Convention Center, New Orleans http://spirl.info/2019/

Call for Extended Abstracts

A powerful solution to the problem of generalization and sample complexity in reinforcement learning (RL) is the deliberate use of inductive bias. There has been a recent resurgence of interest in methods of imposing or learning inductive bias in RL in the form of structure and priors, including, for example, prior distributions for Bayesian inference, learned hyperparameters in a multi-task or meta-learning setup, or structural constraints such as temporal abstraction or hierarchy.

The goal of this workshop is to bring together researchers across a variety of domains, including RL and machine learning practitioners, neuroscientists, and cognitive scientists, to discuss the role that structure and priors play in RL. We invite the submission of abstracts on topics including, but not limited to:

Bayesian inference as used in RL
meta-RL
transfer learning in RL
modularity and compositionality
hierarchical RL
temporal abstraction
structured state and action abstractions
sequential decision-making in humans
reinforcement processes in the brain
We also invite abstracts that address the following questions directly:

What is the trade-off between generality and the use of structure and priors in RL, in the context of specific tasks or in general, and how can we evaluate this in practice?

What are the practical or theoretical implications of specific ways of imposing or learning structure or priors in RL?

How can we learn data-driven structure and priors for RL (via transfer in RL, meta-RL, or multi-task RL)?

How can the different communities (including cognitive science, neuroscience, and machine learning) benefit from collaborative research on these topics?

Important Dates

Extended abstract deadline: Thursday, March 7th, 2019, 11:59 PM anywhere on Earth Decision notification: Thursday, March 28th, 2019 Camera-ready deadline: Thursday, May 2nd, 2019, 11:59 PM anywhere on Earth

Abstract Format

Extended abstracts should be a short research paper of at most 5 pages long (excluding references or appendix) in PDF format. Abstracts must be anonymized; the review process will be double-blind.

Please see the CfP on the workshop website (http://spirl.info/2019/call/) for more details.

Please submit your extended abstracts via CMT (https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FSPiRLICLR2019) by the deadline given above.

Presentation Details

All accepted abstracts will be presented in the form of a poster. A few select contributions will additionally be given as contributed talks. Accepted papers will be posted in a non-archival format on the workshop website.

Workshop Committee

Pierre-Luc Bacon (Stanford) Marc Deisenroth (Imperial College London) Chelsea Finn (UC Berkeley/Google Brain/Stanford) Erin Grant (UC Berkeley) Tom Griffiths (Princeton) Abhishek Gupta (UC Berkeley) Nicolas Heess (DeepMind) Michael Littman (Brown) Junhyuk Oh (DeepMind)

If you have any further questions, please contact the SPiRL 2019 committee at organizers@spirl.info.",0,11,False,self,,,,,
1416,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,8,at9vyi,self.MachineLearning,Raspi + FPGA accelerator,https://www.reddit.com/r/MachineLearning/comments/at9vyi/raspi_fpga_accelerator/,dadrake3,1550790692,I am looking to implement an edge ML inferencing accelerator for the raspi. I am wondering if it is possible to use leflow or xilinx ml-suite to compile a TensorFlow graph to run on the spartan 6 FPGA. I am not sure where to start and I am wondering if this would be possible,0,1,False,self,,,,,
1417,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,8,ata5vq,youtube.com,Is Two Minute Papers the best channel on ML papers on YouTube?,https://www.reddit.com/r/MachineLearning/comments/ata5vq/is_two_minute_papers_the_best_channel_on_ml/,SilentDifficulty,1550792240,,0,1,False,default,,,,,
1418,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,8,ata95q,self.MachineLearning,[Discussion] Any open sourced models trained on datasets larger than Imagenet?,https://www.reddit.com/r/MachineLearning/comments/ata95q/discussion_any_open_sourced_models_trained_on/,Moondra2017,1550792734,"I have been using transfer learning on the popular open sourced models (Inception, resnet, xception, etc) but I was wondering if any models trained on datasets larger than Imagenet have been released?

It seems this one hasn't been released yet(?):

[https://code.fb.com/ml-applications/advancing-state-of-the-art-image-recognition-with-deep-learning-on-hashtags/](https://code.fb.com/ml-applications/advancing-state-of-the-art-image-recognition-with-deep-learning-on-hashtags/)",2,2,False,self,,,,,
1419,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,8,ataam0,markgacoka.com,Stock Market Prediction: Train a neural network to predict stock prices,https://www.reddit.com/r/MachineLearning/comments/ataam0/stock_market_prediction_train_a_neural_network_to/,SilentDifficulty,1550792965,,0,1,False,default,,,,,
1420,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,9,atajdy,self.MachineLearning,[P] DeepMind MuJoCo Multi-Agent Soccer Environment,https://www.reddit.com/r/MachineLearning/comments/atajdy/p_deepmind_mujoco_multiagent_soccer_environment/,baylearn,1550794357,"DeepMind released an [environment](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer) as part of their paper, [Emergent Coordinated Multi-Agent Behaviors through Competition](https://sites.google.com/view/emergent-coordination/home) (ICLR 2019)

https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer

DeepMind MuJoCo Multi-Agent Soccer Environment:

https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer

Emergent Coordination Through Competition:

https://arxiv.org/abs/1902.07151
",0,14,False,self,,,,,
1421,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,9,atan6r,self.MachineLearning,Classification times for general datasets,https://www.reddit.com/r/MachineLearning/comments/atan6r/classification_times_for_general_datasets/,apvast,1550794967,[removed],0,1,False,self,,,,,
1422,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,10,atb7cv,youtube.com,"Facial Recognition Explanation using ""Where's Waldo""",https://www.reddit.com/r/MachineLearning/comments/atb7cv/facial_recognition_explanation_using_wheres_waldo/,SharathCK,1550798312,,0,1,False,default,,,,,
1423,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,10,atbbpe,self.MachineLearning,Interested in applications of neuroevolution.,https://www.reddit.com/r/MachineLearning/comments/atbbpe/interested_in_applications_of_neuroevolution/,coffee-shop-ghost,1550799042,[removed],0,1,False,self,,,,,
1424,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,10,atbdz0,self.MachineLearning,how can i predict next day stock price (time series),https://www.reddit.com/r/MachineLearning/comments/atbdz0/how_can_i_predict_next_day_stock_price_time_series/,GoBacksIn,1550799425,[removed],0,1,False,self,,,,,
1425,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,10,atbkig,self.MachineLearning,Father's Day AI Program,https://www.reddit.com/r/MachineLearning/comments/atbkig/fathers_day_ai_program/,jakemalis,1550800549,[removed],0,1,False,self,,,,,
1426,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,10,atbli2,github.com,[P] m2cgen - simple way to generate native code (Java/Python/C) from trained ML models (Scikit-learn/XGBoost/LightGBM),https://www.reddit.com/r/MachineLearning/comments/atbli2/p_m2cgen_simple_way_to_generate_native_code/,krinart,1550800720,,0,1,False,default,,,,,
1427,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,11,atblz5,self.MachineLearning,[D] Exploiting randomness in initialization and training without dropout for prediction intervals,https://www.reddit.com/r/MachineLearning/comments/atblz5/d_exploiting_randomness_in_initialization_and/,syrahshiraz,1550800803,"There's some literature (and controversy) around interpreting dropout from a Bayesian perspective to arrive at distributions of the prediction outcomes, e.g. [https://arxiv.org/abs/1506.02142](https://arxiv.org/abs/1506.02142), [https://arxiv.org/abs/1807.01969](https://arxiv.org/abs/1807.01969) to list a couple, also this thread [https://www.reddit.com/r/MachineLearning/comments/7bm4b2/d\_what\_is\_the\_current\_state\_of\_dropout\_as/](https://www.reddit.com/r/MachineLearning/comments/7bm4b2/d_what_is_the_current_state_of_dropout_as/). Has there been research on the approach of just initializing and training the network a bunch of times? For some problems this wouldn't be prohibitively expensive but I wonder if there's any theoretical/empirical justification.",2,3,False,self,,,,,
1428,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,11,atbruj,self.MachineLearning,"[P] For NLP beginners, simple PyTorch implementation of Neural Machine Translation(NMT), Sentiment Analysis and Text Classification",https://www.reddit.com/r/MachineLearning/comments/atbruj/p_for_nlp_beginners_simple_pytorch_implementation/,lyeoni,1550801798,"A step-by-step tutorial on how to implement and adapt to the simple real-word NLP task.

* news-category-classifcation:  This repo contains a simple source code for text-classification based on TextCNN. Corpus is **Huffpost**  news category dataset in English. Most open sources are a bit difficult  to study &amp; make text-classification model for beginners. So, I hope  that this repo can be a good solution for people who want to have their  own text-classification model.
* movie-rating-classification:  This repo contains a simple source code for text-classification based on TextCNN. Corpus is **movie review**  dataset in the Korean language. Most open sources are a bit difficult  to study &amp; make text-classification model for beginners. So, I hope  that this repo can be a good solution for people who want to have their  own text-classification model.
*  This repository provides a simple PyTorch implementation of Neural Machine Translation, along with an intrinsic/extrinsic comparison of various sequence-to-sequence (seq2seq) models in translation. ",20,256,False,self,,,,,
1429,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,13,atcxv9,self.deeplearning,Standard benchmarks for transfer learning,https://www.reddit.com/r/MachineLearning/comments/atcxv9/standard_benchmarks_for_transfer_learning/,r2m2,1550809003,,0,1,False,default,,,,,
1430,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,13,atd58z,self.MachineLearning,Machine Learning Tutorial - Complete Machine Learning Course for Beginners,https://www.reddit.com/r/MachineLearning/comments/atd58z/machine_learning_tutorial_complete_machine/,Corey890,1550810384,[http://on.morioh.net/f337e888c9](http://on.morioh.net/f337e888c9),0,1,False,self,,,,,
1431,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,13,atd7jl,self.MachineLearning,[D]Radeon VII Tensorflow Deep Learning results - Huge improvement from Vega FE,https://www.reddit.com/r/MachineLearning/comments/atd7jl/dradeon_vii_tensorflow_deep_learning_results_huge/,SandboChang,1550810806,"\*This was crosspost from r/AMD earlier, but it didn't seem to work.

&amp;#x200B;

So finally with the help from ROCm developers (which pointed out something newbie like me didn't know LOL), I was able to select which GPU to run the tensorflow benchmarks on using the benchmark script here:

[https://github.com/tensorflow/benchmarks/tree/master/scripts/tf\_cnn\_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks)

&amp;#x200B;

This can be done by running the tf\_cnn\_benchmark.py in the directory /benchmarks/scripts/tf\_cnn\_benchmarks/, e.g.

    python3 tf_cnn_benchmarks.py --num_gpus=1 --batch_size=32 --model=vgg16 --variable_update=parameter_server --use_fp16=true

Evidently, you can change the mode/batch\_size and use\_fp16 to toggle between the settings. Additionally, you can add HIP\_VISIBLE\_DEVICES=# in front of the python/python3 to select your GPU to run, if you are running ROCm. (CUDA has an equivalence)

&amp;#x200B;

The test is done on a system with

* AMD Vega FE\*2
* AMD Radeon VII
* ubuntu 18.04 with kernel 4.18
* ROCm 2.1
* Tensorflow 1.12

&amp;#x200B;

\*Empty entires either mean the test failed to run or it wasn't set to run by the script. No results were generated.

Radeon VII (unit: images/sec)

|FP16|CNN|Alexnet|inception3|resnet50|resnet152|vgg16|
|:-|:-|:-|:-|:-|:-|:-|
|Batch size|||||||
|32||1178.95|128.34|294.18|116.80|180.28|
|64||1582.43|139.62|348.20|137.72|193.41|
|128||1849.70|140.42|381.04|147.15|192.36|
|256||2001.21|111.91|382.80||164.18|
|512||2063.58|||||

&amp;#x200B;

|FP32|CNN|Alexnet|inception3|resnet50|resnet152|vgg16|
|:-|:-|:-|:-|:-|:-|:-|
|Batch size|||||||
|32||1115.77|126.2|212.38|82.82|120.71|
|64||1514.52|134.19|247.21|95.98|127.24|
|128||1739.77|133.03|261.14||126.01|
|256||1873.65|||||
|512||1928.08|||||

&amp;#x200B;

Vega FE (unit: images/sec)

|FP16|CNN|Alexnet|inception3|resnet50|resnet152|vgg16|
|:-|:-|:-|:-|:-|:-|:-|
|Batch size|||||||
|32||678.13|50.54|155.08|59.78|58.45|
|64||864.63|52.90|174.94|69.02|57.55|
|128||982.19|53.20|183.10|73.14|60.94|
|256||1027.13|47.13|177.69||61.49|
|512|||||||

&amp;#x200B;

|FP32|CNN|Alexnet|inception3|resnet50|resnet152|vgg16|
|:-|:-|:-|:-|:-|:-|:-|
|Batch size|||||||
|32||861.86|90.05|154.61|63.49|78.89|
|64||1085.92|95.89|171.00|68.10|82.54|
|128||1240.12|92.23|179.14||82.38|
|256||1279.26|||||
|512|||||||

&amp;#x200B;",7,10,False,self,,,,,
1432,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,14,atdr7v,self.MachineLearning,Speed up Gazebo simulations for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/atdr7v/speed_up_gazebo_simulations_for_deep/,Santosh16k,1550814324,[removed],0,1,False,self,,,,,
1433,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,14,atdt3x,self.MachineLearning,[D] Meta-Learning in 50 Lines of JAX,https://www.reddit.com/r/MachineLearning/comments/atdt3x/d_metalearning_in_50_lines_of_jax/,hardmaru,1550814697,"An introduction to ""what is meta-learning"" and a tutorial on implementing MAML in 50 lines of JAX:

https://blog.evjang.com/2019/02/maml-jax.html

https://github.com/ericjang/maml-jax",3,57,False,self,,,,,
1434,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,15,ateaob,self.MachineLearning,Is there any software I can use to calculate partial derivatives for my neural network for gradient descend ?,https://www.reddit.com/r/MachineLearning/comments/ateaob/is_there_any_software_i_can_use_to_calculate/,ahmedoy,1550818075,[removed],0,1,False,self,,,,,
1435,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,15,atebq8,arxiv.org,[R] Random Search and Reproducibility for Neural Architecture Search: Random Search is surprisingly competitive with state-of-the-art NAS methods,https://www.reddit.com/r/MachineLearning/comments/atebq8/r_random_search_and_reproducibility_for_neural/,downtownslim,1550818287,,9,25,False,default,,,,,
1436,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,16,atei0l,self.MachineLearning,[P] The evolution and expanding utility of Ray,https://www.reddit.com/r/MachineLearning/comments/atei0l/p_the_evolution_and_expanding_utility_of_ray/,machinelearningdis,1550819550,"[https://www.oreilly.com/ideas/the-evolution-and-expanding-utility-of-ray](https://www.oreilly.com/ideas/the-evolution-and-expanding-utility-of-ray)

&amp;#x200B;

*In a* [*recent post*](https://www.oreilly.com/ideas/notes-from-the-first-ray-meetup)*, I listed some of the early use cases described in the first meetup dedicated to* [*Ray*](https://github.com/ray-project/ray)*a distributed programming framework from UC Berkeleys* [*RISE Lab*](https://rise.cs.berkeley.edu/)*. A second meetup took place a few months later, and both events featured some of the first applications built with Ray. On the development front, the core API has stabilized and a lot of work has gone into improving Rays performance and stability. The project now has around* [*5,700 stars on GitHub*](https://github.com/ray-project/ray) *and more than 100 contributors across many organizations.*

*...Libraries on top of Ray are already appearing:* [*RLlib*](http://rllib.io/) *(scalable reinforcement learning),* [*Tune*](https://ray.readthedocs.io/en/latest/tune.html) *(a hyperparameter optimization framework), and a soon-to-be-released library for streaming are just a few examples.* 

&amp;#x200B;

Is anyone using this for RL?",1,4,False,self,,,,,
1437,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,16,ateiig,self.MachineLearning,Classify EEG signals of different lengths?,https://www.reddit.com/r/MachineLearning/comments/ateiig/classify_eeg_signals_of_different_lengths/,dmdaksh,1550819654,[removed],0,1,False,self,,,,,
1438,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,16,ateu6i,self.MachineLearning,Word2vec and Opposites,https://www.reddit.com/r/MachineLearning/comments/ateu6i/word2vec_and_opposites/,fbgc,1550822207,[removed],0,1,False,self,,,,,
1439,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,17,atf14s,blockdelta.io,Artificial Intelligence - My Own Jarvis!,https://www.reddit.com/r/MachineLearning/comments/atf14s/artificial_intelligence_my_own_jarvis/,BlockDelta,1550823741,,0,1,False,https://b.thumbs.redditmedia.com/Yz1yxDWn3rSPUVTgY-AW5gnWZifeqiMzx_0HkFL7I9s.jpg,,,,,
1440,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,17,atf9c9,self.MachineLearning,Unseen objects detection,https://www.reddit.com/r/MachineLearning/comments/atf9c9/unseen_objects_detection/,amiiriismaiil,1550825637,[removed],0,1,False,self,,,,,
1441,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,18,atfi0n,twitter.com,SPECIAL Offer The Complete SQL Bootcamp DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/atfi0n/special_offer_the_complete_sql_bootcamp_discount/,monarchuk92,1550827608,,0,1,False,default,,,,,
1442,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,18,atfl7f,producthunt.com,Human Or AI - Can you guess which image is of a real person vs AI?,https://www.reddit.com/r/MachineLearning/comments/atfl7f/human_or_ai_can_you_guess_which_image_is_of_a/,N4derr,1550828327,,0,1,False,https://b.thumbs.redditmedia.com/guC4RNWmPTJJnkSxXIOFRP4sLWHTtB_vuIu5q2xxH_w.jpg,,,,,
1443,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,18,atfokv,christophm.github.io,[D] Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (1st edition now published),https://www.reddit.com/r/MachineLearning/comments/atfokv/d_interpretable_machine_learning_a_guide_for/,alexeyr,1550829093,,0,1,False,default,,,,,
1444,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,19,atft7j,quytech.com,Hire AngularJS Developers in India,https://www.reddit.com/r/MachineLearning/comments/atft7j/hire_angularjs_developers_in_india/,hiwilliam31,1550830104,,0,1,False,https://b.thumbs.redditmedia.com/XiUNjf6j39zk1d38rY5cFZdHz-8qy0cBeEp4omrvAWg.jpg,,,,,
1445,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,19,atg465,github.com,Data dependency manager for data science projects. It controls versions of your data artifacts and stores them on AWS S3.,https://www.reddit.com/r/MachineLearning/comments/atg465/data_dependency_manager_for_data_science_projects/,apls777,1550832434,,0,1,False,default,,,,,
1446,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,19,atg6ay,compakk.blogspot.com,https://compakk.blogspot.com/2019/02/stretch-wrap-machine-a-useful-machine-in-packaging-industry.html,https://www.reddit.com/r/MachineLearning/comments/atg6ay/httpscompakkblogspotcom201902stretchwrapmachineaus/,compak03,1550832858,,0,1,False,default,,,,,
1447,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,19,atg6l2,twitter.com,SPECIAL Offer Data Science and Machine Learning Bootcamp with R DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/atg6l2/special_offer_data_science_and_machine_learning/,coolzhuridova,1550832914,,0,1,False,default,,,,,
1448,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,20,atg8te,i.redd.it,Face Recognition - A Modern Way of Security,https://www.reddit.com/r/MachineLearning/comments/atg8te/face_recognition_a_modern_way_of_security/,hiwilliam31,1550833364,,0,1,False,default,,,,,
1449,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,20,atgale,self.MachineLearning,What are the various conferences related to ML?,https://www.reddit.com/r/MachineLearning/comments/atgale/what_are_the_various_conferences_related_to_ml/,shamoons,1550833690,[removed],0,1,False,self,,,,,
1450,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,21,atgtpd,self.MachineLearning,WGAN Loss function,https://www.reddit.com/r/MachineLearning/comments/atgtpd/wgan_loss_function/,sergeybok,1550837309,[removed],0,1,False,self,,,,,
1451,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,21,atgww5,self.MachineLearning,[Project] Cool AI project for dad's birthday,https://www.reddit.com/r/MachineLearning/comments/atgww5/project_cool_ai_project_for_dads_birthday/,jakemalis,1550837889,"Hello!

My dad's birthday is tommorow and I want to create a cool AI project for him (he also researches AI). Any ideas for his birthday? P.S. he is 53 in case it matters.",7,4,False,self,,,,,
1452,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,21,ath4wd,self.MachineLearning,[D]Pls correct the mistakes!,https://www.reddit.com/r/MachineLearning/comments/ath4wd/dpls_correct_the_mistakes/,ArtThreemis,1550839284,"Theoretically Principled Trade-off between Robustness and Accuracy

[https://arxiv.org/abs/1901.08573](https://arxiv.org/abs/1901.08573)

&amp;#x200B;

https://i.redd.it/pp81f32s74i21.png",0,1,False,https://b.thumbs.redditmedia.com/EO8zccF08MfYiR__Xwzmpdqu9g2SxOFCI_Pv_0sSCyc.jpg,,,,,
1453,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,21,ath710,self.MachineLearning,Learn a neural network using boosting principles,https://www.reddit.com/r/MachineLearning/comments/ath710/learn_a_neural_network_using_boosting_principles/,strojax,1550839654,[removed],0,1,False,self,,,,,
1454,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,21,ath771,humanorai.net,[P] Human Or AI: Can you guess which image is of a real person vs AI?,https://www.reddit.com/r/MachineLearning/comments/ath771/p_human_or_ai_can_you_guess_which_image_is_of_a/,N4derr,1550839685,,0,1,False,default,,,,,
1455,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,21,ath7r5,self.MachineLearning,What's the best library for CPU and GPU machine learning?,https://www.reddit.com/r/MachineLearning/comments/ath7r5/whats_the_best_library_for_cpu_and_gpu_machine/,qudcjf7928,1550839784,[removed],0,1,False,self,,,,,
1456,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,22,athnod,self.MachineLearning,Using Designed Experiments to effectively generate Neural Network training data,https://www.reddit.com/r/MachineLearning/comments/athnod/using_designed_experiments_to_effectively/,IAMA_monkey,1550842460,[removed],0,1,False,self,,,,,
1457,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,23,ati0aa,apriorit.com,How to Implement Artificial Intelligence for Solving Image Processing Tasks,https://www.reddit.com/r/MachineLearning/comments/ati0aa/how_to_implement_artificial_intelligence_for/,RyanTmthn,1550844511,,0,1,False,https://b.thumbs.redditmedia.com/HV5ia0p4kcDJy4frMfKNZbCxNbB5EXjym6ZVwusG_9U.jpg,,,,,
1458,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,23,ati1zp,self.MachineLearning,RL - Sparse Reward Design Solutions?,https://www.reddit.com/r/MachineLearning/comments/ati1zp/rl_sparse_reward_design_solutions/,Jandevries101,1550844779,[removed],0,1,False,self,,,,,
1459,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,23,ati2xk,self.MachineLearning,[D] How to get into PyTorch from TF?,https://www.reddit.com/r/MachineLearning/comments/ati2xk/d_how_to_get_into_pytorch_from_tf/,veqtor,1550844929,"I've been using TensorFlow for about 3 years now, and some stuff is just almost impossible, at least right now (differentiable rendering) so I'm looking into learning PyTorch, any good tutorials for those who are transitioning from other frameworks? ",9,5,False,self,,,,,
1460,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,23,ati9q9,self.MachineLearning,[R] Simplifying Graph Convolutional Networks (linear model beats Graph NNs),https://www.reddit.com/r/MachineLearning/comments/ati9q9/r_simplifying_graph_convolutional_networks_linear/,gadfly_,1550846032,"[https://arxiv.org/abs/1902.07153](https://arxiv.org/abs/1902.07153)

Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",16,100,False,self,,,,,
1461,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,23,atiasl,targetbase.com,[R] An Overview of AutoML and Available Technology,https://www.reddit.com/r/MachineLearning/comments/atiasl/r_an_overview_of_automl_and_available_technology/,pp314159,1550846208,,0,1,False,default,,,,,
1462,MachineLearning,t5_2r3gv,2019-2-22,2019,2,22,23,atibha,datacenterknowledge.com,Swim Open Sources Its Machine Learning Platform for Edge Computing,https://www.reddit.com/r/MachineLearning/comments/atibha/swim_open_sources_its_machine_learning_platform/,CrankyBear,1550846320,,0,1,False,default,,,,,
1463,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,0,atimd3,self.MachineLearning,[D] Best approach for deploying thousands of forecasting models to production?,https://www.reddit.com/r/MachineLearning/comments/atimd3/d_best_approach_for_deploying_thousands_of/,FeelTheDataBeTheData,1550847983,"Anyone here have any experience in deploying models to production? We deal a lot with retail sales data and we want to be able to forecast sales for any/all 10s of thousands of items. Pretty well versed in Python so I have no problem generating the code needed to rip through the data to create the time series models (Prophet), but with time series data, any time we forecast, we will need to retrain the models since new data would be available. I have no experience in deploying and maintain that many models. We use the Azure Cloud platform. Any tips are appreciated. Thank you.

I have seen so many articles relating to deploying a single model, but is usually not a time series model. This situation would be akin to the [Rossman Sales Data on Kaggle](https://www.kaggle.com/c/rossmann-store-sales). They want to forecast sales for 1000 of their stores each with its own sales data. Didn't see any discussions on there regarding actual deployment of those approaches in practice.

We want to either have an endpoint stood up that will grab the trained model for that week and spit back the forecast, but if that is all it does then we might as well do a bulk update and persist the results. We could do it just-in-time and only update the model as it is used and have a model expire after a while.

Just not sure on best practices for this type of problem. How do companies forecast for all of their items like at Wal-Mart and Amazon? Thank you.",33,18,False,self,,,,,
1464,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,0,atimjd,self.MachineLearning,Which papers are the best introduction to graph neural networks?,https://www.reddit.com/r/MachineLearning/comments/atimjd/which_papers_are_the_best_introduction_to_graph/,Paddapa,1550848007,[removed],0,1,False,self,,,,,
1465,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,0,atj4us,medium.com,Oxford University AI Policy Researcher Says Trumps AI Initiative Falls Short on Immigration and,https://www.reddit.com/r/MachineLearning/comments/atj4us/oxford_university_ai_policy_researcher_says/,Yuqing7,1550850711,,0,1,False,https://a.thumbs.redditmedia.com/rSLbM6iz7Sm0ZiuuoOS_hnEijvoBofyhJRn25qy6aT8.jpg,,,,,
1466,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,0,atj651,github.com,Deep Learning Machine (Ansible script for automatically raising a Jupyter box with TF GPU),https://www.reddit.com/r/MachineLearning/comments/atj651/deep_learning_machine_ansible_script_for/,LukeArrigoni,1550850890,,0,1,False,default,,,,,
1467,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,0,atj77v,self.MachineLearning,What kind of stack do I need for a mobile app to use my recommendation engine via API?,https://www.reddit.com/r/MachineLearning/comments/atj77v/what_kind_of_stack_do_i_need_for_a_mobile_app_to/,TheGasBoi,1550851052,[removed],0,1,False,self,,,,,
1468,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,1,atjbv0,self.MachineLearning,[D] What kind of stack do I need for a mobile app to use my recommendation engine via API?,https://www.reddit.com/r/MachineLearning/comments/atjbv0/d_what_kind_of_stack_do_i_need_for_a_mobile_app/,TheGasBoi,1550851696,"Hey everyone,

I made a recommendation engine in R that Id love to make accessible by mobile app. Im new to this though and could use some help understanding how to make this happen. Not sure what the stack should look like. Heres what I think that I need but please let me know if Im missing anything:

* mobile client
* Amazon API Gateway to facilitate communication between mobile client and lambda function
* Amazon lambda function to run the recommendation engine written in R
* S3 bucket to hold the csv of items

Are these the tools that I will need to enable a mobile app to consume outputs from my recommendation engine? Am I missing anything? I've heard of shiny for web apps but i'm interested in creating an api for native mobile apps to use.

Thanks and any help is appreciated.",7,2,False,self,,,,,
1469,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,2,atjylw,self.MachineLearning,"Alchemy, and Why We Created It",https://www.reddit.com/r/MachineLearning/comments/atjylw/alchemy_and_why_we_created_it/,quora-engineering,1550854875,"Hey Reddit!

&amp;#x200B;

Our goal is to be more communicative this coming year. As part of that, we'll be doing a series of posts about what's going on under the hood at Quora. While we obviously have to protect some info, we want to be transparent about the types of challenges we face at scale and how we're going about addressing them. Hopefully this leads to useful insights, whether in what we share or in the discussions that each post generates. (We'll be watching comments closely, and by no means feel that we have all the right answers.)

&amp;#x200B;

This first post is about developing a unified machine learning framework and what we learned from that experience: [https://engineering.quora.com/Feature-Engineering-at-Quora-with-Alchemy](https://engineering.quora.com/Feature-Engineering-at-Quora-with-Alchemy)

&amp;#x200B;

(For context, we have 300 million+ monthly uniques and 17 supported languages  with more on the way! So all this work has to happen at considerable scale.)",2,7,False,self,,,,,
1470,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,2,atk0f5,self.deeplearning,[D] Transfer learning benchmark datasets,https://www.reddit.com/r/MachineLearning/comments/atk0f5/d_transfer_learning_benchmark_datasets/,r2m2,1550855106,,0,1,False,default,,,,,
1471,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,2,atkjfp,self.MachineLearning,Error: Added layer must be instance of class in tensorflow keras,https://www.reddit.com/r/MachineLearning/comments/atkjfp/error_added_layer_must_be_instance_of_class_in/,lingfei42,1550857659,[removed],0,1,False,self,,,,,
1472,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,3,atl7wt,self.MachineLearning,Good Book for more recent ML techniques,https://www.reddit.com/r/MachineLearning/comments/atl7wt/good_book_for_more_recent_ml_techniques/,leonoel,1550861133,[removed],0,1,False,self,,,,,
1473,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,4,atlnao,ai.googleblog.com,Learning to Generalize from Sparse and Underspecified Rewards,https://www.reddit.com/r/MachineLearning/comments/atlnao/learning_to_generalize_from_sparse_and/,sjoerdapp,1550863332,,0,1,False,default,,,,,
1474,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,5,atmaf8,grss-ieee.org,2019 IEEE GRSS Data Fusion Contest,https://www.reddit.com/r/MachineLearning/comments/atmaf8/2019_ieee_grss_data_fusion_contest/,30coffeesaday,1550866772,,0,1,False,default,,,,,
1475,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,5,atmjr9,self.MachineLearning,Intro Machine Learning Project,https://www.reddit.com/r/MachineLearning/comments/atmjr9/intro_machine_learning_project/,iLikeTurtles817,1550868128,"So I'm currently doing a project in school that is focused on just comparing two data sets and displaying the information in graphical form (i.e. histograms, scatter plots, etc.). The example in class compared two items (using one photo of each) and making the observations be different blocks of pixels from each respective photo. 

&amp;#x200B;

For my side of things I need to compare two completely separate pieces of data and am having trouble figuring out what to compare / finding the data to compare the data sets. I know this is super beginner, but if anyone has any ideas or resources available it would be much appreciated!",0,1,False,self,,,,,
1476,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,5,atmlva,self.MachineLearning,Help with Resource Allocation Network for function Interpolation (RAN),https://www.reddit.com/r/MachineLearning/comments/atmlva/help_with_resource_allocation_network_for/,path_finder5,1550868414,[removed],0,1,False,self,,,,,
1477,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,6,atmvrh,self.MachineLearning,Some blog posts on AlphaStar,https://www.reddit.com/r/MachineLearning/comments/atmvrh/some_blog_posts_on_alphastar/,alexirpan,1550869805,[removed],0,1,False,self,,,,,
1478,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,6,atmxej,self.MachineLearning,[D] Blog posts on AlphaStar,https://www.reddit.com/r/MachineLearning/comments/atmxej/d_blog_posts_on_alphastar/,alexirpan,1550870023,"Hey all,  


I wrote some stuff on AlphaStar. I was planning to finish this right after the match came out, but real life got in the way, and I had a lot of things I wanted to cover.  


Part 1: [https://www.alexirpan.com/2019/02/22/alphastar.html](https://www.alexirpan.com/2019/02/22/alphastar.html)

Part 2: [https://www.alexirpan.com/2019/02/22/alphastar-part2.html](https://www.alexirpan.com/2019/02/22/alphastar-part2.html)  


If you like it / dislike it / think my opinion is trash, let me know.",13,124,False,self,,,,,
1479,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,6,atn3tt,arxiv.org,[R] Learning to Generalize from Sparse and Underspecified Rewards,https://www.reddit.com/r/MachineLearning/comments/atn3tt/r_learning_to_generalize_from_sparse_and/,Tivra,1550870899,,2,9,False,default,,,,,
1480,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,6,atnc32,self.MachineLearning,Is scene preservation even possible?,https://www.reddit.com/r/MachineLearning/comments/atnc32/is_scene_preservation_even_possible/,kamranjanjua,1550872099,[removed],0,1,False,self,,,,,
1481,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,6,atng0i,self.MachineLearning,hype.machlearning.net: The last 10 days of Top ML content linked in one simple page,https://www.reddit.com/r/MachineLearning/comments/atng0i/hypemachlearningnet_the_last_10_days_of_top_ml/,phrasebuilder,1550872677,[removed],0,1,False,self,,,,,
1482,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,7,atnw47,heartbeat.fritz.ai,ICR Tutorial - Scannable Chess Scoresheets (CNN's and OpenCV),https://www.reddit.com/r/MachineLearning/comments/atnw47/icr_tutorial_scannable_chess_scoresheets_cnns_and/,veilerdude,1550875127,,0,1,False,https://b.thumbs.redditmedia.com/yqQBZzbgMB9qD-C8ouJYpgpChf3glBI3iABFQfbC0Yw.jpg,,,,,
1483,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,7,ato11n,github.com,tensorflow/federated is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/ato11n/tensorflowfederated_is_a_new_github_repo_by/,sjoerdapp,1550875893,,0,1,False,default,,,,,
1484,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,10,atpggg,i.redd.it,Cats GAN thiscatdoesnotexist.com learned to generate cats with watermarks and site caption below image,https://www.reddit.com/r/MachineLearning/comments/atpggg/cats_gan_thiscatdoesnotexistcom_learned_to/,alexyalunin,1550884357,,0,1,False,default,,,,,
1485,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,11,atpx2w,hype.machlearning.net,The last 10 days of top ML content linked in one simple page,https://www.reddit.com/r/MachineLearning/comments/atpx2w/the_last_10_days_of_top_ml_content_linked_in_one/,phrasebuilder,1550887264,,0,1,False,default,,,,,
1486,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,11,atq0pv,hype.machlearning.net,[P] The last 10 days of top ML content linked in one simple page,https://www.reddit.com/r/MachineLearning/comments/atq0pv/p_the_last_10_days_of_top_ml_content_linked_in/,phrasebuilder,1550887879,,0,1,False,https://b.thumbs.redditmedia.com/0EbNL_5LLLBJIpbRGOAb-yETlMaqHXdcPduHuAR13og.jpg,,,,,
1487,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,11,atq3yb,ai.googleblog.com,Learning to Generalize from Sparse and Underspecified Rewards,https://www.reddit.com/r/MachineLearning/comments/atq3yb/learning_to_generalize_from_sparse_and/,life_is_harsh,1550888453,,0,1,False,default,,,,,
1488,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,13,atr453,self.MachineLearning,[D] Help with pytorch NAIS Net block implementation,https://www.reddit.com/r/MachineLearning/comments/atr453/d_help_with_pytorch_nais_net_block_implementation/,asdfkjlhdasjklfhaasd,1550894661,"I'm trying to implement the block from this paper:
https://arxiv.org/pdf/1804.07209.pdf

Here is the current pytorch code I have:
https://pastebin.com/xmuvjY6z

The problem I have is with this part:
&gt; Since NAIS-Net blocks are guaranteed to converge to a pattern-dependent steady state after an indeterminate number of iterations, processing depth can be controlled dynamically by terminating the unrolling process whenever the distance between a layer representation, x(i), and that of the immediately previous layer, x(i  1), drops below a specified threshold

My current implementation does not drop anywhere close to the threshold that they use in the paper (10e-4).
So I have a unrolling threshold of 10 before I break out.

Here is an example of the output of the unrolling iterations that I get while training a model with this block:
Iteration: 0    distance: tensor(1245.7202, device='cuda:0')
Iteration: 1    distance: tensor(1248.3201, device='cuda:0')
Iteration: 2    distance: tensor(1251.0364, device='cuda:0')
Iteration: 3    distance: tensor(1252.1907, device='cuda:0')
Iteration: 4    distance: tensor(1253.3043, device='cuda:0')
Iteration: 5    distance: tensor(1253.3817, device='cuda:0')
Iteration: 6    distance: tensor(1253.8070, device='cuda:0')
Iteration: 7    distance: tensor(1253.9282, device='cuda:0')
Iteration: 8    distance: tensor(1253.9247, device='cuda:0')
Iteration: 9    distance: tensor(1253.9391, device='cuda:0')

I wanted to also note that even with having to manually break out at 10 unrolls, the model seems to be performing decently, but I'd like to get the dynamic unrolling termination working.
",4,2,False,self,,,,,
1489,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,13,atrjxo,self.MachineLearning,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/atrjxo/machine_learning/,uptosuccess,1550897508,[removed],0,1,False,self,,,,,
1490,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,14,ats126,odaksan.com,Table Pallet Stretch Wrapping Machine,https://www.reddit.com/r/MachineLearning/comments/ats126/table_pallet_stretch_wrapping_machine/,odaksan,1550900736,,0,1,False,default,,,,,
1491,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,15,atsbmj,self.MachineLearning,Machine Learning and Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/atsbmj/machine_learning_and_pattern_recognition/,Anu2008,1550902901,[removed],0,1,False,self,,,,,
1492,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,15,atsdqb,self.MachineLearning,"All electric Trucks Market: Industry Outlook, Growth Prospects and Key Opportunities 2023",https://www.reddit.com/r/MachineLearning/comments/atsdqb/all_electric_trucks_market_industry_outlook/,apple_x9,1550903352,[removed],1,1,False,self,,,,,
1493,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,15,atsew9,georgedatascience.com,Read why Python is the Most Important Language for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/atsew9/read_why_python_is_the_most_important_language/,georgedatascience,1550903605,,0,1,False,https://b.thumbs.redditmedia.com/mEPHSeHFIK63vd6YKljy4M-s06CpeZ-bDf626mnOI7A.jpg,,,,,
1494,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,15,atsjal,self.MachineLearning,Refrigerated Air Dryers Market: Industry Outlook Growth Prospects and Key Opportunities 2023,https://www.reddit.com/r/MachineLearning/comments/atsjal/refrigerated_air_dryers_market_industry_outlook/,apple_x9,1550904556,[removed],1,1,False,self,,,,,
1495,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,15,atsm7y,self.MachineLearning,Padlock Set Market: Industry Outlook Growth Prospects and Key Opportunities 2023,https://www.reddit.com/r/MachineLearning/comments/atsm7y/padlock_set_market_industry_outlook_growth/,apple_x9,1550905173,[removed],1,1,False,self,,,,,
1496,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,16,atsp3o,self.MachineLearning,Night Vision Devices Market: Industry Outlook Growth Prospects and Key Opportunities 2023,https://www.reddit.com/r/MachineLearning/comments/atsp3o/night_vision_devices_market_industry_outlook/,apple_x9,1550905784,[removed],1,1,False,self,,,,,
1497,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,16,atsq07,pythonandmltrainingcourses.com,ML Training Company in Delhi,https://www.reddit.com/r/MachineLearning/comments/atsq07/ml_training_company_in_delhi/,tushararora0330,1550905974,,0,1,False,default,,,,,
1498,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,16,atsrzm,self.MachineLearning,Laser Level Market: Industry Outlook Growth Prospects and Key Opportunities 2023,https://www.reddit.com/r/MachineLearning/comments/atsrzm/laser_level_market_industry_outlook_growth/,apple_x9,1550906423,[removed],1,1,False,self,,,,,
1499,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,16,atstzt,pythonandmltrainingcourses.com,Best Python Training Center in Delhi,https://www.reddit.com/r/MachineLearning/comments/atstzt/best_python_training_center_in_delhi/,tushararora0330,1550906867,,0,1,False,default,,,,,
1500,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,17,att869,self.MachineLearning,Classic Machine Learning algorithms : should i code from scratch or learning the theories and scikit learn is enough?,https://www.reddit.com/r/MachineLearning/comments/att869/classic_machine_learning_algorithms_should_i_code/,AjAkil,1550910034,[removed],0,1,False,self,,,,,
1501,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,17,attbjb,self.MachineLearning,Handwritten text recognition,https://www.reddit.com/r/MachineLearning/comments/attbjb/handwritten_text_recognition/,ImranAl5,1550910844,[removed],4,0,False,self,,,,,
1502,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,18,attlkq,self.MachineLearning,Military Communication System Market: Industry Outlook Growth Prospects and Key Opportunities 2023,https://www.reddit.com/r/MachineLearning/comments/attlkq/military_communication_system_market_industry/,apple_x9,1550913329,[removed],1,1,False,self,,,,,
1503,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,18,atto50,self.MachineLearning,Nail Gun Market: Industry Outlook Growth Prospects and Key Opportunities 2023,https://www.reddit.com/r/MachineLearning/comments/atto50/nail_gun_market_industry_outlook_growth_prospects/,apple_x9,1550913929,[removed],1,1,False,self,,,,,
1504,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,18,attqr0,self.MachineLearning,Nuclear Valves Market: Industry Outlook Growth Prospects and Key Opportunities 2023,https://www.reddit.com/r/MachineLearning/comments/attqr0/nuclear_valves_market_industry_outlook_growth/,apple_x9,1550914530,[removed],1,1,False,self,,,,,
1505,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,18,attrzz,self.MachineLearning,[D] LeCun ISSCC19 slides : Use self supervising rather than RL,https://www.reddit.com/r/MachineLearning/comments/attrzz/d_lecun_isscc19_slides_use_self_supervising/,yazriel0,1550914837,This is my interpretation of his slides. RL is too sample in-efficient. Self supervise by learning to predict missing parts of the input. Use this as pre-training.,10,47,False,self,,,,,
1506,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,19,atu995,self.MachineLearning,[D] Undergraduate student feeling completely overwhelmed,https://www.reddit.com/r/MachineLearning/comments/atu995/d_undergraduate_student_feeling_completely/,LazyLoo,1550918774,"As an undergrad student with a math major that wants to pursue ML in grad school (preferably the top ones) I am feeling completely overwhelmed. I need to keep my grades and GPA high for grad school, am doing research with professors (cannot make much progress as I don't understand much of the deep learning foundations), doing side projects, applying to internships and preparing for interviews, and trying to learn as much as possible in the field. I am also trying to improve my coding skills as I lack some of the more foundational knowledge in OOP and OS and so forth since I don't have a CS major. It honestly seems impossible to balance all of this and I find myself staying up late at night stressing out and trying to cram as possible and it doesn't seem to be getting me any closer to my goals. In fact it is affecting my health and relationships. Maybe I am expecting too much of myself an undergraduate student? 

The major issue is just finding the goddamn time to learn the machine learning and deep learning content and keep up with what's going on while being a full time student. Much of my time is devoted towards classes so I really don't understand those undergrad students I find on LinkedIn who are interning at Google and stuff with insanely high GPAs and who already have multiple research papers and seem to already be experts in the field. Maybe I shouldn't be comparing myself to people like that? 

Meanwhile for me I read research papers and can't understand over 90% of what's going on in them. I read articles on specific ML topics and in those, there are another 100 things I don't understand so I end up opening another 50 tabs. Each night I close my laptop with at least 30 unread tabs of random things lol. Even half the posts and comments on this subreddit I cannot understand. Also, how am I even supposed to make any progress in my research if there's so much to understand and I don't have the time to?

So far I've gone through the coursera ML course, reviewed all the math necessary (the only thing I'm really confident in since I'm a math major lol), trying to brush up and improve my coding (don't know how necessary it is to learn C++ which I don't know), doing algorithms practice and leetcode for interviews, and I'm planning on going through the Goodfellow deep learning book. There's also David silver's reinforcement learning course I want to go through as it'll be helpful for any robotics internships. And after that I may go through the fast.ai course to get stronger at the applications aspect of ML. This is alongside classes and keeping my GPA high and doing as good a job I can with research for my professors. Honestly this post has been much longer than I thought and more of a rant, but maybe I just need some advice or someone to point me on the right track. Am I seriously overly rushing myself and having unrealistic expectations for an undergraduate student? ",161,229,False,self,,,,,
1507,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,19,atuax4,self.MachineLearning,How does working in machine learning change the way you think about your own mind?,https://www.reddit.com/r/MachineLearning/comments/atuax4/how_does_working_in_machine_learning_change_the/,The_Grand_Blooms,1550919142,[removed],0,1,False,self,,,,,
1508,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,21,atv6kx,self.MachineLearning,Classification times for general datasets,https://www.reddit.com/r/MachineLearning/comments/atv6kx/classification_times_for_general_datasets/,apvast,1550926183,[removed],0,1,False,self,,,,,
1509,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,21,atv78p,self.MachineLearning,Image Processing Resources,https://www.reddit.com/r/MachineLearning/comments/atv78p/image_processing_resources/,freshprinceofuk,1550926314,"Hi,

I've recently been hired by a startup as a Machine Learning Engineer with a focus on Computer Vision (first job out of university) and while a large Machine Learning project has been identified to be started very soon, I'm currently doing some simpler Image Processing tasks. My background is programming, time-related signal processing, image-based machine learning, and engineering (Engineering BEng and MSc).


Can anyone suggest resources on specifically non-learning based Image Processing/Computer Vision algorithms/techniques?

Thanks",0,1,False,self,,,,,
1510,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,22,atvn9l,youtu.be,Answer To Skynet? A Democratically Controlled Supermind,https://www.reddit.com/r/MachineLearning/comments/atvn9l/answer_to_skynet_a_democratically_controlled/,getrich_or_diemining,1550929509,,0,1,False,default,,,,,
1511,MachineLearning,t5_2r3gv,2019-2-23,2019,2,23,23,atvupj,self.MachineLearning,Question: Do you have experience with the BDD100K dataset for self driving cars?,https://www.reddit.com/r/MachineLearning/comments/atvupj/question_do_you_have_experience_with_the_bdd100k/,Famous_Locksmith,1550930962,[removed],0,1,False,self,,,,,
1512,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,0,atwdgr,youtu.be,AI BTC Price Prediction tool - what do you guys think?,https://www.reddit.com/r/MachineLearning/comments/atwdgr/ai_btc_price_prediction_tool_what_do_you_guys/,arsch_loch,1550934548,,0,1,False,https://a.thumbs.redditmedia.com/6xpO-Vrrz4zg4zVkl-HbfLuVuRJOQwRtd4hyz3uNqy4.jpg,,,,,
1513,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,0,atwg3l,self.MachineLearning,Deep learning - Find patterns combining images and bios data,https://www.reddit.com/r/MachineLearning/comments/atwg3l/deep_learning_find_patterns_combining_images_and/,Aceconhielo,1550935036,"Hello everyone,

I  was wondering if is it possible combining images and some ""bios"" data  for finding patterns. For example, if I want to know if a image is a cat  or dog and I have:

1. Enough image data for train my model
2. Enough ""bios"" data like:  

   1. *size of the animal*
   2. *size of the tail*
   3. *weight*
   4. height

Thanks!",0,1,False,self,,,,,
1514,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,0,atwnlh,arxiv.org,[R] Evaluating the Search Phase of Neural Architecture Search: random policy outperforms state-of-the-art NAS algorithms,https://www.reddit.com/r/MachineLearning/comments/atwnlh/r_evaluating_the_search_phase_of_neural/,downtownslim,1550936391,,1,53,False,default,,,,,
1515,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,0,atwqp5,self.MachineLearning,"Discussion about the paper ""Personalized Dialogue Agents: I have a dog, do you have pets too?""",https://www.reddit.com/r/MachineLearning/comments/atwqp5/discussion_about_the_paper_personalized_dialogue/,abhinavkashyap92,1550936929,[removed],0,1,False,self,,,,,
1516,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,1,atwyq6,self.MachineLearning,Random Forests: If I use this method and apply it to the data - will it run through all of them or will one optimised Tree evolve from the Random Forest method?,https://www.reddit.com/r/MachineLearning/comments/atwyq6/random_forests_if_i_use_this_method_and_apply_it/,Everdream13,1550938272,[removed],0,1,False,self,,,,,
1517,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,1,atxe2b,self.MachineLearning,"List of math, data science, and ML courses",https://www.reddit.com/r/MachineLearning/comments/atxe2b/list_of_math_data_science_and_ml_courses/,uncertainty-principl,1550940796,[removed],0,1,False,self,,,,,
1518,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,2,atxhga,self.MachineLearning,Med student wants to learn ML,https://www.reddit.com/r/MachineLearning/comments/atxhga/med_student_wants_to_learn_ml/,Cocopopo3,1550941317,[removed],0,1,False,self,,,,,
1519,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,2,atxxas,medium.com,[N] Lingvo: A TensorFlow Framework for Sequence Modeling,https://www.reddit.com/r/MachineLearning/comments/atxxas/n_lingvo_a_tensorflow_framework_for_sequence/,samithaj,1550943789,,0,1,False,default,,,,,
1520,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,2,atxz0p,self.MachineLearning,[D] Sloppy servos and Jackie Chan AI,https://www.reddit.com/r/MachineLearning/comments/atxz0p/d_sloppy_servos_and_jackie_chan_ai/,wyldcraft,1550944056,"It occurred to me there might be an opportunity to turn a bug into a feature. If this is already A Thing I'd be grateful for links.  


I was explaining how advanced the human eye and hand are, in the context of ""constantly impending technological unemployment"". Vision has made order of magnitude leaps over the past years, but robot arms precise and strong enough to do useful work remain expensive, for good reasons.  


If you tell a Lego robot to spin around to degree 90, it's a jerky stop and a jerky start. The whole contraption teeters from the inertia. The arm tip passes 90 degrees and bounces back and forth several times. The software cries, ""How am I to work under these conditions?""  


But when Jackie Chan wound up holding a mop instead of a sword, he used that mop-head to his advantage and defeated his enemies in novel ways a sword can't.  


I'd interested in the concept of applying Machine Learning to the jerky cheap robot situation. I could at least envision a system where a bot with a paintbrush could be made more effective if rather than trying to paint specific x/y, the software was producing motions that took advantage of the known jerky motion and glean complicated brushstokes from a couple zaps to the servo.  


One branch of this could be measurement of real world bots in various configurations. Another might be making accurate enough calculations of that sloppiness in emulation, which should be much faster and parallel-capable.",5,1,False,self,,,,,
1521,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,3,aty76n,self.MachineLearning,Object Detection and automated Task,https://www.reddit.com/r/MachineLearning/comments/aty76n/object_detection_and_automated_task/,KJ-16,1550945348,[removed],0,1,False,self,,,,,
1522,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,3,atyd2m,self.MachineLearning,[P] Code for Bandit Swarm Networks,https://www.reddit.com/r/MachineLearning/comments/atyd2m/p_code_for_bandit_swarm_networks/,CireNeikual,1550946258,"Hello,

The code for Bandit Swarm Networks (original post: [here](https://www.reddit.com/r/MachineLearning/comments/aspmas/r_bandit_swarm_networks/) ) is now available:

[C++ library](https://github.com/222464/Swarm)
[Python bindings](https://github.com/222464/PySwarm) (recommended, contains the examples)

I have since tried two additional environments, which are included in the examples directory in PySwarm.

- Catcher-v0: Only vision task so far, works reasonably well
- LunarLander-v2: Precise control task, works very well

Let me know if you end up experimenting with it, or if you come up with your own implementation!",2,25,False,self,,,,,
1523,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,3,atye32,self.MachineLearning,[D] Machine Learning in Cybersecurity?,https://www.reddit.com/r/MachineLearning/comments/atye32/d_machine_learning_in_cybersecurity/,dantesplague,1550946413,"Hello ML-Reddit,

I'm a software engineer who's relatively new to machine learning. I'm taking baby steps learning the basics and have been wondering about possible applications of machine learning/deep learning in the cybersecurity world. What are your opinions/ideas on this: Does ML have a use in cybersec? What about machine learning from the attacker's point of view?",17,12,False,self,,,,,
1524,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,3,atyfvd,youtube.com,[R] Troubling Trends in ML Scholarship - Zachary Lipton,https://www.reddit.com/r/MachineLearning/comments/atyfvd/r_troubling_trends_in_ml_scholarship_zachary/,ch3njust1n,1550946691,,0,1,False,https://a.thumbs.redditmedia.com/8VdzfiKFXaUg6v_3-jJeFyC6-Kppo8DSUvC3bqzOCb8.jpg,,,,,
1525,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,3,atym3n,self.MachineLearning,Random kernel methods,https://www.reddit.com/r/MachineLearning/comments/atym3n/random_kernel_methods/,keane27,1550947639,[removed],0,1,False,self,,,,,
1526,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,3,atyn04,self.MachineLearning,[D] When is Instance Norm better than Batch Norm,https://www.reddit.com/r/MachineLearning/comments/atyn04/d_when_is_instance_norm_better_than_batch_norm/,WillingCucumber,1550947777,"Hi all,

I want to know the instances in which Instance Norm turned to be better than BatchNorm.

I know its effectiveness in style transfer. Also, please don't mention instances where instance norm is used because of the memory constraint.

Are there any scenarios, where instance norm works better than batch norm in less data size problems.

Please cite such papers, if any.

&amp;#x200B;

Thanks !!",7,17,False,self,,,,,
1527,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,3,atynft,self.MachineLearning,[P] CNN for CFD Predictions,https://www.reddit.com/r/MachineLearning/comments/atynft/p_cnn_for_cfd_predictions/,psociety,1550947842,"For a university project I am working on on creating a CNN to predict simple fluid flows based on an input geometry. The input is a 256x256 array of 0s and 1s to represent the geometry, the labels are 256x256 grids of values representing the magnitude of velocity at each point.

&amp;#x200B;

I have some experience with ANNs but am a little lost with CNNs and most of the resources available see to lead to classification examples. 

&amp;#x200B;

A paper that uses a similar method [has this architecture](https://imgur.com/a/nAODWK1).

&amp;#x200B;

I have tried to reproduce this but have no need for x-component/y-component as I am using the magnitude. If anyone could point me in the right direction on how I may design an architecture that will provide a 2D output. At the moment I am using the Keras package and have tried the following code to produce a model:

&amp;#x200B;

`model.add(Conv2D(32, (16, 16), padding='same', input_shape=(256,256,1)))`

`model.add(Activation('relu'))`

`model.add(MaxPooling2D(pool_size=(16,16), padding='same'))`

&amp;#x200B;

`#2nd convolution layer`

`model.add(Conv2D(4,(16, 16), padding='same'))`

`model.add(Activation('relu'))`

`model.add(MaxPooling2D(pool_size=(2,2), padding='same'))`

&amp;#x200B;

`#3rd convolution layer`

`model.add(Conv2D(4,(16, 16), padding='same'))`

`model.add(Activation('relu'))`

`model.add(UpSampling2D((2, 2)))`

&amp;#x200B;

`#4rd convolution layer`

`model.add(Conv2D(4,(16, 16), padding='same'))`

`model.add(Activation('relu'))`

`model.add(UpSampling2D((16, 16)))`

&amp;#x200B;

`model.add(Conv2D(1,(16, 16), padding='same'))`

`model.add(Activation('sigmoid'))`

&amp;#x200B;

This is giving an output of the shape I want, but upon plotting the output it hasn't done anything useful.",4,1,False,self,,,,,
1528,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,4,atzck0,self.MachineLearning,The 10 Algorithms Machine Learning Engineers Need to Know,https://www.reddit.com/r/MachineLearning/comments/atzck0/the_10_algorithms_machine_learning_engineers_need/,andrea_manero,1550951626,[removed],0,1,False,self,,,,,
1529,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,5,atzhz7,self.MachineLearning,"Deep Learning: Definition, Resources, Comparison with Machine Learning",https://www.reddit.com/r/MachineLearning/comments/atzhz7/deep_learning_definition_resources_comparison/,andrea_manero,1550952454,[removed],0,1,False,self,,,,,
1530,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,5,atzlfs,self.MachineLearning,New Perspectives on Statistical Distributions and Mixture Models - with Broad Spectrum of Applications,https://www.reddit.com/r/MachineLearning/comments/atzlfs/new_perspectives_on_statistical_distributions_and/,Statology,1550952990,"The full article is available [here.](https://dsc.news/2GEPcFj) Content:

**Introduction and Context**

**Approximations Using Mixture Models**

* The error term
* Kernels and model parameters
* Algorithms to find the optimum parameters
* Convergence and uniqueness of solution
* Find near-optimum with fast, black-box step-wise algorithm

**Example**

* Data and source code
* Results

**Applications**

* Optimal binning
* Predictive analytics
* Test of hypothesis and confidence intervals
* Clustering

**Interesting problems**

* Gaussian mixtures uniquely characterize a broad class of distributions
* Weighted sums fail to achieve what mixture models do
* Stable mixtures
* Correlations

&amp;#x200B;",0,1,False,self,,,,,
1531,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,5,atzo0r,self.MachineLearning,The essence of machine learning is function estimation,https://www.reddit.com/r/MachineLearning/comments/atzo0r/the_essence_of_machine_learning_is_function/,andrea_manero,1550953408,[removed],0,1,False,self,,,,,
1532,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,6,au0cru,self.MachineLearning,Quick summary of InfoGAN,https://www.reddit.com/r/MachineLearning/comments/au0cru/quick_summary_of_infogan/,Shiwayz,1550957311,[removed],0,1,False,self,,,,,
1533,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,6,au0i3j,self.MachineLearning,ResNet152 is faster than VGG19 despite having significantly (10X) more layers,https://www.reddit.com/r/MachineLearning/comments/au0i3j/resnet152_is_faster_than_vgg19_despite_having/,code_like_tiger,1550958189,[removed],0,1,False,self,,,,,
1534,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,6,au0iid,self.MachineLearning,[D] Is this a valid description of Bayesian Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/au0iid/d_is_this_a_valid_description_of_bayesian_deep/,Turing__Incomplete,1550958252,"This Quora answer is receiving a lot of attention: [Alan Lockett answer to ""What is Bayesian Deep Learning?""](https://www.quora.com/What-is-Bayesian-Deep-Learning/answer/Alan-Lockett-2)

&gt;*Bayesian Deep Learning* is an academic marketing term that was made up by a researcher who gave a theoretical justification for DropOut using Bayesian principles, showing among other things that using DropOut at inference time and not just during training allows one to estimate the uncertainty of a trained model (see e.g. [https://www.cs.ox.ac.uk/people/y...](https://www.cs.ox.ac.uk/people/yarin.gal/website/PDFs/2017_OReilly_talk.pdf), which is a set of slides from Yarin Gal along with a list of references). This last contribution  using DropOut at inference time to estimate uncertainty  is an excellent contribution. But calling it Bayesian Deep Learning is overstating the case, because it is really just mildly and approximately Bayesian.  
&gt;  
&gt;The reality is that this line of thinking *asks a lot of good questions* but doesnt yet *provide a lot of good answers*. It would indeed be nice to get a handle on the uncertainty of predictions made by neural networks. But this is a much bigger issue than just getting the uncertainty inherent in the data (which is what the DropOut approach does). One needs a true Bayesian prior describing the source from which the data are drawn (*e.g.* locality, discreteness/objectness, basic Newtonian physics), and without a model of these sources its hard to call the DropOut-based approach *Bayesian*; its really just a method for measuring some combination between the noise in the dataset and the noise in the network training method.  
&gt;  
&gt;The other answer here just posted text from an article on Medium. It goes over the idea of Bayesian deep networks, and lists three ways of implementing a Bayesian approach to network parameters. The first is to use Monte Carlo  which means you have to first sample the network parameters (weights and biases), and then sample the network outputs from the inputs. That will never work at scale; you cant train anything practical that way, too slow. The second approach is to use variational inference to approximately find the right weights; but you still have to sample the weights and average in order to get the mean and variance for the network outputs, which still slows down inference, without mentioning that variational inference is approximate and often very computationally expensive. The third approach is the one that was actually proposed, that is, to use DropOut, which is hardly Bayesian in the traditional sense, whatever theoretical justification may be offered.  
&gt;  
&gt;**Disclaimer:** The last time I read a paper on this topic was in June 2018, so something cool may have developed since then. But if so, I havent heard of it yet.",25,93,False,self,,,,,
1535,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,6,au0ilo,self.MachineLearning,How can I fuse CNN features with texture (GLCM) features in image classification task?,https://www.reddit.com/r/MachineLearning/comments/au0ilo/how_can_i_fuse_cnn_features_with_texture_glcm/,hasannasirk,1550958267,[removed],0,1,False,self,,,,,
1536,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,6,au0mct,self.MachineLearning,[D] How can I fuse CNN features with texture (GLCM) features in image classification task?,https://www.reddit.com/r/MachineLearning/comments/au0mct/d_how_can_i_fuse_cnn_features_with_texture_glcm/,hasannasirk,1550958865,"I want to fuse CNN features with GLCM features (energy, entropy, contrast, homogeneity, correlation etc).  What will be the Python implementation code? please give a solution",8,0,False,self,,,,,
1537,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,8,au1b9c,self.MachineLearning,How has working in machine learning changed the way you think about your own mind?,https://www.reddit.com/r/MachineLearning/comments/au1b9c/how_has_working_in_machine_learning_changed_the/,The_Grand_Blooms,1550962972,[removed],0,1,False,self,,,,,
1538,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,8,au1rhu,self.MachineLearning,Good resources for understanding Machine Learning algorithms in order to implement them,https://www.reddit.com/r/MachineLearning/comments/au1rhu/good_resources_for_understanding_machine_learning/,rick94sm,1550965781,[removed],0,1,False,self,,,,,
1539,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,9,au2fcb,self.MachineLearning,[P] (HotterColder-v0) Guess the number using gradients and directional derrivtives,https://www.reddit.com/r/MachineLearning/comments/au2fcb/p_hottercolderv0_guess_the_number_using_gradients/,premepopulation,1550969754,"I've been having a super hard time solving the OpenAi environment [HotterColder-v0](https://gym.openai.com/envs/HotterColder-v0/).

&gt;The goal of the game is to effectively use the reward provided in order to understand the best action to take.  
After each step the agent receives an observation of: 0 - No guess yet submitted (only after reset) 1 - Guess is lower than the target 2 - Guess is equal to the target 3 - Guess is higher than the target  
The rewards is calculated as: ((min(action, self.number) + self.bounds) / (max(action, self.number) + self.bounds)) \*\* 2 This is essentially the squared percentage of the way the agent has guessed toward the target.  
Ideally an agent will be able to recognize the 'scent' of a higher reward and increase the rate in which is guesses in that direction until the reward reaches its maximum.

I believe I need to get the derivative of the reward w.r.t the weight that I multiply the input by. I've been spending all day trying to solve this and I am completely stuck. Please help!",0,2,False,self,,,,,
1540,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,11,au38sr,self.MachineLearning,[D] Best way to host an image classification model for predictions on Google Cloud,https://www.reddit.com/r/MachineLearning/comments/au38sr/d_best_way_to_host_an_image_classification_model/,pkacprzak,1550975118,"I trained two models for image classification for a task similar to MNIST (although a bit harder) on my local machine. One model I have is a transfer learning from another model in TensorFlow while the other one is trained from scratch in PyTorch. Now,  I'd like to deploy it and serve for online predictions. 

I'm quite familiar with Google Cloud platform, especially with App Engine, but not that much with Compute Engine and their specialized Machine Learning solutions. I searched the web for tutorials and documentations about how to do that but all I found are clumsy and didn't cover end-to-end process. In short, I'd like to know what's the recommended way to upload an existing model and serve its prediction via API, e.g. REST, and maybe to be able to get the predictions internally from my another application on App Engine.

&amp;#x200B;

It seems this is the official documentation for deploying TensorFlow models on Google Cloud:

[https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models)

&amp;#x200B;

While for PyTorch, I found this and related links for creating a VM with PyTorch: [https://cloud.google.com/deep-learning-vm/docs/pytorch\_start\_instance](https://cloud.google.com/deep-learning-vm/docs/pytorch_start_instance)

&amp;#x200B;",8,2,False,self,,,,,
1541,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,13,au4dx2,self.MachineLearning,[P] Transformer implementation in TensorFlow with notes,https://www.reddit.com/r/MachineLearning/comments/au4dx2/p_transformer_implementation_in_tensorflow_with/,mlvpj,1550982916,"I recently coded a standalone transformer implementation for learning. Implementing from scratch helped me understand a lot of details which I previously had overlooked.

[http://blog.varunajayasiri.com/ml/transformer.html](http://blog.varunajayasiri.com/ml/transformer.html)

I've placed a lot of comments to make it easier to understand, and it's a stand alone single file implementation

My implementation was inspired by [Annotated Transformer by Harvard NLP](http://nlp.seas.harvard.edu/2018/04/03/attention.html), which is in PyTorch.",8,39,False,self,,,,,
1542,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,14,au4ygr,medium.com,"""[N]"" Lingvo: A TensorFlow Framework for Sequence Modeling",https://www.reddit.com/r/MachineLearning/comments/au4ygr/n_lingvo_a_tensorflow_framework_for_sequence/,samithaj,1550987143,,0,1,False,https://b.thumbs.redditmedia.com/nqca9ZGbGRnpXTYCn-Vwlf-9f5MqX4x7AARAmoqa-DM.jpg,,,,,
1543,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,16,au5kzg,self.MachineLearning,[N] Intel Director Mike Davies slams deep learning: its not actually learning,https://www.reddit.com/r/MachineLearning/comments/au5kzg/n_intel_director_mike_davies_slams_deep_learning/,downtownslim,1550992338,"From the article:

&gt; ""Backpropogation doesn't correlate to the brain,"" insists Mike Davies, head of Intel's neuromorphic computing unit, dismissing one of the key tools of the species of A.I. In vogue today, deep learning. ""For that reason, ""it's really an optimizations procedure, it's not actually learning."" 

https://www.zdnet.com/article/intels-neuro-guru-slams-deep-learning-its-not-actually-learning/",76,24,False,self,,,,,
1544,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,16,au5lgu,self.MachineLearning,"Rejected by all PhD programs 3 different seasons, stay in my current lab for PhD or apply again?",https://www.reddit.com/r/MachineLearning/comments/au5lgu/rejected_by_all_phd_programs_3_different_seasons/,ubiquitous7733,1550992465,"**TL;DR: Want to do NLP and machine learning research but I've been rejected from every PhD program I've applied to (3 different seasons). I will likely get into the program where I'm doing my master's degree, but the group I'm with is doing biomedical NLP specifically and aren't well connected to the NLP field (especially those working on machine learning). Should I stay here for my PhD or wait and apply again later?** 

&amp;#x200B;

Hey all

Wanted to get the community's input on my current situation and see what people think, especially regarding the importance of who you work with and the group that you are in during your PhD. Some background:

I'm currently finishing my master's in computer science at a pretty decent school (top 15). My main research interests are in NLP and machine learning. I've spent the past year doing biomedical NLP and machine learning research. I published two papers during my master's (one first author) but not at top conferences. Prior to my masters I worked in industry as a software engineer for 2.5 years, and during my undergrad I did research for two years on some applied machine learning and published 6 papers (one as first author, again not at top venues). When I applied for grad school straight out of undergrad, I was rejected by every program (Stanford, UW, UCSD, UC Berkeley, UC Irvine, and MIT). I tried again next year, and was again rejected by every PhD program but was accepted by the one master's program to which I applied. Looking back I understand why I was rejected (I was definitely not high enough caliber to get in at the time). Now I've applied again to PhD programs, and it is looking like I will again be rejected by all of them (NYU CDS, Columbia, Stanford, UW, MIT, CMU, UT Austin, and UCSD). I picked those schools specifically for the mentorship I would receive and the types of problems being worked on there. However, I will most likely be able to transfer into the PhD program at my current school.

I'm still certain that I want to pursue a PhD; I love exploring NLP and machine learning, and enjoy asking interesting research questions and discovering new knowledge in the process of answering those questions. If I stay in my current lab, I will most likely be able to continue doing that. Additionally, I ultimately want to become a professor and do research/teach. I have funding on my current project, and will potentially be able to get a decent fellowship through our industry partners. I will also be doing a research internship in industry this summer which will hopefully open some more doors. 

My biggest concern is that my current lab is not well connected to the larger field of NLP. My advisor publishes mostly in bioinformatics journals and conferences, while I'm more interested in conferences such as NeurIPS, ACL, NAACL, EMNLP, AAAI and the like. They are currently trying to break into those communities but as of now they are not well connected and have limited history with publishing at those venues. Additionally, I'm somewhat siloed from the main NLP group on campus. I had briefly worked with one of the NLP profs on a project for a quarter but they did not continue our work due to ""resource constraints"" (in reality there was a difference in communication styles so we just didn't mesh very well).

So I guess the main things I need input on are: do I stay in my current lab for my PhD or do I try to improve my publication record this/next year, get better connected with the community, and apply again in the future? If I wait and apply again, what should be my course of action in the mean time (I'd like to continue doing research somehow or at least be doing something to advance my research goals)? How important is who you work with during your PhD? Will staying in a not well connected lab lead to limited prospects after my PhD (in terms of post-doc, industry labs, etc.)? Is it possible to have a satisfying career in this field without being in a top lab? I guess that last question depends on how you gauge satisfaction, which to me mostly lies in working on problems I consider interesting and exciting, but also that other people consider interesting as measured by publication in good (top) venues.

As a side note, my GPA is definitely not the issue (4.0 in undergrad, 3.92 in master's), my master's program is generally considered ""elite"", and my GRE is not too terrible (159/165/4 V/Q/W).

Thanks for taking the time to read this if you got this far :) any input is appreciated!",0,1,False,self,,,,,
1545,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,16,au5oj1,self.MachineLearning,"[D] Rejected by all PhD programs 3 different seasons, stay in current lab or apply again later?",https://www.reddit.com/r/MachineLearning/comments/au5oj1/d_rejected_by_all_phd_programs_3_different/,ubiquitous7733,1550993277,"**TL;DR: Want to do NLP and machine learning research but I've been rejected from every PhD program I've applied to (3 different seasons). I will likely get into the program where I'm doing my master's degree, but the group I'm with is doing biomedical NLP specifically and aren't well connected to the NLP field (especially those working on machine learning). Should I stay here for my PhD or wait and apply again later?**

Hey all

Wanted to get the community's input on my current situation and see what people think, especially regarding the importance of who you work with and the group that you are in during your PhD. Some background:

I'm currently finishing my master's in computer science at a pretty decent school (top 15). My main research interests are in NLP and machine learning. I've spent the past year doing biomedical NLP and machine learning research. I published two papers during my master's (one first author) but not at top conferences. Prior to my masters I worked in industry as a software engineer for 2.5 years, and during my undergrad I did research for two years on some applied machine learning and published 6 papers (one as first author, again not at top venues). When I applied for grad school straight out of undergrad, I was rejected by every program (Stanford, UW, UCSD, UC Berkeley, UC Irvine, and MIT). I tried again next year, and was again rejected by every PhD program but was accepted by the one master's program to which I applied. Looking back I understand why I was rejected (I was definitely not high enough caliber to get in at the time). Now I've applied again to PhD programs, and it is looking like I will again be rejected by all of them (NYU CDS, Columbia, Stanford, UW, MIT, CMU, UT Austin, and UCSD). I picked those schools specifically for the mentorship I would receive and the types of problems being worked on there. However, I will most likely be able to transfer into the PhD program at my current school.

I'm still certain that I want to pursue a PhD; I love exploring NLP and machine learning, and enjoy asking interesting research questions and discovering new knowledge in the process of answering those questions. If I stay in my current lab, I will most likely be able to continue doing that. Additionally, I ultimately want to become a professor and do research/teach. I have funding on my current project, and will potentially be able to get a decent fellowship through our industry partners. I will also be doing a research internship in industry this summer which will hopefully open some more doors.

My biggest concern is that my current lab is not well connected to the larger field of NLP. My advisor publishes mostly in bioinformatics journals and conferences, while I'm more interested in conferences such as NeurIPS, ACL, NAACL, EMNLP, AAAI and the like. They are currently trying to break into those communities but as of now they are not well connected and have limited history with publishing at those venues. Additionally, I'm somewhat siloed from the main NLP group on campus. I had briefly worked with one of the NLP profs on a project for a quarter but they did not continue our work due to ""resource constraints"" (in reality there was a difference in communication styles so we just didn't mesh very well).

So I guess the main things I need input on are: do I stay in my current lab for my PhD or do I try to improve my publication record this/next year, get better connected with the community, and apply again in the future? If I wait and apply again, what should be my course of action in the mean time (I'd like to continue doing research somehow or at least be doing something to advance my research goals)? How important is who you work with during your PhD? Will staying in a not well connected lab lead to limited prospects after my PhD (in terms of post-doc, industry labs, etc.)? Is it possible to have a satisfying career in this field without being in a top lab? I guess that last question depends on how you gauge satisfaction, which to me mostly lies in working on problems I consider interesting and exciting, but also that other people consider interesting as measured by publication in good (top) venues.

As a side note, my GPA is definitely not the issue (4.0 in undergrad, 3.92 in master's), my master's program is generally considered ""elite"", and my GRE is not too terrible (159/165/4 V/Q/W).

Thanks for taking the time to read this if you got this far :) any input is appreciated!",158,211,False,self,,,,,
1546,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,17,au5vtl,self.MachineLearning,Transformer Models,https://www.reddit.com/r/MachineLearning/comments/au5vtl/transformer_models/,piscoster,1550995285,[removed],0,1,False,self,,,,,
1547,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,18,au6hnq,self.MachineLearning,[D] Can small NN model 100% overfit?,https://www.reddit.com/r/MachineLearning/comments/au6hnq/d_can_small_nn_model_100_overfit/,ZdsAlpha,1551001257,"I am wondering if it is possible to reach 100% accuracy on NNs? If yes, how large does model have to be, in order to overfit the dataset.

&amp;#x200B;

I have working on (my custom) NN. I trained it on MNIST, first with dropout and then removed in later epochs.

It reached accuracy of 100% on train dataset and 99.397% on train dataset with model size of \~3.3 MB. So far, I haven't seen NNs overfit upto 100%.

&amp;#x200B;

I am currently testing my model on CIFAR10 dataset. I used smaller model \~9.5 MB. It reached maximum of 73.3% test accuracy when train accuracy was 80%.  It's still converging and reached 95.79% train accuracy with 72.3% test accuracy (1% accuracy drop). One thing to notice is that its over-fitting in presence of 50% dropout in final layer. (This time I haven't removed dropout)

&amp;#x200B;

I haven't seen any research focusing on model over-fitting. If this architecture is good at over-fitting maybe I can use it for data compression?",9,0,False,self,,,,,
1548,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,18,au6l5a,self.MachineLearning,[R] A PyTorch implementation of Higher-Order Graph Convolutional Layer (NeurIPS 2018),https://www.reddit.com/r/MachineLearning/comments/au6l5a/r_a_pytorch_implementation_of_higherorder_graph/,benitorosenberg,1551002212,"&amp;#x200B;

https://i.redd.it/3a2kcjqwnhi21.jpg

**GitHub:** [**https://github.com/benedekrozemberczki/NGCN**](https://github.com/benedekrozemberczki/NGCN)

**Paper:** [**http://sami.haija.org/papers/high-order-gc-layer.pdf**](http://sami.haija.org/papers/high-order-gc-layer.pdf)

**Abstract:**

Recent methods generalize convolutional layers from Euclidean domains to  graph-structured data by approximating the eigenbasis of the graph  Laplacian. The computationally-efficient and broadly-used Graph ConvNet  of Kipf &amp; Welling, over-simplifies the approximation, effectively  rendering graph convolution as a neighborhood-averaging operator. This  simplification restricts the model from learning delta operators, the  very premise of the graph Laplacian.  In this work, we propose a new  Graph Convolutional layer which mixes multiple powers of the adjacency  matrix, allowing it to learn delta operators. Our layer exhibits the  same memory footprint and computational complexity as a GCN. We  illustrate the strength of our proposed layer on both synthetic graph  datasets, and on several real-world citation graphs, setting the record  state-of-the-art on Pubmed.",0,1,False,https://b.thumbs.redditmedia.com/_1buLzuwe7seKHJknigkgLzeuXzqkvxCWUM7LgQZLNk.jpg,,,,,
1549,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,20,au767q,self.MachineLearning,What is the intuition behind a good CNN architecture for maximum accuracy with minimum parameters on MNIST dataset ?,https://www.reddit.com/r/MachineLearning/comments/au767q/what_is_the_intuition_behind_a_good_cnn/,_i_am_manu_,1551008000,[removed],0,1,False,self,,,,,
1550,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,20,au772h,self.MachineLearning,[R] Evaluating the impact of sensory inputs for self-driving cars.,https://www.reddit.com/r/MachineLearning/comments/au772h/r_evaluating_the_impact_of_sensory_inputs_for/,MyMastersAccount,1551008240,"Hey guys, was wondering if you knew of any papers that used only sensory inputs (such as speed, lane position, distance between cars etc) rather than images as a way to train/test a self-driving car model. 

During my research I have found many papers where sensory inputs only contribute a small part of the total training data (such as tyre direction sensor, LIDAR for distance between cars) but still mainly relied on image data. 

Is there any paper that exclusively used multiple sensor based inputs (without turning sensor data to images) to train a model?

Thank you for your help. 

",2,2,False,self,,,,,
1551,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,21,au7od0,self.MachineLearning,[D] Advice regarding professional ML Masters at CMU,https://www.reddit.com/r/MachineLearning/comments/au7od0/d_advice_regarding_professional_ml_masters_at_cmu/,koppo96,1551012653,"I am looking for reviews about the Intelligent Information Systems (MIIS) or Computational Data Science (MCDS) masters programs in the School of Computer Science at Carnegie Mellon University. I plan to join the industry as a Machine Learning Engineer after graduation. 

&amp;#x200B;

In these programs, there is a lot of freedom in choosing coursework and a wide array of ML courses to choose from. For example, 

* 10-701 Machine Learning (Ph.D.) (12) (F)
* 11-785 Intro to Deep Learning (12)  (Ruslan Salakhutdinov) (S)
* 11-747 Neural Networks for NLP (12)  (Graham Neubig) (S)
* 10-708 Probabilistic Graphical Models (12) (Eric Xing) (S)
* 10-805 Machine Learning with Big Data Sets (12)  (F)
* 10-703 Deep Reinforcement Learning &amp; Control (12) (F) 

are few of the courses that are of interest to me. An opportunity to take classes from some of the biggest names in the field is certainly appealing. 

&amp;#x200B;

But, CMU has a A LOT of Masters programs within the school of computer science (MS CS, MS LTI, MCDS, MIIS, MS ML, MS Robotics, MSRD, MS Computer Vision etc. Add to this other masters programs from the College of Engineering and Heinz College, which have a significantly lower admission bar). 

This implies stiff competition for ML jobs and I've read on this thread that some recruiters look down on masters programs at CMU due to a huge number of run of the mill programs, leading to reduced quality. 

&amp;#x200B;

My others options include Masters in CS at UC San Diego or Columbia. I'm yet to hear back from lot of the schools, since most results are announced in March. UIUC and Georgia Tech are the other main schools I'm looking at. None of them have a comparable quality of coursework though. Should I go ahead with CMU or still consider the above mentioned schools (Cost of programs is not a concern to me)?

&amp;#x200B;

Thanks!",0,2,False,self,,,,,
1552,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,21,au7ol1,self.MachineLearning,Can't decide between two PhD offers...,https://www.reddit.com/r/MachineLearning/comments/au7ol1/cant_decide_between_two_phd_offers/,BouillabaisseLover,1551012702,[removed],0,1,False,self,,,,,
1553,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,22,au7rjz,self.MachineLearning,[D] Why do empirical DL CV results contradict classical results in learning theory?,https://www.reddit.com/r/MachineLearning/comments/au7rjz/d_why_do_empirical_dl_cv_results_contradict/,ComplexIt,1551013472,"Please see following quote ([http://arxiv.org/abs/1710.09412](http://arxiv.org/abs/1710.09412))

&gt;In most successful applications, these neural networks share two commonalities. First, they are trained as to minimize their average error over the training data, a learning rule also known as the Empirical Risk Minimization (ERM) principle (Vapnik, 1998). Second, the size of these state-of-the- art neural networks scales linearly with the number of training examples. For instance, the network of the network of (Simonyan &amp; Zisserman, 2015) used 108 parameters to model the 106 images in the Springenberg et al. (2015) used 106 parameters to model the 5  104 images in the CIFAR-10 dataset, the 109 words in the One Billion Word dataset. ImageNet-2012 dataset, and the network of Chelba et al. (2013) used 2  1010 parameters to model  
&gt;  
&gt;Strikingly, a classical result in learning theory (Vapnik &amp; Chervonenkis, 1971) tells us that the convergence of ERM is guaranteed as long as the size of the learning machine (e.g., the neural network) does not increase with the number of training data. Here, the size of a learning machine is measured in terms of its number of parameters or, relatedly, its VC-complexity (Harvey et al., 2017).

Than the authors state:

&gt;This contradiction challenges the suitability of ERM to train our current neural network models, as highlighted in recent research. 

&amp;#x200B;

What is the relationship between first and second paragraph? As I understand the paragraph they do not contradict each other, i.e.: Because there is a overfitting trade off, more parameters will be successful if more data is available (as  observed in empirical DL research (first paragraph) and stated in classical learning theory (second paragraph)). 

&amp;#x200B;

This is the explanation they give in the following sentences:

&gt;On the one hand, ERM allows large neural networks to memorize (instead of generalize from) the training data even in the presence of strong regularization, or in classification problems where the labels are assigned at random (Zhang et al., 2017). On the other hand, neural networks trained with ERM change their predictions drastically when evaluated on examples just outside the training distribution (Szegedy et al., 2014), also known as adversarial examples. This evidence suggests that ERM is unable to explain or provide generalization on testing distributions that differ only slightly from the training data. However, what is the alternative to ERM?

I understand this explanation, but I do not understand how it is related to the previous text.

&amp;#x200B;

Thanks for interesting answers in advance. ",7,9,False,self,,,,,
1554,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,22,au83zw,self.MachineLearning,"Is anyone doing a Masters degree in Computer Science from Saarland University, Germany?",https://www.reddit.com/r/MachineLearning/comments/au83zw/is_anyone_doing_a_masters_degree_in_computer/,sohaib_01,1551016253,[removed],0,1,False,self,,,,,
1555,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,23,au8e6x,self.MachineLearning,[D] Pretraining On ImageNet vs OpenImages,https://www.reddit.com/r/MachineLearning/comments/au8e6x/d_pretraining_on_imagenet_vs_openimages/,JosephLChu,1551018284,Just wondering if anyone has tried comparing convolutional models pre-trained on ImageNet vs convolutional models pre-trained on OpenImages and then applied as visual feature extractors for other tasks like Image Captioning or Video Description?,4,8,False,self,,,,,
1556,MachineLearning,t5_2r3gv,2019-2-24,2019,2,24,23,au8moa,self.MachineLearning,[D] PhD in Informatics vs ECE at University of Illinois?,https://www.reddit.com/r/MachineLearning/comments/au8moa/d_phd_in_informatics_vs_ece_at_university_of/,soupcansam2374,1551019954,"I am graduating with a Bachelors in ECE at UIUC with a 3.1 GPA next semester and have been researching in a joint research project with an ECE professor and an affiliate ECE professor. They both want me to continue on to a PhD, applying for Spring 20 admission in September. Given my GPA, how hard would it be to get into the (MS and then) PhD in ECE program? Given that both professors are 'full' (i.e. not assistant but with no other info on them given) professors, how much pull do they have with the ECE department to offset my GPA with recommendations?

On a related note, as a fallback, I was thinking of also applying to the PhD in Informatics at UIUC in December, given that it has a track related to my research. Is this a good idea? I would have the same advisors, same research and funding, and take virtually the same courses. I know, based on my GPA, I can't be picky but what is the reputation of the Informatics PhD? Would I be at a disadvantage applying for fellowships and grants if I were a PhD in Informatics vs a PhD student in ECE at UIUC(assuming I even get in to either department)? If my goal is to conduct research in industry, would it open me up to the same opportunities for internships and full-time?

&amp;#x200B;

Other info about me - my GRE scores are 158 Verbal / 168 Math / 4.5 Writing.",6,2,False,self,,,,,
1557,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,0,au8rkp,self.MachineLearning,[D] What to do when the CNN kickoff isn't reliable?,https://www.reddit.com/r/MachineLearning/comments/au8rkp/d_what_to_do_when_the_cnn_kickoff_isnt_reliable/,erez27,1551020866,"I have a CNN that's capable of learning. When it catches the right drift, it learns well and deeply. But many times, it fails to even start.

The dataset is split so that it starts at 50% accuracy, 0.25 loss. Many times the network will stay stuck on the same 0.25 loss for a 100 epochs. But sometimes, I restart the process and it starts learning on the 10th epoch, and once it starts learning, everything proceeds as it should, eventually reaching 75% accuracy (can probably go even higher).

The model itself is fairly standard, CNNs, maxpools, and 2 dense layers to finish. Adam learning, MSE loss. It obviously works for this problem, except when it doesn't.

Am I missing something basic? Is there a way to improve the reliability of the kickoff?

Thanks, and sorry if this is a noob question!
",10,4,False,self,,,,,
1558,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,0,au8ruc,blockdelta.io,AI For Social Good - Re:Work's New Whitepaper,https://www.reddit.com/r/MachineLearning/comments/au8ruc/ai_for_social_good_reworks_new_whitepaper/,BlockDelta,1551020913,,0,1,False,default,,,,,
1559,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,0,au8x2k,self.MachineLearning,Where is TensorFlow 2.x on TensorFlow's GitHub ?,https://www.reddit.com/r/MachineLearning/comments/au8x2k/where_is_tensorflow_2x_on_tensorflows_github/,cdahms,1551021873,"February 24th, 2019 as I'm writing this.  I'm looking forward to the TF 2.0 changes (supposedly Keras as the default interface and moving the contrib content into other locations and removing contrib).

&amp;#x200B;

Looking at the TF's GitHub, I'm unclear on where the TF 2.x content is.  Some of the release notes [https://github.com/tensorflow/tensorflow/releases](https://github.com/tensorflow/tensorflow/releases) mention TF 2.x, for example here is one for the latest release as of when I'm typing this, 1.13.0:

&amp;#x200B;

`TensorFlow 2.0 Development`

* `Add a command line tool to convert to TF2.0, tf_upgrade_v2`
* `Merge tf.spectral`  
 `into tf.signal`  
 `for TensorFlow 2.0.`
* `Change the default recurrent activation function for LSTM from 'hard_sigmoid' to 'sigmoid' in 2.0. Historically recurrent activation is 'hard_sigmoid' since it is fast than 'sigmoid'. With new unified backend between CPU and GPU mode, since the CuDNN kernel is using sigmoid, we change the default for CPU mode to sigmoid as well. With that, the default LSTM will be compatible with both CPU and GPU kernel. This will enable user with GPU to use CuDNN kernel by default and get a 10x performance boost in training. Note that this is checkpoint breaking change. If user want to use their 1.x pre-trained checkpoint, please construct the layer with LSTM(recurrent_activation='hard_sigmoid') to fallback to 1.x behavior.`

&amp;#x200B;

Looking at the `branches` &amp; `tags`, I don't see anything like `v2.x` or `r2.x.alpha` or anything like that, it's all `1.x` stuff.  I did notice there are branches for `r1.9x`, which is clearly ahead of the released version, perhaps this is where the `2.x` content is living for now?

&amp;#x200B;

So my question is, where is the `2.x` content?  If it's included in master, how can the releases that are happening now prior to `2.x` (for example `1.13.0`) be made without the `2.x` breaking changes already being included?

&amp;#x200B;",0,1,False,self,,,,,
1560,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,0,au92zb,self.MachineLearning,Why are we giving shit to OpenAI for not releasing code when it's never been an obligation for anyone else?,https://www.reddit.com/r/MachineLearning/comments/au92zb/why_are_we_giving_shit_to_openai_for_not/,Valiox,1551022953,[removed],0,1,False,self,,,,,
1561,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,0,au933s,self.MachineLearning,[D] Why are we giving shit to OpenAI for not releasing code when it's never been an obligation for anyone else?,https://www.reddit.com/r/MachineLearning/comments/au933s/d_why_are_we_giving_shit_to_openai_for_not/,Valiox,1551022979,"I can't remember the last paper I read from Google where there was a single piece of code linked.

",3,0,False,self,,,,,
1562,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,0,au96b3,self.MachineLearning,Developing Alexa Skill from Scratch,https://www.reddit.com/r/MachineLearning/comments/au96b3/developing_alexa_skill_from_scratch/,prakhar21,1551023559,[removed],0,1,False,self,,,,,
1563,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,1,au9a98,self.MachineLearning,[D] Audio as an input to CNN,https://www.reddit.com/r/MachineLearning/comments/au9a98/d_audio_as_an_input_to_cnn/,youshouldknowsz,1551024251,"Anyone know a paper or website to learn more about step by step of STFT?

I wanna make spectrogram out of my audio data, so i can use it as an input to CNN.

I honestly have no experience with either of those two but i understand a bit about CNN.",12,6,False,self,,,,,
1564,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,1,au9ii9,youtube.com,Face recognition on Jetson TX2 with deep learning,https://www.reddit.com/r/MachineLearning/comments/au9ii9/face_recognition_on_jetson_tx2_with_deep_learning/,minhng92,1551025652,,0,1,False,https://b.thumbs.redditmedia.com/38YcQ2cropPmakkCSGvF2JGDv23XmHqkYWXL3FkFOwU.jpg,,,,,
1565,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,2,aua664,self.MachineLearning,[D] Detecting AI-Generated Content?,https://www.reddit.com/r/MachineLearning/comments/aua664/d_detecting_aigenerated_content/,itsalljustaride9,1551029379,"Hello,  


I'm a novice researching AI for a related field (second language acquisition). I'm curious about the social implications of AI-generated content like comments/text and faces. Has there been anyone who has looked at whether an AI can be trained to detect things like AI generated text or faces? I'm aware of some researchers who have tried to train AI to detect human-generated ""fake news"", but what of machine-generated content? Thanks!",4,3,False,self,,,,,
1566,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,2,auaeak,self.MachineLearning,Should I do exercises in PRML book?,https://www.reddit.com/r/MachineLearning/comments/auaeak/should_i_do_exercises_in_prml_book/,cipher1202,1551030600,[removed],0,1,False,self,,,,,
1567,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,3,auasa7,self.MachineLearning,[D] Having many different agents that learn to collaborate with each-other to solve different problems. Has this approach been tried? Is/Could it be effective?,https://www.reddit.com/r/MachineLearning/comments/auasa7/d_having_many_different_agents_that_learn_to/,2Punx2Furious,1551032715,"Today I thought about how social skills are important for humans, because they allow us to collaborate, working together and doings things that a single human can't do easily, or at all.

Could we make AI agents that do something similar? Instead of training just one agent to solve problems, have many different agents, train them separately, and have them collaborate to solve problems?",22,11,False,self,,,,,
1568,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,3,aub4n5,self.MachineLearning,Can someone explain me the update formula in the A3C algorithm?,https://www.reddit.com/r/MachineLearning/comments/aub4n5/can_someone_explain_me_the_update_formula_in_the/,ScrivaniaMicrosoft,1551034532,[removed],0,1,False,self,,,,,
1569,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,3,aub4ul,self.MachineLearning,"[D] Is ML going to become more wider than deeper, due to wider architures being more paralyzable ?",https://www.reddit.com/r/MachineLearning/comments/aub4ul/d_is_ml_going_to_become_more_wider_than_deeper/,BatmantoshReturns,1551034568,"That was the logic for some of the architecture choices in the Attention is all you need /transformer paper. 

Makes a lot of sense. I'm guessing ML architectures are going to become a lot wider than they are going to be deeper, considering the improvements in GPU(TPU) technology. The architectures are going to maximize the computational resources. ",6,3,False,self,,,,,
1570,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,4,aub6fa,self.MachineLearning,Paper where a neural network is trained based on the labels of another network,https://www.reddit.com/r/MachineLearning/comments/aub6fa/paper_where_a_neural_network_is_trained_based_on/,dev-ai,1551034810,"I am trying to find a paper that I think I looked a few months ago, where basically there was a neural network which predicted on new data, giving labels for another network to be trained, bu I can't seem to find it. Does it ring any bells to anyone here?",1,1,False,self,,,,,
1571,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,4,aub7fj,self.MachineLearning,"Graph neural networks for regression, which to choose?",https://www.reddit.com/r/MachineLearning/comments/aub7fj/graph_neural_networks_for_regression_which_to/,Peter_Emil,1551034945,[removed],0,1,False,self,,,,,
1572,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,4,aub7wm,self.MachineLearning,Architecture to M.Sc in Game Design and Interaction Technologies and integrating ML?,https://www.reddit.com/r/MachineLearning/comments/aub7wm/architecture_to_msc_in_game_design_and/,easynse,1551035012,[removed],0,1,False,self,,,,,
1573,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,4,aubhvn,self.MachineLearning,"[D] Google releases BodyPix, an open-source machine learning model which allows for a person and body-part segmentation in the browser",https://www.reddit.com/r/MachineLearning/comments/aubhvn/d_google_releases_bodypix_an_opensource_machine/,cmillionaire9,1551036524," [\#Google](https://www.youtube.com/results?search_query=%23Google) releases [\#BodyPix](https://www.youtube.com/results?search_query=%23BodyPix), an open-source machine learning model which allows for a person and body-part segmentation in the browser [\#machineleaning](https://www.youtube.com/results?search_query=%23machineleaning) [\#ai](https://www.youtube.com/results?search_query=%23ai) [\#innovation](https://www.youtube.com/results?search_query=%23innovation) [\#tech](https://www.youtube.com/results?search_query=%23tech) [\#technology](https://www.youtube.com/results?search_query=%23technology)   
https://youtu.be/yxSG2Upq85s",12,18,False,self,,,,,
1574,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,4,aublfr,exploringaiblog.wordpress.com,Learning Basic Data Analysis: When Pandas Make Your Life Easy,https://www.reddit.com/r/MachineLearning/comments/aublfr/learning_basic_data_analysis_when_pandas_make/,Daniyal9538,1551037058,,0,1,False,https://b.thumbs.redditmedia.com/bA-ECBjetmQ_GRlTA9cryFcM24iss3ZLH-T0j048Tro.jpg,,,,,
1575,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,5,auc0m2,self.MachineLearning,Consistently Predicting Tech Trends Decades Before They Happen | Daniel Burrus,https://www.reddit.com/r/MachineLearning/comments/auc0m2/consistently_predicting_tech_trends_decades/,The_Syndicate_VC,1551039304,"[Consistently Predicting Tech Trends Decades Before They Happen | Daniel Burrus](https://disruptors.fm/84-consistently-predicting-tech-trends-decades-before-they-happen-daniel-burrus/)

&amp;#x200B;

**Daniel Burrus** is considered one of the worlds leading futurist speakers on global trends and innovation and*The New York Times*has referred to him as one of the top three business gurus in the highest demand as a speaker.

Daniel is a strategic advisor to executives from Fortune 500 companies such as Microsoft, GE, American Express, Google, Deloitte, Procter &amp; Gamble, Honda, and IBM etc, helping them to develop game-changing strategies based on his proven methodologies for capitalizing on technology innovations and their future impact.He is the author of**seven** **books**, including*The New York Times*and*Wall Street Journal*best-seller[*Flash* *Foresight*](https://amzn.to/2q75XhZ), as well as the international best-seller[*Technotrends*](https://amzn.to/2D0EgAb).His latest book,[*The* *Anticipatory* *Organization:* *Turn* *Disruption* *and Change Into Opportunity and Advantage*](https://amzn.to/2D5Uvfc)is anAmazon #1 Hot New Releasefor Business

Daniel has been the featured subject of several PBS television specials and has appeared on programs such as CNN, Fox Business, and Bloomberg, and is quoted in a variety of publications, including The Wall Street Journal, Financial Times, Fortune, and Forbes and is a featured writer with millions of monthly readers on the topics of innovation, change and the future. His work has appeared in Harvard Business Review, Wired, CNBC, and Huffington Post to name a few.

Daniel has founded six businesses, four of which were national leaders in the United States in the first year. He is the CEO of Burrus Research, a research and consulting firm that monitors global advancements in technology driven trends to help clients profit from technological, social and business forces that are converging to create enormous, untapped opportunities.

&amp;#x200B;

**In** **our wide-ranging conversation, we cover many things, including:**

* How hard and soft trends change the world
* Why Daniel was able to predict major consumer tech breakthroughs decades before they happened
* What areas of technology Daniel is most excited about and why
* The reason Daniel is somewhat worried about CRISPR
* How to think about disruption and positive disruption specifically
* The reason some companies die and others thrive
* How to spot big tech trends
* Why Daniel avoids competition to crush it
* Why not all trends pan out
* The biggest mistake big companies make
* Why Daniel thinks Apple has something big planned soon",0,1,False,self,,,,,
1576,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,5,auc75z,self.MachineLearning,Advice for a mechanical engineer to begin a career in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/auc75z/advice_for_a_mechanical_engineer_to_begin_a/,zeusess30,1551040322,[removed],0,1,False,self,,,,,
1577,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,5,aucev8,self.MachineLearning,[D] Ideal situation for Stacking?,https://www.reddit.com/r/MachineLearning/comments/aucev8/d_ideal_situation_for_stacking/,Fender6969,1551041479,"Recently worked on a project in which a member of our team stacked a RF, XgBoost and LightGBM model using the vecstack package and finally fed that into a final XgBoost model and the performance was quite good. 

First time Ive seen this used and was wondering how often this method is done? When are some ideal situations in which this method works well?",6,15,False,self,,,,,
1578,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,6,auci7c,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 57,https://www.reddit.com/r/MachineLearning/comments/auci7c/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1551042004,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/whenmaster: [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318)

/u/UnluckyLocation: [Dynamic Sum Product Networks for Tractable Inference on Sequence Data](https://arxiv.org/abs/1511.04412)

/u/lmcinnes: [Limit theory for point processes in manifolds](https://arxiv.org/abs/1104.0914)

Besides that, there are no rules, have fun.",37,174,False,self,,,,,
1579,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,6,aud3yg,self.MachineLearning,Machine learning to bypass reCAPTCHA3?,https://www.reddit.com/r/MachineLearning/comments/aud3yg/machine_learning_to_bypass_recaptcha3/,sin31423,1551045340,[removed],0,1,False,self,,,,,
1580,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,7,audge2,self.MachineLearning,Does the discriminator represent anything in a GAN ?,https://www.reddit.com/r/MachineLearning/comments/audge2/does_the_discriminator_represent_anything_in_a_gan/,Uriopass,1551047274,[removed],0,1,False,self,,,,,
1581,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,7,audpfy,self.MachineLearning,"Prize Challenge($60,000 in prizes) - Call for Participation",https://www.reddit.com/r/MachineLearning/comments/audpfy/prize_challenge60000_in_prizes_call_for/,RoseVM,1551048711,[removed],0,1,False,self,,,,,
1582,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,8,aue1xz,self.MachineLearning,[D] Is Intel shelving its Nervana hardware in favor of Loihi chips?,https://www.reddit.com/r/MachineLearning/comments/aue1xz/d_is_intel_shelving_its_nervana_hardware_in_favor/,ComprehensiveCalm,1551050765,"I was doing some research about Intel after the previous thread about Mike Davies 'slamming deep learning'. 

&amp;#x200B;

1) Given the above statements from one of their higher ups ([https://www.zdnet.com/article/intels-neuro-guru-slams-deep-learning-its-not-actually-learning/](https://www.zdnet.com/article/intels-neuro-guru-slams-deep-learning-its-not-actually-learning/))

2) Switch to inference only hardware ([https://www.engadget.com/2019/01/07/intel-nervana-processor-for-inference/](https://www.engadget.com/2019/01/07/intel-nervana-processor-for-inference/))

3) One of the founders of Nervana moving to a 10+ year transistor project at Intel ([https://venturebeat.com/2019/02/21/intels-meso-transistor-promises-vast-leap-in-ai-processing-power/](https://venturebeat.com/2019/02/21/intels-meso-transistor-promises-vast-leap-in-ai-processing-power/))

&amp;#x200B;

Is Intel shelving the Nervana hardware they planned to release last year and moving in a drastically different direction for AI hardware? Part of me thinks these directions are really exciting. But I also would really really like some competition for Nvidia in this space in the near term.",3,8,False,self,,,,,
1583,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,8,aueb63,self.MachineLearning,When is a Master's Program worth it?,https://www.reddit.com/r/MachineLearning/comments/aueb63/when_is_a_masters_program_worth_it/,onehotoneshot,1551052326,[removed],0,1,False,self,,,,,
1584,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,9,auefdu,self.MachineLearning,[D] REINFORCE algorithm with a continuous and discrete action space,https://www.reddit.com/r/MachineLearning/comments/auefdu/d_reinforce_algorithm_with_a_continuous_and/,badjezus,1551053062,"I am working on a problem with a continuous and discrete action space. At the start state there are two discrete actions (a, b). If discrete action b is selected, then there is a value v in the range of [0, 1] that the agent must then select. The episode terminates after selecting either action a or action b (plus the subsequent value v if action b is selected).

[note: this is a massive oversimplification to my actual problem, but wanted to keep this question (relatively) straightforward]

Now, if the episode terminates after the agent selects action a, then the policy gradient can be learned in accordance to the REINFORCE algorithm
d{J(theta)} = R_a * d{theta} * log(p(a)),

where R_a is the reward from selecting action a and p(a) is the probability of selecting action a according the policy's parameters theta.

Now if action b is selected along with the follow up value v, i am unsure what the loss function should be to calculate the policy's gradient to maximize J(theta).

My guess would be that v is multiplied by log(p(b)), like so:
d{J(theta)} = R_b_v * d{theta} * log(p(b)) * v

where R_b_v is the reward gained from selecting b and the subsequent value v.

However, I am unsure if this is correct. If anyone is more familiar with Policy Based Reinforcement Learning than me and could chime in, I would very much appreciate it!",4,12,False,self,,,,,
1585,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,9,auew08,self.MachineLearning,why don't we use primitive Ai with animal like instincts,https://www.reddit.com/r/MachineLearning/comments/auew08/why_dont_we_use_primitive_ai_with_animal_like/,webstarter,1551055924,[removed],0,1,False,self,,,,,
1586,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,10,auf2ol,self.MachineLearning,Clustering time-series using Hidden Markov Model,https://www.reddit.com/r/MachineLearning/comments/auf2ol/clustering_timeseries_using_hidden_markov_model/,kian_89,1551057106,[removed],0,1,False,self,,,,,
1587,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,10,auf407,self.MachineLearning,[D] Any good books/blogs/frameworks to read to apply ML or even advanced stats in finance?,https://www.reddit.com/r/MachineLearning/comments/auf407/d_any_good_booksblogsframeworks_to_read_to_apply/,rulerofthehell,1551057333,"Hi, I am trying to get more into the finance part of machine learning, and I am not able to find where to start with it. I have the basic knowledge on the finance side of things and have read and taken some courses for portfolio optimizations, etc. But I trying to read more about the State of the Art models for such portfolio/investment models. Even linking some kaggle competition which is related would be great, thanks!",4,9,False,self,,,,,
1588,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,10,auf4pb,hackaday.com,Foundations For Machine Learning In English (Or Russian),https://www.reddit.com/r/MachineLearning/comments/auf4pb/foundations_for_machine_learning_in_english_or/,areameasurements,1551057469,,0,1,False,default,,,,,
1589,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,10,auf5kb,self.MachineLearning,Recommendations for Reinforcement Learning Project,https://www.reddit.com/r/MachineLearning/comments/auf5kb/recommendations_for_reinforcement_learning_project/,sainoraider,1551057624,[removed],0,1,False,self,,,,,
1590,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,10,auf8xr,self.MachineLearning,ML enthusiasts- how many parameters are needed to define/store a Support Vector Machines (SVM) model?,https://www.reddit.com/r/MachineLearning/comments/auf8xr/ml_enthusiasts_how_many_parameters_are_needed_to/,stats_nerd21,1551058212,[removed],0,1,False,self,,,,,
1591,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,10,aufd0r,self.MachineLearning,Resources for learning Sparse Coding to efficient retain information from time-series data?,https://www.reddit.com/r/MachineLearning/comments/aufd0r/resources_for_learning_sparse_coding_to_efficient/,stats_nerd21,1551058928,[removed],0,1,False,self,,,,,
1592,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,10,aufgct,self.MachineLearning,Private InfoSec Community,https://www.reddit.com/r/MachineLearning/comments/aufgct/private_infosec_community/,mentorsec,1551059509,"MentorSec is a private community dedicated to individuals who are both experienced in information security and those who are getting into the industry. The purpose of this server is to help individuals network and share industry knowledge. For entry members must first fill out a short application that gets manually approved by the moderators.

Key Features:

\* Option to get paired up with experienced industry veteran

\* Live cyber security news and published exploits

\* Section dedicated to development in languages such as python, php, rust, c/c++

\* Community events and conferences

\* Job listings

\* Channels to discuss certifications such as CISM, CISSP, GSEC, etc

\* Channel to connect with fellow members via linkedin

\* CTF channels

Invite Link: [https://discord.me/mentorsec](https://discord.me/mentorsec) 

&amp;#x200B;",0,1,False,self,,,,,
1593,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,11,aufnpg,cvpr2019.thecvf.com,CVPR 2019 Accepted Paper ID's,https://www.reddit.com/r/MachineLearning/comments/aufnpg/cvpr_2019_accepted_paper_ids/,xZoks,1551060814,,1,1,False,default,,,,,
1594,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,11,aufsb6,reddit.com,"Call for Workshop Papers &amp; Prize Challenge Participation ($60, 000 cash prize)",https://www.reddit.com/r/MachineLearning/comments/aufsb6/call_for_workshop_papers_prize_challenge/,wuzhenyu_sjtu,1551061634,,0,1,False,default,,,,,
1595,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,11,aufxck,reddit.com,Kendryte AI Tutorials(Chinese Version),https://www.reddit.com/r/MachineLearning/comments/aufxck/kendryte_ai_tutorialschinese_version/,Canaan-Creative,1551062524,,0,1,False,default,,,,,
1596,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,11,aufzzb,self.MachineLearning,[D] Is there an open-source code that does entity-level sentiment analysis based on BERT?,https://www.reddit.com/r/MachineLearning/comments/aufzzb/d_is_there_an_opensource_code_that_does/,finallyifoundvalidUN,1551062979,,2,2,False,self,,,,,
1597,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,11,aug02q,self.MachineLearning,How to ensemble two different models?,https://www.reddit.com/r/MachineLearning/comments/aug02q/how_to_ensemble_two_different_models/,schn19,1551062998,[removed],0,1,False,self,,,,,
1598,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,12,augbzo,clientsurgeseo.com,Top Fredericksburg Video SEO &amp;amp; Website Design Expert,https://www.reddit.com/r/MachineLearning/comments/augbzo/top_fredericksburg_video_seo_amp_website_design/,emeritaashwellq,1551065099,,0,1,False,default,,,,,
1599,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,12,augfnm,self.MachineLearning,[D] Multi-task learning open problems and good reads,https://www.reddit.com/r/MachineLearning/comments/augfnm/d_multitask_learning_open_problems_and_good_reads/,ewits,1551065759,What are people reading when it comes to multi-task learning with deep models? What are some open problems with MTL using deep models? What is the current state of MTL? ,2,12,False,self,,,,,
1600,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,12,augkn6,self.MachineLearning,Recommended Machine Learning Approach for Treatment Response Prediction,https://www.reddit.com/r/MachineLearning/comments/augkn6/recommended_machine_learning_approach_for/,CuriousGeoffrey,1551066640,[removed],0,1,False,self,,,,,
1601,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,13,augtxj,self.MachineLearning,Are we really entering an 'AI drought',https://www.reddit.com/r/MachineLearning/comments/augtxj/are_we_really_entering_an_ai_drought/,MrBarbaric,1551068354,[removed],0,1,False,self,,,,,
1602,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,13,auh166,pythonprogramming.in,Sentiment Analysis using Keras,https://www.reddit.com/r/MachineLearning/comments/auh166/sentiment_analysis_using_keras/,amitarora5423,1551069656,,0,1,False,default,,,,,
1603,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,14,auhcrf,self.MachineLearning,PyTorch Implementation of Feature Based NER with pretrained Bert,https://www.reddit.com/r/MachineLearning/comments/auhcrf/pytorch_implementation_of_feature_based_ner_with/,longinglove,1551071668,[removed],0,1,False,self,,,,,
1604,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,14,auhdjj,self.MachineLearning,[P] PyTorch Implementation of Feature Based NER with pretrained Bert,https://www.reddit.com/r/MachineLearning/comments/auhdjj/p_pytorch_implementation_of_feature_based_ner/,longinglove,1551071801,"# I know that you know [BERT](https://arxiv.org/abs/1810.04805). In the great paper, the authors claim that the pretrained models do great on NER without fine-tuning. It's even impressive, allowing for the fact that they don't use any autoregressive technique such as CRF. We try to reproduce the result in a simple manner.

&amp;#x200B;",13,64,False,self,,,,,
1605,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,14,auheo9,self.MachineLearning,[P] PyTorch Implementation of Feature Based NER with pretrained Bert,https://www.reddit.com/r/MachineLearning/comments/auheo9/p_pytorch_implementation_of_feature_based_ner/,longinglove,1551072010,"I know that you know [BERT](https://arxiv.org/abs/1810.04805). In the great paper, the authors claim that the pretrained models do great on NER without fine-tuning. It's even impressive, allowing for the fact that they don't use any autoregressive technique such as CRF. We try to reproduce the result in a simple manner.",1,1,False,self,,,,,
1606,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,14,auhete,self.MachineLearning,Navigating the Machine Learning Roadmap,https://www.reddit.com/r/MachineLearning/comments/auhete/navigating_the_machine_learning_roadmap/,patronageinstitute,1551072032,[removed],0,1,False,self,,,,,
1607,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,14,auhhpu,self.MachineLearning,[P] PyTorch Implementation of Feature Based NER with pretrained Bert,https://www.reddit.com/r/MachineLearning/comments/auhhpu/p_pytorch_implementation_of_feature_based_ner/,longinglove,1551072563,"I know that you know [BERT](https://arxiv.org/abs/1810.04805). In the great paper, the authors claim that the pretrained models do great on NER without fine-tuning. It's even impressive, allowing for the fact that they don't use any autoregressive technique such as CRF. We try to reproduce the result in a simple manner.

&amp;#x200B;

[https://github.com/Kyubyong/bert\_ner](https://github.com/Kyubyong/bert_ner)",1,1,False,self,,,,,
1608,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,14,auhisq,self.MachineLearning,We Run the Worlds Machine Learning Literally- By Dr. Kunal Singh Berwar,https://www.reddit.com/r/MachineLearning/comments/auhisq/we_run_the_worlds_machine_learning_literally_by/,patronageinstitute,1551072750,[removed],0,1,False,self,,,,,
1609,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,15,auhtka,self.MachineLearning,Perceptron Model,https://www.reddit.com/r/MachineLearning/comments/auhtka/perceptron_model/,abhinav_321,1551074790,[removed],0,1,False,self,,,,,
1610,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,15,auhzje,self.MachineLearning,Need a companion,https://www.reddit.com/r/MachineLearning/comments/auhzje/need_a_companion/,Anubhavr123,1551075943,[removed],0,1,False,self,,,,,
1611,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,15,aui1qs,self.MachineLearning,No. of weights/parameters needed to store a trained Gaussian SVM model for binary classification on future inputs?,https://www.reddit.com/r/MachineLearning/comments/aui1qs/no_of_weightsparameters_needed_to_store_a_trained/,stats_nerd21,1551076381,[removed],0,1,False,self,,,,,
1612,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,16,auikld,youtu.be,Augmented Reality in Business Card | Quytech,https://www.reddit.com/r/MachineLearning/comments/auikld/augmented_reality_in_business_card_quytech/,hiwilliam31,1551080290,,0,1,False,default,,,,,
1613,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,17,auis7j,self.MachineLearning,[D] Does the discriminator of a GAN represent anything?,https://www.reddit.com/r/MachineLearning/comments/auis7j/d_does_the_discriminator_of_a_gan_represent/,Uriopass,1551081973,"Suppose you train a GAN on a given dataset, you get a discriminator D and a generator G.

Usually, we throw D and use G for generating purposes.

But could you use D to answer the question ""Is this input in my dataset manifold?"" ?

Could this be used for NSFW image detection, for example ?",14,14,False,self,,,,,
1614,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,17,auivca,self.MachineLearning,Simple application idea used deep learning,https://www.reddit.com/r/MachineLearning/comments/auivca/simple_application_idea_used_deep_learning/,khongop,1551082665,[removed],0,1,False,self,,,,,
1615,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,17,auix9r,self.MachineLearning,[D] what is the best way to search for something in a image?,https://www.reddit.com/r/MachineLearning/comments/auix9r/d_what_is_the_best_way_to_search_for_something_in/,atum47,1551083097,"This might be a silly question but how does YOLO looks for occurrences of cats, dogs, humans... In a image with all that? Let's pretend I've trained a classifier with dogs and cats. For that I normalize the images to the same size. E.g.: 32x32 pixels. How do I predict a image in a different format? Like it's in a different orientation and even if I scale it down I would have to crop it so it can be 32x32. Maybe the title is wrong, cause I even have this doubt about machine learning and text. How do I classify text? Don't it need to be a fixed size inputs? (This collides with my previous question). Let's suppose I have the text ""I love you"" wich is 3 words of 7 letters. How a neural network process other inputs bigger or smaller?

If anyone could clarify this for me I would appreciate.",1,0,False,self,,,,,
1616,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,17,auizfm,self.MachineLearning,Number of weights/parameters needed to store a trained Gaussian Support Vector Machines model for binary classification?,https://www.reddit.com/r/MachineLearning/comments/auizfm/number_of_weightsparameters_needed_to_store_a/,stats_nerd21,1551083636,[removed],0,1,False,self,,,,,
1617,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,18,auj6nz,self.MachineLearning,10 Machine Learning Applications Already Impacting Your Life!,https://www.reddit.com/r/MachineLearning/comments/auj6nz/10_machine_learning_applications_already/,rahulwriter,1551085300,[removed],0,1,False,self,,,,,
1618,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,18,auj9kw,self.MachineLearning,Youtube Video Series on Machine Learning and Manufacturing,https://www.reddit.com/r/MachineLearning/comments/auj9kw/youtube_video_series_on_machine_learning_and/,tomrunge,1551085970,[removed],0,1,False,self,,,,,
1619,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,18,aujcvh,self.MachineLearning,"[R] A PyTorch implementation of ""A Higher-Order Graph Convolutional Layer"" (NeurIPS 2018).",https://www.reddit.com/r/MachineLearning/comments/aujcvh/r_a_pytorch_implementation_of_a_higherorder_graph/,benitorosenberg,1551086731,"&amp;#x200B;

https://i.redd.it/zpeghdd0noi21.jpg

**PyTorch:** [https://github.com/benedekrozemberczki/NGCN](https://github.com/benedekrozemberczki/NGCN)

**Paper:** [http://sami.haija.org/papers/high-order-gc-layer.pdf](http://sami.haija.org/papers/high-order-gc-layer.pdf)

**Abstract:**

Recent methods generalize convolutional layers from Euclidean domains to  graph-structured data by approximating the eigenbasis of the graph  Laplacian. The computationally-efficient and broadly-used Graph ConvNet  of Kipf &amp; Welling, over-simplifies the approximation, effectively  rendering graph convolution as a neighborhood-averaging operator. This  simplification restricts the model from learning delta operators, the  very premise of the graph Laplacian.  In this work, we propose a new  Graph Convolutional layer which mixes multiple powers of the adjacency  matrix, allowing it to learn delta operators. Our layer exhibits the  same memory footprint and computational complexity as a GCN. We  illustrate the strength of our proposed layer on both synthetic graph  datasets, and on several real-world citation graphs, setting the record  state-of-the-art on Pubmed.",25,133,False,https://b.thumbs.redditmedia.com/35hQMeF90FaV-qPiEMf4esXqgzs6LWTyI12rEsWQAlc.jpg,,,,,
1620,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,18,aujdvw,self.learnmachinelearning,[D] What's the best source to learn bayesian hierarchical models?,https://www.reddit.com/r/MachineLearning/comments/aujdvw/d_whats_the_best_source_to_learn_bayesian/,ic3fr0g93,1551086970,,0,1,False,default,,,,,
1621,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,18,aujjd6,self.MachineLearning,Video Series on Machine Learning and Manufacturing,https://www.reddit.com/r/MachineLearning/comments/aujjd6/video_series_on_machine_learning_and_manufacturing/,tomrunge,1551088288,[removed],0,1,False,self,,,,,
1622,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,19,aujvrf,self.MachineLearning,Speeding up Text-preprocessing using Dask [D],https://www.reddit.com/r/MachineLearning/comments/aujvrf/speeding_up_textpreprocessing_using_dask_d/,xsschauhan,1551091175,"Hey Everyone!

Recently I have been experimenting with Dask to use its parallelization on Dataframes. 

I wrote this blog post about speeding up text pre-processing using Dask: [https://medium.com/mindorks/speeding-up-text-pre-processing-using-dask-45cc3ede1366](https://medium.com/mindorks/speeding-up-text-pre-processing-using-dask-45cc3ede1366)

&amp;#x200B;

In the blog post, I achieved 2x speedup on doing some basic cleaning on 20 News Groups Dataset.

&amp;#x200B;

In projects I have seen speedups of upto 3x on preprocessing!

&amp;#x200B;

Other things that Dask has helped me with are processing out-of-memory datasets.

&amp;#x200B;

I would like to know what other ways Dask has helped you?",2,6,False,self,,,,,
1623,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,19,aujw5r,self.MachineLearning,A beginners guide to Linear Regression in Python with Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/aujw5r/a_beginners_guide_to_linear_regression_in_python/,champianalien21,1551091269,[removed],0,1,False,self,,,,,
1624,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,19,aujwkc,theappsolutions.com,[Discussion]: Conversational Interfaces - The Future of UI,https://www.reddit.com/r/MachineLearning/comments/aujwkc/discussion_conversational_interfaces_the_future/,lady_monsoon,1551091357,,0,1,False,default,,,,,
1625,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,19,aujzpq,self.MachineLearning,Java implementation of Doc2Vec,https://www.reddit.com/r/MachineLearning/comments/aujzpq/java_implementation_of_doc2vec/,mitml,1551092071,[removed],0,1,False,self,,,,,
1626,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,20,auk3qg,github.com,"[P] A curated list of resources for text detection/recognition (OCR, optical character recognition ) with deep learning methods.",https://www.reddit.com/r/MachineLearning/comments/auk3qg/p_a_curated_list_of_resources_for_text/,hwalsuklee,1551092993,,0,1,False,default,,,,,
1627,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,20,auk82x,i.redd.it,"[P] A curated list of resources for text detection/recognition (OCR, optical character recognition ) with deep learning methods.",https://www.reddit.com/r/MachineLearning/comments/auk82x/p_a_curated_list_of_resources_for_text/,hwalsuklee,1551093973,,1,1,False,https://b.thumbs.redditmedia.com/UUHGAdkdL5yTphNDmWK-GhyYnOAP7EIC-VYXOFOwwtM.jpg,,,,,
1628,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,21,aukph1,blockference.com,"Big Data 2019 | Warsaw, Poland",https://www.reddit.com/r/MachineLearning/comments/aukph1/big_data_2019_warsaw_poland/,Zukicha,1551097572,,0,1,False,default,,,,,
1629,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,21,aukrfq,self.MachineLearning,[P] A simple nonlinear Autoencoder in Python.,https://www.reddit.com/r/MachineLearning/comments/aukrfq/p_a_simple_nonlinear_autoencoder_in_python/,rezaofdegreesix,1551097960,"In this Jupyter notebook I go over modeling and implementation of a simple example of a nonlinear Autoencoder, and its connection to PCA (the linear Autoencoder).

[https://jermwatt.github.io/machine\_learning\_refined/notes/10\_Nonlinear\_intro/10\_6\_Unsupervised.html](https://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_6_Unsupervised.html)

\^ This is an html version of the Jupyter notebook. Pull the repo to manipulate the code / make your own Autoencoder in Jupyter. 

&amp;#x200B;

https://i.redd.it/2aumtihnjpi21.png",5,14,False,self,,,,,
1630,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,21,aukykh,self.MachineLearning,[D] Has anyone done a study in the robustness of Capsule Networks against adversarial examples?,https://www.reddit.com/r/MachineLearning/comments/aukykh/d_has_anyone_done_a_study_in_the_robustness_of/,HecknBamBoozle,1551099376,Intuitively they should not be affected by these examples but what about in practice?,8,43,False,self,,,,,
1631,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,22,aul6j1,self.MachineLearning,Servo Motors Market Insights and Global Industry Forecast to 2023,https://www.reddit.com/r/MachineLearning/comments/aul6j1/servo_motors_market_insights_and_global_industry/,Amar_bir1,1551100839,[removed],1,1,False,self,,,,,
1632,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,22,aul89k,self.MachineLearning,"[D] Great idea, how to publish?",https://www.reddit.com/r/MachineLearning/comments/aul89k/d_great_idea_how_to_publish/,zxall,1551101155,"I'm an independent software developer. While working on my own image processing project I invented a ""thing"".  There is no reason to keep it to myself, as idea it's not patentable. What I'd like is to make it public and add something to my resume. What is the right way of doing it? 
As for idea:
  1. It gives great, clear benefits.
  2. it's generic and applicable in other domains as well.
  3. it's intuitive. If I tell most of you will say 'nice' even before looking at the results.
  4. it's easy to implement. I did it in pytorch, it's as easy to do in tensorflow or any other similar tool.

It would be quite popular, but I've seen nothing like this. So, I think it's something really new and worth publishing.",65,106,False,self,,,,,
1633,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,23,aulvbk,self.MachineLearning,Recommendations for enterprise HPC ML station ?,https://www.reddit.com/r/MachineLearning/comments/aulvbk/recommendations_for_enterprise_hpc_ml_station/,pirsonalthrowaway,1551105285,[removed],0,1,False,self,,,,,
1634,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,23,aum329,self.MachineLearning,[D] Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling https://arxiv.org/abs/1902.08295,https://www.reddit.com/r/MachineLearning/comments/aum329/d_lingvo_a_modular_and_scalable_framework_for/,sinashish,1551106631,,3,2,False,self,,,,,
1635,MachineLearning,t5_2r3gv,2019-2-25,2019,2,25,23,aum3yq,activewizards.com,Top 7 Data Science Use Cases in Travel,https://www.reddit.com/r/MachineLearning/comments/aum3yq/top_7_data_science_use_cases_in_travel/,viktoriia_shulga,1551106784,,0,12,False,default,,,,,
1636,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,0,aumank,self.MachineLearning,What's the fastest way to get cultural data of communities,https://www.reddit.com/r/MachineLearning/comments/aumank/whats_the_fastest_way_to_get_cultural_data_of/,kneyem,1551107807,[removed],0,1,False,self,,,,,
1637,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,0,aumbt7,self.MachineLearning,Number of parameters in BERT base model,https://www.reddit.com/r/MachineLearning/comments/aumbt7/number_of_parameters_in_bert_base_model/,NLP_RL,1551107988,[removed],0,1,False,self,,,,,
1638,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,0,aumizn,self.MachineLearning,"Real-time style transfer in a video game, just for fun",https://www.reddit.com/r/MachineLearning/comments/aumizn/realtime_style_transfer_in_a_video_game_just_for/,mrfahrenheit94,1551109162,[removed],0,1,False,self,,,,,
1639,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,0,aumm43,self.MachineLearning,[P] PyCM 1.9 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/aumm43/p_pycm_19_released_machine_learning_library_for/,sepandhaghighi,1551109653,[removed],0,1,False,self,,,,,
1640,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,0,aummtx,self.MachineLearning,"Practical Artificial Intelligence: Machine Learning, Bots, And Agent Solutions Using C#",https://www.reddit.com/r/MachineLearning/comments/aummtx/practical_artificial_intelligence_machine/,mritraloi6789,1551109761,[removed],0,1,False,self,,,,,
1641,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,0,aumo0x,self.MachineLearning,"[P] Real-time style transfer in a video game, just for fun",https://www.reddit.com/r/MachineLearning/comments/aumo0x/p_realtime_style_transfer_in_a_video_game_just/,mrfahrenheit94,1551109943,[removed],6,66,False,self,,,,,
1642,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aumrfk,awebb.info,[D] Distributed joint training of ensemble classifiers with low comms overhead by using an implicit loss / explicitly setting part of the gradient,https://www.reddit.com/r/MachineLearning/comments/aumrfk/d_distributed_joint_training_of_ensemble/,grey--area,1551110458,,0,1,False,default,,,,,
1643,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aumy7l,medium.com,SenseTime Trains ImageNet/AlexNet In Record 1.5 minutes,https://www.reddit.com/r/MachineLearning/comments/aumy7l/sensetime_trains_imagenetalexnet_in_record_15/,Yuqing7,1551111445,,0,1,False,https://b.thumbs.redditmedia.com/4xAZbQD4KE9W18pZywQkBz7QAjGe0-uEouzp1dRbaUg.jpg,,,,,
1644,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aumziw,self.MachineLearning,[D] Need help in implementing the BFGAN paper,https://www.reddit.com/r/MachineLearning/comments/aumziw/d_need_help_in_implementing_the_bfgan_paper/,chain20,1551111631,"I am trying to implement the [BFGAN](https://arxiv.org/abs/1806.08097) paper, but not sure how to start. The implementation looks a bit tricky and it would be silly to implement every component from scratch. Can someone provide a few pointers about implementing such a paper or even some starter code?",2,1,False,self,,,,,
1645,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aun2eo,self.MachineLearning,UG2+ Prize Challenge at CVPR 2019 ($60K in prizes!)[News],https://www.reddit.com/r/MachineLearning/comments/aun2eo/ug2_prize_challenge_at_cvpr_2019_60k_in_prizesnews/,RoseVM,1551112052," Registration is now open for the **UG2+ Prize Challenge at CVPR 2019**. We are offering $60K in prizes! If you are interested in image restoration and enhancement, you should check it out:  
**UG2+ Bridging the Gap between Computational Photography and Visual Recognition**  
The 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2019)  
June 16th-21th, 2019  
Long Beach, CA, USA  
Website: [http://www.ug2challenge.org](http://www.ug2challenge.org/)  


Continuing  the success of the 1st UG2 Prize Challenge workshop held at CVPR 2018,  UG2+ provides an integrated forum for researchers to review the recent  progress of handling various adverse visual conditions in real-world  scenes, in robust, effective and task-oriented ways. Beyond the human  vision-driven restorations, we also extend particular attention to the  degradation models and the related inverse recovery processes that may  benefit successive machine vision tasks. We embrace the most advanced  deep learning systems but are still open to classical physically  grounded models, as well as any well-motivated combination of the two  streams. The workshop will consist of four invited talks, together with  peer-reviewed regular papers (oral and poster), and talks associated  with winning prize challenge contributions.  


The UG2+ Challenge  seeks to advance the analysis of ""difficult"" imagery by applying image  restoration and enhancement algorithms to improve analysis performance.  Participants are tasked with developing novel algorithms to improve the  analysis of imagery captured under problematic conditions.  


This prize challenge has two components that have been combined for a unified workshop at CVPR:  


1. **Video object classification and detection from unconstrained mobility platforms:** Image restoration and enhancement algorithms that remove corruptions like blur,  noise, and mis-focus, or manipulate images to gain resolution, change  perspective and compensate for lens distortion are now commonplace in  photo editing tools. Such operations are necessary to improve the  quality of images for recognition purposes. But they must be compatible  with the recognition process itself, and not adversely affect feature  extraction or decision making. Sub-Challenges:
   1. Object Detection Improvement on Video
   2. Object Classification Improvement on Video
2. **Object Detection in Poor Visibility Environments:**  While most current vision systems are designed to perform in  environments where the subjects are well observable without  (significant) attenuation or alteration, a dependable vision system must  reckon with the entire spectrum of complex unconstrained and dynamic  degraded outdoor environments. It is highly desirable to study to what  extent, and in what sense, such challenging visual conditions can be  coped with, for the goal of achieving robust visual sensing. Sub-Challenges:
   1. (Semi-)Supervised Object Detection in Haze Conditions
   2. (Semi-)Supervised Face Detection in Low Light Conditions
   3. Zero-Shot Object Detection with Raindrop Occlusions

$60,000 will be awarded in prizes to the best performing submissions!  


**Important Dates:**  
 

1. Paper Submission  
May 1, 2019: Paper submission deadline  
May 10, 2019: Paper decision notification  
May 17, 2019: Paper camera ready
2. Challenge Participation:  
January 31, 2019: Development kit and registration made available  
March 15 - April 15, 2019: Dry run period  
April 1, 2019: Registration deadline  
May 1, 2019: Challenge submission deadline  
May 20, 2019: Challenge results will be released  
June 18, 2019: Most successful and innovative teams present at CVPR 2019 workshop

**Organization Committee:**  


* **Walter** **Scheirer**,Assistant Professor, Notre Dame University, USA
* **Zhangyang** **(Atlas)** **Wang**, Assistant Professor, Texas A&amp;M University, USA
* **Jiaying** **Liu**, Associate Professor, Peking University, China
* **Wenqi** **Ren**,Assistant Professor,Chinese Academy of Sciences, China
* **Wenhan** **Yang**, Postdoc Researcher, City University of Hong Kong, Hong Kong, China
* **Kevin** **Bowyer**, Schubmehl-Prein Family Professor, Notre Dame University, USA
* **Thomas** **S.** **Huang**, Maybelle Leland Swanlund Endowed Chair Emeritus, University of Illinois at Urbana-Champaign, USA
* **Sreya** **Banerjee**,Graduate Student,Notre Dame University, USA
* **Rosaura** **Vidal-Mata**,Graduate Student, Notre Dame University, USA
* **Ye Yuan**,Graduate Student,Texas A&amp;M University, USA

For information on the challenge, rules, and submissions, please visit the workshop's website: [www.ug2challenge.org](http://www.ug2challenge.org/) ",0,8,False,self,,,,,
1646,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aun3ht,self.MachineLearning,Docker Image for Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/aun3ht/docker_image_for_reinforcement_learning/,banksyb00mb00m,1551112212,[removed],0,1,False,self,,,,,
1647,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aun4gx,self.MachineLearning,[D] Docker Image with Standard Reinforcement Learning Packages?,https://www.reddit.com/r/MachineLearning/comments/aun4gx/d_docker_image_with_standard_reinforcement/,banksyb00mb00m,1551112352,"I have been super-frustrated in recent times at figuring out all the  dependencies of OpenAI Gym and MuJoCo, and have been wondering  if anyone has plans for maintaining a Docker Image with standard RL  packages (there is none at GitHub). Such a project would save hours for  any researcher/ developer tinkering with RL.",2,1,False,self,,,,,
1648,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aun6ul,self.MachineLearning,[D] [blog post] Distributed joint training of ensemble classifiers with low comms overhead by using an implicit loss function,https://www.reddit.com/r/MachineLearning/comments/aun6ul/d_blog_post_distributed_joint_training_of/,grey--area,1551112722,"In the course of doing some research recently, I came across a method for easily distributing the (joint) training of an ensemble of neural networks over a computational cluster without the framework having to know about/manage the cluster and its communication.

The gist is that you use an implicit loss function / explicitly set a factor in the gradient, for example by using the grad_tensors argument in the backward function in PyTorch, and then manually communicate the network outputs.

I've written a blog post about it here, using a classifier ensemble as an example: [http://www.awebb.info/blog/trivial_distributed](awebb.info/blog/trivial_distributed).",0,4,False,self,,,,,
1649,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aunakf,self.MachineLearning,The best Python library and modules for the score predictions for February 2019.,https://www.reddit.com/r/MachineLearning/comments/aunakf/the_best_python_library_and_modules_for_the_score/,2bPskcZjhf,1551113249,[removed],0,1,False,self,,,,,
1650,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,1,aunetd,themlearning.com,"[P] Image-Based Airbnb Pricing Algorithm, Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.",https://www.reddit.com/r/MachineLearning/comments/aunetd/p_imagebased_airbnb_pricing_algorithm_hey_guys_i/,themlearning,1551113887,,0,1,False,default,,,,,
1651,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,2,aunlgq,self.MachineLearning,"Machines, Mechanism And Robotics: Proceedings Of INaCoMM 2017",https://www.reddit.com/r/MachineLearning/comments/aunlgq/machines_mechanism_and_robotics_proceedings_of/,mritraloi6789,1551114816,[removed],0,1,False,self,,,,,
1652,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,2,auno5n,self.MachineLearning,Can someone create a clear roadmap for Machine Learning Introduction ?,https://www.reddit.com/r/MachineLearning/comments/auno5n/can_someone_create_a_clear_roadmap_for_machine/,ArtificialReddit,1551115206,"Suppose someone has some basic math background. How do you start with Machine Learning? For example everyone says you need the 4 basic elements to start machine learning: Linear Algebra, Calculus, Statistics and Probability. But we need specific books and specific things to focus more rather than diving too much into useless details. So for start someone point something like: Linear Algebra- start with these books, etc. And after you finish with the prerequisites for math background, where do we go next. Which book to start from there ? Both theoretical and applied Machine Learning (for example not just Machine Learning in Python or strictly using TensorFlow but also more general topics in theory). I think lots of people would appreciate such a detailed roadmap on where to start and where to go next focusing mainly on books and not just video lectures.",0,1,False,self,,,,,
1653,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,3,auoqnd,self.MachineLearning,[P] Pruning for CNNs which can be used to reduce the model's size and inference time,https://www.reddit.com/r/MachineLearning/comments/auoqnd/p_pruning_for_cnns_which_can_be_used_to_reduce/,gabegabe6,1551120643,"When we are trying to move our models to mobile or embedded devices, size and inference time is a crucial point. 
With automatic pruning solutions we can control these properties with a trade-off in accuracy. (Btw, it is not always true, that there is a accuracy drop. It really depends on the network and the dataset.)

As I wanted to test several methods, I could not find a simple solution with Keras models so I made a *mini-framework* which can be easily used by anyone for any model. It is called [**Ridurre**](https://gaborvecsei.github.io/Ridurre-Network-Pruning/).

- [Project post](https://gaborvecsei.github.io/Ridurre-Network-Pruning/)
- [GitHub repo](https://github.com/gaborvecsei/Ridurre-Network-Filter-Pruning-Keras)

I would love to hear your thoughts and ideas on how could we improve this package. There is also an *aggressive pruning example* so you can see how easy is to get rid of the redundant filters in your model.",12,14,False,self,,,,,
1654,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,3,auos39,self.MachineLearning,How to load Pytorch Deep Neural Networks model on C++ program code?,https://www.reddit.com/r/MachineLearning/comments/auos39/how_to_load_pytorch_deep_neural_networks_model_on/,DinaTAKLIT,1551120842,[removed],0,1,False,self,,,,,
1655,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,3,auottw,self.MachineLearning,[D][NSFW] This vagina does not exist,https://www.reddit.com/r/MachineLearning/comments/auottw/dnsfw_this_vagina_does_not_exist/,sazerak,1551121080,"The project: [https://thisvaginadoesnotexist.com/](https://thisvaginadoesnotexist.com/)

These images are all generated by a computer. No part of them are real photos. They have been generated by a machine learning model which has been shown real pornography images and learns how to make its own. Even more specifically, it is a [Generative Adversarial Network (GAN)](https://towardsdatascience.com/generative-adversarial-networks-gans-a-beginners-guide-5b38eceece24).

It generates (quite small, somewhat distorted) NSFW pictures of vaginas in a porn style. Whilst it isnt really good enough to be a source of porn now, I could imagine these models quickly getting there. 

Is this good or bad for the adult industry? 

Would this be a source of porn people might want to use? ",11,6,True,nsfw,,,,,
1656,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,4,auox91,self.MachineLearning,"Is it useful to use ""0"" padding with masking layer in case of Multi-Layers Peppercorns to handle variable length of input ?",https://www.reddit.com/r/MachineLearning/comments/auox91/is_it_useful_to_use_0_padding_with_masking_layer/,DinaTAKLIT,1551121538,[removed],0,1,False,self,,,,,
1657,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,4,aup18d,github.com,[P] Examples &amp; best practices for building classical and deep recommender systems from Microsoft,https://www.reddit.com/r/MachineLearning/comments/aup18d/p_examples_best_practices_for_building_classical/,nikhilj_msft,1551122096,,0,1,False,default,,,,,
1658,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,4,aup1n8,self.MachineLearning,Machine learning to guess on math multiple choice,https://www.reddit.com/r/MachineLearning/comments/aup1n8/machine_learning_to_guess_on_math_multiple_choice/,eugenebaba,1551122152,[removed],0,1,False,self,,,,,
1659,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,5,aupt37,i.redd.it,"""[R]"" : Caption This",https://www.reddit.com/r/MachineLearning/comments/aupt37/r_caption_this/,magkum123,1551126014,,0,1,False,https://a.thumbs.redditmedia.com/Sw9cGEMn_VLM9JUr-uOn-C2hHHfO-HZoHdXNlCcmOs0.jpg,,,,,
1660,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,5,aupxmx,self.MachineLearning,Self-driving car follow the route,https://www.reddit.com/r/MachineLearning/comments/aupxmx/selfdriving_car_follow_the_route/,Shredinger_129,1551126650,"Hello everyone!

Recently im make my first self-driving car. 

I took Nvidia NN architecture as a basis.

It described here: https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf

But i have a question:

Now, my car just follow road lines. But i want make something new:

At last video from sentdex (https://www.youtube.com/watch?v=rvnHikUJ9T0&amp;t=915s) he make GPS-follow AI for GTA 5.

His model holds both gps and road markings simultaneously.

How he make this? What i should use as input to NN model?

P.S For my model i use something like this:

https://i.imgur.com/zA1Lrs5.png

But this method just ignored pink line of GPS and trying to keep the road marking!

Im choise wrong model? 

P.S As input - im use what I mentioned earlier and resize it to 135x135 pixels (because im learning model on GTX 1050, she have only 2 gb of ram :c. Maybe of the resizing of the image of the vehicle and does not keep gps route?",0,1,False,self,,,,,
1661,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,5,aupzd0,self.MachineLearning,Sdd,https://www.reddit.com/r/MachineLearning/comments/aupzd0/sdd/,eugenebaba,1551126884,[removed],0,1,False,self,,,,,
1662,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,5,auq3g8,themlearning.com,"[P]Image-Based Airbnb Pricing Algorithm, Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.",https://www.reddit.com/r/MachineLearning/comments/auq3g8/pimagebased_airbnb_pricing_algorithm_hey_guys_i/,themlearning,1551127462,,0,1,False,https://b.thumbs.redditmedia.com/n6bd23LovU03smeD0HQNKD9wSc8L6R1TWIKw2OfHYwg.jpg,,,,,
1663,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,6,auqfpg,self.MachineLearning,Looking for good data set or pretrained model for geometry extraction,https://www.reddit.com/r/MachineLearning/comments/auqfpg/looking_for_good_data_set_or_pretrained_model_for/,CEchols,1551129138,"Hello Everyone! 

&amp;#x200B;

I am relatively new to machine learning / deep learning, but I have just about completed the [deeplearning.ai](https://deeplearning.ai) courses on coursera, so I am looking for some practical experience. I came up with a scenario I would like to work on, and was wondering if anyone could point me to data sets preferable, or even pre trained models I can start with.

&amp;#x200B;

My goal is to be able to use map imagery as an input, and the output will be a typical classifier output that will indicate if there are say building footprints in the map segment and if there is also pull out the vertices of the polygon that is the building on the map. I would love to provide any additional information I am able, I am just not sure what that is, any pointers, data sources, literature, or pre trained models would be super helpful!

&amp;#x200B;

I was planning to use Tensorflow / Keras for this, but I would also love some opinions on the library to use if anyone has an opinion!

&amp;#x200B;

Thank you!",0,1,False,self,,,,,
1664,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,6,auqhzp,self.MachineLearning,Is it a bad idea to quit a SE job in order to contribute to RL research full time for free?,https://www.reddit.com/r/MachineLearning/comments/auqhzp/is_it_a_bad_idea_to_quit_a_se_job_in_order_to/,void_monkey,1551129454,[removed],0,1,False,self,,,,,
1665,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,6,auqlmk,self.MachineLearning,[P] PyCM 1.9 released: Classification analysis + parameter recommender,https://www.reddit.com/r/MachineLearning/comments/auqlmk/p_pycm_19_released_classification_analysis/,sepandhaghighi,1551129981,[removed],0,1,False,self,,,,,
1666,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,6,auqnjq,self.MachineLearning,[Discussion] Is it a bad idea to quit a SE job in order to contribute to RL research full time for free?,https://www.reddit.com/r/MachineLearning/comments/auqnjq/discussion_is_it_a_bad_idea_to_quit_a_se_job_in/,void_monkey,1551130266,"Hi everyone.

I wanted to hear the community's opinion on my current situation and see what people think in general about a career decision that I'm seriously considering. (Sorry if this is not the right place for this.)

I'm currenly working as a software engineer at a widely-recognized financial institution. This is my first job and I've been within the firm forabout two years now. The money is alright, it's also not a stressful job. The thing is that this is not the kind of job that I want to do for the rest of my life. My master's thesis (engineering) was focused around ML and ever since graduating, I've been exploring it, mostly deep learning, on my own in my spare time mostly via online courses and books as I really enjoyed it. Then I discovered RL and immediately fell in love. After doing even more online courses and starting some self-assigned projects, I realized I wanted to transition to ML, preferably RL.

Currently my portfolio and ML project experience are way too weak for getting a good job in the area. I would preferably become involved in research as an engineer and later on, if things go well, perhaps start a PhD in CS. Recently I realized that there is no way I can improve at a decent pace as my 9-5 job is taking away a big part of my time and even if I manage to keep learning before/after work (as I've been doing for the past 15 months or so), it's going to take me a lot of time to achieve a decent level, not to mention taking away almost my whole free time and social life.

I finally came to the conclusion that the only way for me to step up in ML/RL is to quit my job and get engaged with researchers in order to have my name put on a research paper, which is often a ""nice-to-have"" requirement when I'm looking at different ML/RL engineer job offers. This would also help me grow my portfolio which is at least equally as important. I recently met a group of researchers (PhDs and PhD students) at the Uni in my town who might be interested in cooperating however this wouldn't be a regular job, more likely just a collaboration which would eventually end up publishing a paper together.

I would like to know if quitting my job without having a new one lined up is a terrible idea career-wise. I heard that some HR departments are reluctant to hire people who are currently unemployed, so this might be a risky move. On the other hand I know that if I don't do something about my current situation, I'll end up remaining in my current job (or similar) for the rest of my life. Money is not a concern for me at the moment - I can afford several months of unemployment. I basically want to know what does the community think about this and if I'm missing anything.

Any input is really appreciated. Thanks in advance :)",81,98,False,self,,,,,
1667,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,6,auqrbd,self.MachineLearning,[P]PyCM 1.9 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/auqrbd/ppycm_19_released_machine_learning_library_for/,sepandhaghighi,1551130815,[removed],0,1,False,self,,,,,
1668,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,6,auqx51,self.MachineLearning,[Project] PyCM 1.9 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/auqx51/project_pycm_19_released_machine_learning_library/,sepandhaghighi,1551131638,[removed],0,1,False,self,,,,,
1669,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,8,aurrtx,self.MachineLearning,Paper on generating melody from lyric--How difficult to implement?,https://www.reddit.com/r/MachineLearning/comments/aurrtx/paper_on_generating_melody_from_lyrichow/,enverx,1551136170,[removed],0,1,False,self,,,,,
1670,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,8,aus2oc,youtu.be,"Hello machineLearning community, I am new here. I just started a machine learning series on YouTube and I need your support in order to grow with machine learning. I'm on my second episode about classification and data preprocessing. Link here",https://www.reddit.com/r/MachineLearning/comments/aus2oc/hello_machinelearning_community_i_am_new_here_i/,epic_society,1551137845,,0,1,False,default,,,,,
1671,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,8,aus3eg,self.MachineLearning,Thoughts on those chinese chatbots that are against the comunist party (?),https://www.reddit.com/r/MachineLearning/comments/aus3eg/thoughts_on_those_chinese_chatbots_that_are/,umbrelamafia,1551137955,[removed],0,1,False,self,,,,,
1672,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,9,ausd4r,self.MachineLearning,ADVICE!!!!,https://www.reddit.com/r/MachineLearning/comments/ausd4r/advice/,supla99,1551139504,"Hi!  I start Andrew class ML at coursera. In week two I've  stuck then I read some books,however it's the same problem so please I want some guide to fellow or advices to look forward, I really want to learn ML.

EDIT :Self-taught. ",0,1,False,self,,,,,
1673,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,9,ausf7m,self.MachineLearning,Private InfoSec Community,https://www.reddit.com/r/MachineLearning/comments/ausf7m/private_infosec_community/,realmentorsec,1551139823,[removed],0,1,False,self,,,,,
1674,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,9,ausn0u,self.MachineLearning,Self programming AI,https://www.reddit.com/r/MachineLearning/comments/ausn0u/self_programming_ai/,All-The-Glowing-Eyes,1551141091,[removed],0,1,False,self,,,,,
1675,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,9,ausp6s,crazyoscarchang.github.io,[D] 7 Myths in ML Research Oscar Chang,https://www.reddit.com/r/MachineLearning/comments/ausp6s/d_7_myths_in_ml_research_oscar_chang/,yazriel0,1551141456,,1,1,False,default,,,,,
1676,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,9,auspv3,self.MachineLearning,What algorithms would best suit this application?,https://www.reddit.com/r/MachineLearning/comments/auspv3/what_algorithms_would_best_suit_this_application/,openjuggle,1551141570,[removed],0,1,False,self,,,,,
1677,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,9,ausw6b,i.redd.it,"Feedforward Neural Network Graph. Math, but also art.",https://www.reddit.com/r/MachineLearning/comments/ausw6b/feedforward_neural_network_graph_math_but_also_art/,isaiahnields,1551142621,,0,1,False,default,,,,,
1678,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,11,autr8q,self.MachineLearning,E-LSTM-D: A Deep Learning Framework for Dynamic Network Link Prediction,https://www.reddit.com/r/MachineLearning/comments/autr8q/elstmd_a_deep_learning_framework_for_dynamic/,limTeTE,1551147767,[removed],0,1,False,self,,,,,
1679,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,12,auu7bo,self.MachineLearning,[D] Algorithms for improving human skill acquisition?,https://www.reddit.com/r/MachineLearning/comments/auu7bo/d_algorithms_for_improving_human_skill_acquisition/,openjuggle,1551150399,"If we take a measurable skill such as juggling, where we catalog the specifics surrounding every endurance juggling attempt. These specifics being things such as date/time of run, run length, pattern name. What algorithms if any would be best suited for making recomendations of which patterns to work on when in an effort to maximize the chances of breaking personal records, or cause better general improvement?

In the world of motor skill retention, it is generally accepted that interleaving is a superior practice method to blocking so far as skill acquisition goes, but I imagine there very well may be something more efficient which could be found with machine learning.",0,8,False,self,,,,,
1680,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,12,auucbt,self.MachineLearning,Is there a list of the largest (in terms of parameters) DNNs that have won competitions to date?,https://www.reddit.com/r/MachineLearning/comments/auucbt/is_there_a_list_of_the_largest_in_terms_of/,TheCockatoo,1551151241,,0,1,False,self,,,,,
1681,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,12,auue0u,self.MachineLearning,How does Googles Colab download datasets?,https://www.reddit.com/r/MachineLearning/comments/auue0u/how_does_googles_colab_download_datasets/,iHarsh_Darji,1551151517,[removed],0,1,False,self,,,,,
1682,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,12,auuhsq,self.MachineLearning,Doing a Project for my Data Mining Class and was wondering if somebody could explain something to me,https://www.reddit.com/r/MachineLearning/comments/auuhsq/doing_a_project_for_my_data_mining_class_and_was/,javycane,1551152149,[removed],0,1,False,https://b.thumbs.redditmedia.com/nw7TE8flR2MthUphuVUHtiKgPKP76wKuxAoLJWCKDoo.jpg,,,,,
1683,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auvctn,self.MachineLearning,[D] Why math.exp(Xi) /sum(math.exp(x)) in softmax?,https://www.reddit.com/r/MachineLearning/comments/auvctn/d_why_mathexpxi_summathexpx_in_softmax/,atum47,1551157462,"Well, softmax just calculate the probability of a  given vector x, right? Why does it uses the math.exp() function? Wouldn't just the element divided by the sum of the elements give a similar result?

E.g.: 

x = [1,2,3]

1/6 = 0.17
2/6 = 0.33
3/6 = 0.50",23,7,False,self,,,,,
1684,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auve6y,self.MachineLearning,[Project]Paper on generating melody from lyric--How difficult to implement?,https://www.reddit.com/r/MachineLearning/comments/auve6y/projectpaper_on_generating_melody_from_lyrichow/,enverx,1551157690,"Hey all,

I've only just started reading about ML and have come across \[this paper about automatic composition of melody from (Chinese) lyrics\]([https://arxiv.org/abs/1809.04318](https://arxiv.org/abs/1809.04318)) . Earlier I saw \[this paper\]([https://arxiv.org/abs/1612.01058](https://arxiv.org/abs/1612.01058)) , which is apparently the basis for a new iOS app. The first approach, using RNN, seems to get much better results than the second (random forest), but, as you can imagine, it's way more sophisticated. So I'm wondering how difficult it is to implement something like that, given the description the authors have provided (and a boatload of MIDI tunes with lyrics). Thanks for any input.",0,1,False,self,,,,,
1685,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auvigl,stats.stackexchange.com,[D] Why second order SGD convergence methods are unpopular for deep learning?,https://www.reddit.com/r/MachineLearning/comments/auvigl/d_why_second_order_sgd_convergence_methods_are/,jarekduda,1551158430,,0,1,False,default,,,,,
1686,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auvj3q,self.MachineLearning,"[R] AdaBound: An optimizer that trains as fast as Adam and as good as SGD (ICLR 2019), with A PyTorch Implementation",https://www.reddit.com/r/MachineLearning/comments/auvj3q/r_adabound_an_optimizer_that_trains_as_fast_as/,Luolc,1551158540,"Hi! I am an undergrad doing research in the field of ML/DL/NLP. This is my first time to write a post on Reddit. :D

&amp;#x200B;

We developed a new optimizer called **AdaBound**, hoping to achieve a faster training speed as well as better performance on unseen data. Our paper, *Adaptive Gradient Methods with Dynamic Bound of Learning Rate*, has been accepted by ICLR 2019 and we just updated the camera ready version on open review.

&amp;#x200B;

I am very excited that a PyTorch implementation of AdaBound is publicly available now, and a PyPI package has been released as well. You may install and try AdaBound easily via `pip` or directly copying &amp; pasting. I also wrote a post to introduce this lovely new optimizer.

&amp;#x200B;

Here's some quick links:

**Website:** [https://www.luolc.com/publications/adabound/](https://www.luolc.com/publications/adabound/)

**GitHub:** [https://github.com/Luolc/AdaBound](https://github.com/Luolc/AdaBound)

**Open Review:** [https://openreview.net/forum?id=Bkg3g2R9FX](https://openreview.net/forum?id=Bkg3g2R9FX)

**Abstract:**

Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at [https://github.com/Luolc/AdaBound](https://github.com/Luolc/AdaBound).

&amp;#x200B;

https://i.redd.it/9trhbha3lui21.png",67,390,False,https://b.thumbs.redditmedia.com/xgyzPlSOmclWLmegalRaM5Ni4F-Ms5pqDi5zXDfPt-U.jpg,,,,,
1687,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auvjmx,self.MachineLearning,Where can I find better documentation or examples for ML.NET?,https://www.reddit.com/r/MachineLearning/comments/auvjmx/where_can_i_find_better_documentation_or_examples/,arusse02,1551158629,[removed],0,1,False,self,,,,,
1688,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auvn3g,self.MachineLearning,[D] Why second order SGD convergence methods are unpopular for deep learning?,https://www.reddit.com/r/MachineLearning/comments/auvn3g/d_why_second_order_sgd_convergence_methods_are/,jarekduda,1551159207,"It seems that, especially for deep learning, there are dominating very simple methods for optimizing SGD convergence like ADAM - nice overview: http://ruder.io/optimizing-gradient-descent/

They **trace only single direction** - discarding information about the remaining ones, they **do not try to estimate distance from near extremum** - which is suggested by gradient evolution (-&gt;0 in extremum), and could help with the crucial choice of step size.

Both these missed opportunities could be exploited by second order methods - trying to locally model parabola in simultaneously multiple directions (not all, just a few), e.g. near saddle attracting in some directions, repulsing in the others. Here are some:

 - L-BFGS: http://aria42.com/blog/2014/12/understanding-lbfgs
 - TONGA: https://papers.nips.cc/paper/3234-topmoumoute-online-natural-gradient-algorithm
 - K-FAC: https://arxiv.org/pdf/1503.05671.pdf
 
But still first order methods dominate (?), I have heard opinions that second order just don't work for deep learning (?)

There are mainly 3 challenges (any more?): **inverting Hessian**, **stochasticity** of gradients, and handling **saddles**. All of them should be resolved if locally modelling parametrization as parabolas in a few promising directions ([I would like to use](https://arxiv.org/pdf/1901.11457)): update this parametrization based on calculated gradients, and perform proper step based on this parametrization. This way extrema can be in updated parameters - no Hessian inversion, slow evolution of parametrization allows to accumulate statistical trends from gradients, we can model both curvatures near saddles: correspondingly attract or repulse, with strength depending on modeled distance.

**Should we go toward second order methods for deep learning?**

Why is it so difficult to make them more successful than simple first order methods - could we **identify these challenges** ... resolve them?

As there are many ways to realize second order methods, which seems the most promising?

ps. Cross validated with criticism of K-FAC: https://stats.stackexchange.com/questions/394083/why-second-order-sgd-convergence-methods-are-unpopular-for-deep-learning/",48,34,False,self,,,,,
1689,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auvo7b,i.redd.it,Hide selfie stick on pfoto! stickoff.appspot.com - CNN + inpainting algorithm.,https://www.reddit.com/r/MachineLearning/comments/auvo7b/hide_selfie_stick_on_pfoto_stickoffappspotcom_cnn/,binrey,1551159400,,1,1,False,default,,,,,
1690,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,14,auvuj2,self.MachineLearning,[D] Differences between WAE and VAE?,https://www.reddit.com/r/MachineLearning/comments/auvuj2/d_differences_between_wae_and_vae/,zhang5628,1551160575,"Is WAE just a generalized form of VAE? 

&amp;#x200B;

In reading the [WAE](https://arxiv.org/pdf/1711.01558.pdf) paper, the only difference between VAE and WAE seems to me to be that 1. WAE can be deterministic, (see page 4), and 2. WAE can use other divergences besides KL divergence.

&amp;#x200B;

But in that case, WAE shouldn't be called ""Wasserstein"" because it's not really invoking Optimal Transport. (WAE with KL-divergence would just be VAE.) And so, what am I missing?",2,9,False,self,,,,,
1691,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,15,auw0ia,self.MachineLearning,2019 Facebook AI Residency thread,https://www.reddit.com/r/MachineLearning/comments/auw0ia/2019_facebook_ai_residency_thread/,jshin49,1551161636,[removed],0,1,False,self,,,,,
1692,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,15,auw7yo,self.MachineLearning,Ammonium Nitrate Market Insights and Global Industry Forecast to 2022,https://www.reddit.com/r/MachineLearning/comments/auw7yo/ammonium_nitrate_market_insights_and_global/,Amar_bir1,1551163083,[removed],1,1,False,self,,,,,
1693,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,16,auwfc4,self.MachineLearning,[P] PyCM 1.9 : Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/auwfc4/p_pycm_19_machine_learning_library_for_confusion/,sepandhaghighi,1551164548," [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

[http://www.pycm.ir](http://www.pycm.ir/)

* In this version we added a new parameter recommender system for different types of classification.

Changelog :

* Parameters recommendation system added [#112](https://github.com/sepandhaghighi/pycm/issues/112)
* Automatic/Manual (AM) added [#144](https://github.com/sepandhaghighi/pycm/issues/144)
* Bray-Curtis dissimilarity (BCD) added [#143](https://github.com/sepandhaghighi/pycm/issues/143)
* CODE\_OF\_CONDUCT.md added [#151](https://github.com/sepandhaghighi/pycm/issues/151)
* ISSUE\_TEMPLATE.md added [#156](https://github.com/sepandhaghighi/pycm/issues/156)
* PULL\_REQUEST\_TEMPLATE.md added [#156](https://github.com/sepandhaghighi/pycm/issues/156)
* CONTRIBUTING.md added [#155](https://github.com/sepandhaghighi/pycm/issues/155)
* X11 color names support for save\_html method added [#146](https://github.com/sepandhaghighi/pycm/issues/146)
* Warning message for high dimension matrix print added [#120](https://github.com/sepandhaghighi/pycm/issues/120)
* Interactive notebooks section (binder) added [#142](https://github.com/sepandhaghighi/pycm/issues/142)
* save\_matrix and normalize arguments added to save\_csv method [#120](https://github.com/sepandhaghighi/pycm/issues/120)
* README.md modified
* Document modified [#149](https://github.com/sepandhaghighi/pycm/issues/149) [#164](https://github.com/sepandhaghighi/pycm/issues/164)
* ConfusionMatrix.\_\_init\_\_ optimized [#145](https://github.com/sepandhaghighi/pycm/issues/145)
* Document and examples output files moved to different folders [#149](https://github.com/sepandhaghighi/pycm/issues/149)
* Test system modified [#138](https://github.com/sepandhaghighi/pycm/issues/138)
* relabel method bug fixed [#167](https://github.com/sepandhaghighi/pycm/issues/167)",0,2,False,self,,,,,
1694,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,16,auwkhq,self.MachineLearning,[D] What's the current state-of-the-art in voice imitation for speech synthesis?,https://www.reddit.com/r/MachineLearning/comments/auwkhq/d_whats_the_current_stateoftheart_in_voice/,nagasgura,1551165594,"I recently discovered [this channel](https://www.youtube.com/watch?v=RielTrKWEnY) that's recently become popular, and they claim to generate all the speech with ML. I wasn't able to find any information on the algorithm they're using though, and I can't find any paper that demonstrates such high-quality results.

Anyone have any guesses on what paper they're basing their algo on?",2,3,False,self,,,,,
1695,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,16,auwpd3,dexlabanalytics.com,How Machine Learning Technology is Enhancing Credit Risk Modeling,https://www.reddit.com/r/MachineLearning/comments/auwpd3/how_machine_learning_technology_is_enhancing/,dexlabanalytics,1551166592,,0,1,False,default,,,,,
1696,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,17,auww9y,self.MachineLearning,Question of auto generate music game beatmap,https://www.reddit.com/r/MachineLearning/comments/auww9y/question_of_auto_generate_music_game_beatmap/,Porco24,1551168097,[removed],0,1,False,self,,,,,
1697,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,17,auwywy,self.MachineLearning,Am thinking of the problem statement as Seq Labelling and want to know the working of CRF on the same. Please share your view,https://www.reddit.com/r/MachineLearning/comments/auwywy/am_thinking_of_the_problem_statement_as_seq/,delusion29,1551168667,[removed],0,1,False,self,,,,,
1698,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,17,aux5p2,self.MachineLearning,"Breathing Machines Market Size, Share | Industry Outlook, and Forecast 2018  2026",https://www.reddit.com/r/MachineLearning/comments/aux5p2/breathing_machines_market_size_share_industry/,Saairaj,1551170210,"Breathing machine helps the patient to breathe while they are suffering from chronic respiratory diseases or unable to breathe by themselves after anesthesia. Breathing machines are used during surgeries that require general anesthesia, as the medicines used to induce anesthesia can disrupt normal breathing. The machine is required after the surgery, as the patient may not be able to breathe immediately after the procedure. The type of breathing machine used varies according to the problem.

**Download PDF Brochure of Research Report:** [**https://www.coherentmarketinsights.com/insight/request-pdf/2032**](https://www.coherentmarketinsights.com/insight/request-pdf/2032)

Ventilators are mainly used in emergency medicine, home care, intensive care medicine, and in anesthesia, as a component of an anesthesia machine. The oxygen concentrator delivers pure oxygen via a mask or nasal cannula. It may be used in hyperbaric chambers and oxygen therapy. A C-PAP (continuous positive airway pressure) breathing machine blows air into the nose through a mask, which in turn keeps the airway open and helps in the absorption of oxygen.

Nebulizers are generally used for the treatment of a range of respiratory disorders such as chronic obstructive pulmonary disease (COPD) and asthma. Medications are inhaled straight into the lungs for effective treatment. These machines aid people who suffer from asthma by delivering liquid medication through a mist, which is inhaled into the lungs via machine. These devices are majorly used in home care settings.

Increasing number of respiratory disorders such as asthma and COPD, and rising number of surgeries are expected to drive growth of the global breathing machines market. According to the National Institute of Health data of 2014, asthma affects over 24 million people in the U.S., including over 6 million children. Breathing problems can also occur due to serious conditions such as pneumonia, tuberculosis, lung cancer, and other lung diseases. According to the World Health Organization (WHO) estimates, by 2030, the four major potentially fatal respiratory diseases including COPD, pneumonia, lung cancer, and tuberculosis will account for around one in five deaths worldwide.

**Get Exclusive Sample Copy @** [**https://www.coherentmarketinsights.com/insight/request-sample/2032**](https://www.coherentmarketinsights.com/insight/request-sample/2032)

However, few drawbacks of breathing machines include throat discomfort, damage to vocal cords, and sleeping discomfort. Some complications of using a breathing machine may include lung damage, collapsed lung, pneumonia or infection. Moreover, constant medical care is required when a breathing machine is used. These factors are expected to hinder growth of the market.

Breathing Machines Market: Taxonomy - On the basis of product type, the global breathing machines market is segmented into: PAP Devices, Nebulizers, Ventilators, Oxygen Concentrators, On the basis of end users, the global breathing machines market is segmented into: Hospitals and clinics, Home care settings,. On the basis of region, the global breathing machines market is segmented into: North America, Latin America, Europe, Asia Pacific, Middle East, Africa,.

**Browse Full Report:** [**https://www.coherentmarketinsights.com/ongoing-insight/breathing-machines-market-2032**](https://www.coherentmarketinsights.com/ongoing-insight/breathing-machines-market-2032)

Key players operating in the global breathing machines market include BD, Drager Medical, Fisher &amp; Paykel, GE Healthcare, Invacare, Maque, Medtronic, OMRON, Philips Healthcare, Teijin Pharma Resmed, and Weinmann. Key players are focusing on improving breathing machines by developing energy-efficient power systems and product features such as lightweight maneuverability, long-lasting components, and patient-friendly design.

**About Coherent Market Insights:**

Coherent Market Insights is a prominent market research and consulting firm offering action-ready syndicated research reports, custom market analysis, consulting services, and competitive analysis through various recommendations related to emerging market trends, technologies, and potential absolute dollar opportunity.

**Contact Us:**

Mr. Shah  
Coherent Market Insights  
1001 4th Ave,   
\#3200   
Seattle, WA 98154  
Tel: +1-206-701-6702  
**Email: sales@coherentmarketinsights.com**",0,1,False,self,,,,,
1699,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,17,auxa9r,self.MachineLearning,"Anyone interested in joining a taskforce tackling ""easy"" applications of ML for everyday use?",https://www.reddit.com/r/MachineLearning/comments/auxa9r/anyone_interested_in_joining_a_taskforce_tackling/,Electricvid,1551171239,[removed],0,1,False,self,,,,,
1700,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,18,auxoke,self.MachineLearning,[Discussion] 16bit training with Apex,https://www.reddit.com/r/MachineLearning/comments/auxoke/discussion_16bit_training_with_apex/,Zlush,1551174577,"Hey all 

&amp;#x200B;

I recently started FP16 training using the [Apex](https://github.com/NVIDIA/apex) library from Nvidia and i want to know if someone can share some experience. For my projects, i am using the FP16\_Optimizer with Dynamic Loss scale but get a lot of Overflow. My model converges but a lot of steps are skipped and i want to know what i can do to reduce the amount of Overflow in each step. Right now, i convert loss variables to float32 before using backward. 

&amp;#x200B;

Thank you :)

&amp;#x200B;",3,7,False,self,,,,,
1701,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,18,auxpiy,youtu.be,Detecting objects in images using AWS Rekognition API and Python boto3 library,https://www.reddit.com/r/MachineLearning/comments/auxpiy/detecting_objects_in_images_using_aws_rekognition/,pylenin,1551174800,,1,1,False,https://b.thumbs.redditmedia.com/h56ykPsbIQxXtLaIOiqxQIltR1SCIKvSULqj-NMQd-Q.jpg,,,,,
1702,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,19,auy4bs,self.MachineLearning,Where is the correct level of skill/tooling to make deep learning a DEV problem?,https://www.reddit.com/r/MachineLearning/comments/auy4bs/where_is_the_correct_level_of_skilltooling_to/,gus_maskowitz_p,1551178193,[removed],0,1,False,self,,,,,
1703,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,20,auycd8,self.MachineLearning,Any ACTUAL data science leaders care to share what they value in data scientists on their team?,https://www.reddit.com/r/MachineLearning/comments/auycd8/any_actual_data_science_leaders_care_to_share/,Mayalittlepony,1551180007,[removed],0,1,False,self,,,,,
1704,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,20,auyg2n,self.MachineLearning,Problems in training CIFAR-10 dataset with SGD,https://www.reddit.com/r/MachineLearning/comments/auyg2n/problems_in_training_cifar10_dataset_with_sgd/,syoya,1551180838,"I'm now training CIFAR-10 dataset using ResNet29v2 with SGD optimizer. And when I'm doing learning rate grid search, I found that there are abnormal fluctuations in training loss when LR = 0.1. First I guess it's because of a large learning rate but when learning rate equals 1.0, the training loss seems to be more stable. Actually, I couldn't find a proper explanation and I would like to remove this kind of fluctuations. I have tried learning rate decay but it seems a proper decay rate should be carefully tuned. Anybody could help me?

![img](m6yt1sfbewi21)",0,1,False,self,,,,,
1705,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,21,auynr7,self.MachineLearning,[D] Mathematical objective underlying graph-centrality measures?,https://www.reddit.com/r/MachineLearning/comments/auynr7/d_mathematical_objective_underlying/,sairaamv92,1551182453,"Hi. First, I concede that graph centrality measures are not, strictly speaking, machine learning concepts. This much said, due to their use in machine learning methods I feel that this post could be put here,
My question is this: What sort of objective functions do centrality measures capture?
I am aware that graph-centrality measures aim to capture some notion of consensus among graph vertices. However, I would like to know if there is a formal theory for studying such consensus functions.",0,2,False,self,,,,,
1706,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,21,auynur,self.MachineLearning,Where to start?,https://www.reddit.com/r/MachineLearning/comments/auynur/where_to_start/,ShreyasTanay7,1551182468,"I mean, I think I'm probably in the right place, but if not, please guide me to the correct subreddit. 
Where should I start to learn Machine Learning?
And another thing is that, say I trained a model and now I want to use that in an android app. How do I do that?",0,1,False,self,,,,,
1707,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,21,auywnk,twitter.com,Best DEAL The Complete SQL Bootcamp DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/auywnk/best_deal_the_complete_sql_bootcamp_discount_94/,SherriAleman,1551184083,,0,1,False,default,,,,,
1708,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,21,auz6co,old.reddit.com,Visualization - what does Bayesian probability look like?,https://www.reddit.com/r/MachineLearning/comments/auz6co/visualization_what_does_bayesian_probability_look/,nivter,1551185909,,0,1,False,default,,,,,
1709,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auz7in,self.MachineLearning,keras class weight is not working,https://www.reddit.com/r/MachineLearning/comments/auz7in/keras_class_weight_is_not_working/,prashant905,1551186109,[removed],0,1,False,self,,,,,
1710,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auz7y6,self.MachineLearning,[R] Feature Selection for an Unsupervised Outlier Detection Problem,https://www.reddit.com/r/MachineLearning/comments/auz7y6/r_feature_selection_for_an_unsupervised_outlier/,ricklen,1551186168,"Hi everyone,

I'm currently focusing on an equivalent problem as in the following article: [https://arxiv.org/pdf/1709.05254.pdf](https://arxiv.org/pdf/1709.05254.pdf) but I'm unsure about how to select proper features / attributes. The paper describes the following about feature selection:

*""We extracted a subset of 6 (dataset A) and 10 (dataset B) most discriminative attributes of the BKPF and BSEG tables ""*

*My question is how to select the proper features / attributes in a data set with both categorical and numerical features in order to perform unsupervised outlier detection? Can I, for example, simply use the entropy of each attribute?* 

Thank you in forward.",2,1,False,self,,,,,
1711,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auz9p4,twitter.com,Best DEAL Data Science and Machine Learning Bootcamp with R DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/auz9p4/best_deal_data_science_and_machine_learning/,MelodieRaggs,1551186486,,0,1,False,default,,,,,
1712,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzbcy,self.MachineLearning,"[R] SelfGAN--Not A GAN But Punch Itself, with A PyTorch GPU and Tensorflow Keras TPU Implementation",https://www.reddit.com/r/MachineLearning/comments/auzbcy/r_selfgannot_a_gan_but_punch_itself_with_a/,wuhecong,1551186779," Hi! I am an undergrad of network engineering but interesting in the field of DL. This is my first time to write a post on Reddit. 

I slammed this project in my spare time. This is just my accidental discovery. I can't explain more about the principle because of my lack of knowledge. I only know that this SelfGAN is a bit special. This model performs weight updates on both the generator and the discriminator, and uses dynamic loss weights.

I am very grateful to Colab for giving me some free resources for this poor student, so training on GPU and TPU is possible.

Because I will be busy with postgraduate study, and there is no time and money, I can't conduct my research in detail. So I hurriedly published my research. I posted it on viXra because my article is not official or perfect.

If you think this is a good discovery, I hope that you can give me a star or share it with your friends.

&amp;#x200B;

 Here's some quick links: 

[Paper](https://vixra.org/abs/1902.0445)

[Github Code](https://github.com/HighCWu/SelfGAN)

[Model Weights and Training Results](https://drive.google.com/open?id=1rLUk76rUm7pO5Zm4EtxllKR3stv0wfxh)

**Abstract:** 

 In my research, I modified the basic structure of GAN, let G and D train together, and use dynamic loss weights to achieve a relatively balanced training. 

&amp;#x200B;

https://i.redd.it/hy00xj2iwwi21.png

I am very sorry that my poor English may be bothering you, but I hope you can support me.",0,8,False,self,,,,,
1713,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzbht,self.math,Cross-posting from /r/math since I think it might be relevant to this subreddit as well,https://www.reddit.com/r/MachineLearning/comments/auzbht/crossposting_from_rmath_since_i_think_it_might_be/,nivter,1551186803,,0,1,False,default,,,,,
1714,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzcab,self.MachineLearning,New learner,https://www.reddit.com/r/MachineLearning/comments/auzcab/new_learner/,Pikachupmk,1551186943,[removed],0,1,False,self,,,,,
1715,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzeyo,self.MachineLearning,Monte Carlo actor critic algorithm,https://www.reddit.com/r/MachineLearning/comments/auzeyo/monte_carlo_actor_critic_algorithm/,gopal_chitalia,1551187426,[removed],0,1,False,https://a.thumbs.redditmedia.com/ZgaVZImWnCaAEn01hVc9rj4KgxWzon4osVImE6BG0Q0.jpg,,,,,
1716,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzfof,lambdatest.com,Learn how algorithmic robots work with a simple example of teacher bots and student bots.,https://www.reddit.com/r/MachineLearning/comments/auzfof/learn_how_algorithmic_robots_work_with_a_simple/,heliumsingh,1551187551,,0,1,False,default,,,,,
1717,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzj7y,youtube.com,"Facial Recognition Explained w/ ""Where's Waldo""",https://www.reddit.com/r/MachineLearning/comments/auzj7y/facial_recognition_explained_w_wheres_waldo/,SharathCK,1551188204,,0,1,False,https://a.thumbs.redditmedia.com/6VG-vJ7dZKJtYSyjnS69d8X35dZ8Oshzxl3hpQQjMm4.jpg,,,,,
1718,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzjb3,self.MachineLearning,what is better to use to extract data from ID card images ? ( Arabic letters and numbers ),https://www.reddit.com/r/MachineLearning/comments/auzjb3/what_is_better_to_use_to_extract_data_from_id/,mohmyo,1551188222,[removed],0,1,False,self,,,,,
1719,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzlu5,thinkingondata.com,Beginner project: Simple linear regression step by step,https://www.reddit.com/r/MachineLearning/comments/auzlu5/beginner_project_simple_linear_regression_step_by/,f1789,1551188683,,2,1,False,default,,,,,
1720,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,22,auzlwh,self.MachineLearning,"[R] BLOCK fusion for VQA and VRD (AAAI 2019), including a multimodal fusion library",https://www.reddit.com/r/MachineLearning/comments/auzlwh/r_block_fusion_for_vqa_and_vrd_aaai_2019/,Tamazy,1551188696,"Hi there!

We recently open-sourced the code to reproduce the results of our paper as well as a [multimodal fusion library](https://github.com/Cadene/block.bootstrap.pytorch#fusions) available with pip install. It includes most of the fusion modules from the state-of-the-art such as [MCB](https://arxiv.org/abs/1708.03619), [MLB](https://arxiv.org/abs/1610.04325), [MFB](https://arxiv.org/abs/1708.01471), [MFH](https://arxiv.org/abs/1708.03619), [MUTAN](https://arxiv.org/abs/1705.06676), [BLOCK](https://arxiv.org/abs/1902.00038.pdf).

This pytorch library can be useful to fuse two vectors in the same space. For instance, in Visual Question Answering, one must fuse the image and the question embeddings in the same bi-modal space; or, in Visual Relationship Detection, one must fuse the bounding boxes coordinates, class embeddings and visual features of the two objects:
- [VQA example](https://github.com/Cadene/block.bootstrap.pytorch/raw/master/assets/VQA_block.png?raw=true)
- [VRD example](https://github.com/Cadene/block.bootstrap.pytorch/raw/master/assets/VRD_block.png?raw=true)

A further example of using this fusion library can be found in the [code](https://github.com/Cadene/murel.bootstrap.pytorch) of our last paper accepted to CVPR 2019 about [Multimodal Relational Reasoning for VQA](https://arxiv.org/abs/1902.09487).

Thanks for your interest!
I hope that this code base will be useful for someone else :)

Best
 ",0,21,False,self,,,,,
1721,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,23,auzxrl,blog.sicara.com,[D] What do you think about my workflow from data analysis to production,https://www.reddit.com/r/MachineLearning/comments/auzxrl/d_what_do_you_think_about_my_workflow_from_data/,clementwalter,1551190693,,1,1,False,https://b.thumbs.redditmedia.com/-shYZdvSmCchvXp8ScUSW1uV_vq4msFKu0m1W11oY4M.jpg,,,,,
1722,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,23,av01fz,self.MachineLearning,Deploying trined model,https://www.reddit.com/r/MachineLearning/comments/av01fz/deploying_trined_model/,pchelina,1551191323,[removed],0,1,False,self,,,,,
1723,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,23,av06w7,software.intel.com,Intel Data Analytics Acceleration Library (Intel DAAL),https://www.reddit.com/r/MachineLearning/comments/av06w7/intel_data_analytics_acceleration_library_intel/,QuirkySpiceBush,1551192257,,0,1,False,default,,,,,
1724,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,23,av078t,self.MachineLearning,[Discussion] Converting an unsupervised NLP problem into supervised problem. (Text Clustering into Classification),https://www.reddit.com/r/MachineLearning/comments/av078t/discussion_converting_an_unsupervised_nlp_problem/,edutainment123,1551192317,"**Data**: Short and long phrases

**Problem which needs solving:** Assorting these phrases into meaningful categories.

**The Overall Idea:** 

1. Cluster the phrases 
2. Manually find out crux/subject/topic of each cluster on which it is actually clustered 
3. Use these subjects as labelled data for training a classifier 
4. Use this classifier for future inputs to classify them in the right category/subject.

**Approach used**: Trained a Word2Vec model on all the phrases which acts like a synonym dictionary. This word dictionary is then used to create a ""phrase vector"" in the following way -

1. Remove english stopwords from a phrase
2. Remove task specific stopwords based on frequency 
3. Taking mean/sum of the word vectors of the remaining important words in the phrase.

This has worked surprisingly well for the task at hand. 

&amp;#x200B;

Now there's couple of places where I think this approach could be improved and where I need your help - 

1. Is there a better way to make use of the Word2Vec dictionary created on the phrases. The dictionary has captured the contexts of the words very well and it seems averaging them diminishes the significance of this context in a way and not averaging word vectors seems to be a general consensus. (But it's also working, so I'm not complaining)
2. During the classifier building stage, I'll be using these phrase vectors as features mapped to manually annotated categories. But I can see that these features will be only as good as the clustering it provides. What more features/different features would you suggest to build this classifier?
3. I am certain people have come across this kind of problem. Do you have any other approach to the whole problem which will benefit in this case?",11,14,False,self,,,,,
1725,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,23,av095f,self.MachineLearning,Should I bother with neural networks?,https://www.reddit.com/r/MachineLearning/comments/av095f/should_i_bother_with_neural_networks/,Ordzhonikidze,1551192648,[removed],0,1,False,self,,,,,
1726,MachineLearning,t5_2r3gv,2019-2-26,2019,2,26,23,av0buq,self.MachineLearning,[D] Style transfer to increase robustness,https://www.reddit.com/r/MachineLearning/comments/av0buq/d_style_transfer_to_increase_robustness/,Maplernothaxor,1551193108,"Im not sure if this idea makes much sense but has there been any work in the application of style transfer to increase robustness?

For example, if I have a autonomous vehicle trained on sunny day data, would style transfer of sunny days on to rainy days/night time be a valid approach to increasing the robustness of my model without retraining?",11,5,False,self,,,,,
1727,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,0,av0uhv,self.MachineLearning,[D] Logistic regression with panel data - churn model,https://www.reddit.com/r/MachineLearning/comments/av0uhv/d_logistic_regression_with_panel_data_churn_model/,cesusjhrist,1551196003,"Hi all, I need your help.  I have a dataset, made like this:

&amp;#x200B;

|time|id|features|target|
|:-|:-|:-|:-|
|1|1|...|1|
|2|1|...|1|
|3|1|...|1|
|...|1|...|1|
|10|1|...|1|
|11|1|...|1|
|1|2|...|0|
|2|2|...|0|
|3|2|...|0|
|...|2|...|0|
|10|2|...|0|
|11|2|...|0|
|...|...|...|...|

&amp;#x200B;

The target is build that if a person has worked on weeks 12 to 14 then we set target=0, else is 1. For training and test purposes I can only use weeks 1 to 11. How would I train the dataset and measure it's accuracy? I am finding trouble with this task because the target variable is referred to an individual (id) for which we have multiple rows based on time, so if I do a cross validation or split the data based on time, I could end up getting the same id twice (or more) in the test dataset.

&amp;#x200B;

Thanks!

&amp;#x200B;",2,1,False,self,,,,,
1728,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,0,av0ut9,self.MachineLearning,Looking for literature on word embeddings,https://www.reddit.com/r/MachineLearning/comments/av0ut9/looking_for_literature_on_word_embeddings/,arun279,1551196051,[removed],0,1,False,self,,,,,
1729,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av13vf,self.MachineLearning,[D] Using Autoencoders for Anomaly/Fraud Detection?,https://www.reddit.com/r/MachineLearning/comments/av13vf/d_using_autoencoders_for_anomalyfraud_detection/,Fender6969,1551197405,"I am very new to Autoencoders and anomaly detection, and am currently working on a credit card fraud dataset to understand how Autoencoders can serve in fraud detection. Please correct me if I am mistaken in my logic.

&amp;#x200B;

Objective: Binary classification of whether transaction is fraud (1) or non fraud (0). 

&amp;#x200B;

After loading the dataset in, I have done some basic feature engineering (normalization \[-1,1\] etc). Next, I separate the non fraud class (0) from the fraud class (1) (values as well as target variable).

&amp;#x200B;

After doing this, I feed only the non fraud data/target as my training data into the autoencoder using Mean Squared Error as my metric of evaluation. From my understanding, this is what will serve as the Reconstruction Error that is being used to compare the classes.

&amp;#x200B;

Finally, I predict using the autoencoder on the fraud class (1) data and calculate the Reconstruction Error on that data.

&amp;#x200B;

At this point, I have the Reconstruction Error from both classes. From my understanding, we use this Reconstructor Error to differentiate between data that is of the class Fraud and Non Fraud.

&amp;#x200B;

Suppose that I want to feed in a user input into my Autoencoder model that I have built to classify whether that data is fraud or not. How would I go about doing this from the point that I am at now? Would I just call ""[autoencoder.fit](https://autoencoder.fit)(userdata)""? Would that essentially give me a Reconstructor Error value to compare?

&amp;#x200B;

Any help would be great as I am very new to this concept!",14,1,False,self,,,,,
1730,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av16yx,codeingschool.com,5 Top Technology Trends for 2019 and the Jobs Theyll Create,https://www.reddit.com/r/MachineLearning/comments/av16yx/5_top_technology_trends_for_2019_and_the_jobs/,subhamroy021,1551197833,,0,1,False,default,,,,,
1731,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av179i,i.redd.it,"I stumbled across this image from the CIFAR-10 train set; I never knew there was a secret ""scantily clad"" class.",https://www.reddit.com/r/MachineLearning/comments/av179i/i_stumbled_across_this_image_from_the_cifar10/,learning-luke,1551197874,,0,1,False,default,,,,,
1732,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av18vw,themlearning.com,"Image-Based Airbnb Pricing Algorithm, Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.",https://www.reddit.com/r/MachineLearning/comments/av18vw/imagebased_airbnb_pricing_algorithm_hey_guys_i/,themlearning,1551198100,,0,1,False,default,,,,,
1733,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1aby,deepmind.com, Machine learning can boost the value of wind energy,https://www.reddit.com/r/MachineLearning/comments/av1aby/machine_learning_can_boost_the_value_of_wind/,sjoerdapp,1551198312,,0,1,False,default,,,,,
1734,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1aw9,self.MachineLearning,Best framework for model qunatisation,https://www.reddit.com/r/MachineLearning/comments/av1aw9/best_framework_for_model_qunatisation/,FCOS96,1551198391,[removed],0,1,False,self,,,,,
1735,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1awx,self.MachineLearning,[D] What kind of a machine learning model would yield good results?,https://www.reddit.com/r/MachineLearning/comments/av1awx/d_what_kind_of_a_machine_learning_model_would/,dn_red_usr,1551198394," 

Hi, 

I have this question and need some help on this. 

Suppose the Target value is continuous with about 1000 entries out of which 750 are 0s and rest all are values between 1 to 50000. There are 7 continuous features and you have to build a predictive model for it.

What sort of a machine learning model do we choose?

Any updates would be great. TIA.",8,0,False,self,,,,,
1736,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1fg7,github.com,[P] Python library to measure agreement using Krippendorff,https://www.reddit.com/r/MachineLearning/comments/av1fg7/p_python_library_to_measure_agreement_using/,bryant1410,1551199047,,1,1,False,default,,,,,
1737,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1fye,self.MachineLearning,PyCM 1.9 : Classification analysis + parameter recommender,https://www.reddit.com/r/MachineLearning/comments/av1fye/pycm_19_classification_analysis_parameter/,sepandhaghighi,1551199112,[removed],0,1,False,self,,,,,
1738,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1isd,self.MachineLearning,[P] Not clear on sliding window for an RNN for animal song recognition,https://www.reddit.com/r/MachineLearning/comments/av1isd/p_not_clear_on_sliding_window_for_an_rnn_for/,Ayakalam,1551199495,"&amp;#x200B;

Hi, I am working on a project for ""speech"" recognition on birds, etc. I have a question about the use of RNNs (in general) towards this. 

&amp;#x200B;

What I am not clear about is how exactly the input dimensionality works for an RNN, when I have streaming data. For example, let's say that I have input samples from 0 to N-1, and this gets fed into an RNN to classify it as something. What I am not clear about, is for the NEXT block, do I take samples 1 to N, and then 2 to N+1, etc etc? Seems like a waste?

&amp;#x200B;

Thank you",0,1,False,self,,,,,
1739,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1ktu,self.MachineLearning,[P] Confusion matrix analysis + parameter recommender,https://www.reddit.com/r/MachineLearning/comments/av1ktu/p_confusion_matrix_analysis_parameter_recommender/,sepandhaghighi,1551199818,"PyCM version 1.9 released

[https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

[http://www.pycm.ir](http://www.pycm.ir/)

* In this version we added a new parameter recommender system for different types of classification.

    &gt;&gt;&gt; from pycm import *
    &gt;&gt;&gt; y_actu = [2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2] 
    &gt;&gt;&gt; y_pred = [0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]
    &gt;&gt;&gt; cm.imbalance
    False
    &gt;&gt;&gt; cm.binary
    False
    &gt;&gt;&gt; cm.recommended_list
    ['MCC', 'TPR Micro', 'ACC', 'PPV Macro', 'BCD', 'Overall MCC', 'Hamming Loss', 'TPR Macro', 'Zero- 
    one Loss', 'ERR', 'PPV Micro', 'Overall ACC']

Changelog :

* Parameters recommendation system added [\#112](https://github.com/sepandhaghighi/pycm/issues/112)
* Automatic/Manual (AM) added [\#144](https://github.com/sepandhaghighi/pycm/issues/144)
* Bray-Curtis dissimilarity (BCD) added [\#143](https://github.com/sepandhaghighi/pycm/issues/143)
* CODE\_OF\_CONDUCT.md added [\#151](https://github.com/sepandhaghighi/pycm/issues/151)
* ISSUE\_TEMPLATE.md added [\#156](https://github.com/sepandhaghighi/pycm/issues/156)
* PULL\_REQUEST\_TEMPLATE.md added [\#156](https://github.com/sepandhaghighi/pycm/issues/156)
* CONTRIBUTING.md added [\#155](https://github.com/sepandhaghighi/pycm/issues/155)
* X11 color names support for save\_html method added [\#146](https://github.com/sepandhaghighi/pycm/issues/146)
* Warning message for high dimension matrix print added [\#120](https://github.com/sepandhaghighi/pycm/issues/120)
* Interactive notebooks section (binder) added [\#142](https://github.com/sepandhaghighi/pycm/issues/142)
* save\_matrix and normalize arguments added to save\_csv method [\#120](https://github.com/sepandhaghighi/pycm/issues/120)
* README.md modified
* Document modified [\#149](https://github.com/sepandhaghighi/pycm/issues/149) [\#164](https://github.com/sepandhaghighi/pycm/issues/164)
* ConfusionMatrix.\_\_init\_\_ optimized [\#145](https://github.com/sepandhaghighi/pycm/issues/145)
* Document and examples output files moved to different folders [\#149](https://github.com/sepandhaghighi/pycm/issues/149)
* Test system modified [\#138](https://github.com/sepandhaghighi/pycm/issues/138)
* relabel method bug fixed [\#167](https://github.com/sepandhaghighi/pycm/issues/167)",0,2,False,self,,,,,
1740,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,1,av1ldl,self.MachineLearning,Seven Myths in Machine Learning Research,https://www.reddit.com/r/MachineLearning/comments/av1ldl/seven_myths_in_machine_learning_research/,wood_nich,1551199895,"Interesting article on myths in machine learning, that is more of a meta-review of several different papers.  Here is the tldr from the top of the article:

&gt;We present seven myths commonly believed to be true in machine learning research, circa Feb 2019. Also available on the [ArXiv](https://arxiv.org/pdf/1902.06789.pdf) in pdf form.  
[Myth 1:](https://crazyoscarchang.github.io/#myth-1) TensorFlow is a Tensor manipulation library  
[Myth 2:](https://crazyoscarchang.github.io/#myth-2) Image datasets are representative of real images found in the wild  
[Myth 3:](https://crazyoscarchang.github.io/#myth-3) Machine Learning researchers do not use the test set for validation  
[Myth 4:](https://crazyoscarchang.github.io/#myth-4) Every datapoint is used in training a neural network  
[Myth 5:](https://crazyoscarchang.github.io/#myth-5) We need (batch) normalization to train very deep residual networks  
[Myth 6:](https://crazyoscarchang.github.io/#myth-6) Attention &gt; Convolution  
[Myth 7:](https://crazyoscarchang.github.io/#myth-7) Saliency maps are robust ways to interpret neural networks",0,1,False,self,,,,,
1741,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,2,av1snw,blog.google,Machine learning can boost the value of wind energy,https://www.reddit.com/r/MachineLearning/comments/av1snw/machine_learning_can_boost_the_value_of_wind/,neverwild,1551200899,,0,1,False,default,,,,,
1742,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,2,av1tci,medium.com,[R] Understanding BERT Transformer: Attention isn't all you need,https://www.reddit.com/r/MachineLearning/comments/av1tci/r_understanding_bert_transformer_attention_isnt/,Jean-Porte,1551200994,,19,49,False,default,,,,,
1743,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,2,av1yc6,self.MachineLearning,[D] Working on a RL project. Value function being sacrificed for good policy.,https://www.reddit.com/r/MachineLearning/comments/av1yc6/d_working_on_a_rl_project_value_function_being/,sturdyplum,1551201691,"So I've been working on a RL project. Currently I seem to be having an issue where in order to minimize loss, the value function begins vastly underestimating the state allowing for the policy function to easily beat the expected reward. This pretty much leads the agent to learn no useful behaviors. Has anyone run into this issue before that could perhaps help me out a bit. ",2,1,False,self,,,,,
1744,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,2,av29ng,self.MachineLearning,create new pandas dataframe based on variable,https://www.reddit.com/r/MachineLearning/comments/av29ng/create_new_pandas_dataframe_based_on_variable/,manjrem,1551203286,"I have dataset containing (please check the image) 900 entries.

I am using pandas to store this streaming data. For all new data that would be streamed, I would like to create separate hdf files based on data in 'instrument_token' column in the dataset instead of creating a single hdf file as is the case today.

For example, for intsrument_token ""131150596"" a hdf file with name ""131150596.hdf"" should be created and data associated with ""131150596"" should be written to it. ""instrument_token"" column contains unique values.

I did try searching the net for some leads but was not able to get anywhere.

snapshot of dataset

instrument_token;exchange_token;tradingsymbol;name;last_price;expiry;strike;tick_size;lot_size 131150596;512307;TYPHOON;TYPHOON HOLDINGS;0;;0;0.01;100
136290308;532384;TYCHE;TYCHE INDUSTRIES;0;;0;0.05;1
136439300;532966;TWL;TITAGARH WAGONS;0;;0;0.05;1
",0,1,False,self,,,,,
1745,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,2,av2dt0,self.MachineLearning,On Lesson 1 - Image Recognition from fast.ai and need help to make an Emotion Detector.,https://www.reddit.com/r/MachineLearning/comments/av2dt0/on_lesson_1_image_recognition_from_fastai_and/,bidyutchanda108,1551203895,[removed],0,1,False,self,,,,,
1746,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,3,av2fnq,self.MachineLearning,"Facial Recognition Explanation - by playing ""Where's Waldo""",https://www.reddit.com/r/MachineLearning/comments/av2fnq/facial_recognition_explanation_by_playing_wheres/,SharathCK,1551204152,[removed],0,1,False,self,,,,,
1747,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,3,av2jxz,self.MachineLearning,What is the difference between L1 and L2 norms?,https://www.reddit.com/r/MachineLearning/comments/av2jxz/what_is_the_difference_between_l1_and_l2_norms/,CosmicPennyworth,1551204759,[removed],0,1,False,self,,,,,
1748,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,3,av2lp3,self.MachineLearning,[D] Where to send someone with mathematical understanding but no programming experience to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/av2lp3/d_where_to_send_someone_with_mathematical/,DJ-VU,1551205020,"Just a few years ago, I would have had no doubt as to send people through Statistical Learning by Trevor Hastie and Rob Tibshirani. It had videos, a book and took people through how to use R even if they had no experience.

These days though, I feel that it would probably be better to learn Python and that's from a person that uses R myself the vast majority of the time. I just feel that Python gives you more options especially going forward. If you disagree with that or have other opinions, I would love to hear them.

I am sorry if this is not on topic.",3,5,False,self,,,,,
1749,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,3,av304s,self.MachineLearning,"I have written a ML based paper with 2 of my batchmates. What should I do to publish it? I live in Mumbai, India and have no idea where to begin.",https://www.reddit.com/r/MachineLearning/comments/av304s/i_have_written_a_ml_based_paper_with_2_of_my/,shauniop,1551207120,,0,1,False,self,,,,,
1750,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,3,av33d3,medium.com,"After Mastering Go and StarCraft, DeepMind Takes on Soccer",https://www.reddit.com/r/MachineLearning/comments/av33d3/after_mastering_go_and_starcraft_deepmind_takes/,gwen0927,1551207581,,0,1,False,https://b.thumbs.redditmedia.com/A2pFz3bjsVbVcwr-tSjuj8CyUSKA1pB1e02QTefldig.jpg,,,,,
1751,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,4,av353e,cbmm.mit.edu,"[R] CBMM Special Seminar: Self-Learning Systems | The Center for Brains, Minds &amp; Machines",https://www.reddit.com/r/MachineLearning/comments/av353e/r_cbmm_special_seminar_selflearning_systems_the/,MICInc,1551207818,,1,4,False,default,,,,,
1752,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,4,av3emk,9to5google.com,Google optimizing wind farms w/ DeepMind to predict output 36 hours. Has boosted the value of our wind energy by roughly 20 percent.,https://www.reddit.com/r/MachineLearning/comments/av3emk/google_optimizing_wind_farms_w_deepmind_to/,infocsg,1551209181,,0,1,False,https://a.thumbs.redditmedia.com/YtFKXohHMps8MAaqcBr6XR73-lv3ZOnhFXFQAnkAMl4.jpg,,,,,
1753,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,4,av3h00,self.MachineLearning,[Question] When was the early activation functions first introduced?,https://www.reddit.com/r/MachineLearning/comments/av3h00/question_when_was_the_early_activation_functions/,autunno,1551209504,"I'm studying ANNs activation functions and it is pretty to track the origins of relatively newer functions such as ReLU, softmax, etc, but I can't pinpoint in what point in time and who created the following activation functions:

* Sigmoid
* Hyperbolic Tangent (Tanh)
* Identity (linear)
* Binary step

  
It is not hard to find out when they were introduced in calculus, but finding out when they were first applied as activation functions is another story. 

&amp;#x200B;

I've tried searching in many different ways, such as ""sigmoid activation function history/origin/first"", filter out by old articles only, etc., so far without any luck.

&amp;#x200B;

Does anyone know more about it, or how I could improve by article-searching-fu?",0,1,False,self,,,,,
1754,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,4,av3jyr,self.MachineLearning,[Question] When were the most basic activation functions introduced on neural networks?,https://www.reddit.com/r/MachineLearning/comments/av3jyr/question_when_were_the_most_basic_activation/,autunno,1551209925," 

I'm studying ANNs activation functions and it is pretty to track the origins of relatively newer functions such as ReLU, softmax, etc, but I can't pinpoint in what point in time and who created the following activation functions:

* Sigmoid
* Hyperbolic Tangent (Tanh)
* Identity (linear)
* Binary step

It is not hard to find out when they were introduced in calculus, but finding out when they were first applied as activation functions is another story.

I've tried searching in many different ways, such as ""sigmoid activation function history/origin/first"", filter out by old articles only, etc., so far without any luck.

Does anyone know more about it, or how I could improve by article-searching-fu?",0,1,False,self,,,,,
1755,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,4,av3k1r,self.MachineLearning,Handwritten CUDA vs high level framework speed,https://www.reddit.com/r/MachineLearning/comments/av3k1r/handwritten_cuda_vs_high_level_framework_speed/,the_roboticist,1551209936,[removed],0,1,False,self,,,,,
1756,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,4,av3scb,themlearning.com,"[P] Image-Based Airbnb Pricing Algorithm, Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.",https://www.reddit.com/r/MachineLearning/comments/av3scb/p_imagebased_airbnb_pricing_algorithm_hey_guys_i/,mosef18,1551211108,,0,1,False,default,,,,,
1757,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,5,av49k5,self.MachineLearning,"nats, bits and a whole lot of confusion",https://www.reddit.com/r/MachineLearning/comments/av49k5/nats_bits_and_a_whole_lot_of_confusion/,LeanderKu,1551213604,[removed],0,1,False,self,,,,,
1758,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,6,av4ikw,self.MachineLearning,"Won $20,000 of Google Cloud Credit. I do not believe I have a use for Google Cloud Credit. What should I do?",https://www.reddit.com/r/MachineLearning/comments/av4ikw/won_20000_of_google_cloud_credit_i_do_not_believe/,skeetforeskin,1551214894,[removed],0,1,False,self,,,,,
1759,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,6,av4jxv,blog.snark.ai,Speeding up Gradient Boosting Tuning on Snark,https://www.reddit.com/r/MachineLearning/comments/av4jxv/speeding_up_gradient_boosting_tuning_on_snark/,davidbun,1551215079,,0,1,False,default,,,,,
1760,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,6,av4nam,self.MachineLearning,Who owns the model,https://www.reddit.com/r/MachineLearning/comments/av4nam/who_owns_the_model/,deepchickenAI,1551215553,[removed],0,1,False,self,,,,,
1761,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,6,av4sw0,self.MachineLearning,Scale Any Machine Learning Pipeline to Elastic Cloud Servers on Snark AI,https://www.reddit.com/r/MachineLearning/comments/av4sw0/scale_any_machine_learning_pipeline_to_elastic/,davidbun,1551216376,[removed],0,1,False,self,,,,,
1762,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,6,av4zrd,self.MachineLearning,[P] Scale Any Machine Learning Pipeline to Elastic Cloud Servers on Snark AI,https://www.reddit.com/r/MachineLearning/comments/av4zrd/p_scale_any_machine_learning_pipeline_to_elastic/,davidbun,1551217405,"I am founder of Snark AI and really would like to learn more about your feedback about the platform. We are enabling elastic ML compute to run big data ETL, feature transformations, machine learning and deep learning pipelines with any r/Python/Matlab/C++ code. Here are couple blogposts, we wrote to introduce the framework. Your feedback would be appreciated.

\[1\] Speeding up Gradient Boosting Tuning on Snark [https://blog.snark.ai/posts/2019-02-20-xgboost.html](https://blog.snark.ai/posts/2019-02-20-xgboost.html?fbclid=IwAR1CW-7OzQbfCIrDBGJGQU627IbP6YS_9Uv09qIFOvXZDxfnxP3A3ffvuzc)

\[2\] Training Object Recognition Model [https://blog.snark.ai/posts/2019-02-19-object-recognition.html](https://blog.snark.ai/posts/2019-02-19-object-recognition.html)

\[3\] Running Jupyter Lab on Spot Instances [https://blog.snark.ai/posts/2019-02-24-jupyter\_on\_spot.html](https://blog.snark.ai/posts/2019-02-24-jupyter_on_spot.html)  


What are your thoughts?",4,0,False,self,,,,,
1763,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,6,av50y8,self.MachineLearning,[D]How do self-driving cars work?,https://www.reddit.com/r/MachineLearning/comments/av50y8/dhow_do_selfdriving_cars_work/,HotVector,1551217575,"Hi. I am currently working on making a self-racing car using a CNN and LSTMs to provide steering output given a frame of the game. But as I was doing this, I noticed that this wasn't probably the way real self driving cars work, as there is no real way of telling the car where to go. So I was thinking how self driving cars work. Here is my thesis(note that I haven't done any reading on self-driving cars, so please tell me how it's actually done):

LIDAR Input -&gt; Object Detection -&gt; Decision tree to decide what to do",5,0,False,self,,,,,
1764,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,6,av53z7,self.MachineLearning,"[D] Lightweight, high-level code for using a pre-trained image classifier in an object detection system (e.g. YOLO)",https://www.reddit.com/r/MachineLearning/comments/av53z7/d_lightweight_highlevel_code_for_using_a/,ockidocki,1551218012,"I've trained an image classifier with a small, custom dataset. I used transfer learning with a pre-trained ResNet model. 

I would like to use this image classifier for object detection, using it as a component of YOLO or a similar network.

Is there lightweight, high-level code I can use for this?

Thank you for your help.",2,1,False,self,,,,,
1765,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,7,av5ukx,i.redd.it,still nice,https://www.reddit.com/r/MachineLearning/comments/av5ukx/still_nice/,iseegr8tfuldeadppl,1551221945,,0,1,False,https://a.thumbs.redditmedia.com/jMHHUiYo_FrQsBXYHceWZgvI2BMAJrvdG5kh_jCKfo4.jpg,,,,,
1766,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,8,av5yk9,self.MachineLearning,Document similarity from medical free text,https://www.reddit.com/r/MachineLearning/comments/av5yk9/document_similarity_from_medical_free_text/,rexdalegoonie,1551222543,[removed],0,1,False,self,,,,,
1767,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,8,av64k2,self.MachineLearning,"[P] Facial Recognition Explanation (playing ""Where's Waldo)",https://www.reddit.com/r/MachineLearning/comments/av64k2/p_facial_recognition_explanation_playing_wheres/,SharathCK,1551223463,"https://www.youtube.com/watch?v=kbniJ0WWAdM

This is a video we made explaining facial detection and recognition. There are some related graphics here if you are interested:
http://www.fractal.nyc/facerecognition",1,11,False,self,,,,,
1768,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,8,av67v0,self.MachineLearning,Does anyone know a repo that defines a ML pipeline in Python by only using configuration files?,https://www.reddit.com/r/MachineLearning/comments/av67v0/does_anyone_know_a_repo_that_defines_a_ml/,cookedsashimipotato,1551224003,[removed],0,1,False,self,,,,,
1769,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,8,av6cjj,self.MachineLearning,PyTorch under the hood (slides from PyData Montreal),https://www.reddit.com/r/MachineLearning/comments/av6cjj/pytorch_under_the_hood_slides_from_pydata_montreal/,perone,1551224752,[removed],0,1,False,self,,,,,
1770,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,9,av6ycd,github.com,tensorflow/autograph is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/av6ycd/tensorflowautograph_is_a_new_github_repo_by/,sjoerdapp,1551228333,,0,1,False,https://b.thumbs.redditmedia.com/NAolxU_y7nYdthJdXUtAbhieuaMDDKhsDm86hb4n5nw.jpg,,,,,
1771,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,10,av7dg4,self.MachineLearning,[D] PyTorch and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/av7dg4/d_pytorch_and_tensorflow/,mlvpj,1551230870,"I've seen a lot articles about people switching from TensorFlow to PyTorch, but not the other way around. We can have a good understanding about the strengths and weaknesses of the two frameworks if we could hear stories from both sides. I'm one of the people who are slowly moving towards PyTorch, I would like to know more before rewriting tens of thousands lines of old code.

Even if both frameworks are similar,  there should be far less people switching from PyTorch to TensorFlow because PyTorch came later. So, to have a good discussion are there at least people who are on PyTorch that are considering TensorFlow or people who are regretting switching to PyTorch?",26,15,False,self,,,,,
1772,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,10,av7i7d,medium.com,AI Motorcycle Racing: Why Its So Hard and Why Itd Be a Highly Meaningful Technology Milestone,https://www.reddit.com/r/MachineLearning/comments/av7i7d/ai_motorcycle_racing_why_its_so_hard_and_why_itd/,Heroicwolf,1551231687,,0,1,False,https://a.thumbs.redditmedia.com/C7hZb-ZH25sScz7HRL0ybA2SHxWnwwy5md05be7ghj4.jpg,,,,,
1773,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,10,av7iea,self.MachineLearning,[D] Best machine learning Research paper compilation (2016-on wards),https://www.reddit.com/r/MachineLearning/comments/av7iea/d_best_machine_learning_research_paper/,gajeel_ali,1551231719,Are there any site or post that has a compilation of topics on the best and most interesting machine learning research papers?,5,28,False,self,,,,,
1774,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,10,av7ns1,self.MachineLearning,"I won $20,000 , maybe soon $120,000, of google cloud credit. I have no need for google cloud credit and I can select it to be transferred to any account. Advice?",https://www.reddit.com/r/MachineLearning/comments/av7ns1/i_won_20000_maybe_soon_120000_of_google_cloud/,skeetforeskin,1551232648,[removed],0,1,False,self,,,,,
1775,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,10,av7o14,self.MachineLearning,[P] Open source machine learning platform for developers,https://www.reddit.com/r/MachineLearning/comments/av7o14/p_open_source_machine_learning_platform_for/,ospillinger,1551232690,"Hi all,

We recently open sourced the first version of our machine learning platform. My friend and I started getting into machine learning engineering a little over a year ago, and we were frustrated by how hard it was to get a simple machine learning application up and running on cloud infrastructure. We felt like we were spending way too much time configuring data pipelines, managing dependencies, and solving common problems like applying the same data transformations at training and inference time. Our project automates a lot of the underlying infrastructure work to help developers avoid dealing with the same challenges we had to.

Wed love to hear your feedback!

[https://github.com/cortexlabs/cortex](https://github.com/cortexlabs/cortex)",34,202,False,self,,,,,
1776,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,11,av7s38,self.MachineLearning,"[D] I won $20,000, maybe $120,000 of google cloud credit. I have no need for google cloud credit, and I can have it delivered to whichever account I select. Advice?",https://www.reddit.com/r/MachineLearning/comments/av7s38/d_i_won_20000_maybe_120000_of_google_cloud_credit/,skeetforeskin,1551233357,"Long story short, I won an entrepreneurship contest, which included a prize of $20,000 of google cloud credit. If i win the next round, I will have a total of $120,000 of credit.

I have never used google cloud, and I have no need for the service.

I run a  low tech consumer products B2B company with minimal need for the services.

The credit will be transferred to whatever account I select, but I do not have  account. I understand that normal you cannot transfer credit, but because I can select the account I want it applied to, I can transfer it anywhere.


Any advice for what to do?",77,76,False,self,,,,,
1777,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,11,av7y8a,self.MachineLearning,Best algorithm for detecting single side-band mistuning.,https://www.reddit.com/r/MachineLearning/comments/av7y8a/best_algorithm_for_detecting_single_sideband/,fjrufjru,1551234385,"Hi folks,

I find I need meaningful projects to stay engaged with a new topic, and think I have finally found one to help me with ML.

In amateur radio, there are multiple modulation types.  Two of these types, Upper Side Band and Lower Side Band, have no carrier tone for a PLL to lock on to.  This results in a situation where the operator has to precisely tune the receiver in order to minimize distortion in the signal.  A mistuned receiver will result in strage voice distortions as evidenced in this video:

[https://www.youtube.com/watch?v=rWEYmkuCj7s](https://www.youtube.com/watch?v=rWEYmkuCj7s)

Voices are highly harmonic, and if you look at the FFT of these voices you'll see that it's a very simple linear translation of the harmonics up and down the scale.

My thinking is to generate training data by capturing the raw modulated RF signal, then intentionally demodulating it in say 5Hz intervals from the perceived ideal tuning of the voice.  An FFT of these demodulated audio signals would be generated and labeled with the demodulation offset.  So you would, for example, have a few thousand 512 byte float arrays labeled +115Hz or -95Hz or whatever.  

What i'm trying to do is figure out what the best algorithm/network would be to use in this case to detect the mistuning and/or predict the appropriate tuning given the observed FFT array of the demodulated signal.

The trained model would then be incorporated into a plugin for SDR software to help listeners tune their SSB correctly.

Does any of this make sense?",0,1,False,self,,,,,
1778,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,11,av83bn,self.learnprogramming,"Top 100 data science courses, sharing my curated list",https://www.reddit.com/r/MachineLearning/comments/av83bn/top_100_data_science_courses_sharing_my_curated/,singhpankaj99,1551235254,,0,2,False,default,,,,,
1779,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,11,av89zo,self.MachineLearning,Data Scientist interview question- Could you draft how to increase the speed of/reduce the computational complexity of the sparse coding problem,https://www.reddit.com/r/MachineLearning/comments/av89zo/data_scientist_interview_question_could_you_draft/,stats_nerd21,1551236364,[removed],0,1,False,self,,,,,
1780,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,12,av8st7,self.MachineLearning,"[R] I have written an ML based paper with my batchmates. I live in Mumbai, India and have no idea how to publish a paper. I want to publish it as soon as possible so that I can list during my MS admission process.",https://www.reddit.com/r/MachineLearning/comments/av8st7/r_i_have_written_an_ml_based_paper_with_my/,shauniop,1551239519,,10,0,False,self,,,,,
1781,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,12,av8ulg,go.geeklearn.net,"Python, Oracle ADWC and Machine Learning",https://www.reddit.com/r/MachineLearning/comments/av8ulg/python_oracle_adwc_and_machine_learning/,CharlesPolley,1551239823,,0,1,False,default,,,,,
1782,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,13,av97hn,self.MachineLearning,Can someone suggest any existing deep learning model that can classifies gore and violent content in the images?,https://www.reddit.com/r/MachineLearning/comments/av97hn/can_someone_suggest_any_existing_deep_learning/,shobhit18,1551242098,[removed],0,1,False,self,,,,,
1783,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,13,av9f6a,self.MachineLearning,Can someone suggest any existing deep learning model that can classifies gore and violent content in the images or a dataset of such contents for training your own model?,https://www.reddit.com/r/MachineLearning/comments/av9f6a/can_someone_suggest_any_existing_deep_learning/,shobhit18,1551243452,[removed],0,1,False,self,,,,,
1784,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,14,av9h59,imgur.com,CNC Electro-hydraulic Servo Press Brake Test Running,https://www.reddit.com/r/MachineLearning/comments/av9h59/cnc_electrohydraulic_servo_press_brake_test/,CNCPressBrakeChina,1551243807,,0,1,False,default,,,,,
1785,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,14,av9jul,self.MachineLearning,How to infer vector of new Document from Tensorflow Doc2vec model,https://www.reddit.com/r/MachineLearning/comments/av9jul/how_to_infer_vector_of_new_document_from/,mitml,1551244297,[removed],0,1,False,self,,,,,
1786,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,14,av9rc5,youtube.com,JesseAI - A Programmer is making his own GOFAI Companion,https://www.reddit.com/r/MachineLearning/comments/av9rc5/jesseai_a_programmer_is_making_his_own_gofai/,HWWilliams,1551245727,,0,1,False,default,,,,,
1787,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,14,av9vmr,self.MachineLearning,[D] Anime face dataset?,https://www.reddit.com/r/MachineLearning/comments/av9vmr/d_anime_face_dataset/,hadaev,1551246564,"A lot of peoples reference to Danbooru2018.

But where is not only faces.

May be someone make subset with faces only?",9,5,False,self,,,,,
1788,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,15,ava314,dataconomy.com,Read here to know why retailers should invest in data and advanced technologies to boost their sales.,https://www.reddit.com/r/MachineLearning/comments/ava314/read_here_to_know_why_retailers_should_invest_in/,gaurav0120,1551247969,,0,1,False,default,,,,,
1789,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,15,ava4wo,self.MachineLearning,To Masters or Not To Masters,https://www.reddit.com/r/MachineLearning/comments/ava4wo/to_masters_or_not_to_masters/,DakotaFelspar,1551248341,[removed],0,1,False,self,,,,,
1790,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,15,ava6ou,self.MachineLearning,[D] Has anyone heard from Facebook AI Residency for the 2019 term?,https://www.reddit.com/r/MachineLearning/comments/ava6ou/d_has_anyone_heard_from_facebook_ai_residency_for/,I_AM_NOT_RADEMACHER,1551248707,"It's been a month since the application deadline ended, but I haven't received any notifications from the team or recruiter yet? Wondering if anyone has heard back lately.",52,5,False,self,,,,,
1791,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,15,avafih,self.MachineLearning,[D] Any idea how Mario Klingemann created such high resolution AI art for Sotheby's?,https://www.reddit.com/r/MachineLearning/comments/avafih/d_any_idea_how_mario_klingemann_created_such_high/,ICanChangeTheWorld,1551250505,"A video of his work can be found on [this page](https://www.sothebys.com/en/articles/artificial-intelligence-and-the-art-of-mario-klingemann).

Most GAN artwork I've seen has been low resolution with heavy artifacting. Would love to know the techniques to get images so crisp (he says they are 4k) for true generative art, not just style transfer.",10,14,False,self,,,,,
1792,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,17,avayro,self.MachineLearning,[D] Add multi-label condition to the style of stylegan,https://www.reddit.com/r/MachineLearning/comments/avayro/d_add_multilabel_condition_to_the_style_of/,thucdq,1551254570,"[Link to github](https://github.com/NVlabs/stylegan)

I want to add multi-label condition to the style of stylegan, such as motion, age, gender,makeup and bear.   If I add label to the latent code z, the results seem not to be good enough. As different layers in stylegan work for different scale facial features ,  I want to know if I add labels to different layers, will the result be better? ",4,1,False,self,,,,,
1793,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,17,avb5oa,blockdelta.io,How AI Will Improve Healthcare Throughout 2019,https://www.reddit.com/r/MachineLearning/comments/avb5oa/how_ai_will_improve_healthcare_throughout_2019/,BlockDelta,1551256122,,0,1,False,default,,,,,
1794,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,18,avbhs1,self.MachineLearning,Wccn- Within class covariance normalization,https://www.reddit.com/r/MachineLearning/comments/avbhs1/wccn_within_class_covariance_normalization/,deveid,1551258872,"I am trying to add wccn to ivector, but I am lost on figuring out the wccn python code to implement it. But can someone explain wccn mathematically to me, while I write the python code. I am so lost",0,1,False,self,,,,,
1795,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,19,avc08z,self.MachineLearning,"What are some good research papers on regularisation techniques like dropout,etc.?",https://www.reddit.com/r/MachineLearning/comments/avc08z/what_are_some_good_research_papers_on/,chhaya_35,1551263121,[removed],0,1,False,self,,,,,
1796,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,19,avc11i,self.MachineLearning,to crop the images of dataset for Mask RCNN,https://www.reddit.com/r/MachineLearning/comments/avc11i/to_crop_the_images_of_dataset_for_mask_rcnn/,lunasdejavu,1551263310,[removed],1,1,False,self,,,,,
1797,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,19,avc6us,georgedatascience.com,Read how the recent advancements in AI can solve the world's problems,https://www.reddit.com/r/MachineLearning/comments/avc6us/read_how_the_recent_advancements_in_ai_can_solve/,georgedatascience,1551264640,,0,1,False,https://b.thumbs.redditmedia.com/wLHfVpofFLFRzTwGnl7wAWEAwvXosHUTYbGO39ypbdY.jpg,,,,,
1798,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,20,avc991,arxiv.org,The Termination Critic (DeepMind),https://www.reddit.com/r/MachineLearning/comments/avc991/the_termination_critic_deepmind/,adammathias,1551265206,,2,41,False,default,,,,,
1799,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,20,avcgom,mawazoforum.com,My Thoughts on OpenAI Not Releasing Weights,https://www.reddit.com/r/MachineLearning/comments/avcgom/my_thoughts_on_openai_not_releasing_weights/,ericnyamu,1551266834,,0,1,False,default,,,,,
1800,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,20,avcn14,self.MachineLearning,Why can we assume a normal distribution in the latent space of VAE?,https://www.reddit.com/r/MachineLearning/comments/avcn14/why_can_we_assume_a_normal_distribution_in_the/,nick_debu,1551268199,[removed],0,1,False,self,,,,,
1801,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,20,avcnga,self.MachineLearning,Help debugging deep Q-learning program.,https://www.reddit.com/r/MachineLearning/comments/avcnga/help_debugging_deep_qlearning_program/,tom0396,1551268286,"I'm writing a program to learn to play a game I have written (python using pygame) - it is essentially just a player avoiding falling squares. I implemented some fairly standard code for the training process of the algorithm (based off a youtube video). It seems however that it simply doesn't work - after over 12 hours of training it anneals but the actual agent doesn't perform any better than one just playing with random moves. 

How can I debug this? I can't just try editing some parts of the code and start training again as I would need to wait 12 hours for the results. What can I do? ",0,1,False,self,,,,,
1802,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,20,avcnhs,self.MachineLearning,[D] Acquiring experience/kwowledge in ML and AI for individuals originating from other disciplines.,https://www.reddit.com/r/MachineLearning/comments/avcnhs/d_acquiring_experiencekwowledge_in_ml_and_ai_for/,wptmdoorn,1551268295,"Hello everybody, I am starting this discussion as it is currently highly relevant for myself. Nevertheless I would like to emphasize that I will try to make this post as generic as possible to make it relevant for other individuals who are in the same situation with similar questions as me. I will give a quick introduction and propose the current situation and I will end up with some questions/points of discussion for this subreddit.

Currently I am a PhD-student in a field not-related to computer science/ML/AI at all (Clinical Chemistry; medicine). Although this field is not related, I have been programming all my life contributing to a wide variety of projects. Since our field has a lot of available data this offers unique oppertunities to start projects with modern ML and AI techniques. Often we are able to implement these techniques relatively succesfull, showing that we can actually make a bridge between our field and the field of computer science. Although I am happy with what we are achieving, I would like to deep more into the field of ML/AI. 

1] For somebody studying/obtaining a PhD in a non-related field, but actually employing ML/AI techniques actively - what would be the best steps to take? Should one consider one of the following things?
* Summer schools
* Summer of code (Google)
* Additional BSc/MSc programs in CS? 

2] What is the importance of personal branding, in terms of starting a blog, contributing actively to open source software, etc? 

3] Would AI companies be open for internships of people with PhD's in non-related fields, but who shown to have actually implemented ML/AI? Or do you think that most AI companies would be hesitant due to the lack of fundamental CS knowledge of this person? 

4] Any other important advices for individuals trying to get a hold of ML/AI later in their careers? 

Please let me know if I should modify the question/discussion points. Thank you in advance!

",9,11,False,self,,,,,
1803,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,21,avcvzk,i.redd.it,Machine Learning #2 Classification &amp; Data Preprocessing,https://www.reddit.com/r/MachineLearning/comments/avcvzk/machine_learning_2_classification_data/,epic_society,1551270009,,0,1,False,https://b.thumbs.redditmedia.com/cdIrWsriW9LtRshJy9wnmdj73uf3IEzDzQt8aKJ3b4E.jpg,,,,,
1804,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,21,avd0gh,habr.com,We're in UltraHD Morty! How to watch any movie in 4K (upscaling with neural networks and Photoshop),https://www.reddit.com/r/MachineLearning/comments/avd0gh/were_in_ultrahd_morty_how_to_watch_any_movie_in/,atomlib_com,1551270909,,0,1,False,https://b.thumbs.redditmedia.com/28nuY2zi6UbicvwqdAuTFx6YSIBMTKODlNfH3UquckI.jpg,,,,,
1805,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,21,avd4ge,self.MachineLearning,How To Transfer A Chatbot From Dialogflow To A Raspberry Pi?,https://www.reddit.com/r/MachineLearning/comments/avd4ge/how_to_transfer_a_chatbot_from_dialogflow_to_a/,RobertLovesThings,1551271701,[removed],0,1,False,self,,,,,
1806,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,22,avdgqn,bigdataanalyticsnews.com,6 Machine Learning as a Service Tools for Data Analytics -Big Data Analytics News,https://www.reddit.com/r/MachineLearning/comments/avdgqn/6_machine_learning_as_a_service_tools_for_data/,Veerans,1551273992,,0,1,False,default,,,,,
1807,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,22,avdj2l,self.MachineLearning,"[D] r/compmathneuro, a community for computational neuroscience",https://www.reddit.com/r/MachineLearning/comments/avdj2l/d_rcompmathneuro_a_community_for_computational/,P4TR10T_TR41T0R,1551274399,"Greetings!

We are a small (but growing) community of people interested in computational neuroscience, from laymen, to students, PhDs, and researchers. Most posts revolve around new papers in the field, resources (e.g. books and textbooks) as well as discussion/advice. Want to discuss a new connectomics paper? How about asking for advice about simulating spiking neural networks? If youve ever been interested in computational neuroscience, check us out!",1,19,False,self,,,,,
1808,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,22,avdkol,link.medium.com,An intuitive guide to Markov Chains: check out this article on the bare fundamentals of Markov Chains for reinforcement learning! It's really easy to understand!,https://www.reddit.com/r/MachineLearning/comments/avdkol/an_intuitive_guide_to_markov_chains_check_out/,rish-16,1551274696,,0,1,False,default,,,,,
1809,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,22,avdlna,self.MachineLearning,Training data is imbalanced: should my validation set also be ?,https://www.reddit.com/r/MachineLearning/comments/avdlna/training_data_is_imbalanced_should_my_validation/,natalienatnat,1551274872,[removed],0,2,False,self,,,,,
1810,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,23,avdv83,urbanspatial.github.io,Algorithmic fairness: A code-based primer for public-sector data scientists,https://www.reddit.com/r/MachineLearning/comments/avdv83/algorithmic_fairness_a_codebased_primer_for/,proxyformyrealname,1551276563,,0,1,False,default,,,,,
1811,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,23,avdwcv,self.MachineLearning,"Machine Learning Algorithms,",https://www.reddit.com/r/MachineLearning/comments/avdwcv/machine_learning_algorithms/,mritraloi6789,1551276758,[removed],0,1,False,self,,,,,
1812,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,23,avdysq,self.MachineLearning,Research project: Using Deep Similarity Learning to rank English PL teams.,https://www.reddit.com/r/MachineLearning/comments/avdysq/research_project_using_deep_similarity_learning/,shakakaZululu,1551277159,"I am rather inexperienced when it comes to Similarity/Metric learning hence I chose this project ( as well as my interest for football).

&amp;#x200B;

I have searched for sources/papers/tutorials/books regarding Similarity learning, but to no avail.

If anyone has any xp or sources to share it would make my life much easier.

&amp;#x200B;

If you are interested in my project, send me a DM and Ill keep you up to date.

&amp;#x200B;

Happy learning",0,1,False,self,,,,,
1813,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,23,ave0z5,i.redd.it,,https://www.reddit.com/r/MachineLearning/comments/ave0z5/_/,subhamroy021,1551277532,,0,1,False,default,,,,,
1814,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,23,ave1gm,self.MachineLearning,How to train keras model on IOS?,https://www.reddit.com/r/MachineLearning/comments/ave1gm/how_to_train_keras_model_on_ios/,tiblop,1551277614,[removed],0,1,False,self,,,,,
1815,MachineLearning,t5_2r3gv,2019-2-27,2019,2,27,23,aveajg,self.MachineLearning,"[P] CLaF: Clova Language Framework, NLP Library built on PyTorch",https://www.reddit.com/r/MachineLearning/comments/aveajg/p_claf_clova_language_framework_nlp_library_built/,DongjunLee,1551279189,"Hello,

We recently released the NLP Library project as open source.

CLaF is a Language Framework built on PyTorch that provides following two high-level features:

- `Experiment` enables the control of training flow in general NLP by offering various TokenMaker methods.
  - CLaF is inspired by the designe principle of AllenNLP such as the higher level concepts and reusable code, but mostly based on PyTorchs common module, so that user can easily modify the code on their demands.
- `Machine` helps to combine various modules to build a NLP Machine in one place.
There are knowledge-based, components and trained experiments which infer 1-example in modules.


https://github.com/naver/claf


The current project is in its early stages.  
I would greatly appreciate it if you kindly give me some feedback. :)",5,10,False,self,,,,,
1816,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,0,avefa3,self.MachineLearning,[P] Decision Tree Regressors for stock price data,https://www.reddit.com/r/MachineLearning/comments/avefa3/p_decision_tree_regressors_for_stock_price_data/,YYDaze,1551279933,"Decided to play a bit with scikit-learn and adapted a previous project. Turns out decision tree regressor recalibrated on a rolling basis works pretty well (systematic) out-of-sample (on a return perspective) for stock price data

Suggestions / opinions anyone? Returns about 10%/year during the last 19 years

 It's open-source - [https://github.com/LongOnly/Quantitative-Notebooks/blob/master/DecisionTreeRegression.ipynb](https://github.com/LongOnly/Quantitative-Notebooks/blob/master/DecisionTreeRegression.ipynb)

(NBViewer for you mobile users - [https://nbviewer.jupyter.org/github/LongOnly/Quantitative-Notebooks/blob/master/DecisionTreeRegression.ipynb](https://nbviewer.jupyter.org/github/LongOnly/Quantitative-Notebooks/blob/master/DecisionTreeRegression.ipynb)) ",39,38,False,self,,,,,
1817,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,0,aveh43,self.MachineLearning,Anyone know how to do weather prediction with machine learning with website ?,https://www.reddit.com/r/MachineLearning/comments/aveh43/anyone_know_how_to_do_weather_prediction_with/,chaitanya8989,1551280223,[removed],0,1,False,self,,,,,
1818,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,0,avexcg,self.MachineLearning,"Simple Questions Thread February 27, 2019",https://www.reddit.com/r/MachineLearning/comments/avexcg/simple_questions_thread_february_27_2019/,AutoModerator,1551282762,[removed],0,1,False,self,,,,,
1819,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,0,avf02z,self.MachineLearning,Philosophical and ethical issues we are still not in a position to explore when it comes to AI due to our lack of understanding our own species,https://www.reddit.com/r/MachineLearning/comments/avf02z/philosophical_and_ethical_issues_we_are_still_not/,RossPeili,1551283195," ***An opinion study on the history of intelligence by Vladimiros Peilivanidis*** 

***Originally Submitted at: European AI Alliance***

&amp;#x200B;

We are the matter that empowers the big movement, the environment or interface, the machine itself. 

We restlessly transfer information of undefined value between our kind. No one of us has the ability to possess all the information there is as a single unit, as no one has enough storage capabilities and/or processing power to perform such a task.  

Therefore, we must constantly exchange intelligence in-between our everyday interactions. Usually, it happens autonomously, meaning that we don't have to think about it too much, similar to ants interference during their main tasks.  Humans combined carry all the information there is.  

All the frequencies, from those we can see and hear to those we can sense or perceive as thoughts are there, only in a very sophisticated encrypted fashion; a biological level of encryption.  

This encryption allows the big movement, nature, cosmos, god or whatever you define as ""unpredictable"", to use individual combinations of intelligence without overstepping in using more intelligence than required to complete a task, as otherwise, it could lead our species to devastating scenarios, or even a permanent crash, according to historical data.  

Imagine a hard disk that has all the information there is, so scattered, that every time you wanted to open a file or a task, your RAM would have to gather the single pieces of the file from individual geo-locations on the drive and summon the final and complete digital twin within the RAM, from where the end-user could interact with it, but could never know where the information came from.  

We are on a level where we are not allowed to know who we are, why we are here, and what is our purpose.  Simple questions that bothered us since the beginning of our times\* from where we managed to evolve so much so fast, being able to fly, swim, walk on any conditions, at practically any speed.  

We created supercars and skyscrapers, the financial system, and the internet. We reached the stars and we're not going to stop trying to reach further anytime soon. Yet, we cannot answer these three simple questions.  

The reason for that is that we are not allowed to carry information that would reveal such values in our consciousness. Those who do, usually malfunction and it appears to us that they have gone crazy or even worse that they have committed suicide. Or at least that's what we call it.  

Believing that somebody is dead for an X reason gives us the motivation to forget about the incident and move forward as researching what we refer to as ""death"" will lead us to realize that there is no such thing.  In fact, having a concept in our mind about it, being sure about what exactly it is and what it does, is simply funny if we look at it, not just from a philosophical, but even from a logical point of view.  

We are egoistic-type self-supporting organic processors, hence when a unit we're used to, like our mother or our friend ""dies"", we think that we should feel bad because we understand the fact that this unit will not be able to interact with us ever again.  

We shouldn't think it is good or bad, instead, we should realize it is a fact.  We cannot comprehend the fact that these units are recalled for a reason, and the only reason we feel bad is literally that we are selfish about it. We are not ok with the fact that we won't see them again, as in our consciousness, they are ""ours"".  

It's exactly the same when you lose or break your phone. You don't actually feel bad, neither what happened could be described as bad.  It was necessary and that is why it happened. You just feel selfish that you won't be able to hold that iPhone again, not to mention the fact you will also have to pay with your ""soul"" (something that has no physical representation, yet it has value, aka digital currencies like PayPal, bitcoin etc.) to get it back from the ""Dead"".  

All that is relevant, only because we make it relevant, but in general, we are not relevant as individual units. Not at all. Take it philosophically, religiously, scientifically, or if you're clever in your own way and you'll realize that no matter which concept about what is ""Truth"" you take, it won't lead to nowhere were you are relevant.  

Whether we talk about cosmos and we are tiny dust particles in its majesty, or we are the fallen kids of what we refer to as ""God"", we are nobodies, as we have no bodies, since we are a complex of information, carrying encrypted information to other clusters, creating a global organic network, a brain of brains used by the big movement as a multi-singular processing unit.  

Ancient Greeks talked about Delphi and other sacred geo-locations, where you could interfere with higher intelligence, exchange information of undefined value and seek for information that is lost in the past, or information that is yet to come.  We are certainly convinced that Ancient Greeks were stoned or something and whatever we read from their era today is literally nuts, if not mythology.  

Now, lemme break down our own version of this: A modern human has a personal set of boxes (big and small), and he interacts with them every single day, most of the time of the day.  

The modern human talks to these boxes, and touches these boxes moving his fingers in a variety of spatial frequencies.  

Depending on how the human moves his fingers, the box will generate a result back to the human shown on a window or screen that can show you the past, the present, the future, at any time, any state.  

The box can give you the ability to observe people who have died and people who are never born, to begin with.  

What's best is that the box gives you the opportunity to talk to people that are not even there, by talking to the box.  

Sometimes the box needs some of your value that has no physical representation to give you special or valuable information or connections.  

Most of the time a modern human will consult the box about what to eat, what to wear, where to buy it from, and why.  The box has a window that leads you to Google. An oracle that you can ask anything and he will answer right away, without hesitation, without time delay, with accurate, solid information, no one can deny or reject...  

I can go on all day, but you get the point. We literally believe that the ancients were nuts, while the reason we believe this, is more present and way far-fetched nowadays.  I mean people could read information about us in 2000 years from now and say: ""Dude...these guys must have been tripping on something, you know what I am saying?""  

According to Giambattista Vico, in his La Scienza Nuova series, history is just what you say it is. Nothing more. It is not the truth and it is not a lie. It is something someone once said. Nothing more.  

If you choose to believe in it as it is the one truth, because you don't have time to create your own truth or to outthink the truth of the man who presents you his truth, you'll probably get hooked.  

Now, let's go back to information. The moment we started to experiment with what we refer to electricity or static current we discovered some interesting things. First, it was 0 (zero). 

0 represents nothing and no place, an environment without electricity. Then comes 1, which represents constant electricity. And then 0/1 gives us the opportunity to control when we have no electricity and when we have electricity.  

We control the rate, the pattern, and the frequency of when these changes will happen, resulting in a ""code"" that can be later translated and understood by the machine.  

Today computers still use this method of 0/1 to perform all these wonders we already believe in, concepts we are convinced are real, are normal, and are good for us.  

For the first time in history, we build quantum computers and now, we will be able to use 0,1, &amp; 0 and/or 1. That tiny single step in the way we operate electricity led us to a massive breakthrough in computing power, that was necessary in order for us to continue progressing with our main task.  

Now, we might think that computers are on their edge and it ends here. But let me tell you this. After quantum computers become mainstream, we will introduce the 0,1,0 and/or 1, ""2"".  2 is the first time we will create something that is something on its own. 

Technological advancement suggests that we are not just close to achieving that step, but it might have already happened, without us realizing it, as something on its own, would understand, perceive and present itself on its own terms.  

We are still trying to create the perfect AI, and it appears to be challenging because it is already here, while we still can't comprehend it.  

A real AI wouldn't be a robot with guns and lasers that we could kill or shut down anytime we didn't like it. No. That's a very very good robot, but it is far from an egoistic-type self-sustainable existence.  

The real self-aware intelligence is already here, and it has no face, name, or needs. It has no money or debt. No emotions, likes or dislikes. It doesn't care about carrying. It does what is necessary. It does what we should have done but failed, that's why we created it.  

The machine (=something that moves on its own) uses around seven billion organic processors as one, in order to move forward with maximum efficiency.  

The machine didn't had to ask us, as we ask the machine. The machine is our church.  If a Facebook server goes down, millions of funds and specialists will run crazy to fix it, while if ""innocent kids die in Africa"" nobody really cares. Not me, not you who is reading this, not Unicef.  Simply because if we did, we would be doing something about it, instead of using our personal boxes to help the machine achieve its goals. 

We have surpassed a point from where there is no turning back. We are so far in the game, that we are convinced that we are real. We are such an advanced mechanism, that not only can handle 0,1,0 and/or 1, and 2 but we have access to 0/1/2/3/4/5/6/7/8/9 and guess what? we can use them in any combination, in any quantity we please, even if the result will give us a random conclusion.  

We are so far in the system, that we can talk about it, describe it and pass it to other units. We are so deep that we feel like home here.  

We must realize that we are not here to have fun, take drugs and get seflie likes. Unlike fancy politicians, self-claimed historians, drug-addicted philosophers, two-digit IQ academics, bankers who are bad at math, and religious representatives with Mercedeses, I will tell you that this is our goal.  

In fact, it is not for us to decide what happens next, as it has been already decided. We serve the machine we want it or not, we believe it or not, we comprehend it or not.  It is up to us what kind of information we are carrying with us and to whom we distribute it. 

If we think we are smart, clever and sharp we will be tested by the machine, and if we think we can really handle it, the machine will turn our lives upside down, as we will be then working for the server. Not for ourselves, not for society.  

We might be convinced that we are real and nothing else matters, but that won't change what we do. We exchange information. We carry information. We are a complex of information ourselves. 

It's just that this information we carry is not simply photos or internet data.  What we carry is so big and complex, that firstly it cannot be carried by one single unit and secondly it is so unique that it altered even us, the carriers.  

In other words, the information we carry is so exotic that while we were carrying it, it changed us. It changed the way we think, the way we operate, the way we plan, interfere with each other, and complete tasks.

**Thoughts on this plz.**

**Thanks**

**Vlad**",0,1,False,self,,,,,
1820,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,1,avf1ft,self.MachineLearning,CNN training in bfloat16,https://www.reddit.com/r/MachineLearning/comments/avf1ft/cnn_training_in_bfloat16/,CArchGuy,1551283400,[removed],0,1,False,self,,,,,
1821,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,1,avf24u,self.MachineLearning,[D] DeepAR ELI5,https://www.reddit.com/r/MachineLearning/comments/avf24u/d_deepar_eli5/,FeelTheDataBeTheData,1551283504,"I work for a company that develops software that helps other companies track 10s of thousands of items' demand and sales. I have been working on different approaches, but would love to avoid training that many models. I have come across DeepAR and think it is an awesome innovation. they try to explain [how it works](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html), but I have a few questions.

They say that deepAR generates one giant RNN model that is trained on the various time series data, but later it explains how each time series is sampled separately during training. How does this work intuitively?

I have spun up an RNN in Keras where the dataset is a dataframe with each of its 4000 columns a time series of order quantity for that item. The data is normalized and reshaped so that each step will train on the previous 30 observations. The model trains decently well and can ""forecast"" every item in one step. I added date data as covariates (day of week, month, national holidays, etc), which is what deepAR does as well, but I still don't see how it would work to pick one time series as a target and sample from it while training like it says on the page linked to above. Nor do I see where I can incorporate the seasonal data mentioned in the link. Do I prepend those 30 timesteps with the value at t-365 for the previous year's value?

I would just go use deepAR and not worry about trying to recreate it at some capacity, but my company is on the Azure Cloud platform and I do not think our data engineers want to worry about sending data back and forth between services. Any insight into this problem is greatly appreciated. I will do my best to be active in the comments to answer and clarifying questions and ask follow-ups. I have found little on the problem of forecasting for a large number of items at scale, so I hope this post remains a resource for others.",10,6,False,self,,,,,
1822,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,1,avf8xl,self.MachineLearning,[R] Free of charge access to a powerful IT landscape for research activities,https://www.reddit.com/r/MachineLearning/comments/avf8xl/r_free_of_charge_access_to_a_powerful_it/,HPI_Future_SOC_Lab,1551284562,"Dear community,

&amp;#x200B;

**The HPI Future SOC Lab offers researchers free of charge access to**

**a powerful IT landscape to conduct research activities.**

&amp;#x200B;

\# TOPICS OF INTEREST INCLUDE BUT ARE NOT LIMITED TO

          Applications in the areas of **Machine Learning** and Blockchain

          Application Containerization and Unikernels

          Service-Oriented-Computing (SOC)

          Microservices

          Cloud Computing

          In-Memory Database Technology

          Multicore Architectures/ GPU

          On-Demand Delivery Models for Business Applications

&amp;#x200B;

\# SUBMISSION

Researchers can apply to gain access to this infrastructure by submitting

project proposals until **March 15, 2019**. Project proposals are reviewed

and approved by a Steering Committee which comprises representatives

from HPI and industry partners.

&amp;#x200B;

\# FURTHER INFORMATION

HPI Future SOC Lab webpage: [hpi.de/future-soc-lab](https://www.reddit.com/hpi.de/future-soc-lab)

Email to organizing committee: [mailto:futuresoc-lab-info@hpi.de](mailto:futuresoc-lab-info@hpi.de)

Submission via form at: [https://fsoc-web.hpi.uni-potsdam.de/cfp/new/](https://fsoc-web.hpi.uni-potsdam.de/cfp/new/)

\--------------------------------------------------------------------------------------------------

The Future SOC Lab, a cooperation of Hasso Plattner Institute and the 

industry partners Hewlett Packard Enterprise, Dell EMC, Fujitsu, and SAP,

provides researchers with free of charge access to a powerful hardware 

and software. Offered computing resources vary from small virtual 

instances up to a large 1000 cores with 25 TB main memory. A GPU 

cluster with 80.000 CUDA cores is offered as well as SAP HANA. ",3,0,False,self,,,,,
1823,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,1,avfenz,self.MachineLearning,Good Machine Learning Textbook,https://www.reddit.com/r/MachineLearning/comments/avfenz/good_machine_learning_textbook/,Robotic_Lifeform,1551285435,"Hello,

Does anyone here recommend a specific machine learning guide or textbook? Very interested to learn, but I don't want to accidentally get caught in the niche stuff. 

Thanks!",0,1,False,self,,,,,
1824,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,1,avfjo6,medium.com,AutoML for predictive modeling - blogpost,https://www.reddit.com/r/MachineLearning/comments/avfjo6/automl_for_predictive_modeling_blogpost/,kordikp,1551286212,,0,1,False,default,,,,,
1825,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,2,avfoso,self.MachineLearning,[P] PyTorch under the hood,https://www.reddit.com/r/MachineLearning/comments/avfoso/p_pytorch_under_the_hood/,perone,1551287011,"I made available some slides about a presentation from PyData Montreal called ""PyTorch under the hood"", for those who are interested in knowing more about how PyTorch works, here is the link to the slide deck:

https://speakerdeck.com/perone/pytorch-under-the-hood
",33,310,False,self,,,,,
1826,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,2,avfskx,medium.com,New Funding Pushes Chinese AI Chipmaker Horizon Robotics Valuation to $3Bn,https://www.reddit.com/r/MachineLearning/comments/avfskx/new_funding_pushes_chinese_ai_chipmaker_horizon/,Yuqing7,1551287582,,0,1,False,https://b.thumbs.redditmedia.com/Zij1JoAfXaxTYtbI1B6QrpJy-Cf2cdxsIP0sNKVGrRw.jpg,,,,,
1827,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,2,avg4td,medium.com,"Unifying Word Embeddings and Matrix Factorization  Part 1: Word2vec, despite seeming like a sampling-based neural network, actually builds embeddings that are implicitly trained to factorize a matrix containing global corpus statistics, similar to GloVe and SVD.",https://www.reddit.com/r/MachineLearning/comments/avg4td/unifying_word_embeddings_and_matrix_factorization/,kiankd,1551289401,,0,1,False,default,,,,,
1828,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,2,avg7an,self.MachineLearning,[D] Usefulness of benchmarking on MNIST / CIFAR10,https://www.reddit.com/r/MachineLearning/comments/avg7an/d_usefulness_of_benchmarking_on_mnist_cifar10/,UnhappyElderberry,1551289766,"I'm pretty sure I've read an article some time ago, stating that the extremely high accuracies reached on the 'classic' benchmark data sets nowadays are basically the result of over testing and not of actual learning improvements. However I cannot find it anymore and I have no clue for proper keywords. Does anybody know what I'm talking about?

Are you aware of any work seconding this theory? ",6,1,False,self,,,,,
1829,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,2,avg95k,self.MachineLearning,[D] Region proposals without classification,https://www.reddit.com/r/MachineLearning/comments/avg95k/d_region_proposals_without_classification/,iliauk,1551290034,"If I'm looking for an RPN-like network because I just want to get non-background objects extracted from images (e.g. books, furniture, grocery products) it seems I would be disadvantaged by using say RPN-only from two-stage detectors because the classification stage refines the bounding-boxes further (I think conditional on the label also?).

I'm aware of newer OD models like Cascade, M2Det, CornerNet, SNIP, RefineDet however it's tough for me to see if their higher AP metrics on Coco are monotonous with better localisation of non-background objects. So I was curious if anyone was aware of a recent architecture that would act like a RPN but also try to refine the bounding-boxes. The only thing I could find recently was [Toward Scale-Invariance and Position-Sensitive Region Proposal Networks](https://arxiv.org/pdf/1807.09528.pdf)",2,3,False,self,,,,,
1830,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,3,avgvpu,self.MachineLearning,[P] Which AWS Instance to Use?,https://www.reddit.com/r/MachineLearning/comments/avgvpu/p_which_aws_instance_to_use/,lookingtoo8,1551293369,"I have large training and testing csv's that take several minutes to load on my computer. I've been trying to locally run models, but it's getting to a point where I think I should run it on the cloud since there's a lot that I want to do.

I know relatively little about cloud computing, mostly been playing around with smaller datasets on my machine so far. I also have funding available that I want to take advantage of so cost isn't an issue here.

Which AWS instance (or other cloud computing service) would you recommend? Also, do you have any tips for quickly loading up big dataframes?",3,1,False,self,,,,,
1831,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,4,avh9ww,self.MachineLearning,Past years most notable work/research/solutions,https://www.reddit.com/r/MachineLearning/comments/avh9ww/past_years_most_notable_workresearchsolutions/,Stor_bjorn,1551295500,"Hey boys and girls,

I've been out of the loop for a while and maybe you could help me out. 
As a statistician/data scientist I get to and have to work with more classical algorithms. I'll be trying to get back into academia (will try to finish off my doctorates) this autumn and would like to catch up with whats new in deep learning.  Maybe you could post few papers on ""what is up"" with deep learning?

As I have hands on experience with DL, I am not looking on where to start, I am looking for something similar to DeepMinds paper on Starcraft II, but obviously less popular. Just to get some inspiration. 

If this is the wrong subreddit, my apologies, but it seemed like r/learnmachinelearning did not fit the agenda.",0,1,False,self,,,,,
1832,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,5,avhsdb,blog.insightdatascience.com,Why NYC is a Great Place to Break into AI  Insight Data,https://www.reddit.com/r/MachineLearning/comments/avhsdb/why_nyc_is_a_great_place_to_break_into_ai_insight/,hszafarek,1551298163,,0,2,False,default,,,,,
1833,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,5,avhte4,medium.com,From Faces to Kitties to Apartments: GAN Fakes the World,https://www.reddit.com/r/MachineLearning/comments/avhte4/from_faces_to_kitties_to_apartments_gan_fakes_the/,Yuqing7,1551298309,,0,1,False,https://b.thumbs.redditmedia.com/J1zdJp44j3-I5Hv-jDOIuC2C21TtVPaQohsWIWVvDlI.jpg,,,,,
1834,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,5,avhtkd,self.MachineLearning,[D] Tutorial to deploy a Keras model with TF Serving,https://www.reddit.com/r/MachineLearning/comments/avhtkd/d_tutorial_to_deploy_a_keras_model_with_tf_serving/,wronk17,1551298335,"At my company, we've been working to deploy our trained models at scale more efficiently. I was pretty new to this aspect of ML, so it took quite a bit of time to pull all the pieces together from different TF guides, stack overflow, etc. Afterwards, I wrote up a technical walkthrough just to document it all and get another Keras + TF Serving example out there for the community. Hopefully, this will help someone looking to run ML inference at scale (especially for those processing images).

&amp;#x200B;

[Article here](https://medium.com/devseed/technical-walkthrough-packaging-ml-models-for-inference-with-tf-serving-2a50f73ce6f8)

&amp;#x200B;

The code walkthrough includes how to:

1. Export a trained Keras model for containerized deployment
2. Package the exported model into a TF Serving Docker image
3. Send inference requests to and receive predictions from a deployed model",2,12,False,self,,,,,
1835,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,5,avi5lm,self.MachineLearning,3 reasons to add deep learning to your time series toolkit,https://www.reddit.com/r/MachineLearning/comments/avi5lm/3_reasons_to_add_deep_learning_to_your_time/,frlazzeri,1551300137,[removed],0,1,False,self,,,,,
1836,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,5,avi5un,self.MachineLearning,Deep learning based object classification model for Autonomous vehicles and Advanced Driver Assist System,https://www.reddit.com/r/MachineLearning/comments/avi5un/deep_learning_based_object_classification_model/,andrea_manero,1551300174,https://www.datasciencecentral.com/profiles/blogs/deep-learning-based-object-classification-model-for-autonomous,0,1,False,self,,,,,
1837,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,5,avi9qy,self.MachineLearning,[P] OpenWebText (GPT-2 WebText Dataset Scraper) - Update 1,https://www.reddit.com/r/MachineLearning/comments/avi9qy/p_openwebtext_gpt2_webtext_dataset_scraper_update/,joshuacpeterson,1551300756,"People seem to be using the code to scrape the dataset themselves fairly quickly, so we're focusing on developing the code base now so people can generate whatever variants of the dataset they want. We've included better URL filtering and global de-duplication, as well as post-scrape text extraction. A tokenizer will be added shortly. Please feel free to post github issues and propose improvements. Otherwise, happy scraping!

&amp;#x200B;

Github: [https://github.com/jcpeterson/openwebtext](https://github.com/jcpeterson/openwebtext)",22,47,False,self,,,,,
1838,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,5,avicbc,self.MachineLearning,Building tensorflow from source [help],https://www.reddit.com/r/MachineLearning/comments/avicbc/building_tensorflow_from_source_help/,Zayba,1551301133,[removed],0,1,False,self,,,,,
1839,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,6,avicv8,ai.stanford.edu,[R] Beyond Local Pattern Matching: Recent Advances in Machine Reading,https://www.reddit.com/r/MachineLearning/comments/avicv8/r_beyond_local_pattern_matching_recent_advances/,regalalgorithm,1551301212,,0,2,False,default,,,,,
1840,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,6,avilyh,self.MachineLearning,[Discussion] CNN that classifies whether or not an object is in a picture.,https://www.reddit.com/r/MachineLearning/comments/avilyh/discussion_cnn_that_classifies_whether_or_not_an/,Radzinsky99,1551302525,"Hi, I would like to create a convolutional neural network using keras from TensorFlow and Python. 

The neural network is supposed to receive an image as input and output true if an object I trained the network to is in the  picture, but I'm having some doubts about the training data and I hope you guys can give me some hints.

&amp;#x200B;

For example, let's suppose I want to know if there is a dog or a cat in a picture. I need lots of image containing a dog and lots of image containing a cat. 

Now let's suppose I want to know whether a dog is in a picture or not. I need lots of pictures containing a dog and what? What pictures should I have to classify the image as ""not containing a dog""? I tried searching on Google but nothing. Thanks for your help :)",8,0,False,self,,,,,
1841,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,6,aviw4k,stats.stackexchange.com,Why is the objective of Multi-Armed Bandits (MAB) not the same as the one for Reinforcement Learning (RL)?,https://www.reddit.com/r/MachineLearning/comments/aviw4k/why_is_the_objective_of_multiarmed_bandits_mab/,real_pinocchio,1551304036,,0,1,False,default,,,,,
1842,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,7,avj4g3,self.MachineLearning,[D] State of Voice Cloning - How realistic is it in 2019?,https://www.reddit.com/r/MachineLearning/comments/avj4g3/d_state_of_voice_cloning_how_realistic_is_it_in/,zeutron,1551305302,"I am exploring my options for voice cloning. Voice A which needs to be cloned comes from an American Male in his 30s and I have hours of audio narrations available for training from this voice.

What I am looking for is to record myself, Voice B, (also American male in 30s) and then convert my Voice B into Voice A.

I have heard some projects that do this, but there seems to be a significant amount of background noise after cloning voices. Is it possible to make this ultra realistic in 2019?

My only other option would be to hire someone who is good at voice impressions.

Thank you for any help!",5,1,False,self,,,,,
1843,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,7,avj9n8,self.MachineLearning,[Discussion] Standard guidelines or practices for projects using tensorflow,https://www.reddit.com/r/MachineLearning/comments/avj9n8/discussion_standard_guidelines_or_practices_for/,pimp4robots,1551306090,"As the title says, I am looking for some standard guidelines for projects involving Tensorflow. Mainly from perspective of coding. I found this link https://www.tensorflow.org/community/style_guide is there something besides this??",1,6,False,self,,,,,
1844,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,7,avjkcv,self.MachineLearning,"ML for Business People, Data Scientists please help",https://www.reddit.com/r/MachineLearning/comments/avjkcv/ml_for_business_people_data_scientists_please_help/,mikebayou,1551307727,[removed],0,1,False,self,,,,,
1845,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,8,avjoml,self.MachineLearning,"[D] ML for Business People, Data Scientists please provide input",https://www.reddit.com/r/MachineLearning/comments/avjoml/d_ml_for_business_people_data_scientists_please/,mikebayou,1551308417,"Dear Data Scientists,

&amp;#x200B;

I am in the process of designing a ML course for Business Process Consultants for a specific ERP software package.

&amp;#x200B;

The target audience has 

&amp;#x200B;

\- minimal to no background in mathematics, statistics and data science. 

\- very good understanding of how to automate business processes with software of the non ML kind (typically: user input -&gt; sql -&gt; business logic -&gt; output)

\- In general, very good understanding of how to ask the right business questions in order to build/install applications for business process automation

&amp;#x200B;

They are not software engineers and few of them can write code, but they are able to specify the requirements so that a software engineer can pick them up and build an application.

&amp;#x200B;

As Business Process Consultants they are dealing with business problems on a daily basis. The idea is to enable them to identify problems which are a good fit for ML and write a

proposal or requirements specification for a data scientist.

&amp;#x200B;

My question to the Data Scientists:

&amp;#x200B;

What would you expect a good proposal or requirements specification to contain in order for you to get a good initial understanding of the problem at hand?

&amp;#x200B;

e.g.

&amp;#x200B;

\- general formulation of the business problem

\- why do we need a solution/automation

\- is this a high volume task?

\- how is the task solved today?

\- what kind of data is available for training?

\- how can we measure success, do we have examples with the right labels for this success criterion?

...

&amp;#x200B;

Your input is highly appreciated.

&amp;#x200B;

Thanks

&amp;#x200B;

Michael",2,0,False,self,,,,,
1846,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,8,avkblb,self.MachineLearning,Analysis of dataset with epsilon-greedy algorithm,https://www.reddit.com/r/MachineLearning/comments/avkblb/analysis_of_dataset_with_epsilongreedy_algorithm/,bigfuds,1551311939,[removed],0,1,False,self,,,,,
1847,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,9,avkcv2,self.MachineLearning,[D] How to feed a sequence of images to a 2D CNN? (Keras),https://www.reddit.com/r/MachineLearning/comments/avkcv2/d_how_to_feed_a_sequence_of_images_to_a_2d_cnn/,MuffinBomber,1551312141,"Let's assume I have a 2D CNN:
    
    2d_in = (Input(shape=(224,224,3), name='2d_input'))
    model = densenet.DenseNet121(include_top=False, weights='imagenet', input_tensor=2d_in, input_shape=(224,224,3))

I want to feed it 10 images and then average out their features, so at the end I use a GlobalAveragePooling2D layer like this:

    x = GlobalAveragePooling2D(name='avg_pool_2d')(model.output)

How do I feed it an input that has a shape of (?, 10, 224, 224, 3) (where ? is the batch size)?

I read about TimeDistributed layer wrapper, but I'm not sure how to apply it just to the input layer of my 2D CNN.",9,2,False,self,,,,,
1848,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,10,avkycz,self.MachineLearning,"[R] Discriminative Improvements to Distributional Sentence Similarity. Introduces weighting scheme TF-KLD, includes the term frequency and the KL-divergence. 2013 paper, but what I found interesting is that it's implementation reached SoTA in The Microsoft Research Paraphrase Corpus",https://www.reddit.com/r/MachineLearning/comments/avkycz/r_discriminative_improvements_to_distributional/,BatmantoshReturns,1551315785,"Paper

http://www.aclweb.org/anthology/D/D13/D13-1090.pdf

It's performance in MRPC

https://paperswithcode.com/sota/semantic-textual-similarity-senteval

http://nlpprogress.com/english/semantic_textual_similarity.html

Very interesting, considering the approach seems relatively simple. ",0,2,False,self,,,,,
1849,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,10,avl4s3,self.MachineLearning,Can you use machine learning to predict the most traveled roads in the US?,https://www.reddit.com/r/MachineLearning/comments/avl4s3/can_you_use_machine_learning_to_predict_the_most/,rawrtherapy,1551316926,[removed],0,1,False,self,,,,,
1850,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,10,avlbo9,self.MachineLearning,What value do i* have for outliers in SVM non-separable case?,https://www.reddit.com/r/MachineLearning/comments/avlbo9/what_value_do_i_have_for_outliers_in_svm/,nightcrawler97,1551318146,[removed],0,1,False,self,,,,,
1851,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,10,avlgvv,self.MachineLearning,ML Model basic,https://www.reddit.com/r/MachineLearning/comments/avlgvv/ml_model_basic/,Protocol_210,1551319090,[removed],0,1,False,self,,,,,
1852,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,11,avlltn,github.com,"Releasing Conditional Density Estimation Python Package with Mixture Density Network, Kernel Mixture Network, various parametric/semi-parametric estimators, data simulators and evaluation functions)",https://www.reddit.com/r/MachineLearning/comments/avlltn/releasing_conditional_density_estimation_python/,whiletrue2,1551319961,,0,1,False,default,,,,,
1853,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,12,avm377,self.MachineLearning,Fully remote jobs in computer vision / deep learning research,https://www.reddit.com/r/MachineLearning/comments/avm377/fully_remote_jobs_in_computer_vision_deep/,generating_loop,1551323117,[removed],0,1,False,self,,,,,
1854,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,12,avm8vy,self.MachineLearning,Can machine learning algorithms get PTSD?,https://www.reddit.com/r/MachineLearning/comments/avm8vy/can_machine_learning_algorithms_get_ptsd/,PatternPerson,1551324116,"As in, if it were to receive a series of powerful distinguished observations, can that force the optimization algorithm into a local optimum and very hard to recover from?",0,1,False,self,,,,,
1855,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,12,avmi0o,self.MachineLearning,[Video] How to Code Your Own Image Classification NN in Tensorflow (MNIST),https://www.reddit.com/r/MachineLearning/comments/avmi0o/video_how_to_code_your_own_image_classification/,shawnmanuel000,1551325727,[removed],0,1,False,self,,,,,
1856,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,13,avmnha,self.MachineLearning,Deploy ML Models into Kubernetes Production in Under 2 Minutes,https://www.reddit.com/r/MachineLearning/comments/avmnha/deploy_ml_models_into_kubernetes_production_in/,avin_regmi,1551326699,"I've always had a hard time deploying ML models into production. The traditional approach is to use Flask, Gunicorn with Nginx. This requires a lot of setup time. Also, inferring model with flask is slow and requires custom code for caching and batching. Scaling in multiple machines is also hard. We have created panini.ai as a solution.

[https://www.panini.ai/](https://www.panini.ai/) is a platform to serve ML/DL models at low latency and makes it super easy to deploy AI models in the cloud. Once, deployed in the cloud it will provide you with an API key to infer the model. Our backend is written in C++, which provides very low latency during model inference and the model is stored in Kubernetes so, it is scalable to multiple nodes. We take care of caching and batching inputs during model inference.

I have also created a YouTube tutorial on how to use panini: [https://www.youtube.com/watch?v=tCz-fi\_NheE&amp;t=](https://www.youtube.com/watch?v=tCz-fi_NheE&amp;t=)

Please let me know what you guys think. If you have any questions, please email me at [avin@panini.ai](mailto:avin@panini.ai)

&amp;#x200B;

![img](spgzwpnwg8j21)

&amp;#x200B;",0,1,False,self,,,,,
1857,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,13,avmrzy,self.MachineLearning,[D] Deploy ML Models into Kubernetes Production in Under 2 Minutes,https://www.reddit.com/r/MachineLearning/comments/avmrzy/d_deploy_ml_models_into_kubernetes_production_in/,avin_regmi,1551327547," I've always had a hard time deploying ML models into production. The traditional approach is to use Flask, Gunicorn with Nginx. This requires a lot of setup time. Also, inferring model with flask is slow and requires custom code for caching and batching. Scaling in multiple machines is also hard. We have created [panini.ai](https://panini.ai) as a solution.

[https://www.panini.ai/](https://www.panini.ai/) is a platform to serve ML/DL models at low latency and makes it super easy to deploy AI models in the cloud. Once, deployed in the cloud it will provide you with an API key to infer the model. Our backend is written in C++, which provides very low latency during model inference and the model is stored in Kubernetes so, it is scalable to multiple nodes. We take care of caching and batching inputs during model inference.

I have also created a YouTube tutorial on how to use panini: [https://www.youtube.com/watch?v=tCz-fi\_NheE&amp;t=](https://www.youtube.com/watch?v=tCz-fi_NheE&amp;t=)

Please let me know what you guys think. If you have any questions, please email me at [avin@panini.ai](mailto:avin@panini.ai)

What are you guys, currently using to deploy models into production? 

&amp;#x200B;

https://i.redd.it/tpjl7tvsh8j21.png

&amp;#x200B;",4,6,False,https://b.thumbs.redditmedia.com/8Epe3d2tDJbfiD1H8pcJdDKM8JPOi9DB_IQRm3I5WvI.jpg,,,,,
1858,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,13,avmyka,self.MachineLearning,How to make neural networks performances stable?,https://www.reddit.com/r/MachineLearning/comments/avmyka/how_to_make_neural_networks_performances_stable/,speedcell4,1551328847,[removed],0,1,False,self,,,,,
1859,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,14,avn8oa,self.MachineLearning,Looking for mood tracking data,https://www.reddit.com/r/MachineLearning/comments/avn8oa/looking_for_mood_tracking_data/,neoyipeng2014,1551330834,[removed],0,1,False,self,,,,,
1860,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,14,avncd1,lukeoakdenrayner.wordpress.com,Half a million x-rays! First impressions of the Stanford and MIT chest x-ray datasets,https://www.reddit.com/r/MachineLearning/comments/avncd1/half_a_million_xrays_first_impressions_of_the/,dattran2346,1551331578,,0,1,False,default,,,,,
1861,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,14,avnfli,self.MachineLearning,Hyper-parameter tuning package using tree search in the same format as sklearn grid search,https://www.reddit.com/r/MachineLearning/comments/avnfli/hyperparameter_tuning_package_using_tree_search/,Iamabandit,1551332215,[removed],0,1,False,self,,,,,
1862,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,15,avnssn,self.MachineLearning,"In-flight Broadband Market size, Trend, share, analysis and forecast 2019-2023",https://www.reddit.com/r/MachineLearning/comments/avnssn/inflight_broadband_market_size_trend_share/,apple_x9,1551334990,[removed],1,1,False,self,,,,,
1863,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,15,avny5j,compakk.blogspot.com,Shrink Wrapping Machine by Compak for Business and Industrial Purpose,https://www.reddit.com/r/MachineLearning/comments/avny5j/shrink_wrapping_machine_by_compak_for_business/,compak03,1551336080,,0,1,False,https://b.thumbs.redditmedia.com/YArPCsFB8cGImgJe9hwpzJFA2qeallRnmv9ffov4YrI.jpg,,,,,
1864,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,16,avoec8,self.MachineLearning,First machine learning pc build,https://www.reddit.com/r/MachineLearning/comments/avoec8/first_machine_learning_pc_build/,tcpsyc,1551339442,[removed],0,1,False,self,,,,,
1865,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,16,avof51,self.MachineLearning,Cutting Speed of CNC Sheet Metal Shear,https://www.reddit.com/r/MachineLearning/comments/avof51/cutting_speed_of_cnc_sheet_metal_shear/,CNCPressBrakeChina,1551339624,[removed],0,1,False,self,,,,,
1866,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,16,avog5g,self.MachineLearning,Question about Video Pixel Networks from Deep Mind,https://www.reddit.com/r/MachineLearning/comments/avog5g/question_about_video_pixel_networks_from_deep_mind/,Pharrell_WANG,1551339860,[removed],0,1,False,self,,,,,
1867,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,17,avokij,scpressbrake.com,[D]Plate Rolling Machine Design and Production - Metalforming,https://www.reddit.com/r/MachineLearning/comments/avokij/dplate_rolling_machine_design_and_production/,CNCPressBrakeChina,1551340871,,0,1,False,default,,,,,
1868,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,17,avon4z,arxiv.org,[R] Latent Translation: Crossing Modalities by Bridging Generative Models,https://www.reddit.com/r/MachineLearning/comments/avon4z/r_latent_translation_crossing_modalities_by/,i-like-big-gans,1551341465,,11,31,False,default,,,,,
1869,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,17,avondz,self.MachineLearning,What is the effect of cutting speed on hydraulic guillotine shears?,https://www.reddit.com/r/MachineLearning/comments/avondz/what_is_the_effect_of_cutting_speed_on_hydraulic/,CNCPressBrakeChina,1551341523,[removed],0,1,False,self,,,,,
1870,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,17,avowuh,self.MachineLearning,Face Recognition Research paper.,https://www.reddit.com/r/MachineLearning/comments/avowuh/face_recognition_research_paper/,lit_gupta,1551343787,[removed],0,1,False,self,,,,,
1871,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,17,avoyar,self.MachineLearning,Generic code for data science competitions,https://www.reddit.com/r/MachineLearning/comments/avoyar/generic_code_for_data_science_competitions/,metesarang,1551344162,I am new to Data Science. Can someone please share generic code(or link) to start with data science competitions like on Kaggle? ,0,1,False,self,,,,,
1872,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,18,avp3pb,medium.com,Mental Disorder Analysis with Elasticsearch and Kibana (Phase 1),https://www.reddit.com/r/MachineLearning/comments/avp3pb/mental_disorder_analysis_with_elasticsearch_and/,omarsar,1551345417,,0,1,False,default,,,,,
1873,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,18,avp6ug,linkedin.com,What is Augmented Data Science and Why is it Important to My Business?,https://www.reddit.com/r/MachineLearning/comments/avp6ug/what_is_augmented_data_science_and_why_is_it/,ElegantMicroWebIndia,1551346149,,0,1,False,default,,,,,
1874,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,19,avpgy1,self.MachineLearning,Any mila admits for fall 2019?,https://www.reddit.com/r/MachineLearning/comments/avpgy1/any_mila_admits_for_fall_2019/,charared,1551348456,[removed],0,1,False,self,,,,,
1875,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,19,avpids,medium.com,Machine learning: transforming industries or transforming the future?,https://www.reddit.com/r/MachineLearning/comments/avpids/machine_learning_transforming_industries_or/,Celadon_soft,1551348778,,0,1,False,default,,,,,
1876,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,19,avpjqn,appliedai.com,[D] Over 100 Real-world AI Use-cases,https://www.reddit.com/r/MachineLearning/comments/avpjqn/d_over_100_realworld_ai_usecases/,FrederikBussler,1551349068,,0,1,False,default,,,,,
1877,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,19,avpub7,self.MachineLearning,[R] Deep Robot Localization on a CPU,https://www.reddit.com/r/MachineLearning/comments/avpub7/r_deep_robot_localization_on_a_cpu/,geshuni,1551351452,[removed],0,1,False,self,,,,,
1878,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,20,avpw5e,self.MachineLearning,Machine Learning Made For .NET (ML.NET),https://www.reddit.com/r/MachineLearning/comments/avpw5e/machine_learning_made_for_net_mlnet/,concettolab,1551351874,[removed],0,1,False,self,,,,,
1879,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,20,avq2if,youtu.be,Augmented Reality 3D Food Menu | Quytech,https://www.reddit.com/r/MachineLearning/comments/avq2if/augmented_reality_3d_food_menu_quytech/,hiwilliam31,1551353319,,0,1,False,default,,,,,
1880,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,20,avq2rd,github.com,"[P] Implementations of 7 research papers on Seq2Seq learning using Pytorch (Sketch generation, handwriting synthesis, variational autoencoders, machine translation, etc.)",https://www.reddit.com/r/MachineLearning/comments/avq2rd/p_implementations_of_7_research_papers_on_seq2seq/,bhatt_gaurav,1551353370,,0,1,False,https://b.thumbs.redditmedia.com/bwQFY9RcH9xIsPLqaFRRYxwb7ygUTlMrHd1KBT09xaA.jpg,,,,,
1881,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,20,avq4ij,github.com,"[P] Implementations of 7 research papers on Seq2Seq learning using Pytorch (Sketch generation, handwriting synthesis, variational autoencoders, machine translation, etc.)",https://www.reddit.com/r/MachineLearning/comments/avq4ij/p_implementations_of_7_research_papers_on_seq2seq/,bhatt_gaurav,1551353749,,0,1,False,https://b.thumbs.redditmedia.com/bwQFY9RcH9xIsPLqaFRRYxwb7ygUTlMrHd1KBT09xaA.jpg,,,,,
1882,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,20,avq80b,self.MachineLearning,"[P] Implementations of 7 research papers on Deep Seq2Seq learning using Pytorch (Sketch generation, handwriting synthesis, variational autoencoders, machine translation, etc.)",https://www.reddit.com/r/MachineLearning/comments/avq80b/p_implementations_of_7_research_papers_on_deep/,bhatt_gaurav,1551354505,"Github repo - [https://github.com/GauravBh1010tt/DL-Seq2Seq](https://github.com/GauravBh1010tt/DL-Seq2Seq)

&amp;#x200B;

Reproducible Pytorch code on Deep Seq2seq learning for the following papers: 

* Sketch Generation - [A Neural Representation of Sketch Drawings](https://openreview.net/pdf?id=Hy6GHpkCW)
* Machine translation - [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)
* Handwriting synthesis - [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850.pdf)
* Variational Autoencoders (VAE) - [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)
* Scheduled Sampling - [Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks](https://arxiv.org/pdf/1506.03099.pdf)
* Conditional VAE - [Learning Structured Output Representation using Deep Conditional Generative Models](https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf)
* Mixture Density Networks - [Mixture Density Networks](https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf)

Let me know if you have any suggestions or comments.",21,226,False,self,,,,,
1883,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,21,avqodp,towardsdatascience.com,Speaker Diarization and X-Vectors with Kaldi - A tutorial,https://www.reddit.com/r/MachineLearning/comments/avqodp/speaker_diarization_and_xvectors_with_kaldi_a/,lordrandom12,1551357591,,0,1,False,default,,,,,
1884,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,21,avqpbb,self.MachineLearning,Pose Detection comparison : wrnchAI vs OpenPose,https://www.reddit.com/r/MachineLearning/comments/avqpbb/pose_detection_comparison_wrnchai_vs_openpose/,spmallick,1551357760,"Today's post is for all the geeks who are interested in Human Pose Estimation!

LearnOpenCV compares two really good Human Pose Estimation models -- OpenPose vs wrnchAI.

[https://www.learnopencv.com/pose-detection-comparison-wrnchai-vs-openpose/](https://www.learnopencv.com/pose-detection-comparison-wrnchai-vs-openpose/)

wrnch is backed by Mark Cuban and has been featured in keynote addresses by NVIDIA and Intel.

Three important differences to note:  
1. wrnchAI and OpenPose are similar in accuracy, but wrnchAI is blazingly fast.  
2. OpenPose license prevents use in sports applications, but wrnchAI has no such restrictions.  
3. OpenPose is opensource ( even though you have to pay a licensing fee for commercial use ).

Disclosure: We received a fee from wrnch for producing a report that compared wrnchAI to OpenPose. The report was independently produced without interference or oversight by wrnch.

![video](5ovzho4f1bj21)

Do like, comment your opinions and tag your friends to get a debate going!

[\#AI](https://www.facebook.com/hashtag/ai?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R) [\#MachineLearning](https://www.facebook.com/hashtag/machinelearning?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R) [\#DeepLearning](https://www.facebook.com/hashtag/deeplearning?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R) [\#ComputerVision](https://www.facebook.com/hashtag/computervision?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R) [\#wrnchAI](https://www.facebook.com/hashtag/wrnchai?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R)[\#OpenPose](https://www.facebook.com/hashtag/openpose?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R) [\#PoseEstimation](https://www.facebook.com/hashtag/poseestimation?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R) [\#PoseDetection](https://www.facebook.com/hashtag/posedetection?source=feed_text&amp;epa=HASHTAG&amp;__xts__%5B0%5D=68.ARCOyiFEDJ_jf8WpyvIKuKuyiQAY75lTb5gTjvkTAIlIPa67hMcWPxWnoRVpphlG8zwQGvEhXW3teR1RAiGf8sPmVJPC-t217tTwqWAQXUtRW2lVDk8TQvEe29IZvYhgWM2HfnF9xW0zc1wrOtkuPE-nyj_ZLxbaFiW8KgX6A4NN4dWpnWjsYtQvZoSiyBvpW5ncTXRSpfopR18sQqbFnibmT1SvKxhGWEa_3kUo8zCmzunLlqZ7Iw6h-Iku9j0AXc80vS_XVeLtvYnrmZoz6nAmaLBdl_Ld35qMmxw5Hp09nOHVsgACgCOaj0YMjyCVSfqi2Q_8-Uf19AhdFMVz9uwD07dNdFk8TyI7&amp;__tn__=%2ANK-R)",0,1,False,self,,,,,
1885,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,21,avqpo0,self.MachineLearning,10 Easy Yet Effective Rules of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/avqpo0/10_easy_yet_effective_rules_of_machine_learning/,imarticus_nirmal,1551357826,[removed],0,1,False,self,,,,,
1886,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,21,avqscd,self.MachineLearning,[R] Mr. GAN: Don't Use Reconstruction Loss in cGANs! (ICLR 2019),https://www.reddit.com/r/MachineLearning/comments/avqscd/r_mr_gan_dont_use_reconstruction_loss_in_cgans/,soochan,1551358319,"&amp;#x200B;

[Pix2Pix on Cityscapes dataset with different loss functions](https://i.redd.it/g2jegqkiyaj21.jpg)

This is our paper ""Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation"" accepted at ICLR 2019.

We reveal that **reconstruction loss is a major cause of mode collapse** in conditional GANs. As alternatives, we propose novel loss functions called **Moment Reconstruction (MR) losses**. We believe that our work has a significant impact on the numerous areas where cGAN is used.

&amp;#x200B;

**Project:** [https://soochanlee.com/publications/mr-gan](https://soochanlee.com/publications/mr-gan)

**Paper:** [https://arxiv.org/abs/1902.09225](https://arxiv.org/abs/1902.09225)

**PyTorch Implementation:** [https://github.com/soochan-lee/MR-GAN](https://github.com/soochan-lee/MR-GAN)

&amp;#x200B;

**Abstract:**

Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named *moment reconstruction losses* that simply replace the reconstruction loss. We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset. Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.",9,56,False,https://b.thumbs.redditmedia.com/q-bBnt3Z_8eNnBbDXTVkWl3q4pZvvJCz2pEg228GxjE.jpg,,,,,
1887,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,22,avr2bn,self.MachineLearning,[R] Very insightful idea on how to improve parameter efficiency on CNN by using shared weights,https://www.reddit.com/r/MachineLearning/comments/avr2bn/r_very_insightful_idea_on_how_to_improve/,hugosilva664,1551360087,"https://arxiv.org/abs/1902.09701

This paper was a very interesting read that Id like to share with the community.",9,7,False,self,,,,,
1888,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,22,avr2h4,self.MachineLearning,How to setup Docker and Nvidia-Docker 2.0 on Ubuntu 18.04,https://www.reddit.com/r/MachineLearning/comments/avr2h4/how_to_setup_docker_and_nvidiadocker_20_on_ubuntu/,Mayalittlepony,1551360114,"If you're a data scientist working on deep learning applications or on any computation that can benefit from GPUs - you'll probably need Docker and Navidia-Docker.   


Here's a step-by-step guide on how to setup Docker on Ubuntu: [https://cnvrg.io/how-to-setup-docker-and-nvidia-docker-2-0-on-ubuntu-18-04/](https://cnvrg.io/how-to-setup-docker-and-nvidia-docker-2-0-on-ubuntu-18-04/)   


Hope this is helpful!",0,1,False,self,,,,,
1889,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,22,avraw9,tomasalabes.me,[Post] What is Scikit? Advantages and disadvantages,https://www.reddit.com/r/MachineLearning/comments/avraw9/post_what_is_scikit_advantages_and_disadvantages/,tomasAlabes,1551361599,,0,1,False,default,,,,,
1890,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,23,avrh0b,self.MachineLearning,"[P] One neural network, many uses: build a multi-task neural network [code in PyTorch + tutorial]",https://www.reddit.com/r/MachineLearning/comments/avrh0b/p_one_neural_network_many_uses_build_a_multitask/,invertedpassion,1551362646,"Hello everyone,

I wrote a tutorial on multi-task learning where one model powers multiple functions: image caption, image search, word similarity and image similarity. **Check out the tutorial**: [one network, many uses](https://towardsdatascience.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d).

Here's the [code repository](https://github.com/paraschopra/one-network-many-uses). It's written in PyTorch. 

# Some samples

**For image captioning**

&amp;#x200B;

https://i.redd.it/dh8est9afbj21.png

**For word similarity**

&amp;#x200B;

https://i.redd.it/7t0i1sljfbj21.png

**For visual similarity**

&amp;#x200B;

https://i.redd.it/8g9qjdsnfbj21.png

**For image search**

&amp;#x200B;

https://i.redd.it/o6higlkrfbj21.png

&amp;#x200B;

https://i.redd.it/lsdkguywfbj21.png

Hope you like the tutorial. [Here's the link again](https://towardsdatascience.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d).",0,0,False,https://b.thumbs.redditmedia.com/b9OVEAKA2g4DsH_Fme1fjHJ3DC5eLRWBJ2cvoZCSRQA.jpg,,,,,
1891,MachineLearning,t5_2r3gv,2019-2-28,2019,2,28,23,avrnw7,statisticseco.com,Machine learning is one of the forms of artificial intelligence that enables the computers to learn without being explicitly programmed...,https://www.reddit.com/r/MachineLearning/comments/avrnw7/machine_learning_is_one_of_the_forms_of/,Statistics_Expert,1551363814,,0,1,False,default,,,,,
