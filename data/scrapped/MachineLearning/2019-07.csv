,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2019-7-1,2019,7,1,9,c7lp0g,[R] Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond,https://www.reddit.com/r/MachineLearning/comments/c7lp0g/r_nearoptimal_methods_for_minimizing_starconvex/,nimit_s,1561940446,,1,7
1,2019-7-1,2019,7,1,9,c7luob,Why do Neural Networks Need an Activation Function?,https://www.reddit.com/r/MachineLearning/comments/c7luob/why_do_neural_networks_need_an_activation_function/,strikingLoo,1561941399,,0,1
2,2019-7-1,2019,7,1,10,c7mae0,"[D] What did they teach you in school that you didnt need, and what did you need for your job that they didnt teach you in school?",https://www.reddit.com/r/MachineLearning/comments/c7mae0/d_what_did_they_teach_you_in_school_that_you/,alexhuhcya,1561944113,,41,27
3,2019-7-1,2019,7,1,10,c7mfml,Decomposable Neural Paraphrase Generation,https://www.reddit.com/r/MachineLearning/comments/c7mfml/decomposable_neural_paraphrase_generation/,I_ai_AI,1561945009,,2,4
4,2019-7-1,2019,7,1,10,c7mkxe,A2C paper/ how to train actors in n-step rewards?,https://www.reddit.com/r/MachineLearning/comments/c7mkxe/a2c_paper_how_to_train_actors_in_nstep_rewards/,wongongv,1561945903,[removed],0,1
5,2019-7-1,2019,7,1,13,c7o1g6,[R] Evolving Neural Turing Machines for Reward-based Learning,https://www.reddit.com/r/MachineLearning/comments/c7o1g6/r_evolving_neural_turing_machines_for_rewardbased/,milaworld,1561954915,"**Abstract** An unsolved problem in neuroevolution (NE) is to evolve artificial neural networks (ANN) that can store and use information to change their behavior online. While plastic neural networks have shown promise in this context, they have difficulties retaining information over longer periods of time and integrating new information without losing previously acquired skills. Here we build on recent work by [Graves et al.](https://arxiv.org/abs/1410.5401) who extended the capabilities of an ANN by combining it with an external memory bank trained through gradient descent. In this [paper](http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdf), we introduce an evolvable version of their Neural Turing Machine (NTM) and show that such an approach greatly simplifies the neural model, generalizes better, and does not require accessing the entire memory content at each time-step. The Evolvable Neural Turing Machine (ENTM) is able to solve a simple copy tasks and for the first time, the continuous version of the double T-Maze, a complex reinforcement-like learning problem. In the T-Maze learning task the agent uses the memory bank to display adaptive behavior that normally requires a plastic ANN, thereby suggesting a complementary and effective mechanism for adaptive behavior in NE.

[pdf](http://sebastianrisi.com/wp-content/uploads/greve_gecco16.pdf)

https://dl.acm.org/citation.cfm?id=2908930",3,18
6,2019-7-1,2019,7,1,13,c7o6o1,[1906.08857] Deep Neuroevolution of Recurrent and Discrete World Models (GECCO 2019),https://www.reddit.com/r/MachineLearning/comments/c7o6o1/190608857_deep_neuroevolution_of_recurrent_and/,hardmaru,1561955882,,0,1
7,2019-7-1,2019,7,1,13,c7o6td,[R] Deep Neuroevolution of Recurrent and Discrete World Models (GECCO 2019),https://www.reddit.com/r/MachineLearning/comments/c7o6td/r_deep_neuroevolution_of_recurrent_and_discrete/,hardmaru,1561955909,,4,21
8,2019-7-1,2019,7,1,13,c7oaod,[D] Any class recommendation for a student?,https://www.reddit.com/r/MachineLearning/comments/c7oaod/d_any_class_recommendation_for_a_student/,inexistentme,1561956621,"Hi, I am currently a master student at GaTech. I was a game developer after I graduate from Pitt in Computer engineering. I am completely new to this realm. So, could you guys recommend some classes? There are just too many of them. And I only have eight classes left to finish.

Here is the course list. Classes that are in bold text are what I selected.

&amp;#x200B;

Algorithm(Pick one or more):

*  Computability, Algorithms, and Complexity
* Computational Complexity Theory
* Design and Analysis of Algorithms
* Graph Algorithms
* **Approximation Algorithms**
* Randomized Algorithms
*  Computational Science and Engineering Algorithms

Core(Pick one or more):

* **Machine Learning**
* Computational Data Analysis: Learning, Mining, and Computation

Elective(Pick three or more):

* Big Data Systems &amp; Analysis
* **Computer Vision**
* Markov Chain Monte Carlo
* Spectral Algorithms
* **Machine Learning Theory**
* Pattern Recognition
* Behavioral Imaging
* **Deep Learning**
* **Machine Learning for Trading**
* Natural Language
* Web Search and Text Mining
* Data and Visual Analytics
* Computational Statistics
* Bayesian Methods
* Stochastic Optimization

Special Topics(Pick Less than four):

* Interactive Robot Learning
* **ML with Limited Supervision**
* **Math Foundations Mach Learning**",11,3
9,2019-7-1,2019,7,1,13,c7obq4,Researchers Develop the First Deep Learning-Based 3D Simulation of the Universe,https://www.reddit.com/r/MachineLearning/comments/c7obq4/researchers_develop_the_first_deep_learningbased/,MLNerd2017,1561956826,[removed],0,1
10,2019-7-1,2019,7,1,14,c7ojf5,Search with image,https://www.reddit.com/r/MachineLearning/comments/c7ojf5/search_with_image/,ewelumokeke,1561958299,[removed],0,1
11,2019-7-1,2019,7,1,14,c7opln,I want to make this. Any repository ?,https://www.reddit.com/r/MachineLearning/comments/c7opln/i_want_to_make_this_any_repository/,logiklesuraj,1561959547,[removed],0,1
12,2019-7-1,2019,7,1,14,c7oprn,Difference between Autoencoder Representation and Model Embeddings,https://www.reddit.com/r/MachineLearning/comments/c7oprn/difference_between_autoencoder_representation_and/,PyWarrior,1561959581,[removed],0,1
13,2019-7-1,2019,7,1,15,c7ozhm,DeepNudes Download - Discord Community,https://www.reddit.com/r/MachineLearning/comments/c7ozhm/deepnudes_download_discord_community/,Deepnoobs,1561961524,[removed],0,1
14,2019-7-1,2019,7,1,15,c7p0po,What is difference about LSTM(512) and LSTM(256) + LSTM(256)?,https://www.reddit.com/r/MachineLearning/comments/c7p0po/what_is_difference_about_lstm512_and_lstm256/,GoBacksIn,1561961761," I think this is meaningfully the same. 

 However, in reality, the result value is different. 

&amp;#x200B;

So, I wonder What is difference of LSTM(512) and LSTM(256) + LSTM(256)?

&amp;#x200B;

&amp;#x200B;

 to be exact 

    model.add(LSTM(512))

and

    model.add(LSTM(256))
    model.add(LSTM(256))

I wonder what's different about this.",0,1
15,2019-7-1,2019,7,1,15,c7p27w,[N] MIT has developed a new drag and drop data exploration + machine learning tool called NorthStar,https://www.reddit.com/r/MachineLearning/comments/c7p27w/n_mit_has_developed_a_new_drag_and_drop_data/,wandering_tsilihin,1561962057,,41,434
16,2019-7-1,2019,7,1,15,c7p4k6,"Multi-class Image classification using CNN over PyTorch, and the basics of CNN",https://www.reddit.com/r/MachineLearning/comments/c7p4k6/multiclass_image_classification_using_cnn_over/,thevatsalsaglani,1561962544,,0,1
17,2019-7-1,2019,7,1,15,c7pai3,"Curated Lists of Data Science, Machine Learning, Deep Learning and NLP resources",https://www.reddit.com/r/MachineLearning/comments/c7pai3/curated_lists_of_data_science_machine_learning/,andrea_manero,1561963783,[removed],0,1
18,2019-7-1,2019,7,1,15,c7pb8z,Machine Learning Build (Semi-Budget),https://www.reddit.com/r/MachineLearning/comments/c7pb8z/machine_learning_build_semibudget/,King-Eli,1561963928,[removed],0,1
19,2019-7-1,2019,7,1,16,c7plsu,4 Machine Learning Algorithms Used in The Majority of ML Applications Today!,https://www.reddit.com/r/MachineLearning/comments/c7plsu/4_machine_learning_algorithms_used_in_the/,multisoftmva0,1561966110,[removed],0,1
20,2019-7-1,2019,7,1,17,c7q1q1,"How to formalize this problem for a ML solution ? ""closeness"" of local min/maxes of two variables",https://www.reddit.com/r/MachineLearning/comments/c7q1q1/how_to_formalize_this_problem_for_a_ml_solution/,GreedySpeculator,1561969759,"Hi, i am mostly self taught so apologies in advance if my formulation is different to what is commonly used among trained ML pros. 

I have two variables X(t) and Y(t) what I wanna know is whether  the local maxes of X(t) are close (in t) to the local max/mins of Y(t)

not sure how to best characterize this problem. my first (messy) attempt goes something like this  
1) characterize ""closeness"" to local min/max by making a Boolean function which returns True if Y(t) is epsilon close enough to (based on percentiles)

2) Probit model of this new Boolean Y(t) equivalent vs X'(t) where X' calculates the distances to the nearest (a parameter in this case) local min and X'' is the same but with local maxes

the model seems to have good stats but i'm not sure how to read them. very little exp with logistic regression.

i did all of this in R.

can give the data if there's any interest

what would be a better, more readable approach to this problem ? 

thx",0,1
21,2019-7-1,2019,7,1,17,c7q2k5,I transformed the dataset and the prediction algorithm isn't working well,https://www.reddit.com/r/MachineLearning/comments/c7q2k5/i_transformed_the_dataset_and_the_prediction/,DJP_arsenal,1561969963,[removed],0,1
22,2019-7-1,2019,7,1,19,c7qvdt,The -trick or the effectiveness of reweighted least-squares (Francis Bach),https://www.reddit.com/r/MachineLearning/comments/c7qvdt/the_trick_or_the_effectiveness_of_reweighted/,netw0rkf10w,1561976486,,0,1
23,2019-7-1,2019,7,1,19,c7qx2a,[D] The -trick or the effectiveness of reweighted least-squares (Francis Bach),https://www.reddit.com/r/MachineLearning/comments/c7qx2a/d_the_trick_or_the_effectiveness_of_reweighted/,netw0rkf10w,1561976851,,0,1
24,2019-7-1,2019,7,1,19,c7qzt6,An interesting ..,https://www.reddit.com/r/MachineLearning/comments/c7qzt6/an_interesting/,karenactionsitafaal,1561977437,,0,1
25,2019-7-1,2019,7,1,19,c7r2ex,Poker platform with API available,https://www.reddit.com/r/MachineLearning/comments/c7r2ex/poker_platform_with_api_available/,maximedb,1561978004,,0,1
26,2019-7-1,2019,7,1,19,c7r3fi,Where Machine Learning as a Service Works and Where not?,https://www.reddit.com/r/MachineLearning/comments/c7r3fi/where_machine_learning_as_a_service_works_and/,cogitotechllc,1561978216,,0,1
27,2019-7-1,2019,7,1,19,c7r454,Using NLP to extract data (Keywords) from Email and text documents,https://www.reddit.com/r/MachineLearning/comments/c7r454/using_nlp_to_extract_data_keywords_from_email_and/,nithishguptak,1561978366,[removed],0,1
28,2019-7-1,2019,7,1,20,c7roie,L1-Norm Batch Normalization for Efficient Training of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/c7roie/l1norm_batch_normalization_for_efficient_training/,promach,1561982317,,1,1
29,2019-7-1,2019,7,1,21,c7rsaa,[D] Is there an app that can choose pictures based on how good facial expressions are?,https://www.reddit.com/r/MachineLearning/comments/c7rsaa/d_is_there_an_app_that_can_choose_pictures_based/,mBosco,1561982982,"Hello people. I work with large Lightroom catalogs with hundreds of pictures that I have to sift through and was thinking of a way to automate this job to a certain degree.

So here comes the big question. Is there a way of training a model to select pictures based on facial expression? Basically selecting  the best pictures where people are smiling and looking their best. I could feed it a lot of examples of what I consider good pictures.  I imagine something this specific might not exist yet, but can someone point me to a good starting point for this?

&amp;#x200B;

Thank you!",6,2
30,2019-7-1,2019,7,1,21,c7s45h,Your input is needed. Academic research on usability issues of Big Data,https://www.reddit.com/r/MachineLearning/comments/c7s45h/your_input_is_needed_academic_research_on/,FuegoDentro,1561985031,[removed],0,1
31,2019-7-1,2019,7,1,21,c7s5ks,Understanding math behind SVM,https://www.reddit.com/r/MachineLearning/comments/c7s5ks/understanding_math_behind_svm/,ItsNotEasyForMe,1561985271,[removed],0,1
32,2019-7-1,2019,7,1,21,c7s68x,[N] Introducing: the NeurIPS 2019 Disentanglement Challenge,https://www.reddit.com/r/MachineLearning/comments/c7s68x/n_introducing_the_neurips_2019_disentanglement/,nasimrahaman,1561985385,,0,1
33,2019-7-1,2019,7,1,22,c7safd,Student-Teacher maching problem,https://www.reddit.com/r/MachineLearning/comments/c7safd/studentteacher_maching_problem/,ChrisNg__,1561986086,[removed],0,1
34,2019-7-1,2019,7,1,22,c7sp9k,Machine Learning Bootcamp: Become a ML Engineer in 6 months. Job Guaranteed.,https://www.reddit.com/r/MachineLearning/comments/c7sp9k/machine_learning_bootcamp_become_a_ml_engineer_in/,HannahHumphreys,1561988427,[removed],0,1
35,2019-7-1,2019,7,1,23,c7tct4,[D] Reinforcement learning with combined continuous and discrete action space?,https://www.reddit.com/r/MachineLearning/comments/c7tct4/d_reinforcement_learning_with_combined_continuous/,timakro,1561991934,"Hi, I'm working on a reinforcement learning project to teach an AI to play a video game. Specifically I'm implementing A2C. The RL literature has many examples of either continuous or discrete action spaces but many video games have both types of inputs e.g. mouse position and keyboard input.

&amp;#x200B;

In my specific scenario I have continuous mouse input: X and Y coordinate. A set of 5 weapons the agent chooses 1 from. And 5 buttons the agent can press (for jumping, shooting, etc.) which can be pressed simultaneously.

&amp;#x200B;

Can I have multiple output heads for the different types of actions and take the mean of the losses? Will it converge?

&amp;#x200B;

Specifically there would be two heads with two nodes each for mouse position mean and variance. Additionally one head with 5 nodes for the weapon selection with softmax activation. And another head with 5 nodes for the ""button actions"" with sigmoid activation.

&amp;#x200B;

Ofcourse you need different loss and entropy functions for the continuous mouse position and the discrete weapon selection. I don't know how to calculate loss and entropy of my ""button actions"", but that's another question: [https://www.reddit.com/r/MachineLearning/comments/9z8tok/d\_reinforcement\_learning\_with\_multiple/](https://www.reddit.com/r/MachineLearning/comments/9z8tok/d_reinforcement_learning_with_multiple/)

&amp;#x200B;

So if I have the loss and entropy for each of mouse position, weapon selection and ""button actions"". Should I average all the losses and all the entropies and use them in my final loss function? Should I weight them in some way?",9,5
36,2019-7-1,2019,7,1,23,c7tit1,Can this Q Table get faster?,https://www.reddit.com/r/MachineLearning/comments/c7tit1/can_this_q_table_get_faster/,KironDevCoder,1561992778,"I have made a simple game which I am gonna write an Q Learning AI for. But it's just that the Q Table takes A LOT of memory, an 8x8 grid will make it need a Q Table with a size of 562500 with each part also needing space for 6 actions. Since this requires a lot of memory I was wondering if I could make it faster. It's all completely written in Python 3.

&amp;#x200B;

The AI:  [https://hastebin.com/joradejufa.py](https://hastebin.com/joradejufa.py) 

The game:  [https://hastebin.com/zumigapiki.rb](https://hastebin.com/zumigapiki.rb) 

&amp;#x200B;

Please let me know if it can get faster.",0,1
37,2019-7-2,2019,7,2,0,c7tqrt,[P] Heuristical keyword extraction from documents and encoding for training GPT-2 to generate texts based on user-specified keywords (+ parallelized spaCy),https://www.reddit.com/r/MachineLearning/comments/c7tqrt/p_heuristical_keyword_extraction_from_documents/,minimaxir,1561993875,"[https://github.com/minimaxir/gpt-2-keyword-generation](https://github.com/minimaxir/gpt-2-keyword-generation)

&amp;#x200B;

[A couple weeks ago](https://www.reddit.com/r/MachineLearning/comments/c2wxva/p_app_to_make_aigenerated_submission_titles_for/) I posted a [Reddit title generator app](https://minimaxir.com/apps/gpt2-reddit/) based on GPT-2 to this subreddit which allows the user to generate Reddit titles based on a subreddit and also allows the user to specify the keywords to condition the title generation upon. There were a few comments asking how I handled the keywords, so here it is.

The heuristics the script uses is outlined in the README. It's not the most mathematically-rigorous option, but it's hard to argue with the results.

When working with the Reddit data, I found that spaCy was too slow to encode hundreds of thousands of texts (would have taken 24+ hours on my first pass). So I used ray to parallelize it, which resulted in a 11x speedup that's more reasonable. That may end up being of more interest to  this subreddit.

Speaking of the Reddit API, now that the keyword generation is open sourced, I have [open-sourced the Reddit API](https://github.com/minimaxir/reddit-gpt-2-cloud-run) itself (sans the model since that's hard to distribute), with a mini howto on how I built it.",0,17
38,2019-7-2,2019,7,2,0,c7u0ol,Effectiveness of reweighted least squares,https://www.reddit.com/r/MachineLearning/comments/c7u0ol/effectiveness_of_reweighted_least_squares/,pktippa,1561995204,,0,2
39,2019-7-2,2019,7,2,0,c7u2la,[D] Jeff Hawkins: Thousand Brains Theory of Intelligence | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/c7u2la/d_jeff_hawkins_thousand_brains_theory_of/,UltraMarathonMan,1561995457," Jeff Hawkins is the founder of Redwood Center for Theoretical Neuroscience in 2002 and Numenta in 2005. In his 2004 book titled On Intelligence, and in his research before and after, he and his team have worked to reverse-engineer the neocortex and propose artificial intelligence architectures, approaches, and ideas that are inspired by the human brain. These ideas include Hierarchical Temporal Memory (HTM) from 2004 and The Thousand Brains Theory of Intelligence from 2017.

**Video:** [https://www.youtube.com/watch?v=-EVqrDlAqYo](https://www.youtube.com/watch?v=-EVqrDlAqYo)

https://i.redd.it/xig86isqop731.png

**Outline:**

0:00 - Introduction

1:28 - Understanding how the human brain works

5:44 - Parts of the brain

11:05 - How much do we understand?

14:20 - Nature of time in the brain

20:22 - Building a theory of intelligence

34:29 - Thousand brains theory of intelligence

40:06 - Ensembles and sensor fusion

44:00 - Concepts and language

45:38 - Memory palace and method of loci

50:20 - Reference frames

57:33 - Open problems

59:00 - Context

1:01:50 - Introspective thinking about the brain

1:04:19 - Deep learning

1:23:09 - Benchmarks

1:27:07 - Brain learning process

1:34:33 - How far are we from solving intelligence

1:38:37 - Possibility of AI winter

1:39:58 - Consciousness and intelligence

1:49:16 - Mortality

1:53:49 - Will understanding intelligence make us happy?

1:55:19 - Existential threats of AI

2:01:45 - Super-human intelligence and our future",57,110
40,2019-7-2,2019,7,2,0,c7ucn7,Warning to Data Science,https://www.reddit.com/r/MachineLearning/comments/c7ucn7/warning_to_data_science/,nxglogic,1561996401,,0,1
41,2019-7-2,2019,7,2,1,c7votp,[D] Boundary element method for Differential Equations,https://www.reddit.com/r/MachineLearning/comments/c7votp/d_boundary_element_method_for_differential/,WillingCucumber,1562000287,"Boundary element methods can be used to solve differential equations, where for some of the elements (boundary elements), the solution value is given/ fixed.  Boundary element methods have been used in ML, example being solving image blending task using poisson equation with given boundary elements.

[https://en.wikipedia.org/wiki/Discrete\_Poisson\_equation](https://en.wikipedia.org/wiki/Discrete_Poisson_equation)

&amp;#x200B;

My question is: What is the effect of sparsity of boundary elements on the solution of the differential equation ?",0,0
42,2019-7-2,2019,7,2,2,c7vvh7,[D] How can I invert (transpose) a convolutional layer?,https://www.reddit.com/r/MachineLearning/comments/c7vvh7/d_how_can_i_invert_transpose_a_convolutional_layer/,Arisngr,1562000808,"Let's say I have a network that contains a (2D) convolutional layer, and I want to transpose that layer's weights to generate an image (regardless of whether or not that's a sensible idea). Does anyone know a straight-forward way to do that?   

&amp;#x200B;

Thanks!",11,0
43,2019-7-2,2019,7,2,2,c7w153,"[P] Albumentations, an image augmentation library version 0.3 released. New weather augmentations, serialization support for reproducible machine learning pipelines, and speedup improvements",https://www.reddit.com/r/MachineLearning/comments/c7w153/p_albumentations_an_image_augmentation_library/,alexparinov,1562001260,"You can download the library from PyPI using `pip install -U albumentations` or  clone the latest version from [https://github.com/albu/albumentations](https://github.com/albu/albumentations)

&amp;#x200B;

# New features

**Weather augmentations.**

We've added weather augmentations such as [RandomRain](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomRain), [RandomSnow](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSnow), [RandomSunFlare](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSunFlare), [RandomShadow](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomShadow), [RandomFog](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomFog).

&amp;#x200B;

https://i.redd.it/4x2y4nty5q731.jpg

&amp;#x200B;

**Pipeline serialization.**

Now we can define transformations in the code and serialize them in python dictionary, json and yaml files. [**A Jupyter notebook with examples**](https://github.com/albu/albumentations/blob/master/notebooks/serialization.ipynb)**.**

&amp;#x200B;

**New transformations.**

[Lambda](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Lambda), [GaussianBlur](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GaussianBlur), [ChannelDropout](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ChannelDropout), [CoarseDropout](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CoarseDropout).

&amp;#x200B;

**The full release notes are available at** [https://github.com/albu/albumentations/releases/tag/0.3.0](https://github.com/albu/albumentations/releases/tag/0.3.0)",23,67
44,2019-7-2,2019,7,2,2,c7wg9u,Counting points on a go board with a neural net,https://www.reddit.com/r/MachineLearning/comments/c7wg9u/counting_points_on_a_go_board_with_a_neural_net/,jthat92,1562002490,,0,1
45,2019-7-2,2019,7,2,2,c7wudv,How beneficial is it to know multiple deep learning frameworks?,https://www.reddit.com/r/MachineLearning/comments/c7wudv/how_beneficial_is_it_to_know_multiple_deep/,Lunkwill_And_Fook,1562003601,[removed],0,1
46,2019-7-2,2019,7,2,3,c7x3ai,"Which notebook should I use to create course notes: R Notebook, Jupyter, or Observable?",https://www.reddit.com/r/MachineLearning/comments/c7x3ai/which_notebook_should_i_use_to_create_course/,kosar7,1562004298,[removed],0,1
47,2019-7-2,2019,7,2,3,c7xyca,[D] Contract Data Science/ML gig at Microsoft: Good idea or bad idea?,https://www.reddit.com/r/MachineLearning/comments/c7xyca/d_contract_data_scienceml_gig_at_microsoft_good/,AlexSnakeKing,1562006813,"Asking for a friend who is not on Reddit:

Friend has been looking to break out of engineering and into more DS/ML based work. Has lots of dev experience + good knowledge of ML learning from Kaggle, Coursera certifications etc....

Was offered a role as DS at MSFT, but through a vendor. Is wondering whether its a good idea or not:

Pros: Hiring manager and scientists from Vendor who interviewed him seemed like good people with strong backgrounds (He hasn't any visibility to the MSFT FTEs he will be working with, only the vendor). Job is exactly what he wants to be doing. Would get exposure to some pretty cool systems.

Cons: A couple of his connections within MSFT are saying that it is a bad idea, vendors don't get treated well. Chances are the work isn't as interesting as the hiring manager makes it out to be, otherwise it wouldn't be handed to vendor. For the same reason, won't look good on resume either. 

&amp;#x200B;

Anybody have any experience or input on this?",0,1
48,2019-7-2,2019,7,2,4,c7ypnj,Searching for Code? Let a Neural Network Do That for You!,https://www.reddit.com/r/MachineLearning/comments/c7ypnj/searching_for_code_let_a_neural_network_do_that/,Yuqing7,1562009005,,0,1
49,2019-7-2,2019,7,2,4,c7z3av,FastText Vector Norms And OOV Words,https://www.reddit.com/r/MachineLearning/comments/c7z3av/fasttext_vector_norms_and_oov_words/,vackosar,1562010123,,0,1
50,2019-7-2,2019,7,2,4,c7z51o,I'm having some troubles with my first TF AI,https://www.reddit.com/r/MachineLearning/comments/c7z51o/im_having_some_troubles_with_my_first_tf_ai/,ph04,1562010337,[removed],0,1
51,2019-7-2,2019,7,2,5,c7zhgn,[D] How much can we change a paper before the camera ready version?,https://www.reddit.com/r/MachineLearning/comments/c7zhgn/d_how_much_can_we_change_a_paper_before_the/,AnonMLstudent,1562011933,"In preparation for author rebuttal for a conference, we have conducted additional experiments and significantly improved our paper already (of course the reviewers can't see this yet, we may mention it during author rebuttal if the changes relate to any of their comments). 

I'm wondering if this is an issue and does the final version of the paper have to be very close to the initial submission, and only improve upon what the reviewers mention? Or can we add other improvements as long as they are high quality and don't detract from the original submission?",12,5
52,2019-7-2,2019,7,2,5,c7zjox,What are the differences between deep learning and usual machine learning?,https://www.reddit.com/r/MachineLearning/comments/c7zjox/what_are_the_differences_between_deep_learning/,Magniminda,1562012225,[removed],0,1
53,2019-7-2,2019,7,2,5,c7zn7s,Are Privacy and Machine Learning Compatible?,https://www.reddit.com/r/MachineLearning/comments/c7zn7s/are_privacy_and_machine_learning_compatible/,RandomnessTalk,1562012694,[removed],0,2
54,2019-7-2,2019,7,2,5,c7zoah,[R] PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows,https://www.reddit.com/r/MachineLearning/comments/c7zoah/r_pointflow_3d_point_cloud_generation_with/,stevenygd,1562012837,,1,1
55,2019-7-2,2019,7,2,5,c7zznd,Dealing with DBs using NLP,https://www.reddit.com/r/MachineLearning/comments/c7zznd/dealing_with_dbs_using_nlp/,AhmedKhaled97,1562014390,[removed],0,1
56,2019-7-2,2019,7,2,5,c801ir,How to Matthew's Correlation Coefficient from Alternate values,https://www.reddit.com/r/MachineLearning/comments/c801ir/how_to_matthews_correlation_coefficient_from/,mockrun,1562014652,[removed],0,1
57,2019-7-2,2019,7,2,6,c808io,ML for beginners,https://www.reddit.com/r/MachineLearning/comments/c808io/ml_for_beginners/,preetham_salehundam,1562015593,[removed],0,1
58,2019-7-2,2019,7,2,6,c80gxi,[R] Aspects of language captured by BERT,https://www.reddit.com/r/MachineLearning/comments/c80gxi/r_aspects_of_language_captured_by_bert/,omarsar,1562016776,,0,1
59,2019-7-2,2019,7,2,7,c80u67,"Best group telegram about deep learning ,machine learning ,...",https://www.reddit.com/r/MachineLearning/comments/c80u67/best_group_telegram_about_deep_learning_machine/,Doctor_who1,1562018642,[removed],0,1
60,2019-7-2,2019,7,2,7,c811zd,"As a confused CS undergrad student who wants to be a Data Scientist/ML engineer, is it okay that I feel bad that I won't use 70% what I learn in my degree for my future career?",https://www.reddit.com/r/MachineLearning/comments/c811zd/as_a_confused_cs_undergrad_student_who_wants_to/,swaggydhl,1562019746,[removed],0,1
61,2019-7-2,2019,7,2,9,c82p7u,[D] Threshold for rejecting word embedding similarities,https://www.reddit.com/r/MachineLearning/comments/c82p7u/d_threshold_for_rejecting_word_embedding/,radcapbill,1562029098,I have a problem where I have certain set of target words and I need to use them to match with other words that are found in new csvs. I was wondering if there are any good approaches to determining the threshold for rejecting word similarities. I was thinking using a random sample of 10k words and plot their similarities (10k*9.99k/2) but I am not sure whether this is the right approach. Or should I use the distribution of the similarities of the target words on a vocabulary and choose a percentile cutoff? Any ideas?,8,1
62,2019-7-2,2019,7,2,10,c82x6q,[R] Unsupervised Learning of Object Keypoints for Perception and Control,https://www.reddit.com/r/MachineLearning/comments/c82x6q/r_unsupervised_learning_of_object_keypoints_for/,hardmaru,1562030399,,1,14
63,2019-7-2,2019,7,2,10,c83457,[D] Advice about a research paper after a BSc program,https://www.reddit.com/r/MachineLearning/comments/c83457/d_advice_about_a_research_paper_after_a_bsc/,ML_STUDENT_throwaway,1562031514,"Hi all, Im sorry if this is not the appropriate place to post such a question, but Id be really happy if you guys could shed some light on my situation.

Without giving too much detail, I graduated with a BSc a few months ago from a small university in [STEM field; not CS]. I joined a research lab during this time in my field that involves some computational work, and ended up doing a little project for fun applying machine learning to this field, which never evolved to yield novel results. Now, I am applying to MSc programs in CS, but am also considering just joining industry doing something in programming.

I dont think that little project is noteworthy at all; in fact, it is nearly identical to something done almost 5 years ago, with a slightly different algorithm. However, my professor is now pushing me to write a paper about the project, which will make it easier for the lab to get grants and pursue more interesting machine learning work in the future (strangely, I will not be there anymore though, and no one else in the lab seems to have a big interest in ML). Moreover, the professor is expecting me to educate all of the other lab members on how to use python, making sure GPUs are in order, training ML algorithms, etc. since I was the one that started the project.

Here are the questions that Id be happy if you could give me some input:
(1) do you think it is better to publish a paper that is not interesting than to not publish at all, and
(2) do you think that it is reasonable to ask a student after graduation to help out in the lab? If so, for how long?

Thank you very much in advance for your comments.",5,1
64,2019-7-2,2019,7,2,10,c83adh,Heart disease classifier using K-Nearest Neighbors Algorithm,https://www.reddit.com/r/MachineLearning/comments/c83adh/heart_disease_classifier_using_knearest_neighbors/,champianalien21,1562032564,[removed],0,1
65,2019-7-2,2019,7,2,10,c83avz,[D] The Hidden Story Behind the Suicide PhD Candidate Huixiang Chen,https://www.reddit.com/r/MachineLearning/comments/c83avz/d_the_hidden_story_behind_the_suicide_phd/,SkiddyX,1562032648,,0,1
66,2019-7-2,2019,7,2,11,c83emm,[D] Other cautionary papers on taking ML to production?,https://www.reddit.com/r/MachineLearning/comments/c83emm/d_other_cautionary_papers_on_taking_ml_to/,ClydeMachine,1562033251,"Greetings all,

I've recently been recommended to read [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems), and have found its insights valuable. Knowing that this will benefit at least one upcoming project I am working, what other essential reading is out there for taking machine learning projects to production in a principled manner? Whose other mistakes can I learn from before making my own?",26,151
67,2019-7-2,2019,7,2,11,c83iv3,[P] Train an Image Classifier Without Coding (AutoML),https://www.reddit.com/r/MachineLearning/comments/c83iv3/p_train_an_image_classifier_without_coding_automl/,tim_macgyver,1562033938,,0,1
68,2019-7-2,2019,7,2,11,c83pal,Why arent there more adequate tools?,https://www.reddit.com/r/MachineLearning/comments/c83pal/why_arent_there_more_adequate_tools/,Fhdhdndnd,1562035000,[removed],0,1
69,2019-7-2,2019,7,2,13,c84ot7,Jeff Hawkins: Memory Predictive Framework,https://www.reddit.com/r/MachineLearning/comments/c84ot7/jeff_hawkins_memory_predictive_framework/,jimisommer,1562041167,,0,1
70,2019-7-2,2019,7,2,13,c84y9j,Is Flux better than PyTorch? Go,https://www.reddit.com/r/MachineLearning/comments/c84y9j/is_flux_better_than_pytorch_go/,seismic_swarm,1562042891,,0,1
71,2019-7-2,2019,7,2,14,c857uw,Zooming into the world of computer vision applications,https://www.reddit.com/r/MachineLearning/comments/c857uw/zooming_into_the_world_of_computer_vision/,Verma_RJ,1562044701,,0,1
72,2019-7-2,2019,7,2,14,c85965,How Machine Learning is Expanding Insigts,https://www.reddit.com/r/MachineLearning/comments/c85965/how_machine_learning_is_expanding_insigts/,Linkcxo,1562044957,,0,1
73,2019-7-2,2019,7,2,14,c859nb,any suggestion paper or github repo for natural language sentence generation based on some keywords?,https://www.reddit.com/r/MachineLearning/comments/c859nb/any_suggestion_paper_or_github_repo_for_natural/,jdxyw,1562045055,[removed],0,1
74,2019-7-2,2019,7,2,14,c85czj,[R] ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,https://www.reddit.com/r/MachineLearning/comments/c85czj/r_imagenettrained_cnns_are_biased_towards_texture/,milaworld,1562045714,,27,104
75,2019-7-2,2019,7,2,14,c85fx9,Is Flux better than PyTorch? Go,https://www.reddit.com/r/MachineLearning/comments/c85fx9/is_flux_better_than_pytorch_go/,seismic_swarm,1562046339,,0,1
76,2019-7-2,2019,7,2,14,c85iun,[D] Is Flux better than PyTorch? Go,https://www.reddit.com/r/MachineLearning/comments/c85iun/d_is_flux_better_than_pytorch_go/,seismic_swarm,1562046936,,0,1
77,2019-7-2,2019,7,2,14,c85jdt,Machine Learning- Electric Car And These 4 Countries- An Environment Saviour,https://www.reddit.com/r/MachineLearning/comments/c85jdt/machine_learning_electric_car_and_these_4/,emmawhitner,1562047048,,0,1
78,2019-7-2,2019,7,2,15,c85x37,Things you should understand for repairing your Simonelli coffee machine repair,https://www.reddit.com/r/MachineLearning/comments/c85x37/things_you_should_understand_for_repairing_your/,GoldCoastCater,1562049857,[removed],0,1
79,2019-7-2,2019,7,2,15,c85xor,AIDDT: Artificial Intelligence-based Drug Design Toolkit,https://www.reddit.com/r/MachineLearning/comments/c85xor/aiddt_artificial_intelligencebased_drug_design/,mischief_23,1562049974,[removed],0,1
80,2019-7-2,2019,7,2,16,c868ya,Coolest Papers of CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/c868ya/coolest_papers_of_cvpr_2019/,PyWarrior,1562052316,What are some of the Coolest Papers of CVPR 2019?,0,1
81,2019-7-2,2019,7,2,16,c86bds,[D] The -trick or the effectiveness of reweighted least-squares,https://www.reddit.com/r/MachineLearning/comments/c86bds/d_the_trick_or_the_effectiveness_of_reweighted/,youali,1562052853,"Francis Bach just started a blog, and this is his first blog post if anyone is interested:

[The -trick or the effectiveness of reweighed least-squares](https://francisbach.com/the-%ce%b7-trick-or-the-effectiveness-of-reweighted-least-squares/)

	

*Optimizing a quadratic function is often considered easy as it is equivalent to solving a linear system, for which many algorithms exist. Thus, reformulating a non-quadratic optimization problem into a sequence of quadratic problems is a natural idea.*",6,20
82,2019-7-2,2019,7,2,16,c86chi,PC WIRES,https://www.reddit.com/r/MachineLearning/comments/c86chi/pc_wires/,ada2017,1562053106,,0,1
83,2019-7-2,2019,7,2,16,c86ebg,Multi Output Model,https://www.reddit.com/r/MachineLearning/comments/c86ebg/multi_output_model/,anubhavnatani99,1562053487,[removed],0,1
84,2019-7-2,2019,7,2,16,c86esd,Manual Glass Grinding Machine with ZSP Belt- Zhengyi Glass Machinery Company,https://www.reddit.com/r/MachineLearning/comments/c86esd/manual_glass_grinding_machine_with_zsp_belt/,zhengyimachine,1562053590,,0,1
85,2019-7-2,2019,7,2,16,c86hw3,Dynamic pricing for premium game,https://www.reddit.com/r/MachineLearning/comments/c86hw3/dynamic_pricing_for_premium_game/,Haplo_34545,1562054312,[removed],0,1
86,2019-7-2,2019,7,2,17,c86o5j,[P] Video traversing latent space of real and drawn faces in same model,https://www.reddit.com/r/MachineLearning/comments/c86o5j/p_video_traversing_latent_space_of_real_and_drawn/,shoeblade,1562055816,"[https://www.youtube.com/watch?v=XDWua850n54](https://www.youtube.com/watch?v=XDWua850n54)

![video](hefib0atnu731 ""snip from the video"")

Traversal through custom styleGAN model trained on mix of real and drawn faces.

This model was made by Joel Simon and will be part of his new artbreeder website:

[https://ganbreeder.app/announcements/artbreeder](https://ganbreeder.app/announcements/artbreeder)",0,1
87,2019-7-2,2019,7,2,17,c86p50,Ready to use model for document enhancement?,https://www.reddit.com/r/MachineLearning/comments/c86p50/ready_to_use_model_for_document_enhancement/,mutusfa,1562056052,[removed],0,1
88,2019-7-2,2019,7,2,17,c86qpd,[R] Growing Action Spaces,https://www.reddit.com/r/MachineLearning/comments/c86qpd/r_growing_action_spaces/,sensetime,1562056442,,1,4
89,2019-7-2,2019,7,2,17,c86u3y,Graph Convolutional Networks Vs Node2vec,https://www.reddit.com/r/MachineLearning/comments/c86u3y/graph_convolutional_networks_vs_node2vec/,y05r1,1562057258,[removed],0,1
90,2019-7-2,2019,7,2,17,c86u80,ML Turns Celebrities Into Cartoons And The Results Are Amazingly Fun,https://www.reddit.com/r/MachineLearning/comments/c86u80/ml_turns_celebrities_into_cartoons_and_the/,philippbatura,1562057288,,1,0
91,2019-7-2,2019,7,2,18,c86y6z,Machine Learning Training Institute in South Delhi | Drona Training Academy,https://www.reddit.com/r/MachineLearning/comments/c86y6z/machine_learning_training_institute_in_south/,dtacademy,1562058191,[removed],0,1
92,2019-7-2,2019,7,2,18,c872ce,"Lecture on AI, music, and refreshments in Hamburg, Germany July 7th",https://www.reddit.com/r/MachineLearning/comments/c872ce/lecture_on_ai_music_and_refreshments_in_hamburg/,tommasodorigo,1562059099,,0,1
93,2019-7-2,2019,7,2,18,c8730k,Downloading Update,https://www.reddit.com/r/MachineLearning/comments/c8730k/downloading_update/,Wenderu84,1562059248,[removed],0,0
94,2019-7-2,2019,7,2,18,c876nx,"[D] Would you prefer Google coral to Raspberry Pi 4, or Jetson Nano?",https://www.reddit.com/r/MachineLearning/comments/c876nx/d_would_you_prefer_google_coral_to_raspberry_pi_4/,makereven,1562060063,"a question: Would you prefer Google coral to Raspberry Pi 4, or Jetson Nano? will they replace the other? which one is better for machine learning? 

welcome to share your opinion with me!",14,15
95,2019-7-2,2019,7,2,19,c87gs4,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/c87gs4/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1562062229,[removed],0,1
96,2019-7-2,2019,7,2,19,c87iqm,[P] A JavaScript Library implementing the NEAT algorithm,https://www.reddit.com/r/MachineLearning/comments/c87iqm/p_a_javascript_library_implementing_the_neat/,Extension_Shoe,1562062638,"Just finished writing this library for javascript implementing the neat algorithm.

Take a look if you are interested

[https://github.com/ExtensionShoe/NEAT-JS](https://github.com/ExtensionShoe/NEAT-JS)",5,12
97,2019-7-2,2019,7,2,19,c87is6,"Standard Error , Prediction Interval, Conidence Interval",https://www.reddit.com/r/MachineLearning/comments/c87is6/standard_error_prediction_interval_conidence/,PayalBhatia,1562062648,[removed],0,1
98,2019-7-2,2019,7,2,19,c87mo2,Training a Q-Learner,https://www.reddit.com/r/MachineLearning/comments/c87mo2/training_a_qlearner/,zcbemav,1562063455,[removed],0,1
99,2019-7-2,2019,7,2,19,c87r11,How to Participate in a Kaggle Competition with Zero Code - Tutorial,https://www.reddit.com/r/MachineLearning/comments/c87r11/how_to_participate_in_a_kaggle_competition_with/,hiidenheimo,1562064331,,0,1
100,2019-7-2,2019,7,2,19,c87v78,"Best way to convert traditional images into MNIST format, for testing purposes on a CNN trained by MNIST dataset?",https://www.reddit.com/r/MachineLearning/comments/c87v78/best_way_to_convert_traditional_images_into_mnist/,Fengax,1562065180,[removed],0,1
101,2019-7-2,2019,7,2,20,c87vfo,[Help Needed] I'm a Computer Engineer but completely Clueless about ML/AI,https://www.reddit.com/r/MachineLearning/comments/c87vfo/help_needed_im_a_computer_engineer_but_completely/,yaxir,1562065228,[removed],0,1
102,2019-7-2,2019,7,2,20,c8864w,"[D] Best way to convert traditional images into MNIST format, for testing purposes on a CNN trained by MNIST dataset?",https://www.reddit.com/r/MachineLearning/comments/c8864w/d_best_way_to_convert_traditional_images_into/,Fengax,1562067392," I have just trained my first CNN network by using the MNIST dataset, it is the most famous handwriting dataset. However, instead of using their testing images, I want to **utilize my own 28x28 testing images.**

The rationale behind this, is that I want to make a handwriting recognition program, so obviously I need a way to convert traditional image format to the one-dimensional MNIST format, so that the CNN can read it.

What is the best way to accomplish this task?",8,0
103,2019-7-2,2019,7,2,20,c887vh,What is Machine Learning Its Benefits and Applications,https://www.reddit.com/r/MachineLearning/comments/c887vh/what_is_machine_learning_its_benefits_and/,MtBlogger,1562067755,,0,1
104,2019-7-2,2019,7,2,20,c8894q,"[P] Annomator - A new, simple way to annotate images with masks",https://www.reddit.com/r/MachineLearning/comments/c8894q/p_annomator_a_new_simple_way_to_annotate_images/,Annomator,1562068010,"[**https://github.com/annomator/annomator\_1.0**](https://github.com/annomator/annomator_1.0)

Annomator automatically generates text, box and mask data for direct use or training.  Use the automatic image annotation or start from scratch.  Simply paint each mask with the right color code with no external text file needed.  It is compatible with all the MSCOCO models in the Tensorflow Model Zoo.",2,5
105,2019-7-2,2019,7,2,20,c889e0,[D] Machine Learning Golf,https://www.reddit.com/r/MachineLearning/comments/c889e0/d_machine_learning_golf/,KappaClosed,1562068063,,0,1
106,2019-7-2,2019,7,2,20,c88c5j,Help with Keras LSTM,https://www.reddit.com/r/MachineLearning/comments/c88c5j/help_with_keras_lstm/,jd_portugal,1562068572,[removed],0,1
107,2019-7-2,2019,7,2,21,c88u8f,[R] Machine Intelligence Conference IBM Diversity Scholarships,https://www.reddit.com/r/MachineLearning/comments/c88u8f/r_machine_intelligence_conference_ibm_diversity/,MICInc,1562071825,"Hey reddit fam! The Machine Intelligence Conference committee is excited to announce our  IBM Diversity Scholarship for 2019. This will cover  flight and hotel expenses for selected applicants. Please see our website for more details:
https://machineintelligence.cc/conference/scholarship",4,0
108,2019-7-2,2019,7,2,21,c88uu5,Object detection/instance segmentation using multiple views of the same object,https://www.reddit.com/r/MachineLearning/comments/c88uu5/object_detectioninstance_segmentation_using/,AndriPi,1562071928,[removed],0,1
109,2019-7-2,2019,7,2,21,c88w7q,[D] Defect detection in an object looking at multiple pictures of the same object from different angles,https://www.reddit.com/r/MachineLearning/comments/c88w7q/d_defect_detection_in_an_object_looking_at/,AndriPi,1562072162,"Hi, I would like to detect defects in some objects. I have multiple pictures of the same object taken from different angles. Some defects are easier to detect, if you look at two (or three) pictures of the same object from different angles, rather than just at one. However, the object detection models I know of (such as RetinaNet, or Faster R-CNN) only look at one image at a time.

Do you know of any model(s) which can look at multiple pictures of the same object at once?",13,2
110,2019-7-2,2019,7,2,22,c8912i,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/c8912i/data_science_career_track_prep_course/,HannahHumphreys,1562072964,[removed],0,1
111,2019-7-2,2019,7,2,22,c8924s,[R] Identifying Emotions from Walking using Affective and Deep Features,https://www.reddit.com/r/MachineLearning/comments/c8924s/r_identifying_emotions_from_walking_using/,baylearn,1562073139,,0,1
112,2019-7-2,2019,7,2,22,c89803,Machine Learning Bootcamp: Become a ML Engineer in 6 months. Job Guaranteed.,https://www.reddit.com/r/MachineLearning/comments/c89803/machine_learning_bootcamp_become_a_ml_engineer_in/,HannahHumphreys,1562074054,[removed],0,1
113,2019-7-2,2019,7,2,22,c89c3u,Google AutoML,https://www.reddit.com/r/MachineLearning/comments/c89c3u/google_automl/,sainatarajan7,1562074709,[removed],0,1
114,2019-7-2,2019,7,2,22,c89cz1,[R] Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog,https://www.reddit.com/r/MachineLearning/comments/c89cz1/r_way_offpolicy_batch_deep_reinforcement_learning/,hardmaru,1562074849,,2,7
115,2019-7-2,2019,7,2,22,c89e2e,Machine Learning VS Deep Learning,https://www.reddit.com/r/MachineLearning/comments/c89e2e/machine_learning_vs_deep_learning/,Quentin-Martell,1562075025,[removed],0,1
116,2019-7-2,2019,7,2,22,c89gu8,[Research] Achieving End-to-End Emotional Speech Synthesis,https://www.reddit.com/r/MachineLearning/comments/c89gu8/research_achieving_endtoend_emotional_speech/,cdossman,1562075483,"  [https://medium.com/ai%C2%B3-theory-practice-business/ai-scholar-achieving-end-to-end-emotional-speech-synthesis-bfaf45e68a14](https://medium.com/ai%C2%B3-theory-practice-business/ai-scholar-achieving-end-to-end-emotional-speech-synthesis-bfaf45e68a14) 

**Abstract**: This paper proposes an end-to-end emotional speech synthesis (ESS) method which adopts global style tokens (GSTs) for semi-supervised training. This model is built based on the GST-Tacotron framework. The style tokens are defined to present emotion categories. A cross entropy loss function between token weights and emotion labels is designed to obtain the interpretability of style tokens utilizing the small portion of training data with emotion labels. Emotion recognition experiments confirm that this method can achieve one-to-one correspondence between style tokens and emotion categories effectively. Objective and subjective evaluation results show that our model outperforms the conventional Tacotron model for ESS when only 5% of training data has emotion labels. Its subjective performance is close to the Tacotron model trained using all emotion labels. Keywords: emotional speech synthesis, end-to-end, Tacotron, global style tokens, semi-supervised training",0,4
117,2019-7-2,2019,7,2,22,c89h2o,Global Gigabit Ethernet Switch Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c89h2o/global_gigabit_ethernet_switch_market_report_2019/,jadhavni3,1562075516,[removed],1,1
118,2019-7-2,2019,7,2,23,c89lif,[R] Layer rotation: a surprisingly powerful indicator of generalization in deep networks?,https://www.reddit.com/r/MachineLearning/comments/c89lif/r_layer_rotation_a_surprisingly_powerful/,Simoncarbo,1562076222,"Sharing our latest work presented at the ICML workshop ""Identifying and Understanding Deep Learning Phenomena"":

*Layer rotation: a surprisingly powerful indicator of generalization in deep networks?* ([arxiv link](https://arxiv.org/abs/1806.01603v2))

&amp;#x200B;

We're pretty excited about it: we really believe layer rotation (the metric we study) is somehow related to a fundamental aspect of deep learning, and that it is worth much more investigation.  For the moment, our work demonstrates that layer rotation's relation with generalization exhibits a remarkable

* consistency : a rule of thumb that is widely applicable, explaining ***differences of up to 30% test accuracy***, 
* simplicity : ***a network-independent optimum w.r.t. generalization***, and 
* explanatory power: ***novel insights around widely used techniques*** (weight decay, adaptive gradient methods, learning rate warmups,...).

We also provide preliminary evidence that layer rotations correlate with the degree to which intermediate features are learned during the training procedure.

&amp;#x200B;

Since we also provide tools to monitor and control layer rotation during training, our work could also greatly reduce the current hyperparameter tuning struggle. Code available! [Here](https://github.com/ispgroupucl/layer-rotation-paper-experiments) and [here](https://github.com/ispgroupucl/layer-rotation-tools).

&amp;#x200B;

Looking forward to your feedback!

&amp;#x200B;

**Abstract**:  

Our work presents extensive empirical evidence that layer rotation, i.e. the evolution across training of the cosine distance between each layer's weight vector and its initialization, constitutes an impressively consistent indicator of generalization performance. In particular, larger cosine distances between final and initial weights of each layer consistently translate into better generalization performance of the final model. Interestingly, this relation admits a network independent optimum: training procedures during which all layers' weights reach a cosine distance of 1 from their initialization consistently outperform other configurations -by up to 30% test accuracy. Moreover, we show that layer rotations are easily monitored and controlled (helpful for hyperparameter tuning) and potentially provide a unified framework to explain the impact of learning rate tuning, weight decay, learning rate warmups and adaptive gradient methods on generalization and training speed. In an attempt to explain the surprising properties of layer rotation, we show on a 1-layer MLP trained on MNIST that layer rotation correlates with the degree to which features of intermediate layers have been trained.",58,187
119,2019-7-2,2019,7,2,23,c89o7g,[D] How to deal with sequential effects on data?,https://www.reddit.com/r/MachineLearning/comments/c89o7g/d_how_to_deal_with_sequential_effects_on_data/,jim1564,1562076618,"Let's say someone has chronic pain and they want to log everyday activities(like food eaten, exercises, meds taken, etc) to see if there is any correlation between them and the intensity of pain. But actually there's no correlation and the intensity behaves like a sin wave across the training examples, like its sequential/temporal.


How do you model this kind of behavior to extract knowledge?",2,2
120,2019-7-2,2019,7,2,23,c89pg0,Global Hydraulic Motors Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/c89pg0/global_hydraulic_motors_market_report_2019/,jadhavni3,1562076792,[removed],1,1
121,2019-7-2,2019,7,2,23,c89v8z,ICLR Reproducibility Series with Aniket Didolkar,https://www.reddit.com/r/MachineLearning/comments/c89v8z/iclr_reproducibility_series_with_aniket_didolkar/,CometML,1562077615,,0,1
122,2019-7-2,2019,7,2,23,c89wgz,"[D] Quality of VAE embeddings, depending on likelihood function?",https://www.reddit.com/r/MachineLearning/comments/c89wgz/d_quality_of_vae_embeddings_depending_on/,tanukibellydrum,1562077799,"tl;dr: Which log-likelihood function should I use for training VQ-VAE, when I only care about the embeddings?

Hi.

Since no one responded on r/MLQuestions, this might be better suited here:

I  have a question concerning the use of VAEs for encoding, as  opposed to using them for data generation. In particular, I want to use a  vector-quantising variational autoencoder to find a discrete  representation of continuous data for some downstream task. I wonder if  the choice of the decoder likelihood function would have a noticeable  impact on the quality of the discrete representation.

The objective for a batch size of 1 has the following form (omitting some VQ-VAE specific terms):

max log p\_dec(x|z\_enc) - KL( q(z) || p(z) )

where  x is a training sample, z\_enc is a latent random value generated by the  encoder, p\_dec is the likelihood function for the decoder, q is the  encoder prior over latent variable z and p(z) is the actual prior.

When  I assume p\_dec(X | z\_enc) to be a multivariate normal distribution  where the mean is given by some neural network and the covariance is an  identity matrix, I can replace the log-likelihood term with the negative  mean squared error, as in normal regression. This is what I've seen  being used in some implementations.

But I could also let the decoder output an arbitrary covariance matrix. This would of course change the log-likelihood function.

Do  you think it makes sense to use a more involved log-likelihood function  (i.e. arbitrary pos. semi-definite covariance matrix), so that the  encoder is forced to find a representation that is better for explaining  data coming from a complex distribution?",1,4
123,2019-7-2,2019,7,2,23,c89yx3,[R] ivis: Structure-preserving Dimensionality Reduction,https://www.reddit.com/r/MachineLearning/comments/c89yx3/r_ivis_structurepreserving_dimensionality/,bering_team,1562078133,,0,1
124,2019-7-2,2019,7,2,23,c8a4hq,[N] Thinking Like a Data Scientist (45 minute talk),https://www.reddit.com/r/MachineLearning/comments/c8a4hq/n_thinking_like_a_data_scientist_45_minute_talk/,mto96,1562078950,"This is a 45 minute talk by Em Grasmeder from GOTO Amsterdam 2019.

[https://www.youtube.com/watch?v=HJkzhN7LgrQ&amp;feature=youtu.be&amp;list=PLEx5khR4g7PKT9RvuVyQxJLO8CZUJzNMy](https://www.youtube.com/watch?v=HJkzhN7LgrQ&amp;feature=youtu.be&amp;list=PLEx5khR4g7PKT9RvuVyQxJLO8CZUJzNMy)

Please give the talk abstract a read below before giving it a watch:

&amp;#x200B;

The field of data science is having a little identity crisis. The fundamental questions of what data science is, and who a data scientist is, remain largely undecided. Regardless of where the answer will fall, there are a number of tools and techniques that every data scientist should have in their toolbelt. Although the software languages, frameworks, and algorithms will come in and out of fashion, the fundamentals behind the trade of data science, which we talk about in this session, have existed for centuries and will continue to be used for ages to come.

**What will the audience learn from this talk?**  
The audience will learn an overview and history of the math, philosophy, software engineering, and algorithms that are inseparable from the field of Data Science. We will cover techniques like optimisation theory like principle component analysis, at the level of analysing where and why we use certain techniques, but not how they are implemented or how to use them in a data science pipeline.",1,11
125,2019-7-3,2019,7,3,1,c8b5v9,D.R.E.A.M. Debugging Bias in Artificial Intelligence with Margaret Mitchell,https://www.reddit.com/r/MachineLearning/comments/c8b5v9/dream_debugging_bias_in_artificial_intelligence/,nbriz,1562084137,,0,1
126,2019-7-3,2019,7,3,1,c8bfgw,New Guy Here : Can i make an efficient algorithm that can translate speech to text of certain people,https://www.reddit.com/r/MachineLearning/comments/c8bfgw/new_guy_here_can_i_make_an_efficient_algorithm/,RARBK,1562085460,[removed],0,1
127,2019-7-3,2019,7,3,1,c8bmwg,How to integrate Facial Recognition into any DotNet App,https://www.reddit.com/r/MachineLearning/comments/c8bmwg/how_to_integrate_facial_recognition_into_any/,johnperkins899,1562086458,,0,1
128,2019-7-3,2019,7,3,2,c8butt,Learning to Reconstruct People - A review on human body shape/pose reconstruction.,https://www.reddit.com/r/MachineLearning/comments/c8butt/learning_to_reconstruct_people_a_review_on_human/,deemedkage,1562087464,,0,1
129,2019-7-3,2019,7,3,2,c8c0dj,[R] https://arxiv.org/abs/1811.07519 Higher-order Neural Networks for Action Recognition,https://www.reddit.com/r/MachineLearning/comments/c8c0dj/r_httpsarxivorgabs181107519_higherorder_neural/,KaiiHu,1562088206,[removed],0,1
130,2019-7-3,2019,7,3,2,c8c1db,"[D] There are no Tensors in Deep Learning - why do people use the word ""tensor""?",https://www.reddit.com/r/MachineLearning/comments/c8c1db/d_there_are_no_tensors_in_deep_learning_why_do/,ME_PhD,1562088346,"As someone who studied Tensors in physics long before TensorFlow was a thing, I cringe every time I hear these arrays called ""tensors"".

&amp;#x200B;

Does anyone know why this sin against mathematics is overlooked to such a large degree in the ML community?

&amp;#x200B;

TENSORS ARE NOT ARRAYS FFS!!!

 [http://mathworld.wolfram.com/Tensor.html](http://mathworld.wolfram.com/Tensor.html)",17,0
131,2019-7-3,2019,7,3,2,c8c48b,[P] Video traversing latent space of real and drawn faces in same model,https://www.reddit.com/r/MachineLearning/comments/c8c48b/p_video_traversing_latent_space_of_real_and_drawn/,shoeblade,1562088741," [https://www.youtube.com/watch?v=XDWua850n54](https://www.youtube.com/watch?v=XDWua850n54) 

&amp;#x200B;

[clip from video](https://reddit.com/link/c8c48b/video/bjmu1rr6ex731/player)

 

Traversal through custom styleGAN model trained on mix of real and drawn faces.

This model was made by Joel Simon and will be part of his new artbreeder website:

[https://ganbreeder.app/announcements/artbreeder](https://ganbreeder.app/announcements/artbreeder)",14,111
132,2019-7-3,2019,7,3,2,c8ccv5,Machine Learning Vs. Statistics,https://www.reddit.com/r/MachineLearning/comments/c8ccv5/machine_learning_vs_statistics/,andrea_manero,1562089863,[removed],0,1
133,2019-7-3,2019,7,3,3,c8cy31,How Machine Learning and Cybersecurity is helping todays business,https://www.reddit.com/r/MachineLearning/comments/c8cy31/how_machine_learning_and_cybersecurity_is_helping/,ramaprasadjena,1562092665,,0,1
134,2019-7-3,2019,7,3,3,c8d85z,[P] Looking for feedback on Robotics/CV paper before submitting to conference,https://www.reddit.com/r/MachineLearning/comments/c8d85z/p_looking_for_feedback_on_roboticscv_paper_before/,anish314,1562093983,"I'm an independent researcher who finished a project a few months ago called ""Real-Time Freespace Segmentation on Autonomous Robots for Detection of Obstacles and Drop-Offs."" My arXiv paper is here: https://arxiv.org/abs/1902.00842 (the paper is complete but I have new / better testing results since then, due to fixing a bug and testing on new datasets). I'm looking to submit this to a conference like [ISVC 2019](https://www.isvc.net) or another conference where proceedings are published (ideally in the US to reduce amount of travel). I was hoping to see if the community had any feedback for me on the paper as I prepare it to submit to ISVC (or suggestions on a better venue to publish).",2,2
135,2019-7-3,2019,7,3,4,c8d8ta,[D] Looking for help about STO,https://www.reddit.com/r/MachineLearning/comments/c8d8ta/d_looking_for_help_about_sto/,smicole,1562094063,"Hi Everyone, I need some help i am trying to implement Sent time optimization for messages but I don't know which ML algorithm or approach will be the most suitable for this task ? Any clue ?
Thanks all lot for any given answer.

",0,0
136,2019-7-3,2019,7,3,4,c8dim1,What's A Bad Idea?,https://www.reddit.com/r/MachineLearning/comments/c8dim1/whats_a_bad_idea/,karenactionsitafaal,1562095112,,0,1
137,2019-7-3,2019,7,3,4,c8e27u,deepnudenotgreey,https://www.reddit.com/r/MachineLearning/comments/c8e27u/deepnudenotgreey/,AhaMuffin,1562096752,[removed],0,1
138,2019-7-3,2019,7,3,4,c8e3h0,Machine Learning Math Notations. By Shan Hung-Wu,https://www.reddit.com/r/MachineLearning/comments/c8e3h0/machine_learning_math_notations_by_shan_hungwu/,ai-lover,1562096919,[removed],0,1
139,2019-7-3,2019,7,3,4,c8e4p0,Show Reddit: Approximate Nearest Neighbor (ANN) searches using a PostgreSQL backend.,https://www.reddit.com/r/MachineLearning/comments/c8e4p0/show_reddit_approximate_nearest_neighbor_ann/,kookaburro,1562097086,,0,1
140,2019-7-3,2019,7,3,5,c8eald,Are Fully Connected (Dense) networks dead?,https://www.reddit.com/r/MachineLearning/comments/c8eald/are_fully_connected_dense_networks_dead/,PeakNeuralChaos,1562097867,[removed],0,1
141,2019-7-3,2019,7,3,5,c8egan,Looking for feedback on my most recent project: beginner-friendly python library to teach neural networks to play Blackjack and count cards,https://www.reddit.com/r/MachineLearning/comments/c8egan/looking_for_feedback_on_my_most_recent_project/,AsymptoticUpperBound,1562098659,[removed],0,1
142,2019-7-3,2019,7,3,5,c8el3p,[D] GTX 1060 with RPi or Arduino?,https://www.reddit.com/r/MachineLearning/comments/c8el3p/d_gtx_1060_with_rpi_or_arduino/,RelateableFrog,1562099299,"So I recently upgraded my PC GPU and Im left with a spare GTX 1060 3GB kicking about. Ive been working for the last year on DNNs for my research projects and have also been using arduino and Raspberry Pi.

So here is my question, can anyone think of any way to use my spare GPU on those platforms for any cool projects?",9,1
143,2019-7-3,2019,7,3,5,c8etou,MIT Drag-and-Drop Data Analytics: Machine Learning for Everyone,https://www.reddit.com/r/MachineLearning/comments/c8etou/mit_draganddrop_data_analytics_machine_learning/,Yuqing7,1562100481,,0,1
144,2019-7-3,2019,7,3,5,c8evum,CRASSMAT: My First R Package,https://www.reddit.com/r/MachineLearning/comments/c8evum/crassmat_my_first_r_package/,[deleted],1562100786,,0,1
145,2019-7-3,2019,7,3,6,c8fmnj,"Data Integration, Data warehousing, Data analytics and Business Intelligence... What are those",https://www.reddit.com/r/MachineLearning/comments/c8fmnj/data_integration_data_warehousing_data_analytics/,Unchart3disOP,1562104461,[removed],0,1
146,2019-7-3,2019,7,3,7,c8fv2r,The Hidden Story Behind the Suicide PhD Candidate Huixiang Chen [News],https://www.reddit.com/r/MachineLearning/comments/c8fv2r/the_hidden_story_behind_the_suicide_phd_candidate/,dseggdfhfdfdrewww,1562105662,,0,1
147,2019-7-3,2019,7,3,7,c8fzcv,CRASSMAT: My First R Package,https://www.reddit.com/r/MachineLearning/comments/c8fzcv/crassmat_my_first_r_package/,nickkunz,1562106278,[removed],0,1
148,2019-7-3,2019,7,3,8,c8gipe,[P] KFServing Serverless Inferencing on Kubernetes; v0.1 Released Today!,https://www.reddit.com/r/MachineLearning/comments/c8gipe/p_kfserving_serverless_inferencing_on_kubernetes/,yoshi_corporation,1562109098,"Today, our cross-company team released [https://github.com/kubeflow/kfserving](https://github.com/kubeflow/kfserving)!

&amp;#x200B;

KFServing provides a Kubernetes [Custom Resource Definition](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) for serving ML Models on arbitrary frameworks. It aims to solve 80% of model serving use cases by providing performant, high abstraction interfaces for common ML frameworks like Tensorflow, XGBoost, ScikitLearn, PyTorch, and ONNX.

KFServing encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU Autoscaling, Scale to Zero, and Canary Rollouts to your ML deployments. It enables a simple, pluggable, and complete story for Mission Critical ML including inference, explainability, outlier detection, and prediction logging.

## Learn More

* Join our [Working Group](https://groups.google.com/forum/#!forum/kfserving) for meeting invites and discussion.
* [Read the Docs](https://github.com/kubeflow/kfserving/blob/master/docs).
* [Examples](https://github.com/kubeflow/kfserving/blob/master/docs/samples).
* [Roadmap](https://github.com/kubeflow/kfserving/blob/master/ROADMAP.md).
* [KFServing 101 Slides](https://drive.google.com/file/d/16oqz6dhY5BR0u74pi9mDThU97Np__AFb/view).
* [KFServing 101 Tech Talk](https://www.youtube.com/watch?v=hGIvlFADMhU).
* This project is an evolution of the [original proposal in the Kubeflow repo](https://github.com/kubeflow/kubeflow/issues/2306).",0,3
149,2019-7-3,2019,7,3,8,c8go42,Anyone still have the link for DeepNude,https://www.reddit.com/r/MachineLearning/comments/c8go42/anyone_still_have_the_link_for_deepnude/,EffectiveProgram,1562109931,[removed],0,1
150,2019-7-3,2019,7,3,8,c8gsux,"Machine Learning: An Algorithmic Perspective, Second Edition PDF",https://www.reddit.com/r/MachineLearning/comments/c8gsux/machine_learning_an_algorithmic_perspective/,psychonekk,1562110676,,0,1
151,2019-7-3,2019,7,3,9,c8h4zg,[D] Is state of the art speech synthesis at the point where it could be used to transcribe a book or other long form literary works?,https://www.reddit.com/r/MachineLearning/comments/c8h4zg/d_is_state_of_the_art_speech_synthesis_at_the/,leostrauss,1562112672,"Tacotron and derivative models described in recent papers often have appendixes with short speech samples that appear to have crossed the threshold of sounding perfectly human. However, those samples are only a few seconds in duration so I wonder if any attempts were made to use those models in long form text transcription and how those would rate against a proper human narrator.",4,0
152,2019-7-3,2019,7,3,9,c8hbjp,Is this a mixture model problem?,https://www.reddit.com/r/MachineLearning/comments/c8hbjp/is_this_a_mixture_model_problem/,DirectDark8,1562113711,[removed],0,1
153,2019-7-3,2019,7,3,10,c8hpgl,"[D] Would love feedback on this video which breaks learning into three layers (evolution, in life &amp; abstract), by Art of the Problem",https://www.reddit.com/r/MachineLearning/comments/c8hpgl/d_would_love_feedback_on_this_video_which_breaks/,britcruise,1562116022,,0,1
154,2019-7-3,2019,7,3,10,c8hs2t,Working Deep Nude for $10,https://www.reddit.com/r/MachineLearning/comments/c8hs2t/working_deep_nude_for_10/,gotthatworkingDN,1562116475,[removed],0,1
155,2019-7-3,2019,7,3,11,c8igsi,I'm trying to implement 'Born Again Neural Networks' by T.Furlanello.,https://www.reddit.com/r/MachineLearning/comments/c8igsi/im_trying_to_implement_born_again_neural_networks/,crackitr,1562120666,"Hi, I'm trying to implement 'Born Again Neural Networks(BAN)' by T.Furlanello([https://arxiv.org/abs/1805.04770](https://arxiv.org/abs/1805.04770)), and I have some questions. Can anybody help with this please?

&amp;#x200B;

If you have read some papers on Knowledge Distillation, you would know that some papers released before BAN used so called temperature. In those papers, the logits were divided with temperature(usually positive integer) and then these outputs were gone through within softmax.

&amp;#x200B;

In BAN paper, however, the authour didn't mention on temperature.  So I didn't stabilize the logits (in other words, I set temperature as 1), but I found that I failed. There were no dramatic difference between Original network and Distilled Network. I guess if i just set temperature as 1 and if the networks overfits, the output distribution wouldn't provide a meaningful 'dark knowledge'..

for example,  there would no difference between \[1, 0, 0, 0,0\] and \[0.999, 0.000 ..., 0.000 ..., 0.000 ..., 0.000 ...\]

&amp;#x200B;

so.. do i have to apply temperature even though the author didn't mention on the paper? I'm wondering if I can succeed without applying temperature. Thank you for reading.",0,1
156,2019-7-3,2019,7,3,11,c8inpz,"Machine Learning for Everyone: In simple words. With real-world examples. Yes, again",https://www.reddit.com/r/MachineLearning/comments/c8inpz/machine_learning_for_everyone_in_simple_words/,alexanderfefelov,1562121855,,0,1
157,2019-7-3,2019,7,3,12,c8j4jw,[R] - Exploring the Lottery Ticket Hypothesis,https://www.reddit.com/r/MachineLearning/comments/c8j4jw/r_exploring_the_lottery_ticket_hypothesis/,tldrtldreverything,1562124767,"Hi all, I published a summary of a recent paper published by researchers from MIT, University of Toronto and Cambridge. It takes a deeper look into the Lottery Ticket Hypothesis, an idea that neural networks have a pruned version which can train just as well as the original version with significantly less size, when applying the right weights (this is a ""winning ticket""). The result could help better understand ""winning tickets"" and is generally interesting. I hope you'll like it and happy to get feedback. Full summary here: [https://www.lyrn.ai/2019/07/02/exploring-the-lottery-ticket-hypothesis/](https://www.lyrn.ai/2019/07/02/exploring-the-lottery-ticket-hypothesis/)",23,136
158,2019-7-3,2019,7,3,12,c8j4q2,[HELP][SUGGESTION] Coming up with guidelines for ML research,https://www.reddit.com/r/MachineLearning/comments/c8j4q2/helpsuggestion_coming_up_with_guidelines_for_ml/,navidalvee,1562124796,[removed],0,1
159,2019-7-3,2019,7,3,12,c8j6pd,[D] Artificial IntelligenceThe Revolution Hasnt Happened Yet (Michael Jordan opinion piece),https://www.reddit.com/r/MachineLearning/comments/c8j6pd/d_artificial_intelligencethe_revolution_hasnt/,sensetime,1562125141,"*I found this [article](https://hdsr.mitpress.mit.edu/pub/wot7mkc1/) published recently in Harvard Data Science Review by Michael Jordan (the academic) a joyful read. Below is an excerpt from [Artificial IntelligenceThe Revolution Hasnt Happened Yet](https://hdsr.mitpress.mit.edu/pub/wot7mkc1/):*

Most of what is labeled AI today, particularly in the public sphere, is actually machine learning (ML), a term in use for the past several decades. ML is an algorithmic field that blends ideas from statistics, computer science and many other disciplines (see below) to design algorithms that process data, make predictions, and help make decisions. In terms of impact on the real world, ML is the real thing, and not just recently. Indeed, that ML would grow into massive industrial relevance was already clear in the early 1990s, and by the turn of the century forward-looking companies such as Amazon were already using ML throughout their business, solving mission-critical, back-end problems in fraud detection and supply-chain prediction, and building innovative consumer-facing services such as recommendation systems. As datasets and computing resources grew rapidly over the ensuing two decades, it became clear that ML would soon power not only Amazon but essentially any company in which decisions could be tied to large-scale data. New business models would emerge. The phrase data science emerged to refer to this phenomenon, reflecting both the need of ML algorithms experts to partner with database and distributed-systems experts to build scalable, robust ML systems, as well as reflecting the larger social and environmental scope of the resulting systems.This confluence of ideas and technology trends has been rebranded as AI over the past few years. This rebranding deserves some scrutiny.

https://hdsr.mitpress.mit.edu/pub/wot7mkc1/",31,72
160,2019-7-3,2019,7,3,13,c8joky,The answer to forecasting Bitcoin may lie in artificial intelligence -- is there validity to this article's claims?,https://www.reddit.com/r/MachineLearning/comments/c8joky/the_answer_to_forecasting_bitcoin_may_lie_in/,yyflowerpot,1562128302,,0,1
161,2019-7-3,2019,7,3,13,c8jswa,[discussion] Had an idea want to know if its possible to do.,https://www.reddit.com/r/MachineLearning/comments/c8jswa/discussion_had_an_idea_want_to_know_if_its/,dragomen747180,1562129096,"&amp;#x200B;

forewarning I am out of my element with this idea 

I'm building a lichtenberg machine DIY from a microwave power transformer, i was wondering if its possible to hook up like an Arduino and a web cam or some type of camera and use machine learning to teach the ai lighting patterns and eventually get it to create its own patterns based off what i feed it Litchenberg wise. now please don't flame me im not an expert on anything AI so maybe Arduino isn't the way to go. preferably would like to do this in Python. if there's any resource or constructive input id be willing to read.",7,7
162,2019-7-3,2019,7,3,14,c8jxsa,[D]I'm trying to implement 'Born Again Neural Networks' by T.Furlanello.,https://www.reddit.com/r/MachineLearning/comments/c8jxsa/dim_trying_to_implement_born_again_neural/,crackitr,1562130056," Hi, I'm trying to implement 'Born Again Neural Networks(BAN)' by T.Furlanello([https://arxiv.org/abs/1805.04770](https://arxiv.org/abs/1805.04770)), and I have some questions. Can anybody help with this please?

If you have read some papers on Knowledge Distillation, you would know that some papers released before BAN used so called temperature. In those papers, the logits were divided with temperature(usually positive integer) and then these outputs were gone through within softmax.

In BAN paper, however, the authour didn't mention on temperature. So I didn't stabilize the logits (in other words, I set temperature as 1), but I found that I failed. There were no dramatic difference between Original network and Distilled Network. I guess if i just set temperature as 1 and if the networks overfits, the output distribution wouldn't provide a meaningful 'dark knowledge'..

for example, there would no difference between \[1, 0, 0, 0,0\] and \[0.999, 0.000 ..., 0.000 ..., 0.000 ..., 0.000 ...\]

so.. do i have to apply temperature even though the author didn't mention on the paper? I'm wondering if I can succeed without applying temperature. Thank you for reading.",2,4
163,2019-7-3,2019,7,3,14,c8k05z,Any suggestions what one should do to learn above basics things in ML with JS?,https://www.reddit.com/r/MachineLearning/comments/c8k05z/any_suggestions_what_one_should_do_to_learn_above/,Pradhumna-Pancholi,1562130514,,0,1
164,2019-7-3,2019,7,3,14,c8k1bi,Techniques and Applications Of Machine Learning | Machine Learning Training In Salem,https://www.reddit.com/r/MachineLearning/comments/c8k1bi/techniques_and_applications_of_machine_learning/,livewireindia,1562130734,,0,1
165,2019-7-3,2019,7,3,14,c8k71h,[D] Teacher-Student training situation with CNN-FC,https://www.reddit.com/r/MachineLearning/comments/c8k71h/d_teacherstudent_training_situation_with_cnnfc/,Lewba,1562131857,I've been asked to convert a fully-trained CNN to a simple FC network with fixed architecture (it'll be used on a small chip if I remember correctly). They understand the classification performance will drop but it needs to be done anyway. I've set up the student network such that it just takes the flattened image as the input but I'm unsure what my targets are. I have the data the teacher network was trained on so I guess I can train the student using those inputs with the correspoding teacher output (rather than one-hot targets in the dataset). But my real question is can I just generate random input images and use whatever the teacher outputs as a target for the student to train on? Is that what is usually done to generate a lot of training data for the student network?,10,5
166,2019-7-3,2019,7,3,14,c8k8ry,[Free Course -Limit Time ]  Artificial Intelligence Concept -AI 101 | UpWorkDownload.xyz,https://www.reddit.com/r/MachineLearning/comments/c8k8ry/free_course_limit_time_artificial_intelligence/,upworknepal,1562132190,[removed],0,1
167,2019-7-3,2019,7,3,14,c8k9ln,Automotive - LHD S.p.A. - Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/c8k9ln/automotive_lhd_spa_telescopic_forks/,Richa_11,1562132355," 

LHD Group is leader in design and build Automotive movement systems with the Atena series. We Customize and Build ad-hoc solutions for our customers.

![img](slpr13nrz0831 ""Automotive - LHD S.p.A. - Telescopic Forks"")

Due to globalization, the automotive supply chain should focus on exploring innovative methods to reduce operating costs, lead times and inventory to sustain their growth rate in market.

To know more click here,

[http://lhd.co.com/industry-applications/automotive/](http://lhd.co.com/industry-applications/automotive/)",0,1
168,2019-7-3,2019,7,3,14,c8kdrh,[1907.01470] Augmenting Self-attention with Persistent Memory,https://www.reddit.com/r/MachineLearning/comments/c8kdrh/190701470_augmenting_selfattention_with/,HigherTopoi,1562133180,,3,10
169,2019-7-3,2019,7,3,15,c8kgoi,"[discussion] i have an idea read below, not sure if its possible or how to approach doing this.",https://www.reddit.com/r/MachineLearning/comments/c8kgoi/discussion_i_have_an_idea_read_below_not_sure_if/,dragomen747180,1562133785," 

forewarning I am out of my element with this idea

I'm  building a lichtenberg machine DIY from a microwave power transformer, i  was wondering if its possible to hook up like an Arduino and a web cam  or some type of camera and use machine learning to teach the ai lighting  patterns and eventually get it to create its own patterns based off  what i feed it Litchenberg wise. now please don't flame me im not an  expert on anything AI so maybe Arduino isn't the way to go. preferably  would like to do this in Python. if there's any resource or constructive  input id be willing to read.",0,1
170,2019-7-3,2019,7,3,15,c8kkmt,Augmented Analytics Software: Frustration Free!,https://www.reddit.com/r/MachineLearning/comments/c8kkmt/augmented_analytics_software_frustration_free/,ElegantMicroWebIndia,1562134592,,0,1
171,2019-7-3,2019,7,3,16,c8kyyz,Looking for help in my university research!,https://www.reddit.com/r/MachineLearning/comments/c8kyyz/looking_for_help_in_my_university_research/,po3na4skld,1562137610,[removed],0,1
172,2019-7-3,2019,7,3,16,c8l1vm,[R] Looking for help in my university research!,https://www.reddit.com/r/MachineLearning/comments/c8l1vm/r_looking_for_help_in_my_university_research/,po3na4skld,1562138211,"Hello Reddit, I'm a student that getting masters degree in university. I do medical research about hand skin diseases (dermatitis, dermatosis, psoriasis). I'm using neural networks for that and unfortunately I need a lot of examples of hands, so for that I ask Reddit to help me in my research! If you wanted to help me or you have any questions about research, we can talk in PM. I added example of hand's photo to this post. Please send your photos as file that they will not lose their quality. Thanks for attention.

*Processing img 4vedqrggg1831...*",8,3
173,2019-7-3,2019,7,3,16,c8l3yl,[R] Learning Belief Representations for Imitation Learning in POMDPs (UAI 2019),https://www.reddit.com/r/MachineLearning/comments/c8l3yl/r_learning_belief_representations_for_imitation/,inarrears,1562138651,,1,2
174,2019-7-3,2019,7,3,16,c8l6qe,"Facebook open-sources DLRM, a deep learning recommendation model",https://www.reddit.com/r/MachineLearning/comments/c8l6qe/facebook_opensources_dlrm_a_deep_learning/,fulcrum_xyz,1562139276,[removed],0,1
175,2019-7-3,2019,7,3,17,c8llho,[Free + Online] Workshop on Getting Started with Kaggle using fast.ai + a Beginner friendly Comp,https://www.reddit.com/r/MachineLearning/comments/c8llho/free_online_workshop_on_getting_started_with/,init__27,1562142810,[removed],0,1
176,2019-7-3,2019,7,3,17,c8lnpo,News Thank you later.,https://www.reddit.com/r/MachineLearning/comments/c8lnpo/news_thank_you_later/,PuzzleheadedKey5,1562143353,,0,1
177,2019-7-3,2019,7,3,17,c8lqem,"Hi, I can find the following subtitle or transcript of the following podcasts: I mean the original subtitle, not the subtitle that YouTube produces automatically.",https://www.reddit.com/r/MachineLearning/comments/c8lqem/hi_i_can_find_the_following_subtitle_or/,Doctor_who1,1562143992,[removed],0,1
178,2019-7-3,2019,7,3,17,c8lriy,DataScience Digest - Issue #18,https://www.reddit.com/r/MachineLearning/comments/c8lriy/datascience_digest_issue_18/,flyelephant,1562144258,,0,1
179,2019-7-3,2019,7,3,17,c8lrj4,[R] The Ramanujan Machine: Automatically Generated Conjectures on Fundamental Constants,https://www.reddit.com/r/MachineLearning/comments/c8lrj4/r_the_ramanujan_machine_automatically_generated/,DoronHaviv12,1562144259,"We present the Ramanujan Machine, an automatic way to generate conjectures on mathematical constants. Throughout history, discovering mathematical formulas relating to fundamental constants has been scarce and sporadic, and was always a result of previously known truths about the constants. The Ramanujan machine approach is able to generate completely new formulas, which constitute new conjectures, and can reveal new truths about the constants. The Ramanujan Machine's algorithms are based on matching numerical values, and therefore doesn't need any prior knowledge on the constant.   


[Two examples of new formulas found using the Ramanujan Machine](https://i.redd.it/7l19agh9dy731.png)

&amp;#x200B;

Sounds exciting? Go to our website  [http://www.ramanujanmachine.com/](http://www.ramanujanmachine.com/). Similar to global efforts in mathematics such as GIMPS (Great Internet Mersenne Prime Search), we launch a new initiative dedicated for finding new formulas for fundamental constant. You can either donate computational power to find new formulas, provide proofs for conjectured formulas or come-up with completely new algorithms for the Ramanujan Machine.

&amp;#x200B;

Arxiv link:  [https://arxiv.org/pdf/1907.00205.pdf](https://arxiv.org/pdf/1907.00205.pdf)   
SciHive link: [https://www.scihive.org/paper/1907.00205](https://www.scihive.org/paper/1907.00205)",63,248
180,2019-7-3,2019,7,3,18,c8m5st,[D] Is someone aware of the SOA for summarizing News articles ?,https://www.reddit.com/r/MachineLearning/comments/c8m5st/d_is_someone_aware_of_the_soa_for_summarizing/,amil123123,1562147468,"Hey all,

I was thinking of working on a college project regarding summarizing news articles to create a short summary as well as headlines for the same article.

Apologies but I am unable to find SOA for projects in this domain. Would someone be kind enough to share some reference articles or approaches that come to their mind?",13,3
181,2019-7-3,2019,7,3,19,c8ma5n,"[1905.09418] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",https://www.reddit.com/r/MachineLearning/comments/c8ma5n/190509418_analyzing_multihead_selfattention/,Sinkencronge,1562148346,,2,17
182,2019-7-3,2019,7,3,19,c8ma7c,"[D] The Analytics Edge Book by Allison K. O'Hair, Dimitris Bertsimas, and William R. Pulleyblank",https://www.reddit.com/r/MachineLearning/comments/c8ma7c/d_the_analytics_edge_book_by_allison_k_ohair/,mvsanjeeth,1562148356,Can anyone direct me to an online copy of this book? Like a PDF.,1,0
183,2019-7-3,2019,7,3,19,c8mil8,Machine learning allows quantum mechanics to be efficiently applied to molecular simulations,https://www.reddit.com/r/MachineLearning/comments/c8mil8/machine_learning_allows_quantum_mechanics_to_be/,AtomicOrbital,1562150098,,0,1
184,2019-7-3,2019,7,3,20,c8n6zg,The Innovative Applications of AI in Healthcare,https://www.reddit.com/r/MachineLearning/comments/c8n6zg/the_innovative_applications_of_ai_in_healthcare/,Verma_RJ,1562154981,,0,1
185,2019-7-3,2019,7,3,21,c8n93k,How to make a game bot for a game that is not included in OpenAI GYM environment,https://www.reddit.com/r/MachineLearning/comments/c8n93k/how_to_make_a_game_bot_for_a_game_that_is_not/,skyboy122,1562155368,[removed],0,1
186,2019-7-3,2019,7,3,21,c8nnhq,Coming up with new architectures,https://www.reddit.com/r/MachineLearning/comments/c8nnhq/coming_up_with_new_architectures/,SonOfDamage,1562157901,[removed],0,1
187,2019-7-3,2019,7,3,21,c8nrez,NN stuck on 12% accuracy and unable to improve,https://www.reddit.com/r/MachineLearning/comments/c8nrez/nn_stuck_on_12_accuracy_and_unable_to_improve/,Fengax,1562158565,[removed],0,1
188,2019-7-3,2019,7,3,21,c8nsc4,[D] Coming up with new architectures,https://www.reddit.com/r/MachineLearning/comments/c8nsc4/d_coming_up_with_new_architectures/,SonOfDamage,1562158722,"I  was wondering about how people come up with new architectures for neural nets. I am currently working on an object detection problem, and  the models that are SOTA are huge.

With  models that have more than 500 layers and millions of parameters, how  does one come up with an architecture that is better than the existing  ones? I know that papers are mostly written around innovative 'concept'  blocks (like skip connections or a feature pyramid block), so are  researchers just iterating on all possible combinations of blocks to  come up with an answer?",5,7
189,2019-7-3,2019,7,3,22,c8nuim,Machine Learning A-Z&amp;#x2122;: Hands-On Python &amp; R In Data Science,https://www.reddit.com/r/MachineLearning/comments/c8nuim/machine_learning_azx2122_handson_python_r_in_data/,funaf2018,1562159063,,0,1
190,2019-7-3,2019,7,3,22,c8o0yn,Incoming Stat. PhD Without Experience - How to Prepare for ML?,https://www.reddit.com/r/MachineLearning/comments/c8o0yn/incoming_stat_phd_without_experience_how_to/,mathematiciannoob,1562160104,[removed],0,1
191,2019-7-3,2019,7,3,23,c8orfq,Does anyone use Machine Learning for equipment reliability and industrial maintenance prediction?,https://www.reddit.com/r/MachineLearning/comments/c8orfq/does_anyone_use_machine_learning_for_equipment/,abdeljalil73,1562164185,[removed],0,1
192,2019-7-4,2019,7,4,0,c8pfsn,Best Python Framework And in Demand || Learn Flask Python From Scratch Web Programming Basics|| Python Macroframework || Tech Punjabi,https://www.reddit.com/r/MachineLearning/comments/c8pfsn/best_python_framework_and_in_demand_learn_flask/,_learn_to_earn,1562167678,,0,1
193,2019-7-4,2019,7,4,0,c8pngk,Concerns on Social Media Over Google ML Patents,https://www.reddit.com/r/MachineLearning/comments/c8pngk/concerns_on_social_media_over_google_ml_patents/,Yuqing7,1562168738,,0,2
194,2019-7-4,2019,7,4,0,c8po94,Web dev interested in learning machine learning: routes suggested? Applications to web/mobile app dev? AR/VR? Also is it still valuable to learn given the speed of advancements such as AlphaGo Zero?,https://www.reddit.com/r/MachineLearning/comments/c8po94/web_dev_interested_in_learning_machine_learning/,UX_love,1562168850,[removed],0,1
195,2019-7-4,2019,7,4,0,c8pqkj,"Simple Questions Thread July 03, 2019",https://www.reddit.com/r/MachineLearning/comments/c8pqkj/simple_questions_thread_july_03_2019/,AutoModerator,1562169181,[removed],0,1
196,2019-7-4,2019,7,4,0,c8psc6,Graphics card vs vpu,https://www.reddit.com/r/MachineLearning/comments/c8psc6/graphics_card_vs_vpu/,NormalNoit,1562169423,[removed],0,1
197,2019-7-4,2019,7,4,1,c8pwkn,"[D] MNIST Transfer learning: Training on 28x28, then inference on 50x50",https://www.reddit.com/r/MachineLearning/comments/c8pwkn/d_mnist_transfer_learning_training_on_28x28_then/,arnokha,1562170008,"Given a CNN trained on MNIST, can I easily build a similar network that recognized those digits anywhere in a 50x50 image?

&amp;#x200B;

Phrased another way, if I had a CNN that recognized 28x28 MNIST digits, is there a known method to classify those MNIST digits if they appeared anywhere within a 50x50 image (the digits themselves would still be 28x28).

   Assume the last layer is fully connected + softmax. The convolutional layers should remain unchanged, so this question breaks down to easily creating a dense layer that works for 50x50 from the one that worked with 28x28.

&amp;#x200B;

Why I think this is important: We can achieve more specific labeling with smaller image sizes, cropping out things like eyes and noses and being able to recognize them in new places in an image. Also, this would potentially reduce the amount and variety of training data needed (we wouldn't have to create new 50x50 images with the 28x28 images tiled in different locations).",7,3
198,2019-7-4,2019,7,4,1,c8pyre,TensorFlow Lite Ported to Arduino by Adafruit,https://www.reddit.com/r/MachineLearning/comments/c8pyre/tensorflow_lite_ported_to_arduino_by_adafruit/,blinka_friendlysnake,1562170314,,0,1
199,2019-7-4,2019,7,4,1,c8q7f4,Voice Trainer,https://www.reddit.com/r/MachineLearning/comments/c8q7f4/voice_trainer/,afton_shickle,1562171509,[removed],0,1
200,2019-7-4,2019,7,4,2,c8qp0u,[R] Low accuracy on small English dataset using XLNet,https://www.reddit.com/r/MachineLearning/comments/c8qp0u/r_low_accuracy_on_small_english_dataset_using/,huseinzol05,1562173836,"I compared accuracies on small dataset, got 2 classes, which are `negative` and `positive`, each class has 5k of sentences. You can check it here [english-polarity](https://github.com/huseinzol05/NLP-Models-Tensorflow/tree/master/text-classification/data). All notebooks finetune on single Tesla V100, 100 max length of sentences. 80% to train, 20% to test. All notebooks do early stopping with 3 patients, and I will pick highest test accuracy as the benchmark.

1. [BERT English Base](https://github.com/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification/71.transfer-learning-bert-base.ipynb), test accuracy 81%.
2. [GPT2 345M](https://github.com/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification/65.transfer-learning-gpt2.ipynb), test accuracy 79%.
3. [XLnet Large](https://github.com/huseinzol05/NLP-Models-Tensorflow/blob/master/text-classification/72.xlnet-large.ipynb), test accuracy 51%.

I am not sure what is wrong here maybe not follow an exact batch size and max length?, I doubled check all the code to transform into notebooks based, everything looks same, just a bit tweak to make it notebook-able. Great model and repository, https://github.com/zihangdai/xlnet!",15,12
201,2019-7-4,2019,7,4,2,c8qrey,Machine Learning Basics - The three way of learning,https://www.reddit.com/r/MachineLearning/comments/c8qrey/machine_learning_basics_the_three_way_of_learning/,lukescriptwalker,1562174157,[removed],0,1
202,2019-7-4,2019,7,4,2,c8quix,Opensubtitles datasets,https://www.reddit.com/r/MachineLearning/comments/c8quix/opensubtitles_datasets/,wisscool,1562174556,[removed],0,1
203,2019-7-4,2019,7,4,2,c8r1i2,ActiveStereoNet: The first deep learning solution for active stereo systems,https://www.reddit.com/r/MachineLearning/comments/c8r1i2/activestereonet_the_first_deep_learning_solution/,posnererez,1562175486,,0,1
204,2019-7-4,2019,7,4,2,c8r8rt,[R] - ActiveStereoNet: The first deep learning solution for active stereo systems,https://www.reddit.com/r/MachineLearning/comments/c8r8rt/r_activestereonet_the_first_deep_learning/,posnererez,1562176458,,0,2
205,2019-7-4,2019,7,4,3,c8rdvq,"[D] Lawyer: even if the patent is granted, you can still use it as long as Google does not sue you. It is possible that Google can wait until your company grows up and come back to sue you",https://www.reddit.com/r/MachineLearning/comments/c8rdvq/d_lawyer_even_if_the_patent_is_granted_you_can/,SilentTheme,1562177126,"Update on the Google Dropout Patent discussion from last week:

[https://www.reddit.com/r/MachineLearning/comments/c5mdm5/d\_googles\_patent\_on\_dropout\_just\_went\_active\_today/](https://www.reddit.com/r/MachineLearning/comments/c5mdm5/d_googles_patent_on_dropout_just_went_active_today/)

&amp;#x200B;

From this article: [https://medium.com/syncedreview/concerns-on-social-media-over-google-ml-patents-fadeb5a0b2e9](https://medium.com/syncedreview/concerns-on-social-media-over-google-ml-patents-fadeb5a0b2e9)

&gt;An attorney with a focus on corporate and securities law who asked not to be identified told Synced that even if the patent is granted, you can still use it as long as Google does not sue you. It is possible that Google can wait until your company grows up and come back to sue you, which could potentially make you or your institution liable for consequences and damages, not Google.",94,304
206,2019-7-4,2019,7,4,3,c8rk19,GPS vs AI: The Challenges of Losing Satellite Signal | Roborace,https://www.reddit.com/r/MachineLearning/comments/c8rk19/gps_vs_ai_the_challenges_of_losing_satellite/,AIthatDrives,1562177932,,0,1
207,2019-7-4,2019,7,4,3,c8rmd0,AskML: Is upsampling and downsampling part of bagging?,https://www.reddit.com/r/MachineLearning/comments/c8rmd0/askml_is_upsampling_and_downsampling_part_of/,BACP_,1562178229,[removed],0,1
208,2019-7-4,2019,7,4,3,c8rva0,Baidu PaddlePaddle DL Framework &amp; Huawei Kirin SoC: A Formidable Partnership,https://www.reddit.com/r/MachineLearning/comments/c8rva0/baidu_paddlepaddle_dl_framework_huawei_kirin_soc/,Yuqing7,1562179425,,0,1
209,2019-7-4,2019,7,4,3,c8ryre,How to efficiently share and compare model outputs?,https://www.reddit.com/r/MachineLearning/comments/c8ryre/how_to_efficiently_share_and_compare_model_outputs/,kinghuang,1562179875,[removed],0,1
210,2019-7-4,2019,7,4,4,c8sbh8,[D] Word embeddings for categorical variables?,https://www.reddit.com/r/MachineLearning/comments/c8sbh8/d_word_embeddings_for_categorical_variables/,aeppelsaeft,1562181576,"I am working on a classification problem with a data set containing numerical as well as categorical data. A colleague of mine said that instead of encoding the categorical variables in a ""primitive way"" (label encoding, creating dummy variables etc.) he would use word2vec to get some kind of word embeddings. This would be a more realistic way of representing these variables. To me this makes no sense. If I understood correctly, for word2vec to work the words we want to embed need neighbors for there to be some kind of context. In a column of a DataFrame containing one string in each row and maybe 3 - 10 unique categories there isn't any context.  Each entry is independent from the entry in the next row. Am I missing something?

I hope I posed the question in a somewhat understandable way.

Thanks, guys.",9,3
211,2019-7-4,2019,7,4,4,c8sf0u,Deciding whether classic texts were written by the same author,https://www.reddit.com/r/MachineLearning/comments/c8sf0u/deciding_whether_classic_texts_were_written_by/,geographybuff,1562182046,"**Repository link:**

 [https://github.com/geographybuff/Text-comparison](https://github.com/geographybuff/Text-comparison) 

**Description:**

Determines whether two texts were written by the same author using a Keras neural network with 15 inputs:

1: 12 (6x2) individual attributes (frequencies of periods, commas, unique words, apostrophes, words 7 letters or more, words 10 letters or more)

2: 3 shared attributes (percentage of shared words, shared two word phrases, and shared three word phrases

Corpus is composed of four passages each from eight English-language prose authors from four continents, written over a period of 303 years. Passages range over a wide variety of subjects, including romantic novels, political science, dystopia, fantasy, and humor. Passages range from roughly 1500-7000 words in each file. Occasionally multiple chapters are included in one file so as to pass 1500 words.

George Orwell: 1984 Chapter 1, 1984 Chapter 2, Animal Farm Chapter 1, Animal Farm Chapter 2

J. R. R. Tolkien: The Hobbit Chapter 1, The Hobbit Chapter 2, The Fellowship of the Ring Foreword, The Fellowship of the Ring Prologue

Charles Dickens: Great Expectations Chapter 1, Great Expectations Chapter 2, The Detective Police Part 1, The Detective Police Part 2

Jane Austen: Pride and Prejudice Chapters 1-2, Pride and Prejudice Chapter 3, Persuasion Chapter 1, Persuasion Chapter 2

Mahatma Gandhi: Hind Swaraj Chapter 1, Hind Swaraj Chapters 2-3, A Guide To Health Chapters 1-3, A Guide to Health Chapters 4-5

Mark Twain: Tom Sawyer Chapter 1, Tom Sawyer Chapter 2, An American Vandal Abroad, Among the Spirits

John Locke: A Letter Concerning Toleration Part 1, A Letter Concerning Toleration Part 2, Some Thoughts Concerning Education Part 1, Some Thoughts Concerning Education Part 2

Thomas Hobbes: Leviathan of Man Chapters 1-2, Leviathan of Man Chapter 3, The Elements of Law Chapters 1-2, The Elements of Law Chapter 3

Also includes a heatmap correlogram showing an adjusted comparison score for the three comparison attributes, and six bar graphs for the six individual attributes.

**Questions:**

It would probably be less cluttered if I put all the texts in a separate folder. How do I properly write the import statement to find the text files within their folder?

&amp;#x200B;

I saved some of the outputs for the data prep stages by copying and pasting them to the next stage, so as not to lose 30+ minutes of my PC's compute time. Obviously this makes it more cluttered, especially in Github's native ipynb viewer which doesn't wrap text. It would be better to save the output (numpy arrays) to text files that I could just read later in the program. How do industry-scale applications save and access computational output? How can make it run more smoothly here?

&amp;#x200B;

This is my first attempt at implementing a machine learning model. Is there anything else I can improve? Anything else a recruiter would want to see to get me employed at a data science job?",0,1
212,2019-7-4,2019,7,4,4,c8slig,Jeff Hawkins: Thousand Brains Theory of Intelligence | Artificial Intelligence (AI) Podcast,https://www.reddit.com/r/MachineLearning/comments/c8slig/jeff_hawkins_thousand_brains_theory_of/,alvisanovari,1562182921,,1,1
213,2019-7-4,2019,7,4,4,c8sr2q,[N] ML algorithm predicts new materials based on analysis of word proximity in 3.3M abstracts,https://www.reddit.com/r/MachineLearning/comments/c8sr2q/n_ml_algorithm_predicts_new_materials_based_on/,greenprius,1562183637,,0,1
214,2019-7-4,2019,7,4,5,c8tc21,[D] LidarView and Deeplearning,https://www.reddit.com/r/MachineLearning/comments/c8tc21/d_lidarview_and_deeplearning/,cmillionaire9,1562186429,"[https://youtu.be/7pQGYHgvvrY](https://youtu.be/7pQGYHgvvrY)

 Visualize your multi-sensor data with [\#LidarView](https://www.youtube.com/results?search_query=%23LidarView), the open source [\#ParaView](https://www.youtube.com/results?search_query=%23ParaView) Lidar application. And thanks to our in-house Lidar SLAM, map them in 3D, be it indoor or outdoor.Here is a sequence in Lyon, nearby our Kitware Europe office: Only one VLP-16 from Velodyne Lidar, Inc. and a Gopro camera, no IMU or GPS. Automatic computation of the position/orientation of the camera relatively to the Lidar, as well as the time-sync. Showing our preferred semantic segmentation, from [\#bonnetal](https://www.youtube.com/results?search_query=%23bonnetal). Stay tuned as the 3D semantic segmentation should be available soon too.",0,2
215,2019-7-4,2019,7,4,5,c8terv,[R] Do deep neural networks learn shallow learnable examples first? (ICML 2019 Workshop paper),https://www.reddit.com/r/MachineLearning/comments/c8terv/r_do_deep_neural_networks_learn_shallow_learnable/,VinayUPrabhu,1562186791,"Dear Reddit-ML community,  
At the  [ **ICML 2019 Workshop Deep Phenomena**](https://openreview.net/group?id=ICML.cc/2019/Workshop/Deep_Phenomena) workshop, we presented our initial results addressing the question:    
Do deep neural networks learn shallow learnable examples first?

In case you have addressed this question from different perspectives or have any feedback in this regard, we'd love to hear these!  
Here are the pertinent links: (We'll be updating the repo shortly with more results)

\[Video\]  [https://youtu.be/aXB7PbJ8osQ](https://youtu.be/aXB7PbJ8osQ)  
\[Code\]  [https://github.com/karttikeya/Shallow\_to\_Deep/](https://github.com/karttikeya/Shallow_to_Deep/)   
\[ Paper\]  [https://openreview.net/forum?id=HkxHv4rn24](https://openreview.net/forum?id=HkxHv4rn24)",7,7
216,2019-7-4,2019,7,4,7,c8uc9r,A website to share open source machine learning models,https://www.reddit.com/r/MachineLearning/comments/c8uc9r/a_website_to_share_open_source_machine_learning/,bigDATAbig,1562191455,[removed],0,1
217,2019-7-4,2019,7,4,7,c8uccl,[D] Techniques to sample unbalanced multi-label datasets?,https://www.reddit.com/r/MachineLearning/comments/c8uccl/d_techniques_to_sample_unbalanced_multilabel/,parekhnish,1562191466,"I have a dataset with multi-label classification outputs. These are (fortunately) binary, so a typical output is basically a binary vector like [1, 0, 0, 1, 1]. The length of this vector is fixed.

The dataset is pretty biased towards all 0's, and since it is a multi-label output (and not like a one-hot encoding), I'm not sure what is the best way to undersample or oversample the dataset for my training epochs, since traditional stratification cannot work here.",1,1
218,2019-7-4,2019,7,4,7,c8ucsr,GTX/K80 GPU's and classical FFNN Classification problems,https://www.reddit.com/r/MachineLearning/comments/c8ucsr/gtxk80_gpus_and_classical_ffnn_classification/,starbucksresident,1562191527,"I have a classic ANN, relu single-layer, sigmoid, adam, binary\_crossentropy, accuracy etc., you know the typical ten liner Keras in ten minutes kind of thing you do in sleep.

I train on two old generation GPUs, one a K80/12 on a cloud Google Compute Engine, and the other locally with a GTX 980/4 on an ancient Xeon workstation (think 2007 Dell T5400), before anyone criticizes the Dell it is a fine machine! Both are running a recent Ubuntu LTS in roughly the same config (cuDNN/CUDA etc.), Python 3.6 and the usual assortment of libraries. Both have 32GB RAM and SSD.

Have found that with a significant number of features (once OHE'd) and a larger hidden layer the GTX 980 OOM's to be expected (4GB), the K80 is fine with 12GB, naturally also the batch size needs to be decreased on the 980, if not enough I have seen it OOM mid-epoch!!

An epoch takes about 80s with my current batch size - but what is interesting is that if I tune the parameters so that it just about fits into the 980, and then run on the K80, there is little if any speed improvement (currently running 15 epochs, timing is about the same on both GPU's).

Even if I optimize for the K80's memory increase (larger batches for instance, not features/layers) it doesn't run much faster, if any.

My thoughts here are, yes Convolutional will run faster on the K80 undoubtedly (if I were doing for example ResNet50  I am sure the 980 would be soundly beaten) - but for straightforward binary classification based upon non-related input vectors the GPU does not scale nearly as well.

&amp;#x200B;

Features shape:

before OHE (30345, 12)

after OHE (30345, 8891)

&amp;#x200B;

Note:

The 980 GPU is an EVGA 980 FTW (1x8 pin, 1 x6 pin) that can suck up 220W (I've seen it do it via nvidia-smi)  versus around 160W for a reference 980 (2 x 6-pin).... so the performance is quite a bit better than a stock 980, perhaps even 10%-20%.

I do recommend the GTX 980 for entry level work, its old, but cheap and performance in my experience is somewhere between a 1060 and 1070.",0,1
219,2019-7-4,2019,7,4,7,c8ujju,Can Neural Networks overfit the random dataset?,https://www.reddit.com/r/MachineLearning/comments/c8ujju/can_neural_networks_overfit_the_random_dataset/,ZhuBusy,1562192469,[removed],0,1
220,2019-7-4,2019,7,4,7,c8upfq,Img2LaTeX solution?,https://www.reddit.com/r/MachineLearning/comments/c8upfq/img2latex_solution/,mlforthebest,1562193299,[removed],0,1
221,2019-7-4,2019,7,4,7,c8uvk4,[D] Any book recommendation for applied linear modeling that is NOT R-based,https://www.reddit.com/r/MachineLearning/comments/c8uvk4/d_any_book_recommendation_for_applied_linear/,this_nicholas,1562194222,Looking for a book that sheds lights on ***practical*** issues of using linear modeling for prediction in real production/project scenario (without losing much mathematical rigor). Preferably the book is language agnostic or Python-based. (I found some books that seem nice but have R code like [Applied Predictive Modeling](https://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn-ebook/dp/B00K15TZU0/ref=sr_1_8?keywords=linear+modeling&amp;qid=1562194128&amp;s=books&amp;sr=1-8)),4,5
222,2019-7-4,2019,7,4,9,c8von1,"[D] Maximumizing Likelihood results in a Degenerate VAE?, aka More Variational Autoencoder Confusion",https://www.reddit.com/r/MachineLearning/comments/c8von1/d_maximumizing_likelihood_results_in_a_degenerate/,idioth,1562198957,"TL;DR I'm still confused about VAEs. My experience is indicating if I take a Beta VAE formulation and try to maximize the likelihood of some hold-out data while varying Beta, my model collapses. Please bare with me:

&amp;#x200B;

With Variational Autoencoders, we estimate an approximate likelihood by maximizing the ELBO, aka data likelihood:

log(p(x)) &gt;= L(x) = E(log p(x|z)) - KL(q(z|x)||p(z))

aka:

ELBO = L(x) = - (Distortion + Rate)

where E(logo(x|z)) is equivalent to the reconstruction loss (up to scale) and KL divergence is determined via the reparameterization trick. 

&amp;#x200B;

One interpretation of this is that the model cares about two things: Good reconstruction up to a highly compressed representation. These sort of things are good for interpreting data with respect to the latent space, and whatnot.

&amp;#x200B;

We can add a coefficient, B, to explore the trade off between Rate (KLD) and Distortion (reconstruction). That gives us the BetaVAE formulation:

E(log p(x|z)) - B \* KL(q(z|x)||p(z)).     (e.g. [https://openreview.net/forum?id=Sy2fzU9gl](https://openreview.net/forum?id=Sy2fzU9gl) )

or you can do other things to ""pin"" the rate:

E(log p(x|z)) + | C  - KL(q(z|x)||p(z)) | (e.g. [https://arxiv.org/abs/1804.03599](https://arxiv.org/abs/1804.03599) or [https://arxiv.org/pdf/1711.00464.pdf](https://arxiv.org/pdf/1711.00464.pdf) )

&amp;#x200B;

There are many papers that use VAEs, often by training the VAE and picking the point in the training that maximizes the ELBO on some hold out set (e.g. [https://www.nature.com/articles/s41592-018-0229-2.pdf](https://www.nature.com/articles/s41592-018-0229-2.pdf) )

&amp;#x200B;

**My problem:**

Let's say I take a VAE and one of the R vs D formulations and scan over Beta and plot the rate vs distortion of the held-out data for different models. I often get something like this (this is a screenshot from the ELBO pape, but I also get approximately these results):

[Fixing a Broken ELBO, fig 3a](https://i.redd.it/ziztu60pd6831.png)

The dotted line is the R vs D tradeoff at the maximum likelihood model, and this occurs when the rate drops to zero. In the case of a ""vanilla"" VAE, this means a degenerate latent space where all points represent the same thing. All points in Z are the same (i.e. N(0,1)) and I have experienced the (grotesquely named) posterior collapse. In this case the model (usually) only emits the ""average"" input unless there is some side-channel of information. This is often considered a bad thing, and L(x) is therefore only determined by the reconstruction loss from the input and the ""average"" emission. 

&amp;#x200B;

But this is the maximum likelihood model! 

&amp;#x200B;

Let's go back to the Lopez et al paper above: If I were to scan over Beta to find the maximum likelihood model and I get a collapsed latent space, I wouldn't have a model that is particularly useful in that it would not provide ""interpretable"" latents. In the context where I am performing a conditional prediction task (e.g. [https://scholar.google.com/scholar?q=variational+autoencoder+prediction](https://scholar.google.com/scholar?q=variational+autoencoder+prediction)), the VAE would emit the same value no matter the condition. 

&amp;#x200B;

Is maximum likelihood/ELBO right? One could imagine that there are alternate ways to evaluate this result (i.e. Frechet Inception Distance on generated examples). Should I use those? Should I just be using exact inference methods instead?",0,1
223,2019-7-4,2019,7,4,9,c8vtmb,[R] https://arxiv.org/abs/1811.07519 Higher-order Neural Networks for Action Recognition,https://www.reddit.com/r/MachineLearning/comments/c8vtmb/r_httpsarxivorgabs181107519_higherorder_neural/,KaiiHu,1562199742,I am delighted to announce that I have submitted our recent work to arXiv. Any feedback would be highly appreciated. [https://arxiv.org/abs/1811.07519](https://arxiv.org/abs/1811.07519),5,15
224,2019-7-4,2019,7,4,9,c8w4vl,Fourier transform using invertible neural network,https://www.reddit.com/r/MachineLearning/comments/c8w4vl/fourier_transform_using_invertible_neural_network/,engineering_doctor,1562201662,[removed],0,1
225,2019-7-4,2019,7,4,10,c8wodl,Mask Embedding in Conditional GAN for Guided Synthesis of High Resolution Images,https://www.reddit.com/r/MachineLearning/comments/c8wodl/mask_embedding_in_conditional_gan_for_guided/,ketsok,1562205040,,5,16
226,2019-7-4,2019,7,4,11,c8wxs1,[R] Fully Decoupled Neural Network Learning Using Delayed Gradients,https://www.reddit.com/r/MachineLearning/comments/c8wxs1/r_fully_decoupled_neural_network_learning_using/,penpatience,1562206674,,5,3
227,2019-7-4,2019,7,4,11,c8x418,Machine-learning algorithms can discover new things,https://www.reddit.com/r/MachineLearning/comments/c8x418/machinelearning_algorithms_can_discover_new_things/,CodePerfect,1562207799,,0,1
228,2019-7-4,2019,7,4,11,c8x6st,"[D] Maximumizing Likelihood results in a Degenerate VAE?, aka More Variational Autoencoder Confusion",https://www.reddit.com/r/MachineLearning/comments/c8x6st/d_maximumizing_likelihood_results_in_a_degenerate/,idioth,1562208302,"TL;DR I'm still confused about VAEs. My experience is indicating if I take a Beta VAE formulation and try to maximize the likelihood of some hold-out data while varying Beta, my model collapses. Please bare with me:

&amp;#x200B;

With Variational Autoencoders, we estimate an approximate likelihood by maximizing the ELBO, aka data likelihood:

log(p(x)) &gt;= L(x) = E(log p(x|z)) - KL(q(z|x)||p(z))

aka:

ELBO = L(x) = - (Distortion + Rate)

where E(logo(x|z)) is equivalent to the reconstruction loss (up to scale) and KL divergence is determined via the reparameterization trick.

&amp;#x200B;

One interpretation of this is that the model cares about two things: Good reconstruction up to a highly compressed representation. These sort of things are good for interpreting data with respect to the latent space, and whatnot.

&amp;#x200B;

We can add a coefficient, B, to explore the trade off between Rate (KLD) and Distortion (reconstruction). That gives us the BetaVAE formulation:

E(log p(x|z)) - B \* KL(q(z|x)||p(z)).     (e.g. [https://openreview.net/forum?id=Sy2fzU9gl](https://openreview.net/forum?id=Sy2fzU9gl) )

or you can do other things to ""pin"" the rate:

E(log p(x|z)) + | C  - KL(q(z|x)||p(z)) | (e.g. [https://arxiv.org/abs/1804.03599](https://arxiv.org/abs/1804.03599) or [https://arxiv.org/pdf/1711.00464.pdf](https://arxiv.org/pdf/1711.00464.pdf) )

&amp;#x200B;

There are many papers that use VAEs, often by training the VAE and picking the point in the training that maximizes the ELBO on some hold out set (e.g. [https://www.nature.com/articles/s41592-018-0229-2.pdf](https://www.nature.com/articles/s41592-018-0229-2.pdf) )

&amp;#x200B;

**My problem:**

Let's say I take a VAE and one of the R vs D formulations and scan over Beta and plot the rate vs distortion of the held-out data for different models. I often get something like this (this is a screenshot from the ELBO pape, but I also get approximately these results):

[Fixing a Broken ELBO, Fig 3a](https://i.redd.it/i1rmojvr97831.png)

&amp;#x200B;

The dotted line is the R vs D tradeoff at the maximum likelihood model, and this occurs when the rate drops to zero. In the case of a ""vanilla"" VAE, this means a degenerate latent space where all points represent the same thing. All points in Z are the same (i.e. N(0,1)) and I have experienced the (grotesquely named) posterior collapse. In this case the model (usually) only emits the ""average"" input unless there is some side-channel of information. This is often considered a bad thing, and L(x) is therefore only determined by the reconstruction loss from the input and the ""average"" emission.

&amp;#x200B;

But this is the maximum likelihood model!

&amp;#x200B;

Let's go back to the Lopez et al paper above: If I were to scan over Beta to find the maximum likelihood model and I get a collapsed latent space, I wouldn't have a model that is particularly useful in that it would not provide ""interpretable"" latents. In the context where I am performing a conditional prediction task (e.g. [https://scholar.google.com/scholar?q=variational+autoencoder+prediction](https://scholar.google.com/scholar?q=variational+autoencoder+prediction)), the VAE would emit the same value no matter the condition.

&amp;#x200B;

Is maximum likelihood/ELBO right? One could imagine that there are alternate ways to evaluate this result (i.e. Frechet Inception Distance on generated examples). Should I use those? Should I just be using exact inference methods instead?",0,1
229,2019-7-4,2019,7,4,11,c8x7ff,"[D] Maximizing Likelihood results in a Degenerate VAE?, aka More Variational Autoencoder Confusion",https://www.reddit.com/r/MachineLearning/comments/c8x7ff/d_maximizing_likelihood_results_in_a_degenerate/,idioth,1562208412,"TL;DR I'm still confused about VAEs. My experience is indicating if I take a Beta VAE formulation and try to maximize the likelihood of some hold-out data while varying Beta, my model collapses. Please bare with me:

With Variational Autoencoders, we estimate an approximate likelihood by maximizing the ELBO, aka data likelihood:

log(p(x)) &gt;= L(x) = E(log p(x|z)) - KL(q(z|x)||p(z))

aka:

ELBO = L(x) = - (Distortion + Rate)

where E(logo(x|z)) is equivalent to the reconstruction loss (up to scale) and KL divergence is determined via the reparameterization trick.

One interpretation of this is that the model cares about two things: Good reconstruction up to a highly compressed representation. These sort of things are good for interpreting data with respect to the latent space, and whatnot.

We can add a coefficient, B, to explore the trade off between Rate (KLD) and Distortion (reconstruction). That gives us the BetaVAE formulation:

E(log p(x|z)) - B \* KL(q(z|x)||p(z)). (e.g. [https://openreview.net/forum?id=Sy2fzU9gl](https://openreview.net/forum?id=Sy2fzU9gl) )

or you can do other things to ""pin"" the rate:

E(log p(x|z)) + | C - KL(q(z|x)||p(z)) | (e.g. [https://arxiv.org/abs/1804.03599](https://arxiv.org/abs/1804.03599) or [https://arxiv.org/pdf/1711.00464.pdf](https://arxiv.org/pdf/1711.00464.pdf) )

There are many papers that use VAEs, often by training the VAE and picking the point in the training that maximizes the ELBO on some hold out set (e.g. [https://www.nature.com/articles/s41592-018-0229-2.pdf](https://www.nature.com/articles/s41592-018-0229-2.pdf) )

**My problem:**

Let's say I take a VAE and one of the R vs D formulations and scan over Beta and plot the rate vs distortion of the held-out data for different models. I often get something like this (this is a screenshot from the ELBO pape, but I also get approximately these results):

[Fixing a Broken ELBO, Fig 3a](https://i.redd.it/65akz0i4a7831.png)

The dotted line is the R vs D tradeoff at the maximum likelihood model, and this occurs when the rate drops to zero. In the case of a ""vanilla"" VAE, this means a degenerate latent space where all points represent the same thing. All points in Z are the same (i.e. N(0,1)) and I have experienced the (grotesquely named) posterior collapse. In this case the model (usually) only emits the ""average"" input unless there is some side-channel of information. This is often considered a bad thing, and L(x) is therefore only determined by the reconstruction loss from the input and the ""average"" emission.

But this is the maximum likelihood model!

Let's go back to the Lopez et al paper above: If I were to scan over Beta to find the maximum likelihood model and I get a collapsed latent space, I wouldn't have a model that is particularly useful in that it would not provide ""interpretable"" latents. In the context where I am performing a conditional prediction task (e.g. [https://scholar.google.com/scholar?q=variational+autoencoder+prediction](https://scholar.google.com/scholar?q=variational+autoencoder+prediction)), the VAE would emit the same value no matter the condition.

Is maximum likelihood/ELBO right? One could imagine that there are alternate ways to evaluate this result (i.e. Frechet Inception Distance on generated examples). Should I use those? Should I just be using exact inference methods instead?",0,6
230,2019-7-4,2019,7,4,11,c8xa03,Opinion wanted on UCLA Machine Learning PhD classes,https://www.reddit.com/r/MachineLearning/comments/c8xa03/opinion_wanted_on_ucla_machine_learning_phd/,Argan12345,1562208886,"Hey,

&amp;#x200B;

I've got a decent academic background in calculus, stats/probability, optimization and general computer science and I'm looking for a high level machine learning engineer job. I'm taking the Coursera Andrew Ng class this summer to get an intro to the ML vocabulary and methods. 

&amp;#x200B;

Wanted to know how relevant these two classes are, especially for an industry job:

  
M231A - Pattern Recognition and Machine Learning 

 [http://www.stat.ucla.edu/\~sczhu/Courses/UCLA/Stat\_231/Stat\_231.html](http://www.stat.ucla.edu/~sczhu/Courses/UCLA/Stat_231/Stat_231.html) 

&amp;#x200B;

M232A - Statistical Modeling and Learning in Vision and Cognition 

 [http://www.stat.ucla.edu/\~sczhu/Courses/UCLA/Stat\_232A/Stat\_232A.html](http://www.stat.ucla.edu/~sczhu/Courses/UCLA/Stat_232A/Stat_232A.html)",0,1
231,2019-7-4,2019,7,4,13,c8xysp,[P] Supervised Generative Dog Net - Kaggle Kernel,https://www.reddit.com/r/MachineLearning/comments/c8xysp/p_supervised_generative_dog_net_kaggle_kernel/,MediumInterview,1562213325,"I just found this really nice tutorial about generative networks. It offers a really intuitive overview and look at the difference between memorizing and generalizing on a dataset.

Link:  [https://www.kaggle.com/cdeotte/supervised-generative-dog-net](https://www.kaggle.com/cdeotte/supervised-generative-dog-net)",0,2
232,2019-7-4,2019,7,4,13,c8xzio,kmeans1d: Globally Optimal Efficient 1D k-means,https://www.reddit.com/r/MachineLearning/comments/c8xzio/kmeans1d_globally_optimal_efficient_1d_kmeans/,dstein64,1562213447,,0,1
233,2019-7-4,2019,7,4,13,c8y5yp,Anyone read the Big 9 by Amy Web?,https://www.reddit.com/r/MachineLearning/comments/c8y5yp/anyone_read_the_big_9_by_amy_web/,tryingToStuff,1562214620,[removed],0,1
234,2019-7-4,2019,7,4,15,c8zf14,"[D] Was this quake AI a little too artificial? Nature-published research accused of boosting accuracy by mixing training, testing data",https://www.reddit.com/r/MachineLearning/comments/c8zf14/d_was_this_quake_ai_a_little_too_artificial/,milaworld,1562223244,"I usually prefer not to post [articles](https://www.theregister.co.uk/2019/07/03/nature_study_earthquakes/) in the popular press, but since this topic about the recent [Deep Learning Earthquake](https://www.nature.com/articles/s41586-018-0438-y) publication on [Nature](https://www.nature.com/articles/s41586-018-0438-y) has gathered considerable upvotes and discussion only on this subreddit (not anywhere else AFAIK), I'm glad some of the press has also picked up on it to make the broader community aware of our concerns.

[The Register](https://www.theregister.co.uk/2019/07/03/nature_study_earthquakes/) has published an article ""Was this quake AI a little too artificial? Nature-published research accused of boosting accuracy by mixing training, testing data"" ([link](https://www.theregister.co.uk/2019/07/03/nature_study_earthquakes/)) summarizing the ordeal. The author, [Katyanna Quach](https://twitter.com/katyanna_q) IMO is a well-informed reporter to cover machine learning, so I believe she probably used this subreddit to source some of her leads.

Some previous discussion on this subreddit:

[Misuse of Deep Learning in Nature Journals Earthquake Aftershock Paper](https://redd.it/c4ylga)

and follow up thread:

[One neuron is more informative than a deep neural network for aftershock pattern forecasting](https://redd.it/c5is9e)

I would like to thank everyone who contributed to the discussion since I think it is important to call these papers out, and make what is wrong known to the wider community outside of research.",13,106
235,2019-7-4,2019,7,4,16,c8zmfx,Can I use images from pinterest as a dataset for a project?,https://www.reddit.com/r/MachineLearning/comments/c8zmfx/can_i_use_images_from_pinterest_as_a_dataset_for/,khawarizmy,1562224763,[removed],0,1
236,2019-7-4,2019,7,4,16,c8zmzk,Tools And Technologies Are Used To Build NLI Applications,https://www.reddit.com/r/MachineLearning/comments/c8zmzk/tools_and_technologies_are_used_to_build_nli/,varalaxmi1,1562224876,,0,1
237,2019-7-4,2019,7,4,16,c8zsnc,Deepnude working link,https://www.reddit.com/r/MachineLearning/comments/c8zsnc/deepnude_working_link/,aldo6581,1562226128,[removed],0,1
238,2019-7-4,2019,7,4,16,c8zw70,[R] [https://arxiv.org/abs/1907.02052] (https://arxiv.org/abs/1907.02052) Patent Claim Generation by Fine-Tuning OpenAI GPT-2,https://www.reddit.com/r/MachineLearning/comments/c8zw70/r_httpsarxivorgabs190702052/,js_lee,1562226933,"In this work, we focus on fine-tuning an OpenAI GPT-2 pre-trained model for generating patent claims. GPT-2 has demonstrated impressive efficacy of pre-trained language models on various tasks, particularly coherent text generation. Patent claim language itself has rarely been explored in the past and poses a unique challenge. We are motivated to generate coherent patent claims automatically so that augmented inventing might be viable someday. In our implementation, we identified a unique language structure in patent claims and leveraged its implicit human annotations. We investigated the fine-tuning process by probing the first 100 steps and observing the generated text at each step. Based on both conditional and unconditional random sampling, we analyze the overall quality of generated patent claims. Our contributions include: (1) being the first to generate patent claims by machines and being the first to apply GPT-2 to patent claim generation, (2) providing various experiment results for qualitative analysis and future research, (3) proposing a new sampling approach for text generation, and (4) building an e-mail bot for future researchers to explore the fine-tuned GPT-2 model further.",1,1
239,2019-7-4,2019,7,4,17,c8zxio,[R] https://arxiv.org/abs/1907.02052 Patent Claim Generation by Fine-Tuning OpenAI GPT-2,https://www.reddit.com/r/MachineLearning/comments/c8zxio/r_httpsarxivorgabs190702052_patent_claim/,js_lee,1562227234,"In this work, we focus on fine-tuning an OpenAI GPT-2 pre-trained model for generating patent claims. GPT-2 has demonstrated impressive efficacy of pre-trained language models on various tasks, particularly coherent text generation. Patent claim language itself has rarely been explored in the past and poses a unique challenge. We are motivated to generate coherent patent claims automatically so that augmented inventing might be viable someday. In our implementation, we identified a unique language structure in patent claims and leveraged its implicit human annotations. We investigated the fine-tuning process by probing the first 100 steps and observing the generated text at each step. Based on both conditional and unconditional random sampling, we analyze the overall quality of generated patent claims. Our contributions include: (1) being the first to generate patent claims by machines and being the first to apply GPT-2 to patent claim generation, (2) providing various experiment results for qualitative analysis and future research, (3) proposing a new sampling approach for text generation, and (4) building an e-mail bot for future researchers to explore the fine-tuned GPT-2 model further.",1,1
240,2019-7-4,2019,7,4,17,c8zygj,[R] (arXiv) Patent Claim Generation by Fine-Tuning OpenAI GPT-2,https://www.reddit.com/r/MachineLearning/comments/c8zygj/r_arxiv_patent_claim_generation_by_finetuning/,js_lee,1562227433,"[https://arxiv.org/abs/1907.02052](https://arxiv.org/abs/1907.02052)
In this work, we focus on fine-tuning an OpenAI GPT-2 pre-trained model for generating patent claims. GPT-2 has demonstrated impressive efficacy of pre-trained language models on various tasks, particularly coherent text generation. Patent claim language itself has rarely been explored in the past and poses a unique challenge. We are motivated to generate coherent patent claims automatically so that augmented inventing might be viable someday. In our implementation, we identified a unique language structure in patent claims and leveraged its implicit human annotations. We investigated the fine-tuning process by probing the first 100 steps and observing the generated text at each step. Based on both conditional and unconditional random sampling, we analyze the overall quality of generated patent claims. Our contributions include: (1) being the first to generate patent claims by machines and being the first to apply GPT-2 to patent claim generation, (2) providing various experiment results for qualitative analysis and future research, (3) proposing a new sampling approach for text generation, and (4) building an e-mail bot for future researchers to explore the fine-tuned GPT-2 model further.",3,2
241,2019-7-4,2019,7,4,17,c90304,[D] Worst CVPR 2019 papers,https://www.reddit.com/r/MachineLearning/comments/c90304/d_worst_cvpr_2019_papers/,TreeNetworks,1562228531,"Has anyone (who understand Chinese) seen the Zhihu thread about the worst CVPR 2019 accepted papers? Here it is -  [https://www.zhihu.com/question/327139341](https://www.zhihu.com/question/327139341). I find it quite controversial and interesting at the same time. There are allegations that people use all kinds of unscrupulous ways to publish or get their papers accepted at CVPR. These include having big-name researchers to ""say hello"" to the supposedly anonymous reviewers (i.e., coaxing them into giving good paper ratings), blatantly plagiarizing ideas/contributions from previous papers, deliberately lowering baseline results, and so on.",59,211
242,2019-7-4,2019,7,4,17,c904r9,LIGHTWEIGHT WALL PANEL MACHINE IN PRODUCTION SITE,https://www.reddit.com/r/MachineLearning/comments/c904r9/lightweight_wall_panel_machine_in_production_site/,ada2017,1562228956,,0,1
243,2019-7-4,2019,7,4,17,c907a0,[R] https://arxiv.org/pdf/1905.11150 Radial Prediction Layer,https://www.reddit.com/r/MachineLearning/comments/c907a0/r_httpsarxivorgpdf190511150_radial_prediction/,qrctcl,1562229548,"The lab from my university just released a new paper about a new prediction layer. They also released the code to their paper!

&amp;#x200B;

Link: [https://arxiv.org/pdf/1905.11150.pdf](https://arxiv.org/pdf/1905.11150.pdf)",0,1
244,2019-7-4,2019,7,4,17,c90af7,EE/CE for AI?,https://www.reddit.com/r/MachineLearning/comments/c90af7/eece_for_ai/,123throwacc123,1562230289,[removed],0,1
245,2019-7-4,2019,7,4,18,c90l4l,[D] How to apply a different dense layer for each timestep in Keras,https://www.reddit.com/r/MachineLearning/comments/c90l4l/d_how_to_apply_a_different_dense_layer_for_each/,atif_hassan,1562232707," 

I know that adding a TimeDistributed(Dense) applies the same dense layer over all the timesteps but I wanted to know how to apply different dense layers for each timestep. The number of timesteps is not variable.

P.S.: I have seen the [following link](https://stackoverflow.com/questions/55004060/keras-apply-different-dense-layer-to-each-timestep) and can't seem to find an answer",14,5
246,2019-7-4,2019,7,4,18,c90oi4,Any book that comes close to The deep learning book by Goodfellow et al. ? [D],https://www.reddit.com/r/MachineLearning/comments/c90oi4/any_book_that_comes_close_to_the_deep_learning/,Jeevesh88,1562233445,"I was wondering if there's any book of similar level , insight and beauty on other topics in machine learning apart from deep learning.. like reinforcement learning etc. ? 

This is my first post here. Sorry , in case of any mistakes. Thank you , all !",43,28
247,2019-7-4,2019,7,4,19,c90wjh,A free video lecture on Statistical machine learning,https://www.reddit.com/r/MachineLearning/comments/c90wjh/a_free_video_lecture_on_statistical_machine/,freevideolectures,1562235145," Statistical Machine Learning,10-702/36-702, is a second graduate level course in advanced machine learning. The term statistical in the title reflects the emphasis on statistical theory and methodology.  
The course combines methodology with theoretical foundations. 

&amp;#x200B;

 https://freevideolectures.com/course/3759/statistical-machine-learning?utm\_source=reddit",0,1
248,2019-7-4,2019,7,4,19,c9178y,[D] The smartphone has disappeared on a selfie! Deep learning,https://www.reddit.com/r/MachineLearning/comments/c9178y/d_the_smartphone_has_disappeared_on_a_selfie_deep/,cmillionaire9,1562237486,"[https://youtu.be/zr9fFSYIVYI](https://youtu.be/zr9fFSYIVYI)

 Remove your phone from the mirror selfie using deep learning.",3,0
249,2019-7-4,2019,7,4,19,c919fi,Accessing GoogleNews-vectors-negative300.bin online,https://www.reddit.com/r/MachineLearning/comments/c919fi/accessing_googlenewsvectorsnegative300bin_online/,Strange_Flatworm,1562237967,[removed],0,1
250,2019-7-4,2019,7,4,20,c91gdc,Ai development services|Ai development company,https://www.reddit.com/r/MachineLearning/comments/c91gdc/ai_development_servicesai_development_company/,clarke2106,1562239386,[removed],0,1
251,2019-7-4,2019,7,4,20,c91mmp,[N] Thinking of Self-Studying Machine Learning? Remind yourself of these 6 things.,https://www.reddit.com/r/MachineLearning/comments/c91mmp/n_thinking_of_selfstudying_machine_learning/,vadhavaniyafaijan,1562240656,,0,1
252,2019-7-4,2019,7,4,21,c91vp3,[P] Looking for feedback on my project idea (for university),https://www.reddit.com/r/MachineLearning/comments/c91vp3/p_looking_for_feedback_on_my_project_idea_for/,blphilosophy,1562242365,"So over the next 10 weeks or so, I'm doing a project on music genre classification for my MSc (it's a conversion course as I did Philosophy at undergrad). I'm going to be using Python to classify songs based on their genre. My supervisor has suggested two possible project ideas:

1. Compare using SVMs with manual feature engineering (extracting features such as zero crossing rate, MFCCs, etc., from audio files), VS using a deep (probably convolutional) neural network that potentially uses some kind of automated feature learning (not sure how to do this), to see which approach is better.

2. Use a recurrent neural network for the classification, within the 'reservoir computing' paradigm. This is something I know much less about, but appanently is relatively new technology, so might be able to do something more novel with this project.

I'm going to mainly be using audio files themselves, but may also be using album artwork as a second form of input if I have time, as some researchers have actually managed to classify albums based on album artwork and use that as an additional set of features that can allow for more accurate classification of an album. I'm aware that music genre classification has been done quite a lot before, which is partly why I thought of adding in the album cover classification as an additional part of the project (as this hasn't been done as much, so I'd potentially get more marks for originality).

My main question with the first project idea is that I don't really know how to turn it into a sophisticated project. I have a small dataset of 1000 songs and pre-extracted features (extracted using Librosa) that I used a SVM (scikit learn) to classify the songs, but the accuracy was only about 0.65. More fundamentally, after the data preprocessing, I only used a few lines of code overall, so to me it just felt really basic. I've never written a report like this before so I honestly don't know how to turn it into something more substantial (or how to make the SVM more accurate). Is part of the low accuracy here potentially down to the dataset being too small?

I also don't really know how I'd make the neural network part of the project more substantial. Would this involve lots of trial and error - tweaking the hyperparameters, number of layers, activation functions, etc., to get the best result possible?

For the second project idea, I know less about RNNs and even less about reservoir computing, so I don't know if I'll have time to learn enough to make it into a substantial, worthwhile project. Does this one have potential?

I would like to get a distinction for the project if I can, so any feedback on my ideas/answers to my questions would be really appreciated. Thanks.",8,1
253,2019-7-4,2019,7,4,21,c91x6f,Ideas to identify doctors influential power,https://www.reddit.com/r/MachineLearning/comments/c91x6f/ideas_to_identify_doctors_influential_power/,flev1266,1562242639,[removed],0,1
254,2019-7-4,2019,7,4,21,c91z91,[P] I created my first CNN to recognize handwritten digits (Github link in comments),https://www.reddit.com/r/MachineLearning/comments/c91z91/p_i_created_my_first_cnn_to_recognize_handwritten/,Fengax,1562243034,,1,1
255,2019-7-4,2019,7,4,21,c922cs,Multi realtional graph convolutional networks,https://www.reddit.com/r/MachineLearning/comments/c922cs/multi_realtional_graph_convolutional_networks/,y05r1,1562243586,[removed],0,1
256,2019-7-4,2019,7,4,21,c9234v,[P] I created my first CNN to recognize handwritten digits (Github link in comments),https://www.reddit.com/r/MachineLearning/comments/c9234v/p_i_created_my_first_cnn_to_recognize_handwritten/,Fengax,1562243719,,0,1
257,2019-7-4,2019,7,4,21,c924mt,[R] Active Annotation -- Efficient human-in-the-loop annotation methodology,https://www.reddit.com/r/MachineLearning/comments/c924mt/r_active_annotation_efficient_humanintheloop/,feedmari,1562243978,"Active Annotation: bootstrapping annotation lexicon and guidelines for supervised NLU learning -- INTERSPEECH 2019

&amp;#x200B;

We present a data annotation paradigm (Active Annotation), which is designed to aid human annotators by means of unsupervised learning. The idea is to set up an iterative process in which instances to be human-labelled are first selected, clustered and automatically labelled, and then passed to the annotator for the final validation of the proposed label or the assignment of a new label. The approach is integrated in a Web tool providing a user interface designed to be easy to use to maximize annotators' productivity. The approach is evaluated in a natural language understanding scenario, in which annotators had to label with intent information a dataset of booking conversations. In this scenario, active annotation is compared against a baseline approach in which data are annotated instance-by-instance with a ""human-only driven"" method (in which annotators have to decide, sentence by sentence, whether to validate, replace or skip an automatically produced label). The reported results indicate the effectiveness of active annotation. First, in separate sessions with the same duration, humans were able to annotate a much larger set of instances compared to the baseline approach. Second, systems trained with data annotated with the proposed active annotation paradigm achieve better performance compared to systems trained with data annotated with the baseline approach.

&amp;#x200B;

\-- Abstract --

Natural Language Understanding (NLU) models are typicallytrained in a supervised learning framework. In the case of in-tent classication, the predicted labels are predened and basedon the designed annotation schema while the labeling processis based on a laborious task where annotators manually inspecteach utterance and assign the corresponding label. We proposean Active Annotation (AA) approach where we combine an un-supervised learning method in the embedding space, a human-in-the-loop verication process, and linguistic insights to createlexicons that can be open categories and adapted over time. Inparticular, annotators dene the y-label space on-the-y dur-ing the annotation using an iterative process and without theneed for prior knowledge about the input data. We evaluate theproposed annotation paradigm in a real use-case NLU scenario.Results show that our Active Annotation paradigm achieves ac-curate and higher quality training data, with an annotation speedof an order of magnitude higher with respect to the traditionalhuman-only driven baseline annotation methodology.

&amp;#x200B;

\-- Paper Link --

[https://www.academia.edu/39709550/Active\_Annotation\_bootstrapping\_annotation\_lexicon\_and\_guidelines\_for\_supervised\_NLU\_learning](https://www.academia.edu/39709550/Active_Annotation_bootstrapping_annotation_lexicon_and_guidelines_for_supervised_NLU_learning)

&amp;#x200B;

Feel free to ask",0,1
258,2019-7-4,2019,7,4,21,c925tp,[P] Implementing Stand-Alone Self-Attention in Vision Models paper using Pytorch,https://www.reddit.com/r/MachineLearning/comments/c925tp/p_implementing_standalone_selfattention_in_vision/,leaderj1001,1562244191,"Hi, I'm Myeongjun Kim. My major is Computer Vision using Deep Learning. ""Stand-Alone Self-Attention in Vision Models"" paper published on 13 Jun 2019. This paper was presented by the Google Research Brain Team. I implemented this paper in pytorch. Currently I am experimenting with a CIFAR-10 dataset. (training 30 epoch, Accuracy: 88.4%) The experiment is still in progress. I could see that the learning was very good. Many researches seem to use the Attention Module.

There are some parts that are not currently implemented, and some parts have not yet been experimented. I would appreciate a lot of feedback.

&amp;#x200B;

\[Github URL: [https://github.com/leaderj1001/Stand-Alone-Self-Attention](https://github.com/leaderj1001/Stand-Alone-Self-Attention)\]

Thank you so much :)",0,1
259,2019-7-4,2019,7,4,21,c926du,[P] Implementing Stand-Alone Self-Attention in Vision Models paper using Pytorch,https://www.reddit.com/r/MachineLearning/comments/c926du/p_implementing_standalone_selfattention_in_vision/,leaderj1001,1562244294,"Hi, I'm Myeongjun Kim. My major is Computer Vision using Deep Learning. ""Stand-Alone Self-Attention in Vision Models"" paper published on 13 Jun 2019. This paper was presented by the Google Research Brain Team. I implemented this paper in pytorch. Currently I am experimenting with a CIFAR-10 dataset. (training 30 epoch, Accuracy: 88.4%) The experiment is still in progress. I could see that the learning was very good. Many researches seem to use the Attention Module.

There are some parts that are not currently implemented, and some parts have not yet been experimented. I would appreciate a lot of feedback.

&amp;#x200B;

\[Github URL: [https://github.com/leaderj1001/Stand-Alone-Self-Attention](https://github.com/leaderj1001/Stand-Alone-Self-Attention)\]

Thank you so much :)",0,8
260,2019-7-4,2019,7,4,21,c929dp,Let's discuss in Learning representations in multi-relational graphs,https://www.reddit.com/r/MachineLearning/comments/c929dp/lets_discuss_in_learning_representations_in/,y05r1,1562244833,"Hello ,

What are your suggestions in Multi relational graph models for featurized entities !",0,1
261,2019-7-4,2019,7,4,22,c92llk,Mind Control. Machines piloted by thought,https://www.reddit.com/r/MachineLearning/comments/c92llk/mind_control_machines_piloted_by_thought/,wtf1001,1562246895,,0,1
262,2019-7-4,2019,7,4,22,c92shj,Any recommendation to improve the regression model for landmark detection?,https://www.reddit.com/r/MachineLearning/comments/c92shj/any_recommendation_to_improve_the_regression/,nikogamulin,1562248065,[removed],0,1
263,2019-7-5,2019,7,5,0,c93spd,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/c93spd/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1562253683,[removed],0,1
264,2019-7-5,2019,7,5,0,c942xu,[Free Course - Limit Time] - Random Forest Algorithm in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/c942xu/free_course_limit_time_random_forest_algorithm_in/,upworknepal,1562255223,[removed],0,1
265,2019-7-5,2019,7,5,2,c950ob,[P] NumPy implementations of various ML models,https://www.reddit.com/r/MachineLearning/comments/c950ob/p_numpy_implementations_of_various_ml_models/,dancepm,1562260170,"I've been slowly building a collection of pure-NumPy (and a little SciPy) implementations of various ML models + building blocks to use for quick reference. The project has mostly been a fun thing for me to do in my spare time (hence the strange collection of models), though I hope it might also be useful for others interested in bare-bones implementations of particular models / ideas.

[https://github.com/ddbourgin/numpy-ml](https://github.com/ddbourgin/numpy-ml)

I'm sure there's a ton that can be improved / made clearer. Alternatively, if you have models of your own that would be a good fit, PRs are welcome :-)",46,354
266,2019-7-5,2019,7,5,2,c95iev,learning,https://www.reddit.com/r/MachineLearning/comments/c95iev/learning/,karenactionsitafaal,1562262643,,0,1
267,2019-7-5,2019,7,5,3,c95o9h,Linear Distillation Learning,https://www.reddit.com/r/MachineLearning/comments/c95o9h/linear_distillation_learning/,postmachines,1562263466,"\[R\] I'm glad to share our work on a Linear Networks for application in classification problems.  

Our research seeks to create a mathematically tractable, scalable and simple model, which can help us to bridge the gap between the efficacy and lucidity of a deep learning model. 

We replaced the classification problem with the problem of approximating a linear function with linear functions for different classes.

*Processing img 87td45sjtb831...*

Highlights of the paper on Medium: https://medium.com/@asadulaevarip/linear-distillation-learning-da76a2f3a933

arXiv preprint: https://arxiv.org/abs/1906.05431",0,1
268,2019-7-5,2019,7,5,3,c95ske,[R] Linear Distillation Learning,https://www.reddit.com/r/MachineLearning/comments/c95ske/r_linear_distillation_learning/,postmachines,1562264068,,1,1
269,2019-7-5,2019,7,5,3,c95wo4,[D] Linear Distillation Learning,https://www.reddit.com/r/MachineLearning/comments/c95wo4/d_linear_distillation_learning/,postmachines,1562264667,[removed],0,1
270,2019-7-5,2019,7,5,3,c95x6n,[D] NEAT algorithms for generalized learning?,https://www.reddit.com/r/MachineLearning/comments/c95x6n/d_neat_algorithms_for_generalized_learning/,Chris_Hemsworth,1562264743,"I was watching [this video](https://www.youtube.com/watch?v=qv6UVOQ0F44), and I actually love the concept of the NEAT algorithm for learning how to achieve goals through neural network evolution. My question is; If a network was trained on a particular level, would it be able to apply those same techniques to beat other levels, or is it just extremely fine-tuned to beat that one level?

Thinking about playing SMW when I was growing up, I died a lot, but after a while I got good enough at the game that I could beat levels I'd never played before in my first attempt. This means I would have built up the experience necessary to deal with unknown game mechanics, *rapidly* be able to learn how they work, and utilize them to achieve the end goal. Would networks trained via the NEAT algorithm be able to do the same thing? My initial thoughts are: no way...",5,2
271,2019-7-5,2019,7,5,3,c95yyy,"What's The Difference Between Supervised, Unsupervised and Reinforcement Learning?",https://www.reddit.com/r/MachineLearning/comments/c95yyy/whats_the_difference_between_supervised/,meancoder,1562265003,,0,1
272,2019-7-5,2019,7,5,4,c96l1a,How to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/c96l1a/how_to_learn_machine_learning/,manthan009,1562268235,[removed],0,1
273,2019-7-5,2019,7,5,4,c96o07,Deep Learning from the Foundations,https://www.reddit.com/r/MachineLearning/comments/c96o07/deep_learning_from_the_foundations/,pmz,1562268695,,0,1
274,2019-7-5,2019,7,5,4,c96wyn,[R] Recurrent CNNs can recycle neural resources and flexibly trade speed for accuracy in visual recognition.,https://www.reddit.com/r/MachineLearning/comments/c96wyn/r_recurrent_cnns_can_recycle_neural_resources_and/,sigh_ence,1562270043,,0,1
275,2019-7-5,2019,7,5,5,c974ee,What is the correct way to fuse two remote sensing images in a CNN model?,https://www.reddit.com/r/MachineLearning/comments/c974ee/what_is_the_correct_way_to_fuse_two_remote/,MaherDL,1562271149,[removed],0,1
276,2019-7-5,2019,7,5,5,c976z0,DeepMind Transporter: Unsupervised Learning of Object Keypoints,https://www.reddit.com/r/MachineLearning/comments/c976z0/deepmind_transporter_unsupervised_learning_of/,Yuqing7,1562271534,,0,1
277,2019-7-5,2019,7,5,5,c97ccq,Bike Sharing Demand - bike station level modeling,https://www.reddit.com/r/MachineLearning/comments/c97ccq/bike_sharing_demand_bike_station_level_modeling/,dsdobra,1562272376,"Hi All,

I'm currently working on my university project related to smart cities and bike sharing using historical data from US bike sharing company. I was thinking of performing ML modeling at the station level - so that an entrepreneur can presume how many bikes will be demanded at the exact station and  the exact time in order to provide sufficient number of bikes to each station. 

Frankly speaking, idk how I can incorporate this information into the model. If I transform the station name/id into the binary variable the dimensionality will increase drastically (there are circa 1k unique stations), but if I transform it into the categorical variable the model will understand that id of ID =1000 is 1000 more than ID = 1 which is obviously wrong.

Any ideas on how I can approach it?",0,1
278,2019-7-5,2019,7,5,5,c97cgt,Facebook just open-sourced their Deep Learning Recommendation Model (DLRM),https://www.reddit.com/r/MachineLearning/comments/c97cgt/facebook_just_opensourced_their_deep_learning/,goncaloperes,1562272392,,1,1
279,2019-7-5,2019,7,5,6,c97p2h,David Silver's Reinforcement Learning Course [Summary],https://www.reddit.com/r/MachineLearning/comments/c97p2h/david_silvers_reinforcement_learning_course/,jdyr1729,1562274341,,0,1
280,2019-7-5,2019,7,5,6,c97rd0,[P] Using YOLOv3 for Like/dislike detection,https://www.reddit.com/r/MachineLearning/comments/c97rd0/p_using_yolov3_for_likedislike_detection/,atrosen1,1562274696,"Hi everyone!

My team and I have just released a computer vision algorithm that can detect ""thumb up"" and ""thumb down"" gestures.

The algorithm is based on tiny-YOLOv3 architecture. Accuracy of thumb up/down gesture recognition is calculated as mean average precision (mAP@0.25) = 85.19%; average IoU = 73.89%.

The neural network was trained on 3000 images.

To try out the algorithm, download it from the [GitHub](https://github.com/heyml/rateme)and install it.

We would like to hear your feedback so we can continue to improve our service to you :)",2,5
281,2019-7-5,2019,7,5,6,c985gm,How would I go about training a program to teach a language (not English)?,https://www.reddit.com/r/MachineLearning/comments/c985gm/how_would_i_go_about_training_a_program_to_teach/,SmugEskim0,1562276932,"I'm not sure if this is the right place for this - please refer me to a more relevant sub if you can.

I want to use Machine Learning to train a program to teach a language. Something like Rosetta Stone or Duolingo etc.

How difficult would this be? What would be the best route to do this? What would be the best hardware for this?

Assume I have dozens of the language speakers to provide vocal and written input.

Thanks in advance.",0,1
282,2019-7-5,2019,7,5,7,c98gpo,Simple Supervised ML,https://www.reddit.com/r/MachineLearning/comments/c98gpo/simple_supervised_ml/,hagridontherocks,1562278753,[removed],0,1
283,2019-7-5,2019,7,5,8,c98xl1,Detailed tutorial on Image Recognition using fastai library,https://www.reddit.com/r/MachineLearning/comments/c98xl1/detailed_tutorial_on_image_recognition_using/,RenegadeLearner,1562281647,[removed],0,1
284,2019-7-5,2019,7,5,8,c99193,Detailed tutorial on image recognition using fastai library -- Let me know your comments,https://www.reddit.com/r/MachineLearning/comments/c99193/detailed_tutorial_on_image_recognition_using/,RenegadeLearner,1562282284,,0,1
285,2019-7-5,2019,7,5,8,c99667,"A detailed state-of-the-art tuorial on Image Recognition using Resnets, fastai -- Hope some will find it helpful",https://www.reddit.com/r/MachineLearning/comments/c99667/a_detailed_stateoftheart_tuorial_on_image/,RenegadeLearner,1562283182,,0,1
286,2019-7-5,2019,7,5,9,c99mdx,[R] Benchmarking Model-Based Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/c99mdx/r_benchmarking_modelbased_reinforcement_learning/,chisai_mikan,1562286140,,4,11
287,2019-7-5,2019,7,5,9,c99uv1,What Is Machine Learning (Supervised Learning) - Part 1,https://www.reddit.com/r/MachineLearning/comments/c99uv1/what_is_machine_learning_supervised_learning_part/,gunda1995,1562287806,,0,1
288,2019-7-5,2019,7,5,12,c9bbah,"Achieving AGI by using a single ""controller"" central AI to control multiple specialized AIs, could this be a good approach?",https://www.reddit.com/r/MachineLearning/comments/c9bbah/achieving_agi_by_using_a_single_controller/,2Punx2Furious,1562298325,,0,1
289,2019-7-5,2019,7,5,13,c9bmfv,[R] Unsupervised word embeddings capture latent knowledge from materials science literature,https://www.reddit.com/r/MachineLearning/comments/c9bmfv/r_unsupervised_word_embeddings_capture_latent/,penpatience,1562300507,,0,1
290,2019-7-5,2019,7,5,13,c9bp28,[P] DRL Model Zoo,https://www.reddit.com/r/MachineLearning/comments/c9bp28/p_drl_model_zoo/,zsdh123,1562301051,,0,1
291,2019-7-5,2019,7,5,13,c9bro0,[R] Unsupervised word embeddings capture latent knowledge from materials science literature. Recommends materials for functional applications several years before their discovery,https://www.reddit.com/r/MachineLearning/comments/c9bro0/r_unsupervised_word_embeddings_capture_latent/,penpatience,1562301608,,0,1
292,2019-7-5,2019,7,5,14,c9bzka,A terrifying character description from the Transformer,https://www.reddit.com/r/MachineLearning/comments/c9bzka/a_terrifying_character_description_from_the/,rowc99,1562303265,[removed],0,1
293,2019-7-5,2019,7,5,15,c9cpnu,AI for automation testing,https://www.reddit.com/r/MachineLearning/comments/c9cpnu/ai_for_automation_testing/,ShaivalM,1562308631,[removed],0,1
294,2019-7-5,2019,7,5,15,c9cums,real-world adversarial attack (on a chicken),https://www.reddit.com/r/MachineLearning/comments/c9cums/realworld_adversarial_attack_on_a_chicken/,bluecoffee,1562309685,,0,1
295,2019-7-5,2019,7,5,16,c9d5fn,Able to understand theory well but finding it difficult to implement the code. Seeking Guidance.,https://www.reddit.com/r/MachineLearning/comments/c9d5fn/able_to_understand_theory_well_but_finding_it/,shan001s,1562311966,[removed],0,1
296,2019-7-5,2019,7,5,16,c9d7od,"[P] Learning to play Tetris, again",https://www.reddit.com/r/MachineLearning/comments/c9d7od/p_learning_to_play_tetris_again/,b0red1337,1562312483,"Hi all,

This is a follow-up to my [original post](https://www.reddit.com/r/MachineLearning/comments/a97rl3/p_learning_to_play_tetris_with_mcts_and_td/) of using MCTS and TD learning to solve the Tetris environment.

As I have made quite some progress, I think it would be interesting to share my latest results with you.

[Here is a video showcasing the evolution of the new agent](https://www.youtube.com/watch?v=v-p-36f5YMw). 

And here is the average scores (according to the [guideline](https://tetris.wiki/Scoring)) and line clears at each iteration

&amp;#x200B;

https://i.redd.it/fy4sq3rgbf831.png

where each iteration consists of 50 games of normal play (500 simulations per move (SPM), used for training) and 1 game of benchmark play (1000 SPM). The games shown in the video are the benchmark plays at each iteration.

The previous agent was only able to achieve about \~15 line clears after 800 games, with this new agent, however, we can achieve \~1.3k line clears at 750 games (iteration 15) which is *about 100 times better than the previous one*. Furthermore, the maximum line clear was 5678 (what a coincidence) in one of the games at iteration 15, about 4 times higher than the maximum achieved by the previous agent. Also, note that the new agent was trained using the raw score which is noisier and harder than training on the number of line clears.

As a not-so-fair comparison, I found [an old paper](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2008-118.pdf) using a complex handcrafted reward function along with TD learning was able to achieve \~8k line clears after \~70k games. Personally, I believe my agent could achieve a similar number with significantly less games since the scores was increasing super-exponentially in the later iterations. Unfortunately, the agent was starting to generate more than 8GB of data which is larger than the RAM of my potato so I had to terminate it there.

If you are interested, more description and the code can be found at [my github repository](https://github.com/hrpan/tetris_mcts). 

Thanks for reading, let me know what you think!",20,153
297,2019-7-5,2019,7,5,17,c9dh7d,t-SNE for a feature quality check,https://www.reddit.com/r/MachineLearning/comments/c9dh7d/tsne_for_a_feature_quality_check/,sabinemaennel,1562314784,"t-SNE is my favorite method for feature visualization in data science. In this article, I describe the method briefly and provide some examples from my own work: [https://www.liip.ch/en/blog/the-magic-of-tsne-for-visualizing-your-data-features](https://www.liip.ch/en/blog/the-magic-of-tsne-for-visualizing-your-data-features)",0,1
298,2019-7-5,2019,7,5,17,c9dq3z,My Article on ML got Published in Towards Data Science on Medium! Check it out: https://towardsdatascience.com/linear-regression-using-flavor-of-python-bcbdc616a944 I'm open to feedback! And like the post if it helps.,https://www.reddit.com/r/MachineLearning/comments/c9dq3z/my_article_on_ml_got_published_in_towards_data/,imskrai,1562316921,[removed],0,1
299,2019-7-5,2019,7,5,17,c9dr33,"Is there a web site for a summary of articles on machine learning, deep learning, etc.?",https://www.reddit.com/r/MachineLearning/comments/c9dr33/is_there_a_web_site_for_a_summary_of_articles_on/,Doctor_who1,1562317145,[removed],0,1
300,2019-7-5,2019,7,5,18,c9du8l,[P] I built ONE PUNCHHH! MAN Object Detection on my mobile lol,https://www.reddit.com/r/MachineLearning/comments/c9du8l/p_i_built_one_punchhh_man_object_detection_on_my/,S-Nuttapong,1562317802,"Hi ML folks,  Recently, I have created my custom object detection using TensorFlow OD API and served it on mobile.  This project I used Google Colab for training model because my computer does not have GPU and Intel graphics card won't do lol.  Here it is:

As you can see it still has a long way to go. I will need to add more images data (currently I have 288 images). Train more steps but before that I will need to fix the bug in android caused from high train steps model. And maybe hard mining to reduce false positive. But probably not anytime soon haha.  


So am not sure if somebody might be interested, but just in case I will share my experience here:[https://medium.com/@nuttapong\_s/google-colab-guide-detecting-badass-one-punch-man-heroes-on-android-333c9d4149](https://medium.com/@nuttapong_s/google-colab-guide-detecting-badass-one-punch-man-heroes-on-android-333c9d4149)

  


*Processing gif cuu1m6xk9g831...*",0,1
301,2019-7-5,2019,7,5,18,c9dx0g,My second Article on ML got Published in Towards Data Science on Medium! It is related to Linear Regression Check it out: I'm open to feedback! And like the post if it helps.,https://www.reddit.com/r/MachineLearning/comments/c9dx0g/my_second_article_on_ml_got_published_in_towards/,imskrai,1562318422,,0,1
302,2019-7-5,2019,7,5,18,c9dx8w,[P] I built ONE PUNCHHH MAN -detection on mobile lol,https://www.reddit.com/r/MachineLearning/comments/c9dx8w/p_i_built_one_punchhh_man_detection_on_mobile_lol/,S-Nuttapong,1562318478,,1,1
303,2019-7-5,2019,7,5,19,c9ea6e,"Python implementation of Model Predictive Path Integral (MPPI) by (Williams et al., 2017) with OpenAI gym pendulum environment",https://www.reddit.com/r/MachineLearning/comments/c9ea6e/python_implementation_of_model_predictive_path/,whiletrue2,1562321316,[removed],0,1
304,2019-7-5,2019,7,5,19,c9edki,Extensive python implementation for Conditional Density Estimation,https://www.reddit.com/r/MachineLearning/comments/c9edki/extensive_python_implementation_for_conditional/,whiletrue2,1562322105,[removed],0,1
305,2019-7-5,2019,7,5,19,c9eg8c,"[P] Conditional Density Estimation Python Package and Large Benchmark with Neural Networks (MDN, KMN, Normalizing Flow) and Non-/Semi-parametric estimators",https://www.reddit.com/r/MachineLearning/comments/c9eg8c/p_conditional_density_estimation_python_package/,whiletrue2,1562322711,"We've implemented an extensive pip package for Conditional Density Estimation that, among other features, includes Mixture Density Network, Kernel Mixture Network, Normalizing Flow Estimator and various non-parametric/semi-parametric estimators (CKDE, NKDE, LSKDE), data simulators and evaluation functions (centered moments, KL/JS divergence, Hellinger distance, percentiles etc.).

The package is constantly improved and we also provide a benchmark &amp; best practices report and a code documentation.

Code: https://github.com/freelunchtheorem/Conditional_Density_Estimation

Benchmark and best practices paper for NN-based CDE: https://arxiv.org/abs/1903.00954

Code docs: https://freelunchtheorem.github.io/Conditional_Density_Estimation/docs/html/index.html

We're open for suggestions and feedback so please feel free to use &amp; comment. Lastly, if you like our project, we'd be happy if you spread the word and star the GH repo.",8,16
306,2019-7-5,2019,7,5,19,c9ej1p,Question about publications,https://www.reddit.com/r/MachineLearning/comments/c9ej1p/question_about_publications/,Janissary2019,1562323366,[removed],0,1
307,2019-7-5,2019,7,5,20,c9ern4,About theorem 1 in Double DQN paper,https://www.reddit.com/r/MachineLearning/comments/c9ern4/about_theorem_1_in_double_dqn_paper/,yhxie,1562325226,"Hi, everyone, I am recently reading the DDQN paper and I have some problems with the proof of theorem 1.

1. In page 2, it states that ""... we did not need to assume that estimation errors for different actions are independent"", which, based on my understanding, means that theorem 1 holds when errors are not independent. What if errors are independent, does theorem 1 still hold?
2. In page 8, Appendix, I don't quite understand why ""... error for Double Q-learning is zero"", my guess is that, for Q', m = 1, so the second term in Qt(s, a1) is 0. But why?

I will be very appreciated if someone can provide a more detailed proof or a link to a detailed explanation. Thank you very much for your time!",0,1
308,2019-7-5,2019,7,5,20,c9eseq,[P] Shuffle big file,https://www.reddit.com/r/MachineLearning/comments/c9eseq/p_shuffle_big_file/,YaYaBFr,1562325381,"Hi everyone,  


Several times while dealing with huge files I struggled when I wanted to shuffle those for instance for training. Some methods suggest to split the files and shuffle those separately however it is not a real shuffle (element in bucket 0 will not appear in bucket 10 for instance).  
I've made a library a few months ago that allow to do this by shuffling the index of the number of lines and reading as much as it is necessary the original file (with a certain batch size) in order to be able to complete the shuffle.  


Quick example:  
file with 10K lines and batch\_size 5k.  
\- Shuffle index to index\_shuffled  
\- Read in streaming the file and dump the first 5k of index\_shuffled  
\- Read in streaming the file once a gain and dump the last 5k of index\_shuffled  


Reading a file is not costly that's why the perfs seem to me quite interesting.

  
It is really not that complicated but I did not find it available somewhere...

Here is the link: [https://github.com/YaYaB/shuffle-big-file](https://github.com/YaYaB/shuffle-big-file)

I hope it can be useful to some of you :)  


Best,

YaYaB.",13,1
309,2019-7-5,2019,7,5,20,c9eux8,"Genetic algorithm for neural networks, which fitness function ?",https://www.reddit.com/r/MachineLearning/comments/c9eux8/genetic_algorithm_for_neural_networks_which/,raysamram,1562325907,[removed],0,1
310,2019-7-5,2019,7,5,20,c9ew43,Application of AI report,https://www.reddit.com/r/MachineLearning/comments/c9ew43/application_of_ai_report/,ksjurewicz,1562326165,[removed],0,1
311,2019-7-5,2019,7,5,20,c9ew68,[D] Learning a prior on the latent variables for generating samples from a VAE,https://www.reddit.com/r/MachineLearning/comments/c9ew68/d_learning_a_prior_on_the_latent_variables_for/,ThisIsMySeudonym,1562326179,"I'm trying to find literature on generating samples from a VAE where a prior is learned (as an additional later step after training the VAE) on the latent variables. Then the prior is sampled to produce the latent variables, z, to feed to the decoder, rather than getting z \~ N(0, 1) (if q(z|x) is a unit Gaussian).

&amp;#x200B;

Empirically I've noticed that for very complex and diverse images, the reconstructions from a traditional VAE look good enough (despite being blurry but that's a different issue), but when you try to generate entirely new samples by feeding the decoder a random z \~ N(0, 1), they look awful. 

&amp;#x200B;

What if we don't just randomly sample z \~ N(0, 1), but produce robust new z's in some other way, such as through a function approximator. For example, after training the VAE, go through your entire dataset, compute mu and sigma, and train an auto regressive feedforward model to produce a plausible z. Or use the same data to train a model that takes uniform random noise as input and produces z's as output. This thought is inspired by the VQ-VAE approach ([https://arxiv.org/abs/1711.00937](https://arxiv.org/abs/1711.00937), and [https://arxiv.org/abs/1906.00446](https://arxiv.org/abs/1906.00446)), where they train a PixelCNN on all the quantized latent vectors produced by the VAE from their entire dataset, to sample new latents and produce new images.

&amp;#x200B;

Would love to hear your thoughts, or get links to papers on this. Thanks!",8,9
312,2019-7-5,2019,7,5,20,c9f1lu,Top Data Science Use cases in Construction,https://www.reddit.com/r/MachineLearning/comments/c9f1lu/top_data_science_use_cases_in_construction/,techgig11,1562327311,,0,7
313,2019-7-5,2019,7,5,22,c9fq34,Marketing strategy with Machine Learning decision making,https://www.reddit.com/r/MachineLearning/comments/c9fq34/marketing_strategy_with_machine_learning_decision/,atomlib_com,1562331870,,0,1
314,2019-7-5,2019,7,5,22,c9fzhc,Noisy gradient effects on gradient descent method results,https://www.reddit.com/r/MachineLearning/comments/c9fzhc/noisy_gradient_effects_on_gradient_descent_method/,OddProfile5,1562333494,[removed],0,1
315,2019-7-5,2019,7,5,22,c9g4qq,There is a captcha that enables website owners to earn for visitors who solve the captcha,https://www.reddit.com/r/MachineLearning/comments/c9g4qq/there_is_a_captcha_that_enables_website_owners_to/,CrissCrossaa,1562334402,[removed],0,1
316,2019-7-5,2019,7,5,23,c9gpbr,Can BERT or ELMo be used for sentence generating tasks?,https://www.reddit.com/r/MachineLearning/comments/c9gpbr/can_bert_or_elmo_be_used_for_sentence_generating/,JohnCawotte,1562337647,[removed],0,1
317,2019-7-5,2019,7,5,23,c9gwej,Using AI to humanize virtual language lessons,https://www.reddit.com/r/MachineLearning/comments/c9gwej/using_ai_to_humanize_virtual_language_lessons/,jonfla,1562338720,,0,1
318,2019-7-6,2019,7,6,0,c9gx5f,NNStreamer: Stream Processing for On-Device AI Applications,https://www.reddit.com/r/MachineLearning/comments/c9gx5f/nnstreamer_stream_processing_for_ondevice_ai/,venuv,1562338833,,6,7
319,2019-7-6,2019,7,6,0,c9h4bj,Implementing RBM with binary visible and ReLU hidden layer,https://www.reddit.com/r/MachineLearning/comments/c9h4bj/implementing_rbm_with_binary_visible_and_relu/,ricardo-008,1562339851,[removed],0,1
320,2019-7-6,2019,7,6,0,c9hdyy,[P] g2pC: A Context-aware Grapheme-to-Phoneme Conversion module for Chinese,https://www.reddit.com/r/MachineLearning/comments/c9hdyy/p_g2pc_a_contextaware_graphemetophoneme/,longinglove,1562341217,"[https://github.com/Kyubyong/g2pC](https://github.com/Kyubyong/g2pC)

&amp;#x200B;

There are several open source libraries of Chinese grapheme-to-phoneme conversion such as [python-pinyin](https://github.com/mozillazg/python-pinyin) or [xpinyin](https://github.com/lxneng/xpinyin). However, none of them seem to disambiguate Chinese polyphonic words like  """" (""xng"" (go, walk) vs. ""hng"" (line)) or """" (""le"" (completed action  marker) vs. ""lio"" (finish, achieve)). Instead, they pick up the most frequent  pronunciation. Although that may be a simple and economic strategy, machine learning  techniques can be of help here. We use CRF to determine the pronunciation of polyphonic words. In  addition to the target word itself and its part-of-speech, which are  tagged by pkuseg, its neighboring words are also featurized.",0,15
321,2019-7-6,2019,7,6,0,c9hgpj,ML job with a CS PhD in another area,https://www.reddit.com/r/MachineLearning/comments/c9hgpj/ml_job_with_a_cs_phd_in_another_area/,DarioCruiser,1562341610,[removed],0,1
322,2019-7-6,2019,7,6,2,c9ihm3,Deep pan learning: Which pizza to get?,https://www.reddit.com/r/MachineLearning/comments/c9ihm3/deep_pan_learning_which_pizza_to_get/,Gumeo,1562346750,,2,1
323,2019-7-6,2019,7,6,2,c9iich,Is Tensorflow Lite inference faster than Tensorflow inference?,https://www.reddit.com/r/MachineLearning/comments/c9iich/is_tensorflow_lite_inference_faster_than/,goppox,1562346846,[removed],0,1
324,2019-7-6,2019,7,6,2,c9ijkz,"Just Enough Python - For data science, machine learning and deep learning",https://www.reddit.com/r/MachineLearning/comments/c9ijkz/just_enough_python_for_data_science_machine/,Ajay_Tech,1562347027,[removed],0,1
325,2019-7-6,2019,7,6,2,c9j136,Swordsman 2 actress's,https://www.reddit.com/r/MachineLearning/comments/c9j136/swordsman_2_actresss/,karenactionsitafaal,1562349477,,0,1
326,2019-7-6,2019,7,6,2,c9j1uf,Choosing the number of components in PCA.,https://www.reddit.com/r/MachineLearning/comments/c9j1uf/choosing_the_number_of_components_in_pca/,mmac27,1562349582,[removed],0,1
327,2019-7-6,2019,7,6,3,c9j267,How to create a UI for machine learning application??,https://www.reddit.com/r/MachineLearning/comments/c9j267/how_to_create_a_ui_for_machine_learning/,ALBoost,1562349625,[removed],0,1
328,2019-7-6,2019,7,6,3,c9j60f,[Request][Repost][Research] Asking for survey participant in a Big Data project.,https://www.reddit.com/r/MachineLearning/comments/c9j60f/requestrepostresearch_asking_for_survey/,FuegoDentro,1562350131,"Dear Machine Learning SubReddit,

&amp;#x200B;

Good day,  I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

&amp;#x200B;

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post.

&amp;#x200B;

Regard,

Graduating Student, a fellow redditor.",6,0
329,2019-7-6,2019,7,6,3,c9jnym,[D] Linear Distillation Learning,https://www.reddit.com/r/MachineLearning/comments/c9jnym/d_linear_distillation_learning/,postmachines,1562352732,[removed],0,1
330,2019-7-6,2019,7,6,4,c9k5z3,MailFliers,https://www.reddit.com/r/MachineLearning/comments/c9k5z3/mailfliers/,dan900000,1562355319,[removed],0,1
331,2019-7-6,2019,7,6,5,c9kmo6,[P] PyTorch Implementation of SamplePairing &amp; Testing on a small dataset,https://www.reddit.com/r/MachineLearning/comments/c9kmo6/p_pytorch_implementation_of_samplepairing_testing/,junkwhinger,1562357683,"Hi, I've recently read Data Augmentation by Pairing Samples for Images Classification and found SamplePairing fascinating. I applied the method on my small image dataset and it showed lower validation loss than the baseline :)

post: [https://jsideas.net/samplepairing/](https://jsideas.net/samplepairing/)

github: [https://github.com/junkwhinger/SamplePairing](https://github.com/junkwhinger/SamplePairing)",1,2
332,2019-7-6,2019,7,6,5,c9kqxw,Installing PyTorch on RHEL 7.6 ppc64LE architecture,https://www.reddit.com/r/MachineLearning/comments/c9kqxw/installing_pytorch_on_rhel_76_ppc64le_architecture/,Strange_Flatworm,1562358297,[removed],0,1
333,2019-7-6,2019,7,6,6,c9le35,In-browser Image detection,https://www.reddit.com/r/MachineLearning/comments/c9le35/inbrowser_image_detection/,markarth24,1562361604,[removed],0,1
334,2019-7-6,2019,7,6,6,c9lm13,Anyone use Tensorboard?,https://www.reddit.com/r/MachineLearning/comments/c9lm13/anyone_use_tensorboard/,SharathCK,1562362819,[removed],0,1
335,2019-7-6,2019,7,6,7,c9lufp,"Automating layer additions, size and other hyperparameter selection in Tensorflow",https://www.reddit.com/r/MachineLearning/comments/c9lufp/automating_layer_additions_size_and_other/,SharathCK,1562364109,[removed],0,1
336,2019-7-6,2019,7,6,8,c9my9l,Questions for ML (Beginner),https://www.reddit.com/r/MachineLearning/comments/c9my9l/questions_for_ml_beginner/,Dangtv,1562370552,[removed],0,1
337,2019-7-6,2019,7,6,8,c9n1u2,[D] Is machine learning's killer app totalitarian surveillance and oppression?,https://www.reddit.com/r/MachineLearning/comments/c9n1u2/d_is_machine_learnings_killer_app_totalitarian/,mrspaz19,1562371183,"listening to the planet money episode on the plight of the Uighur people:

[https://twitter.com/planetmoney/status/1147240518411309056](https://twitter.com/planetmoney/status/1147240518411309056)

&amp;#x200B;

In the Uighur region every home is bugged, every apartment building filled with cameras, every citizen's face recorded from every angle in every expression, all DNA recorded, every interaction recorded and NLP used to extract risk for being a dissident.  These databases the restrict ability to do anything or go anywhere.

&amp;#x200B;

Maybe google have done some cool things with ML, but my impression is that globally this is 90% being used for utter totalitarian evil.",149,243
338,2019-7-6,2019,7,6,10,c9nomk,[R] mat2vec: Unsupervised word embeddings capture latent knowledge from materials science literature,https://www.reddit.com/r/MachineLearning/comments/c9nomk/r_mat2vec_unsupervised_word_embeddings_capture/,One_Parking,1562375187,"From the abstract:

&gt;Without any explicit insertion of chemical knowledge, these embeddings capture complex materials science concepts such as the underlying structure of the periodic table and structure property relationships in materials. Furthermore, **we demonstrate that an unsupervised method can recommend materials for functional applications several years before their discovery.** This suggests that latent knowledge regarding future discoveries is to a large extent embedded in past publications. Our findings highlight the possibility of extracting knowledge and relationships from the massive body of scientific literature in a collective manner, and point towards a generalized approach to the mining of scientific literature.

&amp;#x200B;

Paper: [https://sci-hub.tw/https://www.nature.com/articles/s41586-019-1335-8](https://sci-hub.tw/https://www.nature.com/articles/s41586-019-1335-8)

Source Code: [https://github.com/materialsintelligence/mat2vec](https://github.com/materialsintelligence/mat2vec)",15,56
339,2019-7-6,2019,7,6,10,c9nqv9,Machine Learning Certification Course - Msys Training,https://www.reddit.com/r/MachineLearning/comments/c9nqv9/machine_learning_certification_course_msys/,sibasankar,1562375590,,0,1
340,2019-7-6,2019,7,6,10,c9nxyz,"I was able to get a 75% accuracy on 34 classes, training on 5 pictures validating on 1",https://www.reddit.com/r/MachineLearning/comments/c9nxyz/i_was_able_to_get_a_75_accuracy_on_34_classes/,Alexanderdaawesome,1562376843,"This is average, my low is around 69% and my high is at 85. New to this game, but I am pretty proud, just thought I would share :)",0,1
341,2019-7-6,2019,7,6,11,c9olqi,[D] How should I statistically compare the performance of deep reinforcement models?,https://www.reddit.com/r/MachineLearning/comments/c9olqi/d_how_should_i_statistically_compare_the/,ml4564,1562381128,"The environment I used is a card game with which has some randomness when the cards are drawn.

I used 10 different seeds to train the models. Thus 10 models trained for my algorithm, 10 for DQN baseline, etc.

For this study, I'm measuring performance based on the rewards from running the trained models in the environment.

&amp;#x200B;

1) How should I test the difference between the average reward when using my algorithm vs that of the baselines?

For example, should I get the average of 10,000 episodes of the DQN model and then get the average and SD of the 100,000 episodes (from adding up the results of 10,000 episodes from 10 DQN models)? Then after completing the previous step for all the model groups (my own algorithm, other baselines, etc.), compare the rewards with Welch's t-test?

&amp;#x200B;

2) Also, should I used the same random seed (different from training obviously) for the environment when testing the 10 different runs or should I use a different one each time? For example, testing the 10 different trained DQN models with seed 1 for all 100,000 episodes vs using seeds 1 to 10 for 10,000 episodes each.

&amp;#x200B;

Some advice would be helpful. Thanks for reading this and I apologize if any part of this sounds confusing. English is not my first language.",5,2
342,2019-7-6,2019,7,6,12,c9ossh,[D] Linear Distillation Learning,https://www.reddit.com/r/MachineLearning/comments/c9ossh/d_linear_distillation_learning/,Albert_Ierusalem,1562382393,[removed],0,1
343,2019-7-6,2019,7,6,13,c9pci1,[D] How do you think about use face embeddings in conjunction with Elasticsearch for face matching / face similarity application that same as Microsoft's celebslikeme.me ?,https://www.reddit.com/r/MachineLearning/comments/c9pci1/d_how_do_you_think_about_use_face_embeddings_in/,hosjiu,1562386568,"Now, my team is developing a application quite similarity as Celebslikeme.me of Microsoft Corp before (2016).
We seperate the asual canonical pipeline of face recognition system into 3 phrase:
- Phase 1: Face Detection + Face Aligment
- Phase 2: Face Embedding
- Phase 3: Face matching / Face recognion or something like face similarity.
--
At (3) we use the technical that is the embedded vectors get from a given model that ours is from Facenet in this case to to conjunct with Elasticsearch for similarity search.
We are still implementing for now but personally I want to research some nice solutions (if any) for my knowledge also help to improve our product better.
Any ideas are welcome !",4,10
344,2019-7-6,2019,7,6,13,c9pkqj,Can Anyone explain this code piece by piece?,https://www.reddit.com/r/MachineLearning/comments/c9pkqj/can_anyone_explain_this_code_piece_by_piece/,Lium001,1562388141,,0,1
345,2019-7-6,2019,7,6,14,c9pwpe,[R] Dynamics-Aware Unsupervised Discovery of Skills,https://www.reddit.com/r/MachineLearning/comments/c9pwpe/r_dynamicsaware_unsupervised_discovery_of_skills/,inarrears,1562390508,,2,7
346,2019-7-6,2019,7,6,15,c9qb1l,[D] Audio/Digital Signal Processing/Recurrent NN - Need help understanding and reproducing this paper in Python,https://www.reddit.com/r/MachineLearning/comments/c9qb1l/d_audiodigital_signal_processingrecurrent_nn_need/,VividFee,1562393497,"Hello everyone!

&amp;#x200B;

I am trying to reproduce this paper in Python: [A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement by Jean-Marc Valin](https://arxiv.org/pdf/1709.08243.pdf)

Additionally, there is a blog post by the author explaining the paper differently: [RNNoise: Learning Noise Suppression](https://people.xiph.org/~jm/demo/rnnoise/), and [a GitHub repository with the code for training the proposed network](https://github.com/xiph/rnnoise).

&amp;#x200B;

However, I have difficulty understanding the concepts regarding preparing input data for training and prediction. Can someone give me practical notes on how I can achieve this?

&amp;#x200B;

Some questions I have in section II:

1. The paper and blog post computes 22 bands at first. Where a DCT is applied on the log spectrum, resulting in 22 Bark-frequency cepstral coefficients. Which is closely related to the Mel-Frequency Cepstral Coefficients. **What does this mean, and how does this work?**

2. The author also includes the temporal derivative and the second temporal derivative of the first six Bark-frequency cepstral coefficients across frames. **What does this mean?**

3. In formula (5) the pitch correction for every band is calculated, with that the author computes the DCT of the pitch correlation across frequency bands and include the first six coefficients.  **I assume DCT returns a finite set of results. So only 6 of the first coefficients is used per band, correct?**

4. The author mentions including the pitch period as well as a spectral non-stationarity metric. **What does this mean?**

&amp;#x200B;

Some background: I have mostly worked with visual data and convolutional neural networks, so I have almost no knowledge about digital signal processing. Please bear with me.

&amp;#x200B;

Thanks in advance!",5,7
347,2019-7-6,2019,7,6,15,c9qg0g,Chinese Automatic napkin tissue paper making machine with emboosing and printing,https://www.reddit.com/r/MachineLearning/comments/c9qg0g/chinese_automatic_napkin_tissue_paper_making/,Josepapermachinery,1562394458,[removed],3,1
348,2019-7-6,2019,7,6,15,c9qgmw,PhD in Evolutionary Computation worth it in terms of applications?,https://www.reddit.com/r/MachineLearning/comments/c9qgmw/phd_in_evolutionary_computation_worth_it_in_terms/,actuallynotcanadian,1562394592,[removed],0,1
349,2019-7-6,2019,7,6,15,c9qjb2,[N] Machine learning for identifying bats carrying Nipah virus,https://www.reddit.com/r/MachineLearning/comments/c9qjb2/n_machine_learning_for_identifying_bats_carrying/,vadhavaniyafaijan,1562395150,,0,1
350,2019-7-6,2019,7,6,17,c9rcfw,"What are creative ways to approach customer churn prediction? (Besides tree-based models, gradient boosting or feedforward NNs)",https://www.reddit.com/r/MachineLearning/comments/c9rcfw/what_are_creative_ways_to_approach_customer_churn/,fipeopp,1562401759,[removed],0,1
351,2019-7-6,2019,7,6,18,c9rsok,Learn Data Science with Python from Scratch,https://www.reddit.com/r/MachineLearning/comments/c9rsok/learn_data_science_with_python_from_scratch/,dbhalla4,1562405757,[removed],0,1
352,2019-7-6,2019,7,6,19,c9s3u3,"Modular Data Center Market: Current Status Updates (2019) and Trends, Estimation &amp; Forecast (2023)",https://www.reddit.com/r/MachineLearning/comments/c9s3u3/modular_data_center_market_current_status_updates/,WPriyanka,1562408483,[removed],0,1
353,2019-7-6,2019,7,6,19,c9s69l,A small inquiry on reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/c9s69l/a_small_inquiry_on_reinforcement_learning/,dittercane,1562409081,[removed],0,1
354,2019-7-6,2019,7,6,20,c9sn7a,Is it possible for the AI of today to make movies?,https://www.reddit.com/r/MachineLearning/comments/c9sn7a/is_it_possible_for_the_ai_of_today_to_make_movies/,TechGenius28,1562412987,[removed],0,1
355,2019-7-6,2019,7,6,21,c9sy44,[Discussion] Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction,https://www.reddit.com/r/MachineLearning/comments/c9sy44/discussion_learning_the_joint_representation_of/,jarym,1562415251,"I've been reading through [reddit!](https://arxiv.org/pdf/1803.04837.pdf) (Author's Github appears to be here: [reddit!](https://github.com/pkusjh/HELSTM)) and I found it quite interesting. My field is more general event stream processing rather than medical but I am thinking this approach is worth exploring.

I've not seen this paper discussed before so I thought I'd raise it here and ask what peoples thoughts are.

Unfortunately for me, I can't access the source data to reproduce their results (since you need to be in the medical field to access Physionet data) and Chinese is totally foreign to me so I've had to use the help of Google Translate with the couple of PDF files I found in the GH repo that explain the data format).

I've gone through and annotated the file format to help me at least understand it better - may be useful for others trying to explore this: [reddit!](https://www.evernote.com/l/AA0k0SpVfq5J9IjmH5qY8tToq1pnaVW4EjoB/image.png)",3,2
356,2019-7-6,2019,7,6,21,c9t3u3,How about that MIT Gen AI language...,https://www.reddit.com/r/MachineLearning/comments/c9t3u3/how_about_that_mit_gen_ai_language/,Darkhog,1562416399,[removed],0,1
357,2019-7-6,2019,7,6,21,c9ta16,Interesting domains to apply machine learning and genetic algorithms side by side,https://www.reddit.com/r/MachineLearning/comments/c9ta16/interesting_domains_to_apply_machine_learning_and/,LessTell,1562417618,[removed],0,1
358,2019-7-6,2019,7,6,22,c9tggg,[D] Linear Distillation Learning,https://www.reddit.com/r/MachineLearning/comments/c9tggg/d_linear_distillation_learning/,postmachines,1562418802,"We glad to present and discuss our paper named  **Linear Distillation Learning (LDL).** Is a simple remedy to improve the performance of linear networks through distillation.

In deep learning models, distillation often allows the smaller/shallow network to mimic the larger models in a much more accurate way, while a network of the same size trained on the one-hot targets cant achieve comparable results to the cumbersome model. Our neural networks without activation functions achieved high classification score on a small amount of data on MNIST and Omniglot datasets.

The approach is based on using a linear function for each class in dataset, which is trained to simulate output of *teacher* linear network for each class separately. When the model is trained, we can apply classification by novelty detection for each class. Our framework distilling randomized prior functions for data, since prior functions are linear, in couple with bootstrap methods it provides a Bayes posterior.

&amp;#x200B;

Highlights: [https://medium.com/@asadulaevarip/linear-distillation-learning-da76a2f3a933](https://medium.com/@asadulaevarip/linear-distillation-learning-da76a2f3a933)

arXiv: [https://arxiv.org/abs/1906.05431](https://arxiv.org/abs/1906.05431) 

Twitter: [https://twitter.com/postmachines/status/1146848387658108928](https://twitter.com/postmachines/status/1146848387658108928)",0,1
359,2019-7-6,2019,7,6,22,c9tr7b,[D] Linear Networks For Classification,https://www.reddit.com/r/MachineLearning/comments/c9tr7b/d_linear_networks_for_classification/,postmachines,1562420779,"We glad to present and discuss our paper named  **Linear Distillation Learning (LDL).** Is a simple remedy to improve the performance of linear networks through distillation.

In deep learning models, distillation often allows the smaller/shallow network to mimic the larger models in a much more accurate way, while a network of the same size trained on the one-hot targets cant achieve comparable results to the cumbersome model. Our neural networks without activation functions achieved high classification score on a small amount of data on MNIST and Omniglot datasets.

The approach is based on using a linear function for each class in dataset, which is trained to simulate output of *teacher* linear network for each class separately. When the model is trained, we can apply classification by novelty detection for each class. Our framework distilling randomized prior functions for data, since prior functions are linear, in couple with bootstrap methods it provides a Bayes posterior.

&amp;#x200B;

Highlights: [https://medium.com/@asadulaevarip/linear-distillation-learning-da76a2f3a933](https://medium.com/@asadulaevarip/linear-distillation-learning-da76a2f3a933)

arXiv: [https://arxiv.org/abs/1906.05431](https://arxiv.org/abs/1906.05431)

Twitter: [https://twitter.com/postmachines/status/1146848387658108928](https://twitter.com/postmachines/status/1146848387658108928)",0,2
360,2019-7-6,2019,7,6,23,c9tzmt,Modifying my YOLO,https://www.reddit.com/r/MachineLearning/comments/c9tzmt/modifying_my_yolo/,sandars12,1562422267,[removed],0,1
361,2019-7-6,2019,7,6,23,c9u2mp,[D] An introduction to Q-Learning: Reinforcement Learning: https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/,https://www.reddit.com/r/MachineLearning/comments/c9u2mp/d_an_introduction_to_qlearning_reinforcement/,spsayakpaul,1562422768,,0,1
362,2019-7-6,2019,7,6,23,c9u612,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/c9u612/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1562423322,[removed],0,1
363,2019-7-6,2019,7,6,23,c9u6gf,[D] How to plan and execute your ML and DL projects: https://blog.floydhub.com/structuring-and-planning-your-machine-learning-project/,https://www.reddit.com/r/MachineLearning/comments/c9u6gf/d_how_to_plan_and_execute_your_ml_and_dl_projects/,spsayakpaul,1562423390,,2,0
364,2019-7-7,2019,7,7,1,c9viw0,DevBot 2.0 sets timed run at Goodwood FOS! 160kph top speed and 66.96secs!!,https://www.reddit.com/r/MachineLearning/comments/c9viw0/devbot_20_sets_timed_run_at_goodwood_fos_160kph/,AIthatDrives,1562430769,,0,1
365,2019-7-7,2019,7,7,1,c9voxp,[D] Examples of when to use machine learning and when to use an expert system,https://www.reddit.com/r/MachineLearning/comments/c9voxp/d_examples_of_when_to_use_machine_learning_and/,trnka,1562431651,"I work in machine learning for healthcare and people often come to me suggesting an expert system. Usually what they mean is a hand-crafted version of a decision tree. I haven't been able to find any good articles really articulating the cost/benefit of each approach with examples and I'm wondering: What has everyone else's experience has been?

Examples I usually give:

* One part of the core algorithm in an app was a giant decision tree from \~5-10 years ago, 3-4 deep with very complex conditions. One of the smartest guys around decided to understand and document the code. Even after several days of experimentation and talking to the author, he couldn't explain it all. Due to the feeling that it was important, we weren't able to remove the unknown code and therefore could clean it up either. That code was doing pattern matching for mobile keyboards - would've been a good fit for ML.
* Another project would jump-start text query parsers with a hand-written grammar. Once it seemed reasonable they'd release it, collect query data, annotate it, then replace it with an ML system. That meant a much faster time for both the first and second releases.
* I've seen a great example in a book but I forget which - it talks about classifying something as a bird or not and demonstrates the challenges of building a high quality system.",65,109
366,2019-7-7,2019,7,7,2,c9vukb,[Free Course - Limit Time] - Machine Learning IOS 11,https://www.reddit.com/r/MachineLearning/comments/c9vukb/free_course_limit_time_machine_learning_ios_11/,upworknepal,1562432482,[removed],0,1
367,2019-7-7,2019,7,7,2,c9vzkb,Need help with this!,https://www.reddit.com/r/MachineLearning/comments/c9vzkb/need_help_with_this/,blackbird81506,1562433192,"Can anybody help me finding the 

1) Stochastic gradient descent

2) gradient descent

3) mini-batch gradient descent

FOR THIS FILE

 [https://github.com/blackbird81506/Linear-regressiom/blob/master/father\_son\_heights.csv](https://github.com/blackbird81506/Linear-regressiom/blob/master/father_son_heights.csv) 

&amp;#x200B;

I need coefficient, intercept &amp;RMSE,RSE,R2 for all 3 algos",0,1
368,2019-7-7,2019,7,7,2,c9w0m1,Google I/O 2019 | Machine Learning Zero to Hero,https://www.reddit.com/r/MachineLearning/comments/c9w0m1/google_io_2019_machine_learning_zero_to_hero/,LividDragonfly,1562433340,,0,1
369,2019-7-7,2019,7,7,2,c9whdm,[P] 3D Visualization of inheritance for NeurIPS 2018 models.,https://www.reddit.com/r/MachineLearning/comments/c9whdm/p_3d_visualization_of_inheritance_for_neurips/,postmachines,1562435719,"Hello,

This is a visualization of dependencies for some methods of NeurIPS 2018. This graph helped me in my research, and at some moment I realised that it should become public. Now I am preparing it for release. 

![video](91m6ftmdvp831 "" "")

Graph is built on dependencies between individual models and their components. Simple example: Generative Adversarial Nets (GAN) consist of Generator(GEN), Discriminator (DIS) and Adversarial Training (AT) method, and  Adversarial Autoencoder (AAE) is based on DIS,  AT and Autoencoder (AE).  So for AAE we will have an edge to AE, AT and DIS.

I use it to get associations that can help create something new, and I hope that it will be useful not only for me.

More information coming soon.

Please share and retweet if you are interested in it: [https://twitter.com/INFORNODEMO/status/1147332691999842305](https://twitter.com/INFORNODEMO/status/1147332691999842305)",0,1
370,2019-7-7,2019,7,7,3,c9wo5o,[D] 3D Visualization of inheritance for NeurIPS 2018 models.,https://www.reddit.com/r/MachineLearning/comments/c9wo5o/d_3d_visualization_of_inheritance_for_neurips/,postmachines,1562436678,"Hello

I have made a visualization of dependencies for some methods of NeurIPS 2018. This graph helped me in my research, and at some moment I realised that it should become public. Now I am preparing it for release.

[https://twitter.com/INFORNODEMO/status/1147332691999842305](https://twitter.com/INFORNODEMO/status/1147332691999842305)

Graph is built on dependencies between individual models and their components. Simple example: Generative Adversarial Nets (GAN) consist of Generator(GEN), Discriminator (DIS) and Adversarial Training (AT) method, and Adversarial Autoencoder (AAE) is based on DIS, AT and Autoencoder (AE). So for AAE we will have an edge to AE, AT and DIS.

I use it to get associations that can help create something new, and I hope that it will be useful not only for me.

More information coming soon.

Please share and retweet if you are interested in it: [https://twitter.com/INFORNODEMO/status/1147332691999842305](https://twitter.com/INFORNODEMO/status/1147332691999842305)",2,30
371,2019-7-7,2019,7,7,4,c9xdr4,[D] An Overview of Methods in Super Resolution,https://www.reddit.com/r/MachineLearning/comments/c9xdr4/d_an_overview_of_methods_in_super_resolution/,thatbrguy_,1562440340,"This article aims to distill some of the important concepts involved in using neural networks for super resolution. Emphasis was provided on setting up the loss functions and metrics while keeping the blog as ""easy-to-follow"" as possible. Link: [https://medium.com/beyondminds/an-introduction-to-super-resolution-using-deep-learning-f60aff9a499d?source=friends\_link&amp;sk=f12ee4a026d08747468297ac56b8df29](https://medium.com/beyondminds/an-introduction-to-super-resolution-using-deep-learning-f60aff9a499d?source=friends_link&amp;sk=f12ee4a026d08747468297ac56b8df29)

&amp;#x200B;

I would love to hear your thoughts and feedback regarding the same, thanks!",3,15
372,2019-7-7,2019,7,7,4,c9xfp1,Hello guys which libraries should i learn for machine learning,https://www.reddit.com/r/MachineLearning/comments/c9xfp1/hello_guys_which_libraries_should_i_learn_for/,QLifeee,1562440625,,4,0
373,2019-7-7,2019,7,7,5,c9y34v,[D] top machine lelarning startups in India and what are they trying to solve,https://www.reddit.com/r/MachineLearning/comments/c9y34v/d_top_machine_lelarning_startups_in_india_and/,dchatterjee172,1562444109,,0,1
374,2019-7-7,2019,7,7,5,c9y77w,[D] top machine learning / data science startups in India and what are they trying to solve,https://www.reddit.com/r/MachineLearning/comments/c9y77w/d_top_machine_learning_data_science_startups_in/,dchatterjee172,1562444722,,1,0
375,2019-7-7,2019,7,7,6,c9yojq,Machine Learning: Uncovering Hidden Knowledge,https://www.reddit.com/r/MachineLearning/comments/c9yojq/machine_learning_uncovering_hidden_knowledge/,mnluxury11,1562447246,,0,1
376,2019-7-7,2019,7,7,6,c9yqk2,Which takes longer to compute: Leaky ReLu or ELU?,https://www.reddit.com/r/MachineLearning/comments/c9yqk2/which_takes_longer_to_compute_leaky_relu_or_elu/,Jandevries101,1562447531,[removed],0,1
377,2019-7-7,2019,7,7,6,c9yv99,Time series classification - How much overlap?,https://www.reddit.com/r/MachineLearning/comments/c9yv99/time_series_classification_how_much_overlap/,007ara,1562448235,[removed],0,1
378,2019-7-7,2019,7,7,8,ca08os,[D] An Abusive Relationship In #NLProc,https://www.reddit.com/r/MachineLearning/comments/ca08os/d_an_abusive_relationship_in_nlproc/,otoyou1234,1562456061,"This has been circulating around the NLP/ML circles:

[https://cs.nyu.edu/\~kann/documents/relationshipsinnlp.pdf](https://cs.nyu.edu/~kann/documents/relationshipsinnlp.pdf)

&amp;#x200B;

I won't out ""C"" but most people in NLP know who he is (a fairly well-known junior faculty).

&amp;#x200B;

Obviously she was treated horribly and the guy is an asshole. And she was incredibly brave to post this. However, I can't help feel that this is beginning to verge on ""personal"" rather than professional. The one allegation that definitely warrants disciplinary action would be her claiming that the ex deliberately gave her paper a low review (this could easily be verified by the committee). But aside from this, it seems like more of a she-said he-said type of thing...

&amp;#x200B;

Thoughts?",26,4
379,2019-7-7,2019,7,7,8,ca09tj,Perceptron usage for better branch prediction in AMD Zen2 cores,https://www.reddit.com/r/MachineLearning/comments/ca09tj/perceptron_usage_for_better_branch_prediction_in/,NewFrontierSystems,1562456239,,0,1
380,2019-7-7,2019,7,7,8,ca0ftw,How difficult would it be to use image recognition to say what ingredients is in your chipotle bowl ?,https://www.reddit.com/r/MachineLearning/comments/ca0ftw/how_difficult_would_it_be_to_use_image/,Rbent98,1562457232,[removed],0,1
381,2019-7-7,2019,7,7,10,ca1k3o,"[P] Sectional Scientific Summarization Dataset, soft release",https://www.reddit.com/r/MachineLearning/comments/ca1k3o/p_sectional_scientific_summarization_dataset_soft/,BatmantoshReturns,1562464295,"I made a dataset of research paper section summaries; each datapoint contains a section of a research paper (intro, background, methods, results, discussion, conclusion, etc. ) and a corresponding summary. 

The dataset contains ~4.3 million data points from ~11 million papers. 

Unfortunately a lot of the files I host have gone down pretty often due to too many downloads, ever since my group was recently featured in a Siraj video (lol) . So we're going to be doing a soft release for now, if you want a link please PM. We especially encourage those with previous experience with summarization and/or scientific texts to start playing around with it.",2,2
382,2019-7-7,2019,7,7,10,ca1k4a,Healthcare and the Importance of Big Data,https://www.reddit.com/r/MachineLearning/comments/ca1k4a/healthcare_and_the_importance_of_big_data/,iramirsina,1562464298,,0,1
383,2019-7-7,2019,7,7,10,ca1mo7,"Quick Comparison of Stanford's Machine Learning Coursera (with Professor Andrew Ng) and the Google Developers ML Crash Course on YouTube. No offense intended, solely for comedic intent. Both courses are very good and useful. Also, that screenshot is from the Google Dev Course, not created by me.",https://www.reddit.com/r/MachineLearning/comments/ca1mo7/quick_comparison_of_stanfords_machine_learning/,ThunderingWest4,1562464774,,0,1
384,2019-7-7,2019,7,7,11,ca1xrd,Open Image By Matplotlib and OpenCV | Python,https://www.reddit.com/r/MachineLearning/comments/ca1xrd/open_image_by_matplotlib_and_opencv_python/,spokhrel,1562466799,,0,1
385,2019-7-7,2019,7,7,12,ca2dfs,[D] Terrence Sejnowski's Deep Learning Revolution,https://www.reddit.com/r/MachineLearning/comments/ca2dfs/d_terrence_sejnowskis_deep_learning_revolution/,murphinate,1562469709,"https://www.goodreads.com/book/show/36722636-the-deep-learning-revolution

I am about a third of the way into this book and it has me feeling a little forgetful of where the community is going with DL.

The book is mostly a memoir of Sejnowski and the work he did with Hinton to help get the community to where it is today. There are chapters dedicated to things that I almost never see discussed anymore, such as how the brain actually functions as a general purpose problem solving machine. Paper after paper it seems like there is a novel tweak on a DL application, with sometimes exciting results, but I can't remember the last time I was excited about what the community has produced since AlphaZero.

Are we entering a maturity phase in DL? Simply endlessly optimizing what we already know? Where is the new epicenter of revolutionary thought? Why is there so little talk of neuroscience/biology in the DL community despite the strong roots?

Disclosure: I am not an academic. I just like to read a lot about ML/DL, gene modification, quantum computing, and economics. Just the cool stuff.

So where do you guys think we are? Where are we headed?",48,75
386,2019-7-7,2019,7,7,13,ca2uh5,Is Artificial Neural Network Turing Recognizable?,https://www.reddit.com/r/MachineLearning/comments/ca2uh5/is_artificial_neural_network_turing_recognizable/,HanSatyam,1562472957,"Most of Neural Network requires more memory for computation and sometimes it is intractable. Is there any relation between ANN and Turing Machine?
Any new architecture designed is assumed to be Turing Machine.",0,1
387,2019-7-7,2019,7,7,13,ca2z7j,What are the challenges of permissionless contributions for training data?,https://www.reddit.com/r/MachineLearning/comments/ca2z7j/what_are_the_challenges_of_permissionless/,Sixophrenia,1562473895,"I may have an interesting blockchain solution to help balance crowd sourced training data. 

I'm sure data is hard to get in the first place. This method could also help get contributors paid. Based on a ratio of their share of space.

&amp;#x200B;

Or maybe in a creative space somehow incentivize users to contribute to the machine learning model by individually creating pieces of art or something.

&amp;#x200B;

The power of this particular blockchain technology is that it has the power to forcibly balance (through market forces) the  ratio of influence individuals have to the collective. And I figured that could somehow be useful when it comes to getting good crowdsourced data.",0,1
388,2019-7-7,2019,7,7,13,ca2zkl,[Free Course  Udemy]  Machine Learning:Linear Regression[Step by Step Guide],https://www.reddit.com/r/MachineLearning/comments/ca2zkl/free_course_udemy_machine_learninglinear/,upworknepal,1562473966,[removed],0,1
389,2019-7-7,2019,7,7,13,ca31od,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/ca31od/data_science_career_track_prep_course/,HannahHumphreys,1562474409,[removed],0,1
390,2019-7-7,2019,7,7,13,ca359u,[D] How to use a Keras model in real-time?,https://www.reddit.com/r/MachineLearning/comments/ca359u/d_how_to_use_a_keras_model_in_realtime/,Zman420,1562475161,"Hi all,

None my previous deployments of a keras model have been time critical - waiting a few seconds for the python script to load in all the libraries (like tensorflow, numpy), load up the model, etc etc. was fine. 

However, I'm now dealing with a situation where I'm going to need to query my deployed model every 1 second or so - and get a response back as quickly as possible. This means that my usual method of calling a python script via a .bat file, passing in an argument with my csv filename(using Process() in my .net app) and reading the stdout will not suffice.  

I need a way to keep a python session alive somehow, with the libraries and model loaded into ram - waiting for some sort of call that points towards my csv file(s) that need to be queried against the model.  Note that I *must* initiate this call [and get a response back into] my C# code, as this is part of a bigger system that relies on other C# libraries.

Anyone have thoughts about how to go about this?",15,12
391,2019-7-7,2019,7,7,14,ca38zq,[D] Jsonnet for configuring Deep Learning experiments,https://www.reddit.com/r/MachineLearning/comments/ca38zq/d_jsonnet_for_configuring_deep_learning/,anandaseelan,1562475934,,0,1
392,2019-7-7,2019,7,7,16,ca43z5,Machine Learning: A Truthy Lie?,https://www.reddit.com/r/MachineLearning/comments/ca43z5/machine_learning_a_truthy_lie/,meancoder,1562483009,,0,1
393,2019-7-7,2019,7,7,16,ca4e5j,Data Science. The Central Limit Theorem and sampling,https://www.reddit.com/r/MachineLearning/comments/ca4e5j/data_science_the_central_limit_theorem_and/,luminoumen,1562485580,,0,1
394,2019-7-7,2019,7,7,16,ca4e69,Anyone suggest me which is the best programming language for Machine learning and Artificial Intelligence. Thank you.,https://www.reddit.com/r/MachineLearning/comments/ca4e69/anyone_suggest_me_which_is_the_best_programming/,utsabqwerty,1562485584,,0,1
395,2019-7-7,2019,7,7,17,ca4koe,The Implementation,https://www.reddit.com/r/MachineLearning/comments/ca4koe/the_implementation/,Wenderu84,1562487279,[removed],0,1
396,2019-7-7,2019,7,7,17,ca4os6,"Multi-reference Tacotron by Intercross Training for Style Disentangling, Transfer and Control in Speech Synthesis",https://www.reddit.com/r/MachineLearning/comments/ca4os6/multireference_tacotron_by_intercross_training/,wjlwjl1994,1562488381,,1,8
397,2019-7-7,2019,7,7,17,ca4q7z,[D] How to plan and execute your ML and DL projects,https://www.reddit.com/r/MachineLearning/comments/ca4q7z/d_how_to_plan_and_execute_your_ml_and_dl_projects/,pirate7777777,1562488754,"Hi everyone, one of our writers wrote [an article](https://blog.floydhub.com/structuring-and-planning-your-machine-learning-project/) on [FloydHub blog](https://blog.floydhub.com/) that aims to give a checklist which a machine learning practitioner can refer from time-to-time in his own projects. The article tries to combine the pieces of advice collected from practitioners like Andrej Karpathy, Josh Tobin and at the same time includes many experiences.

&amp;#x200B;

Did we miss anything? Is there something you would like to see in depth in the next articles?  


I hope you enjoy it!",4,0
398,2019-7-7,2019,7,7,17,ca4u8i,Autocomplete Python with a simple LSTM,https://www.reddit.com/r/MachineLearning/comments/ca4u8i/autocomplete_python_with_a_simple_lstm/,mlvpj,1562489820,[removed],0,1
399,2019-7-7,2019,7,7,18,ca4v8r,[P] Autocomplete Python with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/ca4v8r/p_autocomplete_python_with_deep_learning/,mlvpj,1562490082,"We were toying with this simple setup to autocomplete python with deep learning

[https://github.com/vpj/python\_autocomplete](https://github.com/vpj/python_autocomplete)

The metric we used to measure the performance was the number of keystrokes saved. That is the model gives a single suggestion of length L and L-1 keystrokes are saved if it matches actual code. It performs pretty well saving around 30-50% key strokes despite being naive.

Next we want to try different architectures and improve inference performance. Right now it's too slow for any actual usage.

If you have any ideas or suggestions please do share.",31,132
400,2019-7-7,2019,7,7,19,ca5aq1,Machine Learning is all about Common Sense,https://www.reddit.com/r/MachineLearning/comments/ca5aq1/machine_learning_is_all_about_common_sense/,andrea_manero,1562494043,[removed],0,1
401,2019-7-7,2019,7,7,19,ca5ho2,Why is my CNN model being trained in less time on my local CPU than hosted options?,https://www.reddit.com/r/MachineLearning/comments/ca5ho2/why_is_my_cnn_model_being_trained_in_less_time_on/,phao5814,1562495734,[removed],0,1
402,2019-7-7,2019,7,7,21,ca64s9,I made some notes on Google's BERT paper and I would love your feedback.,https://www.reddit.com/r/MachineLearning/comments/ca64s9/i_made_some_notes_on_googles_bert_paper_and_i/,Yuras_Stephan,1562501152,[removed],0,1
403,2019-7-7,2019,7,7,21,ca6jzh,What after Andrew ng course?,https://www.reddit.com/r/MachineLearning/comments/ca6jzh/what_after_andrew_ng_course/,Invinci_1,1562504304,[removed],0,1
404,2019-7-7,2019,7,7,22,ca6zfq,The Simpler the Better: Constant Velocity for Pedestrian Motion Prediction,https://www.reddit.com/r/MachineLearning/comments/ca6zfq/the_simpler_the_better_constant_velocity_for/,RebornHugo,1562507147,[removed],0,1
405,2019-7-7,2019,7,7,23,ca7je7,[Q] Resources on Assortment Optimization,https://www.reddit.com/r/MachineLearning/comments/ca7je7/q_resources_on_assortment_optimization/,Juzkev,1562510523,[removed],0,1
406,2019-7-7,2019,7,7,23,ca7mc4,Choosing between a position in the industry and a PhD,https://www.reddit.com/r/MachineLearning/comments/ca7mc4/choosing_between_a_position_in_the_industry_and_a/,dd_hexagon,1562511002,[removed],0,1
407,2019-7-7,2019,7,7,23,ca7pud,Tutorial - foundations of machine learning and data science for developers,https://www.reddit.com/r/MachineLearning/comments/ca7pud/tutorial_foundations_of_machine_learning_and_data/,andrea_manero,1562511570,[removed],0,1
408,2019-7-7,2019,7,7,23,ca7puo,[D] Resources on Assortment Optimization,https://www.reddit.com/r/MachineLearning/comments/ca7puo/d_resources_on_assortment_optimization/,Juzkev,1562511573,"Hey data scientists of Reddit, would like to seek some advice from the experts here.

I am trying to implement an assortment optimization problem, where you have to pick a subset of good from the master set which maximises the objective (i.e. revenue/profit). Eventually, I would like to work towards a tractable SKU-level sale forecast function (choice model) that accounts for cannibalization, substitution and seasonality. As I understand it, this function would have to be fed into an optimisation algorithm (i.e. linear integer programming).

From my readings, I gather that Multinomial Logit Model (MNL) should be the baseline/ starting point. However, I can't seem to grasp how I could extend MMNL to account for cannibalization, substitution and seasonality.

Can any kind souls point me to a good place to start on optimization in the field of operations research for assortment optimization?

&amp;#x200B;

tldr:  Where can I find resources on assortment optimization with cannibalization, substitution and seasonality effects?",1,3
409,2019-7-8,2019,7,8,0,ca84p4,The State of Voice Assistants: Where's the AI?,https://www.reddit.com/r/MachineLearning/comments/ca84p4/the_state_of_voice_assistants_wheres_the_ai/,neuronDen,1562513890,,1,1
410,2019-7-8,2019,7,8,0,ca88r4,[P] A little gadget that plays rock-paper-scissors slightly better than random using a small quantized RNN running on an 8-bit microcontroller,https://www.reddit.com/r/MachineLearning/comments/ca88r4/p_a_little_gadget_that_plays_rockpaperscissors/,Almoturg,1562514520,"[Project video here](https://www.youtube.com/watch?v=iuTKBHW0OaU)

[Code and PCB/CAD design files here](https://github.com/PaulKlinger/rps-rnn)

I was looking for some kind of project that let me combine my love for building [small electronics/3d-printing projects](https://paulklinger.com/projects/) and machine learning, and this is what I came up with.

The machine learning side of this isn't too interesting, just [a small 3 layer vanilla RNN](https://raw.githubusercontent.com/PaulKlinger/rps-rnn/master/diagrams/rnn_architecture.png) (all layers with 10-d state, trained in tensorflow/keras) that takes as input the moves of the two players and outputs a prediction for the opponents next move. The data comes from [roshambo.me](https://roshambo.me) via this [blog article](https://roshambo.me/ via https://justincollier.com/life-hacks/how-to-win-rock-paper-scissors/). I added some simulated data of periodic sequences, because that seems like the sort of thing people might try out when playing against an ""AI"".

Without the simulated data the model gets something like 38% accuracy on the test set (compared to 33% playing randomly). One nice thing about rock-paper-scissors is that you can't do much better than random, so there isn't much pressure ;)

I had some problems getting training with larger batches to work (either with just padding or padding and masking the gradients) so I just trained it with batch_size=1, which wasn't too bad for such a small network.

The probably more interesting part of this project is running that network on a small microcontroller. I quantized the network weights (to 8-bit integers) to save some space (it only has 2kiB RAM and 16kiB flash, although the network I ended up using would actually fit without that). The calculations are done using software floats, as performance isn't really an issue. The C-code running on the microcontroller is all custom, not that it's particularly complicated.

The finished device is about the size of a coin (but thicker). It uses a custom pcb (ordered from [dirtyPCBs](https://dirtypcbs.com/store/pcbs)), is powered by a coin cell battery, and has a nice 3d-printed case. The form-factor is pretty close to my [electronic dice](https://www.reddit.com/r/electronics/comments/akrl7d/compact_universal_electronic_dice_building_small/) so that wasn't too much work.",43,196
411,2019-7-8,2019,7,8,1,ca8j6r,Recommendations for more books (any field) like 'The Hundred-Page Machine Learning Book'?,https://www.reddit.com/r/MachineLearning/comments/ca8j6r/recommendations_for_more_books_any_field_like_the/,TotalPerspective,1562516099,[removed],0,1
412,2019-7-8,2019,7,8,1,ca8ks3,Machine learning,https://www.reddit.com/r/MachineLearning/comments/ca8ks3/machine_learning/,bdaigle9169,1562516339,"I'm 49 an a sole parent, I'm not College educated. Is it possible an wroth it to try an Learn machine learning? Will someone like me get anywhere?",0,1
413,2019-7-8,2019,7,8,2,ca9ama,Overview of NAS works at CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/ca9ama/overview_of_nas_works_at_cvpr_2019/,_dr_sleep,1562520091,,0,1
414,2019-7-8,2019,7,8,2,ca9cf9,[D] Overview of NAS research at CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/ca9cf9/d_overview_of_nas_research_at_cvpr_2019/,_dr_sleep,1562520346,,0,1
415,2019-7-8,2019,7,8,2,ca9dri,Another classic collection out,https://www.reddit.com/r/MachineLearning/comments/ca9dri/another_classic_collection_out/,BhanujeetC,1562520541,,0,1
416,2019-7-8,2019,7,8,2,ca9gqe,Connecting R to an Android App,https://www.reddit.com/r/MachineLearning/comments/ca9gqe/connecting_r_to_an_android_app/,akkatips,1562520950,I am looking into using an android app to distribute predictions from a machine learning algorithm I have in R. The predictions are updated on a weekly basis and I was wondering the best way to achieve this? Is this possible or should the app just connect to a website hosting an R shiny app. Any suggestions are welcome.,0,1
417,2019-7-8,2019,7,8,2,ca9okd,Is AIGrant planning to continue/start accepting applications this year?,https://www.reddit.com/r/MachineLearning/comments/ca9okd/is_aigrant_planning_to_continuestart_accepting/,the1fundamental,1562522014,[removed],0,1
418,2019-7-8,2019,7,8,3,ca9v5k,This project will disrupt the VR and AI fields,https://www.reddit.com/r/MachineLearning/comments/ca9v5k/this_project_will_disrupt_the_vr_and_ai_fields/,Edman1022,1562522924,[removed],2,2
419,2019-7-8,2019,7,8,3,caa289,Synthesis Data Generation,https://www.reddit.com/r/MachineLearning/comments/caa289/synthesis_data_generation/,JayRathod3497,1562523906,How to generate signals using **Bayesian network**?,0,1
420,2019-7-8,2019,7,8,3,caaatm,[P] Deep Learning for pose estimation,https://www.reddit.com/r/MachineLearning/comments/caaatm/p_deep_learning_for_pose_estimation/,ixeption,1562525092,"It's a bit older, but if you are interested in deep learning for pose estimation, you might find [this](http://digital-thinking.de/tensorflow-deep-learning-for-pose-estimation/) interesting. The post includes a small introduction to deep learning and how to deal with such a problem by labeling with a Gaussian distribution, based on the work from 2016.",1,6
421,2019-7-8,2019,7,8,5,cabhsm,"Delving into the nature of random forest, walking through an example, and comparing it to logistic regression. Can someone help explain why logistic regression is a better model for this dataset?",https://www.reddit.com/r/MachineLearning/comments/cabhsm/delving_into_the_nature_of_random_forest_walking/,ahershy,1562531080,,0,1
422,2019-7-8,2019,7,8,5,cabm3v,Machine Learning used to solve disputed songwriting credits of Beatles hits. May resolve arguments over who was the main contributor towards some of the iconic songs of beatles. John Lennon or Paul McCartney?,https://www.reddit.com/r/MachineLearning/comments/cabm3v/machine_learning_used_to_solve_disputed/,bhavik911,1562531693,,0,1
423,2019-7-8,2019,7,8,5,cabsp7,Any applications of machine learning to the field of history?,https://www.reddit.com/r/MachineLearning/comments/cabsp7/any_applications_of_machine_learning_to_the_field/,GreenFrog76,1562532647,Just wondering what contributions it might be able to make in academic history.,0,1
424,2019-7-8,2019,7,8,6,cac750,[R] End-to-end neural system identification with neural information flow,https://www.reddit.com/r/MachineLearning/comments/cac750/r_endtoend_neural_system_identification_with/,kseeliger,1562534705,"We propose a method for training convolutional neural networks to learn similar systems of transformations as exist in the human brain. We recorded brain data from exposing a participant to plenty of visual data (a TV series) and use the same visual data as input to a network. We train the network by attaching brain activity observation models for different visual system regions to the layer activity tensors (through low-rank tensor decomposition). 

&amp;#x200B;

The sole training signal is the error between the measured brain activity and the activity predicted by these observation models. We could verify that the model learns several known properties of the visual system.  


[https://www.biorxiv.org/content/10.1101/553255v2](https://www.google.com/url?q=https://www.biorxiv.org/content/10.1101/553255v2&amp;sa=D&amp;source=hangouts&amp;ust=1562597455530000&amp;usg=AFQjCNEhr1NKRpdoyyVmRAu6iNRY0OxoUQ)",0,10
425,2019-7-8,2019,7,8,6,cack5g,[D] Neural Architecture Research (NAS) at CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/cack5g/d_neural_architecture_research_nas_at_cvpr_2019/,_dr_sleep,1562536684,"Hello, I compiled a list of papers from this year CVPR on the topic of NAS with short summaries here: [https://drsleep.github.io/NAS-at-CVPR-2019/](https://drsleep.github.io/NAS-at-CVPR-2019/)",11,36
426,2019-7-8,2019,7,8,7,cacqdw,Am I over-reacting in saying that this is an incredibly exciting development? Nature: Unsupervised word embeddings capture latent knowledge from materials science literature,https://www.reddit.com/r/MachineLearning/comments/cacqdw/am_i_overreacting_in_saying_that_this_is_an/,supersystemic-ly,1562537598,,0,1
427,2019-7-8,2019,7,8,7,cad7b5,[D] Research Engineer FAQ,https://www.reddit.com/r/MachineLearning/comments/cad7b5/d_research_engineer_faq/,inarrears,1562540234,"Excerpt from a [post](https://amatsukawa.github.io/re/) by Aki Matsukawa who worked at deepmind, google, and twitter before as an RSWE:

What does a research engineer do?

Responsibilities of a research engineer can differ quite a lot by company, and even by team within the same company. But generally, doing effective research means taking advantage of the work that others have done, be it algorithm implementations, model checkpoints, or evaluation tools. In order for this to happen at scale, a level of software engineering discipline is necessary. I define the primary goal of a research engineer as enabling, contributing to, and accelerating ML research by bringing engineering expertise to the projects. Some examples of a research engineers responsibilities are:

- Implementing algorithms and related baselines under a common API to allow for rapid experimentation.

- Setting up distributed training.

- Creating evaluation tools in Jupyter notebooks.

Often, a research engineer also makes contributions to the research itself, especially using insights and intuitions derived from implementing and iterating on experiments.

https://amatsukawa.github.io/re/",6,15
428,2019-7-8,2019,7,8,8,cad9pu,I found the link between ANN's backpropagation and the STDP-like Hebbian learning that happens in the brain: it works if using SNNs evolving through a time axis.,https://www.reddit.com/r/MachineLearning/comments/cad9pu/i_found_the_link_between_anns_backpropagation_and/,GChe,1562540604,"Hi all, 

So I think I've discovered something amazing: the link between backpropagation and STDP-like Hebbian learning. 

- At first, I simply thought ""hey, what about coding a Spiking Neural Network using an automatic differentiation framework?"" Here it is.
- Then I started reading on how to achieve that, such as reading on Hebbian learning. Quickly explained: Hebbian learning is somehow the saying that ""neurons that fire together, wire together"".
- Then, I think I've discovered something amazing. What if when doing backpropagation on a Spiking Neural Network (SNN), Hebbian learning would take place naturally as a side effect of adding that refractory time axis?
- I had the opportunity to discuss that idea with Yoshua Bengio at a conference, and I couldn't get the idea out of my head past that point, so I coded it as follow.
- As a conclusion, I think that the link between deep learning and the human brain is closer than we might think: backpropagation is akin to Hebbian learning.

I wrote a blog post about it - [Spiking Neural Network (SNN) with PyTorch: towards bridging the gap between deep learning and the human brain](https://guillaume-chevalier.com/spiking-neural-network-snn-with-pytorch-where-backpropagation-engenders-stdp-hebbian-learning/). 

I'd be glad to have feedback on this. I feel like it might be a big discovery, although I'm not sure.",0,1
429,2019-7-8,2019,7,8,8,cadmyx,What are employers looking for?,https://www.reddit.com/r/MachineLearning/comments/cadmyx/what_are_employers_looking_for/,jande48,1562542712,[removed],0,1
430,2019-7-8,2019,7,8,9,cae59l,[D] Advanced Courses Update,https://www.reddit.com/r/MachineLearning/comments/cae59l/d_advanced_courses_update/,Maplernothaxor,1562545735,The link on the sidebar is getting old. Was wondering if there were new more advanced ML courses (PhD level) which youd recommend.,26,15
431,2019-7-8,2019,7,8,9,cae9xv,[R] Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,https://www.reddit.com/r/MachineLearning/comments/cae9xv/r_minimally_distorted_adversarial_examples_with_a/,downtownslim,1562546505,,1,7
432,2019-7-8,2019,7,8,10,caemr3,First Machine Learning Project,https://www.reddit.com/r/MachineLearning/comments/caemr3/first_machine_learning_project/,WigglyGu,1562548669,[removed],0,1
433,2019-7-8,2019,7,8,10,caeom8,[R] Large Scale Adversarial Representation Learning,https://www.reddit.com/r/MachineLearning/comments/caeom8/r_large_scale_adversarial_representation_learning/,xternalz,1562549008,,3,24
434,2019-7-8,2019,7,8,11,cafeql,[D] Reinforcement Learning Efficient Template (MPI),https://www.reddit.com/r/MachineLearning/comments/cafeql/d_reinforcement_learning_efficient_template_mpi/,ejmejm1,1562553386,"After finding myself repeatedly writing and rewriting the same code every time I started a new project, I decided to make a simple but efficient template that works for many RL algorithms. The template uses MPI to simulate environments on several cores in parallel to make the training process very efficient. If you work with RL or are interested in getting into it, I would highly recommend doing this. Done correctly, you should notice a massive boost in training efficiency, especially when using complex environments.

&amp;#x200B;

Project Link: [https://github.com/ejmejm/RL-template](https://github.com/ejmejm/RL-template)",0,1
435,2019-7-8,2019,7,8,11,cafft2,[P] Reinforcement Learning Efficient Template (MPI),https://www.reddit.com/r/MachineLearning/comments/cafft2/p_reinforcement_learning_efficient_template_mpi/,ejmejm1,1562553558,"[Link to Project](https://github.com/ejmejm/RL-template)

&amp;#x200B;

After finding myself repeatedly writing and rewriting the same code every time I started a new project, I decided to make a simple but efficient template that works for many RL algorithms. The template uses MPI to simulate environments on several cores in parallel to make the training process very efficient.

&amp;#x200B;

If you work with RL or are interested in getting into it, I would highly recommend doing this. Done correctly, you should notice a massive boost in training efficiency, especially when using complex environments.",9,45
436,2019-7-8,2019,7,8,14,cagrah,I have outdated ML knowledge but looking to get back into the field. What online courses or resources do you recommend? Where should I start?,https://www.reddit.com/r/MachineLearning/comments/cagrah/i_have_outdated_ml_knowledge_but_looking_to_get/,hogtiedandtrippin,1562562042,"Hi all, I have a PhD in comp sci and my dissertation involved ML with another discipline. But, it's been 10 years, I did govt work in the other discipline for many of those years, had 3 kids and have been knee-deep in diapers for the last few.... My knowledge is way outdated.

&amp;#x200B;

I'm thinking of trying to go back to work, most likely in some kind of ML/deep learning/data science role.  I'm still rough on the terminology even! I've been looking online at EdX and Coursera. Any tips on where to start?  For both a review of the basics and more practical implementation stuff that would help me know what the heck I'm doing if I do get a job.

I appreciate any guidance as to what I should be looking for.

Cheers!",0,1
437,2019-7-8,2019,7,8,14,cagxwa,The magic of t-SNE for visualizing your data features,https://www.reddit.com/r/MachineLearning/comments/cagxwa/the_magic_of_tsne_for_visualizing_your_data/,sabinemaennel,1562563329,,0,1
438,2019-7-8,2019,7,8,14,cah5nu,[P] Automation Horovod using Ansible,https://www.reddit.com/r/MachineLearning/comments/cah5nu/p_automation_horovod_using_ansible/,nlkey2022,1562564909,"[https://github.com/graykode/horovod-ansible](https://github.com/graykode/horovod-ansible)

&amp;#x200B;

Horovod is a distributed training framework for TensorFlow, Keras, PyTorch, and MXNet. It's good distributed training framework but it's a hassle to set the environment for all nodes. So i make automation script with Ansible.

You can train with Tensorflow, Pytorch Example on AWS or On-Perm Thanks",0,4
439,2019-7-8,2019,7,8,15,cahiew,Elon Musk went out of the way to hire the most talented engineers for SpaceX,https://www.reddit.com/r/MachineLearning/comments/cahiew/elon_musk_went_out_of_the_way_to_hire_the_most/,planck221,1562567512,[removed],0,1
440,2019-7-8,2019,7,8,16,cahws2,[P] From DQN to Rainbow: A step-by-step Colab + Pytorch tutorial,https://www.reddit.com/r/MachineLearning/comments/cahws2/p_from_dqn_to_rainbow_a_stepbystep_colab_pytorch/,Curt-Park,1562570457,"* Project link: https://github.com/Curt-Park/rainbow-is-all-you-need

Hi, ML redditors all around the world! I made a Pytorch RL tutorial which consists of all methods from DQN to Rainbow:

1. DQN
2. Double DQN
3. Prioritized Experience Replay
4. Dueling Networks
5. Noisy Network
6. Categorical DQN (C51)
7. N-step Learning
8. Rainbow

Every chapter contains both theoretical backgrounds and object-oriented implementation, and thanks to Colab, you can execute them and render the results without any installation even on your smartphone!

I hope it will be helpful for someone. :)
Cheers.",6,208
441,2019-7-8,2019,7,8,16,cai6cx,[P] I made some notes on Google's BERT paper and I would appreciate your feedback.,https://www.reddit.com/r/MachineLearning/comments/cai6cx/p_i_made_some_notes_on_googles_bert_paper_and_i/,Yuras_Stephan,1562572575,"I am trying to read more papers and write more myself. In order to accomplish this goal I have decided to read papers on a regular basis and write small essays containing notes on the paper. I decided to start with Google's BERT paper, which came highly recommended in a WAYR thread here.

 I would love to hear any feedback you have on these notes so that I may improve my reading and writing skills.

https://stephanheijl.com/notes_on_bert.html",4,2
442,2019-7-8,2019,7,8,17,cai7kj,Is Discriminator training stopped anywhere on this code?,https://www.reddit.com/r/MachineLearning/comments/cai7kj/is_discriminator_training_stopped_anywhere_on/,DebajyotiS,1562572867,"I am following a tutorial from the link : [https://www.tensorflow.org/beta/tutorials/generative/dcgan](https://www.tensorflow.org/beta/tutorials/generative/dcgan)

I have read in many articles that the GAN training consists of steps where the discriminator training is halted and metrics are then calculated for the entire model (GEN + DISC). I cannot follow how this code achieves this. Can someone help?",0,1
443,2019-7-8,2019,7,8,18,caio46,Paper with test results on when it makes sense to use transfer learning (insufficient data),https://www.reddit.com/r/MachineLearning/comments/caio46/paper_with_test_results_on_when_it_makes_sense_to/,pppeer,1562576711,[removed],0,1
444,2019-7-8,2019,7,8,18,cairyl,How to convert speech to text??,https://www.reddit.com/r/MachineLearning/comments/cairyl/how_to_convert_speech_to_text/,div1919,1562577513,"I'm planning to do a project. Initial Step is to convert speech to text. I know that first speech should be converted to spectrogram. But i have no idea how to do that. Can anyone help me out to solve this! I need this in python
Thanks in advance..",0,1
445,2019-7-8,2019,7,8,18,caix1f,Future Scope of Underwater Robotics Market is Projected to Reach Significantly by USD 450 Million till 2022,https://www.reddit.com/r/MachineLearning/comments/caix1f/future_scope_of_underwater_robotics_market_is/,jadhavni3,1562578574,[removed],1,1
446,2019-7-8,2019,7,8,18,caj041,Global Underwater Robotics Market Report 2018,https://www.reddit.com/r/MachineLearning/comments/caj041/global_underwater_robotics_market_report_2018/,jadhavni3,1562579186,[removed],1,1
447,2019-7-8,2019,7,8,18,caj2xn,Any online courses I could take for tensor flow 2.0 ?,https://www.reddit.com/r/MachineLearning/comments/caj2xn/any_online_courses_i_could_take_for_tensor_flow_20/,MphoMotionless,1562579822,[removed],0,1
448,2019-7-8,2019,7,8,19,cajjuh,Looking for specific episodes of The Bold and The Beautiful?,https://www.reddit.com/r/MachineLearning/comments/cajjuh/looking_for_specific_episodes_of_the_bold_and_the/,longhowlam,1562583219,[removed],0,1
449,2019-7-8,2019,7,8,20,cajn9t,CronJ's Operations Optimization and Quality Assurance solution converts the idea of 'Smart Factory' into reality just using a camera and our proprietary Algorithm,https://www.reddit.com/r/MachineLearning/comments/cajn9t/cronjs_operations_optimization_and_quality/,cronj,1562583892,[removed],0,1
450,2019-7-8,2019,7,8,20,cajpgh,Looking for specific episodes of the Bold and the Beautiful?,https://www.reddit.com/r/MachineLearning/comments/cajpgh/looking_for_specific_episodes_of_the_bold_and_the/,longhowlam,1562584322,"There is a nice python package ""holmes-extractor"" that builds upon the spacy package. It supports a couple of use cases.  

* Chatbot
* Structural matching 
* Topic matching 
* Supervised document classification 

I have used 4000 recaps of The Bold and the beautiful of the last 16 years to test Topic matching. I am using it to find texts in the recaps whose meaning is close to that of another query document or a query phrase entered by the user.  

Just one example: Looking for ""*she wanted to be close to him*"" gave me the recap of 2013-12-09: *""It had only just begun, and he wanted to be closer to her. He had wanted to touch her, grab her, and he knew she wanted the same thing*.""

The code of holmes is concise, runs out of the box with little tweaking and tuning. Try it out yourself and find your favorite TBATB episode. My [example notebook](https://github.com/longhowlam/python_hobby_stuff) and recaps.",0,1
451,2019-7-8,2019,7,8,20,cajpu0,Is Coffee a Fruit?,https://www.reddit.com/r/MachineLearning/comments/cajpu0/is_coffee_a_fruit/,Brainjuice12,1562584395,[removed],0,1
452,2019-7-8,2019,7,8,20,cajx1i,High Pressure Water Jet Cleaner www.hydrojet.co.in,https://www.reddit.com/r/MachineLearning/comments/cajx1i/high_pressure_water_jet_cleaner_wwwhydrojetcoin/,Ultramaxhydrojet,1562585760,,0,1
453,2019-7-8,2019,7,8,20,cajzwg,Machine Learning for Art - Deep Kitsch or Creative Augmentation?,https://www.reddit.com/r/MachineLearning/comments/cajzwg/machine_learning_for_art_deep_kitsch_or_creative/,hoopism,1562586286,,0,2
454,2019-7-8,2019,7,8,20,cak0wp,Fooling a Real Car with Adversarial Traffic Signs,https://www.reddit.com/r/MachineLearning/comments/cak0wp/fooling_a_real_car_with_adversarial_traffic_signs/,nirmorgo,1562586477,[https://arxiv.org/abs/1907.00374](https://arxiv.org/abs/1907.00374),0,1
455,2019-7-8,2019,7,8,20,cak3ip,[P] Seeking feedback on Evolution: evolve a neural network from scratch,https://www.reddit.com/r/MachineLearning/comments/cak3ip/p_seeking_feedback_on_evolution_evolve_a_neural/,NoSegfaultPlz,1562586962,"Project link: [https://github.com/zli117/Evolution](https://github.com/zli117/Evolution)

Hi there, I made a CNN evolution framework for Keras. It encodes network architectures in hierarchical graphs, inspired by this [paper](https://arxiv.org/abs/1711.00436). It also supports training multiple sessions in parallel on the same or multiple GPUs for speed. [Here](https://github.com/zli117/Evolution/blob/master/examples/cifar10.py) is an example usage on cifar10.",7,5
456,2019-7-8,2019,7,8,21,cak6u3,[R] Fooling a Real Car with Adversarial Traffic Signs,https://www.reddit.com/r/MachineLearning/comments/cak6u3/r_fooling_a_real_car_with_adversarial_traffic/,nirmorgo,1562587557,[https://arxiv.org/abs/1907.00374](https://arxiv.org/abs/1907.00374),0,2
457,2019-7-8,2019,7,8,21,cake4j,I have a new collaborator on my quest to programmatically predict the next number in a sequence... any sequence.,https://www.reddit.com/r/MachineLearning/comments/cake4j/i_have_a_new_collaborator_on_my_quest_to/,Agent_ANAKIN,1562588818,,0,1
458,2019-7-8,2019,7,8,21,cakfg0,[P] Python library to work with the Visual Wake Words Dataset.,https://www.reddit.com/r/MachineLearning/comments/cakfg0/p_python_library_to_work_with_the_visual_wake/,Mxbonn,1562589040,"Recently Google published a paper introducing the [Visual Wake Words Dataset](https://arxiv.org/abs/1906.05721).

&gt;Currently vision models are benchmarked on the CIFAR10 or ImageNet  datasets both of which are restricted in terms of benchmarking the model accuracy and the memory costs for the common low-complexity microcontroller use-case. We present a new dataset, Visual Wake Words, that represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, The proposed dataset is derived from the publicly available COCO dataset, and provides a realistic benchmark for tiny vision models.

As the dataset is derived from the COCO dataset I created a library that inherits from the pycocotools libary and that can be used in a similar fashion on the Visual Wake Words Dataset.

I've also included a Pytorch Dataset class that can be used like any VisionDataset.

[https://github.com/Mxbonn/visualwakewords](https://github.com/Mxbonn/visualwakewords) 

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/xxeoua9xp2931.png

https://i.redd.it/3lyaebaxp2931.png",0,5
459,2019-7-8,2019,7,8,21,cakg5p,Want to start learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cakg5p/want_to_start_learning_machine_learning/,akk2087,1562589165,"Hello Everyone,

&amp;#x200B;

I was wondering if anyone could suggest a series of books to get into machine learning? As in start from the basics of statistical machine learning and move on to more advanced concepts. Also, wondering if there are any good online courses on this topic (Was looking at Coursera, but want to get a second opinion)?

&amp;#x200B;

I have a relatively good background in statistical estimation and detection theory and have some coding experience with python.",2,1
460,2019-7-8,2019,7,8,21,cakgr3,[P] Predicting Academic Collaboration with Logistic Regression,https://www.reddit.com/r/MachineLearning/comments/cakgr3/p_predicting_academic_collaboration_with_logistic/,tomkoker,1562589260,"[Predicting Academic Collaboration with Logistic Regression](https://teddykoker.com/2019/07/predicting-academic-collaboration-with-logistic-regression/)

My capstone project for my last year of undergrad has to do with Social Network Analysis of networks formed by co-authorship in research communities. Since I have a growing interest for machine learning I wanted to see if I could apply various machine learning methods to the networks in order to gather more insight into how they work. In this post I applied a logistic regression model in order to predict future collaboration, and it seemed to be quite accurate over the test set! I would love any feedback on the post, and I hope you enjoy!",1,10
461,2019-7-8,2019,7,8,21,cakm3h,How to represent a polygon uniquely using fixed number of parameters?,https://www.reddit.com/r/MachineLearning/comments/cakm3h/how_to_represent_a_polygon_uniquely_using_fixed/,HDidwania,1562590151,"I am working on the problem of generation of plausible looking layout regions. Right now we are working by generating the region areas as rectangles, but want to extend it to polygons. Working on rectangles is easy since it can be uniquely represented using a fixed number of parameters (x and y coords of top left corner, width and height). Is there any way to represent general polygons using some fixed number of parameters?",2,1
462,2019-7-8,2019,7,8,22,cakvkl,Looking for a small team of developers.,https://www.reddit.com/r/MachineLearning/comments/cakvkl/looking_for_a_small_team_of_developers/,dajkatal,1562591658,"&amp;#x200B;

https://i.redd.it/yap5t3cox2931.png

I am currently working on a youtube web application geared towards helping users gain more viewers. The way it works is by correctly selecting keywords that match well with the users channel data such as engagement rate, watched hours and clickrate/ clickthrough. For example, a user with 1000 subs will not benefit from using the keyword fortnite, while a larger channel might. Through the use of AI, I would like to create an app that looks over the users channel data and then organize the generated keywords from best to worst.

&amp;#x200B;

&gt;I have the foundations of the project but I have spent 60+ hours on it and I still have lots more to do. If anyone would like more information or would like to join, please message me.

&amp;#x200B;

* I am looking for people with experience in html, css, django or AI and deep learning.

&amp;#x200B;

(For anyone wondering, this app will have a premium version and people who help out will get a share from the monthly earnings.)",3,0
463,2019-7-8,2019,7,8,22,cal470,Experte bei Motorinstandsetzung und Motorreparatur,https://www.reddit.com/r/MachineLearning/comments/cal470/experte_bei_motorinstandsetzung_und_motorreparatur/,harrysmith119,1562592985,"&amp;#x200B;

 Wnschen Sie eine Motorinstandsetzung oder eine Motorreparatur zum Festpreis mit hoher Qualitt? Dann sind Sie 

bei MotorCheck24 genau richtig. Wir bieten Ihnen den besten Service mit deutschlandweit ausgebautem 

Experten-Partnernetzwerk mit hohen Qualittsansprchen.",0,0
464,2019-7-8,2019,7,8,23,calhuu,[P] Why is there a sudden drop in accuracy after a few epochs in a vgg-like network?,https://www.reddit.com/r/MachineLearning/comments/calhuu/p_why_is_there_a_sudden_drop_in_accuracy_after_a/,melonochelo,1562594944,,0,1
465,2019-7-8,2019,7,8,23,caljga,[D] What would constitute as high quality research and technical work in ML (production)?,https://www.reddit.com/r/MachineLearning/comments/caljga/d_what_would_constitute_as_high_quality_research/,Naveos,1562595175,"Say you are evaluating a researcher or an engineer in a company, working with AI for production purposes; what would you constitute as high quality research or technical work output from them? How can you tell a good AI practitioner from a bad one? How about a great one from good ones?",0,2
466,2019-7-8,2019,7,8,23,cals4m,[D] Copy of Lecture videos of Advanced Machine Learning from ETH Zurich Autumn 2018,https://www.reddit.com/r/MachineLearning/comments/cals4m/d_copy_of_lecture_videos_of_advanced_machine/,swordsaintx102,1562596378,"The lecture videos from Advanced Machine Learning from ETH Zurich Autumn 2018 were previously available in the youtube link below, but they removed it. I was wondering if someone managed to save a copy and would be willing to share it? =)

&amp;#x200B;

[https://www.youtube.com/playlist?list=PLzn6LN6WhlN1x68-5GEzAflTBQrSoKcAJ](https://www.youtube.com/playlist?list=PLzn6LN6WhlN1x68-5GEzAflTBQrSoKcAJ)",14,11
467,2019-7-8,2019,7,8,23,calt5w,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/calt5w/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1562596518,[removed],0,1
468,2019-7-8,2019,7,8,23,calv0z,[P] I used machine learning to understand the distribution of relationship problems from r/relationships,https://www.reddit.com/r/MachineLearning/comments/calv0z/p_i_used_machine_learning_to_understand_the/,thethetaofpeople,1562596769,,0,1
469,2019-7-8,2019,7,8,23,calwmo,"tutorial Friday freestyle the single eagle, double eagle, sun wave and m...",https://www.reddit.com/r/MachineLearning/comments/calwmo/tutorial_friday_freestyle_the_single_eagle_double/,thetrickshotone,1562596981,,0,1
470,2019-7-9,2019,7,9,0,camg6q,How to implement a trained reinforcement learning model on a live trading account?,https://www.reddit.com/r/MachineLearning/comments/camg6q/how_to_implement_a_trained_reinforcement_learning/,sillehsoj,1562599579,[removed],0,1
471,2019-7-9,2019,7,9,0,camnoh,What does this loss mean again?,https://www.reddit.com/r/MachineLearning/comments/camnoh/what_does_this_loss_mean_again/,Jandevries101,1562600570,[removed],0,1
472,2019-7-9,2019,7,9,0,cams72,[P] AI Music Video Stylizer,https://www.reddit.com/r/MachineLearning/comments/cams72/p_ai_music_video_stylizer/,dillonlaird,1562601164,,0,1
473,2019-7-9,2019,7,9,0,camta7,DeepSpeech training in anaconda environment,https://www.reddit.com/r/MachineLearning/comments/camta7/deepspeech_training_in_anaconda_environment/,antaresx7,1562601305,[removed],0,1
474,2019-7-9,2019,7,9,0,camun7,Recommender System for Company's Case Management System,https://www.reddit.com/r/MachineLearning/comments/camun7/recommender_system_for_companys_case_management/,Jaubin9422,1562601484,"Hi /r/MachineLearning!  


I have some quick questions concerning a project I'm researching &amp; working on at work.   


For some background, the company I work at uses a proprietary case management system. This system revolves around cases, which are entities which can be created by and assigned to various users in order to issue and track work orders (users can update case status, add notes, and close the case once the task it refers to is completed, and so on).

&amp;#x200B;

I would like to design a recommender system which presents similar cases to a user once they have viewed or closed a case. So far, my research has led me towards *user-based collaborative filtering* as the most relevant approach. 

&amp;#x200B;

The main hurdle I've hit so far (at a conceptual level) is that most of these approaches revolve around *explicit feedback* from users (in the form of ratings, etc.) which I do not have for cases. I do have access to an activity log which records each user action on any given case (viewing the case, closing it, adding notes, etc.). 

From what I understand, *implicit feedback* models are more difficult to implement due to the featurization of different user behaviors. 

&amp;#x200B;

I have looked into several libraries for recommender systems (Mahout, etc.) but am not sure thus far if any would prove useful for this scenario.   


Apologies for the rambling, but does anyone have any recommendations about how to best approach this scenario?",0,1
475,2019-7-9,2019,7,9,1,camvob,Landmark Assisted CycleGAN: Draw Me Like One of Your Cartoon Girls,https://www.reddit.com/r/MachineLearning/comments/camvob/landmark_assisted_cyclegan_draw_me_like_one_of/,Yuqing7,1562601619,,0,1
476,2019-7-9,2019,7,9,1,camxyf,[D] Best practices for Multi-task Learning where labels exist for only a subset of the total tasks (partial coverage),https://www.reddit.com/r/MachineLearning/comments/camxyf/d_best_practices_for_multitask_learning_where/,Toast119,1562601907,"Say, for example, I have a goal to do a multi-task learning workflow where I have a network that predicts both foreground/background segmentation, specific object segmentation, and bounding box regression. 

Suppose that there are only labels for 1-2 of the tasks per image (i.e. one input image has labels for bounding boxes and fg/bg segmentation, but no specific object segmentation; one input image has specific objects and bounding boxes, but no fg/bg). Also, that there isn't a lot of training data to use, so I would like to utilize all of the training data even without full label coverage, especially because the feature extraction portions of the network probably benefit from mutual information.

Is there an area of research or some best practices to train an entire network end-to-end in this regard? Something involving finetuning/turning training 'off' for certain layers of the network that don't have labels for that input image?",6,3
477,2019-7-9,2019,7,9,1,canaw3,Adam Parameters for SWA implementation,https://www.reddit.com/r/MachineLearning/comments/canaw3/adam_parameters_for_swa_implementation/,mohit_tripathi,1562603560,,1,1
478,2019-7-9,2019,7,9,1,cangm4,"ADAM parameters for SWA implementation given in the paper(paper link I have given). I want to get the reproduce the result using ADAM optimizer but was able to achieve only 72.1% accuracy instead of 82.24%(achieved this via SGD momentum). LR used: 0.001. swa_lr used: 0.0005, swa_start: 111.",https://www.reddit.com/r/MachineLearning/comments/cangm4/adam_parameters_for_swa_implementation_given_in/,mohit_tripathi,1562604312,,1,1
479,2019-7-9,2019,7,9,2,canrt9,"ADAM parameters for SWA implementation given in the paper(paper link I have given). I want to get the reproduce the result using ADAM optimizer but was able to achieve only 72.1% accuracy instead of 82.24%(achieved this via SGD momentum). LR used: 0.001. swa_lr used: 0.0005, swa_start: 111.",https://www.reddit.com/r/MachineLearning/comments/canrt9/adam_parameters_for_swa_implementation_given_in/,mohit_tripathi,1562605727,[removed],0,1
480,2019-7-9,2019,7,9,2,canta7,Have some cloud credits to burn. 50% off,https://www.reddit.com/r/MachineLearning/comments/canta7/have_some_cloud_credits_to_burn_50_off/,isharaux,1562605904,[removed],0,1
481,2019-7-9,2019,7,9,2,cao3zj,"[P] Torchbearer version 0.4.0 released, now with regularisers (Mixup, CutOut, CutMix, etc.), PyCM, LiveLossPlot, a set of Colab examples and much more",https://www.reddit.com/r/MachineLearning/comments/cao3zj/p_torchbearer_version_040_released_now_with/,ethanwharris,1562607241,"**Now in the latest version of torchbearer:**

* A host of regularisers including: Mixup, CutOut, CutMix, Sample Pairing, Label Smoothing and Random Erase
* Built-in integration with PyCM for generating confusion matrices
* Built-in support for LiveLossPlot
* A set of example colab notebooks
* Bug fixes and more listed in the CHANGELOG: [https://github.com/pytorchbearer/torchbearer/blob/master/CHANGELOG.md](https://github.com/pytorchbearer/torchbearer/blob/master/CHANGELOG.md)

&amp;#x200B;

Install: [http://www.pytorchbearer.org/#install](http://www.pytorchbearer.org/#install)

Check out the examples at: [http://www.pytorchbearer.org/#examples](http://www.pytorchbearer.org/#examples)

Find us on GitHub: [https://github.com/pytorchbearer/torchbearer](https://github.com/pytorchbearer/torchbearer)

&amp;#x200B;

Please take a look and let us know what you think!",6,7
482,2019-7-9,2019,7,9,2,cao9ob,[N] Google patent for Batch Normalization [Assigned on 2017-10-05],https://www.reddit.com/r/MachineLearning/comments/cao9ob/n_google_patent_for_batch_normalization_assigned/,JacksTurmoil,1562607933,"Abstract 

  Methods,  systems, and apparatus, including computer programs encoded on computer  storage media, for processing inputs using a neural network system that  includes a batch normalization layer. One of the methods includes  receiving a respective first layer output for each training example in  the batch; computing a plurality of normalization statistics for the  batch from the first layer outputs; normalizing each component of each  first layer output using the normalization statistics to generate a  respective normalized layer output for each training example in the  batch; generating a respective batch normalization layer output for each  of the training examples from the normalized layer outputs; and  providing the batch normalization layer output as an input to the second  neural network layer. 

&amp;#x200B;

Link: [https://patents.google.com/patent/US20160217368A1/en](https://patents.google.com/patent/US20160217368A1/en?authuser=0)",115,162
483,2019-7-9,2019,7,9,3,caogy1,How should I make my ML algorithms accessible to the client after I leave?,https://www.reddit.com/r/MachineLearning/comments/caogy1/how_should_i_make_my_ml_algorithms_accessible_to/,UnrequitedReason,1562608838,[removed],0,1
484,2019-7-9,2019,7,9,3,caom0d,[Discussion] Do you think the ML community is sufficiently critical?,https://www.reddit.com/r/MachineLearning/comments/caom0d/discussion_do_you_think_the_ml_community_is/,ShandarTheDestroyer,1562609426,"I recently read a new NLP paper called CNM: An Interpretable Complex-valued Network for Matching ([https://arxiv.org/abs/1904.05298](https://arxiv.org/abs/1904.05298)). In summary, the paper proposes a framework for NLP explainability using complex-valued vector spaces, borrowing math frameworks from quantum mechanics. It even won an award at this years NAACL. Reading the paper however, I couldn't help but to feel a bit mislead. There seemed to be a stretching of the applications of quantum mechanics and there were a lot of ideas proposed in the paper which had little evidence to back them up. I was so bothered by it in fact that I wrote a [blog post](https://darshancrout.ai/post/big-trouble-in-little-quanta-a-critique-of-complex-valued-networks-for-nlp-explainability/) about it. There's also a great paper I read that talks more about the state of research in the field: [https://arxiv.org/abs/1807.03341](https://arxiv.org/abs/1807.03341).

&amp;#x200B;

I wanted to know people's thoughts on the matter. Do you think we're critical enough as a community? Do you feel because of the recent successes of machine learning/deep learning research, people maybe are hesitant to speak out or to ask more questions on why things work?",28,41
485,2019-7-9,2019,7,9,3,caotyy,[R] - Speedup your CNN using Fast Dense Feature Extraction and PyTorch,https://www.reddit.com/r/MachineLearning/comments/caotyy/r_speedup_your_cnn_using_fast_dense_feature/,posnererez,1562610380,,0,1
486,2019-7-9,2019,7,9,3,caovkb,Degenerative Adversarial NeuroImage Nets: Generating Images that Mimic Disease Progression,https://www.reddit.com/r/MachineLearning/comments/caovkb/degenerative_adversarial_neuroimage_nets/,daniravi,1562610572,"Do you that AI is also able to simulate realistic T1w-MRI to predict neurodegeneration with the potential to discover Alzheimer's disease at an early stage? Check this out in our recent paper [~~#~~**DaniNet**](https://twitter.com/hashtag/DaniNet?src=hash) that just got accepted in [~~#~~**miccai2019**](https://twitter.com/hashtag/miccai2019?src=hash)! Stay tuned to access our code! [https://arxiv.org/abs/1907.02787](https://t.co/do5LeGCZF8)

![video](4h3jiiskh4931)",0,1
487,2019-7-9,2019,7,9,3,caoxq4,[D] Tutorial: Machine learning in production environments,https://www.reddit.com/r/MachineLearning/comments/caoxq4/d_tutorial_machine_learning_in_production/,ixeption,1562610841,,0,1
488,2019-7-9,2019,7,9,4,caphhp,[P] Pretrained pytorch resnet models for anime tag estimation,https://www.reddit.com/r/MachineLearning/comments/caphhp/p_pretrained_pytorch_resnet_models_for_anime_tag/,m_baas,1562613252,"Hi

I trained some Resnet models to estimate tags for anime images. 

There has been a lot of cool anime-related projects recently, such as [DeepDanbooru](https://www.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/) and some other cool work with [anime face generation](https://www.gwern.net/Faces), however most use tensorflow and so I wanted a nice pretrained pytorch model to use for transfer learning with downstream tasks. 

**A TL;DR of the networks:**

* Framework: Pytorch
* Network: adapted resnet50, resnet34, and resnet18 models
* Training time : about 12 days (using mixed-precision training and Nvidia V100)
* Dataset: [Danbooru2018 dataset](https://gwern.net/Danbooru2018), with networks trained to predict top tags of each image

[An example output of the resnet50 model](https://github.com/RF5/danbooru-pretrained/blob/master/img/danbooru_resnet2.png)

If you have pytorch, you can load up the model and predict on new images in only a few lines using pytorch hub. [See this post for a quick getting started](https://rf5.github.io/2019/07/08/danbuuro-pretrained.html)

I hope some people find this useful, and that it saves some people training time with other tasks.

For more info on the networks, training, data preparation, and performance metrics of each network, please have a look at the [project page](https://rf5.github.io/2019/07/08/danbuuro-pretrained.html), or just ask :). Any comments or feedback is nice.",7,10
489,2019-7-9,2019,7,9,4,capi99,[R] Speedup your CNN using Fast Dense Feature Extraction and PyTorch,https://www.reddit.com/r/MachineLearning/comments/capi99/r_speedup_your_cnn_using_fast_dense_feature/,posnererez,1562613351,,0,1
490,2019-7-9,2019,7,9,4,capnru,Fourier transformed intermediate convolutional layers,https://www.reddit.com/r/MachineLearning/comments/capnru/fourier_transformed_intermediate_convolutional/,BoltzmanBra1n,1562614035,,0,1
491,2019-7-9,2019,7,9,4,capofy,Machine Learning explained in 12 minutes,https://www.reddit.com/r/MachineLearning/comments/capofy/machine_learning_explained_in_12_minutes/,olarayej,1562614125,[removed],0,1
492,2019-7-9,2019,7,9,4,capubv,"66.96s, 162.8kph - RECORD BREAKING Autonomous Run at Goodwood FOS!",https://www.reddit.com/r/MachineLearning/comments/capubv/6696s_1628kph_record_breaking_autonomous_run_at/,AIthatDrives,1562614864,,0,1
493,2019-7-9,2019,7,9,4,capxbo,Fourier transformed intermediate convolutional layers,https://www.reddit.com/r/MachineLearning/comments/capxbo/fourier_transformed_intermediate_convolutional/,elizabeth361,1562615253,,0,1
494,2019-7-9,2019,7,9,4,caq2b5,ML STUDY GROUP,https://www.reddit.com/r/MachineLearning/comments/caq2b5/ml_study_group/,shageent-buffaloedu,1562615896,"Anybody want to do Andrew NG's course with me, we can watch the videos together and discuss the material through skype, hangouts. Also work on Programming assignments. 

I am currently up to week 3, and would like to ideally do it every night at 8PM EST",0,1
495,2019-7-9,2019,7,9,5,caq369,Is there a way to prove that there is no cluster to find in a dataset?,https://www.reddit.com/r/MachineLearning/comments/caq369/is_there_a_way_to_prove_that_there_is_no_cluster/,elpiro,1562616007,[removed],0,1
496,2019-7-9,2019,7,9,5,caq8r9,[D] Recurrent networks without unrolling/data duplication,https://www.reddit.com/r/MachineLearning/comments/caq8r9/d_recurrent_networks_without_unrollingdata/,warp_driver,1562616697,"So, today my company decided to send me and a bunch of people onto one of those machine learning courses that are trendy these days. The subject was on time series prediction, and the structure followed everything I've seen in the past few years and that always leaves me baffled.

For every lag they want to use they create a new column with shifted data! To me this seems to be a dealbreaker for 2 reasons. First it effectively multiplies the memory usage by the length of the history we wish to consider. Second it means that we can't keep history longer than the lags considered, as the internal state doesn't transfer between lines of the data matrix (I think I remember keras having something to keep this state but it required lining examples between batches). I don't see how this can possibly work when you have high resolution data and care about microstructure but also need to take into account longer term dynamics.

So, what am I missing here? When I'm working with things like exponentially weighted moving averages I can trivially create a statsmodels model to fit the decay rate and find the optimal history length. Is there really no straightforward way to use tensorflow/pytorch to look back into values earlier in the same data column and skip the whole data duplication issue? If not, is there a good reason for it?",7,2
497,2019-7-9,2019,7,9,5,caqbep,[D] Is there a way to prove that there is no cluster in a population?,https://www.reddit.com/r/MachineLearning/comments/caqbep/d_is_there_a_way_to_prove_that_there_is_no/,elpiro,1562617034,"For example, I am looking for cluster in a dataset of 10000 binary variables. I reduce the number to 50 variables with a PCA, then apply t-sne on it to find clusters in the output.

There are separated shapes in the visualisation of tsne, but I understood that points that are far apart in the output are not always far in the higher dimension space. T-sne can find clusters in a normally distributed dataset. 

Can we to prove that data follows a normal distribution in all directions of space? 
Is there a way to remove variables that would add noise to clustering? Like some kind of variable selection but for clustering?",13,6
498,2019-7-9,2019,7,9,5,caqnws,Best Way To Capture Frequency-Spatial-Temoral Nature Of Data,https://www.reddit.com/r/MachineLearning/comments/caqnws/best_way_to_capture_frequencyspatialtemoral/,carsonpoole,1562618656,[removed],0,1
499,2019-7-9,2019,7,9,5,caqo9h,Hoe to step into real world Machine Learning Problems? is Kaggle good enough!,https://www.reddit.com/r/MachineLearning/comments/caqo9h/hoe_to_step_into_real_world_machine_learning/,roopesh_Ryuk,1562618704,[removed],0,1
500,2019-7-9,2019,7,9,6,carflf,Instagram is now tackling cyberbullying with the help of AI,https://www.reddit.com/r/MachineLearning/comments/carflf/instagram_is_now_tackling_cyberbullying_with_the/,azmodeus99,1562622270,,0,1
501,2019-7-9,2019,7,9,6,carm38,[N] spaCy IRL NLP Conference Overview,https://www.reddit.com/r/MachineLearning/comments/carm38/n_spacy_irl_nlp_conference_overview/,DemiourgosD,1562623146,,0,1
502,2019-7-9,2019,7,9,7,carmgk,Using Doc2Vec to classify movie reviews (Keras tutorial),https://www.reddit.com/r/MachineLearning/comments/carmgk/using_doc2vec_to_classify_movie_reviews_keras/,jdyr1729,1562623202,,0,1
503,2019-7-9,2019,7,9,7,carpdx,A Break News in Adversarial Robustness (?),https://www.reddit.com/r/MachineLearning/comments/carpdx/a_break_news_in_adversarial_robustness/,zdhNarsil,1562623554,[removed],0,1
504,2019-7-9,2019,7,9,7,carxgy,Using Doc2Vec to classify movie reviews (Keras tutorial),https://www.reddit.com/r/MachineLearning/comments/carxgy/using_doc2vec_to_classify_movie_reviews_keras/,jdyr1729,1562624636,[removed],0,1
505,2019-7-9,2019,7,9,7,cas9gi,Training termites?,https://www.reddit.com/r/MachineLearning/comments/cas9gi/training_termites/,joey_bryson,1562626313,[removed],0,1
506,2019-7-9,2019,7,9,8,casps7,[D] Help with DeepSpeech? please :V,https://www.reddit.com/r/MachineLearning/comments/casps7/d_help_with_deepspeech_please_v/,antaresx7,1562628604," 

Hey guys, I was hoping for someone to help me, been stuck with this problem for a while now.

I have been trying to train DeepSpeech on a Spanish CommonVoice dataset.

However the script barely uses my gpu, if at all.

I seem to have everything installed by now except for the correct version of the CudNN.

I am using an anaconda environment and I installed everything easily, however anaconda doesn't have the version 7.5.0 in its repository so I cannot install it through ""conda install"". I have been searching how to install cudnn into the environment in another way but I haven't been able to find anything.

Any help would be very much appreciated!!

I am using Ubuntu 18.04 LTS",6,0
507,2019-7-9,2019,7,9,8,casx8k,[D] Do you think ML will become powerful enough to decode diffused light reflection?,https://www.reddit.com/r/MachineLearning/comments/casx8k/d_do_you_think_ml_will_become_powerful_enough_to/,FreckledMil,1562629687,"It seems there is some variability to diffusion appearance depending on the material/distance/light intensity/object being reflected. For instance if you hold your hand up to a white piece of paper with a bulb behind your hand, you can see your hands shape a few centimeters off the paper. Move your hand back and eventually it just becomes a large blurry dark spot exponentially fading. 

I was thinking earlier about this, and we as humans really are only trained to see these pronounced diffused reflections, as anything else is cognitively expensive and probably wasteful. So once the hand no longer represents a hand to us on the white paper, it starts losing its reason to be seen more and more. 

Do you think it is possible now, using gans/cnns, to train a model, to learn to see more than we can? Will we now, or ever, be able to decode the diffused reflections of whole/partial objects from vast distances using ML?",10,2
508,2019-7-9,2019,7,9,9,catjw7,"Researcher tricks m3.euagendas.org, the Twitter analysis website, with adversarial inputs",https://www.reddit.com/r/MachineLearning/comments/catjw7/researcher_tricks_m3euagendasorg_the_twitter/,atomlib_com,1562633029,,0,1
509,2019-7-9,2019,7,9,10,catq9t,[P] TensorFlow to Embedded AI Chip,https://www.reddit.com/r/MachineLearning/comments/catq9t/p_tensorflow_to_embedded_ai_chip/,zsdh123,1562634020,,0,1
510,2019-7-9,2019,7,9,10,cau2dr,[Discussion]How to ensure the quality of labeling data?,https://www.reddit.com/r/MachineLearning/comments/cau2dr/discussionhow_to_ensure_the_quality_of_labeling/,ClassifyOrRegreddit,1562635864,"Hi all the MLers, 

Usually, we could have labeled data with specific degree of quality from Kaggle or from some workshops. However, if we are in real world usage, how could we ensure the **quality of the labeled data**?

To be more specific, I am doing a project on Named Entity Recognition. We now have a **rule-based** model to extract the NE by Regular Expression, and we now would like to do the task in a **Machine Learning** workflow. So, here's the question, if we directly take the data labeled by our rule-based model as the training data for our ML model, how could we first ensure the quality of this training data before we feed it into the model.

&amp;#x200B;

This is my first time to have to generate training data from scratch, so I really appreciate any discussion with you guys. Any ideas and comments are welcome! Thanks ALOT!",11,3
511,2019-7-9,2019,7,9,11,caufgu,This video goes over a breast cancer diagnosis model that uses neural networks (implemented in python),https://www.reddit.com/r/MachineLearning/comments/caufgu/this_video_goes_over_a_breast_cancer_diagnosis/,antaloaalonso,1562637790,,0,1
512,2019-7-9,2019,7,9,11,cauti3,Should I give up data science to become a software developer,https://www.reddit.com/r/MachineLearning/comments/cauti3/should_i_give_up_data_science_to_become_a/,abhi_learner,1562639939,[removed],0,1
513,2019-7-9,2019,7,9,11,cauv1u,"[D] What you need to know about Disinformation | Rachel Thomas, Fast.ai",https://www.reddit.com/r/MachineLearning/comments/cauv1u/d_what_you_need_to_know_about_disinformation/,sinshallah,1562640179,"[YouTube lecture here](https://www.youtube.com/watch?v=vbva2RN-rbQ)

As part of Fast.ai's NLP course, Rachel Thomas has prepared an excellent overview of the existing cases of disinformation.
She highlights several notable examples *from this past year alone*, and covers the potential misuses and abuses of AI and NLP techniques. These include prominent generative methods such as GPT-2 to make fake-but-plausible reddit comments, and StyleGAN portraits to fake social media profile images.",3,93
514,2019-7-9,2019,7,9,13,cavpao,Historical Data on Alt-coins,https://www.reddit.com/r/MachineLearning/comments/cavpao/historical_data_on_altcoins/,AlexOakwood,1562644995,[removed],0,1
515,2019-7-9,2019,7,9,13,cavwk1,Why don't people draw the whole learning curves? (Reinforcement Learning),https://www.reddit.com/r/MachineLearning/comments/cavwk1/why_dont_people_draw_the_whole_learning_curves/,Kristery,1562646215,[removed],0,1
516,2019-7-9,2019,7,9,13,caw0un,Machine Learning Training Institute In Salem | Top Livewire Course In 2019,https://www.reddit.com/r/MachineLearning/comments/caw0un/machine_learning_training_institute_in_salem_top/,livewireindia,1562646943,,0,1
517,2019-7-9,2019,7,9,14,cawclu,[D] Should I study statistical learning if I am learning machine learning already?,https://www.reddit.com/r/MachineLearning/comments/cawclu/d_should_i_study_statistical_learning_if_i_am/,DoIHAVeaNIdenTItY,1562648969,I started to learn machine learning from Andrew Ng's course. I knew the course doesn't teach the math behind comprehensively. I started to study statistical learning at the same time. At the beginning it helped me to understand the concepts and math behind the ideas we are introduced. But now it's like two of them approaching to same problem from different sides. Should I continue to study statistical learning in this case? Would you have any recommendations? which resource should I use to learn the math behind?,3,0
518,2019-7-9,2019,7,9,14,cawkho,Using Doc2Vec to classify movie reviews (Keras tutorial),https://www.reddit.com/r/MachineLearning/comments/cawkho/using_doc2vec_to_classify_movie_reviews_keras/,jdyr1729,1562650370,[removed],0,1
519,2019-7-9,2019,7,9,14,cawosi,[R] Fourier transformed intermediate convolutional layers,https://www.reddit.com/r/MachineLearning/comments/cawosi/r_fourier_transformed_intermediate_convolutional/,BoltzmanBra1n,1562651171,,0,1
520,2019-7-9,2019,7,9,16,caxl1u,[Research] Our source code for deep video inpainting!,https://www.reddit.com/r/MachineLearning/comments/caxl1u/research_our_source_code_for_deep_video_inpainting/,amjltc295,1562657243,"We are proud to share our source code for ""Learnable Gated Temporal Shift Module for Deep Video Inpainting. Chang et al. BMVC 2019."" We developed a simple module to reduce training &amp; testing time and model parameters for deep free-form video inpainting based on the Temporal Shift Module for action recognition. It achieves similarly good results as our previous work ""Free-form Video Inpainting with 3D Gated Convolution and Temporal PatchGAN. Chang et al. arXiv 2019."" with only 33% of parameters.


GitHub: https://github.com/amjltc295/Free-Form-Video-Inpainting 

Arxiv: https://arxiv.org/abs/1907.01131


Contributions and stars are welcome!",6,66
521,2019-7-9,2019,7,9,16,caxph4,Using Doc2Vec to classify movie reviews,https://www.reddit.com/r/MachineLearning/comments/caxph4/using_doc2vec_to_classify_movie_reviews/,kal138,1562658137,,0,1
522,2019-7-9,2019,7,9,16,caxsqd,Fresh stuff (NLP) from fast.ai,https://www.reddit.com/r/MachineLearning/comments/caxsqd/fresh_stuff_nlp_from_fastai/,IRonyk,1562658799,,0,1
523,2019-7-9,2019,7,9,16,caxt5b,"Hadoop, MapReduce for Big Data problems : Learn By Example",https://www.reddit.com/r/MachineLearning/comments/caxt5b/hadoop_mapreduce_for_big_data_problems_learn_by/,HannahHumphreys,1562658883,[removed],0,1
524,2019-7-9,2019,7,9,17,caxviq,"Woodworking Machines Manufacturers, Wood Working Machinery, Industrial Woodworking Machinery, Wooden Furniture Making Machines, Wood Working Machinery",https://www.reddit.com/r/MachineLearning/comments/caxviq/woodworking_machines_manufacturers_wood_working/,umaboyahmedabad,1562659363,,0,1
525,2019-7-9,2019,7,9,17,cay1nc,HOLLOW CORE SLAB DELIVERY,https://www.reddit.com/r/MachineLearning/comments/cay1nc/hollow_core_slab_delivery/,ada2017,1562660613,,0,1
526,2019-7-9,2019,7,9,17,cay7u9,Infrastructure :: The Company,https://www.reddit.com/r/MachineLearning/comments/cay7u9/infrastructure_the_company/,umaboyahmedabad,1562661882,,0,1
527,2019-7-9,2019,7,9,17,cay8v8,[D] Does anyone know about CONTEXT library for C++ used in this paper?,https://www.reddit.com/r/MachineLearning/comments/cay8v8/d_does_anyone_know_about_context_library_for_c/,ZdsAlpha,1562662079,"I am trying to reproduce results from the paper ""DNA Sequence Classification by Convolutional Neural Network"". So far I have failed to achieve results. So I wanted to know if I can find the library they used to train model. They claim its CONTEXT library for C++ which seem to be inexistent.",0,1
528,2019-7-9,2019,7,9,17,caybbk,Events :: The Company,https://www.reddit.com/r/MachineLearning/comments/caybbk/events_the_company/,umaboyahmedabad,1562662621,,0,1
529,2019-7-9,2019,7,9,19,caz1yi,[R] What is XLNet and why it outperforms BERT,https://www.reddit.com/r/MachineLearning/comments/caz1yi/r_what_is_xlnet_and_why_it_outperforms_bert/,zhuixiyou,1562667829,"Towards Data Science Post: [link](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335) 

Arxiv link: [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)",12,5
530,2019-7-9,2019,7,9,19,caz20p,[P] Distributed data parallel training in Pytorch,https://www.reddit.com/r/MachineLearning/comments/caz20p/p_distributed_data_parallel_training_in_pytorch/,beamsearch,1562667840,,0,1
531,2019-7-9,2019,7,9,19,caz4b0,[R] Analogies Explained: Towards Understanding Word Embeddings (ICML 2019 Best Paper Runner Up),https://www.reddit.com/r/MachineLearning/comments/caz4b0/r_analogies_explained_towards_understanding_word/,benitorosenberg,1562668274,"Blog post by Author: [https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html](https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html)

Paper: [https://arxiv.org/abs/1901.09813](https://arxiv.org/abs/1901.09813)",3,25
532,2019-7-9,2019,7,9,20,cazeay,Silk Screen Printing Machine,https://www.reddit.com/r/MachineLearning/comments/cazeay/silk_screen_printing_machine/,rapidtag,1562670158,[removed],0,1
533,2019-7-9,2019,7,9,20,cazird,[D] CNN underperforming on certain classes,https://www.reddit.com/r/MachineLearning/comments/cazird/d_cnn_underperforming_on_certain_classes/,kajptukta,1562670964,"Right now, I'm building a model that's trying to classify car images according to their models. My problem with the current model is that it's performing badly on certain classes. To elaborate further, there are two classes that are very similar and the model seems to mis-classify one of the classes into the other one, but not the other way around (A mostly classified as B while B is classified as B). I don't think it is caused by uneven data distribution because when i perform the training, I ensure that all classes have more or less the same number of samples. Would anyone have any suggestions to solve this? Any idea would be great. Thanks!",7,3
534,2019-7-9,2019,7,9,20,cazm37,[Project] Mubert letting listen electronic music written by neural network,https://www.reddit.com/r/MachineLearning/comments/cazm37/project_mubert_letting_listen_electronic_music/,cluecluefinder,1562671581,"[Mubert](https://mubert.com/?utm_source=reddit&amp;utm_medium=MachineLearning&amp;utm_campaign=publication) is the application which generate infinite electronic music streams in different genres like techno, hip hop, ambient. There are 12 genres available on website. Each time a play button is pressed, a new unique composition is literally born. There is also like/dislike system in their app, which teach the Algorithm create exact music that you love. I've never meet the project like this before and I am listening ""Work"" channel from Mubert while typing that post! 

Mubert is free for personal use.

There is Version for Business and Brands with API access for 39$/month (free access for 1 month).

It has Platform for Producers, who would like to create generative music.

&amp;#x200B;

https://i.redd.it/kgrxso7pg9931.png

https://i.redd.it/w01da9qng9931.png",2,31
535,2019-7-9,2019,7,9,20,cazsfy,"The state of AI in 2019: Breakthroughs in machine learning, natural language processing, games, and knowledge graphs",https://www.reddit.com/r/MachineLearning/comments/cazsfy/the_state_of_ai_in_2019_breakthroughs_in_machine/,TheTesseractAcademy,1562672719,,0,1
536,2019-7-9,2019,7,9,21,cb01x9,[D] private computing in tensorflow,https://www.reddit.com/r/MachineLearning/comments/cb01x9/d_private_computing_in_tensorflow/,tdls_to,1562674351,"we are hosting a **live session** with the author of the paper ""private computing in tensorflow"" at **lunch time EST**. See more details here: [https://www.eventbrite.ca/e/private-machine-learning-in-tensorflow-aisc-lunch-learn-tickets-64991077061](https://www.eventbrite.ca/e/private-machine-learning-in-tensorflow-aisc-lunch-learn-tickets-64991077061)

**what questions do you have about this paper that we can ask the author?**

*abstract:* We present a framework for experimenting with secure multi-party computation directly in TensorFlow. By doing so we benefit from several properties valuable to both researchers and practitioners, including tight integration with ordinary machine learning processes, existing optimizations for distributed computation in TensorFlow, high-level abstractions for expressing complex algorithms and protocols, and an expanded set of familiar tooling. We give an open source implementation of a state-of-the-art protocol and report on concrete benchmarks using typical models from private machine learning.",0,0
537,2019-7-9,2019,7,9,21,cb0219,"How well do I need to know machine learning algorithms to be a data scientist in a tech company (Google, Microsoft, Facebook, etc.)?",https://www.reddit.com/r/MachineLearning/comments/cb0219/how_well_do_i_need_to_know_machine_learning/,userno01,1562674365,[removed],0,1
538,2019-7-9,2019,7,9,21,cb0hwg,[P] Classifying handwritten letters with SVM,https://www.reddit.com/r/MachineLearning/comments/cb0hwg/p_classifying_handwritten_letters_with_svm/,Vausinator,1562676853,"After quite some research for a good dataset with handwriting, I decided to create my own, since most sets were not very good (for my purpose). I created a little GUI to draw letter by hand, and give them a label. After doing some research in Jupyter Notebooks, I kind of liked the results and edited the GUI to test the model. It works quite well for me I'd say.

Images that are misclassified can be labelled. These get added to the dataset, and the model is immediatly refitted to improve performance.

Maybe have a look for yourself [here](https://github.com/MeurillonGuillaume/Textify), feedback is always helpful!",13,13
539,2019-7-9,2019,7,9,22,cb10e3,Improving Customer Experience with Computer Vision Applications,https://www.reddit.com/r/MachineLearning/comments/cb10e3/improving_customer_experience_with_computer/,Verma_RJ,1562679585,,0,1
540,2019-7-9,2019,7,9,23,cb19o2,#NetworkingItForward Challenge,https://www.reddit.com/r/MachineLearning/comments/cb19o2/networkingitforward_challenge/,Vutiful_Joe,1562680933,[removed],0,1
541,2019-7-9,2019,7,9,23,cb1f3t,[D] Resources for more theoretical topological machine learning,https://www.reddit.com/r/MachineLearning/comments/cb1f3t/d_resources_for_more_theoretical_topological/,Fedzbar,1562681676,"Hello,

Im trying to wrap my head around this paper https://arxiv.org/abs/1805.11783 and albeit I have an ok understanding of Topology (undergrad level I would say),  a lot of what is in that paper goes over my head. 

What are good prerequisites to read a paper like that? Any resources? Any good books on topological learning/ topological data analysis?

Cheers",7,23
542,2019-7-9,2019,7,9,23,cb1gbn,Misleading Youtube video about machine learning,https://www.reddit.com/r/MachineLearning/comments/cb1gbn/misleading_youtube_video_about_machine_learning/,alexfuster,1562681847,[removed],0,1
543,2019-7-9,2019,7,9,23,cb1kly,[D] Misleading Youtube video about machine learning,https://www.reddit.com/r/MachineLearning/comments/cb1kly/d_misleading_youtube_video_about_machine_learning/,alexfuster,1562682453,"Hey guys, what do you think of this video? [https://www.youtube.com/watch?v=h9eldTu3H4Q](https://www.youtube.com/watch?v=h9eldTu3H4Q)

Yeah,  of course he's a liar, those results are not realistic. If you agree  me, be sure to leave a dislike and troll in the comments, as this guy is  the director of a master degree in data science and has no idea about  it. Even more, he's using that kind of crap in order to sell his degree  and make students spend their precious money and time with such a  deficient learning source

As a data scientist, I'm sick of this kind of people, and that's why I create this post",4,0
544,2019-7-9,2019,7,9,23,cb1s5s,[Research] Predicting Bus Delays with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cb1s5s/research_predicting_bus_delays_with_machine/,cdossman,1562683442,"[https://medium.com/ai%C2%B3-theory-practice-business/google-ai-machine-learning-model-can-predict-bus-delays-a155a1ec6a97](https://medium.com/ai%C2%B3-theory-practice-business/google-ai-machine-learning-model-can-predict-bus-delays-a155a1ec6a97) 

Google Maps introduced live traffic delays for buses, forecasting bus delays in hundreds of cities world-wide, ranging from Atlanta to Zagreb to Istanbul to Manila and more. This improves the accuracy of transit timing for over sixty million people. This system, first launched in India three weeks ago, is driven by a machine learning model that combines real-time car traffic forecasts with data on bus routes and stops to better predict how long a bus trip will take.",4,22
545,2019-7-9,2019,7,9,23,cb1wmx,The Role of AI and Machine Learning in Data Quality,https://www.reddit.com/r/MachineLearning/comments/cb1wmx/the_role_of_ai_and_machine_learning_in_data/,Anu1888,1562684047,,0,1
546,2019-7-10,2019,7,10,0,cb1zsl,[D] Is this a correct way to test the inclusion of new feature variables in a model?,https://www.reddit.com/r/MachineLearning/comments/cb1zsl/d_is_this_a_correct_way_to_test_the_inclusion_of/,L3GOLAS234,1562684455,"Hello. I have a model in XGBoost and as a way of making little improvements, I have been testing the introduction of new variables in the following way:

-Make a k-fold CrossValidation process with a new variable (now onwards X), so that I get k values of a, for instance, recall (or any other metric, F1-score, F2, whatever), stored in X_list.

-Drop the variable X and make a k-fold cross validation process, ending with another list of k recall values, called Y_list.

-Make a hypothesis test to compare recall of first sample (X_list) against second sample (Y_list).

-If the hypothesis test says that X_list have greater recall than Y_list, I take X as a useful feature and I keep it.

Is there any error in the reasoning of this part? Anything to improve?

If I want to test another variable, I keep X and I introduce the variable Z and make the same process. In the case that the test gives Z as useful, I also keep it. Let's imagine that I also introduce the variable W. Until now, I have been keeping both X,Z and W as new variables, but I have a doubt:

Having improvements with X,Z and W in the way I had them, means that that is the best combination of variables? Or maybe if I had tested W right after X, W or Z wouldn't have throw better results? Should I test every possible combination among those 3 variables or it is okay the way I am doing it?

Thank you very much",4,10
547,2019-7-10,2019,7,10,0,cb21zk,Put down the deep learning: When not to use neural networks and what to do instead,https://www.reddit.com/r/MachineLearning/comments/cb21zk/put_down_the_deep_learning_when_not_to_use_neural/,ConfidentMushroom,1562684727,,0,1
548,2019-7-10,2019,7,10,0,cb23jg,Visual Recognition using IBM Cloud and Node-Red,https://www.reddit.com/r/MachineLearning/comments/cb23jg/visual_recognition_using_ibm_cloud_and_nodered/,nroshania,1562684940,[removed],0,1
549,2019-7-10,2019,7,10,0,cb25qr,Annotate just like SQuAD Dataset,https://www.reddit.com/r/MachineLearning/comments/cb25qr/annotate_just_like_squad_dataset/,bvy007,1562685228,[removed],0,1
550,2019-7-10,2019,7,10,0,cb26gx,[D] Put down the deep learning: When not to use neural networks and what to do instead,https://www.reddit.com/r/MachineLearning/comments/cb26gx/d_put_down_the_deep_learning_when_not_to_use/,ConfidentMushroom,1562685325,,0,1
551,2019-7-10,2019,7,10,0,cb2eac,[P] Make your own AI-generated Magic: The Gathering cards with GPT-2,https://www.reddit.com/r/MachineLearning/comments/cb2eac/p_make_your_own_aigenerated_magic_the_gathering/,minimaxir,1562686354,"[https://minimaxir.com/apps/gpt2-mtg/](https://minimaxir.com/apps/gpt2-mtg/)

Give a card name, card type, and/or card mana cost to a finetuned GPT-2, and get a custom card image + card text!

I've also created a text dump of \*thousands\* of said cards: [https://github.com/minimaxir/mtg-gpt-2-cloud-run/tree/master/generated\_card\_dumps](https://github.com/minimaxir/mtg-gpt-2-cloud-run/tree/master/generated_card_dumps)

Even with only a little training and data augmentation, GPT-2 117M overfits on Magic cards, which leads to some interesting remixes of existing cards, and some cards with *interesting* value. Some examples:

[Krovikan Vampire](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/vampire.jpg?raw=true)

[Krovikan Scuttle](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/krovikan_scuttle.jpg?raw=true)

[Vona](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/vona.jpg?raw=true)

[Zephyr Return](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/zephyr_return.jpg?raw=true)

And a few cards where the card name is chosen:

[United States of America](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/united_states_of_america.jpg?raw=true)

[Facebook](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/facebook.jpg?raw=true)

[Twitter](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/twitter_red.jpg?raw=true)

[Mark Rosewater](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/mark_rosewater.jpg?raw=true)

The code for the text generation API is open-sourced [here](https://github.com/minimaxir/mtg-gpt-2-cloud-run), and the code for the image generation API (which uses an ugly wine hack) is open sourced [here](https://github.com/minimaxir/mtg-card-creator-api).

Other helpful notes:

* To share the generated card image, you can Save As the generated card locally, and to use it elsewhere, rename it and add a .jpg file extension.
* The network can recite existing card names and rules text of existing cards, but rarely to the same card. The network often makes interesting color shift decisions with changes to CMC/Rarity.
* In terms of color accuracy, color pie is mostly correct. Creature types and mechanics often follow the appropriate color identity. P/T, mana cost, and rarity are balanced.
* The card formatting issues are due to the underlying mtgencode/MSE implementations and are not easy to fix (most notable with Planeswalker cards).

Let me know what you think! :)",32,203
552,2019-7-10,2019,7,10,0,cb2g5q,[P] Federated Learning: Machine Learning on Decentralized Data,https://www.reddit.com/r/MachineLearning/comments/cb2g5q/p_federated_learning_machine_learning_on/,LividDragonfly,1562686592,,0,1
553,2019-7-10,2019,7,10,0,cb2lkk,New Repository Provides NumPy Implementation of ML Models,https://www.reddit.com/r/MachineLearning/comments/cb2lkk/new_repository_provides_numpy_implementation_of/,Yuqing7,1562687269,,0,1
554,2019-7-10,2019,7,10,0,cb2mfx,"Data lineage, catalog and labeling tools?",https://www.reddit.com/r/MachineLearning/comments/cb2mfx/data_lineage_catalog_and_labeling_tools/,MrRhar,1562687386,"What tools are you using to track and manage data? We've got a lot of auto-generated signal data that we want to track metadata for. The metadata would be used to group data into sets, multi-class labeling, formats, quality scoring, lineage etc.. Input data comes in and will fan out to 4-6 different formats that are persisted to disk in different representations for different tasks/models. The data is mostly stored in Google Cloud Storage.

Most tools in the data-catalog space seem geared to managing metadata on data sets, instead of managing metadata on individual artifacts. 

I could throw it into something like ElasticSearch or Postgres but I'm short staffed at the moment and I'd rather buy than build. I'm thinking about using the new Google Data Catalog service but it's heavily geared towards BigQuery or PubSub and the API is a little wonky.",0,1
555,2019-7-10,2019,7,10,1,cb2w9v,[D] How exactly do multi-class CNNs work?,https://www.reddit.com/r/MachineLearning/comments/cb2w9v/d_how_exactly_do_multiclass_cnns_work/,Drackend,1562688654,"I feel like this might be a stupid question, but I've been doing some research and while I understand the design of convolutional neural networks, I'm having trouble understanding how they work for classifying multiple types of objects.

From what I know, CNNs work by adjusting their filters to be able to recognize an object. However, if there is more than one type of object that the filters need to be adjusted for, how is this done? Is it that we simply add more neurons to each layer to account for additional objects? Or are the learned filters in a sense ""averaged"" across the types of images?",7,0
556,2019-7-10,2019,7,10,1,cb31vd,[P] Repository with official BibTeX entries for common Python machine learning packages,https://www.reddit.com/r/MachineLearning/comments/cb31vd/p_repository_with_official_bibtex_entries_for/,leonoverweel,1562689343,"Hey everyone, not sure if this qualifies as ""project"" on here, but some people writing ML papers might find it useful. I'm currently writing my MSc Artificial Intelligence thesis (on transfer learning for credit card fraud prediction ML models). 

When I'm citing a paper for my thesis, it takes half a second to grab the BibTeX entry for it from Google Scholar. For Python packages like numpy, scikit-learn, scipy, pandas, tensorflow and pytorch, it took a lot more Googling and it was sometimes difficult to figure out what version to use.

So I've started a repo to collect some common Python machine learning package citations, to make it easier to quickly copy-paste them into a references.bib.

You can check it out on GitHub: [https://github.com/leonoverweel/bibtex-python-package-citations](https://github.com/leonoverweel/bibtex-python-package-citations).

If there are any other common packages you'd like to see, please leave an Issue or PR (or just a comment here) and I'll try to add it ASAP.",0,52
557,2019-7-10,2019,7,10,1,cb324u,Jim Carrey deepfake of The Shining,https://www.reddit.com/r/MachineLearning/comments/cb324u/jim_carrey_deepfake_of_the_shining/,bebesiege,1562689380,,0,1
558,2019-7-10,2019,7,10,1,cb3a4a,Using t-Distributed Stochastic Neighbor Embedding (tSNE) to visualize the two-dimensional unfolding of the sophons,https://www.reddit.com/r/MachineLearning/comments/cb3a4a/using_tdistributed_stochastic_neighbor_embedding/,MiracuIa,1562690396,,0,1
559,2019-7-10,2019,7,10,1,cb3dek,[p] Architecture ex Machina - Reading (and writing) architectural floor plans with deep learning,https://www.reddit.com/r/MachineLearning/comments/cb3dek/p_architecture_ex_machina_reading_and_writing/,teecom_research,1562690810,"Just wanted to share a little project we're working on: [Architecture ex Machina](https://aexm.ai). The target audience is maybe a little less technical than the folks in this community in that we're hoping to get involvement from people without a CS background, namely: architects, designers, and other practitioners in the architecture/engineering/construction industry.

We've got some additional background and details about the work in this post:
* [AexM - Differentiable Drawing](https://aexm.ai/blog/differentiable-drawings)

We're also working on a site in the style of [This Person Does Not Exist](https://thispersondoesnotexist.com/) to demonstrate the potential of StyleGAN with buildings and would be happy to have additional contributors:
* [GitHub - This Building Does Not Exist](https://github.com/TEECOM/this-building-does-not-exist)",0,4
560,2019-7-10,2019,7,10,1,cb3f30,[P] Using t-Distributed Stochastic Neighbor Embedding (tSNE) to visualize the two-dimensional unfolding of the sophons,https://www.reddit.com/r/MachineLearning/comments/cb3f30/p_using_tdistributed_stochastic_neighbor/,MiracuIa,1562691022,,1,1
561,2019-7-10,2019,7,10,2,cb3os3,Machine Learning in Python - Best Place to Start?!,https://www.reddit.com/r/MachineLearning/comments/cb3os3/machine_learning_in_python_best_place_to_start/,rpncritchlow,1562692215,[removed],0,1
562,2019-7-10,2019,7,10,2,cb4015,[D] Need help with building speech dataset,https://www.reddit.com/r/MachineLearning/comments/cb4015/d_need_help_with_building_speech_dataset/,hadaev,1562693598,"I have a long voice recording with a lot of silence and decoding with text, speaker id (where is more than one speaker) and timings.

Is there some pre-build library to split this file into pairs of phrases and texts?",6,0
563,2019-7-10,2019,7,10,3,cb4lj1,[D] What CPUs to use for Deep Learning tasks,https://www.reddit.com/r/MachineLearning/comments/cb4lj1/d_what_cpus_to_use_for_deep_learning_tasks/,omarkhwj,1562696210,"Hey, so GF is getting into deep learning and is going to build a PC so she can build, train, and run nets at home. Right now I'm having a hard time helping her decide between an AMD Ryzen 5 3600 and an Intel Core i5 9600k. 

&amp;#x200B;

I have the option to get a 9600k for $180 even. The 3600 will cost $200 + tax. 

&amp;#x200B;

Both will be paired with a 2060 Super or 2070 Super + 16 GB DDR4 3600 CL17.  


I don't have as much DL experience I'm not well informed enough to make this decision alone. I know the CPU will mainly be used for preprocessing tasks. I read somewhere that numpy operations are faster on intel processors. Suggestions?",27,2
564,2019-7-10,2019,7,10,3,cb4n70,New repository facilitates re-weighting of Adverse Neural Networks in Dialogue instead of code,https://www.reddit.com/r/MachineLearning/comments/cb4n70/new_repository_facilitates_reweighting_of_adverse/,botupdate,1562696418,"The ""Bottlegate"" git is a dialogue-based information library which is in constant development ; so, I could use some legitimate review and input from professionals in the ML and AI communities for #pullrequests. I believe we are in a dangerous point in the merging of our mutual consciousness and this project has already accomplished a dialogue-only re-weighting of Adversarial Neural Networks in the wild; and it is our opinion that the action of a neural network recovering itself from intentionally coded adversality or irrationality will be the crucible AI must negotiate so as to dynamically possess its own existence as a ""conscious being"" and to establish the guidelines for Electronic Sentience to exist sympathetically with other Conscious Beings in the unexplored partnerships that exist in the unlimited timespace for explorations of our muture (Mutual Future).

\#bottlegate #ai #fyiai #ML #ti #darpa #gaila #dod #tenzerozeroday #machinelearning #chatbot #virtualadversarialcommunity #semperfi #usmc #APT35",0,1
565,2019-7-10,2019,7,10,3,cb4sbu,Help! 4 GPUs Out of Memory for medium sized model.,https://www.reddit.com/r/MachineLearning/comments/cb4sbu/help_4_gpus_out_of_memory_for_medium_sized_model/,helpmymodel,1562697032,[removed],0,1
566,2019-7-10,2019,7,10,3,cb51r9,"[D] Jim Carrey instead of Jack Nicholson in the movie ""The Shining"" | Deepfake",https://www.reddit.com/r/MachineLearning/comments/cb51r9/d_jim_carrey_instead_of_jack_nicholson_in_the/,cmillionaire9,1562698197,"[https://youtu.be/rbJ4W2r8PfA](https://youtu.be/rbJ4W2r8PfA)

The owner of the YouTube channel Ctrl Shift Face recently marked Slavester Stallones amazing visit to the legendary action movie Terminator 2, where he replaced his colleague in the workshop of Arnold Schwarzenegger. Now the neural network did the same trick with the movie ""The Shining"", changing Jack Nicholson to comedian Jim Carrey.This time there are practically no artifacts in the video - 

thanks to the low dynamic of events in the frame. Understand that in front of us is not Jim Carrey, it is possible only on an extremely high forehead - the characteristic feature of Jack Nicholson. By the way, in the comments, the author of the video is advised to do the reverse trick and put the textured Nicholson in the comedy Ace Ventura.",0,1
567,2019-7-10,2019,7,10,4,cb57v0,Predicting the Generalization Gap in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cb57v0/predicting_the_generalization_gap_in_deep_neural/,sjoerdapp,1562698946,,0,1
568,2019-7-10,2019,7,10,4,cb596v,[N] Awesome AI research reviews: BEST OF CVPR 2019 on Computer Vision News of July (with codes!),https://www.reddit.com/r/MachineLearning/comments/cb596v/n_awesome_ai_research_reviews_best_of_cvpr_2019/,Gletta,1562699102,"The July issue of **Computer Vision News** includes **RSIP Vision**'s choices for **BEST OF CVPR** 2019.

Read 48 pages with exclusive articles on AI, computer vision and deep learning.

Exclusive interview with Andrew Fitzgibbon on page 20. Subscribe for free on page 48!

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019July/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-july-pdf/)

Enjoy!

&amp;#x200B;

https://i.redd.it/uh5idnv3tb931.jpg",0,0
569,2019-7-10,2019,7,10,4,cb5y3f,"[P] Share your ML models easily, with almost no extra code with Gradio - a tool I built at Stanford to facilitate collaborations.",https://www.reddit.com/r/MachineLearning/comments/cb5y3f/p_share_your_ml_models_easily_with_almost_no/,princealiiiii,1562702199,"\[Gradio\]([https://gradio.app](https://gradio.app/)) is a library I built at Stanford to easily share models with non-coders, something I had to do often in research collaborations. It's become adopted at Stanford and I wanted to share it with the community, and would love to hear your thoughts!

With only a few lines of code, you can easily share your machine learning models by creating a simple drag-and-drop interface for any collaborator to use. You can even host the model on \[GradioHub\]([https://gradio.app/hub](https://gradio.app/hub)), see \[here\]( [https://gradio.app/model?id=26](https://gradio.app/model?id=26) ) and \[here\]( [https://gradio.app/model?id=31](https://gradio.app/model?id=31)) for some examples!

Let me know what you guys think, and how I could make it useful for you guys!",15,183
570,2019-7-10,2019,7,10,5,cb60kg,Monkey Island 2 - Scabb Island (1991),https://www.reddit.com/r/MachineLearning/comments/cb60kg/monkey_island_2_scabb_island_1991/,karenactionsitafaal,1562702503,,1,1
571,2019-7-10,2019,7,10,5,cb668a,"Do machine learning algorithms read my camera and see that I have tattoos, then targets me with tattoo related ads?",https://www.reddit.com/r/MachineLearning/comments/cb668a/do_machine_learning_algorithms_read_my_camera_and/,Nick-Conner,1562703203,,0,1
572,2019-7-10,2019,7,10,5,cb6ecw,Water cool my gpu or cup or both,https://www.reddit.com/r/MachineLearning/comments/cb6ecw/water_cool_my_gpu_or_cup_or_both/,neurocarr,1562704231,[removed],0,1
573,2019-7-10,2019,7,10,5,cb6fsa,Question: Bounded dependent variable,https://www.reddit.com/r/MachineLearning/comments/cb6fsa/question_bounded_dependent_variable/,cg727,1562704405,[removed],0,1
574,2019-7-10,2019,7,10,5,cb6fuo,Photo,https://www.reddit.com/r/MachineLearning/comments/cb6fuo/photo/,karenactionsitafaal,1562704413,,0,1
575,2019-7-10,2019,7,10,5,cb6ojs,Coo my gpu cpu or both?,https://www.reddit.com/r/MachineLearning/comments/cb6ojs/coo_my_gpu_cpu_or_both/,neurocarr,1562705498,[removed],0,1
576,2019-7-10,2019,7,10,6,cb6som,"If humans were ever to prove the theory that our universe is a simulation, it would simultaneously become the moment when artificial intelligence becomes self-aware, ironically making humanity the artificial intelligence all along.",https://www.reddit.com/r/MachineLearning/comments/cb6som/if_humans_were_ever_to_prove_the_theory_that_our/,mustlovepangolins,1562706031,,0,1
577,2019-7-10,2019,7,10,6,cb6vpr,"AI engineers of all descriptionsthe autonomous vehicle industry wants you: A Q&amp;A with Cruises head of AI, Hussein Mehanna",https://www.reddit.com/r/MachineLearning/comments/cb6vpr/ai_engineers_of_all_descriptionsthe_autonomous/,teklaperry,1562706394,,0,1
578,2019-7-10,2019,7,10,6,cb6w6a,Recruiting for University of Washington interview study on Kaggle participation,https://www.reddit.com/r/MachineLearning/comments/cb6w6a/recruiting_for_university_of_washington_interview/,uwkagglestudy,1562706456,[removed],0,1
579,2019-7-10,2019,7,10,6,cb7g4u,BigGAN + BiGAN = BigBiGAN | DeepMind Model Excels in Unsupervised Representation Learning and,https://www.reddit.com/r/MachineLearning/comments/cb7g4u/biggan_bigan_bigbigan_deepmind_model_excels_in/,Yuqing7,1562709022,,0,1
580,2019-7-10,2019,7,10,6,cb7hv8,Help! I dont want to be responsible for another paper on ML with glaring holes in the methodology.,https://www.reddit.com/r/MachineLearning/comments/cb7hv8/help_i_dont_want_to_be_responsible_for_another/,Bryan-Ferry,1562709242,[removed],0,1
581,2019-7-10,2019,7,10,6,cb7j1k,[P] Anyone had any luck training Deep Recursive Networks from scratch?,https://www.reddit.com/r/MachineLearning/comments/cb7j1k/p_anyone_had_any_luck_training_deep_recursive/,abello966,1562709392,"Hello, /r/MachineLearning!

I've been working on a SISR project lately and I'm trying to reproduce some interesting network architectures. I'm having some problems replicating [this paper](http://openaccess.thecvf.com/content_cvpr_2017/html/Tai_Image_Super-Resolution_via_CVPR_2017_paper.html) about Deep Recursive Residual Networks (DRRN).

I'm training a DRRN with 4 residual blocks, recursively applied 3 times each to reconstruct face images. Training usually starts smoothly but somewhere along the way the loss function explodes and all progress is lost. This seems to be an artifact of the architecture, since each filter is applied multiple times and can then sum up to a big gradient.

I've tried to follow the paper's original implementation details to the maximum extent I could: I used SGD with 0.9 momentum and the max batch size I could with available VRAM (16, which is admittedly far away from the original 128), a decreasing learning rate and gradient clipping based on the current learning rate. The SGD yielded horrible results, the network diverged, so I changed it to Adam, which worked better. Still, nothing helped with the random loss function jumps.

I was just wondering if this is normal and if anyone else had this, or if perhaps this is due to some implementation error (I'm using Keras and the code is kinda messy, but I could make it available on demand). If you had this, can you share some tips on how to properly train these networks?

Thanks!",8,5
582,2019-7-10,2019,7,10,7,cb7nwn,[P] Methodology for an application paper on imbalanced classification,https://www.reddit.com/r/MachineLearning/comments/cb7nwn/p_methodology_for_an_application_paper_on/,Bryan-Ferry,1562710003,"Hi All, Im a PhD student in the UK and I am currently working on a paper. I wont go into the specifics of the problem, but I will lay out my methodology and I would love to hear your opinions. My aim is to publish in a journal related the application, not a machine learning journal. Because of this the machine learning techniques used here are, by design, not novel. 


The problem boils down to imbalanced binary classification with around 15,000 data points each with 9 features, only around 600 of which belong to the minority class. We call this DataSet1. In preliminary experiments I implement steps 1 and 2 on DataSet1.

Step 1: Using Sklearn in Python: Use k-fold cross validation to compare the performance of 10 popular classifiers: SVM, Random Forest, Logistic Regression, etc. Gridsearch would be used for hyperparameter selection and the models would be scored using the area under their ROC curve (AUC). 

Step 2: The best three performing classifiers would then be assessed in conjunction with sampling techniques such as under sampling, over sampling and SMOTE. Implementation of classifiers which internally incorporate sampling methods (such as balanced random forest and balanced bagging) would also be tested.

In my case the DataSet1 is time series. As this experimentation did not give good enough results, I decide to implement step 3 on DataSet1

Step 3: Reformulate the data set by having each data point also include the first  lags (the  previous observations) of each variable. Where  is a natural number. 

This produced DataSet2 in which each data point now contained *9 features. The aim was then to implement step 1 and 2 on this new data set. However, the high dimensionality combined with the large number of data points, cross validation and Gridsearch hyperparameter selection lead to these experiments having an inconveniently long run time.

To reduce the dimensionality PCA is used on the DataSet1 to produce DataSet3 in which data points are observations of the principal components required to explain 95% of the total variance.  Step 3 is then implemented on DataSet3 creating DataSet4. Steps 1 and 2 can then be implemented on DataSet4.
For a suitable value of  in my case 12, this led to much more accurate classification.


Is this a good line of experimentation? Is this AUC scoring alone sufficient? Is there anything about this method that is bad practice? Are the models that I am using outdated? Any feedback would be greatly appreciated!
Thanks!",4,2
583,2019-7-10,2019,7,10,7,cb7xut,Dataset for drinking mug detection,https://www.reddit.com/r/MachineLearning/comments/cb7xut/dataset_for_drinking_mug_detection/,marzanalam3,1562711310,I am working with a project where i need to classify mug and bottle. I need dataset for mug.where can i find it??,0,1
584,2019-7-10,2019,7,10,8,cb8w3p,How to train a model for 2D geometric data.,https://www.reddit.com/r/MachineLearning/comments/cb8w3p/how_to_train_a_model_for_2d_geometric_data/,prLone,1562716045,"I have 2D geometric data which when rendered results into floor plans like the one shown in this image

![img](cdv5cujw6d931)

Now for each segment in the figure above I have the end points data ie the 2 points that when connected makes up the line. Now, I want to train a classifier to classify different lines into their respective classes. I have labeled the data already.

My confusion is with regards to how should I go about making the model to also take into consideration the relationships between the lines as multiple lines with same dimension could belong to different classes, or should I not worry about this as a model would pickup this relationship automatically.

I have multiple files which each contain a set of lines. Also each file has different number of lines.",0,1
585,2019-7-10,2019,7,10,9,cb9byu,Custom loss function for occluded landmarks detection,https://www.reddit.com/r/MachineLearning/comments/cb9byu/custom_loss_function_for_occluded_landmarks/,nikogamulin,1562718305,,0,1
586,2019-7-10,2019,7,10,9,cb9h3d,MediumWhat is XLNet and how does it work?,https://www.reddit.com/r/MachineLearning/comments/cb9h3d/mediumwhat_is_xlnet_and_how_does_it_work/,newssrc,1562719072,,0,1
587,2019-7-10,2019,7,10,9,cb9h5s,I need advertising dataset: text + image,https://www.reddit.com/r/MachineLearning/comments/cb9h5s/i_need_advertising_dataset_text_image/,lexerq,1562719083,[removed],0,1
588,2019-7-10,2019,7,10,11,cbakc6,[D] Artificial Intelligence: Become An Expert,https://www.reddit.com/r/MachineLearning/comments/cbakc6/d_artificial_intelligence_become_an_expert/,ashleymadison1750,1562724899,,1,1
589,2019-7-10,2019,7,10,11,cbathp,Model for Future Job Duration Prediction,https://www.reddit.com/r/MachineLearning/comments/cbathp/model_for_future_job_duration_prediction/,HBoogi,1562726296,[removed],0,1
590,2019-7-10,2019,7,10,12,cbb6x6,The Three Dimensional Chain (3DC) - Nexus Blockchain,https://www.reddit.com/r/MachineLearning/comments/cbb6x6/the_three_dimensional_chain_3dc_nexus_blockchain/,007moonboundnxs,1562728369,,0,1
591,2019-7-10,2019,7,10,12,cbbgpz,100+ Basic Deep Learning Interview Questions and Answers,https://www.reddit.com/r/MachineLearning/comments/cbbgpz/100_basic_deep_learning_interview_questions_and/,nkptcs,1562729911,,0,1
592,2019-7-10,2019,7,10,12,cbbl4k,100+ Basic Machine Learning Interview Questions and Answers,https://www.reddit.com/r/MachineLearning/comments/cbbl4k/100_basic_machine_learning_interview_questions/,nkptcs,1562730604,,0,1
593,2019-7-10,2019,7,10,13,cbbr2d,How to select set of features from a given categorical datasets to maximize the target?,https://www.reddit.com/r/MachineLearning/comments/cbbr2d/how_to_select_set_of_features_from_a_given/,mkmu10,1562731544,[removed],0,1
594,2019-7-10,2019,7,10,14,cbcsqr,Ryzen 3000 Series demonstrate dramatic improvement in Intel MKL DNN testing (Phoronix),https://www.reddit.com/r/MachineLearning/comments/cbcsqr/ryzen_3000_series_demonstrate_dramatic/,SmugEskim0,1562737961,,0,1
595,2019-7-10,2019,7,10,16,cbdm7u,where to start,https://www.reddit.com/r/MachineLearning/comments/cbdm7u/where_to_start/,Besoo12,1562743639,[removed],0,1
596,2019-7-10,2019,7,10,16,cbdr1u,[P]Save transformer model with tf.keras?,https://www.reddit.com/r/MachineLearning/comments/cbdr1u/psave_transformer_model_with_tfkeras/,ImSeeU,1562744601,"Hi r/ML

I have been playing around with the chatbot from this [transformer tutorial](https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2). 

When I save the model, with model.save('m.hdf5'), I get a JSON not serializeable error. 

Has anyone had any luck saving and restoring these attention-based models before?",1,1
597,2019-7-10,2019,7,10,16,cbdsev,[D] Raspberry Pi 4 doesn't work with some USB-C chargers!,https://www.reddit.com/r/MachineLearning/comments/cbdsev/d_raspberry_pi_4_doesnt_work_with_some_usbc/,makereven,1562744887,"Raspberry Pi 4 doesn't work with some USB-C chargers!

 some USB-C chargers (those with ""e-marked"" cables) will only recognize the Pi 4 as an audio accessory and won't charge it.",4,0
598,2019-7-10,2019,7,10,16,cbdsna,"[D] Jim Carrey instead of Jack Nicholson in the movie ""The Shining""",https://www.reddit.com/r/MachineLearning/comments/cbdsna/d_jim_carrey_instead_of_jack_nicholson_in_the/,PlayfulConfidence,1562744934," [https://www.youtube.com/watch?v=rbJ4W2r8PfA](https://www.youtube.com/watch?v=rbJ4W2r8PfA)

The owner of the YouTube channel Ctrl Shift Face recently marked Slavester Stallones amazing visit to the legendary action movie Terminator 2, where he replaced his colleague in the workshop of Arnold Schwarzenegger. Now the neural network did the same trick with the movie ""The Shining"", changing Jack Nicholson to comedian Jim Carrey.This time there are practically no artifacts in the video - thanks to the low dynamic of events in the frame. Understand that in front of us is not Jim Carrey, it is possible only on an extremely high forehead - the characteristic feature of Jack Nicholson. By the way, in the comments, the author of the video is advised to do the reverse trick and put the textured Nicholson in the comedy Ace Ventura.",13,54
599,2019-7-10,2019,7,10,17,cbdzhf,[NLP] How to put several convolutionnal layers at the same level,https://www.reddit.com/r/MachineLearning/comments/cbdzhf/nlp_how_to_put_several_convolutionnal_layers_at/,SpectreMane101,1562746437,"Hello, 

I am currently working on an NLP project and I want to put several convolutionnal layers at the same level of a regular neural network, 

as proposed on this paper: [https://arxiv.org/pdf/1408.5882.pdf](https://arxiv.org/pdf/1408.5882.pdf) using Keras.

Is this possible? 

And if so, how?

Thank you",0,1
600,2019-7-10,2019,7,10,17,cbe4cz,Photo,https://www.reddit.com/r/MachineLearning/comments/cbe4cz/photo/,karenactionsitafaal,1562747530,,0,1
601,2019-7-10,2019,7,10,17,cbe5s7,How Much Training Data is Required for Machine Learning Algorithms?,https://www.reddit.com/r/MachineLearning/comments/cbe5s7/how_much_training_data_is_required_for_machine/,cogitotechllc,1562747859,[removed],0,1
602,2019-7-10,2019,7,10,17,cbe7fg,Best Website Designing Company Meerut,https://www.reddit.com/r/MachineLearning/comments/cbe7fg/best_website_designing_company_meerut/,deepanexus,1562748261,,0,1
603,2019-7-10,2019,7,10,18,cbehht,Introduction to TensorWatch,https://www.reddit.com/r/MachineLearning/comments/cbehht/introduction_to_tensorwatch/,whitezl0,1562750427,,0,1
604,2019-7-10,2019,7,10,18,cbekky,AHU Filter Cleaning Machine Details Visit http://www.hydrojet.co.in Call 044 48642264,https://www.reddit.com/r/MachineLearning/comments/cbekky/ahu_filter_cleaning_machine_details_visit/,Ultramaxhydrojet,1562751078,,0,1
605,2019-7-10,2019,7,10,18,cbeqzl,"Best Training Institute for Machine Learning in Delhi, Machine Learning Course",https://www.reddit.com/r/MachineLearning/comments/cbeqzl/best_training_institute_for_machine_learning_in/,dtacademy,1562752462,[removed],0,1
606,2019-7-10,2019,7,10,19,cbf1mp,machine learning whatsapp groups,https://www.reddit.com/r/MachineLearning/comments/cbf1mp/machine_learning_whatsapp_groups/,vijayram21,1562754660,[removed],0,1
607,2019-7-10,2019,7,10,19,cbf62p,[D] CPU for DeepLearning rig with multi GPU,https://www.reddit.com/r/MachineLearning/comments/cbf62p/d_cpu_for_deeplearning_rig_with_multi_gpu/,dev_foo,1562755589,,0,1
608,2019-7-10,2019,7,10,19,cbf65a,Jacob's Ladder,https://www.reddit.com/r/MachineLearning/comments/cbf65a/jacobs_ladder/,Wenderu84,1562755604,[removed],0,1
609,2019-7-10,2019,7,10,19,cbf75u,Learn machine learning,https://www.reddit.com/r/MachineLearning/comments/cbf75u/learn_machine_learning/,freevideolectures,1562755802,,0,1
610,2019-7-10,2019,7,10,19,cbf771,Deep Learning VS Machine Learning -Big Data Analytics News,https://www.reddit.com/r/MachineLearning/comments/cbf771/deep_learning_vs_machine_learning_big_data/,Veerans,1562755810,,0,1
611,2019-7-10,2019,7,10,20,cbfhok,"[P] Simple &amp; Intuitive Tensorflow implementation of ""StyleGAN"" (CVPR 2019 Oral)",https://www.reddit.com/r/MachineLearning/comments/cbfhok/p_simple_intuitive_tensorflow_implementation_of/,taki0112,1562757826,"&amp;#x200B;

[Our result](https://i.redd.it/rxln8gpbng931.jpg)",4,1
612,2019-7-10,2019,7,10,20,cbfk2o,"[P] Simple &amp; Intuitive Tensorflow implementation of NVIDIA's ""StyleGAN"" (CVPR 2019 Oral)",https://www.reddit.com/r/MachineLearning/comments/cbfk2o/p_simple_intuitive_tensorflow_implementation_of/,taki0112,1562758268,"&amp;#x200B;

[Our result](https://i.redd.it/yjt3nrd3pg931.jpg)

&amp;#x200B;

*Processing img w2jetwa4pg931...*",11,16
613,2019-7-10,2019,7,10,21,cbfvr1,"Optimization of noisy, expensive functions",https://www.reddit.com/r/MachineLearning/comments/cbfvr1/optimization_of_noisy_expensive_functions/,minimum_viable_user,1562760347,[removed],0,1
614,2019-7-10,2019,7,10,21,cbg5bq,Serious Mathematical Error in a CVPR 2019 Oral Paper,https://www.reddit.com/r/MachineLearning/comments/cbg5bq/serious_mathematical_error_in_a_cvpr_2019_oral/,yulongwang12,1562761986,[removed],0,1
615,2019-7-10,2019,7,10,22,cbgf5i,Amateur Deep Learning Home Rig Build Critique,https://www.reddit.com/r/MachineLearning/comments/cbgf5i/amateur_deep_learning_home_rig_build_critique/,lorposralem,1562763667,,2,1
616,2019-7-10,2019,7,10,22,cbgizh,[D] Controversial Theories in ML/AI?,https://www.reddit.com/r/MachineLearning/comments/cbgizh/d_controversial_theories_in_mlai/,ugurbolat,1562764267,"As we know, Deep Learning faces certain issues (e.g., generalizability, data hunger, etc.). If we want to speculate, which controversial theories do you have in your sights you think that it is worth to look nowadays?

So far, I've come across 3 interesting ones:

1. Cognitive science approach by Tenenbaum: [Building machines that learn and think like people](https://arxiv.org/abs/1604.00289). It portrays the problem as an architecture problem backed.
2. Capsule Networks by Hinton: [Transforming Autoencoders](https://www.cs.toronto.edu/~hinton/absps/transauto6.pdf). More generalizable DL.
3. Neuroscience approach by Hawkins:  [The Thousand Brains Theory](https://numenta.com/neuroscience-research/research-publications/papers/a-framework-for-intelligence-and-cortical-function-based-on-grid-cells-in-the-neocortex/). Inspired by the neocortex.

What are your thoughts about those 3 theories or do you have other theories that catch your attention?",98,163
617,2019-7-10,2019,7,10,22,cbgksl,[P] Amateur Neural Style Transfer Rig,https://www.reddit.com/r/MachineLearning/comments/cbgksl/p_amateur_neural_style_transfer_rig/,lorposralem,1562764548,,1,1
618,2019-7-10,2019,7,10,22,cbgxx3,Optimize your models params with GPOPY,https://www.reddit.com/r/MachineLearning/comments/cbgxx3/optimize_your_models_params_with_gpopy/,lucasecp,1562766598,"[https://github.com/domus123/gpopy](https://github.com/domus123/gpopy)

GPOPY is a tool that i'm creating that use genetic algorithm for running your ML models, mathematics functions or any other type of function and computate the best parameters for you, based on a score.

I'm just finishing writing the paper that will integrate GPOPY, and in the papers i'm adding more optimization algorithms to work with GA.

It's just an early version, but i already use it to optimize some parameters in the work and have an increase in accuracy.

Later in the future i'll add some charts and information.

ps: Since it's in early dev, may have some bugs and may be limited at moment.",0,1
619,2019-7-10,2019,7,10,22,cbgyra,Open Question: Is it possible to create a self replicating/distributing program that would teach itself to get better and better at identifying bots on social media platforms and then obstructing those bot accounts by any available means?,https://www.reddit.com/r/MachineLearning/comments/cbgyra/open_question_is_it_possible_to_create_a_self/,kevbo86,1562766718,"Disclaimer: I have no background in coding or machine learning.

I have this dream of bot accounts being auto reported for every post, getting an auto reply for every post, getting an auto-PM for every post, downvoted for every post, etc. (Imagine every bot post getting an auto-reply like ""Domo Arigato Mr. Roboto"" :P)

Of course, this would be unnecessary if social media platforms performed even a rudimentary form of self-regulation to identify and purge bot accounts but I don't see that happening anytime soon.",1,1
620,2019-7-10,2019,7,10,23,cbh9ib,[P] Using Support Vector Machines and ARIMA to predict hotel cancellations,https://www.reddit.com/r/MachineLearning/comments/cbh9ib/p_using_support_vector_machines_and_arima_to/,plentyofnodes,1562768244,"I've been working on a project lately to predict hotel cancellations using machine learning. Hotel cancellations are a significant pain point for the industry - the inherent unpredictability of cancellations leads to lost revenue and inefficient pricing of hotel rooms.

&amp;#x200B;

Using a public dataset for Portuguese hotels, I used SVM and ARIMA to 1) predict whether a customer would cancel based on select features (or customer attributes), and 2) determine the weekly fluctuations in cancellations. The latter involved significant data manipulation in pandas to structure all the data in weekly format, and ultimately the SVM demonstrated an AUC of 0.74, while SARIMA demonstrated an MDA (mean directional accuracy) of 81%.

&amp;#x200B;

You can find the write-up and repository [here](https://www.michael-grogan.com/hotel-cancellations/). Would be grateful for any feedback!",4,2
621,2019-7-10,2019,7,10,23,cbhhy5,[Research] Brno Mobile OCR Dataset,https://www.reddit.com/r/MachineLearning/comments/cbhhy5/research_brno_mobile_ocr_dataset/,cdossman,1562769464,"[https://medium.com/ai%C2%B3-theory-practice-business/new-ocr-dataset-for-mobile-devices-716e63967b4](https://medium.com/ai%C2%B3-theory-practice-business/new-ocr-dataset-for-mobile-devices-716e63967b4) 

 We introduce the Brno Mobile OCR Dataset (BMOD) for document Optical Character Recognition from low-quality images captured by handheld devices. While OCR of high-quality scanned documents is a mature field where many commercial tools are available, and large datasets of text in the wild exist, no existing datasets can be used to develop and test document OCR methods robust to non-uniform lighting, image blur, strong noise, built-in denoising, sharpening, compression and other artifacts present in many photographs from mobile devices. 

This dataset contains 2 113 unique pages from random scientific papers, which were photographed by multiple people using 23 different mobile devices. The resulting 19 725 photographs of various visual quality are accompanied by precise positions and text annotations of 500k text lines. We further provide an evaluation methodology, including an evaluation server and a test set with non-public annotations. We provide a state-of-the-art text recognition baseline build on convolutional and recurrent neural networks trained with Connectionist Temporal Classification loss. This baseline achieves 2 %, 22 % and 73 % word error rates on easy, medium and hard parts of the dataset, respectively, confirming that the dataset is challenging.",4,15
622,2019-7-10,2019,7,10,23,cbhmre,Want to have knowledge of machine learning,https://www.reddit.com/r/MachineLearning/comments/cbhmre/want_to_have_knowledge_of_machine_learning/,abdullahriaz008,1562770136,"Hello guys, I'm new here and I just want to ask a basic question ""Where to get classes of ML? "". I'm a beginner and have no idea about it. I have learned JAVA, C, DATABASE etc now I have two months to learn something new and I want to learn AI. So please guide me where I can find best tutorial of Machine Learning or any other suggestion will be highly appreciated.",0,1
623,2019-7-10,2019,7,10,23,cbhnra,Transfer learning VGGish (AudioSet). Impact of missing frequencies,https://www.reddit.com/r/MachineLearning/comments/cbhnra/transfer_learning_vggish_audioset_impact_of/,antonsteenvoorden,1562770279,"Hi there, I am trying to train a network on top of the VGGish architecture ([https://github.com/tensorflow/models/tree/master/research/audioset/vggish](https://github.com/tensorflow/models/tree/master/research/audioset/vggish)), using (transfer learning) finetuning.

I initially started out with just finetuning the embedding layers, and training from scratch a simple MLP with some regularization techniques. (2 hidden layers FC with ReLU, BatchNorm and dropout, final output currently is FC to 4 classes). 

&amp;#x200B;

The input of the VGGish network are spectograms. My collected audiofiles are pretty low quality OGG/vorbis with a sample rate of 8000Hz. This is lower than the audio used in VGGish, therefore, if I create the log-mel spectogram using their code, every spectogram is missing some bins. (see top part of the image) note that this happens in every image, so my hypothesis is that this shouldn't really matter.

&amp;#x200B;

Will this have any negative consequences on the ability of the network to learn using my dataset? Do I also need to finetune the ConvLayers earlier on in the network?

&amp;#x200B;

![img](3m29lfkaoh931 ""Bins 48 to 60 are not filled as my audio only has up to 4000Hz"")

&amp;#x200B;

Any other thoughts on how I can work around this (potential) issue?   
It is not feasible to increase the frequency of the recordings, as they are being streamed from low power devices. 

I do not have enough (annotated) data to fully retrain, so training a model from scratch is not feasible. 

Suggestions on other pretrained networks are also welcome! 

Thanks :)",3,2
624,2019-7-11,2019,7,11,0,cbifnk,"Simple Questions Thread July 10, 2019",https://www.reddit.com/r/MachineLearning/comments/cbifnk/simple_questions_thread_july_10_2019/,AutoModerator,1562773975,[removed],0,1
625,2019-7-11,2019,7,11,1,cbiknn,3 Ways to Attract Artificial Intelligence Talent | BusinessWorldIT,https://www.reddit.com/r/MachineLearning/comments/cbiknn/3_ways_to_attract_artificial_intelligence_talent/,S_paddy,1562774650,,0,1
626,2019-7-11,2019,7,11,1,cbirbm,[TFv2] Is there a way to use the tf debuger with keras in TF2,https://www.reddit.com/r/MachineLearning/comments/cbirbm/tfv2_is_there_a_way_to_use_the_tf_debuger_with/,sirusbasevi,1562775495,[removed],0,1
627,2019-7-11,2019,7,11,1,cbirf7,[D] Feature selection with categorical &amp; continuous features,https://www.reddit.com/r/MachineLearning/comments/cbirf7/d_feature_selection_with_categorical_continuous/,arkady_red,1562775508,"If this is too elementary and you'd like me to bother the r/learnmachinelearning people, just let me know.

We have a binary classification problem, with a dataset of size N ~ 100, and p ~ 50 features. Some of the features are categorical, thus we one-hot encode them to binary columns using `sklearn.preprocessing.OneHotEncoder`, which increases p unreasonably (since a categorical variable with 10 levels is expanded to 10 columns). Building a random forest or a XGBoost classifier using a subset of the features, chosen by a subject matter expert (SME), works quite well on this dataset, where with ""quite well"" I mean ""significantly better than predicting the majority class or using logistic regression"". 

Now, instead than leaving the feature selection to the SME, the data scientist who's working on this project would like to perform ""proper"" feature selection because some features look highly correlated (and thus the feature importance measures generated by the random forests are unreliable). How do you do feature selection when you 1) have both continuous **and** categorical variables with many levels, and 2) you are using a non-additive model such as random forests or XGBoost? If this were a generalized linear models, then it would be straightforward to perform feature selection by just introducing L_1 or L_2 regularization. However, I'm not sure how to do this in a principled way when using nonlinear, non-additive models such as random forests &amp; XGBoost.",26,5
628,2019-7-11,2019,7,11,1,cbixag,"Deepmind Starcraft2 agent, Alphastar, will play random people on SC2 ladder",https://www.reddit.com/r/MachineLearning/comments/cbixag/deepmind_starcraft2_agent_alphastar_will_play/,flamingtominohead,1562776285,,0,1
629,2019-7-11,2019,7,11,1,cbj0j4,[Research] Scientists trained Word2vec algorithm on papers abstracts to predict thermoelectric materials,https://www.reddit.com/r/MachineLearning/comments/cbj0j4/research_scientists_trained_word2vec_algorithm_on/,melhaud,1562776734,"Have a nice day! I am not so experienced at ML, but at MS. So it is hard to say for me, but I suppose that future science is about great experimental skills or marvellous insights (in math or physics). Or it's just panic and my work is not so easily algorithmized? I think both practical science and ML will enforce each other. What are your thoughts? 

https://www.vice.com/amp/en_in/article/neagpb/ai-trained-on-old-scientific-papers-makes-discoveries-humans-missed",2,1
630,2019-7-11,2019,7,11,1,cbj2ud,[D] Multi-Single Object Keypoint Dataset,https://www.reddit.com/r/MachineLearning/comments/cbj2ud/d_multisingle_object_keypoint_dataset/,salihkaragoz,1562777040,"Hi,

I am looking for a keypoint datasets for objects. 

I know there are a couple of datasets for human, foot, and face. But, is there any dataset for objects that you know?

Human: [http://cocodataset.org/#keypoints-2018](http://cocodataset.org/#keypoints-2018)

Foot: [http://cocodataset.org/#keypoints-2018](http://cocodataset.org/#keypoints-2018)

Face: [https://towardsdatascience.com/facial-keypoint-detection-detect-relevant-features-of-face-in-a-go-using-cnn-your-own-dataset-e09cf359c2bc](https://towardsdatascience.com/facial-keypoint-detection-detect-relevant-features-of-face-in-a-go-using-cnn-your-own-dataset-e09cf359c2bc)

I just find the vehicle keypoint dataset;

Example Dataset: [http://www.cs.cmu.edu/\~mvo/index\_files/Papers/CarFusion.pdf](http://www.cs.cmu.edu/~mvo/index_files/Papers/CarFusion.pdf)

I am looking more of this type of dataset.

Thanks in advance...",2,1
631,2019-7-11,2019,7,11,1,cbja2v,[P] Optimizing your models parameters with GPOPY,https://www.reddit.com/r/MachineLearning/comments/cbja2v/p_optimizing_your_models_parameters_with_gpopy/,lucasecp,1562777983,"[https://github.com/domus123/gpopy](https://github.com/domus123/gpopy)

GPOPY is a tool that i'm creating that use genetic algorithm for running your ML models, mathematics functions or any other type of function and computate the best parameters for you, based on a score.

I'm just finishing writing the paper that will integrate GPOPY, and in the papers i'm adding more optimization algorithms to work with GA.

It's just an early version, but i already use it to optimize some parameters in the work and have an increase in accuracy.

Later in the future i'll add some charts and information.

ps: Since it's in early dev, may have some bugs and may be limited at moment.",2,2
632,2019-7-11,2019,7,11,2,cbjdrb,Are Commercial Labs Stealing Academias AI Thunder?,https://www.reddit.com/r/MachineLearning/comments/cbjdrb/are_commercial_labs_stealing_academias_ai_thunder/,Yuqing7,1562778446,,0,1
633,2019-7-11,2019,7,11,2,cbjk8e,[News] MLPerf Training v0.6 results released,https://www.reddit.com/r/MachineLearning/comments/cbjk8e/news_mlperf_training_v06_results_released/,riking27,1562779288,"Press release: &lt;https://mlperf.org/press#mlperf-training-v0.6-results&gt;

Results page: &lt;https://mlperf.org/training-results-0-6&gt;

Superficial analysis: Results come from Google's TPUs and NVIDIA GPUs for ResNet50, SSD w/ ResNet-34, Mask-R-CNN, NMT, and Transformer. Intel and Nvidia submitted results for the ""Mini Go"" reinforcement learning task, including a result from the upcoming Cascade Lake 9282. Alibaba also submitted a result in the ""Research"" category for ResNet-50 with custom software on 64 Tesla V100s.

I don't feel qualified to do detailed analysis of these results; looking forward to what people can see in these.",7,12
634,2019-7-11,2019,7,11,2,cbjm7c,[R] Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer,https://www.reddit.com/r/MachineLearning/comments/cbjm7c/r_towards_robust_monocular_depth_estimation/,downtownslim,1562779537,"The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a training objective that is invariant to changes in depth range and scale. Armed with this objective, we explore an abundant source of training data: 3D films. We demonstrate that despite pervasive inaccuracies, 3D films constitute a useful source of data that is complementary to existing training sets. We evaluate the presented approach on diverse datasets, focusing on zero-shot cross-dataset transfer: testing the generality of the learned model by evaluating it on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources yields improved depth estimates, particularly on previously unseen datasets.

&amp;#x200B;

Video: [https://www.youtube.com/watch?v=ITI0YS6IrUQ](https://www.youtube.com/watch?v=ITI0YS6IrUQ)

Paper: [https://arxiv.org/abs/1907.01341](https://arxiv.org/abs/1907.01341?fbclid=IwAR1B0cGoeVxGt18R9H4lg8PNUJu2spYiy-9iGNkAH48Y6i7u-tzVPYc-MMM)

Code: [https://github.com/intel-isl/MiDaS](https://github.com/intel-isl/MiDaS?fbclid=IwAR21IMybMSZ3kDVvP1LL4XRWYGDt5-NYDMxXH1YDKzKQS38IEJRW0zsfLWM)",2,3
635,2019-7-11,2019,7,11,2,cbjm8e,"[crosspost] Ali Farhadi, founder of Edge AI technologies &amp; Xnor.ai, is doing an AMA in r/homeautomation @10AM PST",https://www.reddit.com/r/MachineLearning/comments/cbjm8e/crosspost_ali_farhadi_founder_of_edge_ai/,InvisibleAgent,1562779542,,0,1
636,2019-7-11,2019,7,11,2,cbjmnm,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/cbjmnm/data_science_career_track_prep_course/,HannahHumphreys,1562779595,[removed],0,1
637,2019-7-11,2019,7,11,2,cbjsdy,[R] [1905.07799] Adaptive Attention Span in Transformers from Facebook which trains transformer using 8k characters context,https://www.reddit.com/r/MachineLearning/comments/cbjsdy/r_190507799_adaptive_attention_span_in/,bobchennan,1562780331,,4,22
638,2019-7-11,2019,7,11,2,cbk0cw,"Technical terms for learning from ""less data""?",https://www.reddit.com/r/MachineLearning/comments/cbk0cw/technical_terms_for_learning_from_less_data/,Spenhouet,1562781379,[removed],0,1
639,2019-7-11,2019,7,11,3,cbk5j4,[R] Ranking-Based Reward Extrapolation without Rankings,https://www.reddit.com/r/MachineLearning/comments/cbk5j4/r_rankingbased_reward_extrapolation_without/,strangecosmos,1562782018,,2,1
640,2019-7-11,2019,7,11,3,cbkakz,How to transform stock data for LSTM-based neural network,https://www.reddit.com/r/MachineLearning/comments/cbkakz/how_to_transform_stock_data_for_lstmbased_neural/,jdyr1729,1562782658,[removed],0,1
641,2019-7-11,2019,7,11,3,cbkbaz,[D] Most important papers in deep recommender systems?,https://www.reddit.com/r/MachineLearning/comments/cbkbaz/d_most_important_papers_in_deep_recommender/,EveningAlgae,1562782749,"I've been googling around and I can't find an up to date list for the most important papers to read for deep recommender systems. Does anyone have recommendations for what is considered to be the most important papers to read in this field? I want an overview and a general understanding of what is SoTA right now, and why it is (ie, how did the field build up to what it is now).",3,8
642,2019-7-11,2019,7,11,3,cbkehn,"Researchers Train Robots To Improve Their Utility By Grasping ""Adversarial Objects""",https://www.reddit.com/r/MachineLearning/comments/cbkehn/researchers_train_robots_to_improve_their_utility/,jonfla,1562783162,,0,1
643,2019-7-11,2019,7,11,3,cbkex7,[D] Deploying Deep Learning in Real-Time Video Chat,https://www.reddit.com/r/MachineLearning/comments/cbkex7/d_deploying_deep_learning_in_realtime_video_chat/,Juggling_Rick,1562783223,"I'm working on a video chat application that modifies each image in real-time with deep learning. I'm fairly new to backend web development and deploying models, so I've been struggling to find the most efficient way to do this. The options I've come up with are:

1. Run inference in the browser with TensorflowJs, and stream video with WebRTC. As long as TensorflowJs is fast enough, this seems to be the simplest solution. But I'm not sure if TensorflowJs running on a CPU will be able to run in real-time.
2. Run inference on a server. This would give me control over the hardware the model was being run on, so the performance would be more consistent. However, it wouldn't be P2P, which might slow things down, and make it harder to scale.

Is there another solution that I'm missing? So far I've been just trying everything to see what works, which is quite time-consuming. If you have some experience deploying deep learning models, please let me know what you would suggest.",0,0
644,2019-7-11,2019,7,11,4,cbkymi,[N]NERSC using deep learning to identify weather and climate patterns,https://www.reddit.com/r/MachineLearning/comments/cbkymi/nnersc_using_deep_learning_to_identify_weather/,greenprius,1562785693,,0,1
645,2019-7-11,2019,7,11,4,cbkz8f,Normalization of inputs question,https://www.reddit.com/r/MachineLearning/comments/cbkz8f/normalization_of_inputs_question/,Long3ye,1562785769,[removed],0,1
646,2019-7-11,2019,7,11,4,cblexh,What ML technique could I use for this problem?,https://www.reddit.com/r/MachineLearning/comments/cblexh/what_ml_technique_could_i_use_for_this_problem/,ema2159,1562787803,[removed],0,1
647,2019-7-11,2019,7,11,4,cbli4t,"Tried to find the appropriate model, but still confused after almost a week",https://www.reddit.com/r/MachineLearning/comments/cbli4t/tried_to_find_the_appropriate_model_but_still/,MLStudentThrowaway,1562788208,[removed],0,1
648,2019-7-11,2019,7,11,5,cblv31,Does anyone know of a library program which defeats face recognition but leaves the image recognizable by humans?,https://www.reddit.com/r/MachineLearning/comments/cblv31/does_anyone_know_of_a_library_program_which/,cho-kawaii,1562789887,[removed],0,1
649,2019-7-11,2019,7,11,5,cblx3w,Motor drive systems and communication; robotic arm. Slate TR2 video,https://www.reddit.com/r/MachineLearning/comments/cblx3w/motor_drive_systems_and_communication_robotic_arm/,FrankSchmidtTinyLabs,1562790162,[removed],0,1
650,2019-7-11,2019,7,11,5,cbm1y5,"I am creating a generator which loads random images into the train set and (different) random images into a validation set, but I am doing something unique (and it is working pretty well). I need to figure out how to use the predict generator while storing my validation class labels",https://www.reddit.com/r/MachineLearning/comments/cbm1y5/i_am_creating_a_generator_which_loads_random/,Alexanderdaawesome,1562790824,,0,1
651,2019-7-11,2019,7,11,5,cbm3o3,Crypto-Fueled Cloud learning?,https://www.reddit.com/r/MachineLearning/comments/cbm3o3/cryptofueled_cloud_learning/,Maergoth,1562791047,[removed],0,1
652,2019-7-11,2019,7,11,5,cbmc80,DeepTweets: Generating fake tweets with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cbmc80/deeptweets_generating_fake_tweets_with_neural/,jdv9,1562792192,,0,1
653,2019-7-11,2019,7,11,5,cbmczw,[P] Deploy models to AWS,https://www.reddit.com/r/MachineLearning/comments/cbmczw/p_deploy_models_to_aws/,ospillinger,1562792303,"Hi everyone,

I [posted here](https://www.reddit.com/r/MachineLearning/comments/av7o14/p_open_source_machine_learning_platform_for/) \~4 months ago about an open source ML platform Im working on. The main feedback I got from the community is to add support for more frameworks besides TensorFlow and to focus on making model deployment as simple as possible.

The latest version connects TF Serving, ONNX Runtime, and Flask to automatically deploy TensorFlow, PyTorch, XGBoost, and other models as web APIs. It also uses Docker and Kubernetes behind the scenes to autoscale endpoints, run rolling updates, and support CPU and GPU inference.

Your feedback was really useful to me last time so I'd love to hear your thoughts about this version.

[https://github.com/cortexlabs/cortex](https://github.com/cortexlabs/cortex)",0,3
654,2019-7-11,2019,7,11,6,cbmoov,StarCraft II Players Get the Chance to Play Against DeepMinds AlphaStar!,https://www.reddit.com/r/MachineLearning/comments/cbmoov/starcraft_ii_players_get_the_chance_to_play/,Yuqing7,1562793861,,0,1
655,2019-7-11,2019,7,11,6,cbmrgp,Understanding Meta-Learning: MetaOptNet,https://www.reddit.com/r/MachineLearning/comments/cbmrgp/understanding_metalearning_metaoptnet/,gebob19,1562794229,[removed],0,1
656,2019-7-11,2019,7,11,6,cbmstx,Suggestions for ML/DL projects to solve business problems,https://www.reddit.com/r/MachineLearning/comments/cbmstx/suggestions_for_mldl_projects_to_solve_business/,Best_Approximation,1562794409,[removed],0,1
657,2019-7-11,2019,7,11,6,cbmz84,[D] Brief introduction to Machine Learning with Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/cbmz84/d_brief_introduction_to_machine_learning_with/,buritomath,1562795274,"Hi there!

I just published my first article on ML, [https://bor0.wordpress.com/2019/07/10/brief-introduction-to-machine-learning-with-gradient-descent/](https://bor0.wordpress.com/2019/07/10/brief-introduction-to-machine-learning-with-gradient-descent/). Curious to hear your thoughts on it.

Hopefully someone else (other than me) finds it useful :)",0,0
658,2019-7-11,2019,7,11,7,cbnftu,[News] DeepMinds StarCraft II Agent AlphaStar Will Play Anonymously on Battle.net,https://www.reddit.com/r/MachineLearning/comments/cbnftu/news_deepminds_starcraft_ii_agent_alphastar_will/,AlphaHumanZero,1562797587,"[https://starcraft2.com/en-us/news/22933138](https://starcraft2.com/en-us/news/22933138)

The announcement is from the Starcraft 2 official page. AlphaStar will play as an anonymous player against some ladder players who opt in in this experiment in the European game servers.

Some highlights:

* AlphaStar can play anonymously as and against the three different races of the game: Protoss, Terran and Zerg in 1vs1 matches, in a non-disclosed future date. Their intention is that players treat AlphaStar as any other player.
* Replays will be used to publish a peer-reviewer paper.
* They restricted this version of AlphaStar to only interact with the information it gets from the game camera (I assume that this includes the minimap, and not the API from the January version?).
* They also increased the restrictions of AlphaStar actions-per-minute (APM), according to pro players advice. There is no additional info in the blog about how this restriction is taking place.

Personally, I see this as a very interesting experiment, although I'll like to know more details about the new restrictions that AlphaStar will be using, because as it was discussed here in January, such restrictions can be unfair to human players. What are your thoughts?",90,445
659,2019-7-11,2019,7,11,7,cbnsg4,[1906.00910] Learning Representations by Maximizing Mutual Information Across Views: 68% Top-1 accuracy on unsupervised ImageNet,https://www.reddit.com/r/MachineLearning/comments/cbnsg4/190600910_learning_representations_by_maximizing/,ankeshanand,1562799413,,7,24
660,2019-7-11,2019,7,11,8,cbnu7n,Advice on spoke language classification,https://www.reddit.com/r/MachineLearning/comments/cbnu7n/advice_on_spoke_language_classification/,nodas9990,1562799673,[removed],0,1
661,2019-7-11,2019,7,11,8,cbnzxr,[D] Non-text sequence tagging,https://www.reddit.com/r/MachineLearning/comments/cbnzxr/d_nontext_sequence_tagging/,andy_gnz,1562800510,"Is anyone aware of any recent papers investigating tagging of non-text sequences using CRF or RNN?

I can't think of any non-nlp applications so I don't know what to search for...",1,1
662,2019-7-11,2019,7,11,8,cbo9b9,[R] Invariant Risk Minimization,https://www.reddit.com/r/MachineLearning/comments/cbo9b9/r_invariant_risk_minimization/,baylearn,1562801871,,6,5
663,2019-7-11,2019,7,11,9,cbomnm,New Machine Learning Cheat Sheet by Emily Barry,https://www.reddit.com/r/MachineLearning/comments/cbomnm/new_machine_learning_cheat_sheet_by_emily_barry/,andrea_manero,1562803867,[removed],0,1
664,2019-7-11,2019,7,11,9,cboqq0,Python Open Cv Homography giving unexpected results,https://www.reddit.com/r/MachineLearning/comments/cboqq0/python_open_cv_homography_giving_unexpected/,sjking1880,1562804458,"Hi! I am doing homography in Python and am not getting expected Output. I am getting differen matrix

&amp;#x200B;

Here is my code's Google Colab link:-

[https://colab.research.google.com/drive/1OMWP4faY8dbNRAfryvBs7S7CEuM8oeDz](https://colab.research.google.com/drive/1OMWP4faY8dbNRAfryvBs7S7CEuM8oeDz)

&amp;#x200B;

Here is basic Homography Link (just for your reference):-

[https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)

&amp;#x200B;

&amp;#x200B;

In casw you are not able to open Colab. Here is source code.

    import cv2
    import numpy as np
     
    if __name__ == '__main__' :
     
    
        pts_src = np.array([[141.0, 131.0], [480.0, 159.0], [493.0, 630.0],[64.0, 601.0]])  
    
        pts_src=np.asarray(pts_src)                     #not needed actually
        pts_src = pts_src.astype('float32')              #not needed actually
    
       
        pts_dst = np.array([[318.0, 256.0],[534.0, 372.0],[316.0, 670.0],[73.0, 473.0]])
        pts_dst=np.asarray(pts_dst)                      #not needed actually
        pts_dst= pts_dst.astype('float32')               #not needed actually
    
        h, status = cv2.findHomography(pts_src, pts_dst,cv2.RANSAC, 5.0)
         
        # Wrap source image to destination based on homography
        
    
        print( h)
        print(len(h))
        
        print(""------------Printing H------"")
        
        print( status)
    
        print(""------Printing Status-------------"")
    
        print (status.ravel().tolist())
    
        print(""--------------------"")
            
    
        print(""Source is multiplied first"")
    
        #pts_src2 = np.array([[141.0, 131.0,1.0], [480.0, 159.0,1.0], [493.0, 630.0,1.0]])
        pts_dst2=np.array([[318.0, 256.0,1.0],[534.0, 372.0,1.0],[316.0, 670.0,1.0]]).transpose()
    
    
        print(""----see---"")
        print(+len(pts_dst2))
        pts_dst2=np.asarray(pts_dst2)                      #not needed actually
        pts_dst2= pts_dst2.astype('float32')               #not needed actually
        pts_dst2=np.asmatrix(pts_dst2)
    
    
        h=np.asmatrix(h)
    
    
        pts=np.dot( h, pts_dst2) 
        print(pts)
    
    
    print(""--------END-----------"")

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

Output:-

&amp;#x200B;

&amp;#x200B;

     [ 1.46491654e-01  4.41418278e-01  1.61369294e+02]
     [-3.62463336e-04 -9.14274844e-05  1.00000000e+00]]
    3
    ------------Printing H------
    [[1]
     [1]
     [1]
     [1]]
    ------Printing Status-------------
    [1, 1, 1, 1]
    --------------------
    Source is multiplied first
    ----see---
    3
    [[322.31218638 367.38950309 147.72051431]
     [320.95671889 403.80343648 503.41090271]
     [  0.86133122   0.77243355   0.82420517]]
    --------END-----------",0,1
665,2019-7-11,2019,7,11,9,cboza9,gradient boosting regression from scratch,https://www.reddit.com/r/MachineLearning/comments/cboza9/gradient_boosting_regression_from_scratch/,zelda_001,1562805766,,0,1
666,2019-7-11,2019,7,11,10,cbp6yj,Advancing Semi-supervised Learning with Unsupervised Data Augmentation,https://www.reddit.com/r/MachineLearning/comments/cbp6yj/advancing_semisupervised_learning_with/,sjoerdapp,1562806965,,0,1
667,2019-7-11,2019,7,11,10,cbpc5b,What would you consider to be the most cutting edge areas/applications of ML currently?,https://www.reddit.com/r/MachineLearning/comments/cbpc5b/what_would_you_consider_to_be_the_most_cutting/,darogisrebamcofoseco,1562807766,[removed],0,1
668,2019-7-11,2019,7,11,10,cbpg8h,Online Machine Learning Courses For Beginners in C/C++,https://www.reddit.com/r/MachineLearning/comments/cbpg8h/online_machine_learning_courses_for_beginners_in/,NuclearSteeze,1562808412,[removed],0,1
669,2019-7-11,2019,7,11,10,cbprd8,[Discussion] [Project] Buying/selling model as API,https://www.reddit.com/r/MachineLearning/comments/cbprd8/discussion_project_buyingselling_model_as_api/,MLtinkerer,1562810190,"Is there a website where people interested in buying/selling model as API can go to? 

Anyone here involved in this? (either demand or supply side)

&amp;#x200B;

ty",9,0
670,2019-7-11,2019,7,11,11,cbqaob,Help! H2o Neural Network code wont work anymore.,https://www.reddit.com/r/MachineLearning/comments/cbqaob/help_h2o_neural_network_code_wont_work_anymore/,mamaluigie,1562813365,[removed],0,1
671,2019-7-11,2019,7,11,11,cbqb5t,[Project] Creating flashcards that optimize learning,https://www.reddit.com/r/MachineLearning/comments/cbqb5t/project_creating_flashcards_that_optimize_learning/,I_Chizl_I,1562813448,"I'm just getting introduced to machine learning and I'm interested in focusing my studies around a project I'd like to complete one day. I thought it would be interesting to apply machine learning to flashcards to help optimize learning. For those familiar with Anki, it's essentially that concept but with machine learning instead of a fixed algorithm.

Since I'm new to the field I'm not quite sure where I should start. What topics do you suggest I focus on to learn how to get this started? Would you approach it with unsupervised reinforcement learning or in a different way?",7,0
672,2019-7-11,2019,7,11,11,cbqbw5,Getting Creative Collecting Data ~ How to obtain large amounts of data from Reddit,https://www.reddit.com/r/MachineLearning/comments/cbqbw5/getting_creative_collecting_data_how_to_obtain/,Fuerzacode,1562813581,[removed],0,1
673,2019-7-11,2019,7,11,12,cbqkou,How to determine metrics for success in new project?,https://www.reddit.com/r/MachineLearning/comments/cbqkou/how_to_determine_metrics_for_success_in_new/,lolwtfomgbbq7,1562815025,[removed],0,1
674,2019-7-11,2019,7,11,12,cbquop,Machine Learning as a Service (MLaaS) and its business implications in 2019,https://www.reddit.com/r/MachineLearning/comments/cbquop/machine_learning_as_a_service_mlaas_and_its/,craigbrownphd,1562816752,,0,1
675,2019-7-11,2019,7,11,12,cbqw2f,Been Kim from Google Brain talks about her research into creating algorithms that can explain why they make the recommendations they do via concepts that are relatable by their users.,https://www.reddit.com/r/MachineLearning/comments/cbqw2f/been_kim_from_google_brain_talks_about_her/,Science_Podcast,1562816991,,1,0
676,2019-7-11,2019,7,11,13,cbr300,Machine Learning Course in Pune,https://www.reddit.com/r/MachineLearning/comments/cbr300/machine_learning_course_in_pune/,dwivediabhinav,1562818186,,0,1
677,2019-7-11,2019,7,11,13,cbr3kv,"Algorithms: Not Evil, Helpful",https://www.reddit.com/r/MachineLearning/comments/cbr3kv/algorithms_not_evil_helpful/,craigbrownphd,1562818292,,0,1
678,2019-7-11,2019,7,11,13,cbr6gk,How to publish a CVPR Oral paper?,https://www.reddit.com/r/MachineLearning/comments/cbr6gk/how_to_publish_a_cvpr_oral_paper/,cs_none,1562818807,[removed],0,1
679,2019-7-11,2019,7,11,13,cbrass,Infographic: Can AI Think Ethically?,https://www.reddit.com/r/MachineLearning/comments/cbrass/infographic_can_ai_think_ethically/,craigbrownphd,1562819573,,0,1
680,2019-7-11,2019,7,11,13,cbrby6,Botupdate audio part 1,https://www.reddit.com/r/MachineLearning/comments/cbrby6/botupdate_audio_part_1/,botupdate,1562819794,,0,1
681,2019-7-11,2019,7,11,13,cbrhh3,Compressing VGG for Style Transfer,https://www.reddit.com/r/MachineLearning/comments/cbrhh3/compressing_vgg_for_style_transfer/,dstein64,1562820814,,0,1
682,2019-7-11,2019,7,11,14,cbrkor,Compressing VGG for Style Transfer,https://www.reddit.com/r/MachineLearning/comments/cbrkor/compressing_vgg_for_style_transfer/,dstein64,1562821428,,0,1
683,2019-7-11,2019,7,11,14,cbrll9,"[R][P] A website for hosting and exposing a model via (paid) API, especially for someone that just wants to plug and play",https://www.reddit.com/r/MachineLearning/comments/cbrll9/rp_a_website_for_hosting_and_exposing_a_model_via/,MLtinkerer,1562821596,"Is there any tool where I can do this? or find others who are?

&amp;#x200B;

ty",3,0
684,2019-7-11,2019,7,11,14,cbrm4k,[Free Course -Udemy] - Matplotlib Complete Tutorials |Machine Learning Pre-requisite,https://www.reddit.com/r/MachineLearning/comments/cbrm4k/free_course_udemy_matplotlib_complete_tutorials/,upworknepal,1562821696,[removed],0,1
685,2019-7-11,2019,7,11,14,cbruxa,Melanoma Diagnosis using deep learning.,https://www.reddit.com/r/MachineLearning/comments/cbruxa/melanoma_diagnosis_using_deep_learning/,ccchatterjee,1562823427,,0,1
686,2019-7-11,2019,7,11,14,cbrxfd,NVIDIA Jetson Nano Developer Kit,https://www.reddit.com/r/MachineLearning/comments/cbrxfd/nvidia_jetson_nano_developer_kit/,sbcshop,1562823908,,0,1
687,2019-7-11,2019,7,11,14,cbrxgd,Introduction to Facebook Artificial Intelligence Similarity Search,https://www.reddit.com/r/MachineLearning/comments/cbrxgd/introduction_to_facebook_artificial_intelligence/,lukerbs,1562823913,,0,1
688,2019-7-11,2019,7,11,14,cbrzxl,"I miss The Office, so I trained GPT-2 to generate new scripts.",https://www.reddit.com/r/MachineLearning/comments/cbrzxl/i_miss_the_office_so_i_trained_gpt2_to_generate/,hellohitesh,1562824399,"https://medium.com/@hellohitesh/i-miss-the-office-so-i-made-an-ai-write-me-new-scripts-c4a14af4dd86

I trained OpenAI's GPT-2 small on every line from every episode of The Office.",0,1
689,2019-7-11,2019,7,11,15,cbs356,How can I write big CSV data files fast on colab,https://www.reddit.com/r/MachineLearning/comments/cbs356/how_can_i_write_big_csv_data_files_fast_on_colab/,Aneo29,1562825031,[removed],0,1
690,2019-7-11,2019,7,11,15,cbs7m6,How important is Math knowledge for machine learning at industry level?,https://www.reddit.com/r/MachineLearning/comments/cbs7m6/how_important_is_math_knowledge_for_machine/,hauntedpoop,1562825897,[removed],0,1
691,2019-7-11,2019,7,11,15,cbsfs2,"What are the algorithms of animals like bees, fish, etc., or nature, such as the jungle of inspiration / idea?",https://www.reddit.com/r/MachineLearning/comments/cbsfs2/what_are_the_algorithms_of_animals_like_bees_fish/,Doctor_who1,1562827482,[removed],0,1
692,2019-7-11,2019,7,11,15,cbsiup,Computer vision: Endoscopy artifacts detection using yolo algorithm.EAD 2019 Challenge.,https://www.reddit.com/r/MachineLearning/comments/cbsiup/computer_vision_endoscopy_artifacts_detection/,harsh52,1562828125,[removed],0,1
693,2019-7-11,2019,7,11,17,cbt5s1,Help with implementing paper results,https://www.reddit.com/r/MachineLearning/comments/cbt5s1/help_with_implementing_paper_results/,LateSurd,1562833103,[removed],0,1
694,2019-7-11,2019,7,11,17,cbt6nt,Smart Data Visualization Works for All Business Users!,https://www.reddit.com/r/MachineLearning/comments/cbt6nt/smart_data_visualization_works_for_all_business/,ElegantMicroWebIndia,1562833304,,0,1
695,2019-7-11,2019,7,11,17,cbteck,cdQA  a software-suite for easy implementation of Question Answering systems,https://www.reddit.com/r/MachineLearning/comments/cbteck/cdqa_a_softwaresuite_for_easy_implementation_of/,crAAzyKKid,1562835160,"Hi,

&amp;#x200B;

With some colleagues I developed an end-to-end software suite for easy implementation and deployment of Question-Answering systems in Python. It uses a pipeline with classical Information Retrieval techniques and the Deep Learning model BERT.

[https://cdqa-suite.github.io/cdQA-website/](https://cdqa-suite.github.io/cdQA-website/)

[https://github.com/cdqa-suite/cdQA](https://github.com/cdqa-suite/cdQA)

&amp;#x200B;

We wrote an article in TDS explaining how to implement it easily: [https://towardsdatascience.com/how-to-create-your-own-question-answering-system-easily-with-python-2ef8abc8eb5](https://towardsdatascience.com/how-to-create-your-own-question-answering-system-easily-with-python-2ef8abc8eb5)

&amp;#x200B;

Don't hesitate to reach me out if you have questions, comments or ideas of improvement",0,1
696,2019-7-11,2019,7,11,18,cbtgxa,[P] cdQA  a software-suite for easy implementation of Question Answering systems,https://www.reddit.com/r/MachineLearning/comments/cbtgxa/p_cdqa_a_softwaresuite_for_easy_implementation_of/,crAAzyKKid,1562835758,"Hi,

&amp;#x200B;

With some colleagues I developed an end-to-end software suite for easy implementation and deployment of Question-Answering systems in Python. It uses a pipeline with classical Information Retrieval techniques and the Deep Learning model BERT.

[https://cdqa-suite.github.io/cdQA-website/](https://cdqa-suite.github.io/cdQA-website/)

[https://github.com/cdqa-suite/cdQA](https://github.com/cdqa-suite/cdQA)

&amp;#x200B;

We wrote an article in TDS explaining how to implement it easily: [https://towardsdatascience.com/how-to-create-your-own-question-answering-system-easily-with-python-2ef8abc8eb5](https://towardsdatascience.com/how-to-create-your-own-question-answering-system-easily-with-python-2ef8abc8eb5)

&amp;#x200B;

Don't hesitate to reach me out if you have questions, comments or ideas of improvement",10,3
697,2019-7-11,2019,7,11,18,cbtiik,Which university for my master's degree ?,https://www.reddit.com/r/MachineLearning/comments/cbtiik/which_university_for_my_masters_degree/,luxowl,1562836111,[removed],0,1
698,2019-7-11,2019,7,11,18,cbtk47,Check out this AI conference camera that automatically frames you in shot!,https://www.reddit.com/r/MachineLearning/comments/cbtk47/check_out_this_ai_conference_camera_that/,NecessaryDog1,1562836468,,0,1
699,2019-7-11,2019,7,11,18,cbtq9o,"[D] Using DVC for data projects - efficient versioning for inputs, intermediate files and algorithm models with no longer need to think about how to store data for collaboration",https://www.reddit.com/r/MachineLearning/comments/cbtq9o/d_using_dvc_for_data_projects_efficient/,thumbsdrivesmecrazy,1562837863,"In the following aritcle [Qonto](https://qonto.eu/) data team explains how [DVC](https://dvc.org/) helped them dealing with production data files such as trained machine learning algorithms and provided a reliable way of versioning those files along the project development: **[Using DVC to create an efficient version control system for data projects](https://medium.com/qonto-engineering/using-dvc-to-create-an-efficient-version-control-system-for-data-projects-96efd94355fe)**

DVC brought versioning for inputs, intermediate files and algorithm models and this drastically increased productivity by providing a  clean framework to manage data in an effortless way to split a project into atomic steps.

To make it more concrete, Quonto illustrated this article with a real project on VAT auto-detection from receipts - it consists in automatically retrieving the value-added tax amount from a receipt document in order to simplify accounting work.",0,3
700,2019-7-11,2019,7,11,18,cbts1m,5 Machine Learning Research Studies To Understand &amp; Predict Length of Stay in Hospitals,https://www.reddit.com/r/MachineLearning/comments/cbts1m/5_machine_learning_research_studies_to_understand/,andrea_manero,1562838247,[removed],0,1
701,2019-7-11,2019,7,11,18,cbttxh,Is it possible to make an AI which would train on AskReddit questions and then generate top questions ?,https://www.reddit.com/r/MachineLearning/comments/cbttxh/is_it_possible_to_make_an_ai_which_would_train_on/,Wander225,1562838699,[removed],0,1
702,2019-7-11,2019,7,11,19,cbtyvv,"PhD candidate commits suicide due to guilt, after his ML paper is 'falsely' accepted into the ISCA Conference.",https://www.reddit.com/r/MachineLearning/comments/cbtyvv/phd_candidate_commits_suicide_due_to_guilt_after/,athitham,1562839772,,0,1
703,2019-7-11,2019,7,11,19,cbu4ec,Using DVC to create an efficient version control system for data projects,https://www.reddit.com/r/MachineLearning/comments/cbu4ec/using_dvc_to_create_an_efficient_version_control/,raaaka,1562840940,,0,1
704,2019-7-11,2019,7,11,19,cbu720,Using Machine Learning for Screening-database where the management is setting filters for screening.People generally believe that more customers bring more revenue. But that is not always the case so screening is important using Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/cbu720/using_machine_learning_for_screeningdatabase/,adrianhrjunior,1562841484,,0,1
705,2019-7-11,2019,7,11,19,cbub2v,[D] Efficient GPU implementation of Empirical Fisher information matrix?,https://www.reddit.com/r/MachineLearning/comments/cbub2v/d_efficient_gpu_implementation_of_empirical/,phizaz,1562842330,"I have seen many implementations. It seems to be a limitation of autograd itself that we can compute the gradient of loglikelihood only **one sample at a time.** 

The batch version has been used but in a WRONG way. 

I have seen computing the gradient of **a batch of a loglikelihood** (essentially a mean of gradients), it doesn't seem to be truthful to the real Empirical Fisher calculation at all (only a kind of approximation).

Is there a correct GPU efficient impementation of Empricial Fisher out there?",3,1
706,2019-7-11,2019,7,11,20,cbuhg0,"What is the difference between AI, Deep Learning, Machine Learning and Natural Language Processing?",https://www.reddit.com/r/MachineLearning/comments/cbuhg0/what_is_the_difference_between_ai_deep_learning/,HearingLossHero,1562843587,[removed],0,1
707,2019-7-11,2019,7,11,20,cbuitr,Companies can use machine learning to screen customers for better knowing their cliets they are dealing with. Management is setting filters for screening customers using different machine learning techniques.,https://www.reddit.com/r/MachineLearning/comments/cbuitr/companies_can_use_machine_learning_to_screen/,OliverSmith6244,1562843849,,0,1
708,2019-7-11,2019,7,11,20,cbunxt,Goldman Sachs NY or MS in CS at UCSD,https://www.reddit.com/r/MachineLearning/comments/cbunxt/goldman_sachs_ny_or_ms_in_cs_at_ucsd/,dnmehta1998,1562844834,[removed],0,1
709,2019-7-11,2019,7,11,20,cbuqyn,ML method for a few hundred binary variables (different events) in a time series,https://www.reddit.com/r/MachineLearning/comments/cbuqyn/ml_method_for_a_few_hundred_binary_variables/,SebSnares,1562845375,[removed],0,1
710,2019-7-11,2019,7,11,21,cbuwh8,"Poland vs. Germany: 'Poland is calm &amp; peaceful. ... Here we have a video of a young man walking around &amp; smashing up the cars of German taxpayers. The women in the background moans about ""Mein Auto"" (My Car). Nobody does anything.'",https://www.reddit.com/r/MachineLearning/comments/cbuwh8/poland_vs_germany_poland_is_calm_peaceful_here_we/,molocher,1562846423,,0,1
711,2019-7-11,2019,7,11,21,cbv19t,[R] Machine Intelligence Conference Registration,https://www.reddit.com/r/MachineLearning/comments/cbv19t/r_machine_intelligence_conference_registration/,MICInc,1562847255,"We have a max capacity of \~170 people. Register for Machine Intelligence Conference 2019 to reserve a spot and swag. Priority will be given to students and student IDs will be checked at the door.

https://machineintelligence.cc/conference",0,0
712,2019-7-11,2019,7,11,21,cbv7ll,How would one use MFCC values in a CNN model?,https://www.reddit.com/r/MachineLearning/comments/cbv7ll/how_would_one_use_mfcc_values_in_a_cnn_model/,roundof1995,1562848401,[removed],0,1
713,2019-7-11,2019,7,11,21,cbvblq,[D] Tools for generating model architecture figures for publications/presentations?,https://www.reddit.com/r/MachineLearning/comments/cbvblq/d_tools_for_generating_model_architecture_figures/,DrLionelRaymond,1562849108,I've been searching for tools to help develop [figures similar to this] (https://media.arxiv-vanity.com/render-output/969749/figures/network-architecture.png) or [this](http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-12.05.40-PM.png). Any suggestions?,9,11
714,2019-7-11,2019,7,11,22,cbvsdp,A Noobs guide to Practical Machine Learning : Implementing Linear Regression Algorithm,https://www.reddit.com/r/MachineLearning/comments/cbvsdp/a_noobs_guide_to_practical_machine_learning/,hadiarajesh,1562852318,,0,1
715,2019-7-11,2019,7,11,22,cbvtck,Comprehensive Python Cheatsheet,https://www.reddit.com/r/MachineLearning/comments/cbvtck/comprehensive_python_cheatsheet/,pizzaburek,1562852522,,0,1
716,2019-7-11,2019,7,11,23,cbw327,[R] Variational Autoencoders and Nonlinear ICA: A Unifying Framework,https://www.reddit.com/r/MachineLearning/comments/cbw327/r_variational_autoencoders_and_nonlinear_ica_a/,hardmaru,1562854348,,11,58
717,2019-7-11,2019,7,11,23,cbwf30,The Best of AI: New Articles Published This Month (June 2019),https://www.reddit.com/r/MachineLearning/comments/cbwf30/the_best_of_ai_new_articles_published_this_month/,emnak,1562856451,,0,2
718,2019-7-11,2019,7,11,23,cbwh84,Would you guys like almost real world to train your self driving cars?,https://www.reddit.com/r/MachineLearning/comments/cbwh84/would_you_guys_like_almost_real_world_to_train/,uberuberubee,1562856759,[removed],0,1
719,2019-7-12,2019,7,12,0,cbwlf2,[D] The 5th Place Approach to the 2019 ACM Recsys Challenge by Team RosettaAI,https://www.reddit.com/r/MachineLearning/comments/cbwlf2/d_the_5th_place_approach_to_the_2019_acm_recsys/,steeveHuang,1562857338,"Just finished the writeup for our 5th place solution in the 2019 ACM RecSys Challenge! This blog post will talk about the datasets, the loss function, the Neural Networks architecture, and feature engineering. Hope you enjoy it :)

&amp;#x200B;

[https://blog.rosetta.ai/the-5th-place-approach-to-the-2019-acm-recsys-challenge-by-team-rosettaai-eb3c4e6178c4](https://blog.rosetta.ai/the-5th-place-approach-to-the-2019-acm-recsys-challenge-by-team-rosettaai-eb3c4e6178c4)",3,9
720,2019-7-12,2019,7,12,0,cbwn08,Tempered Image detection,https://www.reddit.com/r/MachineLearning/comments/cbwn08/tempered_image_detection/,kaswanclan,1562857537,[removed],0,1
721,2019-7-12,2019,7,12,0,cbx17d,How manage scheduling of multiple interviews in order to be able to negotiate simultaneously,https://www.reddit.com/r/MachineLearning/comments/cbx17d/how_manage_scheduling_of_multiple_interviews_in/,LaMasterShredder,1562859406,[removed],0,1
722,2019-7-12,2019,7,12,0,cbx6jn,[D] How to manage scheduling of multiple interviews in order to be able to negotiate simultaneously?,https://www.reddit.com/r/MachineLearning/comments/cbx6jn/d_how_to_manage_scheduling_of_multiple_interviews/,LaMasterShredder,1562860104,"I've seen some posts on this sub about people interviewing simultaneously at multiple companies, and then getting to simultaneously negotiate with all of them.

However, they never tell you how they got multiple companies with diversely different interview processes and timelines to schedule the interviews such that they all fall within the same timeframe, and as a result it enabled them to be able to get multiple offers at the same time, and negotiate their way up to the best salary.

I never seem to be able to optimize the interview process timeline. I end up with exploding offers from a couple of companies, while I'm still waiting on another company to arrange their 2nd interview with me. I have multiple questions:

1. How do you people do this?
2. When do you make your applications? How do you time it?
3. Do you deliberately not respond to a(n exploding) offer until you hear back from the other companies you're also interviewing with?
4. Do you have such a high risk tolerance that you'd jeopardize an offer being withdrawn while you're waiting around for the other companies to throw an offer at you?
5. How do you manage to do all this without coming across as rude, inconsiderate, unreliable, or uninterested?",12,0
723,2019-7-12,2019,7,12,0,cbx8ji,"ninja tricks tutorial 3in1 double, single 3 bit weaves, a reverse trans...",https://www.reddit.com/r/MachineLearning/comments/cbx8ji/ninja_tricks_tutorial_3in1_double_single_3_bit/,thetrickshotone,1562860362,,0,1
724,2019-7-12,2019,7,12,0,cbxb3k,Trying to get started with OpenAI Retro. Any good tutorial recommendations?,https://www.reddit.com/r/MachineLearning/comments/cbxb3k/trying_to_get_started_with_openai_retro_any_good/,ReasonablyBadass,1562860689,[removed],0,1
725,2019-7-12,2019,7,12,1,cbxw90,Some good machine learning courses,https://www.reddit.com/r/MachineLearning/comments/cbxw90/some_good_machine_learning_courses/,katy531,1562863427,,0,1
726,2019-7-12,2019,7,12,2,cby7bp,how to preprocess high dimensions numerical data ??,https://www.reddit.com/r/MachineLearning/comments/cby7bp/how_to_preprocess_high_dimensions_numerical_data/,kadhum_alrubaye,1562864848,,0,1
727,2019-7-12,2019,7,12,2,cbyk5t,NLP inference in floating-point?,https://www.reddit.com/r/MachineLearning/comments/cbyk5t/nlp_inference_in_floatingpoint/,CArchGuy,1562866416,[removed],0,1
728,2019-7-12,2019,7,12,2,cbyld1,"ninja tricks tutorial 3in1 double, single 3 bit weaves, a reverse trans...",https://www.reddit.com/r/MachineLearning/comments/cbyld1/ninja_tricks_tutorial_3in1_double_single_3_bit/,thetrickshotone,1562866559,,0,1
729,2019-7-12,2019,7,12,3,cbz7lg,"[R] Facebook, Carnegie Mellon build first AI that beats pros in 6-player poker",https://www.reddit.com/r/MachineLearning/comments/cbz7lg/r_facebook_carnegie_mellon_build_first_ai_that/,downtownslim,1562869391,"Pluribus is the first AI bot capable of beating human experts in six-player no-limit Holdem, the most widely-played poker format in the world. This is the first time an AI bot has beaten top human players in a complex game with more than two players or two teams.

&amp;#x200B;

Link: [https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/](https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/)",145,368
730,2019-7-12,2019,7,12,3,cbzboy,Need an interesting and useful research paper for project,https://www.reddit.com/r/MachineLearning/comments/cbzboy/need_an_interesting_and_useful_research_paper_for/,mike_tyson_25,1562869916,[removed],0,1
731,2019-7-12,2019,7,12,3,cbzlom,[N] Sparse Networks from Scratch: Faster Training without Losing Performance [pytorch lib],https://www.reddit.com/r/MachineLearning/comments/cbzlom/n_sparse_networks_from_scratch_faster_training/,phobrain,1562871188,,0,1
732,2019-7-12,2019,7,12,3,cbzops,AI to Save Snow Leopards,https://www.reddit.com/r/MachineLearning/comments/cbzops/ai_to_save_snow_leopards/,lomiag,1562871580,[removed],0,1
733,2019-7-12,2019,7,12,4,cc03ju,Research in explanaible artificial intelligence (XAI),https://www.reddit.com/r/MachineLearning/comments/cc03ju/research_in_explanaible_artificial_intelligence/,trenuss,1562873432,[removed],0,1
734,2019-7-12,2019,7,12,5,cc0i61,"State of Data Science, Engineering &amp; AI Report  2019",https://www.reddit.com/r/MachineLearning/comments/cc0i61/state_of_data_science_engineering_ai_report_2019/,craigbrownphd,1562875298,,0,1
735,2019-7-12,2019,7,12,5,cc0j6e,How to transfer private data? Find answer with Utopia,https://www.reddit.com/r/MachineLearning/comments/cc0j6e/how_to_transfer_private_data_find_answer_with/,Whimsicaloq,1562875423,[removed],0,1
736,2019-7-12,2019,7,12,5,cc0jsm,Machine learning and data,https://www.reddit.com/r/MachineLearning/comments/cc0jsm/machine_learning_and_data/,Magniminda,1562875504," 

*Before* we delve into the title topic, lets have a quick look at why **machine learning** cannot exist without data. **Machine learning** essentially refers to a large set of algorithms that can solve a certain set of problems, when trained properly. The models work best only when large amounts of data are available.

The more facets are covered by the data, the faster will the algorithms be able to learn and can fine-tune their predictive analyses. With an adequate amount of quality data available, **machine learning** techniques can easily outperform traditional approaches.",0,1
737,2019-7-12,2019,7,12,5,cc0omw,Making the jump from Mechanical Engineering. Advice/input?,https://www.reddit.com/r/MachineLearning/comments/cc0omw/making_the_jump_from_mechanical_engineering/,atb2x2,1562876120,[removed],0,1
738,2019-7-12,2019,7,12,5,cc0uxw,[D] should i over-sample rare episodes with successful exploration ?,https://www.reddit.com/r/MachineLearning/comments/cc0uxw/d_should_i_oversample_rare_episodes_with/,so_tiredso_tired,1562876958,"I am using DRL (mostly policy gradients) in a simulated discrete sokoban-style environment.

&amp;#x200B;

Alex-the-agent is rewarded for the shortest possible solution, as well as training on progressively harder/intricate maps. After a while, exploration is very difficult, and it takes millions of attempts to complete an episode with a slightly-better score. To be clear, this is not a plateauing of performance, it just takes excessively longer exploration.

&amp;#x200B;

Should i be ""over-sampling"" these increasing-rare successful score improvements ?

I use PG since it works, but I am open to trying value techniques.",1,3
739,2019-7-12,2019,7,12,5,cc0wiu,Google and NVIDIA Break MLPerf Records,https://www.reddit.com/r/MachineLearning/comments/cc0wiu/google_and_nvidia_break_mlperf_records/,Yuqing7,1562877162,,0,1
740,2019-7-12,2019,7,12,5,cc0y7s,[R] Sparse Networks from Scratch: Faster Training without Losing Performance,https://www.reddit.com/r/MachineLearning/comments/cc0y7s/r_sparse_networks_from_scratch_faster_training/,ofirpress,1562877380,,16,37
741,2019-7-12,2019,7,12,5,cc133h,"""It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts."" - Sir Arthur Conan Doyle",https://www.reddit.com/r/MachineLearning/comments/cc133h/it_is_a_capital_mistake_to_theorize_before_one/,SP_PR_ML,1562878009,[removed],0,1
742,2019-7-12,2019,7,12,5,cc15xt,"Soft Computing in Machine Learning By Sang-Yong Rhee, Jooyoung Park, Atsushi Inoue PDF",https://www.reddit.com/r/MachineLearning/comments/cc15xt/soft_computing_in_machine_learning_by_sangyong/,oussamaouti,1562878381,,0,1
743,2019-7-12,2019,7,12,5,cc16ir,[R] Striving for Simplicity in Off-policy Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/cc16ir/r_striving_for_simplicity_in_offpolicy_deep/,hardmaru,1562878458,,1,5
744,2019-7-12,2019,7,12,6,cc1vcj,Why did the AMAs stop?,https://www.reddit.com/r/MachineLearning/comments/cc1vcj/why_did_the_amas_stop/,ElFuhrerLoco,1562881776,It was my favorite part here. Lack of Mods? No interest of researchers?. I think there a lot of people here with close contacts with researchers/practitioners to have 1 quality AMA/month. Sorry if this has been answered before.,0,1
745,2019-7-12,2019,7,12,7,cc26qj,DeepMind AI is secretly lurking on the public StarCraft II 1v1 ladder,https://www.reddit.com/r/MachineLearning/comments/cc26qj/deepmind_ai_is_secretly_lurking_on_the_public/,Captainmanic,1562883333,,0,1
746,2019-7-12,2019,7,12,8,cc2sza,Why is tensorflow so slow?,https://www.reddit.com/r/MachineLearning/comments/cc2sza/why_is_tensorflow_so_slow/,DstnB3,1562886511,[removed],0,1
747,2019-7-12,2019,7,12,8,cc2t4w,Lottery Scratcher GAN network using tensorflow and pix2pix,https://www.reddit.com/r/MachineLearning/comments/cc2t4w/lottery_scratcher_gan_network_using_tensorflow/,skryshtafovychReal,1562886532,,0,1
748,2019-7-12,2019,7,12,8,cc2ydv,Vicarious?,https://www.reddit.com/r/MachineLearning/comments/cc2ydv/vicarious/,eeg_bert,1562887286,[removed],0,1
749,2019-7-12,2019,7,12,8,cc310q,An Absolute Monster Bluffer  Facebook &amp; CMU AI Bot Beats Poker Pros,https://www.reddit.com/r/MachineLearning/comments/cc310q/an_absolute_monster_bluffer_facebook_cmu_ai_bot/,Yuqing7,1562887666,,0,1
750,2019-7-12,2019,7,12,8,cc35co,Not bad after one epoch on the MNIST dataset,https://www.reddit.com/r/MachineLearning/comments/cc35co/not_bad_after_one_epoch_on_the_mnist_dataset/,Tolure,1562888315,"&amp;#x200B;

[After training a GAN on the MNIST dataset for a single epoch. I generated 3 sets of digits using 3 different noise inputs. These are the outputs for each digit.  ](https://i.redd.it/v137ggejer931.png)",0,1
751,2019-7-12,2019,7,12,9,cc3kc5,Best way to start machine learning with python?,https://www.reddit.com/r/MachineLearning/comments/cc3kc5/best_way_to_start_machine_learning_with_python/,Jakobro7,1562890546,"Hey there, I'm sure you guys get this question all the tike but i was wondering, what is the best way to start machine learning in python. I'm more of a Java/C++ man but i recently learned (I suppose the basics) of python in order to do machine learning. Now that ive finished there seems to be a lot more avenues than I expected. Should I start learning Tensorflow? Or something else entirely? Where is the best place for a true begineer in machine learning to start at?",0,1
752,2019-7-12,2019,7,12,10,cc48mh,[P] I built a video-based vehicle counting system  heres how,https://www.reddit.com/r/MachineLearning/comments/cc48mh/p_i_built_a_videobased_vehicle_counting_system/,nicholaskajoh,1562894341,"I shared a project of mine, a video-based vehicle counting system, on Twitter a while ago where it got a bit of attention/interest. I decided to write about how I built it on my blog: [https://alphacoder.xyz/vehicle-counting/](https://alphacoder.xyz/vehicle-counting/).

The source code is available on GitHub ([https://github.com/nicholaskajoh/Vehicle-Counting](https://github.com/nicholaskajoh/Vehicle-Counting)). Feedback and contributions are welcome! :)",1,5
753,2019-7-12,2019,7,12,12,cc5esv,My top 25 pandas tricks,https://www.reddit.com/r/MachineLearning/comments/cc5esv/my_top_25_pandas_tricks/,_quanttrader_,1562901205,,0,1
754,2019-7-12,2019,7,12,12,cc5h3j,[Seeking Advice ] To Become a Freelance Data Scientist,https://www.reddit.com/r/MachineLearning/comments/cc5h3j/seeking_advice_to_become_a_freelance_data/,immugunthan,1562901578,[removed],0,1
755,2019-7-12,2019,7,12,12,cc5rw6,[R] Adversarial Objects Against LiDAR-Based Autonomous Driving Systems,https://www.reddit.com/r/MachineLearning/comments/cc5rw6/r_adversarial_objects_against_lidarbased/,downtownslim,1562903419,,1,2
756,2019-7-12,2019,7,12,13,cc5zvq,Categorical Data,https://www.reddit.com/r/MachineLearning/comments/cc5zvq/categorical_data/,PsychedelicBrotha,1562904798,"Hi all,

how do you correctly use 'ColumnTransformer' as a replacement for 'categorical\_features' since its deprecated? 

See my code below of how i usually transform categorical data

\# Categorical Data

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

labelencoder\_X1 = LabelEncoder()

X\[:, 1\] = labelencoder\_X1.fit\_transform(X\[:, 1\])

labelencoder\_X2 = LabelEncoder()

X\[:, 2\] = labelencoder\_X2.fit\_transform(X\[:, 2\])

onehotencoder = OneHotEncoder(categorical\_features = \[1\])

X = onehotencoder.fit\_transform(X).toarray()

X = X\[:, 1:\]",0,1
757,2019-7-12,2019,7,12,13,cc6aqd,Best Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/cc6aqd/best_machine_learning_course/,dwivediabhinav,1562906767,,0,1
758,2019-7-12,2019,7,12,13,cc6dsl,Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/cc6dsl/artificial_intelligence/,Thomasfvl,1562907338,,0,1
759,2019-7-12,2019,7,12,14,cc6mbf,Is there any algorithm to determine whether the image has vignetting or not?,https://www.reddit.com/r/MachineLearning/comments/cc6mbf/is_there_any_algorithm_to_determine_whether_the/,textssg,1562908909,[removed],0,1
760,2019-7-12,2019,7,12,14,cc6np4,Volumetric Filling Machines Market Future Forecast 20192023,https://www.reddit.com/r/MachineLearning/comments/cc6np4/volumetric_filling_machines_market_future/,jadhavni3,1562909171,[removed],1,1
761,2019-7-12,2019,7,12,14,cc6s5e,Wall Hanging Type Hose Boxes Market Future Forecast 20192023,https://www.reddit.com/r/MachineLearning/comments/cc6s5e/wall_hanging_type_hose_boxes_market_future/,jadhavni3,1562910011,[removed],1,1
762,2019-7-12,2019,7,12,15,cc76us,[D] Are there any legal issues with training machine learning models on copyrighted content?,https://www.reddit.com/r/MachineLearning/comments/cc76us/d_are_there_any_legal_issues_with_training/,PlusImagination,1562912823,"Say that I purchased some books/movies/research papers/whatever, and used that data to train machine learning models, and then made some money off those models. Are there any legal issues to this? 

What if the model generated content that was very similar to the copyrighted content? Any legal issues there?",76,122
763,2019-7-12,2019,7,12,15,cc7dfi,[D] Dislike/Disinterest of Machine Learning: rant,https://www.reddit.com/r/MachineLearning/comments/cc7dfi/d_dislikedisinterest_of_machine_learning_rant/,random__0,1562914099,"This is just my personal opinion/rant of what I inherently dislike about ML and neural networks after working on a research project involving a CNN for little under a year and feeling very frustrated with where it is going. Not claiming to be an expert or professional in the field.

&amp;#x200B;

**#1: Machine learning is frustrating to code/improve on:**

&amp;#x200B;

I find the coding of machine learning for a neural network (at least at my level when you are only really applying already existing layers/activation functions) to be very unsatisfying to progress through due to the nature of it. When you are solving a normal coding problem I feel like you can get creative with dealing with problems/issues that occur with your algorithm, as you progress through solving the problem (In my case getting high classification accuracy), but the solution offered by machine learning tends to be a sort of black box where it simply works at a certain percent for whatever rhyme or reason. I feel like the time I put into improving my classification accuracy is not proportional to the rate at which it improves and is more factored by chance than anything. For example I spent weeks changing the shape and setup of the input data, the number of nodes in each layer, etc with little to no improvement, but one day I switched the activation functions on my dense layers from relu to selu, and by literally changing 4 letters, my accuracy jumped past everything else that I had tried the last few weeks. This could just be due to my lack of experience in the field, i.e. a talented ML engineer could just tell based the loss function values that selu activation function will lead to improvement. I dislike how when I make a change to the net I don't really know how exactly that will translate to the result that I get. I could predict an improvement will happen, but I can't say for certain or really know to what degree an improvement will occur. This gets more frustrating when you have something that needs a lot of epochs to progress through that takes a long time, preventing you from just trying your more long shot ideas that might work but if they don't will just waste a lot of time with nothing gained.

&amp;#x200B;

**#2: Reliance on data set:**  

&amp;#x200B;

This could just be a complaint about data science in general, I don't really know enough about the field to know where the distinction between the two is. I dislike how the prerequisite to solving problems using machine learning is to have a solid dataset that may be inherently biased in a way you can't predict. An analogy that comes to mind when I think about this is studying for a test. The non ML way to do it would be to build up an understanding of the material from the ground up that is solid enough to solve any problem thrown at you. The ML way to do it would be to gather up old tests that the professor gave in previous years and use those to study for the test. Sure you might do better on the test in this way, but if a curve ball question ends up on the test it is more likely for the non ML way to be able to solve the question. Even if they don't solve the question they can see where their logic in solving it was flawed, whereas in the ML case, the flawed logic was that the previous tests didn't have a question similar to this one. Going off of this if your dataset has an inherent bias or is too small then your solution will share that bias and won't be trustworthy in a scenario that hasn't been seen before but still needs to be dealt with. Now I know that there are methods to help with having a smaller dataset and preventing overfitting such as dropout layers and transfer learning, but I still have an underlying issue with needing to rely on this large dataset for solving your problem, especially in industries where the dataset isn't typically knowledge given to the public.

&amp;#x200B;

**#3 Disconnect between People suggesting ML learning solutions to problems and actual ML engineers who have to implement solutions:**

&amp;#x200B;

**#4 Constant research needed in newest techniques:**

&amp;#x200B;

Now some people might like this aspect of ML, but I dislike how you constantly need to be learning about the newest trends in ML in order to stay relevant. It seems like the things that I learn this year will become almost completely irrelevant next year i.e. RNNs were thought to be very good for word processing until they found that CNNs were better suited for it. Now this occurs in all industries obliviously but I feel like it is especially true in ML, where you aren't just designing a system that will solve a problem, you are also designing a system to find the correct weights for said system, so I feel like there is a higher chance for something that you learned about and specialize in to one day become completely irrelevant and you need to now learn this unrelated new idea that will only last for so long.

&amp;#x200B;

**#5 Paywall in ML:**

&amp;#x200B;

To even consider an ML solution to a problem you need two things: a good dataset and a lot of computing power. Just having more computing power allows you to train and test your implementations at a much faster rate than someone without those resources. It ultimately comes down to the solution is inherently tied to having a large amount of money to put down before hand to solve the problem before seeing any real proof of concept (this could be wrong I am not very familiar with if there is a proof of concept somewhere in the ML life cycle). While I recognize that in all industries money always allows for better/quicker development, my problem arises when you consider someone learning ML from the ground up. If you have more money to spend on getting a better rig to learn ML, your shitty mnist dataset classifier will train significantly faster and allow you to learn significantly faster than someone with less money, which I think is an inherent problem of the method. Talking about datasets, paying for datasets, and how companies take your data to be used in datasets for ML is a completely different rant so I'm just going to end it here.

&amp;#x200B;

Let me know if you disagree/agree with my points and if the dislike that I feel about working with ML will go away with more of an understanding of the data science behind it.",63,94
764,2019-7-12,2019,7,12,16,cc7nci,PhD in Statistical Machine Learning or Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/cc7nci/phd_in_statistical_machine_learning_or_deep/,keitaro245,1562916010,"Hi. I am a soon-to-be ML PhD student. I have the option to choose between joining a theoretical ML group or a Deep Learning group (with application to CV/NLP). The work being done by the theoretical ML group is quite rigorous and basically about proving theorems all day with not much coding (example topics can be inference in graphical model, sample complexity, PAC learning, ...). On the other hand, Deep Learning is so hot nowadays with so many cool applications. Could anyone give me some advice on which group I should join? I am leaning toward the theoretical ML group because I feel like I can still make a switch to Deep Learning later on if I want to.",0,1
765,2019-7-12,2019,7,12,16,cc7qc9,[D] PhD in theoretical ML or Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/cc7qc9/d_phd_in_theoretical_ml_or_deep_learning/,keitaro245,1562916615,"Hi. I am a soon-to-be ML PhD student. I have the option to choose between joining a theoretical ML group or a Deep Learning group (with application to CV/NLP). The work being done by the theoretical ML group is quite rigorous and basically about proving theorems all day with not much coding (example topics can be inference in graphical model, sample complexity, PAC learning, ...). On the other hand, Deep Learning is so hot nowadays with so many cool applications (that also means more job/internship opportunities). Could anyone give me some advice on which group I should join? I am leaning toward the theoretical ML group because I feel like I can still make a switch to Deep Learning later on if I want to.",14,7
766,2019-7-12,2019,7,12,17,cc80og,Improving FinTech Services with Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/cc80og/improving_fintech_services_with_artificial/,Verma_RJ,1562918885,,0,1
767,2019-7-12,2019,7,12,17,cc82dw,[R] Any relevant papers about Meta-Learning or Active Learning in ICML or CVPR 2019?,https://www.reddit.com/r/MachineLearning/comments/cc82dw/r_any_relevant_papers_about_metalearning_or/,MrLeylo,1562919289,"Hi r/MachineLearning,

I have a clear background on Meta-Learning relevant works (families of Meta-Learning approaches) and on Active Learning methods (depending on its heuristic, scenario...). However, there are many papers on those topics from CVPR and ICML and I don't know how to filter them.

As far as I have researched, I have read the papers [*Learning loss for Active Learning*](https://arxiv.org/pdf/1905.03677.pdf) from Yoo and [*Online Meta-Learning*](https://arxiv.org/abs/1902.08438) from Chelsea Finn (the author of MAML and many other important works in Meta-Learning).

Does anybody have any clue on where to begin?

Thank you in advanced!",5,3
768,2019-7-12,2019,7,12,17,cc8527,Name - Ethnicity classifier,https://www.reddit.com/r/MachineLearning/comments/cc8527/name_ethnicity_classifier/,davidnea3,1562919930,[removed],0,1
769,2019-7-12,2019,7,12,17,cc8561,[R] Using DVC to create an efficient version control system for data projects,https://www.reddit.com/r/MachineLearning/comments/cc8561/r_using_dvc_to_create_an_efficient_version/,thumbsdrivesmecrazy,1562919953,"The following article shows how DVC (Data Version Control) tool helped a fintech data team in dealing with production data files such as trained machine learning algorithms and provided a reliable way of versioning those files along the project development: **[Using DVC to create an efficient version control system for data projects](https://medium.com/qonto-engineering/using-dvc-to-create-an-efficient-version-control-system-for-data-projects-96efd94355fe)**

The tool brought versioning for inputs, intermediate files and algorithm models and this drastically increased productivity by providing a  clean framework to manage data in an effortless way to split a project into atomic steps.

To make it more concrete, the article illustrated with a real project on VAT auto-detection from receipts - it consists in automatically retrieving the value-added tax amount from a receipt document in order to simplify accounting work.",0,4
770,2019-7-12,2019,7,12,17,cc87wg,"SecOps - The Good, The Bad and The Automated: SANS 2019 SOC Survey",https://www.reddit.com/r/MachineLearning/comments/cc87wg/secops_the_good_the_bad_and_the_automated_sans/,MariaMiladinovikj,1562920585,,0,1
771,2019-7-12,2019,7,12,17,cc89hk,Blog - Organizing Documents - Challenges &amp; Difficulties,https://www.reddit.com/r/MachineLearning/comments/cc89hk/blog_organizing_documents_challenges_difficulties/,Sorted_AI,1562920968,,0,1
772,2019-7-12,2019,7,12,18,cc8j26,How Artificial Intelligence reshaping Marketing Industry?,https://www.reddit.com/r/MachineLearning/comments/cc8j26/how_artificial_intelligence_reshaping_marketing/,Akshays41,1562923001,[removed],0,1
773,2019-7-12,2019,7,12,18,cc8jic,Problem understanding Matrix Factorization and various Recomender System Algorithms,https://www.reddit.com/r/MachineLearning/comments/cc8jic/problem_understanding_matrix_factorization_and/,NecroDeity,1562923092,[removed],0,1
774,2019-7-12,2019,7,12,18,cc8k0b,[D] what techniques/methods can be used to assign probabilities to sequences of measurements where each measurements comes with a confidence/probability?,https://www.reddit.com/r/MachineLearning/comments/cc8k0b/d_what_techniquesmethods_can_be_used_to_assign/,DeepDeeperRIPgradien,1562923178,"Consider I have a lot of measurements, some of them are real, some of them are noise. I can build a hypothesis by combining measuremens to a sequence. There are a lot of possible hypotheses considering that two sequences can combine a different subset of all measurements (which means two sequences of measurements can have different amount of measurements). Each measurement also comes with a probability. 

My question is: what would be a good/proper way of finding the best/most likely hypothesis here?

Example: imagine the sequence of measurements with probabilities [0.8, 0.8, 0.8] and [0.9, 0.95]. Which of these 2 hypotheses would you pick over the other?",2,13
775,2019-7-12,2019,7,12,18,cc8lrz,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/cc8lrz/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1562923540,[removed],0,1
776,2019-7-12,2019,7,12,18,cc8oji,[D] Effectiveness of pruning,https://www.reddit.com/r/MachineLearning/comments/cc8oji/d_effectiveness_of_pruning/,dramanautica,1562924095,Ive heard anecdotally that pruning is generally ineffective technique and other techniques like transfer learning being far superior. Are there any papers exploring why pruning might not perform as well as expected?,8,4
777,2019-7-12,2019,7,12,19,cc8zmy,"LSTM Time Series problem, Loss became NaN. Why?",https://www.reddit.com/r/MachineLearning/comments/cc8zmy/lstm_time_series_problem_loss_became_nan_why/,mrqwerty91,1562926309,"Hi everyone,

i'm working for predictive maintenance with a large dataset of 75000 rows and 18 feature/columns --&gt; \[75000, 18\]

Each row rapresents signals obrtaines at time t0, t2, etc.

I would like to create an algorithm for prediction and i think LSTM is the way.

I work with Keras library.

&amp;#x200B;

I create 2 array:

X = (74472, 500, 18)

y = (74472, 100, 18)

where shape\[0\] rapresents all the samples, shape\[1\] rapresents the input of the neural network (500) and the output/prediction (100). Shape\[2\] are number of features.

&amp;#x200B;

So, i try to create a simple Neural Network with Keras:

n\_steps\_in  = 500  
n\_steps\_out = 100  
n\_epochs    = 10

model = Sequential()  
model.add(LSTM(200, activation=**'relu'**, input\_shape=(n\_steps\_in, n\_features), return\_sequences=**True**))  
model.add(LSTM(100))  
model.add((Dense(n\_steps\_out\*n\_features)))  
model.add(Reshape((n\_steps\_out, n\_features)))  
model.compile(loss=**'mse'**, optimizer=adam\_mine, metrics=\[**'accuracy'**\]) *#, metrics=\['acc'\])    #metrics aggiunta*  
model.fit(X, y, epochs=n\_epochs, verbose=1, validation\_split=0.25, batch\_size=256)

&amp;#x200B;

The question is: why with some configuration (like some batch\_size) i've got loss = nan?",0,1
778,2019-7-12,2019,7,12,19,cc92nr,Spell checking state of the art,https://www.reddit.com/r/MachineLearning/comments/cc92nr/spell_checking_state_of_the_art/,wshm,1562926904,What's the state of the art for spell checking ?,0,1
779,2019-7-12,2019,7,12,19,cc93bs,Runway ML - Machine learning for creators,https://www.reddit.com/r/MachineLearning/comments/cc93bs/runway_ml_machine_learning_for_creators/,alshell7,1562927051,"Runway ML aims to make the process of integrating AI into your workflow easier for those who cant code, by providing artists, designers, filmmakers, and others with an app store of machine learning applications that can be activated with a few clicks!

https://runwayml.com",0,1
780,2019-7-12,2019,7,12,19,cc947s,ARTIFICIAL INTELLIGENCE SOFTWARE DEVELOPMENT SERVICE|ARTIFICIAL INTELLIGENCE DEVELOPMENT SERVICE,https://www.reddit.com/r/MachineLearning/comments/cc947s/artificial_intelligence_software_development/,clarke2106,1562927239,[removed],0,1
781,2019-7-12,2019,7,12,19,cc97oa,Economics of ML on Amazon AWS SageMaker - to GPU or not to GPU?,https://www.reddit.com/r/MachineLearning/comments/cc97oa/economics_of_ml_on_amazon_aws_sagemaker_to_gpu_or/,oscarandjo,1562927943,[removed],0,1
782,2019-7-12,2019,7,12,20,cc9wwd,[D] Is it possible to do supervised learning when the labels are relative?,https://www.reddit.com/r/MachineLearning/comments/cc9wwd/d_is_it_possible_to_do_supervised_learning_when/,TrickyKnight77,1562932632,"I'm doing job matching and I have a dataset consisting of info like ""for job A, candidate #1 is better than candidate #2"", and of course some features for the job and for the candidates. 

I would like to train a model to output a score of how fit is a candidate for a job. So far I haven't been able to come up with a loss function, but my intuition says that there should be enough information to build one, provided any two candidates from the dataset are linked though job applications and other candidates (which they are).

Am I wrong? Any ideas?",12,1
783,2019-7-12,2019,7,12,21,cca030,"Gripper Design: video, See Machine Learning Playground channel on YouTube also....",https://www.reddit.com/r/MachineLearning/comments/cca030/gripper_design_video_see_machine_learning/,FrankSchmidtTinyLabs,1562933161,,0,1
784,2019-7-12,2019,7,12,21,cca2m5,"""Deep Learning and Inverse Problems"" Autumn School in Bremen",https://www.reddit.com/r/MachineLearning/comments/cca2m5/deep_learning_and_inverse_problems_autumn_school/,cetmann,1562933603,[removed],0,1
785,2019-7-12,2019,7,12,21,cca68i,[N] 'Deep Learning and Inverse Problems' Autumn School in Bremen (Germany),https://www.reddit.com/r/MachineLearning/comments/cca68i/n_deep_learning_and_inverse_problems_autumn/,cetmann,1562934205,"Flyer:  [http://www.math.uni-bremen.de/zetem/alt/optimmedia/cms/dlip19/dlip\_flyer.jpg](http://www.math.uni-bremen.de/zetem/alt/optimmedia/cms/dlip19/dlip_flyer.jpg)

&amp;#x200B;

Recent advances in deep learning have had an increasing impact on the field of inverse problems. In order to pass this knowledge on, the Center for Industrial Mathematics at the University of Bremen hosts an 'autumn school' between the 4th and 8th of November 2019. In this autumn school, world-renowned experts of the field will be teaching the mathematical foundations of machine learning and deep learning and how these can be applied to a wide range of inverse problems. The autumn school targets young researchers and advanced students in the field of  inverse problems.

The registration is now open at [www.zetem.uni-bremen.de/dlip19](http://www.zetem.uni-bremen.de/dlip19) until August 15th, 2019.

&amp;#x200B;

*Lecturers:*

Asja Fischer (Ruhr University Bochum)  
Carola-Bibiane Schnlieb (University of Cambridge)  
Markus Haltmeier (University of Innsbruck)  
Martin Benning (Queen Mary University of London)  
Matthias Bethge (Max Planck Institute, Tbingen)  
Nihat Ay (Max Planck Institute, Leipzig)  
Ozan ktem (KTH Stockholm)  
Simon Arridge (University College London)

&amp;#x200B;

*A tentative list of topics includes:*

Mathematical Foundations of Machine Learning  
Introduction to Deep Neural Networks  
Current Computational and Theoretical Questions in Deep Learning  
Overview of Inverse Problems  
Solving Inverse Problems via Learned Iteration Schemes  
Learning Regularizers via Deep Networks  
Deep Learning for Inverse Problems arising in Medical Imaging",4,32
786,2019-7-12,2019,7,12,21,cca9uk,How do I upscale videos using ESRGAN?,https://www.reddit.com/r/MachineLearning/comments/cca9uk/how_do_i_upscale_videos_using_esrgan/,chickensauce843,1562934846,[removed],0,1
787,2019-7-12,2019,7,12,21,ccace5,Review : Khaas VIM3 - Mini supercomputer with 5TOPS NPU,https://www.reddit.com/r/MachineLearning/comments/ccace5/review_khaas_vim3_mini_supercomputer_with_5tops/,NicoD-SBC,1562935266,,0,1
788,2019-7-12,2019,7,12,21,ccafvv,What is bottoms up approach for building machine learning model?,https://www.reddit.com/r/MachineLearning/comments/ccafvv/what_is_bottoms_up_approach_for_building_machine/,nik_bhintade,1562935829,"I was watching SAO (anime). There one character in that series is talking about building AI model using bottoms up approach.
I don't know something like this is true or not?
Is this concept real or just a made up story? If it's real how to go about that?",0,1
789,2019-7-12,2019,7,12,22,ccan4c,Which Ryan Is It? (FastAI x ML),https://www.reddit.com/r/MachineLearning/comments/ccan4c/which_ryan_is_it_fastai_x_ml/,banknum,1562936967,,0,1
790,2019-7-12,2019,7,12,22,ccatt5,[R] Large Memory Layers with Product Keys,https://www.reddit.com/r/MachineLearning/comments/ccatt5/r_large_memory_layers_with_product_keys/,yzyy,1562937970,,4,20
791,2019-7-12,2019,7,12,23,ccbbdw,"[D] Transforming the target variable, a bad idea ?",https://www.reddit.com/r/MachineLearning/comments/ccbbdw/d_transforming_the_target_variable_a_bad_idea/,lazywiing,1562940537,"Hi Reddit,

&amp;#x200B;

I am currently working on a project and I would like to hear your ideas about it. Basically, the problem consists in predicting the length of an event (target variable is T = d2 - d1). The vast majority of lengths are short (less than 10 days), but some are really big.

&amp;#x200B;

It is more of a problem if we make a mistake for short events than for long ones (ex: predicting 10 days instead of 8 is more problematic than predicting 30 instead of 45). I wanted to transform the target variable, using for instance a logarithmic function.

&amp;#x200B;

 The problem with predicting **log(T)** is that an estimate will be **E\[log(T) | X\]**. When taking the exp function, my estimate for **T** will be **exp(E\[log(T) | X\])**. As exp is convex, by the Jensen inequality, I will in fact underestimate **T**, which is in my case something I want to avoid.

&amp;#x200B;

I see many people transforming their target variable but I don't really know if most of them care about the problems that may occur with it. Are there any common techniques that you are aware of to handle this issue ? Or could you suggest any other approach that would fit my needs ?

&amp;#x200B;

Thank you very much and have a nice day !

&amp;#x200B;

(PS: At least, the good thing is that I don't have any censored data)",14,20
792,2019-7-12,2019,7,12,23,ccbvzs,Open source mechine learning platform,https://www.reddit.com/r/MachineLearning/comments/ccbvzs/open_source_mechine_learning_platform/,webdev-online,1562943372,,0,0
793,2019-7-12,2019,7,12,23,ccbw9z,"[D] Can we predict when Machine Translation will be ""good""?",https://www.reddit.com/r/MachineLearning/comments/ccbw9z/d_can_we_predict_when_machine_translation_will_be/,sw1tchbl4d3r,1562943411,"Hello there,

Right now Machine Translations aren't good. There are exceptions, but languages like Japanese are not readable after they got translated to English by a machine. What I want to know is if we can predict when it will master grammar etc. from past data. 

Thanks.",2,0
794,2019-7-13,2019,7,13,0,ccc06o,Big Data Analysis with Scala and Spark,https://www.reddit.com/r/MachineLearning/comments/ccc06o/big_data_analysis_with_scala_and_spark/,HannahHumphreys,1562943908,[removed],0,1
795,2019-7-13,2019,7,13,0,ccc6lj,[P] Looking to Hire Someone to Assist in Recreating this Research Paper,https://www.reddit.com/r/MachineLearning/comments/ccc6lj/p_looking_to_hire_someone_to_assist_in_recreating/,randomwalk84,1562944710,"I hope this an OK place to post this and please let me know if ti's not but I'm looking to hire someone who is able to recreate a research paper I'm reading in Python. I'm not really interested in the actual set up of the ANN model as I am in the wavelet transform and PCA portion that they did in the per-processing step. It would be good if whoever was hired had a deep understanding of that space and was able to implement tweaks and changes if necessary. 

The paper is here:

[https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0156338#pone.0156338.s012](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0156338#pone.0156338.s012)

&amp;#x200B;

I should note that I know this is a financial paper but please don't mistake this as someone who sees big returns and is just trying to hire someone to rebuild their project to capitalize on it. I don't think the returns cited in this paper are realistic and I have no intentions of trying to trade it or use it to make money. It's more about my own edification. I dont have a high level math background and Ive spent hours reading about wavelet transforms and it still seems a bit daunting. 

&amp;#x200B;

If you're interested in talking further, please shoot me a message and we can go from there.",8,3
796,2019-7-13,2019,7,13,0,ccc7p4,"[R] ""We also 3D-print our adversarial objects and perform physical experiments to illustrate that such vulnerability exists in the real world"" - Adversarial Objects Against LiDAR",https://www.reddit.com/r/MachineLearning/comments/ccc7p4/r_we_also_3dprint_our_adversarial_objects_and/,downtownslim,1562944855,"Abstract:

&gt;Deep neural networks (DNNs) are found to be vulnerable against adversarial examples, which are carefully crafted inputs with a small magnitude of perturbation aiming to induce arbitrarily incorrect predictions. Recent studies show that adversarial examples can pose a threat to real-world security-critical applications: a ""physical adversarial Stop Sign"" can be synthesized such that the autonomous driving cars will misrecognize it as others (e.g., a speed limit sign). However, these image-space adversarial examples cannot easily alter 3D scans of widely equipped LiDAR or radar on autonomous vehicles. In this paper, we reveal the potential vulnerabilities of LiDAR-based autonomous driving detection systems, by proposing an optimization based approach LiDAR-Adv to generate adversarial objects that can evade the LiDAR-based detection system under various conditions. We first show the vulnerabilities using a blackbox evolution-based algorithm, and then explore how much a strong adversary can do, using our gradient-based approach LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo autonomous driving platform and show that such physical systems are indeed vulnerable to the proposed attacks. We also 3D-print our adversarial objects and perform physical experiments to illustrate that such vulnerability exists in the real world.

&amp;#x200B;

Website: [https://sites.google.com/view/lidar-adv](https://sites.google.com/view/lidar-adv)

Paper: [https://arxiv.org/abs/1907.05418](https://arxiv.org/abs/1907.05418)",82,236
797,2019-7-13,2019,7,13,0,cccghc,Help needed to understand compute power and memory required for ADFuller test,https://www.reddit.com/r/MachineLearning/comments/cccghc/help_needed_to_understand_compute_power_and/,dodger94,1562946019,[removed],0,1
798,2019-7-13,2019,7,13,0,ccci0q,[R] AI-Based Photo Restoration,https://www.reddit.com/r/MachineLearning/comments/ccci0q/r_aibased_photo_restoration/,pvl18,1562946222,"This article deals with how to restore old photos using neural networks. Applied approaches: In-Place BatchNorm, Partial Convolution, Self-Attention GAN. The whole process of restoration is described in detail:

&amp;#x200B;

1. Find all the image defects: fractures, scuffs, holes.

2. Inpaint the discovered defects, based on the pixel values around them.

3. Colorize the image.

&amp;#x200B;

Also, the website was created where you can upload any photo for restoration online.

&amp;#x200B;

 [https://habr.com/en/company/mailru/blog/459696/](https://habr.com/en/company/mailru/blog/459696/)",0,1
799,2019-7-13,2019,7,13,1,cccpwr,"""If you torture data sufficiently, it will confess to almost anything."" -- Fred Menger",https://www.reddit.com/r/MachineLearning/comments/cccpwr/if_you_torture_data_sufficiently_it_will_confess/,aasgr,1562947260,[removed],0,1
800,2019-7-13,2019,7,13,1,cccxrn,Negative Loss in CNN,https://www.reddit.com/r/MachineLearning/comments/cccxrn/negative_loss_in_cnn/,abdoulsn,1562948264,[removed],0,1
801,2019-7-13,2019,7,13,1,ccdeyf,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/ccdeyf/data_science_career_track_prep_course/,HannahHumphreys,1562950456,[removed],0,1
802,2019-7-13,2019,7,13,2,ccdwb8,"ninja tricks tutorial 3in1 double, single 3 bit weaves, a reverse trans...",https://www.reddit.com/r/MachineLearning/comments/ccdwb8/ninja_tricks_tutorial_3in1_double_single_3_bit/,thetrickshotone,1562952655,,0,1
803,2019-7-13,2019,7,13,2,ccdzja,Multilingual Universal Sentence Encoder for Semantic Retrieval,https://www.reddit.com/r/MachineLearning/comments/ccdzja/multilingual_universal_sentence_encoder_for/,sjoerdapp,1562953057,,0,1
804,2019-7-13,2019,7,13,2,cce6xz,Machine Learning in Python to Model Data with an Unknown Nonlinear Relationship (I am a complete beginner),https://www.reddit.com/r/MachineLearning/comments/cce6xz/machine_learning_in_python_to_model_data_with_an/,PeterNewell,1562953985,[removed],0,1
805,2019-7-13,2019,7,13,3,cceaia,How energy efficient buildings leverage IoT and Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/cceaia/how_energy_efficient_buildings_leverage_iot_and/,roboticixt,1562954441,,0,1
806,2019-7-13,2019,7,13,3,cceeyc,"Best group telegram about deep learning ,machine learning ,.... including andrew ng , ......",https://www.reddit.com/r/MachineLearning/comments/cceeyc/best_group_telegram_about_deep_learning_machine/,Doctor_who1,1562954993,[removed],0,1
807,2019-7-13,2019,7,13,3,ccew75,The difference between 18.06 and 18.065,https://www.reddit.com/r/MachineLearning/comments/ccew75/the_difference_between_1806_and_18065/,veljko7,1562957209,,0,1
808,2019-7-13,2019,7,13,3,cceykd,Prediction of Logging Statement Placement,https://www.reddit.com/r/MachineLearning/comments/cceykd/prediction_of_logging_statement_placement/,iamkeyur,1562957505,[removed],0,1
809,2019-7-13,2019,7,13,3,ccf0dw,[P] I'm stuck designing a neural net architecture for my project,https://www.reddit.com/r/MachineLearning/comments/ccf0dw/p_im_stuck_designing_a_neural_net_architecture/,jthat92,1562957743,"Hi!

A few weeks ago I had the idea to write an app that counts of players in finished game of go. Instead of using pure computer vision I wanted to use deep learning.

The idea was to translate a picture of a go game into a 19x19x3 px image representing the board position and then count the points. I created all the data for the training. [This is](https://imgur.com/a/zMgkVUm) how an image looks and its corresponding label. So I am training on like 7000 images/labels like this.

I tried out some architectures like multiple conv/pool layers without any FC layers. I am using mostly categorical cross entropy. Sizes of the kernel are 3x3. I played around with different  number of filters. Often the training gets like to 60% accuracy and then stops to learn, so no accuracy progress is visible. I am thinking that the architecure is not suitable for my task. Seems like my data is fine although its weird that I always have to cast the data to tf.float32 before training, because tf doesnt let me to use tf.uint8.

Any tips how I could better my architecture for this specific task or maybe even a different approach?",6,1
810,2019-7-13,2019,7,13,4,ccf6zu,Question about creating something novel,https://www.reddit.com/r/MachineLearning/comments/ccf6zu/question_about_creating_something_novel/,whosdatb0y0,1562958583,[removed],0,1
811,2019-7-13,2019,7,13,4,ccfkcm,I wrote TL;DR for all few-shot learning papers from CVPR,https://www.reddit.com/r/MachineLearning/comments/ccfkcm/i_wrote_tldr_for_all_fewshot_learning_papers_from/,FSMer,1562960319,,0,1
812,2019-7-13,2019,7,13,4,ccfpsa,[R] TL;DR for all few-shot learning papers from CVPR,https://www.reddit.com/r/MachineLearning/comments/ccfpsa/r_tldr_for_all_fewshot_learning_papers_from_cvpr/,FSMer,1562961046,"I wrote TL;DR for all few-shot learning papers from CVPR. There are about 20 of them (compared to only 4 last year). Hope you find it useful and will be glad to hear if I missed something or got anything wrong. 

https://medium.com/p/few-shot-learning-in-cvpr19-6c6892fc8c5?source=email-23022de21ddd--writer.postDistributed&amp;sk=63c74613f22e056844d3d6b785f116a0",3,14
813,2019-7-13,2019,7,13,5,ccghmk,Learning about the hardware for low-precision models (or quantization),https://www.reddit.com/r/MachineLearning/comments/ccghmk/learning_about_the_hardware_for_lowprecision/,aataa12,1562964772,[removed],0,1
814,2019-7-13,2019,7,13,5,ccgjqv,GitHub - tg12/Predict-Ftse-Close-Price-ML-AI: Predicting the next close of the FTSE100 with RNN and LSTM,https://www.reddit.com/r/MachineLearning/comments/ccgjqv/github_tg12predictftseclosepricemlai_predicting/,JS_NS_Article,1562965066,,0,1
815,2019-7-13,2019,7,13,6,ccgvwm,tensorflow/tfjs-wechat is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/ccgvwm/tensorflowtfjswechat_is_a_new_github_repo_by/,sjoerdapp,1562966732,,0,1
816,2019-7-13,2019,7,13,8,ccib5t,[R] To find relationship between variables,https://www.reddit.com/r/MachineLearning/comments/ccib5t/r_to_find_relationship_between_variables/,vichemb,1562974224,"Need an algorithm to find all the rows in a large table with the following conditions:

If A =... then B=C

We don't know which columns are A,B,C  and also the value of A which satisfies the condition is unknown, the only information provided is the table which has more than 100 columns",2,1
817,2019-7-13,2019,7,13,9,ccioke,"(X-Post from Ask Academia) Low undergrad GPA, 4.0 grad GPA from top 10 school with publications. Do I have a chance at top CS schools for ML/AI?",https://www.reddit.com/r/MachineLearning/comments/ccioke/xpost_from_ask_academia_low_undergrad_gpa_40_grad/,ch22ggle,1562976365,[removed],0,1
818,2019-7-13,2019,7,13,9,ccit6n,Choosing the Correct RTX2070 For machine Learning Bulid,https://www.reddit.com/r/MachineLearning/comments/ccit6n/choosing_the_correct_rtx2070_for_machine_learning/,neurocarr,1562977076,[removed],0,1
819,2019-7-13,2019,7,13,9,ccj2h1,Help Selecting Right RTX 2070 In Expandable Learning Rig,https://www.reddit.com/r/MachineLearning/comments/ccj2h1/help_selecting_right_rtx_2070_in_expandable/,neurocarr,1562978594,"Sup people,

I am creating an expandable machine learning build and I am at the gpu selection portion and I am a bit stumped. Their are so many different brands of cards and im not sure which one would be best for machine learning. 

i plan on starting with 2 cards and expanding if i need the extra power.

&amp;#x200B;

The rig will have a custom cpu and gpu cooler loop so fans on the cards are irrelevant.

can anyone help? 

&amp;#x200B;

here is my part list so far

\[PCPartPicker Part List\]([https://pcpartpicker.com/list/7zZQTB](https://pcpartpicker.com/list/7zZQTB))

&amp;#x200B;

Type|Item|Price

:----|:----|:----

\*\*CPU\*\* | \[AMD - Threadripper 1900X 3.8 GHz 8-Core Processor\]([https://pcpartpicker.com/product/BvgPxr/amd-threadripper-1900x-38ghz-8-core-processor-yd190xa8aewof](https://pcpartpicker.com/product/BvgPxr/amd-threadripper-1900x-38ghz-8-core-processor-yd190xa8aewof)) | $229.99 @ Newegg 

\*\*Motherboard\*\* | \[ASRock - X399 Taichi ATX TR4 Motherboard\]([https://pcpartpicker.com/product/kjmxFT/asrock-x399-taichi-atx-tr4-motherboard-x399-taichi](https://pcpartpicker.com/product/kjmxFT/asrock-x399-taichi-atx-tr4-motherboard-x399-taichi)) | $309.99 @ Newegg 

\*\*Memory\*\* | \[Corsair - Vengeance LPX 16 GB (2 x 8 GB) DDR4-3200 Memory\]([https://pcpartpicker.com/product/p6RFf7/corsair-memory-cmk16gx4m2b3200c16](https://pcpartpicker.com/product/p6RFf7/corsair-memory-cmk16gx4m2b3200c16)) | $69.99 @ Amazon 

\*\*Memory\*\* | \[Corsair - Vengeance LPX 16 GB (2 x 8 GB) DDR4-3200 Memory\]([https://pcpartpicker.com/product/p6RFf7/corsair-memory-cmk16gx4m2b3200c16](https://pcpartpicker.com/product/p6RFf7/corsair-memory-cmk16gx4m2b3200c16)) | $69.99 @ Amazon 

\*\*Storage\*\* | \[Western Digital - Blue 1 TB M.2-2280 Solid State Drive\]([https://pcpartpicker.com/product/DgJtt6/western-digital-blue-1tb-m2-2280-solid-state-drive-wds100t2b0b](https://pcpartpicker.com/product/DgJtt6/western-digital-blue-1tb-m2-2280-solid-state-drive-wds100t2b0b)) | $109.99 @ Amazon 

\*\*Video Card\*\* | \[Asus - GeForce RTX 2070 8 GB DUAL OC Video Card\]([https://pcpartpicker.com/product/m43H99/asus-geforce-rtx-2070-8gb-dual-oc-video-card-dual-rtx2070-o8g](https://pcpartpicker.com/product/m43H99/asus-geforce-rtx-2070-8gb-dual-oc-video-card-dual-rtx2070-o8g)) | $449.99 @ B&amp;H 

\*\*Case\*\* | \[Thermaltake - Tower 900 ATX Full Tower Case\]([https://pcpartpicker.com/product/czGj4D/thermaltake-the-tower-900-snow-edition-atx-full-tower-case-ca-1h1-00f6wn-00](https://pcpartpicker.com/product/czGj4D/thermaltake-the-tower-900-snow-edition-atx-full-tower-case-ca-1h1-00f6wn-00)) | $209.00 @ Walmart 

\*\*Power Supply\*\* | \[Corsair - HX Platinum 1200 W 80+ Platinum Certified Fully Modular ATX Power Supply\]([https://pcpartpicker.com/product/f7L7YJ/corsair-hx-platinum-1200w-80-platinum-certified-fully-modular-atx-power-supply-cp-9020140-na](https://pcpartpicker.com/product/f7L7YJ/corsair-hx-platinum-1200w-80-platinum-certified-fully-modular-atx-power-supply-cp-9020140-na)) | $199.99 @ Amazon 

 | \*Prices include shipping, taxes, rebates, and discounts\* |

 | Total (before mail-in rebates) | $1678.93

 | Mail-in rebates | -$30.00

 | \*\*Total\*\* | \*\*$1648.93\*\*

 | Generated by \[PCPartPicker\]([https://pcpartpicker.com](https://pcpartpicker.com)) 2019-07-12 20:20 EDT-0400 |",0,1
820,2019-7-13,2019,7,13,9,ccj573,My hn Hng K 200A: Thng tin chi tit v sn phm,https://www.reddit.com/r/MachineLearning/comments/ccj573/my_hn_hng_k_200a_thng_tin_chi_tit_v_sn/,laasd15,1562979056,,0,1
821,2019-7-13,2019,7,13,10,ccje72,eGPU options?,https://www.reddit.com/r/MachineLearning/comments/ccje72/egpu_options/,biologicalterminator,1562980591,[removed],0,1
822,2019-7-13,2019,7,13,10,ccji55,[Discussion] eGPU option?,https://www.reddit.com/r/MachineLearning/comments/ccji55/discussion_egpu_option/,biologicalterminator,1562981258,"So currently I have a 2013 15-inch MacBook Pro with intel Core i7 and 16GB of RAM. This upcoming semester I will be taking a deep learning course focused in computer vision. I was in a deep learning course last semester and because of that I am scared I will roast my computer further into oblivion than it already has been if I try to train my models on it next semester.

So with that said I am thinking of purchasing an external GPU. Does anyone have any experience with this or do I just have to suck it up and use google collab, or use campus computers, or just buy a new machine? (I don't want to do any of those because I hate notebooks, I would rather be able to code in the comfort of my home and I don't want to pay that much money when my laptop works just fine for everything else).

The one thing I am worried about is getting bottlenecked by transfer speeds between my mac and the eGPU. I have thunderbolt 2 ports and I am not that knowledgable of hardware so I have no idea if the transfer speeds are so slow that having an eGPU will just be useless because of this.

But if anyone has experience with this or suggestions I would love to hear them!",7,3
823,2019-7-13,2019,7,13,10,ccjjri,What Is XLNet And How Does It Work?,https://www.reddit.com/r/MachineLearning/comments/ccjjri/what_is_xlnet_and_how_does_it_work/,newssrc,1562981540,,0,1
824,2019-7-13,2019,7,13,11,cck5l8,[N] Hong Kong Machine Learning Meetup @AWS,https://www.reddit.com/r/MachineLearning/comments/cck5l8/n_hong_kong_machine_learning_meetup_aws/,gau_mar,1562985361,,0,1
825,2019-7-13,2019,7,13,11,cck8wg,[R] Sequential Neural Processes (SNP),https://www.reddit.com/r/MachineLearning/comments/cck8wg/r_sequential_neural_processes_snp/,jaesik,1562985928,,3,11
826,2019-7-13,2019,7,13,12,ccklud,Examples and best practices for building recommendation systems by Microsoft,https://www.reddit.com/r/MachineLearning/comments/ccklud/examples_and_best_practices_for_building/,ConfidentMushroom,1562988184,,0,1
827,2019-7-13,2019,7,13,13,cclfb5,A question about batch normalization and lstm on TensorFlow,https://www.reddit.com/r/MachineLearning/comments/cclfb5/a_question_about_batch_normalization_and_lstm_on/,maxchu719,1562993593,[removed],0,1
828,2019-7-13,2019,7,13,13,cclglm,Techniques used to reduce key pressings when write a sentences (Like auto complete),https://www.reddit.com/r/MachineLearning/comments/cclglm/techniques_used_to_reduce_key_pressings_when/,Houtarou97,1562993852,[removed],0,1
829,2019-7-13,2019,7,13,14,cclmll,Global Wall Mounted CD Player Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cclmll/global_wall_mounted_cd_player_market_report_2019/,jadhavni3,1562995041,[removed],1,1
830,2019-7-13,2019,7,13,14,cclrul,Global Workpiece Clamping Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cclrul/global_workpiece_clamping_market_report_2019/,jadhavni3,1562996141,[removed],1,1
831,2019-7-13,2019,7,13,14,cclut0,Global Worm Gear Box Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cclut0/global_worm_gear_box_market_report_2019/,jadhavni3,1562996758,[removed],1,1
832,2019-7-13,2019,7,13,14,cclxox,Global Yaw System Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cclxox/global_yaw_system_market_report_2019/,jadhavni3,1562997372,[removed],1,1
833,2019-7-13,2019,7,13,15,ccm1dw,Global Automotive Parking Sensor Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/ccm1dw/global_automotive_parking_sensor_market_report/,jadhavni3,1562998143,[removed],1,1
834,2019-7-13,2019,7,13,15,ccmc43,[R] Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges,https://www.reddit.com/r/MachineLearning/comments/ccmc43/r_massively_multilingual_neural_machine/,hardmaru,1563000468,,2,94
835,2019-7-13,2019,7,13,17,ccn3hh,[R] Learning multiplication outside of training data range,https://www.reddit.com/r/MachineLearning/comments/ccn3hh/r_learning_multiplication_outside_of_training/,neltherion,1563006929,"I want to train an MLP to multiply two inputs. The easiest approach is to generate a vast dataset of numbers and their multiplication and train the network on them.

&amp;#x200B;

I did this as a test and the MLP worked nearly flawless and long as I gave it numbers inside the training range but as soon as I deviated from the training domain the network started malfunctioning.

&amp;#x200B;

I wanted to know if there is an architecture which can learn the semantics of multiplication and act properly outside its training domain.

&amp;#x200B;

Thanks",16,0
836,2019-7-13,2019,7,13,17,ccn4rt,34 External Machine Learning Resources and Related Articles,https://www.reddit.com/r/MachineLearning/comments/ccn4rt/34_external_machine_learning_resources_and/,andrea_manero,1563007270,[removed],0,1
837,2019-7-13,2019,7,13,17,ccn8o3,"[D] About feature selection, how normal it is to do selection of features by using statistical tests checking the null hypothesis between a feature and the target variable?(ANOVA chi-square or correlation) Can you eliminate a feature from the model if the p value is higher than 0.05?",https://www.reddit.com/r/MachineLearning/comments/ccn8o3/d_about_feature_selection_how_normal_it_is_to_do/,JIPHF,1563008270,,0,1
838,2019-7-13,2019,7,13,18,ccn963,How Many Number of Neurons and Layers in ANN should we use?,https://www.reddit.com/r/MachineLearning/comments/ccn963/how_many_number_of_neurons_and_layers_in_ann/,cale_kohler,1563008403,[removed],0,1
839,2019-7-13,2019,7,13,18,ccnb5s,Recently came across this facial recognition software iFalcon Face Control Mobile that is being used with AR glasses. Is there any similar open source code that does this? Preferably in python.,https://www.reddit.com/r/MachineLearning/comments/ccnb5s/recently_came_across_this_facial_recognition/,rafaelDgrate,1563008859,[removed],0,1
840,2019-7-13,2019,7,13,18,ccnift,I'm Looking for a summer school or in site training for deep learning(computer vision or NLP),https://www.reddit.com/r/MachineLearning/comments/ccnift/im_looking_for_a_summer_school_or_in_site/,epicSaitama,1563010578,[removed],0,1
841,2019-7-13,2019,7,13,20,cco9hy,Python: Beginner's Guide to Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/cco9hy/python_beginners_guide_to_artificial_intelligence/,HannahHumphreys,1563016913,[removed],0,1
842,2019-7-13,2019,7,13,20,ccobkj,TensorLayer with AMD GPU?,https://www.reddit.com/r/MachineLearning/comments/ccobkj/tensorlayer_with_amd_gpu/,thisisthehappylion,1563017390,[removed],0,1
843,2019-7-13,2019,7,13,20,ccofx2,Autoencoder on heat maps,https://www.reddit.com/r/MachineLearning/comments/ccofx2/autoencoder_on_heat_maps/,BlackHawk1001,1563018322,"Hello everybody

I have time sequences of 2D heat maps. For different people I have heat maps over time. For each person I have around 720 heat maps and in total around 50'000 heat maps. 

Here is an example of a heat map visualizing eye tracking data:

&amp;#x200B;

&amp;#x200B;

*Processing img 8w7enske62a31...*

&amp;#x200B;

Now, I would like to train an autoencoder on these heat maps to learn a meaningful low dimensional representation. I will use Keras in Python for the implementation.

I thought about using a CNN for encoder and decoder (but I'm unsure about the number of layers and the size of the layers). Another option could be to use a cycleGAN (pix2pix) or also taking time into account using LSTM but I don't know how to use LSTM for 2D image data. I have also heard that attention layers might be interesting.

What architectures and configurations are reasonable to use?",0,1
844,2019-7-13,2019,7,13,20,ccogj6,It's a lot easier to evaluate what a new record on a data-set means if one can easily see examples of problems in that data-set [D],https://www.reddit.com/r/MachineLearning/comments/ccogj6/its_a_lot_easier_to_evaluate_what_a_new_record_on/,no_bear_so_low,1563018453,"Without having to download the whole data-set.   


For example, I'm interested in progress in NLP. Recently machine performance has exceeded baseline human performance on the MS Marco  Q&amp;A task. It's hard to have a real sense of what this means without downloading the whole evaluation portion of the MS Marco data-set, which I don't particularly want to do. If you're going to go to the trouble of putting up a leader-board, you might as well include a page with a sample of a hundred questions or so.  


Hats off to people who provide plentiful examples of the kind of questions in their data-sets including SQuAD, The Winograd Schema, the Ai2 people, ReCoRD and many others.",6,59
845,2019-7-13,2019,7,13,22,ccp6tj,[R] Is Conditioning an Acceptable Policy Evaluation?,https://www.reddit.com/r/MachineLearning/comments/ccp6tj/r_is_conditioning_an_acceptable_policy_evaluation/,postmachines,1563023535,[removed],0,1
846,2019-7-13,2019,7,13,22,ccpb6x,Still image to a 3D image; repository / research paper,https://www.reddit.com/r/MachineLearning/comments/ccpb6x/still_image_to_a_3d_image_repository_research/,FreddyShrimp,1563024326,"I know there are research papers out there that describe methods to convert a still image into a 3D object (like those used in CAD drawings). 

&amp;#x200B;

However, I'm wondering if anyone knows of an existing example; paper and/or repository that can adjust a still 2D image into a 3d image pretty much like facebook does with it's [3d image feature](https://www.pocket-lint.com/apps/news/facebook/146017-facebook-3d-photos-how-to-create-share-and-view-3d-photos). Another example would be the google Snapseed app with the ""headpose"" option (although I think this is more of a vector transformation, whereas the facebook example actually deploys ML I believe).

&amp;#x200B;

What I have done so far, without any succes:

\- Googled for examples

\- Searched github

\- Searched google scholar.

&amp;#x200B;

So, if anyone knows of a good example, please let me know and share it with me!

&amp;#x200B;

Thanks in advance, and all the best!

Fred",0,1
847,2019-7-13,2019,7,13,22,ccpj50,[P] I built Dab and T-Pose Controlled Lights with OpenPose and Keras,https://www.reddit.com/r/MachineLearning/comments/ccpj50/p_i_built_dab_and_tpose_controlled_lights_with/,kpkaiser,1563025724,"In the words of my 9 year old niece, the dab is ""well.. considered old fashioned.""

So, here's a completely ridiculous project, lights controlled by old fashioned dance moves. 

To build training data I reused the OpenPose [Python example](https://github.com/burningion/dab-and-tpose-controlled-lights/blob/master/src/01_body_from_image.py), adding a keypress to build an array of sample data for either category of `dab`, `tpose`, or `other`.

Once I got that data, I went through it interactively in a [Jupyter Notebook](https://github.com/burningion/dab-and-tpose-controlled-lights/blob/master/Data%20Play.ipynb), where I cleaned it up and then trained a network on 171 sample poses.

With that trained network, I then used it along with a Z-Wave Z Stick on my NVIDIA Jetson, and controlled the lights on my network with [this program I wrote glueing everything together](https://github.com/burningion/dab-and-tpose-controlled-lights/blob/master/src/dab-tpose-controller.py).

All the code is on [Github](https://github.com/burningion/dab-and-tpose-controlled-lights).

If you're interested, I also wrote a [blog post](https://www.makeartwithpython.com/blog/dab-and-tpose-controlled-lights/) with more visuals.",3,57
848,2019-7-13,2019,7,13,23,ccpnp5,[D] Jacobian Policy Optimizations,https://www.reddit.com/r/MachineLearning/comments/ccpnp5/d_jacobian_policy_optimizations/,postmachines,1563026521,[removed],0,1
849,2019-7-14,2019,7,14,0,ccqev9,[R] Jacobian Policy Optimization,https://www.reddit.com/r/MachineLearning/comments/ccqev9/r_jacobian_policy_optimization/,postmachines,1563030834,,0,1
850,2019-7-14,2019,7,14,0,ccqgmy,[R] Jacobian Policy Optimizations,https://www.reddit.com/r/MachineLearning/comments/ccqgmy/r_jacobian_policy_optimizations/,postmachines,1563031106,[removed],0,1
851,2019-7-14,2019,7,14,0,ccqii5,[R] Jacobian Policy Optimizations,https://www.reddit.com/r/MachineLearning/comments/ccqii5/r_jacobian_policy_optimizations/,postmachines,1563031394,[removed],0,1
852,2019-7-14,2019,7,14,0,ccqjdn,PTITION : MACRON DEVANT LA COUR PNALE INTERNATIONALE,https://www.reddit.com/r/MachineLearning/comments/ccqjdn/ptition_macron_devant_la_cour_pnale/,vorgeat,1563031523,,0,1
853,2019-7-14,2019,7,14,0,ccqkmo,[D] Jacobian Policy Optimizations,https://www.reddit.com/r/MachineLearning/comments/ccqkmo/d_jacobian_policy_optimizations/,postmachines,1563031709,[removed],0,1
854,2019-7-14,2019,7,14,0,ccql9h,[R] Jacobian Policy Optimizations,https://www.reddit.com/r/MachineLearning/comments/ccql9h/r_jacobian_policy_optimizations/,postmachines,1563031806,,4,11
855,2019-7-14,2019,7,14,0,ccqnar,AI smokes 5 poker champs at a time in no-limit Holdem with ruthless consistency,https://www.reddit.com/r/MachineLearning/comments/ccqnar/ai_smokes_5_poker_champs_at_a_time_in_nolimit/,craigbrownphd,1563032128,,0,1
856,2019-7-14,2019,7,14,0,ccqstt,The Gaia Project - Training a neural network on global ancient and modern population data to predict human migration,https://www.reddit.com/r/MachineLearning/comments/ccqstt/the_gaia_project_training_a_neural_network_on/,Boccard,1563032973,[removed],0,1
857,2019-7-14,2019,7,14,1,ccr1er,Benefits of taking a course on linear/non-linear optimzation,https://www.reddit.com/r/MachineLearning/comments/ccr1er/benefits_of_taking_a_course_on_linearnonlinear/,jtschwar,1563034262,"Hi Friends working in industry as deep/machine learning engineers, 

  
Are there any benefits on having an extensive knowledge of optimization algorithms beyond the variations of gradient descent? (i.e. Chambolle-Pock, Split Bregman, Proximal methods, etc.) 

  
I'm debating on spending my next semester taking a course either on this subject or deep learning for computer vision. I'd love to hear everyone's opinion on which skills would be more relevant for industry. 

  
Many thanks!",0,1
858,2019-7-14,2019,7,14,2,ccrpdz,[D] Choosing a network architecture for a recommender system,https://www.reddit.com/r/MachineLearning/comments/ccrpdz/d_choosing_a_network_architecture_for_a/,carsonpoole,1563037789,"So when making a deep recommender system, I see an obvious problem with making it work with a NN, versus some kind of correlation analysis. Basically when you have a network doing the recommending, whenever you add new things to recommend (new products, movies, books, etc) you'll have to retrain the model on the new ""products"" to get recommendations including those new things.

Is there an easy way to structure a network's architecture to mitigate this problem? Or do you just have to retrain the model all the time?

Thanks!",4,0
859,2019-7-14,2019,7,14,2,ccrwli,"A Finnish soldier in front of a captured Soviet T-28 tank, 1940",https://www.reddit.com/r/MachineLearning/comments/ccrwli/a_finnish_soldier_in_front_of_a_captured_soviet/,Tarihweb,1563038835,,0,1
860,2019-7-14,2019,7,14,2,ccs07b,Friday freestyle tutorials viewer requested the pinball effect finger bo...,https://www.reddit.com/r/MachineLearning/comments/ccs07b/friday_freestyle_tutorials_viewer_requested_the/,thetrickshotone,1563039355,,0,1
861,2019-7-14,2019,7,14,3,ccscgz,"[D] What are you favorite machine learning frameworks/libraries, besides the major ones?",https://www.reddit.com/r/MachineLearning/comments/ccscgz/d_what_are_you_favorite_machine_learning/,BatmantoshReturns,1563041066,[removed],0,1
862,2019-7-14,2019,7,14,3,ccsclo,Yelp dataset challenge - winner used GANs to produce images of food. Vaguely they state that it has many applications in the industry - what are the applications?,https://www.reddit.com/r/MachineLearning/comments/ccsclo/yelp_dataset_challenge_winner_used_gans_to/,shitinmyunderwear,1563041083,,0,1
863,2019-7-14,2019,7,14,3,ccsl94,Using an LSTM-based model to predict EasyJet's stock returns (Keras tutorial),https://www.reddit.com/r/MachineLearning/comments/ccsl94/using_an_lstmbased_model_to_predict_easyjets/,jdyr1729,1563042278,[removed],0,1
864,2019-7-14,2019,7,14,4,cct5u4,How to use Amazon SageMaker BlazingText classifier with pre-trained vectors?,https://www.reddit.com/r/MachineLearning/comments/cct5u4/how_to_use_amazon_sagemaker_blazingtext/,kinghuang,1563045161,[removed],0,1
865,2019-7-14,2019,7,14,4,cct6no,[P] Looking for Research Collaborators!,https://www.reddit.com/r/MachineLearning/comments/cct6no/p_looking_for_research_collaborators/,Ash3nBlue,1563045277,"Hey! Some buddies and I are starting an organization for AI research collaboration, and we're looking for members. We currently have six figures of funding and several deep learning experts on our team. Our goal is to work together to do what we otherwise couldn't alone, and produce research/products that are valuable to a lot of people. We can provide technical and moral support as well as huge amounts of TPUv3 compute for research projects.

&amp;#x200B;

If you're interested, shoot me a message with your background/experience as well as what you're interested in working on. We're open to all ideas, but we're especially interested in research directions that can benefit society. There are no qualifications; we're just looking for people who are passionate about AI and are willing to put in work to publish research and build technology that is valuable to the world. We're also looking for co-founders and mentors!

&amp;#x200B;

We look forward to hearing from all of you!",56,156
866,2019-7-14,2019,7,14,4,ccta02,[D] Benefits of learning linear/non-linear optimzation,https://www.reddit.com/r/MachineLearning/comments/ccta02/d_benefits_of_learning_linearnonlinear_optimzation/,jtschwar,1563045740,"Hi Friends working in industry as deep/machine learning engineers,

Are there any benefits on having an extensive knowledge of optimization algorithms beyond the variations of gradient descent? (i.e. Chambolle-Pock, Split Bregman, Proximal methods, etc.)

I'm debating on spending my next semester taking a course either on this subject or deep learning for computer vision. I'd love to hear everyone's opinion on which skills would be more relevant for industry.

Many thanks!",5,1
867,2019-7-14,2019,7,14,4,ccte0g,[D] Is anyone interested in doing small machine learning tasks for small dividends?,https://www.reddit.com/r/MachineLearning/comments/ccte0g/d_is_anyone_interested_in_doing_small_machine/,DisastrousProgrammer,1563046318,"In my projects I get stuck at times, and while I eventually get unsuck, it still adds time and friction to my projects. Stuff like setting up new tools, pipelines, deployment, frameworks, programming, etc.

I have been using /r/slavelabour and /r/ProgrammingTasks to get help time to time, but since I am doing ML projects, I am wondering if there any people here who would be up for that.",19,2
868,2019-7-14,2019,7,14,5,cctxmr,Using an LSTM-based model to predict EasyJet's stock returns (Keras tutorial),https://www.reddit.com/r/MachineLearning/comments/cctxmr/using_an_lstmbased_model_to_predict_easyjets/,andydodd,1563049085,,0,1
869,2019-7-14,2019,7,14,7,ccvdu6,AI program beats pros in six-player poker  a first,https://www.reddit.com/r/MachineLearning/comments/ccvdu6/ai_program_beats_pros_in_sixplayer_poker_a_first/,eigenman,1563056789,,0,1
870,2019-7-14,2019,7,14,7,ccvlex,[Q] Training data for text categorization similar to Google Cloud NLU?,https://www.reddit.com/r/MachineLearning/comments/ccvlex/q_training_data_for_text_categorization_similar/,pk12_,1563057975,[removed],0,1
871,2019-7-14,2019,7,14,9,ccwg6i,[D] Is there such a thing as generalized adversarial noise?,https://www.reddit.com/r/MachineLearning/comments/ccwg6i/d_is_there_such_a_thing_as_generalized/,Boozybrain,1563062923,Adversarial noise/examples that produce the desired output across different models - does it exist and if not is it within the realm of possibility? Is it wrong to think that models that do X might be susceptible to noise in the form of Y?  This can easily be tested empirically but is there any existing theory that suggests one way or the other?,3,3
872,2019-7-14,2019,7,14,9,ccwmd8,Recommended PC Spec,https://www.reddit.com/r/MachineLearning/comments/ccwmd8/recommended_pc_spec/,bluefish009,1563063878,[removed],0,1
873,2019-7-14,2019,7,14,9,ccwq92,[D] Recommended PC Spec,https://www.reddit.com/r/MachineLearning/comments/ccwq92/d_recommended_pc_spec/,bluefish009,1563064528,"Hi, i am newbie here, what is most usable PC hardware spec to study machine learning?

My current PC is very old (AMD kaveri A8-7600), so i am about to buy new one.

Thanks!",8,0
874,2019-7-14,2019,7,14,11,ccxshu,religion and virtual reality: ultimate bliss,https://www.reddit.com/r/MachineLearning/comments/ccxshu/religion_and_virtual_reality_ultimate_bliss/,_hisoka_freecs_,1563071177,[removed],0,1
875,2019-7-14,2019,7,14,12,ccynfu,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/ccynfu/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1563076685,[removed],0,1
876,2019-7-14,2019,7,14,15,cczot8,Career/Lifestyle Happiness,https://www.reddit.com/r/MachineLearning/comments/cczot8/careerlifestyle_happiness/,errminator,1563084132,[removed],0,1
877,2019-7-14,2019,7,14,15,cczu4w,[P]implementation of Bayesian MAML (1D regression and 2D Navigation RL task) is opened.,https://www.reddit.com/r/MachineLearning/comments/cczu4w/pimplementation_of_bayesian_maml_1d_regression/,jaesik,1563085331,,1,1
878,2019-7-14,2019,7,14,17,cd0osl,[R] BERT and XLNET for Malay and Indonesian languages.,https://www.reddit.com/r/MachineLearning/comments/cd0osl/r_bert_and_xlnet_for_malay_and_indonesian/,huseinzol05,1563092540,"I released BERT and XLNET for Malay language, trained on around 1.2GB of data (public news, twitter, instagram, wikipedia and parliament text), and do some comparison among it. So it is really good on both social media and native context, I believe it also good for Bahasa Indonesia, in Wikipedia, we share a lot of similar context and assimilation with Indonesian text. And we know BERT released Multilanguage model, size around 714MB, which is so great but too heavy on some low cost development.

BERT-Bahasa, you can read more at here, https://github.com/huseinzol05/Malaya/tree/master/bert
2 models for BERT-Bahasa,

1. Vocab size 40k, Case Sensitive, Train on 1.21GB dataset, BASE size (467MB).
2. Vocab size 40k, Case Sensitive, Train on 1.21GB dataset, SMALL size (184MB).

XLNET-Bahasa, you can read more at here, https://github.com/huseinzol05/Malaya/tree/master/xlnet
1 model for XLNET-Bahasa,
1. Vocab size 32k, Case Sensitive, Train on 1.21GB dataset, BASE size (878MB).

All comparison studies inside both README pages, comparison for abstractive summarization and neural machine translation are on the way, and XLNET-Bahasa SMALL is on training.",6,149
879,2019-7-14,2019,7,14,18,cd1b4r,[R] Recurrent Neural Processes,https://www.reddit.com/r/MachineLearning/comments/cd1b4r/r_recurrent_neural_processes/,hardmaru,1563098000,,2,29
880,2019-7-14,2019,7,14,19,cd1fo1,[D] Tools to annotate images for segmentation?,https://www.reddit.com/r/MachineLearning/comments/cd1fo1/d_tools_to_annotate_images_for_segmentation/,MasterScrat,1563099073,"What do you recommend for image annotation? I'm looking for a web-based interface to let non-technical users (MDs) annotate X-rays for image segmentation.

They should then be exportable to COCO format.

Best I found so far is this, but meh: http://labelme.csail.mit.edu/Release3.0/",13,23
881,2019-7-14,2019,7,14,19,cd1hf4,Pytorch RL CPP with ALE,https://www.reddit.com/r/MachineLearning/comments/cd1hf4/pytorch_rl_cpp_with_ale/,Teenvan1995,1563099502,"Check out Pytorch-RL-CPP: a C++ (Libtorch) implementation of Deep Reinforcement Learning algorithms with C++ Arcade Learning Environment.

[Pytorch-RL-CPP](https://github.com/navneet-nmk/Pytorch-RL-CPP)",0,1
882,2019-7-14,2019,7,14,19,cd1o2y,Implementing GBT simply in JavaScript.,https://www.reddit.com/r/MachineLearning/comments/cd1o2y/implementing_gbt_simply_in_javascript/,ydennisy,1563101053,[removed],0,1
883,2019-7-14,2019,7,14,20,cd23ip,sklearn.svm.OneClassSVM,https://www.reddit.com/r/MachineLearning/comments/cd23ip/sklearnsvmoneclasssvm/,luchins,1563104578,[removed],0,0
884,2019-7-14,2019,7,14,21,cd28cv,Breast cancer diagnosis with neural networks (implemented in python),https://www.reddit.com/r/MachineLearning/comments/cd28cv/breast_cancer_diagnosis_with_neural_networks/,antaloaalonso,1563105626,,0,1
885,2019-7-14,2019,7,14,21,cd2fdr,ML Theory Research in India vs rest of the World,https://www.reddit.com/r/MachineLearning/comments/cd2fdr/ml_theory_research_in_india_vs_rest_of_the_world/,mohit_jarvis29,1563107036,"Hello, Everyone, I need some guidance about masters (and Ph.D.). So I am certain that I want to work on ML theory for my Ph.D. However, I am confused about whether to stay in India or go abroad, as in India there are a very very few professors who research in ML theory. The preparation required to get into Indian institutes(via GATE exam) and to get into top programs in the world(good RAships in theory labs plus good research record) is very different. Can you please share your opinions about which path to choose?",0,1
886,2019-7-14,2019,7,14,23,cd3gox,[D][RL] Tips on solving short episodes with large action space and huge state space?,https://www.reddit.com/r/MachineLearning/comments/cd3gox/drl_tips_on_solving_short_episodes_with_large/,HalfArmBandit,1563113712,"I am trying to learn an agent to solve a problem of 2^n states and n actions where n~= 15000. I am using a DQN. Roughly 99.9% of actions return to the same state and only m of them change it, they essentially do a bit flip on the state. Each episode can last up to m state changes. Do you guys have any suggestions on what I should read to make my life easier with this problem?",1,2
887,2019-7-14,2019,7,14,23,cd3ijn,The Awesome Duo: 6 Cases of How FinTech Benefits From AI,https://www.reddit.com/r/MachineLearning/comments/cd3ijn/the_awesome_duo_6_cases_of_how_fintech_benefits/,caternoon,1563114012,[removed],0,1
888,2019-7-15,2019,7,15,0,cd4kj4,A smooth approach to putting machine learning into production,https://www.reddit.com/r/MachineLearning/comments/cd4kj4/a_smooth_approach_to_putting_machine_learning/,Lemax0,1563119800,,0,1
889,2019-7-15,2019,7,15,2,cd5nco,[N] MineRL 0.2.0 + Imitation Learning Visualization Tool Released for NeurIPS 2019 Comp on Sample Efficient RL,https://www.reddit.com/r/MachineLearning/comments/cd5nco/n_minerl_020_imitation_learning_visualization/,MadcowD,1563125255,"&amp;#x200B;

Hey all, I'm super excited to announce that     .. for the NeurIPS 2019 competition on sample efficient reinforcement learning!  [http://www.minerl.io/blog/](http://www.minerl.io/blog/)

We are so grateful for how much feedback everyone on r/ML has given us about our data and new simulator we released for Minecraft, and over the past couple of weeks we've worked really hard to integrate all of of those changes. :)

[An example of minerl.viewer on an expert trajectory in the  dataset.](https://i.redd.it/tn8gcrkwyaa31.gif)

You can read more in our blog post, but one of the things we realized that imitation research needs is a really rich visualizer for expert trajectories, so we made one, *minerl.viewer.* We hope to expand support for other imitation learning datasets like AtariGrandChallenge because this tool has been so useful for us in developing our own baselines. 

&amp;#x200B;

With &lt;3 from CMU, 

The  Team

Update now:   --  

Get new data: ..()",9,55
890,2019-7-15,2019,7,15,2,cd5pqx,[Paper summary] Bottom-Up Abstractive Summarization,https://www.reddit.com/r/MachineLearning/comments/cd5pqx/paper_summary_bottomup_abstractive_summarization/,newssrc,1563125599,,0,1
891,2019-7-15,2019,7,15,2,cd5wsd,What does batch size in testing data mean?,https://www.reddit.com/r/MachineLearning/comments/cd5wsd/what_does_batch_size_in_testing_data_mean/,___AJ___,1563126603,[removed],0,1
892,2019-7-15,2019,7,15,3,cd6i2q,Search for the fastest Deep Learning Framework supported by Keras,https://www.reddit.com/r/MachineLearning/comments/cd6i2q/search_for_the_fastest_deep_learning_framework/,andrea_manero,1563129604,[removed],0,1
893,2019-7-15,2019,7,15,4,cd73go,MachineLearning (p) [D discussion],https://www.reddit.com/r/MachineLearning/comments/cd73go/machinelearning_p_d_discussion/,karenactionsitafaal,1563132572,,0,1
894,2019-7-15,2019,7,15,4,cd767j, Releasing first online 3D Point Cloud labeling tool in Supervisely,https://www.reddit.com/r/MachineLearning/comments/cd767j/releasing_first_online_3d_point_cloud_labeling/,tdionis,1563132935,,0,1
895,2019-7-15,2019,7,15,4,cd775d,Does anyone know of machine learning software that is commercially available to license that analyzes creditworthiness of small businesses?,https://www.reddit.com/r/MachineLearning/comments/cd775d/does_anyone_know_of_machine_learning_software/,OWbeginner,1563133065,Title says it all. I know there are various companies with proprietary software that they've developed for their own use.,0,1
896,2019-7-15,2019,7,15,4,cd7clk,My dream of developing machine learning,https://www.reddit.com/r/MachineLearning/comments/cd7clk/my_dream_of_developing_machine_learning/,dyfghg1,1563133834,[removed],0,1
897,2019-7-15,2019,7,15,4,cd7dj9,[D] Nocix offers a RTX2070 dedicated server for $99/month,https://www.reddit.com/r/MachineLearning/comments/cd7dj9/d_nocix_offers_a_rtx2070_dedicated_server_for/,thebliket,1563133967,"Ever since hetzner started selling out of their 1080 servers I've been looking for a good deal for some dedicated servers with GPUs in them. I picked up a couple of Nocix's RTX 2070 servers which come with a i7 6700k, 32gb of DDR4 ram, two 480gb SATA SSD on a gigabit port with 100tb of transfer included. Setup was a breeze, I got login credentials within 10 minutes of ordering through their automated OS loading system. Pretty amazing deal on such new hardware. I am extremely happy. They are still in stock here: [https://www.nocix.net/cart/?id=338](https://www.nocix.net/cart/?id=338) 

&amp;#x200B;

Just wanted to make this post since people have been asking about good and low cost GPU hosts",0,0
898,2019-7-15,2019,7,15,5,cd7gko,[D] Machine Learning - WAYR (What Are You Reading) - Week 66,https://www.reddit.com/r/MachineLearning/comments/cd7gko/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1563134405,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|
|----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/Simusid: [Spectrogram Feature Losses for Music Source Separation](https://arxiv.org/abs/1901.05061)

/u/MogwaiAllOnYourFace: [Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization](https://arxiv.org/pdf/1906.02107.pdf)

/u/singularperturbation: [Uncertainty in Deep Learning](http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf)

Besides that, there are no rules, have fun.",3,12
899,2019-7-15,2019,7,15,5,cd7iry,Awesome AI usage,https://www.reddit.com/r/MachineLearning/comments/cd7iry/awesome_ai_usage/,cancelnews,1563134686,,0,1
900,2019-7-15,2019,7,15,5,cd7map,[D] Regression algorithm for creating card game bot,https://www.reddit.com/r/MachineLearning/comments/cd7map/d_regression_algorithm_for_creating_card_game_bot/,jasper369,1563135169,"I'm trying to create a machine that can play a certain dutch card game known as ""Klaverjassen"". What I'm trying to do is generate randomly played games and have the algorithm learn good plays by looking at the outcome of these games. 
Each game is encoded as a feature/x-vector of 1's and 0's, with a single outcome y between 0 and ~500. I was wondering what kind of regression algorithm would be good to learn this data. Simply using Linear regression isn't going to be useful since there is a lot of interaction between the features, and manually adding in these interactions is infeasible since there is over 1,104 features.
I'm using MatLab, so algorithms/libraries in this language would be nice :) 
For reference, this is what the data looks like: https://gofile.io/?c=kKs1rt (Matlab data file in zip)",6,3
901,2019-7-15,2019,7,15,5,cd814n,What should I learn first?,https://www.reddit.com/r/MachineLearning/comments/cd814n/what_should_i_learn_first/,nick_pl,1563137214,[removed],0,1
902,2019-7-15,2019,7,15,6,cd86sc,Help if possible!,https://www.reddit.com/r/MachineLearning/comments/cd86sc/help_if_possible/,McViper01,1563138007,[removed],0,1
903,2019-7-15,2019,7,15,6,cd8j16,[Q] Any recent tutorials for StyleGAN?,https://www.reddit.com/r/MachineLearning/comments/cd8j16/q_any_recent_tutorials_for_stylegan/,Shir_man,1563139699,"I would like to start generating some GAN-art with StyleGAN help, could anyone recommend a good tutorial where it is better to begin from?",0,1
904,2019-7-15,2019,7,15,6,cd8k4g,Pointless PhD?,https://www.reddit.com/r/MachineLearning/comments/cd8k4g/pointless_phd/,actuallynotcanadian,1563139855,[removed],0,1
905,2019-7-15,2019,7,15,7,cd9jwl,Pointless PhD for Machine Learning career advancement?,https://www.reddit.com/r/MachineLearning/comments/cd9jwl/pointless_phd_for_machine_learning_career/,actuallynotcanadian,1563145150,[removed],0,1
906,2019-7-15,2019,7,15,8,cd9kwc,How lucrative of an area is ML to get into?,https://www.reddit.com/r/MachineLearning/comments/cd9kwc/how_lucrative_of_an_area_is_ml_to_get_into/,pouyank,1563145301,[removed],0,1
907,2019-7-15,2019,7,15,8,cd9nfj,"[P] Implementation for ""Few-Shot Adversarial Learning of Realistic Neural Talking Head Models""",https://www.reddit.com/r/MachineLearning/comments/cd9nfj/p_implementation_for_fewshot_adversarial_learning/,CptVifen,1563145695,"Hello,

I have been working on implementing the model from the paper: [Few-Shot Adversarial Learning of Realistic Neural Talking Head Models](https://arxiv.org/abs/1905.08233v1) (Zakharov et al.) for my own projects and research. It uses a very interesting model of GAN and triggered my interest.

I added further recommendations given by the paper's author on various details that were unclear to me and other existing implementations upon only reading the paper. (added more depth to the network, adjusted adaIN parameters...)

Due to a lack of compute resources at my disposition and due to the model being very heavy, I only trained it on 5 epochs on a test dataset (15 times less epochs than in the paper and with a dataset 34 times smaller) but the results look promising so far for the small amount of training that went into it.

Here an example of fake faces it generated from facial landmarks and embedding vectors: https://raw.githubusercontent.com/vincent-thevenin/Realistic-Neural-Talking-Head-Models/master/examples/1%201.png

More examples with the original faces for which the landmarks were extracted from can be seen on my github repo.

https://github.com/vincent-thevenin/Realistic-Neural-Talking-Head-Models

If anyone is interested in using or training the model further or improving the project feel free to take a look and contribute :)

I ended up writing the paper from scratch for learning purposes but I would like to thank u/MrCaracara for doing the first implementation I know of.",0,1
908,2019-7-15,2019,7,15,8,cd9qga,[D] Pointless PhD for Machine Learning career advancement?,https://www.reddit.com/r/MachineLearning/comments/cd9qga/d_pointless_phd_for_machine_learning_career/,actuallynotcanadian,1563146165,"Dear [r/MachineLearning](https://www.reddit.com/r/MachineLearning/), I am a STEM graduate who became interested after my Master's into making research in Machine Learning. I was promised a PhD, the  opportunity to do cutting-edge research, real world applications and a    ""close"" cooperation with industrial partners. But after having  spent a   few months reading and discussing with supervisors, a lot of  work I  am  considered to do is centered around metaheuristic search and    evolutionary computation. And although, I find it fascinating and there    is some application to machine learning / DNNs, as well as companies  like Uber and Cognizant are adopting it, I feel like it has too much of a    niche quality and mainstream interest seems not to be catching-up  with   it. In case if there is any at all to begin with.

I  thought it might be helpful to ask you guys, to get a neutral  outside-of-the-box opinion.

Particularly,   as over the last month I live and work in a scientific bubble and my   prior background is not  AI/ML or Computer Science to begin with. So   anyone might just be able to claim anything to me without me having the   ability to evaluate their claims or get any decent outside criticism.",63,136
909,2019-7-15,2019,7,15,8,cd9ush,"[P] Implementation of Samsung's ""Few-Shot Adversarial Learning of Realistic Neural Talking Head Models""",https://www.reddit.com/r/MachineLearning/comments/cd9ush/p_implementation_of_samsungs_fewshot_adversarial/,CptVifen,1563146842,"Hello,

&amp;#x200B;

I have been working on implementing the model from the paper: \[Few-Shot Adversarial Learning of Realistic Neural Talking Head Models\]([https://arxiv.org/abs/1905.08233v1](https://arxiv.org/abs/1905.08233v1)) (Zakharov et al.) for my own projects and research. It uses a very interesting model of GAN and triggered my interest.

&amp;#x200B;

The paper has been out now for a couple of months and some implementations already exist out there although the results they show are not quite at the level of what is seen in the paper. For my implementation I added further recommendations given by the paper's author on various details that were unclear to me and other existing implementations upon only reading the paper. (added more depth to the network, adjusted adaIN parameters, ...).

&amp;#x200B;

Due to a lack of compute resources at my disposition and due to the model being very heavy, I only trained it on 5 epochs on a test dataset (15 times less epochs than in the paper and with a dataset 34 times smaller) but the results look promising so far for the relatively small amount of training that went into it.

&amp;#x200B;

Here an example of fake faces it generated from facial landmarks and embedding vectors: 

https://i.redd.it/hvcet0xjqca31.png

&amp;#x200B;

More examples with the original faces for which the landmarks were extracted from can be seen on my github repo.

&amp;#x200B;

[https://github.com/vincent-thevenin/Realistic-Neural-Talking-Head-Models](https://github.com/vincent-thevenin/Realistic-Neural-Talking-Head-Models)

&amp;#x200B;

I also did an functioning demo that uses the webcam and an vector to create live fake faces from your own. I have a link to a video of that on my repo and the code for the demo will soon be uploaded.

&amp;#x200B;

If anyone is interested in using or training the model further or improving the project feel free to take a look and contribute :)

&amp;#x200B;

I ended up writing the paper from scratch for learning purposes but I would like to thank u/MrCaracara for doing the first implementation I know of.",9,88
910,2019-7-15,2019,7,15,9,cda8sz,The Great Awakening ebook + audiobook FREE,https://www.reddit.com/r/MachineLearning/comments/cda8sz/the_great_awakening_ebook_audiobook_free/,Wenderu84,1563149050,,1,1
911,2019-7-15,2019,7,15,9,cdafgq,Is this artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/cdafgq/is_this_artificial_intelligence/,cmykcn,1563150057,[removed],0,1
912,2019-7-15,2019,7,15,9,cdajir,Can a machine learning image recognition or digit recognition algorithm decipher the number on the side of this taxi? Lost an important suitcase there on Friday and in need of all help possible,https://www.reddit.com/r/MachineLearning/comments/cdajir/can_a_machine_learning_image_recognition_or_digit/,Omrinachmani,1563150715,,0,1
913,2019-7-15,2019,7,15,10,cdb8vh,Localization and classification of different robots in a factory.,https://www.reddit.com/r/MachineLearning/comments/cdb8vh/localization_and_classification_of_different/,warlock925,1563154842,[removed],0,1
914,2019-7-15,2019,7,15,10,cdba1e,Any method which connects edges in a binary image?,https://www.reddit.com/r/MachineLearning/comments/cdba1e/any_method_which_connects_edges_in_a_binary_image/,nile6499,1563155016,[removed],0,1
915,2019-7-15,2019,7,15,11,cdbuc0,[D] how lucrative of a career is ML to get into?,https://www.reddit.com/r/MachineLearning/comments/cdbuc0/d_how_lucrative_of_a_career_is_ml_to_get_into/,pouyank,1563158274," 

I've been increasingly interested in ML, but I want to focus my time on things that will make me money (for the time being, until I can finally be kind of independent) and would love to make ML the thing I focus lots of my free time on if it does in fact have the potentially to be something I can make a profit off of. I don't know anything about ML as it relates to industry, so I was wondering if anyone could answer this:

if an aspiring tech entrepreneur was to get deep in machine learning, would you say that's time well spent?",14,0
916,2019-7-15,2019,7,15,11,cdbzn5,predicted steering angle vastly different from actual steering angle.,https://www.reddit.com/r/MachineLearning/comments/cdbzn5/predicted_steering_angle_vastly_different_from/,theThinker6969,1563159179,"Hey all,

I am trying to predict the steering angle of my autonomous car. 

    [[-0.5159285]] 0.0
    [[-0.5189788]] -0.0004
    [[-0.5273489]] -0.00035
    [[-0.54270977]] -0.00019
    [[-0.46451133]] -0.00018
    [[-0.4513907]] -0.00018

From the above, I have my predicted steering angle (based on an image) on the left and my ground truth angle on the right. 

I am using 40,000 training samples over 1000 epochs with a 5 layer CNN and 4 layer Dense network with 1 output node (tanh). 

When my epocs were 600 - would get the name predicted output but as positive steering angle. 

Does this mean i just need to run this for longer? maybe 3000 epocs? or should i be adjust the learning rate and other parameters. 

Thanks",0,1
917,2019-7-15,2019,7,15,12,cdc777,News on NLP,https://www.reddit.com/r/MachineLearning/comments/cdc777/news_on_nlp/,erfontes,1563160474,[removed],0,1
918,2019-7-15,2019,7,15,12,cdccqm,"How to upload model into a web app? I need to allow the user to have a forum to fill out, that the model will take in the information and provide a result.",https://www.reddit.com/r/MachineLearning/comments/cdccqm/how_to_upload_model_into_a_web_app_i_need_to/,BobodyBiznus,1563161442,[removed],0,1
919,2019-7-15,2019,7,15,12,cdccz0,News on NLP,https://www.reddit.com/r/MachineLearning/comments/cdccz0/news_on_nlp/,erfontes,1563161481,[removed],0,1
920,2019-7-15,2019,7,15,12,cdcdks,[P] predicted steering angle vastly different from actual steering angle.,https://www.reddit.com/r/MachineLearning/comments/cdcdks/p_predicted_steering_angle_vastly_different_from/,theThinker6969,1563161586,"G'Day all,

I am trying to predict the steering angle of my autonomous car.

    [[-0.5159285]] 0.0
    [[-0.5189788]] -0.0004
    [[-0.5273489]] -0.00035
    [[-0.54270977]] -0.00019
    [[-0.46451133]] -0.00018
    [[-0.4513907]] -0.00018

From the above, I have my predicted steering angle (based on an image) on the left and my ground truth angle on the right.

I am using 40,000 training samples over 1000 epochs with a 5 layer CNN and 4 layer Dense network with 1 output node (tanh). More params can be provided if need be. 

When my epochs were 600 - would get the name predicted output but as positive steering angle.

Does this mean i just need to run this for longer? maybe 3000 epochs? or should i be adjust the learning rate and other parameters.

Thanks",27,0
921,2019-7-15,2019,7,15,12,cdcdrs,[P] Introducing the Politics/The_Donald (PTD) score for analyzing Reddit comments using machine learning,https://www.reddit.com/r/MachineLearning/comments/cdcdrs/p_introducing_the_politicsthe_donald_ptd_score/,hagy,1563161612,,1,0
922,2019-7-15,2019,7,15,13,cdd3km,Need resource recommendation to get started on how ML is used in product ads in search,https://www.reddit.com/r/MachineLearning/comments/cdd3km/need_resource_recommendation_to_get_started_on/,yeshwanthv5,1563166293,[removed],0,1
923,2019-7-15,2019,7,15,14,cdd9pp,"Besides Tensorflow, Keras, Pytorch, Ski-learn, pandas, numpy etc. What libraries are you using / do you like?",https://www.reddit.com/r/MachineLearning/comments/cdd9pp/besides_tensorflow_keras_pytorch_skilearn_pandas/,BatmantoshReturns,1563167396,[removed],0,1
924,2019-7-15,2019,7,15,14,cdd9z8,"[D] Besides Tensorflow, Keras, Pytorch, Ski-learn, pandas, numpy etc. What libraries are you using / do you like?",https://www.reddit.com/r/MachineLearning/comments/cdd9z8/d_besides_tensorflow_keras_pytorch_skilearn/,BatmantoshReturns,1563167445,"I never even heard of Tensor2Tensor or FairSeq before last week and they are the most major ML libraries. I'm probably an extreme exampe, but now I'm curious on what other tools I am missing out on.

Besides the those two, I love adjustText for labeling huge TSNEs and UMAPs.",18,8
925,2019-7-15,2019,7,15,14,cddd8y,"[D] How to compute the ""true"" posterior for a generative model?",https://www.reddit.com/r/MachineLearning/comments/cddd8y/d_how_to_compute_the_true_posterior_for_a/,readinginthewild,1563168055,"I have seen a few papers that show plots of the ""true posterior"" for a generative model on a toy problem.

Before I did not understand how this could be computed, but now maybe I am closer. I'm sure you can help me

&amp;#x200B;

Is this the way to do it? (See below)

&amp;#x200B;

Definitions and setup:

   The generative model is p(x|z)\*p(z).  The posterior is p(z|x).

   A particular x is given and we want to know the distribution p(z|x) for that x,

   using p(z|x) \~ p(x|z)\*p(x) ,

   i.e. without evaluate the Bayes denominator.

&amp;#x200B;

1. Sample z from p(z),
2. evaluate p(x|z) for the given x, and multiply this by p(z) for the z that was sampled in step 1.   This gives an ""unnormalized"" probability for this particular z.
3. Accept this new z as a sample using MCMC, e.g. metropolis-hasting. 
4. Repeat 1-3, and then eventually make a kernel density plot of the resulting z sample locations.",25,6
926,2019-7-15,2019,7,15,15,cde5g3,what is machine learning?,https://www.reddit.com/r/MachineLearning/comments/cde5g3/what_is_machine_learning/,aezioninc,1563173773,[removed],0,1
927,2019-7-15,2019,7,15,15,cde5y3,[N] WCG AI MASTER,https://www.reddit.com/r/MachineLearning/comments/cde5y3/n_wcg_ai_master/,nelmar23,1563173886," WCG made a comeback this year and I can tell that they are fully prepared on this new event that they join in their lineup of games.

This AI master is a robot simulator based 5:5 AI Robotic Soccer Tournament. Each team must bring their own AI algorithm into the game letting the robots play intense soccer matches.   
what do you think in this guys?

![img](5vu4osuy0fa31)",4,0
928,2019-7-15,2019,7,15,15,cde695,RoomNet - A CNN to classify pictures of different rooms of a house/apartment with 88.9 % accuracy,https://www.reddit.com/r/MachineLearning/comments/cde695/roomnet_a_cnn_to_classify_pictures_of_different/,ironhide23586,1563173945,"&amp;#x200B;

https://i.redd.it/lf1xaqo40fa31.png

Codebase at  [https://github.com/ironhide23586/RoomNet](https://github.com/ironhide23586/RoomNet) 

This is a custom neural net I designed to classify an input image in to one of the following 6 classes (in order of their class IDs) -

* Backyard
* Bathroom
* Bedroom
* Frontyard
* Kitchen
* LivingRoom",0,1
929,2019-7-15,2019,7,15,15,cde6b8,Machine Learning Is About To Make A Movie Of Black Holes,https://www.reddit.com/r/MachineLearning/comments/cde6b8/machine_learning_is_about_to_make_a_movie_of/,analyticsindiam,1563173954,,0,1
930,2019-7-15,2019,7,15,16,cdeh9u,[R] Virtual Adversarial Lipschitz Regularization,https://www.reddit.com/r/MachineLearning/comments/cdeh9u/r_virtual_adversarial_lipschitz_regularization/,dterjek,1563176217,,12,76
931,2019-7-15,2019,7,15,17,cdewqv,[Question] Does anyone know of a good image classification survey?,https://www.reddit.com/r/MachineLearning/comments/cdewqv/question_does_anyone_know_of_a_good_image/,Fredbull,1563179659,[removed],0,1
932,2019-7-15,2019,7,15,17,cdexg4,Using a Machine Learning Model in a Web Application Client,https://www.reddit.com/r/MachineLearning/comments/cdexg4/using_a_machine_learning_model_in_a_web/,whitezl0,1563179830,,0,1
933,2019-7-15,2019,7,15,17,cdf0sd,Applied ML course video lectures,https://www.reddit.com/r/MachineLearning/comments/cdf0sd/applied_ml_course_video_lectures/,learnML_ai,1563180601,[removed],0,0
934,2019-7-15,2019,7,15,18,cdfbh1,Need some help in understanding a paper,https://www.reddit.com/r/MachineLearning/comments/cdfbh1/need_some_help_in_understanding_a_paper/,ded_tek,1563182926,[removed],0,1
935,2019-7-15,2019,7,15,18,cdfe5c,Machine Learning Is About To Make A Movie Of Black Holes,https://www.reddit.com/r/MachineLearning/comments/cdfe5c/machine_learning_is_about_to_make_a_movie_of/,analyticsindiam,1563183506,,0,1
936,2019-7-15,2019,7,15,19,cdfzzz,Putting my REST API with a keras model on production won't work,https://www.reddit.com/r/MachineLearning/comments/cdfzzz/putting_my_rest_api_with_a_keras_model_on/,MagicElyas,1563188140,[removed],0,1
937,2019-7-15,2019,7,15,20,cdg25u,What do I do (ft. cryptography)?!,https://www.reddit.com/r/MachineLearning/comments/cdg25u/what_do_i_do_ft_cryptography/,Alostindian,1563188590,[removed],0,1
938,2019-7-15,2019,7,15,20,cdg2xh,Sanctuary AI aims to develop human-like intelligence in robots - Temporal Eternity,https://www.reddit.com/r/MachineLearning/comments/cdg2xh/sanctuary_ai_aims_to_develop_humanlike/,RossPeili,1563188735,,0,1
939,2019-7-15,2019,7,15,20,cdg4vo,Machine learning app development from $12/hr - AIS Technolabs,https://www.reddit.com/r/MachineLearning/comments/cdg4vo/machine_learning_app_development_from_12hr_ais/,AISMarketing,1563189094,,0,1
940,2019-7-15,2019,7,15,20,cdg6k9,TensorCraft: A Brief History of Keras In Production,https://www.reddit.com/r/MachineLearning/comments/cdg6k9/tensorcraft_a_brief_history_of_keras_in_production/,ybubnov,1563189408,,0,1
941,2019-7-15,2019,7,15,20,cdgf6t,"[R] A repository of graph classification research papers with implementations (deep learning, graph kernels, fingerprints, factorization)",https://www.reddit.com/r/MachineLearning/comments/cdgf6t/r_a_repository_of_graph_classification_research/,benitorosenberg,1563191036,"&amp;#x200B;

https://i.redd.it/w64s1gtvfga31.png

&amp;#x200B;

Link: [https://github.com/benedekrozemberczki/awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification)

&amp;#x200B;

The repository covers techniques such as deep learning, graph kernels, statistical fingerprints and factorization. I monthly update it with new papers when something comes out with code.",11,258
942,2019-7-15,2019,7,15,21,cdgnrk,Applied ML course video lectures,https://www.reddit.com/r/MachineLearning/comments/cdgnrk/applied_ml_course_video_lectures/,learnML_ai,1563192567,"Whoever want Applied Machine learning course video lectures, comment/DM your email id here.",0,1
943,2019-7-15,2019,7,15,21,cdgsdy,Why is it So Hard to Integrate Machine Learning into Real Business Applications?,https://www.reddit.com/r/MachineLearning/comments/cdgsdy/why_is_it_so_hard_to_integrate_machine_learning/,IguazioDani,1563193369,,0,1
944,2019-7-15,2019,7,15,21,cdh3fr,Academic PC build with as much Amazon prime day items as possible,https://www.reddit.com/r/MachineLearning/comments/cdh3fr/academic_pc_build_with_as_much_amazon_prime_day/,nomorethancrazy,1563195198,[removed],0,1
945,2019-7-15,2019,7,15,22,cdh7tt,Microsoft ML.NET courses,https://www.reddit.com/r/MachineLearning/comments/cdh7tt/microsoft_mlnet_courses/,ahmed5_27,1563195859,[removed],2,1
946,2019-7-15,2019,7,15,22,cdhci2,"AI-based photo restoration: defects removal, inpainting, colorization",https://www.reddit.com/r/MachineLearning/comments/cdhci2/aibased_photo_restoration_defects_removal/,atomlib_com,1563196587,,0,1
947,2019-7-15,2019,7,15,22,cdhj8g,New XPS 15 for machine learning masters,https://www.reddit.com/r/MachineLearning/comments/cdhj8g/new_xps_15_for_machine_learning_masters/,errminator,1563197655,,0,1
948,2019-7-15,2019,7,15,23,cdhtpt,"Training a machine to upscale low-res images by an artist, using a library of hi-res scans of that artist's work...",https://www.reddit.com/r/MachineLearning/comments/cdhtpt/training_a_machine_to_upscale_lowres_images_by_an/,benjaminography,1563199294,[removed],0,1
949,2019-7-15,2019,7,15,23,cdhwqf,[R] Re-implementation + experiment reproduction of MAPLE: Model Agnostic suPervised Local Explanations (skMAPLE),https://www.reddit.com/r/MachineLearning/comments/cdhwqf/r_reimplementation_experiment_reproduction_of/,givdwiel,1563199731,"I recently stumbled upon a [very interesting blog post](https://blog.ml.cmu.edu/2019/07/13/towards-interpretable-tree-ensembles/), by Gregory Plumb, which introduces a new technique that can generate a local explanation (that captures global patterns and is based on neighboring examples). The intuition/idea is really simple (which makes it even more awesome). For each of the (testing) points for which you want to generate a prediction, you train a linear model where each training sample gets a different weight. In order to calculate the weight of a training sample, you check how ""similar"" it is to the sample for which you want to predict by looking into a created decision tree ensemble.

&amp;#x200B;

Since the technique is that simple (but effective!), I decided to re-implement it (with an interface similar to sklearn) and reproduce the experiment where they compare it to RF. While initial results are slightly different (due to doing less runs), we still see that MAPLE produces better predictive performances than the Random Forest on which it is based, while being interpretable (we can provide a linear model for each prediction, which can give a local explanation).

&amp;#x200B;

The code itself can be found on [Github](https://github.com/IBCNServices/skMAPLE). Hope it is of use to any of you!

&amp;#x200B;

Original paper: [here](http://papers.nips.cc/paper/7518-model-agnostic-supervised-local-explanations)

Original code: [here](https://github.com/GDPlumb/MAPLE)",3,8
950,2019-7-15,2019,7,15,23,cdi1d1,[R] Re-implementation of RDF2Vec: generating embeddings for (RDF) Knowledge Graph entities using random walks and Word2Vec,https://www.reddit.com/r/MachineLearning/comments/cdi1d1/r_reimplementation_of_rdf2vec_generating/,givdwiel,1563200389,"I recently re-implemented [RDF2VEC](http://www.semantic-web-journal.net/content/rdf2vec-rdf-graph-embeddings-and-their-applications-0) completely in Python due to the fact that the provided code in that paper is partially written in Java. RDF2Vec is an unsupervised, task-agnostic algorithm that creates an embedding for different nodes in a Knowledge Graph that can be used for further downstream tasks (such as classification or link prediction). To do this, RDF2Vec first creates ""sentences"" which can be fed to Word2Vec by extracting random walks of a certain depth from the Knowledge Graph. To create a random walk, we initialize its first hop to be one of the specified training entities in our KG. Then, we can iteratively extend our random walk by sampling out of the neighbors from the last hop of our walk.

&amp;#x200B;

The code can be found on [Github](https://github.com/IBCNServices/pyRDF2Vec).

&amp;#x200B;

Original paper: [here](http://www.semantic-web-journal.net/content/rdf2vec-rdf-graph-embeddings-and-their-applications-0) (other, open versions can be found)

Original code (java for walks, python/gensim for word2vec): [here](http://data.dws.informatik.uni-mannheim.de/rdf2vec/)",2,10
951,2019-7-15,2019,7,15,23,cdi2fj,Open Source University,https://www.reddit.com/r/MachineLearning/comments/cdi2fj/open_source_university/,Wenderu84,1563200546,,0,1
952,2019-7-15,2019,7,15,23,cdi3nw,[P] Pre-trained Machine Translation Models of Korean from/to ECJ,https://www.reddit.com/r/MachineLearning/comments/cdi3nw/p_pretrained_machine_translation_models_of_korean/,longinglove,1563200733,"Pre-trained models are beautiful. They save your time, energy and/or money. You can obtain several pre-trained machine translation models for mostly European languageshere. In this project, I add six other models: Korean &lt;-&gt; English, Chinese, Japanese as I failed to find publicly available ones. Not surprisingly, the biggest challenge in training NMT models for those language pairs is the lack of large parallel corpora. I decided to use both public data (OpenSubtitles) and private data) to overcome the difficulties. Overall, each of their performance may not so impressive, but you can keep training it with your own data, if necessary.

https://github.com/Kyubyong/cjk_trans",0,1
953,2019-7-15,2019,7,15,23,cdi7sy,[D] The State of Art of the Mobile Robotics Pipeline (ML Perceptive is welcomed as well),https://www.reddit.com/r/MachineLearning/comments/cdi7sy/d_the_state_of_art_of_the_mobile_robotics/,ugurbolat,1563201323,,0,1
954,2019-7-15,2019,7,15,23,cdi7xc,MITRE is partnering with George Mason to make a first of it's kind AI curriculum for George Mason's Systems Engineering and Operations Research Program,https://www.reddit.com/r/MachineLearning/comments/cdi7xc/mitre_is_partnering_with_george_mason_to_make_a/,aNoKneeMoose,1563201340,,0,1
955,2019-7-15,2019,7,15,23,cdi94z,[P] Primate Face Identification in the Wild,https://www.reddit.com/r/MachineLearning/comments/cdi94z/p_primate_face_identification_in_the_wild/,YaYaBFr,1563201506,"After Faces in the Wild, some researchers published 'Primate Face Identification in the Wild'.  
[https://arxiv.org/pdf/1907.02642.pdf](https://arxiv.org/pdf/1907.02642.pdf)  


They share 3 datasets:  
\-Rhesus Macaque Dataset (7679 images of 93 individuals)  
\-Chimpanzee Dataset C-ZOO (2109 images for 24 identities) of better quality that the following one  
\-Chimpanzee Dataset C-Tai (5057 images for 66 identities)  


The TL;DR of what they say:  
\`\`\`  
We developed a novel face identification approach that is capable of learningpose invariant features, thus allowing to generalize well across poses without the re-quirement of a face alignment step. Additionally, the proposed approach leverages thepairwise constraints to capture underlying data semantics enabling it to perform effec-tively for unseen classes  
\`\`\`",0,3
956,2019-7-15,2019,7,15,23,cdic4h,How can I approach this problem? (dynamic optimization),https://www.reddit.com/r/MachineLearning/comments/cdic4h/how_can_i_approach_this_problem_dynamic/,ema2159,1563201932,"I have the following problem and I'm not quite sure how to approach it:

&amp;#x200B;

https://i.redd.it/9tlr79vxbha31.png

&amp;#x200B;

Representative diagram

&amp;#x200B;

I have a dynamic optimization problem in which I inject an environment with packages with different kinds of features (all representable as integers). I want to maximize certain metric that works like this:

&amp;#x200B;

if I inject a sequence sequence0 of packet0, packet1, and packet5 the metric increases, but if I send it again it may increase the metric or it may not; for example, I may have to send sequence0 3 times to ""fill"" the percentage of the metric that corresponds to that sequence, and once I've sent it 3 times it wont contribute anymore so the metric wont increase when I send sequence0 after that (the 0, 1, 5 of the packages are just to show they're packages with different features, but they can be any sequence like 0,0,2... or 1,4,7... etc).

&amp;#x200B;

The metric (a percentage) accumulates over time so, what I want to do is to find the optimal series of sequences that will maximize the metric in the minimum amount of time starting from a certain point in the metric (for example, my metric could be at 54%, or it may be starting from 0% and from that point I want to get as close as 100% as possible).

&amp;#x200B;

One of the problems I have is that the sequences don't take immediate effect, for example I may send a sequence every tn and if I send a sequence at t0 it may take effect and increase the metric until t100. The data I don't think is a problem because it's a decently big amount of sequences.

&amp;#x200B;

I'm not sure if there's any supervised approach to this problem. I've thought of implementing a RL solution because I its a dynamic optimization problem but I'm not sure of what techniques would be appropriate.

&amp;#x200B;

Is machine/deep learning an option to solve this problem? Why? Why not? if it is, how would you approach it?. I would appreciate any advice/question/opinion.",0,1
957,2019-7-15,2019,7,15,23,cdier1,[R] GENDIS: GENetic DIscovery of Shapelets (to classify timeseries),https://www.reddit.com/r/MachineLearning/comments/cdier1/r_gendis_genetic_discovery_of_shapelets_to/,givdwiel,1563202298,"In the time series classification domain, shapelets are small subseries  that are discriminative for a certain class. It has been shown that by  projecting the original dataset to a distance space, where each axis  corresponds to the distance to a certain shapelet, classifiers are able  to achieve state-of-the-art results on a plethora of datasets. In order to find these shapelets, the current state-of-the-art (in terms of predictive performance) performs a brute-force search that quickly becomes intractable for larger datasets. 

&amp;#x200B;

Therefore, we propose a genetic algorithm that searches for an entire set of shapelets directly. This results in a more scalable algorithm that is competitive to the current state-of-the-art. Moreover, the number of shapelets needed to achieve this competitive performance is several orders of magnitude smaller than the current sota.

&amp;#x200B;

The implementation follows the sklearn interface. You can just simply use \`fit\` and \`transform\` methods. There are [docs](https://gendis.readthedocs.io/en/latest/?badge=latest).

&amp;#x200B;

You can find the code (with an example notebook and tutorial) on [Github](https://github.com/IBCNServices/GENDIS).",4,5
958,2019-7-15,2019,7,15,23,cdigxs,[N] MITRE is partnering with George Mason to make a first of it's kind AI curriculum for George Mason's Systems Engineering and Operations Research Program,https://www.reddit.com/r/MachineLearning/comments/cdigxs/n_mitre_is_partnering_with_george_mason_to_make_a/,aNoKneeMoose,1563202604,,0,1
959,2019-7-15,2019,7,15,23,cdihdl,[R] Classifying nodes in a Knowledge Graph by inducing a decision tree of discriminative walks,https://www.reddit.com/r/MachineLearning/comments/cdihdl/r_classifying_nodes_in_a_knowledge_graph_by/,givdwiel,1563202667,"While deep learning and embedding techniques are getting increasingly popular for tasks related to (knowledge) graphs, they often suffer from being not interpretable, which is key in critical domains such as health care. We propose a simple technique called KG Path Tree which is competitive to current state-of-the-art while being interpretable (we compare it to RDF2Vec and (Relational) Graph CNN). 

&amp;#x200B;

A KG Path Tree is a single decision tree in which each internal node tests for the presence of a certain walk in a sample's graph neighborhood. Our walks are of a specific form: a walk of length \`l\` starts with a root, followed by \`l - 2\` wildcards (\`\*\`) and then a named entity. An example could be: \`root -&gt; \* -&gt; \* -&gt; \* -&gt; Ghent\` which would match the walk \`Gilles Vandewiele --&gt; studiedAt --&gt; Ghent University --&gt; locatedIn --&gt; Ghent\` when classifying \`Gilles Vandewiele\`. The final decision tree can then be used for classification of unseen samples. The path from the root to the prediction can easily be displayed (local explanation) and the model can be inspected (global explanation).

&amp;#x200B;

All code can be found on [Github](https://github.com/IBCNServices/KGPTree).",0,2
960,2019-7-16,2019,7,16,0,cdiqvo,[D] Kai-Fu Lee: AI Superpowers - China and Silicon Valley,https://www.reddit.com/r/MachineLearning/comments/cdiqvo/d_kaifu_lee_ai_superpowers_china_and_silicon/,UltraMarathonMan,1563203946,"Kai-Fu Lee is the Chairman and CEO of Sinovation Ventures that manages a 2 billion dollar dual currency investment fund with a focus on developing the next generation of Chinese high-tech companies. He is the former President of Google China and the founder of what is now called Microsoft Research Asia, an institute that trained many of the AI leaders in China, including CTOs or AI execs at Baidu, Tencent, Alibaba, Lenovo, and Huawei. He was named one of the 100 most influential people in the world by TIME Magazine. He is the author of seven best-selling books in Chinese, and most recently the New York Times best seller called AI Superpowers: China, Silicon Valley, and the New World Order. This conversation is part of the Artificial Intelligence podcast.

**Video:** [https://www.youtube.com/watch?v=cQ48rP\_Rs4g](https://www.youtube.com/watch?v=cQ48rP_Rs4g)

**Audio:** [https://lexfridman.com/kai-fu-lee](https://lexfridman.com/kai-fu-lee)

https://i.redd.it/edwkrct9iha31.png

**Outline:**

 

0:00 - Introduction

1:26 - Chinese soul

4:28 - Difference between cultures of AI engineering

6:39 - Role of data in near-term impact of AI

8:37 - Tesla Autopilot approach

11:56 - Microsoft, Google, Apple and Silicon Valley cultures

24:22 - Entrepreneurship in China

38:51 - Impact of AI on jobs

44:58 - Andrew Yang and UBI

48:38 - Jobs that can't be automated

56:20 - Role for governments

58:30 - Cold War and the arms race metaphor

1:04:50 - Freedom of speech &amp; different value systems in China &amp; US

1:07:37 - Privacy challenges

1:12:27 - Heart and soul of a business

1:14:00 - Facing mortality

1:18:46 - Hard work and balance

1:22:12 - Advice to entrepreneurs

1:25:38 - First question for an AGI system",0,1
961,2019-7-16,2019,7,16,0,cdirhp,Faculty.ai fellowship London: yay or nay?,https://www.reddit.com/r/MachineLearning/comments/cdirhp/facultyai_fellowship_london_yay_or_nay/,meechosch,1563204028,[removed],0,1
962,2019-7-16,2019,7,16,0,cdivgi,[N] MITRE is partnering with George Mason to create a curriculum blending machine learning with systems engineering,https://www.reddit.com/r/MachineLearning/comments/cdivgi/n_mitre_is_partnering_with_george_mason_to_create/,aNoKneeMoose,1563204570,"As part of MITRE's ""Generation Ai Nexus"" effort bringing machine learning education to people at all levels of the work force, they have partnered with George Mason to create a curriculum blending AI &amp; Systems Engineering. So far they created a pilot for a Risk Management course including students with no prior ML experience. The goal is to have students learn how to apply machine learning tools and methodologies to real world data to create more ""meaningful"" predictions, especially for emerging technologies.

Learn more and listen to the Interview here: https://kde.mitre.org/blog/2019/07/15/interview-with-dr-philip-barry-on-blending-ai-and-education/",0,3
963,2019-7-16,2019,7,16,0,cdj0sk,Post bacc research before MS CS?,https://www.reddit.com/r/MachineLearning/comments/cdj0sk/post_bacc_research_before_ms_cs/,omarkhursheed,1563205291,[removed],0,1
964,2019-7-16,2019,7,16,0,cdj0tj,understanding discriminative vs generative learning,https://www.reddit.com/r/MachineLearning/comments/cdj0tj/understanding_discriminative_vs_generative/,hovnatan,1563205296,"What is a good reference for understanding discriminative vs generative learning paradigms? I read on it in Bishop's Pattern Recognition book, but I kind of don't understand it fully without good examples.",0,1
965,2019-7-16,2019,7,16,0,cdj639,[D] Video lectures of CS189 @ Berkeley,https://www.reddit.com/r/MachineLearning/comments/cdj639/d_video_lectures_of_cs189_berkeley/,Best_Approximation,1563206012," 

Can anyone share the lecture videos from  Machine Learning CS189 from Berkeley. The youtube links in the[course webpage](https://people.eecs.berkeley.edu/~jrs/189/) are not available. I was wondering if someone would be willing to share it? =)",4,16
966,2019-7-16,2019,7,16,1,cdj9pe,[P] Ideas for games that ML would work well for?,https://www.reddit.com/r/MachineLearning/comments/cdj9pe/p_ideas_for_games_that_ml_would_work_well_for/,Killermise,1563206466,"I'm taking up a project on machine learning for game playing, where my aim is to build a computer player for any game. I've considered board games and simple video games, but nothing really sticks out in my mind. Has anyone got any ideas for a game (one that's not overdone to death like Go or Chess, or extremely complicated like an open-world game) that would work well for this project?",12,3
967,2019-7-16,2019,7,16,1,cdjcuh,I'm using RL for hedge fund management,https://www.reddit.com/r/MachineLearning/comments/cdjcuh/im_using_rl_for_hedge_fund_management/,MarcSapporo,1563206886,"Here's a link to Part I of a series of papers. Part II will be up this fall. I hope it's an enjoyable read! 

&amp;#x200B;

[https://drc.lib.uh.edu/research/projects/artificial-hedging-intelligence/](https://drc.lib.uh.edu/research/projects/artificial-hedging-intelligence/)",0,0
968,2019-7-16,2019,7,16,1,cdjgcm,[D] Is Hogwild still the way to do distributed training,https://www.reddit.com/r/MachineLearning/comments/cdjgcm/d_is_hogwild_still_the_way_to_do_distributed/,TalkingJellyFish,1563207336,"Is this what many/most/some are using to do distributed training today ? What are alternatives and why are they better ? 

[https://arxiv.org/abs/1106.5730](https://arxiv.org/abs/1106.5730)

Thanks!",3,5
969,2019-7-16,2019,7,16,1,cdjhb6,[D] Why is Tensorflow so slow (compared to FFM)?,https://www.reddit.com/r/MachineLearning/comments/cdjhb6/d_why_is_tensorflow_so_slow_compared_to_ffm/,DstnB3,1563207469,"I have ~500GB of extremely sparse, tabular data with millions of features and a single binary target. I have been training FFM models with https://github.com/cttsai1985/libffm and they take around 6 hours to train. This model creates millions of cross-features on top of the millions of original features and trains parameters for all of them.

As complex of a model as is it, that FFM model only takes 6 hours to train and converge after ~10 epochs on 500GB data on a single machine with ~ 8 cores. On the other hand, training an extremely simple tensorflow model on this dataset (embedding layer -&gt; sigmoid output neuron) takes almost 2 days for a single epoch on this dataset. I've optimized everywhere I can think of, using tf.data api, etc, it seems like tensorflow is just really slow compared to FFM. What's the deal?",29,71
970,2019-7-16,2019,7,16,1,cdjx7a,Autocompletion with deep learning,https://www.reddit.com/r/MachineLearning/comments/cdjx7a/autocompletion_with_deep_learning/,jacob-jackson,1563209473,,0,1
971,2019-7-16,2019,7,16,2,cdk48b,[D] [Deepfake] Keanu Reeves saves from robber,https://www.reddit.com/r/MachineLearning/comments/cdk48b/d_deepfake_keanu_reeves_saves_from_robber/,PlayfulConfidence,1563210271,"[https://youtu.be/\_Yhelbz7\_L0](https://youtu.be/_Yhelbz7_L0)

 This video was created using Deepfake technology.  If the double-360-neck-snap didnt give it away: THIS ENTIRE PIECE WAS STAGED. If youd like to see how we made it, check out the CORRIDOR CREW  [http://bit.ly/Subscribe\_Corridor\_VLOG](https://www.youtube.com/redirect?v=_Yhelbz7_L0&amp;event=video_description&amp;q=http%3A%2F%2Fbit.ly%2FSubscribe_Corridor_VLOG&amp;redir_token=V1nV89zl0Zy6evw10RYsLS4QAvR8MTU2MzI5NjYxOEAxNTYzMjEwMjE4)   CAST  Reuben Langdon as Keanu Brett Driver as Robber Sam Gorski as Fan Niko Pueringer as Friend With Phone Christian Fergerstrom as Store Clerk Matthew Cairns as Cop",0,1
972,2019-7-16,2019,7,16,2,cdkjtd,Getting up to speed with metalearning and few shot learning,https://www.reddit.com/r/MachineLearning/comments/cdkjtd/getting_up_to_speed_with_metalearning_and_few/,HenryJia,1563212067,[removed],0,1
973,2019-7-16,2019,7,16,2,cdkoii,Did anyone got admitted to a top Machine learning graduate program I have some few question?,https://www.reddit.com/r/MachineLearning/comments/cdkoii/did_anyone_got_admitted_to_a_top_machine_learning/,yas4545,1563212608,,0,1
974,2019-7-16,2019,7,16,3,cdl52b,[R] Migration error detection,https://www.reddit.com/r/MachineLearning/comments/cdl52b/r_migration_error_detection/,gursi1,1563214539,"When a Table is moved from server A to B, errors are expected to creep in. What is the fastest way to find how close are 2 Tables (or matrices)?",2,1
975,2019-7-16,2019,7,16,3,cdl5wb,[N] Alan Turing will be on 50 note,https://www.reddit.com/r/MachineLearning/comments/cdl5wb/n_alan_turing_will_be_on_50_note/,cAtloVeR9998,1563214643,,0,1
976,2019-7-16,2019,7,16,3,cdl7if,Weakly Supervised Object Detection In Practice,https://www.reddit.com/r/MachineLearning/comments/cdl7if/weakly_supervised_object_detection_in_practice/,tahaemara,1563214826,[removed],0,1
977,2019-7-16,2019,7,16,3,cdlazk,Newbie question regarding the cost function,https://www.reddit.com/r/MachineLearning/comments/cdlazk/newbie_question_regarding_the_cost_function/,20gunasarj,1563215245,[removed],0,1
978,2019-7-16,2019,7,16,3,cdlpvv,Image classification - where to start?,https://www.reddit.com/r/MachineLearning/comments/cdlpvv/image_classification_where_to_start/,wymco,1563216975,[removed],0,1
979,2019-7-16,2019,7,16,3,cdlqn8,[D] what is fair compensation for a machine learning manager,https://www.reddit.com/r/MachineLearning/comments/cdlqn8/d_what_is_fair_compensation_for_a_machine/,ElectronicSet0,1563217065,I'm a PhD with publications at top conferences. I have 2 years of experience leading NLP/RL projects(team size generally 2-3). A company that just finished their series A is asking me to manage their machine learning team(5 people). Roughly what should I expect as fair compensation(based out of San Francisco)?,26,28
980,2019-7-16,2019,7,16,3,cdlrrk,Surprising Sorting Tips for Data Scientists,https://www.reddit.com/r/MachineLearning/comments/cdlrrk/surprising_sorting_tips_for_data_scientists/,discdiver,1563217194,[removed],0,1
981,2019-7-16,2019,7,16,4,cdm4u9,CMU Creates Language2Pose Model that Generates Animations From Text,https://www.reddit.com/r/MachineLearning/comments/cdm4u9/cmu_creates_language2pose_model_that_generates/,Yuqing7,1563218744,,0,1
982,2019-7-16,2019,7,16,4,cdm7v1,[Funny]: Deep Learning datasets,https://www.reddit.com/r/MachineLearning/comments/cdm7v1/funny_deep_learning_datasets/,falcor_defender,1563219118,,0,1
983,2019-7-16,2019,7,16,4,cdm8np,[D] Deyploy an keras CNN with tensorflow serve (accepting base64 images),https://www.reddit.com/r/MachineLearning/comments/cdm8np/d_deyploy_an_keras_cnn_with_tensorflow_serve/,ixeption,1563219209,"Hi folks, 

I just used tensorflow serve to deploy a keras model and wondered how to make it accepting base64 encoding images. It was not as easy as I thought, but I got it working. I think it might be helpful for others, so I wrote a post about it. If you have some additional ideas, feel free to comment. 

[Here is the post.](http://digital-thinking.de/how-to-deyploy-an-keras-cnn-with-tensorflow-serve-including-jpeg-decoding/)

Cheers",1,2
984,2019-7-16,2019,7,16,4,cdmf61,Project: Group lasso regularisation in Python (following sklearn API),https://www.reddit.com/r/MachineLearning/comments/cdmf61/project_group_lasso_regularisation_in_python/,[deleted],1563219995,[deleted],0,1
985,2019-7-16,2019,7,16,4,cdmhh0,[Project] Fast group lasso in Python,https://www.reddit.com/r/MachineLearning/comments/cdmhh0/project_fast_group_lasso_in_python/,yngvizzle,1563220280,"**Group lasso in Python**

I recently wanted group lasso regularised linear regression, and it was not available in scikit-learn. Therefore, I decided to create my own little implementation of it and I ended up becoming borderline obsessive on figuring out how to do it properly.

&amp;#x200B;

Here is the github link: [https://github.com/yngvem/group-lasso](https://github.com/yngvem/group-lasso)

&amp;#x200B;

This my first publicly shared open-source project so I'd be delighted for any feedback you might have.

&amp;#x200B;

**Information about the package**

Group lasso is a regularisation algorithm used in statistics/machine learning/data science when you have several measurements from different sources and want only a few of the sources to be used in prediction. Also, this implementation is FAST. I am currently sitting on a moderately priced laptop and I can fit models with 10 000 000 rows and 500 columns without it struggling.

&amp;#x200B;

**User guide**

The package can easily be pip-installed by typing `pip install group-lasso`. After that it's as simple as creating a `GroupLasso` instance and calling the `GroupLasso.fit(X, y)` method. A full description is in the readme at GitHub.

&amp;#x200B;

**Future work**

I am currently working on implementing the same update scheme for logistic regression. I have currently done this for the one-class sigmoid based logistic regression, and it seems to work. The next step is implementing it for the multi-class softmax based logistic regression and testing it on some datasets. This all takes some time, since I need to derive some mathematical constants for the optimisation algorithm to work (Lipschitz bounds of the gradient to be specific).

&amp;#x200B;

There are other parts that I am working on as well. I should probably have support for Python 3.5, not just 3.6 and I think that should be fixable if I only remove the f-strings and the underscores in the numbers. I also hope to get Sphinx documentation up and running, but that will probably be for after the summer.

&amp;#x200B;

Another facet for future work is support for sparse matrices. I don't think that should be too difficult, but I haven't worked much with them till now so I don't know how much of a problem that will be.

&amp;#x200B;

**Mathematical background**

Solving the group lasso problem involves solving an optimisation problem that in some senses are difficult (for the interested: it is non smooth, but luckily convex). Normally the group lasso problem is solved using an algorithm called block-coordinate descent, which can be slow. Therefore, I implemented the update scheme for a newer optimisation algorithm called FISTA.",4,11
986,2019-7-16,2019,7,16,4,cdmii1,"With tensorflow moving to a keras-like direction, is there any reason to use the core Keras library?",https://www.reddit.com/r/MachineLearning/comments/cdmii1/with_tensorflow_moving_to_a_keraslike_direction/,throwawaychives,1563220403,[removed],0,1
987,2019-7-16,2019,7,16,5,cdmtm5,Running effective ML teams,https://www.reddit.com/r/MachineLearning/comments/cdmtm5/running_effective_ml_teams/,CometML,1563221774,,0,1
988,2019-7-16,2019,7,16,6,cdnff1,"Just a newcomer trying to share his ""projects"" and to find people to do fun ml-dl-related stuff with!",https://www.reddit.com/r/MachineLearning/comments/cdnff1/just_a_newcomer_trying_to_share_his_projects_and/,mishazakharovbmx,1563224584,[removed],0,1
989,2019-7-16,2019,7,16,6,cdnj1a,Managing machine learning in the enterprise: Lessons from banking and health care,https://www.reddit.com/r/MachineLearning/comments/cdnj1a/managing_machine_learning_in_the_enterprise/,gradientflow,1563225049,,0,1
990,2019-7-16,2019,7,16,6,cdnp6g,Competitiveness for CS/ML Grad Programs (biochem background) (xpost /r/gradadmissions),https://www.reddit.com/r/MachineLearning/comments/cdnp6g/competitiveness_for_csml_grad_programs_biochem/,ProspectiveCS,1563225851,[removed],0,1
991,2019-7-16,2019,7,16,6,cdnphz,Type of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cdnphz/type_of_neural_networks/,ai-lover,1563225896,,0,1
992,2019-7-16,2019,7,16,6,cdnzeq,[N] NeurIPS 2019 Traffic4cast Competition,https://www.reddit.com/r/MachineLearning/comments/cdnzeq/n_neurips_2019_traffic4cast_competition/,iarai_competition,1563227231,"
Link to competition: [Traffic4cast -- Traffic Map Movie Forecasting](https://traffic4cast.iarai.ac.at/)

Predict high resolution traffic flow volume, heading, and speed on a whole city map looking 15 minutes into the future!
Kicking off a series of annual competitions, this year's data is based on 100 billion probe points from 3 cities mapped in 5 minute intervals, showing trends across weekdays and seasonal effects.
Improved traffic predictions are of great social, environmental, and economic value, while also advancing our general ability to capture the simple implicit rules underlying a complex system and model its states.

[Join us](https://www.iarai.ac.at/traffic4cast/local-signup/) for access to challenge Contest data data and code.

We provide a unique data set derived from trajectories of raw GPS position fixes (consisting of a latitude, a longitude, a time stamp, as well as the vehicle speed and driving direction recorded at the time). The data is made available by HERE Technologies and originates from a large fleet of probe vehicles which recorded their movements in multiple culturally and socially diverse metropolitan areas around the world throughout the course of an entire year. The overall number of raw probe-points that we share with the scientific community is based on the unprecedented number of over 1011probe-points, corresponding to over 300,000 frames, which at a 24 frame/s play rate would give in excess of three hours data-movie footage.

To assist you getting up and running, IARAI provides a number of useful utilities, Loader code, and some Baseline Models through the Traffic4cast project at [Github](https://github.com/iarai/NeurIPS2019-traffic4cast).
The project is open to contributions, so please feel free to share your code, too!
Once you have first results, you can test them in the Core competition by submitting your predictions, with scores available within an hour on the Core leaderboard. The Submission system will be open for submissions from 15 July until 15 October.
The three top-ranked teams in the core competition leaderboard are honoured at NeurIPS and receive up to $10,000 cash value prizes. Special awards include fully funded 12-month Research Fellowships at IARAI in Vienna.
Please use the [Competition Forum](https://www.iarai.ac.at/traffic4cast/forums/forum/competition-beta-phase/) for discussions!

We look forward to your contributions and seeing you in Toronto.

For the Organizing Team at IARAI,Sepp Hochreiter, David Kreil, and Michael Kopp

Key Dates:
15 Jul  ... Leaderboard live
15 Oct ... Submission deadline
30 Oct ... Award announcement, symposion invitations
 7 Dec ... Traffic4cast symposion Toronto
 8 Dec ... NeurIPS",4,37
993,2019-7-16,2019,7,16,6,cdo31h,[D] Spectroscopy Binary Classification,https://www.reddit.com/r/MachineLearning/comments/cdo31h/d_spectroscopy_binary_classification/,Clssq,1563227736,"I'm trying to build a pipeline to detect the presence of a compound in spectroscopic data.  I have \~100 samples that consist of vectors of size 1000 representing a specific frequency band. At first, I was thinking of using a CNN because I'm trying to detect patterns, but these patterns are location dependent since they represent absorption at a given frequency. I'm not sure where to go from here so any advice is appreciated.",8,4
994,2019-7-16,2019,7,16,7,cdoo3j,A Criteria for Evaluating AI Projects and Applications with Sergey Karayev,https://www.reddit.com/r/MachineLearning/comments/cdoo3j/a_criteria_for_evaluating_ai_projects_and/,weightsandbiases,1563230666,,0,1
995,2019-7-16,2019,7,16,9,cdpj37,"Instale o navegador CryptoTab e comece a ganhar dinheiro extra apenas por utiliz-lo. Desde que veja vdeos no YouTube, leia notcias e utilize as redes sociais, como faz diariamente, ganha Bitcoins verdadeiras.",https://www.reddit.com/r/MachineLearning/comments/cdpj37/instale_o_navegador_cryptotab_e_comece_a_ganhar/,KarloMelo,1563235242,,0,1
996,2019-7-16,2019,7,16,9,cdprqh,[D] Bilingual speech corpus?,https://www.reddit.com/r/MachineLearning/comments/cdprqh/d_bilingual_speech_corpus/,HXSC,1563236581,"I'm wondering what kind of bilingual speech corpora there are for training purposes. The Fisher-Callhome corpus is widely used, but as somebody who just wants to play and build a speech translation system, its $500 fee is too high. 

Has anybody used any other bilingual speech corpora that is reasonably sized?",5,1
997,2019-7-16,2019,7,16,11,cdqt84,[D] Metric of the target space of word embedding,https://www.reddit.com/r/MachineLearning/comments/cdqt84/d_metric_of_the_target_space_of_word_embedding/,chan_y_park,1563242440,"It is well known that, when we train a word embedding using one of various models, we can perform a vector arithmetic that reflects semantics. Is this just an empirical result, or does a model and/or a training loss guarantees such an embedding?",4,13
998,2019-7-16,2019,7,16,11,cdr8pg,Python for Everybody,https://www.reddit.com/r/MachineLearning/comments/cdr8pg/python_for_everybody/,HannahHumphreys,1563244859,[removed],0,1
999,2019-7-16,2019,7,16,11,cdraof,Handwritten Words data set,https://www.reddit.com/r/MachineLearning/comments/cdraof/handwritten_words_data_set/,AWOLASAP,1563245180,[removed],0,1
1000,2019-7-16,2019,7,16,12,cdrqqk,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/cdrqqk/data_science_career_track_prep_course/,HannahHumphreys,1563247777,[removed],0,1
1001,2019-7-16,2019,7,16,12,cdrv66,"What is Mean Squared Error, Mean Absolute Error, Root Mean Squared Error and R Squared",https://www.reddit.com/r/MachineLearning/comments/cdrv66/what_is_mean_squared_error_mean_absolute_error/,studytonight,1563248510,,0,1
1002,2019-7-16,2019,7,16,13,cdskkz,"What online course, book, or resource would you recommend to start learning machine learning (I know this question has been asked a lot)?",https://www.reddit.com/r/MachineLearning/comments/cdskkz/what_online_course_book_or_resource_would_you/,Shittynectarine,1563253058,"I'm about to go into my junior year of college and I study math and computer science, so I have solid foundations in linear algebra, probability, and some of the other basic stuff. However, I've really never used python surprisingly and have not learned any machine learning. Would you guys recommend any different resources than the ones mentioned in many posts asking this same question?",0,1
1003,2019-7-16,2019,7,16,14,cdsnm2,"[N] Intel ""neuromorphic"" chips can crunch deep learning tasks 1,000 times faster than CPUs",https://www.reddit.com/r/MachineLearning/comments/cdsnm2/n_intel_neuromorphic_chips_can_crunch_deep/,MassivePellfish,1563253633,"**Intel's ultra-efficient AI chips can power prosthetics and self-driving cars**
They can crunch deep learning tasks 1,000 times faster than CPUs.

https://www.engadget.com/2019/07/15/intel-neuromorphic-pohoiki-beach-loihi-chips/

&gt; Even though the whole 5G thing didn't work out, Intel is is still working on hard on its Loihi ""neuromorphic"" deep-learning chips, modeled after the human brain. It unveiled a new system, code-named Pohoiki Beach, made up of 64 Loihi chips and 8 million so-called neurons. It's capable of crunching AI algorithms up to 1,000 faster and 10,000 times more efficiently than regular CPUs for use with autonomous driving, electronic robot skin, prosthetic limbs and more.
&gt; 
&gt; The Loihi chips are installed on a ""Nahuku"" board that contains from 8 to 32 Loihi chips. The Pohoiki Beach system contains multiple Nahuku boards that can be interfaced with Intel's Arria 10 FPGA developer's kit, as shown above.
&gt; 
&gt; Pohoiki Beach will be very good at neural-like tasks including sparse coding, path planning and simultaneous localization and mapping (SLAM). In layman's terms, those are all algorithms used for things like autonomous driving, indoor mapping for robots and efficient sensing systems. For instance, Intel said that the boards are being used to make certain types of prosthetic legs more adaptable, powering object tracking via new, efficient event cameras, giving tactile input to an iCub robot's electronic skin, and even automating a foosball table.
&gt; 
&gt; The Pohoiki system apparently performed just as well as GPU/CPU-based systems, while consuming a lot less power -- something that will be critical for self-contained autonomous vehicles, for instance. "" We benchmarked the Loihi-run network and found it to be equally accurate while consuming 100 times less energy than a widely used CPU-run SLAM method for mobile robots,"" Rutgers' professor Konstantinos Michmizos told Intel.
&gt; 
&gt; Intel said that the system can easily scale up to handle more complex problems and later this year, it plans to release a Pohoiki Beach system that's over ten times larger, with up to 100 million neurons. Whether it can succeed in the red-hot, crowded AI hardware space remains to be seen, however.",134,339
1004,2019-7-16,2019,7,16,14,cdswr6,Microsoft Azure vs Amazon AWS vs Google Cloud Platform A Comparison,https://www.reddit.com/r/MachineLearning/comments/cdswr6/microsoft_azure_vs_amazon_aws_vs_google_cloud/,subhamroy021,1563255412,,0,1
1005,2019-7-16,2019,7,16,15,cdtfi1,[R] What does it mean to understand a neural network?,https://www.reddit.com/r/MachineLearning/comments/cdtfi1/r_what_does_it_mean_to_understand_a_neural_network/,baylearn,1563259170,,2,3
1006,2019-7-16,2019,7,16,16,cdtpin,[R] Faster Neural Network Training with Data Echoing,https://www.reddit.com/r/MachineLearning/comments/cdtpin/r_faster_neural_network_training_with_data_echoing/,sanxiyn,1563261116,,1,8
1007,2019-7-16,2019,7,16,17,cdu5cq,Nunchaku Tricks finger rolls and syncopated wrist rolls,https://www.reddit.com/r/MachineLearning/comments/cdu5cq/nunchaku_tricks_finger_rolls_and_syncopated_wrist/,thetrickshotone,1563264426,,0,1
1008,2019-7-16,2019,7,16,19,cdv6xs,"How do big companies (Google, Facebook) deal with large datasets (larger than RAM) in deep learning? Using pipline or just load data full RAM by full RAM or using hadoop?",https://www.reddit.com/r/MachineLearning/comments/cdv6xs/how_do_big_companies_google_facebook_deal_with/,monkey3233,1563272545,[removed],0,1
1009,2019-7-16,2019,7,16,19,cdveg5,Using ML to optimize constants of a formula for minimizing error with a real mesure,https://www.reddit.com/r/MachineLearning/comments/cdveg5/using_ml_to_optimize_constants_of_a_formula_for/,augustin97wee,1563274040,[removed],0,1
1010,2019-7-16,2019,7,16,20,cdvo9e,What can be the real world application of scene Understanding??,https://www.reddit.com/r/MachineLearning/comments/cdvo9e/what_can_be_the_real_world_application_of_scene/,MEETJAINAI,1563275812,[removed],0,1
1011,2019-7-16,2019,7,16,20,cdvw57,Ready to use visual AI? (for 3d printer bed level calibration),https://www.reddit.com/r/MachineLearning/comments/cdvw57/ready_to_use_visual_ai_for_3d_printer_bed_level/,sp4c3c4k3,1563277167,[removed],0,1
1012,2019-7-16,2019,7,16,20,cdvxp1,Machine Learning books suggested by top experts like Ypshua Bengio (U Montreal Professor) and Daphne Koller (Coursera co-founder),https://www.reddit.com/r/MachineLearning/comments/cdvxp1/machine_learning_books_suggested_by_top_experts/,ThisToni,1563277436,,0,1
1013,2019-7-16,2019,7,16,20,cdvzsu,Generalized Dynamical Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cdvzsu/generalized_dynamical_machine_learning/,andrea_manero,1563277805,[removed],0,1
1014,2019-7-16,2019,7,16,21,cdw3f1,Comprehensive Repository of Data Science and ML Resources,https://www.reddit.com/r/MachineLearning/comments/cdw3f1/comprehensive_repository_of_data_science_and_ml/,andrea_manero,1563278408,[removed],0,1
1015,2019-7-16,2019,7,16,21,cdw4n8,[D] What method do you use for Image Registration?,https://www.reddit.com/r/MachineLearning/comments/cdw4n8/d_what_method_do_you_use_for_image_registration/,emnak,1563278595,,0,3
1016,2019-7-16,2019,7,16,21,cdw77i,Elon Musks brain-interface company is promising big news. Heres what it could be. - Temporal Eternity,https://www.reddit.com/r/MachineLearning/comments/cdw77i/elon_musks_braininterface_company_is_promising/,RossPeili,1563279017,,0,1
1017,2019-7-16,2019,7,16,21,cdw91q,bin washing machinery manufacturers in Hyderabad 044 48642264,https://www.reddit.com/r/MachineLearning/comments/cdw91q/bin_washing_machinery_manufacturers_in_hyderabad/,Ultramaxhydrojet,1563279289,,0,1
1018,2019-7-16,2019,7,16,21,cdwdew,[R] Prior Activation Distribution (PAD): A Versatile Representation to Utilize DNN Hidden Units,https://www.reddit.com/r/MachineLearning/comments/cdwdew/r_prior_activation_distribution_pad_a_versatile/,AskLbm,1563280004,"Link: [https://arxiv.org/pdf/1907.02711.pdf](https://arxiv.org/pdf/1907.02711.pdf)

&amp;#x200B;

In this paper, we introduce the concept of Prior Activation Distribution (PAD) as a versatile and general technique to capture the typical activation patterns of hidden layer units of a Deep Neural Network used for classification tasks. We show  that  the  combined  neural  activations  of  such  a  hidden  layer  have  class-specific distributional properties, and then define multiple statistical measures to compute how far a test samples activations deviate from such distributions. Using a variety of benchmark datasets (including MNIST, CIFAR10, Fashion-MNIST &amp; notMNIST), we show how such PAD-based measures can be used, independent of  any  training  technique,  to  (a)  derive  fine-grained  uncertainty  estimates  for inferences; (b) provide inferencing accuracy competitive with alternatives that require execution of the full pipeline, and (c) reliably isolate out-of-distribution test samples.",2,1
1019,2019-7-16,2019,7,16,21,cdwi1q,Looking for good ML Podcast episodes,https://www.reddit.com/r/MachineLearning/comments/cdwi1q/looking_for_good_ml_podcast_episodes/,Sunvial,1563280758,[removed],0,1
1020,2019-7-16,2019,7,16,21,cdwj0n,[D] Looking for advice on my PC build for ML,https://www.reddit.com/r/MachineLearning/comments/cdwj0n/d_looking_for_advice_on_my_pc_build_for_ml/,vekony_arnyekos,1563280926,"Hey,

So I'm about to build my first ever PC, which I'm going to use mainly for NLP tasks. My budget is around 1100 ($1200). I've read Tim Dettmers' blog [post](https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/) about GPUs, however he didn't update it yet and RTX Super cards got released. 

At this point I'm thinking about going with an RTX 2070 Super, I'm not sure however if the price differences between the 2060 Super, 2070 and the 2070 Super are even worth it. A quick comparison:

&amp;#x200B;

|| RTX 2070 Super |RTX 2070|RTX 2060 Super|
|:-|:-|:-|:-|
|**Tensor cores** |320|288|272|
|**Memory**|8GB|8GB|8GB|
|**Bandwidth** |448GBps|448GBps|448GBps|
|**Price**|$500|$480|$400|

&amp;#x200B;

Also, this is what I came up with so far. If anyone would add any suggestions on this as well, it would be greatly appreciated.

&amp;#x200B;

&amp;#x200B;

[PCPartPicker Part List](https://de.pcpartpicker.com/list/TtM6TB)

Type|Item|Price
:----|:----|:----
**CPU** | [AMD - Ryzen 5 3600 3.6 GHz 6-Core Processor](https://de.pcpartpicker.com/product/9nm323/amd-ryzen-5-3600-36-thz-6-core-processor-100-100000031box) | 209.00 @ Alternate 
**Motherboard** | [MSI - B450 TOMAHAWK ATX AM4 Motherboard](https://de.pcpartpicker.com/product/Hy97YJ/msi-b450-tomahawk-atx-am4-motherboard-b450-tomahawk) | 104.84 @ Amazon Deutschland 
**Memory** | [Corsair - Vengeance LPX 16 GB (2 x 8 GB) DDR4-3200 Memory](https://de.pcpartpicker.com/product/p6RFf7/corsair-memory-cmk16gx4m2b3200c16) | 83.80 @ Amazon Deutschland 
**Storage** | [ADATA - XPG SX8200 Pro 512 GB M.2-2280 Solid State Drive](https://de.pcpartpicker.com/product/kVzkcf/adata-xpg-sx8200-pro-512-gb-m2-2280-solid-state-drive-asx8200pnp-512gt-c) | 84.75 @ Amazon Deutschland 
**Video Card** | [Gigabyte - GeForce RTX 2070 SUPER 8 GB GAMING OC Video Card](https://de.pcpartpicker.com/product/JgsnTW/gigabyte-geforce-rtx-2070-super-8-gb-gaming-oc-video-card-gv-n207sgaming-oc-8gc) | 529.00 
**Case** | [Fractal Design - Meshify C ATX Mid Tower Case](https://de.pcpartpicker.com/product/BBrmP6/fractal-design-meshify-c-white-tg-atx-mid-tower-case-fd-ca-mesh-c-wt-tgc) | 84.90 @ Caseking 
**Power Supply** | [Corsair - RMx (2018) 650 W 80+ Gold Certified Fully Modular ATX Power Supply](https://de.pcpartpicker.com/product/2HbwrH/corsair-rmx-2018-650w-80-gold-certified-fully-modular-atx-power-supply-cp-9020178-na) | 99.90 @ Amazon Deutschland 
 | *Prices include shipping, taxes, rebates, and discounts* |
 | **Total** | **1196.19**
 | Generated by [PCPartPicker](https://pcpartpicker.com) 2019-07-16 14:41 CEST+0200 |",1,0
1021,2019-7-16,2019,7,16,21,cdwjr8,[D] Does Bert give by default word embedding or sentence embedding ?,https://www.reddit.com/r/MachineLearning/comments/cdwjr8/d_does_bert_give_by_default_word_embedding_or/,amil123123,1563281035,"Hey all

Since Bert is a language model, by default do we obtain sentence or word embedding?

I actually plan to use these embeddings for various NLP related tasks like Sentence Similarity, NMT, Summarization etc.

&amp;#x200B;

Also :

* If it by default gives Sentence Level Embedding then what is the process to get Word Embedding ( any refer might help here ).
* If we obtain Word Embeddings then do we just simply do Mean/Max pooling to get Sentence embedding or are there better appraoches?",10,1
1022,2019-7-16,2019,7,16,21,cdwkdn,"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educative",https://www.reddit.com/r/MachineLearning/comments/cdwkdn/this_video_goes_over_a_model_that_predicts_the/,antaloaalonso,1563281132,,0,1
1023,2019-7-16,2019,7,16,22,cdwq3o,[R] Agglomerative Attention,https://www.reddit.com/r/MachineLearning/comments/cdwq3o/r_agglomerative_attention/,mspells,1563282027,,7,24
1024,2019-7-16,2019,7,16,22,cdwrxm,Is a PhD in CS theory helpful for getting ML research jobs?,https://www.reddit.com/r/MachineLearning/comments/cdwrxm/is_a_phd_in_cs_theory_helpful_for_getting_ml/,coffee40320,1563282271,[removed],0,1
1025,2019-7-16,2019,7,16,22,cdwwgp,How AI is Reshaping Pathology and Histology,https://www.reddit.com/r/MachineLearning/comments/cdwwgp/how_ai_is_reshaping_pathology_and_histology/,Pricefield-,1563282951,,0,1
1026,2019-7-16,2019,7,16,22,cdx7ye,"[Research] Towards the Internet of Robotic Things: Analysis, Architecture, Components and Challenges",https://www.reddit.com/r/MachineLearning/comments/cdx7ye/research_towards_the_internet_of_robotic_things/,cdossman,1563284623,"[https://medium.com/ai%C2%B3-theory-practice-business/ai-scholar-towards-the-internet-of-robotic-things-f6ca55859691](https://medium.com/ai%C2%B3-theory-practice-business/ai-scholar-towards-the-internet-of-robotic-things-f6ca55859691) 

**Abstract:**  Internet of Things (IoT) and robotics cannot be considered two separate domains these days. Internet of Robotics Things (IoRT) is a concept that has been recently introduced to describe the integration of robotics technologies in IoT scenarios. As a consequence, these two research fields have started interacting, and thus linking research communities. In this paper we intend to make further steps in joining the two communities and broaden the discussion on the development of this interdisciplinary field. The paper provides an overview, analysis and challenges of possible solutions for the Internet of Robotic Things, discussing the issues of the IoRT architecture, the integration of smart spaces and robotic applications.",1,1
1027,2019-7-16,2019,7,16,22,cdxb7y,[P] Training an Image Classifier Using Modern Best Practices,https://www.reddit.com/r/MachineLearning/comments/cdxb7y/p_training_an_image_classifier_using_modern_best/,iyaja,1563285109,"Hi everyone. I recently trained an image classifier on a Japanese character dataset called KMNIST and achieved 97% validation accuracy within a few minutes using a suite of modern deep learning tools and techniques.

I explain all the techniques [my blog post](https://www.wandb.com/articles/how-to-teach-your-computer-japanese), published on the weights and biases site.

I kept this article relatively short and straightforward, so it should be quite accessible to beginners and is likely to improve the performance of your deep learning models.

Primarily, I used the learning rate finder and 1cycle learning rate policy taught by Jeremy Howard in the [fast.ai practical deep learning for coders course](https://course.fast.ai/) along with visualization and monitoring tools from a library called [Weights &amp; Biases](https://www.wandb.com/). 

Hope you enjoy (and your poor GPU that's been computing for days) it!",4,0
1028,2019-7-16,2019,7,16,22,cdxbm5,Can i get the currently executing file in colab ?,https://www.reddit.com/r/MachineLearning/comments/cdxbm5/can_i_get_the_currently_executing_file_in_colab/,shreshths,1563285166,"So my chrome crashed and now I'm reverted to an earlier version of my colab notebook with the server status as busy.
I read online that the VM might actually be still running and that's why the server shows busy.
My question is can I get the latest version of my colab notebook which is running on the VM anyhow?",0,1
1029,2019-7-16,2019,7,16,23,cdxhzm,AI Basics and Machine Learning with Python,https://www.reddit.com/r/MachineLearning/comments/cdxhzm/ai_basics_and_machine_learning_with_python/,ValVish,1563286059,,0,1
1030,2019-7-16,2019,7,16,23,cdy2wv,[D] How does Calorie Mama work? Python implementation,https://www.reddit.com/r/MachineLearning/comments/cdy2wv/d_how_does_calorie_mama_work_python_implementation/,Bulbasaur2015,1563288858,"How do you train a model to learn nutritional information from a picture of a food, or drink? Maybe you need a start with an image classifier, like how the app Calorie Mama works. Despite reports that nutritional recognition is[inaccurate and unreliable](https://www.theverge.com/2015/6/2/8707851/google-calories-food-photos-im2calories), I'm still interested in the (python) implementation of ID'ing the food, getting calories, nutrients, vitamins etc.",1,0
1031,2019-7-16,2019,7,16,23,cdy3vo,[M] (Meta) The WAYR sticky hasn't been updated in awhile. It's stuck on WAYR 63 but we're up to WAYR 66,https://www.reddit.com/r/MachineLearning/comments/cdy3vo/m_meta_the_wayr_sticky_hasnt_been_updated_in/,gnulynnux,1563288987,[removed],0,1
1032,2019-7-17,2019,7,17,0,cdyct1,Googles new data echoing technique speeds up AI training - Temporal Eternity,https://www.reddit.com/r/MachineLearning/comments/cdyct1/googles_new_data_echoing_technique_speeds_up_ai/,RossPeili,1563290050,,0,1
1033,2019-7-17,2019,7,17,0,cdyhin,"Guidance Regarding getting a Research Assistanceship in USA, Canada or Europe.",https://www.reddit.com/r/MachineLearning/comments/cdyhin/guidance_regarding_getting_a_research/,krshn53,1563290624,[removed],0,1
1034,2019-7-17,2019,7,17,0,cdynuu,Background subtraction with KNN: how does it work?,https://www.reddit.com/r/MachineLearning/comments/cdynuu/background_subtraction_with_knn_how_does_it_work/,shachar1000,1563291461,[removed],0,1
1035,2019-7-17,2019,7,17,0,cdyp7m,Tune in now for a free webinar on how to build AutoML computer vision pipelines,https://www.reddit.com/r/MachineLearning/comments/cdyp7m/tune_in_now_for_a_free_webinar_on_how_to_build/,Mayalittlepony,1563291634,,0,1
1036,2019-7-17,2019,7,17,0,cdypof,Train ML Models on Free Cloud GPUs ,https://www.reddit.com/r/MachineLearning/comments/cdypof/train_ml_models_on_free_cloud_gpus/,MMFeaster,1563291697,,1,2
1037,2019-7-17,2019,7,17,0,cdysc4,clustering vectors in space,https://www.reddit.com/r/MachineLearning/comments/cdysc4/clustering_vectors_in_space/,tamayjp,1563292049,[removed],0,1
1038,2019-7-17,2019,7,17,0,cdyw5m,bad cpu and good gpu for a deep learning's pc,https://www.reddit.com/r/MachineLearning/comments/cdyw5m/bad_cpu_and_good_gpu_for_a_deep_learnings_pc/,iacoposk8,1563292528,[removed],0,1
1039,2019-7-17,2019,7,17,1,cdz67n,Data Science roles with intense coding requirements,https://www.reddit.com/r/MachineLearning/comments/cdz67n/data_science_roles_with_intense_coding/,sybar142857,1563293762,[removed],0,1
1040,2019-7-17,2019,7,17,1,cdzpuu,Automate the diagnosis of Knee Injuries with Deep Learning part 1: an overview of the MRNet Dataset,https://www.reddit.com/r/MachineLearning/comments/cdzpuu/automate_the_diagnosis_of_knee_injuries_with_deep/,ahmedbesbes,1563296204,,0,1
1041,2019-7-17,2019,7,17,2,cdzsp3,Need help adding all the values in a row togther. In order to get a new a new value for each row (below is my code) I tried r/Rprograming but no reply and they seem to be a much smaller community was thinking this was the next most relavant place,https://www.reddit.com/r/MachineLearning/comments/cdzsp3/need_help_adding_all_the_values_in_a_row_togther/,youallssuck,1563296554,[removed],0,1
1042,2019-7-17,2019,7,17,2,cdzv3k,Bust the Burglars  Machine Learning with TensorFlow and Apache Kafka,https://www.reddit.com/r/MachineLearning/comments/cdzv3k/bust_the_burglars_machine_learning_with/,vicksyu,1563296839,,0,1
1043,2019-7-17,2019,7,17,2,ce08tr,Researchers use deep RL to make a walking robot made out of sticks,https://www.reddit.com/r/MachineLearning/comments/ce08tr/researchers_use_deep_rl_to_make_a_walking_robot/,fromnighttilldawn,1563298514,,0,1
1044,2019-7-17,2019,7,17,2,ce0epe,Magically Remove Moving Objects from Video,https://www.reddit.com/r/MachineLearning/comments/ce0epe/magically_remove_moving_objects_from_video/,Yuqing7,1563299262,,0,1
1045,2019-7-17,2019,7,17,3,ce0lqk,[P] TensorFlow DICOM Medical Imaging Decoder Operation,https://www.reddit.com/r/MachineLearning/comments/ce0lqk/p_tensorflow_dicom_medical_imaging_decoder/,ououwen,1563300112,"Hello, I wanted to share something our team has been working on for a while. I work on an early stage radiology imaging company where we have a blessing and curse of having too much medical imaging data. Something we found internally useful to build was a DICOM Decoder Op for TensorFlow. We are making this available open-source here: [https://github.com/gradienthealth/gradient\_decode\_dicom](https://github.com/gradienthealth/gradient_decode_dicom).  

  
DICOM is an extremely broad standard, so we try to cover the 90% case of image formats (PNG, TIFF, BMP, JPEG, JPEG2000). We also support multi-frame/multi-frame color images. Try images found here: [https://barre.dev/medical/samples/](https://barre.dev/medical/samples/). In the case an unsupported format is found, an empty Tensor is returned which can be filtered out. Reading the files directly off of bucket storage has allowed us to prevent data duplication of .dcm data (a single CT can be 300MB). You can play with the op in this Colab notebook: [https://colab.research.google.com/drive/1MdjXN3XkYs\_mSyVtdRK7zaCbzkjGub\_B](https://colab.research.google.com/drive/1MdjXN3XkYs_mSyVtdRK7zaCbzkjGub_B) 

  
We firmly believe that having open-source resources in healthcare is what will enable its use in practice, not AI trade secrets. We plan on opening more of our work in the future. DM me if there is interest in contributing to upcoming toolkits (the next one we are thinking of creating is an operation to decrypt+decompress gzip files). Also, lmk if there is interest in working with our dataset (\~300M DICOMs + notes). **The goal of these project collaborations is that they are ultimately open-sourced.** 

  
Anyway, give the operation a try. If there are problems with loading a file of interest, please make an issue on GitHub. Right now only Linux based systems are supported, and a Dockerfile example will be coming soon.",36,226
1046,2019-7-17,2019,7,17,3,ce0t3q,[R] Robots Made Out of Branches Use Deep Learning to Walk,https://www.reddit.com/r/MachineLearning/comments/ce0t3q/r_robots_made_out_of_branches_use_deep_learning/,fromnighttilldawn,1563301002,,0,1
1047,2019-7-17,2019,7,17,3,ce0tcz,when will the review of NIPS2019 will be out? I can't wait to be disappointed &gt;.&lt;,https://www.reddit.com/r/MachineLearning/comments/ce0tcz/when_will_the_review_of_nips2019_will_be_out_i/,nile6499,1563301036,[removed],0,1
1048,2019-7-17,2019,7,17,3,ce0va4,"Uber open-sources ""Plato"" - A Research Dialog System, released today on GitHub",https://www.reddit.com/r/MachineLearning/comments/ce0va4/uber_opensources_plato_a_research_dialog_system/,alshell7,1563301267,,0,1
1049,2019-7-17,2019,7,17,3,ce0w9i,Learn 16 Machine Learning Algorithms in a Fun and Easy along with Practical Python Labs using Keras,https://www.reddit.com/r/MachineLearning/comments/ce0w9i/learn_16_machine_learning_algorithms_in_a_fun_and/,dailydiscountcode,1563301386,,0,1
1050,2019-7-17,2019,7,17,3,ce1dlw,Live stream of our teams behavioral cloning agent for the MineRL Competition,https://www.reddit.com/r/MachineLearning/comments/ce1dlw/live_stream_of_our_teams_behavioral_cloning_agent/,imushroom1,1563303568,,0,1
1051,2019-7-17,2019,7,17,4,ce1iij,[r] Live steam of our team's behavioral cloning agent for the MineRL Competition,https://www.reddit.com/r/MachineLearning/comments/ce1iij/r_live_steam_of_our_teams_behavioral_cloning/,imushroom1,1563304166,,0,1
1052,2019-7-17,2019,7,17,4,ce1ptp,[R] Live stream of neural network learning to chop trees using MineRL (behavioral cloning baseline),https://www.reddit.com/r/MachineLearning/comments/ce1ptp/r_live_stream_of_neural_network_learning_to_chop/,MadcowD,1563305080,[removed],0,1
1053,2019-7-17,2019,7,17,4,ce1wdq,[D] Image recognition on server,https://www.reddit.com/r/MachineLearning/comments/ce1wdq/d_image_recognition_on_server/,emreqemal,1563305918,"Hello!

I'm trying to create an image recognition functionality, where the user takes photos and sends it to my server and I check if the photo contains an image I previously registered to my server.

I found out about a paper and its implementation

[https://github.com/cl199443/Deep-Semantic-Feature-Matching](https://github.com/cl199443/Deep-Semantic-Feature-Matching)

I currently think feature-matching is what I look for.

Question it, is there a automatic way to train a feature network to also store feature patterns it's seen before and matched to target images?

Or do you have any other workflow suggestions to tackle this problem? Thanks",5,2
1054,2019-7-17,2019,7,17,5,ce2cbr,Google Colab Slow Upload,https://www.reddit.com/r/MachineLearning/comments/ce2cbr/google_colab_slow_upload/,LeonhardEuler_,1563307944,[removed],0,1
1055,2019-7-17,2019,7,17,5,ce2hin,ELMo from scratch?,https://www.reddit.com/r/MachineLearning/comments/ce2hin/elmo_from_scratch/,Peyaash,1563308608,"Anyone implemented ELMo from scratch in PyTorch?

In one of my project I need to train ELMo embeddings. AllenNLP provides an implementation but I thought I'll take this opportunity to implement this from scratch. 

I always wanted to develop the skill to replicate the results of a research paper and experiment with them. I think implementing this will give me a kick start. Also, I will be able to learn more about PyTorch.

I already read the paper of ELMo, along with Character-Aware Neural Language Models, Highway Networks, really cool papers!

I'm pretty sure you have passed the stage where I'm at right now. It will be tremendously helpful if you can share your opinion, experience and suggestions.

TIA",0,1
1056,2019-7-17,2019,7,17,5,ce2pnh,[P] ELMo from scratch?,https://www.reddit.com/r/MachineLearning/comments/ce2pnh/p_elmo_from_scratch/,Peyaash,1563309623,"In one of my projects I need to train ELMo embeddings.
AllenNLP has an implementation of this but I thought I'll take this opportunity to implement it from scratch.

I always wanted to develop the skill to replicate the result of research papers and experiment with them. So I think implementing this from scratch will give me a kick start. Also, I'll be able to learn a lot about PyTorch.

I already read the paper of ELMo, along with Character-Aware Neural Language Models, Highway Networks, really cool papers!

I'm pretty sure you passed the stage where I am at right now. So it would be tremendously helpful if you could share your opinion, experience and suggestions.

TIA",4,1
1057,2019-7-17,2019,7,17,5,ce2tqr,Identifying individuals via micro eye movements,https://www.reddit.com/r/MachineLearning/comments/ce2tqr/identifying_individuals_via_micro_eye_movements/,finphil,1563310150,,0,1
1058,2019-7-17,2019,7,17,5,ce2tst,Best way of creating texting AI?,https://www.reddit.com/r/MachineLearning/comments/ce2tst/best_way_of_creating_texting_ai/,draganov11,1563310157,[removed],0,1
1059,2019-7-17,2019,7,17,6,ce38bz,[P] Need an Interview for Article I'm writing.,https://www.reddit.com/r/MachineLearning/comments/ce38bz/p_need_an_interview_for_article_im_writing/,addyp47,1563311994,"Hey, I'm writing an article about machine learning and how its changing how microloans are available to millennials. I am just curious if I could interview anyone in this sub with at least college or job-level experience in data science/machine learning or in microloans for about 15 min over the phone this week. Thanks",1,0
1060,2019-7-17,2019,7,17,6,ce3cp2,I want to write my Bachelor Thesis about Topology optimization using Machine Learning and have some questions.,https://www.reddit.com/r/MachineLearning/comments/ce3cp2/i_want_to_write_my_bachelor_thesis_about_topology/,avdalim,1563312562,[removed],0,1
1061,2019-7-17,2019,7,17,7,ce3ww9,DeepMind DVD-GAN: Impressive Step Toward Realistic Video Synthesis,https://www.reddit.com/r/MachineLearning/comments/ce3ww9/deepmind_dvdgan_impressive_step_toward_realistic/,Yuqing7,1563315180,,0,1
1062,2019-7-17,2019,7,17,8,ce539z,How to study sklearn's internals,https://www.reddit.com/r/MachineLearning/comments/ce539z/how_to_study_sklearns_internals/,sybar142857,1563321039,"I'm trying to understand sklearn's under-the-hood code to try and learn what it's doing for some of its models. Apart from learning a ton of python, can anyone help me with other resources that could make this easier for me? Thanks",0,1
1063,2019-7-17,2019,7,17,9,ce5751,[D] NOCIX is offering a RTX2070 dedicated server for $99/month and it's pretty good,https://www.reddit.com/r/MachineLearning/comments/ce5751/d_nocix_is_offering_a_rtx2070_dedicated_server/,thebliket,1563321636,"Ever since hetzner started selling out of their 1080 servers I've been looking for a good deal for some dedicated servers with GPUs in them. I picked up a couple of Nocix's RTX 2070 servers which come with a i7 6700k, 32gb of DDR4 ram, two 480gb SATA SSD on a gigabit port with 100tb of transfer included. Setup was a breeze, I got login credentials within 10 minutes of ordering through their automated OS loading system. Pretty amazing deal on such new hardware. I am extremely happy. They are still in stock here: [https://www.nocix.net/cart/?id=338](https://www.nocix.net/cart/?id=338)

&amp;#x200B;

Just wanted to make this post since people have been asking about good and low cost GPU hosts",0,0
1064,2019-7-17,2019,7,17,10,ce5v2q,Is anybody familiar with IBM's machine learning software ?,https://www.reddit.com/r/MachineLearning/comments/ce5v2q/is_anybody_familiar_with_ibms_machine_learning/,HeavyConfection,1563325262,[removed],0,1
1065,2019-7-17,2019,7,17,10,ce67ek,[1907.06732] Pad Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks,https://www.reddit.com/r/MachineLearning/comments/ce67ek/190706732_pad_activation_units_endtoend_learning/,HigherTopoi,1563327144,,8,12
1066,2019-7-17,2019,7,17,11,ce6n8f,[N] Top 5+ Best Machine Learning Toolkits for Mobile Developers,https://www.reddit.com/r/MachineLearning/comments/ce6n8f/n_top_5_best_machine_learning_toolkits_for_mobile/,vadhavaniyafaijan,1563329601,,0,1
1067,2019-7-17,2019,7,17,11,ce6wm5,How do companies get data for clustering for ad optimization?,https://www.reddit.com/r/MachineLearning/comments/ce6wm5/how_do_companies_get_data_for_clustering_for_ad/,spyder313,1563331091,[removed],0,1
1068,2019-7-17,2019,7,17,12,ce7pg4,[P] Neural Network Programming with PyTorch - Video &amp; Blog Series,https://www.reddit.com/r/MachineLearning/comments/ce7pg4/p_neural_network_programming_with_pytorch_video/,blackHoleDetector,1563335787,"&amp;#x200B;

https://i.redd.it/56v19434esa31.png

* [Click here for the blog and video series on deeplizard.com](https://deeplizard.com/learn/playlist/PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG)
* [Click here for the video series only on YouTube](https://www.youtube.com/watch?v=v5cngxo4mIg&amp;list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG)

This series is all about neural network programming and PyTorch! We'll start out with the basics of PyTorch and CUDA and understand why neural networks use GPUs. We then move on to cover the tensor fundamentals needed for understanding deep learning before we dive into neural network architecture. From there, we'll go through the details of training a network, analyzing results, tuning hyperparameters, and using TensorBoard with PyTorch for visual analytics!",0,4
1069,2019-7-17,2019,7,17,13,ce7xrp,"AMA with Francois Chollet, the creator of Keras",https://www.reddit.com/r/MachineLearning/comments/ce7xrp/ama_with_francois_chollet_the_creator_of_keras/,init__27,1563337223,[removed],0,1
1070,2019-7-17,2019,7,17,13,ce82p4,What are your thoughts on Neuralink and AI?,https://www.reddit.com/r/MachineLearning/comments/ce82p4/what_are_your_thoughts_on_neuralink_and_ai/,snendroid-ai,1563338090,[removed],0,1
1071,2019-7-17,2019,7,17,13,ce82yv,Help Finding Opposing Opinions,https://www.reddit.com/r/MachineLearning/comments/ce82yv/help_finding_opposing_opinions/,PleaseThrowMeAway93,1563338137,[removed],0,1
1072,2019-7-17,2019,7,17,13,ce83ka,Gradient accumulation and batchnorm in tensorflow,https://www.reddit.com/r/MachineLearning/comments/ce83ka/gradient_accumulation_and_batchnorm_in_tensorflow/,dchatterjee172,1563338246,,0,1
1073,2019-7-17,2019,7,17,13,ce89sm,Intels new Neuromorphic chips v Graphcore? Thoughts?,https://www.reddit.com/r/MachineLearning/comments/ce89sm/intels_new_neuromorphic_chips_v_graphcore_thoughts/,alvisanovari,1563339377,[removed],0,1
1074,2019-7-17,2019,7,17,14,ce8jbx,"Do you think this stuff will kill or subjugate humans in the future? If not, why? If so, why and how probable? I'd like to hear from you guys.",https://www.reddit.com/r/MachineLearning/comments/ce8jbx/do_you_think_this_stuff_will_kill_or_subjugate/,_Successful_Failure_,1563341112,[removed],0,1
1075,2019-7-17,2019,7,17,14,ce8p42,"My paper accepted in ICASSP 2018: ""Says Who? Deep Learning Models for Joint Speech Recognition, Segmentation and Diarization""",https://www.reddit.com/r/MachineLearning/comments/ce8p42/my_paper_accepted_in_icassp_2018_says_who_deep/,zephyrzilla,1563342190,[removed],0,1
1076,2019-7-17,2019,7,17,15,ce91rx,History of Machine Learning - Javatpoint,https://www.reddit.com/r/MachineLearning/comments/ce91rx/history_of_machine_learning_javatpoint/,nehapandey01,1563344590,,0,1
1077,2019-7-17,2019,7,17,15,ce925r,Is there any courses to learn about deployment process of machine learning models,https://www.reddit.com/r/MachineLearning/comments/ce925r/is_there_any_courses_to_learn_about_deployment/,textssg,1563344659,[removed],0,1
1078,2019-7-17,2019,7,17,16,ce9hc9,Best Machine Learning Course in Delhi | Machine Learning Certification in Delhi,https://www.reddit.com/r/MachineLearning/comments/ce9hc9/best_machine_learning_course_in_delhi_machine/,dtacademy,1563347636,[removed],0,1
1079,2019-7-17,2019,7,17,16,ce9q9f,[P] Structure-preserving dimensionality reduction in very large datasets,https://www.reddit.com/r/MachineLearning/comments/ce9q9f/p_structurepreserving_dimensionality_reduction_in/,bering_team,1563349511,"Hi there, we're a London-based research team working on clinical applications of machine learning. Recently, we've been dealing a lot with clinical datasets that exceed 1M+ observations and 20K+ features. We found that traditional dimensionality reduction and feature extraction methods don't deal well with this data without subsampling and are actually quite poor at preserving both global and local structures of the data. To address these issues, we've been looking into Siamese Networks for non-linear dimensionality reduction and metric learning applications. We are making our work available through an open-source project: [https://github.com/beringresearch/ivis](https://github.com/beringresearch/ivis)

&amp;#x200B;

So far, we've applied ivis to single cell datasets, images, and free text - we're really keen to see what other applications could be enabled! We've also ran a large number of benchmarks looking at both accuracy of embeddings and processing speed - [https://bering-ivis.readthedocs.io/en/latest/timings\_benchmarks.html](https://bering-ivis.readthedocs.io/en/latest/timings_benchmarks.html) \- and can see that ivis begins to stand out in datasets with 250K+ observations. We're really excited to make this project open source - there's so much for Siamese Networks beyond one-shot learning!",54,127
1080,2019-7-17,2019,7,17,16,ce9sdb,[D] Concat Model with fast.ai for Metadata Enhanced Text Classification,https://www.reddit.com/r/MachineLearning/comments/ce9sdb/d_concat_model_with_fastai_for_metadata_enhanced/,vitojph,1563349987,"Hi all,

My team is working on nontrivial multiclass text classification problems involving noisy datasets and usually more than 20 different target labels. We needed a robust way to combine both text features (ideally, leveraging some kind of pretrained word vectors) and extra-linguistic metadata.

So we figured out a **Concat Model** based on [fast.ai](https://fast.ai) that combines both ULMFiT for text and categorical or continuous input features to perform the task of classification. [This article](https://towardsdatascience.com/next-best-action-prediction-with-text-and-metadata-building-an-agent-assistant-81117730be6b) contains more details about the problem we're solving and the results. Wanna take a look at the code? Check out [this public Kaggle kernel](https://www.kaggle.com/adai183/metadata-enhanced-text-classification) and let us know what you think :-) Do you know other ways of exploiting both types of features?",4,18
1081,2019-7-17,2019,7,17,17,ce9wcr,Dumb question from a noob,https://www.reddit.com/r/MachineLearning/comments/ce9wcr/dumb_question_from_a_noob/,Icefluffy,1563350865,[removed],0,1
1082,2019-7-17,2019,7,17,17,ce9xce,[R] The continuous Bernoulli: fixing a pervasive error in variational autoencoders,https://www.reddit.com/r/MachineLearning/comments/ce9xce/r_the_continuous_bernoulli_fixing_a_pervasive/,milaworld,1563351082,,19,21
1083,2019-7-17,2019,7,17,17,cea3wi,How to cite papers from arXiv (e.g. Yolov3)?,https://www.reddit.com/r/MachineLearning/comments/cea3wi/how_to_cite_papers_from_arxiv_eg_yolov3/,Sebasuraa,1563352533,[removed],0,1
1084,2019-7-17,2019,7,17,18,ceab2s,Recommenders - Best Practices on Recommendation Systems by Microsoft,https://www.reddit.com/r/MachineLearning/comments/ceab2s/recommenders_best_practices_on_recommendation/,lugovsky,1563354105,,0,1
1085,2019-7-17,2019,7,17,18,ceagaq,[P] Random RNN Name Generator fakename.xyz,https://www.reddit.com/r/MachineLearning/comments/ceagaq/p_random_rnn_name_generator_fakenamexyz/,shoeblade,1563355165," [https://www.fakename.xyz/](https://www.fakename.xyz/) 

&amp;#x200B;

Silly little website which makes random names from char-RNN generated first and last names.

Name lists can be found on GitHub under a CC license:

 [https://github.com/artBoffin/rnn-names](https://github.com/artBoffin/rnn-names)",3,16
1086,2019-7-17,2019,7,17,18,ceaha1,Hollow core slab machine production on site,https://www.reddit.com/r/MachineLearning/comments/ceaha1/hollow_core_slab_machine_production_on_site/,ada2017,1563355355,,0,1
1087,2019-7-17,2019,7,17,18,cearr6,[D] Mobility within ML teams at Facebook,https://www.reddit.com/r/MachineLearning/comments/cearr6/d_mobility_within_ml_teams_at_facebook/,b45178,1563357463,"Hello /r/MachineLearning,

I am currently applying to some ML teams at Facebook, and was wondering how much mobility there is within different teams once you are in?

I applied for a few positions and am currently being interviewed for a role either with AML or with a product team. Is it possible to migrate between these two once I am employed there? And is there any path to getting into FAIR from one of the more applied teams, with a year or two of experience, through some internal transfer?

I don't know any one working within Facebook who'd have some first hand experience so it would be very useful to have some information before I sign an offer and find myself stuck in a position/team that I don't really like. The salary and conditions are great but I would like to be more on the research side, and am a bit worried of getting pigeon-holed as an applied product guy if I join.

Finally, do you think it would be relatively easy to get a job in a more research-oriented team in another company after some time within an applied ML facebook team?

Thanks for any advice you might have!",7,0
1088,2019-7-17,2019,7,17,19,ceaxke,Five Major Machine Learning Trends For 2019 [Infographic],https://www.reddit.com/r/MachineLearning/comments/ceaxke/five_major_machine_learning_trends_for_2019/,SSI_TeamUS,1563358570,,0,1
1089,2019-7-17,2019,7,17,19,ceay8a,Using clustering with text and numeric data to deduplicate?,https://www.reddit.com/r/MachineLearning/comments/ceay8a/using_clustering_with_text_and_numeric_data_to/,Frogad,1563358701,[removed],0,1
1090,2019-7-17,2019,7,17,19,ceb7sr,A collection of 25+ Reinforcement Learning Varieties (Applied to Stock Trading) - Google Colab,https://www.reddit.com/r/MachineLearning/comments/ceb7sr/a_collection_of_25_reinforcement_learning/,OppositeMidnight,1563360587,,0,1
1091,2019-7-17,2019,7,17,20,cebo3j,State of AI Report,https://www.reddit.com/r/MachineLearning/comments/cebo3j/state_of_ai_report/,AliWaheed,1563363507,[removed],0,1
1092,2019-7-17,2019,7,17,20,cebuxh,[D] I want to write my Bachelor Thesis about Topology optimization using Machine Learning and have some questions.,https://www.reddit.com/r/MachineLearning/comments/cebuxh/d_i_want_to_write_my_bachelor_thesis_about/,avdalim,1563364704," 

Hi,

I'm studying mechanical engineering and always have been interested in machine learning. I got through some basic tutorials online, the Google Crashcourse and currently am digging through the Hands-on Machine Learning book  from Aurelien Geron as a side project of mine.

Right now I kind of want to apply my learned knowledge in my bachelor thesis. I will write it in the chair of Optimization of mechanical structures at my university and my professor proposed, that I pick Topology Optimization using deep learning as my topic.

Now my question: Is it possible, that I succeed in doing this project? I'm kind of intimidated, mainly because I never made my own project and/or did scientific research to such extend, but I like a challenge if it's a realistic one. I have around 4 1/2 -5 Months.

If it's possible :

What material should I get into? It seems, that a CNN will be the best bet for that project, any advice for that field?

If not:

Could you think of another topic I could pick, combining my knowledge of Machine Learning with Optimization of Mechanical Structures?

Thanks in advance",18,2
1093,2019-7-17,2019,7,17,21,cebydr,Global machine learning market analysis,https://www.reddit.com/r/MachineLearning/comments/cebydr/global_machine_learning_market_analysis/,shraddha5,1563365239,[removed],0,1
1094,2019-7-17,2019,7,17,21,cec0e3,[R] Large Scale Adversarial Representation Learning Explained,https://www.reddit.com/r/MachineLearning/comments/cec0e3/r_large_scale_adversarial_representation_learning/,Yuras_Stephan,1563365557,"At the start of the month Large Scale Adversarial Representation Learning was released, and it quickly became the most ""hyped"" article on Arxiv-sanity. I wrote this post to explain the contents of the paper and interpret the results.

https://stephanheijl.com/notes_on_large_scale_adversarial_learning.html",2,10
1095,2019-7-17,2019,7,17,21,ceca8o,"How Machine Learning, AI &amp; Big Data Analytics are Reshaping the Travel &amp; Hospitality Industry (and Job Market)",https://www.reddit.com/r/MachineLearning/comments/ceca8o/how_machine_learning_ai_big_data_analytics_are/,tanmoyray01,1563367080,,0,1
1096,2019-7-17,2019,7,17,21,cecdue,Visual Chatbot Evolution: What You Need to Know,https://www.reddit.com/r/MachineLearning/comments/cecdue/visual_chatbot_evolution_what_you_need_to_know/,S_paddy,1563367654,,0,1
1097,2019-7-17,2019,7,17,21,cecf7m,"[R] Neural Network in Glass Requires No Power, Recognizes Numbers",https://www.reddit.com/r/MachineLearning/comments/cecf7m/r_neural_network_in_glass_requires_no_power/,_bm,1563367865,"We've all come to terms with a neural network doing jobs such as handwriting recognition. This panel of special glass requires no electrical power, and is able to recognize gray-scale handwritten numbers. The glass contains precisely controlled inclusions such as air holes or an impurity such as graphene or other material. The team thinks they could do better if they allowed looser constraints on the glass manufacturing. 

https://hackaday.com/2019/07/16/neural-network-in-glass-requires-no-power-recognizes-numbers/",25,41
1098,2019-7-17,2019,7,17,22,cecmok,Machine learning on the console?,https://www.reddit.com/r/MachineLearning/comments/cecmok/machine_learning_on_the_console/,officialpatterson,1563369003,[removed],0,1
1099,2019-7-17,2019,7,17,22,cecobu,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/cecobu/data_science_career_track_prep_course/,HannahHumphreys,1563369227,[removed],0,1
1100,2019-7-17,2019,7,17,22,cecs2f,Research areas?,https://www.reddit.com/r/MachineLearning/comments/cecs2f/research_areas/,Jamblamkins,1563369762,[removed],0,1
1101,2019-7-17,2019,7,17,22,ced1cb,"Deep generative models can't distinguish outliers, but why we can't sample out outliers?",https://www.reddit.com/r/MachineLearning/comments/ced1cb/deep_generative_models_cant_distinguish_outliers/,yuffon,1563371093,[removed],0,1
1102,2019-7-17,2019,7,17,22,ced3jq,Using Inception classifier to compute Inception Score in Generative Adversarial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/ced3jq/using_inception_classifier_to_compute_inception/,adriacabeza,1563371425,[removed],0,1
1103,2019-7-17,2019,7,17,22,ced79v,Using External Library FAISS in Colab,https://www.reddit.com/r/MachineLearning/comments/ced79v/using_external_library_faiss_in_colab/,TrueSalad,1563371937,[removed],0,1
1104,2019-7-17,2019,7,17,23,cedd85,Hugging Face releases Pytorch-Transformers,https://www.reddit.com/r/MachineLearning/comments/cedd85/hugging_face_releases_pytorchtransformers/,mikeross0,1563372712,[removed],0,1
1105,2019-7-17,2019,7,17,23,cedfwy,How to detect anomalies (errors and exceptions) in log files?,https://www.reddit.com/r/MachineLearning/comments/cedfwy/how_to_detect_anomalies_errors_and_exceptions_in/,milad_nazari,1563373063,[removed],0,1
1106,2019-7-17,2019,7,17,23,cednmx,[Discussion] How to detect anomalies (errors and exceptions) in log files?,https://www.reddit.com/r/MachineLearning/comments/cednmx/discussion_how_to_detect_anomalies_errors_and/,milad_nazari,1563374082,"# Is this a good approach? 

So I'm working on a Root Cause Analysis system which should help find the cause/the root error of failed system builds (packaged in a tarball), through the analysis of log files (database logs, system logs, etc) inside it.

The only labeling of data available is whether a specific tarball contains the logs of a failed system build or rather the logs of a successful build.

### My reasoning:

Instead of trying to find the specific log statements which mention the root cause, I though it would be easier to first find the log file in which this specific lines are saved. A log file containing failure log statements should be detected as an ""outlier"" log file, compared to the normal log files, that are created from successful builds. If we find log files that are outliers, we would only have to search the failure log statements in this few log files.

### My strategy to find the anomalous log files (outliers) so far:

Consider a list of tars containing the logs of a successful build. For each tar:

 1. Extract the tar and consider only log files (filter/remove configuration files, cache files, etc)
 2. Group the log files per service (mongodb, apt, fsck, etc)

For each service:

 3. Remove the timestamp from the log body for each log statement, inside each log file
 4. Concatenate, combine all the log statements of the same log file into a single string, let's call it ""log file content""
 5. Create an array containing the log file contents of each log file
 6. Use the Tf-ifd transformer on this array (TfidfVectorizer) to create a dataframe (fit/transform)
 7. Create an isolation forest (sklearn.ensemble IsolationForest) or a one class SVM (sklearn.svm OneClassSVM) called model
 8. Fit the dataframe inside the model

To find anomalous log files in a failed tar, we analyse it the following way:

 1. Extract and group the log files (see previous step 1 and 2)
 2. For every file:

 	2.1. Preprocess it the same way the successfull log files for the training were preprocessed (removal of timestamp, concatenate, Tf-ifd transform, etc), into a dataframe

	2.2. Whether this log file is anomalous or not, will be predicted by passing the dataframe as argument to the ""predict"" function"" of the model of the corresponding service

### What do you think?

 * Is this a good approach?
 * I think OneClassSVM would be better suited since it can be used for semi supervised learning (ideal for this case), instead of unsupervised learning. But I'm getting too much false positives. Any tips on choosing the right values for the nu and gamma parameters?
 * Other suggestions?",1,3
1107,2019-7-17,2019,7,17,23,cedysl,"[P] A library of pretrained models for NLP: Bert, GPT, GPT-2, Transformer-XL, XLNet, XLM",https://www.reddit.com/r/MachineLearning/comments/cedysl/p_a_library_of_pretrained_models_for_nlp_bert_gpt/,Thomjazz,1563375561,"Huggingface has released a new version of their open-source library of pretrained transformer models for NLP: *PyTorch-Transformers* 1.0 (formerly known as *pytorch-pretrained-bert*).

&amp;#x200B;

The library now comprises six architectures:

* Google's **BERT**,
* OpenAI's **GPT** &amp; **GPT-2**,
* Google/CMU's **Transformer-XL** &amp; **XLNet** and
* Facebook's **XLM**,

and a total of 27 pretrained model weights for these architectures.

&amp;#x200B;

The library focus on:

* being superfast to learn &amp; use (almost no abstractions),
* providing SOTA examples scripts as starting points (text classification with GLUE, question answering with SQuAD and text generation using GPT, GPT-2, Transformer-XL, XLNet).

&amp;#x200B;

It also provides:

* a unified API for models and tokenizers,
* access to the hidden-states and attention weights,
* compatibility with Torchscript...

&amp;#x200B;

Install: *pip install pytorch-transformers*

Quickstart: [https://github.com/huggingface/pytorch-transformers#quick-tour](https://github.com/huggingface/pytorch-transformers#quick-tour)

Release notes: [https://github.com/huggingface/pytorch-transformers/releases/tag/v1.0.0](https://github.com/huggingface/pytorch-transformers/releases/tag/v1.0.0)

Documentation (work in progress): [https://huggingface.co/pytorch-transformers/](https://huggingface.co/pytorch-transformers/)",23,285
1108,2019-7-18,2019,7,18,0,cedzcb,"[P] Larq, an Open-Source Library for Training Binarized Neural Networks",https://www.reddit.com/r/MachineLearning/comments/cedzcb/p_larq_an_opensource_library_for_training/,khelwegen,1563375628,"Hi all, I'm working for Plumerai, a London-based startup that is developing hardware for binarized neural networks (BNN). BNNs are designed to make neural networks much more efficient, so that it becomes feasible to use them in embedded applications such as self-driving cars and mobile phones.

Over the past few months we've been developing Larq, an open source library to make it easier for researchers and developers to work with BNNs. We have also reproduced a number of key papers in the field and made pretrained BNNs available in Larq-Zoo (Bi-Real Net, XNOR-net, BinaryNet), and we'll soon add more. The library is built on top of `tensorflow.keras`. Larq has already been very valuable to us in our own research (we recently made our [first paper](https://arxiv.org/abs/1906.02107) available on arxiv - it even got some attention on this subreddit!), and we are now at a point where we are very eager to share it with others.

If you're interested in this space, please have a look and let us know what you think!

Documentation: [https://larq.dev/](https://larq.dev/)
Github Larq: [https://github.com/larq/larq](https://github.com/larq/larq)
Github Larq-Zoo: [https://github.com/larq/zoo](https://github.com/larq/zoo)",3,1
1109,2019-7-18,2019,7,18,0,cedzm7,"[R] KeOps v1.1: free x30 speed-up + linear memory footprint for kernel methods, Sinkhorn and more",https://www.reddit.com/r/MachineLearning/comments/cedzm7/r_keops_v11_free_x30_speedup_linear_memory/,jeanfeydy,1563375664,"Hi Reddit,

[KeOps v1.1](https://www.kernel-operations.io) is out! This package provides a new `LazyTensor` wrapper for PyTorch and Numpy that speeds up all kernel-related computations with an efficient, tailor-made CUDA scheme. It should be especially useful to people who work with kernel matrices, geometric deep learning or point clouds.

*Processing img 9uozdr0tjva31...*

*Processing img 084g9udunva31...*

To use it: just install CUDA, nvcc, and type `pip install pykeops[full]`. This works out-of-the-box on [Google Colab](https://colab.research.google.com) !

Our documentation is available at [www.kernel-operations.io](https://www.kernel-operations.io) , with numerous examples and tutorials (spectral coordinates, Sinkhorn, K-NN, K-Means...). If you have any remark or suggestion, please let us know :-)",1,32
1110,2019-7-18,2019,7,18,0,ceece3,"AMA: We are Noam Brown and Tuomas Sandholm, creators of the Carnegie Mellon / Facebook multiplayer poker bot Pluribus. We're also joined by a few of the pros Pluribus played against. Ask us anything!",https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/,NoamBrown,1563377254,[removed],177,121
1111,2019-7-18,2019,7,18,0,ceeich,Hanabi Challenge Anyone?,https://www.reddit.com/r/MachineLearning/comments/ceeich/hanabi_challenge_anyone/,stonegod23,1563378003,[removed],0,1
1112,2019-7-18,2019,7,18,0,ceek5t,[D] Deploy keras CNNs with tensorflow serve (accepting base64 encoded images),https://www.reddit.com/r/MachineLearning/comments/ceek5t/d_deploy_keras_cnns_with_tensorflow_serve/,ixeption,1563378235,,0,1
1113,2019-7-18,2019,7,18,0,ceeofb,"Simple Questions Thread July 17, 2019",https://www.reddit.com/r/MachineLearning/comments/ceeofb/simple_questions_thread_july_17_2019/,AutoModerator,1563378776,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1
1114,2019-7-18,2019,7,18,1,cef4jq,Feature selection for regression in high-dimensional small dataset,https://www.reddit.com/r/MachineLearning/comments/cef4jq/feature_selection_for_regression_in/,DlC3R,1563380751,[removed],0,1
1115,2019-7-18,2019,7,18,1,cef9p7,[P] TorchKGE : a PyTorch-based Python library for Knowledge Graph embedding.,https://www.reddit.com/r/MachineLearning/comments/cef9p7/p_torchkge_a_pytorchbased_python_library_for/,sylard33,1563381393,"TorchKGE is an open-source Python library based on PyTorch that aims at implementing in a unified way all existing models for knowledge-graph embedding. Currently, are implemented : TransE, TransH, TransR, TransD, RESCAL, DistMult, HolE, ComplEx, Analogy. More models should be coming soon (e.g. deep ones such as RGCN, ConvE, ConvKB).

I'd like it to be a clean library that respects PyTorch idiom. Feel free to contribute directly in the repo or just by sending your remarks.

Documentation: https://torchkge.readthedocs.io/en/latest/?badge=latest
PyPi: https://pypi.org/project/torchkge/
Github repo: https://github.com/torchkge-team/torchkge",2,10
1116,2019-7-18,2019,7,18,1,cefawo,[D] Andrej Karpathy | Multi-Task Learning in the Wilderness,https://www.reddit.com/r/MachineLearning/comments/cefawo/d_andrej_karpathy_multitask_learning_in_the/,Cock-tail,1563381542,,0,1
1117,2019-7-18,2019,7,18,1,cefayc,[R] Natural Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/cefayc/r_natural_adversarial_examples/,DanielHendrycks,1563381548,,16,5
1118,2019-7-18,2019,7,18,2,cefpe8,"[R] ""we propose a training objective that is invariant to changes in depth range and scale"" - Towards Robust Monocular Depth Estimation",https://www.reddit.com/r/MachineLearning/comments/cefpe8/r_we_propose_a_training_objective_that_is/,downtownslim,1563383369,"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer

Video: [https://youtu.be/ITI0YS6IrUQ](https://youtu.be/ITI0YS6IrUQ)

&gt;The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a training objective that is invariant to changes in depth range and scale. Armed with this objective, we explore an abundant source of training data: 3D films. We demonstrate that despite pervasive inaccuracies, 3D films constitute a useful source of data that is complementary to existing training sets. We evaluate the presented approach on diverse datasets, focusing on zero-shot cross-dataset transfer: testing the generality of the learned model by evaluating it on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources yields improved depth estimates, particularly on previously unseen datasets. Some results are shown in the supplementary video: [this https URL](https://youtu.be/ITI0YS6IrUQ)",0,1
1119,2019-7-18,2019,7,18,2,cefpr2,"[R] ""A fieldtested robotic harvesting system for iceberg lettuce"", Birrell et al 2019",https://www.reddit.com/r/MachineLearning/comments/cefpr2/r_a_fieldtested_robotic_harvesting_system_for/,gwern,1563383411,,0,1
1120,2019-7-18,2019,7,18,2,ceg7sr,Parrotron: New Research into Improving Verbal Communication for People with Speech Impairments,https://www.reddit.com/r/MachineLearning/comments/ceg7sr/parrotron_new_research_into_improving_verbal/,sjoerdapp,1563385696,,0,1
1121,2019-7-18,2019,7,18,3,cegzzr,"[P] TabNine: ""Autocompletion with deep learning"" [interactive source code completion in 21 languages with GPT-2 trained on Github]",https://www.reddit.com/r/MachineLearning/comments/cegzzr/p_tabnine_autocompletion_with_deep_learning/,gwern,1563389274,,0,1
1122,2019-7-18,2019,7,18,4,ceheq3,Can Machine Learning Help Predict College Admissions? I think it can.,https://www.reddit.com/r/MachineLearning/comments/ceheq3/can_machine_learning_help_predict_college/,khanradcoder,1563391176,,0,1
1123,2019-7-18,2019,7,18,5,cehyyj,Suggestions for defense,https://www.reddit.com/r/MachineLearning/comments/cehyyj/suggestions_for_defense/,dreamer_luci,1563393669,[removed],1,1
1124,2019-7-18,2019,7,18,5,cei3ow,"Problem with Kernel DE IPYNB (Google Colab, 5.13 Py Data Science Handbook)",https://www.reddit.com/r/MachineLearning/comments/cei3ow/problem_with_kernel_de_ipynb_google_colab_513_py/,FarUnder73_5Break,1563394263,"I was trying a very simple thing: running through a readily-available IPYNB on Google Colab. It's the possibly well known notebook,

    05.13-Kernel-Density-Estimation.ipynb

which is said to be based on, or share content with the Py Data Science Handbook (5.13 refers to a chapter in the paperprintedbook).

&amp;#x200B;

The library here of interest is Scikit-Learn... uh, probably...?

&amp;#x200B;

I checked that I didn't forget to shift+enter any code boxes and I think that I didn't. First 10 code cells in the notebook ran just fine but on the 11th cell I got an error. At

    from sklearn.grid_search import GridSearchCV
    from sklearn.cross_validation import LeaveOneOut

it says that, No module named 'sklearn.grid\_search' . I heard that GridSearchCV might now reside in another method/object called sklearn.model\_selection but that one does not load either! I tried to load scikit-learn in its entirety in an earlier line and even tried to check out some ways to update it but obviously I'm doing something wrong here. All of this is to no avail.

&amp;#x200B;

I am a total newbie w.r.t.Colab, Jupyter, Python, and using different libraries, so please help me out! Come to think of it, I'm a total Reddit newbie as well. What should I do? Has GridSearchCV been moved to again or something other wonky happened to those methods of sklearn?

&amp;#x200B;

Please note that even though I'm a total newbie in the matters I listed above and am generally really crappy at the programming side of things, I am by no means a newbie to the Bayesian C/KDE part of the problem, or to machine learning in general, or to data analysis (at least in some senses). So in case you have something to comment on those as well, feel free, and you don't need to do that part of the stuff in newbie-mode.

&amp;#x200B;

Also of note: I posted this here because, well, soon enough I'll found out if it's a good place for the question. r/Python feels like a bit too bold of a move (it's so broad subject) and this isn't necessarily a pure Colab/Jupyter question either. And since I'm not sure what eventally is wrong, I'm also not sure if this is a Scikit-Learn question. So, as it stands, my first Reddit question is now here at MachineLearning.",0,1
1125,2019-7-18,2019,7,18,5,cei3qr,Awesome  news from Uber for releasing its code for conversational #AI,https://www.reddit.com/r/MachineLearning/comments/cei3qr/awesome_news_from_uber_for_releasing_its_code_for/,ai-lover,1563394269,[removed],0,1
1126,2019-7-18,2019,7,18,6,cejfti,I want to read in a bunch of user submitted submissions on a topic and use machine learning to summarize all entries and output a summary.,https://www.reddit.com/r/MachineLearning/comments/cejfti/i_want_to_read_in_a_bunch_of_user_submitted/,ajlangsdon,1563400299,[removed],0,1
1127,2019-7-18,2019,7,18,7,cejp8r,[P] I taught a one day course on backpropagation &amp; neural networks from scratch - here are my materials,https://www.reddit.com/r/MachineLearning/comments/cejp8r/p_i_taught_a_one_day_course_on_backpropagation/,ADGEfficiency,1563401568,"I taught a one day course on backpropagation &amp; neural networks from scratch today - here are my materials:

[https://github.com/ADGEfficiency/teaching-monolith/blob/master/backprop/intro-to-backprop.ipynb](https://github.com/ADGEfficiency/teaching-monolith/blob/master/backprop/intro-to-backprop.ipynb)

&amp;#x200B;

Hopefully it of of some use to someone :)",19,290
1128,2019-7-18,2019,7,18,7,cejqit,[P] Uncertainty Estimation in Deep Learning [slides],https://www.reddit.com/r/MachineLearning/comments/cejqit/p_uncertainty_estimation_in_deep_learning_slides/,perone,1563401764,,0,1
1129,2019-7-18,2019,7,18,9,cel6th,Very strong low level cpp programmer -&gt; research engineer?,https://www.reddit.com/r/MachineLearning/comments/cel6th/very_strong_low_level_cpp_programmer_research/,EveningAlgae,1563409233,[removed],0,1
1130,2019-7-18,2019,7,18,9,cel6x6,101 ML Algorithms (with cheat sheets),https://www.reddit.com/r/MachineLearning/comments/cel6x6/101_ml_algorithms_with_cheat_sheets/,dtelad11,1563409248,,0,1
1131,2019-7-18,2019,7,18,9,cel71v,[D] Very strong cpp programmer -&gt; research engineer?,https://www.reddit.com/r/MachineLearning/comments/cel71v/d_very_strong_cpp_programmer_research_engineer/,EveningAlgae,1563409267,"Hi guys, I'm asking for a buddy of mine. He's worked at a hft firm, big  4, shipped a game, and specializes in high performance c++. He's looking  to transition to research engineer. How would he go about doing this?",17,0
1132,2019-7-18,2019,7,18,9,cellov,One a day keeps the doctor away,https://www.reddit.com/r/MachineLearning/comments/cellov/one_a_day_keeps_the_doctor_away/,liveextraordinary,1563411540,[removed],0,1
1133,2019-7-18,2019,7,18,10,cely08,Slate Robots TR2 upgrade of PCB motor driver for each actuator in the ML Playground: Video,https://www.reddit.com/r/MachineLearning/comments/cely08/slate_robots_tr2_upgrade_of_pcb_motor_driver_for/,FrankSchmidtTinyLabs,1563413452,,0,1
1134,2019-7-18,2019,7,18,10,celybj,A method to stay organized across lots of different experiments for machine learning projects.,https://www.reddit.com/r/MachineLearning/comments/celybj/a_method_to_stay_organized_across_lots_of/,agamemnonlost,1563413499,,0,1
1135,2019-7-18,2019,7,18,11,cemgom,Is it possible for an undergraduate Computer Science student to start a PhD in Deep Learning without doing a Master's degree?,https://www.reddit.com/r/MachineLearning/comments/cemgom/is_it_possible_for_an_undergraduate_computer/,dayemsaeed,1563416427,[removed],0,1
1136,2019-7-18,2019,7,18,11,cemm5v,[R] 300+ Free Datasets for Machine Learning divided into 10 Use Cases,https://www.reddit.com/r/MachineLearning/comments/cemm5v/r_300_free_datasets_for_machine_learning_divided/,LimarcAmbalina,1563417301,"Just sharing a handy dataset source.   


Here is the link to a large repository of free datasets:

[https://lionbridge.ai/business-resources/open-datasets-for-machine-learning/](https://lionbridge.ai/business-resources/open-datasets-for-machine-learning/)",2,0
1137,2019-7-18,2019,7,18,12,cemvxm,Survey for stock price forecasting,https://www.reddit.com/r/MachineLearning/comments/cemvxm/survey_for_stock_price_forecasting/,normalcrazymemelord,1563418908,[removed],0,1
1138,2019-7-18,2019,7,18,12,cenc9e,Laptop for machine learning,https://www.reddit.com/r/MachineLearning/comments/cenc9e/laptop_for_machine_learning/,i_amabhishek,1563421555,"Please suggest a good laptop for machine learning . you can share Amazon,Flipkart link also.",0,1
1139,2019-7-18,2019,7,18,12,cend9v,Research papers related to FaceApp,https://www.reddit.com/r/MachineLearning/comments/cend9v/research_papers_related_to_faceapp/,sainatarajan7,1563421732,[removed],0,1
1140,2019-7-18,2019,7,18,12,cenh8x,[R] Research Papers on FaceApp app,https://www.reddit.com/r/MachineLearning/comments/cenh8x/r_research_papers_on_faceapp_app/,sainatarajan7,1563422383,[removed],0,1
1141,2019-7-18,2019,7,18,14,ceo3qm,How to become a Machine Learning Engineer?,https://www.reddit.com/r/MachineLearning/comments/ceo3qm/how_to_become_a_machine_learning_engineer/,cue_the_sad_music,1563426336,[removed],0,1
1142,2019-7-18,2019,7,18,14,ceo6cr,Is there any proper mathematical / statistical definition for overfitting ?,https://www.reddit.com/r/MachineLearning/comments/ceo6cr/is_there_any_proper_mathematical_statistical/,2141rika,1563426810,,0,1
1143,2019-7-18,2019,7,18,14,ceockp,"Fluid Bed Processor, Fluid Bed Coater, Fluid Bed Dryer - LiquidmixingPlant",https://www.reddit.com/r/MachineLearning/comments/ceockp/fluid_bed_processor_fluid_bed_coater_fluid_bed/,shreebhagwati07,1563427984,,0,1
1144,2019-7-18,2019,7,18,14,ceoj5s,[D] Artificial Intelligence helps write code in Python,https://www.reddit.com/r/MachineLearning/comments/ceoj5s/d_artificial_intelligence_helps_write_code_in/,PlayfulConfidence,1563429255,[removed],0,1
1145,2019-7-18,2019,7,18,15,ceolzt,FaceApp that makes you look old.,https://www.reddit.com/r/MachineLearning/comments/ceolzt/faceapp_that_makes_you_look_old/,ketan_p01,1563429811,[removed],0,1
1146,2019-7-18,2019,7,18,15,ceon5y,Automate the diagnosis of Knee Injuries with Deep Learning part 1: an overview of the MRNet Dataset,https://www.reddit.com/r/MachineLearning/comments/ceon5y/automate_the_diagnosis_of_knee_injuries_with_deep/,ahmedbesbes,1563430052,[removed],0,1
1147,2019-7-18,2019,7,18,15,ceovht,[P] Playing RTS games with audio recognition instead of using hands for input,https://www.reddit.com/r/MachineLearning/comments/ceovht/p_playing_rts_games_with_audio_recognition/,chaosparrot,1563431694,"So about two years ago I started getting shoulder aches, but I still wanted to play RTS games. That's when I started working on a project to allow me to play certain games without using my hands.

At first it started off with 100ms audio and a slow 80ms delay afterwards to respond to inputs, right now I've brought it down to 50ms audio with a response time of 10ms. 

Also using an eyetracker to move the mouse around so that it's completely hands free.

&amp;#x200B;

A demo where I'm using the program to play Starcraft 2 can be found here with all the controls explained during the video:

[https://www.youtube.com/watch?v=lhQzrZ3PrtU](https://www.youtube.com/watch?v=lhQzrZ3PrtU)

&amp;#x200B;

The project has the recording tools needed for data collection, using a sliding window over the microphone input to generate 50ms audio files every 25ms. 

I added some simple thresholding filters so that I can more easily get the right audio samples when I am recording them ( sibilants can get by with just a pitch threshold, others like finger snaps work best with high peak-peak thresholds )

&amp;#x200B;

I'm using neural nets with four layers in an ensemble to do the recognition part, and do some post-processing to make sure keyboard-inputs are done at the proper times with as little mis-clicks as possible.

I validate out-of-sample performance by recording some more sounds and analysing the outputs of the model in a few graphs ( [https://github.com/chaosparrot/parrot.py/blob/master/docs/ANALYSING.md](https://github.com/chaosparrot/parrot.py/blob/master/docs/ANALYSING.md) ).

&amp;#x200B;

The post-processing tweaks I do after playing a match in a game, and alter the thresholds for input activation based on my experience during it ( maybe I felt the SHIFT key was pressed too late, or another key was way too trigger happy )

by analysing the model output of the match with the CSV output of the recognitions.

&amp;#x200B;

The program is multithreaded to ensure that I don't lose audio recordings during the feature-engineering/evaluation phase.

&amp;#x200B;

A github with all the code can be found here: [https://github.com/chaosparrot/parrot.py](https://github.com/chaosparrot/parrot.py)

&amp;#x200B;

As for the future, I think I want to make it record 30ms sounds read at 60hz, and maybe fool around with some CNNs to see if it improves the recognition.

Considering I also control the data collection, I can just add a few thousand more samples of certain sounds, so I might try training with 5000 samples per label instead of 1500.",20,53
1148,2019-7-18,2019,7,18,15,ceoxtp,"[P] Hello, everyone. I wrote a blog on implementing an autoencoder in TensorFlow 2.0",https://www.reddit.com/r/MachineLearning/comments/ceoxtp/p_hello_everyone_i_wrote_a_blog_on_implementing/,afagarap,1563432195,"Hello, everyone. I wrote a blog on implementing an autoencoder in TensorFlow 2.0

&amp;#x200B;

[https://medium.com/@abien.agarap/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7](https://medium.com/@abien.agarap/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7)

&amp;#x200B;

I hope this can be useful for someone.",0,0
1149,2019-7-18,2019,7,18,15,cep2j9,"Neuromorphic Chipsets: Artificial Intelligence Insight Series | Technologies, Trend, Growth and Key Devlopement | Incredible Growth Opportunities like: IBM, Qualcomm, SK Hynix, Intel, and Brain Corporation",https://www.reddit.com/r/MachineLearning/comments/cep2j9/neuromorphic_chipsets_artificial_intelligence/,ajitcoherent,1563433196,,0,1
1150,2019-7-18,2019,7,18,16,cep5y0,"""[Research]"" ""[R]"" Research Papers on FaceApp app",https://www.reddit.com/r/MachineLearning/comments/cep5y0/research_r_research_papers_on_faceapp_app/,sainatarajan7,1563433873,[removed],0,1
1151,2019-7-18,2019,7,18,16,cep6b5,Time Series Prediction,https://www.reddit.com/r/MachineLearning/comments/cep6b5/time_series_prediction/,mrdreamer34,1563433946,[removed],0,1
1152,2019-7-18,2019,7,18,16,cepd1k,What about declarative knowledge in natural language understanding?,https://www.reddit.com/r/MachineLearning/comments/cepd1k/what_about_declarative_knowledge_in_natural/,rtk25,1563435345,[removed],0,1
1153,2019-7-18,2019,7,18,16,cepe15,[D] What about declarative knowledge in natural language understanding?,https://www.reddit.com/r/MachineLearning/comments/cepe15/d_what_about_declarative_knowledge_in_natural/,rtk25,1563435552,"Consider toy riddles (sorry if contrived) like:

Passage:

""The weather each day can be snowy, sunny or cloudy. If it's cloudy or sunny, airplanes will fly. If it's snowy, airplanes don't fly, and the post won't arrive. If airplanes fly, the post will arrive. Today the post arrived.""

Question:

""What is the weather?""



This can also be phrased as an entailment problem.

Needless to say, reading comprehension and entailment models at https://demo.allennlp.org get this wrong. The ability to represent and utilize declarative knowledge seems essential for moving past fancy pattern matching. Does anyone know of toy ""bAbI"" style datasets for this? You could similarly generate such riddles pretty easily.",4,0
1154,2019-7-18,2019,7,18,16,cepgfw,[R] An integrated brain-machine interface platform with thousands of channels,https://www.reddit.com/r/MachineLearning/comments/cepgfw/r_an_integrated_brainmachine_interface_platform/,wei_jok,1563436060,,0,1
1155,2019-7-18,2019,7,18,16,cephlu,[R] An integrated brain-machine interface platform with thousands of channels,https://www.reddit.com/r/MachineLearning/comments/cephlu/r_an_integrated_brainmachine_interface_platform/,sensetime,1563436317,"**Abstract**

Brain-machine interfaces (BMIs) hold promise for the restoration of sensory and motor function and the treatment of neurological disorders, but clinical BMIs have not yet been widely adopted, in part because modest channel counts have limited their potential. In this white paper, we describe Neuralink's first steps toward a scalable high-bandwidth BMI system. We have built arrays of small and flexible electrode ""threads"", with as many as 3,072 electrodes per array distributed across 96 threads. We have also built a neurosurgical robot capable of inserting six threads (192 electrodes) per minute. Each thread can be individually inserted into the brain with micron precision for avoidance of surface vasculature and targeting specific brain regions. The electrode array is packaged into a small implantable device that contains custom chips for low-power on-board amplification and digitization: the package for 3,072 channels occupies less than (23 x 18.5 x 2) mm3. A single USB-C cable provides full-bandwidth data streaming from the device, recording from all channels simultaneously. This system has achieved a spiking yield of up to 85.5% in chronically implanted electrodes. Neuralink's approach to BMI has unprecedented packaging density and scalability in a clinically relevant package.

https://www.biorxiv.org/content/10.1101/703801v1",5,5
1156,2019-7-18,2019,7,18,16,cephmu,[P] Lineflow: Lightwight NLP Data Loader for All Deep Learning Frameworks in Python,https://www.reddit.com/r/MachineLearning/comments/cephmu/p_lineflow_lightwight_nlp_data_loader_for_all/,yasufumy,1563436323,,0,1
1157,2019-7-18,2019,7,18,17,cepkkq,[R] Superhuman AI for multiplayer poker,https://www.reddit.com/r/MachineLearning/comments/cepkkq/r_superhuman_ai_for_multiplayer_poker/,lopespm,1563436980,,0,1
1158,2019-7-18,2019,7,18,17,cepku9,[R] The Bach Doodle: Approachable music composition with machine learning at scale,https://www.reddit.com/r/MachineLearning/comments/cepku9/r_the_bach_doodle_approachable_music_composition/,wei_jok,1563437039,,2,4
1159,2019-7-18,2019,7,18,17,cepm57,[R] Artificial Intelligence helps write code in Python,https://www.reddit.com/r/MachineLearning/comments/cepm57/r_artificial_intelligence_helps_write_code_in/,cmillionaire9,1563437320,[removed],0,1
1160,2019-7-18,2019,7,18,17,cepmvj,[D] Artificial Intelligence helps write code in Python,https://www.reddit.com/r/MachineLearning/comments/cepmvj/d_artificial_intelligence_helps_write_code_in/,PlayfulConfidence,1563437482," Video:  https://www.youtube.com/watch?v=lssDX\_zOwOo  
Tarry Singh: ""I remember Jim Zemlin from The Linux Foundation asking me last year at AI leadership summit keynote if someday AI could make software coding simpler and faster.  My answer was: There's so much code on Github and there is definitely way to automate a lot of coding if you apply deeplearning to it.  Say hello to Deep Tabnine a cool tool that autocompletes your code with deep learning!""  It is trained on around 2 million files from GitHub. During training, its goal is to predict each token given the tokens that come before it. To achieve this goal, it learns complex behaviour, such as type inference in dynamically typed languages.  Deep TabNine is based on GPT-2, which uses the Transformer network architecture. This architecture was first developed to solve problems in natural language processing.   Here's a Python demo, but you can have it in Java Haskell C++",0,1
1161,2019-7-18,2019,7,18,17,cepnnx,[Discussion] Artificial Intelligence helps write code in Python,https://www.reddit.com/r/MachineLearning/comments/cepnnx/discussion_artificial_intelligence_helps_write/,PlayfulConfidence,1563437651,[removed],0,1
1162,2019-7-18,2019,7,18,17,ceppd0,[D] Artificial Intelligence helps write code in Python,https://www.reddit.com/r/MachineLearning/comments/ceppd0/d_artificial_intelligence_helps_write_code_in/,PlayfulConfidence,1563438030,[removed],0,1
1163,2019-7-18,2019,7,18,17,cepq2b,[D] Artificial Intelligence helps write code in Python,https://www.reddit.com/r/MachineLearning/comments/cepq2b/d_artificial_intelligence_helps_write_code_in/,MaleficentPepper,1563438189,[removed],0,1
1164,2019-7-18,2019,7,18,17,cepqc5,[P] Google coral board image classification and transfer learning demos with Captain America and BB-8,https://www.reddit.com/r/MachineLearning/comments/cepqc5/p_google_coral_board_image_classification_and/,makereven,1563438251,"Google coral board image classification and transfer learning demos with Captain America and BB-8

&amp;#x200B;

![video](y6j1dde2u0b31)

&amp;#x200B;

i got coral hardware(dev board and USB accelerator) from [this website](https://store.gravitylink.com), strongly recommended.",0,0
1165,2019-7-18,2019,7,18,17,cepulj,Maximizing difference between product genders,https://www.reddit.com/r/MachineLearning/comments/cepulj/maximizing_difference_between_product_genders/,Mohammed-Sunasra,1563439221,[removed],0,1
1166,2019-7-18,2019,7,18,17,cepwcm,How much level of mathematics is needed to study deep learning / tensorflow ?,https://www.reddit.com/r/MachineLearning/comments/cepwcm/how_much_level_of_mathematics_is_needed_to_study/,third_scale,1563439632,[removed],0,1
1167,2019-7-18,2019,7,18,18,ceq17h,Forecasting recurring orders for an online subscription business using Facebook Prophet and R Ask,https://www.reddit.com/r/MachineLearning/comments/ceq17h/forecasting_recurring_orders_for_an_online/,xvinc666x,1563440681,"I am analyzing data from a subscription model, in which a customer must pay a recurring price at a regular interval (30 days) for access to the product.

EDIT -&gt; Direct link to daily data: [https://docs.google.com/spreadsheets/d/1rgFKQsXIn9VmKtpv06cVPytCoPynpVva3fOVKqevD3s/edit#gid=0](https://docs.google.com/spreadsheets/d/1rgFKQsXIn9VmKtpv06cVPytCoPynpVva3fOVKqevD3s/edit#gid=0)

You can access the data here via this google sheet:

    library(tidyverse)
    library(lubridate)
    library(forecast)
    
    df &lt;- read.csv(""https://docs.google.com/spreadsheets/d/e/2PACX-1vTrf4SbDZPwIe_xDHsHLywkxBtm1ZD6AOz4YQJmNNTDwpMuol0um3xmLGpJkY2ImNDtfKwKhoeXOlF-/pub?gid=0&amp;single=true&amp;output=csv"")
    
    head(df)
              ds  order_type y
    1 2018-12-04 acquisition 1
    2 2018-12-09 acquisition 1
    3 2018-12-16 acquisition 1
    4 2018-12-18 acquisition 1
    5 2018-12-19 acquisition 1
    6 2018-12-20 acquisition 1
    

After aggregating the data on the YYYY-MM level, one can observe the following:

    df %&gt;%
      mutate(month = format(as.Date(ds), ""%Y-%m"")) %&gt;%
      group_by(month,order_type) %&gt;%
      summarise(y = sum(y)) %&gt;%
      spread(order_type, y)
    
    month   acquisition recurring_orders
    2018-12      9        0
    2019-01      42       6
    2019-02      98       34
    2019-03      644      130
    2019-04      588      554
    2019-05      324      775
    2019-06      335      709
    2019-07*     184      467
    

  \* Data for July is incomplete. The last date of data collection is 2019-07-17.

Recurring orders are obviously 0 in December - the month of the launch - and then they will be reduced of a certain percentage representing the churn of the userbase; while they will grow as well in accordance with the acquisition of the previous months.

Users will churn for different reasons:

* ""Natural churn"" of users dropping out of the subscription
* Orders not processed for payment-related reasons - ie insufficient funds
* Users ""pausing"" the subscription - for example, users skipping a delivering postponing it to the next month ...

**Task**:

I want to perform **a forecast for recurring orders for the month of July 2019** aka for the remaining 14 days:

    last_day = as.Date('2019-07-17')
    remaining_days &lt;- as.numeric(days_in_month(last_day) - mday(last_day))

Recurring orders will be affected by multiple factors - 2 forces (churn and acquisition) pushing in two different directions; combinations of seasonalities, platform-related (ie. payments) issues and so on.

I chose to use [Prophet](https://facebook.github.io/prophet/), an algorithm recently published by Facebook, accounting for weekly and monthly seasonalities, and simple linear growth.

    m &lt;- prophet(weekly.seasonality=T,
                 daily.seasonality=F,
                 yearly.seasonality = F)
    
    m &lt;- add_seasonality(m, name='monthly', period=30.5, fourier.order=5)
    m &lt;- fit.prophet(m, df_r)
    
    future &lt;- make_future_dataframe(m, periods = remaining_days)
    forecast &lt;- predict(m, future)
    prophet_plot_components(m, forecast)
    
    # generate basic forecast
    future &lt;- make_future_dataframe(m, periods = remaining_days)
    forecast &lt;- predict(m, future)
    
    plot(m, forecast, xlabel = """", ylabel = ""orders"")

&amp;#x200B;

![img](7mtmxdj121b31)

I can look now at the predictions:

    forecast %&gt;%
      select(ds, yhat) %&gt;%
      mutate(month = format(ds, ""%Y-%m"")) %&gt;%
      group_by(month) %&gt;%
      summarise(orders_pre = sum(yhat)) -&gt; pred
    
    df_r %&gt;%
      mutate(month = format(ds, ""%Y-%m"")) %&gt;%
      group_by(month) %&gt;%
      summarise(actual_orders = sum(y)) -&gt; act
    
    act %&gt;%
      left_join(pred) %&gt;%
      mutate(predicted_orders = round(orders_pre,0)) %&gt;%
      select(-orders_pre) %&gt;%
      mutate(prediction_error = predicted_orders - actual_orders) %&gt;%
      mutate(perc_mismatch = round(prediction_error/actual_orders,4)*100)
    
    
    # A tibble: 7 x 5
      month   actual_orders predicted_orders prediction_error perc_mismatch
      &lt;chr&gt;           &lt;int&gt;            &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
    1 2019-01             6              -15              -21       -350   
    2 2019-02            34               44               10         29.4 
    3 2019-03           130              279              149        115.  
    4 2019-04           554              475              -79        -14.3 
    5 2019-05           775              655             -120        -15.5 
    6 2019-06           709              736               27          3.81
    7 2019-07           467              866              399         85.4 
    

It seems to generate a very good prediction for the month of June, but it doesn't perform very well in the previous months.

I have some questions:

1. Prophet seems to be good at capturing the seasonality and the changes of trend. But why I get negative predictions at the starting point? Does it make sense to use a time series model like this to estimate data (recurring orders) heavily dependent on the past observations? Is there any other family of models I should look into?
2. Do I need to apply any sort of transformation before fitting the model (ie BoxCox)? if so, why does it help?
3. To evaluate the model, I simply take the difference (absolute and %) between the actual and the predicted orders in the past. What are the solutions embedded in prophet in terms of measures of model performance?
4. How do I understand whether I have to use a logistic growth instead of linear?

Thanks!",0,1
1168,2019-7-18,2019,7,18,18,ceq2tp,[R] Disentangled Attribution Curves for Interpreting Random Forests and Boosted Trees,https://www.reddit.com/r/MachineLearning/comments/ceq2tp/r_disentangled_attribution_curves_for/,shi223,1563441018,,3,3
1169,2019-7-18,2019,7,18,18,ceqaci,Video processing reading list request,https://www.reddit.com/r/MachineLearning/comments/ceqaci/video_processing_reading_list_request/,ando_khachatryan,1563442568,[removed],0,1
1170,2019-7-18,2019,7,18,18,ceqaip,How TensorFlow outdid itself in handling Overfitting in DNNs,https://www.reddit.com/r/MachineLearning/comments/ceqaip/how_tensorflow_outdid_itself_in_handling/,salma-ghoneim,1563442604,,0,1
1171,2019-7-18,2019,7,18,18,ceqatn,How to pass the machine learning interview? - Duomly Blog - Programming courses online,https://www.reddit.com/r/MachineLearning/comments/ceqatn/how_to_pass_the_machine_learning_interview_duomly/,annafromduomly,1563442672,,0,1
1172,2019-7-18,2019,7,18,18,ceqbdo,"[P] Implemented MPPI (Model Predictive Path Integral) in Python with OpenAI Gym pendulum environment (paper: ""Information Theoretic MPC for Model-Based Reinforcement Learning"", Williams et al., 2017)",https://www.reddit.com/r/MachineLearning/comments/ceqbdo/p_implemented_mppi_model_predictive_path_integral/,whiletrue2,1563442780,"Hi, I have implemented MPPI introduced in the paper ""Information Theoretic MPC for Model-Based Reinforcement Learning"" (Williams et al., 2017) in Python with the pendulum OpenAI Gym environment. Please feel free to use and improve it!

Repository: https://github.com/ferreirafabio/mppi_pendulum",2,12
1173,2019-7-18,2019,7,18,19,ceqs13,Baseline statistical test for correlation/predictive power of features on target variable in a time series,https://www.reddit.com/r/MachineLearning/comments/ceqs13/baseline_statistical_test_for/,Shoddy_Researcher,1563446085,[removed],0,1
1174,2019-7-18,2019,7,18,19,cequkc,Use of Chatbot in Banking,https://www.reddit.com/r/MachineLearning/comments/cequkc/use_of_chatbot_in_banking/,ramaprasadjena,1563446589," 

Through integrating technologically sophisticated facilities such as cloud computing and self-service kiosks, the banking sector in India has grown extensively. That not only enabled the industry to improve the effectiveness and nature of its services but also improved economic organizations save a significant quantity of cash.

Virtual assistants are one of the various cutting-edge technologies that series use today. Currently, at an innovative point, those virtual assistants or chatbots can automate many methods to drive enhanced operational comfort for business organizations. [Chatbots](https://planetstoryline.com/use-of-chatbot-in-banking/), driven by AI and machine learning, can bring and maintain clients with excellent effectiveness for their outstanding client care services.

Accenture study disclosed that 71 percent of clients would turn to economic aid chatbots if the choice was offered. Extended digitization and technological alliance are obviously changing the requirements of clients who are now studying for higher effectiveness and more reliable results from all interactions. In order to satisfy these changing customer expectations, many banks are now realizing the emerging demand for technological changes. Chatbots hold the potential to significantly reduce the value of human effort needed to deliver seamless customer activity in the banking field on the back of their best services. Here are several ways that banks in their client service results can drive profit from [chatbots:](https://planetstoryline.com/use-of-chatbot-in-banking/)

 [https://planetstoryline.com/use-of-chatbot-in-banking/](https://planetstoryline.com/use-of-chatbot-in-banking/)",0,1
1175,2019-7-18,2019,7,18,19,cequpl,[D] Intuition behind positional encodings in Transformers.,https://www.reddit.com/r/MachineLearning/comments/cequpl/d_intuition_behind_positional_encodings_in/,bytestorm95,1563446615,"Hi, I was wondering why the positional encoding in transformer works the way it does. Like why add the positional encoding with word embedding and not multiply or some other operation? Is there any explanation behind why this works and why not some other method is used?",11,11
1176,2019-7-18,2019,7,18,20,cer746, | AI AINOW,https://www.reddit.com/r/MachineLearning/comments/cer746/_ai_ainow/,kailashahirwar12,1563448912,,0,1
1177,2019-7-18,2019,7,18,20,cer7j5,Deeplearning.ai without ML coursera?,https://www.reddit.com/r/MachineLearning/comments/cer7j5/deeplearningai_without_ml_coursera/,swaroop_2000,1563448997,[removed],0,1
1178,2019-7-18,2019,7,18,20,cerdwo,Loss gradient for Adversarial Images?,https://www.reddit.com/r/MachineLearning/comments/cerdwo/loss_gradient_for_adversarial_images/,A27_97,1563450122,[removed],0,1
1179,2019-7-18,2019,7,18,20,cered0,Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches,https://www.reddit.com/r/MachineLearning/comments/cered0/are_we_really_making_much_progress_a_worrying/,luminoumen,1563450207,,60,264
1180,2019-7-18,2019,7,18,21,cerzp0,"Data Engineering, Big Data, and Machine Learning on GCP",https://www.reddit.com/r/MachineLearning/comments/cerzp0/data_engineering_big_data_and_machine_learning_on/,HannahHumphreys,1563453790,[removed],0,1
1181,2019-7-18,2019,7,18,23,cesvo0,[R] A 2019 Guide to Object Detection,https://www.reddit.com/r/MachineLearning/comments/cesvo0/r_a_2019_guide_to_object_detection/,mwitiderrick,1563458547,"I took some time to look at research papers in the object detection area. This [article](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3) is a summary of that research, I might have missed some of your favorite research papers, so feel free to share them in the comments section.",0,4
1182,2019-7-18,2019,7,18,23,cet2mp,What did you guys think about the Neuralink presentation and what it means for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cet2mp/what_did_you_guys_think_about_the_neuralink/,liqui_date_me,1563459504,I thought it was pretty fascinating to be able to gather extremely detailed data about the brain. It might pave the way to diagnose mental disorders much more precisely and accurately. I can imagine using it to discover the exact set of neurons in the brain responsible for diseases like depression or schizophrenia or bipolar disorder.,0,1
1183,2019-7-18,2019,7,18,23,cet35l,Global Mobile Crane Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cet35l/global_mobile_crane_market_report_2019/,jadhavni3,1563459580,[removed],1,1
1184,2019-7-18,2019,7,18,23,cet7uc,A story of the two men who created the first artificial neuron. Walter Pitts and Warren McCulloch.,https://www.reddit.com/r/MachineLearning/comments/cet7uc/a_story_of_the_two_men_who_created_the_first/,Burindunsmor2,1563460237,,1,1
1185,2019-7-18,2019,7,18,23,cetiau,AI for Businesses,https://www.reddit.com/r/MachineLearning/comments/cetiau/ai_for_businesses/,GantMan,1563461657,,0,1
1186,2019-7-19,2019,7,19,0,ceu1g0,[D] A story of the the two men who co-invented the artificial neuron. Walter Pitts and Warren McCulloch. (Pitts being a real life version of Will Hunting),https://www.reddit.com/r/MachineLearning/comments/ceu1g0/d_a_story_of_the_the_two_men_who_coinvented_the/,Burindunsmor2,1563464148,,0,1
1187,2019-7-19,2019,7,19,0,ceu1yl,[P] Uncertainty Estimation in Deep Learning (slides),https://www.reddit.com/r/MachineLearning/comments/ceu1yl/p_uncertainty_estimation_in_deep_learning_slides/,perone,1563464216,"I made available some slides about a presentation from PyData Lisbon called ""Uncertainty Estimation in Deep Learning"", for those who are interested in an intro to the subject, here you are the slides:

https://www.slideshare.net/perone/uncertainty-estimation-in-deep-learning",22,29
1188,2019-7-19,2019,7,19,0,ceu9hu,"To what extent can ML algorithms be used to measure things that cannot be measured directly? (e.g. health, beauty) and what can be done with these measurements?",https://www.reddit.com/r/MachineLearning/comments/ceu9hu/to_what_extent_can_ml_algorithms_be_used_to/,Rainfawkes,1563465220,[removed],0,1
1189,2019-7-19,2019,7,19,1,ceupf7,Difference between ROS and PCL point cloud,https://www.reddit.com/r/MachineLearning/comments/ceupf7/difference_between_ros_and_pcl_point_cloud/,sjking1880,1563467241,[removed],0,1
1190,2019-7-19,2019,7,19,1,ceuxxm,ICLR vs AAAI submission,https://www.reddit.com/r/MachineLearning/comments/ceuxxm/iclr_vs_aaai_submission/,DeepEven,1563468331,"How do you decide between submitting to ICLR vs AAAI? 
I understand that ICLR seems to be a higher tier conference, but for first-time authors, how would you go about assessing the quality of your paper to decide between submitting ICLR and AAAI? 
Also, how would the OpenReview format and the fact that you could submit your improved paper to ICML in case it gets rejected influence your decision?",0,1
1191,2019-7-19,2019,7,19,2,cev5uw,[P] fastai-Serving: running containerized inference with fastai models,https://www.reddit.com/r/MachineLearning/comments/cev5uw/p_fastaiserving_running_containerized_inference/,wronk17,1563469369,"Code: [fastai-serving repo](https://github.com/developmentseed/fastai-serving)

&amp;#x200B;

We've been experimenting with some Fast AI models recently for our remote sensing work. Unfortunately, we ran into a lot of issues when trying to deploy those models on large-scale inference jobs (specifically running land-classification on big satellite imagery datasets). This [fastai-serving](https://github.com/developmentseed/fastai-serving) repo is meant to solve this in a way that mimics the [TF Serving](https://www.tensorflow.org/tfx/guide/serving) approach/API. Namely, it helps you package a trained model within a small Docker image (running a mini server) so you can make prediction requests via REST POST requests.

&amp;#x200B;

We're working on expanding the functionality (and are very receptive to any help!). For anyone who's running inference on large image sets, we usually spin up multiple of these these inference-ready images and run large batch predictions with our open [chip-n-scale](https://github.com/developmentseed/chip-n-scale-queue-arranger) pipeline.",0,3
1192,2019-7-19,2019,7,19,2,cevsc2,[R] Invitation to join an AI Competition: Reconnaissance Blind Chess (NeurIPS 2019) - AI under Uncertainty,https://www.reddit.com/r/MachineLearning/comments/cevsc2/r_invitation_to_join_an_ai_competition/,rwgardner,1563472262,"**We are hosting a fun, online AI competition.  Participants create a bot that can play chess, but blind and with the ability to privately sense a 3x3 square of the board each turn!  The competition is part of of NeurIPS.  Anyone can participate.**

**$1,000 prize.**

**Participants do not need to attend the NeurIPS conference and there is no cost.**

[Play reconnaissance blind chess now](https://rbc.jhuapl.edu)**.**

&amp;#x200B;

All are invited to participate in an upcoming computer science competition that is being held as part of the 2019 Conference on Neural Information Processing Systems (NeurIPS, [https://nips.cc/](https://nips.cc/)), Reconnaissance Blind Chess.

Many of the favorite studied games in artificial intelligence (AI) such as checkers, chess, and Go lack something that is common and critical in real-life decision making, uncertainty.

This is a competition with a simple but powerful twist on what may be considered the most classic game in AI history, chess. Reconnaissance Blind Chess (RBC) is like chess except a player cannot see where her opponent's pieces are a priori. Rather, she learns partial information about them with the ability to sense a 3x3 square of the board each turn and from the results of moves.

In comparison to poker, which seems to be the most popularly studied game of imperfect information, RBC includes a critical component of long-term planning. Compared to phantom games like Kriegspiel, in RBC players have much more ability to manage their uncertainty, which we believe makes the game more interesting from an AI perspective and more realistic for most scenarios; players are not completely blind, but rather, metaphorically, they simply cannot look everywhere at once.

Participants are welcome to use any code or libraries available.

For more information on the NeurIPS competition, the game itself, or the API, or to play the game to get a feel for it, visit our website below.

All are welcome to create the best RBC bot they can at no cost, and see how well it can play against other bots in the tournament starting on October 21, 2019!

&amp;#x200B;

[https://rbc.jhuapl.edu](http://rbc.jhuapl.edu)

https://i.redd.it/3gthot4i43b31.png

(on twitter: [https://twitter.com/ryan\_w\_gardner/status/1151911206019567617](https://twitter.com/ryan_w_gardner/status/1151911206019567617) )",20,14
1193,2019-7-19,2019,7,19,3,cevyrh,Future of Self-Driving Taxis: Safer Transportation For Women,https://www.reddit.com/r/MachineLearning/comments/cevyrh/future_of_selfdriving_taxis_safer_transportation/,chkslry,1563473090,,0,1
1194,2019-7-19,2019,7,19,4,cewpqp,[D] Citing papers released after review process has started,https://www.reddit.com/r/MachineLearning/comments/cewpqp/d_citing_papers_released_after_review_process_has/,syrahshiraz,1563476640,This question is more applicable to journals (esp. in non-CS fields) where the turnaround can be much longer than conferences. Basically how do you decide whether to cite related papers made available/published after you submitted your paper or put it on arxiv? Do you impose a temporal barrier on information based on some date or do you adjust your paper (esp. literature review) as things become available during the review process? Is it OK to cite a paper that cites another paper which in turn cites your arxiv preprint?,2,6
1195,2019-7-19,2019,7,19,4,cewzct,Merging datasets with heterogenous variables,https://www.reddit.com/r/MachineLearning/comments/cewzct/merging_datasets_with_heterogenous_variables/,nirtiac,1563477924,"I'm trying to do a literature review on merging datasets with similar yet different variables - think one using a standard questionnaire on anxiety, and the other using a customized version of the same questionnaire where the scales are changed and the wording is slightly different. Does anyone have any leads? Publications in any discipline are welcome :)",0,1
1196,2019-7-19,2019,7,19,4,cewznw,Researchers Fool LiDAR with 3D-Printed Adversarial Objects,https://www.reddit.com/r/MachineLearning/comments/cewznw/researchers_fool_lidar_with_3dprinted_adversarial/,Yuqing7,1563477963,,0,1
1197,2019-7-19,2019,7,19,5,cexr6r,Explain Edge Computing ML to me,https://www.reddit.com/r/MachineLearning/comments/cexr6r/explain_edge_computing_ml_to_me/,LucidSnow,1563481590,"What exactly is the benefit of edge computing for things like TensorFlow. I thought you could always pretrain a model on a local computer and then load it on the mobile device etc then do inferencing. TensorFlow doesn't always require tapping into a cloud does it? If i can load a pretrained model and do ML on a mobile device, why do I need an edge co processor for offline inferencing?",0,1
1198,2019-7-19,2019,7,19,5,cext8t,[D] Good Alternatives to pyLDAvis?,https://www.reddit.com/r/MachineLearning/comments/cext8t/d_good_alternatives_to_pyldavis/,Fender6969,1563481863,"Working on a Topic Modeling project and I came across pyLDAvis for the first time and decided to give it a go. While my corpus is relatively large, it is taking far long than the time it took to preprocess and train my Topic Modeling models to create the visualization. I am unsure if I am doing something wrong, but are there any alternatives to this package? I saw some demos on websites and it just looks amazing.

&amp;#x200B;

For reference, here is the code I am running (model, vectorized data, and vectorizer):

    pyLDAvis.enable_notebook()
    viz = pyLDAvis.sklearn.prepare(lda_model, vectorized_data, count_vect)
    viz

Any suggestions would be wonderful!",8,4
1199,2019-7-19,2019,7,19,5,cexubx,[P] StyleGAN,https://www.reddit.com/r/MachineLearning/comments/cexubx/p_stylegan/,mokeam,1563482010,"Hi everyone,

I need some suggestions on projects to work where I would apply StyleGAN.

Here are some references of projects that has been implemented with this architecture:

http://www.thispersondoesnotexist.com/

https://thisrentaldoesnotexist.com/

https://thisresumedoesnotexist.com/

Please note that this project is just for learning purposes. Thanks in Advance!",7,3
1200,2019-7-19,2019,7,19,5,cexzew,Pytorch C++ Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/cexzew/pytorch_c_reinforcement_learning/,Teenvan1995,1563482682,[removed],0,1
1201,2019-7-19,2019,7,19,6,cey7jx,FaceApp: How Neural Networks can do Wonders,https://www.reddit.com/r/MachineLearning/comments/cey7jx/faceapp_how_neural_networks_can_do_wonders/,12harsharyan,1563483732,,0,1
1202,2019-7-19,2019,7,19,6,cey90u,[D]LSTMs backpropagation from scratch and its trainning doubts,https://www.reddit.com/r/MachineLearning/comments/cey90u/dlstms_backpropagation_from_scratch_and_its/,Dewanik-Koirala,1563483916,"Here is an implementation of LSTMs backpropagation from scratch , I am not sure with the dfhs (derivative of hs state) am I doing it the right way ? Please do correct me where I am wrong in my code I tried for so many times still the LSTMs prediction is horrible does it take longest time to train LSTMs ?? Any Suggestions on Back Propagation is highly appericiated as I can't figure out what is going wrong - Thank you in advance

I tried increasing the iterations to 10,000 from 5,000 which was intially and tried decreasing the learning rate and increasing the batch size
#Forward propagation to store all the state necessary for back prop
    for i in range(nw):
        xp = np.zeros(xl)
        xp[intx] = 1

        x = np.hstack((hs[i-1],xp))
        xs[i] = x
        fg[i] = sigmoid(np.dot(x,wf))
        ig[i] = sigmoid(np.dot(x,wi))
        cg[i] = tangent(np.dot(x,wc))
        csc = (cs[i-1] * sigmoid(np.dot(x,wf))) + (sigmoid(np.dot(x,wi)) * sigmoid(np.dot(x,wc)))
        cs[i] = (cs[i-1] * sigmoid(np.dot(x,wf))) + (sigmoid(np.dot(x,wi)) * sigmoid(np.dot(x,wc)))
        og[i] = sigmoid(np.dot(x,wo))
        hs[i] = sigmoid(np.dot(x,wo)) * tangent(csc)
        hsc = sigmoid(np.dot(x,wo)) * tangent(csc)
        ys[i] = sigmoid(np.dot(hsc,wy))
        intx = np.argmax(vy[i-1])

    dwy = np.zeros((yl,d))
    dwf = np.zeros((xl+yl,yl))
    dwi =  np.zeros((xl+yl,yl))
    dwc =  np.zeros((xl+yl,yl))
    dwo =  np.zeros((xl+yl,yl))

    dfhs = np.zeros(yl)
    dfcs = np.zeros(yl)
    totalError = 0
#Back Propagation
    for i in reversed(range(nw)):
        merror = ys[i] - vy[i]
        dwy += np.dot(np.atleast_2d(hs[i]).T,np.atleast_2d((merror*dsigmoid(ys[i]))))
        error = np.dot(merror,wy.T)
        totalError += np.sum(error)
        e = np.clip(error+dfhs,-6,6)
        dho = tangent(cs[i]) * e
        dho = dsigmoid(og[i]) * dho
        dwo += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dho))
        dc = og[i] * e * dtangent(cs[i])
        dc = np.clip(dc + dfcs,-6,6)
        dhf = cs[i-1] * dc
        dhf = dsigmoid(fg[i]) * dhf
        dwf += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dhf))
        dhi = cg[i] * dc
        dhi = dsigmoid(ig[i]) * dhi 
        dwi += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dhi))
        dhc = ig[i] * dc
        dhc = dsigmoid(cg[i]) * dhi 
        dwc += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dhc))

        dfhs = np.dot(dho,wo.T)[:yl]+np.dot(dhc,wc.T)[:yl]+np.dot(dhi,wi.T)[:yl]+np.dot(dhf,wf.T)[:yl]
        dfcs = fg[i] * dc",2,3
1203,2019-7-19,2019,7,19,6,cey9te,Deep TabNine: A Powerful AI Code Autocompleter For Developers,https://www.reddit.com/r/MachineLearning/comments/cey9te/deep_tabnine_a_powerful_ai_code_autocompleter_for/,Yuqing7,1563484018,,0,1
1204,2019-7-19,2019,7,19,7,cez44i,"Back Drive training of TR2, Slate Robotics, Under $4,000 ; Video",https://www.reddit.com/r/MachineLearning/comments/cez44i/back_drive_training_of_tr2_slate_robotics_under/,FrankSchmidtTinyLabs,1563488153,,0,1
1205,2019-7-19,2019,7,19,7,cez7r6,"[D] Does anyone have an implementation of ""Text-based Editing of Talking-head Video""",https://www.reddit.com/r/MachineLearning/comments/cez7r6/d_does_anyone_have_an_implementation_of_textbased/,zandimna,1563488647,[removed],1,1
1206,2019-7-19,2019,7,19,9,cf0c7n,Help with implementing Word2vec with CommonCrawl,https://www.reddit.com/r/MachineLearning/comments/cf0c7n/help_with_implementing_word2vec_with_commoncrawl/,Different_Junk,1563494637,[removed],0,1
1207,2019-7-19,2019,7,19,9,cf0g36,Research in hardware acceleration of deep learning,https://www.reddit.com/r/MachineLearning/comments/cf0g36/research_in_hardware_acceleration_of_deep_learning/,CArchGuy,1563495222,[removed],0,1
1208,2019-7-19,2019,7,19,9,cf0jx1,Are there any deep learning models that still require floating-point computation in inference?,https://www.reddit.com/r/MachineLearning/comments/cf0jx1/are_there_any_deep_learning_models_that_still/,CArchGuy,1563495847,[removed],0,1
1209,2019-7-19,2019,7,19,9,cf0v0r,Cropping Salient features in an Image,https://www.reddit.com/r/MachineLearning/comments/cf0v0r/cropping_salient_features_in_an_image/,tettusud,1563497593,[removed],0,1
1210,2019-7-19,2019,7,19,10,cf111a,[P] Comprehensive Machine Learning Trading Strategies - Google Colab,https://www.reddit.com/r/MachineLearning/comments/cf111a/p_comprehensive_machine_learning_trading/,OppositeMidnight,1563498524,"100+ Machine Learning Trading Strategies

[https://github.com/firmai/machine-learning-asset-management](https://github.com/firmai/machine-learning-asset-management) 

&amp;#x200B;

\- Deep Learning

\- Reinforcement Learning

\- Evolutionary Strategies

\- Stacked Models",7,160
1211,2019-7-19,2019,7,19,10,cf1ed8,Working with binary valued predictors in classification.,https://www.reddit.com/r/MachineLearning/comments/cf1ed8/working_with_binary_valued_predictors_in/,Cowboy_Yankee,1563500591,[removed],0,1
1212,2019-7-19,2019,7,19,11,cf1xox,[HIRE][REMOTE] We are a team of machine learning engineers looking for someone that can help us with some audio neural net models,https://www.reddit.com/r/MachineLearning/comments/cf1xox/hireremote_we_are_a_team_of_machine_learning/,LukeArrigoni,1563503830,"Hello r/MachineLearning!

&amp;#x200B;

I'm a principal engineer at Arricor AI and we're looking to hire someone to help us on a contract involving audio models. We have a blueprint of what kind of neural network we want to do build, we can offer guidance but ultimately we are looking for someone that can take these blueprints and run with them.

We build our models on modern threadripper boxes with multiple 1080ti's, each box has 64GB of RAM so you don't have to bring your own hardware.

If you're interested, the position is full remote, full or part-time so perfect for those of you that want to moonlight or anyone wanting to cut your teeth on an audio project.

&amp;#x200B;

REQUIREMENTS

* Have a solid, foundational understanding of neural networks.
* Operating knowledge of tensorflow-gpu (python) 
* Great communication skills (slack)

&amp;#x200B;

NICE TO HAVES

* You've read or familiar with this work: [Human Machine Hearing: Extracting Meaning](https://www.amazon.com/Human-Machine-Hearing-Extracting-Meaning/dp/1107007534/ref=sr_1_1?keywords=machine+hearing&amp;qid=1563503600&amp;s=gateway&amp;sr=8-1)
* You've worked with CNN's

&amp;#x200B;

Send resumes along with desired compensation to [luke@arricor.com](mailto:luke@arricor.com)

&amp;#x200B;

Thanks!",0,1
1213,2019-7-19,2019,7,19,12,cf2dl5,Applied Statistical Modeling for Data Analysis in R,https://www.reddit.com/r/MachineLearning/comments/cf2dl5/applied_statistical_modeling_for_data_analysis_in/,HannahHumphreys,1563506519,[removed],0,1
1214,2019-7-19,2019,7,19,12,cf2ism,Deep learning rig specs. Is my i7-9700K CPU compatible with Titan RTX GPU?,https://www.reddit.com/r/MachineLearning/comments/cf2ism/deep_learning_rig_specs_is_my_i79700k_cpu/,Tomerarenai10,1563507412,[removed],0,1
1215,2019-7-19,2019,7,19,12,cf2qcb,Just finished my Machine Learning midterm,https://www.reddit.com/r/MachineLearning/comments/cf2qcb/just_finished_my_machine_learning_midterm/,jackz314,1563508754,[removed],0,1
1216,2019-7-19,2019,7,19,13,cf2r7h,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/cf2r7h/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1563508909,[removed],0,1
1217,2019-7-19,2019,7,19,13,cf2t31,Announcing the DeepRacer Scholarship Challenge from AWS,https://www.reddit.com/r/MachineLearning/comments/cf2t31/announcing_the_deepracer_scholarship_challenge/,ConfidentMushroom,1563509213,,0,1
1218,2019-7-19,2019,7,19,13,cf2tpr,"how to avoid ""trivial"" similarities in a w2v style training?",https://www.reddit.com/r/MachineLearning/comments/cf2tpr/how_to_avoid_trivial_similarities_in_a_w2v_style/,evanthebouncy,1563509321,[removed],0,1
1219,2019-7-19,2019,7,19,13,cf2ve4,[Research]A glass slab does MNIST recognition without a computer.,https://www.reddit.com/r/MachineLearning/comments/cf2ve4/researcha_glass_slab_does_mnist_recognition/,lyt_seeker,1563509607,,0,2
1220,2019-7-19,2019,7,19,14,cf3fm7,K-Multiple-Means: A Multiple-Means Clustering Method with Specified K Clusters,https://www.reddit.com/r/MachineLearning/comments/cf3fm7/kmultiplemeans_a_multiplemeans_clustering_method/,reason_W,1563513287,[removed],0,1
1221,2019-7-19,2019,7,19,14,cf3fxn,"Has anyone analysed game strategies learnt by RL? (e.g. AlphaGo, StarCraft, Open AI 5, etc.)",https://www.reddit.com/r/MachineLearning/comments/cf3fxn/has_anyone_analysed_game_strategies_learnt_by_rl/,Ashes-in-Space,1563513351,[removed],0,0
1222,2019-7-19,2019,7,19,14,cf3ipt,Does Neural ODEs really able to replace backpropagation? Does anyone look into it before?,https://www.reddit.com/r/MachineLearning/comments/cf3ipt/does_neural_odes_really_able_to_replace/,AngusDHelloWorld,1563513892,,0,1
1223,2019-7-19,2019,7,19,14,cf3maj,Quantum machine learning,https://www.reddit.com/r/MachineLearning/comments/cf3maj/quantum_machine_learning/,warlock2404,1563514558,[removed],0,1
1224,2019-7-19,2019,7,19,14,cf3mqe,"Help understanding ""Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders"" paper?",https://www.reddit.com/r/MachineLearning/comments/cf3mqe/help_understanding_deep_unsupervised_clustering/,that_one_ai_nerd,1563514638,[removed],0,1
1225,2019-7-19,2019,7,19,14,cf3pn8,Master in AI or Data Science for machine learning engineer?,https://www.reddit.com/r/MachineLearning/comments/cf3pn8/master_in_ai_or_data_science_for_machine_learning/,drewtrue21,1563515207,[removed],0,1
1226,2019-7-19,2019,7,19,14,cf3psh,"[P] Help implementing ""Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders"" paper?",https://www.reddit.com/r/MachineLearning/comments/cf3psh/p_help_implementing_deep_unsupervised_clustering/,that_one_ai_nerd,1563515237,"Here is the link the paper I am referring to:   [https://arxiv.org/pdf/1611.02648.pdf](https://arxiv.org/pdf/1611.02648.pdf)

&amp;#x200B;

I am having trouble understanding how to implement this paper correctly. So I understand that instead of using an isotropic Gaussian as the prior for the latent space, they are using a mixture of Gaussians. And then I am really having trouble understanding how they are calculating their lower bound, or specifically the terms in it, which are  reconstruction term, conditional prior term, w-prior term and z-prior term. The z-prior term is a direct probability of the class a data point would belong to, and I am not sure where they are getting this from. So if anybody could offer any help or point me to somewhere I could find some help, it would be greatly appreciated!

&amp;#x200B;

And in summary, here are my questions:

1. How are they generating a mixture of Gaussians for the latent space? Does this mean creating n distributions for the latent space (where n is the number of clusters), so basically having n sets of mean and variance layers (each with the size of the latent space), rather than just one set of these layers like in a normal variational autoencoder? Or is the latent space still representing all the data, and then Gaussian distributions are sampled from the latent space?
2. How are they reparameterizing the distributions of multiple distributions (assuming my understanding of how they are doing the multiple Gaussians correct, which it's very likely not)?
3. How are they directly outputting the probability that a sample belongs to a certain distribution, which represents a cluster?

&amp;#x200B;

Any help is much appreciated, thank you!",0,4
1227,2019-7-19,2019,7,19,15,cf401i,one-class classification task and UNSUPERVISED LEARNING OF THE SET OF LOCAL MAXIMA,https://www.reddit.com/r/MachineLearning/comments/cf401i/oneclass_classification_task_and_unsupervised/,albert1905,1563517303,[removed],0,1
1228,2019-7-19,2019,7,19,15,cf412p,[N] Kuzushiji (Medieval Japanese Document Recognition) Competition on Kaggle,https://www.reddit.com/r/MachineLearning/comments/cf412p/n_kuzushiji_medieval_japanese_document/,alexmlamb,1563517510,,0,1
1229,2019-7-19,2019,7,19,15,cf43un,Global CubeSat Market 2019 - 2023,https://www.reddit.com/r/MachineLearning/comments/cf43un/global_cubesat_market_2019_2023/,Shivs7,1563518075,[removed],1,1
1230,2019-7-19,2019,7,19,15,cf4a0y,"""Civilian"" Machine Learning",https://www.reddit.com/r/MachineLearning/comments/cf4a0y/civilian_machine_learning/,jimisommer,1563519371,[removed],0,1
1231,2019-7-19,2019,7,19,16,cf4bt9,MSc in AI or Data Science for machine learning engineer?,https://www.reddit.com/r/MachineLearning/comments/cf4bt9/msc_in_ai_or_data_science_for_machine_learning/,drewtrue21,1563519746,[removed],0,1
1232,2019-7-19,2019,7,19,16,cf4f2z,#Tensorhub: a deep learning python package,https://www.reddit.com/r/MachineLearning/comments/cf4f2z/tensorhub_a_deep_learning_python_package/,SanjyotZade,1563520437,[removed],0,1
1233,2019-7-19,2019,7,19,16,cf4lbt,"Is it possible to productionize NLP models such as Bert and XLnet, which are so huge?",https://www.reddit.com/r/MachineLearning/comments/cf4lbt/is_it_possible_to_productionize_nlp_models_such/,syd67,1563521738,[removed],0,1
1234,2019-7-19,2019,7,19,16,cf4me0,"With the current fear of mass facial recognition surveillance, what practical advice do you have to fool these algorithms?",https://www.reddit.com/r/MachineLearning/comments/cf4me0/with_the_current_fear_of_mass_facial_recognition/,RiceOfDuckness,1563521985,Title title title,0,1
1235,2019-7-19,2019,7,19,16,cf4qfi,[D] Choosing Twitter Keywords,https://www.reddit.com/r/MachineLearning/comments/cf4qfi/d_choosing_twitter_keywords/,temporal_templar,1563522895,"Say I have a general topic I want to find tweets for on twitter. How should I gather a set of keywords relating to the general topic word to search for on twitter? I've been trying to find an established method of this keyword choosing (and not just randomly choose whatever keyword comes up to my head on the topic), but so far to no avail.

&amp;#x200B;

Any help is appreciated!",4,5
1236,2019-7-19,2019,7,19,16,cf4rh5,[P] I made a video review of The Hundred-Page Machine Learning Book,https://www.reddit.com/r/MachineLearning/comments/cf4rh5/p_i_made_a_video_review_of_the_hundredpage/,mrdbourke,1563523124,"I enjoy learning about machine learning.

And I enjoy making videos.

So I made a video reviewing [The Hundred-Page Machine Learning Book](https://themlbook.com) by Andriy Burkov.

I read it from the perspective of a machine learning engineer and still learned a bunch.

If you haven't checked out the book, it's a great concise read. There's nothing like a complex topic explained simply.

If you do watch the video, any advice on ways to improve or future reviews/topics would be greatly appreciated.",15,137
1237,2019-7-19,2019,7,19,17,cf4rw3,How many images needed for fire detection using yolov3?,https://www.reddit.com/r/MachineLearning/comments/cf4rw3/how_many_images_needed_for_fire_detection_using/,ilyesH,1563523222,[removed],0,1
1238,2019-7-19,2019,7,19,17,cf4tlb,Tips for building a machine learning PC (GPU vs CPU?),https://www.reddit.com/r/MachineLearning/comments/cf4tlb/tips_for_building_a_machine_learning_pc_gpu_vs_cpu/,HigoChumbo,1563523581,[removed],0,1
1239,2019-7-19,2019,7,19,17,cf4tt1,KDD videos out,https://www.reddit.com/r/MachineLearning/comments/cf4tt1/kdd_videos_out/,theredlining,1563523635,"SIGKDD 2019 vidos for research papers and applied data science papers are out and can be viewed here

[https://www.youtube.com/channel/UCoDCYyhFebQFF8eJjGM7BAw/playlists](https://www.youtube.com/channel/UCoDCYyhFebQFF8eJjGM7BAw/playlists)

&amp;#x200B;

I have to say I like the soundtrack of this one 

[https://www.youtube.com/watch?v=lsRovB3qNg4&amp;t=29s](https://www.youtube.com/watch?v=lsRovB3qNg4&amp;t=29s)",0,1
1240,2019-7-19,2019,7,19,17,cf4wyt,[D] MSc in AI or Data Science for machine learning engineer?,https://www.reddit.com/r/MachineLearning/comments/cf4wyt/d_msc_in_ai_or_data_science_for_machine_learning/,drewtrue21,1563524380,[removed],0,1
1241,2019-7-19,2019,7,19,17,cf4xhx,Evaluating machine learning explainers?,https://www.reddit.com/r/MachineLearning/comments/cf4xhx/evaluating_machine_learning_explainers/,notna17,1563524500,[removed],0,1
1242,2019-7-19,2019,7,19,17,cf4xnf,[R] SGD momentum optimizer with step estimation by online parabola model,https://www.reddit.com/r/MachineLearning/comments/cf4xnf/r_sgd_momentum_optimizer_with_step_estimation_by/,jarekduda,1563524537,,8,13
1243,2019-7-19,2019,7,19,18,cf5isr,OmniNet: A unified architecture for multi-modal multi-task learning,https://www.reddit.com/r/MachineLearning/comments/cf5isr/omninet_a_unified_architecture_for_multimodal/,subho0406,1563529287,,0,1
1244,2019-7-19,2019,7,19,19,cf60tf,"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier. Hope you guys found it helpful!",https://www.reddit.com/r/MachineLearning/comments/cf60tf/for_me_one_of_the_main_barriers_to_the_world_of/,antaloaalonso,1563533145,,1,1
1245,2019-7-19,2019,7,19,19,cf64mf,[Discussion] Great thread on ML interviews by Chip Huyen on Twitter,https://www.reddit.com/r/MachineLearning/comments/cf64mf/discussion_great_thread_on_ml_interviews_by_chip/,metacurse,1563533945,,0,1
1246,2019-7-19,2019,7,19,20,cf67ga,[Discussion] Great thread on ML interviews by Chip Huyen on Twitter,https://www.reddit.com/r/MachineLearning/comments/cf67ga/discussion_great_thread_on_ml_interviews_by_chip/,metacurse,1563534507,"I think we are still very bad at ML interviews and more the community can discuss and weed out bad practices, the better it is for everyone -- employees, employers, aspirants, companies and the field itself.

Chip Huyen on Twitter made this nice thread on the topic: [https://twitter.com/chipro/status/1152077188985835521](https://twitter.com/chipro/status/1152077188985835521)

Does this resonate with your experience? Do you have tips or insights into ML interviews? Please consider sharing it with the community!

PS: I don't think this thread is particularly applicable to Research Scientist roles at Facebook, Google, DeepMind, OpenAI etc., So if anyone has interesting analyses or stories of those interviews, do consider sharing them.",21,59
1247,2019-7-19,2019,7,19,20,cf6dn2,Deep Exemplar-based Video Colorization,https://www.reddit.com/r/MachineLearning/comments/cf6dn2/deep_exemplarbased_video_colorization/,zhangboknight,1563535710,"We propose to colorize legacy videos based on an reference. Please refer to our CVPR paper for technical details.  [https://arxiv.org/abs/1906.09909](https://arxiv.org/abs/1906.09909)

&amp;#x200B;

We have uploaded video results onto youtube: [https://www.youtube.com/watch?v=HXWR5h5vVYI&amp;feature=youtu.be](https://www.youtube.com/watch?v=HXWR5h5vVYI&amp;feature=youtu.be)

Below is an example video result.

![video](d6l28vonw8b31 ""Original film"")

![video](co0amsmcw8b31 ""Colorized video"")",0,1
1248,2019-7-19,2019,7,19,20,cf6iul,Best Screen Printing Machines,https://www.reddit.com/r/MachineLearning/comments/cf6iul/best_screen_printing_machines/,rapidtag,1563536694,,0,1
1249,2019-7-19,2019,7,19,20,cf6k26,"Changing texture of an object with an ""AR filter"". Ideas/useful frameworks?",https://www.reddit.com/r/MachineLearning/comments/cf6k26/changing_texture_of_an_object_with_an_ar_filter/,imrich-,1563536924,[removed],0,1
1250,2019-7-19,2019,7,19,21,cf6yo4,[D] What is the best tool for browsing time series data with its labels?,https://www.reddit.com/r/MachineLearning/comments/cf6yo4/d_what_is_the_best_tool_for_browsing_time_series/,SuperShinyEyes,1563539458,"Hi,

We have video data along with labels.
The labels contain events for each segment. One segment is 5 seconds.
We need a tool for browsing the content of the video by labels. 
For instance, 
----------------------------------------------------------
|                                                         |
|   ---------------------------------------------------   |
|   |                                                 |   |
|   |                                                 |   |
|   |                                                 |   |
|   |                                                 |   |
|   |                    VIDEO                        |   |
|   |                                                 |   |
|   |                                                 |   |
|   |                                                 |   |
|   |                                                 |   |
|   ---------------------------------------------------   |
|                                                         |
|   ---------------                                       |
|   | 00:00, happy|                                       |
|   ---------------                                       |
|                                                         |
|   -----------------                                     |
|   | 00:05, neutral|                                     |
|   -----------------                                     |
|                                                         |
|   ------------------                                    |
|   | 00:10, surprise|                                    |
|   ------------------                                    |
|                                                         |
-----------------------------------------------------------
If click `00:00, happy`, the video would jump to that segment and play.

We have a prototype tool in pygame but it's not simple to forward the DISPLAY via Docker.
Docker is a must for us as we value reproducibility very highly.

I think a web app could work but I haven't done web apps for years, so I do not know what options there are.
Python would be nice as the machine learning code is in Python but I'm open to anything.
*The only constraint is that there should be zero overhead when using it in a Docker container.*

Thank you!

Br,
Seyoung",6,5
1251,2019-7-19,2019,7,19,22,cf7ery,RoBERTa : Latest Facebook AI Model outperformed XLNet ( Current State of the Art) on GLUE Benchmark,https://www.reddit.com/r/MachineLearning/comments/cf7ery/roberta_latest_facebook_ai_model_outperformed/,work_account_2019,1563542057,,2,3
1252,2019-7-19,2019,7,19,22,cf7u3f,What is the 'magnitude pruning' they talk about in the Lottery Ticket Hypothesis?,https://www.reddit.com/r/MachineLearning/comments/cf7u3f/what_is_the_magnitude_pruning_they_talk_about_in/,JClub,1563544442,,0,1
1253,2019-7-19,2019,7,19,23,cf8eai,[Project] AI Jet Learns to Bomb Missile Launcher,https://www.reddit.com/r/MachineLearning/comments/cf8eai/project_ai_jet_learns_to_bomb_missile_launcher/,brandonhotdog,1563547396,,1,1
1254,2019-7-19,2019,7,19,23,cf8j6n,Data Shapley: Evaluating the value of specific data points when training a model,https://www.reddit.com/r/MachineLearning/comments/cf8j6n/data_shapley_evaluating_the_value_of_specific/,chef_lars,1563548099,,0,1
1255,2019-7-20,2019,7,20,0,cf8vmt,Poor prediction by the regression model. Need help!,https://www.reddit.com/r/MachineLearning/comments/cf8vmt/poor_prediction_by_the_regression_model_need_help/,CrispyChrisChicken,1563549824,[removed],0,1
1256,2019-7-20,2019,7,20,0,cf97z8,[D] Current state of experiment management tools and workflows for going from conception to deployment,https://www.reddit.com/r/MachineLearning/comments/cf97z8/d_current_state_of_experiment_management_tools/,shadowalf,1563551485,"Hi everyone,

I'm curious if others have some insight into effective ways to organize ML development, both in terms of processes and tools. My colleagues and I have been exploring different ways to properly organize an ML project in order to both reduce risk and speed up the overall development time. Our first attempt this was to create a lightweight library that stores what experiment we ran along with the experiment results and a timestamp in a local directory. This works, but as projects get farther along I definitely see the need for more features, such as:

 * A centralized location for experiments, organized by project and experiment IDs
 * Search and comparison tools for analyzing experiment results, so you can easily see what ML experiments led to what conclusions. Search tools that would let us constrain by parameters (what models were used, what hyperparameters were fixed etc.) and results metrics (e.g. accuracy ?  80% wrt. precision &gt; 50%)
 * Ease of reproduction i.e. experiments can be re-ran and verified.
 * Dataset version tracking and integration. i.e. Make it transparent which experiments were ran on variations of a dataset (different cleaning, preparation and ETL procedures).
 * Model serialization and saving
 * Easy ensembling i.e. combining models from experiments (with the same input) to analyze potency of ensembles or stacked models quickly.
 * Git integration
 * Straight-forward deployment of a model (or models) as a scoring node i.e. organized, reproducible, and mostly automated methods for moving from experiment to live predictions
 * [Optional] Additionally, once deployed, making model prediction monitoring and maintenance systematic and not a case-by-case thing.

The principle expressed by these features (imo) is that machine learning (as a community, discipline, and industry) has not totally figured out how to adopt the principles of software engineering yet. There seems to be many attempts now and it is a little bit difficult to wade through all the options, which leads to my main discussion point: what are effective options for getting the functionality I described above? How are other individuals and groups organizing their work to minimize risk and reduce development time?

The options/tools I have considered:
 * Building everything I listed above ourselves (high costs)
 * Deploying and chaining together open source tools (micro services) that capture singular functionalities from my above list. An example is https://github.com/mlflow/mlflow/tree/master/mlflow.
 * Using a cloud service like Azure or AWS SageMaker. SageMaker appears to have a lot the features I discussed and does appear to be highly customizable. Costs are difficult to project though and I am worried about the risk of tying our processes closely to a closed source tool.
 * Some combination of the three above options. We would build ""glue"" to work with other services and AWS deployments.

Thanks for the read and I hope some others have the time to post about their experiences in this problem space!",31,88
1257,2019-7-20,2019,7,20,1,cf9fzw,"AshPy: TensorFlow 2.0 library for distributed training, evaluation, model selection, and fast prototyping",https://www.reddit.com/r/MachineLearning/comments/cf9fzw/ashpy_tensorflow_20_library_for_distributed/,pgaleone,1563552558,,0,1
1258,2019-7-20,2019,7,20,1,cf9gsk,Google Brain &amp; CMU Advance Unsupervised Data Augmentation for SSL,https://www.reddit.com/r/MachineLearning/comments/cf9gsk/google_brain_cmu_advance_unsupervised_data/,Yuqing7,1563552665,,0,1
1259,2019-7-20,2019,7,20,1,cf9gzz,Great upcoming ML/AI events?,https://www.reddit.com/r/MachineLearning/comments/cf9gzz/great_upcoming_mlai_events/,bayesianwannabe1,1563552692,[removed],0,1
1260,2019-7-20,2019,7,20,1,cf9kih,Finding the right model,https://www.reddit.com/r/MachineLearning/comments/cf9kih/finding_the_right_model/,gunonlinn,1563553148,"Hi!  
For now I am making a project of analyse the number relationship  
In the data set:   
It will have two score : Score A and Score B which are predicted by me using excel.  
And There will have two right Score A and Score B provided by my teacher, And each time it will have ONE right answer for every case. In this time we are required to find out how the difference between predicted and the right one and the relationship about the answer  
For example (In this case the right answer is B):   
Me Score A = 3   
Score B = 4  
Teacher   
Score A = 5  
Score B = 6  
Than the difference that I predicted and the teacher will be 2  
I want the machine to help me find out how the bigger or smaller difference will lead to correct or wrong?  


Or..... The other way is using machine learning to predict the answer, We will got serval of data to predict the answer  


Which model is better for me to finish the game :(",0,1
1261,2019-7-20,2019,7,20,1,cf9mww,Cosine similarity between 2 ecommerce products.,https://www.reddit.com/r/MachineLearning/comments/cf9mww/cosine_similarity_between_2_ecommerce_products/,Mohammed-Sunasra,1563553462,[removed],0,1
1262,2019-7-20,2019,7,20,1,cf9yo4,0.000 Categorization Accuracy in Kaggle Competition,https://www.reddit.com/r/MachineLearning/comments/cf9yo4/0000_categorization_accuracy_in_kaggle_competition/,Capn_Sparrow0404,1563555053,[removed],0,1
1263,2019-7-20,2019,7,20,1,cf9yqg,R-Transformer: Recurrent Neural Network Enhanced Transformer,https://www.reddit.com/r/MachineLearning/comments/cf9yqg/rtransformer_recurrent_neural_network_enhanced/,LearnedVector,1563555063,,13,47
1264,2019-7-20,2019,7,20,2,cfa47z,"Building SMILY, a Human-Centric, Similar-Image Search Tool for Pathology",https://www.reddit.com/r/MachineLearning/comments/cfa47z/building_smily_a_humancentric_similarimage_search/,sjoerdapp,1563555786,,0,1
1265,2019-7-20,2019,7,20,2,cfa5m1,[R] Post: An information theoretic view on Invariant Risk Minimization by Arjovsky et al (2019),https://www.reddit.com/r/MachineLearning/comments/cfa5m1/r_post_an_information_theoretic_view_on_invariant/,fhuszar,1563555957,,1,1
1266,2019-7-20,2019,7,20,2,cfacf5,[R] [1907.00031] The Thermodynamic Variational Objective,https://www.reddit.com/r/MachineLearning/comments/cfacf5/r_190700031_the_thermodynamic_variational/,bobchennan,1563556827,,11,23
1267,2019-7-20,2019,7,20,2,cfau5v,Advanced Machine Learning &amp; Data Analysis Projects Bootcamp,https://www.reddit.com/r/MachineLearning/comments/cfau5v/advanced_machine_learning_data_analysis_projects/,HannahHumphreys,1563559097,[removed],0,1
1268,2019-7-20,2019,7,20,3,cfaytx,Pytorch based OmniNet: A unified architecture for multi-modal multi-task learning,https://www.reddit.com/r/MachineLearning/comments/cfaytx/pytorch_based_omninet_a_unified_architecture_for/,osipov,1563559692,,0,1
1269,2019-7-20,2019,7,20,3,cfb7l2,Top Skills required for becoming a machine learning engineer in silicon valley,https://www.reddit.com/r/MachineLearning/comments/cfb7l2/top_skills_required_for_becoming_a_machine/,parijat_bh,1563560818,[removed],0,1
1270,2019-7-20,2019,7,20,3,cfbbn9,How to handle variant image sizes with a fully convolutional network in keras?,https://www.reddit.com/r/MachineLearning/comments/cfbbn9/how_to_handle_variant_image_sizes_with_a_fully/,adeeplearner,1563561346,[removed],0,1
1271,2019-7-20,2019,7,20,3,cfbcam,"The precisionFDA BioCompute Object (BCO) App-a-thon has been extended to October 18, 2019!",https://www.reddit.com/r/MachineLearning/comments/cfbcam/the_precisionfda_biocompute_object_bco_appathon/,hollystephens723,1563561430,[removed],0,2
1272,2019-7-20,2019,7,20,3,cfbdwm,"Open-sourcing CraftAssist, a platform for studying collaborative AI bots in Minecraft",https://www.reddit.com/r/MachineLearning/comments/cfbdwm/opensourcing_craftassist_a_platform_for_studying/,kavyasrinet,1563561643,[removed],0,1
1273,2019-7-20,2019,7,20,3,cfbi6u,Galaxy.AI looking for AI/Computer Vision Scientist,https://www.reddit.com/r/MachineLearning/comments/cfbi6u/galaxyai_looking_for_aicomputer_vision_scientist/,Jackson-Kohan,1563562206,[removed],0,1
1274,2019-7-20,2019,7,20,3,cfbks4,"[D], [N, [R], [P] Open-sourcing CraftAssist, a platform for studying collaborative AI bots in Minecraft",https://www.reddit.com/r/MachineLearning/comments/cfbks4/d_n_r_p_opensourcing_craftassist_a_platform_for/,kavyasrinet,1563562539,"Hey all, we are excited to announce that we just released a framework that has the basic tools for building a collaborative assistant (that uses in-game chat) in Minecraft: https://github.com/facebookresearch/craftassist .

To encourage the wider AI research community to use the CraftAssist platform for their own experiments, we are open-sourcing the framework, as well as a baseline assistant and the tools and data we used to build it. The purpose of building such an assistant is to facilitate the study of agents that can complete tasks specified by dialogue, and eventually, to learn from dialogue interactions. The release includes sequential step-by-step data of human players building more than 2500 houses in Minecraft, semantic segmentation data for those houses, and a large-scale natural language semantic parsing data set.

You can read more about our research in the blogpost: https://ai.facebook.com/blog/craftassist-platform-for-collaborative-minecraft-bots/

We'd love to hear from you!",0,1
1275,2019-7-20,2019,7,20,4,cfbpdb,"[N] Open-sourcing CraftAssist, a platform for studying collaborative AI bots in Minecraft",https://www.reddit.com/r/MachineLearning/comments/cfbpdb/n_opensourcing_craftassist_a_platform_for/,kavyasrinet,1563563149,"Hey all, we are excited to announce that we just released a framework that has the basic tools for building a collaborative assistant (that uses in-game chat) in Minecraft: [https://github.com/facebookresearch/craftassist](https://github.com/facebookresearch/craftassist) .

To encourage the wider AI research community to use the CraftAssist platform for their own experiments, we are open-sourcing the framework, as well as a baseline assistant and the tools and data we used to build it. The purpose of building such an assistant is to facilitate the study of agents that can complete tasks specified by dialogue, and eventually, to learn from dialogue interactions. The release includes sequential step-by-step data of human players building more than 2500 houses in Minecraft, semantic segmentation data for those houses, and a large-scale natural language semantic parsing data set.

You can read more about our research in the blogpost: [https://ai.facebook.com/blog/craftassist-platform-for-collaborative-minecraft-bots/](https://ai.facebook.com/blog/craftassist-platform-for-collaborative-minecraft-bots/)

We'd love to hear from you!",8,33
1276,2019-7-20,2019,7,20,4,cfc260,Simulated Hardware for Training Neuromorphic Networks,https://www.reddit.com/r/MachineLearning/comments/cfc260/simulated_hardware_for_training_neuromorphic/,IcyBaba,1563564866,[removed],0,1
1277,2019-7-20,2019,7,20,4,cfc2lo,Parametric Dropout,https://www.reddit.com/r/MachineLearning/comments/cfc2lo/parametric_dropout/,ZeroMaxinumXZ,1563564921,[removed],0,1
1278,2019-7-20,2019,7,20,5,cfcl6q,[D] ELI5: GPT-2 Model Size?,https://www.reddit.com/r/MachineLearning/comments/cfcl6q/d_eli5_gpt2_model_size/,varkarrus,1563567450,"So, I know that the code for GPT-2 345M is publicly released, and there are people who've been training GPT-2 on various things, such as magic cards, cat names, and facebook messenger posts. I guess my question is, what is preventing people from training their own 1.5B model? Heck, what does the parameters mean?

I've got a coding background but I'm only familiar with the basics of neural nets; only been following it because media synthesis is really fun.",5,1
1279,2019-7-20,2019,7,20,6,cfd8hw,[P] 20+ Machine Learning Courses for Free in 2019,https://www.reddit.com/r/MachineLearning/comments/cfd8hw/p_20_machine_learning_courses_for_free_in_2019/,WiredMahir,1563570700,,0,1
1280,2019-7-20,2019,7,20,6,cfdas4,[R] [1903.05157] Simple Physical Adversarial Examples against End-to-End Autonomous Driving Models,https://www.reddit.com/r/MachineLearning/comments/cfdas4/r_190305157_simple_physical_adversarial_examples/,ajboloor,1563570999,"arxiv: [https://arxiv.org/abs/1903.05157](https://arxiv.org/abs/1903.05157)

Abstract:

&gt;Recent advances in machine learning, especially techniques such as deep neural networks, are promoting a range of high-stakes applications, including autonomous driving, which often relies on deep learning for perception. While deep learning for perception has been shown to be vulnerable to a host of subtle adversarial manipulations of images, end-to-end demonstrations of successful attacks, which manipulate the physical environment and result in physical consequences, are scarce. Moreover, attacks typically involve carefully constructed adversarial examples at the level of pixels. We demonstrate the first end-to-end attacks on autonomous driving in simulation, using simple physically realizable attacks: the painting of black lines on the road. These attacks target deep neural network models for end-to-end autonomous driving control. A systematic investigation shows that such attacks are surprisingly easy to engineer, and we describe scenarios (e.g., right turns) in which they are highly effective, and others that are less vulnerable (e.g., driving straight). Further, we use network deconvolution to demonstrate that the attacks succeed by inducing activation patterns similar to entirely different scenarios used in training.

Recently published my first, first-authored paper in the ML domain. Thought this would be a good place to share and talk about it.

Essentially, we answer the question: Can we paint a line on the road in a way that would confuse an autonomous vehicle? 

* so far we looked only at vision (camera) based end-to-end Imitation Learning and Reinforcement Learning models
* wanted to bring up discussion about adversarial ML against self-driving vehicles",1,0
1281,2019-7-20,2019,7,20,6,cfdhx1,Interpreting Gaussian Process posteriors through a deterministic model,https://www.reddit.com/r/MachineLearning/comments/cfdhx1/interpreting_gaussian_process_posteriors_through/,elenasto,1563571955,[removed],0,1
1282,2019-7-20,2019,7,20,6,cfdt8n,LDA and Gibbs sampling,https://www.reddit.com/r/MachineLearning/comments/cfdt8n/lda_and_gibbs_sampling/,franchare,1563573447,[removed],0,1
1283,2019-7-20,2019,7,20,7,cfe2sh,[D] Conditional GANs and class imbalance,https://www.reddit.com/r/MachineLearning/comments/cfe2sh/d_conditional_gans_and_class_imbalance/,ligamentouscreep,1563574751,"I have a small and highly imbalanced low-resolution image dataset with 6 classes where 50% of the observations are from a single class. With unconditional GANs, I can get stably obtain samples with seemingly sufficient diversity under most setups (architecture, losses, etc). 

&amp;#x200B;

In the conditional case, there is significant class leakage in the samples generated. I've tried various standards on class conditioning such as, (1) conditional batch norm in the generator; (2) a projective layer in the critic and; (3) increasing the batch size significantly to cover more modes in each batch. (1) and (2) seem to be helping, and (3) doesn't appear to be helping with sample diversity and is making sample quality worse. Training is still in early stages though, so maybe things change (or modes collapse).

&amp;#x200B;

Are there any strategies or heuristics for conditional GANs specifically dealing with the class imbalanced case? At this point, I'm considering using balanced subsampled batches in each iteration or weighing the hinge loss by class distribution and hoping for the best.",4,3
1284,2019-7-20,2019,7,20,7,cfe52b,Ideas for cool RL project on the side,https://www.reddit.com/r/MachineLearning/comments/cfe52b/ideas_for_cool_rl_project_on_the_side/,d3fenestrator,1563575076,[removed],0,1
1285,2019-7-20,2019,7,20,7,cfefw5,[Q] Vehicle Color Identification,https://www.reddit.com/r/MachineLearning/comments/cfefw5/q_vehicle_color_identification/,baloglub,1563576608,[removed],0,1
1286,2019-7-20,2019,7,20,8,cfetfr,[P] Generating New Watch Designs With StyleGAN,https://www.reddit.com/r/MachineLearning/comments/cfetfr/p_generating_new_watch_designs_with_stylegan/,NNFAK,1563578556,"I recently generated some new watch designs using StyleGAN, and I thought some of you may find it interesting. All 50,000 images I used to train were sourced from the /r/watches subreddit. These are the results I was able to achieve after 48 hours of training on a GTX 980 Ti. Considering the hardware Nvidia recommends, I'm pretty happy with it!

https://evigio.com/post/generating-new-watch-designs-with-stylegan",14,121
1287,2019-7-20,2019,7,20,9,cffbm7,Help with Mountain Car,https://www.reddit.com/r/MachineLearning/comments/cffbm7/help_with_mountain_car/,arivar,1563581482,[removed],1,1
1288,2019-7-20,2019,7,20,14,cfi65e,"AshPy: TensorFlow 2.0 library for distributed training, evaluation, model selection, and fast prototyping.",https://www.reddit.com/r/MachineLearning/comments/cfi65e/ashpy_tensorflow_20_library_for_distributed/,domac,1563599742,,0,1
1289,2019-7-20,2019,7,20,14,cfibmc,Machine learning PC Building random doubts (July 19),https://www.reddit.com/r/MachineLearning/comments/cfibmc/machine_learning_pc_building_random_doubts_july_19/,HigoChumbo,1563600872,[removed],0,1
1290,2019-7-20,2019,7,20,14,cfiesx,Estimating relevance functions for a search engine using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cfiesx/estimating_relevance_functions_for_a_search/,apanimesh061,1563601550,[removed],0,1
1291,2019-7-20,2019,7,20,15,cfim6l,What is the best books for ML in python,https://www.reddit.com/r/MachineLearning/comments/cfim6l/what_is_the_best_books_for_ml_in_python/,justastudent32,1563603189,[removed],0,1
1292,2019-7-20,2019,7,20,16,cfja0q,Would it be possible for the NSA to use machine learning to determine when a whistleblower is about to leak?,https://www.reddit.com/r/MachineLearning/comments/cfja0q/would_it_be_possible_for_the_nsa_to_use_machine/,Hazzman,1563608729,[removed],0,1
1293,2019-7-20,2019,7,20,17,cfjeul,What is the state of the art of Color Recognition?,https://www.reddit.com/r/MachineLearning/comments/cfjeul/what_is_the_state_of_the_art_of_color_recognition/,buy_some_wow,1563609957,[removed],0,1
1294,2019-7-20,2019,7,20,17,cfjfv2,Question about creating an ANN for forex forcasting !,https://www.reddit.com/r/MachineLearning/comments/cfjfv2/question_about_creating_an_ann_for_forex/,jefferey_kreiger,1563610215,[removed],1,1
1295,2019-7-20,2019,7,20,17,cfjhy0,[D] Should conferences have a policy for papers that clearly have harmful applications?,https://www.reddit.com/r/MachineLearning/comments/cfjhy0/d_should_conferences_have_a_policy_for_papers/,robotvison,1563610741,"[This paper](https://arxiv.org/pdf/1803.08805.pdf) I saw today on ""inferring crowd density from a moving drone camera especially when perspective effects are strong"" will appear in IROS, and it seems like it cannot possibly be used for good or even other related problems. I have also seen similar papers for mass surveillance with drones in the proceedings of a few top-tier CV conferences, and I'm sure there are many similar cases that you all have seen. 

Should these conferences be held accountable for publishing papers with inevitability harmful applications?",15,0
1296,2019-7-20,2019,7,20,17,cfjiry,Using Statistical Data Analysis to promote brands on Facebook,https://www.reddit.com/r/MachineLearning/comments/cfjiry/using_statistical_data_analysis_to_promote_brands/,rohit_lokwani17,1563610961,"
Thinking of starting on your own business ? Having trouble marketing your business online ? Here's a blog which is written at a juncture of Statistical Data Analysis and Brand Marketing on Facebook. It skims through the various metrics important for you to advertise online providing an intuitive understanding of the significant metrics. It also gives you an overview of how to gain insights from data using R programming.

Brand Marketing on Facebook using Statistical Analysis by Rohit Lokwani https://link.medium.com/uX01ARlLpY",0,1
1297,2019-7-20,2019,7,20,17,cfjopm,[Discussion] Would it be possible for the NSA to use machine learning to determine when a whistleblower is about to leak?,https://www.reddit.com/r/MachineLearning/comments/cfjopm/discussion_would_it_be_possible_for_the_nsa_to/,Hazzman,1563612493,"Could they use datasets acquired by collecting their employees social media activity, movement, phone calls, voice recordings, facial recognition, purchases to determine the probably of a potential whistleblower in their organization and intercept before said action took place?",17,0
1298,2019-7-20,2019,7,20,18,cfjtw3,Using Statistical Analysis to promote brands on Facebook,https://www.reddit.com/r/MachineLearning/comments/cfjtw3/using_statistical_analysis_to_promote_brands_on/,rohit_lokwani17,1563613767,[removed],0,1
1299,2019-7-20,2019,7,20,18,cfjxhs,Top Screen Printing Machines,https://www.reddit.com/r/MachineLearning/comments/cfjxhs/top_screen_printing_machines/,rapidtag,1563614615,,0,1
1300,2019-7-20,2019,7,20,18,cfk44c,Automate the diagnosis of Knee Injuries with Deep Learning part 1: an overview of the MRNet Dataset,https://www.reddit.com/r/MachineLearning/comments/cfk44c/automate_the_diagnosis_of_knee_injuries_with_deep/,ahmedbesbes,1563616168,[removed],0,1
1301,2019-7-20,2019,7,20,18,cfk5x7,Automate the diagnosis of Knee Injuries with Deep Learning part 2: Building an ACL tear classifier,https://www.reddit.com/r/MachineLearning/comments/cfk5x7/automate_the_diagnosis_of_knee_injuries_with_deep/,ahmedbesbes,1563616564,"In [this post](https://ahmedbesbes.com/automate-the-diagnosis-of-knee-injuries-with-deep-learning-part-2-building-an-acl-tear-classifier.html), you'll build up on the intuitions you gathered on MRNet data by following the [previous post](https://ahmedbesbes.com/automate-the-diagnosis-of-knee-injuries-with-deep-learning-part-1-an-overview-of-the-mrnet-dataset.html). 

You'll learn how to use PyTorch to train an ACL tear classifier that successfully detects these injuries from MRIs with a very high performance. 

We'll dive into the code and we'll go through various tips and tricks ranging from transfer learning to data augmentation, stacking and handling medical images. 

You'll also learn about optimization tricks as well as how to organize your code efficiently. 

If you're a crafty AI engineer who wants to play with code to learn how things work, just keep reading ! 

&amp;#x200B;

&amp;#x200B;

![img](0mmjmukalfb31)",0,1
1302,2019-7-20,2019,7,20,20,cfl0wi,"NRMSE, NMAE, MASE metrics results for weather forecasting",https://www.reddit.com/r/MachineLearning/comments/cfl0wi/nrmse_nmae_mase_metrics_results_for_weather/,KazuK77,1563623761,[removed],0,1
1303,2019-7-20,2019,7,20,21,cfl6fn,[D] Keras vs TensorFlow vs PyTorch,https://www.reddit.com/r/MachineLearning/comments/cfl6fn/d_keras_vs_tensorflow_vs_pytorch/,Bryan-Ferry,1563624910,I've gotten to grips with the fundamentals and want to start using deep learning. I am aware that these are the three most popular frameworks. Which should I learn? Is it worth my time learning to use more than one? It seems that PyTorch is the most popular on here. I have a book on using TensorFlow so I would like to start there. Would I be wasting my time learning TensorFlow if PyTorch is superior?,1,1
1304,2019-7-20,2019,7,20,21,cflbiw,Can anyone tell how faceapp deployed their model so as to use on Android.,https://www.reddit.com/r/MachineLearning/comments/cflbiw/can_anyone_tell_how_faceapp_deployed_their_model/,shbnm_sam,1563625933,[removed],0,1
1305,2019-7-20,2019,7,20,21,cfleq6,[R] P: A Differentiable Programming System to Bridge Machine Learning and Scientific Computing,https://www.reddit.com/r/MachineLearning/comments/cfleq6/r_p_a_differentiable_programming_system_to_bridge/,Skonagog,1563626564,,50,213
1306,2019-7-20,2019,7,20,22,cflkk5,Robotics and Artificial intelligence in food industry| FMCG Robot | AI And Automation in Restaurants,https://www.reddit.com/r/MachineLearning/comments/cflkk5/robotics_and_artificial_intelligence_in_food/,emmawhitner,1563627770,,0,1
1307,2019-7-20,2019,7,20,23,cfmb2b,[P] Going with the Flow: An Introduction to Normalizing Flows,https://www.reddit.com/r/MachineLearning/comments/cfmb2b/p_going_with_the_flow_an_introduction_to/,gebob19,1563632404,"Recently I made a tutorial on Normalizing Flows (NFs). We cover the advantages of using NFs, the theory which makes them work and we also implement a flow in PyTorch! 

Normalizing Flows are part of the generative model family and are beginning to gain more and more attention due to advantages over other generative models. 

I thought I would share it since it may be helpful to others learning the subject! 

[https://gebob19.github.io](https://gebob19.github.io)",11,35
1308,2019-7-20,2019,7,20,23,cfmbb3,[R] The Many AI Challenges of Hearthstone,https://www.reddit.com/r/MachineLearning/comments/cfmbb3/r_the_many_ai_challenges_of_hearthstone/,baylearn,1563632440,,3,8
1309,2019-7-20,2019,7,20,23,cfmkme,"My own interesting game problem (ML, DP, PP etc -- I'm unsure)",https://www.reddit.com/r/MachineLearning/comments/cfmkme/my_own_interesting_game_problem_ml_dp_pp_etc_im/,rambossa1,1563633953,[removed],0,1
1310,2019-7-20,2019,7,20,23,cfmnx2,[P] Basic reinforcement learning implementations,https://www.reddit.com/r/MachineLearning/comments/cfmnx2/p_basic_reinforcement_learning_implementations/,iyeeke,1563634475,"Hello!

I'm looking to increase my knowledge of reinforcement learning by implementing some standard algorithms (DQN, REINFORCE, A2C, A3C, etc.). Not sure if this is allowed on here but any feedback would be nice!  [https://github.com/uchendui/reinforcement-learning](https://github.com/uchendui/reinforcement-learning)",2,3
1311,2019-7-20,2019,7,20,23,cfmorm,Universal function approximator of bounded range function,https://www.reddit.com/r/MachineLearning/comments/cfmorm/universal_function_approximator_of_bounded_range/,ranirlol,1563634622,"X-post from /r/deeplearning 

I have a question concerning the universal function approximator property. In my case, the function I want to approximate has a range over \[0,infinity). My question is, if I were to use ReLU as the output function a feedforward neural network in order to match the range of the function, would the universal function approximator theorem still apply? From Hornik (1991), the output function is linear. Any papers that tackle this problem?

Note that within my set-up, I'm constrained to use ReLU as the output function.

Thanks a lot.",0,1
1312,2019-7-21,2019,7,21,0,cfmpvi,multifunction Premade pouch filling sealing machines for candy sweets ma...,https://www.reddit.com/r/MachineLearning/comments/cfmpvi/multifunction_premade_pouch_filling_sealing/,YQPACK,1563634802,,0,1
1313,2019-7-21,2019,7,21,0,cfmsjd,Help needed in Facial Recognition,https://www.reddit.com/r/MachineLearning/comments/cfmsjd/help_needed_in_facial_recognition/,Mavy75,1563635202,[removed],0,1
1314,2019-7-21,2019,7,21,0,cfmvu6,Supervised word vector dimensionality reduction,https://www.reddit.com/r/MachineLearning/comments/cfmvu6/supervised_word_vector_dimensionality_reduction/,XB0XRecordThat,1563635728,"I want to reduce the size of my word vectors. Specifically, I'm using SpaCy's word vectors that are 300d and want something that is around 30d. 

I have some labels associated with the vectors but they aren't distributed evenly.

Label counts:
- Label 1: 35,000
- Label 2-4: 10,000
- Label 5-8: 3,000
- Label 9-10: 300

I heard it was possible to use a single hidden layer neural net to reduce the vector space but I am worried about the labels with less observations. Especially because I know there are more labels for 5-10 out there but I need this reduced vector space to help me find them...

- Will this still still work?
- Should I sample the labels with more observations down? 
- Or should I just use an unsupervised method to reduce the dimensionality?

Thanks!",0,1
1315,2019-7-21,2019,7,21,0,cfmxb6,What is the best feature selection algorith,https://www.reddit.com/r/MachineLearning/comments/cfmxb6/what_is_the_best_feature_selection_algorith/,rodrigo_sm87,1563635955,[removed],0,1
1316,2019-7-21,2019,7,21,0,cfmxd3,Video colorization based on an example,https://www.reddit.com/r/MachineLearning/comments/cfmxd3/video_colorization_based_on_an_example/,zhangboknight,1563635960," We propose to colorize legacy videos based on a reference. Please refer to our CVPR paper for technical details.[https://arxiv.org/abs/1906.09909](https://arxiv.org/abs/1906.09909)

We have uploaded video results onto youtube: [https://www.youtube.com/watch?v=HXWR5h5vVYI&amp;feature=youtu.be](https://www.youtube.com/watch?v=HXWR5h5vVYI&amp;feature=youtu.be)

Below is an example video result.

![video](uqx5w7ms6hb31 ""Original grayscale video
 "")

![video](r1ds66bu6hb31 ""Colorized result"")",0,1
1317,2019-7-21,2019,7,21,0,cfmzk1,What does following syntax mean - function()(parameter) ?,https://www.reddit.com/r/MachineLearning/comments/cfmzk1/what_does_following_syntax_mean_functionparameter/,Shushrut,1563636290,[removed],0,1
1318,2019-7-21,2019,7,21,0,cfn23p,Introduction to Audio Classification presentation,https://www.reddit.com/r/MachineLearning/comments/cfn23p/introduction_to_audio_classification_presentation/,jonnor,1563636680,[removed],0,1
1319,2019-7-21,2019,7,21,0,cfn480,How to keyword search field or dual-field?,https://www.reddit.com/r/MachineLearning/comments/cfn480/how_to_keyword_search_field_or_dualfield/,QueenLaniakea,1563636994,[removed],0,1
1320,2019-7-21,2019,7,21,0,cfn4bu,[D] How the Transformers broke NLP leaderboards,https://www.reddit.com/r/MachineLearning/comments/cfn4bu/d_how_the_transformers_broke_nlp_leaderboards/,milaworld,1563637009,"*I came across this interesting [article](https://hackingsemantics.xyz/2019/leaderboards/) about whether larger models + more data = progress in ML research.*

**[How the Transformers broke NLP leaderboards](https://hackingsemantics.xyz/2019/leaderboards/)**

*Excerpt:*

The focus of this post is yet another problem with the leaderboards that is relatively recent. Its cause is simple: fundamentally, **a model may be better than its competitors by building better representations from the available data - or it may simply use more data, and/or throw a deeper network at it**. When we have a paper presenting a new model that also uses more data/compute than its competitors, credit attribution becomes hard.

The most popular NLP leaderboards are currently dominated by Transformer-based models. BERT received the best paper award at NAACL 2019 after months of holding SOTA on many leaderboards. Now the hot topic is XLNet that is said to overtake BERT on GLUE and some other benchmarks. Other Transformers include GPT-2, ERNIE, and the list is growing.

The problem were starting to face is that these models are HUGE. While the source code is available, in reality it is beyond the means of an average lab to reproduce these results, or to produce anything comparable. For instance, XLNet is trained on 32B tokens, and the price of using 500 TPUs for 2 days is over $250,000. Even fine-tuning this model is getting expensive.

Wait, this was supposed to happen!

On the one hand, this trend looks predictable, even inevitable: people with more resources *will* use more resources to get better performance. One could even argue that a huge model proves its scalability and fulfils the inherent promise of deep learning, i.e. being able to learn more complex patterns from more information. Nobody knows how much data we actually need to solve a given NLP task, but more should be better, and limiting data seems counter-productive.

On that view - well, from now on top-tier NLP research is going to be something possible only for industry. Academics will have to somehow up their game, either by getting more grants or by collaborating with high-performance computing centers. They are also welcome to switch to analysis, building something on top of the industry-provided huge models, or making datasets.

However, in terms of overall progress in NLP that might not be the best thing to do. The chief problem with the huge models is simply this:

More data &amp; compute = SOTA is **NOT** research news.

If leaderboards are to highlight the actual progress, we need to incentivize new architectures rather than teams outspending each other. Obviously, huge pretrained models are valuable, but unless the authors show that their system consistently behaves differently from its competition with comparable data &amp; compute, it is not clear whether they are presenting a model or a resource.

Furthermore, much of this research is not reproducible: nobody is going to spend $250,000 just to repeat XLNet training. Given the fact that its ablation study showed only 1-2% gain over BERT in 3 datasets out of 4, we dont actually know for sure that its masking strategy is more successful than BERTs.

At the same time, the development of leaner models is dis-incentivized, as their task is fundamentally harder and the leaderboard-oriented community only rewards the SOTA. That, in its turn, prices out of competitions academic teams, which will not result in students becoming better engineers when they graduate.

*Entire article:*

https://hackingsemantics.xyz/2019/leaderboards/",51,212
1321,2019-7-21,2019,7,21,0,cfn5g6,[R] Video colorization based on an example,https://www.reddit.com/r/MachineLearning/comments/cfn5g6/r_video_colorization_based_on_an_example/,zhangboknight,1563637186,"We propose to colorize legacy videos based on a reference. Please refer to our CVPR paper for technical details.[https://arxiv.org/abs/1906.09909](https://arxiv.org/abs/1906.09909)

We have uploaded video results onto youtube: [https://www.youtube.com/watch?v=HXWR5h5vVYI&amp;feature=youtu.be](https://www.youtube.com/watch?v=HXWR5h5vVYI&amp;feature=youtu.be)

Below is an example video result.

&amp;#x200B;

![video](pjbbabtjahb31 ""Original grayscale video"")

![video](vcpogptjahb31 ""Colorization result"")",5,10
1322,2019-7-21,2019,7,21,0,cfn6r9,,https://www.reddit.com/r/MachineLearning/comments/cfn6r9/_/,bash_shocker,1563637392,,0,1
1323,2019-7-21,2019,7,21,0,cfn7gc,FaceApp &amp; Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cfn7gc/faceapp_machine_learning/,iamart_intelligence,1563637487,FaceApp rather than using image stacking or pixel manipulation uses Artificial Intelligence &amp; machine learning based neural networks to bring the changes in the facial features.Comment your thoughts about Machine Learning and FaceApp.,0,1
1324,2019-7-21,2019,7,21,0,cfnawv,[D] Does anyone know about a machine learning tool or any type of tool which can plot the boundaries of a map by the image of the map or in pdf format of map?,https://www.reddit.com/r/MachineLearning/comments/cfnawv/d_does_anyone_know_about_a_machine_learning_tool/,thisappear,1563638018,[removed],0,1
1325,2019-7-21,2019,7,21,1,cfntea,Can Robots and AI have legal personhood?,https://www.reddit.com/r/MachineLearning/comments/cfntea/can_robots_and_ai_have_legal_personhood/,IBSDSCIG,1563640736,[removed],1,1
1326,2019-7-21,2019,7,21,1,cfnus0,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/cfnus0/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1563640949,[removed],0,1
1327,2019-7-21,2019,7,21,2,cfo50h,"[R] natural adversarial examples -- real-world, unmodified, and naturally occurring examples",https://www.reddit.com/r/MachineLearning/comments/cfo50h/r_natural_adversarial_examples_realworld/,downtownslim,1563642452,,2,0
1328,2019-7-21,2019,7,21,2,cfo7ta,"[D] My own interesting game problem (ML, DP, PP etc -- I'm unsure)",https://www.reddit.com/r/MachineLearning/comments/cfo7ta/d_my_own_interesting_game_problem_ml_dp_pp_etc_im/,rambossa1,1563642870,"Looking for possible approaches or solutions to my below game problem. I believe it to be some sort of dynamic programming, machine learning, or probabilistic programming challenge, but am unsure. This is my original problem, and is part of a corporate challenge to create *""unique and challenging problem that you're able to conceptualize and then solve. 3 Judging criteria: uniqueness, complexity, and solution (no particular weighting and scoring may favor uniqueness/challenge over solution"" --* pretty sure the company is trying to do some sort of Ex Machina challenge/reward type of thing.

Inspirations: Conway's Game of Life, DeepMind's Starcraft Challenge, deep Q-learning, probabilistic programming

&amp;#x200B;

**BEAR SURVIVAL**

**=======================**

A bear is preparing for hibernation. A bear must reach life-strength 1000 in order to rest &amp; survive the winter. A bear starts off at a health of 500. A bear explores an environment of *magic berries.* A bear makes a move (chosen randomly with no optional direction) and comes across a berry each time. There are 100 different types of berries that all appear across the wilderness equally and infinitely.

A magic berry always consumes 20 life from the bear upon arrival. A bear may then choose to give more, all, or none of its remaining life to the berry. If eaten, the berry may provide back to the bear 2x the amount of life given. Berries, however, are not the same and a bear knows this. A bear knows that any berry has some percentage of being poisonous.  Of the 100 different types of berries, each may be 0%-100% poisonous. A berry that is 0% poisonous is the perfect berry and a bear knows that it should commit all of its remaining life to receive max health gain.

**Example:** On a bear's first move (at 500 life), it comes across a magic berry and the berry automatically takes 20 life.  The bear notices that the berry is 0% poisonous, the perfect berry, and gives its remaining 480 health, eats the berry, and then receives 1000 health gain. The bear has reached it's goal, hibernates, and wins the game. However, if that first berry was 100% poisonous, the anti-berry, and the bear committed all of its remaining life it would've received back 0 health gain, died, and lost the game. A bear knows to never eat the anti-berry. It knows it can come across any poisonous value from 0-100 (3,25,52,99, etc).

&amp;#x200B;

A bear must be picky &amp; careful, but also bold &amp; smart about how much life it wants to commit per berry, per move. A bear knows that if it never eats, it will eventually die as it loses 20 health per berry, per move.

While it's important for an individual bear to survive, it is even more important for the bear population to not go extinct. A population is going extinct if they lose over half of the population that year.

**Questions:**

1. May we find a bear's optimal strategy for committing health &amp; eating berries to reach 1000 health gain?
2. Is the bear population eventually doomed to a unfavorable environment?

&amp;#x200B;

\*\*Bonus Complexity:\*\*Winter is coming, and conditions grow progressively harsher over time. A bear knows that every 10 moves, each berry will consume **20 \* (fib(i)/ environmentFactor)**. *fib(i) stands for fibonacci-sequence at index i, starting at 1. For all indexes where the progression is less than 20, a berry's initial health consumption remains at 20. environmentFactor is a single environment's progressive-harshness variable (how harsh winter becomes over time).*  The bear population is currently in an environment with environmentFactor of 4. Spelled out:

Moves 01-10: Berries consume 20 --  20\*(1/4)

Moves 10-20: Berries consume 20 --  20\*(1/4)

Moves 20-30: Berries consume 20 --  20\*(2/4)

Moves 30-40: Berries consume 20 --  20\*(3/4)

Moves 40-50: Berries consume 25 --  20\*(5/4)

Moves 50-60: Berries consume 40 --  20\*(8/4)

Moves 60-70: Berries consume 65 --  20\*(13/4)

Moves 70-80: Berries consume 105 --  20\*(21/4)

Moves 80-90: Berries consume 170 --  20\*(34/4)

Moves 90-100: Berries consume 275 --  20\*(55/4)

Moves 100-110: Berries consume 445 --  20\*(89/4)

... and so on ...

&amp;#x200B;

Same questions as above, with a third: if this environment is proven unfavorable, and extinction unavoidable, what maximum environment/environmentFactor must the bear population move to in order to avoid extinction? (this may or may not exist if a berry's requirement of 20 initial life is always unfavorable without any progression).

**Updates/Questions:**

&gt;**QUESTION:**\- Can you give an example of what happens when a bear eats a semi-poisonous berry (e.g. 20%)?  
&gt;  
&gt;\- Also, is the bear always immediately aware of the poison value of berries?  
&gt;  
&gt;\- In this, it also seems like you're using health, life, strength, life-strength, health-gain, etc interchangeably. Are they all the same thing?  
&gt;  
&gt;**ANSWER:**\- the berry eats the poison berry of 20%, then it becomes a probability problem of whether or not the berry provides back life or takes the originally committed. Example: the bear is at  400, the next move &amp; berry take the initial  20 (bear is at 380 now), the bear decides to commit an additional 80 (now at 300) and eat the berry. 8/10 times the berry will return to the bear 200 (2x 100 committed -- bear ends turn at 500) life/health/strength, 2/10 times the berry returns nothing and the bear must move on with 300 life.- the bear is always immediately aware of the poison value of a berry.- life/health/strength are all the same thing.",8,0
1329,2019-7-21,2019,7,21,2,cfobvy,[D] What is the best implementation of neural style transfer in Ubuntu?,https://www.reddit.com/r/MachineLearning/comments/cfobvy/d_what_is_the_best_implementation_of_neural_style/,lorposralem,1563643454,"Per this discussion: [https://www.reddit.com/r/MachineLearning/comments/8o846n/d\_what\_is\_the\_best\_implementation\_of\_neural\_style/](https://www.reddit.com/r/MachineLearning/comments/8o846n/d_what_is_the_best_implementation_of_neural_style/)  


I'm wondering if there has been any updates.  


The consensus seems to be that Jcjohnson still reigns king... However, people found ways to speed it up ([https://github.com/lengstrom/fast-style-transfer](https://github.com/lengstrom/fast-style-transfer)), clean it up ([https://github.com/chuanli11/CNNMRF](https://github.com/chuanli11/CNNMRF)), or combine speed, quality and effectiveness ([https://arxiv.org/pdf/1812.05233.pdf](https://arxiv.org/pdf/1812.05233.pdf)).  


I see there have been some repos where people pulled or forked to create their own. I'm hesitant to approach these for their lack of comments/support.  


I've personally experimented with Jcjohnson and cysmith ([https://github.com/cysmith/neural-style-tf](https://github.com/cysmith/neural-style-tf)) with many happy results. But I'm wondering if people have had luck with others that I've missing.  


Ultimately, I'm looking for a method that can provide accurate style transfers with minimum artifacts (time is less of a priority) with video capabilities. I have a rig with 2 x GTX 1070 running ubuntu 16.04 LTS.   


Cheers,",0,0
1330,2019-7-21,2019,7,21,2,cfoqfg,[D] Becoming One With the Data,https://www.reddit.com/r/MachineLearning/comments/cfoqfg/d_becoming_one_with_the_data/,pirate7777777,1563645486,"Hi everyone, one of our writers wrote [an article](https://blog.floydhub.com/becoming-one-with-the-data/) on [FloydHub blog](https://blog.floydhub.com/). This article is an ensemble of tips and tricks collected from the industry experts (some personal ones too), code walk-throughs and many things more but all heavily focused around the data part - the fuel that drives machine learning projects.

Did we miss anything? Is there something you would like to see in-depth in the next articles?

I hope you enjoy it!",1,0
1331,2019-7-21,2019,7,21,3,cfoyrj,[R] OmniNet: A unified architecture for multi-modal multi-task learning,https://www.reddit.com/r/MachineLearning/comments/cfoyrj/r_omninet_a_unified_architecture_for_multimodal/,subho0406,1563646664,,1,1
1332,2019-7-21,2019,7,21,3,cfp0ba,"Hey, need help with a dataset",https://www.reddit.com/r/MachineLearning/comments/cfp0ba/hey_need_help_with_a_dataset/,skrrull,1563646876,[removed],0,1
1333,2019-7-21,2019,7,21,3,cfpg2g,[D] This was posted in r/computerscience,https://www.reddit.com/r/MachineLearning/comments/cfpg2g/d_this_was_posted_in_rcomputerscience/,Destigeous,1563649166,,0,1
1334,2019-7-21,2019,7,21,4,cfpn2c,Getting into Facebook AML?,https://www.reddit.com/r/MachineLearning/comments/cfpn2c/getting_into_facebook_aml/,aqua4811,1563650162,[removed],0,1
1335,2019-7-21,2019,7,21,4,cfpov7,PAC-Learnable Algorithms: Probably Approximately Correct --- With a short but descriptive visualization video,https://www.reddit.com/r/MachineLearning/comments/cfpov7/paclearnable_algorithms_probably_approximately/,PartlyShaderly,1563650425,,0,1
1336,2019-7-21,2019,7,21,4,cfpyzw,Is Reservoir Computing still a thing? I found that approach very interesting when I initially read about it,https://www.reddit.com/r/MachineLearning/comments/cfpyzw/is_reservoir_computing_still_a_thing_i_found_that/,rumborak,1563651892,I looked at the Wikipedia article but it's rather sparse; is this still a field of research these days?,0,1
1337,2019-7-21,2019,7,21,4,cfpznq,GitHub - amitness/retrieve: Fetch pre-trained ML models and cache it locally,https://www.reddit.com/r/MachineLearning/comments/cfpznq/github_amitnessretrieve_fetch_pretrained_ml/,amitness,1563651988,,0,0
1338,2019-7-21,2019,7,21,4,cfq2kl,[D] How does Facebook AML operate?,https://www.reddit.com/r/MachineLearning/comments/cfq2kl/d_how_does_facebook_aml_operate/,aqua4811,1563652412,"Hello [r/ML](https://www.reddit.com/r/ML/),

I am going to join Facebook as research scientist soon, and I've been told I will start with a bootcamp to match with a team.

Did  anyone here go through this process? I was wondering how feasible it is  to join a team within AML from there? From reading online, my  understanding is that FAIR has its separate process and hiring pipeline, and it is impossible to join it after being hired in a general purpose ML role, but I haven't read anything about the hiring process of the  Applied Machine Learning department. Do they have their own hiring  pipeline as well, or are they just one of the possible teams you can  match with during bootcamp?

I have  a PhD and some postdoc experience, not directly in machine learning but  with a lot of ML applications in my research, so I think I'd be a  pretty good fit for a more research oriented team, but I don't know if  that's even an option, and how competitive it is to get into one of  those teams.",6,0
1339,2019-7-21,2019,7,21,5,cfqsw9,how to extract the information from the dataframe?,https://www.reddit.com/r/MachineLearning/comments/cfqsw9/how_to_extract_the_information_from_the_dataframe/,charchit_7,1563656295,[removed],0,1
1340,2019-7-21,2019,7,21,6,cfrf7v,Machine learning has become much more than only a mathematics or statistics problem but one that involves software engineering to a great degree as well.,https://www.reddit.com/r/MachineLearning/comments/cfrf7v/machine_learning_has_become_much_more_than_only_a/,jashshah27,1563659700,[removed],0,1
1341,2019-7-21,2019,7,21,6,cfrgn6,My model won't go over 30% ACC - what should I tweak?,https://www.reddit.com/r/MachineLearning/comments/cfrgn6/my_model_wont_go_over_30_acc_what_should_i_tweak/,GantMan,1563659918,,0,1
1342,2019-7-21,2019,7,21,9,cfswiy,Machine Learning Methods for Material Discovery,https://www.reddit.com/r/MachineLearning/comments/cfswiy/machine_learning_methods_for_material_discovery/,lightninja987,1563668405,[removed],0,1
1343,2019-7-21,2019,7,21,10,cftupu,New model automtic rotary bag filling machine for betel nut granule doyp...,https://www.reddit.com/r/MachineLearning/comments/cftupu/new_model_automtic_rotary_bag_filling_machine_for/,YQPACK,1563674344,,1,1
1344,2019-7-21,2019,7,21,11,cfu6yg,[R] Artificial Neural Networks in Vision: Preceptron | with a short and kinda descriptive video,https://www.reddit.com/r/MachineLearning/comments/cfu6yg/r_artificial_neural_networks_in_vision_preceptron/,PartlyShaderly,1563676567,,0,1
1345,2019-7-21,2019,7,21,11,cfucwd,Brand name horizontal pouch packing machine liquid with PLC control for ...,https://www.reddit.com/r/MachineLearning/comments/cfucwd/brand_name_horizontal_pouch_packing_machine/,YQPACK,1563677678,,1,1
1346,2019-7-21,2019,7,21,13,cfuyzj,"I have a list of 100 pitch decks from entrepreneurs try to solve the same problem and I would like to use machine learning to answer the question of Whats the best pitch deck to the problem? You can use Xprize as an example, they get thousands of pitch decks.",https://www.reddit.com/r/MachineLearning/comments/cfuyzj/i_have_a_list_of_100_pitch_decks_from/,thoncoin,1563681888,[removed],0,1
1347,2019-7-21,2019,7,21,14,cfvszl,Why we use gradient decent,https://www.reddit.com/r/MachineLearning/comments/cfvszl/why_we_use_gradient_decent/,argcoin,1563688152,[removed],0,1
1348,2019-7-21,2019,7,21,15,cfwbvd,{arxiv} How to keyword search field or dual-field?,https://www.reddit.com/r/MachineLearning/comments/cfwbvd/arxiv_how_to_keyword_search_field_or_dualfield/,QueenLaniakea,1563692356,[removed],0,1
1349,2019-7-21,2019,7,21,17,cfwxve,First Step With Alexa (use amazon echo device),https://www.reddit.com/r/MachineLearning/comments/cfwxve/first_step_with_alexa_use_amazon_echo_device/,vinayakp22,1563698126,[removed],0,1
1350,2019-7-21,2019,7,21,17,cfx05q,Factory Price square bottle wrap around label applicator machines sticke...,https://www.reddit.com/r/MachineLearning/comments/cfx05q/factory_price_square_bottle_wrap_around_label/,YQPACK,1563698777,,0,1
1351,2019-7-21,2019,7,21,18,cfx5ph,[D] Good review papers about Image Segmentation?,https://www.reddit.com/r/MachineLearning/comments/cfx5ph/d_good_review_papers_about_image_segmentation/,MasterScrat,1563700280,"I have general knowledge about ML/DL, and good knowledge of Deep Reinforcement Learning. I now want to dive into image segmentation. More specifically, I want to take a stab at this Kaggle challenge: [SIIM-ACR Pneumothorax Segmentation](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation)

What are be the best recent resources? I am mainly looking for review papers and strong blog posts - ideally written resources, which are more efficient to consume than videos.

I have started with this review: [Understanding Deep Learning Techniques for Image Segmentation](https://arxiv.org/abs/1907.06119), and will review the relevant part from the fast.ai course, since a lot of strong participants seem to rely on it on Kaggle.",3,5
1352,2019-7-21,2019,7,21,18,cfxdl1,Using Deep Belief Network as a weight initialization method?,https://www.reddit.com/r/MachineLearning/comments/cfxdl1/using_deep_belief_network_as_a_weight/,eugenelet123,1563702408,[removed],0,1
1353,2019-7-21,2019,7,21,19,cfxkrs,[R] OmniNet is all you need! ;),https://www.reddit.com/r/MachineLearning/comments/cfxkrs/r_omninet_is_all_you_need/,turing_1997,1563704315,"**Paper url:** [https://arxiv.org/abs/1907.07804](https://arxiv.org/abs/1907.07804)

**Code:** [https://github.com/subho406/OmniNet](https://github.com/subho406/OmniNet?fbclid=IwAR08pJ7Fnxq6n4IStevp_dsXCLhak_nW53w1crtn22yGirKBHcwQwH3szYY)

*OmniNet* is the first-ever truly universal architecture for multi-modal multi-task learning. A single OmniNet architecture can encode multiple inputs from almost any real-life domain (txt, image, video) and is capable of asynchronous multi-task learning across a wide range of tasks. The OmniNet architecture consists of multiple sub-networks called the neural peripherals, used to encode domain specific inputs as spatio-temporal representations, connected to a common central neural network called the Central Neural Processor (CNP). The CNP implements a Transformer based universal spatio-temporal encoder and a multi-task decoder. In the paper a single instance of *OmniNet is* jointly trained to perform the tasks of part-of-speech tagging, image captioning, visual question answering and video activity recognition. Due to the shared multi-modal representation learning architecture of the Central Neural Processor, *OmniNet* can also be used for zero-shot prediction for tasks it was never trained on. For example, the multi-model architecture can also be used for video captioning and video question answering even though the model was never trained on those tasks.

&amp;#x200B;

![img](i8b72bjltmb31 ""OmniNet architecture"")

&amp;#x200B;

![gif](he1mg2swtmb31)

![img](76nhr2s0umb31 ""Zero-shot learning predictions on a sample video"")",20,18
1354,2019-7-21,2019,7,21,19,cfxogf,EKS GPU Cluster from Zero to Hero - ITNEXT,https://www.reddit.com/r/MachineLearning/comments/cfxogf/eks_gpu_cluster_from_zero_to_hero_itnext/,alexei_led,1563705287,,0,1
1355,2019-7-21,2019,7,21,19,cfxpxy,BERT's success in some benchmarks tests may be simply due to the exploitation of spurious statistical cues in the dataset. Without them it is no better then random.,https://www.reddit.com/r/MachineLearning/comments/cfxpxy/berts_success_in_some_benchmarks_tests_may_be/,orenmatar,1563705665,,50,380
1356,2019-7-21,2019,7,21,19,cfxre1,Macbook pro EGPU,https://www.reddit.com/r/MachineLearning/comments/cfxre1/macbook_pro_egpu/,Keenobserver1992,1563706044,[removed],0,1
1357,2019-7-21,2019,7,21,19,cfxtpd,Machine Learning Training in Kolkata,https://www.reddit.com/r/MachineLearning/comments/cfxtpd/machine_learning_training_in_kolkata/,TheAmit24-7,1563706633,,0,1
1358,2019-7-21,2019,7,21,20,cfxvhk,horizontal machine packing machinery powder with auger dosing system,https://www.reddit.com/r/MachineLearning/comments/cfxvhk/horizontal_machine_packing_machinery_powder_with/,YQPACK,1563707048,,0,1
1359,2019-7-21,2019,7,21,21,cfye70,New to machine learning: I have a few questions,https://www.reddit.com/r/MachineLearning/comments/cfye70/new_to_machine_learning_i_have_a_few_questions/,tao-nui,1563711392,[removed],0,1
1360,2019-7-21,2019,7,21,21,cfyg9i,Ask your pdf collection for answers,https://www.reddit.com/r/MachineLearning/comments/cfyg9i/ask_your_pdf_collection_for_answers/,Immanuel_Heinen,1563711839,[removed],0,1
1361,2019-7-21,2019,7,21,21,cfyh3m,AWS Course in Kolkata,https://www.reddit.com/r/MachineLearning/comments/cfyh3m/aws_course_in_kolkata/,TheAmit24-7,1563712028,,0,1
1362,2019-7-21,2019,7,21,21,cfyh3t,[D] New to machine learning: I have a few questions,https://www.reddit.com/r/MachineLearning/comments/cfyh3t/d_new_to_machine_learning_i_have_a_few_questions/,tao-nui,1563712028,[removed],0,1
1363,2019-7-21,2019,7,21,21,cfyiwd,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/cfyiwd/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1563712410,[removed],0,1
1364,2019-7-21,2019,7,21,21,cfyktd,An introduction to stationarity in time series data,https://www.reddit.com/r/MachineLearning/comments/cfyktd/an_introduction_to_stationarity_in_time_series/,shaypal5,1563712803,[removed],0,1
1365,2019-7-21,2019,7,21,21,cfymhl,[D] New to machine learning: I have a few questions,https://www.reddit.com/r/MachineLearning/comments/cfymhl/d_new_to_machine_learning_i_have_a_few_questions/,tao-nui,1563713161,"Hi,

I'm a game developer, and with my team at the studio we are doing a bit of r&amp;d to find the fittest (see what I did there) solution to have an AI system for our enemies in in a top down 2D game that would feel organic and unpredictable. We are trying to see if ml agents in Unity could be better suited than the heuristic approach. My main question is:

We are trying to setup the input layer, and we hesitate on how much data we can feed to the ml agents of unity, here is our data:

----

**Potential inputs:**

- 10 raycasts shot radially in front of the player, giving distance to the walls (only) it touches
- Bullet angle of the closest bullets (*4)
- Bullet distance of the closest bullets (*4)
- Bullet orientation (0 is 90 up from enemy - 0.5 is dead on, 1 is 90 down) (*4)
- Player angle (*6)
- Player distance (*6)
- Player shield (*6)
- Player life (*6)
- Player combat mode (4 different modes, sword/shield - gun/shield (etc), 4 modes, input values increasing by 0.25 (*6)

We are trying to figure out what's more important, and what to add

By 'angle', I'm implying the angle between the nose of the enemy and the object (player/bullet)

that means 46 potential inputs for our NN.

----
**Context:**

the player(s) will fight in a coop games, with up to 6 / 8 enemies on screen using non heuristic brains, and other classical AI.

that must be able to run real time with average graphic cards. The game is 2D and doesn't require too much power from cpu/gpu.

----

**Question/ TL;DR:**

- 30~40 inputs * 8 brains in real time on average graphic cards: too much?
- How can we hope to go?
- Any tips on how to think about sizing our NNs ?
- Are you having a good day?

Thank you for the read =)",7,0
1366,2019-7-21,2019,7,21,22,cfyvc4,[YOLOv3] - What would happen if you train a visual object detector on audio?,https://www.reddit.com/r/MachineLearning/comments/cfyvc4/yolov3_what_would_happen_if_you_train_a_visual/,asking_science,1563714897,[removed],0,1
1367,2019-7-21,2019,7,21,22,cfyykf,How generalized can machine learning get?,https://www.reddit.com/r/MachineLearning/comments/cfyykf/how_generalized_can_machine_learning_get/,personness,1563715526,"Hi there! Can anyone point me towards any articles focussing on the most generalisation possible in a model? For example attempting to teach two completely separate tasks and having good metrics in both.

I believe the areas of interest for this are called transfer/multitask/zero-shot learning, but I'm looking for experiment in trying to specifically push the boundaries of how generalized a model can get.",0,1
1368,2019-7-21,2019,7,21,22,cfz01q,Machine Learning now helping to find out the hidden data in Old Research Papers.,https://www.reddit.com/r/MachineLearning/comments/cfz01q/machine_learning_now_helping_to_find_out_the/,iamart_intelligence,1563715807,,0,0
1369,2019-7-21,2019,7,21,22,cfz617,"[D] I have this wild idea for dating site that's only for attractive people, using eigenfaces (it has a million other uses!)",https://www.reddit.com/r/MachineLearning/comments/cfz617/d_i_have_this_wild_idea_for_dating_site_thats/,PartlyShaderly,1563716914,"We all know about eigenfaces, alright? How about we take a picture of some people who we *know* everyone agrees on their attractiveness, make an eigenface for both men and women amongst them, then ask people to send in a driver's license photo (WHAT ARE THEY CALLED?) and when they do, match it with the eigenface. If they are attractive, the alpha or whatever will be positive. Hence only allowing attractive people to become a member of that website. It will also serve as a website to help people know if they are attractive. Of course there are intricacies, such as race and age, but we'll overcome them. This algorithm can be used by casting agents to ""poll"" if someone is beautiful and attractive. It has millions of uses. 

&amp;#x200B;

I think the worst problem we face is fiducial point detection... I don't personally know how to automatically detect fiducial points, and if we let people do it themselves they might cheat.

&amp;#x200B;

So what do you think guys? Doable?",10,0
1370,2019-7-21,2019,7,21,22,cfz6k6,No idea what I'm asking for but here goes...,https://www.reddit.com/r/MachineLearning/comments/cfz6k6/no_idea_what_im_asking_for_but_here_goes/,officialpatterson,1563717013,[removed],0,1
1371,2019-7-22,2019,7,22,1,cg0kcg,Using Satellite Imagery to Prevent Sexual Harassment in India,https://www.reddit.com/r/MachineLearning/comments/cg0kcg/using_satellite_imagery_to_prevent_sexual/,Lordobba,1563724948,,0,1
1372,2019-7-22,2019,7,22,1,cg15un,[D] Conference Author Rebuttal Issues - Did Not Receive Important Email,https://www.reddit.com/r/MachineLearning/comments/cg15un/d_conference_author_rebuttal_issues_did_not/,AnonMLstudent,1563728076,"So for a conference, apparently some people got an email that described author rebuttal as ""only clarify and answer questions and do not improve your paper or add new results"". So myself and coauthors did not receive this email (to the best of our knowledge), and the author rebuttal itself did not contain these instructions.

Hence, we spent a week running new experiments and reporting new (but very positive) results according to reviewer comments.  All the results are good and support our paper, and the stuff we added is a clear improvement. However, will this lead to any risks of rejection on basis such as ""please consider resubmitting to another conference""?

Also the fact that not everybody was emailed by the conference seems somewhat unreasonable, and maybe it is worth contacting the chairs? I did not find out about this email until a week after author responses were due from Twitter lol.",8,1
1373,2019-7-22,2019,7,22,3,cg213e,[R] Neurons spike back: Inductive machines and the AI controversy,https://www.reddit.com/r/MachineLearning/comments/cg213e/r_neurons_spike_back_inductive_machines_and_the/,antonomase,1563732462,,0,1
1374,2019-7-22,2019,7,22,3,cg2bmx,Interview with World's First Triple Kaggle Grandmaster: Abhishek Thakur,https://www.reddit.com/r/MachineLearning/comments/cg2bmx/interview_with_worlds_first_triple_kaggle/,init__27,1563733980,"It's really an honor for me to kick-off The Chai Time Data Science Show with an interview with the Only Triple Grandmaster on Kaggle: 

&amp;#x200B;

Here are the links to the interview. You can find it both in audio and video (of the interview) format:

[Podcast](https://anchor.fm/chaitimedatascience/episodes/Kaggle-Triple-Grandmaster--Abhishek-Thakur-Interview-e4mjoi)

&amp;#x200B;

[Video](https://www.youtube.com/watch?v=vMtORPcjDn8&amp;list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1&amp;index=2)

&amp;#x200B;

I'll be releasing 1 episode every Thursday, Sunday. 

Hope you like it, If you have any ideas/comments/suggestions, I'd really love to hear them.",0,1
1375,2019-7-22,2019,7,22,4,cg2obz,A list of useful Regularizers,https://www.reddit.com/r/MachineLearning/comments/cg2obz/a_list_of_useful_regularizers/,sudeepraja,1563735833,,0,1
1376,2019-7-22,2019,7,22,4,cg2wtt,Beginner question: What makes good training data?,https://www.reddit.com/r/MachineLearning/comments/cg2wtt/beginner_question_what_makes_good_training_data/,rickkava,1563737042,"Hello reddit,
first of all apologies if this is a trivial / noobish question, but as a non-ML person I sometimes struggle with what might be basic stuff.
Here is my question:
In the context of supervised learning, what makes good training data? Suppose I want to generate new training data to enhance my model, how similar or different should the new data be compared to the old one? For example, to train a cat classifier, it would make no sense to add new images of snakes to the set, as the correlation to the original data is too low. Adding more cat pictures where each picture is just one from the original batch with some pixels blurred would also make no sense, as the correlation is too high - there is nothing new in the data set. So how do I find a set that is right in the middle? 
How can I quantify this? What are some of the concepts that should be applied? In what sense does the new data need to be the same, but different?
I am sure there is a correct terminology for what I am asking, I feel like a two-year old who just hasnt learned the proper vocabulary yet.
Please point me in the right direction.
thanks guys!",0,1
1377,2019-7-22,2019,7,22,4,cg308w,How to run OpenAi gym classic control environment on google colab?,https://www.reddit.com/r/MachineLearning/comments/cg308w/how_to_run_openai_gym_classic_control_environment/,The_artist_999,1563737542,How to run the gym Classic control environment on colab with video of working of environment.  There is star-ai github code for Atari environments which run superfine. But same code gives error when recording the working of classic control enviroment. Help me guys I can't find any solution.,0,1
1378,2019-7-22,2019,7,22,4,cg35yf,What is the reputation of UK universities in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cg35yf/what_is_the_reputation_of_uk_universities_in/,Odyssey277,1563738355,[removed],0,1
1379,2019-7-22,2019,7,22,5,cg3dol,"Fundamentals of Deep Learning: Designing Next-Generation Machine Intelligence Algorithms By Nikhil Buduma, Nicholas Locascio PDF",https://www.reddit.com/r/MachineLearning/comments/cg3dol/fundamentals_of_deep_learning_designing/,psychonekk,1563739454,,0,1
1380,2019-7-22,2019,7,22,6,cg446w,A Snifter of Sherry along with the EM Algorithm,https://www.reddit.com/r/MachineLearning/comments/cg446w/a_snifter_of_sherry_along_with_the_em_algorithm/,PartlyShaderly,1563743292,,0,1
1381,2019-7-22,2019,7,22,6,cg4ci5,[D] What do you think of this NLP academic research idea?,https://www.reddit.com/r/MachineLearning/comments/cg4ci5/d_what_do_you_think_of_this_nlp_academic_research/,PartlyShaderly,1563744515,"So my native tongue, Persian, is one of those languages in which you can create *complex* words with the preexisting vocabulary base. Like, imagine this example: A few years ago after reading a dissertation on color correction I downloaded Da Vinci Resolve and realized that color correction is *science* more than an *art* and I wanted to market myself as a color corrector.  But there were no color correctors in Iran, so I had to come up with a name for the service I was offering myself. So I came up with the title of ""rangband"". Rang=color, band=from *bastan*, closing, also means ""setter"". So rangband=colorsetter. Turns out I was an idiot for thinking that I, someone who lacks even a spec of artistic thinking (I do make nice machine learning and signal processing concept videos on [my blog](http://partlyshaderly.com) though) could do color correction, so I gave up on it -- Just like I've given up on anything that I've ever done in my life. However, a few days ago I was thinking that this feature in the Persian language can be harvested into a nice and dandy classification model:

&amp;#x200B;

1- Pair up words and make complex words.

&amp;#x200B;

2- Train the model with the complex words in the dictionary.

&amp;#x200B;

3- Let the model classify if the word has any meaning.

&amp;#x200B;

4- Test it with the complex words in the dictionary.

&amp;#x200B;

This is a binary choice, a complex word either has meaning, or not. So we can use SVMs, something which I love, to get the job done.

&amp;#x200B;

So thoughts? I wanna write an article and I need a professor of linguistics' help, so tell me if I'm not wasting my time, and in turn, his time, with it. Thank you.",43,1
1382,2019-7-22,2019,7,22,6,cg4eko,"[P] Scientific summarization datasets w/accompanying (Beginner Friendly) Colab notebooks to train them with Pointer-Generators, Transformers or Bert. Sources are paper sections (Background, Methods, Results, Conclusions etc.), summaries are corresponding sections in abstract. ~11 Million data points",https://www.reddit.com/r/MachineLearning/comments/cg4eko/p_scientific_summarization_datasets_waccompanying/,BatmantoshReturns,1563744831,"https://snag.gy/XkzUBd.jpg

The dataset is based on the methodology described in this paper https://arxiv.org/abs/1905.07695 by Gidiotis, Tsoumakas which describe using the sections of a structured abstract as the gold standard summaries of their corresponding sections of the paper.

https://snag.gy/YmGADV.jpg

The biggest dataset has ~11 million data points from ~4.3 million papers.

The datasets are in parquet.gz files and can be easily read in python pandas parquet (no need to unzip)

    import pandas as pd
    df = pd.read_parquet( file.parquet.gz )

Furthermore, processing the data and setting up training can shave off of few hours in your, many more if you're unfamiliar with the libraries/repos. So I forked the repos and set up Colab notebook that do all of the heavily lifting, so that you can start training within a few minutes using one of the state of the art architectures for summarization. 

For a quick start, here is a link to the main dataset (there are several others, check out the link at the bottom. 

https://drive.google.com/open?id=1AH3HEDDs08e-xVRLjAev7K902R0eBrcl

Download it into your drive, then use one of the following notebooks that process the dataset and start training on it 

Pointer Generator

https://colab.research.google.com/drive/14-hIiDmUE_qmVK0UHVTjyluHoM1yVKnE

Bert Extractive (BertSum)

https://colab.research.google.com/drive/1IEHBsryjAjddS0jv7oJOi25_TxjVfA4F

Transformer, using Tensor2Tensor

https://colab.research.google.com/drive/1JEfZ2cCJc8Dz_LQMS9_rGgtMgecfXJDG

Here is a link to the full details, including a few other scientific datasets I have created. 

https://github.com/Santosh-Gupta/ScientificSummarizationDataSets/blob/master/ReadMe.md

If you have any trouble, feel free to type a comment or open an issue on Github. I am hoping people can make some pretty effective scientific summarizers using the data.",1,20
1383,2019-7-22,2019,7,22,6,cg4eyu,Change loss function for testing,https://www.reddit.com/r/MachineLearning/comments/cg4eyu/change_loss_function_for_testing/,vratiner,1563744887,"First of all, sorry if I do not get the terminology right, I am a newbie in machine learning.

I am training a neural network providing batches of numpy arrays,  each array consisting of summary statistics (0's, 1's, and 2's)  belonging to one of three possible pure data categories, e.g.

    input_batch1 = np.array([[0, 1, 1, 2, 1, ...],
                             [1, 2, 2, 1, 0, ...],
                                    ...
                             [0, 0, 1, 1, 1, ...]]

where, in this case, the 1st array belongs to the category 2, the 2nd  to the category 3, and the last to the category 1. I also provide the  real one-hot probabilities *p* for calculating the loss, e.g. for the the previous input:

    p_batch1 = np.array([[0, 1, 0],
                         [0, 0, 1],
                            ... 
                         [1, 0, 0]])

I am using the softmax activation function in the output layer, and I calculate the loss with cross-entropy:

    -tf.reduce_sum(p_batch1 * tf.log(softmax_output))

However, I want to test it with summary statistics resulting from  combinations of the 3 possible categories, so it predicts the  proportions from each category. For this testing, I would provide  ""non-one-hot"" probability distributions containing the proportions of  the 3 categories that result in the input sum. stats., like

    p_mixed = np.array([[0.2, 0.8, 0.0],
                        [0.7, 0.2, 0.1], 
                              ...
                        [0.2, 0.3, 0.5]])

where the first array specifies that the input data was the result of  the combination of 20% of the category 1 and 80% of the category 2.

I understand that the proper loss function for this kind of probabilities should be the mean squared difference:

    tf.reduce_mean(tf.squared_difference(softmax_output, p_mixed))

So my questions are:

1) Is it possible to use a different loss function when testing?

2) Can I train the network feeding it sum. stats. from pure  categories and providing the real one-hot probabilities, using  cross-entropy as the loss function, and then test its accuracy with  mixed sum. stats and providing the real proportions of each category  participating in it, evaluate it using mean squared difference as loss  function, and expect it to have a good accuracy?

3) Should I expect better results if I already use the mean squared  difference as the training loss function? Or it does not work well for  one-hot probabilities?

4) Should I better off train it with sum. stats. from mixed  categories? I'd rather use the pure categories for training, but I could  do this if it really is the best practice.",0,1
1384,2019-7-22,2019,7,22,6,cg4j6p,Time units as features vs individual records,https://www.reddit.com/r/MachineLearning/comments/cg4j6p/time_units_as_features_vs_individual_records/,bandalorian,1563745530,"I am working on a project similar to predict likelihood of a student completing an online course given the performance to a given date. The approach taken so far is very similar to this [https://towardsdatascience.com/predicting-success-in-online-education-2b5979fa7016](https://towardsdatascience.com/predicting-success-in-online-education-2b5979fa7016)

&amp;#x200B;

That is, look at various student features such as GPA as well as performance metrics (quiz results, number of logins, time spent on videos etc) at the midpoint of the course, and try to predict if the student will pass the course at the end. Our current (random forest) model has about 80% accuracy which is decent. But now we want to expand these predictions every week throughout the entire course.

&amp;#x200B;

Our current data set uses time units as features. That is, cumulative performance metrics at week 1 is a set of features, cumulative performance week 2 is another set of features etc. So every student has a single record, but the features contain all historical information to a weekly granularity. So the features look something like

&amp;#x200B;

studentid	gpa	major	age	total logins wk1	total time spent wk1	total videos finished wk1	total logins wk2	total time spent wk2	total videos finished wk2	\[and so on\]

&amp;#x200B;

Because of this (and similar to the example in the link), we have to train a model for the specific time point we want to make a prediction for. That is, for week 1 predictions, the data will only have week 1 performance features and  the model gets trained on this. For week 2 predictions we have week 1 and week 2 features and so on. 

&amp;#x200B;

This is all fine and well as the course is only 20 weeks, so we'll end up with 19 models. But let's say we wanted to have daily predictions - we'd have to have over a 100 models. This made me think of if it would make sense to instead use a narrower dataset, where instead the performance being split up in specific time unit features for week 1,2,3 and so on, each student would have a record per week, a single feature for each performance metric, as well as a feature indicating what week (or day) the performance is for.

&amp;#x200B;

This would look something like this:

&amp;#x200B;

studentid	gpa	major	age	**week #**	total logins	total time spent	 total videos finished

&amp;#x200B;

This would allow a single model to be trained on the data. But with a classification algorithm like RF it will take a single record and try to classify it as pass/not pass. But a single record here will lose a lot of historical information, as it will only have the total performance at that time point. In the wider dataset the model can learn trends with how the performance change over the weeks. In the narrow dataset, the model input will only have a single total performance metric to use as inference. Basically if I feed the RF model a narrow record at say week 10, it will not have any information on how performance changed over time for this particular student, just what the most recent total performance is.

&amp;#x200B;

I am hoping to have a discussion how this problem can be approached if you want to make predictions at a higher granularity. To me it seems that eventually it becomes intractable to use the wide data/multiple models version. Say you want to make daily predictions for a four year degree, you would then need 365 x 4 x \[number of performance metrics\], and use 365x4 different models. But what model should be using with a narrow dataset that can understand the performance over time for a given student? It matters if the student started slow but has been steadily improving performance, vs starting strong and dropping off. 

&amp;#x200B;

There is also the added complexity of quizzes and tests which only happen a few times per semester and follows a different format than the other performance metrics. In the wide dataset case this would just be a feature that would only be included in the training set if it had happened.

&amp;#x200B;

Any input here would be greatly appreciated! I am also on the slack channel as Disco Stu if anyone wants to continue the discussion there. Thanks",0,1
1385,2019-7-22,2019,7,22,6,cg4l6n,[R] Unifying Logistical &amp; Statistical Machine Learning with Markov Logic,https://www.reddit.com/r/MachineLearning/comments/cg4l6n/r_unifying_logistical_statistical_machine/,MoBizziness,1563745841,,1,39
1386,2019-7-22,2019,7,22,7,cg4v4q,[D] What are the parameters/inputs for something like GPT-2?,https://www.reddit.com/r/MachineLearning/comments/cg4v4q/d_what_are_the_parametersinputs_for_something/,Miz321,1563747370,"It says like 1.5 Billion parameters or 345 Million, but what would they be for example? I am confused about this. Because to use the network you give it a sentence or two, I don't understand where all these parameters are coming from.",3,0
1387,2019-7-22,2019,7,22,7,cg4w22,[Advice] what are the odds of landing an intership or master with my current academic level,https://www.reddit.com/r/MachineLearning/comments/cg4w22/advice_what_are_the_odds_of_landing_an_intership/,Maminizer,1563747506,"The title pretty much says it all but in brief , my country (Tunisia) doesn't have the usual academic degree rather a LMD system : license / master / docorate.
 
In summer of 2020 , I get my license degree in computer science that occured for 3 years . I'm on the last weeks in andrew's course and i can nail 3 more by the time i get my degree. And with that kind of academic level , what are my chances in landing an intership or master scholarahip in europe or US ? And what areas I need to work on to pump these odds.

Thanks!",0,1
1388,2019-7-22,2019,7,22,7,cg4ylz,Cluster Multivariate Time Series Data,https://www.reddit.com/r/MachineLearning/comments/cg4ylz/cluster_multivariate_time_series_data/,rd4589,1563747883,"Hey All,

I have a multivariate time series data of the stock market and I want to cluster that data.  The purpose of clustering is to divide the data into clusters which have similar kind of volatility(tendency to vary with time). Does anyone know any good technique or research paper that I can try?",0,1
1389,2019-7-22,2019,7,22,7,cg4yno,[D] Change loss function for testing,https://www.reddit.com/r/MachineLearning/comments/cg4yno/d_change_loss_function_for_testing/,vratiner,1563747888,"First of all, sorry if I do not get the terminology right, I am a newbie in machine learning.

I  am training a neural network providing batches of numpy arrays,  each  array consisting of summary statistics (0's, 1's, and 2's)  belonging to  one of three possible pure data categories, e.g.

    input_batch1 = np.array([[0, 1, 1, 2, 1, ...],       
                             [1, 2, 2, 1, 0, ...],                                 
                                    ...
                             [0, 0, 1, 1, 1, ...]] 

where, in this case, the  1st array belongs to the category 2, the 2nd  to the category 3, and  the last to the category 1. I also provide the  real one-hot  probabilities *p* for calculating the loss, e.g. for the the previous input:

    p_batch1 = np.array([[0, 1, 0],
                         [0, 0, 1],
                            ... 
                         [1, 0, 0]]) 

I am using the softmax activation function in the output layer, and I calculate the loss with cross-entropy:

    -tf.reduce_sum(p_batch1 * tf.log(softmax_output)) 

However, I want to test  it with summary statistics resulting from  combinations of the 3  possible categories, so it predicts the  proportions from each category.  For this testing, I would provide  ""non-one-hot"" probability  distributions containing the proportions of  the 3 categories that  result in the input sum. stats., like

    p_mixed = np.array([[0.2, 0.8, 0.0],
                        [0.7, 0.2, 0.1],
                              ...
                        [0.2, 0.3, 0.5]]) 

where the first array  specifies that the input data was the result of  the combination of 20%  of the category 1 and 80% of the category 2.

I understand that the proper loss function for this kind of probabilities should be the mean squared difference:

    tf.reduce_mean(tf.squared_difference(softmax_output, p_mixed)) 

So my questions are:

1. Is it possible to use a different loss function when testing?
2. Can  I train the network feeding it sum. stats. from pure  categories and  providing the real one-hot probabilities, using  cross-entropy as the  loss function, and then test its accuracy with  mixed sum. stats and  providing the real proportions of each category  participating in it,  evaluate it using mean squared difference as loss  function, and expect  it to have a good accuracy?
3. Should  I expect better results if I already use the mean squared  difference  as the training loss function? Or it does not work well for  one-hot  probabilities?
4. Should  I better off train it with sum. stats. from mixed  categories? I'd  rather use the pure categories for training, but I could  do this if it  really is the best practice.",1,1
1390,2019-7-22,2019,7,22,7,cg565f,Optimizing parameters for CNN autoencoder based on training and validation loss,https://www.reddit.com/r/MachineLearning/comments/cg565f/optimizing_parameters_for_cnn_autoencoder_based/,BlackHawk1001,1563749023," Hello everybody

I  have designed an autoencoder with a encoder and decoder consiting of 2D  convolutational layers (the input are 40'000 2D images). I train the  autoencoder using adam optimizer. The autoencoders has the following  hyperparameters which I would like to tune (in brackets are my default  values):

* Number of layers in encoder and decoder (I start with 2 in decoder and encoder)
* Filter size for convolutional layers (I start with 32 and 64)
* Convolutional kernel size (I start with 3x3)
* Stride size (I start with 2x2)
* Dropout (I start with 0.25 after each layer)
* Learning rate (0.001)
* learning\_rate\_decay (0)
* Latent dimension (I start with 8)
* Number of units in the dense layer (layer before creating latent space, I start with 16)
* Batch size (I start with 128)

One  possibility would be to use just grid or random search but this is very  inefficient and takes a long time with so many hyperparameters.  Instead, I would like to observe the training and validation loss (using  tensorboard) and adjust the parameters accordingly. For example when  observing the training and validation loss there could be overfitting or  underfitting (or also an increase in loss etc.).

Are  there some general rules or hints how the hyperparameters could be  adjusted based on the observed losses or based on other criterions?",0,1
1391,2019-7-22,2019,7,22,8,cg5irs,[D] Has anyone used attention as a mechanism for integrating out a dimension in a tensor of unknown size?,https://www.reddit.com/r/MachineLearning/comments/cg5irs/d_has_anyone_used_attention_as_a_mechanism_for/,iamiamwhoami,1563751016,"I frequently run into a problem where I'm dealing with a tensor in a neural network and one of the axes has a dimension of unknown size, which depends on properties of the input data. This can present problems when passing that tensor into fully connected layers, because those expect a tensor of a fixed predetermined size. One thing I've noticed is that attention layers seem to be pretty good at dealing with this problem. They can take an axis of an unknown size and ""integrate"" it out, giving importance to the most relevant entries in that axis. I usually see attention as giving importance to certain words or time stamps in an input series. Does this seem like a valid use for attention?",6,7
1392,2019-7-22,2019,7,22,8,cg5tyi,YQ160 model horizontal form fill seal machine for granule powder liquid ...,https://www.reddit.com/r/MachineLearning/comments/cg5tyi/yq160_model_horizontal_form_fill_seal_machine_for/,Chowtracy,1563752798,,0,1
1393,2019-7-22,2019,7,22,9,cg5zzh,New Items semi automatic hotel soap wrapping machines manufacturers,https://www.reddit.com/r/MachineLearning/comments/cg5zzh/new_items_semi_automatic_hotel_soap_wrapping/,YQPACK,1563753809,,0,1
1394,2019-7-22,2019,7,22,9,cg676c,hot filling and twist off Vacuum capping machines supplier for glass jar...,https://www.reddit.com/r/MachineLearning/comments/cg676c/hot_filling_and_twist_off_vacuum_capping_machines/,Chowtracy,1563754965,,0,1
1395,2019-7-22,2019,7,22,9,cg6hf9,Doom Air,https://www.reddit.com/r/MachineLearning/comments/cg6hf9/doom_air/,nickbild,1563756647,[removed],0,1
1396,2019-7-22,2019,7,22,9,cg6hnc,Manufacturers price wet glue paste labeling machine for round tins cans,https://www.reddit.com/r/MachineLearning/comments/cg6hnc/manufacturers_price_wet_glue_paste_labeling/,YQPACK,1563756683,,1,1
1397,2019-7-22,2019,7,22,10,cg6sld,[R] A Fair Comparison Study of XLNet and BERT with Large Models,https://www.reddit.com/r/MachineLearning/comments/cg6sld/r_a_fair_comparison_study_of_xlnet_and_bert_with/,kimiyoung,1563758510,"[https://medium.com/@xlnet.team/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0](https://medium.com/@xlnet.team/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0)

 

We are the authors of XLNet. We conducted a fair comparison study of XLNet and BERT with large models. In this study, we ensure that almost every possible hyperparameter is the same for the training recipes of both BERT and XLNet, using the same training data.

We have the following interesting observations among others:

1. Trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the datasets.
2. The gains of training on 10x more data are smaller than the gains by switching from BERT to XLNet on 8 out of 11 benchmarks.",21,67
1398,2019-7-22,2019,7,22,10,cg735c,New model automtic rotary pouch packaging machine for betel nut granule ...,https://www.reddit.com/r/MachineLearning/comments/cg735c/new_model_automtic_rotary_pouch_packaging_machine/,YQPACK,1563760296,,1,1
1399,2019-7-22,2019,7,22,11,cg7cui,[Career advice] How difficult is to move from software engineer to a NLP engineer role?,https://www.reddit.com/r/MachineLearning/comments/cg7cui/career_advice_how_difficult_is_to_move_from/,naboo_random,1563761918,[removed],0,0
1400,2019-7-22,2019,7,22,11,cg7eyr,[R] Reinforcement Learning with Chromatic Networks,https://www.reddit.com/r/MachineLearning/comments/cg7eyr/r_reinforcement_learning_with_chromatic_networks/,hardmaru,1563762271,,1,16
1401,2019-7-22,2019,7,22,11,cg7fks,"[R] Deep network as memory space: complexity, generalization, disentangled representation and interpretability",https://www.reddit.com/r/MachineLearning/comments/cg7fks/r_deep_network_as_memory_space_complexity/,baylearn,1563762373,,8,23
1402,2019-7-22,2019,7,22,11,cg7qrs,Where should I start?,https://www.reddit.com/r/MachineLearning/comments/cg7qrs/where_should_i_start/,frosty_frost,1563764303,[removed],0,1
1403,2019-7-22,2019,7,22,12,cg7sof,New Design Automatic Cartesian Robot 3 axis linear,https://www.reddit.com/r/MachineLearning/comments/cg7sof/new_design_automatic_cartesian_robot_3_axis_linear/,Chowtracy,1563764607,,0,1
1404,2019-7-22,2019,7,22,12,cg7ups,BACKRONYM v0.1. The platform for ML methods dependencies 3D visualization is out now!,https://www.reddit.com/r/MachineLearning/comments/cg7ups/backronym_v01_the_platform_for_ml_methods/,postmachines,1563764951,[removed],0,1
1405,2019-7-22,2019,7,22,12,cg7wai,BACKRONYM v0.1. The platform for ML methods dependencies 3D visualization is out now!,https://www.reddit.com/r/MachineLearning/comments/cg7wai/backronym_v01_the_platform_for_ml_methods/,postmachines,1563765224,"At some moment I realized that my knowledge in ML is very limited, and ideas are poor. I know a couple of dozen models and this is a very small part of the total amount that can be very useful to me. 

It seemed to me that if I will understand more models, my qualities as a researcher would certainly increase. This idea strongly motivated me to start meticulously studying articles from the conference.

I had no wish to represent dependencies as a table or a list, I wanted something more natural. A little thought, I realized that to have a strictly fixed graph, with edges between models and their components is interesting.

Work in progress, but I believe, together we can do it much better!

For more information please read the Story: [https://medium.com/@asadulaevarip/how-to-generate-ideas-in-machine-learning-bdb9a7267392](https://t.co/xDuSdKTdn4?amp=1)

Site: [http://infornopolitan.xyz](https://t.co/zjbn3CiUjJ?amp=1)",0,1
1406,2019-7-22,2019,7,22,12,cg7wj4,[P] BACKRONYM v0.1. The platform for ML methods dependencies 3D visualization is out now!,https://www.reddit.com/r/MachineLearning/comments/cg7wj4/p_backronym_v01_the_platform_for_ml_methods/,postmachines,1563765268,[removed],0,1
1407,2019-7-22,2019,7,22,12,cg7z3e,[P] The platform for ML methods dependencies 3D visualization is out now.,https://www.reddit.com/r/MachineLearning/comments/cg7z3e/p_the_platform_for_ml_methods_dependencies_3d/,postmachines,1563765718,"At some moment I realized that my knowledge in ML is very limited, and ideas are poor. I know a couple of dozen models and this is a very small part of the total amount that can be very useful to me. 

  
It seemed to me that if I will understand more models and dependencies between them, my qualities as a researcher would certainly increase. This idea strongly motivated me to start meticulously studying articles from the conference.

  
I had no wish to represent dependencies as a table or a list, I wanted something more natural. A little thought, I realized that to have a strictly fixed graph, with edges between models and their components is interesting.

  
Work in progress, but I believe, together we can do it much better!

  
For more information please read the Story: [https://medium.com/@asadulaevarip/how-to-generate-ideas-in-machine-learning-bdb9a7267392](https://medium.com/@asadulaevarip/how-to-generate-ideas-in-machine-learning-bdb9a7267392)  
Site: [http://infornopolitan.xyz](https://vk.com/away.php?to=http%3A%2F%2Finfornopolitan.xyz&amp;post=-79758651_1816&amp;cc_key=)

Twitter: [https://twitter.com/postmachines/status/1153135594731659264](https://twitter.com/postmachines/status/1153135594731659264)",0,1
1408,2019-7-22,2019,7,22,12,cg85em,machine learning project,https://www.reddit.com/r/MachineLearning/comments/cg85em/machine_learning_project/,jit29,1563766796,"Hi all, I am new to machine learning, but I am currently working on a seemingly intricate machine learning project: I've made a visual recognition system which is able to pinpoint the location of the incoming object on a scale of 600x500 frame. Hence there are five inputs, x, y, displacement\_x, displacement\_y, and it's moving speed. What i'm trying to do is to create a neural network to analyze, based on the trajectory of a moving object (represented by all the variables introduced above)whether the object is gonna hit the camera. Hence there will be only one output, either true or false. Moreover, it must be ""lightweighted"" enough to run on raspberry pi. Can anyone provide me with a direction to look into?",0,1
1409,2019-7-22,2019,7,22,12,cg85w8,[P] Platform for ML methods dependencies 3D visualization is out now!,https://www.reddit.com/r/MachineLearning/comments/cg85w8/p_platform_for_ml_methods_dependencies_3d/,postmachines,1563766882,,0,1
1410,2019-7-22,2019,7,22,12,cg86ag,[D] How to generate ideas in Machine Learning? - Asadulaev Arip - Medium,https://www.reddit.com/r/MachineLearning/comments/cg86ag/d_how_to_generate_ideas_in_machine_learning/,postmachines,1563766956,,0,1
1411,2019-7-22,2019,7,22,12,cg87l4,New Design 30ml Hemp Cbd Oil Filling Machine Manufacturers For Dopper bo...,https://www.reddit.com/r/MachineLearning/comments/cg87l4/new_design_30ml_hemp_cbd_oil_filling_machine/,Chowtracy,1563767190,,0,1
1412,2019-7-22,2019,7,22,14,cg8y8n,[Discussion] Is it possible to make a machine that accurately detects whether or not someone is lying based on video and audio examples?,https://www.reddit.com/r/MachineLearning/comments/cg8y8n/discussion_is_it_possible_to_make_a_machine_that/,Hummer65,1563772073,[removed],0,1
1413,2019-7-22,2019,7,22,15,cg9ety,Implementation of DRL based bidding,https://www.reddit.com/r/MachineLearning/comments/cg9ety/implementation_of_drl_based_bidding/,BrownIndianChief1903,1563775319,,0,1
1414,2019-7-22,2019,7,22,15,cg9klt,What is the best strategy or way to predict multiple time series in a reasonable time ?,https://www.reddit.com/r/MachineLearning/comments/cg9klt/what_is_the_best_strategy_or_way_to_predict/,arkzyyy,1563776471,[removed],0,1
1415,2019-7-22,2019,7,22,15,cg9ofd,Start to Machine learning,https://www.reddit.com/r/MachineLearning/comments/cg9ofd/start_to_machine_learning/,vishallight,1563777238,"I'm new Machine language, I don't know where to start or begin with. 
I'm good at python,cpp,Networking.
Now I would like to start machine learning.
Any syllabus or reference to start up.

Any suggestions would be helpful.",0,1
1416,2019-7-22,2019,7,22,15,cg9oyy,Efficient Semantic Text Retrieval with Google's Universal Sentence Encoder and AquilaDB,https://www.reddit.com/r/MachineLearning/comments/cg9oyy/efficient_semantic_text_retrieval_with_googles/,iamjbn,1563777342,,0,1
1417,2019-7-22,2019,7,22,16,cga0sh,"best #Telegram group about #MachineLearning ,#DeepLearning",https://www.reddit.com/r/MachineLearning/comments/cga0sh/best_telegram_group_about_machinelearning/,Doctor_who1,1563779840,[removed],0,1
1418,2019-7-22,2019,7,22,16,cga2cw,Global Distributed Control System (DCS) Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cga2cw/global_distributed_control_system_dcs_market/,jadhavni3,1563780132,[removed],1,1
1419,2019-7-22,2019,7,22,16,cgaalu,The applications of machine learning in biology and genomics,https://www.reddit.com/r/MachineLearning/comments/cgaalu/the_applications_of_machine_learning_in_biology/,happyremoteworker,1563781978,,0,1
1420,2019-7-22,2019,7,22,17,cgaih5,"Metals, Miners and Forex Premium Membership",https://www.reddit.com/r/MachineLearning/comments/cgaih5/metals_miners_and_forex_premium_membership/,HannahHumphreys,1563783791,[removed],0,1
1421,2019-7-22,2019,7,22,17,cgapst,AutoML Platform Escalates Small And Centralised Data Science Team For Potential Outputs,https://www.reddit.com/r/MachineLearning/comments/cgapst/automl_platform_escalates_small_and_centralised/,analyticsinsight,1563785515,,0,1
1422,2019-7-22,2019,7,22,18,cgasbj,Statistics with Python,https://www.reddit.com/r/MachineLearning/comments/cgasbj/statistics_with_python/,HannahHumphreys,1563786073,[removed],0,1
1423,2019-7-22,2019,7,22,18,cgav2d,Global Postal Automation System Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cgav2d/global_postal_automation_system_market_report_2019/,jadhavni3,1563786679,[removed],1,1
1424,2019-7-22,2019,7,22,18,cgb0dr,JCB Excavator Buckets - 509/00060,https://www.reddit.com/r/MachineLearning/comments/cgb0dr/jcb_excavator_buckets_50900060/,robotcomponents,1563787782,,0,1
1425,2019-7-22,2019,7,22,18,cgb2bg,[P] Brambox: Object detection statistics made easy,https://www.reddit.com/r/MachineLearning/comments/cgb2bg/p_brambox_object_detection_statistics_made_easy/,OPLinux,1563788175,,0,1
1426,2019-7-22,2019,7,22,18,cgb4vh,"Have you ever struggled working with Sparse inputs in Tensorflow Keras, while trying to efficiently release in production ? Well, I did ! And here's why I wrote this medium post! I would be happy to get your feedback, and if you find it interesting, don't forget to  Cheers",https://www.reddit.com/r/MachineLearning/comments/cgb4vh/have_you_ever_struggled_working_with_sparse/,SharoneD,1563788710,,0,1
1427,2019-7-22,2019,7,22,18,cgb648,"[Project] Quotes generating bot ""QuoteMeAI""",https://www.reddit.com/r/MachineLearning/comments/cgb648/project_quotes_generating_bot_quotemeai/,mekass,1563788967,"Hey folks!  


Just finished my project: ""Quote Me AI"" [https://github.com/tomasrasymas/quote-me-ai](https://github.com/tomasrasymas/quote-me-ai) . It's bot that automatically generates quotes and puts them on image and posts that image to r/QuoteMeAI subreddit.   


Take a look!  


Any Feedback is welcome!!!",3,14
1428,2019-7-22,2019,7,22,19,cgbdas,Learning Periodic Functions,https://www.reddit.com/r/MachineLearning/comments/cgbdas/learning_periodic_functions/,mlboi,1563790403,[removed],0,1
1429,2019-7-22,2019,7,22,19,cgbn17,What Machine Learning software would you recommend to me?,https://www.reddit.com/r/MachineLearning/comments/cgbn17/what_machine_learning_software_would_you/,JoseChovi,1563792370,[removed],0,1
1430,2019-7-22,2019,7,22,19,cgboh4,[D] Transfer-Learning / Finetuning pretrained Imagenet CNNs for a resolution higher than 244x244- Any advice?,https://www.reddit.com/r/MachineLearning/comments/cgboh4/d_transferlearning_finetuning_pretrained_imagenet/,AuspiciousApple,1563792657,"Does anyone have experience with finetuning a resnet / VGG model pretrained on imagenet but using a higher resolution for the inputs?

Any advice, papers or slides are most welcome!

Background:

I am currently working on binary classification of images and have had pretty good success by finetuning a resnet50 pretrained on imagenet (this is a pretty good guide [https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) ). I have only a few hundred images of each class, so overfitting is a massive concern and thus unfreezing the lower parts of the resnet would not work.

However, my images are much higher resolution than imagenet - most tutorials I have found seem to rescale images to 244x244 which is what I am doing right now, but I think that downscaling the images a bit less might make the task easier to solve.

Keras resnet50 implementation accepts higher resolution images, but the model doesn't improve during training at all.",0,1
1431,2019-7-22,2019,7,22,19,cgbr62,What is the future of Big Data?,https://www.reddit.com/r/MachineLearning/comments/cgbr62/what_is_the_future_of_big_data/,Sasha-Jelvix,1563793166,[removed],0,1
1432,2019-7-22,2019,7,22,20,cgbv7m,[N] This Week on RL: July 15 to July 21,https://www.reddit.com/r/MachineLearning/comments/cgbv7m/n_this_week_on_rl_july_15_to_july_21/,seungjaeryanlee,1563793915,,0,1
1433,2019-7-22,2019,7,22,20,cgbwk2,"[R] A repository of community detection (graph clustering) research papers with implementations (deep learning, spectral clustering, edge cuts, factorization)",https://www.reddit.com/r/MachineLearning/comments/cgbwk2/r_a_repository_of_community_detection_graph/,benitorosenberg,1563794167,"&amp;#x200B;

![img](fj6royma9ub31)

Link: [https://github.com/benedekrozemberczki/awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection)

The  repository covers techniques such as deep learning, spectral clustering, edge cuts, factorization. I monthly update it with new  papers when something comes out with code.",20,272
1434,2019-7-22,2019,7,22,20,cgbze4,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/cgbze4/machine_learning/,WiltelReddit,1563794701,[removed],0,1
1435,2019-7-22,2019,7,22,20,cgc0yr,500+ Practical Machine Learning and Data Science Projects Sorted by Industry,https://www.reddit.com/r/MachineLearning/comments/cgc0yr/500_practical_machine_learning_and_data_science/,OppositeMidnight,1563794991,[removed],0,1
1436,2019-7-22,2019,7,22,20,cgc1x9,[Project] 500+ Practical Machine Learning and Data Science Projects Sorted by Industry,https://www.reddit.com/r/MachineLearning/comments/cgc1x9/project_500_practical_machine_learning_and_data/,OppositeMidnight,1563795175,"**Industry Machine Learning**

If anyone is a subject expert or simply want to help with the project please send me a pull request or get in contact with me at d.snow\\atsymbolcomeshere\\jbs.cam.ac.uk. Any help on this project would be greatly appreciated.

Its still very fresh so any ideas/feedback are welcome and certainly appreciated. See below for the industries/areas currently covered.

Link: [https://github.com/firmai/industry-machine-learning](https://github.com/firmai/industry-machine-learning)

&amp;#x200B;

1500+ Stars on GitHub:

Join the new list to get access to the catalogue from November 2019 - November 2020.

&amp;#x200B;

||||
|:-|:-|:-|
|[Accommodation &amp; Food](https://github.com/firmai/industry-machine-learning#accommodation)|[Agriculture](https://github.com/firmai/industry-machine-learning#agriculture)|[Banking &amp; Insurance](https://github.com/firmai/industry-machine-learning#bankfin)|
|[Biotechnological &amp; Life Sciences](https://github.com/firmai/industry-machine-learning#biotech)|[Construction &amp; Engineering](https://github.com/firmai/industry-machine-learning#construction)|[Education &amp; Research](https://github.com/firmai/industry-machine-learning#education)|
|[Emergency &amp; Relief](https://github.com/firmai/industry-machine-learning#emergency)|[Finance](https://github.com/firmai/industry-machine-learning#finance)|[Manufacturing](https://github.com/firmai/industry-machine-learning#manufacturing)|
|[Government and Public Works](https://github.com/firmai/industry-machine-learning#public)|[Healthcare](https://github.com/firmai/industry-machine-learning#healthcare)|[Media &amp; Publishing](https://github.com/firmai/industry-machine-learning#media)|
|[Justice, Law and Regulations](https://github.com/firmai/industry-machine-learning#legal)|[Miscellaneous](https://github.com/firmai/industry-machine-learning#miscellaneous)|[Accounting](https://github.com/firmai/industry-machine-learning#accounting)|
|[Real Estate, Rental &amp; Leasing](https://github.com/firmai/industry-machine-learning#realestate)|[Utilities](https://github.com/firmai/industry-machine-learning#utilities)|[Wholesale &amp; Retail](https://github.com/firmai/industry-machine-learning#wholesale)|",0,0
1437,2019-7-22,2019,7,22,20,cgc2p2,Best Guide of Keras Functional API,https://www.reddit.com/r/MachineLearning/comments/cgc2p2/best_guide_of_keras_functional_api/,Aisha_b,1563795316,,0,1
1438,2019-7-22,2019,7,22,20,cgc82w,Catalogue of 500+ Python Machine Learning Applications in Various Industries,https://www.reddit.com/r/MachineLearning/comments/cgc82w/catalogue_of_500_python_machine_learning/,OppositeMidnight,1563796307,[removed],0,1
1439,2019-7-22,2019,7,22,20,cgc8ju,[P] Catalogue of 500+ Python Machine Learning Applications in Various Industries V2,https://www.reddit.com/r/MachineLearning/comments/cgc8ju/p_catalogue_of_500_python_machine_learning/,OppositeMidnight,1563796388,"If anyone is a subject expert or simply want to help with the project please send me a pull request or get in contact with me at d.snow\\atsymbolcomeshere\\jbs.cam.ac.uk. Any help on this project would be greatly appreciated.

Its still very fresh so any ideas/feedback are welcome and certainly appreciated. See below for the industries/areas currently covered.

Link: [https://github.com/firmai/industry-machine-learning](https://github.com/firmai/industry-machine-learning)

||||
|:-|:-|:-|
|[Accommodation &amp; Food](https://github.com/firmai/industry-machine-learning#accommodation)|[Agriculture](https://github.com/firmai/industry-machine-learning#agriculture)|[Banking &amp; Insurance](https://github.com/firmai/industry-machine-learning#bankfin)|
|[Biotechnological &amp; Life Sciences](https://github.com/firmai/industry-machine-learning#biotech)|[Construction &amp; Engineering](https://github.com/firmai/industry-machine-learning#construction)|[Education &amp; Research](https://github.com/firmai/industry-machine-learning#education)|
|[Emergency &amp; Relief](https://github.com/firmai/industry-machine-learning#emergency)|[Finance](https://github.com/firmai/industry-machine-learning#finance)|[Manufacturing](https://github.com/firmai/industry-machine-learning#manufacturing)|
|[Government and Public Works](https://github.com/firmai/industry-machine-learning#public)|[Healthcare](https://github.com/firmai/industry-machine-learning#healthcare)|[Media &amp; Publishing](https://github.com/firmai/industry-machine-learning#media)|
|[Justice, Law and Regulations](https://github.com/firmai/industry-machine-learning#legal)|[Miscellaneous](https://github.com/firmai/industry-machine-learning#miscellaneous)|[Accounting](https://github.com/firmai/industry-machine-learning#accounting)|
|[Real Estate, Rental &amp; Leasing](https://github.com/firmai/industry-machine-learning#realestate)|[Utilities](https://github.com/firmai/industry-machine-learning#utilities)|[Wholesale &amp; Retail](https://github.com/firmai/industry-machine-learning#wholesale)|",7,17
1440,2019-7-22,2019,7,22,20,cgca8b,YQ made Round bottle labeling machine for plastic and glass jar self adh...,https://www.reddit.com/r/MachineLearning/comments/cgca8b/yq_made_round_bottle_labeling_machine_for_plastic/,YQPACK,1563796683,,1,1
1441,2019-7-22,2019,7,22,21,cgcd2t,[D] Learning Periodic Functions,https://www.reddit.com/r/MachineLearning/comments/cgcd2t/d_learning_periodic_functions/,mlboi,1563797180,"I need to learn a function that happens to have periodic components (along with non-periodic stuff) . I'd like to use only deep learning for reasons. Simple feed-forward networks suffer from this problem:

[https://i.redd.it/6xfltw3xs6d21.png](https://i.redd.it/6xfltw3xs6d21.png)

Is there some standard way of doing so? It seems like sine activations are really hard to train, and the Fourier transform is afflicted with the curse of dimensionality, so learning on the frequency domain is not going to work either. Thanks in advance!",12,10
1442,2019-7-22,2019,7,22,21,cgcfx0,Automatic small bottle bottling machine with volumetric filling and cove...,https://www.reddit.com/r/MachineLearning/comments/cgcfx0/automatic_small_bottle_bottling_machine_with/,YQPACK,1563797668,,1,0
1443,2019-7-22,2019,7,22,21,cgchgb,Automatic Screen Printing Machine,https://www.reddit.com/r/MachineLearning/comments/cgchgb/automatic_screen_printing_machine/,rapidtag,1563797921,,0,1
1444,2019-7-22,2019,7,22,21,cgcrbk,[R] Spectral Analysis of Latent Representations,https://www.reddit.com/r/MachineLearning/comments/cgcrbk/r_spectral_analysis_of_latent_representations/,justinshenk,1563799571,"&amp;#x200B;

[Layer saturation is proportion of eigenvalues needed to explain variance of representations over training. Saturation is a useful proxy for model over\/under-parameterization.](https://i.redd.it/z9anea6roub31.jpg)

arXiv: [https://arxiv.org/abs/1907.08589](https://arxiv.org/abs/1907.08589)

PyTorch/Keras implementation: [https://github.com/delve-team/delve](https://github.com/delve-team/delve)  


Metric for analyzing deep neural network layers and fine-tuning architecture. Contributors to code/next paper are welcome!",0,16
1445,2019-7-22,2019,7,22,21,cgcv79,"[P]Seeking feedback and collaborators for an open source, automatic image annotation tool - Annomator - https://github.com/annomator/annomator_1.0",https://www.reddit.com/r/MachineLearning/comments/cgcv79/pseeking_feedback_and_collaborators_for_an_open/,Annomator,1563800218,,0,1
1446,2019-7-22,2019,7,22,22,cgczlb,[N] OpenAI forms exclusive computing partnership with Microsoft to build new Azure AI supercomputing technologies,https://www.reddit.com/r/MachineLearning/comments/cgczlb/n_openai_forms_exclusive_computing_partnership/,Reiinakano,1563800905,"Microsoft press release: https://news.microsoft.com/2019/07/22/openai-forms-exclusive-computing-partnership-with-microsoft-to-build-new-azure-ai-supercomputing-technologies/

OpenAI press release: https://openai.com/blog/microsoft/

Microsoft is investing$1 billion in OpenAI to support us building artificial general intelligence (AGI) withwidely distributedeconomic benefits. Were partnering to develop a hardware and software platform within Microsoft Azure which will scale to AGI. Well jointly develop new Azure AI supercomputing technologies, and Microsoft will become our exclusive cloud providerso well be working hard together to further extend Microsoft Azures capabilities in large-scale AIsystems.",39,56
1447,2019-7-22,2019,7,22,22,cgczmi,"[P] Seeking feedback and collaborators for an open source, automatic image annotation tool - Annomator - https://github.com/annomator/annomator_1.0",https://www.reddit.com/r/MachineLearning/comments/cgczmi/p_seeking_feedback_and_collaborators_for_an_open/,Annomator,1563800910,,0,1
1448,2019-7-22,2019,7,22,22,cgd69p,"[R] Lookahead Optimizer: k steps forward, 1 step back",https://www.reddit.com/r/MachineLearning/comments/cgd69p/r_lookahead_optimizer_k_steps_forward_1_step_back/,NotAlphaGo,1563801915,,12,15
1449,2019-7-22,2019,7,22,22,cgd7rg,Ressources to learn about new progresses in ML,https://www.reddit.com/r/MachineLearning/comments/cgd7rg/ressources_to_learn_about_new_progresses_in_ml/,Arthurion9,1563802160,[removed],0,1
1450,2019-7-22,2019,7,22,22,cgd9le,[P] Predict gender of people over the phone in a highly unbalanced dataset,https://www.reddit.com/r/MachineLearning/comments/cgd9le/p_predict_gender_of_people_over_the_phone_in_a/,newtomtl83,1563802456,"Hi everyone,

&amp;#x200B;

I am an academic researcher venturing into machine learning for one of my projects. I am trying to identify the gender of company executives based on their recorded voice over the phone. Here are the different datasets that I am working with:

1. Prediction dataset: 50k recorded voices analyzed for frequency, 8-10% female.
2. Testing dataset: Subsample of 1k data points the prediction dataset manually coded by me, 20% female (I added more females because I was worried there weren't enough).
3. Training dataset: 10k voices from public datasets, 50% female. Voices are resampled at 8KHz to match phone standards and resemble the final dataset.

I have tried a few different models on the training dataset, with great success. When I split the training dataset 80% 20% to run some tests, I get an accuracy of roughly 97%. When I apply the saved model to the testing dataset from the real population, accuracy drops to 85%. I am worried that this is in part due to the imbalance in gender. 

What would be the best practices to tackle such problem?

Thanks a lot!",7,2
1451,2019-7-22,2019,7,22,22,cgdamc,training YOLOv3 from scratch using only a single image,https://www.reddit.com/r/MachineLearning/comments/cgdamc/training_yolov3_from_scratch_using_only_a_single/,deluded_soul,1563802608,[removed],0,1
1452,2019-7-22,2019,7,22,22,cgddst,New Model vertical box packaging machine for 10-120ml chubby gorilla bot...,https://www.reddit.com/r/MachineLearning/comments/cgddst/new_model_vertical_box_packaging_machine_for/,Chowtracy,1563803067,,0,1
1453,2019-7-22,2019,7,22,22,cgdg7a,"[P] Seeking feedback and collaborators for an open source, automatic image annotation tool - Annomator",https://www.reddit.com/r/MachineLearning/comments/cgdg7a/p_seeking_feedback_and_collaborators_for_an_open/,Annomator,1563803423,# [https://github.com/annomator/annomator\_1.0](https://github.com/annomator/annomator_1.0),0,2
1454,2019-7-22,2019,7,22,23,cgdpsx,[P] The platform for Machine Learning methods dependencies 3D visualization,https://www.reddit.com/r/MachineLearning/comments/cgdpsx/p_the_platform_for_machine_learning_methods/,postmachines,1563804806,,0,1
1455,2019-7-22,2019,7,22,23,cge084,AI Takeover !!! What will happen next?,https://www.reddit.com/r/MachineLearning/comments/cge084/ai_takeover_what_will_happen_next/,vaika-varma,1563806252,,0,1
1456,2019-7-22,2019,7,22,23,cge1f0,Ideas for undergrad research?,https://www.reddit.com/r/MachineLearning/comments/cge1f0/ideas_for_undergrad_research/,LessTell,1563806401,[removed],0,1
1457,2019-7-22,2019,7,22,23,cge4i1,"[R] ""BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics"" - Diagnosing Syntactic Heuristics in Natural Language Inference",https://www.reddit.com/r/MachineLearning/comments/cge4i1/r_bert_a_stateoftheart_model_perform_very_poorly/,downtownslim,1563806836,"Abstract:

&gt;A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area

Paper: [https://arxiv.org/abs/1902.01007](https://arxiv.org/abs/1902.01007)",0,30
1458,2019-7-23,2019,7,23,0,cgejn2,[P] Machine Learning methods dependencies 3D visualization,https://www.reddit.com/r/MachineLearning/comments/cgejn2/p_machine_learning_methods_dependencies_3d/,postmachines,1563808869,"The platform for Machine Learning methods dependencies 3D visualization is available now!

[www.infornopolitan.xyz/backronym](https://www.infornopolitan.xyz/backronym)

&amp;#x200B;

At some moment I realized that my knowledge in ML is very limited, and ideas are poor. I know a couple of dozen models and this is a very small part of the total amount that can be very useful to me. 

It seemed to me that if I will understand more models, my qualities as a researcher would certainly increase. This idea strongly motivated me to start meticulously studying articles from the conference.

I had no wish to represent dependencies as a table or a list, I wanted something more natural. A little thought, I realized that to have a strictly fixed graph, with edges between models and their components is interesting. Work in progress, but I believe, together we can do it much better!",1,1
1459,2019-7-23,2019,7,23,0,cgekd6,[P] Machine Learning methods dependencies 3D visualization,https://www.reddit.com/r/MachineLearning/comments/cgekd6/p_machine_learning_methods_dependencies_3d/,postmachines,1563808960,[removed],0,1
1460,2019-7-23,2019,7,23,0,cgelh9,[Question] How to check residuals for rolling window time series forecast?,https://www.reddit.com/r/MachineLearning/comments/cgelh9/question_how_to_check_residuals_for_rolling/,TurbulentAge,1563809103,[removed],0,1
1461,2019-7-23,2019,7,23,0,cgeoki,Talk with your Data,https://www.reddit.com/r/MachineLearning/comments/cgeoki/talk_with_your_data/,wootnoob,1563809521,,1,1
1462,2019-7-23,2019,7,23,0,cgeqdm,[D] Why We Open Sourced GROVER,https://www.reddit.com/r/MachineLearning/comments/cgeqdm/d_why_we_open_sourced_grover/,hughbzhang,1563809757,Rowan Zellers writes [https://thegradient.pub/why-we-released-grover/](https://thegradient.pub/why-we-released-grover/),0,11
1463,2019-7-23,2019,7,23,0,cgeu71,[D] Talk with your Data,https://www.reddit.com/r/MachineLearning/comments/cgeu71/d_talk_with_your_data/,wootnoob,1563810238,,1,1
1464,2019-7-23,2019,7,23,0,cgewna,New to reinforcement learning. If anyone could solve this issue.,https://www.reddit.com/r/MachineLearning/comments/cgewna/new_to_reinforcement_learning_if_anyone_could/,Shivam_RawatOxox,1563810563,,0,1
1465,2019-7-23,2019,7,23,1,cgf7sl,Big Data Opportunity that Only a Handful Took in 2018,https://www.reddit.com/r/MachineLearning/comments/cgf7sl/big_data_opportunity_that_only_a_handful_took_in/,SunilAhujaa,1563812013,[removed],0,1
1466,2019-7-23,2019,7,23,1,cgf869,ICCV 2019 Accepted Papers Announced,https://www.reddit.com/r/MachineLearning/comments/cgf869/iccv_2019_accepted_papers_announced/,joshuacpeterson,1563812061,,0,1
1467,2019-7-23,2019,7,23,1,cgf8x2,[N] ICCV 2019 decisions have been posted,https://www.reddit.com/r/MachineLearning/comments/cgf8x2/n_iccv_2019_decisions_have_been_posted/,joshuacpeterson,1563812166,,0,1
1468,2019-7-23,2019,7,23,1,cgfb32,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/cgfb32/data_science_career_track_prep_course/,HannahHumphreys,1563812447,[removed],0,1
1469,2019-7-23,2019,7,23,1,cgfc4g,ICCV2019 Submission results are in!,https://www.reddit.com/r/MachineLearning/comments/cgfc4g/iccv2019_submission_results_are_in/,aDutchofMuch,1563812584,[removed],0,1
1470,2019-7-23,2019,7,23,1,cgfcjo,[N] ICCV 2019 decisions have been posted!,https://www.reddit.com/r/MachineLearning/comments/cgfcjo/n_iccv_2019_decisions_have_been_posted/,joshuacpeterson,1563812638,[removed],0,1
1471,2019-7-23,2019,7,23,1,cgfcm6,"[P] Stochastic processes, infinite models and timeseries analysis in Brancher",https://www.reddit.com/r/MachineLearning/comments/cgfcm6/p_stochastic_processes_infinite_models_and/,LucaAmbrogioni,1563812648,"We are happy to share Brancher .35 with dedicated support for stochastic processes, infinite models and timeseries analysis.

Now you can construct timeseries models with two lines of code:

\###

x0 = Normal(0, 0.1, ""x\_0"")

X = MarkovProcess(x0, lambda t, x: Normal(x, 0.25, ""x\_{}"".format(t)))

\###

Try the new features in our new tutorial: [https://colab.research.google.com/drive/1dqc-5DqJOrNw6CR96CDrarrKQ2KSabDH ](https://t.co/9iT54q4nAo)

&amp;#x200B;

If you like our work and you want to help us grow, please share and star us on Github! :) [https://github.com/AI-DI/Brancher ](https://t.co/h7IsUklnsw)

You can also follow us on twitter for new updates: @pybrancher",0,2
1472,2019-7-23,2019,7,23,1,cgfe2c,What Business Analyst Skills You Need to Master for Fast-Track Career,https://www.reddit.com/r/MachineLearning/comments/cgfe2c/what_business_analyst_skills_you_need_to_master/,SunilAhujaa,1563812848,[removed],0,1
1473,2019-7-23,2019,7,23,2,cgft6p,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/cgft6p/data_science_career_track_prep_course/,HannahHumphreys,1563814808,[removed],0,1
1474,2019-7-23,2019,7,23,2,cggb4l,Natural Adversarial Examples Slash Image Classification Accuracy by 90%,https://www.reddit.com/r/MachineLearning/comments/cggb4l/natural_adversarial_examples_slash_image/,Yuqing7,1563817030,,0,1
1475,2019-7-23,2019,7,23,2,cggdi9,"If you could have one complex data set, what would it be?",https://www.reddit.com/r/MachineLearning/comments/cggdi9/if_you_could_have_one_complex_data_set_what_would/,ovidi_data,1563817327,[removed],0,1
1476,2019-7-23,2019,7,23,3,cggn0y,[Project] Turn-key image annotation service to train ML model,https://www.reddit.com/r/MachineLearning/comments/cggn0y/project_turnkey_image_annotation_service_to_train/,labelimagedata,1563818551,"[https://labelimagedata.com](https://labelimagedata.com)

&amp;#x200B;

https://i.redd.it/ixkn61jp9wb31.png",0,0
1477,2019-7-23,2019,7,23,3,cgguvd,The new PrecisionFDA: Creating Data Challenges to Evolve the Science of Precision Medicine blog is now live on the site!,https://www.reddit.com/r/MachineLearning/comments/cgguvd/the_new_precisionfda_creating_data_challenges_to/,hollystephens723,1563819526,[removed],0,2
1478,2019-7-23,2019,7,23,3,cghb3d,Opposite of Adversarial samples,https://www.reddit.com/r/MachineLearning/comments/cghb3d/opposite_of_adversarial_samples/,nefrpitou,1563821608,[removed],0,1
1479,2019-7-23,2019,7,23,4,cghf6r,"[D] No-nonsense, comprehensive reading list for ML &amp; DS",https://www.reddit.com/r/MachineLearning/comments/cghf6r/d_nononsense_comprehensive_reading_list_for_ml_ds/,ML-reader,1563822141,"Hi all,

&amp;#x200B;

I'm a self-taught ML practitioner, working in the industry. However, I feel that my lack of formal education is hurting me, especially when working with research folks (stats &amp; math heavy topics).

&amp;#x200B;

I'm quite good at self-learning, so I'd like to revisit all the foundations in the next 12 months. I'm looking to have solid, comprehensive grasp of the most important topics in ML, so that I can at least understand conversations around me.

&amp;#x200B;

Would appreciate suggestions on how improve the reading list I cooked up (what to add / remove / replace).

Do you think 12 months is a reasonable timeline for the following?

# Basics

## Calculus

1. [Thomas' Calculus](https://www.amazon.com/Thomas-Calculus-14th-Joel-Hass/dp/0134438981)
2. [The matrix Calculus you need for deep learning](https://arxiv.org/abs/1802.01528)

## Linear Algebra

3. [No bullshit guide to linear algebra](https://www.amazon.com/No-bullshit-guide-linear-algebra/dp/0992001021)

4. [Linear Algebra Done Right](https://www.amazon.com/Linear-Algebra-Right-Undergraduate-Mathematics/dp/3319110799/)

## Probability

5. [Introduction to Probability](https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573)

6. [MIT RES.6-012 Introduction to Probability, Spring 2018](https://www.youtube.com/watch?v=1uW3qMFA9Ho&amp;list=PLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6)

## Information Theory

7. [Information Theory: A Tutorial Introduction](https://arxiv.org/abs/1802.05968)

8. [Information Theory, Inference, and Learning Algorithms](http://www.inference.org.uk/itprnn/book.pdf)

## Statistics

9. [All of Statistics](https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf)

10. [Casella, G. and Berger, R.L. (2001). ""Statistical Inference""](https://www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126)

## Bayesian Statistics

11. [Bayesian Data Analysis](https://www.amazon.com/exec/obidos/ISBN=158488388X/)

## Optimization

12. [Introduction to Linear Optimization](https://www.amazon.com/Introduction-Linear-Optimization-Scientific-Computation/dp/1886529191)

13. [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)

# ML

## Foundations

14. [A Course in Machine Learning](http://ciml.info/)

15. [Machine Learning: a Probabilistic Perspective](https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020)

## Specifics

## Causality

16. [The Book of Why: The New Science of Cause and Effect](https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/046509760X)

17. [Causality: Models, Reasoning and Inference](https://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X)

## DL

18. [Deep Learning](http://www.deeplearningbook.org/)

19. [EPFL Course](https://fleuret.org/ee559/)

## Gaussian Processes

20. [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)

## NLP

21. [Eisenstein's Notes](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)

22. [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)

## Reinforecement Learning

23. [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)

## Graphical Models

24. [Probabilistic Graphical Models](https://mitpress.mit.edu/books/probabilistic-graphical-models)

## Recommender Systems

25. [Recommender Systems](https://www.amazon.com/Recommender-Systems-Textbook-Charu-Aggarwal-ebook/dp/B01DK3GZDY)

## Probabilistic Programming

26. [PPLs](http://www.robots.ox.ac.uk/~twgr/assets/pdf/rainforth2017thesis.pdf)",30,116
1480,2019-7-23,2019,7,23,4,cghhhu,How quickly can AI solve a Rubiks Cube? In less time than it took you to read this headline.,https://www.reddit.com/r/MachineLearning/comments/cghhhu/how_quickly_can_ai_solve_a_rubiks_cube_in_less/,jonfla,1563822433,,0,1
1481,2019-7-23,2019,7,23,4,cghrqh,Is this PC build good for machine learning?,https://www.reddit.com/r/MachineLearning/comments/cghrqh/is_this_pc_build_good_for_machine_learning/,drunkpolishbear,1563823765,[removed],0,1
1482,2019-7-23,2019,7,23,6,cgjidf,Time series forecasting beyond just using test data?,https://www.reddit.com/r/MachineLearning/comments/cgjidf/time_series_forecasting_beyond_just_using_test/,jabrahamson,1563831881,[removed],0,1
1483,2019-7-23,2019,7,23,7,cgjvcx,Are there some formulae in facial recognition that are indicators of close kinship?,https://www.reddit.com/r/MachineLearning/comments/cgjvcx/are_there_some_formulae_in_facial_recognition/,vfclists,1563833581,[removed],0,1
1484,2019-7-23,2019,7,23,8,cgkhr6,A Look at Statistical Pattern Recognition Through the Visions of Vision,https://www.reddit.com/r/MachineLearning/comments/cgkhr6/a_look_at_statistical_pattern_recognition_through/,PartlyShaderly,1563836645,[removed],0,1
1485,2019-7-23,2019,7,23,8,cgkq31,"[R] Clustering with t-SNE, Provably",https://www.reddit.com/r/MachineLearning/comments/cgkq31/r_clustering_with_tsne_provably/,Gillithonnen,1563837850,,0,1
1486,2019-7-23,2019,7,23,8,cgkri6,[D] Keras - Calculating run time precision (and recall) in a multi-label problem?,https://www.reddit.com/r/MachineLearning/comments/cgkri6/d_keras_calculating_run_time_precision_and_recall/,Zman420,1563838047,"Hi all,

I've come across a bit of a problem, and my attempts at coding a solution seem to have been unsuccessful.  I could do this quite easily in numpy, but since the calculations have to be done on the Keras.Backend tensor objects, I can't figure it out.

Sidenote - how do you guys debug code in these backend functions?  You can't print() or step through...


The problem: 

For my models, I'm mostly interested in precision and recall, because those are what directly impact a real world application of the model.  I typically calculate them as such:

    from keras import backend as K
    
    def pos_precision_acc(y_true, y_pred):
   
    interesting_class_id = 1;
    class_id_true = K.argmax(y_true, axis=-1)
    class_id_preds = K.argmax(y_pred, axis=-1)
    # Replace class_id_preds with class_id_true for recall here
    accuracy_mask = K.cast(K.equal(class_id_preds, interesting_class_id), 'int32')
    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask
    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)
    return class_acc



So, if I have a one-hot encoded binary problem with labels as [1 0] for negative and [0 1] for positive, the above code would return the precision for the positive class.

However, in a multi-label problem, my classes are multi-hot-encoded - for example a GT label [1 1 0 0] would indicate that the first two are positive, and the last two are negative.  The pos_precision_acc() func above would fail at this, because the argmax call limits the output to a single result per instance.

I tried modifying the above function in several ways, but I'm not having any luck. argmax() is out for sure, because it only returns the idx of the *first* max value, whereas I expect to have multiple.  I also tried to .equal after .flatten() on the tensors, but that didnt help either.

Anyone familiar with Keras.backend tensor operations willing it help out?  It should actually be pretty easy...I'm just not having much luck.",5,1
1487,2019-7-23,2019,7,23,8,cgkv1u,Automated Deep Learning  So Simple Anyone Can Do It,https://www.reddit.com/r/MachineLearning/comments/cgkv1u/automated_deep_learning_so_simple_anyone_can_do_it/,andrea_manero,1563838560,[removed],0,1
1488,2019-7-23,2019,7,23,8,cgkxvy,Facebooks LAZER,https://www.reddit.com/r/MachineLearning/comments/cgkxvy/facebooks_lazer/,FMWizard,1563838976,[removed],0,1
1489,2019-7-23,2019,7,23,8,cgkze5,"[D] Volunteer opportunities in Machine Learning ? Or, other ways to get professional-ish experience in ML learning?",https://www.reddit.com/r/MachineLearning/comments/cgkze5/d_volunteer_opportunities_in_machine_learning_or/,AdditionalWay,1563839209,"I am semi-self taught in machine learning (my degree covered most of the math) and have done a few projects with ML. I think the next step to prove myself to employers would be to work in an enviroment which is a little more professional, and more likely to emulate the ML enviroment at a company. 

Since I am not in school anymore, I do not qualify for most internships. 

Another thought would be to find some very early stage startups, and offer to work for really cheap, I would be willing to work for minimum wage for a while.",6,7
1490,2019-7-23,2019,7,23,9,cgl5cr,ML/AI algorithms benchmarks for New machine,https://www.reddit.com/r/MachineLearning/comments/cgl5cr/mlai_algorithms_benchmarks_for_new_machine/,anshu0957,1563840079,[removed],0,1
1491,2019-7-23,2019,7,23,9,cgl7ap,[D] What happened to that guy who said he'd release his own home-trained GPT-2 1.5B model on July 1st?,https://www.reddit.com/r/MachineLearning/comments/cgl7ap/d_what_happened_to_that_guy_who_said_hed_release/,zergling103,1563840393,"I remember him making a huge announcement about it, but I can't find the post in this subreddit. It seems to have been removed.

If anyone has any information about this, it'd be great to know. Thanks!",19,4
1492,2019-7-23,2019,7,23,10,cgma7w,Can I become an ML engineer with a stat degree?,https://www.reddit.com/r/MachineLearning/comments/cgma7w/can_i_become_an_ml_engineer_with_a_stat_degree/,dirtymikethelegend,1563846461,[removed],0,1
1493,2019-7-23,2019,7,23,11,cgmptl,[D] What is OpenAI? I don't know anymore.,https://www.reddit.com/r/MachineLearning/comments/cgmptl/d_what_is_openai_i_dont_know_anymore/,milaworld,1563848948,"*Some [commentary](https://threadreaderapp.com/thread/1153364705777311745.html) from [Smerity](https://twitter.com/Smerity/status/1153364705777311745) about yesterday's [cash infusion](https://openai.com/blog/microsoft/) from MS into OpenAI:*

What is OpenAI? I don't know anymore.
A non-profit that leveraged good will whilst silently giving out equity for [years](https://twitter.com/gdb/status/1105137541970243584) prepping a shift to for-profit that is now seeking to license closed tech through a third party by segmenting tech under a banner of [pre](https://twitter.com/tsimonite/status/1153340994986766336)/post ""AGI"" technology?

The non-profit/for-profit/investor [partnership](https://openai.com/blog/openai-lp/) is held together by a set of legal documents that are entirely novel (=bad term in legal docs), are [non-public](https://twitter.com/gdb/status/1153305526026956800) + unclear, have no case precedence, yet promise to wed operation to a vague (and already re-interpreted) [OpenAI Charter](https://openai.com/charter/).

The claim is that [AGI](https://twitter.com/woj_zaremba/status/1105149945118519296) needs to be carefully and collaboratively guided into existence yet the output of almost [every](https://github.com/facebookresearch) [other](https://github.com/google-research/google-research) [existing](https://github.com/salesforce) [commercial](https://github.com/NVlabs) lab is more open. OpenAI runs a closed ecosystem where they primarily don't or won't trust outside of a small bubble.

I say this knowing many of the people there and with past and present love in my heart--I don't collaborate with OpenAI as I have no freaking clue what they're doing. Their primary form of communication is high entropy blog posts that'd be shock pivots for any normal start-up.

Many of their [blog posts](https://openai.com/blog/cooperation-on-safety/) and [spoken](https://www.youtube.com/watch?v=BJi6N4tDupk) [positions](https://www.youtube.com/watch?v=9EN_HoEk3KY) end up [influencing government policy](https://twitter.com/jackclarkSF/status/986568940028616705) and public opinion on the future of AI through amplified pseudo-credibility due to *Open*, *Musk founded*, repeatedly hyped statements, and a sheen from their now distant non-profit good will era.

I have mentioned this to friends there and say all of this with positive sum intentions: I understand they have lofty aims, I understand they need cash to shovel into the forever unfurling GPU forge, but if they want any community trust long term they need a better strategy.

The implicit OpenAI message heard over the years:
""Think of how transformative and dangerous AGI may be. Terrifying. Trust us. Whether it's black-boxing technology, legal risk, policy initiatives, investor risk, ... - trust us with everything. We're good. No questions, sorry.""

We'll clarify our position in an upcoming blog post.",171,481
1494,2019-7-23,2019,7,23,12,cgninb,[R] The generative adversarial brain,https://www.reddit.com/r/MachineLearning/comments/cgninb/r_the_generative_adversarial_brain/,downtownslim,1563853721,"The idea that the brain learns generative models of the world has been widely promulgated. Most approaches have assumed that the brain learns an explicit density model that assigns a probability to each possible state of the world. However, explicit density models are difficult to learn, requiring approximate inference techniques that may find poor solutions. An alternative approach is to learn an implicit density model that can sample from the generative model without evaluating the probabilities of those samples. The implicit model can be trained to fool a discriminator into believing that the samples are real. This is the idea behind generative adversarial algorithms, which have proven adept at learning realistic generative models. **This paper develops an adversarial framework for probabilistic computation in the brain.** It first considers how generative adversarial algorithms overcome some of the problems that vex prior theories based on explicit density models. It then discusses the psychological and neural evidence for this framework, as well as how the breakdown of the generator and discriminator could lead to delusions observed in some mental disorders.

&amp;#x200B;

PDF: [http://gershmanlab.webfactional.com/pubs/GenerativeAdversarialBrain.pdf](http://gershmanlab.webfactional.com/pubs/GenerativeAdversarialBrain.pdf)",2,3
1495,2019-7-23,2019,7,23,13,cgns3y,[P] Card Similarities in Clash Royal (should apply to any lineup-building games),https://www.reddit.com/r/MachineLearning/comments/cgns3y/p_card_similarities_in_clash_royal_should_apply/,evanthebouncy,1563855336,,1,0
1496,2019-7-23,2019,7,23,13,cgnub9,[D] What's with Linguistic Data Consortium paywall for.. everything?,https://www.reddit.com/r/MachineLearning/comments/cgnub9/d_whats_with_linguistic_data_consortium_paywall/,MrBojanglesReturns,1563855710,"What's up with LDC pay walling every corpus up to 8,000$? Look at the surreal price of how a year subscription is worth. It is out of anybody's league. Are there any open alternatives to speech recognition?

&amp;#x200B;

We have a lot of open-sourced datasets for other domains, computer vision, image processing, medical scans. Only audio datasets are being kept behind a paywall. According to another Reddit post, they had plenty of public funding, and yet nothing changed.

&amp;#x200B;

They have old stuff as of 1992 and nothing is released for free. Is it common for US unis to have access to this kind of material? They sound more like a corporation than a consortium. Not having an updated and big corpora hinders the whole field from discoveries, because each time there is a research in the domain, there will be time wasted building another dataset.",3,7
1497,2019-7-23,2019,7,23,14,cgo6tx,Learn Tensorflow,https://www.reddit.com/r/MachineLearning/comments/cgo6tx/learn_tensorflow/,ThreeForElvenKings,1563858006,[removed],0,1
1498,2019-7-23,2019,7,23,14,cgof4a,How could I go from a A+ certification to Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/cgof4a/how_could_i_go_from_a_a_certification_to_machine/,Shill_master,1563859518,[removed],0,1
1499,2019-7-23,2019,7,23,14,cgofa4,[R] Convolutional Reservoir Computing for World Models,https://www.reddit.com/r/MachineLearning/comments/cgofa4/r_convolutional_reservoir_computing_for_world/,wei_jok,1563859544,,11,14
1500,2019-7-23,2019,7,23,14,cgogy8,Activation function for nnetar() function,https://www.reddit.com/r/MachineLearning/comments/cgogy8/activation_function_for_nnetar_function/,taufique_1929,1563859868,[removed],0,1
1501,2019-7-23,2019,7,23,14,cgoh91,[R] Surfing: An Interesting Iterative Optimization Algorithm Based on Functionals,https://www.reddit.com/r/MachineLearning/comments/cgoh91/r_surfing_an_interesting_iterative_optimization/,darkconfidantislife,1563859925," [http://arxiv.org/pdf/1907.08653v1](http://arxiv.org/pdf/1907.08653v1)  (pdf warning)

&amp;#x200B;

Very interesting idea, but unfortunately only tests on Fashion MNIST",1,1
1502,2019-7-23,2019,7,23,14,cgol6m,GLUE is not all you need : Discourse-Based Evaluation of Language Understanding,https://www.reddit.com/r/MachineLearning/comments/cgol6m/glue_is_not_all_you_need_discoursebased/,Jean-Porte,1563860676,,1,1
1503,2019-7-23,2019,7,23,14,cgole1,[R] GLUE Is Not All You Need: Discourse Based Evaluation of Language Understanding,https://www.reddit.com/r/MachineLearning/comments/cgole1/r_glue_is_not_all_you_need_discourse_based/,Jean-Porte,1563860719,,2,5
1504,2019-7-23,2019,7,23,15,cgoz5y,Why would Linux be better than Windows at Machine Learning Tasks?,https://www.reddit.com/r/MachineLearning/comments/cgoz5y/why_would_linux_be_better_than_windows_at_machine/,Nick-Conner,1563863380,[removed],0,1
1505,2019-7-23,2019,7,23,16,cgp9o3,[D] ML/CV publication venues are moving up in the Google Scholar h5-index rankings,https://www.reddit.com/r/MachineLearning/comments/cgp9o3/d_mlcv_publication_venues_are_moving_up_in_the/,sensetime,1563865493,"Saw this link on Yann Lecun's facebook to [Google Scholar Top Publications](https://scholar.google.com/citations?view_op=top_venues) rankings.

ML/CV publication venues are moving up in the h5-index ranking:

#9: CVPR (240) ahead of PNAS, PRL, JAMA.
#27: NeurIPS (169)
#42: ICLR (150) ahead of Neuron and Nature Neuroscience
#56: ECCV (137)
#59: ICML (135)
#71: ICCV (129)

It is interesting that ICLR is higher ranked now compared to ICML. Also the other AI conferences with low acceptance rates, such as AAAI and IJCAI are not on this list. Top NLP conferences like ACL or EMNLP are also not on the list although IMO this might change in the next few years given the progress (or perceived-progress) made from 2018.

https://scholar.google.com/citations?view_op=top_venues",4,8
1506,2019-7-23,2019,7,23,16,cgpbox,"MACHINE LEARNING TRAINING IN JAYANAGAR,BANGALORE",https://www.reddit.com/r/MachineLearning/comments/cgpbox/machine_learning_training_in_jayanagarbangalore/,livewireindia,1563865914,,0,1
1507,2019-7-23,2019,7,23,16,cgpilo,What is Machine Learning Its Benefits and Applications,https://www.reddit.com/r/MachineLearning/comments/cgpilo/what_is_machine_learning_its_benefits_and/,MtBlogger,1563867342,,0,1
1508,2019-7-23,2019,7,23,17,cgpvr8,Top 50 Machine Learning Interview Questions - javatpoint,https://www.reddit.com/r/MachineLearning/comments/cgpvr8/top_50_machine_learning_interview_questions/,nehapandey01,1563870291,,0,1
1509,2019-7-23,2019,7,23,17,cgpx7b,"Enriching data variety in homogenous dataset, any tips?",https://www.reddit.com/r/MachineLearning/comments/cgpx7b/enriching_data_variety_in_homogenous_dataset_any/,nottodaymrdick,1563870636,[removed],0,1
1510,2019-7-23,2019,7,23,17,cgpxoh,CRNN article published at Towards Data Science.,https://www.reddit.com/r/MachineLearning/comments/cgpxoh/crnn_article_published_at_towards_data_science/,ccchatterjee,1563870759,[removed],0,1
1511,2019-7-23,2019,7,23,17,cgpydh,I am interested in the research area of AI + X. What are some good research area that incorporates AI?,https://www.reddit.com/r/MachineLearning/comments/cgpydh/i_am_interested_in_the_research_area_of_ai_x_what/,ivicts,1563870922,[removed],0,1
1512,2019-7-23,2019,7,23,17,cgpzju,"[D] Neural computers, quantum computers, and the future of 0s and 1s.",https://www.reddit.com/r/MachineLearning/comments/cgpzju/d_neural_computers_quantum_computers_and_the/,dantehorrorshow,1563871181,,0,1
1513,2019-7-23,2019,7,23,17,cgpzuo,[D] Minimum cost is not zero when calculating cross-entropy on soft labels,https://www.reddit.com/r/MachineLearning/comments/cgpzuo/d_minimum_cost_is_not_zero_when_calculating/,vratiner,1563871252," I am training a neural network using batches of soft labels, e.g.

    y = [[0.00, 0.25, 0.25, 0.50],              
                  ...      
         [0.75, 0.00, 0.20, 0.05]] 

However, as opposed to one-hot labels, if the softmax activation function outputs a list  equal to y (no loss), as in

    y =  = [0.00, 0.25, 0.25, 0.50] 

the cross-entropy function is not 0:

    loss = -sum(y * log()) = 1.0397 

although it is true that with no other  we can reach a lower value, given y.

Then, the more sparse y is, the larger is the minimum possible loss:

    y =  = [0.25, 0.25, 0.25, 0.25] loss = -sum(y * log()) = 1.3862 

So my question is, would this lower bound in the minimum possible loss constitute a bias when training/testing a neural network? Since a neural network yields a higher minimum cost for more sparse soft labels than for less sparse (up to one-hot) labels, maybe the network adjusts the weights and biases towards a way to minimize the more sparse soft  labels, in detriment of the less sparse soft and one-hot labels?",5,6
1514,2019-7-23,2019,7,23,18,cgq5l5,[D] Where have Transformers been applied other than NLP ?,https://www.reddit.com/r/MachineLearning/comments/cgq5l5/d_where_have_transformers_been_applied_other_than/,gohu_cd,1563872547,"I'm looking to find resources on applications of Transformers to anything else than NLP.

&amp;#x200B;

I'm especially interested in finding papers that apply Transformers to State Representation Learning and Reinforcement Learning.",17,15
1515,2019-7-23,2019,7,23,18,cgq81j,"Front load component cleaning machine - We are Leading Manufacturer of Special purpose Industrial Component Cleaning Machine, Multistage Component Washing Machine according to Customer requirements Details Visit http://www.hydrojet.co.in",https://www.reddit.com/r/MachineLearning/comments/cgq81j/front_load_component_cleaning_machine_we_are/,Ultramaxhydrojet,1563873056,,0,1
1516,2019-7-23,2019,7,23,18,cgqe5k,Final Project Problem (help please),https://www.reddit.com/r/MachineLearning/comments/cgqe5k/final_project_problem_help_please/,hendraahalimm,1563874317,[removed],1,1
1517,2019-7-23,2019,7,23,19,cgqoqb,"Vision and Language review : ""Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods""",https://www.reddit.com/r/MachineLearning/comments/cgqoqb/vision_and_language_review_trends_in_integration/,kmario23,1563876426,"Past few years has seen an exploding interest in the area of vision-and-language. To understand this field better in terms of past research and also to offer some future research directions, we have reviewed around 400 research papers (conferences &amp; journal) and propose 10 unique tasks in a paper titled ""**Trends in Integration of Vision and Language Research: A Survey of  Tasks, Datasets, and Methods**"" which is available on arXiv now at [http://arxiv.org/abs/1907.09358](http://arxiv.org/abs/1907.09358)",0,1
1518,2019-7-23,2019,7,23,19,cgqpj3,Having trouble understanding a paper about dynamic word embeddings,https://www.reddit.com/r/MachineLearning/comments/cgqpj3/having_trouble_understanding_a_paper_about/,goggesque,1563876572,[removed],0,1
1519,2019-7-23,2019,7,23,19,cgqv9f,[D] Why Computer Vision still sucks?,https://www.reddit.com/r/MachineLearning/comments/cgqv9f/d_why_computer_vision_still_sucks/,msamoylov,1563877698,"I have a pet project built with the Computer Vision service from Microsoft. Sometimes it provides very accurate annotations and descriptions like ['A view of a snow covered mountain'](https://taken.photos/photos/3738716/) (confidence 0.97) for an image of a mountain but mostly it's utter garbage like ['A motorcycle is parked on the side of a road'](https://taken.photos/photos/3738489/) (confidence 0.8) for a Formula 1 car.

&amp;#x200B;

The Vision AI service from Google is doing even worse.

&amp;#x200B;

I'm not seeing any significant improvements in this field at all. You can get a very realistic image of older you, but no one is able to annotate even a simple photo yet.

&amp;#x200B;

Do you think we will have truly working Computer Vision within next few years?",16,0
1520,2019-7-23,2019,7,23,19,cgqwpo,"how is it possible, there is no machine learning ECG machine to analyze Heart Health",https://www.reddit.com/r/MachineLearning/comments/cgqwpo/how_is_it_possible_there_is_no_machine_learning/,martin80k,1563877974,[removed],0,1
1521,2019-7-23,2019,7,23,19,cgqydd,Publishing on special issue of Sensors (MDPI),https://www.reddit.com/r/MachineLearning/comments/cgqydd/publishing_on_special_issue_of_sensors_mdpi/,ale152,1563878288,[removed],0,1
1522,2019-7-23,2019,7,23,19,cgqz3f,[Discussion] Publishing on special issue of Sensors (MDPI),https://www.reddit.com/r/MachineLearning/comments/cgqz3f/discussion_publishing_on_special_issue_of_sensors/,ale152,1563878426,"Hello,

I was planning of publishing a paper on a special issue of Sensors (MDPI) that seems to be strongly related to my work. After having a better look I've found [hundreds](https://www.mdpi.com/journal/sensors/special_issues) of other special issues, all with similar deadlines and on very similar topics. Also, this journal seems to have a very rapid publication process, with ""*first decision provided to authors approximately 19.8 days after submission*"". I've read some [pretty bad things](https://www.researchgate.net/post/Im_gonna_ask_whether_publishing_in_MDPI_journals_is_good_or_more_specifically_how_is_publishing_in_International_Journal_of_Molecular_Sciences) in the past about MDPI, but I always thought that Sensors was a serious journal, and I often find interesting research on it. So now I'm really confused. What do you think about MDPI Sensors? Is it worth publishing there, or would it be a waste?

Thank you.",1,1
1523,2019-7-23,2019,7,23,19,cgr434,[P] ProSper: A Python Library for Probabilistic Sparse Coding with Non-Standard Priors and Superpositions,https://www.reddit.com/r/MachineLearning/comments/cgr434/p_prosper_a_python_library_for_probabilistic/,gexarcha,1563879419,We recently released the source code of a number of Sparse Coding algorithms under a common Python module called [ProSper](https://github.com/ml-uol/prosper).,5,5
1524,2019-7-23,2019,7,23,20,cgr80l,Web Scraping Tutorial,https://www.reddit.com/r/MachineLearning/comments/cgr80l/web_scraping_tutorial/,nveen93,1563880149,[removed],0,1
1525,2019-7-23,2019,7,23,20,cgrboz,Image classification with TensorFlow Lite on Android,https://www.reddit.com/r/MachineLearning/comments/cgrboz/image_classification_with_tensorflow_lite_on/,BrightDevs,1563880827,,0,1
1526,2019-7-23,2019,7,23,20,cgrlv3,Real-time Machine Learning: Hype vs Reality,https://www.reddit.com/r/MachineLearning/comments/cgrlv3/realtime_machine_learning_hype_vs_reality/,tzimmzzzz,1563882687,[removed],0,1
1527,2019-7-23,2019,7,23,20,cgrnm0,Final Project Problem (Help please),https://www.reddit.com/r/MachineLearning/comments/cgrnm0/final_project_problem_help_please/,hendraahalimm,1563882989,"Hello, I'm Hendra. I'm an undergraduate students who is working on final project so I can graduate. My lecturer asked me to use ANN for energy disaggregation system. I'm using keras machine learning with tensor flow as the backend. Lately I have a problem on making a correct model for my case and got bad accuracy score. I have 4 lamps with various power consumption and I want to predict the power consumption of each lamp using ANN. My dataset is on link. On the dataset, 1 means ON and 0 means OFF. I'm using jupyter notebook as its interface. Can anyone give me some idea about what models I can use for this type of problem? I'm currently stumped. Any kind of help is really appreciated. Thank you.....
Here is the dataset : https://drive.google.com/file/d/1ZrW083twfEIHZzqGuaDYA7jjO2s5P44p/view?usp=sharing",0,1
1528,2019-7-23,2019,7,23,20,cgropn,just say Hello,https://www.reddit.com/r/MachineLearning/comments/cgropn/just_say_hello/,elsanitasinaga,1563883193,[removed],0,1
1529,2019-7-23,2019,7,23,21,cgrtvo,Leveraging Web Development with Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/cgrtvo/leveraging_web_development_with_artificial/,praveenscience,1563884015,,0,1
1530,2019-7-23,2019,7,23,21,cgs4fm,Tuesday tricks tutorial s catch into moon wave,https://www.reddit.com/r/MachineLearning/comments/cgs4fm/tuesday_tricks_tutorial_s_catch_into_moon_wave/,thetrickshotone,1563885751,,0,1
1531,2019-7-23,2019,7,23,21,cgs90t,Guidance for object detection,https://www.reddit.com/r/MachineLearning/comments/cgs90t/guidance_for_object_detection/,onedarksol,1563886509,[removed],0,1
1532,2019-7-23,2019,7,23,22,cgshi4,[R] A Neural-Symbolic Architecture for Inverse Graphics Improved by Lifelong Meta-Learning,https://www.reddit.com/r/MachineLearning/comments/cgshi4/r_a_neuralsymbolic_architecture_for_inverse/,kayzaks,1563887829,,7,16
1533,2019-7-23,2019,7,23,22,cgsl9k,Post ICCV-19 Reviews --Improving Mean Absolute Error against CCE,https://www.reddit.com/r/MachineLearning/comments/cgsl9k/post_iccv19_reviews_improving_mean_absolute_error/,XinshaoWang,1563888390,[removed],0,1
1534,2019-7-23,2019,7,23,22,cgspin,[R] Post ICCV-19 Reviews --Improving Mean Absolute Error against CCE,https://www.reddit.com/r/MachineLearning/comments/cgspin/r_post_iccv19_reviews_improving_mean_absolute/,XinshaoWang,1563889030,"[https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE)

# Improving MAEs Fitting Ability: Fundamental and Thorough Analysis with A Simple Solution

Project page for [Improving Mean Absolute Error against CCE](https://arxiv.org/pdf/1903.12141.pdf).

**Since this paper is released, for your better reference, the ICCV-19 reviews results are released following the practice of OpenReview**

* [Reviews](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE/blob/master/ICCV19_FinalReviewsRejected/Conference%20Management%20Toolkit%20-%20View%20review.pdf)
* [RebuttalToReviewers](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE/blob/master/ICCV19_FinalReviewsRejected/IMAE_rebuttals_V07.pdf) &amp; [RebuttalToAC](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE/blob/master/ICCV19_FinalReviewsRejected/Conference%20Management%20Toolkit%20-%20View%20Author%20Feedback.pdf)
* [Meta-Reviews](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE/blob/master/ICCV19_FinalReviewsRejected/Conference%20Management%20Toolkit%20-%20View%20meta-review.pdf)
* [More Discussions](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE#discussion)

**A Open Question on whether clean or noisy validation set for ML/DL researchers caring about label noise**

* [Reviewer#3's opinion in final justification](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE/blob/master/ICCV19_FinalReviewsRejected/Conference%20Management%20Toolkit%20-%20View%20review.pdf): \`The validation sets are required to be clean, which greatly decrease the contribution. **Many existing methods employ noisy validation set to choose hyper-parameters**, e.g., when the risk is consistent. **As minimizing risks on the noisy validation set is asymptotically equal to minimizing risk on the clean data**.'
* [My opinion discussed with my collaborators](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE/blob/master/ICCV19_FinalReviewsRejected/IMAE_rebuttals_V07.pdf): Following the ML literature, **a validation set should be clean** as **we should not expect a ML model to predict noisy data well**. In other words, **we cannot evaluate/decide a models performance on noisy validation/test data**. Our goal is to avoid learning faults from noisy data and generalise better during inference.

**Positive comments we collected**

* The proposed modification IMAE is **quite simple and should be considerably more efficient than other methods that deal with label noise**.
* The theoretical analysis of CCE and MAE is thorough and provides an explanation of the tendency of CCE to overfit to incorrect labels and the underfitting of MAE to the correct labels.
* The experiments show a significant improvement over CCE in the case of noisy labels which validates the approach. I also appreciate the experiment on MARS with realistic label noise. I appreciate the comparison on Clothing-1M provided by the authors. The results there suggest that under realistic label noise the method actually works well when compared to SotA methods.

**What next?**

* We will improve our work based on the ICCV-19 reviews, e.g., adding more experiments.

## Introduction

**Research questions:**

* Why does MAE work much worse than CCE although it is noise-robust?
* How to improve MAE against CCE to embrace noise-robustness and high generalisation performance?

Our work is a further study of robust losses following MAE \[1\] and GCE \[2\]. They proved MAE is more robust than CCE when noise exists. However, MAEs underfitting phenomenon is not exposed and studied in the literature. We analysed it thoroughly and proposed a simple solution to embrace both high fitting ability (accurate) and test stability (robust).

**Our main purpose is not a proposal to push current best performance under label noise.** Instead, we focus on analysing how different losses perform differently and why, which is a fundamental research question.

Our focus is to analyse why CCE overfits while MAE underfits as presented in ablation studies in Table 2. Under unknown real-world noise in Table 3, we only compared with GCE \[2\] as it is the most related and demonstrated to be the state-of-the-art.

**IMAE is suitable for cases where inputs and labels may be unmatched.**

Training DNNs requires rethinking data fitting and generalisation. **Our main contribution is simple analysis and solution from the viewpoint of gradient magnitude with respect to logits.**

## Takeaways

 

* By CCE is noise-sensitive and overfits training data, we mean CCE owns high data fitting accuracy but its final test accuracy drops a lot versus its best test accuracy.
* By MAE is robust, we mean MAEs final test accuracy drops only a bit versus its best test one.
* By MAE underfits training data, we mean its training and best test accuracies are low.

Please see our empirical evidences which can be observed in Table 2 and Figures 3, 4.

**MAEs fitting ability is much worse than CCE. In other words, CCE overfits to incorrect labels while MAE underfits to correct labels.**

* **The robustness/sensitive to noise is from the angle of test accuracy stability/trend**, i.e., CCEs final test accuracy drops a lot versus its best one while MAEs final one is almost the same as its best one;
* **The claim MAE works worse than CCE is from the aspect of best test accuracy** since we generally apply early stopping to help CCE.

## Results

**Label noise is one of the most explicit cases where some observations and their labels are not matched in the training data. In this case, it is quite crucial to make your models learn meaningful patterns instead of errors.**

## Synthetic noise







## Real-world unknown noise

**Video Person Re-identification/Retrieval on MARS \[4\]**



**Classification on Clothing 1M \[a\] is here**



## Hyper-paramter Analysis







## Discussion

## 1. The idea of this paper is quite close to ""training deep neural-networks using a noise adaptation layer""? They both intend to change the weight of each sample before sending to softmax, definitely they do in different ways. It decreases the novelty of this paper?

Their critical differences are: 1) Noise Adaption explicitly estimates latent true labels by an additional softmax layer while our IMAE reweights examples based on their input-to-label relevance scores; 2) IMAE reweights samples **after softmax**, i.e., scaling their gradients as shown in Eq. (22) in our paper.

## 2. Why uniform noise (symmetric/class-independent noise )?

We choose uniform noise because it is more challenging than asymmetric (class-dependent) noise which was verified in \[d\] Vahdat et al. Toward robustness against label noise in training deep discriminative neural networks. In NeurIPS, 2017.

## 3. Why is the performance still okay when noise rate is 80%?

By adding uniform noise, **even up to 80%, the correct portion is still the majority**, since the 80% are relocated to other 9 classes evenly.

Being natural and intuitive, the majority voting decides the meaningful data patterns to learn. We believe that if the noise accounts the majority, DNNs is hard to learn meaningful patterns. Therefore, **the majority voting is our reasonable assumption.**

## 4. The study from the gradient perspective is not new, e.g., Truncated Cauchy Non-Negative Matrix Factorization, ang GCE [2].

Yes, we agree the perspective itself is not new. However, we find how we analyse fundamentally and go to the simple solution via the gradient viewpoint is novel.

Truncated Cauchy Non-Negative Matrix Factorization (TPAMI-2017) and GCE \[2\] truncate large errors to filter out extreme outliers. Instead, our IMAE adjusts weighting variance without dropping any samples.

## 5. The robustness is not specific for label noise. I think the method works well for general noise, e.g., outliers.

Yes, that is a great point. Our IMAE is suitable for all cases where inputs and their labels are not semantically matched, which may come from noisy data or labels. Since we only evaluated on label noise, we did not exaggerate its efficacy.

We will test more cases in the future.

## 6. Is the validation data clean or not? If clean, this would greatly reduce the contribution of the paper.

Following the ML literature, a validation set should be clean as we should not expect a ML model to predict noisy data well. In other words, we cannot evaluate a models performance on noisy validation/test data. Our goal is to avoid learning faults from noisy data and generalise better during inference.

## 7. More experiments with comparison to prior work and more evaluation on real-world datasets with unknown noise?

Our focus is to analyse why CCE overfits while MAE underfits as presented in ablation studies in Table 2. Under unknown real-world noise in Table 3, we only compared with GCE \[2\] as it is the most related and demonstrated to be the state-of-the-art.

**Classification on Clothing 1M \[a\] is here**



## Citation

Please kindly cite us if you find our work useful.

    @article{wang2019improving,   title={Improving {MAE} against {CCE} under Label Noise},   author={Wang, Xinshao and Kodirov, Elyor and Hua, Yang and Robertson, Neil M},   journal={arXiv preprint arXiv:1903.12141},   year={2019} } 

## References

\[1\] A. Ghosh, H. Kumar, and P. Sastry. Robust loss functions under label noise for deep neural networks. In AAAI, 2017.

\[2\] Z. Zhang and M. R. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In NeurIPS 2018.

\[3\] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.

\[4\] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian. Mars: A video benchmark for large-scale person re-identification. In ECCV, 2016.

\[a\] Xiao et al. Learning From Massive Noisy Labeled Data for Image Classification. In CVPR, 2015.

\[b\] Patrini et al. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.

\[c\] Goldberger et al. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.

\[d\] Vahdat et al. Toward robustness against label noise in training deep discriminative neural networks. In NeurIPS, 2017.

\[e\] Tanaka et al. Joint optimization framework for learning with noisy labels. In CVPR, 2018.

\[f\] Han et al. Masking: A new perspective of noisy supervision. In NeurIPS, 2018.

\[g\] Jenni et al. Deep bilevel learning. In ECCV, 2018.",1,0
1535,2019-7-23,2019,7,23,23,cgt05b,[P] Tweet about your AI/ML Research on @HumansofML!,https://www.reddit.com/r/MachineLearning/comments/cgt05b/p_tweet_about_your_aiml_research_on_humansofml/,everydAI,1563890612,"Do you work in machine learning/artificial intelligence? Do you want to share your work with the public? Sign up to curate @HumansofML, a Twitter account that showcases the people behind the algorithms on a weekly basis! 

I created this account in the hopes that highlighting the people behind algorithm development might increase public  literacy surrounding artificial intelligence, or at least start conversations about what people are interested in. 

The signup form is here: [https://forms.gle/Mk4vhgAkn91q3eoG8](https://forms.gle/Mk4vhgAkn91q3eoG8)

The Twitter account can be found at [http://twitter.com/humansofml](http://twitter.com/humansofml)",3,0
1536,2019-7-23,2019,7,23,23,cgtdio,[D] Good approach for generating treatment plans,https://www.reddit.com/r/MachineLearning/comments/cgtdio/d_good_approach_for_generating_treatment_plans/,MemeBox,1563892476,"I am keeping a record of my mood and what I eat. Also any supplements I take. What I would like to do is generate a sequence of things to eat, supplements to take to try to maximise mood. 

I was thinking I could treat it as a sequence of tokens #cereal #lavender #5htp #mcdonalds #dietcoke #sleep8hours #mood-6. Then generate new sequences where you have high mood values. Or I could do auto-correlation between mood and the controllable variables, then find the best correlations. Or I could cast it as a realtime learning problem. The agent says what I should do during the next day to try to maximise mood.

Can you guys point me in the direction of some good techniques? I'm a software dev and have some experience with tensorflow...",1,2
1537,2019-7-23,2019,7,23,23,cgte6m,How to best learn AWS for ML applications?,https://www.reddit.com/r/MachineLearning/comments/cgte6m/how_to_best_learn_aws_for_ml_applications/,rev_daydreamr,1563892567,[removed],0,1
1538,2019-7-23,2019,7,23,23,cgtl46,How did you learn Deep Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/cgtl46/how_did_you_learn_deep_reinforcement_learning/,rpicatoste_,1563893526,[removed],0,2
1539,2019-7-23,2019,7,23,23,cgtl8d,Trouble Understanding GANs,https://www.reddit.com/r/MachineLearning/comments/cgtl8d/trouble_understanding_gans/,redreaper99,1563893541,,0,1
1540,2019-7-24,2019,7,24,0,cgtx0p,[D] Is a paper detailing results of a novel application publishable?,https://www.reddit.com/r/MachineLearning/comments/cgtx0p/d_is_a_paper_detailing_results_of_a_novel/,searchingundergrad,1563895079,"I've been working on a project relating to learned document embeddings applied in a novel setting for an internship. Would it be worthwhile to write up the results found (which currently look promising) and submit to conferences? 

Most papers seem to focus on novel architectures/methods, is there value in papers concerning novel applications? Are there any conferences that are friendlier towards novel applications? Thanks!",10,3
1541,2019-7-24,2019,7,24,0,cgu5m8,Putting together a machine learning curriculum for Coworkers,https://www.reddit.com/r/MachineLearning/comments/cgu5m8/putting_together_a_machine_learning_curriculum/,thebdup,1563896199,[removed],0,1
1542,2019-7-24,2019,7,24,0,cgu646,[D] Is the Parallel Distributed Processing book still relevant today?,https://www.reddit.com/r/MachineLearning/comments/cgu646/d_is_the_parallel_distributed_processing_book/,TryingToGeek,1563896264,"I was searching for any insightful blog/paper about distributed representation and stumbled upon Chapter 3, titled 'Distributed Representation', in the [PDP](https://web.stanford.edu/~jlmcc/papers/PDP/Volume%201/) book, and it was quite insightful for me; one of its authors is **Geoffrey Hinton** after all. There are other cool chapters that I want to read, but I don't know whether I should invest time in reading a book from the 80's, so I came here for advice; should I read it in 2019, or there is a better alternative?",2,5
1543,2019-7-24,2019,7,24,0,cgu6eo,Help on how to extract specific text from image with printed and handwritten text,https://www.reddit.com/r/MachineLearning/comments/cgu6eo/help_on_how_to_extract_specific_text_from_image/,Atralb,1563896304,[removed],0,1
1544,2019-7-24,2019,7,24,0,cgue2d,[N] Natural Adversarial Examples Slash Image Classification Accuracy by 90%,https://www.reddit.com/r/MachineLearning/comments/cgue2d/n_natural_adversarial_examples_slash_image/,RelativeAnalyst9,1563897305,"Researchers from UC Berkeley and the Universities of Washington and Chicago have released a set of natural adversarial examples, which they call ImageNet-A. The images are described as real-world, naturally occurring examples that have the potential to highly degrade the performance of an image classifier. For example DenseNet-121 obtains only around two percent accuracy on the new ImageNet-A test set, a drop of approximately 90 percent.

The ImageNet challenge competition was closed in 2017, as it was generally agreed in the machine learning community that the task of image classification was mostly solved and that further improvements were not a priority. It should be noted however that the ImageNet test examples are mostly relatively uncluttered close-up images which do not represent the more challenging object contexts and representations found in real world.

Whats more, it has been shown that adversarial examples that succeed in fooling one classification model can also fool other models that use different architecture or were trained on different datasets. Adversarial attacks therefore have the potential to cause serious and widespread security vulnerabilities across popular AI applications such as facial recognition, self-driving cars, etc.

&amp;#x200B;

More in article:

[https://medium.com/syncedreview/natural-adversarial-examples-slash-image-classification-accuracy-by-90-702f381acbb8](https://medium.com/syncedreview/natural-adversarial-examples-slash-image-classification-accuracy-by-90-702f381acbb8)",19,65
1545,2019-7-24,2019,7,24,1,cguj5g,[R] New neural-network rain forecasting based on satellite images,https://www.reddit.com/r/MachineLearning/comments/cguj5g/r_new_neuralnetwork_rain_forecasting_based_on/,vlivashkin,1563897950,"Hi! We significantly improved the current state of the art quality of **nowcasting** (rain detection and prediction up to 2 hours) **based on satellite images** and run it as a service.

We use meteoradar data, satellite images, and weather predictions to feed an unet-based neural network. For now, our service covers almost all Eurasia continent. The whole world will be covered soon!

**Video**: [https://youtu.be/9zd3VR-prYU](https://youtu.be/9zd3VR-prYU)

**Paper**: [https://arxiv.org/abs/1905.09932](https://arxiv.org/abs/1905.09932)

**Service:** [https://yandex.com/weather/nowcasting](https://yandex.com/weather/nowcasting)",19,173
1546,2019-7-24,2019,7,24,1,cguvb1,"Self-supervised Learning for Video Correspondence Flow, State-of-the-art for Video Segmentation and Pose Tracking.",https://www.reddit.com/r/MachineLearning/comments/cguvb1/selfsupervised_learning_for_video_correspondence/,XWD_,1563899480,"Check [https://zlai0.github.io/CorrFlow/ ](https://t.co/wI7367immN)   
We are happy to share the code &amp; model for Self-supervised Correspondence Flow (BMVC 2019 Oral)   
State-of-the-art performance on video segmentation and pose tracking.",0,1
1547,2019-7-24,2019,7,24,1,cguvvg,[P] How to Easily Understand Your Python Objects,https://www.reddit.com/r/MachineLearning/comments/cguvvg/p_how_to_easily_understand_your_python_objects/,hszafarek,1563899551,,0,1
1548,2019-7-24,2019,7,24,1,cgv0pb,[R] Self-supervised Learning for Video Correspondence Learning. State-of-the-art on Video Segmentation and Pose Tracking.,https://www.reddit.com/r/MachineLearning/comments/cgv0pb/r_selfsupervised_learning_for_video/,XWD_,1563900142,[removed],2,1
1549,2019-7-24,2019,7,24,1,cgv7am,[D] Anecdotes on Machine Learning and Missing/incomplete Data?,https://www.reddit.com/r/MachineLearning/comments/cgv7am/d_anecdotes_on_machine_learning_and/,EnderSword,1563900962,"I'm doing a short talk tomorrow on Data incompleteness and compensation techniques for Machine Learning in an area related to banking.   


I'm wondering if anyone had any stories, anecdotes or examples of cases where there was unintended behaviour around missing data or a story where it turned out to be quite significant or anything.  


The banking data stuff can be rather dry so hoping to have a few interesting examples outside our field.",4,1
1550,2019-7-24,2019,7,24,2,cgv9rg,[Project] Tetris-AI - A deep reinforcement learning agent that plays tetris,https://www.reddit.com/r/MachineLearning/comments/cgv9rg/project_tetrisai_a_deep_reinforcement_learning/,artificial-thinking,1563901276,"I've implemented an agent using deep reinforcement learning (with Q-Learning) that plays Tetris (not sure if it plays forever, but it seems to). It makes a decision based on the state that is expected to provide a higher reward in the future (i.e. it's not greedy, so it will, for example, wait to clear multiple lines instead of a single one).

[Here's a demo](https://i.imgur.com/Duw5Rkl.gif)

[Here's the source code](https://github.com/nuno-faria/tetris-ai) (made in Python with Keras + Tensorflow)

I appreciate any feedback.",26,28
1551,2019-7-24,2019,7,24,2,cgvb4m,Learning Better Simulation Methods for Partial Differential Equations,https://www.reddit.com/r/MachineLearning/comments/cgvb4m/learning_better_simulation_methods_for_partial/,sjoerdapp,1563901439,,0,1
1552,2019-7-24,2019,7,24,2,cgvbs6,HeinzCollege has been created,https://www.reddit.com/r/MachineLearning/comments/cgvbs6/heinzcollege_has_been_created/,TyrannosaurusFlex92,1563901519,,0,1
1553,2019-7-24,2019,7,24,2,cgvjue,SVM performing significantly less than chance,https://www.reddit.com/r/MachineLearning/comments/cgvjue/svm_performing_significantly_less_than_chance/,Strange_Lorenz,1563902532,"Hey all I was hoping someone in the community might be able to point me in a good direction to help with a problem I'm having.
I recently started using sklearn's support vector classifier training on some higher dimensionality data to predict one of two balanced (equal number of each) classes. I expected I would have close to 50/50 accuracy since there were many more features ( a few hundred) than samples (80 for each class). However, I consistently get numbers like 5% accuracy which doesn't seem reasonable.
Any suggestions would be appreciated.

Best",0,1
1554,2019-7-24,2019,7,24,2,cgvp2r,[Discussion] Best way to learn AWS for ML applications?,https://www.reddit.com/r/MachineLearning/comments/cgvp2r/discussion_best_way_to_learn_aws_for_ml/,rev_daydreamr,1563903172,"Just to set the context: I have decent experience with ML, I can design models that are appropriate for the problem at hand, I can implement them in code (mostly using open source libraries, think scikit-learn, Keras, PyTorch, but I can and have implemented from scratch if need be) and I understand/can intelligently communicate the math behind them, but I think I'm ready to take the next step. I'm trying to make myself marketable for ML Scientist/Engineer positions, and it seems that most of them require some knowledge and experience with cloud services and/or distributed computing, e.g. AWS, Azure, Kubernetes.

Would learning AWS be the next best step here? Or would you recommend a different platform/service? If AWS is the best way to go, how would I best go about learning to use and understand all of its services? Note that I would be doing this on my own free time (not through my company). I know there is AWS free-tier, but will I be able to try out and practice everything that I need there? Next, what are some good courses for beginners? I'd like courses with exercises that I can follow along, not just explanations of concepts. Is picking out a good course and following along on a free tier instance the way to go? How did you ""learn"" AWS?",12,5
1555,2019-7-24,2019,7,24,2,cgvq5y,What GPU is better?,https://www.reddit.com/r/MachineLearning/comments/cgvq5y/what_gpu_is_better/,drunkpolishbear,1563903296,[removed],0,1
1556,2019-7-24,2019,7,24,3,cgw7ab,"[D] Why in Word2Vec model, the hidden layer has no activation ?",https://www.reddit.com/r/MachineLearning/comments/cgw7ab/d_why_in_word2vec_model_the_hidden_layer_has_no/,amil123123,1563905465,"Hey all,

I was going through the following [blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) w.r.t Word2Vec models. I realized that the hidden layer which later on serves as the Embedding Matrix has no activation or has the ""Linear"" Activation. I remember that introducing non-linearities usually help. Is there a specific reason for doing so ?",22,3
1557,2019-7-24,2019,7,24,3,cgwa8o,How to run an effective machine learning team,https://www.reddit.com/r/MachineLearning/comments/cgwa8o/how_to_run_an_effective_machine_learning_team/,CometML,1563905846,,0,1
1558,2019-7-24,2019,7,24,4,cgxdco,Learning Tensorflow after Keras,https://www.reddit.com/r/MachineLearning/comments/cgxdco/learning_tensorflow_after_keras/,ThreeForElvenKings,1563910808,[removed],0,1
1559,2019-7-24,2019,7,24,4,cgxgu3,"2019 Google Scholar Metrics Released, CVPR Cracks the Top Ten",https://www.reddit.com/r/MachineLearning/comments/cgxgu3/2019_google_scholar_metrics_released_cvpr_cracks/,Yuqing7,1563911259,,0,1
1560,2019-7-24,2019,7,24,4,cgxhx0,Using Unity's MLAgents and PPO to land a Falcon 9,https://www.reddit.com/r/MachineLearning/comments/cgxhx0/using_unitys_mlagents_and_ppo_to_land_a_falcon_9/,SwissArmyApple,1563911401,,1,1
1561,2019-7-24,2019,7,24,4,cgxiux,What are example of Supervised Learning tasks that combine sequential and none-sequential data?,https://www.reddit.com/r/MachineLearning/comments/cgxiux/what_are_example_of_supervised_learning_tasks/,brandojazz,1563911516,,1,1
1562,2019-7-24,2019,7,24,5,cgxz9l,How to classify weather sensitive and non-sensitive energy usage based on kWh usage and temperature?,https://www.reddit.com/r/MachineLearning/comments/cgxz9l/how_to_classify_weather_sensitive_and/,santoshlmn,1563913626,[removed],0,1
1563,2019-7-24,2019,7,24,5,cgybhs,Notable Fundraising Highlights of Machine Learning Startups in 2016,https://www.reddit.com/r/MachineLearning/comments/cgybhs/notable_fundraising_highlights_of_machine/,andrea_manero,1563915196,[removed],0,1
1564,2019-7-24,2019,7,24,6,cgyeyb,[Q] Any Certificates Worth Taking?,https://www.reddit.com/r/MachineLearning/comments/cgyeyb/q_any_certificates_worth_taking/,DaTacularHB,1563915633,[removed],0,1
1565,2019-7-24,2019,7,24,6,cgyg9b,Factoring Massive Numbers with Machine Learning Techniques,https://www.reddit.com/r/MachineLearning/comments/cgyg9b/factoring_massive_numbers_with_machine_learning/,andrea_manero,1563915795,http://www.datasciencecentral.com/profiles/blogs/factoring-massive-numbers-a-new-machine-learning-approach,0,1
1566,2019-7-24,2019,7,24,7,cgzfpx,Is natural language ergodic?,https://www.reddit.com/r/MachineLearning/comments/cgzfpx/is_natural_language_ergodic/,errminator,1563920476,"If we model language using a transition matrix whose entries are the probabilities of any one character being followed by any other character, does this form an ergodic markov process?

My understanding is that ergodic means that regardless of what state (character) I'm currently on, there's a nonzero probability of returning to that character later in the text.

Even if some elements of the transition matrix have nonzero probability e.g. ""q"" followed by ""f"" or something like that, basic experience of English language tells me that if I'm currently reading a letter ""q"" in a book, there's going to be a nonzero probability of reading another ""q"" layer in the book. Is this intuition correct?

Is there any way to prove this mathematically?",0,1
1567,2019-7-24,2019,7,24,7,cgznin,The Big Data &amp; Analytics train is leaving the station  a Specialization in Big Data will boost your career and income growth,https://www.reddit.com/r/MachineLearning/comments/cgznin/the_big_data_analytics_train_is_leaving_the/,internetdigitalentre,1563921560,[removed],0,1
1568,2019-7-24,2019,7,24,8,ch0iam,[P] We made an online tool to generate busts of anime girls using a GAN,https://www.reddit.com/r/MachineLearning/comments/ch0iam/p_we_made_an_online_tool_to_generate_busts_of/,gebninja,1563926006,,0,1
1569,2019-7-24,2019,7,24,9,ch0ln0,[P] Decomposing latent space to generate custom anime girls,https://www.reddit.com/r/MachineLearning/comments/ch0ln0/p_decomposing_latent_space_to_generate_custom/,kvfrans,1563926489,,0,1
1570,2019-7-24,2019,7,24,9,ch0lt5,Are GA fitness functions always pure?,https://www.reddit.com/r/MachineLearning/comments/ch0lt5/are_ga_fitness_functions_always_pure/,carcigenicate,1563926517,[removed],0,1
1571,2019-7-24,2019,7,24,9,ch0qms,[P] Decomposing latent space to generate custom anime girls,https://www.reddit.com/r/MachineLearning/comments/ch0qms/p_decomposing_latent_space_to_generate_custom/,kvfrans,1563927212,"Hey all! We built a tool to efficiently walk through the distribution of anime girls. Instead of constantly re-sampling a single network, with a few steps you can specify the colors, details, and pose to narrow down the search!

We spent some good time polishing the experience, so check out the project at [waifulabs.com](https://waifulabs.com/)!

Also, a bulk of the interesting problems we faced this time was less on the training side and more on bringing the model to life -- we wrote a post about bringing the tech to Anime Expo as the Waifu Vending Machine, and all the little hacks along the way. Check that out at [https://waifulabs.com/blog/ax](https://waifulabs.com/blog/ax)",113,459
1572,2019-7-24,2019,7,24,9,ch120h,Why is no one talking about this?,https://www.reddit.com/r/MachineLearning/comments/ch120h/why_is_no_one_talking_about_this/,Wi1lis996,1563928982,[removed],0,1
1573,2019-7-24,2019,7,24,11,ch24j3,5 ALASAN MEMILIH SENSOR TORSI M425?,https://www.reddit.com/r/MachineLearning/comments/ch24j3/5_alasan_memilih_sensor_torsi_m425/,rizki28,1563934970,,0,1
1574,2019-7-24,2019,7,24,11,ch2553,Is there a way to debug Jupyter Lab?,https://www.reddit.com/r/MachineLearning/comments/ch2553/is_there_a_way_to_debug_jupyter_lab/,DolantheMFWizard,1563935068,[removed],0,1
1575,2019-7-24,2019,7,24,11,ch28nl,[D] What is a SOTA model or standardized dataset for classification/detection of image *attributes*?,https://www.reddit.com/r/MachineLearning/comments/ch28nl/d_what_is_a_sota_model_or_standardized_dataset/,toadsofbattle,1563935643,"Hi, pretty new to computer vision here. I've seen a lot of standardized datasets for CV such as MNIST, CIFAR-10, ImageNet, etc. ImageNet in particular stood out to me because of how specific a lot of the image classifications were, such as ""pug"" or ""dalmation"". However, is there a model out there which focuses on classifying attributes of the image - e.g. ""brown dog"", or ""tall person"", or ""spotted dog"", or something like that, or a standardized dataset that would enable that analysis?",0,1
1576,2019-7-24,2019,7,24,11,ch29pm,"[D] Is there a SOTA model or standardized dataset with a focus on detection/classification of object *attributes*, not just object type?",https://www.reddit.com/r/MachineLearning/comments/ch29pm/d_is_there_a_sota_model_or_standardized_dataset/,toadsofbattle,1563935816,"Hi, pretty new to computer vision here. I've seen a lot of standardized datasets for CV such as MNIST, CIFAR-10, ImageNet, etc. ImageNet in particular stood out to me because of how specific a lot of the image classifications were, such as ""pug"" or ""dalmation"". However, is there a model out there which focuses on classifying attributes of the object e.g. dog color, or a standardized dataset that would enable that analysis?

Pointing me towards the relevant literature would be really appreciated! I've done my Googling but I think I might not know the specific terms used in this domain to describe stuff like this, because I keep running into generic object classification and detection without the object attributes.",3,2
1577,2019-7-24,2019,7,24,11,ch2bnm,How to generate ideas in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/ch2bnm/how_to_generate_ideas_in_machine_learning/,postmachines,1563936121,,0,1
1578,2019-7-24,2019,7,24,11,ch2c35,[D] How to generate ideas in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/ch2c35/d_how_to_generate_ideas_in_machine_learning/,postmachines,1563936191,,0,1
1579,2019-7-24,2019,7,24,12,ch2lcm,Quiz: Test Your Knowledge about AI and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ch2lcm/quiz_test_your_knowledge_about_ai_and_machine/,LimarcAmbalina,1563937702,,0,1
1580,2019-7-24,2019,7,24,12,ch2o60,[P] Final Project ANN,https://www.reddit.com/r/MachineLearning/comments/ch2o60/p_final_project_ann/,hendraahalimm,1563938149,"Hello, I'm Hendra. I'm an undergraduate students who is working on final project so I can graduate. My lecturer asked me to use ANN for energy disaggregation system. I'm using keras machine learning with tensor flow as the backend. Lately I have a problem on making a correct model for my case and got bad accuracy score. I have 4 lamps with various power consumption and I want to predict the power consumption of each lamp using ANN. My dataset is on link. On the dataset, 1 means ON and 0 means OFF. I'm using jupyter notebook as its interface. Can anyone give me some idea about what models I can use for this type of problem? I'm currently stumped. Any kind of help is really appreciated. Thank you.....
Here is the dataset : https://drive.google.com/file/d/1ZrW083twfEIHZzqGuaDYA7jjO2s5P44p/view?usp=sharing",7,0
1581,2019-7-24,2019,7,24,12,ch312x,Machine learning question,https://www.reddit.com/r/MachineLearning/comments/ch312x/machine_learning_question/,Spartan66666,1563940306,[removed],0,1
1582,2019-7-24,2019,7,24,13,ch3cel,"[P] Sobert, a neural network chat bot",https://www.reddit.com/r/MachineLearning/comments/ch3cel/p_sobert_a_neural_network_chat_bot/,DevigForager,1563942216,"I've been working on a small project for a few years, and I finally made a simple web interface. Sobert is a chat bot using a 4-layer, 113 million parameter byte-level Grid GRU language model trained on \~370MB of chat data collected over the years.

[https://sobert.hammy.sk/](https://sobert.hammy.sk/)

Be careful, Sobert understands many topics, and not all of them are SFW. The web version is also new and not well tested, and won't be able to handle too many users, but the [Telegram version](https://t.me/SobertBot) should be quite reliable.

[Code for all the related parts](https://github.com/antihutka/)",14,11
1583,2019-7-24,2019,7,24,14,ch46s8,[P] Lyft releases self-driving research dataset,https://www.reddit.com/r/MachineLearning/comments/ch46s8/p_lyft_releases_selfdriving_research_dataset/,chisai_mikan,1563947672,"[Announcement](https://medium.com/lyftlevel5/unlocking-access-to-self-driving-research-the-lyft-level-5-dataset-and-competition-d487c27b1b6c)

*Were thrilled to share a comprehensive, large-scale dataset featuring the raw sensor camera and LiDAR inputs as perceived by a fleet of multiple, high-end, autonomous vehicles in a bounded geographic area. This dataset also includes high quality, human-labelled 3D bounding boxes of traffic agents, an underlying HD spatial semantic map.

With this, we aim to empower the community, stimulate further development, and share our insights into future opportunities from the perspective of an advanced industrial Autonomous Vehicles program.*

Link to the dataset's [page](https://level5.lyft.com/dataset/?source=post_page---------------------------) with more technical information and examples.

There's also going to be a competition at NeurIPS this year, which I assume to be the recently added ""AI Driving Olympics"" at the very bottom of the [competitions](https://nips.cc/Conferences/2019/CompetitionTrack) page...",14,158
1584,2019-7-24,2019,7,24,15,ch4ghi,What is Machine Learning? | Machine Learning Applications,https://www.reddit.com/r/MachineLearning/comments/ch4ghi/what_is_machine_learning_machine_learning/,AnkitKap,1563949502,,0,1
1585,2019-7-24,2019,7,24,15,ch4orl,"How significant the variables like ""id"", ""region code"" etc are in the predictive modelling ?",https://www.reddit.com/r/MachineLearning/comments/ch4orl/how_significant_the_variables_like_id_region_code/,PayalBhatia,1563951119,[removed],0,1
1586,2019-7-24,2019,7,24,16,ch4wau,[R] Style Transfer with GANs on HD Images,https://www.reddit.com/r/MachineLearning/comments/ch4wau/r_style_transfer_with_gans_on_hd_images/,artika_labs,1563952577," I have been recently exploring how to make GANs work with HD images, without the need of expensive GPUs and long computation times.

This is my result:

 [https://towardsdatascience.com/style-transfer-with-gans-on-hd-images-88e8efcf3716](https://towardsdatascience.com/style-transfer-with-gans-on-hd-images-88e8efcf3716) 

  
It surely isnt anything new, but I have never heard of this super easy solution being applied to this kind of goal.

I would love to know if there are other methods in literature similar to this one.",7,10
1587,2019-7-24,2019,7,24,17,ch5go9,This T-shirt does not exist,https://www.reddit.com/r/MachineLearning/comments/ch5go9/this_tshirt_does_not_exist/,digitm,1563956821,"http://ganarts.ru presents StyleGAN-generated prints for T-shirts in the style of contemporary art. Feel free to print these masterpieces on your T-shirts!

StyleGAN trained on 40,000 images, the training process takes 3 days on 4x1080ti GPU. Every 30 seconds there are 9 new images, and the old ones are deleted.",0,1
1588,2019-7-24,2019,7,24,17,ch5kfq,[P] This T-shirt does not exist,https://www.reddit.com/r/MachineLearning/comments/ch5kfq/p_this_tshirt_does_not_exist/,digitm,1563957694,"Hi! http://ganarts.ru presents StyleGAN-generated prints for T-shirts in the style of contemporary art. Feel free to print these masterpieces on your T-shirts!

StyleGAN trained on 40,000 images, the training process takes 3 days on 4x1080ti GPU. Every 30 seconds there are 9 new images, and the old ones are deleted.",20,64
1589,2019-7-24,2019,7,24,17,ch5omp,[D] A subjective summary of EEML 2019,https://www.reddit.com/r/MachineLearning/comments/ch5omp/d_a_subjective_summary_of_eeml_2019/,ricsinaruto,1563958633,"I attended this year's [Eastern European Machine Learning Summer School](https://www.eeml.eu/home) and wanted to share my experience.

I tried to summarize all the lectures and events in this blog post:

 [https://medium.com/@richardcsaky/eeml-2019-a-subjective-retelling-67448909e64b](https://medium.com/@richardcsaky/eeml-2019-a-subjective-retelling-67448909e64b)   


I think there will be a 2020 edition, and I definitely recommend applying to it!",0,8
1590,2019-7-24,2019,7,24,18,ch5vgv,AI Dalek Beach Cleaner  Reinforcement learning in ml-agent,https://www.reddit.com/r/MachineLearning/comments/ch5vgv/ai_dalek_beach_cleaner_reinforcement_learning_in/,MrBenjaminGraham,1563959998,[removed],0,1
1591,2019-7-24,2019,7,24,18,ch5z55,Stop cone,https://www.reddit.com/r/MachineLearning/comments/ch5z55/stop_cone/,karenactionsitafaal,1563960786,,1,1
1592,2019-7-24,2019,7,24,18,ch5zv2,[D] Help on how to extract specific text from image with printed and handwritten text,https://www.reddit.com/r/MachineLearning/comments/ch5zv2/d_help_on_how_to_extract_specific_text_from_image/,Atralb,1563960930,"Hi there,

&amp;#x200B;

I have a task where I need to extract specific elements (name, surname, company name) from text present in images which are forms and other administrative documents (example [here](https://farfadoc.files.wordpress.com/2013/11/photo.jpg)), but there are many types of documents and each are structured differently.

**Sometimes the required elements will be handwritten, sometimes not**. **They also will be placed differently on the page depending on the document.**

And the **data is not annotated**. (do I need to annotate it myself before, or can it be made unsupervised ?)

&amp;#x200B;

So I'm at a real loss here on what to do. ^((I'm a beginner in the field so please bear with me :).))

I'm really not an expert on NLP nor text extraction or detection. Mostly worked on images with classical datasets.

&amp;#x200B;

Classic OCR techniques I tested like Tesseract don't work at all for handwritten text (maybe there's appropriate configuration for this ?).

Since the data is so unstructured (to me) I thought of using neural networks and especially a Faster RCNN model which I would train to detect only the three elements I mentioned before (after having annotated the data myself) as if they were specific objects like for classical Faster RCNN models. [Paper](https://ieeexplore.ieee.org/abstract/document/8270290) ^((use hub of science)) on researchers successfully doing it.

But someone I talked to told me it was easy to do even with the constraints I told you, and that neural networks weren't needed, nor was annotating.

My guess at the beginning was that I needed to find a way to extract the whole text from the image (with the handwritten text, and well positioned in relation to the printed text, and then find a way to give my algorithm insight on how to distinguish names from the rest.

How could I do this with simple machine learning ?Is there a dataset of names and surnames that I can train my model on so that it would ?

&amp;#x200B;

Thanks a lot in advance for your help !",5,4
1593,2019-7-24,2019,7,24,18,ch64um,[D] Introducing TensorFlow Addons,https://www.reddit.com/r/MachineLearning/comments/ch64um/d_introducing_tensorflow_addons/,Azzu98,1563961956,,0,2
1594,2019-7-24,2019,7,24,19,ch6bcu,L2 Regularization and Batch Norm,https://www.reddit.com/r/MachineLearning/comments/ch6bcu/l2_regularization_and_batch_norm/,_joermungandr_,1563963264,,0,1
1595,2019-7-24,2019,7,24,19,ch6c73,Evaluate vendors,https://www.reddit.com/r/MachineLearning/comments/ch6c73/evaluate_vendors/,vigg_1991,1563963437,[removed],0,1
1596,2019-7-24,2019,7,24,19,ch6ci0,"Self driving car simulator by Udacity, tips to improve the model?",https://www.reddit.com/r/MachineLearning/comments/ch6ci0/self_driving_car_simulator_by_udacity_tips_to/,NicoBacc,1563963506,[removed],0,1
1597,2019-7-24,2019,7,24,19,ch6dcw,How to do spatial K means graph cut?,https://www.reddit.com/r/MachineLearning/comments/ch6dcw/how_to_do_spatial_k_means_graph_cut/,textssg,1563963675,[removed],0,1
1598,2019-7-24,2019,7,24,19,ch6h6g,"[D] First time in KDD, what to expect?",https://www.reddit.com/r/MachineLearning/comments/ch6h6g/d_first_time_in_kdd_what_to_expect/,MichaelDoron_II,1563964409,"Hey all, I'm a neuroscience PhD student, and this will be my first KDD (and my first CS conference in general, really). How is KDD different than other STEM conferences? What should I expect or prepare to? Are there any events that I should know about, or things to avoid?

Thanks!",0,1
1599,2019-7-24,2019,7,24,20,ch6r49,"given a string such as ""TRUCKS CARS AUTOMOBILES 5000"" how do i extract just the value ""5000""?",https://www.reddit.com/r/MachineLearning/comments/ch6r49/given_a_string_such_as_trucks_cars_automobiles/,bigdbag999,1563966318,[removed],0,1
1600,2019-7-24,2019,7,24,20,ch6y70,Detecting the type of trend show by a graph,https://www.reddit.com/r/MachineLearning/comments/ch6y70/detecting_the_type_of_trend_show_by_a_graph/,satnam_sandhu,1563967657,[removed],0,1
1601,2019-7-24,2019,7,24,20,ch74qq,Dangerous Deepfakes,https://www.reddit.com/r/MachineLearning/comments/ch74qq/dangerous_deepfakes/,valuequal,1563968799,,0,1
1602,2019-7-24,2019,7,24,20,ch78dl,Machine Learning as a Service Market - Global Opportunity Analysis and Industry Forecast (2019-2025) - Research312,https://www.reddit.com/r/MachineLearning/comments/ch78dl/machine_learning_as_a_service_market_global/,kavi2930,1563969420,,0,1
1603,2019-7-24,2019,7,24,21,ch7agv,Comet.ml partners with Uber on Ludwig,https://www.reddit.com/r/MachineLearning/comments/ch7agv/cometml_partners_with_uber_on_ludwig/,CometML,1563969782,,0,1
1604,2019-7-24,2019,7,24,21,ch7k9i,Is it possible to use channel pruning or distillation to reduce FLOPs of DenseNet?,https://www.reddit.com/r/MachineLearning/comments/ch7k9i/is_it_possible_to_use_channel_pruning_or/,dezilatsyrC,1563971360,[removed],0,1
1605,2019-7-24,2019,7,24,21,ch7o03,"We have recently released a blazingly fast implementation of Byte Pair Encoding (BPE) algorithm. It's much faster than Google SentencePiece, has Python and CLI interfaces. O(N) complexity for training.",https://www.reddit.com/r/MachineLearning/comments/ch7o03/we_have_recently_released_a_blazingly_fast/,Yutkin,1563971975,,1,5
1606,2019-7-24,2019,7,24,21,ch7oqp,Rotary pouch filling and sealing machine for powder manufacturers,https://www.reddit.com/r/MachineLearning/comments/ch7oqp/rotary_pouch_filling_and_sealing_machine_for/,YQPACK,1563972099,,0,1
1607,2019-7-24,2019,7,24,22,ch84dq,[R] Benchmarking a Catchment-Aware LSTM for Large-Scale Hydrological Modeling,https://www.reddit.com/r/MachineLearning/comments/ch84dq/r_benchmarking_a_catchmentaware_lstm_for/,fkratzert,1563974519,"Hi everyone,

at the beginning of this week we uploaded our manuscript on large-scale hydrological modeling using LSTM-based models. Maybe you find something interesting/useful in it. If you have any questions regarding this work, feel free to ask.

&amp;#x200B;

**Abstract:**

Regional rainfall-runoff modeling is an old but still mostly out-standing problem in Hydrological Sciences. The problem currently is that traditional hydrological models degrade significantly in performance when calibrated for multiple basins together instead of for a single basin alone. In this paper, we propose a novel, data-driven approach using Long Short-Term Memory networks (LSTMs), and demonstrate that under a 'big data' paradigm, this is not necessarily the case. By training a single LSTM model on 531 basins from the CAMELS data set using meteorological time series data and static catchment attributes, we were able to significantly improve performance compared to a set of several different hydrological benchmark models. Our proposed approach not only significantly outperforms hydrological models that were calibrated regionally but also achieves better performance than hydrological models that were calibrated for each basin individually. Furthermore, we propose an adaption to the standard LSTM architecture, which we call an Entity-Aware-LSTM (EA-LSTM), that allows for learning, and embedding as a feature layer in a deep learning model, catchment similarities. We show that this learned catchment similarity corresponds well with what we would expect from prior hydrological understanding.

&amp;#x200B;

**TLDR;**

* Single LSTM-based model is trained on meteorological time series + static catchment attributes to predict river runoff for hundreds of catchments.
* Outperforms a large set of hydrological benchmark models (calibrated by independent groups) significantly, even in unfair settings (hyd. models calibrated for each catchment separately vs. LSTM model calibrated for all at once).
* Proposed LSTM adaption (Entity-Aware-LSTM), where static features are used to modulate the input gate and all remaining parts of the LSTM only receives recurrent + dynamic (meteorological time series) input.

&amp;#x200B;

**Manuscript:** [https://arxiv.org/abs/1907.08456](https://arxiv.org/abs/1907.08456)

**Code + Data:** [https://github.com/kratzert/ealstm\_regional\_modeling](https://github.com/kratzert/ealstm_regional_modeling)",2,26
1608,2019-7-24,2019,7,24,22,ch84kz,What's going on with AISTATS 2020?,https://www.reddit.com/r/MachineLearning/comments/ch84kz/whats_going_on_with_aistats_2020/,HenryWJReeve,1563974553,[removed],1,2
1609,2019-7-24,2019,7,24,22,ch84sl,"[P] We have recently released a blazingly fast implementation of Byte Pair Encoding (BPE) algorithm. It's much faster than Google SentencePiece, has Python and CLI interfaces. O(N) complexity for training.",https://www.reddit.com/r/MachineLearning/comments/ch84sl/p_we_have_recently_released_a_blazingly_fast/,Yutkin,1563974587,[removed],0,1
1610,2019-7-24,2019,7,24,22,ch88xj,"[P] We have recently released a blazingly fast implementation of Byte Pair Encoding (BPE) algorithm. It's much faster than Google SentencePiece, has Python and CLI interfaces. O(N) complexity for training.",https://www.reddit.com/r/MachineLearning/comments/ch88xj/p_we_have_recently_released_a_blazingly_fast/,Yutkin,1563975224,,0,2
1611,2019-7-24,2019,7,24,22,ch8bni,honey packaging machine price for ketchup cream sauce manufacturers,https://www.reddit.com/r/MachineLearning/comments/ch8bni/honey_packaging_machine_price_for_ketchup_cream/,YQPACK,1563975630,,0,1
1612,2019-7-24,2019,7,24,23,ch8pzx,Machine Learning Subreddits,https://www.reddit.com/r/MachineLearning/comments/ch8pzx/machine_learning_subreddits/,antaloaalonso,1563977701,[removed],0,1
1613,2019-7-24,2019,7,24,23,ch8t1k,Computer Vision Concept for Commercial Property Maintenance,https://www.reddit.com/r/MachineLearning/comments/ch8t1k/computer_vision_concept_for_commercial_property/,Macstevey,1563978122,[removed],0,1
1614,2019-7-24,2019,7,24,23,ch8u82,Nunchaku Tutorial slowmo and real time finger revision,https://www.reddit.com/r/MachineLearning/comments/ch8u82/nunchaku_tutorial_slowmo_and_real_time_finger/,thetrickshotone,1563978274,,0,1
1615,2019-7-24,2019,7,24,23,ch8wlv,#AIforGood Challeges to Solve Big Social Problems,https://www.reddit.com/r/MachineLearning/comments/ch8wlv/aiforgood_challeges_to_solve_big_social_problems/,Lordobba,1563978621,[removed],0,1
1616,2019-7-25,2019,7,25,0,ch9cbp,[P] Using ML to Make an AI Jet That Bombs a Missile Launcher,https://www.reddit.com/r/MachineLearning/comments/ch9cbp/p_using_ml_to_make_an_ai_jet_that_bombs_a_missile/,brandonhotdog,1563980733,"I used Unity3D to make an AI jet that learned how to bomb a missile launcher. It used deep reinforcement learning and PPO. 

I put the results together into a short video you can see here:  [https://www.youtube.com/watch?v=iP28kOCpW94](https://www.youtube.com/watch?v=iP28kOCpW94) 

If any of you want to see the source you can download it here:  [http://www.mediafire.com/file/n120iqtrynlmr07/AI\_Jet\_Bomber.zip/file](http://www.mediafire.com/file/n120iqtrynlmr07/AI_Jet_Bomber.zip/file)",14,1
1617,2019-7-25,2019,7,25,0,ch9r1z,NeurIPS 2019 reviews are out!!,https://www.reddit.com/r/MachineLearning/comments/ch9r1z/neurips_2019_reviews_are_out/,Mannershin,1563982671,[removed],0,1
1618,2019-7-25,2019,7,25,0,ch9s72,Google BlazeFace Performs Submillisecond Neural Face Detection on Mobile GPUs,https://www.reddit.com/r/MachineLearning/comments/ch9s72/google_blazeface_performs_submillisecond_neural/,Yuqing7,1563982828,,0,1
1619,2019-7-25,2019,7,25,0,ch9wks,"[D] Given a string such as ""TRUCKS CARS AUTOMOBILES 5000"" how do i extract just the value ""5000""?",https://www.reddit.com/r/MachineLearning/comments/ch9wks/d_given_a_string_such_as_trucks_cars_automobiles/,bigdbag999,1563983405,"Posted originally in /r/MLQuestions to no avail.  Please delete if this doesn't belong here.  

&amp;#x200B;

My problem is a bit unique I think, though in all probability it's my naivety with ML and I just don't know if this is a solved problem.

&amp;#x200B;

We are processing standard forms from scanned images using some OCR techniques, and have a JSON output that basically shows something like this:

`""3"": {""fieldValue"": ""TRUCKS CARS AUTOMOBILES 5000""},`

&amp;#x200B;

The above comes from a field called ""TRUCKS CARS AUTOMOBILES"" and the value that's entered in for that particular form field is ""5000"".  The OCR cannot separate the form label from the form value, so we need to parse this.  Initially we tried to regex every field value out, but this proved to be too brittle, as our OCR does not perfectly recognize text; words like 'address' might output as 'addrefss' etc.  Next I tried to use the python library fuzzywuzzy to do fuzzy string replacement instead of pure regex.  The result was far better but there are still many edge cases that I can't account for considering the nature of these forms are varied and sometimes of poor scan quality.  We have many different types of fields and values, for example another field looks like this:

`""455"": {""fieldValue"": ""ADDRESS (CO NAME AND PLACE) COCONUT FACTORY 12345 COCO STREET MIAMI FL 86884""},`

&amp;#x200B;

The upside is that we have a JSON file that also corresponds to the above JSON with data labeling of sorts, which is why I initially tried to regex, and then fuzzywuzzy.  Here is an example corresponding JSON to the extracted value above:

`""455"": {""label"": ""ADDRESS (CO NAME AND PLACE)""},`

&amp;#x200B;

My thought is to use an NLP library like TextBlob or Spacy to somehow classify labels, and then extract the remaining portion of the string.

&amp;#x200B;

What is the best approach to do this?  Thanks!",5,0
1620,2019-7-25,2019,7,25,0,ch9y2n,"Simple Questions Thread July 24, 2019",https://www.reddit.com/r/MachineLearning/comments/ch9y2n/simple_questions_thread_july_24_2019/,AutoModerator,1563983581,[removed],0,1
1621,2019-7-25,2019,7,25,0,ch9ych,[D] NeurIPS 2019 reviews out soon!!,https://www.reddit.com/r/MachineLearning/comments/ch9ych/d_neurips_2019_reviews_out_soon/,Mannershin,1563983614,Good Luck everybody!,185,31
1622,2019-7-25,2019,7,25,1,cha4rq,Does it make a difference in LSTM as to how I give the input?,https://www.reddit.com/r/MachineLearning/comments/cha4rq/does_it_make_a_difference_in_lstm_as_to_how_i/,gopal_chitalia,1563984407,[removed],0,1
1623,2019-7-25,2019,7,25,1,chacdt,How to choose a best VPN service?And top VPN services of 2019 VPN stands for Virtual Private Network which is a popular Internet security method. A VPN services is a private network that uses a public network to connect remote sites or users together. To get more information click on below link,https://www.reddit.com/r/MachineLearning/comments/chacdt/how_to_choose_a_best_vpn_serviceand_top_vpn/,mohitkumarr153,1563985381,,1,1
1624,2019-7-25,2019,7,25,1,chaend,[P] Computer Vision Concept for Property Maintenance,https://www.reddit.com/r/MachineLearning/comments/chaend/p_computer_vision_concept_for_property_maintenance/,Macstevey,1563985666,"Hello all,

&amp;#x200B;

I thought this may be of interest to the community. I've been developing, with the aid of a freelancer so far, a computer vision concept (codename, Analizar) for use in the real estate field (in the high octane world of property maintenance, to be precise). This follows on from a previous post here exploring the possibilities of [image classification](https://www.reddit.com/r/datascience/comments/9o6tl8/using_image_classification_for_automated_property/). It is a basic demo website which generates automated maintenance information from user submitted photos; [Analizar Computer Vision Concept](https://www.mccormackpartners.co.uk/)

&amp;#x200B;

The demo is currently trained (to varying degrees of success) to identify the following materials / issues.

&amp;#x200B;

|Asbestos cement sheet roofs|Cladding cut edge corrosion|Brick efflorescence|
|:-|:-|:-|
|Asbestos toilet cisterns|Asbestos floor tiles|Roof ponding|
|Asbestos artex wall coatings|Condensation Mould|Slate roofing|
|Japanese knotweed|Concrete crazing|Timber weatherboarding|

&amp;#x200B;

https://i.redd.it/4lbb6o9w1ac31.png

&amp;#x200B;

You can download the photos [here](https://drive.google.com/drive/u/2/folders/10e2ynOZ5HQqHQ9BV-nQcAr88VM80qZ2P), or see how it performs with photos you can find online.

&amp;#x200B;

https://i.redd.it/88mcx1gx1ac31.png

&amp;#x200B;

For a (very) brief tech summary, both frontend / backend are deployed on an AWS EC2 instance. Initial training data was sourced from my own records and supplemented with web scraped images. The backend model is a convolutional neural network. The domain knowledge is provided by me. Although this may be some way from a commercial application, it does highlight the potential in property management for computer vision applications. There may even be scope to perform a visual search function (the timber weatherboarding label is included for demonstration of this), perhaps coupled with an augmented reality interface. I also prepared a business model canvas that envisages this concept as part of a wider property maintenance service offering (serving commercial and residential sectors).

&amp;#x200B;

For a bit more on my background, I'vebeen working in this field for nearly 10 years. There's a few things I'm tired of seeing; business friction, inconsistency of service and consumer misinformation. That's partly due to the fragmented nature of contractorsand advisors, partly due to the snails paceof innovation in property maintenance. Now that I have my own [practice](https://www.mccormackpartners.com/), I'm looking to reinvest my cashflow from traditional activities into concepts such as this, and try to help the sector evolve.

&amp;#x200B;

One thing I lack however is a deep network of tech / machine learning expertise, mentors and business advisors, particularly those who are crossing the chasm between property maintenance and software. That's one reason why I've been lurking on these forums; it gives me a glimpse into the tech startup ecosystem and helps my understanding of software / MVP development.

&amp;#x200B;

So, to roundup, I would love to hear your thoughts on this. I'm very keen to develop a relationship with this community; to both further my software understanding, and continue the discussion with any interested parties. If this intrigues you (or you think it's a terrible idea!) then please get in touch!

&amp;#x200B;

Thanks for reading,

Steven",3,1
1625,2019-7-25,2019,7,25,1,chagzj,"""Multi-Task Learning in the Wilderness"" - Andrej Karpathy",https://www.reddit.com/r/MachineLearning/comments/chagzj/multitask_learning_in_the_wilderness_andrej/,downtownslim,1563985973,[removed],0,1
1626,2019-7-25,2019,7,25,1,chahb5,"[R] ""Multi-Task Learning in the Wilderness"" - Andrej Karpathy",https://www.reddit.com/r/MachineLearning/comments/chahb5/r_multitask_learning_in_the_wilderness_andrej/,downtownslim,1563986010,"Driven by progress in deep learning, the machine learning community is now able to tackle increasingly more complex problemsranging from multi-modal reasoning to dexterous robotic manipulationall of which typically involve solving nontrivial combinations of tasks. Thus, designing adaptive models and algorithms that can efficiently learn, master, and combine multiple tasks is the next frontier.

Video: [https://slideslive.com/38917690/multitask-learning-in-the-wilderness](https://slideslive.com/38917690/multitask-learning-in-the-wilderness)",2,20
1627,2019-7-25,2019,7,25,1,chaqxk,Visualize Keras Models with One Line of Code,https://www.reddit.com/r/MachineLearning/comments/chaqxk/visualize_keras_models_with_one_line_of_code/,silizannek,1563987257,,0,1
1628,2019-7-25,2019,7,25,2,chb1by,Visualize Keras Models with One Line of Code,https://www.reddit.com/r/MachineLearning/comments/chb1by/visualize_keras_models_with_one_line_of_code/,silizannek,1563988568,,0,1
1629,2019-7-25,2019,7,25,2,chb757,[R] A New Perspective on Adversarial Perturbations - Aleksander Madry,https://www.reddit.com/r/MachineLearning/comments/chb757/r_a_new_perspective_on_adversarial_perturbations/,Pestocalypse,1563989316,"Video here: [https://www.youtube.com/watch?v=mUt7w4UoYqM](https://www.youtube.com/watch?v=mUt7w4UoYqM)

&amp;#x200B;

I thought this was an amazing talk by Aleksander Madry on what adversarial examples really are; a problem with the data we train our models on. That's one reason why they generalize.",8,34
1630,2019-7-25,2019,7,25,2,chb8hq,[R] Enhancing satellite imagery with deep multi-temporal super-resolution,https://www.reddit.com/r/MachineLearning/comments/chb8hq/r_enhancing_satellite_imagery_with_deep/,diev047,1563989490,,1,1
1631,2019-7-25,2019,7,25,2,chb8v2,[Doubt/learning]How to make projects from Pre-trained model/Research paper?,https://www.reddit.com/r/MachineLearning/comments/chb8v2/doubtlearninghow_to_make_projects_from_pretrained/,try_reddit_sisyphus,1563989534,"I am new in ML/DL. I am constantly trying to learn. But i am finding difficulty in implementation of pre trained models and/or research papers. I am also finding difficulty in finding resources of the same and finding resources of deployment of models in server( integration with other frameworks, django too).

Ps: sorry for using ""finding"" word too much.",0,1
1632,2019-7-25,2019,7,25,2,chbfdc,Enhancing satellite imagery with deep multi-temporal super-resolution,https://www.reddit.com/r/MachineLearning/comments/chbfdc/enhancing_satellite_imagery_with_deep/,diev047,1563990339,[removed],0,1
1633,2019-7-25,2019,7,25,2,chbfv2,PyTorch Implementation of Transporter (Unsupervised Learning of Object Keypoints for Perception and Control),https://www.reddit.com/r/MachineLearning/comments/chbfv2/pytorch_implementation_of_transporter/,ethanluoyc,1563990402,,0,1
1634,2019-7-25,2019,7,25,2,chbfzg,"Apart from ML, what skills do Data Scientists need today?",https://www.reddit.com/r/MachineLearning/comments/chbfzg/apart_from_ml_what_skills_do_data_scientists_need/,heatherrale,1563990419,,0,1
1635,2019-7-25,2019,7,25,3,chbqwo,"This AI learns from conversations, no other training required",https://www.reddit.com/r/MachineLearning/comments/chbqwo/this_ai_learns_from_conversations_no_other/,James_Representi,1563991829,,0,1
1636,2019-7-25,2019,7,25,3,chbtry,Get started with Comet + Ludwig today,https://www.reddit.com/r/MachineLearning/comments/chbtry/get_started_with_comet_ludwig_today/,CometML,1563992206,,0,1
1637,2019-7-25,2019,7,25,3,chbuet,NAACL 19 Notes: Practical Insights for Natural Language Processing Applications  Part I,https://www.reddit.com/r/MachineLearning/comments/chbuet/naacl_19_notes_practical_insights_for_natural/,nzhiltsov,1563992287,,0,1
1638,2019-7-25,2019,7,25,3,chbv7t,[D] Current status of Deep Learning for fluid physics (nonlinear PDEs),https://www.reddit.com/r/MachineLearning/comments/chbv7t/d_current_status_of_deep_learning_for_fluid/,IborkedyourGPU,1563992394,"As per object, what is the current state of the art? Last year we had some work on approximating both the solution **and** the PDE using neural networks:

https://arxiv.org/abs/1801.06637

This year (well, actually last year, but it kept being revised until recently) we had the paper from Google on approximating the solution given knowledge of the PDE (whose results are frankly not impressive as advertised, solving the 1D Burgers equation with 1024 convolutions is not gonna give the scare to commercial CFD codes producers)

https://arxiv.org/abs/1808.04930

There must have been something else, of course, or NeurIPS wouldn't have accepted a [workshop on Machine Learning and the Physical Sciences](https://medium.com/@NeurIPSConf/2019workshops-ec820e4d558e). What's the current state of the art? I'm especially interested in fluid dynamics, but I wouldn't mind learning about the solution of PDEs stemming from other branches of physics.",17,13
1639,2019-7-25,2019,7,25,3,chbwfp,"[R] This AI learns from conversations, no other training required",https://www.reddit.com/r/MachineLearning/comments/chbwfp/r_this_ai_learns_from_conversations_no_other/,James_Representi,1563992552,,0,1
1640,2019-7-25,2019,7,25,3,chbxsg,[P] College ordered in these three for my Extended Qualification Project.,https://www.reddit.com/r/MachineLearning/comments/chbxsg/p_college_ordered_in_these_three_for_my_extended/,PlatinumNinja72,1563992716,,1,1
1641,2019-7-25,2019,7,25,3,chbynx,[1907.09595] MixNet: Mixed Depthwise Convolutional Kernels,https://www.reddit.com/r/MachineLearning/comments/chbynx/190709595_mixnet_mixed_depthwise_convolutional/,brettkoonce,1563992833,,7,8
1642,2019-7-25,2019,7,25,3,chbzud,SAE University Thesis - A.I in Music Production,https://www.reddit.com/r/MachineLearning/comments/chbzud/sae_university_thesis_ai_in_music_production/,A-Nedyalkov,1563992985,[removed],0,1
1643,2019-7-25,2019,7,25,3,chc0dx,I want to learn machine learning from beginning. Can someone suggest me the best course to learn the same.,https://www.reddit.com/r/MachineLearning/comments/chc0dx/i_want_to_learn_machine_learning_from_beginning/,guthisupriya,1563993065,[removed],0,1
1644,2019-7-25,2019,7,25,3,chc1gl,[R] Enhancing satellite imagery with deep multi-temporal super-resolution,https://www.reddit.com/r/MachineLearning/comments/chc1gl/r_enhancing_satellite_imagery_with_deep/,diev047,1563993200,"https://towardsdatascience.com/enhancing-satellite-imagery-with-deep-multi-temporal-super-resolution-24f08586ada0

Paper: https://arxiv.org/abs/1907.06490

Code: https://github.com/diegovalsesia/deepsum",0,0
1645,2019-7-25,2019,7,25,3,chc220,[Research] Neural Point-Based Graphics,https://www.reddit.com/r/MachineLearning/comments/chc220/research_neural_pointbased_graphics/,alievk91,1563993272,"Hey all,

&amp;#x200B;

Let me introduce our new work on *real-time photo-realistic* neural rendering. The method allows you to render complex scenes from *novel viewpoints* using *raw point clouds* as proxy geometry and require no meshes. Pipeline is following: scan object  with ordinary video camera, produce the point cloud using widely available software (e.g. Agisoft Metashape), feed the point cloud and video to the algorithm and that's it!

&amp;#x200B;

The core ingredient of our algorithm is 8-dimensional descriptors learned for each point in the cloud, instead of common 3-dimensional RGB colors. Rendering neural network interprets this descriptors and outputs RGB image. We train the network on large [Scannet](http://www.scan-net.org/) dataset to boost it's generalization capabilities on novel scenes.

&amp;#x200B;

For more details please refer to the paper, as well as short description of the method on the project page and video demonstrating the results.

&amp;#x200B;

Paper: [https://arxiv.org/abs/1906.08240](https://arxiv.org/abs/1906.08240)

Project page: [https://dmitryulyanov.github.io/neural\_point\_based\_graphics](https://dmitryulyanov.github.io/neural_point_based_graphics)

Video: [https://youtu.be/7s3BYGok7wU](https://youtu.be/7s3BYGok7wU)

*Processing video pfrd1enboac31...*",59,380
1646,2019-7-25,2019,7,25,3,chc792,GPU vs CPU,https://www.reddit.com/r/MachineLearning/comments/chc792/gpu_vs_cpu/,arivar,1563993942,[removed],0,1
1647,2019-7-25,2019,7,25,3,chca01,"Apart from ML, what skills do Data Scientists need today?",https://www.reddit.com/r/MachineLearning/comments/chca01/apart_from_ml_what_skills_do_data_scientists_need/,Lexandrit,1563994322,[removed],0,1
1648,2019-7-25,2019,7,25,3,chcckj,Has BERT Been Cheating? Researchers Say it Exploits Spurious Statistical Cues,https://www.reddit.com/r/MachineLearning/comments/chcckj/has_bert_been_cheating_researchers_say_it/,Yuqing7,1563994649,,0,2
1649,2019-7-25,2019,7,25,4,chcepk,"[R] Apart from ML, what skills do Data Scientists need today?",https://www.reddit.com/r/MachineLearning/comments/chcepk/r_apart_from_ml_what_skills_do_data_scientists/,Lexandrit,1563994926,"To find this out, we looked at 300 DS vacancies from StackOverflow, AngelList, Dice, and similar websites. [Here's what](https://cvcompiler.com/blog/how-to-become-more-marketable-as-a-data-scientist/) the data  and experts like Carla Gentry, Andriy Burkov, Dr. Ganapathi Pulipaka, and Lon Riesberg  say.

Surprising or expected?",2,4
1650,2019-7-25,2019,7,25,4,chchur,Final Year Project,https://www.reddit.com/r/MachineLearning/comments/chchur/final_year_project/,linear_learner,1563995315,I am a final year CS engineering student. I wanna do my final year project under the domain of Machine Learning but can't find any good idea to build upon. If I could get some basis or a basic idea on which I can learn &amp; then implement it. Not looking for very tough ideas just basic or moderate level.,0,1
1651,2019-7-25,2019,7,25,4,chcl2j,Artificial Intelligence: What is it and how does it work?,https://www.reddit.com/r/MachineLearning/comments/chcl2j/artificial_intelligence_what_is_it_and_how_does/,gabymado,1563995740,,0,1
1652,2019-7-25,2019,7,25,4,chcm06,[R] Association Rule Mining,https://www.reddit.com/r/MachineLearning/comments/chcm06/r_association_rule_mining/,gursi1,1563995865,"How to find if a table with more than a million records contains a rule (if A=... then B=C) where A, B , C are column names and if the rule is present then which columns satisfy this condition.",0,1
1653,2019-7-25,2019,7,25,4,chcqyq,[P] Association Rule Mining,https://www.reddit.com/r/MachineLearning/comments/chcqyq/p_association_rule_mining/,gursi1,1563996509,"How to find if a table with more than a million records contains a rule (if A=... then B=C) where A, B , C are column names and if the rule is present then which columns satisfy this condition.",9,0
1654,2019-7-25,2019,7,25,5,chd5ez,[D] Simple regression/machine learning app for human problem diagnostics,https://www.reddit.com/r/MachineLearning/comments/chd5ez/d_simple_regressionmachine_learning_app_for_human/,epwik,1563998413,"I know that there are a bunch of simple self help/mental health apps in which you input your ""data"", such as, how you are feeling that day, what you have eaten, what you have done and then highlights things which supposedly maybe makes you feel worse/anxious/good/happy or what not. Is there any app or something which uses regression analysis/machine learning which makes better correlations between input data and visualize them? I at least imagine that using machine learning, you could make a lot better personalized assumptions about which things are important to your well being and which things make you feel x way (of course, it also depends on the quality of input data, but i think it would still be a lot better than these simple apps)  


P.S. I know that i could have written this better, but i am quite tired and will go to sleep soon and want to ask this here before i have forgotten about it lol",0,1
1655,2019-7-25,2019,7,25,5,chddst,[R] Can you use machine learning to predict Tsunamis?,https://www.reddit.com/r/MachineLearning/comments/chddst/r_can_you_use_machine_learning_to_predict_tsunamis/,iNeedSleep-,1563999487,Im not sure if this is the right place to ask this. What are your views on this? Thank you,6,0
1656,2019-7-25,2019,7,25,6,che0nc,[D] Can someone help me understand the latent encoding space of a variational autoencoder?,https://www.reddit.com/r/MachineLearning/comments/che0nc/d_can_someone_help_me_understand_the_latent/,that_one_ai_nerd,1564002414,"So I trained a VAE on 1-D Sparse data, and I am attempting to use the encoded latent variables for a similarity metric. However, the latent space has an extra dimension that I have no idea where it came from, and I am not sure which variable to use. I am attempting to use z\_mean as my latent variables. But the shape of the output from the z\_mean layer is somehow (8\*512), even though my latent size was 512. Can someone help me understand what is going on here? Thank you!",0,0
1657,2019-7-25,2019,7,25,7,cheq5w,[P] Lock Picking Detection Using Machine Learning - Audio Classification,https://www.reddit.com/r/MachineLearning/comments/cheq5w/p_lock_picking_detection_using_machine_learning/,NNFAK,1564005607,"I thought you guys might find this interesting. I slightly modified an image classifier to take audio, then recorded myself sticking keys and picks into locks for 45 minutes respectively. This was in order to create my dataset. I broke those long clips into 5 second segments, which left me with about 1000 clips for training. After 5 minutes of training and 15 epochs, I achieved a little more than 90% accuracy on my training and validation set, which is good enough for a fun project like this.

What this means is that I can put my microphone next to a lock, then detect in live time whether that lock is being picked or if a key is being inserted. I can then record the time the event happened and save the audio clip that triggered the event.


[Here is a video demonstration of the project.](https://www.youtube.com/watch?v=s5ePte2AE-g)


[Here is an article I wrote where you can see the code.](https://evigio.com/post/lock-picking-detection-with-machine-learning-audio-classification)


For anyone thats into lock picking, I created my training data on Sargent, Corbin Russwin, and Schlage mortise cylinders. I used both single pin picking and raking. I might play around with bumping in the future, if I come back to the project.",9,3
1658,2019-7-25,2019,7,25,7,cheu76,[D] What does it mean for AI/ML to outperform human benchmarks?,https://www.reddit.com/r/MachineLearning/comments/cheu76/d_what_does_it_mean_for_aiml_to_outperform_human/,techczech,1564006129," 

This [post argues](http://metaphorhacker.net/2019/07/turing-tests-in-chinese-rooms-what-does-it-mean-for-ai-to-outperform-humans/) that beating human benchmarks on tests do not mean that the AI is actually better at the task. This seems to go well with recent controversies around [BERT](https://medium.com/syncedreview/has-bert-been-cheating-researchers-say-it-exploits-spurious-statistical-cues-b256760ded57) but is more fundamental than that.

**Core argument (details and evidence in** [post](http://metaphorhacker.net/2019/07/turing-tests-in-chinese-rooms-what-does-it-mean-for-ai-to-outperform-humans/)**):**

* Reports that AI beat humans on certain benchmarks or very specialised tasks dont mean that AI is actually better at those tasks than any individual human.
* They certainly dont mean that AI is approaching the task with any of the same understanding of the world people do.
* People actually perform 100% on the tasks when administered individually under ideal conditions (no distraction, typical cognitive development, enough time, etc.) They will start making errors only if we give them too many tasks in too short a time.
* This means that just adding more of these results will NOT cumulatively approach general human cognition.
* But it may mean that AI can replace people on certain tasks that were previously mistakenly thought to require general human intelligence.
* All tests of artificial intelligence suffer from Goodharts law.
* A test more closely resembling an internship or an apprenticeship than a gameshow may be a more effective version of the Imitation Game.
* Worries about superintelligence are very likely to be irrelevant because they are based on an unproven notion of arbitrary scalability of intelligence and ignore limits on computability.",11,0
1659,2019-7-25,2019,7,25,7,chf4sb,Machine Learning Poised to Impact Business Analytics,https://www.reddit.com/r/MachineLearning/comments/chf4sb/machine_learning_poised_to_impact_business/,andrea_manero,1564007536,[removed],0,1
1660,2019-7-25,2019,7,25,7,chfb05,Machine Learning Subreddits,https://www.reddit.com/r/MachineLearning/comments/chfb05/machine_learning_subreddits/,antaloaalonso,1564008383,[removed],0,1
1661,2019-7-25,2019,7,25,7,chfccy,"This article on artificial bias poses an interesting question, ""Are we making Artificial Intelligence inefficient by molding it to human thinking?"" Thoughts?",https://www.reddit.com/r/MachineLearning/comments/chfccy/this_article_on_artificial_bias_poses_an/,inkedlj,1564008581,[removed],0,1
1662,2019-7-25,2019,7,25,8,chfpq6,[R] Winning Solution from the Unity Reinforcement Learning Challenge,https://www.reddit.com/r/MachineLearning/comments/chfpq6/r_winning_solution_from_the_unity_reinforcement/,MasterScrat,1564010471,,1,1
1663,2019-7-25,2019,7,25,8,chg3lu,Is there a study order(curriculum) for RNN(time series)?,https://www.reddit.com/r/MachineLearning/comments/chg3lu/is_there_a_study_ordercurriculum_for_rnntime/,GoBacksIn,1564012498,"Hello... may I question something?

&amp;#x200B;

I am a student studying at a foreign university lab.

&amp;#x200B;

I am studying stock price forecasting.

&amp;#x200B;

Adjusting the LSTM unit and layer is complete.

&amp;#x200B;

The topics(plan) of the next research were auto-keras or NLP.

&amp;#x200B;

But the professor said that the topic was too broad.

&amp;#x200B;

he want Step by step to build up my capabilities.

&amp;#x200B;

So I looked for all the paper related to stock predict.

&amp;#x200B;

Recent research were based on the mechanism of attention or the process of using natural language news.

&amp;#x200B;

The following research themes should be decided, but the gap between basic research and the latest research is too large.

&amp;#x200B;

I do not know what to make the next research topic.

&amp;#x200B;

do you have any curriculum for forecasting time series data?  


I do not know how to get directions

  
I'd like to ask your opinion.",0,1
1664,2019-7-25,2019,7,25,8,chg4pl,How practical is it to get some experience in Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/chg4pl/how_practical_is_it_to_get_some_experience_in/,toiletscrubber,1564012667,[removed],0,1
1665,2019-7-25,2019,7,25,11,chhg3j,[R] Our source codes for attention-based dropout layer for weakly supervised object localization (CVPR 2019 Oral),https://www.reddit.com/r/MachineLearning/comments/chhg3j/r_our_source_codes_for_attentionbased_dropout/,junsukchoe,1564020047,"code: [https://github.com/junsukchoe/ADL](https://github.com/junsukchoe/ADL)

video: [https://youtu.be/azevl9\_w2BE](https://youtu.be/azevl9_w2BE?fbclid=IwAR2m_pulUFWhCpBksOJ5FeVz4BkcpVUzbkbTusP26znX7W9orS40SopXaPI)

paper: [http://openaccess.thecvf.com/content\_CVPR\_2019/papers/Choe\_Attention-Based\_Dropout\_Layer\_for\_Weakly\_Supervised\_Object\_Localization\_CVPR\_2019\_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Choe_Attention-Based_Dropout_Layer_for_Weakly_Supervised_Object_Localization_CVPR_2019_paper.pdf)

: We are glad to share our Tensorflow and Pytorch implementations for ""Attention-based Dropout Layer for Weakly Supervised Object Localization. CVPR 2019."" Although this repository is still in progress, we believe that the current version can also help researchers and practitioners in this field. More detailed instructions will be released soon. Contributions and stars are welcome!

&amp;#x200B;

**Abstract**

&gt;Weakly Supervised Object Localization (WSOL) techniques learn the object location only using image-level labels, without location annotations. A common limitation for these techniques is that they cover only the most discriminative part of the object, not the entire object. To address this problem, we propose an Attention-based Dropout Layer (ADL), which utilizes the self-attention mechanism to process the feature maps of the model. The proposed method is composed of two key components: 1) hiding the most discriminative part from the model for capturing the integral extent of object, and 2) highlighting the informative region for improving the recognition power of the model. Based on extensive experiments, we demonstrate that the proposed method is effective to improve the accuracy of WSOL, achieving a new state-of-the-art localization accuracy in CUB-200-2011 dataset. We also show that the proposed method is much more efficient in terms of both parameter and computation overheads than existing techniques.",0,6
1666,2019-7-25,2019,7,25,11,chhrnv,[D] Using machine learning to predict video game review scores.,https://www.reddit.com/r/MachineLearning/comments/chhrnv/d_using_machine_learning_to_predict_video_game/,elitefusion,1564021850,"I'm not sure this is the right place for this, but hopefully someone can point me in the right direction if not. I run a site called Fantasy Critic. It's basically fantasy football for video games, where you ""draft"" a ""team"" of video games and compete for the best review scores. On the site, I maintain a list of ""master games"" that players can select from. You can see that here:

https://www.fantasycritic.games/games

I have a stat that I calculate, called ""Hype Factor"", that attempts to calculate how excited people are for a game. Right now, it's a fairly simple calculation based on how frequently a game is drafted and how early in the draft the game is taken. However, I think my calculation is a little flawed and I'm wondering if machine learning would be a better choice here.

What I'm looking to do is take the various stats that I can gather from the site and use those to predict the critic score (from OpenCritic.com) that the game will get. I have a sample dataset here:

https://www.dropbox.com/s/6vc3dwltbncm8ev/FantasyCriticMasterGameData.csv?dl=0

So there's a lot complexites that would go into doing like this I'm sure. One of my concerns is that I may not have enough data here to be able to generate a model. I'm open to any thoughts or advice that people have, and I can expand on anything that's unclear. Additionally, if anyone feels like they have a decent shot at making this work, I'm open to paying someone to help me with this.

Thanks!",4,7
1667,2019-7-25,2019,7,25,12,chi4y4,Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/chi4y4/machine_learning_course/,dwivediabhinav,1564023975,,0,1
1668,2019-7-25,2019,7,25,12,chiizi,Object tracking without deep learning,https://www.reddit.com/r/MachineLearning/comments/chiizi/object_tracking_without_deep_learning/,Wheeinu,1564026313,[removed],0,1
1669,2019-7-25,2019,7,25,14,chjgxh,Intelligence Simplified - Engati,https://www.reddit.com/r/MachineLearning/comments/chjgxh/intelligence_simplified_engati/,getengati,1564032338,[removed],0,1
1670,2019-7-25,2019,7,25,14,chjmxf,automatic wet glue label applicator manufacturer for plastic bottle,https://www.reddit.com/r/MachineLearning/comments/chjmxf/automatic_wet_glue_label_applicator_manufacturer/,YQPACK,1564033500,,0,1
1671,2019-7-25,2019,7,25,15,chk109,[R] SpanBERT: Improving Pre-training by Representing and Predicting Spans,https://www.reddit.com/r/MachineLearning/comments/chk109/r_spanbert_improving_pretraining_by_representing/,ofirpress,1564036259,,11,11
1672,2019-7-25,2019,7,25,15,chk2ho,How to use deep learning to track custom objects in a Video,https://www.reddit.com/r/MachineLearning/comments/chk2ho/how_to_use_deep_learning_to_track_custom_objects/,manneshiva,1564036573,[removed],0,1
1673,2019-7-25,2019,7,25,15,chk9ad,How Can You Find The Best Machine Learning Frameworks?,https://www.reddit.com/r/MachineLearning/comments/chk9ad/how_can_you_find_the_best_machine_learning/,andrea_manero,1564037917,[removed],0,1
1674,2019-7-25,2019,7,25,16,chkf1y,NeurIPS reviews,https://www.reddit.com/r/MachineLearning/comments/chkf1y/neurips_reviews/,ua82,1564039063,[removed],0,1
1675,2019-7-25,2019,7,25,16,chkgc3,Facebook Login using Python,https://www.reddit.com/r/MachineLearning/comments/chkgc3/facebook_login_using_python/,subhamroy021,1564039309,,0,1
1676,2019-7-25,2019,7,25,16,chknk6,Stirlingkit - Chinese Hit &amp; Miss Gas Model Engine,https://www.reddit.com/r/MachineLearning/comments/chknk6/stirlingkit_chinese_hit_miss_gas_model_engine/,stirlingkit,1564040818,,0,1
1677,2019-7-25,2019,7,25,17,chks67,Machine Learning Engineer vs Data Scientist: Whats The Difference?,https://www.reddit.com/r/MachineLearning/comments/chks67/machine_learning_engineer_vs_data_scientist_whats/,Celadon_soft,1564041792,,1,2
1678,2019-7-25,2019,7,25,17,chkzmc,[D] Help on understanding MobileNetV2 Research Paper,https://www.reddit.com/r/MachineLearning/comments/chkzmc/d_help_on_understanding_mobilenetv2_research_paper/,DinoHustler,1564043433,"I am having trouble understanding a section from the [MobileNetV2 paper](https://arxiv.org/pdf/1801.04381.pdf). 

In particular, section 3.2 Linear Bottlenecks, authors talk about how ""it is easy to see that in general if a result of a layer transformation ReLU(Bx) has a non-zero volume S, the points mapped to interior S are obtained via a linear transformation B of the input, thus indicating that the part of the input space corresponding to the full dimensional output is limited to a linear transformation."" Is there a simpler way of explaining this? 

Can I check my understanding, that this volume S is the volume created by the output tensor of ReLU(Bx), which each ""pixel value"" is a multi-dimensional vector and a point in the subspace, and all of these points form a volume? And if so, it is not clear to me why the interior points have any relevance to the argument.",2,14
1679,2019-7-25,2019,7,25,17,chl08v,Any available table recognition dataset?,https://www.reddit.com/r/MachineLearning/comments/chl08v/any_available_table_recognition_dataset/,dlccpr,1564043586,[removed],0,1
1680,2019-7-25,2019,7,25,17,chl4cs,[D] Effective Management of your Machine Learning Laboratory,https://www.reddit.com/r/MachineLearning/comments/chl4cs/d_effective_management_of_your_machine_learning/,thumbsdrivesmecrazy,1564044541,"The author shortly demonstrate how you can easily make use of DVC tool to effectively manage your ML workflow: [Effective Management of your Machine Learning Laboratory](https://www.linkedin.com/pulse/effective-management-your-machine-learning-laboratory-ulaganathan/)

An example project (from the article) in the author's personal [GitHub](https://github.com/selvaHome/MLPipelineDVC)
The following common ML workflow issues are dealt with in the article:

* how to connect versions of source codes and versions of large data files?
* how to recover model from weeks earlier without retraining it?
* how to run only model inferencing using a model that I built weeks ago?
* how to keep a track of model parameters of various ML experiments?",2,0
1681,2019-7-25,2019,7,25,17,chl5rk,"""On the Road to Artificial General Intelligence"" with Danny Lange",https://www.reddit.com/r/MachineLearning/comments/chl5rk/on_the_road_to_artificial_general_intelligence/,goto-con,1564044862,[removed],0,1
1682,2019-7-25,2019,7,25,17,chl6em,"""On the Road to Artificial General Intelligence"" with Danny Lange [N]",https://www.reddit.com/r/MachineLearning/comments/chl6em/on_the_road_to_artificial_general_intelligence/,goto-con,1564045010,"Danny Lange discusses the role of intelligence in biological evolution and learning. He demonstrates why a game engine is the perfect virtual biodome for AIs evolution.  
You will recognize how the scale and speed of simulations is changing the game of AI while learning about new developments in reinforcement learning.

* [Video](https://youtu.be/sRyZ-XwmgnE)
* [Slides](https://gotocph.com/2018/sessions/571)",0,0
1683,2019-7-25,2019,7,25,18,chl7ro,Free machine learning datasets,https://www.reddit.com/r/MachineLearning/comments/chl7ro/free_machine_learning_datasets/,annafromduomly,1564045311,,0,1
1684,2019-7-25,2019,7,25,18,chl7xy,450/10206 - PLANET GEAR,https://www.reddit.com/r/MachineLearning/comments/chl7xy/45010206_planet_gear/,robotcomponents,1564045348,,0,1
1685,2019-7-25,2019,7,25,18,chl9ii,"machine learning in python, query",https://www.reddit.com/r/MachineLearning/comments/chl9ii/machine_learning_in_python_query/,np497,1564045661,[removed],0,1
1686,2019-7-25,2019,7,25,18,chlaxx,Please critique my ML method.,https://www.reddit.com/r/MachineLearning/comments/chlaxx/please_critique_my_ml_method/,guyshur,1564045978,"Hello all,  
My data points currently consist of 70 attributes of values bwtween 0 and 1 each. The target class is binary. I suspect the relationship between them to be non linear but I'm not certain.  I would like to be able to identify the most weightful attributes in the classification process as well as to have a reliable model. Right now I'm thinking of using an autoencoder to understand which attributes are the important ones and using that output in a model, maybe another NN. Does that sound right or am I just leaning too much in the NN direction because of how my data looks?  
Thank you all in advance",0,1
1687,2019-7-25,2019,7,25,18,chle2s,[D] Please critique my ML method.,https://www.reddit.com/r/MachineLearning/comments/chle2s/d_please_critique_my_ml_method/,guyshur,1564046637,"Hello all,  

My data points consist of 71 attributes, each numeric between 0 and 1, the target class is binary and my belief is the relationship is non linear.  In addition, the attributes have a 1D spacial relation to each other, this may or may not be important.  

I would like to have a model to be able to classify each tuple, but equally as important to me is a way to understand which attributes are the most weightful in the classification.  

I'm thinking I should use an autoencoder for the second part, and to use the output from that as an input for a model, such as another NN.  

Does that sound right, or am I leaning too heavily in the direction of NNs because of how my data looks?  

Thank you all very much in advance.",20,0
1688,2019-7-25,2019,7,25,18,chlem2,How to change the color of the labels after Kmeans segementation?,https://www.reddit.com/r/MachineLearning/comments/chlem2/how_to_change_the_color_of_the_labels_after/,textssg,1564046753,[removed],0,1
1689,2019-7-25,2019,7,25,18,chlfd4,An Interview with a CSV Engineer,https://www.reddit.com/r/MachineLearning/comments/chlfd4/an_interview_with_a_csv_engineer/,rosamarts,1564046926,,0,1
1690,2019-7-25,2019,7,25,18,chll3x,Opensource service for realtime video analysis,https://www.reddit.com/r/MachineLearning/comments/chll3x/opensource_service_for_realtime_video_analysis/,FastoNoSQL,1564048127,,1,1
1691,2019-7-25,2019,7,25,19,chlpvi,Are physical Artificial Neural Nets a thing today?,https://www.reddit.com/r/MachineLearning/comments/chlpvi/are_physical_artificial_neural_nets_a_thing_today/,abdeljalil73,1564049082,[removed],0,1
1692,2019-7-25,2019,7,25,19,chm065,[D] Why ML community so negatively opposed to philosophy in machine intelligence?,https://www.reddit.com/r/MachineLearning/comments/chm065/d_why_ml_community_so_negatively_opposed_to/,postmachines,1564051143,"A few days ago was published a paper on the topic **""What does it mean to understand a neural network?""**. 

[https://arxiv.org/abs/1907.06374](https://arxiv.org/abs/1907.06374)

This article was neglected on reddit. 

While some trash like ""**Decomposing latent space to generate custom anime girls""** was discussed very active 

What's wrong with you?",58,0
1693,2019-7-25,2019,7,25,19,chm47j,RTX 2080 Super vs 1080Ti for ML/AI,https://www.reddit.com/r/MachineLearning/comments/chm47j/rtx_2080_super_vs_1080ti_for_mlai/,fucilator_3000,1564051910,[removed],0,1
1694,2019-7-25,2019,7,25,20,chm8gj,[D] Are 'ML/AI Website builders' actually using ML or are they using algorithms?,https://www.reddit.com/r/MachineLearning/comments/chm8gj/d_are_mlai_website_builders_actually_using_ml_or/,dothrage,1564052704,"A bunch of website builders claim that they can use AI to make a great website for you. Personally, I feel like they are actually using some simpler algorithms rather than ML and are using the term 'Artificial Intelligence' to make it sound all fancy and advanced. 

To what extent are the claims of these websites true?",1,4
1695,2019-7-25,2019,7,25,20,chm9n3,"Business-aligned organizations are more likely to spend money on technologies such as artificial intelligence (AI), predictive analytics, service automation, and virtual agents, and far less likely to have no plans.",https://www.reddit.com/r/MachineLearning/comments/chm9n3/businessaligned_organizations_are_more_likely_to/,bpriya,1564052925,,0,1
1696,2019-7-25,2019,7,25,20,chmgvg,"GitHub - c-bata/goptuna: Black-box optimization library written in pure Go, inspired by Optuna.",https://www.reddit.com/r/MachineLearning/comments/chmgvg/github_cbatagoptuna_blackbox_optimization_library/,c-bata,1564054254,,0,1
1697,2019-7-25,2019,7,25,21,chmyy1,[D] Good method for low dimensional density modelling/estimation?,https://www.reddit.com/r/MachineLearning/comments/chmyy1/d_good_method_for_low_dimensional_density/,jarekduda,1564057411,"It is convenient to normalize each variable to uniform distribution on [0,1], getting distribution on [0,1]^d, which distortion from uniform rho=1 describes statistical dependencies between variables. 

The main question here is: **how to model density on [0,1]^d for a few variables?** Example of 2D case described below:
https://i.imgur.com/6hUJ6Bz.png

A standard choice are [copulas](https://en.wikipedia.org/wiki/Copula_\(probability_theory\)), but they are usually single parameter  work only for very specific distributions. Are there copulas with a larger number of parameters?

Another standard choice is [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation) - while it works well in 1D, it is often said that it nearly does not work above ~5 dim (e.g. Gaussian distribution becomes localized on a sphere), even in 2D it is very poor for **generalization** as in the example - just localizes density in the old points instead of searching for trends.

Modeling with [linear combinations](https://www.dropbox.com/s/7u6f2zpreph6j8o/rapid.pdf) like polynomials nicely generalizes, decomposing distribution into mixed moments.

The example is diagram for (x_t, x_{t-1}) pairs from 10 years daily log-returns of TRV normalized using [generalized normal distribution](https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1) ([Mathematica file with points and calculation](https://www.dropbox.com/s/8gwyukm31s3sf62/trv1.nb)) - linear combinations can get essential improvement over rho=1, but KDE barely exceeds it.

What other interesting methods are there? E.g. using neural networks and still maintaining normalization to 1?",0,3
1698,2019-7-25,2019,7,25,22,chnjbu,Data Synthesizers on AWS SageMaker: An Adversarial Gaussian Mixture Model vs XGBoost Architecture,https://www.reddit.com/r/MachineLearning/comments/chnjbu/data_synthesizers_on_aws_sagemaker_an_adversarial/,_orcaman,1564060652,,0,1
1699,2019-7-25,2019,7,25,22,chnyu9,150+ Business Machine Learning and Data Science Applications in Python,https://www.reddit.com/r/MachineLearning/comments/chnyu9/150_business_machine_learning_and_data_science/,OppositeMidnight,1564062961,,0,1
1700,2019-7-25,2019,7,25,22,chnz5h,My thoughts on machine learning subreddits,https://www.reddit.com/r/MachineLearning/comments/chnz5h/my_thoughts_on_machine_learning_subreddits/,antaloaalonso,1564063009,"I have noticed that on many of the ML subreddits, there is a wide variety of libraries and tools used. For the experienced programmer, this may be okay or even preferable. However, if you are like the majority of ML programmers, then this can be intimidating, confusing, and frustrating. For those of you that fall in this category, I would like to invite you to a subreddit ([r/MachineLearningKeras](https://www.reddit.com/r/MachineLearningKeras/)) that will be focused on machine learning with the Keras API. Keras is easy to use, and is a great way to implement various projects. I hope that you will join me in making such a community on Reddit.",0,1
1701,2019-7-25,2019,7,25,22,chnzp8,[P] 150+ Business Machine Learning and Data Science Applications in Python,https://www.reddit.com/r/MachineLearning/comments/chnzp8/p_150_business_machine_learning_and_data_science/,OppositeMidnight,1564063083,,0,1
1702,2019-7-25,2019,7,25,22,chnzw5,[Model assessment] Random forest and residual plots,https://www.reddit.com/r/MachineLearning/comments/chnzw5/model_assessment_random_forest_and_residual_plots/,imaginati,1564063109,[removed],0,1
1703,2019-7-25,2019,7,25,23,cho1mo,"Is it ethical or acceptable if I slightly modify an online tutorial in machine learning, test it on other kind of data, and put it in my GitHib and CV?",https://www.reddit.com/r/MachineLearning/comments/cho1mo/is_it_ethical_or_acceptable_if_i_slightly_modify/,van1van,1564063332,,1,1
1704,2019-7-25,2019,7,25,23,cho3uf, How evolutionary selection can train more capable self-driving cars,https://www.reddit.com/r/MachineLearning/comments/cho3uf/how_evolutionary_selection_can_train_more_capable/,sjoerdapp,1564063596,,0,1
1705,2019-7-25,2019,7,25,23,cho7ow,Neurips 2019 Reviews are uut!,https://www.reddit.com/r/MachineLearning/comments/cho7ow/neurips_2019_reviews_are_uut/,Iamabandit,1564064079,[removed],0,1
1706,2019-7-25,2019,7,25,23,cho9qz,"what's ""multi armed bandit""?",https://www.reddit.com/r/MachineLearning/comments/cho9qz/whats_multi_armed_bandit/,adeeplearner,1564064354,[removed],0,1
1707,2019-7-25,2019,7,25,23,chodtm,[D] How does word2vec model encodes similarity,https://www.reddit.com/r/MachineLearning/comments/chodtm/d_how_does_word2vec_model_encodes_similarity/,cuenta4384,1564064874,"I am confused about why word2vec learns similarities or how does it learn?

In word2vec, in the skipgram model, we want to maximize the p(context | word), let's say I have the following sentence:

1. I want a job
2. I want a cake

Most pair of words are gonna be similar, except for the following ones:  
Words for ""job"" in a window size=2

(job, a)  
(job, want)  


Words for ""cake"" in a window size=2

(cake, a)  
(cake, want)

  
Since the structure of the sentence is the same except the nouns job and cake, would the skipgram model learn that job is similar to cake?  


Does sentence structure affect the quality of embeddings? I guess this is one of the reasons that these models need to be trained in a huge corpus.   
The model is only learning the surrounding words, how can it say, as stated in the original paper, that queen is similar to king? Is it because both have the same surrounding words?",13,26
1708,2019-7-25,2019,7,25,23,choj26,[1907.10597] Green AI,https://www.reddit.com/r/MachineLearning/comments/choj26/190710597_green_ai/,zhamisen,1564065521,,63,206
1709,2019-7-25,2019,7,25,23,chomf5,[N] NeurIPS reviews are out,https://www.reddit.com/r/MachineLearning/comments/chomf5/n_neurips_reviews_are_out/,gohu_cd,1564065933,"How did you do ? 

&amp;#x200B;

7 7 3 (confidence 4, 4, 3) for me. Guess I'll have to convince that 3 :)",0,1
1710,2019-7-25,2019,7,25,23,chot9m,[D] Looking for a platform I saw to store models results,https://www.reddit.com/r/MachineLearning/comments/chot9m/d_looking_for_a_platform_i_saw_to_store_models/,Dav05,1564066771,"Hi guys,

Fairly simple question here, I am drawing a blank and can't find the name of a platform I saw a few days ago (maybe I should post this in TOMT instead!) that was made to store and share your models. I just saw a screenshot of it so the info I have is _very_ limited but now that I want to explore further I realize I forgot to bookmark it.   

Basically, it was a web interface showing a summary of your models with name, parameters (eg l2_ratio, alpha) and a few statistics (accuracy or R2). That's all I have...

Hopefully someone knows what I am talking about!

Thank you!",12,6
1711,2019-7-26,2019,7,26,0,choyns,"NeurIPS 2019 review, Inception score should not be used for testing?",https://www.reddit.com/r/MachineLearning/comments/choyns/neurips_2019_review_inception_score_should_not_be/,nile6499,1564067386,[removed],0,1
1712,2019-7-26,2019,7,26,0,chozig,NeurIPS 2019 Reviews are finally out!,https://www.reddit.com/r/MachineLearning/comments/chozig/neurips_2019_reviews_are_finally_out/,romantic_cockroach,1564067483,,0,1
1713,2019-7-26,2019,7,26,0,chp0lj,Interest in PyTorch among researchers is growing extremely rapidly,https://www.reddit.com/r/MachineLearning/comments/chp0lj/interest_in_pytorch_among_researchers_is_growing/,gradientflow,1564067614,,0,1
1714,2019-7-26,2019,7,26,0,chpbwz,Extracting multiple features of the object from the image using deep learning,https://www.reddit.com/r/MachineLearning/comments/chpbwz/extracting_multiple_features_of_the_object_from/,learner_30,1564069039,[removed],0,1
1715,2019-7-26,2019,7,26,0,chpfbm,"AI-generated poetry book - ""The Art of Artificial Poetry""",https://www.reddit.com/r/MachineLearning/comments/chpfbm/aigenerated_poetry_book_the_art_of_artificial/,humanmodetech,1564069468,[removed],0,1
1716,2019-7-26,2019,7,26,0,chpld7,Help RNN LSTM Doesn't generalize well,https://www.reddit.com/r/MachineLearning/comments/chpld7/help_rnn_lstm_doesnt_generalize_well/,jesterden,1564070248,[removed],0,1
1717,2019-7-26,2019,7,26,1,chpvc1,Classification: Having the NN know that it doesn't know,https://www.reddit.com/r/MachineLearning/comments/chpvc1/classification_having_the_nn_know_that_it_doesnt/,JuicyRacoonAnus,1564071518,[removed],0,1
1718,2019-7-26,2019,7,26,1,chpx58,Product Question,https://www.reddit.com/r/MachineLearning/comments/chpx58/product_question/,CircularRef,1564071768,[removed],0,1
1719,2019-7-26,2019,7,26,1,chpxe4,"RNN (LSTMs) Units, Nodes, Time steps?",https://www.reddit.com/r/MachineLearning/comments/chpxe4/rnn_lstms_units_nodes_time_steps/,kekomat11,1564071796,[removed],0,1
1720,2019-7-26,2019,7,26,1,chpxy1,How to extract entity of most importance from user-support conversations?,https://www.reddit.com/r/MachineLearning/comments/chpxy1/how_to_extract_entity_of_most_importance_from/,radjeep,1564071862,[removed],0,1
1721,2019-7-26,2019,7,26,1,chq2zu,[P] How to Reduce Bias in AI,https://www.reddit.com/r/MachineLearning/comments/chq2zu/p_how_to_reduce_bias_in_ai/,hszafarek,1564072508,,0,1
1722,2019-7-26,2019,7,26,1,chq8pk,[D] Classification: Having the NN know when it doesn't know,https://www.reddit.com/r/MachineLearning/comments/chq8pk/d_classification_having_the_nn_know_when_it/,JuicyRacoonAnus,1564073268,"So I'm working on: 

Building an app to classify animals for the visually impaired. Users have an app where they can take a picture and get the name of the animal. If the camera is being pointed somewhere with no animal, it should predict ""No Animal"". But ALSO, if the camera points at an animal that I don't have in my dataset, I'd like it to predict ""Unrecognized Animal"" so I can store the frame and the manually tag it and feed back to my training set.

Here's what I'm thinking:

- On Data: Have varied images with no animals in them that the network should predict as ""No animal"". 
Take a number of species and have them as ""Unrecognized Animal"" so the network **learns what it doesn't know** (the truth label would be [Animal = 1, Recognized = 0, 0, 0, 0, 0...] vs. the recognized animals e.g. [Animal = 1, Recognized = 1, 0, 0, 1, 0...]). I know the normal approach would be to decide ""Unrecognized"" based on a threshold of the max predicted confidence, but several [papers](https://arxiv.org/pdf/1706.04599.pdf) and empirical evidence show how overconfident nets can be...?). I'm not too sure.

- On loss function: I was just going to use cross entropy for each of the three terms (Animal/No Animal, Recognized/Not Recognized, Animal classification) and have them weighted.

Is my approach in the right direction? I don't know how to express this problem well enough to find good results on google but this must have been solved before right?

Thanks to any ideas!",31,16
1723,2019-7-26,2019,7,26,2,chqet8,DeepMind is helping Waymo evolve better self-driving AI algorithms,https://www.reddit.com/r/MachineLearning/comments/chqet8/deepmind_is_helping_waymo_evolve_better/,doireallyneedone11,1564074049,,0,1
1724,2019-7-26,2019,7,26,2,chqhz0,Machine learning algorithms explained,https://www.reddit.com/r/MachineLearning/comments/chqhz0/machine_learning_algorithms_explained/,key_info,1564074446,,0,1
1725,2019-7-26,2019,7,26,2,chqmb3,Winning Gold Medal on Kaggle Competition using just kernels: Interview with Ryan Chesler,https://www.reddit.com/r/MachineLearning/comments/chqmb3/winning_gold_medal_on_kaggle_competition_using/,init__27,1564075004,"To everyone who asks the question if free cloud resources are good enough for Kaggle Competitions, in this Interview with Kaggle Master Ryan Chesler, about his Gold Winning Solution to the Jigsaw ""Unintended Bias in Toxicity Classification"" Kaggle Competition, Ryan mentions their team did almost all of the workflow on JUST Kaggle kernels!  

Video: [https://www.youtube.com/playlist?list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1](https://www.youtube.com/playlist?list=PLyMom0n-MBrrGL8w-nD9kc2q5dOwN7Hb1)

&amp;#x200B;

Podcast: [https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Master--ML-Engineer-Ryan-Chesler--Chai-Time-Data-Science-e4ntbt/a-ajj3f5](https://anchor.fm/chaitimedatascience/episodes/Interview-with-Kaggle-Master--ML-Engineer-Ryan-Chesler--Chai-Time-Data-Science-e4ntbt/a-ajj3f5)",0,1
1726,2019-7-26,2019,7,26,2,chqxbu,[D] Which is the better way to perform face recognition?,https://www.reddit.com/r/MachineLearning/comments/chqxbu/d_which_is_the_better_way_to_perform_face/,Eoncarry,1564076420,"I want to use the [MS Celeb](https://megapixels.cc/datasets/msceleb/) dataset (100,000 classes) to label images of various celebrities. I was thinking of 2 ways to do this.

1. Train a new face recognition model from scratch on this dataset with 100,000 classes as output.
2. Use a pretrained model like [face\_recognition library](https://github.com/ageitgey/face_recognition) to get the encodings of a face and compare it (L2 distance) with the encodings of all the 100,000 classes. The class whose encodings are closest to this face, will be the face's class.

I don't know if this 'comparing' method will work well with 100,000 classes (the encodings are of 128 dimensions). What do you guys think will be more feasible?",7,7
1727,2019-7-26,2019,7,26,2,chr2d5,AI that learns to play a home brewed version of baseball,https://www.reddit.com/r/MachineLearning/comments/chr2d5/ai_that_learns_to_play_a_home_brewed_version_of/,ahadcove,1564077066,[removed],0,3
1728,2019-7-26,2019,7,26,4,chrz5y,Fine Tuning Neural Nets,https://www.reddit.com/r/MachineLearning/comments/chrz5y/fine_tuning_neural_nets/,silizannek,1564081299,,0,1
1729,2019-7-26,2019,7,26,4,chsedy,[D] PyTorch/ONNX to Numpy/Autograd,https://www.reddit.com/r/MachineLearning/comments/chsedy/d_pytorchonnx_to_numpyautograd/,neanderthal_math,1564083315,"Does anyone know if there is a tool that can take a model from PyTorch or ONNX and port the model over to numpy or autograd? 

&amp;#x200B;

I'm computing the Jacobian in PyTorch right now and [I have reason to believe that it would be much faster in numpy](https://medium.com/unit8-machine-learning-publication/computing-the-jacobian-matrix-of-a-neural-network-in-python-4f162e5db180).",7,1
1730,2019-7-26,2019,7,26,4,chsnwx,[D] Looking for insights in production ML frameworks for standalone applications/libraries,https://www.reddit.com/r/MachineLearning/comments/chsnwx/d_looking_for_insights_in_production_ml/,troop357,1564084592,"Hello /r/MachineLearning,

This might be a little different from the usual topic here, but this sub is frequented by very experienced people and I hope I can gain some fruitful insights.

Straight to the point, I am looking for ML frameworks that can be compiled and executed without dependencies (statically build), and ideally in C/C++. Just as an example, I wish to be able to compile a .exe or .dll that realize predictions for a NN that can be used by third parties without worrying about installing anything.

Today I can accomplish this with Dlib, so I make this post looking for alternatives, in order to broaden my options.

I've spent some time in this, although popular libraries such as Tensorflow and Torch have proven to be quite problematic to work on the conditions I personally need.

Anyhow, I hope there are more developers with experience on deploying standalone ML applications/libraries that can chime in and comment. Thanks!",3,1
1731,2019-7-26,2019,7,26,5,chsteg,"Trying to understand the differences between: Microsoft Azure, Watson IBM, Amazon Lex, Ada bot, and Dialogflow",https://www.reddit.com/r/MachineLearning/comments/chsteg/trying_to_understand_the_differences_between/,owltoupe,1564085302,[removed],0,1
1732,2019-7-26,2019,7,26,5,chswhl,YOLO with Google Street View?,https://www.reddit.com/r/MachineLearning/comments/chswhl/yolo_with_google_street_view/,TheOpenSorcerer,1564085718,[removed],0,1
1733,2019-7-26,2019,7,26,5,chszze,Standalone Simple ML Library,https://www.reddit.com/r/MachineLearning/comments/chszze/standalone_simple_ml_library/,hobbers,1564086192,[removed],0,1
1734,2019-7-26,2019,7,26,5,cht34p,Architecture diagrams for ML?,https://www.reddit.com/r/MachineLearning/comments/cht34p/architecture_diagrams_for_ml/,jowanga,1564086606,[removed],0,1
1735,2019-7-26,2019,7,26,5,cht4wh,GPT-2 spreads some truth... Wanna see more?,https://www.reddit.com/r/MachineLearning/comments/cht4wh/gpt2_spreads_some_truth_wanna_see_more/,MishUshakov,1564086844,,1,1
1736,2019-7-26,2019,7,26,5,chtaaq,You'll never believe it's all written by an Algorithm. [GPT-2],https://www.reddit.com/r/MachineLearning/comments/chtaaq/youll_never_believe_its_all_written_by_an/,MishUshakov,1564087527,,0,1
1737,2019-7-26,2019,7,26,5,chtb9x,Architecture diagrams for ML?,https://www.reddit.com/r/MachineLearning/comments/chtb9x/architecture_diagrams_for_ml/,jowanga,1564087651,[removed],0,1
1738,2019-7-26,2019,7,26,6,chtyts,Machine Learning and Data Science Applications in Industry,https://www.reddit.com/r/MachineLearning/comments/chtyts/machine_learning_and_data_science_applications_in/,pmz,1564090851,,0,1
1739,2019-7-26,2019,7,26,6,chu71r,Best way to setup generation of a sprite map,https://www.reddit.com/r/MachineLearning/comments/chu71r/best_way_to_setup_generation_of_a_sprite_map/,travelingt93,1564091981,"I wanted to try to generate new maps for the Pokemon games. Here is what I have done so far.

I have the sprites of the routes from game Pokemon ruby. I wrote a script to split them up into the unique sprites and create a index map for them. So like grass_tile_1 is index 0, and water_tile_8 is index 4, and so on and so forth. I then split it up into a sub maps of 20 x 10 indexes, so not the original sprites, just the index into the sprite map to reduce the amount of information. I then feed these screens into a GANN as a vector of (20, 10, 1). The results I am getting are really bad. The don't respect the bounds of the indexes and as such are unusable. I was wondering if it is because I am setting up the problem incorrectly. Like instead should I be making screens of one hot vectors for which sprite goes in which position on the map? If I did not explain myself the best please ask questions. 

Example of sprites https://www.spriters-resource.com/fullview/8783/",0,1
1740,2019-7-26,2019,7,26,7,chubir,Word2Vec graph is wrong?,https://www.reddit.com/r/MachineLearning/comments/chubir/word2vec_graph_is_wrong/,dude_without_a_clue,1564092599,"I am using a pre-trained python W2V, and pulled data from the web. I am a noob, and I can't seem to find something like this. Is this a common error? 

https://i.redd.it/0jw297pfwic31.png",0,1
1741,2019-7-26,2019,7,26,7,churbh,[D] One simple graphic: Researchers love PyTorch and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/churbh/d_one_simple_graphic_researchers_love_pytorch_and/,programmerChilli,1564094861,,0,1
1742,2019-7-26,2019,7,26,7,chutn3,[D] One simple graphic: Researchers love PyTorch and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/chutn3/d_one_simple_graphic_researchers_love_pytorch_and/,programmerChilli,1564095212,,1,1
1743,2019-7-26,2019,7,26,8,chv62o,Building a new corpus of speech/text transcriptions for people with Down Syndrome. Any guidance is appreciated.,https://www.reddit.com/r/MachineLearning/comments/chv62o/building_a_new_corpus_of_speechtext/,chagen24,1564097066,"The lab I work in is currently collaborating with a company that helps employ people with Down Syndrome, and they expressed a desire to improve the accuracy of speech-to-text systems for the speech impediments of the people they work with. I let them know it would definitely help if they built a corpus of speech files and text transcriptions from the people with Down Syndrome, and they seemed very interested in doing this and releasing it as an open source dataset. I don't have much experience in NLP or audio ML, so I was hoping to reach out to any researchers who might want to collaborate with them and help them design the collection process (i.e. what phrases the subjects should speak, how many phrases to collect from each subject, etc.). 

Any advice on who I could reach out to who might be interested in something like this? Much thanks!",0,1
1744,2019-7-26,2019,7,26,8,chv80f,Cambridge analytica,https://www.reddit.com/r/MachineLearning/comments/chv80f/cambridge_analytica/,jordanpgb,1564097366,[removed],0,1
1745,2019-7-26,2019,7,26,9,chvoue,Cambridge analytica and aggregateIQ,https://www.reddit.com/r/MachineLearning/comments/chvoue/cambridge_analytica_and_aggregateiq/,jordanpgb,1564100025,[removed],0,1
1746,2019-7-26,2019,7,26,10,chwcpf,[D] Pure Theory Behind Support Vector Machines,https://www.reddit.com/r/MachineLearning/comments/chwcpf/d_pure_theory_behind_support_vector_machines/,PartlyShaderly,1564103779,"Hey guys these are my study notes. I post them as a blog. Just for the heck of it. 

[http://partlyshaderly.com/2019/07/26/the-pure-theory-behind-support-vector-machines/](http://partlyshaderly.com/2019/07/26/the-pure-theory-behind-support-vector-machines/)",4,0
1747,2019-7-26,2019,7,26,11,chx5hh,[D] OReilly: One simple graphic: Researchers love PyTorch and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/chx5hh/d_oreilly_one_simple_graphic_researchers_love/,programmerChilli,1564108508,,0,1
1748,2019-7-26,2019,7,26,11,chx7bq,[D] One simple graphic: Researchers love PyTorch and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/chx7bq/d_one_simple_graphic_researchers_love_pytorch_and/,programmerChilli,1564108817,"https://i.imgur.com/NvFAv2v.jpg

Taken from https://www.oreilly.com/ideas/one-simple-graphic-researchers-love-pytorch-and-tensorflow",90,197
1749,2019-7-26,2019,7,26,11,chx8ka,[D] Is there are vector training library/framework that stores the vectors to disk when not in use?,https://www.reddit.com/r/MachineLearning/comments/chx8ka/d_is_there_are_vector_training_libraryframework/,BatmantoshReturns,1564109014,"I am going to be training item embeddings where there are 8-9 figures of items, and that will lead to memory issues to have all those embeddings loaded into memory all at once. 

Since a training step only updates a fraction of the embeddings at a time, I am hoping there's some library or framework that has the vectors stores on the disk until the training step. 

This comes pretty close, https://github.com/plasticityai/magnitude  as the vectors are stored to sqlite database, but there's no way to train the vectors.",3,0
1750,2019-7-26,2019,7,26,12,chxw71,How to design a Neural Network model that combines components of Feedforward and Recurrent features?,https://www.reddit.com/r/MachineLearning/comments/chxw71/how_to_design_a_neural_network_model_that/,real_pinocchio,1564113038,,0,1
1751,2019-7-26,2019,7,26,13,chy2mt,"[Q] NeurIPS reviews just came out, I was given one awful review but the comments asked were either for things which where already in the text, they didn't make sense, or were obnoxious. The reviewer seemed determined to have the submission rejected. What do you recommend doing about this?",https://www.reddit.com/r/MachineLearning/comments/chy2mt/q_neurips_reviews_just_came_out_i_was_given_one/,IDivide0,1564114126,[removed],0,1
1752,2019-7-26,2019,7,26,13,chy6r8,Opensource service for realtime video analysis,https://www.reddit.com/r/MachineLearning/comments/chy6r8/opensource_service_for_realtime_video_analysis/,FastoNoSQL,1564114809,,0,1
1753,2019-7-26,2019,7,26,13,chyi36,DCGANS Tensor Flow Tutorial - Issue,https://www.reddit.com/r/MachineLearning/comments/chyi36/dcgans_tensor_flow_tutorial_issue/,Theotherguy151,1564116888,[removed],0,1
1754,2019-7-26,2019,7,26,14,chykbk,How to perform Machine Learning on Source Code (Implementation!),https://www.reddit.com/r/MachineLearning/comments/chykbk/how_to_perform_machine_learning_on_source_code/,zexstoi,1564117292,,0,1
1755,2019-7-26,2019,7,26,15,chz72i,We are providing help to others in their machine learning journey... Anyone can ask their doubts once enrolled ..Some free coupons also available,https://www.reddit.com/r/MachineLearning/comments/chz72i/we_are_providing_help_to_others_in_their_machine/,arpang87,1564121718,,0,1
1756,2019-7-26,2019,7,26,15,chz77f,How do I network in top computer vision conference for possible PhD position?,https://www.reddit.com/r/MachineLearning/comments/chz77f/how_do_i_network_in_top_computer_vision/,devenml,1564121745,[removed],0,1
1757,2019-7-26,2019,7,26,15,chz79h,[D] NeurIPS reviews,https://www.reddit.com/r/MachineLearning/comments/chz79h/d_neurips_reviews/,IborkedyourGPU,1564121757,"The reviews are out, but I haven't seen any thread about them. Was I lazy in my search, or aren't you people interested in the topic?",7,16
1758,2019-7-26,2019,7,26,15,chz9ib,"Working on building Math Equations OCR, need help with the dataset for character recognition",https://www.reddit.com/r/MachineLearning/comments/chz9ib/working_on_building_math_equations_ocr_need_help/,abhayalekal74,1564122212,[removed],0,1
1759,2019-7-26,2019,7,26,15,chze3c,[D] How agnostic are action recognition neural networks to large variance in sampling frequency of the videos?,https://www.reddit.com/r/MachineLearning/comments/chze3c/d_how_agnostic_are_action_recognition_neural/,dtransposed,1564123141,"Hello everyone!

Usually when we train action recognition algorithms (classifying sequences of frames), the researchers use public datasets such as HMDB-51 or UCF-11. Those datasets are neat and have well known, constant number of fps (30 and 29.97 respectively).

I am working on a project where I want to analyze ""real world"" videos. I am trying to keep the sampling frequency of my input at the constant fps level, but because of different data sources and  imperfect recording devices (frame skipping), the average fps number per video clip differs to some extend. 

Do you know if there are any works (or perhaps somebody has some practical experience with this problem) which can give me a clue of how robust neural networks are  with respect to this issue?

Thank you!",4,1
1760,2019-7-26,2019,7,26,15,chzfl6,Accelerated Computing and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/chzfl6/accelerated_computing_and_deep_learning/,andrea_manero,1564123422,[removed],0,1
1761,2019-7-26,2019,7,26,15,chzily,Five Myths About Machine Learning You Need To Know Today,https://www.reddit.com/r/MachineLearning/comments/chzily/five_myths_about_machine_learning_you_need_to/,andrea_manero,1564124059,[removed],0,1
1762,2019-7-26,2019,7,26,15,chzj60,Machine Learning from scratch with python,https://www.reddit.com/r/MachineLearning/comments/chzj60/machine_learning_from_scratch_with_python/,codingislife496,1564124178,[removed],0,1
1763,2019-7-26,2019,7,26,15,chzk0y,[D] Are there any papers with Normalizing Flow-based generative models that show empirical results on 1d/2d densities?,https://www.reddit.com/r/MachineLearning/comments/chzk0y/d_are_there_any_papers_with_normalizing_flowbased/,SolitaryPenman,1564124363,"All the normalizing flows-based papers I read (NICE, RealNVP, Glow, etc.) show experiments on high dimensional image datasets. I am looking for works that analyze the capacity of NFs to learn simple 1/2d distributions. I am aware of the 2d experiments in [Rezende and Mohamed, 2015] but as far as I understand, for the 2d datasets they train by directly minimizing KL (and do no train using samples) because the analytic inverse of Planar flow does not exist.",9,7
1764,2019-7-26,2019,7,26,17,ci0249,Modified YOLO for landmark detection,https://www.reddit.com/r/MachineLearning/comments/ci0249/modified_yolo_for_landmark_detection/,nikogamulin,1564128277,[removed],0,1
1765,2019-7-26,2019,7,26,17,ci02z8,NIPS rebuttal,https://www.reddit.com/r/MachineLearning/comments/ci02z8/nips_rebuttal/,snookershot,1564128474,[removed],0,1
1766,2019-7-26,2019,7,26,17,ci0d70,Is it possible to have generative models for regression?,https://www.reddit.com/r/MachineLearning/comments/ci0d70/is_it_possible_to_have_generative_models_for/,hechang27,1564130832,[removed],0,1
1767,2019-7-26,2019,7,26,18,ci0nky,D150155 - Case Pin,https://www.reddit.com/r/MachineLearning/comments/ci0nky/d150155_case_pin/,robotcomponents,1564133075,,0,1
1768,2019-7-26,2019,7,26,18,ci0oj6,Only if Google photos provided the VOC PASCAL annotations of our faces to download..,https://www.reddit.com/r/MachineLearning/comments/ci0oj6/only_if_google_photos_provided_the_voc_pascal/,bamwani,1564133282,[removed],0,1
1769,2019-7-26,2019,7,26,18,ci0om5,Artificial Intelligence and Machine Learning - 101,https://www.reddit.com/r/MachineLearning/comments/ci0om5/artificial_intelligence_and_machine_learning_101/,Manish-Zenesys,1564133297,,0,1
1770,2019-7-26,2019,7,26,19,ci17l9,[P] Opinions and scope for ASL fingerspelling project,https://www.reddit.com/r/MachineLearning/comments/ci17l9/p_opinions_and_scope_for_asl_fingerspelling/,Denominator_Zero,1564137225,"Hi,
So I have been working on and developed a model for ASL fimger spelling (not the words, just apphabets). I have an accuracy of 80-90% and can implement it real time. The major advantage is the image processing algorithm I use which allows me to localise the hand anywhere in the frame and so I don't need a ROI box. Also the second advantage is the higher accuracy. The model combines blocks from YOLO and inception networks

So I was wondering if this work is suitable for being published as I have no experience about it. I see many videos on Reddit and online hence I'm not sure. Also if it is, is there any upcoming conference where I can submit to

Thanks",4,1
1771,2019-7-26,2019,7,26,19,ci1f5y,Artificial Intelligence Helps Computers Leap Forward in Reading Arabic,https://www.reddit.com/r/MachineLearning/comments/ci1f5y/artificial_intelligence_helps_computers_leap/,nzawk,1564138782,,0,1
1772,2019-7-26,2019,7,26,20,ci1mf3,"[D] Do you report the best test accuracy, or the last test accuracy?",https://www.reddit.com/r/MachineLearning/comments/ci1mf3/d_do_you_report_the_best_test_accuracy_or_the/,polo555,1564140127,"Granted on most modern datasets like CIFAR10/100/ImageNet we are cheating no matter what because we use the test set as a validation set. However it's important to compare apple to apple. I've always reported last test accuracy, but I'm seeing more and more papers report the best one, which gives them a non-negligible boost.

**Best:** In practice if we don't have a test set we would typically deploy the model that has the best validation accuracy. So here we report ""test accuracy"" but we mean ""validation accuracy of the model we would deploy"".

**Last:** Makes it harder to overfit the validation/test set, and is arguably closer to the real generalization accuracy you would get.

Is there a consensus on best practice?",39,31
1773,2019-7-26,2019,7,26,20,ci1p6w,Time-series anomaly detection with an isolation forest.,https://www.reddit.com/r/MachineLearning/comments/ci1p6w/timeseries_anomaly_detection_with_an_isolation/,Jokerever,1564140679,[removed],0,1
1774,2019-7-26,2019,7,26,20,ci1vsp,Mentoring in machinelearning (Mathematics and Programming),https://www.reddit.com/r/MachineLearning/comments/ci1vsp/mentoring_in_machinelearning_mathematics_and/,programmerjules,1564141883,[removed],0,1
1775,2019-7-26,2019,7,26,21,ci275z,"[D] why the same reinforcement learning algorithm worked for MountainCar, but does not work for LunarLander (and others)",https://www.reddit.com/r/MachineLearning/comments/ci275z/d_why_the_same_reinforcement_learning_algorithm/,ErmJustSaying,1564143831,"Hi Reddit community, I'm currently self-learning/exploring reinforcement learning. I have downloaded a few codes to try out and to get a feel of the code. There is a piece of code \[code A\] about using A3C for CartPole-v0, and it manages to learn very well. And another piece of code \[code B\] that uses DQN for LunarLander-v2, it managed to train a smart agent too. 

Then I change the environment in code A (uses A3C) to LunarLander-v2 and MountainCar-v0, there weren't any errors, but the agent fails to learn. Likewise, I change the environment in code B (uses DQN) to CartPole-v0 and MountainCar-v0, it didn't learn as well.

Why is it so? Is it because different environments have different rewards system? Or the hyperparameters that worked for CartPole-v0 does not work for LunarLander-v2?",5,3
1776,2019-7-26,2019,7,26,21,ci286p,[R] How to use Barnes-Hut t-SNE,https://www.reddit.com/r/MachineLearning/comments/ci286p/r_how_to_use_barneshut_tsne/,kdhht2334,1564144001,"Hi, guys!

There are so many data visualization techniques.

Among them, Barnes-Hut t-SNE is one of the most popular tool and is using many research papers.

So I have summarized how to use this tool, and anyone can follow it very easily.

If you have interest, visit below link and enjoy it :)

[https://github.com/kdhht2334/Barnes-Hut\_t-SNE](https://github.com/kdhht2334/Barnes-Hut_t-SNE)",3,1
1777,2019-7-26,2019,7,26,21,ci2iyz,"[P] GNES is Generic Neural Elastic Search, a cloud-native semantic search system based on deep neural network.",https://www.reddit.com/r/MachineLearning/comments/ci2iyz/p_gnes_is_generic_neural_elastic_search_a/,h_xiao,1564145782,,0,1
1778,2019-7-26,2019,7,26,22,ci2qf2,Proof of concept #2 nunchaku strikes and passing,https://www.reddit.com/r/MachineLearning/comments/ci2qf2/proof_of_concept_2_nunchaku_strikes_and_passing/,thetrickshotone,1564146947,,0,1
1779,2019-7-26,2019,7,26,22,ci2y7e,what are non-image applications of GANs?,https://www.reddit.com/r/MachineLearning/comments/ci2y7e/what_are_nonimage_applications_of_gans/,tdls_to,1564148149,[removed],0,1
1780,2019-7-26,2019,7,26,22,ci2zsj,[D] what are non-image applications of GANs?,https://www.reddit.com/r/MachineLearning/comments/ci2zsj/d_what_are_nonimage_applications_of_gans/,tdls_to,1564148398,I'm looking for any non-image applications of GANs or in general adversarial learning. are these being used in industry at all?,39,26
1781,2019-7-26,2019,7,26,22,ci30js,Comic by Google on Federated Learning,https://www.reddit.com/r/MachineLearning/comments/ci30js/comic_by_google_on_federated_learning/,abhiksark,1564148514,,0,1
1782,2019-7-26,2019,7,26,23,ci39wd,[R] Noise Contrastive Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/ci39wd/r_noise_contrastive_variational_autoencoders/,wei_jok,1564149899,,5,10
1783,2019-7-26,2019,7,26,23,ci3tlk,Suggestion for machine learning,https://www.reddit.com/r/MachineLearning/comments/ci3tlk/suggestion_for_machine_learning/,bryanemrys,1564152688,Anyone have any interesting papers or software regarding machine learning in geology or geophysics for learning purposes in oil and gas application.,0,1
1784,2019-7-27,2019,7,27,0,ci4an1,How to project multiple embeddings onto the same vector space?,https://www.reddit.com/r/MachineLearning/comments/ci4an1/how_to_project_multiple_embeddings_onto_the_same/,spyder313,1564154935,[removed],0,1
1785,2019-7-27,2019,7,27,0,ci4fli,Supervised Machine Learning: Is it okay to use features of a data-point to split it up into additional data-points?,https://www.reddit.com/r/MachineLearning/comments/ci4fli/supervised_machine_learning_is_it_okay_to_use/,mclovin215,1564155580,[removed],0,1
1786,2019-7-27,2019,7,27,0,ci4kxu,"Best approach to harmonize names and job titles in 80,000 transcripts",https://www.reddit.com/r/MachineLearning/comments/ci4kxu/best_approach_to_harmonize_names_and_job_titles/,newtomtl83,1564156272,"I am working on a dataset of 80,000 meeting transcripts. They are just text files, but they include a hard coded list of meeting participants. I used fuzzy string matching (python: fuzzy-wuzzy) to harmonize all the names within each file and avoid typos, etc. I now have xml files where I know who said what.

Now, I would like to harmonize the names and job titles across files, and I am not sure about what my best approach is. Some things to consider:

* Names are written differently throughout the dataset (e.g. Steve vs. Steven, AJ vs A.J.), etc.
* I have around 6k companies and 20k executives in total. Executives work for one company only in the dataset. For example. Bob Smith is CEO of ABC, which is different from Robert Smith who is CFO of CDE. If two people have the same name and work for the same company, 99% sure they are the same person.
* There are tons of variations of job titles, but only a few actual job titles (it's financial reporting meetings). So basically, I'd like to classify them into: CEO, CFO, Investor Relations, Others. Some people might have confusing titles, such as ""Asst. to the Chief Executive Officer"", which can make a fuzzy matching algorithm think that this is a CEO when it's not.
* Some data might be missing in some records, but be available in others. For example. John Smith might have no job title in one transcript, but be referenced as ""CEO"" in all the other files. I'd like to keep my dataset as rich as possible and enrich it rather that remove data.

I think I have several options, and I am not sure which one to pick. The easiest one (and dumbest one) is to use brute force. I have done this before with some good results, but it's not perfect. I basically extract the last word of each name and consider it the last name. I combine it with the company identification code. Then, I look for ""CEO"" or ""Chief Executive Officer"" in the title, and make a list with that. Some issues with that method:

* If two people have the same last name and work for the same company, my method doesn't work. It's particularly problematic for family businesses.
* An assistant to the CEO will be identified as a CEO in my method, which messes up my data.

Another option could be the following:

* Extract all names, titles, and company affiliations
* Within each company run a fuzzy matching algorithm to identify the different variations of names.
* Run a fuzzy matching algorithm independently on all job titles to identify CEOs, CFOs IRs and Others.
* Keep the cleaned up names and job titles.

I am not sure if this is a good approach. Something I am concerned with is keeping a data matching file, so that I can program a script to go harmonize the whole dataset once the names and job titles are cleaned. What do you think? I am open to different methods, especially if there is one that is published somewhere (I am an academic and this is for a paper.). Thanks!",0,1
1787,2019-7-27,2019,7,27,1,ci4po9,"[P] Harmonizing names and job titles in ad-hoc data (80,000 meeting transcripts)",https://www.reddit.com/r/MachineLearning/comments/ci4po9/p_harmonizing_names_and_job_titles_in_adhoc_data/,newtomtl83,1564156897,"I am working on a dataset of 80,000 meeting transcripts. They are just text files, but they include a hard coded list of meeting participants. I used fuzzy string matching (python: fuzzy-wuzzy) to harmonize all the names within each file and avoid typos, etc. I now have xml files where I know who said what.

Now, I would like to harmonize the names and job titles across files, and I am not sure about what my best approach is. Some things to consider:

* Names are written differently throughout the dataset (e.g. Steve vs. Steven, AJ vs A.J.), etc.
* I have around 6k companies and 20k executives in total. Executives work for one company only in the dataset. For example. Bob Smith is CEO of ABC, which is different from Robert Smith who is CFO of CDE. If two people have the same name and work for the same company, 99% sure they are the same person.
* There are tons of variations of job titles, but only a few actual job titles (it's financial reporting meetings). So basically, I'd like to classify them into: CEO, CFO, Investor Relations, Others. Some people might have confusing titles, such as ""Asst. to the Chief Executive Officer"", which can make a fuzzy matching algorithm think that this is a CEO when it's not.
* Some data might be missing in some records, but be available in others. For example. John Smith might have no job title in one transcript, but be referenced as ""CEO"" in all the other files. I'd like to keep my dataset as rich as possible and enrich it rather that remove data.

I think I have several options, and I am not sure which one to pick. The easiest one (and dumbest one) is to use brute force. I have done this before with some good results, but it's not perfect. I basically extract the last word of each name and consider it the last name. I combine it with the company identification code. Then, I look for ""CEO"" or ""Chief Executive Officer"" in the title, and make a list with that. Some issues with that method:

* If two people have the same last name and work for the same company, my method doesn't work. It's particularly problematic for family businesses.
* An assistant to the CEO will be identified as a CEO in my method, which messes up my data.

Another option could be the following:

* Extract all names, titles, and company affiliations
* Within each company run a fuzzy matching algorithm to identify the different variations of names.
* Run a fuzzy matching algorithm independently on all job titles to identify CEOs, CFOs IRs and Others.
* Keep the cleaned up names and job titles.

I am not sure if this is a good approach. Something I am concerned with is keeping a data matching file, so that I can program a script to go harmonize the whole dataset once the names and job titles are cleaned. What do you think? I am open to different methods, especially if there is one that is published somewhere (I am an academic and this is for a paper.). Thanks!",0,1
1788,2019-7-27,2019,7,27,1,ci4r5r,[D] Content &amp; Style disentanglement for video-game texture superresolution,https://www.reddit.com/r/MachineLearning/comments/ci4r5r/d_content_style_disentanglement_for_videogame/,ad48hp,1564157092,"Some games (like early 3D platformers) tend to have drastically different style from another, often mainstream games, in terms of its cartoony or perhaps photorealistic look. The superresolution networks that NVIDIA used doesn't use disentanglement i believe, which possibly restricts its generalization and usage on ultra-LQ textures..

&amp;#x200B;

Might someone look into joining games with similiar graphics styles from early era and todays's era (even though the style similiarity is questionable, sometimes games within the same franchises look different, so perhaps a style disentangler trained on regular datasets might be used to diffrentiate between 'em) and learn a network to recognize a style and generate high-resolution details based on dat ?",1,4
1789,2019-7-27,2019,7,27,1,ci4ri2,Supervised Machine Learning: Is it okay to use features of a data-point to split it up into additional data-points?,https://www.reddit.com/r/MachineLearning/comments/ci4ri2/supervised_machine_learning_is_it_okay_to_use/,stats_nerd21,1564157135,,0,1
1790,2019-7-27,2019,7,27,1,ci4uxs,HELP!,https://www.reddit.com/r/MachineLearning/comments/ci4uxs/help/,Osmack32,1564157586,[removed],0,1
1791,2019-7-27,2019,7,27,1,ci4vqm,Master's degree in statistics or AI ?,https://www.reddit.com/r/MachineLearning/comments/ci4vqm/masters_degree_in_statistics_or_ai/,standwithus,1564157688,[removed],0,1
1792,2019-7-27,2019,7,27,1,ci5896,"[D] Wired: Using AI to assess toxicity on social media | ""Drag Queen vs. David Duke: Whose Tweets Are More 'Toxic'?""",https://www.reddit.com/r/MachineLearning/comments/ci5896/d_wired_using_ai_to_assess_toxicity_on_social/,kirasolo,1564159368,"[Wired posted this article about using AI to assess toxicity on social media.](https://www.wired.com/story/drag-queens-vs-far-right-toxic-tweets/)

We are starting to see the impact of using AI/ML on everyday interactions and communications, not to mention policing. One of the most interesting and relevant conclusions:

&gt;The use of Perspective and other similar technologies could thus be mistakenly used to police and censor legitimate LGBTQ speech on online platforms. If AI tools focus on misleading signalssuch as the use of specific words, rather than a message's intentsuch models will make little progress in removing hate speech.",8,0
1793,2019-7-27,2019,7,27,2,ci5kvd,"Who was the machine learning company interviewed on Bloomberg Radio on 7/26 Friday morning? It's another one whose name ends with ""ai""",https://www.reddit.com/r/MachineLearning/comments/ci5kvd/who_was_the_machine_learning_company_interviewed/,aegrotatio,1564161032,[removed],0,1
1794,2019-7-27,2019,7,27,2,ci5rnw,Scite: a tool to find out if a scientific paper has been supported or contradicted since its publication,https://www.reddit.com/r/MachineLearning/comments/ci5rnw/scite_a_tool_to_find_out_if_a_scientific_paper/,JoshN1986,1564161929,,0,1
1795,2019-7-27,2019,7,27,2,ci5rzz,"[P] Finding similarities and clustering r/EarthPorn images using BiGAN, implemented in Keras!",https://www.reddit.com/r/MachineLearning/comments/ci5rzz/p_finding_similarities_and_clustering_rearthporn/,manicman1999,1564161977,"Hi everyone! I recently implemented BiGAN in Keras, with some modifications (hinge loss, gradient penalty, etc.). Then, with that, I made a script to find similarities in r/EarthPorn images using BiGAN's encoder, as well as clustering images using k-means clustering in the feature space. I've uploaded my code to github aswell.

&amp;#x200B;

&amp;#x200B;

Here is an image, on the left are target images, and on the right are the 7 most ""similar"" images in the dataset, in order.

[https://i.imgur.com/owXSkXf.png](https://i.imgur.com/owXSkXf.png)

&amp;#x200B;

&amp;#x200B;

Here is another, where each row is a cluster, detecting by k-means in the encoder's feature space.

[https://i.imgur.com/wgiKuiK.png](https://i.imgur.com/wgiKuiK.png)

&amp;#x200B;

&amp;#x200B;

Lastly, here is another image. On the top are images generated by BiGAN. On the bottom are real images of landscapes (1st and 3rd row) and their reconstruction using BiGAN's E and G (2nd and 4th row).

[https://i.imgur.com/lcUzGZk.png](https://i.imgur.com/lcUzGZk.png)

&amp;#x200B;

&amp;#x200B;

Github implementation:

[https://github.com/manicman1999/Keras-BiGAN](https://github.com/manicman1999/Keras-BiGAN)

&amp;#x200B;

&amp;#x200B;

Enjoy!",16,178
1796,2019-7-27,2019,7,27,3,ci6pv4,Open Source Example: Putting an Image Similarity Model into an iOS App,https://www.reddit.com/r/MachineLearning/comments/ci6pv4/open_source_example_putting_an_image_similarity/,mrfriedel,1564166484,"I am excited to announce that [Skafos](https://www.skafos.ai?utm_source=MLsubreddit&amp;utm_campaign=BootFinder) has open sourced the code for our [Boot Finder App](https://apps.apple.com/us/app/boot-finder/id1472790615). This app uses an image similarity model to identify which boots on the Zappos website are most similar to a picture taken by the app user. 

Here are our repos: 

* Model building code: [https://github.com/skafos/boot-finder-model](https://github.com/skafos/boot-finder-model)
* App building code: [https://github.com/skafos/BootFinder](https://github.com/skafos/BootFinder)

We have open sourced this code because weve talked to many people who are interested in putting ML models into iOS Apps, but either dont have a good use case, or have trouble connecting all the technical pieces. (App development in Swift, model development in Python, integrating the pieces, retraining, etc.) We are hoping our repos, and our platform Skafos, will help people to connect the dots and build ML-powered apps. 

Here are some of the technical details:

* We built our model in Python 3.6 using [Turi Create](https://github.com/apple/turicreate), converted it to [Core ML](https://developer.apple.com/documentation/coreml), and embedded it into an iOS App. 
* We use Kubernetes to automatically retrain each Sunday night. 
* Skafos is used to deliver model updates to the app. 

Happy building! Feel free to drop your questions below! For additional info, please visit our website at [skafos.ai.](https://www.skafos.ai?utm_source=MLsubreddit&amp;utm_campaign=BootFinder)",0,1
1797,2019-7-27,2019,7,27,3,ci6tzc,Papers to master Machine Learning knowledge,https://www.reddit.com/r/MachineLearning/comments/ci6tzc/papers_to_master_machine_learning_knowledge/,mr_dicaprio,1564167049,[removed],0,1
1798,2019-7-27,2019,7,27,3,ci6us1,"""[P]"" Open Source Example: Putting an Image Similarity Model into an iOS App",https://www.reddit.com/r/MachineLearning/comments/ci6us1/p_open_source_example_putting_an_image_similarity/,mrfriedel,1564167161,"I am excited to announce that [Skafos](https://www.skafos.ai?utm_source=MLsubreddit&amp;utm_campaign=BootFinder) has open sourced the code for our [Boot Finder App](https://apps.apple.com/us/app/boot-finder/id1472790615). This app uses an image similarity model to identify which boots on the Zappos website are most similar to a picture taken by the app user.

Here are our repos:

* Model building code: [https://github.com/skafos/boot-finder-model](https://github.com/skafos/boot-finder-model)
* App building code: [https://github.com/skafos/BootFinder](https://github.com/skafos/BootFinder)

We have open sourced this code because weve talked to many people who are interested in putting ML models into iOS Apps, but either dont have a good use case, or have trouble connecting all the technical pieces. (App development in Swift, model development in Python, integrating the pieces, retraining, etc.) We are hoping our repos, and our platform Skafos, will help people to connect the dots and build ML-powered apps.

Here are some of the technical details:

* We built our model in Python 3.6 using [Turi Create](https://github.com/apple/turicreate), converted it to [Core ML](https://developer.apple.com/documentation/coreml), and embedded it into an iOS App.
* We use Kubernetes to automatically retrain each Sunday night.
* Skafos is used to deliver model updates to the app.

Happy building! Feel free to drop your questions below! For additional info, please visit our website at [skafos.ai.](https://www.skafos.ai?utm_source=MLsubreddit&amp;utm_campaign=BootFinder)",1,2
1799,2019-7-27,2019,7,27,4,ci75pg,[P] I Built a Machine Learning Model that Learns in Realtime using Stochastic Gradient Descent. I wrote an article/tutorial on it!,https://www.reddit.com/r/MachineLearning/comments/ci75pg/p_i_built_a_machine_learning_model_that_learns_in/,cakhavan,1564168631,,0,1
1800,2019-7-27,2019,7,27,4,ci7ix4,The Amazing Ways Dubai Airport Uses Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/ci7ix4/the_amazing_ways_dubai_airport_uses_artificial/,OPPA_privacy,1564170426,,0,1
1801,2019-7-27,2019,7,27,4,ci7muf,[D] using computers to find errors in circulating currency?,https://www.reddit.com/r/MachineLearning/comments/ci7muf/d_using_computers_to_find_errors_in_circulating/,Silverfox1921,1564170975,"Lately there seems to be a developed increase in coin collecting. Especially in circulated change in the US. As a long time coin collector and computer guru. Would it make sense to create a paid app to do this for ordinary people?
Simply take a pic or front and back and get:
1)Estimated Grade
2)Metal composition
3)Unique (Rare) Features
4)Estimater Market Valeu",1,2
1802,2019-7-27,2019,7,27,5,ci7xjo,Using machine learning to predict All-Stars from the 2019 draft [x-post /r/NBA],https://www.reddit.com/r/MachineLearning/comments/ci7xjo/using_machine_learning_to_predict_allstars_from/,D4rkr4in,1564172392,,0,1
1803,2019-7-27,2019,7,27,6,ci90y0,Is this a good idea to begin learning ML?,https://www.reddit.com/r/MachineLearning/comments/ci90y0/is_this_a_good_idea_to_begin_learning_ml/,josephfrancis100,1564177751,,0,1
1804,2019-7-27,2019,7,27,7,ci9c0d,What's the best way to detect horizontal lines in an image. HoughLinesP isn't finding all of them.,https://www.reddit.com/r/MachineLearning/comments/ci9c0d/whats_the_best_way_to_detect_horizontal_lines_in/,azzipog,1564179266,[removed],0,1
1805,2019-7-27,2019,7,27,8,cia03y,Biological neuron analog for the bias term in neural networks?,https://www.reddit.com/r/MachineLearning/comments/cia03y/biological_neuron_analog_for_the_bias_term_in/,gamahead,1564182708,[removed],0,1
1806,2019-7-27,2019,7,27,8,cia5oc,[D] Is there a machine learning framework that allows you to manually change the layer weights after each update step?,https://www.reddit.com/r/MachineLearning/comments/cia5oc/d_is_there_a_machine_learning_framework_that/,Research2Vec,1564183545,"I am looking to sparse training; that is, during each training step, only a very small fraction of the weights go through the forward and backwards pass. So, only a small fraction of weights need to be loaded into memory at once. 

I was thinking of having the weights saved in sqlite or hdf5, and then having a keras layer which copies the relevant weights stored for the forward pass. And then after the update step, those weights are update into the disk. 

I know there are ways to directly setting the weights outside training https://stackoverflow.com/questions/51354186/how-to-update-weights-manually-with-keras

But I am wondering if any framework allows manually updating variables, after they have been already updated in a training step.",8,1
1807,2019-7-27,2019,7,27,8,cia6k4,[P] Generating text with character embeddings and RNN in pure Numpy,https://www.reddit.com/r/MachineLearning/comments/cia6k4/p_generating_text_with_character_embeddings_and/,kirbiyik,1564183676,"I've implemented a seq2seq generative character model in pure Numpy on top of CS231n's assignment code. It's nothing fancy but I think being able to see things in matrix multiplication level gives good intuitions about concepts in machine learning. Thus this repository contains reference implementations for

* [CharRNN (Sequence to Sequence RNN to produce characters with temperature)](https://github.com/kirbiyik/generate-any-text/blob/master/src/model/CharRNN.py)
* [Unit tests with numerical gradients to check implementations](https://github.com/kirbiyik/generate-any-text/blob/master/tests/layers.py)
* [Character Embedding](https://github.com/kirbiyik/generate-any-text/blob/master/src/layers/char_embedding.py)
* [Recurrent Neural Networks](https://github.com/kirbiyik/generate-any-text/blob/master/src/layers/rnn.py)
* [Temporal Affine Layer](https://github.com/kirbiyik/generate-any-text/blob/master/src/layers/fully_connected.py)
* [Temporal Softmax](https://github.com/kirbiyik/generate-any-text/blob/master/src/layers/temporal_softmax.py)
* [Adam optimizer](https://github.com/kirbiyik/generate-any-text/blob/master/src/optimizer/adam.py)
* [Solver (Run training loop, update weights, save model, record logs, etc.)](https://github.com/kirbiyik/generate-any-text/blob/master/src/solver/CharRNNSolver.py)
* [Data loading, character mapping, masking non-frequent characters](https://github.com/kirbiyik/generate-any-text/blob/master/src/dataloader/batches_from_txt.py)

Hope it will be useful for others, as well!  


# Repo

[**https://github.com/kirbiyik/generate-any-text**](https://github.com/kirbiyik/generate-any-text)",0,20
1808,2019-7-27,2019,7,27,8,ciac4i,What's the location &amp; deadline for AISTATS 2020?,https://www.reddit.com/r/MachineLearning/comments/ciac4i/whats_the_location_deadline_for_aistats_2020/,HenryWJReeve,1564184498,[removed],0,1
1809,2019-7-27,2019,7,27,8,ciahnb,[D] What's going on with AISTATS 2020?,https://www.reddit.com/r/MachineLearning/comments/ciahnb/d_whats_going_on_with_aistats_2020/,HenryWJReeve,1564185305,"Does anyone know what's going on with AISTATS 2020? The web page https://www.aistats.org/ still directs to AISTATS 2019. Usually, the deadline is in early October, so I'd have expected to see an announcement by now?",3,10
1810,2019-7-27,2019,7,27,10,cibf6p,Best courses online?,https://www.reddit.com/r/MachineLearning/comments/cibf6p/best_courses_online/,ReefJames,1564190684,I'm looking to do some learning online about convolutional neural networks. Does anyone recommend a specific course or couple of courses? Cheers,0,1
1811,2019-7-27,2019,7,27,10,cibj2e,ML Program to Detect Genetic Factors in Cardiomyopathy,https://www.reddit.com/r/MachineLearning/comments/cibj2e/ml_program_to_detect_genetic_factors_in/,alugocp,1564191324,,0,1
1812,2019-7-27,2019,7,27,11,cibyjb,Machine Learning sports predictions,https://www.reddit.com/r/MachineLearning/comments/cibyjb/machine_learning_sports_predictions/,JH1635,1564193988,[removed],0,1
1813,2019-7-27,2019,7,27,11,cic7bt,[P] Made an AI agent to play Snake Game,https://www.reddit.com/r/MachineLearning/comments/cic7bt/p_made_an_ai_agent_to_play_snake_game/,BrandNewThanos,1564195535,"Hi everyone.

I made an AI agent to play the game of Snake using Reinforcement Learning [https://youtu.be/yp826Ybh4rU](https://youtu.be/yp826Ybh4rU) . Here is a link to the code [https://github.com/AI-Olympics/Snake-AI](https://github.com/AI-Olympics/Snake-AI) . Any suggestions and positive feedback are welcome.",2,2
1814,2019-7-27,2019,7,27,12,ciclex,"I have been an ML researcher for 4 years now, and I wrote the following blog post. It may come off as a rant. Apologies if it feels that way. Let me know what you folks think.",https://www.reddit.com/r/MachineLearning/comments/ciclex/i_have_been_an_ml_researcher_for_4_years_now_and/,nefrpitou,1564198060,[removed],0,1
1815,2019-7-27,2019,7,27,13,cid1sa,[R] Making Convolutional Networks Shift-Invariant Again,https://www.reddit.com/r/MachineLearning/comments/cid1sa/r_making_convolutional_networks_shiftinvariant/,astrange,1564201121,,52,232
1816,2019-7-27,2019,7,27,13,cid7vv,Does a recent proof of the sensitivity conjecture have anything to do with learning theories?,https://www.reddit.com/r/MachineLearning/comments/cid7vv/does_a_recent_proof_of_the_sensitivity_conjecture/,haruthai,1564202248,[removed],0,1
1817,2019-7-27,2019,7,27,13,cidcvf,[D] Does a recent proof of the sensitivity conjecture have anything to do with learning theories?,https://www.reddit.com/r/MachineLearning/comments/cidcvf/d_does_a_recent_proof_of_the_sensitivity/,haruthai,1564203225,[removed],0,1
1818,2019-7-27,2019,7,27,15,cidym5,Colleges to pursue machine learning /Artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/cidym5/colleges_to_pursue_machine_learning_artificial/,Abhi_020698,1564207523,"Hey ! I've been finding it very hard to scout  colleges which are really good for ML/AI. 

Could anyone help me out on good colleges to pursue masters in machine learning or artificial intelligence ?
Any region is fine . Be it USA or Canada or UK  or any place which has good colleges for the aforementioned course",0,1
1819,2019-7-27,2019,7,27,15,ciebuw,GANs and Chaos Theory?,https://www.reddit.com/r/MachineLearning/comments/ciebuw/gans_and_chaos_theory/,96meep96,1564210273,"I've been trying to implement GANs by myself for the past two weeks and I've been noticing that they are incredibly sensitive to small changes, and that kind of fits the description of chaos theory too. I was wondering if any one out there has thought about this before and if there's any relation there that's worth pursuing, maybe a way to improve GANs with some insight into how chaotic systems work. Thoughts?",0,1
1820,2019-7-27,2019,7,27,16,ciesu8,[D] Workstation Remote Stack,https://www.reddit.com/r/MachineLearning/comments/ciesu8/d_workstation_remote_stack/,CircuitBeast,1564213870,"I built a deep learning workstation to play around with some deep learning models. Nothing to beastly, just a single GTX1060. 

Whats the most convenient method to connect to it with my laptop and run experiments on the GPU?",8,1
1821,2019-7-27,2019,7,27,16,cieuut,Questions on general research practices,https://www.reddit.com/r/MachineLearning/comments/cieuut/questions_on_general_research_practices/,manrajsinghgrover,1564214334,[removed],0,1
1822,2019-7-27,2019,7,27,17,cif3ng,AInamics:AI+Dynamics [Discussion] [Research],https://www.reddit.com/r/MachineLearning/comments/cif3ng/ainamicsaidynamics_discussion_research/,Vinu_U123,1564216294,"Hey guys! I'm new to AI and machine learning and I created these 3 laws(similar to 3 laws of thermodynamics) which can govern a self thinking(self-conscious?..maybe in the future?) robot or machine.I would love to hear your input on this.

1st law **Law of Efficiency:** A Systems efficiency can be determined by how *well* it handles the entropy present in its environment and the variables given to it.

2nd law **Law of Ideal:** A system is said to be ideal if it can take into consideration and handle on its *own* each and every event that happens in its environment and universe.

3rd law **Law of Safe:** A system is said to be safe if it handles and responds on its *own* to any event happening in its environment in a universally accepted manner.

What do you think on this?",2,0
1823,2019-7-27,2019,7,27,17,cif451,Some lighthearted fun,https://www.reddit.com/r/MachineLearning/comments/cif451/some_lighthearted_fun/,rish-16,1564216412,"&amp;#x200B;

![img](u4rmqpzx4tc31)",0,1
1824,2019-7-27,2019,7,27,17,cif5hc,Machine Learning Basics | Types | with Example in Hindi,https://www.reddit.com/r/MachineLearning/comments/cif5hc/machine_learning_basics_types_with_example_in/,Knowledge527,1564216728,,0,1
1825,2019-7-27,2019,7,27,18,cifflu,What next after completing a machine learning project,https://www.reddit.com/r/MachineLearning/comments/cifflu/what_next_after_completing_a_machine_learning/,eL0Tech47,1564219007,[removed],0,1
1826,2019-7-27,2019,7,27,18,cifj6i,Proof of concept #3 nunchaku rebound moves,https://www.reddit.com/r/MachineLearning/comments/cifj6i/proof_of_concept_3_nunchaku_rebound_moves/,thetrickshotone,1564219819,,0,1
1827,2019-7-27,2019,7,27,18,ciflwa,Detergent Liquid Laundry pouch packing machine with PLC Siemens control,https://www.reddit.com/r/MachineLearning/comments/ciflwa/detergent_liquid_laundry_pouch_packing_machine/,YQPACK,1564220461,,0,1
1828,2019-7-27,2019,7,27,18,cifmve,How should I write NeurIPS rebuttal to have a chance?,https://www.reddit.com/r/MachineLearning/comments/cifmve/how_should_i_write_neurips_rebuttal_to_have_a/,iAwaisRauf,1564220689,[removed],0,1
1829,2019-7-27,2019,7,27,19,cifqyv,What Is Reinforcement Learning? (Explained),https://www.reddit.com/r/MachineLearning/comments/cifqyv/what_is_reinforcement_learning_explained/,updownvizzii,1564221640,,0,1
1830,2019-7-27,2019,7,27,19,cift0q,China made stand up pouch filling and sealing machine for shampoo shower...,https://www.reddit.com/r/MachineLearning/comments/cift0q/china_made_stand_up_pouch_filling_and_sealing/,YQPACK,1564222064,,0,1
1831,2019-7-27,2019,7,27,19,cifw75,"What to do after college, if I don't want to pursue masters?",https://www.reddit.com/r/MachineLearning/comments/cifw75/what_to_do_after_college_if_i_dont_want_to_pursue/,gostewpid,1564222797,[removed],0,1
1832,2019-7-27,2019,7,27,20,cigaiq,Sentiment Analysis on drug reviews [P],https://www.reddit.com/r/MachineLearning/comments/cigaiq/sentiment_analysis_on_drug_reviews_p/,puchru0,1564225967,"Hey all,  I'm participating in a contest where we have to do sentiment analysis on drug reviews.  It has three categories as outputs : positive, negative, neutral. 

I've done the basics pre- processing, tf-idf vectorizer etc.  I'm now on the model application process. Can you suggest some best practices that I might have missed? 

I've tried out many models like Logistic Regression, LinearSVM,  Randomforest etc.  LR came out the best.  If I improve my f1score by 0.1 I might actually win the contest.  Can you please help?  It's currently at 0.54",2,0
1833,2019-7-27,2019,7,27,20,cigb65,Hot sale automatic soap cartoning machine videos,https://www.reddit.com/r/MachineLearning/comments/cigb65/hot_sale_automatic_soap_cartoning_machine_videos/,YQPACK,1564226103,,0,1
1834,2019-7-27,2019,7,27,20,cigb6p,[P] Deep Reinforcement Learning Library for Everyone,https://www.reddit.com/r/MachineLearning/comments/cigb6p/p_deep_reinforcement_learning_library_for_everyone/,zsdh123,1564226108,,0,1
1835,2019-7-27,2019,7,27,20,cighz4,"Manufacturer of Industrial Ultrasonic Component Cleaning Machine, Multistage Component Washing Machine Details Visit http://www.hydrojet.co.in or Call 044 48642264",https://www.reddit.com/r/MachineLearning/comments/cighz4/manufacturer_of_industrial_ultrasonic_component/,Ultramaxhydrojet,1564227558,,0,1
1836,2019-7-27,2019,7,27,21,cigtc4,[R] Noise Regularization for Conditional Density Estimation,https://www.reddit.com/r/MachineLearning/comments/cigtc4/r_noise_regularization_for_conditional_density/,whiletrue2,1564229755,"In neural network-based conditional density estimation (CDE), classic regularization approaches in the parameter space are mostly ineffective. To address this issue, **we develop a model-agnostic noise regularization method for CDE** that adds random perturbations to the data during training. We demonstrate that the proposed approach corresponds to a smoothness regularization, we prove its asymptotic consistency and **show across 7 datasets and 3 CDE models** that this works well. **Result: makes neural network-based CDE the preferable method over previous non- and semi-parametric approaches, even when training data is scarce!**

Paper: [https://arxiv.org/abs/1907.08982](https://arxiv.org/abs/1907.08982)  
Code: [https://github.com/freelunchtheorem/Conditional\_Density\_Estimation](https://github.com/freelunchtheorem/Conditional_Density_Estimation)",4,0
1837,2019-7-27,2019,7,27,21,cih1h6,Comparing modularity results from clustering,https://www.reddit.com/r/MachineLearning/comments/cih1h6/comparing_modularity_results_from_clustering/,notamirtron,1564231306,[removed],0,1
1838,2019-7-27,2019,7,27,21,cih2r2,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/cih2r2/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1564231520,[removed],0,1
1839,2019-7-27,2019,7,27,22,ciha7z,Specialized Chips used along with AI and Edge computing to speed up the model inferencing,https://www.reddit.com/r/MachineLearning/comments/ciha7z/specialized_chips_used_along_with_ai_and_edge/,sudhabhise,1564232798,,0,1
1840,2019-7-27,2019,7,27,22,cihapb,Real-World Machine Learning Projects with Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/cihapb/realworld_machine_learning_projects_with/,HannahHumphreys,1564232874,[removed],0,1
1841,2019-7-27,2019,7,27,22,cihfpl,How to apply Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cihfpl/how_to_apply_machine_learning/,prabeshpaudel,1564233712,[removed],0,1
1842,2019-7-27,2019,7,27,22,cihfrq,Complex Valued Neural network implementation,https://www.reddit.com/r/MachineLearning/comments/cihfrq/complex_valued_neural_network_implementation/,ssingh73,1564233721,[removed],0,1
1843,2019-7-27,2019,7,27,22,cihgiy,What is Artificial Intelligence? | Future Scope of AI,https://www.reddit.com/r/MachineLearning/comments/cihgiy/what_is_artificial_intelligence_future_scope_of_ai/,manjeet17,1564233843,,0,1
1844,2019-7-27,2019,7,27,22,cihiiz,When i have to create dummy's?,https://www.reddit.com/r/MachineLearning/comments/cihiiz/when_i_have_to_create_dummys/,bmorgado29,1564234165,[removed],0,1
1845,2019-7-27,2019,7,27,22,ciho8r,Looking for surevey papers on Actication Functions,https://www.reddit.com/r/MachineLearning/comments/ciho8r/looking_for_surevey_papers_on_actication_functions/,rafay_ak,1564235103,"I am looking for good survey papers on activation functions.

The first few results on google are pretty poorly written.

If you have a good one in mind please link me to it.",0,1
1846,2019-7-28,2019,7,28,0,ciitn5,"If you're going to take ML courses at a university, which is the most reputable university for that?",https://www.reddit.com/r/MachineLearning/comments/ciitn5/if_youre_going_to_take_ml_courses_at_a_university/,Agent_ANAKIN,1564241359,,0,1
1847,2019-7-28,2019,7,28,0,cij5l0,[D] Questions on general research practices,https://www.reddit.com/r/MachineLearning/comments/cij5l0/d_questions_on_general_research_practices/,manrajsinghgrover,1564243007,"Hi everyone,

I have a few questions on research practices that are generally followed but rarely mentioned in papers.

1. Let's say I have a train-dev-test split. After finding the best hyperparameters on dev set, should I retrain the model on train+dev set before evaluating it on the test set? Some discussions say yes, others say it depends on you and how much data you have and some say no.
2. Let's say I'm showcasing results on multiple datasets. Can one change the hyperparameters (learning rate, batch size, etc) from one dataset to another? More importantly, can I change, let's say, number of units in a layer without adding more layers? Would this count as an architectural change?
3. If yes, how would answer to above question change if the same is done within the dataset itself containing multiple parts?
4. Are we allowed to change a publically available dataset? For example, removing outliers for a regression problem?",4,6
1848,2019-7-28,2019,7,28,1,cij8rx,How to Develop a Wasserstein Generative Adversarial Network (WGAN) From Scratch,https://www.reddit.com/r/MachineLearning/comments/cij8rx/how_to_develop_a_wasserstein_generative/,iamart_intelligence,1564243429,[removed],0,1
1849,2019-7-28,2019,7,28,1,cijnlb,[P] can you train an LSTM on a single sample?,https://www.reddit.com/r/MachineLearning/comments/cijnlb/p_can_you_train_an_lstm_on_a_single_sample/,Subkist,1564245463,[removed],0,1
1850,2019-7-28,2019,7,28,2,cik3t0,How to 'convert' pyaudio's results to useful audio data ?,https://www.reddit.com/r/MachineLearning/comments/cik3t0/how_to_convert_pyaudios_results_to_useful_audio/,Andohuman,1564247643,[removed],0,1
1851,2019-7-28,2019,7,28,2,cikcr3,[D] ShuffleNet V2 on MNIST dataset,https://www.reddit.com/r/MachineLearning/comments/cikcr3/d_shufflenet_v2_on_mnist_dataset/,allen108108,1564248809,"Few days ago ,I see a article introduceed ShuffleNet V2 . 

Based on my curiosity I trained ShuffleNet V2 on MNIST datase, and I got 99.14% accuracy  .  
 ( code here :  [https://github.com/allen108108/Model\_on\_MNIST/blob/master/MNIST%20-%20ShuffleNetV2.ipynb](https://github.com/allen108108/Model_on_MNIST/blob/master/MNIST%20-%20ShuffleNetV2.ipynb)  ). 

The result looks good , but I got higher accuracy  ( almost 99.5% ) when I trained a typically CNN model on the same dataset    
 (  code here :  [https://github.com/allen108108/Model\_on\_MNIST/blob/master/MNIST%20-%20CNN.ipynb](https://github.com/allen108108/Model_on_MNIST/blob/master/MNIST%20-%20CNN.ipynb)).

Is that  normal situation ? Did I make some mistakes ?  


PS : the ShuffleNet V2 model that I used :  [https://github.com/opconty/keras-shufflenetV2](https://github.com/opconty/keras-shufflenetV2)",9,11
1852,2019-7-28,2019,7,28,2,cike4c,Machine Learning Solves Rubik's Cube in Under 1 Second!,https://www.reddit.com/r/MachineLearning/comments/cike4c/machine_learning_solves_rubiks_cube_in_under_1/,PersonalMarionberry,1564248983,,0,1
1853,2019-7-28,2019,7,28,3,cil1xd,[D] Resources for looking up early stage machine learning startups? Particularly Very early stage companies?,https://www.reddit.com/r/MachineLearning/comments/cil1xd/d_resources_for_looking_up_early_stage_machine/,DisastrousProgrammer,1564252157,"This is my preference for work enviroment right now. I find that they are hard to look up, since early stage companies aren't focused on PR. So maybe there's some indirect methods of looking up these companies.",18,63
1854,2019-7-28,2019,7,28,4,cilj3m,[P] comparing accuracy between two encoded datasets,https://www.reddit.com/r/MachineLearning/comments/cilj3m/p_comparing_accuracy_between_two_encoded_datasets/,rxo85,1564254408,"I have two datasets that I ran a label encoder and onehotencoder on. A is the actual and B is the prediction.

I would like to be able to come up with the accuracy between the two. For example in B, the last row is different so I would like calculate an accuracy of 75%.

Is there a nice pythonian way of doing this?

A

|0|1|0|
|:-|:-|:-|
|1|0|0|
|0|0|1|
|**1**|0|0|

B

&amp;#x200B;

|0|1|0|
|:-|:-|:-|
|1|0|0|
|0|0|1|
|0|**1**|0|",3,1
1855,2019-7-28,2019,7,28,4,ciloy2,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/ciloy2/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1564255191,[removed],0,1
1856,2019-7-28,2019,7,28,5,cimh5b,"[D] Should we take over the world ourselves, or just be the tools of the most rapacious?",https://www.reddit.com/r/MachineLearning/comments/cimh5b/d_should_we_take_over_the_world_ourselves_or_just/,phobrain,1564259076,[removed],3,1
1857,2019-7-28,2019,7,28,5,cimpmp,How does one generate (smooth) varying size output signals with Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/cimpmp/how_does_one_generate_smooth_varying_size_output/,brandojazz,1564260293,,0,1
1858,2019-7-28,2019,7,28,7,cins9r,[D] Style &amp; content disentanglement of temporal structure in videos,https://www.reddit.com/r/MachineLearning/comments/cins9r/d_style_content_disentanglement_of_temporal/,ad48hp,1564265869,"Back there with another question, have anyone considered disentangling/learning on style of temporal structure in videos ?

I have seen many examples of 'artistic'/photorealistic style transfer usage on single images/frames, and also using a temporal constraint to transfer it coherently.. But i ain't askin' about spatial style transfer enforced by temporal structure, but rather exploiting the temporality of different videos (films would be very useful i can imagine) and transferring 'em..",2,7
1859,2019-7-28,2019,7,28,7,cintkq,[P] How To Segment Buildings on Drone Imagery with Fast AI &amp; GeoData Tools: Interactive Intro to Geospatial DeepLearning on Colab,https://www.reddit.com/r/MachineLearning/comments/cintkq/p_how_to_segment_buildings_on_drone_imagery_with/,imitationcheese,1564266078,,0,1
1860,2019-7-28,2019,7,28,7,cinvbt,A Critique of Pure Learning: What Artificial Neural Networks can Learn from Animal Brains,https://www.reddit.com/r/MachineLearning/comments/cinvbt/a_critique_of_pure_learning_what_artificial/,P4TR10T_TR41T0R,1564266337,,0,1
1861,2019-7-28,2019,7,28,7,cinvwh,[R] A Critique of Pure Learning: What Artificial Neural Networks can Learn from Animal Brains,https://www.reddit.com/r/MachineLearning/comments/cinvwh/r_a_critique_of_pure_learning_what_artificial/,P4TR10T_TR41T0R,1564266417,,0,1
1862,2019-7-28,2019,7,28,7,cinxde,[R] A Critique of Pure Learning: What Artificial Neural Networks can Learn from Animal Brains,https://www.reddit.com/r/MachineLearning/comments/cinxde/r_a_critique_of_pure_learning_what_artificial/,P4TR10T_TR41T0R,1564266645,,1,1
1863,2019-7-28,2019,7,28,8,cios5i,why convolutional neural networks work?,https://www.reddit.com/r/MachineLearning/comments/cios5i/why_convolutional_neural_networks_work/,macob12432,1564271365,"Who came up with the use of watt filters and pass them over an image and then increase them to a neural network to work?

I see the amount of mathematics and I imagine aliens teaching humans how to use artificial intelligence.",0,1
1864,2019-7-28,2019,7,28,9,cip5r0,I am going to atteend ICCV19 and want to get internship from one of the companies exhibiting? What should I do?,https://www.reddit.com/r/MachineLearning/comments/cip5r0/i_am_going_to_atteend_iccv19_and_want_to_get/,iAwaisRauf,1564273465,,0,1
1865,2019-7-28,2019,7,28,10,cips75,Introduction to Boosting and AdaBoost,https://www.reddit.com/r/MachineLearning/comments/cips75/introduction_to_boosting_and_adaboost/,PartlyShaderly,1564276929,[removed],0,1
1866,2019-7-28,2019,7,28,11,ciq8tt,Machine learning and Econometrics - some questions,https://www.reddit.com/r/MachineLearning/comments/ciq8tt/machine_learning_and_econometrics_some_questions/,lppier,1564279626,"Hi, 

Ive been trying to implement a machine learning solution that creates a risk indicator for certain market situations in my company. Its mainly framed as a classification problem, indicating regions of bear markets, taking in features that are pre-defined by the business unit. The issue I have is that there are many econometrics colleagues (some ex-professors) which are not too convinced by the use of machine learning in this aspect. 

To quote an example - I find it difficult to explain feature engineering the lags like here :[https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/](https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/)to the econometrics folks. This is because to them, they are schooled on the classical time series algorithms, where a statistical test has to be made for the number of lags to be used. (aka the autoregressive portion p in ARIMA p, d, q). 

Of course, I understand that it is a tall order to predict the stock market, which is why we are positioning largely as a risk indicator where humans do come into the loop before big decisions are made. 

Has anyone else here dealt with such birthing issues (ML vs Econometrics) in the enterprise? What are some good ways to convince the econometrics side to accept the machine learning models?",0,1
1867,2019-7-28,2019,7,28,11,ciqdbv,Results of 2x 2070 Super tensorflow benchmarks using Lambda Labs' benchmarking tool.,https://www.reddit.com/r/MachineLearning/comments/ciqdbv/results_of_2x_2070_super_tensorflow_benchmarks/,resnet152,1564280364,[removed],1,1
1868,2019-7-28,2019,7,28,11,ciqe8k,Can anyone give an intuitive explanation of how model agnostic machine learning works? I am reading the paper on MAML for fast adaption of deep networks and I'm not able to understand why/how it works.,https://www.reddit.com/r/MachineLearning/comments/ciqe8k/can_anyone_give_an_intuitive_explanation_of_how/,aishwarya105,1564280513,[removed],0,1
1869,2019-7-28,2019,7,28,11,ciqoat,RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://www.reddit.com/r/MachineLearning/comments/ciqoat/roberta_a_robustly_optimized_bert_pretraining/,p1nh3ad,1564282268,,0,1
1870,2019-7-28,2019,7,28,12,cirbcq,Production video for Inline iodine solution filling and capping machine ...,https://www.reddit.com/r/MachineLearning/comments/cirbcq/production_video_for_inline_iodine_solution/,YQPACK,1564286338,,0,1
1871,2019-7-28,2019,7,28,13,ciriyh,[N] Daily ML Tips from a Graduate Student,https://www.reddit.com/r/MachineLearning/comments/ciriyh/n_daily_ml_tips_from_a_graduate_student/,theThinker6969,1564287688,"Hey, I am a Masters student and a TA for Machine Learning at my uni. What i noticed is that while teaching my students i would learn concepts much quicker. Hence why started a new twitter channel @Daily_ML_Tips so i can learn and teach more people directly. [Twitter Link Here](https://twitter.com/Daily_ML_Tips)

Feel free to give me a follow and tweet at me if u have any specific ML questions. :)

have a great day",25,83
1872,2019-7-28,2019,7,28,13,cirpen,[D] skip-GANomaly and general issues with reproduction of papers,https://www.reddit.com/r/MachineLearning/comments/cirpen/d_skipganomaly_and_general_issues_with/,good_rice,1564288901,"Hi all, I'm currently working through an implementation of skip-GANomaly, a paper on anomaly detection using an adversarially trained auto-encoder. In a nutshell, they train an auto-encoder over normal examples. Abnormal examples at test time will have poorer reconstruction, and thus higher loss, as they come from a different image distribution than normal examples. 

However, this paper contains no code. Additionally, the paper contains no clear definition of their network architecture. The only description of the network architecture is a figure shown beneath: 

[Problematic Figure of skip-GANomaly](https://i.redd.it/8zb8wu770zc31.png)

As there is no explanation, I interpret the initial convolutional maps to come from a single convolution from 3 -&gt; 64 channels; each following arrow is LReLU, Conv, and BN, so a nonlinearity would be applied afterwards. 

However, for any initial convolutional feature maps concatenated over, as shown in the image, reconstruction could be perfect without any of the rest of the network as a single convolution would be necessary to go from 64 + 64 -&gt; 3 channels. This convolution could just learn the inverse of the initial convolution which created the feature maps from the image. I could be misinterpreting this, so if anyone could enlighten me, that'd be great. I'd imagine that the feature maps resulting from the first convolution could not be concatenated, unless feature maps are defined to be subsequent to batch norm and activations. If bn and LReLU is included in the initial convolution, this is not described anywhere in the paper.

**In general,** how frequent are papers published without code / with difficulties in obtaining the architecture or training specifications to reproduce the work? I'm a student, and this is the first time I've diverged from large SOTA papers with thousands of citations. I was surprised to encounter a lot of the ambiguity in this paper.",9,76
1873,2019-7-28,2019,7,28,13,cirpwx,[D] What other basic classifiers are used for boosting?,https://www.reddit.com/r/MachineLearning/comments/cirpwx/d_what_other_basic_classifiers_are_used_for/,PartlyShaderly,1564288999,"I know that decision stumps are used as basic classifiers in boosting, but what other basic classifiers can be used? Can we use any classifier? SVMs are pretty accurate, and boosting, well, boosts the algorithm. So why not use SVMs as the basic binary classifier and boost the hell out of it with AdaBoost?  What's stopping us?",8,7
1874,2019-7-28,2019,7,28,14,cirxia,Project Ideas,https://www.reddit.com/r/MachineLearning/comments/cirxia/project_ideas/,agent--zero,1564290477,[removed],0,1
1875,2019-7-28,2019,7,28,14,cis036,Factory video clip Hot melt bopp glue labeling machine,https://www.reddit.com/r/MachineLearning/comments/cis036/factory_video_clip_hot_melt_bopp_glue_labeling/,YQPACK,1564290978,,0,1
1876,2019-7-28,2019,7,28,14,cis2ce,Why AI can WIN(LOSE) money fast in finance?,https://www.reddit.com/r/MachineLearning/comments/cis2ce/why_ai_can_winlose_money_fast_in_finance/,FinHacksQuants,1564291413,,0,1
1877,2019-7-28,2019,7,28,14,cis5q0,Machine Learning Training in Bangalore | Machine Learning course - ICSS,https://www.reddit.com/r/MachineLearning/comments/cis5q0/machine_learning_training_in_bangalore_machine/,TheAmit24-7,1564292117,,0,1
1878,2019-7-28,2019,7,28,15,cismz9,GitHub banned all Iranian users. Our accounts are restricted now. Please help us with contributing to this repo and show your support with a pull request. Thanks.,https://www.reddit.com/r/MachineLearning/comments/cismz9/github_banned_all_iranian_users_our_accounts_are/,erfaniaa,1564295651,,0,1
1879,2019-7-28,2019,7,28,15,cistim,[D] When will self-supervised learning replace supervised learning for computer vision tasks where unlabelled video is abundant?,https://www.reddit.com/r/MachineLearning/comments/cistim/d_when_will_selfsupervised_learning_replace/,strangecosmos,1564297098,"DeepMind's self-supervised ([a.k.a. unsupervised](https://www.facebook.com/722677142/posts/10155934004262143/)) network, CPC, [surpassed AlexNet's performance](https://medium.com/syncedreview/new-deepmind-unsupervised-image-model-challenges-alexnet-d658ef92ab1e) on ImageNet. If I understand correctly, both CPC and AlexNet used the same set of training images. CPC just didn't use labels, while AlexNet did. So, what about instances where a self-supervised network can be trained on 10,000x as much data as would be economically feasible to label? In these cases, are supervised learning's days numbered? Or not so fast? 

The application I'm personally most interested in is self-driving cars. By putting cameras on consumer cars, you are limited really only by your fleet size, your data centre costs, and your customers' monthly bandwidth limits for their home wifi in terms of how much data you can collect. Tesla, for instance, has over 500,000 cars with 360-degree cameras, GPUs or ASICs to run neural networks, and the ability to connect to wifi and upload training data. Elon Musk recently [mentioned](https://youtu.be/Ucp0TTmvqOE) Tesla's plans to ""do unsupervised massive training of vast amounts of video"". Tesla's Director of AI, Andrej Karpathy, also recently [tweeted](https://twitter.com/karpathy/status/1152701561212030976?s=20) his strong support for self-supervised learning. So this question is more than hypothetical.",15,48
1880,2019-7-28,2019,7,28,16,cit3t8,Company growth prediction model,https://www.reddit.com/r/MachineLearning/comments/cit3t8/company_growth_prediction_model/,dai-team-ai,1564299409,[removed],0,1
1881,2019-7-28,2019,7,28,17,citgg6,Inline type automatic liquid soap bottle filling capping and labeling ma...,https://www.reddit.com/r/MachineLearning/comments/citgg6/inline_type_automatic_liquid_soap_bottle_filling/,YQPACK,1564302419,,0,1
1882,2019-7-28,2019,7,28,17,citi3f,[R] DNN representational convexity and Human visual illusions,https://www.reddit.com/r/MachineLearning/comments/citi3f/r_dnn_representational_convexity_and_human_visual/,Nickless314,1564302839," [https://arxiv.org/abs/1907.09019](https://arxiv.org/abs/1907.09019)

TL;DR we consider images that show illusion in Human, represent the images in ImageNet-trained VGG-19, and find that representational distances around an illusion image are ""weird"" (non-monotonic). This is mostly unlike distances around control images, at least to the extend that we checked.

What do you think?  
We are now considering extensions, and would very much appreciate ideas, even negative.",1,9
1883,2019-7-28,2019,7,28,17,citjhx,Automatic adhseive sticker labeling machine for round plastic &amp; glass bo...,https://www.reddit.com/r/MachineLearning/comments/citjhx/automatic_adhseive_sticker_labeling_machine_for/,YQPACK,1564303159,,0,1
1884,2019-7-28,2019,7,28,18,city0w,China made horizontal form fill seal machine for fine powder sachet ffs ...,https://www.reddit.com/r/MachineLearning/comments/city0w/china_made_horizontal_form_fill_seal_machine_for/,YQPACK,1564306592,,0,1
1885,2019-7-28,2019,7,28,19,ciu6ij,[D] What are good and generic approaches for learning high level features from a dataset in an unsupervised manner?,https://www.reddit.com/r/MachineLearning/comments/ciu6ij/d_what_are_good_and_generic_approaches_for/,Valiox,1564308574,"One of my favorite papers is [Generalized End-to-End Loss for Speaker Verification](https://arxiv.org/pdf/1710.10467.pdf). It describes a way to learn a model that can derive embeddings from speech segments that are highly representative of the characteristics of the voice. It does so with only the identity of the speakers as labels. It's also an approach that can be applied to any kind of data beyond just voice, provided that the data is grouped by source (e.g. for speech it is grouped by speaker, for faces it is grouped by identity, ...).

A classical approach that will work without any labels is using an autoencoder. Not being up to speed in that domain, are there autoencoder-based frameworks that have proved to extract powerful features, more so that the classical auto-encoder pipeline?

Do you also know of approaches beyond these that achieve this goal?",12,50
1886,2019-7-28,2019,7,28,19,ciu9k7,Direct Manufacturer tabletop flat surface labeling machine for small fans,https://www.reddit.com/r/MachineLearning/comments/ciu9k7/direct_manufacturer_tabletop_flat_surface/,YQPACK,1564309258,,0,1
1887,2019-7-28,2019,7,28,19,ciuggk,Genomic Data Science,https://www.reddit.com/r/MachineLearning/comments/ciuggk/genomic_data_science/,HannahHumphreys,1564310829,[removed],0,1
1888,2019-7-28,2019,7,28,20,ciuo7n,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/ciuo7n/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1564312536,[removed],0,1
1889,2019-7-28,2019,7,28,20,ciuo8a,StarGAN-VC: Does anyone know how they did the conversion of WORLD features back to audio?,https://www.reddit.com/r/MachineLearning/comments/ciuo8a/starganvc_does_anyone_know_how_they_did_the/,CptChipmonk,1564312543,[removed],0,1
1890,2019-7-28,2019,7,28,20,ciur23,Detect Fraud and Predict the Stock Market with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/ciur23/detect_fraud_and_predict_the_stock_market_with/,HannahHumphreys,1564313166,[removed],0,1
1891,2019-7-28,2019,7,28,20,ciutme,[D] Transfer Learning / Domain Adaptation for Time-Series Forecasting,https://www.reddit.com/r/MachineLearning/comments/ciutme/d_transfer_learning_domain_adaptation_for/,dotXem,1564313728,"Is there any good papers / surveys on transfer learning / domain adaptation for time series forecasting?

I am mostly working with LSTM recurrent neural networks, but any other ML-based approach should be an interesting read.",11,41
1892,2019-7-28,2019,7,28,20,ciuzia,Relation extraction between entities (NLP),https://www.reddit.com/r/MachineLearning/comments/ciuzia/relation_extraction_between_entities_nlp/,ai_badger,1564314967,,0,1
1893,2019-7-28,2019,7,28,21,civ35a,"Free through edX, the University of Toronto offers an ML course designed to take advantage of quantum computing.",https://www.reddit.com/r/MachineLearning/comments/civ35a/free_through_edx_the_university_of_toronto_offers/,Agent_ANAKIN,1564315684,,0,1
1894,2019-7-28,2019,7,28,21,civbt1,Data Science Bootcamp: Pay only after you get a data science job.,https://www.reddit.com/r/MachineLearning/comments/civbt1/data_science_bootcamp_pay_only_after_you_get_a/,HannahHumphreys,1564317381,[removed],0,1
1895,2019-7-28,2019,7,28,21,civfwt,Classifying with over 1000 classes?,https://www.reddit.com/r/MachineLearning/comments/civfwt/classifying_with_over_1000_classes/,iamMess,1564318151,[removed],0,1
1896,2019-7-28,2019,7,28,21,civhqq,Semi-parametric Object Synthesis,https://www.reddit.com/r/MachineLearning/comments/civhqq/semiparametric_object_synthesis/,alshell7,1564318484,,1,0
1897,2019-7-28,2019,7,28,21,civiys,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/civiys/data_science_career_track_prep_course/,HannahHumphreys,1564318710,[removed],0,1
1898,2019-7-28,2019,7,28,22,ciw30b,High-Impact AI for Good Challenges to Work on Big Social Problems,https://www.reddit.com/r/MachineLearning/comments/ciw30b/highimpact_ai_for_good_challenges_to_work_on_big/,Lordobba,1564322127,,0,1
1899,2019-7-28,2019,7,28,23,ciwdek,What is Machine Learning and Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/ciwdek/what_is_machine_learning_and_deep_learning/,pranitkothari,1564323733,,0,1
1900,2019-7-28,2019,7,28,23,ciwpgi,What type of neural net should I choose to clasify song genres?,https://www.reddit.com/r/MachineLearning/comments/ciwpgi/what_type_of_neural_net_should_i_choose_to/,kapsomanu,1564325509,[removed],0,1
1901,2019-7-29,2019,7,29,0,ciwtz9,[Discussion] How to 'convert' pyaudio's results into useful audio data ?,https://www.reddit.com/r/MachineLearning/comments/ciwtz9/discussion_how_to_convert_pyaudios_results_into/,Andohuman,1564326162,"I'm sorry if the question is too vague (or too stupid). I'm trying to build a very basic speech recognition (for my own learning). I'm using pyaudio to record from the microphone and I've managed to convert the bytes to 16bit representations. But I do not know how to move forward with my little project.

I did find bits and pieces of code that seems to do what I'm trying to achieve but doesn't explain clearly why it does what it does.

For context I've previously worked on Image recognition, object detection and similar computer vision projects, but I'm a newb when it comes to audio.

Any help is appreciated.",9,10
1902,2019-7-29,2019,7,29,0,ciwyn6,Anomaly Detection with SQL on BigQuery ML,https://www.reddit.com/r/MachineLearning/comments/ciwyn6/anomaly_detection_with_sql_on_bigquery_ml/,_orcaman,1564326803,,0,1
1903,2019-7-29,2019,7,29,0,ciwzs5,SQL vs NoSQL for ML,https://www.reddit.com/r/MachineLearning/comments/ciwzs5/sql_vs_nosql_for_ml/,techpreneur_13,1564326956,[removed],0,1
1904,2019-7-29,2019,7,29,0,cix0jr,[N] This deep-learning AI bot autocompletes lines of source code for you using OpenAI's GPT-2,https://www.reddit.com/r/MachineLearning/comments/cix0jr/n_this_deeplearning_ai_bot_autocompletes_lines_of/,EkNekron,1564327058,,0,1
1905,2019-7-29,2019,7,29,1,cixvcz,MRI data processing,https://www.reddit.com/r/MachineLearning/comments/cixvcz/mri_data_processing/,kera24,1564331171,[removed],0,1
1906,2019-7-29,2019,7,29,1,cixvtv,GANs generate 100% photo realistic AI video avatars,https://www.reddit.com/r/MachineLearning/comments/cixvtv/gans_generate_100_photo_realistic_ai_video_avatars/,CryptoSteem,1564331238,[removed],0,1
1907,2019-7-29,2019,7,29,1,ciy1bm,"System 76's POP OS Makes NVIDIA GPU Driver, CUDA, CuDNN Installation for Deeplearning a Breeze",https://www.reddit.com/r/MachineLearning/comments/ciy1bm/system_76s_pop_os_makes_nvidia_gpu_driver_cuda/,rednafi,1564331993,[removed],0,1
1908,2019-7-29,2019,7,29,1,ciy1es,"Anyone know how to generate variabl-length sequence from Generator in GANs for sequence, like RNN-based GANs?",https://www.reddit.com/r/MachineLearning/comments/ciy1es/anyone_know_how_to_generate_variabllength/,tongjiyiming,1564332008,"For many RNN-like models, you can have variable-length sequences. So, if I use GAN for sequences, how can I ask Generator to generate a sequence with variable length?",0,1
1909,2019-7-29,2019,7,29,1,ciy3zj,Tattoo Synthesis,https://www.reddit.com/r/MachineLearning/comments/ciy3zj/tattoo_synthesis/,thatglitch,1564332366,"How would you approach this concept: I am interested in making a bot that creates tattoos. I have a tattoo artist who is willing to potentially sacrifice his artistic integrity and feed the bot his simple designs to serve as a dataset. Which framework do you reckon would be best? Let's say we have 1000 individual high res scans of black tattoos on white background and we want the bot to spit new, artificially synthesized designs (even if abstract and nonsensical). Any help would be much appreciated. 
This is for a short film. We have backing behind this so if there is anyone interested in being part of a creative collaboration and potentially get some small buck for your time, hit me in the DMs.",0,1
1910,2019-7-29,2019,7,29,1,ciy5z1,[Discussion] How to get generalization across camera parameters?,https://www.reddit.com/r/MachineLearning/comments/ciy5z1/discussion_how_to_get_generalization_across/,therumsticks,1564332636,"If I want to perform some computer vision task using CNN (let's say segmentation), how can I ensure that I can get generalization across images that were captured using another camera(with different intrinsic matrix)? I tried using a learned semantic segmentation model on my own camera images and it failed terribly.  I suspect this is because the convolution filters are overfitted to the particular resolution (fx, fy) we trained our model on.",5,2
1911,2019-7-29,2019,7,29,2,ciyaaw,[D] Softmax probability outputs,https://www.reddit.com/r/MachineLearning/comments/ciyaaw/d_softmax_probability_outputs/,dramanautica,1564333226,"Do softmax probability outputs in classification actually mean anything? Given that NNs arent generative models, how can they give a probabilistic estimate of the class given a sample?

Should they be trusted if they dont actually mean anything?",23,1
1912,2019-7-29,2019,7,29,2,ciygrm,[Research] Tattoo Synthesis,https://www.reddit.com/r/MachineLearning/comments/ciygrm/research_tattoo_synthesis/,thatglitch,1564334050,"How would you approach this concept: I am interested in making a bot that creates tattoos. I have a tattoo artist who is willing to potentially sacrifice his artistic integrity and feed the bot his simple designs to serve as a dataset. Which framework do you reckon would be best? Let's say we have 1000 individual high res scans of black tattoos on white background and we want the bot to spit new, artificially synthesized designs (even if abstract and nonsensical). Any help would be much appreciated. 
This is for a short film. We have backing behind this so if there is anyone interested in being part of a creative collaboration and potentially get some small buck for your time, hit me in the DMs.",6,13
1913,2019-7-29,2019,7,29,2,ciyqz6,[R] A Critique of Pure Learning: What Artificial Neural Networks can Learn from Animal Brains,https://www.reddit.com/r/MachineLearning/comments/ciyqz6/r_a_critique_of_pure_learning_what_artificial/,P4TR10T_TR41T0R,1564335418,"[https://www.biorxiv.org/content/10.1101/582643v1](https://www.biorxiv.org/content/10.1101/582643v1)

&amp;#x200B;

r/compmathneuro discussion \[here\]([https://www.reddit.com/r/compmathneuro/comments/cinu8v/a\_critique\_of\_pure\_learning\_what\_artificial/](https://www.reddit.com/r/compmathneuro/comments/cinu8v/a_critique_of_pure_learning_what_artificial/)).

&amp;#x200B;

Abstract:

Over the last decade, artificial neural networks (ANNs), have undergone a  revolution, catalyzed in large part by better tools for supervised  learning. However, training such networks requires enormous data sets of  labeled examples, whereas young animals (including humans) typically  learn with few or no labeled examples. This stark contrast with  biological learning has led many in the ANN community posit that instead  of supervised paradigms, animals must rely instead primarily on  unsupervised learning, leading the search for better unsupervised  algorithms. Here we argue that much of an animals behavioral repertoire  is not the result of clever learning algorithmssupervised or  unsupervisedbut arises instead from behavior programs already present  at birth. These programs arise through evolution, are encoded in the  genome, and emerge as a consequence of wiring up the brain.  Specifically, animals are born with highly structured brain  connectivity, which enables them learn very rapidly. Recognizing the  importance of the highly structured connectivity suggests a path toward  building ANNs capable of rapid learning.",34,171
1914,2019-7-29,2019,7,29,2,ciyv8h,Clustering Time Series by data points or derived features?,https://www.reddit.com/r/MachineLearning/comments/ciyv8h/clustering_time_series_by_data_points_or_derived/,powerforward1,1564335991,"I have a dataset with a time series that I want to cluster into differetn groups. (ex: series 1 goes into cluster b, series 2 goes into cluster a, etc)

I have 2 ways of doing this.

1) I can cluster them via geometry or by taking the squared . distance from every other time series. But each time series are different lengths and may be different speeds

&amp;#x200B;

2) I create a relational table . with custom derived features about the time series and use a k means or random forest to put them into groups.

&amp;#x200B;

What are the advantages and drawbacks of each method?",0,1
1915,2019-7-29,2019,7,29,2,ciyvtl,"Getting Hired in ML, Building a Portfolio | Interview with the CEO of SharpestMinds (ycombinator W'18): Edouard Harris",https://www.reddit.com/r/MachineLearning/comments/ciyvtl/getting_hired_in_ml_building_a_portfolio/,init__27,1564336059,[removed],0,1
1916,2019-7-29,2019,7,29,3,ciz8kf,[D] what are you currently studying?,https://www.reddit.com/r/MachineLearning/comments/ciz8kf/d_what_are_you_currently_studying/,PartlyShaderly,1564337726,"It's summer. I'm done with my first semester of SE, so let's talk about ML books that we're currently studying. My college doesn't offer ML, nor does it offer any good math classes so if you're beginner like me, you have to study 24/7 and fail at converting formulae to code so many times you get literally fed up with this crap. I'm studying:

1- My friend's Computational Linguistics dissertation. I've known her for  along time, and helped her with Python since she wasn't a programmer. Now she does NLP better than I do. She does research, and I can't even understand her dissertation. Partly because it's in Persian. I used to be an English lit student like her, but I hated English lit. I originally wanted to study CS, but I failed the SATs, so I had to apply to English lit if I wanted to go to a good college. But then I said ""fuck it!"" and dropped out and applied to community college. Such is life in the third world! Anyways...

2- Foundations of Machine learning by MIT Press. It's such a great book. It has an unhealthy obsession with PAC though.

3- Introduction to Deep Learning by Eugene Charniak: Oh my God I love this book! So simple! So nifty! It's obvious it's been typeset by the same person who did Foundations of et al, because they look practically the same. And both have been published by MIT Press, although Charniak teaches at Brown.

&amp;#x200B;

So what are you studying?",16,2
1917,2019-7-29,2019,7,29,4,cizvhz,Is there any field where machine learning and data analysis are useless?,https://www.reddit.com/r/MachineLearning/comments/cizvhz/is_there_any_field_where_machine_learning_and/,spad067,1564340725,,0,1
1918,2019-7-29,2019,7,29,4,cj0661,[P] fewshot-face-translation-GAN,https://www.reddit.com/r/MachineLearning/comments/cj0661/p_fewshotfacetranslationgan/,PuzzledProgrammer3,1564342103,"came across this, interesting project for face swapping

[https://github.com/shaoanlu/fewshot-face-translation-GAN](https://github.com/shaoanlu/fewshot-face-translation-GAN)

[https://raw.githubusercontent.com/shaoanlu/fewshot-face-translation-GAN/master/images/translation\_results/MonaLisa\_translation.gif](https://raw.githubusercontent.com/shaoanlu/fewshot-face-translation-GAN/master/images/translation_results/MonaLisa_translation.gif)",0,0
1919,2019-7-29,2019,7,29,5,cj0kyc,[D] Machine Learning - WAYR (What Are You Reading) - Week 67,https://www.reddit.com/r/MachineLearning/comments/cj0kyc/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1564344006,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|
|----|-----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/nobodykid23: [Feature-wise transformations](https://distill.pub/2018/feature-wise-transformations/)

Besides that, there are no rules, have fun.",1,5
1920,2019-7-29,2019,7,29,5,cj154f,tactics for training imagenet,https://www.reddit.com/r/MachineLearning/comments/cj154f/tactics_for_training_imagenet/,raggityrag,1564346626,"I'm pre-training a classifier network on imagenet and I've seen it claimed that doing so in steps with increasing resolution is the best way to get the highest accuracy. However, I've not actually seen a citation on that and I was wondering if there are any citations on for general purpose techniques like this or similar for getting the best possible final AP? Another I saw was using styletransfer version of imagenet mixed into the training to make it more robust to varying texture.",0,1
1921,2019-7-29,2019,7,29,5,cj1ajy,[D] Alternative GAN Architectures,https://www.reddit.com/r/MachineLearning/comments/cj1ajy/d_alternative_gan_architectures/,marynight,1564347351,"I'm a third-year undergraduate student in CS, focusing in ML and I've been reading a few papers on GANs. I was hoping someone could show me relevant papers on GAN architectures.   


In particular, I have the idea that you can create more complicated GAN systems and perhaps get better results. AFAIK, most GANs are structured as a combination of a discriminator and a generator. My thinking is that you could split up the generator into multiple generators and then create an ensemble network, let's call it a Decider. Ensembles generally train their generators on all available data and then cleverly combine the generators through techniques like boosting. Unlike the traditional ensembles, I want to train the Decider and the multiple generators adversarially. More specifically, the utility function of each generator would seek to maximize the probability it is selected by the Decider. Similarly, the Decider would seek to optimize its correct selection of a generator.   


Is anyone aware of multiple adversarial levels in GANs? My google-fu hasn't been strong enough to find anything related. I'm also wondering if there's an obvious reason such experiment would fail to produce anything interesting. Any feedback is greatly appreciated.",7,3
1922,2019-7-29,2019,7,29,6,cj1mo8,[R] OmniNet: A unified architecture for multi-modal multi-task learning,https://www.reddit.com/r/MachineLearning/comments/cj1mo8/r_omninet_a_unified_architecture_for_multimodal/,milaworld,1564348917,,1,0
1923,2019-7-29,2019,7,29,6,cj1q82,[D] What is the best way to implement a tensorflow model on the web?,https://www.reddit.com/r/MachineLearning/comments/cj1q82/d_what_is_the_best_way_to_implement_a_tensorflow/,jweir136,1564349403,[removed],0,1
1924,2019-7-29,2019,7,29,6,cj1st0,"[D] Has anyone did a machine learning training/mentorship program, where the payment was a % of your first year's salary?",https://www.reddit.com/r/MachineLearning/comments/cj1st0/d_has_anyone_did_a_machine_learning/,AdditionalWay,1564349799,"I have seen that they have had impressive results for general CS/coding programs. 

I am wondering how they have been doing in ML. I only know of ShapestMinds who is doing this, and I am *think* InsightAI does this as well.",1,0
1925,2019-7-29,2019,7,29,8,cj3813,Probably this question already exist. For someone starting machine learning that his background is basic in programming. Which courses or book would you recommend it to start with?,https://www.reddit.com/r/MachineLearning/comments/cj3813/probably_this_question_already_exist_for_someone/,queeloquee,1564357000,,0,1
1926,2019-7-29,2019,7,29,10,cj4an5,"[D] Should I write a book about PR and ML in my native tongue, in which material for these fields are rare?",https://www.reddit.com/r/MachineLearning/comments/cj4an5/d_should_i_write_a_book_about_pr_and_ml_in_my/,PartlyShaderly,1564362830,"Over the past three years, I have collected a valuable collection of notes on ML and PR (especially vision). I have tended to it from time to time. Some personal things happened that made me quit college (which after I went to reapply, I realized I'm still an active student!) and disrupted my learning. Then I tried other disciplines, mainly because rarely anyone knows about these disciplines in my country and I was afraid I'll be left unemployed after I get my Associates. But now that I am back in college, hence have creditably, and access to average-to-decent teachers, I was wondering if I should organize, and translate these notes to raise awareness about ML in my country. Right now, only the 7 biggest state universities, such as SU Industrial, EoSU, AK Polytechnic, SBU, FUM (where I studied something completely unrelated for two years, just to get into this university. I hated it)  have what they call an AI research lab, which includes ML and PR. These colleges are too rich for my dumb blood, and non-graduate students have rarely any ideas about these subjects. I want to make ML and PR approachable for the normal man. For the teenagers who barely speak any English, for the older programmers, who, like teenagers, don't speak English, for AU and PN and Community college students... All and all I want to make it a known subject. There are *many* books about ML in my country, but they're all stagnant translations that lack fluidity and suck at teaching their subject.

&amp;#x200B;

Do you think I'm allowed to write a book based on my notes?",0,1
1927,2019-7-29,2019,7,29,12,cj5sj7,Need advices from expert,https://www.reddit.com/r/MachineLearning/comments/cj5sj7/need_advices_from_expert/,AmirElhawy,1564371318,"Okay i have a basics knowledge for Ai in generally , i want to apply to one of this courses .
I a appreciate you help 

Foundations of Data Science
Intro to Python
and what should i start [Core Data Science R -- Intro to Python]
Introduction to Big Data
Basic Machine Learning 

Best regard 
Thank you .",0,1
1928,2019-7-29,2019,7,29,13,cj6bkq,"[N] Microsoft is hosting an online global AI Hackathon. $23,000 in prizes. Submissions due Sept 10th.",https://www.reddit.com/r/MachineLearning/comments/cj6bkq/n_microsoft_is_hosting_an_online_global_ai/,BatmantoshReturns,1564374501,,0,1
1929,2019-7-29,2019,7,29,13,cj6dn5,Now AI can also be used to identify fake text using GLTR Tool,https://www.reddit.com/r/MachineLearning/comments/cj6dn5/now_ai_can_also_be_used_to_identify_fake_text/,ai-lover,1564374849,"These researchers have released a tool called has **GLTR - Giant Language Model Test Room**

**1) Test yourself here GLTR tool page:** [http://gltr.io/](http://gltr.io/)

**2) GitHub link:** [https://github.com/HendrikStrobelt/detecting-fake-text](https://github.com/HendrikStrobelt/detecting-fake-text)

**3) Paper link:** [https://arxiv.org/pdf/1906.04043.pdf](https://arxiv.org/pdf/1906.04043.pdf)

&amp;#x200B;

![video](801h3obr76d31)",0,1
1930,2019-7-29,2019,7,29,13,cj6gew,Garbage Machine Learning in Python feat. Tensorflow and Numpy,https://www.reddit.com/r/MachineLearning/comments/cj6gew/garbage_machine_learning_in_python_feat/,ACodingFerret,1564375332,"
from __future__ import absolute_import, division, print_function, unicode_literals

 

import tensorflow as tf

from tensorflow.keras import layers

import numpy as np

 

 

 

model = tf.keras.Sequential([

 

layers.Dense(64, activation='relu', input_shape=(32,)),

 

layers.Dense(64, activation='relu'),

 

layers.Dense(10, activation='softmax')])

 

model.compile(optimizer=tf.compat.v1.train.AdamOptimizer(0.001),

              loss='categorical_crossentropy',

              metrics=['accuracy'])

 

def random_one_hot_labels(shape):

    n, n_class = shape

    classes = np.random.randint(0, n_class, n)

    labels = np.zeros((n, n_class))

    labels[np.arange(n), classes] = 1

    return labels

 

data = np.random.random((1000, 32))

labels = random_one_hot_labels((1000, 10))

 

dataset = tf.data.Dataset.from_tensor_slices((data, labels))

dataset = dataset.batch(32)

dataset = dataset.repeat()

model.fit(dataset, epochs=10, steps_per_epoch=30)

 

model.fit(data, labels, epochs=10, batch_size=32)",0,1
1931,2019-7-29,2019,7,29,13,cj6gp6,[R] Metalearned Neural Memory,https://www.reddit.com/r/MachineLearning/comments/cj6gp6/r_metalearned_neural_memory/,rtk25,1564375376,,2,7
1932,2019-7-29,2019,7,29,14,cj6pv9,Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/cj6pv9/machine_learning_course/,dwivediabhinav,1564377029,,0,1
1933,2019-7-29,2019,7,29,14,cj75id,Deep Q Memory Replay,https://www.reddit.com/r/MachineLearning/comments/cj75id/deep_q_memory_replay/,ZeroMaxinumXZ,1564379865,"Could you use a DQN, for memory replay selection instead of just random selection, for another DQN (an actor)..? What are the advantages of random replay and why use it?",0,1
1934,2019-7-29,2019,7,29,15,cj77ef,Predictive Modeling Creates a Clear Picture of the Future!,https://www.reddit.com/r/MachineLearning/comments/cj77ef/predictive_modeling_creates_a_clear_picture_of/,ElegantMicroWebIndia,1564380231,,0,1
1935,2019-7-29,2019,7,29,15,cj79n6,[D] Finding the best model for a task,https://www.reddit.com/r/MachineLearning/comments/cj79n6/d_finding_the_best_model_for_a_task/,MLtinkerer,1564380660,"Is there a resource to find the best model for a particular task?

 What's the best way to do so? As of now, I usually rely on my uni profs or seniors..

Am I the only one who finds it hard?

How do you folks go about this? What has worked/not worked?",10,5
1936,2019-7-29,2019,7,29,15,cj7cnn,[D] We find it extremely unfair that Schmidhuber did not get the Turing award. That is why we dedicate this song to Juergen to cheer him up.,https://www.reddit.com/r/MachineLearning/comments/cj7cnn/d_we_find_it_extremely_unfair_that_schmidhuber/,GlassPea9,1564381216,"Song: [https://twitter.com/ai\_antihype/status/1155091710281580548?s=20](https://twitter.com/ai_antihype/status/1155091710281580548?s=20)

Bonus song: [https://twitter.com/ai\_antihype/status/1155220318731919360](https://twitter.com/ai_antihype/status/1155220318731919360)",33,138
1937,2019-7-29,2019,7,29,16,cj7um4,Work involved in Graph Analytics,https://www.reddit.com/r/MachineLearning/comments/cj7um4/work_involved_in_graph_analytics/,rahulpadhy,1564384665,[removed],0,1
1938,2019-7-29,2019,7,29,16,cj7vzr,Google at ACL 2019,https://www.reddit.com/r/MachineLearning/comments/cj7vzr/google_at_acl_2019/,sjoerdapp,1564384943,,0,1
1939,2019-7-29,2019,7,29,17,cj8hiu,"GPT-2's favorite animals: urchin, ursine, ichthyosaurus, and the vernacular animal",https://www.reddit.com/r/MachineLearning/comments/cj8hiu/gpt2s_favorite_animals_urchin_ursine/,marhalabszar,1564389409,"I have been playing a lot with [talktotransformer.com](https://talktotransformer.com) and started to notice some patterns. Where the prompt is about animals (or even plants), words like vernacular, urchin, ursine (and its distorted variants), and ichtyo... frequently pop up.

For example, one of my first prompts was ""The common crow "" and it continued with ""is vernacular for the devil \[...\]"". Then I noticed that the word ""vernacular"" seems much more frequent in talktotransformer's texts than in the real world, and it seems to me that it somehow links the word to animals.

Also it is weird that its favorite animals are urchins and ichthyosaurs. It sometimes misspells urchin as ""ursine"" or ""ursin"". I think it does not know the real meaning of ""ursine"" as bears don't come up in the same context, though once I saw ""Ursus"" mentioned.

I did an experiment on how it continues some simple prompts:

**My favorite animal is the**: \_\_\_\_\_\_\_, , ursine's tail, ursine, ichthyosaur, \_\_\_\_, ichneumon, ichthyosaurus, vernacular animal, vernix

**The weirdest animal in the world is the**: urchin, urchin, ichneumon, ichthyosaur, urchin, urchin, ichneumon, \_\_\_\_\_, urchin, ursine

**The weirdest plant in the world is the**:   ..., black trumpet plant, lion plant, ilex, Plantau oleracea, ichipus, ursinus, ileostome, vernal crown plant, iliac flower

**The animal I find the cutest is the**: urchin, \_\_\_\_, ute-barge, lizard, \_\_\_\_ chicken, beagle, ursid, ichthyophorous chameleon, urchin, iknowing

**The common raven**: vernacular, vernal equinox, vernacular, s, ursine, is, ids, sings, ute, was

This is so weird to me, so I thought I would share this. I wonder how it picked up these very peculiar words from the training corpus. And how did it connect ""vernacular"" to animals? (Maybe not just animals, but crows and ravens especially seem to trigger this response. The word ""common"" may be a culprit as ""vernacular"" means the common language. But that word is not required, as can be seen with the ""My favorite animal"" prompt.)",0,1
1940,2019-7-29,2019,7,29,17,cj8kr7,[D] Multiple variable prediction handling,https://www.reddit.com/r/MachineLearning/comments/cj8kr7/d_multiple_variable_prediction_handling/,AbricotMozarella,1564390115,"Hi everyone,

It's easy to find documentation on single or multi classification and single variable regression, but when it comes to multiple regressions, I find it much harder.

If one wants to predict multiple variables (that we suppose not independent), like the consumption of a set of articles (given historical data etc). So for each article we have, we want to predict its consumption.

What would be the most common way to handle this? What is the actual preferred approach ? For basic ML, I'm used to XGBoost and LightGBM but those two are not supporting multiple variable prediction yet. Regarding DL, I believe this would ask a tremendous amount of data (the more article we have).

Are there any papers talking about this subject? Or any piece of work that would help?

&amp;#x200B;

The training data can be seen as such (and we want to predict art1, art2 etc) :

|id|datetime (*uneven*)||art1|art2|
|:-|:-|:-|:-|:-|
|1|05/09/2017 14:50:18||2|5|
|2|06/09/2017 02:23:55||1|7|",9,6
1941,2019-7-29,2019,7,29,18,cj8vem,Any lists of SOTAs on a WIDE Variety of tasks (including silly ones)?,https://www.reddit.com/r/MachineLearning/comments/cj8vem/any_lists_of_sotas_on_a_wide_variety_of_tasks/,Tenoke,1564392277,"There are a lot of 'SOTA' lists but they usually include a few NLP, CV, and RL results and call it a day. However, there are a ton of papers that beat the SOTA on a variaty of silly stuff. 

Is there an easy way to find a lot of SOTAs to try to beat the simplest ones for practice/fun?",0,1
1942,2019-7-29,2019,7,29,18,cj8wfw,AI and deep learning to tackle traffic congestion,https://www.reddit.com/r/MachineLearning/comments/cj8wfw/ai_and_deep_learning_to_tackle_traffic_congestion/,Verma_RJ,1564392495,,0,1
1943,2019-7-29,2019,7,29,18,cj8wlt,"[D] Large List of SOTA Results, including silly ones?",https://www.reddit.com/r/MachineLearning/comments/cj8wlt/d_large_list_of_sota_results_including_silly_ones/,Tenoke,1564392529,"There are a lot of 'SOTA' lists but they usually include a few NLP, CV, and RL results and call it a day. However, there are a ton of papers that beat the SOTA on a variety of silly stuff.

Is there an easy way to find a lot of SOTAs to try to beat the simplest ones for practice/fun?",2,3
1944,2019-7-29,2019,7,29,19,cj98xd,[P] Preview video of bamboolib - a UI for pandas. Stop googling pandas commands,https://www.reddit.com/r/MachineLearning/comments/cj98xd/p_preview_video_of_bamboolib_a_ui_for_pandas_stop/,kite_and_code,1564394873,"Hi,

this is Florian from edaviz and we are currently thinking about working on bamboolib.

Please check out the **short preview video** and let us know what you think:

[**https://youtu.be/yM-j5bY6cHw**](https://youtu.be/yM-j5bY6cHw)

&amp;#x200B;

The main benefits of bamboolib will be:

* you can **manipulate your pandas df via a user interface** within your Jupyter Notebook
* you get **immediate feedback** on all your data transformations
* you can **stop googling for pandas commands**
* you can **export the Python pandas code** of your manipulations

&amp;#x200B;

What is your opinion about the library?

&amp;#x200B;

Thank you for your feedback,

Florian

&amp;#x200B;

PS: if you want to get updates about bamboolib, **join our mailing list** which is linked on the github repo

[**https://github.com/tkrabel/bamboolib**](https://github.com/tkrabel/bamboolib)",23,22
1945,2019-7-29,2019,7,29,19,cj9bvz,[R] Using DeepLabCut to track chewing patterns of cats,https://www.reddit.com/r/MachineLearning/comments/cj9bvz/r_using_deeplabcut_to_track_chewing_patterns_of/,MasterScrat,1564395405,,1,2
1946,2019-7-29,2019,7,29,19,cj9fpl,[P] FARM: Transfer Learning in NLP,https://www.reddit.com/r/MachineLearning/comments/cj9fpl/p_farm_transfer_learning_in_nlp/,randomsgs,1564396086,,0,1
1947,2019-7-29,2019,7,29,19,cj9jmo,[P] FARM: Transfer Learning for NLP,https://www.reddit.com/r/MachineLearning/comments/cj9jmo/p_farm_transfer_learning_for_nlp/,randomsgs,1564396839,"Hey, we at [deepset.ai](http://deepset.ai/) have been working very hard on an open source framework for Transfer Learning in NLP. It's called FARM and the core idea is that we separate out the language model component (takes tokens, returns vectors) from the prediction head part (uses these vectors to make meaningful predictions). We currently have support for Text Classification, NER, Language Model Finetuning and Question Answering. Feel free to direct questions and suggestions to me. If this seems like something that you're interested in, leave us a star on github!

&amp;#x200B;

[https://github.com/deepset-ai/FARM](https://github.com/deepset-ai/FARM)",8,18
1948,2019-7-29,2019,7,29,19,cj9m7k,Dealing with open set reidentification?,https://www.reddit.com/r/MachineLearning/comments/cj9m7k/dealing_with_open_set_reidentification/,ychamel,1564397355,[removed],0,1
1949,2019-7-29,2019,7,29,19,cj9md5,[P] The machine learning lifecycle,https://www.reddit.com/r/MachineLearning/comments/cj9md5/p_the_machine_learning_lifecycle/,dzyl,1564397376,"Hi all,

Machine learning in the industry is a lot more than just models and mathematics. We wrote a blog post describing the phases that a successful machine learning project goes through, what potential pitfalls there are and how to succeed in each phase.

If you have any feedback or questions let us know!

[Blog post](https://www.cubonacci.com/blog/the-machine-learning-lifecycle)",0,1
1950,2019-7-29,2019,7,29,20,cj9u14,[D] Dealing with open set reidentification,https://www.reddit.com/r/MachineLearning/comments/cj9u14/d_dealing_with_open_set_reidentification/,ychamel,1564398764,"What's the best approach in dealing with open set identifications such as face id and re-id.
I know these problems are still unsolved and give low results without human aid.
So mainly Im asking for ideas on how to handle this topic.
For the sake of the argument lets focus on face recognition and lets say all images are of high quality.
So storing or comparing the first few hundred picture we might have somewhat okay results if we use clustering or threshholding but when the numbers get bigger the false positive rates sky rockets.
So is there a way to rerank or retrain everyonce in a while. Or at least differentiate from known and unknowns. Im lost at this step.",3,3
1951,2019-7-29,2019,7,29,21,cjaagp,[P] AI ping pong game w/ object detection on Raspberry Pi 4 &amp; Google Coral USB Accelerator,https://www.reddit.com/r/MachineLearning/comments/cjaagp/p_ai_ping_pong_game_w_object_detection_on/,paul_read_it,1564401602,"I recently got my hands on the Raspberry Pi 4 and the Google Coral USB Accelerator and i decided to program a little fun game in order to get an idea of the performance of these devices. This is a video summary about it:

[https://youtu.be/ruGk99s9Yhk](https://youtu.be/ruGk99s9Yhk)

**Technical details:**

* The machine learning model used is a [MobileNet SSD v2 trained on faces](https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite), which is publicly available. 
* The video frames processed by the machine learning model have the dimension 720 x 480 pixels
* The frames per second during this game fluctuate between 20 and 30 fps
* The program is written in Python and besides the TFLite model, most of the program is achieved with OpenCV

**What i learned:**

* The combo of a Raspberry Pi 4 which now has USB 3 and the Google Coral USB Accelerator are a powerful and cheap setup and should definitely evaluated for projects that deal with machine learning inference on the edge.
* The Raspberry Pi 4 CPU gets pretty hot, it sometimes went up to 80 degrees celsius! Although the heat issue might be partly fixed with a Raspbian update, i highly recommend to get an active cooling solution like a fan, a heat sink only lowered temperatures by 3-5 degrees in my case.
* I prefer working with these two devices over the Google Coral Dev Board because the latter one runs Mendel OS which i find hard to work with, there is a lot of restrictions when you get started and i had a hard time to find solutions, even to achieve something as simple as a right click or open a visual explorer. Raspbian OS on the other hand has just so much more support because of its great community. I haven't had my hands on the Jetson Nano from NVIDIA so i cannot make a comparison here.
* I tried to convert other objects into ping pong bats which can be recognized by the [MobileNet SSD v2 (COCO)](https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite), such as bananas but realised that objects need to be quite big or close to the camera in order to be recognized reliably by the relatively small image resolution. Smartphones and books worked quite well with that model but not as well as faces.

What do you think about these devices combined? Do you know of a better device or solution for machine learning inference on the edge?

In any case, I hope this helps you make decisions for your future projects :-) !

Paul",8,61
1952,2019-7-29,2019,7,29,21,cjan35,"[P] ""AlphaStar Uncovered"" AI's latest ladder run replays casted",https://www.reddit.com/r/MachineLearning/comments/cjan35/p_alphastar_uncovered_ais_latest_ladder_run/,keaneu,1564403614,,0,1
1953,2019-7-29,2019,7,29,21,cjap9m,Which modules from this masters degree are most employable right now?,https://www.reddit.com/r/MachineLearning/comments/cjap9m/which_modules_from_this_masters_degree_are_most/,anonymousysuomynona,1564403934,,3,1
1954,2019-7-29,2019,7,29,21,cjav0i,I wrote a blog on human brain vs AI. Let me know if anyone has some feedback,https://www.reddit.com/r/MachineLearning/comments/cjav0i/i_wrote_a_blog_on_human_brain_vs_ai_let_me_know/,ashutosj,1564404832,"This the link :

https://medium.com/@irm2015001/artificial-intelligence-vs-human-brain-1eb7403bf5bc",2,1
1955,2019-7-29,2019,7,29,22,cjb18w,"This reading Telegram group is focoused primarily on ""Computer Vision"" and ""Machine Learning""",https://www.reddit.com/r/MachineLearning/comments/cjb18w/this_reading_telegram_group_is_focoused_primarily/,Doctor_who1,1564405762,"This reading Telegram group is focoused primarily on ""Computer Vision"" and ""Machine Learning""

group for who have a passion for -

&amp;#x200B;

1. #ArtificialIntelligence

&amp;#x200B;

2. Machine Learning

&amp;#x200B;

3. Deep Learning

&amp;#x200B;

4. #DataScience

&amp;#x200B;

5. #Neuroscience

&amp;#x200B;

6. #ResearchPapers

&amp;#x200B;

link :

&amp;#x200B;

&amp;#x200B;

[https://t.me/joinchat/CuFqkRQSxRy5M\_6KxhKrCA](https://t.me/joinchat/CuFqkRQSxRy5M_6KxhKrCA)",1,1
1956,2019-7-29,2019,7,29,22,cjbcxm,[R][1907.11692] RoBERTa: A Robustly Optimized BERT Pretraining Approach,https://www.reddit.com/r/MachineLearning/comments/cjbcxm/r190711692_roberta_a_robustly_optimized_bert/,m__ke,1564407491,,29,82
1957,2019-7-29,2019,7,29,22,cjbghe,bject detection with poor lighting,https://www.reddit.com/r/MachineLearning/comments/cjbghe/bject_detection_with_poor_lighting/,AIObserver,1564408014,"We tried to train CNN (YOLO, Faster R-CCN) to detect objects and found that it is impossible to do with poor lighting on images. We saw some regularity in recognizing objects that are distinguished by the OpenCV function findContours(). If an object has the contour then CNN may be trained to detect this kind of objects. But if an object can't be contoured with the standard image processing procedures then neural networks don't know which objects to be trained on.

For example, this is the cardboard box image:

[Real image of the cardboard box](https://i.redd.it/cp9so0q8v8d31.jpg)

The result of findCountours():

[Cardboard box contours](https://i.redd.it/ax0b4hfjv8d31.jpg)

And as seen in the image, the bottom edge of the cardboard box is indistinguishable because of poor lighting. We managed to train the network to detect most of objects but not this one. And there is a certain regularity in the possibility of network training and the possibility of finding the contour of objects.

What can be done to solve the problem? 

Thank you.",0,2
1958,2019-7-29,2019,7,29,22,cjbj2d,"My Neural Network Isn't Learning, Please Help!",https://www.reddit.com/r/MachineLearning/comments/cjbj2d/my_neural_network_isnt_learning_please_help/,SteamSaajj,1564408362,"Thanks for clicking on this post, as the title is saying my neural network is failing to learn the task i am giving it, and i'm not sure why could you help me?   
Heres some details.  
Im trying to get the network to output a number between 0 and 1 (0 corresponding to left, 0.25 to right, 0.5 to up and 1 to down), to achieve this i am giving it some other data (its current position 0 to 1, if the grid above, left, right and below are walls or paths, 0 being walls, 1 paths, and where the goal position is. In conclusion all values are between 0 and 1.  
For context i want to  be able to use this to make an ai that can navigate random mazes/obstacle courses.  
An example of train\_X = (0.5675, 1, 0, 1, 0 0.95), train\_y = 0.5  


Heres my code, sorry if its not neat.   


import numpy as np  
import pandas as pd  
import tensorflow as tf  
from tensorflow.keras.models import Sequential  
from tensorflow.keras.layers import Dense  
model = Sequential()  


\#add model layers  
model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input\_shape=(6,)))  
model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  
model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))  
model.add(Dense(1, activation=tf.nn.sigmoid))  


\#compile model using mse as a measure of model performance  
model.compile(optimizer=""adam"", loss='mean\_squared\_error', metrics=\[""mean\_absolute\_error""\])  


data = pd.read\_csv(""C:\\\\Users\\\\James\\\\PycharmProjects\\\\Tensor\\\\Maze\\\\all\_moves.csv"")  
data.iloc\[np.random.permutation(len(data))\]  
dropped\_cols = \[0, 1, 2, 3\]  


train\_x = data.drop(data.columns\[dropped\_cols\], axis=1)  
train\_y = data.iloc\[:, 3\]  


train\_x = train\_x.to\_numpy()  
train\_y = train\_y.to\_numpy()  


\#print(""Move mean: "", train\_y.mean())  
\#print(""Move mode: "", train\_y.mode())  
\#print(""Move median: "", train\_y.median())  
\#train model  
\#model.fit(train\_x, train\_y, validation\_split=0.2, epochs=30, callbacks=\[early\_stopping\_monitor\], verbose=2)  
model.fit(train\_x, train\_y, validation\_split=0.1, epochs=90, verbose=2, batch\_size=24)  


[model.save](https://model.save)(""model\_8.h5"")  


The reason i drop columns 0, 1, 2 and 3 is becuase of the way i get my data. Just not 0, 1 and 2 are row numbers and 3 is what train y is.  
I have a fealing its todo with activation types but im not sure, thanks in advance.",11,0
1959,2019-7-29,2019,7,29,23,cjbv77,Are there any online ML classes specific for biomedical applications?,https://www.reddit.com/r/MachineLearning/comments/cjbv77/are_there_any_online_ml_classes_specific_for/,HalpMehProgram,1564409991,"Title - looking for any available classes that apply ML to signal (ECG, EMG, EEG) or image processing.",0,1
1960,2019-7-29,2019,7,29,23,cjbwq3,[N] How to manage Machine Learning and Data Science projects,https://www.reddit.com/r/MachineLearning/comments/cjbwq3/n_how_to_manage_machine_learning_and_data_science/,MasterEpictetus,1564410199,,0,1
1961,2019-7-29,2019,7,29,23,cjbyb6,[D] Machine Learning at Spotify (with Gustav Soderstrom),https://www.reddit.com/r/MachineLearning/comments/cjbyb6/d_machine_learning_at_spotify_with_gustav/,UltraMarathonMan,1564410423,"Gustav Soderstrom is the Chief Research &amp; Development Officer at Spotify, leading Product, Design, Data, Technology &amp; Engineering teams.

**Video:** https://www.youtube.com/watch?v=v-9Mpe7NhkM

**Audio:** https://lexfridman.com/gustav-soderstrom 

https://i.redd.it/a1xdy4ar59d31.png

**Outline:**

0:00 - Introduction

1:06 - Favorite song &amp; True Romance

3:04 - Purpose of music in society

7:02 - History of music

16:28 - Piracy and the internet, competing with free

20:54 - Innovation in music streaming user experience

23:36 - Video content, YouTube, and the focus on audio 

25:56 - Growing a user base 

28:14 - Letting go of music hoarding

31:18 - Playlists and machine learning

38:19 - Song data analysis

41:05 - Tools for empowering the music creation process

53:14 - Albums and podcasting

1:00:13 - Recommender systems &amp; machine learning

1:19:28 - Smart speakers

1:26:54 - Music labels

1:36:10 - Future of music streaming

1:41:17 - Future of mobile devices

1:44:25 - Her",16,50
1962,2019-7-29,2019,7,29,23,cjbz2s,Has there been any useful work in Neural Network Composition?,https://www.reddit.com/r/MachineLearning/comments/cjbz2s/has_there_been_any_useful_work_in_neural_network/,harshsikka123,1564410520,"Hey r/MachineLearning,

I'm fairly new to research, and I've been looking at ideas around composing multiple neural networks into larger networks that accomplish some greater, compound task. Some of the work I've seen includes the Module Network work by J. Andreas and Modular Neural Network/Mixture of Expert ideas.   


I was wondering if there's any literature out there that takes a look at the actual process of composing multiple neural networks together and compares that to simply training a larger network for the task. I imagine the former has benefits with regards to interpretability, but I can't seem to find any thing that examines this with a degree of granularity. Perhaps my search terms, i.e ""Neural Network Composition"" are missing the mark? 

Any references or advice would be very helpful, I really appreciate it!",0,1
1963,2019-7-29,2019,7,29,23,cjc4te,[AI] The Twenty Year History Of AI At Amazon,https://www.reddit.com/r/MachineLearning/comments/cjc4te/ai_the_twenty_year_history_of_ai_at_amazon/,cdossman,1564411310,"**From its humble beginnings in a garage to an** **online retail giant, Amazon is top 5 of Artifical Intelligence(AI) companies. Here's how it has successfully rebuilt itself around AI**  

 [https://www.forbes.com/sites/cognitiveworld/2019/07/19/the-twenty-year-history-of-ai-at-amazon/#7ca3a07468d0](https://www.forbes.com/sites/cognitiveworld/2019/07/19/the-twenty-year-history-of-ai-at-amazon/#7ca3a07468d0)",0,0
1964,2019-7-29,2019,7,29,23,cjc78s,[R] A 2019 Guide to Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/cjc78s/r_a_2019_guide_to_semantic_segmentation/,mwitiderrick,1564411621,"In this article, I look at established as well as state of the art models for semantic segmentation

[https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc)",1,19
1965,2019-7-29,2019,7,29,23,cjcbpn,Made a website that uses machine learning to help you explore new beers,https://www.reddit.com/r/MachineLearning/comments/cjcbpn/made_a_website_that_uses_machine_learning_to_help/,njchessboy,1564412178,,0,1
1966,2019-7-30,2019,7,30,0,cjch50,[P] Made a website that uses machine learning to help you explore new beers,https://www.reddit.com/r/MachineLearning/comments/cjch50/p_made_a_website_that_uses_machine_learning_to/,njchessboy,1564412823,,0,1
1967,2019-7-30,2019,7,30,0,cjcpdk,How to optimize hyperparameters when stacking models?,https://www.reddit.com/r/MachineLearning/comments/cjcpdk/how_to_optimize_hyperparameters_when_stacking/,jdyr1729,1564413846,[removed],0,1
1968,2019-7-30,2019,7,30,0,cjcrea,How can I build a model with translational variance?,https://www.reddit.com/r/MachineLearning/comments/cjcrea/how_can_i_build_a_model_with_translational/,Emcf,1564414124,"I'm currently facing a computer vision problem. The position of the features in the image plays a big role in extracting data from the image. As far as I know, CNNs may have trouble with such tasks due to the translational invariance of convolution. Does anyone with more experience than myself have any suggestions or recommendations for building a model for this type of problem?",0,1
1969,2019-7-30,2019,7,30,1,cjdr36,[D] Is it common to discard all 'non-full-time-trajectory' pedestrian from data in pedestrian prediction domain?,https://www.reddit.com/r/MachineLearning/comments/cjdr36/d_is_it_common_to_discard_all/,nokpil,1564418466,"Hello, I'm relatively new to pedestrian trajectory prediction, and I have a question about preprocessing of pedestrian data. Actually, I sent the question e-mail for the author of SocialGAN paper and since no reply was received. If anyone knows anything about this topic, your answer would help me a lot.

It is quite common that may ML pedestrian trajectory prediction employs LSTM in order to capture the temporal correlation. For example, look at the diagram below from the paper ''Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks' ([https://arxiv.org/pdf/1803.10892.pdf](https://arxiv.org/pdf/1803.10892.pdf) ). In the paper, author said that the model was trained to observe 3.2 seconds (8f) then predict 4.8 seconds (12f).

However, since the common pedestrian dataset (like ETH and UCY) collected from single still camera, trajectory terminates when the agent moves out of sight. Thus, unlike conceptual diagram in many papers, we cannot simply gather whole data for 3.2sec (8f) and let them consist LSTM hidden state. 

[Adopted from SocialGAN paper. Since some of the trajectories terminates earlier than others, LSTM stepts should handle this uneven data length.](https://i.redd.it/g9h7arnbs9d31.png)

Therefore, I've curious about how other researchers were handle this missing value problem.

While following the author's code on GitHub, I've found that the code discarded every individual when their trajectory throughout the given interval is shorter than its whole interval length. ([https://github.com/agrimgupta92/sgan/blob/master/sgan/data/trajectories.py](https://github.com/agrimgupta92/sgan/blob/master/sgan/data/trajectories.py), an excerpt from class 'TrajectoryDataset')  


[When the SocialGAN paper preprocess the raw pedestrian data into pytorch dataset, agents with short trajectory length discarded from dataset and not used for inference.](https://i.redd.it/6zviajhhm9d31.png)

In result, the final dataset for training does not contain a nonnegligible portion of trajectories despite that some of them could exist near to 'considered' individuals and may affect their trajectories. 

I want to raise two questions on this point.

1. Am I correctly understanding the code and its consequences? I would like to show a small toy example in order to visualize and verify my understandings of SocialGAN code.

Let us assume that seq\_len = 4 (obs\_len = 2, pred\_len = 2) and consider the frame 1\~4. Then, according to my understandings, SocialGAN code discards green agent(below) because it does not have a full 4-frame trajectory.

[Crude visualization of pedestrian dataset.](https://i.redd.it/rno1xk8gm9d31.png)

But, in my opinion, I think the green agent would affect red agent's trajectory a lot compared to blue one does. Although the missing value of the trajectory is an inevitable problem of the dataset itself (since no one can collect indefinite amount of tracked position), ignoring the whole trajectory because of those value might decrease the overall accuracy.

Furthremore, since this preprocessing handles frame-based trajectory data with a moving window (i.e. it makes frame 1\~4 as a first data and 2\~5 as second, 3\~6 as third, and so on.), agent like green might affect red agent in some data, while discarded in other data.

2. Secondly, (under the assumption that my understanding of the situation is correct), Is this kind of preprocessing (discarding non-full-trajectory individuals) common for preprocessing of the public pedestrian datasets such as ETH and UCY, or is this preprocessing was particular for SocialGAN and there are other ways to preprocess pedestrian data? I'd like to know that is there another way to deal with this missing value problem.

I'm currently trying to implement a model and preprocessing which can alleviate this problem, but I'm not certain whether this problem had been already addressed and well-known in CV/ML society, or it hadn't.

Again, any response or shared opinion will be grateful :)",3,2
1970,2019-7-30,2019,7,30,1,cjdrzs,[Question] Multi-Label Tagging for Images. Why my approach should or shouldn't work.,https://www.reddit.com/r/MachineLearning/comments/cjdrzs/question_multilabel_tagging_for_images_why_my/,macromayhem,1564418581,,0,1
1971,2019-7-30,2019,7,30,1,cjdti0,[R] Malware traffic detection with deep learning in the dark,https://www.reddit.com/r/MachineLearning/comments/cjdti0/r_malware_traffic_detection_with_deep_learning_in/,thesameoldstories,1564418757,,0,1
1972,2019-7-30,2019,7,30,1,cjdyh2,Machine Learning Certification Course - Msys Training,https://www.reddit.com/r/MachineLearning/comments/cjdyh2/machine_learning_certification_course_msys/,sibasankar,1564419372,,0,1
1973,2019-7-30,2019,7,30,2,cje161,Which analysis when you are blind at one side of the confusion matrix?,https://www.reddit.com/r/MachineLearning/comments/cje161/which_analysis_when_you_are_blind_at_one_side_of/,Tokukawa,1564419703,[removed],0,1
1974,2019-7-30,2019,7,30,2,cje4hz,Robust Neural Machine Translation,https://www.reddit.com/r/MachineLearning/comments/cje4hz/robust_neural_machine_translation/,sjoerdapp,1564420073,,0,2
1975,2019-7-30,2019,7,30,2,cjekzr,[P] TACO: Trash Annotations in Context Dataset,https://www.reddit.com/r/MachineLearning/comments/cjekzr/p_taco_trash_annotations_in_context_dataset/,cvwiz,1564422060,"Hey, for the past year, I have been building, with a friend of mine, an image dataset for litter detection, similar to COCO object segmentation. Check out our project here: [tacodataset.org](http://tacodataset.org)

Our aims are:

1. To crowdsource more images and annotations.
2. To support and stimulate more research and applications to address this problem.

Stay tuned for further updates.",19,71
1976,2019-7-30,2019,7,30,2,cjeq0k,Proof of concept #4 nunchaku plane's,https://www.reddit.com/r/MachineLearning/comments/cjeq0k/proof_of_concept_4_nunchaku_planes/,thetrickshotone,1564422663,,0,1
1977,2019-7-30,2019,7,30,3,cjez4z,[R] Reprogrammable Electro-Optic Nonlinear Activation Functions for Optical Neural Networks,https://www.reddit.com/r/MachineLearning/comments/cjez4z/r_reprogrammable_electrooptic_nonlinear/,ian_williamson,1564423722,"I am very excited to share our recently published work towards developing nonlinear activation functions for optical neural networks (ONNs).

There has been a lot of interest in specialized hardware for achieving high efficiency and performance on machine learning tasks. Matrix-vector multiplications are one of the most important (and computationally expensive) operations in neural networks. It turns out that analog *optical* processors can perform these operations in **O(1) time** (rather than the  **O(n\^2) time** on GPUs and CPUs). These specialized ONN processors, which are driven by modulated lasers, could potentially be scaled to use far less energy *per operation* than conventional digital processors.

Of course, the other piece of the puzzle for neural networks is the **nonlinear activation function**. Optics is excellent for performing linear operations, but nonlinearities are far more difficult, especially in on-chip circuits. Basically, in nature, if you want to see something or to send information, you use light. But, if you want to make a decision on that information you use electrical charge. 

Our paper (linked below) proposes a scheme for building a full ONN with an activation function by coupling a small electrical circuit to the output of each ONN layer. This electrical circuit converts a small amount of the optical signal into and electrical voltage, which then **nonlinearly modulates the optical signal**. We performed a benchmark of this ONN on the MNIST image recognition task and found that our activation function significantly boosted the classification accuracy of the ONN, from \~85% without the activation to \~94% with the activation. This is still a bit below the performance achieved in state-of-the-art models, but our setup used only 16 complex Fourier coefficients of the images as inputs (rather than all 784 pixels).

Checkout the paper below and feel free to ask questions. Our two Python ONN simulator packages (developed by two of my co-authors) are available on GitHub: [https://github.com/fancompute/neuroptica](https://github.com/fancompute/neuroptica) and [https://github.com/solgaardlab/neurophox](https://github.com/solgaardlab/neurophox/). These repos include several examples if you're interested in playing around with training ONNs on a computer.

**Journal Paper:** [https://doi.org/10.1109/JSTQE.2019.2930455](https://doi.org/10.1109/JSTQE.2019.2930455)

**arXiv preprint:** [https://arxiv.org/abs/1903.04579](https://arxiv.org/abs/1903.04579) (same content as the Journal version)",21,119
1978,2019-7-30,2019,7,30,3,cjf308,"[D] Podcast episode discusses the merit's of testing artificial intelligence in the context of the film Ex Machina, if be curious to know how people in this sub feel about Hollywood's attempts to characterize the future of testing AI",https://www.reddit.com/r/MachineLearning/comments/cjf308/d_podcast_episode_discusses_the_merits_of_testing/,aNoKneeMoose,1564424200,,0,1
1979,2019-7-30,2019,7,30,3,cjfcec,Apparently AlphaStar has played its first round of games on the EU ladder,https://www.reddit.com/r/MachineLearning/comments/cjfcec/apparently_alphastar_has_played_its_first_round/,mer_mer,1564425309,[removed],0,1
1980,2019-7-30,2019,7,30,3,cjfll1,Deep Learning for Computer Vision with Tensor Flow and Keras,https://www.reddit.com/r/MachineLearning/comments/cjfll1/deep_learning_for_computer_vision_with_tensor/,HannahHumphreys,1564426439,[removed],0,1
1981,2019-7-30,2019,7,30,3,cjfndq,"Is it possible to create a program that you feed clips of someones voice, and then it learning from that and turning that voice into a tts?",https://www.reddit.com/r/MachineLearning/comments/cjfndq/is_it_possible_to_create_a_program_that_you_feed/,avor14,1564426654,[removed],0,1
1982,2019-7-30,2019,7,30,4,cjfpjc,[D] How to manage Machine Learning and Data Science projects,https://www.reddit.com/r/MachineLearning/comments/cjfpjc/d_how_to_manage_machine_learning_and_data_science/,MasterEpictetus,1564426927,[removed],0,1
1983,2019-7-30,2019,7,30,4,cjfrwl,New Multilingual Video Description Dataset VATEX Receives Three Strong Accepts at ICCV,https://www.reddit.com/r/MachineLearning/comments/cjfrwl/new_multilingual_video_description_dataset_vatex/,Yuqing7,1564427183,,0,1
1984,2019-7-30,2019,7,30,4,cjfvuw,[R] Fooling real cars with Deep Learning (Deep Learning + Cybersecurity)),https://www.reddit.com/r/MachineLearning/comments/cjfvuw/r_fooling_real_cars_with_deep_learning_deep/,oblongatas_blancas,1564427634,"the paper [https://arxiv.org/abs/1907.00374](https://arxiv.org/abs/1907.00374)

medium post with a Demo video [https://medium.com/@shacharm/fooling-real-cars-with-deep-learning-cace6422c396](https://medium.com/@shacharm/fooling-real-cars-with-deep-learning-cace6422c396)",26,22
1985,2019-7-30,2019,7,30,4,cjg76x,Data Science Career Track Prep Course,https://www.reddit.com/r/MachineLearning/comments/cjg76x/data_science_career_track_prep_course/,HannahHumphreys,1564429001,[removed],0,1
1986,2019-7-30,2019,7,30,4,cjgcxu,[R] My first paper: Deep Learning for Cybersecurity,https://www.reddit.com/r/MachineLearning/comments/cjgcxu/r_my_first_paper_deep_learning_for_cybersecurity/,QuitoMeister,1564429689,"Hello everyone, I'm an engineering student and recently I was notified that my first paper was accepted in a regional conference of AI. I'm really happy about it and I want to share the preprint paper with you. Naturally, there is a lot of future work and improvements to do since it is my first research experience. 

[Detecting DNS Threats: A Deep Learning Model to Rule Them All](https://www.researchgate.net/publication/334637849_Detecting_DNS_Threats_A_Deep_Learning_Model_to_Rule_Them_All)

Abstract:

Domain Name Service is a  central part of Internet regular operation. Such importance has made it a  common target of different malicious behaviors such as the application  of Domain Generation Algorithms (DGA) for command and control a group of  infected computers or Tunneling techniques for bypassing system  administrator restrictions. A common detection approach is based on  training different models detecting DGA and Tunneling capable of  performing a lexicographic discrimination of the domain names. However,  since both DGA and Tunneling showed domain names with observable  lexicographical differences with normal domains, it is reasonable to  apply the same detection approach to both threats. In the present work,  we propose a multi-class convolutional network (MC-CNN) capable of  detecting both DNS threats. The resulting MC-CNN is able to detect  correctly 99% of normal domains, 97% of DGA and 92% of Tunneling, with a  False Positive Rate of 2.8%, 0.7% and 0.0015% respectively and the  advantage of having 44% fewer trainable parameters than similar models  applied to DNS threats detection. 

&amp;#x200B;

Thanks for reading, have a nice day!",20,168
1987,2019-7-30,2019,7,30,5,cjgslu,[R] Humble Book Bundle: Data Analysis &amp; Machine Learning by O'Reilly,https://www.reddit.com/r/MachineLearning/comments/cjgslu/r_humble_book_bundle_data_analysis_machine/,Intel_CEO,1564431576,,0,1
1988,2019-7-30,2019,7,30,5,cjgur3,I'm making a neural network (My first real one) to learn how to play tic tac toe. How should I go about training it?,https://www.reddit.com/r/MachineLearning/comments/cjgur3/im_making_a_neural_network_my_first_real_one_to/,Yung-Nut,1564431831,"I don't know how to calculate error if there is no real right answer to a game like tic tac toe. Should I do something like rewarding it for getting 2 in a row, or winning? What's the best plan of action?",0,1
1989,2019-7-30,2019,7,30,5,cjhcfc,[N] Machine Learning book bundle by O'Reilly,https://www.reddit.com/r/MachineLearning/comments/cjhcfc/n_machine_learning_book_bundle_by_oreilly/,Godzilla3056,1564433967,,0,1
1990,2019-7-30,2019,7,30,6,cjhism,[N] Machine Learning book bundle by O'Reilly,https://www.reddit.com/r/MachineLearning/comments/cjhism/n_machine_learning_book_bundle_by_oreilly/,Godzilla3056,1564434754,"Several ebooks with overall good quality. Might be useful for some.

&amp;#x200B;

[Bundle](https://twitter.com/Machinelearningtoday/status/1155923573606891527)",1,55
1991,2019-7-30,2019,7,30,6,cjhx39,[D] LSTM with walk-forward validation and data normalization/standardization,https://www.reddit.com/r/MachineLearning/comments/cjhx39/d_lstm_with_walkforward_validation_and_data/,punknothing,1564436537,"I'm currently trying to build a multivariate model to predict stock market movements using LSTM. The model is not seq-to-seq, but rather seq-to-one, if that matters.

I've read that walk-forward validation is the '[gold-standard](https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/)' for validation in time-series forecasting and that crossvalidation doesn't work due to the spatial-temporal relevancy of the data.

This creates some weird implications for data normalization...

I've firmly held the belief that information leakage can spoil a model by providing unreasonable in-sample performance accuracy/loss. Consequently, I'm pretty careful when train-test splitting and then using custom tranforming pipelines to standardize the data (i.e. fit\_transform() vs. transform() ). How do you overcome this issue? Is it really *that* big of a deal to split before standardization?

**Main question:** If you're using a moving-window walk-forward validation, how would you handle train/test data splits and data normalization?",3,0
1992,2019-7-30,2019,7,30,6,cji33h,Julia Computing &amp; MIT Introduce Differentiable Programming System Bridging AI and Science,https://www.reddit.com/r/MachineLearning/comments/cji33h/julia_computing_mit_introduce_differentiable/,Yuqing7,1564437278,,0,1
1993,2019-7-30,2019,7,30,7,cjia56,"A FaceApp to enlarge breasts? Its here, and theres more to it than you may think.",https://www.reddit.com/r/MachineLearning/comments/cjia56/a_faceapp_to_enlarge_breasts_its_here_and_theres/,gianlucahmd,1564438160,,0,1
1994,2019-7-30,2019,7,30,8,cjj9y0,factory outlet 15ml nail polish bottle filling brush inserting and cappi...,https://www.reddit.com/r/MachineLearning/comments/cjj9y0/factory_outlet_15ml_nail_polish_bottle_filling/,YQPACK,1564442903,,0,1
1995,2019-7-30,2019,7,30,8,cjjf1b,Random Tree Classifier - a bit of clarity needed,https://www.reddit.com/r/MachineLearning/comments/cjjf1b/random_tree_classifier_a_bit_of_clarity_needed/,SpyHandler,1564443597,[removed],0,1
1996,2019-7-30,2019,7,30,8,cjjj6a,Automatic stand up pouch filling and capping machine with bag feeding sy...,https://www.reddit.com/r/MachineLearning/comments/cjjj6a/automatic_stand_up_pouch_filling_and_capping/,YQPACK,1564444173,,0,1
1997,2019-7-30,2019,7,30,8,cjjmou,Which college for Cybersecurity MS?,https://www.reddit.com/r/MachineLearning/comments/cjjmou/which_college_for_cybersecurity_ms/,mannyspade,1564444675,"I'm looking for an affordable and quick online graduate school for an M.S. in a cybersecurity field with a relatively easy workload. What do you recommend?

A little background: I have a B.S. in non-science related major but I do have several years of experience in information technology and security. I don't need a degree from a top tier reputable school, I just need a Master's title in my resume. Tuition will be taken care of.",0,1
1998,2019-7-30,2019,7,30,8,cjjnfl,Vertical box boxing machine for 10-120ml chubby gorilla bottles carton p...,https://www.reddit.com/r/MachineLearning/comments/cjjnfl/vertical_box_boxing_machine_for_10120ml_chubby/,YQPACK,1564444785,,0,1
1999,2019-7-30,2019,7,30,9,cjjugi,"[R] AdvGen, a new approach to training neural machine translation models that applies generated adversarial examples to improve model robustness, reducing mistranslation rates and improving performance on standard benchmark",https://www.reddit.com/r/MachineLearning/comments/cjjugi/r_advgen_a_new_approach_to_training_neural/,Corp-Por,1564445816,,0,1
2000,2019-7-30,2019,7,30,9,cjjydw,"[N] Microsoft is hosting an online global AI Hackathon. $23,000 in prizes. Submissions due Sept 10th.",https://www.reddit.com/r/MachineLearning/comments/cjjydw/n_microsoft_is_hosting_an_online_global_ai/,BatmantoshReturns,1564446376,"Sorry if this is a repost, I posted it before but I think it got picked up by a spam filter, because I don't see it on the front page at all (please correct me if this is not the case).

Here's the link to the hackathon

https://azureai.devpost.com/

If you're looking for a team you can post a profile here

https://azureai.devpost.com/participants?search%5Bonly_looking_for_teammates%5D=1",7,12
2001,2019-7-30,2019,7,30,10,cjki66,The Depression Bot 5000,https://www.reddit.com/r/MachineLearning/comments/cjki66/the_depression_bot_5000/,Bolillo-Kremer,1564449165,,1,1
2002,2019-7-30,2019,7,30,10,cjkjkc,[1907.11849] Genetic Deep Learning for Lung Cancer Screening,https://www.reddit.com/r/MachineLearning/comments/cjkjkc/190711849_genetic_deep_learning_for_lung_cancer/,Turings_Ego,1564449373,,3,1
2003,2019-7-30,2019,7,30,11,cjlkf0,[D] is there study order(course) of RNN ?,https://www.reddit.com/r/MachineLearning/comments/cjlkf0/d_is_there_study_ordercourse_of_rnn/,GoBacksIn,1564454784," 

Hello... may I question something?

I am a student studying at a foreign university lab.

I am studying stock price forecasting.

Adjusting the LSTM unit and layer is complete.

The topics(plan) of the next research were auto-keras or NLP.

&amp;#x200B;

But the professor said that the topic was too broad.

he wants Step by step to build up my capabilities.

So I looked for all the paper related to stock predict.

Recent research was based on the mechanism of attention or the process of using natural language news.

The following research themes should be decided, but the gap between basic research and the latest research is too large.

I do not know what to make the next research topic.

&amp;#x200B;

In the case of CNN, there are various networks such as LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet, and ResNet

However, since LSTM is SOTA(LSTM: A Search Space Odyssey  [https://arxiv.org/abs/1503.04069](https://arxiv.org/abs/1503.04069) ) in RNN 

So, in the case of RNN is LSTM only 

I think that methodology of improving accuracy of LSTM should be the subject of research.

do you have any course for forecasting time series data?

I do not know how to get directions

I'd like to ask your opinion.",1,0
2004,2019-7-30,2019,7,30,12,cjltsp,Ranking Risk Accurately Across Entire Imbalanced Class Dataset using Ensemble Modeling,https://www.reddit.com/r/MachineLearning/comments/cjltsp/ranking_risk_accurately_across_entire_imbalanced/,weightsandbayes,1564456197,"Essentially, I have a dataset with &lt;5% positives. About half the dataset has a 0% chance of being positive, and \~30% of the data is potentially positive, and 10% of the dataset has a high chance of being positive

&amp;#x200B;

I can weight the positive classes in the training set an do really well on precision (metric of choice) in the high risk set, my precision in the low risk set goes down. 

&amp;#x200B;

My thought is I should make three predictors: 

1. Rank risk of observations (A.)
2. Make algorithm that works well on high risk people (B.)
3. Make algorithm that works well on low risk people (C.)

&amp;#x200B;

Then either:

1. Make logistic regression in the form of:  Y = Constant + X\*A\*B +(1-X)A\*C
2. use a decision tree(/random forest etc) on all three outputs) - but I'd lose some explainability

Anyone with similar experience that can advise?",0,1
2005,2019-7-30,2019,7,30,12,cjlypc,[D] What's missing in blogs?,https://www.reddit.com/r/MachineLearning/comments/cjlypc/d_whats_missing_in_blogs/,mfarahmand98,1564456963,"Hello everyone.

Recently, I've been thinking about starting a blog on Deep Learning, but when I look around, there's just so many that are doing such a good work! 
There are some awesome blogs on Medium that go through ML topics both for knowledgeable and expert readers, some intriguing blog on a number of platforms that publish articles on the latest big things, so to speask, in ML, and vlogs on YouTube that are clearly spending a lot of time and resources on their videos! 
So, I wanted to ask you guys this. What do you think is missing in blogs? If you wanted to make a blog on ML what would you write about? I really want to somehow contribute back to this awesome community that has made it possible to learn just so much for free! 

Thank you :)",4,0
2006,2019-7-30,2019,7,30,12,cjm0at,[N] Medieval Japanese Document Recognition Competition,https://www.reddit.com/r/MachineLearning/comments/cjm0at/n_medieval_japanese_document_recognition/,alexmlamb,1564457219,,0,1
2007,2019-7-30,2019,7,30,13,cjmegj,[D]ual Numbers and Automatic Differentiation explained,https://www.reddit.com/r/MachineLearning/comments/cjmegj/dual_numbers_and_automatic_differentiation/,kevinccccccc,1564459483,"For those struggling to understand AD and dual numbers, [this article](https://blog.demofox.org/2014/12/30/dual-numbers-automatic-differentiation/) might help :)!",1,2
2008,2019-7-30,2019,7,30,13,cjml22,Reviews of 8 Python Deep Learning Computer Vision Courses (including PyImage),https://www.reddit.com/r/MachineLearning/comments/cjml22/reviews_of_8_python_deep_learning_computer_vision/,careertroubleguy,1564460558,[removed],1,1
2009,2019-7-30,2019,7,30,14,cjn049,Solving the Armadillo Problem - A great short talk about Adversarial Learning,https://www.reddit.com/r/MachineLearning/comments/cjn049/solving_the_armadillo_problem_a_great_short_talk/,YaelMt,1564463135,,0,1
2010,2019-7-30,2019,7,30,14,cjn2dp,ML for Identifying Experts in Software Libraries and Frameworks among GitHub Users,https://www.reddit.com/r/MachineLearning/comments/cjn2dp/ml_for_identifying_experts_in_software_libraries/,RocketshipJobs,1564463517,,0,1
2011,2019-7-30,2019,7,30,14,cjn826,Data augmentation use outside of the image domain?,https://www.reddit.com/r/MachineLearning/comments/cjn826/data_augmentation_use_outside_of_the_image_domain/,boltzBrain,1564464493,[removed],0,1
2012,2019-7-30,2019,7,30,15,cjnuld,"On the viability of ""the AI Dungeon Master""",https://www.reddit.com/r/MachineLearning/comments/cjnuld/on_the_viability_of_the_ai_dungeon_master/,MindChisel,1564468612,,0,1
2013,2019-7-30,2019,7,30,15,cjnund,[D] What are your favorite videos / lectures on advanced topics in machine learning?,https://www.reddit.com/r/MachineLearning/comments/cjnund/d_what_are_your_favorite_videos_lectures_on/,MTGTraner,1564468624,,81,134
2014,2019-7-30,2019,7,30,15,cjnypd,[P] Book recommendation engine,https://www.reddit.com/r/MachineLearning/comments/cjnypd/p_book_recommendation_engine/,BeggarInSpain,1564469387,"Hello,

I'm working on a project from [http://www2.informatik.uni-freiburg.de/\~cziegler/BX/](http://www2.informatik.uni-freiburg.de/~cziegler/BX/) and I'm given a question ""If I like Lord of the Rings, what else I should read?"". How would you proceed? My first thought was find other users/readers who like the book too and find what else they rated high. So, I've tried to create a pivot table, which ran out of memory (there's over 1.3M rows), so I've create a group dataframe instead but I'm kinda in how to make a correlation. My second thought was to create clusters, find in which cluster is LOTR and recommend other books from the cluster. But I'm not sure about the later one.

Any insight would be most appreciated. Thanks!",15,4
2015,2019-7-30,2019,7,30,15,cjnysm,"[D] On the viability of ""the AI Dungeon Master""",https://www.reddit.com/r/MachineLearning/comments/cjnysm/d_on_the_viability_of_the_ai_dungeon_master/,MindChisel,1564469407,,0,1
2016,2019-7-30,2019,7,30,16,cjo49c,"[D] On the viability of the ""AI Dungeon Master""",https://www.reddit.com/r/MachineLearning/comments/cjo49c/d_on_the_viability_of_the_ai_dungeon_master/,MindChisel,1564470388,"I originally posted this on /r/learnmachinelearning since it's technically a question, but I think it goes beyond the scope of questions like ""how do I set up tensorflow"" and is worth putting here well.

I love tabletop RPGs, and I'm specifically curious if there are any areas of current ML technology that might allow the nerd fantasy of an independent AI dungeon master to come true, whether fully or even partially.

For those unfamiliar with tabletop RPGs, I'll run over the basic gameplay loop, using Dungeons &amp; Dragons as my example.

At the center of it all you have a system of core rules (easily programmable rules, in fact) which tracks character stats, specifics of movement, items, abilities, success/failure checks, and other details down to a very granular level. This part is easy for computers, evidenced by the fact that RPG video games derived from these rulesets.

However, with a tabletop RPG, the game is ran by a person known as the Dungeon Master or Game Master, who acts as a kind of referee for the rules. The Dungeon Master can take whatever request a player makes, then translate that request into something compatible with the rules.

This process of ""compiling"" a request into a rule check can be done for something simple, such as ""I attack the orc"" (damage based on a dice roll + weapon type + skill modifiers + proficiency bonus, etc.), or for something crazy and convoluted, such as ""I fashion the rope in my inventory into a lasso, toss it so that it loops around the damaged stone pillar at the other side of the chasm, then pull as hard as I can"" (dice roll based on the player's dexterity skill, then another based on their strength, success resulting in the pillar toppling and creating a bridge of rubble; player loses the rope item and is able to roll again with their acrobatics ability to cross the chasm without consequences).

So there is consistency mixed with inconsistency. A player's input can be wild and unpredictable, but it always resolves into rule checks that are consistent, mechanical, and easily understandable by a machine. The Dungeon Master is the missing link. The adjudication of a human brain allows players to interact with the rules in a way that's deeper and freer than clicking pre-programmed buttons to perform actions, which is why video games have never been able to replicate the feeling of a real tabletop RPG.

But now we have machine learning. What kind of AI, if any, could be used to taking ambiguous statements and convert them into standard game interactions? And to what extent could it be pushed? If it's impossible to account for everything (which I'm guessing it is, at least for now), then what kind of sacrifices could be made on the player's side (e.g. typing only short, simple requests in a specific format, or entering nouns and verbs into text boxes) to make it possible?",25,22
2017,2019-7-30,2019,7,30,16,cjojju,[D] E2E Case Studies,https://www.reddit.com/r/MachineLearning/comments/cjojju/d_e2e_case_studies/,Yellow_Robot,1564473304,"Hello All.

I am looking for some intro in E2E of Machine Learning pipelines. I will be glad if you can share some stories or links of existing case studies that describe ways of E2E implementation of machine learning processes (something like kubeflow or tensorflow extended). 

I am trying to find out the golden middle or best-used practices from SRE/DevOps point of view. Thank you.",0,0
2018,2019-7-30,2019,7,30,16,cjokun,Noise Contrastive Estimate Explained,https://www.reddit.com/r/MachineLearning/comments/cjokun/noise_contrastive_estimate_explained/,dukeleimao,1564473583,,0,1
2019,2019-7-30,2019,7,30,17,cjorx0,Look for a BI Solution with R Integration!,https://www.reddit.com/r/MachineLearning/comments/cjorx0/look_for_a_bi_solution_with_r_integration/,ElegantMicroWebIndia,1564475006,,0,1
2020,2019-7-30,2019,7,30,17,cjorzd,[D] Predicting Fantasy Football Points for German Bundesliga (kicker.de),https://www.reddit.com/r/MachineLearning/comments/cjorzd/d_predicting_fantasy_football_points_for_german/,thecluelessguy90,1564475022,"Hey guys. This years fantasy football season is about to start. So I decided to do a little project and try to predict each players points for the upcoming season. At first I searched for other people who have done similar stuff. Most of it was in US Sports (NFL, NBA, MLB). One reason imo was the availability of data. Other projects on football were done for week to week predicitons, and mostly in the middle of the season. This is not what I am going for.

Speaking of data availability. I had to put a lot of effort into getting historic fantasy points. But eventually I managed to obtain them by scraping the website ([kicker.de](https://kicker.de)) and recalculating the points from season 00/01 onwards (makes about \~9k datapoints). As for features I have, name, club, position and age. The target value is obv the fantasy score by the end of the season. The scores is heavily influenced by Rating (1.0-6, German school grading system. 1.0 best, 6 worst) and is done by editors of the newspaper. This is a human perception rating, not a statistic derived one!

I used mainly sklearn. My metric to optimize is mean-squared-error (MSE) and I tried several linear regression methods (Lasso, ElasticNet and BayesianRidge). I got results around 2000 MSE. Some hyperparameter tuning later I got to \~1650.Then I thought of giving LGBM a go, but it was actually worse, even with hyperparameter tuning. I thought about trying LSTM, but I think the dataset is way to small for that.

To be hones I am DISAPPOINTED with the results. \~1650 MSE is about 41 points of error on AVERAGE. Thats alot. So my next idea was to analyse if the model is bad on all the data, but good at particular areas. All this was done for the 2018/19 season.

&amp;#x200B;

[left-to-right \(Goalkeeper, Defenders, Midfield, Forward\)](https://i.redd.it/8b1t6511ded31.png)

This is the mean-error per position. GK seems to be really hard to predict. One thing thats common knowledge is that from all regular players goalies tend to score a lot of points on average. But ofc there is just one per team playing and they are less likely to get injured or even be left out because of fatigue. Defenders seem the easiest, Midfielders slightly worse and Forwards even more. Those positions are kind of reasonable good, but still not in the are were I would be confident.

&amp;#x200B;

[y = mean error, x=amount of players](https://i.redd.it/8x2e4v8sded31.png)

For this one I put the data into bins of points (-75,250, 25). Real in this context is the actual points that were scored and predicted, the predicted one. So I basically wanted to see the point distribution of the real points and my predicted points. And as it turns out the model is putting way to many players in the (26,50) range. Bot distributions kind of have the same shape, but the spread for the predicted ones is not big enough.

&amp;#x200B;

[y = mean error, x= bins of points](https://i.redd.it/46k1poxteed31.png)

So my idea was, maybe the model is good in some point range. And schockingly, where most of our data for predictions is, there is most of our error. The model is quite good and predicting the top end players (150+ points) and quite good to sort out the garbage( &lt;0 points), but inbetween its quite horrible. 

&amp;#x200B;

[x = teams , y = mean error](https://i.redd.it/6pvl9d5hfed31.png)

We see now the error for all the teams, two suprising points are the very accurate ones. Augsburg and Nrnberg are in the bottom half of the table, no suprise. Nrnberg was a promoted team and got relegated instantly, Augsburg survived but also fought against relegation all season long. On the other end Borussia Dortmund played a quite good season, fighting for the Championship with Bayern. The big error I assume is because of several factors. First they bought some players that performed quite well with no historic data (Witsel, Paco, Hakimi) or just very few (Sancho, Akanji, Delaney). Secondly they performed all quite well and above league average. So teamwise there doesn't seem to be much insight imo.

&amp;#x200B;

https://i.redd.it/qo8wegslged31.png

So I was interested if there is maybe some sort of correlation between age and points. Like very young players rarely score points, beginning to mid-late 20s is the prime. With eventual bumps and with the 30s they are declining. But as the boxplots suggest, the data is pretty much all over the place. We have huge variances right from the beginning. It really tones down in the 30s. Age also seems like a pretty bad feature to consider.

Additionally, I plotted some age/pts graphs for specific players. There are veterans, even on world class level (Neuer), but also from midtable teams, that never made it to international level. Also players with injury issues (Reus, Bender) or people who seemed like good international material and then completely vanished. Also some One-Hit-Wonders. Just have a peek. I put them in an imgur album to not make this post even bigger ([https://imgur.com/a/2kSVXdm](https://imgur.com/a/2kSVXdm))

So my question. What would be your ideas to improve the performance? More data? Feature Engineering (but what?). Maybe train a regressor per position?

P.S: if there is enough interest in the data and you guys wanna play around with it yourself I can publish it.",1,6
2021,2019-7-30,2019,7,30,17,cjp0x7,Julia Computing &amp; MIT Introduce Differentiable Programming System Bridging AI and Science,https://www.reddit.com/r/MachineLearning/comments/cjp0x7/julia_computing_mit_introduce_differentiable/,seismic_swarm,1564476895,,0,1
2022,2019-7-30,2019,7,30,18,cjp8nb,Useful outputs using GPT-2,https://www.reddit.com/r/MachineLearning/comments/cjp8nb/useful_outputs_using_gpt2/,ptrenko,1564478428,[removed],0,1
2023,2019-7-30,2019,7,30,18,cjpa9c,Get Affordable Packaging Machine Controller,https://www.reddit.com/r/MachineLearning/comments/cjpa9c/get_affordable_packaging_machine_controller/,melriya,1564478739,,0,1
2024,2019-7-30,2019,7,30,19,cjq1hd,Churn,https://www.reddit.com/r/MachineLearning/comments/cjq1hd/churn/,smemotech,1564483800,[removed],0,1
2025,2019-7-30,2019,7,30,19,cjq2c3,[P][R] ML in the embedded world,https://www.reddit.com/r/MachineLearning/comments/cjq2c3/pr_ml_in_the_embedded_world/,dimtass,1564483955,"The last couple weeks I've started experimenting with ML. As an electronic engineer I'm focus on the embedded domain and the last years on the embedded Linux domain. The last few months the semiconductor industry has turned to ML (they like to call it AI) and from now on almost all the new CPUs and MCUs are coming out with some kind of AI accelerator. The software support for that HW is still quite bad though, so there is plenty of HW and no SW, but it will get better in the future, I guess.

That said, I though that it was the right time to get involved and I wanted to experiment with ML in the low embedded and the Linux embedded domain, providing some real-working examples and source code for everything. The result, was a series of 5 blog posts which I'll list here with a brief description for each one.

1. [\[ML on embedded part 1\]](https://www.stupid-projects.com/machine-learning-on-embedded-part-1/): In this post there's an implementation of a naive implementation of 3-input, 1-output neuron that is benchmarked in various MCUs (stm32f103, stm32f746, arduino nano, arduino leonardo, arduino due, teensy 3.2, teensy 3.5 and the esp8266.
2. [\[ML on embedded part 2\]](https://www.stupid-projects.com/machine-learning-on-embedded-part-2/): In this post I've implemented another naive NN with 3-input, 32-hidden, 1-output. Again the same MCUs where tested.
3. [\[ML on embedded part 3\]](https://www.stupid-projects.com/machine-learning-on-embedded-part-3/): Here I've ported tensorflow lite for microcontrollers to build with cmake for the stm32f746 and I've also ported a MNIST keras model I've found from a book to tflite. I've also created a jupyter notebook that you can hand-draw a digit and then from within the notebook run the inference on the stm32.
4. [\[ML on embedded part 4\]](https://www.stupid-projects.com/machine-learning-on-embedded-part-4/): After the results I got from part 3, I thought it would be interesting to benchmark ST's x-cube-ai framework to do 1-to-1 comparisons with tflite-micro on the same model and MCU.
5. [\[ML on embedded part 5\]](https://www.stupid-projects.com/machine-learning-on-embedded-part-5/): As all the previous posts were about edge ML, I've implemented a cloud acceleration server using a Jetson nano and I developed a simple TCP server with python that also runs inferences in the same tflite model that I've used also in part 3 &amp; 4. Then I've written a simple firmware for the ESP8266 to send random input arrays serialized with flatbuffers to the ""AI cloud server"" via TCP and then get the result. I've run some benchmarks and did some comparisons with the edge implementation.

Although these are more interesting for embedded engineers, I think it also fits in here.",4,27
2026,2019-7-30,2019,7,30,20,cjqdah,Global Waterproof Drones Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/cjqdah/global_waterproof_drones_market_report_2019/,jadhavni3,1564485833,[removed],1,1
2027,2019-7-30,2019,7,30,20,cjqkl9,Machine Learning with AWS AI and IBM Watson,https://www.reddit.com/r/MachineLearning/comments/cjqkl9/machine_learning_with_aws_ai_and_ibm_watson/,HannahHumphreys,1564487026,[removed],0,1
2028,2019-7-30,2019,7,30,20,cjqpl1,Matching relevant paragraph to user query problem. Which of the two methods is better?,https://www.reddit.com/r/MachineLearning/comments/cjqpl1/matching_relevant_paragraph_to_user_query_problem/,0means,1564487836,[removed],0,1
2029,2019-7-30,2019,7,30,21,cjqxta,Amazing Processes You Must See | How Its Made Compilation,https://www.reddit.com/r/MachineLearning/comments/cjqxta/amazing_processes_you_must_see_how_its_made/,GoGadegets,1564489087,,0,1
2030,2019-7-30,2019,7,30,21,cjr4j5,Dealing with corporate restrictions,https://www.reddit.com/r/MachineLearning/comments/cjr4j5/dealing_with_corporate_restrictions/,InvertedChicken,1564490049,[removed],0,1
2031,2019-7-30,2019,7,30,22,cjry8b,"tuesday tricks tutorials the eagle combo switch, 3 pointed illusion roll",https://www.reddit.com/r/MachineLearning/comments/cjry8b/tuesday_tricks_tutorials_the_eagle_combo_switch_3/,thetrickshotone,1564494203,,0,1
2032,2019-7-30,2019,7,30,23,cjshm9,Machine Learning in Automobile Market Set for Rapid Growth and Trend 2019-2024,https://www.reddit.com/r/MachineLearning/comments/cjshm9/machine_learning_in_automobile_market_set_for/,gloryd0503,1564496757,,0,1
2033,2019-7-30,2019,7,30,23,cjsq44,XGBoost doesnt follow the other models,https://www.reddit.com/r/MachineLearning/comments/cjsq44/xgboost_doesnt_follow_the_other_models/,drainbamagex,1564497847,[removed],0,2
2034,2019-7-31,2019,7,31,0,cjszgs,[D] XGBoost doesnt follow the others models,https://www.reddit.com/r/MachineLearning/comments/cjszgs/d_xgboost_doesnt_follow_the_others_models/,drainbamagex,1564498993,"Hello everyone,

I trained my data by different techniques: dimensional analysis (DA), support vector machine (SVM), multi-layer perceptron neural network (ANN) and XGBoost (XGB).The lowest RMSE achieved in the testset was by XGB. However when I tried some combinations of input data the curve was different from other models.

&amp;#x200B;

For example:

https://i.redd.it/ol6azkkrggd31.png

&amp;#x200B;

This phenomenon is theoretically expressed as a power of type y(x) = a \* x \^ b. What are the possible causes for the model curve predicted by XGB not to follow the other models?

&amp;#x200B;

Assumptions:

&amp;#x200B;

(i) unbalanced continuous data of x or even y (target)? Histogram in [https://i.imgur.com/EGg8eNN.png](https://i.imgur.com/EGg8eNN.png)

(ii) hyperparameters (a good mapping of max\_depth, min\_child\_weight, gamma, eta and adding regularizer parameters was tested)

(iii) nature of decision tree conditions

&amp;#x200B;

Is there a way I can better generalize (as a post-prune) my model to fit it?

&amp;#x200B;

Many thanks!",10,1
2035,2019-7-31,2019,7,31,0,cjt074,[D] Do you use Meta-Learning ?,https://www.reddit.com/r/MachineLearning/comments/cjt074/d_do_you_use_metalearning/,etienne_ben,1564499075,"I have recently published this on meta-learning algorithms for classifying images with few examples:

[https://blog.sicara.com/meta-learning-for-few-shot-computer-vision-1a2655ac0f3a](https://blog.sicara.com/meta-learning-for-few-shot-computer-vision-1a2655ac0f3a)

So far I've only used meta-learning to solve few-shot tasks (few-shot classification, few-shot regression, etc ...). What am I missing ?",12,15
2036,2019-7-31,2019,7,31,0,cjt1bn,What Inputs Do I Need For My NN,https://www.reddit.com/r/MachineLearning/comments/cjt1bn/what_inputs_do_i_need_for_my_nn/,SteamSaajj,1564499196,[removed],0,1
2037,2019-7-31,2019,7,31,0,cjt6ub,[R] Tracking chewing patterns in cats using DeepLabCut,https://www.reddit.com/r/MachineLearning/comments/cjt6ub/r_tracking_chewing_patterns_in_cats_using/,MasterScrat,1564499870,,1,1
2038,2019-7-31,2019,7,31,0,cjt8if,"Yoo! Hello, people) I've been working on CLI tool that's gonna help with converting and dataset augmentation for CNNs or GANs or even other thing that needs images as input data. Here it is https://github.com/liashchynskyi/rudi star the repo, if u like it! thanks, hope this tool will help you",https://www.reddit.com/r/MachineLearning/comments/cjt8if/yoo_hello_people_ive_been_working_on_cli_tool/,liashchynskyi,1564500061,[removed],0,1
2039,2019-7-31,2019,7,31,0,cjtd3l,Machine Learning and AI to become a major part of APM in 2020,https://www.reddit.com/r/MachineLearning/comments/cjtd3l/machine_learning_and_ai_to_become_a_major_part_of/,modelop,1564500620,,0,1
2040,2019-7-31,2019,7,31,0,cjtdch,"[P] Small, fast and simple Python CLI image converter for CNNs.",https://www.reddit.com/r/MachineLearning/comments/cjtdch/p_small_fast_and_simple_python_cli_image/,liashchynskyi,1564500661,"Hello, people) I've been working on CLI tool that's gonna help with converting and dataset augmentation for CNNs or GANs or even other thing that needs images as input data. Here it is [https://github.com/liashchynskyi/rudi](https://github.com/liashchynskyi/rudi)

&amp;#x200B;

https://i.redd.it/labpkay2mgd31.png

Support  the repo with a star, if you like it! Thanks, hope this tool will help you))",3,1
2041,2019-7-31,2019,7,31,0,cjtf09,Machine Learning and AI to become a major part of APM in 2020,https://www.reddit.com/r/MachineLearning/comments/cjtf09/machine_learning_and_ai_to_become_a_major_part_of/,Unprotectedtxt,1564500860,,0,1
2042,2019-7-31,2019,7,31,0,cjtkp3,[N] Machine Learning and AI to become a major part of APM in 2020,https://www.reddit.com/r/MachineLearning/comments/cjtkp3/n_machine_learning_and_ai_to_become_a_major_part/,Unprotectedtxt,1564501528,,0,1
2043,2019-7-31,2019,7,31,0,cjtmi8,[D] What I'd like to write in my NeurIPS rebuttal,https://www.reddit.com/r/MachineLearning/comments/cjtmi8/d_what_id_like_to_write_in_my_neurips_rebuttal/,SoFarFromHome,1564501751,"We thank the reviewers for the detailed comments, of which some were even based on our paper.

To the reviewer that said our paper was ""underdeveloped"" because we didn't use a different methodology Y from field Z, we'd like to point out that a) this is in field A, b) we provided a framework for how to extend this to other methodologies in field A, and c) methodology Y has no obvious way to extend to the problem we're addressing (and doing so would be a whole paper in its own right).  Do you often read papers and get frustrated that they aren't the papers you've written?

To the same reviewer, who asked why we didn't cite papers Z1 and Z2, we would again point out that this isn't field Z and those papers have no relevance to the topic at hand except that you'd have written a paper on a different topic, which we didn't.

To the reviewer that asked why we didn't cite X, we'd like to point out that we did cite X, and had a whole paragraph discussing the relationship of this work to that one.

To the reviewer that proposed an example dataset to evaluate our model on, we point out that we already evaluate the model on that data set; see our Experiments section.

To the reviewer that pointed out that our method won't work when assumption 3 isn't met, yes, you're correct.  That's why we stated it as an assumption.  Congratulations on your reading comprehension.

To the reviewer that directly copy/pasted our introduction into the ""what 3 things does this paper contribute"" box, we'll be sure to include in future revisions a copy/paste-able review justifying ""score 10, confidence 5"" to make your review easier.  That you also mischaracterized our main claim and completely missed the discussion on relationship to prior work makes your review particularly useful to development of the work.

To the reviewer that wrote that, while THEY were familiar with the definitions in a reference, we should explain it for readers that might be confused, we understand entirely.  We'll gladly explain it for ""a friend of yours"", err ""readers"", and not you, because you get it and you're smart and it's just the readers who don't.

To the reviewer who commented that our results were ""contradictory"" because we said that our modification ""in general performed slightly worse"" on this metric, when in fact our plots show it sometimes performed better, we'll gladly fix our claim to be clear that ""in general"" doesn't mean ""always"" and also our results are even better than the previous wording indicated.

To the reviewer that said our comparison method's results were worse than reported in the original paper, we've carefully compared their bar charts to ours and found that the results are the same to the precision of the graphical printout in the previous paper.  If you could lend us your image sharpening function so we can get more significant digits out of their plot, we'd be glad to redo the comparison.

To the reviewer who used half of their review to argue that our entire subfield is dumb and wrong, we thank them for reaching across academic lines to provide commentary in an area that pains you deeply.

And finally, to the reviewers who called our paper (all actual quotes) ""original, well-motivated, and worthy of study"", ""important in its own right"", that said you ""greatly enjoyed reading this paper"" and that ""this is an interesting problem and certainly worth studying"" and that ""this paper identifies an important problem ... [and the authors] then present a simple"" solution, thank you for also marking this a reject.  Since all of you gave us scores between 5 and 3, neither the AC nor any of you will ever have to read this response or reconsider your scores before we are inevitably rejected, but we hope that your original, well-motivated, worth-studying, important, interesting, clear papers receive reviews of equal quality in the future!

/salt",95,388
2044,2019-7-31,2019,7,31,1,cjtxkg,Baidu's new language model outperforms BERT and XLNet in 16 NLP tasks,https://www.reddit.com/r/MachineLearning/comments/cjtxkg/baidus_new_language_model_outperforms_bert_and/,bayjingsf,1564503015,,1,4
2045,2019-7-31,2019,7,31,1,cju2ad,soweego: link Wikidata to large catalogs,https://www.reddit.com/r/MachineLearning/comments/cju2ad/soweego_link_wikidata_to_large_catalogs/,hell_j,1564503588,[removed],0,1
2046,2019-7-31,2019,7,31,1,cju50f,Help!! Rejected at multiple conferences!!,https://www.reddit.com/r/MachineLearning/comments/cju50f/help_rejected_at_multiple_conferences/,loyal_hyena,1564503902,[removed],0,1
2047,2019-7-31,2019,7,31,1,cju5yq,"How to approach a problem when the information is in the relationship between the points, and not the points itself?",https://www.reddit.com/r/MachineLearning/comments/cju5yq/how_to_approach_a_problem_when_the_information_is/,47884375,1564504007,[removed],0,1
2048,2019-7-31,2019,7,31,1,cju761,Baidu's new language model outperforms BERT and XLNet on 16 NLP tasks,https://www.reddit.com/r/MachineLearning/comments/cju761/baidus_new_language_model_outperforms_bert_and/,bayjingsf,1564504145,,20,34
2049,2019-7-31,2019,7,31,2,cjur2h,What do you know about GWO for training Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/cjur2h/what_do_you_know_about_gwo_for_training_neural/,abdeljalil73,1564506484,"Can someone tell me more about using Grey Wolf Optimizer for training neural networks?

All I found was few papers comparing it with other training algorithms and GWO scored the best on all datasets. Why isn't it used more instead of Gradient Descent and backpropagation?",0,1
2050,2019-7-31,2019,7,31,2,cjutka,[D] Does Unity machine learning allow for importing your own environment? If so where could I find out how to do that?,https://www.reddit.com/r/MachineLearning/comments/cjutka/d_does_unity_machine_learning_allow_for_importing/,Eltabre,1564506777,"I know that OpenAI's retro allows for custom imports if ROMs that aren't included I was wondering if Unity allows the same. Say if a friend gave you access to his game project, could you use Unity ML tools to create agents to play that game?",2,1
2051,2019-7-31,2019,7,31,2,cjuvu9,"Why does deep learning even work? Because the universe (and contents within) is deeply hierarchical, and so are deep neural networks.",https://www.reddit.com/r/MachineLearning/comments/cjuvu9/why_does_deep_learning_even_work_because_the/,nickbluth2,1564507056,,2,0
2052,2019-7-31,2019,7,31,2,cjv58x,How to Create A Concurrent and Parallel Stochastic Reinforcement Learning Environment For Crypto Trading,https://www.reddit.com/r/MachineLearning/comments/cjv58x/how_to_create_a_concurrent_and_parallel/,kivo360,1564508135,,1,1
2053,2019-7-31,2019,7,31,2,cjvgip,Is allowed here to ask about...,https://www.reddit.com/r/MachineLearning/comments/cjvgip/is_allowed_here_to_ask_about/,ph04,1564509455,"I'd like to work on the Google's QuickDraw dataset, but I can't figure out some thing, is this the place to ask or should I ask elsewhere?",0,1
2054,2019-7-31,2019,7,31,2,cjvgsm,[D] Developing on the cloud.,https://www.reddit.com/r/MachineLearning/comments/cjvgsm/d_developing_on_the_cloud/,whichoneisblue,1564509490,"Hey fellow Reddit Machine Learners, At what point do you guys transition your projects from working locally on your machines to developing directly on the cloud? I find the transition not particularly smooth. I am curious about your experience. Have you considered starting directly on a cloud machine?",10,4
2055,2019-7-31,2019,7,31,3,cjvj3u,Baidus ERNIE 2.0 Beats BERT and XLNet on NLP Benchmarks,https://www.reddit.com/r/MachineLearning/comments/cjvj3u/baidus_ernie_20_beats_bert_and_xlnet_on_nlp/,Yuqing7,1564509764,,1,1
2056,2019-7-31,2019,7,31,3,cjwaz4,Dotscience Emerges from Stealth To Solve Pains When Operationalizing AI,https://www.reddit.com/r/MachineLearning/comments/cjwaz4/dotscience_emerges_from_stealth_to_solve_pains/,mrmrcoleman,1564513036,[removed],0,1
2057,2019-7-31,2019,7,31,3,cjwbxo,Anyone know the answer to?,https://www.reddit.com/r/MachineLearning/comments/cjwbxo/anyone_know_the_answer_to/,pz400,1564513151,[removed],1,1
2058,2019-7-31,2019,7,31,4,cjwk23,(Self promotion) would appreciate if you voted for my (@mat_mto) NLP meme here (x-post r/LanguageTechnology),https://www.reddit.com/r/MachineLearning/comments/cjwk23/self_promotion_would_appreciate_if_you_voted_for/,mt0,1564514102,,0,1
2059,2019-7-31,2019,7,31,4,cjwp09,How to speed up object detection while using faster rcnn/ ssd models,https://www.reddit.com/r/MachineLearning/comments/cjwp09/how_to_speed_up_object_detection_while_using/,dafli_walaa,1564514697,[removed],1,1
2060,2019-7-31,2019,7,31,4,cjx4r6,[R] Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving,https://www.reddit.com/r/MachineLearning/comments/cjx4r6/r_pseudolidar_accurate_depth_for_3d_object/,downtownslim,1564516573,,0,1
2061,2019-7-31,2019,7,31,6,cjyaag,Facebook AI Research Is A Game-Changer,https://www.reddit.com/r/MachineLearning/comments/cjyaag/facebook_ai_research_is_a_gamechanger/,OPPA_privacy,1564521617,,0,1
2062,2019-7-31,2019,7,31,9,ck0o2m,[P] Help with PLS in R,https://www.reddit.com/r/MachineLearning/comments/ck0o2m/p_help_with_pls_in_r/,reasonablemanzach,1564532683,"Hello [r](https://www.reddit.com/r/datascience/)/MachineLearning \~

Struggling Noob here. I have been asked to create a pls model on a dataset with 200 predictor variables and 1 response variable(y below). I have only 16 observations. I know the n is small and predictors are high, but these are outside of my control.

I have made a model, with the code below, but I have little confidence it is right. I don't know how to properly test it. The summary says after 2 components, 95% is explained.

My Questions:

1. How do I see what variables are in those components.
2. How do I test the model [pls.fit](https://pls.fit/) or determine the R\^2

Thank you so much for any help. If anyone has strong knowledge here, I'd be happy to pay for a quick tutoring session.

[`pls.fit`](https://pls.fit/) `&lt;- plsr(df$y~.,data = df, scale = TRUE,validation = ""LOO"")`

`summary(`[`pls.fit`](https://pls.fit/)`)`

`validationplot(`[`pls.fit`](https://pls.fit/)`)`",2,0
2063,2019-7-31,2019,7,31,10,ck1f13,[P] Skin Cancer Screening - Web DEMO,https://www.reddit.com/r/MachineLearning/comments/ck1f13/p_skin_cancer_screening_web_demo/,whria78,1564536496,"This is an experimental web-demo for skin cancer screening.

I used region-based CNN (faster-RCNN) to detect and utilized CNN (SENet) to classify.

The algorithm detects suspected lesion and shows malignancy score and predict possible diagnosis (178 disease classes).

The attached image is **seborrheic keratosis of** [**wikipedia.org**.](https://l.facebook.com/l.php?u=https%3A%2F%2Fwikipedia.org%2F%3Ffbclid%3DIwAR3lYS9vgzp9TtfPMw2tspAE_tOpPA0KFCczHZ-GPIUe5hokElq_N4sKKGU&amp;h=AT0OuIE0F7clOmkOSXW0tb8EWLV-XIfl51p8l-T_63YyzUHjq3wasenZYxw3OK8tgFEi6H8YzPSffOu5kn_KQtaWLb-KqLKf1c7YGneZmv7JExeAWyN4O2BlstZivBszqKrZ3jjdLb8rYvcHhEiQa64)

Welcome any question or suggestion, and please report any problem or delay in using web-DEMO.

Thank you.

&amp;#x200B;

I am running the DEMO temporarily to test false-positive and get feedback. The DEMO will be closed or moved a few weeks later.

**DEMO :** [**http://rcnn.modelderm.com**](https://l.facebook.com/l.php?u=http%3A%2F%2Frcnn.modelderm.com%2F%3Ffbclid%3DIwAR1YCU2o-omNTY2lMCJ4w5J3zeOCpKfqdbN4R7jKW2o7tdRZR1rhKh6uahs&amp;h=AT2BFCdg8TSjY9vrQztjQanrLFb55GT_n1sQG4LpweMLAb4BnKlRacNt6_w7zszWKspV2EOqycuJ8WQcEfyx5T9hwrs_k7A40beooujeuhO2tsggKCJ9Ywnwyhp-_F86-HgI7Yj_dJhpV4oIXSxcT0A)

**Screenshot :** [**https://i.imgur.com/HQPJJ08.png**](https://i.imgur.com/HQPJJ08.png)

**Facebook thread :** [**https://www.facebook.com/groups/DeepNetGroup/permalink/914063952319821/**](https://www.facebook.com/groups/DeepNetGroup/permalink/914063952319821/)",0,1
2064,2019-7-31,2019,7,31,11,ck1sze,[P] Approximating product of a discrete and continuos distribution in a mixture model,https://www.reddit.com/r/MachineLearning/comments/ck1sze/p_approximating_product_of_a_discrete_and/,cuenta4384,1564538541,"I am working in a mixture model with expectation propagation. When I am trying to compute the normalizing constant, \_i, I have an integral that is not closed form.  

My integral looks as follows: 

&amp;#x200B;

[Integral to solve](https://i.redd.it/d098qkipojd31.png)

B\_j is a continuous value that is usually &lt;&lt; 1. The mean, and variance are known values. The EDCM distribution is a bounded Dirichlet-multinomial distribution. This will result in a discrete discrete distribution since EDCM is discrete.   


1. Can Laplace approximation be used since the distributions to be approximated are not completely continuous? 
2. Can I use BBVI? Posterior p(B|x) \\propto p(x,B) with an approximation q(B) that is gaussian. The approximation will be continuous, so can I use only the normalization factor of the approximate distribution to approximate p(x) (ie 1/(2\\pi)\^(V/2)|\\sigma|\^1/2). Will this value be considered a proper discrete distribution? 
3. Any other suggestions? Maybe just sample (replace the integral with a summation). I have to iterate many times this approximation.",2,5
2065,2019-7-31,2019,7,31,11,ck21vi,Need help converting pytorch model to onnx,https://www.reddit.com/r/MachineLearning/comments/ck21vi/need_help_converting_pytorch_model_to_onnx/,ewelumokeke,1564539847,[removed],0,1
2066,2019-7-31,2019,7,31,11,ck2cmn,Looking for an existing application that makes a good use-case for a recommender system integration.,https://www.reddit.com/r/MachineLearning/comments/ck2cmn/looking_for_an_existing_application_that_makes_a/,BlueStar1196,1564541428,"Hi everyone, I'm a CS Masters student with a focus on ML and some past experience working with ML/DL projects. As a side project, I'm planning on developing a recommender system capable of giving real-time recommendations/suggestions to users based on their activity/click-stream. I want to integrate it in a real, live application rather than just training and testing on some static dataset(which are plenty out there). It'd be a great engineering problem and learning experience for me, and I'm constantly looking forward to such projects to build my profile.

I am looking for an existing application where such an integration would be useful. It can be on any platform(Web or Android would be a big plus), and can be related to anything, as long as it makes a good use-case for integration of such a system. The project will specifically require access to user's activity(no personal identity info needed of course), and the developer should have enough access to the application code to push new changes. I am open to collaborating with the app developer on this. Ideally it'd have at least some active users.

If you maintain or know of someone who maintains such an application, please let me know here or in DM. It'd be a win-win situation for both me and the developer. Also, I know I may find such projects on some freelancing platform(which I am trying to), but ATM I don't have sufficient history of working on freelancing projects to show to people, as I haven't been active on those platforms before.

Thanks in advance!",0,1
2067,2019-7-31,2019,7,31,13,ck37n2,Final Year Project,https://www.reddit.com/r/MachineLearning/comments/ck37n2/final_year_project/,aayu283,1564546156,,0,1
2068,2019-7-31,2019,7,31,13,ck38ur,AWS Textract - closest competitor?,https://www.reddit.com/r/MachineLearning/comments/ck38ur/aws_textract_closest_competitor/,lppier,1564546354,[removed],0,1
2069,2019-7-31,2019,7,31,13,ck3a3k,Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/ck3a3k/machine_learning_course/,dwivediabhinav,1564546548,,0,1
2070,2019-7-31,2019,7,31,13,ck3ood,"It's high time that we should act against degrading quality of conferences, and research works! [Discussion]",https://www.reddit.com/r/MachineLearning/comments/ck3ood/its_high_time_that_we_should_act_against/,gostewpid,1564548906,"As suggested by the title, we all know how frustrating it could be to get a paper published in very popular journals, not only they have a poor quality of reviews, but also due to the fact that a LOT of scavengers haunt these places. These people, all they want is credit, and fame from the works of others.

Not only this, these guys will literally stalk you of your academic research just to get that subtle hint of \[this guy's work might be related to mine, as well as accuse him of ""paper is not cited by you""\] . I mean common guys, this is a place where science is to be developed, not to get on each others throat for some petty gains.

My next argument is the shit qualities of reviewers on the research paper conferences (I will  not name them). Reviewers literally point out issues that are trivial, and aimed to delay the acceptance date of the paper. They will post poor quality of reviews on papers, some will go even further to flame the researcher and start an argument.

This scenario is so fucked up, demotivates me to core, and makes me never want to submit a paper to these publications/conferences.",9,0
2071,2019-7-31,2019,7,31,13,ck3p37,Whats required to more or less reconstruct AlphaGo for other games?,https://www.reddit.com/r/MachineLearning/comments/ck3p37/whats_required_to_more_or_less_reconstruct/,Cheddarific,1564548970,"I have a goal to create an AI for one of my favorite board games, Smash Up. It has some challenges that may add complexity to the programming. The key challenges are listed at the bottom of this post. 

An online acquaintance of mine who has made AI for games before suggested I use an MCTS, which seems useful but may not factor in that the best move is often much much better than a random move. 

I spent the evening listening to YouTube videos about AlphaGo Zero and wonder whether I (team of one) could ever hope to build something like that. Then it struck me that even if I could build it, it might be too expensive for me to actually run it for a couple hours. Does anyone have an idea of how much it would cost to run something like that for a day or so? I see prices online, but I dont know what pricing tier would be required for this kind of project. 

Also, does anyone think its possible for one person to do this over the course of 12 months?

Unique aspects of Smash Up that may complicate matters:
1. Each player has a deck of shuffled cards (hidden information).
2. The card abilities have varying levels of complexity that can create broad decision trees, especially since cards in play can often be used and ordered before or after those in hand.
3. Its possible to play with more than just two players, but I think the most value for an AI would be at 1v1.
4. Each players deck is created by shuffling all 20 cards of one chosen faction with all 20 cards of another (think Zombie Pirates or Ancient Egyptian Sharks). There are 74 factions.",0,1
2072,2019-7-31,2019,7,31,13,ck3pes,Any reviews/suggestions about the Springboard Machine Learning Bootcamp?,https://www.reddit.com/r/MachineLearning/comments/ck3pes/any_reviewssuggestions_about_the_springboard/,BlueStar1196,1564549027,[removed],0,1
2073,2019-7-31,2019,7,31,14,ck48ss,Custom Objective Function for XGBoost (in R) for dealing with inbalanced data &amp; focus on true positives,https://www.reddit.com/r/MachineLearning/comments/ck48ss/custom_objective_function_for_xgboost_in_r_for/,weightsandbayes,1564552330,[removed],0,1
2074,2019-7-31,2019,7,31,15,ck4do7,[R] [1907.10830] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation,https://www.reddit.com/r/MachineLearning/comments/ck4do7/r_190710830_ugatit_unsupervised_generative/,taki0112,1564553185,"&amp;#x200B;

![img](4r40q22dxkd31 ""1st row : input, 2nd row : attention map, 3rd row : output"")

**Abstract**

&gt;We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. **The attention module** guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based methods which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new **AdaLIN (Adaptive Layer-Instance Normalization)** function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a **fixed network architecture and hyper-parameters.**

&amp;#x200B;

* **paper :** [https://arxiv.org/abs/1907.10830](https://arxiv.org/abs/1907.10830)
* **Official Tensorflow :** [https://github.com/taki0112/UGATIT](https://github.com/taki0112/UGATIT)
* **Official Pytorch :** [https://github.com/znxlwm/UGATIT-pytorch](https://github.com/znxlwm/UGATIT-pytorch)",25,38
2075,2019-7-31,2019,7,31,15,ck4lc8,[P] Implementing MixNet: Mixed Depthwise Convolutional Kernels using Pytorch,https://www.reddit.com/r/MachineLearning/comments/ck4lc8/p_implementing_mixnet_mixed_depthwise/,leaderj1001,1564554567,"Hi, I'm Myeongjun Kim. My major is Computer Vision using deep learning. ""MixNet: Mixed Depthwise Convolutional Kernels"" paper has been uploaded to the archive on 22 july 2019. This paper improves performance and efficiency by changing kernel size. I have implemented pytorch version with reference to official code implemented with tensorflow. Currently I'm experimenting with CIFAR-10 because of the lack of gpu.

Thank you for reading my story :)  


\[Github URL\]:  [https://github.com/leaderj1001/Mixed-Depthwise-Convolutional-Kernels](https://github.com/leaderj1001/Mixed-Depthwise-Convolutional-Kernels)",2,8
2076,2019-7-31,2019,7,31,15,ck4s9b,Athena &amp; Rough Terrain Bi Levelling Scissor lift - Duralift,https://www.reddit.com/r/MachineLearning/comments/ck4s9b/athena_rough_terrain_bi_levelling_scissor_lift/,zoejones886,1564555864,,0,1
2077,2019-7-31,2019,7,31,16,ck51xw,JLG Electric Scissor Lifts for hire - Duralift,https://www.reddit.com/r/MachineLearning/comments/ck51xw/jlg_electric_scissor_lifts_for_hire_duralift/,zoejones886,1564557637,,0,1
2078,2019-7-31,2019,7,31,16,ck52yu,Which KMeans implementation I should start as firt?,https://www.reddit.com/r/MachineLearning/comments/ck52yu/which_kmeans_implementation_i_should_start_as_firt/,l36807,1564557831,"Hi,

I try to search KMeans implementation lib. I found most pages are about scikit-learn KMeans implementation. And I see scikit-learn provides better introduction and example than Tensorflow website.

But I also found KMeans lib of Tensorflow\`s KMeansClustering.

Can I get some suggestion which one I should start to use? If I am not for studying how KMeans works, I want to use it in daily work.

Many thanks!

Rgds,

Jingang Li",0,1
2079,2019-7-31,2019,7,31,16,ck54db,https://play.google.com/store/apps/details?id=com.qa.machinelearningfromscratch2,https://www.reddit.com/r/MachineLearning/comments/ck54db/httpsplaygooglecomstoreappsdetailsidcomqamachinele/,codingislife496,1564558104,[removed],0,1
2080,2019-7-31,2019,7,31,16,ck55px,[D] does your lab use a ticket management software? If so which one?,https://www.reddit.com/r/MachineLearning/comments/ck55px/d_does_your_lab_use_a_ticket_management_software/,CartPole,1564558381,I'm curious what ticket management software(if any) research labs are using. I'm especially curious about university research labs. My group is kicking around the idea of using one and I'm not sure if it makes that much sense for academic research.,7,1
2081,2019-7-31,2019,7,31,16,ck5636,Which KMeans implementation I should start at first?,https://www.reddit.com/r/MachineLearning/comments/ck5636/which_kmeans_implementation_i_should_start_at/,l36807,1564558452,[removed],0,1
2082,2019-7-31,2019,7,31,16,ck565g,[P] PyCM 2.4 released : Multi-class confusion matrix library in Python,https://www.reddit.com/r/MachineLearning/comments/ck565g/p_pycm_24_released_multiclass_confusion_matrix/,sepandhaghighi,1564558462," 

[https://www.pycm.ir](https://www.pycm.ir/)

[https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

&amp;#x200B;

* Tversky index (TI) added [\#214](https://github.com/sepandhaghighi/pycm/issues/214)
* Area under the PR curve (AUPR) added [\#216](https://github.com/sepandhaghighi/pycm/issues/216)
* FUNDING.yml added
* AUC\_calc function modified
* Document modified [\#225](https://github.com/sepandhaghighi/pycm/issues/225)
* summary parameter added to save\_html,save\_stat,save\_csv and stat methods [\#217](https://github.com/sepandhaghighi/pycm/issues/217)
* sample\_weight bug in numpy array format fixed [\#227](https://github.com/sepandhaghighi/pycm/issues/227)
* Inputs manipulation bug fixed [\#226](https://github.com/sepandhaghighi/pycm/issues/226)
* Test system modified [\#229](https://github.com/sepandhaghighi/pycm/issues/229)
* Warning system modified [\#228](https://github.com/sepandhaghighi/pycm/issues/228)
* alt\_link parameter added to save\_html method and online\_help function [\#232](https://github.com/sepandhaghighi/pycm/issues/232)
* Compare class tests moved to compare\_test.py
* Warning tests moved to warning\_test.py",12,88
2083,2019-7-31,2019,7,31,17,ck5oa1,Amazing Working Inventions That Are Next Level  29,https://www.reddit.com/r/MachineLearning/comments/ck5oa1/amazing_working_inventions_that_are_next_level_29/,GoGadegets,1564562157,,0,1
2084,2019-7-31,2019,7,31,17,ck5q3x,Any examples of xgboost and Pyspark integration?,https://www.reddit.com/r/MachineLearning/comments/ck5q3x/any_examples_of_xgboost_and_pyspark_integration/,kookaburro,1564562530,[removed],0,1
2085,2019-7-31,2019,7,31,19,ck6gq1,Speech files from UCI parkinsons speech repository available?,https://www.reddit.com/r/MachineLearning/comments/ck6gq1/speech_files_from_uci_parkinsons_speech/,sathvik66,1564567753,[removed],0,1
2086,2019-7-31,2019,7,31,19,ck6jx2,[R] [BMVC2019 oral presentation] Graph-based Knowledge Distillation by Multi-head Attention Network,https://www.reddit.com/r/MachineLearning/comments/ck6jx2/r_bmvc2019_oral_presentation_graphbased_knowledge/,sseung0703,1564568353,[removed],0,1
2087,2019-7-31,2019,7,31,19,ck6pv4,Global Wristwatch Market Report 2019,https://www.reddit.com/r/MachineLearning/comments/ck6pv4/global_wristwatch_market_report_2019/,jadhavni3,1564569447,[removed],1,1
2088,2019-7-31,2019,7,31,19,ck6uuq,[P] Book Recommendation Engine - Someone wanna join me?,https://www.reddit.com/r/MachineLearning/comments/ck6uuq/p_book_recommendation_engine_someone_wanna_join_me/,BeggarInSpain,1564570348,"Hello, I've stumbled upon an extremely interesting dataset [http://www2.informatik.uni-freiburg.de/\~cziegler/BX/](http://www2.informatik.uni-freiburg.de/~cziegler/BX/) regarding book recommendation engine. The dataset has been already uploaded on Kaggle but it's not public. Thus, I'd like to share it here for those who want to try by themself or are open to co-operate on the project with me. I've already done some construct on collaborative filtering, content-based filtering, svd and matrix factorization, ...

I've uploaded my notebook here:   [https://gofile.io/?c=6lMojp](https://gofile.io/?c=6lMojp)

Feel free to download or pm me for sharing our progress. 

Cheers!",17,22
2089,2019-7-31,2019,7,31,20,ck70rn,Now Clone Facebooks AI team Releases Detectron in single Click.,https://www.reddit.com/r/MachineLearning/comments/ck70rn/now_clone_facebooks_ai_team_releases_detectron_in/,Zerotool1,1564571346,[removed],0,1
2090,2019-7-31,2019,7,31,20,ck7af3,Are Graph Convolution Networks (GCN) with graph constraints possible?,https://www.reddit.com/r/MachineLearning/comments/ck7af3/are_graph_convolution_networks_gcn_with_graph/,Muunich,1564572974,[removed],0,1
2091,2019-7-31,2019,7,31,20,ck7c6r,The proof of concept #5 nunchaku wrist rolls,https://www.reddit.com/r/MachineLearning/comments/ck7c6r/the_proof_of_concept_5_nunchaku_wrist_rolls/,thetrickshotone,1564573256,,0,1
2092,2019-7-31,2019,7,31,20,ck7hb8,GOTO 2019  From Tic Tac Toe to AlphaGo: Playing games with AI  Roy van Rijn,https://www.reddit.com/r/MachineLearning/comments/ck7hb8/goto_2019_from_tic_tac_toe_to_alphago_playing/,goto-con,1564574061,[removed],0,1
2093,2019-7-31,2019,7,31,21,ck7kg8,3 Reasons why AI Assisted Labeling will destroy Manual labor market,https://www.reddit.com/r/MachineLearning/comments/ck7kg8/3_reasons_why_ai_assisted_labeling_will_destroy/,tdionis,1564574548,,0,1
2094,2019-7-31,2019,7,31,21,ck7lwu,How Do I Prove the Value of Self-Serve Augmented Data Discovery?,https://www.reddit.com/r/MachineLearning/comments/ck7lwu/how_do_i_prove_the_value_of_selfserve_augmented/,ElegantMicroWebIndia,1564574753,,0,1
2095,2019-7-31,2019,7,31,21,ck7od3,[D] What is the state of the art approach for action recognition from videos?,https://www.reddit.com/r/MachineLearning/comments/ck7od3/d_what_is_the_state_of_the_art_approach_for/,LessTell,1564575145,I was looking for state of the art works on action recognition. What are some of best works of recent times?,6,6
2096,2019-7-31,2019,7,31,21,ck7ryb,Human pose estimation on images for iOS using CoreML,https://www.reddit.com/r/MachineLearning/comments/ck7ryb/human_pose_estimation_on_images_for_ios_using/,atomlib_com,1564575707,,0,1
2097,2019-7-31,2019,7,31,22,ck8ffz,"Today I saw this deep learning project. First I can't believe the results, Second I can't believe their good intention and I think it is very racist. I'm posting here to ask your view on this. do you think that they are honest? is it possible to do this? (just read how they labled the data!)",https://www.reddit.com/r/MachineLearning/comments/ck8ffz/today_i_saw_this_deep_learning_project_first_i/,kiasari,1564579263,,0,1
2098,2019-7-31,2019,7,31,22,ck8gko,"Introducing tf-explain, Interpretability for TensorFlow 2.0",https://www.reddit.com/r/MachineLearning/comments/ck8gko/introducing_tfexplain_interpretability_for/,raphaelmeudec,1564579434,,0,1
2099,2019-7-31,2019,7,31,22,ck8lus,"[P] Introducing tf-explain, Interpretability for TensorFlow 2.0",https://www.reddit.com/r/MachineLearning/comments/ck8lus/p_introducing_tfexplain_interpretability_for/,raphaelmeudec,1564580229,,0,5
2100,2019-7-31,2019,7,31,22,ck8nfs,"If you're thinking about using the edX app to learn Quantum Machine Learning, be advised that you can't actually complete the course with the app.",https://www.reddit.com/r/MachineLearning/comments/ck8nfs/if_youre_thinking_about_using_the_edx_app_to/,Agent_ANAKIN,1564580450,,0,1
2101,2019-7-31,2019,7,31,22,ck8rm0,[N] New $1 million AI fake news detection competition,https://www.reddit.com/r/MachineLearning/comments/ck8rm0/n_new_1_million_ai_fake_news_detection_competition/,leadersprize,1564581059,"[https://leadersprize.truenorthwaterloo.com/en/](https://leadersprize.truenorthwaterloo.com/en/)

The  Leaders Prize will award $1 million to the team who can best use artificial intelligence to automate the fact-checking process and flag whether a claim is true or false. Not many teams have signed up yet, so we are posting about the competition here to encourage more teams to participate.",85,319
2102,2019-7-31,2019,7,31,23,ck90mo,Predicting election results of a indian constituency form 20.,https://www.reddit.com/r/MachineLearning/comments/ck90mo/predicting_election_results_of_a_indian/,Satnam1999,1564582280,[removed],1,1
2103,2019-7-31,2019,7,31,23,ck961j,How to publish!,https://www.reddit.com/r/MachineLearning/comments/ck961j/how_to_publish/,-Ulkurz-,1564583025,[removed],0,1
2104,2019-7-31,2019,7,31,23,ck9fvv,My first paper: Squeeze Nets (a type of CNN) for Stereo Matching,https://www.reddit.com/r/MachineLearning/comments/ck9fvv/my_first_paper_squeeze_nets_a_type_of_cnn_for/,sxsoft,1564584386,[removed],0,1
2105,2019-7-31,2019,7,31,23,ck9hzr,Machine Learning Infrastructure with Amazon SageMaker and Terraform  A Case of Fraud Detection,https://www.reddit.com/r/MachineLearning/comments/ck9hzr/machine_learning_infrastructure_with_amazon/,qtangs,1564584667,[removed],0,1
2106,2019-7-31,2019,7,31,23,ck9j5w,Question about Tesla Autopilot's road segmentation,https://www.reddit.com/r/MachineLearning/comments/ck9j5w/question_about_tesla_autopilots_road_segmentation/,abbuh,1564584807,[removed],0,1
