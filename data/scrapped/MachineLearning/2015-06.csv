,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2015-6-1,2015,6,1,10,381033,Prof. Dr. Max Welling explaining Deep Learning,https://www.reddit.com/r/MachineLearning/comments/381033/prof_dr_max_welling_explaining_deep_learning/,booketor,1433122662,,1,0
1,2015-6-1,2015,6,1,13,381hnf,Experiences with Target Propagation?,https://www.reddit.com/r/MachineLearning/comments/381hnf/experiences_with_target_propagation/,alexmlamb,1433131787,"I've been working through this paper on target propagation: 

http://arxiv.org/abs/1407.7906

http://arxiv.org/pdf/1412.7525v3.pdf

I'm wondering if anyone has implemented target propagation and studied the method empirically.  Does anyone have theano code?  Has anyone tried extending the method to RNNs?  ",1,2
2,2015-6-1,2015,6,1,13,381k5n,Is there any package joint training RNN and CNN?,https://www.reddit.com/r/MachineLearning/comments/381k5n/is_there_any_package_joint_training_rnn_and_cnn/,ihacku,1433133260,"For jointly training a RNN/CNN model, one needs to propagate the error from RNN to CNN. I've searched a lot and didn't find any joint RNN/CNN implementation. 

Do I have to scaffold RNNLIB[3] and Caffe[4] myself to do this? Is there any package already implemented this?

Specifically, I need a RNN package with MDLSTM[1] and CTC[2] implemented.

Thanks in advance.


**References:**

[1] A. Graves and J. Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks. NIPS 2008, Vancouver, Canada, pp. 545-552.

[2] A. Graves, S. Fernndez, F. Gomez, J. Schmidhuber. Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. ICML 2006, Pittsburgh, USA, pp. 369-376.

[3] http://sourceforge.net/projects/rnnl/

[4] http://caffe.berkeleyvision.org",9,3
3,2015-6-1,2015,6,1,20,382fjd,Sentiment Analysis using SVM,https://www.reddit.com/r/MachineLearning/comments/382fjd/sentiment_analysis_using_svm/,Murium,1433156451,Can anyone please explain me all the steps to classify  a text as positive or negative using SVM in simple words.I fully understood how to classify text using Naive Bayes. I would be very thankful.,5,1
4,2015-6-1,2015,6,1,20,382j8h,Tips for successful email marketing campaigns,https://www.reddit.com/r/MachineLearning/comments/382j8h/tips_for_successful_email_marketing_campaigns/,webseoanalytics,1433158981,,0,1
5,2015-6-1,2015,6,1,21,382n6e,Agrison Tractors,https://www.reddit.com/r/MachineLearning/comments/382n6e/agrison_tractors/,agrison,1433161429,,0,1
6,2015-6-1,2015,6,1,21,382pbq,Training of sentiment data using SVM,https://www.reddit.com/r/MachineLearning/comments/382pbq/training_of_sentiment_data_using_svm/,Murium,1433162648,"ELI5: Please explain the steps to train data for sentiment analysis of text using SVM.
",4,0
7,2015-6-1,2015,6,1,21,382r39,Why invest in Content Marketing,https://www.reddit.com/r/MachineLearning/comments/382r39/why_invest_in_content_marketing/,webseoanalytics,1433163583,,0,1
8,2015-6-1,2015,6,1,22,382srr,"Finally, Machine Learning without Writing Code",https://www.reddit.com/r/MachineLearning/comments/382srr/finally_machine_learning_without_writing_code/,tarpus,1433164403,,0,0
9,2015-6-2,2015,6,2,0,3837g0,Deep Learning algorithm trained to recognize gaze direction with computer-generated dataset,https://www.reddit.com/r/MachineLearning/comments/3837g0/deep_learning_algorithm_trained_to_recognize_gaze/,fabioperez,1433171187,,9,39
10,2015-6-2,2015,6,2,1,383lgd,Reinforcement noob question.,https://www.reddit.com/r/MachineLearning/comments/383lgd/reinforcement_noob_question/,ai_noob,1433176839,"I'm wondering how, in reinforcement learning you would treat multiple outputs during training.  For example let's say I have an agent that can move up, down, left, right.  I am using temporal difference learning with a neural net that has 4 outputs.  One for each direction.  When running the agent if I choose ""up"" and get it's td score from the next iteration.  How do I set the ideal outputs for the backpropagation?  My guess would be to set ""up"" to the ideal score observed and the other 3 directions to their output as to not change any of the weights flowing directly into them.  Is there a better way?  Am I completely off?",7,0
11,2015-6-2,2015,6,2,3,38436k,Robots that can adapt like animals (Nature cover article),https://www.reddit.com/r/MachineLearning/comments/38436k/robots_that_can_adapt_like_animals_nature_cover/,PLOSScienceWednesday,1433183686,,0,3
12,2015-6-2,2015,6,2,3,3843x2,If you could only take 1 data set to a desert island what would it be and why?,https://www.reddit.com/r/MachineLearning/comments/3843x2/if_you_could_only_take_1_data_set_to_a_desert/,DanielSlater8,1433183966,After some interesting data sets and thought this was a fun way to ask...,7,0
13,2015-6-2,2015,6,2,3,3844z9,Noob question on creating a lstm: size of connection from memoryCell to outputLayer is memoryCell count * outputLayer count. Why?,https://www.reddit.com/r/MachineLearning/comments/3844z9/noob_question_on_creating_a_lstm_size_of/,[deleted],1433184375,"I'm trying to build a lstm and I am having an issue with the output-gates. When I make a connection from the memoryCell to the output Layer, the size of that connection is the amount of memoryCells times the amount of output layers. In my case ( 16 * 5 = 80 ). Then when I try to connect the outputGate to the connection of the memoryCell to the outputLayer, I can't as I get an error that the size of the outputGate ( 1 ) and the connection ( 80 ) are not the same.

I'm creating an instance of the lstm like so new lstm(1, 16, 5). 1 input-layers, 16 memoryCells and 5 output-layers. And in that I get the error that the gate can not be made.

     //connections from memory cell
    var output = memoryCell.project(this.outputLayer);
    /*memoryCell has a size of 16 and outputLayer has a size of 5
   the size of output is 80.

    //...

    this.outputGate.gate(output, Layer.gateType.OUTPUT);
    /*how can I create this gate when the size of the gate is 1 and the size of the output is 80?*/

The library I am using is throwing an error that the connection must be from something of the same size to something of that same size. I am able to make an input-gate and forget-gate, but not the output-gate because the size is different. It make sense the size is the same and in the author's implementation of an lstm, he does the same thing, but without an error. However, I'm trying to create my own lstm constructor for learning purposes and I can't figure out how to go beyond or fix this. Don't the sizes need to be the exact same? And I need 5 output layers in this implementation. The author's constructor works with any size output-layers, but mine only does with a size of 1 despite what seems like the same code to me at least. 
",0,0
14,2015-6-2,2015,6,2,4,384bjk,Which architecture of RNN would you recommend for speech recognition?,https://www.reddit.com/r/MachineLearning/comments/384bjk/which_architecture_of_rnn_would_you_recommend_for/,AraneusAdoro,1433186974,,6,1
15,2015-6-2,2015,6,2,4,384ddd,Recommendation Engine Confusion,https://www.reddit.com/r/MachineLearning/comments/384ddd/recommendation_engine_confusion/,GTDinkleberg,1433187683,"I've been assigned the task of creating a recommendation engine to suggest movies and television shows. I'm having a lot of trouble getting started as I've found the volume of material to be overwhelming, and i'm not even sure what language would be the best to use. I was told R might be a good choice (I don't know it, but i heard that it's mostly used for statistics and it has a steep learning curve). I'm not even sure if i might need more than one. I've taken a look at some of andrew ng's material on coursera (linear/logistic regression, and most of the recommender system material) but i'm still not sure how to actual program it either. So, i was hoping you guys could give me some advice on how to get started. What should i focus on learning really well first so i can modify the algorithms appropriately to suit my needs? Would using prediction.io be a good idea? What languages should i look into using, and would R be worth learning for this particular instance? With simulatenous gradient descent for Collaborative filtering, how do you pick a feature scaling constant that will work for both the user rating and the movie feature if they are just arbitrary values initially?

Background: I've had zero experience with machine learning or data science. I'm a third year in college with no classes relating to machine learning taken. 

I'd really appreciate any help at all. Thank You.

Edit: Thanks for the resources, they are very helpful!",3,0
16,2015-6-2,2015,6,2,5,384jtr,What implicit feedback for a news recommender system?,https://www.reddit.com/r/MachineLearning/comments/384jtr/what_implicit_feedback_for_a_news_recommender/,ta-da-scientist,1433190161,"I am working on a content-based recommender system for news articles where users don't give explicit feedback like ratings, so we use implicit feedback.

Here are possible metrics for estimating whether a user liked an article, but don't feel limited to those:

* the user opened the page of the article
* how long did the user keep the tab open
* how long did the user spend reading the article (see how it can be estimated [here](https://medium.com/data-lab/mediums-metric-that-matters-total-time-reading-86c4970837d5))
* the user left a comment (whatever he wrote)
* the user shared the article (or ""liked"" it, sent it by email, ...)

What metrics are usually used? How well do they predict the user's opinion?
Basically, I would like to know which are worth implementing.

I doubt there is a definite answer, so your experience and case studies are welcome.

",2,0
17,2015-6-2,2015,6,2,7,384yvs,Hardware for Machine Learning (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/384yvs/hardware_for_machine_learning_xpost/,compsens,1433196171,,0,0
18,2015-6-2,2015,6,2,7,3853z7,How to query ANN with features not in training set?,https://www.reddit.com/r/MachineLearning/comments/3853z7/how_to_query_ann_with_features_not_in_training_set/,desegel,1433198258,"Hi! I'm a noob regarding ML, and I found a nice python library that generally fit my needs:  [NearPy.io](http://nearpy.io).
My features are strings (words).  I could build a occurence-counting vector by it, but how could I search for a new vector that contains strings that are not in the training set?  
The number and order of features would be absolutely different.

Thanks!

(EDIT:  these words/strings are not in natural language so I can't have a corpus)",7,0
19,2015-6-2,2015,6,2,8,385dnd,How to fast-track into entry level AI project/work?,https://www.reddit.com/r/MachineLearning/comments/385dnd/how_to_fasttrack_into_entry_level_ai_projectwork/,BinaryAlgorithm,1433202315,"I've been wondering how to respec certifications to work in AI or related field (possibly also robotics). In a world where employers only seem to look at credentials, I'm unsure what they are looking *for* exactly. I programmed in VB6 for about 15 years (since I was 12), and did some C# and other languages and even assembly, but what I really need is a training experience. I have an MBA instead of a CS degree but I don't have time right now to go re-do my degree (don't feel it's particularly useful at this point compared to real life experiences); I'd rather take a few pertinent training classes or work an internship because that's practical and efficient. I can't seem to find any community here in Reno NV for AI (maybe I need to look nationally and coordinate online, but where/how? I'm not that good at finding it apparently); I have guessed that socialization is the key to finding some opportunity. Right now I just design A-Life simulations at home, but I would like to learn more working on a real AI project. I have a specific interest in intelligent agents / digital organisms and general AI development, but I see most of the work is on narrow AI for vision and other specific problem domains.

I am completely lost at how to find a mentor for this. I originally learned to program because I wanted to make games, so my approach is half art and half science (and I think some creativity and philosophy is necessary in these fields - to think out of the box and design new patterns). Outside a university, I haven't seen where I can get into this. I don't think it's an intractable problem; after all, I can read and learn what I don't know. It's just unlocking an opportunity to learn that is challenging. Does anyone have ideas on where I can go, who I can talk to? What communities have projects I can learn from/participate in? There must be some action I can take to advance, to get help with it (even if just theory discussions)?

I believe AI is what will fundamentally change mankind, and thus is among the most important focus points along with supporting technologies like quantum computers. I am also interested in computer ethics, a very small but important field. I believe I will have ideas to contribute when I encounter new information; I just want the chance to do so.

Thank you for any input regarding this. I feel very lost right now when people keep telling me what I can't do based only on credentials when I know that I can do it with a passion; I can learn, after all ...",5,0
20,2015-6-2,2015,6,2,10,385po4,Transition-Based Dependency Parsing with Stack Long Short-Term Memory,https://www.reddit.com/r/MachineLearning/comments/385po4/transitionbased_dependency_parsing_with_stack/,SuperFX,1433207485,,0,16
21,2015-6-2,2015,6,2,12,3866k1,Extracting data from real estate websites,https://www.reddit.com/r/MachineLearning/comments/3866k1/extracting_data_from_real_estate_websites/,richardbeerd,1433215197,"Hey guys, just discovered /r/machinelearning. I hope this question is relevant to the sub.

I'm trying to find ways of extracting data on home price, #bathrooms, square footage etc. from realtor websites. Does anyone know how I could go about doing this? 

Thanks",5,1
22,2015-6-2,2015,6,2,13,386dgw,[1506.00619] Blocks and Fuel: Frameworks for deep learning,https://www.reddit.com/r/MachineLearning/comments/386dgw/150600619_blocks_and_fuel_frameworks_for_deep/,[deleted],1433218668,,3,16
23,2015-6-2,2015,6,2,13,386hro,6 Common Problems That Can Occur In Your Coffee Machine,https://www.reddit.com/r/MachineLearning/comments/386hro/6_common_problems_that_can_occur_in_your_coffee/,jassicaon,1433221134,,5,0
24,2015-6-2,2015,6,2,14,386kiy,Are there any datasets freely available on which I can test the autocomplete algorithm I have developed?,https://www.reddit.com/r/MachineLearning/comments/386kiy/are_there_any_datasets_freely_available_on_which/,rohanpota,1433222826,,2,0
25,2015-6-2,2015,6,2,14,386lqz,"Best cost function for sparse, real-valued data?",https://www.reddit.com/r/MachineLearning/comments/386lqz/best_cost_function_for_sparse_realvalued_data/,CreativePunch,1433223622,"Hi all,

Suppose the target output of my network is an MxN matrix where 95% of the values are 0.0 and the other values are anywhere between 0.0 and 1.0, what would be a good loss function to use for this data?

As long as my NN outputs a lot of 0's the MSE would be really small, but it has a hard time learning the values properly that are bigger than 0

Any ideas? Thanks!",1,0
26,2015-6-2,2015,6,2,14,386moj,Proceedings of The 32nd International Conference on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/386moj/proceedings_of_the_32nd_international_conference/,cast42,1433224238,,0,19
27,2015-6-2,2015,6,2,15,386njm,Questionable Result on ImageNet Benchmark in Baidu's Deep Image Paper,https://www.reddit.com/r/MachineLearning/comments/386njm/questionable_result_on_imagenet_benchmark_in/,ResHacker,1433224802,,16,53
28,2015-6-2,2015,6,2,15,386ss8,What Factors Increased The Demand of Global and Chinese Oil-Sealed Vacuum Pump Industry 2010-2020 ?,https://www.reddit.com/r/MachineLearning/comments/386ss8/what_factors_increased_the_demand_of_global_and/,profresearchreports,1433228351,,0,1
29,2015-6-2,2015,6,2,18,3873ov,PyData Berlin 2015: Probabilistic Programming and Sports Analytics (Video of presentation of Youtube),https://www.reddit.com/r/MachineLearning/comments/3873ov/pydata_berlin_2015_probabilistic_programming_and/,cast42,1433236654,,1,2
30,2015-6-2,2015,6,2,18,3875rm,Automatic music genre classification using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3875rm/automatic_music_genre_classification_using/,john_philip,1433238234,,2,5
31,2015-6-2,2015,6,2,19,387741,"[Discuss] To machine learning researchers, this isn't just Baidu's problem",https://www.reddit.com/r/MachineLearning/comments/387741/discuss_to_machine_learning_researchers_this_isnt/,rantana,1433239277,"Machine learning is a wonderful research field. The pace of progress and the applicability to my line of work is so high that I have to keep up with papers that are only months or even weeks old. It's a great feeling to be on the cutting edge in industry. I can't think of any other research field that creates tangible results as fast as this one.

...but please cut the [marginally state-of-the-art](http://www.reddit.com/r/MachineLearning/comments/2wk8yi/discuss_the_elephant_in_the_room_of_machine/) crap. You may think that this recent [Baidu ImageNet retraction](http://www.image-net.org/challenges/LSVRC/announcement-June-2-2015) is just a lapse in moral judgement, but it's part of a bigger issue in the community. 

-Those who participate in writing state-of-the-art in their papers without any statistic showing how significant that number are just as guilty. You're hiding the reproducibility of your research. If it takes too long to run more than one sample, note that in your results.

-Those that consider state-of-the-art as a bar that should be met for publication (especially reviewers) are just as guilty. You're creating an environment that supports this state-of-the-art endemic. Having collarborated with those in research, it clearly pressures researchers in a highly competitive field to run and re-run their analyses with tiny tweaks till they get the result they want.

So those of you that are writing the phrase state-of-the-art in your next papers, think about what it means before writing it. It may seem like a golden ticket to getting your paper published, but stop for a moment and think about how you came to your result. How much effort did you spend on the actual idea and how much did you spend on trying to get it to converge to the number you wanted?

Hal Daume III wrote a [great piece about this issue](http://nlpers.blogspot.com/2014/11/the-myth-of-strong-baseline.html). Here's an excerpt:

&gt;[...] There are lots of ways to be better than such a baseline, and so ""beating"" it does not teach me anything. I always tell students not to get too pleased when they get state of the art performance on some standard task: someone else will beat them next year. If the only thing that I learn from their papers is that they win on task X, then next year there's nothing to learn from that paper. The paper has to teach me something else to have any sort of lasting effect: what is the generalizable knowledge.

&gt; The point is that an evaluation is not an end in itself. An evaluation is there to teach you something, or to substantiate something that I want to teach you. If I want to show you that X is important, then I should show you an experiment that isolates X to the best of my ability and demonstrates an improvement, preferably also with an error analysis that shows that what I claim my widget is doing is actually what it's doing.

To deep learning researchers: Stop throwing all the tricks in the book to push your numbers up (ie dropout, sophisticated optimizers, fancy parameter initialization, dataset augmentation). If you need these tricks to prove your point, then I'm sorry to say you don't have one. Your results are meaningless since you've confounded it with so many other things. **Instead of spending so much effort pushing your method to state-of-the-art performance, how about spending some time simplifying it.** Show that the intuitions you had for your idea is in fact what's happening. Coming from someone on the application side, **if your idea isn't simple to implement or intuitive, no one will use it.** 

Occam's razor has so much truth in this field. There are too many times where I reproduce a result only to find it only works exactly in the conditions described in the paper. This is a great way to waste another person's time.

....In other words, I just want to stop reading crappy papers.......",45,131
32,2015-6-2,2015,6,2,21,387h3a,Real-time image recognition demo?,https://www.reddit.com/r/MachineLearning/comments/387h3a/realtime_image_recognition_demo/,morganKo,1433246563,"Hi everyone,

In order to introduce Machine Learning to non-IT people, I am trying to find a real-time image recognition demo, or something close to that.

I have tried to access to the Stanford Vision Lab demo, but the webpage seems broken (https://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/).

I am thinking of trying to run my own https://github.com/karpathy/neuraltalk, but it doesn't seem trivial.

Many thanks for your help!
",1,2
33,2015-6-2,2015,6,2,22,387r5d,Introducing Facebook AI Research Paris,https://www.reddit.com/r/MachineLearning/comments/387r5d/introducing_facebook_ai_research_paris/,mrprint,1433251832,,4,8
34,2015-6-2,2015,6,2,23,387vko,what base classifier/regressors do you use for stacking?,https://www.reddit.com/r/MachineLearning/comments/387vko/what_base_classifierregressors_do_you_use_for/,godspeed_china,1433253905,"As stacking is computational intensive (10 CV or LOO), I think the base classifier must be fast. As I known decision tree and linear regression is that fast. However linear regression is not powerful enough. What classifier do you use for stacking?",1,1
35,2015-6-2,2015,6,2,23,3880fp,Looking for a NLP API to help parse support tickets,https://www.reddit.com/r/MachineLearning/comments/3880fp/looking_for_a_nlp_api_to_help_parse_support/,NoelDavies,1433256017,"I've been looking online (via mashape, and via other sites including reviews, etc) for a Natural Language Processing API / Service / Package / etc that will allow me to parse out text into basic actions.

For example something like ""Remove this account immediately. 
my user id is 999999""

to ['delete' =&gt; 'account', 'id' =&gt; 999999]",2,0
36,2015-6-2,2015,6,2,23,3881a0,A Deeper Look at Planning as Learning from Replay,https://www.reddit.com/r/MachineLearning/comments/3881a0/a_deeper_look_at_planning_as_learning_from_replay/,pierrelux,1433256356,,0,0
37,2015-6-2,2015,6,2,23,3881d9,Variational Generative Stochastic Networks with Collaborative Shaping,https://www.reddit.com/r/MachineLearning/comments/3881d9/variational_generative_stochastic_networks_with/,pierrelux,1433256392,,2,2
38,2015-6-3,2015,6,3,0,388ahy,Modelling time series data with Convolutional RBM,https://www.reddit.com/r/MachineLearning/comments/388ahy/modelling_time_series_data_with_convolutional_rbm/,[deleted],1433259928,"I am trying to model time-series data with Convolutional RBM (Lee's model). I am doing mean zero, variance one normalization for each file before sending to CRBM. Time-series data is of variable length e.g. 40,000 samples in one file. The problem I am facing is when reconstruction is good weights are not learned and vice versa. I also tried using only probabilities in reconstruction. I am using 50-80 filters (K-Groups in CRBM). I tried to debug CRBM and found that standard deviation factor (sigma) in CRBM seems to have problem. As I am making unity of each training example, it may be good to have unit sigma. However, doing so reconstruction improves but weights become more and more noisy in each iteration. My questions are:
1) How to choose sigma (standard deviation parameter) in CRBM (it also have roots in sparse RBM) ?
2) How to properly initialize weights and biases in CRBM so that units will not saturate?
3) Is it good to ignore probabilistic max-pooling (as right now I'm using only single machine) in sampling and use only probabilities (soft-max units) in reconstruction and other calculations?
Hoping for positive reply.
Thank you",0,1
39,2015-6-3,2015,6,3,1,388lz1,computing lexical chains without using wordnet,https://www.reddit.com/r/MachineLearning/comments/388lz1/computing_lexical_chains_without_using_wordnet/,new2machinelearning,1433263972,"Hi folks .. i am trying to work on extracting important sentences in a document (meaning any generic document including but not limited to news paper articles) and i have figured that the wordnet based approach of finding synsets for all noun terms is proving quite challenging for tech heavy articles. Tech articles are quite heavy on NNP's (for instance using words like twitter, uber , mail-client etc) and i doubt if WordNet can help here. Has any one here faced similar issues and if so any pointers on what can help ? I am toying with the idea of using collocations (as synonyms) of nouns from a base of 100's of articles that i am currently collecting from different sources. Given that the articles are going to be specific to a certain domain i would need something similar for all major domains. Any flaws you notice / pointers here will be accepted with a bow :)

Regards,
vikram",3,0
40,2015-6-3,2015,6,3,2,388nso,The Unknown Perils of Mining Wikipedia,https://www.reddit.com/r/MachineLearning/comments/388nso/the_unknown_perils_of_mining_wikipedia/,benjaminwilson,1433264473,,1,94
41,2015-6-3,2015,6,3,2,388r0f,Google x Amazon x Microsoft ML service comparison - Part 2,https://www.reddit.com/r/MachineLearning/comments/388r0f/google_x_amazon_x_microsoft_ml_service_comparison/,seni9,1433265523,,2,0
42,2015-6-3,2015,6,3,2,388vrz,Using Convolutional RBM to model time-series data,https://www.reddit.com/r/MachineLearning/comments/388vrz/using_convolutional_rbm_to_model_timeseries_data/,Hardiksailor,1433267061,"I am trying to model time-series data with Convolutional RBM  (Lee's model which I modified it for 1-D convolutions). I am doing mean zero, variance one normalization for each file before sending to CRBM. Time-series data is of variable length e.g. 40,000 samples in one file. The problem I am facing is when reconstruction is good weights are not learned and vice versa. I also tried using only probabilities in reconstruction. I am using 50-80 filters (K-Groups in CRBM). I tried to debug CRBM and found that standard deviation factor (sigma) in CRBM seems to have problem. As I am making unity variance of each training example, it may be good to have unit sigma. However, doing so reconstruction improves but weights become more and more noisy in each iteration. My questions are: 
1) How to choose sigma (standard deviation parameter) in CRBM (it also have roots in sparse RBM) ? 2) How to properly initialize weights and biases in CRBM so that units will not saturate? 
3) Is it good to ignore probabilistic max-pooling (as right now I'm using only single machine) in sampling and use only probabilities (soft-max units) in reconstruction and other calculations? Hoping for positive reply. Thank you",0,0
43,2015-6-3,2015,6,3,2,388x8c,OpenCL for Your Embarrassingly Parallelizable Code,https://www.reddit.com/r/MachineLearning/comments/388x8c/opencl_for_your_embarrassingly_parallelizable_code/,pl0d,1433267525,,9,11
44,2015-6-3,2015,6,3,3,3895l9,How can you tell if you can use linear regression when you have more than one feature?,https://www.reddit.com/r/MachineLearning/comments/3895l9/how_can_you_tell_if_you_can_use_linear_regression/,[deleted],1433270195,"I'm doing Andrew NG's coursera course and just finished up linear regression. There are a few things I don't quite get yet, though. The coursework first has you do linear regression with just one feature as seen [here](http://i.imgur.com/yDWsvZR.png). 

Now because we are dealing with 2 dimensions we can plot it and see if linear regression will work. Because if the plots pretty much form a line we'll be okay. But how do you know if you can use lienar regression if you have many features? E.g. you can't plot it and see, oh yeah, there's a line there. Or can you?",2,1
45,2015-6-3,2015,6,3,3,38998v,[ Help ] Perceptron is reporting class as whatever the last class it was trained with even if input was explicitly trained with another class.,https://www.reddit.com/r/MachineLearning/comments/38998v/help_perceptron_is_reporting_class_as_whatever/,[deleted],1433271379,"I have a perceptron that I made with [synaptic.js](https://github.com/cazala/synaptic) with 1 input, 1 hidden, and 5 output layers and no matter with what I train it with, when I activate it, the output is the last class I trained it with.

For example if I train it like so: [{input:[0.1234], output:[0, 1, 0, 0, 0]}, {input:[0.4321], output:[0,0,0,0,1]}] and then activate it with [0.4321] I get the second output-layer as the class.

This appears to be something resembling recurrence with the sequence-class belong to the whole sequence. But what recurrent is there here:

    function LSTM(input, blocks, hidden, output){
        this.layerCount=Array.prototype.slice.call(arguments);
        //create the layers
        this.inputLayer = new Layer(input);
        this.hiddenLayer = new Layer(hidden);
        this.outputLayer = new Layer(output);
	
	//hidden connections
	this.inputLayer.project(this.hiddenLayer);
	this.hiddenLayer.project(this.outputLayer);
	
        //set the layers of the neural network
        this.set({
            input: this.inputLayer,
            hidden: [this.hiddenLayer],
            output: this.outputLayer
        });
    }

This only happens with the perceptron class I built, not the one built it. But as far as I can tell, they are the same.

I'd greatly appreciate any and all help!",0,1
46,2015-6-3,2015,6,3,4,389e7b,Statistical and Mathematical Functions with DataFrames in Spark,https://www.reddit.com/r/MachineLearning/comments/389e7b/statistical_and_mathematical_functions_with/,gradientflow,1433272939,,0,4
47,2015-6-3,2015,6,3,4,389fgj,Automatic Environmental Complexity Increase triggered by Agent Success,https://www.reddit.com/r/MachineLearning/comments/389fgj/automatic_environmental_complexity_increase/,BinaryAlgorithm,1433273354,"As the simplest example I can think of, imagine Agents driven by neural networks and modified each generation with GA and mutations. Let us say the environmental challenge is to find and consume substance A (""food"") faster than it is consumed internally. When the Agents meet a criteria (say, a high enough average lifespan indicating a high degree of success feeding themselves), the environment adds a new complexity, substance B. The input nodes are created to detect B similar to A, and maybe output nodes to consume it (if consumption of B is mutually exclusive with the action to consume A). Now, new connections and nodes need to be formed to connect the new inputs and outputs. This may continue until the number of substances to be consumed is too high for the number of actions available per unit of time and consumption rate; realistically the success threshold will just not be met beyond a certain point if it is a fixed value (but the Agents will optimize to the degree they can).

There are 2 sets of questions (mainly design philosophy) regarding the Agents adapting to new I/O in the environment, and the environment creating new I/O complexity for the Agents:

1) When a new I/O set is formed, should the network generate some structures (like a cluster with random connections to existing clusters) automatically, or create them slowly over time as it normally does? The question regards mapping new I/O, in much the same way that a person can learn to use a cybernetic limb (new limb). We normally think in terms of the internal nodes shifting and changing, but it's possible to gain new percepts and new actions in a simulation here and I haven't seen that discussed before and am not sure how to approach it.

2) While this demonstrates the environment training the Agents incrementally over time, it is simplistic and only adds challenges in one manner (of one sort); how can an environment ""learn"" how to train Agents? Should its utility be based on ""killing them faster"" in a competitive manner such that the environment and Agents are directly at odds (Agents already compete with each other in the environment)? Can the environmental model actually be run by a neural network itself? I am asking this because I am limited in how I can think up a simulation challenge and I wonder if the environment can go further by creating its own kind of Agents with different constraints and actions than the ones being tested, and adjusting parameters - this would require a very open-ended simulation, more like a sandbox; also, the difference between the environment rating itself based on ""beating"" the tested Agents as opposed to rating itself based on their success (harmful/challenging vs. helpful/assist) - perhaps if it changes between these roles, making the simulation harder until they begin to fail, then making it easier until they begin to succeed, as long as over time the ""difficulty"" keeps increasing (but how to measure ""difficulty""?)

I feel that understanding the theory of how the simulation/environment can ""teach"" may be just as important as how Agents ""learn"". It can have access to parameters that tell it how the Agents are doing, or it may evaluate that on its own scale somehow. Has anyone thought about this?",0,1
48,2015-6-3,2015,6,3,5,389lmk,Train/Validation/Test,https://www.reddit.com/r/MachineLearning/comments/389lmk/trainvalidationtest/,regularized,1433275291,"Suppose I have 20000 examples for training, 10000 for validation and 30000 for testing. Can I fix the hyperparameters using the validation test and retrain the classifier using 20000 training AND 10000 validation examples? Then I will apply my classifier to the test set. 

What is the accepted usage in the ML community? Can the validation set be added to the training set at the very last point? Note that the test set remain intact, I use the test set just once.",5,2
49,2015-6-3,2015,6,3,5,389mlk,A Rigorous &amp; Readable Review of RNNs,https://www.reddit.com/r/MachineLearning/comments/389mlk/a_rigorous_readable_review_of_rnns/,tfimg24,1433275591,,0,6
50,2015-6-3,2015,6,3,6,38a5cv,Why does training a support vector machine reduce its VC-dimension?,https://www.reddit.com/r/MachineLearning/comments/38a5cv/why_does_training_a_support_vector_machine_reduce/,IHaveAPointyStick,1433281755,"The reasoning of the slides I got this from is ""the irrelevant points fall away"" - but in which way is this relevant? A Hyperplane can always seperate the same number of of points, regardless of how many ""support vectors"" were used to generate it, no?",0,0
51,2015-6-3,2015,6,3,7,38a7ln,What are some good academic or trade journals that typically post PhD student ads for machine learning-related research assistantships?,https://www.reddit.com/r/MachineLearning/comments/38a7ln/what_are_some_good_academic_or_trade_journals/,sleepicat,1433282581,,1,0
52,2015-6-3,2015,6,3,7,38ac73,How would you use a bayesian classifier with a deep rnn for classification?,https://www.reddit.com/r/MachineLearning/comments/38ac73/how_would_you_use_a_bayesian_classifier_with_a/,[deleted],1433284384,Just looking for a discussion and ideas. Wondering how these two could be merged and why.,0,0
53,2015-6-3,2015,6,3,8,38ahxl,"Input Encoding for NN vs. FSM, some ideas",https://www.reddit.com/r/MachineLearning/comments/38ahxl/input_encoding_for_nn_vs_fsm_some_ideas/,BinaryAlgorithm,1433286777,"I have been considering how input parsing in a simulation works with each one (or can work). For NN I have always encoded the input either as a continuum (-1 to 1, or 0 to 1 for parameters that fall within a min/max range) or as a categorical boolean (1 if matching/applicable, 0 if not applicable) which can encode 1 bit of information. This often requires many units for a ""simple simulation input"" like a map tile.

I considered using an FSM network instead with symbol encoding (just integers really). It seems easier to map simulation parameters directly to these inputs. For example, an input which senses the type of map tile to the left of the agent may have values A-Z or 0-9 (it can be any integers but this selection helps readability), and the subtype perhaps the same range of values; so the information is encoded in 1-2 processing units instead of N, where N is the number of types. With the NN mapping it can be 1 node per possibility (the input node has direct meaning) or it could be log2 N where the NN has to learn what the combination of active nodes means but each bit alone has no distinct meaning, but there are less units to process.

In the FSM network I imagined, inputs are concatenated to an input string then run through a rules engine/mapping table (the rules evolve over time). So if for example the tiles in all 4 cardinal directions were connected to a processing unit from the inputs, you might have AB + CD + EF + GH (assuming 2 units per tile), and Z the current internal state, and thus the string ""ZABCDEFGH"" would be run through the rules list until a match is found; perhaps the new state next tick would be ""J"". I also learned that an FSM that saves previous states or has a state change delay may act similarly to an RNN, for example if the next 5-10 states were buffered. It is of course possible to have a loop like states A-&gt;B-&gt;C-&gt;A or A-&gt;A (no change without specific inputs) and so ""memory"" of states can be preserved over arbitrary length time in a similar manner to the LSTM NN units.

The advantage of such a design may be that it is discrete: if my output unit A-E corresponds to the numbers 1-5, and the selected number is (1) the temporal index of a memory block to recall, N steps ago, or (2) the index of an input to send a signal to and/or the value to send, then I can do some things that I think would be more difficult for an NN to learn/encode (and if it did, it would be difficult for me to figure out how it did it - here it is more transparent by halting the simulation and reviewing the values at time T). So, the units can perform discrete indexing ""decisions"" and the category boundaries are not ambiguous. NN can technically also do this using a step function, but there is no continuous value relationship here with the categories that really makes sense (0.1 going to 0.2 and bumping the step function from B to C). Instead of add/multiply/activation function, the FSM performs an arbitrary mapping of input + current state to output state (which could also be an output string to each receiving unit depending on complexity).

The disadvantage is that the FSM is brittle: if the string order changes (links are added/removed) then it may fail, unless the rules are mapped via values and the specific unit index from which the input is coming (so that the order of characters in the string doesn't matter, only their sources). It needs to be malleable like an NN to network structural changes.

The reason I am thinking about it is temporal memory encoding of ""engrams"". I have thus far not been able to have an NN perform this operation: ""recall my inputs (or specific unit values in X cluster) from T time units in the past"". An LSTM cell will hold a value for an indefinite period of time potentially, but not necessarily from a specific time. The FSM structure above can both address the memory cluster index and the temporal index for the purpose of recall. It's possible that spiking NN can also do memory association and recall but I haven't explored that yet.

The purpose of that is so that the agent can make some kind of association: 30 time steps ago, it ate a red plant (some specific object type like ""QV""). Now it is poisoned or otherwise damaged. The look-back mechanism requires some memory to correlate action:consequence, but they are not always immediately associated in time. How exactly a network can learn that input ""QV"" is an object to be avoided when the consequence is not directly attributable to it (it's possibly NOT the cause of the consequence) is currently beyond my understanding. Human beings associate things incorrectly all the time (attributing causes for things that are incorrect based on correlations, or not connecting cause and effect over long periods of time). An LSTM cell can retain a value... but how can we ""remember"" input sets, output sets, and possibly the internal state in the future when we are looking for causes across time? Further, how can we use that information for any kind of ""learning""? Except... since it is a simulation we can reverse time and take a different branch (action) and then compare the new future where we are not poisoned and say ""aha!"" that's where it happened. In humans, memory allows us to simulate going back in time (to a degree) to try to find that decision node, and maybe dreams are the alternate simulation. In a computer however we can store and go back precisely if we wanted to, and perhaps the agent can find their own mistake - they are not ""temporally bound"" like we are. They can evolve until they correct the decision maybe? OK that is interesting... I haven't seen any reading on that before.

Anyone care to expand on the idea?",1,0
54,2015-6-3,2015,6,3,9,38aq89,Where to get help understanding papers?,https://www.reddit.com/r/MachineLearning/comments/38aq89/where_to_get_help_understanding_papers/,siblbombs,1433290307,"I try to read and understand new papers as the come across arvix and here, however I don't have a strong background in math so sometimes I run into things that I struggle to understand. There is generally pretty interesting discussion about papers that get posted here, however it doesn't seem like the right forum for getting help on understanding specific equations since everyone probably has better things to do than just tutor people all day.

Does anyone have experience with/know of a service like fivver where you can go to get ~15 minutes with some that can answer questions on the types of things that come up in some more technical papers?",11,2
55,2015-6-3,2015,6,3,9,38awgt,"Approaching Machine Learning with limited sets of data in Neural Networks? Sorry, am noob.",https://www.reddit.com/r/MachineLearning/comments/38awgt/approaching_machine_learning_with_limited_sets_of/,devDorito,1433292882,"Let's say i want to create a NN algo that focuses on a limited amount of data, say a NN that generates its own script from some game, let's say Halo, which is LISP-esque. There's only thousands of lines of meaningful code I could get my hands on, versus C++ or C-style syntax. Larger languages have hundreds of thousands, if not millions of lines of meaningful code which can be used.

How does this change the approach? If someone has a NN I could run and see the results for myself, it would be interesting. I ask because I saw a blog discussing this a while back. The NN and the code produced by the algo couldn't make normal-looking code until it had analyzed a very large amount of code.

Would creating some kind of script generator which would just randomly generate syntactically correct code be useful?",17,0
56,2015-6-3,2015,6,3,16,38c6qx,Multiple LSTM Layers vs. Deep Gates within LSTM,https://www.reddit.com/r/MachineLearning/comments/38c6qx/multiple_lstm_layers_vs_deep_gates_within_lstm/,alexmlamb,1433316406,"Hello, 

Has anyone considered using deep gates within an LSTM instead of using multiple LSTM layers?  This actually seems to be a lot closer to what is done for neural turing machines (which has a single memory layer and uses a deep controller network to drive the gates).  

Best, 

Alex.  ",5,5
57,2015-6-3,2015,6,3,16,38c85f,PAPIs.io 2015 Preliminary Program Announced,https://www.reddit.com/r/MachineLearning/comments/38c85f/papisio_2015_preliminary_program_announced/,czuriaga,1433317399,,0,1
58,2015-6-3,2015,6,3,17,38cbsi,History of reasearch papers,https://www.reddit.com/r/MachineLearning/comments/38cbsi/history_of_reasearch_papers/,keptavista,1433319809,"After reading [about marginal state of the art] (http://www.reddit.com/r/MachineLearning/comments/387741/discuss_to_machine_learning_researchers_this_isnt/) thread I kinda wonder what are the papers which brought some real breakthrough. I am not familiar about deep learning but it would be interesting to have a list of the most important papers in chronological order and some context. For example what was state at that time, what were the problems, state of the art then and what problem the authors of that paper solved. I think it could be a great resource to familiarise with domain and learn  from the original sources for new practitioners. For example what was the first paper about deep learning? Maybe such a list already exists or maybe some more experienced members of this community could attempt to make this short history? Not necessarily about deep learning I think all domains can be very interesting!",6,0
59,2015-6-3,2015,6,3,19,38covb,Great Review of Linear Algebra,https://www.reddit.com/r/MachineLearning/comments/38covb/great_review_of_linear_algebra/,changingourworld,1433328747,,9,86
60,2015-6-3,2015,6,3,22,38d7xu,Recommending items to more than a billion people,https://www.reddit.com/r/MachineLearning/comments/38d7xu/recommending_items_to_more_than_a_billion_people/,alexcasalboni,1433338736,,0,8
61,2015-6-3,2015,6,3,23,38ddb8,SVM multiple classes,https://www.reddit.com/r/MachineLearning/comments/38ddb8/svm_multiple_classes/,[deleted],1433340802,"I read this [post] (http://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/) about SVM and it explains very well the principle of SVM classification. However, I have to deal with more than two classes. Can someone explain to me (like I'm five) what happens if I introduce for example yellow balls to the example?

Thanks!",0,1
62,2015-6-3,2015,6,3,23,38ddeb,Notes on Document Embedding with Paragraph Vectors,https://www.reddit.com/r/MachineLearning/comments/38ddeb/notes_on_document_embedding_with_paragraph_vectors/,benjaminwilson,1433340834,,5,6
63,2015-6-3,2015,6,3,23,38dgq0,Is Theano really all that?,https://www.reddit.com/r/MachineLearning/comments/38dgq0/is_theano_really_all_that/,[deleted],1433342006,"Theano is often put forward as an excellent tool for neural network research because it allows fast prototyping (with automatic differentiation and easy GPU acceleration being the main selling points).

I would agree that for trying out a new nonlinearity or cost function it's hard to beat the ease of Theano. However, a few counterexamples:

CNN - Several of the more innovative things to appear in the past year ([sparse convnets](http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/sparse3d.pdf) and [fractional pooling](http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/fmp.pdf) and [batchwise dropout](http://arxiv.org/abs/1502.02478) came from Benjamin Graham, whose [C++ code](https://github.com/btgraham/SparseConvNet) is fairly concise. While it may be possible to reimplement these ideas in Theano, it doesn't appear straightforward to me, and as far as I know no one has done it yet.

RNN - while there are now a number of good RNN implementations using Theano, when they were beginning their resurgence a few years ago the performance was quite low and required a lot of semi-manual optimization. 

Given this, does using Theano for cutting edge research really make sense? Or is it better to use Torch (apparently preferred by DeepMind and Facebook AI Research) or even straight C/C++?",2,0
64,2015-6-3,2015,6,3,23,38di7r,Understanding SVM in multi class scenario's.,https://www.reddit.com/r/MachineLearning/comments/38di7r/understanding_svm_in_multi_class_scenarios/,JdeB0,1433342569,"I have a remote sensing background and I am currently diving deeper in understanding of different classification algorithms. 

I read this [post](http://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/) about SVM and it explains very well the principle of SVM classification. However, what if there are yellow balls too? I read about an one v. one approach. I tried to understand this but I am not sure if I get it. I think it works like this:

if you have three colors (blue, red and yellow) you can create three classifiers: one deciding if unknown value x is blue or red, blue or yellow, and red or yellow. Depending on the answers, you get:

* blue blue red, 
* blue blue yellow
* red blue yellow
* etc.

Then the outcomes of every classifier are using in a vote, and X = the color with the most votes.

My questions are:

* 1. I had to think of an unknown value x, since it is a classification. In the example however, there are no ""unknown values"", only class separation. Am I misunderstanding something about SVM?
* 2. How are ties handled? 
* 3. Is this more or less a good description of how SVM deals with more than two classes?

Thanks!
",4,1
65,2015-6-3,2015,6,3,23,38dils,New state-of-the-art on CIFAR,https://www.reddit.com/r/MachineLearning/comments/38dils/new_stateoftheart_on_cifar/,flipcaul,1433342715,,29,11
66,2015-6-4,2015,6,4,4,38ezta,Logistic Regression using Theano (Ipython notebook),https://www.reddit.com/r/MachineLearning/comments/38ezta/logistic_regression_using_theano_ipython_notebook/,cast42,1433360988,,0,0
67,2015-6-4,2015,6,4,5,38fahs,Machine Learning Magic &amp; The Intelligent App Revolution,https://www.reddit.com/r/MachineLearning/comments/38fahs/machine_learning_magic_the_intelligent_app/,mattwoodget,1433364733,,0,0
68,2015-6-4,2015,6,4,6,38fd3z,Interview with Wei Wang: Will Machines Replace Humans?,https://www.reddit.com/r/MachineLearning/comments/38fd3z/interview_with_wei_wang_will_machines_replace/,theblackboxphd,1433365643,,0,1
69,2015-6-4,2015,6,4,7,38fpux,What does Word2Vec patent stop me from?,https://www.reddit.com/r/MachineLearning/comments/38fpux/what_does_word2vec_patent_stop_me_from/,adityajoshi5,1433370502,"There has been lot of discussion since Word2Vec was patented last month. Here's a sample of such a discussion from the web:
http://www.reddit.com/r/MachineLearning/comments/37b1bl/

However asking from a noob's point of view, does this patent thingy stop me from:

1. Pursuing academic research (and writing papers) on the systems that use and cite Word2Vec?

2. Building a commercial app/software/service that uses Word2Vec somewhere?

3. Using Word2Vec in my or my organization's internal projects/model building etc?

4. Any other use-case where it may or may not affect me?

",0,0
70,2015-6-4,2015,6,4,7,38frw2,What concepts will I absolutely need to know before learning Process Mining?,https://www.reddit.com/r/MachineLearning/comments/38frw2/what_concepts_will_i_absolutely_need_to_know/,mango_dingo,1433371273,"I'm doing research in process mining but haven't done any machine learning yet, apart from reading textbooks and papers this past week. What are the fundamentals leading to process mining?",0,2
71,2015-6-4,2015,6,4,8,38g2mr,basic question regarding neural network's error,https://www.reddit.com/r/MachineLearning/comments/38g2mr/basic_question_regarding_neural_networks_error/,billconan,1433375575,"I'm looking at this book to learn neural network:

http://neuralnetworksanddeeplearning.com/chap2.html

at first, when explaining the theory, it defines the error function as the sum of all mean squares of all training outputs. i.e. I need to feed the network with all the inputs to get an error and use that error to do one back-propagation.


but later on, when explaining the actual back-propagation algorithm, it seems that it only feeds one input to the network at a time and gets an error only for that input, and updates the network based on the error. And it will repeat these steps on all training inputs.


but is that error calculated with just one input the right error to use?  

what if the gradient I get based on the error can only improve the outcome for that particular input? how can we know that this gradient can improve the network for all training inputs?
what if after gradient descent, the overall performance of the network gets worse? 




",4,0
72,2015-6-4,2015,6,4,9,38g7wp,[1504.00548] Learning to Understand Phrases by Embedding the Dictionary,https://www.reddit.com/r/MachineLearning/comments/38g7wp/150400548_learning_to_understand_phrases_by/,[deleted],1433377560,,4,14
73,2015-6-4,2015,6,4,13,38h6w3,Ways to improve a Regression Model,https://www.reddit.com/r/MachineLearning/comments/38h6w3/ways_to_improve_a_regression_model/,jbrambledc,1433392681,"I am currently doing a Ridge Regression model, and am beginning to run out of ideas for how to improve the model. I've had the following ideas so far.

1. trim the featureset. since it's time series data, only use the most recent entries. This has worked extremely well, but I am at the optimal amount of features.

2. Use ensembles. Average multiple models together. This begins to converge quite early to the point where more than three models doesnt have a huge effect.

3. Shrink alpha. This has helped a fair amount. 

Any other ideas? the time series data currently consists of values between 0 and 1. anyway to transform the data? by the way I am using sci kit learn.",7,1
74,2015-6-4,2015,6,4,14,38he1n,Predictive Analytics Beta - Grab a limited spot for reddit users only.,https://www.reddit.com/r/MachineLearning/comments/38he1n/predictive_analytics_beta_grab_a_limited_spot_for/,[deleted],1433396712,,0,1
75,2015-6-4,2015,6,4,16,38hljk,Learn: How to Get Crowdfunding For Your Business!,https://www.reddit.com/r/MachineLearning/comments/38hljk/learn_how_to_get_crowdfunding_for_your_business/,[deleted],1433401480,,0,0
76,2015-6-4,2015,6,4,20,38i4o9,"Does it seem really realistic? I'm not sure about how they plan to do it, and I know Deep learning sometimes outperform humans, but that seems a bit much.",https://www.reddit.com/r/MachineLearning/comments/38i4o9/does_it_seem_really_realistic_im_not_sure_about/,ChocoMoi,1433415691,,6,1
77,2015-6-4,2015,6,4,20,38i4rv,Using Amazon Machine Learning to Predict the Weather,https://www.reddit.com/r/MachineLearning/comments/38i4rv/using_amazon_machine_learning_to_predict_the/,john_philip,1433415747,,3,25
78,2015-6-4,2015,6,4,20,38i84f,Hound demo,https://www.reddit.com/r/MachineLearning/comments/38i84f/hound_demo/,keptavista,1433417947,,12,29
79,2015-6-4,2015,6,4,21,38ibew,How much are the terms optimization and programming interchangeable ?,https://www.reddit.com/r/MachineLearning/comments/38ibew/how_much_are_the_terms_optimization_and/,Nydhal,1433419832,,7,0
80,2015-6-4,2015,6,4,23,38itmb,"Conversation with Patrick Wendell, release manager of Apache Spark",https://www.reddit.com/r/MachineLearning/comments/38itmb/conversation_with_patrick_wendell_release_manager/,gradientflow,1433428344,,0,0
81,2015-6-4,2015,6,4,23,38ivnb,Baidu forced to withdraw last month's Imagenet test results,https://www.reddit.com/r/MachineLearning/comments/38ivnb/baidu_forced_to_withdraw_last_months_imagenet/,trashacount12345,1433429260,,52,123
82,2015-6-5,2015,6,5,0,38iytj,Learning to Read with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/38iytj/learning_to_read_with_recurrent_neural_networks/,tfimg24,1433430615,,10,0
83,2015-6-5,2015,6,5,1,38jb2q,[1506.01110] Multi-view Machines,https://www.reddit.com/r/MachineLearning/comments/38jb2q/150601110_multiview_machines/,improbabble,1433435233,,3,5
84,2015-6-5,2015,6,5,1,38jcl4,What are some tools you use to handle large CSV datasets?,https://www.reddit.com/r/MachineLearning/comments/38jcl4/what_are_some_tools_you_use_to_handle_large_csv/,[deleted],1433435787," So I'm a beginner messing around with some basic application of ML, applying simple classifiers and stuff, and I stumbled along a CTR challenge Avazu posted a while back on Kaggle: https://www.kaggle.com/c/avazu-ctr-prediction.
My problem is that the training set here is a single CSV ~2GB in size, and possibly a few hundred million entries. I tried opening this dataset in the WEKA viewer, and even after increasing the stack size to 4GB, it stopped responding after some time, forcing me to close it. This brings me back to the question, what tools/libraries do you guys use to handle such large datasets?

I know how to program in Python, but don't know if Scikit-Learn would be able to handle datasets so big.",0,0
85,2015-6-5,2015,6,5,3,38jusg,Understanding Andrew Ng Notes,https://www.reddit.com/r/MachineLearning/comments/38jusg/understanding_andrew_ng_notes/,MusicIsLife1995,1433442465,"http://cs229.stanford.edu/notes/cs229-notes1.pdf

Can someone proof the 4 equalities posted on page 9 of his notes? I'm having trouble since I'm not sure how to express these types of things.",7,0
86,2015-6-5,2015,6,5,4,38k4s8,Our perfect submission,https://www.reddit.com/r/MachineLearning/comments/38k4s8/our_perfect_submission/,[deleted],1433445444,,0,1
87,2015-6-5,2015,6,5,4,38k7qg,Ipython notebook about Support vector machines (SVMs),https://www.reddit.com/r/MachineLearning/comments/38k7qg/ipython_notebook_about_support_vector_machines/,cast42,1433446387,,2,3
88,2015-6-5,2015,6,5,4,38kaju,"Potentially being taken on as an undergrad research assistant in ML, what basics, papers, terms, etc, should I know??? Thanks all.",https://www.reddit.com/r/MachineLearning/comments/38kaju/potentially_being_taken_on_as_an_undergrad/,TheGooseRider,1433447338,,2,0
89,2015-6-5,2015,6,5,5,38ke97,Removing image background to stop overfitting?,https://www.reddit.com/r/MachineLearning/comments/38ke97/removing_image_background_to_stop_overfitting/,thenewborn,1433448523,"I'm working on a classifier using about 40,000 images (20,000 for each class). The dataset doesn't seem to contain duplicates, and I'm using randomly selected sets with a split of 60:20:20 for training, validation, and testing.

I achieved an error rate less than 5% on the test set, but when testing with new data, the error rate jumps to about 30%. After looking at the difference between the original data and the new, it seems that the (camera specific) background contains features that make it very easy to do well with the original data, but they do not generalize.

What is the best solution to this, aside from getting a new training set?
My ideas are:

* Group the images by camera and subtract the mean.
* Replace the background with random noise instead.
* Use only a part of each image, getting rid of the background, but sacrificing useful features.

Edit: fixed formatting.",6,3
90,2015-6-5,2015,6,5,5,38kiu6,Analyzing social media data with Python (Ipython notebook),https://www.reddit.com/r/MachineLearning/comments/38kiu6/analyzing_social_media_data_with_python_ipython/,cast42,1433449975,,0,0
91,2015-6-5,2015,6,5,9,38likn,Obama-RNN - Machine generated political speeches,https://www.reddit.com/r/MachineLearning/comments/38likn/obamarnn_machine_generated_political_speeches/,samim23,1433462898,,17,28
92,2015-6-5,2015,6,5,12,38may3,Interactive clustering using k-means.,https://www.reddit.com/r/MachineLearning/comments/38may3/interactive_clustering_using_kmeans/,[deleted],1433474778,,4,4
93,2015-6-5,2015,6,5,16,38n3kp,A Critical Review of Recurrent Neural Networks for Sequence Learning,https://www.reddit.com/r/MachineLearning/comments/38n3kp/a_critical_review_of_recurrent_neural_networks/,highorderbits,1433490490,,1,2
94,2015-6-5,2015,6,5,17,38n93v,"Twitter users clustering, based on tweets",https://www.reddit.com/r/MachineLearning/comments/38n93v/twitter_users_clustering_based_on_tweets/,warriorkitty0,1433494454,"Hi ML!

I would like to do a fun project where I get all the people I follow on Twitter and put them into 3 groups, based on their tweets:

1.) those who talk about programming

2.) those who talk about cats (I love cats)

3.) others

Then I would unfollow ""others"" because I'm only interested in cats and computer programming. :) I don't care about the computer language, protocols to get tweets, etc. I'm fairly good at that area. What I would like to ask you are some suggestions about data models and where to start, how would you do the clustering once when you get a list of a tweets, few algorithms to checkout and learn, etc...

This would be my first ""bigger"" ML project and a one I'm gonna show on job interviews so I would like to do it good and I'm not afraid to spend some time doing the project and learn a lot.


Thank you very much.",10,6
95,2015-6-5,2015,6,5,19,38ngz2,Machine learning for package users with R (6): Xgboost (eXtreme Gradient Boosting),https://www.reddit.com/r/MachineLearning/comments/38ngz2/machine_learning_for_package_users_with_r_6/,TJO_datasci,1433499974,,0,1
96,2015-6-5,2015,6,5,19,38nj42,Face reconstruction,https://www.reddit.com/r/MachineLearning/comments/38nj42/face_reconstruction/,saix47,1433501413,I start doing project relate to face reconstruction. Is there any machine learning algorithm for studying? Do you have suggestions what papers I should studying? Thanks in advanced. ,4,0
97,2015-6-5,2015,6,5,22,38o0dt,Aerosolve: a machine learning package built for humans,https://www.reddit.com/r/MachineLearning/comments/38o0dt/aerosolve_a_machine_learning_package_built_for/,galapag0,1433510886,,0,8
98,2015-6-5,2015,6,5,23,38o54w,Interesting multimodal datasets,https://www.reddit.com/r/MachineLearning/comments/38o54w/interesting_multimodal_datasets/,[deleted],1433512978,"I am looking for multimodal datasets, i.e image + text, video +audio etc. I found the MIR Flickr dataset, which contains pictures from Flickr with tags. Do you know any other multimodal datasets?",6,8
99,2015-6-5,2015,6,5,23,38oa0o,Restaurant context analysis on text using Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/38oa0o/restaurant_context_analysis_on_text_using_machine/,ankitsablok89,1433515004,"Hi guys. I just started working on a problem that I call ""Restaurant Context Analysis"". So, the problem in brief is that there is a large body of text available to me in which people describe their dining experiences and the cuisines they like the most, for instance consider the Yelp data set. Now while describing the experiences people often compare a similar dish to the one they had in the restaurant for which they describe their experience. Now, the problem that I am trying to solve is, I want to map the specific dishes to the restaurants in which they are served or are available and that I want to do solely from the raw text that is available to me, using some kind of contextual text analysis, that is I want to programmatically determine if the dish is also served in some other restaurant or not from the text itself. I am interested to know if there are engines or libraries available that do this for me.",4,3
100,2015-6-5,2015,6,5,23,38od19,Any voice learning program?,https://www.reddit.com/r/MachineLearning/comments/38od19/any_voice_learning_program/,JakeTheMaster,1433516314,"A software/project(or may opensource) which can learn from your human voice and then produce its own voice just like how you can say.


All you need is to read some specific passages/words. Then the computer can speak like how you can say. 




&gt; It could be a little different from speech command tools. It's not about controlling your computer via recognizing your voice. But it's more about to learn your voice, analyse your voice and produce a new 'machine' voice just like how you can say.

* In another way, it's like you've cloned your voice.


Any software/project like this? Thanks!",11,9
101,2015-6-6,2015,6,6,0,38ojh5,Twitter Sentiment Analysis tool using ConvNet in Python,https://www.reddit.com/r/MachineLearning/comments/38ojh5/twitter_sentiment_analysis_tool_using_convnet_in/,[deleted],1433518893,,0,1
102,2015-6-6,2015,6,6,1,38otzy,Machine Learning Model Predicting Tomorrow's Belmont Stakes. Will American Pharoah make it?,https://www.reddit.com/r/MachineLearning/comments/38otzy/machine_learning_model_predicting_tomorrows/,atakante,1433522927,,0,2
103,2015-6-6,2015,6,6,2,38oyc2,Overfitting or generalized? Comparison of ML classifiers,https://www.reddit.com/r/MachineLearning/comments/38oyc2/overfitting_or_generalized_comparison_of_ml/,urinec,1433524614,,0,1
104,2015-6-6,2015,6,6,2,38p2k1,Illegal Immigration &amp; the Known Unknowns - Computerphile [data mining],https://www.reddit.com/r/MachineLearning/comments/38p2k1/illegal_immigration_the_known_unknowns/,[deleted],1433526175,,0,0
105,2015-6-6,2015,6,6,7,38qd8p,Multiple Regression via Support Vector Machine,https://www.reddit.com/r/MachineLearning/comments/38qd8p/multiple_regression_via_support_vector_machine/,yahma,1433545034,"Looking through some of the popular libraries for SVM's (ie. Scikit Learn and Accord.NET), it seems they both support multi-class classification via SVM; however, regression analysis with multiple outputs via SVM seems to not be supported (unless I am missing something).  When one wishes regression analysis that require multiple outputs, do you typically forego SVM's and use other regression techniques (ie. Neural networks)?",4,3
106,2015-6-6,2015,6,6,8,38qfsw,Airbnb releases Aerosolve:Machine learning for humans,https://www.reddit.com/r/MachineLearning/comments/38qfsw/airbnb_releases_aerosolvemachine_learning_for/,[deleted],1433546125,,0,1
107,2015-6-6,2015,6,6,11,38r2lq,A bot for Agar.io,https://www.reddit.com/r/MachineLearning/comments/38r2lq/a_bot_for_agario/,Kalessar,1433557279,"I was thinking of implementing a bot for the popular web browser game [agar.io](http://agar.io), with the intention of trying to use some modern techniques in machine learning, and was wondering if anyone had some advice as to which techniques might produce interesting results.

A couple of years ago I was messing around with predator/prey simulations using neural networks and genetic algorithms. The results could often be surprisingly complex, but it would require hundreds of generations before even the simplest behaviors manifested themselves. I'm hoping this time to implement an AI that could learn in a more continuous manner from past experiences, instead of through random mutation. That is to say, navigate the solution space in a directed manner (backpropagation) and have the ability to learn features in sequential real-time data (maybe using RNNs?). 

What interests me with this game is both its simplicity and the fact that it is played by a number of human players, allowing the AIs to be pitted directly against smart contestants in online games and thus eliminating a number of low-complexity traps that can arise from competitive co-evolution.

So, if anyone has any ideas, research papers to read or techniques that might be fun to implement, please let me know! Moreover, if current tools or packages can be used to optimize the rate of improvement and make use of GPUs (instead of having to code everything from scratch, which can of course be in itself an interesting project), I would be interested in hearing advice from people with experience using such tools.",20,24
108,2015-6-6,2015,6,6,14,38rjr2,Hopping on the deep learning bandwagon - my first steps playing with deep learning,https://www.reddit.com/r/MachineLearning/comments/38rjr2/hopping_on_the_deep_learning_bandwagon_my_first/,yanirse,1433567024,,1,0
109,2015-6-6,2015,6,6,15,38rs0l,"Generating Recommendations using Apache Mahout, My attempt at writing",https://www.reddit.com/r/MachineLearning/comments/38rs0l/generating_recommendations_using_apache_mahout_my/,YahooGuys,1433572836,,0,0
110,2015-6-6,2015,6,6,18,38s3ri,Implementing Fisher's LDA in Python,https://www.reddit.com/r/MachineLearning/comments/38s3ri/implementing_fishers_lda_in_python/,cast42,1433583272,,0,1
111,2015-6-6,2015,6,6,18,38s4uo,Overfitting or generalized? Comparison of ML classifiers - a series of articles,https://www.reddit.com/r/MachineLearning/comments/38s4uo/overfitting_or_generalized_comparison_of_ml/,TJO_datasci,1433584341,,0,1
112,2015-6-6,2015,6,6,20,38sa24,Has anybody already saved the Coursera Machine Learning course made by Andrew NG onto files ?,https://www.reddit.com/r/MachineLearning/comments/38sa24/has_anybody_already_saved_the_coursera_machine/,jokoon,1433588774,"I'd like to watch those videos in a more peaceful and quiet environment maybe on my smartphone when I'm in public transport or at the library, I hate my apartment, I don't have a data plan on my smartphone and the library sucks.

For some reason this page https://class.coursera.org/ml-003/lecture doesn't let me download the file, so I have to use the other page, open each one, and click download manually, and the proposed file is not even properly name (index.mp4).

Maybe because this is a new year, but the videos seem to be the same, and I really can't watch it at home, I really want to save it.",11,0
113,2015-6-6,2015,6,6,22,38sj40,Vacuum Pressure Pumps for Diary Making Plants,https://www.reddit.com/r/MachineLearning/comments/38sj40/vacuum_pressure_pumps_for_diary_making_plants/,falconpumps,1433595891,,0,1
114,2015-6-6,2015,6,6,23,38sonf,Are there any opensource/commercial image classifiers?,https://www.reddit.com/r/MachineLearning/comments/38sonf/are_there_any_opensourcecommercial_image/,[deleted],1433599340,"With all those deep neural networks tagging images left and right I would of thought that someone by now would of made an app to allow people to search and classify every single image on their harddrives. But I can't seem to find one.



Is it because the problem isn't yet tractable? Is someone working on it?",3,1
115,2015-6-6,2015,6,6,23,38sqsy,"For neural networks, are there systematic approaches to finding the optimal number of nodes in the hidden layer?",https://www.reddit.com/r/MachineLearning/comments/38sqsy/for_neural_networks_are_there_systematic/,organic-neural-net,1433600582,"I remember in my first ML class my professor mentioning that the only ""rule of thumb"" for selecting the number of nodes in the hidden layer is that it should be roughly between the size of the input and output layers. Last night, I was messing around with a neural net program I wrote on the MNIST dataset and it seemed to me that the choice of the size of the hidden layer visibly affected performance. w/ input size being 28^2 and output being 10, i noticed it performed much better when the hidden layer size was closer to 100 nodes as opposed to the mean, ~400 nodes.

i'm aware of the grid-search approach to hyperparameter fitting and i was wondering if such any other systematic approaches exists in the context of choosing how many hidden nodes to include.

thank you for reading!",4,8
116,2015-6-6,2015,6,6,23,38sub5,"If you could start a Ph.D. In Machine Learning today, what would your research interest be?",https://www.reddit.com/r/MachineLearning/comments/38sub5/if_you_could_start_a_phd_in_machine_learning/,about3fitty,1433602541,,143,82
117,2015-6-7,2015,6,7,0,38sw8g,Classification using Pandas and Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/38sw8g/classification_using_pandas_and_scikitlearn/,validated1,1433603561,,8,27
118,2015-6-7,2015,6,7,0,38swtv,Has anyone tried using machine learning with the stock market?,https://www.reddit.com/r/MachineLearning/comments/38swtv/has_anyone_tried_using_machine_learning_with_the/,math238,1433603871,,20,2
119,2015-6-7,2015,6,7,0,38syhc,Best List of Statistical Data Mining Tutorials,https://www.reddit.com/r/MachineLearning/comments/38syhc/best_list_of_statistical_data_mining_tutorials/,john_philip,1433604717,,0,29
120,2015-6-7,2015,6,7,1,38t4rd,"Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines",https://www.reddit.com/r/MachineLearning/comments/38t4rd/continually_updated_data_science_python_notebooks/,donnemartin,1433607658,,8,76
121,2015-6-7,2015,6,7,6,38u7z2,Material for Andrew Ng's SEE Machine Learning course?,https://www.reddit.com/r/MachineLearning/comments/38u7z2/material_for_andrew_ngs_see_machine_learning/,bagelorder,1433626411,"I am currently [watching the lectures](https://www.youtube.com/watch?v=UzxYlbK2c7E&amp;list=PLA89DCFA6ADACE599) of his Stanford machine learning class (not coursera)

But the materials (i.e. the assignments and solutions) aren't online anymore, I am only able to find the materials for his 2014 class.

Does anyone know where I can get the old materials? The Stanford Engineering Everywhere page is down too.",7,0
122,2015-6-7,2015,6,7,8,38um80,"What's a good ML method for organizing PDFs by content, keyword, author, etc.?",https://www.reddit.com/r/MachineLearning/comments/38um80/whats_a_good_ml_method_for_organizing_pdfs_by/,sleepicat,1433633644,"Let's say I have a bunch of PDF files containing journal articles. Then I'd like to create a visual map, like a graph or mind map, with nodes that represent common keywords found in the articles. I'm not sure if this would be supervised or unsupervised (maybe I'd try both).  Any suggestions?  

I personally don't have a lot of experience with natural language machine learning methods, but maybe this would fall into that category. ",2,2
123,2015-6-7,2015,6,7,9,38uvr6,[1506.01186] No More Pesky Learning Rate Guessing Games,https://www.reddit.com/r/MachineLearning/comments/38uvr6/150601186_no_more_pesky_learning_rate_guessing/,[deleted],1433638737,,0,1
124,2015-6-7,2015,6,7,21,38wf4b,Materials for Learning Machine Learning: Any additional resources you can recommend?,https://www.reddit.com/r/MachineLearning/comments/38wf4b/materials_for_learning_machine_learning_any/,jackbrucesimpson,1433678470,,9,26
125,2015-6-7,2015,6,7,21,38wj1r,"X-Post from /r/compsci: Can a program read a short text, create a question based on the text, and grade the answer?",https://www.reddit.com/r/MachineLearning/comments/38wj1r/xpost_from_rcompsci_can_a_program_read_a_short/,Surfn2live,1433681388,"I've been trying to find a program that can take a small text input such as this:

""North American apple harvesting began with the settlers at Jamestown in 1607. They brought with them seeds and cuttings from Europe, and while the original varieties planted were not all suited for cultivation. ""

Create as many questions from that text as possible, say: ""Where did American apple harvesting begin?""

And grade any answer, ""Jamestown"" or ""apple harvesting began in Jamestown,"" to be correct.

I've found these two research papers on the topic,  (PDFs) [Evaluating HILDA in the CODA Project](http://www.aaai.org/ocs/index.php/FSS/FSS11/paper/download/4158/4486) and [Automatic Factual Question Generation from Text](https://errico.srv.cs.cmu.edu/research/thesis/2011/michael_heilman.pdf) but I am not a computer scientist and I did not put together that the program was created. The papers mostly discuss the challenges of the syntax and semantics of English and/or language in general.

Those are 3 and 4 year old papers. Other than that all I've found is that the program doesn't yet exist and is a quite complicated problem.

If anyone knows of something, I would love to hear about it. I am interested in the code to create a commercial product around it.
",6,12
126,2015-6-7,2015,6,7,22,38wm8d,"The honest DARPA Robotics Challenge The robotics community, for all its intellectual prowess, is rife with hypocrisy.",https://www.reddit.com/r/MachineLearning/comments/38wm8d/the_honest_darpa_robotics_challenge_the_robotics/,samim23,1433683614,,0,0
127,2015-6-8,2015,6,8,0,38wxzz,What techniques should I use to categorize reddit comments?,https://www.reddit.com/r/MachineLearning/comments/38wxzz/what_techniques_should_i_use_to_categorize_reddit/,[deleted],1433690419,"This is just a project I'm doing for fun, but I want to build a bot that reads comments and categorizes them. On top of that, I would like to rate them within that category, for example say the comment is within the category ""feminism"", then I would like to give is a rating from 1-10 based on the relevance.

I know this is pretty difficult, I've been playing around with multiple NLP libraries and Google's word2vec, and with that I've been thinking about some ideas to represent each comment as a small vector. It doesn't have to be very accurate, just something that performs better than a random implementation, and that can learn from new data.

I don't have much training data, I would have to manually tag a bunch of comments, so I would need something that can initially be trained with in the order of 100s of examples. 

Can someone give me some suggestions on what techniques are best suited for this task? ",3,1
128,2015-6-8,2015,6,8,0,38x2f6,Machine Translation Helps Patent Offices Worldwide - Pangeanic Translation Technologies &amp; News,https://www.reddit.com/r/MachineLearning/comments/38x2f6/machine_translation_helps_patent_offices/,deenafeegley,1433692703,,1,0
129,2015-6-8,2015,6,8,1,38x3qu,What Machine Learning Teaches Us About Ourselves,https://www.reddit.com/r/MachineLearning/comments/38x3qu/what_machine_learning_teaches_us_about_ourselves/,lebinh,1433693358,,5,9
130,2015-6-8,2015,6,8,2,38xelx,Question about Person Identification,https://www.reddit.com/r/MachineLearning/comments/38xelx/question_about_person_identification/,__null__,1433698698,What could be the role of Person Identification within semantic video anaylisis scenarios such as Facility surveilance or Ambient Assisted Living for the elderly ?,0,0
131,2015-6-8,2015,6,8,3,38xibm,Analysis of two output methods in neural network-based OCR,https://www.reddit.com/r/MachineLearning/comments/38xibm/analysis_of_two_output_methods_in_neural/,mateogianolio,1433700406,,0,1
132,2015-6-8,2015,6,8,4,38xsd1,Rehabilitation of Count-based Models for Word Vector Representations,https://www.reddit.com/r/MachineLearning/comments/38xsd1/rehabilitation_of_countbased_models_for_word/,elyase,1433705032,,2,10
133,2015-6-8,2015,6,8,4,38xueq,Ideal computer for simulation &amp; calculations: FX8320E @ 4.3Ghz or a GTX 960?,https://www.reddit.com/r/MachineLearning/comments/38xueq/ideal_computer_for_simulation_calculations/,[deleted],1433705980,"I'm running some calculations for a Turing-complete, deep learning system. On a Raspberry Pi 2, it has taken roughly one day to learn the entire works of Shakespeare and not with the largest neuron network it could have. The good news is I have a larger rig that I just made a partition on for Debian. 

My rig has an overclocked 8320E. It is 4.32Ghz with 8 cores. 

Then I have a Nvidia GTX 960. This is not the best graphics card and I primarily use it for gaming. But would it be faster than my CPU? 

I have 16GB of RAM, if that helps. I'm pretty sure the neural network program I'm using is multithreaded because anything else I do on a computer it is running on slows calculation time down significantly.

Also, the neural network uses CUDA for learning and processing. So it could take advantage of that. 

If the only way to find out is to do it, then I will but I didn't want to wait a long time to do very layered networks (With low entropy :\ )",4,0
134,2015-6-8,2015,6,8,5,38y1y3,I made a framework for creating Neural Networks easily and was looking for advice,https://www.reddit.com/r/MachineLearning/comments/38y1y3/i_made_a_framework_for_creating_neural_networks/,iRaphael,1433709387,"Hey /r/machinelearning! 

I'm a rising sophomore CS undergrad who recently got very interested in AI and ML. I took Andrew Ng's machine learning class on coursera and, for the past week or so, I've been writing my own implementation of a [framework for neural networks in python called ""mental""](https://github.com/iRapha/mental/).

Right now, you can create networks of any shape, set learning rates, regularization parameters, max iterations, and even plot the cost function vs iterations.

I know this sub is full of PhDs who probably use frameworks much more sophisticated than this, but I thought that mental was a pretty cool little module for when you want to test things without worrying about specifics of a framework. Plus it was an amazing learning experience for me :)

But the reason I'm posting is less about promotion. I was looking for advice on what to do next. I have a few things I know I want to get done (more builtin plotting options, more activation functions) but other than that I open for advice. Should I try to make this neural network implementation better (how)? More efficient? Or do you guys think I should try to implement other ML algorithms, like a recommender system or a k-means cluster?

Again, I am kind of new to this so any advice is appreciated :D
",11,6
135,2015-6-8,2015,6,8,6,38y53v,Twitter Sentiment Analysis using ConvNet in Python,https://www.reddit.com/r/MachineLearning/comments/38y53v/twitter_sentiment_analysis_using_convnet_in_python/,yaolubrain,1433710847,,10,19
136,2015-6-8,2015,6,8,7,38yf8q,"""I started deep learning and I am serious about it: Start with one GTX 680 and buy more GTX 680s as you feel the need for them; save money for Pascal GPUs in 2016 Q2/Q3 (they will be much faster than current GPUs)""",https://www.reddit.com/r/MachineLearning/comments/38yf8q/i_started_deep_learning_and_i_am_serious_about_it/,lozj,1433715581,"Agree or disagree?  

Post is https://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/ but Reddit won't let me submit it as a link.

",37,11
137,2015-6-8,2015,6,8,7,38yh8n,O'Reilly publishing Deep Learning book this winter.,https://www.reddit.com/r/MachineLearning/comments/38yh8n/oreilly_publishing_deep_learning_book_this_winter/,theirfReddit,1433716577,,38,77
138,2015-6-8,2015,6,8,11,38z8nf,Introductory stuff on Vector Space Models,https://www.reddit.com/r/MachineLearning/comments/38z8nf/introductory_stuff_on_vector_space_models/,ravo87,1433730087,"Hi, I want to start learning about vector space models and continuous vector space models in context of NLP. Could you point me to a good starting points - articles/papers/books, etc? Thanks",6,0
139,2015-6-8,2015,6,8,13,38zp4q,SURF improvements,https://www.reddit.com/r/MachineLearning/comments/38zp4q/surf_improvements/,no_porner,1433738754,"Hey all, I am working on a project that detects object in a video. I am using SURF (Speeded up Robust Features) technique for that. I have implemented the basic algorithm. Now I am looking for some improvements that can be done or something which can help me show that this is the new work done by me. Could you suggest some papers or code which can guide me.
Thank you",2,0
140,2015-6-8,2015,6,8,14,38zw9w,Bringing Deep Learning to the Grocery Store,https://www.reddit.com/r/MachineLearning/comments/38zw9w/bringing_deep_learning_to_the_grocery_store/,john_philip,1433743033,,0,0
141,2015-6-8,2015,6,8,16,3903dk,Exploration of Ordinal Data Possibilities with Games,https://www.reddit.com/r/MachineLearning/comments/3903dk/exploration_of_ordinal_data_possibilities_with/,sheepsy90,1433747854,"Hello,

I am currently writing my master thesis in computer science with the Topic ""Ordinal Data in Games"". I already did a small study and want to refine my results now.

I therefore need a lot more people who can answer my small survey. You decide yourself how long do you want to participate. The questions are not depended on each other.

Just give it a try and read the one page description on http://heleska.de:8000/ and continue afterwards directly to the study.

I would be glad if you help me.

Greetings Sheepy
",0,3
142,2015-6-8,2015,6,8,16,390409,ActionML - Professional Machine Learning Development with Apache Spark,https://www.reddit.com/r/MachineLearning/comments/390409/actionml_professional_machine_learning/,predictionio,1433748311,,0,0
143,2015-6-8,2015,6,8,16,3904f8,Disposable Items in Haryana,https://www.reddit.com/r/MachineLearning/comments/3904f8/disposable_items_in_haryana/,devenderakumar,1433748642,,0,0
144,2015-6-8,2015,6,8,18,390eh4,Software for learning structure and updating fit of a bayesian network?,https://www.reddit.com/r/MachineLearning/comments/390eh4/software_for_learning_structure_and_updating_fit/,HelmsmanRobertson,1433756546,"Hi all,

I'm looking for software packages/libraries that can perform the following:

1. learn the network structure of some data, `D1`.
2. fit network parameters to learned structure, using `D1`.
3. update parameters to fitted structure (output of step #2) using new evidence, `D2`.

I've found various software packages to perform steps #1 &amp; #2, e.g

* PEBL (python)
* deal (R)
* bnlearn (R)

but none of these seem to do all 3 steps. Or at least, there's no documented way to do this. Does anyone know of any software, or any way to use the above software to do this?

Thanks, ",3,1
145,2015-6-8,2015,6,8,19,390jzf,"Graphical models: how to come up with a representation, are there best practices?",https://www.reddit.com/r/MachineLearning/comments/390jzf/graphical_models_how_to_come_up_with_a/,ML_Learner,1433760748,"I'm currently trying to get into graphical modeling (using Bayesian Networks for now) and am kind of stuck. How do I come up with a suitable representation for a problem? I'm not trying to **improve** machine learning techniques, but instead I'd like to **use** it on a certain problem. In my case, I'll be using Matlab and Murphy's Bayes Net Toolbox (freely available here: [source](https://github.com/bayesnet/bnt) ).

All the machine learning books I've checked out so far go into great depth about machine learning (i.e. Murphy's ""Machine Learning: A probabilistic perspective"" or Koller/Friedman ""Probabilistic graphical models: principles and techniques""), but I've not seen a more superficial publication which talks about how some often occuring types of problems should be modeled.

For example, a simple classification task: there are a number of observable, continuous features (random variables) which should be used to predict human behavior.
What's a good starting point for a model?

For example, DeVaul uses this to classify one of two classes ([source: about in the middle of this file](https://www.media.mit.edu/wearables/mithril/BNT/mixtureBNT.txt)):

             +-------+
    Node 1   | Class |
             |  A/B  |
             +-------+
                | \
                |  \
                |   \
                |    V
                |  +-----------+
                |  | component |  Node 2
                |  |    1/2    |
                |  +-----------+
                |       /
                |      /
                V     V
               .--------.
     Node 3   /          \
             /  Gaussian  \
             \  mu, sigma /
              \          / 
               \________/ 

I've not seen this one anywhere else. Is this a good starting point?

Another one I've seen is the model in figure 1 on page 2 of [this paper](http://arxiv.org/pdf/1504.00060.pdf). It's a temporal model, but just use the left column of it and that's basically a 'normal' static bayesian network. I would love to understand the reasoning how someone comes up with a model like this.

TL;DR version: are there any good practices out there for representing standard problems as graphical models? Is this dicussed/explained anywhere?",3,5
146,2015-6-8,2015,6,8,20,390n55,Similarity between sets of documents based on (latent) topic modeling?,https://www.reddit.com/r/MachineLearning/comments/390n55/similarity_between_sets_of_documents_based_on/,Mattoss,1433762988,"Hey,

I have this setting where I try to determine which of a set of documents is closest to a specific set of documents. While thinking about this problem I thought this should be possible by modeling topics inferred by these sets of documents. Unfortunately I have no experience with topic modeling yet, so I was wondering if you could point me to resources that deal with this problem. My ideas so far where to infer an arbitrary number of topics of the ""target"" set of documents, and then to look at each document in each of my other document sets in order to see how much they are similar to these topics. The other idea was to model topics for each document set and try to compare each sets' distribution over topics. However I don't know how to do this when using arbitrary topics. The third idea was to use some kind of clustering approach for all documents and try to see which clusters are closest. Maybe you could comment on these ideas? Thx, Matt",10,2
147,2015-6-8,2015,6,8,22,39116i,RNNs and temporal convolutions for gesture recognition in video,https://www.reddit.com/r/MachineLearning/comments/39116i/rnns_and_temporal_convolutions_for_gesture/,flipcaul,1433771157,,0,22
148,2015-6-8,2015,6,8,23,3914qp,Google won Microsoft COCO competition!,https://www.reddit.com/r/MachineLearning/comments/3914qp/google_won_microsoft_coco_competition/,test3545,1433772856,,22,92
149,2015-6-9,2015,6,9,1,391o3l,Behind the Machine Learning Approach Driving Google's Green Datacenters,https://www.reddit.com/r/MachineLearning/comments/391o3l/behind_the_machine_learning_approach_driving/,[deleted],1433781181,,0,2
150,2015-6-9,2015,6,9,1,391r4o,Your Biggets ML Question?,https://www.reddit.com/r/MachineLearning/comments/391r4o/your_biggets_ml_question/,tfimg24,1433782417,"Hey all,
I'm an editor over at Terminal.com's blog and we're working on article ideas for our ML readers. This seems like a really great community, and if you post your most common/difficult questions here I'd love to put them to our writers and see what comes of it.
Cheers! ",3,0
151,2015-6-9,2015,6,9,2,391uah,"Rodeo 0.4: Spark, themes, resizable panes",https://www.reddit.com/r/MachineLearning/comments/391uah/rodeo_04_spark_themes_resizable_panes/,theglamp,1433783677,,0,1
152,2015-6-9,2015,6,9,2,391y4q,Alex Smola does a nice overview on Deep Learning frameworks and hardware,https://www.reddit.com/r/MachineLearning/comments/391y4q/alex_smola_does_a_nice_overview_on_deep_learning/,USER_PVT_DONT_READ,1433785277,,26,8
153,2015-6-9,2015,6,9,2,391yxu,"Computational Implications of Reducing Data to Sufficient Statistics [PDF, draft, abstract in comments]",https://www.reddit.com/r/MachineLearning/comments/391yxu/computational_implications_of_reducing_data_to/,carmichael561,1433785599,,2,9
154,2015-6-9,2015,6,9,3,3929qt,how feasible is a people search engine using facial recognition?,https://www.reddit.com/r/MachineLearning/comments/3929qt/how_feasible_is_a_people_search_engine_using/,loveComputer,1433789947,"I did some experimenting with the animetrics api and it wasn't bad I put in portrait type photos of three people but with some at odd angles and tested it, against a photo of one of the people. The correct one was assigned a score of about .85, and the two incorrect ones were assigned .65 and .45, but its hard from doing experiments to get a good sense of these things at scale as the number of look a likes increases.",3,1
155,2015-6-9,2015,6,9,5,392kao,HTM.java Receives New Network API,https://www.reddit.com/r/MachineLearning/comments/392kao/htmjava_receives_new_network_api/,numenta,1433794146,,0,1
156,2015-6-9,2015,6,9,5,392nwy,Interview Questions for Data Scientist Positions,https://www.reddit.com/r/MachineLearning/comments/392nwy/interview_questions_for_data_scientist_positions/,[deleted],1433795581,,5,2
157,2015-6-9,2015,6,9,7,3938ii,Does the choice of operating system make a big difference for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3938ii/does_the_choice_of_operating_system_make_a_big/,OrangePurpleGreen,1433804276,"I will be attending KTH in Sweden this autumn to study Machine Learning. I need to invest in a reasonably good laptop to accompany me there. I only know I want a 13.3"" with a battery that lasts the day.

Most of the people I know swear by MacBook Pros, but will I encounter any unforeseen problems or difficulties with OS X? Perhaps not much of the open source software will be available? Would dual booting OS X with either Windows or Linux be the best way?

If not going the mac route then I would be dual booting Windows and Linux. Is that perhaps the better choice?",18,0
158,2015-6-9,2015,6,9,9,393nbq,State of Hyperparameter Selection,https://www.reddit.com/r/MachineLearning/comments/393nbq/state_of_hyperparameter_selection/,arshakn,1433811164,,4,10
159,2015-6-9,2015,6,9,10,393oth,Videos to download for travel,https://www.reddit.com/r/MachineLearning/comments/393oth/videos_to_download_for_travel/,ownallogist,1433811858,"Hi all,

Can anyone recommend some great video resources for beginners? Possibly with practical examples in python? I've heard positive things about Sentdex's youtube channel. Will be traveling a ton in the upcoming months and was hoping to spend the time learning through watching. I am currently taking notes on Ng's Machine Learning lectures so will have those in paper form but would also like to download some other videos. 

Thanks!",0,0
160,2015-6-9,2015,6,9,10,393t53,[Jeff Hawkins / On Intelligence] What are the problems with the theory?,https://www.reddit.com/r/MachineLearning/comments/393t53/jeff_hawkins_on_intelligence_what_are_the/,vit47,1433813880,"I've read through Jeff Hawkins' book titled 'On Intelligence' and also Ray Kurzweil's 'How to Create Mind'...it all sounds pretty convincing but are there any obvious holes in the HTM theory? I guess at this point I'm left wondering why they haven't hooked up various sensors (vision, pressure, etc) to a robot running the Numenta python code and let it move around and learn how to navigate mazes or something similar. Is there a missing link here in the theory that I'm not understanding? 

I'm also not understanding how one would give the robot any sort of purpose. It seems like you'd have a bunch of inputs going into this robot with an HTM network and it would just sit there like a lump. Do you have to feed it data sets to get it started like with traditional neural networks?",15,7
161,2015-6-9,2015,6,9,11,3940mx,"Anyone interested in going through the ""Open Source Data Science Masters"" curriculum as an absolute data science beginner with me? (x-post)",https://www.reddit.com/r/MachineLearning/comments/3940mx/anyone_interested_in_going_through_the_open/,[deleted],1433817374,,0,1
162,2015-6-9,2015,6,9,11,3943au,HTMs - What does it mean when converting to sparse distributed representation?,https://www.reddit.com/r/MachineLearning/comments/3943au/htms_what_does_it_mean_when_converting_to_sparse/,manliest_destiny,1433818585,"I am reading through some of the high level papers and it talks about converting bits with a varying percentage of active bits to a sparse region where 2% are active. 

Can I think of this analogously like a hash function? There are of course differences, because collisions would be less frequent I'd assume, but surely it applies the same idea of converting a massive amount of data into something smaller that can be referenced.

Am I off in thinking this?",2,1
163,2015-6-9,2015,6,9,12,394av6,Online learning with R,https://www.reddit.com/r/MachineLearning/comments/394av6/online_learning_with_r/,rudyl313,1433822361,"Are there any good libraries for doing online learning (i.e. streaming the rows of data to train the model instead of having all the data in memory) with R?

What kinds of models can I build with an online approach? ",2,1
164,2015-6-9,2015,6,9,17,394yrf,Large-scale Simple Question Answering with Memory Networks,https://www.reddit.com/r/MachineLearning/comments/394yrf/largescale_simple_question_answering_with_memory/,[deleted],1433837167,,0,1
165,2015-6-9,2015,6,9,17,3951b3,Power Electronics For Electric Vehicles 2015-2025,https://www.reddit.com/r/MachineLearning/comments/3951b3/power_electronics_for_electric_vehicles_20152025/,rakeshmrr,1433839116,,0,1
166,2015-6-9,2015,6,9,18,395423,[1506.01497] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,https://www.reddit.com/r/MachineLearning/comments/395423/150601497_faster_rcnn_towards_realtime_object/,Cubbee_wan,1433841250,,2,12
167,2015-6-9,2015,6,9,19,39586d,Variational Dropout and the Local Reparameterization Trick,https://www.reddit.com/r/MachineLearning/comments/39586d/variational_dropout_and_the_local/,dpkingma,1433844277,,4,18
168,2015-6-9,2015,6,9,19,3958i1,Graduation Project: Choosing a target market,https://www.reddit.com/r/MachineLearning/comments/3958i1/graduation_project_choosing_a_target_market/,[deleted],1433844527,"Hello Reddit,


First of all excuse me for any spelling/grammatical errors I made in this post as English is not my primary language. The purpose of this post is to collect guidelines, tips, experiences and references to literature (or any other form of information) as input for my graduation project.

 

**About me**

I'm a 21 years old dutch student, doing a study to become a software engineer (I'm not sure about the name in english). Currently I'm busy with my graduation project, for which I'll need to choose and test several Machine Learning algorithms for a case given to me within the company. I have no prior experience to Machine Learning, so it's a bit of a struggle (but I chose for it myself since I find the subject very intriguing).

**The Question in general**

Given a product, which market characteristics are a best match?

So basically the input will be product, and characters output.
Product =&gt; {C1, C2, Cn... }

So for example, a marketer wants to advertise for a specific product, product A. He'll give this product as input. Based on this, the system should respond with the target market he should make advertisements for. An example would be: Female, 20-30 years, has a car

**The problem**

For my assignment, I'll have to choose a Machine learning algorithm, that will be able to select the most succesful market segment characteristics for that product. Based on transactional data, which could look something like this:

http://i.imgur.com/t8ttVZL.png

So a product can occur multiple times, like here (with other characteristics):

http://imgur.com/IqD1iyC


(I don't have any real data, but I've been told it's safe to assume merged together it'll look like this when dummyfied).


Given this, the product (products will be clustered in relevant before this step, but my assignment is about this step) should predict what categories are relevant for the marketers.

I've done quite a bit of literature research, and tried to find how other people solved similar problems. After a lot  of searching, I've made the following list:

* Naive bayes
* Neural Network
* Random forest
* Decision Tree
*Support Vector Machine (of Support Vector Network)
*Logistic Regression
* Apriori (ass. rules)
* Eclat (ass. rules)
* FP-growth (ass. rules)
* k-means
* Collaborative filtering
* Principal Component Analysis(PCA)

Now I'm pretty sure that PCA isn't the solution. It might be useful to shorten the amount of features, but it won't solve my main problem.

So far, by prototyping a bit, I found that Collaborative filtering and Ass. rules solutions seem to give a reasonable solution, but that's purely from an informal, exploratory view.

However, that's as far as I've come.

**The Questions**

I'm trying to shorten the list to candidates to test for the given project. I assume it's a supervised problem (characteristics are the labels). But I can't seem to wrap my head on how this should be implemented. Most examples I find have multiple predictors and a single response variable, while I'm trying to predict multiple responses, based on a single predictor.

1. Am I on the right track? I've really been doubting myself a lot regarding the choices. These options seem to be used for marketing/market segmentation, or how you'll call it, but I don't see them solving my problem.
2. If I am, how can I shorten the current list so I have enough time to test the remaining algorithms? Right now what I've come up with are: Transparency, predict multiple labels, work with binary data and work with transactional data (so only YES purchases, not NO purchases which means probability of buying is out of the question).


I haven't had a course in mathematics in my study, so I can't really translate the difficult formula's usually given.

I hope I've been clear enough with my post.

Thanks in advance
 

Edit: Clarified, second screenshot and better explanation


",1,1
169,2015-6-9,2015,6,9,19,3958ym,A Recurrent Latent Variable Model for Sequential Data,https://www.reddit.com/r/MachineLearning/comments/3958ym/a_recurrent_latent_variable_model_for_sequential/,kkastner,1433844863,,26,13
170,2015-6-9,2015,6,9,19,395aid,Computer independently solves 120-year-old biological mystery (Wired UK),https://www.reddit.com/r/MachineLearning/comments/395aid/computer_independently_solves_120yearold/,keptavista,1433846012,,22,80
171,2015-6-9,2015,6,9,20,395ddf,Developing World Wide Cloud Baby,https://www.reddit.com/r/MachineLearning/comments/395ddf/developing_world_wide_cloud_baby/,captaink,1433848061,,1,0
172,2015-6-9,2015,6,9,20,395esc,Use Pandas and Scikit learn for Classification tasks,https://www.reddit.com/r/MachineLearning/comments/395esc/use_pandas_and_scikit_learn_for_classification/,john_philip,1433848974,,6,21
173,2015-6-9,2015,6,9,23,395wmy,Student can't decide what ML project to do. Does a ML project to decide for him.,https://www.reddit.com/r/MachineLearning/comments/395wmy/student_cant_decide_what_ml_project_to_do_does_a/,dive118,1433858952,,6,3
174,2015-6-9,2015,6,9,23,395yyk,[1506.02516] Learning to Transduce with Unbounded Memory,https://www.reddit.com/r/MachineLearning/comments/395yyk/150602516_learning_to_transduce_with_unbounded/,egrefen,1433859984,,5,6
175,2015-6-10,2015,6,10,0,3964mq,Stacked What-Where Autoencoders,https://www.reddit.com/r/MachineLearning/comments/3964mq/stacked_whatwhere_autoencoders/,vikkamath,1433862468,,2,0
176,2015-6-10,2015,6,10,0,396988,Machine Learning of Software User's Behaviour,https://www.reddit.com/r/MachineLearning/comments/396988/machine_learning_of_software_users_behaviour/,BobBeaney,1433864320,"I am learning about Machine Learning by myself. I have a passable understanding of some common basic Machine Learning algorithms and have seen common examples covered in introductory courses : digit recognition, spam email classification, movie recommender systems and so on. 

I am also quite interested in finding out more about complex software systems that can learn their users behaviour. I am sure that such systems exist (eg I have an Outlook addin that learns how I like to file my emails) but I don't know where to look for more info or even what the right search terms to use. At the heart of the matter I suspect that the same kind of algorithms would be used to learn user behaviour as are used to recognize hand written digits. However I think there are probably other aspects involved (eg how do you create an adaptive ""learning"" user interface that doesn't drive your users crazy) and I would be interested in real experiences and case studies. I suspect that there are journals, forums, websites and blogs that deal with these topics specifically but I am not sure where to look. Can someone point me in the right direction (even if it is to say ""No that kind of thing does not exist"")
",5,0
177,2015-6-10,2015,6,10,2,396o0n,Experience training Support Vector Data Description (SVDD) with counter examples?,https://www.reddit.com/r/MachineLearning/comments/396o0n/experience_training_support_vector_data/,mryanbell,1433870214,"I am currently working on a problem where [SVDD](http://insy.ewi.tudelft.nl/sites/default/files/ML_SVDD_04.pdf) seems like an ideal classification approach. My training data covers the range of my target class pretty well, but I have spotty coverage of the negative class. SVDD will allow me to train a decision boundary that explicitly surrounds the positive class examples in my training set. Anything outside of that enclosed surface should be classified as being a negative example.

Now for my question: Does anyone have any experience training SVDD *including counter-examples?* Tax &amp; Duin discuss this possibility in their paper and it seems useful to me. I *have* counter-examples after all, and even if they don't fully represent the space of all counter-examples, including them in training will still be helpful.

Are there any implementations of SVDD out there that allow for this? There is an [extension to libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/) that implements SVDD, but it is not immediately clear to me whether it can handle counter-examples. Will I have to write my own SVDD implementation? Not a big deal if so, but I would prefer to avoid it if something exists already. ",0,0
178,2015-6-10,2015,6,10,4,39793c,Is everything I know about ML/stats outdated?,https://www.reddit.com/r/MachineLearning/comments/39793c/is_everything_i_know_about_mlstats_outdated/,townie92,1433878439,"Came across this article http://www.datasciencecentral.com/profiles/blogs/data-science-without-statistics-is-possible-even-desirable

Almost every point in the article runs counter to what I have learned about stats/ml/data science. I understand that the most salient methods/models vary widely depending on the application and that there are areas where ""new"" statistical techniques work better than ""old"" ones. But the argument that ""old"" statistical techniques are barely used in data science is, to put it mildly, surprising to me.


",12,3
179,2015-6-10,2015,6,10,5,397dma,"Hands on with a simple problem (Rotated OCR Text), advices??",https://www.reddit.com/r/MachineLearning/comments/397dma/hands_on_with_a_simple_problem_rotated_ocr_text/,[deleted],1433880177,"Hello all,

Long time ~~programmer~~ tinkerer (C++, but mostly Matlab with a dab of Python) first time in Neural net.

I followed Andrew Ng's coursera class, to help me get a rudimentary overview of ML concepts. I am halfway through following Geoffrey Hinton's class, which goes into greater details about bits and pieces of NN. I have been through Theano's first tutorials, and feel that I am starting to get a hang of things, though on a Macbook Air, it feels like experimenting will be painful.

I feel I have reached a point where I should start experimenting more and apply some of the techniques covered in Geoffrey Hinton's class. I have selected a simple problem, that of skewed numbers for OCR purposes. Essentially, I would like to OCR text that may have been warped (as in perspective projection) and rotated.

In order to keep the problem simple, I have decided to use a monospace font, with black printed on white. So we are looking at digits ranging from 0-9, with rotation angles ranging from -15 degrees to +15 degrees, and non-uniform scaling ranging from -20% to +20% on the left side and -20% to +20% on the right side of the letters (this whole approach is subject to debate, but I have found this to be most practical to generate test cases and size the search space).

So, if we break this down by single degrees and single %, the search space yields 30 * 40 * 40 for each digit, so a total of 480000 combinations, spread over a grid of w x p (= n) pixels.

Does this mean I need a minimum 480000 x n  inputs?
If I want to use convolutional NN, does this means I need to use this say x 4 or x 16?

What about the hidden layers?

Last bit, regarding outputs: would it be possible to have 10 outputs (for each digits) + 3 (rotation + scale1 + scale2)?

Thanks for any pointers, considerations or other approaches.

PS: Any good books or references on the design of NN (choice of parameters etc..)?






",0,0
180,2015-6-10,2015,6,10,5,397f3z,SparkR will ship with Apache Spark version 1.4,https://www.reddit.com/r/MachineLearning/comments/397f3z/sparkr_will_ship_with_apache_spark_version_14/,gradientflow,1433880769,,3,24
181,2015-6-10,2015,6,10,6,397s62,DeepMind Nature paper question,https://www.reddit.com/r/MachineLearning/comments/397s62/deepmind_nature_paper_question/,[deleted],1433885898,"Stupid question but still looking for clarification: In the Nature paper, why are they using expected values when creating their cost-value function Q? What is it about the atari games that are stochastic? Are there random values being generated in the games, or are they completely deterministic (yet largely complex, state/action space wise). 

Or is the expected value taken just because the user does not know how actions will affect the game? This is a fundamental difference in the way I've traditionally though of stochastic control, that actions affect the * system * by altering transition probabilities and immediate rewards, not through affecting the user's knowledge of these system properties.  Any thoughts are appreciated!

Edit: From the downvotes I'm guessing this is the wrong subreddit for this question, so suggestions for other places to post are appreciated as well!",9,5
182,2015-6-10,2015,6,10,6,397uxi,Developments in deep learning by Geoff Hinton,https://www.reddit.com/r/MachineLearning/comments/397uxi/developments_in_deep_learning_by_geoff_hinton/,ojaved,1433886954,,1,0
183,2015-6-10,2015,6,10,8,39897w,Significant Machine Learning Milestones?,https://www.reddit.com/r/MachineLearning/comments/39897w/significant_machine_learning_milestones/,devDorito,1433893092,"I've been following this sub for a bit and wanted to have a discussion on what you would consider significant milestones have been reached in machine learning.

",29,5
184,2015-6-10,2015,6,10,9,398gtj,Learning to Transduce with Unbounded Memory | The latest publication from Google DeepMind,https://www.reddit.com/r/MachineLearning/comments/398gtj/learning_to_transduce_with_unbounded_memory_the/,mrprint,1433896595,,0,2
185,2015-6-10,2015,6,10,13,3999hj,The Subreddit Simulator - Pure genius.,https://www.reddit.com/r/MachineLearning/comments/3999hj/the_subreddit_simulator_pure_genius/,Dwood15,1433910293,,2,4
186,2015-6-10,2015,6,10,15,399ku2,ELI5 Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/399ku2/eli5_variational_autoencoders/,CreativePunch,1433916852,"Hi all,

I have been reading up on variational autoencoders and I am slowly starting to understand it but I was wondering if anyone could give me a good explanation.

I also am particularly lost on the reparametrization trick which seems to be very important for VAE's

Thanks!",4,5
187,2015-6-10,2015,6,10,15,399m06, Why its time for small business to get onboard with machine learning,https://www.reddit.com/r/MachineLearning/comments/399m06/why_its_time_for_small_business_to_get_onboard/,ITmonitoring,1433917612,,0,1
188,2015-6-10,2015,6,10,15,399m14,Using LDA to identify rotated characters,https://www.reddit.com/r/MachineLearning/comments/399m14/using_lda_to_identify_rotated_characters/,jackbrucesimpson,1433917629,"I was reading this [paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4232994) and am interested in trying to use LDA to identify rotated shapes that look like letters. Would the best way to test this idea out be to take a training set of images (30x30 pixels), convert the pixels in each image to a column in a matrix and then perform an LDA on them to see if this can work? I'm interested in LDA rather than PCA because there's a fair bit of variation in the brightness of my characters and Eigenfaces is supposed to not cope too well with this.",0,0
189,2015-6-10,2015,6,10,15,399nv6,China Automotive Heat Exchanger Industry Market Research Report 2014-2018,https://www.reddit.com/r/MachineLearning/comments/399nv6/china_automotive_heat_exchanger_industry_market/,rakeshmrr,1433918883,,0,1
190,2015-6-10,2015,6,10,15,399ooe,Testing image recognition platforms on a famous psychology image.,https://www.reddit.com/r/MachineLearning/comments/399ooe/testing_image_recognition_platforms_on_a_famous/,Noncomment,1433919466,,15,60
191,2015-6-10,2015,6,10,16,399p2q,Visualizing and Understanding Recurrent Networks,https://www.reddit.com/r/MachineLearning/comments/399p2q/visualizing_and_understanding_recurrent_networks/,iori42,1433919725,,1,15
192,2015-6-10,2015,6,10,16,399pde,Path-SGD: Path-Normalized Optimization in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/399pde/pathsgd_pathnormalized_optimization_in_deep/,iori42,1433919928,,3,10
193,2015-6-10,2015,6,10,16,399pdi,How accurate is current object recognition through texture analysis?,https://www.reddit.com/r/MachineLearning/comments/399pdi/how_accurate_is_current_object_recognition/,[deleted],1433919930,"Could you hypothetically tell the difference between a white towel and a white bed sheet purely through image processing? Are there any tools that do this currently?
",0,0
194,2015-6-10,2015,6,10,16,399s29,Deep learning techniques that do NOT rely on gradient descent?,https://www.reddit.com/r/MachineLearning/comments/399s29/deep_learning_techniques_that_do_not_rely_on/,zakou,1433921785,"Are there credible alternatives, i.e. that could approach the state-of-the-art on images or language, but that construct a deep representation in other ways?",13,3
195,2015-6-10,2015,6,10,17,399w7u,Validation set size tradeoffs in NNs,https://www.reddit.com/r/MachineLearning/comments/399w7u/validation_set_size_tradeoffs_in_nns/,ddofer,1433925010,"I've started playing with CNNs (In Lasagne/Keras) - my instinct in general is to use a ""large"" validation set (out of my training data) when training my model. 
However, this means my model is training without all that data! (And makes it ""weaker"" for, say, making a prediction on unknown data for Kaggle :) ). 

I was curious how people work-around this, assuming more data is not available. 

One option would be to select an architecture and number of epochs (i.e hyperparameters) based on the validation error with a large validation set hold-out, then retrain the network with no hold-out data. 
This suffers, in that more training epochs might be needed given the increased amount of data. 

Another option would be to build an ensemble of models trained on the partial data /validation hold-out. This is statistically sound at least, but we're still training weaker models, which is a problem in cases where I suspect that a larger model with more data might be better. It's also VERY expensive computationally, when building CNNs. 

One could also try option 1, but keep a small (e.g. ""1%"") validation set as a ""sanity check"", although this is likely to be biased, and somewhat misses the point. 
 
Any other options? Or am I missing something in how the validation sets are constructed and sampled in training Nets?",2,0
196,2015-6-10,2015,6,10,18,39a05v,Dimensionality reduction at the command line with PCA and t-SNE from Tapkee,https://www.reddit.com/r/MachineLearning/comments/39a05v/dimensionality_reduction_at_the_command_line_with/,cast42,1433928129,,3,0
197,2015-6-10,2015,6,10,19,39a433,[1506.02025] Spatial Transformer Networks,https://www.reddit.com/r/MachineLearning/comments/39a433/150602025_spatial_transformer_networks/,aloisg,1433931051,,8,12
198,2015-6-10,2015,6,10,20,39a8qf,Why does Latent Dirichlet Allocation assume that topics are distributed like a Dirichlet?,https://www.reddit.com/r/MachineLearning/comments/39a8qf/why_does_latent_dirichlet_allocation_assume_that/,cavalierejody,1433934414,"Why do not choose a Gaussian or other distribution?
Thanks",5,2
199,2015-6-10,2015,6,10,20,39ad2c,Free intermediate course in R - Datacamp,https://www.reddit.com/r/MachineLearning/comments/39ad2c/free_intermediate_course_in_r_datacamp/,clbam8,1433937280,,0,0
200,2015-6-10,2015,6,10,21,39ai8q,Chainer: A flexible framework of neural networks for deep learning,https://www.reddit.com/r/MachineLearning/comments/39ai8q/chainer_a_flexible_framework_of_neural_networks/,chaika_user,1433940386,,4,5
201,2015-6-10,2015,6,10,23,39at0y,Cool data mining software!,https://www.reddit.com/r/MachineLearning/comments/39at0y/cool_data_mining_software/,datapablo,1433945765,,0,0
202,2015-6-10,2015,6,10,23,39av5e,"Generating Magic cards using deep, recursive neural networks",https://www.reddit.com/r/MachineLearning/comments/39av5e/generating_magic_cards_using_deep_recursive/,[deleted],1433946712,,11,54
203,2015-6-10,2015,6,10,23,39ax98,Converting continuous data to discrete data for multi-class classification.,https://www.reddit.com/r/MachineLearning/comments/39ax98/converting_continuous_data_to_discrete_data_for/,ThrowawayTartan,1433947625,"Hey everyone!

So I've got a question that isn't directly ML but it's kind of interesting. I've got a set of continuous data associated with movement(e.g running, crawling and so forth) where it's measured in terms of time, and angle of flex.

The way I see it, multi-class classification wouldn't be able to classify things accurately because the data isn't discrete. So, is there a way to convert the continuous data to a discrete form without losing data? I've tried Googling but couldn't find anything that seemed satisfying (the example they were handling was really simple, or there were no replies) Or is there another algorithm/ ML-framework that I should be employing? ",12,0
204,2015-6-11,2015,6,11,1,39bd6y,Call for ML Writers,https://www.reddit.com/r/MachineLearning/comments/39bd6y/call_for_ml_writers/,tfimg24,1433954490,"Hey all,

I'm an editor over at Terminal.com and I posted here a few days ago calling for questions about ML for our writers to field. As I expected we got some thought-provoking responses -- this is a really great community.

I wanted to return and mention that we are always actively seeking out strong, opinionated writers from the ML community (among others). Our founders have a deep background in ML, and it is a large part of the vision for the platform. Articles don't have to be strictly about Terminal (though we certainly like those). As an example, here's one of our more popular posts from the last few weeks.

http://blog.terminal.com/recurrent-neural-networks-deep-net-optimization-lstm/

If you're interested in writing for our blog please drop me a line here or via direct message with a little bit about your background, what you're working on in the field, and a link to some of your writing and I'd be happy to look it over.",0,0
205,2015-6-11,2015,6,11,2,39bo7k,Can softmax be used with cross entropy?,https://www.reddit.com/r/MachineLearning/comments/39bo7k/can_softmax_be_used_with_cross_entropy/,billconan,1433958890,"Hello guys,

sorry for this basic question. I'm following Stanford's machine learning for NLP. but the material for that course isn't easy to understand, so I'm also reading this online book:

http://neuralnetworksanddeeplearning.com/chap3.html

inside this book, it says that sigmoid can be used with cross-entropy. and softmax can be used with log-likelihood cost.

I therefore have the impression that they need to appear in pairs. if the last activation is sigmoid, cross-entropy should be used. if the last activation is softmax, log-likelihood should be used. mixture of softmax with cross-entropy should be avoided, the reason is that you want the derivatives be simple.


But at few different places, the standord material seems to suggest to use softmax with cross-entropy. I just want to confirm that they can be mixed?",4,0
206,2015-6-11,2015,6,11,3,39bqbe,Mahout's SGD Logistic Regression -- can we get the output of scoring to calibrate to probability(event = 1) ???,https://www.reddit.com/r/MachineLearning/comments/39bqbe/mahouts_sgd_logistic_regression_can_we_get_the/,wil_dogg,1433959703,"Hi

We are embedding a simple logistic regression step into our machine learning system so that the output of our Java-based genetic algorithm (GA) can be automatically calibrated to the scale of p(event = 1) where the p_event score will never be outside of the range of (0,1) and will be well calibrated within that range.  The current output of the GA is an interval level vector of scores that rank orders very well and has a nice normal-ish distribution, but the scale is arbitrary and meaningless.  Transforming that to P(event) scale is needed in order to use the score in downstream BI and decision sciences.

Currently we calibrate using SAS PROC LOGISTIC and the usual transformation of the resulting log odds to p_event, but I want to eliminate the I/O step that involves scoring the raw data file, running PROC LOGISTIC, saving the output of that regression, and then writing the equation to get the log odds, and then transforming log odds to probability of event, and then finally saving the data set at the end of that last data step. 

The issue we are running into is that we have implemented Mahout's SGD classifier, which yields final scaling that ranges from 0 to 1, but the calibration is off.  Sometimes it is off by just a little, sometimes it is very far off.  Here's an example of where it is far off:

http://i.imgur.com/eYBF3eK.jpg

DEPVAR_ARTHRITIS is the dependent variable, predicted probability is what we get when we take the score generated by the GA and run it through logistic regression, derive the log odds, and transform to p_arthritis, and MLRP is what we get from implementing Mahout's SGD logistic regression as described on this web page:

http://blog.trifork.com/2014/02/04/an-introduction-to-mahouts-logistic-regression-sgd-classifier/

As you can see the Mahout's value is scaled correctly between 0 and 1 across the entire range of observed scores, but it is not calibrated as well as what we get when we use simple logistic regression in SAS or SPSS.  In fact, the calibration sucks.  The rate of arthritis diagnosis is .62, but the average p_arthritis score coming out of the system is .66.  Ugh.

However, the results vary from case to case.  In this second example, the average value of the output of Mahout's procedure is equal to the actual event rate to the 3rd decimal place, but the calibration is off at both tails of the distribution, relative to the calibration achieved by running the score the GA creates through PROC LOGISTIC and then scaling the log odds to P_OUTCOME.

http://i.imgur.com/SGo9vnC.jpg

We are not beholden to Mahout's SGD method of logistic regression, we just need something that works in Java, that can take the output of what the GA has created that is on an arbitrary numeric scale, and translate that to p_event in a manner that is well calibrated and meaningful / easy to interpret / works every time.  The faster it operates the better.

Any assistance / guidance would be appreciated, am posting here first to give redditors first dibs on a solution but also will follow up with Mahout experts at the various websites if we come up dry here.",0,0
207,2015-6-11,2015,6,11,4,39c1tg,Does anybody know what kind of data the Google Prediction API?,https://www.reddit.com/r/MachineLearning/comments/39c1tg/does_anybody_know_what_kind_of_data_the_google/,[deleted],1433964099,"I working on a job application right now that's asking me to build an example predictive model use the Google Prediction API. My idea was to build a regression model for predicting box office performance for ~20 movies given 1000 tweets for each movie. I'm not sure whats going on under the hood there/if it's gonna do sentiment analysis and regression for me.

Thanks in advance, sorry if this was the wrong place to post this but every other likely sub seemed dead in regards to these kinds of questions.",0,1
208,2015-6-11,2015,6,11,4,39c7jk,Videos of a complete course on Deep Learning by Nando de Freitas (University of Oxford) - 2015,https://www.reddit.com/r/MachineLearning/comments/39c7jk/videos_of_a_complete_course_on_deep_learning_by/,ojaved,1433966290,,7,107
209,2015-6-11,2015,6,11,5,39cb8b,Is it practical to use a trained neural net on novel data in another experiment?,https://www.reddit.com/r/MachineLearning/comments/39cb8b/is_it_practical_to_use_a_trained_neural_net_on/,about3fitty,1433967639,"Trying to understand the boundaries of the transferability of neural nets.

Assuming that the training phase of a neural network takes time and resources, it would be nice to shorten or do away with this step altogether if possible.

What is a situation where you can take a net previously trained by another researcher and use it on novel data? How similar does the data have to be to yield reliable results? Is it more of a gradient that improves with each iteration?

I don't see a lot of researchers taking other nets as a starting point to try and shorten the number of iterations it takes for them to get their neural nets up to speed and I'm wondering why this technique isn't used.

Thanks!",1,1
210,2015-6-11,2015,6,11,5,39cfoe,Kaggle videotutorials for beginners,https://www.reddit.com/r/MachineLearning/comments/39cfoe/kaggle_videotutorials_for_beginners/,[deleted],1433969261,,0,3
211,2015-6-11,2015,6,11,5,39cg10,Cross selling + Classifier modeling strategy?,https://www.reddit.com/r/MachineLearning/comments/39cg10/cross_selling_classifier_modeling_strategy/,rsdenijs,1433969388,"(I repost this from the Data Science stackexchange, which does not seem to have much activity yet)
http://datascience.stackexchange.com/questions/6075/cross-selling-classifier-modeling-strategy

Product A or B can be offered to a customer in a cross-selling setting. Through A/B testing, data was collected about the customers that buy A and those that buy B.

Now there are different ways to model this. We could have two models that model P(buysA; X) and P(buysB; X) and offer the product that we believe has the highest probability of being bought. The problem with this is that it requires well calibrated probabilities.

Another possibility is to use a model that directly chooses between showing A or B. The problem with this case is that there is no direct data where the customer HAD to choose between both, either they were presented with one OR the other.

What would be the sane approach to model this?",0,0
212,2015-6-11,2015,6,11,5,39ch71,word-rnn modification of Andrej Karpathy's char-rnn code (handles UTF8 encoded text input),https://www.reddit.com/r/MachineLearning/comments/39ch71/wordrnn_modification_of_andrej_karpathys_charrnn/,namp243,1433969807,,12,6
213,2015-6-11,2015,6,11,6,39chs8,Bayesian Penalty/Boost Function,https://www.reddit.com/r/MachineLearning/comments/39chs8/bayesian_penaltyboost_function/,whatcausespolitics,1433970029,,0,1
214,2015-6-11,2015,6,11,6,39cqmj,How Airbnb Uses Big Data And Machine Learning To Guide Hosts To The Perfect Price,https://www.reddit.com/r/MachineLearning/comments/39cqmj/how_airbnb_uses_big_data_and_machine_learning_to/,squarerootof-1,1433973402,,0,0
215,2015-6-11,2015,6,11,7,39cs8d,"In implementing sentiment analysis, how do I distinguish between inputs / sequences when training an RNN?",https://www.reddit.com/r/MachineLearning/comments/39cs8d/in_implementing_sentiment_analysis_how_do_i/,[deleted],1433974002,"I'm trying to implement sentiment analysis with an LSTM and when I train it with various inputs and corresponding outputs, the inputs are all associated with the last input's output as if it is all a sequence. Which make sense, it is because of the recurrency.

I came up with a way to turn off the recurrent connections and it works fine, but then it is essentially a single-hidden-layer feed-forward network. There the classification works fine.

I have built a lstm. My plan is to split up the sentence using nlp techniques and then activating separate networks with the adjectives and the emotions. Is there a better way I can do this?

How can I implement sentiment-analysis that can train with a sentence or a paragraph like a review with an rnn?

I'd greatly appreciate any help.",3,0
216,2015-6-11,2015,6,11,9,39df08,Open source implementation for Facebook's Memory Network?,https://www.reddit.com/r/MachineLearning/comments/39df08/open_source_implementation_for_facebooks_memory/,[deleted],1433983323,"Hi guys,

Was wondering if anyone knows of an implementation of Facebook's Memory Network using torch or any one of the popular deep learning frameworks out there.",0,1
217,2015-6-11,2015,6,11,9,39dgea,[1506.03340] Teaching Machines to Read and Comprehend,https://www.reddit.com/r/MachineLearning/comments/39dgea/150603340_teaching_machines_to_read_and_comprehend/,egrefen,1433983867,,7,19
218,2015-6-11,2015,6,11,10,39dnxe,Neural Network Implementation Error?,https://www.reddit.com/r/MachineLearning/comments/39dnxe/neural_network_implementation_error/,Jt98,1433986831,"I've written a toy neural network in Java. I ran it several million times with the same outputs with only the randomized weights changing from run to run. The average of all of the outputs is not 0.5, as I would have expected. The code is in this Github Repository: [https://github.com/jack-t/petulant-octo-bear](https://github.com/jack-t/petulant-octo-bear).

As it stands now, the class `Bootstrap` will run a 3-layer network with a fixed arrangement and set of inputs.

I'd appreciate any comments on the structure of the program, and on the correctness (or incorrectness) of the output.  

Jack",3,0
219,2015-6-11,2015,6,11,11,39dwul,"DARPA Offshoot's $13K GPU Box That Understands, Synthesizes Conversations",https://www.reddit.com/r/MachineLearning/comments/39dwul/darpa_offshoots_13k_gpu_box_that_understands/,[deleted],1433990082,,0,0
220,2015-6-11,2015,6,11,15,39et9e,"""Many state-of-the-art supervised application now potentially can replace conventional classifiers with conceptor network""",https://www.reddit.com/r/MachineLearning/comments/39et9e/many_stateoftheart_supervised_application_now/,downtownslim,1434004356,,17,7
221,2015-6-11,2015,6,11,16,39f0bh,Model-Based Machine Learning,https://www.reddit.com/r/MachineLearning/comments/39f0bh/modelbased_machine_learning/,alexcasalboni,1434008560,,0,11
222,2015-6-11,2015,6,11,19,39fgsg,Deep Learning Lecture 1: Introduction,https://www.reddit.com/r/MachineLearning/comments/39fgsg/deep_learning_lecture_1_introduction/,aranag,1434020243,,0,7
223,2015-6-11,2015,6,11,20,39flbp,Inject Moulding Machines Exporters,https://www.reddit.com/r/MachineLearning/comments/39flbp/inject_moulding_machines_exporters/,Md-Malik,1434023190,,0,1
224,2015-6-11,2015,6,11,20,39fmcj,Ice Cream Machines Exporters,https://www.reddit.com/r/MachineLearning/comments/39fmcj/ice_cream_machines_exporters/,Md-Malik,1434023806,,0,1
225,2015-6-11,2015,6,11,21,39foqm,Soap Making Machines Exporters,https://www.reddit.com/r/MachineLearning/comments/39foqm/soap_making_machines_exporters/,Md-Malik,1434025126,,0,1
226,2015-6-11,2015,6,11,22,39fxn7,Anyone have any experience with the TETRAD project?,https://www.reddit.com/r/MachineLearning/comments/39fxn7/anyone_have_any_experience_with_the_tetrad_project/,bubbachuck,1434029527,"I'm fairly new to ML and have mainly been using WEKA.  TETRAD's interface looks promising...does anyone have any experience with it?  On the site, it notes that ""Tetrad is limited to models of categorical data (which can also be used for ordinal data) and to linear models (""structural equation models') with a Normal probability distribution, and to a very limited class of time series models.""  Not being able to handle numerical data seems like a big minus.

http://www.phil.cmu.edu/tetrad/",4,2
227,2015-6-11,2015,6,11,23,39g3bk,SVD left singular vectors interpretation,https://www.reddit.com/r/MachineLearning/comments/39g3bk/svd_left_singular_vectors_interpretation/,mmahesh,1434032070,,0,1
228,2015-6-11,2015,6,11,23,39g3hn,"Hey Reddit, I made a 4(+2) part tutorial on how to implement a Neural Network in Python. Let me know what you think.",https://www.reddit.com/r/MachineLearning/comments/39g3hn/hey_reddit_i_made_a_42_part_tutorial_on_how_to/,Xochipilli,1434032134,,33,177
229,2015-6-11,2015,6,11,23,39g564,15 Easy Solutions To Your Data Frame Problems In R,https://www.reddit.com/r/MachineLearning/comments/39g564/15_easy_solutions_to_your_data_frame_problems_in_r/,martijnT,1434032849,,0,0
230,2015-6-11,2015,6,11,23,39g6j5,Next Breakthrough in Deep Learning: Stronger Priors and Better Ways of Modelling Invariances,https://www.reddit.com/r/MachineLearning/comments/39g6j5/next_breakthrough_in_deep_learning_stronger/,fhuszar,1434033420,,11,9
231,2015-6-11,2015,6,11,23,39g8k3,Output standardization with dynamically changing training set?,https://www.reddit.com/r/MachineLearning/comments/39g8k3/output_standardization_with_dynamically_changing/,RossoFiorentino,1434034246,"Dear all,

I am working on a project where i use a Deep Neural Network with ReLU activations as a regressor. The content of the training set is updated over time. Because the outputs can differ significantly in magnitude i would like to standardize them. Due to the dynamic changing of the training set i cannot use any standard methods for standardization. Does anyone know of any good solutions/literature for solving this problem?",1,0
232,2015-6-12,2015,6,12,0,39gdc7,Why google is investing in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/39gdc7/why_google_is_investing_in_deep_learning/,aadeshnpn,1434036008,,0,0
233,2015-6-12,2015,6,12,1,39gq9q,How to Improve Your Predictive Model,https://www.reddit.com/r/MachineLearning/comments/39gq9q/how_to_improve_your_predictive_model/,atakante,1434040635,,0,1
234,2015-6-12,2015,6,12,2,39gx2y,NBA (basketball finals) &amp; spatio-temporal pattern recognition: the science of moving dots,https://www.reddit.com/r/MachineLearning/comments/39gx2y/nba_basketball_finals_spatiotemporal_pattern/,gradientflow,1434043039,,1,2
235,2015-6-12,2015,6,12,4,39hkzp,IHTAI: an Unsupervised Online Machine Learning Library in JavaScript,https://www.reddit.com/r/MachineLearning/comments/39hkzp/ihtai_an_unsupervised_online_machine_learning/,[deleted],1434051456,,0,0
236,2015-6-12,2015,6,12,5,39hvbp,Question about pandas with sci-kit learn...,https://www.reddit.com/r/MachineLearning/comments/39hvbp/question_about_pandas_with_scikit_learn/,validated1,1434055251,"If I have a DataFrame with one column of categorical data and another column with text description data, how can I vectorize column one with just one hot encoding and column two with just countvectorizer, and return an array I can then fit to a model?

Essentially, how do I preprocess each column individually and then concat them to a fittable array?

Thanks! ",5,0
237,2015-6-12,2015,6,12,7,39ieiv,Data pipeline at Oyster,https://www.reddit.com/r/MachineLearning/comments/39ieiv/data_pipeline_at_oyster/,lumengxi,1434062929,,0,0
238,2015-6-12,2015,6,12,9,39irmy,[x-post to SMT] A Haar cascade image classification training platform,https://www.reddit.com/r/MachineLearning/comments/39irmy/xpost_to_smt_a_haar_cascade_image_classification/,pwoolf,1434068699,"[Original submission in /r/SomebodyMakeThis](http://www.reddit.com/r/SomebodyMakeThis/comments/3963n7/smt_a_harr_cascade_image_classification_training/)

I keep seeing this problem come up where someone wants to identify a class of objects from a digital image.  The most common object is faces, and there are off the shelf classifiers (such as a haar cascade) for opencv.

The procedure for training a [haar cascade using open cv](http://note.sonots.com/SciSoftware/haartraining.html) is well defined, but labor intensive.  It involves:

* Collecting images that contain the object
* Annotating the images to draw boxes around the parts of the image that contain the object of interest.
* Annotating the images to draw boxes around the parts of the image that DO NOT contain the object of interest.
* Training the software (which can take days or weeks of computation time)
* Validating the model with test data that was not in the training set.

The image collection could be provided by the user/client.  Then this batch of images could be farmed out to, say, Mechanical Turks or volunteers for annotation.  

The annotators would receive a link that contains the picture and a simple browser based paint interface.  The annotator would then paint the regions that had the object, say with a solid red color.  This would be something like the [Peekaboom platform](https://www.cs.cmu.edu/~biglou/Peekaboom.pdf) but updated (peekaboom was written in 2006)

The annotation would then be sent back to the server to define regions where the image was present and not.  These image segments could then be put into the Haar cascade learning pipeline.

This would make a nice digital business actually.  The client would be responsible for providing the image set, some description of the object, and some positive controls (to train and validate the human annotators).  The company would then farm out the annotation, farm out the haar cascade learning, and then provide the resulting haar cascade as a product. 

The company could go one step further and offer to retain the haar cascade and apply it to batches of new images from the client.  This way the client never needs to use/implement any image recognition software, they just get what they want of object identification.

None of these steps is particularly hard, it is just a matter of organizing them into a pipeline and marketing the product.

Examples of things that people have wanted to identify include:

* defective parts in a manufacturing process
* misapplied labels
* spoiled pieces of fruit
* certain types of vehicles 
* certain types of cells under a microscope

If this pipeline were inexpensive enough it could be used ubiquitously for mundane tasks such as:
 
* creating an approximate inventory of my refrigerator or pantry from a digital image (i.e. identify milk, yogurt, apples, celery etc).
* Annotating people's pictures of their food (i.e. mashed potatoes, peas, carrots..)
* Helping me search my photos for my dog, cat, or pet iguana.
* Diagnosing an iron deficiency in my spider plant

Maybe this already exists, and if so let me know!  ",2,0
239,2015-6-12,2015,6,12,10,39j2vd,There was a company webpage that had a list of recommended books on how one could get started studying AI/ML. Some books even had recommended chapters. Anyone know what I'm talking about?,https://www.reddit.com/r/MachineLearning/comments/39j2vd/there_was_a_company_webpage_that_had_a_list_of/,Frozen_Turtle,1434074086,"I remember the web page had pictures of the books, and it was linked in a few months/weeks ago in some kind of AMA-esque dialog with a scientist or professor. The books escalated in difficulty - it was more like a curriculum than a list of favorite books. Also, I'm pretty sure it was on some private company's site, not a university page.

Perhaps this is too vague, but I hope someone else saw that comment and saved it.

Edit: Even if you don't know what my exact link, please feel free to post your favorite book or reading list!",6,1
240,2015-6-12,2015,6,12,11,39j7e3,"Early Stopping is Nonparametric Variational Inference - ""We can use this bound to optimize hyperparameters instead of using cross-validation. """,https://www.reddit.com/r/MachineLearning/comments/39j7e3/early_stopping_is_nonparametric_variational/,InaneMembrane,1434076269,,5,16
241,2015-6-12,2015,6,12,12,39jd7v,Baidu Fires Researcher Tied to Contest Disqualification,https://www.reddit.com/r/MachineLearning/comments/39jd7v/baidu_fires_researcher_tied_to_contest/,doomie,1434079173,,20,41
242,2015-6-12,2015,6,12,13,39jnuk,Basics of Machine Learning explained to a Newbie,https://www.reddit.com/r/MachineLearning/comments/39jnuk/basics_of_machine_learning_explained_to_a_newbie/,john_philip,1434084897,,0,2
243,2015-6-12,2015,6,12,14,39jpag,Play Music with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/39jpag/play_music_with_recurrent_neural_networks/,john_philip,1434085735,,2,11
244,2015-6-12,2015,6,12,14,39jr9y,Manufacturer of industrial Machinery,https://www.reddit.com/r/MachineLearning/comments/39jr9y/manufacturer_of_industrial_machinery/,nitaengineering,1434086906,,0,1
245,2015-6-12,2015,6,12,14,39jro4,[1506.03099] Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/39jro4/150603099_scheduled_sampling_for_sequence/,downtownslim,1434087164,,1,8
246,2015-6-12,2015,6,12,18,39kcug,time series datasets,https://www.reddit.com/r/MachineLearning/comments/39kcug/time_series_datasets/,langdatyagee,1434102046,What are timeseries datasets equivalent of mnist ? ,4,3
247,2015-6-12,2015,6,12,18,39kdkc,Classifying Economics articles using LDA,https://www.reddit.com/r/MachineLearning/comments/39kdkc/classifying_economics_articles_using_lda/,yomritoyj,1434102587,,0,0
248,2015-6-12,2015,6,12,21,39krvt,Blue Angels Program Brings High-Risk Obstetric Care to Rural Kentucky,https://www.reddit.com/r/MachineLearning/comments/39krvt/blue_angels_program_brings_highrisk_obstetric/,Solomonn-Bryant_,1434112191,,0,2
249,2015-6-12,2015,6,12,21,39ksk4,Generating Magic cards using neural networks,https://www.reddit.com/r/MachineLearning/comments/39ksk4/generating_magic_cards_using_neural_networks/,alexcasalboni,1434112581,,3,0
250,2015-6-12,2015,6,12,22,39kyex,Machine Learning is new Sport,https://www.reddit.com/r/MachineLearning/comments/39kyex/machine_learning_is_new_sport/,aadeshnpn,1434115714,,0,0
251,2015-6-12,2015,6,12,22,39kyw4,CRF implementation with numerical features,https://www.reddit.com/r/MachineLearning/comments/39kyw4/crf_implementation_with_numerical_features/,vdashv,1434115971,"Hello,
does someone know about a CRF implementation that can work with numerical features as inputs (preferably that can work on unixes) and does not encode them in strings/one-hot like most of implementations?
Thank you",2,2
252,2015-6-12,2015,6,12,22,39kzgv,Convolutions on a multi-level sequence?,https://www.reddit.com/r/MachineLearning/comments/39kzgv/convolutions_on_a_multilevel_sequence/,ddofer,1434116289,"I'm working on a ML project where I have multiple levels of representations/features for each character and position in a sequence.  E.G : The layers of features could be i) ""Letter at each position (1 hot encoding)"", ii) ""Capital letter: 0/1"" , iii) ""How often does this letter occur in this context [0-1]"". 

The sequence will be of a fixed length each time, and it's a classification task. I wanted to try a CNN with this, (given poor performance of Random forests and an RBF-SVM), especially the idea of ""Average-Pooling""  and convolutional layers. I'm working in Python with existing packages (preferably Lasagne).

My question is whether I should use 2D convolutions in this case, rather than 1D.  i.e. 2D convolutions of shape (nX1). 
As an aside, does anyone have some SIMPLE, highly annotated examples of preparing data in a format that can then be read by a CNN? (Or RNN for that matter?) - I'm only used to working with data in the ""tidy"" vector space format, i.e a matrix, with rows being samples and columns being the features and labels. 

Thanks!
 ",8,2
253,2015-6-12,2015,6,12,23,39l8df,Prediction.io Classification Template Engine Step-by-step Guide,https://www.reddit.com/r/MachineLearning/comments/39l8df/predictionio_classification_template_engine/,kafechew,1434120410,,1,1
254,2015-6-13,2015,6,13,1,39lmdq,Man Vs. Machine: How Humans Are Driving The Next Age Of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/39lmdq/man_vs_machine_how_humans_are_driving_the_next/,vsr1237a,1434126268,,0,6
255,2015-6-13,2015,6,13,2,39lsp5,Kurzweil reveals what he is secretly working on at Google. Anyone else catch this?,https://www.reddit.com/r/MachineLearning/comments/39lsp5/kurzweil_reveals_what_he_is_secretly_working_on/,Akyu,1434128808,"So in a recent talk given at Google (previously available [here](https://www.youtube.com/watch?v=bf0I-Uhqd8I), but now the video is no where to be found, if someone finds a working link post in the comments), Kurzweil gives his usual talk about Law of accelerating returns, and the singularity.

But he also mentions the name of the project he has been working on at Google, Project Descartes. He doesn't go into much detail but he mentions that they are experimenting with Hierarchical LSTMs, and they are using them in ""dialogue agents"". Google search (and Bing, just to make sure) reveals nothing on Project Descartes, so they've definitely been keeping a lid on it.

As far as I know, this is the first public mention of what he has been up to at Google, so I figure it's of interest to /r/machinelearning. Any thoughts on Hierarchical LSTMs? Is anyone familiar with work done in this area previously?

EDIT: And just to further corroborate until a working copy of the video is found, [this thread](https://www.reddit.com/r/thisisthewayitwillbe/comments/39ffdu/talks_at_google_kurzweil_touches_on_numenta/) also mentions what I have talked about.",34,38
256,2015-6-13,2015,6,13,2,39lsto,Randomer Forests (x-post/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/39lsto/randomer_forests_xpostcompressivesensing/,compsens,1434128858,,2,10
257,2015-6-13,2015,6,13,3,39m2nw,[1506.03767] Spectral Representations for Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/39m2nw/150603767_spectral_representations_for/,dhammack,1434132730,,3,20
258,2015-6-13,2015,6,13,4,39mc4q,TED-RNN: Machine Generated TED-Talks,https://www.reddit.com/r/MachineLearning/comments/39mc4q/tedrnn_machine_generated_tedtalks/,samim23,1434136609,,5,6
259,2015-6-13,2015,6,13,5,39mk4r,"The AI That Learned Magic the Gathering. Reed Morgan Milewicz, a programmer and computer science researcher, may be the first person to teach an AI to do Magic, literally.",https://www.reddit.com/r/MachineLearning/comments/39mk4r/the_ai_that_learned_magic_the_gathering_reed/,jonbarnesgeology,1434139967,,8,0
260,2015-6-13,2015,6,13,5,39mnxe,"FaceGrab: Grab everyone's Facebook profile image without any login, and create an image dataset useful for ML, Image processing and more.",https://www.reddit.com/r/MachineLearning/comments/39mnxe/facegrab_grab_everyones_facebook_profile_image/,aaggarwall,1434141581,,6,7
261,2015-6-13,2015,6,13,5,39mor7,Are there any automated browsing tools that I could use to gather data for doing my own machine learning?,https://www.reddit.com/r/MachineLearning/comments/39mor7/are_there_any_automated_browsing_tools_that_i/,loveComputer,1434141938,"Would I get blocked from websites? It seems like there is no tool that is mainstream in this area, so I'm trying to figure out why",7,2
262,2015-6-13,2015,6,13,6,39mti0,Deep Learning beats Human in IQ Tests [MIT Technology Review],https://www.reddit.com/r/MachineLearning/comments/39mti0/deep_learning_beats_human_in_iq_tests_mit/,muktabh,1434144008,,3,0
263,2015-6-13,2015,6,13,6,39myrd,"G+ post from Pierre Sermanet about ""trend at CVPR 2015 of renaming existing deep learning techniques following minor modifications.""",https://www.reddit.com/r/MachineLearning/comments/39myrd/g_post_from_pierre_sermanet_about_trend_at_cvpr/,test3545,1434146331,,9,51
264,2015-6-13,2015,6,13,8,39nb08,Looking for someone to do a ML/coding mock interview practice.,https://www.reddit.com/r/MachineLearning/comments/39nb08/looking_for_someone_to_do_a_mlcoding_mock/,totolin,1434152046,"Hi fellow redditor,
    I have an applied math background and is recently transitioning into the area of machine learning. I would like to do some mock interview if possible. I had a phone interview with Amazon and if you need practice I can also ask you the questions they asked me as well. Please shoot me a PM if you want to do a mock interview with me. Thank you for your time.
",0,3
265,2015-6-13,2015,6,13,12,39o246,"Hugo Larochelle, iRBM, ""We've come up with an unsuspectingly simple way of having an RBM with adaptable hidden layer size, with no need for tuning the number of hidden units""",https://www.reddit.com/r/MachineLearning/comments/39o246/hugo_larochelle_irbm_weve_come_up_with_an/,test3545,1434166613,,7,69
266,2015-6-13,2015,6,13,12,39o2ie,"[VIDEO] Oriol Vinyals ""We adapted the attention mechanism of the sequence-to-sequence framework to make it possible for the model to output pointers to inputs""",https://www.reddit.com/r/MachineLearning/comments/39o2ie/video_oriol_vinyals_we_adapted_the_attention/,test3545,1434166842,,1,10
267,2015-6-13,2015,6,13,17,39oqpl,Recommendations using Mahout 0.10.1 and Hadoop 2.6,https://www.reddit.com/r/MachineLearning/comments/39oqpl/recommendations_using_mahout_0101_and_hadoop_26/,YahooGuys,1434183693,,0,0
268,2015-6-13,2015,6,13,22,39pdfu,Planarian regeneration model discovered by artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/39pdfu/planarian_regeneration_model_discovered_by/,graycube,1434201888,,1,6
269,2015-6-13,2015,6,13,22,39pg62,How to teach a classifier that certain features are related?,https://www.reddit.com/r/MachineLearning/comments/39pg62/how_to_teach_a_classifier_that_certain_features/,YourWelcomeOrMine,1434203751,"I've been using off-the-shelf machine learning classifiers like weka and sklearn for a couple of years, but am just now learning the theories behind them.

My data concerns keystrokes, and so features take the form of, e.g. ""Hold_Time_Shift_Key"" or ""Pause_Time_Shift_Key."" Is there a way to teach a classifier that these two features are different dimensions of the same event? Or is this something I'd even want to do? Is it better to just let the classifier learn, on its own, how features are related or not related?",5,1
270,2015-6-13,2015,6,13,23,39pgrr,is there something to learn in this problem?,https://www.reddit.com/r/MachineLearning/comments/39pgrr/is_there_something_to_learn_in_this_problem/,[deleted],1434204178,"thank you all, deleted to prevent ''my'' students to find my usernam :)",12,2
271,2015-6-14,2015,6,14,0,39po3h,Deep Learning Machine Beats Humans in IQ Test | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/39po3h/deep_learning_machine_beats_humans_in_iq_test_mit/,xamdam,1434208597,,18,36
272,2015-6-14,2015,6,14,0,39po8i,Machine Learning Algorithms and Modularity,https://www.reddit.com/r/MachineLearning/comments/39po8i/machine_learning_algorithms_and_modularity/,ssushant,1434208678,,0,0
273,2015-6-14,2015,6,14,0,39pqgw,Ex-Baidu Researcher Ren Wu Denies Wrongdoing (x-post /thisisthewayitwillbe),https://www.reddit.com/r/MachineLearning/comments/39pqgw/exbaidu_researcher_ren_wu_denies_wrongdoing_xpost/,vikkamath,1434209959,,50,21
274,2015-6-14,2015,6,14,1,39pulb,Biologically realistic NNs?,https://www.reddit.com/r/MachineLearning/comments/39pulb/biologically_realistic_nns/,asymptotics,1434212179,"I've read about a few neuroscience initiatives to simulate neurons that try to realistically model aspects of neurons that are absent from ANNs (such as neurotransmitters).

Have there been attempts to use these more realistic neurons for vision/NLP-type tasks? Or if they're still to expensive to train, is there any theory that shows that a certain biologically-plausible architecture *should* be more accurate than ANNs once the technology makes it feasible? ",48,9
275,2015-6-14,2015,6,14,1,39pypu,NAACL 2015 accepted papers,https://www.reddit.com/r/MachineLearning/comments/39pypu/naacl_2015_accepted_papers/,muktabh,1434214369,,1,9
276,2015-6-14,2015,6,14,2,39q1t4,Non linear change of pdf,https://www.reddit.com/r/MachineLearning/comments/39q1t4/non_linear_change_of_pdf/,bluedunnock,1434215983,,11,0
277,2015-6-14,2015,6,14,5,39qk6h,Machine learning used to play Super Mario World.,https://www.reddit.com/r/MachineLearning/comments/39qk6h/machine_learning_used_to_play_super_mario_world/,asrianCron,1434225603,,116,239
278,2015-6-14,2015,6,14,6,39qvao,Music Generation with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/39qvao/music_generation_with_recurrent_neural_networks/,Steve_the_Scout,1434231588,,9,10
279,2015-6-14,2015,6,14,13,39rww0,Machine Learning and the Law,https://www.reddit.com/r/MachineLearning/comments/39rww0/machine_learning_and_the_law/,citruwasabi,1434254646,Can Machine Learning help with jurisprudence?,5,2
280,2015-6-14,2015,6,14,16,39scvl,"What's wrong with deep learning, a talk by yann lecun at CVPR 2015",https://www.reddit.com/r/MachineLearning/comments/39scvl/whats_wrong_with_deep_learning_a_talk_by_yann/,evc123,1434268504,,7,43
281,2015-6-14,2015,6,14,19,39slr8,Suggestions for restaurants recommendation system,https://www.reddit.com/r/MachineLearning/comments/39slr8/suggestions_for_restaurants_recommendation_system/,midododo,1434278142,"I am trying to implement a restaurant recommendation system for users based on types of dishes they like and types of restaurants, I read a couple of papers regarding different types of recommendation systems including content based and collaborative filtering and was thinking about implementing a KNN clustering algorithm, do you think it's a good idea or should I consider a different algorithm, also if you have any suggested readings that would help, that would be great",4,0
282,2015-6-14,2015,6,14,23,39t0kp,AskML: How do you manage expectations in applied work?,https://www.reddit.com/r/MachineLearning/comments/39t0kp/askml_how_do_you_manage_expectations_in_applied/,srkiboy83,1434290897,"With all the hoopla around Data Science, Machine Learning, and all the success stories around, there are a lot of both justified, as well as overinflated, expectations from Data Scientists and their predictive models.

My question to practicing Statisticians, Machine Learning experts, and Data Scientists of Reddit is - how do you manage expectations from the businesspeople in you company, particularly with regards to predictive accuracy of models? To put it trivially, if your best model can only achieve 90% accuracy, and upper management expects nothing less than 99%, how do you handle situations like these?
",10,31
283,2015-6-15,2015,6,15,2,39tjp5,Is transfer of consciousness possible in the near future?,https://www.reddit.com/r/MachineLearning/comments/39tjp5/is_transfer_of_consciousness_possible_in_the_near/,[deleted],1434301984,,1,0
284,2015-6-15,2015,6,15,4,39tyhu,"Kaggle | Otto Product Classification Winners Interview: 2nd place, Alexander Guschin",https://www.reddit.com/r/MachineLearning/comments/39tyhu/kaggle_otto_product_classification_winners/,clbam8,1434309467,,4,28
285,2015-6-15,2015,6,15,5,39u7x6,Any Open Source Implementations of Neural Turing Machines or Memory Networks?,https://www.reddit.com/r/MachineLearning/comments/39u7x6/any_open_source_implementations_of_neural_turing/,simonhughes22,1434314195,I would like to play around with memory networks (Jason Weston) or neural turing machines (similar concept) for a difficult classification task I am working on. Does anyone know of any good open source implementations out there?,10,6
286,2015-6-15,2015,6,15,7,39ujo2,Reinforcement learning in LUA,https://www.reddit.com/r/MachineLearning/comments/39ujo2/reinforcement_learning_in_lua/,Aerospacio,1434320143,"Hello,

I'm a Msc student in machine learning and i wonder if anybody knows a library for Reinforcement learning/neural networks written in lua, i searched but couldnt find any.

Thanks for reading!",12,1
287,2015-6-15,2015,6,15,9,39v0r2,Interview Questions for Data Scientist Positions,https://www.reddit.com/r/MachineLearning/comments/39v0r2/interview_questions_for_data_scientist_positions/,D33B,1434329593,,36,86
288,2015-6-15,2015,6,15,15,39w0dg,ICLR 2015 Poster Highlights,https://www.reddit.com/r/MachineLearning/comments/39w0dg/iclr_2015_poster_highlights/,glassackwards,1434350413,,1,8
289,2015-6-15,2015,6,15,15,39w0do,Kaggle Ensembling Guide,https://www.reddit.com/r/MachineLearning/comments/39w0do/kaggle_ensembling_guide/,john_philip,1434350418,,1,30
290,2015-6-15,2015,6,15,17,39w72t,Generative Image Modeling Using Spatial LSTMs,https://www.reddit.com/r/MachineLearning/comments/39w72t/generative_image_modeling_using_spatial_lstms/,iori42,1434356101,,0,22
291,2015-6-15,2015,6,15,17,39w8n5,Features for Naive Bayes Classifier,https://www.reddit.com/r/MachineLearning/comments/39w8n5/features_for_naive_bayes_classifier/,tatamae,1434357585,"I have trained a multinomial naive bayes classifier on a bunch of documents with word counts as features. When I predict a new document, do the word counts matter or is merely a boolean representation of the features enough (since the likelihood of features is already learned)? 

Also, how does it affect things if I change the word counts to tf-idf values? Theoretically, multinomial naive bayes should be used for discrete features only but I feel intuitively tf-idf should also work (maybe even give better results). ",7,4
292,2015-6-15,2015,6,15,18,39wd1u,Your computer learning (evolving) how to make a 2D car: BoxCar2D,https://www.reddit.com/r/MachineLearning/comments/39wd1u/your_computer_learning_evolving_how_to_make_a_2d/,RRopemaker,1434361763,,0,4
293,2015-6-15,2015,6,15,19,39wdug,Machine Learning for Computer Vision - Lecture 11 - Clustering,https://www.reddit.com/r/MachineLearning/comments/39wdug/machine_learning_for_computer_vision_lecture_11/,aranag,1434362518,,2,1
294,2015-6-15,2015,6,15,20,39wkhm,Yann LeCun's answer to: Why are very few schools involved in deep learning research? Why are they still hooked on to Bayesian methods?,https://www.reddit.com/r/MachineLearning/comments/39wkhm/yann_lecuns_answer_to_why_are_very_few_schools/,clbam8,1434368134,,17,50
295,2015-6-15,2015,6,15,20,39wld2,games are breaking the brains.,https://www.reddit.com/r/MachineLearning/comments/39wld2/games_are_breaking_the_brains/,omkanth,1434368814,,1,1
296,2015-6-15,2015,6,15,22,39wsis,Sentiment Analysis &amp; Predicting Scores with Rapidminer,https://www.reddit.com/r/MachineLearning/comments/39wsis/sentiment_analysis_predicting_scores_with/,Nick595,1434373691,"Hi all,

For my MSc Thesis I am delving deeper in predition analysis. I know that Rapidminer has an accelerator for Sentiment Analysis, but I am wondering how they create the scores that are given as a result. 
If I understand well, a SVM is used on the dataset that you enter. However, the result of a SVM was to create a hyperplane that seperates points +1 from -1 right? So how are the continuous variables in the accelerator created then, and are they actually valid?",0,3
297,2015-6-16,2015,6,16,2,39xrg6,ROC Curves in Python and R,https://www.reddit.com/r/MachineLearning/comments/39xrg6/roc_curves_in_python_and_r/,theglamp,1434390055,,0,3
298,2015-6-16,2015,6,16,2,39xrzn,Rana el Kaliouby: This app knows how you feel -- from the look on your face | TED Talk,https://www.reddit.com/r/MachineLearning/comments/39xrzn/rana_el_kaliouby_this_app_knows_how_you_feel_from/,ahamino,1434390274,,0,0
299,2015-6-16,2015,6,16,2,39xsml,Practical MOOC or Tutorial for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/39xsml/practical_mooc_or_tutorial_for_neural_networks/,mbaroi,1434390523,"Hey Guys, 

I'm a self taught programmer with a background in Java and know some R and Python. I've been interested in ML and want to build a Neural Network for a project. I understand that the mathematical and statistical parts are important in this field, however, tutorials/MOOCs like Dr. Ng's, Heaton's, etc haven't helped me build a neural network. 

Are there any practical tutorials or books that walk me through building an neural network from scratch? The concepts seem very abstract for me without anything practical to build. I still have no idea how to code a neural network. 

My apologies if this is a stupid question, but I am genuinely interested and want to learn more. 

P.S. I am aware the wiki and went through them. I've downloaded the PDF version's of the statistics free version books. 

**Update:** I also found this recently and I'm going through it... looks interesting and practical. It's a series and he walks you through the whole process: https://www.youtube.com/watch?v=k890Dr5OkZg&amp;list=PLRJx8WOUx5XdosSIpI34ijGVAxCSG_jjT

Thanks.",6,5
300,2015-6-16,2015,6,16,3,39xw2q,Are there any distributed computing platforms to efficiently train models?,https://www.reddit.com/r/MachineLearning/comments/39xw2q/are_there_any_distributed_computing_platforms_to/,changingourworld,1434391965,"[Distributed computing](https://en.wikipedia.org/wiki/List_of_distributed_computing_projects) allows people to donate their unused computing time. If people in this sub aren't training models 24/7 they could allow others to use some of their compute time. I could also see many people in /r/Futurology who are interested in AI, being willing to donate compute time.

Are there any distributed computing platforms that currently exist? How difficult is it to create one?",2,2
301,2015-6-16,2015,6,16,3,39xzza,.lua to .py,https://www.reddit.com/r/MachineLearning/comments/39xzza/lua_to_py/,tushar1408,1434393579,"I am working(updating) on a project which is written in lua programming language but i am more comfortable with python.
any suggestions?  ",11,0
302,2015-6-16,2015,6,16,4,39y8h9,What is that bird? Birds identification using computer vision,https://www.reddit.com/r/MachineLearning/comments/39y8h9/what_is_that_bird_birds_identification_using/,booketor,1434397152,,7,7
303,2015-6-16,2015,6,16,4,39y8v5,Best Masters in Data Science for Continuing Learners / Those Working and Pursuing Degree,https://www.reddit.com/r/MachineLearning/comments/39y8v5/best_masters_in_data_science_for_continuing/,merrillcook1,1434397316,,0,1
304,2015-6-16,2015,6,16,5,39ybgh,Open-source projects and community for promotion of Data Science and Machine Learning in Information Security,https://www.reddit.com/r/MachineLearning/comments/39ybgh/opensource_projects_and_community_for_promotion/,galapag0,1434398455,,0,1
305,2015-6-16,2015,6,16,5,39yj8y,Why isn't code submission mandatory for top ML conferences?,https://www.reddit.com/r/MachineLearning/comments/39yj8y/why_isnt_code_submission_mandatory_for_top_ml/,elderprice1234,1434401628,"Science has to be reproducible. Thankfully, within ML, the cost of reproducing is relatively low, if the code accompanying the paper is released. More and more authors are volunteering to open source their code, but I was wondering why, given the low cost of compliance, this is not enforced as part of a publication?

(if one wants to keep one's implementation private, one could still submit the code to the conference but just not open source it)

I ask this because I was trying to implement http://cs.stanford.edu/~quocle/paragraph_vector.pdf
but it turns out the results are not reproducible.",53,103
306,2015-6-16,2015,6,16,6,39yqr2,Encoder-decoder to generate sentence vectors?,https://www.reddit.com/r/MachineLearning/comments/39yqr2/encoderdecoder_to_generate_sentence_vectors/,spurious_recollectio,1434404809,"I'm wondering if anyone has any literature or experience in using encoder-decoder RNNs or LSTMs to generate sentence/phrase vectors to use for e.g. clustering or search.  The idea would be have an RNN auto-encode a phrase by requiring it to encode then decode the same phrase and to use the resulting encoding vector (i.e. the final hidden state of the encoder) as a ""phrase vector"".  I've tried doing this on a smallish corpus but the resulting vectors don't seem to cluster particularly well.  Ideally one would want the encoding to live in some semantic space so that nearby vectors correspond to phrases with similar meanings, not just words.  It might well be that the task of simply encoding then decoding a phrase does not really require the network to learn any meaning so the resulting vectors might just be some compressed bag of words.  Another issue I worry about is that the hidden states of the network are not really a linear representation (in the sense of word2vec vectors that can be meaningfully added or subtracted).  I would appreciate any thoughts on or references for this.",8,4
307,2015-6-16,2015,6,16,7,39yt4v,Healthcare Claims/Electronic Medical Records Machine Learning Papers Suggestions,https://www.reddit.com/r/MachineLearning/comments/39yt4v/healthcare_claimselectronic_medical_records/,brentjoseph,1434405850,"Hi,

I was wondering if anyone knew of any good applied papers to suggest for a journal club with a theme of ""machine learning in medicine and health policy"". 

Specifically we would be interested in papers using healthcare claims or electronic medical record data.

Thanks!",4,0
308,2015-6-16,2015,6,16,7,39yvqy,Understanding Maximum Likelihood Error Function for RNN Encoder-Decoder,https://www.reddit.com/r/MachineLearning/comments/39yvqy/understanding_maximum_likelihood_error_function/,[deleted],1434407018,"I am having trouble understanding exactly how the RNN Encoder-Decoder used in [1,2] is trained. The authors give qualitative descriptions of the model but don't explicitly give the error function. [1] states that the model is trained to maximize the log probability of the correct output sequence, that is the sum of the logs of the probabilities. But how is the probability of an output sequence given by the model? Does the whole softmax (probability over vocabulary) outputted at each timestep become input to the next timestep? It seems like that must be the case, because in training mode, it doesn't seem like we can sample from the softmax- that would not be differentiable. 

[1] - [Sutskever et al.](http://arxiv.org/abs/1409.3215)
[2] - [Cho et al.](http://arxiv.org/pdf/1406.1078v3.pdf).
",3,1
309,2015-6-16,2015,6,16,9,39zbv5,Jesus Markoving Christ. Markov-Chain generated Bible quotes,https://www.reddit.com/r/MachineLearning/comments/39zbv5/jesus_markoving_christ_markovchain_generated/,gameMeBot,1434414725,,36,45
310,2015-6-16,2015,6,16,9,39zej6,Would this be an example of catastrophic failure similar to that found in neural networks? [Interesting video on how to relearn cycling],https://www.reddit.com/r/MachineLearning/comments/39zej6/would_this_be_an_example_of_catastrophic_failure/,ralisevervant,1434416090,,4,20
311,2015-6-16,2015,6,16,10,39zfm6,Go go gadget human char-rnn,https://www.reddit.com/r/MachineLearning/comments/39zfm6/go_go_gadget_human_charrnn/,[deleted],1434416623,S,0,0
312,2015-6-16,2015,6,16,10,39zlfg,Robot with a Rat's Brain,https://www.reddit.com/r/MachineLearning/comments/39zlfg/robot_with_a_rats_brain/,UncoChen,1434419562,,23,27
313,2015-6-16,2015,6,16,13,3a02qm,"Hi r/machinelearning, we organized the first data science camp for kids. We need your opinion!",https://www.reddit.com/r/MachineLearning/comments/3a02qm/hi_rmachinelearning_we_organized_the_first_data/,harshnisar,1434428465,"We thought we would experiment with teaching grade 6th-9th kids some data science! We think it is important to introduce students to think in a data-driven way.
 
Data science is more than a hype or a career choice; it's a way of thinking and approaching problems that is structured and organized and which everyone can inculcate in their day to day lives.
 
Last Saturday we organized the first data science camp for kids. It wasn't easy - a lot of thought went into designing the data set and a hands-on exercise.
 
We've created a **massive** blog full of mentor experiences, organising experiences, children feedback (really amusing and cute), photos and most **importantly** a guide for you to organize your own data science camp for kids!
 
We aren't profiting from it in anyway. The camp tuition was free. We've even put our material online so that people can replicate this.

Here is the link to the blog which lists everything: http://www.datasciencekids.org

We really want to know what criticism r/machinelearning has to offer :)",1,1
314,2015-6-16,2015,6,16,14,3a08se,"PredictionIO - an open source ML stack built on Spark, HBase, and Spray",https://www.reddit.com/r/MachineLearning/comments/3a08se/predictionio_an_open_source_ml_stack_built_on/,blowjobtransistor,1434432147,,0,1
315,2015-6-16,2015,6,16,16,3a0jsu,'Love is all you need!': The Beatles' Lyrics Analysis,https://www.reddit.com/r/MachineLearning/comments/3a0jsu/love_is_all_you_need_the_beatles_lyrics_analysis/,bjarnegosling,1434440143,,3,0
316,2015-6-16,2015,6,16,17,3a0mv7,Network weights for a top network trained on ImageNet,https://www.reddit.com/r/MachineLearning/comments/3a0mv7/network_weights_for_a_top_network_trained_on/,Markkarmmark,1434442768,"Hi,

I lack the hardware to train a deep net on a good subset of ImageNet. I wonder if Google or anyone gives out their synapse weights so i can score the model with new pictures. 

I'm looking to have a high dimensional representation of the original picture that i can use as features for another model.

Searching the internet i come as far as the config files for the cuda framework to do my own training, but i fail to find the weight files.

Thanks for pointing me in the right direction.",10,3
317,2015-6-16,2015,6,16,17,3a0nph,Any successes using Variational Recurrent Auto-Encoders?,https://www.reddit.com/r/MachineLearning/comments/3a0nph/any_successes_using_variational_recurrent/,CreativePunch,1434443514,"Hi all,

I have succesfully implemented a Variational Auto-Encoder and was able to reproduce the results on the MNIST dataset. I am now trying to make it recurrent following the paper on Variational Recurrent Auto-Encoders at: http://arxiv.org/abs/1412.6581

I am wondering if anyone else has implemented a Variational Recurrent Auto-Encoder yet (a github search came up empty) and more importantly if they have any success using it, especially on a larger dataset such as the nottingham midi database.

Reason I am asking is because the authors of the paper seem to have trained it only on a small dataset of 8 midi files, which upon the inspection of the generated music at https://youtu.be/cu1_uJ9qkHA seems to have lead to gross overfitting in conjunction with the fact they used 500 hidden units for their experiment on only 50 timesteps with 44 input nodes. Am I wrong in thinking this?

Personally I tried training it on the nottingham database without any good results; All my model does is repeat the same notes for every timestep and it does not saturate the output activations (sigmoid) either.",8,4
318,2015-6-16,2015,6,16,19,3a0vuh,2x2 convolution with zero-padding,https://www.reddit.com/r/MachineLearning/comments/3a0vuh/2x2_convolution_with_zeropadding/,hadeson,1434450416,"Hello, 
I wonder if anyone has experience of using 2x2 filters with zero-padding in a convolution network? Also would a stack of two 2x2 convolution layers equivalent to a 3x3 layer?",1,0
319,2015-6-16,2015,6,16,20,3a0yhq,"Twitter 'Who_to_follow' recommendation: PageRank, KMeans-clustering approach",https://www.reddit.com/r/MachineLearning/comments/3a0yhq/twitter_who_to_follow_recommendation_pagerank/,bala_io,1434452588,,0,4
320,2015-6-16,2015,6,16,20,3a12to,Design Elements for the Internet of Things 2.0,https://www.reddit.com/r/MachineLearning/comments/3a12to/design_elements_for_the_internet_of_things_20/,mooserider2,1434455867,,3,0
321,2015-6-16,2015,6,16,21,3a14ay,"""How to Grow a Mind: Statistics, Structure and Abstraction"" - J. Tenenbaum, Lecture @ NIPS'10",https://www.reddit.com/r/MachineLearning/comments/3a14ay/how_to_grow_a_mind_statistics_structure_and/,USER_PVT_DONT_READ,1434456875,,16,40
322,2015-6-16,2015,6,16,22,3a19gl,https://medium.com/@oslokommuneper/machine-learning-in-a-week-a0da25d59850,https://www.reddit.com/r/MachineLearning/comments/3a19gl/httpsmediumcomoslokommunepermachinelearninginaweek/,rishiarora,1434460092,Machine Learning in a week.,1,0
323,2015-6-16,2015,6,16,22,3a19su,featureforge 0.1.6 - A Python library to build and test machine learning features,https://www.reddit.com/r/MachineLearning/comments/3a19su/featureforge_016_a_python_library_to_build_and/,copybin,1434460291,,0,0
324,2015-6-16,2015,6,16,22,3a19z0,Holt-Winters parameter optimization with gradient descent,https://www.reddit.com/r/MachineLearning/comments/3a19z0/holtwinters_parameter_optimization_with_gradient/,polyfractal,1434460405,"I'm working on a parameter optimizer for Holt-Winters moving average, since the parameters are rather tricky to tune by hand.  Unfortunately, I've run into some problems (largely due to my lack of calculus knowledge).

- I'm using the component form of the HW equation here: https://www.otexts.org/fpp/7/5
- I attempted to take the partial derivative of the forecast formula with respect to alpha, beta and gamma
- These partial derivatives are then used as gradients in a gradient-descent optimizer.

Results are hit or miss.  The optimizer sometimes converges to a legitimate answer, but more often than not gives garbage results.  I've tried the whole gamut of learning rates and have never found a ""sweet spot"", so I don't think the optimizer itself is wrong.

My current theory is that either my calculus is wrong and the gradients are faulty, or perhaps optimizing parameters in this manner is not a valid approach?  Because the HW formula is technically recursive (it depends on previous values), does that mean I need to continue differentiating down the rabbit hole of previous, dependent values?  Currently I'm just treating the previous values as constants.

I've also read in at least one paper that moving averages are non-convex, so gradient based optimization may fall into a local minimum.  Maybe that's why my results are poor?

Some other options I'm thinking about exploring:

- Finite differencing to approximate the gradients, instead of trying to calculate them by hand
- [Automatic differencing ala this tutorial](http://nbviewer.ipython.org/gist/mdenil/83c88b7e7f3ff4572a0b)
- Some blackbox, derivative-free optimizer

As an aside, the R forecast package seems to use L-BFGS-B with, I think, finite differences (e.g. not specifying a gradient for the `optim` function).

Any tips/pointers would be greatly appreciated!",1,0
325,2015-6-16,2015,6,16,22,3a1ebc,Image generated by a Convolutional Network,https://www.reddit.com/r/MachineLearning/comments/3a1ebc/image_generated_by_a_convolutional_network/,swifty8883,1434462820,,122,596
326,2015-6-16,2015,6,16,23,3a1lzf,IBM Sparks Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3a1lzf/ibm_sparks_machine_learning/,[deleted],1434466629,,0,1
327,2015-6-17,2015,6,17,0,3a1nwy,Python Stream Clustering Library similar to MOA,https://www.reddit.com/r/MachineLearning/comments/3a1nwy/python_stream_clustering_library_similar_to_moa/,warriortux,1434467512,"Hello,

I am trying to implement a Stream Clustering algorithm, and was wondering whether there is any library (or something part of the Scipy or Scikit-learn) that can handle streaming data.

I am looking for something similar to MOA: http://moa.cms.waikato.ac.nz/

",0,0
328,2015-6-17,2015,6,17,0,3a1oen,my recurrent neural network art,https://www.reddit.com/r/MachineLearning/comments/3a1oen/my_recurrent_neural_network_art/,godspeed_china,1434467717,"http://bbs.fudan.edu.cn/upload/PIC/1434288192-0501.png
http://bbs.fudan.edu.cn/upload/PIC/1434288200-8237.png
http://bbs.fudan.edu.cn/upload/PIC/1434288213-6528.jpg
http://bbs.fudan.edu.cn/upload/PIC/1434288219-9703.jpg
http://bbs.fudan.edu.cn/upload/PIC/1434288226-1810.jpg
http://bbs.fudan.edu.cn/upload/PIC/1434288234-6321.jpg",8,7
329,2015-6-17,2015,6,17,0,3a1p4t,Where could I find literature about how to model optimal decision making in business?,https://www.reddit.com/r/MachineLearning/comments/3a1p4t/where_could_i_find_literature_about_how_to_model/,dsocma,1434468028,"I know there are optimization theory approaches, but its also an artificial intelligence problem.  For example what if you modeled it as a Partially Observable Markov Decision Process?",2,1
330,2015-6-17,2015,6,17,0,3a1tn6,The Future of AI - a Non Alarmist Viewpoint,https://www.reddit.com/r/MachineLearning/comments/3a1tn6/the_future_of_ai_a_non_alarmist_viewpoint/,simonhughes22,1434470048,,0,2
331,2015-6-17,2015,6,17,1,3a1yj4,Using Machine Learning to predict the outcome of a zzuf fuzzing campaign,https://www.reddit.com/r/MachineLearning/comments/3a1yj4/using_machine_learning_to_predict_the_outcome_of/,galapag0,1434472148,,0,0
332,2015-6-17,2015,6,17,2,3a24wx,Copyright laws and machine learning algorithms,https://www.reddit.com/r/MachineLearning/comments/3a24wx/copyright_laws_and_machine_learning_algorithms/,IllegalThings,1434474791,If you train a learning algorithm using copyright images and then generate an image from the training data. What sort of legal/copyright implications would arise from the generated image? Would the copyright be owned by the person generating the image? Would it be in violation of all copyrights of all images used as training data?,7,2
333,2015-6-17,2015,6,17,2,3a27d7,"A course on ""Machine Learning for Computer Vision"" by Dr. Rudolph Triebel (TU Munich)- 2014",https://www.reddit.com/r/MachineLearning/comments/3a27d7/a_course_on_machine_learning_for_computer_vision/,ojaved,1434475744,,6,9
334,2015-6-17,2015,6,17,3,3a2c05,Class of machine learning models for matchups,https://www.reddit.com/r/MachineLearning/comments/3a2c05/class_of_machine_learning_models_for_matchups/,[deleted],1434477675,"I'm an incoming sophmore in a university, learning about machine learning. I'm working on a solution to the kaggle march madness, and I'm looking for a model with a specific behavior. 

Games can be predicted by looking at the nature of a matchup between two teams. When evaluating matchups, its important to look at the play style of the two teams and see how they clash, and the play style of a team is reflected in game stats (across several games for that team). By play style, I mean a team can be highly defensive, offensive, 3 point shooting team, etc. 

Consider some sort of a graph model where the nodes are the different teams. Edges between teams would describe the relationship of their matchup (which would be dictated based on the games between those two teams). 

When it comes time to predict matchups, it could be possible that two teams have never played each other previously. However, one can predict what kind of a matchup these two teams will have based on the matchups of the teams they have played mutually. 

For example, if team A has played team C and team B has played team C. What does this tell us about how team A can play against team B? 

This example can of course be extended to more teams, and a model that can do this is exactly what I'm looking for. **The ability to infer about a matchup by looking at matchups against common teams.** I imagine this would be some sort of a graph structure. 

I apologize if my questions are rather vague or maybe flat out wrong. I'm familiar with most of the introductory ML models, but I'm still learning. 

If anyone can elaborate on my thoughts or point me in the right direction it would be great. ",3,1
335,2015-6-17,2015,6,17,3,3a2crl,Bayesian Dark Knowledge &lt;paper by Kevin Murphy's group at Google &gt;,https://www.reddit.com/r/MachineLearning/comments/3a2crl/bayesian_dark_knowledge_paper_by_kevin_murphys/,muktabh,1434477981,,0,18
336,2015-6-17,2015,6,17,3,3a2d5f,I redrew the Scikit-learn algorithm cheat sheet in a more minimal style.,https://www.reddit.com/r/MachineLearning/comments/3a2d5f/i_redrew_the_scikitlearn_algorithm_cheat_sheet_in/,groovesnark,1434478153,"Hey, everyone. I've seen this[ Scikit-learn cheat sheet/flow chart](http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html) a while back from Andreas Mueller. And while I liked the general principle, I disliked the design. To me, it was rather difficult to read. So I fired up Illustrator and drew a cleaner version. I thought someone out there might appreciate it so here it is:

http://i.imgur.com/ryOuViG.png",7,9
337,2015-6-17,2015,6,17,3,3a2erh,Introduction to Neural Machine Translation with GPUs (Part 2),https://www.reddit.com/r/MachineLearning/comments/3a2erh/introduction_to_neural_machine_translation_with/,clbam8,1434478787,,0,12
338,2015-6-17,2015,6,17,4,3a2lku,"High Schooler getting into AI, how should I get started?",https://www.reddit.com/r/MachineLearning/comments/3a2lku/high_schooler_getting_into_ai_how_should_i_get/,moinnadeem,1434481579,"Hey! 

I'm a rising senior attempting to get started in AI. I've been programming since the fifth grade, and am now researching Intelligent Agents at my local university. What would be the best thing for me to get started doing on a hands on level? My professor has me reading ""AI: A Modern Approach"", but I also want to get coding soon. Any suggestions on what to get started with would be really appreciated, thanks! ",5,1
339,2015-6-17,2015,6,17,4,3a2moo,Better understanding MarI/O,https://www.reddit.com/r/MachineLearning/comments/3a2moo/better_understanding_mario/,neanderslob,1434482028,"Hi all, there's a fantastic video that's been floating around of teaching a program how to play Super Mario via neuroevolution.  Check it out here (worth it): https://www.youtube.com/watch?v=qv6UVOQ0F44  Anyway, the narrator gives a basic overview of the neural network involved but I feel like a couple things are left unexplained.  I'm wondering if anyone can shed some light on them.

In particular, my confusion seems to center around MarI/O's negotiation of obstacles (black squares).  In the video it's mentioned (at least in one instance) that a black square triggers a jump, but I can't imagine this is hard-coded (and doesn't seem to be based on the video).  

Below I have some hypotheses as to how certain aspects of the learning algorithm work that seemed to be glossed over on the video.  I'd love to hear what everyone's thoughts are on them.

1)  When MarI/O is hit/killed by an object, the program iterates over random possible earlier points to jump until he lands on the ground with a new threshold of fitness (and therefore has successfully negotiated the obstacle).

2)  When he reaches a new fitness level after negotiating an obstacle, his right/left movement is randomized again until he re-discovers how to increase fitness (move right) after jumping.

3)  Once he successfully negotiates an obstacle, he puts this technique in his ""obstacle negotiation bag-o-tricks.""  Upon seeing another obstacle, he iterates through his learned tricks.  If none work, he resorts to the iterations described in Item 1 to find a new jump-point that will achieve maximum fitness.  Once he finds one that works, he adds it to his memory to try in the future.

Finally, the video never described what constituted a species or genome.  I'm guessing the genome refers to an individual sequence of commands that can be combined randomly to search for fitness gains, a species is a collection of different genomes that randomly combine.  I'm further guessing that genomes cannot combine across different species.  Finally, generation is one iteration of intra-species genome combinations, across all species.  If this is true, I wonder the following: what marks the creation of a new genome within a species and a new species within a generation?

Anyway, if any of the above makes sense, feel free to let me know your thoughts.  If you actually have read the paper on this type of neuroevolution please feel free to give a definitive answer.  If (like me) you haven't but have thoughts about the above assumptions/questions please share those too.  ",7,1
340,2015-6-17,2015,6,17,4,3a2qj7,How to generalize prediction performance to further data?,https://www.reddit.com/r/MachineLearning/comments/3a2qj7/how_to_generalize_prediction_performance_to/,ienaplissken,1434483659,"Let's suppose we are training a model (say a ConvNet for image classification) to fit a dataset. After the proper splitting into train, validation and test sets, we train the net and obtain a certain accuracy, e.g. 80% on the test set. Let assume a test set with an appropriate size (not noisy).

Now, more data are coming, and our deployed net classify them with much smaller accuracy, say 50%. The images, of course, belong to the same classes of the original dataset, but they clearly have not the same distribution.

Is it a ""normal"" behavior, an implicit dataset bias? We can of course expand our dataset with the incoming new data, but is it the right thing to do? Or am I missing something?",5,1
341,2015-6-17,2015,6,17,4,3a2ssn,What is the best tool for ANN building?,https://www.reddit.com/r/MachineLearning/comments/3a2ssn/what_is_the_best_tool_for_ann_building/,redlikeazebra,1434484586,"I like matlabs toolbox and easy to use matrix manipulations, deployability, but I am unsure of the commonplace software used by the experts.",7,0
342,2015-6-17,2015,6,17,5,3a2yvf,New ML &amp; Data Science Platform!,https://www.reddit.com/r/MachineLearning/comments/3a2yvf/new_ml_data_science_platform/,pr_lumi,1434487108,,0,0
343,2015-6-17,2015,6,17,6,3a39ta,Video labeling tools?,https://www.reddit.com/r/MachineLearning/comments/3a39ta/video_labeling_tools/,basicbean3,1434491760,"Are there any decent semi-automatic tools for video segmentation and labeling?

I am thinking of something that would allow to select object boundary (kinda like Photoshop's ""magic wand"") in the first frame, choose a label for it, then propagate the boundaries forward in the sequence while allowing the user to make corrections.",2,0
344,2015-6-17,2015,6,17,8,3a3idr,Speech Recognition from Brain Activity,https://www.reddit.com/r/MachineLearning/comments/3a3idr/speech_recognition_from_brain_activity/,Noncomment,1434495688,,1,0
345,2015-6-17,2015,6,17,8,3a3m0o,Real-Time Object Detection With YOLO,https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/,pjreddie,1434497539,,15,14
346,2015-6-17,2015,6,17,8,3a3n47,Deep learning for cannabis strain identification. Deep convolutional networks at it again.,https://www.reddit.com/r/MachineLearning/comments/3a3n47/deep_learning_for_cannabis_strain_identification/,jimmie21,1434498122,,7,1
347,2015-6-17,2015,6,17,15,3a4x8t,Beam search,https://www.reddit.com/r/MachineLearning/comments/3a4x8t/beam_search/,bge0,1434523572,So I have read a few papers that refer to this concept of matching the outputs of rnn encoders to decoders. What exactly is this? Is it just linear search on some subset?,12,2
348,2015-6-17,2015,6,17,16,3a4zuh,How much of an effect does colorspace have on image recognition networks?,https://www.reddit.com/r/MachineLearning/comments/3a4zuh/how_much_of_an_effect_does_colorspace_have_on/,quadrapod,1434525678,"Machine learning isn't really my forte but I've been curious, it would seem to me at least that as humans since we get so much more information out of luminescence than we do from color that our images would all be better understood and recognized in YCbCr than RGB.  Simply because we're more likely to vary distort or drop color information than we are luminescence information.  Is there any merit to this theory, and likewise has anyone actually done work on this and compared various matching confidences in deep neural networks taught in differing colorspaces. 
",2,0
349,2015-6-17,2015,6,17,16,3a50jb,[1506.02626] Learning both Weights and Connections for Efficient Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3a50jb/150602626_learning_both_weights_and_connections/,[deleted],1434526224,,0,1
350,2015-6-17,2015,6,17,16,3a50zj,Chances for Acceptance into ML Ph.D. Program,https://www.reddit.com/r/MachineLearning/comments/3a50zj/chances_for_acceptance_into_ml_phd_program/,mlQuestionThrowaway,1434526616,"I'm an Electrical &amp; Computer Engineering Masters student at a large private school in the US that's well known, although not for its engineering program. I am wondering what my chances are of ever being able to attend a top 15 ML Ph.D. program given the following:

1. My M.S. thesis involves statistics, signal processing, and a small bit of machine learning. It is good research, not ground-breaking, but it is a novel application of statistical techniques in its particular domain.
2. I am the primary author on one regional IEEE conference paper, and a secondary author on a national IEEE conference paper. However, neither papers are in the machine learning field. 
3. I am in the final stages of writing the first draft of a stats-heavy signal processing journal article, but its acceptance is far from a guarantee.
4. My GRE scores were 97% for verbal, 86% for quant., and 80% for writing.
5. My GPA is a 3.9, and my undergraduate GPA (at the same school) was a 3.28.
6. My programming work is about 50/50 split between Python and Matlab, and I'm comfortable in both.

I will graduate in May of 2016. I am debating either continuing into Ph.D. straightaway or entering industry for a while. I'm looking to determine how crazy it would be for me to apply for some top ML schools, or if I should save my money, time, and dignity and play it a little more safely.

/r/machinelearning, what are my chances of ever being able to enter an excellent ML graduate program, now or in the future? Are there any steps beside what I've listed that I should be taking at this point to help me achieve this goal? Is there prejudice against applicants who have waited a while to continue on to get their doctorate? I'm assuming next May is my most likely time to get accepted into any form of Ph.D. program, but I'm looking to crowdsource confirmation of my intuition. ",8,0
351,2015-6-17,2015,6,17,17,3a549t,"Tonight: Paris Machine Learning Meetup #10 Season 2 Finale: ""And so it begins"": Deep Learning, Recovering Robots, Vowpal and Hadoop, Predicsis, Matlab, Bayesian test, Experiments on #ComputationalComedy &amp; A.I.",https://www.reddit.com/r/MachineLearning/comments/3a549t/tonight_paris_machine_learning_meetup_10_season_2/,compsens,1434529543,,4,9
352,2015-6-17,2015,6,17,21,3a5lpi,Facial-recognition talks collapse over privacy issues - BBC News,https://www.reddit.com/r/MachineLearning/comments/3a5lpi/facialrecognition_talks_collapse_over_privacy/,organic-neural-net,1434543466,,0,23
353,2015-6-17,2015,6,17,21,3a5med,"Ambulance Service, Data in Search of a Question",https://www.reddit.com/r/MachineLearning/comments/3a5med/ambulance_service_data_in_search_of_a_question/,Simusid,1434543934,"I am brand new to ML and I've been self-learning via the Andrew Ng videos.   I work at an ambulance service and have a dump of 10k ambulance run data (non-HIPAA data) that consists of:

* date/time for dispatch, arrival on scene, departure from scene, arrival at hospital
* odometer mileage (we bill per mile)
* incident priority 1 (life threat), 2 (serious), 3 (normal), 4 (no transport)
* call reason (selection from a pre-populated list)
* Destination hospital (one of about 6)
* clinical impression (pp list but different from call reason)
* age/sex of patient

So I have two goals.  First, I want to understand some ML tools and how I can properly use them.   Second, and maybe more important, I would love to get some operational insight into our ambulance organization.  That is, I would love to discover some new fact that is hidden in our data that I can use to make an improvement in our service.

Obviously we have no control over when 911 will be called.   But we do know that we get 7 calls per day on average, and it is essentially a poisson process.  We ""bin"" the call count per hour and know our peak demand and slow times per day.   Our ""analysis"" is very rudimentary and mostly done with excel.  

I would be interested if anyone has any suggestions for research questions based on this data that are suitable for an ML topic.  ",2,5
354,2015-6-17,2015,6,17,23,3a63gn,Anomaly Detection in a Single Command Line,https://www.reddit.com/r/MachineLearning/comments/3a63gn/anomaly_detection_in_a_single_command_line/,osroca,1434553026,,0,1
355,2015-6-18,2015,6,18,0,3a676r,Aerosolve: Machine learning for humans,https://www.reddit.com/r/MachineLearning/comments/3a676r/aerosolve_machine_learning_for_humans/,It_Is1-24PM,1434554675,,0,2
356,2015-6-18,2015,6,18,1,3a6cnf,ZEN-RRNN - On Meditation &amp; Machines,https://www.reddit.com/r/MachineLearning/comments/3a6cnf/zenrrnn_on_meditation_machines/,samim23,1434557049,,1,0
357,2015-6-18,2015,6,18,1,3a6efs,Brain-to-text: decoding spoken phrases from phone representations in the brain,https://www.reddit.com/r/MachineLearning/comments/3a6efs/braintotext_decoding_spoken_phrases_from_phone/,xamdam,1434557801,,0,6
358,2015-6-18,2015,6,18,1,3a6fem,Decided on ML as a learning topic for my vacation. What books does MLReddit recommend?,https://www.reddit.com/r/MachineLearning/comments/3a6fem/decided_on_ml_as_a_learning_topic_for_my_vacation/,pure_x01,1434558226,"When i'm on vacation i like to spend some time learning and this time i decided on ML. I would like books that covers the following topics:

* An overview of the recent advancements and the different areas
* Machine Learning
* Data Mining
* Text Extraction
* Any other recommended books

If they are available on kindle it would be nice :-)

Thanks",6,3
359,2015-6-18,2015,6,18,2,3a6kox,Is there an area where neural networks work best?,https://www.reddit.com/r/MachineLearning/comments/3a6kox/is_there_an_area_where_neural_networks_work_best/,ModerateEntropy,1434560484,"**[edit] Please, take this as a question born from honest curiosity. It seems some people thought I'm looking down on NNs, and that's not the case.**

This is kind of general, but recently I went to a conference where someone had this quote in her presentation:
""Neural networks will one day rule the world (Kurzweil, 1999)""

This got me wondering if presently, after 16 years of that statement being made, there is any type of problem/application where neural networks performance is really superior to other methods.
I have worked for a few years in classification, and neural networks where never the optimal solution to any of my tasks.

What's your input or experience about this?
",17,2
360,2015-6-18,2015,6,18,2,3a6lw2,(theoretical fun post) what would it take for a vision-language system to explain visual irony?,https://www.reddit.com/r/MachineLearning/comments/3a6lw2/theoretical_fun_post_what_would_it_take_for_a/,Articulated-rage,1434561015,,21,38
361,2015-6-18,2015,6,18,2,3a6q18,Does modeling with Random Forests requre cross-validation?,https://www.reddit.com/r/MachineLearning/comments/3a6q18/does_modeling_with_random_forests_requre/,srkiboy83,1434562781,"As far as I've seen, opinions tend to differ about this. Best practice would certainly dictate using cross-validation (especially if comparing RFs with other algortihms on the same dataset). On the other hand, the original source states that the fact OOB error is calculated during model trainig is enough of an indicator of test set performance. Even Trevor Hastie, in a relatively recent talks says that ""Random Forests provide free cross-validation"". Intuitively, this makes sense to me, if training and trying to improve a RF-based model on one dataset.

So, what's MLReddit's opinion on this?

",4,4
362,2015-6-18,2015,6,18,3,3a6uh7,Where to get started with AI/machine learning?,https://www.reddit.com/r/MachineLearning/comments/3a6uh7/where_to_get_started_with_aimachine_learning/,RedditBazinga,1434564584,I understand the basics of programming and have a degree in mathematics. Is there any programming language that is recommended? Where would I get started in general?,2,0
363,2015-6-18,2015,6,18,3,3a6unu,IBM Working with Spark Creators on Open Source Machine Learning Language/Platform,https://www.reddit.com/r/MachineLearning/comments/3a6unu/ibm_working_with_spark_creators_on_open_source/,[deleted],1434564663,,0,2
364,2015-6-18,2015,6,18,3,3a712u,Launch Karapthy's Char-RNN,https://www.reddit.com/r/MachineLearning/comments/3a712u/launch_karapthys_charrnn/,[deleted],1434567392,,0,1
365,2015-6-18,2015,6,18,4,3a71p5,Is reverse engineering of device drivers with machine learning possible?,https://www.reddit.com/r/MachineLearning/comments/3a71p5/is_reverse_engineering_of_device_drivers_with/,fimari,1434567641,"Is this a thing? Anyone out there who tried that?
Where to start? Webcam, Keyboard, Mouse?",3,1
366,2015-6-18,2015,6,18,5,3a7b5y,ImageNet Data,https://www.reddit.com/r/MachineLearning/comments/3a7b5y/imagenet_data/,Xixiduro,1434571605,"Hello :)

I'm a researcher and i'm trying to get imagenet dataset to make experiments in deep learning for my masters degree. I already search in web but i can't find the data.

I made some experiments using the training model available in caffe framework with cool results, however i don't need that amount of classes and i want to implement my own network

If anyone know a repository or a direct link to download the raw data i will be grateful.

Thanks ",1,0
367,2015-6-18,2015,6,18,5,3a7fxt,Launch Karpathy's Char-RNN,https://www.reddit.com/r/MachineLearning/comments/3a7fxt/launch_karpathys_charrnn/,tfimg24,1434573662,,0,0
368,2015-6-18,2015,6,18,5,3a7gxy,More data always better than better algorithms? by Xavier Amatriain,https://www.reddit.com/r/MachineLearning/comments/3a7gxy/more_data_always_better_than_better_algorithms_by/,tomtung,1434574092,,0,0
369,2015-6-18,2015,6,18,6,3a7jpl,Picked up this as I'm interested in sentiment-analysis I'm finding it worth-while perhaps others here will too. Just published by Cambridge.,https://www.reddit.com/r/MachineLearning/comments/3a7jpl/picked_up_this_as_im_interested_in/,theirfReddit,1434575249,,0,0
370,2015-6-18,2015,6,18,6,3a7mol,"Best tools for GLMNET, GAMs and Mixed Effects Models on Large Data?",https://www.reddit.com/r/MachineLearning/comments/3a7mol/best_tools_for_glmnet_gams_and_mixed_effects/,NotAHomeworkQuestion,1434576517,"I have 10,000 features (the vast majority are sparse) measured on 1,000,000 samples. For different applications Ill need to use GLMNET, generalized additive models and mixed effects models. Options Ive found so far:

R: reading this matrix into R obviously doesnt work. Theres the sparse matrix option in the Matrix package which I know glmnet and lmer4 can use (not sure about any of the gam packages). However Matrix doesnt seem to have a way of loading from a file that isnt in some special format. Even if I do go to the hassle of converting it, Im not sure whether these functions will be able to handle data of this size in a reasonable time frame. Does anyone have thoughts about loading and runtimes in this regard?

H2O: Ive been using this through R. Its buggy as hell (both versions 2 and 3) but managed to get GLMNET to give me something reasonable (though I have trouble completely trusting it given the flagrant bugs in simpler munging functions). Theres no support for GAMs or mixed effects.

Other popular options like Python, Spark, etc. dont seem to support anything but the most basic of stats/ML models.

Would something like Stan (Gelmans new version of BUGS) work? Is the R Matrix package capable of handling this? Are there other options people are aware of that would work? I cant believe theres nothing out there given how frequently this problem must come up.",1,1
371,2015-6-18,2015,6,18,6,3a7omn,HarlMCMC Shake,https://www.reddit.com/r/MachineLearning/comments/3a7omn/harlmcmc_shake/,polylogarithmic,1434577332,"Youtube videos comparing Metropolis Hastings to other sampling techniques, all to the tune of **Harlem Shake**!

**Original:** 

* [Metropolis Hastings vs. HMC](https://www.youtube.com/watch?v=Vv3f0QNWvWQ)
by [David Duvenaud](http://people.seas.harvard.edu/~dduvenaud/) &amp; [Tamara Broderick](http://people.csail.mit.edu/tbroderick/index.html)


**Follow ups:**

* [Metropolis Hastings vs. Parallel Temporing](https://www.youtube.com/watch?v=J6FrNf5__G0)
by [Dave Campbell](http://people.stat.sfu.ca/~dac5/Dave_Campbell/Dave_Campbell.html)

* [Metropolis Hastings vs. emcee](https://www.youtube.com/watch?v=yow7Ol88DRk)
by [Dan Foreman-Mackey](http://dan.iel.fm)


",1,9
372,2015-6-18,2015,6,18,6,3a7pd6,"Twitter acquires machine learning startup, Whetlab.",https://www.reddit.com/r/MachineLearning/comments/3a7pd6/twitter_acquires_machine_learning_startup_whetlab/,jchaines,1434577654,,0,11
373,2015-6-18,2015,6,18,10,3a8efs,The Unreasonable Effectiveness of Random Forests,https://www.reddit.com/r/MachineLearning/comments/3a8efs/the_unreasonable_effectiveness_of_random_forests/,D33B,1434590025,,30,24
374,2015-6-18,2015,6,18,11,3a8o9o,Question about how to structure a time series (R)NN problem,https://www.reddit.com/r/MachineLearning/comments/3a8o9o/question_about_how_to_structure_a_time_series_rnn/,ekoneko,1434595142,"In the past 5 months or so I have done a lot of reading and learning about ML in general, and neural networks in particular, and I may be able to apply what I've learned in an upcoming project for school (Mech Eng, final year).

I don't have the full spec yet, but it will probably be some sort of time series analysis for prediction and/or anomaly detection.

All of the neural network TS prediction problem discussions I have seen have been N (usually a fairly small number, frequently just one or two) input nodes, a reasonable size input nodes, and a single output node.

But if the problem works out like I think it will, we will need to predict multiple parameters - say 10 or so out of maybe twice that many (that is, we're not predicting ambient temp, humidity, time, etc). Maybe more, maybe less.

So would it make sense to put all of my parameters in to individual input nodes, and have one output node per parameter that I want to predict?

Or should I train multiple nets, each with a single output node?

My brain tells me it should be fine, though may require more hidden units. It's entirely possible that my brain is wrong, as I'm afraid I'm neither a doctor nor a data scientist (P &lt; 0.001). Also, it may end up being a recurrent net, depending on the data. Probably, but I'm not sure yet.

Thanks!

**tl;dr** Simply: is it realistic/workable to have multiple output predicting neurons for time series analysis? Or is there some issue with that design that I haven't come across?",1,0
375,2015-6-18,2015,6,18,11,3a8phr,Inceptionism: Going Deeper into Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3a8phr/inceptionism_going_deeper_into_neural_networks/,LLCoolZ,1434595862,,100,327
376,2015-6-18,2015,6,18,13,3a92p4,Going Deep into Neural Networks - Google,https://www.reddit.com/r/MachineLearning/comments/3a92p4/going_deep_into_neural_networks_google/,john_philip,1434603506,,0,0
377,2015-6-18,2015,6,18,14,3a93ln,Looking to do a ML beginner project on UK election data,https://www.reddit.com/r/MachineLearning/comments/3a93ln/looking_to_do_a_ml_beginner_project_on_uk/,[deleted],1434604085,"Specifically how AV systems would change parliaments makeup. While the simple answer to this is just compare majorities, like in Kaggles Titanic Data, Id like to try ML techniques on it. Perhaps also comparing it with countries that doe have AV.

Im really quite new at this, despite being a physics grad with a final year course in statistical models and hypothesis testing. So if someone could tell me what technique would be most realiable I can read about it and implement. ",0,1
378,2015-6-18,2015,6,18,14,3a945k,Who can/cannot learn programming?? The Data Science Answer,https://www.reddit.com/r/MachineLearning/comments/3a945k/who_cancannot_learn_programming_the_data_science/,varaggarwal,1434604449,,0,0
379,2015-6-18,2015,6,18,14,3a95zg,The History of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3a95zg/the_history_of_machine_learning/,john_philip,1434605679,,1,3
380,2015-6-18,2015,6,18,15,3a99h5,2015 Global Anti-Electromagnetic radiation fabric performance Tester Industry,https://www.reddit.com/r/MachineLearning/comments/3a99h5/2015_global_antielectromagnetic_radiation_fabric/,janiebmr,1434608164,"2015 Global Anti-Electromagnetic radiation fabric performance Tester Industry Report is a professional and in-depth research report on the worlds major regional market conditions of the Anti-Electromagnetic radiation fabric performance Tester industry, focusing on the main regions (North America, Europe and Asia) and the main countries (United States, Germany, Japan and China) .

Read more details at: http://www.bigmarketresearch.com/global-anti-electromagnetic-radiation-fabric-performance-tester-industry-2015-research-report-market

",0,1
381,2015-6-18,2015,6,18,15,3a9awn,word-level vs character-level models,https://www.reddit.com/r/MachineLearning/comments/3a9awn/wordlevel_vs_characterlevel_models/,eldeemon,1434609405,"[Karpathy's char-nn](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) seems to have sparked a lot of excitement about character-level modeling. Are character-level models being used anywhere, outside of the cute examples we've seen? How exactly do they stack up against word-level models, which appear to be the default choice for so many tasks? Has anyone tried word-level ""hallucinations,"" to borrow Alex Graves' term for sampling a sequence from a softmax? What's the best burrito you've had?",11,1
382,2015-6-18,2015,6,18,15,3a9bqv,Yann LeBron,https://www.reddit.com/r/MachineLearning/comments/3a9bqv/yann_lebron/,[deleted],1434609990,,1,0
383,2015-6-18,2015,6,18,16,3a9dzd,2015 Global Fabric Resistivity Meter Industry,https://www.reddit.com/r/MachineLearning/comments/3a9dzd/2015_global_fabric_resistivity_meter_industry/,janiebmr,1434611582,,0,1
384,2015-6-18,2015,6,18,17,3a9hgg,Different sizes of labeled sets,https://www.reddit.com/r/MachineLearning/comments/3a9hgg/different_sizes_of_labeled_sets/,krimtheguy,1434614403,"I'm trying to predict some hospital readmissions (which seem about a 6% of the total admissions in my dataset, once I label the admissions I have). 

Having this, I have a 94% of rows (admissions) labeled as 0 and the 6% labeled as 1. The dataset is big enough (200k+ rows) (even though I tried with smaller numbers to avoid overfitting). Finally once I try to fit the model (tried several techniques such as neural networks, randomForests, SVM ...) I end up with the conclussion that my model correctly classifies a 90% of data, the real problem here is that most of that data was going to be 0 already (and is correctly labeled as 0), where my goal is to classify correctly the 1's (which only seems to correctly classify about 7% of the label 1 rows)

My concern here is that maybe I'm missing something, maybe I have to limit the training data in some way that the percentage of 1's and 0's go closer. Does anyone know about this?

Thanks in advance",2,0
385,2015-6-18,2015,6,18,17,3a9k8q,Don't Miss These Scripts: Otto Group Product Classification,https://www.reddit.com/r/MachineLearning/comments/3a9k8q/dont_miss_these_scripts_otto_group_product/,john_philip,1434616907,,1,2
386,2015-6-18,2015,6,18,18,3a9njf,Beginner question to neural networks,https://www.reddit.com/r/MachineLearning/comments/3a9njf/beginner_question_to_neural_networks/,EndianOgino,1434619874,"So I'm new to machine learning and there is a question stuck in my mind I can't answer. My background is experimental physics and I have gained basic knowledge about neural networks. 

So I jump right into it.
Given a train dataset and a neural network. When training the network it is feed row by row to adjust the weights of the network. For example for row j you have a certain set of activation values throughout the network which yield an output vector which is under some minimization optimal. Next row j+1 is trained which changes again the activation values according to its optimal output. 

So my question is, if you predict the output vector of row j wont you get a different output vector as previously by the initial training? Or put more general, arent rows which are not in some sense orthogonal to each other influencing mutually the result of the network? In that case the outcome of a network is dependent on the order of training. Thus an ensemble of networks trained with a different with a randomized order of the dataset should be more robust than a single one.

I hope there is someone who can shed some light on my simple intuition.",3,2
387,2015-6-18,2015,6,18,19,3a9r56,Artificial Intelligence Insight from Deep Learning (Geoff Hinton Interview),https://www.reddit.com/r/MachineLearning/comments/3a9r56/artificial_intelligence_insight_from_deep/,vikkamath,1434623029,,0,7
388,2015-6-18,2015,6,18,19,3a9t16,The whole collection of pictures &amp; paintings repainted by Google's deep neural-networks,https://www.reddit.com/r/MachineLearning/comments/3a9t16/the_whole_collection_of_pictures_paintings/,theirfReddit,1434624648,,2,5
389,2015-6-18,2015,6,18,21,3a9zft,Glass Processing Machines Exporters,https://www.reddit.com/r/MachineLearning/comments/3a9zft/glass_processing_machines_exporters/,Md-Malik,1434629412,,0,1
390,2015-6-18,2015,6,18,21,3a9zy1,What can neural networks teach us about the way we perceive the world?,https://www.reddit.com/r/MachineLearning/comments/3a9zy1/what_can_neural_networks_teach_us_about_the_way/,AlanZucconi,1434629746,"Despite modern neural networks have more in common with algebra then biology, we cannot ignore the similarities with the way real neurons work. Findings in machine learning and mathematics have already been useful to understand how the brain works. For instance, certain dopaminergic neurons have been shown to implement, de-fact, a reinforcement learning protocol in which dopamin corresponds to the ""expected reward"" of an event.  Without such a deep knowledge of machine learning, it would have been harder to understand how OUR software works just using chemistry.

The way modern neural networks detect images is very similar to the way other living creatures achieve visual understanding. For instance, fruit flies have a hierarchical stratification of neurons in which each one encode a specific feature such as a colour, a movement, a shape, an orientation, etc.

By looking at the pictures in the recent article [Inceptionism: Going Deeper into Neural Networks](http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html) I cannot stop wondering how similar they look to visual artefacts creates by drugs such as LSD ([example 1](http://i.imgur.com/8Ur6pOr.jpg), [example 2](http://i.imgur.com/fbPqJqE.jpg), [example 3](http://www.zoomquilt.org/)).

Is this just a coincidence, or do you think we are actually getting closer to understand how certain drugs affects the visual cortex? Could machine learning be a bottom-up way to understand ourselves?",8,0
391,2015-6-18,2015,6,18,21,3aa0fz,Granite Processing Machinery Exporters,https://www.reddit.com/r/MachineLearning/comments/3aa0fz/granite_processing_machinery_exporters/,Md-Malik,1434630019,,0,1
392,2015-6-18,2015,6,18,21,3aa29f,Grinding Machinery Wholesalers,https://www.reddit.com/r/MachineLearning/comments/3aa29f/grinding_machinery_wholesalers/,Md-Malik,1434631068,,0,1
393,2015-6-18,2015,6,18,21,3aa3te,"Machine Learning: How Close Are We to Something Like I, Robot?",https://www.reddit.com/r/MachineLearning/comments/3aa3te/machine_learning_how_close_are_we_to_something/,aternoy,1434632003,,0,0
394,2015-6-18,2015,6,18,21,3aa3xw,Maturity of and software for Bayesian non-parametric models,https://www.reddit.com/r/MachineLearning/comments/3aa3xw/maturity_of_and_software_for_bayesian/,boxstabber,1434632072,"I've recently become more familiar with Bayesian non-parametric models (after reading the [well-written turorial](http://web.mit.edu/sjgershm/www/GershmanBlei12.pdf)). Chinese Restaurant Process describes a problem I face in a professional (note - not research) very well and I would like to try modelling my problem with it.

After a brief survey of what's available though, I got the impression that BNP models are not exactly mature enough to be used in a professional setting. Are there existing libraries that would make it easier to use? Are they mature/efficient? I am not in a position to develop everything from scratch. 

Sorry for unfamiliarity with the area.",4,1
395,2015-6-18,2015,6,18,22,3aa64f,Classification based on a single image?,https://www.reddit.com/r/MachineLearning/comments/3aa64f/classification_based_on_a_single_image/,jdsutton,1434633350,"I've been thinking about how a person can look at a single photograph of someone else, and then be able to recognize that person by sight later on. Are there any algorithms which can perform this kind of classification well based on just one image?",3,2
396,2015-6-18,2015,6,18,23,3aacf1,Best resource for learning R?,https://www.reddit.com/r/MachineLearning/comments/3aacf1/best_resource_for_learning_r/,MusicIsLife1995,1434636746,I noticed that there isn't much documentation on R. Where can I find good books/docs on it? Particular books that explain best practices.,5,0
397,2015-6-19,2015,6,19,0,3aakg0,Reinforcement Learning Lectures by DeepMind's David Silver,https://www.reddit.com/r/MachineLearning/comments/3aakg0/reinforcement_learning_lectures_by_deepminds/,changingourworld,1434640695,,0,28
398,2015-6-19,2015,6,19,0,3aaki1,Algorithmically generated erotica through multi-layer recurrent neural networks [NSFW text],https://www.reddit.com/r/MachineLearning/comments/3aaki1/algorithmically_generated_erotica_through/,mypetrobot,1434640724,,29,59
399,2015-6-19,2015,6,19,0,3aao2t,The Reason for the Effectiveness of Random Decision Forests,https://www.reddit.com/r/MachineLearning/comments/3aao2t/the_reason_for_the_effectiveness_of_random/,TinKamHo,1434642384,"Some historical background:   The first method of random decision forests [Ho 1995, 1998] was invented as a way to perform ""learning with random guesses"" in the context of E.M. Kleinberg's theory of stochastic discrimination:

""An overtraining-resistant stochastic modeling method for pattern recognition"", E.M. Kleinberg, Annals of Statistics, Volume 24, Number 6 (1996), 2319-2349.     

Quoting the Abstract of Kleinberg's paper:
""We will introduce a generic approach for solving problems in pattern recognition based on the synthesis of accurate multiclass discriminators from large numbers of very inaccurate 'weak' models through the use of discrete stochastic processes. Contrary to the standard expectation held for the many statistical and heuristic techniques normally associated with the field, a significant feature of this method of 'stochastic modeling' is its resistance to so-called 'overtraining.' The drop in performance of any stochastic model in going from training to test data remains comparable to that of the component weak models from which it is synthesized; and since these component models are very simple, their performance drop is small, resulting in a stochastic model whose performance drop is also small despite its high level of accuracy.""

The theory describes an extreme form of ensemble learning;  while many other ensemble learning algorithms can find some explanation under this theory.  The original random decision forest was directly motivated by this theory,  and was specifically designed for the inherent parallelism, the ability to generalize based on subsetting the features, and the uniform discriminative power for the sample space as offered by a tree partition procedure.    More about these in the above mentioned papers.",2,8
400,2015-6-19,2015,6,19,1,3aatvo,/r/MachineLearning hits 40K subscribers,https://www.reddit.com/r/MachineLearning/comments/3aatvo/rmachinelearning_hits_40k_subscribers/,TrendingBot,1434644931,,26,76
401,2015-6-19,2015,6,19,1,3aawzy,Program Learns to Exploit Bugs in NES Games,https://www.reddit.com/r/MachineLearning/comments/3aawzy/program_learns_to_exploit_bugs_in_nes_games/,tfimg24,1434646321,,5,2
402,2015-6-19,2015,6,19,3,3ab7ns,Pretty sure this is where that weird picture with a bunch of eyes came from,https://www.reddit.com/r/MachineLearning/comments/3ab7ns/pretty_sure_this_is_where_that_weird_picture_with/,coderman9,1434650986,,16,50
403,2015-6-19,2015,6,19,3,3abbos,Bumping: Why can a decision tree not learn the chessboard pattern,https://www.reddit.com/r/MachineLearning/comments/3abbos/bumping_why_can_a_decision_tree_not_learn_the/,cast42,1434652706,,4,18
404,2015-6-19,2015,6,19,5,3abox5,Hand-picked selection of this week's top ML news and resources,https://www.reddit.com/r/MachineLearning/comments/3abox5/handpicked_selection_of_this_weeks_top_ml_news/,mylittleai,1434658437,,0,0
405,2015-6-19,2015,6,19,5,3abr2v,"How to Pick Magic Numbers (a ""gentle"" introduction to Bayesian Optimization meant for beginners; feedback would be appreciated!)",https://www.reddit.com/r/MachineLearning/comments/3abr2v/how_to_pick_magic_numbers_a_gentle_introduction/,jmhessel,1434659402,,5,20
406,2015-6-19,2015,6,19,8,3acbfo,what is gradInput and gradOutput in Torch7's nn package?,https://www.reddit.com/r/MachineLearning/comments/3acbfo/what_is_gradinput_and_gradoutput_in_torch7s_nn/,abrocod,1434668855,"Hi I am a starter of using Torch's 'nn' package. In the past two weeks, I am extremely confused about the meaning of gradInput and gradOutput in the Torch's 'nn' library. I believe the 'grad' here means gradient, but what exactly are those two variables refers to?

Thanks for anyone's help! ",1,0
407,2015-6-19,2015,6,19,8,3acfgz,how to debug when convergence is slow?,https://www.reddit.com/r/MachineLearning/comments/3acfgz/how_to_debug_when_convergence_is_slow/,billconan,1434670969,"hello,

I'm trying to implement the word2vec model. I'm using skipgram with softmax error.

but the cost doesn't seem decrease. I don't know what's wrong. I have passed gradient check. I don't know what else I can try to debug this?",1,2
408,2015-6-19,2015,6,19,8,3acfsp,Future of industry jobs in ML?,https://www.reddit.com/r/MachineLearning/comments/3acfsp/future_of_industry_jobs_in_ml/,asymptotics,1434671139,"As companies (outside the tech giants) are only starting to implement ML methods and hire people with an ML background, over the next few years the job market could be quite different.

Most traditional corporations probably don't need a big dedicated team doing CV/NLP-type ML, and most ""data scientist"" postings I've seen from those companies seem fairly similar to business analyst-type roles (""knowledge of SQL and SAS/R, linear regression,etc."")

Do you think there will be an increasing need for people with good knowledge of neural networks, random forests, SVMs, etc. but who don't have a PhD in CS, or is the growth going to be mostly on the data scientist/engineer side?",3,4
409,2015-6-19,2015,6,19,11,3acxr0,"I wrote an article on Deep Learning and SEO. Because of your feedback Reddit, I was able to get it on TechCrunch =)",https://www.reddit.com/r/MachineLearning/comments/3acxr0/i_wrote_an_article_on_deep_learning_and_seo/,natesikes,1434680768,,1,2
410,2015-6-19,2015,6,19,13,3ada4y,good paper for batch LDA?,https://www.reddit.com/r/MachineLearning/comments/3ada4y/good_paper_for_batch_lda/,poporing88,1434687963,I have 200k training samples with a feature dimension of 3k. It is memory intensive to perform LDA in matrix form. Is there a good paper for batch LDA that could give as close as possible results to LDA?,3,0
411,2015-6-19,2015,6,19,14,3adg6p,RE.WORK Deep Learning Summit Boston 2015,https://www.reddit.com/r/MachineLearning/comments/3adg6p/rework_deep_learning_summit_boston_2015/,evc123,1434692054,,4,7
412,2015-6-19,2015,6,19,14,3adhrd,torch7: why ever use nn's container classes?,https://www.reddit.com/r/MachineLearning/comments/3adhrd/torch7_why_ever_use_nns_container_classes/,kitofans,1434693142,"i'm trying to learn torch7 and i'm not really seeing why i would ever use nn's container class. It seems that nngraph's gmodule class is strictly better. is there some speed advantage, or something of this sort, to using nn's containers (such as, say, sequential)?",2,9
413,2015-6-19,2015,6,19,15,3adivh,Global Manual angle seat valve Industry 2015 Deep Market Research Report,https://www.reddit.com/r/MachineLearning/comments/3adivh/global_manual_angle_seat_valve_industry_2015_deep/,stevensrachel,1434693936,,0,0
414,2015-6-19,2015,6,19,21,3aeazf,Recommender system libraries in Python,https://www.reddit.com/r/MachineLearning/comments/3aeazf/recommender_system_libraries_in_python/,mlguy_throwaway,1434717604,"Hi,

Can someone recommend a good recommendation system library for Python? I need to use collaborative filtering and item based filtering algorithms. All the libraries I have had a look at have either very poor documentation or are not in development anymore. Would really appreciate any help here.",2,1
415,2015-6-19,2015,6,19,21,3aeb59,Is text summarization implementation doable for a non-english language? (Romanian),https://www.reddit.com/r/MachineLearning/comments/3aeb59/is_text_summarization_implementation_doable_for_a/,TrafalgarLaw_,1434717704,"Hello! I am thinking what to do for dissertation, but I havent found much on different languages, and my knowledge of Natual language processing and other related fields is poor. So as the title says, is it doable for a guy with average programming skills or a dead end? If yes, please leave some links where I can get started with this. Thanks in advance!",6,3
416,2015-6-19,2015,6,19,23,3aenkf,"The Theano tutorial confused me, so I made a stripped-down version of the MLP example",https://www.reddit.com/r/MachineLearning/comments/3aenkf/the_theano_tutorial_confused_me_so_i_made_a/,syllogism_,1434724653,,30,73
417,2015-6-20,2015,6,20,0,3aewy1,Google sets up feedback loop in its image recognition neural network - which looks for patterns in pictures - creating these extraordinary hallucinatory images (x-post from /r/technology),https://www.reddit.com/r/MachineLearning/comments/3aewy1/google_sets_up_feedback_loop_in_its_image/,daithibowzy,1434729272,,2,0
418,2015-6-20,2015,6,20,1,3af4hq,[1412.6583] Discovering Hidden Factors of Variation in Deep Networks,https://www.reddit.com/r/MachineLearning/comments/3af4hq/14126583_discovering_hidden_factors_of_variation/,glassackwards,1434732781,,5,12
419,2015-6-20,2015,6,20,2,3af6fx,[1411.1784] Conditional Generative Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/3af6fx/14111784_conditional_generative_adversarial_nets/,glassackwards,1434733634,,0,10
420,2015-6-20,2015,6,20,3,3afgck,predicting multiple choice values between two lists on one form,https://www.reddit.com/r/MachineLearning/comments/3afgck/predicting_multiple_choice_values_between_two/,Y3PP3R,1434738121,"Hi,
I'm more of a programmer than an AI guy, so I need help understanding my problem.

Context:
I have a set of forms with two lists: activities/risks, and measures. I'm assuming there is a relation between the two.

Question:
Given a set of forms, with two multiple choice lists (I estimate 20-40 items per list, same items on all forms), can we predict the values that are selected in set B given the values in set A, for any given form?
What is the kind of problem? And the kind of algorithm that solves this problem?

My own idea:
I'm thinking I can represent the list of inputs and outputs as a list of 0's and 1's, and I guess this might be input for a neural network? Don't know enough about that yet, if I might be right, more pointers are appreciated in the context of my problem.",3,0
421,2015-6-20,2015,6,20,4,3aflbi,[1506.05751] Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/3aflbi/150605751_deep_generative_image_models_using_a/,alecradford,1434740443,,3,27
422,2015-6-20,2015,6,20,4,3afn9i,carbide bandsaw blades,https://www.reddit.com/r/MachineLearning/comments/3afn9i/carbide_bandsaw_blades/,similura1,1434741365,,0,1
423,2015-6-20,2015,6,20,4,3afp5v,How to properly code labeled input data to an ANN,https://www.reddit.com/r/MachineLearning/comments/3afp5v/how_to_properly_code_labeled_input_data_to_an_ann/,jekyllstein,1434742235,"I know best practices is to normalize your input variables to have mean 0 and var of 1 but let's say some of your input variables are a label rather than a number.  

I'd normally convert such a label into a binary vector who's length is equal to the number of possible labels but I was wondering if the vector should be [0 0 0 1 0 0 ....] if the 4th label applied or [-1 -1 -1 1 -1 -1 ....].  In other words, should the labels that don't apply be converted to 0's or -1's.  

I know for doing classification in the output using a sigmoid function the y examples should be in the format [0 0 0 1 0 ...], but this question is for input rather than output labels.",3,0
424,2015-6-20,2015,6,20,5,3afxip,electric motors Toronto,https://www.reddit.com/r/MachineLearning/comments/3afxip/electric_motors_toronto/,similura1,1434746172,,0,1
425,2015-6-20,2015,6,20,6,3ag1aj,"Collective Intelligence predicted the Super Bowl, Stanley Cup, and NBA champions.",https://www.reddit.com/r/MachineLearning/comments/3ag1aj/collective_intelligence_predicted_the_super_bowl/,[deleted],1434748041,,0,1
426,2015-6-20,2015,6,20,6,3ag2un,This image was generate by a human,https://www.reddit.com/r/MachineLearning/comments/3ag2un/this_image_was_generate_by_a_human/,[deleted],1434748751,,0,1
427,2015-6-20,2015,6,20,6,3ag314,This image was generated by a human ;-),https://www.reddit.com/r/MachineLearning/comments/3ag314/this_image_was_generated_by_a_human/,samim23,1434748838,,3,0
428,2015-6-20,2015,6,20,7,3ag83k,Incremental search expansion: a key to scalable pattern discovery?,https://www.reddit.com/r/MachineLearning/comments/3ag83k/incremental_search_expansion_a_key_to_scalable/,bkaz,1434751293,"Most current ML methods are statistical: they dont compare individual inputs one-to-one. Rather, they perform many-to-one weighted input or input-match summation for whole sequences or receptive fields of inputs. Such inherent coarseness creates two major problems: low unsupervised scalability or very long training necessary for ANNs, and artifacts of recognizing things that obviously arent there.

To resolve these issues, I propose a strictly incremental search. Incremental means that comparison must start with minimal-complexity inputs: pixels, and expand with minimal increments in cost of search: distance and derivation. Both increase complexity of discovered patterns, which are then selected as higher-level inputs. The first level of such expansion must be gray-scale cross-comparison between consecutive pixels within horizontal scan line, - something no one else is starting with. 

Each level is increment in cost of search projected to discover maximal additive match, where match is defined as lossless compression. Its a combination of lossless and lossy compression: comparison forms losslessly compressed patterns, and such compression is the criterion for lossy selection of these patterns as a higher-level inputs. This combined compression should make for scalable search: the quantity of inputs on higher levels will decrease maximally faster than their predictive value.

This is a form of deep learning / hierarchical clustering, increasingly fuzzy on higher levels. But actual implementation: www.cognitivealgorithm.info is very different from anything Ive come across. 
Please let me know if I am missing something.",10,0
429,2015-6-20,2015,6,20,7,3agb9a,Learning a sequence embedding using similarities?,https://www.reddit.com/r/MachineLearning/comments/3agb9a/learning_a_sequence_embedding_using_similarities/,cypherx,1434752863,"I have a collection of several thousand sequences and a (sparse) matrix of similarities between those sequences. I would like to learn a vector representation for the sequences which preserves similarities, so that the `distance(embed(seq1), embed(seq2))` is inversely proportional to `similarity(seq1, seq2)`. 

Does anyone have suggestions for a good technique to accomplish this? I was thinking of trying to use a RNN which scans over both sequences but I'm not sure how to tie the weights of the two inputs or even what the objective function should be. 

Clarifications:

* The sparsity of the similarities is due to missing/insufficient data, not actual known dissimilarity. 
* I want to get vector representations for sequences beyond the training dataset. (Is ""embedding"" the wrong word to use here?)",4,2
430,2015-6-20,2015,6,20,7,3agbm4,Miguel Nicolelis and Ronald Cicurel: The Singularity Isnt Near and the Brain Cant Be Simulated,https://www.reddit.com/r/MachineLearning/comments/3agbm4/miguel_nicolelis_and_ronald_cicurel_the/,fariax,1434753039,,7,0
431,2015-6-20,2015,6,20,7,3agcxd,Nueral networks as bachelor thesis,https://www.reddit.com/r/MachineLearning/comments/3agcxd/nueral_networks_as_bachelor_thesis/,[deleted],1434753750,"I need a little bit of advice. One of the themes for my bachelor thesis is using image recognition using neural networks.

I barely passed Signals and Systems and basics of Artificial Inteligence. But also I am deeply interested in neural networks and genetic algorithms. But right now I know absolute basics, I only dream that I could learn it and use it.

I am inspired by posts like this 
http://karpathy.github.io/2015/05/21/rnn-effectiveness/
http://googleresearch.blogspot.cz/2015/06/inceptionism-going-deeper-into-neural.html

..but the thing is - I don't know if I can do it.
I suck at math, I have problems with programming optimal algorithms, I tend to do things the hard way. 


I feel like this stuff is out of my league, that only geniuses from Google can do it.

Should I even bother?",6,0
432,2015-6-20,2015,6,20,8,3agkw3,What's a business idea that should be easily doable considering how far machine learning has come in the last few years?,https://www.reddit.com/r/MachineLearning/comments/3agkw3/whats_a_business_idea_that_should_be_easily/,[deleted],1434758143,"I was throwing around some ideas with my friends. Would be interested what some of you have in mind.

Our ideas:

* Automatic Summary of all news sources, and gathering similar ones together (i.e all Galaxy S6 reviews would be automatically found through RSS and put together)

* Preference learning RSS reader

* Predict how much karma your reddit post will get

Stuff like that. What's something easy that no one has monetized yet?",1,0
433,2015-6-20,2015,6,20,9,3agm41,How exactly are the Google Neural Network images created?,https://www.reddit.com/r/MachineLearning/comments/3agm41/how_exactly_are_the_google_neural_network_images/,ralf_,1434758832,"How exactly are the images made? There are some phrases used in the Guardian article like ""emphasise the feature it recognises"" or ""feedback the image back into the neural network"" which sound very lofty. For example:

&gt; Say you want to know what sort of image would result in banana. Start with an image full of random noise, then gradually tweak the image towards what the neural net considers a banana.

Ok, that is the broad concept. But I am interested in the details: how did they ""tweak the image"" of noise? Randomly changing pixels?",17,8
434,2015-6-20,2015,6,20,9,3agq5y,deepy: Highly extensible deep learning framework based on Theano,https://www.reddit.com/r/MachineLearning/comments/3agq5y/deepy_highly_extensible_deep_learning_framework/,[deleted],1434761267,,0,1
435,2015-6-20,2015,6,20,10,3agrk9,deepy: Highly extensible deep learning framework based on Theano,https://www.reddit.com/r/MachineLearning/comments/3agrk9/deepy_highly_extensible_deep_learning_framework/,fariax,1434762101,,20,27
436,2015-6-20,2015,6,20,11,3agz7p,What dataset should I use for image classification?,https://www.reddit.com/r/MachineLearning/comments/3agz7p/what_dataset_should_i_use_for_image_classification/,Phooey138,1434766794,"I can find tons of them, but I don't know which to use so that my results are as meaningful as possible to other people (and myself). Are there datasets used for popular competitions, or used in a lot of research, so that results can easily be compared? ",5,5
437,2015-6-20,2015,6,20,16,3ahox5,What is Apache Spark? || How is it different from Hadoop? || Setting up your first Spark Cluster,https://www.reddit.com/r/MachineLearning/comments/3ahox5/what_is_apache_spark_how_is_it_different_from/,YahooGuys,1434785290,,0,0
438,2015-6-20,2015,6,20,18,3ahumj,Has inceptionism been applied to speech?,https://www.reddit.com/r/MachineLearning/comments/3ahumj/has_inceptionism_been_applied_to_speech/,Stabbles,1434791148,"Google's blog post on inceptionism (http://googleresearch.blogspot.cz/2015/06/inceptionism-going-deeper-into-neural.html) is rather popular now. But I figured other people are working on similar things as well, and this made me curious wether this technique has been applied to neural nets for speech recognition. Does anyone know if this has been tried before?",3,0
439,2015-6-21,2015,6,21,0,3ailr7,"Suddenly, a leopard print sofa appears",https://www.reddit.com/r/MachineLearning/comments/3ailr7/suddenly_a_leopard_print_sofa_appears/,[deleted],1434813767,,0,1
440,2015-6-21,2015,6,21,0,3ailzi,"Suddenly, a leopard print sofa appears",https://www.reddit.com/r/MachineLearning/comments/3ailzi/suddenly_a_leopard_print_sofa_appears/,yogthos,1434813888,,46,140
441,2015-6-21,2015,6,21,0,3aip7u,State of the art in relation extraction in the wild,https://www.reddit.com/r/MachineLearning/comments/3aip7u/state_of_the_art_in_relation_extraction_in_the/,helshowk,1434815803,What methods/implementations have people found to work the best in the wild for relation extraction?  I've tried a handful out without too much success and this recent Google Research blog [post] (http://googleresearch.blogspot.com/2015/06/a-multilingual-corpus-of-automatically.html) seems to claim that this problem is sort of solved for English text but I'm not entirely convinced.,4,1
442,2015-6-21,2015,6,21,2,3aiw4q,Baum-Welch algorithm,https://www.reddit.com/r/MachineLearning/comments/3aiw4q/baumwelch_algorithm/,Rexmagi,1434819731,"I have been designing an machine learning application for a research project I am working on and i have little to no experience working with hidden Markov model. I have been trying to understand how the Baum-Welch algorithm works so I can build the application and was wondering if the wikipedia article on the subject is any good because it has been the clearest explanation i could find. (https://en.wikipedia.org/wiki/BaumWelch_algorithm)
if it how do I do the update step for the emission set if it a continuously random variable and not a discretely random variable.
if you think you are able to very clearly describe the algorithm feel free to post your thoughts. ",3,3
443,2015-6-21,2015,6,21,3,3aj2kv,Neural Network: Implementing a Split layer?,https://www.reddit.com/r/MachineLearning/comments/3aj2kv/neural_network_implementing_a_split_layer/,RossoFiorentino,1434823232,"I am working on my own neural network package. I am trying to implement a split layer which splits the output of the previous layer by duplication into multiple outputs on top of which other neural networks are constructed. Such a network looks something like this:

https://docs.google.com/drawings/d/1HR89pkT_FeydjUMknCXXOxiKCnVm7VDHGh-m5UPgRkE/pub?w=432&amp;h=442

In the forward step the input to the split layer is simply duplicated. In the backwards step i sum the delta values of the top layers to create the new input delta value for the split layer.

I think this is the correct way to implement a split layer but i am not sure? Does anyone have any experience implementing such a layer?",3,1
444,2015-6-21,2015,6,21,4,3ajcwh,Followup to Sethbling's video of MarI/O playing Super Mario,https://www.reddit.com/r/MachineLearning/comments/3ajcwh/followup_to_sethblings_video_of_mario_playing/,mattsains,1434828763,,7,39
445,2015-6-21,2015,6,21,4,3ajdcw,Utilitarian Learning in virtual environments?,https://www.reddit.com/r/MachineLearning/comments/3ajdcw/utilitarian_learning_in_virtual_environments/,jcannell,1434828996,"EDIT: added TLDR

TLDR summary: using a video game graphics engine as the generative model to train a CNN vision system.  The CNN would be trained to output the high level compact scene descriptor: the camera pose params, and perhaps a small list of a few visible dynamic objects with their own pose params.  Has it been done?

Supervised learning works very well when: 1.) the training objective actually measures/captures our true intentions for the network's future functionality and utility and 2.) the training data distribution is close to the production data distribution (which can be very different than the test distribution, although this problem is not specific to supervised learning).

Training a deepnet on Imagenet using standard SL results in systems that are really good at the Imagenet benchmark, but this does not directly translate into very general and robust human-level visual performance, partly because of issue #1.

Unsupervised learning is not a panacea - it treats every pattern as equally important and can easily waste huge amounts of computation attempting to learn unimportant noisy details.

In theory reinforcement learning is a solution - as long as one can translate intended functionality into an appropriate utility function.

However in practice just using RL for everything is impractical: there are training complexities due to sparse/infrequent rewards, and having to train every component together is undesirable.

There are huge advantages to independent modular training: ie vision and other pattern recog systems can be trained on their own datasets, and once perfected those systems can then be integrated as building blocks in larger agents/systems - possibly with retraining.

So how can we bridge the gap between RL and SL?  The idea is we want to design an objective function such that high performance closely predicts high system utility when later used for various applications.

To make this idea more concrete, consider the case of a self-driving car.  You could start with a vision system trained on Imagenet and equivalent benchmarks for depth/motion estimation, but the imagenet categories do not really match the discriminations important for the driving task.

So my modest proposal: use a video game engine as the generative model; use a compressed high level 3D scene descriptor as the training criteria for the vision system.

As Hinton said - vision is the inverse of graphics.  Video game engines are very powerful generative models; an enormous amount of work has gone into carving reality at the joints in just the right way, using all the clever approximations, etc.  

So we know, *a priori*, that a (suitably compressed) version of the high level 3D scene descriptor used in a photoreal graphics engine is more or less a good generic goal representation for vision: because we have a generative model that can render photoreal images starting from that compact descriptor (and using reasonable amounts of computation).  This can automatically prevent the network from learning unimportant noisy details - as those are generated procedurally from stochastic noise during rendering.

Now I realize there is a body of existing work on learning generative models, but this would be different - using an existing graphics engine as the generative model.

Domain adversarial training could be used (if necessary) to force the network to generalize well between virtual and real images.

Has anyone trained a CNN like this?  This is an avenue of research I am interested in exploring.  I realize that it may require large amounts of compute and or data - but of course using a video game engine also more or less completely solves the data issue.",8,4
446,2015-6-21,2015,6,21,6,3ajmel,"Theoretical Soundness of ""Scheduled Sampling"" Paper",https://www.reddit.com/r/MachineLearning/comments/3ajmel/theoretical_soundness_of_scheduled_sampling_paper/,alexmlamb,1434834120,"Hello, 

The recent paper on ""Scheduled Sampling"" with RNNs showed strong empirical results, but I'm not sure if it's theoretically sound.  

In short, the usual way to train a probabilistic RNN is to always use observed inputs at train time and then use samples from the network as inputs at test time.  This paper considers using samples from the network as inputs at test time with a probability that increases as training progresses.  They justify this as a curriculum learning approach, as learning with the network's samples as input is harder than using the observed values.  

However, I think that the method is wrong, at least if one wants to maintain a probabilistic interpretation of the model.  I'll first present an example of where it would fail and then I'll provide an explanation.  

Suppose the first element in a sequence is drawn from a uniform distribution U(0,100).  Each following element is equal to the last element minus one.  Now suppose we let the RNN use its own samples as input for the first 10 steps and uses observed values for the next 10 steps.  Since the model hasn't seen any of the observed sequence yet, it basically has to guess the value that the sequence will start at.  After the first 10 steps, it starts receiving input from the true distribution, which could be completely inconsistent with what the model sampled.  So to give a more concrete example, the samples could be 20,19,18,17,16,15, and then the observed values following it could be 100,99,98,97,96,95.  This means that the model is seeing a historical sequence that could not be drawn from the data distribution.  Essentially it means that the model is learning the wrong conditional distribution, even late in training.  

I think that one should pick a point in the series, and use observed values before that point and use sampled values after that point.  I think that this method is more sound.  ",5,1
447,2015-6-21,2015,6,21,7,3ajsmv,Is videolectures.net gone?,https://www.reddit.com/r/MachineLearning/comments/3ajsmv/is_videolecturesnet_gone/,[deleted],1434837674,"Apologies; this is not necessarily a machine learning topic, but http://videolectures.net/ had an epic number of machine learning videos from conferences and such. Is it permanently down? Is there a descent alternative or mirror?",5,7
448,2015-6-21,2015,6,21,8,3ak0od,Fei Fei Li (http://vision.stanford.edu/) on teaching vision to computers.,https://www.reddit.com/r/MachineLearning/comments/3ak0od/fei_fei_li_httpvisionstanfordedu_on_teaching/,ArgonJargon,1434842487,,0,0
449,2015-6-21,2015,6,21,8,3ak2ip,MNIST Digits for Node.js,https://www.reddit.com/r/MachineLearning/comments/3ak2ip/mnist_digits_for_nodejs/,cazala,1434843564,,3,0
450,2015-6-21,2015,6,21,10,3akbii,"Best Big Data, Data Science, Data Mining, and Machine Learning podcasts",https://www.reddit.com/r/MachineLearning/comments/3akbii/best_big_data_data_science_data_mining_and/,Paige_Roberts,1434849092,,0,0
451,2015-6-21,2015,6,21,12,3aknmj,Is there a chessbot that works by learning not just calculating all the possible moves?,https://www.reddit.com/r/MachineLearning/comments/3aknmj/is_there_a_chessbot_that_works_by_learning_not/,thebluehedgehog,1434857008,"Is there a chess AI that learn(s)(ed) via a database or playing against itself, bots, or humans, instead of using weights and relying upon calculating all possible outcomes and picking the best route by probability?",12,6
452,2015-6-21,2015,6,21,18,3ald6z,blackberry and android became friends in new mobile.,https://www.reddit.com/r/MachineLearning/comments/3ald6z/blackberry_and_android_became_friends_in_new/,omkanth,1434879894,,1,1
453,2015-6-21,2015,6,21,18,3alddt,How do you log your experiments...,https://www.reddit.com/r/MachineLearning/comments/3alddt/how_do_you_log_your_experiments/,tareumlaneuchie,1434880127,"I used to program algorithms in computational geometry many years ago. Back then, an iteration on hand made test cases would take a few minutes at the most.

With ML however, this is an entirely different best. Even if you reduced the number of classes in your training set (for example, 3 digits instead of 10), the iteration still take a fair amount of time.

I understand that one need to tinker with his model, perhaps try a different toolkit (Pybrain, scikit-learn, Theano) before getting the design of the NN right. On top of that, since this is a hobby sitting on top of a full time job and 3 kids, I can't really devote as much time as I would like to... So I have to be very efficient.

My question is thus as follow: how do you keep track of all the changes in parameters during runs? Would a simple logbook do? Are there any particular hints that you can share on reaching an optimal design for the NN besides brute force trial and error?",25,13
454,2015-6-21,2015,6,21,20,3alih5,What's wrong with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3alih5/whats_wrong_with_deep_learning/,alexeyr,1434886296,,13,68
455,2015-6-21,2015,6,21,21,3alns2,How to create those Google ANN image recognition feedback images?,https://www.reddit.com/r/MachineLearning/comments/3alns2/how_to_create_those_google_ann_image_recognition/,curious_scourge,1434891101,"I'm a seasoned programmer, but haven't ever worked with NNs.

What github/code repo could I start off with, to try train a neural network on image recognition, so that I can then try create those cool Googly eyed feedback images?

",4,0
456,2015-6-21,2015,6,21,22,3alr08,drawCNN - a cool tool to get insight,https://www.reddit.com/r/MachineLearning/comments/3alr08/drawcnn_a_cool_tool_to_get_insight/,fimari,1434893715,,4,28
457,2015-6-21,2015,6,21,22,3als8s,IBM Pours Researchers(3500) And Resources Into Apache Spark Project,https://www.reddit.com/r/MachineLearning/comments/3als8s/ibm_pours_researchers3500_and_resources_into/,InaneMembrane,1434894707,,0,1
458,2015-6-21,2015,6,21,23,3alt3k,"In ML, how do you prove you know what you're doing to non-technical people?",https://www.reddit.com/r/MachineLearning/comments/3alt3k/in_ml_how_do_you_prove_you_know_what_youre_doing/,robinhoode,1434895346,"Machine learning is a topic I keep coming back to when I think about leaving behind the Ruby on Rails world for greener pastures. 

I would really like to have a game plan to break into ML.

With Rails, I simply had to show a functioning website and my non-technical managers would have an understanding of my abilities without knowing the details.

With ML, I have a much harder time understanding how to show my skills to non-techs. The project ideas I see propose are very localized, like analyzing rain patterns or hand-written digits. The projects don't seem to generalize well into a full-blown money-making product.

How do I overcome this?",9,0
459,2015-6-22,2015,6,22,0,3alzfk,Debugging Neural Network for (Natural Language) Tagging,https://www.reddit.com/r/MachineLearning/comments/3alzfk/debugging_neural_network_for_natural_language/,nat47,1434899440,"I've been coding a Neural Network for recognizing and tagging parts of speech in English (written in Java). The code itself has no 'errors' or apparent flaws. Nevertheless, it is not learning -- the more I train it does not change its ability to predict the testing data. The following is information about what I've done, please ask me to update this post if I left something important out.

I wrote the neural network and tested it on several different problems to make sure that the network itself worked. I trained it to learn how to double numbers, XOR, cube numbers, and learn the sin function to a decent accuracy. So, I'm fairly confident that the actual algorithm is working.

The network using using the sigmoid activation function. The learning rate is .3, Momentum is .6. The weights are initialized to rng.nextFloat() -.5) * 4

I then got the Brown Corpus data-set and simplified the tagset to 'universal' with NLTK. I used NLTK for generating and saving all the corpus and dictionary data. I cut the last 15,000 sentences out of the corpus for testing purposes. I used the rest of the corpus (about 40,000 sentences of tagged words) for training.

The neural network layout is as follows: There is an input neuron for each Tag. Output Layer: There is one output neuron for each tag. The network is taking inputs for 3 words: first: the word coming before the word we want to tag, second: the word that needs to be tagged, third: the word that follows the second word. So, total number of inputs are 3x(total number of possible tags). The input values are numbers between 0 and 1. Each of the 3 words being fed into the input layer is searched for in a dictionary (made up by the 40,000 corpus, the same corpus that is used for training). The dictionary holds the number of times that each word has been tagged in the corpus as what part of speech.

    For instance, the word 'cover' is tagged as a noun 1 time and a verb 3 times.

Percentages of being tagged are computed for each part of speech that the word is associated as, and this is what is fed into the network for that particular word. So, the input neuron designated as NOUN would receive .33 and VERB would receive .66. The other input neurons that hold tags for that word receive an input of 0.0. This is done for each of the 3 words to be inputted. If a word is the first word of a sentence, the first group of tags are all 0. If a word is the last word of a sentence, the final group of input neurons that hold the tag probabilities for the following word are left as 0s. I've been using 10 hidden nodes (I've read a number of papers and this seems to be a good place to start testing with)

None of the 15,000 testing sentences were used to make the 'dictionary.' So, when testing the network with this partial corpus there will be some words the network has never seen. Words that are not recognized have their suffix stripped, and their suffix is searched for in another 'dictionary.' Whatever is most probable for that word is then used as inputs for it.

This is my set-up, and I started trying to train the network. I've been training the network with all 40,000 sentences. 1 epoch = 1 forward and backpropagation of every word in each sentence of the 40,000 training-set. So, just doing 1 epoch takes quite a few seconds. Just by knowing the word probabilities the network did pretty well, but the more I train it, nothing happens. The numbers that follow the epochs are the number of correctly tagged words divided by the total number of words.

First run 50 epochs: 0.928218786

100 epochs: 0.933130661

500 epochs: 0.928614499 took around 30 minutes to train this

Tried 10 epochs: 0.928953683

Using only 1 epoch had results that pretty much varied between .92 and .93

So, it doesn't appear to be working...

I then took 55 sentences from the corpus and used the same dictionary that had probabilities for all 40,000 words. For this one, I trained it in the same way I trained my XOR -- I only used those 55 sentences and I only tested the trained network weights on those 55 sentences. The network was able to learn those 55 sentences quite easily. With 120 epochs (taking a couple seconds) the network went from tagging 3768 incorrectly and 56 correctly (on the first few epochs) to tagging 3772 correctly and 52 incorrectly on the 120th epoch.

This is where I'm at, I've been trying to debug this for over a day now, and haven't figured anything out.
",6,2
460,2015-6-22,2015,6,22,1,3am6de,Basic Machine Learning Instagram Bot,https://www.reddit.com/r/MachineLearning/comments/3am6de/basic_machine_learning_instagram_bot/,TehDing,1434903367,,0,20
461,2015-6-22,2015,6,22,1,3ama7l,Coding loops on a neural network (probably a silly question),https://www.reddit.com/r/MachineLearning/comments/3ama7l/coding_loops_on_a_neural_network_probably_a_silly/,DarkAnHell,1434905501,"So i'm trying to make my own library for NN in Go, just because i'm bored and i wanted to try out the language. Probably not the best for this type of things, but that's not the case. Just remarking it so we can get over the *there are things like that (and better) already* :P.

The thing is:
I've seen some references, comments, etc. to special connections between neurons which basically form a loop. So, the ""signal"" of the neuron goes backwards to itself or to a previous neuron.

How do you code that?
I mean, if the signal goes back everytime it passes through that neuron, do you just store the value for the next pass? or is there another way?

It's probablly just that, but i wanted to make sure about it before coding aything :)",3,0
462,2015-6-22,2015,6,22,1,3ama7v,Evolutionary development of fully connected NNs.,https://www.reddit.com/r/MachineLearning/comments/3ama7v/evolutionary_development_of_fully_connected_nns/,Hexorg,1434905506,"I've been wanting to create an evolutionary growth of NNs where they start as fully connected, and the evolution can then create convolution layers and feedback loops. I start with a network that looks like this: http://i.imgur.com/ZxwVoRQ.png (Just a K_n_ graph) where green nodes are inputs, red nodes are outputs, and grey nodes are internal layers. My hypothesis was that the evolutionary algorithms could figure out on their own whether to use convolutions or feed-back loops. The way I achieve feedback is to recalculate new state of the network for all but input neurons using previous state as an input to the network, repeat as many times as there are non-input neurons. All outputs are bounded to [0,1] range by a sigmoid function. Evolition uses cross-over and random mutations, with a direct genotype-&gt;phenotype encoding (there is no ""junk DNA"" and every chromosome is directly writing a single neuron weight).

I've applied this network to a simple move-tank's-threads simulation  where one output controls the speed of left thread, one controls the speed on right thread, and the input is the x,y coordinates of a vector towards the closest food. 

I'm really not getting good results. I'm not even getting OK results. After 1000 generations some of my tanks are still spinning in circles. 

So I wanted to get some advices. How can I improve the performance? Is my search space is just too big? Is indirect DNA encoding worth a shot (get some junk DNA in there)? Maybe some other ideas?",1,1
463,2015-6-22,2015,6,22,4,3amqk6,A question on word-splitting (NLP),https://www.reddit.com/r/MachineLearning/comments/3amqk6/a_question_on_wordsplitting_nlp/,[deleted],1434914174,"I'm trying to figure out what's the right way to answer this question.
Suppose you're given a word like ""therapist"". It can be split into ""the rapist"". The question is: given such a word (which can be split up), how do you decide if it *should* be split up or not? Say you have aggregate statistics about distributions of these words: P(""the""), P(""rapist""), P(""therapist""), P(""rapist|the""), etc. How would you make a principled decision, and what information would need to make such a decision?

[X-posted from /r/askstatistics, as this forum might be more appropriate]

Added later: as it so happens, I just stumbled upon Peter Norvig's great article about this exact problem: http://norvig.com/ngrams/ch14.pdf  . I'm going through it right now. Highly recommended.",2,0
464,2015-6-22,2015,6,22,4,3amqqs,"Fuck Reddit, Fuck r/machinelearning and Fuck All of You",https://www.reddit.com/r/MachineLearning/comments/3amqqs/fuck_reddit_fuck_rmachinelearning_and_fuck_all_of/,sixwings,1434914265,,9,0
465,2015-6-22,2015,6,22,5,3an007,We have stumbled in the era of machine psychology,https://www.reddit.com/r/MachineLearning/comments/3an007/we_have_stumbled_in_the_era_of_machine_psychology/,suhrob,1434919141,,18,45
466,2015-6-22,2015,6,22,6,3an4l9,MarI/O - Machine Learning for Video Games,https://www.reddit.com/r/MachineLearning/comments/3an4l9/mario_machine_learning_for_video_games/,xyz0i0j0k,1434921583,,0,2
467,2015-6-22,2015,6,22,6,3an61n,"""Teaching machines to read and comprehend"" paper by Deepmind",https://www.reddit.com/r/MachineLearning/comments/3an61n/teaching_machines_to_read_and_comprehend_paper_by/,[deleted],1434922371,,0,1
468,2015-6-22,2015,6,22,6,3an64s,"""Teaching Machines to Read and Comprehend"" paper by Deepmind",https://www.reddit.com/r/MachineLearning/comments/3an64s/teaching_machines_to_read_and_comprehend_paper_by/,[deleted],1434922432,,1,4
469,2015-6-22,2015,6,22,7,3ane7s,What do you do when your model is training?,https://www.reddit.com/r/MachineLearning/comments/3ane7s/what_do_you_do_when_your_model_is_training/,invert3r,1434926707,"It takes a few minutes to an hour to train a model on a large dataset on a cluster. I need to tune parameters and tweak features and experiment with different models. What do you do when your model is training, looking for some productivity tips. ",7,0
470,2015-6-22,2015,6,22,11,3anzmw,RNN Jay-Z likes your body,https://www.reddit.com/r/MachineLearning/comments/3anzmw/rnn_jayz_likes_your_body/,CarbonAvatar,1434938786,,7,10
471,2015-6-22,2015,6,22,15,3aom6d,Global Liquid Filling Machine Industry 2015 Deep Market Research Report,https://www.reddit.com/r/MachineLearning/comments/3aom6d/global_liquid_filling_machine_industry_2015_deep/,janiebmr,1434952881,"For overview analysis, the report introduces Liquid Filling Machine basic information including definition, classification, application, industry chain structure, industry overview, policy analysis, and news analysis, etc.For international and China market analysis, the report analyzes Liquid Filling Machine markets in China and other countries or regions (such as US, Europe, Japan, etc) by presenting research on global products of different types and applications, developments and trends of market, technology, competitive landscape, and leading suppliers and countries 2010-2015 capacity, production, cost, price, profit, production value, and gross margin. For leading suppliers, related information is listed as products, customers, application, capacity, market position, and company contact information, etc. 2015-2020 forecast on capacity, production, cost, price, profit, production value, and gross margin for these markets are also included.

Read more details at: http://www.bigmarketresearch.com/global-liquid-filling-machine-industry-2015-deep-research-report-market
",0,1
472,2015-6-22,2015,6,22,15,3aoo66,Automatically generated captions for Inceptionism images,https://www.reddit.com/r/MachineLearning/comments/3aoo66/automatically_generated_captions_for_inceptionism/,iori42,1434954393,,8,23
473,2015-6-22,2015,6,22,15,3aoq1i,Global Glass Fiber Textile Machine Industry 2015 Deep Market Research Report,https://www.reddit.com/r/MachineLearning/comments/3aoq1i/global_glass_fiber_textile_machine_industry_2015/,janiebmr,1434955977,"For overview analysis, the report introduces Glass fiber textile machine basic information including definition, classification, application, industry chain structure, industry overview, policy analysis, and news analysis, etc.For international and China market analysis, the report analyzes Glass fiber textile machine markets in China and other countries or regions (such as US, Europe, Japan, etc) by presenting research on global products of different types and applications, developments and trends of market, technology, and competitive landscape, and leading suppliers and countries 2009-2014 capacity, production, cost, price, profit, production value, and gross margin. For leading suppliers, related information is listed as products, customers, application, capacity, market position, and company contact information, etc. 2015-2020 forecast on capacity, production, cost, price, profit, production value, and gross margin for these markets are also included.

Read more details at: http://www.bigmarketresearch.com/global-glass-fiber-textile-machine-industry-2015-deep-research-report-market
",0,1
474,2015-6-22,2015,6,22,16,3aot90,need help in lua/torch and python,https://www.reddit.com/r/MachineLearning/comments/3aot90/need_help_in_luatorch_and_python/,tushar1408,1434958730,"Anyone here who is fluent in lua-torch and Python.
i want to convert following code snippet given in lua/torch into python


    local input = nn.Identity()()
    local c = nn.Linear(self.in_dim, self.mem_dim)(input)
    local h
    if self.gate_output then
    local o = nn.Sigmoid()(nn.Linear(self.in_dim, self.mem_dim)(input))
    h = nn.CMulTable(){o, nn.Tanh()(c)}
    else
    h = nn.Tanh()(c)
    end


thanks in advance.",2,0
475,2015-6-22,2015,6,22,18,3aoywo,Neural Network - Simple Music Composition,https://www.reddit.com/r/MachineLearning/comments/3aoywo/neural_network_simple_music_composition/,[deleted],1434964132,"So it's the summer before my high school junior year, I've got a LOT of free time and I'm very, very new to machine learning. I like music so I thought I might as well try to make a simple music composer to learn about neural networks. Sorry about the video quality, I'm not too great at that stuff. If anyone knows a good (and preferably easy) resource to read about recurrent neural networks, I would be very grateful :) ",0,1
476,2015-6-22,2015,6,22,21,3ape67,Dataset with size more than 1 TB available for FREE download,https://www.reddit.com/r/MachineLearning/comments/3ape67/dataset_with_size_more_than_1_tb_available_for/,john_philip,1434976642,,5,4
477,2015-6-22,2015,6,22,21,3ape77,Neural Net Inception?,https://www.reddit.com/r/MachineLearning/comments/3ape77/neural_net_inception/,DyingAdonis,1434976660,Would it be possible to have a neural net model another neural net (presumably modeling some other function)? Would it just learn the weights to make the final output and bypass the inner net? Am I talking nonsense?,3,1
478,2015-6-22,2015,6,22,22,3apgvn,Neural Conversation Model (Google's chatbot),https://www.reddit.com/r/MachineLearning/comments/3apgvn/neural_conversation_model_googles_chatbot/,muktabh,1434978336,,33,76
479,2015-6-22,2015,6,22,22,3apmdd,Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid),https://www.reddit.com/r/MachineLearning/comments/3apmdd/smarter_parameter_sweeps_or_why_grid_search_is/,D33B,1434981595,,15,32
480,2015-6-22,2015,6,22,23,3apn7d,Quick guide into Amazon EC2 cluster for Machine learning?,https://www.reddit.com/r/MachineLearning/comments/3apn7d/quick_guide_into_amazon_ec2_cluster_for_machine/,krimtheguy,1434982025,"Hi, I'm doing some research on some big, very big dataset (currently using about ~0.5% of that in my own computer, just for data wrangling)  and when it comes to the end, I will have to use the entire dataset to train and test some predictions.

Since the dataset is pretty big (~500GB) I will need some extra computation which I can't reach at home or at university (even asking for that, they wont let me use that amount of resources). That's why I thought about using amazon EC2, using a cluster and trying to parallelize the process. The problem in here is that I never used such a thing, and I don't want to have a bill of 2000 at the end of the month, so I prefer to ask before which options I have and if possible, to point me to some tutorial/explanation on how to do this

Thanks in advance",1,1
481,2015-6-22,2015,6,22,23,3apohw,Book: Machine Learning in Python: Essential Techniques for Predictive Analysis,https://www.reddit.com/r/MachineLearning/comments/3apohw/book_machine_learning_in_python_essential/,booketor,1434982702,,0,15
482,2015-6-22,2015,6,22,23,3apre2,Python for Image Understanding: Deep Learning with Convolutional Neural Nets | PyData London 2015,https://www.reddit.com/r/MachineLearning/comments/3apre2/python_for_image_understanding_deep_learning_with/,clbam8,1434984204,,0,1
483,2015-6-23,2015,6,23,0,3aq0e5,Using artificial intelligence to detect nudity,https://www.reddit.com/r/MachineLearning/comments/3aq0e5/using_artificial_intelligence_to_detect_nudity/,MajorDeeganz,1434988546,,3,0
484,2015-6-23,2015,6,23,1,3aq7ue,mini-batch gradient descent learning,https://www.reddit.com/r/MachineLearning/comments/3aq7ue/minibatch_gradient_descent_learning/,ghrarhg,1434991972,I am interested in tasks showing mini-batch gradient descent performing better than stoichastic gradient descent. Does anyone know of papers or studies that have shown this? Thank you!,5,0
485,2015-6-23,2015,6,23,2,3aqbp1,sklearn feature extraction using HashingVectorizer,https://www.reddit.com/r/MachineLearning/comments/3aqbp1/sklearn_feature_extraction_using_hashingvectorizer/,warriortux,1434993611,"Hello,

I am trying to work with sklearn, and I have the following code. I am trying to use the HashingVector on some dynamic data. I though the pair_wise metrics, would take the sparse features, but they do not. I have to convert them to numpy arrays? I did it and it seems to be working fine. Am I doing it the right way??

    from sklearn.feature_extraction.text import HashingVectorizer
    from sklearn.metrics.pairwise import paired_distances
    
    # No corpus specified here, will try to fit data later
    hv = HashingVectorizer(n_features=20, non_negative=True)
    # Two samples to fit
    t1=['this is a text']
    t2=['this is a test']
    # Text to vectors
    t1_vector = hv.fit_transform(t1)
    t2_vector = hv.fit_transform(t2)
    # Get cosine distance
    distance = paired_distances(t1_vector.toarray(), t2_vector.toarray(), metric='cosine')
    print(distance)

EDIT: If I am not using the .toarray(), the code throws a TypeError: matrix is not square",2,0
486,2015-6-23,2015,6,23,2,3aqcaa,Why does perceptron converge intuitively?,https://www.reddit.com/r/MachineLearning/comments/3aqcaa/why_does_perceptron_converge_intuitively/,techrat_reddit,1434993883,"I have been reading a couple of proofs of perceptron convergence, but I cannot seem to intuitively understand why perceptron converge on linearly separable data. Any ideas?",5,1
487,2015-6-23,2015,6,23,3,3aqkjz,Unknown future text dataset clustering + classification + topic modelling?,https://www.reddit.com/r/MachineLearning/comments/3aqkjz/unknown_future_text_dataset_clustering/,ml__throwaway,1434997639,"TL;DR Would like to cluster future content into (labeled or unlabeled) clusters/categories, label (if unlabeled) said clusters, and be able to do topic modelling. 

Forgive me if my question sounds stupid: (hence the throwaway account). 

How should one implement ""unsupervised learning"" text clustering followed by ""reinforced learning"" classification of said clusters, and topic modelling?
The reason why I want to go with manual (human) or automatic reinforced learning on top of an unsupervised algorithm is because I do not yet know what my data is to look like. 
What I want to do is let people post, cluster their posts based on similarity, label said clusters (topics/topic modelling?) and be able to generate better individual feeds for said user. Things should be dynamic, so if today a user likes posts that have topics such as  ""love"", ""nature"", or ""gym"", I'd like to be able to provide him with a data feed that matches his interests. 

I'm not sure how to best go about this: stop words + lemmatization + porter-stemmer + wordnet + tf-idf, followed by k-means clustering? 
Storing the K-Means clusters in db 
Assign 1(or more) label(s)/topic(s) to the clusters 
When user interacts with a post flagged as belonging to topic ""t5"" for example, increase userID-topic weight? 
Store/update user-topic weight, and using a probabilistic model, decide on the % of posts having ""t5"", % of posts having ""t6"" and retrieve accordingly?  

Posts also have hashtags, but hashtags can be inaccurate, so maybe I could use the hashtags to back propagate changes? We're now talking neural networks ... I don't have a lot of expertise in machine learning, so I'm not sure how to approach this.
I really want to implement something that makes sense, could scale well, could be integrated with my existing data store (mysql) (I plan on using youtube/vitess to scale), and doesn't sound too messed up.",6,0
488,2015-6-23,2015,6,23,3,3aqnxc,Question about matlab implementation of SVM / leave one out cross validation.,https://www.reddit.com/r/MachineLearning/comments/3aqnxc/question_about_matlab_implementation_of_svm_leave/,neurone214,1434999120,"Hey everyone. As usual, this is probably obvious but perhaps I'm too close to the problem to see what's going wrong here. 

I have a set of predictors and outcomes, rateDat and goal, respectively. I set up a partitioned model using the following: 

    mdl = fitcsvm(rateDat,goal,'CVPartition',c,'KernelFunction','Polynomial','PolynomialOrder',po, ...
        'KernelScale',ks,'BoxConstraint',bc,'KKTTolerance',kkt);

Then I check the cross validation accuracy with the partition scheme given by c (which is leave one out) 

    kfoldLoss(mdl,'mode','individual') 

The loss for each example in the data set is 0. 

Now, I try something slightly different. Rather than using cvpartition to generate a partitioning scheme, I fit individual models using all data but one example (i.e., manual leave one out), then check for each model whether it accurately predicts the one that's left out.  The accuracy is quite poor despite using the same model parameters. 

    i = 1:length(goal)
        selVector = false(length(goal),1);
        selVector(i) = true; 
        mdl =   fitcsvm(rateDat(~selVector,:),goal(~selVector),'KernelFunction','Polynomial','PolynomialOrder',po, ...
            'KernelScale',ks,'BoxConstraint',bc,'KKTTolerance',kkt); %note here all trials but one are used to estimate the model. 
        prediction = predict(mdl,rateDat(selVector,:)); %now we look to see what the predicted class is for the trial left out. 
        classAcc(i) = goal(selVector) == prediction; %and see if it matches our class def. goal and prediction should equate given how goal is   defined. 
    end


So, again, when I do this, the classification accuracy is abysmal.  Am I missing something here about how use of the 'CVPartition' option works? ",0,0
489,2015-6-23,2015,6,23,5,3aqze0,Flying faster with Twitter Heron,https://www.reddit.com/r/MachineLearning/comments/3aqze0/flying_faster_with_twitter_heron/,[deleted],1435004217,,0,0
490,2015-6-23,2015,6,23,5,3ar221,Google DeepMind demo does not seem to learn. Any ideas what's wrong?,https://www.reddit.com/r/MachineLearning/comments/3ar221/google_deepmind_demo_does_not_seem_to_learn_any/,chaddjohnson,1435005371,"Hey guys,

I've been following Google's progress with DeepMind and saw [this](http://www.reddit.com/r/MachineLearning/comments/2xhe36/running_deepminds_atari_ai_on_a_homepc/) thread, so I downloaded the source and ran it on my Titan Z in an effort to learn to play Breakout. However, after running the agent for two to three days straight, it seems to play no more intelligently than an untrained version.

Here are the steps I took ([source](http://superintelligence.ch/deepmind/)):

1. Cloned https://github.com/soumith/deepmind-atari. I'm using this fork because it provides a fix for compatibility with changes to the Torch nn library.
1. Changed `../torch/bin/luajit train_agent.lua $args` to `../torch/bin/qlua train_agent.lua $args` in run_gpu.
1. Changed `display=false` to `display=true` in `torch/share/lua/5.1/alewrap/AleEnv.lua`
1. Ran `./run_gpu breakout` for two to three days. This produced the network file dqn/DQN3_0_1_breakout_FULL_Y.t7.
1. Changed `netfile=""\""convnet_atari3\""""` in run_gpu to `netfile=""\""DQN3_0_1_breakout_FULL_Y.t7\""""`.

My machine reached approximately 12 million steps. Maybe this is not enough?

I'm concerned that the changes introduced with https://github.com/soumith/deepmind-atari/commit/88fffeaa04724d1b34f2586f8fc08ebc44256cb0 may have broken this demo. Is this at all possible? Does anyone have any suggestions on what might be wrong? I greatly appreciate any input and would really like to get this working!

**EDIT**: I figured out how to make this work. Basically I reverted change 88fffea (in deepmind-atari) and then used forked versions of cunn, cutorch, torch7, and nn. I then reverted changes to these dependencies, removing changes after April 6, 2015, as change 88fffea (in deepmind-atari) occurred on April 7, 2015, and I modified install_dependencies.sh to install the dependencies from local directories. The simulation now learns as it should. I'm not sure if the issue was due to change 88fffea or a change in one of the dependencies I forked.",7,16
491,2015-6-23,2015,6,23,5,3ar3ks,"Recognising confusing categories, fx. unicode glyphs?",https://www.reddit.com/r/MachineLearning/comments/3ar3ks/recognising_confusing_categories_fx_unicode_glyphs/,walling,1435006048,"I have been thinking about this problem for some time, and maybe you can help me. Let's say I am training a model to classify unicode characters. I could possibly run into some problems, because many glyphs look similar. What is this phenomena called, and what can you do to detect it? Maybe I don't have to do anything for the training data and a CNN would actually output categories like ""1"" 30%, ""I"" 25%, ""l"" 24%, etc. I might just have to try it.",1,0
492,2015-6-23,2015,6,23,5,3ar40i,How to teach a machine to recognize human emotion,https://www.reddit.com/r/MachineLearning/comments/3ar40i/how_to_teach_a_machine_to_recognize_human_emotion/,ahamino,1435006250,,0,0
493,2015-6-23,2015,6,23,7,3arekv,Please help me understand Non-Parametric Modeling,https://www.reddit.com/r/MachineLearning/comments/3arekv/please_help_me_understand_nonparametric_modeling/,Afritus,1435011044,"Hi guys,

I have problems to fully understand the concept (and functionality) of non-parametric modeling, where only the data is given.

According to the information I have found so far, the goal is to estimate a density function (is this correct?). It does so by using different methods such as calculating a histogram, which can't be continuous, though. Therefore, there was another method presented in our lecture - the concept of Kernel density estimation. Moreover, also Dirac delta functions were mentioned in this context.
Unfortunately, I am not studying math and thus have some difficulties to understand how the Kernel density estimation really works. Does anyone know an intuitive explanation on this topic?

&amp;nbsp;

Also, are following concepts considered being part of non-parametric modeling:

- EM algorithm and k-Means (I think no, because we introduce parameters)
- k-NN (I think yes, because we are working without parameters here)

&amp;nbsp;

Regards and thanks for your effort!
",2,1
494,2015-6-23,2015,6,23,12,3ascxs,How are Evolutionary Algorithms different than Learning Algorithms?,https://www.reddit.com/r/MachineLearning/comments/3ascxs/how_are_evolutionary_algorithms_different_than/,ScientiaOmniaVincit,1435028628,"Most of the concepts in Learning Algorithms are very similar to the Evolutionary Algorithms. Is there a limit where we can say we are doing ""learning"" instead of ""evolution""? ",13,4
495,2015-6-23,2015,6,23,13,3askpq,"Great new interview with Geoffrey Hinton (U of T Magazine, June 2015)",https://www.reddit.com/r/MachineLearning/comments/3askpq/great_new_interview_with_geoffrey_hinton_u_of_t/,vaginitischlamydia,1435032941,,3,70
496,2015-6-23,2015,6,23,13,3asooy,"[1506.06724] Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books [UofT, MIT]",https://www.reddit.com/r/MachineLearning/comments/3asooy/150606724_aligning_books_and_movies_towards/,g4n0n,1435035363,,0,12
497,2015-6-23,2015,6,23,14,3asqxv,[1506.06714] A Neural Network Approach to Context-Sensitive Generation of Conversational Responses,https://www.reddit.com/r/MachineLearning/comments/3asqxv/150606714_a_neural_network_approach_to/,g4n0n,1435036808,,7,9
498,2015-6-23,2015,6,23,14,3asto8,Image generation with Adversarial Networks in your browser,https://www.reddit.com/r/MachineLearning/comments/3asto8/image_generation_with_adversarial_networks_in/,iori42,1435038723,,18,25
499,2015-6-23,2015,6,23,14,3astuv,Decision Tree with with multiple options,https://www.reddit.com/r/MachineLearning/comments/3astuv/decision_tree_with_with_multiple_options/,isolar89,1435038878,"Hey all,
Is it possible to do a decision tree that instead of a binary breakdown have 4 options?
IE. Instead of branching in 1 or 0 have it branch in 1,2,3,4?

I'm using the tree() function in R and I haven't seen anything like it and can't figure out why not.

Thanks
",3,1
500,2015-6-23,2015,6,23,16,3at10u,Introduction to ML - Decision Trees,https://www.reddit.com/r/MachineLearning/comments/3at10u/introduction_to_ml_decision_trees/,aranag,1435044430,,2,6
501,2015-6-23,2015,6,23,16,3at1jl,What are good international or national (India) journals for Data Mining?,https://www.reddit.com/r/MachineLearning/comments/3at1jl/what_are_good_international_or_national_india/,tushar1408,1435044888,Particular for decision trees and incremental learning.,0,0
502,2015-6-23,2015,6,23,18,3at7uy,"Comparing OLS, Lasso, Ridge and ElastiNet Regression Models in R [notebook]",https://www.reddit.com/r/MachineLearning/comments/3at7uy/comparing_ols_lasso_ridge_and_elastinet/,cast42,1435050543,,1,1
503,2015-6-23,2015,6,23,19,3atc4c,[Question] Medical Datasets and/or Medical Image Datasets,https://www.reddit.com/r/MachineLearning/comments/3atc4c/question_medical_datasets_andor_medical_image/,trtm,1435054135,"Dear ML Community, I would like to know if there are any (free available) medical datasets and/or medical image datasets? The area is not important, they just need to be in the medical field.

I am looking for something similar to this: http://brain-map.org

Many thanks in advance",3,1
504,2015-6-23,2015,6,23,19,3ate8j,How to Pick Magic Numbers,https://www.reddit.com/r/MachineLearning/comments/3ate8j/how_to_pick_magic_numbers/,alexeyr,1435055929,,3,3
505,2015-6-23,2015,6,23,22,3atqys,Black box optimization competition. Expensive track with a 1000 Euro prize fund.,https://www.reddit.com/r/MachineLearning/comments/3atqys/black_box_optimization_competition_expensive/,bbcomp,1435064821,,14,1
506,2015-6-23,2015,6,23,23,3au2fs,My new favourite machine learning paper: connection between adversarial networks and kernel scoring rules (maximum mean discrepancy),https://www.reddit.com/r/MachineLearning/comments/3au2fs/my_new_favourite_machine_learning_paper/,fhuszar,1435070819,,1,11
507,2015-6-24,2015,6,24,1,3aujap,See examples of Machine Learning live in action,https://www.reddit.com/r/MachineLearning/comments/3aujap/see_examples_of_machine_learning_live_in_action/,redgansai,1435078463,,3,0
508,2015-6-24,2015,6,24,3,3auzyh,"Tagged videos for the complete course ""Introduction to Machine Learning"" by Dr. Dhruv Batra (Virginia Tech)",https://www.reddit.com/r/MachineLearning/comments/3auzyh/tagged_videos_for_the_complete_course/,ojaved,1435085495,,8,61
509,2015-6-24,2015,6,24,4,3av4pc,multi layer perceptron with a grouped input layer,https://www.reddit.com/r/MachineLearning/comments/3av4pc/multi_layer_perceptron_with_a_grouped_input_layer/,tehgargoth,1435087389,"All the examples I've seen for interconnecting neurons in NN's always have ALL inputs feeding into a shared hidden layer.  I've never seen an algorithm that has inputs split into groups, where each group feeds into a dedicated first hidden layer per group, which then feeds into a second hidden layer shared between all nodes in the first hidden layer.  I hope this makes sense.. I was thinking about this when reading about how running input subsets into their own networks and averaging the outputs a lot of times ends up with more useful outputs.",11,0
510,2015-6-24,2015,6,24,4,3av9vd,Machine learning researcher,https://www.reddit.com/r/MachineLearning/comments/3av9vd/machine_learning_researcher/,[deleted],1435089520,"I lead business development for a very successful proprietary trading firm based in NY. I'm posting here in case the below job would be of any interest to r/machinelearning. 

We have brought together some of the worlds brightest traders, researchers, and technologists to collaborate and solve some of the most intellectually stimulating and technically challenging problems faced in any industry. Our cross discipline team holds degrees in Computer Science, Electrical Engineering, Math, Physics, and Statistics. We share ideas and collaborate to maximize our differing experiences and backgrounds. Our team finds inspiration everywhere, including academia and parallel industries, sparking our next great ideas. 

As a researcher in this team you will focus on gleaning insights from vast, heterogeneous, high dimensional data sets. Utilizing novel statistical and machine learning approaches, you will build the models that drive our alpha forward and uncover the new trading opportunities. Our research teams close collaboration with technology necessitates a strong understanding of algorithms and experience developing software in C++. 

Are you driven by your intellectual thirst? Are you grounded by your pragmatism? Do you hold yourself accountable, while working well with others?

Necessary Skills
	Academic research experience in a highly quantitative discipline (Computer Science, Physics, Math, Statistics, Engineering, etc). Preferably an advanced degree or at a minimum graduate level coursework during undergrad studies.
	Strong understanding of machine learning and statistical approaches, the underlying assumptions they make, their limitations, and the proper use case for each approach. 
	Experience developing software in both high and low level programming languages.
",3,0
511,2015-6-24,2015,6,24,6,3avq4i,General Sequence Learning using Recurrent Neural Networks. Great Talk for Beginners in RNNs!,https://www.reddit.com/r/MachineLearning/comments/3avq4i/general_sequence_learning_using_recurrent_neural/,ojaved,1435096494,,3,44
512,2015-6-24,2015,6,24,12,3awqme,What are some good classification algorithm for data sets with lots of missing values?,https://www.reddit.com/r/MachineLearning/comments/3awqme/what_are_some_good_classification_algorithm_for/,winstonl,1435114849,"Edit 1: The scenario is the following:

A venture capital firm would like to predict which companies are interesting based on some stats like money raised to date, the industry they are in, # of employees, etc. And they get this information from 3 online databases. In some of the companies available on the databases, information are missing. My guess would be they are missing because the firm is small or has been inactive for a period time, and hence not updates.",9,5
513,2015-6-24,2015,6,24,12,3awu8u,[1506.06825] DeepStereo: Learning to Predict New Views from the World's Imagery,https://www.reddit.com/r/MachineLearning/comments/3awu8u/150606825_deepstereo_learning_to_predict_new/,g4n0n,1435116747,,3,11
514,2015-6-24,2015,6,24,13,3ax2x6,Algorithmic Music Generation with Recurrent Neural Networks [video],https://www.reddit.com/r/MachineLearning/comments/3ax2x6/algorithmic_music_generation_with_recurrent/,True-Creek,1435121958,,9,8
515,2015-6-24,2015,6,24,15,3axa7a,How should I approach this problem?,https://www.reddit.com/r/MachineLearning/comments/3axa7a/how_should_i_approach_this_problem/,isolar89,1435126824,"I'm stuck in a project I'm working on teach myself Machine Learning basics in R.

I have a data set containing ZipCodes, Stores, median_age, per_capita_income and ethnicity (White,Black,Hisp,Asian).

I basically want to model which ZipCodes have a high probability of having a Store.  

All variables look normal (except ethnicity). The problems I'm running are:
I have 1k ZipCodes with stores out of 33k ZipCodes, so I believe this is hurting my model.  Can I select a subset of cities without Stores to make my model? I'm afraid my subset selection will bias my model.

Also, I have been debating whether this is a tree problem (due to ethinicties) or a logistic regression problem. 

Any advice/opinion would be appreciated.
Thanks!",6,0
516,2015-6-24,2015,6,24,15,3axbsy,Why deep learning is a hindrance to true AI,https://www.reddit.com/r/MachineLearning/comments/3axbsy/why_deep_learning_is_a_hindrance_to_true_ai/,[deleted],1435128004,,3,0
517,2015-6-24,2015,6,24,17,3axir9,Questions regarding Fraud Detection,https://www.reddit.com/r/MachineLearning/comments/3axir9/questions_regarding_fraud_detection/,methodds,1435133642,"I am working on a machine learning project for Fraud detection and would love to hear what you think about my approach and maybe new ideas.
The data come from a big social survey in which interviews are conducted by employees from an extern company. As this interviews are time consuming, a small amount of interviews only pretends to actually survey people and fake the data instead. Some of them got caught and this is why a labeled dataset for subsequent waves of this study exists.

The basic idea therefore is to detect the fake interviews by machine learning. There are some problems though:

- classes are extremely imbalanced (at max 2% fakers per wave).
- Some Fakers did only fake 1 interview (not a lot data available)
- The questions between the waves vary quite a bit, so it is not easy to construct universal features.

My current approach (based on literature for this topic) is: 
1. Pooling all Interviews, so that each Interviewer in a wave is one row.
2. Using Benford's Law to detect differences between expected digit distributions and observed digit distributions
3. Combining data for 2 (out of 4 waves), using synthetic sampling (SMOTE + ENN) to generate better class balance for classification
4. Training a model on the two waves (using grid search with cross validation to optimize paramers and then testing on the last two waves.

I tried several models and the results were okay, but not amazing.

Do you have any ideas for maybe better solutions for this problem? And what models would you have used?",7,0
518,2015-6-24,2015,6,24,17,3axjag,Textio: words + data = magic,https://www.reddit.com/r/MachineLearning/comments/3axjag/textio_words_data_magic/,kraakf,1435134101,,0,0
519,2015-6-24,2015,6,24,17,3axlhm,Stars are not garant for profit in movies,https://www.reddit.com/r/MachineLearning/comments/3axlhm/stars_are_not_garant_for_profit_in_movies/,gitamar,1435136075,,1,0
520,2015-6-24,2015,6,24,18,3axn8w,Textio: Predict based on your job ad how well you can hire,https://www.reddit.com/r/MachineLearning/comments/3axn8w/textio_predict_based_on_your_job_ad_how_well_you/,[deleted],1435137549,,0,0
521,2015-6-24,2015,6,24,18,3axozh,Making a ML/General purpose computer. Considering an r9 390 as my gpu. Should I not?,https://www.reddit.com/r/MachineLearning/comments/3axozh/making_a_mlgeneral_purpose_computer_considering/,AfraidOfToasters,1435139109,"First off, I am very new to this and I want to learn for my own interest. So far I understand that choosing an AMD card basically means ill be learning OpenCL as opposed to CUDA. I don't quite know all the finer details. Some people seem to say that OpenCL allows more control over (i want to say optimization) things. 


After seeing a string of things kinda bashing CUDA after a string of things recommending Nvidia cards I'm confused more than anything. I'm really not sure but I feel like this may be a good choice despite the lack of recommendations on the card(its a brand new card). The main reason I am strongly considering this as my card is because it gives me lots of memory which sounds like would be needed in implementing things like convolutional networks.

Other possible options would be nvidia and amd cards around the same price range (give or take ~$100) as far as the computer is concerned. As for something to learn on I'm open to suggestions. I've heard a few things about AWS and similar services but i also hear it would be another thing i would have to learn. 


Any help or general thoughts are welcome.

Edit: I think i mean OpenCL... not OpenGL",23,2
522,2015-6-24,2015,6,24,19,3axqix,What are some interesting areas of research in the intersection of NLP and ML/DL?,https://www.reddit.com/r/MachineLearning/comments/3axqix/what_are_some_interesting_areas_of_research_in/,ravo87,1435140490,,5,5
523,2015-6-24,2015,6,24,20,3axxlk,DeepStereo: Learning to Predict New Views from the Worlds Imagery [Video + Paper in comments],https://www.reddit.com/r/MachineLearning/comments/3axxlk/deepstereo_learning_to_predict_new_views_from_the/,Barbas,1435146296,,9,43
524,2015-6-24,2015,6,24,23,3aybmh,CRF generating invalid sequence,https://www.reddit.com/r/MachineLearning/comments/3aybmh/crf_generating_invalid_sequence/,vvlkq,1435154792,"I am experimenting with CRF++. I created a training set using the BILOU enconding and everything worked fine. When I started using this model with different content I started noticing some invalid sequences.

My training instance is something like:
This   is      an     Multi       Word    Entity
OUT  OUT   OUT   BEGIN    IN        LAST

But the CRF can return invalid sequences where a LAST appears after an OUT. Is this correct CRF behavior (given the 0 prob in transition OUT-LAST)?",1,0
525,2015-6-25,2015,6,25,0,3ayowh,"Neural Network for Flappy Bird (how original, I know)",https://www.reddit.com/r/MachineLearning/comments/3ayowh/neural_network_for_flappy_bird_how_original_i_know/,rantonels,1435161081,,6,8
526,2015-6-25,2015,6,25,1,3ayqy9,Any kind of char RNN for text generation in python?,https://www.reddit.com/r/MachineLearning/comments/3ayqy9/any_kind_of_char_rnn_for_text_generation_in_python/,krimtheguy,1435161942,"Was wondering if there was any kind of RNN written in python that allows to be trained over some big text and creates text from that model taht was trained (something like [this](https://github.com/karpathy/char-rnn) but written in python, not in lua since I want to add it to a personal project written in python)

Thanks in advance",4,3
527,2015-6-25,2015,6,25,1,3ayvlh,Imaginary Big Data Problems,https://www.reddit.com/r/MachineLearning/comments/3ayvlh/imaginary_big_data_problems/,tfimg24,1435163900,,8,0
528,2015-6-25,2015,6,25,1,3aywtd,Machine Learning for Robotics and Computer Vision,https://www.reddit.com/r/MachineLearning/comments/3aywtd/machine_learning_for_robotics_and_computer_vision/,john_philip,1435164412,,0,5
529,2015-6-25,2015,6,25,2,3az4qj,Large Scale Deep neural net falling down the rabbit hole,https://www.reddit.com/r/MachineLearning/comments/3az4qj/large_scale_deep_neural_net_falling_down_the/,benanne,1435167893,,69,150
530,2015-6-25,2015,6,25,5,3azryw,"Currently processing on a 2010 Macbook Air... Looking to spend ~$4k on some hardware, any suggestions?",https://www.reddit.com/r/MachineLearning/comments/3azryw/currently_processing_on_a_2010_macbook_air/,rccr90,1435178221,"I work with neural networks, support vector machines, and thousands of CSVs. I would primarily use this computer python and R. What kind of hardware am I looking for? When I say hardware, I mean a model of computer (if it exists) or a specific component other than RAM that will help me do my thing.

Right now just to get some experiments set up my computer goes 10+ hours straight doing basic math on large data sets and I know it doesn't do my laptop well. ",22,0
531,2015-6-25,2015,6,25,6,3azxra,CS224d: Deep Learning for Natural Language Processing Project Reports,https://www.reddit.com/r/MachineLearning/comments/3azxra/cs224d_deep_learning_for_natural_language/,neuromorphics,1435180736,,6,11
532,2015-6-25,2015,6,25,6,3azypy,Distance metric used in contrastive loss function,https://www.reddit.com/r/MachineLearning/comments/3azypy/distance_metric_used_in_contrastive_loss_function/,AlexRothberg,1435181203,"In [Hadsell 06](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf), the authors use the L2 norm for the distance metric whereas in the paper from the preceding year, they state [Chopra 05](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf):
&gt;Second, we must emphasize that using the square norm
instead of the L1 norm for the energy would not be appropriate.
Indeed, if the energy were the square norm of the difference
between the output vectors of the two patterns, the gradient of the energy with respect to the parameter would
vanish as the energy approached zero. This would create
a dangerous plateau in the loss function. This could lead
to failure of the machine to learn in cases where the two
images are impostors and the corresponding energy is near
zero.

I don't see any explanation for this change.",0,0
533,2015-6-25,2015,6,25,6,3b0017,Getting started with Neuromorphics,https://www.reddit.com/r/MachineLearning/comments/3b0017/getting_started_with_neuromorphics/,neuromorphic101,1435181785,"So I've heard some of you bashing at neuromorphic technology. But it seems, from the data, that they're the best way for me to get into biologically realistic AI/computational neuroscience. Does anyone know how I could start building neuromorphic hardware? Thanks!",13,0
534,2015-6-25,2015,6,25,6,3b0270,why does the word2vec model randomize context size?,https://www.reddit.com/r/MachineLearning/comments/3b0270/why_does_the_word2vec_model_randomize_context_size/,billconan,1435182812,"I'm following the stanford nlp class. in their assignment code, they randomize the size of context.

C is the provided context size, for example 5,

but they always generate a random context size C1, what's good of doing this? they don't seem to mention this detail in any material.

    for i in xrange(batchsize):
        C1 = random.randint(1,C)
        centerword, context = dataset.getRandomContext(C1)


This is not unique to the stanford example code, I checked other github projects, they seem to do the same.

I can't think of why?",3,2
535,2015-6-25,2015,6,25,8,3b0dwu,Why Deep Learning Is a Hindrance to Progress Toward True AI,https://www.reddit.com/r/MachineLearning/comments/3b0dwu/why_deep_learning_is_a_hindrance_to_progress/,[deleted],1435188307,,0,1
536,2015-6-25,2015,6,25,8,3b0gna,Any companies annotate and label confidential data securely?,https://www.reddit.com/r/MachineLearning/comments/3b0gna/any_companies_annotate_and_label_confidential/,malcolmgreaves,1435189667,"Hello guys and gals!

I have a bit of a conundrum for a project I'm working on. I have a lot of confidential user documents (i.e. have to make sure that they never become public). I want to be able to have people read and annotate these documents for a specific learning task. 

Normally, I'd turn to Amazon's mechanical turk. However, there are no security or confidentiality agreements on this platform (thinking more along the lines of binding legal agreements to ensure confidentiality). 

Are there any companies that provide this kind of confidential data labeling service? Does anyone have any experience with these companies?",0,1
537,2015-6-25,2015,6,25,8,3b0huw,Conditional Random Fields as Recurrent Neural Networks: Semantic Image Segmentation [Live Demo],https://www.reddit.com/r/MachineLearning/comments/3b0huw/conditional_random_fields_as_recurrent_neural/,g4n0n,1435190263,,2,11
538,2015-6-25,2015,6,25,9,3b0n85,"Math/cs double major here, I want to get into machine learning",https://www.reddit.com/r/MachineLearning/comments/3b0n85/mathcs_double_major_here_i_want_to_get_into/,trill212,1435193025,,7,0
539,2015-6-25,2015,6,25,9,3b0nlm,"Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues [PDF] [UC Berkeley, Facebook AI]",https://www.reddit.com/r/MachineLearning/comments/3b0nlm/beyond_frontal_faces_improving_person_recognition/,g4n0n,1435193209,,0,4
540,2015-6-25,2015,6,25,10,3b0r1g,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/3b0r1g/ask_me_anything_dynamic_memory_networks_for/,LLCoolZ,1435194987,,4,25
541,2015-6-25,2015,6,25,12,3b17g3,[1506.00333] Learning to Answer Questions From Image using Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/3b17g3/150600333_learning_to_answer_questions_from_image/,ResHacker,1435203350,,0,4
542,2015-6-25,2015,6,25,12,3b17u4,Simple implementation of a feedforward Neural Net in vanilla Python/NumPy for demo purposes,https://www.reddit.com/r/MachineLearning/comments/3b17u4/simple_implementation_of_a_feedforward_neural_net/,[deleted],1435203570,,0,0
543,2015-6-25,2015,6,25,16,3b1rl6,Torch-Theano-hybrid?,https://www.reddit.com/r/MachineLearning/comments/3b1rl6/torchtheanohybrid/,HelmsmanRobertson,1435216714,"I know they use different languages, so you'll have to pardon me for a potential dumb question.

1. I really like the modular paradigm of Torch's 'nn' package. However, extending the package is hard for people who might be bad with calculus. 
2. I really like that Theano works symbolically, but prototyping crazy networks seems intimidating. 

Is there any hope that someone will find a way to combine both platforms' shining features?",4,4
544,2015-6-25,2015,6,25,17,3b1vzj,Where to learn Torch idioms?,https://www.reddit.com/r/MachineLearning/comments/3b1vzj/where_to_learn_torch_idioms/,HelmsmanRobertson,1435220400,"I normally go to stackoverflow for questions about language ""best practices"" and what not, but was surprised to see the underwhelming usage of the ""Torch"" tag.

Where do people go to learn (or ask) Torch best practice (questions)?",17,14
545,2015-6-25,2015,6,25,18,3b21ep,"Taking Humans out of the Deep Learning Loop - Ryan Adams, Prof at Harvard &amp; CEO at Whetlab (acquired by Twitter) - YouTube video",https://www.reddit.com/r/MachineLearning/comments/3b21ep/taking_humans_out_of_the_deep_learning_loop_ryan/,reworksophie,1435225345,,0,1
546,2015-6-25,2015,6,25,21,3b2bl5,Training a Neural Network on variable length vectors?,https://www.reddit.com/r/MachineLearning/comments/3b2bl5/training_a_neural_network_on_variable_length/,[deleted],1435233805,"I have a way of converting a sentence into a vector of numbers which correlate to sentiment. For example:

""The cat is happy"" -&gt; ""0 0 0 1"" or 
""The cat is depressed"" -&gt; ""0 0 0 -1"" or
""The cat is not happy"" -&gt; ""0 0 0 -1 1""

I am producing a data set of vectors such as those shown above with a target value which indicates whether the sentence overall had a positive sentiment +1 or a negative sentiment -1. 

The relationship between the vectors and the target can be quite complex if the sentence contains double negatives or is complex in some other way so I want to use a NN to learn this relationship.

My (probably trivial) question is, how can I use a neural network when I don't know how long the input vector might be i.e. if the sentence contains 20 words the vector would be of length 20.

I am thinking about using a Recurrent Neural Network and feeding the words in ""one at a time"" but I'd like to find out how other researchers have tackled this problem before I reinvent the wheel. ",16,24
547,2015-6-25,2015,6,25,22,3b2kf8,Question about momentum-based gradient descent,https://www.reddit.com/r/MachineLearning/comments/3b2kf8/question_about_momentumbased_gradient_descent/,thefunkyoctopus,1435239055,"When updating the weights and biases via the velocity vector, is there a corresponding velocity value for each individual weight and bias, or rather is the velocity value specific to a neuron, applying the same velocity value to all the weights and the bias of that neuron?",1,0
548,2015-6-25,2015,6,25,22,3b2m9f,Likelihood in EM algorithm,https://www.reddit.com/r/MachineLearning/comments/3b2m9f/likelihood_in_em_algorithm/,davidun,1435240031,"Hi. If I understand correctly, the likelihood is obtained in the E-step (for instance in the BaumWelch algorithm the likelihood can be obtained form the forward-backward procedure). However, the final step in each EM-iteration is the M-step. That means the likelihood computed in the step k, actually ""belongs"" to the parameters of step (k-1). Is that really so, or am I missing something?
Thanks you.",6,0
549,2015-6-25,2015,6,25,23,3b2odj,Most output classes in a neural network?,https://www.reddit.com/r/MachineLearning/comments/3b2odj/most_output_classes_in_a_neural_network/,MysteriousArtifact,1435241150,"Looking back on my studies, every neural network I've ever come across (including deep ones) had at most 10-20 output classes.

My understanding is that the more classes the network needs to discriminate between,  the more data is required, and generally the longer it takes to train.

* So what is the most complex task (most output classes) you have ever encountered in literature or on your own? 
* To date, are there any networks with hundreds, thousands, or millions of output classes?",10,0
550,2015-6-25,2015,6,25,23,3b2tlv,TED-RNN - Machine generated TED-Talks,https://www.reddit.com/r/MachineLearning/comments/3b2tlv/tedrnn_machine_generated_tedtalks/,[deleted],1435243655,,0,1
551,2015-6-25,2015,6,25,23,3b2ueb,Attention-Based Models for Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/3b2ueb/attentionbased_models_for_speech_recognition/,clbam8,1435244026,,0,5
552,2015-6-26,2015,6,26,0,3b2vp0,Is Cox proportional hazards regression considered an ML algorithm?,https://www.reddit.com/r/MachineLearning/comments/3b2vp0/is_cox_proportional_hazards_regression_considered/,bubbachuck,1435244595,"Cox regression is used to model survival data in clinical research.  [Here is a primer](http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0CCcQFjAC&amp;url=http%3A%2F%2Fwww.medicine.ox.ac.uk%2Fbandolier%2Fpainres%2Fdownload%2Fwhatis%2Fcox_model.pdf&amp;ei=lxaMVcC_H8j0-QHCnKBw&amp;usg=AFQjCNE1kGdZEu5AOSJNl1nZTbLdQdd_rg&amp;sig2=n7Zwfwv034C5mB7OCvcxxg&amp;bvm=bv.96782255,d.cWw)

Based on the methodlogy presented on page 4 of the PDF, it appears to be similar to linear regression and uses MLE to fit the regression coefficients.  However, if you look up published comparisons between Cox and ML methods, it appears to be treated as a separate entities.

Would those in the ML community consider Cox regression a machine learning algorithm?",0,1
553,2015-6-26,2015,6,26,0,3b32m7,"High dimensional, multimodal target distribution for MCMC",https://www.reddit.com/r/MachineLearning/comments/3b32m7/high_dimensional_multimodal_target_distribution/,Professional_123,1435247733,"Hi!

I'm wondering if any benchmark target distributions exist :)

Thanks!",6,1
554,2015-6-26,2015,6,26,1,3b37yv,Tv hire perth,https://www.reddit.com/r/MachineLearning/comments/3b37yv/tv_hire_perth/,claude82jet,1435250045,,0,1
555,2015-6-26,2015,6,26,2,3b3dmn,A cleaned wikipedia dataset,https://www.reddit.com/r/MachineLearning/comments/3b3dmn/a_cleaned_wikipedia_dataset/,Bhavishya1,1435252629,"I am doing some research on sequence generators, and I am in need of a cleaned wikipedia dataset- meaning that all the links, images, side notes should be removed and only the textual content should be present. Can anybody provide a link to such an dataset? 

Failing that, can you tell me how to develop such a dataset from scratch?
Thanks.",14,20
556,2015-6-26,2015,6,26,2,3b3jf8,Computer shop perth,https://www.reddit.com/r/MachineLearning/comments/3b3jf8/computer_shop_perth/,phil51air,1435255018,,0,1
557,2015-6-26,2015,6,26,3,3b3pzb,Laptop screen repair perth,https://www.reddit.com/r/MachineLearning/comments/3b3pzb/laptop_screen_repair_perth/,claude82jet,1435257777,,0,1
558,2015-6-26,2015,6,26,3,3b3q6q,Github and ML theory,https://www.reddit.com/r/MachineLearning/comments/3b3q6q/github_and_ml_theory/,uncountableB,1435257879,"Hey guys! I'm about to start uploading some machine learning algorithms I've implemented from Stanford's CS 229 course onto my Github. I've also been doing the theory part of the problem sets, too, and I was just wondering if employers would care if I put up latex-ed solutions to those on there too.

My reasoning is that it would show that I'm not just black-boxing everything and that I have an understanding of the theoretical background as well, and that could be valuable, but I want to know if anyone agrees/disagrees with this reasoning. Would employers care at all?",10,1
559,2015-6-26,2015,6,26,3,3b3rba,Project that uses SURF,https://www.reddit.com/r/MachineLearning/comments/3b3rba/project_that_uses_surf/,no_porner,1435258404,"Hey guys,
I am looking for a object detection project that uses SURF or some of its variant (as the SURF is patented). It would be great if you could guide me what little comparison thing can I do with SURF like comparing it with SIFT(which is already done). I really need your help guys. Please note that I don't need it for commercial purpose, just for the class project.
Thank you.",4,0
560,2015-6-26,2015,6,26,4,3b3xnu,Building the most productive data science stack with Python,https://www.reddit.com/r/MachineLearning/comments/3b3xnu/building_the_most_productive_data_science_stack/,sixerspl,1435261183,,2,0
561,2015-6-26,2015,6,26,5,3b41l0,Namely (NYC) is looking to hire a Lead Data Scientist (come! its a lot of fun),https://www.reddit.com/r/MachineLearning/comments/3b41l0/namely_nyc_is_looking_to_hire_a_lead_data/,manueslapera,1435262958,,0,0
562,2015-6-26,2015,6,26,5,3b43xk,Laptop screen repair perth,https://www.reddit.com/r/MachineLearning/comments/3b43xk/laptop_screen_repair_perth/,leedill9,1435263920,,0,1
563,2015-6-26,2015,6,26,6,3b48zp,How to do product matching?,https://www.reddit.com/r/MachineLearning/comments/3b48zp/how_to_do_product_matching/,Icko_,1435266170,"So here is the problem: in my firm we are crawling a bunch of sites and am extracting data for products - washing machines, laptops, bicycles etc.  
The problem: we need is to see if two products from the two sites are the same.  
The data: the titles for each site - e.g. in amazon there is a dishwasher **HOTPOINT-ARISTON LSTB-6B019 EU** which is the same as Technomania's **Built-in dishwasher Hotpoint LSTB 6B019, 10 pack, 6 programs, Class A+, White**. The good news is I have a *lot* of test data.  
How are we currently solving the problem: lots and lots of data entries :D  
What I tried: I played around with fuzzy string matching, match of longest string etc. but the results are pretty bad.  
What do you think might work?",7,0
564,2015-6-26,2015,6,26,7,3b4ieb,Example-based educational resources?,https://www.reddit.com/r/MachineLearning/comments/3b4ieb/examplebased_educational_resources/,nickbuch,1435270470,"Im a software developer with a math/stats background looking to hit the ground running with machine learning.  ive started with this text and understand it, but am failing to grasp how even Boolean functions can be applied, though I understand the formalism.
http://robotics.stanford.edu/people/nilsson/mlbook.html

suggestions?",0,5
565,2015-6-26,2015,6,26,8,3b4u3z,Fun paid survey from Microsoft Machine Learning! Help us name things.,https://www.reddit.com/r/MachineLearning/comments/3b4u3z/fun_paid_survey_from_microsoft_machine_learning/,[deleted],1435276398,,17,0
566,2015-6-26,2015,6,26,9,3b4vrf,How to create self-aware neural net?,https://www.reddit.com/r/MachineLearning/comments/3b4vrf/how_to_create_selfaware_neural_net/,[deleted],1435277245,"Humans can think about their thoughts, so how would the same thing be done with a neural net? Sorry if this is in the wrong place.",1,0
567,2015-6-26,2015,6,26,10,3b56jq,Macbook rentals perth,https://www.reddit.com/r/MachineLearning/comments/3b56jq/macbook_rentals_perth/,body23fang,1435282930,,0,1
568,2015-6-26,2015,6,26,11,3b5aqf,Popular Deep Learning Tools - A Review,https://www.reddit.com/r/MachineLearning/comments/3b5aqf/popular_deep_learning_tools_a_review/,vonnik,1435285153,,3,35
569,2015-6-26,2015,6,26,11,3b5efn,Intel* Deep Learning Framework,https://www.reddit.com/r/MachineLearning/comments/3b5efn/intel_deep_learning_framework/,downtownslim,1435287081,,6,23
570,2015-6-26,2015,6,26,12,3b5hhr,Compression of input data for neural nets?,https://www.reddit.com/r/MachineLearning/comments/3b5hhr/compression_of_input_data_for_neural_nets/,Dwood15,1435288742,"Let's say I have 256 unique objects that i can feed to my neural network for every tile on a map or whatever, to indicate a different tile. I want my ai to learn to navigate this map (a la sethbling's mario ai) 

Would it be efficient and/or prudent to convert that number to an 8-long array of -1 or 1's ?

Currently mario can only sense if a tile or sprite is within a range of himself, but he/it can't differentiate between the various types of sprites around itself. I figure this would reduce the amount of computation required if i were to have a separate individual neuron for each sprite type. What do you think?",3,4
571,2015-6-26,2015,6,26,13,3b5qur,Applicable Models?,https://www.reddit.com/r/MachineLearning/comments/3b5qur/applicable_models/,caesarten,1435294172,"Hi all,

I'm working on an interesting problem at work, and was hoping to try out some new things. I'm working with health care data, and have data for multiple patients over time, up until, and after a certain outcome. Every observation also has various lab values, medication values, etc associated with it.

My task is to predict whether that outcome will occur given these data over time.

I'm going to try RNN/LSTM since they both seem very applicable. I'm also going to try a normal feed-forward NN network and Random Forests with 'flattened' inputs, (where I take long data over time for a patient, and condense it into wide data containing n - some time period observations, along with new features based on those condensed time periods.)

Is there anything else you would recommend trying out? I'm considering looking into survival analysis for this as well, since time to outcome closely matches things like clinical trials.

Thanks!",0,0
572,2015-6-26,2015,6,26,14,3b5uez,Practical Experience with IRNN algorithm?,https://www.reddit.com/r/MachineLearning/comments/3b5uez/practical_experience_with_irnn_algorithm/,alexmlamb,1435296447,"Hello, 

Has anyone had practical experience with using Hinton's IRNN algorithm (using reels in a standard RNN but initializing the weight matrices to the identity matrix)?  It seems appealing to me because it avoids saturating activation functions and is much simpler than LSTM.  ",9,6
573,2015-6-26,2015,6,26,15,3b5xx1,State of Multi-Task Learning with Deep Architectures?,https://www.reddit.com/r/MachineLearning/comments/3b5xx1/state_of_multitask_learning_with_deep/,budhdub,1435299006,"Anybody out there working on multi-task learning with neural networks?

Saw a few papers long back (remember only [Improving Multiview Face Detection with
Multi-Task Deep Convolutional Neural Networks](http://research.microsoft.com/en-us/um/people/chazhang/publications/wacv2014_chazhang.pdf)) where it really didnt seem all that impressive. 

Any newer works which have shown a place for this paradigm?",2,0
574,2015-6-26,2015,6,26,15,3b60jh,A complete solution of a kaggle competition (knowledge) - Bike Sharing Demand Prediction,https://www.reddit.com/r/MachineLearning/comments/3b60jh/a_complete_solution_of_a_kaggle_competition/,john_philip,1435300955,,1,4
575,2015-6-26,2015,6,26,15,3b6185,Machine Learning: The Basics,https://www.reddit.com/r/MachineLearning/comments/3b6185/machine_learning_the_basics/,aranag,1435301567,,6,67
576,2015-6-26,2015,6,26,15,3b61a5,Machine Learning via Large-scale Brain Simulations,https://www.reddit.com/r/MachineLearning/comments/3b61a5/machine_learning_via_largescale_brain_simulations/,aranag,1435301620,,1,6
577,2015-6-26,2015,6,26,16,3b63eb,Fireside chat with Yann Lecun,https://www.reddit.com/r/MachineLearning/comments/3b63eb/fireside_chat_with_yann_lecun/,ojaved,1435303285,,0,2
578,2015-6-26,2015,6,26,16,3b654k,"Is Language Identification really solved? ""Possibly not,"" says DSL.",https://www.reddit.com/r/MachineLearning/comments/3b654k/is_language_identification_really_solved_possibly/,DSLSharedTask,1435304796,,1,2
579,2015-6-26,2015,6,26,18,3b6e05,How do I find a sci-kit learn teacher/mentor?,https://www.reddit.com/r/MachineLearning/comments/3b6e05/how_do_i_find_a_scikit_learn_teachermentor/,validated1,1435312708,"I have gotten pretty in depth and I often have questions that SO cannot figure out. I want to buy skype lessons from someone really good at sci-kit learn... Probably just an hour a week... If you know where I can find someone, let me know!
",4,0
580,2015-6-26,2015,6,26,19,3b6hkw,Youtube Video: Neural Networks for Newbies,https://www.reddit.com/r/MachineLearning/comments/3b6hkw/youtube_video_neural_networks_for_newbies/,john_philip,1435315707,,0,13
581,2015-6-26,2015,6,26,20,3b6idm,lua-Torch7 tutorials?,https://www.reddit.com/r/MachineLearning/comments/3b6idm/luatorch7_tutorials/,tushar1408,1435316410,WHere can i find lua torch video tutorials?,4,0
582,2015-6-26,2015,6,26,20,3b6kdo,The Unreasonable Effectiveness of Random Projections in Computer Science (x-post r/CompressiveSensing),https://www.reddit.com/r/MachineLearning/comments/3b6kdo/the_unreasonable_effectiveness_of_random/,compsens,1435317963,,3,8
583,2015-6-26,2015,6,26,23,3b6zvo,The Google AI Neural Network T-Shirt,https://www.reddit.com/r/MachineLearning/comments/3b6zvo/the_google_ai_neural_network_tshirt/,pierrelux,1435327500,,2,1
584,2015-6-26,2015,6,26,23,3b75ak,Series on Deep Learning: Setting up GPU learning on EC2,https://www.reddit.com/r/MachineLearning/comments/3b75ak/series_on_deep_learning_setting_up_gpu_learning/,arshakn,1435330052,,2,2
585,2015-6-26,2015,6,26,23,3b75sd,How do machines learn meaning?,https://www.reddit.com/r/MachineLearning/comments/3b75sd/how_do_machines_learn_meaning/,benjaminwilson,1435330270,,0,0
586,2015-6-27,2015,6,27,0,3b76wo,Machine Learning on Graphs,https://www.reddit.com/r/MachineLearning/comments/3b76wo/machine_learning_on_graphs/,zenscr,1435330812,"Hi /r/machinelearning!

I'd like to do regression and classification tasks on graph representations (e.g. of computer programs). What's the best way to do this?

Maybe graph kernels + SVM/SVR would be an option and I'd love to try it with my data, but I hardly find any sophisticated implementations. What I've found are just some MATLAB scripts from http://goo.gl/0wH0Fu. If I don't find any better implementations, then I'll give it a try, but I'd like to know if there are better alternatives out there. Any ideas?

Thanks!",0,1
587,2015-6-27,2015,6,27,2,3b7qfl,Partially Derivative Episode 26: The Big Reveal!,https://www.reddit.com/r/MachineLearning/comments/3b7qfl/partially_derivative_episode_26_the_big_reveal/,chrisalbon,1435339382,,3,0
588,2015-6-27,2015,6,27,2,3b7uyt,About an idea for a side project...,https://www.reddit.com/r/MachineLearning/comments/3b7uyt/about_an_idea_for_a_side_project/,[deleted],1435341352,"Hello everyone,

I have been looking for a fun personnal project to do recently. I might have some idea but I would like some feedback about this first. Not so long ago, I wrote a controller for this contest : https://codegolf.stackexchange.com/questions/44707/lab-rat-race-an-exercise-in-genetic-algorithms that can be found here : https://github.com/matovitch/Ratlab-. And I have been thinking to extend the concept of this ""game"".

The idea would be, with the help of the players, to create a virtual ecosystem. The world for this ecosystem to thrive in would be a 2d procedurally generated grid-based map of various dirt and sea with a torus geometry to avoid edges problems. Each player would have to chose a race with a predefined attack strategy (to be determined). He will then have to code the behaviour of one species of that race for one turn of the game in a very specific virtual machine. This machine will have 3 different memories : visual, genetic, neural (and a stack). The visual memory would be a read-only memory of the surrounding cells of an individual. The genetic one is also read only and would be given as an input to an individual by the simulation which will perform the breading process. The neural one will be scrambled at each individual birth, but from the player perspective he could use it as he wants. The aims of the player code will be to fill 3 registers that will determine the magnitude and direction of the next ""acceleration"" vector for the individual and the attack vs do-not-attack on the next turn (attack should have an associated penalty cost to avoid always-attack strategy). The simulation would adjust the attack damage points for each species to keep a balanced ecosystems so that even the badly-written species could survive. Additonally, the virtual machine would include two read-only ""health"" and ""age"" registers.

After that, one could stream the data of the simulation to a G/WUI client and have some fun watching at its own species fight to survive. (one could associate color palette to a species and use the hash of the ""kernel"" code as a seed for procedural skin generation per species (or dna)...but back to reality).

I have thought more in depth to the architecture of the virtual machine (including ""vectorized"" instruction for linear algebra).

I thinks that could be pretty fun but I can see a few drawbacks. First, it's being sunny outside and I am lucky enough to live in a really nice area vs. this seems like a *lot* of work. Second, it's more a design question, why a virtual machine...why not a simple protocol with the players as clients ? Each player could chose its language as he wants, use existing libraries. Well the game would become asynchronous, and quite unfair regarding latency, memory or computing power (as high frequecy trading ;)), a ""cheater"" could use the stream of the simulation as input data (for info on the map even if we impose a broadcast delay)...but I thought I had some stronger arguments against it I can't remember so this may be a viable option ? Then a lot of people in machine learning mostly use higher level language like python, R, Julia, (even last C++). So a very specific RISC assembly might frighten even the bravest to participate. One could create some DSL C-like language but that at least double the effort.

I would be very glad if you could reframe my thoughts. What' s the easiest? What's the funniest ?

Thanks in advance for your help. Best Regards,

p.s : Sorry for the raw formatting and probably for my deficient english...the explanations might not be perfect either feel free to ask if you have any doubt.",0,0
589,2015-6-27,2015,6,27,3,3b81u6,Examples of adding deterministic information to statistical machine learning models?,https://www.reddit.com/r/MachineLearning/comments/3b81u6/examples_of_adding_deterministic_information_to/,sleepicat,1435344262,"Let's say I have a bunch of multivariate data that I'm trying to classify. Some of the data and parameters are correlated chemically, some of the data and parameters are correlated spatially, some of the data and parameters are related by physical laws, but I don't know ALL of the possible relationships, so there exist (presumably) unknown correlations as well as noise.  

What machine learning approaches are good at incorporating deterministic information (where available) while being flexible enough to learn from the observations as well? 

",2,5
590,2015-6-27,2015,6,27,4,3b85f8,Can deep learning help you find the perfect girl?,https://www.reddit.com/r/MachineLearning/comments/3b85f8/can_deep_learning_help_you_find_the_perfect_girl/,devries89,1435345845,,11,4
591,2015-6-27,2015,6,27,4,3b8bqe,Physically Created Artificial Neurons Can Communicate in the Same Way as Human Neurons: Is this ANN in a biological form,https://www.reddit.com/r/MachineLearning/comments/3b8bqe/physically_created_artificial_neurons_can/,redlikeazebra,1435348651,,4,0
592,2015-6-27,2015,6,27,5,3b8f9b,16 Free Data Science Books,https://www.reddit.com/r/MachineLearning/comments/3b8f9b/16_free_data_science_books/,deltawk,1435350265,,16,244
593,2015-6-27,2015,6,27,6,3b8nko,[Hardware advice] i7-4790K vs i7-5920K,https://www.reddit.com/r/MachineLearning/comments/3b8nko/hardware_advice_i74790k_vs_i75920k/,elanmart,1435354114,"Hey guys, 

I'm a CS undergrad and I'd like to buy myself a workstation. My personal interests revovle around NLP (with strong focus on applications of RNNs), however I don't know if I'd ever be able to pursue them. 

I'd like to develop general ML skills (to be able to find a job, and becouse ML is awesome in general), either via Kaggle, my own projects and simple research on my home uni (not deep learning research tho). 

I don't want to use EC2, since spot instances are unreliable, and regular are expensive and time-consuming.

I'm wondering what CPU should I get: i7-4790K vs i7-5820K.

The former is cheaper, can use DDR3 RAM and cheaper motherboard. The latter is more powerfull,
but more expensive. Also, I think that i7-4790K is limited to 32GB of RAM.

Considering that programming part-time I make $250 / month and can't spend all of that on 
my hardware, should I go for the more expensive build, or the difference is not worth the money?

Thanks for any advice!",11,2
594,2015-6-27,2015,6,27,17,3baew7,Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images,https://www.reddit.com/r/MachineLearning/comments/3baew7/deep_neural_networks_are_easily_fooled_high/,clbam8,1435395016,,11,5
595,2015-6-27,2015,6,27,22,3baulb,Dr.Wang: A disease diagnosis system based on PubMed Abstracts,https://www.reddit.com/r/MachineLearning/comments/3baulb/drwang_a_disease_diagnosis_system_based_on_pubmed/,godspeed_china,1435410828,"http://chuantu.biz/t2/10/1435410072x-1376440182.png
User input a set of symptoms, and the system suggests possible diseases, genetic candidate genes and possible drugs to help doctors.",0,1
596,2015-6-28,2015,6,28,0,3bb7f6,Help Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3bb7f6/help_neural_networks/,Oleicas,1435419169,"Hi, I've learned the basics of neural networks, and only did a simple neural network at excel, I would like to learn more about it and become able to build my own neural networks.
Can you suggest me some ways to learn, some tutorials, and a easy software for a noob like me ... Sorry for my bad english ...
 ",6,0
597,2015-6-28,2015,6,28,2,3bbijy,Why is a simple logistic regression model outperforming my neural nets?,https://www.reddit.com/r/MachineLearning/comments/3bbijy/why_is_a_simple_logistic_regression_model/,airalcorn2,1435425184,"**Edit #2**: Huge shoutout to /u/thatguydr for cracking the mystery. [My data was not normalized](http://www.reddit.com/r/MachineLearning/comments/3bbijy/why_is_a_simple_logistic_regression_model/cskoc6u), which was apparently a big mistake. After normalizing the data, not only did the scikit-learn implementation of logistic regression shoot up to an accuracy of 74%, but the neural net achieved an accuracy of 76% after merely seconds of training. I feel pretty embarrassed that the solution was so simple, but I kind of suspected that might be the case. Thanks a lot for being so helpful /r/MachineLearning. I really love this subreddit and I feel like I've learned so much from it.

**Edit #3**: Some additional discussion on feature normalization.

* http://stats.stackexchange.com/questions/41704/how-and-why-do-normalization-and-feature-scaling-work
* http://www.quora.com/How-does-normalization-of-data-help-in-Machine-Learning

**Edit #4**: In case anyone comes across this post in the future, I think I must have had a bug in my original code, or something screwy was going on, because the neural net started underperforming again relative to the R logistic regression model, but upon switching back to unnormalized data, the deep net was again able to outperform the R model. My suggestion would be to experiment with both (which seems to be good advice for most things in machine learning).

*****

I'm trying to build a classifier for some data that has two classes. The data consists of ~300,000 examples with 47 features, of which 18 are binary (representing one-hot encodings of two categorical features with five and 13 categories, respectively) and the rest are real valued. The data is very sparse and the class split is 55/45.

Running this data through a logistic regression model in R (using glm) achieves an accuracy of ~74% on the test set in a matter of seconds. Oddly, scikit-learns implementation of logistic regression consistently performs worse than Rs, hovering around ~70-71% accuracy. Some googling led me to [this recent post on Stack Overflow](http://stackoverflow.com/questions/28747019/comparison-of-r-statmodels-sklearn-for-a-classification-task-with-logistic-reg), which, unfortunately, doesnt have any answers. I feel like this discrepency could potentially be relevant to the accuracy of my neural nets, but Im not sure.

Moving on, the various nets I've attempted to train (using both Theano and Torch implementations) consistently top out at around ~71% accuracy. I've tried using models of varying complexity (single hidden layer with few units, single hidden layer with more units [e.g., 500], and two or three hidden layers with varying numbers of units), and with various learning rates and mini batch sizes, but I can't seem to break past that ~71% wall (though these models do always end up surpassing the logistic regression model, albeit not by much).

I know that, in theory, the neural nets should be able to at least match the accuracy of the logistic regression classifiers, so what's happening? I'd be happy with overfitting the model at this point (I haven't even been applying any regularization to the cost functions), so what steps could I take to guarantee that? Ive also come across [this post on Stack Overflow](http://datascience.stackexchange.com/questions/731/how-to-fight-underfitting-in-a-deep-neural-net) discussing underfitting neural nets, but, even after following that advice, my nets still are only able to match the accuracy of the scikit-learn logistic regression model, not the one implemented in R.

**Edit #1**: Thanks for your feedback, everyone. I think I may have omitted a key detail here that might be leading people astray... **the training error is also topping out at ~71%**. My understanding is that neural nets should basically be able to ""memorize"" the training set as they are made more complex, but that is not happening here, which is why I think something screwy is going on. As /u/mostly_reasonable pointed out, I'm pretty sure this data set is ""noisy"" in that 100% accuracy on the training set probably isn't possible, but the R logistic regression model consistently achieves ~74% accuracy on different training/testing sets, so the neural net has not yet reached the theoretical maximum.",36,25
598,2015-6-28,2015,6,28,3,3bbs0c,[1506.06272] Aligning where to see and what to tell: image caption with region-based attention and scene factorization,https://www.reddit.com/r/MachineLearning/comments/3bbs0c/150606272_aligning_where_to_see_and_what_to_tell/,downtownslim,1435430127,,0,8
599,2015-6-28,2015,6,28,4,3bbv9u,"Is there a public available code for some paper using CNN for semantic segmentation? It can be in Caffe, Torch, or Thenao.",https://www.reddit.com/r/MachineLearning/comments/3bbv9u/is_there_a_public_available_code_for_some_paper/,andrewbarto28,1435431881,"Sorry, misspelled Theano in the title.",2,2
600,2015-6-28,2015,6,28,4,3bbxqw,Machine Learning in Fintech Conference organized by Startup.ML &amp; Bloomberg,https://www.reddit.com/r/MachineLearning/comments/3bbxqw/machine_learning_in_fintech_conference_organized/,arshakn,1435433164,,0,4
601,2015-6-28,2015,6,28,4,3bbyh6,"""Super Mario Kart"" played by genetically evolved Neural Networks: SethBling's ""Super MarI/O Kart""",https://www.reddit.com/r/MachineLearning/comments/3bbyh6/super_mario_kart_played_by_genetically_evolved/,AlanZucconi,1435433569,,34,169
602,2015-6-28,2015,6,28,4,3bbyus,Growing Pains for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3bbyus/growing_pains_for_deep_learning/,sqrt,1435433774,,1,0
603,2015-6-28,2015,6,28,4,3bbze2,Advice: Determining to probability a text belongs to class A,https://www.reddit.com/r/MachineLearning/comments/3bbze2/advice_determining_to_probability_a_text_belongs/,CrossfitFTW,1435434071,"I've got a problem where I've got two sets of texts, each belonging to class A or class B. Each has about 2000 documents, each with about 100 words. My task is to rank a set of unseen texts by the probability that they belong to class A.

I've been using scikit-learn's svm, which uses Platt scaling for the probabilities. It works decently, but I'm sure I can do better. 

Any advice? :)",3,1
604,2015-6-28,2015,6,28,6,3bc9o4,Vector to Vector translation- deeplearning,https://www.reddit.com/r/MachineLearning/comments/3bc9o4/vector_to_vector_translation_deeplearning/,oren_a,1435439677,"Hello,
I have a problem and was wondering if I can use deep-learning to solve it. I have a lists of 7 features, and for each list I have 7 scores.

For examples for the features:

[0.2,0.6,0.2,0.6,0.1,0.3,0.1]

I get the following scores:

[100,0,123,2,14,15,2]

_________________________

For examples for the features:

[0.1,0.2,0.3,0.6,0.5,0.1,0.2]

I get the following scores:

[10,10,13,22,4,135,22]

etc..

Any ideas of how to utilize deep learning to train a network that giving a list of features will give me back the correct scores.

Thanks


",4,0
605,2015-6-28,2015,6,28,6,3bcaf8,Chatbots that assess the truth of their statements?,https://www.reddit.com/r/MachineLearning/comments/3bcaf8/chatbots_that_assess_the_truth_of_their_statements/,cymatist,1435440096,Can anyone point me to any examples of Turing test style AIs that monitor the truth value of the things they say? Or even any attempts in that direction? Many thanks in advance for any leads.,8,2
606,2015-6-28,2015,6,28,7,3bci3r,A Neural Conversational Model by Google,https://www.reddit.com/r/MachineLearning/comments/3bci3r/a_neural_conversational_model_by_google/,[deleted],1435444336,"http://arxiv.org/pdf/1506.05869v2.pdf

quite an interesting chatbot they made there.

&gt; Conversation 4: Philosophical Q&amp;A

&gt;Human: what is the purpose of life ?

&gt;Machine: to serve the greater good .

&gt;Human: what is the purpose of living ?

&gt;Machine: to live forever .

&gt;Human: what is the purpose of existence ?

&gt;Machine: to find out what happens when we get to the planet earth .

&gt;Human: where are you now ?

&gt;Machine: i m in the middle of nowhere .

&gt;Human: what is the purpose of dying ?

&gt;Machine: to have a life .

&gt;Human: what is the purpose of being intelligent?

&gt;Machine: to find out what it is .

&gt;Human: what is the purpose of emotions ?

&gt;Machine: i don t know .",19,2
607,2015-6-28,2015,6,28,7,3bcl08,Feeding a game map into a neural network,https://www.reddit.com/r/MachineLearning/comments/3bcl08/feeding_a_game_map_into_a_neural_network/,ekapalka,1435445980,"Hypothetically, I have a robust feedforward neural network structure and a grid-based side scrolling game.  The locations of every block composing the game (with a map size up to 256x256), every object to avoid, every goal, and the player are known (as coordinates).   I'd like to train a neural network to play the game, but I'm not sure how to give it this information (if it's even possible).  

&amp;nbsp;

I'm sure neural networks aren't the best way to go about solving this problem;  training has to take place at the speed of the game and it's going to take a lot of neurons as far as I can tell (like, *a lot* if it ends up going the way I think it will).  I've looked into JSP+ and D* to ""simply"" navigate from goal to goal, but I'd like to see if it's possible to do with neural networks.  I have a fairly high end dedicated machine and months to train if need-be.  

&amp;nbsp;

Is this possible, and if so could someone provide me with a direction to start heading? Thanks! 

&amp;nbsp;

[edit]  Thank you all for your help!  I have a lot of reading and / or hundreds of hours of online classes to get started on",5,0
608,2015-6-28,2015,6,28,9,3bct6c,Deep Learning Lecture on Optimization and LSTM by Alex Smola (CMU),https://www.reddit.com/r/MachineLearning/comments/3bct6c/deep_learning_lecture_on_optimization_and_lstm_by/,ojaved,1435450774,,1,13
609,2015-6-28,2015,6,28,10,3bd0ba,Need suggestions on senior thesis project related to ML,https://www.reddit.com/r/MachineLearning/comments/3bd0ba/need_suggestions_on_senior_thesis_project_related/,gopher_logik,1435455209,"Hi r/MachineLearning,

I'll be a senior this Fall graduating with degrees in math and computer science. I'll be doing a project this fall semester and want to do something machine learning-ish.

I have the following criteria in mind for selection a topic:

* related to ML
* a hot / active topic
* nothing that requires tremendous background knowledge in other area (i.e. particle physics)
* something that I could code up in R and / or Python as an example


I was thinking of something related to neural networks. I am reading How to Create a Mind by Ray Kurzweil, and am really enjoying it. However, I can't come up with an idea to do a project on. 

Thanks for any suggestions!",6,0
610,2015-6-28,2015,6,28,11,3bd6vy,"Papers for Hebbian Learning ""with hardening""",https://www.reddit.com/r/MachineLearning/comments/3bd6vy/papers_for_hebbian_learning_with_hardening/,bklooste,1435459383,"Hi r/MachineLearning,

Im looking at some sort of scheme using Hebbian learning where positive / negative feedback results in hardening / softening the weights to make later changes harder or easier.   As this is pretty obvious it should be covered in some papers does anyone know any ? I'm drowned in all these ANN papers but haven't seen anything like this.",0,2
611,2015-6-28,2015,6,28,21,3bechs,SVD... Who's the new dataset?,https://www.reddit.com/r/MachineLearning/comments/3bechs/svd_whos_the_new_dataset/,calojero,1435496078,"I studied SVD, the math.
I know that it can be used in machine learning for dimensional reduction, so starting from a dataset M we can obtain M' that has the same number of rows, but fewer columns.
Given a dataset M, one can decompose it in:
M=USV*.
Now, who explicitly is M' in terms of U, S, V*?
Thanks. 
",10,0
612,2015-6-28,2015,6,28,23,3beijn,Best Undergrad Degree for Data Science? Need Advice!,https://www.reddit.com/r/MachineLearning/comments/3beijn/best_undergrad_degree_for_data_science_need_advice/,ibtrippindoe,1435500645,"Long term, I plan on pursuing a career in data science, by doing either a masters or PhD in the subject and then going to work in tech/business. I was wondering what people here think about what my best option is for an undergraduate degree. 

I can choose to do pure computer science, or do a combined degree in compsci and mathematics, or compsci and statistics, or Cognitive Systems: Computational Intelligence and Design ( The Cognitive Systems Program is a multi-disciplinary undergraduate program involving 4 departments: Computer Science, Linguistics, Philosophy and Psychology. COGS provides students with a thorough grounding in the principles and techniques used by intelligent systems (both natural and artificial) to interact with the world around them..) If anybody can offer some advice as to which of these would be my best choice to prepare me for a masters or PhD in data science, it would be greatly appreciated. ",18,0
613,2015-6-29,2015,6,29,1,3bezds,A markov bot that generates a tweet for you by using your tweet history give it a try here http://tweety.herokuapp.com,https://www.reddit.com/r/MachineLearning/comments/3bezds/a_markov_bot_that_generates_a_tweet_for_you_by/,ankush_sharma,1435510520,,11,4
614,2015-6-29,2015,6,29,2,3bf44r,Are there general rules that tell you how many support vectors you should have given a sample of say n data points?,https://www.reddit.com/r/MachineLearning/comments/3bf44r/are_there_general_rules_that_tell_you_how_many/,winstonl,1435513045,"I am using a support vector machine to predict a data set with 650 observations. There are roughly 15 covariates, and then end result is to classify whether the observations belong to group A or group B.

I ran a SVM, and ended up just over 300 support vectors (315 I think). Are there general rules that tell you how many support vectors you should have given the data I have, or is that done on a case-by-case basis?",10,14
615,2015-6-29,2015,6,29,2,3bf6g7,Image recognition to learn statistics?,https://www.reddit.com/r/MachineLearning/comments/3bf6g7/image_recognition_to_learn_statistics/,whatmath,1435514319,"Hi /ML.
I am currently choosing between two tracks in my masters. I will take machine learning, advanced machine learning, A.I and Neural networks.

My choice now boils down to either taking applied statistical methods in computer science + time series analysis OR computer vision and image analysis + image based recognition and classification.

My thoughts are that taking statistics and time series can be kinda dull compared to image analysis where you actually do something ""tangible"". BUT my biggest concern is that if I would like to work with data science (i.e. Big data, sorry for the buzz word) I'll lack some skills?


I guess my question boils down to: Will I be able to work as a data scientist if I would become proficient in computer vision/image recognition or does that field have little in common with ""Big Data""?

Thank you or your time reading through this and I hope you understand my concerns!",3,0
616,2015-6-29,2015,6,29,6,3bfw9d,Where can I find open phd positions in ML?,https://www.reddit.com/r/MachineLearning/comments/3bfw9d/where_can_i_find_open_phd_positions_in_ml/,regularized,1435527191,Can you point me out some mailgroups/websites where I can find the announcements of open phd positions?,6,1
617,2015-6-29,2015,6,29,8,3bg7wt,Licensing issues for data?,https://www.reddit.com/r/MachineLearning/comments/3bg7wt/licensing_issues_for_data/,seymour1_research,1435533209,"Hi everyone!

As many of you might know, Kaggle datasets vary in terms of licensing: some are open, some are good for academic use, and some you have to ask the distributor for permission to use for other projects.

This got me thinking: let's pretend that there's a website with a lot of copyrighted images, free to download. Do any of these actions have licensing issues, assuming I'm not able to get in touch with the copyright holder in advance?

1. Using the images to create models, then crediting the website with the dataset used to create them in a journal article

2. Same as 1), but also publishing the processed dataset with features used (not enough information to recreate any images in the dataset)

3. Same as 2), but also publishing image variants generated by a model

4. Using the images to create a model which may be sold as a corporate product

I feel like 1 and 2 are probably okay, while 3 and 4 may not be. However, I have absolutely no basis for this claim, legally or otherwise. Does anyone have any experience, or can anyone link me to any discussions or papers on this subject? I'm also interested in whether similar reasoning applies to other domains, especially written software.",7,2
618,2015-6-29,2015,6,29,10,3bgp7p,Neurogenetic evolution with artificial neural networks,https://www.reddit.com/r/MachineLearning/comments/3bgp7p/neurogenetic_evolution_with_artificial_neural/,asianfrommit,1435542814,,4,0
619,2015-6-29,2015,6,29,11,3bgu52,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/3bgu52/ask_me_anything_dynamic_memory_networks_for/,[deleted],1435545534,,1,4
620,2015-6-29,2015,6,29,13,3bh796,Scientists use genetic algorithms to evolve ANNs. What does nature use to evolve biological neural networks?,https://www.reddit.com/r/MachineLearning/comments/3bh796/scientists_use_genetic_algorithms_to_evolve_anns/,[deleted],1435553495,"*Edit:**

My original post was probably vague. Following some comments and down-votes, I'd like to re-formulate the question:

 - How do biological brains learn?
 - If the mechanism used by biological systems is known: Is it used in Machine Learning? If not, why not?",9,0
621,2015-6-29,2015,6,29,13,3bh7ka,Local CNC &amp; Machining Solutions,https://www.reddit.com/r/MachineLearning/comments/3bh7ka/local_cnc_machining_solutions/,progressmachine,1435553699,,0,0
622,2015-6-29,2015,6,29,14,3bhb4b,Steps Toward Full Visualization of Real-Time Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3bhb4b/steps_toward_full_visualization_of_realtime/,KeponeFactory,1435556143,,8,37
623,2015-6-29,2015,6,29,14,3bhcsi,Has anyone had experience performing PCA or LDA in scikit-learn in order to make predictions?,https://www.reddit.com/r/MachineLearning/comments/3bhcsi/has_anyone_had_experience_performing_pca_or_lda/,[deleted],1435557333,,1,0
624,2015-6-29,2015,6,29,15,3bhey5,"Understanding Proof in ""Generative Autoencoders"" work",https://www.reddit.com/r/MachineLearning/comments/3bhey5/understanding_proof_in_generative_autoencoders/,alexmlamb,1435558876,"Hello, 

I understand how the method from this paper works computationally, and I can see a loose similarity with the Metropolis-Hastings algorithm.  

http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models.pdf

However, I am having trouble understanding why samples from the model will converge to the data generating distribution.  

The main thing that I'm getting hung up on is the claim that the principal eigenvector of the transition operator ""corresponds to the data generating distribution"".  ",0,5
625,2015-6-29,2015,6,29,15,3bhfuw,A Statistical View of Deep Learning: What is Deep?,https://www.reddit.com/r/MachineLearning/comments/3bhfuw/a_statistical_view_of_deep_learning_what_is_deep/,iori42,1435559550,,1,19
626,2015-6-29,2015,6,29,15,3bhgfz,Deep down the rabbit hole: CVPR 2015 and beyond,https://www.reddit.com/r/MachineLearning/comments/3bhgfz/deep_down_the_rabbit_hole_cvpr_2015_and_beyond/,iori42,1435559988,,1,42
627,2015-6-29,2015,6,29,16,3bhkyq,Statistical and causal approaches to machine learning,https://www.reddit.com/r/MachineLearning/comments/3bhkyq/statistical_and_causal_approaches_to_machine/,aranag,1435563795,,0,1
628,2015-6-29,2015,6,29,17,3bhof5,Awesome Deep Vision,https://www.reddit.com/r/MachineLearning/comments/3bhof5/awesome_deep_vision/,kjw0612,1435567066,,0,2
629,2015-6-29,2015,6,29,18,3bhsw5,"Gravel Slurry Pump, Clean Water Pump Manufacturer",https://www.reddit.com/r/MachineLearning/comments/3bhsw5/gravel_slurry_pump_clean_water_pump_manufacturer/,gsslurrypump,1435571144,,0,0
630,2015-6-29,2015,6,29,19,3bhvyi,"How do I store the tree(a decision tree formed using Id3 algo,in java) in main memory or secondary memory?",https://www.reddit.com/r/MachineLearning/comments/3bhvyi/how_do_i_store_the_treea_decision_tree_formed/,rohanpota,1435573808,I have stored the tree earlier in the oracle database for my project but now I would like to store it in the main or secondary to achieve speed up in computation.,1,0
631,2015-6-29,2015,6,29,21,3bi49c,Study Group for ML/Data Science for summer courses,https://www.reddit.com/r/MachineLearning/comments/3bi49c/study_group_for_mldata_science_for_summer_courses/,hamartiated,1435580393,"Hey guys! There was a similar recent post in /r/datascience, but it didn't quite work out in terms of actually building a study group. Feel free to PM me, if you'd like to do some courses together or just to go through a particular curriculum like [the open data science masters](http://datasciencemasters.org/).",7,21
632,2015-6-29,2015,6,29,21,3bi50u,"Newbie question - better to do small training sets and iterate, or begin with larger amounts of data?",https://www.reddit.com/r/MachineLearning/comments/3bi50u/newbie_question_better_to_do_small_training_sets/,hemingwayfan,1435580854,"Hi all, 

I'm working with a lot of data, over 52 million rows, and running into trouble analyzing it on the computing environment I have. 

One option is to simply subset the data, develop an algorithm, and then then continue to test and refine it on other subsets. 

Are there any serious drawbacks to this? 

I feel like the constructionist method (my own phrasing) could be great - iterating until you find something that works. However, I wonder if you get different results with the larger set of data initially. 

TLDR: 
A) Small sets of data, iterate, and refine
B) Large data, let ML sort it all out

Do you get different answers? Has anyone done black and white comparison using both methods? 

Pardon the question, still in the Coursera class level... :) ",5,0
633,2015-6-29,2015,6,29,22,3bi9qg,Brainstorming: Interesting and novel problems (queries) for academic citation networks (e.g. DBLP),https://www.reddit.com/r/MachineLearning/comments/3bi9qg/brainstorming_interesting_and_novel_problems/,hadian,1435583806,"Academic citation networks, such as DBLP, google scholar data, and Microsoft academic graph, contain very useful information that can be mined and extracted in some way. Previous work has extensively focused on a limited number of problems, such as ""overlapping community detection"", ""topic detection"" and ""journal recommendation"".

What else do you think would be interesting to extract from such datasets? I mean, any kind of problem such as ""I want to know that XXXXX"" would be great, e.g. ""which field of research will potentially become hot again after decades of inactivity?""",0,1
634,2015-6-29,2015,6,29,23,3bik7w,Resource allocation and optimization with ML,https://www.reddit.com/r/MachineLearning/comments/3bik7w/resource_allocation_and_optimization_with_ml/,timurlenk,1435589523,"I'm a beginner in the field of ML and I was thinking about the following theoretical problem:

Let's assume we are running a cinema multiplex that has multiple screening rooms. We are showing multiple films at the same time and we want a ML algorithm to come up with a schedule for which screen should show which film and also predict how many ticket/popcorn desks should be open before the show. 

To clarify, the screening rooms have different capacities, pictures have different durations and patrons may behave differently based on which demographic the film targets, etc. 

The algorithm could be fed any kind of data to improve it's predictions, such as IMDB rating, genre of film, reviews, film launches, maybe weather as well as past performance in the real world. Also, the algorithm should be able to learn from operator input (for example the operator would prefer to open screening rooms that are close to each other for easier staffing or close down rooms for refurbishment).

The objective of the algorithm would be to produce a daily schedule for the multiplex, allocating each picture to a room and predicting how long ahead of time should the ticket and popcorn tills be opened in order to ensure that no patrons queue for more than X minutes while also making sure that no staff is sitting idle (multiple tills can be opened at different times).

Do you think this would be a suitable problem for ML? What would be your recommended approach to this problem? I'm thinking about neural networks with some form of evolution. 

The idea is that a sufficiently evolved algorithm would be able to find interesting correlations (let's say for example that certain genres of pictures attract demographics that prefer to pay cash, making the ticket sales a longer process or during winter time patrons arrive earlier at the cinema, etc). 


Note that this is a hypothetical problem (and I know very little about the film business) however I feel that a solution would be applicable to any resource allocation problem where humans are involved (market checkout, festival security lanes and food stands, etc).

Thanks for any pointers.


",1,0
635,2015-6-30,2015,6,30,0,3bim8a,Using Apache Spark DataFrames for Processing of Tabular Data,https://www.reddit.com/r/MachineLearning/comments/3bim8a/using_apache_spark_dataframes_for_processing_of/,caroljmcdonald,1435590489,,1,8
636,2015-6-30,2015,6,30,0,3bioyc,Predicting random number,https://www.reddit.com/r/MachineLearning/comments/3bioyc/predicting_random_number/,veeraman,1435591829,"Is it possible to predict what number I would give, if i give my first 1000 numbers randomly?
I am thinking what sort of algorithm you apply since its not a computer based random number generator, rather a human thinking.
Also, how many minimum numbers an algorithm would need to start predicting?
",4,1
637,2015-6-30,2015,6,30,1,3biyg2,Going even deeper...,https://www.reddit.com/r/MachineLearning/comments/3biyg2/going_even_deeper/,samim23,1435596074,,0,0
638,2015-6-30,2015,6,30,2,3bj4iw,So where is it all headed?,https://www.reddit.com/r/MachineLearning/comments/3bj4iw/so_where_is_it_all_headed/,Chispy,1435598752,"What are some applications for AI in the near future that will enhance our lives in the near future?

 Are we going to use AI as a real time mind augmentation to facilitate information retrieval and enhanced decision making? Will AI know us better than we know ourselves and therefore be useful to make more informed and rational decisions for us?

 What are some useful ways we could take advantage of it to benefit the average person? How would it be integrated? How will it be used with augmented reality?

 I personally don't know that much about machine learning, but I enjoy envisioning future scenarios with the tech. One example is tripping out, similar to a drug induced hallicionation, but with information of our surroundings. For example, walking by a plant and having a vivid 3d representation of a chloryphyll pop out and show me how it creates energy from light. Or when I'm walking in downtown, and can explore every person similar to how you explore them using social media pages, but on a much deeper and more profound level of interaction.

 Are these good ideas? Too simple? Are there other useful applications that not many people know about, and how far do you believe they are?",9,0
639,2015-6-30,2015,6,30,3,3bjasl,PAPIs.io 2015 Tickets Giveaway,https://www.reddit.com/r/MachineLearning/comments/3bjasl/papisio_2015_tickets_giveaway/,czuriaga,1435601433,,0,1
640,2015-6-30,2015,6,30,4,3bjl5m,Question for anyone familiar with Mocha in Julia and the HDF5 format,https://www.reddit.com/r/MachineLearning/comments/3bjl5m/question_for_anyone_familiar_with_mocha_in_julia/,Jonjo9,1435605826,"So I've been trying to learn Julia and I found this convnet tutorial applied to MNIST data - http://mochajl.readthedocs.org/en/latest/tutorial/mnist.html 

It's really useful and straightforward, but then I decided to try and apply this technique to a .csv file (since that's what I most commonly encounter), and I just simply can't figure out how to convert it into the appropriate HDF5 format. Specifically, I wanted to try out this model on the MNIST test set from Kaggle - https://www.kaggle.com/c/digit-recognizer/data which is in a .csv format, but I just hit a roadblock when I try to figure out how to use it in Mocha.

Anyone else have experience dealing with .csv's in Mocha?


Thanks for any help ",2,0
641,2015-6-30,2015,6,30,4,3bjphw,Determine the predictive factor of a variable from a dataset,https://www.reddit.com/r/MachineLearning/comments/3bjphw/determine_the_predictive_factor_of_a_variable/,piscoster,1435607637,"Hello guys,

I was wondering, how to determine HOW predictive a variable from a dataset is?

I appreciate your answer!",4,0
642,2015-6-30,2015,6,30,5,3bjuqi,Rise of the Bots: Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3bjuqi/rise_of_the_bots_deep_learning/,[deleted],1435609844,,1,0
643,2015-6-30,2015,6,30,7,3bk8vh,Help with training recurrent neural networks,https://www.reddit.com/r/MachineLearning/comments/3bk8vh/help_with_training_recurrent_neural_networks/,[deleted],1435616247,"I have built a lstm network that I want to use to classify passages of text. I understand how when activating the network, I am telling it what the whole passage is, but in training it-- how do I tell it when one passage ends and the next one starts.

Suppose I have input:passage1, output:class2 and then in the same training I train input:passage2, output:class4. The network will read all words in the passage and alter the weights, but then the second passage will come in and the weights will be altered again for class4.

What can I do to separate two training input-passages so that one is trained with one class and the next is trained with another class and so on and so forth.

Otherwise all my passages will be trained as one big passage with the last ideal-output / class.",7,0
644,2015-6-30,2015,6,30,9,3bkp3x,AdaBoost Alpha (error) Values.,https://www.reddit.com/r/MachineLearning/comments/3bkp3x/adaboost_alpha_error_values/,[deleted],1435624393,"Hey /MachineLearning,

I was going through the pseudocode for the AdaBoost algorithm, and it's basically just a linear combination of weak classifiers weighted by these alpha values.  The alpha values are equal to 

0.5*Log((1 - e_{t})/e_{t})

where e_{t} is the error rate of the classifier.  But where is this coming from? I've seen the picture and I get the gist of why you'd want a shape like that, but is there some reason to choose this over some other weight?  Does this just happen to give the nicest solutions?

I'm a mathematician by training so it always bothers me when I have no idea where some factor is coming from.  :'(


edit: I can't figure out the LaTeX formatting, so it's in ascii.  Sorry!",0,1
645,2015-6-30,2015,6,30,11,3bkzsn,Creator of Spark on Scaling Machine Learning Roots Beyond Webscale,https://www.reddit.com/r/MachineLearning/comments/3bkzsn/creator_of_spark_on_scaling_machine_learning/,[deleted],1435629809,,0,1
646,2015-6-30,2015,6,30,11,3bl07n,Generalization Bounds for Neural Networks through Tensor Factorization,https://www.reddit.com/r/MachineLearning/comments/3bl07n/generalization_bounds_for_neural_networks_through/,[deleted],1435629997,,0,1
647,2015-6-30,2015,6,30,12,3bl6xr,"Label Consistency for Image Analysis - Bengio, Dean, Le",https://www.reddit.com/r/MachineLearning/comments/3bl6xr/label_consistency_for_image_analysis_bengio_dean/,[deleted],1435633424,,1,0
648,2015-6-30,2015,6,30,12,3bl8f8,Did Google just patent classification?,https://www.reddit.com/r/MachineLearning/comments/3bl8f8/did_google_just_patent_classification/,rantana,1435634204,,71,185
649,2015-6-30,2015,6,30,13,3blfk2,Fitness Landscape Analysis,https://www.reddit.com/r/MachineLearning/comments/3blfk2/fitness_landscape_analysis/,Deterministic-Chaos,1435638059,,0,2
650,2015-6-30,2015,6,30,13,3blfzh,Does anyone know of an easy way to draw convolutional networks (Khrizhevsky style)? Is any other style recommended?,https://www.reddit.com/r/MachineLearning/comments/3blfzh/does_anyone_know_of_an_easy_way_to_draw/,ecobost,1435638318,"with all numbers on the diagrams and all (something like this [example](http://benanne.github.io/images/architecture.png)). I know If I really want to I could even do it in Word, but I guess people already use some kind of software for it. Recommendations are welcome.",4,4
651,2015-6-30,2015,6,30,14,3blki0,Unsupervised Feature Learning and Deep Learning by Andrew Ng,https://www.reddit.com/r/MachineLearning/comments/3blki0/unsupervised_feature_learning_and_deep_learning/,john_philip,1435641144,,1,4
652,2015-6-30,2015,6,30,14,3blknf,Useful Machine Learning Cheat Sheets,https://www.reddit.com/r/MachineLearning/comments/3blknf/useful_machine_learning_cheat_sheets/,dianalennon,1435641235,,1,33
653,2015-6-30,2015,6,30,14,3bll9c,"What is the distinction, if any, between theoretical machine learning and computational learning theory?",https://www.reddit.com/r/MachineLearning/comments/3bll9c/what_is_the_distinction_if_any_between/,Maybenabe,1435641622,,2,0
654,2015-6-30,2015,6,30,15,3blppl,An executives guide to machine learning,https://www.reddit.com/r/MachineLearning/comments/3blppl/an_executives_guide_to_machine_learning/,[deleted],1435644591,,0,0
655,2015-6-30,2015,6,30,15,3bltg7,Sensual Machines - Adult Computer Vision Experiment.,https://www.reddit.com/r/MachineLearning/comments/3bltg7/sensual_machines_adult_computer_vision_experiment/,samim23,1435647396,,1,0
656,2015-6-30,2015,6,30,16,3blx4e,[xpost r/rust] OpenCL+Nvidia GPU Access in Rust via Arrayfire [Help Appreciated!],https://www.reddit.com/r/MachineLearning/comments/3blx4e/xpost_rrust_openclnvidia_gpu_access_in_rust_via/,bge0,1435650352,,0,0
657,2015-6-30,2015,6,30,18,3bm2dc,Which is the dependent variable in the UCI Liver dataset? [x-post stats.stackexchange],https://www.reddit.com/r/MachineLearning/comments/3bm2dc/which_is_the_dependent_variable_in_the_uci_liver/,jmmcd,1435654930,"https://archive.ics.uci.edu/ml/datasets/Liver+Disorders lists some papers which have used this dataset. I went to one, 
[Turney](http://www.jair.org/media/120/live-120-1428-jair.pd), and it says:

&gt; The target concept was defined using the sixth column: Class 0 was defined as drinks &lt; 3 and class 1 was defined as drinks  3
&gt; Column seven was originally used to split the data into training and testing sets.

In support of this, the file [`bupa.names`](http://mlr.cs.umass.edu/ml/machine-learning-databases/liver-disorders/bupa.names) says 

&gt; \7. selector  field used to split data into two sets.

This doesn't sound like a dependent variable.

But I went to another, [Jiang and Zhou](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/isnn04a.pdf) and it says there are 6 attributes (hence, presumably, the seventh is the one they use as the dependent variable).

I also found [this](http://www.researchgate.net/profile/Peter_Rosen6/publication/239666316_The_Impact_of_Data_Perturbation_Techniques_on_Data_Mining/links/5409d44a0cf2d8daaabf98c9.pdf) which claims that the seventh variable is:

&gt; a dependent variable with two levels, indicating presence or absence of a liver disorder

Are a lot of people assuming that the last (seventh) column is the dependent variable, and hence (if they're wrong) doing classification on a completely random variable?

[x-post http://stats.stackexchange.com/posts/159087]",1,0
658,2015-6-30,2015,6,30,18,3bm53l,A Ruby Gem I wrote that does Sentiment Analysis for the German language,https://www.reddit.com/r/MachineLearning/comments/3bm53l/a_ruby_gem_i_wrote_that_does_sentiment_analysis/,nexe,1435657241,"Couldn't find any Ruby Gem that does this for German and a friend needed one so I found some [resources](http://wortschatz.informatik.uni-leipzig.de/download/) and threw together a small Gem since I think it's a shame that Ruby isn't on par with Python and R when it comes to ML. Maybe somebody here might need this too. Feedback appreciated.


[https://github.com/pachacamac/stimmung](https://github.com/pachacamac/stimmung)",4,0
659,2015-6-30,2015,6,30,20,3bmcqi,Introduction Machine Learning using scikit-learn on the Kaggle Otto Challenge,https://www.reddit.com/r/MachineLearning/comments/3bmcqi/introduction_machine_learning_using_scikitlearn/,cast42,1435663615,,0,1
660,2015-6-30,2015,6,30,20,3bme8c,Where can I find....,https://www.reddit.com/r/MachineLearning/comments/3bme8c/where_can_i_find/,nsmithn77,1435664733,Where can I find some of example codes on SVM algorithms for python?,1,0
661,2015-6-30,2015,6,30,21,3bmidq,Visualising High-Dimensional Data (feature reduction using SVD and visualisation using t-SNE),https://www.reddit.com/r/MachineLearning/comments/3bmidq/visualising_highdimensional_data_feature/,cast42,1435667545,,0,13
662,2015-6-30,2015,6,30,23,3bmrlz,Vowpal Wabbit: A Machine Learning System ( x-post r/CompressiveSensing),https://www.reddit.com/r/MachineLearning/comments/3bmrlz/vowpal_wabbit_a_machine_learning_system_xpost/,compsens,1435672841,,0,7
663,2015-6-30,2015,6,30,23,3bmvj0,"Renormalization group theory on Social networks, how good is it?",https://www.reddit.com/r/MachineLearning/comments/3bmvj0/renormalization_group_theory_on_social_networks/,Professional_123,1435674700,I'm just wondering if any attempts have been made to predict the exponents of power-laws and whether it can predict the average degrees of separation (folklore says it's 6). I do know a bit about renormalization group from physics so I'm very eager to know! Thanks.,5,1
664,2015-6-30,2015,6,30,23,3bmxqh,What kind of neural network to use as regression model?,https://www.reddit.com/r/MachineLearning/comments/3bmxqh/what_kind_of_neural_network_to_use_as_regression/,krimtheguy,1435675699,"Hi, 

I'm doing some research on a dataset and will have to apply regression on it, to predict some costs.

Already having the cost as a term between 0 and 1, want to use several models to get the prediction (already have prepared Linear Regression and RandomForest) and will try to do something like an ensemble. Can anyone point me the direction to use a Neural Network that outputs a real value (that value between 0 and 1 so I can re-scale lately)? All I can find out there (for python using Keras, pybrain, Lasagne...) focus on classification not on regression.

EDIT: The problem is that I don't want to predict with certain probabilities some classes, all I want is a single float value between 0 and 1 that after re-scaling will give me the predicted cost

Thanks in advance",5,0
