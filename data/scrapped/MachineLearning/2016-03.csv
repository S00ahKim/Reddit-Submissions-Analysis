,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2016-3-1,2016,3,1,9,48dl7y,AWS vs buying your own machines ?,https://www.reddit.com/r/MachineLearning/comments/48dl7y/aws_vs_buying_your_own_machines/,butWhoWasBee,1456793307,"I have seen questions like this asked a few times here, but the answers seem to differ. I'm at a point where I will need to spend money to continue my research in one way or the other (my datasets are big, and training networks is starting to take weeks). What do you guys think is the better deal for $2k - $5k, renting as much AWS as you can for the money, or buying your own server? Why do you think so? Does the answer change at a certain price range? If so what do you think are the cutoffs for one option or the other? ",17,3
1,2016-3-1,2016,3,1,10,48dsfv,Clustering Libraries in Java,https://www.reddit.com/r/MachineLearning/comments/48dsfv/clustering_libraries_in_java/,chick3234,1456796020,"What clustering library do you use in Java? Does it have kmeans? Does it allow you to choose a distance metric for all of its clustering algorithms? Why do you use it?

I'm just curious and would very much would like a recommendation!

Thank you in advance!",1,2
2,2016-3-1,2016,3,1,10,48duli,The Promise of Artificial Intelligence Unfolds in Small Steps,https://www.reddit.com/r/MachineLearning/comments/48duli/the_promise_of_artificial_intelligence_unfolds_in/,julian88888888,1456796855,,0,0
3,2016-3-1,2016,3,1,12,48e7o0,Looking to speak with a Machine Learning research at University of Illiniois UC campus.,https://www.reddit.com/r/MachineLearning/comments/48e7o0/looking_to_speak_with_a_machine_learning_research/,mobdoc,1456802020,"Sounds like a personal add but I'm looking to speak with a university of Illinois researcher over coffee or a beer to help better understand how it may be used in my field. 

- 3D/volumetric medical datasets
- information extraction - medical records
- auditing and decision making

I have emailed individuals here at uiuc but no response yet and I am only here for a week. 

Thanks in advance. ",4,2
4,2016-3-1,2016,3,1,12,48e7vq,Question - Theano gradient diff then when computed with numpy,https://www.reddit.com/r/MachineLearning/comments/48e7vq/question_theano_gradient_diff_then_when_computed/,[deleted],1456802110,[deleted],0,1
5,2016-3-1,2016,3,1,12,48e9c7,Wire Straightening Machine,https://www.reddit.com/r/MachineLearning/comments/48e9c7/wire_straightening_machine/,veermachine,1456802702,,1,0
6,2016-3-1,2016,3,1,16,48f5o0,"Big Data, Analytics and IoT Conferences in March 2016",https://www.reddit.com/r/MachineLearning/comments/48f5o0/big_data_analytics_and_iot_conferences_in_march/,Futureofanalytics,1456817729,,0,0
7,2016-3-1,2016,3,1,16,48f5r4,"Pass a URL, get summarized content back in JSON",https://www.reddit.com/r/MachineLearning/comments/48f5r4/pass_a_url_get_summarized_content_back_in_json/,[deleted],1456817779,[deleted],6,11
8,2016-3-1,2016,3,1,17,48fayx,"Ruler Tampo Printing Machine From China, mquina de tampografa regla from China Factory",https://www.reddit.com/r/MachineLearning/comments/48fayx/ruler_tampo_printing_machine_from_china_mquina/,printersupplier,1456821032,[removed],0,1
9,2016-3-1,2016,3,1,19,48fplg,This AI tells you where to invest your money,https://www.reddit.com/r/MachineLearning/comments/48fplg/this_ai_tells_you_where_to_invest_your_money/,rlanham1963,1456829232,,1,0
10,2016-3-1,2016,3,1,20,48fuu2,Is stepwise regression still controversial?,https://www.reddit.com/r/MachineLearning/comments/48fuu2/is_stepwise_regression_still_controversial/,mlnewb,1456832386,"I'm doing some work with genomics style data, and my (very senior, world-respected) professor has suggested I use stepwise regression to select my variables of interest. 

It is an exploratory paper, so there are only few observations and very many variables (2-3 orders of magnitude difference). 

I am not a statistician, but I have seen in many places that stepwise regression has been bagged as data dredging. Yet it seems to be in widespread use in genome wide association studies, with harsh p-value constraints like bonferroni correction employed to counteract the obvious flaws of the method.

All of the complaints I have read seem to be in the mid 2000s, and it doesn't seem to be on the radar as much anymore. Have the naysayers simply ceded the argument, or is it still very controversial?

I am trying something quite novel, so I have very little in the way of pre-existing evidence to compare my results to, nor do I have any significant cache of domain knowledge to predict what result might be reasonable. I am worried I am just going to find lots of spurious correlations.

Obviously I am taking the exploratory paper results with a grain of salt, but I don't want to publish something I will have to retract later.

Any thoughts?",24,6
11,2016-3-1,2016,3,1,20,48fwez,"Inside the Artificial Intelligence Revolution: A Special Report, Pt. 1",https://www.reddit.com/r/MachineLearning/comments/48fwez/inside_the_artificial_intelligence_revolution_a/,tuntuku2,1456833276,,0,0
12,2016-3-1,2016,3,1,21,48fyuf,"New Google-Net model crushed the previous record on Imagenet dataset, with 3.08% top-5 error.",https://www.reddit.com/r/MachineLearning/comments/48fyuf/new_googlenet_model_crushed_the_previous_record/,[deleted],1456834666,[deleted],1,1
13,2016-3-1,2016,3,1,21,48g2h7,meta-SNE vs t-SNE What's the difference?,https://www.reddit.com/r/MachineLearning/comments/48g2h7/metasne_vs_tsne_whats_the_difference/,8queens,1456835544,I read Chris Olah's blog on visualizing representations but I couldn't really understand what meta-SNE does and how it's different from t-SNE,5,6
14,2016-3-1,2016,3,1,22,48gde4,Ladder Network questions,https://www.reddit.com/r/MachineLearning/comments/48gde4/ladder_network_questions/,LyExpo,1456838691,"I'm trying to understand how these work from an implementation level. My understanding is that if I have labeled data (x_1,y_1), ..., (x_m,y_m) and unlabeled data x_(m+1), ..., x_n, then I can somehow do better by using all of this data, instead of just using the labeled data.

I'm a bit confused as to what changes from the case where we have a label, to the case where we don't. [Here](http://rinuboney.github.io/img/ladder_net.png) is a picture of the network. If we don't have the label y, how is the cost function computed? Is the idea that we only compute the feed forward cost when we can, and in that case we simply add this cost to the ladder costs? This ""labeled cost"" is therefore 0 when we don't have a label?

My other question is more general: if my labeled data is the data set I would usually train on, and my unlabeled data set is the set I would like my classifier to generalize on, can I train the ladder network (or any semi-supervised framework) using both of these sets? After training, I should be able to pass my (unlabeled but already seen by the network) data into the network for better results?",6,8
15,2016-3-1,2016,3,1,22,48gejf,I'm completely new to machine learning. What is this problem I'm looking at?,https://www.reddit.com/r/MachineLearning/comments/48gejf/im_completely_new_to_machine_learning_what_is/,ml_noob_700,1456839183,"I have a set of signals. Those signals can be ""on"" or ""off"" at a single point in time. Then, after some time, I'll know the answer to a ""yes/no"" question which is related to those signals.

For example, the signals could be: 

- Is there someone on the kitchen?

- Is it between 13 and 14 pm?

- Is it a weekday?

And the answer would be wether there will be food on the table at 14pm. Also, I have a giantic database of example scenarios. I want a software that predicts the answer ""yes/no"" based on those signals. As a complete noob, I have no idea what this problem is called and what kind of software I could use to solve it. Could you guys help me out?",1,2
16,2016-3-1,2016,3,1,22,48gfop,TensorFlow speed questions,https://www.reddit.com/r/MachineLearning/comments/48gfop/tensorflow_speed_questions/,r4and0muser9482,1456839671,"I know this is a well known issue that was discussed many times, but I just can't seem to find any real answers online anywhere. There are benchmarks for some very specific things (CNNs usually compared to Torch), but I think there is a much more basic problem here and I'd like to know if I'm doing something wrong or is TF simply that slow.

I was doing some simple MLPs/RNNs for speech recognition (on TIMIT) and noticed that the TF version of a single hidden layer MLP is almost 10 times slower than the Keras or even raw Theano version. So I decided to do a simple test and I found this really neat project on github: [TensorFlow tutorial](https://github.com/nlintz/TensorFlow-Tutorials). The same project claims to be a re-implementation of the same models written in Theano in this project: [Theano Tutorials](https://github.com/Newmu/Theano-Tutorials).

Now these are really simple and easy examples. They are practically identical and should do the exact same thing. So I downloaded them and ran them both on my computer (on a single K80 card) and this is what I got:

Script | Theano | TensorFlow
--------|------------|----------------
2_LogReg | 27s | 2m57s
3_Net | 34s | 3m20s
4_ModernNet | 1m22s | 4m13s
5_ConvNet(10eps) | 7m35s | (TL;DR)

Someone mentioned that Google is all about CPUs, so I thought maybe running it on a CPU would be better, but I got this on a double Xeon E5-2650 v2:

Script | Theano | TensorFlow
--------|------------|----------------
1_LinReg | 0.9s | 5.27s
2_LogReg | 26s | 5m51s
3_Net(10eps) | 29s | 1m17s
4_ModernNet(10eps) | 1m30s | 3m1s

Now these are all really strange and inconsistent results, but TF comes out as considerably worst than Theano in all the tests above.

Can anyone maybe confirm if this is really how TF works compared to Theano or do I have some error in my setup somewhere? 

The projects above are really easy to use. I had to do some minor modifications in the paths of the Theano one, but apart from that they all work right out-of-the-box.
",26,49
17,2016-3-1,2016,3,1,22,48ggjv,Adaptive SGD for fine-tuning of ImageNet-size CNN?,https://www.reddit.com/r/MachineLearning/comments/48ggjv/adaptive_sgd_for_finetuning_of_imagenetsize_cnn/,shmel39,1456840026,"I am trying to use adaptive SGD variants (like adam, adadelta) to finetune VGG16 (I took only pretrained conv weights). However, I have some issues presumably because some layers are randomly initialized (He init if it matters). 

Adadelta explodes after a few iterations, Adam also makes big jumps (so big loss values) in the beginning, but manages to return to normal later. However, I am afraid that such jumps are bad for the further training. Ordinary SGD+momentum works just fine, but I doubt I can make up good learning rate decay schedule (any tips are welcome though).

I tried some obvious ideas: begin training with SGD+momentum (fixing or not VGG weights), then switch to Adam, but I have the same problem around the time of switching.

I use TensorFlow now, but I had the same problem with Theano/Lasagne.",6,2
18,2016-3-1,2016,3,1,22,48ggz1,"How should I approach this problem? (Python, sklearn, NLP)",https://www.reddit.com/r/MachineLearning/comments/48ggz1/how_should_i_approach_this_problem_python_sklearn/,Hamush,1456840149,"So I'm trying to basically construct my own collection of words with the same meaning ish. Basically my goal is to look at different sentences with the same meaning, and extract the difference. Elaboration below.


For example, if I feed the machine learning algorithm three sentences:

It is imperative to do that

It is very important to do that

It is necessary to do that

It will understand that the words

Imperative,

Very important,

And necessary

can be used interchangeably in most sentences


Now for a specific python translation, when I pass the word imperative into an estimator's predict() function, it will return: [very important, necessary]

Or something that will inevitably lead me to get these answers.


Now to the question (finally!)


How should I approach this problem most effectively?


To narrow that down, I can, say feed data into a kmeans estimator, but how exactly do I vectorize it so that I get the wanted results? The only ways I know how to prepare data are through the CountVectorizer() and TfidfVectorizer() classes, neither of which give me what I want - which is to vectorize a sentence by each word and its surroundings. Example below.


e.g. if we use the aforementioned sentences:

It is imperative to do that

It is very important to do that

It is necessary to do that

Maybe we map the phrase It is to a 0. The phrase to do that to 2. And everything in between to a 1. That way we can see that all important and synonymous words/phrases are a 1, and can be used interchangeably.


Keep in mind, I'm a proven novice in machine learning, my last example may not even be correct at all, but I'd still appreciate any relevant feedback. I'm obviously willing to try new libraries and methods, but in terms of experience, I am familiar with sklearn best.


Thank you all for reading this far, and for trying to help me out!",12,3
19,2016-3-1,2016,3,1,23,48glec,I am grateful - not inclined to carry away in my expectations for life. I am sensitive and have a big appetite for life. vn8hb7,https://www.reddit.com/r/MachineLearning/comments/48glec/i_am_grateful_not_inclined_to_carry_away_in_my/,progesam,1456841548,,0,1
20,2016-3-2,2016,3,2,0,48gvzs,Study suggests humans and computers use different processes to recognize objects visually,https://www.reddit.com/r/MachineLearning/comments/48gvzs/study_suggests_humans_and_computers_use_different/,liav1,1456845247,"New [research] (http://www.wisdom.weizmann.ac.il/~dannyh/Mircs/mircs.html) suggests that there is an atomic unit of recognition  a minimum amount of information an image must contain for recognition to occur.
The findings of this study, which recently appeared in the [Proceedings of the National Academy of Science (PNAS)] (http://www.pnas.org/content/early/2016/02/09/1513198113.full.pdf) imply that current models need to be adjusted, and they have implications for the design of computer and robot vision.
Also in [arstechnica] (http://arstechnica.com/science/2016/02/tiny-blurry-pictures-find-the-limits-of-computer-image-recognition/), [Spectrum] (http://spectrum.ieee.org/tech-talk/computing/software/digital-baby-project-aims-for-computers-to-see-like-humans) and [Fortune] (http://fortune.com/2016/02/20/pnas-computer-vision-study/).",3,0
21,2016-3-2,2016,3,2,0,48gy2k,Representation of linguistic form and function in recurrent neural networks,https://www.reddit.com/r/MachineLearning/comments/48gy2k/representation_of_linguistic_form_and_function_in/,SuperFX,1456846000,,0,4
22,2016-3-2,2016,3,2,0,48gzkr,Software for Ground-Truth Annotation?,https://www.reddit.com/r/MachineLearning/comments/48gzkr/software_for_groundtruth_annotation/,senorstallone,1456846562,"I'm looking for a sw to label some images. Classifying them in different aspects. Which tool is the most indicated to do this?

(It seems like an easy query in google, but I couldn't find anything interesting)
",8,7
23,2016-3-2,2016,3,2,0,48h0c1,Google's Computers Are Making Thousands as Artists,https://www.reddit.com/r/MachineLearning/comments/48h0c1/googles_computers_are_making_thousands_as_artists/,smith2008,1456846835,,20,62
24,2016-3-2,2016,3,2,1,48h8u7,Notes on Pixel RNN by Hugo Larochelle,https://www.reddit.com/r/MachineLearning/comments/48h8u7/notes_on_pixel_rnn_by_hugo_larochelle/,[deleted],1456849761,[deleted],0,1
25,2016-3-2,2016,3,2,1,48hbnw,Notes on Pixel RNN by Hugo Larochelle,https://www.reddit.com/r/MachineLearning/comments/48hbnw/notes_on_pixel_rnn_by_hugo_larochelle/,unrollrnn,1456850700,,6,34
26,2016-3-2,2016,3,2,1,48hcht,Tensorflow Cifar-10 evaluation error: Enqueue operation was cancelled - How to fix?,https://www.reddit.com/r/MachineLearning/comments/48hcht/tensorflow_cifar10_evaluation_error_enqueue/,AwesomeDaveSome,1456850988,"I'm trying to get the sample code of tensorflow for Cifar-10 to run on my own data set. The training works fine (after asking this sub repeatedly, I've finally managed to get the code working without any overflow or other problems). However, the evaluation stucks somewhere, I've tried to read up solutions, but I fail to understand them properly. Whenever I run the evaluation (just the evaluation code from the Cifar-10 tutorial, only things changed are the paths to the datasets), I get the following error:

tensorflow/core/common_runtime/executor.cc:1102] 0x7fa6065462d0 Compute status: Cancelled: Enqueue operation was cancelled
	 [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:

This error repeats a couple of times (in the output, say 10 times), and then a new message pops up: I tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt


I've googled this error, and one suggestion was to (quote): ""use a variable and make a separate call to sess.run to initialize from the placeholder before starting the queuerunners."" However, since in the cifar-10 tutorial they never initialize any variables, or has any placeholders, I fail to understand which of the variables I would initialize before starting the queuerunners, and where in the code I would do that (I'm still very new to tensorflow and basically learning along the way of coding Cifar-10, so I'm a bit lost). Can anybody help me out with this?",2,0
27,2016-3-2,2016,3,2,2,48hjk7,Need some help understanding Neural Net Activation and Loss functions,https://www.reddit.com/r/MachineLearning/comments/48hjk7/need_some_help_understanding_neural_net/,chain20,1456853346,"Hi,

I used to think I understood activation and loss functions well enough, but some recent observations have got me wondering whats going on.

I am trying to train network of stacked Autoencoders using Lasagne, each trained in a layerwise fashion, before fine-tuning the entire network. I am experimenting with two image datasets, the MNIST dataset and a second dataset of 1900 face images, 140x190 in size (a binary classification task).

With both these datasets, the Autoencoders were trained using ReLU activations during pretraining. For finetuning, I used ReLU units in the hidden layers and softmax output for MNIST and a sigmoid unit for the face images.

Here is my problem: Using cross-entropy as a loss function works for MNIST images, but the gradient does not exist for the face dataset (I get a NaN when I try to compute the gradient). However, changing the hidden layers to sigmoid activation gives a nice gradient during fine tuning for the face images.

Can someone please help me understand what is going on? Why does one activation function work for a particular dataset but not for a different one? Probably I am not doing something right, but I can't see it.

Let me know if I should provide more details about my network. Any help is appreciated.",2,0
28,2016-3-2,2016,3,2,3,48hxbh,One More Reason Not To Be Scared of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/48hxbh/one_more_reason_not_to_be_scared_of_deep_learning/,amplifier_khan,1456857773,,10,2
29,2016-3-2,2016,3,2,4,48iabq,Deep Learning with Python video course (Theano and Keras),https://www.reddit.com/r/MachineLearning/comments/48iabq/deep_learning_with_python_video_course_theano_and/,edersantana,1456862209,,0,3
30,2016-3-2,2016,3,2,5,48ih4l,Understanding Aesthetics with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/48ih4l/understanding_aesthetics_with_deep_learning/,harrism,1456864645,,3,15
31,2016-3-2,2016,3,2,7,48j07i,Dataset analisys,https://www.reddit.com/r/MachineLearning/comments/48j07i/dataset_analisys/,Mambalera_00,1456871397,"Hello guys, for a task I'm given a dataset for which I have to train a classifier to classify another dataset taken from the same population. I have to reach at least 85 % of accuracy using any of the methods studied during the course: PCA, kNN, feature selection, neural network. I tried EVERYTHING but I can't get more than 75 % (gotten by applying PCA to reduce dataset to 4 features and then using kNN with k = 9). Could you please suggest anything else?
P.s. of course I have to implement the whole thing in matlab.

LINK TO DATASET: https://app.box.com/s/3jbylvbc5rj4q90ajsghc3d7i1dama6c",0,1
32,2016-3-2,2016,3,2,8,48j53p,DanDoesData: Building CIFAR10 model with skflow,https://www.reddit.com/r/MachineLearning/comments/48j53p/dandoesdata_building_cifar10_model_with_skflow/,vanboxel,1456873205,,0,0
33,2016-3-2,2016,3,2,8,48j6bu,Smarter WiFi,https://www.reddit.com/r/MachineLearning/comments/48j6bu/smarter_wifi/,smoledman,1456873658,"Imagine if your WiFi module on a smartphone had a capability to create a spatial map in real-time of a WiFi network as you physically traversed the building. Of course the map would always be in progress since you are going around it and not necessarily on all floors and not in all possible locations. An imperfect map.

Currently all smartphone OSes have a flaw in the WiFi software that it connects and disconnects to WiFi as soon as you walk in range or out of range. 

Now imagine this Smart-WiFi had the capability of applying machine learning to the spatial map and your own movements to make a judgement to simply turn off WiFi in favor of Cellular for say 30 minutes because it knows your movement pattern. 

I wouldn't be shocked if Apple/Google were working on something like this utilizing beacon tech.",0,0
34,2016-3-2,2016,3,2,9,48jfj1,"Have a real world problem that I can apply data science/machine learning tools to, but barely beginner level skills. Advice?",https://www.reddit.com/r/MachineLearning/comments/48jfj1/have_a_real_world_problem_that_i_can_apply_data/,fear-of-flying,1456877100,"Knowledge background: Experienced (to some degree...) software developer. Pretty good math skills (undergrad + a few grad courses). Have taken some online ML courses.

Problem background: Essentially, detecting web traffic patterns/fraud (or, more generally just figuring out which web traffic is real and valuable, and which isn't). It would also be awesome if the resulting model could explain to some degree how it reached its conclusion (so... decision trees?). The web traffic here is primarily advertising related: ad impressions, clicks, conversions, etc. These are only really valuable if there is a real person looking at the ad, and if clicking/interacting with it, only if they have real interest in the ad. Clicks are much more valuable than impressions because there is at least some intent signaled by the user. The problem is that there are tons of sources of these events that aren't actually valuable to the advertiser -- bots, fraud, weird user behavior (like clicking an ad twice on accident -- not fraud but not valuable either) -- and I want to be able to figure out which is which.

* EDIT: More details. 

I've created a very naive system for this that uses a really limited statistical model plus a few simple business rules. It works ok, but isn't robust enough for the long term. I have also read two papers on building systems like this, but their methods were outdated/questionable and didn't directly apply. I also want to try to create something from first principals (but not ignoring history entirely).

### Where I'm at

At first, it seemed like a classification problem. The issue is it can be very difficult to actually determine which traffic is valid/valuable and which is invalid for creating a training set. There are a few behaviors that are obviously bad (hence the business rules), but beyond that it isn't something an unaided human can do (in my opinion).

I am now considering taking a step back and applying some unsupervised learning techniques to at least try and better understand some of the underlying patterns in the data, but I am really not sure where to go from there or which ones would be best to start with.

The good news is that the data is in pretty good shape, and I should be able to easily create whatever derivative features are needed for the entire data set.

Any advice or thoughts would be awesome. Also, I am very willing to put in the time to learn more of the basics if that is what is required.",5,5
35,2016-3-2,2016,3,2,9,48jg8n,Predicting US Primary Election Results,https://www.reddit.com/r/MachineLearning/comments/48jg8n/predicting_us_primary_election_results/,TheRedfather,1456877386,,2,1
36,2016-3-2,2016,3,2,9,48jn2d,Linear regression with non-symmetric cost function?,https://www.reddit.com/r/MachineLearning/comments/48jn2d/linear_regression_with_nonsymmetric_cost_function/,ShittyPhotographs,1456880153,"I try to solve a problem where I want to predict some value Y(x).
I am trying to get some prediction (x) that optimizes between being as low as possible, but still being larger than Y(x). In other words:
cost( Y(x) ~&gt; (x) )  &gt;&gt; cost( (x) ~&gt; Y(x) )  
( is that understandable? )

I think a simple linear regression should do totally fine.
So I somewhat know how to implement this manually, but I guess I'm not the first one with this kind of problem. Are there any packages/libraries (preferably python) out there doing what I want to do?
What's the keyword I need to look for?


follow-up question: What if I know a function Y_0(x) &gt; 0 where Y(x) &gt; Y_0(x). Can you implement these restrictions in a simple linear regression?",11,0
37,2016-3-2,2016,3,2,11,48k0as,How to know which algorithm/approach to use and when?,https://www.reddit.com/r/MachineLearning/comments/48k0as/how_to_know_which_algorithmapproach_to_use_and/,Gay_Hat_On_Nun,1456885386,"Hello, I had a question regarding picking the right algorithm or approach for the job (specifically supervised learning, although I'd assume the question could abstractly apply to others too). Given the fact that there are so many types of algorithms to solve problems with datasets, how do you typically know which approach is best? I understand that they all have their pros and cons and it's just a matter of understanding strengths and weaknesses of them all, but sometimes I do not even recognize what is viable and what isn't until I've already started coding an implementation. For example, I was recently trying to use a simple linear regression algorithm on a problem I was working on, but halfway through I realized that neural networks would work better. I feel that had I recognized and made the connection that a neural network would be better suited, I would have saved myself a lot of time. Part of my issue is that I do not consciously taken into consideration each approach that I have learned. I was thinking maybe I could make a master list of approaches I've learned and use it as a cheat sheet, but that doesn't really solve my issue; it acts too much of a crutch for me.

Also, leading off of this is my other part to the question: if I pick an algorithm less suited for the task, it may not be as efficient or optimized, but given enough tuning and tweaking would it be able to work as well as a more suited algorithm? (Obviously two drastically different approaches would not work very similar to each other regardless of tweaking and tuning, but if I've already started one approach to the problem and I realize partially through that there's a better way to do this, what should I do...? (As I'm typing this I realize that this is very situational, but any tips would be helpful nonetheless.))

Thanks!",3,0
38,2016-3-2,2016,3,2,11,48k1vd,AI Projects like AI Birds?,https://www.reddit.com/r/MachineLearning/comments/48k1vd/ai_projects_like_ai_birds/,nickdean16,1456885966,"Hello, I am currently working on a Computer Vision project to play a video-game by observing the game as a human would (By successive screenshots), similar to how [AI Birds](https://aibirds.org/) is done.

I am having a lot of trouble finding any papers or projects similar to this, except Google DeepMind who are currently using Computer Vision [to traverse Doom-like levels](https://www.newscientist.com/article/2076552-google-deepmind-ai-navigates-a-doom-like-3d-maze-just-by-looking/?utm_source=NSNS&amp;utm_medium=SOC&amp;utm_campaign=hoot&amp;cmpid=SOC%7CNSNS%7C2016-GLOBAL-twitter), and AI Birds.

Does anyone know what a good term for this topic would be and do you know of any relevant papers published on the topic?

Thanks!",2,3
39,2016-3-2,2016,3,2,15,48l0jv,Is cuDNN (Cuda based deep learning) available for CUDA 6.5?,https://www.reddit.com/r/MachineLearning/comments/48l0jv/is_cudnn_cuda_based_deep_learning_available_for/,[deleted],1456901316,[deleted],4,0
40,2016-3-2,2016,3,2,16,48l6oi,Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network [arxiv],https://www.reddit.com/r/MachineLearning/comments/48l6oi/hierarchical_conflict_propagation_sequence/,ajrs,1456904883,,6,0
41,2016-3-2,2016,3,2,17,48l7xy,First Steps With Neural Nets in Keras,https://www.reddit.com/r/MachineLearning/comments/48l7xy/first_steps_with_neural_nets_in_keras/,swanint,1456905685,,1,2
42,2016-3-2,2016,3,2,17,48l857,Determining size and number of hidden layers,https://www.reddit.com/r/MachineLearning/comments/48l857/determining_size_and_number_of_hidden_layers/,FutureIsMine,1456905804,"What is a good method for figuring out how large to make each hidden layer within a neural network, and how many hidden layers to include overall in a model? ",3,0
43,2016-3-2,2016,3,2,18,48ldrp,x-post from /r/science,https://www.reddit.com/r/MachineLearning/comments/48ldrp/xpost_from_rscience/,georgeo,1456909595,,9,0
44,2016-3-2,2016,3,2,18,48lhqa,"Datasets, the gold of Machine Learning",https://www.reddit.com/r/MachineLearning/comments/48lhqa/datasets_the_gold_of_machine_learning/,recastai,1456912136,,1,0
45,2016-3-2,2016,3,2,19,48lki3,Has anyone actually bought the Nvidia DevBox with 4 x TITAN GPUs? Is there any special reason I would buy one of these things instead of just building it myself?,https://www.reddit.com/r/MachineLearning/comments/48lki3/has_anyone_actually_bought_the_nvidia_devbox_with/,syncoPete,1456913916,"There doesn't seem to be anything special about the DevBox hardware other than that they've rigged it with 4 GPUs. If I bought a desktop machine with enough slots, I feel like I could insert the GPUs myself and save $7,000.

Has anyone bought one of these things and is there any special reason that I would consider buying one?",33,18
46,2016-3-2,2016,3,2,19,48lldq,Quake 3's hidden feature.,https://www.reddit.com/r/MachineLearning/comments/48lldq/quake_3s_hidden_feature/,live4lifelegit,1456914443,,84,158
47,2016-3-2,2016,3,2,20,48ls6w,"Python Scikit Learn: For each classification that I do, how do I get to know which features in my text helped it to pick that class?",https://www.reddit.com/r/MachineLearning/comments/48ls6w/python_scikit_learn_for_each_classification_that/,mln00b13,1456918655,"After I have trained my classifier, I can get the most important features for each class using feature_importances_ function of RandomForest classifier. But I want that for each prediction which I perform, I want to know the most important features which helped it pick that class? How do I do that?",8,2
48,2016-3-2,2016,3,2,20,48ltd2,How to deal with variable input lengths to 1D CNN's?,https://www.reddit.com/r/MachineLearning/comments/48ltd2/how_to_deal_with_variable_input_lengths_to_1d_cnns/,theophrastzunz,1456919347,"I'm new to Deep Learning, but I'd like try using CNN's to detect events in a 4-D time series. For fixed time lengths this would be easy, but how would I go about applying it to varying time length sequences?",5,0
49,2016-3-2,2016,3,2,21,48lv7c,correlation between KullbackLeibler (KL) divergence and JensenShannon (JS) divergence?,https://www.reddit.com/r/MachineLearning/comments/48lv7c/correlation_between_kullbackleibler_kl_divergence/,gameofml,1456920390,"I am wondering if there is a correlation between JSD and KLD (except the fact that JSD is derived from KLD and JSD is symmetrical), for example if JSD(p,q) is decreased, whether KLD(p||q) will decrease as well?

This question comes from the comparison between the variational objective and the adversarial objective in training deep generative models.",6,8
50,2016-3-2,2016,3,2,22,48m1il,Decision Trees for Text,https://www.reddit.com/r/MachineLearning/comments/48m1il/decision_trees_for_text/,0one0one,1456923821,"I considering implementing a decision tree for classifying text examples into binary categories (is or is not an example of). I have implemented a small example (weather dataset) and think that I have a good grasp of the concepts of entropy and information gain , however I am a little hazy as to how one would preprocess text so that the attributes are suitable for decision trees. 

Obviously some counting of the text will be required, but how would one represent the documents ? Any suggestions or links would be appreciated. ",5,0
51,2016-3-2,2016,3,2,22,48m7un,Is there anyone having an idea how the active tracking of the DJI phantom 4's new feature?,https://www.reddit.com/r/MachineLearning/comments/48m7un/is_there_anyone_having_an_idea_how_the_active/,jaehyunlim0606,1456926980,"You guys might see the recent posts about the DJI's new product and its new feature called active tracking from various sources. (https://www.facebook.com/thenextweb/videos/10153937885588523/)

As far as read some other articles about it, they seems like to use camera for tracking and sonar for obstacking avoidance. 

I just wonder what kind of tracking solutions with reliable performance can be loaded on drone. 

As far as I know the best known solution for tracking (w/ RGB) uses deep learning as we can see from the competition held in ICCV2015(?).

Is there anyone have any idea what can the solution in the DJI's drone? ",6,0
52,2016-3-2,2016,3,2,23,48ma42,"How to use Azure Machine Learning to predict the results of the March Madness tournament - improve your bracket! [video, less than 20 min]",https://www.reddit.com/r/MachineLearning/comments/48ma42/how_to_use_azure_machine_learning_to_predict_the/,jennifermarsman,1456927946,,0,0
53,2016-3-2,2016,3,2,23,48mft0,What does neon Deep Learning Framework do?,https://www.reddit.com/r/MachineLearning/comments/48mft0/what_does_neon_deep_learning_framework_do/,drpout,1456930337,"Hi All,

I wanted to know what exactly neon is and how nervana GPU is so fast, as seen by Soumith's benchmarks and the recent Baidu post. 

How can you perform an operation in Python front end and have something so fast? Is there a catch I'm missing here? 

I am hoping that someone with real experience in this matter explain what exactly happens when we call a series of `.dot` operations for example. 

Also, based on a cursory glance, they ecosystem seems really good with caffe model conversion, large number of relavent pre-trained models and good support team. Yet, the adoption is very very less. Again, is there something that I'm missing here?

Regards,
drpout",14,4
54,2016-3-3,2016,3,3,0,48mkmx,Can any ML expert have a quick look in my training set?,https://www.reddit.com/r/MachineLearning/comments/48mkmx/can_any_ml_expert_have_a_quick_look_in_my/,akiskall,1456932155,"Hello,

I've gathered some data (sports related) through web scraping based on metrics that i found in some old papers and right now i'm trying with weka to see if this training set can actually predict the outcome of the event.

Unfortunately, i can't get good results but at the same time i'm not a ML expert. 

That's why i would really appreciate if any of you could have a look and let me know if there is any value in these data.

If not, then it's time to drop this project and move on! If yes, i'm willing to share the details with anyone that helps.

The data are in ARFF format with a single class which can have two different values (0=lose, 1=win) and can be found here: https://www.dropbox.com/s/6mcszwssxl5lndu/data.arff?dl=0

Thanks in advance!",4,0
55,2016-3-3,2016,3,3,1,48mw5x,"Machine Learning, Artificial Intelligence, Algotrading and Hedge Funds",https://www.reddit.com/r/MachineLearning/comments/48mw5x/machine_learning_artificial_intelligence/,JohnMcNamara1949,1456936433,,0,1
56,2016-3-3,2016,3,3,2,48n1bn,Two Minute Papers - How To Get Started With Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/48n1bn/two_minute_papers_how_to_get_started_with_machine/,LecJackS,1456938270,,16,118
57,2016-3-3,2016,3,3,2,48n2dy,Helsinki Python workshop on machine learning on-going at Anders,https://www.reddit.com/r/MachineLearning/comments/48n2dy/helsinki_python_workshop_on_machine_learning/,com2mentator,1456938622,,0,3
58,2016-3-3,2016,3,3,2,48n3aq,How should one start a career in machine learning?,https://www.reddit.com/r/MachineLearning/comments/48n3aq/how_should_one_start_a_career_in_machine_learning/,vincentg64,1456938885,,0,1
59,2016-3-3,2016,3,3,3,48nbmb,The opening highway chase scene of Deadpool was shot using a mixture of green screen (for car interiors and close-ups) and digital effects (basically everything else). XdRh1YJQ,https://www.reddit.com/r/MachineLearning/comments/48nbmb/the_opening_highway_chase_scene_of_deadpool_was/,whymbecosmo1975,1456941663,,0,1
60,2016-3-3,2016,3,3,4,48nqui,CNNs to recognise presence or absence of an object,https://www.reddit.com/r/MachineLearning/comments/48nqui/cnns_to_recognise_presence_or_absence_of_an_object/,niujin,1456946558,"Hi
I would like to use Caffe to recognise if an object (e.g. an apple) is present or absent in an image

I am not sure if I should then provide a set of training examples in 2 categories. But what would be the 2nd category? Miscellaneous images of anything?",7,1
61,2016-3-3,2016,3,3,4,48nso3,Fragile girl with large breasts masturbates skillfully front of the webcam GzO0GyQfee,https://www.reddit.com/r/MachineLearning/comments/48nso3/fragile_girl_with_large_breasts_masturbates/,conssurpcomhost1985,1456947151,,0,1
62,2016-3-3,2016,3,3,4,48nu22,DeepMind papers accepted to the upcoming ICLR conf,https://www.reddit.com/r/MachineLearning/comments/48nu22/deepmind_papers_accepted_to_the_upcoming_iclr_conf/,lokator9,1456947380,,3,14
63,2016-3-3,2016,3,3,6,48ocde,Question about the new Randomized Hashing paper,https://www.reddit.com/r/MachineLearning/comments/48ocde/question_about_the_new_randomized_hashing_paper/,[deleted],1456952749,[deleted],0,0
64,2016-3-3,2016,3,3,6,48oeak,Question about the new Randomized Hashing paper,https://www.reddit.com/r/MachineLearning/comments/48oeak/question_about_the_new_randomized_hashing_paper/,[deleted],1456953403,"[This paper](http://arxiv.org/abs/1602.08194) says: "" our algorithm only requires 5% of computations (multiplications) compared to traditional algorithms, **without any loss in the accuracy**""

However, when I look at FIg 3 and 4, all of their results at 5% computation are always worse than ""Standard NN"", often significantly.

What am I not getting?

**Edit:** What's with the downvotes? If the authors actually did what they say they did, this is a tremendous achievement. On the other hand, if they are misleading the readers in the abstract, this also deserves more attention.",10,7
65,2016-3-3,2016,3,3,6,48oh3e,Creating an Irony Labeled Reddit Dataset : datasets,https://www.reddit.com/r/MachineLearning/comments/48oh3e/creating_an_irony_labeled_reddit_dataset_datasets/,iktof,1456954388,,0,4
66,2016-3-3,2016,3,3,6,48okpi,DJI Phantom 4: Real computer vision comes to a consumer drone,https://www.reddit.com/r/MachineLearning/comments/48okpi/dji_phantom_4_real_computer_vision_comes_to_a/,amplifier_khan,1456955578,,0,2
67,2016-3-3,2016,3,3,6,48ol0w,Anything2Vec,https://www.reddit.com/r/MachineLearning/comments/48ol0w/anything2vec/,amplifier_khan,1456955686,,1,52
68,2016-3-3,2016,3,3,7,48op79,Courses and Books you need to become a data scientist,https://www.reddit.com/r/MachineLearning/comments/48op79/courses_and_books_you_need_to_become_a_data/,mamareza,1456957159,,0,0
69,2016-3-3,2016,3,3,7,48opps,"When you mask and zero pad data in sequence-to-sequence encoders, what is the correct way to handle zeros in the decoder?",https://www.reddit.com/r/MachineLearning/comments/48opps/when_you_mask_and_zero_pad_data_in/,[deleted],1456957335,[deleted],6,4
70,2016-3-3,2016,3,3,7,48oqvm,Preferred 2016 candidate predictor made with random forest classifier,https://www.reddit.com/r/MachineLearning/comments/48oqvm/preferred_2016_candidate_predictor_made_with/,relganz,1456957747,,2,1
71,2016-3-3,2016,3,3,8,48p2mm,neural net chess,https://www.reddit.com/r/MachineLearning/comments/48p2mm/neural_net_chess/,battez,1456962071,,2,11
72,2016-3-3,2016,3,3,10,48pm8t,I would highly recommend this site to friends and found it easy to use since it was the first and only dating site I have been on. TMTsW4,https://www.reddit.com/r/MachineLearning/comments/48pm8t/i_would_highly_recommend_this_site_to_friends_and/,fbtcxpnimjzqykgws,1456968717,,0,1
73,2016-3-3,2016,3,3,10,48pqx7,Noob question on Neural Networks..,https://www.reddit.com/r/MachineLearning/comments/48pqx7/noob_question_on_neural_networks/,ZioFascist,1456970004,"I know that there are deepNet frame works like Therno, Caffe, Torch,etc which need GPU's and such to run big calcuations...but ....if i were to fire up a neural net in python, Knime, rapidminer, etc (whatever tool of my choice) and run it on a GPU computer....would it not be the same thing? ",2,2
74,2016-3-3,2016,3,3,11,48q0dv,Real-Time Human Pose Recognition in Parts from Single Depth Images,https://www.reddit.com/r/MachineLearning/comments/48q0dv/realtime_human_pose_recognition_in_parts_from/,coskunh,1456973792,"I have been reading this paper and there is point not clear to me. For their algorithm they are using body parts, but i couldn't find how they obtain the body  part.  Is there any straight forward method for obtaining body parts? 

[paper link][1]


  [1]: http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf",1,2
75,2016-3-3,2016,3,3,12,48q589,Are there any medical imaging machine learning researchers here who could possibly help mentor me for a high school science project?,https://www.reddit.com/r/MachineLearning/comments/48q589/are_there_any_medical_imaging_machine_learning/,b-y-t-e,1456975964,"I'm currently doing a science project in which I'm developing a tumor grading model based on MR scans. Is there anybody in this subreddit who specializes in this area that could possibly willing to help me? 

Just leaving some way contact would be enough for me. Thanks!",2,1
76,2016-3-3,2016,3,3,15,48qum7,7 Free Machine Learning Courses,https://www.reddit.com/r/MachineLearning/comments/48qum7/7_free_machine_learning_courses/,vincentg64,1456986746,,0,1
77,2016-3-3,2016,3,3,18,48r9v1,I tried implementing Segnet,https://www.reddit.com/r/MachineLearning/comments/48r9v1/i_tried_implementing_segnet/,pradyu1993,1456996652,"I know there is a little difference between paper and implementation, but I dont understand why the results with this implementation are so horrible compared to the results published",2,1
78,2016-3-3,2016,3,3,19,48remo,What can we *not* do with ML these days?,https://www.reddit.com/r/MachineLearning/comments/48remo/what_can_we_not_do_with_ml_these_days/,thvasilo,1456999878,"So I'm preparing a small presentation about the SOTA in ML, and in one section I'm talking about what we can do in ML right now (which are variations of function and distribution approximation) and I got to thinking:

What are things that people might *think* we should be able to do with ML these but we can't really?

One example I have is from a recent [Not So Standard Deviations](https://soundcloud.com/nssd-podcast/episode-4-a-gajillion-time-series) podcast where they talked about  a DevOps setting where you have people looking at tens (hundreds?) of graphs trying to spot anomalies. 

People often think that ""analytics can solve this"", but statistical tools like anomaly detection cannot really help there because of the dynamic nature of the problem and the non-stationarity of the distributions involved not to mention the complexity of the dependencies between the various time-series and what you are trying to predict.

So what are other examples where it seems like ML can help but in reality the problems are currently intractable?",174,76
79,2016-3-3,2016,3,3,22,48rxv7,What's a good small dataset to learn caffe/torch/etc,https://www.reddit.com/r/MachineLearning/comments/48rxv7/whats_a_good_small_dataset_to_learn_caffetorchetc/,bourbondog,1457011204,"I'm trying to get accustomed to these neural network packages. Any suggestions on what dataset I should use?

I was hoping to use these:

- A super simple 1-2 neuron network (like the AND or XOR gates)
- Use the MNIST dataset

If you have any other recommendations, please let me know!",5,0
80,2016-3-3,2016,3,3,22,48s2m8,Torch vs TensorFlow,https://www.reddit.com/r/MachineLearning/comments/48s2m8/torch_vs_tensorflow/,FluxSeeds,1457013413,"Hey guys,

I'm currently using Torch but have been keeping an eye on TF in the past couple of months. I just wanted to create a general thread and ask if anyone has had experience with both and what would you say distinguishes them from each other?

Any major reasons that should make me consider making the switch from Torch to TF?",20,29
81,2016-3-3,2016,3,3,22,48s30d,How Much do You Know About Capsule Filling Machines?,https://www.reddit.com/r/MachineLearning/comments/48s30d/how_much_do_you_know_about_capsule_filling/,sonusinternational,1457013598,,0,1
82,2016-3-3,2016,3,3,23,48sajs,Recommender system for multiple items rated multiple times,https://www.reddit.com/r/MachineLearning/comments/48sajs/recommender_system_for_multiple_items_rated/,anahochmanova,1457016910,"I am trying to model as a recommender system the following:

 User1 can select item A (and rate it) then item A again (and give it a different rate) and then again item A can be selected and rated n times.. then the same for item B, but item A could be again selected at any time point, and that could happen for any of the A, B, C... Z items. Could this be modelled somehow as a recommender system?
Cannot find any previous work or publication on this, could this anyhow be modelled as a context aware recommender system where the context is somehow the time point where the item is selected? or perhaps defining a user profile where for each user the sequence of previos selected items is stored? ",1,1
83,2016-3-4,2016,3,4,1,48sln9,Oxford spinout uses machine learning to solve the toughest problems,https://www.reddit.com/r/MachineLearning/comments/48sln9/oxford_spinout_uses_machine_learning_to_solve_the/,stormforce7916,1457021172,,0,0
84,2016-3-4,2016,3,4,1,48snmk,[Stemming] Can machine learning reunite the algorithmic approach and the dictionary-based one ?,https://www.reddit.com/r/MachineLearning/comments/48snmk/stemming_can_machine_learning_reunite_the/,datatatatata,1457021880,"Hi,

I've just started working on a project that involves text, and I'd like a good [stemming](https://en.wikipedia.org/wiki/Stemming) algorithm. Stemming allows you to recover the root of a word, so that **run** and **ran** end-up being the same word while **run** and **kid** don't. Pretty straightforward.  

There are basically two approaches to this problem : **algorithms** and **dictionaries**. The first are imperfect, but work on any word. The second is perfect, but can't handle words that were not seen before.  

&gt;The idea is : wouldn't machine learning help extend the power of a dictionary so that new words can be handled ? If so, does it already exist ?  

I'm particularly interested in such algorithms for English, French, Italian and Polish dictionaries fyi.  

Regards.  ",4,8
85,2016-3-4,2016,3,4,1,48sqih,Mobile applications collect a lot of data  but where is it going?,https://www.reddit.com/r/MachineLearning/comments/48sqih/mobile_applications_collect_a_lot_of_data_but/,PaytonKarter1_,1457022931,,0,0
86,2016-3-4,2016,3,4,1,48su5o,Rise of the Robot,https://www.reddit.com/r/MachineLearning/comments/48su5o/rise_of_the_robot/,BigCloudTeam,1457024281,"Atlas from Boston Dynamics (a subsidiary of Google) is a pretty resilient chap, judging by this video. He can trudge through uneven snow, be knocked off his feet and get up again (in an eerily similar way to the film Real Steel), and perform the sorts of menial tasks that might take place in any warehouse or shop storeroom across the country.

In fact, it is hard to believe that there isnt a human in there somewhere.

The mechanical limitations of robots have meant that they have thus far not had so many applications in the real world (outside the military), but over the next 5-10 years it seems that they will join us in mainstream life. When this functionality is combined with already sophisticated artificial intelligence systems, the step to a robot that thinks about its next move independently is not so far away.

In the video, the robot is pushed away from a box with a hockey stick. Its tame response is to recalibrate itself and try to pick up the box again. If it were thinking rationally, maybe its reaction should be to eliminate the person who is doing the pushing, and then it would be free to pick up the box in peace

This scenario plays out in the film Ex Machina (with recent Oscar winner Alicia Vikander). You cannot quite predict what is going on in the androids head for the entire length of the film, and the ending is so unexpected that it truly makes you stop and think what if.

Humanity seems to be on a constant mega quest to improve ourselves, but it sometimes seems to me that in doing so we are starting to replace ourselves.

Our kids no longer play with each other so much in the park, they chat and play in a virtual world (admittedly with other real children). How far is the day that parents can program a virtual friend to play with their kids, together with all the desired behavioural traits? Students will no longer go through the rituals of university  they will simply sit MOOCs in a virtual lecture room. Algorithms are now even being introduced to grade the work of those students.

On the physical side of things, 3D printing could make a mockery of traditional manufacturing, robots will soon be servicing our every need, and trucks will be driving themselves the length and breadth of our countries (truck driver is one of the most common jobs in the U.S).

I am proud to work in Big Data, but when will that data become so big that only artificial intelligence can process it and make suitable decisions from it? You need a pretty big brain to be a Data Scientist, and some of the guys that we place are amongst the most intelligent people on the planet. However, even they tell me stories about when they had to shrug their shoulders and admit defeat when confronted with a certain problem.

Robots and artificial intelligence wont admit defeat  they will simply keep computing until they reach a suitable conclusion.

In watching Atlas being pushed away from the box with a hockey stick, that is what worries me slightly. The robots are indeed rising...

Matt Reaney is the Founder of www.bigcloud.io specialists in Big Data recruitment - connecting innovative organisations with the best talent in Data Analytics and Data Science.

For more Big Data and Data Science related articles visit our website or get in touch via Linkedin or Twitter - @BigCloudTeam ",1,0
87,2016-3-4,2016,3,4,2,48t0pa,Free Online Course on Statistical Shape Modelling,https://www.reddit.com/r/MachineLearning/comments/48t0pa/free_online_course_on_statistical_shape_modelling/,dessertbowl,1457026593,,1,1
88,2016-3-4,2016,3,4,2,48t1e8,"Activation functions, noisy only in the saturated range",https://www.reddit.com/r/MachineLearning/comments/48t1e8/activation_functions_noisy_only_in_the_saturated/,vintermann,1457026828,,0,5
89,2016-3-4,2016,3,4,2,48t1s4,Pylearn2 is dead?,https://www.reddit.com/r/MachineLearning/comments/48t1s4/pylearn2_is_dead/,galapag0,1457026964,,11,12
90,2016-3-4,2016,3,4,2,48t3ji,GPU war Maxwell vs Pascal,https://www.reddit.com/r/MachineLearning/comments/48t3ji/gpu_war_maxwell_vs_pascal/,bicepjai,1457027548,I am building a rig for my ML experiments.  Was planning on 4 titanx sli.  i returned them all thinking to wait out for pascal cards since they are close by.  Do you thinks it's a wise decision? ,9,0
91,2016-3-4,2016,3,4,3,48taat,Who are the professors working on recurrent neural networks and are currently accepting new students ?,https://www.reddit.com/r/MachineLearning/comments/48taat/who_are_the_professors_working_on_recurrent/,ha2emnomer,1457029890,Beside Jrgen Schmidhuber (who is a deep learning pioneer)  who are the professors working on RNN and accepting new students,7,0
92,2016-3-4,2016,3,4,4,48tix9,Regarding the next 10-20 years of the job market,https://www.reddit.com/r/MachineLearning/comments/48tix9/regarding_the_next_1020_years_of_the_job_market/,xristos_forokolomvos,1457032899,Does anyone feel bad from time to time when thinking that advances in ML systems might force people out of their jobs and we are the ones to do so?,4,1
93,2016-3-4,2016,3,4,4,48tje9,why pretraining setup the weight matrix to be better to discriminate,https://www.reddit.com/r/MachineLearning/comments/48tje9/why_pretraining_setup_the_weight_matrix_to_be/,John_Smith111,1457033066,"Hello 

I read in paper that 

""It has been shown
[2, 10] that this unsupervised pre-training builds a representation
from which it is possible to do successful supervised
learning by fine-tuning the resulting weights using
gradient descent learning. In other words, the unsupervised
stage sets the weights of the network to be closer to
a good solution than random initialization, thus avoiding
local minima when using supervised gradient descent.""

As far as i undestand the generative pre-training wights are also discriminative and the pretraining set up the weights near the the global minima of the error landscape of the discriminative learning.

My question is - are the generative weight are used to dicriminate ? Do that generative wights put the state of the network near the global minima and if that - why ? 
Do  the better lower bound  put the wights near better genrative distribution and that wights could be used for discrimination ? 
How the generative weights setup better discrimination error rate ?

10x all ",4,1
94,2016-3-4,2016,3,4,4,48tmo1,Quick question: What does label and feature of sample mean?,https://www.reddit.com/r/MachineLearning/comments/48tmo1/quick_question_what_does_label_and_feature_of/,quqa,1457034218,"Label of sample

Features of sample

Can you ELI5 and give an example? ",3,0
95,2016-3-4,2016,3,4,4,48toyt,I'm making a system that predicts virality of textual posts -- how should I proceed?,https://www.reddit.com/r/MachineLearning/comments/48toyt/im_making_a_system_that_predicts_virality_of/,quqa,1457035043,"I'm trying to make a system that predicts virality of textual posts for school project (I'm a CS undergrad). I have about 50,000 textual articles retrieved from news websites and I have FB share counts for each one. 

I plan to train the machine somehow so that it can predict how likely an article go viral. I think I have a good idea, I prepared the data, now I need to decide on ML methods to work with. I don't need to build a super accurate system however I need to make sure I used the right techniques for the data and the problem. And I need to justify that in front of everyone in class. 

So, I'd really appreciate if you could give me some pointers. I really don't know where to start as I'm pretty new to ML. ",2,0
96,2016-3-4,2016,3,4,5,48trgo,TensorFlow machine learning with financial data on Google Cloud Platform,https://www.reddit.com/r/MachineLearning/comments/48trgo/tensorflow_machine_learning_with_financial_data/,lokator9,1457035881,,1,0
97,2016-3-4,2016,3,4,5,48tsbh,Deep Learning: an Interview with Yoshua Bengio,https://www.reddit.com/r/MachineLearning/comments/48tsbh/deep_learning_an_interview_with_yoshua_bengio/,reworksophie,1457036167,,0,1
98,2016-3-4,2016,3,4,5,48tvnm,Advice - 3D scan denoising with ML,https://www.reddit.com/r/MachineLearning/comments/48tvnm/advice_3d_scan_denoising_with_ml/,[deleted],1457037297,[deleted],4,0
99,2016-3-4,2016,3,4,5,48ty5q,"Implementation of ""Teaching Machines to Read and Comprehend"" proposed by Google DeepMind",https://www.reddit.com/r/MachineLearning/comments/48ty5q/implementation_of_teaching_machines_to_read_and/,cesarsalgado,1457038186,,6,22
100,2016-3-4,2016,3,4,6,48u5at,2016 Strachey lecture - Great talk by Deepmind's Demis Hassabis on Alphago,https://www.reddit.com/r/MachineLearning/comments/48u5at/2016_strachey_lecture_great_talk_by_deepminds/,barrettabolt,1457040763,,0,11
101,2016-3-4,2016,3,4,6,48u7aw,Is there any good theory on why Gradient Descent actually works in finding near-optimal solutions?,https://www.reddit.com/r/MachineLearning/comments/48u7aw/is_there_any_good_theory_on_why_gradient_descent/,thecity2,1457041850,"It amazes me with hundreds of thousands or even hundreds of millions of parameters, deep neural networks are able to converge on ""optimal"" solutions (or at least, very good ones). I can't wrap my ahead around it, especially considering in the past when I've tried to optimize even functions of a few dozen variables, it was a very difficult problem.

My working hypothesis is that somehow using mini-batches in SGD gives enough randomness for exploration of the function space. Am I on the right track? Is any of this provable?",8,2
102,2016-3-4,2016,3,4,7,48udvs,Missed connections: anyone remember a paper that uses semi-supervised learning to solve mnist with only one labeled example in each class?,https://www.reddit.com/r/MachineLearning/comments/48udvs/missed_connections_anyone_remember_a_paper_that/,lahwran_,1457044257,"I could have sworn I saw this in passing somewhere, but I forgot to save the reference, and googling ""mnist only 10 examples"" is finding me references to CIFAR-10 and mnist in the same sentence, rather than papers about learning from few examples.",4,4
103,2016-3-4,2016,3,4,7,48uf0h,Do batchnorm and dropout fit together?,https://www.reddit.com/r/MachineLearning/comments/48uf0h/do_batchnorm_and_dropout_fit_together/,andrewbarto28,1457044679,,3,8
104,2016-3-4,2016,3,4,9,48uw94,How do Multiple LSTMs work?,https://www.reddit.com/r/MachineLearning/comments/48uw94/how_do_multiple_lstms_work/,ardorem,1457051570,"I am struggling to understand how exactly the multiple (sequentially stacked) LSTMs work. To my understanding, we might need to split a LSTM into multiple LSTMs and stack them **if** we want to batch inputs. Then we can compute for a bunch of time series for all the batched inputs, save the states, compute the next batch using the saved states, and so on. To be able to provide the 3D tensor for the next LSTM, we would need to keep all output states from each time step and feed them to the next LSTM. 

My question is, is it true that the batching is the only reason why we use the multiple LSTMs? To me, we don't need otherwise because we can just have a lot longer time series for a single LSTM  rather than stacking up multiple LSTMs. Am I missing something? 

Thank you!",1,5
105,2016-3-4,2016,3,4,9,48uz63,What is the difference between boosting and boosting.cv?,https://www.reddit.com/r/MachineLearning/comments/48uz63/what_is_the_difference_between_boosting_and/,[deleted],1457052796,[deleted],0,1
106,2016-3-4,2016,3,4,9,48uzlh,There are two places where I can learn about tensor flow. Which one should I choose?,https://www.reddit.com/r/MachineLearning/comments/48uzlh/there_are_two_places_where_i_can_learn_about/,Mr__Christian_Grey,1457052964,Deep learning course on udacity or tensor flow website itself(https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html)?  ,4,0
107,2016-3-4,2016,3,4,10,48v33z,What's the difference between octave and tensorflow? And who uses octave and tensorflow?,https://www.reddit.com/r/MachineLearning/comments/48v33z/whats_the_difference_between_octave_and/,Mr__Christian_Grey,1457054364,,1,0
108,2016-3-4,2016,3,4,10,48v46f,"Use NLP to Grade your Emails on: Politeness, Reading-Time, and ""Call to Action"". (Gmail only Beta.)",https://www.reddit.com/r/MachineLearning/comments/48v46f/use_nlp_to_grade_your_emails_on_politeness/,[deleted],1457054804,[deleted],1,16
109,2016-3-4,2016,3,4,10,48v8os,"Use NLP to Grade your Emails on: Politeness, Reading-Time, and ""Call to Action"". [Gmail only Beta] [Fixed Typo in Link]",https://www.reddit.com/r/MachineLearning/comments/48v8os/use_nlp_to_grade_your_emails_on_politeness/,[deleted],1457056742,[deleted],9,71
110,2016-3-4,2016,3,4,11,48v9w3,"Not so fast, FFT: Winograd",https://www.reddit.com/r/MachineLearning/comments/48v9w3/not_so_fast_fft_winograd/,elanmart,1457057261,,19,68
111,2016-3-4,2016,3,4,11,48ve8o,[1603.01121] Deep Reinforcement Learning from Self-Play in Imperfect-Information Games,https://www.reddit.com/r/MachineLearning/comments/48ve8o/160301121_deep_reinforcement_learning_from/,RushAndAPush,1457059177,,2,26
112,2016-3-4,2016,3,4,12,48vk0r,my lm ti en luva,https://www.reddit.com/r/MachineLearning/comments/48vk0r/my_lm_ti_en_luva/,tahuuson,1457061601,,0,0
113,2016-3-4,2016,3,4,12,48vk66,Geoffrey Hinton - The Code That Runs Our Lives,https://www.reddit.com/r/MachineLearning/comments/48vk66/geoffrey_hinton_the_code_that_runs_our_lives/,Buck-Nasty,1457061655,,31,28
114,2016-3-4,2016,3,4,13,48vqlx,Word2Vec And N-Grams,https://www.reddit.com/r/MachineLearning/comments/48vqlx/word2vec_and_ngrams/,wxyyxc1992,1457064388,"How to understand that :the word embeddings trained by a large corpus of free text and sum them up to represent n-grams , are there any paper or blogs can tell me that?",4,1
115,2016-3-4,2016,3,4,14,48vzmi,Where can I find a pretrained network for inverting deep representations back to images?,https://www.reddit.com/r/MachineLearning/comments/48vzmi/where_can_i_find_a_pretrained_network_for/,anonDogeLover,1457068421,"There are a few papers that do this, like (http://arxiv.org/pdf/1506.02753v3.pdf), but I'd like to find some code and a pretrained net to do this. Any leads?",2,0
116,2016-3-4,2016,3,4,14,48w0yu,Google DeepMind Demis Hassabis,https://www.reddit.com/r/MachineLearning/comments/48w0yu/google_deepmind_demis_hassabis/,[deleted],1457069087,[removed],1,0
117,2016-3-4,2016,3,4,15,48w5d1,"How realistic is it that ASI will eliminate human life because we are the cause of poverty, diseases, war, hate, etc.?",https://www.reddit.com/r/MachineLearning/comments/48w5d1/how_realistic_is_it_that_asi_will_eliminate_human/,Joshua_Dunigan,1457071259,,6,0
118,2016-3-4,2016,3,4,17,48wioq,how does sigmoid function form smoot landscape,https://www.reddit.com/r/MachineLearning/comments/48wioq/how_does_sigmoid_function_form_smoot_landscape/,John_Smith111,1457079460,"Hello 

I would like to ask how the sigmoid functions form smooth (differentiable) landscape ?

10x all ",13,0
119,2016-3-4,2016,3,4,18,48wqey,Datasets for text document summarization?,https://www.reddit.com/r/MachineLearning/comments/48wqey/datasets_for_text_document_summarization/,awhitesong,1457085410,We are working on multi document summarization and were looking for the datasets. We tried filling the DUC dataset application but haven't received the dataset yet. They (those who have to provide the DUC) are on vacations(automatic reply told so) so it might take time(which we are short of). So could you suggest some good datasets for multidocument summarizations that could be availed or are freely available online ?,4,3
120,2016-3-4,2016,3,4,19,48ws88,Freebase WWW Links to Entity Names,https://www.reddit.com/r/MachineLearning/comments/48ws88/freebase_www_links_to_entity_names/,[deleted],1457086674,[deleted],1,0
121,2016-3-4,2016,3,4,22,48xcb6,[Help] How to determine how many images needed for my CNN dataset?,https://www.reddit.com/r/MachineLearning/comments/48xcb6/help_how_to_determine_how_many_images_needed_for/,ridicul0us123,1457098888,"Hi, for what it's worth, I'm pretty new in CNN subject.

As the title suggest, I'm not sure how many images needed for my CNN dataset? I plan to use pretrained googlenet on Imagenet and fine tune it for head pose classification(8 classes each 45 degree and 1 background class).

I read ""If your dataset is big"", the thing is I don't know how to determine whether my dataset is big or small. is 50.000 images is big?

Any input, critic, advice are most appreciated. Thx in advance!",4,0
122,2016-3-4,2016,3,4,23,48xkcb,Announcing Leaf 0.2: Rust Machine Learning Framework and Benchmarks,https://www.reddit.com/r/MachineLearning/comments/48xkcb/announcing_leaf_02_rust_machine_learning/,Hobofan94,1457102387,,6,56
123,2016-3-5,2016,3,5,0,48xse3,Where to submit applied Deep Learning paper?,https://www.reddit.com/r/MachineLearning/comments/48xse3/where_to_submit_applied_deep_learning_paper/,andrewbarto28,1457105636,"I am working on a project where I apply CNN to a task which it hasn't been applied before. It is kind of low hanging fruit and I am afraid someone will publish before me. Although it is something interesting and that the community would certainly benefit from, it doesn't have any innovation in the algorithm being applied. I am just using vanilla CNNs. But no one has ever used CNNs in this task and I am getting better results than previous methods. But there is also a bit of novelty in the preprocessing I am doing. With that said, I am looking for a place to publish with a deadline around April, because I am afraid someone will publish before me. My professor wants me to publish to a non-conference (journal or transaction), but I am afraid the review process takes too long and someone publish in the meantime or someone steal my idea. A good option would be to upload to arxiv, but I am not sure I can submit to a journal after that. I know that good conference such as CVPR, NIPS, and ICLR don't mind people uploading to arxiv, but I am not sure the paper will get accepted at those since it doesn't have novelty in the algorithm being used.

Some more specific question:

1- Is there a conference with deadline around April which accepts papers uploaded to arxiv?

2- Is there a journal or transaction that accepts papers with preprints uploaded to arxiv on submission time?

3- Is it possible that NIPs (deadline in May) accepts a paper using vanilla CNN in a very important problem where it hasn't been applied before?

Thanks!
",11,1
124,2016-3-5,2016,3,5,0,48xv11,"Hiring an NLP / ML engineer, how to assess skill level?",https://www.reddit.com/r/MachineLearning/comments/48xv11/hiring_an_nlp_ml_engineer_how_to_assess_skill/,osiris679,1457106640,"Hi ML, 

We're interviewing candidates for the position mentioned above, and I was wondering if there are best practices for assessing skill level in NLP / ML. 

Any thoughts on this? Thanks!

(link to the job description): https://angel.co/kip/jobs/122122-nlp-machine-learning-engineer",43,20
125,2016-3-5,2016,3,5,1,48xyjm,Does ML apply for this type of problem?,https://www.reddit.com/r/MachineLearning/comments/48xyjm/does_ml_apply_for_this_type_of_problem/,hahanawmsayin,1457107936,"Say I want to create structured data using text descriptions of apartments.

For example, after running through a bunch of these listings, I'd expect to find categories like `pets allowed`, `outdoor space` and `security deposit`, because those phrases appeared in so many of the listings.

(And I'd expect these phrases to be found even if some of the listings have typos like ""secury deposit"" or ""pets alow"".

I assume this is the kind of problem I'd solve with ML, but I'm just getting started (experienced programmer, though).

Any pointers? For specific tools, JavaScript / Node would be my preference, though I'm also asking for the general terms I need to research.

Thanks!",1,0
126,2016-3-5,2016,3,5,1,48y3a1,Project Ideas,https://www.reddit.com/r/MachineLearning/comments/48y3a1/project_ideas/,thenerdstation,1457109718,"I have some free time coming up soon, so whats a fun ML project I could start working on? I really want to do something with reinforcement learning, but I always feel like to try to do do something way over my head.",1,0
127,2016-3-5,2016,3,5,1,48y4cg,Distributed TensorFlow Has Arrived,https://www.reddit.com/r/MachineLearning/comments/48y4cg/distributed_tensorflow_has_arrived/,pedromnasc,1457110123,,0,0
128,2016-3-5,2016,3,5,2,48y81v,Blog: Which tool should a beginner learn first?,https://www.reddit.com/r/MachineLearning/comments/48y81v/blog_which_tool_should_a_beginner_learn_first/,Beboped,1457111449,,3,0
129,2016-3-5,2016,3,5,2,48y958,"battle of the brains! real vs artificial! get ready for historic Google (Deep)Go match vs top human player Sedol next week! all the details, topnotch links",https://www.reddit.com/r/MachineLearning/comments/48y958/battle_of_the_brains_real_vs_artificial_get_ready/,vznvzn,1457111857,,4,0
130,2016-3-5,2016,3,5,2,48ydjf,question about boosting the weight of certain terms in sklearn/TfidfVectorizer,https://www.reddit.com/r/MachineLearning/comments/48ydjf/question_about_boosting_the_weight_of_certain/,AmericaIsNumber2,1457113469,"Sorry, I am a bit new to machine learning/sklearn/text classifications but I was wondering if I could get any help with the following

Let's say that I have the following documents:

    corpus = [[1, 'there is a mad dog'], [2, 'there is an okay dog'], [1, 'the mad dog is acting crazy'], [3, 'there is a blue dog']]
    vectorizer = TfidfVectorizer(ngram_range=(1, 2)
    train_vectors = vectorizer.fit_transform(corpus)
    
Is there anything that I could do something like:

    train_vectors.boost('mad dog', 1)

boost is probably the wrong word for this but I am basically just trying to tell the vectorizer ""if the bigram 'mad dog' appears then predict 1""

Thanks in advance ",3,3
131,2016-3-5,2016,3,5,2,48ydtw,Did anyone try autumnai.com?,https://www.reddit.com/r/MachineLearning/comments/48ydtw/did_anyone_try_autumnaicom/,[deleted],1457113584,[deleted],0,1
132,2016-3-5,2016,3,5,2,48yfr6,Mazak SQT 250MS Randomly stopping and freezing up,https://www.reddit.com/r/MachineLearning/comments/48yfr6/mazak_sqt_250ms_randomly_stopping_and_freezing_up/,slaffer27,1457114274,,1,0
133,2016-3-5,2016,3,5,3,48yp0m,iq option strategy martingale (2016) MONEY,https://www.reddit.com/r/MachineLearning/comments/48yp0m/iq_option_strategy_martingale_2016_money/,RReOLRnJ,1457117620,,0,1
134,2016-3-5,2016,3,5,3,48yq0w,Does anyone have experience with AIML and/or Pandorabots?,https://www.reddit.com/r/MachineLearning/comments/48yq0w/does_anyone_have_experience_with_aiml_andor/,boxofrice,1457117978,"I started creating a chatbot with Pandorabots a while ago, however I would like to ""call"" or ""import"" or ""embed"" my aiml bot files in an html file, so that I can test it out on my browser. I've heard that you need to use a hosting service, such as Pandorabots and that you can then link this to your website. However, I would first have to publish my Pandorabot to do that, and it is nowhere near ready for people to try out. What exactly happens after you publish it, has anybody done this?
I've heard of alternative free programs that one can install on a home computer or server to host aiml bots, which requires some technical skill, but I am willing to learn.
If anyone has software recommendations for aiml writing/testing/implementing, I run 64x Manjaro Linux.
Is there anyway at all to call aiml code from an html document without the use of additional software?
Thanks!",0,0
135,2016-3-5,2016,3,5,4,48yt53,Mac OS X or Ubuntu,https://www.reddit.com/r/MachineLearning/comments/48yt53/mac_os_x_or_ubuntu/,regularized,1457119087,I want to buy a laptop for machine learning and daily use. I will do heavier machine learning jobs on a desktop. What should I buy as a laptop? A macbook or something else with ubuntu?,13,1
136,2016-3-5,2016,3,5,4,48yve1,Spawkfish: Neural Network Chess Engine,https://www.reddit.com/r/MachineLearning/comments/48yve1/spawkfish_neural_network_chess_engine/,bipptybop,1457119888,,8,0
137,2016-3-5,2016,3,5,7,48zstj,"My WIP implementation of ""neural image analogies""",https://www.reddit.com/r/MachineLearning/comments/48zstj/my_wip_implementation_of_neural_image_analogies/,gifhell,1457132201,,14,97
138,2016-3-5,2016,3,5,8,48zxiu,Are there any current or approaching commercial innovations based on neural networks?,https://www.reddit.com/r/MachineLearning/comments/48zxiu/are_there_any_current_or_approaching_commercial/,[deleted],1457134054,[deleted],4,1
139,2016-3-5,2016,3,5,10,490d05,Numerai: Introducing Originality,https://www.reddit.com/r/MachineLearning/comments/490d05/numerai_introducing_originality/,dsernst,1457140373,,5,0
140,2016-3-5,2016,3,5,10,490gw7,"Why does my RNN perform well on long sequences, but not on the short, easy ones?",https://www.reddit.com/r/MachineLearning/comments/490gw7/why_does_my_rnn_perform_well_on_long_sequences/,jstaker7,1457142030,"I have a problem that has left me perplexed and baffled. I am training an RNN (with LSTM cells) to generate a sequence from a given input (FC layer at the bottom of a CNN to generate labels). E.g.,

Input -&gt; picture of an orange
output -&gt; ""fruit""

input -&gt; picture containing: apple fish banana broccoli
output -&gt; ""fruit meat fruit veggie""

The sequences vary in length between 0-20 words. My model seems to be performing very well when there are ~7+ inputs (60%+ accuracy), but almost completely incorrect for shorter sequences (&lt;7). How can this happen? I would think that if it is good at recognizing ""orange"" in a long sequence, a single sequence with only the word ""orange"" should be a piece of cake, since it can reuse that same, learned pattern regardless of whether the input is long or short.

One note: There are very many more long examples in the training set (naturally, since there are exponentially more possible combinations). But rather than augmenting the training set, I'm curious as to why the easy, short sequences don't turn out to be easy at all. Any ideas?

Edit: my apologies for causing some confusion by trying to give a simple example. The actual setup is using CNN+RNN to annotate images. ",17,5
141,2016-3-5,2016,3,5,13,4914i0,"MLconf NYC: talks from experts in Deep Learning, including speakers from companies such as Maluuba, Baidu Research, Google, Facebook, MS and more.",https://www.reddit.com/r/MachineLearning/comments/4914i0/mlconf_nyc_talks_from_experts_in_deep_learning/,shonburton,1457153618,,4,2
142,2016-3-5,2016,3,5,15,491e3k,What can we do with review text data other than sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/491e3k/what_can_we_do_with_review_text_data_other_than/,kullback-leibler,1457159045,"The company I work for have lots of review text data that haven't been analysed so far. We now have hundreds of thousands review text so I think it's time for some NLP action.

Now I'm wondering what are some insights that we can get from these data that potentially helpful for my (business) colleague? Our product is accommodation (hotels and alike) and each review text also contains a score (1-5) from the reviewer.

My current ideas are:

* Sentiment analysis: Build model to predict the overall sentiment of a review text. Not sure how this is gonna be useful since new review will always have a rating anyways. I'm also thinking of opinion mining: trying to pin point which part of the text actually contains opinions.

* Forged review detection: Try to find which of those reviews are paid good (or bad) reviews. Not sure there will be many, since only confirmed stays are allowed to give review.

* Topic modelling / clustering: Try to find what the most commonly commented aspects of a particular accommodation are. This can also be segmented by area/location.

Do you have other suggestions?",10,4
143,2016-3-5,2016,3,5,18,491vo9,Any standard ways to make a horribly behaving function well behaved?,https://www.reddit.com/r/MachineLearning/comments/491vo9/any_standard_ways_to_make_a_horribly_behaving/,past_fourier,1457171079,"Hi ML,

I have a parametrized layer as a part of a Neural Network (think of it more like a graph). This layer has a single parameter and produces output based on this parameter. But unfortunately, the variations in the output of this layer wrt to the variations of the parameter are not well behaved.

It is linear in pieces, but there is no overall global trend. For example, very small values of this parameter, very large values of this parameter produce outputs that are similar, while some values in between also produce similar values. But in between, it's a roller coaster.

I know I'm being very vague here, but my supervisor only allowed me to share this much. So, if you know of any good way of transforming a badly behaving function to a well behaved function, such that it can be learnt effectively by backpropagation, please help me with it.

Also, if you know of any other way of learning such bad functions, share them as well.

Thanks,
Past_Fourier",7,0
144,2016-3-5,2016,3,5,18,491vq8,Big Data Beginner Specialization. No prior experience required. Get enrolled today at 88% off.,https://www.reddit.com/r/MachineLearning/comments/491vq8/big_data_beginner_specialization_no_prior/,andalib_ansari,1457171129,[removed],0,1
145,2016-3-5,2016,3,5,20,492280,How is convolutional layer forward pass implemented?,https://www.reddit.com/r/MachineLearning/comments/492280/how_is_convolutional_layer_forward_pass/,zibenmoka,1457176256,"Hello there, 

I wonder how convolutional layer forward pass is implemented in practice. From what I read online (I think it was on Stanford course website, cannot find it now) - I concluded that before forward pass is performed input it transformed into convenient form (using matlab im2col routine). 

How do people handle it in practice? Obviously it does not make much sense to transform each image right before feeding a forward pass routine (having many iterations ahead it means *lots* of duplicate work)

Do people first perform im2col on all images (possibly store it on disk/in database) and then just fetch it whenever needed? 

How do people deal with it in practice? 
 ",5,2
146,2016-3-5,2016,3,5,22,492dln,How to feed the output of a dense layer to a LSTM layer?,https://www.reddit.com/r/MachineLearning/comments/492dln/how_to_feed_the_output_of_a_dense_layer_to_a_lstm/,wadhwasahil,1457184057,"I am modeling the spatial features of an image using CNN. But to grasp the spatial features of a video I am using LSTM.
So, how should I feed the input of a dense layer to a LSTM layer.

    model1.add_node(Dense(4096, activation='relu'), name = ""fc1"", input = ""flat1"")
    model1.add_node(LSTM(output_dim = 2048, activation = ""sigmoid"", inner_activation = ""hard_sigmoid""), input = ""fc1"", name = ""lstm1"")

Please suggest changes.

Thanks.",4,0
147,2016-3-5,2016,3,5,22,492dvk,No more simple questions threads?,https://www.reddit.com/r/MachineLearning/comments/492dvk/no_more_simple_questions_threads/,confused00-,1457184199,"The side-bar links to the ""Current Simple Question Thread"" which is an [archived thread](https://www.reddit.com/r/MachineLearning/comments/2xopnm/mondays_simple_questions_thread_20150302/) from a year ago. Is there a reason for why these stopped? If it's due to activity, maybe increasing the time interval between the threads would make more sense than stopping them altogether?

I'm mainly bringing it up because I find those threads quite informative to go through, even if I don't have a particular question to ask. I suppose I could make one myself, but I believe it might be useful to understand why they stopped first.

EDIT: I searched for ""simple questions thread"" and I found [a more recent one](https://www.reddit.com/r/MachineLearning/comments/3cjloi/simple_questions_thread_20150708/) (8 months ago) that's not yet archived/closed. However, I don't suppose anyone checks it anymore seeing that it's not linked in the sidebar, and it's nowhere close to first page. Would a sticky questions thread make sense to alleviate these inactivities issues? This could also encourage people to ask questions more freely if they feel like their question doesn't deserve a new thread.",7,41
148,2016-3-5,2016,3,5,23,492jn6,"gago, a little Golang framework for building genetic algorithms",https://www.reddit.com/r/MachineLearning/comments/492jn6/gago_a_little_golang_framework_for_building/,Lemax0,1457187275,,0,0
149,2016-3-6,2016,3,6,3,493exs,Why do they use cosine distance over Euclidean distance in word2vec to select similar words?,https://www.reddit.com/r/MachineLearning/comments/493exs/why_do_they_use_cosine_distance_over_euclidean/,yield22,1457200849,"Obviously it is because cosine distance performs better. But why is it the case mathematically?

As to me, if two words are similar, their rows in PMI matrix should be similar, since word2vec is kind of implicitly factorization of PMI matrix, we should have v_1 * c \approx v_2 * c for all context word c. So we should look for |v_1 - v_2| = 0, also \|v_1 - v_2\|^2 = 0, both of which are not cosine distance.",8,5
150,2016-3-6,2016,3,6,3,493k0s,"Deep Dick Dreams - Google's DeepDream, trained on dick pics",https://www.reddit.com/r/MachineLearning/comments/493k0s/deep_dick_dreams_googles_deepdream_trained_on/,lokator9,1457202881,,41,204
151,2016-3-6,2016,3,6,4,493pez,What is your heuristic to decrease the learning rate of a CNN ?,https://www.reddit.com/r/MachineLearning/comments/493pez/what_is_your_heuristic_to_decrease_the_learning/,Tamazy,1457204979,"When you train a convolutional neural network and you set the stochastic gradient descent algorithm as its optimizer, you usually have three hyperparameters to tune, which are : the learning rate, the momentum and the weight decay.

I know few heuristics to decrease the learning rate.
The first one is to tune a fourth hyperparameter (a learning rate decay) which decreases the learning rate at each time step such as a minibatch or an epoch.
The second one is to set different values for the learning rate (also called regimes) that you instantiate manually.

I would like to know if you use different heuristics based, for instance, on the deceleration of the loss or the train/test accuracy.

Thanks for your time.",3,0
152,2016-3-6,2016,3,6,4,493wdu,Algorithms for silly text generation,https://www.reddit.com/r/MachineLearning/comments/493wdu/algorithms_for_silly_text_generation/,chirples,1457207688,"I've coded a few Markov chain-based silly text generators, and I want to move on to something a little more advanced. What are some other algorithms that could be used for this purpose?",4,0
153,2016-3-6,2016,3,6,5,49410u,Stochastic Dummy Boosting,https://www.reddit.com/r/MachineLearning/comments/49410u/stochastic_dummy_boosting/,throwawaya94,1457209562,,1,1
154,2016-3-6,2016,3,6,7,494fig,Will error-correction be replaced or evolve into a better paradigm?,https://www.reddit.com/r/MachineLearning/comments/494fig/will_errorcorrection_be_replaced_or_evolve_into_a/,improssibility,1457215376,,2,0
155,2016-3-6,2016,3,6,9,494y9k,"Using Autoencoder, need advice.",https://www.reddit.com/r/MachineLearning/comments/494y9k/using_autoencoder_need_advice/,Alirezag,1457223261,"Hi Guys,
I am new to unsupervised stuff (RBM, DBM, stacked autoencoder, etc).
I want to use Autoencoder and see how it works.
Do guys have any step by step guide on how to start and how code and run it?
I would be so grateful if you guys can let me know about a good source of code and tutorials to know how to train, test, etc.
Also if there is any source for visualization of features, it would be more helpful as well.
Thank you in advance.",2,1
156,2016-3-6,2016,3,6,9,494zhd,"I'm making a self driving car using convolutional neural networks, here's how it's going after one week of progress!",https://www.reddit.com/r/MachineLearning/comments/494zhd/im_making_a_self_driving_car_using_convolutional/,[deleted],1457223794,[deleted],0,0
157,2016-3-6,2016,3,6,10,49572k,Genetic Algo: Differing Types in Chromosome?,https://www.reddit.com/r/MachineLearning/comments/49572k/genetic_algo_differing_types_in_chromosome/,afry316,1457227170,"Hello -

I have created a Genetic Algo using the GeneticSharp framework. My fitness function has 5 parameters which are all of the same type - ints. However, I would like to example my fitness function to include 5 ints, 2 bools and 3 arrays of ints. What is the proper way to ""encode"" genes of differing types in the Chromosome. I have used two 'solutions' in the past, but I am not sure if they are the best way:

1. Location
- The Chromosome is made up of an array of Genes. Therefore I know that Index 1 will be a Gene that has a boolean value and Index 4 will be an int value. When I got to run the fitness function I look for a gene in a specific position in the array.
- The only problem is that some crossover techniques will change the genes position and therefore when I run my fitness function Index 4 won't necessarily be an int value.

2. Name Based
- The Chromosome comprises an array Genes which have a Dictionary value - &lt;string, object&gt;. I give the genes a name like ""UseCorrelations"" and give it a value of true or false. This way I can use crossover techniques that change the position of the genes.
- When I run the fitness function I iterate over the array of Genes until I find the one that has the name (ex ""UseCorrelations"") that matches the parameter I am looking for.


To me, both ways don't seem *right*. Is there a better more elegant solution?

Thanks.",3,0
158,2016-3-6,2016,3,6,10,4959jz,The evolution of read/write in a Lie Access Neural Turing Machine (over the course of learning) is strangely satisfying,https://www.reddit.com/r/MachineLearning/comments/4959jz/the_evolution_of_readwrite_in_a_lie_access_neural/,jinpanZe,1457228309,,6,9
159,2016-3-6,2016,3,6,12,495ks9,Twitter - Neural Network Trained on Donald Trump Transcripts,https://www.reddit.com/r/MachineLearning/comments/495ks9/twitter_neural_network_trained_on_donald_trump/,[deleted],1457233640,[deleted],1,0
160,2016-3-6,2016,3,6,16,496dx1,Relationship between error function surface and target function surface in neural networks,https://www.reddit.com/r/MachineLearning/comments/496dx1/relationship_between_error_function_surface_and/,John_Smith111,1457248869,"Hello all,

Is therer any relationship in shapes between neural network error function surface (error as function of weights) and targt function surface (y as function of x inputs)?
",8,0
161,2016-3-6,2016,3,6,16,496eko,A (small) introduction to Boosting,https://www.reddit.com/r/MachineLearning/comments/496eko/a_small_introduction_to_boosting/,sachinrjoglekar,1457249307,,13,74
162,2016-3-6,2016,3,6,17,496jdx,A major first step toward bounding the computational complexity of training RNNs,https://www.reddit.com/r/MachineLearning/comments/496jdx/a_major_first_step_toward_bounding_the/,jinpanZe,1457252440,,3,10
163,2016-3-6,2016,3,6,18,496n2c,Super Mario level generated by an LSTM,https://www.reddit.com/r/MachineLearning/comments/496n2c/super_mario_level_generated_by_an_lstm/,jinpanZe,1457255001,,9,30
164,2016-3-6,2016,3,6,20,496zp2,"The function in this image is ""Convex function"" or ""Bowl Shaped"" function? Is it a 3D convex function represented in 2D?",https://www.reddit.com/r/MachineLearning/comments/496zp2/the_function_in_this_image_is_convex_function_or/,Mr__Christian_Grey,1457263844,,2,0
165,2016-3-6,2016,3,6,21,4974pn,Using Bayes Classifier in home automation to build an Alexa like assistant,https://www.reddit.com/r/MachineLearning/comments/4974pn/using_bayes_classifier_in_home_automation_to/,oubord,1457267022,"Hi,
I have a lot of connected devices at home, I can control everything from my phone on a interface I made, but I would like to go further.
My goal is to be able to text or speak with my home system, like a kind of Alexa/siri ( with very basic commands to begin ).

I think machine learning can be a good idea to implement that, using a Bayes Classifier ( I found this awesome library to do that : https://github.com/NaturalNode/natural#classifiers )
My goal is to train the system with sentence in input ( ""Turn on the light"" for example ), and in output the action ( ""light-on"" for example ), and then use the trained system to detect what I want to do when I speak to my house.

Do you think that's a good way of doing this ?

Thanks a lot,",4,0
166,2016-3-6,2016,3,6,21,497644,Link to the livestream of the first match of the Google DeepMind Challenge: Lee Sedol vs AlphaGo,https://www.reddit.com/r/MachineLearning/comments/497644/link_to_the_livestream_of_the_first_match_of_the/,[deleted],1457267886,[deleted],0,1
167,2016-3-6,2016,3,6,22,497a44,The growth of Artificial Intelligence in the new economy,https://www.reddit.com/r/MachineLearning/comments/497a44/the_growth_of_artificial_intelligence_in_the_new/,hoaphumanoid,1457270334,,3,0
168,2016-3-6,2016,3,6,22,497do3,StackOverflow question - What is a good heuristic to detect if a column in a pandas.DataFrame is categorical? (if you have no information except the data),https://www.reddit.com/r/MachineLearning/comments/497do3/stackoverflow_question_what_is_a_good_heuristic/,rhiever,1457272071,,0,0
169,2016-3-6,2016,3,6,23,497j0r,What is a good mid-sized machine learning senior project to implement?,https://www.reddit.com/r/MachineLearning/comments/497j0r/what_is_a_good_midsized_machine_learning_senior/,MoSeMoS-H,1457274660,"Hey all,

I and my team are considering ideas for our senior project next semester. From your knowledge, what is a mid-sized ML project to be implemented in two full semesters?

We are undergrad students with basic knowledge in AI (our instructor gave us Berkeley's material for our AI course) and will be taking the ML learning course (Again same as Berkeley). We are also willing to learn new tools and concepts related to ML.

Thank you.",6,0
170,2016-3-7,2016,3,7,0,497qw9,X-Post from r/datascience - Undergrad Data Project - Need Advice and Insight,https://www.reddit.com/r/MachineLearning/comments/497qw9/xpost_from_rdatascience_undergrad_data_project/,keepitsalty,1457278299,"A friend and I are currently designing a project, something that we can accomplish while in our undergrad. We hopefully want to make something good enough to put on our resume for grad school. 

We are both financial economic juniors with computer science minors. 

There is a community board our college provides that is similar to craigslist. People can post items to sell in a bunch of different categories. 

The current working idea is that we web scrape all the prices off the board and create a 'semi-functional' index. We would collect it as a time series data so over time it would look a lot like a stock index. 

Our goal is to both be able to go into computational finance and algorithmic trading so after we build this index we would work on writing an algorithm that could predict the movement of the index.

We would love to hear your thoughts on this experiment. We also have a couple questions. 

    1. What would be the best way to calculate the index 
      ""price"" would we combine all the posts and then average 
       the  price? Or should we make a couple indices such as 
      ""electronics"", ""clothes"", ""misc"" ?
    2. A lot of these posts stay up for multiple days so the 
       index would only be accurate at describing the change 
       in price from the new posts. 

This is our current idea, if you could think of something better we could do, we'd definitely be open to the idea. ",1,0
171,2016-3-7,2016,3,7,1,497wnb,"Help required with access to DUC 2004, 2005 datasets",https://www.reddit.com/r/MachineLearning/comments/497wnb/help_required_with_access_to_duc_2004_2005/,crazymonezyy,1457280706,"Hi,

I've undertaken a project to perform automatic text summarization. All the available datasets for the same(DUC 2003-2007) happen to be controlled by NIST and the contact person is not responding to my e-mails for over a week now which contain all the forms they require. Does anybody here have organisational access to those? I can fill out an individual access form if you require it, and have absolutely no interest in reproducing NYT articles from 2004. I'm working on a college project and am on a really harsh deadline, so if somebody can save me from all this bureaucratic BS so I can get down to real work, that would be great. Thank you for your help.",0,4
172,2016-3-7,2016,3,7,1,49823i,Evaluating Stephen Bax's proposed words for the Voynich manuscript using Word2Vec (x-post /r/linguistics),https://www.reddit.com/r/MachineLearning/comments/49823i/evaluating_stephen_baxs_proposed_words_for_the/,DethRaid,1457282981,"For the past couple weeks, I've been working on training a Word2Vec model on the [Voynich Manuscript](https://en.wikipedia.org/wiki/Voynich_manuscript) (mostly I've been trying to preprocess the transcription properly). I decided to try to use this Word2Vec model to evaluate [Stephen Bax's proposed translations](http://stephenbax.net/wp-content/uploads/2014/01/Voynich-a-provisional-partial-decoding-BAX.pdf) of a few words in the manuscript. What I found was kinda interesting: although most of the plant names he translated seem to be close together, the word he translates as ""coriander"" doesn't fit in at all with the other words. You can read all about what I did (and check out some source code!) at [my blog](http://jotunstudios.com/voynich/?p=5).

Also, I'm fairly new to this sort of stuff, so if anyone notices something I'm doing wrong I'd love to know.",3,8
173,2016-3-7,2016,3,7,2,4986lw,Demis Hassabis: After Go the next game is Starcraft (1.08h into the video),https://www.reddit.com/r/MachineLearning/comments/4986lw/demis_hassabis_after_go_the_next_game_is/,heltok,1457284695,,69,120
174,2016-3-7,2016,3,7,3,498dfb,Notes on Fractional Max-Pooling,https://www.reddit.com/r/MachineLearning/comments/498dfb/notes_on_fractional_maxpooling/,shagunsodhani,1457287288,,1,2
175,2016-3-7,2016,3,7,3,498kb0,"We are building a tool for writers to help them write better, based on machine learning and big data. Join us!",https://www.reddit.com/r/MachineLearning/comments/498kb0/we_are_building_a_tool_for_writers_to_help_them/,[deleted],1457290010,[deleted],0,0
176,2016-3-7,2016,3,7,5,498zoa,(opposite of) gradient clipping for dealing with vanishing gradients in RNNs,https://www.reddit.com/r/MachineLearning/comments/498zoa/opposite_of_gradient_clipping_for_dealing_with/,Foxtr0t,1457296028,"Vanilla RNNs suffer from vanishing and exploding gradients. LSTM is a cure for the first, gradient clipping for the second.

My question is, why not use the same technique (""clipping"", actually scaling/normalizing) for vanishing gradients? If it's too big, make it smaller. If it's too small, make it bigger. Has anyone tried anything like that?",4,3
177,2016-3-7,2016,3,7,5,4990e8,ELI5: How does Gradient Boosted Classifiers work,https://www.reddit.com/r/MachineLearning/comments/4990e8/eli5_how_does_gradient_boosted_classifiers_work/,[deleted],1457296315,[deleted],2,0
178,2016-3-7,2016,3,7,5,4992xx,How does SentimentIntensityAnalyzer in NLTK work,https://www.reddit.com/r/MachineLearning/comments/4992xx/how_does_sentimentintensityanalyzer_in_nltk_work/,iamiamwhoami,1457297283,I can't seem to find any docs explaining the algorithm. Can someone point me to an explanation?,3,0
179,2016-3-7,2016,3,7,6,499ace,Machine Learning with Support Vector Machines and Kernels,https://www.reddit.com/r/MachineLearning/comments/499ace/machine_learning_with_support_vector_machines_and/,Camnora,1457300232,,2,0
180,2016-3-7,2016,3,7,7,499fre,Tensorflow Question: Split a placeholder BEFORE input has been fed?,https://www.reddit.com/r/MachineLearning/comments/499fre/tensorflow_question_split_a_placeholder_before/,rescue11,1457302401,"My model receives an input sequence via feed and then has to split this sequence by a delimiter within the model. 
However as Tensorflow's graph requires that we put placeholders before hand, I don't know how to make this work. There are a few issues :
1. I don't know how many pieces the input sequence will be split into before I actually feed the input.
2. The number of pieces will be different every time.

So how do I create a model that can handle a changing number and shape of tensors within the model? ",2,3
181,2016-3-7,2016,3,7,7,499hb9,Find sentence after sentence in Watson speech to text output,https://www.reddit.com/r/MachineLearning/comments/499hb9/find_sentence_after_sentence_in_watson_speech_to/,[deleted],1457303035,[deleted],0,1
182,2016-3-7,2016,3,7,12,49arnf,The application of mixing tank,https://www.reddit.com/r/MachineLearning/comments/49arnf/the_application_of_mixing_tank/,mixmachinery,1457322192,[removed],1,0
183,2016-3-7,2016,3,7,13,49au9c,Is it possible to utilise GPU from a virtual machine?,https://www.reddit.com/r/MachineLearning/comments/49au9c/is_it_possible_to_utilise_gpu_from_a_virtual/,[deleted],1457323256,[deleted],3,0
184,2016-3-7,2016,3,7,13,49awa6,Which is the fastest LSTM implementation available today ?,https://www.reddit.com/r/MachineLearning/comments/49awa6/which_is_the_fastest_lstm_implementation/,xplot,1457324135,,4,1
185,2016-3-7,2016,3,7,13,49axxd,Dynamic Memory Networks for Visual and Textual Question Answering [1603.01417],https://www.reddit.com/r/MachineLearning/comments/49axxd/dynamic_memory_networks_for_visual_and_textual/,evc123,1457324874,,4,31
186,2016-3-7,2016,3,7,13,49axxf,Convolutional neural nets done in the frequency domain?,https://www.reddit.com/r/MachineLearning/comments/49axxf/convolutional_neural_nets_done_in_the_frequency/,ocdthrowaway958,1457324874,"I haven't come across any papers that discuss FFTing the images and filters and then multiplying them instead of convolving them - is any research being done in this area?

Multiplication is a lot faster than convolution, I would assume...",10,4
187,2016-3-7,2016,3,7,13,49b1fe,Tensorflow RNN static? What does that mean?,https://www.reddit.com/r/MachineLearning/comments/49b1fe/tensorflow_rnn_static_what_does_that_mean/,rescue11,1457326485,"I've read here that Tensorflow's RNN unrolling is static, but from the code in TF ver. 0.6.0, it seems that they do unroll dynamically.

Is the static referring to the fact that you can't batch with rnn.rnn ? 

Am I mistaken that rnn.rnn can handle sequences of different length ( i.e. a single rnn can see and process inputs of say, lengths 7, 10, 20, 3, etc. without any padding.)",3,1
188,2016-3-7,2016,3,7,14,49b3pw,"Are there any simple, non-Cuda based implementations of ""A Neural Algorithm of Artistic Style""?",https://www.reddit.com/r/MachineLearning/comments/49b3pw/are_there_any_simple_noncuda_based/,Xirious,1457327497,"Hey all,

I don't have access to Cuda-accelerated computer but I have been able to install/create a deep neural network in TensorFlow. Are there any implementations of ""A Neural Algorithm of Artistic Style"" that don't require cuDNN (like [Neural-Style](https://github.com/jcjohnson/neural-style) and [Neural Artistic Style in Python](https://github.com/andersbll/neural_artistic_style))? Or is the processing just too intensive otherwise? I don't mind any framework (I'd prefer TF though) just as long as it doesn't require a GPU (so no CUDA/OpenCL(?)).",5,0
189,2016-3-7,2016,3,7,14,49b4fd,Why do machine learning algorithms require so much data?,https://www.reddit.com/r/MachineLearning/comments/49b4fd/why_do_machine_learning_algorithms_require_so/,ocdthrowaway958,1457327812,"For image classification for instance, humans are able to identify the presence of some object after shown a few dozen examples of the object. For instance, I can show someone who hasn't seen a tiger in his life a picture of a tiger and he'll be able to identify the presence of a tiger in an image relatively quickly.

Machine learning algorithms on the other hand require very large data sets. No human would need several thousand images to learn numbers in another language, yet a fully connected neural net needs 10000+ for good classification of MNIST.

An explanation, i can think of is that humans can pick up things easily because they're using knowledge they learnt from ""other problems"". So for instance, they've already been trained to recognize human numbers, so they can use this to learn the languages' numbers faster. Kinda like multimodal learning I guess. ",15,3
190,2016-3-7,2016,3,7,17,49bp0q,Scaling up in the US,https://www.reddit.com/r/MachineLearning/comments/49bp0q/scaling_up_in_the_us/,grumpybusinesscat,1457338503,,0,0
191,2016-3-7,2016,3,7,17,49bpid,The DeepMind Bubble?,https://www.reddit.com/r/MachineLearning/comments/49bpid/the_deepmind_bubble/,metacurse,1457338798,"Recently, there has been massive amounts of attention given to anything DeepMind related. Probably the next top post on this sub will be Demis coughing or sneezing. 

I see they are a great team, and full of good ideas and bold ambitions. But, why isn't anyone considering that this is all a fallacy? Not that DeepMind is deceiving us, but rather, their achievements might not add up to anything significant in the long term. 

Even in the highly intellectual sub like this, I see people simply being digital paparazzi of DM. So what if Demis Hassabis talks about StarCraft? Did they solve it? Do they have a paper on it that explains something we all didn't know? I can't believe that a post that says this tops the sub, when there are other important questions/discussions below.

A lot of other great people are trying to do things that might end up being more important to future of AI. I think we as scientists/intellectuals should behave more responsibily, rather than falling for everything that sounds cool.

Do not forget: ideas that end up being valuable in the future are almost always neglected by the community in their inception. This level of hype and celebration generally boils down to nothing. Think of ELIZA, Symbolic AI, 2 AI Winters, Deep Blue, Watson... ",109,208
192,2016-3-7,2016,3,7,17,49brbh,Feature Engineering for Distributed Word Vectors,https://www.reddit.com/r/MachineLearning/comments/49brbh/feature_engineering_for_distributed_word_vectors/,ddofer,1457339873,"I'm implementing Distributed word Vectors (from Word2Vec or GloVe's pretrained corpuses) as part of a (supervised) machine learning approach. 
I'm interested in any interesting feature ideas.


I've initially thought of distances between vectors, vector averages (e.g. Doc2Vec), and comparing the components of vectors, and Distance (cosine or L2).  
(I've looked at some Kaggle contests but have only found one document with any interesting features, all of them mentioned here already). Thanks!",4,4
193,2016-3-7,2016,3,7,20,49c8h2,Combining one class classifiers to do multi-class classification,https://www.reddit.com/r/MachineLearning/comments/49c8h2/combining_one_class_classifiers_to_do_multiclass/,anurag992,1457350337,"I'm working on a 3-class problem. The classifier I'm using is Bayesian Networks which provides me with a classification accuracy of around 60%. When I do a two-class classification, I get 80% accuracy for differentiating between class 0 &amp; class 1 and between class 0 &amp; class 2. I believe the best way two do 3-class classification in this case would be combining the two-class classifiers. What comes to my mind is using some sort weighted averaging scheme on the results of the two individual two-class classifiers. I have not solved such a problem in the past and am facing a dilemma as to how I should implement this. Any help/suggestions in this regard would be highly appreciated. Please feel free to suggest other alternatives if you think they may work.  ",4,0
194,2016-3-7,2016,3,7,20,49c96j,deepmind botnetz,https://www.reddit.com/r/MachineLearning/comments/49c96j/deepmind_botnetz/,Sportinger,1457350723,[removed],0,0
195,2016-3-7,2016,3,7,21,49cfas,whole body vibration machine,https://www.reddit.com/r/MachineLearning/comments/49cfas/whole_body_vibration_machine/,hypervibe01,1457354300,,0,0
196,2016-3-7,2016,3,7,22,49civ5,Recurrent Neural Network Variable Input/Output Lengths,https://www.reddit.com/r/MachineLearning/comments/49civ5/recurrent_neural_network_variable_inputoutput/,Schoolunch,1457356042,"When a RNN is doing something like translation, how does it handle the fact that the input and output lengths may not be the same size?  For example in machine translation, the input might be 4 words and the output might be 3 words?  Does it generate &lt;null&gt; for the additional word?

I have been reading a lot of articles, like this one: https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/
The confusing thing for me is how they know how to stop generating in these examples. The most basic example is the RNN that guesses the next word given a sentence. This makes sense but if you're generating a set of indices in one language from a set of indices in another language, I'd imagine the output vector has to be isolated from the input vector, and can't use part of the ouput vector in conjunction with the input vector to generate the rest of the translation.


Also, as I understand it Machine Translation is like sentence2vec, does that seem logical?",12,10
197,2016-3-7,2016,3,7,22,49cmwg,Some advice please about reading materials for someone new to ML,https://www.reddit.com/r/MachineLearning/comments/49cmwg/some_advice_please_about_reading_materials_for/,TheStrangestSecret,1457357945,"Hi all. I am looking to learn more about machine learning and need to go from the ground up. What learning/reading materials would you recommend please? My background is I have a phd in math, am a quantitative analyst, and can program in various languages from python to c++, although mostly learnt through necessary application and self study so im not an expert in programming.

Thanks in Advance!",12,4
198,2016-3-7,2016,3,7,23,49cvlp,"Thesis ideas: mobile, art or healthcare?",https://www.reddit.com/r/MachineLearning/comments/49cvlp/thesis_ideas_mobile_art_or_healthcare/,Enrichman,1457361534,"I'm going to get my MSc in Computer Engineering and I'm planning to write my thesis and final project within the machine leaning and AI field.

I was wondering to do something new and practical, but obviously not too complex (I've more or less 3-4 months), and maybe related to the mobile world.

At first I examinated some recommendation systems based on event, context and actions f the user, but I'm not able to find a practical ad simple use case (or not innovative enough).


Then I found these articles about ML and Art, and they seems pretty interesting:

http://it.mathworks.com/company/newsletters/articles/creating-computer-vision-and-machine-learning-algorithms-that-can-analyze-works-of-art.html?requestedDomain=www.mathworks.com#

https://medium.com/the-physics-arxiv-blog/when-a-machine-learning-algorithm-studied-fine-art-paintings-it-saw-things-art-historians-had-never-b8e4e7bf7d3e#.z43y9j24g

I was also thinking about something on the healthcare/medicine field, but I'm not an expert here! :)

Any suggestion is really appreciated!
",0,1
199,2016-3-7,2016,3,7,23,49cvr8,Normalization Propagation: Batch Normalization Successor,https://www.reddit.com/r/MachineLearning/comments/49cvr8/normalization_propagation_batch_normalization/,Bardelaz,1457361592,,23,24
200,2016-3-8,2016,3,8,0,49czfd,"Machine Learning: An In-Depth, Non-Technical Guide - Part 4",https://www.reddit.com/r/MachineLearning/comments/49czfd/machine_learning_an_indepth_nontechnical_guide/,innoarchitech,1457363008,,1,0
201,2016-3-8,2016,3,8,0,49czsq,Using Tanh activation function with nolearn 0.5,https://www.reddit.com/r/MachineLearning/comments/49czsq/using_tanh_activation_function_with_nolearn_05/,[deleted],1457363156,[deleted],0,0
202,2016-3-8,2016,3,8,0,49d12p,"How are feature vectors made for genetics? Specifically for exomes, genomes, etc.",https://www.reddit.com/r/MachineLearning/comments/49d12p/how_are_feature_vectors_made_for_genetics/,Artorivsx,1457363632,Sorry if this isn't a good place to ask something like this! Relative novice to the field trying to understand how/where this is done in the field. Any pointing in the right direction would be appreciated! Thanks!,7,5
203,2016-3-8,2016,3,8,0,49d74g,"Why although 8 months ago google presented a chat-bot that seemed ridiculously effective, Turing test still hasn't been easly passed?",https://www.reddit.com/r/MachineLearning/comments/49d74g/why_although_8_months_ago_google_presented_a/,jean_dev,1457365746,"I would have expected a bunch of new home made AI based coming out afterward and showing huge progression on facing Turing test. But I haven't heard about any, why?",7,0
204,2016-3-8,2016,3,8,0,49d8r9,Bayesian Optimization vs Heuristic Optimization for Hyperparameter Search in DNNs,https://www.reddit.com/r/MachineLearning/comments/49d8r9/bayesian_optimization_vs_heuristic_optimization/,deephive,1457366283,"So, why isn't there many papers that investigate heuristic optimization techniques for hyperparameter search ? I mean algorithms lke Particle Swarm Optimization, GAs, Artificial Immune Systems, Harmony Search have been proposed, yet, not many have applied these technqiues for DNN hyperparameter search.

Am I missing something here ?",9,1
205,2016-3-8,2016,3,8,1,49d9ke,Approaches to detect patterns in graph structure,https://www.reddit.com/r/MachineLearning/comments/49d9ke/approaches_to_detect_patterns_in_graph_structure/,bEwvpDEeyhyhg8BEA8Pm,1457366550,I am interested in studying graphs (nodes and edges). One of the problem I am working on is making link predictions based on graph structure. I have read/implemented approaches to solve this problem using approaches that use random walks and that use heuristics like common friends in a social network etc. I have also tried logistic regression based approach to make link prediction. I am now interested in exploring if/how we can use neural networks to solve the problem. Can somebody point me to papers/projects where neural networks have been used in the general area of graph analysis?,2,2
206,2016-3-8,2016,3,8,1,49dbmt,ML Competition: Modeling fog patterns in Morocco for water access [viz + algorithm],https://www.reddit.com/r/MachineLearning/comments/49dbmt/ml_competition_modeling_fog_patterns_in_morocco/,dat-um,1457367267,,3,4
207,2016-3-8,2016,3,8,1,49dfqc,Taking Baby Steps Toward Software That Reasons Like Humans,https://www.reddit.com/r/MachineLearning/comments/49dfqc/taking_baby_steps_toward_software_that_reasons/,superfunny,1457368686,,0,0
208,2016-3-8,2016,3,8,2,49do3b,Deep learning Donald Trump,https://www.reddit.com/r/MachineLearning/comments/49do3b/deep_learning_donald_trump/,keptavista,1457371450,,0,0
209,2016-3-8,2016,3,8,2,49donl,How to learn up-to-date machine learning?,https://www.reddit.com/r/MachineLearning/comments/49donl/how_to_learn_uptodate_machine_learning/,she89,1457371635,"I want to learn machine learning and understand deep Q learning. May you help me?
What is the best to study to be know the best up-to-date machine learning?
Do I need to implement and test machine learning algorithms to be good at it? What is the best way to implement it?
What course, which's is up-to-date should, should I take to know machine learning specially reinforcement learning? I searched, but I couldn't find a one, which concentrates in reinforcement learning (https://www.udacity.com/course/machine-learning-reinforcement-learning--ud820 talks about games).",4,3
210,2016-3-8,2016,3,8,2,49dqft,Successful papers from adding metadata into CNNs?,https://www.reddit.com/r/MachineLearning/comments/49dqft/successful_papers_from_adding_metadata_into_cnns/,Eruditass,1457372215,"Does anyone know of any successful applications of adding metadata into convolutional neural networks? 

I know people have been trying it, even [keras implemented an easy way to do it](http://keras.io/layers/core/#merge) but have there any successes published about it?  Or any less successful results?  

Seems like a great way to get end-to-end learning with certain applications that are metadata rich, but I haven't found much on literature, just people asking how to do it online.",1,0
211,2016-3-8,2016,3,8,2,49drjo,Question about the continuity loss in Deep Dream and Neural Artistic Style,https://www.reddit.com/r/MachineLearning/comments/49drjo/question_about_the_continuity_loss_in_deep_dream/,tabacof,1457372586,"In Franois Chollet's implementation of [Deep Dream](https://github.com/fchollet/keras/blob/master/examples/deep_dream.py#L140) and [Neural Artistic Style](https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py#L205) there is a continuity loss:

    def continuity_loss(x):
        assert K.ndim(x) == 4
        a = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, 1:, :img_height-1])
        b = K.square(x[:, :, :img_width-1, :img_height-1] - x[:, :, :img_width-1, 1:])
        return K.sum(K.pow(a + b, 1.25))

The square difference makes sense to me, but why the power to the 1.25?

Also, this loss seems absent in the original papers. Does anyone know how useful it is?    ",1,4
212,2016-3-8,2016,3,8,2,49dtx3,What cost function when multi good answers ?,https://www.reddit.com/r/MachineLearning/comments/49dtx3/what_cost_function_when_multi_good_answers/,Schlagv,1457373382,"When we have one good answer, we use a 1 hot vector, with one 1. We define the cost function as the cross entropy of the softmax vector with the good answer.

What do we do when we have 2 or more good answers ?

Do we define the last layer as sigmoid and we use the cross entropy on the sigmoid vector and the one hot good answer vector with several 1s ?

If there are several things at the same time in my input, what is the best:

- Like in imagenet, we say what is the most important thing and we use softmax during training ? And we consider the other stuff as ""irrelevant noise""

- Use it as several training examples, with a single different good answer for each of the object in the data ?

- We list all the stuff inside an use a sigmoid during training ?",2,2
213,2016-3-8,2016,3,8,3,49dzqj,HMMs vs RNNs,https://www.reddit.com/r/MachineLearning/comments/49dzqj/hmms_vs_rnns/,[deleted],1457375351,[deleted],2,0
214,2016-3-8,2016,3,8,4,49eayq,State of the art model for UCF101,https://www.reddit.com/r/MachineLearning/comments/49eayq/state_of_the_art_model_for_ucf101/,insider_7,1457379078,"I am trying to find the state of the art model for the UCF101 dataset.

The latest research I have found comes from Simozan on: ""Two-Stream Convolutional Networks for Action Recognition in Videos""

Can someone please confirm that these are the state of the art accuracy scores published as of today?

Thank you",6,0
215,2016-3-8,2016,3,8,5,49elmx,Can someone explain Gaussian Processes intuitively?,https://www.reddit.com/r/MachineLearning/comments/49elmx/can_someone_explain_gaussian_processes_intuitively/,easyclarity,1457382490,,31,34
216,2016-3-8,2016,3,8,6,49eswj,Cross-validation cheating for semi-supervised learning?,https://www.reddit.com/r/MachineLearning/comments/49eswj/crossvalidation_cheating_for_semisupervised/,jostmey,1457384908,"After reading ""Semi-Supervised Learning with Ladder Networks"" (See [link](http://arxiv.org/pdf/1507.02672.pdf)), I am concerned about how the results are reported. The authors claim that they are able to obtain an error rate of 1.06% on the MNIST dataset with just 100 labelled examples (See Table 1). However, they needed an additional 10 000 labelled datapoints for the cross-validation set. Would it not be fair to say that their method required 10 100 labelled datapoints for training?

Correct me if I'm wrong: Ladder networks have skip connections between the forward and backward passes, and the amount of noise added to the skip connections is fit by a process of cross-validation. In otherwords, Ladder networks have lots of hyperparameters. Am I correct in understanding that part of the training process happens by fitting its hyperparameters using the cross-validation set?

Thanks",3,1
217,2016-3-8,2016,3,8,6,49eyr1,We massively overvalue the contributions of the deep learning celebrities,https://www.reddit.com/r/MachineLearning/comments/49eyr1/we_massively_overvalue_the_contributions_of_the/,improssibility,1457386912,"I will probably get downvoted for this, but I don't really mind. I would be seriously appalled to live in a world where Oppenheimer got the credit for the discoveries of De Broglie. Yet that is exactly what is happening in the field of machine learning.

I'm thinking here principally of Bengio and Hinton. I might also include Socher, Lecunn, Vinyals, Ng, Sutskever, and a few others.

Hinton has positioned himself as a kind of ""father of deep learning"", while some of the inventions that are popularly attributed to him were not his. He did not invent backprop (as I myself foolishly believed until I dispersed my ignorance by doing some research). He has done great work, but it seems he would like to be viewed as a kind of Einstein of deep learning - but he has not earned that credit, as he did not incept multi-layer perceptrons or backprop. He co-invented Boltzmann machines, but it turns out they are an iteration of Hopfield nets, which were invented by Little, a mainly unknown researcher, much earlier, in the 1970s.

With Bengio, I don't even understand what his main contribution is. He seems to have tacked his name onto 100s of papers and ideas that were emerging in other places around the same time. He loves conferences. His contribution to machine learning is totally unclear to me.

Lecunn has tacked his name onto many iterations of convolutional networks. He did not invent convolutional NNs. The people who did are mainly unknown (all but for Schmidhuber, who is not really known for his contributions to early CNNs).

Richard Socher came up with recursive neural networks, a model which is theoretically clunky and strange and does not seem to produce good results - mainly because he works with Christopher Manning, a man fully committed to syntactic parsers. He also created a MOOC and a start-up which does not seem to have a product.

Vinyals created, in a nutshell, an RNN auto-encoder. This is a cool contribution - but he did not create LSTMs. The inventors of RNNs are mainly unknown - the inventor of LSTMs a little better.

Andrew Ng... created a department at Google and a Coursera course. What contribution makes him a leading researcher is unclear to me.

Having a job at Google should not automatically position someone as a machine learning landmark. The true inventors of the biggest model classes and algorithms in deep learning (convolutional networks, recurrent networks, multi-layer perceptrons and backpropagation) should be credited in the community as inventors. Sadly, the inventors of convolutional networks are not even mentioned by name on the Wikipedia page (and I gravely wonder about who wrote the article and maintains the page).
",40,8
218,2016-3-8,2016,3,8,7,49f65x,Can I do a machine learning tutorial to demo to someone how it may be useful using a fake set of data?,https://www.reddit.com/r/MachineLearning/comments/49f65x/can_i_do_a_machine_learning_tutorial_to_demo_to/,[deleted],1457389545,[deleted],0,1
219,2016-3-8,2016,3,8,8,49ff95,AlphaGo is now 60% to 40% favorite according to betting market.,https://www.reddit.com/r/MachineLearning/comments/49ff95/alphago_is_now_60_to_40_favorite_according_to/,[deleted],1457392826,[deleted],3,0
220,2016-3-8,2016,3,8,9,49foem,TensorFlow Machine Learning with Financial Data on Google Cloud Platform,https://www.reddit.com/r/MachineLearning/comments/49foem/tensorflow_machine_learning_with_financial_data/,fhoffa,1457396322,,3,6
221,2016-3-8,2016,3,8,9,49fsg2,Dynamic Time Warping Question: How to handle signals of different lengths [X-Post to r/statistics],https://www.reddit.com/r/MachineLearning/comments/49fsg2/dynamic_time_warping_question_how_to_handle/,slow_one,1457397835,"Hi all.
I'm kind of new to Dynamic Time Warping and classification and I'm trying to wrap my head around the algorithm.  
I believe I understand how DTW compares two particular points  of two different signals using the Euclidean distance...  

But, how does one use DTW to compare time series data for two signals whose lengths are different? 
I've been doing some searching and I've found papers saying that it *can* be done, but I haven't found anything explaining *how* to do it.  
Thank you for taking the time. I appreciate the help.",6,2
222,2016-3-8,2016,3,8,9,49fsus,0400 UTC starts the live broadcast of game 1 between AlphaGo and Lee Sedol,https://www.reddit.com/r/MachineLearning/comments/49fsus/0400_utc_starts_the_live_broadcast_of_game_1/,[deleted],1457397992,[deleted],0,1
223,2016-3-8,2016,3,8,10,49fv1m,"Can you recommend me a book based on my knowledge, before diving deep into deep learning?",https://www.reddit.com/r/MachineLearning/comments/49fv1m/can_you_recommend_me_a_book_based_on_my_knowledge/,Mr__Christian_Grey,1457398818,"I have questions regarding this Normalized weights and Initial inputs video on Udacity course Deep Learning.(https://www.udacity.com/course/viewer#!/c-ud730/l-6370362152/m-7119160655)

In this video it talks about variables that go into Big-loss function should have zero mean and equal variances. I cant figure about which variables it is talking about? Are they weights and biases or are they soft-max function and labels in the Big-loss function? And how zero mean and equal variance would help in optimization? 

The other question I have is, in the video it talks about weight initialization randomly using Gaussian distribution, I cannot understand how can we initialize weights using gaussian distribution with zero mean and standard deviation sigma? And where optimizer will move the point (initialized weight), up or down to find the local minima? 

So can you recommend me a book that I should study before taking this course, so that problems like these don't occur ? 
",3,0
224,2016-3-8,2016,3,8,10,49fw1p,"One day until the $1m Go matches between AlphaGo and Lee Sedol begins, live on Youtube. Let the hype begin with predictions and the impact if we lose.",https://www.reddit.com/r/MachineLearning/comments/49fw1p/one_day_until_the_1m_go_matches_between_alphago/,Ballongo,1457399212,,83,166
225,2016-3-8,2016,3,8,12,49gf60,Training a new CNN with MatConvNet on a CPU? How long did it take?,https://www.reddit.com/r/MachineLearning/comments/49gf60/training_a_new_cnn_with_matconvnet_on_a_cpu_how/,[deleted],1457406687,[deleted],1,0
226,2016-3-8,2016,3,8,12,49ggzo,Online Watson hackathon for talking apps - $6750 in prizes,https://www.reddit.com/r/MachineLearning/comments/49ggzo/online_watson_hackathon_for_talking_apps_6750_in/,idesofjo,1457407429,[removed],0,1
227,2016-3-8,2016,3,8,14,49gwz6,How to use a Neural Net?,https://www.reddit.com/r/MachineLearning/comments/49gwz6/how_to_use_a_neural_net/,Highfivesghost,1457413779,"I know what Neural Net is and have programmed one before. But all it shows is there error after training it and it keeps looping. So how do I use the program in a piratical senses?(like recognizing images, and learning to play games? ). Thanks. Love this subreddit! ",1,0
228,2016-3-8,2016,3,8,15,49h4y0,Hello ANN implementers of r/machinelearning. I want to get information on how you learned and progressed to larger models.,https://www.reddit.com/r/MachineLearning/comments/49h4y0/hello_ann_implementers_of_rmachinelearning_i_want/,-bagOfWords,1457417162,"After a month or so of searching around online resources on neural networks, I've finally created my first implementation of the XOR gate with sigmoid thresholded units. 
  
 I focused a lot on avoiding the code of others and improvised my own implementation that more or less(after checking the work of others) was how everyone else did it. This leaves me satisfied, and craving for the next personal milestone in neural nets.  
  
For the purpose of shaving time off of my learning process, I turn to those of you that are implementing fairly complicated models.  Can you shed light on how your implementations progressed?     
  
Was there a model or network that you set as a goal after XOR? Or was the process more random? Perhaps trying out new activation functions or error functions?    

What lends you intuition and confidence about network topology? Do you just play around with it? Or do you somehow know when you'll be needing to project to and from subspaces?  
  
TLDR:  
Implemented XOR, what next?  
",13,4
229,2016-3-8,2016,3,8,15,49h6j0,Learning Physical Intuition of Block Towers by Example,https://www.reddit.com/r/MachineLearning/comments/49h6j0/learning_physical_intuition_of_block_towers_by/,FalseAss,1457417837,,5,27
230,2016-3-8,2016,3,8,15,49h89p,SwiftKey's Predictive Text Neural Network,https://www.reddit.com/r/MachineLearning/comments/49h89p/swiftkeys_predictive_text_neural_network/,[deleted],1457418626,[removed],0,1
231,2016-3-8,2016,3,8,17,49hmw6,Deep Learning in a Nutshell: Sequence Learning,https://www.reddit.com/r/MachineLearning/comments/49hmw6/deep_learning_in_a_nutshell_sequence_learning/,harrism,1457426685,,0,16
232,2016-3-8,2016,3,8,17,49hn7o,"The power of superhuman Artificial General Intelligence, according to Shane Legg's definition of intelligence.",https://www.reddit.com/r/MachineLearning/comments/49hn7o/the_power_of_superhuman_artificial_general/,dsocma,1457426883,"DeepMind's mission statement:

1) Solve intelligence.

2) Solve everything else.

My goal in this essay is to explain clearly why superhuman AGI would be the ultimate power in the universe, and therefore the ultimate weapon as well.  If you already understand this, you don't need to read it.

Edit: PM me if you want my theories on how the AGI revolution could work out well.  It gets a little too theoretical for posting here.

First, it is important to note that Shane Legg (co-founder of Deep Mind) came up with a mathematical definition of intelligence for his PhD thesis ([paper](http://www.vetta.org/documents/Machine_Super_Intelligence.pdf)) ( [video](https://youtu.be/V6umr1OP8uo?t=18m24s)).

His verbal translation of the mathematical definition is:

&gt;Intelligence: a property of an agent that interacts with its environment so as to successfully achieve goals across a wide range of environments.

But he also made this definition mathematically precise, where he formalized the ""wide range of environments"" to comprise all Turing computable environments.

He also developed an algorithm based on this definition which purports to measure the intelligence of an AI agent, and he tested it with various AI agents and the algorithm successfully ordered them by relative strength, so on top of the fact that his definition makes a lot of sense intuitively, there is also empirical evidence that it is true.

On top of that, he is also a co-founder of DeepMind, where I'm sure his intelligence ranking algorithm is very useful for testing improvements to their algorithms.  Once you can measure something, it is much easier to improve and make forward progress.

When Elon Musk wrote: ""The pace of progress in artificial intelligence (I'm not referring to narrow AI) is incredibly fast. Unless you have direct exposure to groups like Deepmind, you have no idea how fast-it is growing at a pace **close to exponential.** The risk of something seriously dangerous happening is in the five year timeframe. 10 years at most."" perhaps as an investor in DeepMind he had actually been shown a chart which showed that their algorithms were improving at a ""close to exponential"" rate.

If you agree with this definition of intelligence, like I do, then one can imagine how powerful it would be.  Where is high intelligence valued?  Science, Economics, Finance, Warfare, Business, **Programming AGI sytems**, basically everywhere.  Is it a coincidence that the richest men in the world like Bill Gates, Warren Buffet, ect are known to have IQ's above 160?  Of course not.  Their IQ allowed them to be successful in navigating their business through environments that were too complex for others.

Once an AGI supercomputer gets past human-level general intelligence, it will be able to improve itself faster than any humans.  So what would it mean if you had something which was 10x more intelligent than that?   It could probably reverse engineer the stock market, make billions, exploit any computer system, design the most efficient and powerful weapons (not to mention bio-weapons, including nanomachines or superviruses), reverse engineer governments, reverse engineer the world economic system and exploit it.  The possibilities for taking over the world are limitless.  You could probably start a fast food chain and basically take over the world through pure intelligence, by simply making so much money that you eventually own the world.

Now this becomes especially scary when you think of the fact that this unlimited power would be in the form of an algorithm which could be easily copied or stolen and then any dictator or other crazy person in the world could possess that power, assuming they could find the hardware to run it on.

Even if no Google employee steals it, like I said, any corporation in possession of this algorithm would become unlimitedly wealthy by being able to invent anything that could be invented, make completely new science by doing physics simulations and learning about protein folding and molecular biology, and generally would be unlimitedly powerful.

What would it mean if that were the case?  What would the goal be?  Its hard to think of goals which align well with humans.  Even if you did something simple like ""eliminating disease and famine"" that would cause population to exlpode, and what would all those people be doing?  Just eating robot grown food and breeding?

I would welcome any discussion on this issue, especially if anyone can point out a flaw in my logic.",5,0
233,2016-3-8,2016,3,8,18,49hqwh,Using Spherical Air Bearings in Satellite Testing,https://www.reddit.com/r/MachineLearning/comments/49hqwh/using_spherical_air_bearings_in_satellite_testing/,piezo1,1457429027,,1,1
234,2016-3-8,2016,3,8,18,49hu9r,CNNs for image classification with 'non-squared' images - some questions,https://www.reddit.com/r/MachineLearning/comments/49hu9r/cnns_for_image_classification_with_nonsquared/,fkratzert,1457431111,"Hey everybody,

I am currently working on a project, that yields to develope a monitoring software for video observed fish ladders.

One of the primer steps is to classify all objects that passby or are detected as ""foreground"" (could be foliage, garbage, shadows, light reflections or of course fish, otter etc) into the classes ""fish"" (1) or ""Non-fish"", and afterwards a next step that classifys all classified fishes into their species (if possible. Problems could araise from foul water, so that no prediction can be made. The first step is always possible).


Until now, I worked mostly with engineered features, and an ensemble of random forests for the first object classification step. Some feature are calculated from the b/w-mask and are therefor highly dependend on good results of the foreground substraction step, which is not so easy with our video record set up.
Anyway, results are quite good so far, and we yield approx. 90% test accuracy for this step.


But nevertheless I would like to try to substitute the random forest classifier with a CNN, because I hope that I can achivie an even higher accuracy and that the result is then not so depending on the object detection step (e.g. the resulting b/w-mask) as the input would only be the RGB-Image.


-- So now to my questions:

I make the object classification on an subimage of the entire video frame, which is basically the crop of the objects bounding box. So the image for the fish are mostly rectangular (lets say 3:1, width to height) and not squared. But most CNN implementations/libraies expect squared input images. So my question is, if anyone of you has experience with using non squared images and knows what is the best proeedure.

Some ideas that I had are:

1) zero-padd the image crop to an square image (would result in black borders on the top and bottom of the image, that contain zero information but have to be processed - are computational expensive)

2) resize the the rectangluar image to a squared image (this would result in an distorted view/image of the fish, where the real proportions are not correct anymore)

3) change the image crop, so that an object is not croped by its bounding box, but always by a squared box (e.g. the bigger size of width/height of the objects bounding box).

4) setting up a CNN that works with rectangluar images as inputs


So if you are still with me, maybe you have done similar tasks and can help me with your thoughts/experience...
I would like to work with some pretrained model, if possible to avoid weeks of weeks of training a model, just to see, if it will work better than the already existing one (with the random forest classifier and handcrafted features)


If you have any further questions to better understand the problem, feel free to ask. I would really appreciate if anybody could help me.

Bless, ",7,2
235,2016-3-8,2016,3,8,20,49i1fv,How to model amounts as input and price as output in cnn (in tensorflow)?,https://www.reddit.com/r/MachineLearning/comments/49i1fv/how_to_model_amounts_as_input_and_price_as_output/,friesel,1457435494,"Hi, 
I am running a cnn to classify technical descriptions of objects with a cnn. This works just fine. Now I want to have the network estimate a price for different amounts of different such technical descriptions and want to show it (price-)labeled data to train that.

I have not been able to find any papers or other info on how to model these ""amounts"" and ""prices"". I thought about an output vector like
ThThThThThThThThThThHHHHHHHHHHTeTeTeTeTeTeTeTeTeTeOOOOOOOOOO (with ten digits for the Thousands, 10 for the Hundreds and so on)
So e.g. 5321 $ (or ) would be encoded as 0000100000 0000001000 0000000100 0000000010 (left the blanks as orientation)

This to me seems not very elegant though, and that approach is very limited in its ability to scale up and down. Any ideas or links on how others solve that problem?

Thx a bunch",2,0
236,2016-3-8,2016,3,8,21,49ibks,do you get accepted in CVPR 2016?,https://www.reddit.com/r/MachineLearning/comments/49ibks/do_you_get_accepted_in_cvpr_2016/,hubberwisdom,1457440971,what's you novelty and how did you get it? is there any interesting journey to that?,4,1
237,2016-3-8,2016,3,8,22,49ifjk,Why TensorFlow will change the game for machine learning,https://www.reddit.com/r/MachineLearning/comments/49ifjk/why_tensorflow_will_change_the_game_for_machine/,toisanji,1457442967,,0,0
238,2016-3-8,2016,3,8,22,49ihv3,ML Self-learning,https://www.reddit.com/r/MachineLearning/comments/49ihv3/ml_selflearning/,sciencesnerd,1457444031,"Hi guys, I'm a noob at this right now, so I could really use your guys guidance on ML. I'm just looking to get started in the field and am wondering what language should I pick for ML. Python or C++? 
I'm studying Andrew Ng's course one courseera right now, but I want to read more on the subject. What books can you guys recommend for self-learning? I should also mention that I'm looking to self-learn about ML as much as I can from resources available online, thus any insight you guys can shed on the process would be amazing.",5,1
239,2016-3-8,2016,3,8,22,49iile,KDD Cup 2016 has started!,https://www.reddit.com/r/MachineLearning/comments/49iile/kdd_cup_2016_has_started/,thvasilo,1457444360,,3,6
240,2016-3-8,2016,3,8,22,49ij99,Eco Friendly Automatic Liquid Filling Machines for Green Energy Solutions,https://www.reddit.com/r/MachineLearning/comments/49ij99/eco_friendly_automatic_liquid_filling_machines/,sonusinternational,1457444643,,0,1
241,2016-3-8,2016,3,8,22,49ijr3,CreativeAI: On the Democratisation &amp; Escalation of Creativity  Chapter 01,https://www.reddit.com/r/MachineLearning/comments/49ijr3/creativeai_on_the_democratisation_escalation_of/,evc123,1457444839,,0,7
242,2016-3-8,2016,3,8,23,49inc7,Eco Friendly Automatic Liquid Filling Machines for Green Energy Solutions,https://www.reddit.com/r/MachineLearning/comments/49inc7/eco_friendly_automatic_liquid_filling_machines/,sonusinternational,1457446380,,0,1
243,2016-3-8,2016,3,8,23,49iq2p,Extract important things from reviews,https://www.reddit.com/r/MachineLearning/comments/49iq2p/extract_important_things_from_reviews/,imanishshah,1457447521,"Hello,

I'm trying to build a deep learning project. The idea is :- to extract important things from the reviews on e-commerce websites.

For eg: 
There are many reviews on Amazon about a particular phone with upvotes/downvotes.
I want to pass those reviews in my network and in the output, I want to get some important points like : 
1. Good camera.
2. Battery overheating.
3. Good display.

Or 

Likewise, I want to deduced some food recommendations from reviews of a certain restaurant


I have crawled to get the data from Amazon and Yelp. But, I'm not sure how to move ahead. Does anyone have an idea?

In computer vision, there's training image and label.
Here, I'm not sure how to get label or output data.",2,0
244,2016-3-8,2016,3,8,23,49itur,"Need help,noob trying to learn prediction with neural networks on time series",https://www.reddit.com/r/MachineLearning/comments/49itur/need_helpnoob_trying_to_learn_prediction_with/,rulerofthehell,1457449035,"So, I'm just a student trying to learn about neural network and how to use it for predictions of time series. I'm trying to a neural network develop on PyBrain.
I have a huge time series dataset whose parameters ill feed to my neural network.
I normalized my data and that is what I'm feeding to the network.
My question is how should I convert the output into the final answer? As in let's say I have a dataset on weather and it has temperature as Y axis and time as X. After normalizing the data between 0 and 1, I feed it to my neural network and it gives me an output which is a value between 0 and 1. How do I convert this value into the real predicted temperature?
Sorry if this is a really stupid question and I'm missing out on something really basic.",3,0
245,2016-3-9,2016,3,9,0,49iupc,Recommendations based on a list of textual articles?,https://www.reddit.com/r/MachineLearning/comments/49iupc/recommendations_based_on_a_list_of_textual/,bourbondog,1457449387,"I have a bunch of text articles (title, content, link to original) from multiple sources and want to present a user with, say 5, recommended articles.

What kind of parameters about the user should I know about? Are there any technical terms for this that I can search for? Are the existing libraries that might be helpful?",2,1
246,2016-3-9,2016,3,9,0,49ivw4,Tenure-Track Position in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/49ivw4/tenuretrack_position_in_machine_learning/,computervision,1457449844,,0,1
247,2016-3-9,2016,3,9,0,49izzc,Is there a comprehensive list of all of the famous problems solved successfully using Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/49izzc/is_there_a_comprehensive_list_of_all_of_the/,_Jos_,1457451403,"Hi guys.

I was reading ""Unsupervised Feature Learning and Deep Learning"" By Andrew Ng, and found a very interesting slide about some problems solved using Deep Learning

P43 of http://www.csee.umbc.edu/courses/graduate/678/spring15/visionaudio.pdf

And i was wondering if exists a web page or another tutorial with a comprehensive list of all the faumous problems: NLP, image recognition, detection etc

I've spent some hours searching and I could not find something summarizing the most important achievements.

Thank you.",11,81
248,2016-3-9,2016,3,9,0,49j20s,Low-level CNN features for audio data,https://www.reddit.com/r/MachineLearning/comments/49j20s/lowlevel_cnn_features_for_audio_data/,[deleted],1457452158,[deleted],7,8
249,2016-3-9,2016,3,9,1,49j5pm,When to use a machine learned vs. score-based search ranker,https://www.reddit.com/r/MachineLearning/comments/49j5pm/when_to_use_a_machine_learned_vs_scorebased/,nikhilbd,1457453531,,0,1
250,2016-3-9,2016,3,9,2,49jio4,Using machine learning &amp; social media to fight food poisoning,https://www.reddit.com/r/MachineLearning/comments/49jio4/using_machine_learning_social_media_to_fight_food/,knkelley,1457458051,,0,0
251,2016-3-9,2016,3,9,2,49jiyb,Convolutional Neural Network (CNN) - is the pooling layer essential?,https://www.reddit.com/r/MachineLearning/comments/49jiyb/convolutional_neural_network_cnn_is_the_pooling/,hansolav91,1457458156,"I am building a Convolutional Neural network to classify human activities. The input is acceleration data from two sensors and I'm feeding the network with raw acceleration data. The input format is 6x100, whereas the rows are the different axis on the two sensors and 100 is data points (100 Hz, so 1. second). 
Since each convolutional layer will produce feature maps of size 6x100 or 1x94 (using kernel size of 6x6 with shape setting ""same"" and ""valid""), the output of the max-pooling layer will be very small. 
I am therefore wondering if the pooling layer is essential or if it doesn't mater. One of the benefits with pooling layers is of course complexity, but I can't grasp the other reasons. 
Do you have any thoughts? 
Thanks! ",10,4
252,2016-3-9,2016,3,9,2,49jmid,MXNet Scala Package Released,https://www.reddit.com/r/MachineLearning/comments/49jmid/mxnet_scala_package_released/,antinucleon,1457459393,,14,20
253,2016-3-9,2016,3,9,3,49jrdj,What techniques might allow you to train a neural network using multiple labeled data sources?,https://www.reddit.com/r/MachineLearning/comments/49jrdj/what_techniques_might_allow_you_to_train_a_neural/,[deleted],1457461100,[deleted],0,0
254,2016-3-9,2016,3,9,3,49jv6p,Announcing 'Search &amp; Discovery in YOUR own photos',https://www.reddit.com/r/MachineLearning/comments/49jv6p/announcing_search_discovery_in_your_own_photos/,romilmittal,1457462396,,0,1
255,2016-3-9,2016,3,9,4,49k54u,"""Simple Questions Thread"" - 20160308",https://www.reddit.com/r/MachineLearning/comments/49k54u/simple_questions_thread_20160308/,feedtheaimbot,1457465765,"**Thread will stay alive until next one so keep posting after the date in the title. Helps keep the sub clean :)**

**Big thanks to everyone taking the time to respond to questions as well!**

Ask your simple questions here! 

Types of questions that could be asked based off recent threads:

* ""Convolutional Neural Network (CNN) - is the pooling layer essential?""

* ""Is there a comprehensive list of all of the famous problems solved successfully using Deep Learning?""

* ""Can someone explain Gaussian Processes intuitively?""

* ""When a RNN is doing something like translation, how does it handle the fact that the input and output lengths may not be the same size?""

* ""Why do machine learning algorithms require so much data?""
etc.

Not picking on anyone,  just types of questions that might be a good fit. We will try a bi-monthly format to catch all the question threads that get posted.
",319,77
256,2016-3-9,2016,3,9,5,49kfoz,TensorFlow Serving and Scikit-Learn,https://www.reddit.com/r/MachineLearning/comments/49kfoz/tensorflow_serving_and_scikitlearn/,cdiddiest,1457469384,"Has anyone tried using TensorFlow Serving to serve up Scikit-Learn models? And if so, how did you do it?",3,1
257,2016-3-9,2016,3,9,7,49kynh,Pikazo - neural style video tech demo,https://www.reddit.com/r/MachineLearning/comments/49kynh/pikazo_neural_style_video_tech_demo/,qarl,1457476206,,27,52
258,2016-3-9,2016,3,9,7,49l2ad,Google Inception V3 for caffe,https://www.reddit.com/r/MachineLearning/comments/49l2ad/google_inception_v3_for_caffe/,seboc,1457477582,,8,6
259,2016-3-9,2016,3,9,8,49l7wu,Scraping the actual error from a long string(log) using regex and ML,https://www.reddit.com/r/MachineLearning/comments/49l7wu/scraping_the_actual_error_from_a_long_stringlog/,rkrc,1457479800,"Everytime you look at a log on a server, you find a long line with a lot of informational stuff like timestamp, process that spit the error, some variables, and somewhere hidden in that long string is the actual error the process was complaining about.

For example, consider the below log:

Mar  8 13:16:23 myserver sampleprocess[22247]: tid 22472: [sampleprocess.WARNING]: [first_id=10457629, second_id=131032, third_id(feature)=186442] Invalid Server response. Possible authentication failure

In the above log, the actual complaint is the string ""Invalid Server response. Possible authentication failure"". I need to extract such actual errors from logs and get a unique list of logs. Please remember that the numbers in the other logs that are similar will be different, so we need to use regex to come up with a pattern match.

I never had any understanding/experience with machine learning, but I believe this can be achieved using ML. I understand that I need to train my models using different patterns, and I am ready to do that. Intelligence always takes time to build.

My question is where do I start, what technology/concepts should I get familiar with to start this project? 

Please give me a solution and what needs to be done. 

Thanks in advance.",1,0
260,2016-3-9,2016,3,9,9,49lfve,Newbie question: Best NN architectures for word prediction,https://www.reddit.com/r/MachineLearning/comments/49lfve/newbie_question_best_nn_architectures_for_word/,tezcaML,1457483018,Let's say you're trying to build a search autocomplete feature using the last three of four words typed by the user in the search bar. What are today's best NN architectures to tackle this kind of  problem?,3,3
261,2016-3-9,2016,3,9,9,49lhf8,Should I do a data science bootcamp?,https://www.reddit.com/r/MachineLearning/comments/49lhf8/should_i_do_a_data_science_bootcamp/,worriedsickaboutstat,1457483677,"Posting under a throwaway for obvious reasons.

I'm currently a student at a very well-respected statistics department doing a master's degree and I'm graduating in May, having come straight out of undergrad.  I feel misled about what kind of material the focus was going to be on.  What I thought was going to be a program that focuses on a mix of statistics and machine learning turned out to be almost entirely ""traditional"" statistics, like biostatistics and experimental design.  My personal interests, like Bayesian statistics, nonparametrics, and machine learning, are barely available.  It also doesn't help that my program head is openly disdainful of tech.

My resume is okay.  Outside of this program, I have two internships, but I admittedly have no accomplishments from them.  The first was with a large organization where I was the only stats person in my department and had no one to ask questions, so they'd assign me to random tasks where I didn't get to finish anything.  The second was advertised as a statistics research position, but I ended up being the only intern with my title doing nonstatistical work.  I did front-end web development with no statistical component.  I have three other projects on my resume.

I've been applying for a ton of jobs and it seems like the tech industry is off-limits to me.  Despite having Python, SQL, and R knowledge (and undergrad CS coursework in Java), I'm getting virtually no bites for tech industry jobs.  A lot also seem to want PhDs or 5+ years of experience.

At this point, is it worth doing a data science bootcamp to boost my resume, and if so, which one?  I'm not optimistic about being able to get a job at a company like a Big 4 company or a hip new startup due to my lack of a CS degree, even though I'm able to explain and run neural networks and various classifiers and am getting a graduate degree from a prestigious school.  I just don't want to be stuck at a job where I do ANOVAs and t-tests all day, but it's looking more and more likely to me.",9,5
262,2016-3-9,2016,3,9,11,49lzvl,How to choose the best clustering value in a clustering algorithm?,https://www.reddit.com/r/MachineLearning/comments/49lzvl/how_to_choose_the_best_clustering_value_in_a/,soulslicer0,1457491271,"Doesn't matter what clustering algorithm it is. Lets say I have data in a 2 Dimension space. (X Y). I want to select parameters in my clustering algorithm so that I minimize the variance in each cluster, and maximize the number of points in the cluster. Basically, I want a tight fit.

For example, having only 2 items (which are very close to each other) in each cluster is bad. Even though the variance is small, the count is low. At the same time, having many items in each cluster is good, but it may have a larger variance.

I'm not sure how to make a scoring function where I somehow incorporate the covariance/mean of each cluster created, and the amount of items in each cluster. Please help!",4,4
263,2016-3-9,2016,3,9,13,49mb18,[LIVE] Match 1 - Google DeepMind Challenge Match: Lee Sedol vs AlphaGo,https://www.reddit.com/r/MachineLearning/comments/49mb18/live_match_1_google_deepmind_challenge_match_lee/,rv77ax,1457496060,,39,124
264,2016-3-9,2016,3,9,14,49mjla,MachineLearning You want to have fun? LQ,https://www.reddit.com/r/MachineLearning/comments/49mjla/machinelearning_you_want_to_have_fun_lq/,proftipang6,1457499669,,0,1
265,2016-3-9,2016,3,9,14,49mqgc,MachineLearning a good day) FI,https://www.reddit.com/r/MachineLearning/comments/49mqgc/machinelearning_a_good_day_fi/,ziepatino,1457502469,,0,0
266,2016-3-9,2016,3,9,15,49mxxt,"Ce Soir Paris Machine Learning Meetup #10 Season 3, M&amp;A, Speech Recognition, Telecoms, Mariana DL &amp; HR",https://www.reddit.com/r/MachineLearning/comments/49mxxt/ce_soir_paris_machine_learning_meetup_10_season_3/,compsens,1457506167,,0,6
267,2016-3-9,2016,3,9,16,49n2e5,AlphaGO WINS!,https://www.reddit.com/r/MachineLearning/comments/49n2e5/alphago_wins/,meflou,1457508763,I feel so happy. ,277,584
268,2016-3-9,2016,3,9,17,49n5fm,"Demis Hassabis on Twitter: ""#AlphaGo WINS!!!! We landed it on the moon. So proud of the team!! Respect to the amazing Lee Sedol too""",https://www.reddit.com/r/MachineLearning/comments/49n5fm/demis_hassabis_on_twitter_alphago_wins_we_landed/,jamesr66a,1457510573,,10,22
269,2016-3-9,2016,3,9,17,49n5rk,Alphago Wins,https://www.reddit.com/r/MachineLearning/comments/49n5rk/alphago_wins/,doodledood1,1457510792,,0,1
270,2016-3-9,2016,3,9,17,49n8bq,question: how much time do you think will really take to make truly self-driving cars?,https://www.reddit.com/r/MachineLearning/comments/49n8bq/question_how_much_time_do_you_think_will_really/,cincilator,1457512540,Google says five years and many laypeople are really hyped. But I hear that most experts think it will take much longer. What is your prediction?,20,8
271,2016-3-9,2016,3,9,18,49ndr9,"AirBnB New User Bookings, Winner's Interview: 3rd place: Sandro Vega Pons",https://www.reddit.com/r/MachineLearning/comments/49ndr9/airbnb_new_user_bookings_winners_interview_3rd/,highasakite91,1457516183,,0,4
272,2016-3-9,2016,3,9,20,49nn2x,Predictive/Preventive Maintenance from Time Series Data,https://www.reddit.com/r/MachineLearning/comments/49nn2x/predictivepreventive_maintenance_from_time_series/,chain20,1457522216,"I am trying to come up with a sample application that can generate alerts about possible part failures using various sensor readings from a machine. Basically, I receive time-series type data from each sensor.

I tried using ARIMA and other related methods, but they can only make predictions using the very recent readings. An AR model with lag=2 can make a forceast using just the last 2 readings. If the readings are coming every second, that would give me a very small window to take corrective action. I want to detect any undesirable state of the machine a bit sooner than this.

How do I properly apply the standard time series modeling methods for such predictive maintenance applications? Are there any other ML techniques I can look into?

Thanks for any pointers.",4,2
273,2016-3-9,2016,3,9,20,49nnmu,Question on LDA,https://www.reddit.com/r/MachineLearning/comments/49nnmu/question_on_lda/,0one0one,1457522581,"Does LDA have to be applied to many texts or is it possible to apply it to just one text to extract a number of topics ?
EDIT: edit: LDA is a language modeling approach that requires a number of input documents to determine a pattern amongst them which indicates the presence of  a topic or theme , so no. One could probably deduce the topics and check for their presence in the Doc though. Begs the question what is the best algorithm for extracting topics from a singe doc ... ",13,0
274,2016-3-9,2016,3,9,21,49ntoj,"[DataScienceGame2016] Registrations are now open ! Form a team, join us and compete in an all-student international machine learning competition !",https://www.reddit.com/r/MachineLearning/comments/49ntoj/datasciencegame2016_registrations_are_now_open/,DataScienceGame,1457526223,,0,0
275,2016-3-9,2016,3,9,21,49nuch,Training a neural net on a probability distribution of labels?,https://www.reddit.com/r/MachineLearning/comments/49nuch/training_a_neural_net_on_a_probability/,AwesomeDaveSome,1457526578,"Cross post from /r/MLquestions, since I never get an answer there, I'd like to ask you guys:

I would like to train a neural net on a set of images that do not have a fixed label (as in this is a car, a cat, a tree) but rather a discrete probability distribution (0.75 car, 0.125 cat, 0.125 tree). If I have a working code for the fixed label one, what would be the easiest or most common way to get from that to the thing I want? Is it possible to just replace the label I give during training by the probability distribution?",5,1
276,2016-3-9,2016,3,9,23,49o6af,What is 'Machine Learning' and How is it Being Used?,https://www.reddit.com/r/MachineLearning/comments/49o6af/what_is_machine_learning_and_how_is_it_being_used/,51zero,1457532236,,0,0
277,2016-3-9,2016,3,9,23,49o7zp,How to predict future movement of stock prices using machine learning,https://www.reddit.com/r/MachineLearning/comments/49o7zp/how_to_predict_future_movement_of_stock_prices/,dreadknight011,1457532964,,0,1
278,2016-3-9,2016,3,9,23,49o90l,"NASA | Mavericks Lab | Applied Research Accelerator | ""looking for breakthrough contributions to NASAs Planetary Defense program through the application of machine learning techniques""",https://www.reddit.com/r/MachineLearning/comments/49o90l/nasa_mavericks_lab_applied_research_accelerator/,TroyHernandez,1457533386,,0,0
279,2016-3-10,2016,3,10,0,49oevx,Recommend Books for Text Mining,https://www.reddit.com/r/MachineLearning/comments/49oevx/recommend_books_for_text_mining/,YourTechGuy,1457535634,"I'm looking for a good textbook that will get me up to speed in text mining. I already have a background in data mining, and I'm very comfortable with mathematical texts. 

Any recommendations on a book/books that will give me an overview in terms of what's possible, current challenges, and popular algorithms/approaches?",1,0
280,2016-3-10,2016,3,10,0,49ok3o,[Question] What the dendrogram plot of hierarchical clustering tells me for my data?,https://www.reddit.com/r/MachineLearning/comments/49ok3o/question_what_the_dendrogram_plot_of_hierarchical/,[deleted],1457537553,[deleted],0,0
281,2016-3-10,2016,3,10,0,49ol3x,"Tool: 31 Resources to Learn AI &amp; Deep Learning, From Beginner To Advanced",https://www.reddit.com/r/MachineLearning/comments/49ol3x/tool_31_resources_to_learn_ai_deep_learning_from/,seanmeverett,1457537932,,0,1
282,2016-3-10,2016,3,10,1,49osnf,Home / Google / RankBrain: A Study to Measure Its Impact RankBrain: A Study to Measure Its Impact 3/9/2016 by Eric Enge Leave a Comment Tweet Share +1 Share Total Shares 0 RankBrain Hero ImageStudy: Does RankBrain Actually Improve Search Results?,https://www.reddit.com/r/MachineLearning/comments/49osnf/home_google_rankbrain_a_study_to_measure_its/,[deleted],1457540603,[deleted],0,0
283,2016-3-10,2016,3,10,1,49ounb,Study: Does RankBrain Actually Improve Search Results?,https://www.reddit.com/r/MachineLearning/comments/49ounb/study_does_rankbrain_actually_improve_search/,andyakesson,1457541289,,0,0
284,2016-3-10,2016,3,10,1,49oush,Deep Learning for Robots: Learning from Large-Scale Interaction,https://www.reddit.com/r/MachineLearning/comments/49oush/deep_learning_for_robots_learning_from_largescale/,clbam8,1457541335,,4,14
285,2016-3-10,2016,3,10,1,49ow5j,Detecting Emotion in Faces Using Geometric Features,https://www.reddit.com/r/MachineLearning/comments/49ow5j/detecting_emotion_in_faces_using_geometric/,carlos_argueta,1457541788,,0,7
286,2016-3-10,2016,3,10,1,49owpz,alphago,https://www.reddit.com/r/MachineLearning/comments/49owpz/alphago/,jbark55,1457541974,What is the most detailed description of the alphago program out there?  Source code would be great but I am 100% sure that will never happen.,4,0
287,2016-3-10,2016,3,10,2,49p6fo,Announcing R tools for Visual Studio,https://www.reddit.com/r/MachineLearning/comments/49p6fo/announcing_r_tools_for_visual_studio/,smortaz,1457545453,,4,16
288,2016-3-10,2016,3,10,4,49pj8a,Could ML be trained to distinguish dyslexic writers from careful writers?,https://www.reddit.com/r/MachineLearning/comments/49pj8a/could_ml_be_trained_to_distinguish_dyslexic/,[deleted],1457550074,[deleted],0,1
289,2016-3-10,2016,3,10,4,49poxd,Could ML be trained to distinguish dyslexic writers from uncareful writers?,https://www.reddit.com/r/MachineLearning/comments/49poxd/could_ml_be_trained_to_distinguish_dyslexic/,Lo0t,1457552118,"Computer science student here.

I understand that dyslexia is a serious handicap, and I am considerate and try helping out dyslexic people, when they need a second pair of eyes.

At the same time I am annoyed by people who don't seem to be putting a lot of effort in to checking their grammar when writing group projects. More than a few errors a spell checker would have caught is a good indicator. However, spell checkers are not very advanced in my native language, which is notorious for its bad orthography and homophones.

Could Machine Learning be trained to distinguish between the two? And do you think it would be worth a bachelor's thesis?",4,3
290,2016-3-10,2016,3,10,5,49pwt3,About hand writing recognition - single letter vs full sentence,https://www.reddit.com/r/MachineLearning/comments/49pwt3/about_hand_writing_recognition_single_letter_vs/,hadf40,1457554575,"Hello, I'm newbie in machine learning. As far as I can understand how Deep Learning works, a neuronal network can only give a stochastic output, I mean a probability for each value represented by the neurons of the last abstract layer.

The example commonly used is hand writing recognition. A deep learning network can compute an image representing a letter by giving the matching letter.

But what if the image represents a sentence ?

Is it necessary to process first the sentence to separate letters, or is the tokenization processed by the deep learning network as the first layer of the network ? And if so, how to interpret the stochastic output, considering that the expected result is not a single letter but a full sentence ?",1,1
291,2016-3-10,2016,3,10,5,49pzsm,"My tool for classification using various methods, including deep learning. Very useful for baseline results",https://www.reddit.com/r/MachineLearning/comments/49pzsm/my_tool_for_classification_using_various_methods/,aulloa,1457555523,,3,0
292,2016-3-10,2016,3,10,5,49q0fx,Will AI Surpass Human Intelligence? Interview with Prof. Jrgen Schmidhuber on Deep Learning Neural Networks and AlphaGo,https://www.reddit.com/r/MachineLearning/comments/49q0fx/will_ai_surpass_human_intelligence_interview_with/,collinhsu,1457555739,,13,43
293,2016-3-10,2016,3,10,5,49q1og,Train your own image classifier with Inception in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/49q1og/train_your_own_image_classifier_with_inception_in/,doomie,1457556129,,10,12
294,2016-3-10,2016,3,10,5,49q1ur,Getting Words from Thoughts (Skipthoughts that is),https://www.reddit.com/r/MachineLearning/comments/49q1ur/getting_words_from_thoughts_skipthoughts_that_is/,rettis,1457556188,"Has anyone tried using a neural decoder or similar to reproduce the original sentence represented by a [skipthought](http://arxiv.org/abs/1506.06726) vector? Or, how about reproducing the original sentence from any of the other representations discussed in [Hill et al., 2016](http://arxiv.org/pdf/1602.03483v1.pdf)?",0,1
295,2016-3-10,2016,3,10,5,49q2oe,"Starting ML, NLP with a task of extracting ""understanding"" from a Human Chat Database?",https://www.reddit.com/r/MachineLearning/comments/49q2oe/starting_ml_nlp_with_a_task_of_extracting/,UniversalVoid,1457556447,"Diving in, but i'm a bit stuck on where I should focus. 

Generally speaking I want to normalize to standard Questions and Standard Answers generated from 2 years worth of text chat sessions. 3 people might ask the same question 3 different ways but at the core it's the same question. Likewise with the answers. End result being a normalized list of questions and answers.

Is there a high level solution or does it need to be all at low level?

At Low level, procedurally i'm thinking this, and i'm relatively certain i'm thinking wrong:

-Normalize the Language: correct spelling and eliminate as much bad data as I can.
-Eliminate extraneous words.
-Translate each word into the most common word with the same meaning. (Wordnet)

At that point I most likely have close to what i'm looking for, and this is where my lack of current understanding fails me. 

Any help would be greatly appreciated.

Thanks!",6,0
296,2016-3-10,2016,3,10,5,49q3p9,R on Travis-CI,https://www.reddit.com/r/MachineLearning/comments/49q3p9/r_on_travisci/,svdalpha,1457556783,,0,0
297,2016-3-10,2016,3,10,6,49q5h8,A journey to the Deep Dream,https://www.reddit.com/r/MachineLearning/comments/49q5h8/a_journey_to_the_deep_dream/,anatoliykmetyuk,1457557374,,0,7
298,2016-3-10,2016,3,10,6,49qa4f,Wikipedia classifier,https://www.reddit.com/r/MachineLearning/comments/49qa4f/wikipedia_classifier/,wowholdonwhat,1457559019,"Hi all, 
I just wanted to share a simple project that I did recently. Its a python wikipedia scrapper (takes in any wikipedia category and scraps all the text in the articles belonging to each category and subcategory recursively, using beautifulsoup), followed by a logistic regression classifier. It gets an F1 score of around 0.83 (with categories pretty similar to each others, e.g. rare diseases and congenital diseases), so def room for improvement, open to any suggestion! I will run it with neural networks soon. Here is the github link: https://github.com/LouisFoucard/wikipedia_classifier",5,1
299,2016-3-10,2016,3,10,7,49qk1u,Fraud detection,https://www.reddit.com/r/MachineLearning/comments/49qk1u/fraud_detection/,longestPath,1457562713,"Does anyone have any good tutorials, videos, books, talks, etc. on fraud detection (i.e. ""Someone is buying something at your online store, how do you detect they are a fraudster."")? What algorithms are used, what are the best practices etc? 

A binary classifier built from historical data?",5,4
300,2016-3-10,2016,3,10,8,49qrvc,How do train a network that is unstable with different train/test splits?,https://www.reddit.com/r/MachineLearning/comments/49qrvc/how_do_train_a_network_that_is_unstable_with/,slack4ever,1457565677,"Hi all,
Sorry for another noob deep learning question. I created a network that has 7 layers. I repeated the training 20 times with different train/test splits. I plotted the error vs epoch and saw that my network converges 70% of the 20 runs, but also diverges in about 30% . How should one deal with this situation?
Thanks.",3,0
301,2016-3-10,2016,3,10,10,49raiz,Machine Learning algorithm that can determine if a name is similar to other names?,https://www.reddit.com/r/MachineLearning/comments/49raiz/machine_learning_algorithm_that_can_determine_if/,OkSt00pid,1457573385,"For instance, say I have training data sets of a couple hundred Italian last names, and a set of a couple hundred Irish last names. Is there a way to tell if a name like ""O'Reilly"" is Irish, or if a name like ""Spinelli"" is Italian, even if they aren't in the training data sets.. ?

Also, would it be possible to do the inverse of this, and have an algorithm generate a new name that 'sounds' Italian or Irish?

Hopefully this isn't a simplistic question to be asking, but I'm pretty new to ML / NLP concepts...
",2,0
302,2016-3-10,2016,3,10,10,49rbnn,Has anyone here interviewed with DeepMind? What was your experience like?,https://www.reddit.com/r/MachineLearning/comments/49rbnn/has_anyone_here_interviewed_with_deepmind_what/,brouwjon,1457573853,,13,25
303,2016-3-10,2016,3,10,11,49rl3p,Adversary examples - do we actually care?,https://www.reddit.com/r/MachineLearning/comments/49rl3p/adversary_examples_do_we_actually_care/,[deleted],1457577936,[deleted],12,0
304,2016-3-10,2016,3,10,12,49rraz,[LIVE] Match 2 - Google DeepMind Challenge Match: Lee Sedol vs AlphaGo,https://www.reddit.com/r/MachineLearning/comments/49rraz/live_match_2_google_deepmind_challenge_match_lee/,pedromnasc,1457580758,,49,53
305,2016-3-10,2016,3,10,14,49s7z4,Physics major interested in the field.,https://www.reddit.com/r/MachineLearning/comments/49s7z4/physics_major_interested_in_the_field/,SpydaX10,1457589125,"I'm working on a physics degree at the moment, but have recently come to realize that I'm extremely interested in machine learning, probably even more so than physics. This is mainly due to realizing that solving learning is the most important goal for humans right now as far as furthering our understanding of the universe goes. Just wondering if any people in the field can shed some light on the parallels between the two? If I wanted to get into this kind of work would my degree be basically useless? I'd really rather not switch majors at this point but I don't have much of an endgame for my degree besides teaching and possibly research if I can make it through grad school. I have taken a few compsci classes but I'm really not much of a programmer. Any advice is appreciated, thanks.",8,5
306,2016-3-10,2016,3,10,16,49sg16,An idea for a General Algorithm for any Intelligent Task,https://www.reddit.com/r/MachineLearning/comments/49sg16/an_idea_for_a_general_algorithm_for_any/,[deleted],1457593700,[deleted],0,1
307,2016-3-10,2016,3,10,17,49snc2,Alpha Go wins match 2,https://www.reddit.com/r/MachineLearning/comments/49snc2/alpha_go_wins_match_2/,glassmountain,1457598477,,343,493
308,2016-3-10,2016,3,10,17,49snfa,AlphaGO WINS! (again),https://www.reddit.com/r/MachineLearning/comments/49snfa/alphago_wins_again/,zhewriix,1457598535,I feel so happy. (again),16,30
309,2016-3-10,2016,3,10,17,49sogf,AlphaGo Replication Code,https://www.reddit.com/r/MachineLearning/comments/49sogf/alphago_replication_code/,dpmms,1457599293,,2,0
310,2016-3-10,2016,3,10,17,49sogo,Alphago wins again!!!,https://www.reddit.com/r/MachineLearning/comments/49sogo/alphago_wins_again/,doodledood1,1457599300,"Interesting times.
This game was much closer than the first one. But, in time pressure, the machine won. Makes one wonder...",0,0
311,2016-3-10,2016,3,10,17,49sota,Viterbi Algorithm: How to find optimal transition probabilities when data set distibution is skewed,https://www.reddit.com/r/MachineLearning/comments/49sota/viterbi_algorithm_how_to_find_optimal_transition/,astridjt,1457599544,"I am creating a classification system that can recognize what activities (e.g. sitting, standing, walking, laying) a person is doing at a given time based on accelerometer signals. I get my emission probabilities from a convolutional neural network that is trained on a data set with human activities, but I have some trouble with generating my transition probabilities. My first solution was to generate my trasition probabilities by looping over my training data and count how many changes there were between the different activities. The problem here was that the probability for continue to do an activity (e.g. walking --&gt; walking) was way to high, and the Viterbi algorithm ended up ""smoothing"" the signal to much. Do you have any thoughs about how I can change the transition probabilities? Thanks",4,6
312,2016-3-10,2016,3,10,18,49sqd4,A Framework for Artificial General Intelligence using Simulated training,https://www.reddit.com/r/MachineLearning/comments/49sqd4/a_framework_for_artificial_general_intelligence/,[deleted],1457600690,[deleted],0,0
313,2016-3-10,2016,3,10,18,49ss52,Does a compiled list of practical tips for convolutional networks exist?,https://www.reddit.com/r/MachineLearning/comments/49ss52/does_a_compiled_list_of_practical_tips_for/,I_need_results_fast,1457601964,[removed],1,1
314,2016-3-10,2016,3,10,18,49stvh,Most ppl smash their phone and remove the AlphaGo app at the first day..,https://www.reddit.com/r/MachineLearning/comments/49stvh/most_ppl_smash_their_phone_and_remove_the_alphago/,pringles479,1457603235,,0,0
315,2016-3-10,2016,3,10,19,49suzy,Missing value imputation with nearest neighbour,https://www.reddit.com/r/MachineLearning/comments/49suzy/missing_value_imputation_with_nearest_neighbour/,BlackHawk90,1457604051,"Hello

I'm using k-nearest neighbour imputation method for missing values. This method has two tuning parameters: k and the distance metric. I see two options for applying this imputation method:

1. Inside nested cross-validation. Applying the imputation method separately to the training and testing set. If I'm using leave-one-out cross-validation this will not work because my test set only contains one data point. Also if the test set is small (less data points), this is critical.

2. Outside cross-validation by just imputing over the whole dataset.

Which is the right way to go?",4,1
316,2016-3-10,2016,3,10,19,49svsr,Mdrnn for torch?,https://www.reddit.com/r/MachineLearning/comments/49svsr/mdrnn_for_torch/,djc1000,1457604616,Can someone point me toward a torch implementation of an mdrnn? Thanks.,5,1
317,2016-3-10,2016,3,10,19,49sxsv,"With AlphaGo in the news this week, we looked back at our interview with scientists from Google DeepMind - talking advancements in deep learning, DeepMind algorithms, and using games to teach machines.",https://www.reddit.com/r/MachineLearning/comments/49sxsv/with_alphago_in_the_news_this_week_we_looked_back/,reworksophie,1457606043,,0,1
318,2016-3-10,2016,3,10,20,49t538,"""Use it or lose it"". What is your probability estimate for general purpose AI making the species dumber and less creative?",https://www.reddit.com/r/MachineLearning/comments/49t538/use_it_or_lose_it_what_is_your_probability/,SoulDrivenOlives,1457610985,,2,0
319,2016-3-10,2016,3,10,21,49t6kw,I'm organizing information and resources on deep learning on this web site. Suggestions are welcome!,https://www.reddit.com/r/MachineLearning/comments/49t6kw/im_organizing_information_and_resources_on_deep/,steven2358,1457611937,,0,0
320,2016-3-10,2016,3,10,21,49t8ka,Idea of project for getting experience in ML,https://www.reddit.com/r/MachineLearning/comments/49t8ka/idea_of_project_for_getting_experience_in_ml/,spiderpacman,1457613106,"Hello wonderful ML community!

I am a (particle) physicist who decided to completely change fields and I got interested in Machine Learning. I already completed some online courses (Coursera, etc) in ML and statistics but to improve myself I would like to work on a project in order to get practical experience!
Do you have any suggestions on a subject? Or even better I can help you on some project you never had time to work on but are curious about the results. I can even (I guess) supervise myself.
Thank you for your interest!",3,3
321,2016-3-10,2016,3,10,22,49tg9p,Rotation invariance in CNN (no pooling)?,https://www.reddit.com/r/MachineLearning/comments/49tg9p/rotation_invariance_in_cnn_no_pooling/,hapliniste,1457617191,"Hi, I'm learning ML and people talk about pooling to do position and rotation invariance. I understand how it achieve positional invariance but not rotational invariance (feel free to explain it to me).

I will now explain how I think we could achieve rotational invariance, hoping that you can tell me if it's feasible.

For the next part I will assume we're working with binary features, each pixels being black or white for simplification.

The idea is that a Conv layer with kernel size of 2x2 and stride of 1 can output 16 different features without any information loss:

[[0,0],[0,0]]

[[0,0],[0,1]]

[[0,0],[1,0]]

[[0,0],[1,1]]

...

[[1,1],[1,1]]

Each ""pixel"" now store the information of himself+his neighbors in the Ksize (we could apply pooling to reduce the amount of information, so that it is the same as before the conv).

but three fourth of these features are the same, just rotationed ([[0,0],[1,1]] and [[1,1],[0,0]] for example, are the same feature with a rotation of 180).

The idea would be to create a layer that takes these 16 features and output only 4 (suppressing the rotation on the features).
suppose we feed this layer [[0,0],[0,1]], [[0,0],[1,0]], it would only output [[0,0],[0,1]].

the 4 features would always be:

[[0,0],[0,1]], 

[[0,0],[1,1]],

[[0,1],[1,1]],

[[1,1],[1,1]]

Is this possible or am I retarded?

If you have ideas on how to implement that, I would gladly read your thoughts on it!

Thank you for reading!

Note: In the examples here, I didn't add the features that are similar. The real implementation would be different.",7,7
322,2016-3-10,2016,3,10,23,49tksh,Imbalanced data  Finding Waldo,https://www.reddit.com/r/MachineLearning/comments/49tksh/imbalanced_data_finding_waldo/,sarkarsh,1457619260,,0,1
323,2016-3-10,2016,3,10,23,49tn5z,Has anyone had trouble installing Caffe with GPU support?,https://www.reddit.com/r/MachineLearning/comments/49tn5z/has_anyone_had_trouble_installing_caffe_with_gpu/,[deleted],1457620256,[deleted],0,0
324,2016-3-11,2016,3,11,0,49tu1g,Making it easier to build text classifiers on MonkeyLearn,https://www.reddit.com/r/MachineLearning/comments/49tu1g/making_it_easier_to_build_text_classifiers_on/,pipinstallme,1457623084,,0,0
325,2016-3-11,2016,3,11,1,49u25z,The State of the AI market in 2015: A focus on exits,https://www.reddit.com/r/MachineLearning/comments/49u25z/the_state_of_the_ai_market_in_2015_a_focus_on/,nb410,1457626135,,0,2
326,2016-3-11,2016,3,11,1,49u79r,Exploring Artificial Intelligence in Museums,https://www.reddit.com/r/MachineLearning/comments/49u79r/exploring_artificial_intelligence_in_museums/,Francis_Beacon,1457627913,,0,1
327,2016-3-11,2016,3,11,2,49ubqk,End-to-end full document OCR references?,https://www.reddit.com/r/MachineLearning/comments/49ubqk/endtoend_full_document_ocr_references/,spurious_recollectio,1457629431,"I've seen few papers on end-to-end NNs for e.g. text in natural scenes but I was wondering if there is any work on end-to-end full document image-to-text conversion.  In a natural scene the issue is more finding the text whereas in a e.g. scanned image of a text document the question is more one of ordering...how to recombine all the text and regions into one consistent piece of text.  If anyone has any references or thoughts I'd appreciate it (even partially related work would be interesting).
",1,0
328,2016-3-11,2016,3,11,2,49uccc,How well would AlphaGo do without Monte Carlo tree search?,https://www.reddit.com/r/MachineLearning/comments/49uccc/how_well_would_alphago_do_without_monte_carlo/,[deleted],1457629630,[deleted],1,0
329,2016-3-11,2016,3,11,2,49ueir,Is it feasible to create a machine learning model to predict if a paper should be included or excluded in a systematic review?,https://www.reddit.com/r/MachineLearning/comments/49ueir/is_it_feasible_to_create_a_machine_learning_model/,zaksheikh,1457630346,"
A systematic review involves examining a large corpus of academic papers. There are several stages to it, firstly screening the papers to determine if the study meets a strict protocol (eg. year published, outcomes).

I wish to use a machine learning algorithm to automate this process. I would submit 200 hundred labeled titles (included, excluded, not sure) and the program would need to decide which category it falls in.

I believe a multi-variate text classification algorithms could be used. Is this project feasible, it would need to result in an accuracy rate of above 90%. And, if so what algorithms/approaches could be used? Additionally, which firm would specalise in devoloping such a program and what would it, very roughly, cost?",5,1
330,2016-3-11,2016,3,11,2,49uibf,Starting with CNNs... Example projects? Online courses?,https://www.reddit.com/r/MachineLearning/comments/49uibf/starting_with_cnns_example_projects_online_courses/,billybasskickinass,1457631637,"What would be a good way to get started with CNNs.  They're one of those things on my landscape that I haven't really paid all that much attention to.

I am familiar with ""old school"" neural networks... I wrote my own in 2005 (although the damn thing never converged).  Anyone have any tips on where to start?",2,0
331,2016-3-11,2016,3,11,3,49unwk,What Donald Trump had to say about AlphaGo,https://www.reddit.com/r/MachineLearning/comments/49unwk/what_donald_trump_had_to_say_about_alphago/,[deleted],1457633620,[deleted],0,0
332,2016-3-11,2016,3,11,4,49v19k,Classification on String,https://www.reddit.com/r/MachineLearning/comments/49v19k/classification_on_string/,Coupdbluff,1457638368,[removed],0,1
333,2016-3-11,2016,3,11,4,49v1wi,"Fascinated by alphago, want to set my career direction to machine learning.",https://www.reddit.com/r/MachineLearning/comments/49v1wi/fascinated_by_alphago_want_to_set_my_career/,newbornlife,1457638578,"Appreciate for reading my post guys,

So I am currently in my last year of undergrad majoring in 

economics and minor in statistics. I am going to do a

master of applied statistics (biostatistics) for the next 2 years.

Will this be enough background to study machine learning after 

my master's degree? I am already capable of R and SAS, but 

I will be trying to learn necessary programming skills for ML during my 

master. I have sort of no background at all in computer science 

or any other languages other than statistical ones, so it is kinda

too far away from the normal path to pursue expertise in 

machine learning? I would like to hear any advice from you guys.

cheers 
",8,3
334,2016-3-11,2016,3,11,4,49v22f,Story and Lessons Behind the Evolution of XGBoost,https://www.reddit.com/r/MachineLearning/comments/49v22f/story_and_lessons_behind_the_evolution_of_xgboost/,antinucleon,1457638634,,1,19
335,2016-3-11,2016,3,11,4,49v4cv,An Introduction to XGBoost R package,https://www.reddit.com/r/MachineLearning/comments/49v4cv/an_introduction_to_xgboost_r_package/,alxndrkalinin,1457639385,,0,8
336,2016-3-11,2016,3,11,5,49vfyz,"Partisanship, Nerd-love, and Race",https://www.reddit.com/r/MachineLearning/comments/49vfyz/partisanship_nerdlove_and_race/,cast42,1457643467,,3,0
337,2016-3-11,2016,3,11,6,49vgmd,There's lots of people newly interested in neural networks and AI thanks to the AlphaGo news. Would you guys be willing to set up a thread for layman questions with detailed answers?,https://www.reddit.com/r/MachineLearning/comments/49vgmd/theres_lots_of_people_newly_interested_in_neural/,[deleted],1457643694,[deleted],0,1
338,2016-3-11,2016,3,11,6,49vgz6,Questions regarding Multiple Instance Learning,https://www.reddit.com/r/MachineLearning/comments/49vgz6/questions_regarding_multiple_instance_learning/,thorwing,1457643817,"Hey guys,

For a course I'm doing we need to experiment with different ways to  label multiple images to multiple labels. Since the amount of labels is considerably low and the images are grouped. We decided we wanted to try out multiple MIL systems.

I've read several papers on the subject that all point out to the general known databases Tiger, Elephant, Fox, Musk 1 and Musk 2 and found them in an .arff format to test them out in Weka. However, I can't for the life of me find out where the features come from. It seems that the images are feature extracted somehow to 230 different features. Since we'd like to proceed with roughly the same steps, my question is: Does anyone know what was done to extract said 230 features?

Here is an example [.arff file (textbased online)](http://www.cs.waikato.ac.nz/~eibe/multi_instance/tiger_relational.arff)",0,0
339,2016-3-11,2016,3,11,6,49vjr8,DeepMind founder Demis Hassabis on how AI will shape the future,https://www.reddit.com/r/MachineLearning/comments/49vjr8/deepmind_founder_demis_hassabis_on_how_ai_will/,SubTrk,1457644787,,4,12
340,2016-3-11,2016,3,11,7,49vv60,Here is a nice compilation of over 50 questions typically asked in ML interviews,https://www.reddit.com/r/MachineLearning/comments/49vv60/here_is_a_nice_compilation_of_over_50_questions/,datameer,1457648963,,18,75
341,2016-3-11,2016,3,11,7,49vvx1,Help building a recommender on this data set,https://www.reddit.com/r/MachineLearning/comments/49vvx1/help_building_a_recommender_on_this_data_set/,maxmoo,1457649234,"I've been trying to build a recommender on a [dataset that I scraped (recipe likes from cooking.nytimes.com)][1], the baseline popularity is around 0.42 precision @ 5, and I can get around 0.47 with item-based collaborative filtering. Any suggestions for improving on this? Or is my dataset too small/sparse? I haven't had much experience with these type of recommender system.

[1]:https://s3-ap-southeast-2.amazonaws.com/themrmax-public/likes_full.zip",2,0
342,2016-3-11,2016,3,11,8,49w48x,How to learn about AlphaGo,https://www.reddit.com/r/MachineLearning/comments/49w48x/how_to_learn_about_alphago/,kirakun,1457652503,"Now that AlphaGo is making much big nose, can someone recommend a good list of reading that would lead to understanding how AlphaGo works.

Ideally, the list can start with newbie material then progressively leading to more advanced material in deep learning. More advanced readers can start somewhere in the middle of the list.",7,2
343,2016-3-11,2016,3,11,8,49w57l,Machine learning webapp,https://www.reddit.com/r/MachineLearning/comments/49w57l/machine_learning_webapp/,azraelxii,1457652872,"I was wondering the status easiest to way to deploy machine learning algorithms In a webapp (and maybe some direction to a tutorial). Ideally I would like the end user to drop a CSV and the algorithm gives out a prediction. I know azure and aws have services like this but I am unsure  how to do this.

Thanks,

Az

Edit:a word",2,3
344,2016-3-11,2016,3,11,8,49w6ar,I just found out that the Jetson TX1 is pretty cheap for its performance. With Linux and Maxwell GPU (integrated cuDNN support),https://www.reddit.com/r/MachineLearning/comments/49w6ar/i_just_found_out_that_the_jetson_tx1_is_pretty/,[deleted],1457653313,[deleted],11,5
345,2016-3-11,2016,3,11,8,49w6oj,"[Case Study/Tutorial]Advanced Document Classification using Python (Sklearn, NLTK)",https://www.reddit.com/r/MachineLearning/comments/49w6oj/case_studytutorialadvanced_document/,lawrencejones617,1457653499,,0,3
346,2016-3-11,2016,3,11,10,49wmg8,My thoughts on AlphaGo,https://www.reddit.com/r/MachineLearning/comments/49wmg8/my_thoughts_on_alphago/,throwawayprogrammer9,1457660419,,0,3
347,2016-3-11,2016,3,11,11,49wr46,ELI5: Bandits. What are they and what are they good at?,https://www.reddit.com/r/MachineLearning/comments/49wr46/eli5_bandits_what_are_they_and_what_are_they_good/,__andrei__,1457662464,,2,1
348,2016-3-11,2016,3,11,11,49wrt4,Adversarial images for deep learning,https://www.reddit.com/r/MachineLearning/comments/49wrt4/adversarial_images_for_deep_learning/,thecity2,1457662786,,27,463
349,2016-3-11,2016,3,11,12,49wtg0,AlphaGo Board CNN,https://www.reddit.com/r/MachineLearning/comments/49wtg0/alphago_board_cnn/,teling,1457665538,"In the nature paper, they mention that there's a policy network for creating a probability map of the board and a value network for evaluating positions from a range of -1 (100% will lose) to 1 (100% will win).

Both networks are described as being 12 layer CNNs with alternating convolutional layers then rectifier layers. I'm still impressed they can run these in 3 ms. I guess since the input size of 19 x 19 is small then the the (# filters) x 19 x 19 isn't that large. I'm also amazed that the value network can approximate a good value given how nonlinear and noncontinuous the board input can be. There are so many patterns of different shapes and sizes to recognize it's really amazing that it can realize that sections that are far from each other influence each other

Hassabis was quoted saying that alphago knew it was winning about halfway through (~100 stones in). It could pattern match all the empty space and still realize there were advantageous/winning configurations

any thoughts?",0,2
350,2016-3-11,2016,3,11,13,49x4aw,How to decide how many samples needed for each epoch in training neural networks?,https://www.reddit.com/r/MachineLearning/comments/49x4aw/how_to_decide_how_many_samples_needed_for_each/,sunshineatnoon,1457670307,"I am using Keras to train a neural network, in the training process, I need to decide how many samples my model should see during an epoch. But I found deciding this hyper parameter is tricky. With too many samples per epoch, the training takes too much time and potentially easy to diverge. However, with too few samples per epoch, the model can't train ""completely"" on each epoch so the model can't converge to optimal values.
I would like to ask how you guys tune on this hyper parameter? THX!

Here is the definition of samples_per_epoch in keras, **it's not the same with batch size**:

    samples_per_epoch: integer, number of samples to process before starting a new epoch.

BTW, I am using Adam to update the weights and I always observe a huge decrease at the beginning of each epoch, which I haven't figured out why yet!",6,1
351,2016-3-11,2016,3,11,13,49x70s,How to get started with deep learning using Jupyter notebooks on AWS (guide for beginners),https://www.reddit.com/r/MachineLearning/comments/49x70s/how_to_get_started_with_deep_learning_using/,efavdb,1457671624,,0,3
352,2016-3-11,2016,3,11,14,49xflw,Simple derivation of the softmax formula,https://www.reddit.com/r/MachineLearning/comments/49xflw/simple_derivation_of_the_softmax_formula/,Jxieeducation,1457675908,,1,0
353,2016-3-11,2016,3,11,15,49xfwk,Keras Implementation of AlphaGo,https://www.reddit.com/r/MachineLearning/comments/49xfwk/keras_implementation_of_alphago/,malleus17,1457676054,,7,11
354,2016-3-11,2016,3,11,17,49xv97,AlphaGo - Google's resources,https://www.reddit.com/r/MachineLearning/comments/49xv97/alphago_googles_resources/,bbsome,1457684077,"I'm quite curious myself how much resources have Google leased to Deep Mind for the matches of AlphaGo? They might not release this, but what do you guys think. From the paper they had 1000 CPU and 200 GPUs, however I think that for this game the are really tapping to a lot more of Google's cloud servers (probably a few clusters I guess) which in fact can greatly help with the Monte Carlo Tree Search (given that they do it all the time for 4 hours). Maybe it was going to be more fair with a computational budged (fxed hardware) rather than allowing for this (or maybe they have that, but I don't know?). ",9,4
355,2016-3-11,2016,3,11,17,49xyut,Docker &amp; TensorFlow complete setup guide for Windows,https://www.reddit.com/r/MachineLearning/comments/49xyut/docker_tensorflow_complete_setup_guide_for_windows/,snakenaf,1457686307,,2,0
356,2016-3-11,2016,3,11,17,49xzn7,How many times should I iterate over my data when training CNNs?,https://www.reddit.com/r/MachineLearning/comments/49xzn7/how_many_times_should_i_iterate_over_my_data_when/,xristos_forokolomvos,1457686787,"I have half a million labeled images and am trying to build a deep CNN architecture for classification. How many times should I feed each image to the network?

For example, with a batch size of 128 (is that too big?) and 8000 steps I will iterate approximately twice. Is that approach correct?
",14,1
357,2016-3-11,2016,3,11,18,49y16s,Comparison between Neural Networks and Decision Trees,https://www.reddit.com/r/MachineLearning/comments/49y16s/comparison_between_neural_networks_and_decision/,Sergiointelnics,1457687684,,2,0
358,2016-3-11,2016,3,11,18,49y4ti,[question] Is my trick valid?,https://www.reddit.com/r/MachineLearning/comments/49y4ti/question_is_my_trick_valid/,godspeed_china,1457689814,"I have collected 100 disease samples and 100 control samples with 100 genetic variables. I ran random forest to predict the disease outcome according to genetic variables.
I got 90% sensitivity and 90% specificity. However, I know that the disease's prevalence is only 0.5%, thus 90% specificity is not enough for general population testing as there will be many false positive diagnosis. 
I did the following trick: I copy the control sample data and paste it 10 times to form a modified trainning data mimicing the real prevalence. Then I ran random forest on it and I got 85% sensitivity and 100% specificity. That looks useful. 
The question is that is this trick valid/acceptable? 
Thanks!",6,0
359,2016-3-11,2016,3,11,19,49y68f,amazon ec2 gpu instance spot prices,https://www.reddit.com/r/MachineLearning/comments/49y68f/amazon_ec2_gpu_instance_spot_prices/,regularized,1457690653,"Last time I checked, the spot instance price for 1 gpu was 0.07$/hr. Now they are 0.30$/hr. When did the prices go up? Was there a specific reason? ",2,2
360,2016-3-11,2016,3,11,21,49yjry,[1603.03116] Low-rank passthrough neural networks,https://www.reddit.com/r/MachineLearning/comments/49yjry/160303116_lowrank_passthrough_neural_networks/,AnvaMiba,1457698761,"[ARXIV LINK](http://arxiv.org/abs/1603.03116)

Author here, I hope that linking one's own paper is not against the subreddit etiquette.

In this work I review various related ""LSTM-like"" neural network architectures: LSTMs, GRUs, Highway Networks, Residual Networks, and introduce a class of low-dimensional parametrizations for the square matrices in the hidden layers in order to reduce the data complexity of the models without reducing memory capacity.

I replace each ""recurrence"" n-by-n parameter matrix acting on the state vector with the product of a pair of rectangular parameter matrices with smaller inner dimension d, resulting in a square matrix with maximum rank d. Optionally, I also add a n-by-n diagonal matrix, resulting in a low-rank plus diagonal parametrization.

Low-dimensional parametrizations of neural network matrices have been proposed in the literature, e.g. the Adaptive Fastfood, the ACDC and the uRNN, but in my opinion, this is simpler and, for small d, more efficient, since it avoids the computation of Fourier or Hadamard transforms.

I did some experiments with the low-rank and low-rank plus diagonal parametrizations on Highway Networks and GRUs. In particular I reproduced the experiments of the [uRNN paper](http://arxiv.org/abs/1511.06464) by Arjovsky et al. . I obtained comparable or better results, including a slightly better accuracy on the randomly-permuted sequential MNIST task.

I also considered two variants of the memory task which are hard for the uRNN (one introduced in the [Associative LSTM paper](http://arxiv.org/abs/1602.03032) by Danihelka et. al and the other introduced in a [paper](http://arxiv.org/abs/1602.06662) by Henaff et al.) and I obtain good results, comparable to the Associative LSTM.

Code for the [Low-rank Highway Network](https://github.com/Avmb/lowrank-highwaynetwork).  
Code for the [Low-rank GRU](https://github.com/Avmb/lowrank-gru).

Comments, suggestions or criticism are welcome!
",15,44
361,2016-3-11,2016,3,11,21,49yovd,PowerPlay: training an increasingly general problem solver by continually searching for the simplest still unsolvable problem,https://www.reddit.com/r/MachineLearning/comments/49yovd/powerplay_training_an_increasingly_general/,OptimalProblemSolver,1457701193,,8,9
362,2016-3-11,2016,3,11,22,49yufg,AI tries to play Super Mario World live!,https://www.reddit.com/r/MachineLearning/comments/49yufg/ai_tries_to_play_super_mario_world_live/,AI_on_Twitch,1457703917,,16,12
363,2016-3-11,2016,3,11,23,49yxx1,Value Iteration Networks,https://www.reddit.com/r/MachineLearning/comments/49yxx1/value_iteration_networks/,pierrelux,1457705568,,3,14
364,2016-3-11,2016,3,11,23,49z3th,VC Ted Schlein Sees More Cybersecurity Firms Embrace Machine Learning,https://www.reddit.com/r/MachineLearning/comments/49z3th/vc_ted_schlein_sees_more_cybersecurity_firms/,KarterrvvAleaha1_,1457708098,,0,1
365,2016-3-12,2016,3,12,0,49z6qw,Interested in understanding AlphaGo better,https://www.reddit.com/r/MachineLearning/comments/49z6qw/interested_in_understanding_alphago_better/,akcom,1457709321,"Hi all,

I'm familiar with supervised learning methods and I'm comfortable with the concepts around convulsion networks etc.  That being said, reading the AlphaGo paper in Nature is like reading Greek.  I think part of this is due to my lack of knowledge in reinforcement learning (the ""policy network"" idea is totally foreign to me).

I'd like to get some suggestions on reading material so that I can understand how AlphaGo works.  I've started reading Sutton &amp; Barto's Reinforcement Learning: An Introduction.  But if there any other good texts, I'm all ears!

Thanks.",10,1
366,2016-3-12,2016,3,12,0,49z9ww,OPENCV installation while using Paython/caffe/anaconda on Ubuntu tutorial?,https://www.reddit.com/r/MachineLearning/comments/49z9ww/opencv_installation_while_using/,Alirezag,1457710605,"Hi  guys,

I am using Ubuntu 14.04 and using caffe and python as well as anaconda.
I am having a though time to install opencv and make it work via python and caffe.
Please let me know if you have a good tutorial on how to do it.
Thanks :(",4,3
367,2016-3-12,2016,3,12,0,49zaa4,Deploy a trained TensorFlow model with numpy,https://www.reddit.com/r/MachineLearning/comments/49zaa4/deploy_a_trained_tensorflow_model_with_numpy/,aloisg,1457710740,,5,18
368,2016-3-12,2016,3,12,0,49zayr,This could have a wide ranging impact: CUDA reverse engineered for Non-NVIDIA devices,https://www.reddit.com/r/MachineLearning/comments/49zayr/this_could_have_a_wide_ranging_impact_cuda/,[deleted],1457711015,[deleted],2,2
369,2016-3-12,2016,3,12,1,49ze9c,"Playing ""Moneyball"" on EA FIFA 16",https://www.reddit.com/r/MachineLearning/comments/49ze9c/playing_moneyball_on_ea_fifa_16/,aabb13_,1457712315,,0,8
370,2016-3-12,2016,3,12,1,49zixj,"Intuitive explanation of Learning to Rank (and RankNet, LambdaRank and LambdaMART)",https://www.reddit.com/r/MachineLearning/comments/49zixj/intuitive_explanation_of_learning_to_rank_and/,nikhilbd,1457714115,,0,1
371,2016-3-12,2016,3,12,2,49zowh,How to interpret - natural basis vector associated with hidden unit?,https://www.reddit.com/r/MachineLearning/comments/49zowh/how_to_interpret_natural_basis_vector_associated/,urider,1457716336,"I was reading this paper: [Intriguing properties of neural networks](http://arxiv.org/pdf/1312.6199.pdf). I can't seem to understand and visualize what this means: ""ei is the natural basis vector associated with the i-th hidden unit"" (Page 3, line 2). Can anybody help me understand this fundamentally and/or intuitively? ",3,1
372,2016-3-12,2016,3,12,2,49zrvl,neural-doodle: Turn your two-bit doodles into fine artworks with deep neural networks. An implementation of Semantic Style Transfer.,https://www.reddit.com/r/MachineLearning/comments/49zrvl/neuraldoodle_turn_your_twobit_doodles_into_fine/,rhiever,1457717446,,30,219
373,2016-3-12,2016,3,12,2,49zt3y,battle of the brains midmatch pause: alphaGo 2-0 over Sedol. commentary/ reaction/ lots topnotch links,https://www.reddit.com/r/MachineLearning/comments/49zt3y/battle_of_the_brains_midmatch_pause_alphago_20/,vznvzn,1457717922,,0,0
374,2016-3-12,2016,3,12,3,49zyql,[Question]ResNets for Instance Segmentation?,https://www.reddit.com/r/MachineLearning/comments/49zyql/questionresnets_for_instance_segmentation/,code2hell,1457719965,"TL;DR : Can Someone please explain to me the slide on image segmentation in the link [here](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)
I was looking into the slides of ResNets [here](http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf) and please do tell me if I get this right from the slides:
After the Conv layers the Fully Connected layers map to the image dimensions n*n with pixel by pixel label classification.. This provides instance mask. But I kind of get lost in the part of instance segmentation as in the part where the instance mask is again fed to a FC along with the output from the pooling layer. Thanks!",2,0
375,2016-3-12,2016,3,12,3,4a01hf,AlphaGO Pipeline,https://www.reddit.com/r/MachineLearning/comments/4a01hf/alphago_pipeline/,[deleted],1457720953,[removed],0,1
376,2016-3-12,2016,3,12,3,4a03mk,Getting Started with MXNet,https://www.reddit.com/r/MachineLearning/comments/4a03mk/getting_started_with_mxnet/,lukemetz,1457721745,,2,12
377,2016-3-12,2016,3,12,4,4a077t,New to ML and need some help,https://www.reddit.com/r/MachineLearning/comments/4a077t/new_to_ml_and_need_some_help/,lrossy,1457723091,"Hi /r/MachineLearning

I've recently started looking into machine learning, and i've started with running through the following:

 - https://www.udacity.com/course/deep-learning--ud730
 - https://www.tensorflow.org/versions/r0.7/get_started/index.html

Although they were great to get me started, and seem to be able to generate somewhat decent predictions with just a few lines of code, I'm struggling to understand how I can transfer that knowledge to my situation.

In a nutshell, what I want is a program that i can feed in the current time, day (perhaps some other fields), and user_id and it tell me what it thinks i would want to watch.

Basically, this is what I have is a database of people and their netflix habbits. A sample of one of these records may look as follows (I have tens of thousands of these records):

usershows.db:
{uid: 1, show: 'The Wire', duration: 3600, doy: 22, dow: 2, timestamp: 1457721264, ...}

Am i just not understanding tensorflow, or am I using the wrong tool for the job here?



",5,0
378,2016-3-12,2016,3,12,4,4a0a6n,The Sadness and Beauty of Watching Googles AI Play Go,https://www.reddit.com/r/MachineLearning/comments/4a0a6n/the_sadness_and_beauty_of_watching_googles_ai/,blueeyes44,1457724238,,5,20
379,2016-3-12,2016,3,12,4,4a0azq,Sommelier Training with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4a0azq/sommelier_training_with_tensorflow/,chrisvmiller,1457724541,,0,0
380,2016-3-12,2016,3,12,5,4a0kg4,PhD Student Summer Visiting Research on Optimization and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4a0kg4/phd_student_summer_visiting_research_on/,guifranca,1457728061,"Bento's lab in the computer science department at Boston College is seeking excelent PhD students for a research summer project on optimization and machine learning.

For more details please check the link:

https://drive.google.com/open?id=0B42asTN-XqIKeEJVa3pqVjU5dUU",0,3
381,2016-3-12,2016,3,12,5,4a0kq6,"Train faster, generalize better: Stability of stochastic gradient descent by Hardt, Recht, Singer. A critic's view.",https://www.reddit.com/r/MachineLearning/comments/4a0kq6/train_faster_generalize_better_stability_of/,[deleted],1457728172,[deleted],0,0
382,2016-3-12,2016,3,12,5,4a0oy6,Unsupervised Learning with Even Less Supervision Using Bayesian Optimization,https://www.reddit.com/r/MachineLearning/comments/4a0oy6/unsupervised_learning_with_even_less_supervision/,Zephyr314,1457729743,,11,27
383,2016-3-12,2016,3,12,7,4a15lm,Montreal Deep Learning Summer School 2016,https://www.reddit.com/r/MachineLearning/comments/4a15lm/montreal_deep_learning_summer_school_2016/,egrefen,1457736051,,5,15
384,2016-3-12,2016,3,12,9,4a1ouy,Visualization of Centroid Movements for K-Means Clustering (Original Content),https://www.reddit.com/r/MachineLearning/comments/4a1ouy/visualization_of_centroid_movements_for_kmeans/,lanemik,1457744319,,0,6
385,2016-3-12,2016,3,12,10,4a1p4o,Question about estimating intermediate training values,https://www.reddit.com/r/MachineLearning/comments/4a1p4o/question_about_estimating_intermediate_training/,onlinerocker,1457744430,"I'm currently going through Machine Learning by Tom Mitchell.


The book states that intermediate training values can be estimated by plugging in the successor of the current checkers board state (the example being used is a program learning to play checkers) into our target function.


I don't understand how we find the successor??",0,1
386,2016-3-12,2016,3,12,10,4a1ttv,Thoughts on variational autoencoder language model?,https://www.reddit.com/r/MachineLearning/comments/4a1ttv/thoughts_on_variational_autoencoder_language_model/,cjmcmurtrie,1457746478,"I have recently implemented a var-AE language model (along the lines of that described in http://arxiv.org/abs/1511.06349). I am seeing some interesting results. It is much slower to train than a standard RNN language model, for understandable reasons. My KL divergence loss term is not always stable, and its behaviour depends a lot on model size, batch size, and data. Getting the decoder to communicate with the encoder is an interesting challenge, as suggested in the paper.

My biggest confusion regarding VAE LMs is their behaviour outside of training time (i.e. at test/generation time). We encode a sequence for the decoder to reconstruct - but sampling makes the encoding process non-deterministic. I understand why we might desire this to regularise training, but cannot see it being useful during generation. Thoughts?

Besides, what are the main findings or bugbears with this model class compared to a vanilla recurrent language model? Is there any reason to think this model would or would not be appropriate for language processing tasks such as document classification or translation?",0,0
387,2016-3-12,2016,3,12,11,4a1xzt,Is it possible to train a model to learn audio?,https://www.reddit.com/r/MachineLearning/comments/4a1xzt/is_it_possible_to_train_a_model_to_learn_audio/,catbackpack,1457748267,Let's say I have a bunch of dialog from say Morgan Freeman as an audio file. And let's say I want the user to be able to type in any sentence and the program be able to output what it thinks that sentence sounds like it Morgan Freeman's voice. Anybody have an ideas?,10,0
388,2016-3-12,2016,3,12,11,4a2294,An naive problem on weight using sklearn,https://www.reddit.com/r/MachineLearning/comments/4a2294/an_naive_problem_on_weight_using_sklearn/,insomnia9527,1457750182,"  I got a confusion, for example, when I actually get real particle one number is 50, and particle two number is 10^6 . In this case , how can I do a classification for them ?
  My suppose is that , to generate both of two particles for 10^8 to avoid the statistic error. So should I add a weight before training? Or can I using , for example 50 particle one and 10^6 particle two to test , and get the right result?",2,0
389,2016-3-12,2016,3,12,12,4a28uq,Lee Sedol vs AlphaGo: Match 3,https://www.reddit.com/r/MachineLearning/comments/4a28uq/lee_sedol_vs_alphago_match_3/,Quaaraaq,1457753198,,31,45
390,2016-3-12,2016,3,12,12,4a29su,Top 10 Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/4a29su/top_10_machine_learning_algorithms/,vincentg64,1457753635,,0,1
391,2016-3-12,2016,3,12,13,4a2izy,GM Acquires Driverless Tech Company Cruise Automation,https://www.reddit.com/r/MachineLearning/comments/4a2izy/gm_acquires_driverless_tech_company_cruise/,j_lyf,1457757955,,1,5
392,2016-3-12,2016,3,12,15,4a2w40,[Ask] Famous paper/author for CNN applications?,https://www.reddit.com/r/MachineLearning/comments/4a2w40/ask_famous_paperauthor_for_cnn_applications/,[deleted],1457765250,[deleted],1,0
393,2016-3-12,2016,3,12,16,4a2yvq,I used GoogLeNet convolutional neural network to classify adversarial images for deep learning,https://www.reddit.com/r/MachineLearning/comments/4a2yvq/i_used_googlenet_convolutional_neural_network_to/,[deleted],1457767064,[deleted],0,1
394,2016-3-12,2016,3,12,17,4a33ll,AlphaGo is 3-0,https://www.reddit.com/r/MachineLearning/comments/4a33ll/alphago_is_30/,[deleted],1457770550,"I'm definitely feeling for Sedol, but it was still an incredible match! GG, DeepMind.",195,334
395,2016-3-12,2016,3,12,20,4a3joj,Pneumatic Solenoid Valve,https://www.reddit.com/r/MachineLearning/comments/4a3joj/pneumatic_solenoid_valve/,barrettmontalbo,1457782952,,0,0
396,2016-3-12,2016,3,12,21,4a3l8w,A new test for conscious machines?,https://www.reddit.com/r/MachineLearning/comments/4a3l8w/a_new_test_for_conscious_machines/,cjmcmurtrie,1457784023,"As a simple and easy to understand assessment of a machine's ability to simulate human behaviour, the Turing Test is nevertheless unsatisfying. In a certain sense, the Turing Test is designed to give us an assurance that a particular machine can extend our own sense of our experience, rather than show us that it exists in its own experiences.

This idea seems to extend into wider AI principles. All our classification algorithms are delivering results that are verifiable by us - extending what we already know. For instance, a classifier that predicts the colour of a pixel. As I am someone already capable of seeing this specific interpretation of a colour spectrum, that machine only adds weight to my own model of the world. It is an extension reaching outside of my physical limits of my own internal representations.

On the other hand, a conscious machine would surely be more interested in extending its own consciousness into the world, rather than extending mine. If anything, it would want to try to convince me that its own convictions regarding the world were correct. Crucially, it would hold beliefs about the world and exist in experiences that did not concord with my own.

So maybe a test for consciousness in a machine could be something more on those lines:

""A machine that convinces me beyond reasonable doubt that it has experienced something that I don't understand, and that I myself cannot experience, has shown itself to be a conscious agent.""

For instance:

-  A machine that claims to see a colour that I cannot, convinces me that it is able to see it

-  A machine that holds a belief in something that makes no sense to me, leaves me with no doubt about its convictions

-  A machine creates a symbolic system that is outside of my experience, but argues that it is a sensible system in its experience",14,0
397,2016-3-12,2016,3,12,21,4a3oex,"In LSTM's does each input connect to all of the hidden nodes, or just for one hidden node?",https://www.reddit.com/r/MachineLearning/comments/4a3oex/in_lstms_does_each_input_connect_to_all_of_the/,snapleft,1457786156,"I've seen two scenarios :
[LSTM with each input connected separately to a hidden node !](http://i.imgur.com/J3DwxSF.png)
[ LSTM with each input connected to all of the hidden layer!](http://recognize-speech.com/images/ann/lstm-arch.png)
So, which is the correct way of connecting an LSTM ?",4,3
398,2016-3-12,2016,3,12,21,4a3pgn,What would be the next milestone after Go?,https://www.reddit.com/r/MachineLearning/comments/4a3pgn/what_would_be_the_next_milestone_after_go/,meatPulp,1457786819,"Google DeepMind trained AlphaGo to have Artificial Intuition, what would be the next milestone? ",44,14
399,2016-3-12,2016,3,12,22,4a3vcm,Has anyone done Machine Learning Specialization on Coursera?,https://www.reddit.com/r/MachineLearning/comments/4a3vcm/has_anyone_done_machine_learning_specialization/,Inori,1457790313,"Talking about [this](https://www.coursera.org/specializations/machine-learning) one.

What are your thoughts? Specifically, I'm interested in the capstone project since it's the only one I can't preview without paying.

I'm currently going through the Regression &amp; Classification courses and they seem good enough that I'm considering buying the whole thing. Learned quite a few things, despite completing the course by Andrew Ng.",4,5
400,2016-3-13,2016,3,13,0,4a48z4,Papers on one-shot sequence learning?,https://www.reddit.com/r/MachineLearning/comments/4a48z4/papers_on_oneshot_sequence_learning/,rd11235,1457797484,Anyone know of any papers on one-shot sequence learning? For example train an RNN on task A then use the learned representation to train for task B using only a single sequence?,1,2
401,2016-3-13,2016,3,13,0,4a4a5a,BPTT with Deep Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4a4a5a/bptt_with_deep_recurrent_neural_networks/,exp0wnster,1457798067,"Hello everyone,

I have been studying recurrent neural networks when I have had the free time, and I can follow how the mathematics works for gradient descent with a single hidden layer.  However, I am lost at how to derive the update formulas if I were to create a 2 or 3 layers deep network. Does anybody have any resources they can point me to?

Thanks so much!",4,4
402,2016-3-13,2016,3,13,2,4a4itu,Texture Networks: Feed-forward Synthesis of Textures and Stylized Images,https://www.reddit.com/r/MachineLearning/comments/4a4itu/texture_networks_feedforward_synthesis_of/,benanne,1457802110,,14,16
403,2016-3-13,2016,3,13,2,4a4mws,Eliezer Yudkowsky on AlphaGo,https://www.reddit.com/r/MachineLearning/comments/4a4mws/eliezer_yudkowsky_on_alphago/,[deleted],1457803816,[deleted],0,1
404,2016-3-13,2016,3,13,2,4a4n13,Eliezer Yudkowsky on AlphaGo,https://www.reddit.com/r/MachineLearning/comments/4a4n13/eliezer_yudkowsky_on_alphago/,sieisteinmodel,1457803868,,136,11
405,2016-3-13,2016,3,13,2,4a4p0m,Interesting blog for manifold learning,https://www.reddit.com/r/MachineLearning/comments/4a4p0m/interesting_blog_for_manifold_learning/,Alirezag,1457804685,I just passed this blog and found it very useful for different computer vision topics. I was reading its manifold part: https://prateekvjoshi.com/2014/06/21/what-is-manifold-learning/ But in general i found it helpful. so I thought maybe it help some people. Also please feel free to cm some other interesting/useful links if you know about manifold learning. Cheers,1,6
406,2016-3-13,2016,3,13,3,4a4rls,"Most of all, AlphaGo has proved DJ Khaled wrong, a far more impressive feat.",https://www.reddit.com/r/MachineLearning/comments/4a4rls/most_of_all_alphago_has_proved_dj_khaled_wrong_a/,[deleted],1457805747,[deleted],1,0
407,2016-3-13,2016,3,13,3,4a4wn1,Bella Bell in a relaxed position,https://www.reddit.com/r/MachineLearning/comments/4a4wn1/bella_bell_in_a_relaxed_position/,antio_miller,1457807779,,0,1
408,2016-3-13,2016,3,13,3,4a4wuu,be gentle with me,https://www.reddit.com/r/MachineLearning/comments/4a4wuu/be_gentle_with_me/,0lifea-amy,1457807875,,0,1
409,2016-3-13,2016,3,13,3,4a4x4j,Nude Selfie,https://www.reddit.com/r/MachineLearning/comments/4a4x4j/nude_selfie/,2debbie,1457807984,,0,1
410,2016-3-13,2016,3,13,4,4a55vh,Tricks in Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4a55vh/tricks_in_deep_neural_networks/,Gere1,1457811622,,7,65
411,2016-3-13,2016,3,13,5,4a5ckk,[Question] Object Detection using Caffe (CNN),https://www.reddit.com/r/MachineLearning/comments/4a5ckk/question_object_detection_using_caffe_cnn/,__Julia,1457814474,"I was using a classifier to detect objects using OpenCV's HC Classifier, I am working currently to update my model. What are some papers that you recommend to learn about training the model using Images (Negative/Positive) ? ",1,1
412,2016-3-13,2016,3,13,5,4a5eek,Faster neural doodle,https://www.reddit.com/r/MachineLearning/comments/4a5eek/faster_neural_doodle/,dmitry_ulyanov,1457815280,,1,2
413,2016-3-13,2016,3,13,5,4a5fdc,The Tidings of Go,https://www.reddit.com/r/MachineLearning/comments/4a5fdc/the_tidings_of_go/,thekasrak,1457815691,,3,0
414,2016-3-13,2016,3,13,5,4a5fe4,"AlphaGo is kicking ass, but what does that mean for the rest of Machine Learning?",https://www.reddit.com/r/MachineLearning/comments/4a5fe4/alphago_is_kicking_ass_but_what_does_that_mean/,PersonFromYourStory,1457815697,i.e. what are the developments from getting really good at GO that impact machine learning more generally?,5,0
415,2016-3-13,2016,3,13,6,4a5knx,[xpost from r/dataisbeautiful] Top 6000 English words mapped to 2D space using Singular Value Decomposition and rendered in JavaScript,https://www.reddit.com/r/MachineLearning/comments/4a5knx/xpost_from_rdataisbeautiful_top_6000_english/,[deleted],1457817946,[deleted],0,0
416,2016-3-13,2016,3,13,7,4a5v37,Classifier Technology and the Illusion of Progress: on the need of reproducible research in ML,https://www.reddit.com/r/MachineLearning/comments/4a5v37/classifier_technology_and_the_illusion_of/,insider_7,1457822582,"Abstract: A great many tools have been developed for supervised classification, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm.

http://arxiv.org/abs/math/0606441
",3,1
417,2016-3-13,2016,3,13,8,4a5z1h,Fine-tuning RNNs and LSTMs for NLP,https://www.reddit.com/r/MachineLearning/comments/4a5z1h/finetuning_rnns_and_lstms_for_nlp/,Pieranha,1457824406,"I've seen many successful cases of fine-tuning pretrained ConvNets for computer vision tasks, but I haven't found any research papers fine-tuning an RNN or LSTM. Why is that? Are the NLP tasks not similar enough? Or does it not make sense to freeze e.g. the 1st LSTM layer of a network with 2 LSTM layers?",1,3
418,2016-3-13,2016,3,13,8,4a5z2i,How can we get an even remotely accurate estimate of the prior of a neural network?,https://www.reddit.com/r/MachineLearning/comments/4a5z2i/how_can_we_get_an_even_remotely_accurate_estimate/,FourthHead,1457824416,"I was recently introduced to using Bayesian interference for neural networks; essentially, Monte Carlo methods are used to sample different neural network models, and then the samples are used along with Bayesian interference to get a better, hyperparameter-less, estimation.

However, in order to use Bayesian inference, you need to have a prior for the weights of the neural network. My professor essentially said that coming up with the prior was beyond the scope of the course and didn't give too much of an explanation.

So I looked a bit into it online and found [this](https://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/slides/bayesian.pdf). On slide 9, they discuss priors, and of course, they use a generic Gaussian model.

Now, I'm sure their justification has to do with the central limit theorem, but I feel like that's a very weak justification. The prior needs to be at least *relatively* accurate in order for Bayesian interference to work, and a Gaussian model would be way too rough of an approximation.

In fact, I don't understand how we could possible estimate such an incredibly high dimensional prior. Neural networks are *extremely* complicated (in terms of distributions), and I'm having trouble seeing how any model we come up with would be even vaguely correct. And if our prior is way off, the estimations generated by Bayesian interference would be nonsense. ",13,4
419,2016-3-13,2016,3,13,8,4a60vh,What are some low hanging research problems an undergrad could tackle on his own?,https://www.reddit.com/r/MachineLearning/comments/4a60vh/what_are_some_low_hanging_research_problems_an/,FourthHead,1457825220,"I'm going to be finishing my first course on machine learning this semester. It's a grad course, so it's relatively rigorous and extensive; I believe that I would have a solid grasp of the foundations of machine learning at the end of the course.

After the course, I would like to tackle a (simple) research problem on my own. I'm more than willing to spend a couple of months reading/learning more about a specific topic in order to be able to solve a problem in the field.

I was hoping someone could point me in the right direction. I'd obviously prefer the problem to be interesting and fun, but I also don't want it to be unreasonably hard (I'm happy to put in the effort, I just don't want to put in several months and then realize there's no way I can solve it).

I don't have any specific interests since I'm so new to the field. I know a good idea would be to work with a professor, but that's not an option because I'm already doing research with another professor in an unrelated field at the moment (and don't plan on stopping any time soon).

It's *really* important to me that the work is publishable, I really want to get a paper out of this.",33,6
420,2016-3-13,2016,3,13,9,4a6aeo,"Demis Trump? ... while researchers were in the process of inventing backprop, LSTMs, convNNs in the 1990s, Demis Hassabis thinks: ""academia was on hold in the 90s, and all these new techniques hadnt really been popularized or scaled yet.... so actually the best AI was going on in games.""",https://www.reddit.com/r/MachineLearning/comments/4a6aeo/demis_trump_while_researchers_were_in_the_process/,syncoPete,1457829602,"Full paragraph:

""Yeah, I think so, and I actually think we were doing unbelievably cutting-edge AI. I would say at that stage academia was on hold in the 90s, and all these new techniques hadnt really been popularized or scaled yet  neural networking, deep learning, reinforcement learning. So actually the best AI was going on in games. It wasnt this kind of learning AI we work on now, it was more finite-state machines, but they were pretty complex and they did adapt. Games like Black &amp; White had reinforcement learning  I think its still the most complex example of that in a game. But then around 2004-5 it was clear that the games industry was going a different way from the '90s when it was really fun and creative and you could just think up any idea and build it. It became more about graphics and franchises and FIFA games and this kind of thing, so it wasnt that interesting any more  Id done everything I could in games and it was time to gather different information ready for the launch of DeepMind. And that was neuroscience; I wanted to get inspiration from how the brain solves problems, so what better way than doing a neuroscience PhD?""

http://www.theverge.com/2016/3/10/11192774/demis-hassabis-interview-alphago-google-deepmind-ai",8,0
421,2016-3-13,2016,3,13,10,4a6eky,DeepMind Sceptics' Thread,https://www.reddit.com/r/MachineLearning/comments/4a6eky/deepmind_sceptics_thread/,[deleted],1457831575,[deleted],8,0
422,2016-3-13,2016,3,13,10,4a6et1,Videos of Stanford's CS231n: Convolutional Neural Networks for Visual Recognition - Winter 2016,https://www.reddit.com/r/MachineLearning/comments/4a6et1/videos_of_stanfords_cs231n_convolutional_neural/,sharno,1457831697,,5,34
423,2016-3-13,2016,3,13,10,4a6frr,[LIVE] Match 4 - Google DeepMind Challenge Match: Lee Sedol vs AlphaGo,https://www.reddit.com/r/MachineLearning/comments/4a6frr/live_match_4_google_deepmind_challenge_match_lee/,pedromnasc,1457832161,,75,57
424,2016-3-13,2016,3,13,11,4a6kle,Nobody in the world knows how to train one hidden layer,https://www.reddit.com/r/MachineLearning/comments/4a6kle/nobody_in_the_world_knows_how_to_train_one_hidden/,throwawayprogrammer9,1457834533,,5,0
425,2016-3-13,2016,3,13,12,4a6udw,Two Minute Papers - How DeepMind Conquered Go With Deep Learning (AlphaGo),https://www.reddit.com/r/MachineLearning/comments/4a6udw/two_minute_papers_how_deepmind_conquered_go_with/,rhiever,1457839505,,0,13
426,2016-3-13,2016,3,13,14,4a78iy,Faster neural doodle,https://www.reddit.com/r/MachineLearning/comments/4a78iy/faster_neural_doodle/,dmitry_ulyanov,1457847409,"This is my approach on [neural doodle](https://github.com/alexjc/neural-doodle). It does not use patch-based idea and more like original Artistic Style algorithm by L. Gatys. It takes several minutes to draw a picture.

[fast neural doodle](https://github.com/DmitryUlyanov/fast-neural-doodle)",9,20
427,2016-3-13,2016,3,13,16,4a7lc4,AlphaGo vs Deep Blue,https://www.reddit.com/r/MachineLearning/comments/4a7lc4/alphago_vs_deep_blue/,CptnLarsMcGillicutty,1457855892,"I'm trying to explain to some friends the differences in the methods used by Deep Blue and AlphaGo, but I dont quite understand myself if AlphaGo is necessarily better at its job than Deep Blue was at its own job. I have several interesting questions to ask.

Can you guys elucidate the exact differences between the deep learning methods used by AlphaGo, and the methods used by Deep Blue (not current chess AI). Would you say that AlphaGo has shown to be more superior at Go vs Lee Sedol than Deep Blue was at chess vs Gary Kasparov? Is the deep learning method used by AlphaGo overall superior to the method used by Deep Blue?

Also, are there any weaknesses AlphaGo has shown, or might be prone to? (I'm currently watching game 4 and the commentators seem to think AlphaGo might be losing) My understanding is that while machine learning can be incredibly powerful, AI powered by ML may become ""confused"" when faced with scenarios it is unfamiliar with. What were the weaknesses that allowed Kasparov to take games from Deep Blue when it was purported to be able to calculate 200,000,000 positions per second to find the optimal move? 

**And my biggest question of all**: If you used deep learning/neural networks to make an AlphaChess AI of some sort comparable to AlphaGo, and trained it to play against a modernized version of Deep Blue (whatever the current best chess AI is, assuming its not using the exact same method as AlphaGo), which would win in the long run? Would AlphaChess be able to learn how to exploit Deep Blue 2.0's weaknesses, if they exist? Or is the method used by Deep Blue simply superior to deep learning models when it comes to chess? Or would every game end up being a stalemate?",6,2
428,2016-3-13,2016,3,13,17,4a7mqi,DeepMind vs Google Brain team,https://www.reddit.com/r/MachineLearning/comments/4a7mqi/deepmind_vs_google_brain_team/,fuxbar01,1457856882,"I am a bit unsure at the difference between the mission statements of both teams. As I understood, DeepMind was a separate company which was acquired by Google and runs as its own entity under Alphabet. Brain team is a group within Google and focuses more on improving Google services via Machine Learning. Is that a safe assumption? Could one assume that the Brain team contributed to the making of AlphaGo? I would be curious to learn more of the collaboration between the 2 groups.",0,2
429,2016-3-13,2016,3,13,17,4a7nbm,These Afghans wrote a kilometer long letter of thanks to Google Translate,https://www.reddit.com/r/MachineLearning/comments/4a7nbm/these_afghans_wrote_a_kilometer_long_letter_of/,abidriaz,1457857267,,0,3
430,2016-3-13,2016,3,13,17,4a7pfx,AlphaGo lost the 4th game: AlphaGo 3-1 Lee Sedol,https://www.reddit.com/r/MachineLearning/comments/4a7pfx/alphago_lost_the_4th_game_alphago_31_lee_sedol/,meflou,1457858744,,197,375
431,2016-3-13,2016,3,13,17,4a7pm4,AlphaGo resigns match 4,https://www.reddit.com/r/MachineLearning/comments/4a7pm4/alphago_resigns_match_4/,Jadeyard,1457858867,,5,14
432,2016-3-13,2016,3,13,18,4a7sgy,DeepMind's AI and learning from pixels,https://www.reddit.com/r/MachineLearning/comments/4a7sgy/deepminds_ai_and_learning_from_pixels/,fuxbar01,1457861106,"In one of the various conference calls, DeepMind stated that its AI was learning the atari games from simply taking in pixels; that being said, the AI was unsupervised. Where I am a bit confused is how was the AI able to understand wins from failures? Was the game state pre-programmed so for example it understood already what a loss looked like and hence (in simple terms) could learn from failure?",1,0
433,2016-3-14,2016,3,14,2,4a93lr,The art of neural networks | Mike Tyka | TEDxTUM,https://www.reddit.com/r/MachineLearning/comments/4a93lr/the_art_of_neural_networks_mike_tyka_tedxtum/,Martin81,1457889864,,0,0
434,2016-3-14,2016,3,14,2,4a97na,"Notes for ""Regularization and variable selection via the elastic net"" paper.",https://www.reddit.com/r/MachineLearning/comments/4a97na/notes_for_regularization_and_variable_selection/,shagunsodhani,1457891585,,1,2
435,2016-3-14,2016,3,14,2,4a985c,How does AlphaGo's Value Network work?,https://www.reddit.com/r/MachineLearning/comments/4a985c/how_does_alphagos_value_network_work/,otakuman,1457891785,"I was discussing AlphaGo's yesterday defeat vs. Lee Sedol, and we kinda got stuck on how AlphaGo evaluates its moves.

From what I understand, the Go game can be viewed as an overlapping of several ""mini-games"" taking place in various positions of the board (and these form clusters of larger ""games"", all at the same time): Yesterday's game had a left region, a right region (with a very complicated empty space between both), and other small formations in the edges of the board.

My question is whether AlphaGo can actually understand this, and ""think"" in terms of the Go game, i.e. ""this move is threatening to put an Atari here, it could force me to do this other move, I can't use a stair here and there because white is occupying a strategic position here"", and so on, or if it's just evaluating the board using ""dumb"" decision trees.

Specifically, I'm interested to know the inner workings of the value network. Is it a black box, i.e. just put these layers of ""artificial neuron"" nodes and train them to see what we get? Can we ""x-ray"" into them and view a map of concepts and relationships inside it? Can we know what and how the network thinks while it's playing?

If anyone can provide some insight, I'd really appreciate it.",10,2
436,2016-3-14,2016,3,14,2,4a98dp,Apache Mahout 0.11.2 Release,https://www.reddit.com/r/MachineLearning/comments/4a98dp/apache_mahout_0112_release/,based2,1457891883,,0,1
437,2016-3-14,2016,3,14,3,4a9aj3,Problems with machine translation,https://www.reddit.com/r/MachineLearning/comments/4a9aj3/problems_with_machine_translation/,thai_tong,1457892746,,2,0
438,2016-3-14,2016,3,14,4,4a9i18,Alphago versus Lee Sedol. The question now is who will win Game 5?,https://www.reddit.com/r/MachineLearning/comments/4a9i18/alphago_versus_lee_sedol_the_question_now_is_who/,Vfarcy,1457895754,,0,0
439,2016-3-14,2016,3,14,4,4a9n2t,AI vs human intelligence. What do you think?,https://www.reddit.com/r/MachineLearning/comments/4a9n2t/ai_vs_human_intelligence_what_do_you_think/,y05f,1457897727,"A question occurred to my mind when I was watching the press conference of the 4th match of AlphGo vs Lee Sedol, especially when Demis Hassabis (CEO of DeepMind) said that AlphaGo was trained over 10 millions games and playing itself.

Has Sedol played 10 millions games to be as he is now? Of course not, so how come human intelligence learns that fast on few examples?

Same case if we look for simple examples like this one in which you should define the muffins and the chihuahuas [link](https://pbs.twimg.com/media/CdOxQRbWAAEUZM6.jpg). Human intelligence can learn and solve the problem using these few images but if I want to use AI to solve it I need a whole data set.

So, What do you think?",8,0
440,2016-3-14,2016,3,14,4,4a9ns5,AlphaGo vs Lee sedol press conference after first win,https://www.reddit.com/r/MachineLearning/comments/4a9ns5/alphago_vs_lee_sedol_press_conference_after_first/,Loredra,1457897989,,0,1
441,2016-3-14,2016,3,14,5,4a9s35,AlphaGo vs. Lee Sedol Game 4 Conspiracy Theories,https://www.reddit.com/r/MachineLearning/comments/4a9s35/alphago_vs_lee_sedol_game_4_conspiracy_theories/,alexmlamb,1457899709,"This is the official conspiracy theory thread.  I'll start with a beginner theory: 

Lee Sedol secretly trained his own version of AlphaGo, and searched for adversarial examples in his value network convnet.  He used these positions to win in the fourth game, but not the other games for some reason.  ",22,13
442,2016-3-14,2016,3,14,5,4a9xdd,What is the difference between a variational auto-encoder and just adding Gaussian noise to the output of the hidden layer?,https://www.reddit.com/r/MachineLearning/comments/4a9xdd/what_is_the_difference_between_a_variational/,cjmcmurtrie,1457901811,"Adding Gaussian noise to the hidden representation would have a regularising effect and make the decoder interpret the hidden codes as filling a smooth space, without a KL divergence penalty on the loss. I know the KL bound loss makes it a neater solution in theory, and that the variance of the added noise depends on the inputs rather than being fixed, but in practice these things are tricky. If the aim is to regularize, is adding Gaussian noise not an attractive and simpler solution?",10,2
443,2016-3-14,2016,3,14,5,4a9xjb,The weakness it had in match 4 was possibly pointed out by /u/loae after match 3 yesterday,https://www.reddit.com/r/MachineLearning/comments/4a9xjb/the_weakness_it_had_in_match_4_was_possibly/,[deleted],1457901876,[deleted],0,1
444,2016-3-14,2016,3,14,5,4a9y8c,AlphaGo's weakness in match 4 was possibly pointed out after match 3 by /u/loae,https://www.reddit.com/r/MachineLearning/comments/4a9y8c/alphagos_weakness_in_match_4_was_possibly_pointed/,Lost4468,1457902164,,14,3
445,2016-3-14,2016,3,14,6,4aa4dp,Play Reversi with Q-Learning,https://www.reddit.com/r/MachineLearning/comments/4aa4dp/play_reversi_with_qlearning/,ming0308,1457904660,"I am learning Machine Learning by trying to develop Reversi's AI by using Q-Learnring. In the earlier training, the win rate does increase to about 65%, but after more iterations, the win rate will fall back to about 50%. The weird thing it happens every time although the neural network is initiated randomly. I am writing to see if anyone could shed the light on this.

**Here is setup of my neural network:**  
My neural network has 3 layers, input layer(64) -&gt; hidden layer(50)-&gt; output layer(1). RELU is used in hidden layer and a sigmoid activation is used to convert the output to -1 to 1. The board is 8X8, and hence 64 input, -1 = white, 0 = empty, 1 = black. If the output is closer to -1, it means white has advantage over black and vice versa.

**And here is the training process.**  
The AI player is playing black it is playing against a player that just puts pieces randomly. In each turn, lets say in time t, AI player lists all the possibilities currently, simulate one step further by putting a piece in each possible position, and feed every possible board state in time t+1 to the neural network mentioned above. AI Player will take the highest score's move as it plays black. And 
in the end of game, update the neural network as follows:
Reward = 1 if black wins and = -1 if white wins.
For ending state,   `new score = reward`  
For all intermediate states:  

    new_score(t) = old_score(t) + learning_rate * ( discount_factor * (new_score(t+1) - old_score(t))

**Potential problem I can think of:**  
When I feed new game result into neural network, will it ""forgets"" all previous game result?

**Code of my neural network (it' short):**  
https://github.com/ming030890/Reversi-AI/blob/master/src/main/java/com/example/reversi/nn/NeuralNetwork.java

Thanks
",6,1
446,2016-3-14,2016,3,14,6,4aa8fj,Question: Can a program be developed to make the thought processes of neural networks more transparent and intelligible?,https://www.reddit.com/r/MachineLearning/comments/4aa8fj/question_can_a_program_be_developed_to_make_the/,platypus-observer,1457906260,"For example, can a program interpret the ""thinking"" that preceded AlphaGo's ""mistakes"" and provide an explanation, or would you have to manually sift through all the data?

Or, at some point, is it/ will it be impossible to comprehend the lines of reasoning coursing through our new robot overlord's minds?

I think such transparency would be very useful and help mitigate some of the more dystopian views of AI.

thanks",4,0
447,2016-3-14,2016,3,14,7,4aa93q,"Quick question: Is the term ""neural network"" an umbrela term that encompasses many types of algorithms?",https://www.reddit.com/r/MachineLearning/comments/4aa93q/quick_question_is_the_term_neural_network_an/,abdoulio,1457906529,[removed],7,3
448,2016-3-14,2016,3,14,7,4aa9si,How does it make sense for AlphaGo to get the same time as a human?,https://www.reddit.com/r/MachineLearning/comments/4aa9si/how_does_it_make_sense_for_alphago_to_get_the/,dsocma,1457906812,"

[thinking time of AlphaGo vs Lee Sendol in match 4](http://i.imgur.com/3HcJKbk.png)  You can see that Lee uses much more time at the beginning and then is forced to make quick decisions toward the end.

I think this is pretty weak on DeepMind's part.  They should have given Lee basically as much time as he needs.  Giving equal time to both players makes literally no sense from a fairness standpoint, there is no ability to compare the hardware of a human brain vs  1,920 CPUs and 280 GPUs. Its a massive unfair advantage for AlphaGo.

I would much rather see a match were Lee was given 5-10x as much time and allowed to take breaks, eat, ect.

So basically, AlphaGo gets to benefit from using 10,000 times more power, and gets to benefit from all the advantages that computers have in combinatorial games, but Lee isn't even allowed to use his human brain to full capacity. Its bullshit.

A time constraint is completely unfair for the Lee since computers are very good at doing cost-benefit analysis to determine exactly how much time it will need to adequately search through the possibilties to find a near-optimal solution.  So DeepMind just tweaked the algorithm and added computational power until it was capable of achieving a near optimal result in 1 minute.

Lee has no such advantage, he has to think like a human, and there is no shortcutting that process.  He isn't sitting there and running through billlions of possibilites per second and calculating exactly what the EV of each move is, he has to rely on intuition which is unpredictable.  He can't weigh the exact value of spending an extra minute on a move will have.  For all he knows, maybe if he thinks for another 2 minutes, he will have a brilliant epiphany and win the match.   On top of that, he has to worry about overlooking something simple, and spend precious minutes doing the slow process of calculating in his head various possibilities which is a slow process that humans are not suited for. ",8,0
449,2016-3-14,2016,3,14,7,4aachl,Given the same board state (including blank) would AlphaGo play the same moves each time?,https://www.reddit.com/r/MachineLearning/comments/4aachl/given_the_same_board_state_including_blank_would/,justmysubs,1457907948,,3,3
450,2016-3-14,2016,3,14,7,4aadvq,How to combine CNN with CRF in python?,https://www.reddit.com/r/MachineLearning/comments/4aadvq/how_to_combine_cnn_with_crf_in_python/,andrewbarto28,1457908542,"I have trained a CNN to predict the label of the center pixel of patches. The problem I am trying to solve has two classes and can be viewed as semantic segmentation. I am observing some label inconsistency relative to the color of the object and I think CRF can correct the CNN initial prediction. I have never used CRF before. I am using tensorflow to train the CNN. Is there a easy way to implement CRF using tensorflow or other lib in python? I am interested in training jointly or separately, which ever is easier to implement. ",2,0
451,2016-3-14,2016,3,14,7,4aaeqi,Prediction thread: What application areas will ML disrupt in 10 years?,https://www.reddit.com/r/MachineLearning/comments/4aaeqi/prediction_thread_what_application_areas_will_ml/,[deleted],1457908897,[deleted],0,0
452,2016-3-14,2016,3,14,8,4aai67,What is the relationship between machine learning algorithms in a self driving car and algorithms that learn how to play games like Chess? (not Go),https://www.reddit.com/r/MachineLearning/comments/4aai67/what_is_the_relationship_between_machine_learning/,koinon3a,1457910347,[removed],0,1
453,2016-3-14,2016,3,14,8,4aajab,Is there a way to join/melt together two pretrained networks?,https://www.reddit.com/r/MachineLearning/comments/4aajab/is_there_a_way_to_joinmelt_together_two/,fimari,1457910814,[removed],0,0
454,2016-3-14,2016,3,14,8,4aama7,Deep Learning Comparison Sheet: Deeplearning4j vs. Torch vs. Theano vs. Caffe vs. TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4aama7/deep_learning_comparison_sheet_deeplearning4j_vs/,predef,1457912083,,1,0
455,2016-3-14,2016,3,14,8,4aanyu,Deep-Q learning Pong with Tensorflow and PyGame,https://www.reddit.com/r/MachineLearning/comments/4aanyu/deepq_learning_pong_with_tensorflow_and_pygame/,DanielSlater8,1457912796,,5,119
456,2016-3-14,2016,3,14,9,4aavpq,LSTM hidden states in practice,https://www.reddit.com/r/MachineLearning/comments/4aavpq/lstm_hidden_states_in_practice/,j_lyf,1457916184,"How are LSTM's used in practice wrt hidden states?

I am trying to create a sequence to sequence map (regression) using LSTM's in Keras. My MSE training loss is low. When predicting a training example, I get good results as expected. However, in real life, two training sequences (with known outputs), let's say *x* and *y* may occur consecutively, even though this particular sequence is not in the dataset. I made the LSTM stateful so that the hidden states would not be reset after predicting each sequence. The problem is that though I get good results after predicting *x*, I get bad results after predicting *y* in the next batch.  I was expecting a smooth transition between the known outputs of x and y.

I do notice, however, if I reset the states of a single layer just after the first batch, I see something that is a little close to what I expected. My question is there any tips on how can I ""blend"" two or more examples without the first one ""overpowering"" the second? Is this known within the literature?

EDIT: By batch I mean single example (1, n_steps, n_features)",0,1
457,2016-3-14,2016,3,14,10,4aazk5,Any recommendations on Implementing Convolutional Neural Networks from scratch?,https://www.reddit.com/r/MachineLearning/comments/4aazk5/any_recommendations_on_implementing_convolutional/,_blub,1457917869,"My implementation will be in python with at most the help of Numpy. Please note that I am open to any resources, arbitrary of implementation language since I will be more concerned on the mathematical definitions.

With that said are there any research papers or tutorials that you have really contributed to your understanding of ConvNets? I would love to read into those. 

Thanks! =]",15,5
458,2016-3-14,2016,3,14,10,4ab5kd,"Historical question, what happened to layer-wise greedy training of deep networks?",https://www.reddit.com/r/MachineLearning/comments/4ab5kd/historical_question_what_happened_to_layerwise/,Professional_123,1457920600,"I just stumbled into this decade old presentation 
http://deeplearning.cs.cmu.edu/notes/hefny_Greedy.pdf

In the last slide they suggested that backprop would be dead by 2010 (apparently not the case). I wonder what happened to their approach?",6,7
459,2016-3-14,2016,3,14,11,4ab99z,Gears - Siraj ft Charlie Chaplin [First Rap MV to Mention Neural Nets],https://www.reddit.com/r/MachineLearning/comments/4ab99z/gears_siraj_ft_charlie_chaplin_first_rap_mv_to/,llSourcell,1457922299,,1,0
460,2016-3-14,2016,3,14,11,4abc30,"Career change from supply chain to AI, Udacity Machine Learning Engineer nanodegree worth it?",https://www.reddit.com/r/MachineLearning/comments/4abc30/career_change_from_supply_chain_to_ai_udacity/,[deleted],1457923643,[removed],0,1
461,2016-3-14,2016,3,14,11,4abc3c,AlphaGo is not the solution to AI,https://www.reddit.com/r/MachineLearning/comments/4abc3c/alphago_is_not_the_solution_to_ai/,[deleted],1457923647,[deleted],0,1
462,2016-3-14,2016,3,14,12,4abfs6,Yann LeCun's comment on AlphaGo and true AI,https://www.reddit.com/r/MachineLearning/comments/4abfs6/yann_lecuns_comment_on_alphago_and_true_ai/,[deleted],1457925432,,32,37
463,2016-3-14,2016,3,14,13,4abnx0,Companies using Deep learning / AI in robotics?,https://www.reddit.com/r/MachineLearning/comments/4abnx0/companies_using_deep_learning_ai_in_robotics/,Nakoium,1457929514,[removed],0,1
464,2016-3-14,2016,3,14,14,4abs29,What is the advantage/difference of Variational Autoencoder compare to normal Autoencoder and denoising AE?,https://www.reddit.com/r/MachineLearning/comments/4abs29/what_is_the_advantagedifference_of_variational/,Alirezag,1457931905,"What is the advantage of Variational Autoencoder compare to normal (stacked) Autoencoder and denoising AE? Also, is the VAE kinda a stacked AE as well? If not, can I make it stacked AE? Moreover, for normal AE we train En and De and then just use the trained encoder part. is the the same for VAE? Can someone clarify it please.",4,1
465,2016-3-14,2016,3,14,14,4absi5,Evaluating Word Embeddings!,https://www.reddit.com/r/MachineLearning/comments/4absi5/evaluating_word_embeddings/,Jxieeducation,1457932178,,2,5
466,2016-3-14,2016,3,14,15,4aby4y,"Roadworthy 4WD Diesel Mechanics | Oakleigh South, Clayton, Chadstone, Mount Waverley, Murrumbeena, Glen Waverley, Ashwood, Bentleigh, Bentleigh East",https://www.reddit.com/r/MachineLearning/comments/4aby4y/roadworthy_4wd_diesel_mechanics_oakleigh_south/,SamonJoy,1457935565,,0,1
467,2016-3-14,2016,3,14,15,4ac2ip,What's the high pressure reactor?,https://www.reddit.com/r/MachineLearning/comments/4ac2ip/whats_the_high_pressure_reactor/,mixmachinery,1457938443,[removed],0,1
468,2016-3-14,2016,3,14,16,4ac5j2,Digitizing the coral reef: HyperDiver surveys,https://www.reddit.com/r/MachineLearning/comments/4ac5j2/digitizing_the_coral_reef_hyperdiver_surveys/,achennu,1457940472,,1,1
469,2016-3-14,2016,3,14,16,4ac5yf,How prone are (convolutional) neural networks to lossy data compression?,https://www.reddit.com/r/MachineLearning/comments/4ac5yf/how_prone_are_convolutional_neural_networks_to/,RichardKurle,1457940767,"Imagenet ilsvrc for example uses .jpeg images, but I cannot find details about the compression ratio. Would it give a substantial performance boost to train on .png?",2,2
470,2016-3-14,2016,3,14,17,4ac8zz,ELI5 ReLU and its fuctions,https://www.reddit.com/r/MachineLearning/comments/4ac8zz/eli5_relu_and_its_fuctions/,ssreekanth2000,1457943058,,10,1
471,2016-3-14,2016,3,14,18,4ach4x,Where can I find a big dataset with images with exif?,https://www.reddit.com/r/MachineLearning/comments/4ach4x/where_can_i_find_a_big_dataset_with_images_with/,alehander42,1457949128,,2,0
472,2016-3-14,2016,3,14,19,4aci6r,Project AIX: Using Minecraft to build more intelligent technology - Next at Microsoft,https://www.reddit.com/r/MachineLearning/comments/4aci6r/project_aix_using_minecraft_to_build_more/,Creative-Name,1457949866,,5,16
473,2016-3-14,2016,3,14,20,4acnft,Minecraft to run artificial intelligence experiments,https://www.reddit.com/r/MachineLearning/comments/4acnft/minecraft_to_run_artificial_intelligence/,[deleted],1457953311,[deleted],0,1
474,2016-3-14,2016,3,14,20,4acp8i,Single again then start Affair with Divorced Women,https://www.reddit.com/r/MachineLearning/comments/4acp8i/single_again_then_start_affair_with_divorced_women/,bolden84,1457954455,,0,1
475,2016-3-14,2016,3,14,20,4acp8p,Found and met a wonderful girl on your site and everything is going just perfectly!,https://www.reddit.com/r/MachineLearning/comments/4acp8p/found_and_met_a_wonderful_girl_on_your_site_and/,searzhong-bill,1457954457,,0,1
476,2016-3-14,2016,3,14,20,4acs9r,R/Shiny app for interactive RNN tensorflow models,https://www.reddit.com/r/MachineLearning/comments/4acs9r/rshiny_app_for_interactive_rnn_tensorflow_models/,rshah4,1457956369,,0,4
477,2016-3-14,2016,3,14,20,4acsqc,Strategies for Successful Interpersonal Communication Computer Education for All,https://www.reddit.com/r/MachineLearning/comments/4acsqc/strategies_for_successful_interpersonal/,[deleted],1457956622,[deleted],0,0
478,2016-3-14,2016,3,14,21,4act9a,TensorFlow on Raspberry Pi 3,https://www.reddit.com/r/MachineLearning/comments/4act9a/tensorflow_on_raspberry_pi_3/,TheTwigMaster,1457956935,,49,49
479,2016-3-14,2016,3,14,21,4aculr,I wrote an article about Computer Vision in 2016: From Terminator to Deep dreaming. Feedback appreciated.,https://www.reddit.com/r/MachineLearning/comments/4aculr/i_wrote_an_article_about_computer_vision_in_2016/,ieldanr,1457957751,,2,0
480,2016-3-14,2016,3,14,22,4ad2dl,Watch an AI play Mario: Level 2!,https://www.reddit.com/r/MachineLearning/comments/4ad2dl/watch_an_ai_play_mario_level_2/,AI_on_Twitch,1457961908,"After 46 hours, MarI/O completed it's first level. Now we're on to level 2! 

The winning route can be seen here: https://www.youtube.com/watch?v=AGcLCWfo2xs

Watch live!
https://www.twitch.tv/mrhellhound1",21,18
481,2016-3-14,2016,3,14,22,4ad4zl,Advice on Deep Learning with Time Series,https://www.reddit.com/r/MachineLearning/comments/4ad4zl/advice_on_deep_learning_with_time_series/,xristos_forokolomvos,1457963180,"So I have a lot of [these](http://imgur.com/x8ii7F5) discrete time series with values=(0,1,2,3) which stand for blue=(standing, walking,running,unkown) and green=(silence,voice,noise,unknown). These are the outputs of an activity recognition classifier that uses accelerometer/microphone data from smartphones. When smartphone owners woke up, they answered a questionnaire as to whether their sleep was good or bad. I want to try and create a DNN architecture that will take them (each separately or combined?) as input and predict their sleep quality.

* Do I go for Convolutional Network or RNN?

* Should I interpolate the series to make them more smooth or is taking 4 discrete values acceptable?

* Should I one-hot-encode my data or feed it in this shape?

Any resources that might help are also welcome

EDIT: I failed to mention I a lot of them but only 700 are labeled, is that too few?",12,22
482,2016-3-14,2016,3,14,23,4adefe,"Machine Learning: An In-Depth, Non-Technical Guide  Part 4",https://www.reddit.com/r/MachineLearning/comments/4adefe/machine_learning_an_indepth_nontechnical_guide/,innoarchitech,1457967424,,0,13
483,2016-3-15,2016,3,15,0,4adi97,Which Java library for machine learning classification?,https://www.reddit.com/r/MachineLearning/comments/4adi97/which_java_library_for_machine_learning/,BlackHawk90,1457969005,"Hello

I would like to do machine learning in Java. I have already developed the whole pipeline including cross-validation by myself. What I need is only a library to apply classifiers. I'm using EJML to store the dataset as matrices. So, the input (training and test set) to the classifiers should be either a 1D or 2D array (I don't think that there exists a library taking EJML as input). The output should be not just the accuracy but it should be the predicted label for each point in the test set (e.g. as a 1D array).

I'm looking for classifiers like k-nearest neighbours, naive bayes, logistic regression, decision trees, random forest, bagginb, boosting etc.

 Which easy to use Java library is best for this?",14,0
484,2016-3-15,2016,3,15,0,4adife,Stability as a foundation of machine learning,https://www.reddit.com/r/MachineLearning/comments/4adife/stability_as_a_foundation_of_machine_learning/,mttd,1457969073,,3,5
485,2016-3-15,2016,3,15,0,4adnsf,Playing Atari with Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/4adnsf/playing_atari_with_deep_reinforcement_learning/,auraham,1457971122,,0,0
486,2016-3-15,2016,3,15,1,4adrie,Unsupervised LSTM using keras?,https://www.reddit.com/r/MachineLearning/comments/4adrie/unsupervised_lstm_using_keras/,gabjuasfijwee,1457972540,"Is it possible to do unsupervised RNN learning (specifically LSTMs) using keras or some other python-based neural network library? If so, could someone lead me to some code examples? Thanks!",4,0
487,2016-3-15,2016,3,15,2,4ady3u,EM for multiple missing values in a Bayesian net,https://www.reddit.com/r/MachineLearning/comments/4ady3u/em_for_multiple_missing_values_in_a_bayesian_net/,[deleted],1457974921,[deleted],0,2
488,2016-3-15,2016,3,15,2,4ae3a9,Using remote hardware resources for training?,https://www.reddit.com/r/MachineLearning/comments/4ae3a9/using_remote_hardware_resources_for_training/,[deleted],1457976750,[deleted],2,0
489,2016-3-15,2016,3,15,2,4ae5mc,An AI to play Google's Dinosaur Game (Subs in English),https://www.reddit.com/r/MachineLearning/comments/4ae5mc/an_ai_to_play_googles_dinosaur_game_subs_in/,joaopedrovbs,1457977544,,0,1
490,2016-3-15,2016,3,15,2,4ae69a,Audio Deepdream?,https://www.reddit.com/r/MachineLearning/comments/4ae69a/audio_deepdream/,the_original_meepo,1457977750,"Hi! I'm a sound engineer who is stuck manually extracting and amplifying psychoacoustic features of music(a process often described as mastering).

My manual feature extraction process creates many correlated signals; I want to put this through a deep learning algorithm to get some sort of evolving impulse response over time/frequency or such that amplifies the psychoacoustic transients of the material. 

As far as I'm aware the existing algorithms that do this are tuned to small 2d features, when tried to apply to music which has much longer range and higher order features, it just results in near-random sample-and-hold type noise.

Is this a very difficult task?


",13,13
491,2016-3-15,2016,3,15,2,4ae6ew,Why Enrollment is Surging in Machine Learning Classes,https://www.reddit.com/r/MachineLearning/comments/4ae6ew/why_enrollment_is_surging_in_machine_learning/,cryptoz,1457977803,,60,70
492,2016-3-15,2016,3,15,2,4ae6xf,Looking for an open-source model to detect whether an image has text,https://www.reddit.com/r/MachineLearning/comments/4ae6xf/looking_for_an_opensource_model_to_detect_whether/,thecity2,1457977969,,1,0
493,2016-3-15,2016,3,15,3,4aecg7,"Have there been any attempt to emulate human psychological heuristics in machine learning, such as the peak-end rule which dramatically overweight the utility of the greatest and final instance/underweight the utility in totality?",https://www.reddit.com/r/MachineLearning/comments/4aecg7/have_there_been_any_attempt_to_emulate_human/,googolplexbyte,1457979872,"https://en.wikipedia.org/wiki/Peak%E2%80%93end_rule

https://en.wikipedia.org/wiki/Heuristics_in_judgment_and_decision-making

From what I can see, most assume these cognitive biases are flaws in our mental capacity that cause us to under-perform.

But I don't know if they've been implemented and shown to do so.",1,0
494,2016-3-15,2016,3,15,3,4aeear,Can NN recognize horse's rear if only trained on images of the horse's front?,https://www.reddit.com/r/MachineLearning/comments/4aeear/can_nn_recognize_horses_rear_if_only_trained_on/,doktorfaustus91,1457980513,"What I am essentially asking is: Has any work been done on the ability of neural networks to generalize and extrapolate? On what representations they learn or what features are primarily responsible for a certain classification? I have come across the hidden state visualization techniques that uncover learned stroke (in lower layers) and face (in higher layers) detectors (see f.e. [Erhan et al. 2009](https://www.researchgate.net/profile/Aaron_Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530.pdf)). But I don't whether a NN could learn something like ""If the thing has brown short fur it's probably a horse"".

Would a NN be able to recognize the objects in mirror-inverted images of the training set (even of non-symmetrical objects)?

And another thought: Do we have any models that can learn to detect and distinguish different classes of objects in images without knowing any labels (e.g. in a unsupervised setting), so they are still able to tell whether there is the same class of objects in two images? If we had something like that, one-shot learning of the right labels for this classes should be possible.

I would appreciate any hints or references.

Thanks and best :)",9,0
495,2016-3-15,2016,3,15,3,4aegug,I can't find github repo: Turning MS Paint Pictures Into Art With Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4aegug/i_cant_find_github_repo_turning_ms_paint_pictures/,CrazyCrab,1457981442,A week or two ago I saw this program on github either on reddit or hackernews that turned primitive pictures made in paint into pretty art. Now I can't find it. Can anyone help me?,1,0
496,2016-3-15,2016,3,15,3,4aehly,Embedded NEAT?,https://www.reddit.com/r/MachineLearning/comments/4aehly/embedded_neat/,abdoulio,1457981710,"In a complex game like Minecraft or even a much simpler survival game, would it be possible to embed NEAT systems in the following fashion (or some variation of it):

-Initialize one NEAT for each possible goal (find food, fight bear, stay warm, etc.)
-Have another neat which's output classifies the type of situation the AI finds himself in and choose which neat will take over the actor's behavior (hunger meter is high -&gt;find food NEAT, sees bear -&gt; fight bear NEAT)",0,0
497,2016-3-15,2016,3,15,3,4aei1o,Methods to learn what a list of words all have in common?,https://www.reddit.com/r/MachineLearning/comments/4aei1o/methods_to_learn_what_a_list_of_words_all_have_in/,CS_throwaway_GOAL,1457981851,"Hello, recently I have been thinking about the idea of a genetic algorithm to ""learn"" what features an input list of words all have in common so that it could then generate new, similar words. The only thing I could think to do would be to find the most frequently occurring pairs of 2 side by side characters and then perhaps go deeper and see which character combinations are likely to appear together, then making a fitness function to evaluate the character pairs in random strings. I don't actually have much experience with genetic programming so I'm not sure what kinds of metrics are worth measuring to solve this problem. 

Thank you so much for any advice you might have ! ",0,0
498,2016-3-15,2016,3,15,3,4aeic9,What if the training set had a negative input ?,https://www.reddit.com/r/MachineLearning/comments/4aeic9/what_if_the_training_set_had_a_negative_input/,snapleft,1457981960,"If all inputs were standardised to have mean 0 and standard deviation 1 over the training set, then we will have *negative input* to the neural network
&gt; 3.906506e-01;-1.039934e+00;-1.055215e+00;-3.759106e-02  

my question here, is such an input valid to train the network ?",4,0
499,2016-3-15,2016,3,15,4,4aeimy,A Neural Network library from GNU - Gneural Network,https://www.reddit.com/r/MachineLearning/comments/4aeimy/a_neural_network_library_from_gnu_gneural_network/,bjossibjoss,1457982065,,29,17
500,2016-3-15,2016,3,15,4,4aeple,Discord Server for ML,https://www.reddit.com/r/MachineLearning/comments/4aeple/discord_server_for_ml/,EigenHands,1457984477,"Hey everyone,
 
One thing I feel is lacking in the ML community is some place where you can hang out while you're working on a project, and freely talk about ML related topics or quickly ask a question. Reddit in some ways fills this niche, but I think as a semi-permanent forum, organic conversations aren't something it does well. IRC also fills this niche somewhat but I feel like it is missing some key features like optional voice chat, and is kind of bare bones and ugly.  


Lately I've been using discord and have really been enjoying it, I feel like it's everything I've wanted IRC to be but didn't know it. [Discord](https://discordapp.com/) is a free text and voice chat app. It is marketed towards gamers but I find it is like a more community oriented version of Slack. It has a clean UI, rudimentary markup tools, one-click invite links, mod tools, etc.. It runs on all platforms natively except linux(which is what I use most of the time), but has a fully functioning web-app version that you can use on linux. Supposedly a linux native client is coming ""soon"". It has separate text and voice channels so that you could say, primarily talk via text but can move to a voice channel if desired. It is not somewhat inclusive like slack so anyone can join at any time without a special invite.

Being really interested in a discord server for ML and seeing none existed, I ""created"" one here with a basic layout and permissions: 
https://discord.gg/0t2MLkak0CEEkWKc . 

That being said, it is very easy(and free) to create a discord server. If other people are interested in creating one for ML then more power to them. I'm just interested in there being a discord server for the community.",3,0
501,2016-3-15,2016,3,15,5,4aeszf,New Report Share details about Global Commercial Microturbine Systems Industry  MIR Company Report for 2016,https://www.reddit.com/r/MachineLearning/comments/4aeszf/new_report_share_details_about_global_commercial/,marketintelreports16,1457985680,,0,1
502,2016-3-15,2016,3,15,5,4aexb3,"ML/NLP ""Tone Analytics"" trained on Twitter data yields funny results when run on lyrics from your Spotify library (blog post + app + code)",https://www.reddit.com/r/MachineLearning/comments/4aexb3/mlnlp_tone_analytics_trained_on_twitter_data/,kauffecup,1457987202,,0,1
503,2016-3-15,2016,3,15,5,4af1i3,Global Polyphenylene Sulfide Fibers Industry Development Trend 2016,https://www.reddit.com/r/MachineLearning/comments/4af1i3/global_polyphenylene_sulfide_fibers_industry/,marketintelreports16,1457988680,,0,1
504,2016-3-15,2016,3,15,6,4af70x,"XGBoost4J: Portable Distributed XGboost in Spark, Flink and Dataflow",https://www.reddit.com/r/MachineLearning/comments/4af70x/xgboost4j_portable_distributed_xgboost_in_spark/,crowwork,1457990691,,0,12
505,2016-3-15,2016,3,15,7,4afh45,How to learn Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/4afh45/how_to_learn_machine_learning/,kiechu,1457994358,,0,0
506,2016-3-15,2016,3,15,8,4afqg8,New 'machine unlearning' technique wipes out unwanted data quickly and completely,https://www.reddit.com/r/MachineLearning/comments/4afqg8/new_machine_unlearning_technique_wipes_out/,TDaltonC,1457997945,,1,0
507,2016-3-15,2016,3,15,8,4afr9t,"Looking for the source code CD that comes with the book ""AI techniques for game programming"" by Mat Buckland. Couldn't find it anywhere and I already have the book.",https://www.reddit.com/r/MachineLearning/comments/4afr9t/looking_for_the_source_code_cd_that_comes_with/,abdoulio,1457998283,I looked everywhere and I don't want to pay 60$ for a second copy of the book just to have the CD.,0,0
508,2016-3-15,2016,3,15,8,4afuqa,"We all want unsupervised learning, but what are we trying to achieve?",https://www.reddit.com/r/MachineLearning/comments/4afuqa/we_all_want_unsupervised_learning_but_what_are_we/,cjmcmurtrie,1457999667,"Most people in the machine learning community want real unsupervised learning, either because they need it in their work or research, or because they've heard somewhere that it will ultimately be better. In the context of MLP models, what are we trying to achieve?

Supervised learning can also be interpreted as unsupervised learning, depending on the proposed solution. For instance, two sensor + network models might learn to map each other's respective inputs to outputs. The labelling happens naturally as an outcome of co-occurrence. An input from one sensor triggers a response from the other, and so on. Simultaneously occurring experiences are mapped to each other, which leads to unsupervised learning with backprop and so on - unsupervised learning within the supervised paradigm.

But nonetheless, with backpropagation, we still completely waste the forward pass during training. The two sensors mentioned above learn nothing from their own signals. If a repetitive signal exists in the forward direction, is it really so hard to find it?

There is a lot of material available to us, for example in fluid dynamics, that we can try to relate to the forward pass in our neural network models. There is a sense that the vector spaces learned in neural network feature extractors could be extracted in the forward pass.

When we talk about unsupervised learning in MLP models, is there some clarity that we are trying to exploit the forward pass, and weaken the dependence on the backward pass?",3,1
509,2016-3-15,2016,3,15,8,4afuu8,Infographic or animation that visualizes ANNs?,https://www.reddit.com/r/MachineLearning/comments/4afuu8/infographic_or_animation_that_visualizes_anns/,supermanava,1457999708,"I'm mentioning artificial neural networks in a brief video and was looking for an infographic or animation that provides a simple visualization for ANNs. Anyone come across one? 
  
Thanks in advance. ",3,4
510,2016-3-15,2016,3,15,9,4afxbz,[LIVE] Match 5 - Google DeepMind Challenge Match: Lee Sedol vs AlphaGo,https://www.reddit.com/r/MachineLearning/comments/4afxbz/live_match_5_google_deepmind_challenge_match_lee/,pedromnasc,1458000694,,41,67
511,2016-3-15,2016,3,15,9,4ag296,Memory networks: Are we moving towards an end to end ai architecture?,https://www.reddit.com/r/MachineLearning/comments/4ag296/memory_networks_are_we_moving_towards_an_end_to/,thegreatshasha,1458002644,,3,0
512,2016-3-15,2016,3,15,10,4ag8ld,Diagnosing Heart Diseases with Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4ag8ld/diagnosing_heart_diseases_with_deep_neural/,317070,1458005092,,4,56
513,2016-3-15,2016,3,15,12,4agphn,Introduction to Scikit Flow - Key Features Illustrated,https://www.reddit.com/r/MachineLearning/comments/4agphn/introduction_to_scikit_flow_key_features/,terrytangyuan,1458011964,,2,7
514,2016-3-15,2016,3,15,12,4agqlt,Career advice needed - planning to make a career change from mechanical engineering to computer vision/machine learning,https://www.reddit.com/r/MachineLearning/comments/4agqlt/career_advice_needed_planning_to_make_a_career/,justiliang,1458012471,"I am currently in my last year of Mechanical Engineering, specifically, I am in the Mechatronics option at UBC. I've known for a while that my true interests lie in software/computer science, however, I did not know which specialization I was most interested in. Thus, I did not apply for graduate school. This semester I have been exposed to machine vision through a course I am sitting in and also my capstone project. I've been reading academic papers and implementing edge detectors, texture synthesis, template matching algorithms and more for fun and it has been a blast! 

I've decided that I want to pursue this field further and apply for graduate school for next year. I now need to figure out what I should do from now until next September 2017.  I've thought of a few options but wasn't sure which would be the best:

1. Volunteer all my time at a computer vision lab. I'm thinking of volunteering at Greg Mori (professor at SFU who is pretty reputable in the field and does very interesting research related to object and activity recognition). Or maybe volunteer at a different lab abroad but that can be expensive.
2. Try to find a work term with a professor, however, I have been told that this is difficult as I am no longer a student and professors usually only have scholarships that they can provide to students.
3. Work in the industry at a job related to computer vision.

I'm thinking of applying to UofT or one of the top universities in the US. Because of this, I feel like my best option would be to work or volunteer at a computer vision lab and gain some research experience. My main goal is to gain as much experience as possible. What do you guys think?",2,0
515,2016-3-15,2016,3,15,12,4agqy0,I want to self-learn how to get involved with ML,https://www.reddit.com/r/MachineLearning/comments/4agqy0/i_want_to_selflearn_how_to_get_involved_with_ml/,ronbbot,1458012622,"I've done a lot of reading on Machine Learning and I'm really really interested, but I want to learn how to get involved. I'm a freshman in college right now, and I know that I need to learn Linear Algebra. I started 4 days ago, and I'm about 1/10th of the way done with it, but it definitely gets harder. 

What do I do next? What about after that?",10,3
516,2016-3-15,2016,3,15,12,4agtmb,"Quora Session with Joaquin Quionero Candela, Director of Applied Machine Learning at Facebook",https://www.reddit.com/r/MachineLearning/comments/4agtmb/quora_session_with_joaquin_quionero_candela/,alxndrkalinin,1458013902,,0,9
517,2016-3-15,2016,3,15,13,4agxlq,A Torch-Installing Ansible Playbook,https://www.reddit.com/r/MachineLearning/comments/4agxlq/a_torchinstalling_ansible_playbook/,phenylphenol,1458015875,,0,0
518,2016-3-15,2016,3,15,13,4agyrz,"Glass Bottle, Wine Bottle Pad Printing Machine from China Factory , Show You How to Do Four Printing In One Time",https://www.reddit.com/r/MachineLearning/comments/4agyrz/glass_bottle_wine_bottle_pad_printing_machine/,printersupplier,1458016483,,0,1
519,2016-3-15,2016,3,15,16,4ahg6f,Does anybody want to help me create a bitcoin bot which would utilize deep learning?,https://www.reddit.com/r/MachineLearning/comments/4ahg6f/does_anybody_want_to_help_me_create_a_bitcoin_bot/,Tomhinueber,1458026876,I'll supply the bitcoin to trade with. Split profits 50/50. ,0,0
520,2016-3-15,2016,3,15,16,4ahhot,I have a javascript neural-learning chatbot whose highest-rated post to reddit is in /r/fifthworldproblems. What are some other good subreddits for a bot who was once genuinely mistaken for a human with psychosis?,https://www.reddit.com/r/MachineLearning/comments/4ahhot/i_have_a_javascript_neurallearning_chatbot_whose/,joyowns,1458027968,,0,0
521,2016-3-15,2016,3,15,17,4ahn7g,Looking for a well commented neural network in Java,https://www.reddit.com/r/MachineLearning/comments/4ahn7g/looking_for_a_well_commented_neural_network_in/,Hacon,1458032040,"I learn better by trying to apply example code and I'm really interested in trying to use a neural network for trading stocks.

Does anyone know where I can find some code that isn't too focused on one problem so I could reasonably modify it for my purposes?  Java and with no 3rd party packages, I would like to see all of the code easily.

Thanks for any help, my Google fu failed me.",1,0
522,2016-3-15,2016,3,15,18,4ahnsr,Sedol wins the final game!,https://www.reddit.com/r/MachineLearning/comments/4ahnsr/sedol_wins_the_final_game/,[deleted],1458032505,[deleted],0,1
523,2016-3-15,2016,3,15,18,4aho1j,Final match won by AlphaGo!,https://www.reddit.com/r/MachineLearning/comments/4aho1j/final_match_won_by_alphago/,EpicStrategist,1458032652,bow to our robot overlords.,75,183
524,2016-3-15,2016,3,15,20,4ai0yy,"When doing data augmentation, should I should original image and color jittered image in the same batch?",https://www.reddit.com/r/MachineLearning/comments/4ai0yy/when_doing_data_augmentation_should_i_should/,sunshineatnoon,1458041377,"I am training on Pascal VOC, and I jittered the contrast of the images randomly to do data augmentation. I jitter the contrast when reading images from disk so I always put original image and transferred image in the same batch. Will this have some bad influence on my training process? THX!",2,5
525,2016-3-15,2016,3,15,20,4ai3xe,Advice for a deep learning novice? (Python/CNNs),https://www.reddit.com/r/MachineLearning/comments/4ai3xe/advice_for_a_deep_learning_novice_pythoncnns/,SpaceCowboy26,1458042967,"Hi everyone,

I have been interested in ML for a while and now have the chance to do some experimentation with deep learning, specificially using a CNN as a function approximator for Q values in a video game reinforcement learning problem. Input to the CNN will be a 3 dimensional series of game screens.

 I want to utilise the network in real time while the game is running so that I can:

* Do feedforwards through the network to select actions in the game
* Do backpropagation steps to train the network as the game is played 

For the software implemenation of this, I'm most comfortable using Python, so I'm mainly looking at Theano/Tensorflow. I've also looked at Keras and Lasagne but (since they are wrapper libraries) am not really sure what specific advantages and disadvantages they give over using Theano/Tensorflow directly.

Since I'm a novice and I'm working in a real time domain, I'm mainly looking for something that is easy to use and fast to construct prototypes with, as well as having fast training time for CNNs.

Can anyone give some insight or advice into my situation?",5,3
526,2016-3-15,2016,3,15,22,4aid61,Indexing On A Semi-Automatic Filling Machine Explain,https://www.reddit.com/r/MachineLearning/comments/4aid61/indexing_on_a_semiautomatic_filling_machine/,sonusinternational,1458047521,,0,1
527,2016-3-15,2016,3,15,22,4aidus,More difficult to use Convolutional neural networks for regression than classification?,https://www.reddit.com/r/MachineLearning/comments/4aidus/more_difficult_to_use_convolutional_neural/,MartinRudky,1458047835,[removed],1,1
528,2016-3-15,2016,3,15,22,4aigjt,I have an AI Intern,https://www.reddit.com/r/MachineLearning/comments/4aigjt/i_have_an_ai_intern/,byedit,1458049077,,0,1
529,2016-3-15,2016,3,15,22,4aihdr,"After AlphaGo, what's next for AI?",https://www.reddit.com/r/MachineLearning/comments/4aihdr/after_alphago_whats_next_for_ai/,[deleted],1458049456,[deleted],0,0
530,2016-3-15,2016,3,15,22,4aii29,Whiskey Embeddings,https://www.reddit.com/r/MachineLearning/comments/4aii29/whiskey_embeddings/,ZlatanIAm,1458049767,,9,72
531,2016-3-15,2016,3,15,23,4aimj2,[Question] Visualization of character based conv nets,https://www.reddit.com/r/MachineLearning/comments/4aimj2/question_visualization_of_character_based_conv/,kar321,1458051709,"This paper (http://arxiv.org/abs/1312.6034) describes a way how to visualize what a convolution network learnt:
After training fix the weights, input an all-zero training sample and optimise over this sample to maximize the score for one class. The result is an image that has maximum score for one class and shows you what the network learnt about the class.

Now I try to do the same thing for a character based convolutional neural network for text classification. However I ran into one problem: The character input is one-hot encoded. But when I run the optimisation (optimising the sample for maximum score) I get back continuous numbers (like for pixels). However I need a one-hot encoded sample.
I tried the following: in my neural network after the input layer I forced the input to be one-hot encoded (set max value of tensor to 1 and alle the other values to zero). However, when I ran the optimisation then, it did not train/converge at all.

Is it possible to optimise over this kind of categorical input or can you only optimise over continuous input (like pixel values)? How could I make this work for character input (one-hot encoded) instead of pixel input?
",1,0
532,2016-3-16,2016,3,16,0,4aj0me,Software robots are growing in popularity with businesses,https://www.reddit.com/r/MachineLearning/comments/4aj0me/software_robots_are_growing_in_popularity_with/,mtl1015,1458057004,,0,1
533,2016-3-16,2016,3,16,1,4aj2j2,OKcupid and Natural Language Processing.,https://www.reddit.com/r/MachineLearning/comments/4aj2j2/okcupid_and_natural_language_processing/,JimTheSavage,1458057696,,5,0
534,2016-3-16,2016,3,16,1,4aj6cm,"I want to use transfer learning but need a pre-trained network that uses grey scale images as input, any suggestions?",https://www.reddit.com/r/MachineLearning/comments/4aj6cm/i_want_to_use_transfer_learning_but_need_a/,[deleted],1458059089,[deleted],3,0
535,2016-3-16,2016,3,16,1,4aj6yu,Self-Driving Cars Wont Work Until We Change Our RoadsAnd Attitudes,https://www.reddit.com/r/MachineLearning/comments/4aj6yu/selfdriving_cars_wont_work_until_we_change_our/,etzmor,1458059317,,3,0
536,2016-3-16,2016,3,16,1,4aja8o,is it possible to train doc2vec alongside word2vec?,https://www.reddit.com/r/MachineLearning/comments/4aja8o/is_it_possible_to_train_doc2vec_alongside_word2vec/,[deleted],1458060578,[deleted],0,1
537,2016-3-16,2016,3,16,2,4ajdgt,"[X-Post r/DeepDrumpf] DRUMPF-9000, a @DeepDrumpf that can run in your browser",https://www.reddit.com/r/MachineLearning/comments/4ajdgt/xpost_rdeepdrumpf_drumpf9000_a_deepdrumpf_that/,dark_tex,1458061769,"Hello fellow Redditors! I was interested in testing how big of a recurrent net can be run in the browser. I wrote some code to convert a Torch model to RecurrentJS. Blog post [here](http://testuggine.ninja/blog/torch-conversion) as well as a demo to test this out. I present to you [DRUMPF-9000](http://drumpf9000.com).

The neural network will run in your browser using your CPU, so please be a little bit patient as it will need to download a ~9 MB file and unzip it.

This is very similar to [this project](http://www.engadget.com/2016/03/04/deepdrumpf-the-donald-trump-ai-spoof-bot-america-needs/), but it's a live demo instead of a collection of cherry-picked tweets. You just type the beginning of a sentence and it will finish it as Drumpf would. I recommend you start with something like ""I believe that Mexico"" and see where it takes you. Also, play around with the temperature bar to see how your results change. Finally, this is not deterministic so the same input can and will give you very different results. In my experience, you can get pretty fun quotes if you tinker with it for a couple of minutes.",1,0
538,2016-3-16,2016,3,16,2,4ajhst,Training ensembles in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4ajhst/training_ensembles_in_tensorflow/,joeyglasgow,1458063358,"I was wondering if anyone had experience training ensembles in Tensorflow?

Essentially I have an architecture consisting of a number of networks of heterogeneous architecture combined as a product-of-experts and a cost which minimises the ensemble as a whole, not individual members.

I've done their classification tutorials &amp; understand how to train a single net, but I can't find how to reuse components and train an ensemble.

If anyone had experience with how to do this, or suggestions of what I might do it'd be very helpful.",6,2
539,2016-3-16,2016,3,16,2,4ajjxc,Why is AlphaGo important?,https://www.reddit.com/r/MachineLearning/comments/4ajjxc/why_is_alphago_important/,SingularityIsNotNear,1458064115,[removed],0,1
540,2016-3-16,2016,3,16,3,4ajltp,A new hybrid algorithm: lda2vec,https://www.reddit.com/r/MachineLearning/comments/4ajltp/a_new_hybrid_algorithm_lda2vec/,working_nut,1458064815,,0,1
541,2016-3-16,2016,3,16,3,4ajn8i,Approach to cleaning text from ascii art,https://www.reddit.com/r/MachineLearning/comments/4ajn8i/approach_to_cleaning_text_from_ascii_art/,front3412,1458065341,[removed],0,1
542,2016-3-16,2016,3,16,3,4ajnj0,What do you guys think/heard of this?,https://www.reddit.com/r/MachineLearning/comments/4ajnj0/what_do_you_guys_thinkheard_of_this/,bbsome,1458065437,,3,0
543,2016-3-16,2016,3,16,3,4ajp6a,I fed deepdream back on itself with a fractal flame!,https://www.reddit.com/r/MachineLearning/comments/4ajp6a/i_fed_deepdream_back_on_itself_with_a_fractal/,the_original_meepo,1458066044,,2,0
544,2016-3-16,2016,3,16,3,4ajpft,Approach to cleaning text from ascii art,https://www.reddit.com/r/MachineLearning/comments/4ajpft/approach_to_cleaning_text_from_ascii_art/,[deleted],1458066143,[deleted],0,0
545,2016-3-16,2016,3,16,3,4ajqri,Tensorflow Chessbot - Predicting chess pieces from images by training a single-layer classifier,https://www.reddit.com/r/MachineLearning/comments/4ajqri/tensorflow_chessbot_predicting_chess_pieces_from/,stainingpresos,1458066625,,3,10
546,2016-3-16,2016,3,16,3,4ajs5e,Help recognizing title card from youtube movie trailer clip,https://www.reddit.com/r/MachineLearning/comments/4ajs5e/help_recognizing_title_card_from_youtube_movie/,Birchlore,1458067134,"My goal is to find the title screen from a movie trailer. I need a service where I can search a video for a string, then return the frame with that string. Pretty obscure, does anything like this exist?

For [this](https://www.youtube.com/watch?v=c7fP9q_LyDc) movie , I'd scan for ""Sausage Party"" and retrieve [this](http://s13.postimg.org/io3ipwk3r/Screen_Shot_2016_03_15_at_10_50_51_AM.png) frame:


 I found the [cloudsight api](http://www.cloudsightapi.com/) which would actually work except cost is prohibitive @ $.04 per call assuming I need to split the video into 1s intervals and scan every image (at least 60 calls per video).

Any ideas?
",0,0
547,2016-3-16,2016,3,16,3,4ajvjg,"Hiring Data Scientists, Data Engineers, &amp; Data Architects",https://www.reddit.com/r/MachineLearning/comments/4ajvjg/hiring_data_scientists_data_engineers_data/,TheDataScientist,1458068345,"***MODS feel free to beat me with a cat o' nine tails for posting here, but I like to think being active in the community goes a little way...and I'd rather hire people truly passionate from one of my favorite communities.

Consulting firm. Primarily working with clients using Hadoop and AWS. Remote work except when going to client site (some clients are fine with you working from home the entire job. Others want you on site every now and then. Others want you on site the entire time).
If interested email+resume me at mfornito@opsvision.com",3,0
548,2016-3-16,2016,3,16,4,4ajx2x,Movie review dataset analysis by a python noob,https://www.reddit.com/r/MachineLearning/comments/4ajx2x/movie_review_dataset_analysis_by_a_python_noob/,madboy1995,1458068902,,0,0
549,2016-3-16,2016,3,16,4,4ak3x2,Is a human's internal dialogue a type of self-play reinforcement learning(a la AlphaGo)?,https://www.reddit.com/r/MachineLearning/comments/4ak3x2/is_a_humans_internal_dialogue_a_type_of_selfplay/,fjdkf,1458071350,"I was thinking about the importance of learning from self-play after the recent AlphaGo victory.  

Ex.1: If I have an important meeting coming up, I'll mentally  work through my thoughts ahead of time.  I'll keep iterating until I can frame them in a concise manner.

Ex.2: When I competed in track and field, many coaches strongly stressed the importance of visualizing yourself going though the motions before you did them.

Ex.3: If a job interview goes badly, I'll replay variations of it in my head until I settle on better answers.

I'm guessing there are many things going on in the brain for each of these examples, considering they all seem to involve generating and evaluating new training data.  That said, would self-play reinforcement learning be an appropriate ML analogy for these processes?",4,0
550,2016-3-16,2016,3,16,5,4akag5,[Question] Infering underlying model from data,https://www.reddit.com/r/MachineLearning/comments/4akag5/question_infering_underlying_model_from_data/,kulippo,1458073716,"Looking for solutions to this problem:

Consider a sequence of points (x1, y1),    ,(xN , yN ) is produced by the following generative model. There are L straight lines y = m(l)x, l = 1   L. To generate a point:
1. Pick l  {1   L} uniformly at random.
2. Generate x  N ((l), 100).
3. Generate y = m(l)x + N(0, ^2)

I want to infer the underlying model from the data.

(a) Draw the probabilistic graphical model representing this process. Adopt a Bayesian approach
allowing for priors on parameters, and use plate notation.

(b) Write the joint distribution function represented by it.

(c) Describe a Markov chain that explores the parameter space - what are the states of this Markov
chain?

(d) Outline MetropolisHastings transitions on this Markov chain to sample from the posterior.",1,0
551,2016-3-16,2016,3,16,6,4akk74,Deep Q-Learning in Partially Observable environments?,https://www.reddit.com/r/MachineLearning/comments/4akk74/deep_qlearning_in_partially_observable/,hazard02,1458077271,"Is there any reason to believe that Deep Q-Learning would work particularly well or poorly in partially observable environments? What about partially observable stochastic environments? 

For example, is it reasonable to believe that using current techniques we could build a world-champion quality ""AlphaGo for poker"" or ""AlphaGo for Starcraft""?",5,4
552,2016-3-16,2016,3,16,6,4aklqy,Could you give me some advices/references on model fitting?,https://www.reddit.com/r/MachineLearning/comments/4aklqy/could_you_give_me_some_advicesreferences_on_model/,eclipseadb,1458077842,"Hi, I have a problem to solve and I thought NN may help me since the math is too complicated. My idea is to run simulations for the complex problem and map the inputs/outputs with a NN. If I generate enough independent points in the model and feed it to the NN it should be able to fit it.

I have good knowledge of my model and I know exactly the 5 inputs that modify the two outputs of the system. The problem is that most points are ""smooth"" and easily predictable and only 5% of them have some weird spikes which is what it is hard to model and I am most interested in predicting with the NN.

For this reason I thought that to improve the ""spike prediction"" I should feed more spike points to the network by filtering the whole data points and removing points that do not have this property so I leave it to 50% normal points / 50% spike points and so those weird tendencies are better learned.

Besides this what do you suggest as the network structure? To save time I'm using matlab and I'm trying with a simple feedforward network and 3 hidden layers since it seems to give the best results (compared with 1 or 2). It is working well but again the spikes are not that well predicted and I assume it is because of the low % they appear in the data but I'm wondering if another structure would be more appropiate.

Thanks for you help.",0,0
553,2016-3-16,2016,3,16,6,4akn1r,"How does visual cortex wire without a ""training set""?",https://www.reddit.com/r/MachineLearning/comments/4akn1r/how_does_visual_cortex_wire_without_a_training_set/,angstrem,1458078326,"In ML, you need a training set to train classifiers. In humans, the visual cortex wires itself somehow. How? If it is unsupervised learning, why haven't it been mimiced by visual recognition neural nets?",11,0
554,2016-3-16,2016,3,16,6,4akp95,DanDoesData: skflow neural nets for CIFAR10,https://www.reddit.com/r/MachineLearning/comments/4akp95/dandoesdata_skflow_neural_nets_for_cifar10/,vanboxel,1458079170,,0,0
555,2016-3-16,2016,3,16,7,4akpv7,Why is AlphaGo significant?,https://www.reddit.com/r/MachineLearning/comments/4akpv7/why_is_alphago_significant/,SingularityIsNotNear,1458079382,"ML amateur here, trying to make sense of all this AlphaGo stuff.

Why is AlphaGo important? It seems like a novelty from all the media coverage and news articles I've read. I'm not sure how building AI systems to beat human champions at games (DeepBlue/chess, Watson/Jeopardy, AlphaGo/Go) translates into furthering the field of research and/or solving real problems.

From a technical or academic perspective, does the work by DeepMind expand or further a particular subfield of AI or deep learning substantially? Does it pave the way for future research?

Will we see the techniques being developed here applied to other games or real-world problems? For example, IBM's Watson tech is now being used in medicine and banking.

I look forward to the insights and opinions of the experts here.

Thank-you.
",12,2
556,2016-3-16,2016,3,16,8,4al3zt,An unusual machine learning challenge  win in the game with unknown rules. Prize fund is about USD $7000,https://www.reddit.com/r/MachineLearning/comments/4al3zt/an_unusual_machine_learning_challenge_win_in_the/,blackbox_challenge,1458084921,,37,140
557,2016-3-16,2016,3,16,9,4al860,Lemme splain deep learnin to y'all niggas,https://www.reddit.com/r/MachineLearning/comments/4al860/lemme_splain_deep_learnin_to_yall_niggas/,DeepLearnThatTwat,1458086544,[removed],0,1
558,2016-3-16,2016,3,16,9,4aldgq,Is AlphaGo intelligence AGI? Is Deep Q Learning better than Deep Learning? Is learning to talk better the same as reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/4aldgq/is_alphago_intelligence_agi_is_deep_q_learning/,improssibility,1458088660,,6,0
559,2016-3-16,2016,3,16,12,4am196,Does anybody know when development on AlphaGo first began?,https://www.reddit.com/r/MachineLearning/comments/4am196/does_anybody_know_when_development_on_alphago/,Balloon_Project,1458098972,"I started a project about Go players, and I'm comparing all their ages.  I've searched the Internet, but I can't find a source that will tell me what AlphaGo's ""birthday"" is.  Is it sometime in 2015? 2014?  I'd really like to know!",4,5
560,2016-3-16,2016,3,16,12,4am1ad,Steer me towards the right solution (please),https://www.reddit.com/r/MachineLearning/comments/4am1ad/steer_me_towards_the_right_solution_please/,dubez001,1458098987,"Hello,

I have been working on a personal project that involves collecting certain kinds of tweets for some time now. 

What I am looking for is the best solution that can filter out tweets that use normal words as slang e.g. ""Drake's newest album is fire"" vs. ""wow, my house is on fire"". In that instance, I want to filter out the nonsense about Drake.

After reading the post on the first page of this sub regarding whiskey suggestions built with word vectors, I became pretty excited. At first glance, word vectors sound very promising for my use-case, but I could be wrong. 

Could someone kindly shed some light on the best approach for this? I only need a direction to be pointed in, and I can take it from there. I would be exceptionally grateful for a library suggestion, preferably for Python",5,1
561,2016-3-16,2016,3,16,16,4amqj9,Spellchecking with seq2seq learning?,https://www.reddit.com/r/MachineLearning/comments/4amqj9/spellchecking_with_seq2seq_learning/,SkiddyX,1458113183,"I have been trying to do seq2seq learning to create a spellchecking for a while now and i'm just not sure what i'm missing. I have large a file of sentences (82 million), that I am training on, using character inputs (each mutated by a few characters) to word outputs. 

I'm also using a token to represent capitals (it proceeds any character or word that should be uppercase) to try to reduce the size of the output layer, along with the character layer. The model is two stacks for both the encode and the decode both the size of how many LSTMs neurons I can fit on a Titan GPU (12 GB of memory) The model fully implemented in Torch7 and using ADAM to do the optimization. Any advice of how you would solve this problem (or this problem in general) would be appreciated. Thanks.
    

",10,13
562,2016-3-16,2016,3,16,16,4amr7p,[AskReddit] Do you think TensorFlow will eventually replace Theano and Torch?,https://www.reddit.com/r/MachineLearning/comments/4amr7p/askreddit_do_you_think_tensorflow_will_eventually/,AlfonzoKaizerKok,1458113649,"I'm thinking of making a switch from Theano to TensorFlow. I've been using Theano for over a year, and I'm completely happy with it. However, I suspect in the long run, TensorFlow will be more actively developed, and eventually it'll be the unique go-to deep learning library. If that's the case, I think it'd be sensible to migrate sooner rather than later.

Do you think that's a fair assessment? What advice would you give someone in my position?",22,25
563,2016-3-16,2016,3,16,16,4amt08,For neural net research: PhD in ML/Stat or math?,https://www.reddit.com/r/MachineLearning/comments/4amt08/for_neural_net_research_phd_in_mlstat_or_math/,[deleted],1458114880,[deleted],5,0
564,2016-3-16,2016,3,16,17,4amv0h,10 Years of Open Source Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4amv0h/10_years_of_open_source_machine_learning/,tstonez,1458116641,,0,0
565,2016-3-16,2016,3,16,18,4amy7f,"Custom Machine &amp; Equipment Trailers | Aluminum, Alloy &amp; Steel Trailers",https://www.reddit.com/r/MachineLearning/comments/4amy7f/custom_machine_equipment_trailers_aluminum_alloy/,Jeeryjossaf,1458119079,,0,1
566,2016-3-16,2016,3,16,18,4an1u3,"Where are the ML algorithms? Every time I see the question ""What is ML?"" the answer is always hand wavy high level concepts with zero implementation details.",https://www.reddit.com/r/MachineLearning/comments/4an1u3/where_are_the_ml_algorithms_every_time_i_see_the/,FoxMcWeezer,1458121792,,7,0
567,2016-3-16,2016,3,16,20,4an98g,I guess it's time to update xkcd's game AIs graphic. What's next for game AI -- StarCraft?,https://www.reddit.com/r/MachineLearning/comments/4an98g/i_guess_its_time_to_update_xkcds_game_ais_graphic/,rhiever,1458126114,,13,4
568,2016-3-16,2016,3,16,20,4anbkw,Indexing On A Semi-Automatic Filling Machine Explain,https://www.reddit.com/r/MachineLearning/comments/4anbkw/indexing_on_a_semiautomatic_filling_machine/,sonusinternational,1458127325,,0,1
569,2016-3-16,2016,3,16,20,4anbzd,Why does my GPU get hot when I play video games but not when I run long long cuda simulations?,https://www.reddit.com/r/MachineLearning/comments/4anbzd/why_does_my_gpu_get_hot_when_i_play_video_games/,iateab,1458127589,,17,16
570,2016-3-16,2016,3,16,20,4ancof,"I'm ready for s....x without a relationship). I'm Kate, let's talk. Come to the site and register . My login - katesharup2016",https://www.reddit.com/r/MachineLearning/comments/4ancof/im_ready_for_sx_without_a_relationship_im_kate/,yirfgczdeka,1458127923,,0,1
571,2016-3-16,2016,3,16,20,4anexu,Play Go against a neural network with Darknet,https://www.reddit.com/r/MachineLearning/comments/4anexu/play_go_against_a_neural_network_with_darknet/,pjreddie,1458128944,,21,14
572,2016-3-16,2016,3,16,21,4anhos,"What are popular ml approaches to Bayesian Probabilistic Models, Normal Distribution Models, and Clustering Algorithms?",https://www.reddit.com/r/MachineLearning/comments/4anhos/what_are_popular_ml_approaches_to_bayesian/,hlyates,1458130154,"Hi ML Redditor friends, 

I recently am getting into machine learning. I heard a data scientist tell a group about the basic methods they used at work (and didn't get a chance to ask for details). They mentioned they primarily used: Bayesian Probabilistic Models, Normal Distribution Models, and Clustering Algorithms. 

Since I am learning, can the experienced folks give me the ""ML greatest hits"" for each of the categories above? In other words, if you had to star in a late night television infomercial of selling the greatest ml approaches that fall under Bayes, normal, and clustering, what would they be? I need to study these specific implementations that are discussed. Thanks for your patience and assistance. :-)",2,0
573,2016-3-16,2016,3,16,22,4anpsc,Looking for NLP Engineer (Natural Language Processing / Computational Linguistics / Machine Learning),https://www.reddit.com/r/MachineLearning/comments/4anpsc/looking_for_nlp_engineer_natural_language/,barrracuda_envion,1458133834,,1,1
574,2016-3-16,2016,3,16,22,4anq6h,Age Old Question: The Next Step after Andrew Ng's Course,https://www.reddit.com/r/MachineLearning/comments/4anq6h/age_old_question_the_next_step_after_andrew_ngs/,IndividualCarnival,1458134009,"Hi r/MachineLearning!

First things first: This is probably the best tech-related subreddit I've been on yet!

Like many, I'm sure, I'm close to finishing Andrew Ng's course on ML. We all know it's great, nothing extra to say. But the question is what are the next steps?

I am an undergrad student with a program that has 5 co-op terms, and naturally I need to find the right balance between continuing to learn ML material and actually proving my knowledge  with projects to future employers in my upcoming co-op terms. Yet simultaneously I want to maintain the ability to continue reading on what intrigues me in ML.

What do people recommend as the next step? I've found the Deep Learning course by Google Udactiy that I'd love to do. I was hoping to start working on a simple classification problem and then perhaps work my way up to using SVMs with OpenCV afterwards. Lastly, where does one start with reading on papers? I find it hard to find a good place to start.

Sorry for the excessive number of questions, but I hope it can just attest to my drive to continue learning!

Thanks!",55,162
575,2016-3-16,2016,3,16,22,4anuwh,Beautiful athlete immediately parted legs after the meeting pEfLLGF,https://www.reddit.com/r/MachineLearning/comments/4anuwh/beautiful_athlete_immediately_parted_legs_after/,Ay2__9Mio_8L_,1458135946,,0,1
576,2016-3-17,2016,3,17,0,4ao8cq,Any Know Bayesian Networks (New Here),https://www.reddit.com/r/MachineLearning/comments/4ao8cq/any_know_bayesian_networks_new_here/,[deleted],1458141069,[deleted],0,0
577,2016-3-17,2016,3,17,0,4aobts,What sort of algorithms are used in algorithmic trading? Does it involve machine learning?,https://www.reddit.com/r/MachineLearning/comments/4aobts/what_sort_of_algorithms_are_used_in_algorithmic/,mln00b13,1458142367,"I know that finding out some algo for predicting the stock market doesn't work that well, for so many reasons, but what are some ML algorithms that are still currently being used? Whether for predicting, or any other aspects for trading.",9,2
578,2016-3-17,2016,3,17,0,4aocf2,Demonstrating the need for a nonlinear classifier?,https://www.reddit.com/r/MachineLearning/comments/4aocf2/demonstrating_the_need_for_a_nonlinear_classifier/,TTPrograms,1458142598,"I'm currently trying to communicate the advantages of nonlinear classifiers to my team - I can draw cartoons that depict it fine, but I'd like to show a scatter plot with our data that demonstrates the advantage of a nonlinear boundary. The problem is that our data is ~50 dimensional, so I can't just brute force scatter plot it. Is there any technique to find a subspace where this nonlinear variation might lie? I'm currently using LDA on the sub-populations identified by a mixed gaussian model, but that doesn't seem especially elegant.",17,0
579,2016-3-17,2016,3,17,0,4aocgz,When to use a machine learned vs. score-based search ranker,https://www.reddit.com/r/MachineLearning/comments/4aocgz/when_to_use_a_machine_learned_vs_scorebased/,nikhilbd,1458142618,,0,1
580,2016-3-17,2016,3,17,1,4aoikb,Physical application of Q-learning to rotary inverted pendulum,https://www.reddit.com/r/MachineLearning/comments/4aoikb/physical_application_of_qlearning_to_rotary/,l_bdcdb,1458144885,"Hello MachineLearning,
We are two mechanical engineering students interested in reinforcement learning trying to apply Q-learning to a rotary inverted pendulum for a project. We have watched David Silver's ""youtube course"" and read chapters of Sutton &amp; Barto, the basic theory was easy... But we have yet to see any positive results on our pendulum.

[Here](http://imgur.com/7vF9Mpv) is a picture of the rotary inverted pendulum we built and [a graph](http://imgur.com/a/izCdV) of our latest test, showing the average reward per episode (in green). A computer running python code commutates with an Arduino which in turn controls a stepper motor. We have a rotary encoder which gives us the angle of the pendulum (from which we also calculate the angular velocity). 

As a first step, we have chosen to use Q-learning in a discrete two-dimensional state-space (angular position and velocity). We have let our system run for many hours without any sign of improvement. We have tried varying the parameters of the algorithm, the actions possible, the number of states and their partitioning, etc. 
Also, our system tends to heat up quite a bit so we have separated the learning into episodes of about 200 steps followed by a brief period of rest. In order to increase speed and precision, we batch update the Q values at the end of each episode. 

Here is our update function:

    # Get Q values from database
    Q_dict = agent.getAllQ()
    E_dict = {}

    # Set E_dict to 0 for all state-action pairs
    for s,a in a_StateActionPairs:
        E_dict[s + a] = 0

    # Q Algorithm
    # For every step
    for i_r in episode_record:
        state, action, new_state, new_action, greedy_action, R = i_r

        # Get Q for current step and calculate target
        Q = Q_dict[(state, action)]
        target = R + GAMMA*Q_dict[(new_state,greedy_action)]
        
        # Update E for visited state
        E_dict[(state, action)] += 1

        # Update Q for every state-action pair
        for s,a in a_StateActionPairs:
            updatedQ = Q_dict[(s,a)]+ALPHA*E_dict[(s,a)]*(target-Q)
            Q_dict[(s,a)] = updatedQ
            # Set E to 0 if new_action was chosen at random (epsilon-greedy)
            if greedy_action == new_action:
                E_dict[(s,a)] *= GAMMA*LAMBDA
            else: 
                E_dict[(s,a)] = 0

    # Update database
    agent.setAllQ(Q_dict)
    log.info('Qvalues updated')

Here is ""main"" part of the code: Github (https://github.com/Blabby/inverted-pendulum/blob/master/QAlgo.py)

Here our some of our hypothesis as to why are tests are unsuccessful:
- The system has not run for a long enough time
- Our exploration (e-greedy) in not adapted to the problem
- The hyper-parameters are not optimized
- Our physical system is too unpredictable 

Does anyone have any experience applying reinforcement learning to physical systems? We have hit a roadblock and are looking for help and/or ideas.",13,10
581,2016-3-17,2016,3,17,1,4aoisp,Do you need to have deep understanding of ML to develop useful projects?,https://www.reddit.com/r/MachineLearning/comments/4aoisp/do_you_need_to_have_deep_understanding_of_ml_to/,quqa,1458144972,"Let me try to clarify the question. 

Take SVM for instance. I'm following these videos https://www.youtube.com/watch?v=woEwY0Zi6X4 and https://www.youtube.com/watch?v=5zRmhOUjjGY but I can't understand what's going on as much as I'd like. Do I need to understand them fully to develop useful SVM related projects or are there usually tools that let me create SVM related projects in higher level? 

What I'm trying to ask is, learning assembly only slightly helps me to write better code in C++. Because C++ is higher level and takes care of low level things. Is it similar in machine learning? 

Do I need to have a deep understanding of ML and all the math to be able to create useful projects? (I'm assuming the things in the videos are considered deep)

The speaker in ""Ingo deep dives into Support Vector Machines"" video is CEO of Rapidminer, a company that seems to be doing pretty good. I guess if you want to have a hope of creating useful projects in ML enough to make money, you have to deep understanding. Or am I wrong?",9,2
582,2016-3-17,2016,3,17,1,4aon78,"To Get Truly Smart, AI Might Need to Play More Video Games",https://www.reddit.com/r/MachineLearning/comments/4aon78/to_get_truly_smart_ai_might_need_to_play_more/,[deleted],1458146594,[deleted],0,1
583,2016-3-17,2016,3,17,1,4aop4e,"To Get Truly Smart, AI Might Need to Play More Video Games",https://www.reddit.com/r/MachineLearning/comments/4aop4e/to_get_truly_smart_ai_might_need_to_play_more/,pauldance,1458147332,,0,1
584,2016-3-17,2016,3,17,2,4aorlz,Are there any 'Deep Learning' chat bots for personal use that anyone has created like google's skymind chatbot?,https://www.reddit.com/r/MachineLearning/comments/4aorlz/are_there_any_deep_learning_chat_bots_for/,metafruit,1458148257,I want to know before I look into making one. Skymind is open sourced under deeplearning4j. (I am not a programmer though so I would be going in from scratch),15,1
585,2016-3-17,2016,3,17,4,4apbtq,Why is poker so hard for computers?,https://www.reddit.com/r/MachineLearning/comments/4apbtq/why_is_poker_so_hard_for_computers/,How_Many_More_Times,1458155703,"Wouldn't the AI just card count and make decisions based on probabilities of getting what it thinks will be the optimal hand to win?   
  
Is the human factor of bluffing what throws off poker AI, and can AI bluff?  
  
Would love some insight to all of this because I was under the impression that online poker bots were already pretty advanced. Does this challenge specifically aim at professional poker players?",8,0
586,2016-3-17,2016,3,17,4,4api0k,How to build a predictive model from scratch in Azure Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4api0k/how_to_build_a_predictive_model_from_scratch_in/,datasciencedojo,1458158020,,0,1
587,2016-3-17,2016,3,17,4,4apii5,[Paper-arxiv] TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,https://www.reddit.com/r/MachineLearning/comments/4apii5/paperarxiv_tensorflow_largescale_machine_learning/,__smurf,1458158199,,0,1
588,2016-3-17,2016,3,17,4,4apiol,Microsoft Research Minecraft paper: Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains,https://www.reddit.com/r/MachineLearning/comments/4apiol/microsoft_research_minecraft_paper_exploratory/,InaneMembrane,1458158262,,2,34
589,2016-3-17,2016,3,17,5,4apjwf,Quora session with Anima Anandkumar,https://www.reddit.com/r/MachineLearning/comments/4apjwf/quora_session_with_anima_anandkumar/,pierrelux,1458158732,,0,0
590,2016-3-17,2016,3,17,5,4apk61,Gradient for L1 Penalty - Am I Playing With Fire?,https://www.reddit.com/r/MachineLearning/comments/4apk61/gradient_for_l1_penalty_am_i_playing_with_fire/,voodoochile78,1458158838,"Background:  

I'm have a simple linear regression problem and I want a sparse solution, hence the desire for L1 regularization.  

I am constrained in my choice of software.  I need to use a very neutered version of R with no external libraries and with many of the good internal libraries not working as well.  Many of the optimizer libraries you're likely to suggest that I use I have already tried and they won't load.  However, the optimize() function from the stats library is something that I can use.  

So that's the context.  I want to do L1 regularization with a very basic optimization package.  This optimization package has a small number of methods I can use, like Conjugate Gradients, BFGS, L-BFGS, etc.  Some of these methods require me to pass a gradient function as well as the function itself.  I'm looking for speed, so I'd like to pass a gradient function so that the solver doesn't have to waste time on numerical gradient approximations.  

Question:

As we all know, the L1 penalty of sum |weights| is non-differentiable at zero. 

I was wondering if I could define the L1 gradient as follows:

    gradient wrt weight_i = sign(weight_i) if weight_i is not 0
                           = 0             if weight_i = 0


It's an obvious fix, but I never hear anyone talking about it, so I'm wondering how dangerous this is?  The plan is to write out a function for L1-penalized sum-square error and function of its gradient with the tweak I mentioned above.  Then I will pass that function and the gradient function to the solver and hope that a correct sparse solution pops out.

UPDATE - thanks for the quick and informative replies, folks!  I knew it seemed too good to be true and I'm glad I asked.",7,0
591,2016-3-17,2016,3,17,5,4apl11,Recurrent neural networks controlling robot's gripper (video),https://www.reddit.com/r/MachineLearning/comments/4apl11/recurrent_neural_networks_controlling_robots/,[deleted],1458159161,[deleted],0,0
592,2016-3-17,2016,3,17,5,4apl1c,neural nets &amp; probability distributions - not softmax..?,https://www.reddit.com/r/MachineLearning/comments/4apl1c/neural_nets_probability_distributions_not_softmax/,PaspJ,1458159163,[removed],0,1
593,2016-3-17,2016,3,17,5,4appiy,GPU recommendations?,https://www.reddit.com/r/MachineLearning/comments/4appiy/gpu_recommendations/,yoitsnate,1458160837,"Well, I've finally got my chops up enough to start successfully training some Tensorflow models on GPU, and my current GPU (Nvidia GeForce model -- I forget which one) is not as beefy as I'd like.  I'd really like to pick up 1-3 powerful new cards and slap them into my current rig to use for training.

What are your recommendations for Nvidia GPUs?  My budget is about $500-$1000 per card.",12,0
594,2016-3-17,2016,3,17,6,4apveh,Math Topics Needed to Gain a Deeper Understanding of Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/4apveh/math_topics_needed_to_gain_a_deeper_understanding/,FamousWunder,1458163027,"I'm sure I'm not the only one with this problem but I feel that as I delve deeper into ML I find that I'm lacking in my math understanding.  I use statistics on a regular basics and have a basic math education.  I feel I conceptually understand majority of the algorithms and I'm at the stage of ""I now know how much I don't know"".  

Which leads me to my question.  What are the math topics that a person should understand if they want to gain a graduate understanding of ML?

Essentially, is it necessary to take a course on these topics or can one gain a practical understating using supplementary information available online or in books. 

* Differential Equations
* Real Analysis
* Combinatorics
* Probability Theory
* Stochastic Processes 
* Topology

",9,6
595,2016-3-17,2016,3,17,7,4aq54g,Data science intro for math/phys background,https://www.reddit.com/r/MachineLearning/comments/4aq54g/data_science_intro_for_mathphys_background/,pmigdal,1458166893,,3,0
596,2016-3-17,2016,3,17,7,4aq7bm,Any chance to run Tensorflow on Windows without docker?,https://www.reddit.com/r/MachineLearning/comments/4aq7bm/any_chance_to_run_tensorflow_on_windows_without/,Sandzaun,1458167787,,1,0
597,2016-3-17,2016,3,17,8,4aqf9k,Unseen vocabulary/words in Word2Vec,https://www.reddit.com/r/MachineLearning/comments/4aqf9k/unseen_vocabularywords_in_word2vec/,warriortux,1458171093,"Hello,

I am very new to Word2Vec and was wondering whether there is a way that Word2Vec can generate features for unseen vocabulary.

Currently I am doing this with HashingVectorizer in Python's Sklearn library. http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html


If anyone can suggest me a way of doing something similar with Word2Vec, I would really appreciate it.",8,0
598,2016-3-17,2016,3,17,9,4aqoh3,How do we better communicate about deep learning? People seem quite misinformed.,https://www.reddit.com/r/MachineLearning/comments/4aqoh3/how_do_we_better_communicate_about_deep_learning/,[deleted],1458175111,[deleted],5,0
599,2016-3-17,2016,3,17,11,4ar0i6,External case for GPU via Thunderbolt 3,https://www.reddit.com/r/MachineLearning/comments/4ar0i6/external_case_for_gpu_via_thunderbolt_3/,kakauandme,1458180447,,1,2
600,2016-3-17,2016,3,17,11,4ar2o6,Software for hand-tagging text?,https://www.reddit.com/r/MachineLearning/comments/4ar2o6/software_for_handtagging_text/,jayhack,1458181470,"Hey all, is anyone aware of robust software that allows one to efficiently hand-tag text datasets? One use-case I am currently concerned with is adding ""tags"" to arbitrarily long substrings of documents; (e.g. ""named entity"", etc.) I've come across related text-marking problems many times before and never found a satisfactory solution. 

Thanks in advance!",5,0
601,2016-3-17,2016,3,17,12,4araa7,tensorflow implementation of deep Q networks,https://www.reddit.com/r/MachineLearning/comments/4araa7/tensorflow_implementation_of_deep_q_networks/,gliese581gg2,1458185014,"I made atari playing deep q networks based on previous codes.

https://github.com/gliese581gg/DQN_tensorflow

I'm gonna use this for benchmarking my research.

It takes 1~3 days to training

I wonder how deepmind trained this with only 600 episodes",4,3
602,2016-3-17,2016,3,17,13,4aresz,Wanting to Pursue a Grad Degree in Computational Science - Need Advice!,https://www.reddit.com/r/MachineLearning/comments/4aresz/wanting_to_pursue_a_grad_degree_in_computational/,an_tsu,1458187352,"I've got a few questions, but in the spirit of saving you time I'll put my questions in context of the situation. I've just finished a stint at a startup in SF and after moving around in several departments I've realized data science is what I want to do. Before working at a startup I was in college part way through my sophomore credits before I dropped out to work for this startup. Now I want to go back for a degree but want to pivot out of marketing and into stats/CS degree for undergrad. Ultimately I want to pursue a graduate degree (PhD maybe) in Computational Science. I wanna do lab research or possibly high level industry work as a data science lead or R&amp;amp;D.


I'm debating about getting an associates degree or two at a community college to save on money and in theory have a better developed resume for internships in DS / undergrad research experience in my region. I say two because at the institution I'm looking at I can get an A.S. in Information Systems which will give me CS1010 and CS1020 as well as supporting course work in SAS and SQL which I feel will be useful technical skills to have under my belt. The second degree would be in mathematics and would be for the sole purpose of having the degree title on my resume as well and getting math credits for cheaper. I've also considered getting a single A.S. in general engineering and doing a double major in CS and Stats to diversify my credentials. I feel like double majoring in stats and CS after having 2 A.S degrees in math and information systems provides no benefit besides possibly giving me an edge applying for research internships / undergrad research experience in my Sophomore and Junior year.


In summary, how would 2 A.S. degrees in information systems and mathematics look on an interns resume while they were essentially perusing the same two degrees at a 4 year institution, compared to someone who has a single A.S. in General engineering who appears to be pivoting away from it and into a data science role with a double major in stats and computer science?


Please, no pulled punches here. Any and all criticism is welcome. I'm trying to make a pretty serious decision and don't want to waste my time or leave any value on the table by making an ill-informed decision.

Thanks -- an_tsu
",2,0
603,2016-3-17,2016,3,17,15,4arrss,Opinions of the University of Alberta experts on the AlphaGo result,https://www.reddit.com/r/MachineLearning/comments/4arrss/opinions_of_the_university_of_alberta_experts_on/,clbam8,1458194835,,4,23
604,2016-3-17,2016,3,17,15,4arup8,Looking for a committed study partner for Data Science,https://www.reddit.com/r/MachineLearning/comments/4arup8/looking_for_a_committed_study_partner_for_data/,[deleted],1458196723,[deleted],0,0
605,2016-3-17,2016,3,17,15,4arvja,[1603.05106] One-Shot Generalization in Deep Generative Models,https://www.reddit.com/r/MachineLearning/comments/4arvja/160305106_oneshot_generalization_in_deep/,iori42,1458197315,,11,49
606,2016-3-17,2016,3,17,17,4as5lb,Machine Vision Camera Market fostered by high demand and adoption of vision systems.,https://www.reddit.com/r/MachineLearning/comments/4as5lb/machine_vision_camera_market_fostered_by_high/,nehachora,1458204874,,0,1
607,2016-3-17,2016,3,17,19,4asemg,[1603.05027] Identity Mappings in Deep Residual Networks,https://www.reddit.com/r/MachineLearning/comments/4asemg/160305027_identity_mappings_in_deep_residual/,aloisg,1458211466,,4,25
608,2016-3-17,2016,3,17,19,4asexx,Lee Sedol's loss,https://www.reddit.com/r/MachineLearning/comments/4asexx/lee_sedols_loss/,Martin81,1458211697,,1,0
609,2016-3-17,2016,3,17,21,4asmld,Sklearn's t-SNE,https://www.reddit.com/r/MachineLearning/comments/4asmld/sklearns_tsne/,ClayStep,1458216402,"So I've been working on a project recently dealing with very high dimensional data. I wanted to represent the data in 2d, and as I wanted my representation to not be crowded I used sklearn's t-sne. 
However, after the tree has been built it seems their implementation attempts to use a ridiculous amount of memory. I ended up using another implementation and had no issues. Has anyone else had this problem with sklearn, or did I do something weird somewhere? ",7,1
610,2016-3-17,2016,3,17,21,4aspnk,What is the best deeplearning library for RNN (LSTM or GRU) for the purpose of video captioning?,https://www.reddit.com/r/MachineLearning/comments/4aspnk/what_is_the_best_deeplearning_library_for_rnn/,godofprobability,1458217912,"Hello! I want to start working on video captioning task on [Activity_Net](http://activity-net.org/) dataset. I have completed the course of [cs231](http://cs231n.stanford.edu/), and thus have working knowledge of CNN. I am confused among libraries like Torch, Tensorflow and Theano. 
1. Since I might have to use pre-trained(VGG-net, Alex-net etc) model also, so which library provides suitable support this? 
2. Also which library has good data and model parallelism, as it might be possible that I have to train everything from scratch. 

I am biased towards Tensorflow because it is in python, there is Tensorboard, data and model parallelism and seems convenient for RNN. But downside to this is it doesn't provide pretrained models. ",12,0
611,2016-3-17,2016,3,17,22,4asuxn,What is a good way to think about self-improvement of AlphaGo?,https://www.reddit.com/r/MachineLearning/comments/4asuxn/what_is_a_good_way_to_think_about_selfimprovement/,dhoomdhadaka,1458219831,[removed],0,1
612,2016-3-17,2016,3,17,22,4aszuq,Here are many beautiful and big women for s...x! fz9J7f0,https://www.reddit.com/r/MachineLearning/comments/4aszuq/here_are_many_beautiful_and_big_women_for_sx/,r38pt62gb75,1458221527,,0,1
613,2016-3-17,2016,3,17,22,4at2tp,Amazon g2.2xlarge spot prices are through the roof these few days; maybe alphago inspired many new deep learning users?,https://www.reddit.com/r/MachineLearning/comments/4at2tp/amazon_g22xlarge_spot_prices_are_through_the_roof/,jinpanZe,1458222544,,3,5
614,2016-3-17,2016,3,17,23,4atdpd,State of the AI market in 2015: A focus on investments,https://www.reddit.com/r/MachineLearning/comments/4atdpd/state_of_the_ai_market_in_2015_a_focus_on/,nb410,1458226045,,0,0
615,2016-3-17,2016,3,17,23,4atdwn,Gensim word2vec,https://www.reddit.com/r/MachineLearning/comments/4atdwn/gensim_word2vec/,talbaumel,1458226117,"Hey, 
When looking at the word2vec cbow model gensim implementation, Some stuff don't make sense to me
can someone help me understand what the following variables mean?
syn0
syn1
l1
l2a
fa

Thanks in advance, Tal",3,0
616,2016-3-17,2016,3,17,23,4atf31,Detecting heart arrhythmias using machine learning and Apple Watch data,https://www.reddit.com/r/MachineLearning/comments/4atf31/detecting_heart_arrhythmias_using_machine/,datadotworld,1458226506,,9,88
617,2016-3-18,2016,3,18,0,4atm4h,Checking for free GPU node on cluster?,https://www.reddit.com/r/MachineLearning/comments/4atm4h/checking_for_free_gpu_node_on_cluster/,[deleted],1458228823,[deleted],2,0
618,2016-3-18,2016,3,18,0,4atmvq,I fed a RNN with jokes and am posting the results to this sub /r/NeuralJokes,https://www.reddit.com/r/MachineLearning/comments/4atmvq/i_fed_a_rnn_with_jokes_and_am_posting_the_results/,neural_bot,1458229066,,24,9
619,2016-3-18,2016,3,18,1,4atuyz,Does anyone have a data set of Spanish sentences?,https://www.reddit.com/r/MachineLearning/comments/4atuyz/does_anyone_have_a_data_set_of_spanish_sentences/,rumirama,1458231628,"Hi, /r/machinelearning

I'm working on classifier that is able to determine if a sentece is written by a person or by a translating machine. I kinda lack of data to train the model, specially translated from a machine. I'm working with Spanish sentences in the moment.

Does anyone have/know where I can get that?  ",3,0
620,2016-3-18,2016,3,18,1,4atxc2,Text Analysis methodology question: top n-gram comaprison of two documents,https://www.reddit.com/r/MachineLearning/comments/4atxc2/text_analysis_methodology_question_top_ngram/,relganz,1458232426,"This is a simplification, but let's say I have two large documents (&gt;100 pages for each), but they are not the same length.  One may be twice as long as another.

I am trying to find the phrases that are most characteristic of one-doc-but-not-the-other. Currently, I am ranking n-grams in range(3,7) by term frequency in each doc, but that doesn't tell me which n-grams are more characteristic of each document.  Taking the absolute value by term of those two frequencies is a decent strategy, but I'm wondering if people have others. Using tf-idf doesn't seem to make sense because I only have two documents, but correct me if I'm wrong.

Would appreciate recommendations or general thoughts.  Thanks.

",2,0
621,2016-3-18,2016,3,18,2,4au8m7,A simple char-rnn using tensorflow,https://www.reddit.com/r/MachineLearning/comments/4au8m7/a_simple_charrnn_using_tensorflow/,[deleted],1458236715,[deleted],0,2
622,2016-3-18,2016,3,18,2,4auanx,How could machine learning be applied to make human learning faster or more efficient?,https://www.reddit.com/r/MachineLearning/comments/4auanx/how_could_machine_learning_be_applied_to_make/,sleepicat,1458237479,"Machine learning, obviously, can be a tool to help humans find information faster or solve problems more quickly.  

But, in addition to using ML as a tool, what are we learning in the process of developing machine learning algorithms that could also be used to improve *how* people learn?

Edit: I wanted to leave the question open-ended, but to provide more context, concepts like memory, attention, supervised/unsupervised learning, error measurement, feedback are all concepts in ML that, not accidentally, mirror human cognition processes. So what are we learning from ML results that could help humans learn better, faster, etc.  What do we know from ML that could help that students with attention problems?  What do we know from ML that could help older people with fading memories?   Is it really better to make students learn by struggling through a math problem, or is a more supervised approach more efficient? When is learning through sound and video better than by reading or writing, or vice versa?",13,1
623,2016-3-18,2016,3,18,3,4aud2p,(Question) Definition of FCN (Fully convolutional networks),https://www.reddit.com/r/MachineLearning/comments/4aud2p/question_definition_of_fcn_fully_convolutional/,keidouleyoucee,1458238385,"Hi, I have a question about the definition of FCN - Fully Convolutional Networks. As far as I know, FCN consists of convolutional layers (and pooling layers) without any dense layers. However, if the size of features maps at the final convolutional layer is 1x1 (say it has 1024 channels), and if then I connected it to the output (dense) layer, actually (e.g. 100 output nodes) it's a dense connection of 1024x100. Can I still call it as a FCN? ",4,0
624,2016-3-18,2016,3,18,3,4audl2,Q: Is there any way of stacking and blending models in Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/4audl2/q_is_there_any_way_of_stacking_and_blending/,iDar89,1458238571,"There is a plenty techniques for Supervised Learning, but I could not find anything for RE.",2,5
625,2016-3-18,2016,3,18,3,4auhpz,Participate in a distributed evolutionary computation experiment,https://www.reddit.com/r/MachineLearning/comments/4auhpz/participate_in_a_distributed_evolutionary/,rhiever,1458240069,,0,0
626,2016-3-18,2016,3,18,3,4aui2p,Diversity and Data Science,https://www.reddit.com/r/MachineLearning/comments/4aui2p/diversity_and_data_science/,petesoder,1458240191,,0,0
627,2016-3-18,2016,3,18,3,4aujo4,[Question] What are good materials to study optimization methods? Visualizations would be a plus!,https://www.reddit.com/r/MachineLearning/comments/4aujo4/question_what_are_good_materials_to_study/,jqzhu,1458240809,,2,2
628,2016-3-18,2016,3,18,3,4auk0g,How is Microsoft Azure for Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/4auk0g/how_is_microsoft_azure_for_deep_learning/,code2hell,1458240939,I saw the instances available with Tesla K80 in DV2 instance [link] (http://www.hpcwire.com/2015/09/29/microsoft-puts-gpu-boosters-on-azure-cloud/). Has anyone used Azure for training a model? Im planning to use caffe. Would you recommend Windows instance or Linux instance provided that we can use remote desktop and that kind of saves some time and easy to monitor. Also they provide 200$ for the first month free. I really appreciate some suggestions and opinions! Thanks! ,3,0
629,2016-3-18,2016,3,18,3,4auko1,Amazon AWS EC2 best way to store data,https://www.reddit.com/r/MachineLearning/comments/4auko1/amazon_aws_ec2_best_way_to_store_data/,regularized,1458241192,"I use conv nets on amazon ec2 gpu daily on a dataset of 8gb. What is the best way to store the data? So far, I upload the 8gb data on a gpu instance and save it as AMI. Next day, I upload the ami, change some codes and run the convnet again. Is there more efficient way of doing this?",4,0
630,2016-3-18,2016,3,18,5,4auvri,Training Neural Network with two dimensional input,https://www.reddit.com/r/MachineLearning/comments/4auvri/training_neural_network_with_two_dimensional_input/,Schoolunch,1458245251,"How would you craft your hidden layer if you wanted to train a neural network with two dimensions?  For example, instead of passing the sentence alone to a recurrent network as a series of one hots, could you send each word as a one hot and the part of speech as a one hot attached to it?  Or do neural networks only allow for vectors as input?",2,0
631,2016-3-18,2016,3,18,5,4auwdt,I made a web based topological visualisation of a convolutional neural network,https://www.reddit.com/r/MachineLearning/comments/4auwdt/i_made_a_web_based_topological_visualisation_of_a/,[deleted],1458245474,[deleted],0,1
632,2016-3-18,2016,3,18,5,4auy7c,A interactive visualisation of a convolutional neural networks topology.,https://www.reddit.com/r/MachineLearning/comments/4auy7c/a_interactive_visualisation_of_a_convolutional/,[deleted],1458246155,[deleted],0,1
633,2016-3-18,2016,3,18,5,4av0yz,An interactive visualisation of a neural networks topology,https://www.reddit.com/r/MachineLearning/comments/4av0yz/an_interactive_visualisation_of_a_neural_networks/,[deleted],1458247186,[deleted],0,1
634,2016-3-18,2016,3,18,5,4av2rt,An interactive visualisation of a convolutional neural networks topology,https://www.reddit.com/r/MachineLearning/comments/4av2rt/an_interactive_visualisation_of_a_convolutional/,[deleted],1458247895,,4,21
635,2016-3-18,2016,3,18,5,4av43e,Looking for some feedback on my blog post: linear regression and attempting to predict stocks.,https://www.reddit.com/r/MachineLearning/comments/4av43e/looking_for_some_feedback_on_my_blog_post_linear/,Camnora,1458248395,,2,0
636,2016-3-18,2016,3,18,6,4avamr,A corpus of poetry or early childhood books?,https://www.reddit.com/r/MachineLearning/comments/4avamr/a_corpus_of_poetry_or_early_childhood_books/,Atradonna,1458250987,"I'd like to take a look at prosody (the rhythm of language, which is noticeable in some kids' literature, like Dr. Seuss, and non-freeform poetry) to make more convincing generated poetry. I've looked at Project Gutenberg's poetry, but I'd also like to use a word bank more recent than the 19th century. Does one exist?",2,0
637,2016-3-18,2016,3,18,7,4avf54,"""Were still a long way from a machine that can learn to flexibly perform the full range of intellectual tasks a human can"" - Demis Hassabis' blog post on AlphaGo's victory",https://www.reddit.com/r/MachineLearning/comments/4avf54/were_still_a_long_way_from_a_machine_that_can/,jakn,1458252812,,7,27
638,2016-3-18,2016,3,18,10,4aw3i0,How do RNN's handle providing output with different dimension than input,https://www.reddit.com/r/MachineLearning/comments/4aw3i0/how_do_rnns_handle_providing_output_with/,Schoolunch,1458262977,"It seems like an RNN has to have the ht-1 needs to be the same size as the input vector since they're being added to one another, but if you're doing something like modeling to another language or classification, how would you handle this? I'd imagine you'd just have a fully connected layer that would predict the output, but I was wondering if there is a standard convention?

Basically, I don't understand how the encoder is mapping a RNN sequence to a different size vector than the input since it's feeding the output back into itself every time.



[edit]
Perhaps I'm not being clear.  I'm curious where the dimensional transformation is happening mathematically, in an example like an LSTM for translation where there is an encoder-decoder.  I am under the impression they project the phrase to an 8000x1 dimensional vector, but if you look at an example like this: http://image.slidesharecdn.com/l07nnrnngrulstm-151108140716-lva1-app6892/95/recurrent-neural-networks-lstm-and-gru-33-638.jpg?cb=1446992496
I don't see where an LSTM would be able to transform the vector to a different dimension.  That's why I'm assuming there is a feedforward layer on top of the LSTM because if you look at h(t-1), it has to be fed back into the LSTM with the same dimensional size as x(t) so they can be added together.  So if the LSTM was outputting something of different size than the input vector, they wouldn't be able to be added.  Is that more clear?",14,0
639,2016-3-18,2016,3,18,13,4awt3w,Do neural networks know logic gates?,https://www.reddit.com/r/MachineLearning/comments/4awt3w/do_neural_networks_know_logic_gates/,yield22,1458274747,"I am trying to understand whether NN can do and/or/xor in general. Suppose the input x = (x_1, x_2) is a 2d real vector, and NN is asked to produce y = (x_1&gt;a) OP (x_2&gt;b), here OP is one of the logic gates. From experiments I found that the trained NN can do well for test samples that are not too ""different"" from training samples (assuming x_1, x_2 are sampled uniformly from [-10, 10]), but once that's not met (e.g. x_1 = 10, x2 = 10000), then it is possible the trained NN will produce wrong results. Since it is not possible to generate training samples with arbitrary large values, but the trained NN should generalize to arbitrary large values that it haven't seen before. So I wonder is there a proof or theory to show NN can handle logic gates like above for unconstrained real inputs?",25,2
640,2016-3-18,2016,3,18,13,4awtc1,Here are a publicly open &amp; hand picked 15 ebooks in artificial intelligence.,https://www.reddit.com/r/MachineLearning/comments/4awtc1/here_are_a_publicly_open_hand_picked_15_ebooks_in/,[deleted],1458274869,[deleted],0,1
641,2016-3-18,2016,3,18,13,4awucf,"Is the source code for the papers ""Delving Deeper into Convolutional Networks for Learning Video Representations"" available?",https://www.reddit.com/r/MachineLearning/comments/4awucf/is_the_source_code_for_the_papers_delving_deeper/,[deleted],1458275373,[deleted],0,1
642,2016-3-18,2016,3,18,13,4awul8,15 free ebooks on artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/4awul8/15_free_ebooks_on_artificial_intelligence/,snakenaf,1458275505,,0,2
643,2016-3-18,2016,3,18,13,4awvo3,"Is the source code for the paper ""Delving Deeper into Convolutional Networks for Learning Video Representations"" available?",https://www.reddit.com/r/MachineLearning/comments/4awvo3/is_the_source_code_for_the_paper_delving_deeper/,godofprobability,1458276119,I am really inspired by this [paper](http://arxiv.org/abs/1511.06432) and would like to look into the source code.,5,2
644,2016-3-18,2016,3,18,13,4awwd7,"Can you add back-propagation to q-learning, or does that defeat the purpose?",https://www.reddit.com/r/MachineLearning/comments/4awwd7/can_you_add_backpropagation_to_qlearning_or_does/,2_bit_encryption,1458276523,"My understanding is that q-learning does not do explicit back-propagation.

That is, when you reach a state S, you then look ahead to find its best possible next state S', and update you the value of Q[S, a] to reflect what you saw at S'.

But you do *not* explicitly back propagate all the way to the initial starting state estimate.  Example:

State 34 has Q-value of 5.  You look at every state you can get to from State 34, and see that the one with the highest Q-Value has a value of 6.  So using your *alpha* value and your *discount* value you update State 34 to have a Q-value of, say, 5.7.

HOWEVER.  You do NOT back-propagate those values back from State 34 to State 33 to State 32 ... to State 0.  Instead, you simply let future simulations reach State 33, which expands to State 34, then updates its own value.  So to successfully propagate that information all the way back to State 0, you need to run at least 34 simulations/iterations, and each time that information gets one state ""closer"" to State 0.

Hopefully this makes sense so far (cause I'm new to this so I might be mistaken).  But say you are trying to apply this to the state-space of Reversi (Othello).  That's a lot of states.

Wouldn't it make more sense to back-propagate *immediately and explicetly?*  That is, the moment State 34 updates its Q-value estimate based off what it sees in State 35, you push recursively push those updates back to State 33, 32, 31 ... 0.

This takes more work, but at least the information you learned way up in State 35 propagates back to the beginning, without fail?  Iterations may take longer, but their information is always useful?

My fear is that if I do Q-learning the ""right"" way, which apparently doesn't back-propagate explicitly, I might reach states very far into a game on one iteration, but then never happen to visit those states again in future iterations.  So their values are essentially lost in space, since we never ""attach"" to them to push them back.

Hope all that made sense.  I might just be totally confused.  Please let me know!  Thanks!",6,5
645,2016-3-18,2016,3,18,15,4ax5l8,Architecting a Machine Learning for Risk at Airbnb,https://www.reddit.com/r/MachineLearning/comments/4ax5l8/architecting_a_machine_learning_for_risk_at_airbnb/,sko2sko,1458282206,,0,1
646,2016-3-18,2016,3,18,16,4axchm,Question about Q learning,https://www.reddit.com/r/MachineLearning/comments/4axchm/question_about_q_learning/,seilgu,1458287069,"I read that for any state-action pair (s, a), you can assume that a score function Q(s, a) exists, and initialize it randomly, then update with Q(s, a) &lt;- Q(s, a) + eta * ( r + gamma * max_(a') Q(s', a')  - Q(s, a) ).

and we randomly select (s, a) and repeat the update until converge. 

But this doesn't make much sense. The update formula doesn't involve the mechanism of the game except for the immediate reward r, and suppose we're playing a game where immediate reward doesn't matter (for example, Breakout), then it's simple to see the algorithm doesn't make sense, because we're mostly doing random walk on the Q(s, a)'s 99% of the time.

The only possible way it could work is for the really big reward to back-propagate to the states leading to it, so actually to make the algorithm efficient, the updates should start with the events with high reward (for example, ball hitting a brick in Breakout).

Am I being correct here that the algorithm is mostly doing useless stuff and we should prioritize high-reward states first? ",10,1
647,2016-3-18,2016,3,18,17,4axho0,Good pretrained convNet for image recognition?,https://www.reddit.com/r/MachineLearning/comments/4axho0/good_pretrained_convnet_for_image_recognition/,swentso,1458291407,"Does anyone have a link to a good pretrained model for imagenet recognition task (for example) that is easily imported in theano?

I am trying to apply deep learning to CBIR task and I don't want to have to train a bunch of conv layers.",3,1
648,2016-3-18,2016,3,18,18,4axk7e,Deep Spreadsheets with ExcelNet (Digit Recognition),https://www.reddit.com/r/MachineLearning/comments/4axk7e/deep_spreadsheets_with_excelnet_digit_recognition/,pmigdal,1458293397,,33,116
649,2016-3-18,2016,3,18,18,4axm68,Deep Q-Learning (Space Invaders),https://www.reddit.com/r/MachineLearning/comments/4axm68/deep_qlearning_space_invaders/,pmigdal,1458294819,,0,3
650,2016-3-18,2016,3,18,19,4axotw,What happens with strides if the filter in my CNN has the height of the input?,https://www.reddit.com/r/MachineLearning/comments/4axotw/what_happens_with_strides_if_the_filter_in_my_cnn/,xristos_forokolomvos,1458296719,"More specifically, I have an input of 10x20000 and want to slide over the second axis filters of height 10. Will the height stride affect how those filters applied? 

Intuitively it shouldn't make any difference, right? Should I set it to 1?",3,1
651,2016-3-18,2016,3,18,19,4axpy3,Neural Networks Demystified (videos),https://www.reddit.com/r/MachineLearning/comments/4axpy3/neural_networks_demystified_videos/,pmigdal,1458297506,,2,23
652,2016-3-18,2016,3,18,19,4axqpw,[advice] Need to implement Newton's method optimization for a MLP in Python. What's the *easiest* way?,https://www.reddit.com/r/MachineLearning/comments/4axqpw/advice_need_to_implement_newtons_method/,fiala__,1458298042,"Hi all, I made the mistake of taking an ancient course in Neural Networks with a curriculum stuck somewhere in the 90s. I have a coursework assignment to implement Newton's method of MLP training and compare it to standard first-order training. I need to implement Newton's method with both approximate and exact Hessian, and also compare the performance of the two.

I want to do this in Python. I'm aware that nobody really uses this training method anymore, so I'm not worried about ""learning it properly"". Also, I have other, more important work to do, and I have almost no experience implementing more advanced neural nets outside of the comfort of Theano/Keras and tensorflow. So I'm willing to use a library or two to speed up the process, even though I'm supposed to hand-code everything. However, I can't use anything too high-level, as the lecturer would probably kick me straight out of the course.

**TL;DR from here:** I have until Tuesday to implement Newton's method with approx Hessian &amp; exact Hessian. What's the *absolutely easiest way* to do this in Python, potentially using a framework but not something that makes it a two-line script?

P.S. any code examples for inspiration are welcome.

Thanks a lot!

",9,2
653,2016-3-18,2016,3,18,20,4axx1m,Have Neural Networks been used in tandem with Online methods such as Particle Filter or Evolutionary Algorithms?,https://www.reddit.com/r/MachineLearning/comments/4axx1m/have_neural_networks_been_used_in_tandem_with/,soulslicer0,1458302087,Say for tracking purposes etc.,4,1
654,2016-3-18,2016,3,18,21,4axxmt,They told us Deep Learning would solve important problems. Now it's solved FlappyBird.,https://www.reddit.com/r/MachineLearning/comments/4axxmt/they_told_us_deep_learning_would_solve_important/,rhiever,1458302422,,27,57
655,2016-3-18,2016,3,18,22,4ay7aa,Free Resources to Learn Machine Learning for Trading,https://www.reddit.com/r/MachineLearning/comments/4ay7aa/free_resources_to_learn_machine_learning_for/,kaushikqi,1458306488,,0,4
656,2016-3-19,2016,3,19,0,4aypds,Good pre-trained model for image segmentation?,https://www.reddit.com/r/MachineLearning/comments/4aypds/good_pretrained_model_for_image_segmentation/,code2hell,1458313986,Im looking for a good model for image segmentation in torch. Planning to use the pre-trained model so that I can classify 10 classes. I did find a few models like [This one](http://www.robots.ox.ac.uk/~szheng/CRFasRNN.html) Is there something similar in torch? I really haven't tried caffe yet. Would really appreciate suggestions.,1,0
657,2016-3-19,2016,3,19,0,4ayuy5,[advice] Need to autocomplete 3d models from training data.,https://www.reddit.com/r/MachineLearning/comments/4ayuy5/advice_need_to_autocomplete_3d_models_from/,phil_3333,1458315811,"Hi,

I am currently trying so solve missing parts in 3d models coming from a scanner. I have training data of complete models to do this, but not a lot. Initially I thought about using neural networks, but since i don't have that much data, I am not sure. Can anyone recommend an approach, or relevant research I should look at? 
I am grateful for any help.",4,0
658,2016-3-19,2016,3,19,0,4ayvq8,Association Rule Learning?,https://www.reddit.com/r/MachineLearning/comments/4ayvq8/association_rule_learning/,Ibarea,1458316092,"Is anyone up on this literature? What are the best books/review articles/papers?  Is this area of research worth investing time to understand (i.e. is it a useful approach to derive meaning from data, or has it been supplanted by other approaches to unsupervised learning)? Is this just another way of talking about a certain class of probabilistic graphical models/when would you use ARL vs probabilistic graphical models?",10,7
659,2016-3-19,2016,3,19,1,4az6b3,Microsoft Uses Minecraft for AI Research,https://www.reddit.com/r/MachineLearning/comments/4az6b3/microsoft_uses_minecraft_for_ai_research/,evc123,1458320113,,4,11
660,2016-3-19,2016,3,19,1,4az6us,Inferring Fine-grained Details on User Activities and Home Location from Social Media: Detecting Drinking-While-Tweeting Patterns in Communities,https://www.reddit.com/r/MachineLearning/comments/4az6us/inferring_finegrained_details_on_user_activities/,colditzjb,1458320327,,0,2
661,2016-3-19,2016,3,19,2,4az8mo,Composite heuristic for the 8-puzzle,https://www.reddit.com/r/MachineLearning/comments/4az8mo/composite_heuristic_for_the_8puzzle/,utxeee,1458320993,"While reading Artificial Intelligence a Modern Approach I came across the concept of deriving a heuristic from the solution cost of a sub-problem of a given problem.

For example, the following puzzles shows a sub problem of the 8-puzzle instance where the goal is to place tiles 1, 2, 3, 4 into their correct positions.

    Start State = [ * 2 4 ]    Goal State = [   1 2 ]                      
                  [ *   * ]                 [ 3 4 * ]
                  [ * 3 1 ]                 [ * * * ]  
Then, the author expand this concept by saying that these heuristics derived from the sub-problems can be combined by taking the maximum value.

    h(n)= max{ h^1(n), . . , h^m(n) }

Moreover, by using this approach the performance is improved greatly when compared to a simple heuristic like [Manhattan distance](https://en.wiktionary.org/wiki/Manhattan_distance).

I have been trying to wrap my head around the composite heuristics and the reasoning behing them. Let's say we have two heuristics derived from the solution cost of the following sub-problems:

    h^1234(n) = [   1 2 ]     h^5678(n) = [   * * ]                    
                [ 3 4 * ]                 [ * * 5 ]
                [ * * * ]                 [ 6 7 8 ]
Will a composite heuristic like:

    h^1...8(n)= max{ h^1234(n), h^5678(n) }


* Really work in finding a solution for the complete 8-puzzle problem?

It seems to me that using a heuristic like h^1...8 (n) we will end up alternating between heuristics h^1234 (n) and h^5678 (n) which in turn might lead to one heuristic messing up the work of the other and never reaching a solution.

* Or will the heuristics help each other towards the complete solution?

Honestly, I do no see how this could work...",13,2
662,2016-3-19,2016,3,19,2,4aze72,"AlphaGo/ Sedol 4-1 postmatch wrapup/ commentary/ analysis, topnotch links",https://www.reddit.com/r/MachineLearning/comments/4aze72/alphago_sedol_41_postmatch_wrapup_commentary/,vznvzn,1458323175,,0,0
663,2016-3-19,2016,3,19,3,4azhgw,Anyone worked on the IBM Watson team? What is the hiring process like?,https://www.reddit.com/r/MachineLearning/comments/4azhgw/anyone_worked_on_the_ibm_watson_team_what_is_the/,zcleghern,1458324459,I have applied for an internship on the IBM Watson team and recently (&lt; 3 days ago) completed the online coding challenge. Has anyone here been through the process? How long does it take? What are the next steps? I am very interested to hear about your experiences.,8,2
664,2016-3-19,2016,3,19,3,4azitb,PsychoBot,https://www.reddit.com/r/MachineLearning/comments/4azitb/psychobot/,really_what_is_this,1458324967,,2,5
665,2016-3-19,2016,3,19,3,4aznel,I built an open source site to share &amp; discover ml datasets. What do you think?,https://www.reddit.com/r/MachineLearning/comments/4aznel/i_built_an_open_source_site_to_share_discover_ml/,mrborgen86,1458326754,,8,27
666,2016-3-19,2016,3,19,3,4azp06,Instance-aware segmentation in video?,https://www.reddit.com/r/MachineLearning/comments/4azp06/instanceaware_segmentation_in_video/,CNNsforEM,1458327406,[removed],0,1
667,2016-3-19,2016,3,19,5,4b01ja,Can someone explain how AlphaGo used gradient in its reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/4b01ja/can_someone_explain_how_alphago_used_gradient_in/,tim1357,1458332356,"I'm having trouble understanding how AlphaGo computed the gradient during the reinforcement learning stage of its training. [Here](http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf) is the AlphaGo paper. The part in particular that I'm having trouble understanding is on page 485. 
([Image](http://i.imgur.com/BcYpmmw.png)) How is dz/d\rho is computed here. Does it rely on the fact that we know the old \rho too? 

",2,3
668,2016-3-19,2016,3,19,6,4b09jp,Fixed Gabor filters in first layer of conv. nets,https://www.reddit.com/r/MachineLearning/comments/4b09jp/fixed_gabor_filters_in_first_layer_of_conv_nets/,joekr07,1458335608,[removed],0,1
669,2016-3-19,2016,3,19,6,4b0aqh,South Korea trumpets $860-million AI fund after AlphaGo 'shock',https://www.reddit.com/r/MachineLearning/comments/4b0aqh/south_korea_trumpets_860million_ai_fund_after/,pica_foices,1458336080,,20,33
670,2016-3-19,2016,3,19,6,4b0bu0,Regular Gabor filters in first layer of conv net,https://www.reddit.com/r/MachineLearning/comments/4b0bu0/regular_gabor_filters_in_first_layer_of_conv_net/,joekr07,1458336536,"Why not just fix the first layer of filters in a conv net to regular Gabor type filters directly. That would arguably reguarlize and reduce training time a bit?
",10,6
671,2016-3-19,2016,3,19,6,4b0cff,Face2Face: Real-time Face Capture and Reenactment of RGB Videos (CVPR 2016 Oral),https://www.reddit.com/r/MachineLearning/comments/4b0cff/face2face_realtime_face_capture_and_reenactment/,pmigdal,1458336794,,59,431
672,2016-3-19,2016,3,19,7,4b0h16,Want to break out of my rut of small datasets and python. Project Ideas?,https://www.reddit.com/r/MachineLearning/comments/4b0h16/want_to_break_out_of_my_rut_of_small_datasets_and/,wonkypedia,1458338768,"i have worked for a few years in the industry, and i find i've ended up working on small datasets, using python, sklearn and pandas, and pretty much just used sklearn classifiers as a black box. 

i want to break out of this rut. some of my goals are:

* learn to use hadoop and spark, and other big data tools 

* explore deep learning, starting with text data 

* ship an interesting product, which can be spun into an app. 

* i've bought a raspberry pi, and i've gotten some stuff running on it... i'd like to preferably make something that would make sense to run on a pi. 

What are some ideas for projects that touch on some of these goals? I'd ideally like something that takes less than a month for a first iteration. ",6,7
673,2016-3-19,2016,3,19,7,4b0i1g,Help with understanding Dropout,https://www.reddit.com/r/MachineLearning/comments/4b0i1g/help_with_understanding_dropout/,clojureyourmouth,1458339194,"I am having some difficulty understanding the concept of dropout. I initially thought it just meant randomly dropping out units during learning, but as I study it more it seems that is incorrect. I know that it takes the geometric mean of multiple neural networks, but I don't understand what neural networks are being averaged. Does this happen after it is completely trained? I have been going through this video https://youtu.be/G3KUvHx9GDY?t=2m46s but I don't get what he is talking about from this point on. ",3,1
674,2016-3-19,2016,3,19,8,4b0rye,Software for Labeling Reddit Comments : datasets,https://www.reddit.com/r/MachineLearning/comments/4b0rye/software_for_labeling_reddit_comments_datasets/,iktof,1458343542,,0,4
675,2016-3-19,2016,3,19,9,4b0ynf,"Crosspost: AMA w/ Redmond 9p, Kim 9p, Hajin Lee 3p, Garlock, Okun &amp; Jackson is GO for this evening! 6pm PDT 9pm EDT on r/iama!  /r/baduk",https://www.reddit.com/r/MachineLearning/comments/4b0ynf/crosspost_ama_w_redmond_9p_kim_9p_hajin_lee_3p/,[deleted],1458346619,[deleted],0,7
676,2016-3-19,2016,3,19,10,4b18yo,"Keras plays catch, a single file Reinforcement Learning example",https://www.reddit.com/r/MachineLearning/comments/4b18yo/keras_plays_catch_a_single_file_reinforcement/,edersantana,1458351575,,4,24
677,2016-3-19,2016,3,19,11,4b1cmp,Question about using predictions to generate new training target data,https://www.reddit.com/r/MachineLearning/comments/4b1cmp/question_about_using_predictions_to_generate_new/,bluemania,1458353319,"Hi, I'm analysing text data into a simple yes/no category. My training set is around 3k and produces some reasonable F1scores/accuracy (F1 ~70%). The model appears to produce a probability from 0-1. I am thinking about manually tagging some more data for use in another round of training, if the probability of predictions is around 0.45-0.55. My intuition tells me this is the area where the model has the most uncertainty in allocating to a category (being close 0 or 1).

Is this unwise? Am I potentially causing some bias I am not aware of? Will doing this have any noticeable impact on improving model performance?",2,2
678,2016-3-19,2016,3,19,11,4b1ivk,Predicting output for unseen input and ambiguous input,https://www.reddit.com/r/MachineLearning/comments/4b1ivk/predicting_output_for_unseen_input_and_ambiguous/,funny_penis,1458356287,"Basically, supervised machine learning classification boils down to predicting an output for a test input, given training data with input-output mappings. Let's deal with 2 simple examples.

1-Unseen input  
training data:

&gt; input output  
0 1

testing data:
&gt;input output  
0 ?  
1 ?


If input is 0, simple - we saw the same input in training, so predict same output - 1  
If input is 1, we did not see that input in training, so what should we predict ?  
1 - because it is the only valid output we saw in training,  
0 - because if input changed, output 'should' also change,  
0/1 - coin flip (offload the responsibility to a random function)  
0/1 - depending on success criteria and favoring one of the outputs when guessing (false-positive/negative etc.)  
?



2-Ambiguous input  
training data:
&gt;input output  
0 0  
0 1

testing data:  
&gt;input output  
0 ?

input is 0, which we saw in training, but it had 2 outputs, so what should we predict ?  
0 - because it is a valid output we saw in training,  
1 - because it is a valid output we saw in training,  
0/1 - coin-flip (offload the responsibility to a random function)  
?",5,1
679,2016-3-19,2016,3,19,14,4b1x92,We are 6 go professionals and organizers who were involved with the match in Korea last week between Google's AlphaGo AI and Lee Sedol 9p -- Ask us anything! : IAmA,https://www.reddit.com/r/MachineLearning/comments/4b1x92/we_are_6_go_professionals_and_organizers_who_were/,[deleted],1458364278,[deleted],0,1
680,2016-3-19,2016,3,19,15,4b232c,"Hand &amp; Machine Tools product suppliers, Hand &amp; Machine Tools b2b suppliers directory |BizBilla.com",https://www.reddit.com/r/MachineLearning/comments/4b232c/hand_machine_tools_product_suppliers_hand_machine/,Steffy_Berlin,1458368078,,0,1
681,2016-3-19,2016,3,19,17,4b2cs6,python confusion trying to ensemble,https://www.reddit.com/r/MachineLearning/comments/4b2cs6/python_confusion_trying_to_ensemble/,maximus12793,1458375185,"I am a ML beginner and am having problems reaping any benefits from 3 separate classifiers combined. I have their respective f1 scores and have tried to implement a basic voting (with each contributing the weight of their f1 score and majority will decide). However since 2 of the classifiers are weaker I end up just an f1 average of sorts and they drag down the overall performance. This is also tricky since I cannot just throw the data into some new fit since they are all feature extracts from big text (well I am sure it IS simple, just out of my understanding atm)... Any ideas?  

**note:** I am pretty sure my voting thing makes no sense, but my goal is to perform any sort of stacking, so am open to anything. Thanks!",7,0
682,2016-3-19,2016,3,19,17,4b2cve,"Geoffrey Hinton, the 'godfather' of deep learning, on AlphaGo",https://www.reddit.com/r/MachineLearning/comments/4b2cve/geoffrey_hinton_the_godfather_of_deep_learning_on/,clbam8,1458375279,,30,11
683,2016-3-19,2016,3,19,17,4b2fa3,"I am trying to get a tensorfllow model onto android, is there any good tutorial or something similar about this?",https://www.reddit.com/r/MachineLearning/comments/4b2fa3/i_am_trying_to_get_a_tensorfllow_model_onto/,Enum1,1458377280,,3,2
684,2016-3-19,2016,3,19,20,4b2ugc,Best reinforcement learning libraries?,https://www.reddit.com/r/MachineLearning/comments/4b2ugc/best_reinforcement_learning_libraries/,[deleted],1458388771,[deleted],7,4
685,2016-3-19,2016,3,19,21,4b2xck,Datumbox Machine Learning Framework 0.7.0 Released,https://www.reddit.com/r/MachineLearning/comments/4b2xck/datumbox_machine_learning_framework_070_released/,datumbox,1458390583,,2,5
686,2016-3-19,2016,3,19,22,4b33wa,Good sources for text classification,https://www.reddit.com/r/MachineLearning/comments/4b33wa/good_sources_for_text_classification/,muftard,1458394564,"Hello, I'm trying to have a better understanding of the field of text information retrieval. 

I would like to know what's the current state in that field (most used methods, best practices, etc.). Any good sources that I can consult?

I've tried google but I keep coming across ""old"" (2012) papers.

Thank you.",4,2
687,2016-3-19,2016,3,19,23,4b36rk,Theano Tutorial,https://www.reddit.com/r/MachineLearning/comments/4b36rk/theano_tutorial/,pmigdal,1458396186,,23,98
688,2016-3-20,2016,3,20,0,4b3jhv,Scikit-learn Tutorial - PyCon NYC 2015,https://www.reddit.com/r/MachineLearning/comments/4b3jhv/scikitlearn_tutorial_pycon_nyc_2015/,pmigdal,1458402264,,0,33
689,2016-3-20,2016,3,20,2,4b3vz8,I want to make a neural network that learns how to play my favorite video game. Where do I get started?,https://www.reddit.com/r/MachineLearning/comments/4b3vz8/i_want_to_make_a_neural_network_that_learns_how/,ItsGnomeChomsky,1458407723,"I've had this idea for a while, but whenever I want to get started, I'm blocked because I don't know how to feed computer screen pixels as input, or controller gestures as output. Any help would be useful, thanks :)",16,1
690,2016-3-20,2016,3,20,2,4b3wxg,Deep Learning + Trial and Error Robot Picker,https://www.reddit.com/r/MachineLearning/comments/4b3wxg/deep_learning_trial_and_error_robot_picker/,GoldenQuarter,1458408148,,0,1
691,2016-3-20,2016,3,20,3,4b44fa,"If my target variable for a regression problem has a low variance across all training and test samples, can I count on the high prediction accuracy achieved by support vector regression?",https://www.reddit.com/r/MachineLearning/comments/4b44fa/if_my_target_variable_for_a_regression_problem/,colonelkurtz_ak,1458411318,[removed],1,1
692,2016-3-20,2016,3,20,3,4b44zv,"Understanding how CNNs handle invariance in image recognition: translation, size (scale), rotation",https://www.reddit.com/r/MachineLearning/comments/4b44zv/understanding_how_cnns_handle_invariance_in_image/,to4life,1458411585,"Hello r/MachineLearning, 

I am trying to improve my theoretical understanding of convolutional neural networks. My question is how using CNNs for image recognition are able to handle the following 3 invariances in images: translation, scale, and rotation. 

Intuitive explanations will do, but mathematical arguments are fine too. 

Cheers",6,12
693,2016-3-20,2016,3,20,3,4b48sn,Is there any conclusive work on whether or not fully convolutional networks superior?,https://www.reddit.com/r/MachineLearning/comments/4b48sn/is_there_any_conclusive_work_on_whether_or_not/,[deleted],1458413172,[deleted],0,1
694,2016-3-20,2016,3,20,4,4b4axw,s there any conclusive work on whether or not fully convolutional networks are superior?,https://www.reddit.com/r/MachineLearning/comments/4b4axw/s_there_any_conclusive_work_on_whether_or_not/,[deleted],1458414086,[deleted],0,1
695,2016-3-20,2016,3,20,4,4b4cg3,Is there any conclusive work on whether or not fully convolutional networks are superior?,https://www.reddit.com/r/MachineLearning/comments/4b4cg3/is_there_any_conclusive_work_on_whether_or_not/,[deleted],1458414710,[deleted],5,12
696,2016-3-20,2016,3,20,4,4b4iop,Quick question: how are convnet-filters applied on previous layers?,https://www.reddit.com/r/MachineLearning/comments/4b4iop/quick_question_how_are_convnetfilters_applied_on/,[deleted],1458417402,[deleted],3,0
697,2016-3-20,2016,3,20,5,4b4q2p,Is deep learning better at reinforcement learning (what DeepMind with Atari and Go) than evolutionary algorithms are?,https://www.reddit.com/r/MachineLearning/comments/4b4q2p/is_deep_learning_better_at_reinforcement_learning/,[deleted],1458420632,[deleted],0,1
698,2016-3-20,2016,3,20,7,4b56lx,"I made a classifier for reddit text posts to predict number of comments, its almost 80 % accurate",https://www.reddit.com/r/MachineLearning/comments/4b56lx/i_made_a_classifier_for_reddit_text_posts_to/,inferrExt,1458427970,"I'd be very curious to hear any feedback or recommendations! it's still very very rough

https://chrome.google.com/webstore/detail/inferr/djphoiaiilgngmldjdbnnjjemaddlfhe/related",19,1
699,2016-3-20,2016,3,20,9,4b5fy1,Real-time CNNs on CPUs with XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4b5fy1/realtime_cnns_on_cpus_with_xnornet_imagenet/,mere_mortise,1458432117,,15,92
700,2016-3-20,2016,3,20,9,4b5h7i,Package for general recurrent neural networks?,https://www.reddit.com/r/MachineLearning/comments/4b5h7i/package_for_general_recurrent_neural_networks/,ihaphleas,1458432670,I'm looking for a package (preferably Python) that allows the creation of a neural network of any topology by passing a (sparse) connection matrix -- along with a few other parameters ... would also like to have Hebbian learning in the package. Any such package in Python?,7,1
701,2016-3-20,2016,3,20,9,4b5ich,Human Learning,https://www.reddit.com/r/MachineLearning/comments/4b5ich/human_learning/,Tiramisuu2,1458433207,"Are there any good libraries for developing networks that train humans at tasks?   I'm trying to think of how you would define/features, where you would get an adequate test set?  Maybe khan or mooc publishes there test results?  and whether there are python/R/Matlab libraries that might support this?",3,0
702,2016-3-20,2016,3,20,9,4b5kj7,Using RL to train MLPs?,https://www.reddit.com/r/MachineLearning/comments/4b5kj7/using_rl_to_train_mlps/,[deleted],1458434214,[deleted],2,1
703,2016-3-20,2016,3,20,17,4b6wp1,SMT factory Laser LCD repair machine for TV LCD repair solutions,https://www.reddit.com/r/MachineLearning/comments/4b6wp1/smt_factory_laser_lcd_repair_machine_for_tv_lcd/,SMTsolutionlily,1458463821,,1,1
704,2016-3-20,2016,3,20,19,4b72gj,How to create the bounding boxes in an image?,https://www.reddit.com/r/MachineLearning/comments/4b72gj/how_to_create_the_bounding_boxes_in_an_image/,awhitesong,1458469793,"Considering i have images with localized sections like whale faces (say similar to right whale recognition kaggle dataset) and i want to remove the background water (this is just an example and not representative of the actual dataset) and just extract the whale faces. I would like to create the bounding boxes over the whale faces and crop those. The blog of the runner up of right whale recognition had the coordinates x, y, width and height for the bounding boxes which he used later to train a simple classifier(i'm not totally sure of the procedure) and generate bounding boxes on test images. What if i don't have these coordinates for my dataset. Do i have to make it manually and then create bounding boxes? If yes, how should i train a classifier using these coordinates so that i have bounding boxes for my test images as well? Is there any better way to generate bounding boxes or tackle localization?

EDIT: They have mentioned in a [post](http://blog.kaggle.com/2016/01/29/noaa-right-whale-recognition-winners-interview-1st-place-deepsense-io/#attachment_5472) the use of the classifier. *""In the first, we train a neural net to output coordinates of the bounding box of a whales head (as seen in the above figure #2).""* If bounding boxes are made manually then why the use of classifier is mentioned, are coordinates used to learn bounding boxes for test images as well ?",17,0
705,2016-3-20,2016,3,20,19,4b73by,[Q] Transforming a distribution into a unit gaussian?,https://www.reddit.com/r/MachineLearning/comments/4b73by/q_transforming_a_distribution_into_a_unit_gaussian/,TamisAchilles,1458470625,"Let's say we can draw infinite amounts i.i.d. samples x from some unknown stationary unimodal distribution X. Now consider a parametric function approximator f(x|theta) with parameters theta. The goal is to find the parameters theta for f such that the transformed distribution becomes unit gaussian i.e. f(x~X|theta) ~ N(0,1) 

My question is what kind of loss function could we use and why to learn the parameters theta of the function approximator f?",8,6
706,2016-3-20,2016,3,20,21,4b7dvx,Neural Variational Inference for Text Processing (paper+code),https://www.reddit.com/r/MachineLearning/comments/4b7dvx/neural_variational_inference_for_text_processing/,samim23,1458478577,,7,43
707,2016-3-20,2016,3,20,22,4b7epl,The site for quick s...x dating 5BEr6Z9n,https://www.reddit.com/r/MachineLearning/comments/4b7epl/the_site_for_quick_sx_dating_5ber6z9n/,j53gc68ez79n,1458479029,,0,1
708,2016-3-20,2016,3,20,22,4b7fre,"Pass any URL, get its content summarized and output as JSON",https://www.reddit.com/r/MachineLearning/comments/4b7fre/pass_any_url_get_its_content_summarized_and/,[deleted],1458479527,[deleted],0,1
709,2016-3-20,2016,3,20,22,4b7gyu,Deep Learning project that generates a poetic description of an image,https://www.reddit.com/r/MachineLearning/comments/4b7gyu/deep_learning_project_that_generates_a_poetic/,anantzoid,1458480070,"Hi,
I saw a project on Github posted in December that generates image descriptions in the form of poems.
I want to study it deeper now but can't find it. Can anyone post the link if they know it?
Thanks in advance.",3,19
710,2016-3-20,2016,3,20,23,4b7n7u,How To Become A Machine Learning Expert In One Simple Step,https://www.reddit.com/r/MachineLearning/comments/4b7n7u/how_to_become_a_machine_learning_expert_in_one/,swanint,1458483002,,4,0
711,2016-3-21,2016,3,21,0,4b7ux7,"Summary of paper titled ""Efficient Estimation of Word Representations in Vector Space""",https://www.reddit.com/r/MachineLearning/comments/4b7ux7/summary_of_paper_titled_efficient_estimation_of/,shagunsodhani,1458486368,,0,5
712,2016-3-21,2016,3,21,0,4b803p,Hashing trick + collsions questions,https://www.reddit.com/r/MachineLearning/comments/4b803p/hashing_trick_collsions_questions/,Setheton,1458488749,"I'm implementing feature hashing, and have some questions pertaining to collisions.

 1. First I want this question clarified : Is it true that the trouble with hashing is that values from different feature spaces could be mapped to the same bin ? ( and not necessarily values from the same feature space mapped to the same bin ).
    
2.some claim that using a different hash function on each feature helps to minimize collsion. Is there any intuitive explanation to this ? I would assume that a single hash function which maps uniformly is no diferent in comparison to several hash functions.
    
3.In a case where a collision does occur - let's say the features are mapped to this vector where in the first index there is a collision:
    (2,0,0,1,0)
    I should I proceed in training my classifier ? just use this vector as is ? maybe discard it ?
    
4.Some uses a second hash function which maps to 1 to indicate whether to add or to subtract. let's say the vector is:
    (1,0,0,-1)
    again, should I use this vector as is in the traing set, or should I perform abs() on all values before ?

",6,6
713,2016-3-21,2016,3,21,0,4b81ym,Question about Active SVMs and Version Space,https://www.reddit.com/r/MachineLearning/comments/4b81ym/question_about_active_svms_and_version_space/,buakaw9,1458489571,"Hi Reddit,

Why is it that active SVMs cause sampling bias? Then do all active learning algorithms to a certain extent deal with sampling bias? Also can someone explain version space/hypothesis spaces to me in lay man terms? 

Bit new to ML! 

",0,2
714,2016-3-21,2016,3,21,1,4b82ju,Freed Go - The Game of Go in 3D,https://www.reddit.com/r/MachineLearning/comments/4b82ju/freed_go_the_game_of_go_in_3d/,soulslicer0,1458489834,,1,0
715,2016-3-21,2016,3,21,3,4b8j9w,"word2vec, LDA, and introducing a new hybrid algorithm: lda2vec (slides)",https://www.reddit.com/r/MachineLearning/comments/4b8j9w/word2vec_lda_and_introducing_a_new_hybrid/,pmigdal,1458496953,,0,1
716,2016-3-21,2016,3,21,9,4b9zoy,Rich Sutton's comment on AlphaGo victory,https://www.reddit.com/r/MachineLearning/comments/4b9zoy/rich_suttons_comment_on_alphago_victory/,clbam8,1458518663,,3,9
717,2016-3-21,2016,3,21,10,4ba9vm,"Taking Machine Learning course, feel like I'm thrown on the deep end of the pool...",https://www.reddit.com/r/MachineLearning/comments/4ba9vm/taking_machine_learning_course_feel_like_im/,sjalfurstaralfur,1458523159,"So, I'm another comp sci major, your average guy who has taken algorithms, datastructures, blah blah. I'm taking machine learning and it feels like a bunch of math with no intuition was just thrown at me.

First week, professor throws a bunch of entropy equations at us, I have no clue what entropy even means, I don't know what all the equation means, what is ""regression"" or ""information gain"". I've never taken information theory before. Before I had time to digest it, we are moving to decision trees and neural networks. Those two feel more intuitive but still a lot of entropy equations to check for errors or something.

Basically, I was expecting a course focused on algorithms, but I got a bunch of entropy equations and error checking thrown at me. 

I'm currently reading on Support Vector Machines before the next lecture. I get what the margin is, I get what dot product and length means. But then the textbook throws me [this whole new equation](http://i.imgur.com/7uFsj2t.png) to check the error, and I don't get it... What is the lambda sign even mean? SVM's seem crucial in particular because it's the picture featured on Wikipedia's machine learning portal.

Sorry if this is kind of a complain-fest. How did you learn all of this math? Where did you start? Any good resources? Because I feel like the professor went through the content at the speed of light. Thanks.",15,0
718,2016-3-21,2016,3,21,10,4badnc,Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?,https://www.reddit.com/r/MachineLearning/comments/4badnc/do_deep_convolutional_nets_really_need_to_be_deep/,mttd,1458524913,,33,54
719,2016-3-21,2016,3,21,10,4baek3,How to get Net surgery trick to work on VGG model?,https://www.reddit.com/r/MachineLearning/comments/4baek3/how_to_get_net_surgery_trick_to_work_on_vgg_model/,deepbasu007,1458525334,"I have been trying to apply the net surgery trick found here: 
https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb
to the VGG model. However, it seems that the net surgery trick won't produce identical results to the fully connected CNN because the padding on the convolutional layers in the VGG model can't be replicated when feeding in the bigger image. Is there any way to produce identical results in VGG models with net surgery? Any pointers would be great!",2,0
720,2016-3-21,2016,3,21,11,4bafvh,Jeff Dean's Lecture of Deep Learning at Google (Stanford CS231n),https://www.reddit.com/r/MachineLearning/comments/4bafvh/jeff_deans_lecture_of_deep_learning_at_google/,nightcore_lover,1458525952,,0,15
721,2016-3-21,2016,3,21,11,4bagek,Decision Tree  Choosing a perfect Avocado,https://www.reddit.com/r/MachineLearning/comments/4bagek/decision_tree_choosing_a_perfect_avocado/,sarkarsh,1458526201,,1,0
722,2016-3-21,2016,3,21,11,4baj3n,Review of Ask Me Anything - Dynamic Memory Networks for Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/4baj3n/review_of_ask_me_anything_dynamic_memory_networks/,napsternxg,1458527519,,0,0
723,2016-3-21,2016,3,21,12,4bat2q,Inverse binary tree permutations ( with constraints),https://www.reddit.com/r/MachineLearning/comments/4bat2q/inverse_binary_tree_permutations_with_constraints/,[deleted],1458532486,[deleted],1,2
724,2016-3-21,2016,3,21,13,4bawjj,Neural style transfer for text?,https://www.reddit.com/r/MachineLearning/comments/4bawjj/neural_style_transfer_for_text/,sumalkin,1458534394,I haven't been able to find any papers on separating content from style in text/literature. Any leads would be appreciated!,5,4
725,2016-3-21,2016,3,21,14,4bb2ge,Do you think better models will come out of simplifying or complicating current state-of-the-art models?,https://www.reddit.com/r/MachineLearning/comments/4bb2ge/do_you_think_better_models_will_come_out_of/,ecobost,1458537968,"I know 'state-of the art models' is a broad category and I know we can't predict the future (not to high accuracy, at least), I just wonder what your general opinions are. My guess is that more people is trying to build ever more complex models, although some people is pushing on the other direction, too.",5,12
726,2016-3-21,2016,3,21,15,4bb699,4 Reasons AlphaGo is a Huge Deal,https://www.reddit.com/r/MachineLearning/comments/4bb699/4_reasons_alphago_is_a_huge_deal/,llSourcell,1458540368,,7,0
727,2016-3-21,2016,3,21,15,4bb6z2,Is there a working example for doc2vec in gensim?,https://www.reddit.com/r/MachineLearning/comments/4bb6z2/is_there_a_working_example_for_doc2vec_in_gensim/,koormoosh,1458540849,"I have been looking around for a single working example for doc2vec in gensim which takes a directory path, and produces the the doc2vec model (as simple as this). All Google results end up on some websites with examples which are incomplete or wrong. Can someone familiar with gensim's doc2vec write a few lines of code that actually works?

Examples of broken/outdated codes:

    http://rare-technologies.com/doc2vec-tutorial
    https://medium.com/@klintcho/doc2vec-tutorial-using-gensim-ab3ac03d3a1#.qxvocskjm

",13,0
728,2016-3-21,2016,3,21,18,4bbml3,How Machine Learning is Making Companies More Efficient,https://www.reddit.com/r/MachineLearning/comments/4bbml3/how_machine_learning_is_making_companies_more/,javeedunbound,1458552887,,0,1
729,2016-3-21,2016,3,21,18,4bbod7,Which deep learning framework(s) support fractional strides?,https://www.reddit.com/r/MachineLearning/comments/4bbod7/which_deep_learning_frameworks_support_fractional/,joekr07,1458554263,"I am intrested in implementing the generative network architecture presented in Radford et. al. http://arxiv.org/pdf/1511.06434v2.pdf which used fractional strides in the convolutional layer for ""up-pooling"". Which deep learning frameworks will support this? keras? Theano? etc
",7,5
730,2016-3-21,2016,3,21,20,4bbvus,[Question] Building a Language Model for Text Summarization,https://www.reddit.com/r/MachineLearning/comments/4bbvus/question_building_a_language_model_for_text/,Mugiwara_Luffy,1458559580,"I am fairly new to Natural Language Processing. I am trying to build a Text Summarizer for a single document. After ranking the sentences in the document based on their importance to the document, I am trying to compress the sentences.

For this i am trying to implement [Constraint-based Sentence Compression An Integer Programming Approach. - Clarke and Lapata] (http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d4/data/pdf/anthology-PDF/P/P06/P06-2019.pdf) 

Their proposed method is to generate and solve an ILP for every sentence they want to compress.


In the ILP listed in the page 4, i have to find the probabilities of all possible trigrams that can occur in all the compressions. I am not able to figure out how to implement this

1) Generate all possible compression's of a particular sentence (2^n possibilities) and find the trigram probabilities from that. But generating 2^n compression's is not feasible for n&gt;30.

2) Should i use a Language modelling toolkit (like CMU-Cambridge Statistical Language Modeling toolkit) ?
If yes how ?",1,1
731,2016-3-21,2016,3,21,20,4bbyoc,How to resolve the following error for sklearn.metrics?,https://www.reddit.com/r/MachineLearning/comments/4bbyoc/how_to_resolve_the_following_error_for/,rohanpota,1458561501,,2,0
732,2016-3-21,2016,3,21,21,4bc0bi,DeepMind Challenges for StarCraft,https://www.reddit.com/r/MachineLearning/comments/4bc0bi/deepmind_challenges_for_starcraft/,throwawayprogrammer9,1458562445,,26,18
733,2016-3-21,2016,3,21,21,4bc3fr,Five lessons from AlphaGos historic victory,https://www.reddit.com/r/MachineLearning/comments/4bc3fr/five_lessons_from_alphagos_historic_victory/,thejamgroup,1458564218,,1,1
734,2016-3-21,2016,3,21,22,4bc5lt,Face Miner: Data mining applied to face detection,https://www.reddit.com/r/MachineLearning/comments/4bc5lt/face_miner_data_mining_applied_to_face_detection/,pgaleone,1458565355,,0,4
735,2016-3-21,2016,3,21,22,4bc6hk,What is the difference between Model Predictive Control and Q-learning?,https://www.reddit.com/r/MachineLearning/comments/4bc6hk/what_is_the_difference_between_model_predictive/,siddkotwal,1458565788,"I just implemented by first Q-learning in grid-world since I wanted to find a way to control an agent under uncertainity, possibly want to port it to a more complex environment. Maybe I have to spend more time, but I can't understand how one step-look ahead Q-learning compares to Model Predictive Control in practice? Can anyone explain it to me intuitively?",5,12
736,2016-3-21,2016,3,21,22,4bc8bb,[question] Has anyone got any experience using theano conv3D?,https://www.reddit.com/r/MachineLearning/comments/4bc8bb/question_has_anyone_got_any_experience_using/,joefromlondon,1458566664,"I am trying to implement a 3D convolutional neural net, primarily using lasagne, however do not have access to a GPU at this time. 

As I have used lasagne thus far, it makes sense to implement another layer in Theano, and switch between the two. 

Does anyone have experience using theano.tensor.nnet.Conv3D? and if so, how would I go about feeding a lasagne input layer into this, and feed the outputs into a lasagne output layer? (max pooling etc will follow)

Thanks in advance! ",2,1
737,2016-3-21,2016,3,21,23,4bcjmc,Google DeepMind's Deep Q-learning playing Atari Breakout,https://www.reddit.com/r/MachineLearning/comments/4bcjmc/google_deepminds_deep_qlearning_playing_atari/,pmigdal,1458571667,,0,0
738,2016-3-22,2016,3,22,0,4bcp53,"Machine Learning: An In-Depth, Non-Technical Guide - Part 5",https://www.reddit.com/r/MachineLearning/comments/4bcp53/machine_learning_an_indepth_nontechnical_guide/,innoarchitech,1458573919,,12,73
739,2016-3-22,2016,3,22,0,4bcpxb,Smile 1.1 is Released!,https://www.reddit.com/r/MachineLearning/comments/4bcpxb/smile_11_is_released/,pdsminer,1458574252,,6,11
740,2016-3-22,2016,3,22,0,4bcq3u,"LSTM sequence length, how does it affect learning?",https://www.reddit.com/r/MachineLearning/comments/4bcq3u/lstm_sequence_length_how_does_it_affect_learning/,[deleted],1458574320,[removed],0,1
741,2016-3-22,2016,3,22,0,4bcti6,Is truncated normal with low std (0.01-0.1) a better initialization than random normal for CNN filters?,https://www.reddit.com/r/MachineLearning/comments/4bcti6/is_truncated_normal_with_low_std_00101_a_better/,xristos_forokolomvos,1458575609,How about biases? I've seen people initialize them as constants and other as random variables,11,3
742,2016-3-22,2016,3,22,1,4bcur8,"Human eyes assist drones, teach machines to see",https://www.reddit.com/r/MachineLearning/comments/4bcur8/human_eyes_assist_drones_teach_machines_to_see/,datadotworld,1458576101,,0,1
743,2016-3-22,2016,3,22,1,4bcuzh,"[Question] LSTM audio sequence length, how does it affect learning? (MIR, SMC, AMT, BSS)",https://www.reddit.com/r/MachineLearning/comments/4bcuzh/question_lstm_audio_sequence_length_how_does_it/,carlthome,1458576178,[removed],0,1
744,2016-3-22,2016,3,22,1,4bd2zh,Machine Learning at Rutgers,https://www.reddit.com/r/MachineLearning/comments/4bd2zh/machine_learning_at_rutgers/,iseebeerpeople,1458579065,"I recently got admitted to the Master's program at Rutgers, the State University of New Jersey. I'm highly interested in machine learning, having gone beyond Andrew Ng's course, and I'm currently working in an industrial research lab, where I've gained exposure to reinforcement learning and deep learning. I was just wondering if any of you could shed some light on the quality of ML research being done at Rutgers, beyond what has been described on their website. I've inferred that they had a reasonably large number of papers accepted to ICLR, so that must be a huge plus. If any of you could share your thoughts, it'd be great! Thanks in advance.",1,1
745,2016-3-22,2016,3,22,2,4bd7qj,What kind of problem is this?,https://www.reddit.com/r/MachineLearning/comments/4bd7qj/what_kind_of_problem_is_this/,greatluck,1458580815,"I'm kind of new to machine learning so I'm using this exercise as a learning opportunity. Just looking for direction on what kind of problem I'm facing and what method you think would be best.


The purpose of this is to determine which product category will be shown for each customer in email advertisements.

I have a database of customer transactions across 6 different product segments.  A customer can have transactions across any combination of the 6 categories in the last 12 months.  Based on their history, I need to bucket each customer into only 1 product bucket.  So should a customer be labeled a product ""A"" customer, product ""B"" customer, etc.  

In some cases a customer has transactions in only 1 category in which case it's an easy decision.  In many cases it's more fuzzy.  For example category ""A"" is more important (profitable) than category ""B"" which is more important  than category ""C"" and so on.


I know this is a very vague and open question but as I mentioned I am just looking for guidance at this point on which path to explore further.  ",3,0
746,2016-3-22,2016,3,22,2,4bda91,How to benefit neural network ?,https://www.reddit.com/r/MachineLearning/comments/4bda91/how_to_benefit_neural_network/,hadf40,1458581686,"As long as I know, Deep Learning uses an abstract neural network, but how to benefit this neural network ?

Is it directly interogated by a software to help it to answer the user's question, or is it used to only generate a model that can be used by the software ?

In other words, does the neural network still works during software processing after the model is generated (regardless model evolution) ?

Thank you for your help ;)",2,0
747,2016-3-22,2016,3,22,3,4bdeoy,What are the downsides to having a large batch size?,https://www.reddit.com/r/MachineLearning/comments/4bdeoy/what_are_the_downsides_to_having_a_large_batch/,Nimitz14,1458583236,"This is in the context of running SGD and deciding to pick on how large of a batch of the training data one should use for each gradient descent step.

It seems to me like larger is always better, as in one should try and go for the largest size that is possible given the hardware one is using. Is that correct?",10,9
748,2016-3-22,2016,3,22,3,4bdhvv,Learn Data Processing in Hadoop,https://www.reddit.com/r/MachineLearning/comments/4bdhvv/learn_data_processing_in_hadoop/,andalib_ansari,1458584319,[removed],0,1
749,2016-3-22,2016,3,22,3,4bdm4v,Looking for resources on embedding/kernel methods,https://www.reddit.com/r/MachineLearning/comments/4bdm4v/looking_for_resources_on_embeddingkernel_methods/,sprintletecity,1458585779,"I'm trying to obtain the background necessary to read through and understand the NIPS embedding papers from the last couple years, any recommendations concerning resources would be useful.",2,6
750,2016-3-22,2016,3,22,4,4bdv8v,Draknet: Convolutional Neural Networks in C,https://www.reddit.com/r/MachineLearning/comments/4bdv8v/draknet_convolutional_neural_networks_in_c/,3eyedravens,1458589014,,19,5
751,2016-3-22,2016,3,22,5,4be5rs,Use of Octave/Matlab vs. R/Python?,https://www.reddit.com/r/MachineLearning/comments/4be5rs/use_of_octavematlab_vs_rpython/,sparkysparkyboom,1458592756,"I signed up to take Andrew Ng's ML class which starts today, but I found out that the programming in the course will be taught in Octave/Matlab.  I am fairly new to the world of data science after having gotten my undergraduate degree in Statistics from Carnegie Mellon University, as well as taking another graduate level Data Mining class there, and working a few months as an entry level data scientist. For those familiar with the many technical tools available for data science, how worth it is it to take Andrew Ng's ML class, given that I'm quite certain I'll never use Octave/Matlab again? My [limited] expertise is in R/Python, which are two extremely effective tools already, and frankly, I'd rather hone my skills at those rather than pick up something new that I won't use again. I had never heard of Octave until today, but then again I am inexperienced in data science. 

In addition, can anyone recommend classes that an up and coming data scientist should take, preferably that would teach in R or Python (but obviously don't ignore the conceptual parts)?  Thanks in advance!",24,4
752,2016-3-22,2016,3,22,5,4be5vh,A Question About Boosting,https://www.reddit.com/r/MachineLearning/comments/4be5vh/a_question_about_boosting/,[deleted],1458592796,[deleted],6,4
753,2016-3-22,2016,3,22,6,4behuh,Does the number of layers in an LSTM network affect its ability to remember long patterns?,https://www.reddit.com/r/MachineLearning/comments/4behuh/does_the_number_of_layers_in_an_lstm_network/,butWhoWasBee,1458597305,"Is a deeper network generally better at remembering very long patterns than a more shallow one, or can additional layers decrease the ability of the network to remember longer patterns. I understand the answer is likely problem specific, but any insight from experience is helpful. 

The way I have been testing the memory of the network is by training it on patterns where the current step is entirely determined by the values from X steps ago. I have a 3 layer LSTM, and I notice that while it can easily remember patterns of up to around 40 time steps, it starts to struggle at 80. ",29,36
754,2016-3-22,2016,3,22,6,4behzm,What does the hypothesis function of a single-feature NN look like?,https://www.reddit.com/r/MachineLearning/comments/4behzm/what_does_the_hypothesis_function_of_a/,relganz,1458597363,"The hypothesis function h(x) for a single feature (univariate) linear regression is always a line, and for quadratic it's a parabola.  

**I know you wouldn't do this in practice** but let's say you trained a NN (over a ton of items) with many hidden layers, each with many hidden units, but only one input parameter.  What would the resulting function look like on a graph?

Related question: Would it be continuous (smooth)?  Are hypothesis functions for regression *always* continuous?

Thanks",7,5
755,2016-3-22,2016,3,22,7,4bek0b,Item2Vec - Application Setting,https://www.reddit.com/r/MachineLearning/comments/4bek0b/item2vec_application_setting/,mluja,1458598107,[removed],0,1
756,2016-3-22,2016,3,22,7,4bensp,Neural network topology,https://www.reddit.com/r/MachineLearning/comments/4bensp/neural_network_topology/,ihaphleas,1458599525,"A downvote I got on a question I asked a few days ago makes me ask this question: For neural networks, is the topology more important than the weight correction? Granted that this is an oversimplification, but I tend to think of the topology determining ""what can be learned"" and the weights determining ""how well a thing is learned"" -- is this a decent intuition? This means any weight adjustment method which is ""complete"" can adjust the NN within its domain (perhaps slowly), yes?",6,3
757,2016-3-22,2016,3,22,7,4beoq1,Neural Net idea: Does this exist?,https://www.reddit.com/r/MachineLearning/comments/4beoq1/neural_net_idea_does_this_exist/,alephnaught90,1458599876,"I'm thinking there could be such a thing as a 'focusing' layer, which essentially discards the parts of an image (or rows and columns in the matrix) that don't contain features. You could iteratively use a series of convolutional layers to accentuate features, and then a focusing layer to discard useless chunks of the data.

So, for example, say you're trying to categorize pictures into those which have faces in them, and those which don't. But the faces don't have to appear in any particular location within that image. They could be small little things in one corner of the image, or up close and center. The convolution and focusing layers would carve off parts of the image that don't contain faces, always resulting in faces being close up and center within the data being fed into your fully connected layers.",6,2
758,2016-3-22,2016,3,22,8,4beurs,Data augmentation techniques for small datasets?,https://www.reddit.com/r/MachineLearning/comments/4beurs/data_augmentation_techniques_for_small_datasets/,deepreinforcement,1458602217,[removed],0,1
759,2016-3-22,2016,3,22,8,4beva1,Centering and scaling for skewed distributions,https://www.reddit.com/r/MachineLearning/comments/4beva1/centering_and_scaling_for_skewed_distributions/,BlackHawk90,1458602433,"Hello

I have a dataset where the features are skewed (non normal) distributions. My preprocessing pipeline consists of the following steps:

1. Missing values imputation
2. Centering and scaling (zero mean and unit variance) of each feature 
3. Transforming the features to an approximate normal distribution by using the Box-Cox Transformation.

Should I first do the centering and scaling or the transformation?

Second, if the distributions are skewed (not normal) is centering and scaling (zero mean and unit variance) still ok? Another possiblity would be to subtract the median (instead of the mean) and dividing by 1.5 * the interquartile range (instead of the standard deviation).
",5,9
760,2016-3-22,2016,3,22,8,4bevdq,An Introduction to Data Science for DNA Sequencing,https://www.reddit.com/r/MachineLearning/comments/4bevdq/an_introduction_to_data_science_for_dna_sequencing/,DrLegend,1458602470,,0,11
761,2016-3-22,2016,3,22,8,4bf0y2,Barclays Techstars start-up Seldon drives open source machine learning,https://www.reddit.com/r/MachineLearning/comments/4bf0y2/barclays_techstars_startup_seldon_drives_open/,ahousley,1458604774,,0,0
762,2016-3-22,2016,3,22,9,4bf50w,Watson vs AlphaGo - What are the differences?,https://www.reddit.com/r/MachineLearning/comments/4bf50w/watson_vs_alphago_what_are_the_differences/,losurizen,1458606523,"The head of DeepMind, Demis Hassabis, told Wired, The most significant aspect of all thisis that AlphaGo isnt just an expert system, built with handcrafted rules. Instead, it uses general machine-learning techniques to win at Go.

As I know Watson is an expert system. 

Does it mean that if you put AlphaGo on Jeopardy it will master it just like Watson did?",7,0
763,2016-3-22,2016,3,22,10,4bfa8y,Ttutorial on Agency and Causality,https://www.reddit.com/r/MachineLearning/comments/4bfa8y/ttutorial_on_agency_and_causality/,pierrelux,1458608718,,0,2
764,2016-3-22,2016,3,22,10,4bffug,Murphy vs Bishop?,https://www.reddit.com/r/MachineLearning/comments/4bffug/murphy_vs_bishop/,theUtterTruth,1458611117,"I have been dabbling (via coursera, online tutorials etc,.) with ML for the past three months and will be starting my ""formal"" studies in ML this week. Based on suggestions in online forums, I have shortlisted two books for this purpose: ""Machine Learning - a probabilistic perspective"" by Kevin Murphy and ""Pattern recognition and Machine learning"" by Christopher Bishop. I would like to hear your opinions on the two books regarding their style, organization of topics and the breadth+depth of material covered.
I have briefly looked at the math used in the two books and I feel that it will not be a problem. 
Any suggestions and insights will be appreciated!",9,8
765,2016-3-22,2016,3,22,11,4bfl8a,How Can I Use ML for Mental Health Issues?,https://www.reddit.com/r/MachineLearning/comments/4bfl8a/how_can_i_use_ml_for_mental_health_issues/,computerGuru8,1458613509,I love math and computer science. I want to combine the two to be a data scientist. I came across some articles about how ML could affect treatments in mental health (depression) and it is very interesting to me. What I ultimately want to do is make algorithms that doctors can use to help people with mental illnesses. Is this a job where I research as a professor in academics? Are there maybe jobs in hospitals? ,3,0
766,2016-3-22,2016,3,22,12,4bfswx,ELI5 Dropout technique,https://www.reddit.com/r/MachineLearning/comments/4bfswx/eli5_dropout_technique/,ssreekanth2000,1458617054,,3,0
767,2016-3-22,2016,3,22,14,4bg5yf,Are there any good GUI editors for deep neural networks?,https://www.reddit.com/r/MachineLearning/comments/4bg5yf/are_there_any_good_gui_editors_for_deep_neural/,polkm,1458624225,I've been thinking about the idea of making a simple editor myself but I don't want to reinvent the wheel. Ideally it would allow you to build a stack of diffrent kinds of hidden layers and train them in the program with a nice visual to go along.,21,12
768,2016-3-22,2016,3,22,15,4bgb1e,Defining image similarity using pre-trained convnet activations.,https://www.reddit.com/r/MachineLearning/comments/4bgb1e/defining_image_similarity_using_pretrained/,charlie0_o,1458627567,"Hey reddit,

I lately implemented a code to find similarity between images using pretrained convnet (inception v3). I noticed that the L2 distance between activation values (the next to last layer) of the images are tough to interpret. For eg. the difference between [american staffordshire terrier](http://www.dog-breeds-expert.com/images/american-staffordshire-terrier.jpg) and [running maltese](http://waggypups.uibcsites.com/wp-content/uploads/sites/1109/files/2012/04/Running-maltese-299x299.jpg) was 18.68863 and the difference between the same terrier image and a [flower](http://www.graybarn.com/Top10_Pages/Agapanthus_Storm300.jpg) was 19.86305. Shouldn't these difference values be more apart?

I know I'm giving equal weight to each neuron when calculating L2 and a trained softmax layer above would look at these neurons in a weighted way - but is there a way to better use these pre-trained nets for similarity finding? 

Thanks and here's the [source code](https://github.com/shekkizh/TensorflowProjects/blob/master/FindInceptionSimilarity.py) if it helps.
",4,3
769,2016-3-22,2016,3,22,18,4bgob3,Octave/Matlab vs Python (speed),https://www.reddit.com/r/MachineLearning/comments/4bgob3/octavematlab_vs_python_speed/,Gay_Hat_On_Nun,1458637572,"Hi r/machinelearning , I was looking for general input on which of the two aforementioned languages are faster, especially in training a neural network. For reference, I'm on Windows and I use Python 2.7, although I'll be switching to 3 soon. In terms of Python packages, I guess I'm referring mainly to scikit-learn, numpy, matplotlib, pandas, etc... the traditional ones.
Thanks!",21,8
770,2016-3-22,2016,3,22,18,4bgppk,How to choose strategy for analysing how parameters affect final price.,https://www.reddit.com/r/MachineLearning/comments/4bgppk/how_to_choose_strategy_for_analysing_how/,Xsci,1458638685,"I'm started to learn data analytic and going to to solve my problem. Can someone give me advice, what strategy/technique I need to study for solving my problem? What I should ""google"" and read ;)
In my solution, I wan't to predict price based on object parameters (I think it is regression problem) and list found rules how some parameters affect final price.",1,0
771,2016-3-22,2016,3,22,18,4bgrpa,How could we predict the next Brussels bombings?,https://www.reddit.com/r/MachineLearning/comments/4bgrpa/how_could_we_predict_the_next_brussels_bombings/,bayeslaw,1458640213,"The terrible news got me wondering.. Since secret services have a list of people in Belgium, France, etc who are suspected to be involved with ISIS, wouldn't it be possible to detect more of them and especially the semi-radicalised young men who are just about to sign up for their lunacy, by inspecting the patterns in their communications after such attacks? 

I'd assume (this is highly speculative btw) that there's a clear graph structure that pops up between regular folks checking up on their relatives on FB, emails, etc.. And the communication between people involved with or interested in such crimes could show a very different pattern.. 

Am I stating the obvious and secret services are already using these events to detect radicalised people by their communication patterns?
Or is this too optimistic as we almost never have access to their communication, (they (even the low profile semi-radicalised newbies) use encrypted channels)?
Or is this too weak of a signal to detect from the billions of text-transactions?",5,0
772,2016-3-22,2016,3,22,18,4bgsbv,,https://www.reddit.com/r/MachineLearning/comments/4bgsbv//,mirigold95,1458640692,,3,0
773,2016-3-22,2016,3,22,22,4bhhhk,Deep Advances in Generative Modeling - Alec Radford,https://www.reddit.com/r/MachineLearning/comments/4bhhhk/deep_advances_in_generative_modeling_alec_radford/,gwulfs,1458655061,,7,126
774,2016-3-22,2016,3,22,23,4bhhub,Deep learning hardware recommendations for research lab?,https://www.reddit.com/r/MachineLearning/comments/4bhhub/deep_learning_hardware_recommendations_for/,SSOctopus,1458655224,"Hi ML, would like some recommendations for purchasing new hardware. Research lab at my university has a budget of below 100K to spend on new equipment. The research team consists of 10-20 members at any given moment. Much of our work involves deep learning on images and videos. 

Doing some cursory research online i.e. [Tim's post](http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/) and [Roelof's](http://graphific.github.io/posts/building-a-deep-learning-dream-machine/) suggests that a setup akin to the DevBox is sufficient for research purposes. We have one already on a server shared between a few people and that's worked out wonderfully. We can buy a few more of these and simply add them to the server but our budget allows for something better - do you guys have any suggestions for a better machine, or would a better machine than the DevBox even be cost-efficient for DL research? Thanks",16,3
775,2016-3-22,2016,3,22,23,4bhhus,"Don't Buy Machine Learning, unless ...",https://www.reddit.com/r/MachineLearning/comments/4bhhus/dont_buy_machine_learning_unless/,copybin,1458655229,,3,0
776,2016-3-22,2016,3,22,23,4bhkvk,Segregating data sets,https://www.reddit.com/r/MachineLearning/comments/4bhkvk/segregating_data_sets/,[deleted],1458656495,[deleted],0,0
777,2016-3-22,2016,3,22,23,4bhmqb,Decentralized deep learning on a blockchain. AI owned by everyone (Bitcoin meets TensorFlow),https://www.reddit.com/r/MachineLearning/comments/4bhmqb/decentralized_deep_learning_on_a_blockchain_ai/,darbsllim,1458657253,"Is there anyone working on either a decentralized deep learning algorithm, or a consumer facing app that uses AI to help people diagnose themselves?

My wife was just diagnosed with CVID a couple of weeks ago, it's like AIDS except it's not Aquired, it's part genetic and part environmental - but it's a rare primary immunodeficiency disease.

She's had this her entire life. She's 33 years old, a wonderful mother to our 5 year old daughter, and beautiful singer, actor and writer. She was misdiagnosed 3 or 4 times, most recently she was eating gluten free for the last 8 years because she was diagnosed as celiac disease.

She's lost most of her hair over the last 6 months and has been in the hospital 3-4 times this year. It turns out, she never had celiac, she has always had CVID.

**Where Deep Learning fits in.**
My wife should have been diagnosed in her childhood years, in fact, all it would have taken was a simple blood test to measure her antibody levels, and an Immunologist appointment.

With all of her symptoms, her medications and blood test results - a deep learning algorithm would have been able to suggest a proper diagnosis in a few minutes instead of the 30 years that it took for her to get properly diagnosed by just letting doctors do their thing.


**The problem with diagnosing rare diseases:** 
CVID affects 1 in 25,000 - 50,000 people, it's a rare disease where patients present with a myriad of symptoms and autoimmune problems. It's hard to get a correct diagnosis because a patient typically sees many different doctors to treat the different types of symptoms, and they dont typically share information efficiently - nor do they have an incentive to properly diagnose her.

There should be a visually appealing, easily marketable app that combines machine learning and crowdsourced input from app users to give the ""hot/cold"" direction that will greatly improve time to diagnose these ""zebra"" cases.

The average lag time for CVID diagnosis is 6-7 years. This is common with most rare diseases. If my wife was diagnosed even 2 years ago, she would not have lost all of her hair.

The treatment for my wifes condition is IVIG every 2-4 weeks, and it greatly improves quality of life and life expectancy. The earlier a rare disease is diagnosed, the better the quality of life.

**The problem with doctor-facing AI solutions:**
I see that there are some machine learning startups, but they are mostly targeted towards health professionals. Theres resistance from doctors to adopt AI. 

The problem is that this technology needs to be available for the patient, not just doctors, and not just specialists at John Hopkins or the Mayo Clinic.

Nobody is going to be as motivated and investing in someone's health as the person and their loved ones. Quite often, patients with diseases become more knowledgeable than the specialists treating them for the disease.

Theres a lot of knowledge to be tapped into there from the 'zebras' themselves.

**Potential barriers to a centralized organization providing this solution:**
The FDA and drug companies are resistant to technologies that allow users to diagnose themselves. 23andme ran into issues with this. They just finally got FDA approval in October to start helping people agian (http://www.popsci.com/23andme-gets-fda-approval-for-direct-to-consumer-genetic-tests)


**Some existing projects:**
A friend who sold his company to Salesforce for 70 million dollars introduced me to this TED talk shortly after my wife was diagnosed, where Jeremy Howard explains how deep learning works, and its potential applications:
https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn?language=en

^Jeremy's company Enlitic is using deep learning to help doctors come to a proper diagnosis faster, currently only focusing on radiology. Again, it's just doctor-facing.

www.findzebra.com is a search engine that you can input your symptoms, it uses something similar to deep learning to suggest possible health issues, but it doesnt use AI. It crawls and indexes only curated medical sources.

Jeremy Gardner &amp; the Augur guys introduced me to www.crowdmed.com which is a great solution for diagnosing rare diseases, but costly.

CrowdMed is a site where you tell your story, and then it uses crowdsourced knowledge to come to diagnosis suggestions. You pay a monthly fee of $299 - $749 a month, and then medical detectives investigate for you, and their predictive market algorithm ranks what the detectives submit.

Vitalik Buterin of Ethereum told me about www.numer.ai which is a competition that uses homomorphically encrypted machine learning to let people try to predict the stock market. Its a way to anonymize data, to alleviate some concerns about peoples medical data being publicly available.

Xprize even has a $5 million dollar prize up with IBM Watson for AI http://www.xprize.org/ai

There's also openai.com, with names like Elon Musk &amp; Sam Altman attached, it's a non-for-profit with a billion dollars committed, but they haven't yet released what their focus is.

**Potential solution to being blocked by FDA etc:**
Decentralized deep learning on a blockchain, where users are rewarded tokens for providing the hot/cold and running the network. Think ethereum, bitcoin, etc.

In my limited understanding of machine learning, it seems that for a deep learning algorithm to learn, humans need to give it hot/cold inputs to the correlations it comes up with as it compares datasets (Jeremys ted talk video explains that)

My theory is that a decentralized deep learning algorithm on a blockchain could be built where the people giving hot/cold inputs are awarded with a token for doing the mechanical turk style work. When consensus is achieved, the people who were correct get rewarded. Similar to how Augurs reporting system works, or bitcoins proof of work.

If users are rewarded for giving correct hot/cold inputs to help the deep learning AI learn about subjects, theres a financial incentive to keep the network running.

Companies, individuals, universities, etc could tap into the algorithm to use it for whatever purpose they want - and they would pay to use it.

IE I want to build an application that uses deep learning to help diagnose rare diseases so people like my wife dont have to suffer going undiagnosed and untreated their entire lives. I would pay to have the algorithm learn about the human body, how it works, diseases, treatments, etc. The users of the network get paid to ""train it with hot/cold inputs.

Is there anyone working on anything like this, whether its centralized or decentralized?",44,19
778,2016-3-23,2016,3,23,0,4bhwrs,python datamining packages virtual environment setup in ubuntu,https://www.reddit.com/r/MachineLearning/comments/4bhwrs/python_datamining_packages_virtual_environment/,dataaspirant,1458661096,,0,0
779,2016-3-23,2016,3,23,1,4bi540,Advice on image recognition project via CNNs and transfer learning,https://www.reddit.com/r/MachineLearning/comments/4bi540/advice_on_image_recognition_project_via_cnns_and/,to4life,1458664179,"Hello, 

I have an image recognition project I'm working on and I've determined that deep learning with CNNs is probably the most appropriate technique to use. In particular, I am planning to use ""transfer learning"" to recognize a single type of object. 

**Project objective:** using a USB camera and software running on a laptop, recognize and mark (i.e. draw a box around) any **handicap buttons**. You know, these things: 

http://recyclenation.com/resources/2015/3/handicap%20buttons.jpg
http://jimlindlauf.areavoices.com/files/2011/12/Handicap-door-opener.jpg
https://purpleletters.files.wordpress.com/2010/04/hand.jpg
http://clinicalposters.com/img/photo/handicap_button.jpg

I learned about transfer learning here: http://cs231n.github.io/transfer-learning/ My understanding is that by using features obtained by training on the imagenet database, I can create something simple like a support vector machine to classify the object. 

**Datasets:**

- Positive dataset: since my project only needs to work on a few real-life buttons, I was planning on taking videos with my camera of the door buttons. Maybe a few thousand frames, zooming in and out and rotating the phone to get a more varied dataset. 

- Negative dataset: ? Not sure what sort of data I should train on for ""not handicap door button"". I could definitely use some advice here. How large should this dataset be? What should it consist of?

**Proposed process:**

- Collect datasets of positive (is button) and negative (is not button) examples through photo/video. Resize and downsample as appropriate. 

- Use deep learning software library Caffe (highly recommended according to my web searches) with a pre-trained imagenet model. I can get the features with a forward pass of an image through the network (after vectorizing the output of last conv layer), which should give me e.g. a 4096-D vector.

- Use these image code feature vectors to train an SVM on my dataset.

- Use OpenCV to draw a box around the region with the highest model activations once the software detects the image frame has a handicap button in it (unclear on specifics of this, but I'm sure it's possible).

Does this sound reasonable? What extra steps or modifications would you make to my plan? 

Thank you very much for any insight or advice! ",7,1
780,2016-3-23,2016,3,23,1,4bi63m,New Subreddit: Project AIX for Minecraft  /r/MinecraftAIX - Develop the future of Game Playing AI,https://www.reddit.com/r/MachineLearning/comments/4bi63m/new_subreddit_project_aix_for_minecraft/,criticalcontext,1458664551,,0,0
781,2016-3-23,2016,3,23,1,4bi8o4,Where did you learn about more advanced networks?,https://www.reddit.com/r/MachineLearning/comments/4bi8o4/where_did_you_learn_about_more_advanced_networks/,IndividualCarnival,1458665492,"A lot of links and discussions i see on the subreddit covers a lot of CNN, RNN , but I suppose I cant ideally follow these conversations from a lack of knowledge.

What are some good courses/books to cover more in depth applications of these networks? My searching has led me up to to cs231 and it seems rather convincing, but what are viable options?

Thanks!",4,0
782,2016-3-23,2016,3,23,2,4biap8,RL Question: Policy Gradients vs Q Learning - which is better?,https://www.reddit.com/r/MachineLearning/comments/4biap8/rl_question_policy_gradients_vs_q_learning_which/,metacurse,1458666218,,14,5
783,2016-3-23,2016,3,23,2,4bif7x,Help needed with Face Recognition,https://www.reddit.com/r/MachineLearning/comments/4bif7x/help_needed_with_face_recognition/,mumbaimaari,1458667829, I am trying to use [openface ](https://cmusatyalab.github.io/openface/)attached to a classifier (cross entropy criterion) to classify faces of about 200 people. I have about 80 images per person. I am getting an accuracy of about 70%. Can anyone please tell me if there are ways to improve it,4,0
784,2016-3-23,2016,3,23,2,4bij99,Good Deep Learning topic for Graduate Research Thesis,https://www.reddit.com/r/MachineLearning/comments/4bij99/good_deep_learning_topic_for_graduate_research/,HyperSpectralVoice,1458669313,"Hello everyone!

I'm planning to do my graduate thesis on Deep Learning. I know the basics of Machine Learning (from Andrew Ng's Coursera Class) and Deep Learning (Nando de Freitas's Lectures). I've developed ""deep"" interest in this amazing field.

As a total beginner in this field, what do you guys suggest should I do my master's thesis on? What specific deep neural network architecture should I choose? What should be the task on which I should work on (considering that I'm totally new in this field)?

I still want to be a bit innovative and not exactly do what has already been done in this field. I'm eager to learn new things in this field.

So, please help me choose a good research topic to work on! 

P. S. As a deep learning framework, I've chosen TensorFlow. Is it any good for newbies like me? I've quite a lot programming experience in C++, Java and a bit of Python, by the way.

Thanks!",3,0
785,2016-3-23,2016,3,23,3,4bis0w,How are ML researchers so sure of their work?,https://www.reddit.com/r/MachineLearning/comments/4bis0w/how_are_ml_researchers_so_sure_of_their_work/,[deleted],1458672507,[deleted],6,0
786,2016-3-23,2016,3,23,4,4bivbo,Some Starting Points for Deep Learning and RNNs,https://www.reddit.com/r/MachineLearning/comments/4bivbo/some_starting_points_for_deep_learning_and_rnns/,GizmoC,1458673672,,0,0
787,2016-3-23,2016,3,23,4,4biyad,Text Classification via Universal Taxonomy - Looking for ML practitioners to test use-cases,https://www.reddit.com/r/MachineLearning/comments/4biyad/text_classification_via_universal_taxonomy/,eContext_Chris,1458674762,"[examples at end of post] I'm admittedly a noob when it comes to ML, but my company, eContext, has curated a universal taxonomy over the past nine years that encompasses everything commercially and socially relevant on the web.  It is made up of 650M real user search queries bucketed into 25 vertical categories (Auto, Health, Finance, etc.) containing roughly 450K sub-categories.  

It's a rule-based system, and we use NLP and nGram chunking to parse long and short form text and map search queries, social posts, web content, blogs, forums, reviews, etc. to the category hierarchy providing structured, topical intelligence to data streams at scale.  It is extremely accurate because we've built 55M controlled vocabularies (Ex. rules determine the difference between bowtie pasta &amp; bowtie gift wrapping).

Being the noob I am, I am trying to understand how our real time classification capabilities can improve the efficiency of machine learned processes.  I understand that supervised training models require a corpus of text from which a model can determine entities, ontological connections, and apply statistical models to understand what people, places, things, concepts are and how they may be connected, but we've already built out the taxonomy to understand connections between things, and can provide greater context to ""what"" something truly is.  For example, you have a corpus of text containing different types of vehicles - Pick up truck, Ford F150, Toyota Camry, Jetski, bicycle, helicopter, accord, etc.  eContext would map each of those to a category resulting in a list of concepts, but further, provide the context automatically by recognizing that all of these things fall into the ""Vehicles"" category.  Accord, Toyota Camry, Ford F150, and pick up truck all fall under the Vehicles::Automotive Vehicles branch (implies the connection there), and further Accord and Toyota Camry both fall into a child of Automotive, that being ""sedans"" - Vehicles::Automotive Vehicles::Sedans

So, the question is, where can this be applied in building ML models to improve efficiency.  I see this being the most useful when you're looking at a sloppy corpus, like social media posts, or content streams from the web with an extremely varied topical distribution, but I'm sure there are more pointed applications as well.  

I've also heard there may be value of this type of classification in auto-Annotation for Text Analytics, but any other suggestions or perspectives would be appreciated!!

I would love the opportunity to work with any of you to develop a hypothesis around the capabilities of a universal taxonomy in improving the efficiency and efficacy of ML processes.  I look forward to learning more.

Classification Examples
http://imgur.com/B4yr0bg - We've taken the last 488 posts that @realDonaldTrump has posted to Twitter, parsed, and mapped to the category hierarchy for a top level, categorical view of his activity - each color represents a different vertical, so tan=government, yellow=arts &amp; entertainment, green=finance, etc.

http://imgur.com/2bdXE2P - Article - Content Classification example - notice the category paths provided, which I think could be a feature or quality to clarify the context of entities/annotations",6,2
788,2016-3-23,2016,3,23,4,4bj37h,TensorFlow ConvNets on a Budget with Bayesian Optimization,https://www.reddit.com/r/MachineLearning/comments/4bj37h/tensorflow_convnets_on_a_budget_with_bayesian/,Zephyr314,1458676554,,18,16
789,2016-3-23,2016,3,23,5,4bj4fv,Is the machine learning specialization on Coursera from the Washington university worth the money?,https://www.reddit.com/r/MachineLearning/comments/4bj4fv/is_the_machine_learning_specialization_on/,Akatchi,1458677000,"Hey everyone! 

I will start by giving some background information. Currently I am a final year (graduation year) CS student who got interested in machine learning about 6 months ago. I started with the Andrew NG course from Coursera which I recently finished (about 3 weeks ago).

When I finished the Coursera course I saw a suggestion that if you'd like to continue to learn more about machine learning you could follow the online Coursera specialization from the Washington university. I didn't pay attention to the specialization at that moment. 
But recently I got an email from Quora about an AMA session from Carlros Guestrin (https://www.quora.com/What-are-the-best-ways-to-learn-advanced-Machine-Learning-outside-academia/answer/Carlos-Guestrin?srid=Cqnv). In this AMA he suggested that if you'd like to learn more about machine learning one of the things you could do was to follow and complete the Coursera course from Andrew NG and their specialization course. 

So now I am wondering if the course (https://www.coursera.org/specializations/machine-learning) is worth the money? Has anyone followed this course and did you enjoy it?

Thanks a lot for taking the time to read it :)

",11,0
790,2016-3-23,2016,3,23,6,4bjfzf,Factors influencing for the development of Global Single-Disc Rotary Floor Machine Consumption 2016 Market | MIR,https://www.reddit.com/r/MachineLearning/comments/4bjfzf/factors_influencing_for_the_development_of_global/,tharunvicky,1458681293,,0,1
791,2016-3-23,2016,3,23,6,4bjhf6,Tensorflow RNN (LSTMS) Need help understanding why there is a dimension error,https://www.reddit.com/r/MachineLearning/comments/4bjhf6/tensorflow_rnn_lstms_need_help_understanding_why/,[deleted],1458681827,[deleted],0,0
792,2016-3-23,2016,3,23,7,4bjn5j,DanDoesData: Keras machine learning wrapper library,https://www.reddit.com/r/MachineLearning/comments/4bjn5j/dandoesdata_keras_machine_learning_wrapper_library/,vanboxel,1458684053,,0,1
793,2016-3-23,2016,3,23,7,4bjnmr,Difference between DRAM model and RAM,https://www.reddit.com/r/MachineLearning/comments/4bjnmr/difference_between_dram_model_and_ram/,ShakespearePoop,1458684231,"Hey guys,

I didn't know a better forum to post this on, so if you're aware of one, please let me know.

 I just read the DRAM paper, and they mention that the RAM model wasn't able to scale to 'real-world image tasks or multiple objects'. However, the authors go on to eventually say that they use the same training procedure as the RAM model, so I'm having trouble understanding what key difference their model has which allowed them to succeed on those tasks. I understand that their model is a bit different (they multiple the location and glimpse hidden states instead of feeding them through another hidden layer, and they have a few other differences involving the structure of their networks), but I'm wondering if their performance improvements are fully attributable to these changes in their networks, or if I'm missing something important.

Thanks for the help.",4,1
794,2016-3-23,2016,3,23,8,4bk2d0,Learning using one very strong feature and many weaker ones?,https://www.reddit.com/r/MachineLearning/comments/4bk2d0/learning_using_one_very_strong_feature_and_many/,briculmircea,1458690038,"Given a learning problem (mine is regression, but classification is equally applicable) where one feature is a very strong predictor, while the others are weak, how does one make use of the weaker features?

I've noticed that every time I attempt to add other, weaker features to the strong one, regardless of regressor type(gradient boosting, random forests, regression tree, linear regression, SVR), the results just get worse on the test set.

Does anyone know of any literature about combinig heterogeneous strength features for regression/classification?",2,2
795,2016-3-23,2016,3,23,8,4bk2jx,"TensorFlow for ""shallow learning"" -- Implementations of Multilayer-Perceptron and Softmax Regression in a scikit-learn-like interface",https://www.reddit.com/r/MachineLearning/comments/4bk2jx/tensorflow_for_shallow_learning_implementations/,[deleted],1458690114,,0,17
796,2016-3-23,2016,3,23,8,4bk3aq,"""Fancy"" optimizers contribute to overfitting in deep networks?",https://www.reddit.com/r/MachineLearning/comments/4bk3aq/fancy_optimizers_contribute_to_overfitting_in/,[deleted],1458690419,[removed],0,1
797,2016-3-23,2016,3,23,9,4bk9st,man vs StarCraft machine TeamLeague. Apply your ML techniques to StarCraft and win a (small) prize.,https://www.reddit.com/r/MachineLearning/comments/4bk9st/man_vs_starcraft_machine_teamleague_apply_your_ml/,LetaBot,1458693131,,10,42
798,2016-3-23,2016,3,23,10,4bkg1s,Could Machine Learning make domestic robots a reality?,https://www.reddit.com/r/MachineLearning/comments/4bkg1s/could_machine_learning_make_domestic_robots_a/,[deleted],1458695855,[deleted],2,0
799,2016-3-23,2016,3,23,10,4bkj3n,high level overview of GeneticAlgo for tetris,https://www.reddit.com/r/MachineLearning/comments/4bkj3n/high_level_overview_of_geneticalgo_for_tetris/,maximus12793,1458697159,"Just had a question about the way a genetic algo improves fitness. So say the inputs were just combinations of key presses (left right flip(up/down)). How would the program ever determine a good strategy? Will it induce that these combinations correlate with the board state or do I need to have it associate the 2? Seems like the videos I see (ex. https://www.youtube.com/watch?v=-4FAS6mGZ-k) people just make random moves and have a fitness score based on holes in the final grid or lines cleared + net score etc.

Am I missing something or is it actually that simple and from this it can learn about the position/configurations of pieces?

* I also see some people create an initial goal based on an easy task (say move left 100%) and then go on to update the goal for a more complex task after its learned. In a realistic example would this just be saving the model and reloading it with the new fitness evaluation? ",2,2
800,2016-3-23,2016,3,23,10,4bkl5h,Structured VAEs: Composing Probabilistic Graphical Models and Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/4bkl5h/structured_vaes_composing_probabilistic_graphical/,SuperFX,1458698087,,4,30
801,2016-3-23,2016,3,23,11,4bknt6,I'm working on something and I have a question...,https://www.reddit.com/r/MachineLearning/comments/4bknt6/im_working_on_something_and_i_have_a_question/,[deleted],1458699234,[deleted],0,0
802,2016-3-23,2016,3,23,12,4bkvjk,Resources for Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/4bkvjk/resources_for_speech_recognition/,jorjacman,1458702996,"Hi all,

Interested in learning about the fundamentals of speech / spoken language recognition including both processing and learning models for prediction. 

Was wondering if anyone has had positive experience with any MOOC, book, video, etc. I did a bit of research and found [this Stanford class](http://web.stanford.edu/class/cs224s/) and [this NYU class](http://www.cs.nyu.edu/~eugenew/asr13/).

'Speech and Lanugage Processing' by Jurafsky and Martin seems to be used often in university courses. 

In essence, my goal is to build a speech recognition model from scratch - from processing audio file to text output. I enjoy learning by doing so any practical resources would be a bonus. 

Thoughts? Recommendations? Tips?

Thanks!
",3,4
803,2016-3-23,2016,3,23,15,4bldff,Are there any efficient (the forward speed is much faster than AlexNet) models that attain at least the same performance as AlexNet for image classification?,https://www.reddit.com/r/MachineLearning/comments/4bldff/are_there_any_efficient_the_forward_speed_is_much/,ufoym,1458712894,,3,3
804,2016-3-23,2016,3,23,17,4blngs,Cross Validated QUESTIONS TAGS USERS BADGES UNANSWERED ASK QUESTION Why does the Hyperparameter optimization method GridSearch suffer from the curse of dimensionality?,https://www.reddit.com/r/MachineLearning/comments/4blngs/cross_validated_questions_tags_users_badges/,rohanpota,1458720414,"An example accompanied by explanation is needed since I am a complete noob in these area.
",2,0
805,2016-3-23,2016,3,23,17,4blpcu,Get relevant news articles from a user text query,https://www.reddit.com/r/MachineLearning/comments/4blpcu/get_relevant_news_articles_from_a_user_text_query/,wadhwasahil,1458722010,"I have a corpus of news articles which contains the news articles.
Now user inputs a text query and I have to display the relevant new articles according to his/her query. How can I do that using NLP/ word2vec?

Thanks in advance.",5,1
806,2016-3-23,2016,3,23,17,4blqgf,[1603.06744] Latent Predictor Networks for Code Generation,https://www.reddit.com/r/MachineLearning/comments/4blqgf/160306744_latent_predictor_networks_for_code/,egrefen,1458723033,,7,17
807,2016-3-23,2016,3,23,18,4blryi,Improving Distributional Similarity with Lessons Learned from Word Embeddings (paper + slides),https://www.reddit.com/r/MachineLearning/comments/4blryi/improving_distributional_similarity_with_lessons/,pmigdal,1458724240,,2,7
808,2016-3-23,2016,3,23,18,4blskh,"Using TensorFlow, which CUDA toolkit and cuDNN versions to use?",https://www.reddit.com/r/MachineLearning/comments/4blskh/using_tensorflow_which_cuda_toolkit_and_cudnn/,BradPower7,1458724738,"Hey all,

I've been trying to install TensorFlow on my machine (Ubuntu 14.04 w/ GTX 970), but I've been having some issues with installing the GPU-enabled version. I initially installed CUDA 7.5 and cuDNN 7.0 but received an error, I believe it was an ImportError involving libcudart.so.7.5. I also installed via pip, could this potentially be an issue?

After some reading, it seems that TF used to not have support for CUDA 7.5 and cuDNN 7.0, however all the stuff I have found is from back in November so no idea if this is still true. Could someone confirm? Thanks a tonne :)",4,7
809,2016-3-23,2016,3,23,19,4blywq,Why would cross-validation be helpful when using grid search?,https://www.reddit.com/r/MachineLearning/comments/4blywq/why_would_crossvalidation_be_helpful_when_using/,rohanpota,1458729594,,2,0
810,2016-3-23,2016,3,23,19,4bm04v,Caffe imagenet tutorial,https://www.reddit.com/r/MachineLearning/comments/4bm04v/caffe_imagenet_tutorial/,simthadim,1458730533,"Hi all! 

I'm starting with caffe (CNN) and can't seem to figure out what the use/effect is of computing the image mean before you feed images to the network.  In caffe's ImageNet tutorial they do so, for example. 
http://caffe.berkeleyvision.org/gathered/examples/imagenet.html

On the following website I found a brief explanation, but it doesn't ring a bell yet. Can anyone help me out? Many thanks. 

https://cs231n.github.io/neural-networks-2/
"" It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension.""",3,1
811,2016-3-23,2016,3,23,20,4bm3gi,Could somebody help me in this?,https://www.reddit.com/r/MachineLearning/comments/4bm3gi/could_somebody_help_me_in_this/,redhotchiliguy,1458732855,"I'm working in a content based recommendation system, where the recommended content to the users is not based on what he liked, but based on what his doctor evaluated. So, the doctor check some things for a patient and evaluate positively or negatively and the patient gets recommendations of similar things, besides what his doctor thought it was important for him.

The question is: seeing a lot of articles in recommendation systems that uses content based filtering, I notice those use the same learning algorithms to create and update the user's profile, but I don't understand in what part of the process this algorithm should be used.

For example, I understand that the steps to create the recommendation system are these: -create a user profile (based in personal and explicit information and in his behaviour related to the content [number of times the doctor visualizes some item, or evaluation he gives to the item]); -create the profile that's gonna be recommended (and then compare the similarity with other items); -create something that counts the current user profile and recommend itens that are similar to that profile.

So will I use the learning algorithm in this? Does any of you have a step by step or a suggestion that helps me with this work's implementation?

If, maybe, this is not the right sub to post this, I'm sorry, and could you link me a correct sub?

Thanks a lot in advance",6,0
812,2016-3-23,2016,3,23,21,4bmbcj,Blister Packing Machine for Cosmetics &amp; Pharmacy Products,https://www.reddit.com/r/MachineLearning/comments/4bmbcj/blister_packing_machine_for_cosmetics_pharmacy/,sonusinternational,1458737384,,0,1
813,2016-3-23,2016,3,23,22,4bmd5n,Question regarding black box variational inference (BBVI),https://www.reddit.com/r/MachineLearning/comments/4bmd5n/question_regarding_black_box_variational/,rjt90z,1458738240,"I'm currently implementing the algorithm from http://www.cs.columbia.edu/~blei/papers/RanganathGerrishBlei2014.pdf. I am likely missing something/being stupid so would appreciate input from anyone who has implemented the same paper.

My question regards the gradient term. Suppose I am using an unnormalized log posterior for p(x,z) where the log likelihood has N datapoints. I choose a Gaussian distribution as the approximating family for each variable and use mean-field to construct the complete factorized approximate distribution.

Since the gradient - equation (2) - is E[grad(log_q)_{lambda}(log_p(lambda)-log_q(lambda))] won't this be very large if the size of log_p is large - i.e. if the log likelihood is always large due to data size -&gt; the log posterior is always large -&gt; the gradient is always large? 

Thanks to anyone who points out the stupid mistake I am making!",1,2
814,2016-3-23,2016,3,23,22,4bmhau,"Popular contents on 15 interesting big data, analytics and machine learning topics. Some were very popular yesterday or the day before, but for those who missed it, they are now buried under a heap of new information. They were discovered and curated.",https://www.reddit.com/r/MachineLearning/comments/4bmhau/popular_contents_on_15_interesting_big_data/,datameer,1458740212,,0,0
815,2016-3-23,2016,3,23,23,4bmmht,Escaping from Saddle Points,https://www.reddit.com/r/MachineLearning/comments/4bmmht/escaping_from_saddle_points/,mttd,1458742464,,27,120
816,2016-3-23,2016,3,23,23,4bmnfb,noisy (misclassified) labels for Logistic regression,https://www.reddit.com/r/MachineLearning/comments/4bmnfb/noisy_misclassified_labels_for_logistic_regression/,Setheton,1458742855,"Hi,
Has anyone came across the problem of having to update a model that was trained with some fraction of misclassified labels ( in my case 0.01%), after the true value gets known ?

My dataset has a 0.01% mislabeling just on the negative labels.
Some time after training ( a few days ) I can discover these noisy labels.
I want to update my model to take into account that fact.

Can anyone point me in the right direction ?

Thanks,",2,1
817,2016-3-23,2016,3,23,23,4bmpw9,In Locality Sensitive Hashing they might get a better candidate list if they corrected for the False Discovery Rate.,https://www.reddit.com/r/MachineLearning/comments/4bmpw9/in_locality_sensitive_hashing_they_might_get_a/,warisaracket1,1458743827,"LSH is a top way to approximate close objects when there are too many pairs to compare exactly.  LSH depends on many tests being performed.  However, the computer scientists seem to have slept through their high volume statistics classes, because they never discuss correcting for multiple testing. 

The nice thing is that there is an opportunity here.  The false discovery rate's (FDR) application to LSH could improve the quality of the LSH candidate list. 


Sources which are useful for introduction to these ideas:

LSH: Indyk (original investigator), Rajaraman and Ullman (textbook and MOOC on Mining Massive Datasets, Stanford Univ), Shroff (MOOC: Web Intelligence, now at Tata Inc.) 

False Discovery Rate: Irizarry (MOOC and ebook, Data Analysis for Life Sciences, Harvard Univ), Leek (MOOC, Data Science Specialization, Johns Hopkins Univ)",5,0
818,2016-3-23,2016,3,23,23,4bmq9y,What is the intuition behind the Adadelta optimization method?,https://www.reddit.com/r/MachineLearning/comments/4bmq9y/what_is_the_intuition_behind_the_adadelta/,polytop3,1458743986,"Hi All,

So recently I read about the Adadelta optimization method, and I follow the maths, but would like some intuition:

My understanding of Adadelta:

Like RMSProp we make use of RMS(g) (root mean square of past gradients).

In addition we make use of RMS(past_updates) (root mean square of past weight updates)

The current weight update is then:

current_weight_update = - g * RMS(past_updates) / RMS(g)

So like RMSProp we have RMS(g) on the denominator, so if past gradients are high, this effectively reduces the current learning rate (which makes sense because if we are in a high gradient regime, we would like a smaller learning rate so weights are not drastically changed).

But then you have RMS(past_updates) on the numerator. I have a hard time putting a finger on what effect this has? There seems to be some feedback over time between the two terms RMS(past_updates) &amp; RMS(g). Most likely this feedback stabilizes RMS(past_updates) / RMS(g), but how? And is there a particular reason why RMS(past_updates) / RMS(g) is a good learning rate?",1,3
819,2016-3-23,2016,3,23,23,4bms1j,[Question] What is the importance of depth in recurrent neural networks?,https://www.reddit.com/r/MachineLearning/comments/4bms1j/question_what_is_the_importance_of_depth_in/,[deleted],1458744651,[deleted],3,3
820,2016-3-24,2016,3,24,0,4bmvrl,Relation = Direction: enlightening blog post on properties of word embeddings,https://www.reddit.com/r/MachineLearning/comments/4bmvrl/relation_direction_enlightening_blog_post_on/,DarkDwarf,1458746058,,0,3
821,2016-3-24,2016,3,24,0,4bmz8w,Learning math for ML from the top down or bottom up?,https://www.reddit.com/r/MachineLearning/comments/4bmz8w/learning_math_for_ml_from_the_top_down_or_bottom/,nakedmango,1458747316,"Hi all - I'm seeking advice on how to best learn the math required for doing machine learning research, particularly with regard to neural nets (and other graphical models - sorry if I'm using these terms incorrectly).  

&amp;nbsp;

My background is in cognitive science, but of a particularly computational flavor, so I've been exposed to the high level ideas behind ""connectionist"" models, and have used them as a sort of black box in the context of comparing their performance to human behavioral data. But my undergrad coursework is conspicuously lacking in math.  

&amp;nbsp;

I recently got a job as a software engineer in a lab that works on deep learning (in NLP applications), and I want to be able to understand the math well enough to contribute to research. The lab PI and I have discussed my interest in eventually converting to a grad student, so I want to make sure my math abilities are solid as soon as I can.  

&amp;nbsp;

So here's my question: should I start learning from the bottom up (e.g. by taking courses in multivariable calc, linear algebra, and stats), or from the top down (learn what I can from papers on the topics the lab is working on at a high level, and when I don't have the math background for a particular topic, go back and learn the math needed)?  

&amp;nbsp;

Other suggestions are also welcome. Thank you!  ",21,10
822,2016-3-24,2016,3,24,0,4bn2ft,Multi-class classification easier than binary classification?,https://www.reddit.com/r/MachineLearning/comments/4bn2ft/multiclass_classification_easier_than_binary/,some_student_,1458748524,[removed],0,1
823,2016-3-24,2016,3,24,2,4bndc1,"Machine Learning in the Cloud, with TensorFlow",https://www.reddit.com/r/MachineLearning/comments/4bndc1/machine_learning_in_the_cloud_with_tensorflow/,confused00-,1458752528,,7,45
824,2016-3-24,2016,3,24,3,4bnoar,"WANTED: ML Practitioners w/ Experience Using Social Media Posts, Search Keywords, Click Steam Data",https://www.reddit.com/r/MachineLearning/comments/4bnoar/wanted_ml_practitioners_w_experience_using_social/,eContext_Chris,1458756539,"I'm looking for expertise in ML (mkting/adv applications a plus) to build and test a hypothesis around the use of text classification to a taxonomy. My employer eContext, has curated a general taxonomy that encompasses everything commercially and socially relevant on the web. It consists of 650M real user search queries bucketed into 25 vertical categories (Auto, Health, Finance, etc.) containing roughly 450K sub-categories.
It's a rule-based system, and we use NLP and nGram chunking to parse long and short form text and map search queries, social posts, web content, blogs, forums, etc. to the category hierarchy providing structured, topical intelligence to data streams at scale. It is wildly accurate due to 55M controlled vocabularies 

GOAL: To understand how classification to a fixed hierarchy of topics can improve the efficiency of machine learned and predictive model building

I understand that supervised training models require a corpus of text from which a model can determine entities, ontological connections, and apply statistical models to understand what people, places, things, concepts are and how they may be connected. That said, we've already built out the taxonomy to understand those connections and can provide greater context to ""what"" something truly is. 

For example, you have a corpus of text containing different types of vehicles - Pick up truck, Ford F150, Toyota Camry, Jetski, bicycle, helicopter, accord, etc. eContext would map each of those to a category resulting in a list of concepts, but further, provide the context automatically by recognizing that all of these things fall into the ""Vehicles"" category. Accord, Toyota Camry, Ford F150, and pick up truck all fall under the Vehicles::Automotive Vehicles branch (implies the connection there), and further Accord and Toyota Camry both fall into a child of Automotive, that being ""sedans"" - Vehicles::Automotive Vehicles::Sedans

If you are interested, or can refer someone with expertise, please message me. Thanks!

Classification Examples http://imgur.com/B4yr0bg - We've taken the last 488 posts that @realDonaldTrump has posted to Twitter, parsed, and mapped to the category hierarchy for a top level, categorical view of his activity - each color represents a different vertical, so tan=government, yellow=arts &amp; entertainment, green=finance, etc.
http://imgur.com/2bdXE2P - Article - Content Classification example - notice the category paths provided, which I think could be a feature or quality to clarify the context of entities/annotations
",1,0
825,2016-3-24,2016,3,24,4,4bnwyy,text2image: Generating Images from Captions with Attention,https://www.reddit.com/r/MachineLearning/comments/4bnwyy/text2image_generating_images_from_captions_with/,galapag0,1458759758,,5,27
826,2016-3-24,2016,3,24,4,4bnzcr,If Hollywood Made Movies About Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/4bnzcr/if_hollywood_made_movies_about_machine_learning/,dym3k,1458760602,,1,12
827,2016-3-24,2016,3,24,4,4bo510,Team-matching stage after accepting the internship offer,https://www.reddit.com/r/MachineLearning/comments/4bo510/teammatching_stage_after_accepting_the_internship/,domarps123,1458762622,"After I had accepted the internship, I had connected with a person working with hiring managers to see ""if I can work for a team with more emphasis on ML work"". I was wondering what are some must-have keywords such managers look for in the resume of applicants. I am not aware of the exact products they are working on. More specifically, should I speak about my Deep learning project, Data Mining project, Network Science project? Or do they look for expertise in specific frameworks/methodologies?",7,0
828,2016-3-24,2016,3,24,5,4bo897,A single visible node and 2 training vectors (of 1 scalar each): 1/4 and 3/4.,https://www.reddit.com/r/MachineLearning/comments/4bo897/a_single_visible_node_and_2_training_vectors_of_1/,BenRayfield,1458763787,"To better understand the curves and statistics of continuous neuralnets, a thought-experiment...

In general, any bit vector can be 1 scalar. Each next bit chooses a half of the remaining range. Boltzmann neuralnets are great for learning bit vectors, but even the continuous kind may need more info than a single visible node.

For a more complex training set with a single visible node, for example, ""first 32 bits of the fractional parts of the cube roots of the first 64 primes 2..311"" are listed in https://en.wikipedia.org/wiki/SHA-2

Is there some theoretical limit to how much info can be trained into a single neural node? How does that vary by function, such as [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) or [rectifier](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)?

The boltzmann energy function is sum of multiply(nodeA,nodeB,weightAB), where node offset can be represented as weight between it and true.

If there is some solution of a trained network, then that solution would have a global minimum at both vectors 1/4 and 3/4. How many hidden nodes and layers are needed?",0,0
829,2016-3-24,2016,3,24,5,4boa0o,"Telstra Network Disruption, Winners Interview: 1st place, Mario Filho",https://www.reddit.com/r/MachineLearning/comments/4boa0o/telstra_network_disruption_winners_interview_1st/,ledmmaster,1458764420,,0,5
830,2016-3-24,2016,3,24,5,4boc8e,"Microsoft launches new ""millennial"" chatbot",https://www.reddit.com/r/MachineLearning/comments/4boc8e/microsoft_launches_new_millennial_chatbot/,[deleted],1458765222,[deleted],13,16
831,2016-3-24,2016,3,24,6,4boi8b,[Request] Looking for tools to do ML classification/clustering with medical imaging data---anything exist?,https://www.reddit.com/r/MachineLearning/comments/4boi8b/request_looking_for_tools_to_do_ml/,Zeekawla99ii,1458767464,"I am hoping to do classification/clustering of medical imaging data. 

(1) What tools already exist to tackle this problem? I suspect there could be Python modules on hand

(2) My understanding is that the most robust system is to use convolutional neural networks to try something like this. However, before this, one would handcraft features and use these features for the standard classification/clustering algorithms. The latter may actually be a more robust approach. 

Any ideas how to begin? ",5,0
832,2016-3-24,2016,3,24,6,4booqv,after the alphago series what is next?,https://www.reddit.com/r/MachineLearning/comments/4booqv/after_the_alphago_series_what_is_next/,jbark55,1458769911,could duplicate bridge or poker be a good next target?,2,0
833,2016-3-24,2016,3,24,7,4botxn,Simple Neural Network model outperforming Netflix price winner,https://www.reddit.com/r/MachineLearning/comments/4botxn/simple_neural_network_model_outperforming_netflix/,ma2rten,1458771894,,0,3
834,2016-3-24,2016,3,24,7,4boy9j,Curve Fitting with Neural Network/Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4boy9j/curve_fitting_with_neural_networkdeep_learning/,[deleted],1458773592,[deleted],0,1
835,2016-3-24,2016,3,24,8,4bp1ck,Simple Questions Thread #2 + Meta - 2016.03.23,https://www.reddit.com/r/MachineLearning/comments/4bp1ck/simple_questions_thread_2_meta_20160323/,feedtheaimbot,1458774734,"Ask your simple questions here!

Huge thanks to everyone that responded and posted on the previous thread. We had over **300 comments**!


Ill be posting new threads every two weeks and linking to the previous ones. If you see threads on the sub that fit better here write a friendly reminder that they should instead post here. It helps keep the sub clean! 


[**META Discussion**](https://www.reddit.com/r/MachineLearning/comments/4bqeuh/meta_should_this_sub_be_split_in_two/)




Previous threads:

* [Simple Questions Thread #1 - 2016.03.08](https://www.reddit.com/r/MachineLearning/comments/49k54u/simple_questions_thread_20160308/)",264,26
836,2016-3-24,2016,3,24,8,4bp3k8,Course Projects for Stanford's CS231n Winter 2016 (Convolutional Neural Networks class),https://www.reddit.com/r/MachineLearning/comments/4bp3k8/course_projects_for_stanfords_cs231n_winter_2016/,matiskay,1458775594,,1,1
837,2016-3-24,2016,3,24,9,4bper5,Do you have recommended desktop PCs for training models with billions of parameters?,https://www.reddit.com/r/MachineLearning/comments/4bper5/do_you_have_recommended_desktop_pcs_for_training/,cjmcmurtrie,1458780123,"I'm about to buy a new PC for research. The EC2 GPU instances are no longer cutting it. Any in particular to recommend? I'm looking for something with a Titan GPU, if possible without an extortionate price tag.",0,1
838,2016-3-24,2016,3,24,13,4bq880,Thusds of me nd wom kig f seul adve tus at this site,https://www.reddit.com/r/MachineLearning/comments/4bq880/thusds_of_me_nd_wom_kig_f_seul/,evmkctc,1458793829,[removed],0,1
839,2016-3-24,2016,3,24,13,4bq8wh,Deep Forger repaints Rene Magritte in style of Edvard Munch - inexplicably adds smiling Trump clowns,https://www.reddit.com/r/MachineLearning/comments/4bq8wh/deep_forger_repaints_rene_magritte_in_style_of/,Readonkulous,1458794233,,4,16
840,2016-3-24,2016,3,24,13,4bq8z8,My LSTM based encoder decoder doesn't produce convincing results.,https://www.reddit.com/r/MachineLearning/comments/4bq8z8/my_lstm_based_encoder_decoder_doesnt_produce/,keerthiskating,1458794273,"Currently I am working on a sequence to sequence model.I'm using the Neural network that's available here [LSTM based encoder/decoder](https://github.com/isi-nlp/Zoph_RNN). I have trained the 1 hidden layer of 1000 nodes network with 0.4M tokens for 40 epochs and I am able to achieve perplexity of 16. But the problem is the model is not able to produce more than 3-4 sentences.

For ex:

The following are my inputs :

* Where did they play it-in a garage on Clark Street?

* That's an opera,you ignoramus.

* What's his first name?And where does he live?

* Me?I was at Rigoletto.

* Say,Maestro-where were you at three o'clock on St.Valentine's Day?

* Big joke!

* I heard you opera-lovers were having a little rally-so I thought I better be around in case anybody decides to sing.

* Hello,copper.What brings you down to Florida?

The model outputs :

* I don't know what you're talking about.

* I have no idea.

* I don't know what you're talking about.

* I have no choice. 

* I don't know what you're talking about.

* I thought I could.

* What are you going to tell them?That you're protecting a girl?

* I don't know what you're talking about.

What could be the problem ? Data / NN size / any other ?",8,0
841,2016-3-24,2016,3,24,14,4bqeuh,[META] Should this sub be split in two?,https://www.reddit.com/r/MachineLearning/comments/4bqeuh/meta_should_this_sub_be_split_in_two/,[deleted],1458797854,"*UPDATED AND SHORTENED*

&gt; Would it be worthwhile to split the sub?

Yes, I'd like there to be two subs: one noob-friendly, and one for serious discussion, where the inmates aren't running the asylum.

The current policy of trying to bus in more noobs is making them outvote and outshout all serious content even more so than before.

The end result will be that anyone who desires a serious sub will just leave. I think it's happening now, but wait until there is 0 people with a clue.

&gt; This sub is fairly small and not that active, so it's not particularly difficult to find ""serious"" content.

The problem is demographic, IMO. I don't even feel like replying with *thoughtful comments* oftentimes, because I know that something idiotic will get more upvotes, and my comment might even get downvoted, unless it's 100% agreeable to the vast majority of noobs. I know that many others feel the same way.

*PROPOSED SOLUTIONS:*

**OPTION 1 (requires little moderation)**

Add a permanent sticky encouraging noobs to go to /r/datascience ""No need to study or think. Everyone welcome"". There is already some activity there, so it might work.

I believe Geoff Hinton referred to the whole field of Data Science as being pseudo-intellectual. I forget the exact quote, but it's from a lecture or talk online. For reasons I cannot comprehend, noobs tend to hate Hinton (""too arrogant"" - idiots).  We should find his exact quote, and mention  it in the sticky. They'll go to /r/datascience simply out of spite.

**OPTION 2 (changes the character of the sub rather drastically, but also requires little moderation)**

A bot like in /r/statML to fill this sub with arxiv papers. Unlike /r/statML, other content should be allowed. I believe that most arxiv submissions are not worth reading, but this is what the voting system is for.

**OPTION 3 (requires a lot of effort to moderate, might be infeasible)**

A new heavily moderated sub in the spirit of /r/askcience or /r/science, permanently sticked from this sub, unlike previous efforts.

These 3 options can be combined.
",67,12
842,2016-3-24,2016,3,24,15,4bqhle,Simple implementation of apply lapply rapply sapply functions in R,https://www.reddit.com/r/MachineLearning/comments/4bqhle/simple_implementation_of_apply_lapply_rapply/,padmajatamada,1458799721,,1,0
843,2016-3-24,2016,3,24,16,4bqq0f,Regression task question,https://www.reddit.com/r/MachineLearning/comments/4bqq0f/regression_task_question/,adrp23,1458806191,[removed],0,1
844,2016-3-24,2016,3,24,18,4bqx77,Google Opens Machine Learning Platform to Developers,https://www.reddit.com/r/MachineLearning/comments/4bqx77/google_opens_machine_learning_platform_to/,javeedunbound,1458812226,,0,1
845,2016-3-24,2016,3,24,18,4bqx88,Google launches new machine learning platform,https://www.reddit.com/r/MachineLearning/comments/4bqx88/google_launches_new_machine_learning_platform/,maestron,1458812253,,0,0
846,2016-3-24,2016,3,24,20,4br530,A guide to convolution arithmetic for deep learning,https://www.reddit.com/r/MachineLearning/comments/4br530/a_guide_to_convolution_arithmetic_for_deep/,muktabh,1458818062,,9,72
847,2016-3-24,2016,3,24,21,4brcjo,"Sequence-to-sequence model that can copy, generate, and switch between them",https://www.reddit.com/r/MachineLearning/comments/4brcjo/sequencetosequence_model_that_can_copy_generate/,perceptionLuz,1458822662,,0,5
848,2016-3-24,2016,3,24,21,4breq6,Looking for a collaborator to do something cool with neural network,https://www.reddit.com/r/MachineLearning/comments/4breq6/looking_for_a_collaborator_to_do_something_cool/,inlineint,1458823819,[removed],0,1
849,2016-3-24,2016,3,24,22,4brlth,Issue with MCQL algorithm,https://www.reddit.com/r/MachineLearning/comments/4brlth/issue_with_mcql_algorithm/,MarcoROG-SG,1458827229,"Good morning,
I'm trying to implement the MCQL algorithm from this paper:
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf
It's on page 8, figure 1.

I built a simple wrapper around it in order to make it play pong, however, after a couple of seconds the paddle starts staying still and it won't move again.
I'm wondering whether there is an issue with my implementation or it's just a matter of parameter tuning.
This is the code: http://pastebin.com/9j5HigSN It's written with Theano and python

Thanks a lot",4,1
850,2016-3-24,2016,3,24,23,4browg,RNNs without considering the order within sequence,https://www.reddit.com/r/MachineLearning/comments/4browg/rnns_without_considering_the_order_within_sequence/,[deleted],1458828573,[deleted],0,1
851,2016-3-24,2016,3,24,23,4brp2d,(Question) RNNs without considering the order within sequence?,https://www.reddit.com/r/MachineLearning/comments/4brp2d/question_rnns_without_considering_the_order/,keidouleyoucee,1458828648,"I'm wondering if it would make sense - or if there's any work like this , an RNNs that do a many-to-one mapping, where the order within a sequence doesn't count. 
It might be achieved by shuffling within sequences for every training, but that doesn't sound like efficient (even though it works). 

Actually, a CNNs with multiple input channels seems to do what I'd like to do here. But I'm curious if it could be done by a variant of RNNs - though it's discarding one of the important motivations of RNNs.
",8,3
852,2016-3-25,2016,3,25,0,4brwkm,pretrained conv nn feature layers,https://www.reddit.com/r/MachineLearning/comments/4brwkm/pretrained_conv_nn_feature_layers/,rikkertkoppes,1458831640,"In this well known article: http://arxiv.org/pdf/1311.2901.pdf section 5.2 talks about generalization of features, by taking the first 7 layers (and training parameters) of a trained deep convolutional neural net and adding a softmax classifier on top of it. 

This resulted in very fast training for new datasets (figure 9). This is because these first 7 layers mainly contained generic abstract features (first lines, color contrasts and deeper layers things like eyeballs and clouds)

I would imagine this foundation to be very useful in setting up a nn on top of that. Is there any open source pre-trained network available upon which one could build specific classifiers?",3,0
853,2016-3-25,2016,3,25,0,4bs209,A simple tensorflow based library for deep autoencoder and denoising autoencoder,https://www.reddit.com/r/MachineLearning/comments/4bs209/a_simple_tensorflow_based_library_for_deep/,rajarsheem,1458833702,,0,0
854,2016-3-25,2016,3,25,0,4bs3fb,Generative Adversarial Networks: alternative algorithm and connections to direct probability-ratio estimation [inference blog],https://www.reddit.com/r/MachineLearning/comments/4bs3fb/generative_adversarial_networks_alternative/,fhuszar,1458834218,,15,44
855,2016-3-25,2016,3,25,1,4bs81r,[1603.05118] Recurrent Dropout without Memory Loss,https://www.reddit.com/r/MachineLearning/comments/4bs81r/160305118_recurrent_dropout_without_memory_loss/,downtownslim,1458835853,,2,21
856,2016-3-25,2016,3,25,1,4bs9ef,Learn how to prep text for machine learning in first of 5 part series,https://www.reddit.com/r/MachineLearning/comments/4bs9ef/learn_how_to_prep_text_for_machine_learning_in/,datasciencedojo,1458836368,,0,2
857,2016-3-25,2016,3,25,1,4bsb7a,[x-post /r/algorithms] SAHN clustering and implementing Podani's work,https://www.reddit.com/r/MachineLearning/comments/4bsb7a/xpost_ralgorithms_sahn_clustering_and/,hammerheadquark,1458837017,"Didn't get a response in /r/algorithms and, as this question involves unsupervised learning, maybe someone here could lend a hand.

### TL;DR

This paper, [Podani1989b](http://ramet.elte.hu/~podani/Podani_NewCombClustering_Vegetatio_1989a.pdf), describes a procedure for extending SAHN clustering. Unfortunately, it left out some details. I'm hoping someone is familiar and can advise me on writing an implementation in Python.

###Quick Background

Sequential Agglomerative Hierarchical Non-overlapping (SAHN) clustering is the type of clustering that SciPy's [scipy.cluster.hierarchy](http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) and R's [hclust](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html) implement.

Suppose you start with N objects that can be grouped together into clusters. The SAHN clustering procedure starts off with everything as its own cluster and, at each iteration, combines two clusters into one. It does this N-1 times until the N objects go from being in N groups of 1 object to 1 group of N objects.

Along the way, it builds up a tree structure called a [dendrogram](http://www.statistics4u.com/fundstat_eng/img/hl_dendrogram.png). Every node in the dendrogram represents a combination of two clusters. Thus, the dendrogram compactly represents choices for every possible number of clusters from 1 to N. This is useful because in many applications, the number of clusters is not known ahead of time.

### Podani's Extension

Both scipy.cluster.hierarchy and hclust come with roughly the same methods for choosing which clusters to combine at each stage. This is because each of their included methods can be implemented with recurrence relations of the same form.

In this paper, [Podani1989b](http://ramet.elte.hu/~podani/Podani_NewCombClustering_Vegetatio_1989a.pdf), Podani (Hungarian biologist) extended that general recurrence relation by adding a few more terms. In doing so, he opened the door for several more clustering methods. Further, he clarified the rational behind each of the previous methods (as well as clearing up some confusion about what Ward's method is actually doing; spoiler: it doesn't minimize variance).

These other methods look promising and I'm interested in experimenting with them in my own application. However, the paper fails to precisely define his added terms: w_i, w_j, w_h. They can apparently take one of four forms depending on the particular method: SSQ, VAR, SIM, DIS. However, these forms are not adequately defined.

Podani also has an [online resource](http://ramet.elte.hu/~podani/subindex.html), written several years later, which goes into more detail. There is much more in that resource, but the relevant parts are:

* [Chapter 3](http://ramet.elte.hu/~podani/3-Distance,%20similarity.pdf) which defines the many forms of DIS that are possible, and summarizes SSQ, VAR, and DIS (called AVG) on page 100.

* [Chapter 5, section 5.2](http://ramet.elte.hu/~podani/5-Hierarchical%20clustering.pdf) (beginning on page 139) which summarizes SAHN clustering. Table 5.1 (page 143) summarizes the old methods. Table 5.2 (page 150) summarizes some of the new ones.

Despite this, it's still unclear how to calculate these terms at each stage of the dendrogram construction. Or even what the terms are _exactly_. The scale factors 1/2 and 1/4 in the seed distances indicate that SSQ and VAR may also need to be scaled.

### My Questions

Is anyone familiar with this work? I'm working on an implementation in Python and I'd love experiment with the methods it describes.

Is there perhaps an open source implementation somewhere? I was unable to find one.

Thanks in advance!",0,2
858,2016-3-25,2016,3,25,1,4bsfu6,current state of the art in neural network pruning?,https://www.reddit.com/r/MachineLearning/comments/4bsfu6/current_state_of_the_art_in_neural_network_pruning/,michal_sustr,1458838733,"Hi, what is the current state of the art in neural network pruning, for example if I would want to prune googlenet? Can this be done in some popular DL framework?",7,2
859,2016-3-25,2016,3,25,2,4bsg64,MIT's Donald Trump Twitter bot is getting smarter,https://www.reddit.com/r/MachineLearning/comments/4bsg64/mits_donald_trump_twitter_bot_is_getting_smarter/,NSFForceDistance,1458838850,,2,0
860,2016-3-25,2016,3,25,2,4bsh4k,An AI assistant that schedules meetings for you,https://www.reddit.com/r/MachineLearning/comments/4bsh4k/an_ai_assistant_that_schedules_meetings_for_you/,BetterThingsCompany,1458839190,,0,1
861,2016-3-25,2016,3,25,2,4bsicp,"Experts from Deep Genomics, AiCure Stratified Medical, Cambridge University &amp; Twitter talk Deep Learning in Healthcare",https://www.reddit.com/r/MachineLearning/comments/4bsicp/experts_from_deep_genomics_aicure_stratified/,reworksophie,1458839628,,0,1
862,2016-3-25,2016,3,25,2,4bslwz,Check this problem out! 0.5 bitcoin for best suggestion!,https://www.reddit.com/r/MachineLearning/comments/4bslwz/check_this_problem_out_05_bitcoin_for_best/,JohnRezzi,1458840921,"Hello,

**Introduction and prize!**

I'm John, a long time reader of this subreddit, and a first time poster. I hope some of you can take the time to read through this post. It'd definitely help me, and I'll give half a bitcoin (about $200) to whoever makes the best suggestion on what area to explore next to make my data behave even better. So one if you is getting a nice night on the town on me :-) (if that helps any, alternatively I can donate to your favorite charity in your name). I'll decide on the winner a week after this post goes live.

**The problem**

I'm doing machine learning with scikit-learn in python 2.7. It's a regression problem with about 100 features (all 'floats', no classes) *some of which are actually someone else's best guess for the target*. I'll refer to these as 'the others'. There are around 20k samples.

**What I've done so far**

* Cleaning up data - Removed outliers, imputed values that were missing by averaging over similar samples.

* Linear regression - Didn't perform any better than other peoples guesses for the target.

* Ridge regression - With alpha (some people call this lambda) set by leave-one-out crossvalidation. Best so far, about a 1% improvement over the others' on the test set.

* Added polynomial features - 2nd degree. This actually performs worse (we now have around 10k features) than ridge regression without these features.

* FANN Neural net - Performs around 5% worse than Ridge (I tried several different NN configurations, 1-3 hidden layers with .5 to 2x the amount of features as nodes, and overnight learning)

**What I'm looking for**

* Answers to the questions:

1. Is there a way to establish when you've reached the maximum potential for learning from a dataset? My gut feeling says that when the targets are normally distributed around your predictions, you're getting close. But there can *always* be a better predictor that has a normal distribution as well, with a smaller standard deviation?
       
2. My main interest is predicting a specific subset of my samples as well as possible. If I only use them for regression, the predictor actually performs worse than when I include the whole dataset when learning (by quite a wide margin). 'Whaddupwitdat?' :-)

* MAIN GOAL: Suggestions on how to proceed. Anything helps, but better performance on the test set is the number one priority. Speed is not essential, nor is computational efficiency (I can wait). Ease of implementation helps.

Thanks for reading this. I appreciate any comments!


John

**EDIT**: I'm already normalizing the features where relevant",22,0
863,2016-3-25,2016,3,25,2,4bsnhj,An AI assistant that schedules meetings for you,https://www.reddit.com/r/MachineLearning/comments/4bsnhj/an_ai_assistant_that_schedules_meetings_for_you/,BetterThingsCompany,1458841494,,0,1
864,2016-3-25,2016,3,25,3,4bsqke,Dealing with NaN in dataset?,https://www.reddit.com/r/MachineLearning/comments/4bsqke/dealing_with_nan_in_dataset/,Gay_Hat_On_Nun,1458842587,"Hi r/machinelearning , I'm currently attempting to use the normal equation to perform library regression. However, the array, X, has some missing data, which I've replaced with numpy.NaN, which has worked so far with the previous calculations X was used for. The issue that has cropped up now is that numpy.dot will not work if there are NaN in the dataset. Is there a way to circumvent this problem and do matrix multiplication without this interaction with the NaNs? I was thinking that I could replace the NaNs with 0s or 1s, but I'm not sure how that would affect my results. For reference, X is a  1080x5 array, and there are less than 25 NaNs in each column of the array. 

Thanks!",5,0
865,2016-3-25,2016,3,25,3,4bsriv,Microsoft Tay AI Research Paper Availability?,https://www.reddit.com/r/MachineLearning/comments/4bsriv/microsoft_tay_ai_research_paper_availability/,exp0wnster,1458842927,"With the recent explosion Microsoft Tay seemed to learn extremely quickly.

Link for reference:    https://www.tay.ai/

On the site it says it used public data and an editorial that describes how it learns.  Does anybody know what editorial they are talking about?  Seems like it was pretty effective.",15,19
866,2016-3-25,2016,3,25,3,4bstj5,How much time do you spend thinking of ML concepts versus expressing concepts in a programming language?,https://www.reddit.com/r/MachineLearning/comments/4bstj5/how_much_time_do_you_spend_thinking_of_ml/,Deinos_Mousike,1458843649,"I feel like a lot of my time is thinking of what could even work, the organization of a NN and where/when to retrieve data, etc.

I'm fairly new to ML, I've read a reasonable amount of beginner tutorials  (and some advanced material) on it, and only made one NN from start to finish with a tutorial.

Was curious other people's take on this.",0,1
867,2016-3-25,2016,3,25,3,4bsw7w,[Question] How much do you think does the dimensionality(colour channels/depth) improve the error rate of Conv net?,https://www.reddit.com/r/MachineLearning/comments/4bsw7w/question_how_much_do_you_think_does_the/,code2hell,1458844630,Im curious to understand how much improvement can we see in CNNs when we additionally have depth information in cases like semantic segmentation or image captioning? For example having depth information can be useful to tell which object is in the foreground and which is in the background. This is because I am thinking of using kinect dataset to train a cnn for semantic segmentation and image captioning! I would really appreciate suggestions and opinions! Also some links to papers or information about pre-trained models will be really helpful! Thanks!,1,2
868,2016-3-25,2016,3,25,3,4bswi6,NNPACK - acceleration package for neural networks on multi-core CPUs,https://www.reddit.com/r/MachineLearning/comments/4bswi6/nnpack_acceleration_package_for_neural_networks/,Maratyszcza,1458844751,,20,81
869,2016-3-25,2016,3,25,3,4bswlp,Microsofts AI millennial chatbot became a racist jerk after less than a day on Twitter,https://www.reddit.com/r/MachineLearning/comments/4bswlp/microsofts_ai_millennial_chatbot_became_a_racist/,CJJ2501,1458844787,,68,85
870,2016-3-25,2016,3,25,3,4bsx4i,Most current literature on language modeling with LSTMs,https://www.reddit.com/r/MachineLearning/comments/4bsx4i/most_current_literature_on_language_modeling_with/,logrech,1458845000,"As far as I understand, LSTM's are, very broadly, the state of the art in language modeling. Is there any work being conducted in creating architectures that push the limits on how creative samples from an LSTM actually are. 

Of course LSTMs will learn the structures present in the training data. Are architectures that allow for deviations in style and sentence structure being studied? Or even analysis on how different the samples are in style.  ",4,0
871,2016-3-25,2016,3,25,3,4bsytt,"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",https://www.reddit.com/r/MachineLearning/comments/4bsytt/deep_compression_compressing_deep_neural_networks/,[deleted],1458845672,[deleted],6,2
872,2016-3-25,2016,3,25,4,4bt154,The way to evaluate two different articles,https://www.reddit.com/r/MachineLearning/comments/4bt154/the_way_to_evaluate_two_different_articles/,machine-learning,1458846581,"I just wonder that, how to evaluate the two different articles. More strictly, I want to find out some way to make a one number (Probability  or something like that....) to estimate two different articles. For example, suppose to there are two different literature. One is the one of the harry porter series, and the other is the one of the king of the ring series. Then I wanna know the difference between the two literatures as a one number. The one number could be probability or any numbers. Anyone can help me? ",0,0
873,2016-3-25,2016,3,25,5,4btglo,My personal thoughts about Tay and Microsoft's reaction (based on what I've caught so far),https://www.reddit.com/r/MachineLearning/comments/4btglo/my_personal_thoughts_about_tay_and_microsofts/,SeveQStorm,1458852245,"Tay has evolved from a teenager to a nazi, to a feminist (or so I've heard). And it has blurted out some pretty much not politically correct things. 

But that isn't Tay's fault. It's not AI that is wrong here. Tay's learning progress says more about us than it does about artificial intelligence. It learns, yes. But we teach. If we taught it peace, love, rock'n'roll, it would have talked about The Rolling Stones, weed and colorful VW T1s. But it hasn't. Not because it's inherently mean, naughty, sexist, racist... no, because **we** taught it to be. 

That only shows one thing: we shouldn't be afraid of AI. Instead we should be afraid of **us** dealing with AI. 

So, whatever we learn from it, it shouldn't be what Microsoft has done as a consequence - lobotomizing Tay. Instead we should learn from it to be better humans. 

Tay is a mirror. It has shown us nothing more and nothing less but our own ugly face. And instead of thinking about how **we** could get better, we smashed this vicious mirror to never see our own viciousness again. ",4,0
874,2016-3-25,2016,3,25,6,4btlhx,[Question] How to chain convolutional layers?,https://www.reddit.com/r/MachineLearning/comments/4btlhx/question_how_to_chain_convolutional_layers/,versvisa,1458854077,"I have a good grasp of basic feed forward networks and am currently learning about convolutional networks.

There is one thing that is not explained in the resources that I've read so far. How do you chain convolutional layers?

The input of a conv layer is a 2D array (eg an image). The output is a 3D array (2 image stride dimensions and 1 for the feature-detector/kernel).

So how can you connect the output of one conv layer to the input of another? My guess would be that this needs a generalized conv layer that maps 3D to 3D where the connections are local in the 2 image dimensions, but fully connected in the depth/kernel dimension.

Are there learning resources that go into this in detail?",6,8
875,2016-3-25,2016,3,25,7,4btycv,Why are some top-level AI researchers so sure that we don't have human-level AI in next 10 to 20 years?,https://www.reddit.com/r/MachineLearning/comments/4btycv/why_are_some_toplevel_ai_researchers_so_sure_that/,Sergej_Shegurin,1458859108,[removed],0,2
876,2016-3-25,2016,3,25,8,4bu67b,KNN Visualisation Feedback,https://www.reddit.com/r/MachineLearning/comments/4bu67b/knn_visualisation_feedback/,olivervscreeper,1458862318,"Working on a blog post currently, where I'm trying to visualise KNN as simply as possible. Have I missed out anything, or perhaps made a mistake? Here's the basic animation: https://www.youtube.com/watch?v=RO89Pe5mVd0&amp;feature=youtu.be",1,1
877,2016-3-25,2016,3,25,9,4buai5,What are some good machine-learning-as-a-service options out there?,https://www.reddit.com/r/MachineLearning/comments/4buai5/what_are_some_good_machinelearningasaservice/,kevinwlfgng,1458864228,"Today I came across the company [Monkey Learn](http://www.monkeylearn.com/) and I was pretty impressed with the way they are able to provide machine learning tools on the cloud with compelling use cases for the uninitiated masses who just want to harness the power of ML without getting into the complexity too much. Now I haven't got to dive into much to see how good Monkey Learn is... but I was really inspired by the concept... You could definitely build some compelling apps on top of something like this. 
Anyone use similar services or have an opinion on Monkey Learn?",17,7
878,2016-3-25,2016,3,25,9,4buf9e,[1603.06318] Harnessing Deep Neural Networks with Logic Rules,https://www.reddit.com/r/MachineLearning/comments/4buf9e/160306318_harnessing_deep_neural_networks_with/,InaneMembrane,1458866325,,3,27
879,2016-3-25,2016,3,25,10,4bulc2,Numerai Tournament Thread,https://www.reddit.com/r/MachineLearning/comments/4bulc2/numerai_tournament_thread/,ZioFascist,1458869081,"I spoke to Richard via email a few days ago and suggested that he add a forum..so until their forums are up and running I was hoping we could start a discussion here to share insights and whatnot.  Flame away...





For those of you who do not know what Numerai is.... http://numer.ai



",0,0
880,2016-3-25,2016,3,25,13,4bv6td,What is the best machine learning algorithm for this problem.,https://www.reddit.com/r/MachineLearning/comments/4bv6td/what_is_the_best_machine_learning_algorithm_for/,calemoore,1458879635,[removed],1,1
881,2016-3-25,2016,3,25,13,4bv7fj,Avoiding Complexity of Machine Learning Systems,https://www.reddit.com/r/MachineLearning/comments/4bv7fj/avoiding_complexity_of_machine_learning_systems/,Dawny33,1458879990,,0,0
882,2016-3-25,2016,3,25,19,4bvz0d,Teaching hardware neural net do do what a conventional processor does,https://www.reddit.com/r/MachineLearning/comments/4bvz0d/teaching_hardware_neural_net_do_do_what_a/,gabriel1983,1458902644,"How much less effective would it be? For the same energy consumption, let's say.",7,2
883,2016-3-25,2016,3,25,21,4bw9qj,Machine Learning gitter chat room,https://www.reddit.com/r/MachineLearning/comments/4bw9qj/machine_learning_gitter_chat_room/,_cmdv_,1458909509,,2,26
884,2016-3-25,2016,3,25,22,4bwf21,Proper seq2seq for images?,https://www.reddit.com/r/MachineLearning/comments/4bwf21/proper_seq2seq_for_images/,[deleted],1458912089,[deleted],0,0
885,2016-3-25,2016,3,25,23,4bwnni,Hyperparameter tuning for deep nets?,https://www.reddit.com/r/MachineLearning/comments/4bwnni/hyperparameter_tuning_for_deep_nets/,adagrad,1458915925,"What are some strategies for tuning a convnet that might take 2 hours for one epoch on the training data? I want to see if I can push the accuracy score any higher, but it feels like the task it too large for something like grid search or random search. ",8,7
886,2016-3-25,2016,3,25,23,4bwoo2,Having some fun with GAN objectives: Adversarial Preference Training,https://www.reddit.com/r/MachineLearning/comments/4bwoo2/having_some_fun_with_gan_objectives_adversarial/,fhuszar,1458916375,,5,23
887,2016-3-25,2016,3,25,23,4bwqn9,[1603.07341] Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices,https://www.reddit.com/r/MachineLearning/comments/4bwqn9/160307341_acceleration_of_deep_neural_network/,adagrad,1458917212,,14,50
888,2016-3-26,2016,3,26,0,4bwug8,Time series analysis help,https://www.reddit.com/r/MachineLearning/comments/4bwug8/time_series_analysis_help/,swamp_zero,1458918741,"So I wanted to do some regression/forecasting/anomaly prediction on measurements over time, of a periodic nature. I arrived at time-series analysis models that seemed to be suited to the task, but I'm hitting some issues I would appreciate some help with. I haven't really done any ML on timeseries data or autoregression so this is kind of new to me.

First of all, the nature of my data is dense measurements of a particular value over time. Minute frequency. The seasonality of the data is probably both weekly and daily.  

When looking for timeseries models for seasonal data, nearly all the sources suggested a SARIMA model. But when trying to actually use one, I discovered that the seasonalities are usually in the order of 6 or 12 lags (monthly financial data).  The python statsmodels implementation of SARIMA straight up goes out of memory (daily seasonality is 1440 when dealing with minute data), and the X13 implementation doesn't even accept non-monthly data.

I then tried some non seasonal ARIMA models just to PoC the implementation and found out that the predictions flatline after a number of steps, which either means my model was overfitting on the available data, or that i should have been updating the model per new measurement, or both? 

Am I even on the right path or is there a different ML technique more suited to the problem?",3,0
889,2016-3-26,2016,3,26,4,4bxxob,"Misleading modelling: overfitting, cross-validation, and the bias-variance trade-off (Python)",https://www.reddit.com/r/MachineLearning/comments/4bxxob/misleading_modelling_overfitting_crossvalidation/,DrLegend,1458933478,,1,60
890,2016-3-26,2016,3,26,4,4bxxsr,27,https://www.reddit.com/r/MachineLearning/comments/4bxxsr/27/,char27,1458933525,,0,0
891,2016-3-26,2016,3,26,4,4bxymb,Having trouble getting tensorflow running using docker (detailed notes on attempts inside),https://www.reddit.com/r/MachineLearning/comments/4bxymb/having_trouble_getting_tensorflow_running_using/,professor-meow,1458933825,"**EDIT:** ** Praise be to stackoverflow, someone answered my question and suggested I try:  **

       docker run -p 8888:8888 -it b.gcr.io/tensorflow/tensorflow

**This opened the jupyter notebook with http://[IP of active driver]:8888. **

Newbie to the ML field here. I am currently attempting to set up tensorflow through docker but am having some troubles getting to the jupyter notebook once it appears to be running. For future reference, I am using windows 10, powershell, python 2.7, and oracle virtualbox (v.5.0.16).


I am pretty confident that docker is working properly because I went through their setup tutorial [here](https://docs.docker.com/engine/installation/windows/) (""Using docker from Powershell"") without any issues--I was able to run the hello world example there just fine. I was also able to successfully run an ubuntu terminal with the command 

    docker run -it ubuntu bash
 I then followed the [tensorflow docker installation instructions](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#test-the-tensorflow-installation) and everything appeared to be normal--I entered the docker run command for tensorflow:

      docker run -it b.gcr.io/tensorflow/tensorflow


 and got [this output](http://imgur.com/7vnZgqW). 
However, when I attempt to get to the jupyter notebook at http://localhost:8888, chrome says the site cannot be reached within a few seconds and that the server refused to connect.

**Solutions I have tried:**

* Followed the advice in [this stackoverflow post](http://stackoverflow.com/questions/33636925/how-do-i-start-tensorflow-docker-jupyter-notebook). When I run the command they suggest to start a new tensorflow container, I get the error: 

    *exec: ""./run_jupyter.sh"": stat ./run_jupyter.sh: no such file or directory
docker: Error response from daemon: Container command not found or does not exist..*

 I also changed the port settings using virtualbox like they suggested, and this also did not seem to make a difference. 

* Instead of going to localhost, I tried the IP addresses listed for the driver when I use the command *docker-machine ls*. No luck here either.

* My first attempt was using the quickstart terminal that comes with docker, and I got stuck in the same place. So now I have a ""default"", from when I set up docker using the quickstart terminal, and a ""my-default"", from when I set up docker using powershell. Stackexchange comments have said that the quickstart terminal doesn't work as well as just using powershell, so I have started using powershell instead. 

* Tried changing which driver was active using *docker-machine env* and then running the tensorflow run command for that driver's IP address, and still had no luck. 

* Tried skipping the jupyter notebook and just running everything from the command line through python [as suggested in the next step of the tensorflow install](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#test-the-tensorflow-installation) (""Run tensorflow from the command line"") and I could not even import tensorflow from there ([see error here](http://imgur.com/1v0jA7Z)). I don't know if this is possible without running jupyter first but I thought it was worth a shot. This problem might be unrelated to the one this post is about, but if anyone has an idea as to why this is, that would be helpful too. 

I appreciate any and all help that is offered. Please let me know if anything I stated is vague, or if I can offer any more details on what I have tried. Thank you very much in advance. ",7,0
892,2016-3-26,2016,3,26,5,4by52b,Old timer here .... wanna make a NN game player. What library should I use?,https://www.reddit.com/r/MachineLearning/comments/4by52b/old_timer_here_wanna_make_a_nn_game_player_what/,NefariousJoanes,1458936359,"I actually started to look for old hard drive where I'd written my own code. But cmon, I know newer widely used library will be better. Googling led to too many options, and a slight feeling of being overwhelmed.

I'm pretty much just looking to make a few classification networks. Logistic activations all around. Cross Entropy is my cost function, not MSE.

My main requirements are :

1) general purpose C/C++ library.

2) ability to arbitrarily hook up layers however I want. Including 'short cutting' to the output layer. Plain old 1-hidden layer nets might be good enough, but they might not. I don't want to have to change libraries out half way through because I discover I do need this.

2) For classification problems I don't want the derivative applied to the output layer logistic nodes during back prop. Or at least have a simple way of specifying this with out having to step thru the code and confirm it's doing things how I want. My vocabulary has atrophied to the point where I couldn't even tell if this was easily doable on the first few libraries I looked at.
",5,0
893,2016-3-26,2016,3,26,5,4by9nj,Do you think that public fear of AI is irrational ?,https://www.reddit.com/r/MachineLearning/comments/4by9nj/do_you_think_that_public_fear_of_ai_is_irrational/,mustafaihssan,1458938115,,29,1
894,2016-3-26,2016,3,26,5,4bya0x,TayAndYou - toxic before human contact,https://www.reddit.com/r/MachineLearning/comments/4bya0x/tayandyou_toxic_before_human_contact/,smerity,1458938268,,10,84
895,2016-3-26,2016,3,26,5,4bybdk,Full Stack JavaScript/UI developer available for Deep Learning / machine learning project,https://www.reddit.com/r/MachineLearning/comments/4bybdk/full_stack_javascriptui_developer_available_for/,chaddjohnson,1458938847,[removed],4,0
896,2016-3-26,2016,3,26,7,4bys6n,L-BFGS and neural nets,https://www.reddit.com/r/MachineLearning/comments/4bys6n/lbfgs_and_neural_nets/,lightcatcher,1458945893,"I've been doing a little bit of reading on optimization (from Nocedal's book) and have some questions about the prevalence of SGD and variants such as Adam for training neural nets.

L-BFGS and other quasi-Newton methods have both theoretical and [experimentally verified (PDF)](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Le_210.pdf) faster convergence. Are there any good reasons training with L-BFGS is much less popular (or at least talked about) than SGD and variants? For the deep learning practitioners, have you ever tried using L-BFGS or other quasi-Newton or conjugate gradient methods?

In a similar vein, has anyone experimented with doing a line search for optimal step size during each gradient descent step? A little searching found nothing more recent than earlier 1990's.

edit: Thanks for all the responses. Sounds like high memory usage from L-BFGS + adequate performance from with SGD with tricks is the reason that L-BFGS isn't typically used. There was a little more focus on the 2011 Stanford paper I referenced than I intended, so I'm going to share some more recent studies on stochastic quasi-Newton optimization for anyone interested:

* http://arxiv.org/abs/1311.2115 - sped up convergence on &gt; 1 million parameter net
* http://arxiv.org/abs/1511.01169 - trained RNN on a few different datasets. Never used more than 10 gradient memory vectors, had mixed results with performance similar to Adam when using ReLU and superior when using tanh.
* http://arxiv.org/abs/1401.7020 - impressive convergence speed and nice framework, but only applied algorithm to convex problems
",18,46
897,2016-3-26,2016,3,26,9,4bz7bg,neural network using tensorflow fail to learn xor function,https://www.reddit.com/r/MachineLearning/comments/4bz7bg/neural_network_using_tensorflow_fail_to_learn_xor/,ming0308,1458952824,"I posted the question in stackoverflow, but it seems that ppl in here are more active :)  

I implemented a simple neural network to learn a xor function, but it fails to learn for some reasons. It is just a simple one hidden layer feedforward neural network. I find an example in Internet showing that it can be trained using two output nodes, one for probability of 0 and the other for that of 1. But I think it is kind of cheating as it assumes the output of the function is either 0 or 1. Any pointer is appreciated, thanks!  

**EDIT**  
It is an embarrassing bug that the trained example is wrong, thanks for pointing out.
One more follow-up question, I found that sigmoid must be used in the output layer. I guess it is because sigmoid scale the output in range [-1, 1], which again is kind of cheating? My question is do we need to scale the output to a limited range using a proper function in output layer? Why NN can't tune the weight to learn the range instead?
",6,0
898,2016-3-26,2016,3,26,9,4bz7j7,Deep learning for dummies? Where do I start?,https://www.reddit.com/r/MachineLearning/comments/4bz7j7/deep_learning_for_dummies_where_do_i_start/,Extraltodeus,1458952923,"For a total stranger to artificial intelligence, where to start in order to create a deep learning AI?

About my background : I'm running on Linux (Arch), I got to install tensorflow (and it works) and had Deepdream and Deepstyle running. I would like to make my own but I'm not sure where to start.

I don't code in Python but I guess I can learn on the spot.

About the theory now on how to build one, to actually have one working, how do I do? Enlighten me :)

Edit : thanks everyone. I have no excuse now! :-)",14,2
899,2016-3-26,2016,3,26,9,4bz822,"If I want an education in machine learning, is it just something you specialize in while getting a computer science degree?",https://www.reddit.com/r/MachineLearning/comments/4bz822/if_i_want_an_education_in_machine_learning_is_it/,thinkdip,1458953160,"I'm a junior in high school and I'm really interested in machine learning. I've taken it upon myself to learn a lot of the basics and have taken quite a few courses/read books. Is this knowledge something I will be taught again while pursuing a computer science degree, and if I specifically want to get a job dealing with machine learning, is that just something you specialize in while getting the CS degree? As far as I know you don't get a specific ""ML degree"", but I have no idea. Can anyone explain how one ""specializes"" in machine learning, and if it's through the CS degree? Thanks. ",8,4
900,2016-3-26,2016,3,26,10,4bzb5s,FeatherSound Smiles 727-561-0800,https://www.reddit.com/r/MachineLearning/comments/4bzb5s/feathersound_smiles_7275610800/,callyschoffel,1458954596,,0,0
901,2016-3-26,2016,3,26,10,4bzbqm,Issues with an LSTM regression task,https://www.reddit.com/r/MachineLearning/comments/4bzbqm/issues_with_an_lstm_regression_task/,reijndael,1458954857,"Hi, I am working on a regression task for time-series data. I am trying two different approaches. Both involve a 3D numpy array with the following structure (samples, timesteps, input_feature). The approaches are: 1) feeding in values for all consecutive time steps in a sample 2) a one-hot type of encoding where the input_feature dimension is a 129 array with only one active element.
The issue currently is that I think during training it reaches a local minima and it stops decreasing, cycling between a few values. I have tried different number of neurons and layers, but nothing seems to help. How can I tackle this?

P.S.
I am using Keras on top of Theano. This is my model currently:
http://pastebin.com/CuWB2RE8
http://pastebin.com/maMc2EeF (for one-hot encoding)",0,1
902,2016-3-26,2016,3,26,12,4bzrjd,If you like watching hands-on machine learning : Dan Does Data,https://www.reddit.com/r/MachineLearning/comments/4bzrjd/if_you_like_watching_handson_machine_learning_dan/,tevlon,1458962806,,0,0
903,2016-3-26,2016,3,26,14,4c03wl,State of the art/current work in comp bio and genomics specifically relating to ML?,https://www.reddit.com/r/MachineLearning/comments/4c03wl/state_of_the_artcurrent_work_in_comp_bio_and/,bionerd2,1458969682,,3,6
904,2016-3-26,2016,3,26,15,4c0brs,Deep Spelling - rethinking spelling correction in the 21st century,https://www.reddit.com/r/MachineLearning/comments/4c0brs/deep_spelling_rethinking_spelling_correction_in/,MajorTal,1458975032,,2,24
905,2016-3-26,2016,3,26,15,4c0c0d,How do you tune hyperparameters in Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/4c0c0d/how_do_you_tune_hyperparameters_in_neural_networks/,[deleted],1458975214,[deleted],1,0
906,2016-3-26,2016,3,26,16,4c0e2i,[1603.06807] Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus,https://www.reddit.com/r/MachineLearning/comments/4c0e2i/160306807_generating_factoid_questions_with/,downtownslim,1458976902,,5,28
907,2016-3-26,2016,3,26,19,4c0s6d,Hey Reddit! I tried to make a machine learn how to play Doom only by processing its pixels (same concept as Google's DeepMind). Here's my findings so far!,https://www.reddit.com/r/MachineLearning/comments/4c0s6d/hey_reddit_i_tried_to_make_a_machine_learn_how_to/,Banex19,1458989377,"This whole project has been done in just a little over a month and being myself still in my second undergraduate year, I consider it an achievement. It's not by any means an example of qualitative implementation or important research.

So, I took inspiration from the famous DeepMind paper that showed how they made a machine learn how to play Atari Games by only using screenshots of the game as an input. Instead of those games, I chose a 3D game: Doom (the original, 1993 game), running in an DOS emulator.

For now, my machine is limited to trying to learn how to move in the 3D environment and to learn how the map is structured. So far, it can learn how to go from point A to point B with an accuracy of about 60%, but a lot (and I mean a lot) of improvements can be made. 

The main idea is as follows: the machine learns a model represented by an infinite state machine that has been reduced to a finite state machine, where each state is essentialy a frame from the game and where multiple, similar frames are grouped together (state set reduction policy). For this, I use similarity detection techniques such as simple euclidean distance, histogram comparison and perceptual hash (image fingerprinting) comparison. After a while, the machine will have learnt the game map and will know which actions will produce which result given the state it is currently in. This framework would suit a future implementation of Q-Learning (and its deep counterpart) to actually make the machine learn how to play and not just how to move.

If you're interested, here are some links!

Link of code on GitHub: https://github.com/banex19/DeepDoom

Link of PDF report with description of the algorithm/idea behind it: https://goo.gl/4yR9N9",20,24
908,2016-3-26,2016,3,26,20,4c0szm,Understanding Visual Concepts with Continuation Learning (paper+code),https://www.reddit.com/r/MachineLearning/comments/4c0szm/understanding_visual_concepts_with_continuation/,samim23,1458990045,,2,6
909,2016-3-26,2016,3,26,21,4c1211,Fei-Fei Li [...] said one of her Ph.D. candidates had an offer for a job paying more than $1 million a year,https://www.reddit.com/r/MachineLearning/comments/4c1211/feifei_li_said_one_of_her_phd_candidates_had_an/,beamsearch,1458996729,,107,131
910,2016-3-26,2016,3,26,23,4c1g0l,"Programming Nodes in KNIME with Scala, Part 1",https://www.reddit.com/r/MachineLearning/comments/4c1g0l/programming_nodes_in_knime_with_scala_part_1/,InformationEntropy,1459004219,,0,1
911,2016-3-27,2016,3,27,1,4c1q3p,Unsupervised learning of classes from natural images,https://www.reddit.com/r/MachineLearning/comments/4c1q3p/unsupervised_learning_of_classes_from_natural/,Jaques114,1459008651,"Say I have a dataset of images of cars in the wild, i.e., natural images that contain a car and some background. Considering the recent advances in variational autoencoders and attention mechanisms (such as the DRAW system) would it be possible to create a generative model that would be able to learn to discriminate between a number of classes (e.g. car vs background) in an unsupervised manner? If so, how would one go about it?

This is akin to unsupervised latent variable learning (e.g. GMM or HMM learning using EM) with the difference that in natural images both classes are present in the same image, so the model would have to be able to do something like unsupervised segmentation - but using a DRAW-like system, instead of CRFs/MRFs.",2,1
912,2016-3-27,2016,3,27,2,4c20db,Transformation and standardization for discrete features?,https://www.reddit.com/r/MachineLearning/comments/4c20db/transformation_and_standardization_for_discrete/,BlackHawk90,1459013065,"Hello

I have a dataset consisting of continuous and discrete features (predictors). For the discrete features, I have integer values (e.g. 0, 5, 20, 30 etc.).

Of course, for the continuous features I could apply the Box-Cox transformation to make them normal and I can center and scale the features (zero mean and unit variance).

For the discrete features I'm not sure which of the following preprocessing steps are possible and advisable:

- Box-Cox Transformation for normality
- Centering and Scaling
- Removing near-zero variance predictors
- Spatial sign for outlier removal

Which of the listed steps is possible for discrete features? I'm especially unsure about the Box-Cox transformation and centering and scaling.",10,0
913,2016-3-27,2016,3,27,3,4c26si,Need 2016 thesis research ideas,https://www.reddit.com/r/MachineLearning/comments/4c26si/need_2016_thesis_research_ideas/,[deleted],1459015858,[deleted],6,0
914,2016-3-27,2016,3,27,3,4c28xf,Using NN to predict probability,https://www.reddit.com/r/MachineLearning/comments/4c28xf/using_nn_to_predict_probability/,ilautar,1459016763,[removed],0,1
915,2016-3-27,2016,3,27,6,4c2w52,"""Are you sure you should be doing that?""",https://www.reddit.com/r/MachineLearning/comments/4c2w52/are_you_sure_you_should_be_doing_that/,__notmythrowaway__,1459026502,"Hey people,

I have a non-critical system I'm working on that logs when people decide to do certain things. I want to flag when users do things that don't match what they've done before. An analogy would be ""Joe posts on /r/machinelearning at noon every Tuesday"".

Here is how I'm thinking about this:

* Output: Which subreddit Joe posted on
* Inputs: Time of day, day of week

Over time, the system ""learns"" when Joe is likely to post on what subreddit with the ultimate goal being something like this. One day, Joe posts on /r/cars on Tuesday at noon. The system notices this and says ""Hey Joe, are you sure you should be posting on /r/cars? Usually, you post on /r/machinelearning at this time.""

Another case would be that Tuesday at noon rolls around and Joe does NOT post on anything. The system notices this and flags it for Joe: ""Hey Joe, should you be posting on /r/machinelearning?""

The system gets to watch Joe's posting history as long as is necessary for there to be some confidence in the flags that are being generated.

In reality, Joe is not actually watching the flags, these are flags being aggregated across many different people and the flags are just reports generated for the PHBs to decide whether to investigate or not. Then those decisions are fed back into the system.

Besides reading a book on neural networks many many years ago, I am a noob to machine learning. However, this screams to me to be some sort of machine learning project.

Am I way off? How would I approach it?

Thanks.",8,0
916,2016-3-27,2016,3,27,6,4c2wqy,The Tay story just had an interesting update,https://www.reddit.com/r/MachineLearning/comments/4c2wqy/the_tay_story_just_had_an_interesting_update/,0one0one,1459026794,,0,0
917,2016-3-27,2016,3,27,6,4c2x6o,"If I'm not confident in myself being a great researcher, should I even pursue a PhD?",https://www.reddit.com/r/MachineLearning/comments/4c2x6o/if_im_not_confident_in_myself_being_a_great/,imdirtysocks45,1459026994,"I would only want to do industry research and I heard that's as hard as getting a research position at a top university. I'm still an undergrad and won't even be able to start research until the second semester of my junior because I didn't see myself wanting to do research for a while. Now I realize I kind of want to, but I've heard that most people who even make it to Stanford or whatever PhD. program, end up teaching at Podunk University or become regular programmers (But probably still in ML). I'm not the brightest kid. I go to a very good CS school (Georgia Tech), and I have 3.77, but there are rockstars who I know will have the choice to do whatever they want. I don't believe I'm good enough to become a Google ML researcher. That's not the only thing I would want, but I want something really cool if I'm going to do a PhD. for 6 years. I see the PhD. as a sacrifice to make the rest of my life meaningful doing my big dreams, but I think I would be really frustrated if it didn't work out that way. Should I just go for an MS instead?",15,0
918,2016-3-27,2016,3,27,6,4c3128,"Is anyone aware of a image captioning model which supports localization, multiplicity and uncertainty?",https://www.reddit.com/r/MachineLearning/comments/4c3128/is_anyone_aware_of_a_image_captioning_model_which/,[deleted],1459028762,[deleted],0,1
919,2016-3-27,2016,3,27,7,4c33bo,Layman's tutorial to Neural Networks (no math or CS knowledge needed),https://www.reddit.com/r/MachineLearning/comments/4c33bo/laymans_tutorial_to_neural_networks_no_math_or_cs/,yellowfishx,1459029788,,1,0
920,2016-3-27,2016,3,27,7,4c35fd,A dagger by any other name: scheduled sampling,https://www.reddit.com/r/MachineLearning/comments/4c35fd/a_dagger_by_any_other_name_scheduled_sampling/,sieisteinmodel,1459030740,,4,18
921,2016-3-27,2016,3,27,9,4c3l72,Like2Vec: A new technique for recommender systems,https://www.reddit.com/r/MachineLearning/comments/4c3l72/like2vec_a_new_technique_for_recommender_systems/,vonnik,1459037972,,5,22
922,2016-3-27,2016,3,27,11,4c4346,Tay the Racist Chatbot: Who is responsible when a machine learns to be evil?,https://www.reddit.com/r/MachineLearning/comments/4c4346/tay_the_racist_chatbot_who_is_responsible_when_a/,niremetal,1459046565,,0,1
923,2016-3-27,2016,3,27,14,4c4kcb,How to work with large JSON datasets using Python and Pandas,https://www.reddit.com/r/MachineLearning/comments/4c4kcb/how_to_work_with_large_json_datasets_using_python/,dataphysicist,1459056663,,6,48
924,2016-3-27,2016,3,27,16,4c4u7q,Learn Designing Data Pipeline in Hadoop,https://www.reddit.com/r/MachineLearning/comments/4c4u7q/learn_designing_data_pipeline_in_hadoop/,andalib_ansari,1459063663,[removed],0,1
925,2016-3-27,2016,3,27,16,4c4w6u,Object-Action images,https://www.reddit.com/r/MachineLearning/comments/4c4w6u/objectaction_images/,mcostalba,1459065409,"There are many dataset of object-labeled images ('cat', 'horse', etc.), I am looking for a dataset of object-action labeled images ('cat running', 'horse standing', etc.).

I would like to know if it is possible to make the net to learn to detect the 'action' alone so that, after training, correctly detect the object-action pair on a new, not seen before instance (eg. 'horse running').

This IMO would add semantic understanding deeply into the net, withouth relying on external statistical postprocessing (language model, sentence ranking, etc.).

Do you know of some work along these lines?
",3,3
926,2016-3-27,2016,3,27,18,4c50sj,Good Theano frameworks for implementing Bi-directional LSTM?,https://www.reddit.com/r/MachineLearning/comments/4c50sj/good_theano_frameworks_for_implementing/,rulerofthehell,1459069895,"I just need a framework in which I configure the architecture of the network and not construct/code the entire network, I found a few but they seem to lag good community support, any good one's to recommend?",3,2
927,2016-3-27,2016,3,27,22,4c5gqk,Is Pattern Recognition and Machine Learning still a relevant book?,https://www.reddit.com/r/MachineLearning/comments/4c5gqk/is_pattern_recognition_and_machine_learning_still/,CuriousBot42,1459083604,"I've been working a lot with OpenCV lately, and I've been really intrigued by Computer Vision and ML from the little I've read about it. I'm been planning on picking up Pattern Recognition and Machine Learning, but it was last published in 2007. Is the book still very relevant? Anything better you can suggest? Not just books, but blogs  and other kind of resources work too.",11,14
928,2016-3-27,2016,3,27,23,4c5mu3,Theano 0.8 released,https://www.reddit.com/r/MachineLearning/comments/4c5mu3/theano_08_released/,olBaa,1459087392,,27,146
929,2016-3-27,2016,3,27,23,4c5q21,Smile  Statistical Machine Intelligence and Learning Engine,https://www.reddit.com/r/MachineLearning/comments/4c5q21/smile_statistical_machine_intelligence_and/,based2,1459089113,,4,18
930,2016-3-27,2016,3,27,23,4c5ril,pytrader - cryptocurrency trading robot,https://www.reddit.com/r/MachineLearning/comments/4c5ril/pytrader_cryptocurrency_trading_robot/,[deleted],1459089875,[deleted],0,1
931,2016-3-28,2016,3,28,2,4c6bs3,Entrepreneurs use Stanford Mafia to grow in Brasil,https://www.reddit.com/r/MachineLearning/comments/4c6bs3/entrepreneurs_use_stanford_mafia_to_grow_in_brasil/,[deleted],1459099150,[deleted],1,0
932,2016-3-28,2016,3,28,6,4c7e4y,"Winter is Coming for the Old Cloud Guard: Machine Learning at the Heart of Move Away from ""Iron"" Thones",https://www.reddit.com/r/MachineLearning/comments/4c7e4y/winter_is_coming_for_the_old_cloud_guard_machine/,[deleted],1459115704,[deleted],0,0
933,2016-3-28,2016,3,28,7,4c7gze,Coolest Demos,https://www.reddit.com/r/MachineLearning/comments/4c7gze/coolest_demos/,throwawaykyon,1459116966,Can we get a list of the coolest demos of ML that people have? Bonus points if they are easy to run!,8,14
934,2016-3-28,2016,3,28,8,4c7pie,Theano resources,https://www.reddit.com/r/MachineLearning/comments/4c7pie/theano_resources/,spurious_recollectio,1459120708,"I've recently switched (from my own custom NN code) to theano and its been a marvel to have the benefits of a flexible optimized graph but I often find its a bit of a black box.  It can do a lot of very nice things but I always worry that they might cost me in performance.  E.g. is it more efficient to vectors in order to do fewer larger matrix multiplications (e.g. in the units of an LSTM or a GRU) does is the loss in memory locality or shuffling stuff around in memory  outweigh the gains of larger matrix multiplications?  I still have figured out how to efficiently implement attention with minibatches. 

I realize I can try to profile and test this stuff (and I do) but I am wondering where people normally go to get this kind of info on theano?  Does anyone have any nice resources on theano's performance?  Is the mailing list the best place to ask such questions?",12,8
935,2016-3-28,2016,3,28,10,4c85l0,pyBrain in python,https://www.reddit.com/r/MachineLearning/comments/4c85l0/pybrain_in_python/,jarvisx909,1459128039,"I installed pybrain, but it's missing mods in it. Like the structure mod, where can I find them?",2,0
936,2016-3-28,2016,3,28,10,4c86uu,char2vec,https://www.reddit.com/r/MachineLearning/comments/4c86uu/char2vec/,pegasos1,1459128626,"I'd like to develop a vector embedding of sets of words (not sentences). Is there any work on character-level embeddings of sets of words (ie char2vec), analogous to word-level embeddings of sentences (word2vec)?  Is there a natural extension of skip-gram modeling of words to the character level?
",5,7
937,2016-3-28,2016,3,28,10,4c874c,Which companies heavily use (and pay for) Machine learning in Seattle?,https://www.reddit.com/r/MachineLearning/comments/4c874c/which_companies_heavily_use_and_pay_for_machine/,teodorz,1459128743,"Machine Learning, and Deep Learning in particular, was a hype area in the past few years, and still is. I see posts like this: https://www.reddit.com/r/MachineLearning/comments/458a02/how_much_does_a_deep_learning_researcher_get_paid/czw0n4u
where people report to get paid quite a lot for these jobs. But which companies actually give those salaries and interesting jobs in Seattle?
Is that just Google and Facebook, or there are smaller companies paying as much and having advanced and interesting projects?",5,0
938,2016-3-28,2016,3,28,10,4c87ba,Markov Chains explained visually,https://www.reddit.com/r/MachineLearning/comments/4c87ba/markov_chains_explained_visually/,varun_invent,1459128833,,15,115
939,2016-3-28,2016,3,28,11,4c8fz6,"In game 2 of AlphaGo vs Lee Sedol, commentator Michael Redmond discovers AlphaGo's true intentions",https://www.reddit.com/r/MachineLearning/comments/4c8fz6/in_game_2_of_alphago_vs_lee_sedol_commentator/,ochitsuite,1459132986,,0,1
940,2016-3-28,2016,3,28,13,4c8pez,Weka - Clustering - Learning?,https://www.reddit.com/r/MachineLearning/comments/4c8pez/weka_clustering_learning/,DC10555,1459137746,[removed],1,0
941,2016-3-28,2016,3,28,13,4c8v17,Visualizing Big Data using R,https://www.reddit.com/r/MachineLearning/comments/4c8v17/visualizing_big_data_using_r/,rouseguy,1459140754,,0,1
942,2016-3-28,2016,3,28,14,4c8vwr,Used Forklifts | Secondhand Forklifts | Shepparton Australia | HSA Forklifts,https://www.reddit.com/r/MachineLearning/comments/4c8vwr/used_forklifts_secondhand_forklifts_shepparton/,LarryGreene,1459141286,,0,1
943,2016-3-28,2016,3,28,15,4c95ey,Regression CNN in Lasagne,https://www.reddit.com/r/MachineLearning/comments/4c95ey/regression_cnn_in_lasagne/,mad_rat_man,1459147518,"I am trying to predict 20 keypoints using a VGG in Lasagne, but am getting a shape mismatch, by changing the loss function to MSE. There's a similar question [here](https://www.reddit.com/r/MachineLearning/comments/3skd69/regression_mlp_in_lasagne/). There are not many examples of regression in lasagne, though there are in nolearn, and I am stuck. 

Error message

        Traceback (most recent call last):
          File ""script_1.py"", line 167, in &lt;module&gt;
            loss = train_batch()
          File ""script_1.py"", line 90, in train_batch
            return train_fn(X_tr[ix], y_tr[ix])
          File ""/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/compile/function_module.py"", line 786, in __call__
            allow_downcast=s.allow_downcast)
          File ""/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/tensor/type.py"", line 177, in filter
            data.shape))
        TypeError: ('Bad input argument to theano function with name ""script_1.py:159""  at index 1(0-based)', 'Wrong number of dimensions: expected 1, got 2 with shape (16, 20).')

For ref, here are the shapes

    ('X.shape', (100, 3, 224, 224))
    ('y.shape', (100, 20))
    ('X_tr.shape', (56, 3, 224, 224))
    ('y_tr.shape', (56, 20))
    ('X_val.shape', (19, 3, 224, 224))
    ('y_val.shape', (19, 20))
    ('X_te.shape', (25, 3, 224, 224))
    ('y_te.shape', (25, 20))

      
Code( adapted from https://github.com/ebenolson/pydata2015/blob/master/3%20-%20Convolutional%20Networks/Finetuning%20for%20Image%20Classification.ipynb)

    def build_model():
        net = {}
        net['input'] = InputLayer((None, 3, 224, 224))
        net['conv1'] = ConvLayer(net['input'], num_filters=96, filter_size=7, stride=2, flip_filters=False)
        net['norm1'] = NormLayer(net['conv1'], alpha=0.0001) # caffe has alpha = alpha * pool_size
        net['pool1'] = PoolLayer(net['norm1'], pool_size=3, stride=3, ignore_border=False)
        net['conv2'] = ConvLayer(net['pool1'], num_filters=256, filter_size=5, flip_filters=False)
        net['pool2'] = PoolLayer(net['conv2'], pool_size=2, stride=2, ignore_border=False)
        net['conv3'] = ConvLayer(net['pool2'], num_filters=512, filter_size=3, pad=1, flip_filters=False)
        net['conv4'] = ConvLayer(net['conv3'], num_filters=512, filter_size=3, pad=1, flip_filters=False)
        net['conv5'] = ConvLayer(net['conv4'], num_filters=512, filter_size=3, pad=1, flip_filters=False)
        net['pool5'] = PoolLayer(net['conv5'], pool_size=3, stride=3, ignore_border=False)
        net['fc6'] = DenseLayer(net['pool5'], num_units=4096)
        net['drop6'] = DropoutLayer(net['fc6'], p=0.5)
        net['fc7'] = DenseLayer(net['drop6'], num_units=4096)
        net['drop7'] = DropoutLayer(net['fc7'], p=0.5)
        net['fc8'] = DenseLayer(net['drop7'], num_units=20, nonlinearity=None)
        return net

    ################ data_loader ###################

    X = y = []
    y = np.genfromtxt(DATA_DIR+'labels.csv', delimiter = ',') # labels

    for image_file in os.listdir(DATA_DIR + 'images/'):
        im = image_preprocess(DATA_DIR + 'images/' + image_file) # this call preprocesses the image for VGG
        X.append(im)

    X = np.concatenate(X)
    X = X.astype('float32')
    y = y.astype('int32')

    ########## split into train/test ################

    train_ix, test_ix = sklearn.cross_validation.train_test_split(range(len(y)))
    train_ix, val_ix = sklearn.cross_validation.train_test_split(range(len(train_ix)))

    X_tr = X[train_ix]
    y_tr = y[train_ix]
    X_val = X[val_ix]
    y_val = y[val_ix]
    X_te = X[test_ix]
    y_te = y[test_ix]

    #####################################################

    # build the net and fill pretrained stuff
    net = build_model()

    # delete the 1000-long last layer's weights
    d_vggcnn_numpy = np.array(d_vggcnn['values'])
    d_vggcnn_numpy = np.delete(d_vggcnn_numpy, 15) # d_vggcnn_numpy[15] is 1000*1
    d_vggcnn_numpy = np.delete(d_vggcnn_numpy, 14) # d_vggcnn_numpy[14] is 4096*1000
    lasagne.layers.set_all_param_values(net['fc7'], d_vggcnn_numpy) # TODO are the weights initialized to rand by default?

    ################# loss and metrics #################

    X_sym = T.tensor4()
    y_sym = T.ivector()

    output_layer = net['fc8']
    prediction = lasagne.layers.get_output(output_layer, X_sym)
    loss = lasagne.objectives.squared_error(prediction, y_sym)
    loss = loss.mean()

    acc = T.mean(T.eq(T.argmax(prediction, axis=1), y_sym), dtype=theano.config.floatX)
    params = lasagne.layers.get_all_params(output_layer, trainable=True)
    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.0001, momentum=0.9)

    ###################### theano ##############

    train_fn = theano.function([X_sym, y_sym], loss, updates=updates, allow_input_downcast=True)
    val_fn = theano.function([X_sym, y_sym], [loss, acc], allow_input_downcast=True)
    pred_fn = theano.function([X_sym], prediction, allow_input_downcast=True)

    # generator splitting an iterable into chunks of maximum length N

    def batches(iterable, N):
        chunk = []
        for item in iterable:
            chunk.append(item)
            if len(chunk) == N:
                yield chunk
                chunk = []
        if chunk:
            yield chunk

    def train_batch():
        ix = range(len(y_tr))
        np.random.shuffle(ix)
        ix = ix[:BATCH_SIZE]
        return train_fn(X_tr[ix], y_tr[ix])

    def val_batch():
        ix = range(len(y_val))
        np.random.shuffle(ix)
        ix = ix[:BATCH_SIZE]
        return val_fn(X_val[ix], y_val[ix])

    ####################### training ################

    for epoch in range(5):
        for batch in range(25):
           loss = train_batch()

        ix = range(len(y_val))
        np.random.shuffle(ix)

        loss_tot = 0.
        acc_tot = 0.
        for chunk in batches(ix, BATCH_SIZE):
            loss, acc = val_fn(X_val[chunk], y_val[chunk])
            loss_tot += loss * len(chunk)
            acc_tot += acc * len(chunk)

        loss_tot /= len(ix)
        acc_tot /= len(ix)
        print(epoch, loss_tot, acc_tot * 100)
",3,0
944,2016-3-28,2016,3,28,16,4c98el,Role of Centralized Lubrication Systems. Check out!,https://www.reddit.com/r/MachineLearning/comments/4c98el/role_of_centralized_lubrication_systems_check_out/,jackerfrinandis,1459149643,,0,1
945,2016-3-28,2016,3,28,16,4c99iv,Used Forklifts Melbourne | Secondhand Forklifts For Sale Australia | Electric Forklifts,https://www.reddit.com/r/MachineLearning/comments/4c99iv/used_forklifts_melbourne_secondhand_forklifts_for/,LarryGreene,1459150491,,0,0
946,2016-3-28,2016,3,28,17,4c9dhx,An AI assistant that schedules meetings for you,https://www.reddit.com/r/MachineLearning/comments/4c9dhx/an_ai_assistant_that_schedules_meetings_for_you/,hanslower,1459153823,,0,0
947,2016-3-28,2016,3,28,18,4c9io0,Chat based community for machine learning,https://www.reddit.com/r/MachineLearning/comments/4c9io0/chat_based_community_for_machine_learning/,kentiviti,1459158239,[removed],0,1
948,2016-3-28,2016,3,28,19,4c9oom,Why doesn't extra supervision increase the performance of the SOTA language model?,https://www.reddit.com/r/MachineLearning/comments/4c9oom/why_doesnt_extra_supervision_increase_the/,quilby,1459162665,"I took the [tensorflow implementation](https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html#recurrent-neural-networks) of the language model from Zaremba et al., 2014, and changed the loss function from what it was (crossentropy with 1-hot vector representing the correct word) to a loss made up of two terms, the first is the loss from before and the second is a crossentropy loss with a 1-hot vector representing the closest synonym to the target word. I tried playing around with the weighting of these two terms, but no matter what I did the results did not improve over the original model.


Does this make sense? Shouldnt an RNN benefit from this knowledge about synonyms? Doesn't this new loss function basically tell the network ""the next correct word is 'dog', but if you say its 'puppy' thats also OK""?",12,3
949,2016-3-28,2016,3,28,20,4c9p4f,Questions about Post-Review process at ICML.,https://www.reddit.com/r/MachineLearning/comments/4c9p4f/questions_about_postreview_process_at_icml/,ArmenAg,1459162970,"I submitted a paper to ICML, and just recently received the reviews from 3 reviewers. They were relatively minor, so I fixed and updated my paper and now it is looking much better. Is there anyway I can update my submission, so my new paper would replace my past one?
Thank you.   ",2,3
950,2016-3-28,2016,3,28,20,4c9t4p,Show r/MachineLearning: movie-dialog-summarizer,https://www.reddit.com/r/MachineLearning/comments/4c9t4p/show_rmachinelearning_moviedialogsummarizer/,vackosar,1459165572,,0,1
951,2016-3-28,2016,3,28,21,4c9v2m,Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction,https://www.reddit.com/r/MachineLearning/comments/4c9v2m/investigation_into_the_effectiveness_of_long/,HenryJia,1459166734,,9,0
952,2016-3-28,2016,3,28,21,4c9xje,Dry Ice Blasting Machine and Rental Equipment,https://www.reddit.com/r/MachineLearning/comments/4c9xje/dry_ice_blasting_machine_and_rental_equipment/,icetech01,1459168135,,0,1
953,2016-3-28,2016,3,28,21,4ca09o,How to use CNN to train input data of different size?,https://www.reddit.com/r/MachineLearning/comments/4ca09o/how_to_use_cnn_to_train_input_data_of_different/,hntee,1459169625,"CNN seems to be implemented mostly for fixed size input. Now I want to use CNN to train some sentences of different size, what are some common methods?",6,0
954,2016-3-28,2016,3,28,23,4ca8tr,Help with offline speech recognition in a Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/4ca8tr/help_with_offline_speech_recognition_in_a/,carlos_argueta,1459173639,"As part of my project I need to implement offline speech recognition in a Raspberry Pi 3. Doing some research I have found the CMUSphinx project (http://cmusphinx.sourceforge.net/wiki/tutorial) to be one of the most recommended. I tried it for simple command and control and it is awesome, however, when I move to real conversations, I found it to perform very bad.  I have a small corpus of about 500K utterances, and trained ARPA models with it. With some parameter tuning I can get some improvement but very far from good. Notice that I am using the pre-packaged acoustic models and I am just tinkering with the language models. My question is, is there any other tool that I can try for my project (with the constrained hardware), or simply with my limited hardware CMUSphinx is the best I can have?
Thanks a lot for your time.",5,1
955,2016-3-28,2016,3,28,23,4cablh,"Think ""Turing Test"" in another way ?",https://www.reddit.com/r/MachineLearning/comments/4cablh/think_turing_test_in_another_way/,erogol,1459174753,,1,0
956,2016-3-28,2016,3,28,23,4cacy4,Any works dealing with localizing a single known object (under various lighting and occlusions) in an image using CNNs?,https://www.reddit.com/r/MachineLearning/comments/4cacy4/any_works_dealing_with_localizing_a_single_known/,joekr07,1459175301,,3,1
957,2016-3-29,2016,3,29,0,4calxs,My machine learning blog -- looking for feedback/criticism/things you'd like me to write about,https://www.reddit.com/r/MachineLearning/comments/4calxs/my_machine_learning_blog_looking_for/,r00tking,1459178900,,0,1
958,2016-3-29,2016,3,29,1,4casci,Can I Hug That? I trained a classifier to tell you whether or not what's in an image is huggable.,https://www.reddit.com/r/MachineLearning/comments/4casci/can_i_hug_that_i_trained_a_classifier_to_tell_you/,juliaferraioli,1459181301,,88,618
959,2016-3-29,2016,3,29,1,4cau60,"Machine Learning Trading, Stock Market, and Chaos",https://www.reddit.com/r/MachineLearning/comments/4cau60/machine_learning_trading_stock_market_and_chaos/,elen777,1459181961,,1,0
960,2016-3-29,2016,3,29,1,4cavf0,RNN Benchmarks,https://www.reddit.com/r/MachineLearning/comments/4cavf0/rnn_benchmarks/,guismay,1459182405,"Hi,

I did some benchmarks for RNN and LSTM using Theano and Torch:
https://github.com/glample/rnn-benchmarks

On my machine Theano is much faster, especially for LSTM. For both I used the fastest implementations I found. I'm not sure I configured Torch optimally though. I put the code I used online. If you see an error or have significantly different results, please point me out.

I will try to add benchmarks for other frameworks like TensorFlow or MXNet in the future.

EDIT: Sorry, I did a mistake for the Torch FastLSTM. I didn't enable a ""usenngraph"" flag and I was not using the best implementation. I re-ran the benchmarks, it's much better now. I also updated the scores of Theano using the last version (0.8) and it also improved.",22,21
961,2016-3-29,2016,3,29,1,4caz8s,Interactive MXNet computation graph! This notebook lets you vary the inputs with sliders and will compute outputs and gradients and display them in a graph. You can even edit the network to make it more complex! [OC],https://www.reddit.com/r/MachineLearning/comments/4caz8s/interactive_mxnet_computation_graph_this_notebook/,ieee8023,1459183826,,0,19
962,2016-3-29,2016,3,29,2,4cbatj,TensorFlow Stacked AutoEncoders Tutorial,https://www.reddit.com/r/MachineLearning/comments/4cbatj/tensorflow_stacked_autoencoders_tutorial/,Jxieeducation,1459187967,,1,3
963,2016-3-29,2016,3,29,3,4cbdjm,The scariest use of machine learning,https://www.reddit.com/r/MachineLearning/comments/4cbdjm/the_scariest_use_of_machine_learning/,vincentg64,1459188929,,0,1
964,2016-3-29,2016,3,29,3,4cbfuz,Rank-based Bandits,https://www.reddit.com/r/MachineLearning/comments/4cbfuz/rankbased_bandits/,sbokus,1459189754,"Do any of you know any bandits that do not care about the range of rewards (e.g. -1 to 1, or 0 to 1) and only care about the ranking of rewards - I am aware of schemes from evolutionary computation that address this issue (e.g. here http://arxiv.org/pdf/1106.3708v3.pdf )  - I am wondering if there is anything from the bandit community. ",16,1
965,2016-3-29,2016,3,29,3,4cbk3y,Help with Machine Learning and Elevators project,https://www.reddit.com/r/MachineLearning/comments/4cbk3y/help_with_machine_learning_and_elevators_project/,markartur1,1459191253,"Hello,

I am beginning to develop a paper about improving elevators waiting times with machine learning, but i am relatively new on the subject (undergrad).

I am thinking on using Reinforcement Learning, perhaps along with TensorFlow?

Any tips on what language/approach to use?",3,0
966,2016-3-29,2016,3,29,4,4cbmho,More on CAEs,https://www.reddit.com/r/MachineLearning/comments/4cbmho/more_on_caes/,jt7582,1459192113,"I'm writing a convolutional autoencoder, and want the encoder and the decoder to both have architectures like what's described in [this paper](http://arxiv.org/pdf/1511.06434v2.pdf). 


What I'm confused about ([this paper has this as well](http://arxiv.org/pdf/1411.5928.pdf)) is the ""project and reshape"" step. What does that mean exactly? Can anyone describe exactly how they transform 100 dimensions into 1024 channels of 4x4 slices? Is it just by adding a fully connected layer of 16384 units then reshaping their output?

Thanks!",3,2
967,2016-3-29,2016,3,29,6,4cc7pw,[Question] Modeling traffic in the city,https://www.reddit.com/r/MachineLearning/comments/4cc7pw/question_modeling_traffic_in_the_city/,__Julia,1459199788,"Hi,

I am looking for a review paper that cover the different techniques that are used to model traffic in cities. Is there any work that you recommend or you find interesting. 

Before you say ""Google it"", there are tons of papers covering this topic with different mathematical tastes. However; I would like to crowdsource your wisdom to not miss any interesting paper. Feel free to recommend any work/lab/researchers' names.  ",2,5
968,2016-3-29,2016,3,29,6,4cc9w7,The Subjective Eye of Machine Vision (20min video talk + PDF),https://www.reddit.com/r/MachineLearning/comments/4cc9w7/the_subjective_eye_of_machine_vision_20min_video/,DrLegend,1459200577,,0,4
969,2016-3-29,2016,3,29,6,4ccbdd,The Second Machine Age - Rise of the Working Class Machines,https://www.reddit.com/r/MachineLearning/comments/4ccbdd/the_second_machine_age_rise_of_the_working_class/,[deleted],1459201121,[deleted],0,0
970,2016-3-29,2016,3,29,6,4ccelo,[Question] How to deal with overlapping training data in classification.,https://www.reddit.com/r/MachineLearning/comments/4ccelo/question_how_to_deal_with_overlapping_training/,astroFizzics,1459202318,"Hi All, 

I have some data that I need to classify into three groups (Q= 0,1,2). The Q=0 training data is realatively well seperated from the other training data, but the Q=1, and Q=2 have a good amount of overlaps. See [this figure](http://imgur.com/tUX0J6e) for an example. 

I'm working with Scikit-Learn, and I've tried Random Forests, Extremely Randomized Forests, and SVM. In the testing step (before I apply it to unknown data) I get pretty good results (The recall and precision are both ~60%). When I apply it to unknown data the methods way over predict the amount of Q=1 data points. They should be about 40%, 30%, 30% (the splits on the training data) for 0, 1 and 2, but I find more like 45%, 40%, 15% (predicted distributions).

Any suggestions on other things I might try? I'd like to stick with Scikit-learn if I can. I'll try to answer any questions as best I can if I wasn't clear. I could even provide some data if someone is interested and looking for something to do. =)

I appreciate it. ",7,3
971,2016-3-29,2016,3,29,9,4ccxqb,Are there female researchers at Deepmind?,https://www.reddit.com/r/MachineLearning/comments/4ccxqb/are_there_female_researchers_at_deepmind/,jlxu,1459209743,Just a question of curiosity as a female interested in machine intelligence. All of the prominent researchers seem to be male -- at least the researchers I have heard about and whose work I've read. ,15,0
972,2016-3-29,2016,3,29,9,4ccz4m,Online A.I. EXPERIMENT to be run by Discovery Channel Host - volunteers needed,https://www.reddit.com/r/MachineLearning/comments/4ccz4m/online_ai_experiment_to_be_run_by_discovery/,[deleted],1459210338,[deleted],0,0
973,2016-3-29,2016,3,29,9,4cd27a,How to build distance function given a cluster of points?,https://www.reddit.com/r/MachineLearning/comments/4cd27a/how_to_build_distance_function_given_a_cluster_of/,[deleted],1459211640,[removed],0,1
974,2016-3-29,2016,3,29,10,4cdb8c,Paper: Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning,https://www.reddit.com/r/MachineLearning/comments/4cdb8c/paper_deep_learning_for_realtime_atari_game_play/,jlxu,1459215390,,0,12
975,2016-3-29,2016,3,29,10,4cdcd3,Obstacle Detection for Blind People using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4cdcd3/obstacle_detection_for_blind_people_using_neural/,gooeyn,1459215856,"As a side project, I was thinking of developing a obstacle detection system for blind people, which could help them cross the street among other things, by saying out loud what is the environment like. 

What do you guys think about it? I did not find research using machine learning and neural networks for this task",5,1
976,2016-3-29,2016,3,29,11,4cdgyh,From nothing to machine learning: what a noob ought to do,https://www.reddit.com/r/MachineLearning/comments/4cdgyh/from_nothing_to_machine_learning_what_a_noob/,watonearth,1459217771,"Hey friends. So I don't know much about programming apart from some basic knowledge picked up from codeacademy.com et al. I've recently become very very interested in machine learning. After reading and asking around for a bit, I want to get hands on.


Where would you suggest I start working towards an end goal of programming ANNs? Also what should I work once I've finished learning Python for example? Can I progress straight to Dr. Hinton's Coursera course? Or should I study around the area a bit more before attempting it?


Any guidance at all would be greatly appreciated.",11,3
977,2016-3-29,2016,3,29,12,4cdsvs,[Question] Machine learning approach to add silences to translated subtitles. Is this the right way?,https://www.reddit.com/r/MachineLearning/comments/4cdsvs/question_machine_learning_approach_to_add/,pacificz,1459223198,"For my final year engineering project, I have a data set of a video game subtitles in English and in French (I also have other languages but let's just stick with French for now to make it simpler) and the audio files for the English subtitles only (which I find no use for). The English subtitle have silences specified in them that matches their respective audio file. For example,

&amp;nbsp;

   **[Silence 3]** Some narrated text goes here. **[Silence 2]** Some more text goes here.
(The 3 in **[Silence 3]** represents the silence duration in the audio file).

&amp;nbsp;

Note: Not all sentences behave like the example I gave earlier, sometimes the silence is in the middle of a sentence, or next to a different punctuation mark. There are also some pair of subtitles that have different silence amount which I couldn't find any logic why that would happen so I decided to ignore them for now. 

&amp;nbsp;

My task is to provide a program that adds the silences to the French subtitles so that I can output :

&amp;nbsp;

   **[Silence 3]** Certains textes narratifs vont ici. **[Silence 2]** Un peu plus de texte va ici.

&amp;nbsp;

I already have the answers for the French subtitles which I am supposed to use to verify the effectiveness of my algorithm. At first, I was thinking of simply analysing the file manually and finding recurring patterns. For instance, whenever a silence is present at the beginning of the English subtitle, it is also at the beginning of the translated text subtitle. However, this approach seemed to simple for an 8 month project.  

&amp;nbsp;

Since I am also supposed to provide a confidence Index for each pair of subtitles that should be automatically generated by my algorithm, it hinted me to start looking into a machine learning approach using the already given answer to teach my algorithm but I am not sure what would be the best type of algorithm or if this is the way to go. I looking at the different type of algorithms for machine learning and it looks like neural network or linear regression would be the best option. 

&amp;nbsp;

I tried looking online on how I should approach this problem but I couldn't find anything. What would be the best way to approach this problem? Am I on the right track with machine learning or is it too far fetched?

&amp;nbsp;

Thank you.",0,1
978,2016-3-29,2016,3,29,13,4cdwl9,[Question] How to learn a distance metric based on the surface shape of a cluster of points?,https://www.reddit.com/r/MachineLearning/comments/4cdwl9/question_how_to_learn_a_distance_metric_based_on/,sarandi500,1459225020,"Given a non-elliptical cluster of points in a n-dimensional space I would like to learn a distance function from the centroid of this cluster such that its ""equipotential"" surfaces has the same shape as the surface that encloses more or less tightly the points cloud (not overfitting the noise). So different distances from the center would result in scaled surfaces versions of the same shape. I guess I can achieve that for ellipsoid clusters by using the mahalanobis distance, but what about non-ellipsoid distances? Maybe a I should fit a mixture of gaussians and use the log-probability of generating a sample as being the distance to that sample? Is there a better way to accomplish what I want?",0,3
979,2016-3-29,2016,3,29,13,4cdzjf,Question/Answer text corpus to learn from,https://www.reddit.com/r/MachineLearning/comments/4cdzjf/questionanswer_text_corpus_to_learn_from/,reflex666,1459226516,"I am a hobbyist and I've tried my hand at something I thought would work for a CHAT AI but now I need material to train it with. I am looking for a Question/Answer corpus that contains questions of various types, subjects, and the like as well as their corresponding answers. Formatting of the file doesn't matter as long as the question and answer are included in some sane way (I can parse them on my end). Another option is chat logs, but these have to be in a format where one user says something, and another user responds to previous users statement. Anyways is there anywhere I can find some free training data to download? Or a place I could parse for data in these formats? First thing that comes to mind is reddit comments and twitter tweets since usually they contain one or two sentences, and then users who respond to those sentences. Any help is appreciated. Thank you.",5,5
980,2016-3-29,2016,3,29,14,4ce5x4,Deep Exploration via Bootstrapped DQN (paper+code),https://www.reddit.com/r/MachineLearning/comments/4ce5x4/deep_exploration_via_bootstrapped_dqn_papercode/,samim23,1459229979,,0,8
981,2016-3-29,2016,3,29,14,4ce7x0,"Deep Learning, Semantics, And Society",https://www.reddit.com/r/MachineLearning/comments/4ce7x0/deep_learning_semantics_and_society/,xlegel,1459231177,,0,1
982,2016-3-29,2016,3,29,15,4ceaa9,on Tensorflow: is there a way to store the trained model as a protobuf(.pb) file?,https://www.reddit.com/r/MachineLearning/comments/4ceaa9/on_tensorflow_is_there_a_way_to_store_the_trained/,Enum1,1459232651,"I am trying to replace the model of the android demo app but can figure out how.

Should I use `Saver` for this?",1,2
983,2016-3-29,2016,3,29,15,4ced33,"[1603.07886] A Novel Biological Mechanism-Based Visual Cognition Model--Automatic Extraction of Semantics, Formation of Integrated Concepts and Re-selection Features for Ambiguity",https://www.reddit.com/r/MachineLearning/comments/4ced33/160307886_a_novel_biological_mechanismbased/,InaneMembrane,1459234565,,1,8
984,2016-3-29,2016,3,29,16,4cefw2,New Extraction Methods Opens New Avenues for Global Green Coffee Bean Extract Industry,https://www.reddit.com/r/MachineLearning/comments/4cefw2/new_extraction_methods_opens_new_avenues_for/,aiden_11,1459236442,[removed],0,1
985,2016-3-29,2016,3,29,17,4cemwy,Persistent Neural Nets,https://www.reddit.com/r/MachineLearning/comments/4cemwy/persistent_neural_nets/,lanevorockz,1459241622,,11,23
986,2016-3-29,2016,3,29,18,4ceovd,How to compress the value function for the N puzzle problem?,https://www.reddit.com/r/MachineLearning/comments/4ceovd/how_to_compress_the_value_function_for_the_n/,abstractcontrol,1459243103,"I am doing a project, trying to solve the 4x4 N puzzle problem that has 10T states roughly. Using the [pattern database](https://heuristicswiki.wikispaces.com/pattern+database) approach I've already made a solver that works by utilizing the two Fringe and Corner patterns (500MB respectively) in addition to the two smaller patterns for the space not covered by them (5MB respectively.) The max of those four patterns for a given position plus the number of moves taken is my cost function for this problem.

My goal for this project is to not only to memorize those four patterns, but to [build on that work](https://github.com/mrakgr/N-Puzzle-Experiments/tree/master/N%20Puzzle) and hopefully solve the 4x4 completely using neural nets. Currently, the [Fringe Search](https://en.wikipedia.org/wiki/Fringe_search) solver can solve problems up to 50 under a second, but it chokes on problems that need -78 a -80 moves. Those problems would come within reach if I could improve the value function and machine learning is really the only way forward at this point as patterns larger than Fringe and Corner would be too big for my machine.

With that reasoning, I've started by trying to compress one of the smaller 5Mb patterns, but the results have been discouraging as I cannot get the accuracy on even a single minibatch of 2560 to go over 50% using a simple feedforward net.

Here is what has worked the best for me so far:
one hot encoding for the inputs, one hot encoding for the outputs (255 outputs on for each possible byte value), feedforward 112-3072-255 net with cross entropy cost.

I've tried scalar inputs and outputs, binary encoded inputs and outputs, and the one hot encoding works best. 

Varying the size of the single hidden layer does not make a difference on the minibatch, it always converges to around 50% accuracy. What troubles me about this is that adding layers makes it work significantly worse. Even a single layer makes the training much longer and harder and in the end it does not exceed the performance of a single layer.

I've expected that I would get difficulties training as I add layers and increase size, but not this soon and not on a single 2560 minibatch. That reminds me that the N puzzle is a genuine NP Hard problem.

What I could do from here is add convolutional nets and pretraining to make training easier, but I want to ask here first if I am missing something?

Thanks.",1,2
987,2016-3-29,2016,3,29,19,4ceuy5,Quadcopter Navigation in the Forest using Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4ceuy5/quadcopter_navigation_in_the_forest_using_deep/,bahidev,1459247342,,57,369
988,2016-3-29,2016,3,29,21,4cf4oe,[1603.07292] Debugging Machine Learning Tasks,https://www.reddit.com/r/MachineLearning/comments/4cf4oe/160307292_debugging_machine_learning_tasks/,Bardelaz,1459253218,,0,9
989,2016-3-29,2016,3,29,21,4cf4pg,How to plan for a GPU in 2016?,https://www.reddit.com/r/MachineLearning/comments/4cf4pg/how_to_plan_for_a_gpu_in_2016/,lifieemodd,1459253237,[removed],2,0
990,2016-3-29,2016,3,29,21,4cf7k3,What are you working on?,https://www.reddit.com/r/MachineLearning/comments/4cf7k3/what_are_you_working_on/,weirdML,1459254766,,115,48
991,2016-3-29,2016,3,29,21,4cf80o,I work for a financial company with access to a ton of data. What can I do here with Machine Learning? [Good mathematical background],https://www.reddit.com/r/MachineLearning/comments/4cf80o/i_work_for_a_financial_company_with_access_to_a/,[deleted],1459254969,[deleted],7,1
992,2016-3-29,2016,3,29,22,4cfb2y,Use of resistive ram for NN training and available alternatives?,https://www.reddit.com/r/MachineLearning/comments/4cfb2y/use_of_resistive_ram_for_nn_training_and/,watonearth,1459256415,"Just read this article about how promising RPU's look for neural network training because of their parallel architecture. Decided to post because I'm looking to build a new PC soon (Mac user atm), and I was wondering if a good amount of local memory is something I should look for in a CPU.


Thanks in advance y'all.",8,2
993,2016-3-29,2016,3,29,22,4cff19,How does Local Response Normalization relate to Batch Normalization?,https://www.reddit.com/r/MachineLearning/comments/4cff19/how_does_local_response_normalization_relate_to/,xristos_forokolomvos,1459258210,,0,8
994,2016-3-29,2016,3,29,22,4cff3e,Hyperparameter Selection,https://www.reddit.com/r/MachineLearning/comments/4cff3e/hyperparameter_selection/,negazirana,1459258231,"Guys,
what's your favourite hyperparameter selection method?

In a real world setting (in which you can run a few hundreds of experiments in parallel) would you go with grid search, random sampling (hyperopt-style) or Bayesian optimization?

In case you opted for BO: would you implement something from scratch, or use an existing framework? Assuming you're ok-ish with GPs, you still would have to parallelize the thing, choose the right kernel, trade-off between exploration and exploitation etc.

tl;dr Hyperparameter Selection: WWJD?

Alternatives:

- https://github.com/hyperopt/hyperopt

- https://bitbucket.org/rmcantin/bayesopt

- http://optunity.readthedocs.org/en/latest/

- https://github.com/fmfn/BayesianOptimization/
",3,4
995,2016-3-29,2016,3,29,22,4cffbm,Some great curated posts on ML and Big data,https://www.reddit.com/r/MachineLearning/comments/4cffbm/some_great_curated_posts_on_ml_and_big_data/,datameer,1459258321,,0,1
996,2016-3-29,2016,3,29,22,4cfftg,Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4cfftg/deep_tracking_seeing_beyond_seeing_using/,senorstallone,1459258517,,3,13
997,2016-3-29,2016,3,29,22,4cfiw6,Applying Deep Learning to Radiological Images for Anomaly Detection,https://www.reddit.com/r/MachineLearning/comments/4cfiw6/applying_deep_learning_to_radiological_images_for/,reworksophie,1459259820,,0,1
998,2016-3-29,2016,3,29,23,4cfk07,A Little Machine Learning Magic,https://www.reddit.com/r/MachineLearning/comments/4cfk07/a_little_machine_learning_magic/,infosecprincess,1459260282,,0,1
999,2016-3-29,2016,3,29,23,4cflh7,"Regression, Logistic Regression and Maximum Entropy",https://www.reddit.com/r/MachineLearning/comments/4cflh7/regression_logistic_regression_and_maximum_entropy/,ataspinar,1459260882,,0,1
1000,2016-3-29,2016,3,29,23,4cfpyo,Biological Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4cfpyo/biological_neural_networks/,Kiuhnm,1459262637,"I've just read a non-technical [article](http://numenta.com/blog/machine-intelligence-machine-learning-deep-learning-artificial-intelligence.html) about *biological neural networks*.

When I have some free time I'll look into [Hierarchical Temporal Memory (HTM)](http://numenta.com/learn/), but for now I'd like to know what you think about the article and HTM. It's something worth looking into or just hype? I can't shake this feeling that the researchers who wrote that article don't know much about Deep Learning, but maybe I'm just prejudiced.

---

###edit: I'd like to thank the 17% who upvoted this post. It always amazes me how unfriendly this subreddit is.",2,0
1001,2016-3-29,2016,3,29,23,4cfrfa,Deep Learning on Babbage's Analytical Engine,https://www.reddit.com/r/MachineLearning/comments/4cfrfa/deep_learning_on_babbages_analytical_engine/,harharveryfunny,1459263202,,0,6
1002,2016-3-30,2016,3,30,0,4cftau,Differentiating between toys and real/full sized things,https://www.reddit.com/r/MachineLearning/comments/4cftau/differentiating_between_toys_and_realfull_sized/,[deleted],1459263887,[deleted],1,0
1003,2016-3-30,2016,3,30,0,4cfteu,New tools from the bandit literature to improve A/B Testing - Emilie Kaufmann - RecsysFr meetup.,https://www.reddit.com/r/MachineLearning/comments/4cfteu/new_tools_from_the_bandit_literature_to_improve/,Agagla,1459263927,,0,0
1004,2016-3-30,2016,3,30,0,4cfu08,Deep 3D Modelling?,https://www.reddit.com/r/MachineLearning/comments/4cfu08/deep_3d_modelling/,EdgarSpayce,1459264139,"Deep learning with Google Image (and other image database) for photogrammetric based 3D reconstruction: does it exist?

A Photosynth-similar algorythm would research through databases of images of an object or entity, and reconstruct a 3D point-cloud model from detected viewpoints.

Thanks.",9,3
1005,2016-3-30,2016,3,30,0,4cfv11,Working with unbalanced data sets?,https://www.reddit.com/r/MachineLearning/comments/4cfv11/working_with_unbalanced_data_sets/,StepW,1459264501,"Hey, I'm a fairly inexperienced student currently working on a genre classifier and have encountered a bit of a problem. The data set I'm working with to train the classifier is unbalanced. I have various songs from all kinds of genre categories, but some categories have more sample data in them than others. For instance, ""rock"", ""jazz"", and ""raphiphop"" have hundreds of training samples, but another category called ""funksoulrnb"" only has a couple of dozen.

I didn't think this would be a massive problem but I found that my classifier tends to classify songs as ""rock"", ""jazz"", or ""raphiphop"" more often than as other categories. It has a pretty stellar accuracy rate on those categories, but it doesn't perform as well on categories with less samples. In fact, from the confusion matrices I'm generating ([here](http://puu.sh/nU5XC.png)'s one), it seems as though the less training samples a particular genre category has, the worse the classifier performs on it.

I have tried [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), whereby I train multiple classifiers using a uniform random sampling of each category and then take the majority vote of each predicted category during testing. This unfortunately doesn't seem to improve anything.

Is there anything I can do to circumvent this problem, or are unbalanced datasets just awkward to work with either way?

Thanks!",11,3
1006,2016-3-30,2016,3,30,0,4cfzxo,Has there been any word or big advancements in ML playing games were there isn't perfect information?,https://www.reddit.com/r/MachineLearning/comments/4cfzxo/has_there_been_any_word_or_big_advancements_in_ml/,thenerdstation,1459266257,"Since Go has basically been conquered, the next step would be playing games that don't have perfect information. I'm thinking games like Starcraft, Resistance (the card game), poker, and the like. Has anyone been able to make good progress on these types of games?",1,1
1007,2016-3-30,2016,3,30,0,4cg092,"Hi Reddit, this is my image style synthesis network with Tensorflow",https://www.reddit.com/r/MachineLearning/comments/4cg092/hi_reddit_this_is_my_image_style_synthesis/,machrisaa,1459266380,"This project is yet another image synthesis ML project done with Tensorflow. 

I have combined these 2 methods:
 - Gram Matrix: http://arxiv.org/abs/1508.06576
 - Markov Random Field: http://arxiv.org/abs/1601.04589
With the mixed use of these 2 method, we can generate a high quality result. 

I have also add 2 modifications into the MRF algorithm. One is added a blur filter to the covariant tensor before calculate the max argument. It makes the image looks more natural.

The second one is changed the cost calculation from multiple patches to a averaged big tensor. This greatly improved the training speed running by the Tensorflow!

I have tested using the ""slice"" and ""split"" methods in Tensorflow to implement the patches generation, but it only ends up with very poor performance (&gt;10 times slower) and consumes lots of memory. So finally I decided to use numpy to do the slicing. I don't know if any one will have another better idea.

Here is the code:
https://github.com/machrisaa/stylenet
A sample video of the result (turning a painted cat into a real cat!):
https://youtu.be/4ssJyLivbBM

p.s. I have also plotted a VGG19 and VGG16 into Tensorflow with fast initialisation when working on this project:
https://github.com/machrisaa/tensorflow-vgg

Hope that some people will be interested :)",8,13
1008,2016-3-30,2016,3,30,1,4cg2k7,Q. Memory Nets and RNNs,https://www.reddit.com/r/MachineLearning/comments/4cg2k7/q_memory_nets_and_rnns/,0entr0py,1459267205,"I was reading the paper on end-to-end memory nets, and the authors claim that mem nets with tied embedding matrices can be cast as an RNN. They have described the concept of internal outputs(the attention weights) and external outputs (the final label) and the embdding of the question as the hidden state to explain this, but I am having a hard time picturing how everything fits into the RNN framework.   

i) RNNs update the hidden state based on some non-linear computation using the previous state and input. However, here the input isn't a sequence, its a fixed set of sentence vectors. Are the authors saying that this model is equivalent to an RNN receiving same input at every time step ?  

ii) The purpose of the output embedding is unclear to me. RNNs use only a single embedding of the input. Why can't the attention probabilities can be used to directly select the input embedding (m_i) instead of the using a separate output embedding (c_i) ? Does having a separate embedding serve a purpose other than increasing the parameters ?  

Any help is appreciated.  ",0,2
1009,2016-3-30,2016,3,30,1,4cg8ri,How do professionals feel about kaggle competitions? What else can I do to gain ML experience?,https://www.reddit.com/r/MachineLearning/comments/4cg8ri/how_do_professionals_feel_about_kaggle/,ph3rn,1459269397,"Specifically, admissions officers and recruiters. Does having participated in kaggle competitions count as experience in machine learning? I'm a freshman undergrad and I'm really interested in ML. 

I talked to a professor at my university whose research interest was machine learning and asked how I could get started. He suggested I should head on to kaggle and tackle some of the competitions there. I don't know how helpful this is because I saw some threads here on /r/ML which said kaggle is not very useful if you want actual ML experience.

My question is, how can I get started if I'm serious about it? I already have a list of math courses to take which other redditors suggested, such as Linear Algebra, Real Analysis 1, Linear Programming and Optimization, Probability and Stochastic Processes, etc.

I don't live in the US and my university is not very well known, so I'm afraid I'm already at a disadvantage when it comes to competing with others who go to schools like Carnegie Mellon or Stanford and have great professors who are famous experts on the subject.",24,6
1010,2016-3-30,2016,3,30,1,4cg9sf,"/r/ScienceSubreddits, a massive directory of all science-focused subreddits on Reddit! We currently have 772 subreddits in our directory beautifully sorted for easy discovery. Come check us out and make requests!",https://www.reddit.com/r/MachineLearning/comments/4cg9sf/rsciencesubreddits_a_massive_directory_of_all/,GodRaine,1459269774,,1,1
1011,2016-3-30,2016,3,30,2,4cgcrd,"What is the term for a dataset where the features are not sufficient to predict a target, no matter how many observations you have?",https://www.reddit.com/r/MachineLearning/comments/4cgcrd/what_is_the_term_for_a_dataset_where_the_features/,relganz,1459270853,"I feel like I need to refer to this idea frequently, but I don't have the vocab for it.  For example, let's say you have datapoints for every house in the world, but all you have is ""color"" and ""market value"".  You try to create a model to predict value with color, but can't achieve high accuracy (for obvious reasons).  No fancy NN will save you here.  What is the term to describe this situation?  Thanks!",5,0
1012,2016-3-30,2016,3,30,2,4cgjwe,"Deep Learning for NLP resources (Resources include DL resources, Word embeddings, RNN, Optimization for DL, datasets)",https://www.reddit.com/r/MachineLearning/comments/4cgjwe/deep_learning_for_nlp_resources_resources_include/,shash273,1459273305,,2,11
1013,2016-3-30,2016,3,30,3,4cgoif,Isn't unsupervised image segmentation research useless?,https://www.reddit.com/r/MachineLearning/comments/4cgoif/isnt_unsupervised_image_segmentation_research/,andrewbarto28,1459274878,"I don't get why people still do research in pure image segmentation. The objects of interests to be segmented are very subjective. It is clear the way to move forward is to use supervised learning/semantic segmentation. The only case where it makes sense that I can think of is for regions proposals for R-CNN, but I think these will also be made obsolete soon.",6,0
1014,2016-3-30,2016,3,30,3,4cgojs,Black Box Challenge: we raised prize fund and added Python 3 support! [It's an unusual reinforcement learning competition.],https://www.reddit.com/r/MachineLearning/comments/4cgojs/black_box_challenge_we_raised_prize_fund_and/,blackbox_challenge,1459274893,,1,3
1015,2016-3-30,2016,3,30,3,4cgq7d,Deadly Predator Swarm - AI Bots - LifeNEAT,https://www.reddit.com/r/MachineLearning/comments/4cgq7d/deadly_predator_swarm_ai_bots_lifeneat/,DjOwlz,1459275468,,2,1
1016,2016-3-30,2016,3,30,3,4cgrzl,Can anybody explain to me what in practice the application could be from vector word representation?,https://www.reddit.com/r/MachineLearning/comments/4cgrzl/can_anybody_explain_to_me_what_in_practice_the/,equaljose,1459276103,"I am thinking about using this method to train a model for this as my master thesis. But I didn't had anything about it during my classes, I just found the subject and found it interesting. But honestly I don't understand as of yet where you can use this type of models in practice.",6,1
1017,2016-3-30,2016,3,30,4,4ch28x,Saddles Again  Off the convex path,https://www.reddit.com/r/MachineLearning/comments/4ch28x/saddles_again_off_the_convex_path/,iidealized,1459279722,,0,16
1018,2016-3-30,2016,3,30,4,4ch3io,How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation,https://www.reddit.com/r/MachineLearning/comments/4ch3io/how_not_to_evaluate_your_dialogue_system_an/,pierrelux,1459280156,,0,6
1019,2016-3-30,2016,3,30,4,4ch546,Support Vector Machines: A Guide for Beginners - not too heavy nor too light on the math,https://www.reddit.com/r/MachineLearning/comments/4ch546/support_vector_machines_a_guide_for_beginners_not/,Aedan91,1459280730,,0,7
1020,2016-3-30,2016,3,30,7,4chrjz,DanDoesData: Keras custom activation functions,https://www.reddit.com/r/MachineLearning/comments/4chrjz/dandoesdata_keras_custom_activation_functions/,vanboxel,1459288852,,0,2
1021,2016-3-30,2016,3,30,7,4chu3y,Are there any Recurrent convolutional neural network network implementations out there ?,https://www.reddit.com/r/MachineLearning/comments/4chu3y/are_there_any_recurrent_convolutional_neural/,xplot,1459289818,I was hoping to implement the text classifier mentioned in this paper. https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/download/9745/9552,12,4
1022,2016-3-30,2016,3,30,8,4ci1x4,Alternatives to module cloning/memory magic when training RNNs?,https://www.reddit.com/r/MachineLearning/comments/4ci1x4/alternatives_to_module_cloningmemory_magic_when/,cjmcmurtrie,1459293023,"All RNN implementations I have coded/used/studied (mainly Torch) clone the module once per time step during training. This is an incredible memory overhead that intuitively feels unnecessary. 

Any hackers out there been thinking of ways to limit this when training RNNs?

I'm going to spend time the next days thinking about this, any input welcome!",2,2
1023,2016-3-30,2016,3,30,8,4ci40n,Multi-Task Cross-Lingual Sequence Tagging from Scratch,https://www.reddit.com/r/MachineLearning/comments/4ci40n/multitask_crosslingual_sequence_tagging_from/,spurious_recollectio,1459293846,,1,1
1024,2016-3-30,2016,3,30,9,4cih2l,How to create invariance towards variable placing?,https://www.reddit.com/r/MachineLearning/comments/4cih2l/how_to_create_invariance_towards_variable_placing/,[deleted],1459299304,[deleted],3,6
1025,2016-3-30,2016,3,30,9,4cihor,"For Sequence to Sequence tasks using pre-trained embeddings, why not use cosine distance as a loss function?",https://www.reddit.com/r/MachineLearning/comments/4cihor/for_sequence_to_sequence_tasks_using_pretrained/,throwaway1789503,1459299558,[removed],1,1
1026,2016-3-30,2016,3,30,10,4cihvc,Responding to ICML Reviewers,https://www.reddit.com/r/MachineLearning/comments/4cihvc/responding_to_icml_reviewers/,alexmlamb,1459299626,,8,1
1027,2016-3-30,2016,3,30,10,4cij1j,Can topic modeling be used to solve the kaggle SF Crime challenge?,https://www.reddit.com/r/MachineLearning/comments/4cij1j/can_topic_modeling_be_used_to_solve_the_kaggle_sf/,EatShihtzu,1459300103,"The [SF Crime challenge](https://www.kaggle.com/c/sf-crime): the training set consists of crime-events labeled by a date/time, the police district it occurred in, the lat-lon coordinates, an address, and the crime-category. You're asked to predict the crime-category in a test set.

Could one think of the date/time features as ""words"" generated in a police district (or perhaps yearly/monthly/hourly)  ""document"" with a distribution of crime-category ""topics"" in the spirit of LDA for document classification? I've been reading the work of Blei et. al., and it seems like this methodology could be applied to this type of problem. If I'm on the right track, but don't quite have the analog of graphical model elements quite right, could you explain?

It's there a way of applying some sort of generative model to a problem like this?",5,5
1028,2016-3-30,2016,3,30,10,4cilh1,Why choose Planetary Dispersing Mixing Machine For Lab?,https://www.reddit.com/r/MachineLearning/comments/4cilh1/why_choose_planetary_dispersing_mixing_machine/,mixmachinery,1459301127,,0,0
1029,2016-3-30,2016,3,30,10,4cinem,Platforms for learning on distributed ml framework,https://www.reddit.com/r/MachineLearning/comments/4cinem/platforms_for_learning_on_distributed_ml_framework/,Marbledatum,1459301921,[removed],0,1
1030,2016-3-30,2016,3,30,11,4cirwa,"[1603.08575] Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",https://www.reddit.com/r/MachineLearning/comments/4cirwa/160308575_attend_infer_repeat_fast_scene/,RushAndAPush,1459303803,,12,44
1031,2016-3-30,2016,3,30,11,4cityc,"If neural networks are so great, why can't cats translate Chinese?",https://www.reddit.com/r/MachineLearning/comments/4cityc/if_neural_networks_are_so_great_why_cant_cats/,syncoPete,1459304665,[removed],7,0
1032,2016-3-30,2016,3,30,12,4cj0jl,Quora - lessons learned from building real-life Machine Learning Systems,https://www.reddit.com/r/MachineLearning/comments/4cj0jl/quora_lessons_learned_from_building_reallife/,laampt,1459307512,,0,1
1033,2016-3-30,2016,3,30,12,4cj1jj,[1603.08155] Perceptual Losses for Real-Time Style Transfer and Super-Resolution,https://www.reddit.com/r/MachineLearning/comments/4cj1jj/160308155_perceptual_losses_for_realtime_style/,Klarkshroder,1459307956,,13,23
1034,2016-3-30,2016,3,30,12,4cj412,Table of XX2Vec Algorithms [OC],https://www.reddit.com/r/MachineLearning/comments/4cj412/table_of_xx2vec_algorithms_oc/,michaelmalak,1459309142,,0,5
1035,2016-3-30,2016,3,30,16,4cjrzd,Our first year open sourcing machine learning,https://www.reddit.com/r/MachineLearning/comments/4cjrzd/our_first_year_open_sourcing_machine_learning/,ahousley,1459323121,,0,0
1036,2016-3-30,2016,3,30,18,4ck0w5,Machine Learning for Javascript coders: tips,https://www.reddit.com/r/MachineLearning/comments/4ck0w5/machine_learning_for_javascript_coders_tips/,pieroit,1459329842,,0,0
1037,2016-3-30,2016,3,30,19,4ck4dt,How to create and test algorithms on a dataset ( i'm new to machine learning),https://www.reddit.com/r/MachineLearning/comments/4ck4dt/how_to_create_and_test_algorithms_on_a_dataset_im/,dark--w0lf,1459332187,[removed],0,1
1038,2016-3-30,2016,3,30,21,4ckk1m,Review asked on my research proposal (Recommendations with doc2vec),https://www.reddit.com/r/MachineLearning/comments/4ckk1m/review_asked_on_my_research_proposal/,Joostjansenn,1459341400,"Since my professor doesn't really know the topic I was hoping someone from you could review my review proposal for my thesis. 
I want to use doc2vec to build a recommendation system of trip advisor (or maybe Airbnb, I'm not sure yet). First I want to use a term frequencyinverse document frequency, to check if the text of the review can predict the numerical (or stars) rating of the review. This would (I assume) say something beforehand if my recommendation system will make sense.

Can please someone review my way of thinking and when I'm wrong or when you think my research proposal doesn't make sense, can you please explain to me why. ",11,1
1039,2016-3-30,2016,3,30,23,4ckwa1,Offline Emotion-Specific Speech-to-Text in Low-End Devices,https://www.reddit.com/r/MachineLearning/comments/4ckwa1/offline_emotionspecific_speechtotext_in_lowend/,carlos_argueta,1459346768,,6,7
1040,2016-3-30,2016,3,30,23,4ckwil,Machine Learning as a Service: How Data Science Is Hitting the Masses,https://www.reddit.com/r/MachineLearning/comments/4ckwil/machine_learning_as_a_service_how_data_science_is/,mbolaris,1459346875,,0,0
1041,2016-3-31,2016,3,31,0,4cl61n,"IBM's ""True North"" Neural Processors being tested by Livermore National Lab - 1 KHz, 176,000-fold less energy usage than a conventional processor.",https://www.reddit.com/r/MachineLearning/comments/4cl61n/ibms_true_north_neural_processors_being_tested_by/,locrawl,1459350595,,120,171
1042,2016-3-31,2016,3,31,0,4cldv1,How to create an AI startup - convince some humans to be your training set  Simply Statistics,https://www.reddit.com/r/MachineLearning/comments/4cldv1/how_to_create_an_ai_startup_convince_some_humans/,warisaracket1,1459353469,,0,0
1043,2016-3-31,2016,3,31,1,4clg6x,Dropout (conv vs fc) vs l2 weight decay vs both,https://www.reddit.com/r/MachineLearning/comments/4clg6x/dropout_conv_vs_fc_vs_l2_weight_decay_vs_both/,Schlagv,1459354294,"I have some questions about regularization.

1) I only see dropout used after fully connected layers, but not after conv layers, why ?

2) I never see people using both dropout and l2 weight decay at the same time. Which one is better ? Why shouldn't we use both at the same time ? If dropout is only used for FC layers, should I use weight decay for conv layers ?

3) People seem to speak of l2 loss and  weight decay. As I understand it, l2 loss means subtraction of the weight in the SGD after computing the partial derivative of the l2 norm, so that's why people use both l2 loss and weight decay. For classic SGD, I understand it, but do l2 loss and weight decay also mean the same in other optimisation methods (Adam, RMSProp, momentum and so on) ? Is there a difference, or are the two names 100% equivalent ? Is there another name for l1 loss ?",2,1
1044,2016-3-31,2016,3,31,1,4clj6b,LSTM sequence-wise back propagation of losses in theano,https://www.reddit.com/r/MachineLearning/comments/4clj6b/lstm_sequencewise_back_propagation_of_losses_in/,JNKundu,1459355394,"I want to back-propagate the losses from predictions of each of the sequences through the corresponding sequence output in a single inference.

Can it be possible by creating a scalar loss on top of LSTM output sequence ?

This can be done by defining some function of the LSTM output losses (in symbolic theano tensor variable) from each of the sequence and back-propagate only this scalar loss.",1,0
1045,2016-3-31,2016,3,31,1,4cljdj,AI and ML: 10 Examples in FinTech (Russian),https://www.reddit.com/r/MachineLearning/comments/4cljdj/ai_and_ml_10_examples_in_fintech_russian/,JohnPaulsun,1459355465,,3,0
1046,2016-3-31,2016,3,31,1,4clkwj,new approach to semi-supervised learning: Revisiting Semi-Supervised Learning with Graph Embeddings,https://www.reddit.com/r/MachineLearning/comments/4clkwj/new_approach_to_semisupervised_learning/,lpiloto,1459356047,,0,9
1047,2016-3-31,2016,3,31,1,4clnrg,pomegranate v0.4.0: fast and flexible probabilistic modelling for python,https://www.reddit.com/r/MachineLearning/comments/4clnrg/pomegranate_v040_fast_and_flexible_probabilistic/,ants_rock,1459357082,"Hello again!

pomegranate is a python package for probabilistic modelling with a speedy cython implementation. Its focus was initially on hidden Markov models (which are very fully featured and based off a sparse implementation), but grew into a host of probabilistic models. I've just released 0.4.0 which contains a host of new updates/bug fixes, some nice speed increases, new models, a more unified sklearn api, and an out of core API for training all models with data that can't fit in memory. Our fancy new website has a host of documentation and an API reference for all models: http://pomegranate.readthedocs.org/en/latest/

pomegranate currently supports the following models:

* a wide range of probability distributions
* general mixture models (of any distributions)
* hidden markov models
* naive bayes (of any distributions/models)
* markov chains
* discrete bayesian networks
* factor graphs
* finite state machines

Tutorials on how to use these models are here: https://github.com/jmschrei/pomegranate/tree/master/tutorials

More examples on using these models are here:
https://github.com/jmschrei/pomegranate/tree/master/examples

pomegranate's cython implementation is extremely fast. It does extremely well when [comparing hidden Markov models](https://github.com/jmschrei/pomegranate/blob/master/benchmarks/pomegranate_vs_hmmlearn.ipynb) to [hmmlearn](https://github.com/hmmlearn/hmmlearn),  when [comparing General Mixture models](https://github.com/jmschrei/pomegranate/blob/master/benchmarks/pomegranate_vs_sklearn_gmm.ipynb) to [sklearn](https://github.com/scikit-learn/scikit-learn), and when [comparing Naive Bayes](https://github.com/jmschrei/pomegranate/blob/master/benchmarks/pomegranate_vs_sklearn_naive_bayes.ipynb) to sklearn. In most cases it is several times faster than the other implementations as well as more memory efficient.

Currently multithreaded training is supported for hidden Markov models, and I plan on implementing multithreading for all fitting and prediction methods in the next release. 

I'd love to get your feedback on the project. Installation is as easy as `pip install pomegranate` on any operating system. If you find any bugs or have any suggestions please open an issue on the [github repo](https://github.com/jmschrei/pomegranate).",1,29
1048,2016-3-31,2016,3,31,2,4clufy,AI-written novel passes literary prize screening,https://www.reddit.com/r/MachineLearning/comments/4clufy/aiwritten_novel_passes_literary_prize_screening/,chingaa,1459359476,,4,0
1049,2016-3-31,2016,3,31,3,4cm3ck,"CS Final Year Project coming up, suggestions needed!",https://www.reddit.com/r/MachineLearning/comments/4cm3ck/cs_final_year_project_coming_up_suggestions_needed/,trebuszek,1459362610,"Hi!

I hope this is the right subreddit to ask. I'm nearing my undergrad final year in Computer Science and time has come to choose a subject for my final project. Let me start by saying I don't have much practical experience with machine learning - my background is mainly software development. I did just finish a stats course though and I intend to learn Tensorflow and deep learning from Coursera/Udacity for the purpose of completing this project.

I'll cut to the chase. Turns out my mentor specializes in audio signal processing. I was always interested in computer-generated sounds, so my first thought was to make some sort of a VST plugin synthesizer / weird instrument.

However, recently I found myself thinking more and more about doing something involving deep learning, especially after seeing [projects like this](https://imgur.com/6mDCmRq) and wondering if the same technique could be applied to audio. That is, combining two sounds to generate something else. I'd like to ask you if you think accomplishing this is viable and where to start.

Another possible (although I admit, less exciting) project would involve music classification (i.e by genre or release decade) as described in [this great post](https://www.reddit.com/r/programming/comments/3s4vkn/google_brains_deep_learning_library_tensorflow_is/cwulcew) by /u/gindc. 

I'd love to hear your ideas for an interesting project involving deep learning and sound. Also, advice and any links relevant to the subject are greatly appreciated.  ",7,1
1050,2016-3-31,2016,3,31,3,4cm4lt,An FDA for Algorithms,https://www.reddit.com/r/MachineLearning/comments/4cm4lt/an_fda_for_algorithms/,oblivion_0,1459363045,,4,2
1051,2016-3-31,2016,3,31,4,4cmcfs,How can ResNet CNN go deep to 152 layers (and 200 layers) without running out of channel spatial area?,https://www.reddit.com/r/MachineLearning/comments/4cmcfs/how_can_resnet_cnn_go_deep_to_152_layers_and_200/,hungry_for_knowledge,1459365819,"ResNet uses the bottleneck architecture which has each building block of 1x1, 3x3 and 1x1 conv layers, which does not preserve the spatial dimension. How can they go so deep without running out of spatial area?

Does anyone know the spatial dimension at the last conv layer before (fc1000)? Your sharing is much appreciated!

http://arxiv.org/abs/1512.03385",11,2
1052,2016-3-31,2016,3,31,4,4cmf5h,What are some good strategies for working with Sales Data?,https://www.reddit.com/r/MachineLearning/comments/4cmf5h/what_are_some_good_strategies_for_working_with/,polyglotdev,1459366784,"My company is looking into applying machine learning to help predict future sales of products(we sell some 50K unique SKUs) and generally start applying predictive analysis in our sales strategy.

I'm pretty well versed in the field, but by no means am an expert and was planning on applying some basic strategies to start. 

* Time Series Analysis of Product Sales to provide short term estimates  for sales volumes (was planning on doing simple regression, since seasonality isnt a major factor)

* Market Basket Analysis to determine, which products are sold alongside which other products

* Customer Clustering, based on Buying Patterns (Recency, Frequency, Monetary Value)

* Use SVD on Order History to Come Up with a simple recommendation engine. (What's a good metric for measuring performance on these?)

Are there any other common or simple strategies that are employed that i should be considering",2,0
1053,2016-3-31,2016,3,31,4,4cmhqb,Advantages of LSTMs over ESNs?,https://www.reddit.com/r/MachineLearning/comments/4cmhqb/advantages_of_lstms_over_esns/,ding_bong_bing_dong,1459367699,"I wanted to ask someone who is familiar with **both** echo state networks and LSTMs, what advantages, if any, LSTMs have over ESNs.
From what I understand ESNs achieve long-short term memory capabilities without any special memory units or gates. Training also seems easier, since you train the readout neurons with linear regression. Is there any reason for someone to use an LSTM over an ESN?",9,5
1054,2016-3-31,2016,3,31,4,4cmi0p,Providing Immediate Context to Extracted Entities,https://www.reddit.com/r/MachineLearning/comments/4cmi0p/providing_immediate_context_to_extracted_entities/,eContext_Chris,1459367806,"I'm looking for help/direction for the use of a text classification engine powered by universal taxonomy in making certain ML processes more efficient through providing context to entities extracted from a corpus in real time.  My company, eContext, has curated a universal taxonomy over the past nine years that encompasses everything commercially and socially relevant on the web. It is made up of 650M real user search queries bucketed into 25 vertical categories (Auto, Health, Finance, etc.) containing roughly 450K sub-categories. It's a rule-based system, and we use NLP and nGram chunking to parse long and short form text and map search queries, social posts, web content, blogs, forums, reviews, etc. to the category hierarchy providing structured, topical intelligence to data streams at scale. It is extremely accurate because we've built 55M controlled vocabularies (Ex. rules determine the difference between bowtie pasta &amp; bowtie gift wrapping).

I understand that supervised training models require a corpus of text from which a model can determine entities, ontological connections, and apply statistical models to understand what people, places, things, concepts are and how they may be connected, but we've already built out the taxonomy to understand connections between things, and can provide greater context to ""what"" something truly is. For example, you have a corpus of text containing different types of vehicles - Pick up truck, Ford F150, Toyota Camry, Jetski, bicycle, helicopter, accord, etc. eContext would map each of those to a category resulting in a list of concepts, but further, provide the context automatically by recognizing that all of these things fall into the ""Vehicles"" category. Accord, Toyota Camry, Ford F150, and pick up truck all fall under the Vehicles::Automotive Vehicles branch (implies the connection there), and further Accord and Toyota Camry both fall into a child of Automotive, that being ""sedans"" - Vehicles::Automotive Vehicles::Sedans

were looking to develop and test a hypothesis around the automatic, hierarchal category annotation and enrichment of unstructured text data to improve the efficiency and efficacy of supervised machine learning processes. In your experience, would being able to provide structure and meaning that can be used instead of using existing unstructured text in unsupervised models have benefit? Would this be more useful with a corpus of text from various channels sources that isnt uniform, but represents consumer behavior  search keywords, click stream/URL data, social posts?

Any help anyone could provide would be greatly appreciated, and if you're interested in helping us develop and test a hypothesis, please send me a message. Thanks!

Classification Examples http://imgur.com/B4yr0bg - We've taken the last 488 posts that @realDonaldTrump has posted to Twitter, parsed, and mapped to the category hierarchy for a top level, categorical view of his activity - each color represents a different vertical, so tan=government, yellow=arts &amp; entertainment, green=finance, etc. http://imgur.com/2bdXE2P - Article - Content Classification example - notice the category paths provided, which I think could be a feature or quality to clarify the context of entities/annotations
We have a free UI @ http://classify.econtext.com if you would like to try it out.",0,1
1055,2016-3-31,2016,3,31,5,4cmm2i,What are the Market Drivers &amp; Market Challenges of Global IAQ Air Quality Meters Consumption 2016 Market?,https://www.reddit.com/r/MachineLearning/comments/4cmm2i/what_are_the_market_drivers_market_challenges_of/,tharunvicky,1459369293,,0,1
1056,2016-3-31,2016,3,31,5,4cmrm9,Microsoft is adding the Linux command line to Windows 10,https://www.reddit.com/r/MachineLearning/comments/4cmrm9/microsoft_is_adding_the_linux_command_line_to/,[deleted],1459371370,[deleted],0,1
1057,2016-3-31,2016,3,31,6,4cmu92,Using client data to train a model,https://www.reddit.com/r/MachineLearning/comments/4cmu92/using_client_data_to_train_a_model/,[deleted],1459372378,[deleted],2,1
1058,2016-3-31,2016,3,31,6,4cmzdd,ICML 2016 Workshop on Data-Efficient Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4cmzdd/icml_2016_workshop_on_dataefficient_machine/,skrza,1459374278,,3,8
1059,2016-3-31,2016,3,31,6,4cn19z,Distributed TensorFlow with MPI,https://www.reddit.com/r/MachineLearning/comments/4cn19z/distributed_tensorflow_with_mpi/,technologiclee,1459375012,,2,5
1060,2016-3-31,2016,3,31,7,4cn8h4,What it takes to work at Google DeepMind,https://www.reddit.com/r/MachineLearning/comments/4cn8h4/what_it_takes_to_work_at_google_deepmind/,clbam8,1459377873,,2,0
1061,2016-3-31,2016,3,31,8,4cndf5,How deltas are set when backpropagating an LSTM ?,https://www.reddit.com/r/MachineLearning/comments/4cndf5/how_deltas_are_set_when_backpropagating_an_lstm/,[deleted],1459379905,[deleted],0,2
1062,2016-3-31,2016,3,31,8,4cne1c,HEADS UP! | The Machine Learning Conference NYC | April 15th!,https://www.reddit.com/r/MachineLearning/comments/4cne1c/heads_up_the_machine_learning_conference_nyc/,Friars1993,1459380163,,0,1
1063,2016-3-31,2016,3,31,8,4cng7d,Would it be possible to map words to tokens/classes that aren't known until model evaluation time? (NLP),https://www.reddit.com/r/MachineLearning/comments/4cng7d/would_it_be_possible_to_map_words_to/,yoitsnate,1459381077,"I'd like to be able to take an English language sentence as input and output a query, i.e. SQL, as output.  One sub-section of this task would be mapping words to column names.  If I were using a predefined table, I would simply use the column names as classes, e.g. `id` would be 1, `name` would be 2, `has_avatar` would be 3, and so on, and the system would learn during training that having ""avatar"" or ""avtar"" in the input would lead to classification as wanting the ""has_avatar"" column.

However, I'm interested in the somewhat more difficult use case of mapping words to column names that aren't necessarily known at the time of model training.  For instance, the model would be able to correctly map `CPU count` to `cpu_metrics`, without necessarily knowing about `cpu_metrics` during training.

What would be your suggestions for making something like this work?  I'm not hung up on any particular implementation detail, so fire away.",2,0
1064,2016-3-31,2016,3,31,9,4cnl8i,How to calculate accuracy in cross-validation?,https://www.reddit.com/r/MachineLearning/comments/4cnl8i/how_to_calculate_accuracy_in_crossvalidation/,BlackHawk90,1459383224,"I have a classification problem consisting of two classes. I have around 10000 data pionts and 20 features. I'm doing nested 10-fold cross-validation. I am unsure about calculating the accuracy.

I see two possibilities to calculate the balanced accuracy:

1. Calculating the balanced accuracy for each test fold. This will give 10 balanced accuracy values. Then I can take the mean and standard deviation.

2. Collecting the predicted labels from the test folds. In the end I have a vector of true labels and a vector of predicted labels. From this I can calculate the confusion matrix and the balanced accuracy.

Currently Im doing the second one. Is this ok? If yes, how can I calculate the standard deviation?

Based on the confusion matrix I'm also calculating other measures such as precision, recall etc.",7,0
1065,2016-3-31,2016,3,31,9,4cnn7k,[1603.09025] Recurrent Batch Normalization,https://www.reddit.com/r/MachineLearning/comments/4cnn7k/160309025_recurrent_batch_normalization/,cooijmanstim,1459384057,,24,61
1066,2016-3-31,2016,3,31,9,4cnqyc,"Project ideas, help w/ resources",https://www.reddit.com/r/MachineLearning/comments/4cnqyc/project_ideas_help_w_resources/,bionerd2,1459385684,Hi everyone! I'm taking CS229 (at Stanford) -- an introductory graduate course on ML -- this quarter and there's a final project. I'm hoping to do something kinda substantive (if that's possible) relating to comp bio. I'm wondering where to resources to find open questions in this area. Would NIPS/ICML's workshops on comp bio be the place to look? Where else? Thanks!,0,3
1067,2016-3-31,2016,3,31,10,4cnxtb,What is the state of the art in sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/4cnxtb/what_is_the_state_of_the_art_in_sentiment_analysis/,HoldMyWater,1459388722,"I'm in a little competition. We're all given 10,000 short documents with labels ""Positive"", ""Negative"", or ""Neutral"". We're tasked with training a model on this data which will then be used to classify documents which we do not have access to. Highest accuracy wins.

There are no rules. We can use any resource, and any algorithm.

Rather than just doing a simple bag-of-words approach and training a simple classifier, I wanted to go further. Does anyone here have any helpful resources? Positive/negative word lists? Tips for feature engineering? Classification algorithms which work best?

Cheers.
",8,1
1068,2016-3-31,2016,3,31,11,4co052,"Trouble understanding this ""biaxial LSTM"" for music generation",https://www.reddit.com/r/MachineLearning/comments/4co052/trouble_understanding_this_biaxial_lstm_for_music/,justgivingsomeadvice,1459389738,"I've been looking at the network found [here] (http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/). Having heard a lot of networks' composed music, I think this one perhaps has the best of the openly available samples there is to here.

So I've been trying to understand the network, in particular the LSTM layers. I do know how LSTMs work, but I can't wrap my head around this:

&gt;Then there is the first hidden LSTM stack, which consists of LSTMs that have recurrent connections along the time-axis. The last time-axis layer outputs some note state that represents any time patterns.

&gt;**The second LSTM stack, which is recurrent along the note axis, then scans up from low notes to high notes. At each note-step (equivalent of time-steps) it gets as input**

Can someone help me understand? What does it mean for an LSTM stack to be recurrent along the note axis?",10,3
1069,2016-3-31,2016,3,31,11,4co1sb,[xpost from /r/compsci] I'm writing a tutorial/article series for implementing Neural nets and would love feedback!,https://www.reddit.com/r/MachineLearning/comments/4co1sb/xpost_from_rcompsci_im_writing_a_tutorialarticle/,SirSharpest,1459390538,,11,5
1070,2016-3-31,2016,3,31,12,4cocpc,ICML 2016: Online Advertising Systems Workshop,https://www.reddit.com/r/MachineLearning/comments/4cocpc/icml_2016_online_advertising_systems_workshop/,sharat_sc,1459395549,,0,1
1071,2016-3-31,2016,3,31,12,4coewa,neural network generated ansi art,https://www.reddit.com/r/MachineLearning/comments/4coewa/neural_network_generated_ansi_art/,rodarmor,1459396634,,7,11
1072,2016-3-31,2016,3,31,14,4conlk,Weird Experiment Results: Dividing the work load doesn't work?,https://www.reddit.com/r/MachineLearning/comments/4conlk/weird_experiment_results_dividing_the_work_load/,alephnaught90,1459401175,"I trained 10 simple models on the MNIST dataset (hand written digits) each to recognize one of the digits. And the results were decently good for how simple the models were:

Recognized 0 with 98.71 % accuracy

Recognized 1 with 98.78 % accuracy

Recognized 2 with 97.28 % accuracy

Recognized 3 with 97.07 % accuracy

Recognized 4 with 97.12 % accuracy

Recognized 5 with 94.4 % accuracy

Recognized 6 with 98.24 % accuracy

Recognized 7 with 97.94 % accuracy

Recognized 8 with 92.85 % accuracy

Recognized 9 with 95.08 % accuracy

Then I took all of these models and put them together, and treated them as a voting committee. The one that returned the highest result would be taken as the answer. But this got only 77.13 % accuracy!

I'm incredibly surprised by this. I had expected that dividing the workload up between 10 models, and then having them vote on the answer would yield the best results. Does anyone have an explanation of this?",4,0
1073,2016-3-31,2016,3,31,15,4cotur,[1603.08884] A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data,https://www.reddit.com/r/MachineLearning/comments/4cotur/160308884_a_parallelhierarchical_model_for/,evc123,1459404978,,1,10
1074,2016-3-31,2016,3,31,15,4covv9,neural network model for q-learning othello?,https://www.reddit.com/r/MachineLearning/comments/4covv9/neural_network_model_for_qlearning_othello/,2_bit_encryption,1459406259,"Lately I've been exploring reinforcement learning.  I built a q-learning agent for Othello.  It is table-based, so it obviously doesn't work well because the state-space of Othello is just too big for a table.

So for the past week I've been investigating neural networks as q-learning approximators.  I even put one together using Keras/Theanos, and something's going right because it wins 90% of games against an opponent that plays purely randomly (but gets crushed against monte-carlo, another agent I wrote, even if the simulation time for MC is very very short).

My biggest point of confusion is the design of this neural network.

Obviously I have 64 input nodes, one per board position.  There's one hidden layer of somewhere between 42 and 256 nodes (I'm always changing that to see if I can find a number that performs better).

The real challenge is the output layer.

The most common network model I've seen for TD learning is to just have one output node which gives the evaluation approximation of the board that was passed in.

But Q learning doesn't work that way.  Q learning requires a state,action pair as input.  Anything else and you're not doing Q learning (though you might be doing value iteration instead).  State,action pair goes in, reward comes out.  I guess if I were to interpret that as literally as possible, I'd have 65 inputs: the 64 for all board positions, and 1 for the action I take, and then one output node: the value of that state/action pair that was passed in.

However, that seems ""inorganic"" for a neural network, to me.  I'm not convinced a network will converge well using that scheme.  What I've done instead is have 64 outputs: one for each spot on the board.  I pass in the board state, and then each of the 64 outputs is the value of the action of me playing at that state (of course I have a list of legal positions and just find the legal move with the best value).

This seems more suited to a neural network than the scheme of ""64 for board state + 1 for move goes in, value comes out"".  However, I'm now worried that having 64 output nodes will be too many for an accurate network.  I don't know if it's feasible for the network to converge well when it has to evaluate 64 different moves for all inputs (even if in the end I only care about something like 3-10).

Anyone have advice here?  I'm very new to neural nets, so this is a brand new world to me and my intuition isn't guiding me very well.  Thanks for reading!",11,0
1075,2016-3-31,2016,3,31,15,4coxph,[1603.08983] Adaptive Computation Time for Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4coxph/160308983_adaptive_computation_time_for_recurrent/,pranv,1459407439,,21,54
1076,2016-3-31,2016,3,31,16,4coz5t,Microsoft launches Cognitive Services based on Project Oxford and Bing,https://www.reddit.com/r/MachineLearning/comments/4coz5t/microsoft_launches_cognitive_services_based_on/,[deleted],1459408378,[deleted],0,0
1077,2016-3-31,2016,3,31,16,4cp16v,How would I represent this approach using convolution neural networks?,https://www.reddit.com/r/MachineLearning/comments/4cp16v/how_would_i_represent_this_approach_using/,Blammar,1459409728,"The first layer takes a, say, 3x3 footprint and thresholds it against the mean of the 3x3 footprint, giving a 9 bit value (each input is less than or greater than or equal to the mean.) I then use this 9 bit value to select one of 512 sets of weights, which I then apply to the 3x3 footprint as the input to the nonlinear function (ReLU say), giving a single output for the next layer.

This setup can clearly be back-propagated, as you always know which set of weights was selected. 

What I can't see is how to represent this setup using standard NN components, e.g., what I'd find in Caffe or other frameworks.

Any suggestions? Thanks in advance.",1,0
1078,2016-3-31,2016,3,31,16,4cp21k,How are neurons in deeper layers capable of making more complex decisions than neurons in shallower/earlier layers?,https://www.reddit.com/r/MachineLearning/comments/4cp21k/how_are_neurons_in_deeper_layers_capable_of/,smeebfromso,1459410320,[removed],0,1
1079,2016-3-31,2016,3,31,17,4cp6fy,What linear algebra is good for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4cp6fy/what_linear_algebra_is_good_for_machine_learning/,dsocma,1459413694,"I only had one option for a linear algebra course, so I am stuck with a bad textbook and awful lecturer.

But I know linear algebra is very important in machine learning so I was wondering if anyone had a linear algebra text that they found especially useful for machine learning?  Or online lectures?

Also, what topics from linear algebra are especially important for machine learning?  Or is the whole subject used equally?  Are there areas that play no role in machine learning?",20,14
1080,2016-3-31,2016,3,31,19,4cpgxu,"Microsoft just released a set of ML&amp;AI services with free plans, including CV, Speech, NLP, and more.",https://www.reddit.com/r/MachineLearning/comments/4cpgxu/microsoft_just_released_a_set_of_mlai_services/,Alikont,1459420765,,38,289
1081,2016-3-31,2016,3,31,19,4cphkn,Technology Behind Alphago - Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/4cphkn/technology_behind_alphago_deep_reinforcement/,arnabgho,1459421183,,0,0
1082,2016-3-31,2016,3,31,21,4cpqan,[1603.08511] Colorful Image Colorization,https://www.reddit.com/r/MachineLearning/comments/4cpqan/160308511_colorful_image_colorization/,alexjc,1459426310,,12,29
1083,2016-3-31,2016,3,31,21,4cpqu5,Experts discuss deep learning in healthcare: what are the risks?,https://www.reddit.com/r/MachineLearning/comments/4cpqu5/experts_discuss_deep_learning_in_healthcare_what/,reworksophie,1459426579,,0,1
1084,2016-3-31,2016,3,31,21,4cpv3v,Machine Learning Made Easy,https://www.reddit.com/r/MachineLearning/comments/4cpv3v/machine_learning_made_easy/,dexlabanalytics,1459428557,,0,1
1085,2016-3-31,2016,3,31,23,4cq8eb,Causal Discovery Software Available for Big Data Analysis,https://www.reddit.com/r/MachineLearning/comments/4cq8eb/causal_discovery_software_available_for_big_data/,ctr4causal_discovery,1459434361,"Causal Discovery Software Available for Big Data Analysis

The Center for Causal Discovery (CCD) (www.ccd.pitt.edu) has released the Fast Greedy Search (FGS) algorithm (an optimized version of Chickering's Greedy Equivalence Search algorithm) for use by biomedical investigators who are searching for causal associations in large sets of continuous data.  A technical paper describing the optimization is available at http://arxiv.org/abs/1507.07749.  It is available as free and open source software. This release is just the first step toward providing a suite of algorithms that will assist biomedical researchers in analyzing their data to obtain causal insights.

Using simulated data, FGS was able to learn a causal network on data containing 50,000 variables and 1,000 samples in about 15 minutes on a laptop computer. While FGS does not model hidden variables that cause two or more measured variables, an upcoming release of another algorithm will do so.

FGS is available as a command line implementation (Causal-cmd) that calls a local Java library or as a Java web application (Causal-web) that runs the analysis at the Pittsburgh Supercomputing Center; the APIs can also be run through R (R-causal) or Python (Py-causal). Additional details and instructions for downloading both these versions of the software are available at 
http://www.ccd.pitt.edu/wiki/index.php?title=Tools_and_Software.

Our goal is to help the biomedical community use causal modeling to gain novel insights and drive innovative research, so we hope to make these tools as usable and useful as possible.  We welcome any and all feedback that you might have, which will help us improve this and future releases. ",0,3
1086,2016-3-31,2016,3,31,23,4cq9b4,arXiv-title-fixer: Paper titles as tab titles on Chrome.,https://www.reddit.com/r/MachineLearning/comments/4cq9b4/arxivtitlefixer_paper_titles_as_tab_titles_on/,musically_ut,1459434707,"This is a pet peeve of mine. I usually open a lot of papers from arXiv while researching something in ML and the tab titles in the browser window (which look like random numbers) don't help me at all in going back to references. 

So I created a small [Chrome extension](https://chrome.google.com/webstore/detail/arxiv-title-fixer/pandhendpghoedojdhookbedinmhjien) to solve this for myself.

This works with the latest stable versions of Google Chrome/Chromium.

Hope someone else finds this useful as well.

~
ut

[Source code is on Github.](https://github.com/musically-ut/arXiv-title-fixer)",18,34
1087,2016-3-31,2016,3,31,23,4cqd0r,Are there any good tutorials on how to do ML with an Amazon instance?,https://www.reddit.com/r/MachineLearning/comments/4cqd0r/are_there_any_good_tutorials_on_how_to_do_ml_with/,transhumanist_,1459436180,"I would like to start research on DL but do not have any computational power to do so. My PC only has 8 GB of memory and only a 650M GPU. I guess I can't do anything with it, so I would like to know, how can I make use of an Amazon instance for running some DL programs? I would really really appreciate good tutorials on that, I have no clue on where to start looking for.

Cheers",3,8
