,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2017-1-1,2017,1,1,16,5le4mx,Identifying Clickbaits Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5le4mx/identifying_clickbaits_using_machine_learning/,Lt_Snuffles,1483255442,,0,1
1,2017-1-1,2017,1,1,18,5lefjh,getting started?,https://www.reddit.com/r/MachineLearning/comments/5lefjh/getting_started/,IncrementTimesTwo,1483261615,[removed],0,1
2,2017-1-1,2017,1,1,22,5lf6sc,Variational Auto-encoders vs Restricted Boltzmann Machines,https://www.reddit.com/r/MachineLearning/comments/5lf6sc/variational_autoencoders_vs_restricted_boltzmann/,an0xiousprime,1483278356,,1,1
3,2017-1-2,2017,1,2,0,5lfnd0,Can machine learning algorithms be used to create data that ia similar to the data the algorithm was trained on?,https://www.reddit.com/r/MachineLearning/comments/5lfnd0/can_machine_learning_algorithms_be_used_to_create/,Kramshet,1483286156,[removed],0,1
4,2017-1-2,2017,1,2,2,5lfy3f,ThoughtWorksInc open source'ed DeepLearning.scala: A DSL for creating complex neural networks,https://www.reddit.com/r/MachineLearning/comments/5lfy3f/thoughtworksinc_open_sourceed_deeplearningscala_a/,predef,1483290230,,1,1
5,2017-1-2,2017,1,2,4,5lgko8,Running the convolution step of a trained CNN on an FPGA for real-time performance,https://www.reddit.com/r/MachineLearning/comments/5lgko8/running_the_convolution_step_of_a_trained_cnn_on/,23f34ef32,1483297825,[removed],0,1
6,2017-1-2,2017,1,2,7,5lhjo8,scikit learn problem xpost /mlquestions,https://www.reddit.com/r/MachineLearning/comments/5lhjo8/scikit_learn_problem_xpost_mlquestions/,[deleted],1483309083,[removed],0,1
7,2017-1-2,2017,1,2,7,5lhk0c,[D] sklearn help xpost /mlquestions,https://www.reddit.com/r/MachineLearning/comments/5lhk0c/d_sklearn_help_xpost_mlquestions/,georgeo,1483309181,[removed],10,0
8,2017-1-2,2017,1,2,7,5lhof7,[P] BICO: Speed up k-means on large data sets by using data reduction,https://www.reddit.com/r/MachineLearning/comments/5lhof7/p_bico_speed_up_kmeans_on_large_data_sets_by/,gallmerci,1483310593,,2,30
9,2017-1-2,2017,1,2,9,5li64n,[D] [Humour] Some new year burn for the community,https://www.reddit.com/r/MachineLearning/comments/5li64n/d_humour_some_new_year_burn_for_the_community/,bhaavan,1483316477,,69,52
10,2017-1-2,2017,1,2,9,5lic0b,"Evolutionary Neural Networks, built by me on TensorFlow and Python ",https://www.reddit.com/r/MachineLearning/comments/5lic0b/evolutionary_neural_networks_built_by_me_on/,ThePropterHoc,1483318430,,0,1
11,2017-1-2,2017,1,2,11,5livzv,"[Project][Question] What is a common set of features that are extracted from small musical segments (20 seconds, mono) to use in a logical regression",https://www.reddit.com/r/MachineLearning/comments/5livzv/projectquestion_what_is_a_common_set_of_features/,shmed,1483325360,"Hi everyone,
I'm new to machine learning and I'm trying to set up a small project to allow the machine to ""guess"" howI would rate a small musical segment for me.

As a training data set, lets say I have thousands of small music segments that I personally ratedwith a score from 0 to 100. All segments are20 seconds or less.I'm trying to extractrelevant features from either thefrequency or time domain.I specifically do not want to rely on metadata (letsassume that I do not have access to genre/year/artist/key).

I've done a bit of research online. Most of the feature extractionreadings that I have found are for speech recognition, or rely on metadata (in the cases of music recommendation).

Any pointers or interesting reading as to how to transform an audio segment into an interesting feature set whichcharacterise its musicallity?

Thank you!",12,12
12,2017-1-2,2017,1,2,11,5lixeh,[R] Deeper Depth Prediction with Fully Convolutional Residual Networks,https://www.reddit.com/r/MachineLearning/comments/5lixeh/r_deeper_depth_prediction_with_fully/,KennySmash,1483325891,,1,7
13,2017-1-2,2017,1,2,11,5lixna,Any free datasets of statements classified by Sentiment?,https://www.reddit.com/r/MachineLearning/comments/5lixna/any_free_datasets_of_statements_classified_by/,butWhoWasBee,1483325981,[removed],0,1
14,2017-1-2,2017,1,2,12,5lj3dl,[R] Differential Geometry Boosts Convolutional Neural Networks for Object Detection,https://www.reddit.com/r/MachineLearning/comments/5lj3dl/r_differential_geometry_boosts_convolutional/,KennySmash,1483328096,,15,48
15,2017-1-2,2017,1,2,15,5lju36,WANTED: A career in AI,https://www.reddit.com/r/MachineLearning/comments/5lju36/wanted_a_career_in_ai/,aahmadi,1483338342,[removed],0,1
16,2017-1-2,2017,1,2,15,5ljxly,[D] Suitable ML Research Papers (for a Beginner) to Reproduce,https://www.reddit.com/r/MachineLearning/comments/5ljxly/d_suitable_ml_research_papers_for_a_beginner_to/,upulbandara,1483339847,"I have little experience in ML and planning to reproduce few research papers (preferably using Tensorflow) in order to strengthen my ML knowledge. I'm planning to start with following papers.

1: Show and Tell: A Neural Image Caption Generator [https://arxiv.org/abs/1411.4555] 

2: Predicting Deep Zero-Shot Convolutional Neural Networks
using Textual Descriptions [https://arxiv.org/abs/1506.00511]

Could you please help me to find out few more papers which are worth reading and reproducing? 

Thanks",6,64
17,2017-1-2,2017,1,2,17,5lk9g9,[D] A complete daily plan for studying to become a machine learning engineer.,https://www.reddit.com/r/MachineLearning/comments/5lk9g9/d_a_complete_daily_plan_for_studying_to_become_a/,ZuzooVn,1483345598,,0,1
18,2017-1-2,2017,1,2,17,5lk9vt,[R] The Predictron: End-to-End Learning and Planning (DeepMind),https://www.reddit.com/r/MachineLearning/comments/5lk9vt/r_the_predictron_endtoend_learning_and_planning/,[deleted],1483345826,[deleted],1,3
19,2017-1-2,2017,1,2,17,5lkcuw,Where to start in deep learning?,https://www.reddit.com/r/MachineLearning/comments/5lkcuw/where_to_start_in_deep_learning/,paragkulkarni15,1483347425,[removed],0,1
20,2017-1-2,2017,1,2,18,5lki26,ESI MINESET brings big-data analytics to virtual performance engineering,https://www.reddit.com/r/MachineLearning/comments/5lki26/esi_mineset_brings_bigdata_analytics_to_virtual/,cloudsim738,1483350132,,0,1
21,2017-1-2,2017,1,2,18,5lkifj,[R] [1612.09508] Feedback Networks,https://www.reddit.com/r/MachineLearning/comments/5lkifj/r_161209508_feedback_networks/,aloisg,1483350330,,7,38
22,2017-1-2,2017,1,2,18,5lkjig,Submit to spikefinder challenge to advance neuroscience!,https://www.reddit.com/r/MachineLearning/comments/5lkjig/submit_to_spikefinder_challenge_to_advance/,pberens,1483350913,[removed],0,1
23,2017-1-2,2017,1,2,20,5lkvff,2 6m venner peeling and cutting production line testing all set,https://www.reddit.com/r/MachineLearning/comments/5lkvff/2_6m_venner_peeling_and_cutting_production_line/,woodworking-machine,1483357361,,0,1
24,2017-1-2,2017,1,2,20,5lkw57,Which is the correct order of an ML problem pipeline?,https://www.reddit.com/r/MachineLearning/comments/5lkw57/which_is_the_correct_order_of_an_ml_problem/,hansaw,1483357721,[removed],0,1
25,2017-1-2,2017,1,2,20,5lkx0z,[R] A Story of Discrimination and Unfairness - Prejudice in Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/5lkx0z/r_a_story_of_discrimination_and_unfairness/,luxferre,1483358197,,2,3
26,2017-1-2,2017,1,2,21,5ll4j5,Combining CNN and RNN for spoken language identification,https://www.reddit.com/r/MachineLearning/comments/5ll4j5/combining_cnn_and_rnn_for_spoken_language/,rndnum123,1483361884,,0,1
27,2017-1-2,2017,1,2,22,5ll8re,A Story of Discrimination and Unfairness - Prejudice in Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/5ll8re/a_story_of_discrimination_and_unfairness/,[deleted],1483363701,[deleted],0,1
28,2017-1-2,2017,1,2,23,5lld95,[D] How to initialize/ debug initialization of weights,https://www.reddit.com/r/MachineLearning/comments/5lld95/d_how_to_initialize_debug_initialization_of/,TalkingJellyFish,1483365671,"Hi all. 
I'm working on an RNN model that predicts market movements (For the LOLZ). 
I have a small training set of roughly 3K examples and another 300 points for validation. I'm not expecting to make money with this, but I am expecting that it over fit the data fast. 
I noticed that my model behaves very differently between ""trainings"", in the sense that if I leave all parameters the same and just re run the training I get different behaviors. 
The differences are in time to convergence (which can be very fast or very slow) and accuracy on the test data. 
I assume that the difference is due to initialization of the model parameters (I'm using Tensorflows built in Xavier initializer), since everything else is the same. 
So my questions are
1) Could there be some other cause I'm not thinking of?
2) How would you go about finding an initialization that is consistent? 
I've experimented with adjusting the learning rate, ranging anywhere from 1e1 to 1e6. Definitely more  success with lower learning rates but not consistent. 
Thanks",1,1
29,2017-1-3,2017,1,3,0,5llp3b,[R] Temporal Tessellation for Video Annotation and Summarization,https://www.reddit.com/r/MachineLearning/comments/5llp3b/r_temporal_tessellation_for_video_annotation_and/,dotan_kaufman,1483370318,,4,33
30,2017-1-3,2017,1,3,1,5lm2ga,Where can I find datasets on images to learn machine learning with?,https://www.reddit.com/r/MachineLearning/comments/5lm2ga/where_can_i_find_datasets_on_images_to_learn/,mddrill,1483374815,[removed],0,1
31,2017-1-3,2017,1,3,3,5lmme1,"[P] Built This Tool After I Accidentally Spent $2,300 on Amazon",https://www.reddit.com/r/MachineLearning/comments/5lmme1/p_built_this_tool_after_i_accidentally_spent_2300/,scrappyD00,1483380775,[removed],11,5
32,2017-1-3,2017,1,3,3,5lmork,Machine Learning for Cyber Security,https://www.reddit.com/r/MachineLearning/comments/5lmork/machine_learning_for_cyber_security/,Faizann24,1483381473,,0,1
33,2017-1-3,2017,1,3,5,5ln91b,[R] The Definitive Security Data Science and Machine Learning Guide,https://www.reddit.com/r/MachineLearning/comments/5ln91b/r_the_definitive_security_data_science_and/,galapag0,1483387401,,0,1
34,2017-1-3,2017,1,3,5,5lnasb,Searching for study partners/group,https://www.reddit.com/r/MachineLearning/comments/5lnasb/searching_for_study_partnersgroup/,morningmotherlover,1483387916,[removed],0,1
35,2017-1-3,2017,1,3,6,5lnp6v,A three-part introduction to getting started with GPU-driven deep learning,https://www.reddit.com/r/MachineLearning/comments/5lnp6v/a_threepart_introduction_to_getting_started_with/,socratesKisses,1483392168,,0,1
36,2017-1-3,2017,1,3,6,5lnsqe,When should I use multiple-cell RNNs?,https://www.reddit.com/r/MachineLearning/comments/5lnsqe/when_should_i_use_multiplecell_rnns/,NeuroBoss31,1483393190,[removed],0,1
37,2017-1-3,2017,1,3,6,5lntb4,Why Deep Learning cannot be Applied to Natural Languages Easily,https://www.reddit.com/r/MachineLearning/comments/5lntb4/why_deep_learning_cannot_be_applied_to_natural/,[deleted],1483393374,[deleted],0,1
38,2017-1-3,2017,1,3,9,5losnd,Summary of NIPS 2016,https://www.reddit.com/r/MachineLearning/comments/5losnd/summary_of_nips_2016/,[deleted],1483404224,[deleted],0,1
39,2017-1-3,2017,1,3,9,5lovdn,If computers can win the game Go why can't computers predict a lottery number or a soccer match?,https://www.reddit.com/r/MachineLearning/comments/5lovdn/if_computers_can_win_the_game_go_why_cant/,_nur,1483405127,[removed],0,1
40,2017-1-3,2017,1,3,10,5lp1aj,the link of `Another dataset list ` under the title of Datasets and Challenges for Beginners got a 404 error,https://www.reddit.com/r/MachineLearning/comments/5lp1aj/the_link_of_another_dataset_list_under_the_title/,LancelotHolmes,1483407003,,0,1
41,2017-1-3,2017,1,3,10,5lp1ep,[R] Fused-Layer CNN Accelerators,https://www.reddit.com/r/MachineLearning/comments/5lp1ep/r_fusedlayer_cnn_accelerators/,KennySmash,1483407038,,4,8
42,2017-1-3,2017,1,3,11,5lpgn8,[R] NIPS 2016 Tutorial: Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5lpgn8/r_nips_2016_tutorial_generative_adversarial/,clbam8,1483412112,,16,135
43,2017-1-3,2017,1,3,13,5lptq3,[N] Summary of NIPS 2016,https://www.reddit.com/r/MachineLearning/comments/5lptq3/n_summary_of_nips_2016/,rawnlq,1483416576,,3,53
44,2017-1-3,2017,1,3,14,5lq3ga,Deep Reinforcement Learning for Dialogue Generation,https://www.reddit.com/r/MachineLearning/comments/5lq3ga/deep_reinforcement_learning_for_dialogue/,[deleted],1483420125,[deleted],0,1
45,2017-1-3,2017,1,3,14,5lq48b,[D] Increasing dimension in residual block in Resnet,https://www.reddit.com/r/MachineLearning/comments/5lq48b/d_increasing_dimension_in_residual_block_in_resnet/,nimakhin,1483420433,"I am reading the original paper of Resnet (https://arxiv.org/abs/1512.03385) , I'm confused with the residual block of their architecture.
In page 4, in the residual network paragraph they say ""when the dimension increase, we consider two options A) the shortcut still performs Identity mapping, with extra zero entries padded for increasing dimensions"" , here is my understanding
looking at the Figure 3, the first dotted line, gets its 64x56x56 features and wants to add it to the 128x28x28 features (at its end) which is not possible so my questions are:
1) is my understanding correct?
2) why this is called increase in dimension? 56x56 to 28x28 is a decrease.
3) how it is solved by zero padding? Zeroes are added to 28x28? How to convert 64 to 128?

I appreciate your help",4,3
46,2017-1-3,2017,1,3,14,5lq5kv,How to become data scientist,https://www.reddit.com/r/MachineLearning/comments/5lq5kv/how_to_become_data_scientist/,CoolCK0x009,1483420923,,0,1
47,2017-1-3,2017,1,3,15,5lqhhr,Why Deep Learning cannot be Applied to Natural Languages Easily,https://www.reddit.com/r/MachineLearning/comments/5lqhhr/why_deep_learning_cannot_be_applied_to_natural/,[deleted],1483425623,[deleted],0,1
48,2017-1-3,2017,1,3,15,5lqibp,High Moisture Raw Material Crusher Machine Grinding Mill Machine Supplier,https://www.reddit.com/r/MachineLearning/comments/5lqibp/high_moisture_raw_material_crusher_machine/,frady2015,1483426018,,1,1
49,2017-1-3,2017,1,3,16,5lqnkt,[R] Visualizing the effects of neural network architectural choices on the prediction surface using 2D data,https://www.reddit.com/r/MachineLearning/comments/5lqnkt/r_visualizing_the_effects_of_neural_network/,[deleted],1483428323,[deleted],0,1
50,2017-1-3,2017,1,3,16,5lqrc1,[R] Visualizing the effects of neural network architectural choices on the prediction surface using 2D data,https://www.reddit.com/r/MachineLearning/comments/5lqrc1/r_visualizing_the_effects_of_neural_network/,tanmay2099,1483430069,,0,1
51,2017-1-3,2017,1,3,19,5lr90g,[D] Benchmarks can either encourage or constrain innovation,https://www.reddit.com/r/MachineLearning/comments/5lr90g/d_benchmarks_can_either_encourage_or_constrain/,j_lyf,1483439159,,2,10
52,2017-1-3,2017,1,3,20,5lrg1k,O que  Machine Learning e como aprender sem gastar nada,https://www.reddit.com/r/MachineLearning/comments/5lrg1k/o_que__machine_learning_e_como_aprender_sem/,paulo_zip,1483442717,,0,1
53,2017-1-3,2017,1,3,21,5lrnjp,Variational autoencoder's relation to Bayesian Deep Learning ?,https://www.reddit.com/r/MachineLearning/comments/5lrnjp/variational_autoencoders_relation_to_bayesian/,rishok,1483446213,[removed],0,1
54,2017-1-3,2017,1,3,22,5lrt3n,Haval pompa,https://www.reddit.com/r/MachineLearning/comments/5lrt3n/haval_pompa/,gunaymedya,1483448545,,0,1
55,2017-1-3,2017,1,3,22,5lrzhg,[Discussion] Prof. Schmidhuber - The Problems of AI Consciousness and Unsupervised Learning Are Already Solved,https://www.reddit.com/r/MachineLearning/comments/5lrzhg/discussion_prof_schmidhuber_the_problems_of_ai/,metacurse,1483451118,,173,154
56,2017-1-3,2017,1,3,23,5ls9sz,"[P] A detailed, step by step video course on using XGBoost algorithm",https://www.reddit.com/r/MachineLearning/comments/5ls9sz/p_a_detailed_step_by_step_video_course_on_using/,khozzy,1483454898,,1,28
57,2017-1-3,2017,1,3,23,5ls9u8,[Project]Chest Xray image analysis using Deep learning and Transfer Learning.,https://www.reddit.com/r/MachineLearning/comments/5ls9u8/projectchest_xray_image_analysis_using_deep/,ayush0016,1483454909,,8,17
58,2017-1-4,2017,1,4,2,5lt49w,Tensorflow best practices and reference implementations?,https://www.reddit.com/r/MachineLearning/comments/5lt49w/tensorflow_best_practices_and_reference/,bertdb,1483464120,[removed],0,1
59,2017-1-4,2017,1,4,2,5lt4nf,Google Cloud Natural Language API: improved sentiment analysis and other features,https://www.reddit.com/r/MachineLearning/comments/5lt4nf/google_cloud_natural_language_api_improved/,jkestelyn,1483464223,,0,1
60,2017-1-4,2017,1,4,2,5lt7qe,[P] Multi-Objective Problems: Introduction,https://www.reddit.com/r/MachineLearning/comments/5lt7qe/p_multiobjective_problems_introduction/,shahinrostami,1483465071,,1,15
61,2017-1-4,2017,1,4,2,5ltaah,What is harmonic embedding in facenet?,https://www.reddit.com/r/MachineLearning/comments/5ltaah/what_is_harmonic_embedding_in_facenet/,mackie__m,1483465755,[removed],0,1
62,2017-1-4,2017,1,4,3,5ltjr6,[Discussion] Research groups in Germany working on RL?,https://www.reddit.com/r/MachineLearning/comments/5ltjr6/discussion_research_groups_in_germany_working_on/,[deleted],1483468311,[deleted],14,4
63,2017-1-4,2017,1,4,4,5ltr3q,What are the practical applications of GANs?,https://www.reddit.com/r/MachineLearning/comments/5ltr3q/what_are_the_practical_applications_of_gans/,hazard02,1483470256,[removed],0,1
64,2017-1-4,2017,1,4,4,5ltu48,What is state of the art in image recognition ML?,https://www.reddit.com/r/MachineLearning/comments/5ltu48/what_is_state_of_the_art_in_image_recognition_ml/,FickleLife,1483471076,[removed],0,1
65,2017-1-4,2017,1,4,4,5ltxoh,"[D] LSTM with other activation functions instead of the hyperbolic tangent, is it ever useful?",https://www.reddit.com/r/MachineLearning/comments/5ltxoh/d_lstm_with_other_activation_functions_instead_of/,carlthome,1483472096,"It seems important that the activation function is a sigmoid between -1.0 and 1.0 for a LSTM to work effectively (in order to not only increment the memory, but also to be able to decrement it as much if needed, like for example when opening vs. closing parens in a NLP task). Is there any particular reason why so many frameworks expose the activation function as a parameter, instead of just hard coding a tanh? Are there ever circumstances where other activation functions might be useful/better?",2,4
66,2017-1-4,2017,1,4,4,5lu1hb,DeepMinds work in 2016: a round-up | DeepMind,https://www.reddit.com/r/MachineLearning/comments/5lu1hb/deepminds_work_in_2016_a_roundup_deepmind/,circuithunter,1483473129,,0,1
67,2017-1-4,2017,1,4,5,5lu4z8,[R] Gumbel Machinery,https://www.reddit.com/r/MachineLearning/comments/5lu4z8/r_gumbel_machinery/,antiprior,1483474042,,0,39
68,2017-1-4,2017,1,4,5,5lu9a1,[D] Recommend some papers for an introduction to machine learnings role in genomics and cancer research?,https://www.reddit.com/r/MachineLearning/comments/5lu9a1/d_recommend_some_papers_for_an_introduction_to/,Wonnk13,1483475171,"I was recently diagnosed with stage 2 colon cancer. Because (otherwise healthy) 28 year old triathletes with colon cancer are in short supply my surgeon wanted me to participate in some of his research. I am really stoked to be a real life observation in someone's dataset! 

These discussions opened a whole new world to me. I work professionally as a data scientist but in the infosec / finance space. My knowledge of genomics / cancer extends no further than Miss Frizzle and the Magic Schoolbus. 

I'd love to read some papers or see projects that would be a good survey of work being done in the field. Is there a benchmark dataset for cancer (something like MNIST) I could play with? ",5,13
69,2017-1-4,2017,1,4,6,5luqfl,[Discussion] How utterance length affect neural network in speaker recognition?,https://www.reddit.com/r/MachineLearning/comments/5luqfl/discussion_how_utterance_length_affect_neural/,Lukaskar,1483479824,I'm learning neural networks and trying to create speaker recognition system with tensorflow. I wanted to know how utterance length affect neural network. For example I have 1000 different sound recordings with the same lengths and 1000 different sound recordings with different lenghts. So how theoretically will work neural network with these kind of datas? Will neural network with database of same length recordings will do better or worse? Why?,2,1
70,2017-1-4,2017,1,4,7,5luv45,How Do Neural Networks See The World? The coolest and simplest explanation I've seen of feature visualization and multifaceted feature visualization.,https://www.reddit.com/r/MachineLearning/comments/5luv45/how_do_neural_networks_see_the_world_the_coolest/,[deleted],1483481080,[deleted],0,1
71,2017-1-4,2017,1,4,7,5lv5di,Can you help me understand how google parses through emails to know my flight information,https://www.reddit.com/r/MachineLearning/comments/5lv5di/can_you_help_me_understand_how_google_parses/,sudocaptain,1483483927,[removed],0,1
72,2017-1-4,2017,1,4,8,5lvatb,Machine learning to sort through pictures?,https://www.reddit.com/r/MachineLearning/comments/5lvatb/machine_learning_to_sort_through_pictures/,[deleted],1483485429,[removed],0,1
73,2017-1-4,2017,1,4,8,5lvdzu,DeepMinds work in 2016: a round-up,https://www.reddit.com/r/MachineLearning/comments/5lvdzu/deepminds_work_in_2016_a_roundup/,Gamareth,1483486313,,0,1
74,2017-1-4,2017,1,4,9,5lvux9,Convex Optimization using CVXPY,https://www.reddit.com/r/MachineLearning/comments/5lvux9/convex_optimization_using_cvxpy/,[deleted],1483491100,[deleted],0,1
75,2017-1-4,2017,1,4,9,5lvvwf,[P] Convex Optimization using CVXPY,https://www.reddit.com/r/MachineLearning/comments/5lvvwf/p_convex_optimization_using_cvxpy/,napsternxg,1483491396,,0,7
76,2017-1-4,2017,1,4,10,5lvy2v,Why Organizations Need Adaptive Multi-factor Authentication (MFA),https://www.reddit.com/r/MachineLearning/comments/5lvy2v/why_organizations_need_adaptive_multifactor/,Manic_Psycho1,1483492042,,0,1
77,2017-1-4,2017,1,4,10,5lw4k0,[R] Akid: A Library for Neural Network Research and Production from a Dataism Approach,https://www.reddit.com/r/MachineLearning/comments/5lw4k0/r_akid_a_library_for_neural_network_research_and/,DeepSaurus,1483493983,,2,4
78,2017-1-4,2017,1,4,11,5lwcue,Deep Learning Needs Assembly Hackers,https://www.reddit.com/r/MachineLearning/comments/5lwcue/deep_learning_needs_assembly_hackers/,dendisuhubdy,1483496523,,0,1
79,2017-1-4,2017,1,4,11,5lwf2b,Asymmetric memory usage on multi-gpu training in torch?,https://www.reddit.com/r/MachineLearning/comments/5lwf2b/asymmetric_memory_usage_on_multigpu_training_in/,dimmtree,1483497226,[removed],0,1
80,2017-1-4,2017,1,4,13,5lwztm,What were some of the best ML papers of 2016? Some of the best reviews? The best in computational stats?,https://www.reddit.com/r/MachineLearning/comments/5lwztm/what_were_some_of_the_best_ml_papers_of_2016_some/,Zeekawla99ii,1483503940,[removed],0,1
81,2017-1-4,2017,1,4,13,5lx2rn,[Research] Using machine learning a 4D image in order to predict another 4D image,https://www.reddit.com/r/MachineLearning/comments/5lx2rn/research_using_machine_learning_a_4d_image_in/,good_research,1483504961,"I'm looking at using machine learning to look at the relationship between fMRI time series. Both input and output should be 4D (x, y, z, t). 

It seems like a convolutional neural network would be applicable in that it would capture the spatial and temporal relationships between proximal voxels. However I haven't seen any applications to matrices of such high dimensionality. Before I get too far into it, is there a practical or theoretical reason that this is not possible?",6,6
82,2017-1-4,2017,1,4,14,5lx7px,[P] Pre-trained RNN chatbot,https://www.reddit.com/r/MachineLearning/comments/5lx7px/p_pretrained_rnn_chatbot/,penderprime,1483506664,"Here is a [pre-trained TensorFlow-powered chatbot](https://github.com/pender/chatbot-rnn) that was trained on many gigabytes of Reddit conversations for over 37 days with a Titan X GPU.

The underlying model is a character-based sequence predictor and it uses optional beam search and relevance masking/MMI to formulate its responses. It comes with a python script for interactive chatting.

A sample dialogue transcript (which was not cherry-picked) is included in the github readme.

Also included is a python script to assemble conversations from raw Reddit data, which I hope may be of interest for other similar projects.",61,130
83,2017-1-4,2017,1,4,15,5lxn6b,Need Help for SFPD Incidents data set classification using TensorFlow / tflearn,https://www.reddit.com/r/MachineLearning/comments/5lxn6b/need_help_for_sfpd_incidents_data_set/,pikaynu,1483512519,[removed],0,1
84,2017-1-4,2017,1,4,20,5lyq7c,[P] Oversight - Using inception v3 for home security,https://www.reddit.com/r/MachineLearning/comments/5lyq7c/p_oversight_using_inception_v3_for_home_security/,hebenon_01,1483531094,"It's not clever or new, but I've [created a project](https://github.com/hebenon/oversight) that retrains the Tensorflow inception v3 model and uses it to detect and notify events from IP cameras. It supports sending screen grabs via SMTP, as well as push notifications to a mobile handset via Pushover.

I put in some IP cameras at home, but struggled with getting the inbuilt motion detection system to give sane results. Machine learning to the rescue! There's a bunch of things that I think could still be done (e.g. secrets management, and I've been experimenting with a show-and-tell approach too), but it's here for anyone who might find it useful.",4,14
85,2017-1-4,2017,1,4,21,5lywh5,[N] Unknown bot repeatedly beats top Go players online - so far it's undefeated.,https://www.reddit.com/r/MachineLearning/comments/5lywh5/n_unknown_bot_repeatedly_beats_top_go_players/,undefdev,1483533759,,54,281
86,2017-1-4,2017,1,4,22,5lz65q,Is It Still Worth It To Read Minsky and Papert's 'Perceptrons' Today?,https://www.reddit.com/r/MachineLearning/comments/5lz65q/is_it_still_worth_it_to_read_minsky_and_paperts/,igotthepancakes,1483537421,[removed],0,1
87,2017-1-4,2017,1,4,22,5lz7v3,[R] How Do Neural Networks See The World? The coolest and simplest explanation I've seen of feature visualization and multifaceted feature visualization.,https://www.reddit.com/r/MachineLearning/comments/5lz7v3/r_how_do_neural_networks_see_the_world_the/,TheJonManley,1483538054,,3,81
88,2017-1-4,2017,1,4,22,5lz8a5,Here is the January 2017 issue of Computer Vision News,https://www.reddit.com/r/MachineLearning/comments/5lz8a5/here_is_the_january_2017_issue_of_computer_vision/,finallyifoundvalidUN,1483538190,,0,1
89,2017-1-4,2017,1,4,23,5lz9j7,How to used high speed shear mixer?,https://www.reddit.com/r/MachineLearning/comments/5lz9j7/how_to_used_high_speed_shear_mixer/,mixmachinery,1483538626,,1,1
90,2017-1-4,2017,1,4,23,5lz9v4,Air Writing using Machine Learning (Hidden Markov Model),https://www.reddit.com/r/MachineLearning/comments/5lz9v4/air_writing_using_machine_learning_hidden_markov/,karuntjohn,1483538741,[removed],0,1
91,2017-1-4,2017,1,4,23,5lzi77,Deep learning - how to approach the problem when the output itself has effect on the results,https://www.reddit.com/r/MachineLearning/comments/5lzi77/deep_learning_how_to_approach_the_problem_when/,nbd,1483541507,[removed],0,1
92,2017-1-5,2017,1,5,0,5lznh5,Char RNN with sparse representation in Keras not learning,https://www.reddit.com/r/MachineLearning/comments/5lznh5/char_rnn_with_sparse_representation_in_keras_not/,[deleted],1483543149,[removed],0,1
93,2017-1-5,2017,1,5,0,5lznpj,The Simple Economics of Machine Intelligence,https://www.reddit.com/r/MachineLearning/comments/5lznpj/the_simple_economics_of_machine_intelligence/,Alexey75,1483543229,,0,1
94,2017-1-5,2017,1,5,0,5lzpul,AlphaGo just win 60 games in a row against 60 world top Go players,https://www.reddit.com/r/MachineLearning/comments/5lzpul/alphago_just_win_60_games_in_a_row_against_60/,FiniteElemente,1483543853,[removed],0,1
95,2017-1-5,2017,1,5,0,5lzuc2,"Simple Questions Thread January 04, 2017",https://www.reddit.com/r/MachineLearning/comments/5lzuc2/simple_questions_thread_january_04_2017/,AutoModerator,1483545157,[removed],0,1
96,2017-1-5,2017,1,5,1,5lzxh7,[D]Weight Decay and BatchNormalization,https://www.reddit.com/r/MachineLearning/comments/5lzxh7/dweight_decay_and_batchnormalization/,Ouitos,1483546049,"As I was reading batchNorm paper ([here](https://arxiv.org/pdf/1502.03167v3.pdf), but i suppose everyone knows it) , it occurred to me that there may be a problem with L2 weight Decay.
As explained, we have during training:

 `BN(Wu) = BN((aW)u)` with **W** the weights of the up Layer, **u** its input, and *a* a non-zero scalar

So applying a scale on weights of the layer before our BN is not changing anything. Now suppose we are at the end of convergence, which means gradient is statistically zero. If we stick to weightDecay, shouldn't we get something similar to

W_i &lt;-- W_i - r\*dL/dW_i , with dL/dW_i = lambda\*W_i

W_i &lt;-- W_i\*(1-r\*lambda)

and get for each W_i an exponential decay ? as said earlier it would be insivisible, the only ""parameters"" (which are not for WeightDecay) that will go higher to compensate will be 1/running_var for inference.

Although I thought of that, and that can explain some recipes with Wd going to 0 at the end (e.g., [here](https://github.com/soumith/imagenet-multiGPU.torch/blob/master/train.lua) ), there are some others that simply don't care about that (like [here](https://github.com/szagoruyko/cifar.torch/blob/master/train.lua) ) but the training still seems to be fine.

the original BN paper says this about weightDecay :

&gt;*Reduce the L2 weight regularization.*
&gt;
&gt;While in Inception
&gt; an L2 loss on the model parameters controls overfitting,
&gt; in Modified BN-Inception the weight of this loss is
&gt; reduced by a factor of 5. We find that this improves the
&gt; accuracy on the held-out validation data.

so yeah, reducing it, but what about simply removing it and normalizing **preBN**-*weight distributions* every now and then ?",3,7
97,2017-1-5,2017,1,5,1,5m01xk,[Hiring] Mentor to guide me through using Azure ML Studio to identify Anomalous/Malicious behavior using web logs (x-post r/BigDataJobs),https://www.reddit.com/r/MachineLearning/comments/5m01xk/hiring_mentor_to_guide_me_through_using_azure_ml/,InfoSecMonkey,1483547290,,0,1
98,2017-1-5,2017,1,5,1,5m08n1,TensorKart: self-driving MarioKart with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5m08n1/tensorkart_selfdriving_mariokart_with_tensorflow/,jkestelyn,1483549088,,0,3
99,2017-1-5,2017,1,5,2,5m0dux,How Cornershop uses Machine Learning to improve its time estimations [Spanish],https://www.reddit.com/r/MachineLearning/comments/5m0dux/how_cornershop_uses_machine_learning_to_improve/,xMatias_LAS,1483550459,,0,1
100,2017-1-5,2017,1,5,3,5m0pck,Best 2016 MOOC in Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/5m0pck/best_2016_mooc_in_deep_learning/,YourWelcomeOrMine,1483553536,[removed],0,1
101,2017-1-5,2017,1,5,3,5m0q5s,Anyone know of good text-sentiment classifiers?,https://www.reddit.com/r/MachineLearning/comments/5m0q5s/anyone_know_of_good_textsentiment_classifiers/,Nixonite,1483553756,[removed],0,1
102,2017-1-5,2017,1,5,3,5m0ye3,[D] Weight Normalization = old technique with minor tweak?,https://www.reddit.com/r/MachineLearning/comments/5m0ye3/d_weight_normalization_old_technique_with_minor/,Pieranha,1483555936,"I was interested in applying weight normalization (cf. [this paper](http://papers.nips.cc/paper/6113-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)) to a network trained in Keras with Theano back-end. Then I stumbled across this [Keras pull request](https://github.com/fchollet/keras/pull/4436), where there's a discussion on the importance of the type of weight normalization introduced in the paper versus other another kind of weight normalization ""that has been in use for years"".

I don't know much about weight normalization, but I'm intrigued. Does this new kind of weight normalization improve performance much over the previous kind of weight normalization? What are your thoughts/experiences?",15,3
103,2017-1-5,2017,1,5,4,5m11r8,[D]Network pruning and weight distribution,https://www.reddit.com/r/MachineLearning/comments/5m11r8/dnetwork_pruning_and_weight_distribution/,[deleted],1483556820,[deleted],0,1
104,2017-1-5,2017,1,5,4,5m13bz,[D] Is setting up Nvidia GPU easier and more stable on Windows? Ubuntu is horrible -- Too easy to mistakenly trash a working video driver setup.,https://www.reddit.com/r/MachineLearning/comments/5m13bz/d_is_setting_up_nvidia_gpu_easier_and_more_stable/,datasciguy-aaay,1483557239,[removed],0,1
105,2017-1-5,2017,1,5,4,5m18rs,[D] Motivation for the reparameterization trick,https://www.reddit.com/r/MachineLearning/comments/5m18rs/d_motivation_for_the_reparameterization_trick/,bronxbomber92,1483558682,"The [reparameterization trick](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/) claims to alleviate problems encountered when computing gradients through random variables (e.g. when computing the gradient of [f(z)] where the expectation is wrt a distribution p(z; ) and the gradient is wrt ). **What precisely are these problems that the reparameterization trick helps with?**

In the expectation above, if both f and p are differentiable and p can be sampled from, how does the reparameterization trick improve upon an Monte Carlo estimate of the gradient of f(z)p(z) wrt ?

The motivations for the reparameterization trick that I've found in blogs posts and papers are all very vague to me. Here are a few excerpts:

[[1]](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/)
&gt; One oft-encountered problem is computing the gradient of an expectation of a smooth function f:
&gt; 
&gt; [f(z)]=p(z;)f(z)dz
&gt; 
&gt;This is a recurring task in machine learning, needed for posterior computation in variational inference, value function and policy learning in reinforcement learning, derivative pricing in computational finance, and inventory control in operations research, amongst many others. **This gradient is often difficult to compute because the integral is typically unknown and the parameters , with respect to which we are computing the gradient, are of the distribution p(z;).**

[[2]](https://arxiv.org/abs/1401.4082)
&gt; Gradient descent methods in latent variable models typically require computations of the form  Eq [f ()], where the expectation is taken with respect to a distribution q() with parameters , and f is a loss function that we assume to be integrable and smooth. **This quantity is difficult to compute directly since i) the expectation is unknown for most problems, and ii) there is an indirect dependency on the parameters of q over which the expectation is taken.**

  
**Edit** - I think another way to phrase this question is as follows: why is it difficult to backpropogate through the sampling processing?

**Edit 2** - I found the answer which I've detailed in a post below! I'll keep this post here to serve as a reference incase anyone else has the same confusion in the future :).",6,13
106,2017-1-5,2017,1,5,4,5m193p,The Art of Missing Value Imputations - DZone Big Data,https://www.reddit.com/r/MachineLearning/comments/5m193p/the_art_of_missing_value_imputations_dzone_big/,Sibanjan,1483558771,,0,1
107,2017-1-5,2017,1,5,5,5m1g1x,How to Install OpenAI's Universe and Make a Game Bot [LIVE],https://www.reddit.com/r/MachineLearning/comments/5m1g1x/how_to_install_openais_universe_and_make_a_game/,funtwo2,1483560577,,0,1
108,2017-1-5,2017,1,5,5,5m1i4x,[N] 5 Big Predictions for Artificial Intelligence in 2017,https://www.reddit.com/r/MachineLearning/comments/5m1i4x/n_5_big_predictions_for_artificial_intelligence/,KennySmash,1483561129,,0,0
109,2017-1-5,2017,1,5,6,5m1wsn,[D] Help choosing math courses,https://www.reddit.com/r/MachineLearning/comments/5m1wsn/d_help_choosing_math_courses/,sleeppropagation,1483564997,[removed],0,0
110,2017-1-5,2017,1,5,8,5m2nnh,[D] Random Forests vs. Neural Nets on Times Series?,https://www.reddit.com/r/MachineLearning/comments/5m2nnh/d_random_forests_vs_neural_nets_on_times_series/,congerous,1483572388,"Is any one aware of papers that compare the performance of random forests and neural networks on time series data or otherwise discuss their relative pros and cons? Obviously neural nets are SOTA for unstructured media like images, sound and text, but I'm curious about how they perform on log activity, transactions etc.",35,10
111,2017-1-5,2017,1,5,9,5m2xax,Machine Learning Curriculum; The ultimate list,https://www.reddit.com/r/MachineLearning/comments/5m2xax/machine_learning_curriculum_the_ultimate_list/,off99555,1483575180,,0,1
112,2017-1-5,2017,1,5,9,5m34wn,Problem in RNN attention decoder in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5m34wn/problem_in_rnn_attention_decoder_in_tensorflow/,zzw922cn,1483577389,[removed],0,1
113,2017-1-5,2017,1,5,10,5m3aw8,Is there any difference between the various models of GTX1080 cards from a deep learning point of view?,https://www.reddit.com/r/MachineLearning/comments/5m3aw8/is_there_any_difference_between_the_various/,ThatGuyFromMexico,1483579240,[removed],0,1
114,2017-1-5,2017,1,5,10,5m3dwa,Switch careers from ASIC verification to ML?,https://www.reddit.com/r/MachineLearning/comments/5m3dwa/switch_careers_from_asic_verification_to_ml/,corenfro,1483580193,[removed],0,1
115,2017-1-5,2017,1,5,11,5m3lz5,Neural Net vs Bayesian Network for classification,https://www.reddit.com/r/MachineLearning/comments/5m3lz5/neural_net_vs_bayesian_network_for_classification/,sudzikle,1483582737,[removed],0,1
116,2017-1-5,2017,1,5,11,5m3ojy,Best Practices for Different Size Images?,https://www.reddit.com/r/MachineLearning/comments/5m3ojy/best_practices_for_different_size_images/,USMCamp0811,1483583580,[removed],0,1
117,2017-1-5,2017,1,5,12,5m3w8s,Hierarchical Classification (on R),https://www.reddit.com/r/MachineLearning/comments/5m3w8s/hierarchical_classification_on_r/,bzamith,1483586061,[removed],0,1
118,2017-1-5,2017,1,5,13,5m49lu,A neural turing machine built from scratch,https://www.reddit.com/r/MachineLearning/comments/5m49lu/a_neural_turing_machine_built_from_scratch/,[deleted],1483590393,[deleted],0,1
119,2017-1-5,2017,1,5,13,5m4bus,A neural turing machine built from scratch on numpy,https://www.reddit.com/r/MachineLearning/comments/5m4bus/a_neural_turing_machine_built_from_scratch_on/,thtrieu,1483591143,,0,1
120,2017-1-5,2017,1,5,15,5m4wzm,How to Use Machine Learning Algorithms in Weka,https://www.reddit.com/r/MachineLearning/comments/5m4wzm/how_to_use_machine_learning_algorithms_in_weka/,pmz,1483598997,,0,2
121,2017-1-5,2017,1,5,16,5m54os,Nvidia is ramping up it's AI game to tackle the self driving car market,https://www.reddit.com/r/MachineLearning/comments/5m54os/nvidia_is_ramping_up_its_ai_game_to_tackle_the/,shaialon,1483602363,,0,1
122,2017-1-5,2017,1,5,19,5m5m0a,There Are Three Different Kinds Of Companies Working On Machine Learning Today,https://www.reddit.com/r/MachineLearning/comments/5m5m0a/there_are_three_different_kinds_of_companies/,Alexey75,1483610823,,0,1
123,2017-1-5,2017,1,5,19,5m5ocf,What are the best object detection/localization techniques in a scene via learning?,https://www.reddit.com/r/MachineLearning/comments/5m5ocf/what_are_the_best_object_detectionlocalization/,Maha_,1483611969,[removed],0,1
124,2017-1-5,2017,1,5,19,5m5p88,The Instant Rise of Machine Intelligence?,https://www.reddit.com/r/MachineLearning/comments/5m5p88/the_instant_rise_of_machine_intelligence/,jensgk,1483612411,,0,1
125,2017-1-5,2017,1,5,19,5m5qj2,TensorFlow implementation of Simulated+Unsupervised (S+U) learning from Apple,https://www.reddit.com/r/MachineLearning/comments/5m5qj2/tensorflow_implementation_of/,[deleted],1483613023,[deleted],0,1
126,2017-1-5,2017,1,5,19,5m5r1v,[P] Apple's Simulated+Unsupervised (S+U) learning in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5m5r1v/p_apples_simulatedunsupervised_su_learning_in/,[deleted],1483613279,[deleted],0,1
127,2017-1-5,2017,1,5,19,5m5rx8,[R] [1606.07736] Issues in evaluating semantic spaces using word analogies,https://www.reddit.com/r/MachineLearning/comments/5m5rx8/r_160607736_issues_in_evaluating_semantic_spaces/,pmigdal,1483613707,,5,19
128,2017-1-5,2017,1,5,20,5m5wou,"Neural network questions (since there hasn't been a simple question thread in months, sorry)",https://www.reddit.com/r/MachineLearning/comments/5m5wou/neural_network_questions_since_there_hasnt_been_a/,Optometrist__Prime,1483616003,[removed],0,1
129,2017-1-5,2017,1,5,20,5m5zjt,Demystifying Neural Style Transfer,https://www.reddit.com/r/MachineLearning/comments/5m5zjt/demystifying_neural_style_transfer/,[deleted],1483617355,[removed],0,1
130,2017-1-5,2017,1,5,21,5m61d6,[R] Demystifying Neural Style Transfer,https://www.reddit.com/r/MachineLearning/comments/5m61d6/r_demystifying_neural_style_transfer/,lyttonhao,1483618110,"Author: Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou

Abstract: Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.

[PDF Link](https://arxiv.org/abs/1701.01036)

Our work explains the mystical Gram matrix in neural style transfer! Comments are welcomed.",15,71
131,2017-1-5,2017,1,5,21,5m64fa,On Interpretable Models,https://www.reddit.com/r/MachineLearning/comments/5m64fa/on_interpretable_models/,sachinrjoglekar,1483619423,,0,1
132,2017-1-5,2017,1,5,22,5m6hxh,[R] [1511.07409] Top-Down Learning for Structured Labeling with Convolutional Pseudoprior,https://www.reddit.com/r/MachineLearning/comments/5m6hxh/r_151107409_topdown_learning_for_structured/,NicolasGuacamole,1483624567,,1,9
133,2017-1-5,2017,1,5,23,5m6k6a,[P]Participation in the Numerai competition with a Wide and Deep Network,https://www.reddit.com/r/MachineLearning/comments/5m6k6a/pparticipation_in_the_numerai_competition_with_a/,[deleted],1483625290,[deleted],3,4
134,2017-1-6,2017,1,6,0,5m6xzl,Will an MS in Data Science help me in getting a job in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/5m6xzl/will_an_ms_in_data_science_help_me_in_getting_a/,piykat,1483629733,[removed],0,1
135,2017-1-6,2017,1,6,0,5m756v,"Modeling Contagion Through Social Networks to Explain and Predict Gunshot Violence in Chicago, 2006 to 2014",https://www.reddit.com/r/MachineLearning/comments/5m756v/modeling_contagion_through_social_networks_to/,[deleted],1483631792,[deleted],0,1
136,2017-1-6,2017,1,6,1,5m7e0k,For those of you interested in reinforcement/reinforcement deep learning/ openAI gym - join this slack channel,https://www.reddit.com/r/MachineLearning/comments/5m7e0k/for_those_of_you_interested_in/,FreshZuko,1483634239,[removed],0,1
137,2017-1-6,2017,1,6,1,5m7exf,An absolute beginner's guide to Machine Learning and Image Classification with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5m7exf/an_absolute_beginners_guide_to_machine_learning/,-elektro-pionir-,1483634507,,0,1
138,2017-1-6,2017,1,6,1,5m7i23,[P] Apple's Simulated+Unsupervised (S+U) learning in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5m7i23/p_apples_simulatedunsupervised_su_learning_in/,carpedm20,1483635373,,2,34
139,2017-1-6,2017,1,6,2,5m7ner,Scalable scraper to fetch data from the web (e.g. all TechCrunch article names in seconds),https://www.reddit.com/r/MachineLearning/comments/5m7ner/scalable_scraper_to_fetch_data_from_the_web_eg/,Nimsical,1483636823,,0,1
140,2017-1-6,2017,1,6,2,5m7otv,Unsupervised learning with Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5m7otv/unsupervised_learning_with_tensorflow/,Rhenesys,1483637197,[removed],0,1
141,2017-1-6,2017,1,6,2,5m7tjf,[1701.00939] Dense Associative Memory is Robust to Adversarial Inputs,https://www.reddit.com/r/MachineLearning/comments/5m7tjf/170100939_dense_associative_memory_is_robust_to/,alexmlamb,1483638455,,5,29
142,2017-1-6,2017,1,6,2,5m7v77,Object Detection (Robotics),https://www.reddit.com/r/MachineLearning/comments/5m7v77/object_detection_robotics/,__init_self__,1483638886,[removed],0,1
143,2017-1-6,2017,1,6,3,5m7zsa,[P] TensorKart: self-driving MarioKart with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5m7zsa/p_tensorkart_selfdriving_mariokart_with_tensorflow/,funkmagnet,1483640081,,21,281
144,2017-1-6,2017,1,6,3,5m83zo,Combining Computer Vision and NLP for improved product classification,https://www.reddit.com/r/MachineLearning/comments/5m83zo/combining_computer_vision_and_nlp_for_improved/,mwakanosya,1483641183,,0,2
145,2017-1-6,2017,1,6,4,5m8cw8,ML platforms for financial markets,https://www.reddit.com/r/MachineLearning/comments/5m8cw8/ml_platforms_for_financial_markets/,Gus_Bodeen,1483643468,[removed],0,1
146,2017-1-6,2017,1,6,4,5m8gf2,[R] Contagion Through Social Networks to Explain and Predict Gunshot Violence,https://www.reddit.com/r/MachineLearning/comments/5m8gf2/r_contagion_through_social_networks_to_explain/,dunomaybe,1483644393,,1,15
147,2017-1-6,2017,1,6,5,5m8vne,Training a neural net in JavaScript,https://www.reddit.com/r/MachineLearning/comments/5m8vne/training_a_neural_net_in_javascript/,[deleted],1483648472,[deleted],0,1
148,2017-1-6,2017,1,6,6,5m9awf,Theano vs Tensorflow for research (as opposed to application),https://www.reddit.com/r/MachineLearning/comments/5m9awf/theano_vs_tensorflow_for_research_as_opposed_to/,nikr20,1483652616,[removed],0,1
149,2017-1-6,2017,1,6,6,5m9cfg,[P] Grad admission requiring me a year long project.,https://www.reddit.com/r/MachineLearning/comments/5m9cfg/p_grad_admission_requiring_me_a_year_long_project/,[deleted],1483653023,[deleted],3,2
150,2017-1-6,2017,1,6,7,5m9qy2,Approximate Matrix Multiplication in Backprop?,https://www.reddit.com/r/MachineLearning/comments/5m9qy2/approximate_matrix_multiplication_in_backprop/,jpsety,1483656942,[removed],0,1
151,2017-1-6,2017,1,6,8,5ma0r6,"[R] ""Learning to Remember Rare Events"", Kaiser et al 2016",https://www.reddit.com/r/MachineLearning/comments/5ma0r6/r_learning_to_remember_rare_events_kaiser_et_al/,gwern,1483659739,,10,15
152,2017-1-6,2017,1,6,10,5maipm,CS224n: Natural Language Processing with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5maipm/cs224n_natural_language_processing_with_deep/,[deleted],1483665228,[deleted],0,1
153,2017-1-6,2017,1,6,12,5mbb1c,How can I use machine learning at my job?,https://www.reddit.com/r/MachineLearning/comments/5mbb1c/how_can_i_use_machine_learning_at_my_job/,singledoubt2,1483674215,[removed],0,1
154,2017-1-6,2017,1,6,13,5mbm1h,Fulfill Your Lubrication Needs With Bijur Delimon Lubrication Products,https://www.reddit.com/r/MachineLearning/comments/5mbm1h/fulfill_your_lubrication_needs_with_bijur_delimon/,jackerfrinandis,1483678009,,0,1
155,2017-1-6,2017,1,6,14,5mbq7n,Why Do You Need an Alemite Grease Pump?,https://www.reddit.com/r/MachineLearning/comments/5mbq7n/why_do_you_need_an_alemite_grease_pump/,jackerfrinandis,1483679493,,0,1
156,2017-1-6,2017,1,6,15,5mbzoe,Classifying multi dimensional data using decision trees,https://www.reddit.com/r/MachineLearning/comments/5mbzoe/classifying_multi_dimensional_data_using_decision/,[deleted],1483682963,[removed],0,1
157,2017-1-6,2017,1,6,15,5mbzu3,What CNN and RNN beginners python projects I can run with intel i3 and 2 GB memory?,https://www.reddit.com/r/MachineLearning/comments/5mbzu3/what_cnn_and_rnn_beginners_python_projects_i_can/,[deleted],1483683021,[removed],0,1
158,2017-1-6,2017,1,6,15,5mc1pg,[D] What kind of RNN and CNN python projects can a begineer run with i3 processor and 2 GB memory?,https://www.reddit.com/r/MachineLearning/comments/5mc1pg/d_what_kind_of_rnn_and_cnn_python_projects_can_a/,commafighter,1483683760,[removed],14,4
159,2017-1-6,2017,1,6,18,5mcs0x,[D] Why is everything so quiet on the natural language processing front?,https://www.reddit.com/r/MachineLearning/comments/5mcs0x/d_why_is_everything_so_quiet_on_the_natural/,cjmcmurtrie,1483696545,"At NIPS this year, there was a clear focus GANs, image processing, Bayesian ML and reinforcement learning (as well as parties and RocketAI). I picked out a couple of interesting talks on natural language generation, but it seems to me things are generally quiet on that front, and there certainly was not much buzz in 2016 apart from some efforts in translation.

Is this due to blockers, secrecy or both? I'm fascinated that we can now generate photo-realistic content, but not realistic language utterances. Natural language is a discrete space and most people only use ~2000-3000 words to communicate or ~100-200 characters.",70,132
160,2017-1-6,2017,1,6,20,5md0d6,[R] [1611.05431v1] Aggregated Residual Transformations for Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5md0d6/r_161105431v1_aggregated_residual_transformations/,themoosemind,1483700764,,6,8
161,2017-1-6,2017,1,6,21,5md7ud,[D] GPU cloud computing prices,https://www.reddit.com/r/MachineLearning/comments/5md7ud/d_gpu_cloud_computing_prices/,breadwithlice,1483704353,"There are many different service providers for GPU cloud computing (Amazon, Azure, Google Cloud, IBM Cloud, Nimbix, Aliyun, etc..) but their pricing schemes are almost never publicly displayed.

Do you use GPU cloud computing for your machine learning tasks? If so, what GPU do you use and what's the price per hour per GPU?

It seems to me like cloud computing prices don't follow GPU prices. A GTX 1080 is now around 700$ which, assuming a GPU computing price of 0.4$ per GPU per hour, would take about 2 months of computing time to be worth buying. It seems like it's a bit more effort physically hosting the machine but is worth it for deep learning applications. Thoughts?",27,12
162,2017-1-6,2017,1,6,22,5mdhh9,[R] Neural Semantic Encoders (updated),https://www.reddit.com/r/MachineLearning/comments/5mdhh9/r_neural_semantic_encoders_updated/,tsendsuren,1483708272,,1,6
163,2017-1-6,2017,1,6,22,5mdmwh,[P] Implementation of Recursive Neural networks for parsing Natural scenes and language,https://www.reddit.com/r/MachineLearning/comments/5mdmwh/p_implementation_of_recursive_neural_networks_for/,RobRomijnders,1483710350,"Where can I find an implementation of Socher's [Parsing Natural Scenes and Natural Language with Recursive Neural Networks](http://www.socher.org/index.php/Main/ParsingNaturalScenesAndNaturalLanguageWithRecursiveNeuralNetworks) in Python or by using Caffe/Theano/Torch/Tensorflow?

Similar implementations:

  * [Socher's own Matlab implementation](http://www-nlp.stanford.edu/~socherr/icml2011Data/codeSocherICML2011.zip)
  * Repo's that implement [Improved Semantic Representations .. by Tai, Socher, Manning](http://www.aclweb.org/anthology/P15-1150) *Experiments in this paper assume known Trees, which doesn't apply to  parsing Natural Scenes and Natural language*
    * [In Theano](https://github.com/ofirnachum/tree_rnn)
    * [In Torch](https://github.com/stanfordnlp/treelstm)
    * [Suspicions that a Tensorflow implementation will be released soon](https://openreview.net/pdf?id=ryrGawqex)
  * [SPINN by Stanford NLP](https://github.com/stanfordnlp/spinn)


If not, would anyone want to collaborate in making a new implementation?",0,2
164,2017-1-7,2017,1,7,0,5me721,[P] Multilayer Neural Net using numpy.,https://www.reddit.com/r/MachineLearning/comments/5me721/p_multilayer_neural_net_using_numpy/,belsnickel4ever,1483717107,,5,0
165,2017-1-7,2017,1,7,0,5me9xs,[R] [1608.06993v3] Densely Connected Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/5me9xs/r_160806993v3_densely_connected_convolutional/,[deleted],1483717967,[deleted],1,1
166,2017-1-7,2017,1,7,2,5meque,10 Offbeat Predictions for Machine Learning in 2017,https://www.reddit.com/r/MachineLearning/comments/5meque/10_offbeat_predictions_for_machine_learning_in/,edworldreddit,1483722683,,0,1
167,2017-1-7,2017,1,7,2,5mera6,Hyperparameter optimization for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5mera6/hyperparameter_optimization_for_neural_networks/,yurii-shevchuk,1483722810,,0,1
168,2017-1-7,2017,1,7,2,5meyhl,Why go long on artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/5meyhl/why_go_long_on_artificial_intelligence/,nb410,1483724777,,0,1
169,2017-1-7,2017,1,7,3,5mf5we,Anyone have opinions on the Georgia Tech OMSCS?,https://www.reddit.com/r/MachineLearning/comments/5mf5we/anyone_have_opinions_on_the_georgia_tech_omscs/,[deleted],1483726786,[removed],0,1
170,2017-1-7,2017,1,7,3,5mfdsf,Filter dimensions in CNN,https://www.reddit.com/r/MachineLearning/comments/5mfdsf/filter_dimensions_in_cnn/,mac_i,1483728857,[removed],0,1
171,2017-1-7,2017,1,7,4,5mfjq0,[P] king - man + woman is queen; but why?,https://www.reddit.com/r/MachineLearning/comments/5mfjq0/p_king_man_woman_is_queen_but_why/,pmigdal,1483730421,,37,81
172,2017-1-7,2017,1,7,4,5mfojl,End-to-end Optimized Image Compression,https://www.reddit.com/r/MachineLearning/comments/5mfojl/endtoend_optimized_image_compression/,[deleted],1483731720,[deleted],0,1
173,2017-1-7,2017,1,7,4,5mfqib,[D] Why go long on artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/5mfqib/d_why_go_long_on_artificial_intelligence/,ahousley,1483732244,,0,1
174,2017-1-7,2017,1,7,4,5mfri5,How to Learn to Program at the Level Required for CL/NLP/ML Courses?,https://www.reddit.com/r/MachineLearning/comments/5mfri5/how_to_learn_to_program_at_the_level_required_for/,Liontshikl,1483732544,[removed],0,1
175,2017-1-7,2017,1,7,5,5mg3bu,"[N] Topic models: Past, present, and future",https://www.reddit.com/r/MachineLearning/comments/5mg3bu/n_topic_models_past_present_and_future/,Sebastian-JF,1483735836,,1,23
176,2017-1-7,2017,1,7,6,5mgd3m,"What is Sequential MNIST, Permuted MNIST?",https://www.reddit.com/r/MachineLearning/comments/5mgd3m/what_is_sequential_mnist_permuted_mnist/,[deleted],1483738459,[removed],0,1
177,2017-1-7,2017,1,7,7,5mgkty,Can I compare how the left and right brain works with that of a GAN?,https://www.reddit.com/r/MachineLearning/comments/5mgkty/can_i_compare_how_the_left_and_right_brain_works/,kkawabat,1483740687,[removed],0,1
178,2017-1-7,2017,1,7,7,5mgnzd,"[N] Book review: Deep Learning by Goodfellow, Bengio and Courville",https://www.reddit.com/r/MachineLearning/comments/5mgnzd/n_book_review_deep_learning_by_goodfellow_bengio/,mlmaster17,1483741603,,24,54
179,2017-1-7,2017,1,7,8,5mh0im,Wrote a piece inspired by machine learning. Trying to combine tech themes with my music. Hope you guys like it!,https://www.reddit.com/r/MachineLearning/comments/5mh0im/wrote_a_piece_inspired_by_machine_learning_trying/,TKoComposer,1483745328,,0,1
180,2017-1-7,2017,1,7,9,5mhf3j,[1612.05048v1] Adversarial Message Passing For Graphical Models,https://www.reddit.com/r/MachineLearning/comments/5mhf3j/161205048v1_adversarial_message_passing_for/,alexmlamb,1483749756,,1,1
181,2017-1-7,2017,1,7,10,5mhiua,How to use Q Learning in Video Games Easily,https://www.reddit.com/r/MachineLearning/comments/5mhiua/how_to_use_q_learning_in_video_games_easily/,funtwo2,1483750950,,0,1
182,2017-1-7,2017,1,7,10,5mhpc6,"I made a GUI for Jupyter Notebook, let me know what you think!",https://www.reddit.com/r/MachineLearning/comments/5mhpc6/i_made_a_gui_for_jupyter_notebook_let_me_know/,drummer_ash,1483753091,[removed],0,1
183,2017-1-7,2017,1,7,11,5mhttu,Opinions on sparse distributed representations (SDR) as proposed by Jeff Hawkins of Numenta.,https://www.reddit.com/r/MachineLearning/comments/5mhttu/opinions_on_sparse_distributed_representations/,kookaburro,1483754531,[removed],0,1
184,2017-1-7,2017,1,7,11,5mhw3u,question about gradient descent,https://www.reddit.com/r/MachineLearning/comments/5mhw3u/question_about_gradient_descent/,johnnjoeballs,1483755256,[removed],0,1
185,2017-1-7,2017,1,7,12,5mi8rh,10 minutes with pandas library,https://www.reddit.com/r/MachineLearning/comments/5mi8rh/10_minutes_with_pandas_library/,datagenx,1483759553,,0,1
186,2017-1-7,2017,1,7,12,5mie2z,multi layer perceptron question,https://www.reddit.com/r/MachineLearning/comments/5mie2z/multi_layer_perceptron_question/,johnnjoeballs,1483761442,[removed],0,1
187,2017-1-7,2017,1,7,13,5millz,[P] General neural network,https://www.reddit.com/r/MachineLearning/comments/5millz/p_general_neural_network/,SS_NN,1483764094,,7,0
188,2017-1-7,2017,1,7,14,5miu2x,Multi-agent Q learning?,https://www.reddit.com/r/MachineLearning/comments/5miu2x/multiagent_q_learning/,age20d,1483767121,[removed],0,1
189,2017-1-7,2017,1,7,16,5mj8wn,My-Learning-Path-for-Machine-Learning with Python,https://www.reddit.com/r/MachineLearning/comments/5mj8wn/mylearningpathformachinelearning_with_python/,datagenx,1483773067,[removed],0,1
190,2017-1-7,2017,1,7,17,5mjj4o,Learn to create a best-fit line slope with Machine Learning using Python,https://www.reddit.com/r/MachineLearning/comments/5mjj4o/learn_to_create_a_bestfit_line_slope_with_machine/,dexlabanalytics,1483777570,,1,1
191,2017-1-7,2017,1,7,19,5mjylk,[P] Evolutionary Algorithms: Introduction,https://www.reddit.com/r/MachineLearning/comments/5mjylk/p_evolutionary_algorithms_introduction/,shahinrostami,1483786181,,20,83
192,2017-1-7,2017,1,7,20,5mk3me,[R]SalGAN: Visual Saliency Prediction with Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5mk3me/rsalgan_visual_saliency_prediction_with/,juntingpan,1483788959,,0,22
193,2017-1-7,2017,1,7,20,5mk54s,"Life | A thought experiment. Evolution, AI, Synthetic Life.",https://www.reddit.com/r/MachineLearning/comments/5mk54s/life_a_thought_experiment_evolution_ai_synthetic/,nawazdhandala,1483789832,,0,1
194,2017-1-7,2017,1,7,21,5mk6w7,11 of The best Machine learning Books,https://www.reddit.com/r/MachineLearning/comments/5mk6w7/11_of_the_best_machine_learning_books/,[deleted],1483790771,[deleted],0,1
195,2017-1-7,2017,1,7,21,5mk8sm,[P]: hsr - Hand signals recognition using Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/5mk8sm/p_hsr_hand_signals_recognition_using/,peeyek,1483791773,,5,23
196,2017-1-8,2017,1,8,0,5ml0i7,"[D] Gear for local build : fast core, multicore, GPU ?",https://www.reddit.com/r/MachineLearning/comments/5ml0i7/d_gear_for_local_build_fast_core_multicore_gpu/,Jubijub,1483803656,[removed],8,2
197,2017-1-8,2017,1,8,0,5ml0qz,Intersting Machine Learning linkedin Group,https://www.reddit.com/r/MachineLearning/comments/5ml0qz/intersting_machine_learning_linkedin_group/,quantguy1,1483803748,[removed],0,1
198,2017-1-8,2017,1,8,1,5ml837,"Machine Learning Trading, Stock Market, and Chaos",https://www.reddit.com/r/MachineLearning/comments/5ml837/machine_learning_trading_stock_market_and_chaos/,Markjack99,1483806213,,0,1
199,2017-1-8,2017,1,8,1,5mlcik,CS 20SI: Tensorflow for Deep Learning Research,https://www.reddit.com/r/MachineLearning/comments/5mlcik/cs_20si_tensorflow_for_deep_learning_research/,[deleted],1483807671,[deleted],0,1
200,2017-1-8,2017,1,8,1,5mlczq,What is the best result anyone has achieved on the Aurora4 multi-condition dataset?,https://www.reddit.com/r/MachineLearning/comments/5mlczq/what_is_the_best_result_anyone_has_achieved_on/,Nimitz14,1483807829,[removed],0,1
201,2017-1-8,2017,1,8,1,5mld97,Can anyone explain Xavier initialization for neural networks to a beginner?,https://www.reddit.com/r/MachineLearning/comments/5mld97/can_anyone_explain_xavier_initialization_for/,[deleted],1483807912,[removed],0,1
202,2017-1-8,2017,1,8,4,5mm69z,Handwritting Recognition moving from character level to word level.,https://www.reddit.com/r/MachineLearning/comments/5mm69z/handwritting_recognition_moving_from_character/,BorgesML,1483816615,[removed],0,1
203,2017-1-8,2017,1,8,4,5mm6fy,[R] Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5mm6fy/r_generating_focussed_molecule_libraries_for_drug/,antiprior,1483816662,,3,11
204,2017-1-8,2017,1,8,4,5mm6k7,NVIDIA Drive PX2 self-driving car platform visualized,https://www.reddit.com/r/MachineLearning/comments/5mm6k7/nvidia_drive_px2_selfdriving_car_platform/,[deleted],1483816694,[deleted],0,1
205,2017-1-8,2017,1,8,4,5mm70b,[D] What is the best result anyone has achieved on the Aurora4 multi-condition dataset (ASR)?,https://www.reddit.com/r/MachineLearning/comments/5mm70b/d_what_is_the_best_result_anyone_has_achieved_on/,Nimitz14,1483816823,[removed],0,0
206,2017-1-8,2017,1,8,4,5mm8sr,[D] Anybody interested in an online reading group?  /r/deeplearning,https://www.reddit.com/r/MachineLearning/comments/5mm8sr/d_anybody_interested_in_an_online_reading_group/,jalFaizy,1483817343,,48,162
207,2017-1-8,2017,1,8,6,5mms4m,Using TensorBoard without Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5mms4m/using_tensorboard_without_tensorflow/,piiswrong,1483823087,,0,1
208,2017-1-8,2017,1,8,6,5mn0nk,Learn Data Science and Machine Learning in 2017,https://www.reddit.com/r/MachineLearning/comments/5mn0nk/learn_data_science_and_machine_learning_in_2017/,gdangelo,1483825550,,0,1
209,2017-1-8,2017,1,8,11,5mogqo,"For neural networks, why does the R square seem to not match the MSE?",https://www.reddit.com/r/MachineLearning/comments/5mogqo/for_neural_networks_why_does_the_r_square_seem_to/,[deleted],1483842065,[removed],0,1
210,2017-1-8,2017,1,8,11,5mok9m,I want to understand the use of Gaussian Mixture Models in Hidden Markov Models.,https://www.reddit.com/r/MachineLearning/comments/5mok9m/i_want_to_understand_the_use_of_gaussian_mixture/,suhasp777,1483843317,[removed],0,1
211,2017-1-8,2017,1,8,12,5moo85,[D] What are some tricks to get distributed asynchronous training to work?,https://www.reddit.com/r/MachineLearning/comments/5moo85/d_what_are_some_tricks_to_get_distributed/,changoplatanero,1483844660,"I've been experimenting with distributed asynchronous training using tensorflow. Unless I use large batches and tiny learning rates, then the training doesn't converge. When I do get it to converge it is not worth the trouble because I can get a better loss faster using a single worker. Does anyone know some tips for how to get distributed training to work? Does it usually require a smaller learning rate than the non-distributed case?

My setup is one machine with 40 CPUs. I've been launching two or three workers and one parameter server all on the same machine.",8,10
212,2017-1-8,2017,1,8,13,5mp2dn,I know Java and Python. Any free online resources to help me learn data science?,https://www.reddit.com/r/MachineLearning/comments/5mp2dn/i_know_java_and_python_any_free_online_resources/,[deleted],1483849811,[removed],0,1
213,2017-1-8,2017,1,8,18,5mq5as,11 of the best machine learning books,https://www.reddit.com/r/MachineLearning/comments/5mq5as/11_of_the_best_machine_learning_books/,thexwts,1483868687,,0,1
214,2017-1-8,2017,1,8,20,5mqivb,"[P] PyFlux: time series library for Python, includes Bayesian estimates with PyMC3",https://www.reddit.com/r/MachineLearning/comments/5mqivb/p_pyflux_time_series_library_for_python_includes/,pmigdal,1483876409,,10,78
215,2017-1-8,2017,1,8,21,5mqoo0,"[D] Is there a parallel to record linkage/entity resolution where ML can be applied to the records themselves for ""schema matching"" as such?",https://www.reddit.com/r/MachineLearning/comments/5mqoo0/d_is_there_a_parallel_to_record_linkageentity/,prekrish,1483879440,"In the context of collecting disparate data sets holding similar information, are their examples of algorithms being able to resolve attributes of records being similar (while their values are different) based on their relation to other attributes? (So fuzzy matching would not necessarily apply)

Simplistic example: 2 Datasets regarding vehicle test data (perhaps with headings in different languages)

 - Source A: Metric information: Car, Distance tested (km), Fuel Used (l), Consumption (km/l)

 - Source B: Imperial information: Car, Distance tested (miles), Fuel Used (gallons), Consumption (mpg)

Goal:

 * Mapping between A &amp; B, which recognises that the consumption data in different units, is actually the same because they are simply ""column 2 / column 3"" ?

 * Could it identify that the distance columns were holding similar information with a 1.6x scaling factor applied etc?

 * in this hypothetical example, matching could apply on column 1 to identify cars for record linkage, however I'm curious to see if there are methods which can identify the relationships between attributes, and recognise where attributes are similarly linked in other sets.

[Similar reading i have seen](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.9222&amp;rep=rep1&amp;type=pdf)

Thanks for your input! (hope this discussion isn't too simple/lacks technical detail for this subreddit))",4,5
216,2017-1-8,2017,1,8,23,5mr5of,Decomposing non-deterministic processes into individual deterministic processes,https://www.reddit.com/r/MachineLearning/comments/5mr5of/decomposing_nondeterministic_processes_into/,wehnsdaefflae,1483887121,,0,1
217,2017-1-9,2017,1,9,3,5ms72j,Few questions about tensorflow,https://www.reddit.com/r/MachineLearning/comments/5ms72j/few_questions_about_tensorflow/,invoker430,1483899327,[removed],0,1
218,2017-1-9,2017,1,9,3,5ms7g6,[D] What are natural (computed) pre-images useful for?,https://www.reddit.com/r/MachineLearning/comments/5ms7g6/d_what_are_natural_computed_preimages_useful_for/,themoosemind,1483899434,,0,1
219,2017-1-9,2017,1,9,5,5mt59v,Deep Text Correcter,https://www.reddit.com/r/MachineLearning/comments/5mt59v/deep_text_correcter/,based2,1483909025,,1,3
220,2017-1-9,2017,1,9,6,5mt9bj,[D] Anyone done any work on Doom AI? (vizdoom),https://www.reddit.com/r/MachineLearning/comments/5mt9bj/d_anyone_done_any_work_on_doom_ai_vizdoom/,Tostino,1483910158,"Hey everyone, was just wondering if anyone has worked on making a Doom AI with a reinforcement learning NN?  I've been playing with it the past few days, but having limited success with training it at the moment. If anyone has messed with it with any sort of success, i'd love to know.

I've got only a bit of exposure to NN's in the past, mainly messing with image classification, but thought it'd be fun to try and learn more by building a game AI.

Right now i'm using DL4J/RL4J, because I know Java well, and only have limited exposure to Python.

Feel free to ask any questions if you're interested in messing with this as well, i'd love to have someone else interested to bounce ideas off.",11,18
221,2017-1-9,2017,1,9,7,5mtqbq,[D] Looking for a Better Explanation for some RNN Parts,https://www.reddit.com/r/MachineLearning/comments/5mtqbq/d_looking_for_a_better_explanation_for_some_rnn/,Xanthus730,1483915024,"I recently completed a project in which I made a simple ML framework for use with CNN for playing Go on Mobile. I'm looking to expand on the framework I made with the ability to create LSTM RNNs, but I'm having trouble fully understanding BPTT.

I created a couple diagrams to help with this post:
http://imgur.com/a/Jms6B In the first image, this is the standard RNN unrolled BPTT image I usually see.

In case of backpropagation of error from Output t3, after getting to Mem Cell 2 t3, is error propagated to Mem Cell 2 t2 and Mem Cell 1 t3, or just Mem Cell 1 t3?
If error does propagate from Mem Cell 2 t3 to Mem Cell 2 T2, does is continue back to M.Cell 2 t1, and/or to Mem Cell 1 t2?
If error propagates left and down from each cell, each step, this would mean that error(s) propagate to Mem Cell 1 t1 five times if we do error backprop for each timestep. Is this correct?

If we use basic LSTM memory cells as in image 2, then when (if) we propagate error to the left, are we propagating error on both the Cell State and Input/Ouput (H) lines, or just one or the other?",6,13
222,2017-1-9,2017,1,9,9,5mue6q,Books on Machine Learning with C/C++?,https://www.reddit.com/r/MachineLearning/comments/5mue6q/books_on_machine_learning_with_cc/,TheoryEternity,1483921952,[removed],0,1
223,2017-1-9,2017,1,9,9,5mugco,[R] ANN for short-term price prediction,https://www.reddit.com/r/MachineLearning/comments/5mugco/r_ann_for_shortterm_price_prediction/,Ozzah,1483922609,"Without saying too much of the specifics, for a given time `t`, I have historical and forecast prices for a commodity, and since `t` is in the past, I also have *actual prices* going forward. I also have a whole bunch of other data on related things that are believed to influence the price of the commodity.

I want to use historical and forecast prices, together with other data, to give my own forecast on the prices.

I have a feed-forward activation network with a bipolar sigmoid activation function. Currently there is one input neuron for each data point, which is normalised to `[-1,1]`. I have 2 hidden layers with 2*input and 2*output nodes respectively. The output layer contains one node for each time point in the short term future I'm trying to predict.

In other words, if `I` and `O` are the number of input and output nodes, then the layers go: `[I, 2I, 2O, O]`. The values of the outputs are also normalised to `[-1,1]`, and the mapping is based on market regulated lower- and upper-bounds on the commodity price. Realistic values are `I = 373` and `O = 36`.

So basically for a given time `t`, I want to take what has happened in recent history together with a price forecast to produce a more accurate forecast.

I initially ran a long-term back propagation training using a very large training set and the results weren't terrible, but were still not as good as the existing forecast. Since the existing forecast is reasonably accurate in most cases, it would have been better for the network to just spit out that value.

I was concerned it had become caught in a local minimum, so I put together a genetic algorithm to optimise the network weights and ran it on the same data and ran it for a couple of weeks. After a while new solutions were few and far between, and with very little improvement, so I deemed it converged and tested the best network in the population. Again the results were underwhelming, and worse than the existing forecast we already had.

At this point I'm not really sure what to do. I know the network structure can have a big impact on the network performance, but I'm not sure how to determine what the optimal structure should be. I'm tempted to do a nested GA, where the first level explores network structure and the second level explore network coefficients given a structure. Obviously this would be quite a lot more computationally intensive, so I would only be able to run it on a significantly restricted training set.

But other than that, I'm stumped.

Basically, I just want to produce a better price forecast than what we already have available. It doesn't have to be perfect, just better. We don't generate this existing forecast, it's generated by a regulator, and the market behaviour is also partially responsive to the forecast itself, so there is a bit of a feedback effect. I'm working on the assumption that the relationship between the input data and the output data is quite complex and we don't know how to model it analytically.

Any other ideas?",35,2
224,2017-1-9,2017,1,9,10,5munjy,"[P] Tensorflow implementation of ""The Predictron: End-To-End Learning and Planning""",https://www.reddit.com/r/MachineLearning/comments/5munjy/p_tensorflow_implementation_of_the_predictron/,zhongwenxu,1483924735,,15,71
225,2017-1-9,2017,1,9,10,5murb3,Free O'Reilly Ebooks (xpost from /r/learnprogramming),https://www.reddit.com/r/MachineLearning/comments/5murb3/free_oreilly_ebooks_xpost_from_rlearnprogramming/,Indianbro,1483925865,,0,1
226,2017-1-9,2017,1,9,10,5mutc3,Can someone explain the process of backwards propagation in an ANN?,https://www.reddit.com/r/MachineLearning/comments/5mutc3/can_someone_explain_the_process_of_backwards/,kbrshh,1483926474,[removed],0,1
227,2017-1-9,2017,1,9,12,5mvhm8,Deeper experiences...,https://www.reddit.com/r/MachineLearning/comments/5mvhm8/deeper_experiences/,crazyassmofuka,1483934338,[removed],0,1
228,2017-1-9,2017,1,9,15,5mw3rp,Neural Chinese Transliteratorcan you do better than SwiftKey Keyboard?,https://www.reddit.com/r/MachineLearning/comments/5mw3rp/neural_chinese_transliteratorcan_you_do_better/,[deleted],1483942541,[deleted],0,1
229,2017-1-9,2017,1,9,15,5mw69v,Stanford Unsupervised Deep Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/5mw69v/stanford_unsupervised_deep_learning_tutorial/,shagunsodhani,1483943570,,0,1
230,2017-1-9,2017,1,9,22,5mxluv,Intro to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5mxluv/intro_to_machine_learning/,teamclairvoyant,1483968039,,0,1
231,2017-1-9,2017,1,9,22,5mxmzt,[1701.01724] DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker. Beats pros.,https://www.reddit.com/r/MachineLearning/comments/5mxmzt/170101724_deepstack_expertlevel_artificial/,alito,1483968476,,48,138
232,2017-1-10,2017,1,10,0,5mydx8,"[Project] [C] Cranium  - A portable, header-only, artificial neural network library",https://www.reddit.com/r/MachineLearning/comments/5mydx8/project_c_cranium_a_portable_headeronly/,igetthedripfromywalk,1483977156,,13,3
233,2017-1-10,2017,1,10,0,5mydyy,[D] Resolutions (on what to learn) in 2017?,https://www.reddit.com/r/MachineLearning/comments/5mydyy/d_resolutions_on_what_to_learn_in_2017/,harmonium1,1483977169,"Given how popular ML has become, and how various subfields have been expanded (now it's practically impossible to keep up with new arXiv submissions), we all have some area of ML with which we wish we were better acquainted (for me, it's deep reinforcement learning).

So what new skill (or understanding of an area) do you plan to pick up in 2017?  ",32,15
234,2017-1-10,2017,1,10,1,5mykkh,[N] Call for Papers: 1st Conference on Robot Learning,https://www.reddit.com/r/MachineLearning/comments/5mykkh/n_call_for_papers_1st_conference_on_robot_learning/,vincentvanhoucke,1483978957,,5,49
235,2017-1-10,2017,1,10,1,5mymfw,New Keras Subreddit,https://www.reddit.com/r/MachineLearning/comments/5mymfw/new_keras_subreddit/,emtonsti,1483979472,,1,1
236,2017-1-10,2017,1,10,2,5myuil,[D] Blockchains for Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/5myuil/d_blockchains_for_artificial_intelligence/,[deleted],1483981654,[deleted],0,2
237,2017-1-10,2017,1,10,3,5mz6uq,Welcome to my new Technology/Machine Learning Blog,https://www.reddit.com/r/MachineLearning/comments/5mz6uq/welcome_to_my_new_technologymachine_learning_blog/,lolekxful,1483984974,[removed],0,1
238,2017-1-10,2017,1,10,4,5mzmig,"[D] Help with interpretation of trained networks,",https://www.reddit.com/r/MachineLearning/comments/5mzmig/d_help_with_interpretation_of_trained_networks/,slap_bet,1483989095,"Hi reddit,
During my work I've received critical reviewer comments about the lack of interpretation of my networks. The black-boxy nature of ANNs (especially CNNs) is a pretty common criticism, so I'm sure many of you have encountered the same thing. I was wondering how the readers of this sub normally go demonstrating and interpreting their networks in papers. Do you have a paper that you like to emulate? Have you encountered something that did this well? Perhaps constructing novel input from a trained model, or finding clear examples of meaningful activations within the intermediate layers.",9,6
239,2017-1-10,2017,1,10,4,5mzqxt,Semeval 2017,https://www.reddit.com/r/MachineLearning/comments/5mzqxt/semeval_2017/,kaggle_addict,1483990244,[removed],0,1
240,2017-1-10,2017,1,10,5,5n01i4,"[D] Network in Network (NiN), is it still useful?",https://www.reddit.com/r/MachineLearning/comments/5n01i4/d_network_in_network_nin_is_it_still_useful/,carlthome,1483993030,"Do you use NiN, and if so, for what problems and architectures (RNN, CNN, MLP)? Do you think it's helpful?

[Paper](https://arxiv.org/abs/1312.4400) for those unfamiliar.",6,6
241,2017-1-10,2017,1,10,5,5n02he,Extract clusters from Dirichlet Process in PyMC3,https://www.reddit.com/r/MachineLearning/comments/5n02he/extract_clusters_from_dirichlet_process_in_pymc3/,o-rka,1483993295,,0,1
242,2017-1-10,2017,1,10,8,5n17q9,A hacking challenge based on the MNIST dataset,https://www.reddit.com/r/MachineLearning/comments/5n17q9/a_hacking_challenge_based_on_the_mnist_dataset/,scvalencia,1484004245,,0,1
243,2017-1-10,2017,1,10,8,5n1bmk,What do large weights mean in a neural network?,https://www.reddit.com/r/MachineLearning/comments/5n1bmk/what_do_large_weights_mean_in_a_neural_network/,jstaker7,1484005255,[removed],0,1
244,2017-1-10,2017,1,10,9,5n1kqs,[P] Predicting which politician to tweet at,https://www.reddit.com/r/MachineLearning/comments/5n1kqs/p_predicting_which_politician_to_tweet_at/,_quark,1484007757,"I've been using the [lipad](http://www.lipad.ca) dataset to train a classifier that distinguishes at which member of the Canadian Ministry a user-generated tweet should be directed. It uses transcripts of Parliamentary Debates from 2016 with ~10 000 data points. 

The result is [Cabinet Door](http://www.cabinetdoor.ca). A tool that directs your tweet to the right Minister automatically. Here are some example tweets from #canpoli that are good illustrations of how it works. Just copy+paste into the box at http://www.cabinetdoor.ca:

* How Saskatchewan is Driving Small Wind Producers Out of the Market

* Dilemma for Canada: Follow US by adopting new sanctions, or hunker down and wait for Trump?

* I don't remember the Liberals promising Carbon Taxes during the last elections ???

* Some momentum on #erre in the prov.? Electoral reform commission invites public to join upcoming meetings


Accuracy was ~80% on the validation set. I hoped that the debate transcripts were a good proxy for what other communication with a Minister of Canada would be like. It was implemented with sklearn's SGDClassifier. The data was from 2016's Parliament which has ~22 000 data points that I pared down to ~9500 through intensive data cleaning which also involved reading and manually labeling a number of *fascinating* political debates.

I tried many times to implement document classification using a neural network. I'm not really sure how it's done though, or if it's even possible. I have a trained set of embeddings for my vocabulary but I am unsure how to proceed with this data. I've tried a simple {str:int} dictionary lookup representation of the text into a 1-layer NN. I ended up with 90+% accuracy on the training set while always scoring on par with random guesses on the validation set.

If anyone can recommend good papers on document classification using neural networks it would be much appreciated.",8,46
245,2017-1-10,2017,1,10,9,5n1pzz,[D] How to take the most advantage of machine learning algorithms.,https://www.reddit.com/r/MachineLearning/comments/5n1pzz/d_how_to_take_the_most_advantage_of_machine/,[deleted],1484009266,[deleted],1,0
246,2017-1-10,2017,1,10,10,5n1w8f,A little help with OCR,https://www.reddit.com/r/MachineLearning/comments/5n1w8f/a_little_help_with_ocr/,michaeltheobnoxious,1484011119,[removed],1,1
247,2017-1-10,2017,1,10,10,5n215m,[P] Neural Chinese Transliteratorcan you do better than SwiftKey Keyboard?,https://www.reddit.com/r/MachineLearning/comments/5n215m/p_neural_chinese_transliteratorcan_you_do_better/,longinglove,1484012625,,1,15
248,2017-1-10,2017,1,10,10,5n222r,[1701.02291] QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures,https://www.reddit.com/r/MachineLearning/comments/5n222r/170102291_quicknet_maximizing_efficiency_and/,darkconfidantislife,1484012900,,1,1
249,2017-1-10,2017,1,10,11,5n27l3,Announcing tealtree - new open source gradient boosting decision tree implementation.,https://www.reddit.com/r/MachineLearning/comments/5n27l3/announcing_tealtree_new_open_source_gradient/,logan42g,1484014611,[removed],1,1
250,2017-1-10,2017,1,10,11,5n2con,Why freezing weights of discriminator in GAN?,https://www.reddit.com/r/MachineLearning/comments/5n2con/why_freezing_weights_of_discriminator_in_gan/,[deleted],1484016182,[removed],0,1
251,2017-1-10,2017,1,10,11,5n2eks,Causal Inference using Random Forests,https://www.reddit.com/r/MachineLearning/comments/5n2eks/causal_inference_using_random_forests/,[deleted],1484016753,[deleted],0,1
252,2017-1-10,2017,1,10,11,5n2fih,[D] Why freeze discriminator weights and stack the network in GAN?,https://www.reddit.com/r/MachineLearning/comments/5n2fih/d_why_freeze_discriminator_weights_and_stack_the/,commafighter,1484017054,I am following this tutorial for GAN https://oshearesearch.com/index.php/2016/07/01/mnist-generative-adversarial-model-in-keras/ and here the author freezes the discriminator network for some reason. According to my knowledge freezing a weight means weights of the network cant be further updated. But if a neural network has to learn it should update its weights that includes for both generative and discriminatve network. Then why the author freezes the weights?  and also is stacking of the neural network is necessary in GAN? ,6,4
253,2017-1-10,2017,1,10,12,5n2hm2,[R] Causal Inference through Random Forests,https://www.reddit.com/r/MachineLearning/comments/5n2hm2/r_causal_inference_through_random_forests/,[deleted],1484017716,[deleted],15,26
254,2017-1-10,2017,1,10,13,5n2rx8,Introduction to Automatic Text Summarization,https://www.reddit.com/r/MachineLearning/comments/5n2rx8/introduction_to_automatic_text_summarization/,hariexcel,1484020900,,0,1
255,2017-1-10,2017,1,10,13,5n2si7,[R] [1701.02291] QuickNet: Maximizing Efficiency and Efficacy in Deep Architectures,https://www.reddit.com/r/MachineLearning/comments/5n2si7/r_170102291_quicknet_maximizing_efficiency_and/,darkconfidantislife,1484021093,,16,6
256,2017-1-10,2017,1,10,13,5n2y4q,"[N] Artificial intelligence is the future, and Canada can seize it",https://www.reddit.com/r/MachineLearning/comments/5n2y4q/n_artificial_intelligence_is_the_future_and/,jinjinnjinny,1484022953,,18,18
257,2017-1-10,2017,1,10,15,5n3gbn,"What to do after Andrew Ng's course to learn more about complex learning (video game players, genetic algorithms, etc)",https://www.reddit.com/r/MachineLearning/comments/5n3gbn/what_to_do_after_andrew_ngs_course_to_learn_more/,TrashQuestion,1484029400,[removed],0,1
258,2017-1-10,2017,1,10,16,5n3orf,Nail Making Machine In India,https://www.reddit.com/r/MachineLearning/comments/5n3orf/nail_making_machine_in_india/,nailmakingmachine,1484032934,,0,1
259,2017-1-10,2017,1,10,17,5n3veb,Official implementation for the NIPS 2016 paper k Nearest Neighbors: From Global to Local is now available here.,https://www.reddit.com/r/MachineLearning/comments/5n3veb/official_implementation_for_the_nips_2016_paper_k/,philipperemy,1484036036,,1,1
260,2017-1-10,2017,1,10,18,5n42pe,Deep Learning at GILT,https://www.reddit.com/r/MachineLearning/comments/5n42pe/deep_learning_at_gilt/,sko2sko,1484039743,,0,1
261,2017-1-10,2017,1,10,18,5n4567,Curated gallery of deep learning projects,https://www.reddit.com/r/MachineLearning/comments/5n4567/curated_gallery_of_deep_learning_projects/,sko2sko,1484040986,,0,1
262,2017-1-10,2017,1,10,19,5n49rf,[Project] Curated Generative Adversarial Network (GAN) list,https://www.reddit.com/r/MachineLearning/comments/5n49rf/project_curated_generative_adversarial_network/,kevinzakka,1484043172,,2,74
263,2017-1-10,2017,1,10,21,5n4pdg,[D] What is the configuration of your ML rig?,https://www.reddit.com/r/MachineLearning/comments/5n4pdg/d_what_is_the_configuration_of_your_ml_rig/,AntixK,1484050571,"It would be great to share the configuration of the computers that ML researchers and enthusiasts use for their implementations.

I am aware of the giant multiple distributed GPU networks that research institutions have.  It would be great to share those configurations as well as the ones that independent researchers or startups use. 

[Here is a link](http://graphific.github.io/posts/building-a-deep-learning-dream-machine/) in which this guy has shared his rig config. 

Thanks!! ",64,24
264,2017-1-10,2017,1,10,22,5n4y88,[Discussion] How does mfcc feature size affect recurent neural network?,https://www.reddit.com/r/MachineLearning/comments/5n4y88/discussion_how_does_mfcc_feature_size_affect/,Lukaskar,1484053946,"So I'm learning machine learning and wanted to know how does mfcc feature size affect on RNN (Recurent Neural Network)?

With librosa I extracted mfcc and then delta coefficients and after that I get array of dimension [13, sound_length]

The code of extracting mfcc and delta coefficients with python: (y - sound file data, sr - length of y)

    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

    mfcc_delta = librosa.feature.delta(mfcc, axis=0, order=1)

So theoretically if I want to train network with this kind of data and with data where n_mfcc=39. Who would be better and why? (Ignore all other hyper parameters) I just want to know theoretics of how this parameter affect RNN.
",6,2
265,2017-1-10,2017,1,10,22,5n4zi6,[P] Keras implementation of Neural Semantic Encoders,https://www.reddit.com/r/MachineLearning/comments/5n4zi6/p_keras_implementation_of_neural_semantic_encoders/,aleph__one,1484054381,,0,22
266,2017-1-10,2017,1,10,22,5n53k7,[D] Results from the Best Paper Awards,https://www.reddit.com/r/MachineLearning/comments/5n53k7/d_results_from_the_best_paper_awards/,Mandrathax,1484055858,"Hi guys! Here are the results for /r/MachineLearning's 2016 best paper awards that I tried to put up [here](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/).

You can find the exact point count in [the original thread](https://www.reddit.com/r/MachineLearning/comments/5kxfkb/d_rmachinelearnings_2016_best_paper_award/)

Without further ado, here are the winners, per category.

---

### Best Paper of the year

&gt; No rules! Any research paper you feel had the greatest impact/had top writing, any criterion is good.

Winner : **[Mastering the Game of Go with Deep Neural Networks and Tree Search](http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf)** (warning pdf)

### Best student paper

&gt; Papers from a student, grad/undergrad/highschool, everyone who doesn't have a phd and goes to school. The student must be first author of course. Provide evidence if possible.

Winner : **[Recurrent Batch Normalization](https://arxiv.org/abs/1603.09025)**

### Best paper name

&gt; Try to beat [this](http://www.oneweirdkerneltrick.com/spectral.pdf)

Winner : **[Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)**

### Best paper from academia

&gt; Papers where the first author is from a university / a state research organization (eg INRIA in France). 

Winner : **None**^1

### Best paper from the industry

&gt; Great paper from a multi-billion tech company (or more generally a research lab sponsored by privat funds, eg. openai)

Winner : **[WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)**

### Best rejected paper

&gt; A chance of redemption for good papers that didn't make it trough peer review. Please provide evidence that the paper was rejected if possible.

Winner : **[Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/abs/1608.05343)**

### Best unpublished preprint

&gt; A category for those yet to be published (e.g. papers from the end of the year). This may or may not be redundant with the rejected paper category, we'll see.

Winner : **[Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems](https://arxiv.org/abs/1609.09059)**^2

### Best theoretical paper

&gt; Keep the math coming

Winner : **[Operational calculus on programming spaces and generalized tensor networks](https://arxiv.org/abs/1610.07690)**

### Best non Deep Learning paper

&gt; Because gaussian processes, random forests and kernel methods deserve a chance amid the DL hype train

Winner **[Fast and Provably Good Seedings for k-Means](http://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means)**




^1 : there was no nomination for the academia category which is a bit disappointing in my opinion. Some papers nominated in other categories do fall in this category such as [Lip Reading Sentences in the Wild](https://arxiv.org/abs/1611.05358), [Recurrent Batch Normalization](https://arxiv.org/abs/1603.09025), [Professor Forcing: A New Algorithm for Training Recurrent Networks](https://arxiv.org/abs/1610.09038), [Fast and Provably Good Seedings for k-Means](http://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means), [Toward an Integration of Deep Learning and Neuroscience](http://journal.frontiersin.org/article/10.3389/fncom.2016.00094/full)...

^2 : this category received only one nomination which got only 2 upvotes. I think it might indeed have been redundant with rejected papers.

---

That's it!

Thanks everyone for participating, don't hesitate to give feedback in the comments.

I started this award a bit impulsively so I think it's benefit from better planning next year. The biggest problem this year imho was the small number of nominations so I think this could be improved by somehow anonymising the nomination process and separating it from the votes, etc..

Cheers


EDIT : also thanks A LOT to the mod team for helping by stickying and putting the thread in contest mode :)",28,87
267,2017-1-10,2017,1,10,23,5n5f8c,Applying Deep Learning to Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/5n5f8c/applying_deep_learning_to_natural_language/,adeshpande3,1484059833,,0,1
268,2017-1-11,2017,1,11,1,5n5zhl,[P] Predicting train occupancy of Belgian trains,https://www.reddit.com/r/MachineLearning/comments/5n5zhl/p_predicting_train_occupancy_of_belgian_trains/,givdwiel,1484065901,"iRail (https://irail.be), a mobile application that provides information about train rides in Belgium lately released a new feature in September, SpitsGids. Here, people can specify the occupancy on their train (low, medium or high). Other people that still need to get on that train (in a later station), can then see this occupancy and decide to take a train later or earlier when the occupancy is high.

From September till now, around 3500 occupancy logs were made. Based on these occupancy logs, can we now make a predictive model that accurately predicts the occupancy of a future train?

We released an in-class Kaggle competition, as many hands make light, but especially more efficient, work. Can you beat the benchmark?

https://inclass.kaggle.com/c/train-occupancy-prediction


Here are three blog posts about this problem if you are interested:

- https://medium.com/@gillesvandewiele/predicting-the-occupancy-of-nmbs-trains-using-logs-from-spitsgids-app-v1-0-82b2bcff9bd2#.twrqjqfor
- https://dataminded.be/blog/predicting-occupancy-nmbs-trains
- https://medium.com/@nathan.gs/predicting-occupancy-on-the-belgian-railroads-based-on-spitsgids-irail-data-using-azure-ml-95aa89f22620#.c85rtmn2h

Hope to see you on the competition forums ;)",4,14
269,2017-1-11,2017,1,11,3,5n6jx0,"[P] HyperGAN 0.6.0 ""MultiGAN"" released",https://www.reddit.com/r/MachineLearning/comments/5n6jx0/p_hypergan_060_multigan_released/,what_are_tensors,1484071428,,3,9
270,2017-1-11,2017,1,11,3,5n6ofj,Creating curious machines: Towards general information-seeking agents,https://www.reddit.com/r/MachineLearning/comments/5n6ofj/creating_curious_machines_towards_general/,[deleted],1484072647,[deleted],0,1
271,2017-1-11,2017,1,11,4,5n73fu,"Difference between LogisticRegression(penalty=""l2"") and RidgeClassifier()",https://www.reddit.com/r/MachineLearning/comments/5n73fu/difference_between_logisticregressionpenaltyl2/,incoherentsource,1484076649,[removed],0,1
272,2017-1-11,2017,1,11,4,5n73x1,Convolutional superimposing objects?,https://www.reddit.com/r/MachineLearning/comments/5n73x1/convolutional_superimposing_objects/,Dark_Messiah,1484076776,[removed],0,1
273,2017-1-11,2017,1,11,4,5n77lu,DARPA Goes Meta with Machine Learning for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5n77lu/darpa_goes_meta_with_machine_learning_for_machine/,dabshitty,1484077744,,1,1
274,2017-1-11,2017,1,11,5,5n7naf,Need to talk to a Keras expert about LSTM,https://www.reddit.com/r/MachineLearning/comments/5n7naf/need_to_talk_to_a_keras_expert_about_lstm/,Tsadkiel,1484081984,[removed],0,1
275,2017-1-11,2017,1,11,6,5n7py7,Keras-vis: Toolkit to perform guided backprop for visualizations,https://www.reddit.com/r/MachineLearning/comments/5n7py7/kerasvis_toolkit_to_perform_guided_backprop_for/,raghakot,1484082708,,0,1
276,2017-1-11,2017,1,11,6,5n7ror,Are Markov Logic Networks Truly En Route to a General Purpose Learning Algorithm?,https://www.reddit.com/r/MachineLearning/comments/5n7ror/are_markov_logic_networks_truly_en_route_to_a/,yesman678,1484083183,[removed],0,1
277,2017-1-11,2017,1,11,6,5n7vtf,[R] Creating curious machines: Towards general information-seeking agents,https://www.reddit.com/r/MachineLearning/comments/5n7vtf/r_creating_curious_machines_towards_general/,juharris,1484084303,,5,33
278,2017-1-11,2017,1,11,6,5n7y6d,6.S094: Deep Learning for Self-Driving Cars,https://www.reddit.com/r/MachineLearning/comments/5n7y6d/6s094_deep_learning_for_selfdriving_cars/,Dogsindahouse1,1484084936,,0,1
279,2017-1-11,2017,1,11,6,5n7zpl,[N] TensorFlow 1.0.0-alpha,https://www.reddit.com/r/MachineLearning/comments/5n7zpl/n_tensorflow_100alpha/,whateverr123,1484085344,,53,307
280,2017-1-11,2017,1,11,6,5n7zsg,Selecting a machine learning method for what I think is a topic classification problem.,https://www.reddit.com/r/MachineLearning/comments/5n7zsg/selecting_a_machine_learning_method_for_what_i/,FermiAnyon,1484085366,[removed],0,1
281,2017-1-11,2017,1,11,6,5n80fk,How to Make Mark Zuckerberg's Bot,https://www.reddit.com/r/MachineLearning/comments/5n80fk/how_to_make_mark_zuckerbergs_bot/,stacky777,1484085549,,0,1
282,2017-1-11,2017,1,11,7,5n89ze,[Project] Deep Learning Paper Implementation Series: Spatial Transformer Networks Part I,https://www.reddit.com/r/MachineLearning/comments/5n89ze/project_deep_learning_paper_implementation_series/,kevinzakka,1484088622,,0,20
283,2017-1-11,2017,1,11,8,5n8d4u,Self-driving car Nvidia Autopilot live Keras implementation,https://www.reddit.com/r/MachineLearning/comments/5n8d4u/selfdriving_car_nvidia_autopilot_live_keras/,[deleted],1484089468,[deleted],0,1
284,2017-1-11,2017,1,11,12,5n9owg,"Thoughts on ""Principles and Theory for Data Mining and Machine Learning""?",https://www.reddit.com/r/MachineLearning/comments/5n9owg/thoughts_on_principles_and_theory_for_data_mining/,Slikeer,1484104261,[removed],0,1
285,2017-1-11,2017,1,11,12,5n9r4f,"Beginner here, what is the ideal amount of data needed to train an LSTM Neural Net?",https://www.reddit.com/r/MachineLearning/comments/5n9r4f/beginner_here_what_is_the_ideal_amount_of_data/,madzthakz,1484104988,[removed],0,1
286,2017-1-11,2017,1,11,14,5na9dn,[D] Regarding training GAN discriminators,https://www.reddit.com/r/MachineLearning/comments/5na9dn/d_regarding_training_gan_discriminators/,zergling103,1484111237,"It appears (correct me if I'm wrong) that the conventional wisdom regarding training GANs is that the discriminator should not be too much smarter than the generator during training.

However, it appears as though some of the latest approaches to improve the performance of GANs involve intentionally retarding the discriminator's ability to learn.

For example: https://arxiv.org/pdf/1611.04076v1.pdf 
&gt;""...In terms of GANs learning, the discriminator does not need to perform well.""

&gt;""...The limited classification capability [of the discriminator]...relieves the saturation problem of GANs learning.""

As the paper shows, this works very well. (The results are incredible.) 

That being said, I can't help but think that there is untapped potential that is being stifled. If we were to imagine a perfect discriminator - one that can distinguish a fake image from a real image with 100% accuracy - you'd think one could use it to train a generator that produces images on the cutting edge of indistinguishability. 

Though (I think) I understand the reason why it's not done this way: The closest analogy I've heard is that the generator is trying to create counterfiet money, whereas the discriminator is trying to tell the difference between fake and real money. If the disriminator can perfectly distinguish between real and fake currency, there is literally nothing the generator can do to improve; the only feedback on its performance would be ""you've failed again"" in that the reward is always going to be $0.00 dollars produced.

Perhaps this is where the analogy of an ""adversary"" falls short - maybe it'd be better to think of it in terms of a teacher and a student: Rather than training the teacher solely for the sake of its own performance in accurately spotting fakes, also train the teacher to maximize the rate at which its student would improve.

To put this in more concrete terms: The **""perfect"" discriminator** is one where the gradient is 0 everywhere except at the decision boundary, where the gradient would be undefined. Anything outside the decision boundary would be classified as 100% fake, and anything within it would be 100% real; nothing in between.

However, the **""perfect"" teacher** would be one where it's decision boundary separates real examples from fake ones with 100% accuracy, but where the gradients outside that decision boundary would ""point to"" the closest point inside it. One possible way to do this could be to train it by comparing its classification on a real image and a fake image, and training it to classify the real image at 0 (meaning 100% real) and the fake image at some positive nonzero value proportional to it's distance from the real image in latent space (which should produce a gradient with a constant slope outside the decision boundary). Perhaps there may be a more direct way to train the teacher with the direct goal of producing ideal gradients for generators, given that we already know some of the properties that make discriminators suitable for GAN training.

This would essentially create the effect of the teacher discriminator telling the generator, ""You're not there yet, but here's where you could go with this to improve"".

I think that approaching training GANs from this angle could be more appropriate. Rather than training an **adversarial** discriminator in such a way that intentionally makes it ""dumber"" with its **utility as a teacher being an indirect concequence**, training a discriminator to maximize qualities that make it a good **teacher**, directly, might lead to better and faster results.

Again, feel free to correct me if I'm wrong about anything here. :)

EDIT: One point I'm speculating regarding this paper: https://arxiv.org/pdf/1611.04076v1.pdf 
I'm guessing one aspect of the L2 function that makes it useful for training GANs is that it discourages the generator from producing samples that would normally overshoot the decision boundary by a large margin, extrapolating beyond the training data. I.e., producing samples that are ""unrealistically realistic"", and combats the effect shown in this diagram: http://www.kdnuggets.com/images/sweep_plot.png",6,24
287,2017-1-11,2017,1,11,17,5naz42,[P] Neural Japanese Transliteratorcan you do better than SwiftKey Keyboard?,https://www.reddit.com/r/MachineLearning/comments/5naz42/p_neural_japanese_transliteratorcan_you_do_better/,longinglove,1484121729,,0,0
288,2017-1-11,2017,1,11,17,5nb066,"Have Created a subreddit for cs224n, so that we can study the course together.",https://www.reddit.com/r/MachineLearning/comments/5nb066/have_created_a_subreddit_for_cs224n_so_that_we/,kazi_shezan,1484122192,,0,2
289,2017-1-11,2017,1,11,17,5nb613,Buy Heidelberg Spare Parts at Shop.PrintersParts.com,https://www.reddit.com/r/MachineLearning/comments/5nb613/buy_heidelberg_spare_parts_at_shopprinterspartscom/,printersparts,1484125073,,0,1
290,2017-1-11,2017,1,11,18,5nb7lp,[P] Analysis of Dropout,https://www.reddit.com/r/MachineLearning/comments/5nb7lp/p_analysis_of_dropout/,pgaleone,1484125810,,5,10
291,2017-1-11,2017,1,11,19,5nbgua,"Current best practices for searching for a specific face in a crowd, regardless of facial orientation? Any pretrained models out there (like VGG for faces) that I can finetune to search for specific faces?",https://www.reddit.com/r/MachineLearning/comments/5nbgua/current_best_practices_for_searching_for_a/,TheMoskowitz,1484130348,[removed],0,1
292,2017-1-11,2017,1,11,19,5nbkkh,[P] A ConvLSTM cell for TensorFlow with recurrent batch normalization,https://www.reddit.com/r/MachineLearning/comments/5nbkkh/p_a_convlstm_cell_for_tensorflow_with_recurrent/,carlthome,1484132217,,12,22
293,2017-1-11,2017,1,11,22,5nc3fq,"Tensorflow implementation of FAIR's ""Language Modeling with Gated Convolutional Networks""",https://www.reddit.com/r/MachineLearning/comments/5nc3fq/tensorflow_implementation_of_fairs_language/,anantzoid,1484140419,,0,2
294,2017-1-11,2017,1,11,22,5nc52s,ANDi Games Ltd are bringing machine learning to Android,https://www.reddit.com/r/MachineLearning/comments/5nc52s/andi_games_ltd_are_bringing_machine_learning_to/,KrisWB,1484141030,,0,1
295,2017-1-11,2017,1,11,22,5nc91m,How to train subclasses?,https://www.reddit.com/r/MachineLearning/comments/5nc91m/how_to_train_subclasses/,naya_data,1484142528,[removed],0,1
296,2017-1-12,2017,1,12,0,5ncq2n,Machine Learning Analysis of Ghanas Presidential Election,https://www.reddit.com/r/MachineLearning/comments/5ncq2n/machine_learning_analysis_of_ghanas_presidential/,epigos,1484148122,,0,1
297,2017-1-12,2017,1,12,0,5ncqt8,A Cython/gensim implementation of DeepWalk,https://www.reddit.com/r/MachineLearning/comments/5ncqt8/a_cythongensim_implementation_of_deepwalk/,ksindi,1484148351,,0,1
298,2017-1-12,2017,1,12,0,5ncuvj,My experience in ML in edtech - Which stream a student select after 10th Standard: [Solved using Machine Learning],https://www.reddit.com/r/MachineLearning/comments/5ncuvj/my_experience_in_ml_in_edtech_which_stream_a/,bikashsharmabks,1484149589,,0,1
299,2017-1-12,2017,1,12,0,5ncw4u,"Simple Questions Thread January 11, 2017",https://www.reddit.com/r/MachineLearning/comments/5ncw4u/simple_questions_thread_january_11_2017/,AutoModerator,1484149949,[removed],0,1
300,2017-1-12,2017,1,12,0,5ncwxa,How much sense it makes to change the column to predict on a targeted dataset?,https://www.reddit.com/r/MachineLearning/comments/5ncwxa/how_much_sense_it_makes_to_change_the_column_to/,spi-x-i,1484150176,[removed],0,1
301,2017-1-12,2017,1,12,1,5nd61z,Looking for recommendation on statistic and probability books,https://www.reddit.com/r/MachineLearning/comments/5nd61z/looking_for_recommendation_on_statistic_and/,appleTen99,1484152771,[removed],0,1
302,2017-1-12,2017,1,12,2,5ndhym,Qualcomm is optimizing the Snapdragon 835 for machine learning!,https://www.reddit.com/r/MachineLearning/comments/5ndhym/qualcomm_is_optimizing_the_snapdragon_835_for/,lifemoments,1484155963,,0,1
303,2017-1-12,2017,1,12,2,5ndjf6,[News] GTA V integration into Universe is now open-source,https://www.reddit.com/r/MachineLearning/comments/5ndjf6/news_gta_v_integration_into_universe_is_now/,badhri,1484156356,,31,256
304,2017-1-12,2017,1,12,3,5ndo7m,[R] [1701.02386] AdaGAN: Boosting Generative Models,https://www.reddit.com/r/MachineLearning/comments/5ndo7m/r_170102386_adagan_boosting_generative_models/,badhri,1484157632,,4,19
305,2017-1-12,2017,1,12,3,5ndz93,CNN based regression approach for estimation of machinery's remaining useful life,https://www.reddit.com/r/MachineLearning/comments/5ndz93/cnn_based_regression_approach_for_estimation_of/,[deleted],1484160585,[deleted],0,1
306,2017-1-12,2017,1,12,4,5ne9d5,Generating jokes from LSTMs - School Project ideas - Need expertise,https://www.reddit.com/r/MachineLearning/comments/5ne9d5/generating_jokes_from_lstms_school_project_ideas/,Logeekal,1484163306,[removed],1,1
307,2017-1-12,2017,1,12,4,5ned8f,How to do Object Detection with OpenCV [LIVE],https://www.reddit.com/r/MachineLearning/comments/5ned8f/how_to_do_object_detection_with_opencv_live/,llSourcell,1484164329,,0,1
308,2017-1-12,2017,1,12,5,5nef66,When Dirac meets the factorizer: machine learning in bra-ket notation,https://www.reddit.com/r/MachineLearning/comments/5nef66/when_dirac_meets_the_factorizer_machine_learning/,ahousley,1484164863,,0,1
309,2017-1-12,2017,1,12,5,5negep,[D] Why I cannot reproduce my results with the same data - using Resnet,https://www.reddit.com/r/MachineLearning/comments/5negep/d_why_i_cannot_reproduce_my_results_with_the_same/,nimakhin,1484165178,"Hi CNN experts,
I am training a 32-layer Resnet on my data (5000 2D images, 2 classes) and testing it on 500 validation set. Here is one of my results (http://i.imgur.com/jSrzwlM.png). I am aware of overfitting issue although I am using Dropout at fully connected layers. my questions are 1) when i run the experiment again on the same data why i cannot get these results again? is my wight initialization (he_normal) really playing a role here? 2) any ideas for overfitting issue rather than data augmentation? (L2 regularization is not used in the original Resnet paper, so i decided to use dropout instead) ",18,3
310,2017-1-12,2017,1,12,5,5neixd,Noob question regarding keras model here,https://www.reddit.com/r/MachineLearning/comments/5neixd/noob_question_regarding_keras_model_here/,snate28,1484165844,[removed],0,1
311,2017-1-12,2017,1,12,6,5nf4i4,Multi Layer Perceptron,https://www.reddit.com/r/MachineLearning/comments/5nf4i4/multi_layer_perceptron/,monsterkalaspuff,1484171660,,0,1
312,2017-1-12,2017,1,12,7,5nf6o7,[N] Computer Chips Evolve to Keep Up With Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5nf6o7/n_computer_chips_evolve_to_keep_up_with_deep/,Buck-Nasty,1484172239,,0,1
313,2017-1-12,2017,1,12,7,5nf7n2,DoD tested an autonomous swarm of flying drones that can self-organize using Artificial Intelligence and P2P networking,https://www.reddit.com/r/MachineLearning/comments/5nf7n2/dod_tested_an_autonomous_swarm_of_flying_drones/,[deleted],1484172513,[deleted],0,1
314,2017-1-12,2017,1,12,7,5nf91f,Reinforcement Learning via Recurrent Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5nf91f/reinforcement_learning_via_recurrent/,tanmayshankar1,1484172902,"Hello everyone, 

I worked on Reinforcement Learning via Recurrent Convolutional Neural Networks during my Bachelor's thesis last year; I published my work on it at ICPR 2016. I'd love to know what you guys think of this* , and some future avenues as well.  

You can find the paper here - 
https://arxiv.org/abs/1701.02392

And all of my code is up at: https://github.com/tanmayshankar/RCNN_MDP

 *I know this work is mathematically similar to Value Iteration Networks, but there are some subtle differences. Also, this is in the spirit of making model-based methods great again!",3,8
315,2017-1-12,2017,1,12,8,5nfnxd,[D] Understanding how certain classifiers handle missing values/data,https://www.reddit.com/r/MachineLearning/comments/5nfnxd/d_understanding_how_certain_classifiers_handle/,OrthusML,1484177129,"Hi there! 

I'm new to Machine Learning and i'm having trouble understanding how the following classifiers handle missing values/data:

k-NearestNeighbours, 
Decision Trees 
Naive Bayes  

In which scenario would which named classifier be the best? I've just tried the classifiers on a data set and found that NB is suited beter for a bigger data set. kNN is more suited for a smaller data set? Not entirely sure about Decision trees.

Thank you so much for your help already, if you have any references i'd love to read them :) 

Regards

Orthus",5,2
316,2017-1-12,2017,1,12,13,5nhc8i,Training Siamese network with limited training data,https://www.reddit.com/r/MachineLearning/comments/5nhc8i/training_siamese_network_with_limited_training/,[deleted],1484195875,[removed],0,1
317,2017-1-12,2017,1,12,13,5nhe1d,[P] Training siamese network with limited training data,https://www.reddit.com/r/MachineLearning/comments/5nhe1d/p_training_siamese_network_with_limited_training/,sonnguyen128,1484196499,"Dear all, I am currently working on a visual search project which basically enables users to submit an image query and return similar images as search results. I'd like to train a siamese network to learn an effective similarity embedding to compare images. However, as a project with limited budget, I just had a very limited data set (image pairs). I wonder if anyone has worked on the similar project before and like to share experience? Thank you so much
",10,1
318,2017-1-12,2017,1,12,16,5nhzop,How To Enhance the Performance of Your Machinery With Brennan Adapters?,https://www.reddit.com/r/MachineLearning/comments/5nhzop/how_to_enhance_the_performance_of_your_machinery/,jackerfrinandis,1484204942,,0,1
319,2017-1-12,2017,1,12,17,5ni83z,9 Ways Machine Learning Will Aid Marketers in 2017,https://www.reddit.com/r/MachineLearning/comments/5ni83z/9_ways_machine_learning_will_aid_marketers_in_2017/,CentricAE,1484208928,,0,1
320,2017-1-12,2017,1,12,17,5nibi4,Tensorflow Tree LSTM code?,https://www.reddit.com/r/MachineLearning/comments/5nibi4/tensorflow_tree_lstm_code/,[deleted],1484210706,[removed],0,1
321,2017-1-12,2017,1,12,17,5nicby,[Project] Lectures in machine learning from Medical Image Summer School 2016,https://www.reddit.com/r/MachineLearning/comments/5nicby/project_lectures_in_machine_learning_from_medical/,LordKlevin,1484211183,,0,12
322,2017-1-12,2017,1,12,17,5nicno,[Discussion] Siamese/Triplet Network: How to get pairs/triplets?,https://www.reddit.com/r/MachineLearning/comments/5nicno/discussion_siamesetriplet_network_how_to_get/,spidey-fan,1484211321,[removed],0,1
323,2017-1-12,2017,1,12,18,5niedu,[Discussion] Tree LSTM in Keras/Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5niedu/discussion_tree_lstm_in_kerastensorflow/,spidey-fan,1484212200,[removed],0,1
324,2017-1-12,2017,1,12,20,5nisvv,Medical And Healthcare Applications of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5nisvv/medical_and_healthcare_applications_of_machine/,digitalmarketingrobi,1484219476,,0,1
325,2017-1-12,2017,1,12,20,5niu6n,[D] Implementation of recursive convolution neural net(RCNN),https://www.reddit.com/r/MachineLearning/comments/5niu6n/d_implementation_of_recursive_convolution_neural/,datumx_jan,1484220109,"I am working on to implement the approach in the paper https://arxiv.org/pdf/1603.03101v1.pdf on detecting text in wild using recursive convolution neural net and attention modelling. I am a bit new to deep nets and have developed the following interpretation about the recursive CNN given in the paper:

It is given in the paper that while training could be done by "" reusing
the same convolutional weight matrix multiple times at each
layer."" Does this means the following thing in the pseudo code below

layer1

iterating multiple times over 
{
train convolution_layer(weights) with labels    
}

layer2

iterating multiple times over
{
train convolution_layer(weights) with labels
}

Here in the consecutive iteration in a layer we use weights of previous iteration. We use different weight initialisation for different layers here

While using the network for test set, we will input the weights we got from the last iterations.

Please comment on this interpretation and I would be grateful if anyone could suggest some links and literature for the same. Thank you!
",0,21
326,2017-1-12,2017,1,12,21,5nj5p5,[R] [1701.02547] A Convenient Category for Higher-Order Probability Theory,https://www.reddit.com/r/MachineLearning/comments/5nj5p5/r_170102547_a_convenient_category_for_higherorder/,jesuslop,1484225220,,8,14
327,2017-1-12,2017,1,12,22,5nj8vj,Can we predict flu deaths with Machine Learning and R?,https://www.reddit.com/r/MachineLearning/comments/5nj8vj/can_we_predict_flu_deaths_with_machine_learning/,cavedave,1484226451,,4,16
328,2017-1-12,2017,1,12,22,5njdid,GANs (Generative Adversarial Networks) for text input to image output,https://www.reddit.com/r/MachineLearning/comments/5njdid/gans_generative_adversarial_networks_for_text/,shinysup87,1484228138,[removed],0,1
329,2017-1-12,2017,1,12,22,5njfn7,NLP for Machine Learning(Very Precise and basic introduction),https://www.reddit.com/r/MachineLearning/comments/5njfn7/nlp_for_machine_learningvery_precise_and_basic/,A_D_Exploration,1484228928,,0,1
330,2017-1-12,2017,1,12,22,5njfzt,TensorFlow machine learning now optimized for the Snapdragon 835 and Hexagon 682 DSP,https://www.reddit.com/r/MachineLearning/comments/5njfzt/tensorflow_machine_learning_now_optimized_for_the/,johnmountain,1484229057,,0,1
331,2017-1-12,2017,1,12,23,5nji5v,[D] Can anyone explain what distributed here means ? I am new to ML. Im finding it hard to understand this thing.,https://www.reddit.com/r/MachineLearning/comments/5nji5v/d_can_anyone_explain_what_distributed_here_means/,[deleted],1484229832,[deleted],1,0
332,2017-1-12,2017,1,12,23,5njn1b,Make a Photo Filter with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5njn1b/make_a_photo_filter_with_machine_learning/,nikitaljohnson,1484231461,,0,1
333,2017-1-12,2017,1,12,23,5njryo,[R] Approximate (Variational) Inference with Implicit Probabilistic Models (part 1),https://www.reddit.com/r/MachineLearning/comments/5njryo/r_approximate_variational_inference_with_implicit/,fhuszar,1484233032,,2,47
334,2017-1-13,2017,1,13,0,5njy8l,"Advanced Natural Language Processing Tools for Bot Makers  LUIS, Wit.ai, Api.ai and others (UPDATED)",https://www.reddit.com/r/MachineLearning/comments/5njy8l/advanced_natural_language_processing_tools_for/,bogsformer,1484234992,,0,1
335,2017-1-13,2017,1,13,0,5njypn,When is Noise Contrastive Estimation used and what are its advantages over simple regression models to estimate a probability density function?,https://www.reddit.com/r/MachineLearning/comments/5njypn/when_is_noise_contrastive_estimation_used_and/,AlexanderC05,1484235132,[removed],0,1
336,2017-1-13,2017,1,13,0,5njyqt,[D] Any literature on using a Conv LSTM with word embeddings?,https://www.reddit.com/r/MachineLearning/comments/5njyqt/d_any_literature_on_using_a_conv_lstm_with_word/,c0cky_,1484235145,I have seen a few examples of Conv LSTM but haven't seen any literature on what's going on. Is there any papers/articles on these convolution LSTM networks?,3,7
337,2017-1-13,2017,1,13,0,5nk0bi,Qualcomm is Optimizing the Snapdragon 835 and Hexagon 682 DSP for TensorFlow. : DailyTechNewsShow,https://www.reddit.com/r/MachineLearning/comments/5nk0bi/qualcomm_is_optimizing_the_snapdragon_835_and/,smith2017,1484235617,,0,1
338,2017-1-13,2017,1,13,1,5nkdpt,[R] [1701.02877] Generalisation in Named Entity Recognition: A Quantitative Analysis,https://www.reddit.com/r/MachineLearning/comments/5nkdpt/r_170102877_generalisation_in_named_entity/,leondz,1484239403,,1,3
339,2017-1-13,2017,1,13,2,5nkk73,[D] Autonomous driving research pipeline,https://www.reddit.com/r/MachineLearning/comments/5nkk73/d_autonomous_driving_research_pipeline/,harmonium1,1484241156,"Looking at research positions for companies building self-driving vehicles (or software for them), they almost invariably mention that background in C++ and OpenCV is important (often not even mentioning Python!).  For those familiar with modern CV/deep learning research, almost everything (research code-wise) is in Python (or Lua), and few grad students I talk to ever use C++ (many have used it only as undergrads).  I haven't heard of anyone using TensorFlow's C++ API, for example.

So when companies are asking for C++ skills, are they talking about being able to build custom deep learning software, or do they have their own in-house C++ DL library with no Python API due to the importance of speed?  It seems like they're looking for C++ developers with some exposure to deep learning, rather than deep learning researchers (who almost all use Python primarily).",7,14
340,2017-1-13,2017,1,13,3,5nl3am,"[Project] I made a decision when I started that I would make my personal CNN toolbox public by the beginning of this year, no matter what state it is in.",https://www.reddit.com/r/MachineLearning/comments/5nl3am/project_i_made_a_decision_when_i_started_that_i/,[deleted],1484246224,[deleted],0,1
341,2017-1-13,2017,1,13,3,5nl3oc,[P] Another theano-based CNN toolbox.,https://www.reddit.com/r/MachineLearning/comments/5nl3oc/p_another_theanobased_cnn_toolbox/,procarastinizer,1484246316,,1,4
342,2017-1-13,2017,1,13,3,5nl42c,Learn R &amp; Python by demo for Machine learning models in 15 mins.Short &amp; Precise .Visit it :),https://www.reddit.com/r/MachineLearning/comments/5nl42c/learn_r_python_by_demo_for_machine_learning/,A_D_Exploration,1484246416,,0,1
343,2017-1-13,2017,1,13,4,5nlick,Automation and the future of concept art,https://www.reddit.com/r/MachineLearning/comments/5nlick/automation_and_the_future_of_concept_art/,[deleted],1484250194,[removed],0,1
344,2017-1-13,2017,1,13,5,5nlmdn,Clustering with disjunct pairs,https://www.reddit.com/r/MachineLearning/comments/5nlmdn/clustering_with_disjunct_pairs/,wehnsdaefflae,1484251316,[removed],0,1
345,2017-1-13,2017,1,13,5,5nlxdd,Applications of reinforcement learning in computer vision,https://www.reddit.com/r/MachineLearning/comments/5nlxdd/applications_of_reinforcement_learning_in/,[deleted],1484254289,[removed],0,1
346,2017-1-13,2017,1,13,6,5nlzp3,[Discussion] Applications of reinforcement learning in computer vision?,https://www.reddit.com/r/MachineLearning/comments/5nlzp3/discussion_applications_of_reinforcement_learning/,wencc,1484254914,"What are existing applications or potential applications of reinforcement learning in computer vision?
Recently, I got very interested in reinforcement learning, and have been reading the Introduction to Reinforcement Learning book and some recent papers. As a result, I want to do some research in reinforcement learning in the upcoming Spring semester. Since I have done some research in CV last semester, I am looking for reinforcement learning applied in CV. However, not much come up from search online. Any idea or examples you know of such application?
Thanks",14,5
347,2017-1-13,2017,1,13,6,5nm9rb,"[P] Training a neural network to map from x,y of an images pixels to r,g,b.",https://www.reddit.com/r/MachineLearning/comments/5nm9rb/p_training_a_neural_network_to_map_from_xy_of_an/,green349,1484257608,,28,113
348,2017-1-13,2017,1,13,8,5nmrn3,The Google Brain team Looking Back on 2016,https://www.reddit.com/r/MachineLearning/comments/5nmrn3/the_google_brain_team_looking_back_on_2016/,Dogsindahouse1,1484262476,,0,1
349,2017-1-13,2017,1,13,10,5nnot7,[Project]Behavioral Cloning: Navigating a Car in a Simulator,https://www.reddit.com/r/MachineLearning/comments/5nnot7/projectbehavioral_cloning_navigating_a_car_in_a/,[deleted],1484272516,[deleted],0,1
350,2017-1-13,2017,1,13,10,5nnpei,[P] Behavioral Cloning: Navigating a Car in a Simulator,https://www.reddit.com/r/MachineLearning/comments/5nnpei/p_behavioral_cloning_navigating_a_car_in_a/,upulbandara,1484272719,,0,2
351,2017-1-13,2017,1,13,11,5nny7p,[D] Building a machine learning recommender system versus using a SaaS,https://www.reddit.com/r/MachineLearning/comments/5nny7p/d_building_a_machine_learning_recommender_system/,tnetennba3,1484275393,"Hi there,

I am fairly new to machine learning but I have been tasked with creating a recommender system for a video streaming app, for short viral videos. Essentially, we will want each user to have personalised recommendations to aid video discovery. I have one month to do this.

What I was wondering was whether I should i) attempt to build the recommender system from scratch with the aid of open source software like Apache Spark or ii) turn to a Saas like Recombee?

My hesitation with the former option is whether I could feasibly create a good recommender system (in terms of accuracy and coverage) within a month, given my lack of experience too.

What are your thoughts? I appreciate any suggestions!",3,0
352,2017-1-13,2017,1,13,13,5nojfe,"[P] An awesome iTerm2 backend for Matplotlib, so you can plot directly in your terminal.",https://www.reddit.com/r/MachineLearning/comments/5nojfe/p_an_awesome_iterm2_backend_for_matplotlib_so_you/,daleroberts0,1484282253,,3,2
353,2017-1-13,2017,1,13,13,5nomm2,Are we at an inflection point in AI?,https://www.reddit.com/r/MachineLearning/comments/5nomm2/are_we_at_an_inflection_point_in_ai/,alpha_hxCR8,1484283414,[removed],0,1
354,2017-1-13,2017,1,13,14,5noopt,How to obtain necessary fundamentals in learning Machine Learning in a short time period.,https://www.reddit.com/r/MachineLearning/comments/5noopt/how_to_obtain_necessary_fundamentals_in_learning/,0xchamin,1484284136,,0,1
355,2017-1-13,2017,1,13,14,5nosw7,Google is using machine learning to reduce the data needed for high-resolution images,https://www.reddit.com/r/MachineLearning/comments/5nosw7/google_is_using_machine_learning_to_reduce_the/,aaron_parker,1484285565,,0,1
356,2017-1-13,2017,1,13,15,5nozee,Brainstorming a Horror Computer Game Using Google's TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5nozee/brainstorming_a_horror_computer_game_using/,JonoExplainsThings,1484287869,,0,1
357,2017-1-13,2017,1,13,15,5np50k,Neural Network Learns The Physics of Fluids and Smoke | Two Minute Papers,https://www.reddit.com/r/MachineLearning/comments/5np50k/neural_network_learns_the_physics_of_fluids_and/,Buck-Nasty,1484290127,,0,1
358,2017-1-13,2017,1,13,16,5npb4t,"Relevant Literature on ""Adapting Deep Learning Models""?",https://www.reddit.com/r/MachineLearning/comments/5npb4t/relevant_literature_on_adapting_deep_learning/,rishabh467,1484292776,[removed],0,1
359,2017-1-13,2017,1,13,17,5npjgu,Useful tips to teach you how to used planetary mixers for sale!,https://www.reddit.com/r/MachineLearning/comments/5npjgu/useful_tips_to_teach_you_how_to_used_planetary/,mixmachinery,1484296801,,1,1
360,2017-1-13,2017,1,13,17,5npjrk,[Discussion] machine learning engineer career path,https://www.reddit.com/r/MachineLearning/comments/5npjrk/discussion_machine_learning_engineer_career_path/,mitbal,1484296948,[removed],0,1
361,2017-1-13,2017,1,13,18,5npnf0,FREE WEBINAR ON INTELLIGENT INTRANETS!,https://www.reddit.com/r/MachineLearning/comments/5npnf0/free_webinar_on_intelligent_intranets/,shivali1,1484298789,[removed],0,1
362,2017-1-13,2017,1,13,19,5npyqn,6 MIPI CSI-2 Cameras support for NVIDIA Jetson TX1,https://www.reddit.com/r/MachineLearning/comments/5npyqn/6_mipi_csi2_cameras_support_for_nvidia_jetson_tx1/,econsystems,1484304505,,0,1
363,2017-1-13,2017,1,13,21,5nq9uv,[Question] Machine Learning/AI MSc programs in Europe,https://www.reddit.com/r/MachineLearning/comments/5nq9uv/question_machine_learningai_msc_programs_in_europe/,__bee,1484309569,[removed],0,1
364,2017-1-13,2017,1,13,22,5nqlyl,[D] Any good starting point for computational humor?,https://www.reddit.com/r/MachineLearning/comments/5nqlyl/d_any_good_starting_point_for_computational_humor/,JamminJames921,1484314298,"My professor (jokingly) told me that the final frontier of AI is to make good jokes. I found it quite interesting and equally challenging, for it requires an algorithm to learn social contexts in our culture, etc.

Any redditors who might have touched on this? I would like to know of good resources where I can start.

Thank you so much!",26,28
365,2017-1-13,2017,1,13,23,5nr1f3,[R] Proceedings of the 1st Machine Learning for Healthcare Conference,https://www.reddit.com/r/MachineLearning/comments/5nr1f3/r_proceedings_of_the_1st_machine_learning_for/,urish,1484319532,,1,45
366,2017-1-14,2017,1,14,0,5nr6ex,What is the correct way to whiten CIFAR10 dataset in python?,https://www.reddit.com/r/MachineLearning/comments/5nr6ex/what_is_the_correct_way_to_whiten_cifar10_dataset/,Coderx7,1484320969,[removed],0,1
367,2017-1-14,2017,1,14,1,5nrhdt,Are we really going to put data scientists out of business? Is this possible?,https://www.reddit.com/r/MachineLearning/comments/5nrhdt/are_we_really_going_to_put_data_scientists_out_of/,jgawrilo,1484324084,,0,1
368,2017-1-14,2017,1,14,1,5nrq6x,[N] Microsoft acquires deep learning startup Maluuba,https://www.reddit.com/r/MachineLearning/comments/5nrq6x/n_microsoft_acquires_deep_learning_startup_maluuba/,pierrelux,1484326469,,29,101
369,2017-1-14,2017,1,14,2,5nrt1t,[News] Machine Learning: From University to to Industry (a16z podcast episode),https://www.reddit.com/r/MachineLearning/comments/5nrt1t/news_machine_learning_from_university_to_to/,julian88888888,1484327219,,0,1
370,2017-1-14,2017,1,14,2,5ns2ep,DataScience Digest - Issue #5,https://www.reddit.com/r/MachineLearning/comments/5ns2ep/datascience_digest_issue_5/,flyelephant,1484329688,,0,1
371,2017-1-14,2017,1,14,3,5nshde,[P] Port of OpenAI Client to Rust,https://www.reddit.com/r/MachineLearning/comments/5nshde/p_port_of_openai_client_to_rust/,andrew-lucker,1484333691,,6,2
372,2017-1-14,2017,1,14,4,5nsk9m,Deep Learning Summer School 2017,https://www.reddit.com/r/MachineLearning/comments/5nsk9m/deep_learning_summer_school_2017/,epeeista,1484334457,[removed],0,1
373,2017-1-14,2017,1,14,4,5nsopn,"Python code for Reinforcement Learning: An Introduction (2nd Ed, Sutton &amp; Barto)",https://www.reddit.com/r/MachineLearning/comments/5nsopn/python_code_for_reinforcement_learning_an/,thecity2,1484335682,,0,3
374,2017-1-14,2017,1,14,4,5nspnk,An amazing GAN tutorial slides,https://www.reddit.com/r/MachineLearning/comments/5nspnk/an_amazing_gan_tutorial_slides/,terryum,1484335927,,0,1
375,2017-1-14,2017,1,14,4,5nsqgn,[P] Toy Neural Net Visualization in Browser,https://www.reddit.com/r/MachineLearning/comments/5nsqgn/p_toy_neural_net_visualization_in_browser/,thinkdip,1484336149,,9,2
376,2017-1-14,2017,1,14,5,5nt5cj,What is the current state of the art in question answering from passages of text?,https://www.reddit.com/r/MachineLearning/comments/5nt5cj/what_is_the_current_state_of_the_art_in_question/,questionAnswering,1484340272,[removed],0,1
377,2017-1-14,2017,1,14,5,5nt5h6,[P] New browser-based CNN project with MNIST demo and training page,https://www.reddit.com/r/MachineLearning/comments/5nt5h6/p_new_browserbased_cnn_project_with_mnist_demo/,DenseInL2,1484340308,,5,9
378,2017-1-14,2017,1,14,6,5ntdox,#define CTO OpenAI,https://www.reddit.com/r/MachineLearning/comments/5ntdox/define_cto_openai/,circuithunter,1484342583,,0,1
379,2017-1-14,2017,1,14,6,5nte3c,[D] is the recent work from metamind the state of the art in Question Answering?,https://www.reddit.com/r/MachineLearning/comments/5nte3c/d_is_the_recent_work_from_metamind_the_state_of/,questionAnswering,1484342692,http://venturebeat.com/2016/11/15/salesforce-touts-ai-research-progress-beating-google-ibm-and-microsoft-on-question-answering/,2,0
380,2017-1-14,2017,1,14,6,5ntf9x,"I'm working on an iOS version of this a contextual intelligence SDK. I would love it if some fellow devs signed up and gave me some feedback. Upon signing up, I'll send out further details. Thanks again!",https://www.reddit.com/r/MachineLearning/comments/5ntf9x/im_working_on_an_ios_version_of_this_a_contextual/,senabhishek,1484343005,,0,1
381,2017-1-14,2017,1,14,9,5nuavn,A Neural Network was used to look at 1.5 million satellite images of the homes of Democratic and Republican donors to see who installed solar more. Check out the results!,https://www.reddit.com/r/MachineLearning/comments/5nuavn/a_neural_network_was_used_to_look_at_15_million/,sleepinggreyhound,1484352171,,0,1
382,2017-1-14,2017,1,14,9,5nujft,This guy is going to damage ML's reputation.,https://www.reddit.com/r/MachineLearning/comments/5nujft/this_guy_is_going_to_damage_mls_reputation/,[deleted],1484354896,,0,1
383,2017-1-14,2017,1,14,9,5nukzi,Microsoft CNTK NuGet won't install on VisualStudio for Mac,https://www.reddit.com/r/MachineLearning/comments/5nukzi/microsoft_cntk_nuget_wont_install_on_visualstudio/,loretoparisi,1484355392,,0,1
384,2017-1-14,2017,1,14,10,5nuuw0,How to Make a Prediction - Intro to Deep Learning #1,https://www.reddit.com/r/MachineLearning/comments/5nuuw0/how_to_make_a_prediction_intro_to_deep_learning_1/,llSourcell,1484358768,,0,1
385,2017-1-14,2017,1,14,11,5nuxzw,Can someone explain the difference between Binary Encoding &amp; One Hot Encoding,https://www.reddit.com/r/MachineLearning/comments/5nuxzw/can_someone_explain_the_difference_between_binary/,dreyco,1484359831,[removed],0,1
386,2017-1-14,2017,1,14,11,5nv0lb,Applying Deep Learning to Video?,https://www.reddit.com/r/MachineLearning/comments/5nv0lb/applying_deep_learning_to_video/,cctap,1484360749,[removed],0,1
387,2017-1-14,2017,1,14,11,5nv5ez,"Looking for expert in Ansible/Splunk/Docker for project based work. $15,000 to $18,000",https://www.reddit.com/r/MachineLearning/comments/5nv5ez/looking_for_expert_in_ansiblesplunkdocker_for/,dtrain15,1484362416,[removed],0,1
388,2017-1-14,2017,1,14,13,5nvm2b,How to compress with VAE?,https://www.reddit.com/r/MachineLearning/comments/5nvm2b/how_to_compress_with_vae/,houyunqing,1484368456,[removed],0,1
389,2017-1-14,2017,1,14,15,5nvzez,[D] Question about possibilities for a program based on DeepMask,https://www.reddit.com/r/MachineLearning/comments/5nvzez/d_question_about_possibilities_for_a_program/,ericools,1484373791,"I own a small inventory service company and I feel like much of what I do could soon be automated.  This software http://www.digitaltrends.com/computing/facebook-open-source-image-ai/ seems like it should be able to do the hard part of what is required for an AI auditor.

It seems to me that it should be possible to walk down an isle in a store and snap pictures of say a wall of peg hooks with price tags or barcodes turned to face the user and have a program count how many of each price or sku there is and spit it out as a .csv file assuming you have taken reasonably decent images.

Obviously this is going to be limited to easily visible areas (at least for now), but that might be enough to justify development of such a program.  

Does anyone know if this would require extensive training of the AI to recognize the shape and size of every item it would have to count or some kind of trial / error and correction method?

I doubt I could justify paying for such a thing now, but I'm wondering how far off this might be...",7,1
390,2017-1-14,2017,1,14,16,5nwabs,scikit-learn test_size and train_size pitfalls and coming changes,https://www.reddit.com/r/MachineLearning/comments/5nwabs/scikitlearn_test_size_and_train_size_pitfalls_and/,[deleted],1484378697,[deleted],0,1
391,2017-1-14,2017,1,14,16,5nwd1i,[N] scikit-learn test_size and train_size pitfalls and coming changes,https://www.reddit.com/r/MachineLearning/comments/5nwd1i/n_scikitlearn_test_size_and_train_size_pitfalls/,nfliu,1484379980,,2,15
392,2017-1-14,2017,1,14,18,5nwmfw,[D] Is there a paper that you think is written unusually elegantly? What makes for good expository writing in ML?,https://www.reddit.com/r/MachineLearning/comments/5nwmfw/d_is_there_a_paper_that_you_think_is_written/,xristaforante,1484385052,"A broader prompt: what makes a paper worth reading to you, controlling of course for the significance of the central idea?",38,86
393,2017-1-14,2017,1,14,18,5nwphb,Deep Learning Nanodegree Foundation Program  Trailer,https://www.reddit.com/r/MachineLearning/comments/5nwphb/deep_learning_nanodegree_foundation_program/,Dutchcheesehead,1484386827,,0,1
394,2017-1-14,2017,1,14,18,5nwq8k,Correlation plot to visualize correlation values,https://www.reddit.com/r/MachineLearning/comments/5nwq8k/correlation_plot_to_visualize_correlation_values/,A_D_Exploration,1484387277,,0,1
395,2017-1-14,2017,1,14,20,5nwxzo,"""top-down"" approaches in ML for big datasets like point cloud data?",https://www.reddit.com/r/MachineLearning/comments/5nwxzo/topdown_approaches_in_ml_for_big_datasets_like/,jon_master,1484391760,[removed],0,1
396,2017-1-14,2017,1,14,21,5nx8wa,[D] Time spent reading on the clock?,https://www.reddit.com/r/MachineLearning/comments/5nx8wa/d_time_spent_reading_on_the_clock/,Traner,1484397573,"Hey /r/machinelearning!

How much time do you spend during working hours weekly reading papers / attending seminars  /  educating yourself in some way?

",10,9
397,2017-1-14,2017,1,14,22,5nxdle,"Sv Dolum Makinas, Sv Dolum Makinesi, Sv Dolum Makinalar, Sv Dolum Makineleri",https://www.reddit.com/r/MachineLearning/comments/5nxdle/sv_dolum_makinas_sv_dolum_makinesi_sv/,birduman,1484399746,,0,1
398,2017-1-15,2017,1,15,1,5ny4vg,[D] How hard would it be to achieve clothes recognition?,https://www.reddit.com/r/MachineLearning/comments/5ny4vg/d_how_hard_would_it_be_to_achieve_clothes/,celebez,1484410248,"Super machine learning noob here, so excuse me if I sound dumb.

Let's say I want to give image of a person wearing some clothes as an input (or 1 piece of clothes in isolation) and then find similiar clothes in my database of clothes images. How hard would it be? Is it even possible? I was thinking of using tensor flow (or neural network in general). How would you even call such operation? Image search? Or is it classification?",7,1
399,2017-1-15,2017,1,15,2,5nyntx,[D]Papers about decision boundary created by networks with ReLUs / prototype feature matcher ideas like in dense associative network papers?,https://www.reddit.com/r/MachineLearning/comments/5nyntx/dpapers_about_decision_boundary_created_by/,robintibor,1484416077,"Does anybody know papers that look more closely at what kind of decision boundaries are created by (convolutional  or not) networks which use ReLUs? 
The dense associative memory papers (here https://arxiv.org/abs/1606.01164 and here https://arxiv.org/abs/1701.00939) try to illuminate this a bit on this with a view of models along a continuum from ""feature matching"" to ""prototype-based"".
Can anybody link to more papers that investigate these kinds of questions?",2,4
400,2017-1-15,2017,1,15,4,5nz8b6,Feature vs Sample Batch Normalization in Keras?,https://www.reddit.com/r/MachineLearning/comments/5nz8b6/feature_vs_sample_batch_normalization_in_keras/,butWhoWasBee,1484422174,[removed],0,1
401,2017-1-15,2017,1,15,4,5nze6f,Thoughts: Scoring in-person audience engagement through video footage,https://www.reddit.com/r/MachineLearning/comments/5nze6f/thoughts_scoring_inperson_audience_engagement/,Loggerny,1484423926,,0,1
402,2017-1-15,2017,1,15,6,5nzrpl,Practical Deep Learning For Coders18 hours of lessons for free,https://www.reddit.com/r/MachineLearning/comments/5nzrpl/practical_deep_learning_for_coders18_hours_of/,Dogsindahouse1,1484427828,,0,1
403,2017-1-15,2017,1,15,6,5o01b3,Humanity as a collective intelligence,https://www.reddit.com/r/MachineLearning/comments/5o01b3/humanity_as_a_collective_intelligence/,aidanrocke,1484430646,,0,1
404,2017-1-15,2017,1,15,7,5o06tc,Factors for overfitting (especially using LSTM),https://www.reddit.com/r/MachineLearning/comments/5o06tc/factors_for_overfitting_especially_using_lstm/,AlCapown3d,1484432283,[removed],1,1
405,2017-1-15,2017,1,15,9,5o11tz,[Hardware] a workstation for DL around 4x Titan X,https://www.reddit.com/r/MachineLearning/comments/5o11tz/hardware_a_workstation_for_dl_around_4x_titan_x/,dd_2_dd,1484441942,[removed],0,1
406,2017-1-15,2017,1,15,10,5o1cgb,Datumbox Machine Learning Framework version 0.8.0 released,https://www.reddit.com/r/MachineLearning/comments/5o1cgb/datumbox_machine_learning_framework_version_080/,datumbox,1484445527,,0,1
407,2017-1-15,2017,1,15,11,5o1lqk,How to Use Deeplearning4J in Android Apps,https://www.reddit.com/r/MachineLearning/comments/5o1lqk/how_to_use_deeplearning4j_in_android_apps/,dl4j_contributor,1484448810,,0,1
408,2017-1-15,2017,1,15,12,5o1nsi,[D] Recommendation for bounding box annotation?,https://www.reddit.com/r/MachineLearning/comments/5o1nsi/d_recommendation_for_bounding_box_annotation/,st8ic,1484449552,There are obviously lots of image annotation tools out there. I'm in need of a simple one for bounding box coordinates. Does anyone have experience with one that they'd recommend?,6,7
409,2017-1-15,2017,1,15,14,5o2ag7,I wonder if a potential use case for Deepminds text to speech is automated singing... Maybe we could train it on the voices of some pop stars and then give it text and a pitch...,https://www.reddit.com/r/MachineLearning/comments/5o2ag7/i_wonder_if_a_potential_use_case_for_deepminds/,validated1,1484458130,[removed],0,1
410,2017-1-15,2017,1,15,14,5o2cjm, ral stor but Se with 3 diffrnt girls fr 1 mnth,https://www.reddit.com/r/MachineLearning/comments/5o2cjm/_ral_stor_but_se_with_3_diffrnt_girls/,Bennettbridri,1484459021,[removed],0,1
411,2017-1-15,2017,1,15,15,5o2hk5,"(P) Mixed data types -- booleans, ints, and float32",https://www.reddit.com/r/MachineLearning/comments/5o2hk5/p_mixed_data_types_booleans_ints_and_float32/,[deleted],1484461064,[removed],0,1
412,2017-1-15,2017,1,15,15,5o2k41,Geoffrey Hinton's story,https://www.reddit.com/r/MachineLearning/comments/5o2k41/geoffrey_hintons_story/,0xchamin,1484462237,,0,1
413,2017-1-15,2017,1,15,15,5o2lee,What do you think of the udacity's deep learning foundations nanodegree,https://www.reddit.com/r/MachineLearning/comments/5o2lee/what_do_you_think_of_the_udacitys_deep_learning/,0xelectron,1484462784,[removed],0,1
414,2017-1-15,2017,1,15,16,5o2ofh,"For neural nets, what exactly not does regularization do to the parameters?",https://www.reddit.com/r/MachineLearning/comments/5o2ofh/for_neural_nets_what_exactly_not_does/,[deleted],1484464163,[removed],0,1
415,2017-1-15,2017,1,15,17,5o2wix,[P] Autoencoding a single bit with VAE: Understanding the variational family,https://www.reddit.com/r/MachineLearning/comments/5o2wix/p_autoencoding_a_single_bit_with_vae/,[deleted],1484468130,[deleted],0,1
416,2017-1-15,2017,1,15,17,5o2wro,[P] Autoencoding a single bit with VAE: Understanding the variational family,https://www.reddit.com/r/MachineLearning/comments/5o2wro/p_autoencoding_a_single_bit_with_vae/,rui_,1484468254,,8,27
417,2017-1-15,2017,1,15,17,5o302t,Any advice for Machine Learning project to put my cv!,https://www.reddit.com/r/MachineLearning/comments/5o302t/any_advice_for_machine_learning_project_to_put_my/,airforce01,1484470020,[removed],0,1
418,2017-1-15,2017,1,15,18,5o31jb,"[P] Recognizing Traffic Lights With Deep Learning (and winning $5,000)",https://www.reddit.com/r/MachineLearning/comments/5o31jb/p_recognizing_traffic_lights_with_deep_learning/,davidbrai,1484470829,,34,169
419,2017-1-15,2017,1,15,22,5o3w1q,Introduction to Natural Language Processing with fastText,https://www.reddit.com/r/MachineLearning/comments/5o3w1q/introduction_to_natural_language_processing_with/,hoaphumanoid,1484486374,,0,1
420,2017-1-16,2017,1,16,0,5o4ba1,[P]GitHub - rcmalli/keras-squeezenet: SqueezeNet implementation with Keras Framework,https://www.reddit.com/r/MachineLearning/comments/5o4ba1/pgithub_rcmallikerassqueezenet_squeezenet/,commafighter,1484492548,,2,9
421,2017-1-16,2017,1,16,0,5o4glb,[R] The More You Know: Using Knowledge Graphs for Image Classification,https://www.reddit.com/r/MachineLearning/comments/5o4glb/r_the_more_you_know_using_knowledge_graphs_for/,goodside,1484494333,,2,17
422,2017-1-16,2017,1,16,1,5o4ol1,[D] The Anatomy of Deep Learning Frameworks,https://www.reddit.com/r/MachineLearning/comments/5o4ol1/d_the_anatomy_of_deep_learning_frameworks/,gokstudio,1484496840,,0,23
423,2017-1-16,2017,1,16,2,5o562y,GitHub - jbuckman/boilerplate-dynet-rnn-lm: Boilerplate code for quickly getting set up to run language modeling experiments,https://www.reddit.com/r/MachineLearning/comments/5o562y/github_jbuckmanboilerplatedynetrnnlm_boilerplate/,[deleted],1484501976,[deleted],0,1
424,2017-1-16,2017,1,16,3,5o5ffa,PyTorch - An Unofficial Startup Guide (with notes on its current progress),https://www.reddit.com/r/MachineLearning/comments/5o5ffa/pytorch_an_unofficial_startup_guide_with_notes_on/,iamtrask,1484504642,,2,2
425,2017-1-16,2017,1,16,4,5o5of0,Has Gumbel-Softmax been used in Response Generation tasks to Promote Diversity?,https://www.reddit.com/r/MachineLearning/comments/5o5of0/has_gumbelsoftmax_been_used_in_response/,throwaway775849,1484507125,[removed],0,1
426,2017-1-16,2017,1,16,4,5o5ye9,deeplearning for gene ontology,https://www.reddit.com/r/MachineLearning/comments/5o5ye9/deeplearning_for_gene_ontology/,[deleted],1484509942,[removed],0,1
427,2017-1-16,2017,1,16,5,5o60rj,I found s in th Internet website. rl stor for guys,https://www.reddit.com/r/MachineLearning/comments/5o60rj/i_found_s_in_th_internet_website_rl_stor/,Dominicproggi,1484510589,[removed],0,1
428,2017-1-16,2017,1,16,5,5o6533,[Infrastructure] Where do you train your models?,https://www.reddit.com/r/MachineLearning/comments/5o6533/infrastructure_where_do_you_train_your_models/,dejormo,1484511761,[removed],0,1
429,2017-1-16,2017,1,16,5,5o6ake,What is the name of the algorithm?,https://www.reddit.com/r/MachineLearning/comments/5o6ake/what_is_the_name_of_the_algorithm/,chrisabrams,1484513274,[removed],0,1
430,2017-1-16,2017,1,16,5,5o6btc,[R] List of tools and resources related to the use of machine learning for cyber security,https://www.reddit.com/r/MachineLearning/comments/5o6btc/r_list_of_tools_and_resources_related_to_the_use/,WTSxDev,1484513617,,0,17
431,2017-1-16,2017,1,16,7,5o6qsb,How can Deep Convolutional Generative Adversarial Networks (DCGANs) be used for multi-modal image retrieval?,https://www.reddit.com/r/MachineLearning/comments/5o6qsb/how_can_deep_convolutional_generative_adversarial/,MithrandirGr,1484517878,[removed],0,1
432,2017-1-16,2017,1,16,7,5o6svt,[P] GitHub - jbuckman/boilerplate-dynet-rnn-lm: Boilerplate code for language modeling experiments,https://www.reddit.com/r/MachineLearning/comments/5o6svt/p_github_jbuckmanboilerplatedynetrnnlm/,jacobbuckman,1484518469,,0,6
433,2017-1-16,2017,1,16,7,5o6trq,Using a logistic regression to evaluate the outcome of a classifier model using psuedo R2,https://www.reddit.com/r/MachineLearning/comments/5o6trq/using_a_logistic_regression_to_evaluate_the/,rmucsd,1484518734,[removed],0,1
434,2017-1-16,2017,1,16,7,5o6zhp,NVIDIA DIGITS Assists Alzheimers Disease Prediction,https://www.reddit.com/r/MachineLearning/comments/5o6zhp/nvidia_digits_assists_alzheimers_disease/,harrism,1484520390,,0,1
435,2017-1-16,2017,1,16,8,5o77u4,"Intel open sources BigDL, deep learning on Spark",https://www.reddit.com/r/MachineLearning/comments/5o77u4/intel_open_sources_bigdl_deep_learning_on_spark/,[deleted],1484522805,[deleted],0,1
436,2017-1-16,2017,1,16,8,5o783n,"[N] Intel open sources BigDL, for deep learning on Spark",https://www.reddit.com/r/MachineLearning/comments/5o783n/n_intel_open_sources_bigdl_for_deep_learning_on/,NYDreamer,1484522886,,2,14
437,2017-1-16,2017,1,16,8,5o794h,Can we have a data wrangling thread?,https://www.reddit.com/r/MachineLearning/comments/5o794h/can_we_have_a_data_wrangling_thread/,[deleted],1484523186,[removed],0,1
438,2017-1-16,2017,1,16,8,5o79x0,[P] A deep learning traffic light detector using dlib and a few images from Google street view,https://www.reddit.com/r/MachineLearning/comments/5o79x0/p_a_deep_learning_traffic_light_detector_using/,davis685,1484523422,,0,3
439,2017-1-16,2017,1,16,8,5o7a8k,Predict when someone will wake up?,https://www.reddit.com/r/MachineLearning/comments/5o7a8k/predict_when_someone_will_wake_up/,tictactells,1484523517,[removed],0,1
440,2017-1-16,2017,1,16,8,5o7e6w,[R] The Frog of CIFAR 10 - pretty visualisations from a generative model,https://www.reddit.com/r/MachineLearning/comments/5o7e6w/r_the_frog_of_cifar_10_pretty_visualisations_from/,Ninjakannon,1484524743,,2,4
441,2017-1-16,2017,1,16,10,5o7xmp,Cnc All-in-one Veneer Peeling Machine Log Veneer Peeling Machinetimber/log Peeling Machine/The Cutting Thickness 0.25~4.0mm Veneer Rotary Cutting Machine/veneer Peeling Machine,https://www.reddit.com/r/MachineLearning/comments/5o7xmp/cnc_allinone_veneer_peeling_machine_log_veneer/,woodworking-machine,1484530755,,0,1
442,2017-1-16,2017,1,16,11,5o8818,[R] Deep Probabilistic Programming,https://www.reddit.com/r/MachineLearning/comments/5o8818/r_deep_probabilistic_programming/,mimighost,1484534194,,17,82
443,2017-1-16,2017,1,16,13,5o8s7t,What are the best pretrained libraries for sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/5o8s7t/what_are_the_best_pretrained_libraries_for/,butWhoWasBee,1484541239,[removed],0,1
444,2017-1-16,2017,1,16,14,5o8woe,"[Machine Learning Guys:] How should I apply Max-Pooling after Sparse Coding, on intensity vectors for my dataset?",https://www.reddit.com/r/MachineLearning/comments/5o8woe/machine_learning_guys_how_should_i_apply/,[deleted],1484542850,[removed],0,1
445,2017-1-16,2017,1,16,17,5o9ss3,What is the link (if any) between turing completeness and universal approximation theorem?,https://www.reddit.com/r/MachineLearning/comments/5o9ss3/what_is_the_link_if_any_between_turing/,Ayakalam,1484556116,[removed],0,1
446,2017-1-16,2017,1,16,17,5o9t78,What is hot melt glue made of?,https://www.reddit.com/r/MachineLearning/comments/5o9t78/what_is_hot_melt_glue_made_of/,mixmachinery,1484556329,,1,1
447,2017-1-16,2017,1,16,18,5o9v9w,What is an affordable and programmable Drone I can purchase to play around?,https://www.reddit.com/r/MachineLearning/comments/5o9v9w/what_is_an_affordable_and_programmable_drone_i/,xristos_forokolomvos,1484557340,[removed],0,1
448,2017-1-16,2017,1,16,18,5o9vfx,[D] Keras will be added to core TensorFlow at Google,https://www.reddit.com/r/MachineLearning/comments/5o9vfx/d_keras_will_be_added_to_core_tensorflow_at_google/,wei_jok,1484557417,,35,218
449,2017-1-16,2017,1,16,18,5oa0yy,[D] Any ideas on learning a awk like line editor?,https://www.reddit.com/r/MachineLearning/comments/5oa0yy/d_any_ideas_on_learning_a_awk_like_line_editor/,godspeed_china,1484560108,"User gives some examples: the input are several text line with corresponding output text line.  
The task is to learn a mapping function converting each input line to corresponding output line.  If succeed, we will have a neural awk program!
Currently I have no ideas to parameterize the task. Hope somebody could propose a good starting points: features used, architectures...  
Thanks!
",4,0
450,2017-1-16,2017,1,16,19,5oa8as,[P] Traffic signs classification with a CNN (TensorFlow),https://www.reddit.com/r/MachineLearning/comments/5oa8as/p_traffic_signs_classification_with_a_cnn/,navoshta,1484563660,,0,7
451,2017-1-16,2017,1,16,19,5oa8yx,Neural networks and being able to enforce some rules,https://www.reddit.com/r/MachineLearning/comments/5oa8yx/neural_networks_and_being_able_to_enforce_some/,pvkooten,1484564009,[removed],0,1
452,2017-1-16,2017,1,16,20,5oachm,Gaussian word embeddings (McCallum) can't reproduce results,https://www.reddit.com/r/MachineLearning/comments/5oachm/gaussian_word_embeddings_mccallum_cant_reproduce/,landau1,1484565762,[removed],0,1
453,2017-1-16,2017,1,16,20,5oadsc,5 razones para comprar maquinaria de ocasin,https://www.reddit.com/r/MachineLearning/comments/5oadsc/5_razones_para_comprar_maquinaria_de_ocasin/,Barriuso,1484566404,,0,1
454,2017-1-16,2017,1,16,21,5oahat,Best hardware/OS for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/5oahat/best_hardwareos_for_machine_learning/,aviel08,1484568141,[removed],0,1
455,2017-1-16,2017,1,16,22,5oaskz,[D] Practical use of learning theory,https://www.reddit.com/r/MachineLearning/comments/5oaskz/d_practical_use_of_learning_theory/,PsychedelicStore,1484572827,"The Edx course ""Learning from data"" by professor Yaser S. Abu-Mostafa teaches, in its first part, learning theory concepts related to the VC-dimension, in order to assess when learning is feasible. In the PAC framework, feasible means that we can say something on out-of-sample data in a probabilistic way. If one wants a deterministic statement about the feasibility of learning, than the ""no free lunch theorem"" says that, without prior assumptions, there's no way to generalize from a finite set of data to the general input space ( this is a mathematical proof of the Hume's induction problem). The rule of thumb which follows is something like ""we need a number of observations which is at least 10 times the VC dimension of the hypothesis set used to learn"". In case of linear models, this is bounded by the number of model parameters. Another use of this theory led to the design of the SVM classifier. I wonder if there are any other practical uses of learning theory and VC-dim when tackling a general statistical learning problem.",2,13
456,2017-1-16,2017,1,16,23,5oazbr,Up to 32x speedups on numpy/pandas with Weld,https://www.reddit.com/r/MachineLearning/comments/5oazbr/up_to_32x_speedups_on_numpypandas_with_weld/,[deleted],1484575418,[deleted],1,1
457,2017-1-16,2017,1,16,23,5ob5cz,State-of-the-art in reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/5ob5cz/stateoftheart_in_reinforcement_learning/,Delthc,1484577582,[removed],0,1
458,2017-1-16,2017,1,16,23,5ob5es,6 areas of artificial intelligence to watch closely,https://www.reddit.com/r/MachineLearning/comments/5ob5es/6_areas_of_artificial_intelligence_to_watch/,nb410,1484577600,,0,1
459,2017-1-16,2017,1,16,23,5ob7dx,[Discussion] Machine Learning - WAYR (What Are You Reading) - Week 17,https://www.reddit.com/r/MachineLearning/comments/5ob7dx/discussion_machine_learning_wayr_what_are_you/,Mandrathax,1484578300,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

|Previous weeks|
|--------------|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|
|[Week 7](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|
|[Week 8](https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/)|
|[Week 9](https://www.reddit.com/r/MachineLearning/comments/54kvsu/machine_learning_wayr_what_are_you_reading_week_9/)|
|[Week 10](https://www.reddit.com/r/MachineLearning/comments/56s2oa/discussion_machine_learning_wayr_what_are_you/)|
|[Week 11](https://www.reddit.com/r/MachineLearning/comments/57xw56/discussion_machine_learning_wayr_what_are_you/)|
|[Week 12](https://www.reddit.com/r/MachineLearning/comments/5acb1t/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 13](https://www.reddit.com/r/MachineLearning/comments/5cwfb6/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 14](https://www.reddit.com/r/MachineLearning/comments/5fc5mh/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 15](https://www.reddit.com/r/MachineLearning/comments/5hy4ur/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 16](https://www.reddit.com/r/MachineLearning/comments/5kd6vd/d_machine_learning_wayr_what_are_you_reading_week/)|

Most upvoted papers last week :

[Matrix Differential Calculus with Tensors (for Machine Learning)](https://github.com/mtomassoli/papers/blob/master/tensor_diff_calc.pdf) (Warning pdf)

Besides that, there are no rules, have fun.
",1,24
460,2017-1-17,2017,1,17,1,5obmty,"[R] Up to 32x speedups of numpy, pandas, spark, tensorflow with a common runtime (Weld)",https://www.reddit.com/r/MachineLearning/comments/5obmty/r_up_to_32x_speedups_of_numpy_pandas_spark/,halflings,1484583095,,16,59
461,2017-1-17,2017,1,17,1,5obnxn,[D] Multi sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/5obnxn/d_multi_sentiment_analysis/,mva_numbernine,1484583425,"Hi everybody,

I have a model that can predict the sentiment of a given sentence (or paragraph).

Now what i want is to input a document and the model has to cut the document into sequence of words each representing a sentiment i.e. i want to know what sequence of words of the document can be mapped to a sentiment.  Has anyone has an idea on how to perform that?  I tried googling but it was unsuccessful.

Thank you",2,0
462,2017-1-17,2017,1,17,1,5obwuj,"[Project] Keras ""Artistic Style Transfer"" Implementation",https://www.reddit.com/r/MachineLearning/comments/5obwuj/project_keras_artistic_style_transfer/,kevinzakka,1484585922,,4,18
463,2017-1-17,2017,1,17,2,5oc2ct,Good reading for learning about logistics regressions?,https://www.reddit.com/r/MachineLearning/comments/5oc2ct/good_reading_for_learning_about_logistics/,[deleted],1484587420,[removed],0,1
464,2017-1-17,2017,1,17,2,5oc7ly,Scaling Quality on Quora Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5oc7ly/scaling_quality_on_quora_using_machine_learning/,asyashaf,1484588797,,0,1
465,2017-1-17,2017,1,17,3,5ocf1a,Value Iteration Network in NeuPy,https://www.reddit.com/r/MachineLearning/comments/5ocf1a/value_iteration_network_in_neupy/,yurii-shevchuk,1484590670,,0,1
466,2017-1-17,2017,1,17,3,5ocnrc,Got VPS: is there a distro/script/container that can set me up for *NN/Deep machine learning ?,https://www.reddit.com/r/MachineLearning/comments/5ocnrc/got_vps_is_there_a_distroscriptcontainer_that_can/,ledewde_,1484592993,[removed],0,1
467,2017-1-17,2017,1,17,5,5od47p,[P] XLA: a domain-specific compiler for linear algebra that optimizes TensorFlow computations,https://www.reddit.com/r/MachineLearning/comments/5od47p/p_xla_a_domainspecific_compiler_for_linear/,downtownslim,1484597365,,2,22
468,2017-1-17,2017,1,17,6,5odgrq,"[N] neat-python (evolution of arbitrary neural networks) v0.9 release: big refactoring, easier customization (cross-post /r/Python)",https://www.reddit.com/r/MachineLearning/comments/5odgrq/n_neatpython_evolution_of_arbitrary_neural/,CodeReclaimers,1484600653,,2,5
469,2017-1-17,2017,1,17,6,5odmdu,How to train and do inference on a graphical model with evidence provided as a probability distribution?,https://www.reddit.com/r/MachineLearning/comments/5odmdu/how_to_train_and_do_inference_on_a_graphical/,amitgupta151,1484602145,[removed],0,1
470,2017-1-17,2017,1,17,7,5odu7n,Movie clustering from keywords without knowing the clusters,https://www.reddit.com/r/MachineLearning/comments/5odu7n/movie_clustering_from_keywords_without_knowing/,gabegabe6,1484604213,[removed],0,1
471,2017-1-17,2017,1,17,7,5odxna,A program removing everything but the objects it recognises when watching The Wolf of Wall Street,https://www.reddit.com/r/MachineLearning/comments/5odxna/a_program_removing_everything_but_the_objects_it/,AndreasRef,1484605141,,0,1
472,2017-1-17,2017,1,17,7,5oe4hh,Optimally adaptive transform coding: nonlinear version?,https://www.reddit.com/r/MachineLearning/comments/5oe4hh/optimally_adaptive_transform_coding_nonlinear/,[deleted],1484607036,[removed],0,1
473,2017-1-17,2017,1,17,8,5oehpg,41 Key Machine Learning Interview Questions with Answers,https://www.reddit.com/r/MachineLearning/comments/5oehpg/41_key_machine_learning_interview_questions_with/,[deleted],1484610800,[deleted],0,1
474,2017-1-17,2017,1,17,8,5oei3c,[Discussion] optimally adaptive transform coding: nonlinear version?,https://www.reddit.com/r/MachineLearning/comments/5oei3c/discussion_optimally_adaptive_transform_coding/,iridium04,1484610913,"Hello machine learning community, 

I'm looking for any kind of information (papers, links, etc.) about a nonlinear analogy to this ""optimally adaptive transform coding"" as described here: 

http://soma.mcmaster.ca/papers/Paper_34.pdf

I want to implement a neural network based autoencoder for image compression. Training should work like this: every image block vector is not only input for one but for K feedforward networks - each of them corresponds to a certain image block class. For every step the network, which results in the minimal error (MSE) is the only one that is updated. The effect is a selforganized training of encoding and classification at once. In the linked paper there is a description about a linear variant, using PCA. I'm sure, that a nonlinear version exists and has been implemented yet, but after a few days of intensive research on the internet I still can't find what I'm looking for... 

Thanks for any hints! 

Bye, Iridium04",1,3
475,2017-1-17,2017,1,17,8,5oei4c,[N] Reducing Google+ mobile data usage by 1/3 using NN upscaling on photos,https://www.reddit.com/r/MachineLearning/comments/5oei4c/n_reducing_google_mobile_data_usage_by_13_using/,gwern,1484610920,,36,104
476,2017-1-17,2017,1,17,9,5oej7r,41 Key Machine Learning Interview Questions with Answers,https://www.reddit.com/r/MachineLearning/comments/5oej7r/41_key_machine_learning_interview_questions_with/,[deleted],1484611230,[deleted],0,1
477,2017-1-17,2017,1,17,9,5oel49,[D] How would you encode time and gaps in sequence for LSTM?,https://www.reddit.com/r/MachineLearning/comments/5oel49/d_how_would_you_encode_time_and_gaps_in_sequence/,jacek_,1484611786,"I would like to use LSTM/GRU to create a generative model to generate probable sequence of events. My input data is similar to this:

    2000-01-03 Event 1
    2000-01-03 Event 2
    2000-01-04 Event 1
    2002-09-09 Event 1
    2002-09-09 Event 2
    2002-09-09 Event 3
    2002-09-09 Event 4
    2002-09-13 Event 3
    2002-09-14 Event 1
    2002-09-15 Event 2

The problem is events are not evenly spaced in time, there are often significant gaps in the sequence. I have two ideas so far:

1) append normalized time from the beginning (0) to each sample so the input is like this:

    2000-01-03 Event 1 = [1 0 0 0 *0 (first day)]
    2000-01-03 Event 2 = [0 1 0 0 *0 (first day)]
    2000-01-04 Event 1 = [1 0 0 0 *1 (second day)] 
    2002-09-09 Event 1 = [1 0 0 0 *249 (250th day)]
    ...

2) separate class for a gap and append length of the gap (append 0 to other events)

    2000-01-03 Event 1 = [1 0 0 0 *0 *0]
    2000-01-03 Event 2 = [0 1 0 0 *0 *0]
    1 day gap          = [0 0 0 0 *1 *1]
    2000-01-04 Event 1 = [1 0 0 0 *0 *0] 
    249 day gap        = [1 0 0 0 *1 *249]
    2000-09-09 Event 1 = [1 0 0 0 *0 *0]
    ...

What would you do? Have you ever encountered a similar problem? Do you know any good paper/source on the topic (I could not find anything)?
",13,20
478,2017-1-17,2017,1,17,9,5oeqo8,Dting-Online=Sx n the First Dat,https://www.reddit.com/r/MachineLearning/comments/5oeqo8/dtingonlinesx_n_the_first_dat/,Blakenehost,1484613394,[removed],0,1
479,2017-1-17,2017,1,17,11,5ofe6t,DyNet: The Dynamic Neural Network Toolkit,https://www.reddit.com/r/MachineLearning/comments/5ofe6t/dynet_the_dynamic_neural_network_toolkit/,handesy,1484620541,,0,2
480,2017-1-17,2017,1,17,11,5ofges,Prediction and real-time compensation of qubit decoherence via machine learning,https://www.reddit.com/r/MachineLearning/comments/5ofges/prediction_and_realtime_compensation_of_qubit/,[deleted],1484621245,[deleted],0,1
481,2017-1-17,2017,1,17,12,5ofsze,Implementation of Gated Conv Nets (https://arxiv.org/pdf/1612.08083v1.pdf),https://www.reddit.com/r/MachineLearning/comments/5ofsze/implementation_of_gated_conv_nets/,[deleted],1484625181,[deleted],0,1
482,2017-1-17,2017,1,17,13,5ofvq4,Implementation of Gated Conv Nets,https://www.reddit.com/r/MachineLearning/comments/5ofvq4/implementation_of_gated_conv_nets/,astanway,1484626076,,0,1
483,2017-1-17,2017,1,17,13,5ofwi2,[R] Prediction and real-time compensation of qubit decoherence via machine learning,https://www.reddit.com/r/MachineLearning/comments/5ofwi2/r_prediction_and_realtime_compensation_of_qubit/,KennySmash,1484626331,,1,19
484,2017-1-17,2017,1,17,13,5ofz3x,[R] Understanding the Effective Receptive Field in Deep Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5ofz3x/r_understanding_the_effective_receptive_field_in/,m_ke,1484627199,,3,10
485,2017-1-17,2017,1,17,13,5og1mf,"Call for Machine Learning apps, blog posts, and research papers.",https://www.reddit.com/r/MachineLearning/comments/5og1mf/call_for_machine_learning_apps_blog_posts_and/,kumarovski,1484628074,[removed],0,1
486,2017-1-17,2017,1,17,14,5og919,NLP datasets used for benchmarking accuracy?,https://www.reddit.com/r/MachineLearning/comments/5og919/nlp_datasets_used_for_benchmarking_accuracy/,ZeppelinYanks,1484630754,[removed],0,1
487,2017-1-17,2017,1,17,14,5ogb9w,optimal algorithm,https://www.reddit.com/r/MachineLearning/comments/5ogb9w/optimal_algorithm/,postypost69,1484631567,[removed],0,1
488,2017-1-17,2017,1,17,14,5ogbd5,[D] Training LSTMs in practice: tips and tricks?,https://www.reddit.com/r/MachineLearning/comments/5ogbd5/d_training_lstms_in_practice_tips_and_tricks/,to4life2,1484631605,"I haven't found too much info floating around about the actual, specific mechanics of how LSTM training is done. Mostly I see people forming a model with a high-level library e.g. Keras/TF/other model, and calling model.train(sequence_length) or whatever the equivalent is. To me this leaves a few important questions open with respect to how to train LSTMs:

1) How should we iterate through our data while training? Sequentially in time, 1 step (word, video frame, few ms of audio) at a time?

2) Statefulness of the LSTM: the unrolled LSTM cells have internal memory as well as temporal states. During training, we'll typically start it at 0 from what I understand, but when should we reset it? E.g. after iterating through the entire dataset (e.g. entire video or audio sequence)?

3) When testing the network live on new novel inputs, should we seed the LSTM memory cells with any particular values - e.g. something saved during training time - to better ""kickstart"" the prediction activity?

---

To provide more to the discussion, I'm going to outline an example problem I'm working on, some of the approaches I'm taking, and I'd be happy to receive suggestions, as well as discuss general training techniques for LSTMs. For reference, as a learning exercise I coded my own LSTM in TensorFlow, following the standard literature. In the problem explored below, I'm using convolutional LSTMs, as per this paper: 

https://arxiv.org/pdf/1506.04214v2.pdf

**Problem statement:** the input is a video sequence X(:) with frames t going from 1 to N. The framerate is F (in fps) so the video is of length N/F seconds. In my case the toy problem is very small (N = 600, F = 15, for a 40 second video). 

**Goal:** Predict frame X(t+1) from X(t), Xp(t+1) = Network(X(t)). Actually output an image frame. 

Ideally we want to be able to feed a frame X(t), get its prediction Xp(t+1), and feed that back in as input to get a prediction for frame t+2, Xp(t+2) = Network(Xp(t+1)), etc. The better our network can learn to predict the video sequence, the further into the future it will be able to predict. 

**Architecture:** as below - 

vid_frame_tensor(H,W,3) --&gt; CONV_8 --&gt; CONV_16 --&gt; CONV_32 --&gt; CONV_LSTM_32 --&gt; CONV_16 --&gt; CONV_8 --&gt; CONV_3 == next_frame_prediction(H,W,3)

So it's an encoder --&gt; LSTM --&gt; decoder sort of architecture. This network tries to learn visual features and temporal relations simultaneously, as I've heard suggested before. Here are the network details: 

* First of all, everything in small data-wise as I started training on my laptop..
* CONV_M means there are M output filters for that level. I used 5x5 strides for every conv layer (including in the convolutional LSTM). ReLU activations, biases, etc as standard.
* ^ What I'm not doing is using any pooling/downsampling layers. 
* The video: 600 frames (at 15 FPS), with H(eight)=W(idth)=112 pixels; 3-channel RGB
* The network is unrolled (in time) for 20 timesteps, or 1.5 seconds. 
* Loss function is binary crossentropy on all the pixels, b/w Xp(t+1) and the actual X(t+1). The network sums the losses for all the timesteps, i.e. summing loss(Xp(t+1+j), X(t+1+j)) for the j timesteps that are unrolled.
* Optimizer: ADAM, initial learning rate of 0.0005, every often multiplied by 0.9 to schedule a LR slowdown. 
* Weight initialization: the standard Normal(0,1) / sqrt(fan in) that seems to work decently in practice. 
* Training regimen: loop through the video sequentially, advancing pointer t by 1, so at first we train with video frames X(1:21), then X(2:22), ..., X(N-20:N). Then go back to the beginning and repeat. When the training loop gets back to the beginning of the video, the LSTM state variables (internal and hidden state) are reset to zero. This was my first logical idea for how to train the network, based on Karpathy's simple RNN example below:

https://gist.github.com/karpathy/d4dee566867f8291f086

---

Anyways I thought my small project might be an interesting case to seed the discussion. Happy to hear your thoughts, and read links, on anything to do with LSTM training, or suggestions for the network I'm trying to train. I'm a bit of an amateur so I don't have the best intuition yet for various hyperparameters and architectures that would give me a better chance of succeeding at this example task. ",12,13
489,2017-1-17,2017,1,17,15,5ogmva,[D] Which information do you want in a paper about a dataset?,https://www.reddit.com/r/MachineLearning/comments/5ogmva/d_which_information_do_you_want_in_a_paper_about/,themoosemind,1484636299,"I am going to publish a dataset which is similar to MNIST, but with much more classes (&gt; 300). I want to publish a short paper in arXiv which describes the dataset and can be cited if people use it.

What kind of information would you expect / want in such a publication?

I thought of the following:

1. Obtaining the data:
  1. Where it can be downloaded (URL)
  2. Size of file, format of file (see [StackExchange question](http://stackoverflow.com/q/41690728/562769))
  3. MD5 sum to verify
  4. Structure of file
  5. Where labels are and in which format
  6. License (ODbL)
2. Classes: The total number of classes, groups within the classes (e.g. digits, latin uppercase and lowercase characters, miscallenious math characters, arrows, ...), a complete list in the appendix
3. Data:
  1. How data was collected
  2. Format: 32px x 32px, grayscale images (with some example images being shown in the paper)
  3. Training data:
      - Total number (&gt; 150,000)
      - Distribution among classes (class with least data, class with most data, median, an image visualizing the distribution)
  4. Total number of test data items (&gt; 15,000)
4. Baseline: Create a simple classifier, e.g. 3 layer ~~MLP~~ (edit: I wanted to write CNN - something like [this](https://www.tensorflow.org/tutorials/mnist/pros/) which works extremely well on MNIST but is structurally/conceptually quite simple) with dropout. Report accuracy, analyze errors.

Is there anything else?

",18,25
490,2017-1-17,2017,1,17,16,5ogokg,R code for crossvalidation.How to avoid overfitting,https://www.reddit.com/r/MachineLearning/comments/5ogokg/r_code_for_crossvalidationhow_to_avoid_overfitting/,A_D_Exploration,1484637066,,0,1
491,2017-1-17,2017,1,17,18,5oh5l7,[D] Paper posted on r/technology about ML out performing doctors in predicting heart disorder patient survival chances?,https://www.reddit.com/r/MachineLearning/comments/5oh5l7/d_paper_posted_on_rtechnology_about_ml_out/,daskinesis,1484645471,"Saw this article posted on r/technology about ML out performing doctors in predicting heart disorder patient survival chances: http://www.bbc.com/news/health-38635871

Wanted to find the paper, but BBC only linked to the journal Radiology. Does anyone know which paper it was referring to and perhaps a link to it?",8,11
492,2017-1-17,2017,1,17,22,5oi4p2,Data Mining in Python: A Guide,https://www.reddit.com/r/MachineLearning/comments/5oi4p2/data_mining_in_python_a_guide/,[deleted],1484660769,[deleted],0,1
493,2017-1-17,2017,1,17,22,5oi50g,The Data Science Process: What a data scientist actually does day-to-day,https://www.reddit.com/r/MachineLearning/comments/5oi50g/the_data_science_process_what_a_data_scientist/,TheHuntIsOnRN,1484660873,,0,1
494,2017-1-17,2017,1,17,23,5oia73,[Discussion] Quality of university level machine learning courses,https://www.reddit.com/r/MachineLearning/comments/5oia73/discussion_quality_of_university_level_machine/,[deleted],1484662675,[deleted],14,6
495,2017-1-18,2017,1,18,0,5oiidp,"The ""Synthetic Sensor Input"" problem in AV (and ML) verification",https://www.reddit.com/r/MachineLearning/comments/5oiidp/the_synthetic_sensor_input_problem_in_av_and_ml/,yoav_hollander,1484665320,,0,5
496,2017-1-18,2017,1,18,0,5oiii8,An Introduction to AlphaGo Tech Talk,https://www.reddit.com/r/MachineLearning/comments/5oiii8/an_introduction_to_alphago_tech_talk/,DrLegend,1484665350,,0,1
497,2017-1-18,2017,1,18,1,5oiuho,A flaw with convolutional neural networks?,https://www.reddit.com/r/MachineLearning/comments/5oiuho/a_flaw_with_convolutional_neural_networks/,alephnaught90,1484668866,[removed],1,1
498,2017-1-18,2017,1,18,1,5oix89,[N] Zinkevich's write-up on best practices for ML engineering in the real-world,https://www.reddit.com/r/MachineLearning/comments/5oix89/n_zinkevichs_writeup_on_best_practices_for_ml/,carlthome,1484669657,,7,132
499,2017-1-18,2017,1,18,1,5oiziu,AMD or Intel Processor,https://www.reddit.com/r/MachineLearning/comments/5oiziu/amd_or_intel_processor/,[deleted],1484670327,[removed],0,1
500,2017-1-18,2017,1,18,1,5oj6s1,[Discussion] Mathematics of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5oj6s1/discussion_mathematics_of_machine_learning/,mkmeral,1484672282,I'm IB Student (high school diploma) And I will be writing an extended essay on a topic that I will choose. I want to learn while I am writing the essay. So here is my question. What is the most important mathematical concept about machine learning or computer science? ,9,0
501,2017-1-18,2017,1,18,3,5ojlq8,tf-idf implementation in Python for a non Data Scientist?,https://www.reddit.com/r/MachineLearning/comments/5ojlq8/tfidf_implementation_in_python_for_a_non_data/,hamburglin,1484676258,[removed],0,1
502,2017-1-18,2017,1,18,3,5ojnj5,"[D] Question: Best Texts for Statistics, Probability, and Multivariate Calculus for machine learning.",https://www.reddit.com/r/MachineLearning/comments/5ojnj5/d_question_best_texts_for_statistics_probability/,thewoosterisroot,1484676700,"I've been taking Andrew Ng's course on Machine learning, and am now trying to acquire the necessary background knowledge for the course. My background is largely self-taught, I only went as far as pre-calc in college (long story).  I plan on going back for a masters, but for now, I need some good texts.  I found some good courses in Khan Academy, coursera, as well as in the FAQ, but I always do best with a good text.

From my searching so far it seems I need to brush up on the following first:

* Statistics
* Linear Algebra
* Multivariate Calculus

Does anyone have suggestions for the best books to start with on these? I am primarily looking for a good Statistics book first. I did start ""The Elements of Statistical Learning"" linked in the FAQ, but I'm not certain if this book is enough on its own as I do not have a background in statistics.",9,10
503,2017-1-18,2017,1,18,3,5ojujl,[D] Implementing tf-idf in Python for a non Data Scientist.,https://www.reddit.com/r/MachineLearning/comments/5ojujl/d_implementing_tfidf_in_python_for_a_non_data/,hamburglin,1484678579,"Are there any beginner guides for implementing tf-ldf in Python? Hand holding as much as possible?

I script on the side for my technical related career and I have a new pet project I'm working on. I want to use tf-ldf to bubble up say, the top 100 unique/important terms from many different files. 

Understanding the math behind it would be a stretch, but I'll be able to follow a guide, copy or reverse engineer some code.

Can anyone guide me in the right direction?",13,0
504,2017-1-18,2017,1,18,3,5ojwpp,[N] DyNet: The Dynamic Neural Network Toolkit,https://www.reddit.com/r/MachineLearning/comments/5ojwpp/n_dynet_the_dynamic_neural_network_toolkit/,LLCoolZ,1484679168,,13,33
505,2017-1-18,2017,1,18,4,5ojyls,ELI5: How would the separation boundary learnt by a tanh neuron and a sigmoid neuron differ?,https://www.reddit.com/r/MachineLearning/comments/5ojyls/eli5_how_would_the_separation_boundary_learnt_by/,jesacob,1484679668,[removed],0,1
506,2017-1-18,2017,1,18,4,5ok5fo,[P] Generating Abstract Art using a Random Weights Neural Net (Reproducing Otoro.net in NumPy),https://www.reddit.com/r/MachineLearning/comments/5ok5fo/p_generating_abstract_art_using_a_random_weights/,liviu-,1484681526,,5,8
507,2017-1-18,2017,1,18,4,5ok5mt,Hyperparameter optimization methods: Which SMBO is best?,https://www.reddit.com/r/MachineLearning/comments/5ok5mt/hyperparameter_optimization_methods_which_smbo_is/,RoamBear,1484681576,[removed],0,1
508,2017-1-18,2017,1,18,4,5ok5qi,Lane following RC car autopilot using Keras &amp; Tensorflow [P],https://www.reddit.com/r/MachineLearning/comments/5ok5qi/lane_following_rc_car_autopilot_using_keras/,wroscoe,1484681601,,0,11
509,2017-1-18,2017,1,18,5,5okfm5,[N] MinPy 0.30 Release - Imperative NumPy style code with MXNet backend,https://www.reddit.com/r/MachineLearning/comments/5okfm5/n_minpy_030_release_imperative_numpy_style_code/,wei_jok,1484684230,,3,37
510,2017-1-18,2017,1,18,5,5okojv,[D] Question: Has anyone tried to use image input for deep deterministic policy gradient(DDPG) playing Torcs?,https://www.reddit.com/r/MachineLearning/comments/5okojv/d_question_has_anyone_tried_to_use_image_input/,HanyuGuo,1484686610,"I find a pretty good repo online: https://github.com/yanpanlau/DDPG-Keras-Torcs. I was trying to subsitute low-dimensional feature with image input, but it does not work quite well. I found nobody tried this before on the whole Internet except for the author of A3C released a video: https://www.youtube.com/watch?v=0xo1Ldx3L5Q

My question is that is there a specific reason making implementing DDPG playing torcs using image input so hard?",1,2
511,2017-1-18,2017,1,18,5,5okp3z,I'll build you a ML box!,https://www.reddit.com/r/MachineLearning/comments/5okp3z/ill_build_you_a_ml_box/,[deleted],1484686763,[removed],0,1
512,2017-1-18,2017,1,18,6,5okrp6,I'll build you a ML box!,https://www.reddit.com/r/MachineLearning/comments/5okrp6/ill_build_you_a_ml_box/,branmcf,1484687442,[removed],0,1
513,2017-1-18,2017,1,18,7,5ol7od,[D] Who is using Machine Learning and AI to do cool things in their own lives and for the world around them?,https://www.reddit.com/r/MachineLearning/comments/5ol7od/d_who_is_using_machine_learning_and_ai_to_do_cool/,breadteam,1484691564,How you are using ML/AI to help the world around you and what is the practical application? Can you describe your project and what/who it is helping?,65,44
514,2017-1-18,2017,1,18,7,5olcnw,Googles AlphaGo AI Runs the Table on Asias Go Champs,https://www.reddit.com/r/MachineLearning/comments/5olcnw/googles_alphago_ai_runs_the_table_on_asias_go/,inimitableyogi,1484692932,,0,1
515,2017-1-18,2017,1,18,8,5olofq,Ideas on applying ML techniques on meetup data?,https://www.reddit.com/r/MachineLearning/comments/5olofq/ideas_on_applying_ml_techniques_on_meetup_data/,chai_17,1484696205,[removed],0,1
516,2017-1-18,2017,1,18,9,5olwep,[D] Engineering is the bottleneck in (Deep Learning) Research,https://www.reddit.com/r/MachineLearning/comments/5olwep/d_engineering_is_the_bottleneck_in_deep_learning/,evc123,1484698449,,42,111
517,2017-1-18,2017,1,18,10,5om8eq,[FR JOIN] 100% Fre-Sex-Dting-Wbsite. Our girls are lking for just fre-s. Members r from all vr th world. Join us today. Don't wast any mre tim loking for fre-dting-sits.,https://www.reddit.com/r/MachineLearning/comments/5om8eq/fr_join_100_fresexdtingwbsite_our_girls_are/,Isaiahemnai,1484701941,[removed],0,1
518,2017-1-18,2017,1,18,10,5oma32,[R] [1701.04489] Towards a New Interpretation of Separable Convolutions,https://www.reddit.com/r/MachineLearning/comments/5oma32/r_170104489_towards_a_new_interpretation_of/,[deleted],1484702463,[deleted],3,0
519,2017-1-18,2017,1,18,11,5omifh,First games (+commentary) from the Ro16 of the StarCraft AI tournament this Wednesday (18 Jan) 20:00 CET on http://twitch.tv/certicky,https://www.reddit.com/r/MachineLearning/comments/5omifh/first_games_commentary_from_the_ro16_of_the/,[deleted],1484704947,[deleted],0,1
520,2017-1-18,2017,1,18,11,5omiqh,[N]First games (+commentary) from the Ro16 of the StarCraft AI tournament this Wednesday (18 Jan) 20:00 CET on http://twitch.tv/certicky,https://www.reddit.com/r/MachineLearning/comments/5omiqh/nfirst_games_commentary_from_the_ro16_of_the/,LetaBot,1484705036,,1,1
521,2017-1-18,2017,1,18,13,5on8qy,AI can predict when patients will die from heart failure 'with 80% accuracy',https://www.reddit.com/r/MachineLearning/comments/5on8qy/ai_can_predict_when_patients_will_die_from_heart/,jvdalen,1484713421,,0,1
522,2017-1-18,2017,1,18,13,5one27,[D] Under what conditions will GPUs speed up LSTM/GRU training?,https://www.reddit.com/r/MachineLearning/comments/5one27/d_under_what_conditions_will_gpus_speed_up/,pianomano8,1484715323,"tl;dr: what shapes and characteristics of RNNs get good training performance on GPUs?  

I'm running some experiments with Keras (1.2.0) and Theano (0.8 stable from pip) on sequence to sequence learning using RNNs. I'm currently training on an input dataset of shape (20000, 1000, 12) and output dataset of (20000, 1000, 13), and I'm experimenting with networks of up to 20 layers of GRUs and/or LSTMs, with the internal layers having 14 nodes. In Keras, each layer is defined with consume_less='gpu', and I'm using small batch sizes (as low as 1, as high as 20). 

I naively hoped I'd be able to train a lot faster on my GPU (GTX 1060) than my CPU (which seems single threaded, despite me passing OMP_NUM_THREADS=4 and using openblas), but GPU processing seems to take quite a bit longer.  nvidia-smi shows ~60% utilization, so I know it's using it, but it appears the the overhead of moving data/back and forth to the GPU is a huge net loss in my case.  Or, perhaps Keras is not being optimal and I should suck it up and learn raw Theano. 

I know a few things I've done wrong already, such as I should be using a batch size of a multiple of 32. But I also read somewhere that you're supposed to have really wide layers (200+ nodes/layer) to make GPU training worth it.  Perhaps my deep-ish but ""skinny"" network is just not a good match for the GPU, despite the ~13K trainable parameters (for a 10 layer network) and 1000-long input sequences. 

So, my question is, in general, what shapes and characteristics of RNNs get good training performance on GPUs?  ",13,4
523,2017-1-18,2017,1,18,13,5onebv,Point me in the right direction: Network analysis to rank factor levels in data set?,https://www.reddit.com/r/MachineLearning/comments/5onebv/point_me_in_the_right_direction_network_analysis/,bprs07,1484715429,[removed],0,1
524,2017-1-18,2017,1,18,16,5onyhz,Importance Of Lubrication Pumps In Bearings,https://www.reddit.com/r/MachineLearning/comments/5onyhz/importance_of_lubrication_pumps_in_bearings/,jackerfrinandis,1484723351,,0,1
525,2017-1-18,2017,1,18,17,5oo9mi,For Data Science or Data Analysis beginners.Fitting &amp; predicting by Logistic regression in python,https://www.reddit.com/r/MachineLearning/comments/5oo9mi/for_data_science_or_data_analysis/,[deleted],1484728665,[deleted],0,1
526,2017-1-18,2017,1,18,18,5ooefw,[P] DeepWater: A convolutional neural network which identifies water in a satellite image.,https://www.reddit.com/r/MachineLearning/comments/5ooefw/p_deepwater_a_convolutional_neural_network_which/,[deleted],1484731100,[deleted],1,0
527,2017-1-18,2017,1,18,18,5oogjt,AB tests &amp; statistical significance: short discussion with online calculator,https://www.reddit.com/r/MachineLearning/comments/5oogjt/ab_tests_statistical_significance_short/,Ranlot,1484732190,,0,1
528,2017-1-18,2017,1,18,19,5ookxb,DeepTraffic: build a neural agent that performs well on high traffic roads,https://www.reddit.com/r/MachineLearning/comments/5ookxb/deeptraffic_build_a_neural_agent_that_performs/,cavedave,1484734369,,2,16
529,2017-1-18,2017,1,18,19,5oon3l,Best courses for Data science and Machine Learning beginners from top universities,https://www.reddit.com/r/MachineLearning/comments/5oon3l/best_courses_for_data_science_and_machine/,[deleted],1484735473,[deleted],0,1
530,2017-1-18,2017,1,18,19,5oon6q,13 Free Training Courses on Machine Learning and Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/5oon6q/13_free_training_courses_on_machine_learning_and/,NarendhiranS,1484735513,,0,2
531,2017-1-18,2017,1,18,20,5ooufq,Talk about RNNs by Yoshua Bengio (2016),https://www.reddit.com/r/MachineLearning/comments/5ooufq/talk_about_rnns_by_yoshua_bengio_2016/,[deleted],1484738902,[deleted],0,1
532,2017-1-18,2017,1,18,20,5oovyj,[D] Talk about RNNs by Yoshua Bengio (2016),https://www.reddit.com/r/MachineLearning/comments/5oovyj/d_talk_about_rnns_by_yoshua_bengio_2016/,r_dipietro,1484739627,,7,39
533,2017-1-18,2017,1,18,21,5op0z0,"My ""chaRF"" makes Chinese poem and passed ""Turing test""?",https://www.reddit.com/r/MachineLearning/comments/5op0z0/my_charf_makes_chinese_poem_and_passed_turing_test/,godspeed_china,1484741860,[removed],0,1
534,2017-1-18,2017,1,18,21,5op1b4,"[D] chaRF made Chinese Tang Poem that passes ""Turing Test"" ?",https://www.reddit.com/r/MachineLearning/comments/5op1b4/d_charf_made_chinese_tang_poem_that_passes_turing/,godspeed_china,1484742012,"Previously, I made a chaRF model that perform character level language model using random forest. I trained it on Chinese Tang poem dataset. Today I made a ""Turing test"": AI generate 30 lines of poem. I pick out one ""good"" line out of every 3 lines. Thus computer (slightly aided by me ) generate 10 lines of random poem. The negative set was 10 randomly chosen true poem lines from the literature. I randomly mixed/shuffle these 20 lines to create a dataset. The human testing task was to pick out those 10 computer made lines. The predefined ""PASS"" criterion is &lt;=7 success.
I test several colleges and have the following success number:
5,5,6,6,6,7,7,7,8,8,8,9,9
one of the ""9 success"" is achieved by a professional poets. 60% human is fooled by the AI, and even professional poets fail to identify all AI lines.
Personally I am satisfied with the result. What is your opinion?",4,0
535,2017-1-18,2017,1,18,23,5ophio,[P] WaterNet: A convolutional neural network that identifies water in satellite images.,https://www.reddit.com/r/MachineLearning/comments/5ophio/p_waternet_a_convolutional_neural_network_that/,treigerm,1484748046,,22,62
536,2017-1-18,2017,1,18,23,5oppr2,Python 3 solutions to Andrew Ngs ML course on coursera,https://www.reddit.com/r/MachineLearning/comments/5oppr2/python_3_solutions_to_andrew_ngs_ml_course_on/,id5h,1484750691,[removed],0,1
537,2017-1-18,2017,1,18,23,5opq6d,The Current State of Automated Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5opq6d/the_current_state_of_automated_machine_learning/,rhiever,1484750828,,0,1
538,2017-1-19,2017,1,19,0,5oq0z8,[N] This app uses AI to detect objects and read them out loud to help the visually impaired and it's pretty damn accurate.,https://www.reddit.com/r/MachineLearning/comments/5oq0z8/n_this_app_uses_ai_to_detect_objects_and_read/,[deleted],1484753971,[deleted],0,1
539,2017-1-19,2017,1,19,0,5oq1y0,[P] Pointer Network in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5oq1y0/p_pointer_network_in_tensorflow/,carpedm20,1484754245,,1,21
540,2017-1-19,2017,1,19,0,5oq3ug,"Simple Questions Thread January 18, 2017",https://www.reddit.com/r/MachineLearning/comments/5oq3ug/simple_questions_thread_january_18_2017/,AutoModerator,1484754757,[removed],0,1
541,2017-1-19,2017,1,19,0,5oq44n,Is the complexity of Google's search ranking algorithms increasing or decreasing over time?,https://www.reddit.com/r/MachineLearning/comments/5oq44n/is_the_complexity_of_googles_search_ranking/,nikhilbd,1484754827,,0,1
542,2017-1-19,2017,1,19,1,5oq7d8,[Discussion] Gabe Newell mentions ML in his AMA. What might this mean?,https://www.reddit.com/r/MachineLearning/comments/5oq7d8/discussion_gabe_newell_mentions_ml_in_his_ama/,dementiapatient567,1484755719,"Much more narrowly, some of us are thinking about some of the AI work that is being hyped right now. Simplistically we have lots of data and compute capability that looks like the kinds of areas where machine learning should work well.


Better automated support tickets? Surely he's not talking about ML being used for games.",9,0
543,2017-1-19,2017,1,19,1,5oqb5r,[D] Any other summer 2017 DeepMind/London interns interested in coordinating housing?,https://www.reddit.com/r/MachineLearning/comments/5oqb5r/d_any_other_summer_2017_deepmindlondon_interns/,tmp19538,1484756771,[removed],0,2
544,2017-1-19,2017,1,19,2,5oqkpe,[R] Dockerized version of the StackGAN code implementation,https://www.reddit.com/r/MachineLearning/comments/5oqkpe/r_dockerized_version_of_the_stackgan_code/,brannondorsey,1484759317,,24,7
545,2017-1-19,2017,1,19,2,5oqpcx,Facebook Open Sources PyTorch,https://www.reddit.com/r/MachineLearning/comments/5oqpcx/facebook_open_sources_pytorch/,iamtrask,1484760546,,0,2
546,2017-1-19,2017,1,19,2,5oqu8x,Tensors and Dynamic Neural Networks in Python with Strong GPU Acceleration,https://www.reddit.com/r/MachineLearning/comments/5oqu8x/tensors_and_dynamic_neural_networks_in_python/,[deleted],1484761786,[deleted],0,1
547,2017-1-19,2017,1,19,3,5oqynv,[D] Autoencoder Advanced Error Metrics,https://www.reddit.com/r/MachineLearning/comments/5oqynv/d_autoencoder_advanced_error_metrics/,aiapplicant,1484762926,"So, I've seen a lot of autoencoders recently being used for unsupervised learning. ""Towards Deep Symbolic Reinforcement Learning"" for example, uses them to detect and then describe salient objects in video. However, I've thought for awhile now we need something more. In any gradient decent search, there are generally many local minima, and you happen to stumble into one description you hope is the global minima. What if I want my autoencoder to compress my video, but in such a way as to make salient some particular feature, or differentiate one feature from another most of all. For instance, in some games, there are enemies that look the same, but act differently. In the autoencoder process, then, I want a loss that describes the difference in IO, *and* another loss that describes the relevance of the compressed description to my task. Basically, I want to be able to eliminate some minima in favor of others, relative to some other metric in my system, in such a way as it can all backpropagate together. I know this is often called regularization, is there any research being done on adding this additional constraint to autoencoders?",8,6
548,2017-1-19,2017,1,19,3,5or0li,few years late but hallelujah,https://www.reddit.com/r/MachineLearning/comments/5or0li/few_years_late_but_hallelujah/,[deleted],1484763421,[deleted],0,1
549,2017-1-19,2017,1,19,3,5or0te,[N] The team behind Torch7 opensources a pytorch,https://www.reddit.com/r/MachineLearning/comments/5or0te/n_the_team_behind_torch7_opensources_a_pytorch/,juliusScissors,1484763463,,54,178
550,2017-1-19,2017,1,19,4,5orefy,[N] This app uses AI to detect objects and read them out loud to help the visually impaired and it's pretty damn accurate.,https://www.reddit.com/r/MachineLearning/comments/5orefy/n_this_app_uses_ai_to_detect_objects_and_read/,[deleted],1484766987,[deleted],0,1
551,2017-1-19,2017,1,19,4,5orgvf,How to Do Linear Regression the Right Way [LIVE],https://www.reddit.com/r/MachineLearning/comments/5orgvf/how_to_do_linear_regression_the_right_way_live/,llSourcell,1484767602,,0,1
552,2017-1-19,2017,1,19,4,5orhj1,[N] Came across this app that uses Machine Learning to identify objects in real-time to help the blind,https://www.reddit.com/r/MachineLearning/comments/5orhj1/n_came_across_this_app_that_uses_machine_learning/,aasemjs,1484767773,,4,19
553,2017-1-19,2017,1,19,5,5ort3u,"Analysing 8 weeks of AI news, research, VC deals and M&amp;A",https://www.reddit.com/r/MachineLearning/comments/5ort3u/analysing_8_weeks_of_ai_news_research_vc_deals/,nb410,1484770802,,0,1
554,2017-1-19,2017,1,19,5,5orvf5,"Collaborative Platform: Unlock lessons in practical NLP techniques by labeling data for active researchers at edX. Like FoldIt and Captchas, but with text samples and tutorials. [Research][Project]",https://www.reddit.com/r/MachineLearning/comments/5orvf5/collaborative_platform_unlock_lessons_in/,crazylikeajellyfish,1484771422,,0,2
555,2017-1-19,2017,1,19,5,5oryop,[D] Looking for... things such as entropy that can be applied to strings.,https://www.reddit.com/r/MachineLearning/comments/5oryop/d_looking_for_things_such_as_entropy_that_can_be/,hamburglin,1484772273,"I'm using a Shannon entropy function to enhance the context of a set of strings I have, and then sorting on that number. I'm left wondering what other functions/algorithms/scores I can also calculate on a string unrelated to entropy.

Sadly, I do not know what keywords to search for as this is not my field. Do more of these functions exist? Can anyone guide me to a list or name a few if so?

Also, I didn't know what subreddit to post this in.",6,2
556,2017-1-19,2017,1,19,5,5orzuc,Is anyone else here taking Udacity's Deep Learning NanoDegree course??,https://www.reddit.com/r/MachineLearning/comments/5orzuc/is_anyone_else_here_taking_udacitys_deep_learning/,BlueFolliage,1484772574,[removed],0,1
557,2017-1-19,2017,1,19,6,5os37a,[P] Processing Agents,https://www.reddit.com/r/MachineLearning/comments/5os37a/p_processing_agents/,inboble,1484773441,,6,0
558,2017-1-19,2017,1,19,6,5osdcn,Deep Reinforcement Learning Tech Talk,https://www.reddit.com/r/MachineLearning/comments/5osdcn/deep_reinforcement_learning_tech_talk/,DrLegend,1484776067,,0,1
559,2017-1-19,2017,1,19,6,5ose12,[Project] On Spatial Transformer Networks Part II,https://www.reddit.com/r/MachineLearning/comments/5ose12/project_on_spatial_transformer_networks_part_ii/,kevinzakka,1484776258,,0,5
560,2017-1-19,2017,1,19,7,5osgh6,[P] stared/keras-sequential-ascii: ASCII summary for simple sequential models in Keras,https://www.reddit.com/r/MachineLearning/comments/5osgh6/p_staredkerassequentialascii_ascii_summary_for/,pmigdal,1484776916,,10,8
561,2017-1-19,2017,1,19,7,5osqb4,arxivML: An Alexa skill to read latest machine learning papers from arXiv,https://www.reddit.com/r/MachineLearning/comments/5osqb4/arxivml_an_alexa_skill_to_read_latest_machine/,vonnik,1484779585,,0,1
562,2017-1-19,2017,1,19,9,5otbpz,Keras vs PyTorch ?,https://www.reddit.com/r/MachineLearning/comments/5otbpz/keras_vs_pytorch/,jm-mp,1484785825,[removed],0,1
563,2017-1-19,2017,1,19,9,5otdwl,[D] TensorFlow vs. PyTorch: why is dynamic better?,https://www.reddit.com/r/MachineLearning/comments/5otdwl/d_tensorflow_vs_pytorch_why_is_dynamic_better/,whoeverwhatever,1484786504,"There's been a lot of talk about PyTorch today, and the growing number of ""dynamic"" DL libraries that have come up in the last few weeks/months (Chainer, MinPy, DyNet, I'm sure I'm missing some others). Why the sudden interest in this model over something like TensorFlow, which constructs a graph first and then executes?

Dynamic libraries provide an obvious advantage in that they are easier to debug and reason about. Are there any models which are impossible/more complicated to construct with the TensorFlow system compared to a dynamic system? What's changed in the last few months to enable the growing (renewed?) interest in this kind of framework?",50,74
564,2017-1-19,2017,1,19,11,5otxmd,Incremental Learning for Robot Perception through HRI,https://www.reddit.com/r/MachineLearning/comments/5otxmd/incremental_learning_for_robot_perception_through/,[deleted],1484792743,[deleted],0,1
565,2017-1-19,2017,1,19,11,5ou098,[R] Guiding principle to build Multilayer Perceptron,https://www.reddit.com/r/MachineLearning/comments/5ou098/r_guiding_principle_to_build_multilayer_perceptron/,[deleted],1484793578,[deleted],1,1
566,2017-1-19,2017,1,19,11,5ou0f0,arxivML: An Alexa skill to read latest machine learning papers from arXiv,https://www.reddit.com/r/MachineLearning/comments/5ou0f0/arxivml_an_alexa_skill_to_read_latest_machine/,[deleted],1484793632,[deleted],0,1
567,2017-1-19,2017,1,19,12,5ou618,What's it like finding an entry level job in ML right now with a BS and an online course?,https://www.reddit.com/r/MachineLearning/comments/5ou618/whats_it_like_finding_an_entry_level_job_in_ml/,[deleted],1484795450,[removed],0,1
568,2017-1-19,2017,1,19,12,5ou9as,[D] what is the state of the art in generating questions from passages?,https://www.reddit.com/r/MachineLearning/comments/5ou9as/d_what_is_the_state_of_the_art_in_generating/,redditAccount3432,1484796545,"here's the best paper I've found so far.

https://arxiv.org/pdf/1611.03932v1.pdf

Thanks!!",5,4
569,2017-1-19,2017,1,19,12,5oudtg,[D] How do I deal with getting overwhelmed with Math and Algorithms with respect to ML?,https://www.reddit.com/r/MachineLearning/comments/5oudtg/d_how_do_i_deal_with_getting_overwhelmed_with/,vayarajesh,1484798099,"I have been trying to get actively into Machine learning but every time I feel like I have to restart learning with Math and Algorithms. I mainly struggle with Math at the moment as I only know basic Linear Algebra.

How do I get along with Machine Learning without getting discouraged? Every blog post I read says that I don't need to be familiar advanced Math to get started with Machine learning but in the same blog post they mention complex formulae which I don't understand (or at least I feel it is advance Math).

Whenever I start to learn Math, I get lost as to where to start and how much of Math is enough Math to finally dive into ML?

Any suggestions?",15,1
570,2017-1-19,2017,1,19,13,5ouk1h,Enhance The Potential Of Your Machinery With Bijur Delimon Lubricants!,https://www.reddit.com/r/MachineLearning/comments/5ouk1h/enhance_the_potential_of_your_machinery_with/,jackerfrinandis,1484800318,,0,1
571,2017-1-19,2017,1,19,14,5ouv2u,Bayesian probability bias on uneven training set?,https://www.reddit.com/r/MachineLearning/comments/5ouv2u/bayesian_probability_bias_on_uneven_training_set/,rodrigo-silveira,1484804470,,0,1
572,2017-1-19,2017,1,19,16,5ovbme,[D] How to correctly share weights in torch ?,https://www.reddit.com/r/MachineLearning/comments/5ovbme/d_how_to_correctly_share_weights_in_torch/,vighneshbirodkar,1484811635,"I am looking for examples of how to use shared weights in torch, specifically in a non recurrent setting. I understand that according to [this](https://github.com/torch/nn/blob/master/doc/overview.md#a-note-on-sharing-parameters) I am supposed to call clone() with a particular set of parameters. My question is which of the clones I am supposed to call getParameters() on to be able to optimize correctly using one of the optimizers from the optim package. ",2,4
573,2017-1-19,2017,1,19,17,5ovhcg,nlin_Dting=Sx on th First Dt,https://www.reddit.com/r/MachineLearning/comments/5ovhcg/nlin_dtingsx_on_th_first_dt/,Gabrielakre,1484814468,[removed],0,1
574,2017-1-19,2017,1,19,17,5ovktm,What's the difference between 2D convolution on color images and 3D convolution on 3D BW volumes?,https://www.reddit.com/r/MachineLearning/comments/5ovktm/whats_the_difference_between_2d_convolution_on/,SakvaUA,1484816376,[removed],0,1
575,2017-1-19,2017,1,19,19,5ovw09,What are some neural network based approaches for background subtraction like the way [Fabby](https://www.producthunt.com/posts/fabby-2) does?,https://www.reddit.com/r/MachineLearning/comments/5ovw09/what_are_some_neural_network_based_approaches_for/,[deleted],1484822081,[removed],0,1
576,2017-1-19,2017,1,19,19,5ovx19,[N] Knet: beginning deep learning with 100 lines of Julia,https://www.reddit.com/r/MachineLearning/comments/5ovx19/n_knet_beginning_deep_learning_with_100_lines_of/,aicano,1484822633,,5,65
577,2017-1-19,2017,1,19,19,5ovx86,[D] What are some neural network based approaches for background subtraction like the way Fabby does?,https://www.reddit.com/r/MachineLearning/comments/5ovx86/d_what_are_some_neural_network_based_approaches/,n00bto1337,1484822739,"[Fabby](https://www.producthunt.com/posts/fabby-2) for more info. I am fine with approaches which need not be real time, just accurate.",4,1
578,2017-1-19,2017,1,19,20,5ow1ch,[D] Students: Where are you interning this summer?,https://www.reddit.com/r/MachineLearning/comments/5ow1ch/d_students_where_are_you_interning_this_summer/,tmp19538,1484824848,,19,21
579,2017-1-19,2017,1,19,20,5ow36a,Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features,https://www.reddit.com/r/MachineLearning/comments/5ow36a/predicting_nonsmall_cell_lung_cancer_prognosis_by/,americ,1484825710,,0,1
580,2017-1-19,2017,1,19,20,5ow4i4,[D] Trying to understand the quadratic approximation to the hessian,https://www.reddit.com/r/MachineLearning/comments/5ow4i4/d_trying_to_understand_the_quadratic/,TheMiamiWhale,1484826332,"I'm reading over the convergence analysis of gradient descent [here](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/05-grad-descent.pdf) and am fine with everything except for the line on page 6 stating

    ... replacing the usual g(g(x)) by 1/t I (where g is the gradient)

I've been mulling this over all afternoon and evening yesterday and can't figure this out. They are replacing H(x) by a quadratic approximation, which would yield something along the lines of (**edit**: for those curious, the quadratic approximation is about g(x), and then solved for H(x))

    H(x) = (g(x+tv) - g(x))/t

but somehow the difference between the perturbed gradient and regular gradient is equal to the identity matrix. Can someone push me in the right direction?

**Edit**: for those coming to this with the same question, I have found the explanation. The result comes from the assumption that the gradient of `f` is Lipschitz continuous, and that `f` is convex. With these assumptions you can prove the result (See *Introductory Lecture on Convex Optimization* - Y. Nesterov, pg 22 for the proof)",15,4
581,2017-1-19,2017,1,19,20,5ow54b,Could someone give ideas on necessary features to extract from a song for a recommendation system with python.,https://www.reddit.com/r/MachineLearning/comments/5ow54b/could_someone_give_ideas_on_necessary_features_to/,OreFash,1484826629,[removed],0,1
582,2017-1-19,2017,1,19,21,5owcz0,"Highlights of NIPS 2016: Adversarial Learning, Meta-learning and more",https://www.reddit.com/r/MachineLearning/comments/5owcz0/highlights_of_nips_2016_adversarial_learning/,MikeWally,1484829973,,0,1
583,2017-1-19,2017,1,19,21,5owe0v,"Machine learning as a joke, not so sure about it now...",https://www.reddit.com/r/MachineLearning/comments/5owe0v/machine_learning_as_a_joke_not_so_sure_about_it/,planktonfun,1484830401,,0,1
584,2017-1-19,2017,1,19,22,5owm4t,[R] [1701.04862] Towards Principled Methods for Training Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5owm4t/r_170104862_towards_principled_methods_for/,evc123,1484833355,,8,38
585,2017-1-19,2017,1,19,23,5owwmb,"[R] Variational Inference with Implicit Models, Part II: Amortised Inference + code in iPython notebook",https://www.reddit.com/r/MachineLearning/comments/5owwmb/r_variational_inference_with_implicit_models_part/,fhuszar,1484836869,,2,20
586,2017-1-20,2017,1,20,0,5ox7jn,"[N] a few comics by Julia Evans from the ""rules of machine learning""",https://www.reddit.com/r/MachineLearning/comments/5ox7jn/n_a_few_comics_by_julia_evans_from_the_rules_of/,pmigdal,1484840127,,1,10
587,2017-1-20,2017,1,20,0,5oxcht,"[N] New, open-source ODBC Reader for TensorFlow lets you work with data from ODBC sources",https://www.reddit.com/r/MachineLearning/comments/5oxcht/n_new_opensource_odbc_reader_for_tensorflow_lets/,AutumnHavok,1484841542,,0,4
588,2017-1-20,2017,1,20,1,5oxe5a,Predicting with confidence: the best machine learning idea you never heard of,https://www.reddit.com/r/MachineLearning/comments/5oxe5a/predicting_with_confidence_the_best_machine/,[deleted],1484841985,[deleted],0,1
589,2017-1-20,2017,1,20,1,5oxelk,Why it is said that a neuronal network without any hidden layer is a linear classifier?,https://www.reddit.com/r/MachineLearning/comments/5oxelk/why_it_is_said_that_a_neuronal_network_without/,tramada17,1484842112,[removed],0,1
590,2017-1-20,2017,1,20,1,5oxh18,[R] Predicting with confidence: the best machine learning idea you never heard of,https://www.reddit.com/r/MachineLearning/comments/5oxh18/r_predicting_with_confidence_the_best_machine/,Barbas,1484842786,,27,114
591,2017-1-20,2017,1,20,2,5oxrcf,A Machine Learning Approach to Log Analytics,https://www.reddit.com/r/MachineLearning/comments/5oxrcf/a_machine_learning_approach_to_log_analytics/,sjscott80,1484845607,,0,1
592,2017-1-20,2017,1,20,2,5oxu10,"Do LSTM networks completely solve the vanishing/exploding gradients problem, or are they just bandaids?",https://www.reddit.com/r/MachineLearning/comments/5oxu10/do_lstm_networks_completely_solve_the/,futurecoconut,1484846296,[removed],0,1
593,2017-1-20,2017,1,20,2,5oxun0,Machine Learning audiobook recommendations,https://www.reddit.com/r/MachineLearning/comments/5oxun0/machine_learning_audiobook_recommendations/,mortez1,1484846469,[removed],0,1
594,2017-1-20,2017,1,20,2,5oxy1f,[R] Feynman Machine vs Noisy Lorenz Attractor in Two Minutes on a Macbook Pro GPU,https://www.reddit.com/r/MachineLearning/comments/5oxy1f/r_feynman_machine_vs_noisy_lorenz_attractor_in/,fergbyrne,1484847420,,11,0
595,2017-1-20,2017,1,20,3,5oy7n6,What is artificial intelligence? A three part definition  Simply Statistics [x-post r/statistics],https://www.reddit.com/r/MachineLearning/comments/5oy7n6/what_is_artificial_intelligence_a_three_part/,t_rex_tullis,1484849944,,0,2
596,2017-1-20,2017,1,20,4,5oyk5i,Clickme: a web-based game for improving artificial intelligence.,https://www.reddit.com/r/MachineLearning/comments/5oyk5i/clickme_a_webbased_game_for_improving_artificial/,drewlinsley,1484853137,[removed],0,1
597,2017-1-20,2017,1,20,4,5oysdd,How Four AI Startups Help Brands Exploit Customer Reviews,https://www.reddit.com/r/MachineLearning/comments/5oysdd/how_four_ai_startups_help_brands_exploit_customer/,SethGrimes,1484855211,,0,1
598,2017-1-20,2017,1,20,5,5oz6zu,New XPS 15 9560 vs Precision 5520,https://www.reddit.com/r/MachineLearning/comments/5oz6zu/new_xps_15_9560_vs_precision_5520/,deeplearnlaptop,1484858677,[removed],0,1
599,2017-1-20,2017,1,20,7,5ozvbv,Darknet Yolo Algorithm Applied to a Home Movie,https://www.reddit.com/r/MachineLearning/comments/5ozvbv/darknet_yolo_algorithm_applied_to_a_home_movie/,dustball,1484864759,,0,1
600,2017-1-20,2017,1,20,7,5p01c4,NHTSAs full final investigation into Teslas Autopilot shows 40% crash rate reduction,https://www.reddit.com/r/MachineLearning/comments/5p01c4/nhtsas_full_final_investigation_into_teslas/,[deleted],1484866422,[deleted],1,1
601,2017-1-20,2017,1,20,8,5p073i,[D] NHTSAs full final investigation into Teslas Autopilot shows 40% crash rate reduction,https://www.reddit.com/r/MachineLearning/comments/5p073i/d_nhtsas_full_final_investigation_into_teslas/,rumblestiltsken,1484868020,,6,27
602,2017-1-20,2017,1,20,8,5p0bqq,"""Bringing Impressionism to Life with Neural Style Transfer in Come Swim""-Bhautik J Joshi, Kristen Stewart, David Shapiro",https://www.reddit.com/r/MachineLearning/comments/5p0bqq/bringing_impressionism_to_life_with_neural_style/,sidsig,1484869339,,0,1
603,2017-1-20,2017,1,20,10,5p0zr0,Are there any good facial recognition AI's for video?,https://www.reddit.com/r/MachineLearning/comments/5p0zr0/are_there_any_good_facial_recognition_ais_for/,mlthrow123,1484876481,[removed],0,1
604,2017-1-20,2017,1,20,11,5p19ev,Machine Learning/AI Project Ideas for High School Senior?,https://www.reddit.com/r/MachineLearning/comments/5p19ev/machine_learningai_project_ideas_for_high_school/,[deleted],1484879540,[removed],0,1
605,2017-1-20,2017,1,20,11,5p19w1,Neurotale is an iOS app that uses a deep LSTM network to generate stories.,https://www.reddit.com/r/MachineLearning/comments/5p19w1/neurotale_is_an_ios_app_that_uses_a_deep_lstm/,[deleted],1484879692,[deleted],0,1
606,2017-1-20,2017,1,20,13,5p1pce,[R] [1701.04722] Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/5p1pce/r_170104722_adversarial_variational_bayes/,[deleted],1484884822,[deleted],1,1
607,2017-1-20,2017,1,20,14,5p21t0,What advantages does PyTorch have over other dynamic graph libraries?,https://www.reddit.com/r/MachineLearning/comments/5p21t0/what_advantages_does_pytorch_have_over_other/,[deleted],1484889250,[removed],0,1
608,2017-1-20,2017,1,20,14,5p23nm,question about kaggle lung cancer detection competition,https://www.reddit.com/r/MachineLearning/comments/5p23nm/question_about_kaggle_lung_cancer_detection/,[deleted],1484889973,[removed],0,1
609,2017-1-20,2017,1,20,14,5p26f2,Learn Support Vector Machine (SVM) from Scratch in R,https://www.reddit.com/r/MachineLearning/comments/5p26f2/learn_support_vector_machine_svm_from_scratch_in_r/,psangrene,1484891059,,0,1
610,2017-1-20,2017,1,20,17,5p2pfq,Conditioning Seq2Seq,https://www.reddit.com/r/MachineLearning/comments/5p2pfq/conditioning_seq2seq/,bonnytalon,1484899275,[removed],0,1
611,2017-1-20,2017,1,20,17,5p2rb6,Where to buy plasticine clay mixerA question from Israel customers want to inquiry.,https://www.reddit.com/r/MachineLearning/comments/5p2rb6/where_to_buy_plasticine_clay_mixera_question_from/,mixmachinery,1484900225,,1,1
612,2017-1-20,2017,1,20,17,5p2reg,"For Data science beginners:Interpretation of AIC,Deviance,Degree of Freedom,Number of Fisher Scoring iterations,etc",https://www.reddit.com/r/MachineLearning/comments/5p2reg/for_data_science_beginnersinterpretation_of/,A_D_Exploration,1484900269,,0,1
613,2017-1-20,2017,1,20,18,5p2yjk,Resources for a beginner [ unorthodox request~ ],https://www.reddit.com/r/MachineLearning/comments/5p2yjk/resources_for_a_beginner_unorthodox_request/,sid3695,1484904177,[removed],0,1
614,2017-1-20,2017,1,20,18,5p2ywd,Predicting medical chronic conditions with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5p2ywd/predicting_medical_chronic_conditions_with/,B1ackSwan,1484904353,,0,1
615,2017-1-20,2017,1,20,18,5p32eq,[D] Is continually updating the world state a good Idea?,https://www.reddit.com/r/MachineLearning/comments/5p32eq/d_is_continually_updating_the_world_state_a_good/,_ACB_,1484906189,"I want to train a neural network on a game world that can have an unlimitted amount of objects in it. You start with an empty world and then continually add new objects to the world that also may or may not change other objects when entering the world. The goal is to reach a certain world state against an adversary by placing objects from a limited selection available to you without knowing what objects the enemy has. Basically like a card game.

Now there is two problems with this:

1. The amount of objects is unbounded which means there needs to be some kind of encoding done.
2. Placing an object is not as straightforward as one might first think because when you place an object it may trigger effects that may allow you to chose a number of objects in the world or in your selection changing their attributes but also may grant you a new random object.

My idea was to train an autoencoder on the world state where i would feed a replay of each action that happend into the network instead of directly encoding the current world sate. So basically after each action (placing an object or triggering an effect) i would feed that action into the autoencoder to update its internal state. And then use the encoded state to feed it into a second network where i would then evaluate each possible action to choose my next action.

My question now is if that is a good idea or maybe if there is a better course of action because all the research i have seen in playing similar games always encodes all possible resulting states and lets the network calculate the expected win chance which i cant do since it is unfeasable to check each possible result state.",2,5
616,2017-1-20,2017,1,20,20,5p3ce7,[D] Has anyone applied style transfer on top of GAN generated images?,https://www.reddit.com/r/MachineLearning/comments/5p3ce7/d_has_anyone_applied_style_transfer_on_top_of_gan/,visarga,1484911034,"I was thinking that GANs can 'imagine' new things, and style transfer can make pictures look artistic. So a combination would be able to generate both style and content, a ""generative painter"", possibly even conditioned on text. Has anyone tried that?",3,8
617,2017-1-20,2017,1,20,20,5p3g82,Identifying handwritten signatures in document images,https://www.reddit.com/r/MachineLearning/comments/5p3g82/identifying_handwritten_signatures_in_document/,[deleted],1484912935,[removed],0,1
618,2017-1-20,2017,1,20,21,5p3ki2,Machine Learning: Powering the Next Wave of SaaS Solutions,https://www.reddit.com/r/MachineLearning/comments/5p3ki2/machine_learning_powering_the_next_wave_of_saas/,derrickmartins,1484914838,,0,1
619,2017-1-20,2017,1,20,21,5p3op9,"[R] Bringing Impressionism to Life with Neural Style Transfer in Come Swim: co-authored by Kristen Stewart (yes, the twilight actor).",https://www.reddit.com/r/MachineLearning/comments/5p3op9/r_bringing_impressionism_to_life_with_neural/,anonymousTestPoster,1484916574,,27,67
620,2017-1-20,2017,1,20,22,5p3tqc,Do I need a maths degree to research machine learning?,https://www.reddit.com/r/MachineLearning/comments/5p3tqc/do_i_need_a_maths_degree_to_research_machine/,RauchyBear,1484918413,[removed],0,1
621,2017-1-20,2017,1,20,23,5p49dv,Battery Fault Prediction with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5p49dv/battery_fault_prediction_with_machine_learning/,essamseddik,1484923604,[removed],0,1
622,2017-1-21,2017,1,21,0,5p4fca,"Advanced Natural Language Processing Tools for Bot Makers  LUIS, Wit.ai, Api.ai and others (UPDATED)",https://www.reddit.com/r/MachineLearning/comments/5p4fca/advanced_natural_language_processing_tools_for/,bogsformer,1484925393,,0,1
623,2017-1-21,2017,1,21,0,5p4koe,[FREE JOIN] 100% Fre-Sx-Dting-Wbsit. mbers ar from ll over th world. Our girls are looking for just fr-se. Dn't waste any mre time loking fr fr-dting-sits. Jin us toda.,https://www.reddit.com/r/MachineLearning/comments/5p4koe/free_join_100_fresxdtingwbsit_mbers_ar/,Holdenthicvi,1484926914,[removed],0,1
624,2017-1-21,2017,1,21,1,5p4qnb,Looking for pressure sensors for the hands that aren't expensive for ML project,https://www.reddit.com/r/MachineLearning/comments/5p4qnb/looking_for_pressure_sensors_for_the_hands_that/,ryuhphino,1484928567,[removed],0,1
625,2017-1-21,2017,1,21,1,5p4w50,[R] Connecting Generative Adversarial Networks and Actor-Critic Methods,https://www.reddit.com/r/MachineLearning/comments/5p4w50/r_connecting_generative_adversarial_networks_and/,[deleted],1484930100,[deleted],1,1
626,2017-1-21,2017,1,21,2,5p570w,Universe GTA V is down ?,https://www.reddit.com/r/MachineLearning/comments/5p570w/universe_gta_v_is_down/,xingdongrobotics,1484932861,[removed],0,1
627,2017-1-21,2017,1,21,2,5p580v,4 steps to learn TensorFlow when you already know scikit-learn,https://www.reddit.com/r/MachineLearning/comments/5p580v/4_steps_to_learn_tensorflow_when_you_already_know/,kriss75,1484933118,,0,1
628,2017-1-21,2017,1,21,3,5p5jb6,Need some help improving and fixing a neural networking project,https://www.reddit.com/r/MachineLearning/comments/5p5jb6/need_some_help_improving_and_fixing_a_neural/,xyz4d,1484935883,[removed],0,1
629,2017-1-21,2017,1,21,5,5p6e56,Where's the money in ML?,https://www.reddit.com/r/MachineLearning/comments/5p6e56/wheres_the_money_in_ml/,[deleted],1484944167,[removed],0,1
630,2017-1-21,2017,1,21,6,5p6ni3,Best facial keypoint model?,https://www.reddit.com/r/MachineLearning/comments/5p6ni3/best_facial_keypoint_model/,shriphani,1484946821,[removed],0,1
631,2017-1-21,2017,1,21,6,5p6or4,Generate your own datasets aka MNIST using not_notMNIST,https://www.reddit.com/r/MachineLearning/comments/5p6or4/generate_your_own_datasets_aka_mnist_using_not/,[deleted],1484947191,[removed],0,1
632,2017-1-21,2017,1,21,6,5p6qym,[P] not_notMNIST: Dataset generator for classification,https://www.reddit.com/r/MachineLearning/comments/5p6qym/p_not_notmnist_dataset_generator_for/,RafazZ,1484947815,"[Teaser](http://zafar.cc/images/letters.png)

[Personal Blog](http://zafar.cc/not-notmnist-dataset-generation/)

[GitHub Link](https://github.com/zafartahirov/not_notMNIST)

*Sidenote: I am not sure which tag would be the best, so I put a [P]*

I wrote a little script that you can use to generate datasets for classification (like [MNIST](http://yann.lecun.com/exdb/mnist/) or [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html)). It takes fonts that you have, and creates images + label/features pickle (python).

You can find a more detailed explanation on [here](http://zafar.cc/not-notmnist-dataset-generation/). I would really appreciate any critique, issue requests, and pull requests: [GitHub](https://github.com/zafartahirov/not_notMNIST)

The benefits that I personally see is that if you want to test your classification on datasets that involve Unicode alphabets, you can. The problem is that you have to have a lot of fonts to be able to generate a decent dataset. If you have a lot of fonts in your language, I would appreciate if you could share the dataset :)",2,19
633,2017-1-21,2017,1,21,6,5p6thm,[R] [1701.05369] Variational Dropout Sparsifies Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5p6thm/r_170105369_variational_dropout_sparsifies_deep/,darkconfidantislife,1484948553,,2,52
634,2017-1-21,2017,1,21,7,5p743v,Human pose estimation from stereo image pairs,https://www.reddit.com/r/MachineLearning/comments/5p743v/human_pose_estimation_from_stereo_image_pairs/,miat123,1484951701,[removed],0,1
635,2017-1-21,2017,1,21,8,5p7aki,[D] Human pose estimation from stereo image pairs,https://www.reddit.com/r/MachineLearning/comments/5p7aki/d_human_pose_estimation_from_stereo_image_pairs/,miat123,1484953689,"Hello,
I am trying to do human pose estimation (predict the location of 18 joints) using convolutional neural networks. I have a lot of annotated stereo image pairs. I have the following doubts:
1) What kind of convolutional neural network architecture can be suitable for this task? 2)How can I combine the stereo images for the input to the network? Can a simple channel concatenation work? 3) What should the output of the network be? Simple vectorized 2D, 2D heatmap or 3D coordinates directly?
Thank you",6,6
636,2017-1-21,2017,1,21,9,5p7lah,How to Make a Neural Network - Intro to Deep Learning #2,https://www.reddit.com/r/MachineLearning/comments/5p7lah/how_to_make_a_neural_network_intro_to_deep/,llSourcell,1484957066,,0,1
637,2017-1-21,2017,1,21,9,5p7pdq,Scikit Question - what is the difference between SVC and LinearSVC?,https://www.reddit.com/r/MachineLearning/comments/5p7pdq/scikit_question_what_is_the_difference_between/,Nixonite,1484958357,[removed],0,1
638,2017-1-21,2017,1,21,13,5p8qxj,"[N] Barack Obama on Artificial Intelligence, Autonomous Cars, and the Future of Humanity",https://www.reddit.com/r/MachineLearning/comments/5p8qxj/n_barack_obama_on_artificial_intelligence/,rawnlq,1484972164,,3,0
639,2017-1-21,2017,1,21,16,5p9i5q,[D] Opinions on Adversarial Variational Bayes?,https://www.reddit.com/r/MachineLearning/comments/5p9i5q/d_opinions_on_adversarial_variational_bayes/,[deleted],1484983557,[deleted],0,1
640,2017-1-21,2017,1,21,16,5p9ism,[D] Thoughts on Adversarial Variational Bayes?,https://www.reddit.com/r/MachineLearning/comments/5p9ism/d_thoughts_on_adversarial_variational_bayes/,rui_,1484983865,"I came across this paper tonight: https://arxiv.org/abs/1701.04722 and would like to know what people think of this paper. 

It would also be nice if someone can comment on the validity of the values for AVB reported in Table 1. Thanks! ",12,10
641,2017-1-21,2017,1,21,16,5p9ko7,[D] State of Deep Learning Frameworks in 2017 (benchmarks?),https://www.reddit.com/r/MachineLearning/comments/5p9ko7/d_state_of_deep_learning_frameworks_in_2017/,m_ke,1484984801,"With the release of pytorch I wasted some time looking all over twitter/github/gitter/etc for some performance claims. I wasn't able to find anything other than this [comment by Adam Paszke](https://discuss.pytorch.org/t/roadmap-for-torch-and-pytorch/38/6) and [a vague claim in their readme](https://github.com/apaszke/pytorch-dist#performance-and-memory-usage) but it got me thinking about all the new stuff that was announced in the past few months. Some of the highlights include:

- [minpy](https://github.com/dmlc/minpy)
- [dynet](https://github.com/clab/dynet)
- [pytorch](https://github.com/pytorch/pytorch)
- Theano getting a new backend - https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end(gpuarray)
- [Tensorflow XLA](https://www.tensorflow.org/versions/master/experimental/xla/), the 1.0 release and integrating keras as the 10th semiofficial high level interface
- [cudnn v6](https://github.com/pytorch/pytorch/pull/515) coming soon

Deep Learning options now include Theano + Lasagne/Keras, Torch, PyTorch, Tensorflow, Mxnet / minpy, Chainer, Neon, CNTK, Caffe and dynet. It's starting to get hard to keep up with the tradeoffs between them. Soumith's [convnet-benchmarks](https://github.com/soumith/convnet-benchmarks) used to be a great place to check for any progress but it hasn't been updated in a while and as promising as [deepmark](https://github.com/DeepMark/deepmark) looked, it doesn't seem like it's going to happen.

So a few questions:

1. What are you using now? 
2. What are you using it for (CV/NLP/RL)? 
3. What are you most excited about? 
4. Are you considering making a switch to a different framework?
5. What is your biggest bottleneck right now? Memory, compute, data?


Oh, and the hardware side might get more interesting as well. 1080 TI should be out in a month and it looks like intel and AMD might finally start trying.",34,75
642,2017-1-21,2017,1,21,17,5p9oxc,"Ive tried for lts of dating site, but thy askd fe fr using. Ive fund well trusted dting websit whih hlped m t find girls nly for sex",https://www.reddit.com/r/MachineLearning/comments/5p9oxc/ive_tried_for_lts_of_dating_site_but_thy_askd/,Parkerbacheart,1484987027,[removed],0,1
643,2017-1-21,2017,1,21,17,5p9s23,NVidia released a deep reinforcement learning library,https://www.reddit.com/r/MachineLearning/comments/5p9s23/nvidia_released_a_deep_reinforcement_learning/,sangihi,1484988755,,0,1
644,2017-1-21,2017,1,21,18,5p9y6h,[P] bstriner/keras-tqdm: Keras integration with TQDM progress bars,https://www.reddit.com/r/MachineLearning/comments/5p9y6h/p_bstrinerkerastqdm_keras_integration_with_tqdm/,pmigdal,1484992205,,1,25
645,2017-1-21,2017,1,21,18,5p9yos,Se dting site. Huge nd good chic f girls,https://www.reddit.com/r/MachineLearning/comments/5p9yos/se_dting_site_huge_nd_good_chic_f_girls/,Felixpaira,1484992481,[removed],0,1
646,2017-1-21,2017,1,21,18,5p9ys3,How to Build &amp;amp; Integrate a Virtual Personal Assistant,https://www.reddit.com/r/MachineLearning/comments/5p9ys3/how_to_build_amp_integrate_a_virtual_personal/,[deleted],1484992523,[deleted],0,1
647,2017-1-21,2017,1,21,19,5pa1zg,How to Build &amp; Integrate a Virtual Personal Assistant,https://www.reddit.com/r/MachineLearning/comments/5pa1zg/how_to_build_integrate_a_virtual_personal/,[deleted],1484994393,[deleted],0,1
648,2017-1-21,2017,1,21,21,5pagp3,Is there any reason float16 is not the norm ?,https://www.reddit.com/r/MachineLearning/comments/5pagp3/is_there_any_reason_float16_is_not_the_norm/,[deleted],1485002629,[removed],0,1
649,2017-1-21,2017,1,21,21,5pah19,[D] current state of model based reinforcement learning,https://www.reddit.com/r/MachineLearning/comments/5pah19/d_current_state_of_model_based_reinforcement/,davikrehalt,1485002795,"I see a lot of work, by deepmind for instance, on model less deep q learning. But it seems to me that model based methods are the only way for the agent to plan. What is the progress in that direction? Specifically using neural networks to predict rewards so that search/planning is feasible?",4,15
650,2017-1-21,2017,1,21,22,5pamk6,"[D] Can anyone recommend some good research papers, blogs, MOOCs to learn more about Neural Network Optimizers?",https://www.reddit.com/r/MachineLearning/comments/5pamk6/d_can_anyone_recommend_some_good_research_papers/,[deleted],1485005409,[deleted],4,1
651,2017-1-21,2017,1,21,22,5pan11,[R] The cornucopia of meaningful leads: Applying deep adversarial autoencoders for new molecule development in oncology,https://www.reddit.com/r/MachineLearning/comments/5pan11/r_the_cornucopia_of_meaningful_leads_applying/,NicolasGuacamole,1485005643,,3,14
652,2017-1-22,2017,1,22,1,5pbdnj,[D] Visualizing training with PyTorch,https://www.reddit.com/r/MachineLearning/comments/5pbdnj/d_visualizing_training_with_pytorch/,whoeverwhatever,1485016034,"One of the great advantages of TensorFlow is Tensorboard to visualize training progress and convergence. PyTorch is obviously still in its infancy, and to my knowledge doesn't include anything comparable to Tensorboard (yet?), but is there another general-purpose tool that can fill this void? Some custom matplotlib code would probably do the trick, but I'm just wondering if there's any other slightly higher-level library that people use for this kind of stuff.",12,28
653,2017-1-22,2017,1,22,1,5pbf0h,An artificial intelligence system has been developed that performs at human levels on a standard intelligence test.,https://www.reddit.com/r/MachineLearning/comments/5pbf0h/an_artificial_intelligence_system_has_been/,veritas68,1485016500,,0,1
654,2017-1-22,2017,1,22,2,5pbpkr,Need help for Sharded Matrix Multiplication on multiple GPUs,https://www.reddit.com/r/MachineLearning/comments/5pbpkr/need_help_for_sharded_matrix_multiplication_on/,L14dy,1485019877,[removed],1,1
655,2017-1-22,2017,1,22,2,5pbsgc,Stock Market Forecast: Creating a Model for Chaos Mapping and Predictions,https://www.reddit.com/r/MachineLearning/comments/5pbsgc/stock_market_forecast_creating_a_model_for_chaos/,[deleted],1485020779,[deleted],0,1
656,2017-1-22,2017,1,22,2,5pbsht,[D] Forward-backward algorithm to compute CTC loss,https://www.reddit.com/r/MachineLearning/comments/5pbsht/d_forwardbackward_algorithm_to_compute_ctc_loss/,tuan3w,1485020789,"Hi everyone,
I'm currently reading paper: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks( ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf) to understand ctc loss.
But I'm stuck in understanding CTC forward-backward algorithm. As I understand, we need compute 2 probabilities: \alpha_t(s) and \beta_t(s) where \alpha_t(s) is the probability that we can get first s target label at time step t, and \beta_t(s) is the probability that we can get last s target labels since time step t. We will use these probabilities to compute probability of paths that can map to target label sequence.
We will use dynamic programming to compute these probabilities. But I don't really understand the recursive formulation to compute them.
For example, to compute \alpha_t(s), why is not simply as: \alpha_t(s) = (\alpha_{t-1}(s) + \alpha_{t-1}(s-1)) * y^t_{l'_s} ?
Thanks for your help.",4,5
657,2017-1-22,2017,1,22,2,5pbtkr,"I've been learning ML for a few months now, but have been using CPU RAM only. What do you think of the 1050 as a starter card?",https://www.reddit.com/r/MachineLearning/comments/5pbtkr/ive_been_learning_ml_for_a_few_months_now_but/,linklater2012,1485021120,[removed],0,1
658,2017-1-22,2017,1,22,2,5pbu89,"[D] Two models: The first fits the data faster than the second, but the second ends up fitting better after a learning rate drop. Thoughts?",https://www.reddit.com/r/MachineLearning/comments/5pbu89/d_two_models_the_first_fits_the_data_faster_than/,blkorcut,1485021333,"The situation is basically as the title says. I have two models, one fits faster but ends up worse after the learning rate drop. Is there any ""tips and tricks"" sort of thing with looking at these curves that says anything about these two models?",4,8
659,2017-1-22,2017,1,22,3,5pc296,"UC, Berkeley Deep Reinforcement Learning Course",https://www.reddit.com/r/MachineLearning/comments/5pc296/uc_berkeley_deep_reinforcement_learning_course/,[deleted],1485023742,[deleted],0,1
660,2017-1-22,2017,1,22,3,5pc2or,"[D] UC, Berkeley Deep Reinforcement Learning Course",https://www.reddit.com/r/MachineLearning/comments/5pc2or/d_uc_berkeley_deep_reinforcement_learning_course/,[deleted],1485023857,[deleted],7,165
661,2017-1-22,2017,1,22,4,5pcama,Public WaveNet Code with Fastest *Sampling*,https://www.reddit.com/r/MachineLearning/comments/5pcama/public_wavenet_code_with_fastest_sampling/,alexmlamb,1485026227,[removed],0,1
662,2017-1-22,2017,1,22,5,5pcn20,M rl story about 3 times with diffrent girls for ou gus,https://www.reddit.com/r/MachineLearning/comments/5pcn20/m_rl_story_about_3_times_with_diffrent_girls/,Jasperswitzei,1485029981,[removed],0,1
663,2017-1-22,2017,1,22,5,5pctzu,"[Project] A TensorFlow implementation of Facebook's ""Unsupervised Cross-Domain Image Generation""",https://www.reddit.com/r/MachineLearning/comments/5pctzu/project_a_tensorflow_implementation_of_facebooks/,[deleted],1485032054,[deleted],0,1
664,2017-1-22,2017,1,22,6,5pcw1f,"[Project] A TensorFlow implementation of Facebook's ""Unsupervised Cross-Domain Image Generation""",https://www.reddit.com/r/MachineLearning/comments/5pcw1f/project_a_tensorflow_implementation_of_facebooks/,[deleted],1485032654,[deleted],0,1
665,2017-1-22,2017,1,22,6,5pcy0v,"A TensorFlow implementation of Facebook's ""Unsupervised Cross-Domain Image Generation""",https://www.reddit.com/r/MachineLearning/comments/5pcy0v/a_tensorflow_implementation_of_facebooks/,[deleted],1485033261,[deleted],0,1
666,2017-1-22,2017,1,22,6,5pd071,"[Project] A TensorFlow implementation of Facebook's ""Unsupervised Cross-Domain Image Generation""",https://www.reddit.com/r/MachineLearning/comments/5pd071/project_a_tensorflow_implementation_of_facebooks/,yunjey,1485033917,,1,76
667,2017-1-22,2017,1,22,7,5pdc4z,M rel exerince with girl for yu guys,https://www.reddit.com/r/MachineLearning/comments/5pdc4z/m_rel_exerince_with_girl_for_yu_guys/,Michaelnolu,1485037541,[removed],0,1
668,2017-1-22,2017,1,22,9,5pdz9o,[D] Why is the neg-ln curve preferred over a zero-bounded brachistochrone curve for distribution output losses?,https://www.reddit.com/r/MachineLearning/comments/5pdz9o/d_why_is_the_negln_curve_preferred_over_a/,ElderFalcon,1485044770,"Brought up by: https://www.youtube.com/watch?v=skvnj67YGmw

I understand that neg-ln is useful for statistical distribution losses, but what specifically makes it a strong loss function for that type of function? It seems like the actual loss would vary quite a bit depending upon the number of outputs if the outputs were softmaxed. So wouldn't something more consistent be better across a wider number of output classes (i.e 2 classes to 1000+)?

EDIT: The example of 2 classes further specified -- softmaxing between 2 classifier outputs would give an average value per output (evenly distributed) of .5, which is not really all that heavily punished. On the other hand, softmaxing a 1000 class classifier output would yield an average value per output (evenly distributed) of .001, which is heavily punished.",3,2
669,2017-1-22,2017,1,22,10,5pe946,[D] What is the future of deep learning hardware?,https://www.reddit.com/r/MachineLearning/comments/5pe946/d_what_is_the_future_of_deep_learning_hardware/,darkconfidantislife,1485047816,"So, I was thinking about what the future holds for deep learning hardware. It seems like nvidia dominates, but big challengers like Intel (and Nervana!), AMD , Xilinx, etc. are coming up with their own products. Startups like wave computing and graphcore also appear to be doing interesting stuff. 

I'm very interested in FPGAs for deep learning and even more about ASICs and special chips for deep learning. Are there any papers and/or companies you could point me to? What do you guys think about the future of deep learning hardware? I understand that GPUs are already very good because of matrix multipliers and FPUs, but surely an opportunity exists just by lowering precision (for inference mainly, but apparently stochastic rounding works for training also)?",51,14
670,2017-1-22,2017,1,22,11,5peq6s,Why is my GAN generating the same image for any input noise vector?,https://www.reddit.com/r/MachineLearning/comments/5peq6s/why_is_my_gan_generating_the_same_image_for_any/,adeshpande3,1485053522,[removed],0,1
671,2017-1-22,2017,1,22,13,5pf7t0,gpu price performance comparison questions for ML,https://www.reddit.com/r/MachineLearning/comments/5pf7t0/gpu_price_performance_comparison_questions_for_ml/,[deleted],1485059926,[removed],0,1
672,2017-1-22,2017,1,22,14,5pfh94,"Website to buy and sell data for ML researchers, students etc",https://www.reddit.com/r/MachineLearning/comments/5pfh94/website_to_buy_and_sell_data_for_ml_researchers/,akshaynathr,1485063770,[removed],0,1
673,2017-1-22,2017,1,22,15,5pfku8,[P] Practical PyTorch: Classifying Names with a Character-Level RNN,https://www.reddit.com/r/MachineLearning/comments/5pfku8/p_practical_pytorch_classifying_names_with_a/,hawking1125,1485065344,,7,55
674,2017-1-22,2017,1,22,15,5pfpem,Implementing a Binary Classifier from scratch in Python,https://www.reddit.com/r/MachineLearning/comments/5pfpem/implementing_a_binary_classifier_from_scratch_in/,maheshkkumar,1485067574,,0,1
675,2017-1-22,2017,1,22,18,5pg3zs,[R] [1610.09733] PCA meets RG,https://www.reddit.com/r/MachineLearning/comments/5pg3zs/r_161009733_pca_meets_rg/,[deleted],1485075985,[deleted],1,1
676,2017-1-22,2017,1,22,18,5pg4wx,[R] [1610.09733] PCA meets the Renormalization Group,https://www.reddit.com/r/MachineLearning/comments/5pg4wx/r_161009733_pca_meets_the_renormalization_group/,mhlr,1485076601,,1,27
677,2017-1-22,2017,1,22,19,5pg9s3,Should I reimplement existing projects?,https://www.reddit.com/r/MachineLearning/comments/5pg9s3/should_i_reimplement_existing_projects/,__The_Coder__,1485079637,[removed],0,1
678,2017-1-22,2017,1,22,19,5pg9xf,Neural Networks: The mechanics of backpropagation,https://www.reddit.com/r/MachineLearning/comments/5pg9xf/neural_networks_the_mechanics_of_backpropagation/,tvganesh,1485079736,,0,1
679,2017-1-22,2017,1,22,19,5pga85,[R] [1511.02476] Statistical physics of inference: Thresholds and algorithms,https://www.reddit.com/r/MachineLearning/comments/5pga85/r_151102476_statistical_physics_of_inference/,mhlr,1485079902,,1,14
680,2017-1-22,2017,1,22,19,5pgb4s,[D] What industry/academic labs are available for internships?,https://www.reddit.com/r/MachineLearning/comments/5pgb4s/d_what_industryacademic_labs_are_available_for/,MetricSpade007,1485080486,[removed],4,15
681,2017-1-22,2017,1,22,20,5pgjyv,[N] Telegram channel about ML/bots/chatbots,https://www.reddit.com/r/MachineLearning/comments/5pgjyv/n_telegram_channel_about_mlbotschatbots/,maximabramchuk,1485085903,,3,7
682,2017-1-22,2017,1,22,21,5pgows,[D] What methods do you use for hyper-parameters optimization in CNN?,https://www.reddit.com/r/MachineLearning/comments/5pgows/d_what_methods_do_you_use_for_hyperparameters/,pplonski,1485088506,[removed],0,1
683,2017-1-23,2017,1,23,0,5phdmj,AWS EC2 for photogrametry?,https://www.reddit.com/r/MachineLearning/comments/5phdmj/aws_ec2_for_photogrametry/,Phischstaebchen,1485097736,[removed],1,1
684,2017-1-23,2017,1,23,2,5pidk2,[D] is overfitting on a VERY small data set a necessary condition for a neural network to overfit or generalize on a big data set?,https://www.reddit.com/r/MachineLearning/comments/5pidk2/d_is_overfitting_on_a_very_small_data_set_a/,qwertz_guy,1485107467,"How useful is it to test a new architecture on a very small data set, e.g. maybe only 1-5 samples? Let's say the architecture I want to test does not converge to a good training error on a very small SUBSET of a bigger training data set (i.e. same distribution), does this imply that there is a problem with my architecture or optimizer that will also lead to bad results on my big training data set?",13,22
685,2017-1-23,2017,1,23,4,5piykf,The growth of artificial intelligence in e-commerce,https://www.reddit.com/r/MachineLearning/comments/5piykf/the_growth_of_artificial_intelligence_in_ecommerce/,hoaphumanoid,1485112887,,0,1
686,2017-1-23,2017,1,23,4,5pj70j,"""Fake News""",https://www.reddit.com/r/MachineLearning/comments/5pj70j/fake_news/,llSourcell,1485115026,[removed],0,1
687,2017-1-23,2017,1,23,5,5pjd4e,"[D] Contest ""Customer Flow Forecasts on Koubei.com""",https://www.reddit.com/r/MachineLearning/comments/5pjd4e/d_contest_customer_flow_forecasts_on_koubeicom/,Juampilorenzo,1485116530,"Hi! I am looking for someone interested in participating in the contest.

http://blog.udacity.com/2017/01/data-science-enthusiasts-win-10000.html

I'm a Bachelor student and .NET developer with no real experience in the field, I have finished with Andrew Ng's course and I am working with similar ones. I'm really into ML and I would like to get some experience giving it a try",0,2
688,2017-1-23,2017,1,23,5,5pjkav,[D] Deep Learning on Heroku tutorial (Iris classification),https://www.reddit.com/r/MachineLearning/comments/5pjkav/d_deep_learning_on_heroku_tutorial_iris/,gabegabe6,1485118353,,1,4
689,2017-1-23,2017,1,23,6,5pjrbu,"What do you think about SEBOOST (NIPS 2016), which is a method for 2nd derivative updates?",https://www.reddit.com/r/MachineLearning/comments/5pjrbu/what_do_you_think_about_seboost_nips_2016_which/,yuvval,1485120104,[removed],1,1
690,2017-1-23,2017,1,23,6,5pju6z,[D] Research Internship interview preparation,https://www.reddit.com/r/MachineLearning/comments/5pju6z/d_research_internship_interview_preparation/,NeuroBoss31,1485120819,"I'm interviewing for several companies to intern over the summer in mainly machine learning and computer vision. I'm surprised I haven't found a rich post on this question, so I decided to post it here so it could help people in the future. *Note: This is for PhD-level research intern interview and not software intern interview.

0) The general question is: How should I prepare for these interviews.

Equally important sub-questions:

1) How do the interviews change depending on the company: Ex: Google Brain, FAIR, DeepMind, OpenAI, MSR, Baidu, NVIDIA, etc... 

2) What are ""must knows"" in the interviews. Example: Knowing Logistic Regression, Explaining a Loss Function, What is Bayes Rule.

3) What are ""don't worry about its"" in the interviews. Example: I consistently hear that people will get quizzed to code depending on the company, but this rarely seems to be the case for a research intern vs software intern; Do I need to know how to implement Bubble sort? Do I need to know about DQN's and NTM's for DeepMind, or is this overkill for an internship interview? Does FAIR expect me to be highly proficient in Torch?

4) How much coding vs math was involved in your interview. Give an example of the coding problem (because I'm sure they are different from the 'traditional ones' in software intern interviews).

5) Feel free to post your internship interview experience (positive/negative)
",2,43
691,2017-1-23,2017,1,23,6,5pjud4,[D] What type of massive labeled datasets you think are missing from the public domain?,https://www.reddit.com/r/MachineLearning/comments/5pjud4/d_what_type_of_massive_labeled_datasets_you_think/,tossedsaladbowl,1485120865,What type of intricately labeled datasets would be useful to your work if made available? Aside from releasing some of the more obvious proprietary industrial datasets of course. Consider Imagenet as an example.,85,79
692,2017-1-23,2017,1,23,6,5pjw5k,[P] Deep Learning Chess Evaluation Function,https://www.reddit.com/r/MachineLearning/comments/5pjw5k/p_deep_learning_chess_evaluation_function/,sprintletecity,1485121297,"I'm trying to write a [TF version](https://github.com/2014mchidamb/TensorChess/blob/master/Magikarp/model.py) of [this](https://erikbern.com/2014/11/29/deep-learning-for-chess/), but I keep finding that any extended period of training leads my neural network to pretty much converge to 0/same evaluation for all boards. As far as I can tell, I've followed the design in the blog post very closely, save for using ADAM for optimization. I assumed my problems stemmed from dying RELUs/vanishing gradients, but tinkering with the cost function has done me little good... Any advice?",2,3
693,2017-1-23,2017,1,23,6,5pk08h,Question about handwritten numbers recognition with limited data set,https://www.reddit.com/r/MachineLearning/comments/5pk08h/question_about_handwritten_numbers_recognition/,zixmarkiz,1485122271,[removed],0,1
694,2017-1-23,2017,1,23,7,5pk5ci,"Question: If I have a set of coordinates as 10 X and 10 Y values and want to train a neural network to find the slope and intercept of the line that best fits the points. Should I input each X and Y separately, or do I need to encode them somehow so they are one input?",https://www.reddit.com/r/MachineLearning/comments/5pk5ci/question_if_i_have_a_set_of_coordinates_as_10_x/,MrAckerman,1485123555,[removed],0,1
695,2017-1-23,2017,1,23,7,5pk6ln,Really Quick Questions with Sebastian Thrun,https://www.reddit.com/r/MachineLearning/comments/5pk6ln/really_quick_questions_with_sebastian_thrun/,llSourcell,1485123896,,0,1
696,2017-1-23,2017,1,23,9,5pl416,Udacity Nature paper on ML,https://www.reddit.com/r/MachineLearning/comments/5pl416/udacity_nature_paper_on_ml/,insider_7,1485133017,[removed],0,1
697,2017-1-23,2017,1,23,10,5plcl8,Would anybody here be able to use machine learning image recognition to count the number of people in an image/crowd?,https://www.reddit.com/r/MachineLearning/comments/5plcl8/would_anybody_here_be_able_to_use_machine/,LukeSkyWalkerGetsIt,1485135606,[removed],0,1
698,2017-1-23,2017,1,23,11,5plj6u,Handwritten number to digit segmentation,https://www.reddit.com/r/MachineLearning/comments/5plj6u/handwritten_number_to_digit_segmentation/,kpoman,1485137582,[removed],0,1
699,2017-1-23,2017,1,23,12,5plyqt,Crossentropy method using scikit-learn neural nets. Because sometimes it's simple things that work best. Dive-in button: http://mybinder.org/repo/yandexdataschool/sklearn-deeprl,https://www.reddit.com/r/MachineLearning/comments/5plyqt/crossentropy_method_using_scikitlearn_neural_nets/,justheuristic,1485142186,,0,1
700,2017-1-23,2017,1,23,13,5pm8u6,"Hey, I make a blinking video for my robot arm project!Make sure you won't blink your eyes within 60s!!! #Arduino Based#Sorting with Vision #Modular Programming#Laser Engraving#",https://www.reddit.com/r/MachineLearning/comments/5pm8u6/hey_i_make_a_blinking_video_for_my_robot_arm/,poppygogo,1485145374,,0,1
701,2017-1-23,2017,1,23,13,5pmd8k,[P] Create Telegram Stickers using Fully Convolutional Networks (FCNs),https://www.reddit.com/r/MachineLearning/comments/5pmd8k/p_create_telegram_stickers_using_fully/,warmspringwinds,1485146825,,3,69
702,2017-1-23,2017,1,23,14,5pmfst,Four question for Geoff Hinton,https://www.reddit.com/r/MachineLearning/comments/5pmfst/four_question_for_geoff_hinton/,kazi_shezan,1485147651,,0,1
703,2017-1-23,2017,1,23,14,5pmla2,[D] Is simply using TPot on any data set a good idea?,https://www.reddit.com/r/MachineLearning/comments/5pmla2/d_is_simply_using_tpot_on_any_data_set_a_good_idea/,[deleted],1485149361,[deleted],1,5
704,2017-1-23,2017,1,23,14,5pmmla,[D] is there code available for Salesforce research's latest NLP papers?,https://www.reddit.com/r/MachineLearning/comments/5pmmla/d_is_there_code_available_for_salesforce/,redditAccount3432,1485149788,can't seem to find any ... ,2,9
705,2017-1-23,2017,1,23,15,5pmwd5,Learning with small dataset,https://www.reddit.com/r/MachineLearning/comments/5pmwd5/learning_with_small_dataset/,mertsserin,1485152983,[removed],0,1
706,2017-1-23,2017,1,23,15,5pmzi5,Google and others think software that learns to learn could take over some work done by AI experts,https://www.reddit.com/r/MachineLearning/comments/5pmzi5/google_and_others_think_software_that_learns_to/,turingbook,1485154115,,0,1
707,2017-1-23,2017,1,23,16,5pn5x1,"Google can autocorrect ""pupisanddodgs are"" into ""puppies and dogs are"". That's both spelling errors *and* segmentation errors. What algorithms/research should I look into to replicate this (on a domain-specific corpus)?",https://www.reddit.com/r/MachineLearning/comments/5pn5x1/google_can_autocorrect_pupisanddodgs_are_into/,pupsandogs,1485156497,[removed],0,1
708,2017-1-23,2017,1,23,16,5pn9o1,Combination of Structured Output Prediction + Generative Model + Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/5pn9o1/combination_of_structured_output_prediction/,gmkim90,1485158016,[removed],0,1
709,2017-1-23,2017,1,23,17,5pngbr,Explanation of maximum likelihood,https://www.reddit.com/r/MachineLearning/comments/5pngbr/explanation_of_maximum_likelihood/,rahulkulhari3,1485160822,[removed],0,1
710,2017-1-23,2017,1,23,18,5pnnh2,Experience replay when Q-learning a game-AI vs. itself,https://www.reddit.com/r/MachineLearning/comments/5pnnh2/experience_replay_when_qlearning_a_gameai_vs/,[deleted],1485163875,[removed],0,1
711,2017-1-23,2017,1,23,18,5pnpue,Machine Learning: An Introduction to Decision Trees,https://www.reddit.com/r/MachineLearning/comments/5pnpue/machine_learning_an_introduction_to_decision_trees/,nitinkarma,1485164921,,0,1
712,2017-1-23,2017,1,23,19,5pnww2,Wll trustd Intrnt dating with man girls,https://www.reddit.com/r/MachineLearning/comments/5pnww2/wll_trustd_intrnt_dating_with_man_girls/,Rydertanett,1485167825,[removed],0,1
713,2017-1-23,2017,1,23,20,5po8jq,Does anyone have knowledge how to remove music background from audio?,https://www.reddit.com/r/MachineLearning/comments/5po8jq/does_anyone_have_knowledge_how_to_remove_music/,ohassan,1485172511,[removed],0,1
714,2017-1-23,2017,1,23,23,5pp13y,www.google.de,https://www.reddit.com/r/MachineLearning/comments/5pp13y/wwwgooglede/,[deleted],1485181825,[removed],0,1
715,2017-1-24,2017,1,24,0,5pp9pp,[P] Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer,https://www.reddit.com/r/MachineLearning/comments/5pp9pp/p_paying_more_attention_to_attention_improving/,hawking1125,1485184243,,3,116
716,2017-1-24,2017,1,24,0,5ppeft,[P] AI Toolbox - Searchable Directory of Open Source AI Libraries,https://www.reddit.com/r/MachineLearning/comments/5ppeft/p_ai_toolbox_searchable_directory_of_open_source/,scrappyD00,1485185507,,6,6
717,2017-1-24,2017,1,24,0,5ppkjt,[D] How can I improve my modified squeeznet mode?,https://www.reddit.com/r/MachineLearning/comments/5ppkjt/d_how_can_i_improve_my_modified_squeeznet_mode/,[deleted],1485187056,[deleted],3,0
718,2017-1-24,2017,1,24,2,5ppzwa,Feature Engineering: Introduction to Geospatial Data with Python,https://www.reddit.com/r/MachineLearning/comments/5ppzwa/feature_engineering_introduction_to_geospatial/,dreyco,1485190820,,0,1
719,2017-1-24,2017,1,24,3,5pqjiu,[R] Deeper Depth Prediction with Fully Convolutional Residual Networks (Paper + Code + TensorFlow &amp; MatConvNet models),https://www.reddit.com/r/MachineLearning/comments/5pqjiu/r_deeper_depth_prediction_with_fully/,[deleted],1485195507,[deleted],0,1
720,2017-1-24,2017,1,24,3,5pqm3k,Deeper Depth Prediction with Fully Convolutional Residual Networks (paper + code + TensorFlow &amp; MatConvNet models),https://www.reddit.com/r/MachineLearning/comments/5pqm3k/deeper_depth_prediction_with_fully_convolutional/,[deleted],1485196116,[deleted],0,1
721,2017-1-24,2017,1,24,3,5pqn8z,Any suggestions for papers on multimodal machine learning?,https://www.reddit.com/r/MachineLearning/comments/5pqn8z/any_suggestions_for_papers_on_multimodal_machine/,klop2031,1485196395,[removed],0,1
722,2017-1-24,2017,1,24,3,5pqr4p,[R] Deeper Depth Prediction with Fully Convolutional Residual Networks (paper + code + TensorFlow &amp; MatConvNet models),https://www.reddit.com/r/MachineLearning/comments/5pqr4p/r_deeper_depth_prediction_with_fully/,chrirupp,1485197374,,11,27
723,2017-1-24,2017,1,24,4,5pr00z,[Discussion] I am disappointed and down,https://www.reddit.com/r/MachineLearning/comments/5pr00z/discussion_i_am_disappointed_and_down/,[deleted],1485199567,[deleted],0,0
724,2017-1-24,2017,1,24,4,5pr6v6,Training a better Haar and LBP cascade based Eye Detector than OpenCV's default Eye Detector,https://www.reddit.com/r/MachineLearning/comments/5pr6v6/training_a_better_haar_and_lbp_cascade_based_eye/,spmallick,1485201204,,0,1
725,2017-1-24,2017,1,24,4,5pr79m,[R] Adaptive Maneuver Load Alleviation via Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5pr79m/r_adaptive_maneuver_load_alleviation_via/,[deleted],1485201297,[deleted],0,1
726,2017-1-24,2017,1,24,4,5pr83h,How would I go about testing 2 different multi-agent systems against each other? (Info inside),https://www.reddit.com/r/MachineLearning/comments/5pr83h/how_would_i_go_about_testing_2_different/,jemd13,1485201497,[removed],0,1
727,2017-1-24,2017,1,24,5,5prbkn,[N] Chan Zuckerberg Initiative acquires and will free up science search engine Meta,https://www.reddit.com/r/MachineLearning/comments/5prbkn/n_chan_zuckerberg_initiative_acquires_and_will/,[deleted],1485202350,[deleted],0,1
728,2017-1-24,2017,1,24,6,5prn8o,[D] Best Seq2Seq dataset for Transfer Learning,https://www.reddit.com/r/MachineLearning/comments/5prn8o/d_best_seq2seq_dataset_for_transfer_learning/,VordeMan,1485205437,"Hello all!

I apologize for making a separate discussion post for this, I would have posted it on the short questions thread but there wasn't one pinned.

I'm trying to choose a dataset to pretrain a large sequence-to-sequence model on, and I'm wondering if anyone has suggestions on a good dataset to utilize for this purpose.

I'm considering the french-english translation used in the original seq2seq paper, but because my end application is not translation based, I'm curious to see if there is a better corpus out there. Ideally, I would be interested in something like a sentence/paragraph-size paraphrase database, but as far as I know there are no paraphrase databases large enough and so am planning to be flexible.

Any hints or pointers are very appreciated! Thank you!


",5,5
729,2017-1-24,2017,1,24,6,5prujm,International Robotic Sailing Competition,https://www.reddit.com/r/MachineLearning/comments/5prujm/international_robotic_sailing_competition/,MangoNorthEast,1485207341,[removed],0,1
730,2017-1-24,2017,1,24,6,5prv3m,[D] A few questions on Hyperparameter search,https://www.reddit.com/r/MachineLearning/comments/5prv3m/d_a_few_questions_on_hyperparameter_search/,brannondorsey,1485207487,"Hyperparameter search seems like on of the most beneficial practices to learn how to do well when training neural networks. Rather than poking around in the dark, or adjusting your hyperparameters based on intuition, my understanding is that a good hyperparameter search should yield near-optimal hyperparameter tunings on its own. I've begun experimenting with hyperparameter search using Keras with [hyperas](https://github.com/maxpumperla/hyperas) (based on [hyperopt](https://github.com/hyperopt/hyperopt)) but so far I've got a few fundamental questions:

1. Is it most beneficial to test/optimize each hyperparameter (say learning rate, or layer size) independently, and then combine the best result for each hyperparameter you found in your independent searches in your final model? Or are traditional neural network hyperparameters dependent on one another, and thus can't each be optimized on their own?
2. What are the pros and cons of grid search vs random search? Are those generally good enough or are there better state-of-the-art hyperparameter optimization algorithms out there?

Cheers!  ",5,5
731,2017-1-24,2017,1,24,7,5psbj0,[D] What do you do when during training?,https://www.reddit.com/r/MachineLearning/comments/5psbj0/d_what_do_you_do_when_during_training/,Alpy94,1485211792,"So I did a internship last summer. I worked CNNs to be used on rather large images. The training time took around a day, sometimes longer. 

Previously, in all things I worked on, I usually got results in seconds, minutes at most. The long waiting time for results threw my work style out of balance. I would often find myself having started the training and be left with nothing to do.

 I usually plan out the structure of my program to able to handle modifications in small parts. So the first time I start my training, I can work on the other parts. But later on, those would be done.

So I ask this questions to professionals, how do you manage your time? ",31,23
732,2017-1-24,2017,1,24,8,5psg28,Fast Randomized SVD,https://www.reddit.com/r/MachineLearning/comments/5psg28/fast_randomized_svd/,prblynot,1485213020,[removed],0,1
733,2017-1-24,2017,1,24,10,5ptc1c,"Learn TensorFlow and deep learning, without a Ph.D.",https://www.reddit.com/r/MachineLearning/comments/5ptc1c/learn_tensorflow_and_deep_learning_without_a_phd/,edwinksl,1485222368,,0,2
734,2017-1-24,2017,1,24,11,5ptnw9,[1701.04928] Bringing Impressionism to Life with Neural Style Transfer in Come Swim - with actress Kristen Stewart as co-author,https://www.reddit.com/r/MachineLearning/comments/5ptnw9/170104928_bringing_impressionism_to_life_with/,[deleted],1485226230,[deleted],1,1
735,2017-1-24,2017,1,24,12,5ptqix,[R] Bringing Impressionism to Life with Neural Style Transfer in Come Swim - with actress Kristen Stewart as a co-author,https://www.reddit.com/r/MachineLearning/comments/5ptqix/r_bringing_impressionism_to_life_with_neural/,imitationcheese,1485227072,,1,0
736,2017-1-24,2017,1,24,12,5ptwla,Are there any industry standard benchmarks or test sets for sentiment classification?,https://www.reddit.com/r/MachineLearning/comments/5ptwla/are_there_any_industry_standard_benchmarks_or/,butWhoWasBee,1485229060,[removed],0,1
737,2017-1-24,2017,1,24,13,5pu2ns,[D] What does the 'epsilon' config option in keras.json actually do?,https://www.reddit.com/r/MachineLearning/comments/5pu2ns/d_what_does_the_epsilon_config_option_in/,pianomano8,1485231137,"Really dumb question: What does the Keras epsilon config parameter actually do?  

Keras docs say:

    epsilon: float, a numeric fuzzing constant used to avoid dividing by zero in some operations.

..but what does that actually mean?  What is it used for? what will change if you change it?  Does it create a floor for, say, loss? So, anything lower than 10^-7 is assumed to be 0? or anything within 10^-7 of another number is considered equal (in the inexact-floating-point-is-close-enough way?)
",10,0
738,2017-1-24,2017,1,24,13,5pu8m0,Weekly email newsletter with TensorFlow tutorials and articles,https://www.reddit.com/r/MachineLearning/comments/5pu8m0/weekly_email_newsletter_with_tensorflow_tutorials/,[deleted],1485233244,[deleted],0,1
739,2017-1-24,2017,1,24,14,5pubic,[N] Weekly email newsletter with TensorFlow tutorials and articles,https://www.reddit.com/r/MachineLearning/comments/5pubic/n_weekly_email_newsletter_with_tensorflow/,relevate,1485234333,,3,0
740,2017-1-24,2017,1,24,14,5pubij,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://www.reddit.com/r/MachineLearning/comments/5pubij/outrageously_large_neural_networks_the/,[deleted],1485234333,[deleted],0,1
741,2017-1-24,2017,1,24,14,5publr,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://www.reddit.com/r/MachineLearning/comments/5publr/outrageously_large_neural_networks_the/,[deleted],1485234363,[deleted],1,1
742,2017-1-24,2017,1,24,14,5pud72,[Research] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,https://www.reddit.com/r/MachineLearning/comments/5pud72/research_outrageously_large_neural_networks_the/,penguinElephant,1485234965,,32,58
743,2017-1-24,2017,1,24,15,5puoqq,[D] Training CNNs on non-resizeable huge size images,https://www.reddit.com/r/MachineLearning/comments/5puoqq/d_training_cnns_on_nonresizeable_huge_size_images/,NeuroBoss31,1485239778,"I have many images where I need the CNN to decipher if there is target (say person, airplane, tumor, star --depending on the type of dataset). I want the CNN to learn from end-to-end to classify if the images have such targets or not, and suppose I count with 1 Million examples, where half of my images have a target and the other half don't. This last fact is important, since I wouldn't face data unbalance problems that radiologists face for example with tumor vs no-tumor images. I have labels of both images with the target, as well as bounding boxes of the target. 

The problem I face is that the nature of the images requires me not to resize them (since this seems to be the classical CNN approach: rescale an image to 227x227x3, or 32x32x3 and you're set) But the problem is that the image itself is say 3056x2000x3 and the target size (let's suppose a tumor) is 20x20 pixels. Resizing doesn't seem like a good idea now since I will eventuallly just loose the target information if I compress the image too much.

What are my best solutions (and problems I might face, such as explosion in number of parameters) when training a CNN? 

Some thoughts:

1) I can create big filter maps with relatively high stride to subsample the image to a reasonable size, and then go with a traditional Inception-V3, AlexNet, or VGG-Net architecture.

2) I can create an ""CNN-Ensemble"" that sub-samples from many parts of the image to feed its input, and I can concatenate all these feature vectors as a single output, to then perform learning. (Ex: I divide a 554x554x3 image into 4: 227x277x3 images, and concatenate the FCN6 vectors.)

3) I don't consider a CNN, and I got back to classical Computer Vision such as HOG + Sliding Window (or in this, case I could potentially train on image patches and get CNN-features and apply sliding window)",12,33
744,2017-1-24,2017,1,24,17,5pv54l,Are there any pre-trained models that can be used for speech to text conversion?,https://www.reddit.com/r/MachineLearning/comments/5pv54l/are_there_any_pretrained_models_that_can_be_used/,PepLovesJose,1485248316,[removed],0,1
745,2017-1-24,2017,1,24,18,5pva05,[D] xnor.ai,https://www.reddit.com/r/MachineLearning/comments/5pva05/d_xnorai/,senorstallone,1485250972,,15,16
746,2017-1-24,2017,1,24,19,5pvcwl,How to perform PCA(Principal Component Analysis) in R,https://www.reddit.com/r/MachineLearning/comments/5pvcwl/how_to_perform_pcaprincipal_component_analysis_in/,A_D_Exploration,1485252483,,0,1
747,2017-1-24,2017,1,24,19,5pvd29,1 month Im using this free and wll trusted dating website  Iv found 3 girls who grd fr sx,https://www.reddit.com/r/MachineLearning/comments/5pvd29/1_month_im_using_this_free_and_wll_trusted/,Ronanwalldo,1485252569,[removed],0,1
748,2017-1-24,2017,1,24,19,5pvgpi,Evaluating the right AI Platform for your business,https://www.reddit.com/r/MachineLearning/comments/5pvgpi/evaluating_the_right_ai_platform_for_your_business/,virene,1485254475,,0,1
749,2017-1-24,2017,1,24,19,5pvir3,[D] Why in SqueezeNet architecture was used Pooling instead of strided convolution?,https://www.reddit.com/r/MachineLearning/comments/5pvir3/d_why_in_squeezenet_architecture_was_used_pooling/,InfiniteLife2,1485255542,"Strided convolution looks to me as a tool that allows to perform more efficient dimension reduction, than Max Pool since it allows to save more significant to network information with higher versatility. 

One argument for using max pool in SqueezeNet - it's simply allows to reduce number of parameters. Any other?

Also, not architecture spesifiec: Max Pool VS strided Conv?",8,0
750,2017-1-24,2017,1,24,20,5pvjfk,Possible for speech generation using autoencoders?,https://www.reddit.com/r/MachineLearning/comments/5pvjfk/possible_for_speech_generation_using_autoencoders/,jaivardhankapoor,1485255862,[removed],0,1
751,2017-1-24,2017,1,24,22,5pw617,2fuse test 001,https://www.reddit.com/r/MachineLearning/comments/5pw617/2fuse_test_001/,planktonfun,1485265413,,0,1
752,2017-1-24,2017,1,24,23,5pwg81,LDA ELBO goes to -inf,https://www.reddit.com/r/MachineLearning/comments/5pwg81/lda_elbo_goes_to_inf/,sudeshna_roy,1485268898,[removed],0,1
753,2017-1-25,2017,1,25,0,5pwl2l,"RNN, LSTM best practices for features sequence generation",https://www.reddit.com/r/MachineLearning/comments/5pwl2l/rnn_lstm_best_practices_for_features_sequence/,mtsmnp,1485270397,[removed],0,1
754,2017-1-25,2017,1,25,0,5pwtrb,Upgrade to NVIDIA gpu on Mac Pro,https://www.reddit.com/r/MachineLearning/comments/5pwtrb/upgrade_to_nvidia_gpu_on_mac_pro/,snuffleupagus_Rx,1485272958,[removed],0,1
755,2017-1-25,2017,1,25,0,5pwvc9,"[D] Deciding on ConvNet kernel sizes when designing architectures, how do you choose?",https://www.reddit.com/r/MachineLearning/comments/5pwvc9/d_deciding_on_convnet_kernel_sizes_when_designing/,carlthome,1485273419,"How do you decide on the size of the kernels in a ConvNet? 

3x3 seems very popular, but I've seen more exotic sizes such as 5x7 and 9x1 + 1x9 summed together before the activation function. How do you reason about the selection of kernel sizes (assuming infinite computational resources and no particular memory usage requirements)?

I'm particularly thinking in terms of video data or audio spectrograms, but any type of data with spatial structure is of course relevant.

It seems to me in theory it would be good to choose a very big kernel size, because the network could learn to either use all the values or choose to leave a lot of them at zero, but I don't believe this is very good in practice, and that limiting what the feature detectors actually can detect and then stack several gives better results.

Thoughts?",16,22
756,2017-1-25,2017,1,25,1,5px4lx,What cool questions can i ask to a person in AI field?,https://www.reddit.com/r/MachineLearning/comments/5px4lx/what_cool_questions_can_i_ask_to_a_person_in_ai/,[deleted],1485276020,[removed],0,1
757,2017-1-25,2017,1,25,1,5px53a,"Going to War with the Giants: Point-and Click, Automated Machine Learning with MLJAR",https://www.reddit.com/r/MachineLearning/comments/5px53a/going_to_war_with_the_giants_pointand_click/,hugebigfatrhino,1485276153,,0,1
758,2017-1-25,2017,1,25,1,5px582,[D] Neural Network Language Models - projection layer,https://www.reddit.com/r/MachineLearning/comments/5px582/d_neural_network_language_models_projection_layer/,mythaone,1485276181,"Hi! I'm trying very hard to understand how NNLM works and I just can't understand the projection layer. Is there anyone who can explain me how is the projection layer calculated? And is there any paper that explains that? For now I have only found it in [this](http://mi.eng.cam.ac.uk/~gwb24/publications/mphil.thesis.pdf) thesis but I need something more formal so I can cite it.
",8,1
759,2017-1-25,2017,1,25,1,5px83z,[1701.05927] Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis,https://www.reddit.com/r/MachineLearning/comments/5px83z/170105927_learning_particle_physics_by_example/,augustushimself,1485277003,,0,1
760,2017-1-25,2017,1,25,2,5px9uz,[P] Food visualization and recommendation engine in javascript of 50k recipes based on word2vec,https://www.reddit.com/r/MachineLearning/comments/5px9uz/p_food_visualization_and_recommendation_engine_in/,ConspiracyLurr,1485277480,,7,81
761,2017-1-25,2017,1,25,2,5pxjvm,Our system can register into ~20% of all the websites on the internet. Currently making it more robust!,https://www.reddit.com/r/MachineLearning/comments/5pxjvm/our_system_can_register_into_20_of_all_the/,Sn_Shines,1485280220,,0,1
762,2017-1-25,2017,1,25,4,5pxzoq,HackerNews Summarization using NLTK,https://www.reddit.com/r/MachineLearning/comments/5pxzoq/hackernews_summarization_using_nltk/,nextdev,1485284478,,1,1
763,2017-1-25,2017,1,25,4,5pycoa,[D][P][R] WTTE-RNN - Less hacky churn prediction  Focus on the objective,https://www.reddit.com/r/MachineLearning/comments/5pycoa/dpr_wtternn_less_hacky_churn_prediction_focus_on/,ragulpr,1485287897,,12,72
764,2017-1-25,2017,1,25,5,5pyi0n,[D] Organizing Notes on Papers,https://www.reddit.com/r/MachineLearning/comments/5pyi0n/d_organizing_notes_on_papers/,chq12,1485289265,I am wondering what type of organizational systems everyone use to keep track of notes on papers. I keep track of everything in Mendeley but would like to have a way to organize notes on each paper I read. I was thinking about keeping a repo of MD files on github. Any suggestions?,13,4
765,2017-1-25,2017,1,25,5,5pykys,[D] Why transfer learning is better than training from scratch?,https://www.reddit.com/r/MachineLearning/comments/5pykys/d_why_transfer_learning_is_better_than_training/,gabegabe6,1485290021,"Everywhere I see that people get very high accuracy when they use transfer learning.

My question is: Why is it better than training from scratch and why is it good?

What I don't understand (for example with image classification) is that an already trained network contains data (weights) from different images from what we would like to classify. Why it is not reduce accuracy?",9,2
766,2017-1-25,2017,1,25,6,5pyvcp,[D] Numerical Stability of Convolution Algorithms (i.e. Winograd and FFT) at 8-bits,https://www.reddit.com/r/MachineLearning/comments/5pyvcp/d_numerical_stability_of_convolution_algorithms/,[deleted],1485292764,[deleted],4,3
767,2017-1-25,2017,1,25,7,5pz8jq,Filter response in style extraction from images using CNN's,https://www.reddit.com/r/MachineLearning/comments/5pz8jq/filter_response_in_style_extraction_from_images/,[deleted],1485296300,[removed],0,1
768,2017-1-25,2017,1,25,7,5pzbhv,Filter responses in image style recognition,https://www.reddit.com/r/MachineLearning/comments/5pzbhv/filter_responses_in_image_style_recognition/,[deleted],1485297098,[removed],0,1
769,2017-1-25,2017,1,25,8,5pzhkr,[P] PyTorch Installation and Intro (livestream),https://www.reddit.com/r/MachineLearning/comments/5pzhkr/p_pytorch_installation_and_intro_livestream/,vanboxel,1485298873,,2,11
770,2017-1-25,2017,1,25,9,5q043t,Thoughts on CrowdFlower? Is AI = TD +ML +HITL?,https://www.reddit.com/r/MachineLearning/comments/5q043t/thoughts_on_crowdflower_is_ai_td_ml_hitl/,Rafael_Bacardi,1485305579,,0,1
771,2017-1-25,2017,1,25,10,5q07zn,Sentence Classification in PyTorch,https://www.reddit.com/r/MachineLearning/comments/5q07zn/sentence_classification_in_pytorch/,mrdrozdov,1485306805,,0,1
772,2017-1-25,2017,1,25,10,5q0btu,Machine Learning Videos: A collection of recorded talks,https://www.reddit.com/r/MachineLearning/comments/5q0btu/machine_learning_videos_a_collection_of_recorded/,[deleted],1485308037,[deleted],0,1
773,2017-1-25,2017,1,25,10,5q0gd6,[D] Numenta's nupic in production for river data,https://www.reddit.com/r/MachineLearning/comments/5q0gd6/d_numentas_nupic_in_production_for_river_data/,dejormo,1485309499,"Does anyone have a success story to share about nupic's use in production for pattern recognition? 

If not, maybe thoughts and advice?",20,4
774,2017-1-25,2017,1,25,13,5q1895,Helpful Advice To Choosing an Alemite Grease Pump For Lubrication,https://www.reddit.com/r/MachineLearning/comments/5q1895/helpful_advice_to_choosing_an_alemite_grease_pump/,jackerfrinandis,1485318857,,0,1
775,2017-1-25,2017,1,25,13,5q18wk,"Practical applications of ""A Neural Conversational Model""?",https://www.reddit.com/r/MachineLearning/comments/5q18wk/practical_applications_of_a_neural_conversational/,ml_aussie,1485319110,[removed],0,1
776,2017-1-25,2017,1,25,14,5q1ik2,"[N] Machine Learning jobs (San-Francisco, Bay Area) Jan'17",https://www.reddit.com/r/MachineLearning/comments/5q1ik2/n_machine_learning_jobs_sanfrancisco_bay_area/,Dim25,1485322795,,1,0
777,2017-1-25,2017,1,25,15,5q1okj,Reasons To Use Brennan Adapters Fitting,https://www.reddit.com/r/MachineLearning/comments/5q1okj/reasons_to_use_brennan_adapters_fitting/,jackerfrinandis,1485325254,,0,1
778,2017-1-25,2017,1,25,16,5q1tx0,Europe Auto Dimming Rearview Mirrors Market Report 2017,https://www.reddit.com/r/MachineLearning/comments/5q1tx0/europe_auto_dimming_rearview_mirrors_market/,roshanimrr,1485327621,,1,1
779,2017-1-25,2017,1,25,16,5q1wo4,Machine Learning for Reverse Image Search (Small images),https://www.reddit.com/r/MachineLearning/comments/5q1wo4/machine_learning_for_reverse_image_search_small/,march-ai,1485328879,[removed],0,1
780,2017-1-25,2017,1,25,18,5q2ab1,Are there any techniques and models to assign a tailored risk score (credit score rating) on the go on every transaction?,https://www.reddit.com/r/MachineLearning/comments/5q2ab1/are_there_any_techniques_and_models_to_assign_a/,chirau,1485336206,[removed],0,1
781,2017-1-25,2017,1,25,18,5q2bjk,cvpr - 2 weak rejects and 1 strong accept. Is it worth rebuttal?,https://www.reddit.com/r/MachineLearning/comments/5q2bjk/cvpr_2_weak_rejects_and_1_strong_accept_is_it/,SnowRipple,1485336899,[removed],0,1
782,2017-1-25,2017,1,25,20,5q2q17,[P] Word Prediction using Convolutional Neural Networkscan you do better than iPhone Keyboard?,https://www.reddit.com/r/MachineLearning/comments/5q2q17/p_word_prediction_using_convolutional_neural/,longinglove,1485344430,,17,47
783,2017-1-25,2017,1,25,22,5q36eh,Tool for easily sending and running python files on AWS instances,https://www.reddit.com/r/MachineLearning/comments/5q36eh/tool_for_easily_sending_and_running_python_files/,[deleted],1485351142,[deleted],0,1
784,2017-1-25,2017,1,25,23,5q3b5x,Number of parameters vs number of input samples,https://www.reddit.com/r/MachineLearning/comments/5q3b5x/number_of_parameters_vs_number_of_input_samples/,idg101,1485352897,[removed],0,1
785,2017-1-26,2017,1,26,0,5q3oav,[D] Ubuntu or Arch Linux for Machine Learning ???,https://www.reddit.com/r/MachineLearning/comments/5q3oav/d_ubuntu_or_arch_linux_for_machine_learning/,chennai_guy,1485357056,"hey all, I'm learning machine learning and big data stuffs(R Studio,Spark,Anaconda,Cassandra etc) right now! Actually I'm a Linux geek and love using Arch Linux. But for the sake of stability I'm using Ubuntu on my laptop. Now thinking of creating a live pen drive for machine learning and big data stuffs. My question is should I install Ubuntu or Arch Linux(or Antergos) for live pen drive ??",18,0
786,2017-1-26,2017,1,26,0,5q3rtf,[P] Tool for easily sending and running python files on AWS instances,https://www.reddit.com/r/MachineLearning/comments/5q3rtf/p_tool_for_easily_sending_and_running_python/,npielawski,1485358071,,7,12
787,2017-1-26,2017,1,26,0,5q3wwu,"Simple Questions Thread January 25, 2017",https://www.reddit.com/r/MachineLearning/comments/5q3wwu/simple_questions_thread_january_25_2017/,AutoModerator,1485359549,[removed],0,1
788,2017-1-26,2017,1,26,0,5q3ydt,Need help to figure out a machine learning problem,https://www.reddit.com/r/MachineLearning/comments/5q3ydt/need_help_to_figure_out_a_machine_learning_problem/,greyphoenix93,1485359971,[removed],0,1
789,2017-1-26,2017,1,26,1,5q44eh,[D] My hypothesis on a YouTube video,https://www.reddit.com/r/MachineLearning/comments/5q44eh/d_my_hypothesis_on_a_youtube_video/,WuhuSpringfield,1485361590,"Hey guys, I've just made a post in /r/learnmachinelearning, but so far nobody has helped me out.    
Maybe you can.    
The post can be found under [this link](https://www.reddit.com/r/learnmachinelearning/comments/5px6hx/can_you_confirm_my_hypothesis_on_this_youtube/).    
The content follows:    
""    
Hey guys,    
I am currently learning SOMs and I've stumbled across [this beautiful video](https://www.youtube.com/watch?v=zyYZuAQZWTM).    
I'd like you to criticize my hypothesis on what is happening:    
*There is a 32 x 32 grid. Each point (let's call these x) of this map has a two-dimensional weight vector. These weight vectors are uninitialised at the beginning. Iteratively, data points (let's call these y) appear on the map in one of the four colors. Whenever a 'y' appears, the (Euclidean) distance between itself and every weight vector of the 'x's' is computed. The minimum (when more than 1 are minimal, take randomly) is taken and the neighbor function is applied (the BMU and its neighbourhood is adjusted towards the 'y'). The next 'y' will compute its Euclidean distance between itself and the updated weight vectors of the 'x's'.*    
    
    

**Just to be safe: There is no classical dimensional reduction, am I right? The given 32 x 32 map has two dimensions (represented by the weight vectors of its 'x's') and the appearing data points, 'y', have only two dimensions as well.**    
""",3,1
790,2017-1-26,2017,1,26,1,5q4ass,Disambiguating KBpedia Knowledge Graph Concepts,https://www.reddit.com/r/MachineLearning/comments/5q4ass/disambiguating_kbpedia_knowledge_graph_concepts/,yogthos,1485363309,,0,1
791,2017-1-26,2017,1,26,1,5q4bej,[P] Food Classification with Deep Learning in Keras / Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5q4bej/p_food_classification_with_deep_learning_in_keras/,stratospark,1485363478,,7,20
792,2017-1-26,2017,1,26,2,5q4ehv,https://www.raspberrypi.org/blog/google-tools-raspberry-pi/,https://www.reddit.com/r/MachineLearning/comments/5q4ehv/httpswwwraspberrypiorgbloggoogletoolsraspberrypi/,[deleted],1485364277,[deleted],0,1
793,2017-1-26,2017,1,26,2,5q4eqs,[D] google developing machine learning tools for raspberry pi,https://www.reddit.com/r/MachineLearning/comments/5q4eqs/d_google_developing_machine_learning_tools_for/,wei_jok,1485364347,,20,195
794,2017-1-26,2017,1,26,2,5q4f2f,"[Discussion] Any work on video ""temporal super-resolution"" ?",https://www.reddit.com/r/MachineLearning/comments/5q4f2f/discussion_any_work_on_video_temporal/,DrPharael,1485364433,"Recent papers have shown that neural networks are able to do a pretty good job at super-resolution (from one image, generate a new artificial one with a higher resolution).

I am wondering if a similar idea could be used to do ""temporal super-resolution"" on videos. Let me explain: for instance, if I can only record videos at 25 fps, I cannot significantly slow them down in most video editing softwares since it would require some kind of clever interpolation. But maybe a neural network would be able to learn to generate any number of intermediate frames (given a training set of videos acquired with higher framerate). 

Does anyone know if some work has already been done in this direction ?",3,8
795,2017-1-26,2017,1,26,2,5q4jor,Machine Learning MOOC,https://www.reddit.com/r/MachineLearning/comments/5q4jor/machine_learning_mooc/,cinico,1485365640,[removed],0,1
796,2017-1-26,2017,1,26,3,5q4wz6,Dermatologist-level classification of skin cancer with deep neural networks,https://www.reddit.com/r/MachineLearning/comments/5q4wz6/dermatologistlevel_classification_of_skin_cancer/,brandonballinger,1485369172,,0,2
797,2017-1-26,2017,1,26,3,5q4ye3,[P]Supervised Learning in the web browser,https://www.reddit.com/r/MachineLearning/comments/5q4ye3/psupervised_learning_in_the_web_browser/,maka89,1485369545,"**Homepage:** [webandroid.org](https://www.webandroid.org)

I posted a link to a [website](https://www.webandroid.org) I was working on here some time ago. Now, I have completed the core functionality of the web page and added some info on how it works.

The website is called [webandroid.org](https://www.webandroid.org) and allows the user to do some basic supervised learning in the web browser using their own data in .csv format or some example training sets. More info is on the ""Info"" page on the website.

Since ML is becoming more and more popular, I want the page to eventually be used for people who want to learn and apply ML on a superficial level, not having the programming tools/skills or math skills to go deeper.  Although that will require some serious investment in the ""info"" section.

The project has a [github repo](https://github.com/maka89/Javascript-ML) for some of the algorithms. The Neural Networks uses the [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/) library.

Any feedback on the website would be appreciated :)

If anyone is interested in contributing, please contact me. Whether it is adding a new algorithm, fixing issues with the old ones, or maybe most importantly writing on the ""Info"" section, so that it can be used in the future as a good tutorial on ML on a superficial level.




",2,8
798,2017-1-26,2017,1,26,3,5q5149,Looking for a good conference June or July. Any suggestions?,https://www.reddit.com/r/MachineLearning/comments/5q5149/looking_for_a_good_conference_june_or_july_any/,dachhack,1485370241,[removed],0,1
799,2017-1-26,2017,1,26,4,5q5bmq,[D] How do Unrolled GANs work? Implementation?,https://www.reddit.com/r/MachineLearning/comments/5q5bmq/d_how_do_unrolled_gans_work_implementation/,to4life2,1485372937,"I found the following paper very fascinating as it presents a way of training GANs that seem to perform better with respect to mode collapse. I didn't find any other discussions on this topic through subreddit search.

https://arxiv.org/pdf/1611.02163v2.pdf 

However I'm having a tough time understanding the mechanics behind the unrolling. Figure 1 in particular (also reproduced below) is supposed to explain it, but I don't really get it, lots of arrows flying around: 

http://imgur.com/a/dRI1H

Here are my specific questions:

1) Mechanics of how this works. Is the Discriminator updated (via SGD) 3 times before the Generator gradient is updated? Or is something happening where the chain of D gradients are used to compute the G gradient? There's a lot of arrows in the diagram.

They state the following which is relevant but not totally clear to me: 

&gt; It is important to distinguish this from an approach suggested in (Goodfellow et al., 2014), that several update steps of the discriminator parameters should be run before each single update step for the generator. In that approach, the update steps for both models are still gradient descent (ascent) with respect to fixed values of the other model parameters, rather than the surrogate loss we describe in Eq. 9.

2) How would one go about implementing this for a given Discriminator network D(x) with params theta(D)? I found the following software online implementation for Keras, again it's a bit mystical to me but seems to take advantage of re-using a graphical computation structure: 

https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/unrolled_optimizer.py 

3) *Why* does this method help the Generator perform better, e.g. alleviate the mode collapse problem? Is it about giving G some sort of advantage in training against D? Is it because of stability WRT model convergence so that more modes are visited like in the true data? Here's a short vid I found (by the author of the code above) that's supposed to provide some intuition: 

https://www.youtube.com/watch?v=JmON4S0kl04 

---

Thanks for any help with my understanding, and feel free to add more general discussion on the topic. Techniques that train better GANs are sure to be interesting and important! ",14,13
800,2017-1-26,2017,1,26,4,5q5d7o,How to Use Tensorflow for Classification (LIVE),https://www.reddit.com/r/MachineLearning/comments/5q5d7o/how_to_use_tensorflow_for_classification_live/,funtwo2,1485373366,,0,1
801,2017-1-26,2017,1,26,4,5q5e88,[R][N] Videos from the 2016 Tutorials and Conference are available. Workshop videos will be posted at a later date.,https://www.reddit.com/r/MachineLearning/comments/5q5e88/rn_videos_from_the_2016_tutorials_and_conference/,anyonethinkingabout,1485373615,,0,35
802,2017-1-26,2017,1,26,5,5q5i3z,[D] Pre processing the training data for detecting multiple bounding boxes for text recognition in wild (Street view text dataset),https://www.reddit.com/r/MachineLearning/comments/5q5i3z/d_pre_processing_the_training_data_for_detecting/,datumx_jan,1485374628,"Hello, I am working on detecting text in images using street view dataset(SVT), in this dataset for each image we are given with all the words present in the image and the corresponding bounding box enclosing them. I am stuck up on how to pre process these output labels so that I would be able to train a CNN for detecting text(bounding boxes) in the test image. 

Should I take in account the particular word in the output label somehow?

Please suggest suitable strategy and methodology. Thank you!",0,2
803,2017-1-26,2017,1,26,5,5q5l9d,Do the generalization error bound and the VapnikChervonenkis dimension combine to form a strong mathematical argument against a belief in God?,https://www.reddit.com/r/MachineLearning/comments/5q5l9d/do_the_generalization_error_bound_and_the/,theironhide,1485375416,,0,1
804,2017-1-26,2017,1,26,5,5q5nz8,[R] Building a Chatbot: analysis &amp; limitations of modern platforms,https://www.reddit.com/r/MachineLearning/comments/5q5nz8/r_building_a_chatbot_analysis_limitations_of/,thesameoldstories,1485376144,,0,4
805,2017-1-26,2017,1,26,6,5q5v1h,Overview of NIPS 2016,https://www.reddit.com/r/MachineLearning/comments/5q5v1h/overview_of_nips_2016/,hoaphumanoid,1485378037,,0,1
806,2017-1-26,2017,1,26,6,5q5x4v,[R] Dermatologist-level classification of skin cancer with deep neural networks : Nature (Stanford team),https://www.reddit.com/r/MachineLearning/comments/5q5x4v/r_dermatologistlevel_classification_of_skin/,drlukeor,1485378570,,22,65
807,2017-1-26,2017,1,26,6,5q650k,[D] What does CLS in GAN-CLS stand for?,https://www.reddit.com/r/MachineLearning/comments/5q650k/d_what_does_cls_in_gancls_stand_for/,vdashv,1485380708,"Hello, the INT in GAN-INT probably stands for interpolation, as in manifold interpolation. However, does anybody know what does CLS stand for in the matching-aware discriminator variation, with 3 inputs? From the article, the presentation and various other online resources I really have no clue on why is it named like this. Does anybody have an idea?

Edit: link to article: https://arxiv.org/abs/1605.05396",3,0
808,2017-1-26,2017,1,26,8,5q6nq1,"[D] MemoryError: alloc failed with Keras. Why, and how to fix it?",https://www.reddit.com/r/MachineLearning/comments/5q6nq1/d_memoryerror_alloc_failed_with_keras_why_and_how/,[deleted],1485385891,[deleted],1,0
809,2017-1-26,2017,1,26,8,5q6v6s,My first blog in a larger series about containerized machine learning using tensorflow on OpenShift. Would love to get the subs thoughts.,https://www.reddit.com/r/MachineLearning/comments/5q6v6s/my_first_blog_in_a_larger_series_about/,coolhand1,1485388112,,0,1
810,2017-1-26,2017,1,26,9,5q75ks,[D] Ranking loss function for tensorflow,https://www.reddit.com/r/MachineLearning/comments/5q75ks/d_ranking_loss_function_for_tensorflow/,afunkthewmd2,1485391234,"Does anyone have experience with ranking losses such as kendall tau on top of a conv net in tensorflow?  

Instead of purely list-wise comparisons,  perhaps one could go at it as pairwise losses similar to rankSVM style, or ordinal approach with thresholding?

What are your suggestions?  Cheers!",7,6
811,2017-1-26,2017,1,26,9,5q78bx,[P] Zero setup deep learning. Start running Tensorflow on AWS in &lt;30 seconds,https://www.reddit.com/r/MachineLearning/comments/5q78bx/p_zero_setup_deep_learning_start_running/,iluvmylife,1485392067,,26,47
812,2017-1-26,2017,1,26,10,5q7d2c,can I use TPUs on google cloud console somehow?,https://www.reddit.com/r/MachineLearning/comments/5q7d2c/can_i_use_tpus_on_google_cloud_console_somehow/,TPUQuestion,1485393597,[removed],0,1
813,2017-1-26,2017,1,26,10,5q7jem,[R] AI Makes 3D Models From Photos | Two Minute Papers,https://www.reddit.com/r/MachineLearning/comments/5q7jem/r_ai_makes_3d_models_from_photos_two_minute_papers/,[deleted],1485395640,[deleted],0,1
814,2017-1-26,2017,1,26,12,5q84k5,[D] does anyone know when GPUs and TPUs are arriving on Google Cloud Platform?,https://www.reddit.com/r/MachineLearning/comments/5q84k5/d_does_anyone_know_when_gpus_and_tpus_are/,TPUQuestion,1485402702,"does anyone know when GPUs and TPUs are arriving? best info I have is ""early 2017""
https://cloudplatform.googleblog.com/2016/11/announcing-GPUs-for-Google-Cloud-Platform.html",0,8
815,2017-1-26,2017,1,26,13,5q8bkx,[P] HyperBoard: A web-based dashboard for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/5q8bkx/p_hyperboard_a_webbased_dashboard_for_deep/,ACoverfit,1485405209,"**Homepage**: https://github.com/WarBean/hyperboard

**3 minutes demonstration**: [Youtube](https://youtu.be/sWmVZyRfejc) or [Bilibili](http://www.bilibili.com/video/av8215364/).

**Introduction**:

HyperBoard is a simple visualization tool to facilitate hyperparameter tuning for Deep Learning players. It helps you to

* train on a remote server and visualize training curves on the local browser
* update curves in real time
* interactively compare hundreds of training curves for hyperparameter tuning, with filtering and visibility control
* save hundreds of training records on disk and re-load them when needed

HyperBoard resembles Tensorboard and Tensorboard for MXNet. However, HyperBoard is independent from any specific Deep Learning platform and provides extra functions.",1,25
816,2017-1-26,2017,1,26,17,5q98ch,[D] Are there any ways of amalgamating vector/feature and neural-net image descriptors?,https://www.reddit.com/r/MachineLearning/comments/5q98ch/d_are_there_any_ways_of_amalgamating/,Rich700000000000,1485420713,"From the way I see it, there are basically two ways of interpreting images:

&gt; ***Vector/Feature***: SIFT, MSER Regions, Histograms, HOG Gradients, Canny Edge, and last but certainly not least Viola-Jones's HAAR Cascades. 

* OpenCV
* DLIB
* Basically everything from 2000-2011.
* Still used tons today.

&gt; ***Neural-Net***: ImageNet, AlexNet, Caffe, DeepDream, YOLO, Self-Driving Cars, etc

* Construct a network architecture.
* Gather hundreds of gigabytes if not terabytes of data.
* Train.


Now, both of these methods have their pros and cons, and both are still used today: Vector-Feature based methods are good for when you're dealing with complex data that can't just be dumped on a net and has to be fine-tuned or that has to be fast, and Neural-Nets are good for when you have mountains of data and need to find a set of core values, ie: Sorting images into categories, finding faces, finding keypoints, etc.

The question I have is: Is there a way to combine the two? All of the papers I've seen only use one or the other. I recently have come cross two papers that almost perfectly complement each other:

1. [Predicting Good Features for Image Geo-Localization Using Per-Bundle VLAD](http://hyojin.web.unc.edu/files/2015/11/ICCV15_HyoJin.pdf)

2. [Wide-Area Image Geolocalization with Aerial Reference Imagery](https://arxiv.org/pdf/1510.03743v1.pdf)

Except they use the opposite methods. Is there a way to incorporate both of them together? 

",0,2
817,2017-1-26,2017,1,26,18,5q9ddi,[D] Dealing with spelling mistakes in small sized datasets.,https://www.reddit.com/r/MachineLearning/comments/5q9ddi/d_dealing_with_spelling_mistakes_in_small_sized/,Ookispookie,1485423830,"I am currently working on nlp task where I am faced with text that contain a lot of spelling mistakes. I am not talking about slang, such as 'u' instead of 'you', Some examples: ""accurate"" is written as ""acurate"", ""efficient"" is written as ""eficient"". 

Now if the data volume was big I would just train on these spelling mistake if they occurred often enough in the dataset, or simply replace them with a unknown token. However, when I do this I am often stuck with paragraphs that miss important words and are almost impossible to read.

So to deal with this, I was thinking of maybe applying some sort of pre-processing to the text to fix these mistakes:
1) Is there any dataset/dictionary of common spelling mistakes and their fix available?
2) Fix the words by using some sort of metric (Levenshtein maybe) to see if a word is very close to a single alternative, if so, then change it into that word?

Not sure if I want to include this in the end-to-end training of the model as it then becomes very hard to investigate why I get certain results. 
So the good folks at reddit, how have you dealt with this problem before?

Edit: The method should be able to deal with spelling mistakes outside the dataset (new samples).

",10,8
818,2017-1-26,2017,1,26,18,5q9e7l,Great collection of implementations of machine learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/5q9e7l/great_collection_of_implementations_of_machine/,shaan_t,1485424300,,0,1
819,2017-1-26,2017,1,26,18,5q9eri,[P] Inception V4 Implementation in Keras Including Pre-Trained Weights!,https://www.reddit.com/r/MachineLearning/comments/5q9eri/p_inception_v4_implementation_in_keras_including/,kentsommer,1485424641,,12,53
820,2017-1-26,2017,1,26,18,5q9eu5,Tabletop Top Labeling Machine NLT110,https://www.reddit.com/r/MachineLearning/comments/5q9eu5/tabletop_top_labeling_machine_nlt110/,neostarpack,1485424685,,0,1
821,2017-1-26,2017,1,26,18,5q9f0m,Explaining machine learning models using LIME,https://www.reddit.com/r/MachineLearning/comments/5q9f0m/explaining_machine_learning_models_using_lime/,iddober,1485424777,,0,1
822,2017-1-26,2017,1,26,19,5q9fnr,[D] Intrinsically Motivated Multi-Task Reinforcement Learning with open-source Explauto library and Poppy Humanoid Robot,https://www.reddit.com/r/MachineLearning/comments/5q9fnr/d_intrinsically_motivated_multitask_reinforcement/,forestier_seb,1485425126,"Hi !

Here is our new video about the NIPS 2016 demonstration we did in Barcelona:
https://www.youtube.com/watch?v=NOLAwD4ZTW0

If you need any more explanations, here is the good place !

This research project is developed by the Flowers team at Inria and Ensta ParisTech: https://flowers.inria.fr

Here are some links:

- A Python/Explauto Tutorial: http://nbviewer.jupyter.org/github/sebastien-forestier/ExplorationAlgorithms/blob/master/main.ipynb
- A Paper on the Active Model Babbling architecture:
Forestier S, Oudeyer P-Y.  2016.  Modular Active Curiosity-Driven Discovery of Tool Use. 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).  http://sforestier.com/sites/default/files/Forestier2016Modular.pdf

- Poppy Project: an open-source 3D printed low-cost humanoid robotic platform that allows non-roboticists to quickly set up and program robotic experiments. 
https://www.poppy-project.org

- Explauto: an open-source Python library to benchmark active learning and exploration algorithms that includes already implemented real and simulated robotics setups and exploration algorithms. 
https://github.com/flowersteam/explauto

- The poster of the demo: https://hal.inria.fr/hal-01404399/

This project was conducted within a larger long-term research program at the Flowers lab on mechanisms of lifelong learning and development in machines and humans. This research program has in particular lead to a series of novel intrinsically motivated learning algorithms working on high-dimensional real robots and opening new perspectives in cognitive sciences. Papers providing this broader context are:

- Oudeyer, P-Y. and Smith. L. (2016) How Evolution may work through Curiosity-driven Developmental Process, Topics in Cognitive Science, 1-11. https://hal.inria.fr/hal-01404334

- P-Y. Oudeyer, J. Gottlieb and M. Lopes (2016) Intrinsic motivation, curiosity and learning: theory and applications in educational technologies, Progress in Brain Research, 229, pp. 257-284.
https://hal.inria.fr/hal-01404278

- Baranes, A., Oudeyer, P-Y. (2013) Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and Autonomous Systems, 61(1), pp. 49-73.
https://hal.inria.fr/hal-00788440

- Oudeyer P-Y. and Kaplan F. (2007) What is intrinsic motivation? A typology of computational approaches. Frontiers in Neurorobotics, 1:6. http://journal.frontiersin.org/article/10.3389/neuro.12.006.2007/full


Sbastien",9,28
823,2017-1-26,2017,1,26,20,5q9o87,GANs applied to build 3D models from photos,https://www.reddit.com/r/MachineLearning/comments/5q9o87/gans_applied_to_build_3d_models_from_photos/,lucasb001,1485429956,,0,1
824,2017-1-26,2017,1,26,21,5q9tsm,"[D] Convolution filters with border mode ""SAME"" introduce artifacts at image edges, how do you alleviate that?",https://www.reddit.com/r/MachineLearning/comments/5q9tsm/d_convolution_filters_with_border_mode_same/,carlthome,1485432808,"Training ConvNets on data I noticed that pretty much every feature map causes artifacts to appear at the image edges of pretty much all samples after they've been filtered. [Here's an example of a filtered sample from one of the convolution channels.](http://imgur.com/CUjpyts)

I'm using TensorFlow with `border_mode='SAME` meaning the kernel is applied to the pixels at the very edges of the image (by padding with zeros for missing pixel values), and I assume this is part of the problem.

How do you typically alleviate such artifacts without only allowing the kernel so slide up to the image borders (e.g. TensorFlow's `border_mode='VALID'`)? Do you think this might be one of the reasons 3x3 works ""better"" than let's say 9x9?

These artifacts became extra noticeable after trying larger kernel sizes, in accordance with [this](https://www.reddit.com/r/MachineLearning/comments/5pwvc9/d_deciding_on_convnet_kernel_sizes_when_designing/) thread.",8,3
825,2017-1-26,2017,1,26,21,5q9wes,Quickest way to learn tensorflow,https://www.reddit.com/r/MachineLearning/comments/5q9wes/quickest_way_to_learn_tensorflow/,RauchyBear,1485434029,[removed],0,1
826,2017-1-26,2017,1,26,21,5q9yun, rl story for yu gus,https://www.reddit.com/r/MachineLearning/comments/5q9yun/_rl_story_for_yu_gus/,Nathanielpeastli,1485435083,[removed],0,1
827,2017-1-26,2017,1,26,21,5q9zhn,Adversarial Training Methods for Semi-Supervised Text Classification,https://www.reddit.com/r/MachineLearning/comments/5q9zhn/adversarial_training_methods_for_semisupervised/,zlash92,1485435332,,0,1
828,2017-1-26,2017,1,26,23,5qaif3,Videos from NIPS 2016 are now available!,https://www.reddit.com/r/MachineLearning/comments/5qaif3/videos_from_nips_2016_are_now_available/,svnssn,1485441950,,0,1
829,2017-1-26,2017,1,26,23,5qaka6,[D] Have anyone built differentiable neural computer in keras?,https://www.reddit.com/r/MachineLearning/comments/5qaka6/d_have_anyone_built_differentiable_neural/,commafighter,1485442515,[removed],1,5
830,2017-1-26,2017,1,26,23,5qakq8,[Research] [1701.05931] Neural Offset Min-Sum Decoding,https://www.reddit.com/r/MachineLearning/comments/5qakq8/research_170105931_neural_offset_minsum_decoding/,enk100,1485442650,,3,13
831,2017-1-27,2017,1,27,0,5qalm8,[R] Variational Inference using Implicit Models (Part IV): Denoisers instead of Discriminators + iPython notebook,https://www.reddit.com/r/MachineLearning/comments/5qalm8/r_variational_inference_using_implicit_models/,fhuszar,1485442907,,5,28
832,2017-1-27,2017,1,27,0,5qauau,Strange: Increasing classification threshold eventually decreases recall,https://www.reddit.com/r/MachineLearning/comments/5qauau/strange_increasing_classification_threshold/,[deleted],1485445417,[removed],0,1
833,2017-1-27,2017,1,27,0,5qaudq,[D] Articles discussing issues with applying ML in the real world,https://www.reddit.com/r/MachineLearning/comments/5qaudq/d_articles_discussing_issues_with_applying_ml_in/,HowDeepisYourLearnin,1485445443,"There's been some discussion on this sub before on applying ML in the real world for decision making in society (who get's a loan, medical diagnosis etc). I know there has been posted some good articles discussing how ML automagically would remove bias from decisions and the dangers of applying ML blindly, but I am unable to find anything.

Does anyone have any articles on that topic they would recommend?",5,20
834,2017-1-27,2017,1,27,0,5qax5b,[D] Strange: Increasing classification threshold eventually decreases recall,https://www.reddit.com/r/MachineLearning/comments/5qax5b/d_strange_increasing_classification_threshold/,[deleted],1485446230,[deleted],0,2
835,2017-1-27,2017,1,27,1,5qazqe,[D] Sequences of events,https://www.reddit.com/r/MachineLearning/comments/5qazqe/d_sequences_of_events/,dejormo,1485446942,"Hi all, do you know a good paper dealing with predicting a probability of a next event based on previous events in a limited distribution? For example, if we have a series of [datetime, &lt;3 possible actions&gt;], predict action for a given datetime.",4,3
836,2017-1-27,2017,1,27,1,5qb1yt,Machine Learning Videos: A collection of recorded talks,https://www.reddit.com/r/MachineLearning/comments/5qb1yt/machine_learning_videos_a_collection_of_recorded/,[deleted],1485447572,[deleted],0,1
837,2017-1-27,2017,1,27,1,5qb29k,"[P] The ""Perfect Score"" Script for Kaggle Data Science Bowl 2017",https://www.reddit.com/r/MachineLearning/comments/5qb29k/p_the_perfect_score_script_for_kaggle_data/,pmigdal,1485447658,,7,8
838,2017-1-27,2017,1,27,2,5qbcmp,[D] Character Recognition Using H2O,https://www.reddit.com/r/MachineLearning/comments/5qbcmp/d_character_recognition_using_h2o/,lycan2005,1485450465,I'm pretty new in machine learning department. I found [this](https://www.kaggle.com/kobakhit/digit-recognizer/digital-recognizer-in-r) in kaggle. It was about recognizing digits using H2O in R. Its really easy to use for new guy like me and I'm curious whether it could be used for recognizing characters instead. So i tried to generate a small sample size of image from A to Z as my training data and generated another 100 images consists of random characters A to Z as my test data. Pump these 2 sets of data into H2O following methodology from the link and i only able to get 75% accuracy compared to 90+% as mentioned in the link. So i figured i must have prepared the training and test data wrongly. Is there any rule we need to follow when we prepare the training and test data? Or i simply need more training data to increase the accuracy? I'd appreciate some advice from the experts. Thanks.,13,3
839,2017-1-27,2017,1,27,2,5qbjz7,[P] An autonomous vehicle steering model in 99 lines of Python,https://www.reddit.com/r/MachineLearning/comments/5qbjz7/p_an_autonomous_vehicle_steering_model_in_99/,harvitronix,1485452374,,32,130
840,2017-1-27,2017,1,27,4,5qc7ou,[P] Image-to-Image Translation in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/5qc7ou/p_imagetoimage_translation_in_tensorflow/,Tryneus,1485458740,,0,65
841,2017-1-27,2017,1,27,5,5qcngx,Seq2Seq network for fuzzing,https://www.reddit.com/r/MachineLearning/comments/5qcngx/seq2seq_network_for_fuzzing/,neuralml,1485462968,,0,1
842,2017-1-27,2017,1,27,7,5qdbfv,Limit to the number of inputs in a network,https://www.reddit.com/r/MachineLearning/comments/5qdbfv/limit_to_the_number_of_inputs_in_a_network/,l0gr1thm1k,1485469650,[removed],0,1
843,2017-1-27,2017,1,27,8,5qdq09,"NextAI announces Google and IBM partnerships, $5.15 million in funding",https://www.reddit.com/r/MachineLearning/comments/5qdq09/nextai_announces_google_and_ibm_partnerships_515/,richardabrich,1485473810,,0,1
844,2017-1-27,2017,1,27,8,5qdslx,[D] Fastest random forests implementation/package you have ever used?,https://www.reddit.com/r/MachineLearning/comments/5qdslx/d_fastest_random_forests_implementationpackage/,magkum123,1485474566,If you have ever done benchmarking or anything it would be highly appreciated.,10,2
845,2017-1-27,2017,1,27,9,5qdwfk,Is there a better job market for ML + Hardware or ML + software right now?,https://www.reddit.com/r/MachineLearning/comments/5qdwfk/is_there_a_better_job_market_for_ml_hardware_or/,[deleted],1485475698,[removed],0,1
846,2017-1-27,2017,1,27,10,5qe8yo,"[R] "" Universal representations:The missing link between faces, text, planktons, and cat breeds"", Bilen &amp; Vedaldi 2017 [multi-task learning: image classification]",https://www.reddit.com/r/MachineLearning/comments/5qe8yo/r_universal_representationsthe_missing_link/,gwern,1485479607,,1,16
847,2017-1-27,2017,1,27,10,5qegiz,Ethics and Artificial Intelligence: The Moral Compass of a Machine,https://www.reddit.com/r/MachineLearning/comments/5qegiz/ethics_and_artificial_intelligence_the_moral/,chipbag01,1485482126,,0,1
848,2017-1-27,2017,1,27,11,5qek05,StackGAN training time?,https://www.reddit.com/r/MachineLearning/comments/5qek05/stackgan_training_time/,Simusid,1485483302,[removed],0,1
849,2017-1-27,2017,1,27,11,5qepa0,I've been given a ML task. I know nothing; Please help.,https://www.reddit.com/r/MachineLearning/comments/5qepa0/ive_been_given_a_ml_task_i_know_nothing_please/,blue_pops,1485485080,[removed],0,1
850,2017-1-27,2017,1,27,13,5qf70p,Wire Rectifying and Cutting Machine Manufacturer India,https://www.reddit.com/r/MachineLearning/comments/5qf70p/wire_rectifying_and_cutting_machine_manufacturer/,veermachine,1485491650,,0,1
851,2017-1-27,2017,1,27,16,5qfva1,Can't figure out what is wrong with this convnet,https://www.reddit.com/r/MachineLearning/comments/5qfva1/cant_figure_out_what_is_wrong_with_this_convnet/,headace_on_wall,1485502083,[removed],0,1
852,2017-1-27,2017,1,27,17,5qfzle,So my neural network which should learn an XOR gate sometimes makes strange things and I have no idea why (details in comments),https://www.reddit.com/r/MachineLearning/comments/5qfzle/so_my_neural_network_which_should_learn_an_xor/,Vipe777,1485504361,,1,1
853,2017-1-27,2017,1,27,18,5qg66v,[P] Learning by Denoising Part 1: What and why of denoising,https://www.reddit.com/r/MachineLearning/comments/5qg66v/p_learning_by_denoising_part_1_what_and_why_of/,NicolasGuacamole,1485507952,,1,39
854,2017-1-27,2017,1,27,19,5qgg1v,Preparing image dataset for analysis with t-sne,https://www.reddit.com/r/MachineLearning/comments/5qgg1v/preparing_image_dataset_for_analysis_with_tsne/,amateurmango,1485513336,[removed],0,1
855,2017-1-27,2017,1,27,19,5qghoh,Vid2Speech: Speech Reconstruction from Silent Video,https://www.reddit.com/r/MachineLearning/comments/5qghoh/vid2speech_speech_reconstruction_from_silent_video/,[deleted],1485514222,[deleted],0,1
856,2017-1-27,2017,1,27,20,5qgite,[R] Vid2Speech: Speech Reconstruction from Silent Video,https://www.reddit.com/r/MachineLearning/comments/5qgite/r_vid2speech_speech_reconstruction_from_silent/,hooba_stank_,1485514849,,7,28
857,2017-1-27,2017,1,27,20,5qgk7r,Almost no variance in activation output of network model.,https://www.reddit.com/r/MachineLearning/comments/5qgk7r/almost_no_variance_in_activation_output_of/,[deleted],1485515563,[removed],0,1
858,2017-1-27,2017,1,27,20,5qgl0u,[D] Precision starts to decrease at high SVM thresholds,https://www.reddit.com/r/MachineLearning/comments/5qgl0u/d_precision_starts_to_decrease_at_high_svm/,Zman420,1485515966,"Hi all,

Scenario: Binary classification, LibSVM (under Matlab), ~300 features, ~30k training examples, 200k+ testing examples. RBF kernel.

Here's a plot of precision versus SVM decision threshold.

http://imgur.com/hUWzVf2

As you can see, the precision goes up as expected until the threshold hits ~0.8, at which point the precision starts to drop again. 

I'm not sure if I've encountered this before...would you not usually expect precision to go towards 100% as you increase the decision threshold?",4,0
859,2017-1-27,2017,1,27,21,5qgt7u,[R][1701.07738] On Deep Learning-Based Channel Decoding,https://www.reddit.com/r/MachineLearning/comments/5qgt7u/r170107738_on_deep_learningbased_channel_decoding/,enk100,1485519769,,1,7
860,2017-1-27,2017,1,27,21,5qgtzt,[P] 140 Machine Learning Formulas by Rubens Zimbres,https://www.reddit.com/r/MachineLearning/comments/5qgtzt/p_140_machine_learning_formulas_by_rubens_zimbres/,hooba_stank_,1485520136,,3,13
861,2017-1-27,2017,1,27,22,5qgzn4,Question Regarding the DDPG Actor Gradient,https://www.reddit.com/r/MachineLearning/comments/5qgzn4/question_regarding_the_ddpg_actor_gradient/,wisp5,1485522377,[removed],0,1
862,2017-1-27,2017,1,27,22,5qh060,Short guide to deploy Machine Learning,https://www.reddit.com/r/MachineLearning/comments/5qh060/short_guide_to_deploy_machine_learning/,shugert,1485522579,,0,1
863,2017-1-27,2017,1,27,22,5qh0gz,Parallels between life and gradient descent,https://www.reddit.com/r/MachineLearning/comments/5qh0gz/parallels_between_life_and_gradient_descent/,kuntakintai,1485522697,,0,1
864,2017-1-27,2017,1,27,22,5qh42a,How to start with making a program learn how to play a simple game I created?,https://www.reddit.com/r/MachineLearning/comments/5qh42a/how_to_start_with_making_a_program_learn_how_to/,[deleted],1485524020,[removed],0,1
865,2017-1-27,2017,1,27,22,5qh4ed,[D] Is there a CNN implementation for 4D spatial data?,https://www.reddit.com/r/MachineLearning/comments/5qh4ed/d_is_there_a_cnn_implementation_for_4d_spatial/,macncookies,1485524148,"In tensorflow, the shape of the input tensor would be `[batch_size, dim0, dim1, dim2, dim3, num_channels]`. 

The plan B right now is to implement semi-separable convolutions by cyclically moving dim* to the batch axis and perfoming 3D convolutions on the remaining axes, but that's not the real deal. :-)

I don't mind the framework as long as it's anything Python or C++.

Thanks /r/MachineLearning! ",9,1
866,2017-1-27,2017,1,27,23,5qh9mk,NeuroEvolution for 2d autonomous driving lines,https://www.reddit.com/r/MachineLearning/comments/5qh9mk/neuroevolution_for_2d_autonomous_driving_lines/,[deleted],1485525963,[deleted],0,1
867,2017-1-28,2017,1,28,0,5qhjnb,"[D] Does anyone still use old ML (random forest, gbm, kNN, SVM, ...) or everyone uses deep learning?",https://www.reddit.com/r/MachineLearning/comments/5qhjnb/d_does_anyone_still_use_old_ml_random_forest_gbm/,pplonski,1485529275,,0,1
868,2017-1-28,2017,1,28,0,5qhkdc,Linear Regression with Python,https://www.reddit.com/r/MachineLearning/comments/5qhkdc/linear_regression_with_python/,[deleted],1485529493,[deleted],0,1
869,2017-1-28,2017,1,28,0,5qho07,[R] [1701.06547] Adversarial Learning for Neural Dialogue Generation,https://www.reddit.com/r/MachineLearning/comments/5qho07/r_170106547_adversarial_learning_for_neural/,badhri,1485530576,,11,23
870,2017-1-28,2017,1,28,0,5qhpyw,[D] Advanced algorithms for image upsampling?,https://www.reddit.com/r/MachineLearning/comments/5qhpyw/d_advanced_algorithms_for_image_upsampling/,varadg,1485531150,What are the good upsampling / super-resolution algorithms that have come out in the past few years?,5,1
871,2017-1-28,2017,1,28,0,5qhqkx,"TensorFlow 1.0.0-rc0 - with XLA, TF-DBG, new Android demos, Java API and more",https://www.reddit.com/r/MachineLearning/comments/5qhqkx/tensorflow_100rc0_with_xla_tfdbg_new_android/,[deleted],1485531325,[deleted],0,1
872,2017-1-28,2017,1,28,0,5qhqsd,"[N]TensorFlow 1.0.0-rc0 is available - with XLA, TF-DBG, new Android demos, Java API, Python and more",https://www.reddit.com/r/MachineLearning/comments/5qhqsd/ntensorflow_100rc0_is_available_with_xla_tfdbg/,[deleted],1485531389,[deleted],0,1
873,2017-1-28,2017,1,28,0,5qhr1a,"[N] TensorFlow 1.0.0-rc0 is available - with XLA, TF-DBG, new Android demos, Java API and more.",https://www.reddit.com/r/MachineLearning/comments/5qhr1a/n_tensorflow_100rc0_is_available_with_xla_tfdbg/,whateverr123,1485531453,,28,134
874,2017-1-28,2017,1,28,1,5qi069,[P] Linear Regression with Python,https://www.reddit.com/r/MachineLearning/comments/5qi069/p_linear_regression_with_python/,[deleted],1485534130,[deleted],7,11
875,2017-1-28,2017,1,28,2,5qig1n,Modern Feature Selection Review/Resource,https://www.reddit.com/r/MachineLearning/comments/5qig1n/modern_feature_selection_reviewresource/,mrpunta,1485538636,[removed],0,1
876,2017-1-28,2017,1,28,2,5qik43,GAN discriminator accuracy at stopping,https://www.reddit.com/r/MachineLearning/comments/5qik43/gan_discriminator_accuracy_at_stopping/,[deleted],1485539793,[removed],0,1
877,2017-1-28,2017,1,28,3,5qiuiw,Deep Deterministic Policy Gradients in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/5qiuiw/deep_deterministic_policy_gradients_in_tensorflow/,windweller,1485542645,[removed],0,1
878,2017-1-28,2017,1,28,5,5qjccz,GPUs for a new node?,https://www.reddit.com/r/MachineLearning/comments/5qjccz/gpus_for_a_new_node/,doyer,1485547546,[removed],0,1
879,2017-1-28,2017,1,28,5,5qjclo,A tiny zine about machine learning,https://www.reddit.com/r/MachineLearning/comments/5qjclo/a_tiny_zine_about_machine_learning/,mbenbernard,1485547604,,1,1
880,2017-1-28,2017,1,28,5,5qjdtp,[P] NeuroEvolution for 2d autonomous driving lines,https://www.reddit.com/r/MachineLearning/comments/5qjdtp/p_neuroevolution_for_2d_autonomous_driving_lines/,Usually_Kills_Jokes,1485547927,,6,24
881,2017-1-28,2017,1,28,5,5qjico,[D] GAN discriminator accuracy at stopping,https://www.reddit.com/r/MachineLearning/comments/5qjico/d_gan_discriminator_accuracy_at_stopping/,adagrad,1485549144,"In the idealized GAN setting, the discriminator D(x) will be unable to distinguish between training images x and generator samples G(z) at convergence.
However, in practice, we never get to convergence and stop training at some point when the samples from G look ""good enough."" At this point, D is still usually able to distinguish G(z) from x with some accuracy. Is there any work examining what the accuracy of D might say about the quality of samples from G, or is this very architecture-dependent?",6,2
882,2017-1-28,2017,1,28,5,5qjllg,Machine Learning for Everyone - Part 2: Spotting anomalous data,https://www.reddit.com/r/MachineLearning/comments/5qjllg/machine_learning_for_everyone_part_2_spotting/,Ramirond,1485550070,,0,1
883,2017-1-28,2017,1,28,5,5qjlye,Where can I get data as good as NotMNIST for all the characters and numbers for OCR. Thanks in advance.,https://www.reddit.com/r/MachineLearning/comments/5qjlye/where_can_i_get_data_as_good_as_notmnist_for_all/,knee2710,1485550169,[removed],0,1
884,2017-1-28,2017,1,28,6,5qjpl4,"[D] Question about Show, Attend and Tell paper",https://www.reddit.com/r/MachineLearning/comments/5qjpl4/d_question_about_show_attend_and_tell_paper/,showattendtell,1485551209,"Here is the paper: https://arxiv.org/pdf/1502.03044.pdf

My question is related to section 4.1, the one on Stochastic ""Hard"" Attention. Specifically, shouldn't equation 11 be the one I have written as seen here: http://i.imgur.com/JbBFUfG.jpg.

I say this because we are performing the chain rule on the equation. However, in the paper, it seems like they have an extra p(s|a) term for the second term.


EDIT: I have one more question, so in hard attention we are trying to find the most probable location right? As in we attend to the location that has the largest p(s|a)?",4,5
885,2017-1-28,2017,1,28,6,5qjsr0,"Apple joins Amazon, Google, Facebook in 'Partnership on AI' research project",https://www.reddit.com/r/MachineLearning/comments/5qjsr0/apple_joins_amazon_google_facebook_in_partnership/,[deleted],1485552152,[deleted],0,1
886,2017-1-28,2017,1,28,6,5qjwed,AISTATS 2017 accepted papers,https://www.reddit.com/r/MachineLearning/comments/5qjwed/aistats_2017_accepted_papers/,[deleted],1485553196,[deleted],0,2
887,2017-1-28,2017,1,28,7,5qk25x,Database for machine learning .jpg,https://www.reddit.com/r/MachineLearning/comments/5qk25x/database_for_machine_learning_jpg/,powisss,1485554849,[removed],0,1
888,2017-1-28,2017,1,28,7,5qk2kl,Tensorflow RNNs.,https://www.reddit.com/r/MachineLearning/comments/5qk2kl/tensorflow_rnns/,shehzaadzd,1485554974,[removed],0,1
889,2017-1-28,2017,1,28,8,5qkfl9,Question about Reproducing Kernel Hilbert Spaces.,https://www.reddit.com/r/MachineLearning/comments/5qkfl9/question_about_reproducing_kernel_hilbert_spaces/,[deleted],1485558891,[removed],0,1
890,2017-1-28,2017,1,28,11,5qlcg3,How to Do Sentiment Analysis - Intro to Deep Learning #3,https://www.reddit.com/r/MachineLearning/comments/5qlcg3/how_to_do_sentiment_analysis_intro_to_deep/,llSourcell,1485570160,,0,1
891,2017-1-28,2017,1,28,11,5qleeu,[D] Recognition of PLOS Comp Bio in ML?,https://www.reddit.com/r/MachineLearning/comments/5qleeu/d_recognition_of_plos_comp_bio_in_ml/,[deleted],1485570912,[deleted],5,3
892,2017-1-28,2017,1,28,11,5qlgie,It isnt AI yet!,https://www.reddit.com/r/MachineLearning/comments/5qlgie/it_isnt_ai_yet/,virene,1485571724,,0,1
893,2017-1-28,2017,1,28,13,5qlshp,[Project] Clickbait detection using Deep Learning (Github),https://www.reddit.com/r/MachineLearning/comments/5qlshp/project_clickbait_detection_using_deep_learning/,saurabhmathur96,1485576572,,30,208
894,2017-1-28,2017,1,28,14,5qm6ag,[D] How to calculate variational autoencoder log likelihood?,https://www.reddit.com/r/MachineLearning/comments/5qm6ag/d_how_to_calculate_variational_autoencoder_log/,foboi1122,1485582476,"I'm a little confused as to how to calculate the log likelihood of a VAE network. In figure 2 [here](https://arxiv.org/pdf/1602.02282.pdf) , there's an image of ""log-likelihood using one importance
sample during training"". I've seen this log likelihood in other papers as well. Is this the lower bound E[log P(x|z)]? If so, then is P(x|z) just the output of the decoder network given z (reconstruction)?",9,5
895,2017-1-28,2017,1,28,16,5qmipb,Training Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5qmipb/training_neural_networks/,zelex,1485588911,,0,1
896,2017-1-28,2017,1,28,19,5qn02k,Can a NN predict a min or max?,https://www.reddit.com/r/MachineLearning/comments/5qn02k/can_a_nn_predict_a_min_or_max/,pvkooten,1485599233,[removed],0,1
897,2017-1-28,2017,1,28,19,5qn2re,NN learning interaction effect,https://www.reddit.com/r/MachineLearning/comments/5qn2re/nn_learning_interaction_effect/,pvkooten,1485600829,[removed],1,1
898,2017-1-28,2017,1,28,20,5qn7pn,Play Video Games With Machine Learning Trailer - Rayman &amp;amp; Sonic - Ubisoft &amp;amp; EPITA (SCIA),https://www.reddit.com/r/MachineLearning/comments/5qn7pn/play_video_games_with_machine_learning_trailer/,perecastor,1485603633,,0,1
899,2017-1-28,2017,1,28,23,5qnrv1,What is Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/5qnrv1/what_is_machine_learning/,mohammadshadddad,1485613383,,1,1
900,2017-1-28,2017,1,28,23,5qnx7k,SSCAIT bot submission re-opened + finals streamed this Saturday 20:00 CET,https://www.reddit.com/r/MachineLearning/comments/5qnx7k/sscait_bot_submission_reopened_finals_streamed/,LetaBot,1485615493,,0,1
901,2017-1-29,2017,1,29,0,5qo4eo,5 Alexa skills to try this week,https://www.reddit.com/r/MachineLearning/comments/5qo4eo/5_alexa_skills_to_try_this_week/,smartphonemobilenews,1485617975,,0,1
902,2017-1-29,2017,1,29,1,5qogrp,Is there a free or opensource AI package I can use to make billions of dollars?,https://www.reddit.com/r/MachineLearning/comments/5qogrp/is_there_a_free_or_opensource_ai_package_i_can/,[deleted],1485622011,[removed],0,1
903,2017-1-29,2017,1,29,1,5qoj22,Is there a free or opensource AI package I can use to make billions of dollars?,https://www.reddit.com/r/MachineLearning/comments/5qoj22/is_there_a_free_or_opensource_ai_package_i_can/,[deleted],1485622731,[removed],0,1
904,2017-1-29,2017,1,29,2,5qojmb,The improbable but true story of how I joined the machine learning revolution and helped fund VR/AI startup Virtualitics,https://www.reddit.com/r/MachineLearning/comments/5qojmb/the_improbable_but_true_story_of_how_i_joined_the/,mamenode5,1485622911,[removed],0,1
905,2017-1-29,2017,1,29,2,5qotul,"Dating. Free, huge and good choice of girls",https://www.reddit.com/r/MachineLearning/comments/5qotul/dating_free_huge_and_good_choice_of_girls/,Eastondiscgedc,1485626064,[removed],0,1
906,2017-1-29,2017,1,29,4,5qpepn,[D] Using gaussian processes with varying uncertainties?,https://www.reddit.com/r/MachineLearning/comments/5qpepn/d_using_gaussian_processes_with_varying/,aplavin,1485632184,"I try to use gaussian processes for modeling (mostly smoothing and interpolation) data from measurements, which can have very different uncertainties. These uncertainties are known, but can vary a lot: e.g. some points have error +-1, some others +-20. In cases with all measurements having about the same error gaussian processes perform well, but when not - they either give too high noise estimates or give too much weight to bad points.

What are, if any, the options to take these errors into account? I read some material about modeling errors as a ""nested"" gaussian process, but this doesn't fit my case: errors for all (including nearby) points are unrelated.",6,6
907,2017-1-29,2017,1,29,5,5qpl19,The need of a open source model for AI and ML.,https://www.reddit.com/r/MachineLearning/comments/5qpl19/the_need_of_a_open_source_model_for_ai_and_ml/,loretoparisi,1485634117,,0,1
908,2017-1-29,2017,1,29,6,5qpv7s,[P] Linear Regression with Maximum Likelihood,https://www.reddit.com/r/MachineLearning/comments/5qpv7s/p_linear_regression_with_maximum_likelihood/,zebrasecond,1485637211,"I implemented a little example of simple linear regression that minimizes the least squares error through gradient descent.

I learned that for linear regression (with x^(T)*w) least squares and  maximum likelihood have the same solution ([see here](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf)). Therefore, I want to reimplement my code using maximum likelihood with gradient descent.

I want both solutions to be basically the same (**same performance**). However, I don't really know what I should do with ^(2). I guess that this is the variance of . How is ^2 ""treated"" in the least squared solution?

Is it even possible what I'm trying to do or am I missing something?",5,0
909,2017-1-29,2017,1,29,6,5qpvmm,Has someone attempted to apply neural style transfer trained on normal video (or images) to CGI rendered scenes to improve the effect of realism?,https://www.reddit.com/r/MachineLearning/comments/5qpvmm/has_someone_attempted_to_apply_neural_style/,iwantedthisusername,1485637331,[removed],0,1
910,2017-1-29,2017,1,29,10,5qrcw5,Neural Networks :A Primer into brain-inspired computational models,https://www.reddit.com/r/MachineLearning/comments/5qrcw5/neural_networks_a_primer_into_braininspired/,radicalrafi,1485655126,,0,1
911,2017-1-29,2017,1,29,11,5qri8c,Are there any works using generative adversarial networks to generate video (including audio) or just audio/speech?,https://www.reddit.com/r/MachineLearning/comments/5qri8c/are_there_any_works_using_generative_adversarial/,tuming1990,1485657135,[removed],0,1
912,2017-1-29,2017,1,29,13,5qs3ue,Unsupervised Learning made simple,https://www.reddit.com/r/MachineLearning/comments/5qs3ue/unsupervised_learning_made_simple/,virene,1485665287,,0,1
913,2017-1-29,2017,1,29,14,5qsc6o,Machine Learning with Python course 92% off - Worth it?,https://www.reddit.com/r/MachineLearning/comments/5qsc6o/machine_learning_with_python_course_92_off_worth/,[deleted],1485668656,[deleted],0,1
914,2017-1-29,2017,1,29,18,5qt1gp,[D] Where are order-preserving embeddings for sentences used?,https://www.reddit.com/r/MachineLearning/comments/5qt1gp/d_where_are_orderpreserving_embeddings_for/,wildtales,1485681850,"I have read a few papers that show that a weighted average of word embeddings outperforms more complicated embeddings such as Skip-thought vectors for many tasks. One such paper is
https://openreview.net/pdf?id=SyK00v5xx

This makes me wonder, if there is any meaningful application, where order preserving embeddings might be more useful.  Any suggestions?
",7,7
915,2017-1-29,2017,1,29,19,5qt7b2,[R]Making A.I. Systems that See the World as Humans Do,https://www.reddit.com/r/MachineLearning/comments/5qt7b2/rmaking_ai_systems_that_see_the_world_as_humans_do/,hooba_stank_,1485685510,,0,12
916,2017-1-29,2017,1,29,19,5qt7b4,[P] Learning by Denoising Part 2. Connection between data distribution and denoising function,https://www.reddit.com/r/MachineLearning/comments/5qt7b4/p_learning_by_denoising_part_2_connection_between/,NicolasGuacamole,1485685511,,1,27
917,2017-1-29,2017,1,29,19,5qt7oh,[D] What is the difference between using the convLSTM layer and using CNN LSTM sequentially in Keras?,https://www.reddit.com/r/MachineLearning/comments/5qt7oh/d_what_is_the_difference_between_using_the/,rulerofthehell,1485685764,"Hi there,I'm a machine learning newbie and I was a bit confused between the two types of approached used in the keras examples [conv_lstm.py](https://github.com/fchollet/keras/blob/master/examples/conv_lstm.py) and [imdb_cnn_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_cnn_lstm.py), both are approaches used for finding out the spatiotemporal pattern in a dataset which has both [like video or audio file, I assume]. 

While I understand that imdb_cnn_lstm.py is used for classification task and conv_lstm.py is used for prediction task, I'm confused what is the advantages and disadvantages of one network over the other.


Let's say for example I want to train a CRNN network to classify audio files like [musictaggerCRNN ](https://keras.io/applications/#musictaggercrnn)and I want the convolutional part to get the features within each time frame and while the recurrent part to recognize the pattern between each of these time frame's convolutional features. If I add a dense layer at the end of the model provided by conv_lstm.py wont it start classifying like imdb_cnn_lstm.py?",7,21
918,2017-1-29,2017,1,29,20,5qtcv9,Why people use architectures (like VGG) for image classification tasks other than ImageNet challenge?,https://www.reddit.com/r/MachineLearning/comments/5qtcv9/why_people_use_architectures_like_vgg_for_image/,DL-Z_ftw,1485688893,[removed],1,1
919,2017-1-29,2017,1,29,20,5qtedz,Sequence to sequence learning in games. How to deal with discrete and random variables,https://www.reddit.com/r/MachineLearning/comments/5qtedz/sequence_to_sequence_learning_in_games_how_to/,CuthbertJungle,1485689807,[removed],0,1
920,2017-1-29,2017,1,29,21,5qtjt2,How do GANs intuitively work?,https://www.reddit.com/r/MachineLearning/comments/5qtjt2/how_do_gans_intuitively_work/,off99555,1485692834,,0,1
921,2017-1-29,2017,1,29,21,5qtnfh,[Question] Currently writing my MSc thesis on Machine learning and I'm afraid the subject I've chosen may be too complicated.,https://www.reddit.com/r/MachineLearning/comments/5qtnfh/question_currently_writing_my_msc_thesis_on/,Asdrubal_,1485694752,[removed],0,1
922,2017-1-29,2017,1,29,22,5qtpns,[Discussion] FBLearner Flow open alternatives?,https://www.reddit.com/r/MachineLearning/comments/5qtpns/discussion_fblearner_flow_open_alternatives/,fgilad,1485695816,"In last May, FB posted about FBLearner Flow, Their framework for ""AI backbone"" to manage the ML experiments and development.
Since then - this area seems quiet - no open source equivalents, no similar products, open or closed source as far as I was able to find. Anyone knows about an alternative? Anyone interested in developing an open alternative?
",21,25
923,2017-1-29,2017,1,29,22,5qtqsw,Foundations of Machine Learning Boot Camp | Simons Institute for the Theory of Computing,https://www.reddit.com/r/MachineLearning/comments/5qtqsw/foundations_of_machine_learning_boot_camp_simons/,Enforcer0,1485696385,,0,1
924,2017-1-30,2017,1,30,0,5qu9cp,[D] How to normalize data with Inverse Gaussian Distribution?,https://www.reddit.com/r/MachineLearning/comments/5qu9cp/d_how_to_normalize_data_with_inverse_gaussian/,jiminiminimini,1485703822,"I am trying to learn some features from a dataset, which, from its histogram, looks like it has inverse gaussian distribution. How can I convert it to a gaussian distribution and normalize it?",13,7
925,2017-1-30,2017,1,30,0,5qu9cq,Histogram of Oriented Gradients with SVM in Tensorflow.,https://www.reddit.com/r/MachineLearning/comments/5qu9cq/histogram_of_oriented_gradients_with_svm_in/,cprakashagr,1485703822,,0,1
926,2017-1-30,2017,1,30,2,5quv3m,Performance Trends in AI,https://www.reddit.com/r/MachineLearning/comments/5quv3m/performance_trends_in_ai/,Anshel_Liu,1485710783,,0,2
927,2017-1-30,2017,1,30,2,5qv0rz,Machine Learning with Python course 92% off ($49) - Worth it? (x-post /r/LearnMachineLearning),https://www.reddit.com/r/MachineLearning/comments/5qv0rz/machine_learning_with_python_course_92_off_49/,Gidonka,1485712451,,1,1
928,2017-1-30,2017,1,30,3,5qv7mb,Keras example for 3D Convnet?,https://www.reddit.com/r/MachineLearning/comments/5qv7mb/keras_example_for_3d_convnet/,cctap,1485714436,[removed],0,1
929,2017-1-30,2017,1,30,3,5qvcik,Binary Logistic Regression data set with two variables,https://www.reddit.com/r/MachineLearning/comments/5qvcik/binary_logistic_regression_data_set_with_two/,ohhheay,1485715879,[removed],0,1
930,2017-1-30,2017,1,30,4,5qvgkz,Data Science Start-Ups in Focus: H2O.ai - DZone Big Data,https://www.reddit.com/r/MachineLearning/comments/5qvgkz/data_science_startups_in_focus_h2oai_dzone_big/,Sibanjan,1485717071,,0,1
931,2017-1-30,2017,1,30,4,5qvmvu,The First Steps to Analyzing Data in R - DZone Big Data,https://www.reddit.com/r/MachineLearning/comments/5qvmvu/the_first_steps_to_analyzing_data_in_r_dzone_big/,Sibanjan,1485718911,,0,1
932,2017-1-30,2017,1,30,5,5qvw0o,My best #deeplearning resources for the 2 past years up to now,https://www.reddit.com/r/MachineLearning/comments/5qvw0o/my_best_deeplearning_resources_for_the_2_past/,GChe,1485721497,,0,1
933,2017-1-30,2017,1,30,6,5qwd97,NIPS 2017 Location Change Petition,https://www.reddit.com/r/MachineLearning/comments/5qwd97/nips_2017_location_change_petition/,alexmlamb,1485726112,,0,1
934,2017-1-30,2017,1,30,7,5qwlxo,Artificial intelligence colorizing my images...,https://www.reddit.com/r/MachineLearning/comments/5qwlxo/artificial_intelligence_colorizing_my_images/,mapo87,1485728583,,0,1
935,2017-1-30,2017,1,30,7,5qwpjl,Wht_kind_f_fr_nd_gd_dting_I_hv_hsn?__first_str_in_th_Intrnt,https://www.reddit.com/r/MachineLearning/comments/5qwpjl/wht_kind_f_fr_nd_gd_dting_i_hv_hsn/,Judechilrai,1485729633,[removed],0,1
936,2017-1-30,2017,1,30,8,5qwvwo,[D] Using the gaussian function (RBF) to build an SVM for classification,https://www.reddit.com/r/MachineLearning/comments/5qwvwo/d_using_the_gaussian_function_rbf_to_build_an_svm/,sprymill,1485731458,"Let's say we have a classification problem with m=1000 training examples and n=15 features.

Say the training points are not linearly-separable, so we decide to first map our 15 features to a bigger set of more complex features (similar to mapping to polynomial features for linear regression).

Here's how we do it:   
We select a set of P points in the input space as 'landmarks'. For each training example X[i], we compute P new values by evaluating the 'nearness' of point X[i] to each landmark. The 'similarity' is computed using the radial basis function (RBF), also known as the gaussian function.

If the set of landmarks chosen is the entire set of training points, then P=1000 and we are mapping our features from R^15 to R^1000. Now we build an SVM to find the optimal hyperplane to separate our points in this new feature-space.

Is this approach similar to using the RBF as a kernel function in the SVM?  
In the kernel approach, we are using the RBF to compute the pairwise dot-products of features. The features themselves lie in an implicit higher-dimensional space.

In the first approach, we are using the RBF to perform the mapping to higher-dimensional space **explicitly**.  
I understand that the kernel approach is commonly used, but is there something wrong with the mapping approach?  
Why is the kernel approach better?",12,5
937,2017-1-30,2017,1,30,9,5qxgrv,[Project] Seeking contributors to a NLP open source project to perform database queries in natural language,https://www.reddit.com/r/MachineLearning/comments/5qxgrv/project_seeking_contributors_to_a_nlp_open_source/,fabiomendescom,1485737363,"I am starting an open source project called ""JustQuery.me"". The objective is to allow users to perform natural language queries which will be translated to different database or datasources. There will be heavy ML to train a set of possible questions and allow the engine to ask further refining questions if necessary. Anyone interested please check the github project at https://github.com/justquery-me/justqueryme. 

This engine will use neural networks to have multiple outputs that will tag the query into difference dimensions (data source requested, filtering, order, grouping, etc)

If you are interested in contributing please subscribe to the mailing list in the github page.

High level architecture can be found here -&gt; https://groups.google.com/forum/#!topic/justqueryme-development/PmJi7Pwz4SA",6,4
938,2017-1-30,2017,1,30,10,5qxoaz,[R] [1701.07875] Wasserstein GAN,https://www.reddit.com/r/MachineLearning/comments/5qxoaz/r_170107875_wasserstein_gan/,ajmooch,1485739460,,162,148
939,2017-1-30,2017,1,30,11,5qxwqk,[D] pBLSTM implementation,https://www.reddit.com/r/MachineLearning/comments/5qxwqk/d_pblstm_implementation/,dejormo,1485741946,"In LAS [paper](https://arxiv.org/pdf/1508.01211v2.pdf), equation 5 is talking about concatenating data before feeding into the next layer. What does it mean? How to implement this with Tensorflow/Keras?",6,3
940,2017-1-30,2017,1,30,12,5qy85l,Does anyone know the name or origin of this feature selection algorithm?,https://www.reddit.com/r/MachineLearning/comments/5qy85l/does_anyone_know_the_name_or_origin_of_this/,DaftPotato,1485745616,[removed],0,1
941,2017-1-30,2017,1,30,15,5qz5hi,Prediction Zone: Using R With Shiny - DZone Big Data,https://www.reddit.com/r/MachineLearning/comments/5qz5hi/prediction_zone_using_r_with_shiny_dzone_big_data/,Sibanjan,1485757856,,0,1
942,2017-1-30,2017,1,30,16,5qz9qj,This noob just configured tensorflow to use his GPU and couldn't be happier with the time saved.,https://www.reddit.com/r/MachineLearning/comments/5qz9qj/this_noob_just_configured_tensorflow_to_use_his/,MaxConners,1485759844,[removed],0,1
943,2017-1-30,2017,1,30,19,5qzv94,Petition to change the location of NIPS 2017 from due to Trump immigration rules,https://www.reddit.com/r/MachineLearning/comments/5qzv94/petition_to_change_the_location_of_nips_2017_from/,[deleted],1485771209,[deleted],0,1
944,2017-1-30,2017,1,30,19,5qzvd7,Petition to change the location of NIPS 2017 due to Trump immigration ruling,https://www.reddit.com/r/MachineLearning/comments/5qzvd7/petition_to_change_the_location_of_nips_2017_due/,DanielleMolloy,1485771267,,0,2
945,2017-1-30,2017,1,30,20,5r09de,[D] How much better is DL than other ML methods?,https://www.reddit.com/r/MachineLearning/comments/5r09de/d_how_much_better_is_dl_than_other_ml_methods/,jensgk,1485777552,,3,0
946,2017-1-30,2017,1,30,22,5r0mv1,How to take advantage of machine learning for human learning,https://www.reddit.com/r/MachineLearning/comments/5r0mv1/how_to_take_advantage_of_machine_learning_for/,Alexey75,1485782212,,0,1
947,2017-1-30,2017,1,30,22,5r0t0o,A handy little intro to Machine Learning at TEDx Nyenrode,https://www.reddit.com/r/MachineLearning/comments/5r0t0o/a_handy_little_intro_to_machine_learning_at_tedx/,[deleted],1485784155,[deleted],0,1
948,2017-1-30,2017,1,30,23,5r0xj3,A handy little intro to Machine Learning at TEDx Nyenrode,https://www.reddit.com/r/MachineLearning/comments/5r0xj3/a_handy_little_intro_to_machine_learning_at_tedx/,Arminastepan,1485785454,,0,1
949,2017-1-30,2017,1,30,23,5r0yff,Wht_kind_f_fr_nd_gd_dting_I_hv_fund?__rl_lif_str_in_th_Intrnt,https://www.reddit.com/r/MachineLearning/comments/5r0yff/wht_kind_f_fr_nd_gd_dting_i_hv_fund_/,Grahamsesep,1485785695,[removed],0,1
950,2017-1-30,2017,1,30,23,5r14yd,[Discussion] Machine Learning - WAYR (What Are You Reading) - Week 18,https://www.reddit.com/r/MachineLearning/comments/5r14yd/discussion_machine_learning_wayr_what_are_you/,Mandrathax,1485787544,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

|Previous weeks|
|--------------|
|[Week 1](https://www.reddit.com/r/MachineLearning/comments/4qyjiq/machine_learning_wayr_what_are_you_reading_week_1/)|
|[Week 2](https://www.reddit.com/r/MachineLearning/comments/4s2xqm/machine_learning_wayr_what_are_you_reading_week_2/)|
|[Week 3](https://www.reddit.com/r/MachineLearning/comments/4t7mqm/machine_learning_wayr_what_are_you_reading_week_3/)|
|[Week 4](https://www.reddit.com/r/MachineLearning/comments/4ub2kw/machine_learning_wayr_what_are_you_reading_week_4/)|
|[Week 5](https://www.reddit.com/r/MachineLearning/comments/4xomf7/machine_learning_wayr_what_are_you_reading_week_5/)|
|[Week 6](https://www.reddit.com/r/MachineLearning/comments/4zcyvk/machine_learning_wayr_what_are_you_reading_week_6/)|
|[Week 7](https://www.reddit.com/r/MachineLearning/comments/52t6mo/machine_learning_wayr_what_are_you_reading_week_7/)|
|[Week 8](https://www.reddit.com/r/MachineLearning/comments/53heol/machine_learning_wayr_what_are_you_reading_week_8/)|
|[Week 9](https://www.reddit.com/r/MachineLearning/comments/54kvsu/machine_learning_wayr_what_are_you_reading_week_9/)|
|[Week 10](https://www.reddit.com/r/MachineLearning/comments/56s2oa/discussion_machine_learning_wayr_what_are_you/)|
|[Week 11](https://www.reddit.com/r/MachineLearning/comments/57xw56/discussion_machine_learning_wayr_what_are_you/)|
|[Week 12](https://www.reddit.com/r/MachineLearning/comments/5acb1t/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 13](https://www.reddit.com/r/MachineLearning/comments/5cwfb6/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 14](https://www.reddit.com/r/MachineLearning/comments/5fc5mh/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 15](https://www.reddit.com/r/MachineLearning/comments/5hy4ur/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 16](https://www.reddit.com/r/MachineLearning/comments/5kd6vd/d_machine_learning_wayr_what_are_you_reading_week/)|
|[Week 17](https://www.reddit.com/r/MachineLearning/comments/5ob7dx/discussion_machine_learning_wayr_what_are_you/)|

Most upvoted papers last week :

[Maximum Entropy Flow Networks](https://openreview.net/forum?id=H1acq85gx)

Besides that, there are no rules, have fun.
",5,16
951,2017-1-31,2017,1,31,0,5r1gzw,[R] GitHub contributions graph: 6 handshakes theory and PageRank,https://www.reddit.com/r/MachineLearning/comments/5r1gzw/r_github_contributions_graph_6_handshakes_theory/,markovtsev,1485790882,,0,13
952,2017-1-31,2017,1,31,0,5r1hib,[P] AFK-MC2 (r/MachineLearning's 2016 Best Paper) implementation on CUDA,https://www.reddit.com/r/MachineLearning/comments/5r1hib/p_afkmc2_rmachinelearnings_2016_best_paper/,markovtsev,1485791034,,6,32
953,2017-1-31,2017,1,31,1,5r1oqj,Retail Brands Should Start Making Chatbots,https://www.reddit.com/r/MachineLearning/comments/5r1oqj/retail_brands_should_start_making_chatbots/,bogsformer,1485792943,,0,1
954,2017-1-31,2017,1,31,1,5r1qug,Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial for Beginners,https://www.reddit.com/r/MachineLearning/comments/5r1qug/handwritten_digits_classification_an_opencv_c/,spmallick,1485793505,,0,1
955,2017-1-31,2017,1,31,1,5r1unh,How to best achieve sharpness using sigmoids/step functions within NN cost functions,https://www.reddit.com/r/MachineLearning/comments/5r1unh/how_to_best_achieve_sharpness_using_sigmoidsstep/,thatguydr,1485794536,"Hi all,

I frequently have to come up with NN loss functions that are fairly non-standard, and I've noticed that several papers seem to (silently) have the same issue I've been facing.

I know you can perform a step function (or a top k, or the ""differential indices"" of what/where) by using a sigmoid. This is often a bit better than using step functions/conditionals because the sigmoid, being non-flat, lets the function ""know"" where to push itself because it has derivatives that ""point"" in the appropriate direction (unlike step functions, which are flat everywhere).

My issue is that I often need that sigmoid to be *extremely* sharp because the cost/loss changes significantly across the border. I've been doing this so far via batch-sharpening the sigmoid (finding the largest input and scaling the sigmoid to be relatively sharp across that scape). I've also tried adding a (small) sigmoid to a step function (so I can get 99% of the step but keep the differentiability). Both methods are not without headaches, however.

Does anyone know if this is a standard ""solved"" problem? I've never seen anyone post code sharpening the sigmoid to be ""more accurate"" (more like a step function). I know I can't be the only one with this problem, so are there any other options I've missed?

(I typically use TensorFlow or Theano but am comfortable with Torch or any wrapper, if people have alternative solutions.)

Thanks.",2,2
956,2017-1-31,2017,1,31,1,5r1ux5,How are Australian Universities for ML/Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/5r1ux5/how_are_australian_universities_for_mldeep/,[deleted],1485794608,[removed],0,1
957,2017-1-31,2017,1,31,1,5r1xlx,Is IntelliSys a legit conference?,https://www.reddit.com/r/MachineLearning/comments/5r1xlx/is_intellisys_a_legit_conference/,darkconfidantislife,1485795325,[removed],0,1
958,2017-1-31,2017,1,31,2,5r1zgx,"[D] Is ""IntelliSys"" a legitimate conference?",https://www.reddit.com/r/MachineLearning/comments/5r1zgx/d_is_intellisys_a_legitimate_conference/,[deleted],1485795817,[deleted],12,2
959,2017-1-31,2017,1,31,2,5r20gt,Is cs231n available in pdf or book form?,https://www.reddit.com/r/MachineLearning/comments/5r20gt/is_cs231n_available_in_pdf_or_book_form/,idg101,1485796062,I would like to print this out and make a nice hardbound copy.  I can simply print all the webpages but wondered if someone made it into a nice pdf or book already.  Thanks!,0,2
960,2017-1-31,2017,1,31,2,5r22lo,AISTATS 2017 Accepted Papers,https://www.reddit.com/r/MachineLearning/comments/5r22lo/aistats_2017_accepted_papers/,[deleted],1485796581,[deleted],0,1
961,2017-1-31,2017,1,31,2,5r22pj,[D] How are Australian Universities for ML/Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/5r22pj/d_how_are_australian_universities_for_mldeep/,Six_Machine,1485796612,"Like the title says. I am considering a masters program in computer science with a strong focus on Machine Learning and Deep Learning. I was not able to find a research group from australia in [this list](http://deeplearning.net/deep-learning-research-groups-and-labs/) or in this subreddit.

On a sidenote, how is the job scenario in machine learning and how likely are they to hire masters students?",24,23
962,2017-1-31,2017,1,31,2,5r27wu,[D] language model for predicting punctuation,https://www.reddit.com/r/MachineLearning/comments/5r27wu/d_language_model_for_predicting_punctuation/,djc1000,1485797927,"A while back, I saw a model posted here and on GitHub, that used a recurrent network to predict punctuation. But now I can't find the github page or the paper. Can anyone help? Thanks.",4,2
963,2017-1-31,2017,1,31,4,5r2rs5,Multi-task learning - getting one task to help another,https://www.reddit.com/r/MachineLearning/comments/5r2rs5/multitask_learning_getting_one_task_to_help/,deephive,1485803073,[removed],0,1
964,2017-1-31,2017,1,31,5,5r3ame,"Soumith Chintala, Research Engineer at Facebook - Machine Intelligence Summit NY 2016 on GANs",https://www.reddit.com/r/MachineLearning/comments/5r3ame/soumith_chintala_research_engineer_at_facebook/,validated1,1485807906,,0,1
965,2017-1-31,2017,1,31,5,5r3dmc,[N] Deep learning dead languages,https://www.reddit.com/r/MachineLearning/comments/5r3dmc/n_deep_learning_dead_languages/,materialvision,1485808678,,14,58
966,2017-1-31,2017,1,31,5,5r3eoa,The Data That Turned the World Upside Down,https://www.reddit.com/r/MachineLearning/comments/5r3eoa/the_data_that_turned_the_world_upside_down/,aprstar,1485808969,,1,1
967,2017-1-31,2017,1,31,6,5r3it9,[D] How do exogenous/endogenous variables differ from independent/dependent variables in a causal system?,https://www.reddit.com/r/MachineLearning/comments/5r3it9/d_how_do_exogenousendogenous_variables_differ/,[deleted],1485810078,[deleted],0,1
968,2017-1-31,2017,1,31,6,5r3ivs,Asilomar AI principles,https://www.reddit.com/r/MachineLearning/comments/5r3ivs/asilomar_ai_principles/,johnmountain,1485810095,,0,2
969,2017-1-31,2017,1,31,6,5r3q06,[D] L infinity regularization,https://www.reddit.com/r/MachineLearning/comments/5r3q06/d_l_infinity_regularization/,davikrehalt,1485811987,Has the maximum weight been used as regularization successfully before? Or is weight clipping a better approach?,4,3
970,2017-1-31,2017,1,31,6,5r3qro,VAE extensions,https://www.reddit.com/r/MachineLearning/comments/5r3qro/vae_extensions/,dataislyfe,1485812186,[removed],0,1
971,2017-1-31,2017,1,31,6,5r3rci,[R] VAE Extensions,https://www.reddit.com/r/MachineLearning/comments/5r3rci/r_vae_extensions/,dataislyfe,1485812355,"I know people have done work on representing the approximate posterior efficiently for VAEs, and in a more expressive way (i.e., Normalizing Flows/Inverse Auto-regressive Flows). How about transformations like Fourier transforms to learn in Fourier space? Would something like that be possible?
",0,3
972,2017-1-31,2017,1,31,7,5r42g8,flair:News Coursera/U-Wash Fail - Machine Learning Specialization final two classes DOA,https://www.reddit.com/r/MachineLearning/comments/5r42g8/flairnews_courserauwash_fail_machine_learning/,[deleted],1485815319,[removed],0,1
973,2017-1-31,2017,1,31,7,5r43rr,Chemists pin hopes on deep learning for drug discovery | Chemical &amp; Engineering News,https://www.reddit.com/r/MachineLearning/comments/5r43rr/chemists_pin_hopes_on_deep_learning_for_drug/,Grumbit,1485815678,,0,1
974,2017-1-31,2017,1,31,7,5r44ys,Coursera/U-Wash Fail - Machine Learning Specialization final two classes DOA,https://www.reddit.com/r/MachineLearning/comments/5r44ys/courserauwash_fail_machine_learning/,moosekingbob,1485816017,[removed],0,1
975,2017-1-31,2017,1,31,8,5r4bxb,DQN in Keras for Cartpole Game - need help!,https://www.reddit.com/r/MachineLearning/comments/5r4bxb/dqn_in_keras_for_cartpole_game_need_help/,[deleted],1485818018,[removed],0,1
976,2017-1-31,2017,1,31,8,5r4fb0,Coursera Machine Learning specialization courses 5 and 6 cancelled,https://www.reddit.com/r/MachineLearning/comments/5r4fb0/coursera_machine_learning_specialization_courses/,sliangs,1485819013,,0,1
977,2017-1-31,2017,1,31,9,5r4nlm,[P] ML for web scraped e-commerce data,https://www.reddit.com/r/MachineLearning/comments/5r4nlm/p_ml_for_web_scraped_ecommerce_data/,scraping_ML,1485821389,"I have several data sets that include product level transaction data for a large e-commerce website collected through web scraping. I have data including item IDs, titles, prices, quantity sold of the item, the item's category, and pretty much anything else that you might find on a product listing on an e-commerce website.

The data is scraped on a regular basis such that I have a time series for the items.

I'd like to use machine learning to build a data product using this data and would be interested in hearing what ideas this group may have.",2,2
978,2017-1-31,2017,1,31,9,5r4wx7,Clarification about CNNs &amp; filter dimensions?,https://www.reddit.com/r/MachineLearning/comments/5r4wx7/clarification_about_cnns_filter_dimensions/,[deleted],1485824112,[removed],0,1
979,2017-1-31,2017,1,31,10,5r554h,[R] PathNet: Evolution Channels Gradient Descent in Super Neural Networks,https://www.reddit.com/r/MachineLearning/comments/5r554h/r_pathnet_evolution_channels_gradient_descent_in/,hardmaru,1485826617,,2,36
980,2017-1-31,2017,1,31,11,5r5jn6,[P] Neuroevolution solution in Python (with writeup) for OpenAI Gym LunarLander-v2,https://www.reddit.com/r/MachineLearning/comments/5r5jn6/p_neuroevolution_solution_in_python_with_writeup/,CodeReclaimers,1485830932,,8,50
981,2017-1-31,2017,1,31,12,5r5sr5,[P] Conditioned DCGAN to transform faces from male to female and vice versa,https://www.reddit.com/r/MachineLearning/comments/5r5sr5/p_conditioned_dcgan_to_transform_faces_from_male/,david-gpu,1485833736,,2,11
982,2017-1-31,2017,1,31,13,5r5z83,Textual analysis learning method,https://www.reddit.com/r/MachineLearning/comments/5r5z83/textual_analysis_learning_method/,[deleted],1485835863,[removed],0,1
983,2017-1-31,2017,1,31,13,5r61dv,CMU: Deep Reinforcement Learning and Control Course Page (10703),https://www.reddit.com/r/MachineLearning/comments/5r61dv/cmu_deep_reinforcement_learning_and_control/,howdygoop,1485836601,,0,1
984,2017-1-31,2017,1,31,13,5r67ah,"[D] What thing within machine learning, deep learning would people find helpful? I have experience in both ML&amp;DL, and time to write / curate content. Looking for ideas...",https://www.reddit.com/r/MachineLearning/comments/5r67ah/d_what_thing_within_machine_learning_deep/,keshav57,1485838708,,8,4
985,2017-1-31,2017,1,31,15,5r6inp,NIPS 2016 Videos,https://www.reddit.com/r/MachineLearning/comments/5r6inp/nips_2016_videos/,[deleted],1485842934,[deleted],0,1
986,2017-1-31,2017,1,31,15,5r6o2f,[n] Humans beaten by ML tool in No-limit Texas Holdem,https://www.reddit.com/r/MachineLearning/comments/5r6o2f/n_humans_beaten_by_ml_tool_in_nolimit_texas_holdem/,madmooseman,1485845205,,34,120
987,2017-1-31,2017,1,31,16,5r6qiz,Has anyone tried low-data deep learning like training an ImageNet CNN on just 1% of the dataset?,https://www.reddit.com/r/MachineLearning/comments/5r6qiz/has_anyone_tried_lowdata_deep_learning_like/,teling,1485846320,[removed],0,1
988,2017-1-31,2017,1,31,16,5r6qqh,Has anyone used dilated convolutions for classification/labeling tasks?,https://www.reddit.com/r/MachineLearning/comments/5r6qqh/has_anyone_used_dilated_convolutions_for/,differenti,1485846428,[removed],0,1
989,2017-1-31,2017,1,31,18,5r75a2,[R] [1611.01353] Information Dropout: learning optimal representations through noise,https://www.reddit.com/r/MachineLearning/comments/5r75a2/r_161101353_information_dropout_learning_optimal/,TheFlyingDrildo,1485853958,,2,38
990,2017-1-31,2017,1,31,18,5r75xc,Shallow and Deep Networks Intrusion Detection System: A Taxonomy and Survey,https://www.reddit.com/r/MachineLearning/comments/5r75xc/shallow_and_deep_networks_intrusion_detection/,[deleted],1485854274,[deleted],0,1
991,2017-1-31,2017,1,31,18,5r775w,[Research]Shallow and Deep Networks Intrusion Detection System: A Taxonomy and Survey,https://www.reddit.com/r/MachineLearning/comments/5r775w/researchshallow_and_deep_networks_intrusion/,Alex0789,1485854893,,1,2
992,2017-1-31,2017,1,31,18,5r777v,[R] [1602.03686] Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction,https://www.reddit.com/r/MachineLearning/comments/5r777v/r_160203686_medical_concept_representation/,TheFlyingDrildo,1485854918,,1,3
993,2017-1-31,2017,1,31,19,5r7cep,A blog I wrote on the evolution of machine intelligence.,https://www.reddit.com/r/MachineLearning/comments/5r7cep/a_blog_i_wrote_on_the_evolution_of_machine/,ijaysonx,1485857331,,0,1
994,2017-1-31,2017,1,31,22,5r8ack,Predict Customer Churn Using R and Tableau - DZone Big Data,https://www.reddit.com/r/MachineLearning/comments/5r8ack/predict_customer_churn_using_r_and_tableau_dzone/,Sibanjan,1485870593,,0,1
995,2017-1-31,2017,1,31,23,5r8m60,Machine Learning Videos: A collection of recorded talks,https://www.reddit.com/r/MachineLearning/comments/5r8m60/machine_learning_videos_a_collection_of_recorded/,[deleted],1485874402,[deleted],0,1
