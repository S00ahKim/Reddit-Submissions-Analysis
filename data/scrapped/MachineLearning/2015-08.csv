,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,9,3fcwh6,github.com,"My Implementation of CNNs in Theano. Good for a newbie, boring for professionals.",https://www.reddit.com/r/MachineLearning/comments/3fcwh6/my_implementation_of_cnns_in_theano_good_for_a/,[deleted],1438389943,,0,1,False,default,,,,,
1,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,9,3fcxld,ragavvenkatesan.github.io,"Versatile but basic theano implementation of CNN with the usual upgrades: Good for a newbie, too basic for a professional.",https://www.reddit.com/r/MachineLearning/comments/3fcxld/versatile_but_basic_theano_implementation_of_cnn/,procarastinizer,1438390546,,1,4,False,default,,,,,
2,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,11,3fd75n,self.MachineLearning,Anyone using Julia for ML implementation?,https://www.reddit.com/r/MachineLearning/comments/3fd75n/anyone_using_julia_for_ml_implementation/,MusicIsLife1995,1438395970,I'm curious about Julia and its possibilities in being the next best platform.,13,8,False,self,,,,,
3,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,11,3fd7jy,self.MachineLearning,Database of Labelled Data for AI Research,https://www.reddit.com/r/MachineLearning/comments/3fd7jy/database_of_labelled_data_for_ai_research/,llort_atton,1438396218,"I remember a while ago finding a website devoted to a public database meant for AI and machine learning research. There was a lot of labelled data there, and I've been considering starting a project that might need some. I can't remember what the website was called and I can't find any information about it now. Does anyone know where I could find something like this?",4,2,False,self,,,,,
4,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,15,3fdu9u,videolectures.net,Beyond epsilon-greedy exploration - Benjamin Van Roy talk at RLDM,https://www.reddit.com/r/MachineLearning/comments/3fdu9u/beyond_epsilongreedy_exploration_benjamin_van_roy/,Bayes-Ian,1438411170,,0,6,False,http://a.thumbs.redditmedia.com/Cgo2QQOmp0kx2EUfqS4YPGOh1GMFY08vrWXx6LpyKH8.jpg,,,,,
5,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,17,3fe2f2,self.MachineLearning,What are some tools and best practices to become more familiar with your data?,https://www.reddit.com/r/MachineLearning/comments/3fe2f2/what_are_some_tools_and_best_practices_to_become/,onewugtwowugs,1438418791,"I realize more and more that I usually make several assumptions about my data that I dont really have support for. For instance, that my training set is similar to my development set, or that my extracted features indeed are good discriminative indicators.

My current way of getting familiar with the data is basically to browse through a large number of instances until I get a feeling of how it's built. I would like to get a better, more robust workflow of looking closely at the data before I start making assumptions of its structure.

I guess one way would be to perform dimensionality reduction on ones feature representations using something like t-SNE, and make sure the classes looks somewhat separated. This assumes though that I already have a feature representation. If possible, especially for text data, I would like to find tools that will help me before I construct such representations.",2,4,False,self,,,,,
6,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,17,3fe2nf,self.MachineLearning,Is Digital Reasoning's neural network as impressive as it sounds?,https://www.reddit.com/r/MachineLearning/comments/3fe2nf/is_digital_reasonings_neural_network_as/,Buck-Nasty,1438419054,"So this is a question from a layperson, is their claim to have trained the [largest neural network a](http://www.digitalreasoning.com/buzz/digital-reasoning-trains-worlds-largest-neural-network-shatters-record-previously-set-by-google.1672770) serious achievement or merely marketing hype?",3,0,False,self,,,,,
7,MachineLearning,t5_2r3gv,2015-8-1,2015,8,1,20,3fednw,self.MachineLearning,Why is Lua such a popular language for machine learning at the major tech companies?,https://www.reddit.com/r/MachineLearning/comments/3fednw/why_is_lua_such_a_popular_language_for_machine/,jackbrucesimpson,1438429970,"I recently found out about Lua's machine learning library Torch which seems to be popular at Facebook, Google, Twitter, etc. I've always thought of Lua as a rather obscure scripting language, does anyone know why it seems to be so popular with these organisations while R and Python overall seem to be the most popular scripting languages for machine learning?",64,59,False,self,,,,,
8,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,7,3fga3k,simonwinder.com,Free natural image random patch database for training nets or gathering images statistics,https://www.reddit.com/r/MachineLearning/comments/3fga3k/free_natural_image_random_patch_database_for/,robotbugs,1438468462,,0,1,False,http://b.thumbs.redditmedia.com/4DI2egXFIop7r0ZFYYBgBqQXL4wEjVSHZd52ZoPrGRw.jpg,,,,,
9,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,8,3fghq9,simonwinder.com,Understanding back-propogation - simple explanation with clear diagrams instead of just a page of math.,https://www.reddit.com/r/MachineLearning/comments/3fghq9/understanding_backpropogation_simple_explanation/,robotbugs,1438472676,,0,2,False,http://b.thumbs.redditmedia.com/ROiahkgvmVNVd2xslJUqsxBfQn6tduRN0MNMW3S-Ybs.jpg,,,,,
10,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,12,3fh3yq,self.MachineLearning,Derivative in backpropagation through time,https://www.reddit.com/r/MachineLearning/comments/3fh3yq/derivative_in_backpropagation_through_time/,tamagowitch,1438485764,"I find higher order terms of recurrent weight when performing backpropagation through time. Please imaginethe following simple linear RNN.

h(n) = W x(n) + U h(n-1) + b
y(n) = V h(n) + c
       = V (W x(n) + U h(n-1) + b) + c
       = V (W x(n) + U (W x(n-1) + U h(n-2) + b) + b) + c
       = V U U h(n-2) + ...

How to differentiat the term of (V U U h) w.r.t. U? I think (1) is mathematically corrent but (2) looks similar to other first order terms.
(1) V'  (U h)'+(V U)'  h' 
(2) (V U)'  h' 
",4,5,False,self,,,,,
11,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,16,3fhmes,self.MachineLearning,Which AIs can Occams Razor shave nothing off of?,https://www.reddit.com/r/MachineLearning/comments/3fhmes/which_ais_can_occams_razor_shave_nothing_off_of/,BenRayfield,1438499404,"A simple and general definition of AI is the kind of inputs and outputs of AIXI and Deepmind. They receive a bit stream and a score stream and output a bit stream. The job of the AI is to maximize sum of the scores, considering the inputs and its chosen outputs. The score is calculated by any function thats unknown to the AI, for it to learn over time.

Any AI task, such as classifying or associative memory or Rock Paper Scissors, can be wrapped in this learning process by scoring high when thats what happens.

Most software is huge because people try to code solutions to problems instead of a way to solve problems, especially the problem of how to figure out how to solve new problems.",3,0,False,self,,,,,
12,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,17,3fhqpe,jakevdp.github.io,Beautiful data analysis of cycling habits using PCA and Gaussian Mixture Models,https://www.reddit.com/r/MachineLearning/comments/3fhqpe/beautiful_data_analysis_of_cycling_habits_using/,cast42,1438503832,,5,59,False,default,,,,,
13,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,18,3fhthx,self.MachineLearning,Hardware spec. for Recurrent Neural Net and Convolutional Neural Net experiments,https://www.reddit.com/r/MachineLearning/comments/3fhthx/hardware_spec_for_recurrent_neural_net_and/,mphuget,1438506903,"Hello all, 

I found articles on the Internet regarding GPU for Deep Learning experiments (GTX 980 or GTX Titan) but not a lot of thing on PC configuration for such experiments. I am balancing in this moment on using AWS for such experiments or tuning a specific PC. 

I know you will ask me PC configuration depends on the kind of experiments I would like to do. Deep Learning will be used for scene parsing and object detection on images (could be from videos), and I will increase dataset as soon as I gain confidence on the work I do.

Thanks in advance for your help",6,5,False,self,,,,,
14,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,18,3fhueb,gitxiv.com,New deepmind research+code: Spatial Transformer Networks.,https://www.reddit.com/r/MachineLearning/comments/3fhueb/new_deepmind_researchcode_spatial_transformer/,samim23,1438507909,,11,26,False,default,,,,,
15,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,20,3fhz88,drive.google.com,Neural Machine Translation: Progress Report - by Kyunghyun Cho,https://www.reddit.com/r/MachineLearning/comments/3fhz88/neural_machine_translation_progress_report_by/,clbam8,1438513223,,0,12,False,http://b.thumbs.redditmedia.com/i26tYAVWx41b6LeHFJaMruFi7x91IIlTRmbKlo0B46I.jpg,,,,,
16,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,22,3fibji,blog.manugarri.com,Teaching Neural Networks about Monet,https://www.reddit.com/r/MachineLearning/comments/3fibji/teaching_neural_networks_about_monet/,manueslapera,1438523750,,18,24,False,http://b.thumbs.redditmedia.com/8ornkbf2t8mJeFLoulHM31PPqZMLTpyPQxiBj3ZdyCU.jpg,,,,,
17,MachineLearning,t5_2r3gv,2015-8-2,2015,8,2,23,3fidrd,self.MachineLearning,Theano cost function question,https://www.reddit.com/r/MachineLearning/comments/3fidrd/theano_cost_function_question/,bridgebywaterfall,1438525236,"I am trying to learn how to use Theano. I work very frequently with survival analysis and I wanted therefore to try to implement a standard survival model using Theano's automatic differentiation and gradient descent. The model that I am trying to implement is called the Cox model and here is the wikipedia article: https://en.wikipedia.org/wiki/Proportional_hazards_model

Very helpfully, they have written there the partial likelihood function, which is what is maximized when estimating the parameters of a Cox model. I am quite new to Theano and as a result am having difficulties implementing this cost function and so I am looking for some guidance. 

Here is the code I have written so far. My dataset has 137 records and hence the reason I hard-coded that value. T refers to the tensor module, and W refers to what the wikipedia article calls beta, and status is what wikipedia calls C. The remaining variables are identical to wikipedia's notation.

    def negative_log_likelihood(self, y, status):
        v = 0
        for i in xrange(137):
            if T.eq(status[i], 1):
                v += T.dot(self.X[i], self.W)
                u = 0
                for j in xrange(137):
                    if T.gt(y[j], y[i]):
                        u += T.exp(T.dot(self.X[j], self.W))
                v -= T.log(u)

        return T.sum(-v)

Unfortunately, when I run this code, I am unhappily met with an infinite recursion error, which I hoped would not happen. This makes me think that I have not implemented this cost function in the way that Theano would like and so I am hoping to get some guidance on how to improve this code so that it works. 

",4,1,False,self,,,,,
18,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,0,3fikvi,self.MachineLearning,Have researchers given up on doing for AI what physicists want in a unified field equation?,https://www.reddit.com/r/MachineLearning/comments/3fikvi/have_researchers_given_up_on_doing_for_ai_what/,BenRayfield,1438529686,"Not just strong AI or AGI but a small equation or code that explains why it works? An optimized code that does the same thing just faster, is often larger and could be understood through the simpler prototype.

There are not many kinds of intelligence. There are many things to think about. Mountcastle, referenced in recent years papers by Jeff Hawkins and Ray Kurzweil, wrote a neurscience paper about neocortex (which is a majority of our brains and thinks abstractly) working the same way in all its parts. When parts of a brain are damaged, other parts take over the lost functions.

Neural networks, harmony search, evolution, and many other parts of AI minds are each very small components.

For example, [Entropica](http://entropica.com) is an equation that acts toward keeping its longterm options open in an entropy context, so its normal behavior is to keep a pole balanced, or to swing it up if already fallen, since that increases its avaiable choices in the future to continue balancing it or let it fall, compared to the lower chance if being able to swing it up again if it has already fallen.

[AIXI](http://wiki.lesswrong.com/wiki/AIXI) is based on a similarly simple theory. It uses bayesian statistics on, in theory all but in practice random samples of, ways it could modify itself (or some language describing its strategies) which considers which changes to itself would result in finding better strategies longterm. AIXI learned to play Pacman similar to Deepmind learned to play Atari games, using a grid of symbols were the pixels or tiles are and trying to maximize sum of score over time.

But do any of the very small AIs get the job done? Are they too small? Einstein said ""Make things as simple as possible, but not simpler."" We shouldnt remove code that significantly improves the AI's behaviors or efficiency, but we shouldnt write or keep code that can be derived by the AI since it will understand itself better and lead to smarter AI longterm.

This is not how AI is normally built. People are usually in a rush to solve a few real world problems, so they're happy to skip the AI deriving its own algorithms step. As usual, the profoundly unifying equations are discovered by few while the rest of the world isnt looking. Are there more or less people looking for a unifying equation or code of intelligence than there are looking for a unified field equation? Have most experts who once thought it was possible tried and given up on it ever being done?",9,0,False,self,,,,,
19,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,1,3fircu,gitxiv.com,GRUV: Algorithmic Music Generation using Recurrent Neural Networks (code+paper),https://www.reddit.com/r/MachineLearning/comments/3fircu/gruv_algorithmic_music_generation_using_recurrent/,samim23,1438533208,,4,13,False,default,,,,,
20,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,3,3fj6ac,self.MachineLearning,ACTUAL learn at your own pace machine learning programs?,https://www.reddit.com/r/MachineLearning/comments/3fj6ac/actual_learn_at_your_own_pace_machine_learning/,SamSlate,1438540792,"Coursera's machine learning class is offered EXCLUSIVELY during the standard school semesters dates- and I can't even articulate how idiotic this completely arbitrary limitation is. I have *actual* classes I have to attend during those dates, why the hell do they do this?? It accomplishes nothing and only serves to inconvenience it's users with its fabricated barriers to learning. This is not the future of online learning, this is archaic nonsense.

/rant

Where can I get an introduction to machine learning online? 

An online course with exercises would be preferable, but failing that who can recommend a good text book?

thanks.
",17,0,False,self,,,,,
21,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,5,3fjira,github.com,Online Machine Learning in Go,https://www.reddit.com/r/MachineLearning/comments/3fjira/online_machine_learning_in_go/,conjob96,1438547001,,4,5,False,http://b.thumbs.redditmedia.com/TRftp1c3rYaeXcx5F94cdqjkoBfTIq8mDICVEwEqG8g.jpg,,,,,
22,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,5,3fjkp8,peterroelants.github.io,I added a 5th part to my previous tutorial on neural networks. LMKWYT,https://www.reddit.com/r/MachineLearning/comments/3fjkp8/i_added_a_5th_part_to_my_previous_tutorial_on/,Xochipilli,1438547983,,4,79,False,http://a.thumbs.redditmedia.com/MMlDJDHaTa6UNqwNovbt02PXFXv90b6djMK9LOpXYq0.jpg,,,,,
23,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,7,3fjyj8,mri.readthedocs.org,Mri: Monitor training of deep learning models from anywhere (includes Caffe integration),https://www.reddit.com/r/MachineLearning/comments/3fjyj8/mri_monitor_training_of_deep_learning_models_from/,nharada,1438554966,,2,11,False,http://b.thumbs.redditmedia.com/K_T52XOjur84HPqksb8qq6fn2YzINIpFp5VJxUKp-1Y.jpg,,,,,
24,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,8,3fk58y,self.MachineLearning,Feature Transformation. When do I transform quantitative features and why?,https://www.reddit.com/r/MachineLearning/comments/3fk58y/feature_transformation_when_do_i_transform/,jacek_,1438558506,"Hello. I have recently learned about existence of feature transformation techniques (for quantitative data). However I could not find good sources of information on how to decide when transformation will be beneficial and why it can help. Do you know any good papers, articles? What are your experiences and rules you use for feature transformation? Thanks.",1,0,False,self,,,,,
25,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,9,3fkbq2,self.MachineLearning,The end result...,https://www.reddit.com/r/MachineLearning/comments/3fkbq2/the_end_result/,[deleted],1438561996,"I'm a huge fan of machine learning and the attempt to understand how to reproduce human level intelligence with a machine. However, I can't help to think how bleak the implications are once this is achieved. If machines get to the point of being able to out think the smartest of people we will turn over the thrill of discovery over to a machine. Will future generations of children be able to work on those undiscovered laws of physics or will it just be another fact already worked out by a learning computer with nothing left for us mere mortals to do or dream of?",1,0,False,default,,,,,
26,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,9,3fkdmo,self.MachineLearning,Does anyone actually use R for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3fkdmo/does_anyone_actually_use_r_for_machine_learning/,MusicIsLife1995,1438563014,"I've been using R for about a month but after looking at the OO structure of the language, I'm not too satisfied...",11,3,False,self,,,,,
27,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,11,3fkmxz,self.MachineLearning,Spatially contiguous clustering of time series,https://www.reddit.com/r/MachineLearning/comments/3fkmxz/spatially_contiguous_clustering_of_time_series/,SpaceWizard,1438568073,"I have many time series with a spatial arrangement. I can create a distance matrix of how similar these time series are and cluster them. This works pretty well because nearby waveform tend to be similar. It would be better if I could adapt the distance matrix to result in spatially contiguous clusters. Is there an efficient way to create a distance matrix that is a good combination of waveform similarity and also results in spatially contiguous clusters? 

I'm asking about the dist matrix because many clustering functions take it as input, so it might be a decent way to add a spatial constraint to whatever technique makes sense for the data. I could be wrong, and I'm open to any approach. ",2,9,False,self,,,,,
28,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,11,3fkpu7,self.MachineLearning,Phds not is CS nor Math who were trained in ML afterwards. How new was ML really to you?,https://www.reddit.com/r/MachineLearning/comments/3fkpu7/phds_not_is_cs_nor_math_who_were_trained_in_ml/,tacnobac,1438569701,Was there a significant overlap with the math and/or programming you had to pick up for your research even though it wasn't called ML at that time?,3,4,False,self,,,,,
29,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,12,3fkw23,techcrunch.com,Machine Learning And Human Bias: An Uneasy Pair,https://www.reddit.com/r/MachineLearning/comments/3fkw23/machine_learning_and_human_bias_an_uneasy_pair/,mlandergan,1438573137,,2,7,False,http://b.thumbs.redditmedia.com/NoXFQTvu14PvoZr8GkBOXbf_7GwfkQ36FrR9hV5PA5c.jpg,,,,,
30,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,15,3fl9m2,i.imgur.com,Stuck on an lua/iTorch script. Getting an error with the 'size' function.,https://www.reddit.com/r/MachineLearning/comments/3fl9m2/stuck_on_an_luaitorch_script_getting_an_error/,PhysicsNovice,1438581813,,3,0,False,http://b.thumbs.redditmedia.com/onUvl74J-n6KTtEVcYqlVKD7tGQ37KLYI_O_eXQK_AA.jpg,,,,,
31,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,16,3flelf,answers.opencv.org,Does the one class SVM ignore training labels automatically in OpenCV?,https://www.reddit.com/r/MachineLearning/comments/3flelf/does_the_one_class_svm_ignore_training_labels/,jackbrucesimpson,1438585556,,0,0,False,http://b.thumbs.redditmedia.com/9FVhDhTaAtQiUHZrZ4uehDn8xeIHt0i2RRYLhaE5qdI.jpg,,,,,
32,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,20,3flz6a,github.com,Pandashells - Combining the Python data-stack with the shell pipeline,https://www.reddit.com/r/MachineLearning/comments/3flz6a/pandashells_combining_the_python_datastack_with/,Kiudee,1438602858,,0,8,False,http://b.thumbs.redditmedia.com/QGmCIRoIQoDsnN_hEkxtocSjGTaI3pmoFnbk3yefZ4c.jpg,,,,,
33,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,20,3flzhz,self.MachineLearning,Time-series database for DM/ML,https://www.reddit.com/r/MachineLearning/comments/3flzhz/timeseries_database_for_dmml/,chaotic-kotik,1438603074,"Hi /r/machinelearning!

I'm developing a time-series database. My main use case is not monitoring but time-series data mining and I want to validate some ideas behind it.

Main characteristics:

1. Simple numeric time-series is a main target. There is no columns like in InfluxDB, no joins or other complex queries.
2. Lossless compression. All values is saved without modification.
3. Super fast data ingestion and range queries. In contrary to that single series queries is slow (not a main use case).
4. Various queries:
    * Moving average, moving median.
    * Random sampling.
    * Frequent items (I'm using SpaceSaving alg.), heavy hitters and other stats.
    * Piecewise Aggregate Approximation (PAA).
    * Symbolic Aggregate approXimation (SAX).
    * (maybe) Dynamic Time Warping (DTW) over original data.
    * (maybe) Haar Wavelet Transform.
    * (maybe) FFT.

The main idea is to give you something that doesn't looks like SQL database but can be used for data mining. For example, you can ask it to generate SAX representation from original data and use it to find motifs or discords or to answer 1-NN queries approximately. Maybe you'll want to build index using Lucene or jMotif. I can't know it in advance and I can't decide what kind of indexing scheme should be used in the DB.",0,3,False,self,,,,,
34,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,22,3fm6xt,self.MachineLearning,Identifying Poison Ivy?,https://www.reddit.com/r/MachineLearning/comments/3fm6xt/identifying_poison_ivy/,pst2154,1438607644,Anyone make any classification algorithms for identifying poisonous plants? My neighbor pointed a out to me in my yard yesterday. Maybe someone with enough time can use it for a classification project.,1,0,False,self,,,,,
35,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,22,3fm78w,cloudacademy.com,Let's Understand Amazon Machine Learning (Full Course),https://www.reddit.com/r/MachineLearning/comments/3fm78w/lets_understand_amazon_machine_learning_full/,elenaward,1438607827,,10,45,False,http://b.thumbs.redditmedia.com/ABJxlhxI5NghdeENlri3v0zxSoHNjCV_ueWY1XqVfqs.jpg,,,,,
36,MachineLearning,t5_2r3gv,2015-8-3,2015,8,3,23,3fmi0k,startup.ml,Ask-Me-Anything about our Machine Learning Fellowship,https://www.reddit.com/r/MachineLearning/comments/3fmi0k/askmeanything_about_our_machine_learning/,arshakn,1438613322,,1,0,False,http://b.thumbs.redditmedia.com/_AW3W4A8tlmPg1CCmVx1hNeJSxU5OMNGUmgLsZPlubc.jpg,,,,,
37,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,2,3fn43d,linkedin.com,data science for school kids!,https://www.reddit.com/r/MachineLearning/comments/3fn43d/data_science_for_school_kids/,varaggarwal,1438622891,,0,0,False,http://b.thumbs.redditmedia.com/Zps2zMg0E5mCtTTHbMeI7-GiYgT8YYzPfJcA_p5LRWQ.jpg,,,,,
38,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,2,3fn43w,self.MachineLearning,Are the large Google App Engine Machines a decent choice for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3fn43w/are_the_large_google_app_engine_machines_a_decent/,GAEMachineLearner,1438622897,"I'm using a convolutional network for image recognition, and it runs incredibly slowly on my home computer. I don't think it would be reasonable to reduce the resolution because I already have the most I can, and I don't think I should increase max pooling size because of the nature of the images.",6,1,False,self,,,,,
39,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,2,3fn4rg,self.MachineLearning,CNNs making use of camera info?,https://www.reddit.com/r/MachineLearning/comments/3fn4rg/cnns_making_use_of_camera_info/,CarbonAvatar,1438623125,"Are there any conv-net classification/detection algorithms out there that make use of camera information?

For example, if we had lots of training/testing images collected from an aerial drone, where in addition to the image we also had info like altitude, graze angle, meters per pixel, and so on.

If I wanted to modify GoogLeNet / AlexNet / etc. to take advantage of this extra information, where would I inject it in the network? Should it get inserted at the end, near the fully-connected layers?",9,5,False,self,,,,,
40,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,2,3fn80z,github.com,Drop-in Sentiment Analysis Microservice in Go,https://www.reddit.com/r/MachineLearning/comments/3fn80z/dropin_sentiment_analysis_microservice_in_go/,conjob96,1438624473,,0,4,False,http://b.thumbs.redditmedia.com/TRftp1c3rYaeXcx5F94cdqjkoBfTIq8mDICVEwEqG8g.jpg,,,,,
41,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,3,3fn9p3,youtube.com,The first song where every line comes from a different rap song AND the first rap lyrics generated by a computer.,https://www.reddit.com/r/MachineLearning/comments/3fn9p3/the_first_song_where_every_line_comes_from_a/,cgnorthcutt,1438625153,,13,2,False,http://b.thumbs.redditmedia.com/oCuUzxOtQnn0Kq65Ps8USvGsCqGCcR8lsDKCnY1C9fI.jpg,,,,,
42,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,3,3fndih,outlace.com,Beginner Tutorial: Simple Recurrent Neural Network in Python,https://www.reddit.com/r/MachineLearning/comments/3fndih/beginner_tutorial_simple_recurrent_neural_network/,[deleted],1438626699,,0,6,False,default,,,,,
43,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,3,3fnfrw,self.MachineLearning,"Idea: The future of search engines is not for humans, but for AI?",https://www.reddit.com/r/MachineLearning/comments/3fnfrw/idea_the_future_of_search_engines_is_not_for/,piedpipernyc,1438627582,"I'm sure some of this is happening now, but not in a central way.  
My idea for today was that the next ""Google"" was not going to be for people, minorities, or specialized research.  
  
The next thing in search engines is going to be allowing AIs to search our vast knowledge online efficiently and correctly.  
How does one make money from an engine designed solely to search other computing systems?  
I imagine initial AIs will not weigh visual information as heavily; so heavily efficient text searches would take dominance.  
",3,0,False,self,,,,,
44,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,6,3fo73x,self.MachineLearning,Supervised multi-class problem with known class structure?,https://www.reddit.com/r/MachineLearning/comments/3fo73x/supervised_multiclass_problem_with_known_class/,bioMatrix,1438639001,"I'm looking for a method of constructing a multi-class classifier, where the output classes are related by prior knowledge that can be expressed a graph.  Specifically, I'm trying to take advantage of the fact that the classes are similar with known structure, so that some objective function might penalize less for guessing an incorrect but nearby class as compared to a distant class.  Anyone know of such an algorithm?",2,5,False,self,,,,,
45,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,9,3fonzt,self.MachineLearning,"Is it better to apply for Data Science position at a Start-up / less renowned companies than a Software Engineering position at a reputed company( Facebook, Google, Amazon)?",https://www.reddit.com/r/MachineLearning/comments/3fonzt/is_it_better_to_apply_for_data_science_position/,aashus18,1438646648,"I am a MSCS student pursuing specialization in Machine Learning in top 10 CS program (US) . While getting a job in Data Science Field would be my first choice, I would like to know which of the above path is better for my career growth.",7,0,False,self,,,,,
46,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,9,3fosj3,self.MachineLearning,Will learning about Machine Learning give me any deep insight on what subjective experience is?,https://www.reddit.com/r/MachineLearning/comments/3fosj3/will_learning_about_machine_learning_give_me_any/,subjectiveexperience,1438648658,"I've done a bit of work with Machine Learning, primarily making simple OCRs. From what I've done I've mainly just applied Math into making simple programs.

When you get further into Machine Learning do you develop any deep insights on what subjective experience and how it works because this seems a lot more interesting to me than the Math (although I do very much enjoy Math).",5,0,False,self,,,,,
47,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,12,3fpbg1,kastnerkyle.github.io,Using PyTables for Larger-Than-RAM Data Processing,https://www.reddit.com/r/MachineLearning/comments/3fpbg1/using_pytables_for_largerthanram_data_processing/,kkastner,1438657783,,7,20,False,default,,,,,
48,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,13,3fphof,self.MachineLearning,Machine Learning algorithms overview,https://www.reddit.com/r/MachineLearning/comments/3fphof/machine_learning_algorithms_overview/,Question_guy_,1438661055,"There are so many machine learning algorithms that I am overwhelmed. How do I know which algorithm is the best for solving a certain problem? Many of these algorithms are complex so I want to know which one to choose to study. Is there a big simple overview of all currently used algorithms and their uses, pros and cons? I've looked at the article on Wikipedia but from what I've seen it only described how the algorithms worked.

Thanks in advance!",10,14,False,self,,,,,
49,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,21,3fqlyp,blog.dominodatalab.com,Faster deep learning with GPUs and Theano,https://www.reddit.com/r/MachineLearning/comments/3fqlyp/faster_deep_learning_with_gpus_and_theano/,gregory_k,1438690537,,5,0,False,http://a.thumbs.redditmedia.com/i67diPd8isAM28pIjI1k7tb-ILLb6kI21qQvst1jEF8.jpg,,,,,
50,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,21,3fqp6t,auduno.com,Drawing with GoogLeNet,https://www.reddit.com/r/MachineLearning/comments/3fqp6t/drawing_with_googlenet/,matsiyatzy,1438692533,,13,46,False,http://b.thumbs.redditmedia.com/TlSiMtiXQN9A9jhKIT6q-rE3g_cQLIL3tJQNiWOZi8Q.jpg,,,,,
51,MachineLearning,t5_2r3gv,2015-8-4,2015,8,4,22,3fqw9z,self.MachineLearning,Python implementation of OWL-QN for log-linear models with l1 regularization,https://www.reddit.com/r/MachineLearning/comments/3fqw9z/python_implementation_of_owlqn_for_loglinear/,birdfprey,1438696327,"I am currently using SGD to minimize my objective function in a l1-regularized mulitnomial logit model.  I would like to experiment with the [OWL-QN algorithm ](http://research.microsoft.com/en-us/um/people/jfgao/paper/icml07scalable.pdf) but haven't been able to find a implementation in Python.  Would any of you happen to have any info that could help?

I've looked at [PyLBFGS](https://github.com/larsmans/pylbfgs) and can call lbfgs to minimize and can't figure out how to work with owl-qn from this package.",0,1,False,self,,,,,
52,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,0,3fr5t3,self.MachineLearning,Could we use Brain to Computer interfaces and machine learning to recognize images in our minds and save them to a computer accurately?,https://www.reddit.com/r/MachineLearning/comments/3fr5t3/could_we_use_brain_to_computer_interfaces_and/,2Punx2Furious,1438700757,"With current (or almost) technology like Brain-Computer interfaces that can read the signals from your brain and turn them into the images we have in mind (I think I've seen a few of them since a few years ago, but recently I'm not reading much about this technology) combined with machine learning that could refine the process maybe by improving the signal, perception and definition of the images until it's close to perfect.

With these two technologies combined, could we build something that takes the images you are thinking and saves them to a computer, so that you can think of any image, like a subject, a landscape or other complex stuff, and save them as a simple picture file, or even a 3D model?",31,8,False,self,,,,,
53,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,0,3fr7du,arxiv.org,ANNs Applied to Taxi Destination Prediction (paper and code),https://www.reddit.com/r/MachineLearning/comments/3fr7du/anns_applied_to_taxi_destination_prediction_paper/,improbabble,1438701436,,1,14,False,default,,,,,
54,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,0,3fr9gs,facebook.com,Facebook Page Posting AI/ML Stuffs (English/Korean mixed),https://www.reddit.com/r/MachineLearning/comments/3fr9gs/facebook_page_posting_aiml_stuffs_englishkorean/,kjw0612,1438702383,,0,9,False,http://a.thumbs.redditmedia.com/oAN-7D2BqwHYxfe5bLohXSI95iul_JiVdqwojySkA74.jpg,,,,,
55,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,1,3fri3m,self.MachineLearning,Discussion: what do you think the admins plan on doing to prevent brigading/alt accounts,https://www.reddit.com/r/MachineLearning/comments/3fri3m/discussion_what_do_you_think_the_admins_plan_on/,[deleted],1438705881,"They keep saying things like ""it's not easy but we're smart"" yada yada, but what do you guys think they have in the works with respect to white listing ,statistical analysis, etc...

",1,0,False,default,,,,,
56,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,2,3frq33,self.MachineLearning,Beginner questions regarding Recurrent Neural Networks and large scale learning.,https://www.reddit.com/r/MachineLearning/comments/3frq33/beginner_questions_regarding_recurrent_neural/,TrashQuestion,1438709125,"Hello

I am trying to do activity detection using accelerometer data but am somewhat of a novice when it comes to machine learning. I have a handful of questions and hopefully you guys can answer or point me in the direction of answers for me to read up on.

1) how do large companies do distributed machine learning? I have access to a large cluster of computers (running hadoop, but that's not necessarily important) and I want to know if I might be able to take advantage of them when I have a good amount of data. How do large companies train neural nets using large numbers of clustered computers? Is there a standard for it?

2) how do I go about formatting time series data so that it can be properly fed into the neural net? Should I break it up into chunks and feed some sort of magnitude of the chunk? Should I simply feed in each sample I get from my accelerometer? Is there a ""best"" or recommended way to feed in time series data? I want the data to be shift invariant bexuase it will be running continuously once the training is done.
Is there a recommended form for the data to take? I could feed in the time series, or a spectrogram of the frequency magnitudes, etc

3) what ""knobs"" should I know about to adjust the neural network? How do Recurrent Neural Networks differ from regular feed forward+backprop ANNs?

Sorry if these are a little broad, I am just getting my feet wet with Recurrent Neural Networks and its been some time since I took my machine learning class in college.

P.s. I am trying to do this in python right now, are there recommended neural network libraries for python? I know of scikitlearn but it doesn't seem to do neural networks. I am looking at theano right now but it seems overly complex for a beginner. ",12,8,False,self,,,,,
57,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,2,3frtmi,self.MachineLearning,Neuronal Network Error,https://www.reddit.com/r/MachineLearning/comments/3frtmi/neuronal_network_error/,abreu0101,1438710615,"I implemented a NN, but it's not working. Somebody can help me to figure out, what is the problem? I try with simple, and, or , xor function. The cost function seems to decrease, but fail at the momento of classify, I set the iteration to 50,000, change the number of alpha (0.1,0.01,0.001), code two solution, vectorized way and one by one observation.

Here the viewer of notebook:
http://nbviewer.ipython.org/gist/Abreu0101/05f6fe35b08eac1162c7

Thanks in advance.",1,0,False,self,,,,,
58,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,3,3frxdp,programmableweb.com,Top 10 Machine Learning APIs,https://www.reddit.com/r/MachineLearning/comments/3frxdp/top_10_machine_learning_apis/,dabshitty,1438712114,,0,2,False,http://b.thumbs.redditmedia.com/cZjiwyxrIMAKVA25LEOY6zelsmN4DXB1hlkzDZKV9GQ.jpg,,,,,
59,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,4,3fscza,self.MachineLearning,"Convolutional Neural Net stuck at the level of error that would be achieve by pure randomness, what to do?",https://www.reddit.com/r/MachineLearning/comments/3fscza/convolutional_neural_net_stuck_at_the_level_of/,GAEMachineLearner,1438718376,"I'm using theano to classify images into 12 categories. I've packed the data into a pkl file and I BELIEVE it's packed correctly, but I'm too naive in this area to be 100 % sure.",8,2,False,self,,,,,
60,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,11,3fttqv,youtube.com,hydraulic press brake 300T4 M CNC da41 system controll back,https://www.reddit.com/r/MachineLearning/comments/3fttqv/hydraulic_press_brake_300t4_m_cnc_da41_system/,louisfubin,1438742455,,0,1,False,default,,,,,
61,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,12,3ftxrp,cloudacademy.com,The Bayes Classifier: building a tweet sentiment analysis tool,https://www.reddit.com/r/MachineLearning/comments/3ftxrp/the_bayes_classifier_building_a_tweet_sentiment/,john_philip,1438744450,,3,9,False,http://b.thumbs.redditmedia.com/yEwwyx1YiQ-RL0Hd1gScRmzVlJSH_Yqh8-G2kgFAATY.jpg,,,,,
62,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,12,3fu20m,self.MachineLearning,activation of deep-network always results in last of training-data's ideal-class. Is this overfitting? Why would this happen?,https://www.reddit.com/r/MachineLearning/comments/3fu20m/activation_of_deepnetwork_always_results_in_last/,theirfReddit,1438746708,"I'd greatly appreciate any help in this: I have a deep-network consisting of an lstm and a feed-forward network for a classifier. I'm training and activating the deep-network as a whole.
The last ideal-class of the last input in the training data is the result of every activation. Even things that have been explicitly trained with another class. Why would this be? Is this overfitting and do I not have enough training-data? Or could it be something else.

Thank you very much",7,4,False,self,,,,,
63,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,14,3fuamd,self.MachineLearning,Explanation of back propagation through a softmax layer?,https://www.reddit.com/r/MachineLearning/comments/3fuamd/explanation_of_back_propagation_through_a_softmax/,[deleted],1438751776,"Can't seem to find clear explanations online on how this works. 

I want to understand how the gradient is back propagated through a softmax layer. 

Andrej Karpathys minimal char rnn [implementation](https://gist.github.com/karpathy/d4dee566867f8291f086) has an example of this, and I don't understand whats going on here. 

Line 43 is the softmax layer, that's fine. Lines 50 and 51 is where the backprop through the softmax is happening. Could someone explain whats going on here? 

Why is the cross entropy loss not being incorporated into the gradient? As I understand it, Line 51 is subtracting 1 from the probability output that represents the true class of the input (sorry if I butchered that wording). Why? 

[Here](http://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network) is a stackoverflow link that explains the process (I couldn't follow the answers entirely), but to me the answers seem different to karpathy's implementation. 

Would love if someone could clear all of this up!",2,0,False,default,,,,,
64,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,15,3fufaj,reddit.com,X-Post: LDA Topic Model implementation and exploration using PyMC framework. Feedback needed.  /r/textdatamining,https://www.reddit.com/r/MachineLearning/comments/3fufaj/xpost_lda_topic_model_implementation_and/,napsternxg,1438754853,,0,2,False,http://b.thumbs.redditmedia.com/Ra-vlKT2v5W8U5AMqnyyn0PQB8-KIHJ8keyK6aOuKJo.jpg,,,,,
65,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,15,3fuhhh,self.MachineLearning,Conceptualizing a Complete Deep Learning Framework,https://www.reddit.com/r/MachineLearning/comments/3fuhhh/conceptualizing_a_complete_deep_learning_framework/,pranv,1438756387,"The future of Deep Learning will be built on Deep Learning Frameworks that enable researchers to translate their ideas into reality. They also enable us to reproduce and build upon the works of the rest, and thus help propagate and compound the value created. There are new libraries popping up each day with new ideas and yet, at the same time there are deep problems even with well established libraries.

To nurture and grow the ecosystem we can have a discussion about an ideal library which excels in all aspects and satisfies all our needs. It can be used to measure quality of existing frameworks and at the same time provide a solid direction for future development.

Library authors don't do market analysis or user testing. At the same time, the 'code' and 'programming' aspect is usually neglected in literature. This creates a gap in the system. Hence it is our responsibility to inform the authors about our needs. Rather than arguing about which library is better or the choice of language, I hope to start here a constructive discussion about what could be done and how.

Do post about your ideas, needs, problems with existing libraries, speedup tips etc.,

Important Note:
As Juergen Schmidhuber rightly said ""Machine learning is the science of credit assignment. The machine learning community itself profits from proper credit assignment to its members"". So if a novel idea comes up here that you use later, please credit the person appropriately.",32,22,False,self,,,,,
66,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,20,3fv6kz,self.MachineLearning,"Is there any easy to understand a multi-layer RNN/LSTM SGD mini-batch training implementation (e.g. in Python or in C++, with a tiny training example)? It shouldn't be highly optimized; just to understand the main training algorithm.",https://www.reddit.com/r/MachineLearning/comments/3fv6kz/is_there_any_easy_to_understand_a_multilayer/,NovaRom,1438775841,There is a plenty of MLP and CNN SGD examples (e.g. written in a non-optimized Python) which are very simple to understand; but I cannot find anything like multi-layer RNN/LSTM SGD with good comments and without optimizations. ,12,8,False,self,,,,,
67,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,22,3fvgrh,data-learn.com,Online Machine Learning System,https://www.reddit.com/r/MachineLearning/comments/3fvgrh/online_machine_learning_system/,xmatos,1438781559,,1,0,False,http://b.thumbs.redditmedia.com/Fvg3VftraePkuvbabRyEIKTuGvQ1gY3kjR7wQZB-SAI.jpg,,,,,
68,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,22,3fvht8,codesachin.wordpress.com,On the Bias/Variance tradeoff in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3fvht8/on_the_biasvariance_tradeoff_in_machine_learning/,sachinrjoglekar,1438782114,,4,4,False,http://b.thumbs.redditmedia.com/vrHYUiT4cUI1AZ0FL3zbJqETEV5Xo017n8f4u0alG9c.jpg,,,,,
69,MachineLearning,t5_2r3gv,2015-8-5,2015,8,5,23,3fvono,self.MachineLearning,Where to get the fourth printing of Murphy's MLAPP?,https://www.reddit.com/r/MachineLearning/comments/3fvono/where_to_get_the_fourth_printing_of_murphys_mlapp/,richardabrich,1438785378,"According to [the book's webpage](http://www.cs.ubc.ca/~murphyk/MLbook/):

&gt; Latest printing is the fourth printing (Sep. 2013), which fixes a lot of the errors in earlier versions.

I've been looking around for a copy of the latest printing, with no success. The litmus test I am using is on page 83, equation 3.55. This error is listed in the [list of errata](https://docs.google.com/spreadsheets/d/1DVJeB3WjJT8eRqM1WIlXqzpUPm-bnxM70rqkWjdbPuk/edit#gid=0) that is fixed in the fourth printing. According to this spreadsheet, the corrected version should read as:

    p(x_i, y_i|\pi, \theta)=p(y_i|\pi)p(x_i|y_i,\theta)=p(y_i|\pi)\prod_j p(x_ij|y_i,\theta_j)=...

If you have a copy of this book that you purchased after September 2013, could you please check to see if this error has been corrected? And if so, where did you get your copy?

Thanks in advance!",1,1,False,self,,,,,
70,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,0,3fvsfy,github.com,I created a list of data science blogs. Feel free to add interesting blogs.,https://www.reddit.com/r/MachineLearning/comments/3fvsfy/i_created_a_list_of_data_science_blogs_feel_free/,rushter_,1438787042,,5,94,False,http://b.thumbs.redditmedia.com/XQZ4ZfYXYVcWQsr5_yZjTw-ygRnZX7MNa4K8r_swaUY.jpg,,,,,
71,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,0,3fvvnb,self.MachineLearning,"Brief R-script to deal with most common, invalid values in a dataframe?",https://www.reddit.com/r/MachineLearning/comments/3fvvnb/brief_rscript_to_deal_with_most_common_invalid/,MachineLearnSF,1438788453,"Q: 
What is the specific, brief R-script
you use frequently,
to clean / impute
the **80% most common** invalid entries in a data frame?
(for ex: NAs, blanks, characters in numeric columns, etc).",3,0,False,self,,,,,
72,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,0,3fvy2x,github.com,Getting Started with Machine Learning on Kaggle,https://www.reddit.com/r/MachineLearning/comments/3fvy2x/getting_started_with_machine_learning_on_kaggle/,peeyushag,1438789551,,0,7,False,http://a.thumbs.redditmedia.com/7A2Hs5izYvv2WVEX1cVTQXFWKQy5cvDl8AvlzQc5po4.jpg,,,,,
73,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,1,3fw7t5,analyticsvidhya.com,"List of Best Machine Learning, Python, Data Science Discussions on Reddit",https://www.reddit.com/r/MachineLearning/comments/3fw7t5/list_of_best_machine_learning_python_data_science/,john_philip,1438793563,,1,61,False,http://b.thumbs.redditmedia.com/C2V32y-LfCpfuYsBp5M3zYdJBlNJ11bz_oXADQ95BCA.jpg,,,,,
74,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,2,3fwhn7,self.MachineLearning,Classifying A Sequence of Handwritten Digits,https://www.reddit.com/r/MachineLearning/comments/3fwhn7/classifying_a_sequence_of_handwritten_digits/,dk503,1438797592,"What's the best approach for recognising a sequence of handwritten digits?

Initial idea:
- Separate digits
- Recognise them individually using a multilayer feed forward neural network

My thoughts are that separating the digits could be tricky (if they overlap), and that perhaps there is a better approach using a recurrent or convolutional neural network?",4,0,False,self,,,,,
75,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,3,3fwo1t,galvanize.com,Upcoming tools for Python Data Ecosystem,https://www.reddit.com/r/MachineLearning/comments/3fwo1t/upcoming_tools_for_python_data_ecosystem/,kunjaan,1438800107,,0,1,False,http://b.thumbs.redditmedia.com/Rv7p7617AOFEjtZYemELsJvu0UVUgn7wGXWok8Hd1LM.jpg,,,,,
76,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,4,3fwsky,self.MachineLearning,NN Architecture Question from CS231n,https://www.reddit.com/r/MachineLearning/comments/3fwsky/nn_architecture_question_from_cs231n/,nnarch,1438801922,I am working through the [CS231n class](http://cs231n.github.io/) class and am currently at the end of [module 1](http://cs231n.github.io/neural-networks-case-study/). I am having a bit of difficulty conceptualizing the architecture of the toy neural network implementation. I understand that there is 300 input points and that they are all of dimensionality 2. I was under the impression that there would be 2 input nodes one for each dimension of each input so that each node would have a weight to the hidden layer. However in the implementation the Input data is in the shape of 300x2 and the weights are of shape 2x100. Is this essentially computing the same ouput to the hidden layer (plus the bias) but all at once?,3,1,False,self,,,,,
77,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,6,3fxa7g,self.MachineLearning,Predictive analytics/machine learning algorithms in business applications,https://www.reddit.com/r/MachineLearning/comments/3fxa7g/predictive_analyticsmachine_learning_algorithms/,intenskitty,1438809162,"In applying predictive analytics/machine learning to business applications (i.e. sales forecasting, employee churn, max likelihood of spend, sale rep assignments, etc.) do most companies use their own formulated algorithms or do they use algorithms that are available in most statistical tools (i.e. R)?",5,0,False,self,,,,,
78,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,8,3fxr1c,computervisiontalks.com,Tagged and Searchable  Machine Learning Course Videos by Andrew Ng (2014),https://www.reddit.com/r/MachineLearning/comments/3fxr1c/tagged_and_searchable_machine_learning_course/,ojaved,1438816592,,2,2,False,http://b.thumbs.redditmedia.com/rv_662UVhuzmgUiBs7WFvYmXatdchShk-S9V8UDYTEA.jpg,,,,,
79,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,8,3fxs84,outlace.com,Beginner Tutorial: Simple RNN in Python (re-do),https://www.reddit.com/r/MachineLearning/comments/3fxs84/beginner_tutorial_simple_rnn_in_python_redo/,outlacedev,1438817121,,1,2,False,http://b.thumbs.redditmedia.com/h03QhlTIAgm869eENX1j_b_5O80bZGHT72kius0nFIA.jpg,,,,,
80,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,10,3fy4ht,self.MachineLearning,Getting a possible input given an output,https://www.reddit.com/r/MachineLearning/comments/3fy4ht/getting_a_possible_input_given_an_output/,nagasgura,1438823152,"I'm starting to learn neural networks, and I just made a program that learned how to recognize handwritten digits with pretty good accuracy (trained with backpropagation). Now I want to be able to see what the network thinks a perfect number looks like (essentially getting a pixel array which produces a desired number but is not from the dataset). I researched how to do this, but I couldn't find anything useful. Is this even possible?",2,1,False,self,,,,,
81,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,10,3fy6xt,self.MachineLearning,How to load my data into scikit-learn?,https://www.reddit.com/r/MachineLearning/comments/3fy6xt/how_to_load_my_data_into_scikitlearn/,steve_g,1438824372,"I have about 11000 items which I've featurized in a way I like (at least for now).  I'd like to use scikit-learn to try clustering the data.  There are about 120000 features, so a plain item*feature matrix would be pretty big.  The matrix would be sparse, with only about 800000 non-zero entries.

I've cleaned up and featurized my data in a c# program, but I'd like to output a text file that I can load into scikit-learn.  My Google-Fu must be weak, because I'm struggling to find a good link on the proper text file format for my sparse data.

I'd appreciate any help or pointers you can give me.

Thanks.",1,0,False,self,,,,,
82,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,11,3fyf1t,self.MachineLearning,What advice do you want to give to the undergraduate researchers in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3fyf1t/what_advice_do_you_want_to_give_to_the/,machinegaze,1438828302,,4,1,False,self,,,,,
83,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,11,3fyhuz,self.MachineLearning,How do you guys manage learning all this material?,https://www.reddit.com/r/MachineLearning/comments/3fyhuz/how_do_you_guys_manage_learning_all_this_material/,Pandanleaves,1438829690,"I am feeling extremely overwhelmed by the amount of material I should learn to be somewhat decent at data science. So far I've got the basics of statistics down and am pretty good with R, but I still have to learn Python, SQL, making maps, d3, Shiny, and all that stuff. Once I've got those down, I should probably be looking into algorithms that are more specific to my field. I've never felt this overloaded with information before. Right now I'm juggling between an intro to Python course and an SQL tutorial--often I just feel so tired and end up goofing off, so I need a way to organize things and motivate myself. Any advice would be much appreciated!",31,19,False,self,,,,,
84,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,12,3fyl6c,deeplearning4j.org,A Beginner's Guide to Restricted Boltzmann Machines,https://www.reddit.com/r/MachineLearning/comments/3fyl6c/a_beginners_guide_to_restricted_boltzmann_machines/,vonnik,1438831378,,1,7,False,default,,,,,
85,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,13,3fytsf,wmh24.de,WMH24,https://www.reddit.com/r/MachineLearning/comments/3fytsf/wmh24/,bhavesh007,1438836047,,1,1,False,default,,,,,
86,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,15,3fz2kr,hayshydraulics.blogspot.com,Replacing Hydraulic Fittings Damaged By Rats: Some Important Tips,https://www.reddit.com/r/MachineLearning/comments/3fz2kr/replacing_hydraulic_fittings_damaged_by_rats_some/,HaysHydraulics,1438841660,,1,0,False,default,,,,,
87,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,16,3fzau7,gwern.net,Training a Neural Network to generate CSS,https://www.reddit.com/r/MachineLearning/comments/3fzau7/training_a_neural_network_to_generate_css/,alexcasalboni,1438847743,,8,36,False,http://b.thumbs.redditmedia.com/bf8kO8C4GdoEVSce6aPk-O48zbghtLrZBlT3CqJkMoY.jpg,,,,,
88,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,17,3fzdau,nbviewer.ipython.org,Random Forrests to detect Algorithmically Generated Domain Names,https://www.reddit.com/r/MachineLearning/comments/3fzdau/random_forrests_to_detect_algorithmically/,cast42,1438849815,,0,4,False,default,,,,,
89,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,17,3fzdxw,self.MachineLearning,"I just got assigned a task to design an algorithm, where do I start?",https://www.reddit.com/r/MachineLearning/comments/3fzdxw/i_just_got_assigned_a_task_to_design_an_algorithm/,[deleted],1438850351,"I just started my first software engineering job, and I told to design an algorithm for online ad arbitrage. I don't have any machine learning or much statistics experience, but it seemed the best way to go. Are there any algorithms that I could try pulling out of a box and applying, or things I should investigate first?",1,0,False,default,,,,,
90,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,19,3fzo1c,self.MachineLearning,Thermostat Machine learning,https://www.reddit.com/r/MachineLearning/comments/3fzo1c/thermostat_machine_learning/,babamurshid,1438858520,Hello! I am a final year student stuck in my FYP because of machine learning .My task is to make an algorithm that can predict the time  a person enters into the bathroom to take a shower.So that the device can automate the process by looking at the data provided by the users.Hoping for some help!:) ,3,1,False,self,,,,,
91,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,20,3fzp7a,emeraldislebusiness.com,TIPS TO CONSIDER WHEN PICKING AN AUTOMATIC BAGGING MACHINE THAT CAN WORK WITH BOTH PLASTIC AS WELL AS PAPER BAGS,https://www.reddit.com/r/MachineLearning/comments/3fzp7a/tips_to_consider_when_picking_an_automatic/,evansmark333,1438859370,,0,1,False,default,,,,,
92,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,20,3fzrf3,computervisiontalks.com,DIP Lecture 1: Digital Image Modalities and Processing,https://www.reddit.com/r/MachineLearning/comments/3fzrf3/dip_lecture_1_digital_image_modalities_and/,aranag,1438861072,,0,1,False,http://b.thumbs.redditmedia.com/Z-1e7U822EHZ_hcVfZiDlHrZp1HqiMuK79ZJcAmUsRE.jpg,,,,,
93,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,20,3fzrk1,computervisiontalks.com,Machine learning - Introduction,https://www.reddit.com/r/MachineLearning/comments/3fzrk1/machine_learning_introduction/,aranag,1438861157,,1,5,False,http://b.thumbs.redditmedia.com/P5EF-MQYMJ2FcRvlsE7d5Ro-TiJklIRShIVX_y7e67Q.jpg,,,,,
94,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,21,3fzy97,github.com,Help nostradamIQ saving the World! TEDx earthquakes and machine learning,https://www.reddit.com/r/MachineLearning/comments/3fzy97/help_nostradamiq_saving_the_world_tedx/,CuriousBitOfNothing,1438865340,,0,2,False,http://b.thumbs.redditmedia.com/lnkPKvVXUmm7onA6k58Ow3O7fQU6SEzyynkx7-D2LiU.jpg,,,,,
95,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,23,3g072z,self.MachineLearning,Classification on a single continuous input such as sound?,https://www.reddit.com/r/MachineLearning/comments/3g072z/classification_on_a_single_continuous_input_such/,kurowaza,1438869782,"As an example, assume you have sound data with pure tones and musical note metadata attached to the training set.  The twist is that you don't know to pre-process the data to get it into the frequency domain.  More broadly, you have a rich information signal hidden behind an unknown math operation (frequency, filter, derivative, etc).  Which area of machine learning would help tackle this type of problem?  ",3,1,False,self,,,,,
96,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,23,3g0ak5,neurovis.dataphoric.com,An interactive Neural Network visualizer and tutorial,https://www.reddit.com/r/MachineLearning/comments/3g0ak5/an_interactive_neural_network_visualizer_and/,john_philip,1438871522,,1,1,False,default,,,,,
97,MachineLearning,t5_2r3gv,2015-8-6,2015,8,6,23,3g0cil,self.MachineLearning,How to train a RNN with only one output for an entire sequence?,https://www.reddit.com/r/MachineLearning/comments/3g0cil/how_to_train_a_rnn_with_only_one_output_for_an/,shrimpMasta,1438872347,"I would like to train a RNN to accumulate the information of sequences, and output a single vector (instead of a sequence of vectors). Any idea on how to achieve this ? I was thinking about some mean pooling operation, but how to handle the gradient for a mean pooling layer ? 

EDIT:  BPTT is about unwrapping the network, you have an input sequence, a target sequence and you back-propagate in the unwrapped net. Would it be possible, for instance, to have a RNN accepting a character as input and outputing a predicted next character, but during the training you actually try to predict the 5 next characters. This would be to force the network to learn a good representation at each timestep, not only good enough to predict the next character, but also the 4 characters coming after.  Is this possible ?",2,5,False,self,,,,,
98,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,0,3g0h9g,self.MachineLearning,Is this a valid Markov Decision Process model?,https://www.reddit.com/r/MachineLearning/comments/3g0h9g/is_this_a_valid_markov_decision_process_model/,MeraAM,1438874347,"The problem involves moving an Autonomous Underwater Vehicle(AUV) for map exploring. The **agent(AUV)** is controlled by a human user that follows the suggested decisions by the learned **Markov Decision Process(MDP)** model or if not the model will still suggest the next action based on the new state arrived by the agent. The area to be covered is almost a bowl-shaped 3D surface represented by a set of **n** landmarks that need to be visited in order for the map to be complete. The position of the agent is known at all the time,and I'm not concerned about the actual motion of the agent therefore it can just hop from a landmark to another.

So, the MDP would have the following properties:

**s**: A hybrid-state space s = (current_landmark, list_of_unvisited_landmarks)

**a**: Action (a single move action that moves the agent from any landmark to another)

**P(s,a,s')**: The transition matrix is learned from previous expert trajectories of similar areas. It will be in the form **n x n** where each cell has the value $ P(currentLandmark\|previousLandmark)$ which is the probability transiting from a landmark to another for each combination of unvisited landmarks lists.

**r(:,a,:)**: The reward for the action regardless of the state is the squared error between the optimal map(full coverage) and the current map $r(:,a,:)= \|optimalMap - currentMap\|$.

Is having such configuration correct? Does having a single action for each state makes defining the optimal policy hard. If the optimal policy is formed by choosing the most rewarding action, how is it possible in this case?",8,1,False,self,,,,,
99,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,1,3g0s7v,self.MachineLearning,What's the coolest machine learning idea so far in 2015?,https://www.reddit.com/r/MachineLearning/comments/3g0s7v/whats_the_coolest_machine_learning_idea_so_far_in/,feedthecreed,1438878994,"I think it's nice to take stock and reflect on all that's happened this year.

So what result/idea do you think is the most interesting/inspiring in machine learning?

For me, I think it would be the [deepdream](https://github.com/google/deepdream) and [related work](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf).",54,123,False,self,,,,,
100,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,3,3g18on,reddit.comr,r/textdatamining/ - A sub for developers interested in Data Mining,https://www.reddit.com/r/MachineLearning/comments/3g18on/rtextdatamining_a_sub_for_developers_interested/,[deleted],1438885702,,0,1,False,default,,,,,
101,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,4,3g1e03,smerity.com,Question answering on the Facebook bAbi dataset using recurrent neural networks and 175 lines of Python + Keras,https://www.reddit.com/r/MachineLearning/comments/3g1e03/question_answering_on_the_facebook_babi_dataset/,smerity,1438887861,,20,35,False,http://b.thumbs.redditmedia.com/fEZMc8DPyIYIq8OcvM7rIkuZCojqGozgkR_bBXsyJNM.jpg,,,,,
102,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,5,3g1sa6,googleresearch.blogspot.de,The reusable holdout: Preserving validity in adaptive data analysis,https://www.reddit.com/r/MachineLearning/comments/3g1sa6/the_reusable_holdout_preserving_validity_in/,EndianOgino,1438893598,,4,10,False,http://b.thumbs.redditmedia.com/zDltjasIBMXp0VP62kMy2GRUqoonbCmZsGp0sESmMgk.jpg,,,,,
103,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,5,3g1uc3,self.MachineLearning,Eli5 ICA vs PCA?,https://www.reddit.com/r/MachineLearning/comments/3g1uc3/eli5_ica_vs_pca/,classicalhumanbeing,1438894461,What are they and how do they relate. Also is there a bayesian justification available or are solely frequentist concepts?,2,5,False,self,,,,,
104,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,7,3g28si,arxiv.org,Google DeepMind: Teaching Machines to Read and Comprehend,https://www.reddit.com/r/MachineLearning/comments/3g28si/google_deepmind_teaching_machines_to_read_and/,saigeco,1438900922,,4,7,False,default,,,,,
105,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,8,3g2dn4,bbc.co.uk,Logical programming and Causal Reasoning decodes Islamic State strategy,https://www.reddit.com/r/MachineLearning/comments/3g2dn4/logical_programming_and_causal_reasoning_decodes/,dive118,1438903233,,0,2,False,http://b.thumbs.redditmedia.com/giARcMLG7oZMSE0CcgtMqk2fgCgC4ZpEI3FcmpPbvUw.jpg,,,,,
106,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,14,3g3ja8,motherboard.vice.com,These Artworks Were Made by Algorithms,https://www.reddit.com/r/MachineLearning/comments/3g3ja8/these_artworks_were_made_by_algorithms/,john_philip,1438925264,,1,0,False,http://b.thumbs.redditmedia.com/SyrIo6o7s4eYCL5gz_GR1MSgqrGm4jqOaFcCsFBjh4g.jpg,,,,,
107,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,15,3g3ork,arxiv.org,"[1508.01211] Listen, Attend and Spell",https://www.reddit.com/r/MachineLearning/comments/3g3ork/150801211_listen_attend_and_spell/,iori42,1438929172,,0,17,False,default,,,,,
108,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,16,3g3sqo,kaggle.com,"kaggle gone wild: flawed contest changes rules mid-contest, top entrants invalidated",https://www.reddit.com/r/MachineLearning/comments/3g3sqo/kaggle_gone_wild_flawed_contest_changes_rules/,[deleted],1438932225,,0,1,False,default,,,,,
109,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,16,3g3st8,kaggle.com,"kaggle gone wild: flawed contest changes rules mid-contest, top entrants invalidated",https://www.reddit.com/r/MachineLearning/comments/3g3st8/kaggle_gone_wild_flawed_contest_changes_rules/,[deleted],1438932286,,0,1,False,default,,,,,
110,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,16,3g3t28,kaggle.com,"kaggle allows admins to change rules during competition, invalidating work of many top entrants",https://www.reddit.com/r/MachineLearning/comments/3g3t28/kaggle_allows_admins_to_change_rules_during/,dribnet,1438932476,,31,75,False,http://b.thumbs.redditmedia.com/U92N7eMAM5cjJcKqhD8_AJ1fBHozV-nrxB5muj-9apI.jpg,,,,,
111,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,16,3g3tbq,googleresearch.blogspot.be,The reusable holdout: Preserving validity in adaptive data analysis,https://www.reddit.com/r/MachineLearning/comments/3g3tbq/the_reusable_holdout_preserving_validity_in/,[deleted],1438932688,,0,0,False,default,,,,,
112,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,16,3g3um1,ibmresearchnews.blogspot.be,Preserving Validity in Adaptive Data Analysis,https://www.reddit.com/r/MachineLearning/comments/3g3um1/preserving_validity_in_adaptive_data_analysis/,cast42,1438933744,,0,6,False,http://a.thumbs.redditmedia.com/v8DAfrq-8L2M3AudQsk3daIjYx1vpDNA5n6w-6SF0h8.jpg,,,,,
113,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,19,3g44c1,blog.bigml.com,Announcing The Worlds First Automated Early Stage Investing Platform,https://www.reddit.com/r/MachineLearning/comments/3g44c1/announcing_the_worlds_first_automated_early_stage/,czuriaga,1438942230,,0,1,False,default,,,,,
114,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,20,3g49bs,theguardian.com,Interesting data set: Help identify political expense rorts.,https://www.reddit.com/r/MachineLearning/comments/3g49bs/interesting_data_set_help_identify_political/,m1sta,1438946286,,0,3,False,http://b.thumbs.redditmedia.com/AL-HoSZ3IeVksPMghMA28L785rEeGs5uuX4K_damczc.jpg,,,,,
115,MachineLearning,t5_2r3gv,2015-8-7,2015,8,7,21,3g4igb,self.MachineLearning,Defining custom cost function,https://www.reddit.com/r/MachineLearning/comments/3g4igb/defining_custom_cost_function/,just_learning_,1438952286,Can I simply define a custom cost function and minimize the sum of the costs on my training examples with gradient descent?,4,2,False,self,,,,,
116,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,1,3g59uu,blog.jetbrains.com,Googles Deep Dream in PyCharm,https://www.reddit.com/r/MachineLearning/comments/3g59uu/googles_deep_dream_in_pycharm/,xamdam,1438965636,,0,18,False,http://b.thumbs.redditmedia.com/ty72-_KnX1F9y3ee0q18RMnk0CRITJig4pRBbjblpAw.jpg,,,,,
117,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,2,3g5dig,self.MachineLearning,Per pixel image classification,https://www.reddit.com/r/MachineLearning/comments/3g5dig/per_pixel_image_classification/,jrkirby,1438967215,"Is there any image classification papers where they classify each pixel of the image separately? For example an image of a plane that classifies all the plane pixels as plane, and all the other pixels as sky. Any data sets that have something like this also?",3,3,False,self,,,,,
118,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,5,3g649n,kugler-precision.com,"Kugler GmbH - Precision Machines, micromachining, interferometry, metrology and much more",https://www.reddit.com/r/MachineLearning/comments/3g649n/kugler_gmbh_precision_machines_micromachining/,KnutGutzner,1438978959,,1,0,False,default,,,,,
119,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,5,3g64de,self.MachineLearning,Privatization of ML research,https://www.reddit.com/r/MachineLearning/comments/3g64de/privatization_of_ml_research/,listoj,1438979009,"I really don't know what to make of the trend of top ML people going to work for corporations (where the goal is only to maximize profits for its shareholders) instead of universities, which tend to freely share knowledge with the rest of the world. Is this a disturbing trend, or are people making a big deal out of nothing?",23,19,False,self,,,,,
120,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,6,3g6b8q,lab.getbase.com,Is Regression trustworthy? Or how to use metrics to trust the prediction of regression,https://www.reddit.com/r/MachineLearning/comments/3g6b8q/is_regression_trustworthy_or_how_to_use_metrics/,mhfirooz,1438982022,,0,0,False,http://b.thumbs.redditmedia.com/fvb-GB0yxDJolmBqYUunMmCRNabITcpzbXQItQnRutk.jpg,,,,,
121,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,6,3g6fko,github.com,"Outsmart, a didactic puzzle game based on Reinforcement Learning",https://www.reddit.com/r/MachineLearning/comments/3g6fko/outsmart_a_didactic_puzzle_game_based_on/,linschn,1438984067,,10,13,False,http://b.thumbs.redditmedia.com/846Hve9qF2I9YeLaQNRegcs7mIpOpHqchwVMzI8DCDU.jpg,,,,,
122,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,7,3g6jq9,developer.nvidia.com,"cuDNN 3: Train models up to 2x faster on Maxwell and Kepler-powered GPUs, 2x larger models with 16-bit floating point",https://www.reddit.com/r/MachineLearning/comments/3g6jq9/cudnn_3_train_models_up_to_2x_faster_on_maxwell/,downtownslim,1438986019,,4,31,False,http://b.thumbs.redditmedia.com/cuNRxbDJb4bq3rsnrtr_fIGYCKimxSikxCea2VDV5Fw.jpg,,,,,
123,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,11,3g7e8g,self.MachineLearning,Please help: theano - IndexError: invalid slice,https://www.reddit.com/r/MachineLearning/comments/3g7e8g/please_help_theano_indexerror_invalid_slice/,rick168,1439002778,"I am learning theano and writing the codes based on http://deeplearning.net/tutorial/code/mlp.py
The error:
    x: test_set_x[index * batch_size:(index + 1) * batch_size],
	IndexError: invalid slice

Any help appreciated!
",13,0,False,self,,,,,
124,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,12,3g7jw9,self.MachineLearning,What specific technologies do you predict will come in the near future in this field,https://www.reddit.com/r/MachineLearning/comments/3g7jw9/what_specific_technologies_do_you_predict_will/,mike4855,1439006115,"Aside from the typical things laypeople know and talk about, what are you guys excited about over the next 5-10 years?",8,8,False,self,,,,,
125,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,13,3g7ky5,self.MachineLearning,Projects for beginners to get hands on experience in ML,https://www.reddit.com/r/MachineLearning/comments/3g7ky5/projects_for_beginners_to_get_hands_on_experience/,mike4855,1439006714,"What are some cool projects equivalent to ""Hello World"" when it comes to machine learning? Also is there anything on github worth looking at for a beginner testing the ML waters?",12,37,False,self,,,,,
126,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,14,3g7tnb,self.MachineLearning,Help for beginners?,https://www.reddit.com/r/MachineLearning/comments/3g7tnb/help_for_beginners/,kwikadi,1439012506,"Hey guys, so I'm a CS undergrad, who needs to do a project in Machine Learning/Data Mining this semester. I looked around for a project, and found this, basically: 

https://www.kaggle.com/c/dato-native

But the thing is, I have no prior relevant experience. I'm reading up stuff as fast as I can, but I'm still a complete newbie, so I'm not sure if this is too big for me to take on. 

Could you guys help me out with this? Any pointers on whether this is feasible as a first project, on how long it will take to figure out, and the tech/approach to solve this, would be immensely helpful. Its a two person team, and I have till ~15th November to submit this project. (The other person with me is /u/dufferzafar)",1,0,False,self,,,,,
127,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,20,3g8iju,stackoverflow.com,Is this a correct implementation of Skipgram?,https://www.reddit.com/r/MachineLearning/comments/3g8iju/is_this_a_correct_implementation_of_skipgram/,DSLSharedTask,1439035092,,0,2,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
128,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,21,3g8n93,googleresearch.blogspot.ru,The reusable holdout: Preserving validity in adaptive data analysis,https://www.reddit.com/r/MachineLearning/comments/3g8n93/the_reusable_holdout_preserving_validity_in/,alexeyr,1439038708,,0,3,False,http://b.thumbs.redditmedia.com/zDltjasIBMXp0VP62kMy2GRUqoonbCmZsGp0sESmMgk.jpg,,,,,
129,MachineLearning,t5_2r3gv,2015-8-8,2015,8,8,23,3g8t99,coursera.org,Theoretical and practical course for beginners,https://www.reddit.com/r/MachineLearning/comments/3g8t99/theoretical_and_practical_course_for_beginners/,aranajuan,1439042975,,2,3,False,default,,,,,
130,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,0,3g8zdc,github.com,ThingScoop: Search videos for objects contained in them using deep learning,https://www.reddit.com/r/MachineLearning/comments/3g8zdc/thingscoop_search_videos_for_objects_contained_in/,icandoitbetter,1439046537,,2,26,False,http://b.thumbs.redditmedia.com/VW6xuD4xdHau-KYipBdtAd1kzsFdMqVq55VOwTPpeAY.jpg,,,,,
131,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,2,3g9cww,googleresearch.blogspot.com,The reusable holdout: Preserving validity in adaptive data analysis (Google Research),https://www.reddit.com/r/MachineLearning/comments/3g9cww/the_reusable_holdout_preserving_validity_in/,JW_00000,1439053530,,1,4,False,http://b.thumbs.redditmedia.com/zDltjasIBMXp0VP62kMy2GRUqoonbCmZsGp0sESmMgk.jpg,,,,,
132,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,2,3g9gdx,computervisiontalks.com,A fairly thorough explanation of Gaussian Processes by Philipp Hennig (Tagged Videos),https://www.reddit.com/r/MachineLearning/comments/3g9gdx/a_fairly_thorough_explanation_of_gaussian/,ojaved,1439055304,,0,2,False,http://a.thumbs.redditmedia.com/HFT6qKlGdaeigbXxJPsQeQCaaEg-c_MLrnSg4FVnbh8.jpg,,,,,
133,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,4,3g9s5j,self.MachineLearning,Cov Net for rating people on your iPhone!,https://www.reddit.com/r/MachineLearning/comments/3g9s5j/cov_net_for_rating_people_on_your_iphone/,[deleted],1439060914,"Hey guys I just released an app that uses convolutional neural networks to rate people on a scale from 1 to 10. It is a paid app (have to pay for college somehow), but I have promo codes to give out (so you get it for free), so if you are interested just comment below. If you would like to check the app out use the link http://itunes.com/apps/deeprate.",5,1,False,default,,,,,
134,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,4,3g9ska,self.MachineLearning,Anyone else try CuDNN3 yet?,https://www.reddit.com/r/MachineLearning/comments/3g9ska/anyone_else_try_cudnn3_yet/,dhammack,1439061116,"I just installed it (Win8, Titan X) and it did not work. When switching from CuDNN2 to CuDNN3 models stop training entirely. It's like either the fprop or bprop is giving noise and the model can't learn anything. 

CuDNN v3:

    Train on 60000 samples, validate on 10000 samples
    Epoch 0
    60000/60000 [==============================] - 8s - loss: 2.1500 - acc: 0.1652 - val_loss: 2.0239 - val_acc: 0.1943
    Epoch 1
    60000/60000 [==============================] - 8s - loss: 2.0169 - acc: 0.2034 - val_loss: 2.0092 - val_acc: 0.2173
    Epoch 2
    60000/60000 [==============================] - 8s - loss: 2.0089 - acc: 0.2019 - val_loss: 2.0067 - val_acc: 0.2038
    Epoch 3
    60000/60000 [==============================] - 8s - loss: 2.0056 - acc: 0.1973 - val_loss: 2.0039 - val_acc: 0.1956
    Epoch 4
    60000/60000 [==============================] - 8s - loss: 2.0030 - acc: 0.1936 - val_loss: 2.0032 - val_acc: 0.1903
    Epoch 5
    60000/60000 [==============================] - 8s - loss: 2.0013 - acc: 0.1896 - val_loss: 2.0015 - val_acc: 0.1917
    Epoch 6
    60000/60000 [==============================] - 8s - loss: 1.9997 - acc: 0.1918 - val_loss: 2.0005 - val_acc: 0.1942
    Epoch 7
    44288/60000 [=====================&gt;........] - ETA: 2s - loss: 1.9998 - acc: 0.1890forrtl: error (200): program aborting due to control-C event

CuDNN v2 (I just downgraded again):

    Train on 60000 samples, validate on 10000 samples
    Epoch 0
    60000/60000 [==============================] - 5s - loss: 0.4757 - acc: 0.8732 - val_loss: 0.2498 - val_acc: 0.9270
    Epoch 1
    60000/60000 [==============================] - 5s - loss: 0.2189 - acc: 0.9384 - val_loss: 0.1873 - val_acc: 0.9471
    Epoch 2
    59904/60000 [============================&gt;.] - ETA: 0s - loss: 0.1660 - acc: 0.9533

This is simple dense only network on Mnist.",19,0,False,self,,,,,
135,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,5,3ga30g,datasciencecentral.com,Why I don't like IT ppl in my Analytics team,https://www.reddit.com/r/MachineLearning/comments/3ga30g/why_i_dont_like_it_ppl_in_my_analytics_team/,mlthrowaway1,1439066387,,0,0,False,http://b.thumbs.redditmedia.com/A0tA00T46KfzGWFC3Vj46g9qrU5jY9WMrRie51NNpTs.jpg,,,,,
136,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,7,3gaduk,self.MachineLearning,"Wrangling spatio-temporal data, where to start?",https://www.reddit.com/r/MachineLearning/comments/3gaduk/wrangling_spatiotemporal_data_where_to_start/,clumsy_shaver,1439072985,"I'm working on a data set with a 20 year time-series of records tied to longitude/latitude/zip.  As the title said, I'm at a loss on where to start: open to article suggestions, books, etc.  Any help is appreciated",4,7,False,self,,,,,
137,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,8,3gaivj,self.MachineLearning,MATLAB for real-time CV?,https://www.reddit.com/r/MachineLearning/comments/3gaivj/matlab_for_realtime_cv/,deathlymonkey,1439075550,"Hey there!
To begin with, I'm totally new to computer vision. I'm very interested in the subject and have been reading about it a lot lately.
I'm looking to start getting my hands dirty soon. However, I'm not exactly sure about which language/software to use...
I'm pretty fluent with MATLAB and I know a bit of C++. From what I've seen, MATLAB, OpenCV (C++ and Python) and SimpleCV seem to be my best bets for image processing. However, from what I've read, it seems MATLAB might be a bit too slow for real-time video analysis.
Knowing that my goal is to locate and keep track of a moving pingpong ball and the following, which software would you guys recommend?
-Kinda fluent with MATLAB
-Alright with C++ (although my code wouldn't really be optimal, I'm still learning the language)
-I'm open to learning Python for OpenCV knowing it's a high-level language, I'm just not sure it's worth my time for now(I'd learn it eventually, just not sure it's on my priority list right now)
I'm also open to other suggestions!
Thanks!",9,5,False,self,,,,,
138,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,9,3gaooc,github.com,"Compraing Neural Network and Logistic Regression on IRIS dataset using Keras. ""Feature scaling is a must, LR wins""",https://www.reddit.com/r/MachineLearning/comments/3gaooc/compraing_neural_network_and_logistic_regression/,napsternxg,1439078656,,2,0,False,http://b.thumbs.redditmedia.com/8naG55BDrUlI5gHu6nXNuxhCX2VvhM7Cy5PIKwdo9WM.jpg,,,,,
139,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,9,3gaosx,hexahedria.com,Composing Music With Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3gaosx/composing_music_with_recurrent_neural_networks/,[deleted],1439078734,,38,170,False,default,,,,,
140,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,11,3gb69d,self.MachineLearning,Maximizing opaque slow multivariate function,https://www.reddit.com/r/MachineLearning/comments/3gb69d/maximizing_opaque_slow_multivariate_function/,psih128,1439088580,"What's the best method to maximize the value of slow multivariate function? The function is basically the result of backtest operating on temporal data (think stock prices)

Here is what I know about the function:

1. Number of parameters is ~ 10.
2. It takes considerable amount of time to compute the value of the function for single variable set ~ 1 hour.
3. Function seems to be convex against every individual parameter.
4. Function operates on temporal data, so I can serialize computation, i.e. if input data D is split into chunks D = (d1, d2, d3, d4 ...), then F(D, x, 0) = F(d1, x, 0) + F(d2, x, s_1) + F(d3, x, s_2) + .... where s_i is side effect of function F(d_i, ..)
5. The algorithm can be parallelized as I currently have access to 32 core machine.
6. It seems like the function is convex with respect to every individual parameter, but this is not guaranteed.

All my code is in Java and I'm interested in coding crude version of the optimization algorithm myself for education purposes - rather than using some industry solver.
",4,1,False,self,,,,,
141,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,12,3gb8xp,self.MachineLearning,"Where can I get in touch/find people with the right technical machinge learning, aggregating, and information retrieval skills, to make an app like Prismatic?",https://www.reddit.com/r/MachineLearning/comments/3gb8xp/where_can_i_get_in_touchfind_people_with_the/,Dumbo92,1439090034,Or maybe just dsicuss the idea?,8,0,False,self,,,,,
142,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,17,3gc0bk,self.MachineLearning,how to replace variable 'batch_x' through the `givens` parameter,https://www.reddit.com/r/MachineLearning/comments/3gc0bk/how_to_replace_variable_batch_x_through_the/,rick168,1439110035,"Below is correct code.

    f_train = theano.function(
    inputs=[index],
    outputs=cost,
    updates=train_updates,
    givens={
        x: train_x[index * batch_size: (index + 1) * batch_size],
        y: train_y[index * batch_size: (index + 1) * batch_size]
    })

However, if I pull out batch_x from the `givens` parameter as follows:

    batch_x = train_x[index * batch_size:(index+1) * batch_size]
    batch_labels = train_y[index * batch_size:(index+1) * batch_size]
    cost = f_train(batch_x, batch_labels)
    
It will throw an error:    
TypeError: ('Bad input argument to theano function with name ""3.py:196""  at index 0(0-based)', 'Expected an array-like object, but found a Variable: maybe you are trying to call a function on a (possibly shared) variable instead of a numeric array?')

How to replace variable 'batch_x' through the `givens` parameter? Any help appreciated!",2,0,False,self,,,,,
143,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,20,3gc8n6,self.MachineLearning,how to draw a graphic of train_accuracy,https://www.reddit.com/r/MachineLearning/comments/3gc8n6/how_to_draw_a_graphic_of_train_accuracy/,rick168,1439118045,"The code runs fine, and show correct result:10 : cost: 0.541903  rain_accuracy: 92.25%.
However, the graphic is looking a little strange. How to fix? Any help appreciated.

[image!](http://i.stack.imgur.com/fD296.png)
[image2!](https://11350770138305416713.googlegroups.com/attach/979f0e65433ab/1.png?part=0.1&amp;view=1&amp;vt=ANaJVrHp-HkY0OQnLCHTiYA_B5Kro1pcCzsckLKDoXLJSPfl1MK-c6GK1E7Jqu5Smy7RkSJT5cIdTJJ_mE7jeqpXHiiS_MrbITSLm02_4Zwr_Sn6-n8c6nI)



    epochs = 10
    for epoch in range(epochs):
        print epoch+1, "":"",
        
        train_accuracy = []
        train_accuracy2 = []
        for i in range(train_batches):
            batch_x = train_x[i*batch_size:(i+1)*batch_size]
            batch_labels = train_y[i*batch_size:(i+1)*batch_size]
            costs = f_train(batch_x, batch_labels)
            preds = f_test(batch_x)
            acc = sum(preds==batch_labels)/float(len(batch_labels))
            
            train_costs.append(costs)
            train_accuracy.append(acc)
            
            train_accuracy21 = numpy.mean(train_accuracy)*100
            train_accuracy2.append(train_accuracy21)

        print ""cost:"", numpy.mean(train_costs), ""\ttrain:"", str(numpy.mean(train_accuracy)*100)+""%""

    plt.plot(train_accuracy2, c='b')
    plt.xlabel('Number of Epochs')
    plt.ylabel('train_accuracy2')
    plt.show()   ",2,0,False,self,,,,,
144,MachineLearning,t5_2r3gv,2015-8-9,2015,8,9,22,3gcijd,self.MachineLearning,Data point relevance,https://www.reddit.com/r/MachineLearning/comments/3gcijd/data_point_relevance/,_D4n_,1439126450,"Hey guys! I am currently performing a small machine learning study and would love your input on one challenge I am facing. 
I am training a random forest (sklearn) regression model on a small dataset (approx. 300 data points). We have a hypothesis that a specific subset of these 300 data points (approx 10% of the data, so 30 data points) are ""more important"" in terms of creating a ""more accurate"" model. This subset is fixed, it origins from a different data source than the remaining 90%. I am looking for ways to test this hypothesis by putting a meaningful number to this subset and see whether, in fact, the ""special"" 10% are more important, less important, or equivalent for the remaining dataset. Unfortunately, I have no external test data available.

Maybe an additional explanation what I mean with ""data important to train a more accurate model"": the extreme case for ""unimportant"" data would probably be duplicates in the data, as they do not add additional information to the model (except for maybe the data distribution). However, I think it is insufficient to consider ""data similarity"" as a measure of importance, because similar entries (in variable space) with strongly different target value might actually be both very important to data points in their surroundings better.

I did a bit of research and could only find a study by Fu et al (""Representing financial time series based on data point importance""), which exclusively addresses the issue on the specific case of time-series data.

Do you know about any study that measured ""data point importance"" for a generic regression problem? Have you ever considered such a measure? Any input would be highly appreciated!

Thanks a lot!",11,5,False,self,,,,,
145,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,4,3gdql6,self.MachineLearning,How does your workflow look like?,https://www.reddit.com/r/MachineLearning/comments/3gdql6/how_does_your_workflow_look_like/,perceptronico,1439150111,"There's a vast number of topics on various state of the art algorithms, beginner questions and the like, but I'm having a hard time finding one that describes one's complete workflow, regardless of the subject (supervised/unsupervised... ; vision, NLP...). I believe that would be something quite useful to all the people that are trying to start their own project and get their feet wet.",2,11,False,self,,,,,
146,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,5,3gdxz9,self.MachineLearning,News Stories on AI and ML now aggregated at homeAI.info,https://www.reddit.com/r/MachineLearning/comments/3gdxz9/news_stories_on_ai_and_ml_now_aggregated_at/,homeAIinfo,1439153778,"homeAI.info now has a News Story section as well as its information resource directory. Welcome all feedback, add a link or submit a story.",0,0,False,self,,,,,
147,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,7,3gebz9,ted.com,[Ted] Understanding basketball through machine learning and math,https://www.reddit.com/r/MachineLearning/comments/3gebz9/ted_understanding_basketball_through_machine/,ThatDudeOnline,1439160651,,0,1,False,http://b.thumbs.redditmedia.com/xRvHcPOk3N71cbOW-icUx21yFOCWK4ilmP_4GmS0BuY.jpg,,,,,
148,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,8,3geec6,qualcomm.com,Qualcomm Zeroth is advancing deep learning in devices [VIDEO] | Qualcomm,https://www.reddit.com/r/MachineLearning/comments/3geec6/qualcomm_zeroth_is_advancing_deep_learning_in/,downtownslim,1439161830,,0,1,False,default,,,,,
149,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,8,3geh5a,developer.nvidia.com,NVIDIA DIGITS DevBox - The World's Fastest Desktop Deep Learning System,https://www.reddit.com/r/MachineLearning/comments/3geh5a/nvidia_digits_devbox_the_worlds_fastest_desktop/,mckirkus,1439163247,,28,47,False,http://b.thumbs.redditmedia.com/bmz2y-Zue-0aPLJl9_KEfNdlOtEAm2jrIhUik73DSYo.jpg,,,,,
150,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,8,3geizt,youtu.be,Geoff Hinton: Here's how the brain implements backpropagation,https://www.reddit.com/r/MachineLearning/comments/3geizt/geoff_hinton_heres_how_the_brain_implements/,modeless,1439164232,,54,46,False,http://b.thumbs.redditmedia.com/gjzsLSI64UEOortsc6AjgOzY1dilEuIQ0eRdbYdL_kg.jpg,,,,,
151,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,14,3gfl6k,answers.opencv.org,Ideas for separating LDA clusters to improve SVM accuracy?,https://www.reddit.com/r/MachineLearning/comments/3gfl6k/ideas_for_separating_lda_clusters_to_improve_svm/,jackbrucesimpson,1439184682,,4,2,False,http://b.thumbs.redditmedia.com/bjEIIhF_X_osKnT_LHHaNm9U1car4tdTpDC30NfFiIk.jpg,,,,,
152,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,18,3gg327,sumsar.net,Basic MCMC and Bayesian statistics in... BASIC!,https://www.reddit.com/r/MachineLearning/comments/3gg327/basic_mcmc_and_bayesian_statistics_in_basic/,rasmusab,1439199043,,4,40,False,http://b.thumbs.redditmedia.com/sI2K89Xa5yYaEcgWxfAQimGRdVD29yZfwEWrCYYbx6E.jpg,,,,,
153,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,19,3gg5dj,arxiv.org,[1508.01774] An End-to-End Neural Network for Polyphonic Music Transcription,https://www.reddit.com/r/MachineLearning/comments/3gg5dj/150801774_an_endtoend_neural_network_for/,sidsig,1439201208,,8,5,False,default,,,,,
154,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,20,3gge47,self.MachineLearning,BG-A Series Automatic Cup Fill-Seal-Cut Machine,https://www.reddit.com/r/MachineLearning/comments/3gge47/bga_series_automatic_cup_fillsealcut_machine/,sonalbisht101,1439207843,,0,1,False,default,,,,,
155,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,22,3ggl7b,arxiv.org,Do Artificial Reinforcement-Learning Agents Matter Morally?,https://www.reddit.com/r/MachineLearning/comments/3ggl7b/do_artificial_reinforcementlearning_agents_matter/,Noncomment,1439211958,,5,3,False,default,,,,,
156,MachineLearning,t5_2r3gv,2015-8-10,2015,8,10,23,3ggtva,self.MachineLearning,How does Amazon machine learning compare to open source tools?,https://www.reddit.com/r/MachineLearning/comments/3ggtva/how_does_amazon_machine_learning_compare_to_open/,TooSunny,1439216410,"I recently noticed Amazon has a large-scale machine learning service that supports both batch and online training methods. Its FAQ says it uses ""an industry-standard logistic regression algorithm"".  Does anyone know how well it performs against open source implementations like Scipy or Weka?",4,0,False,self,,,,,
157,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,1,3ghaot,blog.longreads.com,"Sentenced to death by machine learning, how ""signature strikes"" are decided",https://www.reddit.com/r/MachineLearning/comments/3ghaot/sentenced_to_death_by_machine_learning_how/,Schlagv,1439223793,,2,24,False,http://b.thumbs.redditmedia.com/vWmV_pJcrKM-CPPZSJLG0FZR2QzuD0eg6XY3hMUTA1Y.jpg,,,,,
158,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,2,3ghnoo,twitter.com,"John H. Holland, father of genetic algorithms and pioneer in complex systems, passed away yesterday",https://www.reddit.com/r/MachineLearning/comments/3ghnoo/john_h_holland_father_of_genetic_algorithms_and/,rhiever,1439229313,,19,238,False,http://b.thumbs.redditmedia.com/aNDD6bm7NZYQvONv8zVx_FDs3tBgpRAWkYwygLfmuJY.jpg,,,,,
159,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,6,3gijuv,theplatform.net,Google Research Solves the Real-Time Pedestrian Detection Problem,https://www.reddit.com/r/MachineLearning/comments/3gijuv/google_research_solves_the_realtime_pedestrian/,[deleted],1439242963,,2,1,False,default,,,,,
160,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,7,3giqxj,github.com,Implementation of Neural Turing Machines,https://www.reddit.com/r/MachineLearning/comments/3giqxj/implementation_of_neural_turing_machines/,cypherx,1439246157,,5,12,False,http://b.thumbs.redditmedia.com/dkcRSqrhvCH48N_oWJ6gGHPj0nb3bXwyiU07mQXP1SY.jpg,,,,,
161,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,9,3gj5kv,devblogs.nvidia.com,Labellio: Scalable Cloud Architecture for Efficient Multi-GPU Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3gj5kv/labellio_scalable_cloud_architecture_for/,harrism,1439253041,,0,4,False,http://b.thumbs.redditmedia.com/SPnDBFbT8ABcvXptkyT-3Qh36rq_77xHVPCt5ykiB9E.jpg,,,,,
162,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,13,3gjzdk,self.MachineLearning,How can I write an activation function for a neural network that takes layer architectures of arbitrary dimensions?,https://www.reddit.com/r/MachineLearning/comments/3gjzdk/how_can_i_write_an_activation_function_for_a/,gamma235,1439267549,"I am making a neural network in Clojure that can take vector of integers, and return a matrix representing the layers: so (make-layers [1 4 5]) would evaluate to:

    [[0]          &lt;-- input
     [0 0 0 0]    &lt;-- hidden
     [0 0 0 0 0]] &lt;-- output 

When I run my activation function on the weights of the network, however, I get a core.matrix error saying that it can't do the matrix multiplication on an input of a one-dimensional vector with the transpose of the weights:

    (core.matrix.operators/* inputs (transpose weights))
    user=&gt; Incompatible shapes, cannot broadcast [1] to [4 2]

I understand why this is not working from the perspective of matrix multiplication, but I am not sure how to rewrite the function to deal with layers of arbitrary length. Here is my current approach:

    (defn propogate-forward [inputs nn]
      (let [weights (vec (for [i nn :when (not (even? (.indexOf nn i)))] i))]
        (mapv #(m/tanh %)
              (mapv #(reduce + %) 
                    (core.matrix.operators/* inputs (transpose weights))))))

Here is a gist that shows what I am working on: https://gist.github.com/gamma235/b8db845a512c60d123af",10,5,False,self,,,,,
163,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,13,3gk0ea,googleblog.blogspot.com,Google trademarks the alphabet.,https://www.reddit.com/r/MachineLearning/comments/3gk0ea/google_trademarks_the_alphabet/,[deleted],1439268162,,1,0,False,default,,,,,
164,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,15,3gk8rl,youtube.com,Mini handle tablet press,https://www.reddit.com/r/MachineLearning/comments/3gk8rl/mini_handle_tablet_press/,huada,1439273520,,0,0,False,default,,,,,
165,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,15,3gk9wa,news.ycombinator.com,[meta-discussion] Radical idea for fixing demographic problems with technical communities,https://www.reddit.com/r/MachineLearning/comments/3gk9wa/metadiscussion_radical_idea_for_fixing/,[deleted],1439274295,,0,1,False,default,,,,,
166,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,16,3gkd5g,self.MachineLearning,How to get the predictation result in csv format from shared variables,https://www.reddit.com/r/MachineLearning/comments/3gkd5g/how_to_get_the_predictation_result_in_csv_format/,rick168,1439276626,"Data is shared variables. I want to get the predictation result in csv format. Below is the code. 
It throws an error. How to fix?  Thank you for your help!

    TypeError: ('Bad input argument to theano function with name ""4.py:305""  at index 0(0-based)', 'Expected an array-like object, but found a Variable: maybe you are trying to call a function on a (possibly shared) variable instead of a numeric array?')

    test_model = theano.function(
        inputs=[index],
        outputs=classifier.errors(y),
        givens={
            x: test_set_x[index * batch_size:(index + 1) * batch_size],
            y: test_set_y[index * batch_size:(index + 1) * batch_size]
        }
    )

    def make_submission_csv(predict, is_list=False):
        if is_list:
            df = pd.DataFrame({'Id': range(1, 101), 'Label': predict})
            df.to_csv(""submit.csv"", index=False)
            return
        pred = []
        for i in range(100):
            pred.append(test_model(test.values[i]))
        df = pd.DataFrame({'Id': range(1, 101), 'Label': pred})
        df.to_csv(""submit.csv"", index=False)
    make_submission_csv(np.argmax(test_model(test_set_x), axis=1), is_list=True)",0,0,False,self,,,,,
167,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,17,3gklou,technologyreview.com,Teaching Machines to Understand Us,https://www.reddit.com/r/MachineLearning/comments/3gklou/teaching_machines_to_understand_us/,john_philip,1439283457,,0,4,False,http://a.thumbs.redditmedia.com/eInntj36KXghrj9NEFHGKmUv-b9KE6bsHEn6EZ-OfI8.jpg,,,,,
168,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,18,3gkme6,self.MachineLearning,Is anyone going to MLConf in Atlanta on Sept 18th?,https://www.reddit.com/r/MachineLearning/comments/3gkme6/is_anyone_going_to_mlconf_in_atlanta_on_sept_18th/,SlightlyCyborg,1439284037,[Here](http://mlconf.com/) is the link,3,5,False,self,,,,,
169,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,19,3gks5x,self.MachineLearning,Open source projects in machine learning,https://www.reddit.com/r/MachineLearning/comments/3gks5x/open_source_projects_in_machine_learning/,verytypical,1439288512,"I successfully completed the course on Machine learning in coursera (by Andrew G) and super excited about the field ! And want to contribute in any interesting open source projects in this domain. I am fairly comfortable in C++, Python, Erlang languages and mostly used Octave for programming assignments in the course.
Anyone hear aware of such projects which need some contributions please let me know. Thanks
Edit: The projects which involve the application of machine learning concepts are also welcome",7,3,False,self,,,,,
170,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,21,3gl17o,arxiv.org,The direction of word vectors obtained through word2vec encodes semantic information. What information is encoded in their norm?,https://www.reddit.com/r/MachineLearning/comments/3gl17o/the_direction_of_word_vectors_obtained_through/,mlberlin,1439295120,,13,7,False,default,,,,,
171,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,21,3gl1lk,self.MachineLearning,Hardware?,https://www.reddit.com/r/MachineLearning/comments/3gl1lk/hardware/,ConciselyVerbose,1439295349,"I have some basic knowledge of machine learning, but am in the process of building a new computer, with one of my goals being to use it for this purpose. I've already made most of my decisions on parts, etc, and am not looking for help in that regard, but it got me curious what type of hardware those more heavily involved use to do so. With that in mind, I'd love to see:

1. What type of ML you're working on.
2. Size of data sets worked with
3. Rough specs of computer
4. How much do you think the power of your hardware affects tour ability to do your work?

Thanks to those willing to answer.",10,0,False,self,,,,,
172,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,21,3gl4ps,blog.christianperone.com,Luigi's Codex Seraphinianus deep learning dreams,https://www.reddit.com/r/MachineLearning/comments/3gl4ps/luigis_codex_seraphinianus_deep_learning_dreams/,[deleted],1439297070,,2,0,False,default,,,,,
173,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,23,3glee1,blog.kaggle.com,Detecting Diabetic Retinopathy in Eye Images - Kaggle tutorial and winners interviews,https://www.reddit.com/r/MachineLearning/comments/3glee1/detecting_diabetic_retinopathy_in_eye_images/,Shishaddict,1439302142,,2,49,False,http://b.thumbs.redditmedia.com/6YlVmbZdE86UHxFLdSOy0hWMoKQXasmLLaInyT0MsNI.jpg,,,,,
174,MachineLearning,t5_2r3gv,2015-8-11,2015,8,11,23,3gli5y,analyticsvidhya.com,Essentials of Machine Learning Algorithms (with Python and R Codes),https://www.reddit.com/r/MachineLearning/comments/3gli5y/essentials_of_machine_learning_algorithms_with/,john_philip,1439303820,,0,14,False,http://b.thumbs.redditmedia.com/S4bqnAVqu-xlWUE-xMgj6bvdiGYo94qU6tVjGBqlD3Q.jpg,,,,,
175,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,0,3glpfu,arxiv.org,[1508.00019] A Minimal Architecture for General Cognition,https://www.reddit.com/r/MachineLearning/comments/3glpfu/150800019_a_minimal_architecture_for_general/,InaneMembrane,1439307018,,9,3,False,default,,,,,
176,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,2,3gm4tm,self.MachineLearning,RNN/LSTM Sequence labeling with Caffe?,https://www.reddit.com/r/MachineLearning/comments/3gm4tm/rnnlstm_sequence_labeling_with_caffe/,zZJollyGreenZz,1439313285,"Has anyone found an example or project that can do RNN/LSTM sequence labeling with Caffe?  Being able to label sequences of images or short time FFT segments would be the ideal case.  If not Caffe, is there an environment that is just as easy to work with?

Any assistance would be greatly appreciated!",3,5,False,self,,,,,
177,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,3,3gmgsw,googleresearch.blogspot.de,The neural networks behind Google Voice transcription,https://www.reddit.com/r/MachineLearning/comments/3gmgsw/the_neural_networks_behind_google_voice/,EndianOgino,1439317974,,7,49,False,http://b.thumbs.redditmedia.com/jgMiRT87feM3lgpUadAz1uXA6e4y48N7N--o_jZxPKs.jpg,,,,,
178,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,3,3gmik0,open.blogs.nytimes.com,Building the Next New York Times Recommendation Engine,https://www.reddit.com/r/MachineLearning/comments/3gmik0/building_the_next_new_york_times_recommendation/,MistakenForYeti,1439318676,,5,38,False,default,,,,,
179,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,5,3gmxo2,medium.com,Baidu explains how it mastered Mandarin with deep learning,https://www.reddit.com/r/MachineLearning/comments/3gmxo2/baidu_explains_how_it_mastered_mandarin_with_deep/,sherjilozair,1439324739,,10,72,False,http://b.thumbs.redditmedia.com/acNG-BwoduGv94wTOfaB8V3d1m5JVA3Hf6q9DicthtY.jpg,,,,,
180,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,12,3goewt,rnowling.github.io,Categorical Variable Encoding and Feature Importance Bias with Random Forests,https://www.reddit.com/r/MachineLearning/comments/3goewt/categorical_variable_encoding_and_feature/,ibgeek,1439348754,,13,4,False,http://a.thumbs.redditmedia.com/XfHGdW91Mj5aFAsU1uAnome2st6uRcvFmVAJe-4PYw0.jpg,,,,,
181,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,13,3goqmo,kdnuggets.com,Big Data Topical Influencers by @KDNuggets,https://www.reddit.com/r/MachineLearning/comments/3goqmo/big_data_topical_influencers_by_kdnuggets/,badadata200M,1439355179,,0,5,False,http://b.thumbs.redditmedia.com/KvpthGNxi5_JMJvRCzegXsczWnskk-SbCcEiKYMxZpE.jpg,,,,,
182,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,15,3gp0b4,self.MachineLearning,NeuroBayes - is it any good? Is there any way to try it?,https://www.reddit.com/r/MachineLearning/comments/3gp0b4/neurobayes_is_it_any_good_is_there_any_way_to_try/,knite,1439361524,"I've seen a few mentions of http://neurobayes.phi-t.de/index.php/public-information in various ML resources online. Most of the information is pretty vague.

Is NeuroBayes worth getting excited about it? Are there any open-source implementations?",4,4,False,self,,,,,
183,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,15,3gp0nu,youtube.com,Full-auto!!,https://www.reddit.com/r/MachineLearning/comments/3gp0nu/fullauto/,huada,1439361786,,1,0,False,default,,,,,
184,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,20,3gpogf,arxiv.org,[pdf] Mining for Causal Relationships: A Data-Driven Study of the Islamic State,https://www.reddit.com/r/MachineLearning/comments/3gpogf/pdf_mining_for_causal_relationships_a_datadriven/,It_Is1-24PM,1439380625,,3,6,False,default,,,,,
185,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,21,3gppye,github.com,benchmark for scalability/speed and accuracy of machine learning libraries for classification,https://www.reddit.com/r/MachineLearning/comments/3gppye/benchmark_for_scalabilityspeed_and_accuracy_of/,__mtb__,1439381538,,7,19,False,http://b.thumbs.redditmedia.com/cY8vNgdmkACcslv47u-ZYCwb4iUtdwHh5bmMogvSc_E.jpg,,,,,
186,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,21,3gptdr,self.MachineLearning,Problems where the input is a dataset,https://www.reddit.com/r/MachineLearning/comments/3gptdr/problems_where_the_input_is_a_dataset/,dylanbyte,1439383670,"I'm trying to think of learning problems where the input would be a set of vectors that are iid samples from some distribution, and the output would be a vector. The number of items in each set could vary. So the data would consist of many input datasets and corresponding output vectors.

Toy example: each input could be a number of samples from a gaussian and the output would be its entropy.

I would greatly appreciate any real-world examples.",5,1,False,self,,,,,
187,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,22,3gpvb0,self.MachineLearning,Is it possible to train a classifier with matricies?,https://www.reddit.com/r/MachineLearning/comments/3gpvb0/is_it_possible_to_train_a_classifier_with/,engineeringisthebest,1439384747,"Hi guys

I'm trying to do some machine learning on the hacker news dataset. I'm seeing if a classifier would be any good at predicting the score of a hacker news submission based on the title.

Now I know the standard way to represent text is with a bag-of-words model, which converts sentences to 1 dimensional vectors. 1-D vectors are obviously the type of data that a classifier is used to dealing with. However I was thinking about using part-of-speech tagging as well, as it may make the classification better. Part-of-speech tagging tags a word with it's grammatical quality (noun, verb etc.). Therefore if I were to use a BOW and POS, each word would be represented by 2 variables, the amount of times it's used in a sentence and the grammatical quality of the word. However this means that now the classifier would need to be trained on a 2D matrix, which is not a problem I am used to.

Do any of you have experience in training classifiers with this type of data? I'm interested to know how you use 2D data to train, if it is possible at all!",10,1,False,self,,,,,
188,MachineLearning,t5_2r3gv,2015-8-12,2015,8,12,23,3gq3jf,self.MachineLearning,How to define predict_model,https://www.reddit.com/r/MachineLearning/comments/3gq3jf/how_to_define_predict_model/,rick168,1439388922,"I am following the tutorial on this link: http://deeplearning.net/tutorial/logreg.html
and add some codes to define predict_model to predict test_set_x:

    predict_model = theano.function(
    inputs=[index],
    outputs=classifier.y_pred,
    givens={
        x: test_set_x[index * batch_size:(index + 1) * batch_size]
        }
    )

but the result is not what I wanted. 

    [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)]

The entire code is now uploaded to this link:

http://u.163.com/I5Xlhstz  
code: 1hepbyPU

test dataset is kaggle csv 100* 784

Previous related discussions can be found at this link:
https://groups.google.com/forum/#!topic/theano-users/ns4BxjHw7mA

Please take a look and help to fix. Thanks you!",4,0,False,self,,,,,
189,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,0,3gqb7c,self.MachineLearning,Explanation of PCA and r/caret for new learner,https://www.reddit.com/r/MachineLearning/comments/3gqb7c/explanation_of_pca_and_rcaret_for_new_learner/,ashramsoji,1439392352,"I am taking the Practical Machine Learning on Coursera and I am confused with one of the assignments. I want to be very clear that I am not posting because I want someone to give me the answer -- I just want help to understand what is happening.

We are learning about PCA and preprocessing. My understanding is that PCA is all about trying to collapse multiple colinear variables together into one factor, and that the PCA algorithm tries to find the best linear combination of these while using the least number of variables. For example, if I have 5 correlated highly variables, the PCA might find that just using 4 of them in a certain combination explains the most variance. Then, when you do the training, it applies whatever that combination of variables and coefficients was a single predictor instead of the individual variables (is that right)?

What I am having trouble understanding is what is happening behind the scenes. For example, in a lecture, there are 58 variables in a dataset (the 58th is the DV) and we are trying to use PCA for prediction. I am trying to understand what the code does and what sort of objects it is creating, and this is where I could use some help. Below is the code and my annotations that I think explain what is happening -- can someone correct me? This is all in the lecture, but the explanation was not very clear.

&gt;Goes through all 57 potential predictors, and finds the best set of 2 predictors that explain the most variance. Creates an object ?????

    preProc &lt;- preProcess(log10(training[,-58]+1),method=""pca"",pcaComp=2)

&gt;I have no idea what this line really does

    trainPC &lt;- predict(preProc,log10(training[,-58]+1))

&gt;Tries to fit the PCA model on the training data to predict TYPE?

    modelFit &lt;- train(training$type ~ .,method=""glm"",data=trainPC)

I am trying to apply this ""knowledge"" to a quiz question, which is explained below. I don't want the answer, just an explanation of what the code is doing.


**ACTUAL QUIZ QUESTION**

Load the Alzheimer's disease data using the commands:

    library(caret)
    library(AppliedPredictiveModeling)
    set.seed(3433)
    data(AlzheimerDisease)
    adData = data.frame(diagnosis,predictors)
    inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
    training = adData[ inTrain,]
    testing = adData[-inTrain,]
Create a training data set consisting of only the predictors with variable names beginning with IL and the diagnosis. Build two predictive models, one using the predictors as they are and one using PCA with principal components explaining 80% of the variance in the predictors. Use method=""glm"" in the train function. What is the accuracy of each method in the test set? Which is more accurate?",4,3,False,self,,,,,
190,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,3,3gr4qs,self.MachineLearning,MLP - What's the relationship between increasing the number of examples and the optimal hyperparameters?,https://www.reddit.com/r/MachineLearning/comments/3gr4qs/mlp_whats_the_relationship_between_increasing_the/,GAEMachineLearner,1439404453,"I'm training an MLP on 580*100 images, classifying them into 12 categories. 

With a learning rate of .001, 10k hidden units in the single layer, and 300 examples, (I know its way too small just for testing) it has a monotonically decreasing error rate. However, when I increase the number of examples to 3000, the error rate just doesn't move at all.

I'm trying to understand how the number of examples is related to the correct hyperparameters. Thanks very much in advance!",5,1,False,self,,,,,
191,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,3,3gr8df,self.MachineLearning,Replicating neural conversations paper,https://www.reddit.com/r/MachineLearning/comments/3gr8df/replicating_neural_conversations_paper/,ma2rten,1439405941,"I would like to replicate the neural conversations paper. I am going to be using an AWS 4 GPU instance. 

I was wondering if anyone here was tried to do the same. I've used theano before, but I know it's very difficult, if not impossible to implement model parallelism efficiently in theano.

Is it possible to do efficient an multi-gpu implementation in torch (using fbcunn?) or should I look into cublas / cudnn? 

Has anyone else here tried to implement this model and ran into any unexpected issues?",0,0,False,self,,,,,
192,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,5,3grko3,self.MachineLearning,Good GPUs for Deep LEarning in Matlab?,https://www.reddit.com/r/MachineLearning/comments/3grko3/good_gpus_for_deep_learning_in_matlab/,[deleted],1439410971,"Hello! I have been lurking this subreddit for a while. I want to apply some GPU-processing to my machine-learning projects in Matlab. I have just started with neural networks and it seems like the functionality for running them on the GPU through Matlab is somewhat lacking. Anybody running neural networks on the GPU in Matlab that has any insights in how smoothly it works? Any special GPUs that works especially good for that programming language? I am waiting for the new Mac Pro but realise that maybe I should put together a windows machine instead.

Edit: Sorry for error in capitalization in headline



",18,0,False,default,,,,,
193,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,5,3grljn,self.MachineLearning,Homepage on the Origins of the Extreme Learning Machines,https://www.reddit.com/r/MachineLearning/comments/3grljn/homepage_on_the_origins_of_the_extreme_learning/,ELM_Xposed,1439411344,"The objective of launching this homepage (http://elmorigin.wix.com/originofelm) is to present the evidences regarding the tainted origins of the extreme learning machines (ELM). As we would like all readers to verify the facts within a short period of time (perhaps 10 to 20 minutes), we have uploaded a dozen of PDF files with highlights and annotations clearly showing the following: 

1. The kernel (or constrained-optimization-based) version of ELM (ELM-Kernel, Huang 2012) is identical to kernel ridge regression (for regression and single-output classification, Saunders ICML 1998, as well as the LS-SVM with zero bias; for multiclass multi-output classification, An CVPR 2007).

2. ELM-SLFN (the single-layer feedforward network version of the ELM, Huang IJCNN 2004) is identical to the randomized neural network (RNN, with omission of bias, Schmidt 1992) and another simultaneous work, i.e., the random vector functional link (RVFL, with omission of direct input-output links, Pao 1994).
 
3. ELM-RBF (Huang ICARCV 2004) is identical to the randomized RBF neural network (Broomhead-Lowe 1988, with a performance-degrading  randomization of RBF radii or impact factors).
 
4. In all three cases above, Huang got his papers published after excluding a large volume of very closely related literature.

5. Hence, all 3 ""ELM variants"" have absolutely no technical originality, promote unethical research practices among researchers, and steal citations from original inventors. For easy verifications on the origins of the ELM, with annotated PDF files, please visit: 
 
http://elmorigin.wix.com/originofelm 
 
Please forward this message to your contacts so that others can also study the materials presented at this website and take appropriate actions, if necessary.

ELM: The Sociological Phenomenon
 
Since the invention of the name extreme learning machines (ELM) in 2004, the number of papers and citations on the ELM has been increasing exponentially. How can this be imaginable for the ELM comprising of 3 decade-old algorithms published by authors other than the ELM inventor? This phenomenon would not have been possible without the support and participation of researchers on the fringes of machine learning. Some (unknowingly and a few knowingly) love the ELM for various reasons:
 
  Some authors love the ELM, because it is always easy to publish ELM papers in an ELM conference or an ELM special issue. For example, one can simply take a decade-old paper on a variant of RVFL, RBF or kernel ridge regression and re-publish it as a variant of the ELM, after paying a small price of adding 10s of citations on Huangs classic ELM papers.
 
  A couple of editor-in-chiefs (EiCs) love the ELM and offer multiple special issues/invited papers, because the ELM conference &amp; special issues will bring a flood of papers, many citations and therefore high impact factors to their low quality journals. The EiCs can claim to have faithfully worked within the peer-review system, i.e. the ELM submissions are all rigorously reviewed by ELM experts.
 
  A few technical leaders, e.g. some IEEE society officers, love the ELM, because it rejuvenates the community by bringing in more activities and subscriptions.
 
   A couple of funding agencies love the ELM, because they would rather fund a new sexy name, than any genuine research.
 
One may ask: how can something loved by so many be wrong?
 
A leading cause of the current Greek economic crisis was that a previous government showered its constituents with jobs and lucrative compensations, in order to gain their votes, thereby raising the debt to an unsustainable level. At that time, the government behavior was welcome by many, but led to severe consequences. Another example of popularity leading to a massive disaster can be found in WW II as Hitler was elected by popular votes.
 
The seemingly small price to pay in the case of the ELM is the diminished publishing ethics, which, in a long run, will fill the research literature with renamed junk, thereby making the research community and respected names, such as IEEE, Thomson Reuters, Springer and Elsevier, laughing stocks. Similar to that previous Greek government and its supporting constituents, the ELM inventor and his supporters are borrowing from the future of the entire research community for their present enjoyment! It is time to wake up to your consciousness.
 
Our beloved peer-review system was grossly abused and failed spectacularly in the case of the ELM. It is time for the machine learning experts and leaders to investigate the allegations presented here and to take corrective actions soon.

5 Easy but Proven Steps to Academic Fame

1. The Brink of Genius: Take a paper published about 20 years ago (so that the original authors have either passed away, retired, or are too well-established/generous to publicly object. Unfortunately, pioneers like Broomhead and Pao have passed away). Introduce a very minor variation, for example, by fixing one of the tunable parameters at zero (who cares if this makes the old method worse, as long as you can claim it is now different and faster). Rewrite the paper in such a way that plagiarism software cannot detect the similarity, so that you are not in any of the IEEE 5 levels of plagiarism. Give a completely new sensational name (hint: the word extreme sounds extremely sexy). 

2. Publication: Submit your paper(s) to a poor quality conference or journal without citing any related previous works.

3. Salesmanship: After publishing such a paper, now it is time to sell the stolen goods! Never blush. Don't worry about ethics. Get your friends/colleagues to use your big thing. Put up your Matlab program for download. Organize journal special issues, conferences, etc. to promote these unethical research practices among junior researchers who would just trust your unethical publications without bothering to read the original works published in the 1980s or 1990s. Of course, the pre-requisite for a paper to be accepted in your special issues/conferences is 10s of citations for your unethically created name and publications. Invite big names to be associated with your unethically created name as advisory board members, keynote speakers, or co-authors. These people may be too busy to check the details (with a default assumption that your research is ethical) and/or too nice to say no. But, once infected with your unethically created name, they will be obliged to defend it for you. 

4. The Smoke Screen: Should others point out the original work, you claim not to know the literature while pointing to a minor variation that you introduced in the first place. Instead of accepting that your work was almost the same as the literature and reverting back to the older works, you promote your work by: (1) repeating the tiny variation; (2) excluding the almost identical works in the list of references or citing and describing them incorrectly; (3) excluding thorough experimental comparisons with nearly identical works in the literature so that worse performance of your minute variations will not be exposed; (4) making negative statements about competing methods and positive statements about your unethically created name without solid experimental results using words like may or analysis; (5) comparing with apparently different methods. You can copy the theories and proofs derived for other methods and apply to your method (with tiny variation from those in the old literature) claim that your method has got a lot of theories while others do not have.

5. Fame: Declare yourself as a research leader so that junior researchers can follow your footsteps. Enjoy your new fortune, i.e., high citations, invited speeches, etc. You dont need to be on the shoulders of giants, because you are a giant! All you have to do to get there is to follow these easy steps! 

One can call the above steps IP (Intelligent Plagiarism), as opposed to stupid (verbatim) plagiarism specified by the IEEE in 5 levels. The machine learning community should feel embarrassed if IP (Intelligent Plagiarism) was originally developed and/or grandiosely promoted by this community, while the community is supposed to create other (more ethical) intelligent algorithms to benefit the mankind.
",0,2,False,self,,,,,
194,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,6,3grtkz,self.MachineLearning,Why multiple epochs are needed for SGD?,https://www.reddit.com/r/MachineLearning/comments/3grtkz/why_multiple_epochs_are_needed_for_sgd/,NovaRom,1439414765,"Intuitively I think only one epoch should be sufficient with a ""proper"" change in SGD algorithm. Do you know if there is any research in this direction (reducing number of training data repetitions without accuracy loss) ? 

Think, You have training data. Then you look at all the data once and got some statistics. Then, you perform a ""perturbated"" SGD with your new knowledge to avoid further training repetitions (additional epochs).",29,0,False,self,,,,,
195,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,7,3gryuu,reddit.com,X-Post /r/datascience: Naive Bayes Movie Recommendation. Do I get it right?,https://www.reddit.com/r/MachineLearning/comments/3gryuu/xpost_rdatascience_naive_bayes_movie/,greeedy,1439417122,,0,1,False,http://a.thumbs.redditmedia.com/PDQadCzYX_x1bU3KrYuhTptu6eDdOVVagFG6q_Afyb4.jpg,,,,,
196,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,10,3gsksw,self.MachineLearning,Double major CS/Math for masters in ML or not?,https://www.reddit.com/r/MachineLearning/comments/3gsksw/double_major_csmath_for_masters_in_ml_or_not/,ucsdbound15,1439427736,"I go to UCSD as can be figured from my username. My end academic goals are to get a masters in machine learning and I am aiming for top schools such as Stanford and MIT.

I'm confused in whether or not I should double major. Here are my choices:

1. Double Major CS/Applied Math

2. Major CS/Minor Math (will be graduating at least a quarter early and save $7000/quarter)

3. Major CS/use the extra time to take some grad level courses in CS

Which is the most beneficial? Number 1 will be the most time consuming and difficult and will probably result in the worst GPA of the three choices, but gives me the most breadth knowledge for ML. Also what is more important to grad schools overall GPA or major GPA?

Any help is appreciated.",12,0,False,self,,,,,
197,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,19,3gu46y,profresearchreports.com,NMR Autosampler 2009-2019 Global and Chinese Market Analysis,https://www.reddit.com/r/MachineLearning/comments/3gu46y/nmr_autosampler_20092019_global_and_chinese/,ProfReport,1439463433,,0,1,False,default,,,,,
198,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,20,3gu5w8,self.MachineLearning,"China film blowing machine manufacturer &amp; supplier, Kingsun Machinery",https://www.reddit.com/r/MachineLearning/comments/3gu5w8/china_film_blowing_machine_manufacturer_supplier/,sonalbisht101,1439464599,"As China film blowing machine manufacturer &amp; supplier, Kingsun Machinery supplies different grade rotary die film blowing machines or film extruders, with high quality and low cost. You can feel free to let us know your production requirements so we can work out a good solutions for your plastic film making. We are always ready for providing you the best service and plastic film extrusion technology.

http://www.kingsunmachinery.com/category/flexible-packaging-machinery/film-blowing-machines-105.html",0,1,False,default,,,,,
199,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,21,3guejd,arxiv.org,The Effects of Hyperparameters on SGD Training of Neural Networks: a Google paper on practical aspects of neural networks architecture,https://www.reddit.com/r/MachineLearning/comments/3guejd/the_effects_of_hyperparameters_on_sgd_training_of/,olBaa,1439470202,,17,26,False,default,,,,,
200,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,22,3guflg,medium.com,A.I Astrology: Predicting Social Characteristics of Famous People using NLP.,https://www.reddit.com/r/MachineLearning/comments/3guflg/ai_astrology_predicting_social_characteristics_of/,samim23,1439470861,,0,4,False,http://a.thumbs.redditmedia.com/ApvICT2-a4HEMDlECb9soq7Ej4kjhfCFkrkgLV7-_k8.jpg,,,,,
201,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,22,3guio9,inference.vc,Representing Model Uncertainty in Reinforcement Learning and Classification: Are Bayesian ConvNets Harder to Fool?,https://www.reddit.com/r/MachineLearning/comments/3guio9/representing_model_uncertainty_in_reinforcement/,fhuszar,1439472549,,0,5,False,http://b.thumbs.redditmedia.com/rODnsesbdadhOMuA1w-mpSCJOeTzLstboLLkVHkfFfs.jpg,,,,,
202,MachineLearning,t5_2r3gv,2015-8-13,2015,8,13,22,3gum4i,self.MachineLearning,Distributed deep learning in Erlang?,https://www.reddit.com/r/MachineLearning/comments/3gum4i/distributed_deep_learning_in_erlang/,ana_s,1439474335,"Hi everyone! Was wondering what you guys thought about implementing distributed deep learning using erlang. Basically the deep neural network would be modelled with each neuron acting as a separate process. Since erlang allows millions of processes and also scales across machines, this method could be used to potentially model large neural networks. Thoughts?",18,5,False,self,,,,,
203,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,0,3guycb,self.MachineLearning,ML for improving search results and information retrieval,https://www.reddit.com/r/MachineLearning/comments/3guycb/ml_for_improving_search_results_and_information/,minervaxox,1439479861,"Hi everyone,

Background: I'm building a search app to help teenagers find maker and science project ideas. 

I know this is a difficult problem to solve, but I was hoping I could improve my search results a least a bit by adding some machine learning. I have a set of difficulties I'm running into., but the main one is the difference between an article that is ""about"" a topic and one that mentions a topic.

I'm willing to go through a large set of articles (1000? more?), and tag them either ""About"" or ""Mention"". Basically, I want to be able to filter out articles that are not really about a certain topic, but just mention the keyword. So, if I'm looking for articles on Interviewing, I don't want articles where a person is being interviewed aka ""Interview with Mr. X about cars"" to come up in my results. If I'm looking for articles about commuting, how can filter out articles like this (http://www.psfk.com/2012/07/ipad-kindle-tablets.html) which really is about Ipads and only mentions commuting in passage.

My hunch is I need run some sort of Machine learning algorithm with features that consider topic, and the word's position in a sentence in the title and in the article (by using a Part-of-speech tagger). But it's just a general idea, and I'm not sure how to exactly achieve this, let alone get good results. 

If you have any tips or know of tools that would help, please let me know!

Thanks



",2,3,False,self,,,,,
204,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,0,3gv1nh,googleresearch.blogspot.com,The reusable holdout: Preserving validity in adaptive data analysis,https://www.reddit.com/r/MachineLearning/comments/3gv1nh/the_reusable_holdout_preserving_validity_in/,carmichael561,1439481293,,0,1,False,http://b.thumbs.redditmedia.com/BYF8jKWi_TaDaHFY1ocFvZaD--Lig3krPPHdntAagMU.jpg,,,,,
205,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,1,3gvamn,self.MachineLearning,Feature creation methods when using custom cost functions,https://www.reddit.com/r/MachineLearning/comments/3gvamn/feature_creation_methods_when_using_custom_cost/,just_learning_,1439485107,"If I define a custom cost function, are there techniques to use to generate features. Could I use the landmark idea used in SVMs or some other technique?",3,0,False,self,,,,,
206,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,2,3gviun,blog.monkeylearn.com,Sentiment Analysis APIs Benchmark,https://www.reddit.com/r/MachineLearning/comments/3gviun/sentiment_analysis_apis_benchmark/,wildcodegowrong,1439488480,,1,1,False,default,,,,,
207,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,3,3gvjn0,ajkl.github.io,Introduction to Julia for Kaggle Titanic dataset,https://www.reddit.com/r/MachineLearning/comments/3gvjn0/introduction_to_julia_for_kaggle_titanic_dataset/,jinkkal,1439488818,,12,6,False,http://a.thumbs.redditmedia.com/cnXoc0NMDRydyt37HAb0F0M7F1qRCk6ZrrRlTFZPZc0.jpg,,,,,
208,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,3,3gvnx7,self.MachineLearning,"best performance, highest reliability, lowest complexity dim. red. for image classification?",https://www.reddit.com/r/MachineLearning/comments/3gvnx7/best_performance_highest_reliability_lowest/,GAEMachineLearner,1439490621,What's the go to for this? I see a toolchest of options ...,1,0,False,self,,,,,
209,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,3,3gvp4f,self.MachineLearning,looking for resources to learn about Variational Methods (and inference),https://www.reddit.com/r/MachineLearning/comments/3gvp4f/looking_for_resources_to_learn_about_variational/,fromtheoffice,1439491113,"Hi,

I need to get a good background on variational methods, especially for inference. I need to do this for my MSc thesis. Any hints? Good books that contain both easily digestible introductory material and more advanced applications?

Some tutorials with Python would also be particularly appreciated...",6,12,False,self,,,,,
210,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,3,3gvqa6,self.MachineLearning,"Project code ""Conversational RNN LSTM"" is incomplete or I do not know to run?",https://www.reddit.com/r/MachineLearning/comments/3gvqa6/project_code_conversational_rnn_lstm_is/,xDarknight0x,1439491594,"Hi, I'm trying to run this code, follow the steps that tells me starting the parseData.py and creates pickle and txt files, and then not know what else to do, in the project information it says ""We then a run LSTM py trained and output to model"" but I see no LSTM.py script, I want to know is if the project is incomplete or I do not know to run; if that is the second answer, I would want someone who can help me, thank you very much

https://github.com/gangopad/ConversationalNeuralNet

Anyone know of any chatbot with neural networks?",0,0,False,self,,,,,
211,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,4,3gw09w,github.com,Example of How to Construct 1D Convolutional Net on Text (keras),https://www.reddit.com/r/MachineLearning/comments/3gw09w/example_of_how_to_construct_1d_convolutional_net/,galapag0,1439495748,,1,4,False,http://b.thumbs.redditmedia.com/3A_cS3pE0_MCsPBCnfDZAF70J7mEWaIAtsTxTBk3fGc.jpg,,,,,
212,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,5,3gw8wd,devblogs.nvidia.com,NVIDIA and IBM Cloud Support ImageNet Large Scale Visual Recognition Challenge,https://www.reddit.com/r/MachineLearning/comments/3gw8wd/nvidia_and_ibm_cloud_support_imagenet_large_scale/,harrism,1439499314,,0,8,False,http://b.thumbs.redditmedia.com/cQgiGwS8LkGvZQboYDN9ZZQAbrFntROoQHTLfGO5mOE.jpg,,,,,
213,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,6,3gwc93,self.MachineLearning,CPU for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3gwc93/cpu_for_machine_learning/,petlra,1439500702,"X-post from /r/buildapc

I'm a college student interested in machine learning, and I want to build a desktop that I'd like to last through my next three years of undergrad. Currently, I'm involved in research in both standard ML (CPU helps for experiments, but its not essential) and deep learning (where GPU matters). For the latter, I have a 980 right now, and plan on upgrading to at least dual Titan X's eventually. I also am interested in Kaggle and ML + computational biology, so having a good CPU is probably also important.

Price is not a big issue, and I have essentially 50% off retail prices via Intel EPP. So, my options are:

* 5960X for ~$500
* 5930K for ~$300
* 5820K for ~$250
* 6700K for ~$200 (expected)

Which one should I get? The 6700k and 5820k only have enough PCIe lanes for 8x bandwidth even at dual SLI. But will the extra cores of the 5960X or 5930K be useful?
",8,0,False,self,,,,,
214,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,6,3gwdi2,pypi.python.org,Lasagne v0.1 released,https://www.reddit.com/r/MachineLearning/comments/3gwdi2/lasagne_v01_released/,abracaradbra,1439501227,,16,72,False,default,,,,,
215,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,7,3gwjaz,self.MachineLearning,ASUS ROG for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3gwjaz/asus_rog_for_deep_learning/,DSLSharedTask,1439503775,https://timdettmers.wordpress.com/2015/03/09/deep-learning-hardware-guide/ has a great guide on which card/chip to get for deep learning but can an off-the-shelf CPU do the same (e.g. ASUS ROG: http://www.amazon.de/gp/product/B00WSXMS9I),6,1,False,self,,,,,
216,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,7,3gwjl3,self.MachineLearning,Computer Science Topics for Machine Learning (Non-CS Background),https://www.reddit.com/r/MachineLearning/comments/3gwjl3/computer_science_topics_for_machine_learning/,alSimmonsSpawn,1439503915,"I am currently employed as a Data Scientist but will be starting my PhD in Economics this fall. I do not come from a computer science background but I am looking to get more into machine learning (on a deeper level for my PhD as I want to explore the possibilities for economics (econometrics) and machine learning). I am currently proficient in R, Python and C (learning C++). 

With that being said, what background should I have in CS to fully explore this topic i.e., Kernel, Compilers, Operating Systems, Algorithms and Data Structures etc...

Any input would be greatly appreciated! Thank you in advance. ",7,1,False,self,,,,,
217,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,19,3gynbf,self.MachineLearning,"Markov chain: likelihood of sample with ""unseen"" observations (probability 0)",https://www.reddit.com/r/MachineLearning/comments/3gynbf/markov_chain_likelihood_of_sample_with_unseen/,thomas_tp,1439549011,"Hi,

I have a large Markov chain and a sample, for which I want to calculate the likelihood. The problem is that some obervations or transisitions in the sample don't occur in the Markov chain, which makes the total likelihood 0 (or the log-likelihood - infinity). I was wondering if there's a way to still have a meaningfull likelihood.

I tried already to filter out these ""unknown"" observations in the sample and report them seperately. But the problem with that is that I want to compare the likelihood of the sample with the likelihood of the same sample, but after a transformation. The transformed sample has a different amount of ""unknown"" observations. So I don't think I can compare these two likelihoods, seeing as they have been calculated with a different amount of observations. 

Is there a way to still calculate a meaningfull likelihood that can be compared? I was thinking about averaging the probabilities of the observations in the sample, but I can't find anything about that being correct.",7,2,False,self,,,,,
218,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,19,3gyo36,self.MachineLearning,Running multiple models in parallel within the same GPU with Theano,https://www.reddit.com/r/MachineLearning/comments/3gyo36/running_multiple_models_in_parallel_within_the/,fariax,1439549637,"Hello! I would like to know if I can run, for example, 30 independent Neural networks with the same architecture, each Neural network running inside a GPU core, each core running train + test. Commonly, the GPU implementations parallelize the execution inside the model. I would like to make the run of a single model sequential, and the parallelism would be on the 30 models running sequentially at their own GPU cores.",16,0,False,self,,,,,
219,MachineLearning,t5_2r3gv,2015-8-14,2015,8,14,22,3gz4q6,self.MachineLearning,Why don't we use motion more often to learn vision ?,https://www.reddit.com/r/MachineLearning/comments/3gz4q6/why_dont_we_use_motion_more_often_to_learn_vision/,Schlagv,1439559834,"I will ask a stupid question: when we try to learn vision patterns, we do it using pictures 99% of the time.

To recognize borders of objects, motion is very helpful, as we see easily that pixel fields move the same way inside each solid object.

Why don't we use more small motions to learn vision tasks (even for segmentation we use fixed images!).

Let's say you play at a 2D game. You will quickly find what is the background and what are characters not because of visual patterns, but because character pixels move together.

RNN-convnets may be hard to implement, but simple convnets would do the job, with more input channels. Instead of RGB, we put RGBx5 to do 5 timesteps, so 15 input channels instead of 3, nothing else to change in the algorithm.

Instead of just learning borders in the first layer, we would learn borders with motion on one side and no motion on the other side, and the direction of the motion.

Making datasets wouldn't be such a hard task given the large amount of videos we have today (or video games).

Are there papers on those ideas ?",18,36,False,self,,,,,
220,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,0,3gzgu4,self.MachineLearning,Any (pure) mathematicians who ended up in machine learning?,https://www.reddit.com/r/MachineLearning/comments/3gzgu4/any_pure_mathematicians_who_ended_up_in_machine/,zomorodian,1439565515,"Hi, I am currently hovering between to PhD's and I was hoping someone here could give me some input on what I should choose.

My background is in pure mathematics, where I just have finished my master's degree. I have offers on two PhD's, one in pure mathematics and one where I will use machine learning heavily. So I wondered if there were anyone here with background in pure mathematics who could tell me how the transition to machine learning went for them.

I don't have much statistics in my degree, and I know next to nothing about machine learning as of now, so I'll have to learn this if I choose machine learning.

I'll highly appreciate anything you have to say on this.",13,3,False,self,,,,,
221,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,1,3gzpc7,blog.datadive.net,Random forest interpretation with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/3gzpc7/random_forest_interpretation_with_scikitlearn/,datadive,1439569092,,0,2,False,default,,,,,
222,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,1,3gzq52,cs.ox.ac.uk,Oxford University Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/3gzq52/oxford_university_machine_learning_course/,dabshitty,1439569413,,13,91,False,default,,,,,
223,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,1,3gzqdi,self.MachineLearning,can you do deep learning in a Ubuntu virtual box?,https://www.reddit.com/r/MachineLearning/comments/3gzqdi/can_you_do_deep_learning_in_a_ubuntu_virtual_box/,poporing88,1439569515,I have Windows platform but want to perform deep learning using NVIDIA Digits ,4,2,False,self,,,,,
224,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,3,3h05x3,self.MachineLearning,"good explanation of ""channels"" in data sets?",https://www.reddit.com/r/MachineLearning/comments/3h05x3/good_explanation_of_channels_in_data_sets/,jstrong,1439575809,"hey guys,

Playing around with neural nets for the first time and following the tutorial in lasagne. One thing that tripped me up is the existence of ""channels"" in the mnist dataset. 

I understand in concept that a color image would have three channels of data, which are essentially three matrices of the same size representing the data of that object in different dimensions. 

The problem I'm working on is not image recognition and I've never done image recognition stuff so this has not come up. My data is the traditional 2d row/column matrix. I can't get the lasagne tutorial build_cnn function to work with that and I'm trying to find a good explanation for data with channels and what I need to be using if my data doesn't have channels. 

Any suggestions? 



",10,1,False,self,,,,,
225,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,5,3h0qqv,self.MachineLearning,why would an MLP decrease the error rate much faster than a logistic sgd for the same batch size?,https://www.reddit.com/r/MachineLearning/comments/3h0qqv/why_would_an_mlp_decrease_the_error_rate_much/,GAEMachineLearner,1439584411,"This seems counterintuitive to me, or is this often the case? Assuming that they both have been well tuned with the right hyperparameters.",1,0,False,self,,,,,
226,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,7,3h18ne,self.MachineLearning,UK Masters in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3h18ne/uk_masters_in_machine_learning/,ah07171,1439592272,"What are people's opinion on Graduate Machine Learning courses in the UK? I believe UCL have one, as well as Cambridge, and of course Edinburgh has Artificial Intelligence MSc. Has anyone had any experience with these courses? Or know someone that has? How are they looked upon by future employers? If by chance you do know. I'm very interested in applying to one of these courses.",9,5,False,self,,,,,
227,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,8,3h1cou,engineering.pinterest.com,Learning about your business from anomalous metrics,https://www.reddit.com/r/MachineLearning/comments/3h1cou/learning_about_your_business_from_anomalous/,dfrankow,1439594178,,0,1,False,default,,,,,
228,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,8,3h1fuy,self.MachineLearning,Help! Looking for a thesis topic. Suggestions?,https://www.reddit.com/r/MachineLearning/comments/3h1fuy/help_looking_for_a_thesis_topic_suggestions/,apple-sauce,1439595720,"Hi guys, as the title says, I am looking for some suggestions for a (MSc) thesis topic. More specifically, something on the *intersection* of **machine learning and business.**  So, how can machine learning can create business value.


My initial idea was researching (big) data and its application to business, and how this creates (business) value. But this may be a bit general?


For example *deep learning* looks very promising and appears to be the next hot topic. 


ANY help is much appreciated :-)


[Thanks!](http://media.tumblr.com/9bdb00dce3735fa212a78eba27570bd4/tumblr_inline_mjma56n4ww1rv04lt.gif)",7,0,False,self,,,,,
229,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,9,3h1k2m,github.com,Am I wrong about this ?,https://www.reddit.com/r/MachineLearning/comments/3h1k2m/am_i_wrong_about_this/,[deleted],1439597894,,0,1,False,default,,,,,
230,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,9,3h1ket,self.MachineLearning,Detecting whether two different photos of a (rectangular) poster are of the same poster?,https://www.reddit.com/r/MachineLearning/comments/3h1ket/detecting_whether_two_different_photos_of_a/,compsc,1439598060,"Don't know much about ML.  Say there are two slightly close-up pictures of a poster which may or may not be the same.  I'd like to estimate whether they're of the same poster.

I guess there would be two steps.  One is to detect a quadrilateral and normalize the shape and size of the (or all) rectangle(s) detected in the image.  Maybe the whole photo can be used to normalize a color histogram as well.  From that point I suppose it'd be like any other image duplication-detection scenario,  aside from the possibility that OCR could be leveraged if there is text.

Any thoughts?

thanks",5,0,False,self,,,,,
231,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,16,3h2pzw,self.MachineLearning,Help with Nvidia DIGITS,https://www.reddit.com/r/MachineLearning/comments/3h2pzw/help_with_nvidia_digits/,Weihua99,1439623274,"Hello, I need to do some work with Nvidia DIGITS, but I don't have a Nvidia GPU. I would like to purchase one, and I was looking at [this one](http://www.amazon.com/EVGA-GeForce-1024MB-Graphics-01G-P3-2615-KR/dp/B00847TOLC/ref=sr_1_2?ie=UTF8&amp;qid=1439623015&amp;sr=8-2&amp;keywords=gt+430) since it's so ridiculously cheap, and supports CUDA. I'm not concerned about speed, but what CUDA version does DIGITS need to be able to run? The GT 610 supports up to Compute 2.1, but I've only seen demos of DIGITS on GTX 980's and Titans which support higher Compute versions. Basically my question is: since I'm not concerned about speed, will this cheap GT 610 work with DIGITS, or do I have to spend a lot of money on a newer GTX video card?",6,0,False,self,,,,,
232,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,20,3h34g8,ml-india.org,"ML - India: fostering, acknowledging and increasing collaboration in India's ml and data eco-system",https://www.reddit.com/r/MachineLearning/comments/3h34g8/ml_india_fostering_acknowledging_and_increasing/,lotus_pond,1439637538,,5,15,False,http://b.thumbs.redditmedia.com/oBBpZBxPqTDElhnlirWRkjWqRQWOSRTd7Fo4wPeXqGs.jpg,,,,,
233,MachineLearning,t5_2r3gv,2015-8-15,2015,8,15,23,3h3k4u,self.MachineLearning,Examples of good PhD theses involving machine learning?,https://www.reddit.com/r/MachineLearning/comments/3h3k4u/examples_of_good_phd_theses_involving_machine/,bstamour,1439649388,"Hi everyone,

I'm just getting my feet wet in machine learning, and also starting a PhD in computer science. The project that I'm working on, while not about machine learning directly, will involve a fair bit of data analysis, in particular classification. I'm wondering if you know of any recently published PhD theses, or even major papers, that you would recommend a newcomer to read in order to gain some perspective/inspiration. I'm not looking for research ideas, just some examples of where machine learning has been used to obtain good results :-)

Thanks for your time.",16,30,False,self,,,,,
234,MachineLearning,t5_2r3gv,2015-8-16,2015,8,16,6,3h4tuy,self.MachineLearning,How do GRUs solve the vanishing gradient problem?,https://www.reddit.com/r/MachineLearning/comments/3h4tuy/how_do_grus_solve_the_vanishing_gradient_problem/,nnnoob123,1439672546,"Hi,

I've been learning about GRUs and LSTMs, and the vanishing gradient problem which motivated LSTMs.

I can see how LSTMs could propagate error signal far back, by ""writing"" to the memory and then later ""reading"" from the memory; the error single from the ""read"" timestep would naturally propagate back to the ""write"" timestep.

However, GRUs, which have been shown to be as good as LSTMs, don't have this memory cell. They only have two gates: one that decides how much to reset the hidden state, and one that decides how much to take the current input into account. It doesn't seem to me like either of these gates should help solve the vanishing gradient problem. The input gate certainly doesnt, as all it can do is ignore an input. But the reset gate should't be able to help either -- if it resets the hidden state, all error signals terminate there, which makes the gradient vanish; if it doesn't reset the hidden state, it's essentially the same as a vanilla RNN, which runs into the vanishing gradient problem.

Does anyone have any good resources to answer this question?

Thanks!",11,18,False,self,,,,,
235,MachineLearning,t5_2r3gv,2015-8-16,2015,8,16,8,3h58is,twitter.com,"Pedro Domingos on Twitter: Geoff Hinton, arriving home from work: ""I did it! I've figured out how the brain works!"" His daughter: ""Oh Dad, not again!"" (True story.)",https://www.reddit.com/r/MachineLearning/comments/3h58is/pedro_domingos_on_twitter_geoff_hinton_arriving/,wonkypedia,1439680207,,5,21,False,http://b.thumbs.redditmedia.com/C_Ad0ZtTBoRH3wDShU2KkJPEl10YFqN0mfODU5opOww.jpg,,,,,
236,MachineLearning,t5_2r3gv,2015-8-16,2015,8,16,13,3h63qv,self.MachineLearning,is there a term for a model like an HMM but with deterministic transitions between the states?,https://www.reddit.com/r/MachineLearning/comments/3h63qv/is_there_a_term_for_a_model_like_an_hmm_but_with/,GAEMachineLearner,1439698585,"Thanks very much in advance, not sure if this exists or not!",4,3,False,self,,,,,
237,MachineLearning,t5_2r3gv,2015-8-16,2015,8,16,15,3h6cmf,codesachin.wordpress.com,Logistic Regression (for dummies),https://www.reddit.com/r/MachineLearning/comments/3h6cmf/logistic_regression_for_dummies/,sachinrjoglekar,1439705036,,5,58,False,http://b.thumbs.redditmedia.com/Sbjv5hPHUze7gAwRB2bAAnaMzbsgPLP1yPId45kotBo.jpg,,,,,
238,MachineLearning,t5_2r3gv,2015-8-16,2015,8,16,20,3h6wua,self.MachineLearning,Suggestions for building an RNN for sentence mapping.,https://www.reddit.com/r/MachineLearning/comments/3h6wua/suggestions_for_building_an_rnn_for_sentence/,machineYearning,1439725056,"I'm familiar with the theory of RNNs and machine learning generally as a postgrad but haven't implemented a recurrent network before. I wonder if anyone has any suggestions. I have watched many tutorials and read around the subject a lot, so my issue is more with the dirty implementation work than anything theoretical (I think!).

The idea is to map sentences (represented as sequences of word vectors) onto another set of fixed-length vectors that I've generated. Since there are semantic dependencies within the sentences, a recurrent network is what I want.

This varies from the sentiment analysis tasks for which there are are solutions online, because I'm not dealing with parse trees and I'm not trying to map sentences to scalar classification labels, but to vectors. In this sense it's maybe more similar to translation.

I have gone through the Theano tutorials but cannot find some model solution similar enough to what I want to dive into modifications. Does anyone know something like this?

To try to be as clear as possible, the input sequences maybe be something like:

     ""I"" -&gt; index -&gt; word vector = [0.23 0.54 0.12 ...].  
     ""want"" -&gt; index -&gt; word vector = [0.16 0.87 0.99 ...].  
     ""RNN"" -&gt; index -&gt; word vector = [0.11 0.09 0.65 ...].  

And the output sequences are a vector per word according to a function I've used to calculate them (i.e. I will optimize error against these 'true' vectors):

     ""I"" -&gt;  training output vector = [0.00 0.00 0.55 ...].  
     ""want"" -&gt; training output vector = [0.00 0.45 0.00 ...].  
     ""RNN"" -&gt;  training output vector = [0.00 0.00 0.75 ...].  

Any bright lights in here have any suggestions for how to get started on this?

Thanks!",7,2,False,self,,,,,
239,MachineLearning,t5_2r3gv,2015-8-16,2015,8,16,23,3h78o0,self.MachineLearning,Something I dont understand in CNNs,https://www.reddit.com/r/MachineLearning/comments/3h78o0/something_i_dont_understand_in_cnns/,[deleted],1439734112,"Ive worked a fair bit with standard neural networks, but not so much with CNNs there is something that eludes me in CNNs.

Suppose we have a black and white image (1 channel), and 10 filters for the first convolutional layer. The output of that layer would be 10 feature maps, one for each filter. Following that however, if we have another convolutional layer, how are the filters applied? Say we have another 10 filters? Do we apply the filters in parallel? If we had 20 (2 per feature map) would that mean we would apply two filters per feature map, resulting in 20 feature maps at the next layer? Does that mean we cant have less filters as we go deeper? (aka if can only be k filters, then n * k filters, then m * n * k filters, etc)
Im just not sure I am visualizing well what happens in the layers after the input... Thanks",3,1,False,default,,,,,
240,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,1,3h7mk1,highnoongmt.wordpress.com,Deep learning for assisting the process of music composition,https://www.reddit.com/r/MachineLearning/comments/3h7mk1/deep_learning_for_assisting_the_process_of_music/,ma2rten,1439742042,,1,62,False,http://b.thumbs.redditmedia.com/3GC7lQ0qWaZhuUqazzFGu0S7DTxlbU6s8jSm_pwPzqo.jpg,,,,,
241,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,2,3h7s9i,self.MachineLearning,Which softwares use neural dropout?,https://www.reddit.com/r/MachineLearning/comments/3h7s9i/which_softwares_use_neural_dropout/,BenRayfield,1439744951,"I've read that neural dropout is used by many, including [DeepLearning4J](http://deeplearning4j.org/features.html), [Theano](https://github.com/mdenil/dropout), and [Torch](https://github.com/torch/nn/blob/master/Dropout.lua)

https://www.reddit.com/r/MachineLearning/comments/3cbui2/why_googles_new_patent_applications_are_alarming/

Google's dropout patent application: http://www.google.com/patents/WO2014105866A1?cl=en

It appears that during the long research that led to dropout, many people were working on it or related things and it became a well connected part of the AI industry, and now one big player, Google, wants a patent for what has been public domain for years.

Whats likely to happen? Will it probably be like Google's MapReduce patent where the court only let Google keep the name but software like Hadoop is unaffected? Its not in http://www.google.com/patents/opnpledge by Google's choice.

Or will the many who do dropout find the value of their businesses and opensource products draining into fear of what Google may do to them when they expand?

If dropout is choosing random subsets to learn from, then isnt a group of friends where some show up to random event and others dont, who learn eachothers ways of thinking better by the different combinations, also a kind of dropout?",7,1,False,self,,,,,
242,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,2,3h7xoz,self.MachineLearning,Has any work been done with unrestricted Boltzmann machines using the learning algorithm published by Microsoft Research?,https://www.reddit.com/r/MachineLearning/comments/3h7xoz/has_any_work_been_done_with_unrestricted/,mr_yogurt,1439747620,"There was a Reddit post about it a month ago ([here](https://www.reddit.com/r/MachineLearning/comments/3dtjit/a_new_more_accurate_algorithm_to_train_boltzmann/)), and I haven't seen any news on it since. Has any work been done with unrestricted Boltzmann machines since this learning algorithm was published?",3,7,False,self,,,,,
243,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,5,3h8fnd,self.MachineLearning,How to get accepted into a ML masters after spending 4 years in industry doing nothing related to ML?,https://www.reddit.com/r/MachineLearning/comments/3h8fnd/how_to_get_accepted_into_a_ml_masters_after/,beatlemaniac007,1439756263,"Hi

I posted to cscareerquestions, but it seems this might be a better place to ask.

Basically, I did my undergrad 4 years ago and have been working as a server-side developer at a big company after that. I am interested in going back to school to do a masters in ML. I prefer a research project to taking courses. But I am aware that my math and stats are probably too rusty for me to be able to contribute to research right away without a couple months of ramp up. What would be the best way to approach a professor to take me on as a pupil. Is industry experience at all valued? Also, I don't have a specific thesis in mind but I would be interested in any topic. Will this be seen as a negative or positive by someone doing research? 

My primary motivation is to focus on a long project and delve deeper than what I get to do at work (which is broken into 2 week sprints). Machine learning is something I am interested in in a broad sense, and I took intro courses in NLP, NN, Bayesian learning, etc in undergrad and I enjoyed them. But that was 4 years ago and only now have I become interested in going deeper into it, but I honestly have no preference for any specific topic that I might want to research.

Also, my overall GPA in undergrad wasn't the greatest. I did well in some ML courses and bad in others.

I am in Canada and would like to do my masters in USA or Europe.

Any advice on how to approach this would be greatly appreciated. Thanks.",15,10,False,self,,,,,
244,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,6,3h8s0m,self.MachineLearning,Any experience with Minerva?,https://www.reddit.com/r/MachineLearning/comments/3h8s0m/any_experience_with_minerva/,penguins916,1439762186,"Hello,

Curious if anybody here has any experience with / on [minerva](https://github.com/dmlc/minerva).

In my own tests minerva seems crazy fast and scales well to multigpu. I am concerned, however, about support for it and its future. The best that I got is it started as a research project, a [paper](http://stanford.edu/~rezab/nips2014workshop/submits/minerva.pdf) was published, about a year passed, and now its being developed open source by dmlc but is still very rough around the edges (odd race conditions and the like). Development seems slow (at least compared some of the other ML libraries) but not dead. In addition a bunch of the devs just started [mxnet](https://github.com/dmlc/mxnet).

I am coming from Theano and looking for something that is faster. Specifically something that easily enables high resource utilization (memory copies should happen the same time as execution and so on) and works on multiple gpu. I know all of this is technically possible in Theano, but it starts to get ugly fast.

Any thoughts on the topic would be great! Thanks!",0,10,False,self,,,,,
245,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,9,3h9c5y,self.MachineLearning,What are some notable CS 229 Machine Learning final projects?,https://www.reddit.com/r/MachineLearning/comments/3h9c5y/what_are_some_notable_cs_229_machine_learning/,[deleted],1439772285,	,2,0,False,default,,,,,
246,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,10,3h9jqs,exponential.singularityu.org,Future of Data Science with Jeremy Howard,https://www.reddit.com/r/MachineLearning/comments/3h9jqs/future_of_data_science_with_jeremy_howard/,nath_leigh,1439776377,,0,0,False,http://b.thumbs.redditmedia.com/2fZSer6xKPYwj_6ncQlVTHfSPQSB76gs3Eq9KBgfcFg.jpg,,,,,
247,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,17,3hajkw,profresearchreports.com,Vibration Feeding Machine Industry 2015 Global and Chinese Market Analysis,https://www.reddit.com/r/MachineLearning/comments/3hajkw/vibration_feeding_machine_industry_2015_global/,ProfReport,1439798938,,0,1,False,default,,,,,
248,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,17,3hal43,answers.opencv.org,Image descriptors to train SVM on advice,https://www.reddit.com/r/MachineLearning/comments/3hal43/image_descriptors_to_train_svm_on_advice/,jackbrucesimpson,1439800268,,0,0,False,http://a.thumbs.redditmedia.com/pRYQzbAKZicdhQn5q6HZLoF1Tj6HF0xdzpaSlQr6ig8.jpg,,,,,
249,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,20,3hay7a,self.MachineLearning,I have the feeling Neural Networks are just a nice fitting tool...,https://www.reddit.com/r/MachineLearning/comments/3hay7a/i_have_the_feeling_neural_networks_are_just_a/,eclipseadb,1439810894,"Hello, I have just begun learning about Neural Networks. They seem amazing but the more I learn about them the ""less magical"" they appear.

To me it seems this is no more than just a fitting tool. You give a ""generic model"" with a lot of parameters a lot of data and try to fit the data as best as you can with this generic model. If your model doesn't work you just add more layers or neurons.

For example I have played with a simple character recognition. I followed a book and I get 95% success. It has 50k characters and validates with 10k independent data. Problem is as I said this seems to me no more than a fitting tool. I don't really see the patter recognition. For instance, if I move on purpose my numbers to the right side of the image they start to fail to be recognized. If it really ""learns patterns"" moving the number should not be a problem, should it? However, if it's just a fitting tool it makes sense this behavior.

If I want to predict things for example as far as I know I need also again a lot of data. This seems to me that what it does is to find some coefficients that fit the data so good (after so many different entries) that at the end if I put something ""new"" the parameters chosen in the neurons will still give a plausible result. If I'm right on this I should be able to design other ""generic models"" to fit data which I'm planning on doing eventually.

Adding layers make sense and you normalize to 0-1 so you don't have unexpected results for new data. It makes sense rather than having for example a neuron defined as sum(wi*xi+b). Since in each layer the data will be similar it makes sense to normalize the output of the previous layer to 0-1. However I'm sure this again is not needed and just helps to improve the solution. Again, since this is a fitting tool you can get the same result (or a similar one) either adding neurons or adding layers also. However, it makes more sense in certain situations to add layers instead of adding too many neurons. For instance, to create a number recognition in binary you just add a layer to transform from decimal to binary. You can mathematically do the same by adding much more neurons but you only need 4 neurons to pass from dec-&gt;binary if you use a new layer and there's no way to minimize this number. Even though adding more neurons instead of a new layer will also work since this is a fitting the optimum is to create a new layer since you only need 4 extra neurons.

Again, I'm very new to this topic so I would like to know from more experienced people if this is not in fact what's going on and I would like if you could provide me some examples that challenge this idea.

After all, NN work even if they lack that magical pattern recognition abilities and turn out to only fit data.

Thank you and let's try to have a nice discussion on the matter.",15,6,False,self,,,,,
250,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,20,3hazj8,self.MachineLearning,Supervised and unsupervised Deep Learning algorithms intro material,https://www.reddit.com/r/MachineLearning/comments/3hazj8/supervised_and_unsupervised_deep_learning/,bearerofbearnews,1439811871,"I am applying to a new a job and I am required to familiarize myself with these two fields on the superficial level. 
I noticed the Wikipedia pages on this are quite big, but I am afraid (out of experience) that I can not fully trust the information listed there. 


Can anyone recommend any sort of course or learning material that I can dive in?",7,20,False,self,,,,,
251,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,21,3hb18m,nautil.us,Repost from Futurology,https://www.reddit.com/r/MachineLearning/comments/3hb18m/repost_from_futurology/,epluribusunim,1439812989,,0,1,False,http://b.thumbs.redditmedia.com/L5aneakX_dnIRl-y-GZEaCAd4NQ2MJDGKYZUWqCz4WI.jpg,,,,,
252,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,22,3hbdt5,self.MachineLearning,Neural networks for small datasets?,https://www.reddit.com/r/MachineLearning/comments/3hbdt5/neural_networks_for_small_datasets/,letoseldon,1439819839,"Are there any learning algorithms that have been developed that allow NNs to extract useful information out of small datasets as well as SVMs, for example?",1,1,False,self,,,,,
253,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,23,3hbebf,youtube.com,A neural network tries to identify objects in ST:TNG intro,https://www.reddit.com/r/MachineLearning/comments/3hbebf/a_neural_network_tries_to_identify_objects_in/,greenmana,1439820116,,36,137,False,http://a.thumbs.redditmedia.com/ECd18_UtbYlElsa3Agk_1JixAqOnG6gkPfr75sPAqB0.jpg,,,,,
254,MachineLearning,t5_2r3gv,2015-8-17,2015,8,17,23,3hbkjh,self.MachineLearning,Exact network architecture for MNIST handwritten digits state of the art,https://www.reddit.com/r/MachineLearning/comments/3hbkjh/exact_network_architecture_for_mnist_handwritten/,MLenthusiast,1439823019,"Is there a (frequently updated) website I can look at to tell the exact network architecture and other parameters to get state of the art results on MNIST handwritten digit recognition? In the past, I've tried training a network for MNIST with what I thought is the same architecture as that described in a paper, but my results were much worse than in the paper. 
I'm looking to write my own version of the code, so pointing me at source code isn't ideal, though if it's relatively short it would still be helpful. ",5,3,False,self,,,,,
255,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,0,3hbo3q,conf.startup.ml,"Deep Learning Hands-on Workshops Neon, Keras, OpenDeep &amp; more",https://www.reddit.com/r/MachineLearning/comments/3hbo3q/deep_learning_handson_workshops_neon_keras/,arshakn,1439824629,,0,0,False,default,,,,,
256,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,0,3hbr0o,speakerdeck.com,Large Scale non linear learning in python,https://www.reddit.com/r/MachineLearning/comments/3hbr0o/large_scale_non_linear_learning_in_python/,faming13,1439825916,,0,7,False,http://a.thumbs.redditmedia.com/6hNy4K_Ujcl5QlODoW8Y3VZy91zzo5rdhA2q7tyN8C8.jpg,,,,,
257,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,1,3hbzit,pjreddie.com,Darknet Reference Network: Same accuracy and speed as AlexNet but with 1/10th the parameters.,https://www.reddit.com/r/MachineLearning/comments/3hbzit/darknet_reference_network_same_accuracy_and_speed/,pjreddie,1439829799,,7,9,False,http://b.thumbs.redditmedia.com/u2qSfJJXPM-HptXohNtEZg3VXTpjZ6uTMnUvfM7czDI.jpg,,,,,
258,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,1,3hc0ip,techcrunch.com,Aipoly Puts Machine Vision In The Hands Of The Visually Impaired,https://www.reddit.com/r/MachineLearning/comments/3hc0ip/aipoly_puts_machine_vision_in_the_hands_of_the/,dabshitty,1439830214,,0,1,False,http://b.thumbs.redditmedia.com/6p4msp_VRksBUnbAK4-SLF2RcQkZ-ZXOFBFoMjZXU5Q.jpg,,,,,
259,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,1,3hc1c3,self.MachineLearning,A question on stored weights for NN,https://www.reddit.com/r/MachineLearning/comments/3hc1c3/a_question_on_stored_weights_for_nn/,MarkoRamius73,1439830570,"A question on ""trained"" NNs. 
From what I understand after you train a NN, a set of connections weights are stored and can be used on new data. For example I can train a deep NN to recognize images and then I can use the weights on new images. Is it correct to assume that I need a lot of computing power to train a deep NN but once I have stored the connection weights I can perform classification (or whatever was the original goal) in a reasonably simple way just applying the stored connection weights to the new input vector?
Thanks",2,1,False,self,,,,,
260,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,2,3hc4kf,bloomberg.com,And the New Yorker Cartoon Contest Winner Is  a Computer,https://www.reddit.com/r/MachineLearning/comments/3hc4kf/and_the_new_yorker_cartoon_contest_winner_is_a/,Zulban,1439831930,,0,8,False,http://b.thumbs.redditmedia.com/IhCwtAQsNWt5zLXuWsyEI7RjgwVDz9QjmaVfjBnj6gU.jpg,,,,,
261,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,3,3hcijy,self.MachineLearning,Is it really appropriate to use a gradient-based method in NN learning? Why doesn't the learning process gets stuck?,https://www.reddit.com/r/MachineLearning/comments/3hcijy/is_it_really_appropriate_to_use_a_gradientbased/,eclipseadb,1439837586,"Hello, there's a problem that's been bugging my mind because it not only affects Neural Networks but some of my work (which is a bit unrelated) and maybe there's a simple proof I'm missing.

The problem in mind is that when we want a NN to learn we use an optimization method to modify the parameters of the network to minimize the error between the wanted output and the output the network gives. The design of the error function as well as other things can change but that's the main idea.

As far as I know (I may be wrong here) then we use a gradient-based method to optimize the machine parameters (that is to learn). I say as far as I know because that's what they use in the book I'm reading and from what I know from optimization theory I don't see any other appropriate method (at least one that is also fast enough). The problem is that a gradient-based method does not guarantee to find a global minimum. What this means is that the perfect network may have 99% success for example but because we optimize the parameters with a gradient based method we may finish with a network with 20% success. The reason is that the method may find a local minimum at 20% success and will not continue. That's how a gradient-based method works.

However, it seems that the NN always have good solutions or at least the book hasn't mentioned a problem (yet). My intuition tells me that as the number of parameter grows the likelihood of having a lot of local minimas is reduced. However it's hard to prove and I would like to know if there's a simple explanation of why using a gradient-based method seems to work. Intuitively we can think that with SO many parameters it's very hard to find a set of parameters from which computing the error gradient yields a value close to zero for ALL other than the actual (and maybe unique) minimum. Is it as simple as this? Any reference I could cite?",12,5,False,self,,,,,
262,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,4,3hckeo,cloudacademy.com,In-depth intro to the principles and practice of Amazon Machine Learning (Course),https://www.reddit.com/r/MachineLearning/comments/3hckeo/indepth_intro_to_the_principles_and_practice_of/,leonardofed,1439838313,,0,1,False,http://b.thumbs.redditmedia.com/ABJxlhxI5NghdeENlri3v0zxSoHNjCV_ueWY1XqVfqs.jpg,,,,,
263,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,4,3hcpmy,insights.ubuntu.com,Meet Mycroft: Open Source Artificial Intelligence Powered by Snappy,https://www.reddit.com/r/MachineLearning/comments/3hcpmy/meet_mycroft_open_source_artificial_intelligence/,veritanuda,1439840434,,1,1,False,default,,,,,
264,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,4,3hcrev,recommend-papers.org,Recommend papers for Deep Learning Symposium @ NIPS'2015,https://www.reddit.com/r/MachineLearning/comments/3hcrev/recommend_papers_for_deep_learning_symposium/,[deleted],1439841185,,0,1,False,default,,,,,
265,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,4,3hcrnw,recommend-papers.org,Recommend papers for Deep Learning Symposium @ NIPS'2015,https://www.reddit.com/r/MachineLearning/comments/3hcrnw/recommend_papers_for_deep_learning_symposium/,downtownslim,1439841283,,0,12,False,default,,,,,
266,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,7,3hdczm,youtube.com,Clockwork Orange sorted by place (using CNNs),https://www.reddit.com/r/MachineLearning/comments/3hdczm/clockwork_orange_sorted_by_place_using_cnns/,icandoitbetter,1439850120,,2,3,False,http://b.thumbs.redditmedia.com/0JI2_-pfxHhRmanFuaAZRDGC7xP7cMrRVhFOlR4bRJg.jpg,,,,,
267,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,7,3hdgx8,self.MachineLearning,Has anyone had experience with IBM's SyNAPSE chips?,https://www.reddit.com/r/MachineLearning/comments/3hdgx8/has_anyone_had_experience_with_ibms_synapse_chips/,mostly_complaints,1439851799,"IBM's highly parallel chips seem really cool and they apparently offer limited training and developer resources. Has anyone had experience with this? Is it basically limited to researchers in the field who are planning to work on this full time?

http://research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml#fbid=ZD7QQrUbZ6I",1,2,False,self,,,,,
268,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,8,3hdj8z,self.MachineLearning,Anyone know any good papers on combining compressed sensing with classification algorithms (e.g. deep learning) for image classification (or other applications)?,https://www.reddit.com/r/MachineLearning/comments/3hdj8z/anyone_know_any_good_papers_on_combining/,pappypapaya,1439852814,,2,10,False,self,,,,,
269,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,11,3he6ju,machinelearningtalks.com,"Tagged and Searchable. Distinguished AI Panel (Eric Horvitz, Peter Lee, Jaime Carbonell) answers questions on big data",https://www.reddit.com/r/MachineLearning/comments/3he6ju/tagged_and_searchable_distinguished_ai_panel_eric/,ojaved,1439863567,,0,1,False,default,,,,,
270,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,17,3hf6k7,blog.thehackerati.com,Eigenstyle - Principal Component Analysis and Fashion,https://www.reddit.com/r/MachineLearning/comments/3hf6k7/eigenstyle_principal_component_analysis_and/,alexcasalboni,1439884849,,17,91,False,http://b.thumbs.redditmedia.com/DbRWX4NsVYBUVjqXVJb4lye8HKb96eoYifzTywSr-uY.jpg,,,,,
271,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,17,3hf6o1,profresearchreports.com,Induction Sealing Machine Industry 2015 Global and Chinese Market Analysis,https://www.reddit.com/r/MachineLearning/comments/3hf6o1/induction_sealing_machine_industry_2015_global/,profresearchreports,1439884931,,0,1,False,default,,,,,
272,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,19,3hfhb3,bizdirect.ca,"Circular Saw Machine Market 2015, Global Landscape And Growth Prospect Industry Research Report",https://www.reddit.com/r/MachineLearning/comments/3hfhb3/circular_saw_machine_market_2015_global_landscape/,profresearchreports,1439893119,,0,1,False,default,,,,,
273,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,19,3hfi5b,andrehaynes.me,[X-POST /r/bitcoin] [PDF] Determining manipulation via 'sock puppet' accounts on the block size debate (on reddit) using machine learning,https://www.reddit.com/r/MachineLearning/comments/3hfi5b/xpost_rbitcoin_pdf_determining_manipulation_via/,muyuu,1439893739,,4,3,False,default,,,,,
274,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,19,3hfjxw,self.MachineLearning,X-ray manufacturer India,https://www.reddit.com/r/MachineLearning/comments/3hfjxw/xray_manufacturer_india/,paulrobinson00,1439895202,,0,1,False,default,,,,,
275,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,20,3hfmth,stackoverflow.com,How to Train scikit-neuralnetwork on images?,https://www.reddit.com/r/MachineLearning/comments/3hfmth/how_to_train_scikitneuralnetwork_on_images/,jackbrucesimpson,1439897172,,4,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
276,MachineLearning,t5_2r3gv,2015-8-18,2015,8,18,22,3hfwjs,stats.stackexchange.com,Guidance with developing a CV Recommender system,https://www.reddit.com/r/MachineLearning/comments/3hfwjs/guidance_with_developing_a_cv_recommender_system/,raduqq,1439902858,,3,0,False,http://b.thumbs.redditmedia.com/ufPPkLMUpF_M7N2GgFJrmAymFBquI81EYH5dG9lmNXg.jpg,,,,,
277,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,0,3hgi3d,stackoverflow.com,Can someone explain what the things in the layers of a neural network like 'Rectifier' and 'Softmax' do?,https://www.reddit.com/r/MachineLearning/comments/3hgi3d/can_someone_explain_what_the_things_in_the_layers/,jackbrucesimpson,1439912457,,3,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
278,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,4,3hhh2u,galvanize.com,Using Pitch f/x and machine learning to analyze baseball pitching strategy,https://www.reddit.com/r/MachineLearning/comments/3hhh2u/using_pitch_fx_and_machine_learning_to_analyze/,marcoborracho,1439926021,,0,0,False,http://b.thumbs.redditmedia.com/L3QDyah5e03oY68QE3m-2zQ0HtIiwPAxNxrDELWJesk.jpg,,,,,
279,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,4,3hhhzc,nuit-blanche.blogspot.com,Manitest: Are classifiers really invariant?,https://www.reddit.com/r/MachineLearning/comments/3hhhzc/manitest_are_classifiers_really_invariant/,compsens,1439926389,,0,5,False,http://b.thumbs.redditmedia.com/6-wBnfLwXkiPKgrB43FXU3cwlmkRDIIE_oN6FQQ9RzM.jpg,,,,,
280,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,4,3hhkrg,youtu.be,Making Robots,https://www.reddit.com/r/MachineLearning/comments/3hhkrg/making_robots/,mszlazak,1439927459,,0,15,False,http://b.thumbs.redditmedia.com/RMxN0mwtZNvZ4yCS9ZWtsIrwHAsJpoWmWoZK2RMIKvg.jpg,,,,,
281,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,5,3hhpws,gitxiv.com,Deep Convolutional Networks on Graph-Structured Data. (LeCun et.al 6.2015 / code+paper),https://www.reddit.com/r/MachineLearning/comments/3hhpws/deep_convolutional_networks_on_graphstructured/,samim23,1439929474,,0,36,False,default,,,,,
282,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,6,3hi1ux,wired.com,IBMs neuromorphic chips for deep learning,https://www.reddit.com/r/MachineLearning/comments/3hi1ux/ibms_neuromorphic_chips_for_deep_learning/,[deleted],1439934111,,0,0,False,default,,,,,
283,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,8,3higdo,deeplearning4j.org,A Beginner's Guide to Eigenvectors and Entropy,https://www.reddit.com/r/MachineLearning/comments/3higdo/a_beginners_guide_to_eigenvectors_and_entropy/,vonnik,1439940199,,6,72,False,default,,,,,
284,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,8,3hijeh,sites.google.com,Slides from Deep Learning Summer School 2015,https://www.reddit.com/r/MachineLearning/comments/3hijeh/slides_from_deep_learning_summer_school_2015/,evc123,1439941481,,5,6,False,default,,,,,
285,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,10,3hivaj,github.com,Generalized Low Rank Models for Exploratory Analysis in python,https://www.reddit.com/r/MachineLearning/comments/3hivaj/generalized_low_rank_models_for_exploratory/,faming13,1439946794,,0,12,False,http://b.thumbs.redditmedia.com/QBrxI3_4P9ytV8YL3ihvz9XRXFQ6i76iq6NhqBN5DxE.jpg,,,,,
286,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,10,3hizci,self.MachineLearning,What is the best way to determine how many memory-cells one needs in an lstm?,https://www.reddit.com/r/MachineLearning/comments/3hizci/what_is_the_best_way_to_determine_how_many/,[deleted],1439948585,,1,1,False,default,,,,,
287,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,11,3hj5w8,self.MachineLearning,"[question regarding FAQ] ""how much math/stats should I know?""",https://www.reddit.com/r/MachineLearning/comments/3hj5w8/question_regarding_faq_how_much_mathstats_should/,dudedudeman1,1439951522,"I'm nearing the end of my undergraduate math degree. With my course selections I've been trying to prepare myself as possible for studying machine learning on my own, maybe grad school eventually (for all my electives I've taken math/stat/programming courses.) I might take an extra term or two to take some extra courses to this end.

I've taken real analysis 1 and 2, wondering if I should take real analysis 3 and 4? I'm also wondering if learning abstract algebra would help me? My school offers two hefty proof based abstract algebra courses that don't seem like a whole lot of  fun to me; but I'll take em if they'll be useful. I'm also planning to take proof based courses in probability and stochastic processes. Anything else I should add to the list? ",9,2,False,self,,,,,
288,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,13,3hji7p,bbc.com,Google's Atlas robot takes a forest hike,https://www.reddit.com/r/MachineLearning/comments/3hji7p/googles_atlas_robot_takes_a_forest_hike/,john_philip,1439957396,,1,10,False,http://b.thumbs.redditmedia.com/den4w2FhchjjuAGfTXgbqT56e4o3DkPbaP8zY6xG5YA.jpg,,,,,
289,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,13,3hjig3,anthonywittemann.github.io,RNNs that talk like Snooki - Training Karpathy's char-rnn on TV scripts,https://www.reddit.com/r/MachineLearning/comments/3hjig3/rnns_that_talk_like_snooki_training_karpathys/,the1witt,1439957504,,0,0,False,http://b.thumbs.redditmedia.com/Z8cpXWuMyut3uhyIBQ-nw6C6SAaDxmPm0EivOcC_uHQ.jpg,,,,,
290,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,13,3hjkyu,reddit.com,[x-post /r/tinycode] they just added flair for machininelearning if you've got some tiny implementations,https://www.reddit.com/r/MachineLearning/comments/3hjkyu/xpost_rtinycode_they_just_added_flair_for/,[deleted],1439958821,,0,1,False,default,,,,,
291,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,15,3hjurh,self.MachineLearning,Can I call Theano with C++ instead of Python?,https://www.reddit.com/r/MachineLearning/comments/3hjurh/can_i_call_theano_with_c_instead_of_python/,jackbrucesimpson,1439964488,,5,0,False,self,,,,,
292,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,16,3hk2bc,self.MachineLearning,Has anyone studied the effect of modifications to the training corpus on language models or on word embeddings?,https://www.reddit.com/r/MachineLearning/comments/3hk2bc/has_anyone_studied_the_effect_of_modifications_to/,benjaminwilson,1439969589,"e.g. removing occurrences of a word to change its frequency?

I am working on this myself, but would be very interested to read what other people have done.",3,5,False,self,,,,,
293,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,17,3hk8rp,nuit-blanche.blogspot.com,A Deep Learning Approach to Structured Signal Recovery (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/3hk8rp/a_deep_learning_approach_to_structured_signal/,compsens,1439974594,,1,1,False,http://b.thumbs.redditmedia.com/bS2b3DMHP2FO1cX61n-c21QXA1HSsGB9BRAMJWEAPQg.jpg,,,,,
294,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,18,3hkdwh,self.MachineLearning,X-ray manufacturer India,https://www.reddit.com/r/MachineLearning/comments/3hkdwh/xray_manufacturer_india/,paulrobinson00,1439978336,,0,1,False,default,,,,,
295,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,21,3hkv2b,self.MachineLearning,Most recent way to install Theano for Windows (10)? Which Visual Studio versions work?,https://www.reddit.com/r/MachineLearning/comments/3hkv2b/most_recent_way_to_install_theano_for_windows_10/,qwertz_guy,1439989169,"New PC, new Windows. Trying to install Theano. I installed Visual Studio 14 Community version, then CUDA 7.5. Now this page http://deeplearning.net/software/theano/install_windows.html is telling me that I need Visual Studio 2010 together with Microsoft Windows SDK for 32bit + 64bit (the linked SDK is for Windows 7...). The installation doc is referring to pretty old versions, is this still the only way to go or have things become simpler? I am pretty unexperienced with development on Windows and would be happy if someone could enlighten me.
--------------

Update, here is how I did it (my thanks to /u/disentangle ):

1. Install Anaconda
2. conda install pip six nose numpy scipy - already ships with Anaconda I think
3. conda install mingw libpython
4. pip install theano
5. Install Visual Studio 2013 Community Edition (VS 2015 DOES NOT WORK WITH CUDA 7.5 AT THIS TIME)
6. Install CUDA Toolkit 7.5
7. Add a ""THEANO_FLAGS"" environment variable with value ""floatX=float32,device=gpu,nvcc.fastmath=True""
8. Add path to VS's C++ compiler (cl.exe) to your PATH environment variable, e.g. C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\bin

",7,7,False,self,,,,,
296,MachineLearning,t5_2r3gv,2015-8-19,2015,8,19,23,3hl3ea,github.com,Semi-supervised Local Fisher Discriminant Analysis on CRAN now!,https://www.reddit.com/r/MachineLearning/comments/3hl3ea/semisupervised_local_fisher_discriminant_analysis/,terrytangyuan,1439993221,,1,0,False,http://b.thumbs.redditmedia.com/f43KlKdP2GJEw5xdQUrhXiPAnplqbNOyzOaJYo9vzBA.jpg,,,,,
297,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,1,3hlm1x,recommend-papers.org,Call for suggested papers to Deep Learning Symposium @ NIPS'2015!,https://www.reddit.com/r/MachineLearning/comments/3hlm1x/call_for_suggested_papers_to_deep_learning/,WnpgyYn,1440000933,,0,2,False,default,,,,,
298,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,3,3hm4tv,self.MachineLearning,"Intro to Machine Learning, Intro to the ROC curve",https://www.reddit.com/r/MachineLearning/comments/3hm4tv/intro_to_machine_learning_intro_to_the_roc_curve/,chestervonwinchester,1440008256,"Hello,

I wrote a couple of blog posts, which I thought I would share. They are aimed at anyone with a solid background in probability and statistics. Let me know what you guys think - any feedback is appreciated :)  Keep in mind though that these were written more as expository notes, so you might have to break out pen and paper. Although, I did try my best to keep it light and clean.

* The first is entitled [What is Machine Learning?!](http://notmatthancock.github.io/2015/07/24/what-is-machine-learning.html) and is a **probabilistic** introduction to binary classification with logistic regression introduced as an example model. I made an [accompanying post](http://notmatthancock.github.io/2015/08/02/machine-learning-part-2-numerical-example.html) which is a numerical example and implementation of building a logistic regression model with Python from scratch.
* The second is entitled [What is the ROC curve?!](http://notmatthancock.github.io/2015/08/18/what-is-the-roc-curve.html), and defines sensitivity, specificity, the ROC curve, and the AUC score as theoretical probabilistic quantities which are estimated using a finite sample. Again, [I made a companion post to illustrate the concept numerically](http://notmatthancock.github.io/2015/08/19/roc-curve-part-2-numerical-example.html) with Python by calculating the ROC curve and AUC score from the previous logistic regression model.",12,36,False,self,,,,,
299,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,3,3hm88k,self.MachineLearning,Can you use deep learning in video game data?,https://www.reddit.com/r/MachineLearning/comments/3hm88k/can_you_use_deep_learning_in_video_game_data/,AJ1010,1440009575,"Hey all! I'm looking to self learn deep learning by doing a project that's about something interesting to me, which is video games. However I am very inexperience and kind of stuck, so am hoping that someone more experienced here could give me an idea to start my project off of. I read that deep learning is most effective for image/speech recognition/NLP, however, is there still potential for it to be used on match history data from a video game? Specifically League of Legends? What are something interesting that people can look for from game match history using deep learning?

Sorry if this is the wrong place to post, I am very inexperienced but am excited to learn and practice!",11,0,False,self,,,,,
300,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,6,3hmxno,github.com,wer are we: accuracy of current speech recognition systems,https://www.reddit.com/r/MachineLearning/comments/3hmxno/wer_are_we_accuracy_of_current_speech_recognition/,r-sync,1440019384,,13,28,False,http://b.thumbs.redditmedia.com/NC1jZ4BKb6ENt8bMVCQEPIQISqof8DidA4zsjUQEOwk.jpg,,,,,
301,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,6,3hmyo4,thespermwhale.com,"Reasoning, Attention, Memory (RAM) NIPS Workshop 2015",https://www.reddit.com/r/MachineLearning/comments/3hmyo4/reasoning_attention_memory_ram_nips_workshop_2015/,iidealized,1440019805,,4,11,False,default,,,,,
302,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,7,3hn9s8,twitter.com,ICLR deadline is postponed to Nov. 12th,https://www.reddit.com/r/MachineLearning/comments/3hn9s8/iclr_deadline_is_postponed_to_nov_12th/,pierrelux,1440024356,,1,5,False,http://a.thumbs.redditmedia.com/VRfrzBXwYv8cEviCvMDbzoLXnAvs86EafupI-J_aMj4.jpg,,,,,
303,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,8,3hnfec,nerds.airbnb.com,Talk: Airbnb Price Tips Powered By Aerosolve Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3hnfec/talk_airbnb_price_tips_powered_by_aerosolve/,shad0w0bserver,1440026708,,1,21,False,http://b.thumbs.redditmedia.com/b-yCwzFeSjwnCIkzY36ztI6h36KgGXeXRaCrqYsOlLw.jpg,,,,,
304,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,11,3ho39x,github.com,"Minimal, Self-Contained GRU implementation",https://www.reddit.com/r/MachineLearning/comments/3ho39x/minimal_selfcontained_gru_implementation/,doctorteeth2,1440037842,,5,10,False,http://b.thumbs.redditmedia.com/BzupuKwSoTByyX9PtUzYPLHXCKHV_P52Oq1zDIsyiiY.jpg,,,,,
305,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,13,3hoi5h,self.MachineLearning,Move along any shortest path toward goal,https://www.reddit.com/r/MachineLearning/comments/3hoi5h/move_along_any_shortest_path_toward_goal/,BenRayfield,1440045634,"AI would be really easy if we could directly run a shortest path algorithm on the space of all possibilities in which the goal is somewhere.

But as goal becomes more complex it must be represented as a function, normally one that gives a higher number the better the AI is doing, based on looking at the world goes into the function.

So now we have to do shortest path from where we start to the closest of the high scoring outputs of that function.

Getting even more complex, the world can only be represented by functions such as describing how things bounce on eachother or how words change eachothers meanings, so its a shortest path search through a ""house of mirrors"" of functions that apply to functions.

But its still shortest path, even if the space navigated is hard to imagine. Move along any shortest path toward goal.",3,0,False,self,,,,,
306,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,17,3hp3up,youtu.be,Jeff Hawkins and The Future of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/3hp3up/jeff_hawkins_and_the_future_of_artificial/,[deleted],1440060658,,0,0,False,default,,,,,
307,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,17,3hp3y1,self.MachineLearning,What is/are the best tools for feature extraction?,https://www.reddit.com/r/MachineLearning/comments/3hp3y1/what_isare_the_best_tools_for_feature_extraction/,raduqq,1440060731,"I'm trying to find a best practice implementation of algorithms or libraries that give good results for feature extraction from text.

I've read a few articles which say that generally a count of words is a ""ok"" feature for text, but does not perform ""miracles"". Also, the same thing goes for  ""POS tagging"".

Any hints on what other features can be extracted from text apart from word count &amp; ""POS tagging"" ?",17,10,False,self,,,,,
308,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,18,3hp8ne,medium.com,How people use smartphones today - survey of the problems people face,https://www.reddit.com/r/MachineLearning/comments/3hp8ne/how_people_use_smartphones_today_survey_of_the/,dhruvghulati,1440064423,,0,2,False,default,,,,,
309,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,20,3hpf7r,youtube.com,An end-to-end Machine Learning ecosystem in Python at ZenDesk,https://www.reddit.com/r/MachineLearning/comments/3hpf7r/an_endtoend_machine_learning_ecosystem_in_python/,kunjaan,1440069200,,4,19,False,http://a.thumbs.redditmedia.com/35oId0q4KWIhR6u-hcFdQlLIjnyFBdD1hOwnToP0eh0.jpg,,,,,
310,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,20,3hphop,self.MachineLearning,Did anyone train an NN with ImageNet as input and word vector representations of the labels as output?,https://www.reddit.com/r/MachineLearning/comments/3hphop/did_anyone_train_an_nn_with_imagenet_as_input_and/,svantana,1440070975,"It just occurred to me that this would be a reasonable way of detecting objects that are not in the test set: just find words with vectors similar to the NN output.

I did a quick google scholar search but nothing came up. The image captioning systems (of Google, Karpathy, etc) work along similar lines I guess, but it would be interesting to see how this fares on object detection alone.",6,2,False,self,,,,,
311,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,20,3hphwx,guanhaopacks.blogspot.in,Filling Machine to Provide Speedy Filling with High Accuracy,https://www.reddit.com/r/MachineLearning/comments/3hphwx/filling_machine_to_provide_speedy_filling_with/,guanhaopackseo,1440071149,,0,1,False,default,,,,,
312,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,20,3hpiat,self.MachineLearning,Feedback loop between data collection and evaluation?,https://www.reddit.com/r/MachineLearning/comments/3hpiat/feedback_loop_between_data_collection_and/,ransil,1440071417,Many machine learning methods assume that the data is collected randomly. But this may not be the case for real-life data. An example would be Google Ads. Which ads are being displayed is determined by some machine learning algorithm. And therefore the choice of the algorithm influences what data is collected. This makes the evaluation of models on such data hard. Do you have any relevant links to literature concerning this topic?,1,1,False,self,,,,,
313,MachineLearning,t5_2r3gv,2015-8-20,2015,8,20,22,3hprpa,ebaytechblog.com,Statistical Anomaly Detection,https://www.reddit.com/r/MachineLearning/comments/3hprpa/statistical_anomaly_detection/,[deleted],1440076567,,0,0,False,default,,,,,
314,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,0,3hq5kz,github.com,List of Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/3hq5kz/list_of_recommender_systems/,Grahar64,1440082995,,2,1,False,http://a.thumbs.redditmedia.com/5rBFRlCLnN6ZnQV-tIRT8WYRlti7Ohdp_Czc9wwi9v4.jpg,,,,,
315,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,0,3hq7yc,self.MachineLearning,Beginner projects,https://www.reddit.com/r/MachineLearning/comments/3hq7yc/beginner_projects/,DONT_GOOGLE_ME,1440083935,"I want to learn more about machine learning but I'm finding it hard to find/think of a project that is beginner friendly        

My background;  
Currently studying Maths and Stats via distance learning  
Currently working as a mobile/web developer  
Have a Diploma in Computer Science (Similar to an associates degree in the US)",8,7,False,self,,,,,
316,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,0,3hq9ag,maori.geek.nz,3 Optimisations that may Improve Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/3hq9ag/3_optimisations_that_may_improve_recommender/,Grahar64,1440084509,,0,2,False,http://b.thumbs.redditmedia.com/6-AxsABCC9hzHltAySVdwk-4RNk5n1Bvg6_mV0FQR5k.jpg,,,,,
317,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,0,3hqbhy,self.MachineLearning,"Can RandomForests, GBM do what CNN + Deep learning have achieved?",https://www.reddit.com/r/MachineLearning/comments/3hqbhy/can_randomforests_gbm_do_what_cnn_deep_learning/,t_minus_1,1440085446,"By looking at the CNN and Deep Learning - It looked like the key insight was applying convolution on the image to generate features which are invariant to rotation and shift. I was wondering if Deep Learning really had any magic - or just any other Supervised Learning such as random forest, GBM can achieve the same performance and accuracy as Deep Learning / CNN - if they were given exact same features. ",12,7,False,self,,,,,
318,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,2,3hquq6,self.MachineLearning,Research in high-dimensional statistics vs. machine learning?,https://www.reddit.com/r/MachineLearning/comments/3hquq6/research_in_highdimensional_statistics_vs_machine/,zyhtsh,1440093237,"As a PhD student starting to think about dissertation topics, I am particularly interested in high-dimensional statistical learning. I wish to find some research review/survey/papers (or webpages, blogs, whatever...) about the state-of-the-art research in this research area, but there seems limited resources I can obtain. My first question is then,

* Could you describe some current interesting research topics in high-dimensional statistics? If you can list any relevant resources (papers, webpages, etc.), that would be really helpful.

In addition, I've noticed that high-dimensional statistical learning is closed related with machine learning research. For example, the idea of penalized regularization in high-dimensional statistics was used in machine learning domain, like support vector machine, boosting tree, (sparse) additive models, etc. My question is,

* what are good research papers about the interplay of high-dimensional statistics and machine learning?

Last, since high-dimensional statistics was really motivated by genetic research (like gene-expression analysis, or genome-wide association studies), most of applications in high-dimensional research are devoted in that area.

* Are there any successful applications of high-dimensional statistics in areas other than genetics, particularly say, image/text mining, recommendation, etc, areas where machine learning techniques have long been used?

* I might be wrong, but as I understand most machine learning algorithms are designed for low-dimensional problems (or at least the number of features is smaller that number of observations). Are there any successful applications of machine learning techniques for modeling high-dimensional data?

Any resources/comments are highly appreciated. Thanks.
",6,4,False,self,,,,,
319,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,3,3hqyrn,computervisiontalks.com,"""The Game Engine in Your Head !"" super interesting talk by Josh Tenenbaum - Fully Tagged",https://www.reddit.com/r/MachineLearning/comments/3hqyrn/the_game_engine_in_your_head_super_interesting/,ojaved,1440094831,,0,1,False,default,,,,,
320,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,8,3hs8ci,self.MachineLearning,User for Great Neural Network Library?,https://www.reddit.com/r/MachineLearning/comments/3hs8ci/user_for_great_neural_network_library/,BrainyGuy,1440114239,"I have invented a really powerful neural network training algorithm. The network can be trained in real time, requiring only about one millisecond per observation. Using the resulting neural network for prediction is extremely fast. This is with stock server hardware. The results are robust even if the data is noisy or it experiences concept drift. There is only one tunable parameter and its exact value is not critical. Even relatively non-technical users can get good results with the algorithm.

The original version of the code is in Haskell. I have been working on JavaScript and C++ versions. I believe it is patentable technology, though that is not a personal goal of mine. It should go without saying that it is the result of many years of experience with neural networks.

I do not have the financial means to launch a company to take advantage of this algorithm. I would love to see it put to good use. Even better, I would love to get paid to work on it. As a last resort, I plan on releasing the code as open source projects (GNU Affero GPL license). But I also need to put food on the table. I would love for somebody to step forward and help me turn it into a commercial product or to turn it into a new feature on an existing software product.

Any takers? Any ideas? I can be reached via creyes123 at yahoo dot com.

UPDATE 1: I was trying to be factual in the description of the code without getting too technical and without giving away too many details. What I was trying to say may have been open to misinterpretation as a result. I'm a practitioner (coder) with many years of experience. Writing research papers is just not something that I do. i.e., no interest and no time.

The design goal of the code was to provide good results quickly via a very user friendly API. The robustness and concept drift features were added to make the code easier to use, if that makes sense. Researchers tend to care about 0.0001% better accuracy at the cost of days of runtime on a GPU. I did not set out to compete with them in any way.

I do not work in a lab. I work in the real world. I wrote the code for a specific commercial application. But I can see uses in many other settings. I have not come across any open source libraries that are even close to what I wrote. They tend to be toolkits for machine learning experts.

Several have suggested testing the code against MNIST. Looking into it. I already know the results would be good but not great. I know I cannot compete with an expert using an existing toolkit. But I'll bet my code could easily beat a non-expert using another toolkit. Hope that clears things up.",27,0,False,self,,,,,
321,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,10,3hslvl,github.com,I'm creating an example Python Machine Learning notebook for newcomers to the field. I'd love your feedback or contributions to make it better.,https://www.reddit.com/r/MachineLearning/comments/3hslvl/im_creating_an_example_python_machine_learning/,rhiever,1440120920,,15,179,False,http://b.thumbs.redditmedia.com/933AkAuK1GO_z4B307htii59Bcwo95Revncf068Pg9c.jpg,,,,,
322,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,11,3hspyl,self.MachineLearning,On the subject of dimensionality reduction.,https://www.reddit.com/r/MachineLearning/comments/3hspyl/on_the_subject_of_dimensionality_reduction/,[deleted],1440122927,"What s currently regarded as the best way to do dimensiontional reduction? Especially on time series. Assume data is not sparse and contains 100+ dimensions and millions of data points.



From my experience of doing it years ago, it's difficult to know which dimensions are more valuable than others so generally the data points which accent a certain objective are used to either initialize the matrix/codebook or are present in much greater number than other data points.



There used to also be a problem with time series, we used to create multiple clustering operations with t=1,2,3... where t is how many series are used. Then have a neural net learn from all of them.



Is Kohonen still the best approach? Albeit massively computationally intensive.",4,1,False,default,,,,,
323,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,11,3hsqbz,self.MachineLearning,How long does it take to get proficient?,https://www.reddit.com/r/MachineLearning/comments/3hsqbz/how_long_does_it_take_to_get_proficient/,charlesbukowksi,1440123101,"Machine Learning and Deep Learning seem like the special sauce in lots of startups these days, from Quora to Uber.

I know this is a subjective question and dependant on multifarious factors, but for someone who has a good grasp on software engineering, how long would it take to get to the point where you could incorporate ideas from ML into your webapps and side projects?

I'm not looking to study graduate level statistics, just to the point where I can build prototypes with these concepts.  Or maybe all the hype is overblown and what makes these startups successful largely isn't fancy algorithms?
",14,1,False,self,,,,,
324,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,11,3hssm9,devblogs.nvidia.com,Wyoming Evolving AI Lab on Visualizing--and Fooling--Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3hssm9/wyoming_evolving_ai_lab_on_visualizingand/,harrism,1440124168,,0,3,False,http://b.thumbs.redditmedia.com/frkPSB9BL0Yt2LgZUEIcvQff8xPIWtlVl3m9g3j5lns.jpg,,,,,
325,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,17,3htp76,self.MachineLearning,Good HMM and GMM library in python for ASR,https://www.reddit.com/r/MachineLearning/comments/3htp76/good_hmm_and_gmm_library_in_python_for_asr/,cicilia123,1440144282,"I'm trying to build an automatic speech recognition program for hindi language. I have already built the feature extraction part. Also, I have language data and language model. Both of these are created in python. Where can I find a good library (with some usage tutorial) to create the acoustic model and the hidden markov models.",6,6,False,self,,,,,
326,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,17,3hts9q,self.MachineLearning,Choosing the right model to predict future values of time-series data,https://www.reddit.com/r/MachineLearning/comments/3hts9q/choosing_the_right_model_to_predict_future_values/,davrb27,1440146921,"I have some time-series data that has one data value per day. This data runs from 01-01-2014 to today. Roughly 600 data points. I would like a way to forecast my data points forward roughly 3 months with a value for each day to get an idea of what values I will have in 3 months.

The data is slightly non-linear with an initial large increase in values day to day tapering off to a very slow increase. Im quite a machine learning/predictive modelling noob so struggling to approach this the right way. All help appreciated.",4,6,False,self,,,,,
327,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,19,3hu1jj,self.MachineLearning,C-arm manufacturer India,https://www.reddit.com/r/MachineLearning/comments/3hu1jj/carm_manufacturer_india/,paulrobinson00,1440154093,,0,1,False,default,,,,,
328,MachineLearning,t5_2r3gv,2015-8-21,2015,8,21,23,3huouh,self.MachineLearning,How does the SHOGUN Toolbox convolutional neural network compare to Caffe and Theano?,https://www.reddit.com/r/MachineLearning/comments/3huouh/how_does_the_shogun_toolbox_convolutional_neural/,jackbrucesimpson,1440167152,"I'm interested in implementing a convolutional neural network in my C++ program where I'm tracking tagged insects (I'm also using OpenCV). I see people mention Caffe, Torch and Theano a lot but I haven't heard the CNN in the SHOGUN Toolbox discussed. Does this CNN work well and would anyone recommend it if you're working in C++? I've used Theano via scikit-neuralnetwork in Python to test out some images and that worked really well, except unfortunately Theano is Python-only.",4,3,False,self,,,,,
329,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,1,3hv69m,dropbox.com,"Slides: ""Reinforcement Learning for Artificial Intelligence"" (Fall 2015) by Richard Sutton",https://www.reddit.com/r/MachineLearning/comments/3hv69m/slides_reinforcement_learning_for_artificial/,pierrelux,1440174599,,4,20,False,http://a.thumbs.redditmedia.com/fAgJzE-U-jlgvXfbRRNEE-YDOauOIyUw0KgRNnq7b14.jpg,,,,,
330,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,1,3hvaa1,self.MachineLearning,"Which of the NIPS 2014 papers are most significant/useful, and why?",https://www.reddit.com/r/MachineLearning/comments/3hvaa1/which_of_the_nips_2014_papers_are_most/,Trepeneur,1440176307,"The NIPS (Advances in Neural Information Processing Systems) 27 papers are interesting to read, but as a newcomer to the field it is hard to decide which ones represent significant improvement on the state of the art: http://papers.nips.cc/book/advances-in-neural-information-processing-systems-27-2014

Which would be your picks, and why?",17,15,False,self,,,,,
331,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,2,3hvaxi,youtu.be,"Given a wireframe, machine writes app code by itself",https://www.reddit.com/r/MachineLearning/comments/3hvaxi/given_a_wireframe_machine_writes_app_code_by/,Wirecog,1440176564,,0,0,False,http://b.thumbs.redditmedia.com/t_nEP-OnZz1_Z9YbH9TtUTUQbVgrYeBgJt276Ala1mc.jpg,,,,,
332,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,2,3hvgc2,self.MachineLearning,How to deal with categorical factors in Python?,https://www.reddit.com/r/MachineLearning/comments/3hvgc2/how_to_deal_with_categorical_factors_in_python/,Chopsting,1440178793,"Like the headline,

I'm trying to learn Python or rather how to work in Python (Coming from R), and I'm curious how to deal with categorical factors in pandas





Id|Category| Something
--|---|---
1|Tech | 231
2|Business | 435
3|News | 342

turned into



Id| Tech|Business|News|Something
---|----|----|----|----
1 | 1 | 0 | 0 | 231
2 | 0 | 1 | 0 | 435
3 | 0 | 0 | 1 | 342



I've pretty much prepared my entire dataframe in pandas being ready for my sklearn algo to run.


But I cant figure out how to turn categorial data into 0/1 binary columns in Python?
Thank you",8,1,False,self,,,,,
333,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,2,3hvhjv,self.MachineLearning,Kaggle digit recognition data,https://www.reddit.com/r/MachineLearning/comments/3hvhjv/kaggle_digit_recognition_data/,maximus12793,1440179282,"https://www.kaggle.com/c/digit-recognizer here is the contest link

copy of question from Kaggle. (seems like not a lot of people are  answering questions there so thought I would try here)

Say I cache all the training on a given model model and want to have a new user take a pen and draw a letter, upload his image (which would be scaled to the given 784x784, in b/w) and get a prediction for the digit made. Is there a good way to go about this? Don't I just need to figure out how to create the values that the data sets currently and give (ints 0-255 etc.) for the digit darkness?

The major difficulty is that I'm just not sure how how to get the same 0-255 type values (or where to find the formula) as given here or if there is a workaround that would do something similar with actual pen/paper digits, despite having been trained on all these other forms. 

This should involve figuring out how png files store gray scale pixel values and then just return a 0 - 255 value for each pixel right?

Thanks!",2,2,False,self,,,,,
334,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,4,3hvuxv,self.MachineLearning,So... what do you actually do with twitter data?,https://www.reddit.com/r/MachineLearning/comments/3hvuxv/so_what_do_you_actually_do_with_twitter_data/,Adamworks,1440184877,"I have been having a lot of fun playing with Twitter's streaming API. But I am really unsure what type of insights can be gleamed from this type of data. Besides word frequencies, word clouds, I am currently taking a bag of word approach and applying classification algorithms to it. Anything else I am missing?",9,3,False,self,,,,,
335,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,4,3hvy7v,nuit-blanche.blogspot.com,ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R,https://www.reddit.com/r/MachineLearning/comments/3hvy7v/ranger_a_fast_implementation_of_random_forests/,compsens,1440186196,,7,38,False,http://b.thumbs.redditmedia.com/KGlgkxTspkqceSQEC8PKCGbZrWe0NClONOxqEipHIAk.jpg,,,,,
336,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,4,3hw0d0,self.MachineLearning,Reproducing On the Origin of Circuits,https://www.reddit.com/r/MachineLearning/comments/3hw0d0/reproducing_on_the_origin_of_circuits/,dpflug,1440187119,"So, I'm sure many of you are familiar with [On the Origin of Circuits](http://www.damninteresting.com/on-the-origin-of-circuits/). It's a fun little story.

I've got programming experience (Python, Ruby, C, Lisps), but nothing involving FPGAs so far.

I find it fascinating. I want to play with it. Is this something I could grab a ~$100 FPGA and learn to do in a weekend or year, or would this require specialized hardware/knowledge?",2,1,False,self,,,,,
337,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,7,3hwhea,self.MachineLearning,Job matching algorithms?,https://www.reddit.com/r/MachineLearning/comments/3hwhea/job_matching_algorithms/,mcjoness,1440194546,"Hey guys,

I was trying to think of a way to tackle an import issue in a lot of companies: job matching. To me, it sounded like a great clustering problem, but maybe I'm not thinking it through enough. I've searched online looking for some papers dealing with job matching clustering algorithms and haven't turned up much. 

The open example is as follows...

-Employees rate their proficiency in each of many skills or take some test to determine that rating. 

-In comes a manager that needs an employee with a set of skills. 

-What is the ""*best*"" way to match an employee with that job?

To me it sounds like an interesting clustering problem. Has anyone ever come across a clustering implementation of this issue? 
 
",8,5,False,self,,,,,
338,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,10,3hx2ok,self.MachineLearning,Any suggestion of monograph subject?,https://www.reddit.com/r/MachineLearning/comments/3hx2ok/any_suggestion_of_monograph_subject/,TiuTalk,1440205237,"I'm finishing my computer science bachelor's degree and I would love to do something related with machine learning (or other AI subtopic) on my monograph (where I will need to implement something, not only theorize about it). I have a really long background in web development, know a lot of Ruby and a little of Python, but language won't be a problem. The thing is, I have no previous experience in Machine Learning. Anyway, you guys have any suggestions about what I could implement? ",3,1,False,self,,,,,
339,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,10,3hx6q5,self.MachineLearning,"Need advice on the use of ensembles (i.e., &gt;1 predictor) on a two-class problem",https://www.reddit.com/r/MachineLearning/comments/3hx6q5/need_advice_on_the_use_of_ensembles_ie_1/,IndianSurveyDrone,1440207442,"I am working on a two-class problem in a high-dimensional feature space. I am wanting to combine the answers of more than one predictor to get better answers on each testing example.

Now, most of you have probably heard of ensembles/meta-predictors/committees etc. The basic idea is to combine the answers from more than one predictor to get a more accurate answer. The easiest way of combining predictors is to simply use a majority vote, and this often works well. However, when one predictor is not so good, then majority vote may not be the best way to go.

I want to combine predictors in a way that is based on the performance of the predictors according to the features of the training examples. 

Here is what I mean by this. Consider two predictors, P1 and P2, predicting data that has two features, F1 and F2, for two classes, C1 and C2. P2 may have a much better performance on the training set, but maybe P1 gets the right answer (and P2 doesn't) when F1 and F2 are small. Naively, we can guess that if a testing example has small values for F1 and F2, we would assign P1 a higher weight, even if P2 has a much better performance overall. 

One way of using this information might be to cluster the examples that each predictor gets right, and see how far away a new testing example is from the centroid. But there is a big problem--when we talk about whether or not a predictor gets an example right, we mean that it chose the correct class. But if one predictor (say P1) just chooses all examples as C1, for example, it will always do better than the other predictor on that class! Put another way, if a testing example were close to P1's centroid, it wouldn't make sense for P1 to have any input on the answer, since it just guessed C1 on everything.

I'm sure someone has looked into this problem before. It seems like the way to combine predictors in this way would be a ""simple"" algorithm; however, I don't know off the top of my head how to tackle it. I have some preliminary work I've done, but haven't come to any conclusions yet. 

If anyone has any comments, or knows of some things off the top of their head, that would be great! Or, if anyone wants to just discuss the problem, that would be fun, too. At the moment, I am going to look into the Google search, ""combining classifiers in feature space"" and see what I can get from there.",2,2,False,self,,,,,
340,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,11,3hxdx3,self.MachineLearning,Why do some people hate neural networks/deep learning?,https://www.reddit.com/r/MachineLearning/comments/3hxdx3/why_do_some_people_hate_neural_networksdeep/,jackbrucesimpson,1440211449,"I've heard people discuss how neural networks/CNNs/deep learning are a black box approach and I shouldn't bother with it and was wondering two things: firstly, why are people so opposed to this technique and secondly are there any alternatives for identifying objects in images that achieve the same speed and accuracy?",20,4,False,self,,,,,
341,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,11,3hxetc,deeplearning4j.org,Amusing Word2vec Results,https://www.reddit.com/r/MachineLearning/comments/3hxetc/amusing_word2vec_results/,vonnik,1440211932,,2,31,False,default,,,,,
342,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,12,3hxj9q,kenandeen.wordpress.com,Reinforcement Learning - Tower of Hanoi,https://www.reddit.com/r/MachineLearning/comments/3hxj9q/reinforcement_learning_tower_of_hanoi/,khaleesi1996,1440214477,,6,5,False,http://a.thumbs.redditmedia.com/UNdPI4tw8_HrMo4zOW-s5G-V67VfgFPCBM1M_7kq4m8.jpg,,,,,
343,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,16,3hy2cq,self.MachineLearning,Implementing GoogLenet for Devanagri (Hindi) character classification.,https://www.reddit.com/r/MachineLearning/comments/3hy2cq/implementing_googlenet_for_devanagri_hindi/,harshitladdha,1440227480,"I want to implement Googlenet for hindi character classification. This net has been used to classify chinese characters in this research paper--  http://arxiv.org/pdf/1505.04925.pdf.

Till now I am only using Googlenet without embedding any directional feature maps.
I have used Googlenet with one and two inception modules. After running for 35 thousand iterations both networks are giving accuracy around 88.5% which almost remains constant till 60 thousand and starts declining  afterwards.

No. of classes = 60
Dataset size = 25000 (both test and train)
Image size = 64x64

base_lr: 0.001
momentum: 0.9
weight_decay: 0.0005
lr_policy: ""inv""
gamma: 0.0001
power: 0.75

I just wanted to know is this optimum accuracy for these nets on this data or nets can be optimized further. This research paper has mentioned accuracy of 96.26% on Chinese Character which are far more in no. of classes and complex as compared to Hindi characters.
But on hindi characters accuracy is stuck at 88%. What can be done to further increase accuracy.

for solver and network architecture files for both networks, see this post --
https://groups.google.com/forum/#!topic/caffe-users/OgQ9k5LOaM4
",5,7,False,self,,,,,
344,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,19,3hydp2,self.MachineLearning,X-ray manufacturer India,https://www.reddit.com/r/MachineLearning/comments/3hydp2/xray_manufacturer_india/,paulrobinson00,1440237932,,0,1,False,default,,,,,
345,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,19,3hyeeq,self.MachineLearning,Thoughts on sequence-to-sequence LSTM model,https://www.reddit.com/r/MachineLearning/comments/3hyeeq/thoughts_on_sequencetosequence_lstm_model/,[deleted],1440238623,"I have a data set of sentences, which are sequences of Socher's pre-trained word vectors. These sequence map 1-to-1 to another series of vectors for each sentence that I have calculated to describe some aspects of the sentences. These vectors are such that their values add to one. For instance, an example mapping would be,

     wordvectors[""headache""] (50 dim vector) --&gt;&gt; 20 dim vector, [0.0, ..., 0.0, 0.25, 0.25, 0.25, 0.25, 0.0... 0,0]

I am training on a small subset of my data (10,000 points) to get things working, but I cannot overfit even with this small data. In fact, I cannot even get accuracy above 0.2 (Keras accuracy measure). This is the model,

    embedding_size = 50
    hidden_size = 512
    output_size = 20
    maxlen = 60

    model = Sequential()
    model.add(JZS1(embedding_size, hidden_size)) # try using a GRU instead, for fun
    model.add(Dense(hidden_size, hidden_size))
    model.add(Activation('relu'))
    model.add(RepeatVector(maxlen))
    model.add(JZS1(hidden_size, hidden_size, return_sequences=True))
    model.add(TimeDistributedDense(hidden_size, output_size, activation=""softmax""))

    model.compile(loss='mse', optimizer='adam')

I have padded my inputs and outputs with zeros to fit the maxlen dimension, but I have not added ""stop symbols"". What kind of vector would I be adding as a stop symbol? Are they essential to train correctly?

Has anyone implemented a seq-to-seq LSTM model that can map vectors of values to vectors of parallel values, like a regression?

Interested in Redditors' thoughts.",1,0,False,default,,,,,
346,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,19,3hyf0z,homes.cs.washington.edu,Recursive Decomposition for Nonconvex Optimization,https://www.reddit.com/r/MachineLearning/comments/3hyf0z/recursive_decomposition_for_nonconvex_optimization/,_spreadit,1440239229,,2,17,False,default,,,,,
347,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,22,3hyuwn,stackoverflow.com,sklearn: TfidfVectorizer stop_words parameter does not work?,https://www.reddit.com/r/MachineLearning/comments/3hyuwn/sklearn_tfidfvectorizer_stop_words_parameter_does/,[deleted],1440251089,,0,0,False,default,,,,,
348,MachineLearning,t5_2r3gv,2015-8-22,2015,8,22,23,3hywvw,self.MachineLearning,Paper Catalogue data extractions and OCR question.,https://www.reddit.com/r/MachineLearning/comments/3hywvw/paper_catalogue_data_extractions_and_ocr_question/,Naturomatic,1440252303,"Peoples, help out  
I need to extract data with each picture, from a catalogue [like this one](http://image.rakuten.co.jp/ecjungle/cabinet/evidence_endcat/endcat4/evidence_4-1814.jpg)  
All I have succeeded so far is extracting only text.  
Any suggestions will help, I have to prove my self at a new job I've got.  
Bring it on",0,0,False,self,,,,,
349,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,3,3hzr01,self.MachineLearning,How should I use the textual data given below in order to carryout a classification task?(Please help by giving some example code),https://www.reddit.com/r/MachineLearning/comments/3hzr01/how_should_i_use_the_textual_data_given_below_in/,rohanpota,1440267287,"The dataset is at:-https://docs.google.com/spreadsheets/d/1oEGFT2MwU9ezWTxdbx0POzvJ9V-hpI8A-Dx610WJz_I/pubhtml

Please help by giving me links to some blogs or some other useful source on how to deal with this situation.
My platform:
Python:-
Scikit-learn,pandas numpy",9,0,False,self,,,,,
350,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,3,3hzs8c,self.MachineLearning,Clustering algorithm for complete weighted directed graph?,https://www.reddit.com/r/MachineLearning/comments/3hzs8c/clustering_algorithm_for_complete_weighted/,hmgp,1440267895,"Each node represents a store, and a link from A to B is weighted based on the percentage of shoppers from A who visited B. I'm not familiar with graph clustering algorithms, but it seems like most of them rely on subgraph density, which doesn't apply here since the graph has uniform density. Maybe I should try a more general clustering algorithm?

Edit: Forgot to include the purpose... I want to understand the relationship between the stores, but it would also be interesting the understand the shoppers.",27,10,False,self,,,,,
351,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,6,3i0i9c,self.MachineLearning,Predicting lat/long with varying levels of precision?,https://www.reddit.com/r/MachineLearning/comments/3i0i9c/predicting_latlong_with_varying_levels_of/,Horatio_SanzCulottes,1440280365,"I've got a data set with varying kinds of binary features and latitude and longitude. My suspicion is that certain features will be good predictors of the approximate latitude/longitude values. It is also the case that each decimal point of a lat/long number has [a level of precision that it implies](https://en.wikipedia.org/wiki/Decimal_degrees). I also might try to adjust/transform the features in such a way that they would no longer be binary.

Is there an algorithm that could be used to predict the latitude/longitude to varying levels of precision? If I hypothetically have up to N decimals of precision on the lat/long data, I would imagine that some items can be predicted all the way to N decimals, some items might only be predictable to n-1 or n-2 decimal places.

One thing I was considering was using a random forest (or similar method) to classify the first decimal that I am concerned about, which can have 10 possible values (0-9). Then, I could separate each possible value for the second decimal, third, etc, and give each one their own model. Given that this would have to be done separately for lat/long, it seems like it would generate a large number of models for even three decimal places. It might not be a problem for the amount of data I am using, but it is probably not the best approach.",11,5,False,self,,,,,
352,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,7,3i0jgw,self.MachineLearning,Need help on how to code special words topic model with a background distribution (SWB) using Java language,https://www.reddit.com/r/MachineLearning/comments/3i0jgw/need_help_on_how_to_code_special_words_topic/,itechy,1440280962,"I need someone help me on coding the collapsed Gibbs samplings equations for SWB model. If you read the paper you will see latent variable x which sampled from document-specific distribution, how could I calculate x values?!. Also, how to calculate values for Nd0, Nd1 and Nd2 variables.",1,1,False,self,,,,,
353,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,8,3i0t5j,self.MachineLearning,Labeled Image Data,https://www.reddit.com/r/MachineLearning/comments/3i0t5j/labeled_image_data/,denimshark,1440285764,Where can I get a ton of images correctly identified for use as training data for something I'm working on? Does anyone provide this service?,1,4,False,self,,,,,
354,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,8,3i0ula,self.MachineLearning,How can I learn to apply machine learning to ASP.NET MVC?,https://www.reddit.com/r/MachineLearning/comments/3i0ula/how_can_i_learn_to_apply_machine_learning_to/,[deleted],1440286496,,12,0,False,default,,,,,
355,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,9,3i0xny,self.MachineLearning,"When using forests of trees to evaluate the importance of features, what is the ""inter-tree"" variability?",https://www.reddit.com/r/MachineLearning/comments/3i0xny/when_using_forests_of_trees_to_evaluate_the/,Poydflink,1440288162,"As seen in http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html . The blue markers are the ""inter-tree"" variability but I cannot understand what's that supposed to mean.",2,6,False,self,,,,,
356,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,10,3i16ar,self.MachineLearning,Independent Bayesian Classifier Combination implementations?,https://www.reddit.com/r/MachineLearning/comments/3i16ar/independent_bayesian_classifier_combination/,cryptocerous,1440292858,"Anyone using ""Independent Bayesian Classifier Combination"" for ensembling classifiers?

What code do you use?",0,4,False,self,,,,,
357,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,12,3i1h34,wtvox.com,"Google is working on a new algorithm: ""Thought Vectors",https://www.reddit.com/r/MachineLearning/comments/3i1h34/google_is_working_on_a_new_algorithm_thought/,slackermanz,1440299069,,52,150,False,http://b.thumbs.redditmedia.com/VyzIXn2hNmmKbVR4HvpRLDX7yLTNtZcSSVnfE924Wnc.jpg,,,,,
358,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,21,3i2mit,self.MachineLearning,Why doesn't the normal equation for least squares simplify?,https://www.reddit.com/r/MachineLearning/comments/3i2mit/why_doesnt_the_normal_equation_for_least_squares/,[deleted],1440331930,"I am doing the CS 229 class of Andrew Ng using his lecture notes and youtube videos. He and [wikipedia](https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#The_general_problem) derive the normal equation as 

theta = (A^T A)^(-1) A^T y

Why do we use this form instead of simplifying it to

theta = A^(-1) y

Which of these two is used in linear regression algorithms? Is the first one somehow faster to compute maybe?",0,1,False,default,,,,,
359,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,22,3i2rkf,stackoverflow.com,Has anyone had experience using Caffe to classify OpenCV Mat images?,https://www.reddit.com/r/MachineLearning/comments/3i2rkf/has_anyone_had_experience_using_caffe_to_classify/,[deleted],1440335503,,0,1,False,default,,,,,
360,MachineLearning,t5_2r3gv,2015-8-23,2015,8,23,22,3i2ux7,i.imgur.com,"MLSS 2015 in Kyoto started today, guess some of you guys are on here!",https://www.reddit.com/r/MachineLearning/comments/3i2ux7/mlss_2015_in_kyoto_started_today_guess_some_of/,Gumeo,1440337838,,13,17,False,http://b.thumbs.redditmedia.com/OvEe3peJjSkkHS3yX51_fwdbW1-r8W2_YRiFxFhherE.jpg,,,,,
361,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,1,3i3cw6,self.MachineLearning,What can I do with this variable?,https://www.reddit.com/r/MachineLearning/comments/3i3cw6/what_can_i_do_with_this_variable/,Chopsting,1440347408,"This might be an incredible silly post for most of you (Sorry for that).


I'm currently running a ML attempt on the 
[Kaggle edX Comp](https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015/) which I particiapted in when it was running. But I wanted to see if I could improve my results and also tackle it with Python rather then R as was taught.


I didn't really check which features are driving my model and to my suprise when I did, it was something I didn't expect at all.


Rank| feature pos | % | feature
---|---|----|----
1| 0| 0.296856| WordCount
2 | 2 | 0.157281| Weekday
3 | 1 | 0.081035 |Hour
4 | 45 | 0.048290| NewsDesk_Business
5 | 10 | 0.032878| NewsDesk_Culture
6 | 32 | 0.027630| NewsDesk_Foreign
7 | 18 | 0.027414| NewsDesk_Magazine
8 | 51 | 0.026559| NewsDesk_Metro
9 | 12 | 0.025563| NewsDesk_Multimedia
10 | 3 | 0.020703| NewsDesk_National

And I looked into WordCount abit, trying to use the log value instead because of the extremes that occur in wordcount
max 10k
min 0
median 377

but my acc decreased. I put alot of effort into labeling everything in the dataset and I more or less ignored looking into the top 3 features.... Is there anything you would have looked into that I might want to try out?

Again sorry if you find this to be a waste of a post...


Edit: Also if someone could clarify how you decide weights on ensemble models?",3,9,False,self,,,,,
362,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,1,3i3d7a,self.MachineLearning,When does a bayesian neural network outperform a normal neural network?,https://www.reddit.com/r/MachineLearning/comments/3i3d7a/when_does_a_bayesian_neural_network_outperform_a/,thai_tong,1440347555,"I have created a neural network learning algorithm to learn how to play poker. I have read about bayesian neural networks and I am considering making one for playing poker.

What sort of problems are better suited to the bayesian approach? I am curious about applications in general not just poker playing.

What sort of characteristics of the data/problem make a non bayesian approach favourable?",9,18,False,self,,,,,
363,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,1,3i3fw5,bicorner.com,Why e-Commerce Cant Afford to Ignore to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3i3fw5/why_ecommerce_cant_afford_to_ignore_to_machine/,derrickmartins,1440348830,,0,1,False,default,,,,,
364,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,2,3i3huo,gitxiv.com,Deep Unsupervised Learning using Nonequilibrium Thermodynamics (paper+code),https://www.reddit.com/r/MachineLearning/comments/3i3huo/deep_unsupervised_learning_using_nonequilibrium/,samim23,1440349717,,2,30,False,default,,,,,
365,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,4,3i406k,gist.github.com,Exploring antonyms with word2vec,https://www.reddit.com/r/MachineLearning/comments/3i406k/exploring_antonyms_with_word2vec/,kcimc,1440358072,,2,23,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
366,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,5,3i48sr,self.MachineLearning,RBM: Changing binary units to ReLUs or gaussian units?,https://www.reddit.com/r/MachineLearning/comments/3i48sr/rbm_changing_binary_units_to_relus_or_gaussian/,ssrg1615,1440361971,"I have written a working implementation of an RBM with binary hidden/visible units in R. I've been looking for a while but just can't figure how to change the binary units to either gaussian or ReLUs.

If I wanted my input data to be real values, would I change the visible units and the hidden units? Or just the visible units?

Lets say I wanted to change both. Currently, I'm calculating the hidden/visible probabilities using the logistic sigmoid function (1/(1+e^(-x))). The ReLU uses max(0, x + N(0,1)). As I currently understand, I would switch all occurrences of the logistic sigmoid function with the ReLU max function. However, this doesn't yield results that make a bit of sense. So I'm not sure what I'm actually supposed to be changing.",4,3,False,self,,,,,
367,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,8,3i4v7n,self.MachineLearning,What are your favorite introductory materials on Gaussian Processes?,https://www.reddit.com/r/MachineLearning/comments/3i4v7n/what_are_your_favorite_introductory_materials_on/,greatm31,1440372047,"I have the GPML (Rasmussen) book and while I can more or less get through it, I still have a tenuous grasp on the material. What are your favorite materials for learning this topic? Know of any well-made talks available online? Or perhaps tutorial-style papers?
",5,29,False,self,,,,,
368,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,9,3i5246,self.MachineLearning,Need advice on algorithms to use on my thesis,https://www.reddit.com/r/MachineLearning/comments/3i5246/need_advice_on_algorithms_to_use_on_my_thesis/,TiuTalk,1440375268,"I'm starting to work on my bachelor's degree thesis and I decided to create a tool that will help people finding good deals when looking for a house/apartment for rent. 

I'm pretty sure I can use Machine Learning to help with this problem and create an interesting tool. 

I started doing some research, studied and implemented my own version of Nearest K using euclidean distance and it worked great for that iris dataset.. But it won't work with realty data and textual data... So where should I go next? What algorithms to study? Should I stick with NearestK? 

Thanks for your help, ",3,6,False,self,,,,,
369,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,11,3i5fr8,self.MachineLearning,Logistic Regression Fundamentals,https://www.reddit.com/r/MachineLearning/comments/3i5fr8/logistic_regression_fundamentals/,Neb519,1440381874,Took my best shot at explaining the fundamentals of logistic regression [here](http://gormanalysis.com/logistic-regression-fundamentals/).  Hope this helps someone.,3,13,False,self,,,,,
370,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,13,3i5xae,arxiv.org,Neural Transformation Machine: A New Architecture for Sequence-to-Sequence Learning,https://www.reddit.com/r/MachineLearning/comments/3i5xae/neural_transformation_machine_a_new_architecture/,ma2rten,1440391103,,3,38,False,default,,,,,
371,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,14,3i5zlw,self.MachineLearning,will the cuda 7.0 work on ubuntu 15.04. And what is the process.,https://www.reddit.com/r/MachineLearning/comments/3i5zlw/will_the_cuda_70_work_on_ubuntu_1504_and_what_is/,tushar1408,1440392545,Thanks in advance,5,0,False,self,,,,,
372,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,16,3i6a6o,self.MachineLearning,Advice On Getting Started Learning From Facebook Messages?,https://www.reddit.com/r/MachineLearning/comments/3i6a6o/advice_on_getting_started_learning_from_facebook/,aaahhhman,1440399729,"Apologies if this is the wrong place to post this. I'm an undergraduate CSE student. My latest project was inspired by r/SubredditSimulator/. I downloaded all of the facebook messages that I've ever sent and my goal is to create a program that can generate a message typical of a particular user. I was thinking this model can be made more complex, such as generating a typical message one would send at a particular time of the day, or the type of response they would send to a message I sent. 

Can someone give me some advice on how to get started with a project like this? I don't have any experience with machine learning, but I have a fair amount of experience programming. Is this doable for someone with my experience/skill level? Is there a particular framework or library that I should use to undertake a project like this? Would C++ be an appropriate language for this, or must I stick to language like Python, Matlab or R that is more typically used for applications like these?",2,2,False,self,,,,,
373,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,17,3i6fp9,self.MachineLearning,What optimization methods work best for LSTMs?,https://www.reddit.com/r/MachineLearning/comments/3i6fp9/what_optimization_methods_work_best_for_lstms/,polytop3,1440403975,"Hi All,

I've been using theano to experiment with LSTMs, and was wondering what optimization methods (SGD, Adagrad, Adadelta, RMSprop, Adam, etc) work best for LSTMs? Are there any research papers on this topic?

Also, does the answer depend on the type of application I am using the LSTM for? If so, I am using LSTMs for text classification (where the text is first converted into word vectors).

Finally, would the answers be the same or different for RNNs?

Any pointers to research papers, or personal insight would be highly appreciated!

LSTMs seem to be quite powerful and I am interested in learning more about how to best use them.

Thanks in advance,
polytop3",13,16,False,self,,,,,
374,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,18,3i6kay,babulnathengineering.com,Drip Pipe Plant | Drip Irrigation Pipe Plant,https://www.reddit.com/r/MachineLearning/comments/3i6kay/drip_pipe_plant_drip_irrigation_pipe_plant/,babulnatheng,1440407799,,0,1,False,default,,,,,
375,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,20,3i6ss4,self.MachineLearning,HDPE Bottle Machinery In India | HDPE Bottle Machinery In Ahmedabad,https://www.reddit.com/r/MachineLearning/comments/3i6ss4/hdpe_bottle_machinery_in_india_hdpe_bottle/,zeelplastm,1440414190,,0,0,False,default,,,,,
376,MachineLearning,t5_2r3gv,2015-8-24,2015,8,24,20,3i6w24,self.MachineLearning,Modelling traffic flow of an entire road network.,https://www.reddit.com/r/MachineLearning/comments/3i6w24/modelling_traffic_flow_of_an_entire_road_network/,johnryan465,1440416506,"I am new to machine learning and I do not know how to model this problem, there is very little data-points so supervised seems impossible, the is no error function as the data is unknown so unsupervised seems out of reach. I my research I have had the idea of modelling the road network as a fluid but I am not sure if that is a good idea.  I am trying to model the number of cars which pass every road in the  road network. I have access to &lt;50 data-points on certain roads(main roads), the location of every building on the road network. The data does not need to be extremely accurate, a rough number is all that is wanted. I may be able to get  more data-sets if you recommenced them. Computing resources are not a problem. Thanks for any guidance you can give.",1,6,False,self,,,,,
377,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,3,3i8acw,suboptimum.wordpress.com,The best thing since &lt;del&gt;sliced bread&lt;/del&gt; cross-validation,https://www.reddit.com/r/MachineLearning/comments/3i8acw/the_best_thing_since_delsliced_breaddel/,suhrob,1440439856,,7,27,False,http://b.thumbs.redditmedia.com/ENsBfIBDdFazaqFIJdknv1NKOvfRsA7kOVvIC1-p62A.jpg,,,,,
378,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,3,3i8cy6,mitsloan.mit.edu,"New digital pen could mean faster Alzheimers, Parkinsons diagnoses",https://www.reddit.com/r/MachineLearning/comments/3i8cy6/new_digital_pen_could_mean_faster_alzheimers/,mevalemadres,1440440874,,0,2,False,http://b.thumbs.redditmedia.com/-ih2BDB8kGp1eSmsHPRveQRKM7jUbRGYhP8sKuDfBFk.jpg,,,,,
379,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,3,3i8f3d,codesachin.wordpress.com,Linear and Quadratic Discriminant Analysis for ML / statistics newbies,https://www.reddit.com/r/MachineLearning/comments/3i8f3d/linear_and_quadratic_discriminant_analysis_for_ml/,sachinrjoglekar,1440441708,,0,4,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
380,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,5,3i8qrw,reddit.com,Looking for a technical and social network where opensource AI research is done,https://www.reddit.com/r/MachineLearning/comments/3i8qrw/looking_for_a_technical_and_social_network_where/,BenRayfield,1440446527,,0,0,False,http://a.thumbs.redditmedia.com/PDQadCzYX_x1bU3KrYuhTptu6eDdOVVagFG6q_Afyb4.jpg,,,,,
381,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,7,3i9a0c,self.MachineLearning,Some Questions from a CS undergrad student about jobs in machine learning,https://www.reddit.com/r/MachineLearning/comments/3i9a0c/some_questions_from_a_cs_undergrad_student_about/,spiritofawolf,1440454500,"Hi all, 

I actually have a few questions. But first, some background info:

I'm a computer science major starting my 3rd year in university shortly, and after a summer pushing paper in a really lame office job I realize it's time I start getting my shit together.

 The only programming experience I have is whatever I've learned in school so far (spent my first 2 years involved in random stuff unrelated to CS), which honestly feels like nothing. I know that my main areas of interest though are machine learning/deep learning, computational neuroscience and bio/neuro inspired computing (think google's DeepMind, maybe even the Human Brain Project, etc) but beyond that, I don't know much about the fields specifically. 

This year I'm really looking to start digging deeper in these areas to hopefully get a better understanding of them and narrow down my interests. I figure a PhD is most definitely in my future but I know for a fact that I do not want to work in academia. 

Okay, onto the questions:

1. What are the most interesting jobs you have ever done or heard of that involved machine learning?

2. Realistically, are there any industry, non-research jobs that involve comp neuro? (I'm aware that this probably isn't the best subreddit to ask that one, but incase anyone has an answer I'll ask it anyway, if not that's okay) 

3. What should I do during and after my undergrad degree (besides research and a PhD) to be qualified for the kinds of jobs mentioned in 1. And 2? 

I realize this post is all over the place and probably goes beyond the scope of this subreddit but any help or advice would be greatly appreciated, thank you :)",5,36,False,self,,,,,
382,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,8,3i9gd1,self.MachineLearning,Asking about concatenation of word vectors to form sentence vectors,https://www.reddit.com/r/MachineLearning/comments/3i9gd1/asking_about_concatenation_of_word_vectors_to/,kullback-leibler,1440457222,"I've read on some articles that one basic way to form sentence vector from word vectors other than averaging is by 'simply' concatenating the word vectors. My question is how then the length of the sentence vector be normalized? Shouldn't different number of words in a sentence result in different length of the sentence vector then? Am I missing something here?

",5,6,False,self,,,,,
383,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,12,3iai3r,youtube.com,"Geoffrey Hinton: ""Some Applications of Deep Learning""",https://www.reddit.com/r/MachineLearning/comments/3iai3r/geoffrey_hinton_some_applications_of_deep_learning/,Buck-Nasty,1440474896,,1,3,False,http://a.thumbs.redditmedia.com/JHkDnDudtDfjBeUWglHtMQZdM9EuIqalGihDpj-3l-4.jpg,,,,,
384,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,14,3iapms,self.MachineLearning,"I want to get a masters in CS specializing in Machine Learning in order to work on ML projects at Big 4. If I go to a school ranked 70th, will this hurt me?",https://www.reddit.com/r/MachineLearning/comments/3iapms/i_want_to_get_a_masters_in_cs_specializing_in/,csquestioneering,1440479453,,8,0,False,self,,,,,
385,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,15,3iavq6,gormanalysis.com,Intuitive intro to logistic regression,https://www.reddit.com/r/MachineLearning/comments/3iavq6/intuitive_intro_to_logistic_regression/,cast42,1440483470,,0,1,False,http://b.thumbs.redditmedia.com/bwaCheA72vP18LYwtHQ_BlRSQPBIZ2wO2_RK1me1lAY.jpg,,,,,
386,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,15,3iayyz,arxiv.org,"'Neural reasoner': achieves ~90% accuracies on positional reasoning and pathfinding, two of the hardest bAbI tasks",https://www.reddit.com/r/MachineLearning/comments/3iayyz/neural_reasoner_achieves_90_accuracies_on/,bluecoffee,1440485519,,9,42,False,default,,,,,
387,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,15,3iaz3e,self.MachineLearning,Why normalize input variables in NN?,https://www.reddit.com/r/MachineLearning/comments/3iaz3e/why_normalize_input_variables_in_nn/,[deleted],1440485606,"I'm reading the 'Efficient Backprop' paper and it's mentioned that the reason to have a zero mean for the input variables is because otherwise the eigenvalue (for the hessian I think) will be very large. This would imply a large condition number so the cost surface will be steep in some directions and shallow in others leading to a slow convergence.

There are several things I'm having trouble with in the above statement

(1) Why would the eigenvalue be large? I can't think of a good reason mathematically

(2) Why would a high eigenvalue imply a high condition number?

(3) If the learning rates are independent for every weight (as is also recommended), then why should it matter if one direction in the cost surface is much steeper than another?

Anyone have any ideas?",0,2,False,default,,,,,
388,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,16,3ib29u,github.com,LeaveOneHotEncoding - transforming from huge categorical variables to numerical,https://www.reddit.com/r/MachineLearning/comments/3ib29u/leaveonehotencoding_transforming_from_huge/,cast42,1440487888,,1,1,False,http://b.thumbs.redditmedia.com/XshsK6ppC_YJaaTmNf4BvcOlKzgGSo6TTAfdKzuqkjA.jpg,,,,,
389,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,18,3ibcjy,self.MachineLearning,Looking for small theoretical pen-and-paper problems in ML,https://www.reddit.com/r/MachineLearning/comments/3ibcjy/looking_for_small_theoretical_penandpaper/,anokhinn,1440496226,"I'll be giving a practical intro course in ML this fall and I'm looking for a book (or other available source) with some interesting problems in machine learning. Ideally, these problems would help students understand better how different ML algorithms work internally. The problems are supposed to be solvable on paper, so no computer required. 

At the moment I'm using a book by Duda-Hart-Stork, Bishop's PRML and MMDS by Leskovec et al. Are there any other sources that may appear to be useful?

Thanks.",5,2,False,self,,,,,
390,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,18,3ibck1,andyljones.tumblr.com,Quick summary of Dwork's holdout reuse algorithm,https://www.reddit.com/r/MachineLearning/comments/3ibck1/quick_summary_of_dworks_holdout_reuse_algorithm/,bluecoffee,1440496228,,1,8,False,http://b.thumbs.redditmedia.com/sGFevTdH-cqwIPu3brviaUKCJ9GnVjadbi-2CEgMY4Y.jpg,,,,,
391,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,19,3ibh5k,arxiv.org,Fast Asynchronous Parallel SGD,https://www.reddit.com/r/MachineLearning/comments/3ibh5k/fast_asynchronous_parallel_sgd/,muktabh,1440499676,,0,4,False,default,,,,,
392,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,21,3ibqpn,self.MachineLearning,ELI5: What makes the human brain so good compared to NNs?,https://www.reddit.com/r/MachineLearning/comments/3ibqpn/eli5_what_makes_the_human_brain_so_good_compared/,heltok,1440505831,My brain seems to be pretty good at seeing some patterns and learning various tasks. Meanwhile my algorithms are really good at some tasks but struggle at other. So what makes my brain better than my NNs? More neurons or is it some other properties as well? ,8,0,False,self,,,,,
393,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,22,3ibx9j,sebastianraschka.com,"Python, Machine Learning, and Language Wars. A Highly Subjective Point of View",https://www.reddit.com/r/MachineLearning/comments/3ibx9j/python_machine_learning_and_language_wars_a/,leonardofed,1440509211,,0,11,False,http://b.thumbs.redditmedia.com/pdwbn-rkqJVp5DmPbbiPiGuvE1LVi-yLmUWq0l7STEo.jpg,,,,,
394,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,22,3iby7t,nuit-blanche.blogspot.com,Distributed Compressive Sensing: A Deep Learning Approach (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/3iby7t/distributed_compressive_sensing_a_deep_learning/,compsens,1440509689,,1,2,False,http://b.thumbs.redditmedia.com/cx1t3MkweMpg3baCkEFe6ErwO9VW4gw14yWK6trimgs.jpg,,,,,
395,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,22,3ibzx0,cloudacademy.com,Let's Understand Amazon Machine Learning (2h 11min course),https://www.reddit.com/r/MachineLearning/comments/3ibzx0/lets_understand_amazon_machine_learning_2h_11min/,leonardofed,1440510500,,0,2,False,http://b.thumbs.redditmedia.com/ABJxlhxI5NghdeENlri3v0zxSoHNjCV_ueWY1XqVfqs.jpg,,,,,
396,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,23,3ic53o,self.MachineLearning,Where can I download the toy clustering dataset in this KDD12 paper?,https://www.reddit.com/r/MachineLearning/comments/3ic53o/where_can_i_download_the_toy_clustering_dataset/,sickcpt,1440512900,"the paper is: Locally-scaled spectral clustering using empty region graphs.
link: wan.poly.edu/KDD2012/docs/p1330.pdf",1,1,False,self,,,,,
397,MachineLearning,t5_2r3gv,2015-8-25,2015,8,25,23,3ic6ml,joschu.github.io,Theano successor is here?,https://www.reddit.com/r/MachineLearning/comments/3ic6ml/theano_successor_is_here/,elanmart,1440513535,,10,24,False,default,,,,,
398,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,0,3iceuk,stackoverflow.com,How to modify Caffe network input for C++ API?,https://www.reddit.com/r/MachineLearning/comments/3iceuk/how_to_modify_caffe_network_input_for_c_api/,jackbrucesimpson,1440516959,,0,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
399,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,1,3icnsv,computervisiontalks.com,Very interesting Talk (tagged). Role of machine learning in Medicine.,https://www.reddit.com/r/MachineLearning/comments/3icnsv/very_interesting_talk_tagged_role_of_machine/,ojaved,1440520607,,0,1,False,http://b.thumbs.redditmedia.com/dyrXhhAbevTcc16L1KHOEMZQG1XkBinZJZHB_qSsumc.jpg,,,,,
400,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,2,3icxn8,self.MachineLearning,presence features?,https://www.reddit.com/r/MachineLearning/comments/3icxn8/presence_features/,whistlerbrk,1440524504,"Hi, I'm new to ML. I just finished Andrew Ng's course and I'm trying to build something. I found an interesting source of data I wanted to play around with and have built a scraper to collect it.

Of course the data is a bit dirty and some examples are missing features entirely that other ones have.

Let's say for example this is ""age"".

In a neural network complex enough can I create a feature ""age_present"" which is 1 or 0 if I have an age present or not (null) for the example? In so doing, and again assuming the network is complex enough, am I guaranteed that a feature will emerge if age is relevant that will use age if present and ignore it if not?

I guess I'm basically asking how you would deal with null values.",0,1,False,self,,,,,
401,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,3,3id2qh,venturebeat.com,Galvanize partners with Nvent to launch Data Engineering courses this Fall,https://www.reddit.com/r/MachineLearning/comments/3id2qh/galvanize_partners_with_nvent_to_launch_data/,dynelle_abeyta,1440526505,,0,1,False,default,,,,,
402,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,3,3id52p,indico.io,Visualizing with t-SNE,https://www.reddit.com/r/MachineLearning/comments/3id52p/visualizing_with_tsne/,[deleted],1440527431,[deleted],0,1,False,default,,,,,
403,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,3,3id7yn,indico.io,Visualizing with t-SNE,https://www.reddit.com/r/MachineLearning/comments/3id7yn/visualizing_with_tsne/,madisonmay,1440528617,,2,11,False,http://b.thumbs.redditmedia.com/Ft3gZUhfFZnk2KPNysZ0Pj-e5qGRSHlG1K8DHFpRYKs.jpg,,,,,
404,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,4,3idf00,cloudacademy.com,Amazon Machine Learning: use cases and a real example in Python (with code),https://www.reddit.com/r/MachineLearning/comments/3idf00/amazon_machine_learning_use_cases_and_a_real/,leonardofed,1440531447,,1,9,False,http://b.thumbs.redditmedia.com/GzOrg-om_Yg74liPuBcxZZNx0FOJ2mrSDKdmPpIyjYk.jpg,,,,,
405,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,5,3idr1o,youtube.com,"Silicon Brain: 1000,000 ARM cores - Computerphile",https://www.reddit.com/r/MachineLearning/comments/3idr1o/silicon_brain_1000000_arm_cores_computerphile/,letoseldon,1440536285,,2,37,False,http://b.thumbs.redditmedia.com/S4IcOoFYRIJdC5mQhVoB50BVZXqUv9LXFPBY6DVJjgc.jpg,,,,,
406,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,7,3ie3gq,nbviewer.ipython.org,A collection of visualizations for IPython,https://www.reddit.com/r/MachineLearning/comments/3ie3gq/a_collection_of_visualizations_for_ipython/,dnprock,1440541353,,0,3,False,http://a.thumbs.redditmedia.com/SDx1ESsDr7mO6Yx59PM9pnwYY8JTsxrxyDnvAN2au58.jpg,,,,,
407,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,11,3if3cx,fixingtao.com,Logic Check  Fixing Communication Through Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3if3cx/logic_check_fixing_communication_through_deep/,itistoday,1440557639,,0,1,False,http://b.thumbs.redditmedia.com/s7SuRJxoJQSdjRwyF6WW79SZHTamq720xlEM0fBUgco.jpg,,,,,
408,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,12,3if5qy,blog.siftscience.com,Large Scale Decision Forests: Lessons Learned,https://www.reddit.com/r/MachineLearning/comments/3if5qy/large_scale_decision_forests_lessons_learned/,prajit,1440558767,,9,48,False,http://b.thumbs.redditmedia.com/K0WzeUgrGmgtaoELRCJsOcyjdSm5Yd9ZWIVKWW1_B-M.jpg,,,,,
409,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,12,3if5sj,theplatform.net,Deep Learning Pioneer Yann LeCun on the Next Generation of Deep Learning Co-Design,https://www.reddit.com/r/MachineLearning/comments/3if5sj/deep_learning_pioneer_yann_lecun_on_the_next/,[deleted],1440558782,[deleted],1,1,False,default,,,,,
410,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,15,3ifpbe,reddit2vec.com,Using Doc2Vec to recommend SubReddits,https://www.reddit.com/r/MachineLearning/comments/3ifpbe/using_doc2vec_to_recommend_subreddits/,jmportilla,1440570207,,7,21,False,default,,,,,
411,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,15,3ifryy,self.MachineLearning,Reshape the input,https://www.reddit.com/r/MachineLearning/comments/3ifryy/reshape_the_input/,rick168,1440572018,"I try to load the data into the network for training:

    seq_u = np.array(train.values[:][:, 1:2], dtype=np.float32)

And here is a screenshot of seq_u:
[image!](http://pan.baidu.com/s/1bQhUA)

seq_u.shape = (40L, 1L)

    time_steps = 5 
    n_u = 1

So I try to reshape it to be (40L, 5L, 1L)

    seq_u = seq_u.reshape(40, time_steps, n_u) 

ValueError: total size of new array must be unchanged.
How to fix? Any help appreciated.

",2,4,False,self,,,,,
412,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,16,3ifwj1,self.MachineLearning,I have all network traffic in a few companies over a 30 day rolling period - what interesting things can I do?,https://www.reddit.com/r/MachineLearning/comments/3ifwj1/i_have_all_network_traffic_in_a_few_companies/,NeoSilky,1440575365,"I'm thinking in terms of creating something for anomaly detection (This set of traffic from this machine is rogue). I have some basic detection methods in place but would love to incorporate some ML.

I saw that Netflix use DBSCAN for in-house server anomaly detection but I think their approach doesn't apply for my use case.

Does anyone have any ideas/advice/algorithms? Any help would be much appreciated!",4,15,False,self,,,,,
413,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,18,3ig4js,self.MachineLearning,"Using K-means, how do I represent Ip's or Countries on the coordinate axis for classification problems?",https://www.reddit.com/r/MachineLearning/comments/3ig4js/using_kmeans_how_do_i_represent_ips_or_countries/,smurf6969,1440581867,,5,1,False,self,,,,,
414,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,19,3ig8q6,self.MachineLearning,Strengths and Weaknesses of Top Machine Learning Companies,https://www.reddit.com/r/MachineLearning/comments/3ig8q6/strengths_and_weaknesses_of_top_machine_learning/,nasazeek,1440585046,"Trying to understand the strengths and weaknesses of top machine learning companies past the hype about Palantir. 

Have come across: Context Relevant, Ayadsi, Digital Reasoning, Palantir, and IBM Watson. 

What are these companies good/bad at? And are there others I should pay attention to? ",0,0,False,self,,,,,
415,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,20,3igb93,joschu.github.io,Announcing the Computation Graph Toolkit (with a Theano-like API),https://www.reddit.com/r/MachineLearning/comments/3igb93/announcing_the_computation_graph_toolkit_with_a/,fnl,1440586977,,3,1,False,default,,,,,
416,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,20,3igccn,self.MachineLearning,Xray machines manufacturers,https://www.reddit.com/r/MachineLearning/comments/3igccn/xray_machines_manufacturers/,paulrobinson00,1440587721,,0,1,False,default,,,,,
417,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,20,3iggbm,blog.acolyer.org,Mining High-Speed Data Streams,https://www.reddit.com/r/MachineLearning/comments/3iggbm/mining_highspeed_data_streams/,mttd,1440590317,,0,5,False,http://b.thumbs.redditmedia.com/TNH_GVdGJXiHSYNaZW7Us1r9W-5P-8H_PGwbGM568BI.jpg,,,,,
418,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,22,3igs7o,self.MachineLearning,Data quantity vs quality,https://www.reddit.com/r/MachineLearning/comments/3igs7o/data_quantity_vs_quality/,ienaplissken,1440596495,"I'm tackling a fine-grained recognition problem. The aim is classifying images. At the very beginning, I have to define both the dataset and the classification process.

Concerning the process, the baseline consists in extracting features through a ConvNet and feeding an SVM with them.

The dataset contains a certain number of photos of the objects of interest. The thing is, I collected any kind of photos: ""good"" ones, where the object is clearly visible, as well as ""bad"" ones, where the quality is awful or the object is something like a blot.

In my test set I use only good photos, in order to have a sound performance evaluation. My doubt is regarding the training set.
Do bad images help or ""confuse"" the classifier? Is it better to have, say, 1000 mixed images per class or just 100 good?

[Here](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf) the authors state that a bigger training set is better than a clearer one. Yet, their thesis is more about errors in labels than image quality.
",8,5,False,self,,,,,
419,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,23,3igwhk,self.MachineLearning,papers on using neural networks in control systems?,https://www.reddit.com/r/MachineLearning/comments/3igwhk/papers_on_using_neural_networks_in_control_systems/,cenofwar,1440598478,I'm going to be working with one of my professors on writing a research paper that has to do with using a neural network to maintain a green house and i was wondering if you guys know of any related works that are not behind a pay wall. i have access to JSTOR. any thing with using neural networks in control systems would be fine. bonus points if it relates to agriculture. thanks to any one that points me in the right direction.,4,5,False,self,,,,,
420,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,23,3igx2m,self.MachineLearning,Possible to apply RNNs to partially rewrite texts?,https://www.reddit.com/r/MachineLearning/comments/3igx2m/possible_to_apply_rnns_to_partially_rewrite_texts/,FiReaNG3L,1440598733,"Hey,

I've been wondering with all the buzz about text-generating (or code, or music) RNNs, would it be possible to build a system to partially rewrite texts, but keep the same meaning? The RNN would change some words / phrase structures for synonyms and ""rewrite"" the text, keeping the same general message but produce a different text?

If you have suggestions of where I should start to play with this idea, or if someone has done it before, let me know!",6,9,False,self,,,,,
421,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,23,3igxoe,multithreaded.stitchfix.com,A Word is Worth a Thousand Vectors,https://www.reddit.com/r/MachineLearning/comments/3igxoe/a_word_is_worth_a_thousand_vectors/,xlegel,1440599016,,6,72,False,http://b.thumbs.redditmedia.com/OTpxhQuW0GbYYpU-p2FZEDDVLCLnD8vC5X1Sq2fRHys.jpg,,,,,
422,MachineLearning,t5_2r3gv,2015-8-26,2015,8,26,23,3ih1dc,self.MachineLearning,Predicting Customers Who Have Moved (Address),https://www.reddit.com/r/MachineLearning/comments/3ih1dc/predicting_customers_who_have_moved_address/,slaw07,1440600690,Has anybody seen any research on how one could predict whether a customer has moved to a different state? I log I.P. addresses when customers log into our site but the location information coming from it are mediocre at best. Are there tell-tale behaviors or some obvious feature that can be utilized to create an accurate moved vs. stayed classifier?,6,2,False,self,,,,,
423,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,0,3ih55d,self.MachineLearning,beginner tutorial or reference code on accumulating the gradients of recurrent networks of variable architecture?,https://www.reddit.com/r/MachineLearning/comments/3ih55d/beginner_tutorial_or_reference_code_on/,[deleted],1440602268,[deleted],1,3,False,default,,,,,
424,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,1,3ihh4s,self.MachineLearning,"new to ML, going through the coursera, need help creating a dataset for a logistic regression (how to turn a short .wav into a training example)",https://www.reddit.com/r/MachineLearning/comments/3ihh4s/new_to_ml_going_through_the_coursera_need_help/,Cezoone,1440607280,"Hello, for practice, I would like to create a supervised learning problem where I record myself saying two different words, many times, and then present a new recording of myself saying one of the words in order to have it classified. 

But how do I turn ""apple.wav"" into a training example with features? I haven't gotten to the point in the coursera which discusses algorithms for auto-selecting features, so I'm just not sure how to go about this part of it.

Thanks for any help!",6,2,False,self,,,,,
425,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,2,3ihqtk,self.MachineLearning,Contractual Modelling with Python - The Shifted-Beta-Geometric Model,https://www.reddit.com/r/MachineLearning/comments/3ihqtk/contractual_modelling_with_python_the/,A_Schwarzenschnitzel,1440611281,"I recently became familiar with the Shifted-Beta-Geometric model by P. Fader and B. Hardie for modelling retention rates in a contractual setting.

However, the way the model was originally presented lacked a couple of things I was interested in (such as the ability to factor in predictor variables on an individual level). So I decided to put some effort into generalizing it and writing a python package to do the job.

If this is a topic that interests you, please checkout the git repo: [ShiftedBetaGeometric](https://github.com/fmfn/ShiftedBetaGeometric), in particular the *examples* folder, and let me know what you think.

Cheers.",1,4,False,self,,,,,
426,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,4,3ii9ug,wired.com,Facebook releases AI personal assistant,https://www.reddit.com/r/MachineLearning/comments/3ii9ug/facebook_releases_ai_personal_assistant/,ma2rten,1440619085,,8,26,False,http://b.thumbs.redditmedia.com/xhHz2GXsH1bJZOwel5QXE-5_8nUK88-OXyr0xNGEOpo.jpg,,,,,
427,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,5,3iia6l,self.MachineLearning,Random Intersection Trees,https://www.reddit.com/r/MachineLearning/comments/3iia6l/random_intersection_trees/,ktpr,1440619226,"When considering ways to discover variable interactions at scale, working from the maximal set down, I came across this 2014 paper on Random Intersection Trees by Shah and Meinshausen. There is a parallelized R implementation. This might be useful to someone.

http://www.jmlr.org/papers/volume15/shah14a/shah14a.pdf",0,1,False,self,,,,,
428,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,5,3iidv2,self.MachineLearning,Is it a good idea: generate sentence vectors using LSTM autoencoders?,https://www.reddit.com/r/MachineLearning/comments/3iidv2/is_it_a_good_idea_generate_sentence_vectors_using/,polytop3,1440620717,"Hi all,

I am new to LSTMs, and was thinking about simple ways to generate vector representations of sentences.

One idea that came to me was to use an encoder-decoder structure similar to [Sequence to Sequence learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).

In that paper, an encoder LSTM takes as input an English sentence, vectorizes it, and then gives it to a decoder LSTM which produces the French translation of the original English sentence.

What I wanted to do was:

1) Have an encoder LSTM take as input a sentence (where each word in the sentence is a vector).

2) The encoder LSTM then produces a vector v, corresponding to the entire input sentence.

3) The decoder LSTM then takes as input v, and tries to reconstruct the original sentence.

The optimization criteria will be the mean squared error between X and Y, where X is our input matrix (each row in this matrix is a word vector corresponding to some word in the original sentence), and Y is  a matrix representing the decoder LSTM output (each row in Y corresponds to the output of the decoder LSTM at a particular time point).

Before investing time coding up this idea in theano, I was wondering what others thought? Is this likely to produce meaningful vector representation of sentences? Or am I trying to pull something out of thin air?",4,8,False,self,,,,,
429,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,6,3iinov,glv.nz,"Why a Mathematician, Statistician, &amp; Machine Learner Solve the Same Problem Differently",https://www.reddit.com/r/MachineLearning/comments/3iinov/why_a_mathematician_statistician_machine_learner/,marcoborracho,1440624779,,24,12,False,http://b.thumbs.redditmedia.com/DbT5aiV2uRgFcleO-VxowX0czHLW3ZlB1epss9xnvnc.jpg,,,,,
430,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,6,3iiocx,arxiv.org,We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs.,https://www.reddit.com/r/MachineLearning/comments/3iiocx/we_introduce_a_model_for_constructing_vector/,downtownslim,1440625055,,0,25,False,default,,,,,
431,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,6,3iipjb,elmorigin.wix.com,The objective of launching this homepage is to present the evidences regarding the tainted origins of the extreme learning machines (ELM),https://www.reddit.com/r/MachineLearning/comments/3iipjb/the_objective_of_launching_this_homepage_is_to/,[deleted],1440625556,[deleted],0,1,False,default,,,,,
432,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,6,3iipuy,self.MachineLearning,NN to replace CRF in KB cleanup,https://www.reddit.com/r/MachineLearning/comments/3iipuy/nn_to_replace_crf_in_kb_cleanup/,spurious_recollectio,1440625702,"[Deepdive](http://deepdive.stanford.edu) uses a CRF to compute probabilities for matching various mentions it extracts from texts to entities and relations  by maximizing the likelihood of the matches subject to user-provided constraints.  E.g. users can enter rules ""company has headquarters"" so if the system finds a mention of Dell and its headquarters it can realize that this is a mention of the company and not the person Dell (this is not the best example but the point is to maximize the joint probability of all observations subject to a set of user defined probabilistic rules or constraints between the observations).

For practical reasons I was wondering if there is any literature on doing something like this with an (R)NN.  I've found some literature on replacing linear chain CRFs with RNNs or on doing this in the context of image segmentation but I don't think those directly apply here.

Sorry for the vagueness of the question but I'm open to any interesting thoughts in this direction.",2,2,False,self,,,,,
433,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,8,3ij6nz,self.MachineLearning,"Confused about why ReLUs show benefit in deep nets, c.f. sigmoidal functions",https://www.reddit.com/r/MachineLearning/comments/3ij6nz/confused_about_why_relus_show_benefit_in_deep/,quaternion,1440633129,"I'm confused about why rectified linear activation functions should show benefits in deep nets over sigmoidal functions. My understanding was that it was precisely the accumulating non-linearities that allowed deeper nets to perform better than a simple 2-layer network. It doesn't make sense to me why sigmoidally-activated nets, with bias weights, wouldn't perform as well as those with ReLUs. Can someone ground my (counter)intuition?",16,14,False,self,,,,,
434,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,15,3ike2l,self.MachineLearning,Ask ML: best practises for lead list prediction,https://www.reddit.com/r/MachineLearning/comments/3ike2l/ask_ml_best_practises_for_lead_list_prediction/,Vulpius,1440655323,"Hi all, I am currently working on a predictive model with the goal of outputting a lead list for a certain products, i.e. a list of n customers which do not yet have a certain product but of whom it is highly likely they'll accept the product when offered by a sales rep. 

An initial idea would be to to build a binary classification model where people having the product are labeled 1 and does who do not carry the label 0. I'd then take the top n people of which the model predicts class 1 but where their true class equals 0 (or alternatively: inspecting the leaf nodes of a decision tree and taking those nodes with &gt;90% positive classes and taking from those the 10% negatives)...

I'm unsure about this approach however. Does anyone have more experience with this application and can provide some pointers? I've thought about applying nearest neighbour methods as well, but deducing a good distance function is tricky considering my large feature space. Thanks!",2,2,False,self,,,,,
435,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,15,3ikevh,self.MachineLearning,Multi Label Using Neural Network,https://www.reddit.com/r/MachineLearning/comments/3ikevh/multi_label_using_neural_network/,tuananh_bk,1440655811,"I want to label some event text data. One Event can have many label. How can I label it with neutral network.

I want to know the form of cost function (loss function). In case of 1 vs all label, I know to use softmax (logistic) function

http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/

how about in multiple labels case?
",2,4,False,self,,,,,
436,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,16,3ikkdn,self.MachineLearning,What data visualizations are you using in your ML process?,https://www.reddit.com/r/MachineLearning/comments/3ikkdn/what_data_visualizations_are_you_using_in_your_ml/,YoungStellarObject,1440659599,"I am especially interested in any approaches to visualize classifier performance for comparison, or feature selection results. 
Also: Do you use custom tools or something widely available?",17,34,False,self,,,,,
437,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,16,3iklpz,arxiv.org,Character-Aware Neural Language Models,https://www.reddit.com/r/MachineLearning/comments/3iklpz/characteraware_neural_language_models/,iori42,1440660585,,0,7,False,default,,,,,
438,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,16,3iklrc,self.MachineLearning,Google seq-to-seq #parameters not adding up,https://www.reddit.com/r/MachineLearning/comments/3iklrc/google_seqtoseq_parameters_not_adding_up/,nnnoob123,1440660611,"Hi all,

I'm trying to understand the google seq-to-seq paper (http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).

They say they use a deep LSTM with 4 layers, and that it contains 384M parameters. They use 1000d word embeddings, and the LSTMs contain 1000 hidden states (and 1000 memory cells). 

They say they have 64M ""pure recurrent connections"" (32M per LSTM), and that they use Grave's formulation from http://arxiv.org/pdf/1308.0850v5.pdf. But this doesn't add up:

1000x1000 for each matrix (because the hidden size is 1000 and so is the word embedding size
11 matrices per LSTM computation (4 of which include the input xt)
4 layers

so, if ""pure recurrent"" meant only those matrices that do not include the input xt, then we'd have 7x1000x1000x4 ~= 32M. And if they did include xt, 11x1000x1000x4 is also not 32M.

Also, they need to have word embeddings for all 160000 + 80000 words, plus a 80000 softmax, which is (160000+80000)x1000 + (1000x80000) parameters right there, which is 320M parameters. Since 384 - 320 = 64, that makes me think that the 32M ""pure"" recurrent connections per LSTM include the input weights. However in Graves' paper there are 11 such matrices, and 11x1000x1000x4 is bigger than 32M, it's 44M. I don't know what LSTM formulation uses 8M (32M/4 bc 4 layers) per layer for a hidden size of 1000 and an input size of 1000, but Graves' one doesnt, and that's what google's paper lists.

Am i misunderstanding something about the paper/model?",2,3,False,self,,,,,
439,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,16,3iklws,arxiv.org,A review of homomorphic encryption and software tools for encrypted statistical machine learning,https://www.reddit.com/r/MachineLearning/comments/3iklws/a_review_of_homomorphic_encryption_and_software/,iori42,1440660716,,0,3,False,default,,,,,
440,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,17,3ikqdy,self.MachineLearning,A Proposal for the realization of an artificial cognitive system for research purposes and applications,https://www.reddit.com/r/MachineLearning/comments/3ikqdy/a_proposal_for_the_realization_of_an_artificial/,Parmeni,1440664107,"I would point out this proposal to build an artificial intelligence system:

https://www.academia.edu/15061257/Proposal_for_the_realization_of_an_artificial_cognitive_system_for_research_purposes_and_applications

In my opinion it is particularly interesting the monography written by the author (first reference, yet unfortunately in Italian). I think it contains innovative ideas particularly interesting.

If you know Italian, or knows someone who knows how to read it, I recommend may notify this work. Personally I think may contain the discovery of the general laws of cognition.
",2,1,False,self,,,,,
441,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,18,3ikv39,self.MachineLearning,Alternatives to CTC-cost for end-to-end speech recognition with RNNs,https://www.reddit.com/r/MachineLearning/comments/3ikv39/alternatives_to_ctccost_for_endtoend_speech/,RichardKurle,1440667886,"When training some form of RNN-model end-to-end, one approach is to use Connectionist Temporal Classification(CTC) to calculate the cost function. One advantage is that the output sequence(phonemes) can have a different length than the input sequence (acoustic features).
What are alternative cost functions / algorithms for end-to-end speech recognition with RNNs?",5,6,False,self,,,,,
442,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,20,3il2sw,apsense.com,An Overview Of Pneumatic Pumps,https://www.reddit.com/r/MachineLearning/comments/3il2sw/an_overview_of_pneumatic_pumps/,jackerfrinandis,1440673731,,0,0,False,default,,,,,
443,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,20,3il6t2,self.MachineLearning,Theano sparse input?,https://www.reddit.com/r/MachineLearning/comments/3il6t2/theano_sparse_input/,Bardelaz,1440676450,Theano only supports [sparse input for CPU computation](http://www.deeplearning.net/software/theano/library/sparse/). I am interested in bag-of-words scenario. Has anyone written a library on top of Theano or something else in python which handles sparse input in the GPU with neural networks?,2,2,False,self,,,,,
444,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,22,3ilhnv,self.MachineLearning,Why is X^T X invertible?,https://www.reddit.com/r/MachineLearning/comments/3ilhnv/why_is_xt_x_invertible/,bagelorder,1440682399,"Let X be an m x n matrix, with the ith row being x^(i), the ith training example. In Andrew Ngs Stanford Engineering Everywhere class he derives that the theta which minimzes J(theta), the squared error function is given by (X^T X)^-1 X^T y

Why should X^T X be invertible?",16,1,False,self,,,,,
445,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,23,3ilo76,youtube.com,AI startup apparently outperforming Deep Q-Learning on Atari games (xpost from r/artificial),https://www.reddit.com/r/MachineLearning/comments/3ilo76/ai_startup_apparently_outperforming_deep/,AmusementPork,1440685428,,44,41,False,http://b.thumbs.redditmedia.com/DZIgWf1kDht4bD2zul27iRm8wJjxvZrHz558AP_28Fo.jpg,,,,,
446,MachineLearning,t5_2r3gv,2015-8-27,2015,8,27,23,3ilr2z,radar.oreilly.com,Bridging the divide: Business users and machine learning experts,https://www.reddit.com/r/MachineLearning/comments/3ilr2z/bridging_the_divide_business_users_and_machine/,gradientflow,1440686742,,0,4,False,http://b.thumbs.redditmedia.com/JikmiMOkEfOdIPCj3j5IO0OcNh2sAQXsm3XiOCJMzng.jpg,,,,,
447,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,0,3iltwd,self.MachineLearning,Suggestions for a neural net approach to real valued regression problem,https://www.reddit.com/r/MachineLearning/comments/3iltwd/suggestions_for_a_neural_net_approach_to_real/,TheSreudianFlip,1440687979,"Hi people, I have a dataset that is composed various attributes, basically image-like data (not really pixel intensities, but can be modeled that way with a color table). Think of it like multiple grids of *m*x*n* pixels, where each pixel is the value of that attribute, and each grid is an image. I'm sorry I can't be more specific, but since the data isn't my own (and I'm just an intern), I can't share it here.

My task is to predict the values of one of the attributes, given some other attributes. So it's sort of a real-valued regression problem, where I predict the value of each pixel (attribute) in a grid. So far, I have used convolutional nets with decent results for a lot of labels, but I need good predictions with very few labeled samples. 

Other people working on this have good results with conditional random fields, but my project is based on neural nets. Any suggestions on how to use neural nets here, or even on how to model the problem differently, will be greatly appreciated!",6,1,False,self,,,,,
448,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,0,3ilu0t,gitxiv.com,Reweighted Wake-Sleep (Yoshua Bengio et al. Paper+Code),https://www.reddit.com/r/MachineLearning/comments/3ilu0t/reweighted_wakesleep_yoshua_bengio_et_al_papercode/,samim23,1440688027,,4,7,False,default,,,,,
449,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,0,3ilu2c,self.MachineLearning,Has anyone tried to completely abstract the RNNs BPTT?,https://www.reddit.com/r/MachineLearning/comments/3ilu2c/has_anyone_tried_to_completely_abstract_the_rnns/,shrimpMasta,1440688048,"All of the libraries I know only offer tools to train RNNs to align two sequences. If you want to go beyond, and align a sequence with a single prediction (many to one), or align a sequence with a sequence of sequences (e.g. for each char you try to predict the 3 next chars instead of just the next one), then you need to unwrap the net and implement all the BPTT yourself. I've recently made a small (didactic) [python library](https://github.com/mpagli/pyNNGraph) implementing sequence to sequence BPTT, before trying to tackle the issue on my own, I was wondering if anyone already tried to work on this ? ",9,5,False,self,,,,,
450,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,0,3im0ad,kickstarter.com,Talking Machines started a kickstarter campaign.,https://www.reddit.com/r/MachineLearning/comments/3im0ad/talking_machines_started_a_kickstarter_campaign/,jsnoek,1440690634,,9,36,False,http://b.thumbs.redditmedia.com/vCdPphDQVV9SwbJJ_rL3WpHwjEQOIGWa_gh5AQG8bZw.jpg,,,,,
451,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,1,3im7cw,colah.github.io,Understanding LSTM Networks,https://www.reddit.com/r/MachineLearning/comments/3im7cw/understanding_lstm_networks/,earslap,1440693537,,6,174,False,http://a.thumbs.redditmedia.com/oca1zSxF_VoqvODtw3BW74MFw7OFNXIGiHaIw-A9KN0.jpg,,,,,
452,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,2,3imfl6,gitxiv.com,Character-Aware Language Models: LSTM language model with CNN over characters (paper+code),https://www.reddit.com/r/MachineLearning/comments/3imfl6/characteraware_language_models_lstm_language/,samim23,1440696849,,0,3,False,default,,,,,
453,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,2,3imgu7,self.MachineLearning,"""Neural Computers"" are now export restricted from the USA. Will this hamper research?",https://www.reddit.com/r/MachineLearning/comments/3imgu7/neural_computers_are_now_export_restricted_from/,londons_explorer,1440697371,"See:

&gt;  Neural computer. (Cat 4)--A computational
device designed or modified to mimic the
behavior of a neuron or a collection of neurons
(i.e., a computational device that is distinguished
by its hardware capability to modulate the
weights and numbers of the interconnections of a
multiplicity of computational components based
on previous data).

See document [here](https://www.bis.doc.gov/index.php/forms-documents/doc_download/1055-ccl4) (Ctrl+F for 'neural').

It would seem that most of what is discussed on this subreddit falls under that definition.   Does anyone have any knowledge of the reason for this restriction, when it was put in place, or if there are significant ""neural computers"" outside published designs/papers?",8,0,False,self,,,,,
454,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,3,3imm68,depiesml.wordpress.com,DL4J Getting Started: One User's Tutorial,https://www.reddit.com/r/MachineLearning/comments/3imm68/dl4j_getting_started_one_users_tutorial/,vonnik,1440699570,,0,1,False,default,,,,,
455,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,3,3imo5d,self.MachineLearning,Classifier ensemble averaging methods?,https://www.reddit.com/r/MachineLearning/comments/3imo5d/classifier_ensemble_averaging_methods/,enasam,1440700370,"Suppose I have fit several distinct classifiers to my data. They each manage about 60-65%ish accuracy.

I assume that they make uncorrelated errors, so I average their results together to get what should be a better prediction than any one classifier. In this case, I average the probability outputs together to obtain my final probability of belonging to a class or not. It seems like my simple average is probably not the optimal method.

So the question is - what is the best method to average the output of the individual classifiers?

I'm guessing that it depends on accuracy - ie I should probably weigh the more accurate models heavier. I'm also guessing that it depends on inter-model correlation - ie two correlated models should be weighted less than a model that produces totally uncorrelated errors. I'm just not too sure how to approach this in a rigorous manner.

Is there established literature or an optimal procedure for a problem like this? Ensemble averaging techniques seem to be what I'm describing, but since it's been a while since I've been in school the most interesting stuff seems to be paywalled.",4,5,False,self,,,,,
456,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,3,3impkf,self.MachineLearning,A question about pixel reading / pattern recognition,https://www.reddit.com/r/MachineLearning/comments/3impkf/a_question_about_pixel_reading_pattern_recognition/,NEED_A_JACKET,1440700940,"Are there solutions out there that could for example detect a person from a video if they had a (theoretically) unlimited number of sample images? IE if you could provide 100,000 images of the person in various lighting conditions, and 100,000 images in similar photos without the person, is there software that could reliably categorise whether the person is in a photo or not? Anywhere close to what a human could do?

Basically my question comes down to; is the limitation on machine learning for pattern recognition often due to a limitation of sample images to ""learn"" from? Or the time it needs to spend learning? Or is the technology / algorithms just not good enough to do it yet? ",3,2,False,self,,,,,
457,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,3,3imr1t,deeplearning4j.org,Sentiment Analysis With Word2Vec and Logistic Regression,https://www.reddit.com/r/MachineLearning/comments/3imr1t/sentiment_analysis_with_word2vec_and_logistic/,vonnik,1440701546,,6,2,False,default,,,,,
458,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,4,3imvfp,computervisiontalks.com,(Tagged and Searchable) Introduction to DIGITS - NVIDIA's Deep Learning GPU based Training System,https://www.reddit.com/r/MachineLearning/comments/3imvfp/tagged_and_searchable_introduction_to_digits/,ojaved,1440703416,,0,2,False,http://b.thumbs.redditmedia.com/N7H42z8O0hv_w80-tilBer5g_kwOJ79UjSk0-pHVPbQ.jpg,,,,,
459,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,4,3imx1m,arxiv.org,A Neural Algorithm of Artistic Style,https://www.reddit.com/r/MachineLearning/comments/3imx1m/a_neural_algorithm_of_artistic_style/,LCDMC,1440704144,,45,117,False,default,,,,,
460,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,4,3imyir,arxiv.org,[1508.06576] A Neural Algorithm of Artistic Style,https://www.reddit.com/r/MachineLearning/comments/3imyir/150806576_a_neural_algorithm_of_artistic_style/,[deleted],1440704783,[deleted],1,2,False,default,,,,,
461,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,5,3in2fr,theplatform.net,Microsoft Extends FPGA Reach To New Deep Learning Approaches,https://www.reddit.com/r/MachineLearning/comments/3in2fr/microsoft_extends_fpga_reach_to_new_deep_learning/,[deleted],1440706443,[deleted],0,1,False,default,,,,,
462,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,5,3in3j9,blog.monkeylearn.com,A Gentle Guide to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3in3j9/a_gentle_guide_to_machine_learning/,wildcodegowrong,1440706916,,0,0,False,default,,,,,
463,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,6,3inerg,reddit.com,What is the best classifier from the ones tested here? : MLQuestions,https://www.reddit.com/r/MachineLearning/comments/3inerg/what_is_the_best_classifier_from_the_ones_tested/,raduqq,1440711664,,0,0,False,http://a.thumbs.redditmedia.com/PDQadCzYX_x1bU3KrYuhTptu6eDdOVVagFG6q_Afyb4.jpg,,,,,
464,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,8,3inq3c,self.MachineLearning,Should I pay $16k for a data science bootcamp? Non-cs eng grad currently in a boring $70k job,https://www.reddit.com/r/MachineLearning/comments/3inq3c/should_i_pay_16k_for_a_data_science_bootcamp/,curious95,1440716703,"The job market in my field is small and locational. Thus I feel lucky to be employed in a place I like, and to be getting the salary I am. However, the work is boring, it's hard to change jobs, and data science work seems so much cooler. 

I've been casually studying data science and machine learning for ~8 months, but it doesn't feel like I'm making significant progress. I want to quit my job and study full time. I have some savings but I'd still struggle to make it past 6 months of no pay check. If I did the bootcamp, I'd defer payment to avoid cutting into my financial runway. 

The bootcamp is three months long, costs $16k, and say their graduates average offer is $90k within six months of graduation. They do not guarantee a job, though they apparently have strong connections to help you along. 

I've thought about slowly starting Master's in CS coursework, and aggressively job hunting as soon as I feel adequately skilled. I know a Master's degree in CS is desired by some employers anyway, and I know the bootcamp credential doesn't count for much. But I wonder if my BS eng degree is enough credential already (assuming I get my skills up to par), and whether I could cut down on opportunity costs by sooner getting a data science salary flowing. 

I think it comes down to risk/reward (one year at the higher salary could more than cover the bootcamp cost), so I'd appreciate any opinions on the data science job market and how I could fare.

TL;DR: Could I be competitive for data science jobs on the west coast after going through a bootcamp? Is a bootcamp ever a better choice than a Master's degree? 



",27,0,False,self,,,,,
465,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,9,3io21q,self.MachineLearning,"Does anyone have/know where I can find for free, without scraping daily Stock Market History",https://www.reddit.com/r/MachineLearning/comments/3io21q/does_anyone_haveknow_where_i_can_find_for_free/,T3ppic,1440722654,"Every place I look via google and common sense either has hard limits on what's available, an api limited to daily price lookups, or has the data but is charging a premium for it. Even just for EOD data. 

Has someone/an open source collective/or something less dubious but also less time consuming that scraping day by day (which would mean for my project waiting at least 7 years to get it all)?

Thanks. ",13,2,False,self,,,,,
466,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,18,3ipfp5,github.com,Snippet: annotate clusters from a correlation matrix with Seaborn and matplotlib,https://www.reddit.com/r/MachineLearning/comments/3ipfp5/snippet_annotate_clusters_from_a_correlation/,cast42,1440752953,,0,6,False,http://b.thumbs.redditmedia.com/6ZxtNncqvyEm8eOTFjRotKgNuBecDtZfUPXog7SyrVY.jpg,,,,,
467,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,20,3ipskb,elmorigin.wix.com,The tainted origins of the extreme learning machines,https://www.reddit.com/r/MachineLearning/comments/3ipskb/the_tainted_origins_of_the_extreme_learning/,downtownslim,1440762754,,0,3,False,default,,,,,
468,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,23,3iq82p,self.MachineLearning,New iOS app given human-like ability but we need your help,https://www.reddit.com/r/MachineLearning/comments/3iq82p/new_ios_app_given_humanlike_ability_but_we_need/,Muggie_App,1440770965,,1,0,False,default,,,,,
469,MachineLearning,t5_2r3gv,2015-8-28,2015,8,28,23,3iqan3,iflscience.com,New Brain-Inspired Chip Can Perform 46 BILLION Synaptic Operations Per Second | IFLScience,https://www.reddit.com/r/MachineLearning/comments/3iqan3/new_braininspired_chip_can_perform_46_billion/,eaperz,1440772117,,22,7,False,http://b.thumbs.redditmedia.com/oH6ESmia6Pl3Ofk-bYVzKfKNpDnGhIXeljPgLxAUvFg.jpg,,,,,
470,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,2,3iqzs9,nbviewer.ipython.org,CNN on MNIST with convolution visualization in Keras,https://www.reddit.com/r/MachineLearning/comments/3iqzs9/cnn_on_mnist_with_convolution_visualization_in/,galapag0,1440782743,,2,8,False,default,,,,,
471,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,3,3ir5h2,self.MachineLearning,1% of human brain in single 19'' rack - The SpiNNaker Project,https://www.reddit.com/r/MachineLearning/comments/3ir5h2/1_of_human_brain_in_single_19_rack_the_spinnaker/,It_Is1-24PM,1440785091,"Computerphile video: https://www.youtube.com/watch?v=2e06C-yUwlc

Human Brain Project: https://www.humanbrainproject.eu/neuromorphic-computing-platform1",2,0,False,self,,,,,
472,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,3,3ir5hd,self.MachineLearning,$$$ Spark Profit  Predict and win! $$$,https://www.reddit.com/r/MachineLearning/comments/3ir5hd/spark_profit_predict_and_win/,john300076,1440785094,,1,0,False,default,,,,,
473,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,6,3irwbs,self.MachineLearning,Neural Nets on regressions rather than classification problems?,https://www.reddit.com/r/MachineLearning/comments/3irwbs/neural_nets_on_regressions_rather_than/,joe056,1440796678,"It seems like most of today's state of the art results where neural networks are concerned involve image classification, protein stuff, and other similarly large feature space classification problems.  I'm more interested in results with these same techniques being applied to a large feature space that is predicting a continuous output.  Does this exist? 

Thanks!
",14,14,False,self,,,,,
474,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,7,3is6sh,self.MachineLearning,ELI5: How are association rules created?,https://www.reddit.com/r/MachineLearning/comments/3is6sh/eli5_how_are_association_rules_created/,tacnobac,1440801312,"I see there are some black boxes where you can plug the data, ask for association rules (eg: product recommendations) and voil.
But, on an ELI5 level, what happens under the hood?",3,16,False,self,,,,,
475,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,11,3it0xc,self.MachineLearning,What approach(s) would be best suited for this juice recipe generator?,https://www.reddit.com/r/MachineLearning/comments/3it0xc/what_approachs_would_be_best_suited_for_this/,jhmacair,1440817045,"I recently got into making my own fruit/vegetable juice. I wanted to try my hand at a fun project to generate new recipes based on a db of existing recipes.

The training data would be as follows: Each recipe contains a list of ingredients and their respective quantities, and an average user rating. E.g.:

----------------------------------------------------------------------

Title : ""Apple Crisp""

Ingredients : 

* Apples : 5
* Celery : 2
* Oranges : 2

Avg. Rating : 4.4

\# of Votes : 29

----------------------------------------------------------------------

What would be a good approach to generate new recipes based on the training data?

How can I adjust the parameters of such approach to optimize for different metrics, e.g.: maximize estimated user rating, or maximize ingredient compatibility?

How could I create an appropriate heuristic for estimating user ratings on generated recipes?

Thank you, any ideas would be greatly appreciated!",6,3,False,self,,,,,
476,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,16,3itpry,self.MachineLearning,Theano-Lights: Deep learning models,https://www.reddit.com/r/MachineLearning/comments/3itpry/theanolights_deep_learning_models/,ivop,1440834644,"Check out the Theano-Lights research framework based on Theano for implementations of the latest Deep learning models and techniques.

The idea is to keep the models concise, close to the mathematical description and not hidden behind layers of object-oriented abstraction. As a result they are easy to read, understand and play with. 

Highlighted models: 

- Virtual adversarial training for permutation-invariant MNIST classification with error rate of 0.65% 

- Word-level language model with LSTM, batch normalization and dropout for a perplexity score on PTB of 75

- Generative models: DRAW with attention, Convolutional VAE and others

Let me know if you have comments or want to see another model implemented. 

https://github.com/Ivaylo-Popov/Theano-Lights",19,33,False,self,,,,,
477,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,16,3itq5q,gitxiv.com,Style Transfer for Headshot Portraits.,https://www.reddit.com/r/MachineLearning/comments/3itq5q/style_transfer_for_headshot_portraits/,samim23,1440835006,,3,14,False,default,,,,,
478,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,20,3iu3l9,self.MachineLearning,Non-cs undergrad wants to pursue masters in Machine Learning/Computer Vision,https://www.reddit.com/r/MachineLearning/comments/3iu3l9/noncs_undergrad_wants_to_pursue_masters_in/,Dragonsarenotreal,1440847904,"I am a chemical engineering undergrad who wishes to pursue masters in Computer Vision/Machine Learning. I have not taken any courses in computer science but I have done an internship where I did some image processing and I work mainly on Computational Fluid Dyanamics. So, my question is, do this happen often in grad schools in US/Canada? If so, how can I improve my chances of getting accepted. ",10,12,False,self,,,,,
479,MachineLearning,t5_2r3gv,2015-8-29,2015,8,29,22,3iubuk,arxiv.org,JavaScript for Distributed Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3iubuk/javascript_for_distributed_deep_learning/,downtownslim,1440853961,,8,20,False,default,,,,,
480,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,5,3ivsu8,self.MachineLearning,[Question] Real-Time analysis : should I choose Spark or Storm,https://www.reddit.com/r/MachineLearning/comments/3ivsu8/question_realtime_analysis_should_i_choose_spark/,___learningCS,1440880813,"I am building machine learning algorithm and I wanna apply on stream of data to make real-time analysis. I have a steam of data coming from machines, and I have a ML Model. Should I use Spark or Storm in this case ? What's more practical when it comes to real-time analysis ?",2,0,False,self,,,,,
481,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,12,3ix3it,stackoverflow.com,Does anyone have any advice for why my Caffe predictions are strange?,https://www.reddit.com/r/MachineLearning/comments/3ix3it/does_anyone_have_any_advice_for_why_my_caffe/,jackbrucesimpson,1440904917,,6,5,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
482,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,14,3ixe5v,self.MachineLearning,Reward AI for overcoming local minima while minimizing arbitrary scoring?,https://www.reddit.com/r/MachineLearning/comments/3ixe5v/reward_ai_for_overcoming_local_minima_while/,Dwood15,1440911484,"I know the common failure points of the ai. I'm also tracking the Ai's series of states every time the ai gets updated. What methods are there to approach rewarding ai for overcoming minima such a way that the ai's performance is scored without arbitrary numbers?

I've read a portion of the 2011 paper on the search for Novelty, but even then the suggested nearest neighbor distance function was arbitrary. AI with either unique series of actions, or a unique place of failure are the ones I want to keep going.

My algorithm is a Neat ANN with a fixed fitness function which has a specific goal the ai is looking to reach. The Ai is in  a 'world' divided into 'regions', each with 'subregions', where the ai needs to overcome obstacles in its path. 

Right now the fitness function just wants the ai to go to the path, so it's not doing any novelty test/reward. 

Neither am I knowledgeable about stats or anything, but so far the best thing I've thought of so far is aggregating the final stats of the genomes to a point, and then testing how far these are from one another, but I'm not sure how I can choose non-arbitrary numbers for that kind of thing. 

I've got the ai's x and y positions of deaths, and my first step has been to generate a heatmap of those states and then reward the ones which make it past the common failure regions more.

Thoughts? Ideas? 

How have you done this yourself?",1,5,False,self,,,,,
483,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,15,3ixiqw,codesachin.wordpress.com,Cross Validation and the Bias-Variance tradeoff (for Dummies),https://www.reddit.com/r/MachineLearning/comments/3ixiqw/cross_validation_and_the_biasvariance_tradeoff/,sachinrjoglekar,1440914997,,0,4,False,http://a.thumbs.redditmedia.com/XUKXloqd9KCHxe2uEkAJHAOmOmJ4pvDhUR1aRq-2ba0.jpg,,,,,
484,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,15,3ixm16,umiacs.umd.edu,Math for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3ixm16/math_for_machine_learning/,[deleted],1440917798,,28,104,False,default,,,,,
485,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,16,3ixo3u,github.com,Caffe2,https://www.reddit.com/r/MachineLearning/comments/3ixo3u/caffe2/,robertsdionne,1440919747,,19,14,False,http://b.thumbs.redditmedia.com/dR8yFuwAcn1bqPDIBvdlkWuEr9YWOpzh618z5XQ95rE.jpg,,,,,
486,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,17,3ixs14,radar.oreilly.com,"Interview with Ilya Sutskever - Unsupervised learning, attention, and other mysteries",https://www.reddit.com/r/MachineLearning/comments/3ixs14/interview_with_ilya_sutskever_unsupervised/,Buck-Nasty,1440923655,,0,22,False,http://b.thumbs.redditmedia.com/lu17oeUBtE4Ty7vozIgkE99CRDlEhxQt0x8WV_DTQOU.jpg,,,,,
487,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,22,3iydnl,self.MachineLearning,Classifier for preference learning,https://www.reddit.com/r/MachineLearning/comments/3iydnl/classifier_for_preference_learning/,genix2011,1440942371,"Does anyone here know of a classifier, that makes label predictions of a test set based on preference relation data?

The training data set would contain relations a &gt; b, which would be 1 if a is preferred to b, otherwise 0. Is there a classifier that can make label predictions of single items (a or b, not as a relation) based on this preferences?

An example:
Let's say the classifier was only trained with [2, 3] &gt; [4, 5]. The classifier should then return P(1 | [2,3]) = 1 and P(0 | [4, 5]) = 0.

Thank you.",2,2,False,self,,,,,
488,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,22,3iyegn,arxiv.org,Using Thought-Provoking Children's Questions to Drive Artificial Intelligence Research,https://www.reddit.com/r/MachineLearning/comments/3iyegn/using_thoughtprovoking_childrens_questions_to/,InaneMembrane,1440942886,,0,2,False,default,,,,,
489,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,23,3iyfyv,self.MachineLearning,Books to learn Machine Learning concepts?,https://www.reddit.com/r/MachineLearning/comments/3iyfyv/books_to_learn_machine_learning_concepts/,FR_STARMER,1440943806,"While Andrew Ng's Coursera course is a good resource, I find that he has trouble explaining the concepts in a concise and understandable way for me. Also, generally when learning CS concepts, I find it super easy to understand when there are many, many programming examples along the way, but alas, I have not found many with Ng.

I believe that going through a book at my own pace and being able to reference it when possible would be the best way for me to learn machine learning. What are the best books out there on machine learning that are informative, but are written practically and use coding examples as the primary method of learning.",4,4,False,self,,,,,
490,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,23,3iygt2,gfycat.com,Neural Art in Action,https://www.reddit.com/r/MachineLearning/comments/3iygt2/neural_art_in_action/,einstein_notation,1440944293,,4,28,False,http://a.thumbs.redditmedia.com/uQfL8XHLzzzq29V_PqEAqh_pMv4j4Eh0DOlLHPNpuj4.jpg,,,,,
491,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,23,3iyik7,self.MachineLearning,What is the state of the art of learning labels from unrelated probability distributions?,https://www.reddit.com/r/MachineLearning/comments/3iyik7/what_is_the_state_of_the_art_of_learning_labels/,TheCocoanaut,1440945258,"Hey there!

So I'm writing my bachelor's thesis currently in which I examine ways to do multi-label classification of movies for their atmospheres. I was just thinking about the different probability distributions I am going to draw from and the humans that  labeled the movies beforehand learned from. Humans obviously didn't had a labeled set of atmospheres of a movie (like spooky, thoughtful) but rather learned these concepts in a totally unrelated domain like inter human communication or media. We then transferred these labelling abilities to movies (by watching and judging them). I am going to learn on metadata obviously, like genre and synopsis.

I was asking myself whether or not this type of learning is being researched right now (learning from an unrelated prob. distribution and applying it to another one) and how the state of the art is. I found domain adaption at wikipedia, but I feel like the article is more about fitting learned concepts to similar ones (not sure though, it is kept pretty short). I guess in the context of deep learning we could develop some deeper understandings of things like spooky and apply them to an entirely new domain (implying we can normalise the underlying representations, right? Or find a common representation space?). Though I fear that this is not going to happen anytime soon I'd like to hear about your thoughts or any hints on previously done research! (I won't need it for my thesis but I'm really interested in this! My thesis is going to be my stepping stone to a M.Sc. of Artificial Intelligence.)

I'd love to hear your thoughts or hints!",6,4,False,self,,,,,
492,MachineLearning,t5_2r3gv,2015-8-30,2015,8,30,23,3iyj77,self.MachineLearning,How to train neural network to consider context?,https://www.reddit.com/r/MachineLearning/comments/3iyj77/how_to_train_neural_network_to_consider_context/,michal_sustr,1440945610,"I'm trying to train a neural network to be able to distinguish context in data. If I have a generated input such as in http://postimg.org/image/gahwhqcct/ . I would like to train it give an output such as in http://postimg.org/image/ohehn1rt9/ . In other words, I want it to be able to train open and close sequences in which it will follow the data. 

I have tried using Keras with following set-up without much success in distinguishing the context:

    model = Sequential()
    model.add(LSTM(1, 100, return_sequences=False))
    model.add(Dropout(0.5))
    model.add(Dense(100, 3, init='uniform'))
    model.add(Activation('relu'))

Any ideas what to do?
",1,1,False,self,,,,,
493,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,1,3iyvdw,self.MachineLearning,When is overfitting actually bad overfitting?,https://www.reddit.com/r/MachineLearning/comments/3iyvdw/when_is_overfitting_actually_bad_overfitting/,raduqq,1440951787,"I've recently trained some classifiers for topic classification and they seem to be doing pretty good. I'm also training and testing them with a variable number of features (ex: I'm starting with 100 features, training them, evaluating and then reducing the features to 95, training, evaluating etc.).

Sometimes, for some number of features I get 100% correct evaluation and for some I get less than 100% (85-95% range).

A friend of mine is saying that when it's 100% it's overfitting and this is never good. Although I have a resonably small training / test set (trainig = 300 instances, test = 20 instances), when I'm testing with other instances I might not get 100%, which I think proves that my classifier does not overfit, but still he is saying that 100% is never good.

I'm assuming that getting consistent 100% correct evaluation is worse than getting in some scenarios. Am I right, or am I wrong? Or it depends?",13,0,False,self,,,,,
494,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,3,3iz8j1,youtube.com,Deep Neural Network Learns Van Gogh's Art,https://www.reddit.com/r/MachineLearning/comments/3iz8j1/deep_neural_network_learns_van_goghs_art/,hockiklocki,1440957831,,32,129,False,http://b.thumbs.redditmedia.com/yqZr2AQGwPZLDvdr1C-PEuUk7RqhJknfoZURSgJr8no.jpg,,,,,
495,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,3,3iz8y9,self.MachineLearning,Hoeffding's Inequality can be &gt; 1?,https://www.reddit.com/r/MachineLearning/comments/3iz8y9/hoeffdings_inequality_can_be_1/,rlabbe,1440958011,"I'm watching the Cal Tech ML course, video 2. We are given Hoeffding's Inequality as  P(abs(nu - mu &gt; epsilon)) &lt;= 2e^-2N\epsilon^2

So, let N=1000, epsilon = 0.01. This gives a value of 1.637 on the r.h.s. This is larger than one, and thus not a probability. I've googled around, found the same equation, but no discussion of the possibility of the value exceeding one. I have not studied the derivation of the inequality, which I admit might provide me insight.

It's an *inequality* so perhaps it doesn't matter. With the sample size N and acceptable error epsilon given should I just conclude that it is 100% likely that abs(nu - mu) &lt; epsilon?
",3,4,False,self,,,,,
496,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,3,3izb4y,youtu.be,PyData - Josh Bloom: Keynote - A Systems View of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3izb4y/pydata_josh_bloom_keynote_a_systems_view_of/,MrZu,1440958999,,0,8,False,http://b.thumbs.redditmedia.com/b8wkt3vdwmEIvdR3qyZuZXlDToSajwsfOZ3GP7Wk1lA.jpg,,,,,
497,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,3,3izdvx,i.imgur.com,Google is literally Satan...,https://www.reddit.com/r/MachineLearning/comments/3izdvx/google_is_literally_satan/,[deleted],1440960186,[deleted],0,0,False,default,,,,,
498,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,4,3izkk7,github.com,[nolearn/lasagne/theano] dimension mis-matched after a small change of a working CNN architecture. Any ideas why this error occurs? I simply can't figure it out,https://www.reddit.com/r/MachineLearning/comments/3izkk7/nolearnlasagnetheano_dimension_mismatched_after_a/,qwertz_guy,1440963016,,0,0,False,http://b.thumbs.redditmedia.com/UoqF2EONQHbAgOvaPXUkWdSunsEs-x7emTcTRgmnawU.jpg,,,,,
499,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,6,3izxwn,youtube.com,Hacking Roller Coaster Tycoon with Genetic Algorithms and Go,https://www.reddit.com/r/MachineLearning/comments/3izxwn/hacking_roller_coaster_tycoon_with_genetic/,FR_STARMER,1440968808,,12,33,False,http://b.thumbs.redditmedia.com/cc_6jY8zapi5iD8qUv4F1Qdn9cwY49m8DvSGn4ls0FE.jpg,,,,,
500,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,6,3izyir,adultsexdating.worldoftanksmody097.ru,[FREE SEX] Free Adult Service! I am Lana! I wanna sex-sex-sex! Free Sign UP!,https://www.reddit.com/r/MachineLearning/comments/3izyir/free_sex_free_adult_service_i_am_lana_i_wanna/,fratthei4427,1440969060,,0,0,False,default,,,,,
501,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,6,3j01c5,self.MachineLearning,Implementations of LSTM networks?,https://www.reddit.com/r/MachineLearning/comments/3j01c5/implementations_of_lstm_networks/,pumping_lemmon,1440970117,"I've checked GitXiv and ArchXiv for everything related. I was wondering if you know of, or can speculate of, better implementations of LSTM networks?",6,6,False,self,,,,,
502,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,7,3j0e7m,self.MachineLearning,Ratio of model size to training set size; how can a model be interpreted as a compressed training set?,https://www.reddit.com/r/MachineLearning/comments/3j0e7m/ratio_of_model_size_to_training_set_size_how_can/,NovaRom,1440975517,"Can a deep model be analysed in terms of compressibility? Is there any theory about relations between compressed training data and generalisation error? Can a model be interpreted as a compressed representation of training data, and if yes, then how a quantitative analysis can help to evaluate model's quality (its generalisation potential)?

We normally try to get a model which is smaller than its training set; but how much smaller is enough and why is there Dropout-like methods of regularisation? Does it make sense to remember whole training data to get ""the best possible"" generalisation accuracy then?",5,2,False,self,,,,,
503,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,9,3j0up0,self.MachineLearning,Can Anyone Recommend some Good Resources to Learn about Structured Learning?,https://www.reddit.com/r/MachineLearning/comments/3j0up0/can_anyone_recommend_some_good_resources_to_learn/,simonhughes22,1440982462," I have a problem where I need to learn a simple graph structure from a text document. I have tinkered with PyStruct and also failed to get the new structured learning module of VW working. Can anyone recommend some additional good books, tutorials and\or code resources for structured learning? I am thinking of implementing SEARN and also the Global Linear Models described in Michael Collins' Coursera NLP class. I would like to learn about some other approaches however that go beyond a simple CRF.",12,9,False,self,,,,,
504,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,11,3j15jh,raw.githubusercontent.com,"Number of A.I. publications by country (dataviz) - China publishes a lot, North America is very ""productive""",https://www.reddit.com/r/MachineLearning/comments/3j15jh/number_of_ai_publications_by_country_dataviz/,rasbt,1440987112,,24,31,False,http://b.thumbs.redditmedia.com/_r_Z6XBGb54pKDBspVldhLW0ByNdaViiCQW8jMMDELI.jpg,,,,,
505,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,11,3j1b9p,stackoverflow.com,Help with neural net back propagation,https://www.reddit.com/r/MachineLearning/comments/3j1b9p/help_with_neural_net_back_propagation/,anon6546549687987,1440989691,,0,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
506,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,16,3j23gu,jegindustries.com,Business - Why Go For Slitting Rewinding Machine,https://www.reddit.com/r/MachineLearning/comments/3j23gu/business_why_go_for_slitting_rewinding_machine/,sonalbisht101,1441005679,,1,1,False,default,,,,,
507,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,17,3j28za,cloudacademy.com,Let's Understand Amazon Machine Learning (Course),https://www.reddit.com/r/MachineLearning/comments/3j28za/lets_understand_amazon_machine_learning_course/,leonardofed,1441009745,,0,3,False,http://b.thumbs.redditmedia.com/ABJxlhxI5NghdeENlri3v0zxSoHNjCV_ueWY1XqVfqs.jpg,,,,,
508,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,17,3j295y,i.imgur.com,"Neural algorithm that ""paints"" photos based on the style of a given painting [ x-post /r/pics ]",https://www.reddit.com/r/MachineLearning/comments/3j295y/neural_algorithm_that_paints_photos_based_on_the/,theirfReddit,1441009892,,27,448,False,http://b.thumbs.redditmedia.com/wtVpSHnpCgjPCraC3Bk0OiXTQpX1EDd-PnIg_42q3NM.jpg,,,,,
509,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,18,3j2cie,nbviewer.ipython.org,http://nbviewer.ipython.org/github/psinger/notebooks/blob/master/bayesian_correlation_pymc.ipynb,https://www.reddit.com/r/MachineLearning/comments/3j2cie/httpnbvieweripythonorggithubpsingernotebooksblobma/,[deleted],1441012136,[deleted],0,1,False,default,,,,,
510,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,18,3j2clq,nbviewer.ipython.org,Bayesian Correlation with PyMC,https://www.reddit.com/r/MachineLearning/comments/3j2clq/bayesian_correlation_with_pymc/,cast42,1441012208,,0,7,False,default,,,,,
511,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,18,3j2fa4,gitxiv.com,A Implementation of the Neural Algorithm of Artistic Style,https://www.reddit.com/r/MachineLearning/comments/3j2fa4/a_implementation_of_the_neural_algorithm_of/,samim23,1441014405,,3,21,False,default,,,,,
512,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,19,3j2hnw,self.MachineLearning,X-ray manufacturer India,https://www.reddit.com/r/MachineLearning/comments/3j2hnw/xray_manufacturer_india/,paulrobinson00,1441016387,,0,1,False,default,,,,,
513,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,19,3j2kvs,self.MachineLearning,What is currently the best method for deep unsupervised feature learning for image classification?,https://www.reddit.com/r/MachineLearning/comments/3j2kvs/what_is_currently_the_best_method_for_deep/,SunnyJapan,1441018784,"If I were to pretrain the neural network on unsupervised data, and then only train the final layer (no finetuning), which unsupervised feature learning method would give me the best results?",1,2,False,self,,,,,
514,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,20,3j2o0u,nuit-blanche.blogspot.ch,Unsupervised Feature Selection on Data Streams &amp; Streaming Anomaly Detection Using Randomized Matrix Sketching (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/3j2o0u/unsupervised_feature_selection_on_data_streams/,compsens,1441020932,,0,2,False,http://a.thumbs.redditmedia.com/mHh8U7txAI5N7eoUosndrm_KtXKRJUfINDr1olZ1gX8.jpg,,,,,
515,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,21,3j2rpw,fastml.com,Evaluating recommender systems,https://www.reddit.com/r/MachineLearning/comments/3j2rpw/evaluating_recommender_systems/,Foxtr0t,1441023316,,0,8,False,http://b.thumbs.redditmedia.com/CMxPC8HZRKdf5ITJsIlz6X6MTDBrJ-JAfumciIClZAw.jpg,,,,,
516,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,22,3j31t6,self.MachineLearning,"Absolute beginner, got 75% accuracy for customer churn prediction based on visits at a location, did I do it right?",https://www.reddit.com/r/MachineLearning/comments/3j31t6/absolute_beginner_got_75_accuracy_for_customer/,bLaind2,1441028649,"**TL;DR: explaining a process of getting the results, maybe an inspiration and or / starting point for others, if I got it right (which I doubt, but that's why I got some questions in here :)**

Many of our customers would love to have customer churn prediction built into their product. So with limited knowledge, onto the project I went. I got some preliminary results, but I'd have some (probably basic) questions for you pro's.

The customers are members who visit the place frequently (1-30 times a month). We have data about customer visits, and the history of purchases made. First, I created a script to fetch customer data:

    for (account in accounts) {
      // from the beginning of customership onwards until the customer remains a customer
      for (month in [1..36]) {
        calculateVector(); 
      }
    }

so for each customer, I calculated a data vector for each month beginning from their customership. The vector contained the amount of visits, and as a last column a boolean flag whether customer was still a customer three months after. So for a customer, who had been to the place in January 3 times, February 4 times and March 5 times, April 2 times, May 1 times, and quit then, the vectors would be like:

    [3,0,0,0,0,0,1] // jan; still customer in april (3months)
    [4,3,0,0,0,0,1] // feb: still customer in may
    [5,4,3,0,0,0,0] // mar: won't be customer anymore in june
    [2,5,4,3,0,0,0] // apr
    [1,2,5,4,3,0,0] // may
    [0,1,2,5,4,3,0] // jun
... and so on, eventually being a lot of zeroes. This is a sample of customer with not very long history, but most customers are with longer history.

Values are normalized between -1 and 1.

Dataset contains 50.000 items, about 50-50 for the both classes (1, 0). I shuffled the dataset, and split it into 70% of training data and 30% validation data.

I quickly got into 75% validation accuracy, and so far checking the customers by probability the results seem to be correct. Although, there's some seasonal fluctuation in the usage, and now the dataset doesn't take this into account, predicting a lot of customers for ending customership even though they were just having summer holidays.

Parameters: input vector size: 36 (months), one hidden layer with 100 units, learning rate 0,1 decay 0,01 momentum 0,9, loss function mse, batch size 32, 100 epochs

However, here's some questions that arose:

* What do you think about my process of getting the data vectors?
* Does it sound reasonable that the highest accuracy is already gained in first or second epoch? Or could it be possible that I hit a local minimum? Somehow I always thought that you need to train at least for minutes?
* Loss decrease stalls very soon. I understand this means that the model is not learning anymore, and I've hit either local minimum or global minimum?
* Is there any way to know whether I'm hitting local or global minimum? I guess with this dataset a 75% accuracy is already good one (can't predict if people move to other city, etc)
* Any ideas why does the accuracy decrease from 75% to 65% if I use a week as a step size instead of a month? The vector size is 156 (3y*52 weeks). By common sense, having more accurate data should provide highe
r accuracy?
* Also, the accuracy decreases to mere 60% if I add the customer purchases to the vector, any ideas why is this? So in this case I'd have a vector of [visits,purchases,class]? e.g. [2,5,4,3,0,0,0,200,0,0,1] wher
e 2,5,4,3 are visits and 200 is purchase amount. I normalized each column separately. The dataset size was 360.000 items.
* Should it have any effect if I change ""non-customer"" values from zero to -1? E.g. now the first data set has [3,0,0,0....] where zeroes would indicate no visits, but the customer actually started in the first
month (with 3 visits).
* Any other data that you'd suggest me to try for this model? Maybe adding a week-number to the beginning of the vector (so that seasonal fluctuations are considered too?), maybe gender, maybe the purchased product type (single-time visits or monthly card)?
* Oh, one more: anyone aware of any IRC channels where machine learning people hang out?

I used an article called ""Using Deep Learning to Predict Customer Churn in a Mobile Telecommunication Network"" as an inspiration, url: http://wiseathena.com/pdf/wa_dl.pdf

Thanks!
",3,10,False,self,,,,,
517,MachineLearning,t5_2r3gv,2015-8-31,2015,8,31,22,3j32j3,self.MachineLearning,OpenCV - Annotation tool for quickly building cascade classifiers,https://www.reddit.com/r/MachineLearning/comments/3j32j3/opencv_annotation_tool_for_quickly_building/,__null__,1441028984,"I would like to ask whether there is a graphical Annotation tool for quickly building cascade classifiers with OpenCV, that is similar to trainingImageLabeler for matlab",0,2,False,self,,,,,
