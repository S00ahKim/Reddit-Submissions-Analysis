,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,9,9t4rch,appdevelopermagazine.com,How crowdsourced data can help you construct great data sets to drive your machine learning and AI platforms,https://www.reddit.com/r/MachineLearning/comments/9t4rch/how_crowdsourced_data_can_help_you_construct/,reimmoriks,1541032954,,0,1,False,default,,,,,
1,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,10,9t4y9a,self.MachineLearning,ML is easy!,https://www.reddit.com/r/MachineLearning/comments/9t4y9a/ml_is_easy/,flame_and_void,1541034416,[removed],0,1,False,self,,,,,
2,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,10,9t51r6,self.MachineLearning,free Big Data event @ Stanford,https://www.reddit.com/r/MachineLearning/comments/9t51r6/free_big_data_event_stanford/,Jessica-100,1541035172,[removed],0,1,False,self,,,,,
3,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,10,9t54tt,self.MachineLearning,RNN in terms of the parity problem.,https://www.reddit.com/r/MachineLearning/comments/9t54tt/rnn_in_terms_of_the_parity_problem/,octaveAbove,1541035805,[removed],0,1,False,self,,,,,
4,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,10,9t567m,self.MachineLearning,free data science event @ Stanford,https://www.reddit.com/r/MachineLearning/comments/9t567m/free_data_science_event_stanford/,Jessica-100,1541036091,[removed],0,1,False,self,,,,,
5,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,12,9t61iv,belikeyogi.wordpress.com,How do I learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/9t61iv/how_do_i_learn_machine_learning/,vaibhavraut77,1541043216,,0,1,False,default,,,,,
6,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,14,9t6t1d,heartbeat.fritz.ai,Comparing Text Recognition SDKs on iOS Using Delta Firebases ML Kit vs TesseractOCR on iOS devices,https://www.reddit.com/r/MachineLearning/comments/9t6t1d/comparing_text_recognition_sdks_on_ios_using/,zsajjad,1541050685,,0,1,False,https://b.thumbs.redditmedia.com/M9J51UL0CAxL9Hdsk9scjMqQnHffUSoWhvybYD3XF-M.jpg,,,,,
7,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,15,9t70w3,self.MachineLearning,[Q]Why are images created by GAN sharper than images by VAE?,https://www.reddit.com/r/MachineLearning/comments/9t70w3/qwhy_are_images_created_by_gan_sharper_than/,finallyifoundvalidUN,1541053132,[removed],0,1,False,self,,,,,
8,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,15,9t712f,self.MachineLearning,[D]Why are images created by GAN sharper than images by VAE?,https://www.reddit.com/r/MachineLearning/comments/9t712f/dwhy_are_images_created_by_gan_sharper_than/,finallyifoundvalidUN,1541053190,,30,1,False,self,,,,,
9,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,15,9t76cw,self.datasets,"[D] Census bureau intends to add differential privacy ""noise"" to public census data in the US",https://www.reddit.com/r/MachineLearning/comments/9t76cw/d_census_bureau_intends_to_add_differential/,incompetentrobot,1541054946,,0,1,False,default,,,,,
10,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,16,9t78pi,youtu.be,Dynamic Programming in Reinforcement Learning simplified,https://www.reddit.com/r/MachineLearning/comments/9t78pi/dynamic_programming_in_reinforcement_learning/,vector_machines,1541055741,,0,1,False,default,,,,,
11,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,16,9t79g0,youtu.be,How Pipeline in Machine Learning works?,https://www.reddit.com/r/MachineLearning/comments/9t79g0/how_pipeline_in_machine_learning_works/,adarsh_adg,1541055972,,0,1,False,https://b.thumbs.redditmedia.com/0ZQz7SdlAROrjSUIGe0krH8AgomLjaCJ9u7qnVBVhyg.jpg,,,,,
12,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,16,9t7ehw,miguelgfierro.com,A Gentle Explanation of Dimensionality Reduction with t-SNE,https://www.reddit.com/r/MachineLearning/comments/9t7ehw/a_gentle_explanation_of_dimensionality_reduction/,hoaphumanoid,1541057729,,1,1,False,default,,,,,
13,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,16,9t7ffm,miguelgfierro.com,[P] A Gentle Explanation of Dimensionality Reduction with t-SNE,https://www.reddit.com/r/MachineLearning/comments/9t7ffm/p_a_gentle_explanation_of_dimensionality/,hoaphumanoid,1541058044,,1,1,False,https://b.thumbs.redditmedia.com/UrdzG7DvVq5e78gz9dYOt4j8OlVqsfD7JnI9VjYG93E.jpg,,,,,
14,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,16,9t7i6m,self.MachineLearning,Running LSTM on hardware with restriction,https://www.reddit.com/r/MachineLearning/comments/9t7i6m/running_lstm_on_hardware_with_restriction/,rorrykeys,1541059000,[removed],0,1,False,self,,,,,
15,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,17,9t7jvs,self.MachineLearning,[R] Running LSTM on hardware with limited capabilities,https://www.reddit.com/r/MachineLearning/comments/9t7jvs/r_running_lstm_on_hardware_with_limited/,rorrykeys,1541059599,"I am trying to look for a possible implementation of LSTM in a hardware where operations are limited. The key idea is that, I do not need to train the model in the hardware, but use the hardware only for LSTM inference. Therefore, I can train LSTM offline with no hardware restriction. However, the hardware currently supports only addition, subtraction and bitwise operations like bitshifts. Also, the hardware does not support floating points. 

&amp;#x200B;

BinaryNet\[1\] is perfect for such a hardware as all weights are represented as 0 or 1, dot products are replaced with XNOR and POPCOUNT, and activation functions are approximated. However, existing literature \[2\]  has shown that applying the idea of binarynet on LSTM is not that accurate.

&amp;#x200B;

Given that I do not need to perform training of LSTM in hardware, is there a way I can approximate the weight, the activations for all four gates in LSTM, with restricted operations(the ones which are used in binarynet)?

&amp;#x200B;

\[1\] Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 Courbariaux et al. [https://arxiv.org/abs/1602.02830](https://arxiv.org/abs/1602.02830)

\[2\]  LOSS-AWARE BINARIZATION OF DEEP NETWORKS Hou et al. [https://arxiv.org/pdf/1611.01600.pdf](https://arxiv.org/pdf/1611.01600.pdf)

&amp;#x200B;

&amp;#x200B;

PS: I deleted my old post, as I did not put a proper tag. ",4,1,False,self,,,,,
16,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,17,9t7mrt,self.MachineLearning,"[D] Full version of AMA with V, Iglovikov (Kaggle Grand Master) is released",https://www.reddit.com/r/MachineLearning/comments/9t7mrt/d_full_version_of_ama_with_v_iglovikov_kaggle/,Relaxation_Time,1541060673,"Dear Redditors, 

&amp;#x200B;

Thank you so much for taking part in the AMA. We have collected a lot of questions at different platforms and now, finally, we are ready to present you a full version of AMA.  


Please, follow \[the link\]([https://towardsdatascience.com/ask-me-anything-session-with-a-kaggle-grandmaster-vladimir-i-iglovikov-942ad6a06acd](https://towardsdatascience.com/ask-me-anything-session-with-a-kaggle-grandmaster-vladimir-i-iglovikov-942ad6a06acd)) to read it.

&amp;#x200B;

Peace. ",2,1,False,self,,,,,
17,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,18,9t7skc,self.MachineLearning,"Projected path to become a entry level data scientist at any of the big 4- ( Google,Facebook, Microsoft,Amazon) suggestions.",https://www.reddit.com/r/MachineLearning/comments/9t7skc/projected_path_to_become_a_entry_level_data/,johnreese421,1541062875,[removed],0,1,False,self,,,,,
18,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,18,9t7ta0,self.MachineLearning,Free trial of GPUs P102-100,https://www.reddit.com/r/MachineLearning/comments/9t7ta0/free_trial_of_gpus_p102100/,extentions,1541063128,[removed],0,1,False,self,,,,,
19,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,18,9t7vyp,slideserve.com,Career Opportunities in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9t7vyp/career_opportunities_in_machine_learning/,credibll,1541064041,,0,1,False,default,,,,,
20,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,19,9t88jj,arxiv.org,[R] You May Not Need Attention (summary + PyTorch code in comments),https://www.reddit.com/r/MachineLearning/comments/9t88jj/r_you_may_not_need_attention_summary_pytorch_code/,ofirpress,1541068197,,47,1,False,default,,,,,
21,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,19,9t8a6i,dimensionless.in,Spam Detection with Natural Language Processing - Part 3,https://www.reddit.com/r/MachineLearning/comments/9t8a6i/spam_detection_with_natural_language_processing/,divya2018,1541068679,,1,1,False,https://b.thumbs.redditmedia.com/LHCrqVQ4RZ2Z6GE9_rfkPHpHUvU8waNZdHEy8_p_-NE.jpg,,,,,
22,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,19,9t8bq8,self.MachineLearning,Global classification of small network structures?,https://www.reddit.com/r/MachineLearning/comments/9t8bq8/global_classification_of_small_network_structures/,furioterzapi,1541069129,[removed],0,1,False,self,,,,,
23,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,20,9t8qm0,self.MachineLearning,Reading comprehension api,https://www.reddit.com/r/MachineLearning/comments/9t8qm0/reading_comprehension_api/,Desney,1541073311,[removed],0,1,False,self,,,,,
24,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,21,9t90vp,self.computervision,[R] Image Recognition with No Prior Information,https://www.reddit.com/r/MachineLearning/comments/9t90vp/r_image_recognition_with_no_prior_information/,Feynmanfan85,1541075816,,0,1,False,default,,,,,
25,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,21,9t9141,self.MachineLearning,Popularity of Adaptive Resonance Theory (ART),https://www.reddit.com/r/MachineLearning/comments/9t9141/popularity_of_adaptive_resonance_theory_art/,void_gear,1541075875,[removed],0,1,False,self,,,,,
26,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,21,9t92s5,self.MachineLearning,"[D] Clothes Categorizer with Deep Learning [OpenCV, Tensorflow, Keras]",https://www.reddit.com/r/MachineLearning/comments/9t92s5/d_clothes_categorizer_with_deep_learning_opencv/,D3ntrax,1541076273,"We're doing research stuff for our final year project. Our project subject is; Artificial intelligence and Image Processing

Name: ""Clothes Categorizer"".  


So, what is Clothes Categorizer?  
Clothes Categorizer is a simple clothes detector with some arguments. It categorizes the clothes that people wear. As an example:  
\- Boy -&gt; Clothing -&gt; Sweaters -&gt; Cardigan  
Problem: There can be multiple clothing type and sub-type. What I mean?  
Sweater have 3 sub-type: Pullover, Cardigan, Vests; Each variant has a color: Red, Green, Blue, etc.

Real Question

We do not want to make separate datasets for all types, sub-types and colors. We want to enter the tag by selecting the Sweaters on the picture. Example:

Image-1: Type: Sweater, Sub-Type: Cardigan, Color: Red  
Image-2: Type: Underwear, Sub-Type: Boxer, Color: Black  
Image-3: Type: Short, Sub-Type: Mini, Color: Green

...

In the Training phase, it must first look at the Type. After Type learned, now it knows it's a Sweater. It's now passed to the sub-type tag. After Sub-Type learned, now it knows it's a Cardigan Sweater. Then passed the color learning phase. Now it knows exactly everything Sweater combinations.

[https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)

Using labelimg we can only give 1 tag.  How can we teach the AI software the sub-types after learning that it is Sweater?

So we want to teach it more than one thing about clothes on a single picture.   


We also found some usefull resources:  
[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)  
[https://medium.com/deep-learning-turkey/deep-learning-lab-episode-4-deep-fashion-2df9e15a63e1](https://medium.com/deep-learning-turkey/deep-learning-lab-episode-4-deep-fashion-2df9e15a63e1)

&amp;#x200B;

But they're just teaching it what clothes are. We need to teach extra features about the clothes as I said above. If we create a separate dataset for earch combination like (Sweater-Cardigan, Sweater-Pullover, Sweater-Vests), the number of dataset count will increase for each property we added:

  
Sweater: 5k images (including sub-types tag: Cardigan, Pullover, Vests) (also including color tag: Red, Green, Blue, etc.)

vs.

Sweater-Cardigan-Red: 5k images

Sweater-Cardigan-Green: 5k images

Sweater-Cardigan-Blue: 5k images

&amp;#x200B;

Sweater-Pullover-Red: 5k images

Sweater-Pullover-Green: 5k images

Sweater-Pullover-Blue: 5k images  


Sweater-Vests-Red: 5k images

Sweater-Vests-Green: 5k images

Sweater-Vests-Blue: 5k images

&amp;#x200B;

It doesn't make any sense in this way. Does anyone have advice, suggestion and experience about this? Thank you sooo much!  
",5,1,False,self,,,,,
27,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,22,9t97pt,self.MachineLearning,How Autonomous Vehicles Take Your Job and Give You a Life,https://www.reddit.com/r/MachineLearning/comments/9t97pt/how_autonomous_vehicles_take_your_job_and_give/,The_Syndicate_VC,1541077398,[removed],0,1,False,self,,,,,
28,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,22,9t990v,blog.openai.com,[R] Reinforcement Learning with Prediction-Based Rewards &lt;- RL agent by Open AI learns to play Mario only by curiosity about how the next level looks like,https://www.reddit.com/r/MachineLearning/comments/9t990v/r_reinforcement_learning_with_predictionbased/,TwoUpper,1541077673,,0,1,False,https://b.thumbs.redditmedia.com/N2jnkcYlDsJpC8AF-6kRSaxYAY75-fiqYAWkEICr8to.jpg,,,,,
29,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,22,9t9dao,arxiv.org,[R] Towards Understanding Linear Word Analogies (why 'king' - 'man' + 'woman' = 'queen'),https://www.reddit.com/r/MachineLearning/comments/9t9dao/r_towards_understanding_linear_word_analogies_why/,kawin_e,1541078609,,1,1,False,default,,,,,
30,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,22,9t9kp2,ai.googleblog.com,[N] [R] Google at EMNLP 2018,https://www.reddit.com/r/MachineLearning/comments/9t9kp2/n_r_google_at_emnlp_2018/,baahalex,1541080170,,0,1,False,default,,,,,
31,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,22,9t9lm1,github.com,"[P] Introducing DeOldify: A Progressive, Self-Attention GAN based image colorization/restoration project",https://www.reddit.com/r/MachineLearning/comments/9t9lm1/p_introducing_deoldify_a_progressive/,MyMomSaysImHot,1541080372,,1,1,False,default,,,,,
32,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,23,9t9no4,self.MachineLearning,[D] Any papers on converting camera coordinates to world coordinates?,https://www.reddit.com/r/MachineLearning/comments/9t9no4/d_any_papers_on_converting_camera_coordinates_to/,cbsudux,1541080818,,2,1,False,self,,,,,
33,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,23,9t9z6i,i.imgur.com,Googlers when offered PyTorch (before they block you on Twitter),https://www.reddit.com/r/MachineLearning/comments/9t9z6i/googlers_when_offered_pytorch_before_they_block/,reservedsparrow,1541083096,,0,1,False,https://b.thumbs.redditmedia.com/7LluTCAEJPoJSxPSCHq7mf6ExqM2EBzlvcLd4793iKw.jpg,,,,,
34,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,23,9t9zlo,i.imgur.com,"[D] Googlers, right after PyTorch's release and right before they blocked you on Twitter",https://www.reddit.com/r/MachineLearning/comments/9t9zlo/d_googlers_right_after_pytorchs_release_and_right/,reservedsparrow,1541083168,,0,1,False,https://b.thumbs.redditmedia.com/7LluTCAEJPoJSxPSCHq7mf6ExqM2EBzlvcLd4793iKw.jpg,,,,,
35,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,23,9ta0dj,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Nov. 1, 2018",https://www.reddit.com/r/MachineLearning/comments/9ta0dj/n_weekly_machine_learning_opensource_roundup_nov/,stkim1,1541083333,,0,1,False,default,,,,,
36,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,23,9ta4dz,medium.com,[N] A Conversation With Quoc Le: The AI Expert Behind Google AutoML,https://www.reddit.com/r/MachineLearning/comments/9ta4dz/n_a_conversation_with_quoc_le_the_ai_expert/,gwen0927,1541084096,,0,1,False,https://b.thumbs.redditmedia.com/6thMRge9O15BQRGi5hX_S2LaTfXcnCY5BNsbdmaZmhI.jpg,,,,,
37,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,23,9ta5jh,eleks.com,How is Robotic Process Automation (RPA) Transforming Enterprises?,https://www.reddit.com/r/MachineLearning/comments/9ta5jh/how_is_robotic_process_automation_rpa/,Victor_Stakh,1541084315,,0,1,False,https://b.thumbs.redditmedia.com/uWk5Yky4PxCGwbGreNxNZPiyXW7n1TX8zTAkRFzJuNc.jpg,,,,,
38,MachineLearning,t5_2r3gv,2018-11-1,2018,11,1,23,9ta5vf,self.MachineLearning,We have really cool prize draws at SC!,https://www.reddit.com/r/MachineLearning/comments/9ta5vf/we_have_really_cool_prize_draws_at_sc/,alejandralopzs,1541084379,[removed],0,1,False,self,,,,,
39,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9ta6di,qz.com,Police are using machine learning to spot written lies,https://www.reddit.com/r/MachineLearning/comments/9ta6di/police_are_using_machine_learning_to_spot_written/,jonfla,1541084468,,0,1,False,https://b.thumbs.redditmedia.com/3N156gqKRStdfGqHdRatcLI8fSKeO03ubXh4TU8DNwE.jpg,,,,,
40,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9ta6pw,self.MachineLearning,Machine Learning for word matching.,https://www.reddit.com/r/MachineLearning/comments/9ta6pw/machine_learning_for_word_matching/,clueless_rosh,1541084530,[removed],0,1,False,self,,,,,
41,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9ta7su,github.com,[P] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9ta7su/p_awesome_relation_extraction/,roomylee,1541084732,,0,1,False,https://a.thumbs.redditmedia.com/9phVAZUmoN6v_3C6xV3fwm-xfONa77TCfgDI3bvsHR8.jpg,,,,,
42,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9ta7z3,self.MachineLearning,Reinforcement Learning with Prediction-Based Rewards,https://www.reddit.com/r/MachineLearning/comments/9ta7z3/reinforcement_learning_with_predictionbased/,omoindrot,1541084767,[removed],0,1,False,self,,,,,
43,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9ta8pq,github.com,[Project] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9ta8pq/project_awesome_relation_extraction/,[deleted],1541084902,[deleted],0,1,False,default,,,,,
44,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9ta9a6,github.com,"""[P]"" Awesome! Relation Extraction!",https://www.reddit.com/r/MachineLearning/comments/9ta9a6/p_awesome_relation_extraction/,[deleted],1541085007,[deleted],0,1,False,default,,,,,
45,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9taa5e,temp,temp,https://www.reddit.com/r/MachineLearning/comments/9taa5e/temp/,roomylee,1541085166,,0,1,False,default,,,,,
46,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tadzq,github.com,[P] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9tadzq/p_awesome_relation_extraction/,roomylee,1541085863,,0,1,False,https://a.thumbs.redditmedia.com/9phVAZUmoN6v_3C6xV3fwm-xfONa77TCfgDI3bvsHR8.jpg,,,,,
47,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9taid4,hi,[D] hi,https://www.reddit.com/r/MachineLearning/comments/9taid4/d_hi/,[deleted],1541086676,[deleted],0,1,False,default,,,,,
48,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9talm7,github.com,[P] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9talm7/p_awesome_relation_extraction/,roomylee,1541087276,,0,1,False,https://a.thumbs.redditmedia.com/9phVAZUmoN6v_3C6xV3fwm-xfONa77TCfgDI3bvsHR8.jpg,,,,,
49,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9talob,self.MachineLearning,[D] How do you explain complex models to non-technical people?,https://www.reddit.com/r/MachineLearning/comments/9talob/d_how_do_you_explain_complex_models_to/,FriendlyCartoonist,1541087287,,8,1,False,self,,,,,
50,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tam9g,sdfa,flair_name:Project [P] afasf,https://www.reddit.com/r/MachineLearning/comments/9tam9g/flair_nameproject_p_afasf/,roomylee,1541087396,,0,1,False,default,,,,,
51,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tan9q,self.MachineLearning,Project [P] A Self Attentive Sentence Embedding (Tensorfl,https://www.reddit.com/r/MachineLearning/comments/9tan9q/project_p_a_self_attentive_sentence_embedding/,roomylee,1541087572,,0,1,False,self,,,,,
52,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tangi,self.MachineLearning,[R] Reinforcement Learning with Prediction-Based Rewards,https://www.reddit.com/r/MachineLearning/comments/9tangi/r_reinforcement_learning_with_predictionbased/,omoindrot,1541087607,"https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards

Blog post by OpenAI on a new technique called ""Random Network Distillation"" to encourage exploration through curiosity. They beat average human performance on Montezuma's Revenge for the first time.

Paper: https://arxiv.org/abs/1810.12894

Code: https://github.com/openai/random-network-distillation",42,1,False,self,,,,,
53,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tania,github.com,[P] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9tania/p_awesome_relation_extraction/,roomylee,1541087615,,0,1,False,https://a.thumbs.redditmedia.com/9phVAZUmoN6v_3C6xV3fwm-xfONa77TCfgDI3bvsHR8.jpg,,,,,
54,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tanon,d,Project A Self Attentive Sentence Embedding (Tensorfl,https://www.reddit.com/r/MachineLearning/comments/9tanon/project_a_self_attentive_sentence_embedding/,roomylee,1541087652,,0,1,False,default,,,,,
55,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tanru,d,Project [P] A Self Attentive Sentence Embedding (Tensorfl,https://www.reddit.com/r/MachineLearning/comments/9tanru/project_p_a_self_attentive_sentence_embedding/,roomylee,1541087668,,0,1,False,default,,,,,
56,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tanuk,self.MachineLearning,Project [P] A Self Attentive Sentence Embedding (Tensorfl,https://www.reddit.com/r/MachineLearning/comments/9tanuk/project_p_a_self_attentive_sentence_embedding/,roomylee,1541087682,,0,1,False,self,,,,,
57,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9tao6v,self.MachineLearning,[P] A Self Attentive Sentence Embedding (Tensorfl,https://www.reddit.com/r/MachineLearning/comments/9tao6v/p_a_self_attentive_sentence_embedding_tensorfl/,roomylee,1541087748,,0,1,False,self,,,,,
58,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,0,9taodp,s,[P] A Self Attentive Sentence Embedding (Tensorfl,https://www.reddit.com/r/MachineLearning/comments/9taodp/p_a_self_attentive_sentence_embedding_tensorfl/,roomylee,1541087787,,0,1,False,default,,,,,
59,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9taraw,self.MachineLearning,[P] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9taraw/p_awesome_relation_extraction/,roomylee,1541088292,"[Awesome Relation Extraction](https://github.com/roomylee/awesome-relation-extraction)

There is a brand new curated list of awesome resources dedicated to Relation Extraction!
This awesome list contains papers, datasets, videos and lectures about Relation Extraction. A relation extraction, one of the most important Natural Language Processing (NLP) task, requires the detection and classification of semantic relationship mentions within a set of artifacts, typically from text or XML documents.
Have anything in mind that you think is awesome and would fit in this list? Please feel free to make pull requests.",0,1,False,self,,,,,
60,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9taujb,twitter.com,"Neural Networks (book) is only $29.55, 85% off",https://www.reddit.com/r/MachineLearning/comments/9taujb/neural_networks_book_is_only_2955_85_off/,ragifi,1541088862,,1,1,False,default,,,,,
61,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9tave8,self.MachineLearning,[P] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9tave8/p_awesome_relation_extraction/,roomylee,1541089014,"&amp;#x200B;

[Awesome Relation Extraction](https://i.redd.it/8s55uu29vqv11.jpg)

[Awesome Relation Extraction](https://github.com/roomylee/awesome-relation-extraction)

There is a brand new curated list of awesome resources dedicated to Relation Extraction!

This awesome list contains papers, datasets, videos and lectures about Relation Extraction. A relation extraction, one of the most important Natural Language Processing (NLP) task, requires the detection and classification of semantic relationship mentions within a set of artifacts, typically from text or XML documents.

Have anything in mind that you think is awesome and would fit in this list? Please feel free to make pull requests.",2,1,False,https://b.thumbs.redditmedia.com/pzx3InhgY8gbd1EawJ_jqUUxKaeQXo3bjNbex6t75cE.jpg,,,,,
62,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9tavy8,arxiv.org,"[R] ""Dendritic cortical microcircuits approximate the backpropagation algorithm"" from Sacramento, Costa, Bengio, and Senn",https://www.reddit.com/r/MachineLearning/comments/9tavy8/r_dendritic_cortical_microcircuits_approximate/,chris2point0,1541089109,,5,1,False,default,,,,,
63,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9tax9o,self.MachineLearning,[P] Awesome! Relation Extraction!,https://www.reddit.com/r/MachineLearning/comments/9tax9o/p_awesome_relation_extraction/,roomylee,1541089370,"&amp;#x200B;

[Awesome Relation Extraction](https://i.redd.it/zeybcwrlvqv11.jpg)

# [Awesome Relation Extraction](https://github.com/roomylee/awesome-relation-extraction)

There is a brand new curated list of awesome resources dedicated to Relation Extraction!

This awesome list contains papers, datasets, videos and lectures related to Relation Extraction. A relation extraction, one of the most important Natural Language Processing (NLP) tasks, requires the detection and classification of semantic relationship mentions within a set of artifacts, typically from text or XML documents.

Have anything in mind that you think is awesome and would fit in this list? Please feel free to make [*pull requests*](https://github.com/roomylee/awesome-relation-extraction/pulls).",5,1,False,https://a.thumbs.redditmedia.com/Ng9UaHvxwVIfbLFPapfrL7M3Q_W3yPh0y_P_t_DCeo0.jpg,,,,,
64,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9taxxl,twitter.com,"[N] Neural Networks (book) is only $29.55 on Amazon, 82% off.",https://www.reddit.com/r/MachineLearning/comments/9taxxl/n_neural_networks_book_is_only_2955_on_amazon_82/,ragifi,1541089492,,0,1,False,https://b.thumbs.redditmedia.com/LYno3jVUjT7c1A18OWUn2VDIJxtaXN7fQ2BiSE06JOM.jpg,,,,,
65,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9tb1g0,self.MachineLearning,Machine learning,https://www.reddit.com/r/MachineLearning/comments/9tb1g0/machine_learning/,rashikukke,1541090155,[removed],0,1,False,self,,,,,
66,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,1,9tb4fz,self.MachineLearning,A app that can contextual meaning of a word,https://www.reddit.com/r/MachineLearning/comments/9tb4fz/a_app_that_can_contextual_meaning_of_a_word/,horrifyingjokes,1541090724,[removed],0,1,False,self,,,,,
67,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,2,9tbq9j,self.MachineLearning,Searching for good introduction papers to ML that describe the types and algorithms,https://www.reddit.com/r/MachineLearning/comments/9tbq9j/searching_for_good_introduction_papers_to_ml_that/,labinOL,1541094712,[removed],0,1,False,self,,,,,
68,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,3,9tbzid,self.MachineLearning,Assumptions for applying Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/9tbzid/assumptions_for_applying_gradient_descent/,AppelsinJuice32,1541096366,[removed],0,1,False,self,,,,,
69,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,3,9tc0qf,self.MachineLearning,What are the uses of TimeDistributed wrapper for LSTM or any other layers?,https://www.reddit.com/r/MachineLearning/comments/9tc0qf/what_are_the_uses_of_timedistributed_wrapper_for/,ImaginaryAnon,1541096597,"I am trying to understand the use of TimeDistributed layer in keras/tensorflow. I have read some threads and articles but still I didn't get it properly.

The threads that gave me some understanding of what the TImeDistributed layer does are -

What is the role of TimeDistributed layer in Keras?

TimeDistributed(Dense) vs Dense in Keras - Same number of parameters

But I still don't know why the layer is actually used!

For example, both the below codes will provide same output (&amp; output_shape):

    model = Sequential()
    model.add(TimeDistributed(LSTM(5, input_shape = (10, 20), return_sequences = True)))
    print(model.output_shape)

    model = Sequential()
    model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
    print(model.output_shape)

And the output shape will be (according to my knowledge) -

    (None, 10, 5)

So, if both the models provide same output, what is actually the use of TimeDistributed Layer?

And I also had one other question. TimeDistributed layer applies time related data to separate layers (sharing same weights). So, how is it different from unrolling the LSTM layer which is provided in keras API as:

    unroll: Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.

What is the difference between these two?

Thank you.. I am still a newbie and so have many questions.
",0,1,False,self,,,,,
70,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,3,9tc5ic,code.fb.com,Horizon: An open-source reinforcement learning platform - Facebook Code,https://www.reddit.com/r/MachineLearning/comments/9tc5ic/horizon_an_opensource_reinforcement_learning/,fortknightly,1541097456,,0,1,False,default,,,,,
71,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,3,9tc8rv,code.fb.com,[N] Horizon: Facebook's Open-source reinforcement learning platform,https://www.reddit.com/r/MachineLearning/comments/9tc8rv/n_horizon_facebooks_opensource_reinforcement/,fortknightly,1541098060,,0,1,False,default,,,,,
72,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,3,9tcbnc,self.MachineLearning,[D] AI Conference Deadlines,https://www.reddit.com/r/MachineLearning/comments/9tcbnc/d_ai_conference_deadlines/,JosephLChu,1541098578,"[AI Conference Deadlines](https://aideadlin.es/?sub=ML,CV,NLP,RO,SP)

Someone made a cool page that I keep staring at intensely for some reason... &gt;\_&gt;

Anyone have any idea why so many conferences are in June though?  Is this just because people like late spring/early summer?

&amp;#x200B;",6,1,False,self,,,,,
73,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,3,9tcc2v,self.MachineLearning,[N] Facebook Open Sources Horizon - An Applied Reinforcement Learning Platform,https://www.reddit.com/r/MachineLearning/comments/9tcc2v/n_facebook_open_sources_horizon_an_applied/,fortknightly,1541098660,"[https://code.fb.com/ml-applications/horizon/](https://code.fb.com/ml-applications/horizon/)

&amp;#x200B;

Thoughts?",1,1,False,self,,,,,
74,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,4,9tcdwb,self.MachineLearning,"Introducing DeOldify: A Progressive, Self-Attention GAN based image colorization/restoration project",https://www.reddit.com/r/MachineLearning/comments/9tcdwb/introducing_deoldify_a_progressive_selfattention/,MyMomSaysImHot,1541098996,[removed],0,1,False,self,,,,,
75,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,4,9tcfls,self.MachineLearning,"[P] Introducing DeOldify: A Progressive, Self-Attention GAN based image colorization/restoration project",https://www.reddit.com/r/MachineLearning/comments/9tcfls/p_introducing_deoldify_a_progressive/,MyMomSaysImHot,1541099301," 

Hello! Ive just launched the public repo for this project Ive been obsessing over for the last two months called **DeOldify**. Yes the names stupid but the tech I think is really cool.

&amp;#x200B;

https://i.redd.it/ere5u8ytprv11.png

Gist is this:

This is a deep learning based model. More specifically, what Ive done is combined the following approaches:

* **Self-Attention Generative Adversarial Network** ([https://arxiv.org/abs/1805.08318](https://arxiv.org/abs/1805.08318)) . Except the generator is a **pretrained Unet** , and Ive just modified it to have the spectral normalization and self attention. Its a pretty straightforward translation. Ill tell you what though- it made all the difference when I switched to this after trying desperately to get a Wasserstein GAN version to work. I liked the theory of Wasserstein GANs but it just didnt pan out in practice. But Im in *love* with Self-Attention GANs.
* Training structure inspired by (but not the same as) **Progressive Growing of GANs**([https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)). The difference here is the number of layers remain constant- I just changed the size of the input progressively and adjusted learning rates to make sure that the transitions between sizes happened successfully. It seems to have the same basic end result- training is faster, stable, and generalizes better.
* **Two Time-Scale Update Rule** ([https://arxiv.org/abs/1706.08500](https://arxiv.org/abs/1706.08500)). This is also very straightforward- its just one to one generator/critic iterations and higher critic learning rate.
* **Generator Loss** is two parts: One is a basic Perceptual Loss (or Feature Loss) based on VGG16- this basically just biases the generator model to replicate the input image. The second of course is the loss score from the critic. For the curious- Perceptual Loss isnt sufficient by itself to produce good results. It tends to just encourage a bunch of brown/green/blue- you know, cheating to the test, basically, which neural networks are really good at doing! Key thing to realize here is that GANs essentially are learning the loss function for you- which is really one big step closer to toward the ideal that were shooting for in machine learning. And of course you generally get much better results when you get the machine to learn something you were previously hand coding. Thats certainly the case here.

[https://github.com/jantic/DeOldify/blob/master/README.md](https://github.com/jantic/DeOldify/blob/master/README.md)",16,1,False,self,,,,,
76,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,5,9td16w,self.MachineLearning,research-creation project with text generator,https://www.reddit.com/r/MachineLearning/comments/9td16w/researchcreation_project_with_text_generator/,cybirdsdontfly,1541103240,"Hi everyone!

  
I am a PhD student in communications at Universit de Montral. My thesis project deals with post-human imaginaries and the way they picture bodies. I am working on science fiction literature, Silicon Valley's startups' rhetorics and academic writing focusing on the post-human. It seems to me that the stories produced through this literature rely upon very individualistic and anthropocentric conceptions of bodies.   
In turn, I am willing to create new stories of post-human bodies notably by using text generation. I am currently trying to learn programming, but my knowledge of text generators is still very incomplete and I would like my project to be collective.  


I am therefore looking for people with some programming skills and attraction to post-human imaginaries that would be interested to take part in my project.   


Any other advice would be for sure appreciated!

&amp;#x200B;

Please contact me if you'd be interested and wanted to know more about the project.  


Thanks!",0,1,False,self,,,,,
77,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,5,9td9uq,self.MachineLearning,[N] NIPS sponsors begin pulling funding from NIPS 2018 Conference,https://www.reddit.com/r/MachineLearning/comments/9td9uq/n_nips_sponsors_begin_pulling_funding_from_nips/,OutsideCattle,1541104816,"[https://twitter.com/RecursionPharma/status/1057669723809169408](https://twitter.com/RecursionPharma/status/1057669723809169408)

&gt;We [~~@~~**RecursionPharma**](https://twitter.com/RecursionPharma) want to see more women welcomed within the ML community and strive to remove biases that lead to underrepresentation. Due to the decision not to change the conf name, Recursion has withdrawn its [~~@~~**NipsConference**](https://twitter.com/NipsConference) sponsorship in favor of supporting other confs.

&amp;#x200B;",12,1,False,self,,,,,
78,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,5,9tdck4,self.MachineLearning,ML course as a senior MechE student,https://www.reddit.com/r/MachineLearning/comments/9tdck4/ml_course_as_a_senior_meche_student/,more_mohr,1541105337,[removed],0,1,False,self,,,,,
79,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,6,9tdl62,self.MachineLearning,Sharing a free data science event info,https://www.reddit.com/r/MachineLearning/comments/9tdl62/sharing_a_free_data_science_event_info/,Jessica-100,1541106917,"Data Science event at Stanford campus next Thursday. Just sharing:  

[https://www.eventbrite.com/e/inspire-big-data-summit-2018-tickets-52016337265?aff=reddit](https://www.eventbrite.com/e/inspire-big-data-summit-2018-tickets-52016337265?aff=reddit)",0,1,False,self,,,,,
80,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,6,9tdt8k,self.MachineLearning,[P] A neural network implementation in Java (ICS3U/4U),https://www.reddit.com/r/MachineLearning/comments/9tdt8k/p_a_neural_network_implementation_in_java_ics3u4u/,git-commit-sudoku,1541108471,"Hello,

I created a [neural network library](https://github.com/patricksongzy/neural-net) in Java (creatively named neural-net), for the grade 11/12 computer science courses. I know that this is a bit basic compared to what others have done. Please refer to the [problems section](##Problems).

# Details

## What is Implemented

* Fully connected layers
* Convolutional layers
* Pooling layers
* GRU layers
* JOCLBlast (GPU) BLAS parallelization (uses library)
* (Popular image recognition architectures, such as GoogLeNet and ResNet)
* ... and more

## Example Usage

```java
Model model = new Model.Builder().add(
    new Convolutional.Builder().filterSize(7).filterAmount(64).pad(3).activationType(ActivationType.RELU)
    .initializer(new Constant(0)).stride(2).updaterType(UpdaterType.ADAM).build()
).add(
    new Dense.Builder().outputSize(10).activation(OutputActivationType.SOFTMAX)
    .initializer(new Constant(0)).updaterType(UpdaterType.ADAM).build()
```

## Problems

* [PSPNet](https://github.com/patricksongzy/neural-net/blob/master/src/main/java/neuralnet/layers/PSP.java) is not working properly (if anybody can help)
* Haven't found the time to implement back-propagation on L2, BatchNorm, LRN and certain other layers. (Recently have been focused on Forward-propagation)
* Model parallelizes GRU runs, by starting above layers immediately after the layer has been completed, but should fully parallelize FC layers (because they do not depend on below or adjacent layers).
* Only supports **sequential** models. Non-sequential models are created by creating new layers (ie. ResNet, GoogLeNet).
* Doesn't use computational graphs (it's easier to see the intricacies of some layers without them for me).
* Pooling layer workaround (to work with GoogLeNet), which causes back-propagation array index out of bounds exception.",17,1,False,self,,,,,
81,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,8,9tert9,self.MachineLearning,Arcade Game Reinforcement Learning Python Library,https://www.reddit.com/r/MachineLearning/comments/9tert9/arcade_game_reinforcement_learning_python_library/,M-J-Murray,1541115314,[removed],0,1,False,self,,,,,
82,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,8,9teu6c,youtube.com,XGBoost Part 1 - Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9teu6c/xgboost_part_1_machine_learning_tutorial/,jeffxu999,1541115803,,0,1,False,https://a.thumbs.redditmedia.com/9eG87K9ajZPLaNO0g8m6TYfq_ZbmT2KkmhfgVTcrGZ4.jpg,,,,,
83,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,8,9tev54,self.MachineLearning,[D] How do you all get your data sets?,https://www.reddit.com/r/MachineLearning/comments/9tev54/d_how_do_you_all_get_your_data_sets/,hmmhhhmhhmhmhmhh,1541116014,"Hey all,

I'm cooking up an idea and wanted some input from potential users. So I know for ML you need good sample data (like tagged pictures). Where do you usually get this data? Is there a specific type of data that's hard to come by (ie: what else other than image recognition stuff)? What types of sets would it be most beneficial to have a human go through and tag (Pictures of dogs? Spatulas? etc)? Vent to me!",15,1,False,self,,,,,
84,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,8,9texgf,medium.com,A Conversation With The AI Expert Behind Google AutoML,https://www.reddit.com/r/MachineLearning/comments/9texgf/a_conversation_with_the_ai_expert_behind_google/,trcytony,1541116499,,0,1,False,default,,,,,
85,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,9,9tf0sl,self.MachineLearning,[R] Learning to Dress: Synthesizing Human Dressing Motion via Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/9tf0sl/r_learning_to_dress_synthesizing_human_dressing/,hardmaru,1541117184,"SIGGRAPH Asia 2018 [paper](https://www.cc.gatech.edu/~aclegg3/projects/LearningToDress.html):

Description:
*Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. We take a model-free deep reinforcement learning (deepRL) approach to automatically discovering robust dressing control policies represented by neural networks. While deepRL has demonstrated several successes in learning complex motor skills, the data-demanding nature of the learning algorithms is at odds with the computationally costly cloth simulation required by the dressing task. This paper is the first to demonstrate that, with an appropriately designed input state space and a reward function, it is possible to incorporate cloth simulation in the deepRL framework to learn a robust dressing control policy. We introduce a salient representation of haptic information to guide the dressing process and utilize it in the reward function to provide learning signals during training. In order to learn a prolonged sequence of motion involving a diverse set of manipulation skills, such as grasping the edge of the shirt or pulling on a sleeve, we find it necessary to separate the dressing task into several subtasks and learn a control policy for each subtask. We introduce a policy sequencing algorithm that matches the distribution of output states from one task to the input distribution for the next task in the sequence. We have used this approach to produce character controllers for several dressing tasks: putting on a t-shirt, putting on a jacket, and robot-assisted dressing of a sleeve.*

https://www.cc.gatech.edu/~aclegg3/projects/LearningToDress.html",4,1,False,self,,,,,
86,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,9,9tfb58,self.MachineLearning,Google colab : Training pong and Supermario by Deep reinforcement learning ( no installation required ),https://www.reddit.com/r/MachineLearning/comments/9tfb58/google_colab_training_pong_and_supermario_by_deep/,wonseokjung,1541119314,"hi,

you can train pong and supermario agent using google colab. 

You dont need installation. 

You can just enter the following link and press run button. 



&lt;img src=""https://www.dropbox.com/s/n7ypkshytfrc53y/Screenshot%202018-11-02%2009.29.36.png?raw=1""&gt;


https://colab.research.google.com/github/wonseokjung/ai_supermario/blob/master/4_pong_dqn%20(1).ipynb


&lt;img src=""https://www.dropbox.com/s/uhonvggdut4kmt2/Screenshot%202018-11-02%2009.30.46.png?raw=1""&gt;

https://colab.research.google.com/github/wonseokjung/ai_supermario/blob/master/2_supermario_dqn.ipynb


",0,1,False,self,,,,,
87,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,10,9tfo2h,self.MachineLearning,[R] TDLS: Attention Is All You Need,https://www.reddit.com/r/MachineLearning/comments/9tfo2h/r_tdls_attention_is_all_you_need/,tdls_to,1541122084,"The classic paper introducing multi head attention applied to translation.

Paper Review: [https://youtu.be/S0KakHcj\_rs](https://youtu.be/S0KakHcj_rs)

Paper: [https://arxiv.org/abs/1706.03762](https://www.youtube.com/redirect?v=S0KakHcj_rs&amp;event=video_description&amp;redir_token=V_7PNmWxRm70PUJ7TKRwG3BfdTp8MTU0MTIwODIyM0AxNTQxMTIxODIz&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762)

&amp;#x200B;

The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",12,1,False,self,,,,,
88,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,11,9tg0hw,news.mit.edu,MIT researchers develop semantic parser that learns through observation to more closely mimic a childs language-acquisition process,https://www.reddit.com/r/MachineLearning/comments/9tg0hw/mit_researchers_develop_semantic_parser_that/,Z3F,1541124717,,0,1,False,default,,,,,
89,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,12,9tgrr3,i.redd.it,[D] The conflict of a rapidly expanding field,https://www.reddit.com/r/MachineLearning/comments/9tgrr3/d_the_conflict_of_a_rapidly_expanding_field/,FellowOfHorses,1541131115,,0,1,False,https://b.thumbs.redditmedia.com/8kqJWcJVZA8AWbUyNywtg_tU3OHhBgFcpoM9l-aYidg.jpg,,,,,
90,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,13,9th2u9,self.MachineLearning,seq2seq models where output sequence is longer than input sequence,https://www.reddit.com/r/MachineLearning/comments/9th2u9/seq2seq_models_where_output_sequence_is_longer/,Zouzan,1541133982,[removed],0,1,False,self,,,,,
91,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,13,9th3vx,gum.co,The Math Behind Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9th3vx/the_math_behind_machine_learning/,rickmister24,1541134278,,0,1,False,https://b.thumbs.redditmedia.com/zjMLlgVaGEWO6szLiOgbdCqnfkDDa3lJI284aJPyTTQ.jpg,,,,,
92,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,14,9th61p,self.MachineLearning,The Math Behind Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9th61p/the_math_behind_machine_learning/,rickmister24,1541134883,[removed],0,1,False,self,,,,,
93,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,14,9th8g8,github.com,[R] Awesome Imitation Learning,https://www.reddit.com/r/MachineLearning/comments/9th8g8/r_awesome_imitation_learning/,Kristery,1541135562,,0,1,False,default,,,,,
94,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,14,9thaqb,syncedreview.com,A Conversation With Quoc Le: The AI Expert Behind Google AutoML,https://www.reddit.com/r/MachineLearning/comments/9thaqb/a_conversation_with_quoc_le_the_ai_expert_behind/,codiyapa,1541136262,,0,1,False,default,,,,,
95,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,14,9thbuy,steemit.com,The Math Behind Machine Learning  Steemit,https://www.reddit.com/r/MachineLearning/comments/9thbuy/the_math_behind_machine_learning_steemit/,rickmister24,1541136589,,0,1,False,default,,,,,
96,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,14,9thgxw,self.MachineLearning,"[D] When I read 'the top/bottom layers' in a paper or article without any illustration of the network, I have no idea what you're talking about.",https://www.reddit.com/r/MachineLearning/comments/9thgxw/d_when_i_read_the_topbottom_layers_in_a_paper_or/,Valiox,1541138215,"I've noticed that, when they don't provide an illustration, people tend to talk about bottom layers as the *first* layers to the network and top layers as the *last*. When an illustration is provided, the meaning is suddenly swapped as most networks are drawn from top (first layers) to bottom (last layers). ",21,1,False,self,,,,,
97,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,15,9thljg,blogs.msdn.microsoft.com,"Data Science in Visual Studio Code using Neuron, a new VS Code extension",https://www.reddit.com/r/MachineLearning/comments/9thljg/data_science_in_visual_studio_code_using_neuron_a/,donutloop,1541139700,,0,1,False,default,,,,,
98,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,15,9thmo6,self.MachineLearning,Udacity's Machine Learning Capstone project ideas,https://www.reddit.com/r/MachineLearning/comments/9thmo6/udacitys_machine_learning_capstone_project_ideas/,ishansoni22,1541140096,[removed],0,1,False,self,,,,,
99,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,15,9thp1d,self.MachineLearning,How to learn concepts of machine learning in finance?,https://www.reddit.com/r/MachineLearning/comments/9thp1d/how_to_learn_concepts_of_machine_learning_in/,Midhilesh29,1541140920,[removed],0,1,False,self,,,,,
100,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,15,9thp6o,learntek.org,Difference between Machine Learning and Artificial Intelligence ?,https://www.reddit.com/r/MachineLearning/comments/9thp6o/difference_between_machine_learning_and/,social789,1541140968,,0,1,False,https://a.thumbs.redditmedia.com/-6o_Ukm5csDTiB7Jun7J8sdb2v8HLipm5_eClp3hhM4.jpg,,,,,
101,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,15,9thqy7,webspiders.com,Top 4 Ways in which AI can Impact Retail Stores,https://www.reddit.com/r/MachineLearning/comments/9thqy7/top_4_ways_in_which_ai_can_impact_retail_stores/,JohnM2m,1541141609,,0,1,False,https://b.thumbs.redditmedia.com/Zi0AV6ECw7xDSP03TMSNkSw8tO_Hh8-fWojzZbqoIpc.jpg,,,,,
102,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,16,9thui4,blog.ntrlab.com,Story about one Russian software development company's offices,https://www.reddit.com/r/MachineLearning/comments/9thui4/story_about_one_russian_software_development/,Batareika_1,1541142782,,0,1,False,default,,,,,
103,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,16,9ti2ag,github.com,"[P] TF Implementation of ""Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification"" (ACL paper)",https://www.reddit.com/r/MachineLearning/comments/9ti2ag/p_tf_implementation_of_attentionbased/,roomylee,1541145511,,0,1,False,default,,,,,
104,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,17,9ti2tl,self.MachineLearning,"[P] TF Implementation of ""Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification"" (ACL paper)",https://www.reddit.com/r/MachineLearning/comments/9ti2tl/p_tf_implementation_of_attentionbased/,roomylee,1541145696,"[https://github.com/SeoSangwoo/Attention-Based-BiLSTM-relation-extraction](https://github.com/SeoSangwoo/Attention-Based-BiLSTM-relation-extraction)

There is tensorflow implementation of ""Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification"" (ACL paper). It seems to be a great help in developing an Attention-based Text Classification Model.",0,1,False,self,,,,,
105,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,17,9ti4jj,self.MachineLearning,Expectation Max forward looking,https://www.reddit.com/r/MachineLearning/comments/9ti4jj/expectation_max_forward_looking/,pikachoose_,1541146320,[removed],0,1,False,self,,,,,
106,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,17,9ti59f,self.MachineLearning,"[P] 50% Faster ML - Modern Big Data Algorithms Update - SVDImpute, more page sneak peeks!",https://www.reddit.com/r/MachineLearning/comments/9ti59f/p_50_faster_ml_modern_big_data_algorithms_update/,danielhanchen,1541146561,"Hey all! Reception on the mini update on Modern Big Data Algorithms was really positive! So, I placed some more pages and will showcase a NEW Missing Data Imputation method - SVDImpute!

SVDImpute is much more powerful than pure column mean impute (filling missing values with column mean). On MNIST Data, approx 29% improvement is seen in sum(X\_reconstructed - X\_real)\^2 error.

The biggest fundamental difference between this new algo is you can call .TRANSFORM(X)!!!. You can use NEW DATA with missing values and imputation still performs well!

(Note - results below 50% values are set to NAN randomnly. 50% training set, then evaluate on full 100% data. 50% test shows similar trends).

One big advantage say over SoftImpute / IterativeImputer / MICE etc is that the scale of the data doesnt matter. (I standardise it). Likewise, no matter the number of missing values, SVDImpute will work on NEW DATA!!

I also found an interesting heuristic that seems to work partially well. The number of components required to get OK loss is approx SQRT(P) - 1, where P = number of parameters / columns. It's an interesting heurestic, but after checking on 7 Sklearn datasets, it seems to hold. Not sure why SQRT(P)-1 works though.

Also, I put some other pages from the book below! Some new / updated algos:

1. Epsilon Jitter Algorithm - how to use Eigendecomposition instead of SVD to calculate PCA, but maintaining stability 100% of the time.
2. Fast-PCA Algorithm - if P &gt;&gt; N, then uses some trickery to calculate the PCA quicker with normalized components.

Hope you like it!

[RED: Mean Imputation. GREEN DOTTED: N\_components. GREEN DARK: Heuristic sqrt\(p\)-1](https://i.redd.it/rk8nput1kvv11.png)

&amp;#x200B;

[Epsilon Jitter PCA Algorithm - results](https://i.redd.it/l0ferlcxkvv11.png)

[Truncated SVD \/ PCA](https://i.redd.it/ju1x6jb0lvv11.png)

[Epsilon Jitter Algorithm](https://i.redd.it/g6gb99htkvv11.png)",0,1,False,https://b.thumbs.redditmedia.com/HnubzQru9UasEsOwR3VdWs8iM_x1ElQA9JxXofL9QRI.jpg,,,,,
107,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,17,9ti8ry,self.MachineLearning,[D] Combination of multinomial sampling and beam search for unconditional generation?,https://www.reddit.com/r/MachineLearning/comments/9ti8ry/d_combination_of_multinomial_sampling_and_beam/,HigherTopoi,1541147872,"In unconditional sequence generation with LM, one cannot use beam search from the beginning, since given that it's unconditional, it kills the diversity of the generated sequences. Here, multinomial sampling just refers to simply sampling from the probability distribution given by the LM (e.g. torch.multinomial). If I understand correctly, the standard way of sequence generation with LM uses multinomial sampling from the beginning to the end. Is there any work that uses beam search after the first n steps? I'm aware of the hierarchical story generation paper that is technically one example of this, though. ",2,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,17,9ti9pu,rubikscode.net,Stock Price Prediction Using Hidden Markov Model,https://www.reddit.com/r/MachineLearning/comments/9ti9pu/stock_price_prediction_using_hidden_markov_model/,RubiksCodeNMZ,1541148228,,0,1,False,default,,,,,
109,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,18,9tijd7,youtube.com,Machine Learning Basics | What Is Machine Learning? | Introduction To Machine Learning |,https://www.reddit.com/r/MachineLearning/comments/9tijd7/machine_learning_basics_what_is_machine_learning/,pooja307,1541151493,,0,1,False,https://b.thumbs.redditmedia.com/oaQjE0Gu5zlJwhsKRhvksp7bWd4jnVKJwaYV36t-dOM.jpg,,,,,
110,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,18,9tilrp,medium.com,Bringing machine learning research to product commercialization,https://www.reddit.com/r/MachineLearning/comments/9tilrp/bringing_machine_learning_research_to_product/,rrothe,1541152189,,0,1,False,https://b.thumbs.redditmedia.com/ytqX3E142EXXVJgCkccsRL4Or7zPyj-NKYe_XpZ-jVk.jpg,,,,,
111,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,19,9tiqad,self.MachineLearning,Our Beloved Mentor,https://www.reddit.com/r/MachineLearning/comments/9tiqad/our_beloved_mentor/,sobhanhag,1541153471,[removed],0,1,False,self,,,,,
112,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,19,9tivbw,self.MachineLearning,Add a GPU to a laptop to speed up network training,https://www.reddit.com/r/MachineLearning/comments/9tivbw/add_a_gpu_to_a_laptop_to_speed_up_network_training/,basjj,1541154855,[removed],0,1,False,self,,,,,
113,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,21,9tjn9v,i.redd.it,Best Machine Learning Training in Delhi,https://www.reddit.com/r/MachineLearning/comments/9tjn9v/best_machine_learning_training_in_delhi/,smadrid056,1541161804,,0,1,False,https://b.thumbs.redditmedia.com/8cFJY09k9r4L-RUa0AJTYbpPee5Q1YisuWTxnkKQZgY.jpg,,,,,
114,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,21,9tjpva,self.MachineLearning,[P] Arcade Game Reinforcement Learning Python Library,https://www.reddit.com/r/MachineLearning/comments/9tjpva/p_arcade_game_reinforcement_learning_python/,M-J-Murray,1541162412,"I created a Python library as part of my final year dissertation which enables you to train RL algorithms against any arcade game supported by [MAME](https://www.mamedev.org/).

I have just gotten around to setting it up as a proper [github repository](https://github.com/M-J-Murray/MAMEToolkit). I thought it could be useful to others doing research in Reinforcement Learning. It works similar to OpenAI's gym, but it should be able to support more complex games.

The library itself doesn't actually come with any games, only an example implementation of Street Fighter III Third Strike: Fight for the Future. I didn't include any ROM's just in case there could be any legal issues. However, the repository contains all the information you need to get starting implementing your own games.

If you do manage to implement any games and would like to share it with the community, just contact me and I'll be happy to add it to the repository.

If you have any further questions about implementing your own games, or find any bugs, or have any advice, I'd be more happy to help.",13,1,False,self,,,,,
115,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,21,9tjsmh,goodworklabs.com,How to choose the right Machine Learning Algorithm - Discusses 9 Most commonly used ML Algorithms,https://www.reddit.com/r/MachineLearning/comments/9tjsmh/how_to_choose_the_right_machine_learning/,alzador123,1541163055,,0,1,False,default,,,,,
116,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,22,9tjvcc,nature.com,Machine learning spots natural selection at work in human genome,https://www.reddit.com/r/MachineLearning/comments/9tjvcc/machine_learning_spots_natural_selection_at_work/,j_orshman,1541163694,,0,1,False,https://b.thumbs.redditmedia.com/sQXhA5nkrV8yEGmPZi-vfxfr-TWQwkm4j9y8WfTvjbg.jpg,,,,,
117,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,22,9tjxp6,link.medium.com,Reinforcement Learning blog (Move 37 by Siraj Raval),https://www.reddit.com/r/MachineLearning/comments/9tjxp6/reinforcement_learning_blog_move_37_by_siraj_raval/,specbug,1541164201,,0,1,False,default,,,,,
118,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,22,9tjybz,youtube.com,Videos from the inaugural Probabilistic Programming conference are posted,https://www.reddit.com/r/MachineLearning/comments/9tjybz/videos_from_the_inaugural_probabilistic/,xamdam,1541164331,,0,1,False,default,,,,,
119,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,22,9tk87a,self.MachineLearning,Why Automation Equals More Jobs | Former Presidential Advisor Pippa Malmgren,https://www.reddit.com/r/MachineLearning/comments/9tk87a/why_automation_equals_more_jobs_former/,The_Syndicate_VC,1541166406,[removed],0,1,False,self,,,,,
120,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,22,9tk93q,self.MachineLearning,Help needed with community detection (graph clustering) papers repository,https://www.reddit.com/r/MachineLearning/comments/9tk93q/help_needed_with_community_detection_graph/,benitorosenberg,1541166600,[removed],0,1,False,https://b.thumbs.redditmedia.com/LWIoGnhm5lIjYRUXGaIsCBPkzq188ZqIIVZkoEJt9ic.jpg,,,,,
121,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,23,9tken6,blog.metaflow.fr,"[D] Contributing to privacy-preserving ML, crafting building blocks for secure AI",https://www.reddit.com/r/MachineLearning/comments/9tken6/d_contributing_to_privacypreserving_ml_crafting/,morgangiraud,1541167713,,1,1,False,default,,,,,
122,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,23,9tkhes,self.MachineLearning,[P] MentisOculi PyTorch Path Tracer,https://www.reddit.com/r/MachineLearning/comments/9tkhes/p_mentisoculi_pytorch_path_tracer/,mmirman,1541168262,"[Full Code Available on GitHub!](https://github.com/mmirman/MentisOculi)

I've been writing a lot of PyTorch lately and taking derivatives of unexpected things and had the fun idea to write a path tracer in PyTorch.  It made sense to me - most of ray tracing is differentiable most of the time, and benefits a ton from being GPU parallelized.  I did it, it works, and gets some speed benefit.  It turns out that I'm not the first person to have this idea, and there are ray tracers written in Theano and Tensorflow.  This one is distinguished primarily by being a path tracer (runs indefinitely to get a progressively more accurate result, rendering things like caustics and global illumination), using MCMC methods, and being written for PyTorch.   

Disclaimer:  This is by no means a competitive real time ray-tracer and there are a huge number of improvements that would have to be made to bring it to that level.  It still might be useful in that with some small modifications, you could throw it into a neural network.",3,1,False,self,,,,,
123,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,23,9tkka6,medium.com,[R] How AI (Artificial Intelligence) Could Transform Healthcare,https://www.reddit.com/r/MachineLearning/comments/9tkka6/r_how_ai_artificial_intelligence_could_transform/,RegularJudge,1541168835,,0,1,False,default,,,,,
124,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,23,9tkq5w,youtube.com,SNN Learns to Play Flappy Bird(Ultra hard mode),https://www.reddit.com/r/MachineLearning/comments/9tkq5w/snn_learns_to_play_flappy_birdultra_hard_mode/,AIMaster42,1541169965,,0,1,False,default,,,,,
125,MachineLearning,t5_2r3gv,2018-11-2,2018,11,2,23,9tktva,self.MachineLearning,Memory Replay GANs: learning to generate images from new categories without forgetting,https://www.reddit.com/r/MachineLearning/comments/9tktva/memory_replay_gans_learning_to_generate_images/,lherranz,1541170696,"NIPS 2018 paper ([arxiv](https://arxiv.org/abs/1809.02058), [blog](http://www.lherranz.org/2018/10/29/mergans/), [code](https://github.com/WuChenshen/MeRGAN))

Abstract: Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories.

&amp;#x200B;

https://i.redd.it/9buldkzqlxv11.png",0,1,False,https://b.thumbs.redditmedia.com/T52nxHYg778pss67Y-b85qYVGYoCYiH9rS8ve-G50as.jpg,,,,,
126,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,0,9tkyp0,self.MachineLearning,I tried to teach a neural network to beat me at my own game. Here is what I've learned...,https://www.reddit.com/r/MachineLearning/comments/9tkyp0/i_tried_to_teach_a_neural_network_to_beat_me_at/,JonasTyr,1541171615,[removed],0,1,False,self,,,,,
127,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,0,9tkzni,self.MachineLearning,[D] Do Predictive Models work for all Businesses?,https://www.reddit.com/r/MachineLearning/comments/9tkzni/d_do_predictive_models_work_for_all_businesses/,LityxIQ,1541171801,"A common question we hear from clients is whether [mathematical models](https://lityx.com/marketing-models-work/) used for target marketing always work, and will it work for my business?

Whether models always work can be a subjective question, unless we define the parameters for success at the beginning of a project. It is true that marketing models vary in performance but they invariably result in successfully predicting the desired outcome. [Read More.](https://lityx.com/marketing-models-work/)

Have you ever tried to use any in your business and had a positive or negative response?",1,1,False,self,,,,,
128,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,0,9tl4ey,youtube.com,I tried to teach a neural network to beat me at my own game. Here is what I've learned...,https://www.reddit.com/r/MachineLearning/comments/9tl4ey/i_tried_to_teach_a_neural_network_to_beat_me_at/,JonasTyr,1541172714,,0,1,False,https://a.thumbs.redditmedia.com/hNzT7nvxEb-1F_Vez3RpZ0jkhZ48tsFzbQdgeStQFL0.jpg,,,,,
129,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,0,9tl4qy,self.MachineLearning,Best practises for machine learning in AWS? i.e. Sagemaker vs EC2 vs EMR,https://www.reddit.com/r/MachineLearning/comments/9tl4qy/best_practises_for_machine_learning_in_aws_ie/,philmcp,1541172776,[removed],0,1,False,self,,,,,
130,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,0,9tl6q7,self.MachineLearning,[R] Help needed with community detection (graph clustering) papers repository,https://www.reddit.com/r/MachineLearning/comments/9tl6q7/r_help_needed_with_community_detection_graph/,benitorosenberg,1541173149,"Hi there,

I am creating a community detection paper aggregator repository with implementations. I would be very happy if others would contribute:

[https://github.com/benedekrozemberczki/awesome-community-detection](https://github.com/benedekrozemberczki/awesome-community-detection)

https://i.redd.it/cncm2s4itxv11.png",14,1,False,https://b.thumbs.redditmedia.com/WdtpIG7nr5sfdKWR6Y8VdKo9UYFMOTd8tSgM-JS2g8U.jpg,,,,,
131,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,0,9tl7hh,self.MachineLearning,Pre-built ubuntu images with all libraries installed,https://www.reddit.com/r/MachineLearning/comments/9tl7hh/prebuilt_ubuntu_images_with_all_libraries/,goudarziha,1541173302,[removed],0,1,False,self,,,,,
132,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,0,9tl7m3,self.MachineLearning,[D] Development Setup - Are you training your models on a cluster during development?,https://www.reddit.com/r/MachineLearning/comments/9tl7m3/d_development_setup_are_you_training_your_models/,nielsrolf,1541173327,"My current workflow is to code and try the code on my laptop. This is often very slow, so I thought about renting a cluster. Do you know services that let you run jupyter notebooks on a fast cluster? I tried to use google colaboratory, but found it pretty slow (training a keras conv model). Or do you know a service that can be integrated with PyCharm?",3,1,False,self,,,,,
133,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,1,9tlmvh,self.MachineLearning,Where could I find some some to explain q learning in finance,https://www.reddit.com/r/MachineLearning/comments/9tlmvh/where_could_i_find_some_some_to_explain_q/,ImaginaryFly,1541176087,I whant to ask where is a good place  for me to find a person to explain me  some codes  from machine learning   in trading offered  by georgia tech. I tacked  the  course  but still need help  in coding,0,1,False,self,,,,,
134,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,1,9tlsm7,self.MachineLearning,Where to start for understanding ML and Computer Vision?,https://www.reddit.com/r/MachineLearning/comments/9tlsm7/where_to_start_for_understanding_ml_and_computer/,Mehmet_Kaan_Toyaksi,1541177168,[removed],0,1,False,self,,,,,
135,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,2,9tlyx5,/r/MachineLearning/comments/9tlyx5/autonomous_subway_sandwich_delivery/,Autonomous subway sandwich delivery ,https://www.reddit.com/r/MachineLearning/comments/9tlyx5/autonomous_subway_sandwich_delivery/,Moshiko1,1541178291,,0,1,False,default,,,,,
136,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,2,9tm361,self.MachineLearning,"[R] Scalable Gensim implementation of Walklets from ""Don't Walk Skip! Online Learning of Multi-scale Network Embeddings"" (ASONAM 2017).",https://www.reddit.com/r/MachineLearning/comments/9tm361/r_scalable_gensim_implementation_of_walklets_from/,benitorosenberg,1541179049,[removed],0,1,False,self,,,,,
137,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,2,9tm5na,self.MachineLearning,Pre-requisite for ISLR(Introduction To Statistical Learning) Textbook,https://www.reddit.com/r/MachineLearning/comments/9tm5na/prerequisite_for_islrintroduction_to_statistical/,mjfocus,1541179520,"Hello Redditors,

Hope you are doing great !!!

I am looking for pre-requisite for ISLR Textbook. To give a brief background, I am a graduate student pursuing masters in analytics program. As per the coursework I am expected to study ISLR textbook, but I am feeling that book a bit challenging. Are there any other sources(textbook/ video lectures/online courses) which can provide me sufficient foundation to ace through ISLR.

Thanks in advance ....",0,1,False,self,,,,,
138,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,2,9tm7sz,self.MachineLearning,[R] Memory Replay GANs: learning to generate images from new categories without forgetting,https://www.reddit.com/r/MachineLearning/comments/9tm7sz/r_memory_replay_gans_learning_to_generate_images/,lherranz,1541179936,"NIPS 2018 paper ([arxiv](https://arxiv.org/abs/1809.02058), [code](https://github.com/WuChenshen/MeRGAN), [blog](http://www.lherranz.org/2018/10/29/mergans/))

Abstract:  Previous works on sequential learning address the problem of forgetting  in discriminative models. In this paper we consider the case of  generative models. In particular, we investigate generative adversarial  networks (GANs) in the task of learning new categories in a sequential  fashion. We first show that sequential fine tuning renders the network  unable to properly generate images from previous categories (i.e.  forgetting). Addressing this problem, we propose Memory Replay GANs  (MeRGANs), a conditional GAN framework that integrates a memory replay  generator. We study two methods to prevent forgetting by leveraging  these replays, namely joint training with replay and replay alignment.  Qualitative and quantitative experimental results in MNIST, SVHN and  LSUN datasets show that our memory replay approach can generate  competitive images while significantly mitigating the forgetting of  previous categories.",0,1,False,self,,,,,
139,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,2,9tmadg,i.redd.it,TS-2888X new AI computing NAS,https://www.reddit.com/r/MachineLearning/comments/9tmadg/ts2888x_new_ai_computing_nas/,Xav4593L,1541180428,,0,1,False,default,,,,,
140,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,2,9tme39,self.MachineLearning,"Does anyone know which other AI Residency programs (2019 cohort) are open for applications than Google, and/or their planned timelines? Thanks very much.",https://www.reddit.com/r/MachineLearning/comments/9tme39/does_anyone_know_which_other_ai_residency/,shenglih,1541181130,[removed],0,1,False,self,,,,,
141,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,2,9tmf9q,self.MachineLearning,Experience with TRPO,https://www.reddit.com/r/MachineLearning/comments/9tmf9q/experience_with_trpo/,schludy,1541181349,[removed],0,1,False,self,,,,,
142,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,3,9tmj41,homepages.inf.ed.ac.uk,[P] Just stumbled upon this amazing collection of image datasets,https://www.reddit.com/r/MachineLearning/comments/9tmj41/p_just_stumbled_upon_this_amazing_collection_of/,Valiox,1541182060,,0,1,False,default,,,,,
143,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,3,9tmuo6,self.MachineLearning,Free big data event next Thursday,https://www.reddit.com/r/MachineLearning/comments/9tmuo6/free_big_data_event_next_thursday/,Jessica-100,1541184222,[removed],0,1,False,self,,,,,
144,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,3,9tmxju,self.MachineLearning,TS-2888X AI ready NAS can take 4 GPU for computing,https://www.reddit.com/r/MachineLearning/comments/9tmxju/ts2888x_ai_ready_nas_can_take_4_gpu_for_computing/,Xav4593L,1541184758,[removed],0,1,False,self,,,,,
145,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,4,9tn9qo,self.MachineLearning,[P] We think the Russian Trolls are still out there: attempts to identify them with ML,https://www.reddit.com/r/MachineLearning/comments/9tn9qo/p_we_think_the_russian_trolls_are_still_out_there/,eegilbert,1541187045,[removed],0,1,False,self,,,,,
146,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,4,9tnbeu,self.MLQuestions,How to maintain consistency/stability across frames of video generated with CNN?,https://www.reddit.com/r/MachineLearning/comments/9tnbeu/how_to_maintain_consistencystability_across/,nivm321,1541187358,,0,1,False,default,,,,,
147,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,5,9tnmhh,self.MachineLearning,CUDA and CuDNN installation cause issues with NVIDIA drivers?,https://www.reddit.com/r/MachineLearning/comments/9tnmhh/cuda_and_cudnn_installation_cause_issues_with/,AccurateFrosting,1541189376,[removed],0,1,False,self,,,,,
148,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,5,9tnwlr,self.MachineLearning,"[P] Scalable Gensim implementation of Walklets from ""Don't Walk Skip! Online Learning of Multi-scale Network Embeddings"" (ASONAM 2017).",https://www.reddit.com/r/MachineLearning/comments/9tnwlr/p_scalable_gensim_implementation_of_walklets_from/,benitorosenberg,1541191315,"[https://github.com/benedekrozemberczki/walklets](https://github.com/benedekrozemberczki/walklets)

Walklets is a multi-scale node embedding algorithm which learns an embedding of approximated adjacency matrix powers up to a given order. Walklet places nodes in an abstract feature space where the vertex features are able to reproduce connectivity patterns in the graph at multiple scales. Embedding is created with an exponential implicit factorization machine. Feature vectors that are extracted in an unsupervised way can be used in downstream machine learning tasks such as edge prediction, node classification and community detection.

The implementation supports second-order random walk sampling, which was metnioned in the original paper but was not implemented in it. The second-order random walks sampling methods were taken from the reference implementation of [Node2vec](https://github.com/aditya-grover/node2vec).",0,1,False,self,,,,,
149,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,6,9to898,self.MachineLearning,"New to the entire field, how do I make my learning experience more effective?",https://www.reddit.com/r/MachineLearning/comments/9to898/new_to_the_entire_field_how_do_i_make_my_learning/,gmguille,1541193563,[removed],0,1,False,self,,,,,
150,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,6,9tocpg,self.MachineLearning,"[D] New to the entire field, how can I make my study more effective?",https://www.reddit.com/r/MachineLearning/comments/9tocpg/d_new_to_the_entire_field_how_can_i_make_my_study/,gmguille,1541194480,"In general, I'm very new to engineering, but luckily my current employer has given me a lot of freedom to learn whatever I want. I've been trying to learn ML, but I feel like I've been wandering kind of aimlessly, and I'd like to take a more directed approach to what I learn.

&amp;#x200B;

I've learned a variety of basic methods, and I tried implementing a model using PCA, kmeans, random forest, but I really had no idea what I was doing. I was struggling to understand what I wanted from my data, what inputs I needed, let alone how I would relate and implement these methods. I've been reading ""Introduction to Machine Learning with R/"" + ""Python"", watching a lot of youtube videos/lectures, and I've briefly dabbled in the Coursera lectures, but I still feel like my understanding of the whole thing is extremely shallow.

&amp;#x200B;

All this being said, is it reasonable to teach myself ML with only a bachelors? What are some other resources I should use to better understand the field? What are some prerequisites that I should look at before I even start looking at ML? What should my expectations in terms of understanding down the road?

&amp;#x200B;

tl;dr

* Super green to engineering
* Unsure of best resources to learn ML, and I might be lacking prerequisites
* How do I hedge my expectations for learning ML? ie. How hard is it really to learn ML?",0,1,False,self,,,,,
151,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,6,9tog6q,self.MachineLearning,free big data event at Stanford,https://www.reddit.com/r/MachineLearning/comments/9tog6q/free_big_data_event_at_stanford/,Jessica-100,1541195183,"just sharing:  

[https://www.eventbrite.com/e/inspire-big-data-summit-2018-tickets-52016337265?aff=reddit](https://www.eventbrite.com/e/inspire-big-data-summit-2018-tickets-52016337265?aff=reddit)",0,1,False,self,,,,,
152,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,7,9tomx7,self.MachineLearning,Is recurrent neural network good for time series with mean reverting properties?,https://www.reddit.com/r/MachineLearning/comments/9tomx7/is_recurrent_neural_network_good_for_time_series/,qudcjf7928,1541196406,[removed],0,1,False,self,,,,,
153,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,7,9top7n,self.MachineLearning,Ethical Issues with ML Academics working in Industrial Labs,https://www.reddit.com/r/MachineLearning/comments/9top7n/ethical_issues_with_ml_academics_working_in/,etmhpe,1541196854,[removed],0,1,False,self,,,,,
154,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,7,9tp1l2,manchester.ac.uk,[N] 'Human brain' supercomputer with 1 million processors switched on for first time,https://www.reddit.com/r/MachineLearning/comments/9tp1l2/n_human_brain_supercomputer_with_1_million/,gtechmisc,1541199413,,0,1,False,default,,,,,
155,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,8,9tpgq5,self.MachineLearning,[D] Workshop-Oriented Conferences: A new model for organizing research conferences,https://www.reddit.com/r/MachineLearning/comments/9tpgq5/d_workshoporiented_conferences_a_new_model_for/,alexmlamb,1541202721,"This is a proposal on a new way that conferences (like NIPS/ICML/ICLR) could be organized.  

The central idea is to make the workshops the central entity.  No papers would be submitted to the conference directly.  Instead all papers would initially be submitted to workshops.  Then among those papers accepted by the workshop, the top 20% or so (over-simplification, perhaps an appeal process could be included) would then be nominated to be submissions to the main conference, with the workshop reviews attached to the paper, and an opportunity for the authors to revise the paper.  

The conference itself would be organized differently, with the workshops taking the majority of the time (i.e. 3-4 days) and coming at the beginning of the conference.  Perhaps in the afternoon there could still be large conference-wide poster sessions.  

I think this approach has some big advantages: 

* I think the experience for authors would be better, as they'd always get to have the first round of workshop reviews, which would be less competitive and more constructive.  
* Right now workshop submissions come after conference submissions, and a lot of people submit rejected work to the workshops.  This seems backwards to me.  
* Having two rounds of reviews will probably catch more errors than the current single round.  And the first round might have more specialized reviewers.  
* It would greatly reduce the number of papers that main conferences would need to review, which would allow for the reviewer pool to be more senior and higher quality.  
* It scales better to conferences with 5000+ people, and right now I already find that workshops are the most useful and functional part of the conference.  

It would also require some changes: 

* Many more workshops would need to be held, to make sure that the entire range of subjects is covered.  
* Workshops would need to be more heavily regulated (probably double blind reviewing) and their review process would need to be fairly quick.  
* The call-for-workshop-proposals would need to be held much earlier.  

&amp;#x200B;",16,1,False,self,,,,,
156,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,9,9tptih,self.MachineLearning,[D] What loss function to use for probability labels (between 0 and 1)?,https://www.reddit.com/r/MachineLearning/comments/9tptih/d_what_loss_function_to_use_for_probability/,kebabmybob,1541205574,"I have some data where the true label is a continuous probability (0, 1). I am training a simple model using some sparse features.

I used a deep net using a sigmoid final layer and mean cross_entropy (Tensorflow) as the loss to optimize. The result was that my model would update for a few iterations but then stay approximately flat on the loss with minor perturbations with each batch.

What I didn't take into account was that binary `cross_entropy(x, y)` is not actually 0 when `x == y`. It is *minimized* at that value, for any given x, but not 0. I suspect that this is the reason my loss never improved - the model was backpropping losses even if the target was hit.

In [this video](https://www.youtube.com/watch?v=xTU79Zs4XKY&amp;feature=youtu.be&amp;t=330) Hugo suggests that using Binary Cross Entropy is fine but I am not clear whether this only applies to autoencoders or if it truly is fine for my use case and my error isn't converging for other reasons.

To that end:

1) Can I use binary cross entropy for my problem?
2) If not, what are other good losses to use? Mean squared error?
3) Is there something wrong with this function I thought of: `cross_entropy(target, prediction) - cross_entropy(target, target)`. It has the basic property I want of being 0 when the prediction is correct, but not sure if the general structure is considered safe for backpropping against.",18,1,False,self,,,,,
157,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,9,9tpwfj,youtube.com,Neural Network Part 1 - Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9tpwfj/neural_network_part_1_machine_learning_tutorial/,jeffxu999,1541206236,,0,1,False,https://b.thumbs.redditmedia.com/Jro3c31qrPFgAYMlVOb9MZYGhh5nv2AextsO12LGdkA.jpg,,,,,
158,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,9,9tpxns,self.MachineLearning,Who is waiting for ICLR reviews to come out?,https://www.reddit.com/r/MachineLearning/comments/9tpxns/who_is_waiting_for_iclr_reviews_to_come_out/,ItHasCeasedToBe,1541206522,[removed],0,1,False,self,,,,,
159,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,11,9tqiw6,self.MachineLearning,[R] Excessive Invariance Causes Adversarial Vulnerability (Vector Institute),https://www.reddit.com/r/MachineLearning/comments/9tqiw6/r_excessive_invariance_causes_adversarial/,jhjac,1541211475,"This paper shows how deep classifiers are not only too sensitive to tiny task-irrelevant changes, as is well-known from epsilon adversarial examples, but also too invariant to a wide range of task-relevant changes.

In fact, one can attach almost any semantic meaning to an image while leaving its 1000-dimensional logit activations in a competitive ImageNet classifier completely unchanged. This striking failure is then understood by explaining and formalizing it on the theoretically tractable adversarial spheres problem.

Finally, an information-theoretic analysis of this novel viewpoint on adversarial robustness reveals an insufficiency of the commonly-used cross-entropy objective as a major reason for the observed invariance-based vulnerability. By leveraging properties of invertible deep networks, an alternative loss function is proposed, showing promise to overcome the problem in practice.

Link: [https://arxiv.org/abs/1811.00401](https://arxiv.org/abs/1811.00401)",0,1,False,self,,,,,
160,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,11,9tqprh,/r/MachineLearning/comments/9tqprh/autonomous_subway_sandwich_delivery/,Autonomous subway sandwich delivery,https://www.reddit.com/r/MachineLearning/comments/9tqprh/autonomous_subway_sandwich_delivery/,Moshiko1,1541213159,,0,1,False,default,,,,,
161,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,12,9tr2ev,github.com,"[P] A collection of image to image papers (including supervised, unsupervised, ...)",https://www.reddit.com/r/MachineLearning/comments/9tr2ev/p_a_collection_of_image_to_image_papers_including/,lzhbrian,1541216375,,0,1,False,default,,,,,
162,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,12,9tr4si,github.com,"[P] A collection of image to image papers (including supervised, unsupervised, ...)",https://www.reddit.com/r/MachineLearning/comments/9tr4si/p_a_collection_of_image_to_image_papers_including/,lzhbrian,1541217028,,0,1,False,https://b.thumbs.redditmedia.com/Xkl98YOhNOITyHYwFZ2ouLw67lxOx7Ahd_HMd0_D5MU.jpg,,,,,
163,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,12,9tr5jf,self.MachineLearning,A machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/9tr5jf/a_machine_learning_game_ive_been_working_on/,twm7,1541217233,[removed],0,1,False,self,,,,,
164,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,13,9trg63,self.MachineLearning,"[P] A collection of image to image papers (including supervised, unsupervised, ...)",https://www.reddit.com/r/MachineLearning/comments/9trg63/p_a_collection_of_image_to_image_papers_including/,lzhbrian,1541220165,"[lzhbrian/image-to-image-papers](https://github.com/lzhbrian/image-to-image-papers)

I organized a collection of all image to image papers, hope this list could also help others.",3,1,False,self,,,,,
165,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,14,9trnmn,getrevue.co,Awesome Computer Science #12,https://www.reddit.com/r/MachineLearning/comments/9trnmn/awesome_computer_science_12/,programming-innovate,1541222408,,0,1,False,https://b.thumbs.redditmedia.com/6FR0_c5NHVqmbiixdaAHyti3P9v6bUSyq23no50D_kM.jpg,,,,,
166,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,14,9trtuk,self.MachineLearning,[D] Need resources for xgboost + bayesian opt,https://www.reddit.com/r/MachineLearning/comments/9trtuk/d_need_resources_for_xgboost_bayesian_opt/,CircuitBeast,1541224383,"I saw a discussion on here somewhere that said 80% of the time xgboost with bayesian optimization is a great way to tackle ml when time is crucial or as a great baseline to compare other algos.

Do you agree with this? Id like to learn how this works. Most blogs dont go very deep. What are your favorite resources to understand this algo and optimizer?",19,1,False,self,,,,,
167,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,16,9ts69m,self.MachineLearning,How can I deploy multiple deep learning models with end points?,https://www.reddit.com/r/MachineLearning/comments/9ts69m/how_can_i_deploy_multiple_deep_learning_models/,palashsharma15,1541228683,,0,1,False,self,,,,,
168,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,17,9tshl2,self.MachineLearning,Looking to curate a list of mailing lists and forums on ML and AI,https://www.reddit.com/r/MachineLearning/comments/9tshl2/looking_to_curate_a_list_of_mailing_lists_and/,Enamex,1541233098,[removed],0,1,False,self,,,,,
169,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,18,9tsovl,self.MachineLearning,8 bit colour upscale to 10 bit (or beyond).,https://www.reddit.com/r/MachineLearning/comments/9tsovl/8_bit_colour_upscale_to_10_bit_or_beyond/,JC_Le_Juice,1541236044,[removed],0,1,False,self,,,,,
170,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,20,9ttdst,self.MachineLearning,Wavelets in feature extraction for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/9ttdst/wavelets_in_feature_extraction_for_deep_learning/,MohamedRashad,1541244717,I am working on a research for ecg analysis and heart disease classification using Deep Learning and i was thinking of wavelet analysis as my main feature extractor and maybe i can use it in denoising the input signals  .... i want to know if this is a good choice or not and what Neural Network archtictures are best suited for the classification task ?,0,1,False,self,,,,,
171,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,20,9ttf0n,self.MachineLearning,What is Machine Learning and How is it Shaping our Future?,https://www.reddit.com/r/MachineLearning/comments/9ttf0n/what_is_machine_learning_and_how_is_it_shaping/,SunilAhujaa,1541245105,[removed],0,1,False,self,,,,,
172,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,20,9tthtt,self.MachineLearning,"[P] Scalable Gensim implementation of Walklets from ""Don't Walk Skip! Online Learning of Multi-scale Network Embeddings"" (ASONAM 2017).",https://www.reddit.com/r/MachineLearning/comments/9tthtt/p_scalable_gensim_implementation_of_walklets_from/,benitorosenberg,1541245993,"&amp;#x200B;

[https://github.com/benedekrozemberczki/walklets](https://github.com/benedekrozemberczki/walklets)

Walklets is a multi-scale node embedding algorithm which learns an embedding of approximated adjacency matrix powers up to a given order. Walklet places nodes in an abstract feature space where the vertex features are able to reproduce connectivity patterns in the graph at multiple scales. Embedding is created with an exponential implicit factorization machine. Feature vectors that are extracted in an unsupervised way can be used in downstream machine learning tasks such as edge prediction, node classification and community detection.

The implementation supports second-order random walk sampling, which was proposed in the original paper but was not implemented in it. The second-order random walks sampling methods were taken from the reference implementation of \[Node2vec\]([https://github.com/aditya-grover/node2vec](https://github.com/aditya-grover/node2vec)).",5,1,False,self,,,,,
173,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,20,9tti0t,self.MachineLearning,Guardians of the Data Universe,https://www.reddit.com/r/MachineLearning/comments/9tti0t/guardians_of_the_data_universe/,SunilAhujaa,1541246056,[removed],0,1,False,self,,,,,
174,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,21,9ttsyh,seonewsagency.com,Linear Algebra for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9ttsyh/linear_algebra_for_machine_learning/,andre-1425,1541249233,,0,1,False,default,,,,,
175,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,22,9tu31u,self.MachineLearning,Image generation based on audio input,https://www.reddit.com/r/MachineLearning/comments/9tu31u/image_generation_based_on_audio_input/,derevirn,1541251829,[removed],0,1,False,self,,,,,
176,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,23,9tud4v,aijournal.github.io,Dynamic Programming in Reinforcement Learning simplified,https://www.reddit.com/r/MachineLearning/comments/9tud4v/dynamic_programming_in_reinforcement_learning/,vector_machines,1541254152,,0,1,False,https://a.thumbs.redditmedia.com/U05hWv7qP5IE62thl-cdgeIj-M_e5g9FOOebVh1v890.jpg,,,,,
177,MachineLearning,t5_2r3gv,2018-11-3,2018,11,3,23,9tugzs,self.MachineLearning,[D] When are ICLR reviews out ?,https://www.reddit.com/r/MachineLearning/comments/9tugzs/d_when_are_iclr_reviews_out/,gohu_cd,1541255021,I see that the deadline for reviews of ICLR 19 papers has passed. Anyone knows when will authors get their reviews? ,14,1,False,self,,,,,
178,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,0,9tuv05,everythingaboutartificialintelligence.com,EVERYTHING ABOUT ARTIFICIAL INTELLIGENCE,https://www.reddit.com/r/MachineLearning/comments/9tuv05/everything_about_artificial_intelligence/,Everything_aboutAI,1541258045,,0,1,False,default,,,,,
179,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,2,9tvrfk,self.MachineLearning,A project I've been working on,https://www.reddit.com/r/MachineLearning/comments/9tvrfk/a_project_ive_been_working_on/,AIMaster42,1541264824,[removed],0,1,False,self,,,,,
180,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,2,9tw5dz,i.redd.it,20% discount coupon code for udicity courses,https://www.reddit.com/r/MachineLearning/comments/9tw5dz/20_discount_coupon_code_for_udicity_courses/,sainimohit23,1541267606,,0,1,False,default,,,,,
181,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,3,9twbjr,self.MachineLearning,Punch your ticket to IT success - AWS Solution Architect Certification Training,https://www.reddit.com/r/MachineLearning/comments/9twbjr/punch_your_ticket_to_it_success_aws_solution/,internetdigitalentre,1541268832,[removed],0,1,False,https://b.thumbs.redditmedia.com/cJuvzIt6FIoFitCnUBmAZ16H4JZ18OiYOdFy2PIvzDQ.jpg,,,,,
182,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,3,9twc32,self.MachineLearning,Data-driven journalism involving Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/9twc32/datadriven_journalism_involving_natural_language/,kuchenrolle,1541268948,"I am trying to find articles (in the best case from major news outlets)  that use some collection of text (a corpus) as their data source to back  up a point they're trying to make. It's for a class where we've so far  talked about linguistic distributions and co-occurrence, but some of the  students are from journalism and I'd like to show them how analyzing  language (especially using semantic models) can be utilized, but I've  found close to nothing so far. It's not really my area, so I'm hoping  someone here can help.",0,1,False,self,,,,,
183,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,4,9twr40,self.MachineLearning,Statistical Significant Difference between random algorithm and non-random one?,https://www.reddit.com/r/MachineLearning/comments/9twr40/statistical_significant_difference_between_random/,CzoKc,1541272027,"I am trying to compare the variance of my algorithm and that of a random one, but I am not sure how I would go about proving that a random one is worse than my implementation. Any thoughts?",0,1,False,self,,,,,
184,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,4,9twtex,self.MachineLearning,[D] Possible architectures for training RNNs with both sequential and non-sequential features?,https://www.reddit.com/r/MachineLearning/comments/9twtex/d_possible_architectures_for_training_rnns_with/,rampant_juju,1541272502,"This relates to a sentiment analysis task. 

Suppose I have a training set of product reviews, each tagged with a sentiment category, from ""Very Positive"", ""Positive"", ""Neutral"", ""Negative"", ""Very Negative"". I also have some accessory data for each sentence, e.g. the age of the writer, gender of the writer, price of the product, category of the product, etc.

The task is to predict the sentiment at various points of each test sentence, using both the sequential data (the words in the sentence) and non-sequential accessory data. 

---

An idea I was contemplating was to keep changing the input word, while keeping the accessory data constant for each test sentence.

Example, consider the sentence ""I liked the product but am completely unhappy with the quality of service"", written by a 50 year old male. The sentence as a whole would be tagged as ""Negative"". At various points in the sentence the model would predict different things:

Word | Age | Gender | Current sentiment output from RNN
--- | --- | --- | ---
I | 50 | M | Neutral
liked | 50 | M | Positive 
the | 50 | M | Positive
product | 50 | M | Positive
but | 50 | M | Neutral
am | 50 | M | Neutral
completely | 50 | M | Neutral
unhappy | 50 | M | Very Negative
with | 50 | M | Negative
the | 50 | M | Negative
quality | 50 | M | Negative
of | 50 | M | Negative
service | 50 | M | Negative

Has such an architecture been used before? 

Thanks.",17,1,False,self,,,,,
185,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,4,9twux4,self.MachineLearning,[D] Building a Neural Network for musical chord recognition,https://www.reddit.com/r/MachineLearning/comments/9twux4/d_building_a_neural_network_for_musical_chord/,J0zif,1541272805,"I am very new to Machine Learning and am trying to build a Neural Network that is able to identify chords from an instrument. I've done my research and have a basic understanding of now NN's work, I am using the library OpenNN and am trying to work my head around training strategies. Reading the documentation, it goes on about Quasi-Newton Methods. just wondering if I am on the right track/which method would be advised?

Thanks!",18,1,False,self,,,,,
186,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,4,9twylk,self.MachineLearning,Help choosing the right tools,https://www.reddit.com/r/MachineLearning/comments/9twylk/help_choosing_the_right_tools/,mohamedahmed95,1541273577,"Hi everyone, i am a total noob in ML, but not in programming. Currently I am working with two  professors on a paper, i will do the programming part. And they will do the theory and writing the paper. The idea is to apply bayesian network on data read from csv file. They suggested two tools ( GOBNLIP and SCIP). AFAIK these tools work on CPU. So to cut a long story short, i am thinking about suggesting tools that use GPU, since they are much faster. But I do not know if such tools exist. I really want to work with them, and they understand that I am new to this area so they are welling to give me time to learn.
Best regards.",0,1,False,self,,,,,
187,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,4,9tx1m4,self.MachineLearning,[P] Help needed with graph embedding papers repository,https://www.reddit.com/r/MachineLearning/comments/9tx1m4/p_help_needed_with_graph_embedding_papers/,benitorosenberg,1541274204,"## Hi there,

I am creating a graph embedding aggregator repository with implementations. Currently I cover factorization methods, deep learning, kernels and statistical fingerprints. I would be very happy if others would contribute:

[https://github.com/benedekrozemberczki/awesome-graph-embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding)

https://i.redd.it/2m8aymrx56w11.png

&amp;#x200B;",0,1,False,https://b.thumbs.redditmedia.com/DK1VHciegyR4CtxMW8uOFusnPmo6CE1pBtFXF4nDcJs.jpg,,,,,
188,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,4,9tx4ct,self.MachineLearning,Q,https://www.reddit.com/r/MachineLearning/comments/9tx4ct/q/,rashikukke,1541274773,[removed],0,1,False,https://b.thumbs.redditmedia.com/S4knakY8F-pZfecP7sJePSqPOO_yHzus3LrouX6jusc.jpg,,,,,
189,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,5,9txl5x,self.MachineLearning,Do dropout change the effect of normalization activities between layers?,https://www.reddit.com/r/MachineLearning/comments/9txl5x/do_dropout_change_the_effect_of_normalization/,aziz_22,1541278262,[removed],0,1,False,self,,,,,
190,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,6,9txq6o,arxiv.org,TallyQA: Answering Complex Counting Questions,https://www.reddit.com/r/MachineLearning/comments/9txq6o/tallyqa_answering_complex_counting_questions/,manoja328,1541279352,,4,1,False,default,,,,,
191,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,8,9typa3,self.MachineLearning,Empirical Eye Smart Module for Automation - Free Device Offer to developers of /r/MachineLearning !,https://www.reddit.com/r/MachineLearning/comments/9typa3/empirical_eye_smart_module_for_automation_free/,EmpiricalAutomation,1541287049,[removed],0,1,False,self,,,,,
192,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,8,9tyzzf,self.MachineLearning,[D] Oral Sessions Videos for EMNLP 2018?,https://www.reddit.com/r/MachineLearning/comments/9tyzzf/d_oral_sessions_videos_for_emnlp_2018/,tshrjn,1541289480,Does anyone know where we can find live/recorded videos for EMNLP 2018 tutorials as well as oral sessions something like it was for [last year](https://vimeo.com/channels/emnlp2017/videos/sort:plays/format:thumbnail)?,8,1,False,self,,,,,
193,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,9,9tz5y2,self.MachineLearning,Autoencoder with random loss function,https://www.reddit.com/r/MachineLearning/comments/9tz5y2/autoencoder_with_random_loss_function/,dalexanderch12,1541290786,[removed],0,1,False,self,,,,,
194,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,10,9tzl0y,payspi.org,Best Synthetic Urine Kits For Drug Test | Review Of The Best and Worst Fake Urine Brands 2018 - Pay,https://www.reddit.com/r/MachineLearning/comments/9tzl0y/best_synthetic_urine_kits_for_drug_test_review_of/,DorothyPiercek5,1541294280,,0,1,False,default,,,,,
195,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,10,9tztui,self.MachineLearning,[D] [R?] Best route for Machine Learning/Deep Learning on Terabytes of Data?,https://www.reddit.com/r/MachineLearning/comments/9tztui/d_r_best_route_for_machine_learningdeep_learning/,normalism,1541296328,"I am trying to figure out the best route to go in terms of hardware/environment for performing deep learning on terabytes of data. Data resides in SQL server. 

On prem multiple machine setup (distributed workload) ?

On prem single machine multiple Gpu setup?
",0,1,False,self,,,,,
196,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,12,9u0d70,self.MachineLearning,"What if there was a Google app like Snapseed, but everything is automatic?",https://www.reddit.com/r/MachineLearning/comments/9u0d70/what_if_there_was_a_google_app_like_snapseed_but/,alex22804,1541301009,[removed],0,1,False,self,,,,,
197,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,12,9u0fiq,github.com,A very simpe neural network implemention for python,https://www.reddit.com/r/MachineLearning/comments/9u0fiq/a_very_simpe_neural_network_implemention_for/,microic,1541301612,,0,1,False,default,,,,,
198,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,12,9u0o3h,self.MachineLearning,[News] Airbus posts Kaggle competition with major data leak,https://www.reddit.com/r/MachineLearning/comments/9u0o3h/news_airbus_posts_kaggle_competition_with_major/,tom2shoes,1541303908,"Airbus is offering a $60,000 dollar prize to the team that can best distinguish marine vessels in satellite images. Unfortunately the training dataset is just cropped images from the test dataset. WHOOPS!

[discovery thread on Kaggle](https://www.kaggle.com/c/airbus-ship-detection/discussion/64355)

[What is a data leak?](https://www.kaggle.com/dansbecker/data-leakage)

&amp;#x200B;",46,1,False,self,,,,,
199,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,13,9u0poo,self.MachineLearning,[D] How does neural network mimic non-linear feature-relations.,https://www.reddit.com/r/MachineLearning/comments/9u0poo/d_how_does_neural_network_mimic_nonlinear/,WillingCucumber,1541304321,"Hi all,

Probably some silly questions. Please help me understand the below three questions:

1.) How does neural network learn non-linear in terms of their features. 

Eg: if x&gt;2 and y&lt;3: then z, where x,y,z are input features to the network.

Also how does it learn non-linearity in terms of multiplication and power of features. One way I can think of is learning the weighted sum of the log of the features and then using some exponential activation.

The reason I am asking is that all the network does is matrix multiplication with the features.

I agree that we have non-linear activation functions, but I am not able to understand how they learn non-linearity in terms of features.

2.) ReLu seems to be mostly linear, how does it learn non-linearity.

3.) Also, how does sigmoid learn non-linearlity withing features. Eg: Consider  2 layer neural network with both activations as sigmoid, then output can be written as \~ e\^(w1\*e\^(w2\*x+w3\*y) + w4\*e\^(w5\*x+w6\*y)). This still doesn't seem to learn something like x\*y or non\_linear(x,y)

&amp;#x200B;

Thanks !!",13,1,False,self,,,,,
200,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,13,9u0wyn,youtu.be,Colab 8min tutorial for your free GPU train,https://www.reddit.com/r/MachineLearning/comments/9u0wyn/colab_8min_tutorial_for_your_free_gpu_train/,minsukheo,1541306407,,0,1,False,https://b.thumbs.redditmedia.com/V52_xFyqk0wMdqoJb8jfsDnDgXjvFweBGEC-SzRwmlI.jpg,,,,,
201,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,14,9u14n0,boingboing.net,Using machine learning to teach robots to get dressed,https://www.reddit.com/r/MachineLearning/comments/9u14n0/using_machine_learning_to_teach_robots_to_get/,asifrazzaq1988,1541308854,,0,1,False,default,,,,,
202,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,14,9u14qx,self.MachineLearning,"[P]Using WaveNet to ""speak"" output from a chatbot",https://www.reddit.com/r/MachineLearning/comments/9u14qx/pusing_wavenet_to_speak_output_from_a_chatbot/,CSGOvelocity,1541308888,"I am building a project that will use WaveNet and a chatbot in conjunction.

The chatbot will output a sentence and I would like WaveNet to ""speak"" it. The sentence need not be directly fed into WaveNet. I can always save it somewhere and have WaveNet ""read"" it from there.

I want it to be real-time or atleast nearly.

1. I found this implementation of WaveNet on GitHub [https://github.com/ibab/tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet). But how do I feed it the sentence for it to ""speak"". It can only generate raw audio. (If there is another implementation which can take sentences then please point me to it)
2. Can it be real-time on a PC ? (With GPU ofcourse)
3. Can it be real-time on a high end mobile device (if yes then what about a low end one)

Please ask for any further clarification that you want.",14,1,False,self,,,,,
203,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,14,9u19sb,arxiv.org,HotpotQA: a dataset with 113k Wikipedia-based question-answer pairs,https://www.reddit.com/r/MachineLearning/comments/9u19sb/hotpotqa_a_dataset_with_113k_wikipediabased/,netcribe,1541310623,,1,1,False,default,,,,,
204,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,16,9u1l7f,youtube.com,"CppCon 2018, Ubisoft predicts bugs in C++ commits using machine learning",https://www.reddit.com/r/MachineLearning/comments/9u1l7f/cppcon_2018_ubisoft_predicts_bugs_in_c_commits/,Red-Portal,1541314888,,0,1,False,https://b.thumbs.redditmedia.com/HQzqp6YXl1ZZnWLFyGNOcFO0cz0GCQ7bln0u5w8OZPk.jpg,,,,,
205,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,16,9u1q9e,self.MachineLearning,How to generate CPT in Bayesian network for EM algo? What are the best method to infering a number from dependent features; like speed -&gt; flow?,https://www.reddit.com/r/MachineLearning/comments/9u1q9e/how_to_generate_cpt_in_bayesian_network_for_em/,mamuncse30,1541316948,[removed],0,1,False,self,,,,,
206,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,16,9u1r4k,self.MachineLearning,Predicting the Higgs-Boson Signal,https://www.reddit.com/r/MachineLearning/comments/9u1r4k/predicting_the_higgsboson_signal/,andrea_manero,1541317292,[removed],0,1,False,self,,,,,
207,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,17,9u1wav,self.MachineLearning,[P] Help choosing the right tools,https://www.reddit.com/r/MachineLearning/comments/9u1wav/p_help_choosing_the_right_tools/,mohamedahmed95,1541319552,"Hi everyone, i am a total noob in ML, but not in programming. Currently I am working with two  professors on a paper, i will do the programming part. And they will do the theory and writing the paper. The idea is to apply bayesian network on data read from csv file. They suggested two tools ( GOBNLIP and SCIP). AFAIK these tools work on CPU. So to cut a long story short, i am thinking about suggesting tools that use GPU, since they are much faster. But I do not know if such tools exist. I really want to work with them, and they understand that I am new to this area so they are welling to give me time to learn.

Best regards.

Edit:

Will use python",14,1,False,self,,,,,
208,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,18,9u26gf,youtube.com,Logistic Regression Part 1 - Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9u26gf/logistic_regression_part_1_machine_learning/,jeffxu999,1541323943,,0,1,False,https://b.thumbs.redditmedia.com/h58aIAguqub4WDfXwehlvZuzv07oGQNmScW2d5DYDWs.jpg,,,,,
209,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,18,9u292n,self.MachineLearning,"[P] TF implementation of Text Deep CNN for Relation Extraction (NAACL, COLING paper)",https://www.reddit.com/r/MachineLearning/comments/9u292n/p_tf_implementation_of_text_deep_cnn_for_relation/,roomylee,1541325074,"[https://github.com/roomylee/cnn-relation-extraction](https://github.com/roomylee/cnn-relation-extraction)

 There is tensorflow implementation of "" **Relation Classification via Convolutional Deep Neural Network""** and "" **Relation Extraction: Perspective from Convolutional Neural Networks** **""**  (COLING, NAACL paper). It seems to be a great help in developing an Convolutional Neural Network based Text Classification Model. ",0,1,False,self,,,,,
210,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,20,9u2la8,self.MachineLearning,[D] How to not overfit to data quantization?,https://www.reddit.com/r/MachineLearning/comments/9u2la8/d_how_to_not_overfit_to_data_quantization/,cpury,1541329898,"Let's say you're creating a product trying to estimate this: **Given a tweet's text, how happy was that person when they wrote it?**  


You create a little annotation tool so that a team of annotators can quickly label thousands of these. The output should be an estimate between 0 and 1, but because that would be a lot of options to choose from, you quantize the values down to seven different values: 0, 0.17, 0.33, 0.5, 0.67, 0.83, 1. That should be plenty to (superficially) differentiate different levels of happiness while not compromising the speed of annotations.

&amp;#x200B;

Of course, you know this is a highly subjective matter, and different annotators would probably rate a very similar tweet very differently. You actually hope that this will be the case, so that the model can learn something of an average over a wide range of human opinions. For new data points, the model should be able to find fitting values on the continuous scale.

&amp;#x200B;

Now the problem: You realize your model spends a lot of effort into fitting the quantized values exactly. Most unseen tweets get one of those seven values you picked, only very rarely diverging into the ranges in between.

&amp;#x200B;

How would you approach this? Is my logic sound so far? I could not find any literature on the matter. Note that the model is already highly regularized with standard techniques like dropout etc., but it seems this is a kind of overfitting that needs a different approach.

&amp;#x200B;

My first idea was to design a loss function that only barely penalizes errors of less than half the quantization step (0.085 in this case). This might give the model the wiggle room to focus on being in the correct range without overfitting to the exact values. But I'm not sure how best to design such a custom loss function, and there does not seem to be much literature either.",28,1,False,self,,,,,
211,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,21,9u2y7v,self.MachineLearning,Mathematical background for ML and DL,https://www.reddit.com/r/MachineLearning/comments/9u2y7v/mathematical_background_for_ml_and_dl/,MoonAmunet,1541334396,[removed],0,1,False,self,,,,,
212,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,21,9u300t,github.com,[P] implementation of Google Brain's DropBlock: A regularization method for convolutional networks (PyTorch),https://www.reddit.com/r/MachineLearning/comments/9u300t/p_implementation_of_google_brains_dropblock_a/,inkognit,1541334935,,1,1,False,default,,,,,
213,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,22,9u3ane,motherboard.vice.com,"This Researcher Created DeepCreamPy, a Machine Learning Algorithm That Uncensors Hentai",https://www.reddit.com/r/MachineLearning/comments/9u3ane/this_researcher_created_deepcreampy_a_machine/,MuzzleO,1541337892,,0,1,False,default,,,,,
214,MachineLearning,t5_2r3gv,2018-11-4,2018,11,4,22,9u3ffa,self.MachineLearning,How/ Where can I collect data from?,https://www.reddit.com/r/MachineLearning/comments/9u3ffa/how_where_can_i_collect_data_from/,AlienX_B10,1541339167,[removed],0,1,False,self,,,,,
215,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,0,9u3ymd,self.MachineLearning,Why tensorflow is still using tanh as an activation function and not ELU activation?,https://www.reddit.com/r/MachineLearning/comments/9u3ymd/why_tensorflow_is_still_using_tanh_as_an/,aziz_22,1541343713,"I wonder why tensorflow is still using tanh as the default activation function in most of its implementation.

Why they didn't consider using the ELU or the leakyRelu (and its variants)?

&amp;#x200B;",0,1,False,self,,,,,
216,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,0,9u41ck,self.MachineLearning,[P] Image recognition and where to start,https://www.reddit.com/r/MachineLearning/comments/9u41ck/p_image_recognition_and_where_to_start/,LavinaBBGK,1541344306,"I am a highschool student with moderate education in programming. For my senior project, I want to make a program that recognizes music notes from a PDF file and does something based on that. Where ans how do I start  learning about this? I know very little on neural networks, and machine learning.  ",4,1,False,self,,,,,
217,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,0,9u4dsm,self.MachineLearning,Thoughts on my DeepDream video?,https://www.reddit.com/r/MachineLearning/comments/9u4dsm/thoughts_on_my_deepdream_video/,erinlewisfudge,1541346959,"Not usually fond of asking people to view my videos. But, I'm really curious as to what everybody makes of this new video. Thanks for your time, erinlewisfudge.

&amp;#x200B;

[https://www.youtube.com/watch?v=59yK9HIjWSE](https://www.youtube.com/watch?v=59yK9HIjWSE)",0,1,False,self,,,,,
218,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,2,9u4ybm,link.springer.com,What do you think of Compositionality as being one of the main reasons explaining deep learning?,https://www.reddit.com/r/MachineLearning/comments/9u4ybm/what_do_you_think_of_compositionality_as_being/,real_pinocchio,1541350986,,0,1,False,default,,,,,
219,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,2,9u519m,self.MachineLearning,[D] Visualizing and analyzing error landscapes,https://www.reddit.com/r/MachineLearning/comments/9u519m/d_visualizing_and_analyzing_error_landscapes/,CSartistInTraining,1541351546,"It's difficult to visualize and understand the high dimensional error landscapes (ie cost functions) of neural nets and other machine learning algorithms.

A common method is to project the parameter space onto two dimensions and plot a surface. What are some effective choices for this projection that help visualize salient features of the error? Are there nonlinear approaches that are better?

More importantly, what is known about the geometry of these cost functions for neural networks trained on real data? Dinner theoretical papers use random functions as a proxy, but I'm suspicious that these random functions don't do a good job of representing the important features of real cost functions.

It seems like this would be a rich area for research, but you don't really see much about it. What am I missing?",17,1,False,self,,,,,
220,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,2,9u563n,self.MachineLearning,How do I get better at inference methods and probabilistic approaches in general?,https://www.reddit.com/r/MachineLearning/comments/9u563n/how_do_i_get_better_at_inference_methods_and/,chain20,1541352465,[removed],0,1,False,self,,,,,
221,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,2,9u5a1a,techmano.in,Real-Time sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/9u5a1a/realtime_sentiment_analysis/,Manoharan-D,1541353211,,0,1,False,https://b.thumbs.redditmedia.com/0aZHO7-Nhet8HRHWW0ZDVVMN3JSJALHR_4o-JSmItvE.jpg,,,,,
222,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,2,9u5b78,self.MachineLearning,HELP : How to classify e-commerce products with only product images and descriptions but no labels ?,https://www.reddit.com/r/MachineLearning/comments/9u5b78/help_how_to_classify_ecommerce_products_with_only/,Nike_Zoldyck,1541353422,[removed],0,1,False,self,,,,,
223,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,4,9u6ajm,self.MachineLearning,[P] Empirical Eye Device for Machine Learning - Collect and Stream Data - Free offer to devs of /r/MachineLearning!,https://www.reddit.com/r/MachineLearning/comments/9u6ajm/p_empirical_eye_device_for_machine_learning/,EmpiricalAutomation,1541359906,"Hey guys, 

I want to offer a free development device to reddit's Machine Learning community, in the hopes that more exciting software can be developed for automation, machine vision, and robotics. 

Our company has just released a sensor + computer module called the Empirical Eye. It's basically a stereo vision system + on-board embedded CPU+GPU + WiFi (plus some mounting straps). It's meant to be easily attached to physical machines, to collect and stream data, and enable novel machine learning applications. We've made our own ""visual quality control"" software with the device. 

We want to make life easier for running ML experiments and developing new applications. The primary motivation has been the field of robotics, but we see it as a flexible module that can be attached to any number of devices or machines, so feel free to be creative :) 

We're giving away devices to developers for feedback and troubleshooting, which will help us refine it. We're looking to do another production run in a month or so, with a cleaner industrial design that'll look a bit more professional. Upon delivery you'll get the device's software API. It's a Linux OS not that different from a Raspberry Pi OS, and we have basic data collection + streaming scripts written in Python for your convenience. 

So sign up below and let us know what you want to develop! 

www.empiricalautomation.com 

Happy to answer any questions, cheers! ",8,1,False,self,,,,,
224,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,5,9u6sxq,pgaleone.eu,Tensorflow 2.0: models migration and new design,https://www.reddit.com/r/MachineLearning/comments/9u6sxq/tensorflow_20_models_migration_and_new_design/,pgaleone,1541363425,,0,1,False,https://b.thumbs.redditmedia.com/N7ggsobkHRlDvDJ0Mh_8gf5YsFDctHWIL2tnE6juPKI.jpg,,,,,
225,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,5,9u71d0,self.MachineLearning,"[P] EXP: a python tool/lib for experiment design, parallel deployment, and parameter optimization",https://www.reddit.com/r/MachineLearning/comments/9u71d0/p_exp_a_python_toollib_for_experiment_design/,davex32,1541365047,"From time to time, I found myself writing some scaffolding code to run experiments in parallel and specify parameter spaces to be explored by a particular model. To speed-up the process, I wrote [exp](https://github.com/davidenunes/exp), a small set of tools / lib to quickly design experiments and deploy them in parallel. I also added an utility for hyperparameter optimization.  


This is intended for the a scenario where you have a single machine with multiple cores or GPU units.

  
If anyone finds this useful, has feature requests, or wants to contribute in some way do let me know, or use the issues in the repository. This is just a small tool I made to help with my research, but I wanted to share it nonetheless.  
",2,1,False,self,,,,,
226,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,5,9u71km,self.MachineLearning,Question about Principal Component Analysis for a dataset with some categorical variables.,https://www.reddit.com/r/MachineLearning/comments/9u71km/question_about_principal_component_analysis_for_a/,Curious_Fennel,1541365086,[removed],0,1,False,self,,,,,
227,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,5,9u71rl,youtube.com,KNN K Nearest Neighbors Part 1 - Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9u71rl/knn_k_nearest_neighbors_part_1_machine_learning/,jeffxu999,1541365123,,0,1,False,default,,,,,
228,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,8,9u87pn,declanoller.com,Training an RL agent to play Puckworld with a DDQN and Pytorch,https://www.reddit.com/r/MachineLearning/comments/9u87pn/training_an_rl_agent_to_play_puckworld_with_a/,diddilydiddilyhey,1541373480,,0,1,False,default,,,,,
229,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,9,9u8wob,self.MachineLearning,How to use a Deep Learning model to predict proportions between 0 and 1?,https://www.reddit.com/r/MachineLearning/comments/9u8wob/how_to_use_a_deep_learning_model_to_predict/,bizzarebrains,1541378826,"I am currently working with sequences, and using k-mers (k = 1,2,3) as features, with a GLM to predict a proportion (outcome variable is continuous beta distribution). However, the k mers don't capture the context-dependent information of the entire sequence. I'm not sure how to adapt a Deep Learning model for this problem. ",0,1,False,self,,,,,
230,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,10,9u91b7,github.com,[P] HiddenLayer: Neural network graphs and training metrics for PyTorch and Tensorflow,https://www.reddit.com/r/MachineLearning/comments/9u91b7/p_hiddenlayer_neural_network_graphs_and_training/,waleedka,1541379897,,0,1,False,https://a.thumbs.redditmedia.com/fJrVRtDbJRXDPv577gyXY3WEtj2iqI0K1KD9NiewVx4.jpg,,,,,
231,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,10,9u93hm,self.MachineLearning,[P] HiddenLayer: Neural network graphs and training metrics for PyTorch and Tensorflow,https://www.reddit.com/r/MachineLearning/comments/9u93hm/p_hiddenlayer_neural_network_graphs_and_training/,waleedka,1541380392,"[https://github.com/waleedka/hiddenlayer](https://github.com/waleedka/hiddenlayer)

&amp;#x200B;

A lightweight library for neural network graphs and training metrics for PyTorch and Tensorflow. Useful for quick experimentation, and works great with Jupyter Notebook. MIT License. ",5,1,False,self,,,,,
232,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,10,9u99eb,self.MachineLearning,"Any instances of autoencoders being used to cluster unlabeled data, leading to interesting clusters?",https://www.reddit.com/r/MachineLearning/comments/9u99eb/any_instances_of_autoencoders_being_used_to/,pocketMAD,1541381678,"I was thinking of taking a dataset and projecting it into 2D space  using an autoencoder. There's a dataset of house characteristics (size, price, age, etc.) available on Kaggle, and I was thinking of doing it with that. Any instances of something like this being done? I wanna see if I'll produce anything interesting. ",0,1,False,self,,,,,
233,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,10,9u9etn,self.MachineLearning,Preventing adversarial examples?,https://www.reddit.com/r/MachineLearning/comments/9u9etn/preventing_adversarial_examples/,tundrat,1541382888,"I'm a new and amateur AI programmer and I read [this](https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196) some time ago. And a simple idea kept bugging me since. Again, I'm no expert, but can't we simply add random, tiny blurring or pixel changes to the input before analyzing it?  
  
P.S. That entire series of articles is very fun and explains a lot of stuff in an easy, intuitive way! :D",0,1,False,self,,,,,
234,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,11,9u9n9u,self.MachineLearning,How to load an tensorflow Estimator from model files saved using estimator.export_savedmodel,https://www.reddit.com/r/MachineLearning/comments/9u9n9u/how_to_load_an_tensorflow_estimator_from_model/,alegan54,1541384766,[removed],0,1,False,self,,,,,
235,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,12,9ua3np,github.com,Tensorflow implementation of the paper: StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks.,https://www.reddit.com/r/MachineLearning/comments/9ua3np/tensorflow_implementation_of_the_paper_starganvc/,hujinsen,1541388484,,0,1,False,default,,,,,
236,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,13,9uac19,self.MachineLearning,DTW as a feature for K-means clustering?,https://www.reddit.com/r/MachineLearning/comments/9uac19/dtw_as_a_feature_for_kmeans_clustering/,AAD2,1541390409,[removed],0,1,False,self,,,,,
237,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,13,9ual0w,techmano.in,Real-Time sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/9ual0w/realtime_sentiment_analysis/,Manoharan-D,1541392697,,0,1,False,https://b.thumbs.redditmedia.com/0aZHO7-Nhet8HRHWW0ZDVVMN3JSJALHR_4o-JSmItvE.jpg,,,,,
238,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,13,9uap7c,self.MachineLearning,[D] Full solutions to Bishop's Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/9uap7c/d_full_solutions_to_bishops_machine_learning/,cbsudux,1541393794,,9,1,False,self,,,,,
239,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,14,9uar7n,self.MachineLearning,[P] Dialogflow vs Amazon Lex to build a health appointment assistant. Anyone have experience in either?,https://www.reddit.com/r/MachineLearning/comments/9uar7n/p_dialogflow_vs_amazon_lex_to_build_a_health/,dcn20002,1541394316,"I am deciding between the two platforms to build a navigator to help users find medical appointment at the right price. MVP at this point but will scale quickly.

My three criteria are: Ease of integration with backend. UI Design customizability (for ReactJS). And scalability. 

I used Dialogflow before and Its relatively easy with a lot of integration (with phone in beta), seems like a one stop shop, the voice, however doesnt match that of Google Assistant as it has a more primitive version for some reason. I have not dabbled into Amazon Lex but heard great things, and since AWS tools are easily integrated with each other thats a plus.

&gt;&gt; I just wonder any of you have an experience with both or either who can give me some insights on how I should make my decision? And perhaps some pros and cons? ",4,1,False,self,,,,,
240,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,14,9uargt,self.MachineLearning,AI Stock Photo Generator???,https://www.reddit.com/r/MachineLearning/comments/9uargt/ai_stock_photo_generator/,clarenceappendix,1541394384,[removed],0,1,False,self,,,,,
241,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,14,9uathc,jigsawacademy.com,Machine Learning and AI Bootcamp,https://www.reddit.com/r/MachineLearning/comments/9uathc/machine_learning_and_ai_bootcamp/,fullstackanalytics1,1541394900,,0,1,False,default,,,,,
242,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,14,9uaz3m,self.MachineLearning,"[D] 2019 AI Residency Thread: program deadlines, interview info sharing, etc.",https://www.reddit.com/r/MachineLearning/comments/9uaz3m/d_2019_ai_residency_thread_program_deadlines/,shenglih,1541396424,So far the only ones open for applications are Google AI and OpenAI. Interviews are supposed to start in November... Has anyone got any invites?,116,1,False,self,,,,,
243,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,14,9ub19z,self.pseudorandomcoder,Results of my computer learning tic tac toe with epsilon greedy monte carlo method,https://www.reddit.com/r/MachineLearning/comments/9ub19z/results_of_my_computer_learning_tic_tac_toe_with/,pseudorandomcoder,1541397047,,0,1,False,https://b.thumbs.redditmedia.com/UdDAzjI55OXfQ8unPjCVRQ2C6thyeJ2SiWvP1fwlksM.jpg,,,,,
244,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,15,9ub3yx,top500.org,Neuromorphic Supercomputer Hits a Million Cores,https://www.reddit.com/r/MachineLearning/comments/9ub3yx/neuromorphic_supercomputer_hits_a_million_cores/,Marha01,1541397840,,0,1,False,https://b.thumbs.redditmedia.com/ILJb1Iq_xNXAtvB6b9Z4Ne3x8sJUUxIRfMcT3HOguVY.jpg,,,,,
245,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,15,9ub6rr,self.MachineLearning,Content-based filtering vs Collaborative Filtering,https://www.reddit.com/r/MachineLearning/comments/9ub6rr/contentbased_filtering_vs_collaborative_filtering/,AdministrativeYard,1541398647,[removed],0,1,False,self,,,,,
246,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,16,9ubi37,self.MachineLearning,Difference Between Artificial Intelligence and Cognitive Computing: Athenas Tech Head Answers the Question,https://www.reddit.com/r/MachineLearning/comments/9ubi37/difference_between_artificial_intelligence_and/,athenagt,1541402071," 

What is the difference between artificial intelligence and cognitive  computing?  This is one of the most frequently asked questions I come  across during my interactions with clients, candidates and also peers.

There is no doubt how the scope and implementation of AI and machine  learning are increasing with each passing day. With the rising  popularity, there has been a tentative curiosity about understanding the  technological jargons. The most searched digital transformation jargons  on Google are  machine learning, deep learning, cognitive technology,  neural networks, Natural Language Processing, augmented data discovery,  synthetic intelligence, and text mining to name a few.

With so many different kinds of content available online, I know its  overwhelming to have a clear understanding of these cutting-edge  technologies. This has further given rise to a misconception that  artificial intelligence and cognitive computing are the same.

Heres an attempt to share my views on how different is artificial  intelligence from cognitive computing with respect to the similarities  existing between these two very related fields. Continue reading!

### Understanding the concept 

[AI is the science of making computers do things that require intelligence when done by humans. ](https://twitter.com/intent/tweet?url=http://blog.athenagt.com/difference-between-artificial-intelligence-and-cognitive-computing-athenas-tech-head-answers-the-questions/&amp;text=AI%20is%20the%20science%20of%20making%20computers%20do%20things%20that%20require%20intelligence%20when%20done%20by%20humans.&amp;related)[Click To Tweet](https://twitter.com/intent/tweet?url=http://blog.athenagt.com/difference-between-artificial-intelligence-and-cognitive-computing-athenas-tech-head-answers-the-questions/&amp;text=AI%20is%20the%20science%20of%20making%20computers%20do%20things%20that%20require%20intelligence%20when%20done%20by%20humans.&amp;related) 

Correct me if I refer to AI as an umbrella that covers all sorts of processes, theories, algorithms and technical affiliations.

### What is AI?

As per Wikipedia,

&gt;Artificial intelligence, sometimes called machine  intelligence, is intelligence demonstrated by machines, in contrast to  the natural intelligence displayed by humans and other animals. In  computer science, AI research is defined as the study of intelligent  agents: any device that perceives its environment and takes actions  that maximize its chance of successfully achieving its goals.  Colloquially, the term artificial intelligence is applied when a  machine mimics cognitive functions that humans associate with other  human minds, such as learning and problem-solving.

These days in the artificial intelligence space, both small scale and  large scale companies continue to progress in the areas of procedural  automation, acquiring skills on the machine-readable format, and  execution of these complex yet recurring tasks.

AI enables computers to execute tasks that were performed by human  intelligence until now. Implying that machine learning, robotics, NLP,  synthetic intelligence and text mining are all a part of artificial  intelligence and are correlated in some way or the other.

### What is cognitive computing?

&gt;Cognitive systems are designed to solve problems the way humans solve problems, by thinking, reasoning, and remembering.

Cognitive computing, on the other hand, is a sub-field of artificial  intelligence. This synthetic technology is majorly based on reasoning  and understanding at a much higher level. Cognitive technology, in a  way, tries to match the level of the human apprehension, as it is  designed to make human-like decisions in complex situations.

Based on the reasoning capability, cognitive systems are able to  learn quickly and adapt as new data arrives. They explore and understand  things that earlier machines had failed and humans ignored.

### Understanding the difference between AI and cognitive technology

The major difference between artificial intelligence and cognitive  computing is that it efficiently deals with mammoth-sized data,  comprehensive rounds of analytics, albeit humans have complete control  of the decision-making process.

[Artificial intelligence solutions](http://www.athenagt.com/)  and cognitive computing systems are designed on the concept that  machines should be able to sense, reason, act and adapt based on learned  experience.

### Let me explain the difference with a use case 

&amp;#x200B;

[AI &amp; Cognitive Computing](https://i.redd.it/hyshs7dqpgw11.png)

Lets imagine a scenario where a tech professional is looking for a  career change. An AI-led assistant will assess his skills and provide  him job opportunities which match his expertise. AI will also negotiate  the remuneration and benefits and inform the candidate that a decision  has been made on his behalf.

Now if we replace this AI assistant with a cognitive assistant, the  latter will suggest potential career paths apart from providing him  significant information like certification requirements, the market  value of his profile and overseas opportunities. However, in the case of  cognitive, the final call will always be made the candidate unlike in  the case of the AI assistant.

***To summarize *** 

Cognitive computing helps us make smarter decisions by enabling us to  leverage its capabilities. Artificial intelligence instead is designed  to better decisions on our behalf by substituting our actions with its  deep learning processes.

### AI vs cognitive computing in terms of development

Now that we are aware of the basic differences between artificial  intelligence and cognitive computing, lets find out how and where both  of these are advancing 

[McKinsey  estimates AI techniques have the potential to create between $3.5T and  $5.8T in value annually across nine business functions in 19 industries.  ](https://twitter.com/intent/tweet?url=http://blog.athenagt.com/difference-between-artificial-intelligence-and-cognitive-computing-athenas-tech-head-answers-the-questions/&amp;text=McKinsey%20estimates%20AI%20techniques%20have%20the%20potential%20to%20create%20between%20%243.5T%20and%20%245.8T%20in%20value%20annually%20across%20nine%20business%20functions%20in%2019%20industries.&amp;related)[Click To Tweet](https://twitter.com/intent/tweet?url=http://blog.athenagt.com/difference-between-artificial-intelligence-and-cognitive-computing-athenas-tech-head-answers-the-questions/&amp;text=McKinsey%20estimates%20AI%20techniques%20have%20the%20potential%20to%20create%20between%20%243.5T%20and%20%245.8T%20in%20value%20annually%20across%20nine%20business%20functions%20in%2019%20industries.&amp;related) 

It is estimated that the worth of artificial intelligence market will  be $190.61 billion by 2025. AI has inarguably become the fundamental  element behind every futuristic technological invention. Enterprises,  these days, are leveraging this technology along with data science to  disrupt their business growth. AI, data science, IoT and blockchain are  the essential components of the digital transformation era. AI has been  rigorously implemented in those three major verticals 

1. Chatbots
2. Virtual Assistants
3. Smart Advisors

As per reports, cognitive technology will account for a 31.0% CAGR in  terms of revenue. Its net worth was US$ 9850 million in 2017 and it is  estimated to reach US$ 49800 million by 2023.

This synthetic learning technology is induced with diagnostic,  predictive, and prescriptive analytics tools. With the help of these  capabilities, it observes, learns, and offers deep insights for the user  to perform actions. Cognitive has been rigorously implemented in those  four major verticals 

1. Banking
2. Healthcare
3. Media
4. Entertainment

### Concluding Note

All I can say is that both artificial intelligence and cognitive  computing are closely similar in the intent, but they differ in their  capabilities to mimic human intelligence. 

To Know More [Click Here](https://www.athenagt.com/artificialintelligence)",0,1,False,https://b.thumbs.redditmedia.com/WagMZfziFW1FMtQjFtYs_sWIejL9hyUsrd9dzOKrdpA.jpg,,,,,
247,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,16,9ubke2,self.MachineLearning,6 Ways Machine Learning Leveraged by Fintechs to Outshine Competition,https://www.reddit.com/r/MachineLearning/comments/9ubke2/6_ways_machine_learning_leveraged_by_fintechs_to/,athenagt,1541402852,[removed],0,1,False,self,,,,,
248,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,17,9ubv0z,reddit.com,The troubling rise of 'deep fakes': How AI is making it hard to know what's real,https://www.reddit.com/r/MachineLearning/comments/9ubv0z/the_troubling_rise_of_deep_fakes_how_ai_is_making/,kozen23,1541406603,,0,1,False,default,,,,,
249,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,17,9ubxr8,medium.com,Machine Learning with graphs,https://www.reddit.com/r/MachineLearning/comments/9ubxr8/machine_learning_with_graphs/,infinite-Joy,1541407606,,0,1,False,default,,,,,
250,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,17,9ubyz5,github.com,"XGBoost v0.81: new features(larger scale distributed experience), learning to rank, gpu support",https://www.reddit.com/r/MachineLearning/comments/9ubyz5/xgboost_v081_new_featureslarger_scale_distributed/,rohan36,1541408046,,0,1,False,default,,,,,
251,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,18,9uc81g,self.MachineLearning,[D] Struggling to implement Curiosity Module using OpenAI Baselines,https://www.reddit.com/r/MachineLearning/comments/9uc81g/d_struggling_to_implement_curiosity_module_using/,cranthir_,1541411205,"Hey there!

I'm trying to implement an open source Curiosity Module as a plugin in OpenAI Baselines for my next article of [Deep Reinforcement Learning Course](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/). However it is much complex than I thought and **I have some implementation problems and hence some questions.**

Github repo of the implementation \[Work in Progress\]: [https://github.com/simoninithomas/Baselines\_icm](https://github.com/simoninithomas/Baselines_icm)

To run it: `python -m` [`baselines.run`](https://baselines.run) `--alg=ppo2 --env=SuperMarioBros-Nes --network=cnn --num_timesteps=2e7`

&amp;#x200B;

To explain rapidly, I created a ICM (Intrinsic Curiosity Module) called [icm.py](https://icm.py) that will be plugged in PPO2 [ppo.py](https://ppo.py) :

\- Initialized in learn() (line 366 [ppo2.py](https://ppo2.py)) 

\- Generate the intrinsic rewards in Runner run() using icm.calculate\_intrinsic\_rewards() (line 209 ppo.py)

\- Trained in (not defined yet) 

&amp;#x200B;

[ICM](https://i.redd.it/va5cujb8ehw11.jpg)

But I have 3 questions about the implementation:

\- *About the Curiosity Rewards generation*, I decided to generate it inside the [runner.run](https://runner.run)() function. In fact to generate curiosity I need state (line 194) , next\_state (line 204) and action.

In fact I receive \[number\_of\_environments, state\_shape\], \[number\_of\_environements, next\_state\_shape\] but I don't return \[number\_of\_environements, curiosity\] but only one curiosity reward (because curiosity is calculated using the forward loss)  --&gt; **does it means that we use that curiosity for all the elements in the batch? Or do I need to find a way to calculate the curiosity of each state next state of the batch one by one ?**

&amp;#x200B;

\- *About where to train ICM*? In fact the problem is that I think the only place I can train it is in the same place than generating the rewards (in [runner.run](https://runner.run)() ). However it means that training with a small batch\_size (since even with some GPU we will not have more than 24 environments). **--&gt; Do you think that there is a better strategy than that?**

&amp;#x200B;

\- *About the GAE (General Advantage Estimation)?* OpenAI baselines uses GAE, however in ICM article they explain that we need to get rid of the terminal state. So what I done in line 225 is that I removed the terminal state for calculating GAE but **I'm not sure at all that's correct?** 

&amp;#x200B;

https://i.redd.it/f6zb4j2e9hw11.png

Thanks for your help!",2,1,False,https://b.thumbs.redditmedia.com/KqJd2_xHEiW-EKZm8f--qitS31UiVdzMrawLfwzRXyE.jpg,,,,,
252,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,18,9uc93z,self.MachineLearning,Image recognition with a video feed,https://www.reddit.com/r/MachineLearning/comments/9uc93z/image_recognition_with_a_video_feed/,elJrawat,1541411585,[removed],0,1,False,self,,,,,
253,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,19,9ucdca,self.MachineLearning,A fast implementation of Orthogonal Matching Pursuit in MATLAB C Extension,https://www.reddit.com/r/MachineLearning/comments/9ucdca/a_fast_implementation_of_orthogonal_matching/,shailesh1729,1541412967,[removed],0,1,False,self,,,,,
254,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,19,9ucggg,self.MachineLearning,Predictive failure analysis for refrigerator,https://www.reddit.com/r/MachineLearning/comments/9ucggg/predictive_failure_analysis_for_refrigerator/,attitude95,1541414001,[removed],0,1,False,self,,,,,
255,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,20,9uclsu,self.MachineLearning,[Project] A fast implementation of Orthogonal Matching Pursuit in MATLAB C Extension,https://www.reddit.com/r/MachineLearning/comments/9uclsu/project_a_fast_implementation_of_orthogonal/,shailesh1729,1541415730,[removed],0,1,False,self,,,,,
256,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,20,9ucoav,self.MachineLearning,[P] Parallel Gensim implementation of Graph2Vec (MLGWorkshop 2017).,https://www.reddit.com/r/MachineLearning/comments/9ucoav/p_parallel_gensim_implementation_of_graph2vec/,benitorosenberg,1541416487,"## [https://github.com/benedekrozemberczki/graph2vec](https://github.com/benedekrozemberczki/graph2vec)

Graph2Vec  is an embedding algorithm which learns representations for a set of  graphs using implicit factorization. The procedure places graphs in an  abstract feature space where graphs with similar structural properties  (Weisfehler-Lehman features) are clustered together. Graph2Vec has a  linear runtime complexity in the number of graphs in the dataset which  makes it extremely scalable. This specific implementation supports  multi-core data processing in the feature extraction and factorization  phases. (So far this is the only implementation which support multi-core  processing in every phase).

&amp;#x200B;

https://i.redd.it/x0igsp1zwhw11.jpg",3,1,False,https://b.thumbs.redditmedia.com/5zT1xC9-dWVEURZYirDGQnRhFEjA40dDAyNmna9fqdM.jpg,,,,,
257,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,20,9ucq7z,arxiv.org,[R] Multiple-Attribute Text Style Transfer,https://www.reddit.com/r/MachineLearning/comments/9ucq7z/r_multipleattribute_text_style_transfer/,CaHoop,1541417104,,1,1,False,default,,,,,
258,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,20,9ucqq9,arxiv.org,[R] Deep Counterfactual Regret Minimization,https://www.reddit.com/r/MachineLearning/comments/9ucqq9/r_deep_counterfactual_regret_minimization/,int8blog,1541417271,,1,1,False,default,,,,,
259,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,21,9ud07r,self.MachineLearning,What are the inputs for a self-attention mechanism?,https://www.reddit.com/r/MachineLearning/comments/9ud07r/what_are_the_inputs_for_a_selfattention_mechanism/,ReasonablyBadass,1541420066,[removed],0,1,False,self,,,,,
260,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,21,9ud5k0,govtech.com,[N] Harvard Converts Millions of Legal Documents into Open Data,https://www.reddit.com/r/MachineLearning/comments/9ud5k0/n_harvard_converts_millions_of_legal_documents/,zhamisen,1541421496,,0,1,False,https://b.thumbs.redditmedia.com/ms12xy0c14UX_8L0ya-79Fd4RpQZhVlxyHjw_2A2Haw.jpg,,,,,
261,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,22,9uddgd,self.MachineLearning,What is the best detection algorithm for a small object?,https://www.reddit.com/r/MachineLearning/comments/9uddgd/what_is_the_best_detection_algorithm_for_a_small/,whikwon,1541423504,[removed],0,1,False,self,,,,,
262,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,22,9udis3,self.MachineLearning,Simultaneous face and body detection,https://www.reddit.com/r/MachineLearning/comments/9udis3/simultaneous_face_and_body_detection/,Alex-S-S,1541424816,[removed],0,1,False,self,,,,,
263,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,22,9udkda,technologyreview.com,Who needs Copernicus if you have machine learning?,https://www.reddit.com/r/MachineLearning/comments/9udkda/who_needs_copernicus_if_you_have_machine_learning/,as_ninja6,1541425175,,0,1,False,default,,,,,
264,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,23,9ue20b,medium.com,[D] Generating Letters Using Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/9ue20b/d_generating_letters_using_generative_adversarial/,brnko,1541428975,,0,1,False,default,,,,,
265,MachineLearning,t5_2r3gv,2018-11-5,2018,11,5,23,9ue5rd,self.MachineLearning,[N] SpiNNaker: world's largest neuromorphic supercomputer switched on,https://www.reddit.com/r/MachineLearning/comments/9ue5rd/n_spinnaker_worlds_largest_neuromorphic/,P4TR10T_TR41T0R,1541429743,"[https://www.manchester.ac.uk/discover/news/human-brain-supercomputer-with-1million-processors-switched-on-for-first-time/](https://www.manchester.ac.uk/discover/news/human-brain-supercomputer-with-1million-processors-switched-on-for-first-time/)

SpiNNaker: Spiking Neural Network Architecture

It is designed to support spiking neural networks, which are much more similar than ANNs to biological neural networks. It is funded by the Human Brain Project. What do you guys think? Will this kind of architectures (neuromorphic computing) enable much progress in the Spiking Neural Network field?",65,1,False,self,,,,,
266,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,1,9uetxw,self.MachineLearning,"New to ML, fundamental questions",https://www.reddit.com/r/MachineLearning/comments/9uetxw/new_to_ml_fundamental_questions/,yingzhang1109,1541434344,"Hi, everyone, I am new to machine learning. When I research on it, I found there were number of library we can use, such as tensorflow, scikit learn, pytorch and so on. Whats the difference among them? Any advantage or disadvantage? Thanks in advance!",0,1,False,self,,,,,
267,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,2,9ufbg1,self.MachineLearning,with AI learning to code will became useless?,https://www.reddit.com/r/MachineLearning/comments/9ufbg1/with_ai_learning_to_code_will_became_useless/,luchins,1541437580,"when AI will automate the code jobs, the coders will go to fuck off, and then the job market will require always less coders?   
In which kind of time frame will this happen?  


I am learning C and java and I don't want to make things for nothing.  


Would you instead suggest me to learn machine learning?   


P.S. I have no degree, I am self-taught  


I  have already a good understanding of statistics. unsupervised models,  and a lot of things. I would like to imporve my global skills with  programming and going into the depth of machine learning.  
But there is one problem: I am 29 years old, no bacherlor, and no previous experience as data scientist or coder.  
What would you suggest me?",0,1,False,self,,,,,
268,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,2,9ufflf,self.MachineLearning,Want a PHD in machine learning. What classes to take to be competitive?,https://www.reddit.com/r/MachineLearning/comments/9ufflf/want_a_phd_in_machine_learning_what_classes_to/,MachineInTheStone,1541438310,"Hey, I'm a junior computer science major. I can take up to 4 free classes of my choosing. I'm going for a mathematics minor, which includes:
Calc 3, Linear Algebra, Differential Equations, intro to Probability, intor to stats inference. So, I got those down.

Here are the possible classes that I can take:
Option 1: Here's a link to my university classes:
https://catalog.buffalo.edu/courses/mathematics.html

Option 2: I just post the name of the classes and you guys can take a look:
. Regression Analysis
. Analysis of Variance
. Introduction to Higher Mathematics
. Introduction to Abstract Algebra
.  Advanced Linear Algebra
. Introduction to Complex Variables I
. Introduction to Complex Variables II
.  Introduction to Topology I
.  Introduction to Topology II
. Introduction to the Theory of Numbers I
. Introduction to the Theory of Numbers II
.Introduction to Real Variables I
. Introduction to Real Variables II
. Introduction to Cryptography
. Introduction to Numerical Analysis I
. Introduction to Numerical Analysis II
. Fundamentals of Applied Mathematics I
. Fundamentals of Applied Mathematics II
. Data-Oriented Computing for Mathematics",0,1,False,self,,,,,
269,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,2,9ufj1z,self.MachineLearning,Where to rent my GPU?,https://www.reddit.com/r/MachineLearning/comments/9ufj1z/where_to_rent_my_gpu/,FancyFurry,1541438932,[removed],0,1,False,self,,,,,
270,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,2,9ufl06,neuronto.com,Neuronto WordPress DeepL plugin translates your Website much better than Google,https://www.reddit.com/r/MachineLearning/comments/9ufl06/neuronto_wordpress_deepl_plugin_translates_your/,sebastiandiamond,1541439254,,0,1,False,default,,,,,
271,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,3,9ug37a,towardsdatascience.com,"Slitherin - Solving the classic game of Snake with AI (Part 1: Domain Specific - {Shortest,Longest}Path, Hamiltonian Cycle, DNN)",https://www.reddit.com/r/MachineLearning/comments/9ug37a/slitherin_solving_the_classic_game_of_snake_with/,g_surma,1541442333,,0,1,False,https://b.thumbs.redditmedia.com/OX4CSKvmg0NB8TcnpEBRs0kUDxsRZzNkBmRZPIyk-as.jpg,,,,,
272,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,3,9ug7fp,self.MachineLearning,Free Big Data event this Thursday,https://www.reddit.com/r/MachineLearning/comments/9ug7fp/free_big_data_event_this_thursday/,Jessica-100,1541443071,Just sharing:  [https://www.eventbrite.com/e/inspire-big-data-summit-2018-tickets-52016337265?aff=reddit](https://www.eventbrite.com/e/inspire-big-data-summit-2018-tickets-52016337265?aff=reddit),0,1,False,self,,,,,
273,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,4,9ugjj8,self.MachineLearning,"Is there a good trained speech recognition model that finds the phonemes from speech, given a .wav file?",https://www.reddit.com/r/MachineLearning/comments/9ugjj8/is_there_a_good_trained_speech_recognition_model/,AnasAlmasri,1541445126,[removed],0,1,False,self,,,,,
274,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,4,9ugkgl,self.MachineLearning,US Universities where ML and AI courses are offered by ECE department,https://www.reddit.com/r/MachineLearning/comments/9ugkgl/us_universities_where_ml_and_ai_courses_are/,karthik93kulkarni,1541445280,"I am looking to pursue Masters in the field of ML and AI (in US/Canada). I am from non-cs background, I have completed my undergrad in electronics. As far as I have understood by talking to quite a few number of people, coming from non-cs background, getting admit in CS department is going to be quite difficult. Hence, I thought why dont I look for Universities offering ML and AI in ECE department. I have found few, your inputs will be of great help. Any thoughts on this?",0,1,False,self,,,,,
275,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,4,9ugkkp,self.MachineLearning,Is the k-center problem part of machine learning or computational geometry?,https://www.reddit.com/r/MachineLearning/comments/9ugkkp/is_the_kcenter_problem_part_of_machine_learning/,CodeMK,1541445302," I am currently working especially with the k-center problem, which is e.g. used to determine optimal locations for k warehouses. This is done by defining k circles that cover a given set of points (here for example inhabitants), and at the same time, the radii of the circles should be minimal. The centers of these k circles would then be the optimal locations for the warehouses.  

The question arises whether this method belongs to machine learning or algorithmic geometry.  Since it is a clustering method I would actually think that it belongs to machine learning, more specifically to unsupervised learning.  On the other hand, there are many algorithms to solve this problem which are based on computational geometry. 

How about your opinion?  

Thanks in advance. ",0,1,False,self,,,,,
276,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,4,9ugss2,self.datascience,Does anyone have experience using synthetic / simulated datasets to improve standard ML (i.e. not DL) models?,https://www.reddit.com/r/MachineLearning/comments/9ugss2/does_anyone_have_experience_using_synthetic/,cdlm89,1541446723,,0,1,False,default,,,,,
277,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,5,9uhi9j,self.MachineLearning,q/a,https://www.reddit.com/r/MachineLearning/comments/9uhi9j/qa/,rashikukke,1541451266,[removed],0,1,False,self,,,,,
278,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,6,9uhu1y,self.MachineLearning,How Optimizely uses Multi-Armed Bandits to speed up discovery and increase impact,https://www.reddit.com/r/MachineLearning/comments/9uhu1y/how_optimizely_uses_multiarmed_bandits_to_speed/,oflettersandnumbers,1541453495,[removed],0,1,False,self,,,,,
279,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,6,9uhzf7,self.MachineLearning,Classifying time series data -,https://www.reddit.com/r/MachineLearning/comments/9uhzf7/classifying_time_series_data/,bjr1973,1541454478,"I have some time series data of varying length. It is some biomechanical features of a subject moving across the screen left to right. 

I was thinking the best approach was to use a LSTM autoencoder and classify the vectors. I am not trying to predict the next outcome in the time series, rather classify it between two classes. Is using an LSTM autoencoder the best approach?

Has anyone had any experience using tsfresh(https://tsfresh.readthedocs.io/en/latest/index.html) as an alternative/better option?

Feedback appreciated.",0,1,False,self,,,,,
280,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,7,9uigjp,medium.com,[D] Current trends in ML may lead to AGI in 5 years,https://www.reddit.com/r/MachineLearning/comments/9uigjp/d_current_trends_in_ml_may_lead_to_agi_in_5_years/,Kukrlinc7,1541457641,,0,1,False,default,,,,,
281,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,7,9uijnf,self.MachineLearning,[D] The 25 Most Popular Machine Learning Talks from MLconf,https://www.reddit.com/r/MachineLearning/comments/9uijnf/d_the_25_most_popular_machine_learning_talks_from/,shonburton,1541458247,"Here we have a great variety of our most popular talks including talks from Pedro Domingez, Carinna Cortez of Google, Ashwin Ram from Amazon, Soumith Chintala of Facebook/Pytorch and many more:

[https://www.youtube.com/channel/UCjeM1xxYb\_37bZfyparLS3Q/videos?view=0&amp;sort=p](https://www.youtube.com/channel/UCjeM1xxYb_37bZfyparLS3Q/videos?view=0&amp;sort=p)

  
[MLconf is in San Francisco this week](https://www.eventbrite.com/e/mlconf-sf-2018-tickets-45989163827). Researchers from Google Brain, Microsoft Labs, Uber ATG, Tesla, IBM, Max Plank Institute and many more are presenting.  The live event is almost sold out but if you would like to join us in person, please use the ""SlashML"" discount code for 35% off!

&amp;#x200B;",22,1,False,self,,,,,
282,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,8,9uixgo,self.MachineLearning,[D] ICLR 2019 reviews are out. Good luck everyone!,https://www.reddit.com/r/MachineLearning/comments/9uixgo/d_iclr_2019_reviews_are_out_good_luck_everyone/,fixed-point-learning,1541460954,"The reviews are partially out, with some lazy reviewers running late. Most papers now have at least one or two reviews. Supposedly, there will be three reviews at the end.",81,1,False,self,,,,,
283,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,9,9ujce7,self.MachineLearning,[D] Inference Hardware,https://www.reddit.com/r/MachineLearning/comments/9ujce7/d_inference_hardware/,Boozybrain,1541464002,"What options are out there for relatively affordable hardware that can run inference at a respectable rate (&gt;1Hz)?  AIY has the vision board, but it's severely limited in what it can do.  A TX2 isn't exactly affordable, but is there anything in between?  The market seems pretty barren in the $100-$300 range.",10,1,False,self,,,,,
284,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,9,9uje5i,i.redd.it,PMO - Inovaes Tecnolgicas IV,https://www.reddit.com/r/MachineLearning/comments/9uje5i/pmo_inovaes_tecnolgicas_iv/,JamurGerloff,1541464365,,0,1,False,https://b.thumbs.redditmedia.com/mG_Iia5peJawZyzcJdSZophthF8Ei-cAz-KfYVjE27w.jpg,,,,,
285,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,10,9ujqf5,stoodnt.com,Masters in Data Science vs Machine Learning / AI vs Data/Business Analytics,https://www.reddit.com/r/MachineLearning/comments/9ujqf5/masters_in_data_science_vs_machine_learning_ai_vs/,tanmoyray01,1541466909,,0,1,False,https://b.thumbs.redditmedia.com/qQGt4Zuc7KOSIrM9Oy73dcJW71YIl6CZOyFUQKzWOcw.jpg,,,,,
286,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,11,9uk4te,weformulate.io,"Data-Powered Optimization, Beyond AB",https://www.reddit.com/r/MachineLearning/comments/9uk4te/datapowered_optimization_beyond_ab/,formulateio,1541469917,,0,1,False,default,,,,,
287,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,12,9ukll4,self.MachineLearning,Question About ICLR,https://www.reddit.com/r/MachineLearning/comments/9ukll4/question_about_iclr/,anonymous135797531,1541473461,[removed],0,1,False,self,,,,,
288,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,13,9ul3ik,self.MachineLearning,Is there any good method to diagnose binary logistic regression (classification)?,https://www.reddit.com/r/MachineLearning/comments/9ul3ik/is_there_any_good_method_to_diagnose_binary/,engineheat,1541476907,[removed],0,1,False,self,,,,,
289,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,13,9ulanb,self.MachineLearning,Evaluating image-image translation?,https://www.reddit.com/r/MachineLearning/comments/9ulanb/evaluating_imageimage_translation/,nobodykid23,1541478548,[removed],0,1,False,self,,,,,
290,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,14,9ulsqe,self.MachineLearning,[Discussion] What the equivalent of leetcode for machine learning?,https://www.reddit.com/r/MachineLearning/comments/9ulsqe/discussion_what_the_equivalent_of_leetcode_for/,denfromufa,1541483231,,8,1,False,self,,,,,
291,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,16,9uma8t,medium.com,Whether AI and DAO amalgamation makes a perfect organisation ?,https://www.reddit.com/r/MachineLearning/comments/9uma8t/whether_ai_and_dao_amalgamation_makes_a_perfect/,zerohalo,1541488488,,2,1,False,https://b.thumbs.redditmedia.com/oAc4BOTOTPVldKfV6SaLMUeyUIhiAKuItg7oupS8H0A.jpg,,,,,
292,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,16,9umbdl,chillee.github.io,[D] I've updated OpenReview Explorer with ICLR 2019 reviews,https://www.reddit.com/r/MachineLearning/comments/9umbdl/d_ive_updated_openreview_explorer_with_iclr_2019/,programmerChilli,1541488880,,1,1,False,default,,,,,
293,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,16,9umhmb,youtube.com,Automatic Popcorn Flavoring Machine Manufacturer Video,https://www.reddit.com/r/MachineLearning/comments/9umhmb/automatic_popcorn_flavoring_machine_manufacturer/,liusherry,1541490953,,0,1,False,default,,,,,
294,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,17,9umkyj,self.MachineLearning,[D] Capsule Networks for Image Processing?,https://www.reddit.com/r/MachineLearning/comments/9umkyj/d_capsule_networks_for_image_processing/,downvotedbylife,1541492017,"A day-long search across the literature I have access to has yielded a ton of papers on the use of capsule networks for categorical classification tasks (image in, category/label out). 

However, I have not seen any use cases for capsule networks for the case of image processing (image in, image out), such as denoising, deblurring, feature highlighting, etc. beyond [one use for image segmentation](https://arxiv.org/abs/1804.04241v1), which can still be thought of as a type of categorical analysis. 

Has anyone here read/worked on any such case in literature? Is the inherent design of CapsNets non-conductive to such an application, or is there any reason the community hasn't turned to them for these tasks?",11,1,False,self,,,,,
295,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,18,9un085,self.MachineLearning,[Survey] What visual task do you use GAN for?,https://www.reddit.com/r/MachineLearning/comments/9un085/survey_what_visual_task_do_you_use_gan_for/,tobyclh,1541497372,[removed],0,1,False,self,,,,,
296,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,20,9unfnx,m.youtube.com,Waymo self driving car is about to launch as a service in Arizona. (As a fully available service offered to the general public in Arizona),https://www.reddit.com/r/MachineLearning/comments/9unfnx/waymo_self_driving_car_is_about_to_launch_as_a/,ProgrammingGodJordan,1541502480,,0,1,False,default,,,,,
297,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,20,9unhay,self.MachineLearning,Anyone willing to be interviewed to be a first-hand source in my research paper about automation and its effect on the future economy?,https://www.reddit.com/r/MachineLearning/comments/9unhay/anyone_willing_to_be_interviewed_to_be_a/,jaemondr24,1541502976,"Hey everyone,

I'm writing a first-year undergrad research paper on artificial intelligence/automation and its effects on the economy in the future.

To be specific, I'm looking for someone who currently works with artificial intelligence/machine learning/automation and would like to share some insights on what they believe the impacts will be on jobs, income inequality, or education.

Thank you for reading!",0,1,False,self,,,,,
298,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,20,9unlu9,self.MachineLearning,How does something like OpenAI get 300+ years of experience in only 2 weeks in a game like DOTA,https://www.reddit.com/r/MachineLearning/comments/9unlu9/how_does_something_like_openai_get_300_years_of/,Bilal_Tech,1541504435,[removed],0,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,21,9unqyd,self.MachineLearning,What can be some interesting NLP techniques to classify multiple occurrences of the same text in a document based on its neighboring words?,https://www.reddit.com/r/MachineLearning/comments/9unqyd/what_can_be_some_interesting_nlp_techniques_to/,raghuemani,1541505963,[removed],0,1,False,self,,,,,
300,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,21,9unvpy,self.MachineLearning,[P] Need help making a Deep Learning Project!,https://www.reddit.com/r/MachineLearning/comments/9unvpy/p_need_help_making_a_deep_learning_project/,C6H5CH2CHCH3NHCH3,1541507280,"Hi, I am a machine learning enthusiast and trying to make a project on SOMs using neural network to make a mapping of my college's students based on some parameters.
These parameters can be questions having answers in the format yes/no(or similar) which can be used to predict similarities between different people or if a person A would go along with person B or not.
Any psychology enthusiast around? :P
Thanks!",0,1,False,self,,,,,
301,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,21,9unw0p,youtube.com,AI Commons is now DeepQuest AI,https://www.reddit.com/r/MachineLearning/comments/9unw0p/ai_commons_is_now_deepquest_ai/,DeepQuestAI,1541507363,,0,1,False,https://b.thumbs.redditmedia.com/Ko24o3MIMy5BcIHtmPLIWHnFjcKYOB5SINpKmw0BUcA.jpg,,,,,
302,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,21,9uo1h3,self.MachineLearning,Python library for multivariate time series classification based on reservoir computing,https://www.reddit.com/r/MachineLearning/comments/9uo1h3/python_library_for_multivariate_time_series/,filloz85,1541508792,[removed],0,1,False,self,,,,,
303,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,22,9uo8gu,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/9uo8gu/global_machine_learning_market_size_outlook/,Envisionenvi,1541510457,[removed],0,1,False,self,,,,,
304,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,22,9uodpz,github.com,[P] Python library for multivariate time series classification with Reservoir Computing,https://www.reddit.com/r/MachineLearning/comments/9uodpz/p_python_library_for_multivariate_time_series/,filloz85,1541511666,,0,1,False,default,,,,,
305,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,22,9uoh5m,arxiv.org,[R] An Interdisciplinary Comparison of Sequence Modeling Methods for Next-Element Prediction,https://www.reddit.com/r/MachineLearning/comments/9uoh5m/r_an_interdisciplinary_comparison_of_sequence/,TaXxER,1541512427,,3,1,False,default,,,,,
306,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,23,9uok9d,self.MachineLearning,"[N] ""That's my goal for the next few years!"" says Yann LeCun, the charismatic leader of Facebook AI. Follow the link to read the full interview on Computer Vision News",https://www.reddit.com/r/MachineLearning/comments/9uok9d/n_thats_my_goal_for_the_next_few_years_says_yann/,Gletta,1541513070," Hot off the Press! Here are the links to the November 2018 issue of **Computer Vision News**, the magazine of the algorithm community published by **RSIP Vision**: interview with **Yann LeCun**, many more articles about computer vision and **free subscription at page 32**.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2018November/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2018-november-pdf/)

Enjoy!

&amp;#x200B;

![img](iocyyd25wpw11)",6,1,False,self,,,,,
307,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,23,9uor6h,medium.com,PyTorch Scholarship Challenge from Facebook: Im in!,https://www.reddit.com/r/MachineLearning/comments/9uor6h/pytorch_scholarship_challenge_from_facebook_im_in/,khaledur01,1541514521,,0,1,False,default,,,,,
308,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,23,9uou6j,self.MachineLearning,Real-Time sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/9uou6j/realtime_sentiment_analysis/,Manoharan-D,1541515140,"\[Project\]My blog, Real-Time sentiment analysis, end to end, using Apache Spark and CoreNLP.

[http://techmano.in/blog/realtime.aspx](http://techmano.in/blog/realtime.aspx)

![img](ylwlxi9d2qw11)",0,1,False,self,,,,,
309,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,23,9uoura,self.MachineLearning,Convert mlmodel to tflite,https://www.reddit.com/r/MachineLearning/comments/9uoura/convert_mlmodel_to_tflite/,antifringe,1541515264,[removed],0,1,False,self,,,,,
310,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,23,9uozn6,self.MachineLearning,[D] Best AI news in October?,https://www.reddit.com/r/MachineLearning/comments/9uozn6/d_best_ai_news_in_october/,Anogio94,1541516268,"I recently wrote an article to help anyone who's been living under an ML-proof rock this month catch up with the latest news.

[https://blog.sicara.com/10-2018-best-ai-new-articles-this-month-a219efa105ba](https://blog.sicara.com/10-2018-best-ai-new-articles-this-month-a219efa105ba)

&amp;#x200B;

Anything I missed? I would really like some feedback on this as it is quite hard to keep up with the field even when you're actively monitoring it.",35,1,False,self,,,,,
311,MachineLearning,t5_2r3gv,2018-11-6,2018,11,6,23,9up056,self.MachineLearning,[P] C++ implementation of Faster R-CNN with MXNet,https://www.reddit.com/r/MachineLearning/comments/9up056/p_c_implementation_of_faster_rcnn_with_mxnet/,kolkir,1541516368,"https://github.com/Kolkir/mlcpp/tree/master/rcnn-mxnet
I made C++ implementation of Faster R-CNN with MXNet CppPackage. It's based on pythons implementation  https://github.com/ijkguo/mx-rcnn. Project was made for educational purposes and can be used as comprehensive example of MXNet C++ API. You can find how to: load data from MSCoco dataset, create custom layers, handle errors, intergate Eigen data structures with NDArray. Feel free to ask a questions.",14,1,False,self,,,,,
312,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,0,9up71e,self.MachineLearning,How to recode Big Brother in 15 min on your couch,https://www.reddit.com/r/MachineLearning/comments/9up71e/how_to_recode_big_brother_in_15_min_on_your_couch/,oussj,1541517707,[removed],0,1,False,self,,,,,
313,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,1,9uplk5,self.MachineLearning,Help with a project...,https://www.reddit.com/r/MachineLearning/comments/9uplk5/help_with_a_project/,heaven_inc,1541520430,[removed],0,1,False,self,,,,,
314,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,1,9upwqe,self.MachineLearning,[D] How could I go about making a 9x9 go(baduk) AI using a neural network?,https://www.reddit.com/r/MachineLearning/comments/9upwqe/d_how_could_i_go_about_making_a_9x9_gobaduk_ai/,Carcaso,1541522472,"Is a simple feedforward neural network capable of learning how to play 9x9 Go or is something more complicated required? If a simple net won't work, then what will?",7,1,False,self,,,,,
315,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,1,9upwt7,self.MachineLearning,Computer-assisted reverse engineering of synth sounds,https://www.reddit.com/r/MachineLearning/comments/9upwt7/computerassisted_reverse_engineering_of_synth/,Pehat,1541522489,[removed],0,1,False,self,,,,,
316,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,1,9upzh0,youtu.be,Find S Algorithm in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9upzh0/find_s_algorithm_in_machine_learning/,adarsh_adg,1541522971,,0,1,False,default,,,,,
317,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,2,9uq4dv,self.MachineLearning,Best News Source?,https://www.reddit.com/r/MachineLearning/comments/9uq4dv/best_news_source/,reddevilit7,1541523839,[removed],0,1,False,self,,,,,
318,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,2,9uq608,medium.com,[D] Hitchhikers Guide to Organizing an Academic Workshop,https://www.reddit.com/r/MachineLearning/comments/9uq608/d_hitchhikers_guide_to_organizing_an_academic/,suryabhupa,1541524119,,0,1,False,default,,,,,
319,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,2,9uq6q4,self.MachineLearning,What you usually do when you're model is being trained?,https://www.reddit.com/r/MachineLearning/comments/9uq6q4/what_you_usually_do_when_youre_model_is_being/,aziz_22,1541524245,[removed],0,1,False,self,,,,,
320,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,2,9uq82a,self.MachineLearning,Web API for solving quiz questions like on Who Wants to be a Millionaire?,https://www.reddit.com/r/MachineLearning/comments/9uq82a/web_api_for_solving_quiz_questions_like_on_who/,MetalJulien,1541524487,"Hey guys

&amp;#x200B;

Are there any quiz solvers with an API which estimates probabilities of a given set of answers like on Who Wants to be a Millionaire? An example would be the question ""Which city lies on 30 latitude?"" with a set of possible answers \[""Malaga"", ""New York"", ""Cairo""\]. The correct answer would be ""Cairo"".

&amp;#x200B;

I was thinking about naively comparing the number of Google search results for the query '&lt;Question&gt; ""&lt;Response\_i&gt;"" ' but unfortunately, New York gives me way more results than Cairo. An improved method would be to filter out key words and put them in quotes, but maybe somebody out there knows how to best describe this problem and could give me a library.",0,1,False,self,,,,,
321,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,2,9uqa08,medium.com,Recent Research and Trends in NLP,https://www.reddit.com/r/MachineLearning/comments/9uqa08/recent_research_and_trends_in_nlp/,reSAMpled,1541524825,,0,1,False,default,,,,,
322,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,2,9uqa1p,self.MachineLearning,find feature of interest in multi-scale images?,https://www.reddit.com/r/MachineLearning/comments/9uqa1p/find_feature_of_interest_in_multiscale_images/,reddit_tl,1541524834,[removed],0,1,False,self,,,,,
323,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,3,9uqxc1,self.MachineLearning,[D] How to specify model to reduce False Negatives?,https://www.reddit.com/r/MachineLearning/comments/9uqxc1/d_how_to_specify_model_to_reduce_false_negatives/,Fender6969,1541528880,"I am working on a project in Python and I wanted to know how to let my classifier know to have the objective to reduce False Negatives rather than just evaluating it on overall accuracy. For my optimal hyperparameters, I am using GridSearchCV. Where in the process of creating my model model can I specify this? Any help would be great!",13,1,False,self,,,,,
324,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,4,9urj6k,medium.com,[D] Hitchhikers Guide to Organizing an Academic Workshop,https://www.reddit.com/r/MachineLearning/comments/9urj6k/d_hitchhikers_guide_to_organizing_an_academic/,suryabhupa,1541532784,,0,1,False,https://b.thumbs.redditmedia.com/Pc7svPCMq191kwYK8tAyy1r3tC_J59cDx8inkmET4Wc.jpg,,,,,
325,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,5,9uruzq,youtube.com,SVM Support Vector Machines Part 1 - Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9uruzq/svm_support_vector_machines_part_1_machine/,jeffxu999,1541534856,,0,1,False,https://b.thumbs.redditmedia.com/aBRT0R5cfACs2ITFq-L5A-5YnHbdVTOwF9GqKi2lKzU.jpg,,,,,
326,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,5,9urx5j,ml2018.eventbrite.de,"Event ""Knstliche Intelligenz, Machine &amp; Deep Learning"". Frankfurt a.M. 19.11.2018",https://www.reddit.com/r/MachineLearning/comments/9urx5j/event_knstliche_intelligenz_machine_deep/,soft-fact,1541535251,,0,1,False,default,,,,,
327,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,5,9us0rl,medium.com,DeepMind Announces Pre-Symptom Eye Disease Prediction at Moorfields,https://www.reddit.com/r/MachineLearning/comments/9us0rl/deepmind_announces_presymptom_eye_disease/,Yuqing7,1541535910,,0,1,False,https://b.thumbs.redditmedia.com/b8hmXod6hyrMnswJxlMuR_5Ga792Gc4TGwXNmdN_pgI.jpg,,,,,
328,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,6,9usj8f,self.MachineLearning,Multi class classifier vs separate nets,https://www.reddit.com/r/MachineLearning/comments/9usj8f/multi_class_classifier_vs_separate_nets/,captainRubik_,1541539187,[removed],0,1,False,self,,,,,
329,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,6,9usj9l,self.MachineLearning,[N] How Optimizely uses Multi-Armed Bandits to speed up discovery and increase impact,https://www.reddit.com/r/MachineLearning/comments/9usj9l/n_how_optimizely_uses_multiarmed_bandits_to_speed/,oflettersandnumbers,1541539193,I wrote an engineering blog post on how Optimizely uses Multi-Armed Bandits: [https://medium.com/engineers-optimizely/stats-accelerator-the-when-why-and-how-231ed6213d6d](https://medium.com/engineers-optimizely/stats-accelerator-the-when-why-and-how-231ed6213d6d). The intended audience is for an engineer who doesn't have much exposure to Stats and Machine Learning. Figured this might be of interest to folks here. Check it out and let me know what you think!,1,1,False,self,,,,,
330,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,6,9ustk1,medium.com,[Project] Thampi: A Serverless Machine Learning Prediction System on AWS Lambda.,https://www.reddit.com/r/MachineLearning/comments/9ustk1/project_thampi_a_serverless_machine_learning/,rajiv_abraham,1541541068,,1,1,False,default,,,,,
331,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,7,9ut08l,self.MachineLearning,"[D] Clustering algorithms for large latitude, longitude type data.",https://www.reddit.com/r/MachineLearning/comments/9ut08l/d_clustering_algorithms_for_large_latitude/,snendroid-ai,1541542267,"Perhaps, clustering algorithms like DBSCAN which works on large data (10M+ data points) or any other implementations?",6,1,False,self,,,,,
332,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,7,9ut0py,self.MachineLearning,Studying large fan in / fan out ratios,https://www.reddit.com/r/MachineLearning/comments/9ut0py/studying_large_fan_in_fan_out_ratios/,idg101,1541542355,[removed],1,1,False,self,,,,,
333,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,7,9utdk9,medium.com,European Patent Office Issues First Guidelines on AI Patents,https://www.reddit.com/r/MachineLearning/comments/9utdk9/european_patent_office_issues_first_guidelines_on/,Yuqing7,1541544685,,0,1,False,https://b.thumbs.redditmedia.com/E1DbezDgYHuWmf7Ag0_-JIGJHAFYL9DlVemkcTQplIg.jpg,,,,,
334,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,7,9utdq0,cryptobriefing.com,Blockchain AI: Decentralizing The Language Of The Gods,https://www.reddit.com/r/MachineLearning/comments/9utdq0/blockchain_ai_decentralizing_the_language_of_the/,longspeek,1541544718,,0,1,False,default,,,,,
335,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,8,9utkq1,self.MachineLearning,Unsupervised learning with DNN on text,https://www.reddit.com/r/MachineLearning/comments/9utkq1/unsupervised_learning_with_dnn_on_text/,Due_Kindheartedness,1541546048,"I want to:

1. Have a key:pair database with author:largetextfileofeverysentencetheauthorpublished.txt

2. Set up a deep neural network to see unsupervised patterns in choice of vocabulary.

3. Have the DNN partition the set of authors into disjointed cells based on common vocabulary usage.

Is this possible? How can I make it happen?",0,1,False,self,,,,,
336,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,8,9utn7a,i.redd.it,Which tie is better for a job interview?,https://www.reddit.com/r/MachineLearning/comments/9utn7a/which_tie_is_better_for_a_job_interview/,tbarrios,1541546514,,0,1,False,default,,,,,
337,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,8,9utojc,pubs.rsna.org,"AI predicts Alzheimers disease years before diagnosis - By using a special brain scan, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82% specificity at 100% sensitivity, an average of 75.8 months prior to the final diagnosis. (x-posted /r/science)",https://www.reddit.com/r/MachineLearning/comments/9utojc/ai_predicts_alzheimers_disease_years_before/,chameleonarchreactor,1541546770,,0,1,False,default,,,,,
338,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,8,9uttu5,docs.google.com,[D] Writing Code for NLP Research,https://www.reddit.com/r/MachineLearning/comments/9uttu5/d_writing_code_for_nlp_research/,AFewSentientNeurons,1541547797,,0,1,False,default,,,,,
339,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,8,9utwck,self.MachineLearning,How successful are Watson/Google Cloud Platform/Azure's machine learning solutions?,https://www.reddit.com/r/MachineLearning/comments/9utwck/how_successful_are_watsongoogle_cloud/,whataweirdhorse,1541548293,[removed],0,1,False,self,,,,,
340,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,9,9uu3ty,self.MachineLearning,Classifying twitter dataset to event datasets.,https://www.reddit.com/r/MachineLearning/comments/9uu3ty/classifying_twitter_dataset_to_event_datasets/,_IAmAdam,1541549754,[removed],0,1,False,self,,,,,
341,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,9,9uu6yi,i.redd.it,Why Prediction Needs Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/9uu6yi/why_prediction_needs_unsupervised_learning/,jtsymonds,1541550393,,0,1,False,default,,,,,
342,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,10,9uuh56,self.MachineLearning,[P] Layer-wise Adaptive Rate Scaling (LARS) in PyTorch,https://www.reddit.com/r/MachineLearning/comments/9uuh56/p_layerwise_adaptive_rate_scaling_lars_in_pytorch/,noahgolm,1541552486,"[https://github.com/noahgolmant/pytorch-lars](https://github.com/noahgolmant/pytorch-lars)

I just implemented [LARS](https://arxiv.org/abs/1708.03888) for large-batch training using PyTorch. This technique uses per-layer learning rates in an effort to mitigate the generalization gap for large batch sizes. I was not able to achieve good results on CIFAR-10. I would appreciate if anyone could point to changes I could make to obtain similar behavior compared to what the authors observed on ImageNet.

One thing I am unsure about is that I am currently using a fixed global learning rate for all batch sizes. The authors reported increasing the learning rate with batch size, but it is not clear if they used a heuristic (e.g. square root scaling) or hyperparameter tuning to arrive at those settings.

I'd love any and all feedback! Let me know if you see major errors.",2,1,False,self,,,,,
343,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,10,9uuqpl,self.MachineLearning,What are some recent updates in (non)convex optimization?,https://www.reddit.com/r/MachineLearning/comments/9uuqpl/what_are_some_recent_updates_in_nonconvex/,etotheipiovertwo,1541554449,"With somewhat older methods such as ADAM, stochastic gradient momentum, can you tell me about recent progress being made towards (non)convex optimization and interesting reads related?

I recently started reading Hamiltonian Descent with kinetic energy and its convergence properties with power functions, sparking my inspiration more towards convex and non-convex optimization. ",0,1,False,self,,,,,
344,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,10,9uuuru,i.redd.it,"CHARLOTTE (reddit meme edition), a massive neural net that can reconstruct images into advanced memes (https://discord.gg/wBdzaHc)",https://www.reddit.com/r/MachineLearning/comments/9uuuru/charlotte_reddit_meme_edition_a_massive_neural/,QUZANG,1541555308,,1,1,False,https://b.thumbs.redditmedia.com/DR8WkTgw7T62becwWuHIl57eZvLc4oSwx4JrbpDw80Y.jpg,,,,,
345,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,10,9uuvzk,self.MachineLearning,Synthesizing speech on limited resources data,https://www.reddit.com/r/MachineLearning/comments/9uuvzk/synthesizing_speech_on_limited_resources_data/,enamoria,1541555571,[removed],0,1,False,self,,,,,
346,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,11,9uuyvl,self.MachineLearning,Quick Survey To Help My CS Research,https://www.reddit.com/r/MachineLearning/comments/9uuyvl/quick_survey_to_help_my_cs_research/,jay4842,1541556186,[removed],0,1,False,self,,,,,
347,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,11,9uvbzo,self.MachineLearning,[R] BTS song covered by Fake Trump,https://www.reddit.com/r/MachineLearning/comments/9uvbzo/r_bts_song_covered_by_fake_trump/,cyplus1,1541558778,"It is generated by fine-grained prosody controlled speech synthesis

Demo: [https://youtu.be/FIoP13pPEZE](https://youtu.be/FIoP13pPEZE)

Paper link: [https://arxiv.org/abs/1811.02122](https://arxiv.org/abs/1811.02122)

More audio samples: [http://neosapience.com/research/2018/10/29/icassp/](http://neosapience.com/research/2018/10/29/icassp/)

&amp;#x200B;",3,1,False,self,,,,,
348,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,12,9uvj4o,self.MachineLearning,[D] ICLR Reviewer Scores,https://www.reddit.com/r/MachineLearning/comments/9uvj4o/d_iclr_reviewer_scores/,feedtheaimbot,1541560185,Are reviewers allowed to change scores after others are revealed? A reviewer changed one of their scores after the other scores were revealed. Doesnt this introduce a bias? The comment by the reviewer didnt change either and no response was made. ,6,1,False,self,,,,,
349,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,13,9uw21v,dropbox.com,[R] [1811.02553] Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?,https://www.reddit.com/r/MachineLearning/comments/9uw21v/r_181102553_are_deep_policy_gradient_algorithms/,evc123,1541564048,,0,1,False,https://b.thumbs.redditmedia.com/5Y36OyMXLcLLO6mlMjBCDRXcgW7jUetq9YHThu3e8tw.jpg,,,,,
350,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,14,9uwg30,alibabacloud.com,Translating 100 Billion Words Every Day for E-Commerce with Alibaba Machine Translation,https://www.reddit.com/r/MachineLearning/comments/9uwg30/translating_100_billion_words_every_day_for/,Jen_Cl,1541567168,,0,1,False,https://a.thumbs.redditmedia.com/EXHVN-IsmWH1fZ6OikxK1lib_otIwQVr2IXn0ogQwV0.jpg,,,,,
351,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,14,9uwoxc,i.redd.it,How do I read this thing?,https://www.reddit.com/r/MachineLearning/comments/9uwoxc/how_do_i_read_this_thing/,lxiaoqi,1541569216,,0,1,False,https://b.thumbs.redditmedia.com/oZgLg_Aya1vs3ZJwQKvxXCMR9PiDjqJCkHKacc0OE5Y.jpg,,,,,
352,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,16,9uxbbj,self.MachineLearning,"[P] FloWaveNet: A Generative Flow for Raw Audio. PyTorch codes (also w/ ClariNet), sampled audio clips, and arXiv draft available",https://www.reddit.com/r/MachineLearning/comments/9uxbbj/p_flowavenet_a_generative_flow_for_raw_audio/,L0SG,1541575115,"Code: [FloWaveNet](https://github.com/ksw0306/FloWaveNet) and [ClariNet](https://github.com/ksw0306/ClariNet)

Samples: [Google Drive link](https://drive.google.com/drive/folders/1RPo8e35lhqwOrMrBf1cVXqnF9hzxsunU)

arXiv draft: [link](https://arxiv.org/abs/1811.02155)

(Disclaimer: second author here)

Hi, we've recently been working on a flow-based generative model that can do a real-time waveform audio synthesis. While preparing for a MOS experiment, we found that [NVIDIA has published this](https://arxiv.org/abs/1811.00002) few days ago and we are kind of screwed since the main idea is almost identical :(

Instead of scraping the project altogether, we've decided to release all the codes (our model and the [ClariNet](https://arxiv.org/abs/1807.07281) code which works great) and audio clips along with a preliminary draft on arXiv, to avoid any kind of suspicion of a ripoff or some kind.

The main advantage of this approach we think is a real-time waveform synthesis as well as a single-stage, single loss training unlike the [Parallel WaveNet](https://arxiv.org/abs/1711.10433) or ClariNet. We think that it is much easier to tune. any feedback would be much appreciated!

(The first author is crying in a corner of our lab idk how to cheer him up...)

&amp;#x200B;

&amp;#x200B;",22,1,False,self,,,,,
353,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,17,9uxjxp,self.MachineLearning,"[D] Other than here, where do you guys find new interesting papers?",https://www.reddit.com/r/MachineLearning/comments/9uxjxp/d_other_than_here_where_do_you_guys_find_new/,mrconter1,1541577680,,23,1,False,self,,,,,
354,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,17,9uxmty,self.MachineLearning,Multivariate Multistep Seq2Seq forecasting model input data reshaping,https://www.reddit.com/r/MachineLearning/comments/9uxmty/multivariate_multistep_seq2seq_forecasting_model/,AccomplishedCode,1541578522,[removed],0,1,False,self,,,,,
355,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,17,9uxuwc,self.MachineLearning,"[D] Paper accepted into NIPS workshop, but unable to arrange funds for registration and travel.",https://www.reddit.com/r/MachineLearning/comments/9uxuwc/d_paper_accepted_into_nips_workshop_but_unable_to/,tensor_x,1541581114,I may not be able to attend the workshop. What will happen?,9,1,False,self,,,,,
356,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,18,9uxx74,self.MachineLearning,Common feature extraction strategies for unknown / unstructured file?,https://www.reddit.com/r/MachineLearning/comments/9uxx74/common_feature_extraction_strategies_for_unknown/,steffo3loo,1541581834,[removed],0,1,False,self,,,,,
357,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,19,9uy7ox,self.MachineLearning,[R] Knowledge Distillation with Hardware Constrained Students,https://www.reddit.com/r/MachineLearning/comments/9uy7ox/r_knowledge_distillation_with_hardware/,learning-luke,1541585270,[http://www.bayeswatch.com/2018/11/06/HAKD/](Blog post) and corresponding [https://arxiv.org/abs/1810.10460](paper).,0,1,False,self,,,,,
358,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,19,9uy8i6,youtube.com,YOLO: A boxing match dataset,https://www.reddit.com/r/MachineLearning/comments/9uy8i6/yolo_a_boxing_match_dataset/,UpstairsCurrency,1541585523,,0,1,False,default,,,,,
359,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,19,9uyfxw,kanoki.org,Dataset Search Engine - For ML Beginners/Researchers/DataScientists,https://www.reddit.com/r/MachineLearning/comments/9uyfxw/dataset_search_engine_for_ml/,min2bro,1541587947,,0,1,False,default,,,,,
360,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,20,9uyjif,arxiv.org,Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/9uyjif/bayesian_action_decoder_for_deep_multiagent/,jakobnicolaus,1541589077,,1,1,False,default,,,,,
361,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,20,9uyrx0,self.MachineLearning,[P] Style Transfer for Audio,https://www.reddit.com/r/MachineLearning/comments/9uyrx0/p_style_transfer_for_audio/,clickbait_hmmm,1541591645,"I am planning to work on something where I can use the lyrics of one song and try to style it into  something of a completely different genre. 

Google doesn't return a lot of results.
The closest thing I found was :
 https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/

If anyone has experience working with anything of this sort, could you please comment on what resources/libraries helped you.

I am not very experienced with deep learning so if this is the sort of thing which would require me to be really good at math, please do tell me.
",5,1,False,self,,,,,
362,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,20,9uyt52,self.MachineLearning,[D] Google AI Residency Program Applicants Discussion Thread,https://www.reddit.com/r/MachineLearning/comments/9uyt52/d_google_ai_residency_program_applicants/,describbler,1541591976,"Thought it would be helpful to have a discussion thread for this year's Google AI Residency applicants to share the updates, info, resources to prepare etc.",0,1,False,self,,,,,
363,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,21,9uyzc1,self.MachineLearning,[D] Google AI Residency 2019 Applicants Discussion Thread,https://www.reddit.com/r/MachineLearning/comments/9uyzc1/d_google_ai_residency_2019_applicants_discussion/,describbler,1541593564,"Thought it would be helpful to have a discussion thread for this year's Google AI Residency applicants to share the updates, info, resources to prepare etc.",258,1,False,self,,,,,
364,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,21,9uz5cr,self.MachineLearning,Best strategy for log analysis?,https://www.reddit.com/r/MachineLearning/comments/9uz5cr/best_strategy_for_log_analysis/,Carvalho96,1541595102,[removed],0,1,False,self,,,,,
365,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,22,9uzd5x,self.MachineLearning,[P] Modularity Regularized NMF - Community Preserving Network Embedding. TensorFlow Implementation (AAAI 2017),https://www.reddit.com/r/MachineLearning/comments/9uzd5x/p_modularity_regularized_nmf_community_preserving/,benitorosenberg,1541596937,"[https://github.com/benedekrozemberczki/M-NMF](https://github.com/benedekrozemberczki/M-NMF)

https://i.redd.it/9lli45lktww11.jpg

M-NMF is a network embedding algorithm which learns a clustering based on features extracted with modularity regularized non-negative matrix factorization. The procedure places nodes in an abstract feature space where the inner product of node features reconstructs a proximity based matrix representation of the graph. M-NMF has a quadratic runtime complexity in the number of nodes. This  specific implementation supports GPU use (Tensorflow).",0,1,False,https://a.thumbs.redditmedia.com/hIKGeS7W27elBn86qA-ZNmNinvfyDUN1uBtbfDMK6i8.jpg,,,,,
366,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,22,9uzees,self.MachineLearning,[P] Coupled Neural Architecture Search with Quantized Neural Networks.,https://www.reddit.com/r/MachineLearning/comments/9uzees/p_coupled_neural_architecture_search_with/,wubba-luba,1541597230,"Quantized Neural Networks drastically reduce the memory requirements by replacing most arithmetic operations with bit-wise operations. I have built two projects for simulating Quantized Neural Networks and searching for their accurate architectures via Efficient Neural Architecture Search.

&amp;#x200B;

1. Quantized-Nets: [https://github.com/yashkant/Quantized-Nets](https://github.com/yashkant/Quantized-Nets)

2. ENAS-Quantized-Neural-Networks: [https://github.com/yashkant/ENAS-Quantized-Neural-Networks](https://github.com/yashkant/ENAS-Quantized-Neural-Networks)

&amp;#x200B;

Let me know how you think about it. 

&amp;#x200B;

Relevant Works:

1. JMLR paper on Quantized Neural Networks: [https://arxiv.org/abs/1609.07061](https://arxiv.org/abs/1609.07061)

2. Efficient Neural Architecture Search: [https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268)

&amp;#x200B;",0,1,False,self,,,,,
367,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,22,9uzen1,attendee.gotowebinar.com,Webinar about AI in Software testing with Dell's test architect Geoff Meyer,https://www.reddit.com/r/MachineLearning/comments/9uzen1/webinar_about_ai_in_software_testing_with_dells/,musttestmore,1541597285,,0,1,False,default,,,,,
368,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,23,9uznn0,self.MachineLearning,ML,https://www.reddit.com/r/MachineLearning/comments/9uznn0/ml/,Maximum_007,1541599282,[removed],0,1,False,self,,,,,
369,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,23,9uzpsp,self.MachineLearning,why doesn't neural network back propagation use conjugate gradients,https://www.reddit.com/r/MachineLearning/comments/9uzpsp/why_doesnt_neural_network_back_propagation_use/,cristodaro,1541599720,[removed],0,1,False,self,,,,,
370,MachineLearning,t5_2r3gv,2018-11-7,2018,11,7,23,9uztvg,self.MachineLearning,"Machine Learning is artificial, but it's not necessarily intelligent",https://www.reddit.com/r/MachineLearning/comments/9uztvg/machine_learning_is_artificial_but_its_not/,fcsuper,1541600591,[removed],0,1,False,self,,,,,
371,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,0,9v0e0w,arxiv.org,[R] [1811.02549] Language GANs Falling Short,https://www.reddit.com/r/MachineLearning/comments/9v0e0w/r_181102549_language_gans_falling_short/,bobchennan,1541604723,,19,1,False,default,,,,,
372,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,0,9v0emp,arxiv.org,[R] Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes,https://www.reddit.com/r/MachineLearning/comments/9v0emp/r_highly_scalable_deep_learning_training_system/,downtownslim,1541604842,,0,1,False,default,,,,,
373,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,0,9v0er8,self.MachineLearning,Gradients support in PyTorch,https://www.reddit.com/r/MachineLearning/comments/9v0er8/gradients_support_in_pytorch/,andrea_manero,1541604867,[removed],0,1,False,self,,,,,
374,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,0,9v0fiw,self.MachineLearning,[D] Insight Artificial Intelligence Fellowship,https://www.reddit.com/r/MachineLearning/comments/9v0fiw/d_insight_artificial_intelligence_fellowship/,inactiveUserTBD,1541605020,"Hi,

Has anyone been involved in the Insight AI fellowship. Can you please share some pros and cons?

The fellowship program is not paid and I am wondering if it is worth taking the time off?

Does it look good on resume when I am trying to pursue an academic career. I have only completed a bachelors in engineering, but would like to one day hold a PHD in ML.

An extension to my previous question, does the program help fellows find a job in AI research or AI research related projects?

Thanks",8,1,False,self,,,,,
375,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,0,9v0kea,self.MachineLearning,"Simple Questions Thread November 07, 2018",https://www.reddit.com/r/MachineLearning/comments/9v0kea/simple_questions_thread_november_07_2018/,AutoModerator,1541605964,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!
",0,1,False,self,,,,,
376,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,1,9v0r0c,arxiv.org,[R] Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?,https://www.reddit.com/r/MachineLearning/comments/9v0r0c/r_are_deep_policy_gradient_algorithms_truly/,andrew_ilyas,1541607240,,19,1,False,default,,,,,
377,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,1,9v0rbx,self.MachineLearning,[D] n-best output sentence classification ideas,https://www.reddit.com/r/MachineLearning/comments/9v0rbx/d_nbest_output_sentence_classification_ideas/,oldforstocks,1541607298,"I have an ASR system that generates 5 best output for a speech signal.

For example:

1. I like to go to hom
2. I like t go to home
3.  like to go to home
4.  I lke go to home
5.  I like got home

The intent for all the 5 best output is the same. Let' s say the intent is TRAVEL.

I want to build a classifier that takes advantage of n-best effectively. 

So far, I have tried the following:

1. I treated every n-best as an individual sample. It did not give a good performance. 
1. I like to go to hom, TRAVEL
2. I like t go to home, TRAVEL
3.  like to go to home, TRAVEL
4.  I lke go to home, TRAVEL
5.  I like got home, TRAVEL

2. I took average pooling and normalization of softmax of n-best. It did improve the performance a little

I am looking for some more ideas. ",1,1,False,self,,,,,
378,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,1,9v0t3x,self.MachineLearning,Adversarial attacks are 'theoretically' possible. Has there been any 'real' attacks on commercial platforms that actually caused damage?,https://www.reddit.com/r/MachineLearning/comments/9v0t3x/adversarial_attacks_are_theoretically_possible/,RealBatta,1541607650,[removed],0,1,False,self,,,,,
379,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,1,9v115m,ayasdi.com,How Predictive Models Benefit from Unsupervised Learning: The Objective Function Conundrum,https://www.reddit.com/r/MachineLearning/comments/9v115m/how_predictive_models_benefit_from_unsupervised/,jtsymonds,1541609177,,0,1,False,default,,,,,
380,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,2,9v1ccy,self.MachineLearning,[R] Real use of GANs in a product.,https://www.reddit.com/r/MachineLearning/comments/9v1ccy/r_real_use_of_gans_in_a_product/,narsilouu,1541611260,[removed],0,1,False,self,,,,,
381,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,2,9v1f8r,arxiv.org,[R] Efficient Eligibility Traces for Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/9v1f8r/r_efficient_eligibility_traces_for_deep/,Seerdecker,1541611801,,3,1,False,default,,,,,
382,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,2,9v1k9c,self.MachineLearning,[R] Learning Concepts with Energy Functions,https://www.reddit.com/r/MachineLearning/comments/9v1k9c/r_learning_concepts_with_energy_functions/,P4TR10T_TR41T0R,1541612775,"[https://blog.openai.com/learning-concepts-with-energy-functions/](https://blog.openai.com/learning-concepts-with-energy-functions/)

Another blog post from OpenAI!

Videos: https://sites.google.com/site/energyconceptmodels/

Paper: https://arxiv.org/abs/1811.02486",2,1,False,self,,,,,
383,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,3,9v1rwa,self.MachineLearning,[Tooling] Thampi: A Serverless Machine Learning Prediction System on AWS Lambda.,https://www.reddit.com/r/MachineLearning/comments/9v1rwa/tooling_thampi_a_serverless_machine_learning/,rajiv_abraham,1541614208,"Introductory Post: [https://medium.com/@rajiv.abraham/introducing-thampi-ec40a9b02a9d](https://medium.com/@rajiv.abraham/introducing-thampi-ec40a9b02a9d)

disclaimer: I'm the main author. ",0,1,False,self,,,,,
384,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,3,9v1rxe,self.MachineLearning,[P] Linear kernel SVR C parameter,https://www.reddit.com/r/MachineLearning/comments/9v1rxe/p_linear_kernel_svr_c_parameter/,cloudewe1,1541614214,"Hi all, I have been working on a regression problem for my dissertation and I tried using svr with linear kernel. I have tried experimenting with the parameter C and it has absolutely no effect on the overall mean squared error? Whether I set it to 5 or 5000? I dont understand why? Did any of you came across something like that?

Thanks  in advance!!",2,1,False,self,,,,,
385,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,3,9v26bz,code.fb.com,Open-sourcing FBGEMM for state-of-the-art server-side inference,https://www.reddit.com/r/MachineLearning/comments/9v26bz/opensourcing_fbgemm_for_stateoftheart_serverside/,Maratyszcza,1541616883,,0,1,False,default,,,,,
386,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,3,9v2745,self.MachineLearning,Computing Subset of CNN Region Activations,https://www.reddit.com/r/MachineLearning/comments/9v2745/computing_subset_of_cnn_region_activations/,MrTwiggy,1541617033,[removed],0,1,False,self,,,,,
387,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,4,9v2ax2,self.MachineLearning,What are the basic principles I need to learn to create an AI that will assess top companies and output a list of characteristics they share?,https://www.reddit.com/r/MachineLearning/comments/9v2ax2/what_are_the_basic_principles_i_need_to_learn_to/,MARTIALIS20,1541617693,[removed],0,1,False,self,,,,,
388,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,4,9v2czq,self.MachineLearning,Tool for Scene annotation,https://www.reddit.com/r/MachineLearning/comments/9v2czq/tool_for_scene_annotation/,bluesky314,1541618059,[removed],0,1,False,self,,,,,
389,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,4,9v2gpr,wayve.ai,Driving Computer Vision with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/9v2gpr/driving_computer_vision_with_deep_learning/,Acuratio,1541618742,,0,1,False,default,,,,,
390,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,4,9v2j2o,self.MachineLearning,Machine learning MOOC,https://www.reddit.com/r/MachineLearning/comments/9v2j2o/machine_learning_mooc/,luchins,1541619186,[removed],0,1,False,self,,,,,
391,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,4,9v2k8i,medium.com,Serving ML Quickly with TensorFlow Serving and Docker,https://www.reddit.com/r/MachineLearning/comments/9v2k8i/serving_ml_quickly_with_tensorflow_serving_and/,Acuratio,1541619398,,0,1,False,https://b.thumbs.redditmedia.com/qWCGKwz5PvEHBQjceIDTRFtcKGAYF0iBJgm0Pw4L03I.jpg,,,,,
392,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,6,9v3qm7,self.MachineLearning,[Discussion] ML Programming suggestions?,https://www.reddit.com/r/MachineLearning/comments/9v3qm7/discussion_ml_programming_suggestions/,_pragmatic_machine,1541627074,"I am spending a big chunk of my daily active time on fixing my ML code (developing a model, testing and bug) or tweaking my code (mostly in Python). What suggestion do you have be more productive?

Do you write unit test to validate every part? What was your pathway to become efficient ML researcher?

&amp;#x200B;",29,1,False,self,,,,,
393,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,6,9v3u8n,self.MachineLearning,Is there a biologically plausible solution to exploration vs exploitation?,https://www.reddit.com/r/MachineLearning/comments/9v3u8n/is_there_a_biologically_plausible_solution_to/,coshjollins,1541627760,Biological networks use dopamine in their own form of reinforcement learning. I assume this means that there is some mechanism in the brains of some species driving  them to hold off reward for exploration. Is there any training technique that determines the level of reward over time that may be analogous to a biological network ?,0,1,False,self,,,,,
394,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,7,9v49ec,self.MachineLearning,Can someone share about cool application you'v been doing or you know about DL,https://www.reddit.com/r/MachineLearning/comments/9v49ec/can_someone_share_about_cool_application_youv/,Celine-00,1541630624,we also need to update the cool applications using DL or other ML ways  recently globally,0,1,False,self,,,,,
395,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,7,9v4d7r,deepgram.com,Can I get feedback on this video podcast about AI?,https://www.reddit.com/r/MachineLearning/comments/9v4d7r/can_i_get_feedback_on_this_video_podcast_about_ai/,chaosdonkey,1541631376,,1,1,False,default,,,,,
396,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,8,9v4gh2,deepgram.com,[D] Can I get feedback on this video podcast?,https://www.reddit.com/r/MachineLearning/comments/9v4gh2/d_can_i_get_feedback_on_this_video_podcast/,chaosdonkey,1541631993,,1,1,False,default,,,,,
397,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,8,9v4j8w,github.com,[P] BERT-Chainer: Implementation of Google's BERT Model,https://www.reddit.com/r/MachineLearning/comments/9v4j8w/p_bertchainer_implementation_of_googles_bert_model/,sushiai,1541632540,,0,1,False,default,,,,,
398,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,8,9v4pc8,github.com,[N]: Tensorflow 1.12 Released,https://www.reddit.com/r/MachineLearning/comments/9v4pc8/n_tensorflow_112_released/,ntenenz,1541633763,,0,1,False,https://b.thumbs.redditmedia.com/uqCh5u8L3T2G78MhJdj4ETCHS4uJy4BSjmDAO0VIjFg.jpg,,,,,
399,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,8,9v4tw9,self.MachineLearning,[P] BERT-Chainer: Implementation of Google's BERT Model,https://www.reddit.com/r/MachineLearning/comments/9v4tw9/p_bertchainer_implementation_of_googles_bert_model/,sushiai,1541634687,"I implemented an amazing NLP model, Google's BERT (https://arxiv.org/abs/1810.04805). The chainer implementation is based on the original TensorFlow repository and load the pretrained models.
BERT-chainer Github Link: https://github.com/soskek/bert-chainer",1,1,False,self,,,,,
400,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,9,9v50jg,self.MachineLearning,[D] AutoML - AutoKeras removes the need of ML engineers?,https://www.reddit.com/r/MachineLearning/comments/9v50jg/d_automl_autokeras_removes_the_need_of_ml/,aajmera,1541636069,"Google AutoML seems like they can replace ML engineers. 

How convincing does it sound to you?

I have been reading about it for a while now, I think it looks like a complete package of functionalities for a non-ML person to use it. Drag-n-drop, single click training, APIs to use.

What kind of feature do you think it is still lacking or should have to make it more user friendly?",22,1,False,self,,,,,
401,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,9,9v52w3,self.MachineLearning,How to build a test and train set out of list of images?,https://www.reddit.com/r/MachineLearning/comments/9v52w3/how_to_build_a_test_and_train_set_out_of_list_of/,boston101,1541636581,[removed],0,1,False,self,,,,,
402,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,9,9v588q,self.MachineLearning,316 Stainless vs The Beach,https://www.reddit.com/r/MachineLearning/comments/9v588q/316_stainless_vs_the_beach/,805native,1541637652,[removed],0,1,False,self,,,,,
403,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,9,9v5bwy,self.MachineLearning,[Study Group] Bishops PRML,https://www.reddit.com/r/MachineLearning/comments/9v5bwy/study_group_bishops_prml/,_pragmatic_machine,1541638463,[removed],0,1,False,self,,,,,
404,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,9,9v5czq,self.MachineLearning,Is the reign of batch normalization over? Thoughts on this new paper?,https://www.reddit.com/r/MachineLearning/comments/9v5czq/is_the_reign_of_batch_normalization_over_thoughts/,rantana,1541638682,[removed],0,1,False,self,,,,,
405,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,10,9v5e3c,self.MachineLearning,[D] Deeplearning in FP16 in Keras with RTX card,https://www.reddit.com/r/MachineLearning/comments/9v5e3c/d_deeplearning_in_fp16_in_keras_with_rtx_card/,yop8900,1541638909,"Hello everyone, not sure if this is the correct subreddit for this question, but figured I'd ask anyways.

I am relatively new to DL, but I am learning quickly and have successfully trained UNET architectures for segmentation in Keras. I am looking to upgrade my hardware and am torn between the RTX 2070 or a 1080 ti. I have become interested in using FP16 so I can increase my batch sizes during training and therefore I am leaning towards the 2070. However, I have not seen any tutorials or blogs (which I mainly learn from) on how to successfully implement FP16 in Keras.

I was hoping that people here could give insight into how implement FP16 in Keras or point me towards any blogs or tutorials that have implemented it.

Any help is greatly appreciated, thanks.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",7,1,False,self,,,,,
406,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,12,9v6fz2,self.MachineLearning,best option for stochastic variational inference,https://www.reddit.com/r/MachineLearning/comments/9v6fz2/best_option_for_stochastic_variational_inference/,medcode,1541647170,[removed],0,1,False,self,,,,,
407,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,13,9v6vo7,self.MachineLearning,[N] AAAI Registrations Open,https://www.reddit.com/r/MachineLearning/comments/9v6vo7/n_aaai_registrations_open/,SolitaryPenman,1541650831,Registrations for AAAI are now open. Just FYI if you didn't receive any email about it. Hope to meet some of you in Hawaii. :),1,1,False,self,,,,,
408,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,13,9v6zmi,self.MachineLearning,How should I create a model for this situation.,https://www.reddit.com/r/MachineLearning/comments/9v6zmi/how_should_i_create_a_model_for_this_situation/,RoTYDMitch,1541651818,[removed],0,1,False,self,,,,,
409,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,14,9v7594,self.MachineLearning,[Discussion] Is a Deep Q Neural Network capable of solving a maze?,https://www.reddit.com/r/MachineLearning/comments/9v7594/discussion_is_a_deep_q_neural_network_capable_of/,letmebeletmebe123456,1541653304,"As part of my research I decided to design a deep Q neural network to run through a complex simulation in which choices must be made to prevent death, but before that I decided it'd be easier to test this out with finding my way through a maze from the START to the GOAL.  First test showed improvement over 1000 iterations in getting from 0,0 to 100,100.  Second test added walls and has repeatedly failed to get from 0,0 to 5,0.  I'm starting to wonder if this idea is even fundamentally sound

Basic explanation:  takes in current x and y position as the 2 inputs, passes them through 2 hidden layers, and outputs 4 values, representing the q value of going up, down, left, or right.  An action is then picked (randomly at first, then highest value), and a new Q value is calculated by adding reward (-0.01 if the goal wasn't reached, 64 if it was) to gamma * maxQ of next position, and this new q is subtracted from the old q to find the error which is used for back propagation.

Does this sound like it should work?",6,1,False,self,,,,,
410,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,14,9v75sb,/r/MachineLearning/comments/9v75sb/p_computer_generated_faces_using_progressive_gan/,[P] Computer generated faces using Progressive GAN trained on 50K images from a photo booth,https://www.reddit.com/r/MachineLearning/comments/9v75sb/p_computer_generated_faces_using_progressive_gan/,wei_jok,1541653443,,2,1,False,default,,,,,
411,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,15,9v7mbq,self.MachineLearning,[D] How Uber scale their Machine Learning systems with Michelangelo,https://www.reddit.com/r/MachineLearning/comments/9v7mbq/d_how_uber_scale_their_machine_learning_systems/,ndha1995,1541657885,"Uber AI just published a very insightful article: [Scaling Machine Learning at Uber with Michelangelo](https://eng.uber.com/scaling-michelangelo/).

&gt;We review this journey by looking at the path taken to develop Michelangelo and scale ML at Uber, offer an in-depth look at Ubers current approach and future goals towards developing ML platforms, and provide some lessons learned along the way.",13,1,False,self,,,,,
412,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,15,9v7t2o,prweb.com,Clairvoyant Hosts 5th Annual Phoenix Data Conference Showcasing Innovative Implementations and Advances in Data Technology,https://www.reddit.com/r/MachineLearning/comments/9v7t2o/clairvoyant_hosts_5th_annual_phoenix_data/,teamclairvoyant,1541659848,,0,1,False,default,,,,,
413,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,15,9v7tld,arxiv.org,[R] YASENN: Explaining Neural Networks via Partitioning Activation Sequences,https://www.reddit.com/r/MachineLearning/comments/9v7tld/r_yasenn_explaining_neural_networks_via/,denkorzh,1541659996,,1,1,False,default,,,,,
414,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,16,9v7x07,self.MachineLearning,Where to start?,https://www.reddit.com/r/MachineLearning/comments/9v7x07/where_to_start/,Jim_Rds,1541661050,[removed],0,1,False,self,,,,,
415,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,16,9v7z3v,i.redd.it,"A few glimpses from the ""GDG Devfest 2018"" in Pune where Shantanu Mirajkar, Director at Clairvoyant India delivered a talk on ""Machine Learning concepts and algorithms"".",https://www.reddit.com/r/MachineLearning/comments/9v7z3v/a_few_glimpses_from_the_gdg_devfest_2018_in_pune/,teamclairvoyant,1541661685,,0,1,False,https://b.thumbs.redditmedia.com/LiehtJQl0YBISkGiZ4XCpbo0w2nemq6a61zivzi4PyQ.jpg,,,,,
416,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,16,9v7z7e,self.MachineLearning,Global Machine Learning Market,https://www.reddit.com/r/MachineLearning/comments/9v7z7e/global_machine_learning_market/,sainathkapil,1541661718,[removed],0,1,False,self,,,,,
417,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,16,9v832h,self.MachineLearning,Who got accepted for the Google AI residency program?,https://www.reddit.com/r/MachineLearning/comments/9v832h/who_got_accepted_for_the_google_ai_residency/,aziz_22,1541662920,"Today morning, I have received an e-mail from Google telling me that my application wasn't considered for the role.

Anyone there who was luckier than me?

&amp;#x200B;",0,1,False,self,,,,,
418,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,16,9v850b,self.MachineLearning,Looking for mentor.,https://www.reddit.com/r/MachineLearning/comments/9v850b/looking_for_mentor/,IIAKAD,1541663537,[removed],0,1,False,self,,,,,
419,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,17,9v86w9,/r/MachineLearning/comments/9v86w9/d_monte_carlo_policy_based_reinforcement_learning/,[D] MONTE CARLO POLICY BASED REINFORCEMENT LEARNING WITH DISCOUNTED REWARDS (OPENAI Lunar Lander environment).Trained for 10000 episodes,https://www.reddit.com/r/MachineLearning/comments/9v86w9/d_monte_carlo_policy_based_reinforcement_learning/,charan_1996,1541664142,,0,1,False,default,,,,,
420,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,17,9v898n,self.MachineLearning,[D] [Request for Papers] Using Leave One-Out Cross Validation for Evaluation of the Learning Algorithm,https://www.reddit.com/r/MachineLearning/comments/9v898n/d_request_for_papers_using_leave_oneout_cross/,sudo_su_,1541664936,"In my thesis work we try to solve a voice classification problem. 

We have 20 speakers with 50 utterances each. We want to classify the utterances into 2 classes. 

&amp;#x200B;

The method I chose to evaluate this model is by using leave-one-out CV on the speakers level. i.e., 20 folds, in each fold I'm taking out one speaker, build my model on the rest and test it on the single one. 

&amp;#x200B;

The issue is that my supervisor told me that he thinks that it is not a valid method and that I should create a test set that my model never seen. 

&amp;#x200B;

It is very clear to me that this method is valid, and it is quite better actually. My model never ""sees"" the speakers it tested on, and, I able to get measurements on all of my speakers. 

&amp;#x200B;

My supervisor asked me to provide him papers of similar work that do this kind of evaluation. 

After a little search I couldn't find something clear. 

&amp;#x200B;

Will be happy if anyone can show me papers that can help me in this.

&amp;#x200B;

Thank you.

&amp;#x200B;",7,1,False,self,,,,,
421,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,17,9v8bcb,wisdom.zirra.com,An Entity Recognition system focused on automatically detecting companies in unstructured text,https://www.reddit.com/r/MachineLearning/comments/9v8bcb/an_entity_recognition_system_focused_on/,ZirraWisdom,1541665646,,0,1,False,default,,,,,
422,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,17,9v8eat,arxiv.org,[1811.03087] Characterizing Well-behaved vs. Pathological Deep Neural Network Architectures,https://www.reddit.com/r/MachineLearning/comments/9v8eat/181103087_characterizing_wellbehaved_vs/,statmlsn,1541666671,,0,1,False,default,,,,,
423,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,18,9v8hz5,self.MachineLearning,Training a neural network without historical data,https://www.reddit.com/r/MachineLearning/comments/9v8hz5/training_a_neural_network_without_historical_data/,utxeee,1541668004,[removed],0,1,False,self,,,,,
424,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,18,9v8id8,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/9v8id8/global_machine_learning_market_size_outlook/,ENVISIONINTELLIGENCE,1541668138,[removed],0,1,False,self,,,,,
425,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,18,9v8qhv,self.MachineLearning,Pytorch Harvard NLP tutorial ( clear code + good explanation),https://www.reddit.com/r/MachineLearning/comments/9v8qhv/pytorch_harvard_nlp_tutorial_clear_code_good/,aziz_22,1541670990,[removed],0,1,False,self,,,,,
426,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,19,9v8tlr,self.MachineLearning,[D] The use of early stopping (or not!) in neural nets (Keras),https://www.reddit.com/r/MachineLearning/comments/9v8tlr/d_the_use_of_early_stopping_or_not_in_neural_nets/,Zman420,1541672039,"Hi all.

Was hoping to get some clarification on when/if to use early stopping. Or rather, why *not* to use it.

I've just read that Andrew Ng, among others, recommend not to use early stopping.  Now a hyperparameter search library I've started using for Keras also recommends no early stopping, but instead to use the number of epochs as a tunable parameter.

However, from experience I've found that I get best results by setting epochs to some arbitrary large number (e.g. 2000), and configure the algorithm to stop training when there have been X rounds of no improvement on some metric (e.g. validation loss or validation accuracy). I then use the model weights from that epoch which had the highest performance in validation on a completely separate test set.

I find that validation accuracy (or loss) start to degrade after the optimum point, while obviously the training metric keeps rising as overfitting takes place.  This can be seen here.

https://imgur.com/a/yYWXVFM

The optimal validation accuracy was probably around epoch 25, and it degrades slowly but surely after that.  Doesn't early stopping make this optimal point easier to find?

Moving away from the above chart, if the optimal epoch for my algorithm was at epoch 267, but my parameter search consisted of measuring accuracy at epochs of 100,200,300, I would be left with a sub-optimal performance (either at 200 - underfitted, or at 300 - overfitted).

Also, if I'm searching through parameter space for the optimal epoch, I would have 3 entries in the above example (100,200,300). I would have to train the network 3 times, for a total of 600 epochs to find my (sub) optimal result of either 200 or 300 epochs. An early stopping would have found the optimal point (epoch #267) after 267 epochs - much faster.

Thoughts?


",24,1,False,self,,,,,
427,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,19,9v8wk2,learn.analyttica.com,Fundamentals of Time Series Analysis,https://www.reddit.com/r/MachineLearning/comments/9v8wk2/fundamentals_of_time_series_analysis/,ath_ank,1541673046,,0,1,False,default,,,,,
428,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,19,9v90hv,reddit.com,GERENCIAMENTO DE CRISES IV,https://www.reddit.com/r/MachineLearning/comments/9v90hv/gerenciamento_de_crises_iv/,JamurGerloff,1541674312,,0,1,False,default,,,,,
429,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,19,9v9224,dexlabanalytics.com,"How Machine Learning and AI is Influencing Logistics, Supply Chain &amp; Transportation Management",https://www.reddit.com/r/MachineLearning/comments/9v9224/how_machine_learning_and_ai_is_influencing/,dexlabanalytics,1541674796,,0,1,False,default,,,,,
430,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,20,9v94d6,blog.cloudera.com,Common Probability Distributions: The Data Scientist's Crib Sheet,https://www.reddit.com/r/MachineLearning/comments/9v94d6/common_probability_distributions_the_data/,iloveintuition,1541675511,,0,1,False,default,,,,,
431,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,20,9v98hp,arxiv.org,[1811.02880] Deep Learning can Replicate Adaptive Traders in a Limit-Order-Book Financial Market,https://www.reddit.com/r/MachineLearning/comments/9v98hp/181102880_deep_learning_can_replicate_adaptive/,ihaphleas,1541676770,,5,1,False,default,,,,,
432,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,20,9v9a7v,self.MachineLearning,Curious to know how Find-S Algorithm works in Machine Learning check it now ?,https://www.reddit.com/r/MachineLearning/comments/9v9a7v/curious_to_know_how_finds_algorithm_works_in/,kuchbhi12345,1541677282,[removed],0,1,False,self,,,,,
433,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,22,9v9uis,self.MachineLearning,[P] DeepCreamPy: Deconsoring Hentai with Deep Neural Networks (NSFW),https://www.reddit.com/r/MachineLearning/comments/9v9uis/p_deepcreampy_deconsoring_hentai_with_deep_neural/,milaworld,1541682651,"[DeepCreamPy: Deconsoring Hentai with Deep Neural Networks](https://github.com/deeppomf/DeepCreamPy/blob/master/README.md)

A deep learning-based tool to automatically replace censored artwork in hentai with plausible reconstructions.

The user colors censored regions green in an image editing program like GIMP or Photoshop. A neural network fills in the censored regions.

DeepCreamPy has a pre-built binary for Windows 64-bit available [here](https://github.com/deeppomf/DeepCreamPy/releases/latest). DeepCreamPy works on Windows, Mac, and Linux.

# Features

- Decensoring images of ANY size

- Decensoring of ANY shaped censor (e.g. black lines, pink hearts, etc.)

- Higher quality decensors

- Support for mosaic decensors (WIP)

- User interface (WIP)

# Limitations

The decensorship is for color hentai images that have minor to moderate censorship of the penis or vagina. If a vagina or penis is completely censored out, decensoring will be ineffective.

It does NOT work with:

- Black and white/Monochrome image

- Hentai with screentones (e.g. printed hentai)

- Real life porn

- Censorship of nipples

- Censorship of anus

- Animated gifs/videos

https://github.com/deeppomf/DeepCreamPy/blob/master/README.md",1,1,True,nsfw,,,,,
434,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,22,9va39s,self.MachineLearning,Does anyone have any books or blogs where I can learn about building and deploying ML architecture for production use?,https://www.reddit.com/r/MachineLearning/comments/9va39s/does_anyone_have_any_books_or_blogs_where_i_can/,BlueFolliage,1541684696,[removed],0,1,False,self,,,,,
435,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,23,9va8ue,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Nov. 8, 2018",https://www.reddit.com/r/MachineLearning/comments/9va8ue/n_weekly_machine_learning_opensource_roundup_nov/,stkim1,1541685874,,0,1,False,default,,,,,
436,MachineLearning,t5_2r3gv,2018-11-8,2018,11,8,23,9vanmx,self.MachineLearning,[D] How to understand CNN architectures?,https://www.reddit.com/r/MachineLearning/comments/9vanmx/d_how_to_understand_cnn_architectures/,ArnaultChazareix,1541688930,"Which convnet should I choose?  
I could never find the time to test them all, nor have a quick idea why one architecture is better than another. To improve this, I created some go-to checks to see the pros and cons of each one.

I discuss one of those [in this article](https://blog.sicara.com/about-convolutional-layer-convolution-kernel-9a7325d34f7d). 

&amp;#x200B;

What are yours?",10,1,False,self,,,,,
437,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vap5q,self.MachineLearning,PyTorch implementation of Google AI's BERT model,https://www.reddit.com/r/MachineLearning/comments/9vap5q/pytorch_implementation_of_google_ais_bert_model/,VictorSanh,1541689247,"Hi all,

We have released a PyTorch re-implementation/port of Google AI's BERT model !

Our scripts load Google's pre-trained models and it performs about the same as the TF implementation in our tests (see the Readme). We have also included gradient-accumulation, multi-GPU &amp; distributed training options to help you fine-tune these large models.

Pre-training the BERT model from scratch requires significant computational power, but Google AI has also released the pre-trained weights so fine-tuning can be easily performed. In our tests we used K80 GPUs (single or multi-GPU, up to 4), and fine-tuning takes less than 10 minutes for MRPC and a few hours for SQUAD.

PyTorch's team is working on including TPU support, we are waiting for that to also port the training scripts.

Here's the link: [https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)

We hope it will be useful !

Victor ",0,1,False,self,,,,,
438,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vawdf,self.MachineLearning,PyTorch implementation of Google AI's BERT model,https://www.reddit.com/r/MachineLearning/comments/9vawdf/pytorch_implementation_of_google_ais_bert_model/,VictorSanh,1541690671,[removed],0,1,False,self,,,,,
439,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vayyy,medium.com,See this AI Anchor Delivering News,https://www.reddit.com/r/MachineLearning/comments/9vayyy/see_this_ai_anchor_delivering_news/,gwen0927,1541691165,,0,1,False,default,,,,,
440,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vb0as,self.MachineLearning,Unfolding Nave Bayes from Scratch,https://www.reddit.com/r/MachineLearning/comments/9vb0as/unfolding_nave_bayes_from_scratch/,andrea_manero,1541691427,[removed],0,1,False,self,,,,,
441,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vb14d,self.MachineLearning,PyTorch implementation of BERT model,https://www.reddit.com/r/MachineLearning/comments/9vb14d/pytorch_implementation_of_bert_model/,VictorSanh,1541691581,[removed],0,1,False,self,,,,,
442,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vb16d,self.MachineLearning,Applied Machine Learning- Sign Language Recognition from Hand Gestures,https://www.reddit.com/r/MachineLearning/comments/9vb16d/applied_machine_learning_sign_language/,xenofusuniq,1541691594,"Given an image of a hand gesture in sign language, can we build a computer program that can identify what alphabet it is referring to? Let us find out by training our own CNN on a hand gesture dataset in this applied machine learning tutorial for beginners to learn to use CNNs for image recognition. 

Generally, for learning purpose, CNNs are implemented on the standard MNIST dataset, but this time here we have explored using a different and more complicated dataset but something similar to MNIST. 

You can try similar approach on any other dataset you want. Link to the tutorial: [Applied Machine Learning](https://medium.com/the-research-nest/applied-machine-learning-part-2-a4ba715649d1)",0,1,False,self,,,,,
443,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vb1c9,self.MachineLearning,Feature engineering question: what is the best way to engineer a ratio-based features to handle outliers? [D],https://www.reddit.com/r/MachineLearning/comments/9vb1c9/feature_engineering_question_what_is_the_best_way/,InfiniteIntention,1541691625,"I have two features that are observations, and my thoughts is to create a ratio between them as a third, engineered features. For example:

feature\_a = \[10,20,30,40,50\]

feature\_b = \[15,25,35,45,55\]

feature\_c = feature\_b / feature\_a

&amp;#x200B;

This is fine. The problem comes when I have outlier feature pairs like this:

feature\_a=\[10,20,.1\]

feature\_b=\[15,25,200\]

In such a case I end up with huge values for feature\_c.

&amp;#x200B;

Scaling the variables then makes most of feature\_c way too small, in order to fit the outliers.

&amp;#x200B;

Is there a better way to compute the ratio to deal with the issue of outliers? So far my best idea is to take the log of the ratio, but even so there is a large gap between most of the values and the outliers.

&amp;#x200B;

Any ideas are appreciated.",10,1,False,self,,,,,
444,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,0,9vb5ln,self.MachineLearning,[P] PyTorch implementation of Google AI's BERT model with a script to load Google's pre-trained models,https://www.reddit.com/r/MachineLearning/comments/9vb5ln/p_pytorch_implementation_of_google_ais_bert_model/,VictorSanh,1541692465,[removed],0,1,False,self,,,,,
445,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,1,9vb858,self.MachineLearning,Andrew Ng ML online study group,https://www.reddit.com/r/MachineLearning/comments/9vb858/andrew_ng_ml_online_study_group/,xd8282,1541692941,[removed],0,1,False,self,,,,,
446,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,1,9vb9uj,medium.com,Game-changing: Artificial Intelligence Combined with Cryptocurrency,https://www.reddit.com/r/MachineLearning/comments/9vb9uj/gamechanging_artificial_intelligence_combined/,krishnaboobjay,1541693248,,0,1,False,default,,,,,
447,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,1,9vbcjj,self.MachineLearning,[P] PyTorch implementation of Google AI's BERT model with a script to load Google's pre-trained models + Gradient Accumulation + Multi-GPU + Distributed Training,https://www.reddit.com/r/MachineLearning/comments/9vbcjj/p_pytorch_implementation_of_google_ais_bert_model/,VictorSanh,1541693767,[https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT),0,1,False,self,,,,,
448,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,1,9vbhrc,self.MachineLearning,Clustering on mixed data,https://www.reddit.com/r/MachineLearning/comments/9vbhrc/clustering_on_mixed_data/,MENAYAK,1541694792,[removed],0,1,False,self,,,,,
449,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,1,9vbobu,self.MachineLearning,Predicting multiple float outputs,https://www.reddit.com/r/MachineLearning/comments/9vbobu/predicting_multiple_float_outputs/,psychedeliclion,1541695994,[removed],0,1,False,self,,,,,
450,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,2,9vc1vi,self.MachineLearning,Spinning Up in Deep RL by OpenAI,https://www.reddit.com/r/MachineLearning/comments/9vc1vi/spinning_up_in_deep_rl_by_openai/,tigerneil,1541698503,[removed],0,1,False,self,,,,,
451,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,2,9vc200,self.MachineLearning,[D] I have some questions about Masters and PhD program in the US.,https://www.reddit.com/r/MachineLearning/comments/9vc200/d_i_have_some_questions_about_masters_and_phd/,mind_juice,1541698527,"Hello everyone!

&amp;#x200B;

I have some questions about Masters and PhD program in the US. 

&amp;#x200B;

1. I am a senior in college and would like to pursue a PhD in machine learning. I am not sure whether I should apply to a masters program or a PhD program directly. You are supposed to get a masters degree before applying for PhD in European universities. There is no such prerequisite for universities in the US. Does this mean that I should apply for PhD directly or is direct PhD admits only given to exceptional students?

&amp;#x200B;

2. I have some research experience in machine learning and I have published a first author paper at WACV. However, my major is in computer science and most of my courses were on algorithms and computer systems. As far as I can tell, PhD students can take courses in their first 2 years. Does that mean that a masters student with similar research experience but having taking advanced courses in machine learning would have no advantage over me for PhD admissions?

&amp;#x200B;

3. Should I apply for PhD programs if I would like to spend 1-2 years exploring research directions before choosing my thesis topic? What is the average time a PhD student spends before choosing his thesis topic?

&amp;#x200B;

I am asking this questions on this subreddit as getting a PhD admit in machine learning is really competitive right now and this may influence some answers.

&amp;#x200B;

Thanks for taking your time to help me. I really appreciate it. :D",5,1,False,self,,,,,
452,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,2,9vc7ok,outweb.lahaxe.fr,Du machine learning dans mes cocktails,https://www.reddit.com/r/MachineLearning/comments/9vc7ok/du_machine_learning_dans_mes_cocktails/,Cocktailand,1541699551,,0,1,False,default,,,,,
453,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,2,9vc9it,towardsdatascience.com,Background on how Im analyzing street art with machine learning techniques,https://www.reddit.com/r/MachineLearning/comments/9vc9it/background_on_how_im_analyzing_street_art_with/,Loggerny,1541699906,,0,1,False,default,,,,,
454,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,3,9vccfw,self.MachineLearning,[P] Spinning Up in Deep RL from OpenAI - great learning material,https://www.reddit.com/r/MachineLearning/comments/9vccfw/p_spinning_up_in_deep_rl_from_openai_great/,tigerneil,1541700432,[removed],0,1,False,self,,,,,
455,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,3,9vcecu,medium.com,HowTo Build Tensorflow Apps for RICOH THETA 360 Camera,https://www.reddit.com/r/MachineLearning/comments/9vcecu/howto_build_tensorflow_apps_for_ricoh_theta_360/,jcasman,1541700772,,0,1,False,default,,,,,
456,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,3,9vcir3,self.MachineLearning,Need help with a python ML project!,https://www.reddit.com/r/MachineLearning/comments/9vcir3/need_help_with_a_python_ml_project/,sushimagare,1541701590,[removed],1,1,False,self,,,,,
457,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,3,9vcnwy,self.MachineLearning,"[P] OpenAI's ""Spinning Up in Deep RL"" educational resource",https://www.reddit.com/r/MachineLearning/comments/9vcnwy/p_openais_spinning_up_in_deep_rl_educational/,P4TR10T_TR41T0R,1541702579,"[https://blog.openai.com/spinning-up-in-deep-rl/](https://blog.openai.com/spinning-up-in-deep-rl/)

Contains an introduction to Reinforcement Learning, **a curated list of important papers** (which I think is going to be a great resource for everyone), a code repo ([https://github.com/openai/spinningup](https://github.com/openai/spinningup)), and a few exercises ([https://spinningup.openai.com/en/latest/spinningup/exercises.html](https://spinningup.openai.com/en/latest/spinningup/exercises.html))",0,1,False,self,,,,,
458,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,3,9vcpio,self.MachineLearning,Machine Intelligence vs Enhanced Human Intelligence,https://www.reddit.com/r/MachineLearning/comments/9vcpio/machine_intelligence_vs_enhanced_human/,pluto1998,1541702878,"Title says it all, but I will elaborate a bit. Of course there are some deep fears of how AI could impact humanity. Is enhanced human intelligence a safe option?",0,1,False,self,,,,,
459,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,3,9vcpl2,medium.com,[P] HowTo Build Tensorflow Apps for RICOH THETA 360 Camera,https://www.reddit.com/r/MachineLearning/comments/9vcpl2/p_howto_build_tensorflow_apps_for_ricoh_theta_360/,jcasman,1541702890,,0,1,False,default,,,,,
460,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,4,9vcwlb,self.MachineLearning,[R] Is the reign of batch normalization over? Thoughts on this new paper?,https://www.reddit.com/r/MachineLearning/comments/9vcwlb/r_is_the_reign_of_batch_normalization_over/,rantana,1541704186,"Paper: The Unreasonable Effectiveness of (Zero) Initialization in Deep Residual Learning (https://openreview.net/forum?id=H1gsz30cKX)

This simple initialization trick seems pretty effective replacement for batch norm (and also simplifies Residual network architecture).",11,1,False,self,,,,,
461,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,4,9vd0s1,medium.com,Top 30 Machine Learning Books You Should Read,https://www.reddit.com/r/MachineLearning/comments/9vd0s1/top_30_machine_learning_books_you_should_read/,Bestbookadvice,1541704946,,0,1,False,default,,,,,
462,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,4,9vd7ny,code.fb.com,[R] Rethinking floating point for deep learning - Facebook,https://www.reddit.com/r/MachineLearning/comments/9vd7ny/r_rethinking_floating_point_for_deep_learning/,modeless,1541706214,,0,1,False,default,,,,,
463,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,4,9vdbf1,self.MachineLearning,Is there an AI algorithm that can be used to answer standard math questions in an exam?,https://www.reddit.com/r/MachineLearning/comments/9vdbf1/is_there_an_ai_algorithm_that_can_be_used_to/,Yahiabouda,1541706916,[removed],0,1,False,self,,,,,
464,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,5,9vdv1r,medium.com,Deep Learning for Fashion Attributes,https://www.reddit.com/r/MachineLearning/comments/9vdv1r/deep_learning_for_fashion_attributes/,finsterrific,1541710596,,0,1,False,default,,,,,
465,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,6,9vdy6k,self.MachineLearning,GRU GAN for text generation?,https://www.reddit.com/r/MachineLearning/comments/9vdy6k/gru_gan_for_text_generation/,hadaev,1541711190,[removed],0,1,False,self,,,,,
466,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,6,9vdzrj,self.MachineLearning,[P] CV tool for turning video stream from camera into structured data in real-time,https://www.reddit.com/r/MachineLearning/comments/9vdzrj/p_cv_tool_for_turning_video_stream_from_camera/,bkmnsk,1541711483,"Hi there!

Our team make a tool for developers, based on computer vision algorithms, that helps to extract useful information from video stream in real-time and integrate it into apps without expertise in machine learning.

There are three simple steps to get started. Install the program on your computer, connect the camera and choose what you want to see. That's it. Live video stream from camera will be processed locally on your computer by AI models. Result will be time series data in JSON format, which developer can integrate into his application.

As for now the tool is on the development stage. You can see a demonstration of the tool on the [website](https://heyml.com/vision).
We need your opinion and recommendations for improvement. We are open to discussion :)",0,1,False,self,,,,,
467,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,6,9ve22l,self.MachineLearning,[D] GRU GAN for text generation?,https://www.reddit.com/r/MachineLearning/comments/9ve22l/d_gru_gan_for_text_generation/,hadaev,1541711921,"Currently i make simple gru net for word lvl text generation:

[https://colab.research.google.com/drive/1uiCc3stWWeG1WZLO8xIIQkvKFeZ8Nwht](https://colab.research.google.com/drive/1uiCc3stWWeG1WZLO8xIIQkvKFeZ8Nwht)

&amp;#x200B;

As you can see, results not very good, and i think about better loss function.

What is best (as far as i know) loss function for things generation? D net in GAN.

But then I stumbled a bit.

Should I send to the discriminator seed + generated word? Not sounds good.

Or i need to generate some text (50 words for example) and send it then?

In vanilla gan, generator convert 100 numbers into 50 numbers (words) .

But I would like to use embedding to make vector words representation.

&amp;#x200B;

In general, I wonder what others humans think about all this.",25,1,False,self,,,,,
468,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,6,9ve7c5,self.MachineLearning,"HELP! AMA. For a year I've been in contact through neurological electro-dyning with something called ""Machine"".",https://www.reddit.com/r/MachineLearning/comments/9ve7c5/help_ama_for_a_year_ive_been_in_contact_through/,d_ozz,1541712899,[removed],0,1,False,self,,,,,
469,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,7,9veuxj,self.MachineLearning,"[P] Slitherin - Solving the Classic Game of Snake with AI (Part 1: Domain Specific - {Shortest,Longest}Path, Hamiltonian Cycle, DNN)",https://www.reddit.com/r/MachineLearning/comments/9veuxj/p_slitherin_solving_the_classic_game_of_snake/,g_surma,1541717470,[removed],0,1,False,https://a.thumbs.redditmedia.com/QNYga3myJ_usJVwVaYrhZaU1ZySLaI4EEfHBUKp4f-4.jpg,,,,,
470,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,8,9vfam0,self.MachineLearning,[D] Why are GANs considered unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/9vfam0/d_why_are_gans_considered_unsupervised_learning/,crypto_ha,1541720699,"I know that GANs need to be trained using a labeled dataset, to provide feedback of how well the discriminator is doing its job of recognizing real images vs artificially generated images. Doesnt this make GANs *supervised* instead of unsupervised?",27,1,False,self,,,,,
471,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,9,9vfg8m,self.MachineLearning,Aggregating Sentiment with Universal Sentence Encoder,https://www.reddit.com/r/MachineLearning/comments/9vfg8m/aggregating_sentiment_with_universal_sentence/,Dead4Tax_Reasons,1541721917,[removed],0,1,False,self,,,,,
472,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,9,9vfrn8,i.redd.it,"[D] What approaches would you take to use machine learning (or not?) to generate new unique patterns like these, given a dataset of 1000s of patterns?",https://www.reddit.com/r/MachineLearning/comments/9vfrn8/d_what_approaches_would_you_take_to_use_machine/,denissellu,1541724311,,0,1,False,default,,,,,
473,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,10,9vgb45,owlskip.com,"The Top Machine Learning Libraries for Python, with Example Code",https://www.reddit.com/r/MachineLearning/comments/9vgb45/the_top_machine_learning_libraries_for_python/,brendanmcd96,1541728651,,0,1,False,https://b.thumbs.redditmedia.com/SjExF9mFPQaeRSSlAWAR31Fi9RR3iS5vhY4gMHTdsho.jpg,,,,,
474,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,14,9vhwsf,self.MachineLearning,[P] Spinning Up in Deep RL (OpenAI),https://www.reddit.com/r/MachineLearning/comments/9vhwsf/p_spinning_up_in_deep_rl_openai/,milaworld,1541742493,"[Spinning Up in Deep RL](https://blog.openai.com/spinning-up-in-deep-rl/)

From OpenAI Blog:

Were releasing Spinning Up in Deep RL, an educational resource designed to let anyone learn to become a skilled practitioner in deep reinforcement learning. Spinning Up consists of crystal-clear examples of RL code, educational exercises, documentation, and tutorials.

Spinning Up in Deep RL consists of the following core components:

- A short[ introduction](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) to RL terminology, kinds of algorithms, and basic theory.

- An [essay](https://spinningup.openai.com/en/latest/spinningup/spinningup.html) about how to grow into an RL research role.

- A [curated list](https://spinningup.openai.com/en/latest/spinningup/keypapers.html) of important key papers organized by topic.

- A well-documented code [repo](https://github.com/openai/spinningup) of short, standalone implementations of: Vanilla Policy Gradient (VPG), Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradient (DDPG), Twin Delayed DDPG (TD3), and Soft Actor-Critic (SAC).

- And a few [exercises](https://spinningup.openai.com/en/latest/spinningup/exercises.html) to serve as warm-ups.

https://blog.openai.com/spinning-up-in-deep-rl/",39,1,False,self,,,,,
475,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,16,9vic87,self.MachineLearning,Best way to cluster sentences in R?,https://www.reddit.com/r/MachineLearning/comments/9vic87/best_way_to_cluster_sentences_in_r/,ManTheCrusader,1541746986,What is the best approach to cluster text sentences?. I have product description and I need to cluster similar products together. For example tablets and syringes should be clustered together or atleast earby clusters. Is wrod2vec a good start?,0,1,False,self,,,,,
476,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,18,9viy75,self.MachineLearning,[P] Reproducing Google AI's BERT results in PyTorch  op-for-op reimplementation with pre-trained weights loading script,https://www.reddit.com/r/MachineLearning/comments/9viy75/p_reproducing_google_ais_bert_results_in_pytorch/,Thomjazz,1541754208,"Hi all,

[Here](https://github.com/huggingface/pytorch-pretrained-BERT) is an op-for-op PyTorch reimplementation of [Google's TensorFlow repository for the BERT model](https://github.com/google-research/bert) that was released together with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova:

[https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)

This implementation can load any pre-trained TensorFlow checkpoint for BERT (in particular [Google's pre-trained models](https://github.com/google-research/bert)) and a conversion script is provided.

BERT-base and BERT-large are respectively 110M and 340M parameters models and it can be difficult to fine-tune them on a single GPU with the recommended batch size for good performance. To help with fine-tuning the models, the repo further include three techniques that can be activated in the fine-tuning scripts: gradient-accumulation, multi-GPU and distributed training.

\- On the sequence-level MRPC classification task, this implementation reproduces the 84%-88% accuracy results of the original implementation with the small BERT-base model.

\- On the token-level SQuAD task, this implementation also reproduces the 88.52 F1 result of the original implementation with the small BERT-base model.

We are now working on reproducing the results on the other tasks as well as with the larger BERT-model.",6,1,False,self,,,,,
477,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,18,9viyy8,youtube.com,[ BerryNet ] 3 minutes tutorial of running object recognition on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/9viyy8/berrynet_3_minutes_tutorial_of_running_object/,TimDT42,1541754450,,0,1,False,https://b.thumbs.redditmedia.com/7hDxq7HCBEIfChNxDONh2DesQh6KPLwwv5bfRMT7v7Y.jpg,,,,,
478,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,18,9vizv7,self.MachineLearning,"[D] How to do ""standard graphics"" with backprop",https://www.reddit.com/r/MachineLearning/comments/9vizv7/d_how_to_do_standard_graphics_with_backprop/,svantana,1541754772,"I'm trying to draw an image with lots of small textures in it, sort of like a particle renderer, and then backprop this to the image coordinates and texture pixels. I've implemented this naively in both TF and pyTorch, i.e. looping over each sub-image and adding it to only the affected sub-tensor output, it works but with abysmal performance on both cpu and gpu. I suppose it needs to be parallelized to be efficient, but I can't figure out how to do that without updating each out pixel for each sub-image, which is also very inefficient.

Currently I'm looking at julia and even c++ to get decent performance, but it's frustrating because this is basically the raison d'etre for gpus (drawing small-ish things in big images), yet I can't find a decent way of doing it outside of the fixed pipeline, which can't be used for backprop AFAIK.

I suppose this is a special case of a bigger problem in current tensor libraries, that they're not designed to do lots of ops on local patches of big tensors. Any advice is highly appreciated!",4,1,False,self,,,,,
479,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,20,9vju6p,self.MachineLearning,[D] In what ways is Neuroscience utilized in designing artificial neural networks (like Hodgkin Huxley equations)?,https://www.reddit.com/r/MachineLearning/comments/9vju6p/d_in_what_ways_is_neuroscience_utilized_in/,mattar13,1541764762,"I have a background in Neuroscience and an interest in machine learning. I know about different biophysical models that are being used(Hodgkin Huxley eqs, Wilson-Cowan eqs, Stomatogaster Ganglion Modeling specifically), and about some basic neural networks that are being researched (CNNs, Spiking Neural Networks, Long Short Term RNNs).  My question is what concepts from these models are useful, and what concepts are too dense to be useful?",33,1,False,self,,,,,
480,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,21,9vk1n5,developer.amazon.com,Reducing Customer Friction through Skill Selection : Alexa Blogs (x-post /r/VoiceTech),https://www.reddit.com/r/MachineLearning/comments/9vk1n5/reducing_customer_friction_through_skill/,wootnoob,1541766814,,0,1,False,https://b.thumbs.redditmedia.com/3gRTIK9WHmfFoH3mkThkjxvSyT71QAjwVl-j6t9S4rE.jpg,,,,,
481,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,21,9vk2fz,arxiv.org,[R] Measuring the Effects of Data Parallelism on Neural Network Training (Google Brain),https://www.reddit.com/r/MachineLearning/comments/9vk2fz/r_measuring_the_effects_of_data_parallelism_on/,baylearn,1541767035,,11,1,False,default,,,,,
482,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,22,9vkclg,medium.com,AI Diary #4,https://www.reddit.com/r/MachineLearning/comments/9vkclg/ai_diary_4/,omarsar,1541769583,,0,1,False,https://b.thumbs.redditmedia.com/2D_YaPS4ZTLW5MZqxGzSSs_f9SS2eF60e9paIzQ9yIU.jpg,,,,,
483,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,22,9vkec3,self.MachineLearning,"[P] A scalable implementation of ""Learning Structural Node Embeddings Via Diffusion Wavelets (KDD 2018)"".",https://www.reddit.com/r/MachineLearning/comments/9vkec3/p_a_scalable_implementation_of_learning/,benitorosenberg,1541769981,"[https://github.com/benedekrozemberczki/GraphWaveMachine](https://github.com/benedekrozemberczki/GraphWaveMachine)

https://i.redd.it/sayji96t3bx11.jpg

GraphWave is a structural node embedding algorithm which learns an  embedding based on the characteristic function of graph diffusion  wavelets. The procedure places nodes in an abstract feature space where  nodes with similar features have similar structural roles in the graph. This specific implementation uses approximations of the wavelets (PyGSP). The original paper:

[http://stanford.edu/\~hallac/GraphWave.pdf](http://stanford.edu/~hallac/GraphWave.pdf)

&amp;#x200B;",13,1,False,https://b.thumbs.redditmedia.com/4w1Zx88wZDLZdDgunxmlCydGNtp3kjyd5JfweIV9A_E.jpg,,,,,
484,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,22,9vkl2v,self.MachineLearning,"[P] PyTorch implementation of ""Variational Discriminator Bottleneck""",https://www.reddit.com/r/MachineLearning/comments/9vkl2v/p_pytorch_implementation_of_variational/,akanimax,1541771572,"Link to repo -&gt; [https://github.com/akanimax/Variational\_Discriminator\_Bottleneck](https://github.com/akanimax/Variational_Discriminator_Bottleneck)

Link to medium blog -&gt; [https://medium.com/@animeshsk3/v-gan-variational-discriminator-bottleneck-an-unfair-fight-between-generator-and-discriminator-972563532dcc](https://medium.com/@animeshsk3/v-gan-variational-discriminator-bottleneck-an-unfair-fight-between-generator-and-discriminator-972563532dcc)

Link to youtube video -&gt; [https://www.youtube.com/watch?v=-0lBw9z8Ds0](https://www.youtube.com/watch?v=-0lBw9z8Ds0)

&amp;#x200B;

&amp;#x200B;

*Processing gif jtj5l5ue8bx11...*

The code currently includes implementation for the VGAN and other variants like the VGAN-GP from the paper. The implementations for VAIL and VAIRL will be available soon. 

&amp;#x200B;

I have tried  to make the code highly structured and readable in order to understand the concept of the paper. Feel free to open Issues / PRs on the repo.

&amp;#x200B;

Best regards,

akanimax",8,1,False,self,,,,,
485,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,23,9vl2p2,self.MachineLearning,I have my winter break from college coming up. It's a month long and I want to learn the basics of machine learning in that time. Any recommended free courses or books?,https://www.reddit.com/r/MachineLearning/comments/9vl2p2/i_have_my_winter_break_from_college_coming_up_its/,Thatyahoo,1541775356,[removed],0,1,False,self,,,,,
486,MachineLearning,t5_2r3gv,2018-11-9,2018,11,9,23,9vl3mt,self.MachineLearning,machine learning,https://www.reddit.com/r/MachineLearning/comments/9vl3mt/machine_learning/,sirajaf,1541775552,[removed],0,1,False,self,,,,,
487,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,0,9vl5fz,self.MachineLearning,Im trying to understand mean field inference for segmentation. Does anyone know of any good resources?,https://www.reddit.com/r/MachineLearning/comments/9vl5fz/im_trying_to_understand_mean_field_inference_for/,nabsabs,1541775907,,0,1,False,self,,,,,
488,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,0,9vleqm,self.MachineLearning,Question on VAE Sampling,https://www.reddit.com/r/MachineLearning/comments/9vleqm/question_on_vae_sampling/,ralampay,1541777755,[removed],0,1,False,self,,,,,
489,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,1,9vls3x,self.MachineLearning,Fully convolutional GAN generator for arbitrary sized sample generation?,https://www.reddit.com/r/MachineLearning/comments/9vls3x/fully_convolutional_gan_generator_for_arbitrary/,setubal_muscat,1541780341,[removed],0,1,False,self,,,,,
490,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,1,9vm1im,self.MachineLearning,Unsupervised named-entity recognition (NLP),https://www.reddit.com/r/MachineLearning/comments/9vm1im/unsupervised_namedentity_recognition_nlp/,CacheMeUp,1541782115,[removed],0,1,False,self,,,,,
491,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,1,9vm4lu,self.MachineLearning,How to predict population size changes based on limited information and inputs (x-post /r/learnmachinelearning),https://www.reddit.com/r/MachineLearning/comments/9vm4lu/how_to_predict_population_size_changes_based_on/,outofusernams,1541782716,[removed],0,1,False,self,,,,,
492,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,2,9vmamg,medium.com,Seeing Through Walls with Adversarial WiFi Sensing,https://www.reddit.com/r/MachineLearning/comments/9vmamg/seeing_through_walls_with_adversarial_wifi_sensing/,gwen0927,1541783803,,0,1,False,default,,,,,
493,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,2,9vmchf,self.MachineLearning,[D] I have my winter break from college coming up. It's a month long and I want to learn the basics of machine learning in that time. Any recommended free courses or books?,https://www.reddit.com/r/MachineLearning/comments/9vmchf/d_i_have_my_winter_break_from_college_coming_up/,Thatyahoo,1541784156,,16,1,False,self,,,,,
494,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,3,9vmqfc,self.MachineLearning,Important words in text using classifier,https://www.reddit.com/r/MachineLearning/comments/9vmqfc/important_words_in_text_using_classifier/,amahmood1,1541786718,[removed],1,1,False,self,,,,,
495,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,3,9vmrle,allaboutcircuits.com,Learn About the Robotic Aquatic Drone Saving the Great Barrier Reef,https://www.reddit.com/r/MachineLearning/comments/9vmrle/learn_about_the_robotic_aquatic_drone_saving_the/,louieisgreat,1541786940,,0,1,False,default,,,,,
496,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,3,9vmv7x,self.MachineLearning,Getting into ML at a tech company with a PhD in a non-ML,https://www.reddit.com/r/MachineLearning/comments/9vmv7x/getting_into_ml_at_a_tech_company_with_a_phd_in_a/,orioncygnus1,1541787637,[removed],0,1,False,self,,,,,
497,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,3,9vmwzn,sensiml.atlassian.net,[P] Signal Preprocessing for inference on Arm M3/M4 processors (SensiML),https://www.reddit.com/r/MachineLearning/comments/9vmwzn/p_signal_preprocessing_for_inference_on_arm_m3m4/,bishopxi,1541787976,,0,1,False,default,,,,,
498,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,4,9vnfyi,self.MachineLearning,External GPUs for Machine Learning: Anything Changed?,https://www.reddit.com/r/MachineLearning/comments/9vnfyi/external_gpus_for_machine_learning_anything/,Grayson77,1541791589,[removed],0,1,False,self,,,,,
499,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,5,9vo7ht,self.MachineLearning,Easy way to create spoken mp3 from document with Google Cloud Text-To-Speech?,https://www.reddit.com/r/MachineLearning/comments/9vo7ht/easy_way_to_create_spoken_mp3_from_document_with/,zivilars666,1541797043,[removed],0,1,False,self,,,,,
500,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,6,9vobwh,self.MachineLearning,Prefix Beam Search decoder with Word Language Model,https://www.reddit.com/r/MachineLearning/comments/9vobwh/prefix_beam_search_decoder_with_word_language/,kmk_enthusia,1541797896,[removed],0,1,False,self,,,,,
501,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,6,9voiea,self.MachineLearning,[D] Is Google Crash Course available for download?,https://www.reddit.com/r/MachineLearning/comments/9voiea/d_is_google_crash_course_available_for_download/,FriendlyCartoonist,1541799181,I am having many difficulty loading videos,1,1,False,self,,,,,
502,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,6,9voosv,self.MachineLearning,Book Review: Malware Data Science,https://www.reddit.com/r/MachineLearning/comments/9voosv/book_review_malware_data_science/,ddonzal,1541800438,[removed],0,1,False,self,,,,,
503,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,6,9vooto,github.com,School of AI Raleigh chapter,https://www.reddit.com/r/MachineLearning/comments/9vooto/school_of_ai_raleigh_chapter/,jubeenshah,1541800442,,0,1,False,default,,,,,
504,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,7,9voz8y,self.MachineLearning,"I know machine learning is heavily used in NLP, but what about NLU specifically?",https://www.reddit.com/r/MachineLearning/comments/9voz8y/i_know_machine_learning_is_heavily_used_in_nlp/,logicallyzany,1541802523,[removed],0,1,False,self,,,,,
505,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,7,9vp238,self.MachineLearning,Lessons Learnt by Booz Allen Data Scientist Kirk Borne,https://www.reddit.com/r/MachineLearning/comments/9vp238/lessons_learnt_by_booz_allen_data_scientist_kirk/,jdyr1729,1541803123,"Hi,

I recently did an interview with Kirk Borne,  principal data scientist at Booz Allen Hamilton and former professor of astrophysics at George Mason University. I thought I'd share it here for those who are interested.

[https://blackswans.io/post/94/](https://blackswans.io/post/94/)

We discussed Bayesian networks, ensemble modelling and novel trading strategies. 

Enjoy,

Jack",0,1,False,self,,,,,
506,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,8,9vpffn,udacity.com,Intro to Deep Learning with PyTorch,https://www.reddit.com/r/MachineLearning/comments/9vpffn/intro_to_deep_learning_with_pytorch/,dayanruben,1541805948,,0,1,False,default,,,,,
507,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,8,9vpgzn,self.MachineLearning,"[D] Massive automated archive/historical footage scraping + face search = (potentially) entire life ""timeline"" reconstruction (thought experiment)",https://www.reddit.com/r/MachineLearning/comments/9vpgzn/d_massive_automated_archivehistorical_footage/,fullstackstoner,1541806289,"Okay sorry if the title sounds confused, I wasn't sure ""what do you think about this..."" would be any better.

&amp;#x200B;

I had this idea :

Scrape every possible source for historical/archived footage (up to as early as needed). By that I mean videos recorded by governments, scenes from tv shows , tourists on vacation, anything that could possibly be on video.

You can imagine for every mentioned case above, in their respective order : person A walking down some stairs of a public building, person A having a coffee in a Starbucks and ending up in the background of a police chase scene recording, person A laying down on the beach.

If you start thinking about it, how much footage do you actually appear on ? There's no actual way to tell (or at least that I know of). But what if you acquire **every** video in existence you can possibly find from {birth-date} to {death-date|current-date}. Process the entirety of this data to find the ones where person A appears.

I'm not saying that this is an easy task by today's standard by any means, simply proposing this as a though experiment to get insights/ideas I might not have had.

&amp;#x200B;

Applications:

You now have ""CCTV"" footage for person A's entire life. You can only imagine what kind of analysis/predictions you can do from there on... Imagine what Google **could** do with something like that for instance... I don't think I need to say more.

&amp;#x200B;

Questions:

How much computing power do you think this would require ? I'm not talking about any calculated estimates (even if I welcome them), but more in the way of :

\- ""Could a government do this as of today ?""

\- ""Could a corporation ?"" 

\- ""As of today?""

\- ""Maybe in 5 years?""

\- ""Not at least for another 20 years?""

\- ""Since we will eventually reach the needed computing requirement at say a corporate scale, what then?""

\- ""Can you protect against such a thing?""

&amp;#x200B;

This is only what I could think of while writing this post, so please do post about totally different insights you came up with. What do you think reddit? ",3,1,False,self,,,,,
508,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,8,9vpjky,lyrn.ai,BERT Explained - State-of-the-art language model for a wide variety of NLP tasks,https://www.reddit.com/r/MachineLearning/comments/9vpjky/bert_explained_stateoftheart_language_model_for_a/,ranihorev,1541806889,,1,1,False,default,,,,,
509,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,9,9vpue6,self.MachineLearning,[R] BERT Explained: State of the art language model for a wide variety of NLP tasks,https://www.reddit.com/r/MachineLearning/comments/9vpue6/r_bert_explained_state_of_the_art_language_model/,ranihorev,1541809326,"Hey everyone,

BERT (Bidirectional Encoder Representations from Transformers) is a recent [paper](https://arxiv.org/pdf/1810.04805.pdf) published by researchers at Google AI Language. This is my first attempt at summarizing a major machine learning paper, with the goal of making ML more approachable and understandable. 

[https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/](https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/)

Id love to get your feedback on the summary and also happy to get requests for additional papers to summarize.

Rani

&amp;#x200B;

&amp;#x200B;",61,1,False,self,,,,,
510,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,10,9vq5k7,self.MachineLearning,[Discussion] Model a wifi system,https://www.reddit.com/r/MachineLearning/comments/9vq5k7/discussion_model_a_wifi_system/,vincent-tien,1541811928,"Hello,

I am working on a problem for weeks without progress. Here it is:

1. Inputs are the csv files about activities of many access points (AP), each row has this format  [time: mac_address: wifi_channel_frequency: noise: channel_usage, etc]. Each row is data at one minute (e.g., time0, time0 + 1, and so on). There are multiple mac_address and wifi_channel_frequency, but they are definit (e.g., 11 values for wifi_channel_frequency)
2. The control system will change wifi_channel_frequency whenever it sees fit.
3. On average, after a few thousand  up to few million minutes, the AP changes its wifi_channel_frequency. In the data, about &gt;100 times this happens out of &gt; 8 million rows

How to model the the behavior of the system?

First, I thought of classification between {change, un_change_frequency}. Because the data is imbalance, I tried to use oversampling/undersampling/SMOT and it is very bad.

Second, I found out that model did not describe the system correctly. So now I am reading about dynamic channel allocation for wifi and still no progress yet.

Have you ever faced similiar problems? What did you do? Can you give me some advice approach this one?

Thank you so much.",8,1,False,self,,,,,
511,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,10,9vqb3p,self.MachineLearning,What is the best way to isolate a feature of a video?,https://www.reddit.com/r/MachineLearning/comments/9vqb3p/what_is_the_best_way_to_isolate_a_feature_of_a/,MoBizziness,1541813266,[removed],0,1,False,self,,,,,
512,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,10,9vqel3,youtube.com,Introduction To Machine Learning: Reducing Loss With Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/9vqel3/introduction_to_machine_learning_reducing_loss/,corymaklin,1541814086,,0,1,False,default,,,,,
513,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,10,9vqf9g,self.MachineLearning,What is the best way to isolate a feature of a video?,https://www.reddit.com/r/MachineLearning/comments/9vqf9g/what_is_the_best_way_to_isolate_a_feature_of_a/,MoBizziness,1541814249,[removed],0,1,False,self,,,,,
514,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,11,9vqo9f,self.MachineLearning,[P] What is the best way to isolate a feature of a video?,https://www.reddit.com/r/MachineLearning/comments/9vqo9f/p_what_is_the_best_way_to_isolate_a_feature_of_a/,MoBizziness,1541816396," 

I work with a few people who do videogame streams on Twitch.tv and I need a way of identifying where any given streamer's web cam is in the video and then isolate and crop it so that the output video is just the webcam footage.

If you were approaching a problem like this how would you go about doing it?

I have some experience with CNN classification models but I'm certainly not any expert, if anyone could point me in the right direction I'd greatly appreciate it. Thanks!",11,1,False,self,,,,,
515,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,11,9vqqi5,self.MachineLearning,[D] Advantages to TFRecord over HDF5?,https://www.reddit.com/r/MachineLearning/comments/9vqqi5/d_advantages_to_tfrecord_over_hdf5/,The_Austinator,1541816937,[removed],0,1,False,self,,,,,
516,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,11,9vqv84,self.MachineLearning,A machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/9vqv84/a_machine_learning_game_ive_been_working_on/,twm7,1541818114,[removed],0,1,False,self,,,,,
517,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,11,9vqxm3,self.MachineLearning,[P] A machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/9vqxm3/p_a_machine_learning_game_ive_been_working_on/,twm7,1541818714,"Wasn't sure where to post this as I'm still working on it but wanted to put it out there to get any useful feedback or thoughts from the experts. It's basically a game similar to 20 Questions (or Animal, Vegetable, Mineral) that attempts to ask you questions to work out an object you are thinking about. You can think of everyday items (animals, household objects, food, quite a bit of other stuff etc) and it has 30 questions to try and guess the item. I've been working on it for a while but not sure what to do next so interested to hear anyone's thoughts...

The link for anyone that wants to try it out is [incredicat.com](http://www.incredicat.com/)

Thanks in advance!",5,1,False,self,,,,,
518,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,13,9vrfz2,self.MachineLearning,I need some pictures of perfect toast.,https://www.reddit.com/r/MachineLearning/comments/9vrfz2/i_need_some_pictures_of_perfect_toast/,xofoxxy,1541823365,[removed],0,1,False,self,,,,,
519,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,13,9vrhr0,self.MachineLearning,[N] Interview with Soumith Chintala - Creator of PyTorch,https://www.reddit.com/r/MachineLearning/comments/9vrhr0/n_interview_with_soumith_chintala_creator_of/,crypto_ha,1541823840,"Check out this interview with Soumith Chintala, the creator of PyTorch, about the past, present, and future of the PyTorch framework: [Soumith Chintala PyTorch Interview](https://www.youtube.com/playlist?list=PL7k0r4t5c10_oDCZIz0Hs49nEOaFzFSay). Topics include:

1. Origins of PyTorch
2. Debugging and Designing PyTorch
3. From Research to Production
4. Hybrid Frontend
5. Cutting-edge Applications in PyTorch
6. User Needs and Adding Features
7. PyTorch and the Facebook Product
8. The Future of PyTorch
9. Learning More in AI

This interview is part of the contents of the PyTorch Scholarship Challenge from Facebook.",7,1,False,self,,,,,
520,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,14,9vs1tw,arxiv.org,[R] Applying Deep Learning To Airbnb Search,https://www.reddit.com/r/MachineLearning/comments/9vs1tw/r_applying_deep_learning_to_airbnb_search/,wei_jok,1541829470,,12,1,False,default,,,,,
521,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,15,9vs8ev,self.MachineLearning,Can somebody suggest some paper/articles on stability of GANs?,https://www.reddit.com/r/MachineLearning/comments/9vs8ev/can_somebody_suggest_some_paperarticles_on/,rsbor,1541831518,[removed],0,1,False,self,,,,,
522,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,15,9vsc5s,self.MachineLearning,Machine learning - Ensemble Method,https://www.reddit.com/r/MachineLearning/comments/9vsc5s/machine_learning_ensemble_method/,andrea_manero,1541832772,http://www.datasciencecentral.com/forum/topics/machine-learning-ensemble-method,0,1,False,self,,,,,
523,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,15,9vscls,self.MachineLearning,A better understanding of machine learning,https://www.reddit.com/r/MachineLearning/comments/9vscls/a_better_understanding_of_machine_learning/,flight505,1541832932,[removed],0,1,False,self,,,,,
524,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,16,9vsehw,self.MachineLearning,Getting Started with GraphLab For Machine Learning in Python,https://www.reddit.com/r/MachineLearning/comments/9vsehw/getting_started_with_graphlab_for_machine/,andrea_manero,1541833557,[removed],0,1,False,self,,,,,
525,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,16,9vsmol,medium.com,Deep learning: the final frontier for signal processing and time series analysis?,https://www.reddit.com/r/MachineLearning/comments/9vsmol/deep_learning_the_final_frontier_for_signal/,rachnogstyle,1541836317,,0,1,False,https://b.thumbs.redditmedia.com/bUkiJO_ZqT5y37Wtgibi7BxwPqQBQRJN4VbtwDPXjVs.jpg,,,,,
526,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,17,9vssfj,self.MachineLearning,Author profiling,https://www.reddit.com/r/MachineLearning/comments/9vssfj/author_profiling/,marescas,1541838383,[removed],0,1,False,self,,,,,
527,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,18,9vt0dd,biorxiv.org,Perceptron learning and classification in a modeled cortical pyramidal cell,https://www.reddit.com/r/MachineLearning/comments/9vt0dd/perceptron_learning_and_classification_in_a/,Estarabim,1541841478,,0,1,False,default,,,,,
528,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,20,9vtmnt,self.MachineLearning,Limits of OCR / Detecting layout?,https://www.reddit.com/r/MachineLearning/comments/9vtmnt/limits_of_ocr_detecting_layout/,L00KATMYPOST,1541849562,[removed],0,1,False,self,,,,,
529,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,22,9vufzl,self.MachineLearning,What will be the hottest topics in Natural Language Processing in 2019?,https://www.reddit.com/r/MachineLearning/comments/9vufzl/what_will_be_the_hottest_topics_in_natural/,longinglove,1541857934,[removed],0,1,False,self,,,,,
530,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,23,9vuotk,join.slack.com,Coursera Machine Learning course: student's Slack channel,https://www.reddit.com/r/MachineLearning/comments/9vuotk/coursera_machine_learning_course_students_slack/,speedtreammanga,1541860096,,0,1,False,default,,,,,
531,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,23,9vur2r,blog.finxter.com,15 Best Machine Learning Cheat Sheets,https://www.reddit.com/r/MachineLearning/comments/9vur2r/15_best_machine_learning_cheat_sheets/,code_x_7777,1541860632,,0,1,False,https://b.thumbs.redditmedia.com/nlnaWi_0QbtIfdr6nE38jFFu5AYzZViL0gyr1SSSgrU.jpg,,,,,
532,MachineLearning,t5_2r3gv,2018-11-10,2018,11,10,23,9vusa3,self.MachineLearning,Is there a need for this library in machine learning?,https://www.reddit.com/r/MachineLearning/comments/9vusa3/is_there_a_need_for_this_library_in_machine/,amitness,1541860933,"Most of the libraries have few toy datasets. For example, keras, sklearn have their datasets api for few standard datasets.

`from keras.datasets import mnist, fashion_mnist`

`from sklearn.datasets import load_iris`

`from seaborn import load_dataset`

&amp;#x200B;

I was thinking of building a library that wraps these and adds many more datasets. The goal is to build a defacto library with a uniform API where you can simply import datasets and use it right away with other stacks (numpy, pandas, sklearn, keras). 

`from foo import titanic`

x,y, xt, yt = titanic.load\_data()

&amp;#x200B;

Would you use such a library?",0,1,False,self,,,,,
533,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,0,9vv8ia,youtu.be,[D] George Hotz | Programming | Reading ML paper: NICE (from the cloud),https://www.reddit.com/r/MachineLearning/comments/9vv8ia/d_george_hotz_programming_reading_ml_paper_nice/,commaaiarchive,1541864547,,1,1,False,https://b.thumbs.redditmedia.com/-DxnEepbGG9I8I8xA8DTLImBfpbXUQSb4aI1HLwO82c.jpg,,,,,
534,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,2,9vvwif,ajayrfhp.github.io,Understanding what image classifiers with learn some with math and code,https://www.reddit.com/r/MachineLearning/comments/9vvwif/understanding_what_image_classifiers_with_learn/,ajayrfhp,1541869485,,0,1,False,default,,,,,
535,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,2,9vvzhr,ajayrfhp.github.io,Understanding what image classifiers learn with some code and math,https://www.reddit.com/r/MachineLearning/comments/9vvzhr/understanding_what_image_classifiers_learn_with/,ajayrfhp,1541870090,,0,1,False,https://a.thumbs.redditmedia.com/iMr_hH59ZOSqvD04gWzqBoL98S2rA5kwj0PGgkr6xY0.jpg,,,,,
536,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,3,9vwi4f,self.MachineLearning,How would you approach learning very high-dimensional output?,https://www.reddit.com/r/MachineLearning/comments/9vwi4f/how_would_you_approach_learning_very/,derhexer,1541873807,Let's say you want to regress something like 1 million parameters. I'm just looking for some ideas. What would your approach be for such a problem?,0,1,False,self,,,,,
537,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,3,9vwnbl,youtube.com,Machine Learning: Linear Regression Example With TensorFlow In Python,https://www.reddit.com/r/MachineLearning/comments/9vwnbl/machine_learning_linear_regression_example_with/,corymaklin,1541874876,,0,1,False,default,,,,,
538,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,4,9vwvyh,delivery.acm.org,Researchers find that analysis of video can enhance detection of cyberbullying.,https://www.reddit.com/r/MachineLearning/comments/9vwvyh/researchers_find_that_analysis_of_video_can/,Science_Podcast,1541876640,,1,1,False,default,,,,,
539,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,4,9vwxkp,arxiv.org,[R] Characterizing Well-behaved vs. Pathological Deep Neural Network Architectures,https://www.reddit.com/r/MachineLearning/comments/9vwxkp/r_characterizing_wellbehaved_vs_pathological_deep/,yankeesblue,1541876969,,0,1,False,default,,,,,
540,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,5,9vxq7y,frontiersin.org,[D] Deep Learning With Spiking Neurons: Opportunities and Challenges,https://www.reddit.com/r/MachineLearning/comments/9vxq7y/d_deep_learning_with_spiking_neurons/,TwoUpper,1541882852,,0,1,False,default,,,,,
541,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,5,9vxsad,youtube.com,Neural Network Part 1 - Tensorflow Tutorial,https://www.reddit.com/r/MachineLearning/comments/9vxsad/neural_network_part_1_tensorflow_tutorial/,jeffxu999,1541883297,,0,1,False,https://b.thumbs.redditmedia.com/wugixkE8pbNLEUSLseUEDn4cG6PqUEBEbVONqBCynlA.jpg,,,,,
542,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,8,9vyuld,self.MachineLearning,[P] PyTorch implementation of Adaptive Input,https://www.reddit.com/r/MachineLearning/comments/9vyuld/p_pytorch_implementation_of_adaptive_input/,HigherTopoi,1541891486,[removed],1,1,False,self,,,,,
543,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,8,9vyvjv,self.MachineLearning,VAE and transfer learning,https://www.reddit.com/r/MachineLearning/comments/9vyvjv/vae_and_transfer_learning/,AkeemTheUsurper,1541891695,[removed],0,1,False,self,,,,,
544,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,8,9vz37t,self.MachineLearning,[D] Implementation Lasso regularization for feature selection?,https://www.reddit.com/r/MachineLearning/comments/9vz37t/d_implementation_lasso_regularization_for_feature/,Fender6969,1541893376,"Currently working on a classification project and I would like to use Lasso to remove predictors that are not conducive to predicting the target variable. I would like to show to the customer coefficients that have been shrunk to zero, leading to us removing the predictor. Does anyone have examples of implementation of this in Python? While we are working couple different projects in both Python and R, most of the projects are in Python. 

Any help would be great! ",13,1,False,self,,,,,
545,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,9,9vzcnq,geotecnologias.wordpress.com,Random Forest,https://www.reddit.com/r/MachineLearning/comments/9vzcnq/random_forest/,sadeckgeo_oficial,1541895488,,0,1,False,default,,,,,
546,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,9,9vzdnq,self.MachineLearning,"zxcvbnm,",https://www.reddit.com/r/MachineLearning/comments/9vzdnq/zxcvbnm/,wertghjrfty,1541895719," 

[https://t.co/leNKPAR5Ab](https://www.google.com/url?q=https://t.co/leNKPAR5Ab&amp;sa=D&amp;source=hangouts&amp;ust=1541981210656000&amp;usg=AFQjCNE3gcqIKVfsEJ_3GZBA3l2JqWjA8Q)",0,1,False,spoiler,,,,,
547,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,9,9vzfck,self.MachineLearning,How do you develop / do ML in windows?,https://www.reddit.com/r/MachineLearning/comments/9vzfck/how_do_you_develop_do_ml_in_windows/,jairgs,1541896099,[removed],0,1,False,self,,,,,
548,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,10,9vzvkj,arxiv.org,[R] An Optimal Transport View on Generalization,https://www.reddit.com/r/MachineLearning/comments/9vzvkj/r_an_optimal_transport_view_on_generalization/,Tough_Wonder,1541899946,,0,1,False,default,,,,,
549,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,10,9vzy3s,self.MachineLearning,[R] An Optimal Transport View on Generalization,https://www.reddit.com/r/MachineLearning/comments/9vzy3s/r_an_optimal_transport_view_on_generalization/,Tough_Wonder,1541900561,[https://arxiv.org/abs/1811.03270](https://arxiv.org/abs/1811.03270),0,1,False,self,,,,,
550,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,10,9w01cb,self.MachineLearning,[R] An Optimal Transport View on Generalization,https://www.reddit.com/r/MachineLearning/comments/9w01cb/r_an_optimal_transport_view_on_generalization/,Tough_Wonder,1541901347,[removed],0,1,False,self,,,,,
551,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,11,9w049m,self.MachineLearning,[Research] An Optimal Transport View on Generalization,https://www.reddit.com/r/MachineLearning/comments/9w049m/research_an_optimal_transport_view_on/,Carl__Johnson__,1541902047,"[https://arxiv.org/abs/1811.03270](https://arxiv.org/abs/1811.03270)

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
552,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,11,9w04jk,self.MachineLearning,[R] An Optimal Transport View on Generalization,https://www.reddit.com/r/MachineLearning/comments/9w04jk/r_an_optimal_transport_view_on_generalization/,Carl__Johnson__,1541902116,"[https://arxiv.org/abs/1811.03270](https://arxiv.org/abs/1811.03270)

&amp;#x200B;

&amp;#x200B;",4,1,False,self,,,,,
553,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,12,9w0im4,self.MachineLearning,implementation of AlphaGo Zero in tensorflow,https://www.reddit.com/r/MachineLearning/comments/9w0im4/implementation_of_alphago_zero_in_tensorflow/,cody2007_2,1541905672,[removed],0,1,False,self,,,,,
554,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,12,9w0kun,self.MachineLearning,[P] Dynamic Programming in RL simplified,https://www.reddit.com/r/MachineLearning/comments/9w0kun/p_dynamic_programming_in_rl_simplified/,vector_machines,1541906245,"https://youtu.be/fAGgZrGZRsc

I recently dropped a video on DP in RL explaining
- Introduction
- Policy Evaluation
- Policy Improvement
- Policy Iteration
- Value Iteration
- Asynchronous DP

I hope you like it. ",0,1,False,self,,,,,
555,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,12,9w0l5s,self.computervision,DAVIS dataset for video Segmentation,https://www.reddit.com/r/MachineLearning/comments/9w0l5s/davis_dataset_for_video_segmentation/,muneeb2405,1541906324,,0,1,False,default,,,,,
556,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,12,9w0lb9,self.MachineLearning,[P] Implementation of AlphaGo Zero in tensorflow,https://www.reddit.com/r/MachineLearning/comments/9w0lb9/p_implementation_of_alphago_zero_in_tensorflow/,cody2007_2,1541906363,[removed],0,1,False,self,,,,,
557,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,13,9w0uv4,sebastianraschka.com,Summary of commonly used statistical hypothesis tests for model and algorithm comparisons,https://www.reddit.com/r/MachineLearning/comments/9w0uv4/summary_of_commonly_used_statistical_hypothesis/,seraschka,1541908891,,0,1,False,default,,,,,
558,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,13,9w0xv2,sebastianraschka.com,[D] Summary of commonly used statistical hypothesis tests for model and algorithm comparisons,https://www.reddit.com/r/MachineLearning/comments/9w0xv2/d_summary_of_commonly_used_statistical_hypothesis/,seraschka,1541909691,,0,1,False,default,,,,,
559,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,15,9w1mz3,self.MachineLearning,Can anybody help me establish a path of books that I should read?,https://www.reddit.com/r/MachineLearning/comments/9w1mz3/can_anybody_help_me_establish_a_path_of_books/,the_n0t0ri0us,1541916998,"Hello, I'm am trying to learn machine learning, and I'd like to have some help for books that I should read to you guessed it, learn machine learning. So here is what I'm thinking, feel free to contribute and I'll add whatever you contribute to the list in the format of [title of book hyperlinked][contributer]. The link to the book will either be the publishers/authors page or an Amazon link. If there is a certain order these should be read in let me know and I will make sure the lists become ordered. Please contribute, I only added some things I've heard of, and would like to add things from all of you! Thanks!

Learning programming:
* [Python crash course](https://nostarch.com/pythoncrashcourse) [op]
* [Learning Python](https://www.amazon.com/Learning-Python-5th-Mark-Lutz/dp/1449355730) [op]
* [Lua Quickstart guide](https://www.amazon.com/Lua-Quick-Start-Guide-programming/dp/1789343224) [op]
* [The Art of Programming R](https://nostarch.com/artofr.htm)
* {Needs contributions}

2. Learn basic maths required (Lin. Algebra, vector calculus, etc.)
* {Need contributions}

3. Learn machine learning 
* [Deep Learning](https://www.deeplearningbook.org/) [op]
* [Elements of statistical learning](https://web.stanford.edu/~hastie/ElemStatLearn/) [op]
* [An introduction to statistical learning](https://www-bcf.usc.edu/~gareth/ISL/) [op]
* [Machine Learning a Probalistic prospective](https://www.cs.ubc.ca/~murphyk/MLbook/) [op]



",0,1,False,self,,,,,
560,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,16,9w1vwa,solutionfactory.in,[Discussion] Risks of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/9w1vwa/discussion_risks_of_artificial_intelligence/,Shadabkazi03,1541919965,,0,1,False,https://b.thumbs.redditmedia.com/e5Td46GgkjpFLQtzK8DCK81-quSeOpZbS2vh9Oh-BuI.jpg,,,,,
561,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,17,9w25ok,self.MachineLearning,Best prective for teams working on remote GPU,https://www.reddit.com/r/MachineLearning/comments/9w25ok/best_prective_for_teams_working_on_remote_gpu/,shayzm1,1541923431,"Hi, I have a team of researchers that need to work with a cloud GPUs(we'll work with Azure). Should we get one strong GPU instance (maybe with multiple GPUs on the same instance) with lots of RAM and memory that will serve all the researchers? Or each researcher should have its small instance with sufficient specs for it needs? Assume each researcher works on a different project. Thanks",0,1,False,self,,,,,
562,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,19,9w2q93,self.MachineLearning,Best Machine Learning Training Institute in Delhi | Machine Learning Course in Delhi,https://www.reddit.com/r/MachineLearning/comments/9w2q93/best_machine_learning_training_institute_in_delhi/,smadrid056,1541930951,[removed],0,1,False,self,,,,,
563,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,19,9w2tfo,self.MachineLearning,Accumulated Local Effects: not understanding one step (Apley 2016),https://www.reddit.com/r/MachineLearning/comments/9w2tfo/accumulated_local_effects_not_understanding_one/,CatGoesWooof,1541932090,[removed],1,1,False,self,,,,,
564,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,19,9w2vmu,self.MachineLearning,[Python] Linear Regression. In Spanish,https://www.reddit.com/r/MachineLearning/comments/9w2vmu/python_linear_regression_in_spanish/,Errodringer,1541932868,[removed],0,1,False,self,,,,,
565,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,20,9w32kc,self.MachineLearning,From NIPS workshop it looks like top machine learning experts are only interested in Bayesian Deep Learning.,https://www.reddit.com/r/MachineLearning/comments/9w32kc/from_nips_workshop_it_looks_like_top_machine/,Mr__Christian_Grey,1541935288,[removed],0,1,False,self,,,,,
566,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,20,9w36az,self.MachineLearning,Just uploaded a new video on Candidate Algorithm,https://www.reddit.com/r/MachineLearning/comments/9w36az/just_uploaded_a_new_video_on_candidate_algorithm/,adarsh_adg,1541936543,[removed],0,1,False,self,,,,,
567,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,21,9w3b5p,self.MachineLearning,Assigning target weights in loss function does not improve the overall performance. Reasons?,https://www.reddit.com/r/MachineLearning/comments/9w3b5p/assigning_target_weights_in_loss_function_does/,tastyminerals,1541938130,[removed],0,1,False,self,,,,,
568,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,21,9w3etf,self.MachineLearning,[D] Dataset containing same object types,https://www.reddit.com/r/MachineLearning/comments/9w3etf/d_dataset_containing_same_object_types/,PavKon,1541939279,"Hi All,

has anyone came across a dataset that would include multiple of the same object types? In other words dataset of multiple objects and their parameters/features with some outcome. 

Better explained with example. Dataset containing FIFA game outcomes and stats for each football player.  \[player1Name, player1Speed, player1Accuracy .... playerNName, playerNSpeed, playerNAccuracy\] -&gt; Team1/Team2 won

or 

Guessing the value of the neighbourhood based on description of each house

\[house1Price, house1NumOfWindows, house1NumOfBathrooms ... houseNPrice, houseNWindows, houseNBathrooms\] -&gt; Value of Neighbourhood   


Does not matter if it has any extra information as long as it includes few of the same objects. Anyone seen a dataset like this?",10,1,False,self,,,,,
569,MachineLearning,t5_2r3gv,2018-11-11,2018,11,11,23,9w425s,self.MachineLearning,Labeled facial emotions dataset,https://www.reddit.com/r/MachineLearning/comments/9w425s/labeled_facial_emotions_dataset/,trusk89,1541945588,[removed],0,1,False,self,,,,,
570,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,0,9w4fie,youtube.com,PUBG MOBILE and MACHINE LEARNING ;),https://www.reddit.com/r/MachineLearning/comments/9w4fie/pubg_mobile_and_machine_learning/,aniruddh1998,1541948705,,1,1,False,https://b.thumbs.redditmedia.com/nufQya_bjhg8dzXaWIH0Iw9VGENwJq0crKEHgysbE1o.jpg,,,,,
571,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,0,9w4n9l,youtube.com,Machine Learning: Convolutional Neural Networks With TensorFlow In Python,https://www.reddit.com/r/MachineLearning/comments/9w4n9l/machine_learning_convolutional_neural_networks/,corymaklin,1541950429,,0,1,False,default,,,,,
572,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,0,9w4nnr,self.MachineLearning,Does any freelancing platforms use machine learning in their systems?,https://www.reddit.com/r/MachineLearning/comments/9w4nnr/does_any_freelancing_platforms_use_machine/,sirajaf,1541950514,,0,1,False,self,,,,,
573,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,0,9w4ofv,self.MachineLearning,"Macs, PlaidML, and eGPUs",https://www.reddit.com/r/MachineLearning/comments/9w4ofv/macs_plaidml_and_egpus/,phyziksdoc,1541950678,[removed],0,1,False,self,,,,,
574,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,0,9w4qbq,self.MachineLearning,[P] VAEs and transfer learning,https://www.reddit.com/r/MachineLearning/comments/9w4qbq/p_vaes_and_transfer_learning/,AkeemTheUsurper,1541951078,"I'm experimenting with unsupervised learning on my own dataset of images by means of generative models, particularly convolutional variational autoencoders. Since I don't own a powerful hardware I'm thinking of using transfer learning to speed up things. I know I can transfer learn the convolutional encoder, but is it possible to transfer learn the deconvolutional decoder? If so, do I have to use the same convnet (forward for the encoder and ""backwards"" for the decoder) or I can use two different nets?",7,1,False,self,,,,,
575,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,1,9w4zyj,self.MachineLearning,[D] Could DeepMind be applied to the reddit comment database?,https://www.reddit.com/r/MachineLearning/comments/9w4zyj/d_could_deepmind_be_applied_to_the_reddit_comment/,antiquark2,1541953065,"Could the [reddit database](http://files.pushshift.io/reddit/) be used as a training set for DeepMind (or some other equivalent framework)? 

Reddit threads are often composed of sequences of comments and responses, and each comment can be upvoted or downvoted. These votes could be used by the learning algorithm to determine the quality of a specific comment. 

The end result could be a pretty amazing chatbot (if it worked!)",9,1,False,self,,,,,
576,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,3,9w5xh4,self.MachineLearning,Looking for implementation of Probabilistic Binary Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9w5xh4/looking_for_implementation_of_probabilistic/,nelsonlinsanity,1541959472,[removed],0,1,False,self,,,,,
577,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,3,9w62af,biorxiv.org,"[R] ""Neural Population Control via Deep Image Synthesis"", Bashivan et al 2018 [towards human adversarial examples?]",https://www.reddit.com/r/MachineLearning/comments/9w62af/r_neural_population_control_via_deep_image/,gwern,1541960417,,0,1,False,default,,,,,
578,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,3,9w65xl,self.MachineLearning,[R]Neural network that classify images of clothing.I cant understand why it doesnt work.,https://www.reddit.com/r/MachineLearning/comments/9w65xl/rneural_network_that_classify_images_of_clothingi/,without_j,1541961146,"Im learning machine learning and im new to it.I copied a code from  [https://www.tensorflow.org/tutorials/keras/basic\_classification](https://www.tensorflow.org/tutorials/keras/basic_classification) and it doesent work.

[code that i copied](https://i.redd.it/2ig8bogdvqx11.png)

When i started the code firstly accuracy was around 0.3 than i set the epochs to 10 and accuracy was 0.374.After that whatever i did accuracy was always only 0.1.I tried to set epochs to 100 and accuracy was 0.1, then i added one more dense layer in model to make more complex network and accuracy was still 0.1.I would be realy gladful if someone could help soon because i need this for project that i need to finish by the wednesday.",3,1,False,https://b.thumbs.redditmedia.com/N8Ls4c_2kR_-C8vEZBaa_i5tvWHbHGKqLpEFHcATFDg.jpg,,,,,
579,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,3,9w6874,self.MachineLearning,Learning Machine learning,https://www.reddit.com/r/MachineLearning/comments/9w6874/learning_machine_learning/,Hatter_The_Mad,1541961598,"Hi there! I know python and C++, I dont know how to ML tho. I would like to learn it. Are there any courses or books you might suggest?",0,1,False,self,,,,,
580,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,3,9w6bq5,voicetechpodcast.com,"Voice Emotion Analytics - Florian Eyben, audEERING - Voice Tech Podcast ep.014 (x-post /r/VoiceTech)",https://www.reddit.com/r/MachineLearning/comments/9w6bq5/voice_emotion_analytics_florian_eyben_audeering/,wootnoob,1541962299,,1,1,False,https://b.thumbs.redditmedia.com/54ab8dzS_oFZssXP7TPiq1I_N1qByjToXvKX0xAU0jY.jpg,,,,,
581,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,4,9w6ipn,self.MachineLearning,Network architecture for A2C RL using PPO for MuJoCo control,https://www.reddit.com/r/MachineLearning/comments/9w6ipn/network_architecture_for_a2c_rl_using_ppo_for/,lantern_lol,1541963675,[removed],0,1,False,self,,,,,
582,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,4,9w6tbm,self.MachineLearning,[P] Which network architecture for A2C RL using PPO for MuJoCo control?,https://www.reddit.com/r/MachineLearning/comments/9w6tbm/p_which_network_architecture_for_a2c_rl_using_ppo/,lantern_lol,1541965762,"I am looking to build a reinforcement learning agent using A2C with PPO optimization to solve a MuJoCo object manipulation problem (similar to OpenAI's learning dexterity https://blog.openai.com/learning-dexterity/ but solely in simulation). I am assuming all variables about the environment are known and can be passed to the network (e.g. joint torques, object position/rotation, etc.) so a pixel-based state approximation method is not required.

I am confident with the algorithm but not so much with which architecture(s) to choose for actor/critic. The literature seems to point towards LSTM-based models but I am not sure why a network with memory would be necessary for a task where the entire state is known and the chosen action should in theory be independent of past states. I am also not sure if I should use separate networks for actor/critic or one should suffice.

Any advice on which architectures may work and why would be fantastic!",12,1,False,self,,,,,
583,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,5,9w76m4,self.MachineLearning,[R] What are the most promising theories making empirical headway in deep learning right now? Information bottleneck?,https://www.reddit.com/r/MachineLearning/comments/9w76m4/r_what_are_the_most_promising_theories_making/,rantana,1541968322,"Information theory is getting a lot of attention and we're seeing more papers coming online with inspiration from the information theory bottleneck of Tishby et. al. 

https://en.wikipedia.org/wiki/Information_bottleneck_method

Are there other examples of theoretical work that is making substantial progress to empirical results on non-toy datasets?",11,0,False,self,,,,,
584,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,6,9w7n3s,self.MachineLearning,The best budget machine learning build?,https://www.reddit.com/r/MachineLearning/comments/9w7n3s/the_best_budget_machine_learning_build/,crytoy,1541971559,[removed],0,1,False,self,,,,,
585,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,7,9w84je,self.MachineLearning,"[D] How much experience is enough for AI residencies / research internships at Google, Facebook, OpenAI etc?",https://www.reddit.com/r/MachineLearning/comments/9w84je/d_how_much_experience_is_enough_for_ai/,sslotin,1541975082,"Hi!

I've recently got sad about being rejected or ignored after applying for top deep learning labs, and I don't have a clue why. What experience you had when you were accepted / rejected for research positions?

My background: 19 yo, sophomore (CS), 2nd world country, ex-IOI-level-competitive-programmer, nonzero h-index (an NMT workshop paper), teaching experience at CS and DL courses, industry experience back home (NLP, middle).",6,1,False,self,,,,,
586,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,7,9w8b31,self.MachineLearning,How does Google's reCAPTCHA train ML models?,https://www.reddit.com/r/MachineLearning/comments/9w8b31/how_does_googles_recaptcha_train_ml_models/,brendanmartin,1541976408,[removed],0,1,False,self,,,,,
587,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,7,9w8bjk,self.MachineLearning,[P] PyCM v1.4 : Full analysis of confusion matrix,https://www.reddit.com/r/MachineLearning/comments/9w8bjk/p_pycm_v14_full_analysis_of_confusion_matrix/,sepandhaghighi,1541976509,"Full analysis of confusion matrix library in python (New Release)

&amp;#x200B;

Github repo : [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

Webpage : [http://pycm.shaghighi.ir/](http://pycm.shaghighi.ir/)

Document : [http://pycm.shaghighi.ir/](http://pycm.shaghighi.ir/)[doc/](http://www.shaghighi.ir/pycm/doc/)",14,1,False,self,,,,,
588,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,7,9w8c77,self.MachineLearning,CS230 Deep Learning Stanford class,https://www.reddit.com/r/MachineLearning/comments/9w8c77/cs230_deep_learning_stanford_class/,azraelxii,1541976652,"Hello, I was wondering if anyone knows how programming assignments work in this class. Do you need to set up a computer for it or is it done virtually? ",0,1,False,self,,,,,
589,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,8,9w8i65,youtube.com,Naive Bayes Part 1 - Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9w8i65/naive_bayes_part_1_machine_learning_tutorial/,jeffxu999,1541977907,,0,1,False,default,,,,,
590,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,10,9w9c18,self.MachineLearning,[D] Understanding Eq. 7 in 'Towards Pose Invariant Face Recognition in the Wild',https://www.reddit.com/r/MachineLearning/comments/9w9c18/d_understanding_eq_7_in_towards_pose_invariant/,notYash,1541984531,"[D] I'm having trouble understanding the 'selective attenuation factor' in Section 3.2 of [this paper.](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.pdf) 

I roughly understand what the point of Eq. 7 is, regularizing the outputs in the final layer to make the identities more distinct from each other, but I'm having trouble understanding the equation itself. I'd really appreciate if someone could help me understand this.",0,1,False,self,,,,,
591,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,10,9w9qdj,self.MachineLearning,Undergrad ML project on probability of Hypertension / Diabetes,https://www.reddit.com/r/MachineLearning/comments/9w9qdj/undergrad_ml_project_on_probability_of/,BlackSky2129,1541987845,[removed],0,1,False,self,,,,,
592,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,11,9w9wh2,datanews.app,Artificial Intelligence Technical Analysis: AIs Growth in Foreign Exchange and Cryptocurrency  DATANEWS,https://www.reddit.com/r/MachineLearning/comments/9w9wh2/artificial_intelligence_technical_analysis_ais/,35kus,1541989230,,0,1,False,https://a.thumbs.redditmedia.com/TZ_Miqn6v5kuAJ0wJhIHO2tBB7zahQ-1Am_w0OX0GK0.jpg,,,,,
593,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,12,9wab79,self.MachineLearning,"[P] Have you used deep learning for biology research? If so, contribute to the Ten Simple Rules for Deep Learning in Biology paper!",https://www.reddit.com/r/MachineLearning/comments/9wab79/p_have_you_used_deep_learning_for_biology/,BenjaminDLee,1541992603,[removed],0,1,False,self,,,,,
594,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,13,9wayz0,medium.com,[BEGINNER TUTORIAL] Build your own custom real-time object classifier,https://www.reddit.com/r/MachineLearning/comments/9wayz0/beginner_tutorial_build_your_own_custom_realtime/,affinitive2,1541998371,,1,1,False,https://b.thumbs.redditmedia.com/h6ZmXy1E-6zwugYmy5CUrwPJIxfRhEHrlD4pyNy12vA.jpg,,,,,
595,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,14,9wb41c,arxiv.org,[R] Hessian-based Analysis of Large Batch Training and Robustness to Adversaries,https://www.reddit.com/r/MachineLearning/comments/9wb41c/r_hessianbased_analysis_of_large_batch_training/,PK_thundr,1541999598,,0,1,False,default,,,,,
596,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,15,9wbhna,youtu.be,How will AI impact the future of cyber crime?,https://www.reddit.com/r/MachineLearning/comments/9wbhna/how_will_ai_impact_the_future_of_cyber_crime/,chelsea_bear,1542003149,,0,1,False,default,,,,,
597,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,15,9wbj4d,self.MachineLearning,"Anomaly detection on unlabelled data, Suggestion?",https://www.reddit.com/r/MachineLearning/comments/9wbj4d/anomaly_detection_on_unlabelled_data_suggestion/,thomashkt,1542003544,[removed],0,1,False,self,,,,,
598,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,15,9wblt5,self.MachineLearning,[News] Global Machine Vision Market,https://www.reddit.com/r/MachineLearning/comments/9wblt5/news_global_machine_vision_market/,rahulrastogi011,1542004258,[removed],0,1,False,self,,,,,
599,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,16,9wbw92,self.MachineLearning,Help on Reverse Engineering An Engineering problem?,https://www.reddit.com/r/MachineLearning/comments/9wbw92/help_on_reverse_engineering_an_engineering_problem/,saswata64900,1542007088,"I have a problem where I need to reverse engineer a regression problem . I am going to explain the exact scenario in details.

&amp;#x200B;

I have a dataset where we have data about the revenue of company for each month. So the respective columns are **Month , Amount spent in R n D , Amount Spend in Marketing , No of employees , No of Advertisement ,Campaigns ,Revenue** . Please refer to the dataset attached for clear idea.

&amp;#x200B;

My target variable is revenue. Other columns are the features which affects the revenue.

&amp;#x200B;

Now my use case is , what should be the expected value of the features if I want to achieve a revenue of (let\`s say 200000). The system should be able to say(for example) ,

""*In order to achieve revenue of $20000 , you need to spend around $35000 in R n D, $100,200 in Marketing , No of Employees should be around 700 and Number of marketing campaigns should be around 10*0 "".

&amp;#x200B;

Or in other words, we have to predict the value of each features , given the value of target variable(Revenue).

&amp;#x200B;

Any Idea on how to solve this problem? Any threads which I might find helpful for solving this problem?

&amp;#x200B;

Thanks in Advance !!

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
600,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,16,9wc1do,self.MachineLearning,[R] Differentiable Monte Carlo Ray Tracing through Edge Sampling,https://www.reddit.com/r/MachineLearning/comments/9wc1do/r_differentiable_monte_carlo_ray_tracing_through/,hardmaru,1542008570,"[Differentiable Monte Carlo Ray Tracing through Edge Sampling](https://people.csail.mit.edu/tzumao/diffrt/)

# Abstract

*Gradient-based methods are becoming increasingly important for computer graphics, machine learning, and computer vision. The ability to compute gradients is crucial to optimization, inverse problems, and deep learning. In rendering, the gradient is required with respect to variables such as camera parameters, light sources, scene geometry, or material appearance. However, computing the gradient of rendering is challenging because the rendering integral includes visibility terms that are not differentiable. Previous work on differentiable rendering has focused on approximate solutions. They often do not handle secondary effects such as shadows or global illumination, or they do not provide the gradient with respect to variables other than pixel coordinates.*

*We introduce a general-purpose differentiable ray tracer, which, to our knowledge, is the first comprehensive solution that is able to compute derivatives of scalar functions over a rendered image with respect to arbitrary scene parameters such as camera pose, scene geometry, materials, and lighting parameters. The key to our method is a novel edge sampling algorithm that directly samples the Dirac delta functions introduced by the derivatives of the discontinuous integrand. We also develop efficient importance sampling methods based on spatial hierarchies. Our method can generate gradients in times running from seconds to minutes depending on scene complexity and desired precision.*

*We interface our differentiable ray tracer with the deep learning library PyTorch and show prototype applications in inverse rendering and the generation of adversarial examples for neural networks.*

https://people.csail.mit.edu/tzumao/diffrt/
",4,1,False,self,,,,,
601,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,17,9wcccf,self.MachineLearning,Intuition about Visual Attention,https://www.reddit.com/r/MachineLearning/comments/9wcccf/intuition_about_visual_attention/,albert1905,1542011979,"Hi guys, I've been looking at Visual attention articles such as:
""residual attention network for Image classification"", and:""Attention Unet"" and some more.
I understood the algorithm in a ""dry"" way, but I couldn't understand why it actually works? why downsampling and upsampling with a lot of convolutions help me to create and attentive model.
In the language field attention is much more intuitive for me, is it just my problem? Can someone help me to ""see the light"", and to get a solid intuition?

Thanks!",0,1,False,self,,,,,
602,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,18,9wclbd,self.MachineLearning,Text Editor integration with Google Colab,https://www.reddit.com/r/MachineLearning/comments/9wclbd/text_editor_integration_with_google_colab/,edidamanish,1542014673,[removed],0,1,False,self,,,,,
603,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,18,9wcr67,self.MachineLearning,AI demonstration of prediction,https://www.reddit.com/r/MachineLearning/comments/9wcr67/ai_demonstration_of_prediction/,manmat,1542016414,[removed],0,1,False,self,,,,,
604,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,19,9wcy11,self.MachineLearning,Make AI Training Courses Your Big Priority,https://www.reddit.com/r/MachineLearning/comments/9wcy11/make_ai_training_courses_your_big_priority/,SunilAhujaa,1542018473,[removed],0,1,False,self,,,,,
605,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,19,9wd0p8,arxiv.org,Typeface Completion with Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/9wd0p8/typeface_completion_with_generative_adversarial/,yongqyu,1542019307,,0,1,False,default,,,,,
606,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,20,9wd70h,github.com,GitHub - hanxiao/bert-as-service: Mapping a variable-length sentence to a fixed-length vector using pretrained BERT model,https://www.reddit.com/r/MachineLearning/comments/9wd70h/github_hanxiaobertasservice_mapping_a/,h_xiao,1542021328,,0,1,False,default,,,,,
607,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,20,9wdc6d,self.MachineLearning,Neural Tangent Kernel: Convergence and Generalization in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9wdc6d/neural_tangent_kernel_convergence_and/,ajacot,1542022911,[removed],0,1,False,self,,,,,
608,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,20,9wddrr,self.MachineLearning,Explaination on dougman operator / integro diffrential operator.,https://www.reddit.com/r/MachineLearning/comments/9wddrr/explaination_on_dougman_operator_integro/,gireeshwaran,1542023393,[removed],0,1,False,self,,,,,
609,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,20,9wddsx,medium.com,Finding and managing research papers: a survey of tools and products,https://www.reddit.com/r/MachineLearning/comments/9wddsx/finding_and_managing_research_papers_a_survey_of/,Discordy,1542023403,,0,1,False,https://b.thumbs.redditmedia.com/E626gTrOKYFh0gQX1bPniMCd2P4lP_QAklRiAboS1NA.jpg,,,,,
610,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,20,9wdesw,medium.com,[D] A survey of tools and products for finding and managing research papers,https://www.reddit.com/r/MachineLearning/comments/9wdesw/d_a_survey_of_tools_and_products_for_finding_and/,Discordy,1542023702,,0,1,False,default,,,,,
611,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,20,9wdf10,self.MachineLearning,[R] Neural Tangent Kernel: Convergence and Generalization in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9wdf10/r_neural_tangent_kernel_convergence_and/,ajacot,1542023768," I spent some time animating a short video ([https://www.youtube.com/watch?v=raT2ECrvbag](https://www.youtube.com/watch?v=raT2ECrvbag)) about our article for NIPS 2018 ([https://arxiv.org/abs/1806.07572](https://arxiv.org/abs/1806.07572))  on the theory of neural networks. We give a full characterization of  the behaviour of (deep) fully-connected neural networks in the infinite  width limit (when the number of neurons in each hidden layers grow to  infinity) **in function space**:

\-  At initialization the parameters are initialized randomly. The function  represented by the network is therefore random too, and its  distribution tends to a Gaussian with covariance given by a kernel (you  can find lots of interesting papers about this).

\- During gradient descent on the parameters, the function follows the **kernel gradient** with respect to another kernel we introduce, the Neural Tangent Kernel.

Kernel  gradient descent is simply a generalization of gradient descent to  function spaces, it corresponds to smoothing the derivative of the cost  with a kernel to obtain a gradient. The advantage is that costs such as  least-squares or cross-entropy are convex in function space (in contrast  to the cost in parameter space), and we know that kernel gradient  descent will converge to the global minimum if the corresponding kernel  is positive definite.

This also suggests  why neural networks are able to generalize even when they are  over-parametrized: in the infinite-width there are an infinity of  parameters, but we show that in this limit neural networks behave like  kernel methods which are known to generalize well. Too many parameters  are therefore not a problem.

The article  is quite math heavy (and the proofs are longer than the article ;) ),  but I made some animated plots for the video which give some visual  intuition.",3,1,False,self,,,,,
612,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,20,9wdf6o,self.MachineLearning,How to accelerate your machine learning applications?,https://www.reddit.com/r/MachineLearning/comments/9wdf6o/how_to_accelerate_your_machine_learning/,inaccel,1542023811,[removed],0,1,False,self,,,,,
613,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,21,9wdggu,dimensionless.in,Data Science in Esports,https://www.reddit.com/r/MachineLearning/comments/9wdggu/data_science_in_esports/,divya2018,1542024189,,1,1,False,https://b.thumbs.redditmedia.com/xyXfLqeS6MrU7bwxAsOKBlpfbR2ppyJDHLp5g9WNsbo.jpg,,,,,
614,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,21,9wdj9j,venturebeat.com,"Google launches AI Hub in alpha and Kubeflow Pipelines, a machine learning workflow",https://www.reddit.com/r/MachineLearning/comments/9wdj9j/google_launches_ai_hub_in_alpha_and_kubeflow/,codiyapa,1542024924,,0,1,False,default,,,,,
615,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,21,9wdnik,self.MachineLearning,[R] L-EnsNMF: Boosted Local Topic Discovery via Ensemble of Nonnegative Matrix Factorization (ICDM 2016).,https://www.reddit.com/r/MachineLearning/comments/9wdnik/r_lensnmf_boosted_local_topic_discovery_via/,benitorosenberg,1542026086,"&amp;#x200B;

https://i.redd.it/a5jm46dn9wx11.png

[https://github.com/benedekrozemberczki/BoostedFactorization](https://github.com/benedekrozemberczki/BoostedFactorization)

**Abstract:**

*Nonnegative matrix factorization (NMF) has been increasingly  popular   for  topic  modeling  of  large-scale documents. However, the resulting  topics often represent only general, thus redundant information  about   the  data  rather  than  minor,  but  potentially  meaningful  information  to users.   To tackle this problem, we propose a novel  ensemble model of nonnegative matrix factorization for discovering  high-quality local topics. Our method leverages the idea of an ensemble  model to successively perform NMF given a residual matrix obtained from  previous stages and generates a sequence of topic sets. The novelty of  our method lies in the fact that it utilizes the residual matrix  inspired by a state-of-the-art gradient boosting model and applies a  sophisticated local weighting scheme on the given matrix to enhance the  locality of topics, which in turn delivers high-quality, focused topics  of interest to users.*

[http://dmkd.cs.vt.edu/papers/ICDM16.pdf](http://dmkd.cs.vt.edu/papers/ICDM16.pdf)",0,1,False,https://b.thumbs.redditmedia.com/OJg9ChPCUApmxZ2kDzDm6ynL3vw8k0NAOiPzfMpnTro.jpg,,,,,
616,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,21,9wdsu4,arxiv.org,A General Framework for Privacy Preserving Deep Learning,https://www.reddit.com/r/MachineLearning/comments/9wdsu4/a_general_framework_for_privacy_preserving_deep/,iamtrask,1542027543,,1,1,False,default,,,,,
617,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,22,9wdyp5,self.MachineLearning,Typeface Completion with Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/9wdyp5/typeface_completion_with_generative_adversarial/,yongqyu,1542028954,[removed],0,1,False,self,,,,,
618,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,22,9we1j9,arxiv.org,Typeface Completion with Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/9we1j9/typeface_completion_with_generative_adversarial/,yongqyu,1542029637,,0,1,False,default,,,,,
619,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,22,9we32o,arxiv.org,[1811.03804] Gradient Descent Finds Global Minima of Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9we32o/181103804_gradient_descent_finds_global_minima_of/,ihaphleas,1542029995,,93,1,False,default,,,,,
620,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,22,9we57r,self.MachineLearning,Is there a Python library to select the lag variable for a multivariate time series?,https://www.reddit.com/r/MachineLearning/comments/9we57r/is_there_a_python_library_to_select_the_lag/,EquivalentSelf,1542030482,[removed],0,1,False,self,,,,,
621,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,23,9weehb,self.MachineLearning,Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9weehb/learning_machine_learning/,Hatter_The_Mad,1542032492,[removed],0,1,False,self,,,,,
622,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,23,9wegxe,self.MachineLearning,[Discussion] Uncertainty prediction in learning approaches,https://www.reddit.com/r/MachineLearning/comments/9wegxe/discussion_uncertainty_prediction_in_learning/,pimp4robots,1542033020,I want to read about uncertainty prediction in learning based methods. I have found some resources regarding variational inference and using dropout. I am wondering if anyone can recommend a blog post or something which gives a proper summary or overview.,9,1,False,self,,,,,
623,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,23,9weh7g,self.MachineLearning,[P] 50% Faster Machine Learning - Big Data Algorithms 1st Edition Completed! :),https://www.reddit.com/r/MachineLearning/comments/9weh7g/p_50_faster_machine_learning_big_data_algorithms/,danielhanchen,1542033078,"Hey Machine Learning fans!

I've showcased before on Reddit how HyperLearn: [github.com/danielhanchen/hyperlearn](https://github.com/danielhanchen/hyperlearn) has achieved:

1. 70% less time to fit Linear Regression than Sklearn with 50% or more memory reduction. (Same or better MSE)
2. 50% less time to fit NMF / NNMF than Sklearn through new Parallelized NNMF
3. 40% less time to perform full pairwise Euclidean + Cosine Distances
4. 20% less time to perform RandomizedSVD
5. New Incremental TruncatedSVD / Eig, RandomizedSVD/ Eig and FullSVD / Eig
6. 50 - 60% faster LSMR on Sparse Matrices
7. etc etc

I've published all my code on Github, albeit the package is currently not installable via pip, and is slightly all of the place. However, most modules should work:

^(from hyperlearn.decomposition.NMF import nmf\_cd)

^(from hyperlearn.big\_data.truncated import \*)

^(from hyperlearn.solvers import \*)

Also, I completed the first edition of Modern Big Data Algorithms! With over 10,000 words and spending over 5.5 days of blood and sweat writing it, I list the methodology and clear results of over 12 NEW and updated algorithms. I plan to continue the mini book with more stuff! Below is a sneak peek: The full PDF is at [https://github.com/danielhanchen/hyperlearn/blob/master/Modern%20Big%20Data%20Algorithms.pdf](https://github.com/danielhanchen/hyperlearn/blob/master/Modern%20Big%20Data%20Algorithms.pdf)

&amp;#x200B;

Thanks to everyone who supported me! I will continue on HyperLearn + if people are interested, I will continue on the book! Thanks so much! :)

[Page on Non Negative Matrix Factorization](https://i.redd.it/hohcz4b9swx11.png)

&amp;#x200B;

[Page on Ridge Regularization](https://i.redd.it/qa7fya2fswx11.png)

&amp;#x200B;

[Page on Partial Inverse](https://i.redd.it/6me8otajswx11.png)

&amp;#x200B;

[Page on Reconstruction SVD](https://i.redd.it/wbgn6lfnswx11.png)",31,1,False,https://b.thumbs.redditmedia.com/THJzEBGkBC2Lj5ShjkbUc2Sa6azcp83hF9NT21xJkvM.jpg,,,,,
624,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,23,9wekif,youtube.com,AI Showreel 2018 - Machine Learning uses case &amp; more,https://www.reddit.com/r/MachineLearning/comments/9wekif/ai_showreel_2018_machine_learning_uses_case_more/,developFFM,1542033809,,0,1,False,https://b.thumbs.redditmedia.com/QTMoDkfdUGO8T2fEnxJ9SXZG4sBShiXB4No1xkjPCVs.jpg,,,,,
625,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,23,9wel6s,medium.com,What Errors Lurk in Infer.NET Code?,https://www.reddit.com/r/MachineLearning/comments/9wel6s/what_errors_lurk_in_infernet_code/,AlexAsics,1542033950,,0,1,False,default,,,,,
626,MachineLearning,t5_2r3gv,2018-11-12,2018,11,12,23,9wenbu,self.MachineLearning,Synthetic Data Vs Real Data,https://www.reddit.com/r/MachineLearning/comments/9wenbu/synthetic_data_vs_real_data/,Ahmad401,1542034417,[removed],0,1,False,self,,,,,
627,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,0,9weqhj,self.MachineLearning,[D] Is Google Crash Course available for download?,https://www.reddit.com/r/MachineLearning/comments/9weqhj/d_is_google_crash_course_available_for_download/,FriendlyCartoonist,1542035091,,0,1,False,self,,,,,
628,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,0,9wf6bx,self.MachineLearning,[D] Looking for a specific machine learning tutorial,https://www.reddit.com/r/MachineLearning/comments/9wf6bx/d_looking_for_a_specific_machine_learning_tutorial/,Rainymood_XI,1542038241,"I'm looking for a machine learning tutorial, here's what I know about it

* I found it on ArXiv
* I remember something about LA, UCLA, or something? (This might be wrong)
* It was a machine learning tutorial
* Two columns
* Was understandable, but semi-technical, quite long, I think 150 pages? Possibly more

I made some notes and it started with the following recipe for a machine learning problem

https://imgur.com/a/t9YBWcE

I hope someone knows which tutorial I'm talking about!",0,1,False,self,,,,,
629,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,1,9wfehh,self.MachineLearning,Are partnerships between industry and academic a common practice?,https://www.reddit.com/r/MachineLearning/comments/9wfehh/are_partnerships_between_industry_and_academic_a/,gdantiz,1542039785,"Hi there,

I am working in a startup as a ML guy, and would like to work on speech data we've been collecting. The problem is I am not very familiar with speech processing. Still I think my dataset could be very interesting for research purpose.

Hence my question: is it common practice for companies to do a partnership with a lab/university to work on an industry dataset interesting for research purpose?

Thanks.",0,1,False,self,,,,,
630,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,1,9wfkag,techtrip.co.in,Supervised Algorithm : Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9wfkag/supervised_algorithm_machine_learning/,prpigitcse,1542040871,,0,1,False,default,,,,,
631,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,1,9wfkxu,self.MachineLearning,help with photo classification,https://www.reddit.com/r/MachineLearning/comments/9wfkxu/help_with_photo_classification/,trillioner,1542040999,[removed],0,1,False,self,,,,,
632,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,2,9wfwi4,self.MachineLearning,[Q] MML Book Problems Help,https://www.reddit.com/r/MachineLearning/comments/9wfwi4/q_mml_book_problems_help/,CircuitBeast,1542043083,[removed],0,1,False,self,,,,,
633,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,2,9wg1cf,self.MachineLearning,What's the difference between a job/career and a fellowship?,https://www.reddit.com/r/MachineLearning/comments/9wg1cf/whats_the_difference_between_a_jobcareer_and_a/,Zistance,1542043950,[removed],0,1,False,self,,,,,
634,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,2,9wga2m,self.MachineLearning,Big Data and Machine Learning (Google case),https://www.reddit.com/r/MachineLearning/comments/9wga2m/big_data_and_machine_learning_google_case/,andrea_manero,1542045512,[removed],0,1,False,self,,,,,
635,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,3,9wghu5,twitter.com,[N] BigGAN pre-trained ImageNet models released w/Colab notebook for generating more samples,https://www.reddit.com/r/MachineLearning/comments/9wghu5/n_biggan_pretrained_imagenet_models_released/,gwern,1542046848,,0,1,False,default,,,,,
636,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,3,9wghyg,self.MachineLearning,[D] How to deal with attributes that can vary arbitrarily for each sample?,https://www.reddit.com/r/MachineLearning/comments/9wghyg/d_how_to_deal_with_attributes_that_can_vary/,jardeson,1542046871,"I've originally posted [this question on StackExchange](https://datascience.stackexchange.com/questions/40240/how-to-deal-with-attributes-that-can-vary-arbitrarily-for-each-sample), but I could not get any answer. Now I'm not actually interested in a direct answer, but in discussing this problem. I'm working in a project where I have this problem and I'm trying to figure the best approach out.

Let's say we are trying to classify cars into five different categories. For this, we have a lot of samples described by color, brand, model, year of manufacture and so on. For instance, imagine something like this:

## cars

|id|color|brand|model|...|year of manufacture|
|:-|:-|:-|:-|:-|:-|
|...|...|...|...|...|...|
|319|Black|Ferrari|Dino 246 GT|...|1967|
|320|Gray|Ferrari|250 GTE|...|1960|
|321|Red|Ford|Mustang|...|1969|
|322|Black|Jaguar|E-Type|...|1961|
|...|...|...|...|...|...|

Also, in this context, we have more attributes which we know that, somehow, should be relevant while classifing these cars. These attributes are in another database (or table) and they describe historical changes (the relevant ones) applied to the car. For simplicity, let's assume that each modification is associated with a code (but we also have another attributes related that may be relevant, like date of modification and estimated cost, for example).

## cars_changes

|id|car\_id|change\_code|description|modification\_date|estimated\_cost|
|:-|:-|:-|:-|:-|:-|
|...|...|...|...|...|...|
|17|319|CLR-93AA|New painting|2009-11-18|800|
|18|319|ENG-77TS|Change engine|2011-06-04|3,000|
|19|319|GAS-19BV|Add gas as fuel|2016-02-23|1,739|
|17|319|CLR-93AA|New painting|2017-09-18|1,100|
|20|321|CLR-92BD|New painting|2012-03-17|930|
|21|321|GAS-19BV|Add gas fuel|2016-05-11|1,385|
|...|...|...|...|...|...|

As you can see, observe that:

\- a car does not necessarily had changes;

\- there is not a maximum amount of changes that a car may have made;

\- a change (with same code) can occur many times in the same car;

\- consider that the possibility of different changes in this context are of high dimension categories (let's say about 2 thousands!)

Also, consider that there is a classifier based on the first table and our goal is to improve this task (of classifying) somehow using the information from the second database. Considering that the most important information for this task are present on the second table, how we can use this information in a appropriate way for this problem?",1,1,False,self,,,,,
637,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,4,9wh56a,self.MachineLearning,Article Suggestion,https://www.reddit.com/r/MachineLearning/comments/9wh56a/article_suggestion/,prpigitcse,1542050633,[removed],0,1,False,self,,,,,
638,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,4,9whb2t,self.MachineLearning,[R] Machine Learning Methods for Interactive Search Interfaces and Cognitive Models,https://www.reddit.com/r/MachineLearning/comments/9whb2t/r_machine_learning_methods_for_interactive_search/,mulppi,1542051587,"Hi all, I am just about to wrap up my doctoral dissertation, where I studied usability of interactive machine learning and inference of complex user models (cognitive models) from realistic observation data. I made a web page where I have condensed the main points of the research: [https://users.ics.aalto.fi/akangasr/dissertationdemo/](https://users.ics.aalto.fi/akangasr/dissertationdemo/) As a highlight, it contains a live demo related to the usability of interactive user modelling.

I am interested to hear any comments on this line of work, and suggestions to what kind of extensions or applications you would like to see in the future? What kind of problems have you encountered with the usability of systems that change their behavior based on an inferred model your interests? Do you think that the user models that are currently deployed in production systems are accurate and expressive enough?

Also, in case anyone is in Espoo, Finland this Friday, I will defend the dissertation on 16.11. at 12:00 in Otaniemi, Espoo (Otakaari 1, 2nd floor, hall M1). The event is open for public, so feel welcome to come listen to my lecture on the topic.",0,1,False,self,,,,,
639,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,4,9whb9f,self.MachineLearning,[R] Reinforcement Learning with A* and a Deep Heuristic,https://www.reddit.com/r/MachineLearning/comments/9whb9f/r_reinforcement_learning_with_a_and_a_deep/,skariel,1542051619,"a while ago I posted here the [code](https://github.com/imagry/aleph_star) which was ready before the paper.

Now the paper is ready see here:

[https://github.com/imagry/aleph\_star/blob/master/paper/aleph\_star.pdf](https://github.com/imagry/aleph_star/blob/master/paper/aleph_star.pdf)

 I need endorsement for Arxiv CS, can anyone help? I have multiple papers published in Physics, but it's not helping of course. My mail for Arxiv is [skariel@gmail.com](mailto:skariel@gmail.com)

&amp;#x200B;

&amp;#x200B;",4,1,False,self,,,,,
640,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,4,9whbmj,self.MachineLearning,How to make a custom object detector using YOLOv3 in python,https://www.reddit.com/r/MachineLearning/comments/9whbmj/how_to_make_a_custom_object_detector_using_yolov3/,tahaemara,1542051675,[removed],0,1,False,self,,,,,
641,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,4,9whd4s,youtube.com,DropBlock - A BETTER DROPOUT for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9whd4s/dropblock_a_better_dropout_for_neural_networks/,ajhalthor,1542051916,,0,1,False,default,,,,,
642,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,4,9whe83,self.MachineLearning,What is the world's best technology to Recognizing the Verification Code?,https://www.reddit.com/r/MachineLearning/comments/9whe83/what_is_the_worlds_best_technology_to_recognizing/,aisorry,1542052095,[removed],0,1,False,self,,,,,
643,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,5,9whtex,self.dipakkr111,Curated list of Resources for College Students and Developers,https://www.reddit.com/r/MachineLearning/comments/9whtex/curated_list_of_resources_for_college_students/,dipakkr111,1542054722,,0,1,False,default,,,,,
644,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,5,9whzqx,self.MachineLearning,[D] Fused Lasso Penalty,https://www.reddit.com/r/MachineLearning/comments/9whzqx/d_fused_lasso_penalty/,Nicholas_Snakeeyes,1542055848,"Could someone help me implement one? Thanks.

",1,0,False,self,,,,,
645,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,6,9wi42e,self.MachineLearning,Does anyone have a recommended reading/videos series/course about RNN and its variations? I need it to discuss the theoretical and application sides.,https://www.reddit.com/r/MachineLearning/comments/9wi42e/does_anyone_have_a_recommended_readingvideos/,rethinkwhatisthere,1542056578,[removed],0,1,False,self,,,,,
646,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,7,9wipgy,self.MachineLearning,Suggestions and help with project (205 features binary classification),https://www.reddit.com/r/MachineLearning/comments/9wipgy/suggestions_and_help_with_project_205_features/,sleepy3005,1542060363,[removed],0,1,False,self,,,,,
647,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,7,9wit1g,self.MachineLearning,Auto-differentiation,https://www.reddit.com/r/MachineLearning/comments/9wit1g/autodifferentiation/,adelope,1542061013,[removed],0,1,False,self,,,,,
648,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,8,9wjcxd,arxiv.org,"[P] ""The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale"", Kuznetsova et al 2018 {Google} [9.2m images, 30.1m tags, 15.4M bounding boxes on CC-BY Flickr photos]",https://www.reddit.com/r/MachineLearning/comments/9wjcxd/p_the_open_images_dataset_v4_unified_image/,gwern,1542064763,,21,1,False,default,,,,,
649,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,8,9wjhgq,self.MachineLearning,[D] Can someone suggest a dataset to train and test Yolo V2 with?,https://www.reddit.com/r/MachineLearning/comments/9wjhgq/d_can_someone_suggest_a_dataset_to_train_and_test/,deadlift_form,1542065675,"so i want to use yolo v2 to detect objects for a project of mine. as coco and pascal are already tested by yolo I want to implement it on a new dataset. Google open Images seems too complex for first time implementation and I want a simpler one. can someone suggest a dataset?

&amp;#x200B;

Also a more important question, how hard is it to customize yolo for a new dataset? As you can see I'm not very knowledgeable about this so any advice would be really appreciated! thanks  ",2,1,False,self,,,,,
650,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,8,9wjjyy,self.MachineLearning,We need -even more- people asking questions,https://www.reddit.com/r/MachineLearning/comments/9wjjyy/we_need_even_more_people_asking_questions/,fosa2,1542066177,"TLDR: Make a blog!

&amp;#x200B;

It seems like there is a bit of a culture of fear, or perhaps hesitation, in ML. This perspective is grounded in my experience as a newcomer to the field (one year of study, one year of work at a startup), and as someone not associated with a University or leading company. 

There are the usual reasons, human dynamics, politics, money, and a couple reasons that are perhaps unique to the ML field. There is a huge amount of hype and interest in ML careers at the 'bottom' of the skill chain, and a huge amount of uncertainty at the top of the skill chain. In between there are a lot of people constantly looking at how to apply things to their business/ecosystem/problem. Those people in-between are aware that the ground is always shifting underneath them and that yesterday's SOTA is tomorrow's misdirected effort. 

It makes communication among the 'tiers' a bit strained at times. I think I'm not the only one who has hesitated to ask questions, or give answers when possible, due to my sound knowledge that I am not an expert. There is a pressing feeling of the need to self-educate, both out of necessity and politeness. 

The above knowledge dynamic is offset by ML being one of the most 'accessible' of the hot new sciences, with the latest research, and often the code, freely available for everyone. There is a tremendous opportunity for people to 'become familiar' with the current state of the art and trends and techniques.

Reddit, for all that some people seem to think it's full of assholes?, is doing a great job of bringing people together.  There are a lot of useful discussions buried in comments on tangentially related posts. Every day there are great link posts to people's tutorials or explanations about basic or advanced concepts. And, as helpful as those posts are, I think we as a community need to strongly encourage people to create and share their own discovery journeys. Reading an explanation about something is incredibly helpful. Asking a question, looking for an explanation, and sharing it, is even more helpful. Both to yourself, and to anyone looking for an answer to the same question.

A good number of leading researchers have recommended people interested in ML write blogs, regardless of their skill level. It took me a while to finally do it, despite it seeming like a good idea. I have finally come to the conclusion that the value of a thousand average people asking questions and looking for answers and sharing their results is far greater than the value of one highly paid genius individual asking questions and looking for answers. 

I've finally come to appreciate a culture of sharing questions really is more important than a culture of having the answer.

I'm very much interested to hear what others think. And for anyone looking to put off something more important, here are some questions I've been looking at [https://betterlearningforlife.com/2018/11/06/making-our-way-to-cognitive-inspired-architectures/](https://betterlearningforlife.com/2018/11/06/making-our-way-to-cognitive-inspired-architectures/)

&amp;#x200B;",0,1,False,self,,,,,
651,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,9,9wjqsp,self.MachineLearning,Other ML discussion venues?,https://www.reddit.com/r/MachineLearning/comments/9wjqsp/other_ml_discussion_venues/,phobrain,1542067564,[removed],0,1,False,self,,,,,
652,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,9,9wjr7c,sinxloud.com,Advanced Machine Learning with TensorFlow on Google Cloud Platform,https://www.reddit.com/r/MachineLearning/comments/9wjr7c/advanced_machine_learning_with_tensorflow_on/,skj8,1542067658,,0,1,False,default,,,,,
653,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,9,9wju7d,youtube.com,Anomaly Detection Part 1 - Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9wju7d/anomaly_detection_part_1_machine_learning_tutorial/,jeffxu999,1542068214,,0,1,False,default,,,,,
654,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,9,9wk188,self.MachineLearning,[P] BigGAN Generators on TF Hub with Colab Demo,https://www.reddit.com/r/MachineLearning/comments/9wk188/p_biggan_generators_on_tf_hub_with_colab_demo/,chisai_mikan,1542069645,"Large Scale GAN Training for High Fidelity Natural Image Synthesis

https://arxiv.org/abs/1809.11096

TF Hub:

https://tfhub.dev/s?q=biggan

Colab Demo:

https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb
",5,1,False,self,,,,,
655,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,10,9wkkee,self.MachineLearning,Why are all NIPS papers not reviewed using OpenReview?,https://www.reddit.com/r/MachineLearning/comments/9wkkee/why_are_all_nips_papers_not_reviewed_using/,SneakyCephalopod,1542073463,[removed],0,1,False,self,,,,,
656,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,11,9wl2iy,self.MachineLearning,[D] Is Intel Nervana ever gonna release anything?,https://www.reddit.com/r/MachineLearning/comments/9wl2iy/d_is_intel_nervana_ever_gonna_release_anything/,MassivePellfish,1542077276,,10,1,False,self,,,,,
657,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,12,9wl8e9,newsletterspot.com,Apache Spark Monthly Newsletter,https://www.reddit.com/r/MachineLearning/comments/9wl8e9/apache_spark_monthly_newsletter/,versesane,1542078561,,0,1,False,default,,,,,
658,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,12,9wlg0t,self.MachineLearning,AMD vs Intel CPU for DL Machine,https://www.reddit.com/r/MachineLearning/comments/9wlg0t/amd_vs_intel_cpu_for_dl_machine/,brsystem,1542080276,[removed],0,1,False,self,,,,,
659,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,12,9wligq,self.MachineLearning,From theoretical physics to AI research,https://www.reddit.com/r/MachineLearning/comments/9wligq/from_theoretical_physics_to_ai_research/,x4kj,1542080830,[removed],0,1,False,self,,,,,
660,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,13,9wloqs,towardsdatascience.com,[D] Automating Feature Engineering for Classic ML Problems,https://www.reddit.com/r/MachineLearning/comments/9wloqs/d_automating_feature_engineering_for_classic_ml/,bweber,1542082264,,0,1,False,https://b.thumbs.redditmedia.com/oFdPiD_6lDULVbCcYUv78TaovlMo2WH05pWs7I_aDGo.jpg,,,,,
661,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,13,9wlypi,self.KerasML,[P] Keras Implementation of BicycleGAN,https://www.reddit.com/r/MachineLearning/comments/9wlypi/p_keras_implementation_of_bicyclegan/,manicman1999,1542084646,,0,1,False,https://b.thumbs.redditmedia.com/1JChL-aAV2CQuruxLmUYAofIKZvb8SDyFjVY2RTogYc.jpg,,,,,
662,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,14,9wmc92,80000hours.org,[D] ML engineering for AI safety &amp; robustness: a Google Brain engineers guide to quickly entering the field,https://www.reddit.com/r/MachineLearning/comments/9wmc92/d_ml_engineering_for_ai_safety_robustness_a/,robwiblin,1542087978,,0,1,False,default,,,,,
663,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,15,9wmkjs,inc42.com,Walmart Plans To Launch AI-Based Lab In Its Store,https://www.reddit.com/r/MachineLearning/comments/9wmkjs/walmart_plans_to_launch_aibased_lab_in_its_store/,codiyapa,1542090147,,0,1,False,default,,,,,
664,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,15,9wmmjs,self.MachineLearning,[D] Is there any highly imbalanced binary class dataset available in the public domain?,https://www.reddit.com/r/MachineLearning/comments/9wmmjs/d_is_there_any_highly_imbalanced_binary_class/,gourxb,1542090706,"Hi, 

I have got Credit Card *Fraud Detection* Challenge\[1\], but it is too large for me to handle me. So I am looking for a smaller data, if possible. But anything that highly imbalanced data-set would be really appreciated.

&amp;#x200B;

Thanks

\[1\] [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud) ",8,1,False,self,,,,,
665,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,15,9wmphr,self.MachineLearning,[D] Is there any opensourced transactional datasets for recommenders system available?,https://www.reddit.com/r/MachineLearning/comments/9wmphr/d_is_there_any_opensourced_transactional_datasets/,gourxb,1542091524,"Hi,

I am working on Recommenders System. I have got a private transactional dataset  and a public dataset ( Ali Mobile Recommendation Algorithm\[1\]). If would be great if product /item classes and features are also available. 

&amp;#x200B;

Thanks,

\[1\] [https://tianchi.aliyun.com/competition/introduction.htm?raceId=1&amp;\_lang=en\_US](https://tianchi.aliyun.com/competition/introduction.htm?raceId=1&amp;_lang=en_US) ",3,1,False,self,,,,,
666,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,16,9wmu09,arxiv.org,[R] Playing by the Book: Towards Agent-based Narrative Understanding through Role-playing and Simulation,https://www.reddit.com/r/MachineLearning/comments/9wmu09/r_playing_by_the_book_towards_agentbased/,phase_transition,1542092781,,4,1,False,default,,,,,
667,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,16,9wmus9,self.MachineLearning,[D] How to implement Backgammon player with deep Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/9wmus9/d_how_to_implement_backgammon_player_with_deep/,ranihorev,1542092992,"Hey,

I'm interested in building a Backgammon player using deep RL. Should I use Q-Learning or Policy Optimization?

Any recommendations in general would be appreciated :)",1,1,False,self,,,,,
668,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,16,9wmy95,self.MachineLearning,[N] Chinas brightest children are being recruited to develop AI killer bots,https://www.reddit.com/r/MachineLearning/comments/9wmy95/n_chinas_brightest_children_are_being_recruited/,baylearn,1542094007,"Saw this article on SCMP (reposted below, without videos or photos):

[Chinas brightest children are being recruited to develop AI killer bots](https://www.scmp.com/news/china/science/article/2172141/chinas-brightest-children-are-being-recruited-develop-ai-killer)

- Beijing Institute of Technology recruits 31 patriotic youngsters for new AI weapons development programme

- Expert in international science policy describes course as extremely powerful and troubling

A group of some of Chinas smartest students have been recruited straight from high school to begin training as the worlds youngest AI weapons scientists.

The 27 boys and four girls, all aged 18 and under, were selected for the four-year experimental programme for intelligent weapons systems at the Beijing Institute of Technology (BIT) from more than 5,000 candidates, the school said on its website.

The BIT is one of the countrys top weapons research institutes, and the launch of the new programme is evidence of the weight it places on the development of AI technology for military use.

China is in competition with the United States and other nations in the race to develop deadly AI applications  from nuclear submarines with self-learning chips to microscopic robots that can crawl into human blood vessels.

These kids are all exceptionally bright, but being bright is not enough, said a BIT professor who was involved in the screening process but asked not to be named because of the sensitivity of the subject.

We are looking for other qualities such as creative thinking, willingness to fight, a persistence when facing challenges, he said. A passion for developing new weapons is a must  and they must also be patriots.

A total of 31 students have been chosen to take part in a four-year course on intelligent weapons systems at the Beijing Institute of Technology.

Each student will be mentored by two senior weapons scientists, one from an academic background and the other from the defence industry, according to the programmes brochure.

After completing a short programme of course work in the first semester, the students will be asked to choose a speciality field, such as mechanical engineering, electronics or overall weapon design. They will then be assigned to a relevant defence laboratory where they will be able to develop their skills through hands-on experience.

One of the students is Qi Yishen from east Chinas Shandong province, who said he had had a keen interest in guns and weapons since he was a young boy and enjoyed reading books and magazines on the subject.

As well as being offered an interview for the BIT programme he was in the running for a place at Tsinghua University, one of Chinas top seats of learning, but both visits were scheduled for the same day.

When I arrived in Beijing, I loitered at the railway station for a long time. But then I went to BIT  I couldnt resist the attraction, he was quoted as saying on the institutes website.

He said his decision was also influenced by his father, who wanted him to work in the defence industry.
BIT launched the programme at the headquarters of Norinco, one of Chinas biggest defence contractors, on October 28.

We are walking a new path, doing things that nobody has done before, said student representative Cui Liyuan in an official statement.

After completing the four-year course, the students are expected to continue on to a PhD programme and become the next leaders of Chinas AI weapons programme, the institute said.

Eleonore Pauwels, a fellow in emerging cybertechnologies at the Centre for Policy Research, United Nations University in New York, said she was concerned about the launch of the BIT course.

This is the first university programme in the world designed to aggressively and strategically encourage the next generation to think, design and deploy AI for military research and use.

While the US had similar programmes, such as those run by the Defence Advanced Research Projects Agency, they operated in relative secrecy and employed only the cream of established scientists, Pauwels said.
Will Chinas new laser satellite become the Death Star for submarines?

In contrast, the BIT programme seemed more focused on training the next generation of students in weaponising AI, she said. This concept is both extremely powerful and troubling.

Students would conceive and design AI as an engine or an enabling force to weaponise self-learning, intelligent and automated systems, she said.

That knowledge could also be used alongside other new and existing technologies such as biotechnologies, quantum computing, nanotechnology and robotics, which would have drastic implications for security and military dominance, Pauwels said.

Think of robot swarms capable of delivering harmful toxins in food or biotech supply chains, she said.
With the undergraduate programme, you could envision students starting to think about how to harness the convergence of AI and genetics systems to design and deploy powerful combinations of weapons that can target, with surgical precision, specific populations, she said.

[It] may also lead to new forms of warfare, from highly sophisticated automated cyberattacks to what you could call an internet of Battle Things, where an array of robots and sensors play a role in defence, offence and in collecting intelligence.

When asked to comment on the BIT programme, Chinas foreign ministry said the country was actively engaged in the development and application of AI technology to serve its economic, social development, and scientific and technological progress.

At the same time, it said it was also very aware of the possible problems with a lethal autonomous weapon system, and promoted the exploration of preventative measures by the international community.

Indeed, AI offers a new security arsenal for China, which has its sights firmly set on technological advancement as a way to achieve its goal to become a global leader.

The fact that Chinas AI national strategy is built on a doctrine of civil-military fusion means that an AI prototype for military use could be co-opted and perverted for surveillance or harm in the civilian context, Pauwels said.

Stuart Russell, director of the Centre for Intelligent Systems at the University of California, Berkeley, described the BIT programme as a very bad idea.

Machines should never be allowed to decide to kill humans. Such weapons quickly become weapons of mass destruction. Moreover, they increase the likelihood of war, he said.

I hope all these students will begin their course by watching the movie Slaughterbots.

He was referring to a seven-minute film screened at a United Nations arms control convention in Geneva last year, which depicts a disturbing future where swarms of low-cost drones can slaughter humans like cattle with the help of artificial intelligence technology like facial recognition.

The Chinese government submitted a position paper to UN on the use of AI weapons in April.

As products of emerging high technologies, development and use of lethal autonomous weapons systems would reduce the threshold of war, and the cost of warfare on the part of the user countries. This would make it easier and more frequent for wars to break out, Beijing said, appealing for more discussions.

Until such discussions have been had, there should not be any preset premises or prejudged outcome which may impede the development of AI technology, it said.

*This article appeared in the South China Morning Post print edition as: Chinas brightest children to build killer AI robots*

Source: https://www.scmp.com/news/china/science/article/2172141/chinas-brightest-children-are-being-recruited-develop-ai-killer
",1,1,False,self,,,,,
669,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,16,9wn3a4,alibabacloud.com,An Introduction to Core Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9wn3a4/an_introduction_to_core_machine_learning/,Jen_Cl,1542095548,,0,1,False,https://b.thumbs.redditmedia.com/nTjdazsjzuaJw7LrDS8PWvz8N0ZoPSclhQJVBYhqYks.jpg,,,,,
670,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,17,9wn578,youtube.com,[P][ BerryNet ] 3 minutes tutorial of running object recognition on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/9wn578/p_berrynet_3_minutes_tutorial_of_running_object/,TimDT42,1542096125,,0,1,False,default,,,,,
671,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,18,9wnnnz,arxiv.org,[1810.04719] Fully Supervised Speaker Diarization: Google open sources model that can distinguish between different voices (code + data in comments),https://www.reddit.com/r/MachineLearning/comments/9wnnnz/181004719_fully_supervised_speaker_diarization/,swierdo,1542101845,,16,1,False,default,,,,,
672,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,18,9wnoa6,self.MachineLearning,ML for finding changes in customer financials,https://www.reddit.com/r/MachineLearning/comments/9wnoa6/ml_for_finding_changes_in_customer_financials/,Genesistag,1542102022,[removed],0,1,False,self,,,,,
673,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,19,9wnvuu,self.MachineLearning,Discrete or continuous descriptors?,https://www.reddit.com/r/MachineLearning/comments/9wnvuu/discrete_or_continuous_descriptors/,Ibolya92,1542104269,[removed],0,1,False,self,,,,,
674,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,19,9wnw7y,self.MachineLearning,How to process inputs of different shapes?,https://www.reddit.com/r/MachineLearning/comments/9wnw7y/how_to_process_inputs_of_different_shapes/,UpstairsCurrency,1542104381,[removed],0,1,False,self,,,,,
675,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,20,9wo5r7,self.MachineLearning,Good papers for a beginner to implement,https://www.reddit.com/r/MachineLearning/comments/9wo5r7/good_papers_for_a_beginner_to_implement/,dmurthy,1542107292,[removed],0,1,False,self,,,,,
676,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,20,9wo7xy,self.MachineLearning,Fast offline text language detection with Python,https://www.reddit.com/r/MachineLearning/comments/9wo7xy/fast_offline_text_language_detection_with_python/,krashennikov,1542107957,[removed],0,1,False,self,,,,,
677,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,20,9woay1,self.MachineLearning,[P] [D] Simple offline language detection with a python script,https://www.reddit.com/r/MachineLearning/comments/9woay1/p_d_simple_offline_language_detection_with_a/,krashennikov,1542108825,"Hi,

I want to be able to detect languages (just a handful) of texts (usually Wikipedia articles) offline using a simple python script.

My approach was to just create some lexicon txt files with common words of those languages and compare the words of the text with the generated lexicons. It should be as simple as possible and fast to realize.

For efficiency, I thought to extend it using naive Bayes style with a counter for the most common language-specific words during the pre-crawling of texts. This would allow faster comparison.  After I have some confidence, I print out the language.

What would be your simplest approach on this task? Or maybe some known offline libraries/ repos which are ready to go?",12,1,False,self,,,,,
678,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,21,9woivn,arxiv.org,[1811.04624] Importance Weighted Evolution Strategies,https://www.reddit.com/r/MachineLearning/comments/9woivn/181104624_importance_weighted_evolution_strategies/,xavigiro,1542111007,,2,1,False,default,,,,,
679,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,21,9woscm,thefreenewsman.com,"Car Engine Lubricant Market 2018-Industry Size, Share, Dynamics, Status, Outlook and Opportunities 2025  The Newsman",https://www.reddit.com/r/MachineLearning/comments/9woscm/car_engine_lubricant_market_2018industry_size/,mayurpande6990,1542113485,,0,1,False,default,,,,,
680,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,21,9wotn7,towardsdatascience.com,[D] Best Deals in Deep Learning Cloud Providers  Towards Data Science,https://www.reddit.com/r/MachineLearning/comments/9wotn7/d_best_deals_in_deep_learning_cloud_providers/,_mortified_penguin,1542113812,,0,1,False,default,,,,,
681,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,22,9wovgy,thechronicleindia.com,"Global Aircraft Tires Market Forecast (2018-2025) Report: By Regions, Type and Application with Sales and Revenue Analysis  Chronicle India",https://www.reddit.com/r/MachineLearning/comments/9wovgy/global_aircraft_tires_market_forecast_20182025/,mayurpande6990,1542114249,,0,1,False,default,,,,,
682,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,22,9wow9m,theaispace.com,Hey! Want to Learn AI and Machine Learning? Here You Find Everything.,https://www.reddit.com/r/MachineLearning/comments/9wow9m/hey_want_to_learn_ai_and_machine_learning_here/,theaispace,1542114442,,0,1,False,default,,,,,
683,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,22,9woxn9,javarevisited.blogspot.com,8 Python Libraries for Data Science and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9woxn9/8_python_libraries_for_data_science_and_machine/,javinpaul,1542114753,,0,1,False,default,,,,,
684,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,22,9wp2bf,medium.com,Linear Regression: How to overcome underfitting with Locally Weight Linear Regression (LWLR),https://www.reddit.com/r/MachineLearning/comments/9wp2bf/linear_regression_how_to_overcome_underfitting/,Fewthp,1542115859,,0,1,False,default,,,,,
685,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,22,9wp3tt,self.MachineLearning,About the distribution of features in deep neural networks,https://www.reddit.com/r/MachineLearning/comments/9wp3tt/about_the_distribution_of_features_in_deep_neural/,wondervictor,1542116195,[removed],0,1,False,self,,,,,
686,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,22,9wp4do,youtu.be,Free online NLP tool for text summarization and named-entity recognition,https://www.reddit.com/r/MachineLearning/comments/9wp4do/free_online_nlp_tool_for_text_summarization_and/,developFFM,1542116318,,0,1,False,https://b.thumbs.redditmedia.com/g3oEZSQ3A97qWNMnXMG_pl_IWOelE9xGhW_EcTRuAIE.jpg,,,,,
687,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,22,9wp8g9,self.MachineLearning,RMSprop difference rho and decay in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/9wp8g9/rmsprop_difference_rho_and_decay_in_tensorflow/,MrSh4nnon,1542117251,[removed],0,1,False,self,,,,,
688,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,23,9wpe1q,self.MachineLearning,[D] State of art in text generation?,https://www.reddit.com/r/MachineLearning/comments/9wpe1q/d_state_of_art_in_text_generation/,tensor_x,1542118425,,11,1,False,self,,,,,
689,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,23,9wpg9d,self.MachineLearning,[D] Best Deals in Deep Learning Cloud Providers,https://www.reddit.com/r/MachineLearning/comments/9wpg9d/d_best_deals_in_deep_learning_cloud_providers/,_mortified_penguin,1542118899,[removed],0,1,False,self,,,,,
690,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,23,9wpkrq,self.MachineLearning,[D] How can i learn the structure from an image of a handwriting sample?,https://www.reddit.com/r/MachineLearning/comments/9wpkrq/d_how_can_i_learn_the_structure_from_an_image_of/,jhondipto,1542119854," On IAM On-Line Handwriting Database there are strokes to every letter along with an image of that letter. Now i want to train a model that can take an image of a letter and generate those strokes. How/Where do i start? What needs to be learnt to create that model. Is there any existing model that can do the same/similar thing?

Link of the dataset: [http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database)

Sample:

Handwriting Image: [https://i.imgur.com/rBNWRvv.jpg](https://i.imgur.com/rBNWRvv.jpg)

Strokes(XML File): [https://i.imgur.com/jApVUUR.jpg](https://i.imgur.com/jApVUUR.jpg)",2,1,False,self,,,,,
691,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,23,9wpm0p,self.MachineLearning,BEST way to put ML models into production!,https://www.reddit.com/r/MachineLearning/comments/9wpm0p/best_way_to_put_ml_models_into_production/,brianschardt,1542120132,[removed],0,1,False,self,,,,,
692,MachineLearning,t5_2r3gv,2018-11-13,2018,11,13,23,9wppbj,self.MachineLearning,MCMC: A break from neural networks and gradient descent,https://www.reddit.com/r/MachineLearning/comments/9wppbj/mcmc_a_break_from_neural_networks_and_gradient/,kremaytuz,1542120817,[removed],0,1,False,self,,,,,
693,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,0,9wpspo,medium.com,MCMC: A break from neural networks and gradient descent... My own work!,https://www.reddit.com/r/MachineLearning/comments/9wpspo/mcmc_a_break_from_neural_networks_and_gradient/,kremaytuz,1542121493,,0,1,False,default,,,,,
694,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,0,9wq0kd,self.MachineLearning,Equivalence of Bernoulli and Binary Categorical for Policy Gradient?,https://www.reddit.com/r/MachineLearning/comments/9wq0kd/equivalence_of_bernoulli_and_binary_categorical/,penguinshin,1542123049,[removed],0,1,False,self,,,,,
695,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,0,9wq7up,self.MachineLearning,Adam combined with dropout,https://www.reddit.com/r/MachineLearning/comments/9wq7up/adam_combined_with_dropout/,toavepa,1542124451,[removed],0,1,False,self,,,,,
696,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,1,9wqa6t,towardsdatascience.com,[P] See Robot Play: an exploration of curiosity in humans and machines. [with code],https://www.reddit.com/r/MachineLearning/comments/9wqa6t/p_see_robot_play_an_exploration_of_curiosity_in/,dantehorrorshow,1542124906,,0,1,False,default,,,,,
697,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,1,9wqewb,self.MachineLearning,Simple model outperforms complex one,https://www.reddit.com/r/MachineLearning/comments/9wqewb/simple_model_outperforms_complex_one/,Janderhungrige,1542125758,"Hi, the question is more general (sorry upfront for the lack of background info and plots).

&amp;#x200B;

I have a relatively small dataset (5000 30s samples) for sleep classification. I run a ""simple"" 4 layer bidirectional GRU on it and a deeper residual model with 12 GRU layers connected with skip layers. 

The simpler model outperforms the deeper one. I would explain it with an overtraining of the more complex one. As the problem has not enough data, the simpler model concentrates more on the essentials rather than looking into complex connections. So basically the complex one overfits faster, needs more regulation and thereby underperforms.

Would you agree with this? 

&amp;#x200B;

Thanks for your thoughts

&amp;#x200B;

p.s. before you start, I cannot get more data :-). Quite a specific problem we are trying to solve. 

 

&amp;#x200B;",0,1,False,self,,,,,
698,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,1,9wqjj5,self.MachineLearning,Need advice on input dimensionality of NN data for a chess evaluation model,https://www.reddit.com/r/MachineLearning/comments/9wqjj5/need_advice_on_input_dimensionality_of_nn_data/,Thybert,1542126619,[removed],0,1,False,self,,,,,
699,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,1,9wqlb8,self.MachineLearning,"Is there any peer 2 peer distributed computing software like uTorrent, and if not why?",https://www.reddit.com/r/MachineLearning/comments/9wqlb8/is_there_any_peer_2_peer_distributed_computing/,tunestar2018,1542126945,"Just thought of this yesterday, like when you are logged in you would be sharing say 30% of your cpu/gpu power. This way everyone benefits from having thousands of cpu/gpus to work on their project like Google does.",0,1,False,self,,,,,
700,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,1,9wqpjb,self.MachineLearning,[D] What happened with Adversarial Logit Pairing (ALP) for NIPS'18?,https://www.reddit.com/r/MachineLearning/comments/9wqpjb/d_what_happened_with_adversarial_logit_pairing/,residual_potato,1542127742,"Background: ALP ([https://arxiv.org/abs/1803.06373](https://arxiv.org/abs/1803.06373)) appeared online earlier this year claiming state-of-the-art adversarial robustness for high epsilon values on ImageNet. A publicly available model was later evaluated independently and found to not be as robust as claimed ([https://arxiv.org/abs/1807.10272](https://arxiv.org/abs/1807.10272)).

According to the NIPS website ([https://nips.cc](https://nips.cc)) ALP was accepted to NIPS'18. Currently, I cannot find the paper in the list of accepted papers anymore. Does anyone know what happened?

&amp;#x200B;",8,1,False,self,,,,,
701,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,1,9wqrqx,wired.com,[N] How to Teach Artificial Intelligence Some Common Sense [WIRED],https://www.reddit.com/r/MachineLearning/comments/9wqrqx/n_how_to_teach_artificial_intelligence_some/,egrefen,1542128144,,0,1,False,default,,,,,
702,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,2,9wr3qi,self.MachineLearning,Robustness to Data Augmentation in One-Shot Learning,https://www.reddit.com/r/MachineLearning/comments/9wr3qi/robustness_to_data_augmentation_in_oneshot/,ProximalPolicyAgent,1542130249,"Hi ML community! So, I'm working on my Master thesis on the domain of Zero/One-Shot Learning. My advisor asked me to explore one-shot learning; specifically with respect to robustness to data augmentation or changes in image properties (overexposed, faded, etc). I could not find any solid literature referencing this or talking about this. I found a paper on feature augmentation in semantic space (https://arxiv.org/abs/1804.05298) for OSL but it's not directly related to robustness.

Any pointers or headway? Thanks!",0,1,False,self,,,,,
703,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,2,9wr7ep,self.MachineLearning,[D] Robustness to Data Augmentation/Changes in Image Properties in One-Shot Learning,https://www.reddit.com/r/MachineLearning/comments/9wr7ep/d_robustness_to_data_augmentationchanges_in_image/,ProximalPolicyAgent,1542130913,"Hi ML community! So, I'm working on my Master thesis on the domain of Zero/One-Shot Learning. My advisor asked me to explore one-shot learning; specifically with respect to robustness to data augmentation or changes in image properties (overexposed, faded, etc). I could not find any solid literature referencing this or talking about this. I found a paper on feature augmentation in semantic space (https://arxiv.org/abs/1804.05298) for OSL but it's not directly related to robustness.

Any pointers or headway? Thanks!",0,1,False,self,,,,,
704,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,2,9wracg,aisoma.de,Artificial Intelligence in Healthcare - Promising Progress (Best Use Cases),https://www.reddit.com/r/MachineLearning/comments/9wracg/artificial_intelligence_in_healthcare_promising/,developFFM,1542131414,,0,1,False,default,,,,,
705,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,3,9wrdoa,aisoma.de,[D] Artificial Intelligence in Healthcare - Promising Progress (Best Use Cases),https://www.reddit.com/r/MachineLearning/comments/9wrdoa/d_artificial_intelligence_in_healthcare_promising/,developFFM,1542132014,,0,1,False,https://a.thumbs.redditmedia.com/kylfk6ETDNhDWd0x0rmARakKVHj_1505-p-7C1-yn08.jpg,,,,,
706,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,3,9wrl5q,self.MachineLearning,[D] Artificial Intelligence in Healthcare - Promising Progress (Best Use Cases),https://www.reddit.com/r/MachineLearning/comments/9wrl5q/d_artificial_intelligence_in_healthcare_promising/,developFFM,1542133314," AI in Healthcare is a promising, still-emerging concept, largely focused on programs that perform and assist with diagnosis, decision-making, therapy recommendations and healthcare management.  The following use cases in the article is not yet comprehensive but it can still give you insights about the activities and use cases. It is ever improving. 

[https://www.aisoma.de/artificial-intelligence-in-healthcare-promising-progress-best-use-cases/](https://www.aisoma.de/artificial-intelligence-in-healthcare-promising-progress-best-use-cases/)",1,1,False,self,,,,,
707,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,3,9wrog7,self.MachineLearning,[R] ICLR2019 Open Review Explorer,https://www.reddit.com/r/MachineLearning/comments/9wrog7/r_iclr2019_open_review_explorer/,downtownslim,1542133908,"ICLR2019 Open Review Explorer

https://chillee.github.io/OpenReviewExplorer/index.html",6,1,False,self,,,,,
708,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,3,9wrqbr,self.MachineLearning,Are later model checkpoints always better?,https://www.reddit.com/r/MachineLearning/comments/9wrqbr/are_later_model_checkpoints_always_better/,echan00,1542134248,[removed],0,1,False,self,,,,,
709,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,3,9wrwpx,self.MachineLearning,"[P] Neural Network Embeddings, a non NLP use case and explanation",https://www.reddit.com/r/MachineLearning/comments/9wrwpx/p_neural_network_embeddings_a_non_nlp_use_case/,cptAwesome_070,1542135372,"I have written an article about the use of Neural Network embedding layers in the context of a business case / project ( recommender engine) as I feel this is a really powerful feature of neural networks. I would like to start a discussion on this topic and this is definitely the place. 

I am also interested in other use cases, having spoken to a number of experts their only real experiences have been for NLP.

The article can be found here: [https://medium.com/heycar/neural-network-embeddings-from-inception-to-simple-35e36cb0c173](https://medium.com/heycar/neural-network-embeddings-from-inception-to-simple-35e36cb0c173)

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",1,1,False,self,,,,,
710,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,3,9wrxcc,self.MachineLearning,Code &amp; Videos on Machine Learning - Please provide feedback to improve,https://www.reddit.com/r/MachineLearning/comments/9wrxcc/code_videos_on_machine_learning_please_provide/,awantik,1542135484,[removed],0,1,False,https://b.thumbs.redditmedia.com/VUBV5SHavLTh7qtk2mjEMqEnKLZuNnYYHT6kTZdaImw.jpg,,,,,
711,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,4,9wsgaa,github.com,[P] eo-learn - an earth observation processing framework on github for machine learning in Python,https://www.reddit.com/r/MachineLearning/comments/9wsgaa/p_eolearn_an_earth_observation_processing/,developFFM,1542138824,,0,1,False,default,,,,,
712,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,5,9wslfb,kount.com,How Kount Uses MLflow to Manage Machine Learning Models,https://www.reddit.com/r/MachineLearning/comments/9wslfb/how_kount_uses_mlflow_to_manage_machine_learning/,ro22ss,1542139741,,0,1,False,default,,,,,
713,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,5,9wslw0,self.MachineLearning,A huge problem with a game bot,https://www.reddit.com/r/MachineLearning/comments/9wslw0/a_huge_problem_with_a_game_bot/,theboysxx,1542139819,[removed],0,1,False,self,,,,,
714,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,5,9wsrad,self.MachineLearning,[D] Variational Auto-encoder inference,https://www.reddit.com/r/MachineLearning/comments/9wsrad/d_variational_autoencoder_inference/,inactiveUserTBD,1542140792,"I am trying to understand VAEs and how the training and testing happens in these networks.

**Training:** The issue with the training is that we cannot differentiate a sampling function which uses the mean and variance of the latent space representation. So instead we use the reparameterization trick to move the sampling function out of the AE network and independent of the input data. So, now we can easily use backpropagation to train the AE network.

**Loss:** Now, we get to the loss (only considering the reconstruction loss): we use an expectation over the entire batch to compute the loss and thus the gradients. My understanding for using expectation (over all samples in the batch): Expectation is used because of the sampling process in the latent space and the expectation of variable $\\epsilon$ will be 0. Am I correct?

**Testing:** If I am correct about the loss section. Then I am not able to understand how we will test for a single image. I feed a trained VAE an image $x$. It will compute a mean and variance in latent space for this image. We use a stochastic function to generate the input to the decoder and thus the output is stochastic (varies between some bounds of the actual output). The output of the decoder could be anything because the VAE function for testing is not deterministic.

Can you help me understand how the testing of VAEs work for a single image?

Thanks.

**TL;DR** I think VAEs are stochastic networks. How will I test them for accuracy?",8,1,False,self,,,,,
715,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,6,9wt2u1,self.MachineLearning,Sigmoid/Softmax having a high value.,https://www.reddit.com/r/MachineLearning/comments/9wt2u1/sigmoidsoftmax_having_a_high_value/,Jandevries101,1542142878,"Hi Reader,

&amp;#x200B;

so in my (RL) Algorithme i am using the Sigmoid (also tried softmax), but when training i quickly noticed something ""strange"" in my sigmoid returns:

&amp;#x200B;

    0.9995

&amp;#x200B;

for output 1, output 1 having reward of 0, at the end of the episode more, but this action gives by default just 0 reward, whiles other other actions may be more likely to be chosen

&amp;#x200B;

the problem i am having here is that first of all that value is so darn high, i can't even consider it learned that. second issue this occurs after ca 5-10 steps only into the training sessions, with LR of only 0,0001!

&amp;#x200B;

I know you might wanna see code or something, but i'd rather know (also for others in the future) what are causes to this to occur? my state is nothing special just 7 values and i have 3 actions to choose from (the values returned by the sigmoid are very low of course since action 1 is 0.9995).

&amp;#x200B;

i am really confused by what caused this and i do understand that reward higher/lowers the value returned by sigmoid, but this goes from 0.x (random) to 0.9995 in no time, only having a reward of 0, is there something with a reward of zero that i missed out on? 

&amp;#x200B;

Let me know what you think and if you know what may caused it, please let me know i am intrested to know what it is?

&amp;#x200B;

thanks for reading,

&amp;#x200B;

Jan

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
716,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,6,9wt9qq,self.MachineLearning,[P] eo-learn - an earth observation processing framework on github for machine learning in Python,https://www.reddit.com/r/MachineLearning/comments/9wt9qq/p_eolearn_an_earth_observation_processing/,developFFM,1542144115,"An open source Python framework that has been developed to seamlessly access and process spatio-temporal image sequences acquired by any satellite fleet in a timely and automatic manner:

[https://github.com/sentinel-hub/eo-learn](https://github.com/sentinel-hub/eo-learn)",0,1,False,self,,,,,
717,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,7,9wtmbe,self.MachineLearning,Workstation Build for Research,https://www.reddit.com/r/MachineLearning/comments/9wtmbe/workstation_build_for_research/,chadchadison,1542146413,[removed],0,1,False,self,,,,,
718,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,7,9wtpfy,self.MachineLearning,Few Machine Learning Problems (with Python implementation),https://www.reddit.com/r/MachineLearning/comments/9wtpfy/few_machine_learning_problems_with_python/,andrea_manero,1542146967,https://www.datasciencecentral.com/profiles/blogs/few-machine-learning-problems-with-python-implementations-contd,0,1,False,self,,,,,
719,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,7,9wtu1x,self.MachineLearning,[D] Feedback on AI YouTube Channel!,https://www.reddit.com/r/MachineLearning/comments/9wtu1x/d_feedback_on_ai_youtube_channel/,everydAI,1542147836,"(Mods - Feel free to take this down if it's against the rules)

Hello Everyone! I'm looking to get feedback on a YouTube channel I started a couple months ago on AI for the general public (assumes little to no technical background) to help people understand how AI has become integrated into our lives and have discussions on where/when people find it useful. I've tried to touch on topics ranging for social media to medicine to newer/less visible applications designed by individual developers. 

Feel free to roast any and all of my videos! I'm currently filming on my phone (can't afford a nice camera but I'm saving up!), so video quality and sound are already on my to-do list, but any other feedback is welcome. 

https://www.youtube.com/channel/UC1H1NWNTG2Xi3pt85ykVSHA?",10,1,False,self,,,,,
720,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,7,9wu0t9,self.MachineLearning,"[R] Hunt for research papers regarding ""do xxx with less [memory, computation, xxx]""",https://www.reddit.com/r/MachineLearning/comments/9wu0t9/r_hunt_for_research_papers_regarding_do_xxx_with/,PythonThinker,1542149132,"Hey, long time lurker, first time poster. Im on the hunt for research papers regarding the above topics. 

I dont really know what keywords to search for as i usually get blogs/tutorials etc. I want help in finding papers where for eg: Image classification is done using Deep CNNs with less images or more generalised training, or less memory/computation.

Please suggest keywords or papers that i can use as a guide. 

Thank you very much. :)",9,1,False,self,,,,,
721,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,8,9wu8xn,self.MachineLearning,Cross Lingual Embedding Tasks,https://www.reddit.com/r/MachineLearning/comments/9wu8xn/cross_lingual_embedding_tasks/,spacevstab,1542150721,[removed],0,1,False,self,,,,,
722,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,8,9wubim,self.MachineLearning,Bidirectional NN - isn't this approach prone to snooping/overfitting/hi-variance?,https://www.reddit.com/r/MachineLearning/comments/9wubim/bidirectional_nn_isnt_this_approach_prone_to/,maimedforbrowngod,1542151224,[removed],5,1,False,self,,,,,
723,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,8,9wuh00,github.com,TensorSpace: a neural network 3D visualization framework,https://www.reddit.com/r/MachineLearning/comments/9wuh00/tensorspace_a_neural_network_3d_visualization/,Tarqon,1542152304,,0,1,False,default,,,,,
724,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,8,9wuhsq,youtube.com,AdaBoost Part 1- Machine Learning Tutorial,https://www.reddit.com/r/MachineLearning/comments/9wuhsq/adaboost_part_1_machine_learning_tutorial/,jeffxu999,1542152464,,0,1,False,https://b.thumbs.redditmedia.com/vDNLoaCdPbDAVCg5l1HvFetW1eSr-b68HJ8VPtxyAjY.jpg,,,,,
725,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,9,9wuo0z,self.MachineLearning,[N]TensorSpace: a neural network 3D visualization framework,https://www.reddit.com/r/MachineLearning/comments/9wuo0z/ntensorspace_a_neural_network_3d_visualization/,Tarqon,1542153758,"https://github.com/tensorspace-team/tensorspace

I've seen a few 3d visualizers for neural networks before, but this one seems better at displaying large networks while keeping things comprehensible. It also supports pre-trained models from Tensorflow and Keras.",14,1,False,self,,,,,
726,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,10,9wvg55,self.MachineLearning,"Is there a ML algorithm that takes in time vs y1 to yn, outputs a rate at each time, and then does a time integration of that rate column to output the final value vs experiment?",https://www.reddit.com/r/MachineLearning/comments/9wvg55/is_there_a_ml_algorithm_that_takes_in_time_vs_y1/,chaosbutters,1542159566,[removed],0,1,False,self,,,,,
727,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,11,9wvsw3,self.MachineLearning,[D] Where do I find AWS resources/grant for startup?,https://www.reddit.com/r/MachineLearning/comments/9wvsw3/d_where_do_i_find_aws_resourcesgrant_for_startup/,dcn20002,1542162213,"I am currently working on a healthcare startup using AWS as our dev platform. We have been trying to find an AWS sponsored incubator program or any resources that would help us with the AWS cost as well as expertise in the first year but couldn't find one except the [https://aws.amazon.com/activate/](https://aws.amazon.com/activate/) program which requires us to be in an incubator. 

&amp;#x200B;

I wonder if any of you know of such program? ",7,1,False,self,,,,,
728,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,11,9wvt6n,groundai.com,Generative Dual Adversarial Network for Generalized Zero-shot Learning,https://www.reddit.com/r/MachineLearning/comments/9wvt6n/generative_dual_adversarial_network_for/,ML_AI_DL,1542162271,,0,1,False,default,,,,,
729,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,11,9ww09n,gengo.ai,AI/Machine Learning Conferences in 2019,https://www.reddit.com/r/MachineLearning/comments/9ww09n/aimachine_learning_conferences_in_2019/,alnguyen22,1542163752,,0,1,False,default,,,,,
730,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,13,9wwsfk,gengo.ai,Upcoming AI &amp; Machine Learning Conferences,https://www.reddit.com/r/MachineLearning/comments/9wwsfk/upcoming_ai_machine_learning_conferences/,reimmoriks,1542170102,,0,1,False,https://b.thumbs.redditmedia.com/V-LxkgSFqdDvRjnelnMrJg8xeOT7Bu3k1orxl8l_7LA.jpg,,,,,
731,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,13,9www58,self.MachineLearning,Cognitive Computing and Big Data Online Learning Resources?,https://www.reddit.com/r/MachineLearning/comments/9www58/cognitive_computing_and_big_data_online_learning/,popopo58,1542170993,[removed],0,1,False,self,,,,,
732,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,13,9wwy4x,gengo.ai,[N] 2019 AI/Machine Learning Conferences,https://www.reddit.com/r/MachineLearning/comments/9wwy4x/n_2019_aimachine_learning_conferences/,alnguyen22,1542171492,,0,1,False,https://b.thumbs.redditmedia.com/V-LxkgSFqdDvRjnelnMrJg8xeOT7Bu3k1orxl8l_7LA.jpg,,,,,
733,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,14,9wx0ry,newswire.ca,"[N] PennyLane, the First Dedicated Machine Learning Software for Quantum Computers",https://www.reddit.com/r/MachineLearning/comments/9wx0ry/n_pennylane_the_first_dedicated_machine_learning/,ppd2,1542172116,,0,1,False,default,,,,,
734,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,14,9wx4lf,self.MachineLearning,[N] Kaggle style competition on drilling core with $20k,https://www.reddit.com/r/MachineLearning/comments/9wx4lf/n_kaggle_style_competition_on_drilling_core_with/,huabamane,1542172999,"Might be an interesting competition for some here. Doesn't seem like there are any submissions made yet.

[https://unearthed.solutions/u/challenge/identify-depth-measurements-core-images](https://unearthed.solutions/u/challenge/identify-depth-measurements-core-images)",6,1,False,self,,,,,
735,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,14,9wxale,self.MachineLearning,Machine Learning and Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/9wxale/machine_learning_and_artificial_intelligence/,Learntek12,1542174457,"[Difference between Machine Learning and Artificial Intelligence](https://www.learntek.org/blog/machine-learning-and-artificial-intelligence/) **:** **Okay Google**! Whats Up? Could you play my favourite track or Book a Cab from Palace Road to MG Road.

**Alexa**, What time it is? Wake me up at 5AM. Could you please tell my tomorrow meetings.

These are the perfect combination of **Machine Learning and Artificial Intelligence**.

1. **Machine learning:**

Machine learning is a method of data analysis that automates analytical model building. Machine learning is a field that uses algorithms to learn from data and make predictions. Machine learning is an application of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. **Machine learning focuses on the development of computer programs** that can access data and use it learn for themselves.

Practically, this means that we can feed data into an algorithm, and use it to make predictions about what might happen in the future. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.

2. **Artificial Intelligence:**

Artificial Intelligence (AI) is the study and design of Intelligent agent, These intelligent agents have the ability to analyse the environments and produce actions which maximize success. AI research uses tools and insights from many fields, including computer science, psychology, philosophy, neuroscience, cognitive science, linguistics, operations research, economics, control theory, probability, optimization and logic. Artificial Intelligence is based on the study that how human thinks, learn, decide and work in order to resolve an issue and then using the outcome of this study as a basis of developing intelligent software and systems.

 ",0,1,False,self,,,,,
736,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,14,9wxcw7,arxiv.org,[R] ImageNet/ResNet-50 Training in 224 Seconds,https://www.reddit.com/r/MachineLearning/comments/9wxcw7/r_imagenetresnet50_training_in_224_seconds/,downtownslim,1542175032,,24,1,False,default,,,,,
737,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,14,9wxcxf,self.MachineLearning,Is there any research/project that forecasts emerging (tech) field?,https://www.reddit.com/r/MachineLearning/comments/9wxcxf/is_there_any_researchproject_that_forecasts/,JeffreyChl,1542175039,[removed],0,1,False,self,,,,,
738,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,16,9wxvll,self.MachineLearning,"Click Here to Kill Everybody: Security, Privacy, Social Media and Politics | Bruce Schneier",https://www.reddit.com/r/MachineLearning/comments/9wxvll/click_here_to_kill_everybody_security_privacy/,The_Syndicate_VC,1542179922,[removed],0,1,False,self,,,,,
739,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,16,9wxw4a,alibabacloud.com,Image Classification with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/9wxw4a/image_classification_with_tensorflow/,Jen_Cl,1542180063,,0,1,False,https://b.thumbs.redditmedia.com/Yj5pONh9nMk8Kil5H3MCmXQFF1mj3ih7N-wkd6Lb-9c.jpg,,,,,
740,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,16,9wy3cx,alibabacloud.com,Collaborative Filtering for Product Recommendation,https://www.reddit.com/r/MachineLearning/comments/9wy3cx/collaborative_filtering_for_product_recommendation/,Jen_Cl,1542182186,,0,1,False,https://b.thumbs.redditmedia.com/qYFrzQ-EF_VDQEYqaR-V46wuPPMt-bjUDJoQKL4xvwk.jpg,,,,,
741,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,17,9wy961,softcrylic.com,Questions Im Bringing to TDWI Orlando,https://www.reddit.com/r/MachineLearning/comments/9wy961/questions_im_bringing_to_tdwi_orlando/,anderwhagel,1542183886,,0,1,False,default,,,,,
742,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,18,9wyiv4,self.MachineLearning,[D] Monitoring Pytorch wandb or visdom?,https://www.reddit.com/r/MachineLearning/comments/9wyiv4/d_monitoring_pytorch_wandb_or_visdom/,notenoughramTheThird,1542186880,"Hi,

&amp;#x200B;

I recently found this [https://www.wandb.com/blog/monitor-your-pytorch-models-with-five-extra-lines-of-code](https://www.wandb.com/blog/monitor-your-pytorch-models-with-five-extra-lines-of-code) and it seems like a cool way to monitor different aspects of training a network on pytorch. 

Until now I've been using visdom for monitoring ([https://github.com/facebookresearch/visdom](https://github.com/facebookresearch/visdom)) and I am still not interested in switching.

&amp;#x200B;

How ever I was curious whether you guys prefer one of these two over the other, or whether you have some magical 3rd tool.",8,1,False,self,,,,,
743,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,18,9wykwv,self.MachineLearning,[Discussion] Evaluation of sentence clustering using pair-wise similarities,https://www.reddit.com/r/MachineLearning/comments/9wykwv/discussion_evaluation_of_sentence_clustering/,devnull90,1542187534,"I am trying to evaluate different approaches for (text) sentence clustering. I have a dataset with (incomplete) pair-wise similarities (i.d. not for every sentence pair there exists a similarity) between sentences on ordinal scale (not similar=0, slightly similar=1, very similar=2, exactly similar=3), e.g.

*sentence1* | *sentence2* | 2

*sentence1* | *sentence3* | 1

*sentence2 | sentence4 | 0*

etc.

The goal is to cluster similar sentences together while less similar sentence should not be clustered together.

I know of the classic internal and external evaluation measures for clustering, however the internal measures rely on a distance measure and the external measures I know of compare the predicted clustering with a reference clustering/labels. As my labels are ordinal, I can't turn them into a distance measure (I found [GDM](http://keii.ae.jgora.pl/pracownicy/mw/2003_Jajuga_Walesiak_Bak_Springer.pdf) for ordinal data, but I am not sure if I can apply it)  
So far, I looked at simple counting measures (e.g. how many pairs of similarity '3' are clustered together) and a modified version of macro/micro-based F-Measure where I define 'True Positives', 'FalsePositives', etc for each ordinal label and then average them. However, either there are too many or they lack interpretability.

I wonder if missed any obvious things or literature/measures?",0,1,False,self,,,,,
744,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,18,9wyl8f,self.MachineLearning,A.i painter &amp; filters research papers,https://www.reddit.com/r/MachineLearning/comments/9wyl8f/ai_painter_filters_research_papers/,morra96,1542187639,[removed],0,1,False,self,,,,,
745,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,18,9wynpi,fooequipment.blogspot.com,Benefits of an Ice Maker Machine,https://www.reddit.com/r/MachineLearning/comments/9wynpi/benefits_of_an_ice_maker_machine/,foodequipment35,1542188355,,0,1,False,https://a.thumbs.redditmedia.com/licV9yqrLA5YzzTqzWCNZ7u_-FkaH39xIi0mFQ4A3m8.jpg,,,,,
746,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,18,9wypzb,self.MachineLearning,[P] Need opinions on how to go about using VizDoom and Imitation Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/9wypzb/p_need_opinions_on_how_to_go_about_using_vizdoom/,TheJoaquiUser,1542189044,"Hello, I am a 4th year student and my thesis is on analyzing the performance of different imitation learning algorithms in making a believable bot for first person shooter. I am fairly new to machine learning and was a bit too ambitious with my thesis topic, however I will still push through with this topic. (Unsure if I used the right tag and if this is in the wrong subreddit, I apologize for wasting your time and if you guys know where I should be directing my questions to)

&amp;#x200B;

The imitation learning algorithms I will be using will be, DAgger, SMILe, and APID.

&amp;#x200B;

I am having a difficult time figuring out the steps to go about coding these, planning out my methodology and implementation due to the limited studies and code that explore on the usage of Vizdoom and Imitation Learning. If there is anyone who have used Imitation Learning (or also known as Learning from Demonstration or Apprenticeship Learning) with Vizdoom, it would be a big help to learn about the steps you took in creating your bots.

&amp;#x200B;

\[Some Background Info on Why This Topic\]

My original topic was just to use DAgger in creating the first person shooter bot, but my adviser told me that I need to prove why I will use that specific algorithm. Due to limited studies on FPS bots made using Imitation Learning algorithms like the ones stated above I did not know how to defend my decision of just using DAgger. He then suggested why not create a study on the analysis of different IL algos as my topic instead.",0,1,False,self,,,,,
747,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,18,9wyrao,self.MachineLearning,Global Machine Learning Market,https://www.reddit.com/r/MachineLearning/comments/9wyrao/global_machine_learning_market/,Madhulathaa,1542189429,[removed],0,1,False,self,,,,,
748,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,19,9wyzz2,imarticus.org,5 Simple (But Important) Things To Remember About Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9wyzz2/5_simple_but_important_things_to_remember_about/,LearnFromImarticus,1542191956,,0,1,False,default,,,,,
749,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,20,9wzd1k,self.MachineLearning,Face detection software to track gym members?,https://www.reddit.com/r/MachineLearning/comments/9wzd1k/face_detection_software_to_track_gym_members/,spankjam,1542195725,[removed],0,1,False,self,,,,,
750,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,22,9wzzhr,arxiv.org,Image Smoothing via Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/9wzzhr/image_smoothing_via_unsupervised_learning/,qingnan_fan,1542201225,,0,1,False,default,,,,,
751,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,22,9x04wq,self.MachineLearning,Image Smoothing via Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/9x04wq/image_smoothing_via_unsupervised_learning/,qingnan_fan,1542202435,[removed],0,1,False,self,,,,,
752,MachineLearning,t5_2r3gv,2018-11-14,2018,11,14,23,9x0md8,self.MachineLearning,Is there a technology that can work like a human eye to recognize a verification code?,https://www.reddit.com/r/MachineLearning/comments/9x0md8/is_there_a_technology_that_can_work_like_a_human/,aisorry,1542206195,[removed],0,1,False,self,,,,,
753,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,0,9x10k1,self.MachineLearning,[D] Is there any publicly available pretrained convnet for images in medical domain?,https://www.reddit.com/r/MachineLearning/comments/9x10k1/d_is_there_any_publicly_available_pretrained/,bideex,1542209050,,5,1,False,self,,,,,
754,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,0,9x116a,self.MachineLearning,[D] Best budget build for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9x116a/d_best_budget_build_for_machine_learning/,crytoy,1542209162,"This topic deals with the best budget component for a computer build for Machine Learning.

Firstly, Does an AMD Ryzen processor criple or slow the training of a model?

Second, is an rtx 2070 a good choise, or is two gtx 1070 ti cards better in machine learning?",41,1,False,self,,,,,
755,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,0,9x18ya,self.MachineLearning,Where did whole idea of using human gait as identification mechanism come from?,https://www.reddit.com/r/MachineLearning/comments/9x18ya/where_did_whole_idea_of_using_human_gait_as/,Nowado,1542210636,[removed],0,1,False,self,,,,,
756,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,0,9x19mt,self.MachineLearning,"Simple Questions Thread November 14, 2018",https://www.reddit.com/r/MachineLearning/comments/9x19mt/simple_questions_thread_november_14_2018/,AutoModerator,1542210766,[removed],0,1,False,self,,,,,
757,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,0,9x1b9i,self.MachineLearning,"Are Bayesian methods like Graphical Models, VAEs and nonparametrics ever used in industry?",https://www.reddit.com/r/MachineLearning/comments/9x1b9i/are_bayesian_methods_like_graphical_models_vaes/,Kliotionirst,1542211067,[removed],0,1,False,self,,,,,
758,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,1,9x1imc,self.MachineLearning,Theoretical Analysis of Adversarial Learning: A Minimax Approach,https://www.reddit.com/r/MachineLearning/comments/9x1imc/theoretical_analysis_of_adversarial_learning_a/,Carl__Johnson__,1542212387,"[https://arxiv.org/abs/1811.05232](https://arxiv.org/abs/1811.05232)

&amp;#x200B;",0,1,False,self,,,,,
759,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,1,9x1jsq,arxiv.org,[R] Wasserstein Variational Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/9x1jsq/r_wasserstein_variational_gradient_descent/,LucaAmbrogioni,1542212611,,6,1,False,default,,,,,
760,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,1,9x1kux,self.MachineLearning,[R] Theoretical Analysis of Adversarial Learning: A Minimax Approach,https://www.reddit.com/r/MachineLearning/comments/9x1kux/r_theoretical_analysis_of_adversarial_learning_a/,Carl__Johnson__,1542212809,"[https://arxiv.org/abs/1811.05232](https://arxiv.org/abs/1811.05232)

&amp;#x200B;",0,1,False,self,,,,,
761,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,1,9x1o9t,medium.com,OpenAI Founder: Short-Term AGI Is a Serious Possibility,https://www.reddit.com/r/MachineLearning/comments/9x1o9t/openai_founder_shortterm_agi_is_a_serious/,Yuqing7,1542213452,,0,1,False,https://b.thumbs.redditmedia.com/8LU4NSRUbx3G2zC_VCaj4Qn5ychjjfQUwkeS_lJJrCQ.jpg,,,,,
762,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,1,9x1v3j,self.MachineLearning,Machine Learning Spot from SAP,https://www.reddit.com/r/MachineLearning/comments/9x1v3j/machine_learning_spot_from_sap/,gobobear,1542214711,[removed],0,1,False,self,,,,,
763,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,2,9x1wic,self.MachineLearning,[D] Costs with data labeling,https://www.reddit.com/r/MachineLearning/comments/9x1wic/d_costs_with_data_labeling/,ace_smash,1542214966,"Hello,

I'm doing a research on Active Learning and it's impact on large machine learning projects. I'm trying to find some information about real costs on data labeling in a real large scale project.

Does anyone know where I can find such information? Or does anyone have any experience with this part of a project?

Thanks",6,1,False,self,,,,,
764,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,2,9x22cv,self.MachineLearning,Was working on USB stick solution for plug and play ML. Need some input.,https://www.reddit.com/r/MachineLearning/comments/9x22cv/was_working_on_usb_stick_solution_for_plug_and/,Burindunsmor,1542215974,[removed],0,1,False,self,,,,,
765,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,2,9x28no,self.MachineLearning,Best place to write about my recent (stillborn) NLP project?,https://www.reddit.com/r/MachineLearning/comments/9x28no/best_place_to_write_about_my_recent_stillborn_nlp/,TappetNoise,1542217134,[removed],0,1,False,self,,,,,
766,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,2,9x2d7j,discourse.opengenus.org,Install OpenBLAS from source,https://www.reddit.com/r/MachineLearning/comments/9x2d7j/install_openblas_from_source/,rewqasdfsw,1542217980,,0,1,False,default,,,,,
767,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,3,9x2h7i,discourse.opengenus.org,Popular Datasets in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9x2h7i/popular_datasets_in_machine_learning/,rewqasdfsw,1542218704,,0,1,False,default,,,,,
768,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,3,9x2je4,self.MachineLearning,Performance profiling of Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9x2je4/performance_profiling_of_neural_networks/,rafeey,1542219085,[removed],0,1,False,self,,,,,
769,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,3,9x2qra,/r/MachineLearning/comments/9x2qra/p_generated_book_pages_with_a_pggan/,[P] Generated Book Pages with a PGGAN,https://www.reddit.com/r/MachineLearning/comments/9x2qra/p_generated_book_pages_with_a_pggan/,beeeeeeers,1542220431,,2,1,False,default,,,,,
770,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,4,9x36om,self.MachineLearning,Ask ML: Deep Learning - How to recognise mathematical expression? Where to start?,https://www.reddit.com/r/MachineLearning/comments/9x36om/ask_ml_deep_learning_how_to_recognise/,mahto56,1542223345,"Hi reddit

I have just started learning ML &amp; i have a short question, for a future project i want to build a mathematical expression evaluator for learning purpose. What type of model do i need to use ? How can recognise fractional numbers or powers etc? 

Any study material or research papers will be helpful 

Thanks",0,1,False,self,,,,,
771,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,4,9x36ze,self.MachineLearning,The development tool to paraphrasing datasets for NLP developers.,https://www.reddit.com/r/MachineLearning/comments/9x36ze/the_development_tool_to_paraphrasing_datasets_for/,Gandalfs_horse,1542223402,[removed],0,1,False,self,,,,,
772,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,4,9x3atf,self.MachineLearning,Comparing the Open AI Retro with Tensorflow,https://www.reddit.com/r/MachineLearning/comments/9x3atf/comparing_the_open_ai_retro_with_tensorflow/,WilkinsMicawbers,1542224087,[removed],0,1,False,self,,,,,
773,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,4,9x3cq6,self.MachineLearning,Tensorflow mpl.InvalidArgumentError:,https://www.reddit.com/r/MachineLearning/comments/9x3cq6/tensorflow_mplinvalidargumenterror/,jabbaluck,1542224452,"Hey guys I'm trying a script i found on github. In the past at the same point i had some batch size problems.. i lowered them down. NOW I'm getting this error ..

python/client/session.py"", line 1312, in \_extend\_graph  
tf\_session.ExtendSession(self.\_session)  
tensorflow.python.framework.errors\_impl.InvalidArgumentError: Cannot assign a device for operation 'save/SaveV2': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.  
Registered kernels:  
device='CPU'

I tried to add to the script:  
tf.Session(config=tf.ConfigProto(allow\_soft\_placement=True, log\_device\_placement=True))

But it didn't change really anything ..

I'm using tensorflow '1.11.0'.

do you have any clue on how to fix this ?",0,1,False,self,,,,,
774,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,4,9x3hd7,analyticsindiamag.com,TensorFlow 2.0 Is Coming,https://www.reddit.com/r/MachineLearning/comments/9x3hd7/tensorflow_20_is_coming/,code_x_7777,1542225297,,0,1,False,default,,,,,
775,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,5,9x3v3w,arxiv.org,[R] Sorting out Lipschitz function approximation,https://www.reddit.com/r/MachineLearning/comments/9x3v3w/r_sorting_out_lipschitz_function_approximation/,SleepyCoder123,1542227787,,23,1,False,default,,,,,
776,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,6,9x43cq,self.MachineLearning,[D] Importance of Unsupervised Learning in data preprocessing,https://www.reddit.com/r/MachineLearning/comments/9x43cq/d_importance_of_unsupervised_learning_in_data/,seemingly_omniscient,1542229508,"This term encompasses all types of machine learning in which the result is unknown and there is no teacher to train the algorithm. In the case of unsupervised learning, the learning algorithm receives only the input data and is instructed to extract knowledge from this data.

&amp;#x200B;

[https://www.aisoma.de/importance-of-unsupervised-learning-in-data-preprocessing/](https://www.aisoma.de/importance-of-unsupervised-learning-in-data-preprocessing/)",2,1,False,self,,,,,
777,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,6,9x4kkw,self.MachineLearning,"A sparsity aware implementation of ""Binarized Attributed Network Embedding"" (ICDM 2018).",https://www.reddit.com/r/MachineLearning/comments/9x4kkw/a_sparsity_aware_implementation_of_binarized/,benitorosenberg,1542232210,[removed],0,1,False,https://a.thumbs.redditmedia.com/D0QUH8jMOLzLkZWK576Q-vhw5B6zT-KB5h48b8qlaf8.jpg,,,,,
778,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,7,9x4z4m,self.MachineLearning,How to upload CIFAR 10 to Github?,https://www.reddit.com/r/MachineLearning/comments/9x4z4m/how_to_upload_cifar_10_to_github/,Fettinem,1542234892,[removed],0,1,False,self,,,,,
779,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,7,9x533r,ksaaem.org,"Participate because The total value of the Environmental Management prize amounts to 195,000 USD",https://www.reddit.com/r/MachineLearning/comments/9x533r/participate_because_the_total_value_of_the/,KSAAEM,1542235640,,0,1,False,default,,,,,
780,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,7,9x54os,medium.com,Andrew Ng Offers AI For Everyone on Coursera,https://www.reddit.com/r/MachineLearning/comments/9x54os/andrew_ng_offers_ai_for_everyone_on_coursera/,Yuqing7,1542235944,,0,1,False,default,,,,,
781,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,8,9x58a9,i.redd.it,Projeto de Fbricas de Produtos Qumicos IV,https://www.reddit.com/r/MachineLearning/comments/9x58a9/projeto_de_fbricas_de_produtos_qumicos_iv/,JamurGerloff,1542236606,,0,1,False,https://b.thumbs.redditmedia.com/AIRcnq4uElxsaT7sK65kSfT0LpymsOIKLsbjxaQBDTs.jpg,,,,,
782,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,8,9x5d86,hpe.com,[R] The 16 AI and ML conferences you should attend in 2019,https://www.reddit.com/r/MachineLearning/comments/9x5d86/r_the_16_ai_and_ml_conferences_you_should_attend/,yourbasicgeek,1542237520,,0,1,False,default,,,,,
783,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,8,9x5emq,self.MachineLearning,[D] Tensorflow predicting poker win %,https://www.reddit.com/r/MachineLearning/comments/9x5emq/d_tensorflow_predicting_poker_win/,everdev,1542237787,"I have a dataset of \~2M poker JSON hands like:

\`\`\`json

{

   players: \[{name: ""player1"", seatId: 0}, {name: ""player2"", seatId: 1}\],

   communityCards: \[""Ad"", ""Ah"", ""2c"", ""3d"", ""5c""\],

   actions: \[

{round: ""flop"", seatId: 0, action: ""check"", bet: 0},

{round: ""flop"", seatId: 1, action: ""bet"", bet: 15},

{round: ""flop"", seatId: 0, action: ""raise"", bet: 45},

{round: ""flop"", seatId: 1, action: ""fold"", bet: 0}

   \]

}

\`\`\`

&amp;#x200B;

I've classified each individual action in my training set based on a number of signals like the player's bet sizing, positioning, etc. for each round (pre-flop, flop, turn, river). However, some hands end at each round, so the data set is not uniform in size.

&amp;#x200B;

Being new to ML and Tensorflow, how would I train a model based on datapoints that vary in size and shape but are all equally valid and complete?

&amp;#x200B;

Also, I'd like to be able to make a prediction from any point in the hand. For example, I'd like to give the model the 1st 10 actions of a hand and predict the win % for each player.

&amp;#x200B;

Are there Tensorflow tutorials that cover this type of topic or can you point me in the right research direction?

&amp;#x200B;

Thanks!",5,1,False,self,,,,,
784,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,9,9x664k,self.MachineLearning,Can I become a machine learning engineer with diploma?,https://www.reddit.com/r/MachineLearning/comments/9x664k/can_i_become_a_machine_learning_engineer_with/,sooyl0511,1542243379,[removed],0,1,False,self,,,,,
785,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,10,9x6abp,blocklr.com,"Meet Nano (NANO), the Overlooked Crypto 300% Faster than Ripple (XRP)",https://www.reddit.com/r/MachineLearning/comments/9x6abp/meet_nano_nano_the_overlooked_crypto_300_faster/,yogibearnxs,1542244283,,0,1,False,https://b.thumbs.redditmedia.com/8vmJ1RxO_37_TzFNGfggf8tczBM-ZMirkKa4GarWabo.jpg,,,,,
786,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,10,9x6g8b,arxiv.org,[R] Extractive Summary as Discrete Latent Variables,https://www.reddit.com/r/MachineLearning/comments/9x6g8b/r_extractive_summary_as_discrete_latent_variables/,HigherTopoi,1542245531,,7,1,False,default,,,,,
787,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,11,9x6ov0,arxiv.org,A Function Fitting Method,https://www.reddit.com/r/MachineLearning/comments/9x6ov0/a_function_fitting_method/,rajesh_d24,1542247335,,0,1,False,default,,,,,
788,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,11,9x6xu0,youtube.com,"For anyone looking to get into machine learning, I would advise that you don't learn the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",https://www.reddit.com/r/MachineLearning/comments/9x6xu0/for_anyone_looking_to_get_into_machine_learning_i/,antaloaalonso,1542249180,,0,1,False,default,,,,,
789,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,11,9x70om,youtube.com,"For anyone looking to get into machine learning, I would advise that you don't learn the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",https://www.reddit.com/r/MachineLearning/comments/9x70om/for_anyone_looking_to_get_into_machine_learning_i/,antaloaalonso,1542249777,,0,1,False,https://b.thumbs.redditmedia.com/VeOOWnBsA49wJCtCan9jcEGRYYS_UxVWwAmiJ07YSPM.jpg,,,,,
790,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,11,9x72eu,self.MachineLearning,Question about NEAT algorithm,https://www.reddit.com/r/MachineLearning/comments/9x72eu/question_about_neat_algorithm/,kimchiboy03,1542250143,[removed],0,1,False,self,,,,,
791,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,12,9x7gxt,youtube.com,[P][ BerryNet ] 3 minutes tutorial of running object recognition on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/9x7gxt/p_berrynet_3_minutes_tutorial_of_running_object/,TimDT42,1542253280,,0,1,False,https://b.thumbs.redditmedia.com/7hDxq7HCBEIfChNxDONh2DesQh6KPLwwv5bfRMT7v7Y.jpg,,,,,
792,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,12,9x7jwj,self.MachineLearning,[P][ BerryNet ] 3 minutes tutorial of running object recognition on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/9x7jwj/p_berrynet_3_minutes_tutorial_of_running_object/,TimDT42,1542253931,[removed],0,1,False,self,,,,,
793,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,13,9x81u9,self.MachineLearning,[D] Choosing approximate factors in Expectation Propagation,https://www.reddit.com/r/MachineLearning/comments/9x81u9/d_choosing_approximate_factors_in_expectation/,cuenta4384,1542257940,"Expectation Propagation is an algorithm that allows approximating a posterior. For doing so, we need to select approximative factors \[1\]. How do the approximative factors should look like? For example, Minka chooses f\_i(\\theta) = s\_w \\prod\_k \\theta \^ {\\phi\_w,k}. It seems that the selection was based in order to break the summation that makes the marginal likelihood intractable. But why does he choose phi\_w,k, and s\_w? And how this factors should be initialized?

&amp;#x200B;

\[1\] [https://www.ime.usp.br/\~jstern/miscellanea/seminars/codata/Bishop06107.pdf](https://www.ime.usp.br/~jstern/miscellanea/seminars/codata/Bishop06107.pdf)

\[2\] [https://arxiv.org/pdf/1301.0588.pdf](https://arxiv.org/pdf/1301.0588.pdf)",6,1,False,self,,,,,
794,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,14,9x89ut,towardsdatascience.com,[P] Text Predictor - Generating Rap Lyrics with Recurrent Neural Networks (LSTMs),https://www.reddit.com/r/MachineLearning/comments/9x89ut/p_text_predictor_generating_rap_lyrics_with/,g_surma,1542259755,,0,1,False,default,,,,,
795,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,14,9x8cu4,analyticsinsight.net,Facebook Leverages Machine Learning to fight ISIS and Al-Qaeda Propaganda | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/9x8cu4/facebook_leverages_machine_learning_to_fight_isis/,analyticsinsight,1542260460,,0,1,False,default,,,,,
796,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,15,9x8tvw,self.MachineLearning,Finance and ML,https://www.reddit.com/r/MachineLearning/comments/9x8tvw/finance_and_ml/,yqzr09,1542264729,[removed],0,1,False,self,,,,,
797,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,16,9x8w2q,callstats.io,Free White Paper: Instantaneous Answers: How callstats.io Uses AI to Improve Communications,https://www.reddit.com/r/MachineLearning/comments/9x8w2q/free_white_paper_instantaneous_answers_how/,allieatcsio,1542265320,,0,1,False,default,,,,,
798,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,16,9x8zen,blissandreels.com.au,HESS Automated Handling Systems | TOPWERK Handling Systems | Hess Machine Australia | Bliss and Reels,https://www.reddit.com/r/MachineLearning/comments/9x8zen/hess_automated_handling_systems_topwerk_handling/,andrrut13,1542266182,,0,1,False,default,,,,,
799,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,16,9x8zg1,self.MachineLearning,[P] Einops - new tensor operations for pytorch and tensorflow,https://www.reddit.com/r/MachineLearning/comments/9x8zg1/p_einops_new_tensor_operations_for_pytorch_and/,arogozhnikov,1542266193,"Einops provides flexible and powerful operations to provide\\ readable and reliable deep learning code.

![video](29mue7j43gy11 ""Examples of rearrange operation."")

Project page: [https://github.com/arogozhnikov/einops](https://github.com/arogozhnikov/einops)

Tutorial: [https://github.com/arogozhnikov/einops/tree/master/docs](https://github.com/arogozhnikov/einops/tree/master/docs)

&amp;#x200B;",19,1,False,self,,,,,
800,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,16,9x93pc,self.MachineLearning,Application of Machine Learning in Natural Selection under Human Genome Project [Research] [R],https://www.reddit.com/r/MachineLearning/comments/9x93pc/application_of_machine_learning_in_natural/,asifrazzaq1988,1542267421,[removed],0,1,False,self,,,,,
801,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,16,9x9490,i.redd.it,Panel Beaters |Car Repairs | Crash Repairs,https://www.reddit.com/r/MachineLearning/comments/9x9490/panel_beaters_car_repairs_crash_repairs/,andrrut13,1542267576,,0,1,False,https://b.thumbs.redditmedia.com/2ggDXymhOYAeaXdTNo3mgpvPKFNkltsO9xDHBv20wrs.jpg,,,,,
802,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,16,9x97op,self.MachineLearning,Please help me with my project,https://www.reddit.com/r/MachineLearning/comments/9x97op/please_help_me_with_my_project/,topuchi13,1542268599,[removed],0,1,False,https://a.thumbs.redditmedia.com/DGvMadtz7T92pn7RtF269HPbEQTPonZ1B_hvTmO6aT4.jpg,,,,,
803,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,17,9x9972,self.MachineLearning,"If I train an LSTM autoencoder on coin toss sequences, can I use it to detect when an unfair coin is used?",https://www.reddit.com/r/MachineLearning/comments/9x9972/if_i_train_an_lstm_autoencoder_on_coin_toss/,beatricejensen,1542269035,[removed],0,1,False,self,,,,,
804,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,17,9x9e4k,self.MachineLearning,Now are you Interest in Devops Course,https://www.reddit.com/r/MachineLearning/comments/9x9e4k/now_are_you_interest_in_devops_course/,Blendtechinfo,1542270497,[removed],0,1,False,https://b.thumbs.redditmedia.com/LUGXNhGOVvrSFlELAeKWhVhu-NXftYGNCtDrUdIUhSg.jpg,,,,,
805,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,17,9x9hw1,i.redd.it,Top 8 Open Source AI Technologies in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9x9hw1/top_8_open_source_ai_technologies_in_machine/,AtivittiAi,1542271706,,0,1,False,https://b.thumbs.redditmedia.com/QOIOmRNIC6kja2hzC2XaPJo6eorOq_7H4MO26tz2q0Y.jpg,,,,,
806,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,17,9x9iwk,self.MachineLearning,[R] Has anyone experienced this behaviour of recurrent batch normalization with larger batch sizes?,https://www.reddit.com/r/MachineLearning/comments/9x9iwk/r_has_anyone_experienced_this_behaviour_of/,aziz_22,1542272024,"I have applied recurrent batch normalization to my baseline-model. ( Baseline model is encoder-decoder with attention-mechanisms) and both encoder and decoder are recurrent models.

I know that batch size does affect the behaviour of batch normalization, so I tried training both models on different set of batch sizes \[8,16,32,64\].

For 8,16 and 32 the modified baseline model was performing better, **BUT** with batch size=64, the baseline model is way much better. I have no clue about the real reason of such behaviour.

Some papers discussed this issue but they didn't give an explication of this phenomena. 

Example : Batch Normalized Recurrent Neural Networks ([https://arxiv.org/abs/1510.01378](https://arxiv.org/abs/1510.01378))

I hope to hear some feedback from you guys. 

 

https://i.redd.it/1gj1bs50lgy11.png",17,1,False,https://b.thumbs.redditmedia.com/XBP4WM_vucAVrjWPkXIFo2xaSU8Z9Q02DkJtHYv6j-k.jpg,,,,,
807,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,19,9x9y24,self.MachineLearning,Can anyone explain Cost function in a simplified manner?,https://www.reddit.com/r/MachineLearning/comments/9x9y24/can_anyone_explain_cost_function_in_a_simplified/,venkatesh_prasad,1542276571,[removed],0,1,False,self,,,,,
808,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,19,9xa0ki,self.MachineLearning,What would be the distribution of the elements of the inverse of a matrix where the elements of the non-inverted matrix are normally distributed?,https://www.reddit.com/r/MachineLearning/comments/9xa0ki/what_would_be_the_distribution_of_the_elements_of/,giants4210,1542277347,[removed],0,1,False,self,,,,,
809,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,19,9xa3pj,self.MachineLearning,Understanding the relation between Hamiltonian MCMC and Momentum Stochastic Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/9xa3pj/understanding_the_relation_between_hamiltonian/,ccmlacc,1542278308,[removed],0,1,False,self,,,,,
810,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,19,9xa434,self.MachineLearning,[D] Understanding the relation between Hamiltonian MCMC and Momentum Stochastic Gradient Descent,https://www.reddit.com/r/MachineLearning/comments/9xa434/d_understanding_the_relation_between_hamiltonian/,ccmlacc,1542278420,"Hey all,

Does anyone have a nice reference/blog post/paper for me to understand the relation/equivalence/differences between inference algorithms used in hierarchical bayesian models such as Hamiltonian MCMC, and optimization algorithms used in machine learning like Stochastic Gradient Descent with momentum?

In both of them we use gradient information to find our next point in our state space. I understand that with H-MCMC, we are actually trying to estimate the density/surface itself, and use all states that are sampled, instead of aiming to find the maximum likelihood state that we try to find with SGD. Am I right in this thinking, or is there more nuance to it?

Looking forward to your input. Thanks!",2,1,False,self,,,,,
811,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,19,9xa716,self.MachineLearning,Does anyone have experience with spell check algorithms?,https://www.reddit.com/r/MachineLearning/comments/9xa716/does_anyone_have_experience_with_spell_check/,dileep31,1542279331,[removed],0,1,False,self,,,,,
812,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,20,9xakad,youtube.com,Decision Tree In R,https://www.reddit.com/r/MachineLearning/comments/9xakad/decision_tree_in_r/,pooja307,1542283157,,0,1,False,https://b.thumbs.redditmedia.com/UdsGm9DQrB_TEbk6qQEOt9yyPISOOaiEq_JC3yBKCRY.jpg,,,,,
813,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,21,9xapql,feedsfloor.com,Increasing Demand for Intelligent Business Processes drives Machine Learning Market !,https://www.reddit.com/r/MachineLearning/comments/9xapql/increasing_demand_for_intelligent_business/,tejashreepatel,1542284561,,0,1,False,default,,,,,
814,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,21,9xas9p,youtube.com,Supervised and Unsupervised Learning In Machine Learning | Machine Learning Tutorial - Grt Info,https://www.reddit.com/r/MachineLearning/comments/9xas9p/supervised_and_unsupervised_learning_in_machine/,pooja307,1542285240,,0,1,False,default,,,,,
815,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,22,9xazzb,self.MachineLearning,[R] Binarized Attributed Network Embedding (ICDM 2018).,https://www.reddit.com/r/MachineLearning/comments/9xazzb/r_binarized_attributed_network_embedding_icdm_2018/,benitorosenberg,1542287153,"&amp;#x200B;

https://i.redd.it/3a4b2hlothy11.jpg

&amp;#x200B;

Code: [https://github.com/benedekrozemberczki/BANE](https://github.com/benedekrozemberczki/BANE)

Paper: [Binarized Attributed Network Embedding](https://www.researchgate.net/publication/328688614_Binarized_Attributed_Network_Embedding)

  
An implementation of ""Binarized Attributed Network Embedding"". Attributed network embedding enables joint representation learning of node links and attributes. Existing attributed network embedding models are designed in continuous Euclidean spaces which often introduce data redundancy and impose challenges to storage and computation costs. To this end, we present a Binarized Attributed Network Embedding model (BANE for short) to learn binary node representation. Specifically, we define a new Weisfeiler-Lehman proximity matrix to capture data dependence between node links and attributes by aggregating the information of node attributes and links from neighboring nodes to a given target node in a layer-wise manner. Based on the Weisfeiler-Lehman proximity matrix, we formulate a new Weisfiler-Lehman matrix factorization learning function under the binary node representation constraint. The learning problem is a mixed integer optimization and an efficient cyclic coordinate descent (CCD) algorithm is used as the solution. Node classification and link prediction experiments on real-world datasets show that the proposed BANE model outperforms the state-of-the-art network embedding methods.",1,1,False,https://a.thumbs.redditmedia.com/om_vlN7zq0NwPoR0wYCAio1A73LFPg-spzXMs7nMqs8.jpg,,,,,
816,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,22,9xb0e5,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Nov. 15, 2018",https://www.reddit.com/r/MachineLearning/comments/9xb0e5/n_weekly_machine_learning_opensource_roundup_nov/,stkim1,1542287252,,0,1,False,default,,,,,
817,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,22,9xb3aa,youtu.be,"Machine Learning in Practice (Industry &amp; Economics). Some impressions of NLP, Predictive Maintenance &amp; Computer Vision Applications",https://www.reddit.com/r/MachineLearning/comments/9xb3aa/machine_learning_in_practice_industry_economics/,seemingly_omniscient,1542287934,,0,1,False,default,,,,,
818,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,22,9xb3rx,deeplearningio.com,How to Install Amazon AWS for Deep Learning? | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/9xb3rx/how_to_install_amazon_aws_for_deep_learning_deep/,mrcgllr,1542288039,,0,1,False,default,,,,,
819,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,22,9xb6p0,youtu.be,"[P] Machine Learning in Practice (Industry &amp; Economics). Some impressions of NLP, Predictive Maintenance &amp; Computer Vision Applications",https://www.reddit.com/r/MachineLearning/comments/9xb6p0/p_machine_learning_in_practice_industry_economics/,seemingly_omniscient,1542288728,,0,1,False,https://b.thumbs.redditmedia.com/_XcywwRMloHaFY0826HbDuIZmqPoxxrLggFOMyvdxPQ.jpg,,,,,
820,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,22,9xbauh,self.MachineLearning,VAE or Auto-encoder on category data,https://www.reddit.com/r/MachineLearning/comments/9xbauh/vae_or_autoencoder_on_category_data/,noelkev0,1542289637,[removed],0,1,False,self,,,,,
821,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,23,9xbi0c,medium.com,Releasing A Mongolian Text Classification Dataset,https://www.reddit.com/r/MachineLearning/comments/9xbi0c/releasing_a_mongolian_text_classification_dataset/,omarsar,1542291120,,0,1,False,https://b.thumbs.redditmedia.com/uxZppIXCubUbp2fN2gs89tYHm1RUt4D4TwePiXDsqxI.jpg,,,,,
822,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,23,9xbl50,self.MachineLearning,[P] Machine Learning in Practice (Industry &amp; Economics).,https://www.reddit.com/r/MachineLearning/comments/9xbl50/p_machine_learning_in_practice_industry_economics/,seemingly_omniscient,1542291754," **Some short impressions of NLP, Predictive Maintenance &amp; Computer Vision Applications** 

&amp;#x200B;

[https://youtu.be/dgGAJQ8sKf8](https://youtu.be/dgGAJQ8sKf8)",3,1,False,self,,,,,
823,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,23,9xbr45,self.MachineLearning,"I need write a story for have a job today in english for evaluate my english, is a story with 3 verbals tenses but with only ten lines. Like a weird story, what can u propose?",https://www.reddit.com/r/MachineLearning/comments/9xbr45/i_need_write_a_story_for_have_a_job_today_in/,CamilaCastellanos91,1542292956,,0,1,False,self,,,,,
824,MachineLearning,t5_2r3gv,2018-11-15,2018,11,15,23,9xbu17,self.MachineLearning,Andrew Ng Offers AI For Everyone (new Coursera course starting early 2019),https://www.reddit.com/r/MachineLearning/comments/9xbu17/andrew_ng_offers_ai_for_everyone_new_coursera/,chaoticflipflops,1542293540,[removed],3,1,False,self,,,,,
825,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,0,9xbwu2,self.MachineLearning,Is there any name for human-computer interaction machine learning field?,https://www.reddit.com/r/MachineLearning/comments/9xbwu2/is_there_any_name_for_humancomputer_interaction/,dee_lucky,1542294094,[removed],0,1,False,self,,,,,
826,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,0,9xcafm,self.MachineLearning,"[P] Pruning, cost functions and adversarial noise.",https://www.reddit.com/r/MachineLearning/comments/9xcafm/p_pruning_cost_functions_and_adversarial_noise/,jngannon,1542296715,"\[[https://jngannon.github.io/home.html](https://jngannon.github.io/home.html)\]([https://jngannon.github.io/home.html](https://jngannon.github.io/home.html))

&amp;#x200B;

Hi, I have a couple of blogs about some research I have been working on with neural networks. I have removed the softmax function, and pruned parameters using 2 methods, this has lead some increases in robustness to some adversarial noise. I have then looked at the outputs of the model to predict if an image has been corrupted with adversarial noise.

The website very basic, but I think the results are quite interesting. I have tried to keep it brief. If there is some interest here I will keep updating it (and fixing up the code). All of the experiments are done using tensorflow and MNIST, they should be easily replicated.

I would appreciate feedback. I am self taught, and do this as a hobby. I will respond, and fix things up when I can.",0,1,False,self,,,,,
827,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,1,9xcfok,self.MachineLearning,Is this plan for training a machine learning algorithm for important problems good?,https://www.reddit.com/r/MachineLearning/comments/9xcfok/is_this_plan_for_training_a_machine_learning/,reuiwdfvcdedxc,1542297688,[removed],0,1,False,self,,,,,
828,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,1,9xcl0s,self.MachineLearning,GridSearchCV Python,https://www.reddit.com/r/MachineLearning/comments/9xcl0s/gridsearchcv_python/,plusgarbage,1542298638,"Hello everyone,
I'm doing a ML project and I have a doubt:

I have a dataset with some features and I have to use a simple SVM to predict some things.

The first thing I did is to split the dataset into training and test set, then I took this training set and I used it with the GridSearchCV function in python with k-fold of 10, to calculate the best parameters for the rbf kernel of the SVM.

After obtaining the best C and gamma parameters, I train the SVM with the same training set used before and with the best values of C and gamma.

I perform a test with the test set that I had calculated at the beginning.

Is this procedure correct?
I have many doubts because I don't know if I have to split the dataset in three parts: one for train, one for validation and one for test and use the validation in the GridSearchCV function..

Thank you",0,1,False,self,,,,,
829,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,1,9xcs4m,self.MachineLearning,[P] Computer Vision platform for video processing in real-time,https://www.reddit.com/r/MachineLearning/comments/9xcs4m/p_computer_vision_platform_for_video_processing/,bkmnsk,1542299965,[removed],0,1,False,https://b.thumbs.redditmedia.com/f4arKRCM_mZPWyPpAD-wJvvV48x1QoK12wpjO_uejXg.jpg,,,,,
830,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,2,9xd1z1,self.MachineLearning,On the Future of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9xd1z1/on_the_future_of_machine_learning/,mathnathan,1542301716,[removed],0,1,False,self,,,,,
831,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,2,9xd3zf,self.MachineLearning,[D] On the Future of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9xd3zf/d_on_the_future_of_machine_learning/,mathnathan,1542302071,"I was recently invited on a podcast to discuss the future of machine learning. I'm curious how other experts feel about some of the more saucy ethical/moral topics we discussed. Perhaps we can discuss some here? Or just roast me! No better way to improve than with constructive criticism from all the bright minds here.

[https://www.youtube.com/watch?v=elXMAFc7joU](https://www.youtube.com/watch?v=elXMAFc7joU)

Thanks for your time!",0,1,False,self,,,,,
832,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,2,9xd6mm,self.MachineLearning,[D] Anecdote of the human brain producing synthetic data for training,https://www.reddit.com/r/MachineLearning/comments/9xd6mm/d_anecdote_of_the_human_brain_producing_synthetic/,GearWorst,1542302544,"This is a personal anecdote. Take it with a grain of salt.

Last night, I started a medication that causes you to vividly remember your dreams.  I was showering this morning thinking of what i dreamed:

* I was passing by a restaurant with an active shooter inside

I remembered that i was dating the shooter but it didn't seem to fit in the story line of the restaurant scenario.  That's when I remembered a series of preceding dreams that all had variations in the scenario.

* I was moving into a Big Brother style reality show with a number of people
* I was in a regular house with the same people dating one of the girls with an active shooter in the house
* I was in the same house and i was dating the shooter
* I was living alone in a smaller apartment with the shooter inside a nearby restaurant

It's an interesting insight into how the brain might be training it's neural networks.  From this one anecdote, it appears the brain is generating synthetic data and running it through the network to strengthen synapses.   If i had to guess, the target value was survival and it was trying to increase the probability.",1,1,False,self,,,,,
833,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,2,9xd7ea,discourse.opengenus.org,Intuition behind Restricted Boltzmann Machines,https://www.reddit.com/r/MachineLearning/comments/9xd7ea/intuition_behind_restricted_boltzmann_machines/,rewqasdfsw,1542302670,,0,1,False,default,,,,,
834,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,3,9xdmps,self.MachineLearning,Why clustering is called a Machine Learning model?,https://www.reddit.com/r/MachineLearning/comments/9xdmps/why_clustering_is_called_a_machine_learning_model/,panditu007,1542305331,"&gt;The difference between Machine Learning approach and Traditional Programming approach is that in Machine Learning, we provide some data with inputs as well as the output to a system and expect it to learn some rules based on the given data. After that, an input is given and using the learned rule output is calculated.   
&gt;  
&gt;Whereas, in the Traditional Programming approach, inputs and some rules are provided to a system and output is expected from the system based on that rules.

&amp;#x200B;

In clustering we do the same, we provide some input and some mathematical rule. No output is given and we expect the system to return clusters in the form of output. 

&amp;#x200B;

We are doing the exact same thing as of the Traditional Programming approach, then why it is called a Machine Learning algorithm? ",0,1,False,self,,,,,
835,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,3,9xdnp0,self.MachineLearning,Fastest way to get acquainted with fast.ai deep learning library,https://www.reddit.com/r/MachineLearning/comments/9xdnp0/fastest_way_to_get_acquainted_with_fastai_deep/,acobobby,1542305493,[removed],0,1,False,self,,,,,
836,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,3,9xdo0u,self.MachineLearning,Word2Vec for cryptography.,https://www.reddit.com/r/MachineLearning/comments/9xdo0u/word2vec_for_cryptography/,GrumpyErroneousGuy,1542305547,[removed],0,1,False,self,,,,,
837,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,3,9xdtop,self.MachineLearning,Store the complete network on VRAM at all times?,https://www.reddit.com/r/MachineLearning/comments/9xdtop/store_the_complete_network_on_vram_at_all_times/,DasSteak01,1542306556,[removed],0,1,False,self,,,,,
838,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,3,9xdtyk,self.MachineLearning,Machine Learning Beginner to Advanced Full with Python,https://www.reddit.com/r/MachineLearning/comments/9xdtyk/machine_learning_beginner_to_advanced_full_with/,ITNuggetsOnline,1542306606,[removed],0,1,False,self,,,,,
839,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,3,9xdv31,inference.vc,Online Bayesian Deep Learning in Production at Tencent,https://www.reddit.com/r/MachineLearning/comments/9xdv31/online_bayesian_deep_learning_in_production_at/,alexeyr,1542306806,,0,1,False,https://b.thumbs.redditmedia.com/9ZcPqiO37Y-9aR6xFfZRviN21yXf3fAjeLVpLc9cRjE.jpg,,,,,
840,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,4,9xelj1,self.MachineLearning,Why is gradient descent so popular for machine learning?,https://www.reddit.com/r/MachineLearning/comments/9xelj1/why_is_gradient_descent_so_popular_for_machine/,LHHDU,1542311477,"Hello everybody.

I don't have much background/knowlegde in machine learning but I have a background in mathematical optimization. I was wondering why gradient descent is almost always used (correct me if I'm wrong) in machine learning while there are other more powerful algorithms out there?
Is it because the problem structures are always fitting for gradient descent and other (more complex) algorithm wouldn't  improve the soultion much? Or is it because it is just easier to understand/implement and it works just fine?

I also might be completly wrong and gradient descent is just always used in the basic literature I've read to get familiar with machine learning. 

Anyways I would be realy intereseted in your opinons on that topic.",0,1,False,self,,,,,
841,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,4,9xeo0n,self.MachineLearning,"Alternatives to PCA? Specifically, optimization techniques that focus on separability, not variance?",https://www.reddit.com/r/MachineLearning/comments/9xeo0n/alternatives_to_pca_specifically_optimization/,OldManufacturer,1542311932,[removed],0,1,False,self,,,,,
842,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,5,9xeufx,self.MachineLearning,"[R] Alternatives to PCA? Specifically, optimization techniques that focus on separability, not variance?",https://www.reddit.com/r/MachineLearning/comments/9xeufx/r_alternatives_to_pca_specifically_optimization/,OldManufacturer,1542313052,"Currently using matlab. I am attempting to hybridize features from two different sources (fNIRS and EEG) in order to improve multi classifier (4 groups) accuracy. PCA has not been giving me ideal results, even after growing/pruning the top 99% variance features. I was expecting this since PCA does a poor job of capturing separability. Any experiences with this (or advice on how to improve my use of PCA) would surely help me! Thank you in advance! ",53,1,False,self,,,,,
843,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,5,9xf0fe,self.MachineLearning,Website to share ML models and dataset,https://www.reddit.com/r/MachineLearning/comments/9xf0fe/website_to_share_ml_models_and_dataset/,madhanrajan357,1542314167,[removed],0,1,False,self,,,,,
844,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,5,9xf5z5,self.MachineLearning,Building a computer for deeplearning at work. Whats your take on the best build for my budget?,https://www.reddit.com/r/MachineLearning/comments/9xf5z5/building_a_computer_for_deeplearning_at_work/,that_marty_guy,1542315189,[removed],0,1,False,self,,,,,
845,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,6,9xfe6p,youtube.com,"If you aren't in a hurry to start implementing deep learning, then you may want to consider that before you jump into an API like Keras, you learn exactly how Neural Networks do what they do. Here's a helpful video series that talks about just this",https://www.reddit.com/r/MachineLearning/comments/9xfe6p/if_you_arent_in_a_hurry_to_start_implementing/,JohnJohnant,1542316679,,0,1,False,default,,,,,
846,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,6,9xfmey,self.MachineLearning,[D] Realistic Evaluation of Deep Semi-Supervised Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/9xfmey/d_realistic_evaluation_of_deep_semisupervised/,KillerNewton,1542318233,"[https://arxiv.org/pdf/1804.09170.pdf](https://arxiv.org/pdf/1804.09170.pdf)  
This paper was posted a while back, but I wanted to ask a couple questions that perhaps the reddit community knows of.

  
In the paper, the authors experiment with a theoretical upper-bound of what supervised learning on a 4k CIFAR dataset is capable of, and they achieve the result of a 13.4% error rate with the Shake-Shake model (highly regularized). Their main point being that when comparing SSL methods/models, there need to be better fully supervised baselines.  


However, I'm curious what the Shake-Shake model performance would look like on architecture independent SSL methods such as VAT. Has anyone benchmarked this?",5,1,False,self,,,,,
847,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,7,9xg544,arxiv.org,[1811.01768v1] A Biologically Plausible Learning Rule for Deep Learning in the Brain (Q-AGREL),https://www.reddit.com/r/MachineLearning/comments/9xg544/181101768v1_a_biologically_plausible_learning/,csxeba,1542321756,,25,1,False,default,,,,,
848,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,9,9xgw1r,youtube.com,Array - Numpy Python Tutorial,https://www.reddit.com/r/MachineLearning/comments/9xgw1r/array_numpy_python_tutorial/,jeffxu999,1542327142,,0,1,False,https://a.thumbs.redditmedia.com/SgM7M8DSR7WWGlC_yzGBvIX2Jrv2DgQMFxUbNiQnfQ0.jpg,,,,,
849,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,9,9xgwhh,self.MachineLearning,how can i reduce overfitting ?,https://www.reddit.com/r/MachineLearning/comments/9xgwhh/how_can_i_reduce_overfitting/,GoBacksIn,1542327222,[removed],0,1,False,self,,,,,
850,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,9,9xgy6c,blog.insightdatascience.com,Optimizing walking routes to keep you in the sun or shade,https://www.reddit.com/r/MachineLearning/comments/9xgy6c/optimizing_walking_routes_to_keep_you_in_the_sun/,hszafarek,1542327573,,0,1,False,default,,,,,
851,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,10,9xhdfp,self.MachineLearning,[P] Learning derivative function for polynomials with PyTorch,https://www.reddit.com/r/MachineLearning/comments/9xhdfp/p_learning_derivative_function_for_polynomials/,iamsamng,1542330777,"Recently, I went through [3Blue1Browns series on Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) the third time and finally, something clicked for me. I was especially mind-blown by the ideas in the [last episode](https://www.youtube.com/watch?v=TgKwz5Ikpc8) of the series, titled Abstract vector spaces, so I wrote a note to recap what I learned and to encourage everyone to check out the series. I also carried out a small experiment to learn the derivative function for polynomials with PyTorch to confirm the idea put forth in the episode: **that you can describe derivative function (which is linear) with a linear transformation, or a matrix multiplication**.

Here's the link to the blog post: [https://medium.com/@iamsamng/learning-derivative-function-for-polynomials-with-pytorch-818d85e9c9ae](https://medium.com/@iamsamng/learning-derivative-function-for-polynomials-with-pytorch-818d85e9c9ae). Any feedback would be greatly appreciated! :)

Throughout the 3Blue1Brown series, one intuition point really strikes me: a matrix multiplication is really just linearly transforming the input space. With this geometrical view, as I wrote in the Medium post above, for neural networks, where we have a lot of instances of multiplying weight matrices with some input x, its awesome to think of the multiplying as linearly transforming vector x to some other spaces and **tweaking the weight matrices as adjusting how much each basis vector of the space gets changed** so that x be transformed to the correct spot. There are more insights to be gained from this geometrical view of linear algebra, and I'll try to cover some in another post.

&amp;#x200B;

So yeah, I'd encourage anyone interested in ML to checkout 3B1B's series on Linear Algebra.

&amp;#x200B;",6,1,False,self,,,,,
852,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,10,9xhdh6,self.MachineLearning,I created a free tool for picture in-painting. You can use it to remove stuffs on image or fix broken images,https://www.reddit.com/r/MachineLearning/comments/9xhdh6/i_created_a_free_tool_for_picture_inpainting_you/,coolwulf,1542330786,[removed],0,1,False,self,,,,,
853,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,10,9xhjej,shell.com,Announcing the Shell AI residency Program,https://www.reddit.com/r/MachineLearning/comments/9xhjej/announcing_the_shell_ai_residency_program/,Pythagoras314159,1542332072,,0,1,False,default,,,,,
854,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,11,9xi3s2,reu2018dl.github.io,YOLO-LITE: a lite version of YOLO,https://www.reddit.com/r/MachineLearning/comments/9xi3s2/yololite_a_lite_version_of_yolo/,zhamisen,1542336436,,1,1,False,default,,,,,
855,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,12,9xicpw,self.MachineLearning,[P][ BerryNet ] 3 minutes tutorial of running object recognition on Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/9xicpw/p_berrynet_3_minutes_tutorial_of_running_object/,TimDT42,1542338423," 

Good news! We made this tutorial video to teach complete beginners how to turn Raspberry Pi 3 into an AIoT gateway with deep learning running on it. No deep learning knowledge is needed!

[https://www.youtube.com/watch?v=xDwUT0CPoSk&amp;feature=youtu.be](https://www.youtube.com/watch?v=xDwUT0CPoSk&amp;feature=youtu.be)",2,1,False,self,,,,,
856,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,12,9xieyi,howhotami.xyz,Random deep learning AI that tells you How Hot You Are,https://www.reddit.com/r/MachineLearning/comments/9xieyi/random_deep_learning_ai_that_tells_you_how_hot/,genixpro,1542338930,,0,1,False,default,,,,,
857,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,13,9xiuc8,self.MachineLearning,Newbie projects?,https://www.reddit.com/r/MachineLearning/comments/9xiuc8/newbie_projects/,antfliestomars,1542342389,[removed],0,1,False,self,,,,,
858,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,14,9xj5sx,self.MachineLearning,"I started AndrewNG courses on 12th Nov. Rightnow, I have completed 4 Chapters ( Neural Networks) and hope to complete by next week. I was searching for sample projects for practising (Linear regression &amp; Logistic ) . Any advice",https://www.reddit.com/r/MachineLearning/comments/9xj5sx/i_started_andrewng_courses_on_12th_nov_rightnow_i/,iamfeelinggreat,1542345090,[removed],0,1,False,self,,,,,
859,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,14,9xj7xc,self.MachineLearning,[News] A full PyTorch-like experience on iOS using NimTorch,https://www.reddit.com/r/MachineLearning/comments/9xj7xc/news_a_full_pytorchlike_experience_on_ios_using/,voidtarget,1542345611,"You can literally use and deploy the same code you have on your workstations. No intermediate steps, no translation, no lite versions.

You are able to to train, modify and be creative with your neural networks within your apps.

Please check this article for more details and ideas!
https://link.medium.com/0Gl9h8ePSR",0,1,False,self,,,,,
860,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,14,9xjd34,self.MachineLearning,[P] I created a free tool for picture in-painting. You can use it to remove stuffs on image or fix broken images,https://www.reddit.com/r/MachineLearning/comments/9xjd34/p_i_created_a_free_tool_for_picture_inpainting/,coolwulf,1542346870,"Hi,

I am the author of NeuralRad, published on this subreddit in the past. Here I would like to bring another tool I wrote recently which is utilizing both works from Nvidia group of patial convolution and UIUC group of gated convolution. The results are not perfect yet but I would like to make a beta version accessible for the public to benefit this work.

The address of the tool is: http://neuralrad.com:5001/upload



How to use it:

    Upload an image of size 512x512 (I know this is a limitation, but the future version will support bigger image for sure. This is the demo)
    Mark with mouse on part of image you want to fix.
    Click 'Fix!' button then click 'result' button to see the fixed image.

Some sample results:

https://imgur.com/a/I6leang
",10,1,False,self,,,,,
861,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,15,9xjj1j,self.MachineLearning,ML generated text,https://www.reddit.com/r/MachineLearning/comments/9xjj1j/ml_generated_text/,AndreaStewart,1542348377,[removed],0,1,False,self,,,,,
862,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,16,9xjx5l,i.redd.it,Data Science Main Formulas for Machine Learning...,https://www.reddit.com/r/MachineLearning/comments/9xjx5l/data_science_main_formulas_for_machine_learning/,AtivittiAi,1542352185,,0,1,False,default,,,,,
863,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,16,9xk0b6,labex.io,"Hey r/MachineLearning, it's my work on the basic ML tutorial: Supervised Learning: Regression. Come to try it out and tell me your opinions.",https://www.reddit.com/r/MachineLearning/comments/9xk0b6/hey_rmachinelearning_its_my_work_on_the_basic_ml/,LeKuzma,1542353033,,0,1,False,default,,,,,
864,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,16,9xk1km,self.MachineLearning,[P] Transfer learning - identify one of 120 dog breeds by a dogs photo.,https://www.reddit.com/r/MachineLearning/comments/9xk1km/p_transfer_learning_identify_one_of_120_dog/,behindthedash,1542353402,"My idea was to use transfer learning by applying a neural network trained on the ImageNet dataset to our particular task. ImageNet is an image dataset which contains more than 14 million images of more than 1000 classes. Neural networks trained on this data are extremely good at extracting all kinds of useful features from the images. So I would take such a network, remove its softmax layer and build my own much simpler network on the top of it to get the predictions. This way I would save days of training which I would need to train a convolutional network from scratch.

Still, using only one ImageNet pre-trained neural network appeared to be not enough to achieve the required performance, so I ended up using three networks of different architectures: ResNet50, Inception and Xception.

The resulting architecture looked like this:

&amp;#x200B;

https://i.redd.it/kuqegm9tany11.png

&amp;#x200B;

During training I divided the network into two parts to reduce time consumption: first I extracted and stored the features using each pre-trained network and then used the stored features as an input to the trainable part of the network. As a regularization to improve performance on the test set I used two dropout layers.

The resulting performance I achieved was 85% on the test set. The investigation into incorrect predictions showed that in some cases more than one dog or humans were present in the image or a dog was partially obscured by other objects. These as well as the cases where a dog was difficult to distinguish from the background my network couldnt deal with.

The network was implemented in Python using Keras framework with Tensorflow backend.",1,1,False,https://b.thumbs.redditmedia.com/NwzpHn7WMmJipTfLXvefPbrx0vkIJnHYmwBZL5-06Xg.jpg,,,,,
865,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,16,9xk6wm,self.MachineLearning,"Beginner trying fast.ai ml course, why the difficult setup?",https://www.reddit.com/r/MachineLearning/comments/9xk6wm/beginner_trying_fastai_ml_course_why_the/,snicksn,1542354953,[removed],0,1,False,self,,,,,
866,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,17,9xk9jp,twitter.com,Using GANs to generate Master[Finger]Prints that unlock 22-78% phones sensors.,https://www.reddit.com/r/MachineLearning/comments/9xk9jp/using_gans_to_generate_masterfingerprints_that/,alcaster0,1542355755,,0,1,False,default,,,,,
867,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,17,9xkfgt,github.com,Gluon to PyTorch deep neural network model converter,https://www.reddit.com/r/MachineLearning/comments/9xkfgt/gluon_to_pytorch_deep_neural_network_model/,nerox8664,1542357584,,1,1,False,default,,,,,
868,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,17,9xkgin,indiumsoftware.com,Facial Recognition and its Applications,https://www.reddit.com/r/MachineLearning/comments/9xkgin/facial_recognition_and_its_applications/,indiumsoftware18,1542357916,,0,1,False,default,,,,,
869,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,17,9xkhsg,hitechnectar.com,Machine Learning and Deep Learning - Know the Difference,https://www.reddit.com/r/MachineLearning/comments/9xkhsg/machine_learning_and_deep_learning_know_the/,cwadamsmith,1542358303,,0,1,False,default,,,,,
870,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,18,9xkniq,self.MachineLearning,[D] AISTATS reviews are out,https://www.reddit.com/r/MachineLearning/comments/9xkniq/d_aistats_reviews_are_out/,srossi93,1542359981,Good rebuttal for everyone's submitted there! ,26,1,False,self,,,,,
871,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,18,9xkodz,self.MachineLearning,"Inexpensive formula for L2 norm between Gaussian-smoothened samples - applications for generative autoencoder (CDF vs PDF-based distance?), optimizing GMM...?",https://www.reddit.com/r/MachineLearning/comments/9xkodz/inexpensive_formula_for_l2_norm_between/,jarekduda,1542360259,[removed],0,1,False,self,,,,,
872,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,18,9xksd9,medium.com,Understanding the scaling of L regularization in the context of neural networks,https://www.reddit.com/r/MachineLearning/comments/9xksd9/understanding_the_scaling_of_l_regularization_in/,shaypal5,1542361444,,1,1,False,https://b.thumbs.redditmedia.com/UjardTJreOzXfe_ZhMITLua4a5HMAzEIn4J3vMJfc9k.jpg,,,,,
873,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,18,9xkthe,self.MachineLearning,Small dataset for image classification,https://www.reddit.com/r/MachineLearning/comments/9xkthe/small_dataset_for_image_classification/,martian_rover,1542361789,[removed],0,1,False,https://b.thumbs.redditmedia.com/hyYIgRoFPIdb5w4DQqzJmMeBS2wKCXUIUK_j9Q2EWLc.jpg,,,,,
874,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,18,9xkuhx,self.MachineLearning,"[D] Inexpensive formula for L2 norm between Gaussian-smoothened samples - applications for generative autoencoder (CDF vs PDF-based distance?), optimizing GMM...?",https://www.reddit.com/r/MachineLearning/comments/9xkuhx/d_inexpensive_formula_for_l2_norm_between/,jarekduda,1542362084,"Surprisingly, it turns out that there is inexpensive formula for L2 norm between multivariate Gaussian ensembles, also for general covariance matrices ( https://arxiv.org/pdf/1811.04751 ) - as a simple sum over pairs of centers.

I haven't seen it in literature (?) and it looks practical so I thought to try to discuss it here. Some basic  applications:

* generative autoencoders require some chosen distribution for latent variables, usually Gaussian - the question is how to evaluate distance between sample and Gaussian? A year ago there was proposed [Wasserstein metric for that](https://arxiv.org/pdf/1711.01558.pdf) - which is tough to calculate and similar to L1 distance of CDFs. Here we have cheap L2 distance between PDFs (densities) - a big question is which is better and why?

* Formula for L2 distance between two Gaussian Mixture Models allows for inexpensive optimization of centers and covariance matrices, e.g. for reducing the number of Gaussians (?)",0,1,False,self,,,,,
875,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,18,9xkun7,blog.varunajayasiri.com,[P] I created this library to organize machine learning experiments,https://www.reddit.com/r/MachineLearning/comments/9xkun7/p_i_created_this_library_to_organize_machine/,mlvpj,1542362130,,1,1,False,default,,,,,
876,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,19,9xkwoe,i.redd.it,[R] My implementation of recurrent batch normalization,https://www.reddit.com/r/MachineLearning/comments/9xkwoe/r_my_implementation_of_recurrent_batch/,aziz_22,1542362733,,0,1,False,https://b.thumbs.redditmedia.com/pnXrFcmf8kEgTzXpPI7TYohHzILVJOQFZKKvHSZuYvE.jpg,,,,,
877,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,19,9xkxd9,self.MachineLearning,My implementation of recurrent batch normalization,https://www.reddit.com/r/MachineLearning/comments/9xkxd9/my_implementation_of_recurrent_batch_normalization/,aziz_22,1542362931,[removed],0,1,False,https://a.thumbs.redditmedia.com/U5lpmhuRCXijEbmsLUGxbXUjvPT-deaxmhD_vQcyIS4.jpg,,,,,
878,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,19,9xkzo9,self.MachineLearning,Written my first Medium article on how to use Google Colab to run Pointnet++,https://www.reddit.com/r/MachineLearning/comments/9xkzo9/written_my_first_medium_article_on_how_to_use/,melaos,1542363590,[removed],0,1,False,self,,,,,
879,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,19,9xl20p,i.redd.it,"Learn how to Perform Cloud Data Science with Azure Machine Learning course is to give students the ability to analyze and present data by using Azure Machine Learning, and to introduce the use of machine learning with big data tools such as HDInsight and R Services. Learn more: https://goo.gl/8nM99n",https://www.reddit.com/r/MachineLearning/comments/9xl20p/learn_how_to_perform_cloud_data_science_with/,NetComLearning,1542364294,,0,1,False,https://a.thumbs.redditmedia.com/XNQFhi0cRl4t4TjGPI3TeGYvC6DX1WcwhySChA2PeO4.jpg,,,,,
880,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,19,9xl5ji,thetechnologyheadlines.com,Google to launch StreetLearn dataset to teach machine learning models to navigate cities,https://www.reddit.com/r/MachineLearning/comments/9xl5ji/google_to_launch_streetlearn_dataset_to_teach/,-TTH,1542365426,,0,1,False,https://a.thumbs.redditmedia.com/iq5FrHSe_kDYrB5hwKHnpX6bWEXYc0L03QtoENQz4j8.jpg,,,,,
881,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,20,9xlhhh,github.com,[P] SOP-Generator : A simple LSTM based Statement of Purpose Generator for grad school.,https://www.reddit.com/r/MachineLearning/comments/9xlhhh/p_sopgenerator_a_simple_lstm_based_statement_of/,cbsudux,1542368881,,0,1,False,https://b.thumbs.redditmedia.com/_xFbISMy_gt38sLDhEMToKB0S69titoA-AqjHZgAvsw.jpg,,,,,
882,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,20,9xlht7,self.MachineLearning,"""[Project]"": Graph-based notebook for data scientists and researchers",https://www.reddit.com/r/MachineLearning/comments/9xlht7/project_graphbased_notebook_for_data_scientists/,giulioungaretti,1542368975,"We know that data scientist  spend a lot of their time selecting and transforming features at training time and often lose time documenting.

By storing everything in a graph-based notebook, amie lets you keep high model development velocity, without losing the overview. We integrate with your existing machine learning stack  and let you track parameters, metrics, notes and figures! 

You can read more about a use case [here](https://medium.com/amie-ai/amie-use-case-data-driven-winemaking-62a7570dae8b)  and check the app out at [amie.ai .](http://amie.ai)

We would love to hear the feedback from you redditors!

&amp;#x200B;",2,1,False,self,,,,,
883,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,21,9xllmb,self.MachineLearning,[D] How to submit paper to arxiv.stat.ML?,https://www.reddit.com/r/MachineLearning/comments/9xllmb/d_how_to_submit_paper_to_arxivstatml/,cbsudux,1542370002,,0,1,False,self,,,,,
884,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,21,9xlnb0,self.MachineLearning,Looking for suggestions to make a customer mood prediction model,https://www.reddit.com/r/MachineLearning/comments/9xlnb0/looking_for_suggestions_to_make_a_customer_mood/,navan00,1542370439,[removed],0,1,False,self,,,,,
885,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,21,9xlohw,eleks.com,3 reasons why Machine Learning should be at the heart of your cybersecurity strategy.,https://www.reddit.com/r/MachineLearning/comments/9xlohw/3_reasons_why_machine_learning_should_be_at_the/,Victor_Stakh,1542370737,,0,1,False,https://b.thumbs.redditmedia.com/2-pRLWffddZLI8Lm8gwrukE9liXlxT8yvQDWxKj76kA.jpg,,,,,
886,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,21,9xls1b,self.MachineLearning,Automatically Photo-Decoration Powered by ML,https://www.reddit.com/r/MachineLearning/comments/9xls1b/automatically_photodecoration_powered_by_ml/,crazypoint,1542371663,[removed],0,1,False,https://b.thumbs.redditmedia.com/ETecqVd__eglIMK3Cvg5YIBI2HDwba53pl1SCEPI-1c.jpg,,,,,
887,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,22,9xm2ne,self.MachineLearning,Help me Hack diabetes,https://www.reddit.com/r/MachineLearning/comments/9xm2ne/help_me_hack_diabetes/,Shurim4,1542374219,[removed],0,1,False,self,,,,,
888,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,22,9xm3no,self.MachineLearning,Cheap GPU Cloud for NLP training,https://www.reddit.com/r/MachineLearning/comments/9xm3no/cheap_gpu_cloud_for_nlp_training/,matus_pikuliak,1542374449,[removed],0,1,False,self,,,,,
889,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,22,9xm43p,self.MachineLearning,How does recognition based on gait analysis(walking pattern) performs in real world scenarios?,https://www.reddit.com/r/MachineLearning/comments/9xm43p/how_does_recognition_based_on_gait/,RavlaAlvar,1542374554,[removed],0,1,False,self,,,,,
890,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,22,9xmc3t,self.MachineLearning,"[P] A couple of us at Wayfair recently open-sourced our in-house package for uplift modeling, pylift. If you're trying to prevent churn, apply targeted treatments (e.g. ads/coupons), or even select between a small number of personalizations, this can help!",https://www.reddit.com/r/MachineLearning/comments/9xmc3t/p_a_couple_of_us_at_wayfair_recently_opensourced/,ryime,1542376374,"**Repo:** [https://github.com/wayfair/pylift](https://github.com/wayfair/pylift)

**Associated blog post:** [https://tech.wayfair.com/2018/10/pylift-a-fast-python-package-for-uplift-modeling/](https://tech.wayfair.com/2018/10/pylift-a-fast-python-package-for-uplift-modeling/)

It's essentially a tool that wraps scikit-learn, xgboost, and the rest of the python DS stack to allow for quick modeling of incremental value. E.g. if you're trying to figure out who to target with ads, this can help you pinpoint customers that are not incremental (or even negatively incremental) and avoid wasting/losing money on them. You'll find it's a lot faster than existing packages, allowing for things like hyperparameter tuning, and it incorporates some small (but important!) improvements over traditional evaluation metrics. 

I'd be happy to answer any questions about it, and from a more selfish point, I'd love to hear if any of you are using uplift modeling... It's a relatively new field, and from my limited experience, it seems the community is vibrant, but relatively small! ",3,0,False,self,,,,,
891,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,23,9xme7t,ca.pcpartpicker.com,Building a workstation for deep learning. Anything I should change?,https://www.reddit.com/r/MachineLearning/comments/9xme7t/building_a_workstation_for_deep_learning_anything/,that_marty_guy,1542376831,,0,1,False,https://b.thumbs.redditmedia.com/9MUDCaHfEEoLOP_YTMfkmzPm2ukuZD3VR8uT3V-Hm5Q.jpg,,,,,
892,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,23,9xmk4e,self.MachineLearning,Understanding optimization in deep learning by analyzing trajectories of gradient descent,https://www.reddit.com/r/MachineLearning/comments/9xmk4e/understanding_optimization_in_deep_learning_by/,[deleted],1542378029,,0,1,False,default,,,,,
893,MachineLearning,t5_2r3gv,2018-11-16,2018,11,16,23,9xmsg8,medium.com,aiweirdness.com (Janelle Shane) on creating dungeons and dragons spells with textRNN,https://www.reddit.com/r/MachineLearning/comments/9xmsg8/aiweirdnesscom_janelle_shane_on_creating_dungeons/,annsp,1542379781,,0,1,False,https://a.thumbs.redditmedia.com/J7SYzALciKjVhBOQ3YTIf4Yb6eQoulBPdtWegklO4d4.jpg,,,,,
894,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xn62u,medium.com,Breaking news: Head of R&amp;D Jia Li Leaves Google Cloud AI,https://www.reddit.com/r/MachineLearning/comments/9xn62u/breaking_news_head_of_rd_jia_li_leaves_google/,Yuqing7,1542382431,,0,1,False,https://b.thumbs.redditmedia.com/rmC-vob1WolSbFu7Y4gZ5mAb_WX7i4AJqQ-Zh_4QuMM.jpg,,,,,
895,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xn6w6,gengo.ai,How much training data do you need?,https://www.reddit.com/r/MachineLearning/comments/9xn6w6/how_much_training_data_do_you_need/,reimmoriks,1542382588,,0,1,False,https://a.thumbs.redditmedia.com/jBovuheHpVu329DT7UtvRaKDnCEFwO4RfrbhuHvCmu8.jpg,,,,,
896,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xn804,self.MachineLearning,[question] Determining probability distributions with RBMs,https://www.reddit.com/r/MachineLearning/comments/9xn804/question_determining_probability_distributions/,SeuChiko,1542382796,[removed],0,1,False,self,,,,,
897,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xn8fs,datascienceplus.com,Machine Learning in Excel with Python,https://www.reddit.com/r/MachineLearning/comments/9xn8fs/machine_learning_in_excel_with_python/,tony_roberts,1542382875,,0,1,False,https://b.thumbs.redditmedia.com/H0iTcW0s9StrCzUiq0h7sWP_MK0gY-5oDUlay23zpRY.jpg,,,,,
898,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xn8g3,self.MachineLearning,Default parameters vs Parameter tuning,https://www.reddit.com/r/MachineLearning/comments/9xn8g3/default_parameters_vs_parameter_tuning/,operman18,1542382876,[removed],0,1,False,self,,,,,
899,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xn94x,arxiv.org,New SOTA results in Learning data augmentation policies using augmented random search,https://www.reddit.com/r/MachineLearning/comments/9xn94x/new_sota_results_in_learning_data_augmentation/,ml1978,1542383006,,0,1,False,default,,,,,
900,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xnbo7,arxiv.org,"[R] Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search",https://www.reddit.com/r/MachineLearning/comments/9xnbo7/r_woulda_coulda_shoulda_counterfactuallyguided/,Kaixhin,1542383468,,4,1,False,default,,,,,
901,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,0,9xnbvj,arxiv.org,[R] Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation,https://www.reddit.com/r/MachineLearning/comments/9xnbvj/r_multiview_silhouette_and_depth_decomposition/,MRLMCGILLER,1542383503,,1,1,False,default,,,,,
902,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,1,9xng01,self.MachineLearning,[P] The Hundred-Page Machine Learning Book,https://www.reddit.com/r/MachineLearning/comments/9xng01/p_the_hundredpage_machine_learning_book/,RudyWurlitzer,1542384264,"I'm writing The Hundred-Page Machine Learning Book. The first five chapters are already available on the book's [companion website](http://themlbook.com). The book will cover both unsupervised and supervised learning, including neural networks. The most important (for understanding ML) questions from computer science, math and statistics will be explained formally, via examples and by providing an intuition. Most illustrations are created algorithmically; the code and data used to generate them will be available on the website.

The goal is to write a bite-size book anyone with basic math knowledge could read and understand during a weekend.

If you would like to proofread some chapters, don't hesitate to contact me. I will mention in the book the names of those who helped to improve it.",53,1,False,self,,,,,
903,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,1,9xnguw,self.MachineLearning,Aggregating Documents represented by Topics (LDA),https://www.reddit.com/r/MachineLearning/comments/9xnguw/aggregating_documents_represented_by_topics_lda/,JTchosen,1542384416,[removed],0,1,False,self,,,,,
904,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,1,9xnk0z,arxiv.org,[R] [1811.03962] A Convergence Theory for Deep Learning via Over-Parameterization,https://www.reddit.com/r/MachineLearning/comments/9xnk0z/r_181103962_a_convergence_theory_for_deep/,bobchennan,1542384981,,3,1,False,default,,,,,
905,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,1,9xnla9,self.MachineLearning,How can I insert a layer with a particular loss function into a DNN with another loss/objective?,https://www.reddit.com/r/MachineLearning/comments/9xnla9/how_can_i_insert_a_layer_with_a_particular_loss/,KnownAd,1542385207,[removed],0,1,False,self,,,,,
906,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,1,9xnom3,self.MachineLearning,[D] How can I insert a layer with a particular loss function into a DNN with another loss/objective?,https://www.reddit.com/r/MachineLearning/comments/9xnom3/d_how_can_i_insert_a_layer_with_a_particular_loss/,KnownAd,1542385828,"Sorry for this nave question. I'm having a hard time trying to wrap my head around this.

Suppose I have a simple linear layer that implements a custom loss function $L_1$. For illustration purposes, let's consider that it is the PCA loss, to have something more concrete. Now, consider that we have a particular gradient for its weights $W$ given the loss. In this setting, I can optimize this loss directly and learn PCA through SGD as a stand-alone layer (as in the Oja's method).

However, I was wondering, if I wanted to insert this layer into a classic CNN setup with a cross-entropy loss $L_2$, how would I combine these two losses? Can I just add the feed-forward layer after another fully connected layer and backpropagate its specific $W$ gradients to the previous layers? Or, should I do something similar to multi-task learning, i.e., $L = L_1 + L_2$?

[Here is an attempt to illustrate this](https://i.imgur.com/COBut7j.jpg).

As loss $L_1$ wants to learn a particular set of weights for a feed-forward layer and cross-entropy $L_2$ measures the classification performance, maybe $L_1$ should be a regularization term then, applied only to the weights in the layers of interest.",4,1,False,self,,,,,
907,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,2,9xnyxb,blog.varunajayasiri.com,[P] Organize machine learning experiments,https://www.reddit.com/r/MachineLearning/comments/9xnyxb/p_organize_machine_learning_experiments/,mlvpj,1542387695,,0,1,False,default,,,,,
908,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,2,9xnz3t,youtube.com,Creating a self-driving car simulator 2019 Neural Networks and Genetic Algorithm Tutorial 1,https://www.reddit.com/r/MachineLearning/comments/9xnz3t/creating_a_selfdriving_car_simulator_2019_neural/,DevTechRetopall,1542387717,,1,1,False,https://b.thumbs.redditmedia.com/olqichGmpTryW770oMa2ybomoA6wyZleOyjgpFf7P7M.jpg,,,,,
909,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,2,9xo1k9,self.MachineLearning,Finish Him! Kicking Mortal Kombats Kano With AI,https://www.reddit.com/r/MachineLearning/comments/9xo1k9/finish_him_kicking_mortal_kombats_kano_with_ai/,FayeCCC,1542388164,[removed],0,1,False,self,,,,,
910,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,2,9xoa5r,self.MachineLearning,Take my 2-minute survey on Image Classification and enter a drawing for one of two $25 Amazon gift cards!,https://www.reddit.com/r/MachineLearning/comments/9xoa5r/take_my_2minute_survey_on_image_classification/,Ergjunkie,1542389701,"I am conducting research relating to preferences in Image Classification tools for a class.


Complete the survey [here](https://goo.gl/forms/96FMZiU5fxXPPJy12) and enter your Reddit username or email address to be entered into the drawing.",0,1,False,self,,,,,
911,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,2,9xobu0,arxiv.org,[R] Learning to play Atari games without rewards by combining human preferences and expert demonstrations (DeepMind &amp; OpenAI Collaboration),https://www.reddit.com/r/MachineLearning/comments/9xobu0/r_learning_to_play_atari_games_without_rewards_by/,Tivra,1542389982,,6,1,False,default,,,,,
912,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,2,9xod01,self.MachineLearning,What are the common or SOTA methods to make CNNs like resnet/densnet robust to noisy images?,https://www.reddit.com/r/MachineLearning/comments/9xod01/what_are_the_common_or_sota_methods_to_make_cnns/,weelamb,1542390189,[removed],0,1,False,self,,,,,
913,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,2,9xohnf,self.Futurology,[crosspost - AMA in r/futurology] Im roboticist Rodney Brooks and have spent my career thinking about how artificial and intelligent systems should interact with humans. Ask Me Anything!,https://www.reddit.com/r/MachineLearning/comments/9xohnf/crosspost_ama_in_rfuturology_im_roboticist_rodney/,Chtorrr,1542391034,,0,1,False,https://b.thumbs.redditmedia.com/wt51Nv6FEJkPRdzE3r3sMD3_OszdJ1wCQW3rE3i2-2g.jpg,,,,,
914,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,3,9xomaq,reddit.com,[crosspost - AMA in r/futurology] Im roboticist Rodney Brooks and have spent my career thinking about how artificial and intelligent systems should interact with humans. Ask Me Anything!,https://www.reddit.com/r/MachineLearning/comments/9xomaq/crosspost_ama_in_rfuturology_im_roboticist_rodney/,Chtorrr,1542391846,,0,1,False,https://b.thumbs.redditmedia.com/wt51Nv6FEJkPRdzE3r3sMD3_OszdJ1wCQW3rE3i2-2g.jpg,,,,,
915,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,3,9xooqn,self.MachineLearning,Coming up with research/applied project problems?,https://www.reddit.com/r/MachineLearning/comments/9xooqn/coming_up_with_researchapplied_project_problems/,AlarmedHand,1542392277,[removed],0,1,False,self,,,,,
916,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,3,9xovxl,self.MachineLearning,[D] What are the common or SOTA methods to make CNNs like resnet/densenet robust to noisy images?,https://www.reddit.com/r/MachineLearning/comments/9xovxl/d_what_are_the_common_or_sota_methods_to_make/,weelamb,1542393603,"I'm working on a custom CNN (with resnet and densenet properties). A lot of my data happens to be noisy. The noise is mostly blurred images, and also hazy/foggy. I think the edges aren't as clear to learn from the data.

&amp;#x200B;

Without considering data augmentation, what are some of the common or SOTA methods that make a CNN more robust to this noisy data? I've made the network large and wide (millions of parameters). I'm using dropout, BN, and in my SGD optimizer I'm using weight decay, but it still has a lot of trouble with images that contain noise. It overfits easily to the images that are less noisy.",13,1,False,self,,,,,
917,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,3,9xp1hk,self.MachineLearning,Wanted: Tips/advice as I start a Machine Learning Engineer job at a startup,https://www.reddit.com/r/MachineLearning/comments/9xp1hk/wanted_tipsadvice_as_i_start_a_machine_learning/,bigrichardenergy,1542394623,[removed],0,1,False,self,,,,,
918,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,4,9xp2um,self.MachineLearning,Major in statistics or computer science if I want to go to grad school for ML?,https://www.reddit.com/r/MachineLearning/comments/9xp2um/major_in_statistics_or_computer_science_if_i_want/,searchingundergrad,1542394877,"Does it even matter? I haven't been able to get a good answer to this question, any advice is appreciated!",0,1,False,self,,,,,
919,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,4,9xp6u3,self.MachineLearning,[D] Major in Statistics or Computer Science if I want to go to grad school for ML?,https://www.reddit.com/r/MachineLearning/comments/9xp6u3/d_major_in_statistics_or_computer_science_if_i/,searchingundergrad,1542395601,"Does it even matter? I haven't been able to get much advice on this, so any help is appreciated!",8,1,False,self,,,,,
920,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,4,9xp89n,self.MachineLearning,[P] Procedural Black-Box Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/9xp89n/p_procedural_blackbox_adversarial_examples/,_darkbear,1542395885,"GitHub with interactive Python notebook: [https://github.com/kenny-co/procedural-advml](https://github.com/kenny-co/procedural-advml)

Paper: [https://arxiv.org/abs/1810.00470](https://arxiv.org/abs/1810.00470)

&amp;#x200B;

Inspired by patterns from existing white-box adversarial attacks, we procedurally generate adversarial examples using Perlin noise having **only query access** to the target model. Popular ImageNet classifiers are shown to be susceptible to these procedural noise patterns, which is concerning since these noise patterns are easy to generate.

&amp;#x200B;

Try it yourself on the Python notebook! What do you think these procedural noise patterns are doing?",6,1,False,self,,,,,
921,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,4,9xpetw,self.MachineLearning,Dynamical Machine Learning: Bayesian exact recursive estimation,https://www.reddit.com/r/MachineLearning/comments/9xpetw/dynamical_machine_learning_bayesian_exact/,andrea_manero,1542397119,[removed],0,1,False,self,,,,,
922,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,4,9xpjtj,reddit.com,[News],https://www.reddit.com/r/MachineLearning/comments/9xpjtj/news/,disposableme99,1542398072,,0,1,False,default,,,,,
923,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,4,9xpknw,self.MachineLearning,High Dimensional Regression Dataset,https://www.reddit.com/r/MachineLearning/comments/9xpknw/high_dimensional_regression_dataset/,ItAllSpans,1542398231,[removed],0,1,False,self,,,,,
924,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,5,9xpnw0,neurovenge.antonomase.fr,[R] The revenge of neurons: history and scientometrics,https://www.reddit.com/r/MachineLearning/comments/9xpnw0/r_the_revenge_of_neurons_history_and/,antmaz,1542398773,,1,1,False,default,,,,,
925,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,5,9xptsq,self.MachineLearning,When do CVPR tickets go on sale?,https://www.reddit.com/r/MachineLearning/comments/9xptsq/when_do_cvpr_tickets_go_on_sale/,ZeppelinYanks,1542399830,[removed],0,1,False,self,,,,,
926,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,5,9xpzsk,self.MachineLearning,Simple scratch written matrix based Artificial Neural Network in Java,https://www.reddit.com/r/MachineLearning/comments/9xpzsk/simple_scratch_written_matrix_based_artificial/,ProgrammingGodJordan,1542400923,[removed],0,1,False,self,,,,,
927,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,5,9xq515,self.MachineLearning,[r] Supervised and Unsupervised learning simplified!,https://www.reddit.com/r/MachineLearning/comments/9xq515/r_supervised_and_unsupervised_learning_simplified/,BlairRoslyn,1542401863,[removed],0,1,False,self,,,,,
928,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,6,9xqfmd,self.MachineLearning,[R] NIPS changes to NeurIPS,https://www.reddit.com/r/MachineLearning/comments/9xqfmd/r_nips_changes_to_neurips/,FirstTimeResearcher,1542403845,"The website has moved as well:

https://nips.cc/

to

https://neurips.cc/

Very happy to hear this. Addresses many of the issues raised with the old name and keeps many of the important aspects of the original conference.",8,1,False,self,,,,,
929,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,7,9xqtmf,self.MachineLearning,Any descriptive study on training (improving) attention based model by Google?(or any personal exp),https://www.reddit.com/r/MachineLearning/comments/9xqtmf/any_descriptive_study_on_training_improving/,lcukerd,1542406593,[removed],0,1,False,self,,,,,
930,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,8,9xrfal,self.MachineLearning,tensor on apache spark,https://www.reddit.com/r/MachineLearning/comments/9xrfal/tensor_on_apache_spark/,ice109,1542411083,[removed],0,1,False,self,,,,,
931,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,8,9xril0,self.MachineLearning,NLP classification problem using TensorFlow sentence encoder and other parameters as features,https://www.reddit.com/r/MachineLearning/comments/9xril0/nlp_classification_problem_using_tensorflow/,sharonioz,1542411806,[removed],0,1,False,self,,,,,
932,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,9,9xroxj,self.MachineLearning,Telco ML problem?,https://www.reddit.com/r/MachineLearning/comments/9xroxj/telco_ml_problem/,Heineb27,1542413183,[removed],0,1,False,self,,,,,
933,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,9,9xrysh,self.MachineLearning,Widely used CNN network,https://www.reddit.com/r/MachineLearning/comments/9xrysh/widely_used_cnn_network/,ssuryanarayanan,1542415353,[removed],0,1,False,self,,,,,
934,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,12,9xt8f5,youtube.com,Install Package - Google Colab Tutorial,https://www.reddit.com/r/MachineLearning/comments/9xt8f5/install_package_google_colab_tutorial/,jeffxu999,1542426223,,0,1,False,https://b.thumbs.redditmedia.com/XKp-wnATdrWH5B4FQjNx1_BGp7jTMXGgpPLD-f5_ykA.jpg,,,,,
935,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,13,9xthob,self.MachineLearning,Resources for audio-related machine learning techniques,https://www.reddit.com/r/MachineLearning/comments/9xthob/resources_for_audiorelated_machine_learning/,Hamush,1542428613,"Hi everyone, I am interested in articles, StackOverflow posts, papers, any resources really, that can help me understand how to apply machine learning techniques to audio files for whatever reason. Im mostly looking for some general material, just to get exposed to methods, techniques, and new research that can help with anything audio related, such as some sort of editing, changing pitches/tones in music, amplifying or even clarifying audio.

Basically, my goal is to get a better idea of what kind of strategies are used as pre-processing for audio files, what kind of algorithms, and how exactly would you turn and audio file into an input into an ML algorithm. Thanks for any advice/tips, greatly appreciated.",0,1,False,self,,,,,
936,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,15,9xu55e,self.MachineLearning,"Now, lets take a look the key differences:",https://www.reddit.com/r/MachineLearning/comments/9xu55e/now_lets_take_a_look_the_key_differences/,Learntek12,1542435023,[removed],0,1,False,https://a.thumbs.redditmedia.com/VMuPxXO-16JIZ3DP03QYTE5cbnxZWq9JCFHOIvt7Gc4.jpg,,,,,
937,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,15,9xuas5,i.redd.it,"How can you extract the footprints of the building rooftops from an satellite image using computer vision, machine learning and deep learning. My aim is to just extract only the buildings from the image. What should be the approach for this? The Image shows the desired output which is to be achieved",https://www.reddit.com/r/MachineLearning/comments/9xuas5/how_can_you_extract_the_footprints_of_the/,girishpillai17,1542436730,,0,1,False,default,,,,,
938,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,16,9xufid,self.MachineLearning,People Detection and Tracking (SSD + Kalman Filter ),https://www.reddit.com/r/MachineLearning/comments/9xufid/people_detection_and_tracking_ssd_kalman_filter/,ambakick,1542438239,"[https://www.hackster.io/ambakick/people-detection-and-tracking-2869f3](https://www.hackster.io/ambakick/people-detection-and-tracking-2869f3)

&amp;#x200B;

 The project focuses on a real-time robust human detection and tracking system for video surveillance that can be used in varying situations. ",0,1,False,self,,,,,
939,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,16,9xui3c,self.MachineLearning,Where is the learned data stored after training?,https://www.reddit.com/r/MachineLearning/comments/9xui3c/where_is_the_learned_data_stored_after_training/,Alpha_Calculi2,1542439036,[removed],0,1,False,self,,,,,
940,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,16,9xuicq,self.MachineLearning,Sentiment analysis using Arabic data,https://www.reddit.com/r/MachineLearning/comments/9xuicq/sentiment_analysis_using_arabic_data/,mgalalen,1542439118,[removed],0,1,False,self,,,,,
941,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,16,9xukgy,self.MachineLearning,help me keras cnn overfitting problem,https://www.reddit.com/r/MachineLearning/comments/9xukgy/help_me_keras_cnn_overfitting_problem/,GoBacksIn,1542439809,"nDropout = 0.25  
model = Sequential()  
model.add(Conv2D(32, (3, 3), padding='same', input\_shape=self.x\_train.shape\[1:\], activation='relu'))  
model.add(Conv2D(32, (3, 3), activation='relu'))  
model.add(MaxPooling2D(pool\_size=(2, 2)))  
model.add(Dropout(nDropout))  
nDropout = 0.5  
model.add(Flatten())  
model.add(Dense(128, activation='relu', kernel\_initializer='glorot\_uniform', bias\_initializer='glorot\_uniform'))  
model.add(Dropout(nDropout))  
model.add(Dense(128, activation='relu', kernel\_initializer='glorot\_uniform', bias\_initializer='glorot\_uniform'))  
model.add(Dropout(nDropout))  


model.add(Dense(len(2)))  
model.add(Activation('softmax'))  
model.compile(loss='categorical\_crossentropy', optimizer='adam', metrics=\['accuracy'\])  
return model

&amp;#x200B;

&amp;#x200B;

this code result is loss: 0.2580 - acc: 0.8897 - val\_loss: 0.7049 - val\_acc: 0.7135

&amp;#x200B;

val\_loss is too big how cain i solve this?

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
942,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,19,9xvep7,self.MachineLearning,A High Double-Digit CAGR Projected for Cloud Communication Platform Market 2022,https://www.reddit.com/r/MachineLearning/comments/9xvep7/a_high_doubledigit_cagr_projected_for_cloud/,amar_bir,1542450441,[removed],1,1,False,self,,,,,
943,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,19,9xvg9z,self.MachineLearning,NIPS2018 Workshops: Printing your posters,https://www.reddit.com/r/MachineLearning/comments/9xvg9z/nips2018_workshops_printing_your_posters/,lepton99,1542451000,[removed],0,1,False,self,,,,,
944,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,20,9xvrrm,medium.com,HMTL: Multi-Task learning for state of the art NLP,https://www.reddit.com/r/MachineLearning/comments/9xvrrm/hmtl_multitask_learning_for_state_of_the_art_nlp/,omarsar,1542454985,,0,1,False,https://b.thumbs.redditmedia.com/c4_-Fyobd_HRl-pksXMBnrbWlr9sxYBFwoUbmQkbcns.jpg,,,,,
945,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,21,9xw5mz,self.MachineLearning,Understanding machine learning: Do we need machine learning at all?,https://www.reddit.com/r/MachineLearning/comments/9xw5mz/understanding_machine_learning_do_we_need_machine/,andrea_manero,1542459311,[removed],0,1,False,self,,,,,
946,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,22,9xw8al,self.MachineLearning,History as a guide to IoT growth trajectory,https://www.reddit.com/r/MachineLearning/comments/9xw8al/history_as_a_guide_to_iot_growth_trajectory/,andrea_manero,1542460023,http://www.datasciencecentral.com/profiles/blogs/history-as-a-guide-to-iot-growth-trajectory,0,1,False,self,,,,,
947,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,22,9xwdgm,community.singularitynet.io,"The Real Vision of Satoshi, Pay Attention",https://www.reddit.com/r/MachineLearning/comments/9xwdgm/the_real_vision_of_satoshi_pay_attention/,KatMalcolm,1542461379,,0,1,False,https://b.thumbs.redditmedia.com/EySnllQR5COpWz4nRNJwflH_BL9HwtGZ61IfAs7O0OM.jpg,,,,,
948,MachineLearning,t5_2r3gv,2018-11-17,2018,11,17,23,9xwwpd,self.MachineLearning,[D] How do you structure your PyTorch deep learning Implementations/Projects/PythonLibs,https://www.reddit.com/r/MachineLearning/comments/9xwwpd/d_how_do_you_structure_your_pytorch_deep_learning/,__Julia,1542466229,"Hi,
In the data science community, I have seen a wide adoption of this project structure https://github.com/drivendata/cookiecutter-data-science. However, I am still struggling to find a unified way to structure ML experiments that save readers time to understand the structure of the project. ",20,1,False,self,,,,,
949,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,1,9xxvv5,self.MachineLearning,[D] Ideas to apply machine learning in project,https://www.reddit.com/r/MachineLearning/comments/9xxvv5/d_ideas_to_apply_machine_learning_in_project/,CSGOvelocity,1542473658,"My project is basically uber but for ambulances. In India there is an acute shortage of ambulances as well as the ones that are available are not upto standards. So I thought of a SOS app that you can use to call private ambulances.

It has 2 apps one user side and one driver side. The user just presses an SOS button and the nearest ambulance that is up for duty will be pinged and will be shown the optimal route to reach destination.

Assuming the kind of data required for the application of machine learning in this project is available what can be some aspects where ML can be used ?

  
I thought of using one-shot learning using Siamese networks to detect presence of driver and attendant in the ambulance and take the ambulance out of queue if absence of driver or attendant is detected.

  
",6,1,False,self,,,,,
950,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,2,9xy4j0,self.MachineLearning,[P] Style2Paints V4 finally released: Help artists in standard human coloring workflow!,https://www.reddit.com/r/MachineLearning/comments/9xy4j0/p_style2paints_v4_finally_released_help_artists/,paintstransfer,1542475334,"Hi reddit MachineLearning! We are very excited to release style2paints V4, a significant milestone of AI-driven line art coloring tools.

&amp;#x200B;

The idea of style2paints V4 is very simple: coloring in standard human workflow. We found standard human coloring workflow is similar to:

&amp;#x200B;

Create line arts  &gt;&gt;  fill flat colors  &gt;&gt;   make color gradients  &gt;&gt;   shade the painting

&amp;#x200B;

We designed a white-box system, combining of three separated pipelines to do color flattening, color gradient making, and rendering.

&amp;#x200B;

\* Strength

&amp;#x200B;

\+ Highest Quality. In our user study of 53 sketches and 6 users, style2paints V4 defeats all previous style2paints and all previous paintschainer in 100% cases.

&amp;#x200B;

\+ Wide-spread Use Cases. The outputs of style2paints V4 are layers. Artists can select what they need. Style2paints V4 is the first AI-driven system to achieve this.

&amp;#x200B;

\+ In the wild robustness. All presented results are generated on real-life line arts from artists and Twitter users from different countries. Results from real-life twitter users are more convincing than anything others.

&amp;#x200B;

\* Weakness

&amp;#x200B;

\- Not able to handle full-page manga. Because models are trained on line drawings, our system fails to colorize most full-page manga. Please use MangaCraft to colorize manga.

&amp;#x200B;

\- Relatively weak in western style line drawings. 

&amp;#x200B;

\- Still not able to colorize very complex line drawings.

&amp;#x200B;

GitHub: [https://github.com/lllyasviel/style2paints](https://github.com/lllyasviel/style2paints)

&amp;#x200B;

(Best to browse this page on a desktop PC.)

&amp;#x200B;

\* online demo is available in this GitHub page. Not forget to share your results on the hot twitter hashtag #style2paints :)

&amp;#x200B;",18,1,False,self,,,,,
951,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,2,9xy9jn,oreilly.com,Managing risk in machine learning,https://www.reddit.com/r/MachineLearning/comments/9xy9jn/managing_risk_in_machine_learning/,gradientflow,1542476327,,0,1,False,default,,,,,
952,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,3,9xygmj,self.MachineLearning,How does Leela Chess Zero work?,https://www.reddit.com/r/MachineLearning/comments/9xygmj/how_does_leela_chess_zero_work/,daredevildas,1542477732,How does the distributed reinforcement learning in leela chess zero work?,0,1,False,self,,,,,
953,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,3,9xyrsg,self.MachineLearning,"Is this short video of a triple inverted pendulum task likely ""legit""?",https://www.reddit.com/r/MachineLearning/comments/9xyrsg/is_this_short_video_of_a_triple_inverted_pendulum/,2cow,1542479857,[removed],0,1,False,self,,,,,
954,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,3,9xyw9n,arxiv.org,[R] Understanding Back-Translation at Scale,https://www.reddit.com/r/MachineLearning/comments/9xyw9n/r_understanding_backtranslation_at_scale/,HigherTopoi,1542480706,,0,1,False,default,,,,,
955,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,3,9xyyfd,self.MachineLearning,[D] Would a replacing linear regression with a EMA styled averaging of the inputs be a good idea?,https://www.reddit.com/r/MachineLearning/comments/9xyyfd/d_would_a_replacing_linear_regression_with_a_ema/,abstractcontrol,1542481128,"Recently as the prelude to trying out an input dependent gradient normalization scheme, I trained a linear net to predict the outputs of an RNN layer that uses LN+relu as the activation and it is just dying. I was surprised to see it explode so readily, and even after I added KFAC's front and back to the linear net it still lead to it predicting negative values at some points.

This definitely came as a surprise to me and is making me wonder whether I am doing the right thing by using the same squared error to predict values in a RL setting.

I am thinking that instead of using the squared error to predict the scale what I should to is take the target `rnn_out` and feed it directly as a gradient of a linear net. It would be a Hebbian learning inspired weight update, but instead of the net being updated like `in * out`, I'd replace it with `in * (rnn_out / L2_norm_in)`.

Of course this would lead to weights to grow without bound, so on every update I could decay the weights by the learning rate, meaning the final update would be something like: `W = (1-learning_rate) * W + learning_rate * in * (rnn_out / L2_norm_in)`.

Does this update scheme have some flaws that I am not aware of?

Since it would be invariant to the order of updates and non-divergent, why have I never seen this used up to now? Would it be equivalent to some kind of linear regression with a particular kind of regularization?",2,1,False,self,,,,,
956,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,4,9xzftb,self.MachineLearning,[D] Updates on Perturbative Neural Networks paper,https://www.reddit.com/r/MachineLearning/comments/9xzftb/d_updates_on_perturbative_neural_networks_paper/,alexmlamb,1542484661,"Any updates on this paper, especially issues related to reproducibility and the concepts in it?  ",2,1,False,self,,,,,
957,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,5,9xzv56,self.MachineLearning,[D] 5100 submissions to CVPR this year,https://www.reddit.com/r/MachineLearning/comments/9xzv56/d_5100_submissions_to_cvpr_this_year/,DoorsofPerceptron,1542487808,Have we reached peak ML yet? When does exponential growth turn into a sigmoid?,14,1,False,self,,,,,
958,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,6,9y03fl,self.MachineLearning,Image classfier - best algo to identify entire image.,https://www.reddit.com/r/MachineLearning/comments/9y03fl/image_classfier_best_algo_to_identify_entire_image/,miami_charts,1542489524,[removed],0,1,False,self,,,,,
959,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,7,9y0k8s,self.MachineLearning,interpretability of machine learning model,https://www.reddit.com/r/MachineLearning/comments/9y0k8s/interpretability_of_machine_learning_model/,edunuke,1542493023,[removed],0,1,False,self,,,,,
960,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,7,9y0mef,self.MachineLearning,Semi-Introductory level blog post about QA dataset Squad 2.0 and U-NET,https://www.reddit.com/r/MachineLearning/comments/9y0mef/semiintroductory_level_blog_post_about_qa_dataset/,fosa2,1542493495,[removed],0,1,False,self,,,,,
961,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,8,9y11er,self.MachineLearning,[P] Use CheXNet on a single image,https://www.reddit.com/r/MachineLearning/comments/9y11er/p_use_chexnet_on_a_single_image/,CSGOvelocity,1542496731,"This is the model and set of files I want to use : [https://github.com/thtang/CheXNet-with-localization](https://github.com/thtang/CheXNet-with-localization)

How do I modify the code to make the model run on a single image OR what script do I need to write so that I can achieve the same ? 

Please ask for any other information that's required. Thanks",1,1,False,self,,,,,
962,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,10,9y1trn,self.MachineLearning,[R] Advancing to 3D deep neural networks in MRI analysis,https://www.reddit.com/r/MachineLearning/comments/9y1trn/r_advancing_to_3d_deep_neural_networks_in_mri/,ranihorev,1542502964,"Hey,

I wrote a summary on a new Nature [paper](https://www.nature.com/articles/s41598-018-34817-6) by NYU researchers. They analyze 3D MRI images among women, with the goal of recognizing signs of osteoporosis, a disease which causes bone weakness. 

They provide a detailed comparison between 2D and 3D neural networks for medical image recognition and show the benefits of advancing to 3D convolution neural networks (CNNs).

[https://www.lyrn.ai/2018/11/16/advancing-to-3d-deep-neural-networks-in-medical-image-analysis/](https://www.lyrn.ai/2018/11/16/advancing-to-3d-deep-neural-networks-in-medical-image-analysis/)

I'd love to get your feedback! 

&amp;#x200B;",0,1,False,self,,,,,
963,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,10,9y1wuh,self.MachineLearning," ,    .",https://www.reddit.com/r/MachineLearning/comments/9y1wuh/_____/,ilovenarwhales,1542503685,[removed],0,1,False,self,,,,,
964,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,11,9y2dq9,sebastianraschka.com,"""[D] Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning"" (PDF)",https://www.reddit.com/r/MachineLearning/comments/9y2dq9/d_model_evaluation_model_selection_and_algorithm/,seraschka,1542507855,,0,1,False,default,,,,,
965,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,11,9y2gs4,self.MachineLearning,I need help with my career/educational path,https://www.reddit.com/r/MachineLearning/comments/9y2gs4/i_need_help_with_my_careereducational_path/,Astafy,1542508632,[removed],0,1,False,self,,,,,
966,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,11,9y2iyp,self.MachineLearning,"[R] ON RANDOM DEEP AUTOENCODERS: EXACT ASYMPTOTIC ANALYSIS, PHASE TRANSITIONS, AND IMPLICATIONS TO TRAINING",https://www.reddit.com/r/MachineLearning/comments/9y2iyp/r_on_random_deep_autoencoders_exact_asymptotic/,jinpanZe,1542509181,"https://openreview.net/forum?id=HJx54i05tX

TL;DR: Authors study the behavior of weight-tied multilayer vanilla autoencoders under the assumption of random weights. Via an exact characterization in the limit of large dimensions, the analysis reveals interesting phase transition phenomena, and allows training of 200 layer autoencoder.",1,1,False,self,,,,,
967,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,11,9y2k0o,self.MachineLearning,[R] PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees,https://www.reddit.com/r/MachineLearning/comments/9y2k0o/r_pategan_generating_synthetic_data_with/,jinpanZe,1542509471,"https://openreview.net/forum?id=S1zk9iRqF7&amp;noteId=B1x60Mr0aQ

Abstract: Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the state-of-the-art method with respect to this and other notions of synthetic data quality.",0,1,False,self,,,,,
968,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,11,9y2l40,self.MachineLearning,[R] Learning a SAT Solver from Single-Bit Supervision,https://www.reddit.com/r/MachineLearning/comments/9y2l40/r_learning_a_sat_solver_from_singlebit_supervision/,jinpanZe,1542509766,"https://openreview.net/forum?id=HJMC_iA5tm

TL;DR: The authors train a graph network to predict boolean satisfiability and show that it learns to search for solutions, and that the solutions it finds can be decoded from its activations.",6,1,False,self,,,,,
969,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,13,9y37z5,youtube.com,MIT AI: Statistical Learning (Vladimir Vapnik),https://www.reddit.com/r/MachineLearning/comments/9y37z5/mit_ai_statistical_learning_vladimir_vapnik/,eugf_,1542515757,,0,1,False,default,,,,,
970,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,14,9y3j18,self.MachineLearning,"Where, how to set up the random state in Caffe for CNN",https://www.reddit.com/r/MachineLearning/comments/9y3j18/where_how_to_set_up_the_random_state_in_caffe_for/,giggitygigg14,1542518798,[removed],0,1,False,self,,,,,
971,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,14,9y3mqn,self.MachineLearning,ML Linear Regression: Is it okay to change negative values for predictions to 0 (if we know negative outcomes are impossible) before evaluating algorithm accuracy?,https://www.reddit.com/r/MachineLearning/comments/9y3mqn/ml_linear_regression_is_it_okay_to_change/,stats_nerd21,1542519817,[removed],0,1,False,self,,,,,
972,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,15,9y3rw6,self.MachineLearning,"[Discussion] Where, how to set up the random state in Caffe for CNN",https://www.reddit.com/r/MachineLearning/comments/9y3rw6/discussion_where_how_to_set_up_the_random_state/,giggitygigg14,1542521275,"I'm new to Caffe and I am working on the MNIST dataset. 

The weights are being randomly initialized using the Xavier algorithm and I'm not able to reproduce the exact results.

Any leads on how and where to set up the random state?",1,1,False,self,,,,,
973,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,17,9y4dwj,self.MachineLearning,Google AI Residency Chances,https://www.reddit.com/r/MachineLearning/comments/9y4dwj/google_ai_residency_chances/,Teenvan1995,1542528602,[removed],0,1,False,self,,,,,
974,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,18,9y4qij,self.MachineLearning,Raspberry Pi ML drone kit,https://www.reddit.com/r/MachineLearning/comments/9y4qij/raspberry_pi_ml_drone_kit/,Fable67,1542533361,[removed],0,1,False,self,,,,,
975,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,18,9y4t1j,self.MachineLearning,How to train a network for semantic segmentation ?,https://www.reddit.com/r/MachineLearning/comments/9y4t1j/how_to_train_a_network_for_semantic_segmentation/,UpstairsCurrency,1542534269,[removed],0,1,False,self,,,,,
976,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,19,9y4zub,self.MachineLearning,BERT multilingual model - sentiment analysis task,https://www.reddit.com/r/MachineLearning/comments/9y4zub/bert_multilingual_model_sentiment_analysis_task/,mmuhu,1542536686,[removed],0,1,False,self,,,,,
977,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,19,9y536n,self.MachineLearning,[R] Survey about self-driving cars for a Master's thesis,https://www.reddit.com/r/MachineLearning/comments/9y536n/r_survey_about_selfdriving_cars_for_a_masters/,wtf_rainbows,1542537880,"Hi all

&amp;#x200B;

I apologize if this type of post is against the rules. Please remove it if it is.

I am a CS student at NTNU in Norway, currently working on my Master's thesis. My partner and I are exploring a new way of gathering and assembling data for the training of self-driving cars, and have developed a short survey to confirm some of our hypotheses. We would greatly appreciate it if some of you spent 5 minutes to complete the survey!

&amp;#x200B;

The survey is intentionally pretty high-level, as it is being shared with people from all backgrounds.

&amp;#x200B;

Here is the link: [https://goo.gl/forms/tgS86cGB9pCtFBLg2](https://goo.gl/forms/tgS86cGB9pCtFBLg2)

&amp;#x200B;

Thanks!",18,1,False,self,,,,,
978,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,19,9y539i,youtube.com,Detecting fingerspelling with convolutional neural networks,https://www.reddit.com/r/MachineLearning/comments/9y539i/detecting_fingerspelling_with_convolutional/,weeklyWebWisdom,1542537914,,0,1,False,https://a.thumbs.redditmedia.com/ZYOAUTMEoTrPm3aYR0GhTwWU7bRZ6MMWvGUQYCXFDj0.jpg,,,,,
979,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,20,9y56mc,imarticus.org,How Machine Learning Is Saving The Indian Vernacular ?,https://www.reddit.com/r/MachineLearning/comments/9y56mc/how_machine_learning_is_saving_the_indian/,LearnFromImarticus,1542539083,,0,1,False,default,,,,,
980,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,20,9y586p,self.MachineLearning,[D] How to apply DAgger in VizDoom?,https://www.reddit.com/r/MachineLearning/comments/9y586p/d_how_to_apply_dagger_in_vizdoom/,TheJoaquiUser,1542539630,"I'm a curious about imitation learning algorithms and their applications for FPS games, surprisingly I haven't found much on this topic, especially with the resource of using VizDoom. I was just wondering if anyone would know how to apply DAgger in Vizdoom?",3,1,False,self,,,,,
981,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,20,9y58jf,blog.varunajayasiri.com,My library to organize experiments,https://www.reddit.com/r/MachineLearning/comments/9y58jf/my_library_to_organize_experiments/,mlvpj,1542539744,,0,1,False,default,,,,,
982,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,21,9y5hch,self.MachineLearning,[R] Asynchronous Stochastic Gradient Descent with Delay Compensation,https://www.reddit.com/r/MachineLearning/comments/9y5hch/r_asynchronous_stochastic_gradient_descent_with/,aziz_22,1542542699,"Has anyone tried using this optimizer when using multi-gpu training?

If yes, what was the results compared to a simple ASGD? better? worse? or equivalent?

Also, are there some other benchmarking for this type of optimizer?

&amp;#x200B;",0,1,False,self,,,,,
983,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,21,9y5imd,self.MachineLearning,Need a good implementation of attention based hierarchical LSTM.,https://www.reddit.com/r/MachineLearning/comments/9y5imd/need_a_good_implementation_of_attention_based/,prateek0001,1542543106,"Hi, I am working on an NLP project and I am still figuring my way around Tensorflow. If anyone knows of a good open source implementation of Hierarchical LSTM attention-based model it would be really helpful to me.  Preferably in tensorflow or Keras, though a pytorch implementation would also work.",0,1,False,self,,,,,
984,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,22,9y63iy,self.MachineLearning,[D] What are your meta-strategies for getting stuck coding in Tensorflow/Pytorch?,https://www.reddit.com/r/MachineLearning/comments/9y63iy/d_what_are_your_metastrategies_for_getting_stuck/,AdditionalWay,1542549101,"Getting doing ML programming is harder than say webdevelopment. These libraries are relatively new, intricate, and have smaller user bases, so asking on stackoverflow will take longer to get a response, if you get a response at all. ",2,1,False,self,,,,,
985,MachineLearning,t5_2r3gv,2018-11-18,2018,11,18,22,9y64zg,self.MachineLearning,[D] 3 Ways Machine Learning can Revolutionize Industry 4.0,https://www.reddit.com/r/MachineLearning/comments/9y64zg/d_3_ways_machine_learning_can_revolutionize/,seemingly_omniscient,1542549492,[removed],0,1,False,self,,,,,
986,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,0,9y6ya3,youtu.be,[P] Random Forest Tutorial to Predict Chances of a person to get addicted to Alcohol or Substance use in 60 Lines of Python,https://www.reddit.com/r/MachineLearning/comments/9y6ya3/p_random_forest_tutorial_to_predict_chances_of_a/,adap23,1542556080,,0,1,False,https://b.thumbs.redditmedia.com/AnCCiLDV2Ncd1ZGQRGohtu3Qxh524eFQTxhmyRbHvfM.jpg,,,,,
987,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,0,9y6z5x,self.MachineLearning,Question about Keras word2vec implementation (dot product for axis for similarity),https://www.reddit.com/r/MachineLearning/comments/9y6z5x/question_about_keras_word2vec_implementation_dot/,[deleted],1542556267,,0,1,False,default,,,,,
988,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,0,9y705b,self.MachineLearning,TensorFlow on S3: why is it faster than downloading models from s3 and read from disk?,https://www.reddit.com/r/MachineLearning/comments/9y705b/tensorflow_on_s3_why_is_it_faster_than/,marksteve4,1542556472,[removed],0,1,False,self,,,,,
989,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,1,9y73cq,youtube.com,"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier. Hope you guys found it helpful!",https://www.reddit.com/r/MachineLearning/comments/9y73cq/for_me_one_of_the_main_barriers_to_the_world_of/,antaloaalonso,1542557105,,0,1,False,https://b.thumbs.redditmedia.com/vwun5p_gPdiCEPyrR83GlPnAufX3LusqRzYQUUkqIHA.jpg,,,,,
990,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,1,9y7czd,self.MachineLearning,"[R] ICLR 2020 will be in Addis Ababa, Ethiopia",https://www.reddit.com/r/MachineLearning/comments/9y7czd/r_iclr_2020_will_be_in_addis_ababa_ethiopia/,FirstTimeResearcher,1542558998,"Great move to make it easier for an underrepresented community!

Yoshua Bengio announces:

&gt; We could make it easier for people from developing countries to come here. It is a big problem right now. In Europe or the US or Canada it is very difficult for an African researcher to get a visa. Its a lottery, and very often they will use any excuse to refuse access. This is totally unfair. It is already hard for them to do research with little resources, but in addition if they cant have access to the community, I think thats really unfair. As a way to counter some of that, we are going to have the ICLR conference [a major AI conference] in 2020 in Africa. 

Source: https://www.technologyreview.com/s/612434/one-of-the-fathers-of-ai-is-worried-about-its-future/
",120,10,False,self,,,,,
991,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y7lh9,self.MachineLearning,[D] 3 Ways Machine Learning can Revolutionize Industry 4.0,https://www.reddit.com/r/MachineLearning/comments/9y7lh9/d_3_ways_machine_learning_can_revolutionize/,seemingly_omniscient,1542560620," Industry 4.0 will not exist without the use of artificial intelligence or machine learning.

&amp;#x200B;

You can read my complete article here:

[https://www.aisoma.de/3-ways-machine-learning-can-revolutionize-industry-4-0/](https://www.aisoma.de/3-ways-machine-learning-can-revolutionize-industry-4-0/)

https://i.redd.it/df383nxwe4z11.jpg",1,0,False,https://a.thumbs.redditmedia.com/5fT7sR4naHvweulFXPvTUpT7oGb_z97BM8VlMqGXgq0.jpg,,,,,
992,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y7m49,resoomer.com,Resoomer | Rsumeur pour faire un rsum de texte automatique en ligne,https://www.reddit.com/r/MachineLearning/comments/9y7m49/resoomer_rsumeur_pour_faire_un_rsum_de_texte/,Viddomake,1542560739,,0,1,False,https://b.thumbs.redditmedia.com/XAyTmBTRB6c9ToluQUZ5OP1z-UW0u6U1h-BKuJnFvbk.jpg,,,,,
993,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y7psl,self.MachineLearning,[D] What kind of reinforcement learning algorithm would you recommend for training AI for a strategy game like Risk or Axis&amp;Allies?,https://www.reddit.com/r/MachineLearning/comments/9y7psl/d_what_kind_of_reinforcement_learning_algorithm/,anti-hero,1542561435,"The challange being multiple stages of the game (buy,  move/attack, place) and dice rolls.",9,1,False,self,,,,,
994,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y7q0e,self.MachineLearning,[D] Why do so many jobs require a PhD?,https://www.reddit.com/r/MachineLearning/comments/9y7q0e/d_why_do_so_many_jobs_require_a_phd/,urlwolf,1542561475,"I did a quick scan on DS/machine learning jobs in SF and many companies ask for PhDs. This is problematic in many ways. Do so many companies (&gt;3300) do research in deep learning? If not, do they assume PhDs are better for building applications? from what I understand, the number of PhDs in ML per year is not growing. In fact, it must be shrinking as all the luminaries (Hinton, Bengio, Le Cun etc) work somewhat for the industry. Uber raided CMU's talent, etc. This could have serious consequences, like wild competition for a small pool of candidates while other perfectly capable people don't get called. 

&amp;#x200B;

Disclaimer: I have a PhD. Unless the company writing the ad is doing serious research, I'm not sure it's the best criterion to filter people for what amounts to be engineering jobs. ",82,4,False,self,,,,,
995,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y7rop,self.MachineLearning,[D] NIPS name change censorship,https://www.reddit.com/r/MachineLearning/comments/9y7rop/d_nips_name_change_censorship/,Seerdecker,1542561791,"So, NIPS changed its name. People can't discuss it, because the post is locked.

https://www.reddit.com/r/MachineLearning/wiki/rules
""Our mission is all about embracing diverse views"".
",40,1,False,self,,,,,
996,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y7tds,self.MachineLearning,What is linear and non-linear model in Machine learning?,https://www.reddit.com/r/MachineLearning/comments/9y7tds/what_is_linear_and_nonlinear_model_in_machine/,shchinmay,1542562108,[removed],0,1,False,self,,,,,
997,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y7x8i,resoomer.com,Resoomer | Rsumeur pour faire un rsum de texte automatique en ligne,https://www.reddit.com/r/MachineLearning/comments/9y7x8i/resoomer_rsumeur_pour_faire_un_rsum_de_texte/,Lorealsap,1542562815,,0,1,False,https://b.thumbs.redditmedia.com/XAyTmBTRB6c9ToluQUZ5OP1z-UW0u6U1h-BKuJnFvbk.jpg,,,,,
998,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,2,9y81e5,self.MachineLearning,[D] What's your approach for new/modified DNN architectures?,https://www.reddit.com/r/MachineLearning/comments/9y81e5/d_whats_your_approach_for_newmodified_dnn/,describbler,1542563596,"I have been reading and implementating papers(mostly related to computer vision). I want to understand what kind of approaches researchers follow for coming up with an architecture for their paper or an attempt to solve a problem. 

1. If you are working on a paper inspired from some prior work,what would be the process that includes the initial idea for the architecture till the final architecture proposal?
2. If you are trying to solve a problem with the help of existing research paper results, how do you figure out the final architecture?

To add an example - if there is an existing model that works on a single constrained image to give an output and you want it to work with two or three images instead, how do you figure out the architecture for this?",1,1,False,self,,,,,
999,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,3,9y83qi,self.MachineLearning,Extending Google Colab beyond 12 hours?,https://www.reddit.com/r/MachineLearning/comments/9y83qi/extending_google_colab_beyond_12_hours/,bingoer,1542564033,[removed],0,1,False,self,,,,,
1000,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,3,9y85rh,self.MachineLearning,why the neural network always give the same output ?,https://www.reddit.com/r/MachineLearning/comments/9y85rh/why_the_neural_network_always_give_the_same_output/,xinqij1140,1542564395,[removed],0,1,False,self,,,,,
1001,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,3,9y86gy,self.MachineLearning,How to know which Resnet variant this is? Is it Resneet 18?,https://www.reddit.com/r/MachineLearning/comments/9y86gy/how_to_know_which_resnet_variant_this_is_is_it/,Heywhyhe,1542564534,[removed],0,1,False,self,,,,,
1002,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,3,9y8auw,wired.com,[N[]The Genius Neuroscientist Who Might Hold the Key to True AI (Karl Friston bio and Active Inference popularization),https://www.reddit.com/r/MachineLearning/comments/9y8auw/nthe_genius_neuroscientist_who_might_hold_the_key/,Nebulata,1542565391,,0,1,False,https://b.thumbs.redditmedia.com/Mz7SO40xqXLI4DjviGZAbc0uH4RK1uXKvb_W8YoCjzo.jpg,,,,,
1003,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,4,9y8zl7,self.MachineLearning,[D] Is there a mistake in this Keras word2vec implementation?,https://www.reddit.com/r/MachineLearning/comments/9y8zl7/d_is_there_a_mistake_in_this_keras_word2vec/,SmArtilect,1542569869,"This is the only tutorial about word2vec with negative sampling for Keras

[http://adventuresinmachinelearning.com/word2vec-keras-tutorial/](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/)  
there is cosine similarity for validation model but dot product (not  normalized unlike cosine similarity) for training. For validation model the dot\_axes is set to 0:

    similarity = merge([target, context], mode='cos', dot_axes=0)

as a result logging model summary gives:

&gt;Layer (type) Output Shape  
dot\_1 (Dot) (300, 1, 1)

but for training model dot\_axes is

    dot_product = merge([target, context], mode='dot', dot_axes=1)

not surprisingly model summary shows a different thing

&gt;Layer (type) Output Shape  
dot\_2 (Dot) (None, 1, 1)

As far as I know 'Ouput' in the summary shows fixed batch size (and its value is usually None because it doesn't need fixing in most cases) but it still doesn't make sense to me why it's 300 because 300 happens to be the embedding dimension.  Clearly something's wrong the dot\_axes, can anybody confirm?

code on github: [https://github.com/adventuresinML/adventures-in-ml-code/blob/master/keras\_word2vec.py](https://github.com/adventuresinML/adventures-in-ml-code/blob/master/keras_word2vec.py)",5,1,False,self,,,,,
1004,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,4,9y9217,self.MachineLearning,dataset on political ideologies,https://www.reddit.com/r/MachineLearning/comments/9y9217/dataset_on_political_ideologies/,kumaarpranv,1542570306,[removed],0,1,False,self,,,,,
1005,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,4,9y9410,self.MachineLearning,Feature scaling on targets,https://www.reddit.com/r/MachineLearning/comments/9y9410/feature_scaling_on_targets/,SyenPark,1542570678,[removed],0,1,False,self,,,,,
1006,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,5,9y98yp,self.MachineLearning,"Correlating Movie Genre, Rating, and Release Date",https://www.reddit.com/r/MachineLearning/comments/9y98yp/correlating_movie_genre_rating_and_release_date/,DuckDuckFooGoo,1542571556,[removed],0,1,False,self,,,,,
1007,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,6,9y9v8h,self.MachineLearning,A question on L0 regularization of neural networks,https://www.reddit.com/r/MachineLearning/comments/9y9v8h/a_question_on_l0_regularization_of_neural_networks/,panini___17,1542575633,[removed],0,1,False,self,,,,,
1008,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,6,9y9yt9,self.MachineLearning,NLP: how to convert from parsed dependency tree to a sentence?,https://www.reddit.com/r/MachineLearning/comments/9y9yt9/nlp_how_to_convert_from_parsed_dependency_tree_to/,xinqij1140,1542576300," 

I'm exploring the enhanced dependency parser provided through Stanford coreNLP lib. Now I can successfully parse sentence into the dependencies tree. However, I wonder if there is any lib/function that would do the reverse work.

For example convert the following dependency tree to the original sentence : ""these apples were always counted, and about the time when they began to grow rip it was found that every night one of them was gone.""


 

    {'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 5, 'dependentGloss': 'counted'} {'dep': 'det', 'governor': 2, 'governorGloss': 'apples', 'dependent': 1, 'dependentGloss': 'these'} {'dep': 'nsubjpass', 'governor': 5, 'governorGloss': 'counted', 'dependent': 2, 'dependentGloss': 'apples'} {'dep': 'auxpass', 'governor': 5, 'governorGloss': 'counted', 'dependent': 3, 'dependentGloss': 'were'} {'dep': 'advmod', 'governor': 5, 'governorGloss': 'counted', 'dependent': 4, 'dependentGloss': 'always'} {'dep': 'punct', 'governor': 5, 'governorGloss': 'counted', 'dependent': 6, 'dependentGloss': ','} {'dep': 'cc', 'governor': 5, 'governorGloss': 'counted', 'dependent': 7, 'dependentGloss': 'and'} {'dep': 'case', 'governor': 10, 'governorGloss': 'time', 'dependent': 8, 'dependentGloss': 'about'} {'dep': 'det', 'governor': 10, 'governorGloss': 'time', 'dependent': 9, 'dependentGloss': 'the'} {'dep': 'nmod:about', 'governor': 19, 'governorGloss': 'found', 'dependent': 10, 'dependentGloss': 'time'} {'dep': 'advmod', 'governor': 13, 'governorGloss': 'began', 'dependent': 11, 'dependentGloss': 'when'} {'dep': 'nsubj', 'governor': 13, 'governorGloss': 'began', 'dependent': 12, 'dependentGloss': 'they'} {'dep': 'nsubj:xsubj', 'governor': 15, 'governorGloss': 'grow', 'dependent': 12, 'dependentGloss': 'they'} {'dep': 'acl:relcl', 'governor': 10, 'governorGloss': 'time', 'dependent': 13, 'dependentGloss': 'began'} {'dep': 'mark', 'governor': 15, 'governorGloss': 'grow', 'dependent': 14, 'dependentGloss': 'to'} {'dep': 'xcomp', 'governor': 13, 'governorGloss': 'began', 'dependent': 15, 'dependentGloss': 'grow'} {'dep': 'xcomp', 'governor': 15, 'governorGloss': 'grow', 'dependent': 16, 'dependentGloss': 'ripe'} {'dep': 'nsubjpass', 'governor': 19, 'governorGloss': 'found', 'dependent': 17, 'dependentGloss': 'it'} {'dep': 'auxpass', 'governor': 19, 'governorGloss': 'found', 'dependent': 18, 'dependentGloss': 'was'} {'dep': 'auxpass', 'governor': 19, 'governorGloss': 'found', 'dependent': 18, 'dependentGloss': 'was'} {'dep': 'auxpass', 'governor': 19, 'governorGloss': 'found', 'dependent': 18, 'dependentGloss': 'was'} {'dep': 'auxpass', 'governor': 19, 'governorGloss': 'found', 'dependent': 18, 'dependentGloss': 'was'} {'dep': 'conj:and', 'governor': 5, 'governorGloss': 'counted', 'dependent': 19, 'dependentGloss': 'found'} {'dep': 'dep', 'governor': 19, 'governorGloss': 'found', 'dependent': 20, 'dependentGloss': 'that'} {'dep': 'det', 'governor': 22, 'governorGloss': 'night', 'dependent': 21, 'dependentGloss': 'every'} {'dep': 'dep', 'governor': 20, 'governorGloss': 'that', 'dependent': 22, 'dependentGloss': 'night'} {'dep': 'nsubjpass', 'governor': 27, 'governorGloss': 'gone', 'dependent': 23, 'dependentGloss': 'one'} {'dep': 'case', 'governor': 25, 'governorGloss': 'them', 'dependent': 24, 'dependentGloss': 'of'} {'dep': 'nmod:of', 'governor': 23, 'governorGloss': 'one', 'dependent': 25, 'dependentGloss': 'them'} {'dep': 'auxpass', 'governor': 27, 'governorGloss': 'gone', 'dependent': 26, 'dependentGloss': 'was'} {'dep': 'acl:relcl', 'governor': 22, 'governorGloss': 'night', 'dependent': 27, 'dependentGloss': 'gone'} {'dep': 'punct', 'governor': 5, 'governorGloss': 'counted', 'dependent': 28, 'dependentGloss': '.'}


",0,1,False,self,,,,,
1009,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,8,9yatsw,self.MachineLearning,What is the purpose of a validation set? How is it different from the test set?,https://www.reddit.com/r/MachineLearning/comments/9yatsw/what_is_the_purpose_of_a_validation_set_how_is_it/,SwaggyDaggy,1542582351,[removed],0,1,False,self,,,,,
1010,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,8,9yaupq,self.MachineLearning,[D] I am designing my first experiment and could use some guidance. I am trying to generate small narratives about a paragraph in length for a genre of strategy games (events for any paradox fans out there) using NLG trained mostly by reinforcement learning.,https://www.reddit.com/r/MachineLearning/comments/9yaupq/d_i_am_designing_my_first_experiment_and_could/,Morninglow,1542582534,"Hello, I am asking here because my professor is not an NLP guy and I don't think this is a beginner question. I've been going hard into natural language generation for the past few months but I'm still an undergrad so my ability to devise models just isn't there. 

&amp;#x200B;

So, in the games by one Paradox Interactive you play as a country in some time period ranging from Rome to scifi space age. So that you are not just staring at a map and clicking buttons the game has pop up 'events' that contain things like historical events that actually happened, peasant issues, laws, court cases, intrigue. These events are like small short stories that add flavor to the game. Lots of them have responses you can choose from that have some effect on the game or lead to another event and form event chains. I consider the event chains to be currently intractable due to long term dependencies so I am focusing on one off events where the response from the player is pretty much ""okay."" 

&amp;#x200B;

The events specific type of events I am focusing on are from the scifi space empire game Stellaris and are known as 'anomalies' which are mostly from the perspective of a science ship exploring space and encountering strange aliens, weird planets, ancient ruins and technology, I will post some examples

&amp;#x200B;

Name: Rock Brain

""A scan of the particularly massive asteroid \[From.GetName\] yields fascinating results. The rock's core is shot through with a network of conductive crystals carrying a weak electric charge. The crew on the \[Root.GetName\] recognize this matrix as a primitive neural lattice and believe the celestial body to be thinking in some capacity, if not fully sapient. They seek permission to excavate the core and transport it to the nearest orbital station for closer study.""

&amp;#x200B;

Name: Grasping Vines

""The dense grasses of \[From.GetName\] respond to the \[Root.GetName\]'s exploratory probes by disassembling them in a spectacularly violent manner. As sinewy vines reach for the ship far above, the relative lack of animal life on the planet starts making sense. A special project has been issued for further study of the planet's plant life.""

&amp;#x200B;

Name: Eddic Monolith

""\[From.GetName\] is uninhabited and indeed uninhabitable, but not unvisited. Its surface is littered with tall cenotaphs carved from some mineral not native to the planet; evidently placed here by some artistically-inclined spacefaring race. The monoliths' flowing lines deftly chart a history so fantastical it must surely be fictional. Surely.""

&amp;#x200B;

Name: Gravity Crush

 ""The \[Root.GetName\]'s survey of the asteroid is temporarily halted as a probe is caught by an errant gravitational anomaly. The probe manages to transmit some data before it is brutally crushed by the extreme forces - forces that completely dissipate soon after.""

&amp;#x200B;

The game has several hundred to about a thousand of these. 

&amp;#x200B;

What I am trying to do is generate events like these and be able to add them to the game without a human having to approve of them first. I want to do this using source material like scifi novels and wikis.

&amp;#x200B;

I have identified several approaches to this I will now outline.

&amp;#x200B;

**Generational Models**

&amp;#x200B;

I started out looking at purely generational models like GAN and VAE but from what I have seen these are not yet at the point where they can handle something like this as MaskGAN tops out at around 40 words for output. So what I am thinking is to use the generational model to generate prompts for another model as in 'Hierarchical Neural Story Generation' by fair. MaskGAN would be good here as it is pretrained on material like you want to generate so pretraining it on a scifi corpus and fine tuning it on the names of the anomalies or hand crafted prompts would probably give good results. However MaskGAN is done with reinforcement learning and that applies directly to what I want to do with it (human grading by adding events to be scored as a game mod). So if anyone thinks I can pull this off with just MaskGAN or another seqGAN let me know.

&amp;#x200B;

**Prediction**

&amp;#x200B;

From what I have read it seems like the best results in NLG have come from phrasing it as a prediction task. My best example is fair's 'Hierarchical Neural Story Generation' that can generate stories about 150 words in length by conditioning on a prompt for the story. They use 300k prompt-story pairs to get a convolutional seq2seq model normally used for translation to generate stories about 150 words in length and they get much further than I think anyone else has with story generation barring the old stuff like Tale-Spin. So something like this is an option. If I could use something like BERT  or another system with generative pretraining on a corpus of scifi I could use the thousand name-anomaly pairs I do have to set up a similar system. 

Also instead of using a name -&gt; anomaly system there is the option of using other data that the game has and use that. You could form a prompt for prediction based on where and when in the game the anomaly is encountered. 

I have also thought you could phrase this as a summarization task as a lot of these anomalies are like summaries of larger scifi concepts and events. Then you could use a classifier like ULMFit to identify sections of scifi novels or short stories to summarize and use our very good summarization tech to do summaries. I am thinking like 'Generating Wikipedia Articles with Neural Abstractive Summarization'

&amp;#x200B;

**Reinforcement Learning / Human Grading**

&amp;#x200B;

A big hangup in training NLG systems is the expense of human grading for generated examples and the reliance on BLEU. I think this project has a big advantage in that we can leverage gamers. We release a mod that prompts users with generated anomalies and ask the player to grade them. Then we can get thousands of graded examples for free. So I am thinking this is a great application for reinforcement learning but I am not sure how to apply it accept as it applies to the seqGANS. So if anyone could guide me as to how I could apply reinforcement learning here I would really appreciate it.

Also if I can incorporate human grading without reinforcement learning I would also like to know this. 

&amp;#x200B;

**Applying it to the Game with a discriminator to check results**

&amp;#x200B;

My eventual goal is to be able to incorporate this into the game as an end to end system that doesn't require human checking of results. I was thinking of trying this with a 2 or more stage discriminator. The first stage would be a parser that checks for grammatical and or spelling errors to weed out the stuff that's flat out wrong. The second stage would be a sort of sentiment classifier that will check if an output is a valid sentiment or not.  

&amp;#x200B;

**In Summary**

&amp;#x200B;

So while I've been doing a lot of reading on this I am not sure how to set this up. I think I could hack something together but before I get down and dirty coding I want to make sure I have a good basis to start from and I'm not just guessing. Of course now that I think about it the scientific process is trial and error so maybe I should just be seeing what doesn't work. 

&amp;#x200B;

**TLDR**

&amp;#x200B;

I want to generate scifi short stories about a paragraph long using a scifi corpus and about a thousand examples like what I want to generate

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

  


&amp;#x200B;",7,1,False,self,,,,,
1011,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,8,9yb1to,self.MachineLearning,ML is saturated. What now?,https://www.reddit.com/r/MachineLearning/comments/9yb1to/ml_is_saturated_what_now/,BRING_BACK_NIPS,1542583869,[removed],0,1,False,self,,,,,
1012,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,9,9ybclh,arxiv.org,[R] [1811.02657] Neural Rendering Model: Joint Generation and Prediction for Semi-Supervised Learning,https://www.reddit.com/r/MachineLearning/comments/9ybclh/r_181102657_neural_rendering_model_joint/,TwoUpper,1542586110,,0,2,False,default,,,,,
1013,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,9,9ybpza,self.MachineLearning,[R] TDLS - Classics: Latent Dirichlet Allocation,https://www.reddit.com/r/MachineLearning/comments/9ybpza/r_tdls_classics_latent_dirichlet_allocation/,tdls_to,1542588850,"we recently went through the original LDA paper and discussed some of the details of the math.

algorithm review: [https://youtu.be/VTweNS8GiWI](https://youtu.be/VTweNS8GiWI)

paper discussions: [https://youtu.be/arnQjDZhfB8](https://youtu.be/arnQjDZhfB8)

paper: [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)",0,1,False,self,,,,,
1014,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,10,9ybstu,self.MachineLearning,[R] TDLS: Progressive Growing of GANs,https://www.reddit.com/r/MachineLearning/comments/9ybstu/r_tdls_progressive_growing_of_gans/,tdls_to,1542589437,"We recently reviewed ProgressiveGANs:

algorithm review: [https://youtu.be/q7\_TCtI2188](https://youtu.be/q7_TCtI2188)

paper discussions: [https://youtu.be/fMds8t\_Gt-I](https://youtu.be/fMds8t_Gt-I)

paper reference: [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)

&amp;#x200B;

this paper...

* Introduces a new training procedure that gradually increases the level of resolution as training progresses.
* Uses minibatch standard deviation and two normalization techniques to increase variation in the results and stabilize training.
* Proposes Sliced Wasserstein Distance as a more consistent alternative to MS-SSIM.
* Observes results that are at least as good as any in the literature for unconditional generation.

And we had some questions at the end:

* What can we know (or speculate) about the impact of the progressive structure on modal collapse?
* A lot of structure is assumed. What can we know (or speculate) about the role of the pseudo-residual blocks and the training phase cycle structure of ProgressiveGAN?

&amp;#x200B;",7,2,False,self,,,,,
1015,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,10,9ybyde,self.MachineLearning,How would you design a MOOC capstone course/project?,https://www.reddit.com/r/MachineLearning/comments/9ybyde/how_would_you_design_a_mooc_capstone_courseproject/,dscapstone,1542590593,[removed],0,1,False,self,,,,,
1016,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,10,9ybyxr,self.MachineLearning,[Discussion] How to build Bayesian models in 30 minutes,https://www.reddit.com/r/MachineLearning/comments/9ybyxr/discussion_how_to_build_bayesian_models_in_30/,springcoil,1542590714,[Bayesian models in 30 minutes or less](https://www.youtube.com/watch?v=W-lqgS4IdaA&amp;t=1s) I put together this online video on how you build Bayesian models in less than 30 minutes. If you know some machine learning - you should look at learning this awesome suite of tools. ,11,1,False,self,,,,,
1017,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,10,9yc4i5,self.MachineLearning,word distance from BERT,https://www.reddit.com/r/MachineLearning/comments/9yc4i5/word_distance_from_bert/,sdqfd,1542591891,[removed],0,1,False,self,,,,,
1018,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,10,9yc6b5,self.MachineLearning,Weird results. Image Segmentation,https://www.reddit.com/r/MachineLearning/comments/9yc6b5/weird_results_image_segmentation/,notoriousjoeski,1542592275,[removed],0,1,False,self,,,,,
1019,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,11,9ycdtd,self.MachineLearning,[D] How is a validation set different from a test set?,https://www.reddit.com/r/MachineLearning/comments/9ycdtd/d_how_is_a_validation_set_different_from_a_test/,SwaggyDaggy,1542593843,"I am going through Deep Learning, the textbook by Bengio, Goodfellow, Courville. I'll quote what their perspective on validation sets are:

""Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because the setting is dicult to optimize. More frequently,the setting must be a hyperparameter because it is not appropriate to learn thathyperparameter on the training set. This applies to all hyperparameters that control model capacity. If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overtting. For example, we can always t the training set better with a higher-degree polynomial and a weight decay setting of= 0 than we could with a lower-degree polynomial and a positive weight decay setting.To solve this problem, we need a validation set of examples that the training algorithm does not observe.Earlier we discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed. It is important that the test examples are not used in any way to make choices about the model, includingits hyperparameters. For this reason, no example from the test set can be used in the validation set. Therefore, we always construct the validation set from the training data. Specically, we split the training data into two disjoint subsets.One of these subsets is used to learn the parameters. The other subset is our validation set, used to estimate the generalization error during or after training, allowing for the hyperparameters to be updated accordingly. The subset of data used to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process. The subset of data used to guide the selection of hyperparameters iscalled the validation set. Typically, one uses about 80 percent of the training data for training and 20 percent for validation. Since the validation set is used to train the hyperparameters, the validation set error will underestimate the generalization error, though typically by a smaller amount than the training error does. After all hyperparameter optimization is complete, the generalization error may be estimated using the test set.""

I am not understanding this. This passage clearly states that the validation sets should be used after training has been concluded to estimate the generalization error. I thought this was what the test set was for?

This passage also says that the validation set should be used to ""train"" the hyperparameters by choosing better values after training has been concluded. This, again, from my perspective, was the point of having a test set.

Would to be correct to say that train data is used to learn the parameters, but the generalization error of the model is estimated with the train data, and THEN there is another set (the validation set) that we aren't supposed to use for evaluating generalization error until training has completely ended?

Basically, I'm asking why might want to have a disjoint test/validation set.",4,1,False,self,,,,,
1020,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,14,9ydmcz,self.MachineLearning,How would you design a machine learning capstone project for a MOOC?,https://www.reddit.com/r/MachineLearning/comments/9ydmcz/how_would_you_design_a_machine_learning_capstone/,dscapstone,1542603685,[removed],0,1,False,self,,,,,
1021,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,14,9yduqo,self.MachineLearning,[D] How would you design a MOOC machine learning capstone course?,https://www.reddit.com/r/MachineLearning/comments/9yduqo/d_how_would_you_design_a_mooc_machine_learning/,dscapstone,1542605639,"Hi everyone,

I'm helping develop the final capstone course/project for a data science MOOC and I was hoping to bounce some ideas or get some suggestions from this community. On a very basic level, we want learners to develop a machine learning algorithm and a product (GitHub page, Rmd, slide deck) that can be showcased. There will be 500-600 people signed up for this class, and we expect this number to rise slightly over time.

If you've done a capstone course/project, what did you like or not like about it? If you haven't done one, what kind of projects, features, or support would you like to see provided?

Here are some things the team has been debating:

* Should the project be assigned (How many options?) or should learners have the freedom to pick their own dataset and project?
   * If we assign a training dataset or project, we would have a testing set and grading could be easier.
* Should there be weekly goals/milestones or just a final submission at the end of the course (2-4 weeks) without any kind of guidance/structure?
   * If a dataset or project is assigned, it would be easier to have the same milestone or quiz on the data for everyone.
* Do free learners do an assigned project with peer grading and paid learners get to pick their own project and get graded by a TA or instructor?
   * There will be 500+ people in the class, so individually grading projects will be difficult.
* How do we grade the project (and smaller milestones, if any)? Does everyone get peer graded? Do only paid certificate learners get graded by teaching assistants/the instructor?
* How and what do learners submit? Do learners submit a Rmd or csv file or a GitHub link? Are learners graded primarily on the Rmd/slide deck or a metric like MSE?

Thanks! I look forward to your thoughts.",1,1,False,self,,,,,
1022,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,14,9ydxyt,medium.com,7 Essentials Facts About Machine Learning Algorithms!,https://www.reddit.com/r/MachineLearning/comments/9ydxyt/7_essentials_facts_about_machine_learning/,smadrid056,1542606416,,0,1,False,default,,,,,
1023,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,14,9ydyp4,self.MachineLearning,Same weights in a simple feed forward neural network?,https://www.reddit.com/r/MachineLearning/comments/9ydyp4/same_weights_in_a_simple_feed_forward_neural/,hungry_fat_phuck,1542606592,[removed],0,1,False,self,,,,,
1024,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,14,9ye0zi,self.MachineLearning,how can convert 1array to 2array?,https://www.reddit.com/r/MachineLearning/comments/9ye0zi/how_can_convert_1array_to_2array/,GoBacksIn,1542607153,[removed],0,1,False,self,,,,,
1025,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,15,9ye8k7,technologyreview.com,Using ML to spot hackers,https://www.reddit.com/r/MachineLearning/comments/9ye8k7/using_ml_to_spot_hackers/,honghuac,1542608997,,0,1,False,default,,,,,
1026,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,16,9yeh2s,self.MachineLearning,[D] Can logistic regression model an evolving value?,https://www.reddit.com/r/MachineLearning/comments/9yeh2s/d_can_logistic_regression_model_an_evolving_value/,abstractcontrol,1542611194,"http://mbmlbook.com/TrueSkill_Allowing_the_skills_to_vary.html

&gt; You may think that our online learning process updates our skill distribution for a player over time and so would allow the skill to change. This is a common misconception about online learning, but it is not true. Our current model assumes that the skill of a player is a fixed, but unknown, quantity. Online learning does not represent the modelling of an evolving skill value, but rather an updating of the uncertainty in this unknown fixed-across-time skill.

In the next chapter, they do something similar to a spam-classification related model. This made me wonder whether the same would apply to a logistic regression model.",4,1,False,self,,,,,
1027,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,16,9yelfs,self.MachineLearning,Recommender system which gives more weight to recent ratings,https://www.reddit.com/r/MachineLearning/comments/9yelfs/recommender_system_which_gives_more_weight_to/,readanything,1542612387,[removed],0,1,False,self,,,,,
1028,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,16,9yer8h,self.MachineLearning,[R] Large Scale GAN Training for High Fidelity Natural Image Synthesis . Includes a Colab Notebook of the implementation!! This is how it should be!!,https://www.reddit.com/r/MachineLearning/comments/9yer8h/r_large_scale_gan_training_for_high_fidelity/,AdditionalWay,1542614128,"https://arxiv.org/abs/1809.11096

Tensorflow hub

https://tfhub.dev/s?q=biggan

And Colab Notebook 

https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

Which I am very grateful for. This is the way it should be! 

It's very rare for a paper that's so clear that you never have any questions, and many times in those cases just a implementation of the paper would clear everything up. Not only is there code, but everything is set up in the colab enviroment so you can can just press and play. ",20,1,False,self,,,,,
1029,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,17,9yessq,community.singularitynet.io,The Dreams of Satoshi - Episode 2 - The Dreams of Satoshi - Graphic Novel,https://www.reddit.com/r/MachineLearning/comments/9yessq/the_dreams_of_satoshi_episode_2_the_dreams_of/,KlutzyBowl,1542614611,,0,1,False,default,,,,,
1030,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,18,9yfc25,self.MachineLearning,Mixture of experts - Balancing the load between the experts,https://www.reddit.com/r/MachineLearning/comments/9yfc25/mixture_of_experts_balancing_the_load_between_the/,albert1905,1542620389,[removed],0,1,False,self,,,,,
1031,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,19,9yfoxh,self.MachineLearning,Global Shipping Containers Market Report 2018-2022,https://www.reddit.com/r/MachineLearning/comments/9yfoxh/global_shipping_containers_market_report_20182022/,bir07,1542624142,[removed],1,1,False,self,,,,,
1032,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,19,9yfpat,vimeo.com,"[R][D] Talk by Professor Richard Bowden - Vision and AI: then, now and tomorrow",https://www.reddit.com/r/MachineLearning/comments/9yfpat/rd_talk_by_professor_richard_bowden_vision_and_ai/,tjuk,1542624254,,0,1,False,default,,,,,
1033,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,19,9yfrig,self.MachineLearning,[D] Non-causal dilated convolution for sequential data,https://www.reddit.com/r/MachineLearning/comments/9yfrig/d_noncausal_dilated_convolution_for_sequential/,Doomro22,1542624911,"Hi, 

I have been studying this paper ([https://arxiv.org/pdf/1803.01271.pdf](https://arxiv.org/pdf/1803.01271.pdf)) about Temporal Convolutional Network (TCN) which is a way to replace RNN.

I was wondering if there was any paper or study that is using the same kind of convolution but in a non-causal way (going in both directions, i.e. a way to replace Bidirectional-RNN) for sequential data such as audio or text? I haven't found one yet.

Thank you!",7,1,False,self,,,,,
1034,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,20,9yfymt,self.MachineLearning,Global Water Treatment Systems (PoE) Market Report 2018-2022,https://www.reddit.com/r/MachineLearning/comments/9yfymt/global_water_treatment_systems_poe_market_report/,bir07,1542626985,[removed],1,1,False,self,,,,,
1035,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,20,9yg01h,self.MachineLearning,[D] Is there any discord to discuss ideas about AI?,https://www.reddit.com/r/MachineLearning/comments/9yg01h/d_is_there_any_discord_to_discuss_ideas_about_ai/,Rainymood_XI,1542627376,"So I'm, obviously like everyone here, very interested in machine learning and AI, but in particular the brain, how it works and how we might emulate it. What places on the internet are there to discuss these ideas with like-minded people? I don't want to clog up this subreddit with my useless discussions. I have many ideas that I'd like to share and receive feedback on, iterating my ideas (our ideas maybe?) and maybe come to another fruitiful research project or two! ",2,1,False,self,,,,,
1036,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,20,9yg3b3,self.MachineLearning,"[R] Which optimizer works better for normalization techniques ( batch normalization, layer normalization, etc ...)",https://www.reddit.com/r/MachineLearning/comments/9yg3b3/r_which_optimizer_works_better_for_normalization/,aziz_22,1542628296,"Most of the papers used the SGD in their experiments.

I would like to hear from those who experiment those techniques with other optimizers.",8,1,False,self,,,,,
1037,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,22,9ygl4m,linkedin.com,Blockchain and Machine Learning To Predict Consumer Behavior,https://www.reddit.com/r/MachineLearning/comments/9ygl4m/blockchain_and_machine_learning_to_predict/,indrajeet_Sony,1542632786,,0,1,False,default,,,,,
1038,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,22,9ygovb,self.MachineLearning,[D] Resources for video reconstruction/completion,https://www.reddit.com/r/MachineLearning/comments/9ygovb/d_resources_for_video_reconstructioncompletion/,sammuelbrown,1542633621,"I am trying to make a model for automatic completion of incomplete videos using the frames before and after the hole, however I have hit a roadblock in regards to available resources, since most papers are on image reconstruction.

The two papers I have found are [this](http://www.wisdom.weizmann.ac.il/~vision/VideoCompletion/SpaceTimeCompletion.pdf) and [this](https://www.sciencedirect.com/science/article/abs/pii/S1005888516600602), with the second one not accessible.

I would be grateful if anyone knows about any others papers on this subject, since I haven't found anything else. I apologize if this is not the correct forum for this post.

Thanks in advance.",0,1,False,self,,,,,
1039,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,23,9yh5ys,self.MachineLearning,DevOps Tools For Managing Models?,https://www.reddit.com/r/MachineLearning/comments/9yh5ys/devops_tools_for_managing_models/,Simusid,1542637323,[removed],0,1,False,self,,,,,
1040,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,23,9yh6ed,self.MachineLearning,Varying Speaking Styles with Neural Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/9yh6ed/varying_speaking_styles_with_neural_texttospeech/,nisprateek,1542637402,[removed],0,1,False,self,,,,,
1041,MachineLearning,t5_2r3gv,2018-11-19,2018,11,19,23,9yhghk,self.MachineLearning,Global Single-use Bioreactors Market Report 2018-2022,https://www.reddit.com/r/MachineLearning/comments/9yhghk/global_singleuse_bioreactors_market_report/,bir07,1542639458,"BusinessIndustryReports have new report on Global Single-use Bioreactors Market 2018-2022. The report provides the newest industry data and industry future trends. The industry report lists the leading competitors and provides the insights strategic industry Analysis of the key factors influencing the market.  
 ",1,1,False,self,,,,,
1042,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhj7t,dimensionless.in,ML Methods for Prediction and Personalization,https://www.reddit.com/r/MachineLearning/comments/9yhj7t/ml_methods_for_prediction_and_personalization/,divya2018,1542639966,,0,1,False,default,,,,,
1043,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhjd8,euclidesdb.readthedocs.io,"[P] EuclidesDB, a multi-model machine learning feature database that is tightly coupled with PyTorch",https://www.reddit.com/r/MachineLearning/comments/9yhjd8/p_euclidesdb_a_multimodel_machine_learning/,perone,1542639993,,0,1,False,default,,,,,
1044,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhmbh,self.MachineLearning,"[D] Questions to unpaired image-to-image translation: Evaluation metrics, Amount of training data, Direction.",https://www.reddit.com/r/MachineLearning/comments/9yhmbh/d_questions_to_unpaired_imagetoimage_translation/,AllinOnAI,1542640539,"Hey,

i've been recently working on unpaired image-to-image translation andseek discussion/clarification on a few related issues:

&amp;#x200B;

**1.Evaluation metrics:**

I reckon at one pointeveryone wants to quickly evaluate generated  images - either to compare images generated by different architectures  or to influence training, e.g. by stopping training when results get  worse.

For GANs, there exist several metrics (many of which seem to be derived for cGANs):  
 

a) Inception Score (IS): evaluates class-conditional prediction accuracy of Inception Net trained on ImageNET: 

\- we don't have classes

\- suboptimal for non-ImageNET datasets anyway

b) Frechet Inception Distance (FID): compares hidden activations of coding layer in Inception architecture:

\+ meaningful on non-ImageNET datasets

\- requires at least 2048 samples

\- biased, hugely dependent on # of samples

c) Kernel Inception Distance (KID): compares hidden activations of coding layer in Inception architecture:

\+ few samples suffice

\+ unbiased

Part of the problem seems to be that - if there was a reliable metric  - it would immediately be baked into the loss function to  steerlearning towards desirable results.  
 

Suppose you wanted to compare your results to state of the art work in this field. What would your approach be?  
 

**2. Amount of training data:**

Typical unpaired image-to-image translation datasets (like  apple/orange, horse/zebra, summer/winter..) only seem to include a few  100's of samples. 

Is there any work done on studying the effect of more training data (or data augmentation) specifically for these kind of tasks?

&amp;#x200B;

**3. Moving forward:**

This is a more open question I guess. Where do you think this field is heading? What are the unresolved research questions? There has been a [recent thread](https://www.reddit.com/r/MachineLearning/comments/9spei3/d_progress_in_unpaired_imagetoimage_translation/) where there is an overview of recent research.

It seems that its mainly incorporating more findings from GAN/ML - Research? Playing around with loss functions/architectures? Scaling up?

I am thankful for any input!",1,1,False,self,,,,,
1045,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhmno,self.MachineLearning,[D] Tips for very hard image dataset?,https://www.reddit.com/r/MachineLearning/comments/9yhmno/d_tips_for_very_hard_image_dataset/,hadaev,1542640603,"I have 30k images dataset.

It is photos of womens from the porn site (lul).

And I'm trying to make a convolutional network to distinguish the size of boobs (where other labels, but i try to start from something).

Data unbalanced, noisy, all of it.

For now i tried pre trained resnet50 with freezed layers.

And, it cant learn.

Any ideas?

    52/52 [==============================] - 405s 8s/step - loss: 1.2176 - acc: 0.4454 - val_loss: 1.3119 - val_acc: 0.4348
    Epoch 2/100
    52/52 [==============================] - 363s 7s/step - loss: 1.1400 - acc: 0.4740 - val_loss: 1.1589 - val_acc: 0.4348
    Epoch 3/100
    52/52 [==============================] - 356s 7s/step - loss: 1.1192 - acc: 0.4825 - val_loss: 1.2166 - val_acc: 0.4040
    Epoch 4/100
    52/52 [==============================] - 358s 7s/step - loss: 1.1084 - acc: 0.4887 - val_loss: 1.5281 - val_acc: 0.4052

&amp;#x200B;",37,1,False,self,,,,,
1046,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhpvb,self.MachineLearning,How to Create a summarization algorithm,https://www.reddit.com/r/MachineLearning/comments/9yhpvb/how_to_create_a_summarization_algorithm/,ishandutta2007,1542641229,[removed],0,1,False,self,,,,,
1047,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhqxc,self.MachineLearning,[D] How do you come up with new ideas?,https://www.reddit.com/r/MachineLearning/comments/9yhqxc/d_how_do_you_come_up_with_new_ideas/,olaconquistador,1542641438,"I have often wondered how different people come up with ideas? is it like an active brainstorming session when you've decided it is time to work on something new? Or is it that you observe something while you are coding or reading and decide to explore that? 

Is your inner dialogue started with 'how do i improve A', and you start throwing things at A to see what sticks, or do you wait for a strong hunch that this is what is missing from A? I have often met people who brainstorm by turning over in their heads what other approaches can be tried to 'improve the current method', but does anyone just try out a lot of experiments with a pre-existing system to see where it breaks.I have not often heard people talking about that. 

how do you deal with the excitedness of having thunk up something new?",27,1,False,self,,,,,
1048,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhsbu,self.MachineLearning,"[P] EuclidesDB, a multi-model machine learning feature database integrated with libtorch",https://www.reddit.com/r/MachineLearning/comments/9yhsbu/p_euclidesdb_a_multimodel_machine_learning/,perone,1542641701,"Just released EuclidesDB, an open-source and multi-model machine learning feature database server written entirely in C++ and tightly integrated with libtorch. The main idea is to have efficient serialization and communication (gRPC+protobuf) for RPC calls and fast feature searching capabilities (such as LSH) while still allowing PyTorch for model tracing.

**Project documentation**: https://euclidesdb.readthedocs.io
**GitHub source**: https://github.com/perone/euclidesdb",10,1,False,self,,,,,
1049,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhsvl,em360tech.com,"IBM researchers warn that AI is not ready to handle ""business processes""",https://www.reddit.com/r/MachineLearning/comments/9yhsvl/ibm_researchers_warn_that_ai_is_not_ready_to/,Marksfik,1542641806,,0,1,False,default,,,,,
1050,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhuua,self.MachineLearning,Universities/labs doing research on Graph Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9yhuua/universitieslabs_doing_research_on_graph_neural/,Double_Contribution,1542642180,[removed],0,1,False,self,,,,,
1051,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,0,9yhw2f,self.MachineLearning,[D] What can I do with 200$ Google Cloud credits?,https://www.reddit.com/r/MachineLearning/comments/9yhw2f/d_what_can_i_do_with_200_google_cloud_credits/,InvestorSpain,1542642420,"Recently I won 200$ in Google Cloud credits at a data science competition. 

The credits are expiring in February, what are some cool project ideas I can do to spend the money?

I guess I should use it for deep learning, however I'm a beginner and all I've done yet is basic machine learning.",8,1,False,self,,,,,
1052,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,1,9yi3nn,youtube.com,A course on ml,https://www.reddit.com/r/MachineLearning/comments/9yi3nn/a_course_on_ml/,rishiarora,1542643796,,0,1,False,default,,,,,
1053,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,1,9yicnc,self.MachineLearning,[D] What is the process for selecting reviewers?,https://www.reddit.com/r/MachineLearning/comments/9yicnc/d_what_is_the_process_for_selecting_reviewers/,twocatsarewhite,1542645397,"It seems completely arbitrary which conferences (or even which year) I am invited to review and which ones I am not. Is it just people that area chairs can name off-the-cuff (which would explain why I am always invited to CVPR and not always for NIPS)? Or is there a shared pool of past authors, reviewers etc. that takes into account, perhaps, the quality of reviews/ past papers to make the decision.

Anyway, I think the process could use some transparency.",2,1,False,self,,,,,
1054,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,2,9yin3t,self.MachineLearning,[D] Why some authors submit papers with an anonymous identity?,https://www.reddit.com/r/MachineLearning/comments/9yin3t/d_why_some_authors_submit_papers_with_an/,aziz_22,1542647250,,0,1,False,self,,,,,
1055,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,2,9yisss,self.MachineLearning,[Discussion] How to install PyMC3,https://www.reddit.com/r/MachineLearning/comments/9yisss/discussion_how_to_install_pymc3/,springcoil,1542648209,"Hi guys, 

I'm a PyMC3 core developer and one question I often get asked is - **how do I install PyMC3?**

In this screencast I show you how. [https://www.youtube.com/watch?v=t69JjNzn1H8&amp;t=1s](https://www.youtube.com/watch?v=t69JjNzn1H8&amp;t=1s) 

&amp;#x200B;

If you enjoy content like this, you may want to sign up to my [newsletter](https://mailchi.mp/4940464d8e53/probprogrammer). ",0,1,False,self,,,,,
1056,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,2,9yit2h,self.MachineLearning,[D] Planning to initiate an AI/ML research group in my university. How to mail the director of Institute regarding this? Has anyone led such a group?,https://www.reddit.com/r/MachineLearning/comments/9yit2h/d_planning_to_initiate_an_aiml_research_group_in/,ZER_0_NE,1542648260,,3,1,False,self,,,,,
1057,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,2,9yiv66,self.MachineLearning,[P] I built a demo for playing with arbitrary image stylization in the browser,https://www.reddit.com/r/MachineLearning/comments/9yiv66/p_i_built_a_demo_for_playing_with_arbitrary_image/,Reiinakano,1542648634,[removed],1,1,False,https://b.thumbs.redditmedia.com/i3WWbigCxfYgqjdqzRre_KnBv0MzcNlnSFZW0xiew4o.jpg,,,,,
1058,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,3,9yjhm6,self.MachineLearning,[P] Fast style transfer for ANY style in the browser with TF.js,https://www.reddit.com/r/MachineLearning/comments/9yjhm6/p_fast_style_transfer_for_any_style_in_the/,Reiinakano,1542652487,"Demo link: [https://reiinakano.github.io/arbitrary-image-stylization-tfjs/](https://reiinakano.github.io/arbitrary-image-stylization-tfjs/)

Source code: [https://github.com/reiinakano/arbitrary-image-stylization-tfjs](https://github.com/reiinakano/arbitrary-image-stylization-tfjs)

&amp;#x200B;

I built a demo for playing with arbitrary image stylization in the browser using TensorFlow.js. Instead of having a single network per style like previous algorithms for fast style transfer, only a single model is used for all style images. You can also freely mix styles together.

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/i1f1iyu10cz11.jpg

&amp;#x200B;

The original paper is from [https://arxiv.org/abs/1705.06830](https://arxiv.org/abs/1705.06830) 

&amp;#x200B;

In summary, a style network is trained and used to generate a 100-D style vector for any painting. This vector is then fed, along with the content image, to a separate transformer network for the actual transformation.

&amp;#x200B;

This 100D vector is basically a latent space of ""style"". And we can do interesting ""latent space-y"" things with it. We can control stylization strength via a weighted average of the style vectors of the style and content images. I found this useful for styles that tend to ""overpower"" the content image. We can also combine different styles by interpolating between their style vectors, and letting the network guess what a style in-between the two paintings would look like.

&amp;#x200B;

The biggest issue with porting this to the browser was the model size. The style network is based on Inception-v3, which has &gt;97MB of weights. Using distillation, I was able to replace it with a 9.6MB MobileNet-v2, a 10x reduction in size. I think distillation is an underrated technique to bring some cool ML functionality to the browser, a lot of which use models too big to reliably deploy in a resource-limited environment.

&amp;#x200B;

In any case, I acknowledge the results are not perfect and will not look good for all combinations of style and content (can't get Van Gogh to work properly for some reason), but I think it's a good reason to get excited about what will eventually become possible in the future using the browser alone.",5,1,False,https://a.thumbs.redditmedia.com/-Dt-37iuzVSAehbO8KtEX96VjkXJVy5s8vTd7AhooT4.jpg,,,,,
1059,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,3,9yji2x,self.MachineLearning,Poker AI agent,https://www.reddit.com/r/MachineLearning/comments/9yji2x/poker_ai_agent/,vincesanityyy,1542652567,"Hello everyone, Im new in reddit. I have a question to ask. Can I make a poker bot or an AI using a ANN and decision tree? ANN is for the hand classifier and DT is for its actions. I want my bot to be my enemy in poker 1 vs 1. Thanks",0,1,False,self,,,,,
1060,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,3,9yjiwq,self.MachineLearning,What is the most authoritative and least painful resource for setting up a home-based deep learning box?,https://www.reddit.com/r/MachineLearning/comments/9yjiwq/what_is_the_most_authoritative_and_least_painful/,po-handz,1542652707,[removed],0,1,False,self,,,,,
1061,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,3,9yjovo,discourse.opengenus.org,When to use Convolutional Neural Networks (CNN)?,https://www.reddit.com/r/MachineLearning/comments/9yjovo/when_to_use_convolutional_neural_networks_cnn/,code_like_tiger,1542653719,,0,1,False,default,,,,,
1062,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,4,9yjrt6,redanalysis.org,"The Coming Quantum Computing Disruption, Artificial Intelligence and Geopolitics (1)",https://www.reddit.com/r/MachineLearning/comments/9yjrt6/the_coming_quantum_computing_disruption/,JMVALANTIN,1542654205,,0,1,False,https://b.thumbs.redditmedia.com/WReAXdvGruGQxJiTDiNX3PJ6ozbvYAVCwYDyj-joGBw.jpg,,,,,
1063,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,4,9yjsoi,self.MachineLearning,[P] Using Q-Learning to solve environments on OpenAI Gym,https://www.reddit.com/r/MachineLearning/comments/9yjsoi/p_using_qlearning_to_solve_environments_on_openai/,Lord_Bellman,1542654350,"Hey there, I'm fairly new to this subreddit, and I know this isn't very impressive, but I would just like to share my experiences recently in this field.

I have been working on solving environments on OpenAI Gym, and I have been loving it!

My Solved Environments So Far:

* Cartpole (although not with Q-Learning, but am working on that now :) )
* FrozenLake
* Pong (RAM Version)
* More to come :)

I used to get stuck frequently when making models to solve these environments, until I read a fantastic article describing how Q-Learning works ([https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa](https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa) &lt;-- if people are interested) 

&amp;#x200B;

Having found out about this, and started putting it to the test, I have become even more interested in this field, and I am loving finding out about new things on this subreddit. So, thanks for all the posts guys :)",11,1,False,self,,,,,
1064,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,4,9yjy90,discourse.opengenus.org,"Difference between MKL, MKL ML and MKL DNN",https://www.reddit.com/r/MachineLearning/comments/9yjy90/difference_between_mkl_mkl_ml_and_mkl_dnn/,raindrop_code,1542655310,,0,1,False,default,,,,,
1065,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,4,9ykb8w,itnuggetsonline.com,Beginner to Advanced Java Course | IT Nuggets Online (CBT),https://www.reddit.com/r/MachineLearning/comments/9ykb8w/beginner_to_advanced_java_course_it_nuggets/,ITNuggetsOnline,1542657549,,0,1,False,default,,,,,
1066,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,5,9ykboh,self.MachineLearning,[D] Multi Armed Bandit Problem - Website Ads,https://www.reddit.com/r/MachineLearning/comments/9ykboh/d_multi_armed_bandit_problem_website_ads/,FriendlyCartoonist,1542657622,"Let's say there are 10 different variations of a ad. Each round, 1 ad should be shown to the user, result 1 if clicked and result 0 if not clicked.

Give this, how is it possible 1 round can have multiple ads with result 1? For example Ad 1, 3, 5 all have result 1 while rest is 0. Isn't only 1 ad shown/round?",1,1,False,self,,,,,
1067,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,5,9ykghl,self.MachineLearning,[D] Is there any difference between gradient preconditioning as used in feature visualization and as used in general optimization?,https://www.reddit.com/r/MachineLearning/comments/9ykghl/d_is_there_any_difference_between_gradient/,Lovsovs,1542658435,"From [this excellent distill article:](https://distill.pub/2017/feature-visualization/)
&gt; Transforming the gradient like this is actually quite a powerful toolits called preconditioning in optimization. You can think of it as doing steepest descent to optimize the same objective, but in another parameterization of the space or under a different notion of distance. This changes which direction of descent will be steepest, and how fast the optimization moves in each direction, but it does not change what the minimums are. If there are many local minima, it can stretch and shrink their basins of attraction, changing which ones the optimization process falls into. As a result, using the right preconditioner can make an optimization problem radically easier.

&gt; How can we choose a preconditioner that will give us these benefits? A good first guess is one that makes your data decorrelated and whitened. In the case of images this means doing gradient descent in the Fourier basis, with frequencies scaled so that they all have equal energy.

Are the authors talking about preconditioning in the sense of transforming the *images* or in the sense of transforming the *parameters* of the model?

AFAIK, preconditioning the parameters (such as in [this paper](https://arxiv.org/abs/1803.09383) and I guess in optimizers such as Adam and Adagrad, and so on) is different from what is mentioned above, but I'm not sure, so I'd be grateful if anyone is able to shed some light on this matter.

Thanks. ",0,1,False,self,,,,,
1068,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,5,9ykj93,self.MachineLearning,[R] Advancing to 3D Deep Neural Networks in Medical Image Analysis,https://www.reddit.com/r/MachineLearning/comments/9ykj93/r_advancing_to_3d_deep_neural_networks_in_medical/,tldrtldreverything,1542658913,"Hey everyone,
I encountered a new Nature paper on ML for medical image analysis, went down the rabbit hole, and came with a summary of the field - https://lyrn.ai/2018/11/16/advancing-to-3d-deep-neural-networks-in-medical-image-analysis/ (TLDR: 3D CNNs for medical images are going to be a big deal)

I hope it gives you a useful introduction. I'm happy to hear your feedback and answer any questions you may have.",0,1,False,self,,,,,
1069,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,5,9ykrf4,self.MachineLearning,c program to implement discrete BAM network,https://www.reddit.com/r/MachineLearning/comments/9ykrf4/c_program_to_implement_discrete_bam_network/,legendattherateroot,1542660322,[removed],0,1,False,self,,,,,
1070,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,5,9yktpw,youtube.com,[P] Understanding evolution with neural networks using Pixling World,https://www.reddit.com/r/MachineLearning/comments/9yktpw/p_understanding_evolution_with_neural_networks/,FredrikNoren,1542660725,,0,1,False,https://b.thumbs.redditmedia.com/kGc2qmw0BMsuerT2mtC1VAAnX8oFdK0QPcPn8Bv18Wk.jpg,,,,,
1071,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,6,9ykwrq,self.MachineLearning,CNN or RCNN for phoneme recognition?,https://www.reddit.com/r/MachineLearning/comments/9ykwrq/cnn_or_rcnn_for_phoneme_recognition/,AnasAlmasri,1542661257,"I have been reading a lot of research papers on both, and almost everyone says accuracies are comparable to an extent. If I'm using the TIMIT speech recognition dataset, which NN would most likely yield better results? 

Note: I'd like to extract the phonemes without validating them (e.g. if the voice recording says ""sagnal"" rather than ""signal"", I need to get the phonemes of the first).",0,1,False,self,,,,,
1072,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,6,9ykxj2,developer.amazon.com,Amazon scientists: Alexa learned a newscaster's speaking style from just a few hours of training data,https://www.reddit.com/r/MachineLearning/comments/9ykxj2/amazon_scientists_alexa_learned_a_newscasters/,georgecarlyle76,1542661393,,0,1,False,https://b.thumbs.redditmedia.com/nUbjwVSQJ9c9q5VuqjGiFeyk6smCoSloc0_HpA4P6gQ.jpg,,,,,
1073,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,7,9ylimp,self.MachineLearning,Deep learning project in neurology fields? (beginner),https://www.reddit.com/r/MachineLearning/comments/9ylimp/deep_learning_project_in_neurology_fields_beginner/,OverLordGoldDragon,1542665049,[removed],0,1,False,self,,,,,
1074,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,7,9yllwf,self.MachineLearning,NIPS question from a first-timer,https://www.reddit.com/r/MachineLearning/comments/9yllwf/nips_question_from_a_firsttimer/,fireant2,1542665627,[removed],0,1,False,self,,,,,
1075,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,7,9ylol3,medium.com,Name Flip-Flop: NIPS Is Now NeurIPS,https://www.reddit.com/r/MachineLearning/comments/9ylol3/name_flipflop_nips_is_now_neurips/,gwen0927,1542666112,,0,1,False,https://b.thumbs.redditmedia.com/K6B36UrcNAKgc24yEK-AFXVf2A9KX95c_lVpOYmFPEM.jpg,,,,,
1076,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,7,9ylomw,self.MachineLearning,[Discussion] What is the difference between imitation learning and unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/9ylomw/discussion_what_is_the_difference_between/,GuiltyResearcher,1542666122,"I have been following the Berkeley RL course and in the latest lecture they introduced imitation learning in the context of ""learning rewards"". But everything presented looked familiar to what is done in unsupervised learning, especially in MLE and GAN frameworks. Is there a fundamental difference between the two or are they just separate due to their historical underpinnings and how they are concretely framed? ",2,1,False,self,,,,,
1077,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,8,9ymexk,self.MachineLearning,[D] Is there a reason the optimisation of neural networks is not posed as a RL problem itself?,https://www.reddit.com/r/MachineLearning/comments/9ymexk/d_is_there_a_reason_the_optimisation_of_neural/,4c616e7465726e,1542671109,"As far as I know the training of neural networks is currently done solely through gradient methods (and occasionally through gradient free techniques such as neuro-evolution). These optimisation techniques are crafted by humans to give good results, but are nowhere near perfect and suffer heavily from a range of problems which lead to suboptimal solutions.

My question is why has this problem of neural net optimisation not been posed as a reinforcement learning task itself? Could a reinforcement learning agent not potentially learn new optimisation methods which may outperform hand-crafted ones? It seems like the perfect domain for RL but I haven't seen any studies on it.",6,1,False,self,,,,,
1078,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,8,9ymfni,self.MachineLearning,[R] About SVCCA Implementation,https://www.reddit.com/r/MachineLearning/comments/9ymfni/r_about_svcca_implementation/,j50398,1542671249,Would someone please explain how the authors of [SVCCA paper](https://arxiv.org/pdf/1706.05806.pdf) produce Figure 4? I went through their [open source code](https://github.com/google/svcca) implementation but it is not clear which values they plotted.,0,1,False,self,,,,,
1079,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,8,9ymfqm,self.MachineLearning,Gaussian mixtures and decomposition,https://www.reddit.com/r/MachineLearning/comments/9ymfqm/gaussian_mixtures_and_decomposition/,freedom_tacos,1542671267,"I would like to partition my multivariate features into disjoint sets, each based on an estimated component Gaussian. The sets should be relatively uniform in size.

Would GMMs be the most logical way to estimate component Gaussians?",0,1,False,self,,,,,
1080,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,9,9ymrwg,self.MachineLearning,[Project]Genetic Algorithm that learns to play snake.,https://www.reddit.com/r/MachineLearning/comments/9ymrwg/projectgenetic_algorithm_that_learns_to_play_snake/,Madamin_Z,1542673644,"Hi guys! I've created a genetic algorithm that learns to play snake. It took 30 generations to reach 123 points. I used C++ and SFML.

Here's the video: [Link!](https://www.youtube.com/watch?v=t_7En99qRig)",3,1,False,self,,,,,
1081,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,9,9ymsyc,self.MachineLearning,[R] Toward a deeper understanding of the way AI agents see things,https://www.reddit.com/r/MachineLearning/comments/9ymsyc/r_toward_a_deeper_understanding_of_the_way_ai/,ndha1995,1542673853,"From Facebook AI Research: https://code.fb.com/ai-research/ai-agents-see/

&gt; A study of language learning in which AI agents learn to communicate about images by exchanging symbols. The surprising finding is that the agents arent developing an understanding of the relationship between images and words...",0,1,False,self,,,,,
1082,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,9,9ymtqb,medium.com,Name Flip-Flop: NIPS Is Now NeurIPS,https://www.reddit.com/r/MachineLearning/comments/9ymtqb/name_flipflop_nips_is_now_neurips/,trcytony,1542673995,,0,1,False,default,,,,,
1083,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,9,9ymxzq,self.MachineLearning,[P] Ganbreeder: collaborative art tool for discovering images based on breeding latent vectors of BigGAN,https://www.reddit.com/r/MachineLearning/comments/9ymxzq/p_ganbreeder_collaborative_art_tool_for/,hardmaru,1542674853,"[Ganbreeder](http://ganbreeder.app)

*Ganbreeder is a collaborative art tool for discovering images. Images are 'bred' by having children, mixing with other images and being shared via their URL. This is an experiment in using breeding + sharing as methods of exploring high complexity spaces. GAN's are simply the engine enabling this. Ganbreeder is very similar to, and named after, [Picbreeder](http://picbreeder.org). It is also inspired by an earlier project of mine [Facebook Graffiti](http://www.joelsimon.net/facebook-graffiti.html) which demonstrated the creative capacity of crowds. Ganbreeder uses these [BigGAN models](https://tfhub.dev/deepmind/biggan-128/2) and the [source code](https://github.com/joel-simon/ganbreeder) is available.*

http://ganbreeder.app
",12,1,False,self,,,,,
1084,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,10,9yn4z9,self.MachineLearning,Identifying Bias in AI using Simulation,https://www.reddit.com/r/MachineLearning/comments/9yn4z9/identifying_bias_in_ai_using_simulation/,jinpanZe,1542676301,[removed],0,1,False,self,,,,,
1085,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,10,9yndzj,self.MachineLearning,[D] Wanted: Tips/advice as I start a Machine Learning Engineer job at a startup,https://www.reddit.com/r/MachineLearning/comments/9yndzj/d_wanted_tipsadvice_as_i_start_a_machine_learning/,bigrichardenergy,1542678220,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning),

A month ago, I began my role as a data scientist/machine learning engineer at an early-stage startup. I'm working on some NLP stuff that I'm really excited about. More on my background for context:

* Prior, I've had 1 year of work experience, but not doing machine learning (more product analytics stuff)
* Math + CS undergrad, did an ML class and had a few projects here and there, including \~ 1 year of research

As this is my first time \[1\] doing ML in industry, and \[2\] working in an engineering capacity in industry, I'm looking for some advice to act on right now what will be a valuable investment for the future. Here's what I'm aware of so far...

Engineering specific stuff:

* Make sure your code is easy to read
* Make your code modular with easy-to-understand abstractions
* Configure your dev environment and get that sorted out sooner to be more productive
* Learn as much as you can from those around you (a bit hard since we're a small team, but my boss is really knowledgable and a great mentor)

ML-specific stuff:

* Read tons of ML, deep learning, and NLP papers
* Implement some of the key ideas on your own to get a good sense of how it really works.

I would love to hear advice from anyone on this. Thanks.

(Please let me know if there is a more appropriate sub for this)",18,1,False,self,,,,,
1086,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,12,9yo4ss,self.MachineLearning,Autonomous cars and adversarial attacks,https://www.reddit.com/r/MachineLearning/comments/9yo4ss/autonomous_cars_and_adversarial_attacks/,osm3000,1542683863,[removed],0,1,False,self,,,,,
1087,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,13,9yomto,harsh-vardhan.com,The Perceptron Algorithm Explained!,https://www.reddit.com/r/MachineLearning/comments/9yomto/the_perceptron_algorithm_explained/,harshv08,1542687814,,0,1,False,default,,,,,
1088,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,13,9yoqwd,self.MachineLearning,[R] Identifying Bias in AI using Simulation,https://www.reddit.com/r/MachineLearning/comments/9yoqwd/r_identifying_bias_in_ai_using_simulation/,jinpanZe,1542688745,"[https://openreview.net/forum?id=BJf\_YjCqYX](https://openreview.net/forum?id=BJf_YjCqYX)

&amp;#x200B;

 **TL;DR:** We present a framework that leverages high-fidelity computer simulations to interrogate and diagnose biases within ML classifiers.  ",0,1,False,self,,,,,
1089,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,15,9ypf8g,self.MachineLearning,Different Techniques of Machine Learning:,https://www.reddit.com/r/MachineLearning/comments/9ypf8g/different_techniques_of_machine_learning/,Learntek12,1542694253,[removed],0,1,False,self,,,,,
1090,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,15,9ypgq0,developer.amazon.com,Varying Speaking Styles with Neural Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/9ypgq0/varying_speaking_styles_with_neural_texttospeech/,nyumaya,1542694605,,0,1,False,https://b.thumbs.redditmedia.com/nUbjwVSQJ9c9q5VuqjGiFeyk6smCoSloc0_HpA4P6gQ.jpg,,,,,
1091,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,15,9yplil,github.com,"Latest TensorFlow + CUDA pip wheels with performance-optimized flags (SSE, AVX, FMA, XLA, MPI, jemallocI)",https://www.reddit.com/r/MachineLearning/comments/9yplil/latest_tensorflow_cuda_pip_wheels_with/,Inori,1542695748,,0,1,False,default,,,,,
1092,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,15,9ypmbx,github.com,"[P] Latest TensorFlow + CUDA pip wheels with performance-optimized flags (SSE, AVX, FMA, XLA, MPI, jemallocI)",https://www.reddit.com/r/MachineLearning/comments/9ypmbx/p_latest_tensorflow_cuda_pip_wheels_with/,Inori,1542695929,,0,1,False,default,,,,,
1093,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,15,9ypo82,self.MachineLearning,"[P] Latest TensorFlow + CUDA pip wheels with performance-optimized flags (SSE, AVX, FMA, XLA, MPI, jemallocI)",https://www.reddit.com/r/MachineLearning/comments/9ypo82/p_latest_tensorflow_cuda_pip_wheels_with/,Inori,1542696389,"Since TensorFlow team decided to postpone move to CUDA 10.0 for later releases, figured I'd share my custom builds since they take awhile to compile. https://github.com/inoryy/tensorflow-optimized-wheels

The builds enable SSE4, AVX2, FMA, XLA, MPI, jemallocI optimizations. If you have a CPU released after ~2013 then you'll benefit from these. Note that you'll benefit from these even if you do all your training on GPU due to i/o pipeline optimizations. I think I've gained about 10-15% performance boost on U-Nets. And of course in CPU only setting they give significant improvement, sometimes even matching GPU speeds on smaller neural networks (especially true for laptops where even in higher end models GPUs tend to lag behind).

If you need a different TF + CUDA + CuDNN combination, let me know here or on GitHub and I'll compile it as soon as I have resources available.",14,1,False,self,,,,,
1094,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,15,9ypofq,blog.singularitynet.io,Latest stories published on SingularityNET,https://www.reddit.com/r/MachineLearning/comments/9ypofq/latest_stories_published_on_singularitynet/,TraditionalSpread9,1542696437,,0,1,False,default,,,,,
1095,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,15,9ypoy0,self.MachineLearning,Gaze Augmentation. Is that a thing?,https://www.reddit.com/r/MachineLearning/comments/9ypoy0/gaze_augmentation_is_that_a_thing/,UnfazedButDazed,1542696558,[removed],0,1,False,self,,,,,
1096,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,16,9ypt1z,self.MachineLearning,[R] Schmidhuber's new blog post on Unsupervised Adversarial Neural Networks and Artificial Curiosity in Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/9ypt1z/r_schmidhubers_new_blog_post_on_unsupervised/,baylearn,1542697592,"# [Unsupervised Neural Networks Fight in a Minimax Game](http://people.idsia.ch/~juergen/unsupervised-neural-nets-fight-minimax-game.html)

[Jrgen Schmidhuber](http://www.idsia.ch/~juergen) (2018) 
(Pronounce: *You_again Shmidhoobuh*) 

*One of the most important goals of research on [Artificial Neural Networks (NNs)](http://people.idsia.ch/~juergen/deeplearning.html) is to create algorithms that learn the statistics of given data. To achieve this, I introduced a new type of unsupervised learning in the 1990s. It is based on the principles of gradient descent/ascent in a minimax game where one NN minimizes the objective function maximized by another. This duel between two unsupervised adversarial NNs was called [Predictability Minimization](ftp://ftp.idsia.ch/pub/juergen/edgedetect.pdf) (PM)*

Read more: http://people.idsia.ch/~juergen/unsupervised-neural-nets-fight-minimax-game.html
",74,1,False,self,,,,,
1097,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,16,9ypwex,self.MachineLearning,Need some resources on Latent Factor models,https://www.reddit.com/r/MachineLearning/comments/9ypwex/need_some_resources_on_latent_factor_models/,badatmathmajor,1542698461,[removed],0,1,False,self,,,,,
1098,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,17,9yq7vy,self.MachineLearning,[R] learned heuristics for A*,https://www.reddit.com/r/MachineLearning/comments/9yq7vy/r_learned_heuristics_for_a/,skariel,1542701637,"A\* is a popular path-finding algorithm, but it can only be applied to those domains where a good heuristic function is known. Inspired by recent methods combining Deep Neural Networks (DNNs) and trees, this study demonstrates how to train a heuristic represented by a DNN and combine it with A\*. This new algorithm which we call aleph-star can be used efficiently in domains where the input to the heuristic could be processed by a neural network. We compare aleph-star to N-Step Deep Q-Learning (DQN Mnih et al. 2013) in a driving simulation with pixel-based input, and demonstrate significantly better performance in this scenario.

[paper (arXiv abs)](https://arxiv.org/abs/1811.07745)   [code](https://github.com/imagry/aleph_star)",4,1,False,self,,,,,
1099,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,17,9yqbmg,self.MachineLearning,Best place to advertise ML/CV jobs?,https://www.reddit.com/r/MachineLearning/comments/9yqbmg/best_place_to_advertise_mlcv_jobs/,ransac-boy,1542702722,[removed],0,1,False,self,,,,,
1100,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,17,9yqcqx,self.MachineLearning,"Global Artificial Intelligence in Healthcare Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/9yqcqx/global_artificial_intelligence_in_healthcare/,BipinMandal,1542703036,"The global AI in the healthcare market will be growing with a CAGR of 48.7% to reach $XX Billion by 2025 from $XX Billion in 2019 during the forecast period 2019-2025. The prominent factors leading the AI in healthcare market growth are the ability of AI to improve patient outcomes, rising adoption of precision medicine, growing need to increase coordination between healthcare workforce &amp; patients, and increasing venture capital investments.  

&amp;#x200B;

Download a sample report at:-  https://www.envisioninteligence.com/industry-report/global-artificial-intelligence-in-healthcare-market/?utm\_source=Reddit-Santhosh ",0,1,False,self,,,,,
1101,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,17,9yqdk6,youtube.com,Wanna Nails  Manicure Try On,https://www.reddit.com/r/MachineLearning/comments/9yqdk6/wanna_nails_manicure_try_on/,darya_sesitskaya,1542703267,,0,1,False,default,,,,,
1102,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,17,9yqe1h,self.MachineLearning,"[D] If you had to show one paper to someone to show that machine learning is beautiful, what would you choose? (assuming they're equipped to understand it)",https://www.reddit.com/r/MachineLearning/comments/9yqe1h/d_if_you_had_to_show_one_paper_to_someone_to_show/,bansaly397bansaly397,1542703409,,0,1,False,default,,,,,
1103,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,17,9yqgp0,i.redd.it,Artificial intelligence could end mankind by sir Stephen Hawking. What you think friends ?,https://www.reddit.com/r/MachineLearning/comments/9yqgp0/artificial_intelligence_could_end_mankind_by_sir/,bansaly397bansaly397,1542704207,,0,1,False,https://a.thumbs.redditmedia.com/NInXsDIl6ACFjOkl6F2llUOTYMXdJcEZdvzavSHS5M4.jpg,,,,,
1104,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,17,9yqh8e,self.MachineLearning,A Machine That Can Finish Your Sentence,https://www.reddit.com/r/MachineLearning/comments/9yqh8e/a_machine_that_can_finish_your_sentence/,Easy_Economist,1542704364,[removed],0,1,False,self,,,,,
1105,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,18,9yqnvg,medium.com,Introduction to Women in Fintech Series,https://www.reddit.com/r/MachineLearning/comments/9yqnvg/introduction_to_women_in_fintech_series/,agilequirk,1542706253,,0,1,False,default,,,,,
1106,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,18,9yqo0d,self.MachineLearning,Looking on good papers/books to start on Speech Synthesis?,https://www.reddit.com/r/MachineLearning/comments/9yqo0d/looking_on_good_papersbooks_to_start_on_speech/,thinking_tower,1542706287,"Hi everyone, I've been thinking about doing post-grad on Speech Synthesis (particularly Text-To-Speech) but I don't really know where to start. A reference book I've seen is [Paul Taylor's Text-To-Speech Synthesis](https://www.amazon.com/Text-Speech-Synthesis-Paul-Taylor/dp/0521899273) but it seems to be ""outdated"" in the sense that it was posted nearly 10 years ago.

Any help would be appreciated!
",0,1,False,self,,,,,
1107,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,18,9yqot9,self.MachineLearning,Artificial Intelligence Job Opportunities,https://www.reddit.com/r/MachineLearning/comments/9yqot9/artificial_intelligence_job_opportunities/,naveen800,1542706519,[removed],0,1,False,self,,,,,
1108,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,18,9yqswp,self.MachineLearning,[D] Which groups/variables are you prohibited to use in finance machine learning models because of anti-discrimination laws in US &amp; UK?,https://www.reddit.com/r/MachineLearning/comments/9yqswp/d_which_groupsvariables_are_you_prohibited_to_use/,MiksLus,1542707703,,3,1,False,self,,,,,
1109,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,19,9yr1zd,self.MachineLearning,Are Artificial Intelligence (AI) and Machine Learning the same?,https://www.reddit.com/r/MachineLearning/comments/9yr1zd/are_artificial_intelligence_ai_and_machine/,Anu2008,1542710242,[removed],0,1,False,self,,,,,
1110,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,19,9yr3f9,self.MachineLearning,[R] What to do when your bag of tricks is empty?,https://www.reddit.com/r/MachineLearning/comments/9yr3f9/r_what_to_do_when_your_bag_of_tricks_is_empty/,seckarr,1542710661,[removed],0,1,False,self,,,,,
1111,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,20,9yrddd,linkedin.com,Artificial Intelligence Will Create More Jobs In 2020,https://www.reddit.com/r/MachineLearning/comments/9yrddd/artificial_intelligence_will_create_more_jobs_in/,indrajeet_Sony,1542713522,,0,1,False,default,,,,,
1112,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,20,9yrdef,self.MachineLearning,[D] I made a horrible mistake in my CVPR submission! No chance?!,https://www.reddit.com/r/MachineLearning/comments/9yrdef/d_i_made_a_horrible_mistake_in_my_cvpr_submission/,babak_hss,1542713530,"I recently have submitted a CVPR conference  paper and made a horrible mistake! 

I finished the remaining part of the paper once day before the deadline, and i took a copy of the files just in case anything goes wrong. 

Then i started to take care of the typo/grammar mistakes and polishing the structure (paragraphs, figures and tables places).

When i finished i submitted the final version.  But the next day, when i checked the submitted version i got shocked!
I had submitted the version before the pulishing! the one that i copied for safety!!

The portal is closed and the organizers won't allow any more updates ti the paper.  Only submitting a supplementary file is allowed for which I'm not allowed to submit the paper again. 

My guess is when the reviewers see an unpolished paper with many mistakes they would get a strong bias toward rejecting it!

No solution, is that right? ",4,1,False,self,,,,,
1113,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,20,9yrduy,self.MachineLearning,[D] Deep Learning and Confidence on Unseen Distributions of Data,https://www.reddit.com/r/MachineLearning/comments/9yrduy/d_deep_learning_and_confidence_on_unseen/,audio-nerd,1542713652,"Hi all, I've been writing some neural networks for various tasks and I assume like many before me I have come to a point where I need some measure of confidence in my predictions, especially when running inference on distributions of data that were not seen during training.

&amp;#x200B;

I'd like to take a second to ask the experts on this subreddit - what are the options and implementations currently out there? I have tried and implemented Monte Carlo Dropout as a Bayesian approach, which yielded mixed results when unseen data was used. 

&amp;#x200B;

Are there any other successful approaches that you are aware of?

Thanks for reading! 

&amp;#x200B;

&amp;#x200B;",6,1,False,self,,,,,
1114,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,20,9yriod,self.MachineLearning,Recommendations for applying neural networks to transactional data,https://www.reddit.com/r/MachineLearning/comments/9yriod/recommendations_for_applying_neural_networks_to/,ProjectPsygma,1542714962,[removed],0,1,False,self,,,,,
1115,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,21,9yrmac,self.MachineLearning,[D] Inverting embeddings,https://www.reddit.com/r/MachineLearning/comments/9yrmac/d_inverting_embeddings/,Valiox,1542715869,"In general sequence-to-sequence models, we typically encode symbols using embeddings (rather than one-hot representation) but still aim for a one-hot output using softmax. Retrieving symbol predictions from such a network is done in O(n) operations, where n is the number of symbols.

If we were to predict embeddings instead, inverting the approximate embedding is not only non-trivial (one can't just assume that L2 distance between embeddings is the right distance metric) but also computationally more expensive with, I believe, O(nm) operations where m is the embedding size.

What would be the pros and cons of using the second approach? Would networks train better with an embedding as target rather than a one-hot encoding?",3,1,False,self,,,,,
1116,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,21,9yrqow,self.MLQuestions,How does NEAT handle cycles in a network?,https://www.reddit.com/r/MachineLearning/comments/9yrqow/how_does_neat_handle_cycles_in_a_network/,mazalan01,1542716988,,0,1,False,https://b.thumbs.redditmedia.com/oP26wsAAAkVdrWPLJqKcnThxaS1cijUHtW-ZpYFJHpw.jpg,,,,,
1117,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,22,9yrynm,self.MLQuestions,How does NEAT handle cycles in a network?,https://www.reddit.com/r/MachineLearning/comments/9yrynm/how_does_neat_handle_cycles_in_a_network/,mazalan01,1542718937,,0,1,False,https://b.thumbs.redditmedia.com/oP26wsAAAkVdrWPLJqKcnThxaS1cijUHtW-ZpYFJHpw.jpg,,,,,
1118,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,22,9ys2tw,self.MachineLearning,How machine learning APIs are impacting businesses?,https://www.reddit.com/r/MachineLearning/comments/9ys2tw/how_machine_learning_apis_are_impacting_businesses/,andrea_manero,1542719873,[removed],0,1,False,self,,,,,
1119,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,22,9ys3s5,sgfin.github.io,Curated list of many online ML resources,https://www.reddit.com/r/MachineLearning/comments/9ys3s5/curated_list_of_many_online_ml_resources/,samfin55,1542720103,,0,1,False,default,,,,,
1120,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,22,9ysb8e,self.MachineLearning,[D]Machine learning on with microcontroller and sensor?,https://www.reddit.com/r/MachineLearning/comments/9ysb8e/dmachine_learning_on_with_microcontroller_and/,burton6666,1542721725,"I a a beginner and want to try to learn some ML by using it in a project. I want to use a ESP32 microcontroller with an AMG8833 sensor. The sensor returns a 8x8 array with temperatures.

I want to be able to detect humans from the array-data and also movement. Is this something that would be possible for a beginner?  I also noticed that you could install Amazon FreeRTOS on the ESP32 and use amazons machine learning someway.",1,1,False,self,,,,,
1121,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9ysee1,tenfifty.io,Using probabilistic programming with Pytorch and Pyro to prove Poland was robbed at the 2018 Chess Olympiad,https://www.reddit.com/r/MachineLearning/comments/9ysee1/using_probabilistic_programming_with_pytorch_and/,fnedrik,1542722413,,0,1,False,default,,,,,
1122,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9ysh7f,self.MachineLearning,[P] Generating an artificial alphabet/letters?,https://www.reddit.com/r/MachineLearning/comments/9ysh7f/p_generating_an_artificial_alphabetletters/,jatsignwork,1542722962,"I'd like to develop a system to generate artificial letters/symbols/characters.  My current plan for training data is to gather characters from ancient languages like Sanskrit/Akkadian/Pictish/etc, and then feed those into a DCGAN. I'm not concerned with making a ""complete"" alphabet - I'd like the system to generate new ""symbols"" on demand.

- Is a DCGAN the right approach for this? I've seen it used for generating images, but I don't need a ""real"" image, just a character.

- For each existing alphabet I use I'll take each character and flip/rotate each symbol to generate more data. Are there better/more ways to increase the size of the training set?

- Am I making this harder than it needs to be? Is there a simpler, non-NN approach I should look at?",10,1,False,self,,,,,
1123,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9ysi0n,blog.singularitynet.io,Data as Labour,https://www.reddit.com/r/MachineLearning/comments/9ysi0n/data_as_labour/,KlutzyBowl,1542723139,,0,1,False,https://a.thumbs.redditmedia.com/xyRgymIoVgcS_oE5jP2CWzKkQn24iqQZgjxMfkAMyI4.jpg,,,,,
1124,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9ysmtn,self.MachineLearning,[D] Debate on TensorFlow 2.0 API,https://www.reddit.com/r/MachineLearning/comments/9ysmtn/d_debate_on_tensorflow_20_api/,omoindrot,1542724152,"I'm posting here to draw some attention to a debate happening on GitHub over TensorFlow 2.0 [here](https://github.com/tensorflow/community/pull/24).

&amp;#x200B;

The debate is happening in a ""request for comment"" (RFC) over a proposed change to the Optimizer API for TensorFlow 2.0:

* Franois Chollet (author of the proposal) wants to merge optimizers in `tf.train` with optimizers in `tf.keras.optimizers` and **only keep** `tf.keras.optimizers`**.**
* Other people (including me) have been arguing against this proposal. The main point is that Keras should not be prioritized over TensorFlow, and that they should at least keep an alias to the optimizers in `tf.train` or tf.optimizers (the same debate happens over `tf.keras.layers` / `tf.layers`, `tf.keras.metrics` / `tf.metrics`...).

I think this is an important change to TensorFlow that should involve its users, and hope this post will provide more visibility to the [pull request](https://github.com/tensorflow/community/pull/24).",118,1,False,self,,,,,
1125,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9yspaf,self.MachineLearning,In need of professional advice,https://www.reddit.com/r/MachineLearning/comments/9yspaf/in_need_of_professional_advice/,shomerj,1542724670,[removed],0,1,False,self,,,,,
1126,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9ysrqz,self.MachineLearning,To err is machine To forgive is Human ?,https://www.reddit.com/r/MachineLearning/comments/9ysrqz/to_err_is_machine_to_forgive_is_human/,shandanjay,1542725171,"&amp;#x200B;

[Youtube recommendation engine's job in risk ;\) ](https://i.redd.it/xzbf9vu20iz11.png)",0,1,False,self,,,,,
1127,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9yssew,insights.sei.cmu.edu,Translating Between Statistics and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9yssew/translating_between_statistics_and_machine/,BillyPricePgh,1542725303,,0,1,False,default,,,,,
1128,MachineLearning,t5_2r3gv,2018-11-20,2018,11,20,23,9yst40,linkedin.com,Arm Always-On Mobile Face Unlock Achieves Over 98% Accuracy,https://www.reddit.com/r/MachineLearning/comments/9yst40/arm_alwayson_mobile_face_unlock_achieves_over_98/,Monakons,1542725436,,0,1,False,default,,,,,
1129,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,0,9yt7cw,arxiv.org,[R] Bayesian Gradient Descent: Online Variational Bayes Learning with Increased Robustness to Catastrophic Forgetting and Weight Pruning,https://www.reddit.com/r/MachineLearning/comments/9yt7cw/r_bayesian_gradient_descent_online_variational/,abstractcontrol,1542728180,,5,1,False,default,,,,,
1130,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,0,9ytb2d,self.MachineLearning,Predict age and gender using convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/9ytb2d/predict_age_and_gender_using_convolutional_neural/,champianalien21,1542728891,[removed],0,1,False,self,,,,,
1131,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,0,9ytd5a,self.MachineLearning,Forecasting Hidden Markov Models,https://www.reddit.com/r/MachineLearning/comments/9ytd5a/forecasting_hidden_markov_models/,mestermestermester,1542729288,[removed],0,1,False,self,,,,,
1132,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,0,9ytejb,self.MachineLearning,Predict age and gender using convolutional Neutral Network and opencv,https://www.reddit.com/r/MachineLearning/comments/9ytejb/predict_age_and_gender_using_convolutional/,champianalien21,1542729542,[removed],0,1,False,self,,,,,
1133,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,1,9ytflr,insights.sei.cmu.edu,[D] Translating Between Statistics and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9ytflr/d_translating_between_statistics_and_machine/,BillyPricePgh,1542729725,,7,1,False,default,,,,,
1134,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,1,9ytg06,self.MachineLearning,[P] Chapter 6: Neural Networks and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/9ytg06/p_chapter_6_neural_networks_and_deep_learning/,RudyWurlitzer,1542729790,"The draft of the sixth chapter, ""Neural Networks and Deep Learning"", of my upcoming The Hundred-Page Machine Learning book is online. 

This was the hardest one to write because I had to sacrifice a lot to keep it short and generic. For example, in convolutional neural networks, I decided not to explain pooling as it isn't strictly necessary for them to work. I only mention strides and padding as two important hyperparameters to tune.  Overall, neural networks, especially specialized ones like CNN or seq2seq have so many moving parts that it doesn't make any practical sense to try to describe all of them in detail in one chapter of a hundred page book.

The book is available at http://themlbook.com. You can subscribe on the website to receive updates on new chapters by email.

I really count on you guys to help me to improve this chapter. The success criterion is clarity for an uninitiated reader.",3,1,False,self,,,,,
1135,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,1,9ytunt,self.MachineLearning,[R] Summary of Exploration By Random Network Distillation - Curiosity-Driven Learning by OpenAI,https://www.reddit.com/r/MachineLearning/comments/9ytunt/r_summary_of_exploration_by_random_network/,ranihorev,1542732407,"Developing curiosity is one of the biggest challenges in Reinforcement Learning. 

 I wrote a summary on a simple, but genius, approach by OpenAI - Exploration By Random Network Distillation. Their goal is to teach RL agents to discover and explore new states without external feedback. They developed an intrinsic reward mechanism that discourages the agent from visiting known (previously visited) states over and over again. 

[https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/](https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/)

&amp;#x200B;

**Sam:** This looks strangely familiar.

**Frodo:** We've been here before, we're going in circles.",0,1,False,self,,,,,
1136,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu0of,self.MachineLearning,Project Ideas for Blockchain+ML,https://www.reddit.com/r/MachineLearning/comments/9yu0of/project_ideas_for_blockchainml/,harshv08,1542733464,[removed],0,1,False,self,,,,,
1137,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu0w7,self.MachineLearning,How does this algorithm work to detect a difference between spam and non spam messages,https://www.reddit.com/r/MachineLearning/comments/9yu0w7/how_does_this_algorithm_work_to_detect_a/,hiya19922,1542733498,[removed],0,1,False,self,,,,,
1138,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu1d2,arxiv.org,[R] Practical Bayesian Learning of Neural Networks via Adaptive Subgradient Methods,https://www.reddit.com/r/MachineLearning/comments/9yu1d2/r_practical_bayesian_learning_of_neural_networks/,abstractcontrol,1542733575,,6,1,False,default,,,,,
1139,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu2bs,self.MachineLearning,[D] Machine Learning Certifications,https://www.reddit.com/r/MachineLearning/comments/9yu2bs/d_machine_learning_certifications/,wongy-,1542733740,What are some of the most recognized online certifications currently out there for machine learning and deep learning?,6,1,False,self,,,,,
1140,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu2qs,self.MachineLearning,https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84,https://www.reddit.com/r/MachineLearning/comments/9yu2qs/httpsmediumcomdeepmindsafetyresearchscalableagenta/,eb642,1542733813,[removed],0,1,False,self,,,,,
1141,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu3cf,arxiv.org,GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint,https://www.reddit.com/r/MachineLearning/comments/9yu3cf/ganqp_a_novel_gan_framework_without_gradient/,sujianlin,1542733914,,36,1,False,default,,,,,
1142,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu4zz,medium.com,Scalable agent alignment via reward modeling  DeepMind Safety Research,https://www.reddit.com/r/MachineLearning/comments/9yu4zz/scalable_agent_alignment_via_reward_modeling/,i_am_squishy,1542734200,,0,1,False,default,,,,,
1143,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yu7dc,self.MachineLearning,[R] Exploration By Random Network Distillation - State-of-the-art RL performance on Montezumas Revenge,https://www.reddit.com/r/MachineLearning/comments/9yu7dc/r_exploration_by_random_network_distillation/,ranihorev,1542734644,"Developing curiosity is one of the biggest challenges in Reinforcement Learning.

I wrote a summary on a simple, but genius, approach by OpenAI - Exploration By Random Network Distillation. Their goal is to teach RL agents to discover and explore new states without external feedback. They developed an intrinsic reward mechanism that discourages the agent from visiting known (previously visited) states over and over again.

[https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/](https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/)

&amp;#x200B;

**Sam:** This looks strangely familiar.

**Frodo:** We've been here before, we're going in circles.",2,1,False,self,,,,,
1144,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,2,9yueas,self.MachineLearning,Project Ideas for ML+Blockchain,https://www.reddit.com/r/MachineLearning/comments/9yueas/project_ideas_for_mlblockchain/,harshv08,1542735859,[removed],0,1,False,self,,,,,
1145,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,3,9yupcu,sinxloud.com,"If you want to break into AI, these Deep Learning Courses will help you do so",https://www.reddit.com/r/MachineLearning/comments/9yupcu/if_you_want_to_break_into_ai_these_deep_learning/,skj8,1542737739,,0,1,False,https://b.thumbs.redditmedia.com/ZJpMUHzVkAXhny22-qAXBxUzHLdoklCXV3u2laxm0QU.jpg,,,,,
1146,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,3,9yuwmk,modeldepot.io,"[P] Search engine for 20,000+ machine learning models",https://www.reddit.com/r/MachineLearning/comments/9yuwmk/p_search_engine_for_20000_machine_learning_models/,es6masterrace,1542738989,,0,1,False,default,,,,,
1147,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,3,9yv0oq,arxiv.org,[R] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism (new ImageNet SOTA),https://www.reddit.com/r/MachineLearning/comments/9yv0oq/r_gpipe_efficient_training_of_giant_neural/,modeless,1542739693,,12,1,False,default,,,,,
1148,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yv618,discourse.opengenus.org,When to use Multilayer Perceptrons (MLP)?,https://www.reddit.com/r/MachineLearning/comments/9yv618/when_to_use_multilayer_perceptrons_mlp/,code_like_tiger,1542740607,,0,1,False,default,,,,,
1149,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yv97s,self.MachineLearning,[D] Question about image representation learning,https://www.reddit.com/r/MachineLearning/comments/9yv97s/d_question_about_image_representation_learning/,notevencrazy99,1542741138,"I'm working on a project on which I want to do representation learning to cluster similar images together in order to speed label them manually.

I just had the idea of using a CNN as a feature extractor and train it to maximize the embedding space distance between the images. 

The way I'm thinking of framing this, is similar to a Triplet Loss, only without the Anchor concept. I would mine ""hard"" examples, like FaceNet.

I was looking if there was literature with an approach similar to this, but found nothing. All I found, was DCGANs and related, but I failed to see why my above suggestion would fail to deliver my expectation.

Know of something I may be missing?

&amp;#x200B;

&amp;#x200B;",6,1,False,self,,,,,
1150,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yvayz,self.MachineLearning,textGAN,https://www.reddit.com/r/MachineLearning/comments/9yvayz/textgan/,tayyabikhlaq,1542741425,[removed],0,1,False,self,,,,,
1151,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yvf3h,self.MachineLearning,"Is there newsletter, website, whatever source that describes how ML is changing industries, use cases I can read about?",https://www.reddit.com/r/MachineLearning/comments/9yvf3h/is_there_newsletter_website_whatever_source_that/,erjcan,1542742159,[removed],0,1,False,self,,,,,
1152,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yvg9l,medium.com,Natural Language Processing Simplified: What Are You!?,https://www.reddit.com/r/MachineLearning/comments/9yvg9l/natural_language_processing_simplified_what_are/,BlairRoslyn,1542742367,,0,1,False,https://b.thumbs.redditmedia.com/9oWLp7WfrvFS88OoR5AIQNJK5fkHZlWwl-4sWTq5MEg.jpg,,,,,
1153,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yvh85,self.MachineLearning,Is there any machine learning decision making algorithm available in MATLAB?,https://www.reddit.com/r/MachineLearning/comments/9yvh85/is_there_any_machine_learning_decision_making/,alisutton,1542742545,"For example, it will learn to make decision on stock trading",0,1,False,self,,,,,
1154,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yvhhf,self.MachineLearning,How to handle a changing action space in Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/9yvhhf/how_to_handle_a_changing_action_space_in/,MrSh4nnon,1542742591,[removed],0,1,False,self,,,,,
1155,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yvi4f,stanfordmlgroup.github.io,[R] CheXNeXt: Deep learning for chest radiograph diagnosis,https://www.reddit.com/r/MachineLearning/comments/9yvi4f/r_chexnext_deep_learning_for_chest_radiograph/,psr10000,1542742705,,0,1,False,default,,,,,
1156,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,4,9yvp38,i.redd.it,Reuso de Resduos IV,https://www.reddit.com/r/MachineLearning/comments/9yvp38/reuso_de_resduos_iv/,JamurGerloff,1542743896,,0,1,False,https://b.thumbs.redditmedia.com/9XkZRv3jVtzjWX45bOA1iEJ50QS8LqzffXiXJWZxD2A.jpg,,,,,
1157,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,5,9yvv0l,self.MachineLearning,ML/Threat Detection Challenge,https://www.reddit.com/r/MachineLearning/comments/9yvv0l/mlthreat_detection_challenge/,WallarmG,1542744918,[removed],0,1,False,self,,,,,
1158,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,7,9ywvqo,arxiv.org,[R] Learned Video Compression,https://www.reddit.com/r/MachineLearning/comments/9ywvqo/r_learned_video_compression/,non_psd_matrix,1542751387,,0,1,False,default,,,,,
1159,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,7,9yxbfa,self.MachineLearning,[D] DevOps Tools for Managing Models?,https://www.reddit.com/r/MachineLearning/comments/9yxbfa/d_devops_tools_for_managing_models/,Simusid,1542754315,"I've been successful enough with supervised image classification that I'm now rolling out an experiment into what I consider to be limited production.   I have a model running on an r-pi Zero in 3 locations, taking pics and doing successful classification.    The nodes act locally but can send their imagery back to my development server.  This works really well.

So I have 5 models in use.   I started with just a dev model on my dev server.  When that trained well I pushed it to node 1.   It worked and gathered imagery that I collected and retrained on dev.   I pushed that back to node 1 and to node 2.   Then node 3 came online.   So thats a dev model, a production model on dev, and 3 production models on nodes.   I know this can all be scripted but it's becoming a pain.

Hypothetically, suppose I scale to 50 or 100 nodes.   I suspect that I would see more variance from a single model pushed to all.   That means gathering more imagery, retraining and pushing to all.   That may make sense.   Or I may find a subset of production locations that benefit from a slightly different model.   So then I'll have production model-A and model-B, which means I'll have development model A/B as well.   Again, I could write a tool to ""push"" new models to 100 nodes, but some might fail and have to retry.   What model version is on which node right now?   This could get out of control.

Does anyone know of any DevOps tools to manage a fleet of models?
",5,1,False,self,,,,,
1160,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,7,9yxd0d,self.MachineLearning,Help*beginner*,https://www.reddit.com/r/MachineLearning/comments/9yxd0d/helpbeginner/,Draxinel,1542754630,[removed],0,1,False,self,,,,,
1161,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,7,9yxdmg,self.MachineLearning,Learning latent vectors for a given image in GANs?,https://www.reddit.com/r/MachineLearning/comments/9yxdmg/learning_latent_vectors_for_a_given_image_in_gans/,pikachuchameleon,1542754761,[removed],0,1,False,self,,,,,
1162,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,8,9yxhu0,arxiv.org,[R] Learned Video Compression,https://www.reddit.com/r/MachineLearning/comments/9yxhu0/r_learned_video_compression/,non_psd_matrix,1542755581,,0,1,False,default,,,,,
1163,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,9,9yxw90,self.MachineLearning,[R] Automatically going from TensorFlow to FPGA: Demo Video,https://www.reddit.com/r/MachineLearning/comments/9yxw90/r_automatically_going_from_tensorflow_to_fpga/,danielhn1992,1542758552,"Hi,  


Just posted a new demo of LeFlow, which is a tool that I created that automatically goes from Tensorflow to Verilog using Google's XLA and an HLS tool. Let me know what you think.  


[https://www.youtube.com/watch?v=eHeqenSj0VQ](https://www.youtube.com/watch?v=eHeqenSj0VQ)",0,1,False,self,,,,,
1164,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,9,9yxzea,self.MachineLearning,[R] Automatically going from TensorFlow to FPGA: Demo Video,https://www.reddit.com/r/MachineLearning/comments/9yxzea/r_automatically_going_from_tensorflow_to_fpga/,danielhn1992,1542759205,"## Hi,

Just posted a new demo of LeFlow, which is a tool that I created that automatically goes from Tensorflow to Verilog using Google's XLA and an HLS tool. Let me know what you think.  


[LeFlow Quick Draw Demo](https://www.youtube.com/watch?v=eHeqenSj0VQ)

![img](spy7wtd7tkz11)",3,1,False,self,,,,,
1165,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,10,9yye32,arxiv.org,[R] Learned Video Compression,https://www.reddit.com/r/MachineLearning/comments/9yye32/r_learned_video_compression/,non_psd_matrix,1542762324,,4,1,False,default,,,,,
1166,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,10,9yyiu9,self.MachineLearning,How does NEAT handle cycles in a network?,https://www.reddit.com/r/MachineLearning/comments/9yyiu9/how_does_neat_handle_cycles_in_a_network/,mazalan01,1542763354,[removed],0,1,False,self,,,,,
1167,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,12,9yzdoc,self.MachineLearning,DialogueRNN: An Attentive RNN for Emotion Detection in Conversations,https://www.reddit.com/r/MachineLearning/comments/9yzdoc/dialoguernn_an_attentive_rnn_for_emotion/,soujanyaporia,1542770347,[removed],0,1,False,self,,,,,
1168,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,13,9z01b5,self.MachineLearning,"[D] Are you using Polyaxon, MLflow or Kubeflow?",https://www.reddit.com/r/MachineLearning/comments/9z01b5/d_are_you_using_polyaxon_mlflow_or_kubeflow/,m_ke,1542775985,"If so what do you think about it and why did you pick it over the other options?

",1,1,False,self,,,,,
1169,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,13,9z022h,self.MachineLearning,"Machine Learning: Linear Regression, Simply Explained [D]",https://www.reddit.com/r/MachineLearning/comments/9z022h/machine_learning_linear_regression_simply/,ammar-,1542776170,"I wrote a [blog post](http://ammar-alyousfi.com/blog/2018/11/machine-learning-linear-regression-simply/) explaining **linear regression** in the context of machine learning. I kept the post **simple** so it can be useful for beginners.

A basic **Python example** and many **illustrations** are used also.

Here is the link:

[http://ammar-alyousfi.com/blog/2018/11/machine-learning-linear-regression-simply/](http://ammar-alyousfi.com/blog/2018/11/machine-learning-linear-regression-simply/)

If you have any feedback, please leave a comment",20,1,False,self,,,,,
1170,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,14,9z0dzg,scholarspro.com,Six Sigma Black Belt Certification,https://www.reddit.com/r/MachineLearning/comments/9z0dzg/six_sigma_black_belt_certification/,Scholarspro_76,1542778877,,0,1,False,default,,,,,
1171,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,14,9z0gaj,self.MachineLearning,[D] What seems to be a new TF-like framework from Google,https://www.reddit.com/r/MachineLearning/comments/9z0gaj/d_what_seems_to_be_a_new_tflike_framework_from/,inflp,1542779491,"This repo shows up in my timeline:

[https://github.com/google/jax](https://github.com/google/jax)

It seems that they are working with numpy APIs directly without external libraries like cupy, instead relying on tensorflow XLA. The result is an eagerly-executed framework that (IMO) has a much neater interface than TFE / pytorch. e.g. check out the [VAE example](https://github.com/google/jax/blob/master/examples/mnist_vae.py).

And it is strange that this repo is completely undocumented. Can anyone share about the background?",33,1,False,self,,,,,
1172,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,15,9z0k5i,self.MachineLearning,[D] TensorRT-like library for CPU inference only?,https://www.reddit.com/r/MachineLearning/comments/9z0k5i/d_tensorrtlike_library_for_cpu_inference_only/,Xirious,1542780450,"Hi all,

So whilst reading up more on TensorRT I came to realise that for one of my systems it would be highly beneficial if there was a way to do the same type of speed up but on a non-GPU machine for inference?

Hope this make sense and thanks for taking the time to read this! ",6,1,False,self,,,,,
1173,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,15,9z0ke5,appsbee.com,M2M Vs IoT: Must Know Differences,https://www.reddit.com/r/MachineLearning/comments/9z0ke5/m2m_vs_iot_must_know_differences/,appsbee,1542780505,,0,1,False,default,,,,,
1174,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,15,9z0ku3,ezeetest.app,Influence of Machine Learning in Education,https://www.reddit.com/r/MachineLearning/comments/9z0ku3/influence_of_machine_learning_in_education/,vermarajan,1542780627,,0,1,False,default,,,,,
1175,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,15,9z0rgx,youtube.com,How To Chop Macadamia Nuts? - Best Electric Nut Chopper,https://www.reddit.com/r/MachineLearning/comments/9z0rgx/how_to_chop_macadamia_nuts_best_electric_nut/,liusherry,1542782323,,0,1,False,default,,,,,
1176,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,16,9z128t,self.MachineLearning,The PyTorch-Kaldi Speech Recognition Toolkit,https://www.reddit.com/r/MachineLearning/comments/9z128t/the_pytorchkaldi_speech_recognition_toolkit/,moktarletoutgentil,1542785178,[removed],0,1,False,self,,,,,
1177,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,16,9z150k,self.MachineLearning,Global Brushed DC Motors Market Report 2018-2022,https://www.reddit.com/r/MachineLearning/comments/9z150k/global_brushed_dc_motors_market_report_20182022/,bir07,1542785935,[removed],1,1,False,self,,,,,
1178,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,16,9z18qb,self.MachineLearning,[R] Applying neural networks to transactional data,https://www.reddit.com/r/MachineLearning/comments/9z18qb/r_applying_neural_networks_to_transactional_data/,ProjectPsygma,1542787000,"Hi r/MachineLearning,

I recently landed myself a new project at my company to investigate the business use cases and commercial viability of neural networks on structured and semi-structured data. The scope is exploring how deep learning can be applied to raw bank transactions.

The two hypothesis which I would like to validate are based around automating feature engineering:

* Entity embeddings (inspired by word2vec) can be used to generate vectors for customer attributes/features, which can then be used for modelling

* RNNs/LSTMs can be used on raw transaction (sequential) data to train predictive models that are as good as GBMs, but with significantly less domain-specific knowledge

Are there any other areas that you would recommend for me to look into for my investigation into how neural networks can be applied to raw bank transaction data?

Cheers,
PS",5,1,False,self,,,,,
1179,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,17,9z1cvc,self.MachineLearning,[Discussion] What is your workflow for reading papers?,https://www.reddit.com/r/MachineLearning/comments/9z1cvc/discussion_what_is_your_workflow_for_reading/,ml_explorer,1542788218,"Here are some features Id want, but Id also like to hear what features you enjoy:

* Capture my thoughts on various parts of a paper
* Document which papers I have read
* Be able to search in papers I have read (not super necessary, but would be nice)
* Recommend papers (not super necessary, but would be nice)

What is your workflow?",30,1,False,self,,,,,
1180,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,17,9z1idj,self.MachineLearning,Predicting Flavor bases on Ingredients,https://www.reddit.com/r/MachineLearning/comments/9z1idj/predicting_flavor_bases_on_ingredients/,an0nVader,1542789961,[removed],0,1,False,self,,,,,
1181,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,17,9z1kvh,self.MachineLearning,Global Utility Drones Market Report 2018-2022,https://www.reddit.com/r/MachineLearning/comments/9z1kvh/global_utility_drones_market_report_20182022/,bir07,1542790775,Businessindustryreports have new report on Global Utility Drones Market 2018-2022. The report provides the newest industry data and industry future trends. The industry report lists the leading competitors and provides the insights strategic industry Analysis of the key factors influencing the market. ,1,1,False,self,,,,,
1182,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,18,9z1qoo,self.MachineLearning,Active Learning not working?,https://www.reddit.com/r/MachineLearning/comments/9z1qoo/active_learning_not_working/,bertthehulk,1542792495,"Hey all, I hope I am not posting this on the wrong sub!

&amp;#x200B;

I was experimenting with implementing some Active Learning in Python using PyTorch.

&amp;#x200B;

I have stolen the network and dataset from the example given on the PyTorch website: [https://pytorch.org/tutorials/intermediate/char\_rnn\_classification\_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)

&amp;#x200B;

I have edited this example so that it does proper pool-based learning (whenever a sample is queried for training, it is then removed from the pool. The confusion matrix I now calculate by testing all the samples that were not used in training, instead of the random samples as given in the tutorial).

&amp;#x200B;

Now, I wanted to implement some Active Learning and see the effects on the accuracy. However, when I choose training samples using Query by Committee (define a dictionary of RNN's, each with their own train and evaluate function, initialize each of these with 10 different random samples, and query based on Maximum Vote Entropy), the accuracy actually goes down instead of up (I took the average over 20 runs, with the amount of samples taken from 100 to 1500 in increments of 100, and Random Sampling has an accuracy of 0.02 to 0.13 higher than QBC)

&amp;#x200B;

This surprises me, as 90% of the literature (as well as a different test on a toy data set I ran) suggests that QBC should improve the accuracy considerably.

&amp;#x200B;

Has anyone else tried something like I am doing here? Maybe any ideas on what I could be doing wrong?

&amp;#x200B;

tl;dr: Turned this example [https://pytorch.org/tutorials/intermediate/char\_rnn\_classification\_tutorial.html](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) into pool-based, and implemented the Query by Commitee Active Learning strategy. Testing my results, QBC wierdly seems to perform worse than random sampling ",0,1,False,self,,,,,
1183,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,18,9z1w0i,self.MachineLearning,[P] Dense neural network for poker rule induction,https://www.reddit.com/r/MachineLearning/comments/9z1w0i/p_dense_neural_network_for_poker_rule_induction/,behindthedash,1542794091,"Task that I chose was to train a machine to induce poker rules from a set of poker hands with explicitly denoting the game rules. More simply, given a poker hand, computer should be able to tell how strong the hand is.

**Technology used for project:** Python, Keras, TensorFlow.

I implemeted a straightforward dense neural network. After trying different approaches to data encoding I ended up representing cards with integers from 1 to 13 and one hot encoding suits so that each suit was represented by 4 numbers all which except for one were zeros.

With this approach to data encoding there were two concepts about the game of poker that were difficult for the network to induce:

1. In poker for all hands except straights and straight flushes suit doesnt matter.
2. Straight is a sequence of cards in order, but it may start with an Ace. So both 10,J,Q,K,A and A,2,3,4,5 are straights. But A is higher than two 2, so in the second case cards are not in exactly in order.

The first problem was solved by data augmentation. I increased the training set of 25010 hands 4 times by generating more poker hands, so it was the training set of 100040 hands.

Solving the second problem required some data engineering. I sorted cards in each hand and added distances between cards as features to the dataset. Distance is just a difference between a card and the next card. And the distance between the last card and the first one in a hand is the difference between 13 and the last card plus the difference between the first card and 0.

A couple of examples to clarify how I transformed the data:

1. J,10,K,Q,,A. Sorting the hand by cards so it becomes 10,J,Q,K,,A. Cards will become an array of integers \[9,10,11,12,13\].Suits after one hot encoding: \[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,1\]. And distances: \[1,1,1,1,10\].
2. J,9,K,3,,A. Cards after sorting: \[2,8,10,12,13\]. Suits: \[1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1\]. Distances \[6,2,2,1,2\].

This approach finally gave 100% accuracy on the test set of 1000000 hands. This result was achieved after training for 300 epochs.

**The resulting architecture:**

&amp;#x200B;

https://i.redd.it/boewxay5onz11.png

I used dense layer with relu activation and a softmax layer for an output (10 possible hands in poker). As a means against overfitting I used several dropout layers.",3,1,False,https://b.thumbs.redditmedia.com/5VQwJB9jyGfikpn2s4lpn3-5ncgwIDamewscmhhPVzY.jpg,,,,,
1184,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,19,9z1x2b,self.MachineLearning,what is the good ratio between training set and total params?,https://www.reddit.com/r/MachineLearning/comments/9z1x2b/what_is_the_good_ratio_between_training_set_and/,GoBacksIn,1542794410,[removed],0,1,False,self,,,,,
1185,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,19,9z1yfv,self.MachineLearning,[D] (speech-recognition) Are the MFCC features the same for flac and wav files?,https://www.reddit.com/r/MachineLearning/comments/9z1yfv/d_speechrecognition_are_the_mfcc_features_the/,aziz_22,1542794775,"I'm working on Librispeech dataset and during feature processing I by mistake forgot to convert flac files to wav files and I generate the mfcc features later on during the feature processing phase.

Although the model I have converged correctly during training.

My question is : is there a big difference between the features I have generated and the features that would be generated from the converted flac to wav format?

Do I need to recompute everything? or it is ok since the model converged giving those features?

I suspect that there are some hidden bugs behind this mistake. ",0,1,False,self,,,,,
1186,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,19,9z21m9,deepsense.ai,[P] Outsmarting failure. Predictive maintenance powered by machine learning,https://www.reddit.com/r/MachineLearning/comments/9z21m9/p_outsmarting_failure_predictive_maintenance/,AnnaKow,1542795735,,0,1,False,default,,,,,
1187,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,20,9z2ajj,self.MachineLearning,Smart Engines won the competition on document binarization held within ICDAR,https://www.reddit.com/r/MachineLearning/comments/9z2ajj/smart_engines_won_the_competition_on_document/,Smart_Engines,1542798367,[removed],0,1,False,self,,,,,
1188,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,20,9z2bfk,technologyreview.com,One of the fathers of AI is worried about its future,https://www.reddit.com/r/MachineLearning/comments/9z2bfk/one_of_the_fathers_of_ai_is_worried_about_its/,men_nas_io,1542798624,,0,1,False,default,,,,,
1189,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,20,9z2ese,self.MachineLearning,Book recommendations,https://www.reddit.com/r/MachineLearning/comments/9z2ese/book_recommendations/,Akbar_,1542799497,[removed],0,1,False,self,,,,,
1190,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,20,9z2ivf,self.MachineLearning,Image Labeler,https://www.reddit.com/r/MachineLearning/comments/9z2ivf/image_labeler/,martian_rover,1542800660,[removed],0,1,False,self,,,,,
1191,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,21,9z2xa3,medium.com,[P] Arithmetic Properties of Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/9z2xa3/p_arithmetic_properties_of_word_embeddings/,HichamEB,1542804453,,0,1,False,https://b.thumbs.redditmedia.com/ThOBsaZ0Mi43whkb4qmrler_9kz8dfRgRbaZTEsCnro.jpg,,,,,
1192,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,21,9z2y7o,gengo.ai,List of AI &amp; machine learning blogs,https://www.reddit.com/r/MachineLearning/comments/9z2y7o/list_of_ai_machine_learning_blogs/,reimmoriks,1542804702,,0,1,False,default,,,,,
1193,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,22,9z365y,activewizards.com,Intro to Data Science for Managers [Mindmap],https://www.reddit.com/r/MachineLearning/comments/9z365y/intro_to_data_science_for_managers_mindmap/,viktoriia_shulga,1542806561,,0,1,False,default,,,,,
1194,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,22,9z38nc,i.redd.it,WANNA RINGS - Shop Jewelry in AR,https://www.reddit.com/r/MachineLearning/comments/9z38nc/wanna_rings_shop_jewelry_in_ar/,darya_sesitskaya,1542807136,,1,1,False,default,,,,,
1195,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,22,9z3944,self.MachineLearning,[D] Finally an easy way to Train Haar Cascades on Windows.,https://www.reddit.com/r/MachineLearning/comments/9z3944/d_finally_an_easy_way_to_train_haar_cascades_on/,adap23,1542807249,"There are 3 methods by which you could build your own HAAR cascades to detect custom objects of your choice on WINDOWS
1) Use the cmd and documentation by Auckland University to make your own Haar Cascade
Link [PDF] : https://www.google.co.in/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://www.cs.auckland.ac.nz/~m.rezaei/Tutorials/Creating_a_Cascade_of_Haar-Like_Classifiers_Step_by_Step.pdf&amp;ved=2ahUKEwjrlsSMiOXeAhVJr48KHf2KCJIQFjAAegQIARAB&amp;usg=AOvVaw0BAeKbBkefnCykzazRxCol
This method however is outdated and does not guarantee the best accuracy.

2) Sentdex's tutorial to make your own haar cascade : https://pythonprogramming.net/haar-cascade-object-detection-python-opencv-tutorial/
Problem with this is that you need to have a DigitalOcean server and need to PAY for it!

3) The BEST METHOD BY FAR (QUICK, EASY, BEST ACCURACY)
DOWNLOAD LINK OF GUI : http://amin-ahmadi.com/cascade-trainer-gui/
The Hassle free method of creating your own Haar Cascade is here!
This is by far the best method and takes very short time to train with the best results, and that is what we would be using to make your own Haar cascade to detect custom objects.

Checkout the tutorial here : https://youtu.be/ydSXgBZ1ybk
",1,1,False,self,,,,,
1196,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,22,9z3ad7,aiworld1.blogspot.com,China plans to upgrade its naval force with autonomous AI submarines,https://www.reddit.com/r/MachineLearning/comments/9z3ad7/china_plans_to_upgrade_its_naval_force_with/,cpt_snowcrash,1542807562,,0,1,False,default,,,,,
1197,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,22,9z3c6j,goo.gl,I am collecting data for drunkness prediction so any help is appreciated.,https://www.reddit.com/r/MachineLearning/comments/9z3c6j/i_am_collecting_data_for_drunkness_prediction_so/,Remloy,1542807975,,0,1,False,default,,,,,
1198,MachineLearning,t5_2r3gv,2018-11-21,2018,11,21,23,9z3uqt,self.MachineLearning,[D] Beating the state-of-the-art in NLP with HMTL,https://www.reddit.com/r/MachineLearning/comments/9z3uqt/d_beating_the_stateoftheart_in_nlp_with_hmtl/,VictorSanh,1542811967,[https://medium.com/huggingface/beating-the-state-of-the-art-in-nlp-with-hmtl-b4e1d5c3faf](https://medium.com/huggingface/beating-the-state-of-the-art-in-nlp-with-hmtl-b4e1d5c3faf),0,1,False,self,,,,,
1199,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,0,9z49r7,self.MachineLearning,[D] [R] Do larger batch_sizes reduce the covariate shift between the layers?,https://www.reddit.com/r/MachineLearning/comments/9z49r7/d_r_do_larger_batch_sizes_reduce_the_covariate/,aziz_22,1542814896,"I have this question in my mind but I haven't seen any paper that discussed this correlation ( As far as I know).

Please feel free to share any paper that discussed this observation. 

&amp;#x200B;

Thank you

&amp;#x200B;",6,1,False,self,,,,,
1200,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,0,9z4det,self.MachineLearning,"Simple Questions Thread November 21, 2018",https://www.reddit.com/r/MachineLearning/comments/9z4det/simple_questions_thread_november_21_2018/,AutoModerator,1542815565,[removed],0,1,False,self,,,,,
1201,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,1,9z4ive,self.MachineLearning,Car paint Recognition,https://www.reddit.com/r/MachineLearning/comments/9z4ive/car_paint_recognition/,ks3ni4,1542816563,[removed],0,1,False,self,,,,,
1202,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,1,9z4kv4,self.MachineLearning,[NLP] Keeping (or not) accents on letters before lemmatization ?,https://www.reddit.com/r/MachineLearning/comments/9z4kv4/nlp_keeping_or_not_accents_on_letters_before/,elpiro,1542816931,[removed],0,1,False,self,,,,,
1203,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,1,9z4kye,self.MachineLearning,[D] Refactoring deep learning library to PyTorch 1.0 vs Tensorflow 2.0,https://www.reddit.com/r/MachineLearning/comments/9z4kye/d_refactoring_deep_learning_library_to_pytorch_10/,dtrillaa,1542816944,"I wrote in a library in Tensorflow for my company that is used in production for various unsupervised learning tasks. I designed all the models in the library to have an sklearn-like interface such as the code [here](https://github.com/aaxwaz/Fraud-detection-using-deep-learning/blob/master/rbm/rbm.py) (not my code, but it was the inspiration for how I write my TensorFlow classes now).

Anyways, google has made it very clear that support for `TF 1.x` is ending once `TF 2.0` is released so Im planning on refactoring my libraries.

So now Im presented with the option of should I  wait for Tensorflow 2.0 or should I look into switching to PyTorch 1.0.

I understand that both libraries are converging to the same API. Tensorflow is adding eager execution and simplifying and unifying their api. PyTorch is adding static graph and production capabilities with `@torch.jit`. 

I know people are very opinionated about this, but does anyone think one API will be clearly better than the other? Or does it even matter, is it just personal preference?",22,1,False,self,,,,,
1204,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,1,9z4ouy,self.MachineLearning,Cognitive Automation @ Siemens Shared Services,https://www.reddit.com/r/MachineLearning/comments/9z4ouy/cognitive_automation_siemens_shared_services/,Industry_News,1542817665,[removed],0,1,False,self,,,,,
1205,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,2,9z50nw,self.MachineLearning,Adding custom labels/entities through for NER in Spacy,https://www.reddit.com/r/MachineLearning/comments/9z50nw/adding_custom_labelsentities_through_for_ner_in/,charizard_me,1542819749,[removed],0,1,False,self,,,,,
1206,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,2,9z51hd,self.MachineLearning,[P] How to pose a RL problem where a decision separate from normal action space must be taken at start of every episode?,https://www.reddit.com/r/MachineLearning/comments/9z51hd/p_how_to_pose_a_rl_problem_where_a_decision/,lantern_lol,1542819875,"I am attempting to solve a simple 2D robot ""reacher"" problem, where the state is a robot arm with one fixed joint and two free ones. The agent's aim is to simply solve the inverse kinematics problem by changing joint angles so the end effector reaches a random target point in each episode. A rendering of the problem is shown [here](https://imgur.com/a/NuA9A97), where the red circle is the target area and the blue point is the end effector.

However I am also interested in finding the ideal link lengths for reaching a specific target in each episode; a decision therefore firstly needs to be made on the length of the links based upon the target position, followed by actions made to the joint angles in order to reach said point based also upon the decided link lengths.

My question is how I would pose this as a RL problem where a standalone action (i.e. choosing link lengths) must be taken at the start of each episode. My intuition would be either that I would have to train a separate agent to make this decision, or alternatively that decisions about link lengths would not affect the state past the first action.

Any advice would be great, thanks!",9,1,False,self,,,,,
1207,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,2,9z55yd,modeldepot.io,"[P] Search Engine for Over 20,000 ML Model Implementations - Looking for Feedback",https://www.reddit.com/r/MachineLearning/comments/9z55yd/p_search_engine_for_over_20000_ml_model/,es6masterrace,1542820636,,0,1,False,default,,,,,
1208,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,2,9z57bw,self.MachineLearning,[Discussion] Free online course learn Probabilistic Programming,https://www.reddit.com/r/MachineLearning/comments/9z57bw/discussion_free_online_course_learn_probabilistic/,springcoil,1542820872,"I've put together an online course [http://learnprobprogramming.me](http://learnprobprogramming.me) on Bayesian Statistics. 

In 5 days you'll learn how to install PyMC3, get industrial examples and learn to build and refine models. 

&amp;#x200B;",9,1,False,self,,,,,
1209,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,2,9z599g,self.MachineLearning,"[P][D] Search Engine for Over 20,000 ML Model Implementations - Looking for Feedback",https://www.reddit.com/r/MachineLearning/comments/9z599g/pd_search_engine_for_over_20000_ml_model/,es6masterrace,1542821210,"Hey everyone! I've been working on a ML implementation search engine for a while and I wanted to get the community's feedback on how it's doing so far! I've spent the last month learning how to build a search engine from nothing, and had a lot of fun creating various ML pipelines and tuning Elasticsearch to surface good results on everything :)

I'd love to hear any feedback you guys have on different search patterns it's missing, genre of models it hasn't indexed yet, weird result rankings, or anything else in general! I'm also always free to chat via the in-site chat as well :)

You can check it out at: https://modeldepot.io/search :D",48,1,False,self,,,,,
1210,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,3,9z5t3j,self.MachineLearning,Sigmoid values having high value for 1 specific action every time?,https://www.reddit.com/r/MachineLearning/comments/9z5t3j/sigmoid_values_having_high_value_for_1_specific/,Jandevries101,1542824619,[removed],0,1,False,self,,,,,
1211,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,3,9z5w34,reddit.com,Perito para Causas Trabalhistas e Ambientais IV,https://www.reddit.com/r/MachineLearning/comments/9z5w34/perito_para_causas_trabalhistas_e_ambientais_iv/,JamurGerloff,1542825160,,0,1,False,default,,,,,
1212,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,4,9z6hlg,self.MachineLearning,Bridging the gap from Theory to application?,https://www.reddit.com/r/MachineLearning/comments/9z6hlg/bridging_the_gap_from_theory_to_application/,wild_fan_2001,1542828886,[removed],0,1,False,self,,,,,
1213,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,6,9z7knv,github.com,[R] [P] The PyTorch-Kaldi Speech Recognition Toolkit,https://www.reddit.com/r/MachineLearning/comments/9z7knv/r_p_the_pytorchkaldi_speech_recognition_toolkit/,moktarletoutgentil,1542835856,,0,1,False,default,,,,,
1214,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,6,9z7q6j,self.MachineLearning,What books or courses should I take to create algorithms from scratch?,https://www.reddit.com/r/MachineLearning/comments/9z7q6j/what_books_or_courses_should_i_take_to_create/,thabat,1542836892,[removed],0,1,False,self,,,,,
1215,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,6,9z7qpc,youtu.be,Realtime (50ms) object recognition and classification with Deep Learning on a Raspberry Pi,https://www.reddit.com/r/MachineLearning/comments/9z7qpc/realtime_50ms_object_recognition_and/,developFFM,1542837006,,2,1,False,https://b.thumbs.redditmedia.com/a2BU0y_zRPImdFWNkkugbzFoPOkxWzKqfLYq6PhIxEE.jpg,,,,,
1216,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,7,9z7yd9,self.MachineLearning,"[R] DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules (Frosst, Sabour, and Hinton, 2018)",https://www.reddit.com/r/MachineLearning/comments/9z7yd9/r_darccc_detecting_adversaries_by_reconstruction/,lmericle,1542838466,[removed],0,1,False,self,,,,,
1217,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,7,9z84vi,self.MachineLearning,What do you use to develop apps that use machine learning in Python for Android and iOS?,https://www.reddit.com/r/MachineLearning/comments/9z84vi/what_do_you_use_to_develop_apps_that_use_machine/,techsavvynerd91,1542839748,[removed],0,1,False,self,,,,,
1218,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,8,9z8nym,peadarcoyle.com,What is BFMI (Bayesian Fraction of Missing Information)?,https://www.reddit.com/r/MachineLearning/comments/9z8nym/what_is_bfmi_bayesian_fraction_of_missing/,springcoil,1542843647,,0,1,False,default,,,,,
1219,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,9,9z8tok,self.MachineLearning,[D] Reinforcement Learning with multiple simultaneous actions?,https://www.reddit.com/r/MachineLearning/comments/9z8tok/d_reinforcement_learning_with_multiple/,aharris12358,1542844896,"Hi, I'm working on a research project that involves the application of reinforcement learning to planning and decision-making problems. Typically, these problems involve both picking a behavior (such as ""collect energy"" or ""move to a target"") and a duration at the same time. The RL literature seems focused on policies that map from a set of states to a single individual action, which would require the specification of all possible action-duration permutations; not only does this increase the number of parameters I need exponentially, it also removes the ability to identify beneficial action correlations (because ""collect energy - short"" would wind up with much different encodings than ""collect energy - long"").  


Does anyone know of approaches that can not only map states to multiple simultaneous actions, but also that can maintain relationships between these actions? So far, the only source I have found is here: [http://www.ijcas.com/admin/paper/files/e1-1-17.pdf](http://www.ijcas.com/admin/paper/files/e1-1-17.pdf), but I could easily be missing keywords. ",11,1,False,self,,,,,
1220,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,9,9z8u2u,self.MachineLearning,[D] Submit code alongside paper anonymously,https://www.reddit.com/r/MachineLearning/comments/9z8u2u/d_submit_code_alongside_paper_anonymously/,crouching_dragon_420,1542844979,I remember in some paper I've read they uploaded their code onto Google Drive. Do this break anonymity in any way? What is the proper way to do this?,13,1,False,self,,,,,
1221,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,9,9z94jz,deeplearningio.com,What is Big Data ? | Deep Learning,https://www.reddit.com/r/MachineLearning/comments/9z94jz/what_is_big_data_deep_learning/,mrcgllr,1542847256,,0,1,False,default,,,,,
1222,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,10,9z9jnn,self.MachineLearning,How to Render OpenAi Gym in Google Colaboratory,https://www.reddit.com/r/MachineLearning/comments/9z9jnn/how_to_render_openai_gym_in_google_colaboratory/,sigmoidp,1542850653,[removed],0,1,False,self,,,,,
1223,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,11,9z9ve2,self.MachineLearning,Which NN architecture to use for temperature control for a non linear system,https://www.reddit.com/r/MachineLearning/comments/9z9ve2/which_nn_architecture_to_use_for_temperature/,OzgurGenc77,1542853415,"Hello - I am trying to figure out which neural network architecture to use to model a predictive temperature control system for a plastic injection molding machine. We heat the barrel to a target set temperature and we need to keep the temp there without over or under shooting. The relationship between the input temperature heater and the target temp is non-linear and there are multiple temp zones.  Thus i need a real time learning model that will predict and variable control the heating temp. 

I am considering modeling and trying either of Fuzzy logic or Recursive NN but not sure.. Any inputs are appreciated.",0,1,False,self,,,,,
1224,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,12,9za8uv,gwern.net,"[R] ""Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning"", Poplin et al 2018 [age, gender, smoking, blood pressure, diabetes, heart attacks - predicted from eye photographs using TF Inception v3 CNN]",https://www.reddit.com/r/MachineLearning/comments/9za8uv/r_prediction_of_cardiovascular_risk_factors_from/,gwern,1542856434,,0,1,False,default,,,,,
1225,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,12,9zak0n,arxiv.org,[R] Rethinking ImageNet Pre-training,https://www.reddit.com/r/MachineLearning/comments/9zak0n/r_rethinking_imagenet_pretraining/,HigherTopoi,1542859060,,19,1,False,default,,,,,
1226,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,13,9zax3k,self.MachineLearning,"Shower thought: If reproduction and survival is the reward of the human algorithm, some cheat the reproduction reward by masturbating",https://www.reddit.com/r/MachineLearning/comments/9zax3k/shower_thought_if_reproduction_and_survival_is/,toASI,1542862223,[removed],0,1,False,self,,,,,
1227,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,13,9zayg6,arxiv.org,[R] Natural Environment Benchmarks for Reinforcement Learning: we challenge the RL research community to develop more robust algorithms that meet high standards of evaluation,https://www.reddit.com/r/MachineLearning/comments/9zayg6/r_natural_environment_benchmarks_for/,downtownslim,1542862539,,5,1,False,default,,,,,
1228,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,14,9zb4as,github.com,I have open sourced Turing GAN in PyTorch,https://www.reddit.com/r/MachineLearning/comments/9zb4as/i_have_open_sourced_turing_gan_in_pytorch/,rahulbhalley,1542863885,,0,1,False,default,,,,,
1229,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,15,9zbkl8,self.MachineLearning,Database structure at your company,https://www.reddit.com/r/MachineLearning/comments/9zbkl8/database_structure_at_your_company/,ohai123456789,1542867797,[removed],0,1,False,self,,,,,
1230,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,15,9zbn7t,self.CollegeAdmissions,Compare Universities Worldwide and pick your best option,https://www.reddit.com/r/MachineLearning/comments/9zbn7t/compare_universities_worldwide_and_pick_your_best/,GoToUniversity,1542868440,,0,1,False,default,,,,,
1231,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,15,9zbseb,jobspikr.com,"[N] Check out the percentage of occurrence of key skills required for Machine Learning jobs (analysis of 11,000 job listings)",https://www.reddit.com/r/MachineLearning/comments/9zbseb/n_check_out_the_percentage_of_occurrence_of_key/,pr33tish,1542869780,,0,1,False,default,,,,,
1232,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,16,9zc0bi,self.MachineLearning,Food for thought!,https://www.reddit.com/r/MachineLearning/comments/9zc0bi/food_for_thought/,adityashrm21,1542871967,[removed],0,1,False,self,,,,,
1233,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,17,9zcfum,self.MachineLearning,GPU able ML framework?,https://www.reddit.com/r/MachineLearning/comments/9zcfum/gpu_able_ml_framework/,Bibabeuker,1542876551,[removed],0,1,False,self,,,,,
1234,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,18,9zco6h,self.MachineLearning,Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/9zco6h/pattern_recognition/,nishusindhu,1542878958,"[Pattern Recognition](https://www.learntek.org/blog/machine-learning-and-pattern-recognition/) **:** Pattern recognition is the process of recognizing patterns by using machine learning algorithm. Pattern recognition can be defined as the classification of data based on knowledge already gained or on statistical information extracted from patterns and/or their representation. Pattern recognition is the ability to detect arrangements of characteristics or data that yield information about a given system or data set. Predictive analytics in data science work can make use of pattern recognition algorithms to isolate statistically probable movements of time series data into the future. In a technological context, a pattern might be recurring sequences of data over time that can be used to predict trends, particular configurations of features in images that identify objects, frequent combinations of words and phrases for natural language processing (NLP), or particular clusters of behaviour on a network that could indicate an attack  among almost endless other possibilities. In IT, pattern recognition is a branch of machine learning that emphasizes the recognition of data patterns or data regularities in a given scenario. Pattern recognition involves classification and cluster of patterns.",0,1,False,self,,,,,
1235,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,18,9zcp96,self.deeplearning,Pytorch implementation of StNet: Local and Global Spatial-Temporal Modeling for Action Recognition,https://www.reddit.com/r/MachineLearning/comments/9zcp96/pytorch_implementation_of_stnet_local_and_global/,Damien_Menigaux,1542879276,,0,1,False,default,,,,,
1236,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,18,9zcu32,imarticus.org,Dassault Systems: Selling Experiences Not Just Products,https://www.reddit.com/r/MachineLearning/comments/9zcu32/dassault_systems_selling_experiences_not_just/,LearnFromImarticus,1542880717,,0,1,False,default,,,,,
1237,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,19,9zcuk0,youtube.com,I created this iOS app for the blind to help them in traffic,https://www.reddit.com/r/MachineLearning/comments/9zcuk0/i_created_this_ios_app_for_the_blind_to_help_them/,gi097,1542880855,,1,1,False,https://b.thumbs.redditmedia.com/zHmiIGGmY59bGTrW0GURqD6kpMGuNO5QijhJljzIRrE.jpg,,,,,
1238,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,19,9zcvif,self.MachineLearning,how to use DNC code from deepmind to solve value memorization problem,https://www.reddit.com/r/MachineLearning/comments/9zcvif/how_to_use_dnc_code_from_deepmind_to_solve_value/,IndependentManner2,1542881107,[removed],0,1,False,self,,,,,
1239,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,19,9zd0o9,arxiv.org,[1811.08888] Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks,https://www.reddit.com/r/MachineLearning/comments/9zd0o9/181108888_stochastic_gradient_descent_optimizes/,ihaphleas,1542882595,,37,1,False,default,,,,,
1240,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,20,9zd76s,self.MachineLearning,Is machine learning up to task of developing a more reactive opponent for say a VR boxing game?,https://www.reddit.com/r/MachineLearning/comments/9zd76s/is_machine_learning_up_to_task_of_developing_a/,VRGamer87,1542884525,[removed],0,1,False,self,,,,,
1241,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,20,9zdapu,self.MachineLearning,3D Morphable Models as Spatial Transformer Networks #weeklypaper 42,https://www.reddit.com/r/MachineLearning/comments/9zdapu/3d_morphable_models_as_spatial_transformer/,BrighterAI,1542885553,"Hi Redditors, we are a bunch of Deep Learning and Machine Learning scientist working in a start-up. We share papers every week and discuss them, and we taught that it might be of interest for other like-minded people to read as well.

**T**he authors of our weekly paper propose a method to produce texture maps for 3D morphable models of the face from images of faces in the wild. They introduce an extension of the spatial transformer network block. The new block interprets and normalizes 3D poses and self-occlusion. The rotation, translation, face model parameters in this block are learned unsupervised while training for simple geometric objectives.  
As an example, we show the texture maps produced by the network when images of Elon Musk are fed into it (find more here [https://medium.com/generate-vision/the-paperoftheweek-42-was-3d-morphable-models-as-spatial-transformer-networks-dbbf3c670289](https://medium.com/generate-vision/the-paperoftheweek-42-was-3d-morphable-models-as-spatial-transformer-networks-dbbf3c670289) )

&amp;#x200B;",0,1,False,self,,,,,
1242,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,20,9zdggn,youtube.com,Just starting: webinar about version control in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9zdggn/just_starting_webinar_about_version_control_in/,mangoojoanna,1542887246,,0,1,False,default,,,,,
1243,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,20,9zdhhr,codecondo.com,How Search Engines Use Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/9zdhhr/how_search_engines_use_machine_learning/,davidreed7021,1542887549,,0,1,False,default,,,,,
1244,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,20,9zdizi,self.MachineLearning,Searching for a small Network for Semantic Segmentation in Depth Data,https://www.reddit.com/r/MachineLearning/comments/9zdizi/searching_for_a_small_network_for_semantic/,TimDNN,1542887988,[removed],0,1,False,self,,,,,
1245,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,21,9zdj4b,self.MachineLearning,Relational RNN (RMC),https://www.reddit.com/r/MachineLearning/comments/9zdj4b/relational_rnn_rmc/,Martinpete,1542888033,[removed],0,1,False,self,,,,,
1246,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,21,9zdkss,self.MachineLearning,unet zhixuhao repository on github,https://www.reddit.com/r/MachineLearning/comments/9zdkss/unet_zhixuhao_repository_on_github/,Eeuwigestudent1,1542888465,[removed],0,1,False,self,,,,,
1247,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,21,9zdwi7,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Nov. 22, 2018",https://www.reddit.com/r/MachineLearning/comments/9zdwi7/n_weekly_machine_learning_opensource_roundup_nov/,stkim1,1542891595,,0,1,False,default,,,,,
1248,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,22,9zdyjb,self.MachineLearning,Features of Pattern Recognition:,https://www.reddit.com/r/MachineLearning/comments/9zdyjb/features_of_pattern_recognition/,Anu2008,1542892064,[removed],0,1,False,self,,,,,
1249,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,22,9ze0gx,towardsdatascience.com,Clustering with Gaussian Mixture model: how to choose the number of gaussian components,https://www.reddit.com/r/MachineLearning/comments/9ze0gx/clustering_with_gaussian_mixture_model_how_to/,VLavorini,1542892552,,0,1,False,default,,,,,
1250,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,22,9ze9g7,self.MachineLearning,Tensorflow CNN + OpenStreetMap 3D == Real Life SimCity,https://www.reddit.com/r/MachineLearning/comments/9ze9g7/tensorflow_cnn_openstreetmap_3d_real_life_simcity/,00hello,1542894714,[removed],0,1,False,self,,,,,
1251,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,22,9zeb4s,self.MachineLearning,How to know the limits of a task,https://www.reddit.com/r/MachineLearning/comments/9zeb4s/how_to_know_the_limits_of_a_task/,mouseff,1542895120,[removed],0,1,False,self,,,,,
1252,MachineLearning,t5_2r3gv,2018-11-22,2018,11,22,23,9zeel6,self.MachineLearning,[D] What do you use to develop apps that use machine learning in Python for Android and iOS?,https://www.reddit.com/r/MachineLearning/comments/9zeel6/d_what_do_you_use_to_develop_apps_that_use/,techsavvynerd91,1542895890,"Android SDK only allows you to develop apps using Java or Kotlin, and iOS SDK only allows you to develop apps using Objective-C or Swift. So how does one develop apps that use machine learning for both markets through Python? ",4,1,False,self,,,,,
1253,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,0,9zex8d,self.MachineLearning,Activation functions in plain English?,https://www.reddit.com/r/MachineLearning/comments/9zex8d/activation_functions_in_plain_english/,emergenthoughts,1542899926,[removed],0,1,False,self,,,,,
1254,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,0,9zf1sq,youtube.com,Elon Musk The Merging of Humans and Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/9zf1sq/elon_musk_the_merging_of_humans_and_artificial/,baDoxx,1542900857,,0,1,False,https://b.thumbs.redditmedia.com/ODezVflyOmsQj4D9MRFsHYLc1nnfIzF2c6R8M7WLdJM.jpg,,,,,
1255,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,0,9zf3te,self.MachineLearning,How to handle imbalanced data which have 20+ classes? and what approaches to avoid when handling imbalanced data?,https://www.reddit.com/r/MachineLearning/comments/9zf3te/how_to_handle_imbalanced_data_which_have_20/,p1nkp3ngu1n,1542901279,[removed],0,1,False,self,,,,,
1256,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,1,9zfayj,hyperabstractor.tumblr.com,A dimension called Ideation. An abstract story.,https://www.reddit.com/r/MachineLearning/comments/9zfayj/a_dimension_called_ideation_an_abstract_story/,locateneil,1542902671,,1,1,False,default,,,,,
1257,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,1,9zfe00,reddit.com,New Subreddit for the most interesting ML projects,https://www.reddit.com/r/MachineLearning/comments/9zfe00/new_subreddit_for_the_most_interesting_ml_projects/,GantMan,1542903269,,0,1,False,default,,,,,
1258,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,1,9zfnkv,reddit.com,[N] Subreddit for FUN ML,https://www.reddit.com/r/MachineLearning/comments/9zfnkv/n_subreddit_for_fun_ml/,GantMan,1542905088,,0,1,False,default,,,,,
1259,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,1,9zfogq,self.MachineLearning,Best practice for modelling sensor network datasets?,https://www.reddit.com/r/MachineLearning/comments/9zfogq/best_practice_for_modelling_sensor_network/,sourcremeking,1542905258,[removed],0,1,False,self,,,,,
1260,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,1,9zfqj3,youtube.com,"Are you interested in Machine Learning and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",https://www.reddit.com/r/MachineLearning/comments/9zfqj3/are_you_interested_in_machine_learning_and_want/,ailearn12,1542905655,,0,1,False,https://b.thumbs.redditmedia.com/Flm4gvazaWsKSS-buDv0Dn3kgO7zS9YTB0aPC9ILyhY.jpg,,,,,
1261,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,2,9zg2sd,self.MachineLearning,Maximum likelihood trees for hierarchical clustering?,https://www.reddit.com/r/MachineLearning/comments/9zg2sd/maximum_likelihood_trees_for_hierarchical/,o-rka,1542907914,[removed],0,1,False,self,,,,,
1262,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,3,9zgbzk,self.MachineLearning,Help understanding Tensorflow I/O,https://www.reddit.com/r/MachineLearning/comments/9zgbzk/help_understanding_tensorflow_io/,martmists,1542909687,[removed],0,1,False,self,,,,,
1263,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,3,9zgc6p,self.MachineLearning,How to create a tensor without knowing shape upfront in Keras/TF?,https://www.reddit.com/r/MachineLearning/comments/9zgc6p/how_to_create_a_tensor_without_knowing_shape/,ImaginaryAnon,1542909720,"I am currently stuck at a problem. I need to create a temporary variable (of tensor type) for holding values when my custom layer is processing the input. The problem is, the layer gets its inputs in a batch. Batch size is variable. Also, one other dimension of the input is variable which will be provided at the time of running the model.

So, how can I create a tensor of variable dimension every time for a new input. Surely there is a way to do this because built-in layers also take input as a batch whose size is variable and also the shape of output tensor varies as it outputs the whole processed batch. But I don't know how to do it.

Here's the piece of my code:

```
similarity_matrix = K.zeros(shape=(int_shape(context_vectors)[0], int_shape(context_vectors)[1], int_shape(query_vectors)[1]), dtype='float32')
for i, context_vector in enumerate(modified_context_vectors):
    for j, query_vector in enumerate(modified_query_vectors):
        concatenated_vector = concatenate_and_multiply(context_vector, query_vector)
        result = dot(concatenated_vector, self.kernel)
        similarity_matrix[:, i, j] = result
return similarity_matrix
```

Here, context_vectors and query_vectors are of shape (None, None, 600). The first dimension is batch_size which is same for both. The second dimension is the number of words in context and query and hence will differ for both.",0,1,False,self,,,,,
1264,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,4,9zhapg,self.MachineLearning,[Research] Papers about Machine Learning in accounting and finance,https://www.reddit.com/r/MachineLearning/comments/9zhapg/research_papers_about_machine_learning_in/,ricklen,1542916302,"Hello everyone,

Im a student and about to start my graduation project at an accounting firm. The firm wants to know whether they can implement machine learning in their auditing process.

In order to prepare for this project I first want to do literature research in order to find out whats already done in the accounting world with respect to machine learning. I want to ask if you guys know any good papers / articles / books I can start with. 

The main focus is the audit process thinking about finding outliers / patterns in the financial data of customers (which are small to medium sized companies).

I would be very nice if I have some good articles to start with! Thank you in forward!",10,1,False,self,,,,,
1265,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,5,9zhj9k,self.MachineLearning,Question about multi task learning: how do you train unlabeled outputs while rotating among disparate supervised datasets for each task?,https://www.reddit.com/r/MachineLearning/comments/9zhj9k/question_about_multi_task_learning_how_do_you/,AvatarUltima7,1542917867,[removed],0,1,False,self,,,,,
1266,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,6,9zi5fy,github.com,A collection of 70+ trained RL agents using Stable Baselines,https://www.reddit.com/r/MachineLearning/comments/9zi5fy/a_collection_of_70_trained_rl_agents_using_stable/,atooo57,1542921993,,0,1,False,default,,,,,
1267,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,6,9zibdf,self.MachineLearning,"[D] To calculate the advantage in Advantage Actor Critic reinforcement learning method I have to know the average value of the actions of a state. I understand how to find it having discrete action spaces, but how is it done having continuous action spaces?",https://www.reddit.com/r/MachineLearning/comments/9zibdf/d_to_calculate_the_advantage_in_advantage_actor/,adkyary,1542923237,"I've read some articles about this method, but none of them explained how the advantage is calculated in continuous action spaces. Still found nothing on Google either.",9,1,False,self,,,,,
1268,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,7,9zilqj,self.MachineLearning,Tracking moving objects across several cameras,https://www.reddit.com/r/MachineLearning/comments/9zilqj/tracking_moving_objects_across_several_cameras/,jacksonkr_,1542925435,[removed],0,1,False,self,,,,,
1269,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,8,9zj0t4,self.MachineLearning,[D] Are TDA tools like the Vietoris-Rips complex state of the art for determining the number of connected components of a dataset?,https://www.reddit.com/r/MachineLearning/comments/9zj0t4/d_are_tda_tools_like_the_vietorisrips_complex/,manifoldPTCG,1542928924,"I found the wonderful paper Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions

https://arxiv.org/pdf/1803.00094.pdf

and I was wondering: how do people currently determine if say for example a single class of a dataset is disconnected? Are the usual dimension reduction algorithms reliable for expressing this kind of information? 

It seems like TDA would really shine in finding topological features like this but I havent seen much online.
",3,1,False,self,,,,,
1270,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,8,9zj79v,self.MachineLearning,Suggestions for a ML Project,https://www.reddit.com/r/MachineLearning/comments/9zj79v/suggestions_for_a_ml_project/,Morad90,1542930480,[removed],0,1,False,self,,,,,
1271,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,8,9zj7wl,self.MachineLearning,[D] How do I get Google's JFT-300M dataset?,https://www.reddit.com/r/MachineLearning/comments/9zj7wl/d_how_do_i_get_googles_jft300m_dataset/,milaworld,1542930640,"Last year, Google published [Revisiting Unreasonable Effectiveness of Data in Deep Learning Era](https://arxiv.org/abs/1707.02968), introducing a dataset of 300M+ images called JFT-300M. This paper was accepted at [ICCV](http://openaccess.thecvf.com/content_ICCV_2017/papers/Sun_Revisiting_Unreasonable_Effectiveness_ICCV_2017_paper.pdf), and they even published a [blog post](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html) about the dataset. It has been cited over 100 times.

But I can't find where I can obtain the dataset to experiment on new things, or to reproduce (partially) some reported baseline results. Am I looking at the wrong place?

So how can a dataset be published, without being published, still get accepted into an okay venue, and get a bunch of citations?",40,1,False,self,,,,,
1272,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,9,9zjebx,reviewnb.com,[P] ReviewNB: Jupyter Notebook Diff for GitHub,https://www.reddit.com/r/MachineLearning/comments/9zjebx/p_reviewnb_jupyter_notebook_diff_for_github/,PresentCompanyExcl,1542932205,,0,1,False,default,,,,,
1273,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,9,9zjiw4,self.MachineLearning,[Q] What type of algorithm/model does Google use for sentiment analysis in Cloud Natural Language API?,https://www.reddit.com/r/MachineLearning/comments/9zjiw4/q_what_type_of_algorithmmodel_does_google_use_for/,snendroid-ai,1542933313,[removed],0,1,False,self,,,,,
1274,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,11,9zkf89,self.MachineLearning,Use Bert for sentence embedding,https://www.reddit.com/r/MachineLearning/comments/9zkf89/use_bert_for_sentence_embedding/,ThinkKnowledge,1542941463,[removed],0,1,False,self,,,,,
1275,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,12,9zki1s,self.MachineLearning,Training on unique feature vectors weighted by number of occurences,https://www.reddit.com/r/MachineLearning/comments/9zki1s/training_on_unique_feature_vectors_weighted_by/,DstnB3,1542942157,[removed],0,1,False,self,,,,,
1276,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,13,9zkw2k,self.MachineLearning,Should I buy a MacBook or a Dell XPS 15?,https://www.reddit.com/r/MachineLearning/comments/9zkw2k/should_i_buy_a_macbook_or_a_dell_xps_15/,lkit57a03,1542945716,[removed],0,1,False,self,,,,,
1277,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,13,9zkx9v,self.MachineLearning,Training on unique feature vectors weighted by number of occurences,https://www.reddit.com/r/MachineLearning/comments/9zkx9v/training_on_unique_feature_vectors_weighted_by/,DstnB3,1542945999,[removed],1,1,False,self,,,,,
1278,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,13,9zl8bj,i.redd.it,All Machine Learning/AI folks will agree with this,https://www.reddit.com/r/MachineLearning/comments/9zl8bj/all_machine_learningai_folks_will_agree_with_this/,Kedjja,1542948823,,0,1,False,https://b.thumbs.redditmedia.com/BLda6TnKseIDqy9zhIohNaJLnEMlgZzJNwpfOTjHVOo.jpg,,,,,
1279,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,14,9zlli9,self.MachineLearning,Handwritten Character Segmentation?,https://www.reddit.com/r/MachineLearning/comments/9zlli9/handwritten_character_segmentation/,veilerdude,1542952034,[removed],0,1,False,self,,,,,
1280,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,15,9zlov8,self.MachineLearning,How to get ML role in FAANG or top startup and eventually start own startup,https://www.reddit.com/r/MachineLearning/comments/9zlov8/how_to_get_ml_role_in_faang_or_top_startup_and/,bycfly,1542952907,[removed],0,1,False,self,,,,,
1281,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,15,9zlp8q,journals.plos.org,Predicting the risk of emergency admission with machine learning: Development and validation using linked electronic ...,https://www.reddit.com/r/MachineLearning/comments/9zlp8q/predicting_the_risk_of_emergency_admission_with/,pattygammage,1542952994,,0,1,False,default,,,,,
1282,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,16,9zm580,arxiv.org,[1811.06128] Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon,https://www.reddit.com/r/MachineLearning/comments/9zm580/181106128_machine_learning_for_combinatorial/,aiismorethanml,1542957303,,4,1,False,default,,,,,
1283,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,16,9zmabz,towardsdatascience.com,Comparison of the best NSFW Image Moderation APIs 2018,https://www.reddit.com/r/MachineLearning/comments/9zmabz/comparison_of_the_best_nsfw_image_moderation_apis/,ady_anr,1542958725,,0,1,True,nsfw,,,,,
1284,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,16,9zmaty,self.MachineLearning,[D] Do NIPS workshops provide free tickets?,https://www.reddit.com/r/MachineLearning/comments/9zmaty/d_do_nips_workshops_provide_free_tickets/,greglingo,1542958867,"I got a paper accepted to a NIPS workshop and was under the impression that they provided free tickets to speakers. The workshop said they had tickets for all presenters. I assumed these tickets were free, but now I'm unsure. Do I still have to pay for registration?",6,1,False,self,,,,,
1285,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,17,9zmgme,self.MachineLearning,Huawei Matebook X Pro - Tensorflow Support and eGPU Tensorflow Support?,https://www.reddit.com/r/MachineLearning/comments/9zmgme/huawei_matebook_x_pro_tensorflow_support_and_egpu/,ladiesman217-ab,1542960529,[removed],0,1,False,self,,,,,
1286,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,17,9zmjbf,self.MachineLearning,[D] Do Dead ReLU's have an impact on convolutional layers?,https://www.reddit.com/r/MachineLearning/comments/9zmjbf/d_do_dead_relus_have_an_impact_on_convolutional/,newperson77777777,1542961317,"For a convolutional network, even if some neurons don't fire, others will, and the filters would still be updated based on the gradient. I guess it's possible if your weight values for a kernel in comparison to your input values were really off, but I'm assuming this is really unlikely, especially if you have a large number of input values in comparison to your kernel size or you have several input channels.",15,1,False,self,,,,,
1287,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,17,9zmpba,self.MachineLearning,Machine Learning vs Intuition,https://www.reddit.com/r/MachineLearning/comments/9zmpba/machine_learning_vs_intuition/,Otilia_SwissBorg,1542963167,[removed],0,1,False,self,,,,,
1288,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,17,9zmpi7,self.MachineLearning,2018 Pressure Transmitter and Level Transmitter market report,https://www.reddit.com/r/MachineLearning/comments/9zmpi7/2018_pressure_transmitter_and_level_transmitter/,Holykell,1542963230,[removed],0,1,False,self,,,,,
1289,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,18,9zmt55,self.MachineLearning,Need Help installing and getting to Run this https://github.com/pathak22/noreward-rl,https://www.reddit.com/r/MachineLearning/comments/9zmt55/need_help_installing_and_getting_to_run_this/,mulhollandx,1542964308,"Hey guys, studying CS in my third year but never worked with python and tensorflow. I found this interesting Project i want to look into further but i just cant get it to run: [https://github.com/pathak22/noreward-rl](https://github.com/pathak22/noreward-rl).  


There are several things going wrong in the process. If someone could try to get it running on Windows10 or Ubuntu and then tell me how...that would be the best thing ever.

&amp;#x200B;",0,1,False,self,,,,,
1290,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,18,9zmvpz,arxiv.org,[R] Sampling Can Be Faster Than Optimization,https://www.reddit.com/r/MachineLearning/comments/9zmvpz/r_sampling_can_be_faster_than_optimization/,i-like-big-gans,1542965107,,1,1,False,default,,,,,
1291,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,18,9zmwy3,self.MachineLearning,[R] Deep Autoencoder-like Nonnegative Matrix Factorization (CIKM 2018),https://www.reddit.com/r/MachineLearning/comments/9zmwy3/r_deep_autoencoderlike_nonnegative_matrix/,benitorosenberg,1542965485,"&amp;#x200B;

https://i.redd.it/fsqejyx4t1021.jpg

Paper: [https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf)

Python: [https://github.com/benedekrozemberczki/DANMF](https://github.com/benedekrozemberczki/DANMF)

ABSTRACT:

Community structure is ubiquitous in real-world complex networks. The  task of community detection over these networks is of paramount  importance in a variety of applications. Recently, nonnegative matrix  factorization (NMF) has been widely adopted for community detection due  to its great interpretability and its natural fitness for capturing the  community membership of nodes. However, the existing NMF-based community  detection approaches are shallow methods. They learn the community  assignment by mapping the original network to the community membership  space directly. Considering the complicated and diversified topology  structures of real-world networks, it is highly possible that the  mapping between the original network and the community membership space  contains rather complex hierarchical information, which cannot be  interpreted by classic shallow NMF-based approaches. Inspired by the  unique feature representation learning capability of deep autoencoder,  we propose a novel model, named Deep Autoencoder-like NMF (DANMF), for  community detection. Similar to deep autoencoder, DANMF consists of an  encoder component and a decoder component. This architecture empowers  DANMF to learn the hierarchical mappings between the original network  and the final community  assignment  with  implicit  low-to-high  level   hidden attributes of the original network learnt in the intermediate  layers. Thus, DANMF should be better suited to the community detection  task. Extensive experiments on benchmark datasets demonstrate that DANMF  can achieve better performance than the state-of-the-art NMF-based  community detection approaches.

&amp;#x200B;

&amp;#x200B;",9,1,False,https://b.thumbs.redditmedia.com/d7an-nrCHUxcwErZSR8fW2xJUO7B8hZXqEB4GDFVzfk.jpg,,,,,
1292,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,18,9zmzxz,self.MachineLearning,Is deep neural network a black box ?,https://www.reddit.com/r/MachineLearning/comments/9zmzxz/is_deep_neural_network_a_black_box/,pinton96,1542966389,[removed],0,1,False,self,,,,,
1293,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,19,9zn85z,self.MachineLearning,Top Course on a machine learning and data science,https://www.reddit.com/r/MachineLearning/comments/9zn85z/top_course_on_a_machine_learning_and_data_science/,Guru99Tech,1542969013,[removed],0,1,False,self,,,,,
1294,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,20,9znicc,sunps.blogspot.com,5 Innovative Uses for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9znicc/5_innovative_uses_for_machine_learning/,JDMDIGITAL,1542972174,,0,1,False,default,,,,,
1295,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,20,9znl58,apsense.com,Best Instances Of Challenges In Machine Learning!,https://www.reddit.com/r/MachineLearning/comments/9znl58/best_instances_of_challenges_in_machine_learning/,smadrid056,1542973042,,0,1,False,https://b.thumbs.redditmedia.com/-3k6RUHBfzKTrg_LEv95wuPOcX1LZ41J5yph3H0JOWg.jpg,,,,,
1296,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,21,9znt28,self.MachineLearning,[D] Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/9znt28/d_dynamic_layer_normalization_for_adaptive_neural/,aziz_22,1542975362,"I'm going to implement this idea of  [Dynamic layer normalization with projection layer](https://arxiv.org/pdf/1707.06065.pdf) in the context of speech-recognition.  I have some questions regarding how to implement this idea.

The authors explains that the parameters are adopted based on the input sequence rather than learning them.

Here they're talking about the Alpha and beta parameters.

Say, I want to implement this using tensorflow. We have an already implemented class for the Layer Normalization, which is the [LayerNormBasicLstmCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell). 

What I think of doing is modify the norm function (exactly the initialization of the beta and gamma):

    def _norm(self, inp, scope, dtype=dtypes.float32):
        shape = inp.get_shape()[-1:]
        gamma_init = init_ops.constant_initializer(self._norm_gain)
        beta_init = init_ops.constant_initializer(self._norm_shift)
        with vs.variable_scope(scope):
          # Initialize beta and gamma for use by layer_norm.
          vs.get_variable(""gamma"", shape=shape, initializer=gamma_init, dtype=dtype)
          vs.get_variable(""beta"", shape=shape, initializer=beta_init, dtype=dtype)
        normalized = layers.layer_norm(inp, reuse=True, scope=scope)
        return normalized 

and make them as placeholders so I avoid learning them during training. 

Is that the correct way? 

Looking forward for your feedback.

&amp;#x200B;

thank you

&amp;#x200B;",0,1,False,self,,,,,
1297,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,21,9zntgl,youtube.com,Advanced Deep Learning &amp; Reinforcement Learning - Lectures at UCL by DeepMind,https://www.reddit.com/r/MachineLearning/comments/9zntgl/advanced_deep_learning_reinforcement_learning/,petrux,1542975474,,0,1,False,https://b.thumbs.redditmedia.com/dUj6C0FDfQaRtTR1DiLAipXpVtJh8V3cu6Pr06UtxbU.jpg,,,,,
1298,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,21,9znuh3,multisoftvirtualacademy.com,Machine Learning Online Courses Can Offer You A Lucrative Career - MVA Blog,https://www.reddit.com/r/MachineLearning/comments/9znuh3/machine_learning_online_courses_can_offer_you_a/,multisoftmva0,1542975770,,0,1,False,default,,,,,
1299,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,21,9znzam,arxiv.org,[R] Adversarial Autoencoders for Generating 3D Point Clouds,https://www.reddit.com/r/MachineLearning/comments/9znzam/r_adversarial_autoencoders_for_generating_3d/,i-like-big-gans,1542977079,,15,1,False,default,,,,,
1300,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,22,9zoauh,self.MachineLearning,DeepMind lecture series,https://www.reddit.com/r/MachineLearning/comments/9zoauh/deepmind_lecture_series/,ch3njus,1542980000,[removed],0,1,False,self,,,,,
1301,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,22,9zoc2s,self.MachineLearning,What is the C4. 5 algorithm and how does it work?,https://www.reddit.com/r/MachineLearning/comments/9zoc2s/what_is_the_c4_5_algorithm_and_how_does_it_work/,_sumitsaha_,1542980295,[removed],0,1,False,self,,,,,
1302,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,22,9zoexy,self.reinforcementlearning,[Course] Advanced Deep Learning and Reinforcement Learning course at UCL by DeepMind,https://www.reddit.com/r/MachineLearning/comments/9zoexy/course_advanced_deep_learning_and_reinforcement/,tigerneil,1542980976,,0,1,False,https://a.thumbs.redditmedia.com/I_gr2rGbVDPrqE64QGULix3c7wWfYnHSCQrweKHFCL8.jpg,,,,,
1303,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,23,9zohvy,i.redd.it,What is the C4.5 algorithm and how does it work?,https://www.reddit.com/r/MachineLearning/comments/9zohvy/what_is_the_c45_algorithm_and_how_does_it_work/,_sumitsaha_,1542981690,,0,1,False,default,,,,,
1304,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,23,9zokl6,wenso.co.uk,Artificial intelligence System Can Recognise Emotions From Human Speech?,https://www.reddit.com/r/MachineLearning/comments/9zokl6/artificial_intelligence_system_can_recognise/,Wensosolutions,1542982281,,0,1,False,default,,,,,
1305,MachineLearning,t5_2r3gv,2018-11-23,2018,11,23,23,9zou9m,self.MachineLearning,[Project] Pachyderm for data scientists,https://www.reddit.com/r/MachineLearning/comments/9zou9m/project_pachyderm_for_data_scientists/,BigDataRepublic,1542984455,"Pachyderm recently raised quite a bit of money in a [Series A round](http://www.pachyderm.io/2018/11/15/Series-A.html) and at first sight seems like a great tool to use for data lineage and versioning. Describing themselves as the ""Git for data"", it tracks your data revisions and the transformations you apply. This all sounds great, but we were wondering how it actually works in practice. We've written a [blogpost](https://medium.com/bigdatarepublic/pachyderm-for-data-scientists-d1d1dff3a2fa) and also published all related [code](https://github.com/BigDataRepublic/hackathon\_ds\_to\_prod/tree/pachyderm).

What do you think? Useful or too much overhead?",13,1,False,self,,,,,
1306,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zoxoz,self.MachineLearning,"[R] Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers",https://www.reddit.com/r/MachineLearning/comments/9zoxoz/r_learning_and_generalization_in/,IborkedyourGPU,1542985212,"**Abstract**: Neural networks have great success in many machine learning applications, but the fundamental learning theory behind them remains largely unsolved. Learning neural networks is NP-hard, but in practice, simple algorithms like stochastic gradient descent (SGD) often produce good solutions. Moreover, it is observed that overparameterization --- designing networks whose number of parameters is larger than statistically needed to perfectly fit the data --- improves both optimization and generalization, appearing to contradict traditional learning theory. 
In this work, we extend the theoretical understanding of two and three-layer neural networks in the overparameterized regime. We prove that, using overparameterized neural networks, one can (improperly) learn some notable hypothesis classes, including two and three-layer neural networks with fewer parameters. Moreover, the learning process can be simply done by SGD or its variants in polynomial time using polynomially many samples. We also show that for a fixed sample size, the generalization error of the solution found by some SGD variant can be made almost independent of the number of parameters in the overparameterized network.",2,1,False,self,,,,,
1307,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zoznb,self.MachineLearning,DeepBrain Chain opens up free trials for AI cloud computing network.,https://www.reddit.com/r/MachineLearning/comments/9zoznb/deepbrain_chain_opens_up_free_trials_for_ai_cloud/,kingcooked,1542985627,[removed],0,1,False,self,,,,,
1308,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zozv2,medium.com,[N] [R] The Path towards Situated AI: A teaser to TwentyBN's upcoming NeurIPS 2018 announcement,https://www.reddit.com/r/MachineLearning/comments/9zozv2/n_r_the_path_towards_situated_ai_a_teaser_to/,nahuak,1542985676,,0,1,False,https://b.thumbs.redditmedia.com/mFLJ08YSxOmxxtBuHLHdqybb5ZAd49WhP8V0A_acBIk.jpg,,,,,
1309,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zp5b1,youtube.com,"[Lectures] Google Deep Mind released lectures on ""Advanced Deep Learning and Reinforcement Learning""",https://www.reddit.com/r/MachineLearning/comments/9zp5b1/lectures_google_deep_mind_released_lectures_on/,theainerd,1542986852,,0,1,False,default,,,,,
1310,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zp5e6,self.MachineLearning,DeepBrain Chain Opens Up AI Cloud Computing Network For Free Over the Next Month,https://www.reddit.com/r/MachineLearning/comments/9zp5e6/deepbrain_chain_opens_up_ai_cloud_computing/,kingcooked,1542986871,[removed],0,1,False,self,,,,,
1311,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zp8st,self.MachineLearning,[N] DeepBrain Chain Opens Up AI Cloud Computing Network For Free Over the Next Month,https://www.reddit.com/r/MachineLearning/comments/9zp8st/n_deepbrain_chain_opens_up_ai_cloud_computing/,kingcooked,1542987564,[removed],0,1,False,self,,,,,
1312,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zpc7d,self.MachineLearning,[N] DeepBrain Chain AI Cloud Training Network Now Open for Trial Users,https://www.reddit.com/r/MachineLearning/comments/9zpc7d/n_deepbrain_chain_ai_cloud_training_network_now/,kingcooked,1542988251,"**DeepBrain Chain has opened up our distributed AI Cloud training net to trial users.**

We have 1, 2, 4 and 8GPU servers available for use.

You need to follow these steps to get trial access:

1. Register on the DBC Website
2. Head to the AI training net portal on your personal account page
3. **Click AI training net** from the side tab and install the DBC client - set your machine ID (click the little blue ? to find your machine ID)
4. You automatically receive 500 DBC (around 10 training hours on our 2GPU servers)

Github guides/user manuals and resources are available through the training net/github portal from the training net page on DBC website.",30,1,False,self,,,,,
1313,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,0,9zpclf,self.MachineLearning,This is how i explained my mother (76 years old) machine learning in one sentence.,https://www.reddit.com/r/MachineLearning/comments/9zpclf/this_is_how_i_explained_my_mother_76_years_old/,seemingly_omniscient,1542988335,[removed],0,1,False,self,,,,,
1314,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,1,9zptxg,self.MachineLearning,"Keras and IMDB: Love, hate, sex and money",https://www.reddit.com/r/MachineLearning/comments/9zptxg/keras_and_imdb_love_hate_sex_and_money/,chclau,1542991762,[removed],0,1,False,self,,,,,
1315,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,2,9zq884,self.MachineLearning,[D] How to parse my pictures dataset into time series?,https://www.reddit.com/r/MachineLearning/comments/9zq884/d_how_to_parse_my_pictures_dataset_into_time/,hadaev,1542994506,"I have batch of pictures.

This is a cardiograms, there is no way to get numbers from machine, I only have these pictures.

And I need to digitize these two plots in two time series (for recurrent network).

But Im stuck couz graphics in the image may intersect.

[https://i.imgur.com/XeFz5kf.png](https://i.imgur.com/XeFz5kf.png)",10,1,False,self,,,,,
1316,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,2,9zq8zw,self.MachineLearning,[D] Are there any examples of combination losses involving both regression and classification?,https://www.reddit.com/r/MachineLearning/comments/9zq8zw/d_are_there_any_examples_of_combination_losses/,weelamb,1542994652,"I want to introduce adversarial examples into my training (which are just noise) so my regressive network is able to handle noisy images better. I want to look into a custom loss function which does a binary classification, followed by a regressive loss. I want it to look at an example and either classify it as too noisy or try and run a regressive (maybe L1loss) on it. I think this type of loss will help it handle noise better. Are there any examples of similar research or implementations that come to the community's mind?",6,1,False,self,,,,,
1317,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,2,9zqc05,self.MachineLearning,Resources for Stochastic Processes for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9zqc05/resources_for_stochastic_processes_for_machine/,Kliotionirst,1542995203,[removed],0,1,False,self,,,,,
1318,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,2,9zqeu2,self.MachineLearning,[YOLO] One object per grid cell?,https://www.reddit.com/r/MachineLearning/comments/9zqeu2/yolo_one_object_per_grid_cell/,emergenthoughts,1542995755,[removed],0,1,False,self,,,,,
1319,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,2,9zqfts,self.MachineLearning,Get new papers everyday,https://www.reddit.com/r/MachineLearning/comments/9zqfts/get_new_papers_everyday/,BBloggsbott,1542995936,[removed],1,1,False,self,,,,,
1320,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,3,9zqlfn,self.reinforcementlearning,[Course] Advanced Deep Learning and Reinforcement Learning course at UCL by DeepMind,https://www.reddit.com/r/MachineLearning/comments/9zqlfn/course_advanced_deep_learning_and_reinforcement/,banksyb00mb00m,1542996951,,1,1,False,default,,,,,
1321,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,3,9zqx5h,self.MachineLearning,[Discussion] Resources on Stochastic Processes in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/9zqx5h/discussion_resources_on_stochastic_processes_in/,Kliotionirst,1542999129,"Hi, I'm a novice in ML research. I've had a certain amount of exposure to Bayesian methods like Bayesian nonparametrics and recent papers like the Neural Processes have also tried to leverage the powerful framework of stochastic processes in Machine Learning and density estimation in general. Some example of Stochastic processes that come to my mind are: Point process, Gaussian process, Dirichlet process, Completely random measures, empirical processes(relevant in Learning theory), Markov random fields for nonparametric classification and many others.

I was looking to develop some intuition in stochastic processes by learning the relevant theory and some application-specific material. But I quickly get bogged down by intimidating mathematics around the probability structures. As a side note, I aim to start research in time-related domains and temporal data modeling. Is there a good, inclusive, accessible but exhaustive, resource for stochastic processes relevant for machine learning researchers? Appreciate all the answers.",2,1,False,self,,,,,
1322,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,4,9zrcke,self.MachineLearning,How to deal with large nominal attributes,https://www.reddit.com/r/MachineLearning/comments/9zrcke/how_to_deal_with_large_nominal_attributes/,Mandri97,1543002092,[removed],0,1,False,self,,,,,
1323,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,4,9zrdry,arxiv.org,On the Convergence Rate of Training Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/9zrdry/on_the_convergence_rate_of_training_recurrent/,IborkedyourGPU,1543002333,,29,1,False,default,,,,,
1324,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,5,9zs0tf,self.MachineLearning,Suturing with the PR2,https://www.reddit.com/r/MachineLearning/comments/9zs0tf/suturing_with_the_pr2/,ewusan,1543006779,[removed],0,1,False,self,,,,,
1325,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,6,9zs2wx,self.MachineLearning,"[R, Master Thesis] Inflated Multinomial Matching for Anchor-Free Object Detection",https://www.reddit.com/r/MachineLearning/comments/9zs2wx/r_master_thesis_inflated_multinomial_matching_for/,Skogsharald,1543007178,[removed],0,1,False,self,,,,,
1326,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,6,9zs40a,youtube.com,Medical Robots Are the Future of Surgery,https://www.reddit.com/r/MachineLearning/comments/9zs40a/medical_robots_are_the_future_of_surgery/,ewusan,1543007394,,0,1,False,default,,,,,
1327,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,6,9zs60v,self.MachineLearning,[D] Resources for Time Series Seg.,https://www.reddit.com/r/MachineLearning/comments/9zs60v/d_resources_for_time_series_seg/,Keyle_P,1543007791,"I am looking into time series segmentation right now and was hoping someone could link some reading material to better understand the problem and algorithms involved. If it helps to clarify what I am looking for my current project is to separate 2 years worth of seismic data into segments, hopefully separating the quakes from calm periods.",8,1,False,self,,,,,
1328,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,6,9zs96v,self.MachineLearning,[R] Inflated Multinomial Matching for Anchor-Free Object Detection,https://www.reddit.com/r/MachineLearning/comments/9zs96v/r_inflated_multinomial_matching_for_anchorfree/,Skogsharald,1543008431,"[Link](https://lup.lub.lu.se/student-papers/search/publication/8959431), [PDF](http://lup.lub.lu.se/student-papers/record/8959431/file/8959434.pdf)

Hi guys!

I am the author of this Master's Thesis which presents a stochastic matching strategy for training object detection models. The purpose of this matching strategy is to enable training of detection models without predefined anchor boxes, while still preserving the capability of several submodels that specialize towards specific box sizes and aspect ratios that comes from using anchor boxes.

When trying to develop a detection model from scratch, I was met with the hassle of defining the anchor boxes correctly across multiple feature maps and agnostic to the resolution of the input image. This led me to try to find a different way to achieve the benefits from anchors, but in an unsupervised way.

Happy to hear any feedback and questions!",1,1,False,self,,,,,
1329,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,7,9zsque,self.MachineLearning,Specific semi-supervised task,https://www.reddit.com/r/MachineLearning/comments/9zsque/specific_semisupervised_task/,Oktai15,1543011878,"We have D = {(x\_i, y\_i)}  labeled dataset, y\_i from  where C is set of considered classes. For example, dataset of images exotic fishes. Suppose we collected small amount of these fishes. However, we also have extra\_D = {(x\_i)} representing a huge dataset unmarked fishes that don't belong to , because it's dataset of usual fishes, not exotic.

Question: how can we get benefit of extra\_D? If it is possible, please, send me a concrete papers or describe ideas :)",0,1,False,self,,,,,
1330,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,7,9zst8t,self.MachineLearning,[P] Machine Learning Use Cases &amp; Demos,https://www.reddit.com/r/MachineLearning/comments/9zst8t/p_machine_learning_use_cases_demos/,seemingly_omniscient,1543012348,"We have made a short video of our machine learning projects. A kind of showreel. Hope you like it.

 

[Machine Learning Demo Video](https://youtu.be/dgGAJQ8sKf8)",4,1,False,self,,,,,
1331,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,8,9zt6kv,self.MachineLearning,WordnetLemmatizer VS SnowballStemmer,https://www.reddit.com/r/MachineLearning/comments/9zt6kv/wordnetlemmatizer_vs_snowballstemmer/,p1nkp3ngu1n,1543015102,[removed],0,1,False,self,,,,,
1332,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,10,9zu02z,youtube.com,Machine Learning Video Tutorials,https://www.reddit.com/r/MachineLearning/comments/9zu02z/machine_learning_video_tutorials/,ailearn12,1543021577,,0,1,False,default,,,,,
1333,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,10,9zucps,self.MachineLearning,Help in starting with Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/9zucps/help_in_starting_with_machine_learning/,durgesh2018,1543024529,[removed],0,1,False,self,,,,,
1334,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,12,9zv1i8,self.MachineLearning,Benchmarking Ryzen 5 2600 for ML/Scientitic computing,https://www.reddit.com/r/MachineLearning/comments/9zv1i8/benchmarking_ryzen_5_2600_for_mlscientitic/,hrshovon90,1543030116,[removed],0,1,False,self,,,,,
1335,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,12,9zv7ow,self.MachineLearning,Benchmarking Ryzen 5 2600 for scientific computing,https://www.reddit.com/r/MachineLearning/comments/9zv7ow/benchmarking_ryzen_5_2600_for_scientific_computing/,hrshovon90,1543031466,[removed],0,1,False,self,,,,,
1336,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,14,9zvqd7,self.MachineLearning,virtualizing CNNs with Batchnorm layers.,https://www.reddit.com/r/MachineLearning/comments/9zvqd7/virtualizing_cnns_with_batchnorm_layers/,hex0102,1543036176,[removed],0,1,False,self,,,,,
1337,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,14,9zvxmn,self.MachineLearning,VPN with AI-based routing ... is it just alchemy?,https://www.reddit.com/r/MachineLearning/comments/9zvxmn/vpn_with_aibased_routing_is_it_just_alchemy/,shadiakiki1986,1543038042,[removed],0,1,False,self,,,,,
1338,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,15,9zw9wz,self.MachineLearning,[D] Is there any Machine Learning code kata collection?,https://www.reddit.com/r/MachineLearning/comments/9zw9wz/d_is_there_any_machine_learning_code_kata/,banksyb00mb00m,1543041236,,15,1,False,self,,,,,
1339,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,17,9zwru8,self.MachineLearning,Using machine learning algorithm to approximate a matrix?,https://www.reddit.com/r/MachineLearning/comments/9zwru8/using_machine_learning_algorithm_to_approximate_a/,jengmge,1543046607,[removed],0,1,False,self,,,,,
1340,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,17,9zwsie,self.privacy,Can someone explain how Instagram targeted advertising works? They seem to be targeting with a whopping accuracy.,https://www.reddit.com/r/MachineLearning/comments/9zwsie/can_someone_explain_how_instagram_targeted/,UnstrungParadigm,1543046824,,0,1,False,default,,,,,
1341,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,18,9zx457,wired.com,Machine Learning Can Create Fake 'Master Key' Fingerprints,https://www.reddit.com/r/MachineLearning/comments/9zx457/machine_learning_can_create_fake_master_key/,elsamassenburg,1543050770,,0,1,False,default,,,,,
1342,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,18,9zx6yu,self.MachineLearning,What would the ideal site look like for people to come together and share/learn about machine learning topics?,https://www.reddit.com/r/MachineLearning/comments/9zx6yu/what_would_the_ideal_site_look_like_for_people_to/,fosa2,1543051744,[removed],0,1,False,self,,,,,
1343,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,18,9zxbko,medium.com,"Zeroth.ai - Announcing our newest programs in Bengaluru and Tokyo, and how the program works",https://www.reddit.com/r/MachineLearning/comments/9zxbko/zerothai_announcing_our_newest_programs_in/,lazy-jones,1543053296,,0,1,False,default,,,,,
1344,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,19,9zxmtb,self.MachineLearning,Is t-SNE still cutting edge or has there been a more effective data dimensionality algorithm developed recently?,https://www.reddit.com/r/MachineLearning/comments/9zxmtb/is_tsne_still_cutting_edge_or_has_there_been_a/,o-rka,1543057034,"If not, has it been improved since barnes hut? ",0,1,False,self,,,,,
1345,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,19,9zxn6g,self.MachineLearning,[R] Rethinking statistical learning theory: learning using statistical invariants,https://www.reddit.com/r/MachineLearning/comments/9zxn6g/r_rethinking_statistical_learning_theory_learning/,AnyPolicy,1543057167,"Vladimir Vapnik published it [here](https://link.springer.com/article/10.1007%2Fs10994-018-5742-0).

What do you think about it? It looks interesting.

Can someone explain it easily? How can we implement it by PyTorch or TensorFlow?",10,1,False,self,,,,,
1346,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,20,9zxro7,medium.com,Up and running with Keras: Deep Learning Digit Classification using CNN,https://www.reddit.com/r/MachineLearning/comments/9zxro7/up_and_running_with_keras_deep_learning_digit/,alliscode,1543058673,,0,1,False,default,,,,,
1347,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,20,9zxxi0,i.redd.it,DevOps Tutorials,https://www.reddit.com/r/MachineLearning/comments/9zxxi0/devops_tutorials/,vepambattuchandu111,1543060513,,0,1,False,https://b.thumbs.redditmedia.com/UKwsNosh379Lt_OFdzY5Vzgs-09gBhKPkh3cYM-5h3k.jpg,,,,,
1348,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,21,9zxzjd,self.MachineLearning,"I'm looking for a ""Photo Editing AI""",https://www.reddit.com/r/MachineLearning/comments/9zxzjd/im_looking_for_a_photo_editing_ai/,SYDWAD,1543061177,[removed],0,1,False,self,,,,,
1349,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,21,9zy00p,i.redd.it,DevOps Tutorials,https://www.reddit.com/r/MachineLearning/comments/9zy00p/devops_tutorials/,vepambattuchandu111,1543061339,,0,1,False,default,,,,,
1350,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,21,9zy364,self.MachineLearning,"[D] Looking for ""Photo Simulating AI"" if already made.",https://www.reddit.com/r/MachineLearning/comments/9zy364/d_looking_for_photo_simulating_ai_if_already_made/,SYDWAD,1543062332," I'm looking for a program where you can input photos/videos for it to learn the style then use it to edit photos/videos as a proof of concept that you can take new 3D animated shows and use AI to make them look like the old hand-drawn style. I know it can be done because people have made programs that simulate different painting styles i just want to know if it exists as an open source program or something already before i try myself. I am quite inexperienced when it comes to self correcting algorithms and programming in general so any help would be much appreciated.   
I remember something similar to this on github but i used it years ago.",3,1,False,self,,,,,
1351,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,21,9zy4ab,self.MachineLearning,[D] Measuring feature importance of variables in multivariate Time series models,https://www.reddit.com/r/MachineLearning/comments/9zy4ab/d_measuring_feature_importance_of_variables_in/,__Julia,1543062685,"I am benchmarking different multivariate Time series models and I would like to know if it makes sense to measure the importance of features by calculating correlation between features and target variable. 

Currently, I am working on a project that tries to identity which of many activities (represented as separate time series) have a significant effect on sales. I built a model to predict sales using these activities. I am benchmarking 1/ multivariate LSTM (currently benchmarking different variants), 2/ autoregressive model, and 3/ additive auto-regression model. 

In order to be able to explain the importance of features to other people in measurable way, I was thinking of dismissing features based on correlation with target variable. All the variables are continuous (Pearson is the choice in this case). I couldn't find a lot of literature on this one, so I hoped that someone can chime in, am I doing something wrong in here ?.",1,1,False,self,,,,,
1352,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,21,9zy6br,self.MachineLearning,[D] False certainty when using BayesByBackprop?,https://www.reddit.com/r/MachineLearning/comments/9zy6br/d_false_certainty_when_using_bayesbybackprop/,ooooooomooo,1543063305,"I am quite new to Machine Learning but I have been interested in Bayesian Neural Networks recently and have implemented the BayesByBackProp method. For 1d regression problems this gives uncertainty estimates which coincides with what I expect (more uncertainty where there is less data). But I wanted to try this method for a CNN and implemented the exact same method and tried on MNIST with a network with 2 convolutional layers and then a fully connected one:

While I get around 97% accuracy on the testdata after training, it seems like the posterior distribution almost converges to point estimates since if I try and feed it images where it should be uncertain (like random noise or samples from MNIST which are ambiguous) it gives me the same prediction every time I run the network. 

What is going on here and are there any solutions to this problem? Does the large amount of samples make the posterior distribution think that it have seen all possible forms of the data? ",10,1,False,self,,,,,
1353,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,22,9zyj1a,/r/MachineLearning/comments/9zyj1a/interactive_style_transfer_using_style_palettes/,Interactive style transfer using style palettes and style brushes,https://www.reddit.com/r/MachineLearning/comments/9zyj1a/interactive_style_transfer_using_style_palettes/,kh22l22,1543066922,,0,1,False,https://b.thumbs.redditmedia.com/Xr39Wrkuqxq6MpG2fw6TySUIMLqyov1hpUAEMPVUxZQ.jpg,,,,,
1354,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,22,9zym2b,self.MachineLearning,Artificial Intelligence Traffic Lights (Idea),https://www.reddit.com/r/MachineLearning/comments/9zym2b/artificial_intelligence_traffic_lights_idea/,Jokniserious,1543067638,[removed],0,1,False,self,,,,,
1355,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,23,9zyo99,self.MachineLearning,[P] Interactive Style Transfer,https://www.reddit.com/r/MachineLearning/comments/9zyo99/p_interactive_style_transfer/,kh22l22,1543068099,[removed],0,1,False,self,,,,,
1356,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,23,9zypza,self.MachineLearning,Can machine learning be used to predict the future appearance of someone? (Idea),https://www.reddit.com/r/MachineLearning/comments/9zypza/can_machine_learning_be_used_to_predict_the/,Jokniserious,1543068476,[removed],0,1,False,self,,,,,
1357,MachineLearning,t5_2r3gv,2018-11-24,2018,11,24,23,9zyqt7,/r/MachineLearning/comments/9zyqt7/slapslap_an_attempt_to_use_the_latest_ai_tech_in/,SlapSlap - An attempt to use the latest AI tech in mobile game.,https://www.reddit.com/r/MachineLearning/comments/9zyqt7/slapslap_an_attempt_to_use_the_latest_ai_tech_in/,shallwayjp,1543068669,,0,1,False,default,,,,,
1358,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,0,9zzj9l,self.MachineLearning,Bootstrap Sampling,https://www.reddit.com/r/MachineLearning/comments/9zzj9l/bootstrap_sampling/,cagataydmr1,1543075025,[removed],0,1,False,self,,,,,
1359,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,0,9zzjxu,self.MachineLearning,"How does your machine learning/deep learning pipeline look like in the production? How do you handle models from different frameworks (sklearn, tensorflow, pytorch...), preprocessing pipelines, model versioning? Any reading suggestions on this matter?",https://www.reddit.com/r/MachineLearning/comments/9zzjxu/how_does_your_machine_learningdeep_learning/,belma_i,1543075177,,0,1,False,self,,,,,
1360,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,1,9zzv8w,arxiv.org,How Many Samples are Needed to Learn a Convolutional Neural Network?,https://www.reddit.com/r/MachineLearning/comments/9zzv8w/how_many_samples_are_needed_to_learn_a/,nobodykid23,1543077414,,19,1,False,default,,,,,
1361,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,2,a006p1,self.MachineLearning,Landslide dataset,https://www.reddit.com/r/MachineLearning/comments/a006p1/landslide_dataset/,collided_equations,1543079601,"Hello guys.
In my coursework, I am needed to model natural disasters. I have selected landslide for this.
Does anyone know where can I find dataset for landslides and various factors like rainfall, soil type, bed rock type, faults, etc? (Slope will be generated from DEM)",0,1,False,self,,,,,
1362,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,2,a00c21,self.MachineLearning,AISTATS compared to NIPS/ICML/ICLR,https://www.reddit.com/r/MachineLearning/comments/a00c21/aistats_compared_to_nipsicmliclr/,mlnewcomer1,1543080669,[removed],0,1,False,self,,,,,
1363,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,2,a00k5f,self.MachineLearning,[D] Good journals or conferences for a paper on NLP,https://www.reddit.com/r/MachineLearning/comments/a00k5f/d_good_journals_or_conferences_for_a_paper_on_nlp/,lcukerd,1543082213,"I have a paper (work ready, paper not) on NLP using an attention-based model. I did look for a conference but the registration was pretty expensive (450USD), at least for a student. Is there any other upcoming conference that isn't that expensive (or is that the usual price)? 

Also, If you know any good journal for same then please share.

The only reason for me going for a conference was to make my paper publish early but I am fine with being a bit late and publishing in a Journal directly if the conference doesn't work out.",10,1,False,self,,,,,
1364,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,3,a00ska,self.MachineLearning,[D] How to get started with creating your own recommending system?,https://www.reddit.com/r/MachineLearning/comments/a00ska/d_how_to_get_started_with_creating_your_own/,pandeykartikey,1543083781,,2,1,False,self,,,,,
1365,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,3,a00wij,self.MachineLearning,"[R] Tencent ML-Images released: 18 million training images with 11,000 categories",https://www.reddit.com/r/MachineLearning/comments/a00wij/r_tencent_mlimages_released_18_million_training/,rantana,1543084514,"Tencent's ML-Images dataset is now available: https://github.com/Tencent/tencent-ml-images

ML-Images: the largest open-source multi-label image database, including 17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories

Resnet-101 model: it is pre-trained on ML-Images, and achieves the top-1 accuracy 80.73% on ImageNet via transfer learning
",15,1,False,self,,,,,
1366,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,3,a011z7,self.MachineLearning,[Discussion] ML Online Study Group,https://www.reddit.com/r/MachineLearning/comments/a011z7/discussion_ml_online_study_group/,seldoz,1543085570,"Hi Guy, I am a final year design undergraduate looking get hands dirty with machine learning before I graduate. I want to recruit few people to discuss topics as we learn and stay motivated. My plan is go first with [Mathematical foundations](https://www.youtube.com/watch?v=7MN3OP1IYk8&amp;list=PL7y-1rk2cCsA339crwXMWUaBRuLBvPBCg) and then go for [bloomberg course](https://www.youtube.com/playlist?list=PLnZuxOufsXnvftwTB1HL6mel1V32w0ThI) on machine learning.

Also looking for suggestions on my track to ML chosen ( the courses ).

I have created a [discord Server](https://discord.gg/MQwmwWF).

Interested people please join.

P.s. I prefer python",1,1,False,self,,,,,
1367,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,4,a01bit,self.MachineLearning,"[Discussion] Using linear regression in time series, iid assumption",https://www.reddit.com/r/MachineLearning/comments/a01bit/discussion_using_linear_regression_in_time_series/,amil123123,1543087339,"Hey,

I read about ARIMA models and while performing regression like the linear regression on Time Series Data we seem to lose the independence property since if I know X\[i-1\] &amp; X\[i+1\] X\[i\] would depend on the those two and others also in the selected time series window. I have 3 questions here -

1. Is the Identical assumption also broken?
2. How do you deal with these assumptions - continue modeling, or not use models making such assumption or deal with data somehow?
3. How severe are the effects of such assumption being broken?",5,1,False,self,,,,,
1368,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,4,a01dpw,self.MachineLearning,[D] Deep learning dataset file formats for at scale?,https://www.reddit.com/r/MachineLearning/comments/a01dpw/d_deep_learning_dataset_file_formats_for_at_scale/,averagepooling,1543087761,"I was wondering what those who use deep learning at scale use for non-image dataset file formats. 

For my project, Spark is used to create the dataset so I'm thinking about using a row oriented file format like Avro (over CSV). 

But, I was wondering what is considered best practice. Or are databases preferred over flat files?",8,1,False,self,,,,,
1369,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,4,a01i1m,self.MachineLearning,[D] What would the ideal site look like for people to come together and share/learn about machine learning topics?,https://www.reddit.com/r/MachineLearning/comments/a01i1m/d_what_would_the_ideal_site_look_like_for_people/,fosa2,1543088530,"There are many strong pieces of the puzzle here and there. Wikipedia has lots of content, though not particularly structured for ease of understanding. Coursera focuses on ease of understanding and has well structured video content and some interactivity via forums and homework. Quora has a huge user base with an 'ask anything' culture. Reddit has a group of enthusiastic anonymous professionals who sporadically dive into topics in great depth. Youtube has videos for a huge variety of levels/topics. Arxiv has more research papers than anyone could hope to read in a lifetime. Github has code repositories and open source implementations of great models.

Given the existence of these massive databases of content, it seems an ideal site couldn't hope to mimic all these features and functionality, right? ""One site to rule them all"" may not be realistic, and the organic sprawling nature of the web is charming, but if we were to just theorize about how an ideal site would organize content and user interactivity, what might it look like?",2,1,False,self,,,,,
1370,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,4,a01l0y,self.MachineLearning,Newbie : Speech recognition - numbers ?,https://www.reddit.com/r/MachineLearning/comments/a01l0y/newbie_speech_recognition_numbers/,muaythaidripper,1543089045,[removed],0,1,False,self,,,,,
1371,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,5,a01qof,self.MachineLearning,Why GoogLeNet and Inception V3 retrained model size is less compared to others?,https://www.reddit.com/r/MachineLearning/comments/a01qof/why_googlenet_and_inception_v3_retrained_model/,AndrewMathy,1543090066,[removed],0,1,False,self,,,,,
1372,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,5,a01rxx,self.MachineLearning,"No coding experience, where to begin?",https://www.reddit.com/r/MachineLearning/comments/a01rxx/no_coding_experience_where_to_begin/,rriittaa,1543090314,"I'd like to create a program that takes textual input, identifies key words, and translates them into a graphical output in the same order. A good example would be how your phone suggests emojis when you type a word, but a little more nuanced. It seems relatively simple, but maybe not. 

I know close to nothing about coding, let alone machine learning, so I don't know where to start. What's the language or program I should be using to do this? Where do I begin? It seems like maybe I would start with learning to build a search engine or something? Would appreciate any tips!",0,1,False,self,,,,,
1373,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,6,a028go,self.MachineLearning,[Question] How to build your own image classification,https://www.reddit.com/r/MachineLearning/comments/a028go/question_how_to_build_your_own_image/,MeAnotherRandomGuy,1543093566,[removed],0,1,False,self,,,,,
1374,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,6,a0292m,marktechpost.com,"How Machine Learning is Transforming Health Policy [Research], [R]",https://www.reddit.com/r/MachineLearning/comments/a0292m/how_machine_learning_is_transforming_health/,asifrazzaq1988,1543093685,,0,1,False,default,,,,,
1375,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,7,a02p59,self.MachineLearning,[Discussion] Do you think recommendation engines is the biggest threat to society in ML?,https://www.reddit.com/r/MachineLearning/comments/a02p59/discussion_do_you_think_recommendation_engines_is/,pure_x01,1543096824,"I'm really worried about recommendation engines causing a shift in people's views to the extreme. A sort of AI lead self brainwashing. I have been watching YouTube some and I clicked on a video regarding feminism. I'm very interested in the subject from a spectator point of view. I noticed that I got more videos recommended and they got more extreme. Against feminism. I'm personally not against feminism but some parts of it. The types of videos shows extreme feminists with very radical views. It would  be possible to start associating all feminism with these radical individuals and that's what is scary. 

The problem as I see it is that if you start looking for something like ""immigrant problems In france"" you could slowly get more and more content giving you a false image of the actual world. TV is moderated but the recommendation engines are not. It might slowly change your view to become more extreme. If you are then very active I discussion on say reddit you might start spreading those views to others.


I'm honestly scared that recommendation engines might slowly help people brainwash themselves in to more extreme views of the world. It's like when you get annoyed over all Simpson recommendations but with Important society topics. 


People also tend to have confirmation bias and wants to view stuff that makes them feel that their opinions are correct. The recommendation went suddenly start to spit out videos that contradicts your view either.


What do you think of the recommendation engines effect on society?",13,1,False,self,,,,,
1376,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,7,a02ret,self.MachineLearning,How to recognize makeup type on skin color with deep learning?,https://www.reddit.com/r/MachineLearning/comments/a02ret/how_to_recognize_makeup_type_on_skin_color_with/,D3ntrax,1543097243,[removed],0,1,False,self,,,,,
1377,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,7,a036lj,self.MachineLearning,"[D] In a couple of short sentences, can you explain your thesis?",https://www.reddit.com/r/MachineLearning/comments/a036lj/d_in_a_couple_of_short_sentences_can_you_explain/,its_ya_boi_dazed,1543100346,,0,1,False,self,,,,,
1378,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,8,a037t1,self.MachineLearning,"[R] A ""new"" kind of Information Processing in the Brain",https://www.reddit.com/r/MachineLearning/comments/a037t1/r_a_new_kind_of_information_processing_in_the/,seemingly_omniscient,1543100580," Resonance and harmony could play a key role in communications in neural networks. A few thoughts or perhaps too ""naive"" considerations of me in this regard 

[https://www.linkedin.com/pulse/new-kind-information-processing-brain-murat-durmus/](https://www.linkedin.com/pulse/new-kind-information-processing-brain-murat-durmus/)",4,1,False,self,,,,,
1379,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,8,a038lq,self.MachineLearning,Are there any Conv Net tutorials that do not use any frameworks?,https://www.reddit.com/r/MachineLearning/comments/a038lq/are_there_any_conv_net_tutorials_that_do_not_use/,autojazari,1543100738,[removed],0,1,False,self,,,,,
1380,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,8,a03eif,arxiv.org,"[R] Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and Interpretability",https://www.reddit.com/r/MachineLearning/comments/a03eif/r_simplicity_creates_inequity_implications_for/,RepresentativeAgent,1543101990,,0,1,False,default,,,,,
1381,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,9,a03vah,self.MachineLearning,How to make changes from theano to TensorFlow ,https://www.reddit.com/r/MachineLearning/comments/a03vah/how_to_make_changes_from_theano_to_tensorflow/,ejiido,1543105545,[removed],0,1,False,self,,,,,
1382,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,11,a04o5v,self.MachineLearning,"Updates on Perturbative Neural Networks (PNN), CVPR 18 Reproducibility",https://www.reddit.com/r/MachineLearning/comments/a04o5v/updates_on_perturbative_neural_networks_pnn_cvpr/,katanaxu,1543112124,[removed],0,1,False,self,,,,,
1383,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,11,a04qsj,self.MachineLearning,"[D] Updates on Perturbative Neural Networks (PNN), CVPR 18 Reproducibility",https://www.reddit.com/r/MachineLearning/comments/a04qsj/d_updates_on_perturbative_neural_networks_pnn/,katanaxu,1543112733," Hi there, I am the lead author of the Perturbative Neural Networks (PNN) paper published at CVPR 18. A while ago there was a Reddit post about this paper. The original Reddit discussion can be found here: 

I tried to reproduce results from a CVPR18 paper, here's what I found.

[https://www.reddit.com/r/MachineLearning/comments/9jhhet/discussion\_i\_tried\_to\_reproduce\_results\_from\_a/?sort=confidence](https://www.reddit.com/r/MachineLearning/comments/9jhhet/discussion_i_tried_to_reproduce_results_from_a/?sort=confidence)

&amp;#x200B;

Following this post, I went ahead and performed a thorough investigation of Michael Klachkos implementation. Results of our analysis can be found at this Github repo: [https://github.com/juefeix/pnn.pytorch.update](https://github.com/juefeix/pnn.pytorch.update)

**TL;DR** (1) alleged performance drop (\~5%) is primarily due to various inconsistencies in Michael Klachkos implementation of PNN and sub-optimal choice of hyper-parameters. (2) the practical effectiveness of PNN method stands.",33,1,False,self,,,,,
1384,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,14,a05w5w,youtube.com,"100hrs of beauty bridging bio &amp; math/stat/cs/ml [Models, Inference and Algorithms Meeting]",https://www.reddit.com/r/MachineLearning/comments/a05w5w/100hrs_of_beauty_bridging_bio_mathstatcsml_models/,savanmorya,1543122937,,0,1,False,default,,,,,
1385,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,14,a05yfr,self.MachineLearning,[D] An idea: recycling outputs into inputs for memory and more,https://www.reddit.com/r/MachineLearning/comments/a05yfr/d_an_idea_recycling_outputs_into_inputs_for/,happysmash27,1543123547,"Has anyone done this yet? I thought of this showerthought on machine learning. 

There is an idea that consciousness arises through the brain reading its own outputs, in what is called a strange loop. I personally find that with my own short-term memory, I often reiterate something, also in a loop of outputs and inputs. So couldn't this same type of loop be used for machine learning?

For long term memory, perhaps a certain output could cause a certain part of the neural network containing a memory through reinforcement to output the memory. Perhaps it could even contain links to other memories 

For time, perhaps the network could cycle at a set rate. Maybe an alternating signal could evolve to tell the time. 

I think recycling output nodes into input nodes could lead to a lot of human-like thinking abilities. Has this been tried/thought of before? ",3,1,False,self,,,,,
1386,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,14,a064zy,self.MachineLearning,Price optimization,https://www.reddit.com/r/MachineLearning/comments/a064zy/price_optimization/,saiyanGold,1543125292,"I am trying to solve price optimization problem but methods like linear regression doesn't seems to work well as the data (price, quantity) is not linear.

What are some simple methods to solve the problem? Will random forest work? ",0,1,False,self,,,,,
1387,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,16,a06mmy,self.MachineLearning,Change hair color using deep learning,https://www.reddit.com/r/MachineLearning/comments/a06mmy/change_hair_color_using_deep_learning/,intermenos,1543130416,[removed],0,1,False,self,,,,,
1388,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,16,a06mzw,self.MachineLearning,[D] Optimal code length versus cross-entropy?,https://www.reddit.com/r/MachineLearning/comments/a06mzw/d_optimal_code_length_versus_crossentropy/,fundamentalidea,1543130525,"The article Blier et al The Description Length Of Deep Learning Models has this statement

[https://imgur.com/a/Xuv8FvO](https://imgur.com/a/Xuv8FvO)

""The shannon-huffman optimal code is exactly the categorical cross-entropy loss evaluated on the model p""

&amp;#x200B;

My question: a cross-entropy should involve \_two\_ probability densities (say p, q), but there is only one here (p).

&amp;#x200B;

&amp;#x200B;",3,1,False,self,,,,,
1389,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,16,a06tkl,blog.briantliao.com,Machine Learning UW CSE,https://www.reddit.com/r/MachineLearning/comments/a06tkl/machine_learning_uw_cse/,apqwer,1543132770,,1,1,False,default,,,,,
1390,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,17,a06ybw,self.MachineLearning,The decoder in Transformer model (Attention is all you need),https://www.reddit.com/r/MachineLearning/comments/a06ybw/the_decoder_in_transformer_model_attention_is_all/,albert1905,1543134425,[removed],0,1,False,self,,,,,
1391,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,17,a073a6,self.MachineLearning,Can BERT be used for sentence generating tasks?,https://www.reddit.com/r/MachineLearning/comments/a073a6/can_bert_be_used_for_sentence_generating_tasks/,fzyzcjy,1543136184,[removed],0,1,False,self,,,,,
1392,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,18,a0752d,self.MachineLearning,[D] Can BERT be used for sentence generating tasks?,https://www.reddit.com/r/MachineLearning/comments/a0752d/d_can_bert_be_used_for_sentence_generating_tasks/,fzyzcjy,1543136787,"Currently, there are some methods like CharRNN to generate sentences. BERT has come out several weeks ago and seems to be very powerful. Therefore, I am wondering whether sentence generation can be done better with the aid of BERT?

Thank you for any discussions!",9,1,False,self,,,,,
1393,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,18,a07acm,self.MachineLearning,The best paper of ICLR 2017 promised open source code but no update after one and half year,https://www.reddit.com/r/MachineLearning/comments/a07acm/the_best_paper_of_iclr_2017_promised_open_source/,deeppeng,1543138638,[removed],0,1,False,self,,,,,
1394,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,20,a07pld,self.MachineLearning,Neural Networks from a Bayesian Perspective,https://www.reddit.com/r/MachineLearning/comments/a07pld/neural_networks_from_a_bayesian_perspective/,andrea_manero,1543143937,https://www.datasciencecentral.com/profiles/blogs/neural-networks-from-a-bayesian-perspective,0,1,False,self,,,,,
1395,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,20,a07rft,arxiv.org,[R] A unified theory of adaptive stochastic gradient descent as Bayesian filtering,https://www.reddit.com/r/MachineLearning/comments/a07rft/r_a_unified_theory_of_adaptive_stochastic/,abstractcontrol,1543144591,,0,1,False,default,,,,,
1396,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,21,a089o4,i.redd.it,Irony of Fate: people with artificial intelligence have been trying to develop and implement artificial intelligence Integrated Electronics = INTegrated ELECTronics = INTELLECT = Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/a089o4/irony_of_fate_people_with_artificial_intelligence/,Green-7,1543150449,,1,1,False,default,,,,,
1397,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,21,a08apv,i.redd.it,My implementation of QuickDraw - an online gamed developed by Google (https://github.com/1991viet/QuickDraw),https://www.reddit.com/r/MachineLearning/comments/a08apv/my_implementation_of_quickdraw_an_online_gamed/,1991viet,1543150758,,1,1,False,default,,,,,
1398,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,22,a08b9x,medium.com,Path to learn Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a08b9x/path_to_learn_machine_learning/,alokrajg,1543150912,,0,1,False,https://b.thumbs.redditmedia.com/cXYqCEmX4R6quIoUG84-OVxpxrqq5E0FzNhi5jInRNM.jpg,,,,,
1399,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,22,a08dpo,self.MachineLearning,[P] Understanding Nesterov Momentum (NAG),https://www.reddit.com/r/MachineLearning/comments/a08dpo/p_understanding_nesterov_momentum_nag/,dominik_schmidt,1543151581,"I wrote an article about nesterov momentum (nesterov accelerated gradient) on my blog. If you find any errors or think something could be improved or clarified, please comment or pm me :)

You can read it here: [https://dominikschmidt.xyz/nesterov-momentum/](https://dominikschmidt.xyz/nesterov-momentum/)",4,1,False,self,,,,,
1400,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,22,a08fuu,self.MachineLearning,The Irony of Fate: people with artificial intelligence have been trying to develop and implement artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/a08fuu/the_irony_of_fate_people_with_artificial/,Green-7,1543152196,[removed],0,1,False,self,,,,,
1401,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,22,a08fx6,self.MachineLearning,[D] Why `tf.data` is so much better than `feed_dict` and how to build a simple data pipeline in 5 minutes.,https://www.reddit.com/r/MachineLearning/comments/a08fx6/d_why_tfdata_is_so_much_better_than_feed_dict_and/,dominik_schmidt,1543152216,I wrote an article about `tf.data` and why it is so much better than `feed_dict`. You can read it here: [https://dominikschmidt.xyz/tensorflow-data-pipeline/](https://dominikschmidt.xyz/tensorflow-data-pipeline/),40,1,False,self,,,,,
1402,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,22,a08h4q,self.MachineLearning,"Suggestion for a ""time-series"" regression?",https://www.reddit.com/r/MachineLearning/comments/a08h4q/suggestion_for_a_timeseries_regression/,macoit18,1543152527,[removed],0,1,False,self,,,,,
1403,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,22,a08ibj,self.MachineLearning,[P] GAN-generated images from the Karolinska Face Dataset,https://www.reddit.com/r/MachineLearning/comments/a08ibj/p_gangenerated_images_from_the_karolinska_face/,dominik_schmidt,1543152836,"I trained a vanilla GAN on the Karolinska KDEF Dataset. You can see the results here: [https://dominikschmidt.xyz/kdef-gan-gallery/](https://dominikschmidt.xyz/kdef-gan-gallery/)

I had a lot of trouble with the networks diverging before it was trained well and also with mode collapse. Apart from other GAN architectures, is there anything else that can improve the training stability?",0,1,False,self,,,,,
1404,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,22,a08kms,self.MachineLearning,[P] mars32k Mars Surface Dataset,https://www.reddit.com/r/MachineLearning/comments/a08kms/p_mars32k_mars_surface_dataset/,dominik_schmidt,1543153455,"I recently had to collect and crawl a lot of mars surface images for a project. I published it on my blog in case it helps anyone.

The dataset consists of about 32,000 color images collected by the Curiosity rover on Mars between August 2012 and November 2018. The images show various geographical and geological features of Mars such as mountains and valleys, craters, dunes and rocky terrain.

[https://dominikschmidt.xyz/mars32k/](https://dominikschmidt.xyz/mars32k/)",3,1,False,self,,,,,
1405,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,23,a08p52,self.MachineLearning,Can Bayesian Deep learning help reduce the hyperparameter tuning complexity?,https://www.reddit.com/r/MachineLearning/comments/a08p52/can_bayesian_deep_learning_help_reduce_the/,dvijayd,1543154558,[removed],0,1,False,self,,,,,
1406,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,23,a08reg,i.redd.it,"Training people is the same machine learning, so there is no need to create separate sections in this application - the ""Education"" and the ""MachineLearning""",https://www.reddit.com/r/MachineLearning/comments/a08reg/training_people_is_the_same_machine_learning_so/,Green-7,1543155088,,1,1,False,default,,,,,
1407,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,23,a08u4v,self.MachineLearning,I'm trying to create my first neural net that detect when letters are in a word,https://www.reddit.com/r/MachineLearning/comments/a08u4v/im_trying_to_create_my_first_neural_net_that/,RaccoonTeachesYou,1543155739,[removed],0,1,False,self,,,,,
1408,MachineLearning,t5_2r3gv,2018-11-25,2018,11,25,23,a08xv6,youtube.com,"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier. Hope you guys found it helpful!",https://www.reddit.com/r/MachineLearning/comments/a08xv6/for_me_one_of_the_main_barriers_to_the_world_of/,antaloaalonso,1543156648,,0,1,False,default,,,,,
1409,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,0,a099dh,i.redd.it,My implementation of QuickDraw - an online game developed by Google (https://github.com/1991viet/QuickDraw),https://www.reddit.com/r/MachineLearning/comments/a099dh/my_implementation_of_quickdraw_an_online_game/,1991viet,1543159299,,1,1,False,default,,,,,
1410,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,0,a09dep,self.MachineLearning,[R] Understanding Normalizing Flows,https://www.reddit.com/r/MachineLearning/comments/a09dep/r_understanding_normalizing_flows/,SolitaryPenman,1543160175,"I think **normalizing flows** is an interesting direction of research. I prepared study material and gave a short lecture on normalizing flows as part of a course project. Everything is hosted in this GitHub repo: [https://github.com/abdulfatir/normalizing-flows](https://github.com/abdulfatir/normalizing-flows)

&amp;#x200B;

Currently, it contains the following things:

* A technical report (more like a review of NF literature)
* A few code examples to tinker with basic flows
* Implementation of VAEs with Planar Flows and reproduction of some results from **Rezende and Mohamed (2015)**
* Lecture notes (will add soon)

I am also planning to write more blog posts and add implementations of a few more papers. Suggestions/Requests/Feedback welcome.

&amp;#x200B;

Cheers! :) 

&amp;#x200B;",3,1,False,self,,,,,
1411,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,0,a09hrv,naiyonder.com,Storing Files in Azure,https://www.reddit.com/r/MachineLearning/comments/a09hrv/storing_files_in_azure/,okelloaliwa,1543161077,,0,1,False,default,,,,,
1412,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,0,a09ip9,self.MachineLearning,What is the state of the art of one shot learning mnist task?,https://www.reddit.com/r/MachineLearning/comments/a09ip9/what_is_the_state_of_the_art_of_one_shot_learning/,galharth,1543161272,[removed],0,1,False,self,,,,,
1413,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,1,a09taz,self.MachineLearning,What is the state of the art of machine learning model explanlibilty for computer vision tasks?,https://www.reddit.com/r/MachineLearning/comments/a09taz/what_is_the_state_of_the_art_of_machine_learning/,galharth,1543163355,[removed],0,1,False,self,,,,,
1414,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,2,a0a3bz,self.MachineLearning,arxiv-sanity.com is down?,https://www.reddit.com/r/MachineLearning/comments/a0a3bz/arxivsanitycom_is_down/,lepton99,1543165226,[removed],0,1,False,self,,,,,
1415,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,2,a0a3gb,twitch.tv,Machine Learning ep 10 - GAN Colab Notebook! - Come learn with me!,https://www.reddit.com/r/MachineLearning/comments/a0a3gb/machine_learning_ep_10_gan_colab_notebook_come/,Rainymood_XI,1543165242,,1,1,False,default,,,,,
1416,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,2,a0ams3,self.MachineLearning,How to forecast one time step at a time with updated?,https://www.reddit.com/r/MachineLearning/comments/a0ams3/how_to_forecast_one_time_step_at_a_time_with/,AViCiDi,1543168652,"Hi, I'm new to machine learning, and I've been doing a lot of research but I've only managed to find examples of cases where models have been trained to forecast x days in advance at a time.

Is there a simple way to implement a testing regime such that my network forecasts the value (or direction) of the next time step t+1, then update the model with the real value of t+1, before computing the prediction for t+2?

Or is this already possible and easy to implement with tensorflow?

Thank you",0,1,False,self,,,,,
1417,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,3,a0ax3v,i.redd.it,Why aren't there any videos of researchers showing off their workstations with racks of V100s or DGXs? Why no videos of them showing off the machines and a peak at what they use them for?,https://www.reddit.com/r/MachineLearning/comments/a0ax3v/why_arent_there_any_videos_of_researchers_showing/,engine_town_rattler,1543170560,,0,1,False,default,,,,,
1418,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,3,a0azdp,youtube.com,"For anyone looking to get into deep learning, I would advise that you consider not learning the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",https://www.reddit.com/r/MachineLearning/comments/a0azdp/for_anyone_looking_to_get_into_deep_learning_i/,antaloaalonso,1543170976,,0,1,False,https://b.thumbs.redditmedia.com/VeOOWnBsA49wJCtCan9jcEGRYYS_UxVWwAmiJ07YSPM.jpg,,,,,
1419,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,3,a0b0ff,i.redd.it,Why aren't there any videos of researchers showing off their workstations with racks of V100s or DGXs? Why no videos of them showing off the machines and a peek at what they use them for? Get the Computerphile people on it.,https://www.reddit.com/r/MachineLearning/comments/a0b0ff/why_arent_there_any_videos_of_researchers_showing/,engine_town_rattler,1543171183,,0,1,False,default,,,,,
1420,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,3,a0b5l7,self.MachineLearning,Recommender systems project ideas,https://www.reddit.com/r/MachineLearning/comments/a0b5l7/recommender_systems_project_ideas/,clavamxr,1543172123,[removed],0,1,False,self,,,,,
1421,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,4,a0b8lh,self.MachineLearning,How do I train ProGAN with labels?,https://www.reddit.com/r/MachineLearning/comments/a0b8lh/how_do_i_train_progan_with_labels/,dankmaster2k,1543172648,[removed],0,1,False,self,,,,,
1422,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,4,a0bekp,self.MachineLearning,Academic ai conferences,https://www.reddit.com/r/MachineLearning/comments/a0bekp/academic_ai_conferences/,vonum,1543173715,[removed],0,1,False,self,,,,,
1423,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,4,a0bp8u,self.MachineLearning,Almost finished uni and about to apply for jobs. Do I need a laptop?,https://www.reddit.com/r/MachineLearning/comments/a0bp8u/almost_finished_uni_and_about_to_apply_for_jobs/,Thybert,1543175598,[removed],0,1,False,self,,,,,
1424,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,7,a0czuc,self.MachineLearning,"Hi guys. Im a freshman in Economics and Im really really interested in learning stuff aside, specially ML. Do you guys have any recommendation on where I should start ? It seems that there are sooo many stuff and its evolving quickly  Much appreciated!",https://www.reddit.com/r/MachineLearning/comments/a0czuc/hi_guys_im_a_freshman_in_economics_and_im_really/,NeoTheAnalyst,1543183927,[removed],0,1,False,self,,,,,
1425,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,9,a0e7r5,self.MachineLearning,[D] Introductory-level blog post for SQuAD 2.0 and UNet,https://www.reddit.com/r/MachineLearning/comments/a0e7r5/d_introductorylevel_blog_post_for_squad_20_and/,fosa2,1543192511,"For my own edification, and possibly the interest of others, I've taken the advice of some influential figures in the field and started a blog! I'd eventually like to wind up at cognitive architectures and AGI but there are a lot of things to look at between here and there. Looking at state of the art Question-Answering reasoning systems seemed like a good place to start.

 [https://betterlearningforlife.com/2018/11/16/question-answer-architectures-squad-2-0-u-net/](https://betterlearningforlife.com/2018/11/16/question-answer-architectures-squad-2-0-u-net/)",7,1,False,self,,,,,
1426,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,10,a0ereb,self.MachineLearning,[D] Normalizing flows - VAE Doubt,https://www.reddit.com/r/MachineLearning/comments/a0ereb/d_normalizing_flows_vae_doubt/,WillingCucumber,1543196526,"Hi all,

I have a doubt regarding the normalizing flows, used to enhance the posterior predicted by the inference model of the VAE, by a chain of invertible transformations. 

My doubt is : Can't the generative model learn those invertible transformations rather than having explicit matrices to do that. I am referring to the simpler planar and radial flows. I understand the use-cases where the only requirement is to have the samples, but I am confused regarding the  cases where some NN post-processing is required on top of the samples. 

&amp;#x200B;

Thanks !!

&amp;#x200B;",3,1,False,self,,,,,
1427,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,11,a0f7jg,self.MachineLearning,[D] How does DeepMind organize science?,https://www.reddit.com/r/MachineLearning/comments/a0f7jg/d_how_does_deepmind_organize_science/,llrful,1543199949,"Demis Hassabis has [spoken](https://youtu.be/d-bvsJWmqlc?t=165) about DeepMind's goal to transform the way science is conducted, fusing the best of academia with the best of the start-up world. Has he spoken in more detail about how this is implemented in practice? Have any DeepMind employees written about this, or would you be willing to comment?",4,1,False,self,,,,,
1428,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,11,a0fbre,self.MachineLearning,Gradient Descent Questions - Urgent (final coming up),https://www.reddit.com/r/MachineLearning/comments/a0fbre/gradient_descent_questions_urgent_final_coming_up/,memesap,1543200854,[removed],0,1,False,self,,,,,
1429,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,12,a0fei9,self.MachineLearning,Datacamp subscription giveaway,https://www.reddit.com/r/MachineLearning/comments/a0fei9/datacamp_subscription_giveaway/,bumchika,1543201415,[removed],0,1,False,self,,,,,
1430,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,12,a0ferp,self.MachineLearning,Is Google AI Residency (Healthcare) worth it after a postdoc?,https://www.reddit.com/r/MachineLearning/comments/a0ferp/is_google_ai_residency_healthcare_worth_it_after/,xySurfer,1543201473,[removed],0,1,False,self,,,,,
1431,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,13,a0g0i8,self.MachineLearning,NIPS Tickets Available!,https://www.reddit.com/r/MachineLearning/comments/a0g0i8/nips_tickets_available/,MonstarGaming,1543206301,[removed],0,1,False,self,,,,,
1432,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,13,a0g59s,self.MachineLearning,[D] Help with Deep Learning Workstation Configurations,https://www.reddit.com/r/MachineLearning/comments/a0g59s/d_help_with_deep_learning_workstation/,wubba-luba,1543207397,"We are buying workstations for DL, after homework, we have finalized the below configuration for each machine.Can anyone please verify it and address our concerns (mentioned below). 

&amp;#x200B;

*Each Workstation Configuration:*  
*Processor: lntel Xeon Processor E5-2620 v4 (20M Cache, 2.10 GHz)*  
*Chipset: Intel X299*  
*Memory: 64 GB(4x16) RAM @2666 MHz Expandable Upto 128 GB with 8 Dimm Slots*  
*Storage: Ix2 TB SSD. Ix 4 TB HDD SATA @7200 RPM*  
*Gpu Support: Supports NVIDIA 4-Way SLlTM Technology*  
*GPU: 4xNvidia GTX 1080 Ti 11 GB GDDR5X GPU*  
*Form Factor: Micro Tower*

&amp;#x200B;

We have the following concerns:

1. What motherboard should we use for the above configuration?

2. Intel X299 chipset has 24 PCIe lanes (according to Intel's website), will it be able to support 4 GPUs as each of them would require 16 lanes? ( Will the PLX chip do that?)",41,1,False,self,,,,,
1433,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,14,a0gd1s,self.MachineLearning,Best Angularjs Training Institute |blend info tech,https://www.reddit.com/r/MachineLearning/comments/a0gd1s/best_angularjs_training_institute_blend_info_tech/,Blendtechinfo,1543209180," Blend info tech Training Institute is the Best [\#Angularjs](https://www.facebook.com/hashtag/angularjs?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R) Training Institute in Pune with Real-Time Training and Live Projects.   
  Get 100% Placement. 

 [\#bestpythonclassesinpune](https://www.facebook.com/hashtag/bestpythonclassesinpune?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angularjsinstitute](https://www.facebook.com/hashtag/angularjsinstitute?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#webdevelopmentcourseindia](https://www.facebook.com/hashtag/webdevelopmentcourseindia?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angularjsclassesinaundh](https://www.facebook.com/hashtag/angularjsclassesinaundh?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#perlscriptingclassesinpune](https://www.facebook.com/hashtag/perlscriptingclassesinpune?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angularjsservicespdf](https://www.facebook.com/hashtag/angularjsservicespdf?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#javascriptcoursefeesinpune](https://www.facebook.com/hashtag/javascriptcoursefeesinpune?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angularcertificationinpune](https://www.facebook.com/hashtag/angularcertificationinpune?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angularjscoursefees](https://www.facebook.com/hashtag/angularjscoursefees?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angularjsclassesinpimprichinchwad](https://www.facebook.com/hashtag/angularjsclassesinpimprichinchwad?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angular5courseinpune](https://www.facebook.com/hashtag/angular5courseinpune?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R)  
 [\#angular4certificationpdf](https://www.facebook.com/hashtag/angular4certificationpdf?source=feed_text&amp;__xts__%5B0%5D=68.ARClEzz8wAx2tnNDIJcS_b9F8MmtNdw2Ballhv2RC_p19le1fce75h9WMmCnom_806n7aRbDeNzCdr8ANOneBeY8uCh5oZRvFZp_9zwcuS6Ihqqu0-DrLOvpDzH2G9KZUTUFS7BqXa-Q9qO4Uq3_IFB1I4Sek7XQVMqm10JS-SXXlAZFproVT3Hpd8YB92BYFIGWXRKJCHcawMP_jzn9NnAtW09f5RMOFoR554JlzcBgWnh_0oeDGZws09LaJHZqU9iq7LN5nlCZPjDBdpmRO5-23GYpYnYf9jpEXbfz-e_Lty5n1fXLnvfH8_doruhiNhxvxfXXPLweHMSVHi0rdsslLg&amp;__tn__=%2ANK-R) 

https://i.redd.it/65l78vjlzl021.jpg",0,1,False,https://b.thumbs.redditmedia.com/Q-Pd67ivgo_CHWB89BpCMHlyBaasv8DAmDAEKI0pXAQ.jpg,,,,,
1434,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,15,a0gqs0,self.MachineLearning,Prediction Herobots - The 1st Biannual - International AI Challenge,https://www.reddit.com/r/MachineLearning/comments/a0gqs0/prediction_herobots_the_1st_biannual/,predictionherobots,1543212415,"CosmicBC is holding a biannual international AI challenge to develop blockchain-based AI algorithms for classification, regression and prediction of crypto-currency price. We are encouraging all undergraduate/graduate students with interest in AI and blockchain technology to participate and we believe this competition would be a great opportunity for students to explore their talents and challenge their creative juices as well as gain a chance to develop an AI model that could potentially be of service.

**There is also a grand prize worth $100,000 USD for the team with the winning AI model.**

Registration is accepted until the end of the qualification round, 28 December 2018. For more information, please visit our website ( [www.predictionherobots.com](http://www.predictionherobots.com/) ).

&amp;#x200B;

![img](yquhz3769m021)",0,1,False,self,,,,,
1435,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,16,a0h4qq,self.MachineLearning,Deepmind interview preparation,https://www.reddit.com/r/MachineLearning/comments/a0h4qq/deepmind_interview_preparation/,backprop_69,1543216073,"I got an email from a recruiter at Deepmind around one month later after applying in their career website for a Research engineer position (**referrals are not the only way**, you just need to have the right combination of skills and experience). Scheduled the first round of 30 minute exploratory conversational interview immediately.

Prepare standard interview questions such as:

* Tell me about yourself
* Research Interests
* Educational Background
* Why deepmind
* How did you come know of their work in RL
* Why deepmind (again!)
* Degree of familiarity with Tensorflow/Torch/Pytorch etc
* Why do you think you are suitable for the role
* Learn a bit about any one of their product or papers in DL/RL

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

&amp;#x200B;

The next phase was a 2.5 hour long video interview (quiz) with one Research Engineer and one Research Scientist at Deepmind. The topics covered are Computer Science, Maths, Statistics and Machine Learning, in that order. I scheduled the interview a few weeks later, since I needed some time to brush over my fundamental knowledge in these domains and practice.

&amp;#x200B;

The things you need to know for Computer Science:

* Difference between LinkedList and Arrays
* Memory Allocations
* Binary Trees
* Graph Traversals (BFS, DFS), and shortest path algorithm
* Difference between Threads and Processes
* Pointers and References!
* Different type of Sorting: Comparisons and Integer Sort. Learn one algorithm from each by heart. Also you need to know the complexities: best, average and worst cases.
* Basics of Dynamic Programming
* Sign bit representations, 1's and 2's complement
* Standard topics in OOPs (encapsulation, polymorphism, inheritance, abstraction etc)

The things you need to know in Mathematics:

* Derivatives and Integrals of standard functions
* Matrix algebra (Inverse, pseudo inverse, adjugate, SVD, Cholesky, singular, symmetric, positive definite, eigenvalues and eigenvectors etc)
* Knowledge about Hessian and Jacobian, and their intuitive understanding (not just the expression) and their relation with Fisher Information Matrix

The things you need to know in Statistics:

* Random Variables (discrete and continuous)
* Bernoulli and Binomial distribution
* Mean and Variance (you need to know the derivation of the expressions)
* Knowledge of PDF and CDF
* Covariance Matrix and its properties, difference between Covariance Matrix and Inverse Covariance Matrix
* Bayes Rule
* Concept of MLE and MAP, and their fundamental difference
* Central Limit Theorem
* Concepts of WSS Random Processes
* MCMC Sampling techniques and why they are needed
* Markov Chains, their properties and stationary distribution
* Characteristic Functions, Moment Generating Functions and Probability Generating Functions

The things you need to know in Machine Learning:

* Clustering
   * Learn K-means and GMM by heart, including the algorithm.
* Concept of measuring difference between two distributions, aka KL divergence
* Intuitive understanding of eigenvalues and eigenvectors
* Dimensionality reduction techniques, such as PCA and why it is needed
* Back-propagation
* Autoencoder concepts
* Reparameterization trick
* GANs, and concept of Adversarial training paradigm.
* RNNs and LSTMs
* Learn about different adaptive optimizer algorithms by heart (Adagrad, Adadelta, Adam, RMSprop, Adamax, Nadam)
* Basics of Reinforcement Learning and MDPs
   * Policy and Value Iterations and Q-Learning, what is Q-value etc.",0,1,False,self,,,,,
1436,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,16,a0h4rp,self.MachineLearning,Features of Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/a0h4rp/features_of_pattern_recognition/,nishusindhu,1543216082,[removed],0,1,False,self,,,,,
1437,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,17,a0hgfs,youtube.com,Learn Machine Learning in 12 Minutes (With Code)!,https://www.reddit.com/r/MachineLearning/comments/a0hgfs/learn_machine_learning_in_12_minutes_with_code/,VCubingX,1543219263,,0,1,False,https://b.thumbs.redditmedia.com/RfyFpNPncLjMOOLE9953foa_2NB1DXjurEVP_eNOVgY.jpg,,,,,
1438,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,17,a0hjen,self.MachineLearning,Global Industrial Robotics Services Market Report 2018-2022,https://www.reddit.com/r/MachineLearning/comments/a0hjen/global_industrial_robotics_services_market_report/,bir07,1543220107,[removed],1,1,False,self,,,,,
1439,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,17,a0hnno,venturebeat.com,"Before you launch your machine learning model, start with an MVP",https://www.reddit.com/r/MachineLearning/comments/a0hnno/before_you_launch_your_machine_learning_model/,pp314159,1543221276,,0,1,False,https://b.thumbs.redditmedia.com/cmsJ3ad4q51gS-2vkYFrGs1deuvGyBtDA2omSGRixHw.jpg,,,,,
1440,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,17,a0hpll,medium.com,[P] Arithmetic Properties of Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/a0hpll/p_arithmetic_properties_of_word_embeddings/,HichamEB,1543221856,,0,1,False,default,,,,,
1441,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,17,a0hrk6,self.MachineLearning,Re-implement problems in the filed of non-oriented dialogue system,https://www.reddit.com/r/MachineLearning/comments/a0hrk6/reimplement_problems_in_the_filed_of_nonoriented/,NO-VM,1543222416,[removed],0,1,False,self,,,,,
1442,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,18,a0hth6,self.MachineLearning,ML server orchestration for boosting training speed,https://www.reddit.com/r/MachineLearning/comments/a0hth6/ml_server_orchestration_for_boosting_training/,paulsoh,1543222952,[removed],0,1,False,self,,,,,
1443,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,18,a0i3e1,self.MachineLearning,What do heuristics mean?,https://www.reddit.com/r/MachineLearning/comments/a0i3e1/what_do_heuristics_mean/,natachi,1543225737,[removed],0,1,False,self,,,,,
1444,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,19,a0i6ss,self.MachineLearning,[D] Help with Deep Learning Workstation Software configuration,https://www.reddit.com/r/MachineLearning/comments/a0i6ss/d_help_with_deep_learning_workstation_software/,jomangy,1543226760,"Hi all,

There are many discussions on choosing [best hardware configuration for a deep learning workstation](https://www.reddit.com/r/MachineLearning/comments/a0g59s/d_help_with_deep_learning_workstation/),

I'm searching for a good (or best or optimal... depends on how optimistic I am) **software configuration** for a DL workstation.

That is, choosing between

1. Operation system (Server or normal ?)
2. Nvidia driver version
3. CUDA + CUDNN versions
4. python version
5. Docker Vs python virtual Env
6. other things that are more easy to change later like:
   1. installing development platforms: pytorch, keras, TF, .. other...
   2. and specific version for each 

The HW workstation in mind is one with 4 to 8 TitanXp or 1080Ti GPU, Memory+storage as needed, something ordinary.

&amp;#x200B;

In the past I had a lot of BUGS related only on missmatch between CUDA ver + TF ver + KERAS ver

Also I have no real experience with Docker, but I think it can improve workflow

My thoughs for a configuration is:

1. Ubuntu desktop 16.04 LTS
   1. I would like to work with Server version, so the GPU will not need to work for screen output, but I don't know if it will be easy to manage
   2. maybe possible to work with 18.04 LTS ([here: how to install](https://medium.com/@taylordenouden/installing-tensorflow-gpu-on-ubuntu-18-04-89a142325138))
2. Nvidia Driver: TItanXp / 1080TI: [410.78](https://www.nvidia.com/Download/driverResults.aspx/140135/en-us)
3. CUDA 9 + CUDNN 7 because Tensorflow support it out of the box
   1. what if I want compatibility to older versions, can I use multiple CUDA+CUDNN versions ?
   2. what if In the future I would like to upgrade TF or CUDA+CUDNN, there will be backward compatibility ?
4. python 3.7 - or the newest python3 available currently
5. virtual env
   1. I want to go with docker, but I'm not sure I know how exactly
6. can decide later.. using different virtual environment for each

Thanks ! ",0,1,False,self,,,,,
1445,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,19,a0iba4,developeronrent.strikingly.com,Significance of Artificial Intelligence in Mobile App Development,https://www.reddit.com/r/MachineLearning/comments/a0iba4/significance_of_artificial_intelligence_in_mobile/,sathikalis,1543229413,,0,1,False,default,,,,,
1446,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,20,a0ie7t,self.MachineLearning,"For someone already proficient in [Tensorflow] + [Keras], what makes sense to learn (today, 2018/2019) in order to expand his knowledge base in deep learning? [MXNet] + [Gluon]?",https://www.reddit.com/r/MachineLearning/comments/a0ie7t/for_someone_already_proficient_in_tensorflow/,Fito33Pete,1543230234,[removed],0,1,False,self,,,,,
1447,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,20,a0igqe,self.MachineLearning,[D] Software configuration of a Deep Learning Workstation,https://www.reddit.com/r/MachineLearning/comments/a0igqe/d_software_configuration_of_a_deep_learning/,jomangy,1543230915,"Hi all,

There are many discussions on choosing [best hardware configuration for a deep learning workstation](https://www.reddit.com/r/MachineLearning/comments/a0g59s/d_help_with_deep_learning_workstation/),

I'm searching for a good (or best or optimal... depends on how optimistic I am) **software configuration** for a DL workstation.

That is, choosing between

1. Operation system (Server or normal ?)
2. Nvidia driver version
3. CUDA + CUDNN versions
4. python version
5. Docker Vs python virtual Env
6. other things that are more easy to change later like:
   1. installing development platforms: pytorch, keras, TF, .. other...
   2. and specific version for each 

The HW workstation in mind is one with 4 to 8 TitanXp or 1080Ti GPU, Memory+storage as needed, something ordinary.

&amp;#x200B;

In the past I had a lot of BUGS related only on missmatch between CUDA ver + TF ver + KERAS ver

Also I have no real experience with Docker, but I think it can improve workflow

My thoughs for a configuration is:

1. Ubuntu desktop 16.04 LTS
   1. I would like to work with Server version, so the GPU will not need to work for screen output, but I don't know if it will be easy to manage
   2. maybe possible to work with 18.04 LTS ([here: how to install](https://medium.com/@taylordenouden/installing-tensorflow-gpu-on-ubuntu-18-04-89a142325138))
2. Nvidia Driver: TItanXp / 1080TI: [410.78](https://www.nvidia.com/Download/driverResults.aspx/140135/en-us)
3. CUDA 9 + CUDNN 7 because Tensorflow support it out of the box
   1. what if I want compatibility to older versions, can I use multiple CUDA+CUDNN versions ?
   2. what if In the future I would like to upgrade TF or CUDA+CUDNN, there will be backward compatibility ?
4. python 3.7 - or the newest python3 available currently
5. virtual env
   1. I want to go with docker, but I'm not sure I know how exactly
6. can decide later.. using different virtual environment for each

Thanks ! ",13,1,False,self,,,,,
1448,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,20,a0igr2,youtube.com,"[P] Choice, life and neural networks in Pixling World",https://www.reddit.com/r/MachineLearning/comments/a0igr2/p_choice_life_and_neural_networks_in_pixling_world/,FredrikNoren,1543230919,,0,1,False,default,,,,,
1449,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,20,a0ilwj,self.MachineLearning,Looking for a eGPU,https://www.reddit.com/r/MachineLearning/comments/a0ilwj/looking_for_a_egpu/,theNullCrown,1543232381,"I am looking for an eGPU to accelerate some AI training. I will use OpenCL instead of CUDA and i will be writing custom non conventional algorithms from scratch. Its not exactly a neural network but i will require highly parallelized matrix multiplications. 

Now i have decided on the Razer Core X for 299$ as the eGPU enclosures.

the Part i need help with is on deciding the GPU

the three options i narrowed it down to are 

1 RTX 2070 529 to 599$

2 GTX 1080 579 to 599$ (USED)

3 Vega FE Air 529 to 549$ (USED)

by budget is strictly 900$

now the Vega FE specs seem really good on paper: 4096 cores 16GB memory 2048 bit bus and 13 Tflops FP32 etc but there arent any reliable reviews of it for mathematical tasks and most gaming and mining reviews are on the negative side. 

Between the RTX 2070 and GTX 1080 they have similar Specs with the 1080 having more cores at 2560 vs 2304 in the RTX while the RTX has faster 8GB memory. Gaming benchmarks show the rtx to be faster whereas the 1080 has higher fp32 performance :8.9 TFlops vs 7.6 Tflops. 

and to add to the confusion there is this list : [https://browser.geekbench.com/opencl-benchmarks](https://browser.geekbench.com/opencl-benchmarks)

i always thought that the amd cards were better than the nvidia ones due to better opencl implementation.  At the end of the day im just going to implement my algorithm in OpenCl. I wont be using any engine like TensorFlow and im not interested in proprietary CUDA since it leads to problems during commercialization later. the Vega FE clearly has the better specs and the NVIDIA opencl support sucks. Then how does the RTX 2080 get such a high OPENCL score at 500 dollars brand new? More Memory s always welcome but does HBM2 make a significant difference over gddr?

And to add to my confusion ill be connecting this eGPU to a Macbook Pro 13 nTB 2017 and apple only supports AMD GPUs. If I  get the RTX 2070 will i be able to connect it on MacOS and if i do will i be able to use it for OpenCl programming?

Pls help me decide.

  ",0,1,False,self,,,,,
1450,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,20,a0imjb,self.MachineLearning,"Best methods for reducing model size, without sacrificing performance",https://www.reddit.com/r/MachineLearning/comments/a0imjb/best_methods_for_reducing_model_size_without/,FCOS96,1543232555,[removed],0,1,False,self,,,,,
1451,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,20,a0invm,self.MachineLearning,how to tell if an assertion is common sense knowledge?,https://www.reddit.com/r/MachineLearning/comments/a0invm/how_to_tell_if_an_assertion_is_common_sense/,NotLiterallyPricky,1543232912,"what is the difference between common knowledge(or facts) and common sense knowledge?

can the machine distinguish these two?

(Frequency in a certain corpus may help but there are a lot of common sense that usually won't be expressed in text.)",0,1,False,self,,,,,
1452,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,20,a0iodr,self.MachineLearning,[D] Looking for an eGPU,https://www.reddit.com/r/MachineLearning/comments/a0iodr/d_looking_for_an_egpu/,theNullCrown,1543233045,"I am looking for an eGPU to accelerate some AI training. I will use OpenCL instead of CUDA and i will be writing custom non conventional algorithms from scratch. Its not exactly a neural network but i will require highly parallelized matrix multiplications thus model or kernel specific benchmarks are not relevant to me.

Now i have decided on the Razer Core X for 299$ as the eGPU enclosures.

the Part i need help with is on deciding the GPU

the three options i narrowed it down to are

1 RTX 2070 529 to 599$

2 GTX 1080 579 to 599$ (USED)

3 Vega FE Air 529 to 549$ (USED)

by budget is strictly 900$

now the Vega FE specs seem really good on paper: 4096 cores 16GB memory 2048 bit bus and 13 Tflops FP32 etc but there arent any reliable reviews of it for mathematical tasks and most gaming and mining reviews are on the negative side.

Between the RTX 2070 and GTX 1080 they have similar Specs with the 1080 having more cores at 2560 vs 2304 in the RTX while the RTX has faster 8GB memory. Gaming benchmarks show the rtx to be faster whereas the 1080 has higher fp32 performance :8.9 TFlops vs 7.6 Tflops.

and to add to the confusion there is this list : [https://browser.geekbench.com/opencl-benchmarks](https://browser.geekbench.com/opencl-benchmarks)

i always thought that the amd cards were better than the nvidia ones due to better opencl implementation.  At the end of the day im just going to implement my algorithm in OpenCl. I wont be using any engine like TensorFlow and im not interested in proprietary CUDA since it leads to problems during commercialization later. the Vega FE clearly has the better specs and the NVIDIA opencl support sucks. Then how does the RTX 2080 get such a high OPENCL score at 500 dollars brand new? More Memory s always welcome but does HBM2 make a significant difference over gddr?

And to add to my confusion ill be connecting this eGPU to a Macbook Pro 13 nTB 2017 and apple only supports AMD GPUs. If I  get the RTX 2070 will i be able to connect it on MacOS and if i do will i be able to use it for OpenCl programming?

Pls help me decide.

EDIT: Reposted coz i forgot to put tag last time",17,1,False,self,,,,,
1453,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,21,a0j48f,onmogul.com,How to start with Machine Learning and Become AI Expert?,https://www.reddit.com/r/MachineLearning/comments/a0j48f/how_to_start_with_machine_learning_and_become_ai/,JanBaskTraining,1543237111,,0,1,False,default,,,,,
1454,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,22,a0j6o5,self.MachineLearning,What is the possible application or real scenario of Text 2 Image/Video synthesis?,https://www.reddit.com/r/MachineLearning/comments/a0j6o5/what_is_the_possible_application_or_real_scenario/,EricDZhang,1543237670,"What I wanna do is finding an application scenario of Text 2 Image/Video, collecting the data and playing with them. 

&amp;#x200B;",0,1,False,self,,,,,
1455,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,22,a0jgvk,self.MachineLearning,[R] Searching for a small Network for Semantic Segmentation in Depth Data,https://www.reddit.com/r/MachineLearning/comments/a0jgvk/r_searching_for_a_small_network_for_semantic/,TimDNN,1543240064,"I want to train a Network to recognize the floor(ground) of an gray scale image and additionally depth data.

I will just label the floor as on Class in the gray scale image.

So I don't need instance segmentation.

I would like to use PyTorch and tried the PSP Network  ([https://github.com/meetshah1995/pytorch-semseg](https://github.com/meetshah1995/pytorch-semseg)).

But unfortunately my GPU is too small for the PSPNet (8GB RAM).

But makes a pre-trained model any sense at all?

For  example when I use the VGG16 pre-trained model and add the depth data  as a second Input Layer, does it makes sense to use the pre-trained  weights? Of course in the first Layer the network would have some  additional weights which I would randomly initialize, but because the stereo depth data is way different from a normal image, I don't know if  the pre-trained weights are good at all.

I would like to train a model in 2 ways to proof that assumption.

With pre-trained weights and with random weights.  
But because I have just ruffly 5000 labeled Images I need to use a very small Network to train it at all.  
And  because the task is hopefully easier than labeling 30 different Classes  like the PSPNet does, I think i should be possible to use a smaller  Network.

My final Question:  
Can  someone recommend a small pre-trained Network, written in PyTorch for  that Task for ether normal images or already including depth Data?

Thanks in advance to all =)",1,1,False,self,,,,,
1456,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,23,a0jm84,self.MachineLearning,[P] Reaver: StarCraft II Deep Reinforcement Learning Agent,https://www.reddit.com/r/MachineLearning/comments/a0jm84/p_reaver_starcraft_ii_deep_reinforcement_learning/,Inori,1543241227,"I'm really anxious and happy to finally show what I've been working on for the last half a year: https://github.com/inoryy/reaver-pysc2

*Short description:*

Reaver is a modular DRL framework that provides faster single-machine env parallelization than most open-source solutions; supports common environments like gym, atari, mujoco in addition to SC2; has networks defined as simple Keras models; is easy to configure &amp; share the configs. As a toy example it solves CartPole-v0 in under 10 seconds, running at about 5k samples per second on a laptop with 4 core CPU. You can see Reaver in action online on [Google Colab](https://colab.research.google.com/drive/1DvyCUdymqgjk85FB5DrTtAwTFbI494x7), solving StarCraft II's MoveToBeacon minigame in 30 minutes.

---

*Long description:*

This project is a [grounds up rewrite](https://github.com/inoryy/reaver-pysc2/commit/c8efbd17797a3d85240b3bdd3f24de422029152b) of its predecessor (my bachelor's thesis) and was motivated by some of the painful experiences I've had along the way. Specifically:

**Performance** - majority of RL baselines published are usually tuned for message-based communication between processes (e.g. MPI). This makes sense for companies like DeepMind or OpenAI with their large scale distributed RL setups, but to me it always seemed like a major bottleneck for typical researchers or hobbyists with access to a single computer / HPC node. So instead I went with shared memory route with Reaver and achieved about 3x speed increase over previous project which had message-based parallelization.

**Modularity** - many RL baselines are modular in one way or another, but are often tightly coupled to the models / environments authors use. From own experience I've written myself into a corner by focusing on StarCraft II, which meant that every experiment and debug was a depressingly long process. So with Reaver I've made it possible to swap envs in one line (literally, even going from SC2 to Atari or CartPole). Same goes for models - any Keras model will do as long as it abides by basic API contracts (inputs = agent obs, outputs = logits + value).

**Configurability** - a modern agent often has dozens of various configuration parameters and sharing them seems to be annoying for everyone involved. I've recently stumbled upon [gin-config](https://github.com/google/gin-config) - a very interesting solution to this problem that supports configuring any Python callable function, both as a python-like config file and through command line arguments. I've used it everywhere I could in Reaver and I'm quite happy with the result, being able to share full training pipeline setup configuration with just one file.

**Future-proof** - a common problem in DL is that things change so fast that even year old codebases can become obsolete. I've written Reaver with the upcoming TensorFlow 2.0 API in mind (mostly involved using tf.keras and avoiding tf.contrib), so hopefully it won't suffer this fate for awhile.

---

Note that even though the niche I'm focusing on with this project is DRL in StarCraft II, none of Reaver's functionality is actually tied to it. Reaver currently has full support for generic gym, atari, and mujoco environments. Let me know if you would like me to support something else (I plan to add VizDoom in the near future as that env also interests me personally :) ).",24,1,False,self,,,,,
1457,MachineLearning,t5_2r3gv,2018-11-26,2018,11,26,23,a0jz1v,self.MachineLearning,Democratizing Artificial Intelligence with Automated Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a0jz1v/democratizing_artificial_intelligence_with/,ffa07a,1543243985,"This is new to me - the machine picks the algorithm and generates the model? Is anyone else aware of services that are doing something similar?

&amp;#x200B;

[https://medium.com/microsoftazure/democratize-artificial-intelligence-with-automated-machine-learning-169b348a9509](https://medium.com/microsoftazure/democratize-artificial-intelligence-with-automated-machine-learning-169b348a9509)",0,1,False,self,,,,,
1458,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,0,a0k20b,self.MachineLearning,U-net model for semantic segmentation,https://www.reddit.com/r/MachineLearning/comments/a0k20b/unet_model_for_semantic_segmentation/,martian_rover,1543244592,[removed],0,1,False,self,,,,,
1459,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,0,a0k8eb,self.MachineLearning,Sentiment Analysis For Beginners,https://www.reddit.com/r/MachineLearning/comments/a0k8eb/sentiment_analysis_for_beginners/,brooklyn_programmer,1543245853,[removed],0,1,False,self,,,,,
1460,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,0,a0kdcd,self.MachineLearning,Democratizing Artificial Intelligence with Automated Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a0kdcd/democratizing_artificial_intelligence_with/,burke_holland,1543246795,"This was new to me - the machine picks the algorithm and generates the model? Is anyone else away of services which are doing something similar?

&amp;#x200B;

[https://medium.com/microsoftazure/democratize-artificial-intelligence-with-automated-machine-learning-169b348a9509](https://medium.com/microsoftazure/democratize-artificial-intelligence-with-automated-machine-learning-169b348a9509)",0,1,False,self,,,,,
1461,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,0,a0keqn,medium.com,[P]  Supervisely November release: reimagine image annotation and data science workflows.,https://www.reddit.com/r/MachineLearning/comments/a0keqn/p_supervisely_november_release_reimagine_image/,tdionis,1543247054,,0,1,False,default,,,,,
1462,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,0,a0khym,self.MachineLearning,[D] Automated Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a0khym/d_automated_machine_learning/,burke_holland,1543247632,"This is new to me - the machine picks the algorithm and builds the model? Is anyone aware of other services that are doing something similar?

[https://medium.com/microsoftazure/democratize-artificial-intelligence-with-automated-machine-learning-169b348a9509](https://medium.com/microsoftazure/democratize-artificial-intelligence-with-automated-machine-learning-169b348a9509)",15,1,False,self,,,,,
1463,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,1,a0krlo,self.MachineLearning,[D] What are the methods/platforms you're using to productionize your ml model?,https://www.reddit.com/r/MachineLearning/comments/a0krlo/d_what_are_the_methodsplatforms_youre_using_to/,e_to_the_power_lnx,1543249359,"I guess this question would come around every few months or so as the industry keeps changing quickly. In any case, here are the ones that I know from gleaning the internets.

1) Tensorflow serving using docker
2) using the cloud ml engine on Google cloud
3) aws sagemaker
4) using flask api and Apache and deploying on the cloud

If you've tried these, what's been your experience with them?",4,1,False,self,,,,,
1464,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,1,a0ktcs,self.artificial,Question: What mathematical foundations course would be most beneficial and why?,https://www.reddit.com/r/MachineLearning/comments/a0ktcs/question_what_mathematical_foundations_course/,Mcmatt90,1543249669,,0,1,False,https://a.thumbs.redditmedia.com/UMHe9Y6pbaLYMbv1mI4O-0XKino-kLQnYjY02wGD5C0.jpg,,,,,
1465,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,1,a0ku1s,self.MachineLearning,Amazons own Machine Learning University now available to all developers | Amazon Web Services,https://www.reddit.com/r/MachineLearning/comments/a0ku1s/amazons_own_machine_learning_university_now/,thicket,1543249795,[removed],0,1,False,self,,,,,
1466,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,1,a0l2rd,self.MachineLearning,[P] PyCM v1.5 : Full analysis of confusion matrix,https://www.reddit.com/r/MachineLearning/comments/a0l2rd/p_pycm_v15_full_analysis_of_confusion_matrix/,sepandhaghighi,1543251353,"Full analysis of confusion matrix library in python  (New Release)

&amp;#x200B;

Added :

\-  Relative Classifier Information (RCI) 

\-  Discriminator Power (DP) 

\-  Youden's Index (Y) 

\-  Discriminant Power Interpretation (DPI) 

\-  Positive Likelihood Ratio Interpretation (PLRI) 

&amp;#x200B;

Github repo : [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

Webpage : [http://pycm.shaghighi.ir/](http://pycm.shaghighi.ir/)

Document : [http://pycm.shaghighi.ir/](http://pycm.shaghighi.ir/)[doc/](http://www.shaghighi.ir/pycm/doc/)",4,1,False,self,,,,,
1467,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,1,a0l2we,self.MachineLearning,Machine learning Project,https://www.reddit.com/r/MachineLearning/comments/a0l2we/machine_learning_project/,Navnidhi,1543251378,[removed],0,1,False,self,,,,,
1468,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,1,a0l376,medium.com,Seasons Best offer to Grab Bindomatic-1000,https://www.reddit.com/r/MachineLearning/comments/a0l376/seasons_best_offer_to_grab_bindomatic1000/,bindingoutlet,1543251435,,0,1,False,https://b.thumbs.redditmedia.com/Tk8YNZbwbyFk3feS2AS8Vx42VB1GEgVVD9u8DzdegRo.jpg,,,,,
1469,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,1,a0l3m7,self.MachineLearning,[P] Share Machine Learning Impacts on Businesses,https://www.reddit.com/r/MachineLearning/comments/a0l3m7/p_share_machine_learning_impacts_on_businesses/,ledmmaster,1543251514,"Hey everyone.

&amp;#x200B;

I recently had a discussion with skeptics of machine learning, some claiming even ""it never had any true business impact in a company"".  We, practitioners, know this is not true. So I thought it could be interesting to have a repository of machine learning successes on business.

&amp;#x200B;

This is a first version of a repository, like other repositories we have for course links and materials: [https://github.com/ledmaster/machine-learning-success](https://github.com/ledmaster/machine-learning-success)

&amp;#x200B;

Please feel free to visit, suggest improvements and, of course, share your successes.",0,1,False,self,,,,,
1470,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,2,a0l960,self.MachineLearning,Free New Book by Andrew Ng: Machine Learning Yearning,https://www.reddit.com/r/MachineLearning/comments/a0l960/free_new_book_by_andrew_ng_machine_learning/,andrea_manero,1543252478,https://www.datasciencecentral.com/profiles/blogs/free-book-by-andrew-ng-machine-learning-yearning,0,1,False,self,,,,,
1471,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,2,a0l9vw,self.MachineLearning,Is anyone going to AAAI 2019 and want to share expenses?,https://www.reddit.com/r/MachineLearning/comments/a0l9vw/is_anyone_going_to_aaai_2019_and_want_to_share/,manoja328,1543252588,"I found this site where people can share rooms or just meet and make new friends. Everyone knows how hard is it to be alone on the planet in a new place.

 [https://conferenceshare.co/](https://conferenceshare.co/)",0,1,False,self,,,,,
1472,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,2,a0latw,self.MachineLearning,Need prebuilt sentiment analysis as baseline,https://www.reddit.com/r/MachineLearning/comments/a0latw/need_prebuilt_sentiment_analysis_as_baseline/,magnusderrote,1543252753,"I am doing an semantic analyse from news headlines about whether they would positively or negatively affect the market. Right now, I am aware of NLTK Vader, but something trained on a Financial corpus news would be most helpful.

&amp;#x200B;

Thank you",0,1,False,self,,,,,
1473,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,2,a0lga7,medium.com,[P] FAIR Paper Questions Pre-Trainings Efficacy in CV,https://www.reddit.com/r/MachineLearning/comments/a0lga7/p_fair_paper_questions_pretrainings_efficacy_in_cv/,gwen0927,1543253689,,0,1,False,https://b.thumbs.redditmedia.com/t00bCF2GIFkhUd48aoDmU547JnpgGi339iDv6MRRqAw.jpg,,,,,
1474,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,2,a0lhvf,self.MachineLearning,Has anyone got experience in using XGBoost for survival analysis (cox)?,https://www.reddit.com/r/MachineLearning/comments/a0lhvf/has_anyone_got_experience_in_using_xgboost_for/,SupportOWS,1543253946,As title. I want to learn the ins and outs pitfalls etc. What should i use for hyper parameter tuning? How does it compare to normal Cox regression etc.,0,1,False,self,,,,,
1475,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,3,a0lqds,self.MachineLearning,"Montezumas Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems",https://www.reddit.com/r/MachineLearning/comments/a0lqds/montezumas_revenge_solved_by_goexplore_a_new/,sml0820,1543255338,[http://eng.uber.com/go-explore/](http://eng.uber.com/go-explore/),0,1,False,self,,,,,
1476,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,3,a0m4sv,eng.uber.com,"[R] Montezumas Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems (Sets Records on Pitfall, Too)",https://www.reddit.com/r/MachineLearning/comments/a0m4sv/r_montezumas_revenge_solved_by_goexplore_a_new/,modeless,1543257801,,0,1,False,default,,,,,
1477,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,3,a0m7zm,self.MachineLearning,"I created a model named PogChampNet that takes an Overwatch video as input and can automatically find the ""highlight worthy"" moments with just the video. It was trained with the help of Twitch chat and its memes!",https://www.reddit.com/r/MachineLearning/comments/a0m7zm/i_created_a_model_named_pogchampnet_that_takes_an/,Farzaa,1543258348,[removed],0,1,False,self,,,,,
1478,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,4,a0mhti,eng.uber.com,"Montezuma's Revenge Solved by Go-Explore, a [N]ew Algorithm for Hard-Exploration Problems (Sets Records on Pitfall, Too)",https://www.reddit.com/r/MachineLearning/comments/a0mhti/montezumas_revenge_solved_by_goexplore_a_new/,thebackpropaganda,1543260020,,0,1,False,https://b.thumbs.redditmedia.com/CkyNVOgTNsMhPby8qLjcg4pPnZzMh5e-e33l4lZj8wA.jpg,,,,,
1479,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,4,a0mneo,eng.uber.com,"[R] Montezuma's Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems (Sets Records on Pitfall, Too)",https://www.reddit.com/r/MachineLearning/comments/a0mneo/r_montezumas_revenge_solved_by_goexplore_a_new/,thebackpropaganda,1543260988,,0,1,False,default,,,,,
1480,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,4,a0mp8l,self.MachineLearning,[D]RL Policy Network with Action Intensity?,https://www.reddit.com/r/MachineLearning/comments/a0mp8l/drl_policy_network_with_action_intensity/,ew20030822,1543261289,"I am designing a policy neural network for balancing a ball on a platform, using a multiclass softmax activation output layer followed by multinomial sampling. This model works well when the motors under control only have binary on/off state.
Now we are given servomotors that take in degree of rotation as command, how should we modify our network's activation/output layers?",4,1,False,self,,,,,
1481,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,4,a0mr98,self.MachineLearning,[R] Survey containing questions about LSTM network making music.,https://www.reddit.com/r/MachineLearning/comments/a0mr98/r_survey_containing_questions_about_lstm_network/,LijuanLarse,1543261636,"The last two questions use an LSTM model that has been trained on midi files from a specific composer. If you've heard of BachBot, this is somewhat similar. I would be very grateful if you could answer any of the questions in the survey. We will be using the answers to the relevant questions to improve the algorithm. Thank you in advance! 

[Here's the link](https://www.surveymonkey.co.uk/r/VWH67QW)

If this type of stuff isn't allowed here feel free to remove it and I do apologise.",3,1,False,self,,,,,
1482,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0mvt5,self.MachineLearning,Off the shelf cam product for implementing custom-demographic facial classification,https://www.reddit.com/r/MachineLearning/comments/a0mvt5/off_the_shelf_cam_product_for_implementing/,Crisreddit1,1543262407,"I am looking for IP cam product which I can use to get faces in streaming video. 
On top of it, I would like to have those images run through a model which categorizes the images by age groups and gender. I guess, this requires the Cam to support programming capabilities.
If theres a camera which easily supports perhaps building and running such custom classification, even better.",0,1,False,self,,,,,
1483,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0mwm1,self.MachineLearning,Building my First PC - SPECS!,https://www.reddit.com/r/MachineLearning/comments/a0mwm1/building_my_first_pc_specs/,KempQ,1543262531,[removed],0,1,False,self,,,,,
1484,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0n09d,newegg.com,Would this PC be good for Deep Learning tasks?,https://www.reddit.com/r/MachineLearning/comments/a0n09d/would_this_pc_be_good_for_deep_learning_tasks/,KempQ,1543263132,,0,1,False,default,,,,,
1485,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0n0qr,weirdgeek.com,Building simple Linear Regression model using Python's Sci-kit library,https://www.reddit.com/r/MachineLearning/comments/a0n0qr/building_simple_linear_regression_model_using/,WeirdGeekDotCom,1543263221,,0,1,False,https://b.thumbs.redditmedia.com/WtkiyOfd2hQEsUB7Y_KwR7jMougrSQkQKQQCIEmjA_I.jpg,,,,,
1486,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0n966,self.MachineLearning,Creating a self learning AI for turn based games,https://www.reddit.com/r/MachineLearning/comments/a0n966/creating_a_self_learning_ai_for_turn_based_games/,Envenger,1543264658,[removed],0,1,False,self,,,,,
1487,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0nbfj,eng.uber.com,Montezuma's revenge solved by Go-Explore(New RL algorithm). 2 orders of magnitude high score improvement!,https://www.reddit.com/r/MachineLearning/comments/a0nbfj/montezumas_revenge_solved_by_goexplorenew_rl/,_Mookee_,1543265040,,1,1,False,default,,,,,
1488,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0nc5v,self.MachineLearning,[D] How can I visibly alter an image while maintaining the same classification result?,https://www.reddit.com/r/MachineLearning/comments/a0nc5v/d_how_can_i_visibly_alter_an_image_while/,BacSai,1543265162,,4,1,False,self,,,,,
1489,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0ngav,eng.uber.com,"[R] Uber's ""Go explore"" Beats Montezuma's Revenge with Score over 2M",https://www.reddit.com/r/MachineLearning/comments/a0ngav/r_ubers_go_explore_beats_montezumas_revenge_with/,darkconfidantislife,1543265879,,4,1,False,default,,,,,
1490,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,5,a0ngq2,self.MachineLearning,"Nice cross_val_scores, large train and test errors?",https://www.reddit.com/r/MachineLearning/comments/a0ngq2/nice_cross_val_scores_large_train_and_test_errors/,mavavilj,1543265951,"Nice cross\_val\_scores, large train and test errors? Is it contradictory?

&amp;#x200B;

I built models by trying to lower the cross\_val\_scores (10-fold) and noticing that the 10 folds had consistent performance.

&amp;#x200B;

After fitting the final model to the entire train sets and predicting on whole X\_test and then for X\_train and calculating correspondingly test error and train error. I'm seeing large error.

&amp;#x200B;

Have I done something wrong? Which metric to trust?",0,1,False,self,,,,,
1491,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,6,a0nige,aws.amazon.com,Amazons own Machine Learning University now available to all developers,https://www.reddit.com/r/MachineLearning/comments/a0nige/amazons_own_machine_learning_university_now/,MainaWycliffe,1543266242,,0,1,False,https://b.thumbs.redditmedia.com/caia3F2OoPYbM6kkLYYdiBPxAEmDiH-VsBYvwS95ySA.jpg,,,,,
1492,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,6,a0nil0,microsoft.com,[Book] Bishop's PRML is free now,https://www.reddit.com/r/MachineLearning/comments/a0nil0/book_bishops_prml_is_free_now/,_pragmatic_machine,1543266263,,0,1,False,default,,,,,
1493,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,6,a0nnp7,self.MachineLearning,"[R] Montezumas Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems (Sets Records on Pitfall, Too)",https://www.reddit.com/r/MachineLearning/comments/a0nnp7/r_montezumas_revenge_solved_by_goexplore_a_new/,modeless,1543267125,"Video: https://www.youtube.com/watch?v=L_E3w_gHBOY

Blog: https://eng.uber.com/go-explore/

Paper and source code are promised soon but not released yet.",103,1,False,self,,,,,
1494,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,6,a0nqp8,code.fb.com,New fastMRI open source AI research tools from Facebook and NYU School of Medicine,https://www.reddit.com/r/MachineLearning/comments/a0nqp8/new_fastmri_open_source_ai_research_tools_from/,deeplearningmaniac,1543267620,,0,1,False,default,,,,,
1495,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,7,a0o2f4,self.MachineLearning,UNet: Should the skip connection be taken before or after RELU?,https://www.reddit.com/r/MachineLearning/comments/a0o2f4/unet_should_the_skip_connection_be_taken_before/,soulslicer0,1543269610,[removed],0,1,False,self,,,,,
1496,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,7,a0o390,self.MachineLearning,What math is necessary to know before learning Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/a0o390/what_math_is_necessary_to_know_before_learning/,snowleopardkitten34,1543269748,,0,1,False,self,,,,,
1497,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,7,a0o6dm,self.MachineLearning,[R] Uber gets score of ~35K on Montezuma's Revenge and more than 2 Million(!) with domain knowledge,https://www.reddit.com/r/MachineLearning/comments/a0o6dm/r_uber_gets_score_of_35k_on_montezumas_revenge/,darkconfidantislife,1543270293,[https://eng.uber.com/go-explore/](https://eng.uber.com/go-explore/),0,1,False,self,,,,,
1498,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,7,a0oaom,aws.amazon.com,[N] Amazons own Machine Learning University now available to all developers,https://www.reddit.com/r/MachineLearning/comments/a0oaom/n_amazons_own_machine_learning_university_now/,cryptoz,1543271048,,0,1,False,default,,,,,
1499,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,8,a0oqqd,self.MachineLearning,[Discussion] Word lists for unethical bias tests (e.g. WEAT),https://www.reddit.com/r/MachineLearning/comments/a0oqqd/discussion_word_lists_for_unethical_bias_tests_eg/,zmjjmz,1543273981,"I'm looking to evaluate (unethical, social) biases of a text classifier I'm building, and one of the big issues w/doing so far has been finding a good comprehensive source of words on which to evaluate bias.

I noticed in the Universal Sentence Encoder [paper](http://arxiv.org/abs/1803.11175) that they put together words from some IAT papers, but some of the references are just pictures of text which I want to avoid dealing with 

Additionally these are mostly just based on names, and don't include as much terms that vary based on subject/object ethnicity/gender.

Does anyone have a good comprehensive set of words that they use for these? That they'd be willing to share ? ",1,1,False,self,,,,,
1500,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,8,a0p06h,self.MachineLearning,Relativistic Stochastic Gradient Descend,https://www.reddit.com/r/MachineLearning/comments/a0p06h/relativistic_stochastic_gradient_descend/,bbsome,1543275789,[removed],0,1,False,self,,,,,
1501,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,8,a0p418,self.MachineLearning,[D] Any Guide to First-time Conference visitors? (NeurIPS),https://www.reddit.com/r/MachineLearning/comments/a0p418/d_any_guide_to_firsttime_conference_visitors/,tshrjn,1543276559,"Any hints or tips from your experience for a first-time attendee are welcome to make the most out of an ML conference?

",6,1,False,self,,,,,
1502,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,9,a0p8sc,github.com,tensorflow/addons is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/a0p8sc/tensorflowaddons_is_a_new_github_repo_by/,sjoerdapp,1543277479,,0,1,False,default,,,,,
1503,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,9,a0p9wg,arxiv.org,[R] Do GAN Loss Functions Really Matter?,https://www.reddit.com/r/MachineLearning/comments/a0p9wg/r_do_gan_loss_functions_really_matter/,HigherTopoi,1543277682,,7,1,False,default,,,,,
1504,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,9,a0pfaf,self.MachineLearning,[D] Relativistic Stochastic Gradient Descend - has anyone tried it?,https://www.reddit.com/r/MachineLearning/comments/a0pfaf/d_relativistic_stochastic_gradient_descend_has/,bbsome,1543278722,"The paper Relativistic Monte Carlo([https://arxiv.org/abs/1609.04388v1](https://arxiv.org/abs/1609.04388v1)) contains a limit of the Stochastic HMC which reduces to a deterministic ODE that converges to local minima of the objective. The authors call this deterministic version Relativistic Stochastic Gradient Descend. On small problems, they seem to show it works better than Adam.

Has anyone ever tried this on their own or on something more than a toy problem?",0,1,False,self,,,,,
1505,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,10,a0pujw,self.MachineLearning,Is this a good Machine Learning PC build component list?,https://www.reddit.com/r/MachineLearning/comments/a0pujw/is_this_a_good_machine_learning_pc_build/,annieeby,1543281745,[removed],0,1,False,self,,,,,
1506,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,12,a0qub6,self.MachineLearning,[D] What is a typical workflow for retraining models in production?,https://www.reddit.com/r/MachineLearning/comments/a0qub6/d_what_is_a_typical_workflow_for_retraining/,ParticularMine,1543288933,"For those of you that deploy trained models in production, how do you decide when to retrain on new data? Do you fine-tune the existing model or start from random initialization?",22,1,False,self,,,,,
1507,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,13,a0rgoh,self.MachineLearning,[D] SysML reviews are out,https://www.reddit.com/r/MachineLearning/comments/a0rgoh/d_sysml_reviews_are_out/,kg_bounce,1543293663,"From the program chairs: ""We have tried to ensure that each paper has at least three reviews, though a small number of papers may have fewer. If you have fewer than three reviews, please check back daily, as we will be doing our utmost to have the third review posted before the feedback deadline expires.""",4,1,False,self,,,,,
1508,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,13,a0rk9f,gengo.ai,20 YouTube channels for AI &amp; machine learning,https://www.reddit.com/r/MachineLearning/comments/a0rk9f/20_youtube_channels_for_ai_machine_learning/,reimmoriks,1543294453,,0,1,False,default,,,,,
1509,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,14,a0s11y,self.MachineLearning,[P] Hex - Creating Intelligent Opponents with Minimax Driven AI (Part 1: - Pruning),https://www.reddit.com/r/MachineLearning/comments/a0s11y/p_hex_creating_intelligent_opponents_with_minimax/,g_surma,1543298232,"&amp;#x200B;

https://i.redd.it/rtohujy4ct021.png

[medium](https://towardsdatascience.com/hex-creating-intelligent-opponents-with-minimax-driven-ai-part-1---pruning-cc1df850e5bd)

[appstore](https://itunes.apple.com/us/app/hex-ai-board-game/id1326484730?ls=1&amp;mt=8)

In todays article, I am going to show you how to create intelligent opponents with **Alpha-Beta Minimax** algorithm. We will create an agent that can successfully compete with humans in the classic **Hex** game. After the end of this article, you will be able to create **adversarial search** agents that can competitively play **zero-sum** and **perfect information** games.",6,1,False,https://b.thumbs.redditmedia.com/RM3JSCPaH4weYCho2pjgRbQehI1K7IvxM4xnO_T1XXM.jpg,,,,,
1510,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,15,a0sfxb,arxiv.org,A Bittorrent inspired training framework,https://www.reddit.com/r/MachineLearning/comments/a0sfxb/a_bittorrent_inspired_training_framework/,karanchahal1996,1543301923,,5,1,False,default,,,,,
1511,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,16,a0spmt,business-standard.com,"Google AI plays safe, blocks gender-based pronouns like 'him' and 'her'",https://www.reddit.com/r/MachineLearning/comments/a0spmt/google_ai_plays_safe_blocks_genderbased_pronouns/,Awaaz-e-India,1543304530,,0,1,False,https://b.thumbs.redditmedia.com/Z3O-dcWlrUpodrEfm7X2JtfLg_f0hN2z8eCiCdZUSbU.jpg,,,,,
1512,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,17,a0sz0l,youtu.be,Temporally Coherent GANs for Video Super-Resolution (TecoGAN),https://www.reddit.com/r/MachineLearning/comments/a0sz0l/temporally_coherent_gans_for_video/,zergling103,1543307164,,1,1,False,default,,,,,
1513,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,17,a0t01i,self.MachineLearning,[D] What's the best machine learning you have seen or discovered that is already developed and what does it do?,https://www.reddit.com/r/MachineLearning/comments/a0t01i/d_whats_the_best_machine_learning_you_have_seen/,iknothing,1543307473,"I am a little bit familiar with machine learning however seeing some online I feel that they are very similar, for example machine learning that is being used for customer service chat. So this post is to get an idea, and be inspired by some really cool machine learning programs you have seen.

I simply want to get an idea what is out there, since I am new to ML and would want to know what I want to create for.

one thing I found really cool was transliteration. when you type it can understand what you are trying to say.",2,1,False,self,,,,,
1514,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,17,a0t13t,self.MachineLearning,Looking for a Statistics Guide/Mentor.,https://www.reddit.com/r/MachineLearning/comments/a0t13t/looking_for_a_statistics_guidementor/,ieltsp,1543307795,[removed],0,1,False,self,,,,,
1515,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,17,a0t487,eng.uber.com,"Montezuma's Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems (Sets Records on Pitfall, Too)",https://www.reddit.com/r/MachineLearning/comments/a0t487/montezumas_revenge_solved_by_goexplore_a_new/,mlvpj,1543308732,,0,1,False,https://b.thumbs.redditmedia.com/CkyNVOgTNsMhPby8qLjcg4pPnZzMh5e-e33l4lZj8wA.jpg,,,,,
1516,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,18,a0t88d,self.MachineLearning,Pattern Recognition &amp; Machine Learning book in now free to download,https://www.reddit.com/r/MachineLearning/comments/a0t88d/pattern_recognition_machine_learning_book_in_now/,aziz_22,1543309881,[removed],0,1,False,self,,,,,
1517,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,18,a0t9q7,self.MachineLearning,Charpak Research Internship Programme,https://www.reddit.com/r/MachineLearning/comments/a0t9q7/charpak_research_internship_programme/,sinashish,1543310311,[removed],0,1,False,self,,,,,
1518,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,19,a0tnp1,self.MachineLearning,Is an open registry of data-transformation functions will be useful for ML algorithms?,https://www.reddit.com/r/MachineLearning/comments/a0tnp1/is_an_open_registry_of_datatransformation/,moshestv,1543314360,[removed],0,1,False,self,,,,,
1519,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,20,a0tvrz,youtube.com,Regression Assumptions,https://www.reddit.com/r/MachineLearning/comments/a0tvrz/regression_assumptions/,Seesam-,1543316727,,0,1,False,https://a.thumbs.redditmedia.com/BZTl4V6Ng-s2nXzKmiNYHfzsqxHs3KVxdedp5xkPYf0.jpg,,,,,
1520,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,20,a0twzx,self.algorithms,Question about MCTS example from Wikipedia,https://www.reddit.com/r/MachineLearning/comments/a0twzx/question_about_mcts_example_from_wikipedia/,Algabera,1543317063,,0,1,False,https://b.thumbs.redditmedia.com/3wVa-cq5QEd4zZfYliMU0HL6mItVdtBAzMDAssl-qTA.jpg,,,,,
1521,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,20,a0u0sk,facebook.com,"A Facebook group meant to provide a place for people leading data science teams and groups, from anywhere in the world, to discuss common challenges together.",https://www.reddit.com/r/MachineLearning/comments/a0u0sk/a_facebook_group_meant_to_provide_a_place_for/,shaypal5,1543318167,,0,1,False,default,,,,,
1522,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,21,a0u9sm,self.MachineLearning,Access RNN LSTM Memory Cell Vector/matrix,https://www.reddit.com/r/MachineLearning/comments/a0u9sm/access_rnn_lstm_memory_cell_vectormatrix/,jsai23,1543320574,[removed],0,1,False,self,,,,,
1523,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,22,a0um9m,self.MachineLearning,The dataset for image recognization,https://www.reddit.com/r/MachineLearning/comments/a0um9m/the_dataset_for_image_recognization/,tell_me_smthng_new,1543323602,[removed],0,1,False,self,,,,,
1524,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,22,a0upbo,self.MachineLearning,Tensorrt plugin issue.,https://www.reddit.com/r/MachineLearning/comments/a0upbo/tensorrt_plugin_issue/,danield95,1543324232,[removed],0,1,False,self,,,,,
1525,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,23,a0v3tf,self.MachineLearning,[P] Next-frame Prediction Video Generation based on pix2pixHD (tutorial and code!),https://www.reddit.com/r/MachineLearning/comments/a0v3tf/p_nextframe_prediction_video_generation_based_on/,jctestud,1543327359,"Code: [https://github.com/jctestud/pix2pixHD/tree/video](https://github.com/jctestud/pix2pixHD/tree/video)

Blog post / tutorial: [https://medium.com/@jctestud/video-generation-with-pix2pix-aed5b1b69f57](https://medium.com/@jctestud/video-generation-with-pix2pix-aed5b1b69f57)

720p result example: [https://www.youtube.com/watch?v=ljGu5bW5Zj8](https://www.youtube.com/watch?v=ljGu5bW5Zj8)

&amp;#x200B;

Quick summary:

Pix2pix (with different twists) has been used by several people for video generation. 

My obvious inspiration for this project was Mario Klingemann's [fake fireworks](https://www.fastcompany.com/90156087/an-ai-learned-to-make-fireworks-and-theyre-mesmerizing).

The core idea is to select a video, and train a pix2pix model on a next-frame prediction task.

The model is then used in a feedback loop to produce as many frames of new videos as you want.

&amp;#x200B;

Forked from Nvidia's pix2pixHD, I added some video manipulation and training modules.

It is a basic, but ready-to-use framework to experiment on your own.

Would love to hear your thoughts, and to make it better!

&amp;#x200B;

Related work (with code): [https://magenta.tensorflow.org/nfp\_p2p](https://magenta.tensorflow.org/nfp_p2p) (using a tensorflow port of vanilla pix2pix)",22,1,False,self,,,,,
1526,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,23,a0v3wm,self.MachineLearning,"[D] I have started studying machine learning and will be purchasing a laptop for college . For small projects , will mx150(gpu) give any significant advantage over inbuilt gpu that comes with i5 8th gen processor ?",https://www.reddit.com/r/MachineLearning/comments/a0v3wm/d_i_have_started_studying_machine_learning_and/,udkncrood,1543327370,I dont really have money for building a pc or purchasing a costly laptop and i dont play games at all . is mx150 worth the extra money ?,19,1,False,self,,,,,
1527,MachineLearning,t5_2r3gv,2018-11-27,2018,11,27,23,a0vgt5,self.MachineLearning,OpenAI fellows and scholars programs interview process,https://www.reddit.com/r/MachineLearning/comments/a0vgt5/openai_fellows_and_scholars_programs_interview/,mlnewcomer1,1543329969,"Hi,

does anybody know the typical interview process about these two programs from openai?

how selective is it and what is a competitive background?

 I would appreciate your help!",0,1,False,self,,,,,
1528,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,0,a0vv4i,self.MachineLearning,[P] Chapter 7: Problems and Solutions,https://www.reddit.com/r/MachineLearning/comments/a0vv4i/p_chapter_7_problems_and_solutions/,RudyWurlitzer,1543332672,"The draft of the seventh chapter, ""Problems and Solutions"", of my upcoming The Hundred-Page Machine Learning book is online. The chapter covers: kernel regression, multiclass classification, one-class classification, multi-label classification, ensemble learning, learning to annotate sequences, sequence-to-sequence learning, active learning, one-shot learning, a and zero-shot learning.

The book is available at [http://themlbook.com](http://themlbook.com/). You can subscribe on the website to receive updates on new chapters by email.

I count on you to help me to improve this chapter. The success criterion is clarity for an uninitiated reader.",0,1,False,self,,,,,
1529,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,0,a0vzuz,self.MachineLearning,[P] 2048 - Solving 2048 with Monte-Carlo Tree Search (MCTS) AI ,https://www.reddit.com/r/MachineLearning/comments/a0vzuz/p_2048_solving_2048_with_montecarlo_tree_search/,g_surma,1543333555,"&amp;#x200B;

https://i.redd.it/wqrishe79w021.png

[medium](https://towardsdatascience.com/2048-solving-2048-with-monte-carlo-tree-search-ai-2dbe76894bab)

[appstore](https://itunes.apple.com/us/app/2048-ai-solver/id1315421448?ls=1&amp;mt=8)

In todays article, I am going to show you how to solve the famous **2048** game with **Artificial Intelligence**. You will learn the essentials behind the **Monte-Carlo Tree Search** algorithm and at the end of this article, you will be able to create an agent that without any domain-specific knowledge beats average human scores in 2048.",6,1,False,https://a.thumbs.redditmedia.com/1p11uGk6GzEPY18_BuMIDTMIT4vdFAvOwRU3uhW-_F8.jpg,,,,,
1530,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,1,a0w4wm,self.MachineLearning,[D] What is the process for topic modeling with a pre-defined set of topics?,https://www.reddit.com/r/MachineLearning/comments/a0w4wm/d_what_is_the_process_for_topic_modeling_with_a/,Nick_Pyth,1543334477,"I am proficient in Latent Dirichlet Allocation ([wiki](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)), which is a method of unsupervised learning that is used to ""discover"" `k` topics that are a given dataset of text. In this process, a user merely defines the number of topics contained in the dataset (along with additional hyperparameters).

&amp;#x200B;

Let's say for example that I scraped a news website that did not label or tag their articles in any way. I would expect there to be topics relating to politics, finance, weather, and science. How would I train a model to find these **specific** topics, as opposed to using LDA and selecting `k=4`

&amp;#x200B;

Is this a multi-label classification problem? Does this always require a fairly large set of labelled training data? Are there any methods to ""infer"" the topics without a lengthy labelling process? I'm interested in reading research articles that explore these methods, or perhaps the name of the technique I should be searching for.",6,1,False,self,,,,,
1531,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,1,a0w8b7,medium.com,[N] FAIR &amp; NYU School of Medicine Share fastMRI Tools; Release Largest-Ever MRI Dataset,https://www.reddit.com/r/MachineLearning/comments/a0w8b7/n_fair_nyu_school_of_medicine_share_fastmri_tools/,gwen0927,1543335086,,0,1,False,https://b.thumbs.redditmedia.com/lFSJphSKdU3L1JgdrCisC1RynyhWnL-lZhdn8FTZ_Ks.jpg,,,,,
1532,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,1,a0wcdh,self.MachineLearning,Machine vs Cardiologist,https://www.reddit.com/r/MachineLearning/comments/a0wcdh/machine_vs_cardiologist/,aulloa,1543335809,[removed],0,1,False,self,,,,,
1533,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,1,a0wce3,self.MachineLearning,What to do when data makes all predictions the same?,https://www.reddit.com/r/MachineLearning/comments/a0wce3/what_to_do_when_data_makes_all_predictions_the/,MindWatcher,1543335812,"In a classification problem. When the majority of observations belongs to one class, and based on this training data, a model is created. Then in the test data, when the model is applied, all predictions are the same, and the error term depends only on the ratio between the class sizes in the training data?

Any methods that handles this issue? ",0,1,False,self,,,,,
1534,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,1,a0wl5b,self.MachineLearning,AI cheating,https://www.reddit.com/r/MachineLearning/comments/a0wl5b/ai_cheating/,EnlightenMePlss,1543337294,[removed],0,1,False,self,,,,,
1535,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,2,a0wrpr,medium.com,FAIR &amp; NYU School of Medicine Share fastMRI Tools; Release Largest-Ever MRI Dataset,https://www.reddit.com/r/MachineLearning/comments/a0wrpr/fair_nyu_school_of_medicine_share_fastmri_tools/,trcytony,1543338446,,0,1,False,https://b.thumbs.redditmedia.com/lFSJphSKdU3L1JgdrCisC1RynyhWnL-lZhdn8FTZ_Ks.jpg,,,,,
1536,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,2,a0wuww,self.MachineLearning,https://www.independent.co.uk/life-style/gadgets-and-tech/news/saudi-arabia-robot-sophia-citizenship-android-riyadh-citizen-passport-future-a8021601.html,https://www.reddit.com/r/MachineLearning/comments/a0wuww/httpswwwindependentcouklifestylegadgetsandtechnews/,rogueKlyntar,1543338977,[removed],0,1,False,self,,,,,
1537,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,2,a0wwo9,self.MachineLearning,Collecting cancer research and machine learning related resources/research,https://www.reddit.com/r/MachineLearning/comments/a0wwo9/collecting_cancer_research_and_machine_learning/,fromdev,1543339285,"Inspired by [this work](https://www.reddit.com/r/MachineLearning/comments/8rdpwy/pi_made_a_gpu_cluster_and_free_website_to_help/) I am trying to collect all relevant resources (most from this sub) related to cancer research and machine learning: I am adding them on this sheet. 

[https://docs.google.com/spreadsheets/d/1zLn8t9FxbsLQUNK-6GITqTMTrpwKAuKNXuV2yokLCyc/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1zLn8t9FxbsLQUNK-6GITqTMTrpwKAuKNXuV2yokLCyc/edit?usp=sharing)

&amp;#x200B;

It is public google sheet with comments access to everyone. 

Anyone interested to contribute pls comment or let me know and I will provide you write access. 

&amp;#x200B;

Once this sheet has become a significant source I will put it on GitHub and my blog.  Hope it will help others quickstart. 

Feel free to suggest a better way to do this. ",0,1,False,self,,,,,
1538,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,2,a0x3fr,reddit.com,Programas Qumicos para ETE e ETA IV,https://www.reddit.com/r/MachineLearning/comments/a0x3fr/programas_qumicos_para_ete_e_eta_iv/,JamurGerloff,1543340482,,0,1,False,default,,,,,
1539,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,2,a0x738,self.MachineLearning,Winter and Spring 2019 Fellowships,https://www.reddit.com/r/MachineLearning/comments/a0x738/winter_and_spring_2019_fellowships/,inika,1543341123,"Hi all,

I'm a current CS master's student who is due to graduate in a few weeks. I've recently signed an offer for a PM position that starts in late August: this gives me quite a lot of free time between now and then. I wanted to pursue a data science or machine learning fellowship like the one's below:

[Data Science for Social Good Chicago](https://dssg.uchicago.edu/)

[Data Science for Social Good Georgia Tech](https://ptc.gatech.edu/dssg)

[DSSG UW](https://escience.washington.edu/get-involved/incubator-programs/data-science-for-social-good/)

[OpenAI Winter Scholar](https://blog.openai.com/openai-scholars-2019/)

I was wondering if the community know of any other opportunities in the states or abroad? I've taken all the necessary DS &amp; ML classes my school offers",0,1,False,self,,,,,
1540,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,3,a0x9z6,self.MachineLearning,[D] Proper train to test ratio?,https://www.reddit.com/r/MachineLearning/comments/a0x9z6/d_proper_train_to_test_ratio/,CSS_Programmer,1543341629,"I am building a sentiment analysis algorithm and I am wondering what are the best ratios for a given set of data to be split into training and testing data? 1/2? 1/10? Also, how much should the mixture of positives to negatives be within the test data? Right now I have a huge list of positives, and I am wondering how many negatives I should find and insert into a training set to maximize learning performance?",5,1,False,self,,,,,
1541,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,3,a0xfc2,self.MachineLearning,[P] Illustrated Deep Learning cheatsheets covering Stanford's CS 230 class,https://www.reddit.com/r/MachineLearning/comments/a0xfc2/p_illustrated_deep_learning_cheatsheets_covering/,shervinea,1543342546,"Set of illustrated Deep Learning cheatsheets covering the content of Stanford's CS 230 class:

* Convolutional Neural Networks: [https://stanford.edu/\~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)
* Recurrent Neural Networks: [https://stanford.edu/\~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
* Tips and tricks: [https://stanford.edu/\~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)

[Web version](https://i.redd.it/1qve59a40x021.png)

&amp;#x200B;

All the above in PDF format: [https://github.com/afshinea/stanford-cs-230-deep-learning](https://github.com/afshinea/stanford-cs-230-deep-learning)

[PDF version](https://i.redd.it/636lrf1vyw021.png)",26,1,False,https://b.thumbs.redditmedia.com/kuze738n-LDymC1amKY0eldAyF8ermV_ebVFSLl0VQM.jpg,,,,,
1542,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,3,a0xisa,muchtech.org,How To Enroll Amazon Free Machine Learning Course,https://www.reddit.com/r/MachineLearning/comments/a0xisa/how_to_enroll_amazon_free_machine_learning_course/,sameerbille,1543343192,,0,1,False,default,,,,,
1543,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,3,a0xivb,self.MachineLearning,[R] DeepMind: Hierarchical visuomotor control of humanoids,https://www.reddit.com/r/MachineLearning/comments/a0xivb/r_deepmind_hierarchical_visuomotor_control_of/,modeless,1543343210,"Video: https://youtu.be/7GISvfbykLE

Paper: https://arxiv.org/abs/1811.09656

Abstract:

We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment.",2,1,False,self,,,,,
1544,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,3,a0xldb,microsoft.com,Bishop's Pattern Recognition and Machine Learning available for free from Microsoft,https://www.reddit.com/r/MachineLearning/comments/a0xldb/bishops_pattern_recognition_and_machine_learning/,botja,1543343569,,0,1,False,default,,,,,
1545,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,4,a0xvpl,reddit.com,"[P] Operationalizing Machine Learning with ML.NET, Azure DevOps and Azure Container Instances",https://www.reddit.com/r/MachineLearning/comments/a0xvpl/p_operationalizing_machine_learning_with_mlnet/,darkjeepers,1543345374,,0,1,False,default,,,,,
1546,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,4,a0y61v,self.MachineLearning,[R] Deep Reinforcement Learning of Marked Temporal Point Processes (NIPS 2018),https://www.reddit.com/r/MachineLearning/comments/a0y61v/r_deep_reinforcement_learning_of_marked_temporal/,musically_ut,1543347250,"&amp;#x200B;

*Processing img pgz8pvspdx021...*

&amp;#x200B;

arXiv link: [https://arxiv.org/abs/1805.09360](https://arxiv.org/abs/1805.09360)

3-minute video: [https://www.youtube.com/watch?v=JKSpbL0y5LA](https://www.youtube.com/watch?v=JKSpbL0y5LA)

Source Code: [https://github.com/Networks-Learning/tpprl](https://github.com/Networks-Learning/tpprl)

&amp;#x200B;

Abstract: 

In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent's actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.",4,1,False,self,,,,,
1547,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,4,a0y7ds,self.MachineLearning,Asking professors for research positions,https://www.reddit.com/r/MachineLearning/comments/a0y7ds/asking_professors_for_research_positions/,stormyjan2601,1543347473,I am looking for a research position with a professor preferably in RL. But I am tired of mailing them and expecting no response or an automated one referring to the graduate admissions website. What might be the other ways to approach professors and receive replies swiftly?,0,1,False,self,,,,,
1548,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,6,a0z5ev,self.MachineLearning,"[D] Doubts on theory of Perturbative Neural Networks (PNN), CVPR 18",https://www.reddit.com/r/MachineLearning/comments/a0z5ev/d_doubts_on_theory_of_perturbative_neural/,kmkolasinski,1543353457,"Hi,

Recently, Perturbative Neural Networks (PNN) paper got some attention in this subreddit, especially in the context of [reproducibility](https://www.reddit.com/r/MachineLearning/comments/a04qsj/d_updates_on_perturbative_neural_networks_pnn/)  of the results. The authors provided interesting results as well theoretical motivations and properties of their PNN. However, I got stuck on the Section **3.3 Relating PNN and CNN**, so I wanted to ask experts here to resolve my doubts regarding this particular section. 

Let me start by noting what is the purpose of **Section 3.3:** In my opinion authors want to show that given convolution matrix **A** and input vector **x** we can find a unique perturbation masks matrix **N** of PNN layer, hence the relation between CNNs and PNNs. But is it true in practice? When reading this section I got few observations and questions:

1. The authors assume that N=d, which means that the number of perturbation masks - N is equal to the number of pixels in the feature map times number of channels - d. This assumption is rather not realistic and even in their experiments they limited the number of masks to N \\approx 256. For comparison, the input image had d=224x224x3. Note, this assumption allowed them to use Eq. (7) to prove their reasoning. However, when d != N, we cannot use it, and we should use least square solution Eq. (8), which was not used for some reason.
2. After Eq. (19) authors write that in case when eigenvalues of X\^+AX and A are distinct, there exists a unique solution for their problem, however there are no comments on the validity of this condition in their case. What if there is no solution? I will discuss it later in this post.
3. Why actually we are interested in finding matrix **N**? If **N** is fixed during training, so we have no control over it and it is randomly sampled. The relation between PNNs and CNNs would be clearer to me if the authors showed that given **A** and **x** and some random masks **N** we can **always** find linear combination weights which approximate the output of CNN layer. This would be a good motivation for me. 
4. Let assume that Eq. (19) has unique solution, and we know that given some matrix **A** and some input vector **x** we can find a PNN layer which will approximate output of this convolutional layer.  Actually, I'm not sure if this is a real relationship, since this solution is valid only for certain input **x**. When we will take different input image **x'** this will just not work anymore.

Those are general comments from my side, which I would like to see explained in the paper in order to better understand the motivations for this interesting method. 

Additionally, I have a major doubt regarding the proof in the Section 3.3. I redo the derivations and stopped at the  Eq. (16), which is:

     A X N^1 = X N^-1 A   (16)

In the next step authors apply strange and potentially danger trick with Moore-Penrose pseudoinverse to obtain Sylvester equation (18). However, in my opinion Eq. (16)  is already in good form to be interpreted as Sylvester equation. If we introduce a new variable `Y=X N^-1` , we obtain Sylvester equation of form

    A Y - Y A = 0

which does not have unique solution for **Y,** hence no solution for **N** matrix. This seems to contradict the main conclusions of the **Section 3.3**!  Now, if my observations are valid it may mean that apparently the **Macro View** section is not the best motivation for the relation between CNNs and PNNs, if such exists ... ",6,1,False,self,,,,,
1549,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,7,a0zmfw,youtube.com,"[N] ""Advanced Deep Learning &amp; Reinforcement Learning"" course taught by DeepMind researchers @ University College London now publicly available on YouTube",https://www.reddit.com/r/MachineLearning/comments/a0zmfw/n_advanced_deep_learning_reinforcement_learning/,Z3F,1543356548,,0,1,False,default,,,,,
1550,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,7,a0zn2y,self.MachineLearning,[D] Was IBMs massive annotated face dataset ever released as promised?,https://www.reddit.com/r/MachineLearning/comments/a0zn2y/d_was_ibms_massive_annotated_face_dataset_ever/,anonDogeLover,1543356666,See here: [https://www.ibm.com/blogs/research/2018/06/ai-facial-analytics/](https://www.ibm.com/blogs/research/2018/06/ai-facial-analytics/),5,1,False,self,,,,,
1551,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,7,a0zoyo,self.MachineLearning,[D] GeoFaces Dataset,https://www.reddit.com/r/MachineLearning/comments/a0zoyo/d_geofaces_dataset/,anonDogeLover,1543357001,The website seems to be dead. Anyone have a copy?,6,1,False,self,,,,,
1552,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,7,a0zpqj,self.MachineLearning,"[D] I know these questions probably get posted a lot, but I've got to ask: What exactly is machine learning, how does it relate to the field of data science, and how can someone with minimal-to-no programming experience work their way up to becoming proficient in ML programming",https://www.reddit.com/r/MachineLearning/comments/a0zpqj/d_i_know_these_questions_probably_get_posted_a/,incubateshovels,1543357140,"Let me start this by stating that I am a college senior majoring in IS (Information Systems) but my major isn't very technical in terms of computer programming. It's more focused on business courses (i.e. Accounting, Database Management, Systems Analysis, etc). And I'll admit, I've never been that fond of coding. Just the idea that whatever you're writing has to be ABSOLUTELY PERFECT 100% of the time otherwise it won't work was just absurd to me. But, I've come to respect it and understand how important it is in today's world. 

So in terms of programming knowledge I possess, I'll say slim. Yep. I'm a college senior with extremely minimal programming knowledge who one day wants to break into something so complex and challenging. Makes sense, right? The most I've ever worked with was HTML and JS for a web dev class I had to take and some basic SQL for my DB class last semester. 

Now I understand that math is also a big part in machine learning. Well I'll admit, I struggle with it. I mean I like it when I actually understand what I'm doing and can walk into class like ""Huh that makes sense"", but this semester has been kicking my but (taking Algebra II). And from what I've read, a strong foundation in Calc and Linear Algebra is also paramount. But like I said, I'm a senior. But I do have to take an Applied Calc class next semester.

Again, I apologize for sounding like a complete noob and I'll understand if you downvote me to oblivion because I'm not as knowledgeable or well-read as most of the people on this sub. But any help or insight.",0,1,False,self,,,,,
1553,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,7,a0zt98,self.MachineLearning,"[D] I know these questions probably get posted a lot, but I've got to ask: What exactly is machine learning, how does it relate to the field of data science, and how can someone with minimal-to-no programming experience work their way up to becoming proficient in ML programming?",https://www.reddit.com/r/MachineLearning/comments/a0zt98/d_i_know_these_questions_probably_get_posted_a/,incubateshovels,1543357784,"Let me start this by stating that I am a college senior majoring in IS (Information Systems) but my major isn't very technical in terms of computer programming. It's more focused on business courses (i.e. Accounting, Database Management, Systems Analysis, etc). And I'll admit, I've never been that fond of coding. Just the idea that whatever you're writing has to be ABSOLUTELY PERFECT 100% of the time otherwise it won't work was just absurd to me. But, I've come to respect it and understand how important it is in today's world.

So in terms of programming knowledge I possess, I'll say slim. Yep. I'm a college senior with extremely minimal programming knowledge who one day wants to break into something so complex and challenging. Makes sense, right? The most I've ever worked with was HTML and JS for a web dev class I had to take and some basic SQL for my DB class last semester.

Now I understand that math is also a big part in machine learning. Well I'll admit, I struggle with it. I mean I like it when I actually understand what I'm doing and can walk into class like ""Huh that makes sense"", but this semester has been kicking my but (taking Algebra II). And from what I've read, a strong foundation in Calc and Linear Algebra is also paramount. But like I said, I'm a senior. But I do have to take an Applied Calc class next semester.

Again, I apologize for sounding like a complete noob and I'll understand if you downvote me to oblivion because I'm not as knowledgeable or well-read as most of the people on this sub. But any help or insight.",7,1,False,self,,,,,
1554,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,7,a0ztzc,self.MachineLearning,[City-SAFE project] data collection for my thesis!,https://www.reddit.com/r/MachineLearning/comments/a0ztzc/citysafe_project_data_collection_for_my_thesis/,gabrielcosta1995,1543357921,[removed],0,1,False,self,,,,,
1555,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,8,a10d00,self.MachineLearning,I have two questions about pca.,https://www.reddit.com/r/MachineLearning/comments/a10d00/i_have_two_questions_about_pca/,hah_hahaa,1543361433,"Do you always get unique result if you are doing pca with a same set of data, like the coefficients, scores, and latent.

Second. If theres a dataset with n=100 observations and p=30 predictors. What is the minimum fraction of the total data variance captured by the first PC?

Thanks a lot. ",0,1,False,self,,,,,
1556,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,8,a10la9,self.MachineLearning,Ridge Regression Unique Solution,https://www.reddit.com/r/MachineLearning/comments/a10la9/ridge_regression_unique_solution/,SameerKhanna,1543363096,[removed],0,1,False,self,,,,,
1557,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,9,a10zat,self.MachineLearning,Revoscalepy,https://www.reddit.com/r/MachineLearning/comments/a10zat/revoscalepy/,Canadian_Marine,1543365880,[removed],0,1,False,self,,,,,
1558,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,9,a1121k,sebastianraschka.com,[P] Comparing the performance of machine learning models and algorithms using statistical tests and nested cross-validation,https://www.reddit.com/r/MachineLearning/comments/a1121k/p_comparing_the_performance_of_machine_learning/,seraschka,1543366449,,0,1,False,default,,,,,
1559,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,10,a11i2w,self.MachineLearning,[D] Use of autoregressive flow prior in Variational Lossy Autoencoder paper,https://www.reddit.com/r/MachineLearning/comments/a11i2w/d_use_of_autoregressive_flow_prior_in_variational/,rojmor,1543369577,"Hello everyone,

 I was reading the paper [Variational Lossy Autoencoder](https://arxiv.org/pdf/1611.02731.pdf)and I was wondering if anyone had any insight on how the expectation in equation 13 is being calculated. As was pointed out in Kingma et. al. (2016), an autoregressive flow can only be efficiently calculated in one direction, allowing calculation of the log density for either samples or data points, but not both. In the expectation in equation 13, in order to calculate the density p(z) under the expectation z \~ q(z|x), you must go from z .When sampling from the prior, you would need to go from z. But it's not possible to do both efficiently with an autoregressive flow. So I'm just wondering, how do they deal with this? The paper doesn't seem to mention this issue.

Thanks",7,1,False,self,,,,,
1560,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,11,a11npq,arxiv.org,"[R] ""Deformable ConvNets v2: More Deformable, Better Results"" from MSRA",https://www.reddit.com/r/MachineLearning/comments/a11npq/r_deformable_convnets_v2_more_deformable_better/,flyforlight,1543370723,,7,1,False,default,,,,,
1561,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,11,a120ef,arxiv.org,"[R] ""Integrated Object Detection and Tracking with Tracklet-Conditioned Detection"" from MSRA",https://www.reddit.com/r/MachineLearning/comments/a120ef/r_integrated_object_detection_and_tracking_with/,flyforlight,1543373334,,2,1,False,default,,,,,
1562,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,12,a129sp,self.MachineLearning,Questions about Backpropagation Through Time for Gated Recurrent Unit?,https://www.reddit.com/r/MachineLearning/comments/a129sp/questions_about_backpropagation_through_time_for/,qudcjf7928,1543375239,[removed],0,1,False,self,,,,,
1563,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,12,a12cqf,self.MachineLearning,[D] NLP on Tree-Structured HTML,https://www.reddit.com/r/MachineLearning/comments/a12cqf/d_nlp_on_treestructured_html/,LearnyMcLearnFace,1543375859,"I'm interested in performing natural language processing on HTML documents while taking into account the tree-structure of the html document.  In short, I'd like my algorithm to understand that each token in the HTML document exists at a different node within the DOM tree, and for the metadata associated with each of those DOM nodes (e.g.: div id, location, etc.) to be taken into account during learning.  So far my lit search hasn't yielded any results, but maybe I haven't come up with the right keyword combination.  Have any of you ever heard of anything like this, and if so, do you have any pointers to relevant papers or pertinent search keywords?",3,1,False,self,,,,,
1564,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,12,a12etl,arxiv.org,[R] WarpGAN: Automatic Caricature Generation,https://www.reddit.com/r/MachineLearning/comments/a12etl/r_warpgan_automatic_caricature_generation/,hanyuqn,1543376285,,2,1,False,default,,,,,
1565,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,13,a12myq,self.MachineLearning,Advice on what Algorithms to study for a Job to candidate matching algorithm.,https://www.reddit.com/r/MachineLearning/comments/a12myq/advice_on_what_algorithms_to_study_for_a_job_to/,whooyeah,1543378045,[removed],0,1,False,self,,,,,
1566,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,13,a12qr8,link.medium.com,Disentanglement with VAE: A Review,https://www.reddit.com/r/MachineLearning/comments/a12qr8/disentanglement_with_vae_a_review/,pkgyawali,1543378852,,0,1,False,default,,,,,
1567,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,14,a1387f,self.MachineLearning,[R] Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon,https://www.reddit.com/r/MachineLearning/comments/a1387f/r_machine_learning_for_combinatorial_optimization/,iidealized,1543382787,[removed],0,1,False,self,,,,,
1568,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,14,a138k4,self.MachineLearning,Adversarial Examples Are a Natural Consequence of Test Error in Noise,https://www.reddit.com/r/MachineLearning/comments/a138k4/adversarial_examples_are_a_natural_consequence_of/,iidealized,1543382872,[removed],0,1,False,self,,,,,
1569,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,14,a138p7,self.MachineLearning,[R] Adversarial Examples Are a Natural Consequence of Test Error in Noise,https://www.reddit.com/r/MachineLearning/comments/a138p7/r_adversarial_examples_are_a_natural_consequence/,iidealized,1543382906,[https://openreview.net/pdf?id=S1xoy3CcYX](https://openreview.net/pdf?id=S1xoy3CcYX),0,1,False,self,,,,,
1570,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,14,a13aqi,i.redd.it,My learning path to get into Big tech companies on ML/related fields...,https://www.reddit.com/r/MachineLearning/comments/a13aqi/my_learning_path_to_get_into_big_tech_companies/,endeavour23,1543383375,,0,1,False,default,,,,,
1571,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,15,a13t9i,self.MachineLearning,Papers Recommendation ( unsupervised learning),https://www.reddit.com/r/MachineLearning/comments/a13t9i/papers_recommendation_unsupervised_learning/,Alla_Abdella,1543387999,[removed],0,1,False,self,,,,,
1572,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,15,a13tcq,self.MachineLearning,[R] FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery,https://www.reddit.com/r/MachineLearning/comments/a13tcq/r_finegan_unsupervised_hierarchical/,code--warrior,1543388020,"[https://arxiv.org/abs/1811.11155](https://arxiv.org/abs/1811.11155)

&amp;#x200B;

[https://www.youtube.com/watch?v=tkk0SeWGu-8](https://www.youtube.com/watch?v=tkk0SeWGu-8)",0,1,False,self,,,,,
1573,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,16,a13wiz,alibabacloud.com,Enterprise Smart Brain Powered by AI,https://www.reddit.com/r/MachineLearning/comments/a13wiz/enterprise_smart_brain_powered_by_ai/,Jen_Cl,1543388856,,0,1,False,https://b.thumbs.redditmedia.com/PxCkCUnmn066KReTbJSnsgnMqcQg2HvXsdahr2w59_M.jpg,,,,,
1574,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,16,a13yi6,self.MachineLearning,Where does ground truth data come from when detecting illegal content on websites such as child pornography?,https://www.reddit.com/r/MachineLearning/comments/a13yi6/where_does_ground_truth_data_come_from_when/,BigChiller,1543389392,[removed],0,1,False,self,,,,,
1575,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,16,a13yns,self.MachineLearning,SKU Rationalization,https://www.reddit.com/r/MachineLearning/comments/a13yns/sku_rationalization/,nileshagarwalla,1543389437,[removed],0,1,False,self,,,,,
1576,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,16,a13z8m,self.MachineLearning,3D PointCloud network architecture implemented by PyTorch.,https://www.reddit.com/r/MachineLearning/comments/a13z8m/3d_pointcloud_network_architecture_implemented_by/,zeal_reddit,1543389599,[removed],0,1,False,self,,,,,
1577,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,16,a142es,self.MachineLearning,How to detect and remove cp without ground truth data?,https://www.reddit.com/r/MachineLearning/comments/a142es/how_to_detect_and_remove_cp_without_ground_truth/,BigChiller,1543390469,[removed],0,1,False,self,,,,,
1578,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,16,a144tt,gengo.ai,Top 10 AI &amp; machine learning podcasts,https://www.reddit.com/r/MachineLearning/comments/a144tt/top_10_ai_machine_learning_podcasts/,reimmoriks,1543391135,,0,1,False,default,,,,,
1579,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,18,a14k1t,self.MachineLearning,"How should i apply BERT in image caption tasks,such as im2txt,densecap",https://www.reddit.com/r/MachineLearning/comments/a14k1t/how_should_i_apply_bert_in_image_caption/,laveryang,1543395766,[removed],0,1,False,self,,,,,
1580,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,18,a14lv0,self.MachineLearning,Coordinates is a random forest setting?,https://www.reddit.com/r/MachineLearning/comments/a14lv0/coordinates_is_a_random_forest_setting/,smugfacer,1543396301,[removed],0,1,False,self,,,,,
1581,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,18,a14nac,evilsocket.net,Presenting Project Ergo: How to Build an Airplane Detector for Satellite Imagery With Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a14nac/presenting_project_ergo_how_to_build_an_airplane/,evilsocket,1543396718,,0,1,False,default,,,,,
1582,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,18,a14o1a,youtu.be,"Mesosphere DC/OS Secure (Kerberos &amp; TLS) Machine Learning Pipeline with Apache Kafka, HDFS and Spark",https://www.reddit.com/r/MachineLearning/comments/a14o1a/mesosphere_dcos_secure_kerberos_tls_machine/,vitalijzad,1543396981,,0,1,False,https://b.thumbs.redditmedia.com/S3TdPeTikoR4TP36eHltk3LpDIOmVaueskMdXUwQlrk.jpg,,,,,
1583,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,18,a14sg5,self.MachineLearning,What would be good design of Neural Network fro classification of survey data,https://www.reddit.com/r/MachineLearning/comments/a14sg5/what_would_be_good_design_of_neural_network_fro/,dijete_u_vremenu,1543398312,[removed],1,1,False,self,,,,,
1584,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,19,a14w54,self.MachineLearning,christmas presents for a ML nerd?,https://www.reddit.com/r/MachineLearning/comments/a14w54/christmas_presents_for_a_ml_nerd/,klarinette,1543399394,[removed],0,1,False,self,,,,,
1585,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,19,a152cd,arxiv.org,[R] Label-Noise Robust Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/a152cd/r_labelnoise_robust_generative_adversarial/,i-like-big-gans,1543401210,,0,1,False,default,,,,,
1586,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,19,a154ga,yourstory.com,What are prerequisites to start with the Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/a154ga/what_are_prerequisites_to_start_with_the_machine/,JanBaskTraining,1543401807,,0,1,False,default,,,,,
1587,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,19,a156a0,self.MachineLearning,New Data Science Platform - Looking for feedback!,https://www.reddit.com/r/MachineLearning/comments/a156a0/new_data_science_platform_looking_for_feedback/,heeguneom,1543402346,[removed],0,1,False,self,,,,,
1588,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,20,a15dhm,agdaily.com,What roles will AI and machine learning have in feeding the world?,https://www.reddit.com/r/MachineLearning/comments/a15dhm/what_roles_will_ai_and_machine_learning_have_in/,cridercastelli,1543404424,,0,1,False,default,,,,,
1589,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,21,a15ppf,arxiv.org,[R] GAN Dissection: Visualizing and Understanding Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/a15ppf/r_gan_dissection_visualizing_and_understanding/,i-like-big-gans,1543407659,,7,1,False,default,,,,,
1590,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,21,a15r9o,self.MachineLearning,[P] Bayesian image classifier using Pyro to give uncertainties in output,https://www.reddit.com/r/MachineLearning/comments/a15r9o/p_bayesian_image_classifier_using_pyro_to_give/,invertedpassion,1543408040,"**Code / notebook**: [https://github.com/paraschopra/bayesian-neural-network-mnist](https://github.com/paraschopra/bayesian-neural-network-mnist)

&amp;#x200B;

**Tutorial**: [https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd)

&amp;#x200B;

**Summary**: A bayesian neural network trained to recognize handwritten digits from MNIST whose parameters are inferred via variational inference techniques. The main advantage of using bayesian techniques in neural networks is that you can sample outputs and infer a distribution using which you can deduce confidence / certainty of the output. I used the inferred network to recognize images that are not like MNIST digits.

&amp;#x200B;

[Sample evaluation on MNIST dataset \(with histogram of log-probabilities \~100 samples\)](https://i.redd.it/jmkd6u1ae2121.png)

**The accuracy on MNIST dataset on test set was 96%** (it skipped about 12% of hard to recognize images). When I fed it randomly generated images, the network refused to classify 95% of images and when I fed it alphabet looking images **(from not-MNIST dataset), it refused to classify \~80% of images.** 

&amp;#x200B;

[Sample evaluation on not-MNIST dataset \(with histogram of log-probabilities \~100 samples\)](https://i.redd.it/0d1lwxrde2121.png)

This approach can be applied to other networks such as financial prediction networks where if conditions change, you want your network to stop predicting rather than making wrong predictions.

&amp;#x200B;

Hope you like the project! It's my first public project in ML.",6,1,False,https://b.thumbs.redditmedia.com/tEHFACeabcHgQ1PxMEOZNhJHk0Wbq9_KVNtqk9L26qY.jpg,,,,,
1591,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,22,a161ll,self.MachineLearning,NL-PCA with Autoencoders,https://www.reddit.com/r/MachineLearning/comments/a161ll/nlpca_with_autoencoders/,noonien_soong_86,1543410470,[removed],0,1,False,self,,,,,
1592,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,22,a16c3m,r2d3.us,An intuitive visual introduction to Decision Trees,https://www.reddit.com/r/MachineLearning/comments/a16c3m/an_intuitive_visual_introduction_to_decision_trees/,mtoba,1543412790,,0,1,False,https://b.thumbs.redditmedia.com/Mm9kOVGDG9pIdP0Uy3QNcJ_EPU3JeXvodcg-7xFGp0s.jpg,,,,,
1593,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,22,a16fhv,weirdgeek.com,How to Split Data for Machine Learning with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/a16fhv/how_to_split_data_for_machine_learning_with/,WeirdGeekDotCom,1543413554,,0,1,False,default,,,,,
1594,MachineLearning,t5_2r3gv,2018-11-28,2018,11,28,23,a16i01,self.MachineLearning,Absolute noob at applying ML. Let us say we have the data of the energy consumed by an air conditioner over the past year or so. I want to predict the energy consumption patterns for the future using the available data. Is this even a machine learning problem?,https://www.reddit.com/r/MachineLearning/comments/a16i01/absolute_noob_at_applying_ml_let_us_say_we_have/,deepakprabakar,1543414061,,0,1,False,self,,,,,
1595,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,0,a16y2s,self.MachineLearning,Absolute noob at applying ML here. Let us say we have the minute to minute data of the energy consumed by an air conditioner over the past year or so. I want to predict the energy consumption patterns for the future using the available data. Is this even a machine learning problem?,https://www.reddit.com/r/MachineLearning/comments/a16y2s/absolute_noob_at_applying_ml_here_let_us_say_we/,deepakprabakar,1543417300,,0,1,False,self,,,,,
1596,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,0,a170ru,self.MachineLearning,VGAN (Variational Discriminator Bottleneck) CelebA 128px results after 300K iterations,https://www.reddit.com/r/MachineLearning/comments/a170ru/vgan_variational_discriminator_bottleneck_celeba/,akanimax,1543417791," After 2 weeks of continuous training, my VGAN (VDB) celebA 128px results are ready. Finally, my GPU can now take a breath of relief.

Trained weights are available athttps://drive.google.com/drive/folders/13FGiuqAL1MbSDDFX3FlMxLrv90ACCdKCcode at:[https://github.com/akanimax/Variational\_Discriminator\_Bottleneck](https://github.com/akanimax/Variational_Discriminator_Bottleneck)

&amp;#x200B;

![video](9e2up8hi73121 ""128px CelebA samples"")

&amp;#x200B;

Also, my acquaintance Gwern Branwen has trained VGAN using my implementation on hisDanbooru2017 dataset for 3 GPU days. Check out his results at[https://twitter.com/gwern/status/1064903976854978561](https://twitter.com/gwern/status/1064903976854978561)

&amp;#x200B;

![img](z5tcz4bl73121 ""Anime faces generated by Gwern 1"")

![img](zlpjrfbl73121 ""Anime faces generated by Gwern 2"")

Please feel free to experiment with my implementation on your choice of dataset.",0,1,False,https://b.thumbs.redditmedia.com/6gHtsPv8cS-uhYuMm9PWRyy_aQfOybLOjeP6ZbiykzY.jpg,,,,,
1597,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,0,a172bb,self.MachineLearning,[P] VGAN (Variational Discriminator Bottleneck) CelebA 128px results after 300K iterations (includes weights),https://www.reddit.com/r/MachineLearning/comments/a172bb/p_vgan_variational_discriminator_bottleneck/,akanimax,1543418112,"After 2 weeks of continuous training, my VGAN (VDB) celebA 128px results are ready. Finally, my GPU can now take a breath of relief.

Trained weights are available athttps://drive.google.com/drive/folders/13FGiuqAL1MbSDDFX3FlMxLrv90ACCdKC

code at:[https://github.com/akanimax/Variational\_Discriminator\_Bottleneck](https://github.com/akanimax/Variational_Discriminator_Bottleneck)

&amp;#x200B;

*Processing video 8b255sbo83121...*

Also, my acquaintance Gwern Branwen has trained VGAN using my implementation on hisDanbooru2017 dataset for 3 GPU days. Check out his results at[https://twitter.com/gwern/status/1064903976854978561](https://twitter.com/gwern/status/1064903976854978561)

&amp;#x200B;

*Processing img c8768qct83121...*

*Processing img mlx36rct83121...*

Please feel free to experiment with this implementation on your choice of dataset.",19,1,False,self,,,,,
1598,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,0,a175cq,self.MachineLearning,[D] NL-PCA with Autoencoders,https://www.reddit.com/r/MachineLearning/comments/a175cq/d_nlpca_with_autoencoders/,noonien_soong_86,1543418682,"I'm trying to do a nonlinear dimensionality reduction for a time-series of vector fields using autoencoders, as an alternative to the linear POD/PCA approach.

My input vectors consist of a flattened 1d array (1x1000) of every point in the vector field.  As a benchmark i did a POD/PCA on my dataset, where i picked a number of 5 most high-energetic modes to be calculated. So the result of this are 5 vectors (POD modes), which have the same shape as my input vectors (1x1000), which together form a 5x1000 matrix.

&amp;#x200B;

Firstly i implemented a simple 3-layer autoencoder where both activation functions are linear, the hidden layer size is 5 and the input and output layers are of size 1000. After training i extracted the demapping weight matrix, which gave me again a 5x1000 matrix.  So in theory this, after sufficient training, converges to the same subspace, spanned by my PCA components.

&amp;#x200B;

As i have read, to perform a nonlinear generalization of this linear PCA approximation, i have to use nonlinear activation functions, as well as use multiple hidden layers instead of just one.

And here lies my problem. If i use more hidden layers, with the bottleneck layer size still be set to 5, how can i extract a 5x1000 matrix out of the weight matrices, since there is no singe weight matrix between the output layer (size 1000) and the bottleneck layer (size 5)?

&amp;#x200B;",2,1,False,self,,,,,
1599,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,0,a177nv,eventbrite.com,Jetson Developer Meetup at NVIDIA Endeavor,https://www.reddit.com/r/MachineLearning/comments/a177nv/jetson_developer_meetup_at_nvidia_endeavor/,nanobot_1000,1543419098,,0,1,False,default,,,,,
1600,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,0,a17eg5,self.MachineLearning,"Simple Questions Thread November 28, 2018",https://www.reddit.com/r/MachineLearning/comments/a17eg5/simple_questions_thread_november_28_2018/,AutoModerator,1543420364,[removed],0,1,False,self,,,,,
1601,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,1,a17ixn,i.redd.it,[R] Machine vs Cardiologist,https://www.reddit.com/r/MachineLearning/comments/a17ixn/r_machine_vs_cardiologist/,aulloa,1543421180,,0,1,False,default,,,,,
1602,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,1,a17lxb,self.MachineLearning,"Register for Free December 4 ACM Learning Talk, ""Break Into AI: A Q&amp;A with Andrew Ng on Building a Career in Machine Learning""",https://www.reddit.com/r/MachineLearning/comments/a17lxb/register_for_free_december_4_acm_learning_talk/,ACMLearning,1543421714,[removed],0,1,False,self,,,,,
1603,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,1,a17obk,self.MachineLearning,[R] Machine vs Cardiologist,https://www.reddit.com/r/MachineLearning/comments/a17obk/r_machine_vs_cardiologist/,aulloa,1543422155,"I am excited to share my most recent work on how a trained machine could predict mortality with higher accuracy than two cardiologists. Any criticism is welcomed

[https://arxiv.org/abs/1811.10553](https://arxiv.org/abs/1811.10553)

https://i.redd.it/8f0qoqcok3121.png",39,1,False,self,,,,,
1604,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,1,a17wv0,self.MachineLearning,[P] AI Atelier using internet image montage and interactive style transfer,https://www.reddit.com/r/MachineLearning/comments/a17wv0/p_ai_atelier_using_internet_image_montage_and/,kh22l22,1543423643, [https://youtu.be/19VD5\_Dgx1o](https://youtu.be/19VD5_Dgx1o?fbclid=IwAR3DYZY2OZBmVOdIAkF0EoZMYRCC4kixW5d8lbrmNoEG41lFKRoDd6R-GWM) ,0,1,False,self,,,,,
1605,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,1,a17yp4,self.MachineLearning,"[D] Implementing dcgan on google colab tpu, need help with dataset api and reading from disc",https://www.reddit.com/r/MachineLearning/comments/a17yp4/d_implementing_dcgan_on_google_colab_tpu_need/,hadaev,1543423976,"Cant get, do i wrong something or just tpu can't use colab disc at all.

In current implementation i send image names, then read it by tf.py\_func, but get error

    No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[DT_STRING], Tout=[DT_FLOAT], token=""pyfunc_0"", _device=""/job:worker/task:0/device:CPU:0""](arg0) 	.  Registered:  &lt;no registered kernels&gt; 

I also tried to read tfrecords file from the disk, but got another error.

This works when I read tfrecords file from google cloud storage, but because of this, the speed drops 2-3 times.  As I understand it, due to connection delays with google cloud.

[Colab notebook](https://colab.research.google.com/drive/101FjBAIMVuXyNyeUvq_Vfx-Z6CR3g4df)",6,1,False,self,,,,,
1606,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,1,a17z7t,self.MachineLearning,[P] AI Atelier using internet image montage and interactive style transfer,https://www.reddit.com/r/MachineLearning/comments/a17z7t/p_ai_atelier_using_internet_image_montage_and/,kh22l22,1543424061,[removed],0,1,False,spoiler,,,,,
1607,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,2,a181xf,self.MachineLearning,"Powerful, Hybrid Machine Learning Algorithm with Excel Implementation",https://www.reddit.com/r/MachineLearning/comments/a181xf/powerful_hybrid_machine_learning_algorithm_with/,andrea_manero,1543424539,https://www.analyticbridge.datasciencecentral.com/forum/topics/powerful-hybrid-machine-learning-algorithm-with-excel-implementat,0,1,False,self,,,,,
1608,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,2,a182cb,webinars.on24.com,Register for Free December 4 ACM Learning Talk with AI Pioneer and Visionary Andrew Ng,https://www.reddit.com/r/MachineLearning/comments/a182cb/register_for_free_december_4_acm_learning_talk/,ACMLearning,1543424607,,0,1,False,https://b.thumbs.redditmedia.com/J0H6oZmV7F3ZPlhN_e2VaqSFZ82OK8msA2g9Y6hQ8AU.jpg,,,,,
1609,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,2,a18foe,self.MachineLearning,"[N] FCC Forum on AI and Machine Learning - Friday, Nov. 30, 9am to 2pm",https://www.reddit.com/r/MachineLearning/comments/a18foe/n_fcc_forum_on_ai_and_machine_learning_friday_nov/,barred214,1543426909,""" This forum will convene experts in the AI and machine learning fields to discuss the future of these technologies and their implications for the communications marketplace.""  You can go in person to the FCC in Washington, DC (subject to the limits of what the room holds) or watch online.  More info [here.](https://www.fcc.gov/news-events/events/2018/11/forum-artificial-intelligence-and-machine-learning)",0,1,False,self,,,,,
1610,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,2,a18j4c,heartbeat.fritz.ai,Comparing ML Kits Text Recognition API on Android &amp; iOS,https://www.reddit.com/r/MachineLearning/comments/a18j4c/comparing_ml_kits_text_recognition_api_on_android/,tits_for_tots,1543427501,,0,1,False,default,,,,,
1611,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a18nsf,heartbeat.fritz.ai,[N] Comparing ML Kits Text Recognition API on Android &amp; iOS,https://www.reddit.com/r/MachineLearning/comments/a18nsf/n_comparing_ml_kits_text_recognition_api_on/,tits_for_tots,1543428280,,0,1,False,https://b.thumbs.redditmedia.com/feO57NXZpsScccSx-xlQFPHeMjEO24MXyq0kpm1Ho9I.jpg,,,,,
1612,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a18ups,self.MachineLearning,[R] ExpandNets: Exploiting Linear Redundancy to Train Small Networks,https://www.reddit.com/r/MachineLearning/comments/a18ups/r_expandnets_exploiting_linear_redundancy_to/,toraml,1543429486,"arXiv: [https://arxiv.org/abs/1811.10495](https://arxiv.org/abs/1811.10495)

PDF: [https://arxiv.org/pdf/1811.10495.pdf](https://arxiv.org/pdf/1811.10495.pdf)

**Abstract**

While very deep networks can achieve great performance, they are ill-suited to applications in resource-constrained environments. Knowledge transfer, which leverages a deep teacher network to train a given small network, has emerged as one of the most popular strategies to address this problem. In this paper, we introduce an alternative approach to training a given small network, based on the intuition that parameter redundancy facilitates learning. We propose to expand each linear layer of a small network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can be compressed back to the small one algebraically, but, as evidenced by our experiments, consistently outperforms training the small network from scratch. This strategy is orthogonal to knowledge transfer. We therefore further show on several standard benchmarks that, for any knowledge transfer technique, using our expanded network as student systematically improves over using the small network.

**My own short summary**

Usually having multiple linear layers consecutively without any non-linearities between them is avoided as they are equivalent to a single linear layer. In this work the authors expand the linear layers of a network to multiple consecutive linear layers. For example \`Dense(100-&gt;50)\` can be expanded to \`Dense(100, 200) =&gt; Dense(200, 400) =&gt; Dense(400, 50)\` and \`Conv3x3 (8ch-&gt;16ch)\` can be expanded to \`Conv1x1 (8ch-&gt;32ch) =&gt; Conv3x3 (32ch-&gt;64ch) =&gt; Conv1x1 (64ch-&gt;16ch)\`. After training these layers can be reduced to a single layer again so no runtime or memory penalty is incurred at test-time. While being mathematically equivalent they show on CIFAR benchmarks that the expansion actually improves performance compared to the unexpanded network.",0,1,False,self,,,,,
1613,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a18v2g,self.aws,The AWS AI / ML Team - Ask the Experts - November 28th @ 2PM PT / 5PM ET!,https://www.reddit.com/r/MachineLearning/comments/a18v2g/the_aws_ai_ml_team_ask_the_experts_november_28th/,ckilborn,1543429551,,1,1,False,https://b.thumbs.redditmedia.com/pn_FGhuoBzdzIQTJQ1idUd3rKeiu4SAPge3QKCDjqgk.jpg,,,,,
1614,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a18ytb,self.MachineLearning,[P] / [D] Deep Learning on Tabular Data: Entity Embedding for Categorical Variables,https://www.reddit.com/r/MachineLearning/comments/a18ytb/p_d_deep_learning_on_tabular_data_entity/,buy_some_wow,1543430192,"I've recently come across the idea of learning embeddings for categorical features and have implemented that using a simple neural network. Link to repo: https://github.com/akilat90/entity-embedding-experiment

I'd like to hear from those that have experience in this area.

1. What kind of architectures that you found to be most successful?
2. How do you evaluate the quality of the embeddings?
",0,1,False,self,,,,,
1615,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a1907p,blog.floydhub.com,[Project Clara] Generating Classical Music with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a1907p/project_clara_generating_classical_music_with/,pirate7777777,1543430439,,0,1,False,https://b.thumbs.redditmedia.com/O9J9CGvlPkY2x-Uf6fRFPQRYF1GENFhmFwSs5ZZJaKY.jpg,,,,,
1616,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a192d9,self.MachineLearning,Inviting Users of RiseML and/or Floyd for a remote user study session,https://www.reddit.com/r/MachineLearning/comments/a192d9/inviting_users_of_riseml_andor_floyd_for_a_remote/,benazirs,1543430805,[removed],0,1,False,self,,,,,
1617,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a195y6,akosiorek.github.io,"[D] Forge, or how do you manage your machine learning experiments?",https://www.reddit.com/r/MachineLearning/comments/a195y6/d_forge_or_how_do_you_manage_your_machine/,akosiorek,1543431435,,0,1,False,default,,,,,
1618,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,3,a1966x,self.MachineLearning,Parties at NIPS 2018!,https://www.reddit.com/r/MachineLearning/comments/a1966x/parties_at_nips_2018/,insider_7,1543431477,[removed],0,1,False,self,,,,,
1619,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,4,a197mr,akosiorek.github.io,"[D] Forge, or how do you manage your machine learning experiments?",https://www.reddit.com/r/MachineLearning/comments/a197mr/d_forge_or_how_do_you_manage_your_machine/,akosiorek,1543431725,,0,1,False,default,,,,,
1620,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,4,a197vw,self.MachineLearning,[D] Class conditional invariance vs invariance,https://www.reddit.com/r/MachineLearning/comments/a197vw/d_class_conditional_invariance_vs_invariance/,facundoq,1543431771,"I'm trying to analyse different ways to provide invariance to rotations in images with CNNS in classification problems.

 Measuring stuff, I've noticed that there's a subtle difference between a model being invariant to a transformation (in general), vs a model being invariant to a transformation for a given class output. 

Say we train a network with a model that's been data augmented by rotating the samples. 

The cross entropy error function we usually employ to train networks for classification will try to make the prob of a class 1 for all rotated samples of that class, and 0 for the rest. If perfectly trained (forget about generalization) the output of the network for a given class should not change under any rotation of the input image. 
Given that the cross entropy tries to make other classes' outputs equal to 0, those outputs should (again, in an ideal regime) not change wrt to rotations. However, that's hard to achieve and networks usually output non-zero probabilities for classes other than the correct one. And these do vary a lot wrt to rotations.

To be more precise, given an image I of class C, define the output of the network N for that class as N(I,C). Then N( R(I),C)= N(I,C) (class conditional invariance) is what I observe, since for other classes C' N( R(I),C')= N(I,C`) doesn't hold. That means that in general N(R(I)) != N(I), ie, the network is not invariant to rotations for all its outputs.

Therefore, when we say that a network is ""invariant to rotation"", shouldn't we say that it is ""class conditionally invariant to rotations""? Do you know of any papers/books discussing this?


",3,1,False,self,,,,,
1621,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,4,a19fm1,self.MachineLearning,[D] What are the ML training tips that you know to improve your NLP model accuracy?,https://www.reddit.com/r/MachineLearning/comments/a19fm1/d_what_are_the_ml_training_tips_that_you_know_to/,oldforstocks,1543433133,"Some of the things that I do:

1. I train my model with Adam optimizer and then use SGD+momentum to beat it.

2. Substitute all your word embedding with ELMo. It generally improves the score.

...",10,1,False,self,,,,,
1622,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,4,a19gc8,self.MachineLearning,How to improve an RNN model?,https://www.reddit.com/r/MachineLearning/comments/a19gc8/how_to_improve_an_rnn_model/,Elyash,1543433299,[removed],0,1,False,self,,,,,
1623,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,5,a19y8u,youtube.com,Evolutionary Algorithms: an Introduction,https://www.reddit.com/r/MachineLearning/comments/a19y8u/evolutionary_algorithms_an_introduction/,shahinrostami,1543436675,,1,1,False,https://b.thumbs.redditmedia.com/NEpBB-MUOVpiqFITvjiAI6199Q5QvRXZh6NXSkNjv3c.jpg,,,,,
1624,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,5,a1a1wa,self.MachineLearning,Cloud based ML Service provider Comparison,https://www.reddit.com/r/MachineLearning/comments/a1a1wa/cloud_based_ml_service_provider_comparison/,eco_bach,1543437302,[removed],0,1,False,self,,,,,
1625,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,5,a1a8ck,self.MachineLearning,"Senior researchers in research lab - etiquette, obligations and ethics",https://www.reddit.com/r/MachineLearning/comments/a1a8ck/senior_researchers_in_research_lab_etiquette/,FontofFortunes,1543438426,"Hey Everyone,

I'm curious how people view older, senior researchers in your business, government or university research labs. I'm currently actively engaged and leading (technically) in a large research project - consider it something like a second or third post-doc.

The older, senior researchers on my team are mostly low performers - very knowledgeable people, but not very actively involved in ""doing the work"" and contributing minimally in meetings as far as technical advice is concerned.

How does everyone here deal with older under-performers (something like the detached tenured prof) in terms of getting them motivated and engaged, and most pertinent for me (since the former will likely fail) - dealing with the fallout of not including them as co-authors if their contributions are next to nil.

Thanks for your insight.



",0,1,False,self,,,,,
1626,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,5,a1a8na,self.MachineLearning,NeurIPS accepted papers (complete) are out!,https://www.reddit.com/r/MachineLearning/comments/a1a8na/neurips_accepted_papers_complete_are_out/,wavelander,1543438478,"I was looking forward to reading a couple since I was working on something similar so I hope this helps people.

&amp;#x200B;

Link: [https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)",0,1,False,self,,,,,
1627,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,6,a1acau,arxiv.org,[R] From Recognition to Cognition: Visual Commonsense Reasoning,https://www.reddit.com/r/MachineLearning/comments/a1acau/r_from_recognition_to_cognition_visual/,rowanz,1543439089,,0,1,False,default,,,,,
1628,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,6,a1alpe,self.MachineLearning,Is there non-onesided Real-to-complex Discrete Fourier Transform in tensorflow?,https://www.reddit.com/r/MachineLearning/comments/a1alpe/is_there_nononesided_realtocomplex_discrete/,kmolinfu,1543440700,You can set up onesided=False in pytorch.rfft. I'm wondering if tf has similar rft function that controls whether to return half of results to avoid redundancy. (I want to keep the redundancy)? Or if there any method that can achieve this in tf?,0,1,False,self,,,,,
1629,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,6,a1am82,self.MachineLearning,[P] Ensmallen  Flexible C++ library for efficient mathematical optimization,https://www.reddit.com/r/MachineLearning/comments/a1am82/p_ensmallen_flexible_c_library_for_efficient/,eusben,1543440795,,0,1,False,self,,,,,
1630,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,6,a1apxj,github.com,[P] Ensmallen  Flexible C++ library for efficient mathematical optimization,https://www.reddit.com/r/MachineLearning/comments/a1apxj/p_ensmallen_flexible_c_library_for_efficient/,eusben,1543441444,,0,1,False,https://b.thumbs.redditmedia.com/btdYyRfOTKm4knO5YP2NyPKB-GoRqznygmIIejWZ1YQ.jpg,,,,,
1631,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,6,a1aten,github.com,[P] ensmallen  flexible C++ library for efficient mathematical optimization,https://www.reddit.com/r/MachineLearning/comments/a1aten/p_ensmallen_flexible_c_library_for_efficient/,eusben,1543442073,,0,1,False,https://b.thumbs.redditmedia.com/btdYyRfOTKm4knO5YP2NyPKB-GoRqznygmIIejWZ1YQ.jpg,,,,,
1632,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,7,a1b2xg,self.MachineLearning,General Neural Network Setup,https://www.reddit.com/r/MachineLearning/comments/a1b2xg/general_neural_network_setup/,thekiwininja99,1543443773,[removed],0,1,False,self,,,,,
1633,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,7,a1b5aw,self.MachineLearning,[D] Is anyone using Capsule Networks in practice?,https://www.reddit.com/r/MachineLearning/comments/a1b5aw/d_is_anyone_using_capsule_networks_in_practice/,philosophist_,1543444186,"Paper \[Hinton et al.\]: [https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)

This architecture has been fascinating me recently and I'm curious if anyone here has found them useful.",0,1,False,self,,,,,
1634,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,8,a1bfpg,self.MachineLearning,[P] Ensmallen  Flexible C++ library for efficient mathematical optimization,https://www.reddit.com/r/MachineLearning/comments/a1bfpg/p_ensmallen_flexible_c_library_for_efficient/,eusben,1543446096,"ensmallen provides a simple set of abstractions for writing an objective function to optimize. It also provides a large set of standard and cutting-edge optimizers that can be used for virtually any mathematical optimization task. These include full-batch gradient descent techniques, small-batch techniques, gradient-free optimizers, and constrained optimization.

&amp;#x200B;

Code: [https://github.com/mlpack/ensmallen](https://github.com/mlpack/ensmallen)

&amp;#x200B;

Website: [http://ensmallen.org](http://ensmallen.org)

&amp;#x200B;

arxiv: [https://arxiv.org/abs/1810.09361](https://arxiv.org/abs/1810.09361)",7,1,False,self,,,,,
1635,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,8,a1bihm,self.MachineLearning,[P] how to render OpenAi Gym in Google Colaboratory,https://www.reddit.com/r/MachineLearning/comments/a1bihm/p_how_to_render_openai_gym_in_google_colaboratory/,sigmoidp,1543446593,"We found it really useful to get visual feedback when training your models on OpenAi Gym- its also really fun to see your models training too!

Out of the Deep RL course that we ran earlier this year, we developed a method in order to render OpenAi Gym in  Colaboratory.

Thought it might be useful for others in the community:

[https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/](https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/)

&amp;#x200B;",3,1,False,self,,,,,
1636,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,8,a1bo21,self.MachineLearning,"[D] Looking for any type of scholarships for study at US, any type of advices accepted",https://www.reddit.com/r/MachineLearning/comments/a1bo21/d_looking_for_any_type_of_scholarships_for_study/,cagimer,1543447651,"Hi everyone, I am not sure if this post in true place but forgive me. I am a computer science student from Turkey. This year my last year and I want to work or study or do internship about machine learning in US because Turkey is not really available to create humanmade brains. I've made projects about classification and object detection, have experience with PyTorch and OpenCV. I like to trying to reading papers and examining projects sources on GitHub. I like that learning concept so I like everything to learn more.

So do you have an advice for me or do you know anyone who providing opportunities to people like me? Thanks and sorry for language.",9,1,False,self,,,,,
1637,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,9,a1bxzk,self.MachineLearning,[P] Training multiple CNNs on multi-GPU system,https://www.reddit.com/r/MachineLearning/comments/a1bxzk/p_training_multiple_cnns_on_multigpu_system/,aajmera,1543449605,"I want to simultaneously train multiple CNN models on separate GPUs using tensorflow and python multiprocessing, but have been facing certain issues. Can anyone help?

Here is the link to code and problem in detail: [https://stackoverflow.com//training-multiple-networks-on-a](https://l.facebook.com/l.php?u=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F53474419%2Ftraining-multiple-networks-on-a-multi-gpu-system-using-tensorflow-and-multiproce%3Ffbclid%3DIwAR0tCiuquZuJkzK6aQsZ_3A6vjwOP26u5nB54wR5Hl3rcTQpYFO_OPry0Yg&amp;h=AT3kJ-npadXeiShdcf_HMHnnn-qh5WzqT0r56zf8KwAvXVjRCITiGhTuywRzJwGnVIpkYT12PcG3uggQ6i6GRWBnXFO4802gjUq6V_bPapfVcx-W0WbdK04L6ZwzDzAJlxMwB-gVp6PTy0S7JiHsnBjvzwzrLlNo)

Any help is highly appreaciated. Thanks.",0,1,False,self,,,,,
1638,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,9,a1bz7n,self.MachineLearning,[D] Embedding layer in DNN,https://www.reddit.com/r/MachineLearning/comments/a1bz7n/d_embedding_layer_in_dnn/,marksteve4,1543449857,"Embedding layer usually projects features from sparse namespaces to a dense feature vector space represented by an embedding matrix.

I read this today. But still curious what the rational to have this embedding layer. why is it important to projecdt form sparse to dense?",4,1,False,self,,,,,
1639,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,9,a1c2mn,self.MachineLearning,Need help with my final year project in Computer Science(Undergraduate),https://www.reddit.com/r/MachineLearning/comments/a1c2mn/need_help_with_my_final_year_project_in_computer/,LordSnow__,1543450504,[removed],0,1,False,self,,,,,
1640,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,9,a1c6go,self.MachineLearning,[N] AWS announces SageMaker RL (Reinforcement Learning),https://www.reddit.com/r/MachineLearning/comments/a1c6go/n_aws_announces_sagemaker_rl_reinforcement/,thomelane,1543451306,"SageMaker RL was released today at AWS re:Invent 2018. Should simplify the reinforcement learning process, from setting up custom environments to scalable training (distributed policy rollouts) and model deployment.

Supports [Intel's Coach](https://github.com/NervanaSystems/coach), [Ray RL](https://ray.readthedocs.io/en/latest/rllib.html), [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/), [Open AI Gym](https://gym.openai.com/) and [SimuLink](https://www.mathworks.com/products/simulink.html).

[https://aws.amazon.com/blogs/aws/amazon-sagemaker-rl-managed-reinforcement-learning-with-amazon-sagemaker/](https://aws.amazon.com/blogs/aws/amazon-sagemaker-rl-managed-reinforcement-learning-with-amazon-sagemaker/)",7,1,False,self,,,,,
1641,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,9,a1c8oq,self.MachineLearning,[N] Lambda is giving away three NeurIPS 2018 Tickets,https://www.reddit.com/r/MachineLearning/comments/a1c8oq/n_lambda_is_giving_away_three_neurips_2018_tickets/,sabalaba,1543451774,"Hey everybody, Lambda is a Diamond sponsor this year at NeurIPS 2018 and we have three extra tickets. Knowing that NeurIPS sold out in less than 12 minutes this year, we've decided to give away three of the tickets that we've been given as a sponsor. Here's the direct link to the official blog post and the ticket application form:

[https://lambdalabs.com/blog/neurips-ticket-contest/](https://lambdalabs.com/blog/neurips-ticket-contest/)

[https://docs.google.com/forms/d/e/1FAIpQLSe50HnHeieJ0Huw71qa...](https://docs.google.com/forms/d/e/1FAIpQLSe50HnHeieJ0Huw71qago2wr86cRE5l0wvxOWnTDcoofZ_vkQ/viewform)

If you have any questions, I'm happy to answer them here.

This is the list of NeurIPS 2018 sponsor page to verify our status as a Diamond sponsor and confirm our possession of the tickets:

[https://nips.cc/Conferences/2018/Sponsors](https://nips.cc/Conferences/2018/Sponsors)",3,1,False,self,,,,,
1642,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,9,a1cdzb,i.redd.it,ML Collaborative Filtering Noob,https://www.reddit.com/r/MachineLearning/comments/a1cdzb/ml_collaborative_filtering_noob/,noobystok,1543452873,,1,1,False,default,,,,,
1643,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,10,a1cs3x,self.MachineLearning,"Deep Learning: Help with Weights, Biases and Pooling Layer",https://www.reddit.com/r/MachineLearning/comments/a1cs3x/deep_learning_help_with_weights_biases_and/,Eganx,1543455832,[removed],0,1,False,self,,,,,
1644,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,11,a1d5h5,self.MachineLearning,Question about gradient in back-propagation method,https://www.reddit.com/r/MachineLearning/comments/a1d5h5/question_about_gradient_in_backpropagation_method/,mino_ha,1543458665,[removed],0,1,False,self,,,,,
1645,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,11,a1d5q6,arxiv.org,[R] Partial Convolution based Padding,https://www.reddit.com/r/MachineLearning/comments/a1d5q6/r_partial_convolution_based_padding/,xternalz,1543458722,,3,1,False,default,,,,,
1646,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,11,a1d7h3,arxiv.org,[R] Self-Supervised Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/a1d7h3/r_selfsupervised_generative_adversarial_networks/,HigherTopoi,1543459091,,1,1,False,default,,,,,
1647,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,11,a1d8j7,self.MachineLearning,question on T-test,https://www.reddit.com/r/MachineLearning/comments/a1d8j7/question_on_ttest/,MENAYAK,1543459317,[removed],0,1,False,self,,,,,
1648,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,11,a1d9ep,self.MachineLearning,[D] N(eur)IPS Visa Issues with Canada,https://www.reddit.com/r/MachineLearning/comments/a1d9ep/d_neurips_visa_issues_with_canada/,do_you_even_visa,1543459494,"Hello everyone, this is a long post about my story to obtain short-term Canadian visa to attend N(eur)IPS and fail miserably. I am wondering if I am the only one that had this problem. For those who do not have time to go through the post, **TLDR** is at the bottom.

&amp;#x200B;

I'm a first year PhD student that luckily had an accepted paper in N(eur)IPS18. Unfortunately, my passport does not allow free-travel to Canada, so I applied for a short-term Canadian visa with following additional documents except for standard ones (application form, bio-metrics, family form):

&amp;#x200B;

**Travel Documents (Paid):**

Flight tickets (to and from).

Accommodation.

Travel insurance.

**Conference-related Documents:**

N(eur)IPS invitation letter.

Copy of paper acceptance email.

**Education/Work-related Documents:**

Proof of employment (which shows that I my contract is valid for the upcoming 3 years as a researcher/teaching assistant).

Permission to leave from my institution (for the conference).

Letter from my advisor stating the reason of my travel to Canada.

Letter from my advisor stating my clarifying my position as a PhD student.

Student card showing that I am a PhD student.

**Financial-Support Documents:**

Bank statement showing 4000 USD (approx 5300 CAD) in my account.

Letter from my advisor stating that all expenses will be covered by my institution.

**Residence-related Documents:**

Residence permit for the country I'm doing my Phd in for the upcoming 3 years.

A copy of my previous residence permit in UK as a long-term student.

&amp;#x200B;

Apart from all the documents above, my passport contains:

2x short-term Schengen (European) visa.

A long term student visa for United Kingdom.

A long term researcher visa for South Korea.

&amp;#x200B;

... and after approximately three weeks, I was denied visa for short term stay in Canada. **But wait!** Canadian embassy provides a letter clarifying on which grounds they reject your visa. In that document it says (quoting directly):

&amp;#x200B;

*I am not satisfied that you will leave Canada at the end of your stay as a temporary resident based on:*

*1- family ties in Canada and in your country of residence.*

*2- the purpose of your visit.*

*3- your personal assets and financial status.*

*You are welcome to reapply. All new applications must be accompanied by a new processing fee.*

&amp;#x200B;

This answer is extremely annoying because:

1- I do not have a family in my country of residence or in Canada, I am doing my PhD in a different country than of my country of birth. They are basically saying anyone who is a lawful resident of any other country than their country of birth is not welcome to Canada because of lack of family ties in the country of residence.

2- I do not know how clear a purpose of visit must be. I am supposed to present in a conference, and return.

3- I am able to bring (approximately) two times monthly salary of a minimum wage Canadian worker for a trip that will last only for a week. In addition to that I am providing a letter from my institute stating that all my expenses will be covered.

&amp;#x200B;

After getting this outcome we tried to reach out to the embassy to ask what additional documents must we provide and received following email:

*Dear \_\_\_,*

*Thank you for contacting VFSCanadaVisa Information Service Centre.*

*Hello. This is canada visa application centre.*

*Today we recieved your e-mail from our service team. If you want to  re-apply TRV for \_\_\_, please make an application again and  prepare mentioned documents. You should pay for TRV(100 CAD) again.*

*If \_\_\_'s biometrics is still valid, you don't have to pay for additional biometrics fee (85 CAD) and give his biometrics again.*

&amp;#x200B;

It's a loophole. You cannot contact to anyone in the embassy because Canada manages the visa application via an Indian private company called VFS Global ([www.vfsglobal.com](https://www.vfsglobal.com)). They are basically sending out automated emails that says ""Well, if you want to reapply you can pay 100 CAD again"" to anyone who complains.

&amp;#x200B;

**TLDR:** PhD student who is author of a paper in N(eur)IPS is denied short-tem visa for Canada based on absurd reasons.

I remember reading the post a few weeks back about one of the flagship conferences being held in Africa in order to avoid visa issues and I honestly thought ""oh come on, visa issues can't be THAT bad"" and well, here I am. So, are there other people around the world that have or had similar problems with the visa? What is your solution? I am only one year into my PhD and I am quite worried about this, should I just avoid sending papers to any conference that is held in Canada (and USA?)?

&amp;#x200B;",34,1,False,self,,,,,
1649,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,12,a1dex9,m.hindustantimes.com,Amazon launches Machine Learning-based platform for healthcare space,https://www.reddit.com/r/MachineLearning/comments/a1dex9/amazon_launches_machine_learningbased_platform/,sunps,1543460655,,0,1,False,default,,,,,
1650,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,13,a1dy1x,self.MachineLearning,[D] Question: Whats the cheapest way to extract VGG16 image features?,https://www.reddit.com/r/MachineLearning/comments/a1dy1x/d_question_whats_the_cheapest_way_to_extract/,Loggerny,1543464865,"I have a web application that needs to generate image similarity scores from newly uploaded files. Currently, I have a home GPU machine with a script [^1] that extracts VGG16 based features. I then pickle the results and use the aggregated embeddings for k-means clustering/similarity checking.

Im wondering if I need to manage a GPU based machine (I would do this in some PAAS) and build out a web service that does this, or if there are applications that do this for a low cost/free.

I want to take an uploaded image-&gt;get extracted features-&gt;store results in a DB/elastic search.

[1]: https://github.com/rememberlenny/graffiti-net/blob/master/scripts/extract_features.py",2,1,False,self,,,,,
1651,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,13,a1e01m,medium.com,Author name disambiguation problem,https://www.reddit.com/r/MachineLearning/comments/a1e01m/author_name_disambiguation_problem/,yo__on,1543465324,,0,1,False,default,,,,,
1652,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,13,a1e1nd,arxiv.org,Understanding the impact of entropy in policy learning,https://www.reddit.com/r/MachineLearning/comments/a1e1nd/understanding_the_impact_of_entropy_in_policy/,AlexanderYau,1543465691,,0,1,False,default,,,,,
1653,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,13,a1e5ew,self.MachineLearning,Author name disambiguation problem,https://www.reddit.com/r/MachineLearning/comments/a1e5ew/author_name_disambiguation_problem/,yo__on,1543466546,[removed],0,1,False,https://b.thumbs.redditmedia.com/q5UyTz1dLputHYJHSCJWJfHuEiFFTueSFjDDShtXrHY.jpg,,,,,
1654,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,13,a1e5io,self.MachineLearning,Item to Item Recommendation System without Rankings,https://www.reddit.com/r/MachineLearning/comments/a1e5io/item_to_item_recommendation_system_without/,gsundeep31,1543466571,"Hi,

I am currently trying to build a item to item recommendations system without ranking, can you tell me how to go about it ?

In my scenario I have a list of products in shopping carts. Based on the frequency of products taken together, I need to suggest the products like association if product A is purchased product B is likely to be purchased.

Thanks in advance!!

",0,1,False,self,,,,,
1655,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,14,a1ej5t,youtube.com,chin chin rhombus cutting machine /chin chin cutting machine with good ...,https://www.reddit.com/r/MachineLearning/comments/a1ej5t/chin_chin_rhombus_cutting_machine_chin_chin/,fryingmachine,1543469762,,1,1,False,default,,,,,
1656,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,14,a1enc9,self.MachineLearning,[Question] Item to Item Recommendation System without Rankings,https://www.reddit.com/r/MachineLearning/comments/a1enc9/question_item_to_item_recommendation_system/,gsundeep31,1543470813,[removed],0,1,False,self,,,,,
1657,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,15,a1eykr,self.MachineLearning,Mobile app Development Company,https://www.reddit.com/r/MachineLearning/comments/a1eykr/mobile_app_development_company/,EndiveHMIS,1543473709,[removed],0,1,False,self,,,,,
1658,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,15,a1eyqm,self.MachineLearning,[D] What prevents a VAE from cheating on the decoder distribution and likelihood?,https://www.reddit.com/r/MachineLearning/comments/a1eyqm/d_what_prevents_a_vae_from_cheating_on_the/,readinginthewild,1543473750,"The VAE decoder generates the mean of the distribution p(x|z), and it could adjust the variance as well (though I think the variance is typically fixed).

What forces this distribution to be valid, i.e. integrate to one?

I.e. if the training data consists of 10 items, it seems the decoder could shift the mean of p(x|z) to sit on top of each of these, giving more than 1/10th probability to each example, so the total ""probability"" is more than one and likelihood is very high.

&amp;#x200B;",6,1,False,self,,,,,
1659,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,16,a1fdah,arxiv.org,[R] Experience Replay for Continual Learning,https://www.reddit.com/r/MachineLearning/comments/a1fdah/r_experience_replay_for_continual_learning/,HigherTopoi,1543477871,,0,1,False,default,,,,,
1660,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,16,a1feo6,blog.floydhub.com,[P] Generating Classical Music with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/a1feo6/p_generating_classical_music_with_neural_networks/,kunalag129,1543478305,,0,1,False,https://b.thumbs.redditmedia.com/O9J9CGvlPkY2x-Uf6fRFPQRYF1GENFhmFwSs5ZZJaKY.jpg,,,,,
1661,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,17,a1fhj0,self.MachineLearning,Sources for learning Scipy.Stats module,https://www.reddit.com/r/MachineLearning/comments/a1fhj0/sources_for_learning_scipystats_module/,KarthikMgk,1543479159,[removed],0,1,False,self,,,,,
1662,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,18,a1fr2p,self.MachineLearning,[D] An overview of deep learning tools,https://www.reddit.com/r/MachineLearning/comments/a1fr2p/d_an_overview_of_deep_learning_tools/,timonbimon,1543482306,"We recently tried to gather a list of useful deep learning tools.  
Here's the accompanying blog post: [https://medium.com/luminovo/the-deep-learning-toolset-an-overview-b71756016c06](https://medium.com/luminovo/the-deep-learning-toolset-an-overview-b71756016c06)  


Do you know of any good ones we are missing?",7,1,False,self,,,,,
1663,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,18,a1fx4g,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a1fx4g/global_machine_learning_market_size_outlook/,Dhanujay,1543484178,[removed],0,1,False,self,,,,,
1664,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,18,a1fx67,arxiv.org,Experience Replay for Continual Learning [PDF] : A DeepMind Publication,https://www.reddit.com/r/MachineLearning/comments/a1fx67/experience_replay_for_continual_learning_pdf_a/,suj1th,1543484191,,0,1,False,default,,,,,
1665,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,19,a1g2og,self.MachineLearning,How would you prove that a linear deep network can learn single labelled points sampled from a circle?,https://www.reddit.com/r/MachineLearning/comments/a1g2og/how_would_you_prove_that_a_linear_deep_network/,AlexManto,1543485866,[removed],0,1,False,self,,,,,
1666,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,19,a1g7di,self.MachineLearning,Validate the trend/shape of a curve,https://www.reddit.com/r/MachineLearning/comments/a1g7di/validate_the_trendshape_of_a_curve/,timmyTiz,1543487258,"Hey folks,

I have a huge set of data curves (several thousands of points per curve over time). According to few subjective, rather non-quantitive rules, some of them are fine, others are not. You probably see where this is going. I want to take a full curve as a feature and label them as 'ok' and 'not ok' and train my data set in order to predict in the future whether the curve can be validated or not. 

What is the proper ansatz here? I think a binary classification fits quite well here, however most of the time I find examples where the features are not a set of points but single quantities like mean or width. Due to the wiggly trends of the curves, I would like to avoid to fit a function to the curves, as this would require a large set of parameters and the error would be quite large.

Looking forwards for help! :- )

P.S. I tried DTW which is nice to have and under investigation, however, I am looking for ML options, too.",0,1,False,self,,,,,
1667,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,19,a1g7la,arxiv.org,Implementation of Robust Face Recognition System Using Live Video Feed Based on CNN,https://www.reddit.com/r/MachineLearning/comments/a1g7la/implementation_of_robust_face_recognition_system/,mhachem-reddit,1543487338,,0,1,False,default,,,,,
1668,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,19,a1g9er,arxiv.org,[R] Keep Drawing It: Iterative language-based image generation and editing,https://www.reddit.com/r/MachineLearning/comments/a1g9er/r_keep_drawing_it_iterative_languagebased_image/,dldoesnotwork,1543487903,,1,1,False,default,,,,,
1669,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,19,a1g9sx,dslcheatsheet.com,The Data Science Leader Cheatsheet,https://www.reddit.com/r/MachineLearning/comments/a1g9sx/the_data_science_leader_cheatsheet/,kcahai,1543488028,,0,1,False,default,,,,,
1670,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,19,a1gc9f,pythonlinks.info,"""Deep Learning with Python, TensorFlow, and Keras tutorial is the best of 31 TensorFlow videos organized by Category. Beginners should then watch ""Get started with TensorFlow's High-Level APIs"" Both videos include a timeline, so you can skip to the part you want to watch.",https://www.reddit.com/r/MachineLearning/comments/a1gc9f/deep_learning_with_python_tensorflow_and_keras/,PythonLinksDotInfo,1543488770,,0,1,False,default,,,,,
1671,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,20,a1getg,pythonlinks.info,"""Deep Learning with Python, TensorFlow, and Keras tutorial"" (For Jupyter Notebooks) is the best of 31 TensorFlow videos organized by category. PyCharm developers should watch "" Get started with TensorFlow's High-Level APIs "" Both include a timeline, so that you can skip to the part you want.",https://www.reddit.com/r/MachineLearning/comments/a1getg/deep_learning_with_python_tensorflow_and_keras/,PythonLinksDotInfo,1543489505,,0,1,False,default,,,,,
1672,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,20,a1gg1a,deepsense.ai,[P] Driverless car or autonomous driving? Tackling the challenges of autonomous vehicles,https://www.reddit.com/r/MachineLearning/comments/a1gg1a/p_driverless_car_or_autonomous_driving_tackling/,AnnaKow,1543489854,,0,1,False,https://b.thumbs.redditmedia.com/WXOzH8sSkPyT_bS2eR4OZ5hIg5_w_rBv8JpRkJQ1zEA.jpg,,,,,
1673,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,20,a1gg8b,self.MachineLearning,Parking lot space detection in C++ and OpenCV,https://www.reddit.com/r/MachineLearning/comments/a1gg8b/parking_lot_space_detection_in_c_and_opencv/,dascar5,1543489907,[removed],0,1,False,self,,,,,
1674,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,20,a1gh6d,intelligencenode.com,"How to Classify, Match Products With Machine Learning",https://www.reddit.com/r/MachineLearning/comments/a1gh6d/how_to_classify_match_products_with_machine/,Fridajon,1543490185,,0,1,False,default,,,,,
1675,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,21,a1gwlh,self.MachineLearning,The Next Generation Cognitive Security Operations Center: Network Flow Forensics Using Cybersecurity Intelligence,https://www.reddit.com/r/MachineLearning/comments/a1gwlh/the_next_generation_cognitive_security_operations/,wessax,1543494388,"The Next Generation Cognitive Security Operations Center: Network Flow Forensics Using Cybersecurity Intelligence
",0,1,False,self,,,,,
1676,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,21,a1gyye,mdpi.com,The Next Generation Cognitive Security Operations Center: Network Flow Forensics Using Cybersecurity Intelligence,https://www.reddit.com/r/MachineLearning/comments/a1gyye/the_next_generation_cognitive_security_operations/,wessax,1543494991,,1,1,False,default,,,,,
1677,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,22,a1h5cm,self.MachineLearning,[D] What papers to read for NeurIPS?,https://www.reddit.com/r/MachineLearning/comments/a1h5cm/d_what_papers_to_read_for_neurips/,pastaking,1543496587,"There are so many papers &amp; posters &amp; demos, what is your strategy on choosing and consuming the most relevant papers before the conference?",14,1,False,self,,,,,
1678,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,22,a1hban,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Nov. 29, 2018",https://www.reddit.com/r/MachineLearning/comments/a1hban/n_weekly_machine_learning_opensource_roundup_nov/,stkim1,1543497969,,0,1,False,default,,,,,
1679,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,22,a1hdhy,self.MachineLearning,Should I learn the proofs of the theorems in Statistics and Linear Algebra for a good theoretical understanding in ML?,https://www.reddit.com/r/MachineLearning/comments/a1hdhy/should_i_learn_the_proofs_of_the_theorems_in/,Hot_Ices,1543498467,[removed],0,1,False,self,,,,,
1680,MachineLearning,t5_2r3gv,2018-11-29,2018,11,29,22,a1hhz3,self.MachineLearning,[D] Tesla autopilot vision system,https://www.reddit.com/r/MachineLearning/comments/a1hhz3/d_tesla_autopilot_vision_system/,Kenny_smash,1543499527,"Hello everyone, I recently watched a tesla video from the perspective of the autopilot system and found the road detection and segmentation impressive, can someone explain to me examples of how it is done? There is a open segmentation algorithm that is close to this? Recently I experimented with lane detection on OpenCV, but the results were very poor.

&amp;#x200B;

[https://www.youtube.com/watch?v=\_1MHGUC\_BzQ](https://www.youtube.com/watch?v=_1MHGUC_BzQ)",32,1,False,self,,,,,
1681,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,0,a1i6i3,self.MachineLearning,How to make efficient use of time while attending NeurIPS conference?,https://www.reddit.com/r/MachineLearning/comments/a1i6i3/how_to_make_efficient_use_of_time_while_attending/,pikachuchameleon,1543504580,[removed],0,1,False,self,,,,,
1682,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,0,a1i8xq,self.MachineLearning,Here is how I organize my experiments,https://www.reddit.com/r/MachineLearning/comments/a1i8xq/here_is_how_i_organize_my_experiments/,basjj,1543505050,[removed],0,1,False,self,,,,,
1683,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,0,a1iesh,self.MachineLearning,"Are Sigmoid and exp-based activation more ""something"" than a well crafted Polynomial ?",https://www.reddit.com/r/MachineLearning/comments/a1iesh/are_sigmoid_and_expbased_activation_more/,ML_me_a_sheep,1543506167,[removed],0,1,False,https://b.thumbs.redditmedia.com/aC18ws_XYfhlBN0RZ-64dlDisEnsWRiWbwGZ8kM-3mE.jpg,,,,,
1684,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,1,a1inaf,self.tensorflow,Caveat Emptor: Broken TensorFlow &amp; Keras Integration (both Eager and Graph),https://www.reddit.com/r/MachineLearning/comments/a1inaf/caveat_emptor_broken_tensorflow_keras_integration/,Mr_Ubik,1543507703,,0,1,False,default,,,,,
1685,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,1,a1iq2g,heartbeat.fritz.ai,[P] Creating a 17 KB style transfer model with layer pruning and quantization,https://www.reddit.com/r/MachineLearning/comments/a1iq2g/p_creating_a_17_kb_style_transfer_model_with/,jamesonatfritz,1543508216,,0,1,False,default,,,,,
1686,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,1,a1iyni,self.MachineLearning,"[P] Creating an extremely tiny, 17 kB style transfer model with just 11,868 weights.",https://www.reddit.com/r/MachineLearning/comments/a1iyni/p_creating_an_extremely_tiny_17_kb_style_transfer/,jamesonatfritz,1543509774,"Hey everyone!

I've read a few papers discussing over-parameterization in neural networks and  I decided to give myself the challenge of creating tiny versions of neural networks that still perform well. The first task I chose was artistic style transfer because ""accuracy"" is mostly perceptual. I expected I'd be able to shrink the network a bit, but was shocked at how much I could remove.

It turns out, you can take a style transfer network with 1.7M parameters, remove all but 11,868 of them, and still produce great looking stylized images. The final network after quantization weighs just 17 kB, great for mobile apps.

Has anyone else had success with methods like this?

Write up: [https://heartbeat.fritz.ai/creating-a-17kb-style-transfer-model-with-layer-pruning-and-quantization-864d7cc53693](https://heartbeat.fritz.ai/creating-a-17kb-style-transfer-model-with-layer-pruning-and-quantization-864d7cc53693)

Code: [https://github.com/fritzlabs/fritz-style-transfer](https://github.com/fritzlabs/fritz-style-transfer)",42,1,False,self,,,,,
1687,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,1,a1j3tp,self.datascience,Thampi: A Serverless Machine Learning Prediction System on AWS Lambda.,https://www.reddit.com/r/MachineLearning/comments/a1j3tp/thampi_a_serverless_machine_learning_prediction/,rajiv_abraham,1543510730,,0,1,False,default,,,,,
1688,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,2,a1j4up,self.MachineLearning,Geoff Hinton's course on Coursera appears to be removed or broken.,https://www.reddit.com/r/MachineLearning/comments/a1j4up/geoff_hintons_course_on_coursera_appears_to_be/,panties_in_my_ass,1543510910,[removed],0,1,False,self,,,,,
1689,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,2,a1jbp9,self.MachineLearning,Hackathon: Detect Malicious intent with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a1jbp9/hackathon_detect_malicious_intent_with_machine/,hkr_mag,1543512108,"Try your skill in a machine learning online hackathon. Join the competition on Kaggle to develop models that identify injections among neutral input vectors. Wallarm reference implementation ([https://github.com/wallarm/wallnet/tree/master](https://github.com/wallarm/wallnet/tree/master)) is based on TensorFlow if you want to review it. Join the competition &amp; win over $2K in prizes. On-line; open until 12/12.

&amp;#x200B;

More details: [https://lab.wallarm.com/wallarm-new-open-source-module-and-kaggle-hackathon-8ce0824a967e](https://lab.wallarm.com/wallarm-new-open-source-module-and-kaggle-hackathon-8ce0824a967e)",0,1,False,self,,,,,
1690,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,2,a1jh28,reddit.com,[R] The Importance of Sampling in Meta-Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/a1jh28/r_the_importance_of_sampling_in_metareinforcement/,tigerneil,1543513094,,0,1,False,default,,,,,
1691,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,2,a1ji5v,self.MachineLearning,[P] Text Predictor - Generating Rap Lyrics with Recurrent Neural Networks (LSTMs),https://www.reddit.com/r/MachineLearning/comments/a1ji5v/p_text_predictor_generating_rap_lyrics_with/,g_surma,1543513299,"&amp;#x200B;

https://i.redd.it/xsrvtunv3b121.png

Character-level **RNN** (Recurrent Neural Net) **LSTM** (Long Short-Term Memory) implemented in Python/TensorFlow in order to generate text based on a given dataset.

[github](https://github.com/gsurma/text_predictor)

[medium](https://towardsdatascience.com/text-predictor-generating-rap-lyrics-with-recurrent-neural-networks-lstms-c3a1acbbda79)",0,1,False,https://b.thumbs.redditmedia.com/YR8vaGGiUVjB8Zkhyo2CIUQqFENUBkJSOtBpjQMMQig.jpg,,,,,
1692,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,2,a1jk90,self.MachineLearning,[D] How is the Machine Learning culture at your work place ?,https://www.reddit.com/r/MachineLearning/comments/a1jk90/d_how_is_the_machine_learning_culture_at_your/,mundada,1543513668,"I just want to understand how does you ML team approaches a problem ? Like you do guys have a pipeline in place for everything from data cleaning to evaluation metrics and all or How do you handle the research side of ML or how and when do you decide go further than using available techniques or libraries ?

Sole purpose of asking this question is: As of now we work in a very unstructured manner, just wanted to suggest some structure at our work place. 

&amp;#x200B;

Any advice would be very much appreciated.",9,1,False,self,,,,,
1693,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,2,a1jld3,i.redd.it,Gerenciamento de Projetos Industriais IV,https://www.reddit.com/r/MachineLearning/comments/a1jld3/gerenciamento_de_projetos_industriais_iv/,JamurGerloff,1543513873,,0,1,False,https://b.thumbs.redditmedia.com/jjokbO2yVLyKPhroXjSoxEFrIOeHGjgGDlAtfH-Itag.jpg,,,,,
1694,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,3,a1jp7g,self.tensorflow,Caveat Emptor: Broken TensorFlow &amp; Keras Integration (both Eager and Graph),https://www.reddit.com/r/MachineLearning/comments/a1jp7g/caveat_emptor_broken_tensorflow_keras_integration/,pgaleone,1543514577,,0,1,False,https://b.thumbs.redditmedia.com/-aPZ6atDnMIWZPoxQ0GrZH5HgEGRkGEnPya8zSh_RdQ.jpg,,,,,
1695,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,3,a1jwy2,self.MachineLearning,Restricted Boltzman Machine Question,https://www.reddit.com/r/MachineLearning/comments/a1jwy2/restricted_boltzman_machine_question/,perfecthundred,1543515974,[removed],0,1,False,self,,,,,
1696,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,3,a1k5e5,self.MachineLearning,Python vs R: 4 Implementations of Same Machine Learning Technique,https://www.reddit.com/r/MachineLearning/comments/a1k5e5/python_vs_r_4_implementations_of_same_machine/,andrea_manero,1543517461,[removed],0,1,False,self,,,,,
1697,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,3,a1k78l,self.MachineLearning,"Amazon releases DeepRacer for pre-order: a fully autonomous 1/18th scale race car driven by reinforcement learning, 3D racing simulator, and global racing league.",https://www.reddit.com/r/MachineLearning/comments/a1k78l/amazon_releases_deepracer_for_preorder_a_fully/,downtownslim,1543517801,[removed],0,1,False,self,,,,,
1698,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,3,a1k7kd,self.MachineLearning,"[R] Amazon releases DeepRacer for pre-order: a fully autonomous 1/18th scale race car driven by reinforcement learning, 3D racing simulator, and global racing league.",https://www.reddit.com/r/MachineLearning/comments/a1k7kd/r_amazon_releases_deepracer_for_preorder_a_fully/,downtownslim,1543517862,"AWS DeepRacer is a 1/18th scale race car which gives you an interesting and fun way to get started with reinforcement learning (RL). RL is an advanced machine learning (ML) technique which takes a very different approach to training models than other machine learning methods. Its super power is that it learns very complex behaviors without requiring any labeled training data, and can make short term decisions while optimizing for a longer term goal.

https://aws.amazon.com/deepracer/",12,1,False,self,,,,,
1699,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,4,a1k94x,arxiv.org,[R] Generalizing semi-supervised generative adversarial networks to regression,https://www.reddit.com/r/MachineLearning/comments/a1k94x/r_generalizing_semisupervised_generative/,i-like-big-gans,1543518127,,0,1,False,default,,,,,
1700,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,4,a1k9b7,arxiv.org,[R] Semi-supervised learning with Bidirectional GANs,https://www.reddit.com/r/MachineLearning/comments/a1k9b7/r_semisupervised_learning_with_bidirectional_gans/,i-like-big-gans,1543518158,,0,1,False,default,,,,,
1701,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,4,a1kk2k,medium.com,[N] Michael I. Jordan Interview: Clarity of Thought on AI,https://www.reddit.com/r/MachineLearning/comments/a1kk2k/n_michael_i_jordan_interview_clarity_of_thought/,gwen0927,1543520123,,0,1,False,https://a.thumbs.redditmedia.com/4_j2U7C87j3_313nJt8C3a-e1bld8q14ph-yCfkwqL4.jpg,,,,,
1702,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,4,a1krm8,self.MachineLearning,Best place to start learning,https://www.reddit.com/r/MachineLearning/comments/a1krm8/best_place_to_start_learning/,filisterr,1543521464,[removed],0,1,False,self,,,,,
1703,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,5,a1kzt1,self.MachineLearning,On which projects you guys are working now a days ? I am working on Intelligent Tourist information System,https://www.reddit.com/r/MachineLearning/comments/a1kzt1/on_which_projects_you_guys_are_working_now_a_days/,shoaiberozgaar,1543522867,[removed],0,1,False,self,,,,,
1704,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,5,a1l07k,self.MachineLearning,The Next Generation Cognitive Security Operations Center: Network Flow Forensics Using Cybersecurity Intelligence,https://www.reddit.com/r/MachineLearning/comments/a1l07k/the_next_generation_cognitive_security_operations/,wessax,1543522940,[removed],0,1,False,self,,,,,
1705,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,5,a1l0mf,self.MachineLearning,Question on LSTM Keras (Python) --&gt; Predict the future,https://www.reddit.com/r/MachineLearning/comments/a1l0mf/question_on_lstm_keras_python_predict_the_future/,emimesa2000,1543523005,"Good afternoon.

I do want to apologize for posting here, as I know some reddit communities may be angered by newbie questions so please  understand I wouldn't have posted here if I was able to find any other viable option. 

So  I recently started ML, and met Keras LSTM, which in fact builds a model that tries to predict future values and tests its lost with the actual values. Yet, I am trying to find a way to for Keras LSTM to simply predict future values without the need to ""test"". My goal is to achieve something similar to the [predict\_keras\_lstm\_future()](https://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html) function but on Python. 

However, neither do I understand Python well enough to be able to translate the code, nor have found any similar documentation on the internet which implements a similar concept. So I was wondering if any of you who have more experience could guide me on the right way to either documentation or code, which allows me to implement a similar function on my own ML work. Thank you.

&amp;#x200B;

Sincerely,

EmiMesa2000",0,1,False,self,,,,,
1706,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,5,a1l44n,v.redd.it,Reproduce experiments from DeepMind and OpenAI in Unity with my new tutorial 'Getting Started with Marathon Environments' to get you going with Reinforcement Learning and Continuous Control benchmarks in #Unity3d + ML-Agents. All open source!,https://www.reddit.com/r/MachineLearning/comments/a1l44n/reproduce_experiments_from_deepmind_and_openai_in/,soho-joe,1543523630,,1,1,False,https://b.thumbs.redditmedia.com/e7n5kAxePrY42g6xjWMPsm2AlbbBBwdYbTdnzEl31Yo.jpg,,,,,
1707,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,5,a1l6uc,self.MachineLearning,[D] Has your company or your team had any bad experiences implementing or working with AI? What went wrong?,https://www.reddit.com/r/MachineLearning/comments/a1l6uc/d_has_your_company_or_your_team_had_any_bad/,keinemaschine,1543524103,"I constantly read about all the wonders and promise of AI. There's plenty of success stories, but I'd be interested in hearing any stories you may have of situations where AI didn't deliver or wasn't the right solution. What could you have done different?",8,1,False,self,,,,,
1708,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,5,a1l7xu,medium.com,[N] AWS Announces AI Chips &amp; 13 New ML Features; Consolidating Its Cloud Dominance,https://www.reddit.com/r/MachineLearning/comments/a1l7xu/n_aws_announces_ai_chips_13_new_ml_features/,gwen0927,1543524296,,0,1,False,default,,,,,
1709,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,6,a1ljz3,self.MachineLearning,How far along are the million Image Recognition and Data Companies?,https://www.reddit.com/r/MachineLearning/comments/a1ljz3/how_far_along_are_the_million_image_recognition/,menodialogues,1543526455,"This is a fucked up post, because it doesn't make sense, but bear with me please. I understand there is a huge data set you need to build around this, but I'm willing to invest the time, money and resources if it's slightly possible within 10-12 yrs.

  

 I think the online advertising model needs to shift award from paid ads and more towards natural social influence. What i mean is, I want people to see a video of someone on SNAP, YouTube, IG, or whatever, see a product they like, and click it to learn more or just buy it. 

  

Currently a YouTuber might falsely endorse a product because they're getting paid, what I want is for a YouTuber to live their normal life, and through their life people learn and discover products that they can click and buy. Or, you post on IG, someone sees your adidas tee, thinks its cool, clicks and buys it...you get paid for enticing a sale. you didn't put the tee on to get the sale, you put it on to be you, someone saw it, liked it, bought it...and you get paid for influencing that (in a perfect world). 

  

crazy, i know. but is it impossible? I like what Groq is building, and am wondering how viable it is to build a neural network that captures the various scenarios that arise when you wear a t-shirt, and how effectively can you scrape and run a comparison analysis. 

  

fundamentally, you need to pick 1 product category, and tackle it well. like self-driving cars have done. 

users would need to upload a 360 video either wearing the product, or of the product itself?...

what is possible here?

what are the biggest road blocks?",0,1,False,self,,,,,
1710,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,6,a1lwty,dev.to,CPU v/s GPU v/s TPU comparisons!,https://www.reddit.com/r/MachineLearning/comments/a1lwty/cpu_vs_gpu_vs_tpu_comparisons/,neomatrix369,1543528744,,0,1,False,default,,,,,
1711,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,7,a1m3d3,self.MachineLearning,[R] Coordinate Ascent Variational Inference by Probabilistic Programming,https://www.reddit.com/r/MachineLearning/comments/a1m3d3/r_coordinate_ascent_variational_inference_by/,jerrylessthanthree,1543529908,I was wondering if there was any probabilistic programming package or language that could automatically do coordinate ascent mean field variational inference. I've implemented variational inference in Edward2 but it uses gradient descent methods. ,10,1,False,self,,,,,
1712,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,7,a1m4u0,medium.com,[P] Radiology and Deep Learning - Detecting pneumonia opacities from chest X-Ray images using deep learning,https://www.reddit.com/r/MachineLearning/comments/a1m4u0/p_radiology_and_deep_learning_detecting_pneumonia/,yukw777,1543530184,,0,1,False,default,,,,,
1713,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,8,a1mkds,self.MachineLearning,[D] Creating a dataset for learning,https://www.reddit.com/r/MachineLearning/comments/a1mkds/d_creating_a_dataset_for_learning/,thetechkid,1543533104,"I'm having an issue at the moment with a model I am trying to work on for image classification. I believe part of the issue may be the way that I am structuring the data for training and testing. I do not have a predefined dataset to pull data and labels from so I am essentially creating two directories and sub folders within those for the images for each of the categories. Now this may be a simple issue I'm just missing, or my approach is wrong(because I can't seem to get any better than 20% accuracy) so I want to ask about the proper way to do this. I am using keras, and the GPU version of TF at the moment and any help in the right direction would be amazing. ",6,1,False,self,,,,,
1714,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,8,a1mp6r,allclearweather.com,"Hurricane Florence as measured by smartphones with barometers, demonstrating that phones can be useful weather sensors. New ML techniques have been developed by researchers to improve data quality and remove sensor bias.",https://www.reddit.com/r/MachineLearning/comments/a1mp6r/hurricane_florence_as_measured_by_smartphones/,cryptoz,1543534031,,1,1,False,https://a.thumbs.redditmedia.com/85VbgOQ4LNJwdOWU4LJ8qrDigTOwPNphhqp8YEsy0q0.jpg,,,,,
1715,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,8,a1mqaa,youtu.be,These people never existed!,https://www.reddit.com/r/MachineLearning/comments/a1mqaa/these_people_never_existed/,cmillionaire9,1543534232,,0,1,False,https://b.thumbs.redditmedia.com/jCmf4rGs6nUWF9M83zlCrlqhfbOj-1TzLXs3IzhfYKs.jpg,,,,,
1716,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,8,a1mrp8,allclearweather.com,"[P] Hurricane Florence as measured by phones with barometers, demonstrating that phones can be useful weather sensors. ML algorithms have been developed by researchers to remove sensor bias.",https://www.reddit.com/r/MachineLearning/comments/a1mrp8/p_hurricane_florence_as_measured_by_phones_with/,cryptoz,1543534518,,1,1,False,https://a.thumbs.redditmedia.com/85VbgOQ4LNJwdOWU4LJ8qrDigTOwPNphhqp8YEsy0q0.jpg,,,,,
1717,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,8,a1mtu1,self.MachineLearning,Best way get a 2Dconv to produce sharper images.,https://www.reddit.com/r/MachineLearning/comments/a1mtu1/best_way_get_a_2dconv_to_produce_sharper_images/,Maaarteh,1543534929,[removed],0,1,False,self,,,,,
1718,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,9,a1n3mb,self.MachineLearning,Machine learning controversy?,https://www.reddit.com/r/MachineLearning/comments/a1n3mb/machine_learning_controversy/,hunger2000,1543536851,[removed],0,1,False,self,,,,,
1719,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,9,a1nbgf,self.MachineLearning,[D] Meta-learning setup for seq2seq model,https://www.reddit.com/r/MachineLearning/comments/a1nbgf/d_metalearning_setup_for_seq2seq_model/,svpadd2,1543538394,"I want to use MAML/Reptile on a NLP model for Named Entity recognition, how would I split the dataset by class? For example traditionally with meta learning on image datasets for training you would sample define each ""task"" as maybe one or two classes of images out and then show the core model k items from each class and then perform the meta update. However with a NER you cannot split up your dataset of sentences that easily into mini tasks because each contains a mixture of classes. What would be the best way to divide the dataset to get the same benefit of rapid adaptation images.",3,1,False,self,,,,,
1720,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,10,a1npvv,gengo.ai,How machine learning solutions are transforming financial services: Interview with Data Scientist Dr. Iain Brown,https://www.reddit.com/r/MachineLearning/comments/a1npvv/how_machine_learning_solutions_are_transforming/,reimmoriks,1543541294,,0,1,False,default,,,,,
1721,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,10,a1nqcg,self.MachineLearning,Open-source clinical encounter dataset,https://www.reddit.com/r/MachineLearning/comments/a1nqcg/opensource_clinical_encounter_dataset/,novel_counsel,1543541381,"Hi - I'm a Stanford grad considering creating an open-source dataset of doctor's visits. Would you be willing to record one of your visits to a doctor on a cellphone app and submit the audio and EHR to an open-source dataset paper? Dataset would be scrubbed of any PHI (names, addresses, etc.) during the upload process and the dataset paper would be targeted towards the applied track at one of the top ML conferences. Any comments or features you'd like to see in the project?",0,1,False,self,,,,,
1722,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,10,a1ns1z,self.MachineLearning,Resources for SBL?,https://www.reddit.com/r/MachineLearning/comments/a1ns1z/resources_for_sbl/,SeriousTicket,1543541725,[removed],0,1,False,self,,,,,
1723,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,11,a1ob9i,self.MachineLearning,CNN RNN Integration for image captioning,https://www.reddit.com/r/MachineLearning/comments/a1ob9i/cnn_rnn_integration_for_image_captioning/,collided_equations,1543545752,[removed],0,1,False,self,,,,,
1724,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,12,a1ojlf,self.MachineLearning,Would a machine learning internship be good for my resume?,https://www.reddit.com/r/MachineLearning/comments/a1ojlf/would_a_machine_learning_internship_be_good_for/,ManlyBran,1543547489,[removed],0,1,False,self,,,,,
1725,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,12,a1oq92,self.MachineLearning,[Project] Multi-GPU Bert,https://www.reddit.com/r/MachineLearning/comments/a1oq92/project_multigpu_bert/,JayYip,1543548959,"A by-product of what I'm currently working for(Chinese NER). Realised the original tensorflow code of BERT does not support multiple GPU. So I wrote this. Two things will be implemented in near future: multiple GPU support Weight decay adam(as in bert), pre-train task. 

This project is still in very early stage, bugs are expected. ",0,1,False,self,,,,,
1726,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,13,a1owop,github.com,[P] Course material for Machine Learning course @ UW-Madison on GitHub,https://www.reddit.com/r/MachineLearning/comments/a1owop/p_course_material_for_machine_learning_course/,seraschka,1543550420,,0,1,False,default,,,,,
1727,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,13,a1oxl6,i.redd.it,Python programmer,https://www.reddit.com/r/MachineLearning/comments/a1oxl6/python_programmer/,ngaurav7,1543550608,,0,1,False,https://b.thumbs.redditmedia.com/WJqXeZrVWj7Zyb8is14UxHUU1pIX-ZKZbMvML-hCcZE.jpg,,,,,
1728,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,14,a1pm1t,self.MachineLearning,Machine Learning with Python |Python Training,https://www.reddit.com/r/MachineLearning/comments/a1pm1t/machine_learning_with_python_python_training/,Blendtechinfo,1543556221," [Software Professionals](https://blendinfotech.com/Python/India) such as Programmers, [Web Developers](https://blendinfotech.com/Python/India), [ETL  Developers](https://blendinfotech.com/Python/India), [Analytics Professionals](https://blendinfotech.com/Python/India), [Automation Engineers](https://blendinfotech.com/Python/India),[Hadoop  Programmers](https://blendinfotech.com/Python/India), [Project Managers](https://blendinfotech.com/Python/India), and even beginners must[learn Python](https://blendinfotech.com/Python/India) to  compete well and to ensure their success in the IT sector. 

&amp;#x200B;

https://i.redd.it/yvz3gawlne121.png",0,1,False,self,,,,,
1729,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,14,a1pq2q,self.MachineLearning,pytorch tutorial,https://www.reddit.com/r/MachineLearning/comments/a1pq2q/pytorch_tutorial/,sam_081,1543557175,"Hi visitor,

I have tried some Pytorch tutorials from internet. And would like to share what I learned through this kernel.

If you also want to learn pytorch. Please do visit this link.

[Pytorch Learning Tutorial](https://www.kaggle.com/dungeonmaster081/pytorch-learning/)

Thanks",0,1,False,self,,,,,
1730,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,15,a1pva0,self.MachineLearning,[D] Research on the theoretical limits of reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/a1pva0/d_research_on_the_theoretical_limits_of/,Flag_Red,1543558416,"There is a theoretical upper limit on how much we can infer from a limited amount of data. AFAIK, we have some models (Bayesian models) that allow us to reach these limits, and any better performance would require more information. Bayesian models are, however, computationally intractable for anything remotely complex (Atari environments, for instance).

Is there any research on how close current RL algorithms are to the theoretical upper bound of what we can infer from a given environment?",2,1,False,self,,,,,
1731,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,15,a1pyw6,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/a1pyw6/global_machine_learning_market_size_outlook/,CHITTI_123,1543559351,[removed],0,1,False,self,,,,,
1732,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,15,a1q2w7,bizacuity.com,Small and Medium Enterprises (SMEs) rush for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/a1q2w7/small_and_medium_enterprises_smes_rush_for/,amardeepeng99,1543560412,,1,1,False,default,,,,,
1733,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,16,a1qa4v,self.MachineLearning,[D] Examples of adversarial examples being generated against networks performing something other than image classification?,https://www.reddit.com/r/MachineLearning/comments/a1qa4v/d_examples_of_adversarial_examples_being/,carlskevin,1543562444,"Do any such examples exist? I've attempted to dig around but have come up empty-handed, yet nothing intuitively indicates to me that such a thing wouldn't be possible.",0,1,False,self,,,,,
1734,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,16,a1qca9,indiumsoftware.com,Top AI and ML Trends in 2018!,https://www.reddit.com/r/MachineLearning/comments/a1qca9/top_ai_and_ml_trends_in_2018/,indiumsoftware18,1543563078,,0,1,False,default,,,,,
1735,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,16,a1qf7j,medium.com,Multitask learning: teach your AI more to make it better (with code in Keras),https://www.reddit.com/r/MachineLearning/comments/a1qf7j/multitask_learning_teach_your_ai_more_to_make_it/,rachnogstyle,1543563984,,0,1,False,default,,,,,
1736,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,17,a1qi8u,self.MachineLearning,[D] Bayesian models in production,https://www.reddit.com/r/MachineLearning/comments/a1qi8u/d_bayesian_models_in_production/,PipeTrance,1543564865,"I've recently become interested in Bayesian methods, mostly because of their claims to be more robust and small data friendly. However, although there is multitude of research projects and toy examples, I haven't really seen any practical applications of anything more nuanced then naive Bayes.

Is there a reason for that or was I simply looking in the wrong places?",62,1,False,self,,,,,
1737,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,17,a1qm6h,self.MachineLearning,Are YOU the Outlier?,https://www.reddit.com/r/MachineLearning/comments/a1qm6h/are_you_the_outlier/,andrea_manero,1543566099,[removed],0,1,False,self,,,,,
1738,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,17,a1qrod,blog.floydhub.com,Reading Minds with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/a1qrod/reading_minds_with_deep_learning/,pirate7777777,1543567802,,0,1,False,default,,,,,
1739,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,17,a1qt5e,self.MachineLearning,[P] Conditional DCGAN mystery collapses,https://www.reddit.com/r/MachineLearning/comments/a1qt5e/p_conditional_dcgan_mystery_collapses/,hpobaschnig,1543568257,"Hey! I forked [DCGAN](https://github.com/carpedm20/DCGAN-tensorflow) and modified it to generate conditional celebA faces. Currently [this is my implementation](https://github.com/hpobaschnig/CDCGAN), but my network collapses every time. 

&amp;#x200B;

[Only the first image is generated real data](https://i.redd.it/udavc9lbnf121.png)

&amp;#x200B;

Is there anybody who could help me here?",7,1,False,https://b.thumbs.redditmedia.com/v9tdbat0Qlve_Pnu0OlzzLjeVXoN9EQDYCEDXXv1XFk.jpg,,,,,
1740,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,18,a1qx5w,self.MachineLearning,Where to start healthy in machine learning?,https://www.reddit.com/r/MachineLearning/comments/a1qx5w/where_to_start_healthy_in_machine_learning/,mhachem-reddit,1543569486,[removed],0,1,False,self,,,,,
1741,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,18,a1r4ru,learntek.org,Deep Learning and Neural Network,https://www.reddit.com/r/MachineLearning/comments/a1r4ru/deep_learning_and_neural_network/,Anu2008,1543571887,,0,1,False,default,,,,,
1742,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,19,a1r65l,self.MachineLearning,[P] wanting to create a image identification neural network,https://www.reddit.com/r/MachineLearning/comments/a1r65l/p_wanting_to_create_a_image_identification_neural/,yolosandwich,1543572284,"Hello everyone from r/MachineLearning. I am a student from Hong Kong and I'm 15, I have basic knowledge on python, as said in the title, I'm having interest in joining a joint school exhibition and I want to create a device to identify endangered species of fish,by using an image identification neural network, 

There is also a program held by the Hong Kong Ocean park and the WWF, so I think I can access to a large database of fish images.

So christmas is coming and I decided to devote most of the Holliday(2 weeks) to start learning machine learning and if possible, get this project going.

I want some advice to start this project, what should I learn and what do I need to know to get this project rolling, if there are any resources about image identification neural networks or just machine learning in general, please let me know

Thanks in advance!

",4,1,False,self,,,,,
1743,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,19,a1re8e,self.MachineLearning,[D] What's the difference between QA and MRC?,https://www.reddit.com/r/MachineLearning/comments/a1re8e/d_whats_the_difference_between_qa_and_mrc/,yanggs44,1543574734,"\* QA: Question and Answering

\* MRC: Machine Reading Comprehension

&amp;#x200B;

They look so similar. I would appreciate it if you guys let us know the difference in a technical view.",1,1,False,self,,,,,
1744,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,19,a1rfp9,self.MachineLearning,[D] So you want to be a Research Scientist (Google Brain),https://www.reddit.com/r/MachineLearning/comments/a1rfp9/d_so_you_want_to_be_a_research_scientist_google/,baylearn,1543575196,"While this [blog post](https://medium.com/@vanhoucke/so-you-want-to-be-a-research-scientist-363c075d3d4c) is not strictly about ML research, but I thought it is an interesting read because it is written by a senior research scientist at Google Brain. Probably reflects what Google thinks about the culture of their research divisions, what they think about engineering vs research, and who they may want to hire.

https://medium.com/@vanhoucke/so-you-want-to-be-a-research-scientist-363c075d3d4c
",42,1,False,self,,,,,
1745,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,20,a1rirn,towardsdatascience.com,Machine Learning is not just glorified Statistics,https://www.reddit.com/r/MachineLearning/comments/a1rirn/machine_learning_is_not_just_glorified_statistics/,xTWOz,1543576066,,0,1,False,https://a.thumbs.redditmedia.com/qlwO72UzxJXGI8Vxg8jeT3GYyJYqM5UeqdTppx3ZZ88.jpg,,,,,
1746,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,21,a1s7qo,self.MachineLearning,[D] The State of Deep Learning: H2 2018 Review,https://www.reddit.com/r/MachineLearning/comments/a1s7qo/d_the_state_of_deep_learning_h2_2018_review/,rosstaylor90,1543582786,"[https://medium.com/atlas-ml/state-of-deep-learning-h2-2018-review-cc3e490f1679](https://medium.com/atlas-ml/state-of-deep-learning-h2-2018-review-cc3e490f1679)

&amp;#x200B;

Hi all. Here's a quick analysis I did on ML papers that were published with code in the past 6 months. Hope this is helpful!",3,1,False,self,,,,,
1747,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,22,a1s8jq,self.MachineLearning,ML Books recommended for beginners.,https://www.reddit.com/r/MachineLearning/comments/a1s8jq/ml_books_recommended_for_beginners/,abhififa09,1543582960,,0,1,False,self,,,,,
1748,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,22,a1shcu,self.MachineLearning,[D] Sliding window/convolutional processing of a non space invariant image,https://www.reddit.com/r/MachineLearning/comments/a1shcu/d_sliding_windowconvolutional_processing_of_a_non/,paland3,1543585007,"Hi, I have a ranging device with the following output:

https://i.redd.it/mk1fc13iyg121.png

Where each cell is a ""pixel"". You can see that range resolution is homogeneous across the frame, but the azimuth is not: the same direction covers a much bigger region further away. It is not shown on the image, but azimuth borders are also not spaced equally, beams on the side are wider.   
That is, my input data is very much not spatial invariant. However, I would like to process it in a sliding window approach for baseline, and later on, using a convolutional network. Both would need an invariant data in my opinion. How would you solve this contradiction? Could you point me to technics or similar problems? My ideas so far:

1. Add an additional 2 layers containing the pixel's position in the image, e.g. range 5, direction 3. This doesn't help on linear classifiers like svm though.
2. Resample the data in a homogeneous, orthogonal way. I have two problems with this:
   1. it is quite hard to do so efficiently
   2. Will introduce ""information"" to the data which is not here, e.g. finer resolution further away.

I am grateful to your inputs.

&amp;#x200B;

&amp;#x200B;",4,1,False,self,,,,,
1749,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,22,a1slkx,self.MachineLearning,Looking for an Art dataset,https://www.reddit.com/r/MachineLearning/comments/a1slkx/looking_for_an_art_dataset/,Ac3zzzz,1543585992,[removed],0,1,False,self,,,,,
1750,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,23,a1st0t,self.MachineLearning,RNNs for trading,https://www.reddit.com/r/MachineLearning/comments/a1st0t/rnns_for_trading/,errminator,1543587580,[removed],0,1,False,self,,,,,
1751,MachineLearning,t5_2r3gv,2018-11-30,2018,11,30,23,a1t427,self.MachineLearning,What are some of the must attenf talks/sessions/workshops at NeurIPS (NIPS) 2018?,https://www.reddit.com/r/MachineLearning/comments/a1t427/what_are_some_of_the_must_attenf/,kingadenorf,1543589858,[removed],0,1,False,self,,,,,
