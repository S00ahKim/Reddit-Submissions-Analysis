,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2014-10-1,2014,10,1,14,2hyt5i,self.MachineLearning,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 3,https://www.reddit.com/r/MachineLearning/comments/2hyt5i/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1412142095,"Greetings!

Update time!

As in my previous update, I am still having problems with the function approximation not being exact enough. I found some papers describing how using function approximation with Q learning can lead to over-estimation of the Q values. Indeed, I noticed that when I changed the reward function, it would typically either not decay from the highest Q values or do so very slowly. To remedy this, I tried force-decaying Q values over time upon state visitation, but this didn't really help.

So after that, I attempted getting a more exact function approximation. I tried using radial basis function networks instead of the plain old feed-forward neural networks. Results here are promising, I immediately noticed a reduction in temporal difference error values (which would indicate it predicting properly more often). As far as I know, radial basis function networks often have their prototypes/centers pre-trained using K-means clustering. However, since I am developing an entirely online algorithm, I had to adapt it to work continuously. To do so, I implemented a sort of competitive unsupervised learning strategy that mirrors an online form of K-means clustering. I tested it on some benchmark supervised learning tasks, and I got much faster learning rates than with feed-forward neural networks (I was able to consistently learn an XOR in 30 updates).

I also began taking a deeper look into policy gradient methods, since these are supposedly less affected by inaccuracies in function approximation. I may end up using a natural policy gradient method.

On the HTM front, I only did some tuning of parameters. I am actually quite pleasantly surprised by how well it works, with all the scrutiny it is under I expected worse. It produces stable patterns (same state gives same pattern, with some generalization allowed) for my function approximators. We will soon see how well it scales up though ;)

Here is a video of it predicting the motion of a box. It is a bit old, I made it when I first got it working, but I thought I should share it anyways. The left side is input, the right side is output. When the left is blue, it is the ""true"" input, when it is red, it is the distributed representation.

Link: [https://www.youtube.com/watch?v=J0s2NXuw9H8&amp;feature=youtu.be](https://www.youtube.com/watch?v=J0s2NXuw9H8&amp;feature=youtu.be)

That's it for this update, progress is being made, slowly but surely!

Edit: Here is a link to the source code. The repository containts many agents, the one these posts are focusing on is called HTMRL and can be found in the directory of the same name! [https://github.com/222464/AILib](https://github.com/222464/AILib)",7,27,False,self,,,,,
1,MachineLearning,t5_2r3gv,2014-10-1,2014,10,1,16,2hz1av,cs.cmu.edu,"Bow: A Toolkit for Statistical Language Modeling, Text Retrieval, Classification and Clustering",https://www.reddit.com/r/MachineLearning/comments/2hz1av/bow_a_toolkit_for_statistical_language_modeling/,TongZhang,1412150188,,2,8,False,default,,,,,
2,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,0,2hzuj5,self.MachineLearning,How do I encode day of the week as a predictor?,https://www.reddit.com/r/MachineLearning/comments/2hzuj5/how_do_i_encode_day_of_the_week_as_a_predictor/,downtownslim,1412175641,"I have predictors in the form of days of the week (Sun, Mon, Tues, Wed, Thu, Fri, Sat). 

I can't use 1-of-C encoding because it does not reflect the adjacency property correctly. Example: Predicting Tuesday when it was actually Wednesday shouldn't incur as large a cost as predicting Saturday when it was actually Wednesday.


I can't use ordinal regression because that doesn't reflect the modulo nature of my categories. Example: Predicting Sunday when it was actually Saturday would incur a huge cost even though they're adjacent.

In order words my data has a relative ordering, but there's no absolute ordering.",10,5,False,self,,,,,
3,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,1,2i01xq,self.MachineLearning,Machine Learning Blog List,https://www.reddit.com/r/MachineLearning/comments/2i01xq/machine_learning_blog_list/,dmztheone,1412179606,,7,63,False,self,,,,,
4,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,2,2i0eig,datasciencecentral.com,The end of the Data Scientist Bubble,https://www.reddit.com/r/MachineLearning/comments/2i0eig/the_end_of_the_data_scientist_bubble/,vincentg64,1412186051,,8,0,False,http://b.thumbs.redditmedia.com/jEyJ_Y1GL_zezNoOfhIz5u-Yb8bKKmaYvbz5LIl5ChU.jpg,,,,,
5,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,3,2i0fnu,self.MachineLearning,How many years of programming experience should someone have before getting into Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/2i0fnu/how_many_years_of_programming_experience_should/,brouwjon,1412186638,"I'm just starting out with programming and computer science; Deep Learning seems really interesting to me. Roughly speaking, how long should I expect to do ""normal"" programming and CS before delving into Deep Learning? ",12,0,False,self,,,,,
6,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,3,2i0j24,startup.ml,if you want a job in data science,https://www.reddit.com/r/MachineLearning/comments/2i0j24/if_you_want_a_job_in_data_science/,[deleted],1412188259,,0,0,False,default,,,,,
7,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,4,2i0rja,reddit.com,/r/compsci weekly questions thread on machine learning,https://www.reddit.com/r/MachineLearning/comments/2i0rja/rcompsci_weekly_questions_thread_on_machine/,cypherx,1412192549,,0,4,False,default,,,,,
8,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,5,2i0v1z,nips.cc,NIPS 2014 accepted papers,https://www.reddit.com/r/MachineLearning/comments/2i0v1z/nips_2014_accepted_papers/,mlalma,1412194387,,16,13,False,default,,,,,
9,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,7,2i1b0z,self.MachineLearning,Why do some insurance companies hire actuaries AND data scientists and what exactly is the separation of responsibilities? (X-POST Actuary),https://www.reddit.com/r/MachineLearning/comments/2i1b0z/why_do_some_insurance_companies_hire_actuaries/,actuarially_sound,1412202886,"I started thinking about this after I saw that [Liberty Mutual Insurance company](http://www.libertymutual.com/), hosted a ""data science"" competition on Kaggle ( [Liberty Mutual Group - Fire Peril Loss Cost
Predict expected fire losses for insurance policies](http://www.kaggle.com/c/liberty-mutual-fire-peril)). As I looked around past competitions I noticed that a there were a few competitions that focused exclusively on things that actuaries traditionally do. They all fell into the specialties of [p&amp;c](http://www.kaggle.com/c/allstate-purchase-prediction-challenge), [consulting](http://www.kaggle.com/c/deloitte-churn-prediction), and [health](http://www.heritagehealthprize.com/c/hhp).

When I looked around the careers sections of some of these companies and other insurance companies, I noticed that some of them hired data scientists and some of them didn't. Of the ones that hired data scientists some companies placed them in the Actuarial/Research department and some place them in the software development/IT department.

So I'm wondering, why would an insurance company hire actuaries and data scientists? What's the rational behind putting them in different departments? What exactly is the separation of responsibilities like?

I also asked this question on [r/actuary](http://www.reddit.com/r/actuary/comments/2hxdiy/why_do_some_insurance_companies_hire_actuaries/).",8,3,False,self,,,,,
10,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,8,2i1iod,self.MachineLearning,Can someone explain the difference between 'cognitive computing' vs. 'deep learning' vs. 'machine learning'?,https://www.reddit.com/r/MachineLearning/comments/2i1iod/can_someone_explain_the_difference_between/,bourbank,1412207269,"Newbie to the topic, trying learn and explore the space a bit.  Would love any recommendations on reading lists or videos to watch as well.  Thanks much!",2,0,False,self,,,,,
11,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,10,2i1t1p,stackoverflow.com,Using machine learning to predict the collapse &amp; stabilization of complex systems? (r/CompSci cross post),https://www.reddit.com/r/MachineLearning/comments/2i1t1p/using_machine_learning_to_predict_the_collapse/,FerretDude,1412213391,,1,7,False,http://a.thumbs.redditmedia.com/aXYrRkeCvV2XLxkjJ0IlzqDQKfJmSxIod_GIKvDSNl0.jpg,,,,,
12,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,11,2i1wg3,self.MachineLearning,A simple model for the Kaggle Bike Sharing competition.,https://www.reddit.com/r/MachineLearning/comments/2i1wg3/a_simple_model_for_the_kaggle_bike_sharing/,nameBrandon,1412215476,"I made this a text post so I could give a bit of info before the link, rather than people just blindly clicking on it. 

This was my first attempt at a Kaggle competition (previously I followed a tutorial on the Titanic set). I learned a lot from that tutorial, so I wanted to share this post on a basic attempt for the bike share competition so that others might learn a bit, as I did.

I'm just getting started in my masters program in Analytics, and haven't had any machine learning classes yet, so apologies in advance if I did or said something stupid.

I've since created a much more accurate model, but I think the model covered in the post is a decent starting point for other newbies, and I'd love to hear what others thought of it! The post uses R for the model, my python isn't quite up to snuff yet.

http://brandonharris.io/kaggle-bike-sharing/
",2,16,False,self,,,,,
13,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,18,2i2sa1,chamunda.in,Roll Compactor,https://www.reddit.com/r/MachineLearning/comments/2i2sa1/roll_compactor/,rohanagraval09,1412242816,,0,1,False,default,,,,,
14,MachineLearning,t5_2r3gv,2014-10-2,2014,10,2,23,2i3e3u,self.MachineLearning,Prediction problem,https://www.reddit.com/r/MachineLearning/comments/2i3e3u/prediction_problem/,subszero,1412261040,"[Prediction Problem](http://i.imgur.com/7GfSW0A.png)

As can be seen from the image, I have a social graph. Each friend visits a set of places. I want to predict the places the user will visit based on his friends visit.

I have pre calculated the similarity scores between a user and all his friends as shown (s1, s2, s3). What would be possible techniques to use this available information to predict the places the user would visit ?


Thanks !",2,0,False,self,,,,,
15,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,2,2i3wkv,self.MachineLearning,How do *you* read/handle a large (several GB+) datasets? What about huge (1TB+) ones?,https://www.reddit.com/r/MachineLearning/comments/2i3wkv/how_do_you_readhandle_a_large_several_gb_datasets/,[deleted],1412270930,"Hey all, I'm interested in learning what folks in /r/machinelearning use to handle large datasets.

Initially I was just going to ask a basic question as to what tools and methods I should use for tackling such problems, but I thought it would be better for everyone in general if people shared their workflows. That way it is more of a discussion and others can comment and recommend alternative methods.

So, how do *you* read / handle large datasets when working on your ML problems and research?",27,10,False,default,,,,,
16,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,2,2i3yvb,icml.cc,Machine Learning that matters [PDF],https://www.reddit.com/r/MachineLearning/comments/2i3yvb/machine_learning_that_matters_pdf/,RinzeWind,1412272105,,2,4,False,default,,,,,
17,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,3,2i43cx,markus.com,How to install Theano on Amazon EC2 GPU instances for deep learning,https://www.reddit.com/r/MachineLearning/comments/2i43cx/how_to_install_theano_on_amazon_ec2_gpu_instances/,holy_onasandwich,1412274432,,3,42,False,default,,,,,
18,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,6,2i4pyw,machinelearningmastery.com,Methodology and Mindset of a Kaggle Master,https://www.reddit.com/r/MachineLearning/comments/2i4pyw/methodology_and_mindset_of_a_kaggle_master/,jasonb,1412286220,,0,0,False,http://b.thumbs.redditmedia.com/KU14bzNz-QLx7rCV48kJXugJ_J5sprXAal3Qr90JUPY.jpg,,,,,
19,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,10,2i5flv,sebastianraschka.com,An Overview of General Performance Metrics of Binary Classifier Systems,https://www.reddit.com/r/MachineLearning/comments/2i5flv/an_overview_of_general_performance_metrics_of/,rasbt,1412301485,,0,3,False,default,,,,,
20,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,11,2i5kca,sebastianraschka.com,The Effect of Scaling and Mean Centering Prior to a Principal Component Analysis,https://www.reddit.com/r/MachineLearning/comments/2i5kca/the_effect_of_scaling_and_mean_centering_prior_to/,rasbt,1412304397,,1,1,False,default,,,,,
21,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,12,2i5pnz,glamour.com,ML Researcher Hanna Wallach Featured in Glamour,https://www.reddit.com/r/MachineLearning/comments/2i5pnz/ml_researcher_hanna_wallach_featured_in_glamour/,FixDeineKabel,1412307907,,5,0,False,http://b.thumbs.redditmedia.com/uOeZMnmjIpBKGYM7Nr9K10dCkq5Fcp14hjbGdNgFR7Q.jpg,,,,,
22,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,13,2i5u9w,discourse.net,[x-post from /r/robotics] 2015 We Robot Conference Call for Papers,https://www.reddit.com/r/MachineLearning/comments/2i5u9w/xpost_from_rrobotics_2015_we_robot_conference/,einthesuperdog,1412311301,,1,2,False,default,,,,,
23,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,15,2i60tf,datasciencecentral.com,Is data science bad at detecting bogus Amazon or Yelp reviews?,https://www.reddit.com/r/MachineLearning/comments/2i60tf/is_data_science_bad_at_detecting_bogus_amazon_or/,vincentg64,1412317015,,4,0,False,default,,,,,
24,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,17,2i68f0,self.MachineLearning,Tackling mathematics in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2i68f0/tackling_mathematics_in_machine_learning/,[deleted],1412325475,"Guyz, 
I recently started my master's course with Machine learning, but feel very scared with the amount of mathematics one need to know when solving problems or proofs. Last proofs I did were 5 years ago. I can understand the baye's theorem and basic linear algebra but the discussion/proof suddenly move to very difficult topics or theorems. TAs and Profs think we know all these and hence dont spend too much time on it. Because of it, I feel completely lost in the courses, it's almost one month now and I feel I know nothing compared to other student and really doubt my decision to take Machine Learning.

I a ready to do whatever it takes, I am currently studying Linear Algebra and stats in my spare time but the things that I am required to know, the proofs I am supposed to derive are just way above the introductory stuff. Any recommendations on what should I do to get better on the mathematics front?",17,22,False,self,,,,,
25,MachineLearning,t5_2r3gv,2014-10-3,2014,10,3,23,2i6wsv,blog.bigml.com,Introducing: Magic Data Goggles!,https://www.reddit.com/r/MachineLearning/comments/2i6wsv/introducing_magic_data_goggles/,czuriaga,1412347547,,0,1,False,http://b.thumbs.redditmedia.com/UYcR9r2REH-ixWTW17aoYdyPy-L7jN674Va6zPlpyJs.jpg,,,,,
26,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,0,2i70d4,online.liebertpub.com,A Data Scientist's Guide to Startup,https://www.reddit.com/r/MachineLearning/comments/2i70d4/a_data_scientists_guide_to_startup/,salilpn,1412349627,,0,1,False,default,,,,,
27,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,2,2i7coh,alyaabbott.wordpress.com,How to Ace a Data Science Interview,https://www.reddit.com/r/MachineLearning/comments/2i7coh/how_to_ace_a_data_science_interview/,alya_abbott,1412356428,,0,38,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
28,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,2,2i7eh3,nlpers.blogspot.com,Is machine learning more important than algorithms?,https://www.reddit.com/r/MachineLearning/comments/2i7eh3/is_machine_learning_more_important_than_algorithms/,mark-v,1412357388,,10,9,False,http://b.thumbs.redditmedia.com/EFdUSdQysh0mL1TEDy-w6JeBxUcuFp9imr83rfxn2Qk.jpg,,,,,
29,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,5,2i7vin,self.MachineLearning,"What is the difference between ""vanishing gradient"" ""exploding gradient"" ?",https://www.reddit.com/r/MachineLearning/comments/2i7vin/what_is_the_difference_between_vanishing_gradient/,rishok,1412366815,"Can someone explain the difference between ""vanishing gradient"" ""exploding gradient"" ? ",8,2,False,self,,,,,
30,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,6,2i84y1,nuit-blanche.blogspot.ch,"All the slides of MMDS 2014, Workshop on Algorithms for Modern Massive Data Sets",https://www.reddit.com/r/MachineLearning/comments/2i84y1/all_the_slides_of_mmds_2014_workshop_on/,compsens,1412372194,,0,5,False,http://b.thumbs.redditmedia.com/AzsqtV5eOr8i7B6j9wgk7cQJgjq31rVafzMhtUD6cSE.jpg,,,,,
31,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,6,2i85y6,self.MachineLearning,[Journal Club] week 40/2014 winner / week 41/2014 poll thread,https://www.reddit.com/r/MachineLearning/comments/2i85y6/journal_club_week_402014_winner_week_412014_poll/,BeatLeJuce,1412372796,"**Current Paper**

The paper to be read this week is  [Bayesian Optimization in High Dimensions via Random Embeddings](http://www.cs.ubc.ca/~hutter/papers/13-IJCAI-BO-highdim.pdf), proposed by /u/Hydreigon92. A discussion thread will be posted this Friday to discuss the paper.

Description:  In his interview with Team Leada, [Peter Norvig said he believes this paper to one that will influence cutting-edge research in machine learning](http://blog.teamleada.com/2014/08/ask-peter-norvig/).

It's a short paper (only 6 pages), and it provides a good overview of the research in random embeddings done over the last few years.



**New Poll**

Please post suggestions for next week's paper in this thread, and use your upvotes to vote for the papers you like. It is of course allowed to post papers that have been proposed in previous weeks. The paper with the most upvotes by Friday, 20:00 CEST will be chosen for the upcoming week.",2,11,False,self,,,,,
32,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,8,2i8i0e,self.MachineLearning,My Attempt at Outperforming Deepmind's Atari Results: UPDATE 4,https://www.reddit.com/r/MachineLearning/comments/2i8i0e/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1412380587,"Hello again!

As I stated in my previous posts, the precision of the function approximation remains a problem. However, I have found that using SARSA instead of Q learning (on-policy instead of off-policy) mitigates the problem a lot since it is no longer bootstrapping on the largest potential value (subject to overestimation) but rather the potential value of the selected action.

Due to this improvement, I decided to move back to feed-forward neural networks with RMS training. The RBF networks train much faster, but they seem to forget things faster (things not part of the replay chain). I assume this is due to the on-line supervised learning that occurs, which can destroy previously learned information quite easily in favor of better matching new information.

I now have HTMRL performing both pole balancing and the mountain car problem with very little training time. It figures both of them out in under a minute usually.

I started scaling up to the Arcade Learning Environment, and did some short test runs. I also did a lot of optimization, since I want it to run in real-time (Deepmind's algorithm was not in real-time as far as I know, and took more than just one standard desktop PC). I have not yet trained it enough to get decent results (5 minutes is not much), but I will probably try an overnight run soon.

For the continuous action edition of the algorithm, I experimented with free-energy based reinforcement learning. It learns a value function as usual, but it can easily derive a continuous policy from the value function. I started implementing replay updates for this as well.

Here is a short video of the system learning the mountain car problem: [https://www.youtube.com/watch?v=IMdgakMZxyE&amp;feature=youtu.be](https://www.youtube.com/watch?v=IMdgakMZxyE&amp;feature=youtu.be)

For those just seeing this for the first time, the source code for this is available here, under the directory ""htmrl"": [https://github.com/222464/AILib](https://github.com/222464/AILib)
It uses the CMake build system.

It's getting there!

See you next time!",6,8,False,self,,,,,
33,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,9,2i8n2k,homepages.inf.ed.ac.uk,Unsupervised Joke Generation.,https://www.reddit.com/r/MachineLearning/comments/2i8n2k/unsupervised_joke_generation/,conic_relief,1412384096,,4,24,False,default,,,,,
34,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,11,2i8uuy,pr.cs.cornell.edu,Deep Learning for Detecting Robotic Grasps,https://www.reddit.com/r/MachineLearning/comments/2i8uuy/deep_learning_for_detecting_robotic_grasps/,neuromorphics,1412389727,,0,7,False,http://b.thumbs.redditmedia.com/LGqCVDeMrJ9cSoZw67WxbjrYTLyogPdxKyOCNLB0ctw.jpg,,,,,
35,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,11,2i8w75,youtube.com,Great 76 minute video lecture on the basics of Random Forests from the University of British Columbia,https://www.reddit.com/r/MachineLearning/comments/2i8w75/great_76_minute_video_lecture_on_the_basics_of/,neelshiv,1412390771,,4,57,False,http://b.thumbs.redditmedia.com/KQseq39LhJN7tTf30-TLw937WCbwz5KdrM_rMeWOsWE.jpg,,,,,
36,MachineLearning,t5_2r3gv,2014-10-4,2014,10,4,14,2i96y8,self.MachineLearning,Can machine learning replace doctors one day?,https://www.reddit.com/r/MachineLearning/comments/2i96y8/can_machine_learning_replace_doctors_one_day/,data_wiz,1412399629,"I mean in terms of diagnosis, we would still need them for signing prescriptions and what not, surgeons for surgeries, the assuring physical presence of a man in white with a stethoscope for the cancer patient etc.

On the other hand, I can foresee an ensemble of models beating an ensemble of doctors in diagnostic prowess (speed, accuracy, consistency) hands down in the future. What do you think?",5,0,False,self,,,,,
37,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,0,2ia3xt,phys.org,Improvements to classical graph theory have potential to impact modern-day problem solving,https://www.reddit.com/r/MachineLearning/comments/2ia3xt/improvements_to_classical_graph_theory_have/,keghn,1412435793,,4,40,False,http://b.thumbs.redditmedia.com/TuNiU2jXKe5thZ-3e1vpiSukuFHf5K2JiRzOPz3Ezfw.jpg,,,,,
38,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,1,2ia9r3,startup.ml,From Electricity to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2ia9r3/from_electricity_to_machine_learning/,gwulfs,1412439702,,0,0,False,http://b.thumbs.redditmedia.com/Kw43L0_hz-HajvUnFXLLwcQnJx5vowdC6Eb50CFVjjo.jpg,,,,,
39,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,1,2iab66,self.MachineLearning,Which universities in the USA have people working on Deep learning?,https://www.reddit.com/r/MachineLearning/comments/2iab66/which_universities_in_the_usa_have_people_working/,YesIAmTheMorpheus,1412440637,"Could you please add academician-groups which are working on Deep learning in the USA?  
  

1. U of Maryland - College Park, Hal Daume

2. New York University  Yann Lecuns and Rob Fergus group

3. UC Berkeley  Bruno Olshausens group

4. University of Washington  Pedro Domingos group

5. University of California Merced  Miguel A. Carreira-Perpinans group

6. University of Michigan  Honglak Lees group

7. U. California Irvine  Pierre Baldis group


",11,7,False,self,,,,,
40,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,5,2iaz8u,arxiv.org,Do Deep Nets Really Need to be Deep? [pdf],https://www.reddit.com/r/MachineLearning/comments/2iaz8u/do_deep_nets_really_need_to_be_deep_pdf/,mikkom,1412455908,,0,13,False,default,,,,,
41,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,7,2ib6hy,self.MachineLearning,How to save and use a trained model in an external application?,https://www.reddit.com/r/MachineLearning/comments/2ib6hy/how_to_save_and_use_a_trained_model_in_an/,caedin8,1412460591,"I have been using WEKA to train a RandomForest model for a binary classification system. I've been using CV to evaluate my model and now I achieve 96% classification accuracy on our supervised data with good generalization.

I want to now save the model and use it to classify in a separate program, potentially as a plug in for Chrome. I am not tied to WEKA. Does any one have experience with this, or suggestions?

Thanks",9,0,False,self,,,,,
42,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,7,2ib71r,blog.iriomk.com,Impressions of a beginner starting with ML,https://www.reddit.com/r/MachineLearning/comments/2ib71r/impressions_of_a_beginner_starting_with_ml/,IrioMk,1412460964,,3,1,False,http://b.thumbs.redditmedia.com/sGFevTdH-cqwIPu3brviaUKCJ9GnVjadbi-2CEgMY4Y.jpg,,,,,
43,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,11,2ibr3e,sebastianraschka.com,Naive Bayes and Text Classification I - Introduction and Theory,https://www.reddit.com/r/MachineLearning/comments/2ibr3e/naive_bayes_and_text_classification_i/,rasbt,1412475264,,2,21,False,http://a.thumbs.redditmedia.com/7QFdN-KU8UX60hSDH8p5LKZ1c-a4VqE2j6BtRcBghY0.jpg,,,,,
44,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,15,2ic8t5,youtube.com,Deep Learning RNNaissance - Juergen Schmidhuber,https://www.reddit.com/r/MachineLearning/comments/2ic8t5/deep_learning_rnnaissance_juergen_schmidhuber/,alexmlamb,1412490169,,4,32,False,http://b.thumbs.redditmedia.com/HLObFd4BKy2NDkMSNH0nh2KsVO2JXRkZbnrgUlqUBtQ.jpg,,,,,
45,MachineLearning,t5_2r3gv,2014-10-5,2014,10,5,19,2icl9j,i.imgur.com,Animated Perceptron Learning Algorithm,https://www.reddit.com/r/MachineLearning/comments/2icl9j/animated_perceptron_learning_algorithm/,TangerineX,1412506648,,10,2,False,http://b.thumbs.redditmedia.com/87CT5AhRuaPFpUjANzIJVXG7g5Pd16lfOqolUMN_q0Q.jpg,,,,,
46,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,0,2id03c,self.MachineLearning,Are multivariate methods considered part of Machine learning?,https://www.reddit.com/r/MachineLearning/comments/2id03c/are_multivariate_methods_considered_part_of/,YesIAmTheMorpheus,1412521854,"Multivariate statistical methods like Canonical correlation analysis, Correspondence analysis?",6,3,False,self,,,,,
47,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,1,2id7rp,nzhiltsov.blogspot.ru,Machine Learning with Knowledge Graphs,https://www.reddit.com/r/MachineLearning/comments/2id7rp/machine_learning_with_knowledge_graphs/,nzhiltsov,1412527005,,0,1,False,http://b.thumbs.redditmedia.com/fSWa8pQpuwGNmMeUggF1HKNsAD00-83HT7s0kd8H3JA.jpg,,,,,
48,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,1,2id9h8,self.MachineLearning,SVD to fill in missing values of a matrix,https://www.reddit.com/r/MachineLearning/comments/2id9h8/svd_to_fill_in_missing_values_of_a_matrix/,subszero,1412528110,"Hi,

I currently have a user by user matrix, with each entry indicating the similarity between the corresponding users. This matrix is sparse, as entries are calculated only for friend pairs.

Now, I want to predict the possible similarity scores between non-friend pairs i.e fill the empty values in the matrix.

I read about [**latent factor models**](http://www.ideal.ece.utexas.edu/seminar/LatentFactorModels.pdf) and the use of SVD to minimise the squared reconstruction error.

But there are a few differences with my use case. I need to perform svd on a user-user matrix, not a user-item matrix. This would mean that I need my result to be of the form A= U U^T , where U is the latent factor matrix for a user. This does not seem to map to SVD, but to Cholesky decomposition. Using Cholesky decomposition does not let me choose the number of latent factors.

I am not sure if I am thinking in the right direction or if there is some fundamental flaw in my approach. Any kind of advice is appreciated.

Thanks.


Edit : Tested out matrix factorisation using the [code](http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/). Factorising my matrix A as UV^T yields much better results than UU^T.

I am still trying to understand what U and V could represent in my case (factorising a user-user matrix).",9,10,False,self,,,,,
49,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,4,2idnzf,machinelearningmastery.com,An Introduction to Feature Selection,https://www.reddit.com/r/MachineLearning/comments/2idnzf/an_introduction_to_feature_selection/,jasonb,1412536995,,0,13,False,http://b.thumbs.redditmedia.com/0e2I6SVp-nU3PdGAUW4g3mKxHQbG-Akn-JkBm6E7W-w.jpg,,,,,
50,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,5,2ids1h,github.com,"DXNN - ""Deus Ex"" Neural Networking",https://www.reddit.com/r/MachineLearning/comments/2ids1h/dxnn_deus_ex_neural_networking/,pseudogrammaton,1412539474,,1,2,False,http://b.thumbs.redditmedia.com/Q26M_klK2spuKPANYtADMOMY3yT4cR16BtNNKfYZcTs.jpg,,,,,
51,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,8,2iecbn,self.MachineLearning,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 5,https://www.reddit.com/r/MachineLearning/comments/2iecbn/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1412551797,"Hey everybody!

Another update!

The reinforcement learning portion is done, so now I turn my attention to the image recognition portion so that I may scale up better to the ALE. I originally tried to go immediately to the ALE, but it takes so long to find out whether or not what you did is working so instead I decided to stay with pole balancing. However, instead of feeding values such as position and velocities of the cart directly to the HTM bottom-most region, I am now using an encoding of the image of the experiment. This provides me with an intermediate step on the path to scaling up to the ALE.

At this point, I could dump the HTM stuff and just do ConvNets, but that feels like a cop out. I have found an interesting paper that compares CLA (HTM) to the state of the art, and they showed that it indeed outperforms things like convolutional neural networks (they used LeNet), mostly due to the time signal, on tasks where sequences of information are presented. I have such a task. So, I will continue working on my HTM implementation and improving it. I am currently in the process of reading that paper (available here: [http://bias.csr.unibo.it/maltoni/HTM_TR_v1.0.pdf](http://bias.csr.unibo.it/maltoni/HTM_TR_v1.0.pdf)) to see how they were able to adapt it for classification (from which I can adapt it to function approximation).

I am still not quite certain if I am encoding the greyscale images properly for the HTM, right now the regions are way too stable. The previously mentioned paper talks of feedback signals from higher regions in the hierarchy that allow for adaptation of lower levels with the context gathered from the higher levels. My current HTM model does not have this, so I am very interested in seeing how that affects things for reinforcement learning.

Right after this post, I will continue reading the paper, and start coding a new HTM model. Besides incorporating any improvements I can gather from the paper, I am moving everything to the GPU using OpenCL. I used to be mainly a graphics developer, so this shouldn't take too long to do :)

For those just seeing this for the first time, the source code for this is available here, under the directory ""htmrl"": [https://github.com/222464/AILib](https://github.com/222464/AILib) It uses the CMake build system.

That's it for this update, until next time!",4,9,False,self,,,,,
52,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,9,2iejpg,bias.csr.unibo.it,"""systematically compared with other pattern classification systems including Convolutional Network...In almost all our experiments HTM accuracy was better than other system tested and learning was also more efficient.""",https://www.reddit.com/r/MachineLearning/comments/2iejpg/systematically_compared_with_other_pattern/,rantana,1412556368,,24,9,False,default,,,,,
53,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,18,2ifnbd,stackoverflow.com,Weird results for collaborative filtering on movielens data set using Spark MLlib. What am I doing wrong?,https://www.reddit.com/r/MachineLearning/comments/2ifnbd/weird_results_for_collaborative_filtering_on/,[deleted],1412588599,,0,0,False,default,,,,,
54,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,23,2ig65z,self.MachineLearning,Best Workstation for deep learning (Pylearn2),https://www.reddit.com/r/MachineLearning/comments/2ig65z/best_workstation_for_deep_learning_pylearn2/,SnowRipple,1412604926,"Hi there!

   I am planning to assemble a high-end workstation that will be able to handle my deep learning (especially convnets with Pylearn2 and Theano) experiments.

   Is there is a danger that Theano may not cooperate too well with some GPUs?

   What would you propose to buy? I mean high end and expensive but not supercomputer, let's say up to $10 000 (I am a researcher and there is quite big budget to use).

Many Thanks,
P",11,11,False,self,,,,,
55,MachineLearning,t5_2r3gv,2014-10-6,2014,10,6,23,2ig66c,pyimagesearch.com,"My experience with the CUDA SDK, CUDAMat, and nolearn on my MacBook Pro...and how I obtained totally lackluster results.",https://www.reddit.com/r/MachineLearning/comments/2ig66c/my_experience_with_the_cuda_sdk_cudamat_and/,zionsrogue,1412604931,,10,10,False,http://b.thumbs.redditmedia.com/1fNj1_o0HJju_ssL1yUwCuHpJWaBaUC8StgOj7eESVM.jpg,,,,,
56,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,1,2igo3x,blog.tradeshift.com,The wondrous world of PDF font mapping,https://www.reddit.com/r/MachineLearning/comments/2igo3x/the_wondrous_world_of_pdf_font_mapping/,shift_happenz,1412614730,,0,0,False,http://b.thumbs.redditmedia.com/Z6yNrI9AvvisUJyaRVw_1jv3tF7AWWmhha9QOuva0II.jpg,,,,,
57,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,3,2igxsm,self.MachineLearning,Is there anything else to computer-program search other than genetic programming?,https://www.reddit.com/r/MachineLearning/comments/2igxsm/is_there_anything_else_to_computerprogram_search/,eluspac,1412619432,"I'm aware of some limited success of genetic programming applied to computer program search. Other than it, is there any alternative approach to the problem of finding computer programs automatically? ",9,2,False,self,,,,,
58,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,4,2ih3g6,gotomeeting.com,Webinar on using Nvidia GPUs to train deep neural nets.,https://www.reddit.com/r/MachineLearning/comments/2ih3g6/webinar_on_using_nvidia_gpus_to_train_deep_neural/,vinayan3,1412622130,,2,4,False,default,,,,,
59,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,4,2ih3qj,self.MachineLearning,Why does machine learning interest YOU ?,https://www.reddit.com/r/MachineLearning/comments/2ih3qj/why_does_machine_learning_interest_you/,[deleted],1412622263,I wanted to hear you guys' take on this,2,0,False,default,,,,,
60,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,4,2ih4gb,self.MachineLearning,Help understanding Neural Networks please?,https://www.reddit.com/r/MachineLearning/comments/2ih4gb/help_understanding_neural_networks_please/,FestivalPillow,1412622601,"I posted this [here](http://www.reddit.com/r/explainlikeimfive/comments/2igq32/eli5_artificial_neural_networks_how_do_they_work/), but thought you guys might be a good place to ask too!

I get the general theory of NNs - they're a rough model of our brain and work by passing input through the ""input layer"", to the hidden layers, computations are performed and one or more output neurons give us a result....

What I don't get is actually how it works in practice.

If someone could explain this to me really simply,or by using some kind of an example, I'd really appreciate it.

Or, imagine I have a game, and want to track a pattern in the game... ie. its a shooter game and I want to see if theres a correlation between different weapons and different levels and their success, could I use a neural network to recognise a pattern in this?

Thanks everyone!

EDIT: I got some really great responses, and thank you everyone who took the time to actually write out these detailed, informative responses. I really appreciate your help.
",4,0,False,self,,,,,
61,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,5,2ihii0,pypi.python.org,Most appropriately named Vowpal Wabbit Python wrapper,https://www.reddit.com/r/MachineLearning/comments/2ihii0/most_appropriately_named_vowpal_wabbit_python/,xamdam,1412628960,,7,22,False,default,,,,,
62,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,8,2ii4kn,prezi.com,Predictive analysis of Pima Indians Diabetes using weka,https://www.reddit.com/r/MachineLearning/comments/2ii4kn/predictive_analysis_of_pima_indians_diabetes/,mascot6699,1412639730,,0,0,False,http://b.thumbs.redditmedia.com/RflbrXHPTu04NPPdGHUSZlmwJ4tKR6ru8miEFkFKcWE.jpg,,,,,
63,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,14,2ij3fg,vencoavendingmachines.com,Vending machines for sales,https://www.reddit.com/r/MachineLearning/comments/2ij3fg/vending_machines_for_sales/,johnkrist,1412659121,,0,1,False,default,,,,,
64,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,14,2ij4nk,self.MachineLearning,What are good resources to refresh myself on statistics and machine learning?,https://www.reddit.com/r/MachineLearning/comments/2ij4nk/what_are_good_resources_to_refresh_myself_on/,[deleted],1412660003,"I'm looking for resources to learn the basics of statistics and machine learning.  

I did my undergraduate in math. My focus was almost entirely pure math, proof and logic courses, mostly Algebra and Topology. My employment prospects are grim.
I'm considering graduate studies in machine learning, but my understanding of statistics is frankly embarrassing. I took the basic stats class but my retention was poor. I feel like I need to brush up before I start reaching out to schools.

Does anyone know good resources for learning and relearning stats for someone who is already mathematically inclined? 
I'm partial to videos and especially to concise texts.",5,9,False,self,,,,,
65,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,17,2iji8z,nlpers.blogspot.de,"""Machine learning is the new algorithms""",https://www.reddit.com/r/MachineLearning/comments/2iji8z/machine_learning_is_the_new_algorithms/,vrld,1412671995,,6,13,False,http://a.thumbs.redditmedia.com/5UJARzCsAlgM_QTL_DrGYc3snraaR_0eZY3crdVyOH4.jpg,,,,,
66,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,21,2ijvfu,self.MachineLearning,Deep learning for object recognition,https://www.reddit.com/r/MachineLearning/comments/2ijvfu/deep_learning_for_object_recognition/,pnambiar,1412683637,"Hello,

I am trying to explore the possiblity of using Deep learning algorithms for object recognition. I have the object already segmented and I would like to find out the best possible framework available for object reconition/image classification. After doing some research some of the options I have come across are-caffe, Torch, Theano, Cuda. 

Any leads, suggestions will be highly appreciated.",19,2,False,self,,,,,
67,MachineLearning,t5_2r3gv,2014-10-7,2014,10,7,21,2ijzaz,blog.thegrandlocus.com,Massive scientific plagiarism detected by statistical analysis,https://www.reddit.com/r/MachineLearning/comments/2ijzaz/massive_scientific_plagiarism_detected_by/,gui11aume,1412686292,,17,85,False,http://a.thumbs.redditmedia.com/AljQPoceRQLMzzvjCsZ-iKtwKpNu-vmPoxDAjs_MHn4.jpg,,,,,
68,MachineLearning,t5_2r3gv,2014-10-8,2014,10,8,6,2ilh3b,self.MachineLearning,How to create RGBD datasets?,https://www.reddit.com/r/MachineLearning/comments/2ilh3b/how_to_create_rgbd_datasets/,[deleted],1412715673,"I have the use cases and the sensor locked down, How do I go about creating the data-set? What are the things I should keep in mind? Are there any literature or articles on this topic? What sort of tools are available or what are the tools that I would need to develop? 
Thanks in advance for your replies. 
",2,0,False,self,,,,,
69,MachineLearning,t5_2r3gv,2014-10-8,2014,10,8,12,2imkiq,harvardsportsanalysis.org,Modeling NFL Overtime as a Markov Chain,https://www.reddit.com/r/MachineLearning/comments/2imkiq/modeling_nfl_overtime_as_a_markov_chain/,[deleted],1412738272,,0,1,False,http://a.thumbs.redditmedia.com/Dgw7-9D1V0Qj0fuqlMEnHiNa19iNhsyjxF6UV_1W850.jpg,,,,,
70,MachineLearning,t5_2r3gv,2014-10-8,2014,10,8,18,2in8xr,youtube.com,Coursera - Social Network Analysis with Lada Adamic,https://www.reddit.com/r/MachineLearning/comments/2in8xr/coursera_social_network_analysis_with_lada_adamic/,Pvanimpe,1412759810,,3,18,False,http://b.thumbs.redditmedia.com/j60-QLP5MLiQ_whpRBcJOpPuJ1dIP6S8x963cRtdYEM.jpg,,,,,
71,MachineLearning,t5_2r3gv,2014-10-8,2014,10,8,21,2inm7d,am.co.za,CNC Machinery,https://www.reddit.com/r/MachineLearning/comments/2inm7d/cnc_machinery/,gsstechnology,1412772655,,0,1,False,default,,,,,
72,MachineLearning,t5_2r3gv,2014-10-8,2014,10,8,23,2inwyk,self.MachineLearning,Audio processing by neural network,https://www.reddit.com/r/MachineLearning/comments/2inwyk/audio_processing_by_neural_network/,ml_epsilon,1412779536,"Hi! First of all, a warning. I am trying to understand this stuff for fun and without sufficient background to understand *serious* textbooks, so stupid questions ahead. I basically know what I know by reading and trying to understand random threads on this subreddit.

Say I have an audio effect and I don't know how it works. It is a function of the current input sample and possibly older input samples (a simple example would be a soft clipper, which maps any input sample to the range [-1, 1] and depends only on the current input sample, and a complex one may be ""playing a sound trough my speakers and recording it with a cheap microphone from under the desk"", which will depend on a lot of old input samples).

I was thinking of generating a very long audio signal, which contains a lot of different audio material (music, drums, vocals, noise, sinusoids, and so on), and passing it trough the effect to produce the output signal. Then I would make a neural network learn this function.

***Questions*** in no particular order.


1. Am I just wasting time or does it make some kind of sense to try this? Since I have never really done any machine learning it would be nice to know in advance if it's not working because it cannot work (easily). :p
2. What learning algorithm should I use? I think I should use SGD by choosing random points in the audio signals at each iteration.
3. What kind of neural network should I use? I am confused a lot on this one. I don't think I understand exactly how a convolutional network works, but If I'm not completely wrong they do not have anything to do with what I'm trying to do and I should use a fully connected MLP.
4. Is this feasible for more complex effects, with hundreds of inputs? Is it feasible in real time?",20,4,False,self,,,,,
73,MachineLearning,t5_2r3gv,2014-10-8,2014,10,8,23,2iny73,on-demand-gtc.gputechconf.com,Unreasonable effectiveness of Deep Learning talk by Yann LeCunn,https://www.reddit.com/r/MachineLearning/comments/2iny73/unreasonable_effectiveness_of_deep_learning_talk/,xamdam,1412780260,,0,2,False,http://b.thumbs.redditmedia.com/AI_tZ1imiOS-32azMf_1Ix6s39wSAMe8lU6z4rnJHsg.jpg,,,,,
74,MachineLearning,t5_2r3gv,2014-10-9,2014,10,9,3,2iom4f,videolectures.net,KDD 2014 Videos up on Videolectures.net!,https://www.reddit.com/r/MachineLearning/comments/2iom4f/kdd_2014_videos_up_on_videolecturesnet/,xamdam,1412792813,,6,33,False,http://b.thumbs.redditmedia.com/RyKSLHlB1IEXFdICbQ8yR_hW33ZEHRknH-AKBSZvA_A.jpg,,,,,
75,MachineLearning,t5_2r3gv,2014-10-9,2014,10,9,5,2ip0uk,self.MachineLearning,Generative models for generating 3d models?,https://www.reddit.com/r/MachineLearning/comments/2ip0uk/generative_models_for_generating_3d_models/,alexgmcm,1412800298,"I was reading about procedural modeling and it made me wonder if anyone has tried getting the data of loads of different 3D models and training a generative classifier on them - so classify all the 3D models that are of chairs, all that are of swords etc.

Then by generating synthetic data one can generate more 3D models of objects without the need to create them by hand.

This could help a lot for CGI/video games etc. where typically one wants a large number of slightly different models to give more variety but artist time is very expensive.

I searched but couldn't find anything, but this seems way too simple to not have been done so I can only presume it is a bad idea for some reason?",5,7,False,self,,,,,
76,MachineLearning,t5_2r3gv,2014-10-9,2014,10,9,11,2iq3c4,self.MachineLearning,"Recursive Neural Networks for Image Segmentation Code? (Socher, Lin, Manning, Ng)",https://www.reddit.com/r/MachineLearning/comments/2iq3c4/recursive_neural_networks_for_image_segmentation/,in_the_fresh,1412822613,"Hi guys, I really want to use this in some current research: http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf

it doesn't look like there's any code that will do it for you off the shelf though! (Socher's matlab code, available on the webpage, doesn't seem to be easy to apply to new images.) Any suggestions? Thanks!",0,0,False,self,,,,,
77,MachineLearning,t5_2r3gv,2014-10-9,2014,10,9,13,2iqcob,motherboard.vice.com,Why 'Frankenstein' Robots Could Be the Future of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/2iqcob/why_frankenstein_robots_could_be_the_future_of/,CaptainHoek,1412829021,,0,2,False,http://a.thumbs.redditmedia.com/Xnkp2WqmNS7u2YIWUmtKRGD-d3zDCSDi8x2Rg7yBgY0.jpg,,,,,
78,MachineLearning,t5_2r3gv,2014-10-9,2014,10,9,13,2iqdsg,self.MachineLearning,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 6,https://www.reddit.com/r/MachineLearning/comments/2iqdsg/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1412829879,"Hi!

Another update!

I have been working on the image-to-features system. Deepmind did not have a separate feature extractor and reinforcement learner. They used a convolutional neural network to predict Q values for discrete actions. While I have both a discrete and continuous version of HTMRL, I really want to focus on the continuous version since it can be applied to more problems in the future. However, the way I am currently obtaining a continuous policy from a Q function makes it far more efficient to separate the structures for image feature extraction and reinforcement learning instead of combining them.

I fixed several large bugs in my HTM implementation that were causing it to predict poorly. In case this HTM stuff doesn't work out though, I tested a convolutional restricted boltzmann machine network on the pole balancing problem.  In case I do end up pursuing this route instead of HTM, I would need to handle partial observability in the reinforcement learning portion of the algorithm.

In HTMRL, partial observability is handled by the HTM itself: It produces novel patterns that combine both current state and predicted future state. A problem I am currently having with it though is that unless I make the receptive field of the HTM columns very small, it will not perceive small changes in the image. It will instead just ignore them. It is possible to remedy this by adding more and more columns, but this drastically slows down the algorithm. Therefore I am currently experimenting with different parameters and topologies to find what works best. If I absolutely cannot get it to work, I still have that Conv-RBM.

In case I do end up using Conv-RBM instead, I would need to handle partial observability differently. Fortunately though, I was already able to solve problems such as the T-maze problem by simply feeding the hidden units used by the free-energy based reinforcement learner back to the visible units. To be honest, I didn't expect such a simple solution to work, but it worked. So, unlike Deepmind's algorithm, I now can learn ""infinitely"" long-term dependencies (theoretically anyways). Deepmind used a fixed-size history window as far as a know, which both increases the complexity of the resulting MDP and cannot account for long-term dependencies that require knowledge beyond this time window.

In case you missed it, the free-energy based reinforcement learner (FERL) I use for the continuous version of HTMRL is derived from this: [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3584292/](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3584292/). I modified it heavily to perform both continuous actions and handle partial observability.

So, to summarize: HTM, or Conv-RBM + recurrent hidden nodes in the reinforcement learner portion. I will probably end up doing both ;)

For those just seeing this for the first time, the source code for this is available here, under the directories ""htmrl"" and ""convrl"": [https://github.com/222464/AILib](https://github.com/222464/AILib) It uses the CMake build system.

Until next time!",1,29,False,self,,,,,
79,MachineLearning,t5_2r3gv,2014-10-9,2014,10,9,16,2iqp4t,dataconomy.com,MIT Have Developed an Algorithm Which Can Predict An Area's Crime Rate Using Google Street View,https://www.reddit.com/r/MachineLearning/comments/2iqp4t/mit_have_developed_an_algorithm_which_can_predict/,futureisdata,1412840608,,0,2,False,http://b.thumbs.redditmedia.com/X6MVeTvl91B1qwbBP_aRirGlxqlq_FPfse6VBXOkMEk.jpg,,,,,
80,MachineLearning,t5_2r3gv,2014-10-9,2014,10,9,23,2iren6,self.MachineLearning,Cost increases in each iteration? My implementation of Multinomial Logistic regression,https://www.reddit.com/r/MachineLearning/comments/2iren6/cost_increases_in_each_iteration_my/,[deleted],1412864702,"Hey guyz,
I am trying to implement multinomail logistic regression using gradient descent, following http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression

My data set has 7 ratings classes from 1 to 7.
Cost in the cost function keeps increasing in each iteration, instead of reducing. Can any one help me with an extra pair of eyes? 

My costFunction: http://pastebin.com/2FJcTkxy

calculatingPOfJ : http://pastebin.com/YNrB91K1",1,0,False,default,,,,,
81,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,2,2irxfr,engineeringblog.yelp.com,Automatically optimizing A/B testing with MOE,https://www.reddit.com/r/MachineLearning/comments/2irxfr/automatically_optimizing_ab_testing_with_moe/,Zephyr314,1412875303,,0,10,False,http://b.thumbs.redditmedia.com/VnxR0R6_0td1lisXknYenX6oh20GzPah9vksbJLvKHM.jpg,,,,,
82,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,3,2is2v6,blog.tradeshift.com,Hundreds compete to improve machine learning algorithm for $5K prize,https://www.reddit.com/r/MachineLearning/comments/2is2v6/hundreds_compete_to_improve_machine_learning/,shift_happenz,1412878169,,3,0,False,http://b.thumbs.redditmedia.com/I5Bq8txPsB0XYqSVirYbNjCArfZ4NeAiWkQj2o-3gQM.jpg,,,,,
83,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,4,2isagf,self.MachineLearning,For what tasks or kind of data are Random forests based models best suited?,https://www.reddit.com/r/MachineLearning/comments/2isagf/for_what_tasks_or_kind_of_data_are_random_forests/,YesIAmTheMorpheus,1412882229,,5,7,False,self,,,,,
84,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,4,2isbkh,machinelearningmastery.com,16 Options To Get Started in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2isbkh/16_options_to_get_started_in_machine_learning/,jasonb,1412882826,,0,2,False,http://b.thumbs.redditmedia.com/9_R5Fo6832DR1hyp7ch7BEG2L6dpyXK5zyz9d6Q-rYg.jpg,,,,,
85,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,5,2ish1n,colah.github.io,Visualizing MNIST: An Exploration of Dimensionality Reduction,https://www.reddit.com/r/MachineLearning/comments/2ish1n/visualizing_mnist_an_exploration_of/,postit,1412885631,,20,80,False,http://b.thumbs.redditmedia.com/n8Kv5b8WP2JNX6Nx1sNTaeIF_YOsLP_reVsT44y6dbI.jpg,,,,,
86,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,5,2isi5a,self.MachineLearning,Does something that extracts new information from a paper exist?,https://www.reddit.com/r/MachineLearning/comments/2isi5a/does_something_that_extracts_new_information_from/,trashacount12345,1412886230,"This is probably a really hard question, but it would be really cool if something in this direction existed already. When reading a scientific paper, I often end up skimming too quickly over the introduction because it contains a ton of information I already know about if its a field I've read about before. However, I fairly frequently find myself having to turn back to the introduction to get something that I skipped over.

So here's what I want (and if it were magically part of Mendeley I would be in heaven): something where I input the papers that I have already read and a new one that I'm about to read, and it tells me the parts of the new paper that I should pay attention to.

Since that probably doesn't exist, I'm curious about the research in this general direction. How far along is language processing in this type of area?",7,3,False,self,,,,,
87,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,15,2iu359,self.MachineLearning,k-NN: using n-grams instead of LSH/Minhash for nearest neighbor search,https://www.reddit.com/r/MachineLearning/comments/2iu359/knn_using_ngrams_instead_of_lshminhash_for/,[deleted],1412923294,"Hi I have a newbie question about k-NN search. My understanding is that LSH hashes similar strings/numbers to a same bin with high probability. But wouldn't a character-based n-gram search also perform the same function and replicate the k-NN search?

For instance, I have vectors that LSH would hash similar values to bins

*Bin#1*
{0,1,1,1,3,4,0}
{0,1,0,1,3,5,0}
{1,1,1,0,5,3,0}

*Bin#2*
{5,4,3,2,1,0,5}
{5,5,3,1,2,0,5}
   :

*and so on....*

If construct strings out of the arrays

{""0111340""}
{""0101350""}
{""1110530""}
   :
   :
etc. 

Then using character n-grams wouldn't I be able to index them and perform a search (using Solr/Lucene for instance) and get a candidate list, before running a string distance metric to order by relevance.

Am I on the right track? Could anyone share their thoughts on this?

x-posted to stackoverflow (didn't get any responses, so trying here)",0,6,False,default,,,,,
88,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,18,2iubxk,industry-update.tumblr.com,Industry Update  We talk about Forklift Prices,https://www.reddit.com/r/MachineLearning/comments/2iubxk/industry_update_we_talk_about_forklift_prices/,steveharriss,1412933572,,0,1,False,default,,,,,
89,MachineLearning,t5_2r3gv,2014-10-10,2014,10,10,22,2iup0n,self.MachineLearning,How does the brain do regularization?,https://www.reddit.com/r/MachineLearning/comments/2iup0n/how_does_the_brain_do_regularization/,furbyhater,1412946392,"I know next to nothing about neuroscience, but I can imagine that the brain needs to perform regularization on it's processes similar to the regularization penalties used in machine learning. Is there any know mechanism for this? Could it be that alcohol and psychotropics would somehow impede the regularization (lower the penalty incurred by thought processes that have high variance depending on the input)?",28,13,False,self,,,,,
90,MachineLearning,t5_2r3gv,2014-10-11,2014,10,11,0,2iv19o,colah.github.io,"Neural Networks, Manifolds, and Topology",https://www.reddit.com/r/MachineLearning/comments/2iv19o/neural_networks_manifolds_and_topology/,rasbt,1412954188,,5,66,False,http://b.thumbs.redditmedia.com/iFByPIti9C2ovtDi1tEU49a5zBbTynz-y0K5FpV5HoQ.jpg,,,,,
91,MachineLearning,t5_2r3gv,2014-10-11,2014,10,11,3,2ivnxu,nlpers.blogspot.com,"Hyperparameter search, Bayesian optimization and related topics",https://www.reddit.com/r/MachineLearning/comments/2ivnxu/hyperparameter_search_bayesian_optimization_and/,vkhuc,1412966513,,5,17,False,http://b.thumbs.redditmedia.com/EFdUSdQysh0mL1TEDy-w6JeBxUcuFp9imr83rfxn2Qk.jpg,,,,,
92,MachineLearning,t5_2r3gv,2014-10-11,2014,10,11,4,2ivtna,self.MachineLearning,sklearn vectorizing PoS &amp; numeric data?,https://www.reddit.com/r/MachineLearning/comments/2ivtna/sklearn_vectorizing_pos_numeric_data/,jonathan881,1412969684,"If I want to train on text, PoS, and numeric vectors how should this be done?  

The only thing that made sense so far was to hstack coo_matrix objects. I use CountVectorizer on the PoS tagged text in dicts and simply normalize the numeric float vectors... convert to coo and stack. 

is this correct? 

thanks all ",6,0,False,self,,,,,
93,MachineLearning,t5_2r3gv,2014-10-11,2014,10,11,6,2iw671,self.MachineLearning,Jobs in Machine/Statistical Learning for an undergrad?,https://www.reddit.com/r/MachineLearning/comments/2iw671/jobs_in_machinestatistical_learning_for_an/,bandalorian,1412976516,"I'll be finisinish my Bachelors in mathematical statistics next year.  I've also done a boat load of free courses on sites like coursera to learn programming. I would love to work with machine learning and AI in the future, and I would like to start looking for a job so I can get experience (and an income!) while I'm going to grad school. 

What kind of jobs are there for someone at the undergraduate level were you get to do statistical learning? Is there a particular field like business intelligence, or natural language processing that would have good job opportunities and would be a good start to a career with this?

The field is still just a collection of concepts for me (Support vector machines, linear regression, classification, decision trees etc etc) - what kind of jobs can you actually do with it that doesn't require a Phd or a completed Grad degree?

Thanks!",5,1,False,self,,,,,
94,MachineLearning,t5_2r3gv,2014-10-11,2014,10,11,6,2iw7lf,vdumoulin.github.io,Your models in Pylearn2: A tutorial on the minimal effort required to develop a new model in Pylearn2,https://www.reddit.com/r/MachineLearning/comments/2iw7lf/your_models_in_pylearn2_a_tutorial_on_the_minimal/,dwf,1412977325,,4,17,False,default,,,,,
95,MachineLearning,t5_2r3gv,2014-10-11,2014,10,11,7,2iw9zy,self.MachineLearning,Generating n-gram predictors from an existing corpus of text; I could use some advice.,https://www.reddit.com/r/MachineLearning/comments/2iw9zy/generating_ngram_predictors_from_an_existing/,Crotchfirefly,1412978734,"I was having trouble choosing a title that would fit the character limit and adequately describe the problem.  My NLP vocabulary is a little limited, so I may be using more words than I need to describe what I'm doing/what I want.

What I'm doing: I have good-sized dataset (about 30000 entries) consisting of a string of raw text (more specifically, an affiliation string from PubMed search results), and several normalized/manually entered classifiers for that string (e.g. country, state/province when applicable, organization name).  I'm using this set to automatically determine which present n-grams (or combinations of present and/or not present n-grams) are likely to do a good job of determining the country/state/org in future affiliation strings.

What I'm trying to improve right now is the selection process for those predictive n-grams.  My criteria for selecting sets of these predictors are, in order of priority, (1) covering all the affiliation lines in the corpus, (2) containing as few n-grams as possible, and (3) containing shorter n-grams (i.e. unigrams preferred over digrams, etc.).

The problem is that even with those criteria, I still have quite a few options in how I go about the selection of my predictive n-grams.  Everything I've done so far works fairly well, but I've definitely hit a plateau in terms of making improvements in accuracy and efficiency.  Is  there any sort of current ""gold-standard"" for addressing this kind of problem?  Are there any resources/guides/blogs that would be worth my time to peruse?  Lastly, what sort of jargon should I acquaint myself with so that I can state my situation and ask my questions more precisely?",3,0,False,self,,,,,
96,MachineLearning,t5_2r3gv,2014-10-11,2014,10,11,23,2iy3mc,nzhiltsov.blogspot.ru,Scalable tensor factorization: Ext-RESCAL 0.7 is out,https://www.reddit.com/r/MachineLearning/comments/2iy3mc/scalable_tensor_factorization_extrescal_07_is_out/,nzhiltsov,1413038057,,0,1,False,http://b.thumbs.redditmedia.com/8VeFBc-KMBK5b1SFMrc7ead1yzwyJOutdjoUmURqhuI.jpg,,,,,
97,MachineLearning,t5_2r3gv,2014-10-12,2014,10,12,7,2iz9rx,web.mit.edu,Building Interpretable Classifiers with Rules using Bayesian Analysis,https://www.reddit.com/r/MachineLearning/comments/2iz9rx/building_interpretable_classifiers_with_rules/,rrenaud,1413065305,,1,31,False,default,,,,,
98,MachineLearning,t5_2r3gv,2014-10-12,2014,10,12,7,2izdpj,self.MachineLearning,Newbie Question about gradients,https://www.reddit.com/r/MachineLearning/comments/2izdpj/newbie_question_about_gradients/,muktabh,1413067839,"Hi,

This is a very basic problem, which I am not sure I am thinking in the right way.
Suppose I define a autoencoder with a function f which calculates reconstruction error of one example.
However, the error function (say g) I want is mean of reconstruction errors of m (where m might be 20,30, basically more than 1) examples.
Now if I write a function grad(f,w), can directly averaging 10 instances of this (weights staying constant) lead to grad(g,w) ?
Gradient appears to be a linear function , 
wikipedia:""The gradient is linear in the sense that if f and g are two real-valued functions differentiable at the point a  Rn, and  and  are two constants, then f + g is differentiable at a, and moreover
    Grad (alpha * f+ beta * g)(a) = alpha Grad f(a) + beta * Grad g (a). ""
However, I am still not sure if this would be right. Please help.",2,0,False,self,,,,,
99,MachineLearning,t5_2r3gv,2014-10-12,2014,10,12,12,2j0253,datasciencemasters.org,The Open Data Science Masters Program,https://www.reddit.com/r/MachineLearning/comments/2j0253/the_open_data_science_masters_program/,hooande,1413085932,,1,67,False,default,,,,,
100,MachineLearning,t5_2r3gv,2014-10-12,2014,10,12,21,2j0sa7,self.MachineLearning,[Journal Club] discussion (Links to new vote/results of last vote inside),https://www.reddit.com/r/MachineLearning/comments/2j0sa7/journal_club_discussion_links_to_new_voteresults/,BeatLeJuce,1413115834,"**Discussed Paper**

You hopefully all had time to go through  [Bayesian Optimization in High Dimensions via Random Embeddings](http://www.cs.ubc.ca/~hutter/papers/13-IJCAI-BO-highdim.pdf) this week. Feel free to discuss your opinions, questions and other comments in this thread!


**Paper for next week**


Next week's paper will be [Neural Variational Inference and Learning in Belief Networks (ICML '14)](http://arxiv.org/abs/1402.0030) by Mnih and Gregor (of Google DeepMind) Poster: http://www.cs.toronto.edu/~amnih/posters/nvil.pdf   (this paper was proposed by /u/vikkamath)


**New Vote**

Please use [this week's poll thread](http://www.reddit.com/r/MachineLearning/comments/2j0scu/journal_club_week_422014_papervoting_thread/) to post new proposed papers / vote for your favourite papers.",2,9,False,self,,,,,
101,MachineLearning,t5_2r3gv,2014-10-12,2014,10,12,21,2j0scu,self.MachineLearning,[Journal Club] week 42/2014 paper-voting thread,https://www.reddit.com/r/MachineLearning/comments/2j0scu/journal_club_week_422014_papervoting_thread/,BeatLeJuce,1413115920,"Please post suggestions for next week's paper in this thread, and use your upvotes to vote for the papers you like. It is of course allowed to post papers that have been proposed in previous weeks. The paper with the most upvotes by Friday, 20:00 CEST will be chosen for the upcoming week.


[Link to this week's thread](http://www.reddit.com/r/MachineLearning/comments/2j0sa7/journal_club_discussion_links_to_new_voteresults/)",1,11,False,self,,,,,
102,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,0,2j15wm,self.MachineLearning,Coolest and most important areas of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/2j15wm/coolest_and_most_important_areas_of_machine/,[deleted],1413127666,"I've been very interested in Machine Learning for a while and I plan to specialize it when I pursue a doctorate degree. That being said, what are some of the cool/important areas of machine learning?

I'm currently interested in medical/space technology, but I want to hear of some other areas in which this topic is applicable.",0,1,False,default,,,,,
103,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,3,2j1ls8,machinelearningmastery.com,5 Machine Learning Areas You Should Be Cultivating,https://www.reddit.com/r/MachineLearning/comments/2j1ls8/5_machine_learning_areas_you_should_be_cultivating/,jasonb,1413137820,,1,0,False,http://b.thumbs.redditmedia.com/PWaQUhMa1uqxvNoD_ztSodI32M3D7fyrPjxTzUcXvoY.jpg,,,,,
104,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,12,2j31gb,self.MachineLearning,Target/Aspect Based Sentiment Analysis using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2j31gb/targetaspect_based_sentiment_analysis_using_deep/,satarupaguha11,1413170457,"Recently, deep learning has been very effectively used to represent words/sentences/paragraphs as vectors and these have been used as features for sentiment classification. These approaches have been proven to be very powerful and have achieved state-of-the-art performances. I was wondering if any work has been done on aspect-based or target-dependent sentiment classification using Deep Learning. I would rephrase the task as follows: Assuming that we are given a sentence and a target, the goal is to find the sentiment label (pos, neg, neutral) towards that target, rather than the sentiment label for the entire sentence.",2,2,False,self,,,,,
105,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,13,2j35pz,analyticsvidhya.com,Introduction to k-nearest neighbors : Simplified (cross post from /r/datascientist and /r/bigdata),https://www.reddit.com/r/MachineLearning/comments/2j35pz/introduction_to_knearest_neighbors_simplified/,kunalj101,1413173334,,0,3,False,http://b.thumbs.redditmedia.com/2fXNQ2cl8_0LA-ErFNc_lH-4Dgb_hcHoL-HoKmOQdgM.jpg,,,,,
106,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,17,2j3lma,self.MachineLearning,What is the best way to store data used for Natural Language Processing?,https://www.reddit.com/r/MachineLearning/comments/2j3lma/what_is_the_best_way_to_store_data_used_for/,chchan,1413188023,"I want to start learning and implementing some Natural Language processing algorithms for sentiment analysis and document classification. What is the best way to store data like corpus, bag of words, lexicons, and chunking grammar?

I am using python and Julia. But I also know some java.

I am thinking of using HDF5 due to its fast read/write speed. My question is what do researchers use and what formats are efficient?",6,9,False,self,,,,,
107,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,17,2j3mnt,self.MachineLearning,Neural network algorithms?,https://www.reddit.com/r/MachineLearning/comments/2j3mnt/neural_network_algorithms/,KatamoriHUN,1413189306,"For a university project, I need to find and to explain an algorithm (not the code but the way it works) that is based on neural networks, but I found it pretty damn hard to find anything via searching in Google. 

Can you tell me something, or a reliable source at least?",9,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,18,2j3oct,erogol.com,Machine Learning Work-Flow (Part 1),https://www.reddit.com/r/MachineLearning/comments/2j3oct/machine_learning_workflow_part_1/,erogol,1413191307,,0,2,False,http://a.thumbs.redditmedia.com/1acL7_65V6lhxk1SDWsvTe__wEdJ_cLIbtf6vR1nLT0.jpg,,,,,
109,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,18,2j3q32,self.MachineLearning,any study groups to exchange ideas and improve together,https://www.reddit.com/r/MachineLearning/comments/2j3q32/any_study_groups_to_exchange_ideas_and_improve/,devllved,1413193341,I quitted my job after working 2 years. Now Im attending grad school in a poor area . There are not may people that I can exchange ideas on artificial ingelligence or machine learning. Is there any study groups or similar org so that I can join and exchange idea online ? many thanks,2,2,False,self,,,,,
110,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,23,2j47g6,pyimagesearch.com,Followup from last week: How I ditched my MacBook Pro for Deep Learning and moved to the Amazon EC2 GPU cloud.,https://www.reddit.com/r/MachineLearning/comments/2j47g6/followup_from_last_week_how_i_ditched_my_macbook/,zionsrogue,1413209609,,10,38,False,http://a.thumbs.redditmedia.com/qaYU80ipiwl8xPgQ_5uJNGkalmUFNOnwoQodApN-QE8.jpg,,,,,
111,MachineLearning,t5_2r3gv,2014-10-13,2014,10,13,23,2j4a55,self.MachineLearning,Any advice on how to improve my accuracy rate in text classification?,https://www.reddit.com/r/MachineLearning/comments/2j4a55/any_advice_on_how_to_improve_my_accuracy_rate_in/,orangejaipur,1413211316,"I'm trying to do a text classification task.

Here are some specs:

* Context file size = 1M+ documents already labeled
* Number of top-labels = 17
* Number of sub-labels = around 130
* Each document is constituted of: a small text representing some retailer's client feedback (about 15-20 words in average) + a number of topics related to the text of the feedback + the sub-label and top-label it belongs to.

I'm using Scikit Learn. For the moment I've tried several things:

* different vectorizers: CountVectorizer (count occurences) or TfidfVectorizer (compute the tfidf)
* different tokenizers: unigrams, bigrams, trigrams
* various algorithms: SVM, Logistic Regression, Multinomial Naives Bayes, Random Forest.
* cascade of classifiers using the hierarchy in the data.

I can't get past 0.57 in accuracy rate (SVM with L1 norm by training on 500k documents) with those parameters... I'm always around 0.5 with those different configurations. And it doesn't really improve after 100k documents.

I'm trying to do some error analysis. I've computed a confusion matrix only with top-labels (test file size = 2k docs, algorithm = SVM):

* top-label : (precision recall f1-score support)
* TL1 0.57 0.25 0.35 16
* TL2 0.00 0.00 0.00 1
* TL3 0.57 0.55 0.56 258
* TL4 0.61 0.47 0.53 277
* TL5 0.46 0.41 0.43 27
* TL6 0.61 0.37 0.46 38
* TL7 0.69 0.31 0.43 35
* TL8 0.84 0.84 0.84 130
* TL9 0.50 0.06 0.11 31
* TL10 0.71 0.63 0.67 111
* TL11 0.64 0.34 0.45 143
* TL12 0.73 0.93 0.82 815
* TL13 1.00 0.17 0.29 12
* TL14 0.72 0.72 0.72 32
* TL15 0.47 0.14 0.22 56
* TL16 0.51 0.88 0.65 81
* TL17 0.80 0.86 0.83 14
* avg / total 0.67 0.68 0.65 2077

As you can see, one top-label creates a lot of confusion. It is the top-label ""product"" (TL12) which as you can imagine is really frequent in the context file (about 50% of the corpus). Some other top-labels that are also semantically related confuse each other a lot.

Also, when I'm checking the documents that were not classified well, I realize that the classifier often gets it wrong even though a very particular word is appearing in the document such as 'satisfied client' and 'thank you' in a document that should have been classified as 'congratulations, thanks'. I don't really understand why. Too much noise?

Do you have any advice?",7,2,False,self,,,,,
112,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,2,2j4ta4,youtube.com,Google Now vs. Siri vs. Cortana,https://www.reddit.com/r/MachineLearning/comments/2j4ta4/google_now_vs_siri_vs_cortana/,[deleted],1413221896,,0,0,False,default,,,,,
113,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,2,2j4uyo,adereth.github.io,Silverman's Mode Estimation Method Explained,https://www.reddit.com/r/MachineLearning/comments/2j4uyo/silvermans_mode_estimation_method_explained/,Adereth,1413222778,,0,6,False,default,,,,,
114,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,4,2j58ax,mysliderule.com,Online Courses to Learn Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2j58ax/online_courses_to_learn_machine_learning/,zxxx,1413229584,,0,1,False,default,,,,,
115,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,6,2j5jld,eventbrite.com,SF Machine Learning Workshops: next.ml,https://www.reddit.com/r/MachineLearning/comments/2j5jld/sf_machine_learning_workshops_nextml/,gwulfs,1413235324,,0,0,False,http://b.thumbs.redditmedia.com/3uJGVAfvS0pKGPnazdCA89EtEjW_dcSc0l2xo2ogYlQ.jpg,,,,,
116,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,9,2j64x2,cireneikual.wordpress.com,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 7,https://www.reddit.com/r/MachineLearning/comments/2j64x2/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1413247164,,3,9,False,http://b.thumbs.redditmedia.com/ZB2xXYvus-KR8FEqwinwo0feGKqSxo-OtlPB3yP9lYk.jpg,,,,,
117,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,15,2j72bi,self.MachineLearning,How to use bigrams features to sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/2j72bi/how_to_use_bigrams_features_to_sentiment_analysis/,ml_student,1413269849,"I have extracted some bigram features(Adjective/Adverb, Noun/Adjective, Adjective/Verb, Noun/Verb) from a POS tagged corpus with the help of its POS tags. The bigrams look as follows:

[('word', 'POS-tag'), ('word', 'POS-tag'), ('word', 'POS-tag')], where POS-tag is the Part of speech of the word. Given this i would like to know how to use the previous bigrams in order to classify some corpus which until now the only preprocessing is the same corpus with a lits of its POS-tags, lemmas and the original words. Reading the documentation of scikit-learn i found the class Text feature extraction and bigram vectorizer, but i dont understand how to classify with a supervised aproach. Also i have some questions about how to do this task:

Do i need to label some opinions as ""positive"" or ""negative""?.
I need to classify with SVM algorithm, as i understand the SVM implementation of scikit only accepts as parameters count vectors. So, do i need to extract a bigram count vector?.

Could anybody provide me some baseline aproach for sentiment analisys?

Thanks guys.",6,2,False,self,,,,,
118,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,16,2j73bm,blog.bigml.com,"Build a Predictive Lead Scoring App with BigML, SalesForce, and Talend Open Studio",https://www.reddit.com/r/MachineLearning/comments/2j73bm/build_a_predictive_lead_scoring_app_with_bigml/,sdesimonebcn,1413270852,,0,0,False,http://b.thumbs.redditmedia.com/COrjaDBNSevtoK0SLbJwQHj6mXzgqBCdrwJYcFViawU.jpg,,,,,
119,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,17,2j789u,self.MachineLearning,What is the best Data Mining algorithm for predicting based on a single variable?,https://www.reddit.com/r/MachineLearning/comments/2j789u/what_is_the_best_data_mining_algorithm_for/,doublebyte1,1413276423,"I have one variable whose value I would like to predict, and I would like to use only one variable as predictor (for instance: predict traffic density based on weather). Initially I thought about using Self-Organizing Maps, which performs unsupervised clustering + regression, but since it has an important component of dimensionality reduction I see it as more appropriated for a large number of variables. Does it make sense to use it for a single variable as predictor? Maybe there are more adequate techniques for this *simple* case: I used ""Data Mining"" instead of ""machine learning"" in the title of my question, because I think maybe a linear regression could do the job...",8,0,False,self,,,,,
120,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,19,2j7d32,self.MachineLearning,Maths vs. Stats?,https://www.reddit.com/r/MachineLearning/comments/2j7d32/maths_vs_stats/,Isfox,1413281799,"Hi All, 

Was hoping I might get some advice from people who know better than me :)

I'm looking to get into the field of machine learning and data analytics. Have an honours in comp sci, and have done a number of coursera courses, but now want to take it further with additional studying.

I'm struggling to figure out if I should focus on mathematics or statistics though - ML seems to be quite math heavy, but I suspect data analytics on a day to day basis might be more statistically oriented ? Are jobs generally one or the other, or a combination of both?

I know I'll definitely need some level of both, just not sure where the balance is. Any advice would be very appreciated! ",9,0,False,self,,,,,
121,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,21,2j7myv,self.MachineLearning,Good Neural Network Development Tool/Programming Language for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/2j7myv/good_neural_network_development_toolprogramming/,nolander_78,1413291181,"Hi! What would be a good tool (ANN Application/Programming Language) to use to teach a robot how to stand/walk? sorry if the question is a little broad but I am looking for a good way for the robot to teach itself how to stand and walk around rather than entering pre-programmed movements, this means the robot would need to reach a particular goal (standing) by moving its legs to a particular position to maintain its balance right? also while walking it needs to predict that making the next movement would not knock it down.",7,0,False,self,,,,,
122,MachineLearning,t5_2r3gv,2014-10-14,2014,10,14,22,2j7qhj,engineering.richrelevance.com,Online Parameter Optimization with Gaussian Process Regression,https://www.reddit.com/r/MachineLearning/comments/2j7qhj/online_parameter_optimization_with_gaussian/,sergeyfeldman,1413293645,,1,4,False,http://b.thumbs.redditmedia.com/v9JAq2qyh2ZLh4v_l6_eiUWDj7P1YAFOX1-5PSIJgmI.jpg,,,,,
123,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,0,2j7ytz,self.MachineLearning,What's the best library out there for experimenting with LSTM networks?,https://www.reddit.com/r/MachineLearning/comments/2j7ytz/whats_the_best_library_out_there_for/,eubarch,1413298816,"I'd like to do some experiments with Long Short Term Memory (LSTM) and Connectionist Temporal Classification (CTC).   I can work in Python, Java, or C/C++.  


I found [this page](http://lstm.iupr.com/) that lists three libraries for LSTMs, but I don't have a good sense of which of these are actively maintained, or if they're flexible enough to be used for something other than what they were original built for (Ocropus seems to be quite OCR-specific).  [Pybrain](http://pybrain.org/pages/features) also seems to be actively developed and have an LSTM algorithm, but I can't find mention of CTC in the documentation.  There's also [this code](https://github.com/MrMormon/lstm-g) written by an undergrad at BU a few years back that was well received but whose development/performance/flexibility/correctness status is something that I don't think is determined.  


Any opinions on the referenced tools?  Did I miss any?


",9,6,False,self,,,,,
124,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,6,2j9716,self.MachineLearning,Hackathon ideas with machine learning,https://www.reddit.com/r/MachineLearning/comments/2j9716/hackathon_ideas_with_machine_learning/,saxman666,1413321769,"I'll be going to a hackathon this weekend and have yet to come up with any project ideas relating to machine learning. What ideas do you all have that I might be able to implement? They can be any level as I'm a fairly experienced programmer. Thanks for the help.
",8,14,False,self,,,,,
125,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,6,2j99fd,self.MachineLearning,I'm on a quest to find out how machine learning relates to the outside world. Care to join?,https://www.reddit.com/r/MachineLearning/comments/2j99fd/im_on_a_quest_to_find_out_how_machine_learning/,[deleted],1413322988,"I'm a masters student (Mathematical Modeling and Computing at the Technical University of Denmark), and my primary focus is on machine learning. As a student, however, it's hard to know where and how machine learning is applicable, and to understand your place in industry and science. So this post is sort of for us students, who as of yet are nothing more than machine learning nerds.

Maybe some of you work in some field where you use machine learning as a tool, or you hire machine learning experts to help you? I'd like your insight.

Maybe you are some sort of machine learning expert, and have applied it in one or several fields?

Stuff I've found:

* I learned in [this](http://youtu.be/v-91JycaKjc?list=LL9dP2jzIApdwiRUJSbOPz6g) talk (among other things) that maybe us machine learning people have a tendency to apply incredibly complicated machine learning systems to everything. 

* I learned in talks about cognitive science and artificial intelligence such as [this one](http://youtu.be/MG_nOddk01E?list=LL9dP2jzIApdwiRUJSbOPz6g), that you can draw a lot of wisdom from these field and apply the concepts in machine learning, and the other way around apply machine learning to solve problems in these fields.

",0,0,False,default,,,,,
126,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,7,2j9c2z,forecastthis.com,can anyone explain 'metalearning' in the context of this vid? is it common parlance in ML?,https://www.reddit.com/r/MachineLearning/comments/2j9c2z/can_anyone_explain_metalearning_in_the_context_of/,[deleted],1413324403,,1,0,False,default,,,,,
127,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,10,2j9xiq,machinelearningmastery.com,Tutorial To Implement k-Nearest Neighbors in Python From Scratch,https://www.reddit.com/r/MachineLearning/comments/2j9xiq/tutorial_to_implement_knearest_neighbors_in/,imsome1,1413336551,,2,0,False,http://a.thumbs.redditmedia.com/BlSvhKcBG089i0s_ViJBbS6QYo5qtfZjWju6qu_yml4.jpg,,,,,
128,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,12,2ja7se,self.MachineLearning,Feature Hashing for Words,https://www.reddit.com/r/MachineLearning/comments/2ja7se/feature_hashing_for_words/,bugged_out_nrea,1413342574,"I'm very new to machine learning, but am required to use it for a class project.

I'm attempting to classify words/tokens and some features associated with them (such as, for example, whether they are uppercase, or whether the word before it is an adjective).

Currently, I'm attempting to use scikit-learn and the SVM module.  I'm somewhat familiar with SVMs as they apply to numerical data, but not to tokens.

So I've been attempting to use http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing to help me.

It seems this will automatically take some feature dictionary and convert it into a matrix useable in SVM by taking each feature and hashing it to produce some index that the feature is associated with.

This sounds perfect for my project, however it's not clear how I will use it.  I've created an SVM and trained it on a small number of feature vectors created from some words using feature hashing.  However, when I try to predict the class of a feature vector produced from a token that wasn't in the original training set, the SVM throws an error.

It seems that the feature hasher produces a different sized vector (at least last time I tried this).  Maybe I did it wrong, but correct me: if the SVM is trained on a certain dimension feature vector, then all feature vectors that are to be predicted must be the same dimension.  Perhaps there is just an issue with my code, I'll post an example in a bit...  Or maybe I'm completely misunderstanding how feature hashing works?

edit: Also maybe I shouldn't be using an SVM at all?  If not, perhaps you can recommend me something? There are roughly 32 features for each token/word and 30 of them are binary features, the last two are the part of speech and the actual token.",2,0,False,self,,,,,
129,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,12,2ja7zo,self.MachineLearning,I am having trouble understanding (and implementing) logistic regression for classifying into three classes.,https://www.reddit.com/r/MachineLearning/comments/2ja7zo/i_am_having_trouble_understanding_and/,TrashQuestion,1413342692,"(For reference, i am using Kevin P Murphy's Book ""Machine Learning: A Probabilistic Perspective"" and implementing with MATLAN - without any toolboxes)

I have a dataset with 392 samples (rows), each sample has 8 features (columns), one of which defines the class (i.e. column 1 of features is divided into three equal bins which define the three classes - low, medium, and high).

I am having a really hard time understanding how to create a logistic regression model to classify a sample into one of these three classes.

I just finished learning and making a linear regression model where I learned aboutboth the Ordinary Least Squares (Closed form) solution for the weight vector, and also Gradient Descent (Open Form) solution. But i never implemented gradient descent because my data was fitted perfectly fine with the OLS solution for weight vector.

I am extremely confused how to create a weight vector for Logistic regression, I understand that it requires use of Gradient Descent because there is no closed form solution. I also read about the Newton method for calculating the weights but I don't understand it at all.

And after you use these methods to calculate weights how do you apply the weights to the sample data? In Linear regression it was simply because you simply multiplied the weights by the features (and higher order features for higher order linear regression), but is it the same in logistic regression?

Moreover my understanding so far is that this model only works for binary classification, so how would i do it for three classes?

Basically my question boils down to this:

How exactly do you find the weight vector for logistic regression (using either gradient descent or newtons method, whichever is easier) and how do you apply the weight vector to the sample to get a classification out of it for 3 classes (not just two).",3,1,False,self,,,,,
130,MachineLearning,t5_2r3gv,2014-10-15,2014,10,15,15,2jaost,drive.google.com,"Deep Learning: Past, Present and Future",https://www.reddit.com/r/MachineLearning/comments/2jaost/deep_learning_past_present_and_future/,mlalma,1413355254,,6,29,False,default,,,,,
131,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,1,2jbvcb,plosone.org,MPF Reinforcement Learning (not my paper),https://www.reddit.com/r/MachineLearning/comments/2jbvcb/mpf_reinforcement_learning_not_my_paper/,CireNeikual,1413390258,,1,7,False,default,,,,,
132,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,4,2jce24,datasciencecentral.com,"Is data science a new paradigm, or recycled material?",https://www.reddit.com/r/MachineLearning/comments/2jce24/is_data_science_a_new_paradigm_or_recycled/,vincentg64,1413399828,,0,0,False,http://b.thumbs.redditmedia.com/OT3keA0bz1-GiIJTPHqI-SZnGKIYi1U8BpA64uIq91I.jpg,,,,,
133,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,6,2jcspo,youtube.com,"""Machine Learning within Law"" - talk by a Law Professor on Machine Learning applied to Law.",https://www.reddit.com/r/MachineLearning/comments/2jcspo/machine_learning_within_law_talk_by_a_law/,crawdidnot,1413407407,,11,29,False,http://b.thumbs.redditmedia.com/YXwkcKZsI64z-TGNNQcU2QStF5HFyo51PW62zgvMdrU.jpg,,,,,
134,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,6,2jctbi,arxiv.org,cuDNN: Efficient Primitives for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2jctbi/cudnn_efficient_primitives_for_deep_learning/,rantana,1413407722,,4,9,False,default,,,,,
135,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,7,2jd3f2,self.MachineLearning,Preparing for an internship,https://www.reddit.com/r/MachineLearning/comments/2jd3f2/preparing_for_an_internship/,MLApprentice,1413413266,"Hi,  
I'm in third year of engineering school and will have to find an internship next year in the course of my studies. I'd like to work on machine learning but am unsure how to prepare.  
Background: I know python, java, matlab, sql, and excel.    
  
I've set myself a goal of implementing one ML algorithm a week. So I've been looking for papers that look fun and publishing my implementations on github.  
Will this have any weight on my resume?   
  
From reading this subreddit I've gathered that going to NIPS in december would be worthless at my level and that participating in Kaggle competitions would be a waste of time.  
Are there other things I could be doing to become more attractive to potential employers?  
",16,3,False,self,,,,,
136,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,12,2jduep,devblogs.nvidia.com,Deep Learning for Computer Vision with Caffe and cuDNN,https://www.reddit.com/r/MachineLearning/comments/2jduep/deep_learning_for_computer_vision_with_caffe_and/,harrism,1413428719,,0,14,False,http://b.thumbs.redditmedia.com/LWRK5QDdnM9v6F13kC1ufNdGUHs5Ie638dMU5OrCjaE.jpg,,,,,
137,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,13,2je27k,rujhaan.com,Natural Language Platform Wit.ai Raises $3M Seed Round Led By Andreessen Horowitz,https://www.reddit.com/r/MachineLearning/comments/2je27k/natural_language_platform_witai_raises_3m_seed/,rujhaandotcom,1413434505,,1,1,False,http://a.thumbs.redditmedia.com/dUB3G3z4PEnDjDFvm4BiroWWjyVomzvNaLzOuQMZna0.jpg,,,,,
138,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,16,2jecze,self.MachineLearning,Why is my gradient descent algorithm stopping so soon?,https://www.reddit.com/r/MachineLearning/comments/2jecze/why_is_my_gradient_descent_algorithm_stopping_so/,[deleted],1413443949,"I am trying to implement a logistic regression classification in MATLAB and am stuck with calculating the correct weights using gradient descent.

I am using a stochastic approach, so i update each weight in the vector of weights individually    for each feature and then move onto the next sample and do again.

I am using the update equation

theta_j := theta_j - alpha * (y_i - h_theta(x_i)) * x_ij

and i break when the difference between the last weight vector and the current one is less than 0.00005. I calculate the ""difference"" between the two vectors by subtracting one from the other and then taking the square root of the dot product of their difference vector. 

The problem is that it seems to stop updating after only four updates, so only the first four of my 8 row weight vector is even updated at all. This happens no matter what my learning rate, alpha, is.

Here is my implementation:

    function weightVector = logisticWeightsByGradientDescentStochastic(trueClass,features)
        %% This function attemps to converge on the best set of weights for a logistic regression order 1
        %% Input:
        % trueClass - the training data's vector of true class values
        % features
        %% Output:
        % weightVector - vector of size n+1 (n is number of features)
        % corresponding to convergent weights
        
        %% Create one vector and append to features
        oneVector = ones( size(features,1) , 1); %create one vector to append to features
        regressData = horzcat(oneVector, features); % create dataset that we will use to calculate regression weights
        
        %% Get Data Size
        dataSize = size(regressData);
        
        %% Initial pick for weightVector
        weightVector = rand( dataSize(2), 1); %create a zero vector equal to size of regressData
        weightVector = 100.*weightVector
        
        %% Choose learning Rate
        learningRate = 1000;
        
        %% Stochastic Gradient Descent
        
        oldWeightVector = weightVector; %set oldWeightVector
        newWeightVector = oldWeightVector; % pre-allocate size for newWeightVector
        difference = Inf; %initial difference to get into loop
        iterCount = 0; %for testing to see how long it takes
        
        while(difference &gt; 0.000005)
        
            for m=1:dataSize(1) %for all samples
                
                for n=1:dataSize(2) %for all features
    
                    %% calculate Sigmoid predicted 
                    predictedClass = evaluateSigmoid(oldWeightVector, regressData(m,:))
    
                    
                    %% Calculate the error
                    error = learningRate .* (trueClass(m) - predictedClass) .* regressData(m,n);
                    
                    %% Update weightVector for feature n
                    newWeightVector(n) = oldWeightVector(n) - error;
    
                    %% Calculate difference
                    vectorDifference = newWeightVector - oldWeightVector; %find difference vector between new and old weight vectors
                    difference = sqrt( dot( vectorDifference, vectorDifference)) %calculate the magnitude of difference between new and old weight vectors
                    
                    iterCount = iterCount + 1;
                    
                    %%Break if difference is below threshold
                    if(difference &lt; 0.00005)
                        break
                    else
                        oldWeightVector = newWeightVector; % update Old Weight Vector for next prediction
                    end
                end %for n
                
                %%Break if difference is below threshold
                if(difference &lt; 0.000005)
                    break
                end   
                
            end %for m
            
        end %while difference &gt; 0.0005
       
        weightVector = newWeightVector
        iterCount
    end


I have also tried doing a global method instead of stochastic, but it still results in extremely large weight values.

Here is my evaluateSigmoid function

    function logisticPrediciton = evaluateSigmoid(weightVector, sample)
        %% This function evaluates the sigmoid with a given weight vector and sample
        %% Input:
        % weightVector - column  vector of n weights
        % sample - row vector sample with n-1 features (a 1 will be appended to the
        % beginning for the constant weight
        
        sample = transpose(sample); % sample is fed in as a row vector, so must be transposed
        
        exponent = exp( -1 .* transpose(weightVector) * sample);
        
        logisticPrediciton = 1 ./ ( 1 + exponent);
        
        
    end

[And here is the data set i am working with.][1] The last column is filtered out, and the first column is turned into 1 or zero based on if it meets a threshold (below 22 is 0, above is 1).


**EDIT**: I know why it is stopping but don't know how to fix it. When the estimator uses the guess weights and happens to classify it as 1 or 0 correctly (matched with the true class), the error is zero which means newWeightVector equals oldWeightVector and the difference becomes zero. How do i avoid this? Its obvious that my guess the correct weight distribution.

  [1]: http://pastebin.com/wdq3KyGf",16,0,False,default,,,,,
139,MachineLearning,t5_2r3gv,2014-10-16,2014,10,16,17,2jeg2h,self.MachineLearning,Curriculum for DL,https://www.reddit.com/r/MachineLearning/comments/2jeg2h/curriculum_for_dl/,hahze,1413447366,"Howdy!

I know a fair amount about ML in general but I don't know much about DL.

Here's what I feel like I need to learn so far

* Auto Encoders -- semantic hashing
* RBM -- in turn to DBN
* Recursive Tensor neural nets
* Recurrent Nets
* word2vec
* 
* Multilayer versions of aforementioned ",7,3,False,self,,,,,
140,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,0,2jfd7u,self.MachineLearning,Weka SMOreg and LIBSVM with linear kernel problems...,https://www.reddit.com/r/MachineLearning/comments/2jfd7u/weka_smoreg_and_libsvm_with_linear_kernel_problems/,__null__,1413474749,"I want to test a dataset in weka using either LIBSVM with an e-SVR or SMOreg for regression. I also choose a linear kernel in both (in SMOreg i use an exponent=1 in a non normalized polykernel).

After cross-validation the root mean squared error (rmse) as well as the mean absolute error return both are equal to **NaN**.

Is this supposed to be an error in the values of the dataset I use (it contains no missing values)... How can I possibly handle this ? Any hints on what should I check ? The data I use for training are the appearence and shape parameter values of an Active Appearence Model created with Menpo.io using python...

Same problem occurs when I cross validate using the MLPY wrapper of Libsvm in python.",10,0,False,self,,,,,
141,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,4,2jfzwv,startup.ml,SF Future of Data Science Party,https://www.reddit.com/r/MachineLearning/comments/2jfzwv/sf_future_of_data_science_party/,gwulfs,1413486402,,0,7,False,http://a.thumbs.redditmedia.com/hiak17cQWGnZxBBPWM7tqBJHio3zUMJ24H-VK_aHo00.jpg,,,,,
142,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,4,2jg3j2,memkite.com,Update with 162 new Deep Learning papers to the Deeplearning.University Bibliography,https://www.reddit.com/r/MachineLearning/comments/2jg3j2/update_with_162_new_deep_learning_papers_to_the/,atveit,1413488277,,0,0,False,default,,,,,
143,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,5,2jgb84,self.MachineLearning,0/1 Classification using Autoencoders,https://www.reddit.com/r/MachineLearning/comments/2jgb84/01_classification_using_autoencoders/,bge0,1413492224,"Hello Everyone,


Was just pondering this the other day. I have a chain of stacked denoising autoencoders that are taking in a signal and reconstructing it based on the previous signal. Now, would it be possible to take this output (some floating point vector) and do a 0/1 output on it WITHOUT supervised criterion? 


I am just trying to evaluate how well the signal is being reconstructed w.r.t the original. I was thinking along the lines of:


    output = sigmoid(y'x), where y is the reconstruction of the autoencoder and x is the input signal (' = transpose).


If someone has any ideas or links to papers I would really appreciate it!


",7,0,False,self,,,,,
144,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,6,2jgeao,fastml.com,"Vowpal Wabbit, Liblinear/SBM and StreamSVM compared",https://www.reddit.com/r/MachineLearning/comments/2jgeao/vowpal_wabbit_liblinearsbm_and_streamsvm_compared/,vkhuc,1413493797,,8,26,False,http://b.thumbs.redditmedia.com/8Im0FCyEVhgG11AHbXdyMlVSkGKVfvVriUyEeACt4IU.jpg,,,,,
145,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,9,2jgxqd,self.MachineLearning,Neural networks researchers,https://www.reddit.com/r/MachineLearning/comments/2jgxqd/neural_networks_researchers/,cruvadom,1413504691,"I'm looking for a PhD position in Germany and around (Switzerland maybe), in the field of Neural Networks (perhaps deep learning). Any recommendations? Any big shots in Germany and around (except Jurgen Schmidhuber)? Any good research groups in Germany and around that you know about?

Thanks!
",8,6,False,self,,,,,
146,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,9,2jh28t,self.MachineLearning,Classification/categorization of time-series (4 features),https://www.reddit.com/r/MachineLearning/comments/2jh28t/classificationcategorization_of_timeseries_4/,pica_foices,1413507496,"I am developing a small project where I have hundreds of runs of time-series data as represented in the figure http://i.imgur.com/8ABDo9b.png (the figure represents the mean of those runs, for two conditions described by four features. Single run data is noisy).

What are the best methods to classify some test runs of this type of data (time-series, 4 features, 2 categories) ? 

I can use some runs to train the methods and the remaining runs to test different methods.

Thanks in advance.

Best regards,
pica_foices",4,5,False,self,,,,,
147,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,10,2jh46h,cireneikual.wordpress.com,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 8,https://www.reddit.com/r/MachineLearning/comments/2jh46h/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1413508703,,4,15,False,http://b.thumbs.redditmedia.com/ouWtH0_L2P7ooJctWiRJBmDhTiEpSHfzqa0I_b6HpZE.jpg,,,,,
148,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,11,2jh8kc,self.MachineLearning,Can Game Theory be effectively combined with Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/2jh8kc/can_game_theory_be_effectively_combined_with/,DomMk,1413511369,"Context: 

I'm an undergraduate studying Electrical Engineering and Applied Math (dual major with a focus on Statistical Learning) and have managed to score a predictive analytics intern position at a Bank. This is inline with what I want to be doing in the future. Next year I have the oppertunity to take Game Theory as one of my Maths Electives (it's apart of Economics, but the unit course coordinator gave me the green flag).

Ultimately, I'm not after immediate tangile results. I'm purely fine with taking Game Theory if it will help build on my intution when dealing with behavioral data. However, I've never taken economics subjects so I have no clue what it will be lilke. Is there anything to gained from taking a subject like Game Theory?",8,7,False,self,,,,,
149,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,13,2jhmzu,arxiv.org,BilBOWA: Fast Bilingual Distributed Representations without Word Alignments,https://www.reddit.com/r/MachineLearning/comments/2jhmzu/bilbowa_fast_bilingual_distributed/,rrenaud,1413520679,,2,5,False,default,,,,,
150,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,13,2jho7x,self.MachineLearning,Cost function for Logistic Regression always infinite [MATLAB],https://www.reddit.com/r/MachineLearning/comments/2jho7x/cost_function_for_logistic_regression_always/,[deleted],1413521614,"I am trying to implement a logistic regression solver in MATLAB and i am finding the weights by stochastic gradient descent. I am running into a problem where my data seems to produce an infinite cost, and no matter what happens it never goes down...
Both these seem perfectly fine, i cant imagine why my cost function would ALWAYS return infinite.

[Here is my training data](http://pastebin.com/Upy0J1bU) where the first column is the class (Either 1 or 0) and the next seven columns are the features i am trying to regress on.",14,0,False,default,,,,,
151,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,19,2ji76a,datascience.stackexchange.com,Stack Overflow - Question about Regression Model,https://www.reddit.com/r/MachineLearning/comments/2ji76a/stack_overflow_question_about_regression_model/,doublebyte1,1413541363,,0,0,False,http://b.thumbs.redditmedia.com/DgkI__UkgxwMY41b_9KM0m0RF-vPkIw6QKnpV7VAYkU.jpg,,,,,
152,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,19,2ji8j7,datascience.stackexchange.com,Anomaly Detection based on Clustering/Markov Models,https://www.reddit.com/r/MachineLearning/comments/2ji8j7/anomaly_detection_based_on_clusteringmarkov_models/,doublebyte1,1413542948,,1,0,False,http://b.thumbs.redditmedia.com/DgkI__UkgxwMY41b_9KM0m0RF-vPkIw6QKnpV7VAYkU.jpg,,,,,
153,MachineLearning,t5_2r3gv,2014-10-17,2014,10,17,22,2jilu4,self.MachineLearning,Looking for the UCI repositories,https://www.reddit.com/r/MachineLearning/comments/2jilu4/looking_for_the_uci_repositories/,Postal2Dude,1413554056,"They have a website, but I don't see how I can download the repository. I'm trying to use these data with a python tool called python-scikit-learn.",1,0,False,self,,,,,
154,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,0,2jiv5s,nlp.linguasys.com,LinguaSys Launches GlobalNLP Natural Language Processing API Portal for Developers -Free Testing,https://www.reddit.com/r/MachineLearning/comments/2jiv5s/linguasys_launches_globalnlp_natural_language/,NLProc,1413559719,,2,0,False,http://b.thumbs.redditmedia.com/qgiMeXk3jc8Qte6XiDJnj2xFV22CPm4_HEqMVg3r0QM.jpg,,,,,
155,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,2,2jj7sz,self.MachineLearning,What is the best random forest (c++/python) library?,https://www.reddit.com/r/MachineLearning/comments/2jj7sz/what_is_the_best_random_forest_cpython_library/,[deleted],1413566750,Preferably GPU accelerated and available on Windows. Any suggestions?,6,11,False,self,,,,,
156,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,2,2jj7th,self.MachineLearning,Where can I find the Netflix prize algorithms,https://www.reddit.com/r/MachineLearning/comments/2jj7th/where_can_i_find_the_netflix_prize_algorithms/,PaulEllenbogen,1413566757,I wanted to run some of the winning Netflix prize algorithms myself. Does anyone know where to find a copy of the code that won BellKor's Pragmatic Chaos the prize?,8,6,False,self,,,,,
157,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,3,2jjfhe,youtube.com,Workshop on Mining Massive Data Sets (videos),https://www.reddit.com/r/MachineLearning/comments/2jjfhe/workshop_on_mining_massive_data_sets_videos/,xamdam,1413570969,,0,9,False,http://b.thumbs.redditmedia.com/08VoXvF4x1r7rUkUOGBmqiNxa9PxyDDgyGrIVYJV9mw.jpg,,,,,
158,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,6,2jk1cy,youtube.com,google explains language understanding,https://www.reddit.com/r/MachineLearning/comments/2jk1cy/google_explains_language_understanding/,evc123,1413583091,,17,55,False,http://b.thumbs.redditmedia.com/etE-25tg2DOUqJg_SCIZX_ApkHxNK2A6vpzNoyE6CsM.jpg,,,,,
159,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,8,2jkcke,mimno.org,Non-parametric Bayes,https://www.reddit.com/r/MachineLearning/comments/2jkcke/nonparametric_bayes/,vkhuc,1413589950,,1,17,False,default,,,,,
160,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,21,2jloxp,self.MachineLearning,[Journal Club] discussion week 43/2014,https://www.reddit.com/r/MachineLearning/comments/2jloxp/journal_club_discussion_week_432014/,[deleted],1413634906,"**Discussed Paper**

You hopefully all had time to go through  [Neural Variational Inference and Learning in Belief Networks (ICML '14)](http://arxiv.org/abs/1402.0030) by Mnih and Gregor this week. Feel free to discuss your opinions, questions and other comments in this thread!


**Paper for next week**


Next week's paper will be

Next week's paper will be [Algorithms for Non-negative Matrix Factorization](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization) by Lee &amp; Seung, NIPS 2000 [(PDF link)](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf)

**New Vote**

Please use this week's poll thread to post new proposed papers / vote for your favourite papers. (Link will be edited in soon)",0,1,False,default,,,,,
161,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,21,2jlp10,self.MachineLearning,[Journal Club] discussion week 42/2014,https://www.reddit.com/r/MachineLearning/comments/2jlp10/journal_club_discussion_week_422014/,BeatLeJuce,1413634998,"**Discussed Paper**

You hopefully all had time to go through  [Neural Variational Inference and Learning in Belief Networks (ICML '14)](http://arxiv.org/abs/1402.0030) by Mnih and Gregor this week. Feel free to discuss your opinions, questions and other comments in this thread!


**Paper for next week**

Next week's discussed paper will be [Algorithms for Non-negative Matrix Factorization](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization) by Lee &amp; Seung, NIPS 2000 [(PDF link)](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf)

**New Vote**

Please use [this week's poll thread](http://www.reddit.com/r/MachineLearning/comments/2jlp1o/journal_club_week_432014_papervoting_thread/) to post new proposed papers / vote for your favourite papers. (Link will be edited in soon)",6,12,False,self,,,,,
162,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,21,2jlp1o,self.MachineLearning,[Journal Club] week 43/2014 paper-voting thread,https://www.reddit.com/r/MachineLearning/comments/2jlp1o/journal_club_week_432014_papervoting_thread/,BeatLeJuce,1413635022,"Please post suggestions for next week's paper in this thread, and use your upvotes to vote for the papers you like. It is of course allowed to post papers that have been proposed in previous weeks. The paper with the most upvotes by Friday, 20:00 CEST will be chosen for the upcoming week.


[Link to this week's discussion thread](http://www.reddit.com/r/MachineLearning/comments/2jlp10/journal_club_discussion_week_422014/)",2,8,False,self,,,,,
163,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,23,2jlwif,github.com,RainbowNlp: NLP framework for various natural language processing tasks including: supervised name entity recognition and relationship extraction,https://www.reddit.com/r/MachineLearning/comments/2jlwif/rainbownlp_nlp_framework_for_various_natural/,based2,1413642008,,0,3,False,http://a.thumbs.redditmedia.com/QRh8jtn3T2vsVccTQRrxlFOqaYoFd16twRsRAxuJiL8.jpg,,,,,
164,MachineLearning,t5_2r3gv,2014-10-18,2014,10,18,23,2jlwmm,self.MachineLearning,How to detrend binary data? (x-post from r/statistics),https://www.reddit.com/r/MachineLearning/comments/2jlwmm/how_to_detrend_binary_data_xpost_from_rstatistics/,holidaytie,1413642112,"Given a time series for a binary outcome, I can group by weeks or months, etc., and plot the proportion of positive cases over time. If a simple linear trend is clear in the time series, removing it at the grouping level is reasonably straight forward, but is there a technique to remove the trend in the underlying binary data? For example, if I want to do a proportions difference test before and after time t on the data, I would expect no difference after removing the linear trend. But how exactly would you remove it? One thought is to adjust the amount of successful cases in each group (for the proportion test) according to the linear rate trend found in the time series data. Is that valid? Are there other approaches?",1,0,False,self,,,,,
165,MachineLearning,t5_2r3gv,2014-10-19,2014,10,19,2,2jmf0i,supportourribbons.com,Guy in my python club made this. Thought people here would like it.,https://www.reddit.com/r/MachineLearning/comments/2jmf0i/guy_in_my_python_club_made_this_thought_people/,Stevo15025,1413654447,,5,51,False,http://b.thumbs.redditmedia.com/HmhkV8h6Dd2of3lsdSYUrnZyzCyK9CLTGUBnHPeOaEs.jpg,,,,,
166,MachineLearning,t5_2r3gv,2014-10-19,2014,10,19,5,2jmuyp,github.com,Automating Music Composition and Melody Generation.,https://www.reddit.com/r/MachineLearning/comments/2jmuyp/automating_music_composition_and_melody_generation/,ArmenAg,1413663850,,2,18,False,http://b.thumbs.redditmedia.com/ZV7RKHzQOsuK-9w0qVPXYZGNFBPpDkIfogeH5tXiJ9M.jpg,,,,,
167,MachineLearning,t5_2r3gv,2014-10-19,2014,10,19,10,2jnn3m,self.MachineLearning,What is a good book/online guide to learning R.,https://www.reddit.com/r/MachineLearning/comments/2jnn3m/what_is_a_good_bookonline_guide_to_learning_r/,[deleted],1413682284,"I have been doing an online course for machine learning on coursera using MATLAB, but i heard there are a ton of industries that use R and i would like to learn that language. What is a good way to go about learning it?",11,6,False,self,,,,,
168,MachineLearning,t5_2r3gv,2014-10-19,2014,10,19,20,2jon5b,blog.datumbox.com,New open-source Machine Learning Framework written in Java,https://www.reddit.com/r/MachineLearning/comments/2jon5b/new_opensource_machine_learning_framework_written/,datumbox,1413717777,,20,34,False,http://b.thumbs.redditmedia.com/JDnOMUeHqXjloHMDcKaSdYww0dHO0zy8aqZjl5OreUo.jpg,,,,,
169,MachineLearning,t5_2r3gv,2014-10-19,2014,10,19,23,2jp0o3,self.MachineLearning,"What is the difference between ""RBF classifier"" and ""SVM with RBF kernel"" ?",https://www.reddit.com/r/MachineLearning/comments/2jp0o3/what_is_the_difference_between_rbf_classifier_and/,hadian,1413730702,,2,1,False,self,,,,,
170,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,2,2jpg8b,self.MachineLearning,What is the current state of reproducible science in the field of machine learning?,https://www.reddit.com/r/MachineLearning/comments/2jpg8b/what_is_the_current_state_of_reproducible_science/,onewugtwowugs,1413740581,"Writing my bachelor's thesis, I noticed that I had several problems with my workflow:

1. Keeping a journal and analyze one's test results is difficult. Doing so and in the meantime running several other tests, knowing which tests are comparable to which is even more difficult. Especially when one's software is regularly changing to meet new needs.
2. Succeeding in getting the same test scores today as I got two months ago is surprisingly hard.

I have been doing some thinking about what is missing, and maybe what I want is a program which allows me to say: ""I have a bunch of experiment results right here, and I believe they were retrieved some time in May. My automatically generated log book seems to confirm that such a result was created on May 18. Please recreate the state of the program as it was back then, and rerun the program with the same parameters as I used in that experiment.""

Essentially, an experiment log book that also stores the commit IDs of the current state of the experimental framework (and in the case where the scientist doesn't use VCS, one could automatically be set up for them). This would potentially solve the second problem, and be the ground work for the solving the first problem, which perhaps could be summarized as ""Evernote for the scientist"".

My question to you is mainly the one asked in the title, but also whether you have had the same problem yourself, and how you came to solve it. Would you see the value in using something like what I just described? ",6,3,False,self,,,,,
171,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,2,2jpgoo,self.MachineLearning,Self-study Linear Algebra textbook for ML and Stats,https://www.reddit.com/r/MachineLearning/comments/2jpgoo/selfstudy_linear_algebra_textbook_for_ml_and_stats/,srkiboy83,1413740841,"I am looking for a good linear/matrix algebra textbook, suitable for self-study, that covers topics relevant to statistics and machine learning. I have access to Gentle's ""Matrix Algebra"", but have found it to be too dry and more of a reference book for a practicioner who's already studied the subject before.

Some of the books I'm considering are:

Seber, ""A Matrix Handbook for Statisticians"";
Searle, ""Matrix Algebra Useful for Statistics"";
Harville, ""Matrix Algebra from a Statistician's Perspective"";
Gruber, ""Matrix Algebra for Linear Models"" (just came out)

and the more generally oriented:

Strang, ""An Introduction to Linear Algebra"";
Strang, ""Linear Algebra and Its Applications"";
Axler, ""Linear Algebra Done Right""

The trouble is, all these text have excellent reviews on Amazon, but so did Gentle's text and it doesn't really suit my purposes.
",15,28,False,self,,,,,
172,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,6,2jq1yc,cireneikual.wordpress.com,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 9,https://www.reddit.com/r/MachineLearning/comments/2jq1yc/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1413752742,,6,10,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
173,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,14,2jrcsy,arxiv.org,ImageNet Large Scale Visual Recognition Challenge,https://www.reddit.com/r/MachineLearning/comments/2jrcsy/imagenet_large_scale_visual_recognition_challenge/,mlalma,1413782238,,0,1,False,default,,,,,
174,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,17,2jrnyq,analyticsvidhya.com,How does Aritificial Neural Network work - simplified,https://www.reddit.com/r/MachineLearning/comments/2jrnyq/how_does_aritificial_neural_network_work/,kunalj101,1413793347,,1,0,False,http://b.thumbs.redditmedia.com/UOJOLNJDXqEUAygtg-VR2vFyGnUnZJd_xYxfJcwgtOg.jpg,,,,,
175,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,21,2js3kb,self.MachineLearning,Training Random Forest; Data Too Big For Memory,https://www.reddit.com/r/MachineLearning/comments/2js3kb/training_random_forest_data_too_big_for_memory/,ml_man,1413809635,"How do people deal with fitting random forests when data is too big for memory? 

Currently I use sklearn in python and sample every N rows s.t. the data fits in memory.",8,5,False,self,,,,,
176,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,23,2jsan0,self.MachineLearning,Pandas 0.15.0 supports categorical values in data frames.. will this now work with sklearn?,https://www.reddit.com/r/MachineLearning/comments/2jsan0/pandas_0150_supports_categorical_values_in_data/,nameBrandon,1413814479,"Pandas update - http://pandas.pydata.org/pandas-docs/stable/whatsnew.html

I had tried to run some previous work I'd done in R through Python using Pandas and sklearn (in order to learn more about Python), and gave up when I realized I couldn't factorize a variable with Pandas. 

Now that that issue is resolved, I'd like to go back at it but I couldn't figure out if sklearn will now utilize these categorical values by default, or if I now need to wait for a sklearn update?

Does anyone know off hand if I'm good to go, or if I need to wait on the sklearn devs?

",4,1,False,self,,,,,
177,MachineLearning,t5_2r3gv,2014-10-20,2014,10,20,23,2jsd4a,self.MachineLearning,Open problems in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2jsd4a/open_problems_in_machine_learning/,InfinityCoffee,1413815958,"What do you consider to be some of the major open problems in machine learning and its associated fields? Both practical and theoretical problems are welcome, but for the sake of conciseness leave out vague problems such as general intelligence, etc. ",14,28,False,self,,,,,
178,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,0,2jsj4d,self.MachineLearning,"People in research groups, what is your setup for shared server/resources?",https://www.reddit.com/r/MachineLearning/comments/2jsj4d/people_in_research_groups_what_is_your_setup_for/,needlzor,1413819348,"So my university finally decided to shell out some money to meet their expectations of our research scaling up to, \*gulp\*, ""big data"". I am not sure as to what we are going to get (they do not want to give too much money either, since it is mainly for us lowly PhD students) but their idea is that we should use it for sharing datasets and running experiments. We are a mainly text mining group.

I do not know much about hardware stuff but I am pretty sure we are going to need a system for people to submit jobs to the server to be run sequentially, so that it doesn't die from overuse. Also, we kind of talked about writing a set of libraries to stream datasets from the server to individual machines for people wanting to run their experiments locally, but not wanting to download all the data on their machine.

Does anybody know of prepackaged software we could use for those two things? What is your setup? We do 99% of our programming in Java if that matters.",9,1,False,self,,,,,
179,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,1,2jsmm0,self.MachineLearning,Whats your impression of our NLP Content Analysis &amp; Summarization platform?,https://www.reddit.com/r/MachineLearning/comments/2jsmm0/whats_your_impression_of_our_nlp_content_analysis/,mercurialkitten,1413821234,"We built a temporary portal for testing of our upcoming API &amp; platform. So far we are quiet pleased with the results, but could use additional feedback from Reddit friends.

This will be subscription-based-service, however we'd be happy to hook-up some of the earliest testers.

Again, this is a temporary subdomain for testing purposes that requires manual pasting of content. It does not have a bulk upload or save feature.

We've received interest from parties that want to perform automated screening of content (submissions, proposals, due diligence, etc.), analysis and summarization of work. Our initial testing indicates that it's within acceptable error rate tolerances.

We're a relatively small group of people that have spent a considerable amount of time on the project and look forward to your feedback!

Instructions: Just dive right in, no login information is required. That said, if we get flooded, we may have to pull it. Thanks.

http://rapv2.intellibutler.com
",6,5,False,self,,,,,
180,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,3,2jt209,self.MachineLearning,How to classify text with scikit learn?,https://www.reddit.com/r/MachineLearning/comments/2jt209/how_to_classify_text_with_scikit_learn/,ml_student,1413829093,"I have extracted some bigram features(Adjective/Adverb, Noun/Adjective, Adjective/Verb, Noun/Verb) from a POS tagged corpus with the help of its POS tags. The bigrams look as follows: [('word', 'word'), ('word', 'word'), ('word', 'word')]. 

Given this i would like to know how to use the previous bigrams in order to make some supervised classification with SVM algorithm, as i know mostly any implementation of SVM only accepts as parameters numbers. How can i create a feature vectore and make some classification (positive, negative, neutral). Could anybody provide me some example?. I read the documentation of scikit and the [example](http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html) of text classifcation, but i dont get how is he building the feature veacture that feeds the algorithm.

Thanks",1,2,False,self,,,,,
181,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,5,2jtlp7,bayesimpact.org,Defending Microfinance with Data Science,https://www.reddit.com/r/MachineLearning/comments/2jtlp7/defending_microfinance_with_data_science/,andrewinshorts,1413838668,,4,8,False,http://b.thumbs.redditmedia.com/-uvsPJPf0upKaLfcMcPZ1EzP9sOAKi2FILYWuQaAqQI.jpg,,,,,
182,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,11,2juldo,self.MachineLearning,Process for finding which machine learning algorithms/package to use?,https://www.reddit.com/r/MachineLearning/comments/2juldo/process_for_finding_which_machine_learning/,holy_onasandwich,1413857841,I'd like to hear your processes for figuring out what machine learning algorithms/packages to use when tackling a machine learning problem. What are the factors you consider? Considerations about the dataset? Considerations about the packages or algorithms? What are the most important factors?,1,0,False,self,,,,,
183,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,11,2jun94,spectrum.ieee.org,"UC Berkeley's Michael Jordan on the epidemic of bad ideas about machine learning, big data, etc.",https://www.reddit.com/r/MachineLearning/comments/2jun94/uc_berkeleys_michael_jordan_on_the_epidemic_of/,boboleo,1413858860,,36,104,False,default,,,,,
184,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,14,2jv2o7,self.MachineLearning,Terence Tao's philosophical take on strong AI,https://www.reddit.com/r/MachineLearning/comments/2jv2o7/terence_taos_philosophical_take_on_strong_ai/,Guoguodi,1413869224,"I thought Terence's answer to this question to be thought provoking, so wanted to hear others' opinions.



Q: **What is your opinion on strong AI?**
 
A: The funny thing about AI is that its a moving target. In the seventies, someone might ask what are the goals of AI? And you might say, Oh, we want a computer who can beat a chess master, or who can understand actual language speech, or who can search a whole database very quickly. We do all that now, like face recognition. All these things that we thought were AI, we can do them. But once you do them, you dont think of them as AI. It has this connotation of some mysterious magical component to it, but when you actually solve one of these problems, you dont solve it using magic, you solve it using clever mathematics. Its no longer magical. It becomes science, and then you dont think of it as AI anymore. Its amazing how you can speak into your phone and ask for the nearest Thai restaurant, and it will find it. This would have been called AI, but we dont think about it like that anymore. So I think, almost by definition, we will never have AI because well never achieve the goals of AI or cease to be caught up with it.


Source:
[Terence Tao Interview](https://docs.google.com/document/d/1rinL25rC8LnMTzZcGjg1axT-0r-oiCnoKKH1DLQlmVA)


[Terence Tao (wiki)](http://en.wikipedia.org/wiki/Terence_Tao)",4,13,False,self,,,,,
185,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,17,2jveoh,self.MachineLearning,Question regarding model selection for linear discriminant analysis models,https://www.reddit.com/r/MachineLearning/comments/2jveoh/question_regarding_model_selection_for_linear/,07crisma,1413881344,"Hi,

I'm working on a classification project for a data mining class, and we have to test several classification models on a dataset. (250,000 observations, 2 groups, 30 independent variables)

I'm working on a linear discriminant analysis model using R-Studio and the MASS and klaR package.

The steps I took:
1) Loaded the data

2) Ran the lda function on Class against the 30 independent variables
(noticed a warning that some of the independent variables are collinear)

steps 3 onwards is where I have issues understanding what I'm doing/ whether I think i'm doing it right.

3) I searched for a model selection technique to trim down the amount of independent variables for LDA. The function I thought which would've helped was ""stepclass"" from ""klaR"". 

Description of stepclass function: ""Forward/backward variable selection for classification using any specified classification function and selecting by estimated classification performance measure from ucpm.""


This was the code i used

ldaselect &lt;- stepclass(Label~DER_mass_MMC+DER_mass_transverse_met_lep+DER_mass_vis+
                         DER_pt_h+DER_deltaeta_jet_jet+DER_mass_jet_jet+DER_prodeta_jet_jet+
                         DER_deltar_tau_lep+DER_pt_tot+DER_sum_pt+DER_pt_ratio_lep_tau+
                         DER_met_phi_centrality+DER_lep_eta_centrality+PRI_tau_pt+
                         PRI_tau_eta+PRI_tau_phi+PRI_lep_pt+PRI_lep_eta+PRI_lep_phi+
                         PRI_met+PRI_met_phi+PRI_met_sumet+PRI_jet_num+PRI_jet_leading_pt+
                         PRI_jet_leading_eta+PRI_jet_leading_phi+PRI_jet_subleading_pt+
                         PRI_jet_subleading_eta+PRI_jet_subleading_phi+
                         PRI_jet_all_pt, data=cdata, criterion =""AS"", method=""lda"")


I figured the criterion should be AS (ability to separate) since LDA seeks to find the variables which create the greatest separation between groups.

However, after running forward, backward, both direction selection, it only returns one variable, ""PRI_tau_pt"" as the only necessary independent variable.

I'm just skeptical of the result, and I was hoping if anyone here could give me some advice, or let me know if I've chosen the wrong steps (or if model selection is even necessary for LDA)

Regards,
Marvin",1,1,False,self,,,,,
186,MachineLearning,t5_2r3gv,2014-10-21,2014,10,21,22,2jvwmp,nbviewer.ipython.org,Gal Varoquaux's Machine Learning course,https://www.reddit.com/r/MachineLearning/comments/2jvwmp/gal_varoquauxs_machine_learning_course/,galapag0,1413897982,,1,4,False,default,,,,,
187,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,0,2jw6w3,primaryobjects.com,Discovering Trending Topics in News: a Tutorial with R,https://www.reddit.com/r/MachineLearning/comments/2jw6w3/discovering_trending_topics_in_news_a_tutorial/,primaryobjects,1413904100,,0,9,False,http://b.thumbs.redditmedia.com/mNEP5vunXaUBFde1DlW8rD7-y49-i1VqWFHu_ci9uEU.jpg,,,,,
188,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,0,2jw8xg,self.MachineLearning,"Use historic data as training data for late/on time payments? (python, scikit)",https://www.reddit.com/r/MachineLearning/comments/2jw8xg/use_historic_data_as_training_data_for_lateon/,tenthirtyone1031,1413905226,"I have some flex time where I can pursue my interests at work as long as I can apply it there. Recently I obtained ~100,000 invoice records over two years. 

Can I use this as training data for a decision tree to analyze new invoices for their chance of being paid late?

I've been using the scikit for python and did the provided sample iris Hello World and exported it to a pdf. It's clear I need to learn more but this is how I pictured my solution so far:

1) Prepare my dataset.

CSV in the format:

    Late | Vendor Name | Department | Invoice Month | Invoice Day
    0    | Vendor1     | 1          |  1            | 20
    1    | Vendor2     | 2          |  1            | 18

etc.

2) Load in to scikit w/ numpy or pandas

3) Run just like the iris/digits dataset?

Is this process wrong? Will I be able to use Vendor names (strings) instead of integer values in scikit just like the provided iris dataset or will I need to make further changes to account for that?

I appreciate any feedback or references to help guide me towards a solution.",4,1,False,self,,,,,
189,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,1,2jwhpl,arxiv.org,Neural Turing Machines,https://www.reddit.com/r/MachineLearning/comments/2jwhpl/neural_turing_machines/,alecradford,1413909838,,10,14,False,default,,,,,
190,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,2,2jwpv1,newsoffice.mit.edu,MIT computer scientists can predict the price of Bitcoin | MIT News,https://www.reddit.com/r/MachineLearning/comments/2jwpv1/mit_computer_scientists_can_predict_the_price_of/,copybin,1413913962,,52,48,False,http://b.thumbs.redditmedia.com/87BSVj8d9v_RxfO5yNu4nmaO0_YsLCoj2M7FNJAqHyM.jpg,,,,,
191,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,2,2jwqh9,blog.dominodatalab.com,DEEP LEARNING WITH DL4J AND DOMINO,https://www.reddit.com/r/MachineLearning/comments/2jwqh9/deep_learning_with_dl4j_and_domino/,AnnaOnTheWeb,1413914280,,0,1,False,http://b.thumbs.redditmedia.com/v4hRtwlrlSPvp6-xXPkKm4aMf_1Z9PzuOhHE5PSH_lo.jpg,,,,,
192,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,2,2jwqkt,chechiklab.biu.ac.il,"Analyzing the omics of the brain: upcoming NIPS workshop bringing together people from neuroscience, cognitive sciences and machine learning. Its goal is to help scientists connect the new wealth of genomic data to issues of cognition and learning.",https://www.reddit.com/r/MachineLearning/comments/2jwqkt/analyzing_the_omics_of_the_brain_upcoming_nips/,urish,1413914323,,0,4,False,http://b.thumbs.redditmedia.com/-4g4pO3oe3R1EX60JsJmEU3zETphog6MdBgPeSPzIFI.jpg,,,,,
193,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,3,2jww6c,meetup.com,Deep Learning Workshop: Boston,https://www.reddit.com/r/MachineLearning/comments/2jww6c/deep_learning_workshop_boston/,gwulfs,1413917034,,0,0,False,http://b.thumbs.redditmedia.com/4hbtjQU7Cz37BYEGajVZYLPid81FYmCJD4qgd3CuPpY.jpg,,,,,
194,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,6,2jxjh8,self.MachineLearning,"Looking for DeepLearning forums, etc...",https://www.reddit.com/r/MachineLearning/comments/2jxjh8/looking_for_deeplearning_forums_etc/,spurious_recollectio,1413928529,"In order to learn about deep learning and NNs I've been trying to roll my own NN code (in addition to going through some coursera videos and reading papers and even a textbook).  I've found that the devil really is in the details and I've often stumbled on problems whose answer I can't seem to find in the above mentioned resources. 

For instance:

- I've been trying to train a deep autoencoder in a layer-wise fashion (for a Kaggle competition) but I don't have any sense for what a reasonable (layer-wise) training error on an  auto-encoder is.  Obviously it can't be expected to reproduce the input data fully faithfully but I'd like some heuristic measure of what a good architecture is.

- To speed things up I tried to use gnumpy (as a drop-in for numpy) but despite various improvements I find the problem set is either too small for the GPU to beat the CPU or its too big to fit in memory.  I know this is wrong and I'm sure other people have had similar issues and it would be useful to get their insights.

I should add (just to be clear) that I'm not out to write another ML library -- I'm implementing stuff myself to learn it.

Anyway, I would like to be discuss my problems with experienced people or other people in the same position but I'm not sure what the right forum is.  These  questions seem a bit too detailed and technical for this subreddit so I would like to ask for any other recommendations on where to go for help (or confirmations that this is a good place to ask).",14,5,False,self,,,,,
195,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,9,2jy2gp,self.MachineLearning,Research jobs in machine learning?,https://www.reddit.com/r/MachineLearning/comments/2jy2gp/research_jobs_in_machine_learning/,generating_loop,1413938713,"Hi /r/MachineLearning!

This spring I received my PhD in mathematics studying geometric group theory. Even before I graduated, I decided to leave academics and after graduation went to work for a ""big 4"" company as a researcher. However, I'm finding that the work I'm doing is mostly statistical modeling and business analysis ( because our group is new and small we don't have the leverage to do projects without immediate business value). I feel unfulfilled and bored. 

Are there any companies doing research into machine learning? I don't mean research into how they can use current ml technology to their benefit, but actually researching new technology and methods, and publishing papers. Is my only option to go back to graduate school? 

EDIT: I should say I don't want to go back to pure math: I miss being on the cutting edge of a field, but I'd prefer it to be a field that's much closer to being useful some day. I'd also like to stay in Seattle.",5,0,False,self,,,,,
196,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,11,2jyc31,self.MachineLearning,How do I get into the field of Machine learning?,https://www.reddit.com/r/MachineLearning/comments/2jyc31/how_do_i_get_into_the_field_of_machine_learning/,pulse303,1413944058,"My questions might sound naive but I hope I get a better picture of ML.
What would be the best path to go towards machine learning.

I want to learn coding and would like to start a path that leads in that direction. 

How far can I get with websites like Codecademy, udacity and other offline and online workshops.
Is it possible to get a job without a formal eduction in CS, or is that basically impossible.

thanks",17,2,False,self,,,,,
197,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,12,2jyhyj,self.MachineLearning,Creating a basic perceptron feed forward neural network for multi-class classification in MATLAB,https://www.reddit.com/r/MachineLearning/comments/2jyhyj/creating_a_basic_perceptron_feed_forward_neural/,TrashQuestion,1413947411,"I am new to neural networks and I want to create a feed forward neural network for mutli-class classification. I am allowed to use any code that is publicly available but not any MATLAB ToolBox because i don't have access to them (so no neural network toolbox). The goal is to classify the data into one of 10 classes. [Here is the data set][1], the class is defined by the three letter code in the last column.

When creating a neural network, do you simply define the amount of nodes and have each node in layer i connect to every single node in layer i+1? And then simply have them learn the weights themselves?

Also is there a source I could follow that has MATLAB code for creating a neural network with any amount of input, any number of nodes, and does multi-class classification that is feed forward.


  [1]: https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data",1,0,False,self,,,,,
198,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,13,2jyowy,self.MachineLearning,I need advice about whether I should implement a machine learning algorithm,https://www.reddit.com/r/MachineLearning/comments/2jyowy/i_need_advice_about_whether_i_should_implement_a/,fight_off_thy_demons,1413951661,"So my college is doing an AI challenge (called megaminerAI). With teams of  1-3 people, the goal is to write a program to compete against other team's programs. It is a 24 hour challenge, noon-noon. The challenge has already been released, so I know what it entails. The problem is, should I do a ML approach, or should I brute force it (aka program for a variety of cases in the game)? No one has implemented a ML algorithm before in the...5 years of the existence of the challenge. 

My only real concern is the time limit, 24 hours, is that a reasonable concern? I am thinking about doing a neural network, but don't know if it would have enough time to train to compete with other teams. ",2,0,False,self,,,,,
199,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,15,2jz03r,self.MachineLearning,Which approach is right for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/2jz03r/which_approach_is_right_for_machine_learning/,capecamorin,1413960541,"Basically we have two ways of approaching Machine Learning.

 * One is by trying to build brain inspired algorithms. Say, Neural Networks Deep Learning and Cognitive models which aims at mimicing brain functions in understanding and solving data.

 * Other is influenced by Statistics which has widely adopted language R and has methods like Bayesian inference.

 * Even Michael Jordan has adopted Statistical approach over Cognitive.
And also has said in in his [recent interview](http://spectrum.ieee.org/robotics/artificial-intelligence/machinelearning-maestro-michael-jordan-on-the-delusions-of-big-data-and-other-huge-engineering-efforts#qaTopicFive) that Neuron is basically a cartoon and we are not doing it right in getting close to understanding our brain.
 * Whereas Jeff Hawkins of Numenta has come up with Cortical Learning Algorithm which is much inspired by brain functioning.

So what is the right approach which one should consider?
",3,0,False,self,,,,,
200,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,18,2jz7aa,plus.google.com,"Neural nets learn complex behaviors, recent papers with Jeff Dean comment.",https://www.reddit.com/r/MachineLearning/comments/2jz7aa/neural_nets_learn_complex_behaviors_recent_papers/,test3545,1413968612,,14,65,False,http://b.thumbs.redditmedia.com/hduM0DWHdBCNuViRRovIiBZf2JKUzMo50QZvJ5GQlaE.jpg,,,,,
201,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,20,2jzfeu,self.MachineLearning,Shark the C++ ML library....,https://www.reddit.com/r/MachineLearning/comments/2jzfeu/shark_the_c_ml_library/,Category_theory,1413977692,"Has anyone ever used it?  If so what are your thoughts on its speed over other ML packages in other languages, such as R, python etc.  ",3,1,False,self,,,,,
202,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,22,2jzmav,self.MachineLearning,Bayesian inference and intractable distributions,https://www.reddit.com/r/MachineLearning/comments/2jzmav/bayesian_inference_and_intractable_distributions/,PsychedelicStore,1413983398,"Hi to all, I'm struggling with a question on my personal studies about bayesian inference, mainly on dinamical system, Kalman and Particle filters. In the latter, we use sampling tecniques to find the state x(t) given observatio up to time t, i.e. we want to find p(x(t)|y(1:t)). We use sampling techqniques because we don't know, for example, the normalizing constant of the bayes formula. For example, in the formulation P(A|B)=P(B|A)P(A)/P(B), I've read that typically we cannot compute P(B), which involves an integral. What I can't understand is why we can't calculate this integral and an example in which, effectively, we can't. I've read that techniques like MCMC have been developed for this purpose but I can't understand when the problem can occur. Thanks in advance!",6,1,False,self,,,,,
203,MachineLearning,t5_2r3gv,2014-10-22,2014,10,22,23,2jzw3l,self.MachineLearning,Guidance needed on a classification task using website data,https://www.reddit.com/r/MachineLearning/comments/2jzw3l/guidance_needed_on_a_classification_task_using/,dalek2point3,1413989352,"Hello, I'm an MIT researcher, and machine learning methods are new to the kind of work that I do.  Here's what I'm looking to do -- I have scraped text data from a large sample of websites, essentially domain names like abc.com, pqr.org, rsv.net etc. I expect to have text data from many million such websites, and in the future I hope to expand this dataset to include data from the Common Crawl Project.

My task is to identify ""high growth"" potential websites from this data. I.e. i want to be able to identify potential startups in this sample and throw out personal websites, websites for SMEs, news websites, blogs etc. 

I have a few questions:
1. Am i right in thinking that this is a standard classification ML problem? what algorithms should I be looking at? Will I be alright using something like python's scikit-learn?

2. How would this task change if I wanted to classify websites into more types, say by industry (travel, holiday, finance etc).

3. Do you know existing papers, researchers who try to classify website text data? any references would be very helpful!",2,0,False,self,,,,,
204,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,1,2k0aga,self.MachineLearning,Learn a similarity measure with missing labels,https://www.reddit.com/r/MachineLearning/comments/2k0aga/learn_a_similarity_measure_with_missing_labels/,[deleted],1413996825,"Suppose I don't have labels that assign a class to each observation, but instead I have labels that assign relationships. Each observation has a few relationships that are labeled as belonging to the same class and a few that we know to belong to a different class. But the relationship matrix is very sparse and the real number of classes is unknown.
The objective is to find an embedding that minimizes the distance of observations belonging to the same class while maximizing the distance of samples belonging to different classes.

Is there a name for this problem? Any pointers welcome.",1,0,False,default,,,,,
205,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,2,2k0dtb,kaggle.com,Finding Chess Rating (Elo) Challenge,https://www.reddit.com/r/MachineLearning/comments/2k0dtb/finding_chess_rating_elo_challenge/,Antimoneyyy,1413998539,,2,8,False,http://b.thumbs.redditmedia.com/U92N7eMAM5cjJcKqhD8_AJ1fBHozV-nrxB5muj-9apI.jpg,,,,,
206,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,8,2k1k0n,cireneikual.wordpress.com,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 10,https://www.reddit.com/r/MachineLearning/comments/2k1k0n/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1414019893,,5,9,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
207,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,8,2k1nte,amplab.cs.berkeley.edu,"Big Data, Hype, the Media and Other Provocative Words to Put in a Title",https://www.reddit.com/r/MachineLearning/comments/2k1nte/big_data_hype_the_media_and_other_provocative/,davmre,1414021943,,5,40,False,http://b.thumbs.redditmedia.com/WWlMk9i8ygnM_3muZqDmno-H6fhR9QHUHz1XZgrb_EA.jpg,,,,,
208,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,9,2k1sfu,self.MachineLearning,Autonomic systems in industry?,https://www.reddit.com/r/MachineLearning/comments/2k1sfu/autonomic_systems_in_industry/,Divided_Pi,1414024442,"Anyone have any idea how widespread these system are or are not? Especially with 'the cloud' becoming so prevalent I would imagine self-managing system would start becoming popular but I'm not sure. Most of the papers I've come across so far are from IBM and are at least 5+ years old. 

I've seen some about Reinforcement Learning being used in relation to this, but haven't seen anything about who is actually using it or concrete applications. I would imagine Google has some level of this for managing its various data centers, but are smaller companies following suit?",1,1,False,self,,,,,
209,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,10,2k205p,thunderboltlabs.com,Getting Started with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2k205p/getting_started_with_machine_learning/,rohshall,1414028707,,0,0,False,http://b.thumbs.redditmedia.com/sQb2R2wWz7mjxYNcM-2aR-5ugN9uOeDzlqNE94VGReU.jpg,,,,,
210,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,10,2k20t7,harshtechtalk.com,Model Hyperparameter Tuning In Scikit Learn Using GridSearch,https://www.reddit.com/r/MachineLearning/comments/2k20t7/model_hyperparameter_tuning_in_scikit_learn_using/,harsh067,1414029071,,0,1,False,default,,,,,
211,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,11,2k24nw,self.MachineLearning,Anyone familiar with methods/algorithms for mutual nearest neighbor clustering?,https://www.reddit.com/r/MachineLearning/comments/2k24nw/anyone_familiar_with_methodsalgorithms_for_mutual/,motley2,1414031300,"
I've applied the common clustering algorithms (kmeans, dbscan, hierarchical) to small datasets (n&lt;1000) with the intention of classifying customer use cases. All of the data is continuous.  I found the results of k-means and other algorithms unsatisfying for one particular reason.  That reason being that if two points are mutually nearest neighbors (they are closer to each other than to any other point), then they should belong to the same cluster.  I realize that most algorithms are trying to minimize the intra-cluster distance and there is no requirement that two mutually nearest neighbors fall into the same cluster.  Have I overlooked a method that considers this requirement or something similar?  I know that this might not be the conventional usage of the term mutual nearest neighbors but I couldn't come up with a better description.  

I wrote an agglomerative script in matlab based on this idea and it seems to perform reasonably well for small data.  An added feature of this method is that the clusters cover the spread in the data rather than focusing on the most densely populated areas.  I realize this feature would be considered a flaw in most clustering applications.

Any insight would be appreciated.  Thanks.  ",4,0,False,self,,,,,
212,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,12,2k2ae4,self.MachineLearning,Fully Connected Neural Network on CIFAR / Imagenet,https://www.reddit.com/r/MachineLearning/comments/2k2ae4/fully_connected_neural_network_on_cifar_imagenet/,alexmlamb,1414034682,"Does anyone have results for CIFAR-100 or Imagenet using fully connected (standard) neural networks instead of convolutional neural networks.  I realize that this would generalize poorly and overfit because of the large number of parameters, but I'm somewhat curious about just how much worse they do relative to convnets and how that changes as the size of the images and the number of data points increases.  

On MNIST fully connected neural networks are still somewhat close to convnets, but this is probably because the images have fairly consistent positions and are always at the center of the frame.  The Deepface paper only used a single layer of convolutions and used locally connected layers after the max-pooling stage as the faces were pre-aligned to a canonical position.  ",1,0,False,self,,,,,
213,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,14,2k2n11,self.MachineLearning,Has anyone made a ranking list of universities for Machine learning by the number of papers published in top conferences?,https://www.reddit.com/r/MachineLearning/comments/2k2n11/has_anyone_made_a_ranking_list_of_universities/,[deleted],1414043617,"Something like this theory ranking http://projects.csail.mit.edu/dnd/ranking/

Microsoft academic search is not being updated for quite some time now. Anything else?",0,0,False,default,,,,,
214,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,17,2k2x39,self.MachineLearning,Any good hands-on resources on data visualzation?,https://www.reddit.com/r/MachineLearning/comments/2k2x39/any_good_handson_resources_on_data_visualzation/,DomMk,1414054699,"As the title says. Lately I've been trying to present my data in a readable and neat way, at least enough to publish on a blog, but I keep falling flat on my face. Everything turns out ugly, and never as cool as I see the graphs online. Are there any good books with datasets that you can work with?",3,4,False,self,,,,,
215,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,18,2k2xfq,googlepolicyeurope.blogspot.be,Google DeepMind acquires two deep learning spin-outs from Oxford University,https://www.reddit.com/r/MachineLearning/comments/2k2xfq/google_deepmind_acquires_two_deep_learning/,egrefen,1414055132,,16,54,False,http://b.thumbs.redditmedia.com/JDhXm1-aAErIKAy6LtMqxm5qPBn8kfPfR9CRz0DLrZE.jpg,,,,,
216,MachineLearning,t5_2r3gv,2014-10-23,2014,10,23,20,2k336t,self.MachineLearning,What inputs are required for what outputs with machine learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/2k336t/what_inputs_are_required_for_what_outputs_with/,KREPLAK,1414062095,"I am a biologist, relatively new to software development as a whole. Our lab has some ideas on inputting positive and negative experimental results into a machine learning algorithm with the goal of finding what exactly makes the results turn out a particular way. 

We have algorithms that design siRNAs, and want to improve them based on experimental data and metadata. 

Specifically, the goal is to find out how the structure of a genomic locus influences the ability of designed siRNAs to silence the particular gene on that locus. Subsequently, we want to be able to make better predictions about the siRNA designs that would be required to optimally silence that gene.  

The big question is what kind of data do we feed the algorithm, and then how does it handle that data and turn it into something useful? How does the input data result in a better siRNA design algorithm? What steps are involved?

Any help on this would be super useful for helping us understand what expertise we need to pull in for a grant, and plan our work for the next few years.

Thanks!",3,0,False,self,,,,,
217,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,1,2k3v17,touch.www.linkedin.com,Data Science is like teenage sex,https://www.reddit.com/r/MachineLearning/comments/2k3v17/data_science_is_like_teenage_sex/,capecamorin,1414081082,,1,0,False,default,,,,,
218,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,3,2k4cw1,dataelixir.com,"Data Elixir is a free weekly newsletter of data-related resources geared towards developers. If you have any suggestions, please leave them in the comments. Thanks!",https://www.reddit.com/r/MachineLearning/comments/2k4cw1/data_elixir_is_a_free_weekly_newsletter_of/,lonriesberg,1414090249,,0,6,False,default,,,,,
219,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,5,2k4ol2,harshtechtalk.com,Get Most Informative Features In Scikit Learn,https://www.reddit.com/r/MachineLearning/comments/2k4ol2/get_most_informative_features_in_scikit_learn/,harsh067,1414096214,,0,1,False,default,,,,,
220,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,7,2k54tu,research.microsoft.com,"""Foundations of Data Science"" (ebook, PDF)",https://www.reddit.com/r/MachineLearning/comments/2k54tu/foundations_of_data_science_ebook_pdf/,rasbt,1414104666,,7,79,False,default,,,,,
221,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,11,2k5ptv,deeplearning.net,New Theano Deep Learning Tutorial Book,https://www.reddit.com/r/MachineLearning/comments/2k5ptv/new_theano_deep_learning_tutorial_book/,kendrick90,1414116467,,5,22,False,default,,,,,
222,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,13,2k6359,self.MachineLearning,Supervised learning but in a different way?,https://www.reddit.com/r/MachineLearning/comments/2k6359/supervised_learning_but_in_a_different_way/,biggumz_,1414125096,"""Supervised learning is the machine learning task of inferring a function from labeled training data."" Is there a name for a problem where the function and output is given and the input has to be figured out? Can SVM be used for such things? tbh i have no idea if that question makes much sense",5,0,False,self,,,,,
223,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,15,2k6alb,self.MachineLearning,What are some SPECIFIC tasks people here feel would be advanced by machine learning?,https://www.reddit.com/r/MachineLearning/comments/2k6alb/what_are_some_specific_tasks_people_here_feel/,reddbullish,1414131451,,3,3,False,self,,,,,
224,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,17,2k6hz1,memkite.com,List of the 75 Most Popular Deep Learning Papers in the Deeplearning.University Bibliography,https://www.reddit.com/r/MachineLearning/comments/2k6hz1/list_of_the_75_most_popular_deep_learning_papers/,atveit,1414139447,,1,0,False,default,,,,,
225,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,17,2k6icl,blog.applysci.com,Google expands AI research,https://www.reddit.com/r/MachineLearning/comments/2k6icl/google_expands_ai_research/,ApplySci,1414139957,,0,0,False,http://b.thumbs.redditmedia.com/gigE74LMRwxAffUrI63F9QN_1RC-7I5THnHhsmk49qw.jpg,,,,,
226,MachineLearning,t5_2r3gv,2014-10-24,2014,10,24,23,2k75et,facebook.com,Yann LeCun on Michael Jordan's misinterpretation of his thoughts about deep learning,https://www.reddit.com/r/MachineLearning/comments/2k75et/yann_lecun_on_michael_jordans_misinterpretation/,[deleted],1414160796,,6,26,False,default,,,,,
227,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,0,2k7bbe,simplystatistics.org,An interactive visualization to teach about the curse of dimensionality (X-post from /r/statistics),https://www.reddit.com/r/MachineLearning/comments/2k7bbe/an_interactive_visualization_to_teach_about_the/,t_rex_tullis,1414164191,,4,0,False,http://b.thumbs.redditmedia.com/2_tpdPrfDzZ27L52e_EZaG-N2J2xkHARrehZO79c6VA.jpg,,,,,
228,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,0,2k7dcm,blog.aylien.com,Analyzing Text in Rapidminer - Part 2: Sentiment Analysis of Rotten Tomatoes Movie Reviews,https://www.reddit.com/r/MachineLearning/comments/2k7dcm/analyzing_text_in_rapidminer_part_2_sentiment/,MikeWally,1414165308,,0,17,False,http://b.thumbs.redditmedia.com/RNa_QM8kMFMCSVlMIIpJ2n7F2BpefBuDjr5v2AQHcxQ.jpg,,,,,
229,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,1,2k7k83,blog.urx.com,The Science of Crawl Part 2: Content Freshness,https://www.reddit.com/r/MachineLearning/comments/2k7k83/the_science_of_crawl_part_2_content_freshness/,jisaacso,1414169066,,0,1,False,http://b.thumbs.redditmedia.com/utyj4IbFRJw6bgNlb2Mw-si61GQWKErK3MPoOLhxtcI.jpg,,,,,
230,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,4,2k825b,self.MachineLearning,What do you think brain does when we instinctively learn something?,https://www.reddit.com/r/MachineLearning/comments/2k825b/what_do_you_think_brain_does_when_we/,[deleted],1414178562,"There are obviously two states of learning - first you learn something, and you have consciously do it. I mean, you have to manage every move and think about the next thing. Then, usually after you get a good sleep, some of that load is shifted to subconscious. The brain has obviously created a higher representation for the whole behaviour set and you do not have to think about it. It enables you to shift brainpower to try something new related to that task, finding better solutions etc.

The simple example is when person learns to use computer mouse. First they struggle but after some time, the subconscious parts of your brain do that for you. So while using the computer, you will just focus on what the computer is actually doing.

What I am wondering is - is there a name to it? I think it can be compared to some kind of dimensionality reduction but not quite. It's as if brain was creating one big representation of several different processes and sensory inputs bundled together - to be used in other bundles later. 

I am really interested on any papers or articles regarding this subject.
",0,1,False,default,,,,,
231,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,6,2k8j7a,self.MachineLearning,Music recommendation systems,https://www.reddit.com/r/MachineLearning/comments/2k8j7a/music_recommendation_systems/,aiwa-lee,1414187855,There has been increasing interest in this area over the years. Competitions such as the Netflix prize have been pushing a lot in that direction and a lot of methods have been applied including the Neural Networks approach by the Spotify team. What do you think is something that hasn't yet been implemented or which area of this type of mechanisms is more worth looking into and what do you think the newer developments will be turning to in the future?,11,2,False,self,,,,,
232,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,13,2k9eyg,self.MachineLearning,Good challenges website to learn?,https://www.reddit.com/r/MachineLearning/comments/2k9eyg/good_challenges_website_to_learn/,thalesmello,1414210284,"I'm looking forward to learn machine learning, but I'm not very good with reading books on the subject in order to do so.

I am quite experienced with programming, and managed to learn a new language (Clojure) only by doing online programming challenges, using 4clojure.com .

I would like to know if a similar website exists, with challenges and instant feedback, do that I can learn both machine learning and data analysis.",7,4,False,self,,,,,
233,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,13,2k9f0b,facebook.com,Yann LeCun on Michael Jordan's interview in IEEE Spectrum,https://www.reddit.com/r/MachineLearning/comments/2k9f0b/yann_lecun_on_michael_jordans_interview_in_ieee/,vkhuc,1414210323,,0,10,False,http://b.thumbs.redditmedia.com/e6ILjN3WrYN0Gmqa8wEPIYUZx9iE6qb3MsG3zHXtt2k.jpg,,,,,
234,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,14,2k9jjy,self.MachineLearning,Geoffrey Hinton will be doing an AMA in /r/MachineLearning on November 10 10AM PST,https://www.reddit.com/r/MachineLearning/comments/2k9jjy/geoffrey_hinton_will_be_doing_an_ama_in/,olaf_nij,1414214058,"I'm happy to announce University of Toronto Professor/Google Distinguished Researcher Geoffrey Hinton will be stopping by /r/MachineLearning on November 10 10AM PST for an AMA.

In keeping with tradition, a thread will be created before the official AMA time for those who won't be able to attend.

Special thanks to Oriol Vinyals for help organizing.
",16,209,False,self,,,,,
235,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,15,2k9mt6,dustinvtran.com,Comparing elastic net to stochastic gradient descent for GLMs,https://www.reddit.com/r/MachineLearning/comments/2k9mt6/comparing_elastic_net_to_stochastic_gradient/,nil-,1414217211,,6,5,False,default,,,,,
236,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,16,2k9q6q,self.MachineLearning,What are the pros and cons of submitting to arXiv vs waiting till the next conference?,https://www.reddit.com/r/MachineLearning/comments/2k9q6q/what_are_the_pros_and_cons_of_submitting_to_arxiv/,T_hank,1414220887,"Recently I have seen many papers on arXiv, which seem to be of good quality yet the authors have not (yet?) published to any mainstream publication. an example of this might be ""From Generic to Specific Deep Representations for Visual Recognition"" . 

What is the community's view on this? some particular questions:

1. What do the big publications think about breaking the double blind with an arXiv upload?

2. Are the amount and volume of reviews that one gets from people reading their work on arXiv worth the risk?

3. How does one get visibility for their work? Is the situation very different if you already are a known entity in the field, vs when you are a student looking for their first publication, say? Of course this would be different when submitting to something like ICLR where arXiv IS the platform, rather than just uploading at any random time of the year. I am asking about the latter.

4. A question that is personally important, if I apply for an internship,  would they rate an arXiv publication?",8,14,False,self,,,,,
237,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,18,2k9w1c,self.MachineLearning,One-class classification problem,https://www.reddit.com/r/MachineLearning/comments/2k9w1c/oneclass_classification_problem/,Bankreis,1414228794,"Greetings!
I'm currently trying to write a summary of pretty much all available one-class classification algorithms and would like to collect some input.

Anyone of you knows some awesome books/papers on the topic or knows some really awesome new algorithms?

Any information on the topic is appreciated!

I have this Thesis http://homepage.tudelft.nl/n9d04/thesis.pdf on the topic and I'm basically looking for more algorithms.

",4,9,False,self,,,,,
238,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,19,2k9xzj,onionesquereality.wordpress.com,Neighbourhood Gerrymandering: An Approach to Discriminative Metric Learning via Latent Structured Prediction,https://www.reddit.com/r/MachineLearning/comments/2k9xzj/neighbourhood_gerrymandering_an_approach_to/,alexeyr,1414231354,,0,3,False,http://b.thumbs.redditmedia.com/COVGm9pYA1DKTlC2ssuhFTLmVqjMarueFocSH6vuMQY.jpg,,,,,
239,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,21,2ka3n7,self.MachineLearning,Thought experiment.,https://www.reddit.com/r/MachineLearning/comments/2ka3n7/thought_experiment/,cajkamd,1414238576,"I'm pretty much a novice in machine learning, but this thought experiment occurred to me:

What if you had worn a gps recorder which records every 5 minutes, and is accurate to say 2 cm for the last 10 years, and you were going to use the last 10 years of data to predict your entire pattern of movement for the next day.  The most accurate predictor of this, would require a rule system which encodes your entire brain and its functions, and the entire environment that you interact with, including other people, and the laws of physics.  In fact, to be as accurate as possible it would have to simulate the entire world.  What if you turn on the news and you see a fire is near your house, and then you drive away?  Well if you had included the wheather patterns and simulated forests and the likelyhood of forest fires you would have been more likely to predict the pattern of movement.

So this would require ridiculously massive amount of computation.

So, when something that seems as simple as your gps coordinates turns out to require computing an whole world, how accurate can machine learning possibly be for many things?  Maybe this explains why stuff like facial recognition is so difficult.  Because to calculate the likelyhood of a face, you have to be able to take an image and take the 2 dimensional pixels and convert it to a 3d surface, which in order to solve correctly would require advanced knowledge of how our world works, including even the type of camera used, the exact location of the camera in 3d space relative to the face, and probably the genetics of how human heads are shaped, including ethnicity, skin tone, ect.  Its a ridiculously complicated problem, so to think that a machine learning algorithm will figure all that out is insane.",8,0,False,self,,,,,
240,MachineLearning,t5_2r3gv,2014-10-25,2014,10,25,22,2ka8sw,finanology.co,It's time for another revolution,https://www.reddit.com/r/MachineLearning/comments/2ka8sw/its_time_for_another_revolution/,[deleted],1414243893,,5,0,False,http://b.thumbs.redditmedia.com/Hvxwu5Xtf4l1ky36jRKVspXrihZEnE6r659bd-oGeqo.jpg,,,,,
241,MachineLearning,t5_2r3gv,2014-10-26,2014,10,26,1,2kaojs,self.MachineLearning,"""Support Vector Machines for Pattern Classification"" by Abe Shigeo",https://www.reddit.com/r/MachineLearning/comments/2kaojs/support_vector_machines_for_pattern/,rqlcdc,1414255183,"anyone can help finding a pdf to this book
http://www.springer.com/engineering/control/book/978-1-84996-097-7?otherVersion=978-1-84996-098-4",4,0,False,self,,,,,
242,MachineLearning,t5_2r3gv,2014-10-26,2014,10,26,7,2kbju2,self.MachineLearning,Modifying Existing Neural Network for multiclass classification,https://www.reddit.com/r/MachineLearning/comments/2kbju2/modifying_existing_neural_network_for_multiclass/,TrashQuestion,1414274677,"I have come across some matlab code that seems to make a neural network for m hidden nodes. I want to extend it to make a neural network for m hidden nodes and 10 outputs for multi-class classification, and any amount of hidden layers.

I have one big question about it though, it seems to make a weight matrix based on the number of samples, which seems completely wrong. Can anyone explain what is going on exactly? And how i might modify this for multi-class classification?

Here is the code, it is broken into two functions

One that simply calls the training of the neural net until it seems to have low error:

    function BackPropAlgo(Input, Output)
        %% Credits to: Anoop.V.S &amp; Lekshmi B G
        %% http://anoopacademia.wordpress.com/2013/09/29/back-propagation-algorithm-using-matlab/
    
        %STEP 1 : Normalize the Input 
        %Checking whether the Inputs needs to be normalized or not
        if (max(Input(:))&lt; 1 || max(Input(:))&gt;-1)
            %Need to normalize
            Norm_Input = Input / max(Input(:));
        else
            Norm_Input = Input;
        end
    
        %Checking Whether the Outputs needs to be normalized or not
        if(max(Output(:)) &lt; 1 || max(Output(:))&gt;-1)
        %Need to normalize
            Norm_Output = Output / max(Output(:));
        else
            Norm_Output = Output;
        end
    
        %Assigning the number of hidden neurons in hidden layer
        m = 2;
    
        %Find the size of Input and Output Vectors
        [l,b] = size(Input);
        [n,a] = size(Output);
    
        %Initialize the weight matrices with random weights 
        V = rand(l,m); % Weight matrix from Input to Hidden
        W = rand(m,n); % Weight matrix from Hidden to Output
        
        %Setting count to zero, to know the number of iterations
        count = 0;
    
        %Calling function for training the neural network
        [errorValue, delta_V, delta_W] = trainNeuralNet(Norm_Input,Norm_Output,V,W);
    
        %Checking if error value is greater than 0.1. If yes, we need to train the
        %network again. User can decide the threshold value
    
        while errorValue &gt; 0.05
            %incrementing count
            count = count + 1;
    
            %Store the error value into a matrix to plot the graph
            Error_Mat(count)=errorValue;
    
            %Change the weight metrix V and W by adding delta values to them
            W=W+delta_W;
            V=V+delta_V;
    
            %Calling the function with another overload.
            %Now we have delta values as well.
            count
            [errorValue, delta_V, delta_W]=trainNeuralNet(Norm_Input,Norm_Output,V,W,delta_V,delta_W);
        end
    
        %This code will be executed when the error value is less than 0.1
        if errorValue &lt; 0.05
            %Incrementing count variable to know the number of iteration
            count=count+1;
            %Storing error value into matrix for plotting the graph
            Error_Mat(count)=errorValue;
        end
    
        %Calculating error rate
        Error_Rate=sum(Error_Mat)/count;
    
        figure;
        %setting y value for plotting graph
        y=[1:count];
    
        %Plotting graph
        plot(y, Error_Mat);
    end

And another that actually performs the updates of the weights in the neural net:

    function [errorValue, delta_V, delta_W] = trainNeuralNet(Input, Output, V, W, delta_V, delta_W)
        %% Credits to: Anoop.V.S &amp; Lekshmi B G
        %% http://anoopacademia.wordpress.com/2013/09/29/back-propagation-algorithm-using-matlab/
        %Description : Function to train the network
        %Function for calculation (steps 4 - 16)
        %To train the Neural Network
        
        %Calculating the Output of Input Layer
        %No computation here.
        %Output of Input Layer is same as the Input of Input  Layer
        Output_of_InputLayer = Input;
    
        %Calculating Input of the Hidden Layer
        %Here we need to multiply the Output of the Input Layer with the
        %synaptic weight. That weight is in the matrix V. 
        Input_of_HiddenLayer = V' * Output_of_InputLayer;
    
        %Calculate the size of Input to Hidden Layer
        [m n] = size(Input_of_HiddenLayer);
    
        %Now, we have to calculate the Output of the Hidden Layer
        %For that, we need to use Sigmoidal Function
        Output_of_HiddenLayer = 1./(1+exp(-Input_of_HiddenLayer));
    
        %Calculating Input to Output Layer
        %Here we need to multiply the Output of the Hidden Layer with the -
        %synaptic weight. That weight is in the matrix W
        Input_of_OutputLayer = W'*Output_of_HiddenLayer;
    
        %Clear varables
        clear m n;
    
        %Calculate the size of Input of Output Layer
        [m n] = size(Input_of_OutputLayer);
    
        %Now, we have to calculate the Output of the Output Layer
        %For that, we need to use Sigmoidal Function
        Output_of_OutputLayer = 1./(1+exp(-Input_of_OutputLayer));
    
        %Now we need to calculate the Error using Root Mean Square method
        difference = Output - Output_of_OutputLayer;
        square = difference.*difference;
        errorValue = sqrt(sum(square(:)));
    
        %Calculate the matrix 'd' with respect to the desired output
        %Clear the variable m and n
        clear m n
    
        [n a] = size(Output); 
        for i = 1:n
            for j = 1:a
                d(i,j) =(Output(i,j)-Output_of_OutputLayer(i,j))*Output_of_OutputLayer(i,j)*(1-Output_of_OutputLayer(i,j));
            end
        end
    
        %Now, calculate the Y matrix
        Y = Output_of_HiddenLayer * d; %STEP 11
    
        %Checking number of arguments. We are using function overloading
        %On the first iteration, we don't have delta V and delta W
        %So we have to initialize with zero. The size of delta V and delta W will
        %be same as that of V and W matrix respectively (nargin - no of arguments)
    
        if nargin == 4
            delta_W=zeros(size(W));
            delta_V=zeros(size(V));
        end
    
        %Initializing etta with 0.6 and alpha with 1
        etta=0.6;
        alpha=1;
    
        %Calculating delta W
        delta_W= alpha.*delta_W + etta.*Y;%STEP 12
    
        %STEP 13
        %Calculating error matrix
        error = W*d;
    
        %Calculating d*
        clear m n
        [m n] = size(error);
    
        for i = 1:m
            for j = 1:n
                d_star(i,j)= error(i,j)*Output_of_HiddenLayer(i,j)*(1-Output_of_HiddenLayer(i,j));
            end
        end
    
        %Now find matrix, X (Input * transpose of d_star)
        X = Input * d_star';
    
        %STEP 14
    
        %Calculating delta V
        delta_V=alpha*delta_V+etta*X;
    end
",6,0,False,self,,,,,
243,MachineLearning,t5_2r3gv,2014-10-26,2014,10,26,8,2kbsfg,arxiv.org,Neural Turing Machines by Google DeepMind,https://www.reddit.com/r/MachineLearning/comments/2kbsfg/neural_turing_machines_by_google_deepmind/,galapag0,1414280242,,0,1,False,default,,,,,
244,MachineLearning,t5_2r3gv,2014-10-26,2014,10,26,14,2kcmpr,self.MachineLearning,help.... how do i install theano in anaconda 2.1,https://www.reddit.com/r/MachineLearning/comments/2kcmpr/help_how_do_i_install_theano_in_anaconda_21/,odysseus00,1414302833,,5,0,False,self,,,,,
245,MachineLearning,t5_2r3gv,2014-10-27,2014,10,27,4,2ke3sw,self.MachineLearning,Introductory paper on Hierarchical Bayes Multitask Learning?,https://www.reddit.com/r/MachineLearning/comments/2ke3sw/introductory_paper_on_hierarchical_bayes/,knownothingknowitall,1414350647,"I'm interested in doing a class project on multitask learning, but the dataset I want to use isn't large enough to do an approach using a neural net with multiple outputs. Therefore I'd like to do a Hierarchical Bayes approach where I impose a shared prior on the weights for all of the tasks. I was wondering if anyone has a recommendation for a simple, clear paper that discusses how to do this for regression, for example. The papers I've been able to find seem overly complex for what I'd like to do. ",2,12,False,self,,,,,
246,MachineLearning,t5_2r3gv,2014-10-27,2014,10,27,8,2key72,self.MachineLearning,On Sparse Distributed Representations and Catastrophic Forgetting,https://www.reddit.com/r/MachineLearning/comments/2key72/on_sparse_distributed_representations_and/,CireNeikual,1414367514,"Hello,

As some of you may know, I am running a series where I try to outperform Deepmind's Atari results. I use a different technique based on HTM (hierarchical temporal memory), but both Deepmind's technique and my technique fall under reinforcement learning.

So recently I posted about the HTM-RL architecture I was going to use. In it I decided to use a multi-layer perceptron to learn Q values from the outputs of HTM regions. In order for this to work, I realized that I would have to end up using an experience replay mechanism, much like Deepmind did.

Since I find experience replay inelegant, I wanted another approach to solve the problem of ""catastrophic forgetting"". Normally if you try to train a multilayer perceptron in an online fashion, newer values tend to destroy previous ones. Experience replay solves this to some extent by performing stochastic gradient descent on a window of past samples. This works well, but the memory requirements are enormous if you extend your replay window.

So I decided to take a hint from HTM, and I developed a new supervised online learning algorithm that does not suffer from catastrophic forgetting. It uses SDRs (sparse distributed representations) to represent its input in a field of nodes. Then, the output is simply a linear combination of the SDR that was formed. This makes it a universal function approximator, but at the same time the spatial pooling (input pattern generalization) greatly reduces catastrophic forgetting by reducing the strength of a backpropagation update depending on the output of the SDR cells.

I ran a simple experiment: I trained this ""SDR-net"" to learn a sine function of its input, by giving it all the samples in order. I then did the same with a MLP. When incrementing the X value by 0.001 at a time and trying to learn the corresponding Y value, the SDR-net got 270 times less error within 20PI!

But don't take my word for it! Source code can be found in my AI experimentation library, ""AILib"". It is named SDRRBFNetwork in the ""rbf"" directory. Link: [https://github.com/222464/AILib/tree/master/Source](https://github.com/222464/AILib/tree/master/Source)

This algorithm has quite possibly already been discovered. But in case it is not, I wanted to post about it here!

TL;DR: sparse distributed representations are awesome!",12,20,False,self,,,,,
247,MachineLearning,t5_2r3gv,2014-10-27,2014,10,27,16,2kg1yw,vencoavendingmachines.com,Buy Soda Vending Machines,https://www.reddit.com/r/MachineLearning/comments/2kg1yw/buy_soda_vending_machines/,senbolly,1414395686,,0,1,False,default,,,,,
248,MachineLearning,t5_2r3gv,2014-10-27,2014,10,27,20,2kgcfb,self.MachineLearning,Goal for machine learning noob?,https://www.reddit.com/r/MachineLearning/comments/2kgcfb/goal_for_machine_learning_noob/,mega_mon,1414407720,"I'm looking to set a 3-month goal of what I'll be able to do or know in the field of machine learning. This will be my first 3 months of attention I give to the subject. I'll be able to invest 6 hours weekly. 

What would your suggestions be for a suitable goal?",28,31,False,self,,,,,
249,MachineLearning,t5_2r3gv,2014-10-27,2014,10,27,22,2kgmo5,self.MachineLearning,Simple Vector Machine help?,https://www.reddit.com/r/MachineLearning/comments/2kgmo5/simple_vector_machine_help/,[deleted],1414416434,"Hello, Machine Learning! I want to know some simple and easy problem and write a source code about it (preferably Java, C and C++). I hope someone helps, thanks!",0,1,False,default,,,,,
250,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,0,2kgxic,wellecks.wordpress.com,Online LDA on Stack Overflow posts,https://www.reddit.com/r/MachineLearning/comments/2kgxic/online_lda_on_stack_overflow_posts/,ab5tract_type,1414423085,,1,14,False,http://b.thumbs.redditmedia.com/Hzutj2PikrEzlfSxYEBQe8pcYgtt12GvJsJBG4P5nhk.jpg,,,,,
251,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,1,2kh2gv,nbviewer.ipython.org,Do deep nets concentrate mass on 1d manifolds in practice? (iPython notebook),https://www.reddit.com/r/MachineLearning/comments/2kh2gv/do_deep_nets_concentrate_mass_on_1d_manifolds_in/,sieisteinmodel,1414425891,,4,20,False,default,,,,,
252,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,3,2khgpb,nuit-blanche.blogspot.com,"Videos of the Workshop on Algorithms for Modern Massive Data Sets, MMDS 2014 (x-post: r/CompressiveSensing )",https://www.reddit.com/r/MachineLearning/comments/2khgpb/videos_of_the_workshop_on_algorithms_for_modern/,compsens,1414433200,,0,2,False,http://b.thumbs.redditmedia.com/hHnJs-swuz4yoD3eRPqP7kXrpy7Gl0hyYNljLfqA5uk.jpg,,,,,
253,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,3,2khgx8,self.MachineLearning,Having some trouble understanding SVM dual form calculations,https://www.reddit.com/r/MachineLearning/comments/2khgx8/having_some_trouble_understanding_svm_dual_form/,MLApprentice,1414433310,"Hello,  
I'm trying to implement a SVM in python with numpy but I'm having a hard time doing so.  
I've been using this link: http://www.tristanfletcher.co.uk/SVM%20Explained.pdf to understand the theory.  
However I'm unsure what kind of multiplications they're applying to their matrices. I'm only familiar with the dot product in the context of 2 vectors, but I've read that standard matrix multiplications can be called dot products. So I think their ""."" operator refers to that. But then I'm unsure what kind of multiplications they're applying to the other matrices.  
Page 5 formula 1.13 to calculate H they do sum(sum(y\*y\*x.x)), how am I supposed to implement that?  I've looked at implementations where numpy.outer was used but I'm unsure why...    
  
I've also tried calculating them with 2 for loops, element by element, and then tried finding a formula that'd give the same result but I end up with sum( sum( Y\*(Y\*X.T).dot( (X.T) ) ) ) which doesn't look anything like what's written in the tutorial.  
So in conclusion I'd be very grateful for any help.  ",5,2,False,self,,,,,
254,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,3,2khhjq,techcrunch.com,Elon Musk thinks artificial intelligence is akin to summoning a demon,https://www.reddit.com/r/MachineLearning/comments/2khhjq/elon_musk_thinks_artificial_intelligence_is_akin/,[deleted],1414433626,,2,0,False,default,,,,,
255,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,3,2khkh7,nbviewer.ipython.org,A follow-up for a rec. discussion about correlation vs. covariance for PCA,https://www.reddit.com/r/MachineLearning/comments/2khkh7/a_followup_for_a_rec_discussion_about_correlation/,[deleted],1414435146,,1,1,False,default,,,,,
256,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,6,2ki5mk,self.MachineLearning,What are the most popular algorithms to model user behavior ?,https://www.reddit.com/r/MachineLearning/comments/2ki5mk/what_are_the_most_popular_algorithms_to_model/,muktabh,1414445709,"Please suggest popular (and/or state of the art) methods of modelling user behavior, say most liked music genres, frequencies of visiting concerts etc  ",3,0,False,self,,,,,
257,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,12,2kj778,bid2.berkeley.edu,BidData apparently holds the title (by a lot!) in a number of ML and computational tasks. Check it out!,https://www.reddit.com/r/MachineLearning/comments/2kj778/biddata_apparently_holds_the_title_by_a_lot_in_a/,in_the_fresh,1414465467,,0,4,False,http://b.thumbs.redditmedia.com/SEq4gSZBA3ZikRgQWxz3XEIw1QAWMm4BaDd96TRH27w.jpg,,,,,
258,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,13,2kjefr,mikelam.azurewebsites.net,How to write a simple recommendation algorithm in Neo4j,https://www.reddit.com/r/MachineLearning/comments/2kjefr/how_to_write_a_simple_recommendation_algorithm_in/,myclamm,1414469944,,0,4,False,default,,,,,
259,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,19,2kk2mz,self.MachineLearning,What is a draw from a Dirichlet Process?,https://www.reddit.com/r/MachineLearning/comments/2kk2mz/what_is_a_draw_from_a_dirichlet_process/,[deleted],1414493571,"I know a draw from a K-D Dirichlet distribution is a probability vector of dimension K. But still it's not clear what is a draw from a Dirichlet Process. What I understood is that a draw from a Dirichlet Process is a partitioning of a space of data points AND a probability measure over this partitioning. In other words, I supposed Dirichlet Process is a distribution over all possible probability measures that can be defined over a space of data points. ",12,3,False,default,,,,,
260,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,20,2kk5hz,self.MachineLearning,TF-IDF using SkLearn with variable corpus,https://www.reddit.com/r/MachineLearning/comments/2kk5hz/tfidf_using_sklearn_with_variable_corpus/,weirdtunguska,1414496319,"Given a large set of documents (book titles, for example), how to compare two book titles that are not in the original set of documents, or without recomputing the entire TF-IDF matrix?

For example,

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

book_titles = [""The blue eagle has landed"",
         ""I will fly the eagle to the moon"",
         ""This is not how You should fly"",
         ""Fly me to the moon and let me sing among the stars"",
         ""How can I fly like an eagle"",
         ""Fixing cars and repairing stuff"",
         ""And a bottle of rum""]

vectorizer = TfidfVectorizer(stop_words='english', norm='l2', sublinear_tf=True)
tfidf_matrix = vectorizer.fit_transform(book_titles) 

To check the similarity between the first and the second book titles, one would do

cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])

and so on. This considers that the TF-IDF will be calculated with respect all the entries in the matrix, so the weights will be proportional to the number of times a token appears in all corpus.

Let's say now that two titles should be compared, title1 and title2, that are not in the original set of book titles. The two titles can be added to the book_titles collection and compared afterwards, so the word ""rum"", for example, will be counted including the one in the previous corpus:

title1=""The book of rum""
title2=""Fly safely with a bottle of rum""
book_titles.append(title1, title2)
tfidf_matrix = vectorizer.fit_transform(book_titles)
index = tfidf_matrix.shape()[0]
cosine_similarity(tfidf_matrix[index-3:index-2], tfidf_matrix[index-2:index-1])

what is really impratical and very slow if documents grow very large or need to be stored out of memory. What can be done in this case? If I compare only between title1 and title2, the previous corpus will not be used.
",4,2,False,self,,,,,
261,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,20,2kk5xs,kevinmcalear.com,Kevin McAlear  Building Hater News,https://www.reddit.com/r/MachineLearning/comments/2kk5xs/kevin_mcalear_building_hater_news/,vamc19,1414496671,,2,14,False,http://a.thumbs.redditmedia.com/EVvsjmDQfXiPR1ukIs5oc2gM-SMpS3T1A2nRJ4XUSu8.jpg,,,,,
262,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,21,2kk9ts,papers.nips.cc,Zero-Shot Learning Through Cross-Modal Transfer (2013) [PDF],https://www.reddit.com/r/MachineLearning/comments/2kk9ts/zeroshot_learning_through_crossmodal_transfer/,galapag0,1414499776,,2,6,False,default,,,,,
263,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,22,2kkcp5,self.MachineLearning,Do electrical engineers use machine learning?,https://www.reddit.com/r/MachineLearning/comments/2kkcp5/do_electrical_engineers_use_machine_learning/,XeonExceldus,1414501837,"I'm hoping to become an one [3rd semester] and also very interested in machine learning, have worked on simple genetic algorithm projects out of curiosity and planning to take some online courses. However sometimes I feel like I'm wasting my time as it's pure CS and feel like we won't use any of it. What do you guys think?",5,0,False,self,,,,,
264,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,22,2kkg9l,self.MachineLearning,Handling missing values in real-time production ML model?,https://www.reddit.com/r/MachineLearning/comments/2kkg9l/handling_missing_values_in_realtime_production_ml/,srkiboy83,1414504130,"I'm building a Random Forest classifier in R, that makes predictions based on a number of unordered categorical/factor variables. The training set doesn't have any missing values/NAs.

Unfortunately, the real-time data, when the model is deployed in production, will have NAs from time to time. When testing in R, the model doesn't make a prediction if a row of data is passed with missing values for certain columns (it throws an &lt;NA&gt;).

My question is - how would I go about handling the missing values in production? Barring any sort of imputation, would explicitly modelling them, i.e. declaring them as an extra category for each variable make any sort of sense?

EDIT: Spelling.",4,1,False,self,,,,,
265,MachineLearning,t5_2r3gv,2014-10-28,2014,10,28,23,2kkn36,self.MachineLearning,"How many ""semesters"" of Statistics, Calculus, and Linear Algebra does one need to start working on Machine Learning?",https://www.reddit.com/r/MachineLearning/comments/2kkn36/how_many_semesters_of_statistics_calculus_and/,brouwjon,1414508042,"I've heard on this sub that these are the 3 branches of math that ML relies on. How much experience should you have with them before you can start educating yourself on ML? Is it enough to just go through an introduction to all three, or should you know them deeper than that? ",5,1,False,self,,,,,
266,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,2,2kl472,self.MachineLearning,State-of-the-art for object tracking in videos,https://www.reddit.com/r/MachineLearning/comments/2kl472/stateoftheart_for_object_tracking_in_videos/,jverm,1414516827,"I looked around, but didn't really got a clear view of what works best for object tracking in videos.

I know some methods with binary masks in deep learning for object detection in images. But videos have temporal information, so I think it would be really inefficient to use these methods frame by frame, especially because consecutive frames are almost the same.

EDIT: By detection, I mean bounding box determination.",10,7,False,self,,,,,
267,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,4,2klo17,self.MachineLearning,Manifold/subspace for neural network parameters?,https://www.reddit.com/r/MachineLearning/comments/2klo17/manifoldsubspace_for_neural_network_parameters/,speechMachine,1414526324,"I have come across a lot of work that tries to exploit the fact that data (images/speech) lie on a common manifold. This has led to a lot of work in the area of trying to learn classifier parameters (neural networks/kernel based methods) such that one could learn the underlying data manifold.

I am aware of methods like Joint Factor Analysis (http://www1.icsi.berkeley.edu/Speech/presentations/AFRL_ICSI_visit2_JFA_tutorial_icsitalk.pdf) and Subspace Gaussian Mixture Models (http://research.microsoft.com/pubs/80931/ubmdoc.pdf) in speaker recognition and speech recognition where parameters of the models themselves are thought to like in a low-dimensional subspace.

For learning efficient parametrization of Neural Networks has anyone ever posited a hypothesis that for a certain task or class of tasks it is possible that weights of multiple layers of a deep neural network could lie on a manifold, and perhaps have some structure to them? If yes any pointers to papers in that direction would be very helpful.",7,3,False,self,,,,,
268,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,8,2kmakv,self.MachineLearning,Help with Theano MLP Tutorial,https://www.reddit.com/r/MachineLearning/comments/2kmakv/help_with_theano_mlp_tutorial/,eric8558,1414537335,"I keep receiving the error 
 x: test_set_x[index * batch_size:(index + 1) * batch_size],
IndexError: invalid slice

My thinking is that the values being used to slice the numpy array are theano variables  is causing the issue. I've googled this and haven't found any answers so does anyone else have any ideas?",7,0,False,self,,,,,
269,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,8,2kmflr,tkrugg.github.io,Cross social network analysis for building quality data sets,https://www.reddit.com/r/MachineLearning/comments/2kmflr/cross_social_network_analysis_for_building/,hoykg,1414539836,,0,0,False,http://b.thumbs.redditmedia.com/vCgV31a-ZMfV60W5F4e4HwM7x14gbO5oxrS18to7oXs.jpg,,,,,
270,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,10,2kmoie,self.MachineLearning,"""Humanitarian"" work within ML?",https://www.reddit.com/r/MachineLearning/comments/2kmoie/humanitarian_work_within_ml/,KKLDM,1414544605,"I am a graduate student in computer science with a mathematical background. I am considering what to write about for my master's thesis, knowing that my choice will determine to some extent what I'll do in the future. A topic within ML is possible. I came to this subreddit looking for more information and browsing the top posts I read interesting AMAs with some leaders in the field. I felt like [the one with Yann Lecun](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun) was particularly interesting - but in my opinion it is disheartening to learn that a man like him is working for Facebook. A part of the post here by Max Welling (commenting on the controversy when Mark Zuckerberg visited the NIPS conference) expressed my thoughts exactly:

&gt;Is it desirable that big companies lure our best students away to improve ad-placement, where they could also have contributed to curing some of the horrible diseases that plague mankind? Perhaps not, but people have the right to make their own decisions.

Expanding on this: what opportunities do ML researchers have when it comes to things like ""curing horrible diseases"" or just doing work that is beneficial to humanity in some tangible way? Is anyone here working doing that kind of work? Can one expect to be able to enter that kind of a career after graduating (from a PhD program or otherwise)?

Thanks for any input.",39,24,False,self,,,,,
271,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,15,2knjkj,self.MachineLearning,Power Law Effect with Pitman-Yor Process,https://www.reddit.com/r/MachineLearning/comments/2knjkj/power_law_effect_with_pitmanyor_process/,[deleted],1414565086,"I know about PYP and that they produce power law effect and am looking at some of the graphs, like:

http://deliveryimages.acm.org/10.1145/1900000/1897842/figs/f1.jpg

and wondering how would you draw the PYP curve given a corpus of English words? The black curve corresponding to English Text is trivial, but the DP or PYP are not. Help me please!

I know that the discount parameter is the key element that distinguishes DP and PYP. But don't know how to relate the probability that PYP assigns to each cluster, to the word frequencies? A simple guess is that they both have a decaying behavior: where in english text there are a few number of words that occur a lot, and the majority of words only occur a few times. The same thing happens in PYP where the there are a few famous clusters (with lots of customers) and the the majority of the clusters only have a few customers. This decaying effect is faster in DP compared to PYP. But my main question is how one can associate words to clusters and draw a graph like what I pointed above?",0,0,False,default,,,,,
272,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,19,2knvzj,self.MachineLearning,What are some of the state of the art techniques for emotion recognition from images?,https://www.reddit.com/r/MachineLearning/comments/2knvzj/what_are_some_of_the_state_of_the_art_techniques/,[deleted],1414578839,,5,10,False,self,,,,,
273,MachineLearning,t5_2r3gv,2014-10-29,2014,10,29,20,2knzr3,self.MachineLearning,Advice for my ML project?,https://www.reddit.com/r/MachineLearning/comments/2knzr3/advice_for_my_ml_project/,mhf32,1414582594,"I'm in my final year of CS university and want to have a final year project related to ML. I thought about creating an app  that scan a table from a paper or other physical support, and create an excel table based from it, but I'm not sure if that's implementable in just one semester and if I still have to read a lot in order to implement it (we had a semester course about NN, I followed a course from EdX about AI back in 2012, and currently following an ML course given by Stanford on coursera). So I want to know your thought if that's feasible or not (in the course of 3 ~ 4 months). And if not, any great suggestion?",12,6,False,self,,,,,
274,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,3,2kp7d7,self.MachineLearning,What are the applications of NP Complete problems in Machine Learning domain?,https://www.reddit.com/r/MachineLearning/comments/2kp7d7/what_are_the_applications_of_np_complete_problems/,riyadparvez,1414607607,,0,1,False,self,,,,,
275,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,3,2kp99r,uarkive.uark.edu,A Comparison of Dropout and Weight Decay for Regularizing Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2kp99r/a_comparison_of_dropout_and_weight_decay_for/,galapag0,1414608555,,6,0,False,default,,,,,
276,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,5,2kpjr9,self.MachineLearning,What are current state of the art data clustering algorithms (or are they all equally good when the similarity metric is properly defined)?,https://www.reddit.com/r/MachineLearning/comments/2kpjr9/what_are_current_state_of_the_art_data_clustering/,[deleted],1414613847,,0,1,False,default,,,,,
277,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,5,2kpokh,self.MachineLearning,Looking for ML research opportunities at universities,https://www.reddit.com/r/MachineLearning/comments/2kpokh/looking_for_ml_research_opportunities_at/,[deleted],1414616185,"Hey everyone,

I am planning to pursue a research internship and a MS in Machine Learning.

I have 3yrs experience working as a developer (2.5 yrs of that as a remote dev for a NY firm). I have developed production apps in Java, Python/Django and JS, and worked on SysOps. I want to move to academia and learn ML. I am currently pursuing Prof. Andrew Ng's ML course on Coursera.

If you guys know of any opportunities(or are looking to fill a position yourself) please let me know.

Thanks in advance!

EDIT: The question is generic because I am new to ML and research and am looking for an entry level position. My plan right now: 1) Finish coursera course 2) start working on problem statements, kaggle &amp; delve deeply into certain techniques 3) concurrently apply for internships.


I am aware of several schools having good programs, but won't my lack of research experience be a problem? How can I best overcome it? Any tips on approaching researchers?",7,0,False,default,,,,,
278,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,6,2kpvln,self.MachineLearning,Could I use the If-Then-Else expression for surveys in social science?,https://www.reddit.com/r/MachineLearning/comments/2kpvln/could_i_use_the_ifthenelse_expression_for_surveys/,alexgotca,1414618992,"Hi guys,
I'm a researcher from Berlin and I'm interested in seeing how questionnaires and surveys that everyone uses in social science could improved/standardized by using the if-then-else format. The purpose would be to create algorithms that could link questions and answers and correlate data and maybe even find causality.
I'm new to machine learning, but from what I gather you guys may be the ones that could offer the best insights on this.

I mean, of course, I could use it but do you thing this would work? Do you think it's a good idea or has anyone used this before? Do you know if there's literature or research done on this topic? ",4,0,False,self,,,,,
279,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,7,2kq0b1,self.MachineLearning,Classifying sounds,https://www.reddit.com/r/MachineLearning/comments/2kq0b1/classifying_sounds/,markvp,1414621144,"Hi everyone,

What would be the best way to classify sounds? I only need to know if a sound is of one particular kind or not, e.g. drums or not, so slamming a locker and gunshots should not be in the category.

I was thinking about using the spectrum and feed the amplitudes, or the average amplitude of every octave, to a logistic regression. Are there better ways?",11,7,False,self,,,,,
280,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,7,2kq48s,self.MachineLearning,What are current state of the art data clustering algorithms?,https://www.reddit.com/r/MachineLearning/comments/2kq48s/what_are_current_state_of_the_art_data_clustering/,perceptronico,1414623043,"As we all know, clustering consists of a plethora of algorithms, eg. prototype algorithms, density-based, graph-theoretic, hierarchical, mixture models, and so on. I believe it's the data that usually dictates the choice of the algorithm (type, dimensionality etc.), and obviously, the appropriate similarity metric will determine the degree of success of any algorithm. 
Still, some algorithms perform better that others (for example, k-means probably won't accomplish good clustering if the ""classes"" aren't spherical, and DBSCAN probably will). I'd really like to know what are some of the most successful clustering algorithms to this day?",15,14,False,self,,,,,
281,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,9,2kqdfa,self.MachineLearning,Neural net input layer representations,https://www.reddit.com/r/MachineLearning/comments/2kqdfa/neural_net_input_layer_representations/,CarbonAvatar,1414627927,"How should I represent my neural net input, if each input node has 3 possible values?

For example, a position on a Go board may be empty, have a white stone, or have a black stone.

Should I use empty = 0, white = -1, black = 1? Or should I use 2 nodes for each input position? Something like empty = (1,0), white = (-1, -1), black = (-1, 1)?",7,0,False,self,,,,,
282,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,14,2kr8md,self.MachineLearning,What are the implications that would apply to Machine Learning if P=NP?,https://www.reddit.com/r/MachineLearning/comments/2kr8md/what_are_the_implications_that_would_apply_to/,TangerineX,1414646823,"The search for P=NP (or P!=NP) is not really making any headway, but I'm wondering how machine learning would be affected if someday we discover that P=NP. Would some learning be made faster? Would some learning be deprecated because direct solving methods would be faster? 

What if it was proved that P!=NP? Would that have any implications towards Machine Learning?",15,5,False,self,,,,,
283,MachineLearning,t5_2r3gv,2014-10-30,2014,10,30,17,2krkdv,self.MachineLearning,What does the No Free Lunch Theorem tell us?,https://www.reddit.com/r/MachineLearning/comments/2krkdv/what_does_the_no_free_lunch_theorem_tell_us/,dylanbyte,1414659000,"People often reference the NFLT when talking about machine learning algorithms, especially when talking about whether one technique is better than another.

I can't quite understand the implications, it seems as though it says that if you pick machine learning learning problems uniformly at random, then no algorithm will outperform chance.

What has this got to do with humans? If our problems were selected uniformly at random then we would never have even bothered creating the field, much less civilisation. 

To me the NFLT does not seem to contradict the existence of the supper dupper deep boosted support vector algorithm that dominates all other algorithms on problems of interested to humans,

Is this right?",10,16,False,self,,,,,
284,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,1,2ksio6,self.MachineLearning,Is it possible to perform sparse filtering for RNNs?,https://www.reddit.com/r/MachineLearning/comments/2ksio6/is_it_possible_to_perform_sparse_filtering_for/,gabjuasfijwee,1414684988,"Is it possible to perform sparse filtering ( http://ai.stanford.edu/~ang/papers/nips11-SparseFiltering.pdf )for recurrent neural networks? What other unsupervised methods for feature creation using RNNs are available? I am interesting in creating time-independent features from multivariate time-dependent data using some sort of neural network approach

thanks!",3,3,False,self,,,,,
285,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,2,2ksupd,self.MachineLearning,How to design a multilabel classification layer in neural networks?,https://www.reddit.com/r/MachineLearning/comments/2ksupd/how_to_design_a_multilabel_classification_layer/,[deleted],1414691128,"In ordinary classification a softmax may be used, where the maximum scoring class may be chosen. In case of multilabel classification how do we know how many labels ought to be chosen, when all the output nodes will have non-zero values. Is there some standard type of layer design, analagous to softmax for single label classification, that may be used off the shelf?",0,1,False,default,,,,,
286,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,5,2kth6d,technologyreview.com,"Google's Secretive DeepMind Startup Unveils a ""Neural Turing Machine"" | MIT Technology Review",https://www.reddit.com/r/MachineLearning/comments/2kth6d/googles_secretive_deepmind_startup_unveils_a/,xamdam,1414702315,,43,114,False,http://b.thumbs.redditmedia.com/VEx8RUpumCeOJthQQmcHvIg0N225NE3xgNLq3_wKntg.jpg,,,,,
287,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,6,2ktnc8,drivendata.org,"NLP modeling competition to help schools use resources better, $7.5K in prizes",https://www.reddit.com/r/MachineLearning/comments/2ktnc8/nlp_modeling_competition_to_help_schools_use/,isms_,1414705406,,0,3,False,http://b.thumbs.redditmedia.com/Szq9l-BpXGKzo7esqWDRlj1rfHdFxFzaemgcZZnHz6A.jpg,,,,,
288,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,6,2ktnpt,numenta.org,2014 Fall NuPIC Hackathon Outcome,https://www.reddit.com/r/MachineLearning/comments/2ktnpt/2014_fall_nupic_hackathon_outcome/,numenta,1414705603,,2,6,False,http://a.thumbs.redditmedia.com/QFFk2B7lz8BawEbFl9GgxfPdRQCYr-eYEGds71sVXn8.jpg,,,,,
289,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,9,2ku2fd,self.MachineLearning,How to design a multilabel classification layer in neural networks?,https://www.reddit.com/r/MachineLearning/comments/2ku2fd/how_to_design_a_multilabel_classification_layer/,donnaprima,1414713603,"In ordinary classification a softmax may be used, where the maximum scoring class may be chosen. In case of multilabel classification how do we know how many labels ought to be chosen, when all the output nodes will have non-zero values. Is there some standard type of layer design, analagous to softmax for single label classification, that may be used off the shelf?",7,3,False,self,,,,,
290,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,9,2ku78v,self.MachineLearning,Fun times with Stacked Generalization,https://www.reddit.com/r/MachineLearning/comments/2ku78v/fun_times_with_stacked_generalization/,tokatumoana,1414716336,"Hey everyone,
I'm working on different implementations of stacked generalization with Matlab, on a dataset which was reported to achieve higher performance scores than I'm actually getting. The issue is that with a stacking ensemble written by myself, that implemented by a postgraduate student, and finally one downloaded in a 3rd party toolbox get similarly low values. The dataset itself is highly unbalanced between 2 classes (99%/1% breakdown on average) and very noisy. Anyone had similar issues with weighted regression based meta-learners? ",0,2,False,self,,,,,
291,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,13,2kur49,fastml.com,Geoff Hinton's Dark Knowledge,https://www.reddit.com/r/MachineLearning/comments/2kur49/geoff_hintons_dark_knowledge/,yogthos,1414728200,,11,17,False,http://a.thumbs.redditmedia.com/rYGZ5rRVOfpJv7ExM9xmBlpli31DIgqi8lEIipyL7h8.jpg,,,,,
292,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,14,2kuz40,self.MachineLearning,Dee Jay ML,https://www.reddit.com/r/MachineLearning/comments/2kuz40/dee_jay_ml/,[deleted],1414734209,"In my experience as a DJ's reputation grows, his ego follows and his ability to satisfy the crowd diminishes as he finds himself playing music that the unawashed masses ought to appreciate rather than what they actually like. So it got me wondering, if you were to put together a machine learning solution to this problem how would you go about it? Is a real time crowd feedback approach feasible? ",2,0,False,self,,,,,
293,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,19,2kvevo,self.MachineLearning,Looking for open ideas,https://www.reddit.com/r/MachineLearning/comments/2kvevo/looking_for_open_ideas/,no_porner,1414751350,"I am pursuing a project on Tracking learning and detection in a video stream. For that, I have read the paper titled "" Tracking-Learning-Detection"" (http://epubs.surrey.ac.uk/713800/1/Kalal-PAMI-2011%281%29.pdf). But as this paper was written in 2011, most of the future work mentioned here has been accomplished. I would be grateful if anyone can guide me on what all open ideas I can still work on? Any help is appreciated.
Thank you.

Edit: Any help guys? ",2,1,False,self,,,,,
294,MachineLearning,t5_2r3gv,2014-10-31,2014,10,31,22,2kvtwk,wearabletechnologies.co.uk,"Finally Google Understands Humans, Thanks To Machine Learning",https://www.reddit.com/r/MachineLearning/comments/2kvtwk/finally_google_understands_humans_thanks_to/,wearables,1414763525,,0,0,False,http://b.thumbs.redditmedia.com/g4LkwYf4HRwrOjujT9_xF_yL6jyHfBLXbdOopxM1RJg.jpg,,,,,
