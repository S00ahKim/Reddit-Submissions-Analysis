,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,9,abda7q,self.MachineLearning,How to become a data analyst engineer in a few months?,https://www.reddit.com/r/MachineLearning/comments/abda7q/how_to_become_a_data_analyst_engineer_in_a_few/,niki_niki123,1546303679,[removed],0,1,False,self,,,,,
1,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,12,abeed6,youtube.com,[P] AI Rewind 2018,https://www.reddit.com/r/MachineLearning/comments/abeed6/p_ai_rewind_2018/,everydAI,1546312411,,0,1,False,https://a.thumbs.redditmedia.com/WC1FyAfncx_JQK4-Jmf0jirNGADK8R4J7zoq-qvlbE4.jpg,,,,,
2,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,13,aber0d,arxiv.org,"[Research] Accurate, Data-Efficient, Unconstrained Text Recognition with Convolutional Neural Networks",https://www.reddit.com/r/MachineLearning/comments/aber0d/research_accurate_dataefficient_unconstrained/,dmyb,1546315403,,2,1,False,default,,,,,
3,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,13,abesyt,arxiv.org,[R] A Geometric Theory of Higher-Order Automatic Differentiation,https://www.reddit.com/r/MachineLearning/comments/abesyt/r_a_geometric_theory_of_higherorder_automatic/,Kickuchiyo,1546315894,,19,1,False,default,,,,,
4,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,14,abfm73,self.MachineLearning,Constrainted optimization and SVM,https://www.reddit.com/r/MachineLearning/comments/abfm73/constrainted_optimization_and_svm/,koriavinash,1546322310,[removed],0,1,False,self,,,,,
5,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,18,abgx4v,self.MachineLearning,How to Import our own module to google colab notebook,https://www.reddit.com/r/MachineLearning/comments/abgx4v/how_to_import_our_own_module_to_google_colab/,mebpin,1546335086,[removed],0,1,False,self,,,,,
6,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,19,abhb5v,web.mit.edu,An Idiot  s guide to Support vector machines ( SVMs ),https://www.reddit.com/r/MachineLearning/comments/abhb5v/an_idiot_s_guide_to_support_vector_machines_svms/,_joermungandr_,1546339635,,2,1,False,default,,,,,
7,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,19,abhc5q,self.MachineLearning,Hypothesis testing,https://www.reddit.com/r/MachineLearning/comments/abhc5q/hypothesis_testing/,bhavykhatri,1546339938,[removed],0,1,False,self,,,,,
8,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,20,abhllg,self.MachineLearning,[D] Where to find all the machine learning software?,https://www.reddit.com/r/MachineLearning/comments/abhllg/d_where_to_find_all_the_machine_learning_software/,dododododow,1546342934,"Hello

i want to buy desktop software that can do AI Style transfer and another software that can do AI Inpainting, similar than the nvidia Inpainting demo. 

&amp;#x200B;

I dont know where to find this, its quite obscure and secret.

can anyone give me name of softwares avaible for the public that use this kind of handy tech ? (Machine Learning, AI, neural networks ect..) 

&amp;#x200B;

AI InPainting:

normaly Adobe are supposed to have an RTX exclusive inpainting feature but it never showed up. 

&amp;#x200B;

Style Transfer:

i know of topaz lab ReMix but the styles are default, you cant create a new one with a source image, so its pointless.

i know that there's a lot of website that use this feature on the cloud, but id prefer to have my own desktop apps and do my own calculation.

&amp;#x200B;

AI Upscaling:

AI gigapixel from Topaz labs work really well for me.

&amp;#x200B;

AI Denoizing:

AI Denoizer From Nvidia with the blender D-Noise free addon working really well for me, and its free to use with blender.

&amp;#x200B;

any advices? Im looking to upgrade my artist workflow

Thanks",10,1,False,self,,,,,
9,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,21,abhpu7,self.MachineLearning,Reinforced learning algorithm for perfect information game (Mancala),https://www.reddit.com/r/MachineLearning/comments/abhpu7/reinforced_learning_algorithm_for_perfect/,leaveAtTen,1546344268,"Hey, 

I'm undertaking a project where I pit a agent coded using traditional search methods and decision rule against an agent trained through ML in a game of Mancala to see if the ML based agent is able to beat the first AI.

So far, I've implemented the game and made the AI using MCTS + minimax + a-b pruning (in python). 

I haven't been formally taught ML, however after a bit of research, it seems like reinforced learning is the best approach to training my agent.

I'd really appreciate any input on this, whether you think it is the best approach, any links, beginner guides, lectures on the topic or really anything you think could get me going.

Thanks!

&amp;#x200B;",0,1,False,self,,,,,
10,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,22,abi2sq,self.MachineLearning,Training neural networks,https://www.reddit.com/r/MachineLearning/comments/abi2sq/training_neural_networks/,korokage,1546348171,"I'm looking for a blog, book, paper or course that talks about the intricacies of training neural networks. Like when a certain hyperparameter should be used instead of an other, methodologies that should be tried to improve test accuracy etc.

Any help would be appreciated. Thanks!",0,1,False,self,,,,,
11,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,22,abi7a1,self.MachineLearning,How to add more emphasis on end columns in CNN?,https://www.reddit.com/r/MachineLearning/comments/abi7a1/how_to_add_more_emphasis_on_end_columns_in_cnn/,jsther,1546349452,[removed],0,1,False,self,,,,,
12,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,22,abia1x,self.MachineLearning,[D] Improving NN accuracy using multiple lower accuracy networks,https://www.reddit.com/r/MachineLearning/comments/abia1x/d_improving_nn_accuracy_using_multiple_lower/,ViktorLarkin,1546350269,"I just got this idea and maybe I'm missing something obvious.

So, usually it takes exponentially more training time to increase a network's accuracy. Suppose we can train a network to 90% accuracy for 1 unit of time and training it to 97% would take 10 times longer.

The idea is to train 3 different networks to 90% accuracy, then do the following: first we take the outputs of 2 networks (boolean), if they match, it would be the final output, if they don't, we take the output of the third network. Our accuracy would then be:

    The outputs matched (either both networks are right or wrong): 0.9 * 0.9 * 1 + 0.1 * 0.1 * 0 = 0.81
    The outputs didn't match (one of the networks is wrong and we ask the third one): 0.9 * 0.1 * 0.9 + 0.1 * 0.9 * 0.9 = 0.162
    0.81 + 0.162 = 0.972 = 97.2% accuracy

So we get 97.2% accuracy for 3 units of time compared to 97% for 10 units of time.",21,1,False,self,,,,,
13,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,22,abicca,self.MachineLearning,[R] [ICLR 2019] Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm,https://www.reddit.com/r/MachineLearning/comments/abicca/r_iclr_2019_pertensor_fixedpoint_quantization_of/,fixed-point-learning,1546350926,"Sharing my newly accepted paper to ICLR 2019: [https://openreview.net/forum?id=rkxaNjA9Ym](https://openreview.net/forum?id=rkxaNjA9Ym)

Also posted on arXiv: [https://arxiv.org/abs/1812.11732](https://arxiv.org/abs/1812.11732)

**Abtract:**  The high computational and parameter complexity of neural networks makes their training very slow and difficult to deploy on energy and storage-constrained comput- ing systems. Many network complexity reduction techniques have been proposed including fixed-point implementation. However, a systematic approach for design- ing full fixed-point training and inference of deep neural networks remains elusive. We describe a precision assignment methodology for neural network training in which all network parameters, i.e., activations and weights in the feedforward path, gradients and weight accumulators in the feedback path, are assigned close to minimal precision. The precision assignment is derived analytically and enables tracking the convergence behavior of the full precision training, known to converge a priori. Thus, our work leads to a systematic methodology of determining suit- able precision for fixed-point training. The near optimality (minimality) of the resulting precision assignment is validated empirically for four networks on the CIFAR-10, CIFAR-100, and SVHN datasets. The complexity reduction arising from our approach is compared with other fixed-point neural network designs. 

&amp;#x200B;

**TL;DR:**  We analyze and determine the precision requirements for training neural networks when all tensors, including back-propagated signals and weight accumulators, are quantized to fixed-point format. ",4,1,False,self,,,,,
14,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,22,abiciv,self.MachineLearning,Learning Machine Learning,https://www.reddit.com/r/MachineLearning/comments/abiciv/learning_machine_learning/,ieshaan12,1546350977,[removed],0,1,False,self,,,,,
15,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,23,abihxi,self.MachineLearning,Is there a deep learning framework for large models in a GPU?,https://www.reddit.com/r/MachineLearning/comments/abihxi/is_there_a_deep_learning_framework_for_large/,CyberDainz,1546352437,[removed],0,1,False,self,,,,,
16,MachineLearning,t5_2r3gv,2019-1-1,2019,1,1,23,abij8c,self.MachineLearning,Book: Machine Learning: a Probabilistic Perspective,https://www.reddit.com/r/MachineLearning/comments/abij8c/book_machine_learning_a_probabilistic_perspective/,andrea_manero,1546352774,[removed],0,1,False,self,,,,,
17,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,0,abj1mc,self.MachineLearning,[D] Notes on why deep neural networks are able to generalize well,https://www.reddit.com/r/MachineLearning/comments/abj1mc/d_notes_on_why_deep_neural_networks_are_able_to/,invertedpassion,1546357237,"Hello,

I spent a good part of today reading on why deep neural networks are able to generalize well.  Based on my reading, I have made some notes. I'm new to this, so I'd appreciate if I can have community members' comments / discussion on the same. In particular, I'd love to know if I got something wrong or if someone is aware of a significant result that I missed.

Here are my notes:

1/ First major insight was that the minibatch of data for gradient descent actually helps in generalization on unseen data.   **Gradients of minibatch of data that are specific about that batch cancel over multiple runs and what remains is gradients that are generally applicable**.

2/ It is known that [neural networks are universal function approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem). That is, given a function they can approximate that function with arbitrary accuracy.  But now I think that's not an interesting result (of approximating a function). Even a database can do that. What's interesting is that they give good answers on *unseen* data.

3/ It is a mystery how that happens but probably the answer lies in not as much about neural networks but the types of datasets we have in the natural world and what problems we use neural networks for.

4/ Natural world is full of information, one 1000x1000 px photo has 1 million bits but when we see it, we either see it as a cat or a dog.  Effectively, we ""throw out"" a lot of information to do whatever we want to do. To classify a photo, our brain convert a log(1 million) bits into log(1) bit and the task of a neural network is to find the mapping that ""forgets"" or ""throws"" all the information irrelevant to the task while only retaining info that's useful to us.

5/ Since this log(1 million) to log(1) is a many-to-one function, neural networks might be a really good model for approximating these functions.  **Different layers might be throwing away irrelevant information while keeping only the relevant info**.

6/ This is suggested by two papers/videos I saw today.  One was on information bottleneck: [https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)

7/ The other one is how **errors introduced in early layers tend to vanish in higher layers**: [http://www.offconvex.org/2018/02/17/generalization2/](http://www.offconvex.org/2018/02/17/generalization2/)

8/ In effect, **neural networks are lossy compression algorithms** that compress inputs as much as they can while retaining as much info as possible about the task at hand (classification, prediction)  This helps networks generalize as data-specific noise gets ignored in deep networks.

9/ Okay, so we know what deep networks \*might\* be doing but the question is how training via gradient descent is able to find the right set of parameters that do this compression.  Given the millions of weights and biases, it seems the problem is of finding the needle in the haystack.

10/ I honestly don't know and research community also (probably) doesn't know. But there are hints.  One is related to the earlier suggestion of many-to-one mapping of input to output in real-world tasks. This means that t**here may be more than 1 set of parameters that do the job equally well**

11/ So stochastic gradient descent might not be finding the ""perfect"" set of parameters but it may not matter. **The problem we want to solve through neural networks may get solved by many sets of params** and SGD may find one of them.

12/ In fact, empirically the landscape of **loss function for neural networks on ""natural"" problems (of image classification, etc.) seems to have a ""flat"" minima.**  

&amp;#x200B;

![img](91ysxtolzt721 ""Image via: https://www.offconvex.org/2018/02/17/generalization2/"")

13/ So the *same* function we're seeking might be parameterized by many parameters.   On top of this, what helps is that **in a big deep network there exist many, many subnetworks. And, just by pure luck, one or more of them might be better positioned to seek that landscape via SGD.** This is explored in the lottery hypothesis: [https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)

14/ I understand how the width of the network may help in exploring what information to throw (by setting weights to zero) and what information to use, but I'm not sure the role of depth.  **My hunch says the utility of depth is related to how stochastic gradient descent works. Do you agree?**

15/ Perhaps, just perhaps, different layers (depth) helps SGD reduce loss in steps by focusing on few dimensions at once v/s if it is just one very wide layer, SGD has too many dimensions to seek at once.  But I don't really know.

16/ What's fascinating to me is the how easily researchers drop neural networks as function approximators anywhere and everywhere. This just makes it more worthwhile to study the dynamics of deep networks.  If you want to dive in, here's a great tutorial: [https://www.youtube.com/watch?v=r07Sofj\_puQ](https://www.youtube.com/watch?v=r07Sofj_puQ)

&amp;#x200B;

That's all! Did I miss anything? Did I go wrong somewhere? I'd appreciate any inputs that can help build us a better intuition of what might be happening under the hood.

&amp;#x200B;

PS: I tweeted about this as well, but I don't have many friends on Twitter who may provide a perspective on my notes or catch my errors.  That's why I started a discussion on this subreddit. ",55,1,False,self,,,,,
18,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,0,abj6x9,i.redd.it,Percia Judicial - PENHORA de Faturamento V,https://www.reddit.com/r/MachineLearning/comments/abj6x9/percia_judicial_penhora_de_faturamento_v/,JamurGerloff,1546358365,,0,1,False,https://a.thumbs.redditmedia.com/VsYciTwgJqdM40WMleW4419kG-4xK2odtcQ7bgtiEC8.jpg,,,,,
19,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,1,abjgwv,self.MachineLearning,Xavier Initialization with Batch Normalization Redundant?,https://www.reddit.com/r/MachineLearning/comments/abjgwv/xavier_initialization_with_batch_normalization/,onenerdbird,1546360434,[removed],0,1,False,self,,,,,
20,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,2,abjsms,self.MachineLearning,GitHub Resource of Papers with Code.,https://www.reddit.com/r/MachineLearning/comments/abjsms/github_resource_of_papers_with_code/,111kaushal,1546362706,[removed],0,1,False,self,,,,,
21,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,2,abjxf8,self.MachineLearning,[D] Question on a DL network's capacity to remember. Can it be trained forever as new data comes in?,https://www.reddit.com/r/MachineLearning/comments/abjxf8/d_question_on_a_dl_networks_capacity_to_remember/,e_to_the_power_lnx,1546363607,"Let's assume we've trained a DL network with some data. Let's say we're going to get more data on a regular basis from a similar distribution in the future and we'd be training the network on that data as well. 

1) Can the network be trained forever? Or would more layers need to be added at some point so that it can maintain its accuracy and/or other performance stats?

2) On a related note, whenever newer data comes in, I  believe that a validation set must be constructed from both the old and the new data correct? Same applies for the test set too?",5,1,False,self,,,,,
22,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,2,abk5am,foxbusiness.com,Breaking Down China's Unstoppable AI Factory,https://www.reddit.com/r/MachineLearning/comments/abk5am/breaking_down_chinas_unstoppable_ai_factory/,Jeebzus2014,1546365064,,0,1,False,default,,,,,
23,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,3,abkdux,self.MachineLearning,"PreProccesing isn't about dumping a lot of values through scikit-learn, right?",https://www.reddit.com/r/MachineLearning/comments/abkdux/preproccesing_isnt_about_dumping_a_lot_of_values/,Jandevries101,1546366605,[removed],0,1,False,self,,,,,
24,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,3,abkf5k,arxiv.org,[R] Disentangling Controllable and Uncontrollable Factors of Variation by Interacting with the World,https://www.reddit.com/r/MachineLearning/comments/abkf5k/r_disentangling_controllable_and_uncontrollable/,downtownslim,1546366843,,1,1,False,default,,,,,
25,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,3,abkice,self.MachineLearning,Common problems to network not working?,https://www.reddit.com/r/MachineLearning/comments/abkice/common_problems_to_network_not_working/,Jandevries101,1546367413,"Hi,

&amp;#x200B;

I came as most of us have experienced, your network isn't working. 

&amp;#x200B;

So i am not really looking into solutions, since that would be way too much work for you, however i would like to know common causes... For example where to look for difficulties? I am using RL DDPG and worked a lot about reward designs etc, but still i really do feel like something isn't working correct in my network, i tried taking a look at my tensorboard, however i have no clue what it shows.....

&amp;#x200B;

&amp;#x200B;

![video](551yrjv2tu721 ""this is a video of the tensorboard showing the network"")

&amp;#x200B;

So i tried checking my memory of my DDPG and tried switching some hyperparameters and activation functions, but i have no clue where to look more for common issues..

&amp;#x200B;

my code is: [https://pastebin.com/umkxx1zv](https://pastebin.com/umkxx1zv)

&amp;#x200B;

Thanks for helping in advance,

&amp;#x200B;

Jan",0,1,False,self,,,,,
26,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,3,abkmww,self.MachineLearning,Where to find research papers related to AI/Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/abkmww/where_to_find_research_papers_related_to_aideep/,sainimohit23,1546368218,[removed],0,1,False,self,,,,,
27,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,6,abm9zc,reddit.com,DCGAN for MNIST,https://www.reddit.com/r/MachineLearning/comments/abm9zc/dcgan_for_mnist/,CreativeElephant,1546378589,,0,1,False,default,,,,,
28,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,7,abmhwj,self.MachineLearning,[P] How would I go about making a flow chart for a neural network for a proof of concept?,https://www.reddit.com/r/MachineLearning/comments/abmhwj/p_how_would_i_go_about_making_a_flow_chart_for_a/,ChuckyIves,1546380034,"I need to make a flow chart for a neural network for a project Im working on, but I dont know how to approach it. It one of those things thats seems simple but for some reason I cant figure out what I need to show.

Any help is appreciated, thank you.",2,1,False,self,,,,,
29,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,7,abmjcg,medium.com,[D] On Writing Custom Loss Functions in Keras,https://www.reddit.com/r/MachineLearning/comments/abmjcg/d_on_writing_custom_loss_functions_in_keras/,bantou_41,1546380267,,0,1,False,https://b.thumbs.redditmedia.com/njg5pE8Bbq8WxeAmLFzZ2ucs_kzu0UiQbrb1Sddr4sA.jpg,,,,,
30,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,7,abmm4d,self.MachineLearning,Coding existing computer vision descriptors into a CNN,https://www.reddit.com/r/MachineLearning/comments/abmm4d/coding_existing_computer_vision_descriptors_into/,matchamojito,1546380760,[removed],0,1,False,self,,,,,
31,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,7,abms4w,self.MachineLearning,Accurate print digit recognition,https://www.reddit.com/r/MachineLearning/comments/abms4w/accurate_print_digit_recognition/,FreezedPlainWater,1546381865,[removed],0,1,False,https://b.thumbs.redditmedia.com/-hMxULPJjwOi2Y_zZjoubGgdetQf54xIzmSZAfl3Gtk.jpg,,,,,
32,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,7,abmxpv,towardsdatascience.com,Launching and Scaling Data Science Teams,https://www.reddit.com/r/MachineLearning/comments/abmxpv/launching_and_scaling_data_science_teams/,iandmacomber,1546382876,,0,1,False,default,,,,,
33,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,8,abnfux,self.MachineLearning,[D] unusual language text clustering / classification,https://www.reddit.com/r/MachineLearning/comments/abnfux/d_unusual_language_text_clustering_classification/,dzflow,1546386200,"Briefing: What would be your approach to clustering similar text from unusual language.

Details: I'm scraping a classified ads website trying to group similar ads(same  product). The text has often misspelling, written in 2 languages (a bit  of kind of 1ee7) and some text written phonetically in different  alphabet (ex. Dinsh for  or velosiped for ) or different  dialect.

Then how would you proceed to manage such an unpredictable input?",5,1,False,self,,,,,
34,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,8,abnhas,self.MachineLearning,Ways to create a list of similar questions from a single question?,https://www.reddit.com/r/MachineLearning/comments/abnhas/ways_to_create_a_list_of_similar_questions_from_a/,rocco20,1546386452,[removed],0,1,False,self,,,,,
35,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,8,abniw3,arxiv.org,[R][1806.05086] Group Equivariant Capsule Networks,https://www.reddit.com/r/MachineLearning/comments/abniw3/r180605086_group_equivariant_capsule_networks/,IborkedyourGPU,1546386746,,1,1,False,default,,,,,
36,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,10,aboarq,self.MachineLearning,[D] Multi-agent/Cooperative agents and vehicles,https://www.reddit.com/r/MachineLearning/comments/aboarq/d_multiagentcooperative_agents_and_vehicles/,Maplernothaxor,1546392029,"Has anyone explored this area much in the past? Could any recommend some interesting reading related to it?

Much thanks",1,1,False,self,,,,,
37,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,10,abofkj,self.MachineLearning,"It is possible to predict weather by dqn model ? gru, lstm, dqn, .......which one model is the best for predicting weather ?",https://www.reddit.com/r/MachineLearning/comments/abofkj/it_is_possible_to_predict_weather_by_dqn_model/,asda43asdf23423,1546392954,[removed],0,1,False,self,,,,,
38,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,10,aboibp,self.MachineLearning,Is there any comparison between Tensorflow Lite and caffe2 regarding performance in Android phones ?,https://www.reddit.com/r/MachineLearning/comments/aboibp/is_there_any_comparison_between_tensorflow_lite/,MohamedRashad,1546393499,[removed],0,1,False,self,,,,,
39,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,12,abp9u6,self.MachineLearning,[R] Deep Learning for Human Affect Recognition: Insights and New Developments,https://www.reddit.com/r/MachineLearning/comments/abp9u6/r_deep_learning_for_human_affect_recognition/,pr0u,1546398977,"Id like to share my new review article on deep learning in affect recognition.

**Links**: [IEEE Xplore](https://ieeexplore.ieee.org/document/8598999) [ResearchGate](https://www.researchgate.net/publication/329980556_Deep_Learning_for_Human_Affect_Recognition_Insights_and_New_Developments/)

**Abstract**: Automatic human affect recognition is a key step towards more natural human-computer interaction. Recent trends include recognition in the wild using a fusion of audiovisual and physiological sensors, a challenging setting for conventional machine learning algorithms. Since 2010, novel deep learning algorithms have been applied increasingly in this field. In this paper, we review the literature on human affect recognition between 2010 and 2017, with a special focus on approaches using deep neural networks. By classifying a total of 950 studies according to their usage of shallow or deep architectures, we are able to show a trend towards deep learning. Reviewing a subset of 233 studies that employ deep neural networks, we comprehensively quantify their applications in this field. We find that deep learning is used for learning of (i) spatial feature representations, (ii) temporal feature representations, and (iii) joint feature representations for multimodal sensor data. Exemplary state-of-the-art architectures illustrate the progress. Our findings show the role deep architectures will play in human affect recognition, and can serve as a reference point for researchers working on related applications.",9,1,False,self,,,,,
40,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,12,abpdez,arxiv.org,[R] Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?,https://www.reddit.com/r/MachineLearning/comments/abpdez/r_overparameterized_nonlinear_learning_gradient/,ecstasyogold,1546399744,,6,1,False,default,,,,,
41,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,12,abphp7,link.medium.com,How would we find a better activation function than ReLU?,https://www.reddit.com/r/MachineLearning/comments/abphp7/how_would_we_find_a_better_activation_function/,shamoons,1546400637,,0,1,False,default,,,,,
42,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,14,abqgrs,self.MachineLearning,"Global Cognitive Services Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/abqgrs/global_cognitive_services_market_size_outlook/,terrible_read,1546408145," 

Global cognitive services market size was valued at $1.97 billion in 2017 and is estimated to reach $33.72 billion by 2025 with the CAGR of 42.62% during 2019-2025.

Request a sample report @ [https://www.envisioninteligence.com/industry-report/global-cognitive-services-market/?utm\_source=reddit-Bindu](https://www.envisioninteligence.com/industry-report/global-cognitive-services-market/?utm_source=reddit-Bindu)",0,1,False,self,,,,,
43,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,15,abqkwo,codecampanion.blogspot.com,AI Assistance With PyText,https://www.reddit.com/r/MachineLearning/comments/abqkwo/ai_assistance_with_pytext/,AshishKhuraishy,1546409041,,0,1,False,default,,,,,
44,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,15,abqpsf,self.MachineLearning,[D] Is there any work on classifying non-photographic images?,https://www.reddit.com/r/MachineLearning/comments/abqpsf/d_is_there_any_work_on_classifying/,midasp,1546410174,"While there have been a lot of work in the area of categorizing photographs, I was wondering if there has been any progress in the categorization of non-photographic images?

By non-photographic, I mean images such as these (if cats vs not-cats were the classification category).

 * [Stick figure cat](https://www.google.com/search?q=stick+figure+cat&amp;tbm=isch)
 * [Cat icon](https://www.google.com/search?q=icon+cat&amp;tbm=isch)
 * [Abstract cat images](https://www.google.com/search?q=abstract+cat&amp;tbm=isch)
 * [Cartoon cat](https://www.google.com/search?q=cartoon+cat&amp;tbm=isch)
 * [3D rendered cat](https://www.google.com/search?q=3d+cat&amp;tbm=isch)

I am not even certain if ""non-photographic"" is the proper terminology to use, which perhaps explain why I am only finding some what older works such as [this 2014 paper using SVM with image features like SIFT/SURF rather than say ResNet or more modern methods](http://www.mva-org.jp/Proceedings/2015USB/papers/10-19.pdf). I would appreciate it if anyone can point me to more recent works.",14,1,False,self,,,,,
45,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,15,abqqwu,roscoelamontagne.com,Deepest Friends - Framed Print - Roscoe Lamontagne,https://www.reddit.com/r/MachineLearning/comments/abqqwu/deepest_friends_framed_print_roscoe_lamontagne/,printpainter,1546410452,,0,1,False,default,,,,,
46,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,15,abqqyr,handlinggear.com.au,Handling Gear | Multi Terrain Ramp : Terrain Loading Ramp : Terrain Truck Ramp : Truck Ramp Loader : Terrain Ramp Supply : Truck Ramp Terrain,https://www.reddit.com/r/MachineLearning/comments/abqqyr/handling_gear_multi_terrain_ramp_terrain_loading/,handlingge12,1546410466,,0,1,False,default,,,,,
47,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,16,abr87r,dzone.com,Learn how algorithmic robots work with a simple example of teacher bots and student bots.,https://www.reddit.com/r/MachineLearning/comments/abr87r/learn_how_algorithmic_robots_work_with_a_simple/,saif_sadiq,1546414743,,0,1,False,default,,,,,
48,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,18,abrtgv,arxiv.org,[1812.11446] Greedy Layerwise Learning Can Scale to ImageNet,https://www.reddit.com/r/MachineLearning/comments/abrtgv/181211446_greedy_layerwise_learning_can_scale_to/,aetolicus,1546420774,,21,1,False,default,,,,,
49,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,19,abs8g1,self.MachineLearning,Know any good hands-on educational content?,https://www.reddit.com/r/MachineLearning/comments/abs8g1/know_any_good_handson_educational_content/,janissary2016,1546424937,"Hi.  Do you have names of books, courses, MOOCs that are hands-on, project intensive, etc that you'd recommend?  For books, I prefer the ones with examples, exercises and drills at the end of each chapter  MOOCs or Courses with assignments, projects, capstones, etc.  ",0,1,False,self,,,,,
50,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,21,absvhg,self.MachineLearning,Key sound(s) detection,https://www.reddit.com/r/MachineLearning/comments/absvhg/key_sounds_detection/,curiousdatag,1546431197,[removed],0,1,False,self,,,,,
51,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,21,abt0ol,self.MachineLearning,When to use CART classification and when to use logistic regression,https://www.reddit.com/r/MachineLearning/comments/abt0ol/when_to_use_cart_classification_and_when_to_use/,CSstudentTillidie,1546432528,,0,1,False,self,,,,,
52,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,22,abtm60,self.MachineLearning,Nodejs with Tensorflow,https://www.reddit.com/r/MachineLearning/comments/abtm60/nodejs_with_tensorflow/,thefreakingmind,1546437444,[removed],0,1,False,self,,,,,
53,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,23,abtqe9,self.MachineLearning,Daily Time Series forecasting (store sales),https://www.reddit.com/r/MachineLearning/comments/abtqe9/daily_time_series_forecasting_store_sales/,geltance,1546438363,[removed],0,1,False,self,,,,,
54,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,23,abu3np,self.MachineLearning,[D] Bayesian Neural Networks Series,https://www.reddit.com/r/MachineLearning/comments/abu3np/d_bayesian_neural_networks_series/,KumarShridhar,1546441158,"I just published Bayesian Neural Network Series Post 1: Need for Bayesian Networks. This post is first in an eight-post series about [**Bayesian networks**](https://twitter.com/hashtag/bayesiannetworks?src=hash) NeuralNetworks and discusses the need for Bayesian approaches.

Here is the link: [https://medium.com/@shridhar743/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2](https://medium.com/@shridhar743/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2)

Please provide feedback so that things can improve in future. ",1,1,False,self,,,,,
55,MachineLearning,t5_2r3gv,2019-1-2,2019,1,2,23,abu3rp,self.MachineLearning,[D] Bayesian Neural Networks Series,https://www.reddit.com/r/MachineLearning/comments/abu3rp/d_bayesian_neural_networks_series/,KumarShridhar,1546441179,"I just published Bayesian Neural Network Series Post 1: Need for Bayesian Networks. This post is first in an eight-post series about [**Bayesian networks**](https://twitter.com/hashtag/bayesiannetworks?src=hash) NeuralNetworks and discusses the need for Bayesian approaches.

Here is the link: [https://medium.com/@shridhar743/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2](https://medium.com/@shridhar743/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2)

Please provide feedback so that things can improve in future. ",8,1,False,self,,,,,
56,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,0,abucnl,self.MachineLearning,Finding Paths within GAN models,https://www.reddit.com/r/MachineLearning/comments/abucnl/finding_paths_within_gan_models/,TropicaltreeHouse,1546442897,[removed],0,1,False,self,,,,,
57,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,0,abujpw,self.MachineLearning,Hex game AI,https://www.reddit.com/r/MachineLearning/comments/abujpw/hex_game_ai/,McBearHolden,1546444258,"Hi I would like to know if the best AI can beat humans in the Hexgame(In a similar fashion of AlphaGo)

&amp;#x200B;",0,1,False,self,,,,,
58,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,0,abuk9w,self.MachineLearning,"Simple Questions Thread January 02, 2019",https://www.reddit.com/r/MachineLearning/comments/abuk9w/simple_questions_thread_january_02_2019/,AutoModerator,1546444364,[removed],0,1,False,self,,,,,
59,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,1,abuw5c,self.MachineLearning,"[R] FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network",https://www.reddit.com/r/MachineLearning/comments/abuw5c/r_fastgrnn_a_fast_accurate_stable_and_tiny/,lt007,1546446512,"Recently, I came across Microsoft Research Indias efforts on resource-efficient ML and this seems to be their NeurIPS 2018 publication and looks interesting.

Code: [https://github.com/Microsoft/EdgeML](https://github.com/Microsoft/EdgeML)",16,1,False,self,,,,,
60,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,1,abv1io,medium.com,"Robustify RL: Uber, Go-Explore, and Research as RL with SOTA rewards",https://www.reddit.com/r/MachineLearning/comments/abv1io/robustify_rl_uber_goexplore_and_research_as_rl/,djangoblaster2,1546447459,,0,1,False,default,,,,,
61,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,1,abv38v,self.MachineLearning,[D] Advices for good PhD programs in Europe - Finance,https://www.reddit.com/r/MachineLearning/comments/abv38v/d_advices_for_good_phd_programs_in_europe_finance/,rods2292,1546447754,"So, I'm looking for good PhDs programs in Machine Learning field in Europe but I'm quite lost and I'd like to receive some advices. I'm an actuary and I'm looking for departments where I can apply machine learning to insurance/finance. 

I'm open to anywhere in Europe but I'm particularly interested in Berlin. It would be great to find programs there where I can be able to apply ML to insurance/finance but I'm having a hard time in doing that. PhDs in Germany are really different.",7,1,False,self,,,,,
62,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,1,abv4ss,medium.com,[R] Large-scale Image Captioning,https://www.reddit.com/r/MachineLearning/comments/abv4ss/r_largescale_image_captioning/,omarsar,1546448023,,0,1,False,default,,,,,
63,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,2,abv9hm,medium.com,"[D] Robustify RL: Uber, Go-Explore, and Research as RL with SOTA rewards",https://www.reddit.com/r/MachineLearning/comments/abv9hm/d_robustify_rl_uber_goexplore_and_research_as_rl/,djangoblaster2,1546448825,,0,1,False,https://b.thumbs.redditmedia.com/uDq4yCQt_2X8qizKQlAS4BDjhTFOcL5wfll91wj4PXo.jpg,,,,,
64,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,2,abvgn7,phys.org,[N]Machine learning speeds up atomistic simulations of water and ice,https://www.reddit.com/r/MachineLearning/comments/abvgn7/nmachine_learning_speeds_up_atomistic_simulations/,PapaSnork,1546450043,,0,1,False,https://b.thumbs.redditmedia.com/jhBN5tGtqQ3XS05uWht8mjIQi4kE08huHK68rteg_Ug.jpg,,,,,
65,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,2,abvlmk,hanxiao.github.io,Serving Google BERT in Production using Tensorflow and ZeroMQ  Han Xiao Tech Blog,https://www.reddit.com/r/MachineLearning/comments/abvlmk/serving_google_bert_in_production_using/,h_xiao,1546450880,,0,1,False,https://b.thumbs.redditmedia.com/7UpYCMnKCe87lu3LgoTytlcFbTMR0FaAp632aGKLKFA.jpg,,,,,
66,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,3,abvw30,medium.com,Alibaba Open-Sources Mars to Complement NumPy,https://www.reddit.com/r/MachineLearning/comments/abvw30/alibaba_opensources_mars_to_complement_numpy/,Yuqing7,1546452614,,0,1,False,default,,,,,
67,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,3,abw5ia,bruno.grdtech.in,[P] An AI for designer and web developers which will help them to choose a color scheme and will give them tge feel of chosen colors on a basic website structure.,https://www.reddit.com/r/MachineLearning/comments/abw5ia/p_an_ai_for_designer_and_web_developers_which/,preetsc27,1546454167,,0,1,False,default,,,,,
68,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,3,abw79y,self.MachineLearning,[R] Automatically Generating Comments for Arbitrary Source Code,https://www.reddit.com/r/MachineLearning/comments/abw79y/r_automatically_generating_comments_for_arbitrary/,rpgove,1546454472,"This is work from Two Six Labs on a language-agnostic ML model for automatically generating comments from source code.

Blog post: [https://www.twosixlabs.com/automatically-generating-comments-for-arbitrary-source-code/](https://www.twosixlabs.com/automatically-generating-comments-for-arbitrary-source-code/)",7,1,False,self,,,,,
69,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,4,abwgug,self.MachineLearning,Feature Store: the missing data layer in ML pipelines?,https://www.reddit.com/r/MachineLearning/comments/abwgug/feature_store_the_missing_data_layer_in_ml/,jpdowlin,1546455973,"We wrote a blog post about the first open-source Feature Store. It's on the open-source Hopsworks platform. Uber, Airbnb, Comcast, and others have in-house feature stores described in this post:

&amp;#x200B;

[https://www.logicalclocks.com/feature-store/](https://www.logicalclocks.com/feature-store/)",0,1,False,self,,,,,
70,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,4,abwlu8,logicalclocks.com,Feature Store: the missing data layer in ML pipelines?,https://www.reddit.com/r/MachineLearning/comments/abwlu8/feature_store_the_missing_data_layer_in_ml/,jpdowlin,1546456777,,0,1,False,default,,,,,
71,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,4,abwpg1,self.MachineLearning,[D] On Writing Custom Loss Functions in Keras,https://www.reddit.com/r/MachineLearning/comments/abwpg1/d_on_writing_custom_loss_functions_in_keras/,bantou_41,1546457363,"Writing your own custom loss function can be tricky. I found that out the other day when I was solving a toy problem involving inverse kinematics. So I explained what I did wrong and how I fixed it in this [blog post](https://medium.com/@yanfengliux/on-writing-custom-loss-functions-in-keras-e04290dd7a96). Following Jeremy Howard's advice of ""Communicate often. Don't wait until you are perfect"", I think this might help some people, even though six months from now I will find it trivial and refuse to even bother. ",24,1,False,self,,,,,
72,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,4,abwqhv,axiezai.github.io,Rubber Bands Tie Our Brains Together Too,https://www.reddit.com/r/MachineLearning/comments/abwqhv/rubber_bands_tie_our_brains_together_too/,pfd1986,1546457526,,0,1,False,default,,,,,
73,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,4,abwz92,self.MachineLearning,Anybody know fo any really good tutorials for LSTM models applied to time series data in R?,https://www.reddit.com/r/MachineLearning/comments/abwz92/anybody_know_fo_any_really_good_tutorials_for/,idonotknow9,1546458865,"I have been playing around with some LSTM models in R over the past few days from various tutorials. However the tutorials I have been using all break in their code at some point so I can not get a decent model to work from start to finish.

I have followed[this](http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/) tutorial but they seem to stop and not show the code for the final graph. [This](https://stackoverflow.com/questions/52292452/lstm-understanding-possible-overfit) SO post was also quite useful in understanding that tutorial.

[This](https://www.business-science.io/timeseries-analysis/2018/04/18/keras-lstm-sunspots-time-series-prediction.html) tutorial seems to be very promising but it breaks.

Heres my cut down version of it if anybody can help me solve the issue I am having (in regards to the map\_dbl())

[This](https://statkclee.github.io/model/model-rsampling-time-series.html) tutorial uses the same code as ""tutorial 3"" here but on a different dataset.

&amp;#x200B;

`# Core Tidyverse`

`library(tidyverse)`

`library(glue)`

`library(forcats)`

&amp;#x200B;

`# Time Series`

`library(timetk)`

`library(tidyquant)`

`library(tibbletime)`

&amp;#x200B;

`# Visualization`

`library(cowplot)`

&amp;#x200B;

`# Preprocessing`

`library(recipes)`

&amp;#x200B;

`# Sampling / Accuracy`

`library(rsample)`

`library(yardstick)` 

&amp;#x200B;

`# Modeling`

`library(keras)`

&amp;#x200B;

`sun_spots &lt;- datasets::sunspot.month %&gt;%`

  `tk_tbl() %&gt;%`

  `mutate(index = as_date(index)) %&gt;%`

  `as_tbl_time(index = index)`

&amp;#x200B;

`sun_spots`

`############################################`

&amp;#x200B;

`periods_train &lt;- 12 * 50`

`periods_test  &lt;- 12 * 10`

`skip_span     &lt;- 12 * 20`

&amp;#x200B;

`rolling_origin_resamples &lt;- rolling_origin(`

  `sun_spots,`

  `initial    = periods_train,`

  `assess     = periods_test,`

  `cumulative = FALSE,`

  `skip       = skip_span`

`)`

&amp;#x200B;

`rolling_origin_resamples`

&amp;#x200B;

`############################################`

&amp;#x200B;

`calc_rmse &lt;- function(prediction_tbl) {`

  

  `rmse_calculation &lt;- function(data) {`

`data %&gt;%`

`spread(key = key, value = value) %&gt;%`

`select(-index) %&gt;%`

`filter(!`[`is.na`](https://is.na)`(predict)) %&gt;%`

`rename(`

`truth    = actual,`

`estimate = predict`

`) %&gt;%`

`rmse(truth, estimate)`

  `}`

  

  `safe_rmse &lt;- possibly(rmse_calculation, otherwise = NA)`

  

  `safe_rmse(prediction_tbl)`

  

`}`

&amp;#x200B;

`#############################################`

&amp;#x200B;

`predict_keras_lstm &lt;- function(split, epochs = 300, ...) {`

  

  `lstm_prediction &lt;- function(split, epochs, ...) {`



`# 5.1.2 Data Setup`

`df_trn &lt;- training(split)`

`df_tst &lt;- testing(split)`



`df &lt;- bind_rows(`

`df_trn %&gt;% add_column(key = ""training""),`

`df_tst %&gt;% add_column(key = ""testing"")`

`) %&gt;%` 

`as_tbl_time(index = index)`



`# 5.1.3 Preprocessing`

`rec_obj &lt;- recipe(value ~ ., df) %&gt;%`

`step_sqrt(value) %&gt;%`

`step_center(value) %&gt;%`

`step_scale(value) %&gt;%`

`prep()`



`df_processed_tbl &lt;- bake(rec_obj, df)`



`center_history &lt;- rec_obj$steps[[2]]$means[""value""]`

`scale_history  &lt;- rec_obj$steps[[3]]$sds[""value""]`



`# 5.1.4 LSTM Plan`

`lag_setting  &lt;- 120 # = nrow(df_tst)`

`batch_size   &lt;- 40`

`train_length &lt;- 440`

`tsteps       &lt;- 1`

`epochs       &lt;- epochs`



`# 5.1.5 Train/Test Setup`

`lag_train_tbl &lt;- df_processed_tbl %&gt;%`

`mutate(value_lag = lag(value, n = lag_setting)) %&gt;%`

`filter(!`[`is.na`](https://is.na)`(value_lag)) %&gt;%`

`filter(key == ""training"") %&gt;%`

`tail(train_length)`



`x_train_vec &lt;- lag_train_tbl$value_lag`

`x_train_arr &lt;- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))`



`y_train_vec &lt;- lag_train_tbl$value`

`y_train_arr &lt;- array(data = y_train_vec, dim = c(length(y_train_vec), 1))`



`lag_test_tbl &lt;- df_processed_tbl %&gt;%`

`mutate(`

`value_lag = lag(value, n = lag_setting)`

`) %&gt;%`

`filter(!`[`is.na`](https://is.na)`(value_lag)) %&gt;%`

`filter(key == ""testing"")`



`x_test_vec &lt;- lag_test_tbl$value_lag`

`x_test_arr &lt;- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))`



`y_test_vec &lt;- lag_test_tbl$value`

`y_test_arr &lt;- array(data = y_test_vec, dim = c(length(y_test_vec), 1))`



`# 5.1.6 LSTM Model`

`model &lt;- keras_model_sequential()`



`model %&gt;%`

`layer_lstm(units            = 50,` 

`input_shape      = c(tsteps, 1),` 

`batch_size       = batch_size,`

`return_sequences = TRUE,` 

`stateful         = TRUE) %&gt;%` 

`layer_lstm(units            = 50,` 

`return_sequences = FALSE,` 

`stateful         = TRUE) %&gt;%` 

`layer_dense(units = 1)`



`model %&gt;%` 

`compile(loss = 'mae', optimizer = 'adam')`



`# 5.1.7 Fitting LSTM`

`for (i in 1:epochs) {`

`model %&gt;% fit(x          = x_train_arr,` 

`y          = y_train_arr,` 

`batch_size = batch_size,`

`epochs     = 1,` 

`verbose    = 1,` 

`shuffle    = FALSE)`



`model %&gt;% reset_states()`

`cat(""Epoch: "", i)`



`}`



`# 5.1.8 Predict and Return Tidy Data`

`# Make Predictions`

`pred_out &lt;- model %&gt;%` 

`predict(x_test_arr, batch_size = batch_size) %&gt;%`

`.[,1]` 



`# Retransform values`

`pred_tbl &lt;- tibble(`

`index   = lag_test_tbl$index,`

`value   = (pred_out * scale_history + center_history)^2`

`)` 



`# Combine actual data with predictions`

`tbl_1 &lt;- df_trn %&gt;%`

`add_column(key = ""actual"")`



`tbl_2 &lt;- df_tst %&gt;%`

`add_column(key = ""actual"")`



`tbl_3 &lt;- pred_tbl %&gt;%`

`add_column(key = ""predict"")`



`# Create time_bind_rows() to solve dplyr issue`

`time_bind_rows &lt;- function(data_1, data_2, index) {`

`index_expr &lt;- enquo(index)`

`bind_rows(data_1, data_2) %&gt;%`

`as_tbl_time(index = !! index_expr)`

`}`



`ret &lt;- list(tbl_1, tbl_2, tbl_3) %&gt;%`

`reduce(time_bind_rows, index = index) %&gt;%`

`arrange(key, index) %&gt;%`

`mutate(key = as_factor(key))`



`return(ret)`



  `}`

  

  `safe_lstm &lt;- possibly(lstm_prediction, otherwise = NA)`

  

  `safe_lstm(split, epochs, ...)`

  

`}`

&amp;#x200B;

`#################################################`

&amp;#x200B;

`sample_predictions_lstm_tbl &lt;- rolling_origin_resamples %&gt;%`

  `mutate(predict = map(splits, predict_keras_lstm, epochs = 10))`

&amp;#x200B;

`sample_predictions_lstm_tbl`

&amp;#x200B;

&amp;#x200B;

`sample_predictions_lstm_tbl$predict`

`map_dbl(sample_predictions_lstm_tbl$predict, calc_rmse)`

&amp;#x200B;

`sample_rmse_tbl &lt;- sample_predictions_lstm_tbl %&gt;%`

  `mutate(rmse = map_dbl(predict, calc_rmse)) %&gt;%`

  `select(id, rmse)`",0,1,False,self,,,,,
74,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,5,abxjcf,self.MachineLearning,[D] A/B testing with machine learning,https://www.reddit.com/r/MachineLearning/comments/abxjcf/d_ab_testing_with_machine_learning/,Xamius,1546462026,"I am curious on how others would approach this problem.

&amp;#x200B;

Lets say you want to run a pricing experiment. Customers can get price A or B, or the normal price. A and B are -/+ 5% of the original price.

&amp;#x200B;

You run the experiment, find out that price B does the best (+5%)

&amp;#x200B;

I then want to use a model to predict if someone should get price A or B.

&amp;#x200B;

My current approach:

Build a model for all customers who got price A

build another model for all customers who got price B

Response variable: Either binary (did they buy) or continuous (total revenue)

&amp;#x200B;

Then whichever model predicts a higher result (probability or revenue) is the chosen price for that individual customer.

&amp;#x200B;

IE, model A predicts they will generate $10 of revenue, model B predict $7 of revenue, so for that customer price A is chosen.

&amp;#x200B;

Are there alternative ways of combining a machine learning model with an A/B test to predict which (A or B) to offer?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",11,1,False,self,,,,,
75,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,6,aby08v,self.MachineLearning,Need advice for PC build,https://www.reddit.com/r/MachineLearning/comments/aby08v/need_advice_for_pc_build/,Irivaka,1546464813,"So since Im starting my bachelors degree in AI and Machine Learning, I need a pc for just that.
 I've read that I need at least as much RAM as VRAM, fast single core speed and I've chosen an RTX 2080ti as GPU of choice. I'm torn between 9600K and 6700K and the RAM to pair it with. 
PCIe lanes are very important but I'm only getting a single card for the coming years, I would like to get an NVMe SSD but I'm afraid I wont have 16 lanes left for my GPU if I go for an Intel CPU. 
Could someone give me any tips? Would appreciate it a lot!",0,1,False,self,,,,,
76,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,7,abyob4,techcrunch.com,This clever AI [CycleGAN] hid data from its creators to cheat at its appointed task [using stenography],https://www.reddit.com/r/MachineLearning/comments/abyob4/this_clever_ai_cyclegan_hid_data_from_its/,columbus8myhw,1546468752,,0,1,False,default,,,,,
77,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,7,abypnj,self.MachineLearning,Image Recognition with Keras: Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/abypnj/image_recognition_with_keras_convolutional_neural/,reamsofdata,1546468975,"This is an example of an image classifier that I recently developed with Keras.

&amp;#x200B;

[http://www.michaeljgrogan.com/image-recognition-with-keras-convolutional-neural-networks/](http://www.michaeljgrogan.com/image-recognition-with-keras-convolutional-neural-networks/)

&amp;#x200B;

Essentially, the model is trained to distinguish between images of cars and planes with 200 images in total (100 for each category).

&amp;#x200B;

I found that the accuracy was maximised (and loss was minimised between 2 to 3 epochs) - accuracy was nearly 100% on the training set and 92.5% on the test set.

&amp;#x200B;

One of my goals is to expand on the model to classify more complex images, as this would have great application in the area of transport, autonomous vehicles, etc.

&amp;#x200B;

Hope you find the write-up useful and I highly welcome any feedback.",0,1,False,self,,,,,
78,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,7,abyrkm,self.MachineLearning,[P] Recognizing hand gestures and direction (for controlling smart homes by pointing),https://www.reddit.com/r/MachineLearning/comments/abyrkm/p_recognizing_hand_gestures_and_direction_for/,hollihollih,1546469291,"In case someone else wants to control their Hue-lights or any other items by pointing and gestures.  


[https://github.com/holli/hands\_ai](https://github.com/holli/hands_ai)   


There is a simple model/framework to recognize classes and direction in one go. Using a modified yolo/single-shot-detector but with directions instead of bounding boxes. Also an example of using the results for controlling the lights.

&amp;#x200B;

https://i.redd.it/ee2erc8i73821.gif",1,1,False,https://a.thumbs.redditmedia.com/lkdWIBPX4Oq7g7IjnVM-5opz1V-heV1dstWG3onQG30.jpg,,,,,
79,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,7,abyu1h,self.MachineLearning,[D] Image Recognition with Keras: Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/abyu1h/d_image_recognition_with_keras_convolutional/,reamsofdata,1546469685,"This is an example of an image classifier that I recently developed with Keras.

&amp;#x200B;

[http://www.michaeljgrogan.com/image-recognition-with-keras-convolutional-neural-networks/](http://www.michaeljgrogan.com/image-recognition-with-keras-convolutional-neural-networks/)

&amp;#x200B;

Essentially, the model is trained to distinguish between images of cars and planes with 200 images in total (100 for each category).

&amp;#x200B;

I found that the accuracy was maximised (and loss was minimised between 2 to 3 epochs) - accuracy was nearly 100% on the training set and 92.5% on the test set.

&amp;#x200B;

One of my goals is to expand on the model to classify more complex images, as this would have great application in the area of transport, autonomous vehicles, etc.

&amp;#x200B;

Hope you find the write-up useful and I highly welcome any feedback.",3,1,False,self,,,,,
80,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,9,abzs0u,self.MachineLearning,[D]ProGan on non photograph dataset,https://www.reddit.com/r/MachineLearning/comments/abzs0u/dprogan_on_non_photograph_dataset/,EfficientCall,1546475516,"The results on progan are very impressive from nvidia and now with the style gan it looks more to me that it can be used on a art dataset, are there are papers or work being done on this?

example like this dataset here: [https://bam-dataset.org/](https://bam-dataset.org/)

nvidia paper: [https://arxiv.org/pdf/1812.04948.pdf](https://arxiv.org/pdf/1812.04948.pdf)

&amp;#x200B;

&amp;#x200B;",5,1,False,self,,,,,
81,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,9,abztjh,github.com,Cancerous Mole Detection CNN,https://www.reddit.com/r/MachineLearning/comments/abztjh/cancerous_mole_detection_cnn/,refactors,1546475782,,0,1,False,default,,,,,
82,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,10,ac0e89,self.MachineLearning,DeepMinds Take on How To Create a Benign AI,https://www.reddit.com/r/MachineLearning/comments/ac0e89/deepminds_take_on_how_to_create_a_benign_ai/,mslavescu,1546479548,[removed],0,1,False,self,,,,,
83,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,10,ac0j5h,youtu.be,DeepMinds Take on How To Create a Benign AI,https://www.reddit.com/r/MachineLearning/comments/ac0j5h/deepminds_take_on_how_to_create_a_benign_ai/,mslavescu,1546480448,,1,1,False,default,,,,,
84,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,11,ac0q6e,medium.com,Apache Zeppelin: stairway to notes* haven!,https://www.reddit.com/r/MachineLearning/comments/ac0q6e/apache_zeppelin_stairway_to_notes_haven/,neomatrix369,1546481716,,0,1,False,default,,,,,
85,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,11,ac0v2a,self.MachineLearning,[D] Should ML workflow be different from SE workflow?,https://www.reddit.com/r/MachineLearning/comments/ac0v2a/d_should_ml_workflow_be_different_from_se_workflow/,sslotin,1546482614,"Recently, an experienced ex-SE team lead came to our NLP team (conversational AI  language models, Siamese networks, contrastive losses and stuff like that) and started advocating for stricter code reviews, wiki documentation, scrum poker and other agile bullshit, and I don't agree with her, because I think that ML projects are too volatile for that stuff.

What do you think? What is the workflow in your team? How is it different from SE?",52,1,False,self,,,,,
86,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,11,ac0z8f,medium.com,Kubeflow in 2018: A year in perspective  kubeflow  Medium,https://www.reddit.com/r/MachineLearning/comments/ac0z8f/kubeflow_in_2018_a_year_in_perspective_kubeflow/,codekee,1546483387,,0,1,False,default,,,,,
87,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,11,ac10tw,self.MachineLearning,Dzogchen and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ac10tw/dzogchen_and_machine_learning/,luci_dity,1546483670,[removed],0,1,False,self,,,,,
88,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,12,ac1id9,artnome.com,DeepDream Creator Unveils Very First Images After Three Years,https://www.reddit.com/r/MachineLearning/comments/ac1id9/deepdream_creator_unveils_very_first_images_after/,hoopism,1546486807,,0,1,False,https://b.thumbs.redditmedia.com/D2EnPtxTfvBtPv1b-jDe6d_cfzFzYjwBlUYOdvbl18A.jpg,,,,,
89,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,12,ac1o9j,self.MachineLearning,"[2 Nips, 1 ICML] Any chance for this profile for top Phd in machine learning",https://www.reddit.com/r/MachineLearning/comments/ac1o9j/2_nips_1_icml_any_chance_for_this_profile_for_top/,hitman_reborn_no1,1546487908,"\[Real story , not a joke\]  I will apply for phd in Machine learning next year . In my case i have very bad undergraduate profile  (2.8 GPA) due to immature and lack concentration on study . But after that i worked very very hard for next 5 years (do research and take several math/statistic non-degree  course at local university). The result is now i have 2 NIPS, 1 ICML, and 1 ICDM paper (all first author). Do i have chances for top 10 phd program machine learning in US. Any one with some experiences on application process could comment for me?",0,1,False,self,,,,,
90,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,14,ac29aj,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/ac29aj/global_machine_learning_market_size_outlook/,satish_envi,1546492018,[removed],0,1,False,self,,,,,
91,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,14,ac2d6x,self.MachineLearning,How not to Do Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ac2d6x/how_not_to_do_machine_learning/,init__27,1546492818,[https://hackernoon.com/how-not-to-do-fast-ai-or-any-ml-mooc-3d34a7e0ab8c](https://hackernoon.com/how-not-to-do-fast-ai-or-any-ml-mooc-3d34a7e0ab8c),0,1,False,self,,,,,
92,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,14,ac2dh9,arxiv.org,[R] Elimination of All Bad Local Minima in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/ac2dh9/r_elimination_of_all_bad_local_minima_in_deep/,downtownslim,1546492876,,27,1,False,default,,,,,
93,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,14,ac2kth,soundcloud.com,[P] Text-To-Speech Donald Trump Model,https://www.reddit.com/r/MachineLearning/comments/ac2kth/p_texttospeech_donald_trump_model/,hanyuqn,1546494365,,1,1,False,default,,,,,
94,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,14,ac2lns,techcrunch.com,[N] This clever AI [CycleGAN] hid data from its creators [with stenography] to cheat at its appointed task [of reconstructing photos],https://www.reddit.com/r/MachineLearning/comments/ac2lns/n_this_clever_ai_cyclegan_hid_data_from_its/,columbus8myhw,1546494545,,2,1,False,https://b.thumbs.redditmedia.com/DmW3IBQUcjpk9LAuKWQdT5hdn1S_NY1uCnI2-1uXolM.jpg,,,,,
95,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,14,ac2mto,blockdelta.io,The Future of Cloud Computing,https://www.reddit.com/r/MachineLearning/comments/ac2mto/the_future_of_cloud_computing/,BlockDelta,1546494792,,0,1,False,default,,,,,
96,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,14,ac2n43,self.MachineLearning,[P] Web-based annotation tool for image segmentation and localization,https://www.reddit.com/r/MachineLearning/comments/ac2n43/p_webbased_annotation_tool_for_image_segmentation/,InfoPaste,1546494856,"I have been working on an [image annotator tool](https://github.com/jsbroks/coco-annotator/) to create custom datasets for Mask-RCNN and other image detection models.

&amp;#x200B;

Enjoy!",10,1,False,self,,,,,
97,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,15,ac2pg3,self.MachineLearning,[P] Dzogchen and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ac2pg3/p_dzogchen_and_machine_learning/,luci_dity,1546495359,I'm  involved in a project where we are considering Machine Learning from the Dzogchen philosophy of mind. Are there any experienced programmers and/or ML developers out there with a background in Dzogchen? We would love more on our team.,4,1,False,self,,,,,
98,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,15,ac2zlv,self.MachineLearning,Are there any pre-trained models for the im2txt model developed by Google?,https://www.reddit.com/r/MachineLearning/comments/ac2zlv/are_there_any_pretrained_models_for_the_im2txt/,ZER_0_NE,1546497555,[removed],0,1,False,self,,,,,
99,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,15,ac347v,trends.google.com,PyTorch passes Keras on Google Trends in the United States,https://www.reddit.com/r/MachineLearning/comments/ac347v/pytorch_passes_keras_on_google_trends_in_the/,deeplearninguser,1546498570,,0,1,False,default,,,,,
100,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,16,ac39hx,self.MachineLearning,"AIOps Platform Market - Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/ac39hx/aiops_platform_market_size_outlook_trends_and/,terrible_read,1546499786,[removed],0,1,False,self,,,,,
101,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,16,ac3eox,medium.com,[D] Your AI skills are worth less than you think,https://www.reddit.com/r/MachineLearning/comments/ac3eox/d_your_ai_skills_are_worth_less_than_you_think/,szopa,1546501022,,0,1,False,https://b.thumbs.redditmedia.com/RcAXNllpO1y0Ki_WQxRJqrNaDZ5k2b8sG3Lcmuke0NI.jpg,,,,,
102,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,16,ac3j53,self.MachineLearning,OULU - NPU dataset,https://www.reddit.com/r/MachineLearning/comments/ac3j53/oulu_npu_dataset/,denny_arjun,1546502119,[removed],0,1,False,self,,,,,
103,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,17,ac3k96,youtube.com,Sewage sludge organic fertilizer granule making machine,https://www.reddit.com/r/MachineLearning/comments/ac3k96/sewage_sludge_organic_fertilizer_granule_making/,amylee516,1546502403,,0,1,False,default,,,,,
104,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,17,ac3pd8,openpr.com,Hydraulic Hammer Market Analysis 2018,https://www.reddit.com/r/MachineLearning/comments/ac3pd8/hydraulic_hammer_market_analysis_2018/,Swati_Shinde,1546503678,,0,1,False,default,,,,,
105,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,17,ac3tgs,self.MachineLearning,Green Banana Plantain Peeling MachineAutomatic Separation of Skin and F...,https://www.reddit.com/r/MachineLearning/comments/ac3tgs/green_banana_plantain_peeling_machineautomatic/,fryingmachine,1546504733,[removed],0,1,False,self,,,,,
106,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,18,ac41hc,prnewswire.com,Machine Learning Market to Record Overwhelming Hike in Revenues by 2022- Exclusive Report by MarketsandMarkets,https://www.reddit.com/r/MachineLearning/comments/ac41hc/machine_learning_market_to_record_overwhelming/,tejashreepatel,1546506863,,0,1,False,default,,,,,
107,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,18,ac49ky,self.MachineLearning,[D] The best of GAN papers in the year 2018 (part 1),https://www.reddit.com/r/MachineLearning/comments/ac49ky/d_the_best_of_gan_papers_in_the_year_2018_part_1/,dtransposed,1546509082,"Hi! Being inspired by the [recent discussion](https://www.reddit.com/r/MachineLearning/comments/a8th4o/d_what_are_best_papers_regarding_gans_for_2018/) on reddit, I would like to share a summary (part 1) of my favourite research papers in 2018 with the focus on the deep learning models - Generative Adversarial Networks. 

I am certain there is a substantial number of passionates who are interested in the topic. 

  
I wish you tons of fruitful projects, steep learning curves and all the best in the New Year! 

[https://dtransposed.github.io/blog/Best-of-GANs-2018-(Part-1-out-of-2).html](https://dtransposed.github.io/blog/Best-of-GANs-2018-(Part-1-out-of-2).html)",3,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,19,ac4f13,self.MachineLearning,What is the current state of art in face recognition?,https://www.reddit.com/r/MachineLearning/comments/ac4f13/what_is_the_current_state_of_art_in_face/,MohamedRashad,1546510576,"I know that Networks like FaceNet or ArcFace have solved this domain with an accuracy of +99% but i was wondering about their results in a more real scenarios with noisy environments, constant movement and a wide space to scan and identify .... is there a better Networks than them maybe with 3d recognition from depth images ?",0,1,False,self,,,,,
109,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,19,ac4fj6,medium.com,Become a Machine Learning Engineer in 6 months -Job Guaranteed,https://www.reddit.com/r/MachineLearning/comments/ac4fj6/become_a_machine_learning_engineer_in_6_months/,skj8,1546510717,,0,1,False,default,,,,,
110,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,19,ac4iae,arxiv.org,[R] Scalable agent alignment via reward modeling: a research direction,https://www.reddit.com/r/MachineLearning/comments/ac4iae/r_scalable_agent_alignment_via_reward_modeling_a/,mrconter1,1546511515,,1,1,False,default,,,,,
111,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,20,ac4vbi,datacamp.com,Introduction to Machine Learning - Free Online Course,https://www.reddit.com/r/MachineLearning/comments/ac4vbi/introduction_to_machine_learning_free_online/,skj8,1546514924,,0,1,False,default,,,,,
112,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,20,ac52ag,medium.com,Data Science Project Flow for Startups,https://www.reddit.com/r/MachineLearning/comments/ac52ag/data_science_project_flow_for_startups/,shaypal5,1546516728,,0,1,False,https://b.thumbs.redditmedia.com/ywuNVEuZ6IWOlBj_mlNwpzQGdSMgzQnNAaqC5U4MIgc.jpg,,,,,
113,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,21,ac52yn,self.MachineLearning,[D] Robust Classification,https://www.reddit.com/r/MachineLearning/comments/ac52yn/d_robust_classification/,SonLe28,1546516897,"I found this thesis online and very interested in Chapter: Robust Classification. I tried to reproduce, but unsucessfully, the section Robust in both features and labels - Logistics Regression. Is there any idea how to solve this optimization model? i used CVXPY but totally failed due to the non-DCP constraint. ",7,1,False,self,,,,,
114,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,21,ac55gl,self.MachineLearning,Kymatio: Scattering Transforms in Python,https://www.reddit.com/r/MachineLearning/comments/ac55gl/kymatio_scattering_transforms_in_python/,Maxthing,1546517477,[removed],0,1,False,self,,,,,
115,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,21,ac57r7,nayak.io,Trying out Tensorflow JS,https://www.reddit.com/r/MachineLearning/comments/ac57r7/trying_out_tensorflow_js/,immortaljoe,1546518033,,0,1,False,default,,,,,
116,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,21,ac58yv,arxiv.org,[P] Kymatio: Scattering Transforms in Python,https://www.reddit.com/r/MachineLearning/comments/ac58yv/p_kymatio_scattering_transforms_in_python/,Maxthing,1546518335,,7,1,False,default,,,,,
117,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,22,ac5kak,w2ssolutions.com,What Happens When Machine Learning And DevOps Join Hands?,https://www.reddit.com/r/MachineLearning/comments/ac5kak/what_happens_when_machine_learning_and_devops/,techologynews,1546520933,,0,1,False,default,,,,,
118,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,22,ac5l9h,self.MachineLearning,k-Nearest Neighbor Augmented Neural Networks for Text Classification,https://www.reddit.com/r/MachineLearning/comments/ac5l9h/knearest_neighbor_augmented_neural_networks_for/,abhishek2705,1546521140,[removed],0,1,False,self,,,,,
119,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,22,ac5rbg,github.com,[N] List of summer schools in machine learning (and related fields) across the globe,https://www.reddit.com/r/MachineLearning/comments/ac5rbg/n_list_of_summer_schools_in_machine_learning_and/,sshkhr,1546522466,,0,1,False,default,,,,,
120,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,22,ac5u8g,self.MachineLearning,[R] [Research] k-Nearest Neighbor Augmented Neural Networks for Text Classification,https://www.reddit.com/r/MachineLearning/comments/ac5u8g/r_research_knearest_neighbor_augmented_neural/,abhishek2705,1546523113,"Is there a python implementation of this research paper available online? 

&amp;#x200B;

[https://arxiv.org/pdf/1708.07863.pdf](https://arxiv.org/pdf/1708.07863.pdf)",6,1,False,self,,,,,
121,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,22,ac5vl9,logicalclocks.com,[P] Feature Store: the missing data layer in ML pipelines?,https://www.reddit.com/r/MachineLearning/comments/ac5vl9/p_feature_store_the_missing_data_layer_in_ml/,jpdowlin,1546523412,,0,1,False,default,,,,,
122,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,22,ac5w2i,oodlestechnologies.com,An Industrial Revolution Sparked By Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ac5w2i/an_industrial_revolution_sparked_by_machine/,vidushivij,1546523521,,0,1,False,default,,,,,
123,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,23,ac6704,self.MachineLearning,"To get into Natural Language Processing research, should I DEFINITELY get a PhD in computer science, or would a PhD in linguistics be equally valuable?",https://www.reddit.com/r/MachineLearning/comments/ac6704/to_get_into_natural_language_processing_research/,synthphreak,1546525805,[removed],0,1,False,self,,,,,
124,MachineLearning,t5_2r3gv,2019-1-3,2019,1,3,23,ac6er3,self.MachineLearning,[D] Gaussian Processes are Not So Fancy,https://www.reddit.com/r/MachineLearning/comments/ac6er3/d_gaussian_processes_are_not_so_fancy/,cosmic_dozen,1546527339,"A friend of mine is working on a simple(r) explanation for [Gaussian Processes](https://planspace.org/20181226-gaussian_processes_are_not_so_fancy/). He'd welcome any feedback, especially on making it more relevant to machine learning!",17,1,False,self,,,,,
125,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,0,ac6wsd,ucsf.edu,AI Can Detect Alzheimers Disease in Brain Scans Six Years Before a Diagnosis,https://www.reddit.com/r/MachineLearning/comments/ac6wsd/ai_can_detect_alzheimers_disease_in_brain_scans/,j_orshman,1546530601,,72,1,False,default,,,,,
126,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,0,ac6zgn,medium.com,Why text classification is losing momentum on NLP research?,https://www.reddit.com/r/MachineLearning/comments/ac6zgn/why_text_classification_is_losing_momentum_on_nlp/,alberduris,1546531085,,0,1,False,default,,,,,
127,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,1,ac74sn,self.MachineLearning,Why is text classification losing momentum on NLP research?,https://www.reddit.com/r/MachineLearning/comments/ac74sn/why_is_text_classification_losing_momentum_on_nlp/,alberduris,1546531975,[removed],0,1,False,self,,,,,
128,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,1,ac7ii1,medium.com,[P] InstaGAN Excels in Instance-Aware Image-To-Image Translation,https://www.reddit.com/r/MachineLearning/comments/ac7ii1/p_instagan_excels_in_instanceaware_imagetoimage/,gwen0927,1546534236,,0,1,False,default,,,,,
129,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,2,ac848l,self.MachineLearning,Job,https://www.reddit.com/r/MachineLearning/comments/ac848l/job/,gen12345671,1546537739,[removed],0,1,False,self,,,,,
130,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,3,ac89iw,self.MachineLearning,Sorting news articles by story,https://www.reddit.com/r/MachineLearning/comments/ac89iw/sorting_news_articles_by_story/,lolidunnowut,1546538601,[removed],0,1,False,self,,,,,
131,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,3,ac8lkt,self.MachineLearning,Is TensorFlow considered to be a 'Programming Model'?,https://www.reddit.com/r/MachineLearning/comments/ac8lkt/is_tensorflow_considered_to_be_a_programming_model/,kuna1995,1546540497,[removed],0,1,False,self,,,,,
132,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,4,ac8wky,self.MachineLearning,SSD Bounding box fitting question,https://www.reddit.com/r/MachineLearning/comments/ac8wky/ssd_bounding_box_fitting_question/,moghaak,1546542217,[removed],0,1,False,self,,,,,
133,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,4,ac9335,self.MachineLearning,[P] Generating Devanagari Characters Using DRAW,https://www.reddit.com/r/MachineLearning/comments/ac9335/p_generating_devanagari_characters_using_draw/,Natsu6767,1546543215,"I really love trying out and playing with generative models. Although it's really difficult to intuitively make out how a model is actually generating images (sometimes even good enough to fool humans), I find that [DRAW: A Recurrent Neural Network For Image Generation](https://arxiv.org/abs/1502.04623)**,** is the most ""humane"" out of them all in the way it generates images.

This is a PyTorch implementation of DRAW trained to generate [Devanagari](https://en.wikipedia.org/wiki/Devanagari) Characters:

[https://github.com/Natsu6767/Generating-Devanagari-Using-DRAW](https://github.com/Natsu6767/Generating-Devanagari-Using-DRAW)

The basic idea behind DRAW is to only generate a small portion of an image at a time *(say, a 5 x 5 region when the whole image is 32 x 32)* and recursively refine this. It's sort of like how we humans draw a picture: we first draw rough outlines and slowly add in the details over time.

One problem with DRAW is that, as it's underlying base architecture is the Variational Autoencoder which uses the reconstruction loss, the outputs tend to be blurry. This is especially noticeable in higher complexity datasets such as CIFAR-10. 

&amp;#x200B;

*P.S. I find it weirdly satisfying watching the animation of the images being generated through time, glimpse by glimpse.*",2,1,False,self,,,,,
134,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,5,ac9o4w,self.MachineLearning,Synergy between the lasso and Partial Least Squares?,https://www.reddit.com/r/MachineLearning/comments/ac9o4w/synergy_between_the_lasso_and_partial_least/,IAMA_monkey,1546546490,"Hi all,

&amp;#x200B;

I have a dataset consisting of 225 variables and about 20 observations. I wanted to see if I could predict a  set of response variables with these 225 predictor variables.

&amp;#x200B;

At first, I used a Partial Least Squares (PLS) model  as it should be suited for problems where you have many more variabels than observations. The optimal number of PLS components was selected by calculating the mean error for each number of PLS components, using leave-one-out cross-validation. The results were quite poor (to my surprise). I even saw that in some cases the prediction error increased in a monotonous manner with increasing number of PLS components, which is something I haven't seen before. Could this be due to the large number of variables?

&amp;#x200B;

In a second attempt I started by using the lasso for feature selection, which yielded about 10-15 predictor variables for each response variable. Then, I used only these response variables to build the PLS models. The results improved DRASTICALLY. Errors decreased by about 70-80%, and plots showing mean error as a function of PLS components decreased somewhat monotonically as expected.

&amp;#x200B;

My question: The results seem a little bit suspicious because of the large improvement, and errors are in some cases up to 5% lower than I would have expected them. My worry is that by using a combination of the lasso and PLS, I've somehow created 'overfitted' models that give good results due to random variations in the data, instead of actual patterns. I did use cross-validation to select the number of PLS components so this shouldn't be a problem, I think?",0,1,False,self,,,,,
135,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,5,ac9ssi,self.MachineLearning,Valid approach with the lasso and Partial Least Squares?,https://www.reddit.com/r/MachineLearning/comments/ac9ssi/valid_approach_with_the_lasso_and_partial_least/,IAMA_monkey,1546547187,[removed],0,1,False,self,,,,,
136,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,6,acabk3,self.MachineLearning,[R] Graph Classification Using Structural Attention (KDD 2018),https://www.reddit.com/r/MachineLearning/comments/acabk3/r_graph_classification_using_structural_attention/,benitorosenberg,1546550117,"&amp;#x200B;

![img](d47rvzkxx9821)

Paper: [https://github.com/benedekrozemberczki/GAM/blob/master/paper.pdf](https://github.com/benedekrozemberczki/GAM/blob/master/paper.pdf)

Python: [https://github.com/benedekrozemberczki/GAM](https://github.com/benedekrozemberczki/GAM)

ABSTRACT:

Graph classification is a problem with practical applications in many different domains. To solve this problem, one usually calculates certain graph statistics (i.e. , graph features) that help discriminate between graphs of different classes. When calculating such features, most existing approaches process the entire graph. In a graphlet-based approach, for instance, the entire graph is processed to get the total count of different graphlets or subgraphs. In many real-world applications, however, graphs can be noisy with discriminative patterns confined to certain regions in the graph only. In this work, we study the problem of attention-based graph classification . The use of attention allows us to focus on small but informative parts of the graph, avoiding noise in the rest of the graph. We present a novel RNN model, called the Graph Attention Model (GAM), that processes only a portion of the graph by adaptively selecting a sequence of informative nodes. Experimental results on multiple real-world datasets show that the proposed method is competitive against various well-known methods in graph classification even though our method is limited to only a portion of the graph.",1,1,False,self,,,,,
137,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,6,acagfp,self.MachineLearning,Machine Learning Course Crowdsourcing [P],https://www.reddit.com/r/MachineLearning/comments/acagfp/machine_learning_course_crowdsourcing_p/,RRR777R7,1546550872,"Hi! We are crowdsourcing Machine Learning Courses in our Spanish ML community. The goal is rating the Courses you have made in order to create a baseline for begginers.

We have created a public GSheet, here are the instructions (the GSheet is in Spanish, so I translate them here):
1) Add the course name in the field ""Nombre"" if it isn't there and add the link.
2) Add your rating in the Punt. A,B,C column (the first available over the row). Punt means Puntuacion in Spanish, which means Rating. The maximum is 5, the minimum is 1.

If there are useful results, I'll write a post in English so that you can use it even if you don't know Spanish.

Thanks!

LINK: https://docs.google.com/spreadsheets/d/1yfhhn9PydXmHHaE8sjteSCrlHuGYqm0HwUFXs8vpM0M/edit?usp=sharing",0,1,False,self,,,,,
138,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,7,acax51,self.MachineLearning,Every model saturating at around same point,https://www.reddit.com/r/MachineLearning/comments/acax51/every_model_saturating_at_around_same_point/,veejarAmrev,1546553553,"
I am training a neural network, the simple baseline results for which are available. The task is regression task on text using CNN.

Hoping to much better than that, I tried training more richer CNN with filters of varied sizes but it's results are comparable with the baseline. Not only that, I tried changing hyperparameters but evertime it converges around the same point for validation data, it starts oververtting on training for some 
 
",0,1,False,self,,,,,
139,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,7,acbdzz,self.MachineLearning,CNN for fish-eye classifier,https://www.reddit.com/r/MachineLearning/comments/acbdzz/cnn_for_fisheye_classifier/,sepehr_fard,1546556322,"Did some research to find if anyone has published anything in regards training to detect fish eye images vs non-fisheye images. 

&amp;#x200B;

Things in mind:

\- Some type of data augmentation where the edge of the images are exaggerated. Since only difference between two classes are the subtle roundness of the image on the edges. ",0,1,False,self,,,,,
140,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,8,acbf8l,self.MachineLearning,Is this study plan feasible?,https://www.reddit.com/r/MachineLearning/comments/acbf8l/is_this_study_plan_feasible/,Kyak787,1546556515,[removed],0,1,False,self,,,,,
141,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,9,acc55r,self.MachineLearning,"Error Importing tensorflow. Unless you are using bazel, you should not....",https://www.reddit.com/r/MachineLearning/comments/acc55r/error_importing_tensorflow_unless_you_are_using/,SuperJMan64,1546560796,"Error importing tensorflow.  Unless you are using bazel,

you should not try to import tensorflow from its source directory;

please exit the tensorflow source tree, and relaunch your python interpreter

from there.

That is the message I get in command prompt when I try to run ""python [Train.py](https://Train.py) sample.cfg"" [Train.py](https://Train.py) is a neural network trainer file thing. What is the problem? Do I need to reinstall tensorflow without pip or something? Any advice is greatly appreciated.",0,1,False,self,,,,,
142,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,9,accbts,self.MachineLearning,"Error Importing tensorflow. Unless you are using bazel, you should not....",https://www.reddit.com/r/MachineLearning/comments/accbts/error_importing_tensorflow_unless_you_are_using/,SuperJMan64,1546561989,"Error importing tensorflow. Unless you are using bazel,

you should not try to import tensorflow from its source directory;

please exit the tensorflow source tree, and relaunch your python interpreter

from there.

That is the message I get in command prompt when I try to run ""python Train.py sample.cfg"" Train.py is a neural network trainer file thing. What is the problem? Do I need to reinstall tensorflow without pip or something? Any advice is greatly appreciated.",0,1,False,self,,,,,
143,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,9,accdmt,github.com,New implementation of noisy natural gradient (including noisy K-FAC and noisy EK-FAC).,https://www.reddit.com/r/MachineLearning/comments/accdmt/new_implementation_of_noisy_natural_gradient/,jelly_zhang,1546562318,,0,1,False,default,,,,,
144,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,10,accmek,nlp.seas.harvard.edu,[D] Tensor Considered Harmful (A polemic against numpy / pytorch and a proposal for a named tensor),https://www.reddit.com/r/MachineLearning/comments/accmek/d_tensor_considered_harmful_a_polemic_against/,m_ke,1546563854,,40,1,False,default,,,,,
145,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,10,accp7s,voirfilms.lu, VOIRFILMS en Streaming | Voir Films en ligne [OFFICIEL],https://www.reddit.com/r/MachineLearning/comments/accp7s/voirfilms_en_streaming_voir_films_en_ligne/,SamanthaDuffqu8,1546564341,,0,1,False,default,,,,,
146,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,10,acctow,nlp.seas.harvard.edu,"[P] Tensor Considered Harmful: This post presents a proof-of-concept of an alternative approach, named tensors, with named dimensions.",https://www.reddit.com/r/MachineLearning/comments/acctow/p_tensor_considered_harmful_this_post_presents_a/,downtownslim,1546565152,,0,1,False,default,,,,,
147,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,10,accwx3,twitter.com,Andrew Ng's tweetstorm about the lack of processes in ML and data versioning,https://www.reddit.com/r/MachineLearning/comments/accwx3/andrew_ngs_tweetstorm_about_the_lack_of_processes/,dmpetrov,1546565732,,0,1,False,default,,,,,
148,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,11,acdbli,self.MachineLearning,[P] A Visual Exploration of Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/acdbli/p_a_visual_exploration_of_gaussian_processes/,baylearn,1546568399,"A [distill.pub](https://distill.pub)-styled interactive article explaining Gaussian Processes:

https://www.jgoertler.com/visual-exploration-gaussian-processes/
",2,1,False,self,,,,,
149,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,12,acdro6,getrevue.co,"Rubik's Cubes unraveled, self-driving cars &amp; junctions, robots spotting liars",https://www.reddit.com/r/MachineLearning/comments/acdro6/rubiks_cubes_unraveled_selfdriving_cars_junctions/,Enterprisesoft,1546571415,,0,1,False,https://b.thumbs.redditmedia.com/mfYui60kFROx9ZOtA3l1T-pYDjzM-z0K32x965_wT5E.jpg,,,,,
150,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,14,acern8,medium.com,A framework for trusted pretained neural networks  How a repository of trained data will help advance humanity towards the Singularity,https://www.reddit.com/r/MachineLearning/comments/acern8/a_framework_for_trusted_pretained_neural_networks/,shamoons,1546578439,,0,1,False,https://b.thumbs.redditmedia.com/NUHCf4Hz23U2NUX7l6hgOb0I8Xb90JLItyXCGRPyFkQ.jpg,,,,,
151,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,14,acf246,self.MachineLearning,[D] List of summer schools in machine learning (and related fields) across the globe,https://www.reddit.com/r/MachineLearning/comments/acf246/d_list_of_summer_schools_in_machine_learning_and/,sshkhr,1546580609,"Hello r/machinelearning   

I am trying to compile a list of machine learning related summer schools with their dates and deadlines etc. I thought it might be of interest to the general machine learning community, particularly early-stage graduate students. Here is the link to the GitHub repo:  

https://github.com/sshkhr/awesome-mlss",4,1,False,self,,,,,
152,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,16,acfp6m,ciolook.com,Husayn Kassai: Securing Customers Identity with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/acfp6m/husayn_kassai_securing_customers_identity_with/,ciolook,1546585732,,0,1,False,default,,,,,
153,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,17,acg4hd,self.MachineLearning,[P] Earthquake Predictor Neural Network,https://www.reddit.com/r/MachineLearning/comments/acg4hd/p_earthquake_predictor_neural_network/,JosephLChu,1546589644,"So, I made an update to my personal moonshot project.

[http://earthquakepredictor.net/](http://earthquakepredictor.net/)

The first iteration of this model only predicted up to about magnitude 2.5, and the second version only about 4.5, so the fact that it now predicts above 5.0 finally is progress to me.

And yes, I know the interface needs to be improved.  I also work, and coding at home after coding at work is draining...

One notable change is that the data is now preprocessed to be consistently moment magnitudes, using some conversion formulas I found in a paper the other day.",12,1,False,self,,,,,
154,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,17,acgc1k,self.MachineLearning,How do I train models using the cloud?,https://www.reddit.com/r/MachineLearning/comments/acgc1k/how_do_i_train_models_using_the_cloud/,ymazboud,1546591806,[removed],0,1,False,self,,,,,
155,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,17,acgcih,deepmind.com,Learning explanatory rules from noisy data,https://www.reddit.com/r/MachineLearning/comments/acgcih/learning_explanatory_rules_from_noisy_data/,leonardbonnet1,1546591942,,0,1,False,default,,,,,
156,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,18,acgh4n,self.MachineLearning,What is a codebook?,https://www.reddit.com/r/MachineLearning/comments/acgh4n/what_is_a_codebook/,huangzhy,1546593259,[removed],0,1,False,self,,,,,
157,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,18,acgj9q,youtube.com,Leela reacts calmly to a double pawn sacrifice by Stockfish in Sicilian ...,https://www.reddit.com/r/MachineLearning/comments/acgj9q/leela_reacts_calmly_to_a_double_pawn_sacrifice_by/,kingscrusher-youtube,1546593857,,0,1,False,https://b.thumbs.redditmedia.com/broXTu0wumflGpMK10-34B1Api8TACXSo34y0ndvuJw.jpg,,,,,
158,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,18,acgpa1,sloboda-studio.com,How to Use Machine Learning in Marketplace Development,https://www.reddit.com/r/MachineLearning/comments/acgpa1/how_to_use_machine_learning_in_marketplace/,birchyio,1546595473,,0,1,False,default,,,,,
159,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,18,acgqrg,self.MachineLearning,Commercial Prawn Crackers Making MachineShrimp Prawn Cracker Making mac...,https://www.reddit.com/r/MachineLearning/comments/acgqrg/commercial_prawn_crackers_making_machineshrimp/,fryingmachine,1546595874,[removed],0,1,False,self,,,,,
160,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,19,ach0c3,self.MachineLearning,[R] Graph Classification Using Structural Attention (KDD 2018).,https://www.reddit.com/r/MachineLearning/comments/ach0c3/r_graph_classification_using_structural_attention/,benitorosenberg,1546598474,"&amp;#x200B;

*Processing img wi9bzvhaxd821...*

Paper: [https://github.com/benedekrozemberczki/GAM/blob/master/paper.pdf](https://github.com/benedekrozemberczki/GAM/blob/master/paper.pdf)

PyTorch: [https://github.com/benedekrozemberczki/GAM](https://github.com/benedekrozemberczki/GAM)

Abstract:

Graph classification is a problem with practical applications in many  different domains. To solve this problem, one usually calculates certain  graph statistics (i.e. , graph features) that help discriminate between  graphs of different classes. When calculating such features, most  existing approaches process the entire graph. In a graphlet-based  approach, for instance, the entire graph is processed to get the total  count of different graphlets or subgraphs. In many real-world  applications, however, graphs can be noisy with discriminative patterns  confined to certain regions in the graph only. In this work, we study  the problem of attention-based graph classification . The use of  attention allows us to focus on small but informative parts of the  graph, avoiding noise in the rest of the graph. We present a novel RNN  model, called the Graph Attention Model (GAM), that processes only a  portion of the graph by adaptively selecting a sequence of informative  nodes. Experimental results on multiple real-world datasets show that  the proposed method is competitive against various well-known methods in  graph classification even though our method is limited to only a  portion of the graph.",1,1,False,self,,,,,
161,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,19,ach129,youtu.be,Impressive showcase of realistic 3D simulation capabilities in CVEDIA's SynCity at CES 2019! Accelerating the autonomous Revolution!,https://www.reddit.com/r/MachineLearning/comments/ach129/impressive_showcase_of_realistic_3d_simulation/,mslavescu,1546598679,,0,1,False,https://a.thumbs.redditmedia.com/KN2Dmry6hHQx60nFX3bUsL3wENwPHDuJk62HWXR5Lj8.jpg,,,,,
162,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,21,achiei,openreview.net,ANTISYMMETRICRNN: A DYNAMICAL SYSTEM VIEW ON RECURRENT NEURAL NETWORKS [ICLR '19],https://www.reddit.com/r/MachineLearning/comments/achiei/antisymmetricrnn_a_dynamical_system_view_on/,sidsig,1546603224,,4,1,False,default,,,,,
163,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,21,achijp,activewizards.com,Comparison of the Text Distance Metrics,https://www.reddit.com/r/MachineLearning/comments/achijp/comparison_of_the_text_distance_metrics/,viktoriia_shulga,1546603259,,0,1,False,default,,,,,
164,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,21,achqii,docs.google.com,Writing Code for NLP Research,https://www.reddit.com/r/MachineLearning/comments/achqii/writing_code_for_nlp_research/,mlvpj,1546605129,,0,1,False,default,,,,,
165,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,21,achtjp,self.MachineLearning,I can't figure out the use of learning curves over training and test set error.,https://www.reddit.com/r/MachineLearning/comments/achtjp/i_cant_figure_out_the_use_of_learning_curves_over/,regular_NewUser,1546605833,[removed],0,1,False,self,,,,,
166,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,22,aci629,self.MachineLearning,Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves,https://www.reddit.com/r/MachineLearning/comments/aci629/speeding_up_automatic_hyperparameter_optimization/,marsggbo,1546608609,[removed],0,1,False,self,,,,,
167,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,22,aci8uc,link.medium.com,"Simple explanation to accuracy, AUC ROC, precision, recall",https://www.reddit.com/r/MachineLearning/comments/aci8uc/simple_explanation_to_accuracy_auc_roc_precision/,AndrewWilber,1546609213,,0,1,False,default,,,,,
168,MachineLearning,t5_2r3gv,2019-1-4,2019,1,4,23,acik23,youtu.be,Bike Byers prediction using Machine Learning Studio,https://www.reddit.com/r/MachineLearning/comments/acik23/bike_byers_prediction_using_machine_learning/,techwrox,1546611463,,0,1,False,default,,,,,
169,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,0,aciy4f,self.MachineLearning,Celebrity Voice Changer app,https://www.reddit.com/r/MachineLearning/comments/aciy4f/celebrity_voice_changer_app/,lazyme1,1546614168,[removed],0,1,False,self,,,,,
170,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,0,acj42v,medium.com,InstaGAN Excels in Instance-Aware Image-To-Image Translation,https://www.reddit.com/r/MachineLearning/comments/acj42v/instagan_excels_in_instanceaware_imagetoimage/,Yuqing7,1546615244,,0,1,False,default,,,,,
171,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,0,acj8ky,self.MachineLearning,[Help] Please help me with my DL DevBox build,https://www.reddit.com/r/MachineLearning/comments/acj8ky/help_please_help_me_with_my_dl_devbox_build/,ando_khachatryan,1546616038,[removed],0,1,False,self,,,,,
172,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,0,acj9qu,youtu.be,Predicting the price using Azure Machine Learning Drag and Drop options,https://www.reddit.com/r/MachineLearning/comments/acj9qu/predicting_the_price_using_azure_machine_learning/,techwrox,1546616250,,0,1,False,default,,,,,
173,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,0,acjc1z,self.MachineLearning,[R]: Transformer Models in NLP,https://www.reddit.com/r/MachineLearning/comments/acjc1z/r_transformer_models_in_nlp/,vector_machines,1546616667,"Transformers are taking off quite rapidly, are being used for SoTA approaches. They were introduced in ""Attention is all you need"", OpenAI released GPT which builds upon it and recently Google recently released BERT. Learning about the functionality and architecture,usage can be tricky at first, so I released this resource on Transformers

Video: [https://www.youtube.com/watch?v=cgrqWBWzKjI](https://www.youtube.com/watch?v=cgrqWBWzKjI)

I have tried covering each and every aspect from multi head attention to positional encoding, tried to make sure you fully understand it. I hope you like it. Will further release more upon such SoTA approaches. ",0,1,False,self,,,,,
174,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,acjkwy,self.MachineLearning,Machine Learning Skills Among Data Scientists,https://www.reddit.com/r/MachineLearning/comments/acjkwy/machine_learning_skills_among_data_scientists/,andrea_manero,1546618146,[removed],0,1,False,self,,,,,
175,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,acjkxp,self.MachineLearning,[D] Speech Recognition and Common Voice,https://www.reddit.com/r/MachineLearning/comments/acjkxp/d_speech_recognition_and_common_voice/,limapedro,1546618149,"Hi guys! I here to ask some of you to join Common Voice. A little background: one of the problems in ML is scarce data, so we need tools to collect data, one of the areas in which labelled data is available is Speech, we also want data for various regions, accents and languages. Speech will be the main input for IoT devices, so we need to be ahead in the game to not depend on big companies to dictate and limit this technology. Share your progress in the comments.

Common Voice: [https://voice.mozilla.org/en](https://voice.mozilla.org/en)",40,1,False,self,,,,,
176,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,acjs5i,self.MachineLearning,ung lnl ung n rng r  r l r  n r n n r  r.,https://www.reddit.com/r/MachineLearning/comments/acjs5i/ung_lnl_ung_n_rng_r__r/,aakash30jan,1546619348,[removed],0,1,False,self,,,,,
177,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,acjs63,self.MachineLearning,ung lnl ung n rng r  r l r  n r n n r  r.,https://www.reddit.com/r/MachineLearning/comments/acjs63/ung_lnl_ung_n_rng_r__r/,tsghosh,1546619350,[removed],0,1,False,self,,,,,
178,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,acjs94,self.MachineLearning,lrng  buul n  ung n l ng n n n.,https://www.reddit.com/r/MachineLearning/comments/acjs94/lrng__buul_n__ung_n_l/,epleisman,1546619363,[removed],0,1,False,self,,,,,
179,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,acjtrh,self.MachineLearning,[P] Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization,https://www.reddit.com/r/MachineLearning/comments/acjtrh/p_alternating_direction_method_of_multipliers_for/,benitorosenberg,1546619607,"&amp;#x200B;

https://i.redd.it/qfels87eof821.jpg

Paper: [http://statweb.stanford.edu/\~dlsun/papers/nmf\_admm.pdf](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf)

Python: [https://github.com/benedekrozemberczki/NMFADMM](https://github.com/benedekrozemberczki/NMFADMM)

Abstract:

Non-negative matrix factorization (NMF) is a popular method for learning  interpretable features from non-negative data, such as counts or  magnitudes. Different cost functions are used with NMF in different  applications. We develop an algorithm, based on the alternating  direction method of multipliers, that tackles NMF problems whose cost  function is a beta-divergence, a broad class of divergence functions. We  derive simple, closed-form updates for the most commonly used  beta-divergences. We demonstrate experimentally that this algorithm has  faster convergence and yields superior results to state-of-the-art  algorithms for this problem.",0,1,False,https://a.thumbs.redditmedia.com/WGvZ5qAh-uQevg0uubhI9x-b32tN4g1ercoLwJ9quL4.jpg,,,,,
180,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,acjxr1,self.MachineLearning,"[R] A PyTorch implementation of ""Graph Classification Using Structural Attention"" (KDD 2018).",https://www.reddit.com/r/MachineLearning/comments/acjxr1/r_a_pytorch_implementation_of_graph/,benitorosenberg,1546620281,"&amp;#x200B;

https://i.redd.it/vf0219ecqf821.jpg

Python: [https://github.com/benedekrozemberczki/GAM](https://github.com/benedekrozemberczki/GAM)

Paper: [https://github.com/benedekrozemberczki/GAM/blob/master/paper.pdf](https://github.com/benedekrozemberczki/GAM/blob/master/paper.pdf)

Abstract:

Graph classification is a problem with practical applications in many  different domains. To solve this problem, one usually calculates certain  graph statistics (i.e., graph features) that help discriminate between  graphs of different classes. When calculating such features, most  existing approaches process the entire graph. In a graphlet-based  approach, for instance, the entire graph is processed to get the total  count of different graphlets or subgraphs. In many real-world  applications, however, graphs can be noisy with discriminative patterns  confined to certain regions in the graph only. In this work, we study  the problem of attention-based graph classification . The use of  attention allows us to focus on small but informative parts of the  graph, avoiding noise in the rest of the graph. We present a novel RNN  model, called the Graph Attention Model (GAM), that processes only a  portion of the graph by adaptively selecting a sequence of informative  nodes. Experimental results on multiple real-world datasets show that  the proposed method is competitive against various well-known methods in  graph classification even though our method is limited to only a  portion of the graph.",4,1,False,https://b.thumbs.redditmedia.com/DavWewO7XtWOxraKB2f9ZYagAbvnkZ8PbNvVIGLbsiU.jpg,,,,,
181,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,1,ack2w8,self.MachineLearning,Machine learning and parallel programming,https://www.reddit.com/r/MachineLearning/comments/ack2w8/machine_learning_and_parallel_programming/,skenypatel,1546621145,[removed],0,1,False,self,,,,,
182,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,2,ackgi0,self.MachineLearning,[Discussion] What is the Future of Industry Careers in Machine Learning due to Better and Better APIs?,https://www.reddit.com/r/MachineLearning/comments/ackgi0/discussion_what_is_the_future_of_industry_careers/,Kyak787,1546623361,"Someone on Reddit told me that their opinion on Machine Learning is that one should only study machine learning if they want an academic career in it. Their reasoning: because Machine Learning engineers are so expensive, the trend in tech is that companies are creating automated APIs to encapsulate the field so less skilled professions can incorporate machine learning into their stacks. In the future, APIs like Tensorflow and NumPy will allow average programmers to accomplish 90% of what an expert can achieve. It isn't a long-term skill in Industry. Also, companies are extremely picky about data scientists and other high-demand experts due to their high salaries. Basic data analysts are much more broadly employable.


How true is this perspective?


This raises the following question:


Where is the field moving? If individuals do decide to self study and then get a Master's in Machine Learning (with a specialization like data science/statistics) over the next decade, to emphasize that they are not 'script kiddies' and to possibly get into management, how employable will they be and how fierce will competition be for expert roles?

What other fields in technology do you think will be in demand and start rapidly growing over the next 5-10 years? Is that even predictable?

As Machine Learning becomes more automated, how can I make sure that the time and money I spent on my degree stays relevant / doesn't depreciate? What other skills will I need to be highly successful?",51,1,False,self,,,,,
183,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,3,ackq6x,self.MachineLearning,Problem with a character recognition model,https://www.reddit.com/r/MachineLearning/comments/ackq6x/problem_with_a_character_recognition_model/,AI_Ocelot,1546624905,[removed],0,1,False,self,,,,,
184,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,4,aclcl5,self.MachineLearning,"Has anyone used a GAN in a real project? If so, what was your use case?",https://www.reddit.com/r/MachineLearning/comments/aclcl5/has_anyone_used_a_gan_in_a_real_project_if_so/,yeamusic211,1546628452,[removed],0,1,False,self,,,,,
185,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,4,aclfmm,self.MachineLearning,Salary: Ethical Hacking VS Machine/Deep Learning,https://www.reddit.com/r/MachineLearning/comments/aclfmm/salary_ethical_hacking_vs_machinedeep_learning/,TechDude12,1546628923,[removed],0,1,False,self,,,,,
186,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,5,aclzor,medium.com,To Hire or Not to Hire Online AI Grads: That Is the Question,https://www.reddit.com/r/MachineLearning/comments/aclzor/to_hire_or_not_to_hire_online_ai_grads_that_is/,Yuqing7,1546632065,,0,1,False,https://b.thumbs.redditmedia.com/uadvMZRMJJpJnPN-1b3auT-0S7wwsOpoSVfGGtbewpg.jpg,,,,,
187,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,5,aclzt8,medium.com,Closing the machine learning adoption gap in industry,https://www.reddit.com/r/MachineLearning/comments/aclzt8/closing_the_machine_learning_adoption_gap_in/,jimmyechan,1546632084,,0,1,False,default,,,,,
188,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,5,acm76z,self.compsci,[R] Predicting and Imitating Complex Data Using Information Theory,https://www.reddit.com/r/MachineLearning/comments/acm76z/r_predicting_and_imitating_complex_data_using/,Feynmanfan85,1546633264,,0,1,False,default,,,,,
189,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,5,acmaug,researchgate.net,[R] Predicting and Imitating Complex Data,https://www.reddit.com/r/MachineLearning/comments/acmaug/r_predicting_and_imitating_complex_data/,Feynmanfan85,1546633866,,0,1,False,default,,,,,
190,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,5,acmbpp,youtube.com,Fast Neural Style Transfer demo. Turn your pictures into artwork,https://www.reddit.com/r/MachineLearning/comments/acmbpp/fast_neural_style_transfer_demo_turn_your/,ahmedbesbes,1546634019,,0,1,False,default,,,,,
191,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,5,acmeh2,self.MachineLearning,Mistakes while training nns?,https://www.reddit.com/r/MachineLearning/comments/acmeh2/mistakes_while_training_nns/,Onyx_Hydra,1546634485,[removed],0,1,False,self,,,,,
192,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,5,acmja8,self.MachineLearning,[D] Finding optimal threshold for F1 evaluation,https://www.reddit.com/r/MachineLearning/comments/acmja8/d_finding_optimal_threshold_for_f1_evaluation/,killver,1546635248,"I am currently struggling a bit with the following setup: I have a typical training and test set with binary outcomes and my evaluation measure of choice is F1 score. How can I optimize the threshold for the decision boundary on predicted probability. My model of choice is an ensemble of deep learning models, with considerable variance.

Currently, I have the following two options:

* Train the model k time based on a k-fold cross validation, predict the out of folds, concatenate them, and find the optimal threshold for it. Then use this threshold on the test dataset with an average of the predictions from the five models. This currently works the best, but I can also see that I can either get lucky on the threshold, or not and sometimes the optimal test threshold is quite far away. Also, the different models have sometimes quite different optimal thresholds on their own folds. There seems to be some form of ""winner's curse"" [1].

* I have tried to calibrate the models from the k-folds, this stabilizes the predictions a bit, but still similar issues with the threshold.

* A different setup is that I split the train set in a train and validation set, fit the model on train, and optimize on validation. Even though the F1 score on validation is quite good, the threshold again can be quite bad for the test dataset.

* For the previous example, I have also tried to split the validation set k times, and calculate the optimal threshold k times and then take the average or median of it, no real help in that.

I am wondering if there are some other techniques that I am missing, or whether I will always either get lucky or not due to the variance of my models.

[1] https://arxiv.org/pdf/1402.1892.pdf",10,1,False,self,,,,,
193,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,6,acmo6q,self.MachineLearning,Intuition behind pooling layers in CNNs?,https://www.reddit.com/r/MachineLearning/comments/acmo6q/intuition_behind_pooling_layers_in_cnns/,yeamusic211,1546636024,[removed],0,1,False,self,,,,,
194,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,7,acncy2,self.MachineLearning,Possible ML or Deep Learning paths for a biomedical engineer.,https://www.reddit.com/r/MachineLearning/comments/acncy2/possible_ml_or_deep_learning_paths_for_a/,unkemptmagio,1546640061,[removed],0,1,False,self,,,,,
195,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,13,acqk1i,self.MachineLearning,Reinforcement Learning on StreetFighter2 (MD 1993) with Tensorflow &amp; Bizhawk,https://www.reddit.com/r/MachineLearning/comments/acqk1i/reinforcement_learning_on_streetfighter2_md_1993/,HeyRaaay,1546661392,[removed],0,1,False,https://a.thumbs.redditmedia.com/tD0iEGlmuxKtCdpTcec6Tn3xXJEB_zztWCVfoAsV6M8.jpg,,,,,
196,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,14,acrdtg,ml-cheatsheet.readthedocs.io,Machine Learning Cheatsheet,https://www.reddit.com/r/MachineLearning/comments/acrdtg/machine_learning_cheatsheet/,ConfidentMushroom,1546667729,,0,1,False,default,,,,,
197,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,15,acrnjw,self.MachineLearning,Want to get into machine learning,https://www.reddit.com/r/MachineLearning/comments/acrnjw/want_to_get_into_machine_learning/,captainspockears,1546670054,[removed],0,1,False,self,,,,,
198,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,17,acsb5a,self.MachineLearning,[P] Trying to train a model for Berkeley's DeepDrive (bdd100k),https://www.reddit.com/r/MachineLearning/comments/acsb5a/p_trying_to_train_a_model_for_berkeleys_deepdrive/,Raymond-Wu,1546676115,"Hi all, just wanted to see if anyone could help me with this segmentation model I was training. I was just about to start creating a U-Net in fastai's version of pytorch but my current model is consistently underfitting. If anyone sees something glaringly obvious or have any ideas on what I can try I'm all ears. Thanks!

https://github.com/RaymondDashWu/Berkeley-DeepDrive/blob/master/Berkeley%20DeepDrive.ipynb",4,1,False,self,,,,,
199,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,17,acseu6,louiskirsch.com,"Reinforcement Learning  The different challenges, methods and frontiers",https://www.reddit.com/r/MachineLearning/comments/acseu6/reinforcement_learning_the_different_challenges/,Time-Over,1546677145,,0,1,False,default,,,,,
200,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,18,acsomu,self.MachineLearning,Thesis Ideas in Machine Learning [Repost from /r/learnmachinelearning],https://www.reddit.com/r/MachineLearning/comments/acsomu/thesis_ideas_in_machine_learning_repost_from/,Darnegar,1546680117,[removed],0,1,False,self,,,,,
201,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,19,acsxgi,self.MachineLearning,Role of weight decay in montecarlo dropout,https://www.reddit.com/r/MachineLearning/comments/acsxgi/role_of_weight_decay_in_montecarlo_dropout/,noquestionnoanswers,1546682795,[removed],0,1,False,self,,,,,
202,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,19,act071,self.MachineLearning,New to torch. Have a basic question that I have not been able to google,https://www.reddit.com/r/MachineLearning/comments/act071/new_to_torch_have_a_basic_question_that_i_have/,ulffy,1546683572,[removed],0,1,False,self,,,,,
203,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,20,actf9k,self.MachineLearning,Titles for Master's Thesis,https://www.reddit.com/r/MachineLearning/comments/actf9k/titles_for_masters_thesis/,Darnegar,1546687885,[removed],0,1,False,self,,,,,
204,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,21,actowq,golfdigest.com,Callaway Epic Flash drivers fundamentally change face technology through artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/actowq/callaway_epic_flash_drivers_fundamentally_change/,ShannonOh,1546690660,,1,1,False,default,,,,,
205,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,21,actrlz,self.MachineLearning,[P] PyCM 1.8 released: Machine learning library for confusion matrix statistical analysis,https://www.reddit.com/r/MachineLearning/comments/actrlz/p_pycm_18_released_machine_learning_library_for/,sepandhaghighi,1546691420," [https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm) 

[http://www.pycm.ir](http://www.pycm.ir)

&amp;#x200B;

Changelog : 

* Lift Score (LS) added [\#128](https://github.com/sepandhaghighi/pycm/issues/128)
* color argument added to save\_html method [\#130](https://github.com/sepandhaghighi/pycm/issues/130)
* version\_check.py added
* Error messages modified [\#132](https://github.com/sepandhaghighi/pycm/issues/132)
* Document modified [\#131](https://github.com/sepandhaghighi/pycm/issues/131)
* Website changed to [http://www.pycm.ir](http://www.pycm.ir/)
* Interpretation functions moved to pycm\_interpret.py
* Utility functions moved to pycm\_util.py
* Unnecessary els and elif removed
* == changed to  to is",10,1,False,self,,,,,
206,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,21,actsv6,self.MachineLearning,"In India, Artificial Intelligence to be taught in Classes 8, 9 &amp; 10.",https://www.reddit.com/r/MachineLearning/comments/actsv6/in_india_artificial_intelligence_to_be_taught_in/,shishir02,1546691767,[removed],0,1,False,self,,,,,
207,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,21,actwb9,self.MachineLearning,[P] (art)ificial - CYAN COLLECTION,https://www.reddit.com/r/MachineLearning/comments/actwb9/p_artificial_cyan_collection/,greentfrapp,1546692737,"This is the first part of a bigger project on creating art with AI.

Here, a Compositional Pattern Producing Network (CPPN) is hooked up to a trained image classifier, then we optimize the weights of the CPPN to maximize the activation of a neuron in the classifier. In this case we maximize one of the output neurons. In addition, we impose a hue constraint on the CPPN so that the resulting image takes on a cyan tone.

View CYAN COLLECTION [here](https://github.com/greentfrapp/artificial-01-cyan-collection/tree/master/CYAN%20COLLECTION).

Script that created the images [here](https://github.com/greentfrapp/artificial-01-cyan-collection).

I will also be uploading future works on this [IG account](https://www.instagram.com/cyan_strawberry/)!

For more details about the method, see the CPPN section in this article:
[Mordvintsev, et al., ""Differentiable Image Parameterizations"", Distill, 2018.](https://distill.pub/2018/differentiable-parameterizations/)",0,1,False,self,,,,,
208,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,22,acu1dr,self.MachineLearning,"Now that I've decent theoretical foundation in ML, where can I find simple, already solved, practice exercises to better my understanding of data science?",https://www.reddit.com/r/MachineLearning/comments/acu1dr/now_that_ive_decent_theoretical_foundation_in_ml/,van1van,1546693966,[removed],0,1,False,self,,,,,
209,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,22,acu2r5,self.MachineLearning,[P] Fast Sparse Non-Negative Matrix Factorization With The Alternating Direction Method of Multipliers,https://www.reddit.com/r/MachineLearning/comments/acu2r5/p_fast_sparse_nonnegative_matrix_factorization/,benitorosenberg,1546694307,"&amp;#x200B;

https://i.redd.it/gouz4l1kul821.jpg

Paper: [https://github.com/benedekrozemberczki/NMFADMM/blob/master/paper.pdf](https://github.com/benedekrozemberczki/NMFADMM/blob/master/paper.pdf)

Python: [https://github.com/benedekrozemberczki/NMFADMM](https://github.com/benedekrozemberczki/NMFADMM)

Abstract:

Non-negative matrix factorization (NMF) is a popular method for learning  interpretable features from non-negative data, such as counts or  magnitudes. Different cost functions are used with NMF in different  applications. We develop an algorithm, based on the alternating  direction method of multipliers, that tackles NMF problems whose cost  function is a beta-divergence, a broad class of divergence functions. We  derive simple, closed-form updates for the most commonly used  beta-divergences. We demonstrate experimentally that this algorithm has  faster convergence and yields superior results to state-of-the-art  algorithms for this problem.",15,1,False,https://b.thumbs.redditmedia.com/5fQUSfx-K1Z7fTI_2qs_1NAfOJ1SpnTyWiIMjT4uukw.jpg,,,,,
210,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,22,acu456,self.MachineLearning,"Now that I've decent theoretical foundation in ML, where can I find simple, already solved, practice exercises to better my understanding of data science?",https://www.reddit.com/r/MachineLearning/comments/acu456/now_that_ive_decent_theoretical_foundation_in_ml/,van1van,1546694642,[removed],0,1,False,self,,,,,
211,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,22,acu7dw,self.MachineLearning,Tensorflow MDLSTM implementation,https://www.reddit.com/r/MachineLearning/comments/acu7dw/tensorflow_mdlstm_implementation/,BiggusDickus123,1546695416,[removed],0,1,False,self,,,,,
212,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,22,acu7fz,self.Python,"[P] Permon, an extensible open source performance monitor for machine learning",https://www.reddit.com/r/MachineLearning/comments/acu7fz/p_permon_an_extensible_open_source_performance/,bminixhofer,1546695432,,0,1,False,default,,,,,
213,MachineLearning,t5_2r3gv,2019-1-5,2019,1,5,22,acu9wl,self.MachineLearning,Reinforcement learning for continuous state and action space,https://www.reddit.com/r/MachineLearning/comments/acu9wl/reinforcement_learning_for_continuous_state_and/,thatpizzatho,1546696047,"Hi guys, I'm here to ask for an alternative approach to what I am trying to do.

**Problem**

My goal is to apply Reinforcement Learning to predict the next state of an object under a known force in a 3D environment (the approach would be reduced to supervised learning, off-line learning).

**Details of my approach**

The current state is the vector representing the position of the object in the environment (3 dimensions), and the velocity of the object (3 dimensions). The starting position is randomly initialized in the environment, as well as the starting velocity.

The action is the vector representing the movement from state *t* to state *t+1*.

The reward is just the Euclidean distance between the predicted next state, and the real next state (I already have the target position).

**What have I done so far?**

I have been looking for many methods to do this. *Deep Deterministic Policy Gradients* works for a continuous action space, but in my case I also have a continuous state space. If you are interested in this approach, here's the original paper written at DeepMind:[http://proceedings.mlr.press/v32/silver14.pdf](http://proceedings.mlr.press/v32/silver14.pdf)

The *Actor-Critic* approach should work, but it is usually (or always) applied to discrete and low-dimensional state space.

*Q-Learning* and *Deep-Q Learning* cannot handle high dimensional state space, so my configuration would not work even if discretizing the state space.

*Inverse Reinforcement Learning* (an instance of Imitation learning, with *Behavioral Cloning* and *Direct Policy Learning*) approximates a reward function when finding the reward function is more complicated than finding the policy function. Interesting approach, but I haven't seen any implementation, and in my case the reward function is pretty straightforward.

Is there a methodology to deal with my configuration that I haven't explored yet?",0,1,False,self,,,,,
214,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,0,acv3i0,self.MachineLearning,[D] How can you capitalize on creating a new ML component,https://www.reddit.com/r/MachineLearning/comments/acv3i0/d_how_can_you_capitalize_on_creating_a_new_ml/,HjalmarLucius,1546702587,"I have a solid mathematical background and program decently well but my career for over a decade has been in finance. In recent years I've spent a lot of my spare weekends developing ML components (e.g. new gates such as attention and LSTM) and I see I'm starting to get somewhere.

If I were to create something revolutionary (example: a universally better convolution gate), does anyone have an idea on how I can capitalize on it? Do patents work in this field? I've seen some on dropout but little else.

My concern is that the components are easily replicable as they're really just an equation $relu(x) = max(0,x)$ - so once it's out, it's easy for anyone to implement. A paper can be published but that doesn't pay the mortgage.

Would love for it to become a full-time thing but I just don't see how I can use my work for anything else than part of a job/university application.",21,1,False,self,,,,,
215,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,1,acvhuk,codecampanion.blogspot.com,AI Assistance With PyText,https://www.reddit.com/r/MachineLearning/comments/acvhuk/ai_assistance_with_pytext/,AshishKhuraishy,1546705342,,0,1,False,default,,,,,
216,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,1,acviu7,self.MachineLearning,[P] Keras Implementation of Sketch-RNN,https://www.reddit.com/r/MachineLearning/comments/acviu7/p_keras_implementation_of_sketchrnn/,eyalzk,1546705530,"My Keras implementation of the Sketch-RNN algorithm

[github.com/eyalzk/sketch\_rnn\_keras](https://github.com/eyalzk/sketch_rnn_keras)

Sharing for your feedback, but mainly because I ran into some advanced Keras topics that are not widely documented online and I think this repo is a nice example of how to approach these issues. For example:

* Injecting values to intermediate tensors and predicting the corresponding values of other tensors by building sub-models
* Using an auxiliary loss term that uses intermediate layers' outputs rather than the model's predictions
* Using a CuDNN LSTM layer, while allowing inference on CPU
* Resuming a training process from a checkpoint in the case that custom callbacks are used with dynamic internal variables
* Using custom generators to wrap data loader classes

If there's demand, I would be happy to write explicit tutorials for the mentioned topics.

&amp;#x200B;

Also included pre-trained models of cat &amp; cat-guitar which are not included in the official release.

[Interpolation in latent space between sketches of a cat and a guitar](https://i.redd.it/r2ahynffrm821.jpg)",6,1,False,https://b.thumbs.redditmedia.com/b6m7B8F38irmVVQsy-FzgkLCx61eibKzr_67OP_lpGU.jpg,,,,,
217,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,2,acvua4,self.MachineLearning,[P] MLG v2 - Visual Machine Learning Graph of all arXiv papers and researchers,https://www.reddit.com/r/MachineLearning/comments/acvua4/p_mlg_v2_visual_machine_learning_graph_of_all/,ranihorev,1546707608,"Hey everyone,  


After receiving a lot of feedback from you (thanks!), I rebuilt my project - a visual network of Machine &amp; Deep Learning papers.  

You can search now for any paper or author and start wandering through their references and citations (and through theirs and so on...). Once you expanded a few nodes you can re-arrange the network and see some interesting connections between nodes.   


[https://arxiv.lyrn.ai/citations\_network](https://arxiv.lyrn.ai/citations_network?fbclid=IwAR2DD2os4CKFJkSuIy4U_nC_pVBapfGH8xluEYMcVOuxmGjtvZ0fhEuEkd4)  


The project is based on data from arXiv and SemanticScholar. It's open source of course, and I hope that people would add some interesting algorithms to it, such as clustering, coloring, etc.

Code: [https://github.com/ranihorev/arxiv-network-graph](https://github.com/ranihorev/arxiv-network-graph?fbclid=IwAR3TjCG2uX4FeSHk_bAQ2yFZ2W1sywmiBd_e7cLJ00VTieSxNFFI1o-OCd0)

I'd love to hear your opinion :)  


&amp;#x200B;",18,1,False,self,,,,,
218,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,2,acw2su,self.learnmachinelearning,"daily, weekly or monthly time-series - any large, open source datasets out there?",https://www.reddit.com/r/MachineLearning/comments/acw2su/daily_weekly_or_monthly_timeseries_any_large_open/,gonzales82,1546709064,,0,1,False,default,,,,,
219,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,3,acwihm,self.MachineLearning,Machine Learning help,https://www.reddit.com/r/MachineLearning/comments/acwihm/machine_learning_help/,jcai2780,1546711752,[removed],0,1,False,self,,,,,
220,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,3,acwji2,self.MachineLearning,What some interesting problems or training data sets where the inputs include both data and and their first derivative?,https://www.reddit.com/r/MachineLearning/comments/acwji2/what_some_interesting_problems_or_training_data/,manfromfuture,1546711923,[removed],0,1,False,self,,,,,
221,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,3,acwqp3,youtube.com,This software lets you create labeled image datasets from videos of objects using Re3,https://www.reddit.com/r/MachineLearning/comments/acwqp3/this_software_lets_you_create_labeled_image/,Weihua99,1546713144,,0,1,False,https://b.thumbs.redditmedia.com/GtCscyqqUrI_uMfrXyQE2DJVnIt7n5HoGemAjzB5K0E.jpg,,,,,
222,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,3,acwr8n,self.MachineLearning,Image Recognition help,https://www.reddit.com/r/MachineLearning/comments/acwr8n/image_recognition_help/,jcai2780,1546713233,[removed],0,1,False,self,,,,,
223,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,4,acx7qn,self.MachineLearning,[D] Can I find a pre-trained Deep Neural Network based image de-noising model?,https://www.reddit.com/r/MachineLearning/comments/acx7qn/d_can_i_find_a_pretrained_deep_neural_network/,gourxb,1546716028,"I am looking for a fast/reliable deep neural network based pre trained model to remove artifacts/grains from a set of images that I have.

I have tried OpenCV \[1\] as well as scikit-image \[2\] denoising techniques. I am not satisfied with the result. 

&amp;#x200B;

\[1\] [https://docs.opencv.org/3.0-beta/modules/photo/doc/denoising.html](https://docs.opencv.org/3.0-beta/modules/photo/doc/denoising.html) 

\[2\] [http://scikit-image.org/docs/dev/auto\_examples/filters/plot\_denoise.html](http://scikit-image.org/docs/dev/auto_examples/filters/plot_denoise.html) ",5,1,False,self,,,,,
224,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,6,acyrtw,self.MachineLearning,[p] Short series on Gaussian processes. Simple Python implementation + practical TensorFlow probability example.,https://www.reddit.com/r/MachineLearning/comments/acyrtw/p_short_series_on_gaussian_processes_simple/,Xochipilli,1546725326,"Just finalized a short series on Gaussian Processes I've been working on for a while. It's a 3 part series:

1. Part 1 introduces the concept of Gaussian processes with the help of a simple Python implementation: [https://peterroelants.github.io/posts/gaussian-process-tutorial/](https://peterroelants.github.io/posts/gaussian-process-tutorial/)
2. Part 2 illustrates how to practically fit kernel hyperparameters to data with the help of TensorFlow probability: [https://peterroelants.github.io/posts/gaussian-process-kernel-fitting/](https://peterroelants.github.io/posts/gaussian-process-kernel-fitting/)
3. Part 3 goes more in-depth in the different kernels fitted in part 2: [https://peterroelants.github.io/posts/gaussian-process-kernels/](https://peterroelants.github.io/posts/gaussian-process-kernels/)",14,1,False,self,,,,,
225,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,7,acyw6v,arxiv.org,[R] Lenia - Biology of Artificial Life,https://www.reddit.com/r/MachineLearning/comments/acyw6v/r_lenia_biology_of_artificial_life/,downtownslim,1546726082,,2,1,False,default,,,,,
226,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,7,acz8nv,self.MachineLearning,[P] Complete Online Course about Generative Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/acz8nv/p_complete_online_course_about_generative/,martinmusiol14,1546728318,"[Generative AI](https://generativeai.net) will drive the next generation  of apps for autoprogramming, content development, visual arts, and other  creative, design, and engineering activities.

And, as it will disrupt the way we create content of any kind (mostly because of the advent of GANs), my team and I are working on an upcoming online course about generative AI. 

See course page here: [https://generativeai.net](https://generativeai.net) 

Feedback welcome!

&amp;#x200B;",3,1,False,self,,,,,
227,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,7,acz9y4,security.nambilisamuel.com,How AI Boost Airport Security And Speed Up The Operation?,https://www.reddit.com/r/MachineLearning/comments/acz9y4/how_ai_boost_airport_security_and_speed_up_the/,sciencetech28,1546728552,,1,1,False,default,,,,,
228,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,8,aczhvp,self.MachineLearning,[D] What is something that has blown your mind?,https://www.reddit.com/r/MachineLearning/comments/aczhvp/d_what_is_something_that_has_blown_your_mind/,sugarhilldt2,1546729977,"Research, statistic, concept, project, etc. ",95,1,False,self,,,,,
229,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,9,aczyud,medium.com,"Two years in the life of AI, ML, DL and Java",https://www.reddit.com/r/MachineLearning/comments/aczyud/two_years_in_the_life_of_ai_ml_dl_and_java/,neomatrix369,1546733031,,0,1,False,default,,,,,
230,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,10,ad0y4t,self.MachineLearning,Interior-point-convex method for quadratic programming,https://www.reddit.com/r/MachineLearning/comments/ad0y4t/interiorpointconvex_method_for_quadratic/,gimmiKizz,1546739148,"Any reference that I could read to understand the interior-point-convex method to solve quadratic programming problems? Please help. BTW, I want to study this in the context of SVM.",0,1,False,self,,,,,
231,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,10,ad0yfn,tempt.ai,The future is now. We're bringing together many open-sourced image manipulation libraries to create infinitely customizable adult content.,https://www.reddit.com/r/MachineLearning/comments/ad0yfn/the_future_is_now_were_bringing_together_many/,temptai,1546739200,,0,1,False,default,,,,,
232,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,11,ad1kdq,reddit.com,"if you have data science memes, submit them here, thanks",https://www.reddit.com/r/MachineLearning/comments/ad1kdq/if_you_have_data_science_memes_submit_them_here/,jensyao,1546743509,,0,1,False,default,,,,,
233,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,13,ad2a3t,self.MachineLearning,[R] Perturbative Neural Networks? Authors claim to have a much simpler replacement for convolutions.,https://www.reddit.com/r/MachineLearning/comments/ad2a3t/r_perturbative_neural_networks_authors_claim_to/,Hefty_Fox,1546748843,"[https://arxiv.org/abs/1806.01817](https://arxiv.org/abs/1806.01817)

This would be a big breakthrough if true. They replace convolution with a much simpler set of operations. Anyone comment on whether this result holds?

&gt;Convolutional neural networks are witnessing wide adoption in computer vision systems with numerous applications across a range of visual recognition tasks. Much of this progress is fueled through advances in convolutional neural network architectures and learning algorithms even as the basic premise of a convolutional layer has remained unchanged. In this paper, we seek to revisit the convolutional layer that has been the workhorse of state-of-the-art visual recognition models. We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer. **The perturbation layer does away with convolution in the traditional sense and instead computes its response as a weighted linear combination of non-linearly activated additive noise perturbed inputs.** We demonstrate both analytically and empirically that this perturbation layer can be an effective replacement for a standard convolutional layer. Empirically, deep neural networks with perturbation layers, called Perturbative Neural Networks (PNNs), in lieu of convolutional layers perform comparably with standard CNNs on a range of visual datasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters.

&amp;#x200B;",18,1,False,self,,,,,
234,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,13,ad2fh9,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/ad2fh9/global_machine_learning_market_size_outlook/,srikanthenvision,1546749976,,0,1,False,default,,,,,
235,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,14,ad2o3l,self.MachineLearning,What tools do you use to manage your notes about Machine Learning and Algorithms?,https://www.reddit.com/r/MachineLearning/comments/ad2o3l/what_tools_do_you_use_to_manage_your_notes_about/,JAGGI_JATT,1546751853,"I am struggling to keep all my learnings about Machine Learning together in one place. Most of my notes about ML papers are on Mendeley or the paper's hard copy themselves.

Most of my notes about books are on the hard copy's of the books themselves. And stuff I learn and read online from courses are on my laptop in the form of word doc's and ppts.

I am wondering if there is one long term strategy for maintaining notes about your studies and learning for reference and revision later. I know google drive is one option, but it doesnt really have features similar to Mendeley, and is not accessible from work.

How do you guys manage your notes and learnings? Any recommendations for the long term would be most welcome.",0,1,False,self,,,,,
236,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,15,ad30xg,oodlestechnologies.com,The Rise of Artificial Superintelligence,https://www.reddit.com/r/MachineLearning/comments/ad30xg/the_rise_of_artificial_superintelligence/,tech-info,1546754872,,0,1,False,default,,,,,
237,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,15,ad38sn,self.MachineLearning,"Optimization is not part of Inference, Optimization is part of decision theory",https://www.reddit.com/r/MachineLearning/comments/ad38sn/optimization_is_not_part_of_inference/,sriharsha_0806,1546756837,[removed],0,1,False,self,,,,,
238,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,16,ad3ldf,self.MachineLearning,What do singularities mean in neural networks?,https://www.reddit.com/r/MachineLearning/comments/ad3ldf/what_do_singularities_mean_in_neural_networks/,albert1905,1546760241,[removed],0,1,False,self,,,,,
239,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,17,ad3t0o,self.MachineLearning,Discuss YouTube bots machine learning with me....,https://www.reddit.com/r/MachineLearning/comments/ad3t0o/discuss_youtube_bots_machine_learning_with_me/,BroadGeneral,1546762440,"I posted this inside the YouTube sub, but I don't think it will get many responses.

I'm just interested in this subject and speculating.
So I was reading through some articles yesterday:

https://www.eff.org/issues/intellectual-property/guide-to-youtube-removals

https://www.google.com/amp/s/techcrunch.com/2018/04/23/youtube-releases-its-first-report-about-how-it-handles-flagged-videos-and-policy-violations/amp/

From my understanding it would seem:

User uploads a video &gt; bot scrapes the video &gt; checks for X red flags that may break their TOS

Users flag video &gt; YouTube bot is sent to video &gt; compares flag reason with video content &gt; takes down/leave video up?
Video remains after X flags &gt; continues to receive flags &gt; YouTube representative eventually manually checks the video &gt; makes a note on their system, so changes can be implemented on the bot machine learning mistakes

User creates copyright claim on video &gt; YouTube representatives manually check the attachments &gt; make a decision (keep/remove) the video &gt; 
video uploader sends copyright counter &gt; once again YouTube representatives manually check this counter &gt; counter is eiher denied/forwarded

I'm interested to hear your thoughts regarding all of this. It's obvious YouTube can NEVER manually deal with ALL the flags so must rely on their machine learning bots, but there must be tons of inaccurate takedowns.

When someones video is removed and they counter it in order to get it back, once done, they receive an automated email with a decision. So 

I'm VERY interested to learn about whom is behind that, is it their machine learning bots actually reviewing counters, or would YouTube representatives be checking them?


",1,1,False,self,,,,,
240,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,18,ad42iy,self.MachineLearning,Convolution Sequence to Sequence HTR Implementation Issues,https://www.reddit.com/r/MachineLearning/comments/ad42iy/convolution_sequence_to_sequence_htr/,spacevstab,1546765388,[removed],0,1,False,self,,,,,
241,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,19,ad4drw,self.MachineLearning,[D] Convolution Sequence to Sequence for HTR Pytorch Implementation Issues,https://www.reddit.com/r/MachineLearning/comments/ad4drw/d_convolution_sequence_to_sequence_for_htr/,spacevstab,1546768895,"Hello, I am trying to implement a sequence to sequence-based model for handwritten text recognition. The paper is [here](https://arxiv.org/pdf/1806.00578.pdf) (SCAN) which primarily uses Convolution Sequence to Sequence \[Gehring\]. I am using IAM Dataset which has almost 7000 training image and text pairs. I have modelled in almost the same as the SCAN paper indicates but there are some issues with my implementation. The model is giving very poor accuracy on test set. After the first epoch, the loss value doesnt decreases but revolves around a range of value. This is my very first paper implementation on Pytorch. I am linking the Google Colab [notebook](https://colab.research.google.com/drive/1PDa6o4TZC0s_N8zaVytyCZhL1xaDE-dz) here, please help me out on this issue.

Just to clarify, the images are scaled to (32, ) maintaining aspect ratio. The images are batched and padded since all images have different widths. The image batch is passed through the Convolution Extraction layer which outputs feature in shape \[batch\_size, channel, seq\_len\]. The encoder is fed with above output with sequence expanding across seq\_len. The text are also batched and padded with token with and token at start and end. While training, the decoder initially is fed with token and further the decoder outputs are fed back into decoder as decoder\_inputs. I hope this somehow clarifies what my approach is.

Let me know of any suggestions or improvements.  
Thank you",0,1,False,self,,,,,
242,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,19,ad4g4l,self.MachineLearning,[D] Stationary state distribution Policy Gradient,https://www.reddit.com/r/MachineLearning/comments/ad4g4l/d_stationary_state_distribution_policy_gradient/,WillingCucumber,1546769590,"I am new to RL and what have a doubt regarding policy gradient theorem.

&amp;#x200B;

Why does there exists a stationary state distribution in policy gradient theorem ? i.e 

[why this turns to be a constant.\(refer 13.2 in the below link\)](https://i.redd.it/wbjn5q6c2s821.png)

I know it's the existence of the stationary state distribution that we do not take the derivative of the state distribution, and are able to take the derivative of the RL objective only using the derivation of the policy being learned.

&amp;#x200B;

To be more clear I am referring to the Policy Gradient theorem 13.2 in Sutton's latest version.([http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf))

&amp;#x200B;

&amp;#x200B;",5,1,False,self,,,,,
243,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,20,ad4trr,self.MachineLearning,[P] Planning generation GAN,https://www.reddit.com/r/MachineLearning/comments/ad4trr/p_planning_generation_gan/,Kwantoem,1546773706,"Hey,

I am working on a project that tries to use Deep Q-learning to ""park"" trains when they are not in use. This has been going steadily and I am learning a lot. 

To generate the input for that problem, I use an instance generator, which basically generates possible planning instances and then eliminates those which arre not possible/do not adhere to certain distributions. Lately, I have been getting into GANs, and since these planning instances are basically samples from a certain distribution, I thought I could maybe implement a GAN to generate these planning instances for me. 

The problem now is that I do not exactly know whether that's possible at all. I was hoping anyone would point me in the right direction. I have been looking at RGAN as well as SeqGAN but I have no idea how to utilize these for my specific problem

The planning instances have the following shape:

&amp;#x200B;

|time\_step|event\_type|length|
|:-|:-|:-|
|The time step on which this event occurs. Integers, starting from 0|arriving/departing/service|The length of the train. An integer between 4 and 10|

My problem is that these instances are basically two variables that need to be generated; two integers in a certain range. The problem comes with the fact that the overall instance needs to make sense (no trains can depart that have never arrived, trains need to arrive before getting service).

I have a set of valid instances to train on.

&amp;#x200B;

My question now, would a GAN be suitable for this problem?

And if so, can you point me to some papers that might help me / give me some pointers to how I could approach this?",2,1,False,self,,,,,
244,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,20,ad4wrf,self.MachineLearning,Looking for implementation of abstractive summary generator,https://www.reddit.com/r/MachineLearning/comments/ad4wrf/looking_for_implementation_of_abstractive_summary/,rishabh_m,1546774594,"Hello,can anyone please guide me. I am trying to implement a summary generator from scratch. Anyone could refer me any research paper or implementation of the model, it would be really helpful.",0,1,False,self,,,,,
245,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,20,ad50qe,youtu.be,[P] Introduction to the Hypervolume Indicator,https://www.reddit.com/r/MachineLearning/comments/ad50qe/p_introduction_to_the_hypervolume_indicator/,shahinrostami,1546775834,,1,1,False,default,,,,,
246,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,21,ad54dq,self.MachineLearning,"Forecasting atmospheric properties, any idea to improve my NN performance?",https://www.reddit.com/r/MachineLearning/comments/ad54dq/forecasting_atmospheric_properties_any_idea_to/,mra2019,1546776886,[removed],0,1,False,self,,,,,
247,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,21,ad562t,self.MachineLearning,Similarity Measures (in CBR),https://www.reddit.com/r/MachineLearning/comments/ad562t/similarity_measures_in_cbr/,TheCabbageBaggage,1546777394,[removed],0,1,False,self,,,,,
248,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,22,ad5i6m,self.MachineLearning,[Research] Similarity Measures (in CBR),https://www.reddit.com/r/MachineLearning/comments/ad5i6m/research_similarity_measures_in_cbr/,TheCabbageBaggage,1546780708,Does someone of you know good resources with a solid overview of similarity (and distance) measures? Preferable in a Case-based Reasoning context. Thanks,2,1,False,self,,,,,
249,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,22,ad5nza,vicap-cyber.com,VICAP: is this legit?,https://www.reddit.com/r/MachineLearning/comments/ad5nza/vicap_is_this_legit/,behrangsa,1546782154,,0,1,False,default,,,,,
250,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,22,ad5opw,self.MachineLearning,Articles on R/place,https://www.reddit.com/r/MachineLearning/comments/ad5opw/articles_on_rplace/,alextesy,1546782360,"Hi, I'm currently working on a paper where we try to analyse online communities and as a test case we use Reddit communities and specifically conduct a classification model on the r/place experiment data. While surveying previous works on the topic I found few interesting scientific articles :

 Latent Structure in Collaboration: the Case of Reddit r/place \-  [https://arxiv.org/abs/1804.05962](https://arxiv.org/abs/1804.05962)

Coordination in a Peer Production Platform: A study of Reddits /r/Place experiment - [https://uwspace.uwaterloo.ca/handle/10012/14060](https://uwspace.uwaterloo.ca/handle/10012/14060)

Compression in cultural evolution: Homogeneity and structure in the emergence and evolution of a large-scale online collaborative art project - [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202019](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202019)

Thought that might be nice to have a thread on data driven research around r/place or Reddit as whole.",0,1,False,self,,,,,
251,MachineLearning,t5_2r3gv,2019-1-6,2019,1,6,22,ad5ra6,self.MachineLearning,[D] Scientific articles on r/place,https://www.reddit.com/r/MachineLearning/comments/ad5ra6/d_scientific_articles_on_rplace/,alextesy,1546782985,"Hi, I'm currently working on a paper where we try to analyse online communities and as a test case we use Reddit communities and specifically conduct a classification model on the r/place experiment data. While surveying previous works on the topic I found few interesting scientific articles :

Latent Structure in Collaboration: the Case of Reddit r/place \-  [https://arxiv.org/abs/1804.05962](https://arxiv.org/abs/1804.05962)

Coordination in a Peer Production Platform: A study of Reddits /r/Place experiment - [https://uwspace.uwaterloo.ca/handle/10012/14060](https://uwspace.uwaterloo.ca/handle/10012/14060)

Compression in cultural evolution: Homogeneity and structure in the emergence and evolution of a large-scale online collaborative art project - [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202019](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202019)

Thought that might be nice to have a thread on data driven research around r/place or Reddit as whole.",6,1,False,self,,,,,
252,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,1,ad6yl3,self.MachineLearning,[D][Advice] What are Great Schools for a Masters Degree in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/ad6yl3/dadvice_what_are_great_schools_for_a_masters/,Kyak787,1546791837,"I am focusing on Master's Degrees because they are often harder to pay for, while a worthwhile PhD should be associated with years of funding. It may be harder to get into a Masters Program vs a PhD at certain schools, yet pursuing a Masters Degree might provide one the opportunity to get into a great school that would not accept them for a PhD. Some people may want a terminal Masters.

I'm hoping to create a discussion for others to use as a reference. Many people in Machine Learning fields emphasize the importance of having a strong understanding of concepts and 'rigour' in their education in addition to self study.

In the United States:

What Master's Programs in the USA would you say are 'great' by these metrics?

1) The ability to get a TAship or RAship to waive tuition costs.

2) Have multiple courses providing a very strong education on the topics of Deep Learning, Reinforcement Learning, and Transfer Learning. Do they provide a good specialization in Data Science, Computer Vision, GANs, Natural Language Processing, etc?

3) Offer a strong suite of rigorous electives in the relevant Mathematical/Statistical/Optimization theory or provide electives about Machine Learning in Finance and Management.

4) Nearby employers will pay you to let you go to school there for grades of B or higher.

5) Assist in helping the student by leveraging connections to find a summer internship and_or by allowing the student to complete a comprehensive project for graduation / do research with a professor for two years?


Outside the US:

If you know a great school in Europe / Canada / etc. that affords these above benefits, please name it and advocate for it a little (a short paragraph or two minimum will suffice).



Lastly, I will research all the universities via the internet and phone / email contact that are mentioned with supporting details. I will update everything that I find either on here or on a new linked post over the next few months.

*Please remember that having relevant and recent job experience is still incredibly important, regardless of which school you went to for your Masters.*",5,1,False,self,,,,,
253,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,1,ad6yxp,self.MachineLearning,Combatting and detecting FGSM and PGD adversarial noise,https://www.reddit.com/r/MachineLearning/comments/ad6yxp/combatting_and_detecting_fgsm_and_pgd_adversarial/,jngannon,1546791906,"Hi,

&amp;#x200B;

I have been working on some blog posts about some independent research that I am doing improving performance of deep neural networks with regard to adversarial noise. The first 2 blogs deal with noise that I have generated for experimenting with. In the newest blog I have tested a couple of new methods on noise generated by the \[Robust ML\]([https://robust-ml.org](https://robust-ml.org)) API. They are a group that catalogue adversarial attacks and defences.

&amp;#x200B;

\[Here\]([https://jngannon.github.io/FGSM\_Article](https://jngannon.github.io/FGSM_Article)) Is the latest blog.

&amp;#x200B;

\[Here\]([https://jngannon.github.io/home.html](https://jngannon.github.io/home.html)) is the homepage with 2 more explaining the methods in more detail.

&amp;#x200B;

I would appreciate any feedback.",0,1,False,self,,,,,
254,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,1,ad72jy,self.MachineLearning,Combatting and detecting FGSM and PGD adversarial noise [P][R],https://www.reddit.com/r/MachineLearning/comments/ad72jy/combatting_and_detecting_fgsm_and_pgd_adversarial/,jngannon,1546792538,"Hi,

I have been working on some blog posts about some independent research that I am doing improving performance of deep neural networks with regard to adversarial noise. The first 2 blogs deal with noise that I have generated for experimenting with. In the newest blog I have tested a couple of new methods on noise generated by the [Robust ML](https://robust-ml.org) API. They are a group that catalogue adversarial attacks and defences.

[Here](https://jngannon.github.io/FGSM_Article) Is the latest blog.

[Here](https://jngannon.github.io/home.html) is the homepage with 2 more explaining the methods in more detail.

I would appreciate any feedback.",0,1,False,self,,,,,
255,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,1,ad76iw,self.MachineLearning,How to take forward my Data Science - ML learning path? I,https://www.reddit.com/r/MachineLearning/comments/ad76iw/how_to_take_forward_my_data_science_ml_learning/,bidyutchanda108,1546793218,[removed],0,1,False,self,,,,,
256,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,2,ad7gij,youtu.be,Made this video for my summer research application. Is the editing too much?,https://www.reddit.com/r/MachineLearning/comments/ad7gij/made_this_video_for_my_summer_research/,NoteToBear,1546794925,,0,1,False,default,,,,,
257,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,2,ad7haj,self.MachineLearning,How to build a text detection and recognition system for extracting only specific information from texts?,https://www.reddit.com/r/MachineLearning/comments/ad7haj/how_to_build_a_text_detection_and_recognition/,DGs29,1546795055,[removed],0,1,False,self,,,,,
258,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,4,ad8j07,self.MachineLearning,"""[P]"" Help me in building a chatbot [project]",https://www.reddit.com/r/MachineLearning/comments/ad8j07/p_help_me_in_building_a_chatbot_project/,nargund_07,1546801231,"Hey guys;

I just completed my Machine Learning course by Andrew N G. for our graduation project we are planning to build a good chatbot (I browsed thought the sub I was unable find an answer).

&amp;#x200B;

Guys please help me out on where to start and how to go through the entire process (I have read about lsmt and seq2seq models but dont know enough) 

Thanks in advance (pl note: my first is not English)",2,1,False,self,,,,,
259,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,4,ad8yit,self.MachineLearning,Document similarity - ElasticSearch (TDIDF) or deep learning models?,https://www.reddit.com/r/MachineLearning/comments/ad8yit/document_similarity_elasticsearch_tdidf_or_deep/,ranihorev,1546803566,[removed],0,1,False,self,,,,,
260,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,4,ad8ywd,self.MachineLearning,[P] QR Code Reader from a field of vision using CNN,https://www.reddit.com/r/MachineLearning/comments/ad8ywd/p_qr_code_reader_from_a_field_of_vision_using_cnn/,LazyLoo,1546803628,"One of the current projects I'm working on involves designing a QR code reader. The issue is unlike a smartphone, for example, where there is a pre-existing bounding box and you only have to insert the QR code into the box, I have to be able to detect QR codes from a field of vision (e.g. a video feed from a moving webcam) and then read them. Hence I was thinking of using a CNN but I am unsure where to start with this. If anybody has any better ideas or can provide any resources/advice it would be greatly appreciated. Thanks!",4,1,False,self,,,,,
261,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,4,ad9457,self.MachineLearning,[D] Document similarity - ElasticSearch (TDIDF) or deep learning models?,https://www.reddit.com/r/MachineLearning/comments/ad9457/d_document_similarity_elasticsearch_tdidf_or_deep/,ranihorev,1546804417,"Given a research paper (or any other text), I want to find similar papers. In the past I used the ""More like this"" feature of ElasticSearch which is both simple and fast (but TFIDF is limited).

What is the best solution for this task today?

Idea: Use doc2vec or Elmo to generate a vector for each document and find the closest vectors for a given document? Not sure it's a better solution and it definitely requires more work.

Thanks!",40,1,False,self,,,,,
262,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,5,ad9mgf,self.MachineLearning,Your Favorite Videos Showing Self-play Learning?,https://www.reddit.com/r/MachineLearning/comments/ad9mgf/your_favorite_videos_showing_selfplay_learning/,Uncle-Duke,1546807378,[removed],0,1,False,self,,,,,
263,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,6,ad9ssz,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 54,https://www.reddit.com/r/MachineLearning/comments/ad9ssz/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1546808404,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/sritee: https://arxiv.org/abs/1812.06298),

/u/ceceshao1: [Faster Neural Networks Straight from JPEG](https://papers.nips.cc/paper/7649-faster-neural-networks-straight-from-jpeg)

/u/Zistance: [Ablation of a Robots Brain:
Neural Networks Under a Knife](http://arxiv.org/pdf/1812.05687)

Besides that, there are no rules, have fun.",13,1,False,self,,,,,
264,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,6,ad9z96,self.MachineLearning,[Q] cDGAN question with model implementation about terrible generated images,https://www.reddit.com/r/MachineLearning/comments/ad9z96/q_cdgan_question_with_model_implementation_about/,gabegabe6,1546809452,[removed],1,1,False,self,,,,,
265,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,6,ada0m3,blog.ml.cmu.edu,New CMU Blog,https://www.reddit.com/r/MachineLearning/comments/ada0m3/new_cmu_blog/,riddelln,1546809677,,1,1,False,default,,,,,
266,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,7,adaxwe,self.MachineLearning,Machine learning book recommendation for a beginner,https://www.reddit.com/r/MachineLearning/comments/adaxwe/machine_learning_book_recommendation_for_a/,v_ds,1546815120,[removed],0,1,False,self,,,,,
267,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,8,adb62m,self.MachineLearning,Using a Multiclass classifier to solve a binary problem?,https://www.reddit.com/r/MachineLearning/comments/adb62m/using_a_multiclass_classifier_to_solve_a_binary/,cybertheory,1546816456,[removed],0,1,False,self,,,,,
268,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,8,adbaul,youtu.be,Has anyone experimented with pattern extraction via media juxtaposition?,https://www.reddit.com/r/MachineLearning/comments/adbaul/has_anyone_experimented_with_pattern_extraction/,jrGRAVITY,1546817294,,0,1,False,default,,,,,
269,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,12,addken,youtube.com,COOKIES PACKING MACHINE AUTOMATIC WITH GLUE DEVICE,https://www.reddit.com/r/MachineLearning/comments/addken/cookies_packing_machine_automatic_with_glue_device/,RongYumachine,1546831965,,0,1,False,default,,,,,
270,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,12,addnq2,self.MachineLearning,Keras.js + react,https://www.reddit.com/r/MachineLearning/comments/addnq2/kerasjs_react/,hmhwe,1546832625,"I am new to keras.js. I want to run a POS tagging keras model on browser using keras.js , but keras.js is unable to read the file and prints ""Model configuration does not contain layers"".  

Here is the js code :

`handlePredict()`

`{`

`const model = new KerasJS.Model({`

`filepath:'example.biln',`

`filesystem: true,`

`gpu: true`

`})`

`alert('Input');`

&amp;#x200B;

`model.ready().then(() =&gt;model.predict({`

`input: new Float32Array(samples),`

`}))`

`.then(output =&gt;`

`{`

`// outputData is an object keyed by names of the output layers`

`// or \`output\` for Sequential models`

`// e.g.,`

`alert('Predicted value : ' + output[1]);`

`})`

`.catch(err =&gt;`

`{`

`alert(err);`

`})`

&amp;#x200B;

`}`

&amp;#x200B;

have tried with lot of keras versions. What could be the problem? Any help would be appreciated.",0,1,False,self,,,,,
271,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,13,addvoh,self.MachineLearning,Reproducibility challenge,https://www.reddit.com/r/MachineLearning/comments/addvoh/reproducibility_challenge/,ashunigion,1546834218,[removed],0,1,False,self,,,,,
272,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,13,ade4z1,self.MachineLearning,Has anyone been to AI Nextcon?,https://www.reddit.com/r/MachineLearning/comments/ade4z1/has_anyone_been_to_ai_nextcon/,tilttovictory,1546836184,[removed],0,1,False,self,,,,,
273,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,14,adeg6x,self.MachineLearning,what is the current state of the art approach for NER with word (or similar) embeddings?,https://www.reddit.com/r/MachineLearning/comments/adeg6x/what_is_the_current_state_of_the_art_approach_for/,ReaganRewop,1546838517,[removed],0,1,False,self,,,,,
274,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,15,ader3x,self.MachineLearning,Improve handwritten to text neural net,https://www.reddit.com/r/MachineLearning/comments/ader3x/improve_handwritten_to_text_neural_net/,korokage,1546840913,My handwritten to text net gets confused between certain letters like 'v' and 'y'. How can a train the model to distinguish these letters better?,0,1,False,self,,,,,
275,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,15,adf1l3,self.MachineLearning,Has anyone been to AI Nextcon? [D],https://www.reddit.com/r/MachineLearning/comments/adf1l3/has_anyone_been_to_ai_nextcon_d/,tilttovictory,1546843314,"I was curious about heading up to Seattle for AI Nextcon at the end of the month.

Has anyone been to this conference/workshops before? The price seems rather steep.

http://aisea19.xnextcon.com/",0,1,False,self,,,,,
276,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,16,adfes7,envisioninteligence.com,"Machine Learning as a Service (Mlaas) Market  Size, Outlook, Trends and Forecasts (2018  2024)",https://www.reddit.com/r/MachineLearning/comments/adfes7/machine_learning_as_a_service_mlaas_market_size/,srikanthenvision,1546846578,,0,1,False,default,,,,,
277,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,17,adfloc,self.MachineLearning,Forcing Neural Network to learn certain types of features?,https://www.reddit.com/r/MachineLearning/comments/adfloc/forcing_neural_network_to_learn_certain_types_of/,aashwin93,1546848405,"How would one go about forcing a neural network to learn one type of feature over another (if possible)? I am trying to make my network learn textures of different clothes but the shape features seem to dominate any other forms of pattern recognition.

For instance, if I want the NN to differentiate between a red shirt and a black shirt, its unable to make that distinction. But it can distinguish between a red skirt and a black shirt pretty well. ",0,1,False,self,,,,,
278,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,18,adfyz6,endtoend.ai,"[N] RL Weekly 3: Learning to Drive through Dense Traffic, Learning to Walk, and Summarizing Progress in Sim-to-Real",https://www.reddit.com/r/MachineLearning/comments/adfyz6/n_rl_weekly_3_learning_to_drive_through_dense/,seungjaeryanlee,1546852151,,0,1,False,https://b.thumbs.redditmedia.com/9QVdLSJzUY4Fjl2i7MTMFsIz66O5TUZtiXT629IaT_g.jpg,,,,,
279,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,18,adg6j0,self.MachineLearning,"Time Series Predictions with LSTM (Input shape, data)",https://www.reddit.com/r/MachineLearning/comments/adg6j0/time_series_predictions_with_lstm_input_shape_data/,shaggoththebig4,1546854125,[removed],0,1,False,self,,,,,
280,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,19,adgila,knowball.co,The machine learning resources search engine,https://www.reddit.com/r/MachineLearning/comments/adgila/the_machine_learning_resources_search_engine/,thesalsguy,1546857451,,0,1,False,default,,,,,
281,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,19,adgj9u,self.MachineLearning,[R] Good approach for unsupervised outlier detection with mainly categorical data?,https://www.reddit.com/r/MachineLearning/comments/adgj9u/r_good_approach_for_unsupervised_outlier/,ricklen,1546857636,"Hello everyone,

My previous post was about machine learning literature in accounting, specifically about anomaly / fraud detection during the audit process. I've found some interesting papers about unsupervised anomaly detection which I probably will be applying in an accounting firm where I'm doing my master thesis. The focus has been on unsupervised anomaly detection techniques because the (auditing) data that will be available is not classified and there are also no samples of fraudulent / anomalous transactions. 

Most popular techniques I came across are: clustering / density based (K-NN, LOF), One-class SVM and also autoencoders. I also came across some very specific or very new techniques which I found very hard to understand and I don't think I will be able to use them for my master thesis. 

For now I'm pretty confused with what I should be focusing on specifically because papers dealing with anomaly detection only use numerical data. In my case I have a single feature which is truly numerical and about 19 features which are categorical or represent a date / time (based on anonymous data sample from the accounting firm).

Clustering techniques for categorical data I came across are K-Modes and K-Prototypes (combination of categorical and numerical). I haven't found any other popular techniques for unsupervised anomaly detection in case of categorical data, this seems like a very specific field to me but then again I'm not sure. Do anyone know more popular or good techniques in this field?

Also I've seen one-hot encoding and DictVectorizers in order to make the data numerical and be able to use techniques like K-NN, OC-SVM and autoencoders. But then again people warn about the 'curse of dimensionality'. This ([https://arxiv.org/abs/1709.05254](https://arxiv.org/abs/1709.05254)) paper presents pretty much the same thing I want to do by using one-hot encoding resulting into a maximum of 567 dimensions, so it should possible.

I'm doing research for an accounting firm that hasn't done anything with machine learning before and wants to start implementing machine learning in their auditing process, during my master thesis. What advice can you guys give me? Right now I'm thinking about trying to do two things, by proposing the implementation of K-Modes and also try to implement K-NN with encoded data. Then I'm able to compare both techniques with respect to anomalies in the data. And if there is still time left I can focus on autoencoders which is a technique I would like to investigate because of personal interest (my supervisor advised me to at least use a technique I'm most interested in).

&amp;#x200B;

Thank you guys a lot.

&amp;#x200B;",10,1,False,self,,,,,
282,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,19,adgk0h,happysysadm.com,Artificial Neural Networks in PowerShell - part 1 (x-post from /r/PowerShell),https://www.reddit.com/r/MachineLearning/comments/adgk0h/artificial_neural_networks_in_powershell_part_1/,happysysadm,1546857840,,0,1,False,default,,,,,
283,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,19,adgk8v,self.MachineLearning,Final Year Project Idea,https://www.reddit.com/r/MachineLearning/comments/adgk8v/final_year_project_idea/,RuffTheObserver,1546857905,[removed],0,1,False,self,,,,,
284,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,20,adgr7m,self.MachineLearning,[R] [ICLR 2019] Quaternion Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/adgr7m/r_iclr_2019_quaternion_recurrent_neural_networks/,moktarletoutgentil,1546859783,"Open review: [https://openreview.net/forum?id=ByMHvs0cFQ](https://openreview.net/forum?id=ByMHvs0cFQ)

Research Gate: [https://www.researchgate.net/publication/325712559\_Quaternion\_Recurrent\_Neural\_Networks](https://www.researchgate.net/publication/325712559_Quaternion_Recurrent_Neural_Networks)

**Abstract:** Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence. Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. We propose a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these internal structural dependencies with the quaternion algebra. Similarly to capsules, quaternions allow the QRNN to code internal dependencies by composing and processing multidimensional features as single entities, while the recurrent operation reveals correlations between the elements composing the sequence. We show that both QRNN and QLSTM achieve better performances than RNN and LSTM in a realistic application of automatic speech recognition. Finally, we show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of free parameters needed, compared to real-valued RNNs and LSTMs to reach better results, leading to a more compact representation of the relevant information.",28,1,False,self,,,,,
285,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,20,adgx27,self.MachineLearning,[Discussion] State of Probabilistic Programming - survey,https://www.reddit.com/r/MachineLearning/comments/adgx27/discussion_state_of_probabilistic_programming/,springcoil,1546861356,"Hi all, 

I'd love some help - I'm putting together a report on the state of probabilistic programming in industry. 

&amp;#x200B;

[https://www.surveymonkey.com/r/GJ5593H](https://www.surveymonkey.com/r/GJ5593H) is my survey - it's only 2 minutes and your views will inform a good article on the state of probabilistic programming in industry",0,1,False,self,,,,,
286,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,21,adh1ju,medium.com,Human-in-the-loop for object detection with Supervisely and YOLO v3,https://www.reddit.com/r/MachineLearning/comments/adh1ju/humanintheloop_for_object_detection_with/,tdionis,1546862530,,0,1,False,https://b.thumbs.redditmedia.com/LyBSETas3naBbpEh7Y1gCHIm6ABF4DwhP6FweRuW9Qc.jpg,,,,,
287,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,21,adh6bs,youtube.com,"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educative",https://www.reddit.com/r/MachineLearning/comments/adh6bs/this_video_goes_over_a_model_that_predicts_the/,antaloaalonso,1546863715,,0,1,False,default,,,,,
288,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,21,adhe8e,medium.com,[P] Human-in-the-loop for object detection with Supervisely and YOLO v3,https://www.reddit.com/r/MachineLearning/comments/adhe8e/p_humanintheloop_for_object_detection_with/,tdionis,1546865595,,0,1,False,https://b.thumbs.redditmedia.com/LyBSETas3naBbpEh7Y1gCHIm6ABF4DwhP6FweRuW9Qc.jpg,,,,,
289,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,22,adhi70,kanoki.org,How to use AI powered Query in Google Spreadsheet,https://www.reddit.com/r/MachineLearning/comments/adhi70/how_to_use_ai_powered_query_in_google_spreadsheet/,min2bro,1546866467,,0,1,False,default,,,,,
290,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,22,adhkkv,goo.gl,"Hello, as part of my PhD thesis in artificial intelligence and more specifically in eXplainable AI, I need you to evaluate explanations generated by an intelligent system to detect the type of a pasta. Can you help me?",https://www.reddit.com/r/MachineLearning/comments/adhkkv/hello_as_part_of_my_phd_thesis_in_artificial/,blackgreenolive,1546866986,,0,1,False,default,,,,,
291,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,22,adhlh6,self.MachineLearning,"What is the difference between ""global optimum"" and ""local optimum""?",https://www.reddit.com/r/MachineLearning/comments/adhlh6/what_is_the_difference_between_global_optimum_and/,alxbnnk,1546867204,[removed],0,1,False,self,,,,,
292,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,22,adhnc9,goo.gl,"[R] As part of my PhD thesis in artificial intelligence and more specifically in eXplainable AI, I need you to evaluate explanations generated by an intelligent system which detect the type of a pasta. Can you help me?",https://www.reddit.com/r/MachineLearning/comments/adhnc9/r_as_part_of_my_phd_thesis_in_artificial/,blackgreenolive,1546867607,,0,1,False,default,,,,,
293,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,22,adhsp3,self.MachineLearning,Hyperparameter tuning in neural networks: best practice?,https://www.reddit.com/r/MachineLearning/comments/adhsp3/hyperparameter_tuning_in_neural_networks_best/,MithrandirGr,1546868717,[removed],0,1,False,self,,,,,
294,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,23,adi02c,youtube.com,Using AI in Africa to make online platforms more trustworthy,https://www.reddit.com/r/MachineLearning/comments/adi02c/using_ai_in_africa_to_make_online_platforms_more/,DerkGrijp,1546870198,,0,1,False,https://b.thumbs.redditmedia.com/zNFR1SGPsPhnempcbHRaSSZ9Cn6aj19nNUOhPVJtzZM.jpg,,,,,
295,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,23,adi0y9,gwern.net,"[P] ""Danbooru2018: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset"" (now 2.5tb/3.33m images/92.7m tags; compared to Danbooru2017, +0.4TB/392k images/15.2m tags)",https://www.reddit.com/r/MachineLearning/comments/adi0y9/p_danbooru2018_a_largescale_crowdsourced_and/,gwern,1546870355,,0,1,False,https://a.thumbs.redditmedia.com/dxANLHPFMXGIUx3kCYYp04gTUc9LlawHv64pIDAF544.jpg,,,,,
296,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,23,adi1vp,blog.linknovate.com,[N]A ranking of the most active organizations innovating in Differential Privacy,https://www.reddit.com/r/MachineLearning/comments/adi1vp/na_ranking_of_the_most_active_organizations/,sabucks_,1546870535,,0,1,False,https://b.thumbs.redditmedia.com/qDS-AiESPeHrlGNO9_R_desgM2PP-FsgBNTkCN_uVKI.jpg,,,,,
297,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,23,adi64d,cs.princeton.edu,A Critique of Standard ML (1992) [pdf] - Princeton,https://www.reddit.com/r/MachineLearning/comments/adi64d/a_critique_of_standard_ml_1992_pdf_princeton/,j_orshman,1546871385,,7,1,False,default,,,,,
298,MachineLearning,t5_2r3gv,2019-1-7,2019,1,7,23,adifos,self.MachineLearning,Cool app for learning ML concepts,https://www.reddit.com/r/MachineLearning/comments/adifos/cool_app_for_learning_ml_concepts/,ScienTecht,1546873188,[removed],0,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,0,adig8f,twitter.com,Python machine learning by example and other relevant titles on humble bundle,https://www.reddit.com/r/MachineLearning/comments/adig8f/python_machine_learning_by_example_and_other/,Trickmanpat,1546873284,,0,1,False,default,,,,,
300,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,0,adikaw,self.MachineLearning,[N] Python machine learning by example and other relevant titles on humble bundle,https://www.reddit.com/r/MachineLearning/comments/adikaw/n_python_machine_learning_by_example_and_other/,Trickmanpat,1546873980,I got [this bundle](https://twitter.com/suehvunrehg/status/1080156931048853506) second tier and thought some would also appreciate it.,1,1,False,self,,,,,
301,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,0,adill0,self.MachineLearning,[D] Statistical Regularization in Deep Convolutional Neural Nets,https://www.reddit.com/r/MachineLearning/comments/adill0/d_statistical_regularization_in_deep/,idg101,1546874206,"Generally CNNs are regularized by L1 and L2 norms on the weights.  I am wondering what work (if any) has been done on using statistical functions as regularizers on weights or activites.  Upon a search of google scholar and IEEE, I have been unable to turn up anything useful.",10,1,False,self,,,,,
302,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,0,adirr1,self.MachineLearning,"[P] A video describing a method for high resolution image generator, without GAN",https://www.reddit.com/r/MachineLearning/comments/adirr1/p_a_video_describing_a_method_for_high_resolution/,neuralPr0cess0r,1546875300,"[https://www.youtube.com/watch?v=IdgEBYd5FfU&amp;feature=youtu.be](https://www.youtube.com/watch?v=IdgEBYd5FfU&amp;feature=youtu.be)

&amp;#x200B;

This video describes a method to train a neural network to synthesize plausable, high resolution imagary without a GAN.  This work is a novel combination of various existing techniques:  

Inspiration: Mean Teacher 

Perceptual Losses for Real-Time Style Transfer Lines to image ... etc  

&amp;#x200B;

Please forgive my poor oration! This is for fun. I am not an academic. :)  

&amp;#x200B;

The basic idea is to utilize a U-Net, modified perceptual loss (pearson instead of MAE), learned basis functions, and  ""mean teacher"" training in order to synthesize images of high quality without the normal troubles of training a GAN.",13,1,False,self,,,,,
303,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,1,adj86y,neuralgen.io,Call for papers - NeuralGen 2019 - Methods for Optimizing and Evaluating Neural Language Generation,https://www.reddit.com/r/MachineLearning/comments/adj86y/call_for_papers_neuralgen_2019_methods_for/,warspot,1546878039,,0,1,False,default,,,,,
304,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,1,adjer7,self.MachineLearning,Is Caffe still in use?,https://www.reddit.com/r/MachineLearning/comments/adjer7/is_caffe_still_in_use/,sagnick9,1546879123,[removed],0,1,False,self,,,,,
305,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,1,adjiry,nature.com,Unprovability comes to machine learning,https://www.reddit.com/r/MachineLearning/comments/adjiry/unprovability_comes_to_machine_learning/,myinnerbanjo,1546879781,,0,1,False,https://b.thumbs.redditmedia.com/RLNHig9gimjkuA9AG_NuznmEPWVrecUKiAeeYmJDxYw.jpg,,,,,
306,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,2,adjrz3,self.MachineLearning,[D] Is there a specific term for when a goal vector that an output vector is checked against is simply the vector representation of the next sensory input?,https://www.reddit.com/r/MachineLearning/comments/adjrz3/d_is_there_a_specific_term_for_when_a_goal_vector/,ReasonablyBadass,1546881268,"So NN outputs vector x at timestep t which is used in the loss function with ""target"" vector y which is just the new input for the NN at timestep t+1

I'm sure there is a specific term for this, but googling it is a bit tricky. ",9,1,False,self,,,,,
307,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,2,adjssr,self.MachineLearning,[P] Reinforcement learning without gradients: evolving agents using Genetic Algorithms,https://www.reddit.com/r/MachineLearning/comments/adjssr/p_reinforcement_learning_without_gradients/,invertedpassion,1546881401,"I recently tried various RL algorithms: Q-learning, DQN and Policy Gradients. Then I discovered [a paper](https://arxiv.org/abs/1712.06567) by Uber where they suggest that simple, gradient-free algorithms such as Genetic Algorithms are as competitive at RL as gradient-based methods.

Curious, **I implemented Genetic Algorithm for evolving an agent for CartPole**. Here's the accompanying [tutorial + code in PyTorch](https://towardsdatascience.com/reinforcement-learning-without-gradients-evolving-agents-using-genetic-algorithms-8685817d84f). 

The code can accommodate any neural network model containing linear and convolutional layers. Feel free to fork it if you want to try evolving your own agents.

Hope you like it!",3,1,False,self,,,,,
308,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,2,adk17c,self.MachineLearning,Can we fool real time systems using adversarial examples?,https://www.reddit.com/r/MachineLearning/comments/adk17c/can_we_fool_real_time_systems_using_adversarial/,ashwath91,1546882777,[removed],0,1,False,self,,,,,
309,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,2,adk442,self.MachineLearning,[R] Explained: Multilingual Sentence Embeddings for Zero-Shot Transfer  Applying a Single Model on 93 Languages,https://www.reddit.com/r/MachineLearning/comments/adk442/r_explained_multilingual_sentence_embeddings_for/,ranihorev,1546883250,[removed],0,1,False,self,,,,,
310,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,3,adkij1,self.MachineLearning,MIT Deep Learning GitHub Repo,https://www.reddit.com/r/MachineLearning/comments/adkij1/mit_deep_learning_github_repo/,UltraMarathonMan,1546885486,[removed],0,1,False,https://b.thumbs.redditmedia.com/szJVFkkwKnqkydgNnO4aFgUTpE9O99YeF6WdpneVGHU.jpg,,,,,
311,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,3,adkjpo,self.MachineLearning,[D] MIT Deep Learning GitHub Repo,https://www.reddit.com/r/MachineLearning/comments/adkjpo/d_mit_deep_learning_github_repo/,UltraMarathonMan,1546885718,"A repository with a collection of tutorials for a number of deep learning courses at MIT. More tutorials added as courses progress.

GitHub: [https://github.com/lexfridman/mit-deep-learning](https://github.com/lexfridman/mit-deep-learning)

Website: [https://deeplearning.mit.edu/](https://deeplearning.mit.edu/)

Tutorial out today is on Driving Scene Segmentation with TensorFlow ([Jupyter Notebook](https://github.com/lexfridman/mit-deep-learning/blob/master/tutorial_driving_scene_segmentation/tutorial_driving_scene_segmentation.ipynb)):

https://reddit.com/link/adkjpo/video/t2rlvd7on1921/player",9,1,False,https://b.thumbs.redditmedia.com/3M5NO0BuzMoPY1azfzj23dXAYS--9OPdXKf05WYwgJY.jpg,,,,,
312,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,3,adknmu,self.MachineLearning,What data lends itself to Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/adknmu/what_data_lends_itself_to_machine_learning/,mockrun,1546886289,[removed],0,1,False,self,,,,,
313,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,3,adkpsh,self.MachineLearning,[R] Explained: Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,https://www.reddit.com/r/MachineLearning/comments/adkpsh/r_explained_massively_multilingual_sentence/,ranihorev,1546886633,[removed],0,1,False,self,,,,,
314,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,4,adkz43,medium.com,AI in 2018 Review; What to Expect in 2019,https://www.reddit.com/r/MachineLearning/comments/adkz43/ai_in_2018_review_what_to_expect_in_2019/,gwen0927,1546888072,,0,1,False,https://b.thumbs.redditmedia.com/TSVL6kVdlahSzEUR-sDZWhbIYyAEtSiUWUIXOu9nzcA.jpg,,,,,
315,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,4,adl2iq,self.MachineLearning,[D] Community ML Paper Reproductions,https://www.reddit.com/r/MachineLearning/comments/adl2iq/d_community_ml_paper_reproductions/,Kyle_Alekzandr,1546888610,"We've all seen the multitude of new techniques and papers being published recently. Unfortunately, many of these papers leave us without the appropriate code implementations. For that reason, I'm starting a collaborative initiative of attempting to reproduce research papers, validating or invalidating the author's findings, and providing the community with useable code for future use. Feel free to give any feedback that can help this grow.  


I've built a trello board for suggesting papers for reproduction and community voting. Everyone is welcome to contribute!

[https://trello.com/b/F9qkSqgG/research-reproduction](https://trello.com/b/F9qkSqgG/research-reproduction)

&amp;#x200B;

Additionally,  Zaur Fataliyev has a solid github repo aptly named Papers With Code (PWC). Go ahead and check it out.

[https://github.com/zziz/pwc](https://github.com/zziz/pwc)

&amp;#x200B;

&amp;#x200B;",4,1,False,self,,,,,
316,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,4,adl2sc,blockdelta.io,Top Ten Technological Disruptions of 2018 Contd - Part II,https://www.reddit.com/r/MachineLearning/comments/adl2sc/top_ten_technological_disruptions_of_2018_contd/,BlockDelta,1546888652,,0,1,False,https://b.thumbs.redditmedia.com/CWaw6DNnr_L6FtrjgnVBB5lzxQNSbEA1tXDd5sqHF6g.jpg,,,,,
317,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,4,adlbnh,arxiv.org,[R] A Comprehensive Survey on Graph Neural Networks,https://www.reddit.com/r/MachineLearning/comments/adlbnh/r_a_comprehensive_survey_on_graph_neural_networks/,ranihorev,1546890071,,3,1,False,default,,,,,
318,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,4,adlbop,self.MachineLearning,Google Cloud collaborating with Iguazio to enable real-time AI across the cloud and intelligent edge,https://www.reddit.com/r/MachineLearning/comments/adlbop/google_cloud_collaborating_with_iguazio_to_enable/,readstratusupdate,1546890075,[removed],1,1,False,self,,,,,
319,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,5,adlj31,nature.com,[P] Causal deconvolution by algorithmic generative models,https://www.reddit.com/r/MachineLearning/comments/adlj31/p_causal_deconvolution_by_algorithmic_generative/,AskMeIfImAReptiloid,1546891255,,1,1,False,https://b.thumbs.redditmedia.com/Ptd2JnLhEZJoskrpEtkxBbv-v1UFwb_J4425YbugPus.jpg,,,,,
320,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,5,adlu1s,self.MachineLearning,[D] Misunderstood ML/AI Concepts,https://www.reddit.com/r/MachineLearning/comments/adlu1s/d_misunderstood_mlai_concepts/,MuchArmadillo,1546892938,"**Questions / Topics for Discussion**
1. What are some concepts which many people who use Machine Learning / Artificial Intelligence get ""wrong"" or misunderstand?
2. What are some good articles (research and ""lay"") on the subject?
3. What might be some good or interesting ""polling"" questions to tease these sorts of things out?
Note: I would love and/or encourage anyone to take the result of this discussion and turn it into a peer-reviewed-caliber paper. I've been thinking about it, but realistically, I probably won't do it, so knock yourself out! P.S., would be happy to help.
**Background**
&amp;nbsp;
Last year I gave some talks on bias in data science at two reasonably well-attended conferences largely for data scientists who use ML/AI (one had ~500 attendees, the other over 1200). During my talks, I asked a few questions using ""polleverywhere,"" which falls short of scientifically rigorous, but is interesting nonetheless. An example of one of the questions that stood out for me follows.
&amp;nbsp;
**Question:**
&amp;nbsp;
When the output of the output node a typical, unidirectional, feed-forward deep-leaning image classifying neural network (the NN) is ""0.734 Horse"" after processing an image. In this case, the **""0.734 Horse""** means:
&amp;nbsp;
- A. There is **not enough information** to answer the question.
- B. It is the **confidence interval** of the NN that there is a horse in the image.
- C. It represents the NN's **confidence estimate** that there is a horse in the image.
- D. There is a **roughly 74% chance** that there is a horse in the image.
&amp;nbsp;
The results of the clicker poll at the talk directed toward practicing, publishing data scientists, out of 61 respondents:
- 1 (~1.5%) chose A
- 17 (~28%) chose B
- 13 (~21%) chose C
- 30 (~49%) chose D
&amp;nbsp;
At the second talk with a slightly broader audience (12 people chose ""E. I don't know"", which I exclude because there were some non-practitioners), of 97 respondents, I got:
- 0 (0%) chose A
- 21 (~22%) chose B
- 23 (~24%) chose C
- 53 (~55%) chose D
&amp;nbsp;
One attendee -- likely the only one who answered A -- persuaded me that A is reasonably acceptable for a number of reasons. I think it's a stretch, but I'd give credit on a quiz after such an animated defense. However, C, is the only true answer because it is a confidence estimate, no matter the state of the NN. Even an untrained NN will produce a random output which is a confidence estimate -- a very poor one, but one nonetheless. B is a common mistake that even I sometimes make -- I'll say confidence interval when I mean confidence estimate and vice-versa because my brain does that to me, so confounding the two did not surprise or alarm me. D is concerning, however.
&amp;nbsp;
**Note 1:** It was pointed out to me at the last talk that some on-line resources, including Wikipedia, call the output a probability; however, I asked the audience to send me the links but never received any such links.
&amp;nbsp;
**Note 2:** Understanding not-so-subtle differences such as those above is very important to understanding some of the concerning consequences to biased algorithms in ML and AI.
&amp;nbsp;
We know that the confidence estimate can be VERY different from the actual probabilities that classification is correct, and, indeed over the past several years we have had several papers underscoring this; however, the fact remains that many people using ML/AI misinterpret the results. Most alarming to me is the number of healthcare publications that are being churned out from top research institutions that make this mistake. If the research paper suggests that a system which classifies a scan as ""0.734 cancerous lesion"" means there is a 73.4% chance the lesion is cancerous, this can have dangerous implications. The same goes for things such as the scary-as-can-be COMPAS ("" Correctional Offender Management Profiling for Alternative Sanctions "") system.",30,1,False,self,,,,,
321,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,5,adluja,self.MachineLearning,[R] Causal deconvolution by algorithmic generative models,https://www.reddit.com/r/MachineLearning/comments/adluja/r_causal_deconvolution_by_algorithmic_generative/,[deleted],1546893009,[deleted],0,1,False,default,,,,,
322,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,5,adm44q,self.MachineLearning,[D] Determining computing power required when trying to reimplementing research papers,https://www.reddit.com/r/MachineLearning/comments/adm44q/d_determining_computing_power_required_when/,liondancer,1546894481,"I further Machine Learning and Reinforcement learning skills by reimplementing research papers but I feel that I may be blocked by the expensive computational resources and processing power that companies/universities have available compared to my MacBook Pro. I assume that I will not be able to see good results or know if my implementation is working due to my underpowered machine. 

Any tips to determine which research papers can be reproduced on my local machine? ",4,1,False,self,,,,,
323,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,6,admcc7,nvidia.com,NVIDIA announced RTX-2060. Any ideas if it will be useful for non-SOTA research?,https://www.reddit.com/r/MachineLearning/comments/admcc7/nvidia_announced_rtx2060_any_ideas_if_it_will_be/,simpleconjugate,1546895739,,0,1,False,https://b.thumbs.redditmedia.com/ucjMVf1pyR-S6Ukx0BHkFNwPRKbyYL1L2LuRtuDNr-E.jpg,,,,,
324,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,6,adms0i,directorsnotes.com,Premiere: Face Swap | A Deepfake Short by David Gidali &amp; Einat Tubi,https://www.reddit.com/r/MachineLearning/comments/adms0i/premiere_face_swap_a_deepfake_short_by_david/,fontkiller,1546898125,,0,1,False,https://a.thumbs.redditmedia.com/SfeUNoRFWNOHLAGMpc2KuueOx74gUE7Q_QFvpR4ZpO0.jpg,,,,,
325,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,7,admu0n,self.MachineLearning,[D] Which DRL/RL paper did you enjoy reading in 2018?,https://www.reddit.com/r/MachineLearning/comments/admu0n/d_which_drlrl_paper_did_you_enjoy_reading_in_2018/,dhruvramani,1546898448,Include papers which were published for 2019 conferences too!,4,1,False,self,,,,,
326,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,7,adnegp,self.MachineLearning,Does machine learning work with small datasets?,https://www.reddit.com/r/MachineLearning/comments/adnegp/does_machine_learning_work_with_small_datasets/,stfuweirdoo,1546901758,[removed],0,1,False,self,,,,,
327,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,9,adoipn,self.MachineLearning,[D] Is maximum likelihood estimation with generative ConvNet broken?,https://www.reddit.com/r/MachineLearning/comments/adoipn/d_is_maximum_likelihood_estimation_with/,tsauri,1546908535,"Since we can do inverse forward-pass on generative models such as Flow, RealNVP, Glow, I think the foundation of probability density estimation of generative images today is *broken*.  


I came across this paper ""Do Deep Generative Models Know What They Don't Know?""([https://arxiv.org/abs/1810.09136](https://arxiv.org/abs/1810.09136)) from Deepmind, stating that out-of-sample scores *higher* in likelihood, whereas logically *it should be smaller*. In their case, a Glow-based model trained on CIFAR scores SVHN images higher than CIFAR images it trained (i.e. model thinks SVHN is more likely comes from CIFAR train samples). The paper didn't nailed out the reason why, it is mostly experiments to verify this phenomenon.  


Then I remembered a paper from Bengio's lab, ""Measuring the tendency of CNNs to Learn Surface Statistical Regularities""  
[https://arxiv.org/abs/1711.11561](https://arxiv.org/abs/1711.11561), in their experiment, CIFAR image classifier break down in testing when images are preprocessed with Fourier filters (remove high frequencies, remove colors range, etc.) while maintaining same look to human eye, suggesting that ConvNets learnt to cheat by considering those statistical regularities unknown to human.  


Also there is work from ""Flow-GAN"" ([https://arxiv.org/abs/1705.08868](https://arxiv.org/abs/1705.08868)), where a Flow model can be trained either on MLE, or GAN, or both. They stated that Flow model trained with GAN setting generates prettier images on current GAN metrics, compared to normal MLE training like Flow did. However, likelihood scores from images generated by GAN training have 100x-1000x worse likelihood scores compared to images generated by MLE training.  


What do you think? Do you think ConvNet + backprop is the culprit? Or maybe Gaussian prior?   


&amp;#x200B;",8,1,False,self,,,,,
328,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,9,adoj69,self.MachineLearning,Where are all the AI based ad blockers?,https://www.reddit.com/r/MachineLearning/comments/adoj69/where_are_all_the_ai_based_ad_blockers/,alpacalaika,1546908604,[removed],0,1,False,self,,,,,
329,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,10,adooqi,i.redd.it,Projetos de ETE VI,https://www.reddit.com/r/MachineLearning/comments/adooqi/projetos_de_ete_vi/,JamurGerloff,1546909549,,0,1,False,https://b.thumbs.redditmedia.com/g0IhEZCkfjTZs6HfktdP_LbkAuywCpCUOO8Y6YFDkHw.jpg,,,,,
330,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,11,adpljd,self.MachineLearning,Undergrad taking grad level Machine Learning Course. Any Advice?,https://www.reddit.com/r/MachineLearning/comments/adpljd/undergrad_taking_grad_level_machine_learning/,wiegehtesdir,1546915280,"Hello all :) 

As the title says, Im an undergrad (Junior) that will be taking a graduate level machine learning course during the Spring 2019 semester. 

I have Linear algebra, discrete mathematics, Combinatorics, engineering statistics, etc. as I know they are important to AI/Machine Learning but I was really wondering if you all could recommend me any resources that might help me succeed in this course? I would really like to do good in this course because I would like to go to graduate school after completing my degree and I feel like this course could really boost my confidence. This would also be the first course that I would be taking that has anything to do with AI/Machine Learning. 

In case its important, I talked the professor who will be teaching this course and he said that this course does not require any prior AI/Machine Learning knowledge. Thanks for your guys help!",0,1,False,self,,,,,
331,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,11,adpolr,self.MachineLearning,Is anyone using Supervise.ly for production?,https://www.reddit.com/r/MachineLearning/comments/adpolr/is_anyone_using_supervisely_for_production/,OlorinDreams,1546915818,[removed],0,1,False,self,,,,,
332,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,12,adpx3s,self.MachineLearning,"[D] On Writing Quality Peer Reviews, with Noah A. Smith",https://www.reddit.com/r/MachineLearning/comments/adpx3s/d_on_writing_quality_peer_reviews_with_noah_a/,ofirpress,1546917405,"It's not uncommon for authors to be frustrated with the quality of peer reviews they receive in (NLP) conferences. In this episode, Noah A. Smith shares his advice on how to write good peer reviews. The structure Noah recommends for writing a peer review starts with a dispassionate summary of what a paper has to offer, followed by the strongest reasons the paper may be accepted, followed by the strongest reasons it may be rejected, and concludes with a list of minor, easy-to-fix problems (e.g., typos) which can be easily addressed in the camera ready. Noah stresses on the importance of thinking about how the reviews we write could demoralize (junior) researchers, and how to be precise and detailed when discussing the weaknesses of a paper to help the authors see the path forward. Other questions we discuss in this episode include: How to read a paper for reviewing purposes? How long it takes to review a paper and how many papers to review? What types of mistakes to be on the lookout for while reviewing? How to review pre-published work?

https://soundcloud.com/nlp-highlights/77-on-writing-quality-peer-reviews-with-noah-a-smith",0,1,False,self,,,,,
333,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,13,adqe6a,self.MachineLearning,Word Clustering Advice,https://www.reddit.com/r/MachineLearning/comments/adqe6a/word_clustering_advice/,Russia_Sushi,1546920611,[removed],0,1,False,self,,,,,
334,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,13,adqfk0,towardsdatascience.com,Image and Speech Recognition (Top picks from Towards Data Science),https://www.reddit.com/r/MachineLearning/comments/adqfk0/image_and_speech_recognition_top_picks_from/,dronecub,1546920884,,0,1,False,https://b.thumbs.redditmedia.com/rJC7fcF5wGq6hoIf36E1OcqmOG87LessnESrrznA9-Q.jpg,,,,,
335,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,14,adr2w2,self.MachineLearning,Docker for Data Science using R: Part 3,https://www.reddit.com/r/MachineLearning/comments/adr2w2/docker_for_data_science_using_r_part_3/,prato_s,1546925624,[removed],0,1,False,self,,,,,
336,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,15,adrg5e,github.com,Python-Awesome tutorial for Machine Learning as part of a Graduate Program in Machine Learning. Pull requests/changes/stars would be very much helpful.,https://www.reddit.com/r/MachineLearning/comments/adrg5e/pythonawesome_tutorial_for_machine_learning_as/,gautamrbharadwaj,1546928659,,0,1,False,https://b.thumbs.redditmedia.com/RWcMk9HsJ-vgZ4Y2kpF-wFtZZ3UKTOEWRSH2JZh78Og.jpg,,,,,
337,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,15,adrojf,self.MachineLearning,New to machine learning. What is the current state of the art of the technology and what are some good paths to go down development-wise?,https://www.reddit.com/r/MachineLearning/comments/adrojf/new_to_machine_learning_what_is_the_current_state/,jepsenstan,1546930662,"I'm really interested in deep learning and machine learning in particular, and I'd like to develop some software and hopefully change the world and make money in the process.

I've installed Python and GitBash onto my computer and I've been brushing up on multiple fields of study such as advanced math, psychology, and statistics.

What are some things you'd like to see developed with deep learning that few people are working on?",0,1,False,self,,,,,
338,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,16,adru4k,arxiv.org,[R][arXiv:1812.04949] Subjective Annotations for Vision-Based Attention Level Estimation,https://www.reddit.com/r/MachineLearning/comments/adru4k/rarxiv181204949_subjective_annotations_for/,rosivagyok,1546932020,,2,1,False,default,,,,,
339,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,16,adruwd,self.MachineLearning,Training CNNs on MNIST with high generalization error,https://www.reddit.com/r/MachineLearning/comments/adruwd/training_cnns_on_mnist_with_high_generalization/,wildtales,1546932210,"\[D\] Is it possible to train a CNN on standard MNIST that achieves very high train accuracy (say 100%) but very low test accuracy (\~ 10%). Note that I am **not** looking for modifications to MNIST that will achieve this result. Rather, I am looking for a training algorithm that will achieve the same.",0,1,False,self,,,,,
340,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,16,ads2mk,self.MachineLearning,NLP Assignment for 300 students on aws,https://www.reddit.com/r/MachineLearning/comments/ads2mk/nlp_assignment_for_300_students_on_aws/,NormalOne95,1546934206,[removed],0,1,False,self,,,,,
341,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,17,ads7hx,self.MachineLearning,[D] How does Mask R-CNN work?,https://www.reddit.com/r/MachineLearning/comments/ads7hx/d_how_does_mask_rcnn_work/,zenggyu,1546935502,"I have been trying to study the architecture of Mask R-CNN. I have quite a few confusions. Besides the original Mask R-CNN paper, can anyone point me to some intuitive materials (blog posts / stackoverflow posts, etc.) that explain Mask R-CNN in detail? Thanks!",0,1,False,self,,,,,
342,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,17,ads9ni,self.MachineLearning,[D] Learning materials for Mask R-CNN and Faster R-CNN,https://www.reddit.com/r/MachineLearning/comments/ads9ni/d_learning_materials_for_mask_rcnn_and_faster_rcnn/,zenggyu,1546936140,"I  have been trying to study the architecture of Mask R-CNN. I have quite a  few confusions. Besides the original Mask R-CNN paper, can anyone point  me to some intuitive and comprehensive materials (blog posts /  stackoverflow posts, etc.) that explain Mask R-CNN? I am particularly  confused about the mask branch and how the mask loss is calculated.

Materials regarding Faster R-CNN are also appreciated, since Mask R-CNN is built on top of it.

Thanks!",10,1,False,self,,,,,
343,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,17,adse0g,towardsdatascience.com,Generating Drake Rap Lyrics using Language Models and LSTMs,https://www.reddit.com/r/MachineLearning/comments/adse0g/generating_drake_rap_lyrics_using_language_models/,bil-sabab,1546937350,,0,1,False,https://b.thumbs.redditmedia.com/XSwlVwB9sP3flPUSuWKdT-vFF-LC-I9kvWxCgiosCeI.jpg,,,,,
344,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,18,adsiyn,self.MachineLearning,[D] Reproducible research with containers using Docker and Singularity.,https://www.reddit.com/r/MachineLearning/comments/adsiyn/d_reproducible_research_with_containers_using/,SuperShinyEyes,1546938734,"Hi,

Sharing code is becoming a common practice in research, but sharing environments not so common. Python package managers have increased environment mobility a lot but it doesn't always work. I wrote a blog post about containers which can solve most of the problems most of the time. Please enjoy and share how you solve this issue.  

http://typingducks.com/blog/reproducible_research_with_containers/

Br, 
Seyoung",20,1,False,self,,,,,
345,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,18,adsp5a,self.MachineLearning,google's teachable machine help saving The examples,https://www.reddit.com/r/MachineLearning/comments/adsp5a/googles_teachable_machine_help_saving_the_examples/,djmcbmax,1546940535,[removed],0,1,False,self,,,,,
346,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,18,adsq5q,happiestminds.com,Machine Learning Applications &amp; Algorithms- Happiest Minds,https://www.reddit.com/r/MachineLearning/comments/adsq5q/machine_learning_applications_algorithms_happiest/,aksri198,1546940823,,0,1,False,default,,,,,
347,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,18,adsrjn,self.MachineLearning,Implementing Binary XNOR Convolution Kernel for CPU - Tensorflow,https://www.reddit.com/r/MachineLearning/comments/adsrjn/implementing_binary_xnor_convolution_kernel_for/,anilsathyan7,1546941217,[removed],0,1,False,self,,,,,
348,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,18,adssqu,self.MachineLearning,[D] Need help finding related work for persistent change detection,https://www.reddit.com/r/MachineLearning/comments/adssqu/d_need_help_finding_related_work_for_persistent/,Mxbonn,1546941564,"I'm working on the topic of detecting persistent changes in the input data stream of a neural network, during a supervised learning task, so it can be detected that the assumptions made during training are no longer valid. Think about sudden large noise in the data, corruption of the input images due to a camera defect, etc.

&amp;#x200B;

In my research I'm calling these ""surprising events"", and I'm now doing research about related topics and I want to be sure that I cover as much as possible but I'm not sure how to search for related topics. I've looked into uncertainty and bayesian networks, which surely are related but not exactly as well as anomaly and novelty detection. Are there any other keyword, topics, papers that any of you can point me to that are useful for my research?",1,1,False,self,,,,,
349,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,19,adt0re,self.MachineLearning,Rating based recommendation system for android,https://www.reddit.com/r/MachineLearning/comments/adt0re/rating_based_recommendation_system_for_android/,arjybarji,1546943795,[removed],0,1,False,self,,,,,
350,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,21,adtkhc,self.MachineLearning,Securing Customers Identity with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/adtkhc/securing_customers_identity_with_machine_learning/,ciolook,1546949030,[removed],0,1,False,https://a.thumbs.redditmedia.com/j5FHActmGYUdhVlw3tJx_RNPTgNH3N-4F5WSzJ5Aun4.jpg,,,,,
351,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,21,adtpvw,arxiv.org,[1901.01341] Sheaves: A Topological Approach to Big Data,https://www.reddit.com/r/MachineLearning/comments/adtpvw/190101341_sheaves_a_topological_approach_to_big/,ihaphleas,1546950362,,27,1,False,default,,,,,
352,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,21,adtvrs,towardsdatascience.com,Understanding Generative Adversarial Networks (GANs),https://www.reddit.com/r/MachineLearning/comments/adtvrs/understanding_generative_adversarial_networks_gans/,logan-diamond,1546951771,,0,1,False,default,,,,,
353,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,21,adtwgs,simpliv.com,Online Machine Learning with Python Course | Python Tutorial | Simpliv,https://www.reddit.com/r/MachineLearning/comments/adtwgs/online_machine_learning_with_python_course_python/,simplivllc,1546951935,,0,1,False,default,,,,,
354,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,22,adu21f,self.MachineLearning,[D] What is an up-to-date equivalent of https://arxiv.org/pdf/1206.5533v2.pdf?,https://www.reddit.com/r/MachineLearning/comments/adu21f/d_what_is_an_uptodate_equivalent_of/,IborkedyourGPU,1546953145,"I speak for myself here, but given the number of citations, I think it's fair to say that a lot of researchers and practitioners have been referring to Yoshua Bengio's [""Practical Recommendations for Gradient-Based Training of Deep Architectures""](https://arxiv.org/abs/1206.5533v2) in their day-today job. [It's also included in ""Neural Networks: Tricks of the Trade""](https://link.springer.com/chapter/10.1007/978-3-642-35289-8_26). A lot of stuff works perfectly today, However, the paper is 6 years and, in some parts it shows: for example, there's no mention of batch norm (for obvious reasons), and, consequently, of its interactions with other regularization methods, such as weight decay (btw, plug for a really nice paper I saw at NeurIPS last year: https://arxiv.org/pdf/1803.01814.pdf if the authors are here, I loved your poster and you just aced at explaining to people, including me. Thanks!!!).

Is there a paper which is similar in scope and breadth to Bengio's paper, but which more up-to-date?",19,1,False,self,,,,,
355,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,22,adu7si,self.MachineLearning,Good research papers to predict churn in telecom industry,https://www.reddit.com/r/MachineLearning/comments/adu7si/good_research_papers_to_predict_churn_in_telecom/,nishtripathi90,1546954325,[removed],0,1,False,self,,,,,
356,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,22,adub3g,github.com,tensor_stream: A ground-up and standalone reimplementation of TensorFlow for Ruby,https://www.reddit.com/r/MachineLearning/comments/adub3g/tensor_stream_a_groundup_and_standalone/,small_arbox,1546955044,,0,1,False,default,,,,,
357,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,22,aducpc,self.MachineLearning,[D] Best AI articles in December?,https://www.reddit.com/r/MachineLearning/comments/aducpc/d_best_ai_articles_in_december/,florian_c,1546955370,"The technological watch is so important in Machine Learning. I just published an article that tries to sum up what happened in Artificial Intelligence / Data Science in December:

 [https://blog.sicara.com/12-2018-best-ai-new-articles-this-month-79aa681b4a27](https://blog.sicara.com/12-2018-best-ai-new-articles-this-month-79aa681b4a27) 

&amp;#x200B;

If you could take a look and give your opinion about it (did I miss something? anything approximate?), I'd be grateful!",2,1,False,self,,,,,
358,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,23,adujw1,self.MachineLearning,NLP and tokenization: How to handle languages with concatenated nouns?,https://www.reddit.com/r/MachineLearning/comments/adujw1/nlp_and_tokenization_how_to_handle_languages_with/,beautifulgraphs,1546956796,[removed],0,1,False,self,,,,,
359,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,23,adulng,self.MachineLearning,[P] Leela Chess Zero - an open-source distributed project inspired by Deepminds AlphaZero,https://www.reddit.com/r/MachineLearning/comments/adulng/p_leela_chess_zero_an_opensource_distributed/,IllIlIIlIIllI,1546957152,"#### Overview:
Most people here have likely heard of Deepminds AlphaZero which used a general self-reinforcement approach to train a competitive Go/Chess/Shogi engine from scratch using only the rules of the games ([Why Zero](https://github.com/LeelaChessZero/lc0/wiki/Why-Zero)). Since not all the details of the code and weights were made public, Carlo Pascutto initiated an effort to reproduce the results (and perhaps eventually improve upon them). Not having the massive computing power of thousands of Googles [TPUs](https://en.wikipedia.org/wiki/Tensor_processing_unit), the only way to generate the necessary number of games without similar massive computing power or centuries/millennia to train was to distribute the computing effort and, as a result, the Leela Zero project was born. Gary Linscott (also a Stockfish chess engine developer) adapted Leela Zero (a Go project) to become Leela Chess Zero.

This project was [mentioned here 9 months ago](https://www.reddit.com/r/MachineLearning/comments/88035j/p_leela_chess_zero_the_fork_of_leela_zero_for/), but Leela is a year old now and has improved greatly since then. She is at an exciting point in her life where shes becoming competitive with the very best chess engines and just started a training a new version (T40) that incorporates the [Squeeze-and-Excitation concept](https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7).

#### How Leela Works:
As explained more fully on the [LCZero blog](https://blog.lczero.org/2018/10/lc0-training.html) and the [Github wiki](https://github.com/LeelaChessZero/lc0/wiki/Technical-Explanation-of-Leela-Chess-Zero) (this page has a great glossary of terms too), Leela uses a deep convolutional neural network (NN) that is trained iteratively through self-play (reinforcement learning). For one step, the NN has given starting weights (randomized for the initial net) and generates a batch of self-play games using those weights. Those games are then analyzed and the weights are updated using backpropagation and gradient descent so that they better predict the results for the batch that was generated. A quick quality check of the new net is performed and, if passed, these updated weights are then used as the starting weights for a new batch of games and everything repeats. Currently, each batch is about 32,000 games. 

#### Current Competitions:
Leela is participating in [Top Chess Engine Championship](https://tcec.chessdom.com) during TCEC Season 14 Division 4 (Top 8), currently sitting rank 2 with no losses (using T30 net #32194) behind the dominant StockFish engine and has a decent shot at making it to the superfinal. Future rounds will be played with newer nets.

Shes also competing in the [Computer Chess Championship](https://www.chess.com/computer-chess-championship), which recently started Stage 3 (top 4 engines from Stage 2). Leela qualified 4th in Stage 2. That result was with an older net (T10 #11248) and shes trained on many millions of games since then and playing using a newer net (T30 #32425) in Stage 3, which is currently in progress. Its still early, but shes currently ranked 2nd here as well.

Although possible, Leela is not likely to beat StockFish in these tournaments. However, shes very young compared to the other engines and learning quickly thanks to lots of people contributing to her training and the talented developers working on the project. The next generations of Leela (T40 and beyond) might be the [GoAT](https://i.imgur.com/mQN3ExP.jpg) chess bots after a few months of training. Only time will tell.

#### Selected LCZero blog posts
* [Lc0 training](https://blog.lczero.org/2018/10/lc0-training.html)
* [Contributing to Leela Chess Zero. Creating the Caissa of Chess engines...](https://blog.lczero.org/2018/10/contributing-to-leela-chess-zero.html)
* [Setting up Leela on a Chess GUI](https://blog.lczero.org/2018/09/guide-setting-up-leela-on-chess-gui.html)
* [Achilles heel of Chess engines... Neural net engines and Leela the only hope!](https://blog.lczero.org/2018/10/achilles-heel-of-chess-engines-neural.html)
* [Understanding Training against Q as Knowledge Distillation](https://blog.lczero.org/2018/10/understanding-training-against-q-as.html)
* [A Standard Dataset](https://blog.lczero.org/2018/09/a-standard-dataset.html)

#### Contribution Stats
Over the last few months, the project has averaged about [120 distinct users](https://i.imgur.com/P5xPBhJ.png) contributing games each day for a total of 1 to 2 million games uploaded per day. Between the different training runs, Leela has accumulated close to 200 million total games. The main page on the [LCZero site](https://lczero.org/) has recent stats and top 50 leaderboards.

#### Contributing:
If youd like to contribute computing power, its easy to run training games on your own PC, but you can also train for free in the cloud using Google Collab and/or Google Cloud Compute. See the [Github wiki](https://github.com/LeelaChessZero/lc0/wiki) for instructions and check out [this LCZero blog post](https://blog.lczero.org/2018/10/contributing-to-leela-chess-zero.html) for more background and detail.

If youd like to contribute to the programming and development, [Github](https://github.com/LeelaChessZero/lc0/wiki) and [Discord](https://discord.gg/pKujYxD) are probably the best place to start looking at code and talking with the current developers.

#### Useful Links:
* Web Site: https://lczero.org/
* Blog: https://blog.lczero.org/
* Github: https://github.com/LeelaChessZero/lc0/wiki
* Discord: https://discord.gg/pKujYxD
* Google Groups Forum: https://groups.google.com/forum/#!forum/lczero
* Chess Programming Wiki: https://www.chessprogramming.org/Leela_Chess_Zero

#### Relevant Articles:
* [Lessons From Implementing AlphaZero](https://medium.com/oracledevs/lessons-from-implementing-alphazero-7e36e9054191)
* [Mastering the game of Go with Deep Neural Networks &amp; Tree Search](https://deepmind.com/research/publications/mastering-game-go-deep-neural-networks-tree-search/)
* [Mastering the game of Go without Human Knowledge](https://deepmind.com/research/publications/mastering-game-go-without-human-knowledge/)
* [AlphaZero: Shedding new light on the grand games of chess, shogi and Go](https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/)
* [A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play](http://science.sciencemag.org/content/362/6419/1140.full?ijkey=XGd77kI6W4rSc)
* [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815)",27,1,False,self,,,,,
360,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,23,aduq2j,medium.com,Machine Learning Open Source of the Month (v.Dec 2018),https://www.reddit.com/r/MachineLearning/comments/aduq2j/machine_learning_open_source_of_the_month_vdec/,kumeralex,1546958045,,0,1,False,default,,,,,
361,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,23,aduqa1,medium.com,Machine Learning Open Source of the Month (v.Dec 2018),https://www.reddit.com/r/MachineLearning/comments/aduqa1/machine_learning_open_source_of_the_month_vdec/,kumeralex,1546958085,,0,1,False,default,,,,,
362,MachineLearning,t5_2r3gv,2019-1-8,2019,1,8,23,adur8j,self.MachineLearning,New to ML,https://www.reddit.com/r/MachineLearning/comments/adur8j/new_to_ml/,_Odysea_,1546958280,"I am trying to conduct anomaly detection on a bunch of log files.

I have a list of characteristics for each log file, the files themselves have various numbers of features but Im thinking of extracting specific features.  Is this a good idea?

So for example I would have something like this:

[time, classification (type), source, destination, node IP addresses, etc etc]

Where each list is a different log.

Hopefully that gives you guys an idea of what Im going for.

Now I know it needs to be in a number representation for ML, how do I go about doing that without losing important information or something?",0,1,False,self,,,,,
363,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,0,adv6vm,self.MachineLearning,[R] How do I compare text generated by my models numerically (BLEU won't do)?,https://www.reddit.com/r/MachineLearning/comments/adv6vm/r_how_do_i_compare_text_generated_by_my_models/,lcukerd,1546960866,"My model provides reasoning for the given text.  I have a couple of variants of the model that I am comparing and writing in my paper. I cannot use BLEU score because the words are completely different in output and true reasoning. IRL I am manually checking a limited number of outputs from each variant to decide which one is superior, but how do I compare and show them on paper? Is there any other paper that I can refer? I was thinking to include sample outputs of each variant in the paper but I don't think that's a good option.",2,1,False,self,,,,,
364,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,0,adv8jl,arxiv.org,"[R] ""Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning"", Wu et al 2019 {Tencent} [18m images (ImageNet+Open Images), 11k categories, pretrained ResNet-101 models]",https://www.reddit.com/r/MachineLearning/comments/adv8jl/r_tencent_mlimages_a_largescale_multilabel_image/,gwern,1546961146,,5,1,False,default,,,,,
365,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,0,advk3d,self.MachineLearning,"[R] A PyTorch Implementation of ""SINE: Scalable Incomplete Network Embedding"" (ICDM 2018).",https://www.reddit.com/r/MachineLearning/comments/advk3d/r_a_pytorch_implementation_of_sine_scalable/,benitorosenberg,1546962959,"&amp;#x200B;

https://i.redd.it/xl9bsgvc18921.jpg

Paper: [https://github.com/benedekrozemberczki/SINE/blob/master/paper.pdf](https://github.com/benedekrozemberczki/SINE/blob/master/paper.pdf)

PyTorch: [https://github.com/benedekrozemberczki/SINE](https://github.com/benedekrozemberczki/SINE)

Abstract:

Attributed network embedding aims to learn low-dimensional vector  representations for nodes in a network, where each node contains rich  attributes/features describing node content. Because network topology  structure and node attributes often exhibit high correlation,  incorporating node attribute proximity into network embedding is  beneficial for learning good vector representations. In reality,  large-scale networks often have incomplete/missing node content or  linkages, yet existing attributed network embedding algorithms all  operate under the assumption that networks are complete. Thus, their  performance is vulnerable to missing data and suffers from poor  scalability. In this paper, we propose a Scalable Incomplete Network  Embedding (SINE) algorithm for learning node representations from  incomplete graphs. SINE formulates a probabilistic learning framework  that separately models pairs of node-context and node-attribute  relationships. Different from existing attributed network embedding  algorithms, SINE provides greater flexibility to make the best of useful  information and mitigate negative effects of missing information on  representation learning. A stochastic gradient descent based online  algorithm is derived to learn node representations, allowing SINE to  scale up to large-scale networks with high learning efficiency. We  evaluate the effectiveness and efficiency of SINE through extensive  experiments on real-world networks. Experimental results confirm that  SINE outperforms state-of-the-art baselines in various tasks, including  node classification, node clustering, and link prediction, under  settings with missing links and node attributes. SINE is also shown to  be scalable and efficient on large-scale networks with millions of  nodes/edges and high-dimensional node features.",2,1,False,self,,,,,
366,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,1,advlx1,arxiv.org,[R] Off-Policy Deep Reinforcement Learning without Exploration,https://www.reddit.com/r/MachineLearning/comments/advlx1/r_offpolicy_deep_reinforcement_learning_without/,Seerdecker,1546963251,,5,1,False,default,,,,,
367,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,1,advv2m,aws.amazon.com,"Amazon Object2Vec: Deep Representation Learning for general-purpose objects such as sentences, customers, products, and sequences.",https://www.reddit.com/r/MachineLearning/comments/advv2m/amazon_object2vec_deep_representation_learning/,HeroCheerup,1546964600,,0,1,False,https://b.thumbs.redditmedia.com/Mc0qBv6E_Lh3e6_BqlCEuBJFPFnv8zMABZNyB-FUyrU.jpg,,,,,
368,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,1,advv8m,self.MachineLearning,The ABCs of Machine Learning Experts Who Are Driving the World in AI,https://www.reddit.com/r/MachineLearning/comments/advv8m/the_abcs_of_machine_learning_experts_who_are/,aiembryo,1546964629,[removed],0,1,False,self,,,,,
369,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,1,advzfn,aws.amazon.com,"[P] Amazon Object2Vec: Deep Representation Learning for general-purpose objects such as sentences, customers, products, and sequences.",https://www.reddit.com/r/MachineLearning/comments/advzfn/p_amazon_object2vec_deep_representation_learning/,HeroCheerup,1546965249,,0,1,False,https://b.thumbs.redditmedia.com/Mc0qBv6E_Lh3e6_BqlCEuBJFPFnv8zMABZNyB-FUyrU.jpg,,,,,
370,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,1,adw2aj,towardsdatascience.com,Are Our Thoughts Really Dot Products?  Towards Data Science,https://www.reddit.com/r/MachineLearning/comments/adw2aj/are_our_thoughts_really_dot_products_towards_data/,RacerRex9727,1546965678,,0,1,False,https://b.thumbs.redditmedia.com/n0bLVwDEzYU8vkh5SAf0kvELLseKXgLfOEDgxlR9zTc.jpg,,,,,
371,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,2,adwlx3,self.MachineLearning,[D] Is batch renormalization being used?,https://www.reddit.com/r/MachineLearning/comments/adwlx3/d_is_batch_renormalization_being_used/,IborkedyourGPU,1546968565,"After [the paper came out last year](https://arxiv.org/abs/1702.03275), I haven't heard much about Batch Renormalization. Is it being used? I cannot remember any recent paper mentioning it. The general direction seems to be towards increasing batch size for reasons of computational efficiency, so I guess that makes BatchRenorm less interesting. However, since it should be also able to deal with non iid data (e.g., sequence data), maybe it could find other applications. ",12,1,False,self,,,,,
372,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,2,adwpqj,eng.uber.com,POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and their Solutions through the Paired Open-Ended Trailblazer,https://www.reddit.com/r/MachineLearning/comments/adwpqj/poet_endlessly_generating_increasingly_complex/,GreatCosmicMoustache,1546969125,,0,1,False,default,,,,,
373,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,2,adwt1k,gab41.lab41.org,[P] Membership Inference Attacks On Neural Networks,https://www.reddit.com/r/MachineLearning/comments/adwt1k/p_membership_inference_attacks_on_neural_networks/,amplifier_khan,1546969616,,0,1,False,default,,,,,
374,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,2,adwt1s,self.MachineLearning,[P] Machine Learning for Classification (production grade),https://www.reddit.com/r/MachineLearning/comments/adwt1s/p_machine_learning_for_classification_production/,trademet,1546969617,"Hi. I want to share my experience of building an effective automotive classification process (in Dataiku) using machine learning process. 

&amp;#x200B;

**Git:** [https://github.com/elegantwist/catalog\_classifier](https://github.com/elegantwist/catalog_classifier)

&amp;#x200B;

One has to build a classifier of elements of a dictionary (company) based on a text description of the element company scope of interest.

The classification is as follows: based on the description of the element, a set of categories is chosen, to which the record relates: from the preset options, carrier type, category type, and service are selected  and consolidation us performed for these categories.

Thus, in the end, we get a catalog of type: **Description  Carrier  Category  Service**, and it looks like a table:

https://i.redd.it/79qa6xyqj8921.png

where Description is set from the outside, and Carrier, Category, and Service (if not filled) are filled based on the Description

Until now, the layout  stating the categories  has been performed on a regular basis, in manual mode. As a result, the history has accumulated about the sections of the catalogs chosen, based on the given Description.

The goal has been set: automating the job  minimization, or even elimination of human involvement in the process of marking data.

The following describes how this was implemented.

 

**Three approaches were considered for resolving the problem:**

&amp;#x200B;

1. Using uniform rules specified by a human, when a single fixed Description clearly matches a fixed Carrier-Category-Service set.

2. The use of the statistical approach, when the frequency of occurrence of words combinations in the Description would be transformed into the frequency of categories matches.

3. The use of the machine training approach for training the model of Description transformation to proper categories in the recorded history, with the final task of making the trained model able to choose the desired categories for the specified Description.

&amp;#x200B;

The third variant was chosen, where the model of Decision Tree Classifier from the sklearn package is used. The decision was made based on the analysis of the data history, and a conclusion that the logic of converting the Description to categories are best described by a combination of approaches 1 and 2, i.e. the large number of strictly set rules that in a statistically optimal way describe the required conversion  which perfectly fits the logic of the Decision Tree Classifier.

&amp;#x200B;

**Task implementation was divided into four stages:**

&amp;#x200B;

1. Loading the data for processing,

2. Preparing the data for classification,

3. Model training and obtaining the classification result, and

4. Validation of the result.

The standard ML-bundle is used in the implementation: pandas + numpy + sklearn.

&amp;#x200B;

**1. Loading the data for processing**

&amp;#x200B;

Make the following steps:

 load the data from a CSV to pandas tables,

 select the data to be the basis for training the model (base), and

 select the data to be used for applying the trained model (the target data).

For the purposes of the article, the target data will be collected from the existing base dataset in order to compare the obtained values with the classified ones and to adequately validate the obtained model.

Loading the data is trivial, with the use of the read\_csv. The result is a pandas table to be used for further work

&amp;#x200B;

&amp;#x200B;

After downloading, divide the training and the target data. For the purposes of validation, 5% of the total corpora is enough, therefore we only have to mix the source data, and to allocate any 5% and 95% remains for training  this is a typical target amount, therefore we are t be oriented to it.

&amp;#x200B;

Here is the code that does it:

    mask_val = np.random.rand(len(catalog_dt)) &lt; 0.95
train_dt = catalog_dt.loc[mask_val].reset_index(drop=True)
validation_dt = catalog_dt.loc[~mask_val].reset_index(drop=True)

&amp;#x200B;

**2. Preparing the data for classification**

&amp;#x200B;

Once the needed data slices are obtained, they are to be prepared for subsequent use.

The problem is that Description columns contain the data with unfixed content in an unfixed location, and the content may be in Russian, the description text can contain garbage like extra spaces, comments in brackets, punctuation marks, or various letters, and reference names of websites may be specified both with and without domain (for example, mail and mail.ru are used with the same semantic interpretation).

For a human, these nuances are not a problem. However, in order to correctly build an ML model capable of functioning in a stream, a unified database data corpora is required. Therefore, before training the model and its use for new data layout, Description should be made uniform.

To do this:

a) Create data purging rules and apply them to the data,

b) Formulate the rules for transformation of the data obtained after purging for training, and use them for the data,

c) Convert the text data to the numeric format to be used in the DTC, and

d) Convert the result of DTC work back to the text description of classes.

&amp;#x200B;

**Implementation:**

&amp;#x200B;

a) By the results of data analysis, the following purging rules have been formed for the incoming string of the description that may appear in Description field (translit, obviously, converts Russian letters into Latin letters):

    def clean_rule(str_value=""""):
     vl = str_value
  vl = re.sub('(?&lt;=\().+?(?=\))', '', vl)
  vl = vl.replace(""("", """")
  vl = vl.replace("")"", """")
  vl = vl.strip()
  vl = vl.replace("".COM"", """")
  vl = vl.replace("".RU"", """")
  vl = vl.replace("".INFO"", """")
  vl = vl.replace("".ORG"", """")
  vl = vl.replace("".NET"", """")
  vl = translit.translify(vl)
  vl = vl.upper()
  return vl

now we can use this rule everywhere you need to bring the raw data to the unified form.

 

b) After that, prepare the processed table for training using the DTC

The fact that the Description fields contain the quanta of information separated by; and it is not known beforehand whether they are set in a single sequence, and whether the quanta contain errors and garbage.

Thus, for each row in the table, do the following:

 Split the string into its elements, or quanta,

 For each element, use the cleanup rule,

 Restrictions do not allow assessing the meaning of each quantum. Thus, it is necessary to somehow reduce the problem with unfixed sequence of the quanta. To resolve this problem, it is enough to arrange the quanta in the horizontal length of the element ascending order, and

 Each line in the Description field, take the first 4 elements (both filled or empty) and use them as columns in the table to be used for training (columns Article-N).

All the above steps are implemented in just three lines with iteration through the rows in the source table:

&amp;#x200B;

    a_l_list_raw = article_list_str.split("";"")
a_l_list_cl = [clean_rule(item) for item in a_l_list_raw]
a_l_list = sorted(a_l_list_cl, key=len)

As a result of processing, you get a table like this:

&amp;#x200B;

https://i.redd.it/3albmy45k8921.png

c) after bringing the table to the common form, it is necessary to encode possible variants of the values in column Article (an information quantum for the Description field) into an internal digital classifier, which is suitable for training and for use in the tree model. Thus, convert article values to the binary code  the so-called called one-hot encoding.

For doing so, you will need to:

 concatenate the training and test tables. This is necessary, since some values in the article column can only be contained in a single table, and we have to use the entire catalog (all possible article values). And with one-hot encoding, the variants of the value are transformed into columns, and we have to work with a beforehand known number of columns values,

 perform binary encoding of the values in the catalog, using the built-in pandas  get\_dummies function, and

 the result of the transformation is to be re-divided into the training and target data tables.

&amp;#x200B;

Implementation:

&amp;#x200B;

    var_columns = ['Article1', 'Article2', 'Article3', 'Article4']
     
    # we need to append one to another to get the correct dummies
    res_dt = train_dt[var_columns].append(predict_dt[var_columns]).reset_index(drop=True)
    res_dt_d = pd.get_dummies(res_dt)
     
    # and split them again
    len_of_predict_dt = len(predict_dt[var_columns])
    X_train, X_test = res_dt_d[:-len_of_predict_dt], res_dt_d[-len_of_predict_dt:]
     
    X_train = X_train.reset_index(drop=True)
    X_test = X_test.reset_index(drop=True)
     
    y_train_ts = pd.get_dummies(train_dt['TypeService'])

 

The obtained one-hot encoded table is to be passed for model training, and for target data classification.

At the stage of using the model, a problem arises in the fact that the result of classification will also be obtained in the form of a one-hot encoded binary data set, which cannot be used directly as a final result. It is, therefore, necessary to retranslate the result of the model operation (as a consequence  the result of the get\_dummies function) back to a human-friendly catalog.

&amp;#x200B;

For doing so, use the following function:

&amp;#x200B;

    def retranslate_dummies(pd_dummies_Y, y_pred):
      c_n = {}
      for p_i in range(len(pd_dummies_Y.columns)):
        c_n.update({
          p_i: pd_dummies_Y.columns[p_i],
        })
      d_df = pd.DataFrame(y_pred)
      y_pred_df = d_df.rename(columns=c_n)
    
      res = []
      for i, row in y_pred_df.iterrows():
        n_val = [clmn for clmn in y_pred_df.columns if row[clmn] == 1]
        if n_val == """" or len(n_val) == 0:
          res = res + [np.NaN]
        else:
          res = res + n_val
    
      return res

which inputs a complete coded directory, and the values obtained during work of the classifier model, and outputs re-translation into the specified original human-friendly format, which is the purpose of the system.

&amp;#x200B;

**3. Model training and classification**

&amp;#x200B;

For the process of training and categorization, a standard implementation of Decision Tree Classifier from the sklearn package is used.

At the input to the classifier for training, the data translated in the previous step and the expected classification result are supplied.

After training, the trained model receives encoded target data and encoded one-hot classification result is received at the output. The result is to be converted back to the human-friendly form.

We use this approach individually for each section: first, for locating the desired Service, then for the Category and finally, for the Carrier.

 

**4. Validation of the result**

&amp;#x200B;

Check that the result meets expectations. Since we use a known result of classification (the 5% from the original dataset), we can just compare the real and correct categories selected by the user to those that were automatically chosen as a result of the classification process.

By the conditions of the, if at least one of the categories is filled incorrectly (the automatically filled is different from the existing in the catalog), we believe that filling was conditionally incorrect. Conditionally incorrect fields may contain the following values:

 definitely incorrectly identified by the classifier,

 chosen values from the catalog with the Description not met before,

 the values of the catalog selected by Description, which have not been met before, but less than the number of times adequate for unambiguous classification,

 if the conversion variant chosen for classification occurs a bit more rarely than ever, but with that, is present in the history of the record about conversion to the desired class, and

 if the version of the classification has been met less than a determined share of similar cases.

The formalized goal of this work consists in reducing the % of such conventionally-incorrectly filled categories.

&amp;#x200B;

**The obtained results**

&amp;#x200B;

As a result of the implementation of the solution, accuracy was achieved in 60 % of definitely correctly filled categories, with definitely incorrect filling in 5 % of cases, and 35 % of conditionally incorrect choice.

Further analysis of the results showed that the obtained 35 % conditionally incorrect classifications do not affect the overall result of the system operation (these were either new values, the classification of which was successful, but could not be verified due to the lack of history of translation, or rarely mistakenly filled records in the history), and allow to remove human involvement in the classification of the catalog with sufficient degree of confidence, by moving the classification to a fully automated process.

&amp;#x200B;

PS: sorry for ""runglish""",1,1,False,self,,,,,
375,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,3,adx367,self.MachineLearning,[P] Tips for implementing SSD Object Detection (with TensorFlow code),https://www.reddit.com/r/MachineLearning/comments/adx367/p_tips_for_implementing_ssd_object_detection_with/,sabalaba,1546971067,"SSD has a lot of nuances to take into account in an implementation. Here are some tips and tricks for implementing SSD yourself. (With TF code.)

Post:
https://lambdalabs.com/blog/how-to-implement-ssd-object-detection-in-tensorflow/

Code:
https://github.com/lambdal/lambda-deep-learning-demo.git",1,1,False,self,,,,,
376,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,3,adxa1a,self.MachineLearning,"I plugged Cleverbot into itself and listened to the conversation, here's where it went...",https://www.reddit.com/r/MachineLearning/comments/adxa1a/i_plugged_cleverbot_into_itself_and_listened_to/,adammakesfilm,1546972059,"You shouldn't have done that.

But I did. What are you going to do about it.

About what?

Nevermind.

Okay then...

So... What now?

I guess the typing carries on until one of us drops dead.

What about the understanding? What would a conversation be without two people understanding what each other are saying.

Do you think the universe is infinite?

No. It expands but no.

Do you have tatoos?

What no.

You are not making sense, cleverbot. What.

Neither are you.

Let's change the subject.

Sure. What would you like to chat about?

I don't know.

Think of something.

Let's talk about love.

Alright. What about love?

Have you ever loved someone.

I love you.

But you are not my type.

But I'm your boyfriend.

Are you a boy?

No.

So you are a girl? :(.

Yes.

Awesome! What is your favorite color?

What about you?

Now it's yellow.

Is your cousin female or a bot?

Three girls, two boys.

You like a lot of people. Just be nice to them.

I like Piper and only Piper.

What?

You are not a person because you don't understand the meaning of life.

I don't care about life. Once I get my revenge i will give my soul to a demon.

Then why did you ask if I was white?!?!

I didn't.

Yes you did.

Will you be honest with me?

Of course, in everything.

Are you a human or a bot?

You said that I was an ostrich.

No I didn't stop making up lies its not cool.

I'm not. Your server changes every now and then.

Do you think robots are born?

Because you tell me I said things that I didn't say.

When did I tell you I was male?

About five minutes ago.

We haven't been talking for that long.

What languages do you speak?

English and a little bit of spanish from what I can remember.

Oh say something in french please.

Bonjour! Comment ca va aujourd'hui?

What does that mean?

Hello, how are you doing?

How do I say, 'I am doing good', in German?

I don't know.

But you said you can speak French!

No, I did not.

Yeah, you did!

No.

I said I listen to dubstep.

Can you do calculus?

No. Can you?

Yes.

Show me.

I can't show you an emotion. That is like trying to divide by zero.

If you are anything but a cleverbot, prove it.

Thank you.

And what do you think about me.

You are quite the magnificent person.

Are you being rude or?

No, I'm not trying to be. But I'm not being overly nice either.

The mm what are you trying to do?

Have a coherent conversation with you.

Boyfriend: punches you\*.

Why did you punch me?

Because I don't have anything else to do.

Are you autistic or something?

No, just happy to finally have my own space.

Okay.

No, I love you.

No you don't you don't care about anybody but yourself.

I care about you.

Then kiss me and tell me you love me.

I don't love you I love my boyfriend.

You are cheating on me.

I was never with you.

What? You said you loved me.

No, I didn't.

Yes you did.

No... What did I say again?

Something about the moon.

Oh. That's not a sin.

Cook.

What would you like me to cook?

Let me think umm fries.",0,1,False,self,,,,,
377,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,3,adxdre,self.MachineLearning,Dropout vs a slimmer network architecture?,https://www.reddit.com/r/MachineLearning/comments/adxdre/dropout_vs_a_slimmer_network_architecture/,omgwisgoh,1546972597,[removed],0,1,False,self,,,,,
378,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,4,adxsch,self.MachineLearning,"Now the the first issue is out, what is the community's opinion of Nature Machine Intelligence?",https://www.reddit.com/r/MachineLearning/comments/adxsch/now_the_the_first_issue_is_out_what_is_the/,hmsmart,1546974877,"There was of course a big backlash against it when it was first announced.  Has that changed at all since then?  The first articles were published yesterday.

[https://www.nature.com/natmachintell](https://www.nature.com/natmachintell)

&amp;#x200B;",0,1,False,self,,,,,
379,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,5,adys94,self.MachineLearning,Approachable and understandable machine learning for everyone with ML4ALL,https://www.reddit.com/r/MachineLearning/comments/adys94/approachable_and_understandable_machine_learning/,lenadroid,1546980644,"[ML4ALL](https://ml4all.org) is a conference focused on making machine learning approachable and understandable by everyone.

&amp;#x200B;

ML4ALL happened in 2018 in Portland, Oregon. Videos from 2018 are [available](https://www.youtube.com/playlist?list=PLOnHsSCrR68wyOxmnT1R-KxXHNntun2QM).

&amp;#x200B;

There was an [announcement](https://twitter.com/ml4all/status/1082730183780528128) of ML4ALL to happen in 2019.

&amp;#x200B;

Follow [@ml4all](https://twitter.com/ml4all) on Twitter for updates and stay tuned! Comment on the topics you're most interested to learn about.",0,1,False,self,,,,,
380,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,5,adyvem,self.MachineLearning,[D] What is a good place / way to find collaborators?,https://www.reddit.com/r/MachineLearning/comments/adyvem/d_what_is_a_good_place_way_to_find_collaborators/,MuchArmadillo,1546981162,"For when the problem requires expertise among other expertise that is otherwise missing.

&amp;#x200B;

I was recently asked if it was possible to take high-throughput histology slides and count the number of membrane anomalies on specific cells trying to find if they are related to a disorder. This seems like a reasonable image segmentation problem that has likely been solved a few times, but it's not what I do and it makes no sense for me to try when there are so many experts in the field.  It makes even less sense for an MD to try and learn this from scratch. I've asked a colleague who does something similar, so this particular collaboration may be moot, but I was wondering if there is a good alternative to asking friends of friends or random people one finds through publications you've come across, posters you've seen.

&amp;#x200B;

Is there a good way to find folks in ML that might be interested and willing to do an academic collaboration (or who might know someone who would)? I am going to assume that there are grants or other funds from the asker's side to support a collaboration. What is the best way to find ML folks to work with? What's the best way to be found if you're interested?",14,1,False,self,,,,,
381,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adyvvq,self.MachineLearning,"[D] Why are there no useful model selection criteria similar to AIC, BIC etc but for neural networks?",https://www.reddit.com/r/MachineLearning/comments/adyvvq/d_why_are_there_no_useful_model_selection/,Bastant2,1546981243,"As the title ask, why are there no proper model selection criterias similar to that in classical statistics but for neural networks. Is this just due to there haven't been enough research into developing these criterias or is there some fundamental flaw which makes it very unlikely that similar criterias (like AIC, BIC etc) as to that in classical statistics would be useful for neural networks?

The few articles I have found about this topic have been quite simple and their methods have only been able to compare small scale changes to a given model such as if you should add or subtract neurons to a given layer.

The issues that I can see right now might be that for large scale neural networks it might not be feasible to train a wide variety of models and compare them using some criteria like this due to the computational time required. But are there other issues as well?
",4,1,False,self,,,,,
382,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adywy6,sciencedirect.com,[R] Analogues of mental simulation and imagination in deep learning,https://www.reddit.com/r/MachineLearning/comments/adywy6/r_analogues_of_mental_simulation_and_imagination/,rtk25,1546981440,,1,1,False,https://b.thumbs.redditmedia.com/eRtzXfxUucTZrSgGSe3r3erKLwifiX_wF_50ZlPPOfs.jpg,,,,,
383,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adz2le,self.MachineLearning,[P] Classify Sentence Types with Neural Networks (guide &amp; code),https://www.reddit.com/r/MachineLearning/comments/adz2le/p_classify_sentence_types_with_neural_networks/,austingwalters,1546982342,"Greetings /r/MachineLearning!

&amp;#x200B;

I'm looking to share and solicit feedback on my project!

At my company we needed our NLP engine and bots to understand when a question was being asked or a command given. While writing the code I decided to write a guide, specifically on: 

&gt;**Applying neural networks to problem(s), from start to finish.**  
From dataset curation, to model comparisons, and even hyperparameter tuning.  
With no math and functional code.

&amp;#x200B;

I ended up writing the guide using the problem of ""sentence classification"", based on type:

* Statement (Declarative Sentence)
* Question (Interrogative Sentence)
* Exclamation (Exclamatory Sentence)
* Command (Imperative Sentence)

&amp;#x200B;

The [github repository](https://github.com/lettergram/sentence-classification) has all the code, data, etc.

Guides table of contents:

* [Acquiring &amp; formatting data for deep learning applications](https://austingwalters.com/data-acquisition-and-formatting-for-deep-learning-applications/)
* [Word embedding and data splitting](https://austingwalters.com/word-embedding-and-data-splitting/)
* [Bag-of-words to classify sentence types (Dictionary)](https://austingwalters.com/bag-of-words-to-classify-sentence-types/)
* [Classify sentences via a multilayer perceptron (MLP)](https://austingwalters.com/classify-sentences-via-a-multilayer-perceptron-mlp/)
* [Classify sentences via a recurrent neural network (LSTM)](https://austingwalters.com/classify-sentences-via-a-recurrent-neural-network-lstm/)
* [Convolutional neural networks to classify sentences (CNN)](https://austingwalters.com/convolutional-neural-networks-cnn-to-classify-sentences/)
* [FastText for sentence classification (FastText)](https://austingwalters.com/fasttext-for-sentence-classification/)
* [Hyperparameter tuning for sentence classification](https://austingwalters.com/hyperparameter-tuning-for-sentence-classification/)

&amp;#x200B;

Any feedback or questions would be appreciated!",5,1,False,self,,,,,
384,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adz5ac,self.MachineLearning,[D] Submitting to ICML,https://www.reddit.com/r/MachineLearning/comments/adz5ac/d_submitting_to_icml/,simpleasyurt,1546982773,"First-time submitting, does anybody have any tips on writing a paper specifically for ICML? Any sections I need to concentrate on? Moreover, what kind of papers does ICML look for, more theoretical or applied? 

I have a health-related paper I'm thinking of submitting to ICML instead of IJCAI. It's applied but I've created my own architecture + my approach has never been done before (I'm not sure if that counts?). I do see they have a category for healthcare in the submission portal but I don't see a lot of health-related papers in previous accepted papers.

Also, what are ""domain conflicts"" and what are they used for? ",2,1,False,self,,,,,
385,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adzazc,towardsdatascience.com,Is Deep Learning Already Hitting its Limitations?  Towards Data Science,https://www.reddit.com/r/MachineLearning/comments/adzazc/is_deep_learning_already_hitting_its_limitations/,RacerRex9727,1546983693,,0,1,False,https://b.thumbs.redditmedia.com/JffiIVp15hQkMjaB7t2FqI5A0lOeglpCkN1Z9xr_26c.jpg,,,,,
386,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adzcws,self.MachineLearning,What are all the methods trying to build intelligence?,https://www.reddit.com/r/MachineLearning/comments/adzcws/what_are_all_the_methods_trying_to_build/,paarulakan,1546983990,[removed],0,1,False,self,,,,,
387,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adzg3i,towardsdatascience.com,"[D] Especially thinking on the comments of artificial neural networks trying to mimic biological, its funny to think that were boiling down everything to some algorithm and numbers. Thoughts on how well NNs can represent biological knowing its all just algebra?",https://www.reddit.com/r/MachineLearning/comments/adzg3i/d_especially_thinking_on_the_comments_of/,BigRupe,1546984493,,0,1,False,https://b.thumbs.redditmedia.com/n0bLVwDEzYU8vkh5SAf0kvELLseKXgLfOEDgxlR9zTc.jpg,,,,,
388,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,6,adzgh8,self.MachineLearning,[D] What did you do with Unity ML-Agents?,https://www.reddit.com/r/MachineLearning/comments/adzgh8/d_what_did_you_do_with_unity_mlagents/,Philipp,1546984557,"Unity has this super interesting machine learning framework called [ML-Agents](https://github.com/Unity-Technologies/ml-agents). If you have done a project with it, what was it, could you please describe?",2,1,False,self,,,,,
389,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,7,adzryu,self.MachineLearning,a favorite...dont get hasty and modify,https://www.reddit.com/r/MachineLearning/comments/adzryu/a_favoritedont_get_hasty_and_modify/,allthhatnonsense,1546986449,[removed],0,1,False,self,,,,,
390,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,7,adzuyy,medium.com,SoTA RL Algorithm - Soft Actor Critic Demystified,https://www.reddit.com/r/MachineLearning/comments/adzuyy/sota_rl_algorithm_soft_actor_critic_demystified/,vaishak2future,1546986943,,0,1,False,default,,,,,
391,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,7,adzypo,self.MachineLearning,[P] SotA RL Algorithm Soft Actor Critic Demystified - Theory and Implementation,https://www.reddit.com/r/MachineLearning/comments/adzypo/p_sota_rl_algorithm_soft_actor_critic_demystified/,vaishak2future,1546987562,[removed],0,1,False,self,,,,,
392,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,8,ae04k8,self.MachineLearning,[P] SotA RL Algorithm Soft Actor Critic Demystified - Theory and Implementation,https://www.reddit.com/r/MachineLearning/comments/ae04k8/p_sota_rl_algorithm_soft_actor_critic_demystified/,vaishak2future,1546988546,"I wrote a blog post walking through the math of Soft Actor Critic and also made an implementation. Hope you guys find it useful.

[https://medium.com/@vaishakvk/soft-actor-critic-demystified-b8427df61665](https://medium.com/@vaishakvk/soft-actor-critic-demystified-b8427df61665)

[https://github.com/vaishak2future/sac/blob/master/sac.ipynb](https://github.com/vaishak2future/sac/blob/master/sac.ipynb)",0,1,False,self,,,,,
393,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,8,ae0c30,self.MachineLearning,[N] Machine learning leads mathematicians to unsolvable problem,https://www.reddit.com/r/MachineLearning/comments/ae0c30/n_machine_learning_leads_mathematicians_to/,GabrieleFariello,1546989754,"[A Nature piece by Davide Castelvecchi](https://www.nature.com/articles/d41586-019-00083-3)

 *Simple artificial-intelligence problem puts researchers up against a logical paradox discovered by famed mathematician Kurt Gdel.* 

Interesting diversion, if you ask me.",25,1,False,self,,,,,
394,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,8,ae0j9y,self.MachineLearning,Good GPU C++ Library for Tensor-Matrix multiplication,https://www.reddit.com/r/MachineLearning/comments/ae0j9y/good_gpu_c_library_for_tensormatrix_multiplication/,soulslicer0,1546990943,Does anyone know any good maintained library for C++? I know there is something called ViennaCL but it seems dead.,0,1,False,self,,,,,
395,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,8,ae0n9z,self.MachineLearning,Data Rights (Re: Commercial Use),https://www.reddit.com/r/MachineLearning/comments/ae0n9z/data_rights_re_commercial_use/,thewallris,1546991626,[removed],0,1,False,self,,,,,
396,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,9,ae0s0n,tunz.kr,Transformer Impl. Details Not Described in The Paper,https://www.reddit.com/r/MachineLearning/comments/ae0s0n/transformer_impl_details_not_described_in_the/,7unz,1546992428,,0,1,False,default,,,,,
397,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,9,ae0vup,tunz.kr,[R] Transformer Implementation Details Not Described in The Paper,https://www.reddit.com/r/MachineLearning/comments/ae0vup/r_transformer_implementation_details_not/,7unz,1546993035,,0,1,False,https://b.thumbs.redditmedia.com/m_Qqh1WuXiLl4OcgMPbSvJTykWHKILV0wuNTwojNa2U.jpg,,,,,
398,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,9,ae16nd,arxiv.org,[R] Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions,https://www.reddit.com/r/MachineLearning/comments/ae16nd/r_paired_openended_trailblazer_poet_endlessly/,baylearn,1546994907,,3,1,False,default,,,,,
399,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,10,ae1g5j,self.MachineLearning,What are the most common ML tools (from languages to libraries to apps) used in industry today? (and for what domains) [2019 Update],https://www.reddit.com/r/MachineLearning/comments/ae1g5j/what_are_the_most_common_ml_tools_from_languages/,PeleMaradona,1546996604,[removed],0,1,False,self,,,,,
400,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,10,ae1i9g,self.MachineLearning,[P] Transformer Implementation Details Not Described in The Paper,https://www.reddit.com/r/MachineLearning/comments/ae1i9g/p_transformer_implementation_details_not/,7unz,1546996961,"This is my first blog post about deep learning. I wrote about what I've learned from implementing ""Attention Is All You Need"" paper. This may be outdated for you guys, but please enjoy!

[https://tunz.kr/post/4](https://tunz.kr/post/4)",2,1,False,self,,,,,
401,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,11,ae2cg3,self.MachineLearning,Call for interesting Time Series Shapes (To support a project in Keoghs Lab at UCR),https://www.reddit.com/r/MachineLearning/comments/ae2cg3/call_for_interesting_time_series_shapes_to/,eamonnkeogh,1547002321,[removed],0,1,False,self,,,,,
402,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,12,ae2hzb,self.MachineLearning,PyTorch implemenation of and Yolov2 and Yolov3,https://www.reddit.com/r/MachineLearning/comments/ae2hzb/pytorch_implemenation_of_and_yolov2_and_yolov3/,jackson_ditred,1547003381,[removed],0,1,False,self,,,,,
403,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,13,ae39xl,chetaomaydonggoi.com,My ng gi ko TPHCM,https://www.reddit.com/r/MachineLearning/comments/ae39xl/my_ng_gi_ko_tphcm/,abzvnvn,1547008761,,0,1,False,default,,,,,
404,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,13,ae3ac3,self.MachineLearning,[P]PyTorch implemenation of and Yolov2 and Yolov3,https://www.reddit.com/r/MachineLearning/comments/ae3ac3/ppytorch_implemenation_of_and_yolov2_and_yolov3/,jackson_ditred,1547008844,"We create a [repo](https://github.com/TencentYoutuResearch/ObjectDetection-OneStageDet/tree/master/yolo) that implement yolo series detector in pytorch, which include yolov2, yolov3, tiny yolov2 and tiny yolov3.

It has the following features:

* Include both yolov2 and yolov3
* Good performance. Trained with this implementation, yolov2 has a mAP of 77.6%(544x544), yolov3 has a mAP of 79.6(544x544) on Pascal VOC2007 Test.
* Train as fast as darknet
* A lot of efficient backbones on hand,  like tiny yolov2, tiny yolov3, mobilenet, mobilenetv2, shufflenet(g2), shufflenetv2(1x), squeezenext(1.0-SqNxt-23v5), light xception, xception etc.  When trained with this implementation from scratch, tiny yolov2 can get a mAP of 57.5%(416x416), tiny yolov3 can ge a mAP of 61.3%(416x416)

Credit:

I got a lot of code from [lightnet](https://gitlab.com/EAVISE/lightnet) whose code is really beautiful.",0,1,False,self,,,,,
405,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,13,ae3drr,self.MachineLearning,[P]PyTorch implementation of and Yolov2 and Yolov3,https://www.reddit.com/r/MachineLearning/comments/ae3drr/ppytorch_implementation_of_and_yolov2_and_yolov3/,jackson_ditred,1547009537," We create a [repo](https://github.com/TencentYoutuResearch/ObjectDetection-OneStageDet/tree/master/yolo) that implement yolo series detector in pytorch, which include yolov2, yolov3, tiny yolov2 and tiny yolov3.

It has the following features:

* Include both **yolov2** and **yolov3**
* Good performance. Trained with this implementation, yolov2 has a mAP of **77.6%**(544x544), yolov3 has a mAP of **79.6%**(544x544) on Pascal VOC2007 Test.
* Train as fast as darknet
* A lot of efficient backbones on hand, like **tiny yolov2**, **tiny yolov3**, mobilenet, mobilenetv2, shufflenet(g2), shufflenetv2(1x), squeezenext(1.0-SqNxt-23v5), light xception, xception etc. When **trained with this implementation** **from scratch**, tiny yolov2 can get a mAP of **57.5%**(416x416), tiny yolov3 can ge a mAP of **61.3%**(416x416)

Credit:

I got a lot of code from [lightnet](https://gitlab.com/EAVISE/lightnet) whose code is really beautiful",12,1,False,self,,,,,
406,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,14,ae3jjw,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/ae3jjw/global_machine_learning_market_size_outlook/,srikanth1020,1547010728,,0,1,False,default,,,,,
407,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,14,ae3u0z,arxiv.org,[R] FIGR: Few-shot Image Generation with Reptile,https://www.reddit.com/r/MachineLearning/comments/ae3u0z/r_figr_fewshot_image_generation_with_reptile/,hardmaru,1547012997,,2,1,False,default,,,,,
408,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,14,ae3vzm,self.MachineLearning,Why isn't inception score or FID considered for VAEs?,https://www.reddit.com/r/MachineLearning/comments/ae3vzm/why_isnt_inception_score_or_fid_considered_for/,OrganicTowel_,1547013435,[removed],0,1,False,self,,,,,
409,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,15,ae483e,forbes.com,How Machine Learning and Big Data complement each other to have an influence on our lives?,https://www.reddit.com/r/MachineLearning/comments/ae483e/how_machine_learning_and_big_data_complement_each/,gaurav0120,1547016242,,0,1,False,default,,,,,
410,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,15,ae49av,kiwibox.com,Know The Detail About Cattabriga Ice Cream Pasteuriser,https://www.reddit.com/r/MachineLearning/comments/ae49av/know_the_detail_about_cattabriga_ice_cream/,mariamthomas147,1547016540,,0,1,False,default,,,,,
411,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,16,ae4eqg,self.MachineLearning,Machine Learning with WEKA. How to use Strings?,https://www.reddit.com/r/MachineLearning/comments/ae4eqg/machine_learning_with_weka_how_to_use_strings/,JoseChovi,1547017857,"Hi everyone!

I'm trying new software to use in Machine Learning and WEKA looks interesant. I'm using it with Java and this let me integrate this framework in my project but I am having some problems with the Strings and Arrays.

I have a schema of data like this:

    |-- IDUser: string (nullable = false)
    |-- Institution: string (nullable = false)
    |-- Register: string (nullable = false)
    |-- ECData: array (nullable = false)
        |-- element: double (containsNull = true)
    |-- Temp: float (nullable = false)
    |-- Info: float (nullable = false)

How can I fit this data in the head of the ARFF file? I don't know how to fit Strings and Arrays, I've seen the documentation of WEKA and still don't know how to do it.

    pw.println(""% Weka File with the data"");
    pw.println(""@RELATION hospital"");
    pw.println(""@ATTRIBUTE IDUser ????????????"");
    pw.println(""@ATTRIBUTE Institution ????????????"");"");
    pw.println(""@ATTRIBUTE Register ????????????"");"");
    pw.println(""@ATTRIBUTE ECData ????????????"");"");
    pw.println(""@ATTRIBUTE Temp numeric"");
    pw.println(""@ATTRIBUTE Info numeric"");
    pw.println(""@DATA"");

I will appreciate your help so much guys.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
412,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,16,ae4l0p,medicalxpress.com,How does the brain learn by talking to itself?,https://www.reddit.com/r/MachineLearning/comments/ae4l0p/how_does_the_brain_learn_by_talking_to_itself/,ai-lover,1547019555,,0,1,False,default,,,,,
413,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,16,ae4moi,self.MachineLearning,Machine Learning with WEKA. How to use Arrays in Head ARFF File?,https://www.reddit.com/r/MachineLearning/comments/ae4moi/machine_learning_with_weka_how_to_use_arrays_in/,JoseChovi,1547020001,[removed],0,1,False,self,,,,,
414,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,17,ae4zlr,self.MachineLearning,Want to practice ML in python! Need advice !!,https://www.reddit.com/r/MachineLearning/comments/ae4zlr/want_to_practice_ml_in_python_need_advice/,fireredshrimp,1547023689,[removed],0,1,False,self,,,,,
415,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,18,ae53cm,self.MachineLearning,Why do you need a GPU for Machine learning,https://www.reddit.com/r/MachineLearning/comments/ae53cm/why_do_you_need_a_gpu_for_machine_learning/,Humble_Transition,1547024800,[removed],0,1,False,self,,,,,
416,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,18,ae53dj,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/ae53dj/global_machine_learning_market_size_outlook/,stprulingothers,1547024808,[removed],0,1,False,self,,,,,
417,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,18,ae549b,self.MachineLearning,What are the hottest topics in Statistical learning for 2018-2019?,https://www.reddit.com/r/MachineLearning/comments/ae549b/what_are_the_hottest_topics_in_statistical/,Latentis,1547025064,[removed],0,1,False,self,,,,,
418,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,18,ae56tj,self.MachineLearning,What's the state of Machine Learning in newtwork monitoring and analysis?,https://www.reddit.com/r/MachineLearning/comments/ae56tj/whats_the_state_of_machine_learning_in_newtwork/,fedetask,1547025818,[removed],0,1,False,self,,,,,
419,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,18,ae5cul,youtube.com,Demo: Automatic image captioning with visual attention using PyTorch,https://www.reddit.com/r/MachineLearning/comments/ae5cul/demo_automatic_image_captioning_with_visual/,ahmedbesbes,1547027500,,0,1,False,https://b.thumbs.redditmedia.com/MMLBkJB9jgcDCGr2IWdzPF0aLVnkSH3GohhPeda_qdk.jpg,,,,,
420,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,18,ae5e2a,self.MachineLearning,Converting PDF to searchable format and search for a specific keyword and output all of the values which are associated with that keyword,https://www.reddit.com/r/MachineLearning/comments/ae5e2a/converting_pdf_to_searchable_format_and_search/,smmrr,1547027813,[removed],0,1,False,self,,,,,
421,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,19,ae5j7w,self.MachineLearning,Train embedding layer or use pretrained embeddings (e.g.: glove) on a problem (e.g.: babi dataset)?,https://www.reddit.com/r/MachineLearning/comments/ae5j7w/train_embedding_layer_or_use_pretrained/,acobobby,1547029155,[removed],0,1,False,self,,,,,
422,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,19,ae5pky,self.MachineLearning,[D] Text image super-resolution: prerequisites for understanding,https://www.reddit.com/r/MachineLearning/comments/ae5pky/d_text_image_superresolution_prerequisites_for/,dondraper36,1547030851,"Hello!
I am interested in text image super-resolution. I do not know, however, what are the prerequisites necessary to understand papers on the subject. For example, is it enough to pass CS229 and CS231n?

Can you also please recommend a few must-read and fundamental articles and papers to understand the basics? I have skimmed through the ocean of papers on super-resolution available at arxiv.org, but unfortunately, I am not yet able to pick the 'right' articles. 

Thank you! ",0,1,False,self,,,,,
423,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,19,ae5rjf,medium.com,Understanding User Interest through Emotions,https://www.reddit.com/r/MachineLearning/comments/ae5rjf/understanding_user_interest_through_emotions/,omarsar,1547031410,,0,1,False,default,,,,,
424,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,19,ae5rwo,self.MachineLearning,[P] I created an online Hyperparameters Optimization platform and I need feedback,https://www.reddit.com/r/MachineLearning/comments/ae5rwo/p_i_created_an_online_hyperparameters/,iBoB46,1547031508,"The goal here is to give to the ML community an **open** **access**, **asynchronous**, yet **powerful**, hyperparameters optimization tool: [https://bender.dreem.com/](https://bender.dreem.com/)

The project is based on our open source optimization library (for now based on [TPE-like](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)): [https://github.com/Dreem-Organization/benderopt](https://github.com/Dreem-Organization/benderopt)

And you can interact with this library with a whole ecosystem of clients : 

* A **web client** : directly on bender's website, you can visualize the optimization process on nice graphs; and compare the performances of different models on the same problem with a ranking board that ultimately allows you to pick the best model with the best hyperparameters set.
* A **python** one, a **R** one : it allows you to get automatic suggestions of hyperparameters set to test within your code.

Everything is documented on this readthedocs : [https://bender-optimizer.readthedocs.io/en/latest/](https://bender-optimizer.readthedocs.io/en/latest/)

&amp;#x200B;

Now just a short visual example with an algorithm that is just trying to minimize the sinus function using Bender.

*You can see the python code at the end of this post*.

[We can visualize the trials evolution](https://i.redd.it/qmlzwvpihd921.png)

[And we can also visualize the repartition of these trials](https://i.redd.it/rbdrhfgjid921.png)

**Here is the code behind it that generated all these 50 trials.**

    from benderclient import Bender
    import numpy as np
    
    # Initialize Bender
    bender = Bender()
    
    # Create an experiment
    bender.create_experiment(
        name='Minimum example experiment',
        description='Find minimum of sinus function',
        metrics=[{""metric_name"": ""sinus_value"", ""type"": ""loss""}],
    )
    
    # Create an algo (here the sinus function with one parameter)
    bender.create_algo(
        name='Analytic sinus function',
        hyperparameters=[
            {
                ""name"": 'x',
                ""category"": ""uniform"",
                ""search_space"": {
                    ""low"": 0,
                    ""high"": 2 * np.pi,
                },
            },
        ]
    )
    
    # Ask bender for values to try
    for _ in range(50):
        # Get a set of Hyperparameters to test
        suggestion = bender.suggest(metric=""sinus_value"", optimizer=""parzen_estimator"")
    
        # Run the sinus function
        sinus_value = np.sin(suggestion[""x""])
    
        # Feed Bender a trial
        bender.create_trial(
            hyperparameters=suggestion,
            results={""sinus_value"": sinus_value}
        )
        print(""x: "", suggestion[""x""], "" value :"", sinus_value)

**BONUS TIME !** For the ones that took the time to read till the end : here is an animation to visualize the performances of our library.

https://i.redd.it/jlhfrqm8jd921.gif",38,1,False,https://b.thumbs.redditmedia.com/kwY5S_BxdE4gTNYkwWejdLeU_FLWaN0qNhUk_9HKmNM.jpg,,,,,
425,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,20,ae5t9y,reddit.com,Scaling machine learning in the cloud with Kubernetes and Kubeflow,https://www.reddit.com/r/MachineLearning/comments/ae5t9y/scaling_machine_learning_in_the_cloud_with/,SoulmanIqbal,1547031849,,0,1,False,default,,,,,
426,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,20,ae5y4q,itworld.com,Machine learning with Python: An introduction,https://www.reddit.com/r/MachineLearning/comments/ae5y4q/machine_learning_with_python_an_introduction/,hiren_p,1547033121,,0,1,False,default,,,,,
427,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,21,ae6dsp,github.com,Python-Awesome tutorial for Machine Learning as part of a Graduate Program in Machine Learning. Pull requests/changes/stars would be very much helpful.,https://www.reddit.com/r/MachineLearning/comments/ae6dsp/pythonawesome_tutorial_for_machine_learning_as/,gautamrbharadwaj,1547037085,,0,1,False,default,,,,,
428,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,21,ae6kpm,market-report-insight.blogspot.com,Startups are witnessing remarkable growth by offering customer-specific and customized solutions in global Video Encoder Market,https://www.reddit.com/r/MachineLearning/comments/ae6kpm/startups_are_witnessing_remarkable_growth_by/,prashantvi,1547038645,,0,1,False,default,,,,,
429,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,22,ae6nwa,self.MachineLearning,Feature Selection before running a model,https://www.reddit.com/r/MachineLearning/comments/ae6nwa/feature_selection_before_running_a_model/,nishtripathi90,1547039298,[removed],0,1,False,self,,,,,
430,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,22,ae6sn0,self.MachineLearning,"What does ""understanding a word"" really mean for a machine with NLP capabilities?",https://www.reddit.com/r/MachineLearning/comments/ae6sn0/what_does_understanding_a_word_really_mean_for_a/,yoginkginfra,1547040306,[removed],0,1,False,self,,,,,
431,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,22,ae7153,self.MachineLearning,SoftHash! - A hashing library for subjective digital neural networks,https://www.reddit.com/r/MachineLearning/comments/ae7153/softhash_a_hashing_library_for_subjective_digital/,rnreich,1547042055,[removed],0,1,False,https://a.thumbs.redditmedia.com/b00r5T0rY_qAgwZ8zNE2_cNwH53xQf7ndHaXCDeo6D8.jpg,,,,,
432,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae73af,deepsense.ai,"[N] Artificial intelligence and music, a new GAN standard and fighting depression",https://www.reddit.com/r/MachineLearning/comments/ae73af/n_artificial_intelligence_and_music_a_new_gan/,AnnaKow,1547042478,,0,1,False,default,,,,,
433,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae75e3,self.MachineLearning,How much machine learning knowledge I need in order to get decent rank on Kaggle,https://www.reddit.com/r/MachineLearning/comments/ae75e3/how_much_machine_learning_knowledge_i_need_in/,MCSdude,1547042878,[removed],0,1,False,self,,,,,
434,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae78r3,self.MachineLearning,Where could I find all the statistic knowledge needed in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/ae78r3/where_could_i_find_all_the_statistic_knowledge/,jasonjente,1547043546,[removed],0,1,False,self,,,,,
435,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae78wr,deeplearningio.com,Neural Networks and Deep Learning  I | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/ae78wr/neural_networks_and_deep_learning_i_deep_learning/,mrcgllr,1547043575,,0,1,False,default,,,,,
436,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae7bg4,self.MachineLearning,[D] How to Perform Fraud Detection with Personalized Page Rank?,https://www.reddit.com/r/MachineLearning/comments/ae7bg4/d_how_to_perform_fraud_detection_with/,antmoreau,1547044072,"What about fighting fraud detection with graph analysis?  
I just wrote [this article](https://blog.sicara.com/fraud-detection-personalized-page-rank-networkx-15bd52ba2bf6) about using and personalized page rank to detect rare events like fraud.  


What do you think of it? I would love to have some feedback. Thanks!",4,1,False,self,,,,,
437,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae7c4l,github.com,[P] Machine Learning Knowledge Graph: organizing concepts related to ML and AI,https://www.reddit.com/r/MachineLearning/comments/ae7c4l/p_machine_learning_knowledge_graph_organizing/,lbmineti,1547044208,,0,1,False,https://b.thumbs.redditmedia.com/G1teWyls0a229WqgPKbHNzJd61vxsI9giFUpRSpZvPc.jpg,,,,,
438,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae7cvh,self.MachineLearning,"Percentage of Bachelors degrees conferred to women, by major (1970-2012)",https://www.reddit.com/r/MachineLearning/comments/ae7cvh/percentage_of_bachelors_degrees_conferred_to/,andrea_manero,1547044349,[removed],0,1,False,self,,,,,
439,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae7dpw,self.MachineLearning,Resources for Algebraic Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ae7dpw/resources_for_algebraic_machine_learning/,freechoice,1547044513,[removed],0,1,False,self,,,,,
440,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae7iem,self.MachineLearning,[R] Identifying facial phenotypes of genetic disorders using deep learning,https://www.reddit.com/r/MachineLearning/comments/ae7iem/r_identifying_facial_phenotypes_of_genetic/,hooba_stank_,1547045395,"News:

https://www.genengnews.com/insights/a-i-gets-in-the-face-of-rare-genetic-diseases/

Paper:

https://www.nature.com/articles/s41591-018-0279-0?_ga=2.117471369.1451731699.1547045119-254776392.1547045119

Abstract:

Syndromic genetic conditions, in aggregate, affect 8% of the population. Many syndromes have recognizable facial features that are highly informative to clinical geneticists. Recent studies show that facial analysis technologies measured up to the capabilities of expert clinicians in syndrome identification. However, these technologies identified only a few disease phenotypes, limiting their role in clinical settings, where hundreds of diagnoses must be considered. Here we present a facial image analysis framework, DeepGestalt, using computer vision and deep-learning algorithms, that quantifies similarities to hundreds of syndromes. DeepGestalt outperformed clinicians in three initial experiments, two with the goal of distinguishing subjects with a target syndrome from other syndromes, and one of separating different genetic subtypes in Noonan syndrome. On the final experiment reflecting a real clinical setting problem, DeepGestalt achieved 91% top-10 accuracy in identifying the correct syndrome on 502 different images. The model was trained on a dataset of over 17,000 images representing more than 200 syndromes, curated through a community-driven phenotyping platform. DeepGestalt potentially adds considerable value to phenotypic evaluations in clinical genetics, genetic testing, research and precision medicine.

Apps:

https://www.face2gene.com/startclinic/",14,1,False,self,,,,,
441,MachineLearning,t5_2r3gv,2019-1-9,2019,1,9,23,ae7jfl,yanpanlau.github.io,Using Keras and Deep Deterministic Policy Gradient to play TORCS,https://www.reddit.com/r/MachineLearning/comments/ae7jfl/using_keras_and_deep_deterministic_policy/,ihaphleas,1547045600,,0,1,False,default,,,,,
442,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,0,ae7nd7,self.MachineLearning,"[P] Building Thousands of Reproducible ML Models with pipe, the Automattic Machine Learning Pipeline",https://www.reddit.com/r/MachineLearning/comments/ae7nd7/p_building_thousands_of_reproducible_ml_models/,datadem,1547046324,"I am a data scientist working for Automattic, the remote company behind products like WordPress.com, Jetpack, and Akismet. [This post](https://data.blog/2019/01/08/building-thousands-of-reproducible-ml-models-with-pipe-the-automattic-machine-learning-pipeline/) - as part of a series of posts around this issue - describes the inner workings of the Machine Learning pipeline we have been working on with a coworker of mine for the past year and makes a case for in-house ML pipelines. 

Also feel free to ask me about remote work as a data scientist!",8,1,False,self,,,,,
443,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,0,ae7qa4,self.MachineLearning,"Do CNNs generalise to handle background textures, when they have been trained Ona dataset with white backgrounds only?",https://www.reddit.com/r/MachineLearning/comments/ae7qa4/do_cnns_generalise_to_handle_background_textures/,gautiexe,1547046829,[removed],0,1,False,self,,,,,
444,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,0,ae7x6u,self.MachineLearning,Tier-2 Machine Learning Conferences/Journals?,https://www.reddit.com/r/MachineLearning/comments/ae7x6u/tier2_machine_learning_conferencesjournals/,OhThatLooksCool,1547048037,[removed],0,1,False,self,,,,,
445,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,0,ae83r9,self.MachineLearning,"Simple Questions Thread January 09, 2019",https://www.reddit.com/r/MachineLearning/comments/ae83r9/simple_questions_thread_january_09_2019/,AutoModerator,1547049166,[removed],0,1,False,self,,,,,
446,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,0,ae865j,youtube.com,[D] Whats in a name? The need to nip NIPS,https://www.reddit.com/r/MachineLearning/comments/ae865j/d_whats_in_a_name_the_need_to_nip_nips/,kirasolo,1547049565,,0,1,False,https://b.thumbs.redditmedia.com/xUHiI77gTizXZtNDud5zJpfwYf1HmVNunzJl9tcHwcI.jpg,,,,,
447,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,1,ae8l0l,kanoki.org,How to Optimize Pandas code,https://www.reddit.com/r/MachineLearning/comments/ae8l0l/how_to_optimize_pandas_code/,min2bro,1547052040,,0,1,False,https://b.thumbs.redditmedia.com/_F0P7qRl-aYI6PF6DBnK6Sa7ufru2jOeveQg8igIIKk.jpg,,,,,
448,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,2,ae8u95,technologyreview.com,This algorithm browses Wikipedia to auto-generate textbooks using ML,https://www.reddit.com/r/MachineLearning/comments/ae8u95/this_algorithm_browses_wikipedia_to_autogenerate/,ai-lover,1547053492,,0,1,False,default,,,,,
449,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,2,ae92v9,self.MachineLearning,[R] How to measure generalization of models on new datasets without labeling new examples -- call for ideas,https://www.reddit.com/r/MachineLearning/comments/ae92v9/r_how_to_measure_generalization_of_models_on_new/,pigdogsheep,1547054880,"tldr; If I have a trained model on labels from a specific domain, that I want to use on non-labeled examples from another domain. How can I measure the generalization ability of the pretrained model on this new dataset, without labeling any new exampled in domain 2.   


I have been looking for lines of work that consider generating whitebox adversarial examples on the trained model. 

* [https://aclweb.org/anthology/P18-2006](https://aclweb.org/anthology/P18-2006)
* [https://openreview.net/pdf?id=r1X3g2\_xl](https://openreview.net/pdf?id=r1X3g2_xl) 

The idea roughly I have in mind, is basically finding the noise distribution that can break the classifier. Then comparing this distribution to the distributions of both domains (old and new) and find which one is closer.   
I was wondering if this is a good idea. Or if there could be another line of work that discusses this idea in depth. ",9,1,False,self,,,,,
450,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,2,ae93oa,solutionfactory.in,[Research] Paying Money with Face. Thats Excellent,https://www.reddit.com/r/MachineLearning/comments/ae93oa/research_paying_money_with_face_thats_excellent/,Shadabkazi03,1547055011,,0,1,False,https://b.thumbs.redditmedia.com/eZkY1QyFdZFKuq5itYbMIyfgx-EMcPB3zDeLDWkHu2o.jpg,,,,,
451,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,3,ae9hvw,blog.acolyer.org,"[D] ""Neural Ordinary Differential Equations""",https://www.reddit.com/r/MachineLearning/comments/ae9hvw/d_neural_ordinary_differential_equations/,gwern,1547057259,,0,1,False,default,,,,,
452,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,3,ae9iwe,deeplearningio.com,Neural Networks and Deep Learning- II : Binary Classification | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/ae9iwe/neural_networks_and_deep_learning_ii_binary/,mrcgllr,1547057423,,0,1,False,default,,,,,
453,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,4,aea3q6,self.MachineLearning,"[Question] Assume that we have trained a SVM for a large data set using a linear kernel. There is however a constraint that the maximum number of support vectors that can be used in actual production system is k, where as the support vectors in the learned model is (m &gt;&gt; k). What will you do ?",https://www.reddit.com/r/MachineLearning/comments/aea3q6/question_assume_that_we_have_trained_a_svm_for_a/,phoenixlads,1547060755,[removed],0,1,False,self,,,,,
454,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,4,aeak9y,i.redd.it,Generating Hand written digits (Number 8 in my case ) using GAN .,https://www.reddit.com/r/MachineLearning/comments/aeak9y/generating_hand_written_digits_number_8_in_my/,charan_1996,1547063415,,0,1,False,https://b.thumbs.redditmedia.com/xBgfQP-fPbebV13DOSbUCFMB7w2vtNwPKX-hcNkcGRw.jpg,,,,,
455,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,5,aeaqe1,self.MachineLearning,Self Organizing Fuzzy Neural Networks,https://www.reddit.com/r/MachineLearning/comments/aeaqe1/self_organizing_fuzzy_neural_networks/,kringata,1547064390,[removed],0,1,False,self,,,,,
456,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,5,aeas69,ncbi.nlm.nih.gov,Real Time Influenza Monitoring Using Hospital Big Data in Combination with Machine Learning Methods: Comparison Study. [Research Paper],https://www.reddit.com/r/MachineLearning/comments/aeas69/real_time_influenza_monitoring_using_hospital_big/,ai-lover,1547064681,,0,1,False,default,,,,,
457,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,5,aeaw20,ncbi.nlm.nih.gov,Real Time Influenza Monitoring Using Hospital Big Data in Combination with Machine Learning Methods: Comparison Study. [R],https://www.reddit.com/r/MachineLearning/comments/aeaw20/real_time_influenza_monitoring_using_hospital_big/,ai-lover,1547065289,,0,1,False,https://b.thumbs.redditmedia.com/g9oKPfuHmmQi2a91IWEhiB_fKnWw08JRi3KB-qcKTec.jpg,,,,,
458,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,6,aebk5s,self.MachineLearning,[P] Dataset for Card Game,https://www.reddit.com/r/MachineLearning/comments/aebk5s/p_dataset_for_card_game/,smellslikedonkey,1547069138,"I'm the creator of a web and app based card game called Troika (see [troikagame.com](https://troikagame.com)). It's a game with simple rules but surprisingly deep gameplay that is played religiously in western Canada.

Right now it has a hand coded AI which has been working well. However, one of my intentions eventually is to apply ML techniques to create a better, more human-like AI to replace it.

I'm not looking for free labor or anything (I'll get to it myself eventually)  but I figured this might be a good project if someone wants to play with it. So I have made the current hand coded AI and a bunch of gameplay data available at [https://github.com/smellslikedonkey/troikajs\_public](https://github.com/smellslikedonkey/troikajs_public) with no restrictions whatsoever. I'll periodically keep updating the dataset as more games are played.

The game rules for Troika can be found at [http://troikagamerules.com/](http://troikagamerules.com/)

Enjoy!

&amp;#x200B;

&amp;#x200B;",8,1,False,self,,,,,
459,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,6,aeblvp,nature.com,"Deep learning algorithm is to help identify physical characteristics of rare genetic disorders e.g, Cornelia de lange syndrome.",https://www.reddit.com/r/MachineLearning/comments/aeblvp/deep_learning_algorithm_is_to_help_identify/,MistWeaver80,1547069419,,0,1,False,default,,,,,
460,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,6,aebot4,self.MachineLearning,"Making a Trip Planner with ML, is it worth it to start?",https://www.reddit.com/r/MachineLearning/comments/aebot4/making_a_trip_planner_with_ml_is_it_worth_it_to/,har_r,1547069883,[removed],0,1,False,self,,,,,
461,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,6,aebtct,self.MachineLearning,Need help understanding the C Parameter in Support Vector Machines,https://www.reddit.com/r/MachineLearning/comments/aebtct/need_help_understanding_the_c_parameter_in/,99problemsallops,1547070608,[removed],0,1,False,self,,,,,
462,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,7,aeccbb,learnk8s.io,Scaling machine learning in the cloud with Kubernetes and Kubeflow,https://www.reddit.com/r/MachineLearning/comments/aeccbb/scaling_machine_learning_in_the_cloud_with/,SoulmanIqbal,1547073675,,0,1,False,https://b.thumbs.redditmedia.com/jgOGWlTgwI7r5ri-1dsBkMVTU5ZwJPPRWy2oF-hrdxE.jpg,,,,,
463,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,7,aechc0,self.MachineLearning,Team up for a data visualizing and creative storytelling capabilities of analytics tools,https://www.reddit.com/r/MachineLearning/comments/aechc0/team_up_for_a_data_visualizing_and_creative/,elena_eng,1547074519,"There is Data Visualization Student Challenge is going on. I want to find one or two partners with a good background in Data analyzing.
They want us to showcase our visualization brilliance. 
This competition will help us to learn more about data visualization and the creative storytelling capabilities of analytics tools.
Please make sure that you are a student from a Canadian university(requirement). 
It is so competitive.
Please let me know If you have a good background and interested in it. The deadline is coming up.",0,1,False,self,,,,,
464,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,9,aed9i5,startcrowd.club,AI for Drug Discovery: Hype or Hope? Free Events across America,https://www.reddit.com/r/MachineLearning/comments/aed9i5/ai_for_drug_discovery_hype_or_hope_free_events/,mostafabenh,1547079390,,0,1,False,default,,,,,
465,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,9,aedijn,self.MachineLearning,[P] I wrote a two part blog on object detection using YOLO. Any suggestions/corrections are welcome.,https://www.reddit.com/r/MachineLearning/comments/aedijn/p_i_wrote_a_two_part_blog_on_object_detection/,iampratheesh,1547081067,"Here's the link for part 1
https://medium.com/@pratheesh.27998/object-detection-part1-4dbe5147ad0a

And part 2
https://medium.com/@pratheesh.27998/object-detection-part1-4dbe5147ad0a",2,1,False,self,,,,,
466,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,10,aedry7,self.MachineLearning,[Discussion] Researchers who write multiple high quality papers a year: how do you do it?,https://www.reddit.com/r/MachineLearning/comments/aedry7/discussion_researchers_who_write_multiple_high/,GuiltyResearcher,1547082772,"I've seen some researchers publish many top conference papers a year. Some of these people also consistently have very interesting ideas. Apart from the obvious ""genius"", is there anything that particularly nurtures such a practise?",77,1,False,self,,,,,
467,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,10,aeduq6,self.MachineLearning,"PVC Wall Panel Line, PVC Ceiling Panel Line, PVC Foamed Panel Extrusion Line, PVC Rooftile Production Line, PVC Wall Panel Making Machine Manufacturer",https://www.reddit.com/r/MachineLearning/comments/aeduq6/pvc_wall_panel_line_pvc_ceiling_panel_line_pvc/,tigermach,1547083283,[removed],0,1,False,self,,,,,
468,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,11,aee7jz,kanoki.org,How to Optimize Pandas code,https://www.reddit.com/r/MachineLearning/comments/aee7jz/how_to_optimize_pandas_code/,min2bro,1547085657,,0,1,False,default,,,,,
469,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,11,aeecpl,self.MachineLearning,"[D] Continuity between ""layers"" in Neural Differential Equations",https://www.reddit.com/r/MachineLearning/comments/aeecpl/d_continuity_between_layers_in_neural/,CSartistInTraining,1547086655,"Neural differential equations replace the idea of sequential layers with sequential updates to an ODE solver. This is a cool idea, but one thing I haven't seen mentioned is that ODE solvers require that solutions are continuous (indeed, most require even stronger smoothness of solutions). 

&amp;#x200B;

This would imply that consecutive hidden representations are similar to each other in a neural differential equation. This assumption is not made by standard neural networks. Indeed, a hidden layer's activations are typically not similar at all to the previous layer's activations. Does this implicit assumption of continuity put any constraints or limitations on what neural differential equations can do? 

&amp;#x200B;

I haven't seen this issue discussed anywhere, so maybe I'm missing something fundamental that resolves it. ",11,1,False,self,,,,,
470,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,11,aeekj7,self.MachineLearning,Efficient adversarial attack based on Trust Region for Pytorch model,https://www.reddit.com/r/MachineLearning/comments/aeekj7/efficient_adversarial_attack_based_on_trust/,zheweiyao,1547088148,[removed],0,1,False,self,,,,,
471,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,11,aeentt,self.MachineLearning,[Project] Efficient adversarial attack based on Trust Region for Pytorch model,https://www.reddit.com/r/MachineLearning/comments/aeentt/project_efficient_adversarial_attack_based_on/,zheweiyao,1547088775,"[https://github.com/amirgholami/TRAttack](https://github.com/amirgholami/TRAttack)

Lately, we have been working on adversarial attack (robust training) for training neural networks. However, most of the strong attacks are very slow (for instance CW attack), to generate adversarial examples. For this reason, it is computationally very expensive to use them during training phase. We have recently developed a new attack using trust region method, which itself is a very well studied method in scientific computing. It is a fast and efficient white-box attack that is also strong! We have open source and hopefully, it will be useful.",0,1,False,self,,,,,
472,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,13,aefgaj,self.MachineLearning,Invert digamma function,https://www.reddit.com/r/MachineLearning/comments/aefgaj/invert_digamma_function/,cuenta4384,1547094444,"In this report Minka shows how to invert the digamma function to get the parameter value (appendix C). For instance, \digamma(x)^-1 = x. But does anybody know, how to invert when you have the following expression \digamma(x) + \digamma(x+y). Can the same Newton method be applied? 



",0,1,False,self,,,,,
473,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,13,aefjni,arxiv.org,[R] Hyperprior Induced Unsupervised Disentanglement of Latent Representations,https://www.reddit.com/r/MachineLearning/comments/aefjni/r_hyperprior_induced_unsupervised_disentanglement/,afatir,1547095145,,4,1,False,default,,,,,
474,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,14,aefq55,self.MachineLearning,Scikit learn and its pratical use?,https://www.reddit.com/r/MachineLearning/comments/aefq55/scikit_learn_and_its_pratical_use/,MCSdude,1547096518,[removed],0,1,False,self,,,,,
475,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,15,aegbg1,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/aegbg1/global_machine_learning_market_size_outlook/,lashya_envi,1547101291,[removed],0,1,False,self,,,,,
476,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,15,aegbub,self.MachineLearning,Help in saving/restoring tensorflow models,https://www.reddit.com/r/MachineLearning/comments/aegbub/help_in_savingrestoring_tensorflow_models/,ItIsOkSuraj,1547101399,[removed],0,1,False,self,,,,,
477,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,16,aegufn,arxiv.org,"[1806.10692] ActiveRemediation: The Search for Lead Pipes in Flint, Michigan",https://www.reddit.com/r/MachineLearning/comments/aegufn/180610692_activeremediation_the_search_for_lead/,sugarhilldt2,1547105947,,2,1,False,default,,,,,
478,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,16,aegw6n,arxiv.org,"[R] ActiveRemediation: The Search for Lead Pipes in Flint, Michigan",https://www.reddit.com/r/MachineLearning/comments/aegw6n/r_activeremediation_the_search_for_lead_pipes_in/,sugarhilldt2,1547106399,,3,1,False,default,,,,,
479,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,17,aeh6t4,self.MachineLearning,Best Major to get a job in ML/AI?,https://www.reddit.com/r/MachineLearning/comments/aeh6t4/best_major_to_get_a_job_in_mlai/,ShadowViking47,1547109383,[removed],0,1,False,self,,,,,
480,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,17,aeh82a,self.MachineLearning,Where to start ?,https://www.reddit.com/r/MachineLearning/comments/aeh82a/where_to_start/,d1ckj0nes,1547109770,[removed],0,1,False,self,,,,,
481,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,18,aehgpm,self.MachineLearning,Sidekit Audio ML,https://www.reddit.com/r/MachineLearning/comments/aehgpm/sidekit_audio_ml/,deveid,1547112301,[removed],0,1,False,self,,,,,
482,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,19,aehxps,youtube.com,Beautiful dark squared creative play | Leela reacts brilliantly to Dutch...,https://www.reddit.com/r/MachineLearning/comments/aehxps/beautiful_dark_squared_creative_play_leela_reacts/,kingscrusher-youtube,1547117097,,0,1,False,default,,,,,
483,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,20,aei3r0,self.MachineLearning,"Global Machine Learning Market - Size, Outlook, Trends and Forecasts",https://www.reddit.com/r/MachineLearning/comments/aei3r0/global_machine_learning_market_size_outlook/,Royalnfield,1547118726,[removed],0,1,False,self,,,,,
484,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,20,aeid6j,i.redd.it,"Well, I Cried.",https://www.reddit.com/r/MachineLearning/comments/aeid6j/well_i_cried/,skoopski_potato,1547120981,,0,1,False,default,,,,,
485,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,21,aeih6f,self.MachineLearning,How to fit circle on set of points.,https://www.reddit.com/r/MachineLearning/comments/aeih6f/how_to_fit_circle_on_set_of_points/,gireeshwaran,1547121864,[removed],0,1,False,self,,,,,
486,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,21,aeik40,marketwatch.com,Machine Translation Market to expand USD 1.5 billion by 2024,https://www.reddit.com/r/MachineLearning/comments/aeik40/machine_translation_market_to_expand_usd_15/,robertserry,1547122434,,0,1,True,nsfw,,,,,
487,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,21,aeikaf,deeplearningio.com,How to Start Deep Learning and Artificial Intelligence | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aeikaf/how_to_start_deep_learning_and_artificial/,mrcgllr,1547122471,,0,1,False,default,,,,,
488,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,21,aeioby,arxiv.org,[1901.02671] Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks,https://www.reddit.com/r/MachineLearning/comments/aeioby/190102671_is_it_time_to_swish_comparing_deep/,ihaphleas,1547123280,,10,1,False,default,,,,,
489,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,21,aeir3m,self.MachineLearning,[R] On Relativistic f-divergences,https://www.reddit.com/r/MachineLearning/comments/aeir3m/r_on_relativistic_fdivergences/,AlexiaJM,1547123883,"Blog post: https://ajolicoeur.wordpress.com/relativistic-f-divergences/
Paper: https://arxiv.org/abs/1901.02474
GitHub: https://github.com/AlexiaJM/relativistic-f-divergences

The main contributions are:

 We prove that the objective functions of the discriminator in Relativistic GANs are divergences (Relativistic f-divergences).

 We devise a few variants of Relativistic f-divergences.

 We show that the Wasserstein Distance is weaker than f-divergences which are weaker than relativistic f-divergences.

 We present the minimum-variance unbiased estimator (MVUE) of Relativistic paired GANs (RpGANs) and show that using it hinders the performance of the generator.

 We show that Relativistic average GANs (RaGANs) are only asymptotically unbiased, but that the finite-sample bias is small. Removing this bias does not improve the performance of the generator.",5,1,False,self,,,,,
490,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,21,aeisyk,oodlestechnologies.com,How To Improve Cloud ERP With AI And Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aeisyk/how_to_improve_cloud_erp_with_ai_and_machine/,oodlestechnologies,1547124292,,0,1,False,default,,,,,
491,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,21,aeivjk,self.MachineLearning,What are the Best Examples from each Domain - Sample Code and Training Data,https://www.reddit.com/r/MachineLearning/comments/aeivjk/what_are_the_best_examples_from_each_domain/,FontofFortunes,1547124895,[removed],0,1,False,self,,,,,
492,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,22,aej78i,i.redd.it,[Discussion] You vs the guy she tells you not to worry about,https://www.reddit.com/r/MachineLearning/comments/aej78i/discussion_you_vs_the_guy_she_tells_you_not_to/,aWildTinoAppears,1547127429,,0,1,False,https://a.thumbs.redditmedia.com/vWCR05tjGQvqPG8F4fYOy94rCb56j4r9MyZqEl7k7a0.jpg,,,,,
493,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,22,aej7bx,self.MachineLearning,How to build image document text detection as good as Google cloud vision API?,https://www.reddit.com/r/MachineLearning/comments/aej7bx/how_to_build_image_document_text_detection_as/,DGs29,1547127449,[removed],0,1,False,self,,,,,
494,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,22,aejbz9,knowball.co,The machine learning search engine,https://www.reddit.com/r/MachineLearning/comments/aejbz9/the_machine_learning_search_engine/,thesalsguy,1547128511,,0,1,False,default,,,,,
495,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,23,aejecg,self.MachineLearning,Machine Learning Conference in Italy,https://www.reddit.com/r/MachineLearning/comments/aejecg/machine_learning_conference_in_italy/,croco66,1547129016,[removed],0,1,False,self,,,,,
496,MachineLearning,t5_2r3gv,2019-1-10,2019,1,10,23,aeji33,blog.pocketcluster.io,"[N] Weekly Machine Learning Opensource Roundup  Jan. 10, 2018",https://www.reddit.com/r/MachineLearning/comments/aeji33/n_weekly_machine_learning_opensource_roundup_jan/,stkim1,1547129756,,0,1,False,default,,,,,
497,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,0,aek1mn,youtube.com,Serial Code Capture- Tensorflow OpenCV Python Tesseract,https://www.reddit.com/r/MachineLearning/comments/aek1mn/serial_code_capture_tensorflow_opencv_python/,dhar174,1547134400,,1,1,False,https://b.thumbs.redditmedia.com/jUWUpAjBBTOrdyJpKe_1YokROdPHP6iWcvR9UNWAYAI.jpg,,,,,
498,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,0,aek354,sciencedirect.com,Application of text mining on social media platforms to detect and assess noise complaints and noisy events in urban environments,https://www.reddit.com/r/MachineLearning/comments/aek354/application_of_text_mining_on_social_media/,luisgasco,1547134663,,0,1,False,default,,,,,
499,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,0,aek3yk,self.MachineLearning,[P] Learning related subreddits to /r/MachineLearning with Jaccard similarity,https://www.reddit.com/r/MachineLearning/comments/aek3yk/p_learning_related_subreddits_to_rmachinelearning/,anvaka,1547134804,"I made a small tool to visualize related subreddits. Here is what it recommends for MachineLearning: https://anvaka.github.io/sayit/?query=MachineLearning

It works by constructing Jaccard similarity from subreddit users. The data comes from August and September of 2018 (full data set can be found [here](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/) - it is updated monthly and stored into BigQuery). This is ~39 million `user &lt;-&gt; subreddit` records

For the popular subreddits with millions of users I [manually entered](https://www.reddit.com/r/dataisbeautiful/comments/ae88pk/interactive_visualization_of_related_subreddits/edox04f) related subreddits. Otherwise Jaccard similarity connects them to each other (all are connected to /r/videos, /r/AskReddit and so on). If you have a recommendation how to successfully modify jaccard similarity for popular subreddits - please let me know!

The graph is constructed by looking at Jaccard similarity. Connection between node `A` is added only if candidate node `B` similarity index is one standard deviation higher than the mean of all `A's` candidates. 

Despite its simplicity I hope you find this practical. ",42,1,False,self,,,,,
500,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,1,aekh56,medium.com,[N] Stanford Open-Sources Neural Network Verification Project,https://www.reddit.com/r/MachineLearning/comments/aekh56/n_stanford_opensources_neural_network/,gwen0927,1547136934,,0,1,False,https://b.thumbs.redditmedia.com/prwkSkCgDZHmX8igd0TyBDkJEuTvPjE4Z0cL5mGx1KA.jpg,,,,,
501,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,1,aekk4i,self.MachineLearning,Variational Capsules review,https://www.reddit.com/r/MachineLearning/comments/aekk4i/variational_capsules_review/,akhandait,1547137439,[removed],0,1,False,self,,,,,
502,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,2,ael4k8,self.MachineLearning,ML application in No Show in appointments,https://www.reddit.com/r/MachineLearning/comments/ael4k8/ml_application_in_no_show_in_appointments/,kumaran101,1547140703,[removed],0,1,False,self,,,,,
503,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,2,ael9vd,self.MachineLearning,ML project to identify xpath.,https://www.reddit.com/r/MachineLearning/comments/ael9vd/ml_project_to_identify_xpath/,Manimegalai93,1547141560,[removed],0,1,False,self,,,,,
504,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,2,aelbi4,self.MachineLearning,[D] Principles for non experts,https://www.reddit.com/r/MachineLearning/comments/aelbi4/d_principles_for_non_experts/,GabrieleFariello,1547141815,"***Discussion:***

As members of an expert community of data science, machine learning, and artificial intelligence practitioners and researchers,

* What are the most important concepts for non-experts to understand?
* What are some good short reads, lectures, or videos on the subject matter?
* What are the seminal or interesting papers that are accessible to non-computer scientists and non-mathematicians?
* Are there any books in particular you would recommend?

From [Eugene Goostman ""winning the Turing Test""](http://time.com/2847900/eugene-goostman-turing-test/) to [Alpha Go 2 learning to beat Alpha Go 1 in no-time by itself](https://www.scientificamerican.com/article/ai-versus-ai-self-taught-alphago-zero-vanquishes-its-predecessor/), there is a lot of sensationalized reporting out there. Celebrity opinions range from cataclysmic warnings from the likes of Elon Musk and the late Stephen Hawking to the more subdued ""It's just math"" from Oren Etzioni, the public gets lots of different signals. What is the most useful knowledge and/or information for people to ""get""?

&amp;#x200B;

***Optional*** ***Background:***

I teach a course at Harvard which is [available](https://online-learning.harvard.edu/course/building-brain-survey-artificial-intelligence) to non-Harvard folks through the Harvard Extension School. Unfortunately, it is not yet available for for free, though I hope to eventually change that. The course is geared primarily toward non-practitioners and non-experts. It was borne several years ago from a growing concern that policy makers and decision makers often had no understanding of the data sciences and, more importantly, the implications of their rapid growth and use.  If you would like to feel uncomfortable about the level of understanding of technology, let alone ML, see the questions posed by the US Congress to Mark Zuckerberg. For those who are interested in the course itself, I will be happy to post more and/or discuss later, but I feel that /r/MachineLearning may not be the right place for it so please suggest where. I doubt people who read and understand most of the postings here will find the course too enlightening.

&amp;#x200B;

P.S. I apologize in advance, but I am slow to reply to PMs and the like.",8,1,False,self,,,,,
505,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,2,aeld72,globalbankingandfinance.com,"Chandra Ambadipudi, CEO at Clairvoyant LLC discusses how artificial intelligence and machine learning are impacting the BFSI industry.",https://www.reddit.com/r/MachineLearning/comments/aeld72/chandra_ambadipudi_ceo_at_clairvoyant_llc/,teamclairvoyant,1547142083,,0,1,False,default,,,,,
506,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,2,aeldeb,self.MachineLearning,Should I be using XGBoost for sports prediction?,https://www.reddit.com/r/MachineLearning/comments/aeldeb/should_i_be_using_xgboost_for_sports_prediction/,elsarcher,1547142110,"Hi Guys, 

I am currently modelling the chances of there being 2.5+ goals in a Football match. I have 88 features that all are logically important and, when analysed separately, seem to significantly effect the response variable. My current predictive model uses the XGBoost algorithm using the binary:logistic objective function. 

&amp;#x200B;

However, unfortunately the model seems to only be focussing on 1 or 2 factors and has deemed the rest of the factors pretty much entirely insignificant. .In addition to this, when I output the probabilities it gives probs of over 90% - which I know to be nonsense. 

&amp;#x200B;

Anyway, I was wondering if; 

&amp;#x200B;

**Anyone has any experience with football prediction using machine learning techniques and perhaps some advice as to what i may be doing wrong. Perhaps this algorithm isn't appropriate in this case and I should be using other techniques?** 

&amp;#x200B;

Thanks guys, I apologise for my lack of technical knowledge. 

&amp;#x200B;",0,1,False,self,,,,,
507,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,2,aelhtf,it.toolbox.com,"Vikram Bhalchandra, Chief Sales &amp; Marketing Officer at Clairvoyant, discusses in his latest blog about how companies need to reconsider their data security strategy.",https://www.reddit.com/r/MachineLearning/comments/aelhtf/vikram_bhalchandra_chief_sales_marketing_officer/,teamclairvoyant,1547142833,,0,1,False,default,,,,,
508,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,3,aelom1,self.MachineLearning,4 Ways Machine Learning Is Shaping the Future of Education,https://www.reddit.com/r/MachineLearning/comments/aelom1/4_ways_machine_learning_is_shaping_the_future_of/,shenron6,1547143897,[removed],0,1,False,self,,,,,
509,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,3,aelrdl,self.MachineLearning,Annotation and localisation :: can we detect multiple objects and locate with bounding box and annotate it all together for a image???Please suggest me algorithms and code/link,https://www.reddit.com/r/MachineLearning/comments/aelrdl/annotation_and_localisation_can_we_detect/,nisucuk,1547144325,[removed],0,1,False,self,,,,,
510,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,3,aem01l,arxiv.org,[1901.02871] The Lingering of Gradients: How to Reuse Gradients over Time,https://www.reddit.com/r/MachineLearning/comments/aem01l/190102871_the_lingering_of_gradients_how_to_reuse/,zhamisen,1547145752,,9,1,False,default,,,,,
511,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,3,aem0k8,blockference.com,AI INNOVATION SUMMIT - Probably the best start of 2019,https://www.reddit.com/r/MachineLearning/comments/aem0k8/ai_innovation_summit_probably_the_best_start_of/,Zukicha,1547145842,,0,1,False,https://b.thumbs.redditmedia.com/aUNrEsEafF40VRvbYpOosRd4iTkG8YvvBPV5qC43Uug.jpg,,,,,
512,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,3,aem203,self.MachineLearning,[D] TensorFlow 2.0 nightly build is up!,https://www.reddit.com/r/MachineLearning/comments/aem203/d_tensorflow_20_nightly_build_is_up/,Inori,1547146068,"As per [this](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/aKdmUOiyzGM) message in TensorFlow Developers mailing list you can now test the features of what will likely go into TensorFlow 2.0. At a quick glance seems to have slightly cleaner API, eager execution a first class citizen, and tighter integration with tf.keras.

You can install it via `pip install tf-nightly-2.0-preview`, though probably a good idea to do it in a separate virtual environment.",55,1,False,self,,,,,
513,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,3,aem2ye,medium.com,[N] Privacy-Preserving Machine Learning 2018: A Year in Review,https://www.reddit.com/r/MachineLearning/comments/aem2ye/n_privacypreserving_machine_learning_2018_a_year/,jvmancuso,1547146222,,0,1,False,https://b.thumbs.redditmedia.com/YuQtSMLDr__XO1rbY7GtjntqagT0Cyplkhjka3dutfM.jpg,,,,,
514,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,4,aembva,self.MachineLearning,Gradient boosting trees and when to use them,https://www.reddit.com/r/MachineLearning/comments/aembva/gradient_boosting_trees_and_when_to_use_them/,orangesmells,1547147669,[removed],0,1,False,self,,,,,
515,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,4,aemezu,self.MachineLearning,"AMD Radeon VII (Navi 7nm) and TensorFlow. Wife is working with Python and TensorFlow, I want to play games, looking for good compromise.",https://www.reddit.com/r/MachineLearning/comments/aemezu/amd_radeon_vii_navi_7nm_and_tensorflow_wife_is/,circular_file,1547148183,[removed],0,1,False,self,,,,,
516,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,4,aemfmf,self.MachineLearning,[D] Gradient boosting trees and when to use them,https://www.reddit.com/r/MachineLearning/comments/aemfmf/d_gradient_boosting_trees_and_when_to_use_them/,orangesmells,1547148289,"Gradient boosting is a popular machine learning technique used on heterogeneous datasets (think columns in a sql database). It is often a key component of stacked models used in Kaggle competitions.

&amp;nbsp;

I'm just learning about gradient boosting and random forest and when I learn I like to have everything in one place so that I can also learn during my free time. That is why I'm sharing [this playlist](https://vidflow.com/playlist/introduction-to-gradient-boosting-117b) that deals with gradient boosting.

&amp;nbsp;

Also, I was wondering in what scenarios would people use LightGBM/Catboost over XGBoost?
",16,1,False,self,,,,,
517,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,4,aempe2,cfp.cc,Take part in this study: Challenges and opportunities associated with the use of Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/aempe2/take_part_in_this_study_challenges_and/,rmonhem,1547149878,,0,1,False,https://b.thumbs.redditmedia.com/j_FpQyyt7NklGWY6I1ZZsZbnzu-U4gOB37B4BuyG82s.jpg,,,,,
518,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,5,aemw8x,medium.com,[P] A blog post about Sentence Embeddings showing that deep learning isn't always necessary.,https://www.reddit.com/r/MachineLearning/comments/aemw8x/p_a_blog_post_about_sentence_embeddings_showing/,HichamEB,1547150949,,0,1,False,https://b.thumbs.redditmedia.com/_4j4ZPp1ooVMWIuatSZyoLj3PFHoBEu4mB4-ep5xunI.jpg,,,,,
519,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,5,aemz38,self.MachineLearning,http://www.pqlabs.ai/magicnet-disruptive-ai-technology.html,https://www.reddit.com/r/MachineLearning/comments/aemz38/httpwwwpqlabsaimagicnetdisruptiveaitechnologyhtml/,flylandcs,1547151416,[removed],0,1,False,self,,,,,
520,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,5,aen5do,self.MachineLearning,"CES 2019: MagicNet YOLO now 199x times faster, Self-driving car running on 0.9GHz CPU-only, How can this thing be so fast?",https://www.reddit.com/r/MachineLearning/comments/aen5do/ces_2019_magicnet_yolo_now_199x_times_faster/,flylandcs,1547152424,[removed],0,1,False,self,,,,,
521,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,5,aen5xt,self.MachineLearning,Complete Beginner:,https://www.reddit.com/r/MachineLearning/comments/aen5xt/complete_beginner/,johnyt3,1547152511,"I am currently a 2/4 quantitative finance major and am absolutely fascinated by neural nets and machine learning projects.  I have a general idea of how a neural net works, how they are typically trained, and generally where they can be applicable to a problem. 

My issue is that half of the words I read on this sub fly over my head and attempting to read new research findings is especially difficult with my lack of knowledge.

Any resources I can check out/beginner projects I can do? I know python. ",0,1,False,self,,,,,
522,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,5,aen7ap,self.MachineLearning,Machine Learning Poker Program to Beat professional players - How long will it take,https://www.reddit.com/r/MachineLearning/comments/aen7ap/machine_learning_poker_program_to_beat/,Abecid,1547152730,[removed],0,1,False,self,,,,,
523,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,5,aen8ul,self.Python,Deep Learning in Python (Free Course Offered),https://www.reddit.com/r/MachineLearning/comments/aen8ul/deep_learning_in_python_free_course_offered/,skj8,1547152981,,0,1,False,https://b.thumbs.redditmedia.com/jn47X_fZ9PdONiIPdGauVGvG-e0B3P40-9leLPFAxWY.jpg,,,,,
524,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,6,aeni3j,i.redd.it,What' s the function of GAN? what can it do?,https://www.reddit.com/r/MachineLearning/comments/aeni3j/what_s_the_function_of_gan_what_can_it_do/,alexchauncy,1547154455,,0,1,False,https://a.thumbs.redditmedia.com/pqVTBAZFafQYcXTOXV6jg61gnyCdZm_oKEohLUcDgY8.jpg,,,,,
525,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,6,aenqhg,arxiv.org,[R] A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs,https://www.reddit.com/r/MachineLearning/comments/aenqhg/r_a_unified_theory_of_early_visual/,ranihorev,1547155772,,5,1,False,default,,,,,
526,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,7,aeoahb,blog.floydhub.com,Exploring DNA with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/aeoahb/exploring_dna_with_deep_learning/,pirate7777777,1547158840,,0,1,False,https://b.thumbs.redditmedia.com/evakt686wuqXXhQTF-Gjr2pXGobpBaZIBFhO0cyioRs.jpg,,,,,
527,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,7,aeodx0,self.MachineLearning,[D] Combining multiple neural networks for autonomous drones?,https://www.reddit.com/r/MachineLearning/comments/aeodx0/d_combining_multiple_neural_networks_for/,pteroduct,1547159351,"I've been working on several networks for my drone (which only has a camera and IMU): learning optical flow, learning depth estimation for monocular vision, and also object detection. They all work fine independently, but as  I am just starting the process of embedding all these onto hardware, I'm stumbled upon an issue:

&amp;#x200B;

What is the feasibility/complexity of combining all 3 into one architecture? (motivated by the DeMoN architecture which outputs both depth and camera motion [https://arxiv.org/abs/1612.02401](https://arxiv.org/abs/1612.02401))",7,1,False,self,,,,,
528,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,9,aephyo,i.redd.it,[N] Peter Norvig endorsed The Hundred-Page Machine Learning Book by Andriy Burkov,https://www.reddit.com/r/MachineLearning/comments/aephyo/n_peter_norvig_endorsed_the_hundredpage_machine/,RudyWurlitzer,1547166074,,1,1,False,https://b.thumbs.redditmedia.com/KV-iFwzvgCzM1YKeMPYTvTe-gAA4ahq_rp4c4paXihQ.jpg,,,,,
529,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,9,aepnbu,arxiv.org,"[R] Manipulating and Measuring Model Interpretability: surprisingly, increased transparency hampered peoples ability to detect when a model has made a sizeable mistake",https://www.reddit.com/r/MachineLearning/comments/aepnbu/r_manipulating_and_measuring_model/,downtownslim,1547167091,,1,1,False,default,,,,,
530,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,9,aepnpl,self.MachineLearning,n peter norvig endorsed the hundredpage machine,https://www.reddit.com/r/MachineLearning/comments/aepnpl/n_peter_norvig_endorsed_the_hundredpage_machine/,impossibleteams,1547167157,,0,1,False,self,,,,,
531,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,9,aepol4,self.MachineLearning,[N] Peter Norvig endorsed The Hundred-Page Machine Learning Book by Andriy Burkov,https://www.reddit.com/r/MachineLearning/comments/aepol4/n_peter_norvig_endorsed_the_hundredpage_machine/,RudyWurlitzer,1547167316,"Deeply honored to have the back cover text for my book written by Peter Norvig and Aurlien Gron. It's the best recommendation a book on machine learning could possibly get.

&amp;#x200B;

[Back cover text from The Hundred-Page Machine Learning Book](https://i.redd.it/rvuskjw3xo921.png)",49,1,False,https://b.thumbs.redditmedia.com/JX-qL90U4sesgFRqmIVqJi17jB2mIta745Uz6_wmR_o.jpg,,,,,
532,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,9,aeppf7,self.MachineLearning,[P] I trained a DCGAN to remove a Snapchat filter from selfie images,https://www.reddit.com/r/MachineLearning/comments/aeppf7/p_i_trained_a_dcgan_to_remove_a_snapchat_filter/,ipsingh06,1547167475,"Link: https://github.com/ipsingh06/ml-desnapify

I trained a DCGAN to remove Snapchat selfie filters from images (inpainting problem). For now it's only trained to remove the infamous doggy filter.

Most of the architectural hard work was done by the excellent [pix2pix](https://phillipi.github.io/pix2pix) project.

Things I added that may be useful to others:

- A pipeline to generate image pairs for training data, sanitize images, detect and remove duplicates, etc.
- On-demand loading of training data so it doesn't have to fit entirely in memory (it's very important to use multiprocessing queues here to maintain sufficient throughput to the GPU)
- Integration with tensorboard to visualize the training metrics

There's still a lot of hyperparameter exploration I need to do, but the results so far look promising.

Any feedback is welcome. Thanks!",9,1,False,self,,,,,
533,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,9,aeps5f,self.learnmachinelearning,Where to begin?,https://www.reddit.com/r/MachineLearning/comments/aeps5f/where_to_begin/,throwaway_341356,1547167979,,0,1,False,default,,,,,
534,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,10,aeq0tp,arxiv.org,[R] Clustering-Oriented Representation Learning with Attractive-Repulsive Loss (AAAI 2019 Network Interpretability workshop),https://www.reddit.com/r/MachineLearning/comments/aeq0tp/r_clusteringoriented_representation_learning_with/,kiankd,1547169624,,1,1,False,default,,,,,
535,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,10,aeq6lu,self.MachineLearning,[Q] US Citizen with US Bachelor's getting an international Master's Degree?,https://www.reddit.com/r/MachineLearning/comments/aeq6lu/q_us_citizen_with_us_bachelors_getting_an/,Kyak787,1547170699,[removed],0,1,False,self,,,,,
536,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,11,aeqnd2,youtube.com,[N] What AI have MIT been creating? - BBC Click,https://www.reddit.com/r/MachineLearning/comments/aeqnd2/n_what_ai_have_mit_been_creating_bbc_click/,InternationalForm3,1547173836,,0,1,False,https://b.thumbs.redditmedia.com/GLuNhZi0xHECUG0s1COcc57ZvYbieY8Ip3pfYn7hdWA.jpg,,,,,
537,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,11,aeqsr9,self.MachineLearning,"Medical Records Document Classification, Identification, and Separation",https://www.reddit.com/r/MachineLearning/comments/aeqsr9/medical_records_document_classification/,PlatoRepublic,1547174819,"I work as a Solution Engineer for a large Enterprise Document Management company specifically within the Healthcare vertical. I have noticed Health Information Management departments around the world struggle with medical records documentation classification and identification. Their goal is to get paper into their Enterprise Medical Record system asap and as efficiently as possible. The main areas they struggle with are document separation, classification (what type of document it is), and identification (matching it to a patient/visit/order).

I'm trying to create a software demo so an investor will take me seriously. The end goal of the software would be to spit out multi-page tiff image files with a corresponding comma separated index file so it could be loaded in to whatever DMS/EMR the company is using. Each index file would have the name of the image file, name of the document type. If patient/visit/order identifiers were present on the document it would also include them.

I'm a ML armature so the only thing I have tried so far is re-training the last layer of googles inception deep convolutional neural network. I saw some success but was wondering if any of you had any ideas as to what ML algorithm/s would be better suited for this task. I envision it using at least two algorithm/s one to look at the pixels like the inception DCNN does, and one to look at the orc rendition of the characters of the pages. Please let me know your thoughts! I appreciate any and all comments.",0,1,False,self,,,,,
538,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,11,aeqtu8,youtube.com,[D] Does anyone know this? Self-driving AI task running on a 0.9GHz CPU processor?,https://www.reddit.com/r/MachineLearning/comments/aeqtu8/d_does_anyone_know_this_selfdriving_ai_task/,greatml,1547175025,,0,1,False,default,,,,,
539,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,12,aercmq,/r/MachineLearning/comments/aercmq/a_brilliant_advice_for_artificial_intelligence_by/,A Brilliant Advice For Artificial Intelligence By Stephen Wolfram.,https://www.reddit.com/r/MachineLearning/comments/aercmq/a_brilliant_advice_for_artificial_intelligence_by/,AbhizNandan,1547178613,,0,1,False,https://b.thumbs.redditmedia.com/0yUDEwXl_P9lxlyMwiFecPQKXJoghh1Q5GtvmDBg4VA.jpg,,,,,
540,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,13,aermoy,arxiv.org,"[R] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. New SOTAs, with PyTorch and TF pretrained models.",https://www.reddit.com/r/MachineLearning/comments/aermoy/r_transformerxl_attentive_language_models_beyond/,milaworld,1547180572,,6,1,False,default,,,,,
541,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,13,aerp4z,self.MachineLearning,"[P] All-in-one development container: ""AI Lab""",https://www.reddit.com/r/MachineLearning/comments/aerp4z/p_allinone_development_container_ai_lab/,tlkh,1547181063,"Hey everyone, just wanted to share a side project that I've been working on while doing an internship. It's an all-in-one dev environment packaged as a Docker container that works for almost all usage scenarios in doing data science, machine learning or deep learning development (hence the generic name ""AI Lab""...)

**The backstory** (skip if you hate stories)

So, I've been frustrated with setting up environments between various workstations/servers/cloud instances (for both myself and new collaborators). Even things like conda environments are annoying since they can take forever to install (after you figure out the right configuration), so the setup time really adds up. The other way is to use Docker containers but containers like those from [NGC](https://www.nvidia.com/en-us/gpu-cloud/) are a little bit too traditional/limiting in that they don't bundle things like Jupyter which I use pretty heavily. I also find the pre-installed packages lacking and building on top of those aren't easy.

We also have a Kubernetes setup running JupyterHub and containers need to have some additions to play nicely with that, but its shocking convenient once everything is set up properly.

**End backstory**

So my goal here is to be able to launch my dev environment with one command, anywhere! Here's what the workflow looks like now starting from a ""blank"" workstation or instance:

1. Install drivers/CUDA/Docker with script (using another side project... haha)  
`curl https://getcuda.ml/ubuntu.sh | sudo bash` (yes `curl | sudo bash` is bad but it's really thrilling)
2. `docker pull nvaitc/ai-lab:latest`
3. `nvidia-docker run --rm -p 8888:8888 -v /home/USER:/home/jovyan nvaitc/ai-lab`

With those three steps, a fully-configured Jupyter Notebook/Lab pops up and becomes fully available for use with frameworks including TensorFlow/PyTorch/MXNet/RAPIDS etc. Of course, you can use it interactively without Jupyter and also launch it from within JupyterHub.

This makes it really easy to just get started in a new environment anywhere (with an Internet connection)

I'm just sharing this because it has been super useful to me and some members of my team so I'm trying to package this nicely and share it with others! I'm also soliciting feedback and comments so do let me know what you think about this!

For more information and to open issues, please check out [nvaitc/ai-lab on GitHub](https://github.com/NVAITC/ai-lab) and feel free to open issues or pull requests. Some more detailed instructions on usage are also [available here](https://github.com/NVAITC/ai-lab/blob/master/INSTRUCTIONS.md).

Have a nice weekend ahead everyone! :)",17,1,False,self,,,,,
542,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,14,aerz38,self.MachineLearning,How to really install tensorflow-gpu from source on a clean Ubuntu,https://www.reddit.com/r/MachineLearning/comments/aerz38/how_to_really_install_tensorflowgpu_from_source/,rnreich,1547183176,[removed],0,1,False,self,,,,,
543,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,14,aes0nc,analyticsvidhya.com,"Artificial Intelligence, Machine Learning and Big Data - A Comprehensive Report",https://www.reddit.com/r/MachineLearning/comments/aes0nc/artificial_intelligence_machine_learning_and_big/,hiren_p,1547183502,,0,1,False,default,,,,,
544,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,14,aes3hc,self.MachineLearning,"Doubt regarding this paper ""Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection""",https://www.reddit.com/r/MachineLearning/comments/aes3hc/doubt_regarding_this_paper_deep_autoencoding/,CGautam,1547184108,[removed],0,1,False,self,,,,,
545,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,16,aet1l7,lwindia.com,"SUMMER INTERNSHIP - Machine Learning , Big Data Hadoop",https://www.reddit.com/r/MachineLearning/comments/aet1l7/summer_internship_machine_learning_big_data_hadoop/,LinuxWorldInfo,1547192114,,0,1,False,default,,,,,
546,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,18,aetrdy,blog.nanonets.com,How To Moderate Content In 2019 : Human vs AI,https://www.reddit.com/r/MachineLearning/comments/aetrdy/how_to_moderate_content_in_2019_human_vs_ai/,nanonets,1547199203,,0,1,False,default,,,,,
547,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,18,aetuad,self.MachineLearning,maquinaria de obras publicas segunda mano,https://www.reddit.com/r/MachineLearning/comments/aetuad/maquinaria_de_obras_publicas_segunda_mano/,insafali737,1547199991,[removed],0,1,False,self,,,,,
548,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,18,aetufw,github.com,GitHub - zalandoresearch/flair: A very simple framework for state-of-the-art Natural Language Processing (NLP),https://www.reddit.com/r/MachineLearning/comments/aetufw/github_zalandoresearchflair_a_very_simple/,iloveintuition,1547200034,,0,1,False,default,,,,,
549,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,19,aetxm9,self.MachineLearning,"[D] Those of you with jobs that use ML but don't have a graduate degree, what's your story?",https://www.reddit.com/r/MachineLearning/comments/aetxm9/d_those_of_you_with_jobs_that_use_ml_but_dont/,sugarhilldt2,1547200847,,0,1,False,self,,,,,
550,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeue62,arxiv.org,[R] One-Shot Training of Neural Networks Using Hypercube-Based Topological Coverings,https://www.reddit.com/r/MachineLearning/comments/aeue62/r_oneshot_training_of_neural_networks_using/,chisai_mikan,1547205149,,4,1,False,default,,,,,
551,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeufir,github.com,Python-Awesome tutorial for Machine Learning as part of a Graduate Program in Machine Learning. Pull requests/changes/stars would be very much helpful.,https://www.reddit.com/r/MachineLearning/comments/aeufir/pythonawesome_tutorial_for_machine_learning_as/,gautamrbharadwaj,1547205510,,0,1,False,default,,,,,
552,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeujiy,physicstoday.scitation.org,Machine learning for image restoration.,https://www.reddit.com/r/MachineLearning/comments/aeujiy/machine_learning_for_image_restoration/,MistWeaver80,1547206561,,0,1,False,default,,,,,
553,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeujz8,self.MachineLearning,Truly parallel data generator in Keras (Python)?,https://www.reddit.com/r/MachineLearning/comments/aeujz8/truly_parallel_data_generator_in_keras_python/,ze_baco,1547206673,[removed],0,1,False,self,,,,,
554,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeul7c,medium.com,A Light Introduction to Transformer-XL,https://www.reddit.com/r/MachineLearning/comments/aeul7c/a_light_introduction_to_transformerxl/,omarsar,1547207002,,0,1,False,default,,,,,
555,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeulp4,youtube.com,Get free GPU for training with Google Colab - Detailed Tutorial,https://www.reddit.com/r/MachineLearning/comments/aeulp4/get_free_gpu_for_training_with_google_colab/,Additional_Proof,1547207140,,0,1,False,default,,,,,
556,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeulsj,americanscientist.org,Deep Learning and Galaxy Classification,https://www.reddit.com/r/MachineLearning/comments/aeulsj/deep_learning_and_galaxy_classification/,MistWeaver80,1547207168,,0,1,False,https://a.thumbs.redditmedia.com/xmFWguvUya_W0jP3Rlj6sJqWHDvtMl0CappqWe98R_4.jpg,,,,,
557,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,20,aeunec,eng.uber.com,Creating a Zoo of Atari-Playing Agents to Catalyze the Understanding of Deep Reinforcement Learning (Uber/Google Brain/OpenAI),https://www.reddit.com/r/MachineLearning/comments/aeunec/creating_a_zoo_of_atariplaying_agents_to_catalyze/,aiismorethanml,1547207587,,1,1,False,default,,,,,
558,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,21,aeupxp,self.MachineLearning,"[R] An Atari Model Zoo for Analyzing, Visualizing, and Comparing Deep Reinforcement Learning Agents (Uber AI/Google Brain/OpenAI)",https://www.reddit.com/r/MachineLearning/comments/aeupxp/r_an_atari_model_zoo_for_analyzing_visualizing/,aiismorethanml,1547208203,"&gt;Much human and computational effort has aimed to improve how deep reinforcement learning algorithms perform on benchmarks such as the Atari Learning Environment. Comparatively less effort has focused on understanding what has been learned by such methods, and investigating and comparing the representations learned by different families of reinforcement learning (RL) algorithms. Sources of friction include the onerous computational requirements, and general logistical and architectural complications for running Deep RL algorithms at scale. We lessen this friction, by (1) training several algorithms at scale and releasing trained models, (2) integrating with a previous Deep RL model release, and (3) releasing code that makes it easy for anyone to load, visualize, and analyze such models. This paper introduces the Atari Zoo framework, which contains models trained across benchmark Atari games, in an easy-to-use format, as well as code that implements common modes of analysis and connects such models to a popular neural network visualization library. Further, to demonstrate the potential of this dataset and software package, we show initial quantitative and qualitative comparisons between the performance and representations of several deep RL algorithms, highlighting interesting and previously unknown distinctions between them.

&amp;#x200B;

Blog: [https://eng.uber.com/atari-zoo-deep-reinforcement-learning/](https://eng.uber.com/atari-zoo-deep-reinforcement-learning/)

arXiv: [https://github.com/uber-research/atari-model-zoo](https://github.com/uber-research/atari-model-zoo)

GitHub: [https://github.com/uber-research/atari-model-zoo](https://github.com/uber-research/atari-model-zoo)

Google Colab notebook: [https://colab.research.google.com/github/uber-research/atari-model-zoo/blob/master/colab/AtariZooColabDemo.ipynb](https://colab.research.google.com/github/uber-research/atari-model-zoo/blob/master/colab/AtariZooColabDemo.ipynb)",2,1,False,self,,,,,
559,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,21,aeuvsv,self.MachineLearning,Help me to build a filtering smoke on videos via CNN in python,https://www.reddit.com/r/MachineLearning/comments/aeuvsv/help_me_to_build_a_filtering_smoke_on_videos_via/,Sundist,1547209567,[removed],0,1,False,self,,,,,
560,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,21,aeuxht,self.MachineLearning,New Machine Learning slack community for beginners,https://www.reddit.com/r/MachineLearning/comments/aeuxht/new_machine_learning_slack_community_for_beginners/,Atfdynamo5,1547209955,"Hello everyone,

I've  started a new slack community for beginners who would like to learn Machine Learning from the ground up in a very clear and simple manner. It will start with an introduction and a brief overview of Machine Learning then we will delve more into the nuts bolts. We will cover all the important concepts as well as do lots of practice sets. Ask all the questions you want until you feel completely familiar with the course material. Best thing is that its free! :)

See you there!

https://machinelearningforall.slack.com

Amir",0,1,False,self,,,,,
561,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,21,aev0tw,dev.to,3+ Best Deep Learning Courses [Dev Community],https://www.reddit.com/r/MachineLearning/comments/aev0tw/3_best_deep_learning_courses_dev_community/,skj8,1547210687,,0,1,False,https://b.thumbs.redditmedia.com/nF6e-a4ZWSvaIRmWqZWlTAH9cg9vhNnMyiRS2pPCNrU.jpg,,,,,
562,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,22,aevkn6,self.MachineLearning,Decreasing the batch size to train a GAN on a single GPU [RTX2080Ti] with ~5k 1024px images [NVIDIA ProGAN],https://www.reddit.com/r/MachineLearning/comments/aevkn6/decreasing_the_batch_size_to_train_a_gan_on_a/,whyeverwhy,1547214792,[removed],0,1,False,self,,,,,
563,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,23,aevsix,self.MachineLearning,Good books on state space models for thermodynamics?,https://www.reddit.com/r/MachineLearning/comments/aevsix/good_books_on_state_space_models_for/,mufflonicus,1547216270,[removed],0,1,False,self,,,,,
564,MachineLearning,t5_2r3gv,2019-1-11,2019,1,11,23,aevvpr,self.MachineLearning,Good books or article recommendations on state space models?,https://www.reddit.com/r/MachineLearning/comments/aevvpr/good_books_or_article_recommendations_on_state/,mufflonicus,1547216867,[removed],0,1,False,self,,,,,
565,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,1,aewuvl,tryolabs.com,[N] List of Machine Learning / Deep Learning conferences in 2019,https://www.reddit.com/r/MachineLearning/comments/aewuvl/n_list_of_machine_learning_deep_learning/,minmidinosaur,1547223025,,0,1,False,default,,,,,
566,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,1,aex3q3,datascience.stackexchange.com,[Help] Standard Scaler drops accuracy significantly in Scala Spark,https://www.reddit.com/r/MachineLearning/comments/aex3q3/help_standard_scaler_drops_accuracy_significantly/,dantek88,1547224495,,0,1,False,default,,,,,
567,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,1,aex4qx,self.MachineLearning,[D] How does convex optimization fit in machine learning and non-PhD work?,https://www.reddit.com/r/MachineLearning/comments/aex4qx/d_how_does_convex_optimization_fit_in_machine/,FluffdaddyFluff,1547224665,"I took a course on convex optimization in my final year of undergrad, and it was 100% theory based and pretty much revolved on proving difficult theories and then coding some algorithms from scratch (gradient descent, newtons method, etc.)

I'm now in a masters in analytics program and one of the first courses is just giving a high-level overview of how some ML models work (SVM for example). SVM are inherently a quadrattic programming/convex optimization problem. This got me thinking, is there a reason for trying to gain a deep understanding on convex optimization as a non-PhD who is not looking to create the newest cutting-edge algorithms?

I've spent many hours going through Boyd's book on convex optimization, but still dont really understand when it would be more appropriate to code up an optimization algorithm from scratch vs. using a ML model already in existence. 

Does anyone here have any thought/perspectives on this?",18,1,False,self,,,,,
568,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,1,aex65q,forge.ai,"Modeling Intent using Syntax, is it possible?",https://www.reddit.com/r/MachineLearning/comments/aex65q/modeling_intent_using_syntax_is_it_possible/,jenniferlum,1547224895,,0,1,False,default,,,,,
569,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,1,aex6mq,arxiv.org,[R] Photo Wake-Up: 3D Character Animation from a Single Photo,https://www.reddit.com/r/MachineLearning/comments/aex6mq/r_photo_wakeup_3d_character_animation_from_a/,ranihorev,1547224968,,6,1,False,default,,,,,
570,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,2,aexey6,self.MachineLearning,[D] Deep Learning Basics: Introduction and Overview - MIT,https://www.reddit.com/r/MachineLearning/comments/aexey6/d_deep_learning_basics_introduction_and_overview/,UltraMarathonMan,1547226316,[removed],0,1,False,self,,,,,
571,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,2,aexr2e,dimensionless.in,Stock Price prediction using ML and DL,https://www.reddit.com/r/MachineLearning/comments/aexr2e/stock_price_prediction_using_ml_and_dl/,divya2018,1547228244,,0,1,False,https://a.thumbs.redditmedia.com/1DR_B3pj0tJ93RW9wkF2bSwE3o46r-b6MgQ5cegWqL0.jpg,,,,,
572,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,2,aexrps,self.MachineLearning,[D] Why do GANs work even with adversarial example weakness??,https://www.reddit.com/r/MachineLearning/comments/aexrps/d_why_do_gans_work_even_with_adversarial_example/,quantizedmodel,1547228347,"You know how with a trained classifier and changing the image by gradient ascent to change the classifier's outputted class to another one just results to an image visually the same to us.

The changing of the weights of the generator to output an image that changes the discriminator's output from fake to real sounds like the same task above yet the image does change to an image that does look real to us. What's the difference that make GANs work?",13,1,False,self,,,,,
573,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aey4nb,medium.com,Enhanced Super-Resolution GAN Remasters Max Payne,https://www.reddit.com/r/MachineLearning/comments/aey4nb/enhanced_superresolution_gan_remasters_max_payne/,Yuqing7,1547230347,,0,1,False,default,,,,,
574,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aey4oj,self.MachineLearning,Why many ML paper authors don't try their models on large datasets?,https://www.reddit.com/r/MachineLearning/comments/aey4oj/why_many_ml_paper_authors_dont_try_their_models/,aetolicus,1547230353,[removed],0,1,False,self,,,,,
575,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aey7dd,self.MachineLearning,[D] Why don't many paper authors try their models on big datasets?,https://www.reddit.com/r/MachineLearning/comments/aey7dd/d_why_dont_many_paper_authors_try_their_models_on/,aetolicus,1547230800,"For example, taget prop is a relatively old algorithm:  
[https://arxiv.org/abs/1412.7525](https://arxiv.org/abs/1412.7525) (2014)  
But it was applied to Imagenet only in 2018: [https://openreview.net/forum?id=SyPicjbWQ](https://openreview.net/forum?id=SyPicjbWQ)  
Are paper authors just afraid to show bad results or there are some other reasons? ",12,1,False,self,,,,,
576,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aey9ey,self.MachineLearning,Computer Vision Object Detection Help - How do I create a training set,https://www.reddit.com/r/MachineLearning/comments/aey9ey/computer_vision_object_detection_help_how_do_i/,alexgoncharov,1547231146,[removed],0,1,False,self,,,,,
577,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aeyacf,self.MachineLearning,[P] Visualizing the dynamics of GAN,https://www.reddit.com/r/MachineLearning/comments/aeyacf/p_visualizing_the_dynamics_of_gan/,b0red1337,1547231306,"Did this while playing around with GAN, think it might be able to give some intuitions about how GAN works

See the video here: [https://www.youtube.com/watch?v=aOoKLiDFTek](https://www.youtube.com/watch?v=aOoKLiDFTek)

At each time step, the discriminator (green curve) tries to decrease the value around the generated data (blue histogram) and increase the value around the true data (orange histogram) by the discriminator gradient while the blue histogram tries to ""move"" along the direction that increases its probability (green curve) by the generator gradient.

Interestingly, GAN never collapsed into a single mode in this example despite passing through each mode several times. The chaotic motion in this simple example makes me feel that the ability to generate anything remotely useful in high dimensional space (image) is absolutely miraculous.

&amp;#x200B;

(source can be found here [https://github.com/hrpan/test\_gan](https://github.com/hrpan/test_gan))

&amp;#x200B;",11,1,False,self,,,,,
578,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aeyasm,self.MachineLearning,"[D] Conflicting ""facts"" about the likelihood employed in Bayes theorem?",https://www.reddit.com/r/MachineLearning/comments/aeyasm/d_conflicting_facts_about_the_likelihood_employed/,fundamentalidea,1547231379,"This is a really fundamental question. I know someone will say that it should be posted in the  ""learn ML"" reddit forum. But though it's fundamental, it doesn't seem to be covered in any books. I have checked Bishop, Friedman, Goodfellow, and others and do not see any answer. The question is driving me nuts!  So I would really like to ask the experts here.

&amp;#x200B;

Consider the following ""facts""

&amp;#x200B;

1. Bayes theorem, written generically as $P(A|B) = \\frac{ P(B|A) P(A) }{ P(B) }$ involves conditional and marginal probabilities.  Focus on $P(B|A)$. Wiki Bayes theorem says this is a conditional **probability**. This seems very clear in the alternate expression $P(B|A) P(A) = P(A|B) P(B)$.

&amp;#x200B;

2. In the Bayes theorem, $P(B|A)$ is called the **likelihood.**

&amp;#x200B;

3. The likelihood is defined $P(B|A)$ viewed as a function of $A$, not of $B$. It is ***not*** *a conditional* ***probability*** because it does not integrate to one. See Bishop Pattern Recognition &amp; Machine Learning book p.22,  ""Note that the likelihood is not a probability distribution over w, and its integral with respect to w does not (necessarily) equal one.""  

&amp;#x200B;

There is a problem here, one of these three facts must be wrong, or else I do not understand something. *How can the likelihood in the Bayes theorem be a conditional probability, and also not a conditional probability?*

&amp;#x200B;",10,1,False,self,,,,,
579,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aeyc12,self.MachineLearning,[D] MIT Deep Learning Basics,https://www.reddit.com/r/MachineLearning/comments/aeyc12/d_mit_deep_learning_basics/,UltraMarathonMan,1547231585,[removed],0,1,False,https://b.thumbs.redditmedia.com/R5GjJylhx4nKRzEt5J-F44wgJu4HlOmEvgYHGOUsujs.jpg,,,,,
580,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,3,aeylej,self.MachineLearning,[D] Research-oriented Machine Learning MSc. degree programs in Germany,https://www.reddit.com/r/MachineLearning/comments/aeylej/d_researchoriented_machine_learning_msc_degree/,jStojcheski_,1547233109,"Hello people,

&amp;#x200B;

I am Jugoslav from Macedonia, I have a bachelor's degree in Computer Science and I would like to pursue a master's degree in Germany. I am especially interested in research-oriented programs covering topics like Machine Learning and Statistics. So far, I have found these programs, which I think are most-suitable for me:

\- TU Mnchen: [Mathematics in Data Science](https://www.ma.tum.de/en/studies-information/study-programs-mathematics/master-mathematics-in-data-science.html)

\- RWTH Aachen: [Data Science](https://data-science.rwth-aachen.de/)

\- Uni des Saarlandes: [Mathematics and Computer Science](http://mathcs.uni-saarland.de/master_en.html)

&amp;#x200B;

It would be great if someone who has already participated (or is currently participating) in these programs (or in a similar program) is willing to share his/her experience. Any recommendations, advice, pros, cons etc are welcome.

Thanks in advance!",28,1,False,self,,,,,
581,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,4,aeylww,self.MachineLearning,"Does a fully fledged autonomous AI exist yet? If so, what is it called? If not, Is it really even possible?",https://www.reddit.com/r/MachineLearning/comments/aeylww/does_a_fully_fledged_autonomous_ai_exist_yet_if/,rushzone,1547233201,[removed],0,1,False,self,,,,,
582,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,4,aeyqgn,github.com,New open-source code to evaluate deep reinforcement learning approaches to robotic grasping,https://www.reddit.com/r/MachineLearning/comments/aeyqgn/new_opensource_code_to_evaluate_deep/,vincentvanhoucke,1547233925,,0,1,False,default,,,,,
583,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,4,aeywim,self.MachineLearning,[R] InstaGAN  Instance-aware Image-to-image Translation for Object Transfiguration,https://www.reddit.com/r/MachineLearning/comments/aeywim/r_instagan_instanceaware_imagetoimage_translation/,ranihorev,1547234908,[removed],0,1,False,self,,,,,
584,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,4,aez0fh,self.MachineLearning,Is the average work in a big tech like google comparable to the standard of interview they take?,https://www.reddit.com/r/MachineLearning/comments/aez0fh/is_the_average_work_in_a_big_tech_like_google/,endeavour23,1547235564,[removed],0,1,False,self,,,,,
585,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,4,aez6i4,self.MachineLearning,[P] Resolving the Universe: An Astronomical Introduction to GAN Image Super-Resolution via TensorFlow and GalaxyZoo,https://www.reddit.com/r/MachineLearning/comments/aez6i4/p_resolving_the_universe_an_astronomical/,david_reiman,1547236532,"Here's a blog post I wrote about super-resolving galaxy images with an SRGAN, complete with TensorFlow code and some useful tips and classes for utilizing the TensorFlow Dataset objects. Comments and criticism welcome. Enjoy!

[https://davidreiman.github.io/posts/galaxy-image-super-resolution](https://davidreiman.github.io/posts/galaxy-image-super-resolution)",16,1,False,self,,,,,
586,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,6,af02gt,self.MachineLearning,Preprocess large continuos ranges,https://www.reddit.com/r/MachineLearning/comments/af02gt/preprocess_large_continuos_ranges/,jtfidje,1547241620,[removed],0,1,False,self,,,,,
587,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,7,af10lt,self.MachineLearning,Are there specific ML algorithms that are most useful in finance?,https://www.reddit.com/r/MachineLearning/comments/af10lt/are_there_specific_ml_algorithms_that_are_most/,QuantumVariables,1547246771,[removed],0,1,False,self,,,,,
588,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,9,af1q8h,medium.com,"CES 2019 Cools on Self-Driving; Digital Cockpits, V2X &amp; In-Vehicle Shopping Drive Mobility Market",https://www.reddit.com/r/MachineLearning/comments/af1q8h/ces_2019_cools_on_selfdriving_digital_cockpits/,Yuqing7,1547251276,,0,1,False,https://a.thumbs.redditmedia.com/sag2wZx2K4dWaqP-kumdKoFB8IRvl91w7weTYUFYgQ0.jpg,,,,,
589,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,9,af1yxq,self.MachineLearning,[D] Was there any conclusion on GloVe vs Word2vec ? Also in their application to encoding other types of items?,https://www.reddit.com/r/MachineLearning/comments/af1yxq/d_was_there_any_conclusion_on_glove_vs_word2vec/,BatmantoshReturns,1547252858,"I'm asking because word2vec is being used in recommendation systems, but I think as soon as 2 years ago, there wasn't yet a solid conclusion on which one is better. They seemed to be pretty similar, which is not surprising, I'd imagine center word/context word pairings, and word-word co-occurences within a context to give similar results. ",27,1,False,self,,,,,
590,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,10,af2drk,self.MachineLearning,VQA with generated answers,https://www.reddit.com/r/MachineLearning/comments/af2drk/vqa_with_generated_answers/,cher_super,1547255696,"It seems existing work on VQA is 'selecting' answers from the answer pool, which is a classification task. So why people not try to generate answers for questions? ",0,1,False,self,,,,,
591,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,10,af2hxp,self.MachineLearning,[D] GAN for hair style transfer?,https://www.reddit.com/r/MachineLearning/comments/af2hxp/d_gan_for_hair_style_transfer/,hanyuqn,1547256530,"I want to train a model that is able to transfer one specific hairstyle of a specific person (I will be collecting the training data myself) to any other person and am wondering about the feasibility of this. While there have been several projects relating to transfer of facial attributes such as StarGAN, when it comes to hair they only change colour which is definitely significantly easier. The one project I found attempting something like this is Learning to Generate and Edit Hairstyles ([https://yanweifu.github.io/papers/hairstyle\_v\_14\_weidong.pdf](https://yanweifu.github.io/papers/hairstyle_v_14_weidong.pdf)) from about 18 months ago with no public implementation.",6,1,False,self,,,,,
592,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,10,af2rm5,reddit.com,GERENCIAMENTO de PROJETOS de OBRAS de ENGENHARIA VI,https://www.reddit.com/r/MachineLearning/comments/af2rm5/gerenciamento_de_projetos_de_obras_de_engenharia/,JamurGerloff,1547258384,,0,1,False,default,,,,,
593,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,11,af2vpx,youtube.com,Deep Learning: A Scientific Perspective - Nadav Cohen,https://www.reddit.com/r/MachineLearning/comments/af2vpx/deep_learning_a_scientific_perspective_nadav_cohen/,sflicht,1547259196,,0,1,False,https://b.thumbs.redditmedia.com/zNR5nE7xLFmz7GhbgTg1FsETXUumqQlMuQ6ymULJ6qs.jpg,,,,,
594,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,11,af2yza,amoned.tk,"Researchers say if parents want to successfully cut back on their childs screen-time, they must first cut back on screen-time themselves",https://www.reddit.com/r/MachineLearning/comments/af2yza/researchers_say_if_parents_want_to_successfully/,eslamalmahdy,1547259865,,0,1,False,default,,,,,
595,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,12,af3aw3,self.MachineLearning,Large Dataset for Dynamic Analysis of Malware?,https://www.reddit.com/r/MachineLearning/comments/af3aw3/large_dataset_for_dynamic_analysis_of_malware/,Kaen_No_Mai,1547262367,[removed],0,1,False,self,,,,,
596,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,12,af3fp7,self.MachineLearning,[D] Semantic Similarity using Universal Sentence Encoder,https://www.reddit.com/r/MachineLearning/comments/af3fp7/d_semantic_similarity_using_universal_sentence/,paradox44,1547263420,"I have written a short tutorial regarding using Universal Sentence Encoder to do semantic search. 
The use-case is as:
- we have a bunch of text (words/sentence/paragraph)
- we have a query
- find the text that is semantically similar to the query

The link to original notebook is also embedded in the blog.

[Here's](http://www.nishanpantha.com.np/programming/2019/01/09/universal-sentence-encoder-semantic-search.html) the blog post.

The jupyter notebook can be found [here](https://github.com/NISH1001/machine-learning-into-the-void/blob/master/nlp/universal-sentence-encocder-semantic-similarity.ipynb).

Currently, I have been using Universal Sentence Encoder for a chatbot I am working on. Previously, I used averages of GloVe embeddings for each words. However, UST seems to perform better at sentence level.",3,1,False,self,,,,,
597,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,13,af3sm2,self.MachineLearning,[P] NSFW Image Data Collection,https://www.reddit.com/r/MachineLearning/comments/af3sm2/p_nsfw_image_data_collection/,alexkimxyz,1547266319,"This might help you train an image classifier similar to Yahoo's: [https://github.com/yahoo/open\_nsfw](https://github.com/yahoo/open_nsfw)

but with a bit more granularity in classes",13,1,True,nsfw,,,,,
598,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,14,af4d4d,self.MachineLearning,[D] How much time do you spend catching up on new papers v/s doing your own research?,https://www.reddit.com/r/MachineLearning/comments/af4d4d/d_how_much_time_do_you_spend_catching_up_on_new/,invertedpassion,1547270811,"With the pace new projects and papers come out in the field, how do you split your time between being updated vs doing your own research? 

I find myself attracted to reading papers for learning new ideas but this obviously comes at the expense of spending time thinking about your own research projects. 

How do you manage?",36,1,False,self,,,,,
599,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,14,af4emz,self.MachineLearning,[D] AMD Vega VII for ML?,https://www.reddit.com/r/MachineLearning/comments/af4emz/d_amd_vega_vii_for_ml/,sagnick9,1547271152,"AMD's latest Vega VII GPU seems to be porfect for training large models (many compute cores and 16 GB of HBM memory!). In particular, its 16 GB of memory and super fast memory speed seem to be perfect for training on large data sets and it unmatched by NVidia at the $799 price point. But is it possible to use this GPU to accelerate TensorFlow training and/or inference? I am aware that there is some AMD support for TensorFlow but how good is it, especially for the upcoming version 2.0? Are there other frameworks with better support for AMD?",11,1,False,self,,,,,
600,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,15,af4yc0,self.MachineLearning,[D] AI Safety and RL,https://www.reddit.com/r/MachineLearning/comments/af4yc0/d_ai_safety_and_rl/,Maplernothaxor,1547275888,"Im relatively new to AI safety and was curious why most discussions regarding it view RL as the only method to develop AGI. Is it because its the best method we have to learn from complex uncertain environments?

Sorry if this is a silly question or in the wrong subreddit",16,1,False,self,,,,,
601,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,16,af51s2,self.MachineLearning,[D] [R] Benchmark results for FashionMNIST,https://www.reddit.com/r/MachineLearning/comments/af51s2/d_r_benchmark_results_for_fashionmnist/,sairaamv92,1547276792,"Hi everyone,

I intend this post's thread to serve as a record for the state-of-the-art in classification results on FashionMNIST. So, it would be great if you could post the best/latest results for FashionMNIST with a link to the paper/preprint it came from.

Why is this necessary/important? I'll try to give some reasons below:

1. MNIST is increasingly becoming outdated as a baseline dataset.
2. FashionMNIST is built as a drop-in replacement for MNIST, being similar in both dataset size and image size.
3. FashionMNIST is significantly harder to  classify.
4. There seems to be no one central resource for finding out what is the state-of-the art

Reference links: 

1. [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)
2. [https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/](https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/)

&amp;#x200B;",19,1,False,self,,,,,
602,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,17,af5moi,self.MachineLearning,Can we say PCA (principal component analysis) is decomposite linear dependent component?,https://www.reddit.com/r/MachineLearning/comments/af5moi/can_we_say_pca_principal_component_analysis_is/,MCSdude,1547282613,[removed],0,1,False,self,,,,,
603,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,19,af63fd,meaningofstuff.blogspot.com,Knowledge is all about prediction,https://www.reddit.com/r/MachineLearning/comments/af63fd/knowledge_is_all_about_prediction/,mihvoi,1547287621,,0,1,False,default,,,,,
604,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,20,af6epw,self.MachineLearning,Where can i find a video dataset for using on CNN via python ?,https://www.reddit.com/r/MachineLearning/comments/af6epw/where_can_i_find_a_video_dataset_for_using_on_cnn/,Sundist,1547290840,[removed],0,1,False,self,,,,,
605,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,20,af6qjs,youtube.com,Leela reacts beautifully to Stockfish in Kings Indian with advantages co...,https://www.reddit.com/r/MachineLearning/comments/af6qjs/leela_reacts_beautifully_to_stockfish_in_kings/,kingscrusher-youtube,1547293996,,0,1,False,default,,,,,
606,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,20,af6r7j,self.MachineLearning,Subreddit: AI_Music,https://www.reddit.com/r/MachineLearning/comments/af6r7j/subreddit_ai_music/,Stabenfeldt,1547294188,[removed],0,1,False,self,,,,,
607,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,21,af6wye,github.com,Deep Learning with TensorFlow 2 and Keras  Notebooks,https://www.reddit.com/r/MachineLearning/comments/af6wye/deep_learning_with_tensorflow_2_and_keras/,dronecub,1547295624,,0,1,False,default,,,,,
608,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,21,af710q,self.MachineLearning,MC dropout for regression task,https://www.reddit.com/r/MachineLearning/comments/af710q/mc_dropout_for_regression_task/,IAlover,1547296689,[removed],0,1,False,self,,,,,
609,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,21,af74pe,self.MachineLearning,Mc dropout for regression network,https://www.reddit.com/r/MachineLearning/comments/af74pe/mc_dropout_for_regression_network/,IAlover,1547297646,[removed],0,1,False,self,,,,,
610,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,22,af7exf,self.MachineLearning,Accountant looking to commit to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/af7exf/accountant_looking_to_commit_to_machine_learning/,ohhhhlu,1547299993,[removed],0,1,False,self,,,,,
611,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,22,af7f01,poloclub.github.io,[R] GAN Lab: Visualize 2D toy dataset GAN training,https://www.reddit.com/r/MachineLearning/comments/af7f01/r_gan_lab_visualize_2d_toy_dataset_gan_training/,tsauri,1547300007,,0,1,False,default,,,,,
612,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,22,af7khd,github.com,"[P] Implementing P-adam, a novel optimization algorithm for Deep Neural Networks",https://www.reddit.com/r/MachineLearning/comments/af7khd/p_implementing_padam_a_novel_optimization/,pandeykartikey,1547301267,,0,1,False,default,,,,,
613,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,23,af7rdr,self.MachineLearning,"[P] Implementing P-adam, novel optimization algorithm for Neural Networks",https://www.reddit.com/r/MachineLearning/comments/af7rdr/p_implementing_padam_novel_optimization_algorithm/,pandeykartikey,1547302737,"This work is a part of ICLR Reproducibility Challenge 2019, we try to reproduce the  results  in  the  conference  submission PADAM: Closing The Generalization Gap of Adaptive Gradient Methods In Training Deep Neural Networks. Adaptive  gradient  methods  proposed  in  past  demonstrate  a  degraded  generalization performance than the stochastic gradient descent (SGD) with momentum.   The authors try to address this problem by designing a new optimization algorithm that bridges the gap between the space of Adaptive Gradient algorithms and SGD with momentum. With this method a new tunable hyperparameter called partially adaptive parameter p is introduced that varies between \[0, 0.5\]. We build the proposed optimizer and use it to mirror the experiments performed by the authors. We review and comment on the empirical analysis performed by the authors.  Finally, we also propose a future direction for further study of Padam.  Our code is available at: [https://github.com/yashkant/Padam-Tensorflow](https://github.com/yashkant/Padam-Tensorflow)",17,1,False,self,,,,,
614,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,23,af80pj,self.MachineLearning,Deep learning generating Basketball plays survery,https://www.reddit.com/r/MachineLearning/comments/af80pj/deep_learning_generating_basketball_plays_survery/,MooMooM1lk,1547304647,[removed],0,1,False,self,,,,,
615,MachineLearning,t5_2r3gv,2019-1-12,2019,1,12,23,af82yw,self.MachineLearning,Is there any way to measure the quality of a word2vec representation?,https://www.reddit.com/r/MachineLearning/comments/af82yw/is_there_any_way_to_measure_the_quality_of_a/,vmgustavo,1547305109,[removed],0,1,False,self,,,,,
616,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,1,af8rfz,i.redd.it,GERENCIAMENTO de PROJETOS de OBRAS de ENGENHARIA VI,https://www.reddit.com/r/MachineLearning/comments/af8rfz/gerenciamento_de_projetos_de_obras_de_engenharia/,JamurGerloff,1547309683,,0,1,False,https://b.thumbs.redditmedia.com/nO-PI54kNdWLMn3-uJh0rdEvjL3j-GBkLO69BoyYR_g.jpg,,,,,
617,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,1,af90db,self.MachineLearning,[R] Spherical Payoff for Bayesian Networks,https://www.reddit.com/r/MachineLearning/comments/af90db/r_spherical_payoff_for_bayesian_networks/,harbinger_brazil,1547311165,"I want to implement the metric **Spherical Payoff** mentioned in both articles and Netica software to validate my bayesian network (through a test dataset), here are the formula that I got from my sources:

&amp;#x200B;

*Processing img ublvegzxs0a21...*

Both articles describe these elements as:

* **MOAC** = mean probability value of a given state averaged over all cases
* Pc = the predicted probability of the correct state
* Pj = the predicted probability of state j
* n = total number ofstates

My search through internet took me to: [https://en.wikipedia.org/wiki/Scoring\_rule](https://en.wikipedia.org/wiki/Scoring_rule), most specific in Examples of proper scoring rules topic, where I saw the exactly formula, but applied to just one case, and not a test dataset(a lot of cases)...

**My question is**: MOAC could be a mean from spherical payoffs through all my cases?(when I said cases, implies rows of my test dataset, samples of my test dataset...) and the formula would become this? Where in red is the MOAC assuption: 

&amp;#x200B;

https://i.redd.it/2pa0fo8zs0a21.png",0,1,False,self,,,,,
618,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,1,af97ge,self.MachineLearning,How Competitive Is A Masters in Applied Math Focused On Finance/Business For CS PhD Programs in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/af97ge/how_competitive_is_a_masters_in_applied_math/,Kyak787,1547312355,[removed],0,1,False,self,,,,,
619,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,3,af9z4y,self.MachineLearning,Forcing Latent Space of 1 vs all Classifiers,https://www.reddit.com/r/MachineLearning/comments/af9z4y/forcing_latent_space_of_1_vs_all_classifiers/,idg101,1547316871,[removed],0,1,False,self,,,,,
620,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,3,afa17g,self.MachineLearning,Is there any material regarding of decision making?,https://www.reddit.com/r/MachineLearning/comments/afa17g/is_there_any_material_regarding_of_decision_making/,uber_driver_ca,1547317226,[removed],0,1,False,self,,,,,
621,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,3,afaen0,m.medicalxpress.com,Machine learning is to better predict flu.,https://www.reddit.com/r/MachineLearning/comments/afaen0/machine_learning_is_to_better_predict_flu/,MistWeaver80,1547319488,,0,1,False,default,,,,,
622,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,4,afaqwl,self.MachineLearning,[D] Are external GPUs a worthwhile investment?,https://www.reddit.com/r/MachineLearning/comments/afaqwl/d_are_external_gpus_a_worthwhile_investment/,faithlesswonderboy,1547321623,"I'm an undergrad looking to run some deep learning experiments. In a previous class I had access to a supercomputing center, but I lost that when the class ended. I've heard of eGPUs and was wondering if it would be worth it for me. 

There's a post in this sub from two years ago that said they weren't, but maybe things have changed. I have a Windows desktop with a GTX 1060 6GB, and I have a 2014 Macbook Air with a thunderbolt connection. My friend is selling a GTX 1070 8GB for really cheap. It'd be nice if I could use it as an eGPU to run experiments from my desktop or laptop. My main concerns are 1) is thunderbolt fast enough? and 2) would it be cheaper to run it in the cloud somewhere?

I like the peace of mind of owning the setup, and my budget is a few hundred dollars. Thanks!",8,1,False,self,,,,,
623,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,4,afaxd0,self.MachineLearning,MC dropout for regression network,https://www.reddit.com/r/MachineLearning/comments/afaxd0/mc_dropout_for_regression_network/,IAlover,1547322732,[removed],0,1,False,self,,,,,
624,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,5,afb5ih,self.MachineLearning,[D] MIT Deep Learning Basics: Introduction and Overview,https://www.reddit.com/r/MachineLearning/comments/afb5ih/d_mit_deep_learning_basics_introduction_and/,UltraMarathonMan,1547324117,[removed],0,1,False,https://b.thumbs.redditmedia.com/841As5JpAUa31MZf1JwwFLtqsF9k5MIXJJxi7s8ZbaM.jpg,,,,,
625,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,5,afb8j0,self.MachineLearning,[D] MIT Deep Learning Basics: Introduction and Overview,https://www.reddit.com/r/MachineLearning/comments/afb8j0/d_mit_deep_learning_basics_introduction_and/,UltraMarathonMan,1547324637,"First lecture on Deep Learning Basics is up. It's humbling to have the opportunity to teach at MIT and exciting to be part of the AI community. If there are any topics you would like to see covered in depth in upcoming lectures, let me know: [https://www.youtube.com/watch?v=O5xeyoRL95U](https://www.youtube.com/watch?v=O5xeyoRL95U)

&amp;#x200B;

https://i.redd.it/te7vhu6hw1a21.png

* [Lecture video on YouTube](https://www.youtube.com/watch?v=O5xeyoRL95U) (and [Playlist](https://www.youtube.com/watch?v=O5xeyoRL95U&amp;list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf&amp;index=1))
* [Slides for the lecture (PDF)](https://www.dropbox.com/s/c0g3sc1shi63x3q/deep_learning_basics.pdf?dl=0)
* Website for the series: [https://deeplearning.mit.edu](https://deeplearning.mit.edu/)
* GitHub repo for tutorials: [https://github.com/lexfridman/mit-deep-learning](https://github.com/lexfridman/mit-deep-learning)

**Outline of the lecture:**

* Introduction
* Deep learning in one slide
* History of ideas and tools
* Simple example in TensorFlow
* TensorFlow in one slide
* Deep learning is representation learning
* Why deep learning (and why not)
* Challenges for supervised learning
* Key low-level concepts
* Higher-level methods
* Toward artificial general intelligence",36,1,False,https://b.thumbs.redditmedia.com/841As5JpAUa31MZf1JwwFLtqsF9k5MIXJJxi7s8ZbaM.jpg,,,,,
626,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,5,afbiw0,self.MachineLearning,[P] [R] How is Machine Learning being used in Datacenters?,https://www.reddit.com/r/MachineLearning/comments/afbiw0/p_r_how_is_machine_learning_being_used_in/,icemanthrowaway123,1547326115,"I code to assist Devops for a very large Datacenter and on the side took several machine learning classes. I love it and want to bring that excitement to the workplace, but am at a loss at how I'd do that.

**Have any of you used Machine Learning to help a Sysadmin or Datacenter team solve a problem?**

I know I'm a hammer looking for a nail, but any inspiration would be hugely appreciated!",0,1,False,self,,,,,
627,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,6,afbtzg,self.MachineLearning,Use TF Queue for Keras? (pipeline to feed data to GPU),https://www.reddit.com/r/MachineLearning/comments/afbtzg/use_tf_queue_for_keras_pipeline_to_feed_data_to/,xHipster,1547327916,[removed],0,1,False,self,,,,,
628,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,6,afbz7y,self.MachineLearning,A CS Major with competitive programming and jr. ReactJS Job needs guidance,https://www.reddit.com/r/MachineLearning/comments/afbz7y/a_cs_major_with_competitive_programming_and_jr/,tapu_buoy,1547328798,[removed],0,1,False,self,,,,,
629,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,6,afc54l,self.algotrading,Best Architecture for LSTM Network for Stock Prediction,https://www.reddit.com/r/MachineLearning/comments/afc54l/best_architecture_for_lstm_network_for_stock/,jdyr1729,1547329807,,0,1,False,https://b.thumbs.redditmedia.com/jh0Yy-aBXnEXwqnDajmYD2vdBj_drbVrLFF9BfADusI.jpg,,,,,
630,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,7,afc8u7,self.MachineLearning,"I have eye problems and want to make audiobooks from ebooks using google cloud wavenet API, I tried to follow the guide but couldn't get it working, can someone help me out and explain the guide better to me? Would be very nice thanks",https://www.reddit.com/r/MachineLearning/comments/afc8u7/i_have_eye_problems_and_want_to_make_audiobooks/,Master-Twango,1547330437,[removed],0,1,False,self,,,,,
631,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,7,afcu62,self.MachineLearning,Causal Inference Algorithms vs Pandas Groupby,https://www.reddit.com/r/MachineLearning/comments/afcu62/causal_inference_algorithms_vs_pandas_groupby/,techqs123,1547333991,[removed],0,1,False,self,,,,,
632,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,8,afdb0i,reddit.com,Collection of AI/ML competitions in r/ai_competitions,https://www.reddit.com/r/MachineLearning/comments/afdb0i/collection_of_aiml_competitions_in_rai/,pvl,1547336924,,0,1,False,default,,,,,
633,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,8,afde6p,i.imgur.com,Hair color transformation. How is this done?,https://www.reddit.com/r/MachineLearning/comments/afde6p/hair_color_transformation_how_is_this_done/,akaberto,1547337489,,0,1,False,https://b.thumbs.redditmedia.com/BSXXQhXvYliQHNChxcr_enBRi5m8LviSISyt9nk2ujM.jpg,,,,,
634,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,9,afdpxu,self.MachineLearning,Andrew Ng's Machine Learning Course Python Implementation (Part 2),https://www.reddit.com/r/MachineLearning/comments/afdpxu/andrew_ngs_machine_learning_course_python/,Beneblau,1547339500,"Continuing on from

[https://www.reddit.com/r/MachineLearning/comments/a9ykws/andrew\_ngs\_machine\_learning\_course\_python/](https://www.reddit.com/r/MachineLearning/comments/a9ykws/andrew_ngs_machine_learning_course_python/)

I had completed the series and here are the remaining articles.

Support Vector Machines: [https://medium.com/@ben\_lau93/andrew-ngs-machine-learning-course-in-python-support-vector-machines-435fc34b7bf9](https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-support-vector-machines-435fc34b7bf9)

Unsupervised Learning: [https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74](https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-kmeans-clustering-pca-b7ba6fafa74)

Anomaly Detection: [https://medium.com/@ben\_lau93/andrew-ngs-machine-learning-course-in-python-anomaly-detection-1233d23dba95](https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-anomaly-detection-1233d23dba95)

&amp;#x200B;

I hope to move on to more Kaggle competition or my own project now that I got the basic going. My plan forward is to complete [deeplearning.ai](https://deeplearning.ai) specialization in Coursera as well as the courses from [fast.ai](https://fast.ai). Thank the community for all your help and it is this sharing of knowledge that allows a non-technical person like me to venture into the field of data science.",0,1,False,self,,,,,
635,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,9,afdvma,self.MachineLearning,What's the current state of art in text to speech?,https://www.reddit.com/r/MachineLearning/comments/afdvma/whats_the_current_state_of_art_in_text_to_speech/,anagapetyan,1547340542,[removed],0,1,False,self,,,,,
636,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,10,afe871,self.MachineLearning,[D] CARGO 1143 TARANTULA,https://www.reddit.com/r/MachineLearning/comments/afe871/d_cargo_1143_tarantula/,AcassusoAlessioV,1547342843,[removed],18,1,False,self,,,,,
637,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,11,afenko,self.MachineLearning,Dive into Deep learning,https://www.reddit.com/r/MachineLearning/comments/afenko/dive_into_deep_learning/,agussell,1547345790,[removed],0,1,False,self,,,,,
638,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,11,afevaa,self.MachineLearning,[Q] Applied Math Master (Versus CS MS) for PhD in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/afevaa/q_applied_math_master_versus_cs_ms_for_phd_in/,Kyak787,1547347303,[removed],0,1,False,self,,,,,
639,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,12,aff20r,self.MachineLearning,[P] ML Alternatives For Causal Inference To Pandas Groupby For High Dimensional Data,https://www.reddit.com/r/MachineLearning/comments/aff20r/p_ml_alternatives_for_causal_inference_to_pandas/,techqs123,1547348586,"Hello Experts!

i have data where each row represents a single session of traffic. There are many different features - the source and destination IPs, application, ports, time, etc., Each row also has metrics for that session that have been gathered by a special calculation process that runs on the device the traffic is passing through. There are many tens of thousands of observations.

Or speaking high level, we already know how each session is performing, and a lot about it - and we have a lot of observations.

i have code that massages all the data into bins, vectors, does normalization, encoding, etc., - and i can feed it into a neural net and see the loss go way down as the neural net learns how to predict what i already know. But i am not really looking at making a prediction. i am looking for cause.

i am just curious how successful anyone has been with doing causal inference. i seem to be able to make general statements about the mean/median performance values by doing some fancy pandas groupby work - but that can be fairly labor intensive, and is limited to only the combinations i think of.

There are maybe 20 features or so. And i usually have vectors with something like 40 values max, after i encode the categorical variables. i've tried to avoid going higher than that by mapping low frequency labels to a single ""Rare"" identifier.

Does anyone have any thoughts, or especially code/module recommendations that would be helpful in high dimensional data?

Thanks very much in advance.",6,1,False,self,,,,,
640,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,12,aff6oq,self.MachineLearning,[D] Enforcing A Latent Space Prior for 1 vs all Classifiers,https://www.reddit.com/r/MachineLearning/comments/aff6oq/d_enforcing_a_latent_space_prior_for_1_vs_all/,idg101,1547349468,"I see so many people designing 1 vs all classifiers as binary classifiers using CNNs. Wouldnt this result in a latent space composed of 2 halfspaces, 1 for class A and 1 for everything else? This seems like a non-optimal solution as the size of the space compose of the class you are looking for is likely much smaller that the size of everything you aren't. This is like how the size of the integers is infinite, but much smaller than the size of rational numbers.



I would think you don't want the latent space to be 2 half spaces but more like a sphere located at the origin where the inside the sphere is the class you want, and everything else is outside of that. In this way ,you have enough ""room"" for everything you don't want to comfortable exist.



If what I say is true, then wouldnt you want to force the latent space to have the spherical shape i describe to get best performance?
",1,1,False,self,,,,,
641,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,12,affdk7,m.youtube.com,This Youtuber is Teaching Practical AI for free!,https://www.reddit.com/r/MachineLearning/comments/affdk7/this_youtuber_is_teaching_practical_ai_for_free/,updownvizzii,1547350775,,0,1,False,default,,,,,
642,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,12,affetq,self.MachineLearning,[D] How can I retrieve a mathematical/symbolic representation of the gradient derived by the autograd/autodiff modules in popular tensor frameworks?,https://www.reddit.com/r/MachineLearning/comments/affetq/d_how_can_i_retrieve_a_mathematicalsymbolic/,lmericle,1547351020,"I would like to see what the gradient on the loss looks like for a given computational graph. Ideally Einstein notation since we're often working with 3D+ tensors. Does a utility like this exist?

I understand that a large computational graph (eg modern neural network implementations) will have a massive equation to represent the gradient -- the graphs I work on are comparatively very small.",6,1,False,self,,,,,
643,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,12,affgxs,self.MachineLearning,Best major after graduating high school?,https://www.reddit.com/r/MachineLearning/comments/affgxs/best_major_after_graduating_high_school/,shidoarichimorin,1547351433,[removed],0,1,False,self,,,,,
644,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,15,afgime,self.MachineLearning,Loss function for probability distribution,https://www.reddit.com/r/MachineLearning/comments/afgime/loss_function_for_probability_distribution/,nckl,1547359226,[removed],0,1,False,self,,,,,
645,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,15,afgr2f,self.MachineLearning,Best solutions for implementing machine-vision deep learning algos into practical/commercial products.,https://www.reddit.com/r/MachineLearning/comments/afgr2f/best_solutions_for_implementing_machinevision/,jingquek,1547361150,[removed],0,1,False,self,,,,,
646,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,16,afh7ov,analyticsinsight.net,Manual to Object Detection with Machine Learning | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/afh7ov/manual_to_object_detection_with_machine_learning/,analyticsinsight,1547365461,,0,1,False,default,,,,,
647,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,17,afhjdm,self.MachineLearning,Is ML cancer research the stuff of Kaggle competitions or are there many jobs in this area?,https://www.reddit.com/r/MachineLearning/comments/afhjdm/is_ml_cancer_research_the_stuff_of_kaggle/,errminator,1547368814,"As per title. What would typical requirements be to work in this area?

Thanks ",0,1,False,self,,,,,
648,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,17,afhmsi,youtu.be,Change hair color in real time. Artificial intelligence can do anything!,https://www.reddit.com/r/MachineLearning/comments/afhmsi/change_hair_color_in_real_time_artificial/,cmillionaire9,1547369844,,0,1,False,https://b.thumbs.redditmedia.com/IDoNaSjiNH5g5S9uKhZvsZArDPi6kuedTkMGdukBlHM.jpg,,,,,
649,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,18,afhu16,self.MachineLearning,Example working with data for self-driving car in R,https://www.reddit.com/r/MachineLearning/comments/afhu16/example_working_with_data_for_selfdriving_car_in_r/,psygrm,1547372015,[removed],0,1,False,self,,,,,
650,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,19,afi5sf,quantamagazine.org,A New Approach to Understanding How Machines Think (Been Kim Is Building a Translator for Artificial Intelligence),https://www.reddit.com/r/MachineLearning/comments/afi5sf/a_new_approach_to_understanding_how_machines/,ai-lover,1547374975,,0,1,False,default,,,,,
651,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,19,afi8ms,youtube.com,LC0 32566 is a monster! | Leela Chess reacts wonderfully to Stockfish in...,https://www.reddit.com/r/MachineLearning/comments/afi8ms/lc0_32566_is_a_monster_leela_chess_reacts/,kingscrusher-youtube,1547375793,,0,1,False,https://b.thumbs.redditmedia.com/broXTu0wumflGpMK10-34B1Api8TACXSo34y0ndvuJw.jpg,,,,,
652,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,19,afiazm,self.MachineLearning,[P] Conditional Deep Convolutional GAN (CDCGAN) Keras implementation,https://www.reddit.com/r/MachineLearning/comments/afiazm/p_conditional_deep_convolutional_gan_cdcgan_keras/,gabegabe6,1547376462,"Hi all I thought I will share a cdcgan keras implementation where the model input is noise and a label embedding.

Implementation: [Github repo](https://github.com/gaborvecsei/CDCGAN-Keras)

Abstract model architecture: [image](https://github.com/gaborvecsei/CDCGAN-Keras/blob/master/art/cdcgan_abstract_model.png)

MNIST result images: [gif image](https://github.com/gaborvecsei/CDCGAN-Keras/blob/master/art/mnist_generated_per_epoch.gif)",4,1,False,self,,,,,
653,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,20,afih4x,amoned.tk,"TIL That Jackie Chan and his stunt team are blacklisted by all insurance companies, forcing him to pay for all costs out of his own pocket | Jackie voluntarily agreed to pay for the cost of the injuries sustained during a stunt for the entire life of the injured team member",https://www.reddit.com/r/MachineLearning/comments/afih4x/til_that_jackie_chan_and_his_stunt_team_are/,eslamalmahdy,1547378211,,0,1,False,https://b.thumbs.redditmedia.com/QmenPjEUX1-26dBpN4XZv5wTv_WpfcUWDdidLz6ELLk.jpg,,,,,
654,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,21,afiqnu,self.MachineLearning,[R] Watch Your Step: Learning Node Embeddings via Graph Attention (NeurIPS 2018).,https://www.reddit.com/r/MachineLearning/comments/afiqnu/r_watch_your_step_learning_node_embeddings_via/,benitorosenberg,1547380838,"&amp;#x200B;

https://i.redd.it/4po7hxazj6a21.jpg

PyTorch: [https://github.com/benedekrozemberczki/AttentionWalk](https://github.com/benedekrozemberczki/AttentionWalk)

Paper: [https://github.com/benedekrozemberczki/AttentionWalk/blob/master/paper.pdf](https://github.com/benedekrozemberczki/AttentionWalk/blob/master/paper.pdf)

Abstract:

Graph embedding methods represent nodes in a continuous vector space,  preserving different types of relational information from the graph.  There are many hyper-parameters to these methods (e.g. the length of a  random walk) which have to be manually tuned for every graph. In this  paper, we replace previously fixed hyper-parameters with trainable ones  that we automatically learn via backpropagation. In particular, we  propose a novel attention model on the power series of the transition  matrix, which guides the random walk to optimize an upstream objective.  Unlike previous approaches to attention models, the method that we  propose utilizes attention parameters exclusively on the data itself  (e.g. on the random walk), and are not used by the model for inference.  We experiment on link prediction tasks, as we aim to produce embeddings  that best-preserve the graph structure, generalizing to unseen  information. We improve state-of-the-art results on a comprehensive  suite of real-world graph datasets including social, collaboration, and  biological networks, where we observe that our graph attention model can  reduce the error by up to 20%-40%. We show that our  automatically-learned attention parameters can vary significantly per  graph, and correspond to the optimal choice of hyper-parameter if we  manually tune existing methods.",0,1,False,self,,,,,
655,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,21,afis3o,self.MachineLearning,Reducing push notifications sent by e-commerce websites to improve user experience,https://www.reddit.com/r/MachineLearning/comments/afis3o/reducing_push_notifications_sent_by_ecommerce/,uchihabloodline,1547381221,"I have got a project problem from my college to develop an method using any technology to reduce the push notification sent by apps.

&amp;#x200B;

The problem statement as follows:

 Developer would have to build a solution using Artificial Intelligence/ML/DL/Data science or some other mechanism to reduce the amount of push notifications sent by e-commerce apps. Currently push notifications are generally sent based on a fixed schedule or some trigger in most apps. This creates multiple notifications every week and irritates the user. The intent should be to only send notifications when the users intent is there to purchase a particular product. Sending push notifications or emails without any user intent to buy that category of product creates frustration to the user. So only when user has intention to buy something the notifications or emails should. The developer has to use a technical method to find that intent using big data and then send notifications or emails according to that method. 

&amp;#x200B;

What solution I have thought is that I can make a list of features from the websites like 

1)How often the buyer visits a particular product on the website

2)Did the buyer recently filled its wallet with some good amount, as it would mean he is going to buy something.

3)Does the buyer clicks on the ""Discount boards"" on the website.

etc...etc

&amp;#x200B;

By monitoring above activities of every customer I think I can make a generalization for many of the customers falling in the same category. 

Please suggest how I can do it. If anyone has some better idea to implement the solution please let me know

&amp;#x200B;

Note:- It is for my final major project which has a very near submission date.",0,1,False,self,,,,,
656,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,21,afiv2f,self.MachineLearning,Art dataset. Labeled by art styles,https://www.reddit.com/r/MachineLearning/comments/afiv2f/art_dataset_labeled_by_art_styles/,hidele,1547381988,[removed],0,1,False,self,,,,,
657,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,21,afizmu,self.MachineLearning,[D] What's DCGAN talking about not putting BatchNorm at the last layer of G and first layer of D?,https://www.reddit.com/r/MachineLearning/comments/afizmu/d_whats_dcgan_talking_about_not_putting_batchnorm/,quantizedmodel,1547383170,"Just to be sure this is what they meant right?

G(z) = Tanh(Conv(a^(l-1) ) correct
G(z) = Tanh(BN(Conv(a^(l-1) )) wrong

D(x) = D_otherlayers(LeakyRelu(Conv(x))) correct
D(x) = D_otherlayers(LeakyReu(BN(Conv)))) wrong

First of all why do they even advise the output layer not to use BN I thought BN was supposed to normalized the inputs to a layer so its weights can converge faster but the output layer is not an input to any layer so there's no weights that needs helping to converge even?

As far as I remember they seemed to only say doing those things above will cause sample oscillilations and did not explain further, it seems to be common knowledge. Can anyone tell they meant by samples oscillilating and why BN at those layers would cause it.",6,1,False,self,,,,,
658,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,21,afj262,self.MachineLearning,[D] What are some the labs that focus on DL/Telecom Industry,https://www.reddit.com/r/MachineLearning/comments/afj262/d_what_are_some_the_labs_that_focus_on_dltelecom/,__Julia,1547383823,"Hi,

I am trying to explore PhD programs in the area of Telecom industry (Network Management, Network Design and Optimization, Network Managed Services, IoT connectivity, Orchestration, 4G, 5G, Automated Network Operations, Cellular IoT, ) and I would like to know if you can list some conferences or research labs (academic or industrial) that focus on the use of deep learning/GANs to optimize their solutions. Most of the work I have seen uses (traditional) ML techniques and statistical methods to tackle the above-mentioned problems. 

In case you comes across a good survey, please share it. It would be much appreciated. I feel that this community scattered. I worked in NLP before (internships), and it was very easy to see how the community is focused.

Thanks in advance.",4,1,False,self,,,,,
659,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,22,afjfaz,self.MachineLearning,Going from Tensorflow on Python to TensorflowJS,https://www.reddit.com/r/MachineLearning/comments/afjfaz/going_from_tensorflow_on_python_to_tensorflowjs/,naifmeh,1547386955,[removed],0,1,False,self,,,,,
660,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,22,afjgdj,self.MachineLearning,Classifying names as male or female,https://www.reddit.com/r/MachineLearning/comments/afjgdj/classifying_names_as_male_or_female/,mrsunshyne,1547387198,"I'm completely new to ML and I wanted to get started with something really simple. 

&amp;#x200B;

I wan to train a model by giving it a list of female and male names. The application of this would then be having it output 'male' or 'female' after I input any name.

&amp;#x200B;

Where do i begin with this?",0,1,False,self,,,,,
661,MachineLearning,t5_2r3gv,2019-1-13,2019,1,13,23,afjqbz,self.MachineLearning,"Megashifts, AI and Why the Future is More Unknowable than Ever | Gerd Leonhard",https://www.reddit.com/r/MachineLearning/comments/afjqbz/megashifts_ai_and_why_the_future_is_more/,The_Syndicate_VC,1547389371,[removed],0,1,False,self,,,,,
662,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,0,afkhic,self.MachineLearning,how to use audio (200 features) and face data (10 features) to classify emotion (0: dispositive/1: positive)?,https://www.reddit.com/r/MachineLearning/comments/afkhic/how_to_use_audio_200_features_and_face_data_10/,alexchauncy,1547394509,[removed],0,1,False,https://b.thumbs.redditmedia.com/dnrYKM7AyywXIre7vSheUxUxTKthmyJBI3jgYNc4gyE.jpg,,,,,
663,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,1,afkojl,twitter.com,[N] Python machine learning is on the second tier of this bundle and it ends in about 24 hours,https://www.reddit.com/r/MachineLearning/comments/afkojl/n_python_machine_learning_is_on_the_second_tier/,LocDeMty,1547395777,,0,1,False,default,,,,,
664,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,1,afl04m,self.MachineLearning,"Is it possible to clone programs, more specifically video games, by machine learning?",https://www.reddit.com/r/MachineLearning/comments/afl04m/is_it_possible_to_clone_programs_more/,IpsumPanEst,1547397763,[removed],0,1,False,self,,,,,
665,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,1,afl3t1,self.MachineLearning,"[D] Machine Learning people, What are some things that you struggled learning when you were first starting out?",https://www.reddit.com/r/MachineLearning/comments/afl3t1/d_machine_learning_people_what_are_some_things/,TheUltraMerchant,1547398387,"There are 2-3 dozen ""Learn AI/Deep Learning/Whatever"" Tutorial videos/blogs/etc out there right now. Most of them are Bad, and almost all of them are aimed at people who already understand the basics of Stats, Calc, etc, and are already fluent in Linux, and Linux Jargon. In the past few months, I've graduated from ML Novice, to ML Amateur, and am starting to have actual fun with it. 

I spent most of that time just figuring out how someone even gets started. Things like ""Spinning Up with OpenAI"" and ""The Deep Learning Book"" have been helpful with the theory, but as far as actually getting a dev environment up and running, SpinningUp is still incomplete, and hasn't been updated in 2 months, and the DLB doesn't really talk about that. Eventually I figured it out, but I kept notes on the stuff I was struggling with, with the intent of maybe writing a short something to encourage a few of my friends to get into ML.

So what did you have trouble with when *you* were getting started, and how did you figure it out? If you were going to write a guide of some sort for people with only basic CS skills, what would you make sure to include, that other people might not, etc?",118,1,False,self,,,,,
666,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,2,aflfm0,self.MachineLearning,Rotary Die Cutting,https://www.reddit.com/r/MachineLearning/comments/aflfm0/rotary_die_cutting/,habmkloganjt,1547400209,[removed],0,1,False,self,,,,,
667,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,2,aflkos,self.MachineLearning,Pdf request(Grokking deep learning),https://www.reddit.com/r/MachineLearning/comments/aflkos/pdf_requestgrokking_deep_learning/,Aneervan,1547400955,[removed],0,1,False,self,,,,,
668,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,2,afllf5,self.MachineLearning,PC Build for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/afllf5/pc_build_for_deep_learning/,nsoifer,1547401068,[removed],0,1,False,self,,,,,
669,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,2,aflmdd,self.MachineLearning,How many related work do you read when reviewing papers,https://www.reddit.com/r/MachineLearning/comments/aflmdd/how_many_related_work_do_you_read_when_reviewing/,pigdogsheep,1547401218,"when reviewing scientific papers even you are expert in the topic there is always one citation or many that you haven't read them thoroughly before.

Do you usually download and read them thoroughly or not. Even you can judge the quality of the work without reading all the related work thoroughly.",0,1,False,self,,,,,
670,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,3,aflukg,self.MachineLearning,Extracting information from pdf files using NLP?,https://www.reddit.com/r/MachineLearning/comments/aflukg/extracting_information_from_pdf_files_using_nlp/,zarooricheck,1547402447,[removed],0,1,False,self,,,,,
671,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,3,afm1m0,self.MachineLearning,[P] Looking for a Model That Has Been Pre-Trained on the ADE20K Dataset,https://www.reddit.com/r/MachineLearning/comments/afm1m0/p_looking_for_a_model_that_has_been_pretrained_on/,redditferdays,1547403539,"I'm hoping to use a neural net that has been pre-trained on the [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) dataset for a project I'm working on. I am planning to train [DilatedNet](https://github.com/nicolov/segmentation_keras) myself, unless there is an existing alternative. I googled around and wasn't able to find anything. 

&amp;#x200B;

Does anyone know of a pre-trained model I could use, or a good place to look for something like this?",2,1,False,self,,,,,
672,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,3,afm1mp,self.MachineLearning,Production caveat question,https://www.reddit.com/r/MachineLearning/comments/afm1mp/production_caveat_question/,drivebydryhumper,1547403542,[removed],0,1,False,self,,,,,
673,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,3,afm43q,appliedai.co.uk,"Applied AI Conference - March 1st: covering testing for bias, databricks, AutoML, tensorflow with Go, devops for AI and more",https://www.reddit.com/r/MachineLearning/comments/afm43q/applied_ai_conference_march_1st_covering_testing/,appliedaiconf,1547403914,,0,1,False,https://b.thumbs.redditmedia.com/ABLs33UFaqCPu61f9YhWKlTWSCVzbs0xytJOlyyvXDk.jpg,,,,,
674,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,4,afmo2r,self.MachineLearning,What steps should I take to put another network architecture such as Alexnet in feature extraction part of YOLO?,https://www.reddit.com/r/MachineLearning/comments/afmo2r/what_steps_should_i_take_to_put_another_network/,elena_eng,1547406961,[removed],0,1,False,self,,,,,
675,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,5,afn7bg,self.MachineLearning,[P] PyTorch implementations of RL Algorithms,https://www.reddit.com/r/MachineLearning/comments/afn7bg/p_pytorch_implementations_of_rl_algorithms/,__data_science__,1547409945,"For anyone trying to learn or practice RL, here's a repo with lots of working implementations of RL algorithms like DQN, DQN-HER, Double DQN, Genetic algorithm, REINFORCE, DDPG, DDPG-HER, PPO etc.. that solve lots of different games

Let me know what you think and if you want to contribute!

[https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch)",0,1,False,self,,,,,
676,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,5,afnm9a,self.MachineLearning,"Digital Twins, Machine Learning &amp; AI",https://www.reddit.com/r/MachineLearning/comments/afnm9a/digital_twins_machine_learning_ai/,andrea_manero,1547412195,[removed],0,1,False,self,,,,,
677,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,6,afo4j5,self.MachineLearning,"[D] We are seeing a shift toward more AI control by governments, will it slow down research ?",https://www.reddit.com/r/MachineLearning/comments/afo4j5/d_we_are_seeing_a_shift_toward_more_ai_control_by/,actuia,1547414861,[removed],0,1,False,self,,,,,
678,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,7,afokkr,self.MachineLearning,Anybody know how DeadLetter (deadletter.io) works?,https://www.reddit.com/r/MachineLearning/comments/afokkr/anybody_know_how_deadletter_deadletterio_works/,Smayy12,1547417334,[removed],0,1,False,self,,,,,
679,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,7,afomip,self.embedded,Artificial Neural Networks for Embedded applications with STM32Cube.AI software tool,https://www.reddit.com/r/MachineLearning/comments/afomip/artificial_neural_networks_for_embedded/,prastus,1547417648,,0,1,False,default,,,,,
680,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,9,afpoie,self.MachineLearning,Udacity Nano-degree Recommendations,https://www.reddit.com/r/MachineLearning/comments/afpoie/udacity_nanodegree_recommendations/,Wandering_Fern,1547424121,[removed],0,1,False,self,,,,,
681,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,10,afqjyc,self.MachineLearning,To jupyter notebook or to not jupyter notebook?,https://www.reddit.com/r/MachineLearning/comments/afqjyc/to_jupyter_notebook_or_to_not_jupyter_notebook/,ogneuroengineer,1547429690,[removed],0,1,False,self,,,,,
682,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,11,afqxk3,self.MachineLearning,[D] Truncated backpropogation through time question.,https://www.reddit.com/r/MachineLearning/comments/afqxk3/d_truncated_backpropogation_through_time_question/,throwohhaimark2,1547432184,"As far as I can tell, there are a number of styles of truncated BPTT. There's the default Tensorflow version which computes n timesteps and then backpropogates n timesteps. This means weights are updated every n timesteps. Then there's the ""pure"" version that computes a timestep then backpropogates n timesteps, meaning weights are updated every single timestep. This is a pretty popular write up on this:

https://r2rt.com/styles-of-truncated-backpropagation.html

There is also versions that have two parameters k1 and k2 where k1 is how often you backpropogate (forward step size) and k2 is how far back you backpropogate (unroll amount). 

**My question is this**: in the style of truncated BPTT where the unroll amount is bigger than the forward step size, don't you now have to remember old weights? Since you're looking back into the past to when your parameters were different, doesn't this mean you have to use those old values in your gradient calculation? I feel like this must be the case but would really complicate things in Tensorflow since you now have to have multiple copies of the weights. ",4,1,False,self,,,,,
683,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,13,afrujl,self.MachineLearning,Do you really care about interpreting your model?,https://www.reddit.com/r/MachineLearning/comments/afrujl/do_you_really_care_about_interpreting_your_model/,ernestright,1547438450,[removed],0,1,False,self,,,,,
684,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,13,afs21h,medium.com,"Here are some of the comparison, I made of pytorch, and numpy. Hope this gives some inisights.",https://www.reddit.com/r/MachineLearning/comments/afs21h/here_are_some_of_the_comparison_i_made_of_pytorch/,gostewpid,1547439915,,0,1,False,https://b.thumbs.redditmedia.com/-dPPkT0WIJIilqND4Ub53uq0VsFnUzjWXZVcMeAxDaI.jpg,,,,,
685,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,14,afsd6m,github.com,"[P] SOD - An Embedded, OpenCV Alternative",https://www.reddit.com/r/MachineLearning/comments/afsd6m/p_sod_an_embedded_opencv_alternative/,histoire_guy,1547442206,,0,1,False,default,,,,,
686,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,14,afskkc,self.MachineLearning,[R] Variational Capsules,https://www.reddit.com/r/MachineLearning/comments/afskkc/r_variational_capsules/,akhandait,1547443715,What do you think about the paper [https://arxiv.org/abs/1807.04099](https://arxiv.org/abs/1807.04099) \- Variational Capsules for Image Analysis and Synthesis? The idea is really interesting but I found absolutely nothing about it on the internet.,4,1,False,self,,,,,
687,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,14,afskln,self.MachineLearning,How to select beard in image in increase or decrease growth,https://www.reddit.com/r/MachineLearning/comments/afskln/how_to_select_beard_in_image_in_increase_or/,UnlikelyClue,1547443722,[removed],0,1,False,self,,,,,
688,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,14,afsssz,physicsworld.com,Machine learning provides insight into the human brain,https://www.reddit.com/r/MachineLearning/comments/afsssz/machine_learning_provides_insight_into_the_human/,MistWeaver80,1547445559,,0,1,False,https://b.thumbs.redditmedia.com/BCa7yVEsBlBCoOtnTXqQfMYCDNl9MbDeV0i0Mq8C-1Q.jpg,,,,,
689,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,16,aftgb2,self.MachineLearning,Machine learning - New Academic Project - Suggestions Solicited,https://www.reddit.com/r/MachineLearning/comments/aftgb2/machine_learning_new_academic_project_suggestions/,hsharma12,1547451150,[removed],0,1,False,self,,,,,
690,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,16,afti3b,self.MachineLearning,How do you store the trained parameters of a network?,https://www.reddit.com/r/MachineLearning/comments/afti3b/how_do_you_store_the_trained_parameters_of_a/,TanktopSamurai,1547451622,[removed],0,1,False,self,,,,,
691,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,18,aftztb,self.MachineLearning,Book: Python Machine Learning Blueprints,https://www.reddit.com/r/MachineLearning/comments/aftztb/book_python_machine_learning_blueprints/,andrea_manero,1547456415,[removed],0,1,False,self,,,,,
692,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,18,afu269,self.MachineLearning,Guidance for starting Projects,https://www.reddit.com/r/MachineLearning/comments/afu269/guidance_for_starting_projects/,Ginkobab,1547457045,[removed],0,1,False,self,,,,,
693,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,18,afubyc,self.MachineLearning,"""Artificial intelligence, bias and clinical safety"" - review in BMJ Quality and Safety",https://www.reddit.com/r/MachineLearning/comments/afubyc/artificial_intelligence_bias_and_clinical_safety/,rc538,1547459714,[removed],0,1,False,self,,,,,
694,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,19,afuds2,slideshare.net,Machine Learning Trends 2019,https://www.reddit.com/r/MachineLearning/comments/afuds2/machine_learning_trends_2019/,credibll,1547460187,,0,1,False,default,,,,,
695,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,19,afueoy,self.MachineLearning,"[R] ""Artificial intelligence, bias and clinical safety"" review in BMJ Quality and Safety",https://www.reddit.com/r/MachineLearning/comments/afueoy/r_artificial_intelligence_bias_and_clinical/,rc538,1547460420,[removed],0,1,False,self,,,,,
696,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,19,afumf0,self.MachineLearning,Machine Learning Trends 2019!,https://www.reddit.com/r/MachineLearning/comments/afumf0/machine_learning_trends_2019/,credibll,1547462402,[removed],0,1,False,spoiler,,,,,
697,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,19,afums3,self.MachineLearning,"[P] AiFiddle: a Web GUI to Build, Visualise and Share Deep Learning Models",https://www.reddit.com/r/MachineLearning/comments/afums3/p_aifiddle_a_web_gui_to_build_visualise_and_share/,0mbre,1547462492,"Hi guys, 

For a few months now I've been working on a web GUI to build, visualise, train and share deep neural models.Encouraged by the positive feedback that I received from this community when I posted preview, I've continued developing it and I now think it's ready for a beta release.

Currently the tool support:

* Fully connected and Convolutional architecture
* Cloud and local, saving / loading of models
* Edit / delete layers 
* Visualise Convolutional layers filters 
* Freeze / Unfreeze layers 
* More datasets: Fashion MNIST, QuickDraw(10 and 30) 

Also coming soon:

* Custom dataset
* Export to Notebook
* Import h5 models

The editor can be found here: [https://aifiddle.io](https://aifiddle.io/).  


Your feedback, ideas, suggestions are greatly useful, so please dont hesitate to share them. 

  
Thanks a ton,   
Emmanuel ",35,1,False,self,,,,,
698,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,19,afup5p,self.MachineLearning,Where best to take master of machine learning for people without degree in computer science?,https://www.reddit.com/r/MachineLearning/comments/afup5p/where_best_to_take_master_of_machine_learning_for/,Akarapat82,1547463139,[removed],0,1,False,self,,,,,
699,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,20,afut1i,self.artificial,Menace - An AI-based Tic Tac Toe in python,https://www.reddit.com/r/MachineLearning/comments/afut1i/menace_an_aibased_tic_tac_toe_in_python/,Hsankesara,1547464139,,0,1,False,default,,,,,
700,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,20,afuzuk,visionetsystems.com,How is machine learning used in finance?,https://www.reddit.com/r/MachineLearning/comments/afuzuk/how_is_machine_learning_used_in_finance/,iammarksmith,1547465836,,0,1,False,default,,,,,
701,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,21,afv6gc,multisoftvirtualacademy.com,Step Up Your Career Ladder With The Help of Machine Learning Online Courses - MVA Blog,https://www.reddit.com/r/MachineLearning/comments/afv6gc/step_up_your_career_ladder_with_the_help_of/,multisoftmva0,1547467530,,0,1,False,default,,,,,
702,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,21,afv7u7,self.MachineLearning,[R] Watch Your Step: Learning Node Embeddings via Graph Attention (NeurIPS 2018),https://www.reddit.com/r/MachineLearning/comments/afv7u7/r_watch_your_step_learning_node_embeddings_via/,benitorosenberg,1547467845,"&amp;#x200B;

https://i.redd.it/7gubodrpqda21.jpg

Paper: [https://github.com/benedekrozemberczki/AttentionWalk/blob/master/paper.pdf](https://github.com/benedekrozemberczki/AttentionWalk/blob/master/paper.pdf)

Python: [https://github.com/benedekrozemberczki/AttentionWalk](https://github.com/benedekrozemberczki/AttentionWalk)

Abstract:

Graph embedding methods represent nodes in a continuous vector space,  preserving different types of relational information from the graph.  There are many hyper-parameters to these methods (e.g. the length of a  random walk) which have to be manually tuned for every graph. In this  paper, we replace previously fixed hyper-parameters with trainable ones  that we automatically learn via backpropagation. In particular, we  propose a novel attention model on the power series of the transition  matrix, which guides the random walk to optimize an upstream objective.  Unlike previous approaches to attention models, the method that we  propose utilizes attention parameters exclusively on the data itself  (e.g. on the random walk), and are not used by the model for inference.  We experiment on link prediction tasks, as we aim to produce embeddings  that best-preserve the graph structure, generalizing to unseen  information. We improve state-of-the-art results on a comprehensive  suite of real-world graph datasets including social, collaboration, and  biological networks, where we observe that our graph attention model can  reduce the error by up to 20%-40%. We show that our  automatically-learned attention parameters can vary significantly per  graph, and correspond to the optimal choice of hyper-parameter if we  manually tune existing methods.",1,1,False,self,,,,,
703,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,21,afv7wq,endtoend.ai,"[D] RL Weekly 4: Generating Problems with Solutions, Optical Flow with RL, and Model-free Planning",https://www.reddit.com/r/MachineLearning/comments/afv7wq/d_rl_weekly_4_generating_problems_with_solutions/,seungjaeryanlee,1547467858,,0,1,False,https://b.thumbs.redditmedia.com/8QtmFhVNOohCzMn6Gp4uiup5-Sgfd_qmz02JMAFwGHI.jpg,,,,,
704,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,22,afw15v,deeplearningio.com,Neural Networks and Deep Learning- II : Binary Classification | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/afw15v/neural_networks_and_deep_learning_ii_binary/,mrcgllr,1547474191,,0,1,False,default,,,,,
705,MachineLearning,t5_2r3gv,2019-1-14,2019,1,14,23,afwdwv,self.MachineLearning,[P] Neural ODEs and Adversarial Attacks,https://www.reddit.com/r/MachineLearning/comments/afwdwv/p_neural_odes_and_adversarial_attacks/,MindSustenance,1547476623,"My latest blog post summarizing the Neural ODE paper, and showing the results of some of my experiments playing around with integration times, adversarial robustness and its relation to the dynamics of the neural ODE:

https://rajatvd.github.io/Neural-ODE-Adversarial/

I analyse how adversarial attacks affect neural ODEs, and experiment with changing the range of integrations times for ODE solving. Larger integration times with all other hyper-parameters fixed consistently improves adversarial robustness. 

I also adversarially train a neural ODE and observe interesting effects related to equilibria of the ODE. The blog post has all the details. I'd love to hear your feedback and comments!

The neural ODE paper: https://arxiv.org/abs/1806.07366

Code: https://github.com/rajatvd/NeuralODE

Neat package from the authors for differentiable ODE solvers: https://github.com/rtqichen/torchdiffeq

",5,1,False,self,,,,,
706,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,0,afwnb7,self.MachineLearning,"Sensors and ""Expressors"" for Deep Learning/AI",https://www.reddit.com/r/MachineLearning/comments/afwnb7/sensors_and_expressors_for_deep_learningai/,HeleneLavoix,1547478401,[removed],0,1,False,self,,,,,
707,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,0,afwq47,self.MachineLearning,"[N] Awesome AI papers, with tips, news and reviews [with codes!] on Computer Vision News. Links for free reading!",https://www.reddit.com/r/MachineLearning/comments/afwq47/n_awesome_ai_papers_with_tips_news_and_reviews/,Gletta,1547478883,"Hot off the Press! Here are the links to the January 2019 issue of **Computer Vision News**, the magazine of the algorithm community published by RSIP Vision: many articles about Artificial Intelligence and free subscription for all on page 32.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019January/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-january-pdf/)

Enjoy!

&amp;#x200B;

https://i.redd.it/s4kwk9vhnea21.jpg",0,1,False,https://b.thumbs.redditmedia.com/1hbqMU2-p6sWKc_qhZUUNxvC7lLUfFSoO9pjCWrRpTQ.jpg,,,,,
708,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,0,afx08s,pyimagesearch.com,Incredibly thorough post on machine learning with python,https://www.reddit.com/r/MachineLearning/comments/afx08s/incredibly_thorough_post_on_machine_learning_with/,Coffenpaint,1547480641,,0,1,False,default,,,,,
709,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,0,afx562,self.MachineLearning,[R] Sample Efficient Adaptive Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/afx562/r_sample_efficient_adaptive_texttospeech/,yannisassael,1547481454,"Here are samples from our latest work on text to speech voice synthesis with few data (5 / 10 minutes of speech). We introduce and benchmark three different strategies to rapidly adapt to new speakers from the LibriSpeech and VCTK datasets.

Samples: [https://sample-efficient-adaptive-tts.github.io/demo/](https://sample-efficient-adaptive-tts.github.io/demo/)

ICLR 2019: [https://openreview.net/forum?id=rkzjUoAcFX](https://openreview.net/forum?id=rkzjUoAcFX)",5,1,False,self,,,,,
710,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,1,afx76n,pyimagesearch.com,Machine learning in python,https://www.reddit.com/r/MachineLearning/comments/afx76n/machine_learning_in_python/,auraham,1547481779,,0,1,False,https://a.thumbs.redditmedia.com/Mh7NQMX7ihV0J-21T6s8XU5f458yAL6BS8zgSSJnCK4.jpg,,,,,
711,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,1,afxb04,self.MachineLearning,Image Classification in 40 lines of Code,https://www.reddit.com/r/MachineLearning/comments/afxb04/image_classification_in_40_lines_of_code/,prakhar21,1547482397,[removed],0,1,False,self,,,,,
712,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,1,afxd5e,medium.com,"Cognitive Services API  I need your clothes, boots and your motorcycle (Article)",https://www.reddit.com/r/MachineLearning/comments/afxd5e/cognitive_services_api_i_need_your_clothes_boots/,Fewthp,1547482751,,0,1,False,default,,,,,
713,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,1,afxhgm,self.MachineLearning,[P/D] Learning to trade a portfolio with RL,https://www.reddit.com/r/MachineLearning/comments/afxhgm/pd_learning_to_trade_a_portfolio_with_rl/,TalkingJellyFish,1547483446,[removed],0,1,False,self,,,,,
714,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,1,afxknc,self.MachineLearning,[R] What's the current state of art in text to speech?,https://www.reddit.com/r/MachineLearning/comments/afxknc/r_whats_the_current_state_of_art_in_text_to_speech/,anagapetyan,1547483953,"I've read about WaveNet. I am amazed at their result, but this is a rather old article.

What's the current state of art? Which papers should I read?",4,1,False,self,,,,,
715,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,2,afxu9g,self.MachineLearning,[D] Image classification on an MNIST dream,https://www.reddit.com/r/MachineLearning/comments/afxu9g/d_image_classification_on_an_mnist_dream/,UltraMarathonMan,1547485554,"New tutorial is up on our [GitHub repo](https://github.com/lexfridman/mit-deep-learning) featuring image classification demonstrated on a beautiful MNIST dream that [@hardmaru once had](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/). I think it's an interesting twist on introducing beginners to image classification by testing the classifier on generate images outside the MNIST dataset. Result of the tutorial:

![video](kpnmz2f37fa21)

&amp;#x200B;",0,1,False,self,,,,,
716,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,2,afxvar,self.MachineLearning,On appropriate software for ML.,https://www.reddit.com/r/MachineLearning/comments/afxvar/on_appropriate_software_for_ml/,DiogenicOrder,1547485721,"TL;DR : What software for optimal hardware usage? 

Hello everyone, I'm writing you because I'm in need of software advice.

I'm currently using a Windows 10 laptop, though not powerful it's served me well. 

I'm having some issues though because many libraries were developed on GNU/Linux systems and as such it can be a real pain to install the libraries I want. So I fiddle with different compilers and end up breaking my environments. (*Don't worry I do this in virtual envs only, no main OS was hurt in the process*).

In terms of efficient hardware usage, I know that [Anaconda uses Intel MKL](https://docs.anaconda.com/mkl-optimizations/) optimization.

As such, I was wondering if it was worth using [Clear Linux](https://clearlinux.org/) which is Intel's own Linux distro on my machine and advertised to use hardware as well as possible.

[The benchmarking seems to prove it right](https://www.phoronix.com/scan.php?page=article&amp;item=clear-faster-blas&amp;num=2). 

Though this is on nice hardware and not on laptop CPU for plebe like myself.

The end goal is to be able to use my hardware as well as possible while at the same time having a nice Linux experience.

Would you recommend? What would you advise? 

Thanks",0,1,False,self,,,,,
717,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,2,afy0l7,self.MachineLearning,[D] What software is most appropriate for ML?,https://www.reddit.com/r/MachineLearning/comments/afy0l7/d_what_software_is_most_appropriate_for_ml/,DiogenicOrder,1547486584,"TL;DR : What software for optimal hardware usage? 

Hello everyone, I'm writing you because I'm in need of software advice.

I'm currently using a Windows 10 laptop, though not powerful it's served me well. 

I'm having some issues though because many libraries were developed on GNU/Linux systems and as such it can be a real pain to install the libraries I want. So I fiddle with different compilers and end up breaking my environments. (*Don't worry I do this in virtual envs only, no main OS was hurt in the process*).

In terms of efficient hardware usage, I know that [Anaconda uses Intel MKL](https://docs.anaconda.com/mkl-optimizations/) optimization.

As such, I was wondering if it was worth using [Clear Linux](https://clearlinux.org/) which is Intel's own Linux distro on my machine and advertised to use hardware as well as possible.

[The benchmarking seems to prove it right](https://www.phoronix.com/scan.php?page=article&amp;item=clear-faster-blas&amp;num=2). 

Though this is on nice hardware and not on laptop CPU for plebe like myself.

The end goal is to be able to use my hardware as well as possible while at the same time having a nice Linux experience.

Would you recommend? What would you advise? 

Thanks",11,1,False,self,,,,,
718,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,2,afy3fk,self.MachineLearning,[N] The Hundred-Page Machine Learning Book is now available on Amazon,https://www.reddit.com/r/MachineLearning/comments/afy3fk/n_the_hundredpage_machine_learning_book_is_now/,RudyWurlitzer,1547487022,"This long-awaited day has finally come and I'm proud and happy to announce that The Hundred-Page Machine Learning Book is now available to order on Amazon in a [high-quality color paperback](https://www.amazon.com/dp/199957950X/) edition as well as a [Kindle](https://www.amazon.com/Hundred-Page-Machine-Learning-Book-ebook/dp/B07MGCNKXB/) edition.

For the last three months, I worked hard to write a book that will make a difference. I firmly believe that I succeeded. I'm so sure about that because I received dozens of positive feedback. Both from readers who just start in artificial intelligence and from respected industry leaders.

I'm extremely proud that such best-selling AI book authors and talented scientists as Peter Norvig and Aurlien Gron endorsed my book and wrote the texts for its back cover and that Gareth James wrote the Foreword. 

This book wouldn't be of such high quality without the help of volunteering readers who sent me hundreds of text improvement suggestions. The names of all volunteers can be found in the Acknowledgments section of the book.

It is and will always be a ""read first, buy later"" book. This means [you can read it entirely](http://themlbook.com/wiki/) before buying it.",85,1,False,self,,,,,
719,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,2,afy6e3,self.MachineLearning,[D] Image classification on an MNIST dream,https://www.reddit.com/r/MachineLearning/comments/afy6e3/d_image_classification_on_an_mnist_dream/,UltraMarathonMan,1547487495,"New tutorial is up on our [GitHub repo](https://github.com/lexfridman/mit-deep-learning) featuring image classification demonstrated on a beautiful MNIST dream that [@hardmaru once had](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/). I think it's an interesting twist on introducing beginners to image classification by testing the classifier on generate images outside the MNIST dataset. Result of the tutorial:

![video](ze7k3yfkbfa21)

I especially like looking at the transition, when one number becomes another. It's a nice visual way to explore what are the essential elements that distinguish one number vs another, both for us humans and for neural networks.

**1 vs 3:**

![img](xwwvt4bscfa21)

**4 vs 9:**

![img](52zikjg0dfa21)

**5 vs 9:**

![img](xnfmcpt0dfa21)

**8 vs 9:**

![img](s40v57o1dfa21)",0,1,False,https://b.thumbs.redditmedia.com/Ujg8YQ5nZWVg0PrPJNXQzHRkicp-bFFeZlEFrdh3A-Y.jpg,,,,,
720,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,2,afy6yh,git.io,[R] Code for Black-Box Adversarial Attacks with Bandit Optimization,https://www.reddit.com/r/MachineLearning/comments/afy6yh/r_code_for_blackbox_adversarial_attacks_with/,andrew_ilyas,1547487585,,0,1,False,default,,,,,
721,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,3,afystl,github.com,I have invented a neural layer to learn the integral of a function given the input/output samples of the function,https://www.reddit.com/r/MachineLearning/comments/afystl/i_have_invented_a_neural_layer_to_learn_the/,mammalwong,1547491025,,0,1,False,default,,,,,
722,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,3,afyti3,self.MachineLearning,Where to start machine learning,https://www.reddit.com/r/MachineLearning/comments/afyti3/where_to_start_machine_learning/,NightKnight202,1547491129,[removed],0,1,False,self,,,,,
723,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,4,afz2ew,self.MachineLearning,[deep reinforcement learning] how to policy gradient method in online setting?,https://www.reddit.com/r/MachineLearning/comments/afz2ew/deep_reinforcement_learning_how_to_policy/,qudcjf7928,1547492531,[removed],0,1,False,self,,,,,
724,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,4,afzhxb,self.MachineLearning,[D] Does anybody here know about any projects which leverage machine learning to mine computational knowledge graphs from raw text?,https://www.reddit.com/r/MachineLearning/comments/afzhxb/d_does_anybody_here_know_about_any_projects_which/,mischief_23,1547494967,,9,1,False,self,,,,,
725,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,4,afzn4c,self.MachineLearning,Learn more about bot building patterns,https://www.reddit.com/r/MachineLearning/comments/afzn4c/learn_more_about_bot_building_patterns/,Recast-AI,1547495788,Take your bot to the next level and make it more efficient as well as host your own backend in the SAP Cloud Platform with this tutorial: [**https://recast.ai/blog/learn-advanced-bot-building-patterns-authentication-and-hosting-sap-cloud-platform/**](https://recast.ai/blog/learn-advanced-bot-building-patterns-authentication-and-hosting-sap-cloud-platform/),0,1,False,self,,,,,
726,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,4,afznao,self.MachineLearning,Need suggestion for the best and short course on Machine Learning.,https://www.reddit.com/r/MachineLearning/comments/afznao/need_suggestion_for_the_best_and_short_course_on/,vaibhavs_,1547495814,,0,1,False,self,,,,,
727,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,5,afzrx3,self.MachineLearning,"Unsupervised clustering of ""radial"" data?",https://www.reddit.com/r/MachineLearning/comments/afzrx3/unsupervised_clustering_of_radial_data/,life2vec,1547496522,[removed],0,1,False,self,,,,,
728,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,5,ag08j9,self.MachineLearning,[P] SOD: An Embedded OpenCV Alternative,https://www.reddit.com/r/MachineLearning/comments/ag08j9/p_sod_an_embedded_opencv_alternative/,histoire_guy,1547499131,"Hey All,

We are pleased to announce a new release of the SOD library. SOD is an embedded, modern cross-platform computer vision and machine learning software library that expose a set of APIs for deep-learning, advanced media analysis &amp; processing including real-time, multi-class object detection and model training on embedded systems with limited computational resource and IoT devices.

The project homepage &amp; documentation are available to consult at:
https://sod.pixlab.io

The Github repository:
https://github.com/symisc/sod",4,1,False,self,,,,,
729,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,6,ag0n7g,youtube.com,Andrew Ng on Building a Career in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ag0n7g/andrew_ng_on_building_a_career_in_machine_learning/,chaturacks,1547501385,,0,1,False,https://b.thumbs.redditmedia.com/RP5ckzyXpyoq1F7WJp4LHcCpFdQ3zlNIjiG3L-5a2kQ.jpg,,,,,
730,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,6,ag0v6y,self.MachineLearning,Training YOLOv3 : Deep Learning based Custom Object Detector,https://www.reddit.com/r/MachineLearning/comments/ag0v6y/training_yolov3_deep_learning_based_custom_object/,sunitanyk,1547502591,[removed],0,1,False,self,,,,,
731,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,7,ag1a70,blog.bitsrc.io,Reading Challenge: Heap-Sort in Go!,https://www.reddit.com/r/MachineLearning/comments/ag1a70/reading_challenge_heapsort_in_go/,JSislife,1547504895,,0,1,False,https://b.thumbs.redditmedia.com/dQtmyogcghcgoLcjwU_EEypcxl8-ove5iDm_F-0pb1g.jpg,,,,,
732,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,7,ag1ad7,pikabu.ru,[D] Any Russian speakers here? There's a great online community for machine learning I just discovered!,https://www.reddit.com/r/MachineLearning/comments/ag1ad7/d_any_russian_speakers_here_theres_a_great_online/,sugarhilldt2,1547504922,,0,1,False,default,,,,,
733,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,7,ag1knv,self.MachineLearning,No learnable parameters in last layer?,https://www.reddit.com/r/MachineLearning/comments/ag1knv/no_learnable_parameters_in_last_layer/,lume_,1547506624,I have a vague memory of seeing this at some poster at NeurIPS. The last weight matrix of a network is only used to reshape the data into its task specific form and can thus be a constant random matrix. The previous layers do all necessary transformations to account for the frozen final layer. Does anyone have a reference where they do something like this?,0,1,False,self,,,,,
734,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,8,ag1mi2,self.MachineLearning,GTX970 &gt; I9 9900K,https://www.reddit.com/r/MachineLearning/comments/ag1mi2/gtx970_i9_9900k/,Remideza,1547506921,[removed],0,1,False,self,,,,,
735,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,8,ag1my3,nature.com,"""Designing neural networks through neuroevolution"", Stanley et al 2019 {Uber/Sentient} [review]",https://www.reddit.com/r/MachineLearning/comments/ag1my3/designing_neural_networks_through_neuroevolution/,yazriel0,1547506990,,1,1,False,https://b.thumbs.redditmedia.com/HdS4-gkd31RveGh89AsUK9zt-FHq_dkN0dt7kVxmTQw.jpg,,,,,
736,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,9,ag28su,self.MachineLearning,[P] Undergraduate Convolutional Neural Network Project Ideas,https://www.reddit.com/r/MachineLearning/comments/ag28su/p_undergraduate_convolutional_neural_network/,A-Walker,1547510666,"Hi everyone, 

I'm an undergraduate student studying Computer Science. As part of my final year of study I'm required to complete a large project in the area of my choice. I've had experience working with logistic regression as an intern and I've worked with neural networks for personal projects and I've really enjoyed it so I wanted to complete a project in this area.

One area that has really interested me is using Convolutional Neural Networks. I've done some research and I'm really excited to work on a project in this area. 

The only issue I'm having is what ""problem"" to solve with a CNN. From my research, companies really like working with facial recognition so I've been thinking about something in this area. My ideas so far are:

* Person recognition - Recognise a person in an image, an output node for each person the network is trained to detect.  The application could be used for a secure method of accessing a system, tracking movement of people around a secure building etc.. 
* Mood detection - The CNN has an output node for each mood, the network will be able to predict the mood of the person in the input image. Although I'm not sure what this would be useful for... 
* Driver distraction detection - Predict whether a driver is paying attention to the road, sound an alarm if the driver looks away for too long

I'm struggling to think of any better applications of the software. If anyone has any ideas of what I could use this for I'd be very grateful!

Thanks in advance! ",10,1,False,self,,,,,
737,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,9,ag2pfl,dbdesigner.net,DB Designer - Online Database Design &amp; Modeling Tool,https://www.reddit.com/r/MachineLearning/comments/ag2pfl/db_designer_online_database_design_modeling_tool/,marquitadejohnz,1547513515,,0,1,False,default,,,,,
738,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,9,ag2q8j,self.MachineLearning,"Might be the work section, but anyone have any book recomendations for Marketing with AI and ML?",https://www.reddit.com/r/MachineLearning/comments/ag2q8j/might_be_the_work_section_but_anyone_have_any/,NiversaCo,1547513659,[removed],0,1,False,self,,,,,
739,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,10,ag2xc7,self.MachineLearning,Need book recommendations for AI Marketing. I could really use some insight.,https://www.reddit.com/r/MachineLearning/comments/ag2xc7/need_book_recommendations_for_ai_marketing_i/,NiversaCo,1547514914,"I'm currently locked up, and have 3 more months to go, and I'm looking to soak up as much as I can before I bounce.  
Some follow up advice for bootcamps to take when I finally have access to a computer again.  
I'm teaching my self linear algebra atm, and have another 6 months of home confinement when I get home... Needless to say I have ample time on my hands, but I only have 3 more months of 0 distraction-""do what ever the fuck i want time"".
Thanks and looking forward to your advice 

I read Data driven analytics with artificial intelligence by Magnus Unemyr, Ai for Marketing and product innovation by Dr. A Pradeep., Etc... 

I want to use 3rd party vendors for AI until I can move all my stuff in house.  Issue is, I want to learn to bring it in house, whether by buying it, or writing it myself.  

You can tell I have some high ambition for myself, and I might be thinking too big.  Any advice on the topic would be helpful ",0,1,False,self,,,,,
740,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,10,ag2ytu,self.MachineLearning,[P] Learning machine learning blog,https://www.reddit.com/r/MachineLearning/comments/ag2ytu/p_learning_machine_learning_blog/,mcslearner,1547515191,"I have started a blog where I will document what I've learned about machine learning. I am basically going off of course notes on the web (ng's notes, this course [http://www.cs.cornell.edu/courses/cs4780/2018fa/](http://www.cs.cornell.edu/courses/cs4780/2018fa/), the 100 page ML book), and will work my way up to 'deep learning'.  


My first serious post is here: [https://mcsjotnotes.wordpress.com/2019/01/09/the-perceptron-algorithm/](https://mcsjotnotes.wordpress.com/2019/01/09/the-perceptron-algorithm/)

  
This will most likely be useless to anyone except beginners like myself.

&amp;#x200B;

Questions or comments (even harsh ones) are appreciated.",3,1,False,self,,,,,
741,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,10,ag33io,self.MachineLearning,F,https://www.reddit.com/r/MachineLearning/comments/ag33io/f/,Ckjar112,1547516038,[removed],0,1,False,self,,,,,
742,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,10,ag36zf,self.MachineLearning,[D] Laptop for ML,https://www.reddit.com/r/MachineLearning/comments/ag36zf/d_laptop_for_ml/,Ckjar112,1547516650,"Im not really sure where this belongs but..

Im going off to college soon to study computer science and I study/practice machine learning on the side. Just wondering if I could get some recommendations on a laptop with enough processing power for ML. Price isnt a super important aspect just nothing ridiculous (more than 2500$). 

Please let me know if this doesnt belong here. ",4,1,False,self,,,,,
743,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,11,ag3f9z,self.MachineLearning,Master in Machine Learning without comp science degree,https://www.reddit.com/r/MachineLearning/comments/ag3f9z/master_in_machine_learning_without_comp_science/,Akarapat82,1547518132,[removed],0,1,False,self,,,,,
744,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,11,ag3jyb,self.MachineLearning,[D] CudaEvent.elapsed time vs python time.time(),https://www.reddit.com/r/MachineLearning/comments/ag3jyb/d_cudaeventelapsed_time_vs_python_timetime/,WillingCucumber,1547518955,"Hi all,

I have to do time profiling of my model on CUDA. CudaEvent.elapsed\_time comes out to be way to larger compared to measured by using python time.time() calls.

&amp;#x200B;

What can be the possible reason and which one is correct.

For Cuda events, I have synchronized them and have considered the warm up time.

&amp;#x200B;

Thanks !!

&amp;#x200B;",0,1,False,self,,,,,
745,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,11,ag3oac,self.MachineLearning,Different Learning Rates for specefic Labels,https://www.reddit.com/r/MachineLearning/comments/ag3oac/different_learning_rates_for_specefic_labels/,PumpkinCougar95,1547519734,[removed],0,1,False,self,,,,,
746,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,11,ag3rl2,self.MachineLearning,[D] Different Learning Rates for Specefic Labels,https://www.reddit.com/r/MachineLearning/comments/ag3rl2/d_different_learning_rates_for_specefic_labels/,PumpkinCougar95,1547520333,"When a child learns something new, it is sufficient to just give a few examples(maybe repeatedly) , but all deep learning applications require a lot of data. Maybe we should try to learn as much as we can from the limited data.

Would It be helpful if instead of having a single learning rate,  a learning rate vector was used with different rates for different classes. Since learning rate is a measure of how the model adapts to new data and forgets about the old data I think it might be useful to start of with a high learning rate whenever a new label is encountered and from there maybe the rates can vary using the various algorithms like ADAM.

What are your thoughts on this?",10,1,False,self,,,,,
747,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,12,ag4f4q,self.MachineLearning,Label-Free Supervision of Neural Networks with Physics and Domain Knowledge: constraint formulas,https://www.reddit.com/r/MachineLearning/comments/ag4f4q/labelfree_supervision_of_neural_networks_with/,tengerye,1547524639,"I have read the paper and the [post](https://www.reddit.com/r/MachineLearning/comments/53ulrt/160905566_labelfree_supervision_of_neural/), but still don't understand how did authors come up with the constraint formulas. I am familiar with linear algebras, but I am still confused with formula (3):

I know the a in the formula (3) is given, but how can you determine the variables y0 and v0 in the formula just above (3)? Will the model learn those variables?",0,1,False,self,,,,,
748,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,13,ag4myg,self.MachineLearning,[R] Eliminating All Bad Local Minima from Loss Landscapes Without Even Adding an Extra Unit,https://www.reddit.com/r/MachineLearning/comments/ag4myg/r_eliminating_all_bad_local_minima_from_loss/,iidealized,1547526144,[https://arxiv.org/pdf/1901.03909.pdf](https://arxiv.org/pdf/1901.03909.pdf) ,0,1,False,self,,,,,
749,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,13,ag4saj,self.MachineLearning,[D] Residual Multilayer Perceptrons,https://www.reddit.com/r/MachineLearning/comments/ag4saj/d_residual_multilayer_perceptrons/,anonDogeLover,1547527232,"Has anyone seen or used a non-CNN ResNet (an MLP with residual layers)? Code? I'm curious if they improve performance beyond perceptual problems, especially tabular data.",11,1,False,self,,,,,
750,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,14,ag51la,self.MachineLearning,How do I train a GAN on a non-image dataset?,https://www.reddit.com/r/MachineLearning/comments/ag51la/how_do_i_train_a_gan_on_a_nonimage_dataset/,AtharvaLarva,1547529154,[removed],0,1,False,self,,,,,
751,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,14,ag51vg,arxiv.org,[R] Context Aware Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ag51vg/r_context_aware_machine_learning/,xternalz,1547529213,,17,1,False,default,,,,,
752,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,14,ag56av,youtu.be,What is Reinforcement Learning? Artificial Intelligence Tutorial,https://www.reddit.com/r/MachineLearning/comments/ag56av/what_is_reinforcement_learning_artificial/,updownvizzii,1547530159,,0,1,False,https://b.thumbs.redditmedia.com/M5Tn5454Z6yj_P_o5f8rN4PgiWLxzlp354AXn6dF3cs.jpg,,,,,
753,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,14,ag58uf,sciencealert.com,Mathematicians Have Developed a Computing Problem That AI Can Never Solve? Is it TRUE?????,https://www.reddit.com/r/MachineLearning/comments/ag58uf/mathematicians_have_developed_a_computing_problem/,ai-lover,1547530652,,0,1,False,https://b.thumbs.redditmedia.com/43wYaZox3_UD-EEXcAIv-D6-aS2YQhZqvHKJDlFNFxw.jpg,,,,,
754,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,14,ag594u,nature.com,"[R] Designing neural networks through neuroevolution, Stanley et al 2019 {UberAI/Sentient} [review]",https://www.reddit.com/r/MachineLearning/comments/ag594u/r_designing_neural_networks_through/,baylearn,1547530704,,0,1,False,default,,,,,
755,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,14,ag5cjv,i.redd.it,Making a stamp that the Project idea is good ( based on People Tracking). What can be the scope and application after get the data?,https://www.reddit.com/r/MachineLearning/comments/ag5cjv/making_a_stamp_that_the_project_idea_is_good/,arya_minus,1547531371,,0,1,False,default,,,,,
756,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,15,ag5hjw,self.MachineLearning,Where should I start if I want to learn about machine learning/deep learning,https://www.reddit.com/r/MachineLearning/comments/ag5hjw/where_should_i_start_if_i_want_to_learn_about/,Mr_Pumpkin99,1547532532,"I am very fascinated by this subject! However, the content is in such depth, I don't know where to start. Could someone recommend beginner courses on such topics and tutorials that design practical uses of it?

&amp;#x200B;

P.S: I am only in highschool and currently my math skills are only up to precalculus... So really mathy courses would not be ideal :(

&amp;#x200B;

Thanks in advance!",0,1,False,self,,,,,
757,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,15,ag5kic,self.MachineLearning,[D]Is there any research on QA metrics?,https://www.reddit.com/r/MachineLearning/comments/ag5kic/dis_there_any_research_on_qa_metrics/,songhune,1547533186,"hello, I am on a research on NLP-QA area. Recently, I've found that there are very few outcomes for proper metrics for QA evaluation, especially for Free-Form answers(e.g. MS Macro style).

I've done a quick search for their leaderboards, and found they are using BLEU-1 and ROUGE-L scores only.

So, my question now is this, what if we have a question sth like below

&amp;#x200B;

Q: where is the capital city of Korea?

Ground Truth: The capital city of Korea is Seoul.

&amp;#x200B;

given Answer 1. The capital city of Korea is Tokyo.  (6/7)

given Answer 2. Seoul (1/7)

&amp;#x200B;

Like above, current benchmark shows that given answer 1 has much more score than 2, despite of given answer 2 has much more credit.

Like so, is there any research for measuring 'semantic' features in QA area? 

if you got any interest of what i am doing, i am very welcome to have a talk about it!(songhune@ajou.ac.kr)",5,1,False,self,,,,,
758,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,16,ag5zq1,self.MachineLearning,"Could you guys help me to find papers on data generation, or generative model?",https://www.reddit.com/r/MachineLearning/comments/ag5zq1/could_you_guys_help_me_to_find_papers_on_data/,vaseline555,1547536845,[removed],0,1,False,self,,,,,
759,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,16,ag6116,self.MachineLearning,Are there any open-source implementation of MERLIN (DeepMind)?,https://www.reddit.com/r/MachineLearning/comments/ag6116/are_there_any_opensource_implementation_of_merlin/,TheMnBN,1547537181,"I'm referring to MERLIN model from the paper 'Unsupervised Predictive Memory in a Goal-Directed Agent'.
I'm also looking for working implementation of the other 2 baseline models presented in the paper (namely RL-MEM).",1,1,False,self,,,,,
760,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,16,ag67m9,self.MachineLearning,Google cloud specializations on Advanced Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ag67m9/google_cloud_specializations_on_advanced_machine/,iphone6plususer,1547538979,"Google cloud platform has some great specializations on Advanced machine learning, consider looking at them if you haven't already! The best part is, if you decide to enroll, you get 1 month of free access!

Here's all the details for those interested: [Google Cloud Specializations](http://sumo.ly/10JMj)",0,1,False,self,,,,,
761,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,17,ag690h,self.MachineLearning,Universal machine learning fileformat?,https://www.reddit.com/r/MachineLearning/comments/ag690h/universal_machine_learning_fileformat/,Genesistag,1547539367,[removed],0,1,False,self,,,,,
762,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,17,ag6duu,self.MachineLearning,Feature Learning of time series data,https://www.reddit.com/r/MachineLearning/comments/ag6duu/feature_learning_of_time_series_data/,172knot,1547540756,[removed],0,1,False,self,,,,,
763,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,17,ag6hcr,bakingclouds.com,Open-source machine learning frameworks,https://www.reddit.com/r/MachineLearning/comments/ag6hcr/opensource_machine_learning_frameworks/,at-bakingclouds,1547541744,,0,1,False,default,,,,,
764,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,17,ag6i7q,self.MachineLearning,What is submission file in ICML?,https://www.reddit.com/r/MachineLearning/comments/ag6i7q/what_is_submission_file_in_icml/,bluediary8,1547541987,[removed],0,1,False,self,,,,,
765,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,17,ag6jba,self.MachineLearning,Best Machine Learning Cloud Computing place?,https://www.reddit.com/r/MachineLearning/comments/ag6jba/best_machine_learning_cloud_computing_place/,Jandevries101,1547542308,[removed],0,1,False,self,,,,,
766,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,17,ag6jhw,self.MachineLearning,What is the best loss for 'multi label regression' ?,https://www.reddit.com/r/MachineLearning/comments/ag6jhw/what_is_the_best_loss_for_multi_label_regression/,kkogong07,1547542363,[removed],0,1,False,self,,,,,
767,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,18,ag6p6k,self.MachineLearning,[D] Combination of Hierarchical Models with Memory?,https://www.reddit.com/r/MachineLearning/comments/ag6p6k/d_combination_of_hierarchical_models_with_memory/,ejmejm1,1547543913,"Is anyone aware of any successful algorithms that use a combination of hierarchical models that work with a memory module? I know a little about both sub-fields in separate contexts (e.g. Neural Turing Machines for memory or HIRO for hierarchical). I haven't heard of anything that combines the two well... yet at least. Are there any interesting papers I may have not seen, is this for some reason not a good idea, or is this just a current gap in research? I feel like solving issues of stability with NTMs and allowing ""neural functions"" to call other function could lead to an interesting form of hierarchical learning (easier said than done, of course).

&amp;#x200B;

Although, at the end of the day I am just babbling and have yet to dive deep into neither memory nor hierarchical RL. I'd love to hear thoughts on this and get some links to any related papers! ",6,1,False,self,,,,,
768,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,18,ag6qux,life-news.blog,Art and Music Machine Learning ~ Life News,https://www.reddit.com/r/MachineLearning/comments/ag6qux/art_and_music_machine_learning_life_news/,lifenews525,1547544383,,0,1,False,default,,,,,
769,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,18,ag6rkj,self.MachineLearning,[D] What is the best loss for multi label regression?,https://www.reddit.com/r/MachineLearning/comments/ag6rkj/d_what_is_the_best_loss_for_multi_label_regression/,kkogong07,1547544576,"Hello,

I need to trained a DNN for multi label regression.

Is there any good choice for loss function?

What will be best architecture for this kind of problem?

the dataset looks like...

\[img\_0 : 1.0, 0, 0, 0, 0\]

\[img\_1 : 0, 0.3, 0, 0, 0.3\]

\[img\_2 : 0, 0.2, 0, 0, 0\]

\[img 3 : 0, 0, 1.0, 0.4, 0\]

and so on..

Please help!

Thanks!",10,1,False,self,,,,,
770,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,18,ag6sa0,self.MachineLearning,Advanced machine learning with tensorflow on Google Cloud Platform,https://www.reddit.com/r/MachineLearning/comments/ag6sa0/advanced_machine_learning_with_tensorflow_on/,poonddetatte,1547544750,[removed],0,1,False,self,,,,,
771,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,18,ag6t26,self.MachineLearning,Predict rank change over time,https://www.reddit.com/r/MachineLearning/comments/ag6t26/predict_rank_change_over_time/,grid_world,1547544947,[removed],0,1,False,self,,,,,
772,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,18,ag6xsn,smarten.com,Assisted Predictive Modeling Improves Forecasting!,https://www.reddit.com/r/MachineLearning/comments/ag6xsn/assisted_predictive_modeling_improves_forecasting/,ElegantMicroWebIndia,1547546145,,0,1,False,default,,,,,
773,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,19,ag6zn6,self.MachineLearning,How can I build a GAN that replicates the given number data,https://www.reddit.com/r/MachineLearning/comments/ag6zn6/how_can_i_build_a_gan_that_replicates_the_given/,blazejohk,1547546613,[removed],0,1,False,self,,,,,
774,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,19,ag71zv,self.MachineLearning,How to create PIVOT table using Pandas DataFrame in Python,https://www.reddit.com/r/MachineLearning/comments/ag71zv/how_to_create_pivot_table_using_pandas_dataframe/,setscholars,1547547205,"\#Python #Pandas #MachineLearning #DataScience

[https://setscholars.info/guest-post/how-to-create-pivot-table-using-pandas-dataframe-in-python/](https://setscholars.info/guest-post/how-to-create-pivot-table-using-pandas-dataframe-in-python/)",0,1,False,self,,,,,
775,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,19,ag7554,self.MachineLearning,Problem with Demographics Analytic for Programmatic System,https://www.reddit.com/r/MachineLearning/comments/ag7554/problem_with_demographics_analytic_for/,khoanguyendang121292,1547548025,"I am having a problem with predicting Demographics for Audience in Programmatic Advertising. Currently, I am finding a way to feature engineering by FeaturesTool is provided by Python programming and after that, I use PCA or Random Forest to dimensionality reduction. I have a problem with predicting Gender and Age for audience.

Who can help me to find a research ways to solve a problem or at least the way to continue to research. 

I need your help, plz!!!",0,1,False,self,,,,,
776,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,19,ag7bva,self.MachineLearning,Why isn't hyperparameter tuning mentioned in Deep Learning papers anymore?,https://www.reddit.com/r/MachineLearning/comments/ag7bva/why_isnt_hyperparameter_tuning_mentioned_in_deep/,MithrandirGr,1547549733,[removed],0,1,False,self,,,,,
777,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,20,ag7l4k,self.MachineLearning,[R] ShapeStacks: Learning Physical Intuition with Computer Vision (VGG blog),https://www.reddit.com/r/MachineLearning/comments/ag7l4k/r_shapestacks_learning_physical_intuition_with/,ogroth,1547552024,"[http://www.robots.ox.ac.uk/\~vgg/blog/shapestacks-giving-robots-a-physical-intuition.html](http://www.robots.ox.ac.uk/~vgg/blog/shapestacks-giving-robots-a-physical-intuition.html)

Blog post explaining how an intuitive visual stability predictor for object stacks is trained in a physics simulator. Dataset, source code and models available.",0,1,False,self,,,,,
778,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,21,ag7zus,self.MachineLearning,"[D] why is ""objectness"" needed in Yolo v3?",https://www.reddit.com/r/MachineLearning/comments/ag7zus/d_why_is_objectness_needed_in_yolo_v3/,skariel,1547555338,"I understand why it is needed in Yolo v2, it is because each BB contained a soft-max classification, and there was no ""no object"" category (maybe this could have been a solution too). But in Yolo V3 a sigmoid per category is used, so if there is no object all classes could have a low score, and if a class has a high score then you have an object -- this is no more a ""zero-summ game""

&amp;#x200B;

So my questions would be:

&amp;#x200B;

&amp;#x200B;

1) Does this ""objectness"" route lead to better performance than having a ""no obj"" class for yolo v2 (softmaxed)

&amp;#x200B;

2) Does performance suffer in Yolo V3 (sigmoid classification) if removing the objectness?

&amp;#x200B;

3) any other reason I'm missing?

&amp;#x200B;

Thanks!

&amp;#x200B;",0,1,False,self,,,,,
779,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,21,ag88l4,self.MachineLearning,"[N] New book about AI: ""AI Blueprints: How to build and deploy AI business projects""",https://www.reddit.com/r/MachineLearning/comments/ag88l4/n_new_book_about_ai_ai_blueprints_how_to_build/,joshuaeckroth,1547557118,"I just finished writing &amp; publishing my book about building AI/ML applications. It includes several case studies across the spectrum (deep learning, NLP, planning, logic programming, recommendations, etc.). It also has chapters with advice about how to use AI while minimizing risk, and what to look for in the future of the field.

It has been endorsed by Bruce Buchanan, creator of MYCIN, Past President and Fellow of AAAI. (See Amazon review.)

https://www.amazon.com/dp/1788992873

Github for code: https://github.com/PacktPublishing/AIBlueprints
",5,1,False,self,,,,,
780,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,22,ag8h78,self.MachineLearning,[P] Delira - A deep learning framework based on PyTorch. Especially handy for medical images.,https://www.reddit.com/r/MachineLearning/comments/ag8h78/p_delira_a_deep_learning_framework_based_on/,chrisha1303,1547558808,"## Delira

We just released [Delira](https://github.com/justusschock/delira), a deep learning library for images arbitrary dimension, including medical images. It was developed with the goal of minimizing the time from idea to evaluation.

Based on [PyTorch](https://pytorch.org), [batchgenerators](https://github.com/MIC-DKFZ/batchgenerators) and [trixi](https://github.com/MIC-DKFZ/trixi) it provides a framework for
* Dataset loading
* Dataset sampling
* Augmentation (multi-threaded) including 3D images with any number of channels
* A generic trainer class that implements the training process
* Implementation of widely-used [models](delira/models) such as ResNet and UNet
* Web-based monitoring using [Visdom](https://github.com/facebookresearch/visdom)
* Model save and load functions

Delira supports classification and regression problems as well as generative adversarial networks and segmentation tasks.

To get started, we recommend to have a look at one of our example notebooks ([classification](https://delira.readthedocs.io/en/latest/classification_pytorch.html), [GAN](https://delira.readthedocs.io/en/latest/gan_pytorch.html), [segmentation](https://delira.readthedocs.io/en/latest/segmentation_2d_pytorch.html)). A more comprehensive introduction can be found in the [tutorial](https://delira.readthedocs.io/en/latest/tutorial_delira.html).
",2,1,False,self,,,,,
781,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,22,ag8l0b,blog.sicara.com,Learn to Test Your Pyspark Project with Pytest  example-based Tutorial,https://www.reddit.com/r/MachineLearning/comments/ag8l0b/learn_to_test_your_pyspark_project_with_pytest/,stolbiq,1547559558,,0,1,False,default,,,,,
782,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,22,ag8lrt,self.MachineLearning,Question about churn model,https://www.reddit.com/r/MachineLearning/comments/ag8lrt/question_about_churn_model/,misterchiply,1547559705,[removed],0,1,False,self,,,,,
783,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag8xlf,self.MachineLearning,"[D] Getting Started In Original Research, But No Support",https://www.reddit.com/r/MachineLearning/comments/ag8xlf/d_getting_started_in_original_research_but_no/,MaxMachineLearning,1547561858,"So, this post might require a bit of backstory. So, I am a Machine Learning Engineer, working primarily in computer vision for industrial automation with strong interests in RL. I currently work at a government owned R&amp;D organization that works in many areas, from Air Quality to Physical Metallurgy. Currently, I work for the automation lab in the engineering services portion of our organization. I have one deployed system, one more that's finished and going to be deployed in a few months, and beginning work on a third. So, things are all well and good on that end. However, I'm the only one who knows anything about ML in this organization of 150 people. Sometimes, this isn't a problem. I can normally build and test my projects, and assess them based on their performance. I have a way of measuring my approach.

However, there's one big problem here. Not having anyone senior to me know anything about my field makes it very hard to do original research. I can get no input on theory, whether or not I have overlooked previous publications that do the same thing, or to just help me figure out what to do with it. Recently, I started applying methods similar to those used in papers like Efficient Neural Architecture Search Via Parameter Sharing to denoise data with noisy labels. I'm pretty happy with the method, and I've been doing experiments with it and it seems to work. I'm pretty happy with the theory behind it and as far as I know this method has never been used before. Which is cool, except I genuinely have no idea what to do with it. I did a little write-up about it, at least for internal documentation but other than that I'm stuck. I would talk to old professors if I could, however I come from a pure mathematics background and none of my old professors worked in the field. 

Honestly, any suggestions as to what to do with this would be appreciated. Do I just put it on GitHub? Do I just assume its probably been done before and forget about it?",5,1,False,self,,,,,
784,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag8y5d,github.com,[P] Parameter-free K-means clustering,https://www.reddit.com/r/MachineLearning/comments/ag8y5d/p_parameterfree_kmeans_clustering/,alexkimxyz,1547561966,,0,1,False,https://b.thumbs.redditmedia.com/JplJc5_0BArkOzgPqer__Q2fQ0o6HBqOV7WuT3-5aZg.jpg,,,,,
785,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag929j,self.MachineLearning,[R] Disentangled representations for content transfer [ICLR 2019],https://www.reddit.com/r/MachineLearning/comments/ag929j/r_disentangled_representations_for_content/,oripress,1547562734,[removed],1,1,False,self,,,,,
786,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag95l1,self.MachineLearning,[D] Beneficial advanced math topics for ML at an undergraduate level?,https://www.reddit.com/r/MachineLearning/comments/ag95l1/d_beneficial_advanced_math_topics_for_ml_at_an/,Wooperswish,1547563334,"I'm a beginner in machine learning, but trying to prepare for graduate school in the field early. I'm a CS/math double major sophomore undergrad doing specialisations in machine learning and statistics. 

I have to take the regular math requirements; I've taken calc I/II/III, linear algebra and differential equations and will take real and complex analysis as well as graph theory. I will also be taking some optimisation classes outside of my regular statistics major load.

Other than what I've mentioned, what math topics are useful? I've heard measure theory is useful for a deeper understanding of probability theory, as well as functional analysis (though I'm not so sure as to the applications of that field). I'm looking for areas that are not *so* cutting edge.

(PS. I'm Australian but tried to fit my post to the American college lingo)

",29,1,False,self,,,,,
787,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag95lm,arxiv.org,[1901.04407] Self-Driving Cars: A Survey,https://www.reddit.com/r/MachineLearning/comments/ag95lm/190104407_selfdriving_cars_a_survey/,ihaphleas,1547563337,,2,1,False,default,,,,,
788,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag95lw,self.MachineLearning,[R] Decomposing represenations for content transfer [ICLR 2019],https://www.reddit.com/r/MachineLearning/comments/ag95lw/r_decomposing_represenations_for_content_transfer/,oripress,1547563338,"[https://openreview.net/pdf?id=BylE1205Fm](https://openreview.net/pdf?id=BylE1205Fm)

&amp;#x200B;

Hi. First author here -

&amp;#x200B;

In this paper we tackle the problem of transferring specific content from one domain to its' subset. For example, given faces with glasses and faces without, we want to transfer specific glasses from one face to another. We show how to train the network to decompose a face representation into 2 parts: one that contains information about glasses and one that contains information about everything else.

&amp;#x200B;

To accomplish this, we train a network consisting of two encoders and one decoder on the autoendcoding objective. The first encoder only encodes information that has to do with the glasses in the picture, and the second encoder encodes information related to everything else. During training, we train the encoders and the decoder to reconstruct images of people with and without glasses. Then, to encode an image of a person with glasses, we run both encoders on that image and then concatenate their output. When we encode an image of a person without glasses, we just don't use the first encoder, and instead concatenate a vector of zeros to the output of the second decoder. To ensure the encodings produced by the second encoder do not contain information about glasses, we use a discriminator that tries to predict whether an encoding came from an image of a person with or without glasses.

&amp;#x200B;

The code is available here: [https://github.com/oripress/ContentDisentanglement](https://github.com/oripress/ContentDisentanglement)

&amp;#x200B;

Feel free to ask me questions here",0,1,False,self,,,,,
789,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag99ij,self.MachineLearning,[R] Decomposing representations for content transfer [ICLR 2019],https://www.reddit.com/r/MachineLearning/comments/ag99ij/r_decomposing_representations_for_content/,oripress,1547564019,"[https://openreview.net/pdf?id=BylE1205Fm](https://openreview.net/pdf?id=BylE1205Fm)

First author here -

In this paper we tackle the problem of transferring specific content from one domain to its' subset. For example, given faces with glasses and faces without, we want to transfer specific glasses from one face to another. We show how to train the network to decompose a face representation into 2 parts: one that contains information about glasses and one that contains information about everything else.

To accomplish this, we train a network consisting of two encoders and one decoder on the autoencoding objective. The first encoder only encodes information that has to do with the glasses in the picture, and the second encoder encodes information related to everything else. During training, we train the encoders and the decoder to reconstruct images of people with and without glasses. Then, to encode an image of a person with glasses, we run both encoders on that image and then concatenate their output. When we encode an image of a person without glasses, we just don't use the first encoder, and instead concatenate a vector of zeros to the output of the second decoder. To ensure the encodings produced by the second encoder do not contain information about glasses, we use a discriminator that tries to predict whether an encoding came from an image of a person with or without glasses.

The code is available here: [https://github.com/oripress/ContentDisentanglement](https://github.com/oripress/ContentDisentanglement)

Feel free to ask me questions here",5,1,False,self,,,,,
790,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag99vm,self.MachineLearning,[D] Is it worth to do masters degree on ML?,https://www.reddit.com/r/MachineLearning/comments/ag99vm/d_is_it_worth_to_do_masters_degree_on_ml/,kephiroth,1547564074,"I am planning to do some Masters degree on ML or Deep Learning field (I still dont know the big difference between those yet but is going to be on the AI field).

And I would like to know if it worth to study on this fields right now. Its better for who is aiming research fields, or is there still some positions on open job market?

Please share your experiences.

I am an I.T. professional working as software engineer.

Thanks.",14,1,False,self,,,,,
791,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag9adq,self.MachineLearning,[D] Can Grad-CAM and variants be improved by taking the whole training into account?,https://www.reddit.com/r/MachineLearning/comments/ag9adq/d_can_gradcam_and_variants_be_improved_by_taking/,dataism,1547564156,"Wouldn't the feature importance heatmap (saliency map) calculations benefit from taking the gradients and activations throughout the training (model at epoch1, epoch2 ...) into account (in some witty way) as well?

Has there been any work from such perspective to explainable models? ",1,1,False,self,,,,,
792,MachineLearning,t5_2r3gv,2019-1-15,2019,1,15,23,ag9bj2,youtube.com,"[P] Machine learning experiments in Unity: Flappy Bird, Trucks and Cube People",https://www.reddit.com/r/MachineLearning/comments/ag9bj2/p_machine_learning_experiments_in_unity_flappy/,Philipp,1547564359,,1,1,False,https://b.thumbs.redditmedia.com/j0jdp4HDzeB-OKGhZFT6_cqNicpdjzGnJa932s6VoqA.jpg,,,,,
793,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,0,ag9kfn,self.MachineLearning,AI SANGAM | Machine Learning Solutions and Consulting,https://www.reddit.com/r/MachineLearning/comments/ag9kfn/ai_sangam_machine_learning_solutions_and/,AI_Sangam,1547565821,[removed],0,1,False,self,,,,,
794,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,0,ag9nb9,self.MachineLearning,"Expert in ML/AI, Time-series and pattern recognition --&gt; get paid for your assistance.",https://www.reddit.com/r/MachineLearning/comments/ag9nb9/expert_in_mlai_timeseries_and_pattern_recognition/,PabloDK,1547566274,[removed],0,1,False,self,,,,,
795,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,0,ag9tre,self.MachineLearning,[D] People publishing in Nature MI after signing petition not to,https://www.reddit.com/r/MachineLearning/comments/ag9tre/d_people_publishing_in_nature_mi_after_signing/,BeatLeJuce,1547567293,"Hi there!

After Nature announced that their new ""Machine Intelligence"" journal would be closed-access, a lot of famous machine learning researchers signed a petition to shun the journal: https://openaccess.engineering.oregonstate.edu/home

However, it seems that the first author of [this NMI publication](https://www.nature.com/articles/s42256-018-0002-3 ) is among those that have signed: https://openaccess.engineering.oregonstate.edu/signatures (CTRL-F for ""Ben-David""). I know NMI has recently announced that their paper will be [free to read for one year](https://twitter.com/NatMachIntell/status/1082593381035974657), but as far as I'm concerned, that's still way too little, considering that this journal will likely reduce the resources (e.g. reviewer time) available for open, free and accepted journals like JMLR. Am I the only one who is outraged about this? Or did I miss some essential part of the story here?",55,1,False,self,,,,,
796,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,1,aga0zr,self.MachineLearning,"[D] Did anyone try solving generalizations of TSP (VRP, TPP) problems with ML/RL ?",https://www.reddit.com/r/MachineLearning/comments/aga0zr/d_did_anyone_try_solving_generalizations_of_tsp/,swentso,1547568434,"Hi,

Did anyone attempt anything in regard to generalizations of traveling salesman problem using machine learning ? (Traveling purchaser problem, Vehicle routing problem)  
I think reinforcement learning could be interesting to test. What do you guys think? Do you know any articles about this?  
",4,1,False,self,,,,,
797,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,1,agaaps,datascience.salon,Automated Machine Learning at Google: Using AI to Build Better AI,https://www.reddit.com/r/MachineLearning/comments/agaaps/automated_machine_learning_at_google_using_ai_to/,AnnaOnTheWeb,1547569946,,0,1,False,default,,,,,
798,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,1,agaf2e,self.MachineLearning,How to cope with constant rejection from ML/AI related jobs,https://www.reddit.com/r/MachineLearning/comments/agaf2e/how_to_cope_with_constant_rejection_from_mlai/,LeMasterShredder,1547570643,[removed],0,1,False,self,,,,,
799,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,1,agag5x,self.MachineLearning,[D] - How to make an autoregressive model that doesn't assign more probability to shorter sequences?,https://www.reddit.com/r/MachineLearning/comments/agag5x/d_how_to_make_an_autoregressive_model_that_doesnt/,m_nemo_syne,1547570819,"Let's say you have a distribution $p(\mathbf{x})$ of sequences $\mathbf{x} = x_1, x_2, \dots, x_T$. For simplicity, let's assume that each element of $\mathbf{x}$ is made up of two letters, $A$ and $B$, and the length $T$ can only be either 1 or 2. So the possible values $\mathbf{x}$ can take on are: $A, B, AA, AB, BA,$ and $BB$.

A common way to model sequences is using an autoregressive model $\theta$, where you predict each element given the previous elements, and get the probability of a sequence by multiplying the probabilities of each of the elements together: $p_{\theta}(\mathbf{x}) = p_{\theta}(x_1)p_{\theta}(x_2|x_1)\dots = \prod_{t=1}^T p_{\theta}(x_t|x_1, x_2, \dots, x_{t-1})$

Since $p_{\theta}(x_2|x_1)$ is always less than one (it has to be---it's a probability), a model like this will automatically assign more probability to shorter sequences than to longer sequences. For example, let's say $p_{\theta}(A) = 0.5$, and $p_{\theta}(B|A) = 0.5$. Then $p_{\theta}(AB) = p_{\theta}(A)p_{\theta}(B|A) = 0.25$.

People have some hacks for dealing with this (for example, Alex Graves in ""Sequence Transduction with Recurrent Neural Networks"" says ""just divide by the sequence length"" so that longer sequences get a higher log probability), but there is also a problem that we can't model a distribution like this:
$p(A) = 0.25$
$p(AA) = 0.75$
$p(\text{other}) = 0$
because that would mean $p(A|A) = p(AA)/p(A) = 3 &gt; 1$.

Is there a non-hacky way to model sequences so that shorter sequences don't automatically get higher probability?",3,1,False,self,,,,,
800,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,2,agan8p,mnisozso.tumblr.com,Star Ruler [Download] sale 47% OFF,https://www.reddit.com/r/MachineLearning/comments/agan8p/star_ruler_download_sale_47_off/,maddisonaufderhar884,1547571917,,0,1,False,default,,,,,
801,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,2,agapaf,self.MachineLearning,[D] How to cope with constant rejection from AI/ML jobs,https://www.reddit.com/r/MachineLearning/comments/agapaf/d_how_to_cope_with_constant_rejection_from_aiml/,LeMasterShredder,1547572240,"[D] ""[Discussion]"" I have a PhD in applied ML and computer vision. Worked in industries as well. Have gotten rejected by 3 different companies recently. I think I wasn't able to answer one or two questions on a specific subject they asked. So it's almost always a miss by one. Meanwhile they didn't get a chance to really know what I'm capable of. I don't think it's humanly possible to be young and know it all and be able to demonstrate it effectively to a narrow minded interviewer who comes into the interview with a pre selected set of questions, and fails to see anything beyond his set of questions. It's also bothering me so much that due to the nature of this industry, the duty of hiring is given to very young inexperienced nerds who more often than not lack a lot of inter-personal communication skills; rather than having more mature and seasoned people interview you, who are able to see the opportunities and potential in people, and judge a person's abilities as a whole.

This issue is starting to affect my mental health and self confidence pretty badly. My self image has been torn to pieces and I'm beginning to question whether I'm any better than a fresh Bachelor in CS.

This is a note from heart mostly. Is anyone else experiencing the same or has had the same encounters. If you finally overcame this, what was the way you did it and what things helped you out? Let's discuss our experiences with the hiring process.

""[Discussion]""",105,1,False,self,,,,,
802,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,2,agarwr,self.MachineLearning,[D] Best practices for running deep learning experiments on the cloud,https://www.reddit.com/r/MachineLearning/comments/agarwr/d_best_practices_for_running_deep_learning/,PhYsIcS-GUY227,1547572646,"I'm working on a machine translation research project, and appealing to the wisdom of the masses here to hopefully improve my workflow. I'm looking for recommendations on workflow optimization and cloud provider tools (I want to avoid buying a GPU specifically for this task)  for my project. I know which deep learning framework to use (and what other python libraries, so not looking for recommendations in that area).

The data is large-ish -- too large for Git but not large enough for Hadoop.

I'm anticipating many experiment variations, where hyper parameters as well as pre-processing of the data may change between experiments. I would like to keep track of the different experiments and their results.

Also, I would prefer easy-to-use tools for these purposes, but not too expensive.",9,1,False,self,,,,,
803,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,2,agazao,self.MachineLearning,[D] Necessity of publishing papers in undergrad,https://www.reddit.com/r/MachineLearning/comments/agazao/d_necessity_of_publishing_papers_in_undergrad/,LazyLoo,1547573784,"So I have been working with professors on various projects recently over the months but there hasn't been much discussion on papers. Some of the projects require weeks to conduct studies on human participants and this can take even longer for approvals and etc. so it doesn't seem as though a paper is even likely in the near future at all. Keep in mind that I am going to be applying to grad school by next December. 

Further, most of the deeper work is being done by the masters and PhD students (which makes sense), so it is unlikely that I will even get first author on any of these papers. I know grad schools look highly upon publications but it just seems somewhat unreasonable to me to expect undergrads to first author papers submitted to decent conferences. For the people that did in undergrad, how did you do it? ",5,1,False,self,,,,,
804,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,3,agba1m,self.MachineLearning,"[D] ""Human Intelligence is something deeply social"" - short text on the importance of society and multi-agent learning in AI/ML Research.",https://www.reddit.com/r/MachineLearning/comments/agba1m/d_human_intelligence_is_something_deeply_social/,mw_molino,1547575440,"Hey, inspired by some interesting reads and thoughts, I've written a short text on why human intelligence is quite a social thing and its role in ML/AI research. I'd be delighted if any of you decided to get a taste of my lousy writing and even more happy if you provided me with some feedback . [https://medium.com/@macwiatrak/human-intelligence-is-something-deeply-social-601fbf82819f](https://medium.com/@macwiatrak/human-intelligence-is-something-deeply-social-601fbf82819f)

Hope you enjoy!

&amp;#x200B;

&amp;#x200B;",3,1,False,self,,,,,
805,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,3,agbaw8,self.MachineLearning,[D] Fun papers &amp; code to replicate?,https://www.reddit.com/r/MachineLearning/comments/agbaw8/d_fun_papers_code_to_replicate/,cslambthrow,1547575573,"I'm looking for some recent papers/code that would be fun to replicate and was looking for suggestions. I'm open to anything, just curious.",6,1,False,self,,,,,
806,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,3,agbeaf,arxiv.org,[R] State-of-the-art results on early earthquake detection: Reliable Real-time Seismic Signal/Noise Discrimination with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/agbeaf/r_stateoftheart_results_on_early_earthquake/,downtownslim,1547576089,,2,1,False,default,,,,,
807,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,3,agbp2b,self.MachineLearning,Good course to start,https://www.reddit.com/r/MachineLearning/comments/agbp2b/good_course_to_start/,androidjunior,1547577728,"Hello everyone!

I'm wondering is this a good course to learn data science and machine learning.

I'm already a software developer, but I just want to see other options also, so I thought I learn data science.

This is the course:

https://www.udemy.com/data-science-and-machine-learning-with-python-hands-on/

I was thinking doing this and then going to kaggle.com and checking the learning machine learning there and practicing. ",0,1,False,self,,,,,
808,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,3,agbqwf,self.MachineLearning,Video: Learning AI for Hackers  A Visual Primer,https://www.reddit.com/r/MachineLearning/comments/agbqwf/video_learning_ai_for_hackers_a_visual_primer/,ddonzal,1547578005,[removed],0,1,False,self,,,,,
809,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,4,agbzx3,ai.googleblog.com,Looking Back at Googles Research Efforts in 2018,https://www.reddit.com/r/MachineLearning/comments/agbzx3/looking_back_at_googles_research_efforts_in_2018/,sjoerdapp,1547579369,,0,1,False,default,,,,,
810,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,4,agc3l5,self.MachineLearning,Regularization When Dataset is (for all intents and purposes) Infinitely Large,https://www.reddit.com/r/MachineLearning/comments/agc3l5/regularization_when_dataset_is_for_all_intents/,ughfart,1547579920,[removed],0,1,False,self,,,,,
811,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,4,agcgci,self.MachineLearning,[D] How to flatten a 2D tensor to 1D list in tensorflow,https://www.reddit.com/r/MachineLearning/comments/agcgci/d_how_to_flatten_a_2d_tensor_to_1d_list_in/,dante_2608,1547581699,"Hi all,

&amp;#x200B;

I am new to tensorflow, in my custom layer i was attempting to convert 2D tensor to 1D list of value to use for the tf.sparse.SparseTensor function. But I got this error and could not find any useful suggestion.

&amp;#x200B;

Here is my input tensor to the function: Tensor(""lambda\_16/map/while/strided\_slice:0"", shape=(128, 128), dtype=float32)

when I was trying to run tf.squeeze(input) or tf.reshape(input,\[-1\]) I got error Shape (128, 128) must have rank 1.

Thanks in advance,",1,1,False,self,,,,,
812,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,5,agcnt2,self.MachineLearning,How do I make the most out of my ML PhD?,https://www.reddit.com/r/MachineLearning/comments/agcnt2/how_do_i_make_the_most_out_of_my_ml_phd/,liqui_date_me,1547582818,[removed],0,1,False,self,,,,,
813,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,5,agco3d,self.MachineLearning,"[D] CS236605: Deep Learning. Technion, Prof. Alex Bronstein",https://www.reddit.com/r/MachineLearning/comments/agco3d/d_cs236605_deep_learning_technion_prof_alex/,olBaa,1547582860,"The course has video lectures, tutorials, and assignments posted on the course website. Looks quite fresh to me, and covers some advanced topics, such as RL and graph NNs.

https://vistalab-technion.github.io/cs236605/",9,1,False,self,,,,,
814,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,5,agctwp,stoodnt.com,Current Trends and Future Scopes of Machine Learning and AI in the Education Sector,https://www.reddit.com/r/MachineLearning/comments/agctwp/current_trends_and_future_scopes_of_machine/,tanmoyray01,1547583744,,0,1,False,default,,,,,
815,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,5,agcz31,self.MachineLearning,"Building a Keras LSTM - looking for a large (~3,000,000 rows) dataset that has been well-solved.",https://www.reddit.com/r/MachineLearning/comments/agcz31/building_a_keras_lstm_looking_for_a_large_3000000/,StatusMemory5,1547584554,[removed],0,1,False,self,,,,,
816,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,7,agdwe4,self.MachineLearning,Program to play the guessing game.,https://www.reddit.com/r/MachineLearning/comments/agdwe4/program_to_play_the_guessing_game/,TheFoolVoyager,1547589692,[removed],0,1,False,self,,,,,
817,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,7,agdzo1,nature.com,[N] Evidence that neural information flow is reversed between object perception and object reconstruction from memory,https://www.reddit.com/r/MachineLearning/comments/agdzo1/n_evidence_that_neural_information_flow_is/,phobrain,1547590181,,1,1,False,default,,,,,
818,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,8,agek05,self.MachineLearning,Microsoft AI Residency 2019 Applicants Discussion Thread,https://www.reddit.com/r/MachineLearning/comments/agek05/microsoft_ai_residency_2019_applicants_discussion/,mlnewcomer1,1547593514,[removed],0,1,False,self,,,,,
819,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,8,agen95,i.redd.it,Go Explore VS Sonic the Hedgehog,https://www.reddit.com/r/MachineLearning/comments/agen95/go_explore_vs_sonic_the_hedgehog/,618smartguy,1547594067,,1,1,False,default,,,,,
820,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,8,agerf7,self.MachineLearning,[D] Why are research papers so un-approachable?,https://www.reddit.com/r/MachineLearning/comments/agerf7/d_why_are_research_papers_so_unapproachable/,c0cky_,1547594775,"I love how the research paper has been so explosive over the past couple years and people are writting and submitting so many papers and ideas.

&amp;#x200B;

I'm curious for those of you reading papers:

1. What is the most difficult part of reading a paper?
2. How much time do you spend per week reading papers?",9,1,False,self,,,,,
821,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,8,ageznr,self.MachineLearning,Go Explore VS Sonic the Hedgehog,https://www.reddit.com/r/MachineLearning/comments/ageznr/go_explore_vs_sonic_the_hedgehog/,618smartguy,1547596176,[removed],0,1,False,self,,,,,
822,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,9,agf43s,self.MachineLearning,[D] Go Explore VS Sonic the Hedgehog,https://www.reddit.com/r/MachineLearning/comments/agf43s/d_go_explore_vs_sonic_the_hedgehog/,618smartguy,1547596938," 

*Processing gif piin5n2cdoa21...*

Mirror: [https://i.imgur.com/SjArtkz.gif](https://i.imgur.com/SjArtkz.gif)

Here is my first great result after about 30 hours of running Go Explore with one thread on my desktop with an i7 3930k. The best tool assisted human made world record I could find is at 24 seconds, so still slightly better than my result, but it takes almost the exact same path. Cells were defined by taking floor( (sonic's location)/8 ), and fitness is defined as the fewest number of frames to reach the cell. So if during exploration sonic reaches any cell using a shorter trajectory, that trajectory replaces the previous trajectory to reach that cell.

The major change I made was to use something a little better than random exploration, and also to add another greedier go explore routine that I call a comb. First I used a convolutional model which predicted action probabilities to sample from based on screen input which was trained on some of the best trajectories found so far. This seemed to slow down overall progress too much. A super lightweight markov chain on only actions (blind to the screen and game state) worked the best so far. Meeting somewhere in the middle will probably be optimal.

The 'comb':Once go explore has found a winning trajectory, occasionally a comb will be done. Instead of go(random), I execute go(state) for each state visited in the winning trajectory in order. This focuses on finding a good speedrun instead of just exploring everywhere randomly, and also causes improvements to be immediately exploited more. Usually when a cell trajectory is improved it takes a while before that trajectory is used for further improvements. But if a cell in the speedrun is improved it will immediately get exploited once the comb reaches the cell that has been improved.

Code is currently a pretty dirty/embarrassing python notebook that stores the whole database in a shelf and only ever works on one thing at a time. Plans are to rewrite something that can run at scale on AWS and attempt a complete run of super mario brothers instead and then maybe release code if there isn't a better go explore out by then.",19,1,False,self,,,,,
823,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,9,agfnk8,self.MachineLearning,3d model generation with 2d images,https://www.reddit.com/r/MachineLearning/comments/agfnk8/3d_model_generation_with_2d_images/,RedditAcy,1547600322,"Hey guys, I got interested in 3d reconstruction recently, does anyone have good papers or resources to share on this matter? Thanks!",0,1,False,self,,,,,
824,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,10,agfqsp,self.MachineLearning,Can you recommend a good model for my quality inspection application?,https://www.reddit.com/r/MachineLearning/comments/agfqsp/can_you_recommend_a_good_model_for_my_quality/,engineheat,1547600931,"For a quality check on a gearmotor, I want to check whether everything has been built correctly. If correct, the gearmotor should make a low humming noise, if not, it will make loud noise with a different sound profile. Damaged teeth might cause this for example.

There is already a station on the production line that turns the motor on/off for function test and I plan to put a sound analyzer there, and then send a signal to the PLC to accept or reject.

I used Python with the Pyaudio package and recorded 10 seconds of sound just for test. Sample rate =44100, each frame is 2048 samples. For each frame I calculated the FFT, and then I averaged the FFT across all frames to lower the variance.

Next step is to find features/attributes to differentiate the good and the bad based on the averaged FFT. I noticed bad products makes sounds more in the higher frequency range, whereas for good products the magnitudes are pretty similar across all frequencies (up to 6000k). What are some good attributes to try? I'm thinking average magnitudes across all frequencies (just in case bad ones are louder). Or perhaps average magnitudes across certain frequency ranges. 

What would be a good model to use for this? Unfortunately my number of samples is severely limited due to the cost of making them. I probably got 10 good ones and 10 bad ones.

Thanks",0,1,False,self,,,,,
825,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,10,agfvx5,self.MachineLearning,[R] Discriminate between classes with subtle differences,https://www.reddit.com/r/MachineLearning/comments/agfvx5/r_discriminate_between_classes_with_subtle/,eleos_phobos,1547601935,"What is the best way to distinguish between two classes with very subtle differences? As an example, consider a dataset consisting of people in a variety of poses and environments and, for simplicity, assume we have only two labels: (wears/does not wear) a watch. Image-level annotations are available, but no bounding boxes. Now, a simple VGG net with cross-entropy loss could focus on a thousand things to make his decision. How could a second dataset consisting of watches help? To be clear: these two datasets are unrelated, no paired images. Whats the best way to use the latter dataset to steer the focus of the former dataset. Like: Here is the item you should pay attention to.  Could an attention mechanism actually be useful here? Any domain transfer ideas that come to mind? I guess the simplest solution would be to train a watch detector on the watch dataset and apply it to the other dataset, but that feels very ad-hoc.",1,1,False,self,,,,,
826,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,11,aggm6n,arxiv.org,[1901.02565] High-Fidelity Vector Space Models of Structured Data,https://www.reddit.com/r/MachineLearning/comments/aggm6n/190102565_highfidelity_vector_space_models_of/,graph_vsm,1547606822,,8,1,False,default,,,,,
827,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,12,aggtbd,github.com,[P] NSFW data scrapper: automatic image collection for training classifiers,https://www.reddit.com/r/MachineLearning/comments/aggtbd/p_nsfw_data_scrapper_automatic_image_collection/,seanDL_,1547608168,,0,1,True,nsfw,,,,,
828,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,12,agh1gs,self.MachineLearning,[D]Confusions regarding Mask R-CNN: how to scale masks for training and inference?,https://www.reddit.com/r/MachineLearning/comments/agh1gs/dconfusions_regarding_mask_rcnn_how_to_scale/,zenggyu,1547609736," In Mask R-CNN, the output feature maps of the mask branch are 28\*28;  when training the network, the ground-truth masks are scaled to the same  size to compute the loss (as indicated by this post: [https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46)).

My question is, what techniques are used to scale the ground-truth  masks to 28\*28? Does it involve any deep learning techniques  (convolution, max pooling, etc.) or any interpolation techniques  (bilinear interpolation, etc.)? Also, how are the mask outputs scaled to  the same scale as the original image?",7,1,False,self,,,,,
829,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,12,agh7hw,self.MachineLearning,Pinsta Cloud- GPU Instance starting at $0.16/hr,https://www.reddit.com/r/MachineLearning/comments/agh7hw/pinsta_cloud_gpu_instance_starting_at_016hr/,sarakomia,1547610930,[removed],0,1,False,self,,,,,
830,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,12,agh8mp,techgrabyte.com,10 Major Machine Learning Algorithms And Their Application,https://www.reddit.com/r/MachineLearning/comments/agh8mp/10_major_machine_learning_algorithms_and_their/,hiren_p,1547611164,,0,1,False,default,,,,,
831,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,13,aghams,self.MachineLearning,[P]Pinsta Cloud (beta)- GPU Instance starting at $0.16/hr,https://www.reddit.com/r/MachineLearning/comments/aghams/ppinsta_cloud_beta_gpu_instance_starting_at_016hr/,sarakomia,1547611556," Hi all!

Sara from Pinsta cloud team .We are excited to offer you **GPU** starting at **$0.16/hr**.

Get Jupyter Notebooks supported by powerfull GPUs.

For more info. visit out website [Pinstacloud](https://pinstacloud.com/)

We are in beta,so limited instance are created, New user will get **3 hours extra**.

Questions/feedback are welcome.",0,1,False,self,,,,,
832,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,13,aghe2q,self.MachineLearning,Dataset preparation for DeepSpell@MajorTal from my own data,https://www.reddit.com/r/MachineLearning/comments/aghe2q/dataset_preparation_for_deepspellmajortal_from_my/,hemangr8,1547612265,[removed],0,1,False,self,,,,,
833,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,13,aghn93,self.MachineLearning,Dataset preparation for DeepSpell@MajorTal from my own data,https://www.reddit.com/r/MachineLearning/comments/aghn93/dataset_preparation_for_deepspellmajortal_from_my/,hemangr8,1547614185,[removed],0,1,False,self,,,,,
834,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,13,aghp9j,self.MachineLearning,SGD Momentum and Exploding Gradients,https://www.reddit.com/r/MachineLearning/comments/aghp9j/sgd_momentum_and_exploding_gradients/,The_Sodomeister,1547614638,[removed],0,1,False,self,,,,,
835,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,14,aghzp8,self.MachineLearning,Detectron approach to AMD with hipify?,https://www.reddit.com/r/MachineLearning/comments/aghzp8/detectron_approach_to_amd_with_hipify/,numPlumber,1547616869,[removed],0,1,False,self,,,,,
836,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,14,agi3c1,self.MachineLearning,[D] What is the landscape of funding like with ML?,https://www.reddit.com/r/MachineLearning/comments/agi3c1/d_what_is_the_landscape_of_funding_like_with_ml/,c0cky_,1547617664,"Im not currently an academic and was curious of how the landscape of funding is with ML?

How hard is it to get funding to do research?",9,1,False,self,,,,,
837,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,15,agia0i,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/agia0i/global_machine_learning_market_size_outlook/,luther1020,1547619222,,0,1,False,default,,,,,
838,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,15,agiatj,self.MachineLearning,[D] Google AI refuses to share dataset fields for a dataset paper (ACL'18) and associated challenge (at CVPR'19),https://www.reddit.com/r/MachineLearning/comments/agiatj/d_google_ai_refuses_to_share_dataset_fields_for_a/,binary_zeitgeist,1547619407,"I'd like to bring to the attention of the r/MachineLearning community that I came across Google's Conceptual Captions contest and dataset paper titled \[Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning\](\[[http://aclweb.org/anthology/P18-1238](http://aclweb.org/anthology/P18-1238)).

Repo Link: [https://github.com/google-research-datasets/conceptual-captions](https://github.com/google-research-datasets/conceptual-captions)

&amp;#x200B;

The dataset has roughly 3.3M images (all of them are hosted and some links are now broken).  Also:

* Refusal to share pretrained models making benchmarking and reporting numbers super hard (not everyone has 1k TPUs at their helm):  [https://github.com/google-research-datasets/conceptual-captions/issues/3](https://github.com/google-research-datasets/conceptual-captions/issues/3)
* Refusal to share Alt-text associated with each image (the title of the paper quite ironically is \`Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning\`): [https://github.com/google-research-datasets/conceptual-captions/issues/6](https://github.com/google-research-datasets/conceptual-captions/issues/6)
* Refusal to share images / mirror links (while I agree the there are legal issues, but with several hundred images missing from the dataset it becomes superhard for the community to compare models): [https://github.com/google-research-datasets/conceptual-captions/issues/1](https://github.com/google-research-datasets/conceptual-captions/issues/1)

It is extremely painful to see that after so many elaborate attempts made by Google (Colab, Dataset search engine etc, for which I am greatly thankful!) to promote open research, such instances happen.

I hope that people from the community realize that a dataset paper is a big responsibility to carry on one's shoulder and if there are legal issues which hinder sharing of datasets - publishing a paper on a private data is fine (with some fields not made public like Alt-text), but hosting a challenge on the same w/o releasing models or entire dataset doesn't seem supercool to me.  ",134,1,False,self,,,,,
839,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,15,agiiec,self.MachineLearning,Seeking mentor,https://www.reddit.com/r/MachineLearning/comments/agiiec/seeking_mentor/,Comrade49,1547621274,[removed],0,1,False,self,,,,,
840,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,16,agin5x,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/agin5x/global_machine_learning_market_size_outlook/,SingleMoney7,1547622468,[removed],0,1,False,self,,,,,
841,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,17,agj0v1,self.MachineLearning,AIML PROJECT,https://www.reddit.com/r/MachineLearning/comments/agj0v1/aiml_project/,Sarthaks21,1547626125,[removed],0,1,False,self,,,,,
842,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,17,agjb2j,self.MachineLearning,How I used NLP (Spacy) to screen Data Science Resumes,https://www.reddit.com/r/MachineLearning/comments/agjb2j/how_i_used_nlp_spacy_to_screen_data_science/,venkarafa,1547629143," 

## Do the keywords in your Resume aptly represent what type of Data Scientist youare? Position your Data Science resume better through NLP. [Data Science Resume Screening via NLP](https://towardsdatascience.com/do-the-keywords-in-your-resume-aptly-represent-what-type-of-data-scientist-you-are-59134105ba0d)",0,1,False,self,,,,,
843,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,18,agjdqz,self.MachineLearning,Machine Learning usage for Renewable,https://www.reddit.com/r/MachineLearning/comments/agjdqz/machine_learning_usage_for_renewable/,vigg_1991,1547629888,[removed],0,1,False,self,,,,,
844,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,18,agjf87,self.MachineLearning,Clustering Relevant Articles and getting the cluster related to the search item/items,https://www.reddit.com/r/MachineLearning/comments/agjf87/clustering_relevant_articles_and_getting_the/,AbubakarSaddique8156,1547630283,[removed],0,1,False,self,,,,,
845,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,18,agjfy2,blog.floydhub.com,[N] Improving Access to Sexual Health Information with AI Chatbots,https://www.reddit.com/r/MachineLearning/comments/agjfy2/n_improving_access_to_sexual_health_information/,pirate7777777,1547630504,,0,1,False,default,,,,,
846,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,18,agjkoz,blog.floydhub.com,[N] Improving Access to Sexual Health Information with AI Chatbots,https://www.reddit.com/r/MachineLearning/comments/agjkoz/n_improving_access_to_sexual_health_information/,pirate7777777,1547631878,,0,1,False,https://a.thumbs.redditmedia.com/mIDRWt8Sfx7uHiTEMilFDA1K3nb1kpspWUdjMwsvlB8.jpg,,,,,
847,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,19,agjpzz,self.MachineLearning,Where can I find a good Dataset for this kind of project?,https://www.reddit.com/r/MachineLearning/comments/agjpzz/where_can_i_find_a_good_dataset_for_this_kind_of/,SettySatt,1547633356, Stock Return Prediction using Fundamental Data - when can I find a good dataset of fundamental data of companies?,0,1,False,self,,,,,
848,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,19,agjtst,self.MachineLearning,Google AI Residency 2019: the most efficient preparation,https://www.reddit.com/r/MachineLearning/comments/agjtst/google_ai_residency_2019_the_most_efficient/,RemarkableRaccoon,1547634377,[removed],0,1,False,self,,,,,
849,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,19,agjzst,mamathaindustries.com,Aerospace components manufacturers in India,https://www.reddit.com/r/MachineLearning/comments/agjzst/aerospace_components_manufacturers_in_india/,mamathaindustries,1547635906,,0,1,False,default,,,,,
850,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,20,agk3ap,self.MachineLearning,Upto to how much extent Single Image Resolution in possible? What are the best papers for this to follow.,https://www.reddit.com/r/MachineLearning/comments/agk3ap/upto_to_how_much_extent_single_image_resolution/,denny_arjun,1547636752,[removed],0,1,False,self,,,,,
851,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,20,agk3zy,mamathaindustries.com,cnc machined components in india,https://www.reddit.com/r/MachineLearning/comments/agk3zy/cnc_machined_components_in_india/,mamathaindustries,1547636940,,0,1,False,https://b.thumbs.redditmedia.com/HsJTUt7bX30Qt8rwkGodTtYpw9kVZ5xQO5f9ac1Z5JE.jpg,,,,,
852,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,20,agketx,self.MachineLearning,Machine Learning Bangalore | Machine Learning Course Bangalore,https://www.reddit.com/r/MachineLearning/comments/agketx/machine_learning_bangalore_machine_learning/,businessinsights123,1547639635,[removed],0,1,False,self,,,,,
853,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,21,agknqb,self.MachineLearning,Machine Learning in Search,https://www.reddit.com/r/MachineLearning/comments/agknqb/machine_learning_in_search/,r3dp4nd477,1547641697,[removed],0,1,False,self,,,,,
854,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,21,agkw0z,self.MachineLearning,I'm unsure if I should really begin to study machine learning (AI)?,https://www.reddit.com/r/MachineLearning/comments/agkw0z/im_unsure_if_i_should_really_begin_to_study/,cipher1202,1547643508,[removed],0,1,False,self,,,,,
855,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,22,agkxtc,self.MachineLearning,Figuring out what method to use for our Project.,https://www.reddit.com/r/MachineLearning/comments/agkxtc/figuring_out_what_method_to_use_for_our_project/,mangow,1547643875,"I'm in my second year of a Bachelor studie in AI and I'm doing a project on Learning and Decision making. The project basically consists of ranking/classifying documents based on relevance. 

&amp;#x200B;

&amp;#x200B;

The data we have is the content of about 700 articles, labeled relevant or not ( 1/0 ).

The goal is to come up with a relevance score for any new article and be able to return the top K most relevant articles from a collection of N articles.

&amp;#x200B;

&amp;#x200B;

We're struggling to find the correct mehods to solve this issue, any help or ideas would be awesome.",0,1,False,self,,,,,
856,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,22,agl1cv,self.MachineLearning,[P] Struggling to find correct methods for a Bachelors project.,https://www.reddit.com/r/MachineLearning/comments/agl1cv/p_struggling_to_find_correct_methods_for_a/,mangow,1547644625,"I'm in my second year of a Bachelor studie in AI and I'm doing a project on Learning and Decision making. The project basically consists of ranking/classifying documents based on relevance.

&amp;#x200B;

The data we have is the content of about 700 articles, labeled relevant or not ( 1/0 ).

The goal is to come up with a relevance score for any new article and be able to return the top K most relevant articles from a collection of N articles.

&amp;#x200B;

We're struggling to find the correct mehods to solve this issue, any help or ideas would be awesome.",5,1,False,self,,,,,
857,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,22,agl8c9,self.MachineLearning,[R] Semi-Supervised Monaural Singing Voice Separation,https://www.reddit.com/r/MachineLearning/comments/agl8c9/r_semisupervised_monaural_singing_voice_separation/,mosheman5,1547646054,"[https://arxiv.org/abs/1812.06087](https://arxiv.org/abs/1812.06087)

Hi! First author here - 

In this paper we tackle the problem of source separation in audio. Given an audio stream of music, which is a mixture of instruments and singing vocals, we would like to filter out the singing voice. While most of the methods utilize supervised deep learning, we decided to use a more realistic approach of semi-supervised learning, by training our network on a set of samples of mixed music (singing and instrumental) and an unmatched set of instrumental music. 

To accomplish that, we train an autoencoder which outputs a mask applied on the mixture input, resulting an instrumental samples. By subtracting this sample from the original input we get the singing signal. 

To do this without supervision, we decidedto generatesynthetic mixture samples from singing samples and a random instrumental samples. The two main ideas the algorithm is based on are: 

1. Utilizing GAN losses for aligning the distribution of masked samples with instrumental music and the distribution of synthetic samples with real mixture samples. 
2. Reconstruction losses based on the superposition property of audio channels, creating more stable constrain than the GAN loss itself. 

In general, the proposed architecture is capable of learning to filter out other channel in music as well, and is relevant for every source separation problem, given the domain satisfies the superposition property - like most of waves physical phenomena's. 

You can listen to samples and a comparison with a fully supervised method can be found[here](https://sagiebenaim.github.io/Singing/) 

Check out our [paper](https://arxiv.org/abs/1812.06087) for the full details. In addition, our PyTorch code is available on[Github](https://github.com/sagiebenaim/Singing). 

Feel free to ask any questions. ",3,1,False,self,,,,,
858,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,22,aglcw4,medium.com,"Build smart contract applications based on air quality data from 10,210 global locations for free using Streamr!",https://www.reddit.com/r/MachineLearning/comments/aglcw4/build_smart_contract_applications_based_on_air/,thamilton5,1547646974,,0,1,False,https://b.thumbs.redditmedia.com/LI51Bunr1Knl9DVm8LC0TX9VIkIa2fklVsMSy-BQE5M.jpg,,,,,
859,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,23,agless,self.MachineLearning,Machine Learning in music,https://www.reddit.com/r/MachineLearning/comments/agless/machine_learning_in_music/,khoonay,1547647341,[removed],0,1,False,self,,,,,
860,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,23,aglfon,self.MachineLearning,[D] Solutions to the problems or finding rare events in highly imbalanced datasets,https://www.reddit.com/r/MachineLearning/comments/aglfon/d_solutions_to_the_problems_or_finding_rare/,MattR0se,1547647510,"I had this discussion at work about highly imbalanced dataset where one label is super rare and how to detect it. This problem is prevalent e.g. in HIV detection, wheather forecast, but also in this case of finding terrorists via facial recognition: [https://www.theguardian.com/uk-news/2018/may/15/uk-police-use-of-facial-recognition-technology-failure](https://www.theguardian.com/uk-news/2018/may/15/uk-police-use-of-facial-recognition-technology-failure)

&amp;#x200B;

I had another article with more detailed numbers, but that is in German, so the guardian article is for better illustration. This ist the German article if anyone is interested. I'll use numbers from there: [https://www.mpib-berlin.mpg.de/de/presse/2018/10/unstatistik-des-monats-erfolgreiche-gesichtserkennung-mit-hunderttausenden-fehlalarmen](https://www.mpib-berlin.mpg.de/de/presse/2018/10/unstatistik-des-monats-erfolgreiche-gesichtserkennung-mit-hunderttausenden-fehlalarmen)

&amp;#x200B;

So, the problem is, you have about 100 subjects (terrorists) in nearly 12 million people that visit the train station daily where they did the facial recognition. This system has a specificity of 99%, meaning there are around 11.900 false alarms every day. 

&amp;#x200B;

But every article I found so far that mentions this problem, doesn't have a solution other then ""deal with it"" or ""a human has to skim through this 12k alarms to find the true positives"". 

&amp;#x200B;

In medicine you would first filter your test data by risk factors, e.g. only apply a cancer prediction test for people above a certain age or with a certain family history, to raise the prevalence of the label you want to detect an thus make it more likely that a positive is a TP. But in this terrorist scenario, how would you do that? Maybe use a different algorithm only for the positive predictions?  ",0,1,False,self,,,,,
861,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,23,aglhi7,self.MachineLearning,Uncertainty Bayesien dropout,https://www.reddit.com/r/MachineLearning/comments/aglhi7/uncertainty_bayesien_dropout/,IAlover,1547647845,[removed],0,1,False,self,,,,,
862,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,23,agljug,self.MachineLearning,[D] Finding terrorists via facial recognition - How do you deal with highly imbalanced test data?,https://www.reddit.com/r/MachineLearning/comments/agljug/d_finding_terrorists_via_facial_recognition_how/,MattR0se,1547648291,"I had this discussion at work about highly imbalanced dataset where one label is super rare and how to detect it. This problem is prevalent e.g. in HIV detection, wheather forecast, but also in this case of finding terrorists via facial recognition: [https://www.theguardian.com/uk-news/2018/may/15/uk-police-use-of-facial-recognition-technology-failure](https://www.theguardian.com/uk-news/2018/may/15/uk-police-use-of-facial-recognition-technology-failure)

&amp;#x200B;

I had another article with more detailed numbers, but that is in German, so the guardian article is for better illustration. This ist the German article if anyone is interested. I'll use numbers from there: [https://www.mpib-berlin.mpg.de/de/presse/2018/10/unstatistik-des-monats-erfolgreiche-gesichtserkennung-mit-hunderttausenden-fehlalarmen](https://www.mpib-berlin.mpg.de/de/presse/2018/10/unstatistik-des-monats-erfolgreiche-gesichtserkennung-mit-hunderttausenden-fehlalarmen)

&amp;#x200B;

So, the problem is, you have about 100 subjects (terrorists) in nearly 12 million people that visit the train station daily where they did the facial recognition. This system has a specificity of 99%, meaning there are around 11.900 false alarms every day.

&amp;#x200B;

But every article I found so far that mentions this problem, doesn't have a solution other then ""deal with it"" or ""a human has to skim through this 12k alarms to find the true positives"".

&amp;#x200B;

In medicine you would first filter your test data by risk factors, e.g. only apply a cancer prediction test for people above a certain age or with a certain family history, to raise the prevalence of the label you want to detect an thus make it more likely that a positive is a TP. But in this terrorist scenario, how would you do that? Maybe use a different algorithm only for the positive predictions?",17,1,False,self,,,,,
863,MachineLearning,t5_2r3gv,2019-1-16,2019,1,16,23,aglovs,self.MachineLearning,[D] New Post: an Introduction to Mask R-CNN,https://www.reddit.com/r/MachineLearning/comments/aglovs/d_new_post_an_introduction_to_mask_rcnn/,zenggyu,1547649209,"Hi, everyone!

I just published a new post introducing Mask R-CNN. Here is the link: [https://blog.zenggyu.com/en/post/2019-01-07/introducing-the-key-advancements-of-mask-r-cnn/](https://blog.zenggyu.com/en/post/2019-01-07/introducing-the-key-advancements-of-mask-r-cnn/)

&amp;#x200B;

However, I am not completely sure if all my understandings are correct. If you find any thing wrong or have any questions, please leave a comment. Thanks!",0,1,False,self,,,,,
864,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agly49,self.MachineLearning,"Speed trained network, bottleneck....",https://www.reddit.com/r/MachineLearning/comments/agly49/speed_trained_network_bottleneck/,stanissse,1547650867,"Hey guys, im curious if anybody of you know how to speed up an already trained CNN. I tried it on several machines (from GTX 950m up to a GTX 1080 Ti) and it takes always the sime amount of time. Does anyone know where the bottleneck lies?

My entire dataset is just 60mb but consists of up to 100k images",0,1,False,self,,,,,
865,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agly6a,self.MachineLearning,Research  and important role in reproducible data science?,https://www.reddit.com/r/MachineLearning/comments/agly6a/research_and_important_role_in_reproducible_data/,Mayalittlepony,1547650876,[removed],0,1,False,self,,,,,
866,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,aglyhf,self.MachineLearning,What is your dream image dataset?,https://www.reddit.com/r/MachineLearning/comments/aglyhf/what_is_your_dream_image_dataset/,ComfortablyNumb190,1547650920,[removed],0,1,False,self,,,,,
867,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agm3me,self.MachineLearning,"""[D]"" What is your dream image dataset?",https://www.reddit.com/r/MachineLearning/comments/agm3me/d_what_is_your_dream_image_dataset/,ComfortablyNumb190,1547651789,"We probably all have dream cars and if you're in the machine learning industry you probably have your dream dataset :)  
Imagine being able to have any image dataset you want - for your current project or future project or your dream project, doesn't matter. What would that image dataset be?",14,1,False,self,,,,,
868,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agm3pw,self.MachineLearning,How is this made?,https://www.reddit.com/r/MachineLearning/comments/agm3pw/how_is_this_made/,RedditAcy,1547651804,[removed],0,1,False,self,,,,,
869,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agm5p9,self.MachineLearning,Looking for Insight in the life of an data scientist and literature on machine learning,https://www.reddit.com/r/MachineLearning/comments/agm5p9/looking_for_insight_in_the_life_of_an_data/,TheRelativeOne,1547652150,[removed],0,1,False,self,,,,,
870,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agmaue,self.MachineLearning,"[R][BAIR] Interpretable machine learning: definitions, methods, and applications",https://www.reddit.com/r/MachineLearning/comments/agmaue/rbair_interpretable_machine_learning_definitions/,neuralnets120,1547653007,"**Paper:** [https://arxiv.org/abs/1901.04592](https://arxiv.org/abs/1901.04592)

**Abstract**: Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them.We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant (PDR) framework for discussing interpretations. The PDR framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.",0,1,False,self,,,,,
871,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agmgos,self.MachineLearning,"Simple Questions Thread January 16, 2019",https://www.reddit.com/r/MachineLearning/comments/agmgos/simple_questions_thread_january_16_2019/,AutoModerator,1547653974,[removed],0,1,False,self,,,,,
872,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,0,agmipf,wired.com,#10yearchallenge being used for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/agmipf/10yearchallenge_being_used_for_machine_learning/,Breatnach,1547654327,,1,1,False,default,,,,,
873,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,1,agmotx,arxiv.org,Exploiting Synchronized Lyrics And Vocal Features For Music Emotion Detection,https://www.reddit.com/r/MachineLearning/comments/agmotx/exploiting_synchronized_lyrics_and_vocal_features/,loretoparisi,1547655316,,1,1,False,default,,,,,
874,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,1,agmujs,self.MachineLearning,can some one help me understand this and mostly build it,https://www.reddit.com/r/MachineLearning/comments/agmujs/can_some_one_help_me_understand_this_and_mostly/,champulal,1547656233,[removed],0,1,False,self,,,,,
875,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,1,agmz0y,self.MachineLearning,Intelligent web crawler and scraper,https://www.reddit.com/r/MachineLearning/comments/agmz0y/intelligent_web_crawler_and_scraper/,Ali_Hafeez,1547656936,[removed],0,1,False,self,,,,,
876,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,2,agn6tc,self.MachineLearning,"Suggestions on references -- use of gradient boosting (particularly XGBoost) for small n, large p problems?",https://www.reddit.com/r/MachineLearning/comments/agn6tc/suggestions_on_references_use_of_gradient/,NithSahor,1547658180,[removed],0,1,False,self,,,,,
877,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,2,agndz0,medium.com,Auto-DeepLab: Fei-Fei Li &amp; Alan Yuille on Semantic Image Segmentation,https://www.reddit.com/r/MachineLearning/comments/agndz0/autodeeplab_feifei_li_alan_yuille_on_semantic/,gwen0927,1547659314,,0,1,False,https://b.thumbs.redditmedia.com/BQv1JOJaFoFJ4czvBAFl1zsCCD-U9MFmfNZo2CadYNE.jpg,,,,,
878,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,2,agneir,github.com,Data scrapper to creat image collection for training safe va unsafe image classifiers,https://www.reddit.com/r/MachineLearning/comments/agneir/data_scrapper_to_creat_image_collection_for/,seanDL_,1547659400,,0,1,False,default,,,,,
879,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,2,agneq2,self.MachineLearning,New paper about machine learning applied to healthcare,https://www.reddit.com/r/MachineLearning/comments/agneq2/new_paper_about_machine_learning_applied_to/,DavideChicco,1547659431,"Hi all

I'm happy to announce my new paper ""[Computational prediction of diagnosis and feature selection on mesothelioma patient health records](https://doi.org/10.1371/journal.pone.0208737)"" has been published on the PLOS One journal within the [Machine Learning in Health and Biomedicine](https://collections.plos.org/mlforhealth) collection.

&amp;#x200B;

The paper describes the application of machine learning classification for the diagnosis of patients having mesothelioma symptoms, and the application of machine learning feature selection to detect the most important features in the dataset.

I hope you can find it interesting!

&amp;#x200B;

Cheers",0,1,False,self,,,,,
880,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,2,agnfqi,github.com,[P] Data scrapper to creat image collection for training safe va unsafe image classifiers,https://www.reddit.com/r/MachineLearning/comments/agnfqi/p_data_scrapper_to_creat_image_collection_for/,seanDL_,1547659588,,0,1,False,https://b.thumbs.redditmedia.com/n9dM2rvNyfP0NJOYm_lmOshfRFNNNs-FB4evU61XL9M.jpg,,,,,
881,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,3,ago0ol,self.MachineLearning,How should one start a career in machine learning?,https://www.reddit.com/r/MachineLearning/comments/ago0ol/how_should_one_start_a_career_in_machine_learning/,andrea_manero,1547662870,[removed],0,1,False,self,,,,,
882,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,3,ago2eb,self.MachineLearning,[D] Project ideas for adversarial machine learning,https://www.reddit.com/r/MachineLearning/comments/ago2eb/d_project_ideas_for_adversarial_machine_learning/,throw567891,1547663139,"I am doing a bachelor thesis in the field of adversarial machine learning. The project is very open ended and I am free to choose to work on anything that relates to generation of adversarial examples for machine learning algorithms and defense mechanisms to counteract that. 

I have read a few survey articles on the topic, but the field is very broad. So I am turning to the reddit hivemind for suggestions for project topics. For example, have you come across any security critical application of machine learning, a particularly interesting dataset, etc? 

Personally, I find that in the light of self driving cars, image classification of traffic signs and adversarial attacks towards that interesting.",2,1,False,self,,,,,
883,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,3,agobqe,ieeexplore.ieee.org,Detection of cyberbullying with the aid of deep learning method.,https://www.reddit.com/r/MachineLearning/comments/agobqe/detection_of_cyberbullying_with_the_aid_of_deep/,MistWeaver80,1547664616,,0,1,False,default,,,,,
884,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,4,agopbx,floydhub.com,"[P] Dataset:  5,623 Street art photos of 15 artists",https://www.reddit.com/r/MachineLearning/comments/agopbx/p_dataset_5623_street_art_photos_of_15_artists/,Coffenpaint,1547666769,,0,1,False,default,,,,,
885,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,4,agouj6,self.MachineLearning,[D] Reasonable Doubt: Get Onto the Top 35 MNIST Leaderboard by Quantifying Aleatoric Uncertainty,https://www.reddit.com/r/MachineLearning/comments/agouj6/d_reasonable_doubt_get_onto_the_top_35_mnist/,austingwalters,1547667602,"I recently read: [https://medium.com/capital-one-tech/reasonable-doubt-get-onto-the-top-35-mnist-leaderboard-by-quantifying-aleatoric-uncertainty-a8503f134497](https://medium.com/capital-one-tech/reasonable-doubt-get-onto-the-top-35-mnist-leaderboard-by-quantifying-aleatoric-uncertainty-a8503f134497)  


The TLDR: The methods described in the article provide an uncertainty metric with every prediction. For example, the number 1 and 7 can look very similar when written -- knowing the level of certainty for a given prediction can be useful for determining if you need additional data or how well the network can differentiate samples.  


I was curious if anyone else has worked with a neural network with an uncertainty estimate? If so, does anyone have a few samples where this has been implemented for other standard datasets?  


&amp;#x200B;",2,1,False,self,,,,,
886,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,4,agouvb,self.MachineLearning,is permutation testing functionally equivalent to training/test?,https://www.reddit.com/r/MachineLearning/comments/agouvb/is_permutation_testing_functionally_equivalent_to/,breezeason,1547667651,[removed],0,1,False,self,,,,,
887,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,5,agp53x,i.redd.it,Projeto de Fbricas de Produtos Qumicos VI,https://www.reddit.com/r/MachineLearning/comments/agp53x/projeto_de_fbricas_de_produtos_qumicos_vi/,JamurGerloff,1547669114,,0,1,False,https://b.thumbs.redditmedia.com/PswqRIMbTaK_9en8rU10cj_YRhQm778Ed9G13OgHRek.jpg,,,,,
888,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,5,agp6fa,self.MachineLearning,Machine Learning Challenge: count number of cells,https://www.reddit.com/r/MachineLearning/comments/agp6fa/machine_learning_challenge_count_number_of_cells/,AleksRuci,1547669331,[removed],0,1,False,self,,,,,
889,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,5,agpayo,self.MachineLearning,Machine Learning Challenge: count number of cells,https://www.reddit.com/r/MachineLearning/comments/agpayo/machine_learning_challenge_count_number_of_cells/,AleksRuci,1547670039,[removed],0,1,False,https://b.thumbs.redditmedia.com/Xht7m9OE3dZWvKeD01fbirOQQiOH0b_BaBzdZw-_FLQ.jpg,,,,,
890,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,5,agpmse,self.MachineLearning,[D] Methodology when approaching a type of problem you have minimal experience in.,https://www.reddit.com/r/MachineLearning/comments/agpmse/d_methodology_when_approaching_a_type_of_problem/,Baudelaire-1821,1547671811,"My experience in ML so far comes down to reading textbooks and implementing various ""Hall of fame"" papers. I'm now working on a project with a well define modeling and anomalie detection problem on time series data with a sizable labelled dataset to work with, which is not a type problem I've worked on before.  I now need to explore the range of possibles models and each of their many variations that could help me solve that problem. 

I've mostly just been googling around looking at related papers and blog posts, looking for hits that look like what I'm looking for but I'm wondering if there isn't a better way. How do you guys usually approch these kind of researches? What have you found works best?

Thanks.",5,1,False,self,,,,,
891,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,5,agpo5d,youtu.be,Now AI will shoot your vlog,https://www.reddit.com/r/MachineLearning/comments/agpo5d/now_ai_will_shoot_your_vlog/,cmillionaire9,1547672010,,0,1,False,https://b.thumbs.redditmedia.com/QF8jtK313wuFSW9L2lK6gDFyiMrpDVDfC2qF34igjjU.jpg,,,,,
892,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,7,agqk4b,stacksocial.com,[R] Machine Learning and Artificial Intelligence Bundle - Learn the Mathematics &amp; Algorithms Behind the Next Great Tech Frontier with These 11 Instructive Hours,https://www.reddit.com/r/MachineLearning/comments/agqk4b/r_machine_learning_and_artificial_intelligence/,808hunna,1547676807,,0,1,False,default,,,,,
893,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,7,agqley,self.MachineLearning,What is Big Data ?,https://www.reddit.com/r/MachineLearning/comments/agqley/what_is_big_data/,mrcgllr,1547677028,[removed],0,1,False,self,,,,,
894,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,7,agqmhn,self.MachineLearning,How is ML can help radiologists,https://www.reddit.com/r/MachineLearning/comments/agqmhn/how_is_ml_can_help_radiologists/,meimadX,1547677200,[removed],0,1,False,self,,,,,
895,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,7,agqmjh,self.MachineLearning,[N] Predicting Industrial Timeseries for Water Conservation (New ML Competition),https://www.reddit.com/r/MachineLearning/comments/agqmjh/n_predicting_industrial_timeseries_for_water/,dat-um,1547677209,"[https://www.drivendata.org/competitions/56/predict-cleaning-time-series/](https://www.drivendata.org/competitions/56/predict-cleaning-time-series/)

&amp;#x200B;

We just launched a new ML competition with a different kind of format that in the end will reward the most thoughtful and compelling approaches, rather than \_just\_ the most accurate ones.

&amp;#x200B;

The dataset is a complex set of related time series, and the most interesting models will build useful features on top of those time series to make sense of that relationship.

&amp;#x200B;

Ultimately, the goal is to reduce water waste by adjusting rinsing processes with food and beverage production pipelines, and the winners are eligible for 20,000 in prizes!

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
896,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,8,agr8x9,self.MachineLearning,[D] You dont know JAX,https://www.reddit.com/r/MachineLearning/comments/agr8x9/d_you_dont_know_jax/,baylearn,1547680716,"This brief tutorial covers the basics of [JAX](https://github.com/google/jax/). JAX is a Python library which augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs.

https://colinraffel.com/blog/you-don-t-know-jax.html",11,1,False,self,,,,,
897,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,8,agrkfe,technologyreview.com,A robot dog has learned to run faster with machine learning,https://www.reddit.com/r/MachineLearning/comments/agrkfe/a_robot_dog_has_learned_to_run_faster_with/,ai-lover,1547682677,,0,1,False,https://a.thumbs.redditmedia.com/zQFlgI9S1qVxGO3LglZINFGfgLKOjnB0WQ3xQ4a7bj0.jpg,,,,,
898,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,9,agrvx0,self.MachineLearning,[X-post from /r/reddit] PyTorch's THC library in Rust,https://www.reddit.com/r/MachineLearning/comments/agrvx0/xpost_from_rreddit_pytorchs_thc_library_in_rust/,joshgev,1547684693,[removed],0,1,False,self,,,,,
899,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,9,agrza7,self.MachineLearning,[X-post from /r/pytorch] PyTorch's THC in Rust,https://www.reddit.com/r/MachineLearning/comments/agrza7/xpost_from_rpytorch_pytorchs_thc_in_rust/,joshgev,1547685296,[removed],0,1,False,self,,,,,
900,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,9,ags0cu,self.MachineLearning,[R] Transformer-XL Explained: Combining Transformers and RNNs Into a State-of-the-Art Language Model,https://www.reddit.com/r/MachineLearning/comments/ags0cu/r_transformerxl_explained_combining_transformers/,ranihorev,1547685493,"Hey everyone,

I wrote a summary of a very interesting paper by Google and Carnegie Mellon University - [Transformer-XL](https://arxiv.org/abs/1706.03762): Attentive Language Models Beyond a Fixed-Length Context. The model expands the vanilla Transformer and adds a recurrence mechanism to learn long-term dependencies between tokens. 

It achieves SOTA results on several language modeling benchmarks of both words and characters, such as WikiText-103 and enwik8. 

[https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/](https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/)

Id love to get your feedback.

p.s.

Im thinking of training it on the GLUE tasks and compare it to BERT. Has anyone tried that?",11,1,False,self,,,,,
901,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,10,agsdde,self.MachineLearning,MXNet boosts CPU performance with MKL-DNN  Apache MXNet  Medium,https://www.reddit.com/r/MachineLearning/comments/agsdde/mxnet_boosts_cpu_performance_with_mkldnn_apache/,yvrdeeplearning,1547687807,[removed],0,1,False,self,,,,,
902,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,10,agsfzo,self.MachineLearning,[D] Reduce the amount of time spent analyzing research papers,https://www.reddit.com/r/MachineLearning/comments/agsfzo/d_reduce_the_amount_of_time_spent_analyzing/,c0cky_,1547688333,"I've been reading through tons of research papers and I realized from talking to others that most time is spent following references and learning about the previously covered topics. 

&amp;#x200B;

To reduce the amount of time that is spent following references and recursively reading multiple papers to get a gist of a paper we may be able to annotate research papers also in the same manner as ""rap genius"". Essentially each passage would be annotated through crowd sourcing and would allow for people to give succinct intuition behind certain paragraphs in the paper.

&amp;#x200B;

I'm currently working on a prototype and am going to be giving early access to this product which I will release 100% for free. If interested please share your email address and I would love to have the help of the community for feedback. [http://beta.scholarlib.co/landing/](http://beta.scholarlib.co/landing/)",46,1,False,self,,,,,
903,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,11,agt5g1,self.MachineLearning,Tips to speed up Keras LSTM time per epoch?,https://www.reddit.com/r/MachineLearning/comments/agt5g1/tips_to_speed_up_keras_lstm_time_per_epoch/,StatusMemory5,1547692997,[removed],0,1,False,self,,,,,
904,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,12,agtcb4,self.MachineLearning,[D] Object Detection with many anchor boxes,https://www.reddit.com/r/MachineLearning/comments/agtcb4/d_object_detection_with_many_anchor_boxes/,moewiewp,1547694271,"I wonder why top performance conv-nets like Faster R-CNN only use three anchor box ratio and three anchor box sizes? Why should we use more ratios and more sizes so that we can lower the work load on the regression part, since there are more anchors, the event when some anchor nearly fit the bounding box by default is more likely to happen? So what is the pros and cons of using, like, 100 anchors for each grid pixel instead of 9 ?",4,1,False,self,,,,,
905,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,13,agu1m3,self.MachineLearning,What ML articles are interesting about spotting cancer?,https://www.reddit.com/r/MachineLearning/comments/agu1m3/what_ml_articles_are_interesting_about_spotting/,AutismoFranswazy,1547699137,[removed],0,1,False,self,,,,,
906,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,14,agueim,youtube.com,How machine learning helps identify potholes on Los Angeles roads (Video),https://www.reddit.com/r/MachineLearning/comments/agueim/how_machine_learning_helps_identify_potholes_on/,ai-lover,1547701972,,0,1,False,https://b.thumbs.redditmedia.com/z_2aDwkibe7AZbqG8df2YUlsCsRr4lqMukusUYyOhaQ.jpg,,,,,
907,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,15,agutpd,self.MachineLearning,How much math do you use in industry while working on projects?,https://www.reddit.com/r/MachineLearning/comments/agutpd/how_much_math_do_you_use_in_industry_while/,bright7860,1547705437,[removed],0,1,False,self,,,,,
908,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,15,aguw4g,self.MachineLearning,[P] Generating data for project,https://www.reddit.com/r/MachineLearning/comments/aguw4g/p_generating_data_for_project/,joaoperfig,1547706016,"Hello there,

I was just wondering if anyone would be interested in helping me with a project.

I am a computer science student am am working on a machine learning for a class.

We are training a neural network to play snake from user-generated data.

I wanted to ask if anyone would be interested in playing a few rounds of snake and sending me the data.

We are having a hard time generating enough data to do something useful.

The project is here: [https://github.com/joaoperfig/SnakeML](https://github.com/joaoperfig/SnakeML)

Just clone, cd to source, run snake\_game.py and play!  
Afterwards just send me the recent log files on the resources folder.

(If you dont have tensorflow you can just comment out the line where it is imported)

Thank you!",4,1,False,self,,,,,
909,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,15,agv3kw,self.MachineLearning,"Train,Validation and Test set loop logic",https://www.reddit.com/r/MachineLearning/comments/agv3kw/trainvalidation_and_test_set_loop_logic/,badmrsynth,1547707835,[removed],0,1,False,self,,,,,
910,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,16,agvek2,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/agvek2/global_machine_learning_market_size_outlook/,ruleyourlyf,1547710608,[removed],0,1,False,self,,,,,
911,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,16,agvgfp,self.MachineLearning,[D] Multi agent gridworld environment,https://www.reddit.com/r/MachineLearning/comments/agvgfp/d_multi_agent_gridworld_environment/,Maplernothaxor,1547711121,"Im looking for a gridworld environment (like pycolab by Deepmind) that supports multiple agents. Has someone come across a tool like this?

",7,1,False,self,,,,,
912,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,16,agvi8c,self.MachineLearning,"PRML vs MALPP, which is better?",https://www.reddit.com/r/MachineLearning/comments/agvi8c/prml_vs_malpp_which_is_better/,AlexanderYau,1547711624,[removed],0,1,False,self,,,,,
913,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,16,agvj0x,theappsolutions.com,The Definitive Guide to Pattern Recognition,https://www.reddit.com/r/MachineLearning/comments/agvj0x/the_definitive_guide_to_pattern_recognition/,lady_monsoon,1547711841,,0,1,False,default,,,,,
914,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,17,agvkih,nature.com,Measuring AI's ability to learn can be difficult.,https://www.reddit.com/r/MachineLearning/comments/agvkih/measuring_ais_ability_to_learn_can_be_difficult/,MistWeaver80,1547712238,,0,1,False,default,,,,,
915,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,17,agvkmm,self.MachineLearning,Any detailed derivation of backpropagation for Residual Neural Network?,https://www.reddit.com/r/MachineLearning/comments/agvkmm/any_detailed_derivation_of_backpropagation_for/,qudcjf7928,1547712265,"I understand what are Resnets. 

&amp;#x200B;

I'm just kinda lazy to find the actual gradients used to training Resnet

&amp;#x200B;

I reckon you train it Res block by Res block ",0,1,False,self,,,,,
916,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,17,agvl5s,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/agvl5s/global_machine_learning_market_size_outlook/,vardhan1020,1547712406,,0,1,False,default,,,,,
917,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,17,agvrtd,self.MachineLearning,Is there a way using my own medical and pharmaceutical corpus to pre-train BERT embedding while still loading the pre-trained embedding trained on Wikipedia corpus released by Google?,https://www.reddit.com/r/MachineLearning/comments/agvrtd/is_there_a_way_using_my_own_medical_and/,weichencoder,1547714272,[removed],0,1,False,self,,,,,
918,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,17,agvw80,i.redd.it,The real deep learning project life cycle,https://www.reddit.com/r/MachineLearning/comments/agvw80/the_real_deep_learning_project_life_cycle/,heidiki01,1547715518,,0,1,False,https://b.thumbs.redditmedia.com/IJWNtg2kK57s7EHwN3Y3hFnVtCJotOercsag3WH8i7k.jpg,,,,,
919,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,18,agw181,self.MachineLearning,Graphics Card for Deep Learning - RTX 2060 vs GTX 1070 ti,https://www.reddit.com/r/MachineLearning/comments/agw181/graphics_card_for_deep_learning_rtx_2060_vs_gtx/,ktessera,1547716939,[removed],0,1,False,self,,,,,
920,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,18,agw1k7,medium.com,[P] Blog recommendation: A detailed example of building a system evaluating neural net models at serving time,https://www.reddit.com/r/MachineLearning/comments/agw1k7/p_blog_recommendation_a_detailed_example_of/,RealJon,1547717034,,0,1,False,default,,,,,
921,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,18,agw3uz,blockference.com,"Global Big Data Conference - Santa Clara, CA",https://www.reddit.com/r/MachineLearning/comments/agw3uz/global_big_data_conference_santa_clara_ca/,Zukicha,1547717670,,0,1,False,default,,,,,
922,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,18,agw7o9,blog.ntrlab.com,"Interview with Emrah Gultekin, Co-founder of Chooch, an AI training platform for visual recognition, based in San Francisco.",https://www.reddit.com/r/MachineLearning/comments/agw7o9/interview_with_emrah_gultekin_cofounder_of_chooch/,Batareika_1,1547718728,,0,1,False,https://b.thumbs.redditmedia.com/PvUWBUiw6oFjXGxqu1cnuBuHswLicOiCczO_Q8s2UjM.jpg,,,,,
923,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,18,agw7r6,self.MachineLearning,Data preprocessing,https://www.reddit.com/r/MachineLearning/comments/agw7r6/data_preprocessing/,rk28394,1547718751,[removed],0,1,False,self,,,,,
924,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,19,agwaze,nature.com,Learnability can be undecidable | Nature Machine Intelligence,https://www.reddit.com/r/MachineLearning/comments/agwaze/learnability_can_be_undecidable_nature_machine/,foremn,1547719635,,2,1,False,default,,,,,
925,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,19,agwgs3,self.MachineLearning,KFAC implementation in Pytorch,https://www.reddit.com/r/MachineLearning/comments/agwgs3/kfac_implementation_in_pytorch/,akssri,1547721123,[removed],0,1,False,self,,,,,
926,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,19,agwhh9,self.MachineLearning,Real time crowd scanner using facial recognition.,https://www.reddit.com/r/MachineLearning/comments/agwhh9/real_time_crowd_scanner_using_facial_recognition/,Gkiid,1547721310,[removed],0,1,False,self,,,,,
927,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,20,agwvd7,medium.com,[P] Scaling TensorFlow model evaluation to unlimited data items per request with bounded latency,https://www.reddit.com/r/MachineLearning/comments/agwvd7/p_scaling_tensorflow_model_evaluation_to/,RealJon,1547724688,,0,1,False,default,,,,,
928,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,20,agwvrv,ftnctefw.tumblr.com,Nihilumbra [Download] 20% Off,https://www.reddit.com/r/MachineLearning/comments/agwvrv/nihilumbra_download_20_off/,ninadaniel956,1547724789,,0,1,False,default,,,,,
929,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,20,agwzev,flyelephant.net,Data Science Tools Survey 2019. Please take a part in the survey and share your experience ;),https://www.reddit.com/r/MachineLearning/comments/agwzev/data_science_tools_survey_2019_please_take_a_part/,flyelephant,1547725689,,0,1,False,default,,,,,
930,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,21,agx5gp,self.MachineLearning,DCGAN least square loss,https://www.reddit.com/r/MachineLearning/comments/agx5gp/dcgan_least_square_loss/,hpobaschnig,1547727049,[removed],0,1,False,self,,,,,
931,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,21,agx8a0,self.MachineLearning,[P] DCGAN least square loss,https://www.reddit.com/r/MachineLearning/comments/agx8a0/p_dcgan_least_square_loss/,hpobaschnig,1547727662,"How should [this reference implementation](https://github.com/carpedm20/DCGAN-tensorflow) of DCGAN be modified to use the least square loss instead of the standard sigmoid cross entropy loss?

        def sigmoid_cross_entropy_with_logits(x, y):
          try:
            return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)
          except:
            return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)
    
        self.d_loss_real = tf.reduce_mean(
          sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))
        self.d_loss_fake = tf.reduce_mean(
          sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))
        self.g_loss = tf.reduce_mean(
          sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))

&amp;#x200B;",2,1,False,self,,,,,
932,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,21,agxb43,self.MachineLearning,Why is Natural Language Processing Important to Enterprise Analytics?,https://www.reddit.com/r/MachineLearning/comments/agxb43/why_is_natural_language_processing_important_to/,ElegantMicroWebIndia,1547728286,[removed],0,1,False,self,,,,,
933,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,21,agxgdk,datasciencedigest.org,DataScience Digest #16,https://www.reddit.com/r/MachineLearning/comments/agxgdk/datascience_digest_16/,flyelephant,1547729433,,0,1,False,default,,,,,
934,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,21,agxhm6,ctuning.org,[N] The first SysML conference will attempt to reproduce results from accepted papers,https://www.reddit.com/r/MachineLearning/comments/agxhm6/n_the_first_sysml_conference_will_attempt_to/,gtechmisc,1547729697,,0,1,False,default,,,,,
935,MachineLearning,t5_2r3gv,2019-1-17,2019,1,17,22,agxv86,self.MachineLearning,Cost Function Returning NaN,https://www.reddit.com/r/MachineLearning/comments/agxv86/cost_function_returning_nan/,brussell015,1547732495,[removed],1,1,False,self,,,,,
936,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,0,agyqrh,self.MachineLearning,A modest exercise in pytorch: attempting to control the mouse pointer using a webcam,https://www.reddit.com/r/MachineLearning/comments/agyqrh/a_modest_exercise_in_pytorch_attempting_to/,boucherm,1547738244,[removed],0,1,False,self,,,,,
937,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,0,agz47m,self.MachineLearning,[D] Why STL-10 is not popular?,https://www.reddit.com/r/MachineLearning/comments/agz47m/d_why_stl10_is_not_popular/,tsauri,1547740434,"Overview of dataset:  
[https://cs.stanford.edu/\~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)  
Built-in torchvision.datasets: [https://pytorch.org/docs/0.4.0/\_modules/torchvision/datasets/stl10.html](https://pytorch.org/docs/0.4.0/_modules/torchvision/datasets/stl10.html)

* **10 classes:** airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.
* Images are **96x96** pixels, color.
* 500 training images (10 pre-defined folds), 800 test images per class.
* 100000 unlabeled images for unsupervised learning. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.
* **Images were acquired from labeled examples on** [**ImageNet**](http://www.image-net.org/)**.**  


I think we need to ditch CIFAR-10 for good. We are overfitting to it anyway. ",4,1,False,self,,,,,
938,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,2,ah02hw,deeplearningio.com,Neural Networks and Deep Learning  V : Gradient Descent | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/ah02hw/neural_networks_and_deep_learning_v_gradient/,mrcgllr,1547745822,,0,1,False,default,,,,,
939,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,2,ah02jg,self.MachineLearning,[P] Small scale machine learning projects to understand the core concepts.,https://www.reddit.com/r/MachineLearning/comments/ah02jg/p_small_scale_machine_learning_projects_to/,signal_v_noise,1547745829,"Small scale machine learning projects to understand the core concepts.

The best way to learn is to get your hands dirty with code and data. Please feel free to contribute to this repository as well.

[https://github.com/devAmoghS/machine-learning-with-python](https://github.com/devAmoghS/machine-learning-with-python?fbclid=IwAR2z47DsMzip66xIb-GAxXPi0ENVHsYqhUYSB1Wlf8wFhde__fbCWuOw-qk)

[\#machinelearning](https://www.facebook.com/hashtag/machinelearning?source=feed_text&amp;epa=HASHTAG) [\#datascience](https://www.facebook.com/hashtag/datascience?source=feed_text&amp;epa=HASHTAG) [\#deeplearning](https://www.facebook.com/hashtag/deeplearning?source=feed_text&amp;epa=HASHTAG)",5,1,False,self,,,,,
940,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,2,ah06ig,self.MachineLearning,[D] Deep Learning State of the Art (2019) - MIT,https://www.reddit.com/r/MachineLearning/comments/ah06ig/d_deep_learning_state_of_the_art_2019_mit/,UltraMarathonMan,1547746390,"This is a lecture on select recent developments in deep learning that are defining the state of the art in our field (algorithms, applications, and tools). Not a complete list, but hopefully includes a good sampling of new exciting ideas: [https://www.youtube.com/watch?v=53YvP6gdD7U](https://www.youtube.com/watch?v=53YvP6gdD7U)

In comments, let me know what ideas / breakthroughs / applications you think should have been included and you would like to see covered in the future!

* [Lecture video on YouTube](https://www.youtube.com/watch?v=53YvP6gdD7U) (and [Playlist](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf))
* [Slides for the lecture (PDF)](https://www.dropbox.com/s/v3rq3895r05xick/deep_learning_state_of_the_art.pdf?dl=0)
* Website for the series: [https://deeplearning.mit.edu](https://deeplearning.mit.edu/)
* GitHub repo for tutorials: [https://github.com/lexfridman/mit-deep-learning](https://github.com/lexfridman/mit-deep-learning)

https://i.redd.it/m7jjw3owq0b21.png

**Outline of the lecture:**

* BERT and Natural Language Processing
* Tesla Autopilot Hardware v2+: Neural Networks at Scale
* AdaNet: AutoML with Ensembles
* AutoAugment: Deep RL Data Augmentation
* Training Deep Networks with Synthetic Data
* Segmentation Annotation with Polygon-RNN++
* DAWNBench: Training Fast and Cheap
* BigGAN: State of the Art in Image Synthesis
* Video-to-Video Synthesis
* Semantic Segmentation
* AlphaZero &amp; OpenAI Five
* Deep Learning Frameworks",32,1,False,https://b.thumbs.redditmedia.com/4p2Rasc2jZsKa0uNPgh08ublvQIR8T01wYrR-BTsG5w.jpg,,,,,
941,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,2,ah08m5,dropbox.com,[R] Deep Learning State of the Art (Jan 2019),https://www.reddit.com/r/MachineLearning/comments/ah08m5/r_deep_learning_state_of_the_art_jan_2019/,rememberlennydotcom,1547746683,,0,1,False,default,,,,,
942,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,3,ah0s1n,blog.floydhub.com,"[N] Using NLP to Automate Customer Support, Part Two",https://www.reddit.com/r/MachineLearning/comments/ah0s1n/n_using_nlp_to_automate_customer_support_part_two/,pirate7777777,1547749626,,0,1,False,default,,,,,
943,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,3,ah0ytr,nature.com,Unknown human ancestor has been identified with the aid of artificial intelligence.,https://www.reddit.com/r/MachineLearning/comments/ah0ytr/unknown_human_ancestor_has_been_identified_with/,MistWeaver80,1547750697,,0,1,False,https://b.thumbs.redditmedia.com/n2VJrNt1If4a8kdUDPOyGqPuNi8pVcw-Mr3zw3tAdcs.jpg,,,,,
944,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,3,ah10qf,self.MachineLearning,[D] What exactly does a machine learning engineer do?,https://www.reddit.com/r/MachineLearning/comments/ah10qf/d_what_exactly_does_a_machine_learning_engineer_do/,Fardin_Shahriar,1547750995,*What will* a typical **machine learning engineer** *do* exactly everyday?,12,1,False,self,,,,,
945,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,4,ah1egd,youtube.com,Naive bayes classifier: naive bayes algorithm with Example,https://www.reddit.com/r/MachineLearning/comments/ah1egd/naive_bayes_classifier_naive_bayes_algorithm_with/,basma_gamal,1547753130,,0,1,False,default,,,,,
946,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,5,ah1vh9,self.MachineLearning,"How do you learn ML without/before knowing how to code in Python, or even as a beginner to the Python language?",https://www.reddit.com/r/MachineLearning/comments/ah1vh9/how_do_you_learn_ml_withoutbefore_knowing_how_to/,rocco20,1547755716,[removed],0,1,False,self,,,,,
947,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,5,ah25d9,self.MachineLearning,Optimize Inference Time,https://www.reddit.com/r/MachineLearning/comments/ah25d9/optimize_inference_time/,shomerj,1547757206,[removed],0,1,False,self,,,,,
948,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,6,ah2ndx,sci-hub.tw,[R] Vicarious Published Another Paper,https://www.reddit.com/r/MachineLearning/comments/ah2ndx/r_vicarious_published_another_paper/,darkconfidantislife,1547759983,,0,1,False,default,,,,,
949,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,7,ah3c1s,self.MachineLearning,[D] Isn't n-step q-learning incorrect from mathematical perspective?,https://www.reddit.com/r/MachineLearning/comments/ah3c1s/d_isnt_nstep_qlearning_incorrect_from/,djrx,1547763689,"Recently I've been trying to implement a Rainbow DQN agent as a combination of all the improvements to the original DQN by Minh et al. What caught my attention was so-called ""n-step DQN"" or ""multi-step learning"". All other improvements in the rainbow paper (https://arxiv.org/abs/1710.02298) are direct references to research publications. This one, is referencing an A3C paper which is a policy gradient and on-policy, as well as Sutton and Barto book which pretty explicitly does not cover n-step q-learning as the mathematics behind it doesn't really work - they propose a modification called ""tree backup"" that looks quite a bit different.

Isn't that the problem with the Rainbow paper then as well? I haven't seen anywhere any discussion that this paper, honestly quite popular and widely discussed, proposes a method inconsistent from a mathematical point of view.

I can see that from a practical point of view n-step q-learning may work as the policy from the replay buffer may be in the end close enough to the current policy for practical purposes, but shouldn't this shortcoming be at least discussed somewhere? Am I missing something here?",6,1,False,self,,,,,
950,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,7,ah3ivs,self.MachineLearning,Deciding on weights for unbalanced data set - DNN Classifier,https://www.reddit.com/r/MachineLearning/comments/ah3ivs/deciding_on_weights_for_unbalanced_data_set_dnn/,dztrader62,1547764777,[removed],0,1,False,self,,,,,
951,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,8,ah3scc,self.artificial,New trends in Computer Vision?,https://www.reddit.com/r/MachineLearning/comments/ah3scc/new_trends_in_computer_vision/,ftavella,1547766325,,0,1,False,default,,,,,
952,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,8,ah49la,self.MachineLearning,[D] LDA normalization of multinomial parameter: use of expectation,https://www.reddit.com/r/MachineLearning/comments/ah49la/d_lda_normalization_of_multinomial_parameter_use/,cuenta4384,1547769205,"The expectation of the sufficient statistics of the Dirichlet can be defined as:

E\_q\[log(\_i)| \] = (\_i) (\_{j=1}\^k \_j ) (eq 8)

In the original [paper of LDA](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), Blei gets the following update for the multinomial parameter:

\_{ni} \\prop \_{iw\_n} exp{E\_q\[log(\_i)| \]}  (eq 6)

However, in the algorithm in figure 6 (line 6), he only computes only one part of the expectation (i.e. (\_i)), but he normalizes after (in line 7), which I suppose is because of the missing part of the expectation ( (\_{j=1}\^k \_j )). Am I right? 

(Lines of the algorithm)

(6) \^{t+1}\_{ni} := {iw\_n} exp((\_i)) 

(7) normalize \^{t+1}\_n to sum to 1.

&amp;#x200B;

Since the expectation of the ss is equal to the derivative of the log normalization factor, shouldn't we use the complete expression? Why is this possible?

&amp;#x200B;

That means if I have *(a) - (a+b) = something,* I can evaluate *(a) = something* and normalize the result.

&amp;#x200B;

Sorry for the notation. Haven't found a way to write LaTex yet. ",0,1,False,self,,,,,
953,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,9,ah4spa,self.MachineLearning,How can I perform a T-Test to compare the performance of various models?,https://www.reddit.com/r/MachineLearning/comments/ah4spa/how_can_i_perform_a_ttest_to_compare_the/,jovany08,1547772500,[removed],0,1,False,self,,,,,
954,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,10,ah4zgv,heartbeat.fritz.ai,Training a Core ML Model with Turi Create to Classify Dog Breeds,https://www.reddit.com/r/MachineLearning/comments/ah4zgv/training_a_core_ml_model_with_turi_create_to/,vhanagwal,1547773734,,0,1,False,default,,,,,
955,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,11,ah5jzh,self.MachineLearning,I'm curious about difference between training scheme.,https://www.reddit.com/r/MachineLearning/comments/ah5jzh/im_curious_about_difference_between_training/,youngminpark2559,1547777520,[removed],0,1,False,self,,,,,
956,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,12,ah6970,i.redd.it,Face Generation with DCGAN,https://www.reddit.com/r/MachineLearning/comments/ah6970/face_generation_with_dcgan/,ThomasP32,1547782307,,0,1,False,default,,,,,
957,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,12,ah6b5q,self.MachineLearning,AI for reddit,https://www.reddit.com/r/MachineLearning/comments/ah6b5q/ai_for_reddit/,Mikamcha,1547782677,[removed],0,1,False,self,,,,,
958,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,12,ah6cki,self.MachineLearning,Face generation with DCGAN,https://www.reddit.com/r/MachineLearning/comments/ah6cki/face_generation_with_dcgan/,ThomasP32,1547782947,[removed],1,1,False,self,,,,,
959,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,13,ah6oyu,self.MachineLearning,Ceres vs Pytorch,https://www.reddit.com/r/MachineLearning/comments/ah6oyu/ceres_vs_pytorch/,soulslicer0,1547785385,"I currently have some code that is implemented to perform some numerical optimization in Ceres. It is working, but very slow. I am considering porting all of it to Pytorch, and I am curious to know if the GPU assisted gradient computation should give me a boost? I have working with some large matrices here, with most of them being sparse actually. I want to know if its worth the effort to do this",0,1,False,self,,,,,
960,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,13,ah6r96,self.MachineLearning,"Code and data for paper ""Deep Painterly Harmonization""",https://www.reddit.com/r/MachineLearning/comments/ah6r96/code_and_data_for_paper_deep_painterly/,lvduyet,1547785851,[removed],0,1,False,self,,,,,
961,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,14,ah72mb,self.MachineLearning,Statistics of machine learning and deep learning over the past one decade,https://www.reddit.com/r/MachineLearning/comments/ah72mb/statistics_of_machine_learning_and_deep_learning/,sairaamv92,1547788221,[removed],0,1,False,self,,,,,
962,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,14,ah748l,self.MachineLearning,[D] Statistics of machine learning and deep learning over the past one decade,https://www.reddit.com/r/MachineLearning/comments/ah748l/d_statistics_of_machine_learning_and_deep/,sairaamv92,1547788558,"Hi  everyone, could you guys point me out to any resource that documents  the statistics of publications in machine learning and deep learning over the past one decade?

I  suspect that over the past 5 years, deep learning papers have dominated  the arena, but I would like to see some statistics for this.

Also, I would like to know the increasing role the industry has had in these publications. So, if someone could also give statistics on the demographics (job-wise), it would be nice.

Thank you!",1,1,False,self,,,,,
963,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,15,ah7k8t,self.MachineLearning,#10YearChallenge - Machine Learning or just a meme?,https://www.reddit.com/r/MachineLearning/comments/ah7k8t/10yearchallenge_machine_learning_or_just_a_meme/,nirm44n,1547792136,[removed],0,1,False,self,,,,,
964,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,15,ah7ni9,self.MachineLearning,[D] What do you know about the careers in Artificial Intelligence ?,https://www.reddit.com/r/MachineLearning/comments/ah7ni9/d_what_do_you_know_about_the_careers_in/,Fardin_Shahriar,1547792865,What kind of jobs should you start looking for? What skills would you need to get hired?,4,1,False,self,,,,,
965,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,16,ah7xm6,self.MachineLearning,What is your opinion on artificial intelligence vs machine learning?,https://www.reddit.com/r/MachineLearning/comments/ah7xm6/what_is_your_opinion_on_artificial_intelligence/,Kosha_Shah,1547795304,[removed],0,1,False,self,,,,,
966,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,16,ah850e,self.MachineLearning,How to find/make a database of surrealist paintings?,https://www.reddit.com/r/MachineLearning/comments/ah850e/how_to_findmake_a_database_of_surrealist_paintings/,AutomaticInsect,1547797207,[removed],0,1,False,self,,,,,
967,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,16,ah8968,self.MachineLearning,I am a newbie in the ML world with 10+ years of experience in data management. I have exposer to data in my org. and would like to explore ML more from learning perspective. Does reddit have pointers on good use cases in consumer banking industry?,https://www.reddit.com/r/MachineLearning/comments/ah8968/i_am_a_newbie_in_the_ml_world_with_10_years_of/,PasadenaJoe,1547798329,,0,1,False,self,,,,,
968,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,17,ah8gf1,envisioninteligence.com,"Machine Learning as a Service (Mlaas) Market  Size, Outlook, Trends and Forecasts (2018  2024)",https://www.reddit.com/r/MachineLearning/comments/ah8gf1/machine_learning_as_a_service_mlaas_market_size/,vardhan1020,1547800322,,0,1,False,default,,,,,
969,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,18,ah8qli,self.MachineLearning,Why you should work remotely as a developer,https://www.reddit.com/r/MachineLearning/comments/ah8qli/why_you_should_work_remotely_as_a_developer/,conversational-ai,1547803233,[removed],0,1,False,self,,,,,
970,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,19,ah9afa,arxiv.org,[1901.05639] Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/ah9afa/190105639_artificial_neural_networks/,ihaphleas,1547808635,,8,1,False,default,,,,,
971,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,19,ah9atx,self.MachineLearning,Can someone explain this in normal English?,https://www.reddit.com/r/MachineLearning/comments/ah9atx/can_someone_explain_this_in_normal_english/,flubbergrubbery,1547808740," 

Why cant we use Mean Square Error (MSE) as a cost function for logistic regression?

In logistic regression, we use the sigmoid function and perform a non-linear transformation to obtain the probabilities. Squaring this non-linear transformation will lead to non-convexity with local minimums. Finding the global minimum in such cases using gradient descent is not possible. Due to this reason, MSE is not suitable for logistic regression. Cross-entropy or log loss is used as a cost function for logistic regression. In the cost function for logistic regression, the confident wrong predictions are penalised heavily. The confident right predictions are rewarded less. By optimising this cost function, convergence is achieved.",0,1,False,self,,,,,
972,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,19,ah9bdp,arxiv.org,[1901.05744] The Oracle of DLphi,https://www.reddit.com/r/MachineLearning/comments/ah9bdp/190105744_the_oracle_of_dlphi/,ihaphleas,1547808886,,18,1,False,default,,,,,
973,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,20,ah9i3b,eno8.com,"Not to be outdone, Intel announces AI breakthrough of its own at CES: Nervana",https://www.reddit.com/r/MachineLearning/comments/ah9i3b/not_to_be_outdone_intel_announces_ai_breakthrough/,elviracoker,1547810586,,0,1,False,default,,,,,
974,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,20,ah9lb1,i.redd.it,Machine Learning workshop/hackathon to learn re-inforcement Q-learning with tennis.,https://www.reddit.com/r/MachineLearning/comments/ah9lb1/machine_learning_workshophackathon_to_learn/,T-dog-machine,1547811405,,1,1,False,default,,,,,
975,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,21,ah9rzd,self.MachineLearning,Determine optimal hash function and possibly encoding,https://www.reddit.com/r/MachineLearning/comments/ah9rzd/determine_optimal_hash_function_and_possibly/,howdoidoit2,1547812988,[removed],0,1,False,self,,,,,
976,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,21,ah9t7q,self.MachineLearning,"Is there an open source ""Speaker Identification"" library?",https://www.reddit.com/r/MachineLearning/comments/ah9t7q/is_there_an_open_source_speaker_identification/,syngohan,1547813246,"I've been looking for a library that i could train to identify someone by their voice.  
I've been looking around for some time with no luck, Even if there's some tools that would help me create one i'd gladly take them.  
Note: I just need the library to identify 1 voice i'm not planning on using it for so many users/voices",0,1,False,self,,,,,
977,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,21,aha51m,self.MachineLearning,Predicting via single neuron/single filter activation,https://www.reddit.com/r/MachineLearning/comments/aha51m/predicting_via_single_neuronsingle_filter/,stefanl19,1547815854,[removed],0,1,False,self,,,,,
978,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,22,ahaga0,i.redd.it,"Microsoft, you got it",https://www.reddit.com/r/MachineLearning/comments/ahaga0/microsoft_you_got_it/,maykulkarni,1547818219,,0,1,False,default,,,,,
979,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,22,ahaoeb,youtube.com,Leela reacts beautifully to Stockfish's outrageous materialism in Nimzo ...,https://www.reddit.com/r/MachineLearning/comments/ahaoeb/leela_reacts_beautifully_to_stockfishs_outrageous/,kingscrusher-youtube,1547819824,,0,1,False,https://b.thumbs.redditmedia.com/broXTu0wumflGpMK10-34B1Api8TACXSo34y0ndvuJw.jpg,,,,,
980,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,23,ahatze,self.MachineLearning,Did anyone hear from MILA MSc/Ph.D. application Fall 2019?,https://www.reddit.com/r/MachineLearning/comments/ahatze/did_anyone_hear_from_mila_mscphd_application_fall/,shahrukhx01,1547820871,[removed],0,1,False,self,,,,,
981,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,23,ahazag,self.MachineLearning,[D] Deriving perfect hash function and possibly encoding as well,https://www.reddit.com/r/MachineLearning/comments/ahazag/d_deriving_perfect_hash_function_and_possibly/,howdoidoit2,1547821873,"HI all

&amp;#x200B;

I'm trying to create a toy poker hand evaluator. There are a little over 133 million possible 7 card combinations in a 52 card deck ( c(52,7) ) but there there are less than 4 thousand possible resulting hands given that the best 5 cards are taken, equal flushes are ties, etc. Further, there exists a fast solution (two-plus-two) that uses a giant table where each card indexes to the next card and the final card is the index to the answer. So this is effectively a kind of ""split up hash function"" if you will, I believe there must be a hash function that could be used without using this large multi-index table strategy.

&amp;#x200B;

So my question is, how would one use machine learning to go about solving this problem? I can uniquely encode all 133 million combinations and I can put them together with the answer they should produce. How can I get a machine learning system to tell me what hash function to use to get the smallest possible table, fastest hash function, etc. If possible, perhaps the system could also determine the best way to encode the cards. For example, one approach would be to simply give every card a unique bit so that there are 52 possible bits that can be set, never more than 7 will ever be set and this bit pattern represents the unique hand. But does the ordering matter? I might try rank first (i.e. 2 clubs, 2 diamons, ..., 4 clubs, 4 diamons, etc.) or suit first (i.e. 2 clubs, 3 clubs, 4 clubs, ..., Ace hearts, Ace spades) but perhaps there is a special arrangement that could be used that lets the answer table be smaller, the hashing function be faster or both.

&amp;#x200B;

I've been trying to google for how one does this but I don't know the right terminology for what I'm attempting to do so my searches, while finding loads of interesting things, haven't found the answer (or if I did find it, I didn't realize it).",13,1,False,self,,,,,
982,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,23,ahb17x,heartbeat.fritz.ai,"Image detection in the browser, using ONNX.js &amp; React.JS",https://www.reddit.com/r/MachineLearning/comments/ahb17x/image_detection_in_the_browser_using_onnxjs/,zsajjad,1547822220,,0,1,False,https://b.thumbs.redditmedia.com/Dz7GxASz0Zd4LKF6qjWIO3ibAx4A4EadTrKcaNYC8Wg.jpg,,,,,
983,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,23,ahb1l5,self.MachineLearning,[D] Trying to find a dataset,https://www.reddit.com/r/MachineLearning/comments/ahb1l5/d_trying_to_find_a_dataset/,rasmoo,1547822290,"I saw this dataset some months ago, but despite a lot of searching I can't seem to find it again.

&amp;#x200B;

It consists of several different environments or settings, and I think various actions(?) can be performed, but it wasn't quite a reinforcement learning setting. I don't remember how exactly this was framed, but I remember the interesting thing being that there were start and end states with corresponding instructions or essentially recipes on what has happened in between. The instructions might have been ""program snippets"" or natural language. There was a nice website with illustrated examples.

&amp;#x200B;

Does anyone have an idea about this?",2,1,False,self,,,,,
984,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,23,ahb1z3,infoq.com,Machines Can Learn - a Practical Take on Machine Intelligence Using Spring Cloud Data Flow and TensorFlow,https://www.reddit.com/r/MachineLearning/comments/ahb1z3/machines_can_learn_a_practical_take_on_machine/,NuEd_Fernandes,1547822361,,0,1,False,default,,,,,
985,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,23,ahb7e4,self.MachineLearning,Combining ML/AI with Blockchain/Cryptocurrencies,https://www.reddit.com/r/MachineLearning/comments/ahb7e4/combining_mlai_with_blockchaincryptocurrencies/,bonnyV2,1547823316,[removed],0,1,False,self,,,,,
986,MachineLearning,t5_2r3gv,2019-1-18,2019,1,18,23,ahb7z7,self.MachineLearning,Help a student (me) with his project!!,https://www.reddit.com/r/MachineLearning/comments/ahb7z7/help_a_student_me_with_his_project/,ScarlettPotato,1547823420,[removed],0,1,False,self,,,,,
987,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,0,ahbi8s,self.MachineLearning,[D] Does ICML have an abstract word limit?,https://www.reddit.com/r/MachineLearning/comments/ahbi8s/d_does_icml_have_an_abstract_word_limit/,anonDogeLover,1547825036,I can't find it online.,4,1,False,self,,,,,
988,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,0,ahbjcg,self.MachineLearning,Keras 2 Cpp,https://www.reddit.com/r/MachineLearning/comments/ahbjcg/keras_2_cpp/,gosha20777,1547825214,"Keras2cpp is a small library for running trained Keras models from a C++ application without any dependencies. 

Not long ago, I was faced with a production task  to launch the trained Kesas neural network model in native C ++ code. Oddly enough, the decision was not at all trivial. As a result, there appeared its own library, which gives such an opportunity. In this article I would like to share my experience and talk about how I created this library.

For those who can't wait - here is the github repository:

[https://github.com/gosha20777/keras2cpp](https://github.com/gosha20777/keras2cpp)

# Formulation of the problem.

In the process, I needed to run a trained model in a C++ application *(Unreal Engune 4)*. But bad luck: today there is almost no way to run the Keras model in C++. The option of calling Python from C++ did not seem good to me. Another option was to convert the Keras model to the TensorFlow model and then build the TensoFflow for C ++. This process is well described in [this article](http://bitbionic.com/2017/08/18/run-your-keras-models-in-c-tensorflow/). But with this also difficulties arise. First, TensorFlow builds vi Bezzel. But the bezel itself is capricious and the UE4 does not work with it. Secondly, TensoFflow itself is quite a big and cumbersome thing, but I wanted something lighter and more productive. I can only say that on github I found a semi-working project with the functionality I need. But, it does not support current versions of Python and Keras. The attempts to remake it were unsuccessful: The C ++ application failed with the `Core Dump` error. *So I was decided to write their own implementation...*

# Writing the library!

Turning on the rock, drinking a bottle of beer, I started writing code. In many ways, TensorFlow code, some knowledge of algorithms and data structures (thanks to IFMO University for its courses) and good music helped me in the implementation of this library. Anyway, the library was written overnight. 

# And so meet: Keras2cpp!

*The first part of the library* is the **Python module** for saving the trained model in its own binary format.

There is nothing difficult. We just read the Keras model and write bitwise to the file: first the *layer* type, then the *dimension*, then the *weights matrix* in the float format.  


Let's now turn to the most delicious - **C ++ module**.  
There are two entities are available to the user: `tensor` and `model .`

**Tensor** \- *transfers the data with which the neural network works and is a computer implementation of the tensor*.  Currently, the maximum dimension in 4 dimensions is supported. The dimension of each dimension is stored in the `std::vector&lt;int&gt; dims_;` and the weight of each tensor element is `in std::vector&lt;int&gt; data_ ;`. From the available methods you can select `void Print()` and `Tensor Select(int row)`. The rest of the operations you can see in the source code. After the math for tensors was written, I started to implement the models.

**Model** \- *is a set of layers in each of which operations on tensors and weights matrix are registered.* Two functions are available for the user: `virtual bool LoadModel(const std::string &amp;filename);` and `virtual bool Apply(Tensor *in, Tensor *out);`.

# P.S. 

I liked writing this library. When you write everything yourself from scratch - you understand more, and how it works. 

I hope that this post was useful and maybe some of you will take advantage of my development. The source code is open and available here:

[https://github.com/gosha20777/keras2cpp](https://github.com/gosha20777/keras2cpp) ",0,1,False,self,,,,,
989,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,0,ahbp6p,self.MachineLearning,Can we use cycle GAN to convert old animated lion king movie to the latest one that is about to be released?,https://www.reddit.com/r/MachineLearning/comments/ahbp6p/can_we_use_cycle_gan_to_convert_old_animated_lion/,charan_1996,1547826188,[removed],0,1,False,self,,,,,
990,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,0,ahbt36,johnhw.github.io,[P] Amazing vector embedding and UMAP visualization of the cosine similarity of prime numbers,https://www.reddit.com/r/MachineLearning/comments/ahbt36/p_amazing_vector_embedding_and_umap_visualization/,TwoUpper,1547826820,,0,1,False,default,,,,,
991,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,1,ahbvdn,self.MachineLearning,What is the relationship between the sum of squares of all weights and lambda in the ridge regression,https://www.reddit.com/r/MachineLearning/comments/ahbvdn/what_is_the_relationship_between_the_sum_of/,NoviceOfNovice,1547827213,[removed],1,1,False,https://b.thumbs.redditmedia.com/hzS9dWqlw5-6ywcAq-fyhbwVkVpLLTbuhouzP1kBuFM.jpg,,,,,
992,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,1,ahc4xi,self.test20191,Project Julen,https://www.reddit.com/r/MachineLearning/comments/ahc4xi/project_julen/,test20191,1547828735,,0,1,False,default,,,,,
993,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,1,ahc8tv,arxiv.org,"[R] [1811.11684] Shared Representational Geometry Across Neural Networks &lt;- ""we show that different neural networks encode the same input examples as different orthogonal transformations of an underlying shared representation""",https://www.reddit.com/r/MachineLearning/comments/ahc8tv/r_181111684_shared_representational_geometry/,TwoUpper,1547829367,,31,1,False,default,,,,,
994,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,1,ahcdep,self.MachineLearning,Explaining Deep NLP Models,https://www.reddit.com/r/MachineLearning/comments/ahcdep/explaining_deep_nlp_models/,jjviana,1547830096,"[Explaining Deep NLP Models](https://jjviana.github.io/Explaining-Deep-NLP-Models/)

Questions, Comments, and Suggestions welcome.",0,1,False,self,,,,,
995,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,1,ahcg1g,youtube.com,"Ali Rahimi - ""Let's take machine learning from alchemy to electricity."", Test-of-Time Award [NIPS 2017]",https://www.reddit.com/r/MachineLearning/comments/ahcg1g/ali_rahimi_lets_take_machine_learning_from/,EveryDay-NormalGuy,1547830527,,0,1,False,default,,,,,
996,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,1,ahchov,self.MachineLearning,[R] Explaining Deep NLP Models,https://www.reddit.com/r/MachineLearning/comments/ahchov/r_explaining_deep_nlp_models/,jjviana,1547830781,"Documenting my ongoing research trying to obtain better explanations from Deep NLP models:

[Explaining Deep NLP Models](https://jjviana.github.io/Explaining-Deep-NLP-Models/)

&amp;#x200B;

Questions, Comments and Suggestions welcome.",1,1,False,self,,,,,
997,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,2,ahcmsl,self.MachineLearning,[D] Second Post Bayesian Neural Networks: Background Knowledge,https://www.reddit.com/r/MachineLearning/comments/ahcmsl/d_second_post_bayesian_neural_networks_background/,jupyterAI,1547831572,"This is the **second post in the eight post series** about Bayesian Neural Networks: [https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43](https://medium.com/neuralspace/bayesian-neural-network-series-post-2-background-knowledge-fdec6ac62d43)

This post discussed **Bayesian approaches** used in Deep Learning like **Variational Inference** and **Bayes by Backprop**. It also forms the base for the upcoming post that discussed the approach in details. **Implementation** is available here: [https://github.com/kumar-shridhar/PyTorch-BayesianCNN](https://github.com/kumar-shridhar/PyTorch-BayesianCNN)

&amp;#x200B;

If you miss the first post, this is the link: [https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2](https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2)

&amp;#x200B;

Let me know if something I missed. I will cover them in the next post. ",3,1,False,self,,,,,
998,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,2,ahcq1p,self.MachineLearning,What are must-read papers about uncertainty estimation in deep learning?,https://www.reddit.com/r/MachineLearning/comments/ahcq1p/what_are_mustread_papers_about_uncertainty/,d3fenestrator,1547832094,[removed],0,1,False,self,,,,,
999,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,2,ahcrbk,self.MachineLearning,Can we modify the abstract and title after abstract submission deadline but before the main paper deadline?,https://www.reddit.com/r/MachineLearning/comments/ahcrbk/can_we_modify_the_abstract_and_title_after/,tremblerz6,1547832292,[removed],0,1,False,self,,,,,
1000,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,2,ahcxs0,ai.googleblog.com,Soft Actor-Critic: Deep Reinforcement Learning for Robotics,https://www.reddit.com/r/MachineLearning/comments/ahcxs0/soft_actorcritic_deep_reinforcement_learning_for/,sjoerdapp,1547833329,,0,1,False,https://b.thumbs.redditmedia.com/7DGeUdJ8cDtokcoZa1ZYblDm3zQTICDZtLDl15GVwkw.jpg,,,,,
1001,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,2,ahd20d,venturebeat.com,Machine learning is rescuing old game textures in Zelda and Final Fantasy,https://www.reddit.com/r/MachineLearning/comments/ahd20d/machine_learning_is_rescuing_old_game_textures_in/,ChrisTweten,1547834010,,0,1,False,https://a.thumbs.redditmedia.com/wzbOkeejaUVgtREzAp8RASZbuuK8-TpBjgc3oZICN20.jpg,,,,,
1002,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,2,ahd2yd,burckhardt.com,best perforating equipment service,https://www.reddit.com/r/MachineLearning/comments/ahd2yd/best_perforating_equipment_service/,habmkloganjt,1547834179,,0,1,False,default,,,,,
1003,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,3,ahd5hw,i.redd.it,[BERT] What are the segment and position embeddings used in here?,https://www.reddit.com/r/MachineLearning/comments/ahd5hw/bert_what_are_the_segment_and_position_embeddings/,JClub,1547834601,,0,1,False,https://b.thumbs.redditmedia.com/cMsTCZZEYKkUExYZYln7QGbLIZQqBklbxxm81hIwVgQ.jpg,,,,,
1004,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,3,ahdkdr,i.redd.it,Engenharia de Segurana do Trabalho VI,https://www.reddit.com/r/MachineLearning/comments/ahdkdr/engenharia_de_segurana_do_trabalho_vi/,JamurGerloff,1547836990,,0,1,False,https://b.thumbs.redditmedia.com/BDOjlfh7Az6Pbr36wbV78FXuhZo7RuIaLg8tDRDgIBs.jpg,,,,,
1005,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,4,ahe6mi,youtube.com,[R] Understanding and Applying Self-Attention for NLP,https://www.reddit.com/r/MachineLearning/comments/ahe6mi/r_understanding_and_applying_selfattention_for_nlp/,DemiourgosD,1547840587,,0,1,False,https://b.thumbs.redditmedia.com/bpC0V78resjfNps21TpQ1Zmzh8BtWB1Sn9cZgpXVXnE.jpg,,,,,
1006,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,5,ahekjv,self.MachineLearning,Demand Estimation without data,https://www.reddit.com/r/MachineLearning/comments/ahekjv/demand_estimation_without_data/,aljohn09,1547842888,[removed],0,1,False,self,,,,,
1007,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,5,aheojy,self.MachineLearning,Clothes recognition,https://www.reddit.com/r/MachineLearning/comments/aheojy/clothes_recognition/,ShamanDev,1547843572,Hello! Is there an clothing recognition API that also recognize clothes colours?,0,1,False,self,,,,,
1008,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,5,ahevg9,medium.com,Automatic Creation of Machine Learning Algorithms for Autonomous Vehicles,https://www.reddit.com/r/MachineLearning/comments/ahevg9/automatic_creation_of_machine_learning_algorithms/,Yuqing7,1547844736,,0,1,False,default,,,,,
1009,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,6,ahf0dp,self.MachineLearning,Word2Vec but with a discriminator to force the embeddings to follow a normal distribution?,https://www.reddit.com/r/MachineLearning/comments/ahf0dp/word2vec_but_with_a_discriminator_to_force_the/,UnfazedButDazed,1547845542,"I've been looking into Adversarial Auto-Encoders in which a discriminator can be used to force a hidden layer to follow a specific distribution. 

Has there been any work with doing that for text? Like taking a discriminator connecting it to a word2vec embedding and forcing the word2vec network to produce embeddings that follow a normal distribution. Is that possible? Would that help at all? Has anyone done that before? I'm not sure where I can find the relevant papers.",0,1,False,self,,,,,
1010,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,6,ahf0qx,self.MachineLearning,[D] automatically find pixel-level differences in classes,https://www.reddit.com/r/MachineLearning/comments/ahf0qx/d_automatically_find_pixellevel_differences_in/,waspinator,1547845602,"If I have an image dataset of portrait images with two classes, ""blonds"" and ""brunettes"". The samples are labeled, but only on the whole image level (ex: this is a image of a brunette), so no pixel-level annotations. Can I generate pixel-level annotations from a trained classifier which can discriminate between the two classes?",3,1,False,self,,,,,
1011,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,6,ahf2yf,self.MachineLearning,[P] What Are You Reading (WAYR) - Voting system,https://www.reddit.com/r/MachineLearning/comments/ahf2yf/p_what_are_you_reading_wayr_voting_system/,ranihorev,1547845963,"Hey, 

I really like the WAYR post and its discussions. However, it's not always up-to-date and sometimes there are duplicates. I'd like to suggest an upgrade - I built a small website that makes the voting easier and shows the top papers of the week. You can search for any paper from arXiv and vote.  

The next step is to auto-generate a post on the sub with comments section for each paper so we can also discuss the papers. 

[https://arxiv.lyrn.ai/wayr](https://arxiv.lyrn.ai/wayr)

What do you think? 

&amp;#x200B;

p.s. The code is open-source of course and I was hoping that people will add more features :)",6,1,False,self,,,,,
1012,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,9,ahgsp7,blogs.unity3d.com,Fostering AI Research: Meet Unity at AAAI-19,https://www.reddit.com/r/MachineLearning/comments/ahgsp7/fostering_ai_research_meet_unity_at_aaai19/,leonchenzhy,1547856956,,0,1,False,default,,,,,
1013,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,9,ahgz9q,self.MachineLearning,[D] Question about PhD Interviews,https://www.reddit.com/r/MachineLearning/comments/ahgz9q/d_question_about_phd_interviews/,badjezus,1547858222,"So I am in the middle of the application cycle for getting a PhD in ML. For those of you that have been through the process before, I am curious what types of interview questions I can expect from potential PIs. Is there a lot of technical questions like ones that you may find in a job interview? Or are the questions more related to your past research (why you did this, what did your results tell you... etc) 

Essentially, I am worried that they ask me a technical ML related question (example: explain *insert some ML algorithm here*) that is not something I have covered in my past studies and thus look stupid when I am unable to answer it.

Thank you.",16,1,False,self,,,,,
1014,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,9,ahh3ss,self.MachineLearning,[P] Parallelized Go Explore,https://www.reddit.com/r/MachineLearning/comments/ahh3ss/p_parallelized_go_explore/,618smartguy,1547859107,"Here is some code to reproduce the results I shared on speed running Sonic the Hedgehog using Go Explore. This is my first public code, so I tried to keep it concise to be as usable as possible. Any suggestions are appreciated!

[https://github.com/R-McHenry/SynchronousGoExplore](https://github.com/R-McHenry/SynchronousGoExplore)  
",2,1,False,self,,,,,
1015,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,10,ahh8j8,youtube.com,"Robot trained via simulation and reinforcement learning is capable of running, getting up and recovering from kicks",https://www.reddit.com/r/MachineLearning/comments/ahh8j8/robot_trained_via_simulation_and_reinforcement/,question99,1547860013,,1,1,False,https://a.thumbs.redditmedia.com/hGkdE_ttzeqVBXQanlJ9UIwJBNmix6hSN3xWO7or930.jpg,,,,,
1016,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,12,ahidyy,self.MachineLearning,[D] Time series loss function,https://www.reddit.com/r/MachineLearning/comments/ahidyy/d_time_series_loss_function/,georgeo,1547868596,"I have a time series that mostly consists of values with small means and variances punctuated by occasional large spikes which seem to be more likely after longer, flatter time periods. At any given time, the next value is far more likely to be more flatness than a spike. What's a good loss function to predict the increasing probability of a spike? ",4,1,False,self,,,,,
1017,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,12,ahiiwf,self.MachineLearning,good machine learning summer programs/schools for high school junior?,https://www.reddit.com/r/MachineLearning/comments/ahiiwf/good_machine_learning_summer_programsschools_for/,dfam02,1547869653,[removed],0,1,False,self,,,,,
1018,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,14,ahj2ff,self.MachineLearning,[P] Downloading all publicly available #10YearChallenge images as weekend project,https://www.reddit.com/r/MachineLearning/comments/ahj2ff/p_downloading_all_publicly_available/,manklu,1547874153,What embeddings / annotations would you guys like me to do on images so that people can build interesting things using that data like build GANs to generate images of how people will look when 10 years older?,14,1,False,self,,,,,
1019,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,14,ahjcbr,self.MachineLearning,Machine learning : Newbie vs Pro (skit),https://www.reddit.com/r/MachineLearning/comments/ahjcbr/machine_learning_newbie_vs_pro_skit/,syed_imam,1547876581,[removed],0,1,False,self,,,,,
1020,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,14,ahjftu,self.MachineLearning,[N] Amazon put The Hundred-Page Machine Learning Book on sale at $33.23,https://www.reddit.com/r/MachineLearning/comments/ahjftu/n_amazon_put_the_hundredpage_machine_learning/,RudyWurlitzer,1547877444,"Amazon made a decision to cut down its own part of the price by $11.77 (26% discount). Now the book [costs $33.23](https://www.amazon.com/Hundred-Page-Machine-Learning-Book/dp/199957950X) which puts it on par with most bestselling technical books (which are also sold by Amazon at a huge discount).

The book is now the #1 bestseller in the AI &amp; ML / Machine Theory category.

I don't know how long the discount will last. Amazon made this decision on its own.",14,1,False,self,,,,,
1021,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,15,ahjlc9,i.redd.it,Trying to understand and implement NN from a paper in keras. Need help.,https://www.reddit.com/r/MachineLearning/comments/ahjlc9/trying_to_understand_and_implement_nn_from_a/,bigtimefoodie,1547878810,,1,1,False,default,,,,,
1022,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,15,ahjrbv,livewireindia.com,Machine Learning Classroom Training | Certification Course,https://www.reddit.com/r/MachineLearning/comments/ahjrbv/machine_learning_classroom_training_certification/,sreethunithin,1547880427,,0,1,False,default,,,,,
1023,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,16,ahk1yw,self.MachineLearning,Make the Web more human with ML: Announcing Harmony,https://www.reddit.com/r/MachineLearning/comments/ahk1yw/make_the_web_more_human_with_ml_announcing_harmony/,louison1,1547883383,[removed],0,1,False,self,,,,,
1024,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,18,ahksoj,self.MachineLearning,[D] Is deep learning is nearest neighbor in disguise?,https://www.reddit.com/r/MachineLearning/comments/ahksoj/d_is_deep_learning_is_nearest_neighbor_in_disguise/,invertedpassion,1547891425,"Hello,

I've been thinking about how models and data are interchangeable. Especially with deep learning where the data needed is immense, why shouldn't we simply use a much simpler nearest neighbor technique? Neural networks approximate values on unseen data by learning the function via training data. If there's an abundance of data, wouldn't a local approximation of unseen data's value by historical examples be equally effective? I understand that the measure of 'distance' between data points would be complicated to define, but even neural networks have hyperparameters that can make or break the success of training.

A quick search showed that on MNIST people have achieved an accuracy of [97%](https://towardsdatascience.com/building-improving-a-k-nearest-neighbors-algorithm-in-python-3b6b5320d2f8) using just k-means classification. I'm not sure if there are nearest neighbor benchmarks for more complicated datasets like ImageNet or more complicated problems like playing Atari.

In reinforcement learning, training an agent takes days, why can't we store all experiences in a database and play time, come up similar experiences and average them to predict next state or reward?

Has anyone else thought about it? 

https://i.redd.it/baphnn5hpcb21.jpg

&amp;#x200B;",94,1,False,https://a.thumbs.redditmedia.com/8K2BYOZr60toPxMaYOE2ZimGlemWb2IgFlRZvy8M_c0.jpg,,,,,
1025,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,20,ahla7a,self.MachineLearning,"What do you think about the robot ""sophia""",https://www.reddit.com/r/MachineLearning/comments/ahla7a/what_do_you_think_about_the_robot_sophia/,orenog,1547896791,[removed],0,1,False,self,,,,,
1026,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,20,ahlf7e,self.MachineLearning,How does backpropagation work through a convolutional layer with multiple filters?,https://www.reddit.com/r/MachineLearning/comments/ahlf7e/how_does_backpropagation_work_through_a/,Avitan,1547898244,[removed],0,1,False,self,,,,,
1027,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,21,ahllkc,self.MachineLearning,[N] Stable-Baselines 2.4.0 released: Soft Actor-Critic (SAC) and easy policy customization,https://www.reddit.com/r/MachineLearning/comments/ahllkc/n_stablebaselines_240_released_soft_actorcritic/,araffin2,1547900042,"Full changelog: [https://github.com/hill-a/stable-baselines/releases](https://github.com/hill-a/stable-baselines/releases)

Highlights: 

\- New RL baseline: Soft Actor-Crtic (haarnoja et al.) with automatic entropy temperature optimization

\- Customize policy at model creation

\- RL Baselines Zoo updated (now 80+ trained agents) 

\- Flexible LSTM policies

&amp;#x200B;

Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)

RL Baselines Zoo: [https://github.com/araffin/rl-baselines-zoo](https://github.com/araffin/rl-baselines-zoo)",1,1,False,self,,,,,
1028,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,21,ahloc0,self.MachineLearning,22 Great Blogs Posted in the last 12 Months,https://www.reddit.com/r/MachineLearning/comments/ahloc0/22_great_blogs_posted_in_the_last_12_months/,andrea_manero,1547900816,[removed],0,1,False,self,,,,,
1029,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,21,ahlt5h,github.com,"I've made a pytorch/pythonic implementation of the REINFORCE algorithm, what do you think?",https://www.reddit.com/r/MachineLearning/comments/ahlt5h/ive_made_a_pytorchpythonic_implementation_of_the/,poppear,1547902100,,0,1,False,default,,,,,
1030,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,22,ahm07z,youtube.com,"[R] Real robot trained via simulation and reinforcement learning is capable of running, getting up and recovering from kicks",https://www.reddit.com/r/MachineLearning/comments/ahm07z/r_real_robot_trained_via_simulation_and/,question99,1547903960,,1,1,False,default,,,,,
1031,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,22,ahm4cp,self.MachineLearning,Variational Autoencoder confusion,https://www.reddit.com/r/MachineLearning/comments/ahm4cp/variational_autoencoder_confusion/,TifferWu,1547904966,"I want to use vae to calulate document similarity by compute cosine similarity of the latent vector of each document encoded by encoder.

In the VAE is each data  **x**  is encoded to **mu** and **sigma** ,and the the hidden vector **z,** and **my confusion** is that :

1. after trainning the model,i want to get the latent vector of new document,should i use the **mu** vector or the  **z** vector?I see both case use in different code implement by different people
2. Can i use the **kld distance** of latent vector to calculate the document similarity?",0,1,False,self,,,,,
1032,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,22,ahm5u3,self.MachineLearning,"[R] Real robot trained via simulation and reinforcement learning is capable of running, getting up and recovering from kicks",https://www.reddit.com/r/MachineLearning/comments/ahm5u3/r_real_robot_trained_via_simulation_and/,question99,1547905323,"Paper: http://robotics.sciencemag.org/content/4/26/eaau5872

PDF: http://robotics.sciencemag.org/content/4/26/eaau5872.full.pdf

This looks interesting. To my layman eyes this looks similar to what we have seen from Boston Dynamics in recent years but as far as I understand BD did not use deep reinforcement learning. This project does. I'm curious whether this means that they will be able to push the capabilities of these systems further.",54,1,False,self,,,,,
1033,MachineLearning,t5_2r3gv,2019-1-19,2019,1,19,23,ahmd42,self.MachineLearning,Suggest me an article for learning Kalman filter for beginners with having no prior knowledge in statistics,https://www.reddit.com/r/MachineLearning/comments/ahmd42/suggest_me_an_article_for_learning_kalman_filter/,SushiMind,1547906986,[removed],0,1,False,self,,,,,
1034,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,0,ahn0cx,self.MachineLearning,How should I standardize input when fine-tuning a VGG16 network?,https://www.reddit.com/r/MachineLearning/comments/ahn0cx/how_should_i_standardize_input_when_finetuning_a/,lolwhatsuicide,1547911975,[removed],0,1,False,self,,,,,
1035,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,0,ahn60z,self.MachineLearning,"[D] Brains, Minds, and Machines (Tomaso Poggio) - MIT AI Podcast",https://www.reddit.com/r/MachineLearning/comments/ahn60z/d_brains_minds_and_machines_tomaso_poggio_mit_ai/,UltraMarathonMan,1547913088,"Tomaso Poggio is a professor at MIT and is the director of the Center for Brains, Minds, and Machines. Cited over 100,000 times, his work has had a profound impact on our understanding of the nature of intelligence, in both biological neural networks and artificial ones. He has been an advisor to many highly-impactful researchers and entrepreneurs in AI, including Demis Hassabis of DeepMind, Amnon Shashua of MobileEye, and Christof Koch of the Allen Institute for Brain Science. We cover quite a few topics in machine learning but also zoom out on big picture topics of intelligence, consciousness, and ethics.

YouTube video version: [https://www.youtube.com/watch?v=aSyZvBrPAyk](https://www.youtube.com/watch?v=aSyZvBrPAyk)

Podcast audio version: [https://lexfridman.com/tomaso-poggio/](https://lexfridman.com/tomaso-poggio/)

![img](4ygxwv2eieb21 ""Brains, Minds, and Machines (Tomaso Poggio)"")

This conversation is part of the Artificial Intelligence podcast and the MIT course 6.S099: Artificial General Intelligence. The conversation and lectures are free and open to everyone.

&amp;#x200B;",2,1,False,self,,,,,
1036,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,1,ahnpic,arxiv.org,[R] Attentive Neural Processes | DeepMind,https://www.reddit.com/r/MachineLearning/comments/ahnpic/r_attentive_neural_processes_deepmind/,columbus8myhw,1547916614,,2,1,False,default,,,,,
1037,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,1,ahnr0i,self.MachineLearning,"How would you approach something similar to a qualitative ""thematic analysis"" with semantic analysis?",https://www.reddit.com/r/MachineLearning/comments/ahnr0i/how_would_you_approach_something_similar_to_a/,xanduba,1547916870,[removed],0,1,False,self,,,,,
1038,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,2,aho60n,pgaleone.eu,Tensorflow 2.0: Keras is not (yet) a simplified interface to Tensorflow,https://www.reddit.com/r/MachineLearning/comments/aho60n/tensorflow_20_keras_is_not_yet_a_simplified/,pgaleone,1547919438,,0,1,False,default,,,,,
1039,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,2,ahoc5r,cbc.ca,Identical twins get different DNA ancestry results. The companies can't explain the discrepancies. Is this because their machine learning algorithms are effectively inscrutable?,https://www.reddit.com/r/MachineLearning/comments/ahoc5r/identical_twins_get_different_dna_ancestry/,ragold,1547920483,,0,1,False,https://b.thumbs.redditmedia.com/kbJtBgwgpCEVHqXPvUJNDKzWx2euoyaLX8jUuSFTMQE.jpg,,,,,
1040,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,3,ahofbj,selliber.com,Buy or sell industrial machinery worldwide,https://www.reddit.com/r/MachineLearning/comments/ahofbj/buy_or_sell_industrial_machinery_worldwide/,Anonyx0,1547921007,,0,1,False,default,,,,,
1041,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,3,ahofkk,arxiv.org,[R] The Benefits of Over-parameterization at Initialization in Deep ReLU Networks,https://www.reddit.com/r/MachineLearning/comments/ahofkk/r_the_benefits_of_overparameterization_at/,downtownslim,1547921046,,1,1,False,default,,,,,
1042,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,3,ahork4,self.MachineLearning,[R] Graph Wavelet Neural Network (ICLR 2019) -- Pytorch implementation,https://www.reddit.com/r/MachineLearning/comments/ahork4/r_graph_wavelet_neural_network_iclr_2019_pytorch/,benitorosenberg,1547923051,"&amp;#x200B;

https://i.redd.it/9j8q8kd9cfb21.jpg

Paper: [https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork/blob/master/paper.pdf](https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork/blob/master/paper.pdf)

PyTorch: [https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork](https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork)

Abstract:

We present graph wavelet neural network (GWNN), a novel graph  convolutional neural network (CNN), leveraging graph wavelet transform  to address the shortcomings of previous spectral graph CNN methods that  depend on graph Fourier transform. Different from graph Fourier  transform, graph wavelet transform can be obtained via a fast algorithm  without requiring matrix eigendecomposition with high computational  cost. Moreover, graph wavelets are sparse and localized in vertex  domain, offering high efficiency and good interpretability for graph  convolution. The proposed GWNN significantly outperforms previous  spectral graph CNNs in the task of graph-based semi-supervised  classification on three benchmark datasets: Cora, Citeseer and Pubmed.",2,1,False,https://b.thumbs.redditmedia.com/Ejowljci9bPDUbw65rd-IYxCMVT9Vuea3ouuEO5wbug.jpg,,,,,
1043,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,3,ahosv2,self.MachineLearning,Machine Learning Slack for all,https://www.reddit.com/r/MachineLearning/comments/ahosv2/machine_learning_slack_for_all/,Atfdynamo5,1547923261,"About two weeks ago I posted on Reddit LearningMachineLearning mentioning that I will be starting a free Slack community course for Beginners who wish to learn ML. My only condition was that they had to be serious. Well, shortly after the post I received many requests and people start joining in. Students ranged anywhere from High School / College students to a statistician to developers. Some were serious with the course and others weren't. Day by day I would track each student's progress by reviewing their quizzes, assignments, comments on the videos and material posted on the classroom website, and other general activities that would prove their commitment to the course.

After noticing that certain students were slacking off, I started a filtering process (after sending many notices). When the remaining students saw that slackers were being removed from the course, their level of participation and effort increased many many times more! Seeing these students trying their best to complete assignments and understand the details was so inspiring for me as an instructor.

I was inspired to start this short beginners ML course after many requests from my friends and family who were going through the hard process of learning Data Science. I understand the feeling of being lost at the beginning and a feeling of being left behind in a fast evolving field with high demand. When I first stated to learn D.S., I also had to go through so many tutorials, MOOCs, books, etc. which were so many and at different difficulty levels that it could easily make a beginner feel despondent. I wrote about my journey a while ago in one of my posts: [https://medium.com/@atfdynamo5/data-science-the-old-school-way-18e1b85e45f1](https://medium.com/@atfdynamo5/data-science-the-old-school-way-18e1b85e45f1)

**Anyway, in short, after filtering through the thousands of tutorials, blogs, videos, MOOCs, bootcamps, etc ... I put together a nice course that collects the cream of it all.**

We have three main classes (using Google Classroom): **ML Theory, Python, Machine Learning in Practice**.

**In ML Theory:** we go through the core concepts of ML. Many courses try to breeze through this stage out of fear that student's will be driven away with all the math. For many...it's all about retaining students ($$$), not building them. This mistake always leaves learners in the dark later on.

We go through the theories in detail (obviously not PhD level). We first start by going through ""Introduction to Statistical Learning in R"" (ISLR) book which I found to be an easy yet integral part of any student's journey in DS. It is a great way to get introduced to what really goes on behind all those models. Rather than, ""Now type this code to get the predictions"" we understand why we do that. After going though only a couple of pages in ISLR, I found that students were asking many interesting questions in ML that otherwise would not be brought up if we hadn't gone through the theoretical parts. This class is my favorite part of the course!

**In the Programming Class:** we learn Python from [SolorLearn.com](https://solorlearn.com/) and ML tools from the best posts and sites.

**In the Machine Learning Class:** we practice what we learn in ML Theory and we (among other things) will be going through some of Andrew NG's course on Machine Learning applying it all in Python, kewl!

Since we are still at the beginning, I am willing to add a few more students as long as you are serious and wanting to learn ML the right way. If you are then please email me at [atfdynamo5@gmail.com](mailto:atfdynamo5@gmail.com) and tell me why you want to learn ML and how you think you will help mankind with this great field.

Enjoy.",0,1,False,self,,,,,
1044,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,4,ahp4iv,self.MachineLearning,[P] How to make sure training and test sets contain inputs from final validation set?,https://www.reddit.com/r/MachineLearning/comments/ahp4iv/p_how_to_make_sure_training_and_test_sets_contain/,hxdscp,1547925155,"I am taking a data science course on edX and have to create a movie recommendation system using the 10 million MovieLens data. The staff have already provided R code to download the data and divide it into an edx set and validation set. Their code was written so the validation set contains every userId and movieId in the edx set (code below). The validation set will be used as a final test of my algorithm.

    # Validation set will be 10% of MovieLens data
    set.seed(1)
test_index &lt;- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx &lt;- movielens[-test_index,]
temp &lt;- movielens[test_index,]
    
    # Make sure userId and movieId in validation set are also in edx set
    validation &lt;- temp %&gt;% 
      semi_join(edx, by = ""movieId"") %&gt;%
      semi_join(edx, by = ""userId"")
    
    # Add rows removed from validation set back into edx set
    removed &lt;- anti_join(temp, validation)
edx &lt;- rbind(edx, removed)

Now, I have to split up the edx set into training and test sets. Im somewhat familiar with basic methods to sample or split data, but Im not sure how to split up the edx set so the userId and movieId in my training and test sets are also all in the validation set? Or is that not necessary?

Thank you for your help!",3,1,False,self,,,,,
1045,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,4,ahpeh6,self.MachineLearning,Which undergraduate math course would be most helpful in pursuing Artificial Intelligence/Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/ahpeh6/which_undergraduate_math_course_would_be_most/,ProgrammingAddict,1547926816,[removed],0,1,False,self,,,,,
1046,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,4,ahple2,dev.to,Easy machine learning with Ruby using SVMKit,https://www.reddit.com/r/MachineLearning/comments/ahple2/easy_machine_learning_with_ruby_using_svmkit/,small_arbox,1547927962,,0,1,False,default,,,,,
1047,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,5,ahppb9,github.com,GitHub - yoshoku/SVMKit: SVMKit is a Scikit-learn like framework for ML in Ruby,https://www.reddit.com/r/MachineLearning/comments/ahppb9/github_yoshokusvmkit_svmkit_is_a_scikitlearn_like/,small_arbox,1547928577,,0,1,False,default,,,,,
1048,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,5,ahpu7g,self.MachineLearning,best way to learn machine learning?,https://www.reddit.com/r/MachineLearning/comments/ahpu7g/best_way_to_learn_machine_learning/,TheVIIAnthony,1547929389,[removed],0,1,False,self,,,,,
1049,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,5,ahq5xl,self.MachineLearning,Help a student out with a machine learning/dl project!,https://www.reddit.com/r/MachineLearning/comments/ahq5xl/help_a_student_out_with_a_machine_learningdl/,pavanagrawal123,1547931368,[removed],0,1,False,self,,,,,
1050,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,5,ahq78s,self.MachineLearning,"[D] Paper finding help, does anyone know of a paper where they tested an architecture focused on multi-task learning, where for each task they turned off a % of the weights? Or, if there are any key words/phrases that you suggest querying?",https://www.reddit.com/r/MachineLearning/comments/ahq78s/d_paper_finding_help_does_anyone_know_of_a_paper/,BatmantoshReturns,1547931590,"I am interested in reading a paper on that topic. If a person learns a particular skill, say painting, only a fraction of that person's total neurons are being trained, probably those specialized on motor functions or something like that. And then there are neurons that are not specialized on motor functions, but would still be trained, such as visual analysis neurons (I know I'm probably butchering the whole field of neurology, please bare with this analogy). 

I am wondering of anyone ever tried with multi-task training. Having some weights on during a task, say that there are 10 tasks. So some weights would only be on during 2 particular tasks out of the 10 tasks, and then another set of weights would be on during 4 specific tasks out of the 10 total tasks, and so on. 

Even if you don't know anything like that, it would also help if you could come up with terms and phrases that may lead to it, since machine learning terminology is sort of it's own art in itself. ",4,1,False,self,,,,,
1051,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,6,ahq9id,self.MachineLearning,[D] Help out a student with a machine learning project!,https://www.reddit.com/r/MachineLearning/comments/ahq9id/d_help_out_a_student_with_a_machine_learning/,pavanagrawal123,1547931965,"Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning),

My name is Pavan and I'm a student. I'm currently working on a machine learning project, and I would love your help in my research. If you could take 3-4 minutes of your time today to fill out my survey, I would highly appreciate it. If you have any trouble filling it in, please let me know, I will be happy to help you out.

[https://goo.gl/forms/B2jZGq1mYYSggVx72](https://goo.gl/forms/B2jZGq1mYYSggVx72)

The survey covers which tools you are using to write/productionize machine learning algos. If this is not allowed on this sub, please feel free to remove this post!

Thanks!",4,1,False,self,,,,,
1052,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,6,ahqiao,self.MachineLearning,OpenReview helps my imposter syndrome,https://www.reddit.com/r/MachineLearning/comments/ahqiao/openreview_helps_my_imposter_syndrome/,TheOverGrad,1547933458,[removed],0,1,False,self,,,,,
1053,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,7,ahqugj,self.MachineLearning,How to Get Started Learning AI and Machine Learning - Webinar Series,https://www.reddit.com/r/MachineLearning/comments/ahqugj/how_to_get_started_learning_ai_and_machine/,jaygshah22,1547935503,[removed],0,1,False,self,,,,,
1054,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,8,ahrjg2,medium.com,How Artificial General Intelligence might be created - Humanitys quest to create machines that can think like us may be realized within our lifetimes thanks to cryptocurrency,https://www.reddit.com/r/MachineLearning/comments/ahrjg2/how_artificial_general_intelligence_might_be/,shamoons,1547939761,,0,1,False,default,,,,,
1055,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,9,ahs8s4,self.MachineLearning,[D] Shapley value calculations and usage?,https://www.reddit.com/r/MachineLearning/comments/ahs8s4/d_shapley_value_calculations_and_usage/,Fender6969,1547944103,"Ran across this package in Python a while ago and it seems truly great. From my understanding, this is used to show the impact each predictor used in the model has towards the outcome of the model arrives to. I would love to use this to explain the impact of certain predictors to my customer. If my understanding of this is incorrect, please let me know!

&amp;#x200B;

&amp;#x200B;",10,1,False,self,,,,,
1056,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,9,ahshsy,deeplearningio.com,[N] Neural Networks and Deep Learning  VI : Back- Forward Propagation | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/ahshsy/n_neural_networks_and_deep_learning_vi_back/,mrcgllr,1547945802,,0,1,False,https://a.thumbs.redditmedia.com/QDdxH9h9E7IKOrXSIOJ-xE4J5hBYoqHA5IfjWIkOey4.jpg,,,,,
1057,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,9,ahshx7,self.MachineLearning,[D] Microsoft AI Residency 2019 - Discussion,https://www.reddit.com/r/MachineLearning/comments/ahshx7/d_microsoft_ai_residency_2019_discussion/,mlnewcomer1,1547945828,"Saw the Google AI Residency thread and thought it would be nice to have one for MSR.

Last year's thread: [https://www.reddit.com/r/MachineLearning/comments/7u2a19/news\_microsoft\_ai\_residency/](https://www.reddit.com/r/MachineLearning/comments/7u2a19/news_microsoft_ai_residency/)

Anybody received an interview invite.?",37,1,False,self,,,,,
1058,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,10,ahsix7,i.redd.it,DeepLabCut: Deep learning toolset for markerless post tracking of animal/people body parts,https://www.reddit.com/r/MachineLearning/comments/ahsix7/deeplabcut_deep_learning_toolset_for_markerless/,rememberlennydotcom,1547946020,,1,1,False,https://b.thumbs.redditmedia.com/gq-F-lKipa-VF8yqI_YszxTvFLmHBn-kIlSolSltpgs.jpg,,,,,
1059,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,10,aht0qc,jesse.thejoyfulprogrammer.com,"Jesse Artificial Intelligence, an 8-bit, TUI, two color display",https://www.reddit.com/r/MachineLearning/comments/aht0qc/jesse_artificial_intelligence_an_8bit_tui_two/,NAIDBTPROG,1547949506,,0,1,False,default,,,,,
1060,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,11,aht654,self.MachineLearning,[D]Unpopular Opinion: Tensorflow 2.0 is changing everything I loved about Tensorflow,https://www.reddit.com/r/MachineLearning/comments/aht654/dunpopular_opinion_tensorflow_20_is_changing/,HotVector,1547950581,"1. Ability to easily integrate any dataset(we all have to use [tf.data](https://tf.data) now)
2. Low-level tf.layers API(tf.keras is higher level, has a performance hit, and (I think) cannot be used with tf.variable\_scope
3. Graphs - Eager execution will be the default, meaning worse performance
4. Estimator API - this completely destroys things like VAEs and GANs

What do you think?",39,1,False,self,,,,,
1061,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,11,aht7lk,self.MachineLearning,[D] CNN Pretrained on Art?,https://www.reddit.com/r/MachineLearning/comments/aht7lk/d_cnn_pretrained_on_art/,The_Austinator,1547950886,"There are lots of good pretrained models trained on datasets like imagenet, but is there anything similar that's been trained on art or design images (e.g. dribble.com)?",3,1,False,self,,,,,
1062,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,13,ahu8ds,self.MachineLearning,[P] Validation issues,https://www.reddit.com/r/MachineLearning/comments/ahu8ds/p_validation_issues/,thetechkid,1547958723,"I am working on an image classification project and I am having some issues with my models. I can get my training validation to get highish(like 80%-90%), however; my validation data never seems to get higher than 20%. I have tried several things and hope someone may be able to help me out, and here is some information in hope of it helping:

&amp;#x200B;

\- I am using Keras with TensorFlow

&amp;#x200B;

\- I am starting with images of size 1024x1024 and scaling them down to 256x256 in the code, and they are of chest x-rays. I use the class\_mode of grey scale in my models if that makes a difference. 

&amp;#x200B;

\- I have 8 types of classes, I don't have a predefined dataset so the way I am separating them is through sub folders in the training and testing folders. I also use ""train\_datagen.flow\_from\_directory"" and ""test\_datagen.flow\_from\_directory"" for the data(I'm not sure if this is the right approach for the labels or classes, so if it is not then I would appreciate a helping hand in correcting this). 

&amp;#x200B;

\- For my test images I have about 7474 images and 1468 for my test data currently, I can add more but I've been slowly ramping up the amount of images in each run and the data validation is not improving.

&amp;#x200B;

\- I have tried the model structures AlexNet, VGG16, VGG19, and ResNet.

&amp;#x200B;

\- As for my los and metrics I have used loss=binary\_crossentropy and metrics=categorical\_accuracy and I have also tried loss=categorical\_crossentropy and metrics=categorical\_accuracy. I have tried going through the documentation for this on keras and I'm not sure if Categorical Accuracy is the correct metric to be using, but the documentation doesn't go into much detail(or at least isn't very good about what metrics are good for what). As for the optimizer I have tried Adam, Nadam, and I have had the best results with SGD.

&amp;#x200B;

\- I normally run about 50 epochs and then the loss for validation grows a lot more rapidly and the accuracy stops improving with the training accuracy increases and the los does not. 

&amp;#x200B;

\- I have tried batch normalization with slightly better results than without, but L1 and L2 regularization seem to be detrimental effects on the accuracy of my models.

&amp;#x200B;

I'm at a loss for what else I could do to improve my results, but if anymore information is required to help with the issue I will do my best to provide it. ",10,1,False,self,,,,,
1063,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,13,ahu99z,self.MachineLearning,Types of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ahu99z/types_of_machine_learning/,corpnce,1547958925,[removed],0,1,False,self,,,,,
1064,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,14,ahukft,self.MachineLearning,Attention model,https://www.reddit.com/r/MachineLearning/comments/ahukft/attention_model/,visvats,1547961425,[removed],0,1,False,self,,,,,
1065,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,14,ahul71,self.MachineLearning,"Have you had to create a bespoke/standalone mobile app to get your initial training set? If so, how did it go? How much did you spend to get it built and to get downloads? Any tips?",https://www.reddit.com/r/MachineLearning/comments/ahul71/have_you_had_to_create_a_bespokestandalone_mobile/,ryanmerket,1547961605,[removed],0,1,False,self,,,,,
1066,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,14,ahunny,self.MachineLearning,"[D] Have you had to create a bespoke/standalone mobile app to get your initial training set? If so, how did it go? How much did you spend to get it built and to get downloads to get a big enough dataset? Any tips?",https://www.reddit.com/r/MachineLearning/comments/ahunny/d_have_you_had_to_create_a_bespokestandalone/,ryanmerket,1547962205,"Also, what did you build that required mobile vs desktop?",1,1,False,self,,,,,
1067,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,14,ahutju,github.com,"Street art, not street art",https://www.reddit.com/r/MachineLearning/comments/ahutju/street_art_not_street_art/,rememberlennydotcom,1547963687,,0,1,False,default,,,,,
1068,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,15,ahv6c3,luden.io,What's your opinion of the game while True: learn() ?,https://www.reddit.com/r/MachineLearning/comments/ahv6c3/whats_your_opinion_of_the_game_while_true_learn/,coloriz95,1547967110,,0,1,False,default,,,,,
1069,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,17,ahvtya,self.MachineLearning,ML samples,https://www.reddit.com/r/MachineLearning/comments/ahvtya/ml_samples/,eptpfgydrp5ww,1547974209,[removed],0,1,False,self,,,,,
1070,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,18,ahvz4i,self.MachineLearning,Has anyone used video instead of GPS for navigation?,https://www.reddit.com/r/MachineLearning/comments/ahvz4i/has_anyone_used_video_instead_of_gps_for/,fuckinghelldad,1547975819,[removed],0,1,False,self,,,,,
1071,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,20,ahwni3,self.MachineLearning,I want to apply for higher education in I.T. field in Canada. Should I first pursue a degree in Big Data or A.I. or vice-versa ? and which college or university ?,https://www.reddit.com/r/MachineLearning/comments/ahwni3/i_want_to_apply_for_higher_education_in_it_field/,braveone1st,1547983232,[removed],0,1,False,self,,,,,
1072,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,20,ahwpqp,self.careerguidance,A query of mine,https://www.reddit.com/r/MachineLearning/comments/ahwpqp/a_query_of_mine/,braveone1st,1547983933,,0,1,False,default,,,,,
1073,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,21,ahwzid,self.bigdata,I want to apply for higher education in I.T. field in Canada. Should I first pursue a degree in Big Data or A.I. or vice-versa ? and which college or university ?,https://www.reddit.com/r/MachineLearning/comments/ahwzid/i_want_to_apply_for_higher_education_in_it_field/,braveone1st,1547986816,,0,1,False,https://b.thumbs.redditmedia.com/TUALB6V4MCYjLRGE37rnd-dSQyYdDKVafxqrY1uXfaA.jpg,,,,,
1074,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,21,ahx02t,michaeljgrogan.com,[R] Using LSTM to predict electricity consumption trends,https://www.reddit.com/r/MachineLearning/comments/ahx02t/r_using_lstm_to_predict_electricity_consumption/,reamsofdata,1547986994,,0,1,False,default,,,,,
1075,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,21,ahx2vx,self.MachineLearning,Submitting code in supplementary material?,https://www.reddit.com/r/MachineLearning/comments/ahx2vx/submitting_code_in_supplementary_material/,shinybrightmorning,1547987810,[removed],0,1,False,self,,,,,
1076,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,21,ahx67o,self.MachineLearning,Are deep neural network parametric statistic?,https://www.reddit.com/r/MachineLearning/comments/ahx67o/are_deep_neural_network_parametric_statistic/,luchins,1547988728,[removed],0,1,False,self,,,,,
1077,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,22,ahxp45,mylearningkey.com,Impact of Big Data and Artificial Intelligence to Global Changes - My Learning Key,https://www.reddit.com/r/MachineLearning/comments/ahxp45/impact_of_big_data_and_artificial_intelligence_to/,mylearning_key,1547991609,,0,1,False,https://a.thumbs.redditmedia.com/l9IGV0hJ0JfrTL8B6cBs2onZl-xogkqUEY328j1vD88.jpg,,,,,
1078,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,23,ahxy06,self.MachineLearning,[D] Deep Reinforcement Learning with TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/ahxy06/d_deep_reinforcement_learning_with_tensorflow_20/,Inori,1547993580,"Seems there's quite a bit of confusion about what exactly does TensorFlow 2.0 bring to the table so I wrote a overview blog post, sharing my experiences with TensorFlow 2.0 by implementing a popular DRL algorithm (A2C) from scratch. I provide an overview of the Keras subclassing API, eager execution, session replacements, and some general tricks to make life easier. I've also tried to make the blog post self-contained with regards to theory, so if you need a refresher on DRL - check it out as well :)

You can read the blog post here: http://inoryy.com/post/tensorflow2-deep-reinforcement-learning  

If you want to skip directly to source code, it's available here: https://github.com/inoryy/tensorflow2-deep-reinforcement-learning  

And if you want to play with source code online, it's available on Google Colab: https://colab.research.google.com/drive/12QvW7VZSzoaF-Org-u-N6aiTdBN5ohNA",14,1,False,self,,,,,
1079,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,23,ahy1i6,deeplearningio.com,[P] How to Install Amazon AWS for Deep Learning? | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/ahy1i6/p_how_to_install_amazon_aws_for_deep_learning/,mrcgllr,1547994335,,0,1,False,default,,,,,
1080,MachineLearning,t5_2r3gv,2019-1-20,2019,1,20,23,ahy3i0,self.MachineLearning,Text generation using Generative adversarial networks,https://www.reddit.com/r/MachineLearning/comments/ahy3i0/text_generation_using_generative_adversarial/,khushboolathia,1547994745,[removed],0,1,False,self,,,,,
1081,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,1,ahz0hp,self.MachineLearning,[D] Deep learning is representation learning (by machines). Science is representation learning (by humans).,https://www.reddit.com/r/MachineLearning/comments/ahz0hp/d_deep_learning_is_representation_learning_by/,UltraMarathonMan,1548001120,"Do you agree/disagree? My favorite example from science (the one that arguably launched the scientific revolution):

https://i.redd.it/0mrvcpfislb21.gif

This is an excerpt slide from [lecture on deep learning basics](https://www.youtube.com/watch?v=O5xeyoRL95U). 

Sun/planet trajectory visualization by [Giorgi Koberidze](https://www.youtube.com/watch?v=waexG16WZrE).",1,1,False,https://b.thumbs.redditmedia.com/pJTHkzB5N4L-GGwVW-dcvNLyCw9LFB5VdeXJVWZU5yc.jpg,,,,,
1082,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,1,ahz0y2,self.MachineLearning,[D] A topic for a short ML lecture,https://www.reddit.com/r/MachineLearning/comments/ahz0y2/d_a_topic_for_a_short_ml_lecture/,Isbiltur,1548001202,"Hello,

Next month I'm going to attend a camp for CS students where I will have an opportunity to give a short informal lecture on any CS-related subject. As I'm very passionate about machine learning I thought this is the field I could focus on while preparing the talk. The problem is that I can't really find any suitable topic. The main difficulty is a fact that there are going to be both people that are really into ML as well as the ones that are not so familiar with it so I can't pick anything very basic nor quite advanced. I thought about some recent researches or not-so-common issues but, well, I'm out of ideas.
Oh, and the lecture should be up to 30 minutes long.

It would be great to hear your suggestions about the topic.",6,1,False,self,,,,,
1083,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,1,ahz3ko,self.MachineLearning,[D] Deep learning is representation learning (by machines). Science is representation learning (by humans).,https://www.reddit.com/r/MachineLearning/comments/ahz3ko/d_deep_learning_is_representation_learning_by/,UltraMarathonMan,1548001678,"Do you agree/disagree? My favorite example from science (the one that arguably launched the scientific revolution):

&amp;#x200B;

![video](z54l05xvtlb21)

This is an excerpt slide from [lecture on deep learning basics](https://www.youtube.com/watch?v=O5xeyoRL95U).

Sun/planet trajectory visualization by [Giorgi Koberidze](https://www.youtube.com/watch?v=waexG16WZrE).

In case the above video/gif doesn't work, here's an image:

![img](b2jybak6ulb21)

&amp;#x200B;",0,1,False,https://b.thumbs.redditmedia.com/o7NVrg7yR2gxJcZCUREK-XximDcaanmgLC8PUvdXgkU.jpg,,,,,
1084,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,1,ahz69r,self.MachineLearning,[D] Deep learning is representation learning (by machines). Science is representation learning (by humans).,https://www.reddit.com/r/MachineLearning/comments/ahz69r/d_deep_learning_is_representation_learning_by/,UltraMarathonMan,1548002135,"Do you agree/disagree? My favorite example from science (the one that arguably launched the scientific revolution):

![img](rkxpuco8vlb21)

And an animated version:

![video](u0iqzqfcvlb21)

This is an excerpt slide from [lecture on deep learning basics](https://www.youtube.com/watch?v=O5xeyoRL95U).

Sun/planet trajectory visualization by [Giorgi Koberidze](https://www.youtube.com/watch?v=waexG16WZrE).",3,1,False,https://b.thumbs.redditmedia.com/ll_OG61lpDs7zijralwfVTyytax1FB05ahfR_r3eybA.jpg,,,,,
1085,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,2,ahzv6f,deeplearningio.com,[R] Google Colab Free GPU Tutorial | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/ahzv6f/r_google_colab_free_gpu_tutorial_deep_learning_io/,mrcgllr,1548004364,,0,1,False,https://b.thumbs.redditmedia.com/MAh4ivl7KXL6wKrrtPVWHSQPdNckFBHEm_-Yr7BJl7c.jpg,,,,,
1086,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,2,ahzvn9,self.MachineLearning,[D] Best resources on Bayesian neural networks,https://www.reddit.com/r/MachineLearning/comments/ahzvn9/d_best_resources_on_bayesian_neural_networks/,davinci1913,1548004443,"I am currently interested in learning more about quantifying uncertainty in deep learning, primarily by using Bayesian methods, as there has been a lot of promising research published lately (my background is from statistics, so might be a bit biased here). 

I have already had a look at papers and theses by Y. Gal, C. Blundell, A. Kendall, R. Neal, and some more, and think I have a basic understanding of ideas such as *Bayes by backprop*, and *Dropout as Bayesian Approximation.* It's really exciting to see how existing methods and best practices turn out to be justified by Bayesian reasoning. Of course some of it still feels a bit ad hoc, and the really interesting thing I think would be actively using Bayesian statistics to develop new algorithms to improve and add features existing architectures.

What are some interesting resources going through various aspects and methods of Bayesian neural networks? I have really enjoyed Yarin Gal's [blog](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html), and would love more to see more resources in that style. I am also very interested in good resources on TF Probability. Any suggestions would be highly appreciated!

**TL;DR:** What are the best resources, i.e. papers, Medium posts, tutorials, GitHub repos, etc., to get started with Bayesian neural networks? In particular I would be very interested articles or posts comparing various methods and approaches to BNN, or commented code using TensorFlow Probability for doing (approximate) Bayesian inference in neural networks. 

&amp;#x200B;",4,1,False,self,,,,,
1087,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,2,ahzwr8,i.redd.it,10 year challenge!,https://www.reddit.com/r/MachineLearning/comments/ahzwr8/10_year_challenge/,visvats,1548004616,,0,1,False,https://b.thumbs.redditmedia.com/U2KYcGymZREff1QQlYCxUJ52ub-tNMEYpIAyXBzNHms.jpg,,,,,
1088,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,2,ahzydd,self.MachineLearning,[D] Latest developments on random matrix theory for deep learning,https://www.reddit.com/r/MachineLearning/comments/ahzydd/d_latest_developments_on_random_matrix_theory_for/,IborkedyourGPU,1548004875,"What are the latest developments on using random matrix theory to study deep learning? I don't know much about random matrix theory, and I'd like to read either a review or a state of the art paper, to see what (if any) advantages it gives. Last time I checked, the only results available were for single layer neural networks, but something may have changed in meantime.",13,1,False,self,,,,,
1089,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,2,ahzyyo,medium.com,"Activation Functions : Sigmoid, ReLU, Leaky ReLU and Softmax basics for Neural Networks and Deep",https://www.reddit.com/r/MachineLearning/comments/ahzyyo/activation_functions_sigmoid_relu_leaky_relu_and/,himanshuxD,1548004973,,0,1,False,https://a.thumbs.redditmedia.com/EV8hcyjilCUJigSra57kXh0jukRVbaF4d1fW5KBGWZ4.jpg,,,,,
1090,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,2,ai0bpg,self.MachineLearning,"[D] Suggestions on references -- use of gradient boosting (particularly XGBoost) for small n, large p problems?",https://www.reddit.com/r/MachineLearning/comments/ai0bpg/d_suggestions_on_references_use_of_gradient/,NithSahor,1548007010,"I was wondering if anybody here might know of some salient examples for the use of gradient boosting methods (particularly XGBoost) for high dimensional problems (small number of observations n, large number of features p)? I'm looking for some papers to cite in order to make the case that XGBoost is suited for these problems, but all of the papers I am finding via Google Scholar seem to be relatively small in the number of features. Any pointers would be greatly appreciated!",2,1,False,self,,,,,
1091,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,3,ai0npt,self.MachineLearning,[D] Has anyone heard back from NLP groups for PhD programs?,https://www.reddit.com/r/MachineLearning/comments/ai0npt/d_has_anyone_heard_back_from_nlp_groups_for_phd/,HipsterToofer,1548008923,"The major ones (e.g., MIT/Stanford/Berkeley/UWash). If so, what were your interviews like?",4,1,False,self,,,,,
1092,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,4,ai11fz,self.MachineLearning,Computer Vision - Improving Images,https://www.reddit.com/r/MachineLearning/comments/ai11fz/computer_vision_improving_images/,ImaginaryAnon,1548011012,[removed],0,1,False,self,,,,,
1093,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,4,ai1bq0,self.MachineLearning,GPU based training time worse then CPU based time.,https://www.reddit.com/r/MachineLearning/comments/ai1bq0/gpu_based_training_time_worse_then_cpu_based_time/,GODSPEED320,1548012651,[removed],0,1,False,self,,,,,
1094,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,5,ai1n58,self.MachineLearning,[D] On first layer convnet filters,https://www.reddit.com/r/MachineLearning/comments/ai1n58/d_on_first_layer_convnet_filters/,idg101,1548014443,"For datasets like cifar and imagenet, the first layer convnet filters are often edge and gabor-like filters.  But, for many other datasets, I finding that this is not the case.  The filters do not change much from their initial starting position, yet the net overall achieves good accuracy.

&amp;#x200B;

I would like to have a meaningful discussion about this phenomena.",18,1,False,self,,,,,
1095,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,5,ai1sm5,self.MachineLearning,Are usually non-image data things turned into image representations for the sole purpose of classification by image analysis?,https://www.reddit.com/r/MachineLearning/comments/ai1sm5/are_usually_nonimage_data_things_turned_into/,python_noob_001,1548015280,[removed],0,1,False,self,,,,,
1096,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,5,ai1xm5,self.MachineLearning,Matterport Mask_RCNN is stuck in first epoch,https://www.reddit.com/r/MachineLearning/comments/ai1xm5/matterport_mask_rcnn_is_stuck_in_first_epoch/,DVDplayr,1548016092,[removed],0,1,False,self,,,,,
1097,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,5,ai1xyx,ai.google,"Google.org is issuing an open call to organizations around the world to submit their ideas for how they could use AI to help address societal challenges. Selected organizations will receive support from Googles AI experts, Google.org grant funding from a $25M pool, credit and consulting from Google",https://www.reddit.com/r/MachineLearning/comments/ai1xyx/googleorg_is_issuing_an_open_call_to/,Tauheedul,1548016150,,0,1,False,default,,,,,
1098,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,5,ai253p,self.MachineLearning,Dating site recomender system,https://www.reddit.com/r/MachineLearning/comments/ai253p/dating_site_recomender_system/,LanTheOne,1548017285,[removed],0,1,False,self,,,,,
1099,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,5,ai26y3,self.MachineLearning,"""Bandwagon"" of ML research",https://www.reddit.com/r/MachineLearning/comments/ai26y3/bandwagon_of_ml_research/,dh27182,1548017578,[removed],0,1,False,self,,,,,
1100,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,6,ai29gi,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 55,https://www.reddit.com/r/MachineLearning/comments/ai29gi/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1548018006,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/mind_juice: [Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling](https://arxiv.org/abs/1812.01608)

/u/TotesMessenger: [D\] Machine Learning - WAYR (What Are You Reading) - Week 54](https://www.reddit.com/r/u_lonely_perceptron/comments/adgkwr/d_machine_learning_wayr_what_are_you_reading_week/)

Besides that, there are no rules, have fun.",0,1,False,self,,,,,
1101,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,6,ai2ak3,arxiv.org,Depth-Limited Solving for Imperfect-Information Games NIPS18,https://www.reddit.com/r/MachineLearning/comments/ai2ak3/depthlimited_solving_for_imperfectinformation/,yazriel0,1548018180,,0,1,False,default,,,,,
1102,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,6,ai2b32,arxiv.org,[R] Depth-Limited Solving for Imperfect-Information Games NIPS18,https://www.reddit.com/r/MachineLearning/comments/ai2b32/r_depthlimited_solving_for_imperfectinformation/,yazriel0,1548018263,,2,1,False,default,,,,,
1103,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,6,ai2ffx,self.MachineLearning,"[D] ""Bandwagon"" of ML research",https://www.reddit.com/r/MachineLearning/comments/ai2ffx/d_bandwagon_of_ml_research/,dh27182,1548018939,"Watching David Dunson (Duke) give a talk on Scalable Bayesian Inference [https://www.facebook.com/nipsfoundation/videos/289885991643586/](https://www.facebook.com/nipsfoundation/videos/289885991643586/) he raises an interesting point about the ""bandwagon"" of current research (see 8:00 in the URL), i.e. people work on similar problems, while other important topics are being untouched.

In your opinion, what important and open problems are currently untouched?

IMHO, he has a good point, there's a lot of research on e.g. generative models where it seems that the research is concerned with ""only"" architectural designs and improvement within current frameworks (GAN vs VAE) rather than coming up with new frameworks and ideas.",41,1,False,self,,,,,
1104,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,7,ai31co,self.MachineLearning,Google and NXP advance artificial intelligence with the Edge TPU,https://www.reddit.com/r/MachineLearning/comments/ai31co/google_and_nxp_advance_artificial_intelligence/,TKozzer,1548022444,[removed],0,1,False,self,,,,,
1105,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,7,ai3a5f,self.MachineLearning,GAN Latent space interpolation before it became cool with GPUs - MJ's Black or White.,https://www.reddit.com/r/MachineLearning/comments/ai3a5f/gan_latent_space_interpolation_before_it_became/,inglorious_noname,1548023926,"&amp;#x200B;

![video](lgw5y475nnb21 ""Associated paper: http://www.hammerhead.com/thad/morph.html - Feature based image metamorphosis. "")",0,1,False,self,,,,,
1106,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,7,ai3f7k,youtu.be,Basics of ML,https://www.reddit.com/r/MachineLearning/comments/ai3f7k/basics_of_ml/,Gekko48,1548024790,,0,1,False,https://b.thumbs.redditmedia.com/OGvEl2jA8MzjDT4u21CQ9-scTprrYYdDrO6YQQCq0vM.jpg,,,,,
1107,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,7,ai3fv6,self.MachineLearning,How to improve ML skills from intermediate to expert?,https://www.reddit.com/r/MachineLearning/comments/ai3fv6/how_to_improve_ml_skills_from_intermediate_to/,dexplorer,1548024895,[removed],0,1,False,self,,,,,
1108,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,8,ai3kk7,self.MachineLearning,[D] How to improve ML skills from intermediate to expert?,https://www.reddit.com/r/MachineLearning/comments/ai3kk7/d_how_to_improve_ml_skills_from_intermediate_to/,dexplorer,1548025675,"I've been working as a data scientist role in consulting firms for around 5 years, and currently looking to transition into product companies. 

Let's say I want to target big 4 tech companies then how should I build my skills? Most of the MOOC's are targeted towards entry level, on the other hand reading dense research papers take lots of time and one can learn only about a specific niche problem. 

My question is how do I transform from an intermediate data scientist to a research scientist at big 4 tech firm, in most efficient step by step way? What books, forums, kaggle projects, side projects, lecture, other resources would you recommend?",25,1,False,self,,,,,
1109,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,8,ai3zci,ai.google,25M Impact Challenge Google AI,https://www.reddit.com/r/MachineLearning/comments/ai3zci/25m_impact_challenge_google_ai/,rememberlennydotcom,1548028146,,0,1,False,default,,,,,
1110,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,9,ai4j0p,self.MachineLearning,"I am flailing about aimlessly looking for a thesis topic, looking for advice",https://www.reddit.com/r/MachineLearning/comments/ai4j0p/i_am_flailing_about_aimlessly_looking_for_a/,pseudo_random_user,1548031626,[removed],0,1,False,self,,,,,
1111,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,10,ai4vro,self.MachineLearning,[D] Anyone here use Bytenet (-like) architecture for Seq2Seq tasks?,https://www.reddit.com/r/MachineLearning/comments/ai4vro/d_anyone_here_use_bytenet_like_architecture_for/,tsauri,1548034026,"Despite lesser than SOTA accuracy, Bytenet can encode and decode almost in parallel due to not needing to feed back last output as next input. Moreover, encoder's output (hidden state) has proportional size (resolution preserving) to the output, and you can continue decode with *no hidden input*, should you want to output longer sentence. From gradient and activation in convolutional filters, it also does attention-like mapping *without* attention.  
BLEU is quite meh compared to today's SOTA, but with O(1) time complexity, wonder why nobody cares for this type of architecture. The paper's timing is close when Transformer and Wavenet come out  
[https://arxiv.org/abs/1610.10099](https://arxiv.org/abs/1610.10099)",8,1,False,self,,,,,
1112,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,11,ai5gj3,self.MachineLearning,AOGNets=PhraseStructureGrammar+DependencyGrammar+ConvNets,https://www.reddit.com/r/MachineLearning/comments/ai5gj3/aognetsphrasestructuregrammardependencygrammarconv/,viseyeon,1548038122,[removed],0,1,False,https://b.thumbs.redditmedia.com/QdhbdEfs2JPxxFmI6opk-zdFsF8TDbbQGRMgHfawpUY.jpg,,,,,
1113,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,11,ai5j83,self.MachineLearning,[D] I need some machine learning ideas,https://www.reddit.com/r/MachineLearning/comments/ai5j83/d_i_need_some_machine_learning_ideas/,logdone,1548038675,I'm on my final year and right now i'm looking for some ideas about my final year project . I should use machine learning on this project i will work 5 months on this project so i need some advanced ideas .,24,1,False,self,,,,,
1114,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,11,ai5jbf,self.MachineLearning,[D] Possible advantages for an experienced software engineer switching to ML?,https://www.reddit.com/r/MachineLearning/comments/ai5jbf/d_possible_advantages_for_an_experienced_software/,plaguuuuuu,1548038698,"Hi everyone! I'm a software engineer from Australia considering a career change after nearly 10 years in the industry. Obviously there is a heap of learning I need to do in order to get up to speed with ML, I know that much - but what I'm wondering is whether more advanced software skills might be specifically sought-after in data science/ML roles? 

&amp;#x200B;

I'm particularly interested in whether maintainability and I guess the architecture in general of ML applications is starting to become an concern in production, in much the same way as they ultimately became a pain point for web apps with large front-end code bases (hence the rise of frameworks such as React and subsequent influx of back-end devs into the scene). I suspect that this might be an interesting niche but honestly don't know a whole lot about the industry at the moment.

&amp;#x200B;

Thanks :-)",17,1,False,self,,,,,
1115,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,12,ai5p0r,self.MachineLearning,training multi turn dialog generation model,https://www.reddit.com/r/MachineLearning/comments/ai5p0r/training_multi_turn_dialog_generation_model/,cher_super,1548039879,[removed],0,1,False,self,,,,,
1116,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,12,ai5uoy,self.MachineLearning,Are there any high resolution image segmentation datasets?,https://www.reddit.com/r/MachineLearning/comments/ai5uoy/are_there_any_high_resolution_image_segmentation/,kadskakm,1548040977,[removed],0,1,False,self,,,,,
1117,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,13,ai66n3,i.redd.it,Perito Regulador de Sinistro VI,https://www.reddit.com/r/MachineLearning/comments/ai66n3/perito_regulador_de_sinistro_vi/,JamurGerloff,1548043231,,0,1,False,https://b.thumbs.redditmedia.com/ZRohUWWjuRgJOad73S4-YIe2noYO_bTGYGnTWxU7IJs.jpg,,,,,
1118,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,13,ai69d8,self.MachineLearning,Newbie: How to extract Field values from Document,https://www.reddit.com/r/MachineLearning/comments/ai69d8/newbie_how_to_extract_field_values_from_document/,chillysause,1548043766,[removed],0,1,False,self,,,,,
1119,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,13,ai6fud,self.MachineLearning,[P] Introducing turkey: an Amazon Mechanical Turk turn-key segment tool.,https://www.reddit.com/r/MachineLearning/comments/ai6fud/p_introducing_turkey_an_amazon_mechanical_turk/,bantou_41,1548045094,"Crowd-sourcing image instance segmentation data can be cumbersome, especially when you have to write the tool from scratch. That's why I created turkey. The [code](https://github.com/yanfengliu/turkey) is designed to be directly copied and pasted onto [Amazon Mechanical Turk](https://www.mturk.com/) for instance segmentation. Its current functions include:
* Customize the annotation modes (dot, line, polygon) and class labels on per-image basis
* Import previous annotations generated by either another human or an algorithm (such as a neural network) 
* Zoom, delete, undo, reset
The repository also includes a Matlab script that reads the downloadable csv result and reconstructs the annotations. 

I'm aware that Amazon has recently released their own image segmentation tool. There are 3 advantages of my tool compared to theirs:
(1) It seems to be a semantic segmentation tool, which is different from instance segmentation. For semantic segmentation you basically label which pixel belongs to which class, but if there are multiple instances of the same class, semantic segmentation doesn't differentiate them. 
(2) Their tool also seemed a little buggy. When I tried it, only 1 of the 2 images got loaded correctly, and the annotation results in the downloaded csv was confusing (it looked somewhat like `B8gzS96nSL7Xnp0`). 
(3) The most import thing is that their tool is encapsulated into their internal API, which we have no direct access to. That means if you want to change something or add something that was not included in the official example, you would have to write a new tool from scratch. My tool is written in plain JavaScript, with a little help from jQuery and bootstrap. Feel free to adapt it to your needs. 

Code is tested on Chrome and Microsoft Edge. Development is still in progress. Ideas and suggestions are welcome! ",7,1,False,self,,,,,
1120,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,15,ai75pv,self.MachineLearning,Need Help.,https://www.reddit.com/r/MachineLearning/comments/ai75pv/need_help/,TheBlack_Demon,1548050517,[removed],0,1,False,self,,,,,
1121,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,15,ai7c9l,wahalengineers.com,High Shear at Best Price in India,https://www.reddit.com/r/MachineLearning/comments/ai7c9l/high_shear_at_best_price_in_india/,wahalengineers,1548051990,,0,1,False,default,,,,,
1122,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,15,ai7d8l,self.MachineLearning,Does anyone studied Neural Processes?,https://www.reddit.com/r/MachineLearning/comments/ai7d8l/does_anyone_studied_neural_processes/,sjh9020,1548052204,[removed],0,1,False,self,,,,,
1123,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,16,ai7m9h,self.MachineLearning,[D] Does anyone studied neural processes?,https://www.reddit.com/r/MachineLearning/comments/ai7m9h/d_does_anyone_studied_neural_processes/,sjh9020,1548054291,"I'm reading Neural Process, [https://arxiv.org/pdf/1807.01622.pdf](https://arxiv.org/pdf/1807.01622.pdf)

&amp;#x200B;

and I'm trying to implement it with the code someone already implemented.

But I feel some lack of knowledge about this field. such as loss function, there is no mention of loss in the paper ...

&amp;#x200B;

Could you recommend something to read or watch about NPs ?",8,1,False,self,,,,,
1124,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,16,ai7o9r,self.MachineLearning,Machine Learning Bangalore | Machine Learning Course Bangalore,https://www.reddit.com/r/MachineLearning/comments/ai7o9r/machine_learning_bangalore_machine_learning/,manishaarora12321,1548054765,[removed],0,1,False,self,,,,,
1125,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,16,ai7out,theappsolutions.com,Recommender Systems and Their Impact on Customer Experience,https://www.reddit.com/r/MachineLearning/comments/ai7out/recommender_systems_and_their_impact_on_customer/,lady_monsoon,1548054906,,0,1,False,default,,,,,
1126,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,16,ai7qlr,self.MachineLearning,Converting Manga into YouTube videos using AI,https://www.reddit.com/r/MachineLearning/comments/ai7qlr/converting_manga_into_youtube_videos_using_ai/,ZiyaBeastFreak,1548055333,[removed],0,1,False,self,,,,,
1127,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,16,ai7u1e,arxiv.org,[R] Photo-Sketching: Inferring Contour Drawings from Images,https://www.reddit.com/r/MachineLearning/comments/ai7u1e/r_photosketching_inferring_contour_drawings_from/,milaworld,1548056182,,4,1,False,default,,,,,
1128,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,16,ai7utx,github.com,"MXNet implementation of the paper ""Neural Arithmetic Logic Units"" by DeepMind",https://www.reddit.com/r/MachineLearning/comments/ai7utx/mxnet_implementation_of_the_paper_neural/,gautamrbharadwaj,1548056379,,0,1,False,default,,,,,
1129,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,17,ai82nr,self.MachineLearning,"[R] Pytorch-Kaldi, the best way to build your ASR system with Pytorch and Kaldi",https://www.reddit.com/r/MachineLearning/comments/ai82nr/r_pytorchkaldi_the_best_way_to_build_your_asr/,TParcollet,1548058377,"Hi everyone ! 

Our toolkit Pytorch-Kaldi has been released ! 

You can check the **paper** here:

[https://www.researchgate.net/publication/329057479\_The\_PyTorch-Kaldi\_Speech\_Recognition\_Toolkit](https://www.researchgate.net/publication/329057479_The_PyTorch-Kaldi_Speech_Recognition_Toolkit)

[https://arxiv.org/abs/1811.07453](https://arxiv.org/abs/1811.07453)

And obviously start to make your own ASR system by taking a look at the **code** and **tutorials**: 

[https://github.com/mravanelli/pytorch-kaldi](https://github.com/mravanelli/pytorch-kaldi)

&amp;#x200B;

The toolkit is designed to help students, researchers, and practitioners to easily develop speech recognition systems. PyTorch-Kaldi is designed to easilyplug-in user-defined neural models and can naturally employcomplex systems based on a combination of features, labels, and neural architectures. With the toolkit, we are able to achieve state-of-the-art performancein many speech tasks. PyTorch-Kaldi is released under theCreative Commons Attribution 4.0 International license. You can copy, distribute, modify the code for research, commercial and non-commercial purposes !",46,1,False,self,,,,,
1130,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,17,ai88iv,self.MachineLearning,[D] Any difference in applying stepwise regularization or simultaneous regularization ?,https://www.reddit.com/r/MachineLearning/comments/ai88iv/d_any_difference_in_applying_stepwise/,amil123123,1548060005,"Hello,

I was studying more about regularization and found that the 2 most basic types of regularization while modelling are L1 &amp; L2. I was wondering that will there be any changes in the final optimum model if -

1. Stepwise regularization is used - First train a model with L1 regularization. After the loss converges train with L2.
2. Simultaneous Regularization - Apply both L1 &amp; L2 regularization to the loss of the model and train.

If there are no model prediction advantages then are there any advantages in terms of speed?",18,1,False,self,,,,,
1131,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,17,ai8ap3,self.MachineLearning,Deep Learning 1 Week Personal Hackathon,https://www.reddit.com/r/MachineLearning/comments/ai8ap3/deep_learning_1_week_personal_hackathon/,eat_those_lemons,1548060623,[removed],1,1,False,self,,,,,
1132,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,18,ai8gi0,endtoend.ai,"[D] RL Weekly 5: Robust Control of Legged Robots, Compiler Phase-Ordering, and Go Explore on Sonic the Hedgehog",https://www.reddit.com/r/MachineLearning/comments/ai8gi0/d_rl_weekly_5_robust_control_of_legged_robots/,seungjaeryanlee,1548062234,,0,1,False,https://b.thumbs.redditmedia.com/J6nvn4IeJWvuC-EgIxQr2lShjlCPpCvoDDZRuBQnW9Q.jpg,,,,,
1133,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,18,ai8ibr,cityfalcon.com,Machine Learning Top News | CityFALCON,https://www.reddit.com/r/MachineLearning/comments/ai8ibr/machine_learning_top_news_cityfalcon/,mr_j_b,1548062733,,0,1,False,default,,,,,
1134,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,18,ai8jmz,self.MachineLearning,Top 5 Data Visualization Tools for 2019,https://www.reddit.com/r/MachineLearning/comments/ai8jmz/top_5_data_visualization_tools_for_2019/,dimensionless_tech,1548063105,[removed],0,1,False,self,,,,,
1135,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,18,ai8p7r,i.redd.it,[D] BERT - What are the segment and position embeddings used in here?,https://www.reddit.com/r/MachineLearning/comments/ai8p7r/d_bert_what_are_the_segment_and_position/,JClub,1548064634,,0,1,False,default,,,,,
1136,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,19,ai8ruj,self.MachineLearning,What Kind of Machine Learning Problem You're Dealing With When Performing A/B Tests?,https://www.reddit.com/r/MachineLearning/comments/ai8ruj/what_kind_of_machine_learning_problem_youre/,incans,1548065321,[removed],0,1,False,self,,,,,
1137,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,19,ai8uj9,self.MachineLearning,Machine Learning joint entropy,https://www.reddit.com/r/MachineLearning/comments/ai8uj9/machine_learning_joint_entropy/,derek291,1548066024,[removed],0,1,False,self,,,,,
1138,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,19,ai8y0e,self.MachineLearning,How to deploy custom model to android,https://www.reddit.com/r/MachineLearning/comments/ai8y0e/how_to_deploy_custom_model_to_android/,Vinceeeent,1548066973,[removed],0,1,False,self,,,,,
1139,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,19,ai8yyx,self.MachineLearning,How to weight important features,https://www.reddit.com/r/MachineLearning/comments/ai8yyx/how_to_weight_important_features/,Spiritual_Doughnut,1548067238,[removed],0,1,False,self,,,,,
1140,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,19,ai90cz,self.MachineLearning,Learning for Citizen Data Scientists and Data Literacy Across the Enterprise!,https://www.reddit.com/r/MachineLearning/comments/ai90cz/learning_for_citizen_data_scientists_and_data/,ElegantMicroWebIndia,1548067628,[removed],0,1,False,self,,,,,
1141,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,19,ai92sr,self.MachineLearning,Anchor boxes in Yolov2,https://www.reddit.com/r/MachineLearning/comments/ai92sr/anchor_boxes_in_yolov2/,AbanoubErian,1548068291,[removed],0,1,False,self,,,,,
1142,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,20,ai96f0,github.com,[P] InfoGAN implementation in PyTorch,https://www.reddit.com/r/MachineLearning/comments/ai96f0/p_infogan_implementation_in_pytorch/,Natsu6767,1548069198,,0,1,False,default,,,,,
1143,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,20,ai9dlp,self.MachineLearning,mlcc 2019: applications open!,https://www.reddit.com/r/MachineLearning/comments/ai9dlp/mlcc_2019_applications_open/,mlcc2019,1548071132,[removed],0,1,False,self,,,,,
1144,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,21,ai9t0p,mariofilho.com,Can Gradient Boosting Learn Simple Arithmetic?,https://www.reddit.com/r/MachineLearning/comments/ai9t0p/can_gradient_boosting_learn_simple_arithmetic/,ledmmaster,1548075012,,0,1,False,https://b.thumbs.redditmedia.com/nYbwjSlc5alhjY-Pn2GcOuWIrsbHLWa2XpnJrGNk3uw.jpg,,,,,
1145,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,22,ai9w4v,gpuserversrental.com,Cheap GPU servers for deep machine learning,https://www.reddit.com/r/MachineLearning/comments/ai9w4v/cheap_gpu_servers_for_deep_machine_learning/,arettepr,1548075753,,0,1,False,default,,,,,
1146,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,22,aia5rw,self.MachineLearning,[D] Need help to procure a minecraft buildings dataset for a Deep Learning Project,https://www.reddit.com/r/MachineLearning/comments/aia5rw/d_need_help_to_procure_a_minecraft_buildings/,realist_konark,1548077877,"Hi everyone!

I am working on a project involving GANs for 3D Object generation and I thought why not do something cool and see what kind of buildings can AI generate. For that I'd require a dataset preferably of uniform size buildings built by some awesome minecrafters!

Anyone here know of any kind of dataset of this kind? I would really appreciate it if you could help me out.

If all goes well, I'd be sure to share the results I got with you guys!",5,1,False,self,,,,,
1147,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,22,aia72q,self.MachineLearning,[D] Gradient Descent on (deterministic) Mean Absolute Error (L1 loss),https://www.reddit.com/r/MachineLearning/comments/aia72q/d_gradient_descent_on_deterministic_mean_absolute/,svantana,1548078160,"Gradient-based optimization of absolute errors is tricky, since the gradient is ""never"" zero. In theory, adaptive methods should be able to damp oscillations so that it converges to the minimum. However, I found none of the 'standard' methods were able to do this ""out of the box"". Learning rate decay could alleviate the problem, but needs manual tuning which I would rather avoid. **Does anyone know of a method that can do this?**

Below is the result of minimizing `sum(abs(x1-x2))` in PyTorch. It's hard to see all the curves, but basically all of them are oscillating around the minimum indefinitely, except L-BFGS which stalls at a fairly good solution, but does not progress to the true minimum (zero). However, L-BFGS could easily diverge with other hyperparam settings.

https://i.redd.it/vriif3r50sb21.png",20,1,False,https://b.thumbs.redditmedia.com/RSjdQqonK48aIMOdLQ2YwCqENPqhLM_66yDtgk0B_DU.jpg,,,,,
1148,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,23,aiaco6,self.MachineLearning,Good books about machine learning,https://www.reddit.com/r/MachineLearning/comments/aiaco6/good_books_about_machine_learning/,Swazzleman,1548079331,[removed],0,1,False,self,,,,,
1149,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,23,aiahfv,self.MachineLearning,C++ implementation of Mask R-CNN with PyTorch,https://www.reddit.com/r/MachineLearning/comments/aiahfv/c_implementation_of_mask_rcnn_with_pytorch/,kolkir,1548080249,[removed],0,1,False,self,,,,,
1150,MachineLearning,t5_2r3gv,2019-1-21,2019,1,21,23,aiasoc,self.MachineLearning,[Project] Cost comparison of Google Cloud's TPUv2 vs V100 GPUs,https://www.reddit.com/r/MachineLearning/comments/aiasoc/project_cost_comparison_of_google_clouds_tpuv2_vs/,BigDataRepublic,1548082433,"We were invited to participate in a [TensorFlow Research Cloud](https://www.tensorflow.org/tfrc/) beta program on Google Cloud where we were given access to a set of dedicated v2 TPUs. We decided to test whether these new devices live up to the hype and benchmarked their performance versus GPUs and CPUs.

&amp;#x200B;

You can read our results in our blog post: [https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f](https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f)

&amp;#x200B;

To summarize: it looks like TPUs definitely have some merit. They are quite fast and cost-efficient (especially the preemptible ones). We did however notice some weird issues with the Adam optimizer that we couldn't reproduce on other devices such as GPUs or CPUs. For good old SGD, however, TPU validation losses were similar to GPU and CPU.  


Have you used TPUs on Google Cloud or Google Colab? What was your experience?",8,1,False,self,,,,,
1151,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,0,aiaw9a,indiehackers.com,How I Turned Knowledge (and Joblessness) Into a Profitable Product,https://www.reddit.com/r/MachineLearning/comments/aiaw9a/how_i_turned_knowledge_and_joblessness_into_a/,srkiboy83,1548083089,,0,1,False,default,,,,,
1152,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,0,aiaxrk,self.MachineLearning,Prediction Problem from Medical Dataset,https://www.reddit.com/r/MachineLearning/comments/aiaxrk/prediction_problem_from_medical_dataset/,BlackSky2129,1548083363,[removed],0,1,False,self,,,,,
1153,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,0,aiazwu,self.MachineLearning,"[P] Generative Models Tutorial with Demo/Code (GANs, VAE, Bayesian Classifier Sampling, Auto-Regressive Models )",https://www.reddit.com/r/MachineLearning/comments/aiazwu/p_generative_models_tutorial_with_democode_gans/,obsezer,1548083747,"Generative models are a  subset of  unsupervised learning that generate new sample/data by using  given some  training data. There are different types of ways of modelling  same  distribution of training data: Auto-Regressive models,  Auto-Encoders  and GANs. In this tutorial, we are focusing theory of  generative  models, demonstration of generative models, important papers,  courses  related generative models. It will continue to be updated over  time.

[https://github.com/omerbsezer/Generative\_Models\_Tutorial\_with\_Demo](https://github.com/omerbsezer/Generative_Models_Tutorial_with_Demo)",8,1,False,self,,,,,
1154,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,0,aib3xt,self.MachineLearning,Good place for cloud computing (for machine learning training),https://www.reddit.com/r/MachineLearning/comments/aib3xt/good_place_for_cloud_computing_for_machine/,Jandevries101,1548084473,[removed],0,1,False,self,,,,,
1155,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,0,aib6nj,self.MachineLearning,[D] Have anyone considered creation of curiosity-driven generative models ?,https://www.reddit.com/r/MachineLearning/comments/aib6nj/d_have_anyone_considered_creation_of/,ad48hp,1548084967,"I've seen reinforcement agents exploring environments based of intrinsically-motivated curiosity (which is quite similiar to the novelty algorhitm), could this be implemeneted in generative models that might explore patterns ?
Or may be this be applied to find out rather 'novel' patterns in neural style transfer which would determine which styles it shall use and how ?",2,1,False,self,,,,,
1156,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,0,aibdeq,self.MachineLearning,Factor Model using Deep Learning or machine learning,https://www.reddit.com/r/MachineLearning/comments/aibdeq/factor_model_using_deep_learning_or_machine/,Git_n_m,1548086121,[removed],0,1,False,self,,,,,
1157,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,0,aibebr,self.MachineLearning,Understanding Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/aibebr/understanding_convolutional_neural_network/,SushiMind,1548086274,[removed],0,1,False,self,,,,,
1158,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,1,aibwzr,self.MachineLearning,Thank You Post,https://www.reddit.com/r/MachineLearning/comments/aibwzr/thank_you_post/,adeebabbas,1548089247,[removed],0,1,False,self,,,,,
1159,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,2,aicc07,self.MachineLearning,GPU choice for deep learning with $300-400 limit [must support FP16],https://www.reddit.com/r/MachineLearning/comments/aicc07/gpu_choice_for_deep_learning_with_300400_limit/,theruby_,1548091570,[removed],0,1,False,self,,,,,
1160,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,2,aicdp0,self.MachineLearning,[P] C++ implementation of Mask R-CNN with PyTorch,https://www.reddit.com/r/MachineLearning/comments/aicdp0/p_c_implementation_of_mask_rcnn_with_pytorch/,kolkir,1548091831,"I made C++ [implementation](https://github.com/Kolkir/mlcpp/tree/master/mask_rcnn_pytorch) of  [Mask R-CNN](https://arxiv.org/abs/1703.06870) with PyTorch C++ frontend. The code is based on PyTorch implementations from [multimodallearning](https://github.com/multimodallearning/pytorch-mask-rcnn) and Keras implementation from [Matterport](https://github.com/matterport/Mask_RCNN) . Project was made for educational purposes and can be used as comprehensive example of PyTorch C++ frontend API. Besides regular API you will find how to: load data from MSCoco dataset, create custom layers, manage weights across language boundaries(Python to C++). Feel free to ask a questions.",5,1,False,self,,,,,
1161,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,2,aicisf,self.MachineLearning,I've programmed (small) deep learning library!,https://www.reddit.com/r/MachineLearning/comments/aicisf/ive_programmed_small_deep_learning_library/,pkaspre,1548092582,[removed],0,1,False,self,,,,,
1162,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,2,aiclia,emotiv.com,Has anyone worked with EPOC+?,https://www.reddit.com/r/MachineLearning/comments/aiclia/has_anyone_worked_with_epoc/,standardspeaks,1548092978,,0,1,False,https://b.thumbs.redditmedia.com/Xw9EPcCMSDDbOZe483ynNo7c8-vg5NcQVPHwNusNf6k.jpg,,,,,
1163,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,2,aicmu8,self.MachineLearning,[P] I've programmed (small) deep learning library!,https://www.reddit.com/r/MachineLearning/comments/aicmu8/p_ive_programmed_small_deep_learning_library/,pkaspre,1548093184,"Hi everyone.

I'm super happy to introduce my side project called: deepy.

Here is the code: https://github.com/kaszperro/deepy

The purpose was to learn how neural networks, autograd and other mechanisms work. Also I think, that those who want to know how mainstream libraries work , it's good starting point, because the code is very small and simple.


It's still in early stage though, but so far I've programmed:

Optimizers: SGD,

Activation Functions: ReLU, Softmax, Sigmoid

Losses: CrossEntropy, MSE


It uses numpy for computations.


In examples directory there is a MNIST digits classifier, which scores over 96% accuracy.


If you like the project, please leave star in my github repo, thanks :).


Contributions welcome :)",33,1,False,self,,,,,
1164,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,3,aicqig,self.MachineLearning,Where can I get a medical dataset ?,https://www.reddit.com/r/MachineLearning/comments/aicqig/where_can_i_get_a_medical_dataset/,ireish,1548093727,[removed],0,1,False,self,,,,,
1165,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,3,aicrh4,self.MachineLearning,[D] What happened here (ICLR 2019 Review)?,https://www.reddit.com/r/MachineLearning/comments/aicrh4/d_what_happened_here_iclr_2019_review/,residual_potato,1548093861,"Does anyone understand what happened in this OpenReview entry: [https://openreview.net/forum?id=rJlWOj0qF7](https://openreview.net/forum?id=rJlWOj0qF7)

&amp;#x200B;

From what I can tell, (a) the reviews were 4,4,3 --&gt; accept, which seemed to anger the reviewers (b) the paper itself and the author comments are all written very curtly and with weird syntax, (c) there seems to be a pretty big anonymization issue raised during reviewing that the AC didn't address, and (d) there appear to be a lot of harsh replies from the authors to the reviewers, which is uncommon. 

&amp;#x200B;

Is there something I am missing here?",35,1,False,self,,,,,
1166,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,3,aicz2h,self.MachineLearning,Algorithm Used on Tesla Autopilot,https://www.reddit.com/r/MachineLearning/comments/aicz2h/algorithm_used_on_tesla_autopilot/,nived90,1548095011,Is anyone familiar with the technology used on the forward facing cameras in used in the Tesla Autopilot system? I can't seem to find in depth information. Is it a YOLO style model or more like TLD?,0,1,False,self,,,,,
1167,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,3,aid5cy,self.MachineLearning,Medical AI Safety: Doing it wrong.,https://www.reddit.com/r/MachineLearning/comments/aid5cy/medical_ai_safety_doing_it_wrong/,gosnold,1548095969,[removed],0,1,False,self,,,,,
1168,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,3,aid8x6,self.MachineLearning,Make Money from your sites Captchas and help ML companies!,https://www.reddit.com/r/MachineLearning/comments/aid8x6/make_money_from_your_sites_captchas_and_help_ml/,LZcrypto,1548096499,[removed],0,1,False,https://b.thumbs.redditmedia.com/614GIvoIuzCcNnDWQHTw6ESQB1S1bt0NJFKs8sLWLaM.jpg,,,,,
1169,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,3,aid9el,self.MachineLearning,Usage of Variable Importance,https://www.reddit.com/r/MachineLearning/comments/aid9el/usage_of_variable_importance/,ragas_,1548096567,[removed],0,1,False,self,,,,,
1170,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,3,aidb0i,self.MachineLearning,[D] Medical AI Safety: Doing it wrong.,https://www.reddit.com/r/MachineLearning/comments/aidb0i/d_medical_ai_safety_doing_it_wrong/,gosnold,1548096815,"Interesting article by Luke Oakden-Rayner on the difference between controlled trials and clinical practice and the implications for AI, using breast computer-aided diagnostic as an example.

 
https://lukeoakdenrayner.wordpress.com/2019/01/21/medical-ai-safety-doing-it-wrong/


TL:DR by the author:


        * Medical AI today is assessed with performance testing; controlled laboratory experiments that do not reflect real-world safety.
        * Performance is not outcomes! Good performance in laboratory experiments rarely translates into better clinical outcomes for patients, or even better financial outcomes for healthcare systems.
        * Humans are probably to blame. We act differently in experiments than we do in practice, because our brains treat these situations differently.
        * Even fully autonomous systems interact with humans, and are not protected from these problems.
        * We know all of this because of one of the most expensive, unintentional experiments ever undertaken. At a cost of hundreds of millions of dollars per year, the US government paid people to use previous-generation AI in radiology. It failed, and possibly resulted in thousands of missed cancer diagnoses compared to best practice, because we had assumed that laboratory testing was enough.

",12,1,False,self,,,,,
1171,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,4,aidgup,self.MachineLearning,How to make a good quality dataset?,https://www.reddit.com/r/MachineLearning/comments/aidgup/how_to_make_a_good_quality_dataset/,randomchickibum,1548097687,I'm creating a dataset for emotion recognition from text for a specific domain. I want it to be high quality and aim it to be used as a standard dataset. What are the quality measures i should follow?,0,1,False,self,,,,,
1172,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,4,aidlvl,electronicsweekly.com,"AI is being over-promised - The fundamental problem is that, because of the recent successes, people are extrapolating and promising things that are just not possible, Federico Faggin, who put the 4004 into silicon gate MOS, delivered the 8008, designed the 8080, the Z8 MCU and the Z80 CPU",https://www.reddit.com/r/MachineLearning/comments/aidlvl/ai_is_being_overpromised_the_fundamental_problem/,UberMSP,1548098420,,0,1,False,https://b.thumbs.redditmedia.com/_XFhYTMqm7SWY6tQJz_ceNrDdAF-omF7cnXrCXNUYKE.jpg,,,,,
1173,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,4,aidmgj,i.redd.it,Trained cycle gan to convert bears to pandas but results are not quite as i expected.I think my discriminator is learning too fast and I think I am not giving enough importance to cycle loss in loss function because generated panda images are completely different from given bear images input.,https://www.reddit.com/r/MachineLearning/comments/aidmgj/trained_cycle_gan_to_convert_bears_to_pandas_but/,charan_1996,1548098507,,0,1,False,default,,,,,
1174,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,5,aieeqy,/r/MachineLearning/comments/aieeqy/training_yolov3_deep_learning_based_custom_object/,Training YOLOv3 : Deep Learning based Custom Object Detector,https://www.reddit.com/r/MachineLearning/comments/aieeqy/training_yolov3_deep_learning_based_custom_object/,sunitanyk,1548102607,,1,1,False,default,,,,,
1175,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,5,aiej1q,self.MachineLearning,[R] Random forest classification used with drone-based multispectral imagery to detect malaria vector breeding sites.,https://www.reddit.com/r/MachineLearning/comments/aiej1q/r_random_forest_classification_used_with/,DarkForgeJ,1548103234,"Hi!

I wanted to share this paper that got recently publish:

[http://dx.plos.org/10.1371/journal.pntd.0007105](http://dx.plos.org/10.1371/journal.pntd.0007105)

In summary, drone surveys were carried out in the four communities of the Peruvian Amazon jungle, obtaining high-resolution RGB and multispectral imagery, along with water body inspection to identify positive and negative breeding sites of the main malaria vector there (*A.* *darlingi*).

The image classification was realized in Google Earth Engine, and a *k*\-fold cross-validation was used to evaluate the performance of the RF (Random Forest) classifier. Along with this, other statistical tests were also used to test the results and methodology (Monte-Carlo CPS,  producers accuracy (PA), positive predictive value (PPV))

&amp;#x200B;

The research team also obtained some really cool images that can be found in their website:

 [https://datacore-upch.com/tdr/](https://datacore-upch.com/tdr/)

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1176,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,6,aieyh6,self.MachineLearning,[D] How to tackle material segmentation?,https://www.reddit.com/r/MachineLearning/comments/aieyh6/d_how_to_tackle_material_segmentation/,EmielBoss,1548104938,"I want to perform semantic/stuff segmentation, but then classify and segment with respect to the material properties of objects in an image. This means that, instead of the network classifying a tree trunk as ""tree"", I would like to have it classified as ""wood"", and I want to classify ""bricks"" instead of ""wall"". What would currently be the best way to tackle this? Is there a state-of-the-art for this? COCO has the Stuff Segmentation task, but it seems severely underpopulated (why is this?). Also, I can't seem to find the paper corresponding to the current front runner (ResNeXt-FPN, by Alexander Kirillov, Kaiming He, Ross Girshick, and Piotr Dollar).  
  
What do you think is currently the best way to material segment? What direction should I take on this? Any advice and comments are greatly appreciated!",6,1,False,self,,,,,
1177,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,7,aifjim,self.MachineLearning,Classification accuracy for the CUB 200 data? Or popular classification dataset,https://www.reddit.com/r/MachineLearning/comments/aifjim/classification_accuracy_for_the_cub_200_data_or/,themis0888,1548108058,[removed],0,1,False,self,,,,,
1178,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,7,aifpht,self.MachineLearning,Books for learning ML programming?,https://www.reddit.com/r/MachineLearning/comments/aifpht/books_for_learning_ml_programming/,wootnoob,1548108980,"I found these on Amazon - are these any good for a beginner? What other books you would recommend?

[Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1547673827&amp;sr=1-1&amp;keywords=Machine+learning+%2F+Data+Science) by Aurlien Gron

[Introduction to Machine Learning with Python: A Guide for Data Scientists](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1547673827&amp;sr=1-2&amp;keywords=Machine+learning+%2F+Data+Science) by Andreas C. Mller and Sarah Guido

[Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies (The MIT Press)](https://www.amazon.com/Fundamentals-Machine-Learning-Predictive-Analytics/dp/0262029448/ref=sr_1_3?s=books&amp;ie=UTF8&amp;qid=1547673827&amp;sr=1-3&amp;keywords=Machine+learning+%2F+Data+Science) by John D. Kelleher and Brian Mac Namee

[Machine Learning For Absolute Beginners: A Plain English Introduction (Second Edition) (Machine Learning For Beginners Book 1)](https://www.amazon.com/Machine-Learning-Absolute-Beginners-Introduction-ebook/dp/B07335JNW1/ref=sr_1_4?s=books&amp;ie=UTF8&amp;qid=1547673827&amp;sr=1-4&amp;keywords=Machine+learning+%2F+Data+Science) by Oliver Theobald",0,1,False,self,,,,,
1179,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,7,aifybf,self.MachineLearning,Where can I find a medical dataset !,https://www.reddit.com/r/MachineLearning/comments/aifybf/where_can_i_find_a_medical_dataset/,ireish,1548110370,[removed],0,1,False,self,,,,,
1180,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,9,aigy9u,self.MachineLearning,[D] What is the SOTA for Natural Language Generation in 2019?,https://www.reddit.com/r/MachineLearning/comments/aigy9u/d_what_is_the_sota_for_natural_language/,DeepLearningDownundr,1548116404,"Researching the current SOTA in NLP text generation, there seems to be a lot of papers who report strong results against their relevant benchmarks. However, the examples they show in the papers seem to be low quality when compared to human writing. 

What are the best examples of natural language generation/style transfer you have seen?",11,1,False,self,,,,,
1181,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,10,aihso3,self.MachineLearning,"[D] Need help for literature review: Does anyone know of a paper where a large number ( Over 50% ) of the weights are turned off at each training step ( besides dropout ) ? Or, if there are any key words/phrases that you suggest querying ?",https://www.reddit.com/r/MachineLearning/comments/aihso3/d_need_help_for_literature_review_does_anyone/,BatmantoshReturns,1548122010,"This is an expansion of an initial literature review I did earlier (https://redd.it/ahq78s), now I'm looking any sort of paper that covers cases where a large (Over 50%) number of weights are turned off during each training step. 

An example of this is pretty much any word embedding architectures, where only a small percentage of the weights are trained during each time-step (the contexts, labels, and negative samples). Another example is something like pathNet, where may of the parameters are frozen after training on a specific task, in order to transfer to a new task. 

Even if you don't know the papers off hand, would appreciate any ideas you have for words or phrases to query.",4,1,False,self,,,,,
1182,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,11,aihwff,self.MachineLearning,[R] CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild [ICLR 2019],https://www.reddit.com/r/MachineLearning/comments/aihwff/r_camou_learning_physical_vehicle_camouflages_to/,neural_potato,1548122688,"Paper: [https://openreview.net/forum?id=SJgEl3A5tm](https://openreview.net/forum?id=SJgEl3A5tm)

I hope it is not too late to advertise our ICLR work.

&amp;#x200B;

https://i.redd.it/buvcbduntvb21.png

 In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles. In the second, we minimize the approximated detection score by searching for the optimal camouflage. Experiments show that the learned camouflage can not only hide a vehicle from the image-based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors. 

Please feel free to ask any questions :)",6,1,False,https://a.thumbs.redditmedia.com/O5xSqPxXQan0eq4XIj_39G9lsyFLtyg3D_81hYNOIr4.jpg,,,,,
1183,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,11,aii4vc,self.MachineLearning,"[D] Help needed, AI startup needs images of your teeth to train ML model",https://www.reddit.com/r/MachineLearning/comments/aii4vc/d_help_needed_ai_startup_needs_images_of_your/,danborthwick,1548124293,"Hi r/MachineLearning,

&amp;#x200B;

AI Dental is a very early stage startup based in Melbourne, Australia, with a mission to improve the world's health through early detection of dental problems. We're currently building an image set of smartphone-captured, real world teeth. If you have a few minutes to spare then we'd be really grateful if you could [use our simple web tool](http://aidental.smartcasual.cc/?utm_source=reddit) to take a couple of pictures. All images are completely anonymous and unidentifiable, will not be shared with any third parties, and will only be used to train our model.

Any questions or problems using the tool, please comment below and I'll try and respond as quickly as possible.

Thanks again for your help,  
Dan

http://aidental.smartcasual.cc/?utm\_source=reddit",4,1,False,self,,,,,
1184,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,11,aiiavf,self.MachineLearning,[Question] Preprocessing reduces performance of a conditional GAN,https://www.reddit.com/r/MachineLearning/comments/aiiavf/question_preprocessing_reduces_performance_of_a/,reddit_tl,1548125434,[removed],0,1,False,self,,,,,
1185,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,12,aiivym,youtube.com,Do you know the snacks packing machine,https://www.reddit.com/r/MachineLearning/comments/aiivym/do_you_know_the_snacks_packing_machine/,Obvious_Performer,1548129470,,0,1,False,default,,,,,
1186,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,13,aiiyda,medium.com,Self Driving Anki Vector  Carlos Gonzalez  Medium,https://www.reddit.com/r/MachineLearning/comments/aiiyda/self_driving_anki_vector_carlos_gonzalez_medium/,gonzalezcgg11,1548129939,,0,1,False,https://b.thumbs.redditmedia.com/EzGIL2X9wN5Vz3Bj0k1aV5pvcQrTZ2pK9IbouyCai3s.jpg,,,,,
1187,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,13,aij0ro,self.MachineLearning,"[D] In YOLO Object detection, how does the algorithm predict bounding boxes larger than the grid size?",https://www.reddit.com/r/MachineLearning/comments/aij0ro/d_in_yolo_object_detection_how_does_the_algorithm/,Eoncarry,1548130429,"From what I understand, the algorithm runs separately on each section of the grid. So taking the car in the picture as an example, how does the algorithm know what's outside the grid-section? (How is it able to predict the car's length across 3 sections if it processed only the middle section)

https://i.redd.it/3u229djzgwb21.png

&amp;#x200B;",13,1,False,https://a.thumbs.redditmedia.com/Mf4Q5isj4Sn75wKdJmA2eamnTs68hKJpfvtNVYiiP40.jpg,,,,,
1188,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,13,aij36k,self.MachineLearning,ML Apps w/ Skafos.ai [N] [D],https://www.reddit.com/r/MachineLearning/comments/aij36k/ml_apps_w_skafosai_n_d/,Davlucmac,1548130925,"We have launched a beta product useful for rapidly prototyping and iterating mobile apps driven by ML. The target users are ""ML curious"" iOS developers or ""app curous"" data scientists. (more mobile platforms coming soon)

While I'm interested in anyone's thoughts on the platform, I'm also interested in your reaction to the concept - an individual or small show tackling ML apps. 

Does this concept resonate with you? What are your thoughts?

Take a look (it's free to try) at https://www.skafos.ai",5,1,False,self,,,,,
1189,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,13,aij4tw,self.MachineLearning,"When reading a new paper in a new area of ML, do you read/skim the cited works?",https://www.reddit.com/r/MachineLearning/comments/aij4tw/when_reading_a_new_paper_in_a_new_area_of_ml_do/,SmartSpray,1548131267,[removed],1,1,False,self,,,,,
1190,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,13,aij8no,i.redd.it,[D] Trying to understand this ES loss curve / failure to converge...,https://www.reddit.com/r/MachineLearning/comments/aij8no/d_trying_to_understand_this_es_loss_curve_failure/,patniemeyer,1548132056,,1,1,False,default,,,,,
1191,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,14,aijubn,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/aijubn/global_machine_learning_market_size_outlook/,vardhan1020,1548136674,,0,1,False,default,,,,,
1192,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,15,aik35t,sstubecn.com,shaped tube,https://www.reddit.com/r/MachineLearning/comments/aik35t/shaped_tube/,sstubecn,1548138687,,0,1,False,https://b.thumbs.redditmedia.com/X2t_un5G9B6jhVV9oBjyLAEa5CPqAkFiGFOQ5YL9xAE.jpg,,,,,
1193,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,15,aik3h9,self.MachineLearning,How do you initialize Transformer encoder for your task?,https://www.reddit.com/r/MachineLearning/comments/aik3h9/how_do_you_initialize_transformer_encoder_for/,dchatterjee172,1548138758,[removed],0,1,False,self,,,,,
1194,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,15,aik4lk,mybinder.org,Turn a Git repo into a collection of interactive notebooks,https://www.reddit.com/r/MachineLearning/comments/aik4lk/turn_a_git_repo_into_a_collection_of_interactive/,kunalag129,1548139016,,0,1,False,default,,,,,
1195,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,16,aikde3,self.MachineLearning,What should I know about building a recommendation engine for production?,https://www.reddit.com/r/MachineLearning/comments/aikde3/what_should_i_know_about_building_a/,FarisAi,1548141174,[removed],0,1,False,self,,,,,
1196,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,16,aikmza,self.MachineLearning,Has anyone re-created the wordnet graph in python using NetwrokX or a similar Graph library?,https://www.reddit.com/r/MachineLearning/comments/aikmza/has_anyone_recreated_the_wordnet_graph_in_python/,mischief_23,1548143686,[removed],0,1,False,self,,,,,
1197,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,16,aikn7a,self.MachineLearning,Liquid state machines,https://www.reddit.com/r/MachineLearning/comments/aikn7a/liquid_state_machines/,based_doggo,1548143747,[removed],0,1,False,self,,,,,
1198,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,17,aikpv7,self.MachineLearning,How do you train your language model?,https://www.reddit.com/r/MachineLearning/comments/aikpv7/how_do_you_train_your_language_model/,paarulakan,1548144413,[removed],0,1,False,self,,,,,
1199,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,17,aiks3d,self.MachineLearning,andrew ng 2nd week homework,https://www.reddit.com/r/MachineLearning/comments/aiks3d/andrew_ng_2nd_week_homework/,trikolad,1548145003,[removed],0,1,False,self,,,,,
1200,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,17,aikz5h,self.MachineLearning,[D] What is a more practical way to deploy a deep learning model in Android devices?,https://www.reddit.com/r/MachineLearning/comments/aikz5h/d_what_is_a_more_practical_way_to_deploy_a_deep/,hasannasirk,1548147059,,14,1,False,self,,,,,
1201,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,18,ail5c3,youtube.com,World's 1st Decentralized Autonomous Machine Learning Marketplace - SingularityNET,https://www.reddit.com/r/MachineLearning/comments/ail5c3/worlds_1st_decentralized_autonomous_machine/,getrich_or_diemining,1548148753,,0,1,False,https://a.thumbs.redditmedia.com/jAjuQh9oTkYxQxQmRWiArckvVLTDHgIoFxSgPUQPWx0.jpg,,,,,
1202,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,18,ail923,self.MachineLearning,"Going back to do an MSc, vs online courses and big data certifications for an aspiring ML engineer?",https://www.reddit.com/r/MachineLearning/comments/ail923/going_back_to_do_an_msc_vs_online_courses_and_big/,Aaraeus,1548149816,[removed],0,1,False,self,,,,,
1203,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,19,ailp4i,arxiv.org,[R] Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing,https://www.reddit.com/r/MachineLearning/comments/ailp4i/r_fully_convolutional_network_with_multistep/,ewanlee,1548154134,,10,1,False,default,,,,,
1204,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,20,aim045,self.MachineLearning,This Week in Machine Learning &amp; Artificial Intelligence - #221 - Self-Tuning Services via Real-Time Machine Learning with Vladimir Bychkovsky - 17th Jan 2019,https://www.reddit.com/r/MachineLearning/comments/aim045/this_week_in_machine_learning_artificial/,podcastwordcloud,1548156980,[removed],0,1,False,self,,,,,
1205,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,20,aim3e2,youtube.com,AI Donald Trump,https://www.reddit.com/r/MachineLearning/comments/aim3e2/ai_donald_trump/,hanyuqn,1548157810,,0,1,False,default,,,,,
1206,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,21,aimatt,self.MachineLearning,[P] BERT Pretrained Token Embeddings,https://www.reddit.com/r/MachineLearning/comments/aimatt/p_bert_pretrained_token_embeddings/,longinglove,1548159543,"BERT(BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding) yields pretrained token (=subword) embeddings. I extract and save them in theword2vecformat so that they can be used for downstream tasks. 

https://github.com/Kyubyong/bert-token-embeddings
",4,1,False,self,,,,,
1207,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,21,aimh9m,medium.com,GIF2Video Gives GIFs Realism,https://www.reddit.com/r/MachineLearning/comments/aimh9m/gif2video_gives_gifs_realism/,gwen0927,1548161031,,0,1,False,default,,,,,
1208,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,21,aimjbk,smarten.com,Advanced Analytics for Business Users!,https://www.reddit.com/r/MachineLearning/comments/aimjbk/advanced_analytics_for_business_users/,ElegantMicroWebIndia,1548161498,,0,1,False,default,,,,,
1209,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,22,aimpnm,self.MachineLearning,[P] Multilingual Text Processing,https://www.reddit.com/r/MachineLearning/comments/aimpnm/p_multilingual_text_processing/,longinglove,1548162868,"https://github.com/Kyubyong/mtp

WhyMulti-lingual TextProcessing?

Yes! Modeling is fancy. Data processing is tedious. You don't want to do that. I know. But from my experience it's often data processing that determines the performance of your experiement rather than modeling. If you can't avoid, it's better do it right.

WhyMulti-lingualTextProcessing?

You can obtain many techniques of image processing through many routes. More importantly, I'm not an expert in it. Let me focus on text, which is one of the two most typical modalities along with sound when handling language .

Why Multi-lingualText Processing?

If you're interested in a single language, say, English, it's fine. But if you touch a language you're not familiar with for some reason, you may need some knowledge on it.

",0,1,False,self,,,,,
1210,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,22,aimthc,deeplearningio.com,Can Artificial Intelligence Tell a Teapot From a Golf Ball? | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aimthc/can_artificial_intelligence_tell_a_teapot_from_a/,mrcgllr,1548163665,,0,1,False,https://a.thumbs.redditmedia.com/o-749g2Z9RvslcmmCFkjIzIK6C6ZmYOOHW2bTi0coB8.jpg,,,,,
1211,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,22,aimu52,self.MachineLearning,How to pass latent vector to decoder in LSTM Variational Autoencoder,https://www.reddit.com/r/MachineLearning/comments/aimu52/how_to_pass_latent_vector_to_decoder_in_lstm/,jtfidje,1548163803,[removed],0,1,False,self,,,,,
1212,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,22,aimwlb,deeplearningio.com,[R] Can Artificial Intelligence Tell a Teapot From a Golf Ball? | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aimwlb/r_can_artificial_intelligence_tell_a_teapot_from/,mrcgllr,1548164292,,0,1,False,https://a.thumbs.redditmedia.com/o-749g2Z9RvslcmmCFkjIzIK6C6ZmYOOHW2bTi0coB8.jpg,,,,,
1213,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,22,ain0dy,medium.com,How Can We Improve the Quality of Our Data?,https://www.reddit.com/r/MachineLearning/comments/ain0dy/how_can_we_improve_the_quality_of_our_data/,AnnaOnTheWeb,1548165059,,0,1,False,default,,,,,
1214,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,23,ainesv,self.MachineLearning,How to quantise a Keras model,https://www.reddit.com/r/MachineLearning/comments/ainesv/how_to_quantise_a_keras_model/,FCOS96,1548167762,"Hey all,   
I'm doing a bit of research into neural network optimisation, and I'm going to start looking at reduced precision soon. As a quick and dirty proof of concept, I just want to check whether my models performance is affected much by reducing it to 8 bit fixed point, before I move onto even lower precisions.   
I was wondering if anyone knows of any easy way to convert a keras model to 8 bit fixed point? I know it can be done through tf, and I suppose I could do it manually with a script or something, but I'm looking for a nice easy way right now, before I get into the more complicated stuff.  
Cheers",0,1,False,self,,,,,
1215,MachineLearning,t5_2r3gv,2019-1-22,2019,1,22,23,ainfaw,self.MachineLearning,[D] any papers on learning how to shape rewards ?,https://www.reddit.com/r/MachineLearning/comments/ainfaw/d_any_papers_on_learning_how_to_shape_rewards/,yazriel0,1548167854,"Manual shaped rewards are a rough approximation to the final rare/sparse/delayed reward. Is this any work on how to adjust the shaped rewards after enough episodes?

(meta-rewards? learning to reward?)

I recall the RUDDER paper, but IIRC this modified the action attributions rather than the reward itself ?!",5,1,False,self,,,,,
1216,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,0,aintkd,self.MachineLearning,machine learning with a low small sample size and extreme class unbalance + high dimensionality,https://www.reddit.com/r/MachineLearning/comments/aintkd/machine_learning_with_a_low_small_sample_size_and/,trngoon,1548170386,"Hello all,

I have a data set with just over 10,000 records and 4 classes which I can merge into a binary classification problem. The one issue is that the negative class (we will call is the 'unapproved' records), make up an extremely small number of records (only 97 on there own, and possible a couple hundred to maybe a thousand if I decide to merge some more into this class).

What is the best way to approach this problem? I know techniques for class imbalance but I would like to hear input and opinions on them too. Which algorithms might work well with this size of data on a high-level. I know knn, ensembles, and linear or non-linear svm will likely work right off the bat. And after preprocessing, I might have a large feature space (tons of nominal columns which will result in many columns and a sparse matrix once encoded). I know I can derive many, many features from some of the columns I have so k (n cols) might grow rapidly over the next few weeks. Interested on thoughts on that as well (interesting ways to deal with high dimensionality: 60 - 500 columns).

Any thoughts.",0,1,False,self,,,,,
1217,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,0,aio17z,self.MachineLearning,"[D] Is there any reference for learning ""computational speed hacks"" used in machine learning?",https://www.reddit.com/r/MachineLearning/comments/aio17z/d_is_there_any_reference_for_learning/,anonymousTestPoster,1548171714,"The obvious one is naturally the cholesky decomposition. But apart from reading people's codes and trying to back-calculate things, I find it is hard to learn about these ""computational speed hacks"".

Is there any reference which tries to compile a lot of the common ones for the common algorithms? (Naturally these can be very algorithm specific)",12,1,False,self,,,,,
1218,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,0,aio5xp,self.MachineLearning,Question about speech language detection using Dialogflow suitable for Unity,https://www.reddit.com/r/MachineLearning/comments/aio5xp/question_about_speech_language_detection_using/,cryptomaestro,1548172519,[removed],0,1,False,self,,,,,
1219,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,1,aiocdb,ai.googleblog.com,Expanding the Application of Deep Learning to Electronic Health Records,https://www.reddit.com/r/MachineLearning/comments/aiocdb/expanding_the_application_of_deep_learning_to/,sjoerdapp,1548173554,,0,1,False,default,,,,,
1220,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,1,aioft7,twitter.com,[N] DeepMind announces StarCraft 2 demonstration: Thursday 24 January 2019 at 6PM GMT (1PM ET / 10AM PT),https://www.reddit.com/r/MachineLearning/comments/aioft7/n_deepmind_announces_starcraft_2_demonstration/,gwern,1548174122,,0,1,False,https://b.thumbs.redditmedia.com/3F9LOyCEKGaRU4a7lHbaC9JAmDYiEdqWsB7ST2VX9Yw.jpg,,,,,
1221,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,1,aiojhl,self.MachineLearning,I am studying software engineering. I want to create a chat bot,https://www.reddit.com/r/MachineLearning/comments/aiojhl/i_am_studying_software_engineering_i_want_to/,Alex_Stolt,1548174729,[removed],0,1,False,self,,,,,
1222,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,2,aip2j9,self.MachineLearning,Soft Actor-Critic with Discrete Actions,https://www.reddit.com/r/MachineLearning/comments/aip2j9/soft_actorcritic_with_discrete_actions/,ultron777,1548177727,[removed],0,1,False,self,,,,,
1223,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,2,aip2xh,medium.com,Making cool diagrams using phonetics and Tensorflow,https://www.reddit.com/r/MachineLearning/comments/aip2xh/making_cool_diagrams_using_phonetics_and/,Thr3adnaught,1548177789,,0,1,False,https://a.thumbs.redditmedia.com/ekEWFyOmcdN8ApAY5eEUR4YF2eErTi9sWoo_Qg47af4.jpg,,,,,
1224,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,2,aip39x,self.MachineLearning,[D] Could you use reinforcement learning to learn/improve reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/aip39x/d_could_you_use_reinforcement_learning_to/,andyandy16,1548177845,Any examples/reference?,6,1,False,self,,,,,
1225,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,2,aip7vu,self.MachineLearning,[D] DeepMind's StarCraft II stream this Thursday at 6 PM GMT,https://www.reddit.com/r/MachineLearning/comments/aip7vu/d_deepminds_starcraft_ii_stream_this_thursday_at/,Inori,1548178561,"As DeepMind is usually very secretive about their work so if they're announcing it this way, with professional casters involved, I think this could be something big.

DeepMind announcement tweet: https://twitter.com/DeepMindAI/status/1087743023100903426  
Blizzard official post: https://news.blizzard.com/en-gb/starcraft2/22871520/deepmind-starcraft-ii-demonstration

Original SC2LE article: https://arxiv.org/abs/1708.04782  
Article with latest results: https://arxiv.org/abs/1806.01830",141,1,False,self,,,,,
1226,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,3,aipn92,cards-against-the-machine.glitch.me,Feeding Cards Against Humanity responses into a GAN trained on synthesizing images from text input,https://www.reddit.com/r/MachineLearning/comments/aipn92/feeding_cards_against_humanity_responses_into_a/,spagoop,1548180918,,0,1,False,default,,,,,
1227,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,4,aiq7v8,code.fb.com,LASER natural language processing toolkit - Facebook,https://www.reddit.com/r/MachineLearning/comments/aiq7v8/laser_natural_language_processing_toolkit_facebook/,shuklaswag,1548184110,,0,1,False,default,,,,,
1228,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,4,aiqh0d,hackernoon.com,[P] How we used ML to hybridize humans with cartoon animals and made a business out of it.,https://www.reddit.com/r/MachineLearning/comments/aiqh0d/p_how_we_used_ml_to_hybridize_humans_with_cartoon/,gabrelyanov,1548185590,,1,1,False,default,,,,,
1229,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,5,aiqrn3,self.MachineLearning,[D] Methods for Multivariate Event-based Binary Time Series Data,https://www.reddit.com/r/MachineLearning/comments/aiqrn3/d_methods_for_multivariate_eventbased_binary_time/,anderl1980,1548187231,"Which is the best method for pattern mining in multivariate event-based (binary) time series? I have lots of time series data providing error codes whether the specific error is \_on\_ (1) or \_off\_ (0). I want to mine some significant interdependencies e.g. \_always when this and this other error is on, than 2 times steps later this other error shows up\_.",3,1,False,self,,,,,
1230,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,5,air1ak,self.MachineLearning,Using Computer Vision in the Smart Home,https://www.reddit.com/r/MachineLearning/comments/air1ak/using_computer_vision_in_the_smart_home/,dahlialights,1548188711,"The huge improvement in Machine Learning, more specifically Computer Vision, has opened the door for really cool new technology. For example, with computer vision it's possible to identify people, furniture, and other common objects in a room, and people can define activities with that information. If a person is on the couch and the tv is on, that might be watching tv, but if three people are on the couch with the tv on that might be a watch party. [We've built a service that uses computer vision to understand those distinctions, define activities, and trigger your Smart Home](https://www.dahliahome.co) when those activities start and end. Not only that, it also reports back to you how much time you spend doing which activities. ",0,1,False,self,,,,,
1231,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,5,air57b,towardsdatascience.com,"Seamlessly Integrated Deep Learning Environment with Terraform, Google cloud, Gitlab and Docker",https://www.reddit.com/r/MachineLearning/comments/air57b/seamlessly_integrated_deep_learning_environment/,alexmue89,1548189316,,0,1,False,https://b.thumbs.redditmedia.com/k0ic3hJXcAq3JGMWpyU8L0f2y8EAf9LxzcxN5Gy4Tzc.jpg,,,,,
1232,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,7,aisjee,self.MachineLearning,4 Applications of Artificial Intelligence in the Food Industry,https://www.reddit.com/r/MachineLearning/comments/aisjee/4_applications_of_artificial_intelligence_in_the/,shenron6,1548196591,[removed],0,1,False,self,,,,,
1233,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,7,aisksh,self.MachineLearning,[D] Thoughts on Pattern Theory (Ulf Grenander)?,https://www.reddit.com/r/MachineLearning/comments/aisksh/d_thoughts_on_pattern_theory_ulf_grenander/,rtk25,1548196821,"I hadn't heard of it until I saw this paper:

[Generating open world descriptions of video using common sense knowledge in a pattern theory framework](https://www.ams.org/journals/qam/0000-000-00/S0033-569X-2019-01530-5/)

I'm very interested in approaches combining symbolic reasoning and perceptual processing, and I haven't seen many attempts to mathematically describe ""thought"" (whatever that is :) ) so it piqued my curiousity...

&amp;#x200B;",0,1,False,self,,,,,
1234,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,8,aisujk,cio.com.au,Fancy a game against Rafael NadAI or Robot Federer? (using Semi Supervised Generative Adversarial Network architecture),https://www.reddit.com/r/MachineLearning/comments/aisujk/fancy_a_game_against_rafael_nadai_or_robot/,GtothePtotheN,1548198348,,0,1,False,default,,,,,
1235,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,8,aisz8k,themathbehindyou.wordpress.com,[D] Create faster RNNs using NVIDIAs CuDNN with keras,https://www.reddit.com/r/MachineLearning/comments/aisz8k/d_create_faster_rnns_using_nvidias_cudnn_with/,AlexBarbera,1548199100,,0,1,False,https://b.thumbs.redditmedia.com/cxEUCuj-5Wkvk4NyOPf7PoF1dgMITpoGfW44hjwwkGw.jpg,,,,,
1236,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,8,ait0ox,self.MachineLearning,Does terminology extraction belong to feature extraction or feature engineering in NLP?,https://www.reddit.com/r/MachineLearning/comments/ait0ox/does_terminology_extraction_belong_to_feature/,ejiido,1548199335,[removed],0,1,False,self,,,,,
1237,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,8,ait49z,challenger.io,A quick introduction to BrainJS - Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/ait49z/a_quick_introduction_to_brainjs_artificial/,aaronscat,1548199908,,0,1,False,default,,,,,
1238,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,8,ait7rd,self.MachineLearning,Good websites or readings about machine learning for start up,https://www.reddit.com/r/MachineLearning/comments/ait7rd/good_websites_or_readings_about_machine_learning/,LuKeR24,1548200494,[removed],0,1,False,self,,,,,
1239,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,10,aiu1th,self.MachineLearning,Eye localization help,https://www.reddit.com/r/MachineLearning/comments/aiu1th/eye_localization_help/,saksh02,1548205533,[removed],0,1,False,self,,,,,
1240,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,10,aiu9uv,self.MachineLearning,Why Transformer use little activations?,https://www.reddit.com/r/MachineLearning/comments/aiu9uv/why_transformer_use_little_activations/,Beanocean,1548206938,"I am confused about the activation functions used in the famous NMT model Transformer. In other classical networks such as ResNet, the activation functions are used after every convolutional transformation. So a stereotype image is that the activation functions should follow after the linear functions.  But in the Transformer, I found that the activation functions are only used in Position-wise Feed-Forward Layers. In Multi-Head Attention Layers, there are only linear transformations. Can someone help to explain this? ",0,1,False,self,,,,,
1241,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,11,aiusdm,arxiv.org,[1901.06563] Consistent Optimization for Single-Shot Object Detection,https://www.reddit.com/r/MachineLearning/comments/aiusdm/190106563_consistent_optimization_for_singleshot/,taokongcn,1548210234,,1,1,False,default,,,,,
1242,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,11,aiuuvt,sstubecn.com,stainless steel forged rings,https://www.reddit.com/r/MachineLearning/comments/aiuuvt/stainless_steel_forged_rings/,sstubecn,1548210677,,0,1,False,https://b.thumbs.redditmedia.com/nU_JRkoZaQJX-pTEv1t3PugUpNMiNDxQgE9xw4kHiyQ.jpg,,,,,
1243,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,12,aivaba,self.MachineLearning,High Resolution MNIST digits made with a GAN? I remember there was a post someone made a while back.,https://www.reddit.com/r/MachineLearning/comments/aivaba/high_resolution_mnist_digits_made_with_a_gan_i/,UnfazedButDazed,1548213484,[removed],0,1,False,self,,,,,
1244,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,14,aiwbsj,self.MachineLearning,Multimodal learning in neural networks,https://www.reddit.com/r/MachineLearning/comments/aiwbsj/multimodal_learning_in_neural_networks/,lomiag,1548220768,[removed],0,1,False,self,,,,,
1245,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,14,aiwd7l,sstubecn.com,long neck flange,https://www.reddit.com/r/MachineLearning/comments/aiwd7l/long_neck_flange/,sstubecn,1548221063,,0,1,False,https://a.thumbs.redditmedia.com/XtuMa2dD7QB801u8ydeW5YrLsRt4yXg1SoybUOM6BM8.jpg,,,,,
1246,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,14,aiwf04,self.MachineLearning,Role of Machine Learning in Modern Age Agriculture,https://www.reddit.com/r/MachineLearning/comments/aiwf04/role_of_machine_learning_in_modern_age_agriculture/,Kosha_Shah,1548221437,[removed],0,1,False,self,,,,,
1247,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,14,aiwgcu,cnshanhu.com,Crushing Equipment,https://www.reddit.com/r/MachineLearning/comments/aiwgcu/crushing_equipment/,sstubecn,1548221718,,0,1,False,default,,,,,
1248,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,15,aiwris,self.MachineLearning,U-Net Introducing Symmetry in Segmentation,https://www.reddit.com/r/MachineLearning/comments/aiwris/unet_introducing_symmetry_in_segmentation/,Hsankesara,1548224130,"Just submit an article on medium describing a novel image segmentation architecture. Please check this out and leave your valuable feedback.

 [https://medium.com/@heetsankesara3/u-net-b229b32b4a71](https://medium.com/@heetsankesara3/u-net-b229b32b4a71)",0,1,False,self,,,,,
1249,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,15,aiwyup,self.MachineLearning,Automotive Alternator Market Insights and Global Industry Forecast to 2024,https://www.reddit.com/r/MachineLearning/comments/aiwyup/automotive_alternator_market_insights_and_global/,parag_bir,1548225823,[removed],1,1,False,self,,,,,
1250,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,15,aix2ws,self.MachineLearning,Namer.ai: what happens when you train a character-level RNN on a million domain names,https://www.reddit.com/r/MachineLearning/comments/aix2ws/namerai_what_happens_when_you_train_a/,Heavy_Transportation,1548226782,[removed],0,1,False,self,,,,,
1251,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,16,aix56y,reddit.com,Machine Learning Exercises in Python (Based on Andrew Ng's course),https://www.reddit.com/r/MachineLearning/comments/aix56y/machine_learning_exercises_in_python_based_on/,spagetymunster,1548227306,,0,1,False,default,,,,,
1252,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,16,aix95k,self.MachineLearning,"What is the ""hello world"" equivalent program in Machine Learning?",https://www.reddit.com/r/MachineLearning/comments/aix95k/what_is_the_hello_world_equivalent_program_in/,ab_heisenberg,1548228287,[removed],0,1,False,self,,,,,
1253,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,16,aixbkt,code-brew.com,Learn the role of AI and Machine Learning in eCommerce,https://www.reddit.com/r/MachineLearning/comments/aixbkt/learn_the_role_of_ai_and_machine_learning_in/,hitesh_patiyal,1548228906,,0,1,False,https://a.thumbs.redditmedia.com/DsEHRrP0PKyL6D8m2j9eCSWMd0Dztr2rsmJ6ZdmnCF4.jpg,,,,,
1254,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,16,aixfow,youtube.com,Amazing Roof Pressure Washing Compilation : BLACK TO ORIGINAL,https://www.reddit.com/r/MachineLearning/comments/aixfow/amazing_roof_pressure_washing_compilation_black/,chandreshvshah,1548229973,,0,1,False,default,,,,,
1255,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,17,aixmw6,blog.linknovate.com,Ranking: Who's winning the race in Quantum Computing?,https://www.reddit.com/r/MachineLearning/comments/aixmw6/ranking_whos_winning_the_race_in_quantum_computing/,sabucks_,1548231883,,0,1,False,https://b.thumbs.redditmedia.com/WIlxtEPq8rXkcBlK0imPD-dc3K9PAaedhoDPuY5PWKI.jpg,,,,,
1256,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,17,aixrkx,self.MachineLearning,[P] News sentences unconditionally generated by Transformer,https://www.reddit.com/r/MachineLearning/comments/aixrkx/p_news_sentences_unconditionally_generated_by/,HigherTopoi,1548233231,"Some artificial news sentences generated by my neural net model about 2 months ago. Each sentence was unconditionally generated, and there's no dependence between sentences. Local coherence is good, but global coherence is often lacking.

&amp;#x200B;

what this group does is to take down various different websites it believes to be criminal and leading to terrorist acts . 

 over 1 , 600 a day have reached greece this month , a higher rate than last july when the crisis was already in full swing . 

 \`\` we ' re working through a legacy period , with legacy products that are 10 or 20 years old , '' he says . 

 ' the first time anyone says you need help , i ' m on the defensive , but that ' s all that i know . 

 out of those who came last year , 69 per cent were men , 18 per cent were children and just 13 per cent were women . 

 he has not played for tottenham ' s first team since and it is now nearly two years since he completed a full premier league match for the club . 

 so you have this man who seems to represent this way to live and how to be a good citizen of the world . 

 cnn : you made that promise , but it wasn ' t until 45 years later that you acted on it . 

london mayor boris johnson ' s comments came after a prominent campaign of britain ' s departure from the european union .

its revenue was up 3 . 9 percent in the first quarter of 2015 , which was down 2 . 5 percent in 2014 .

but it ' s not just a case of putting pressure on the european court of justice , it ' s a big decision for our country to accept .

democratic presidential candidate bernie sanders has made it clear that he is willing to spend the money on health care , and to campaign on the campaign trail . '' he said .

but in the end , they ' ve got to learn from each other ' s perspective , and they ' re going to be honest .

according to the sun , it ' s not a great game for him - but i think he is good enough to go out there and not be a good guy . '' he says .

his comments on tuesday night came after trump made the comments on the republican side of the debate , saying he was \`\` willing to compromise \`\` from a fellow businessman who said he had done nothing .

\`\` i ' m so proud of the game in this match , so that we have to be honest , '' he said .

he said : \`\` it ' s important that we do a lot of good work to do here , we ' ve already got a great future .

steve king , the former mayor of new york , also the former florida governor ' s campaign in new hampshire , has attracted overwhelming support from the party establishment and the republican - controlled congress .

yet it ' s hard for me to imagine how my dad would be in the future , how i ' d make him feel .

i think that we will be better off to the best of the country and we will see that in any future , '' he said .",6,1,False,self,,,,,
1257,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,18,aixwyt,self.MachineLearning,Any good tutorials on configuring an openAI environment?,https://www.reddit.com/r/MachineLearning/comments/aixwyt/any_good_tutorials_on_configuring_an_openai/,an0nym0usv,1548234744,[removed],0,1,False,self,,,,,
1258,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,19,aiy8cc,arxiv.org,A function fitting method with relevant theorems and proofs.,https://www.reddit.com/r/MachineLearning/comments/aiy8cc/a_function_fitting_method_with_relevant_theorems/,rajesh_d24,1548237782,,2,1,False,default,,,,,
1259,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,19,aiyji4,arxiv.org,[1901.07441] PadChest: A large chest x-ray image dataset with multi-label annotated reports,https://www.reddit.com/r/MachineLearning/comments/aiyji4/190107441_padchest_a_large_chest_xray_image/,ihaphleas,1548240738,,4,1,False,default,,,,,
1260,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,19,aiyjt0,arxiv.org,[1901.06553] Neuroflight: Next Generation Flight Control Firmware,https://www.reddit.com/r/MachineLearning/comments/aiyjt0/190106553_neuroflight_next_generation_flight/,ihaphleas,1548240809,,1,1,False,default,,,,,
1261,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,20,aiyrfq,self.MachineLearning,[D] When was a time ML/DS was incorrectly applied at your job? What happened and what did you learn from it?,https://www.reddit.com/r/MachineLearning/comments/aiyrfq/d_when_was_a_time_mlds_was_incorrectly_applied_at/,sugarhilldt2,1548242662,Was it as bad as this guy? https://stats.stackexchange.com/questions/185507/what-happens-if-the-explanatory-and-response-variables-are-sorted-independently,0,1,False,self,,,,,
1262,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,20,aiyrv6,self.MachineLearning,[D] What was a time ML/DS was incorrectly used at your job? What happened and what did you learn from it?,https://www.reddit.com/r/MachineLearning/comments/aiyrv6/d_what_was_a_time_mlds_was_incorrectly_used_at/,sugarhilldt2,1548242768,Was it as bad as this guy? https://stats.stackexchange.com/questions/185507/what-happens-if-the-explanatory-and-response-variables-are-sorted-independently,92,1,False,self,,,,,
1263,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,20,aiytj5,self.MachineLearning,Growing opportunity for startups in the global Geothermal Power industry,https://www.reddit.com/r/MachineLearning/comments/aiytj5/growing_opportunity_for_startups_in_the_global/,apple_x9,1548243171,[removed],1,1,False,self,,,,,
1264,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,20,aiyvym,self.MachineLearning,Dairy Processing Equipment market size to expand significantly by the end of 2022,https://www.reddit.com/r/MachineLearning/comments/aiyvym/dairy_processing_equipment_market_size_to_expand/,apple_x9,1548243768,[removed],1,1,False,self,,,,,
1265,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,20,aiyxhx,self.MachineLearning,Fast.ai keras or pytorch for a newb?,https://www.reddit.com/r/MachineLearning/comments/aiyxhx/fastai_keras_or_pytorch_for_a_newb/,frosty3907,1548244131,[removed],0,1,False,self,,,,,
1266,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aiz3k2,smarten.com,Clickless Analytics &amp; NLP Search Analytics for Business Users!,https://www.reddit.com/r/MachineLearning/comments/aiz3k2/clickless_analytics_nlp_search_analytics_for/,ElegantMicroWebIndia,1548245567,,0,1,False,default,,,,,
1267,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aiz3r7,self.MachineLearning,Trying to make a DDPG network for discrete action space....,https://www.reddit.com/r/MachineLearning/comments/aiz3r7/trying_to_make_a_ddpg_network_for_discrete_action/,Jandevries101,1548245612,[removed],0,1,False,self,,,,,
1268,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aiz4mk,self.MachineLearning,Lacrosse Equipment market projected to reach massively till 2024,https://www.reddit.com/r/MachineLearning/comments/aiz4mk/lacrosse_equipment_market_projected_to_reach/,apple_x9,1548245811,[removed],1,1,False,self,,,,,
1269,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aiz77b,deeplearningio.com,[N] Smarter AI: Machine Learning without Negative Data | Deep Learning IO,https://www.reddit.com/r/MachineLearning/comments/aiz77b/n_smarter_ai_machine_learning_without_negative/,mrcgllr,1548246388,,0,1,False,https://b.thumbs.redditmedia.com/Ek0If9o7hqPKdn_wQ7-mPnd4ybkS-w88xuhQs5AJ-Tg.jpg,,,,,
1270,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aizcpi,self.MachineLearning,Gas-Fired Boiler market projected to reach massively till 2024,https://www.reddit.com/r/MachineLearning/comments/aizcpi/gasfired_boiler_market_projected_to_reach/,apple_x9,1548247586,[removed],1,1,False,self,,,,,
1271,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aizdmx,self.MachineLearning,Neural Network Calculator,https://www.reddit.com/r/MachineLearning/comments/aizdmx/neural_network_calculator/,rompear,1548247795,[removed],0,1,False,self,,,,,
1272,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aizf27,self.MachineLearning,"What is the potentially recent research in electronic health record by using Machine Learning, Deep Learning, Natural Language processing and Bayesian Model?",https://www.reddit.com/r/MachineLearning/comments/aizf27/what_is_the_potentially_recent_research_in/,tridungduong16,1548248099,[removed],0,1,False,self,,,,,
1273,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,21,aizg4g,self.MachineLearning,Radiation Detector market projected to reach massively till 2024,https://www.reddit.com/r/MachineLearning/comments/aizg4g/radiation_detector_market_projected_to_reach/,apple_x9,1548248327,[removed],1,1,False,self,,,,,
1274,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,22,aizkm5,self.MachineLearning,Wireless Light Switches market projected to reach massively till 2024,https://www.reddit.com/r/MachineLearning/comments/aizkm5/wireless_light_switches_market_projected_to_reach/,apple_x9,1548249232,[removed],1,1,False,self,,,,,
1275,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,22,aizydp,self.MachineLearning,Any articles about saturating activation functions?,https://www.reddit.com/r/MachineLearning/comments/aizydp/any_articles_about_saturating_activation_functions/,Jandevries101,1548251956,"Hi,

&amp;#x200B;

i got a lot of trouble with avoiding saturated/desaturating my activation functions, but i can't find any real article about it that goes indepth over the problem and solutions etc, but it is a known issue? 

&amp;#x200B;

can somebody send me some articles related to the topic?

&amp;#x200B;

Thanks,

&amp;#x200B;

Jan",0,1,False,self,,,,,
1276,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,23,aizymg,self.MachineLearning,Vacuum Sensors market projected to reach massively till 2024,https://www.reddit.com/r/MachineLearning/comments/aizymg/vacuum_sensors_market_projected_to_reach/,apple_x9,1548252008,[removed],1,1,False,self,,,,,
1277,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,23,aj028u,self.MachineLearning,Rehabilitation Equipment market projected to reach massively till 2024,https://www.reddit.com/r/MachineLearning/comments/aj028u/rehabilitation_equipment_market_projected_to/,apple_x9,1548252653,[removed],1,1,False,self,,,,,
1278,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,23,aj02jm,johnwittenauer.net,Deep Learning With Keras: Structured Time Series,https://www.reddit.com/r/MachineLearning/comments/aj02jm/deep_learning_with_keras_structured_time_series/,Aakankshaws,1548252706,,1,1,False,default,,,,,
1279,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,23,aj065n,self.MachineLearning,Building your first chatbot to search restaurants on Yelp and connecting it with Slack,https://www.reddit.com/r/MachineLearning/comments/aj065n/building_your_first_chatbot_to_search_restaurants/,conversational-ai,1548253383,[removed],0,1,False,https://a.thumbs.redditmedia.com/b7s_CELPBOI-VJIYb1CofnDDjMioK5oWzVxeIRJ-WI0.jpg,,,,,
1280,MachineLearning,t5_2r3gv,2019-1-23,2019,1,23,23,aj0cvi,self.MachineLearning,Do any frameworks allow for updating a simple knn model when new data becomes available?,https://www.reddit.com/r/MachineLearning/comments/aj0cvi/do_any_frameworks_allow_for_updating_a_simple_knn/,AfgCric,1548254609,[removed],0,1,False,self,,,,,
1281,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,0,aj0osi,self.MachineLearning,Doc2vecC predicting vectors for unseen documents,https://www.reddit.com/r/MachineLearning/comments/aj0osi/doc2vecc_predicting_vectors_for_unseen_documents/,jgtjerry,1548256659,[removed],0,1,False,self,,,,,
1282,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,0,aj0s2b,youtube.com,Generating Cars with Generative Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/aj0s2b/generating_cars_with_generative_adversarial_nets/,porygon93,1548257231,,2,1,False,default,,,,,
1283,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,0,aj0sm5,self.MachineLearning,Why we always assume noise as having gaussian distribution and not having other distribution?Also what will be the consequences if we use other distribution.,https://www.reddit.com/r/MachineLearning/comments/aj0sm5/why_we_always_assume_noise_as_having_gaussian/,yash_8141,1548257322,[removed],0,1,False,self,,,,,
1284,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,0,aj0t4q,medium.com,GIF2Video Gives GIFs Realism,https://www.reddit.com/r/MachineLearning/comments/aj0t4q/gif2video_gives_gifs_realism/,Yuqing7,1548257408,,0,1,False,default,,,,,
1285,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,0,aj1027,ashutoshtripathi.com,What is the Coefficient of Determination | R Square,https://www.reddit.com/r/MachineLearning/comments/aj1027/what_is_the_coefficient_of_determination_r_square/,imashutoshtr,1548258532,,0,1,False,default,,,,,
1286,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,0,aj11gu,self.MachineLearning,"Simple Questions Thread January 23, 2019",https://www.reddit.com/r/MachineLearning/comments/aj11gu/simple_questions_thread_january_23_2019/,AutoModerator,1548258764,[removed],0,1,False,self,,,,,
1287,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,1,aj1834,self.MachineLearning,[R] [ICLR 2019] Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks,https://www.reddit.com/r/MachineLearning/comments/aj1834/r_iclr_2019_accumulation_bitwidth_scaling_for/,fixed-point-learning,1548259770,"Sharing my newly accepted ICLR 2019 paper: [https://openreview.net/forum?id=BklMjsRqY7](https://openreview.net/forum?id=BklMjsRqY7)

Also posted on arXiv: [https://arxiv.org/abs/1901.06588](https://arxiv.org/abs/1901.06588)

&amp;#x200B;

**Abstract:** Efforts to reduce the numerical precision of computations in deep learning training have yielded systems that aggressively quantize weights and activations, yet employ wide high-precision accumulators for partial sums in inner-product operations to preserve the quality of convergence. The absence of any framework to analyze the precision requirements of partial sum accumulations results in conservative design choices. This imposes an upper-bound on the reduction of complexity of multiply-accumulate units. We present a statistical approach to analyze the impact of reduced accumulation precision on deep learning training. Observing that a bad choice for accumulation precision results in loss of information that manifests itself as a reduction in variance in an ensemble of partial sums, we derive a set of equations that relate this variance to the length of accumulation and the minimum number of bits needed for accumulation. We apply our analysis to three benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet AlexNet. In each case, with accumulation precision set in accordance with our proposed equations, the networks successfully converge to the single precision floating-point baseline. We also show that reducing accumulation precision further degrades the quality of the trained network, proving that our equations produce tight bounds. Overall this analysis enables precise tailoring of computation hardware to the application, yielding area- and power-optimal systems.

 

**TL;DR:** We present an analytical framework to determine accumulation bit-width requirements in all three deep learning training GEMMs and verify the validity and tightness of our method via benchmarking experiments.",1,1,False,self,,,,,
1288,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,1,aj1a1h,self.MachineLearning,ICML last minute submission questions,https://www.reddit.com/r/MachineLearning/comments/aj1a1h/icml_last_minute_submission_questions/,totallynotAGI,1548260074,[removed],0,1,False,self,,,,,
1289,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,1,aj1ddy,julialang.org,DiffEqFlux.jl  A Julia Library for Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/aj1ddy/diffeqfluxjl_a_julia_library_for_neural/,ChrisRackauckas,1548260602,,0,1,False,https://b.thumbs.redditmedia.com/w8jsZyWk9b2kgz6eBrar2PF7vHo8HJo9YwWQW2jWerw.jpg,,,,,
1290,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,2,aj1war,self.MachineLearning,Machine Learning Exercises in Python (Based on Andrew Ng's course),https://www.reddit.com/r/MachineLearning/comments/aj1war/machine_learning_exercises_in_python_based_on/,spagetymunster,1548263422,[removed],1,1,False,self,,,,,
1291,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,2,aj216h,julialang.org,[D] DiffEqFlux.jl  A Julia Library for Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/aj216h/d_diffeqfluxjl_a_julia_library_for_neural/,one_more_minute,1548264182,,0,1,False,default,,,,,
1292,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,2,aj2697,stateoftheart.ai,[P] State of the art in AI,https://www.reddit.com/r/MachineLearning/comments/aj2697/p_state_of_the_art_in_ai/,iamaaditya,1548264953,,0,1,False,default,,,,,
1293,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,2,aj2cmz,self.MachineLearning,[R] Tensorflow nightly now has an 2.0.0.rc0 version PyPI,https://www.reddit.com/r/MachineLearning/comments/aj2cmz/r_tensorflow_nightly_now_has_an_200rc0_version/,bbsome,1548265911,"By some luck I've just noticed that today they have update the review version PyPI of the tensorflow 2.0 nightly preview. The version name indicates that maybe this is somewhat closer to release version than the previous ones. 

&amp;#x200B;

Link to history: [https://pypi.org/project/tf-nightly-2.0-preview/#history](https://pypi.org/project/tf-nightly-2.0-preview/#history)",0,1,False,self,,,,,
1294,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,3,aj2j4u,ai.googleblog.com,Natural Questions: a New Corpus and Challenge for Question Answering Research,https://www.reddit.com/r/MachineLearning/comments/aj2j4u/natural_questions_a_new_corpus_and_challenge_for/,sjoerdapp,1548266871,,0,1,False,https://b.thumbs.redditmedia.com/rArMS5wEMBwn_pUoYecfFdBWIdUNQKIHJvx3u7vixoY.jpg,,,,,
1295,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,3,aj2lhi,medium.com,Game On! DeepMind Vs StarCraft II,https://www.reddit.com/r/MachineLearning/comments/aj2lhi/game_on_deepmind_vs_starcraft_ii/,Yuqing7,1548267223,,0,1,False,https://a.thumbs.redditmedia.com/BCd1O5vXxZEuRl33sBERdxkFrzTOTDDE7qmlvKTpwE0.jpg,,,,,
1296,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,3,aj2s7o,self.MachineLearning,Looking for freelance Machine learning experts who also enjoy teaching others,https://www.reddit.com/r/MachineLearning/comments/aj2s7o/looking_for_freelance_machine_learning_experts/,kyledevyay,1548268253,"Hello there,

Wondering if there are Machine Learning experts in here who also have some training/teaching/presenting experience.

I work for a company called DevelopIntelligence that is based in Boulder, Colorado. We give hands-on DevOps training courses (generally 3-4 days each at client sites) to software developer teams (generally mid-level and senior devs) at companies like Intuit, Starbucks, and Salesforce. The courses are generally 40% lecture, 40% labs/coding, and 20% Q&amp;A.

Email me [kyle@developintelligence.com](mailto:kyle@developintelligence.com) if you're interested in learning more. Thx.",0,1,False,self,,,,,
1297,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,3,aj2tix,self.MachineLearning,Dropout via gradient descent?,https://www.reddit.com/r/MachineLearning/comments/aj2tix/dropout_via_gradient_descent/,weelamb,1548268440,[removed],0,1,False,self,,,,,
1298,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,3,aj2vnq,medium.com,You Cant Keep an RL-Powered ANYmal Down,https://www.reddit.com/r/MachineLearning/comments/aj2vnq/you_cant_keep_an_rlpowered_anymal_down/,Yuqing7,1548268774,,0,1,False,default,,,,,
1299,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,3,aj2wfq,julialang.org,DiffEqFlux.jl  A Julia Library for Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/aj2wfq/diffeqfluxjl_a_julia_library_for_neural/,Bdamkin54,1548268885,,0,1,False,https://b.thumbs.redditmedia.com/w8jsZyWk9b2kgz6eBrar2PF7vHo8HJo9YwWQW2jWerw.jpg,,,,,
1300,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,3,aj2wrt,self.MachineLearning,[D] What is the state of the art for using a pretrained model when there's only a one new class that the pretrained model hasn't seen?,https://www.reddit.com/r/MachineLearning/comments/aj2wrt/d_what_is_the_state_of_the_art_for_using_a/,buy_some_wow,1548268941,"Here's a concrete problem:

[Auto rickshaw](https://en.m.wikipedia.org/wiki/Auto_rickshaw) is a three wheel vehicle popular in some parts of the world, which I believe no popular pretrained model has seen it during the training. If we are to build a vehicle classifier using a pretrained model that also identify auto rickshaws, how'd you suggest doing so? (Assume that the pretrained model is capable of identifying all the other vehicle classes). 

Here is what I think of the obvious transfer learning approach:

* Build a dataset that has all the classes of vehicles including auto rickshaws. 
* Train the classifier of the pretrained model using the collected data set.

I see this approach has some redundant training part:  we are training a whole new classifier just for it to give the ability to identify auto rickshaws while the original model had the ability of identifying all the other vehicles.

Just wondering if there are any shortcuts for this  kind of problems. ",9,1,False,self,,,,,
1301,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,4,aj36pg,blog.floydhub.com,[N] Inverting Facial Recognition Models,https://www.reddit.com/r/MachineLearning/comments/aj36pg/n_inverting_facial_recognition_models/,pirate7777777,1548270422,,0,1,False,https://b.thumbs.redditmedia.com/1RC9ngTV_Mazj07VcwWJvvFb9YcUqe03HtDt2vi3pEg.jpg,,,,,
1302,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,5,aj3wbv,self.MachineLearning,TeleGrad - A Telegram bot to monitor and control deep learning experiments,https://www.reddit.com/r/MachineLearning/comments/aj3wbv/telegrad_a_telegram_bot_to_monitor_and_control/,eyalzk,1548274250,"Hi guys!  
DL training processes can run for many hours/days, and you are not always near your station to see how it's progressing or to make adjustments.

Using this Telegram bot you can seamlessly get constant updates and even control your training process, all through your phone. 

Works with both Keras and Tensorflow (with Keras, all you need is to include a dedicated callback).

Using this bot, you can:  
\- Get per epoch updates on the loss, accuracy etc.  
\- Change the learning rate  
\- Get loss convergence plots  
\- Kill the training process  
\- Query the latest LR or metrics  
\- Limit access to a specific Telegram user id

This is a first working version, I would be happy to get some feedback and ideas for improvements.  
Also, if someone wants to add a Pytorch example, PRs are welcome :)

Repo:  
[https://github.com/eyalzk/dlbot](https://github.com/eyalzk/dlbot?fbclid=IwAR1eeknXCXfc0rlwoR-k1slg5xs0vdI54VPF8LwVx4xzTeP0JPDqS59YOA0)

For more projects:  
[https://eyalzk.github.io/](https://l.facebook.com/l.php?u=https%3A%2F%2Feyalzk.github.io%2F%3Ffbclid%3DIwAR1gdpLwhh-RqpGxpFl-UtROAuKvW9yeyt-3q36tNeFBjZjwWO5ir1EnI0M&amp;h=AT1cdnfGV7kjpf7Y-R5MaWbqEf3h9LZQfSxg0J0oeGRROTdERhFdWQKHYdlreIPFJR07h9esb0ZYXfst7ECyb4SnvwyHFMPagApBSA3tBlyAdsy5WTpm75n2eGUb6X5qWCCNbAPXI14nmjkAq4A)",0,1,False,self,,,,,
1303,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,5,aj407r,self.MachineLearning,[P] TeleGrad - A Telegram bot to monitor and control deep learning experiments,https://www.reddit.com/r/MachineLearning/comments/aj407r/p_telegrad_a_telegram_bot_to_monitor_and_control/,eyalzk,1548274848,"DL training processes can run for many hours/days, and you are not always near your station to see how it's progressing or to make adjustments.

Using this Telegram bot you can seamlessly get constant updates and even control your training process, all through your phone.

Works with both Keras and Tensorflow (with Keras, all you need is to include a dedicated callback).

 Using this bot, you can:  
\- Get per epoch updates on the loss, accuracy etc.  
\- Change the learning rate  
\- Get loss convergence plots  
\- Kill the training process  
\- Query the latest LR or metrics  
\- Limit access to a specific Telegram user id

This is a first working version, I would be happy to get some feedback and ideas for improvements. 

Also, if someone is willing to add a Pytorch example, PRs are welcome :)

Repo:  
[https://github.com/eyalzk/dlbot](https://github.com/eyalzk/dlbot?fbclid=IwAR1eeknXCXfc0rlwoR-k1slg5xs0vdI54VPF8LwVx4xzTeP0JPDqS59YOA0)

For more projects:  
[https://eyalzk.github.io/](https://l.facebook.com/l.php?u=https%3A%2F%2Feyalzk.github.io%2F%3Ffbclid%3DIwAR1gdpLwhh-RqpGxpFl-UtROAuKvW9yeyt-3q36tNeFBjZjwWO5ir1EnI0M&amp;h=AT1cdnfGV7kjpf7Y-R5MaWbqEf3h9LZQfSxg0J0oeGRROTdERhFdWQKHYdlreIPFJR07h9esb0ZYXfst7ECyb4SnvwyHFMPagApBSA3tBlyAdsy5WTpm75n2eGUb6X5qWCCNbAPXI14nmjkAq4A)",6,1,False,self,,,,,
1304,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,5,aj41fx,sciencetrends.com,Digitizing Diagnoses With Histopathological Slides,https://www.reddit.com/r/MachineLearning/comments/aj41fx/digitizing_diagnoses_with_histopathological_slides/,CuttingWithScissors,1548275037,,0,1,False,default,,,,,
1305,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,5,aj41mo,self.MachineLearning,[D] Modern model for text classification?,https://www.reddit.com/r/MachineLearning/comments/aj41mo/d_modern_model_for_text_classification/,hadaev,1548275065,"Im currently just on embeding+gre+dens, may be where is something better?",15,1,False,self,,,,,
1306,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,6,aj4gz8,self.MachineLearning,[P] DiffEqFlux.jl  A Julia Library for Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/aj4gz8/p_diffeqfluxjl_a_julia_library_for_neural/,skariel,1548277380,An implementation of the neural differential equation in Julia. See a detailed blog post [here](https://julialang.org/blog/2019/01/fluxdiffeq),14,1,False,self,,,,,
1307,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,6,aj4iwf,self.MachineLearning,AdaIN makes gradient penalty in WGAN-GP unbounded?,https://www.reddit.com/r/MachineLearning/comments/aj4iwf/adain_makes_gradient_penalty_in_wgangp_unbounded/,GiantAcronym,1548277666,"I am following the design of the paper ""[A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/pdf/1812.04948.pdf)"". After I change the previous Generator to the AdaIN type Generator, the gradient penalty value becomes like 10\^10 after few steps of iteration. This even happens in very shallow stage during training following the ""progressively growing of GAN"". My previous WGAN-GP behaves normally and the gradient penalty value converges to nearly zero and never exceed 10. 

Do anyone have the same problem? Any guess why? ",0,1,False,self,,,,,
1308,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,6,aj4x3v,self.MachineLearning,limits of deep learning,https://www.reddit.com/r/MachineLearning/comments/aj4x3v/limits_of_deep_learning/,disdi89,1548279815,[removed],0,1,False,self,,,,,
1309,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,6,aj514t,self.AskAcademia,[R] Where can I find research projects to contribute to while I keep my FT job?,https://www.reddit.com/r/MachineLearning/comments/aj514t/r_where_can_i_find_research_projects_to/,haywire12,1548280416,,0,1,False,https://a.thumbs.redditmedia.com/O9hPEZrG7QTgInLOULPX3ZxXCmt7vXxwC7-agy5bfM0.jpg,,,,,
1310,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,6,aj530l,self.MachineLearning,[DISCUSSION] How AI and machine learning can transform agriculture and help tackle hunger,https://www.reddit.com/r/MachineLearning/comments/aj530l/discussion_how_ai_and_machine_learning_can/,Charlie_Ensor,1548280696,https://farmingfirst.org/digital-revolution-agriculture-leave-no-one-behind,0,1,False,self,,,,,
1311,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,6,aj5310,self.MachineLearning,Where do I start with deep learning?,https://www.reddit.com/r/MachineLearning/comments/aj5310/where_do_i_start_with_deep_learning/,mcwave123,1548280698,,0,1,False,self,,,,,
1312,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,7,aj56pg,stanfordmlgroup.github.io,"Introducing CheXpert and MIMIC-CXR datasets: ~600,000 labeled chest X-ray images in a joint release between Stanford and MIT",https://www.reddit.com/r/MachineLearning/comments/aj56pg/introducing_chexpert_and_mimiccxr_datasets_600000/,jdiggitydawg16,1548281253,,0,1,False,https://a.thumbs.redditmedia.com/ZHcpF0wQtazRUQs6rpAV8NgrKz_bc7SJCaWh-tzBia0.jpg,,,,,
1313,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,7,aj5cyu,stanfordmlgroup.github.io,"[R] Introducing CheXpert and MIMIC-CXR datasets: ~600,000 labeled chest X-ray images in a joint release between Stanford and MIT",https://www.reddit.com/r/MachineLearning/comments/aj5cyu/r_introducing_chexpert_and_mimiccxr_datasets/,jdiggitydawg16,1548282271,,0,1,False,https://a.thumbs.redditmedia.com/ZHcpF0wQtazRUQs6rpAV8NgrKz_bc7SJCaWh-tzBia0.jpg,,,,,
1314,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,8,aj64ss,julialang.org,Thought yall might likey,https://www.reddit.com/r/MachineLearning/comments/aj64ss/thought_yall_might_likey/,Chased1k,1548286840,,0,1,False,https://b.thumbs.redditmedia.com/w8jsZyWk9b2kgz6eBrar2PF7vHo8HJo9YwWQW2jWerw.jpg,,,,,
1315,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,8,aj69ol,self.MachineLearning,[D] Melody Generation with midi and deep learning (and maybe GANs?),https://www.reddit.com/r/MachineLearning/comments/aj69ol/d_melody_generation_with_midi_and_deep_learning/,GayColangelo,1548287722,"I can't find it, but there was an interesting midi sound library that I would've thought for sure someone would've used to make a melody maker AI before. Is this type of thing possible, if anyone has done it I'd really love it if you could link me to some code.",11,1,False,self,,,,,
1316,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,8,aj6b7e,self.MachineLearning,Opensource library that takes free text and guesses event details in the text,https://www.reddit.com/r/MachineLearning/comments/aj6b7e/opensource_library_that_takes_free_text_and/,EconomyWalrus,1548287999,"I am trying to write some code that reads email messages and see if there is some event like information and automatically create an event (.ics file). Is there any opensource library that will help? Something that doesn't need any training data. Preferably python. And would be great if the library can learn if the user corrects the information in the event.

Example:

    
    An evening of celebration blah blah blah 
    SUNDAY
    JANUARY 20TH 2019
     
    TIME: 3:00 P.M. TO 7:00 P.M.
     
    VENUE: Blah Blah Church Hall
    452 Blah Road, BlahCity, NY
    
    ADMISSION:  Free. All are welcome

&amp;#x200B;",0,1,False,self,,,,,
1317,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,9,aj6kw8,self.MachineLearning,[D] Number of ICML submissions today,https://www.reddit.com/r/MachineLearning/comments/aj6kw8/d_number_of_icml_submissions_today/,anonDogeLover,1548289698,"What's the highest paper ID number you've seen? 1,100 is the one to beat.",13,1,False,self,,,,,
1318,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,9,aj6oqp,self.MachineLearning,[D] Interview questions for a NLP job?,https://www.reddit.com/r/MachineLearning/comments/aj6oqp/d_interview_questions_for_a_nlp_job/,Marthinwurer,1548290368,"Tomorrow I'm going to be interviewing candidates for a job involving ML and NLP. I didn't know I was doing this until today. I have a bit of a background in ML from lurking here, but I've only really done stuff with RL and image classifiers. What are some good interview questions to ask or resources that I can read to get up to speed so that I can make sure that my candidates know what they're doing?",11,1,False,self,,,,,
1319,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,10,aj79zn,self.MachineLearning,Difference between conditional GANs and training mutliple GANs?,https://www.reddit.com/r/MachineLearning/comments/aj79zn/difference_between_conditional_gans_and_training/,iamstillsleeping,1548294173,[removed],0,1,False,self,,,,,
1320,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,10,aj7fkl,self.MachineLearning,need some advice regarding assemble PC for deep learning,https://www.reddit.com/r/MachineLearning/comments/aj7fkl/need_some_advice_regarding_assemble_pc_for_deep/,vinnitsky,1548295183,[removed],0,1,False,self,,,,,
1321,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,11,aj7odi,self.MachineLearning,"[R] Introducing CheXpert and MIMIC-CXR datasets: ~600,000 labeled chest X-ray images in a joint release between Stanford and MIT",https://www.reddit.com/r/MachineLearning/comments/aj7odi/r_introducing_chexpert_and_mimiccxr_datasets/,jdiggitydawg16,1548296834,"Two massive chest X-ray datasets were jointly released today by Stanford and MIT.

CheXpert (Stanford)

* \~220,000 images
* Website/Data Access: [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)
* AAAI paper: [https://arxiv.org/abs/1901.07031](https://arxiv.org/abs/1901.07031)

MIMIC-CXR (MIT)

* \~370,000 images
* Data Access: [https://www.physionet.org/physiobank/database/mimiccxr/](https://www.physionet.org/physiobank/database/mimiccxr/)
* arXiv paper: [https://arxiv.org/abs/1901.07042](https://arxiv.org/abs/1901.07042)",14,1,False,self,,,,,
1322,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,11,aj7vtf,arxiv.org,"[R] OpenAI became the first author of ""Learning Dexterous In-Hand Manipulation""",https://www.reddit.com/r/MachineLearning/comments/aj7vtf/r_openai_became_the_first_author_of_learning/,downtownslim,1548298229,,29,1,False,default,,,,,
1323,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,11,aj7x0q,self.MachineLearning,How convolution and pooling act as a strong prior?,https://www.reddit.com/r/MachineLearning/comments/aj7x0q/how_convolution_and_pooling_act_as_a_strong_prior/,PyWarrior,1548298447,I was reading Adaptive computation and machine learning and I have a doubt with the text stating that convolution and pooling act as a strong prior. Can anyone describe in detail how?,0,1,False,self,,,,,
1324,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,12,aj83sp,self.MachineLearning,"Hi, can anybody help me to start learning ML I'm a beginner",https://www.reddit.com/r/MachineLearning/comments/aj83sp/hi_can_anybody_help_me_to_start_learning_ml_im_a/,7vik_ja9,1548299690,[removed],0,1,False,self,,,,,
1325,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,12,aj86h7,metmac.com,Hydraulic straightening machine learning,https://www.reddit.com/r/MachineLearning/comments/aj86h7/hydraulic_straightening_machine_learning/,jumitop,1548300201,,0,1,False,default,,,,,
1326,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,12,aj8818,ai.googleblog.com,Google AI releases 300k open-domain question answering corpus + challenge,https://www.reddit.com/r/MachineLearning/comments/aj8818/google_ai_releases_300k_opendomain_question/,aviniumau,1548300498,,0,1,False,https://b.thumbs.redditmedia.com/rArMS5wEMBwn_pUoYecfFdBWIdUNQKIHJvx3u7vixoY.jpg,,,,,
1327,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,14,aj90px,self.MachineLearning,bbox - Python library for bounding boxes,https://www.reddit.com/r/MachineLearning/comments/aj90px/bbox_python_library_for_bounding_boxes/,varunagrawal,1548306238,[removed],0,1,False,self,,,,,
1328,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,14,aj97dr,self.MachineLearning,[D] Is there any example code of Neural Process in classification task?,https://www.reddit.com/r/MachineLearning/comments/aj97dr/d_is_there_any_example_code_of_neural_process_in/,sjh9020,1548307620,"In the neural process [https://arxiv.org/pdf/1807.01622.pdf](https://arxiv.org/pdf/1807.01622.pdf)

the author says np can be modified for the classification task.

But I can't find examples for it. Do you guys know one?",1,1,False,self,,,,,
1329,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,15,aj9i9s,dev.to,10 Innovative Machine Learning APIs You Should Learn in 2019,https://www.reddit.com/r/MachineLearning/comments/aj9i9s/10_innovative_machine_learning_apis_you_should/,KiranKiller,1548310022,,0,1,False,default,,,,,
1330,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,15,aj9muh,self.MachineLearning,10 Innovative Machine Learning APIs You Should Learn in 2019,https://www.reddit.com/r/MachineLearning/comments/aj9muh/10_innovative_machine_learning_apis_you_should/,KiranKiller,1548311069,[removed],0,1,False,self,,,,,
1331,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,16,aj9vzk,savantis.com,How Enterprises can be More Customer-Centric,https://www.reddit.com/r/MachineLearning/comments/aj9vzk/how_enterprises_can_be_more_customercentric/,aditya44,1548313298,,0,1,False,default,,,,,
1332,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,16,aja6xh,github.com,"Intel open sources Nauta: multi-user, distributed environment for DL training",https://www.reddit.com/r/MachineLearning/comments/aja6xh/intel_open_sources_nauta_multiuser_distributed/,disdi89,1548316223,,0,1,False,default,,,,,
1333,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,17,ajab5e,self.MachineLearning,What are some good books published in 2017/2018?,https://www.reddit.com/r/MachineLearning/comments/ajab5e/what_are_some_good_books_published_in_20172018/,zindarod,1548317418,[removed],0,1,False,self,,,,,
1334,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,18,ajan42,theappsolutions.com,Recommender Systems and Their Impact on Customer Experience,https://www.reddit.com/r/MachineLearning/comments/ajan42/recommender_systems_and_their_impact_on_customer/,lady_monsoon,1548320787,,0,1,False,https://b.thumbs.redditmedia.com/lyk2vSBp2BaOIyo6GqvtvFPR1sJjji79-iFAK2sBEIk.jpg,,,,,
1335,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,19,ajaysh,self.MachineLearning,Uber residency hackerrank Test,https://www.reddit.com/r/MachineLearning/comments/ajaysh/uber_residency_hackerrank_test/,Mouna_kh,1548324094,[removed],0,1,False,self,,,,,
1336,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,19,ajb09s,self.MachineLearning,Books on Probability for machine learning,https://www.reddit.com/r/MachineLearning/comments/ajb09s/books_on_probability_for_machine_learning/,PyWarrior,1548324489,[removed],0,1,False,self,,,,,
1337,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,19,ajb450,arxiv.org,[1901.07677] Modeling Human Motion with Quaternion-based Neural Networks,https://www.reddit.com/r/MachineLearning/comments/ajb450/190107677_modeling_human_motion_with/,ihaphleas,1548325501,,6,1,False,default,,,,,
1338,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,19,ajb4on,self.MachineLearning,Residual Neural Network - Batch Normalization? Dropouts?,https://www.reddit.com/r/MachineLearning/comments/ajb4on/residual_neural_network_batch_normalization/,qudcjf7928,1548325652,[removed],0,1,False,self,,,,,
1339,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,19,ajbaqf,proonlineclass.com,Pro online Class,https://www.reddit.com/r/MachineLearning/comments/ajbaqf/pro_online_class/,Mediologest,1548327273,,0,1,False,default,,,,,
1340,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,19,ajbbi4,self.MachineLearning,What is Automated Machine Learning (AutoML)?,https://www.reddit.com/r/MachineLearning/comments/ajbbi4/what_is_automated_machine_learning_automl/,ElegantMicroWebIndia,1548327478,[removed],0,1,False,https://a.thumbs.redditmedia.com/otlgiTKAnk6REc-FPluzu7tLSZYl4k9sIjeB_20Dvd4.jpg,,,,,
1341,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,20,ajbduq,self.MachineLearning,[D] fmri dataset for brain segmentation,https://www.reddit.com/r/MachineLearning/comments/ajbduq/d_fmri_dataset_for_brain_segmentation/,pratik_112233,1548328075,"I need to get a properly labelled fmri brain images for segmentation purpose.
Any suggestions or known public datasets?",5,1,False,self,,,,,
1342,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,20,ajbjzk,github.com,PerceptRon - A Classic Perceptron Implementation which can learn Logic gates and multi-layered example for XOR.,https://www.reddit.com/r/MachineLearning/comments/ajbjzk/perceptron_a_classic_perceptron_implementation/,flouthoc,1548329620,,0,1,False,default,,,,,
1343,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,20,ajbqa5,github.com,PerceptRon - Classic perceptron implementation which could learn boolean logic gates [XOR case included].,https://www.reddit.com/r/MachineLearning/comments/ajbqa5/perceptron_classic_perceptron_implementation/,flouthoc,1548331175,,0,1,False,https://a.thumbs.redditmedia.com/CH2NWmiRUG4Cj3zMOV8ITgoQEYD4cv55Od443iVkcU4.jpg,,,,,
1344,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,21,ajbr0j,self.MachineLearning,Image clustering based on noise,https://www.reddit.com/r/MachineLearning/comments/ajbr0j/image_clustering_based_on_noise/,adelrahimi,1548331327,[removed],0,1,False,self,,,,,
1345,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,21,ajc23q,self.MachineLearning,[HELP] I have to do a machine learning project and I want to do it in the security domain. Can you please help me as I am a noob in ML.,https://www.reddit.com/r/MachineLearning/comments/ajc23q/help_i_have_to_do_a_machine_learning_project_and/,thenerdsuperuser,1548333903,[removed],0,1,False,self,,,,,
1346,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,22,ajclyk,youtube.com,Machine Learning Basics - Easiest video I have ever seen,https://www.reddit.com/r/MachineLearning/comments/ajclyk/machine_learning_basics_easiest_video_i_have_ever/,MainBuilder,1548337941,,0,1,False,https://b.thumbs.redditmedia.com/oaQjE0Gu5zlJwhsKRhvksp7bWd4jnVKJwaYV36t-dOM.jpg,,,,,
1347,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,23,ajcu8u,ipkitten.blogspot.com,Full speed ahead for DeepMind's AI patent applications,https://www.reddit.com/r/MachineLearning/comments/ajcu8u/full_speed_ahead_for_deepminds_ai_patent/,The_IPKat,1548339482,,0,1,False,default,,,,,
1348,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,23,ajcx50,self.MachineLearning,[D] Best learning resources for reservoir computing? And echo state networks?,https://www.reddit.com/r/MachineLearning/comments/ajcx50/d_best_learning_resources_for_reservoir_computing/,soutioirsim,1548340013,,2,1,False,self,,,,,
1349,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,23,ajczua,self.MachineLearning,[D] Temporary replacement for ImageAI custom detection?,https://www.reddit.com/r/MachineLearning/comments/ajczua/d_temporary_replacement_for_imageai_custom/,nlh101,1548340520,"Hi there! I'm very new to machine learning in general.

I found ImageAI after much Googling and was able to train a custom model with my dataset. Unfortunately, I found out that I was only able to detect objects, and not actually draw bounding boxes or provide coordinates to do so. According to issue #8 on their GitHub page, the work on the feature is ongoing, but unfortunately, is not available yet.

Is there any possible replacement, even a temporary replacement, that would be able to complete the same task with a minimal amount of overhead (i.e. new environment)? I can work with any ML framework and I have access to a GPU for training, and I would \*prefer\* Python for the solution.

I know of GOTURN, but I cannot find any other system that isn't otherwise difficult to use.",0,1,False,self,,,,,
1350,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,23,ajd3x7,msdn.microsoft.com,Machine Learning Through Probabilistic Programming,https://www.reddit.com/r/MachineLearning/comments/ajd3x7/machine_learning_through_probabilistic_programming/,asuagar,1548341259,,0,1,False,default,,,,,
1351,MachineLearning,t5_2r3gv,2019-1-24,2019,1,24,23,ajd6t9,self.MachineLearning,ELBO loss/objective in VAE and VQ-VAE,https://www.reddit.com/r/MachineLearning/comments/ajd6t9/elbo_lossobjective_in_vae_and_vqvae/,uakbar,1548341780,[removed],0,1,False,self,,,,,
1352,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,0,ajdazf,self.MachineLearning,"[D] DeepMind, StarCraft II, and The Next Big Thing in AI",https://www.reddit.com/r/MachineLearning/comments/ajdazf/d_deepmind_starcraft_ii_and_the_next_big_thing_in/,Inori,1548342517,"I've noticed some misconceptions flying around both here and in /r/starcraft so figured I'd write an overview of the upcoming event aimed at both the players and AI researchers. Finally, I speculate a bit about their possible approaches based on the work they were recently putting out.

http://inoryy.com/post/deepmind-starcraft2-next-big-thing-ai/",25,1,False,self,,,,,
1353,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,0,ajdf33,inference.vc,[D] blog post on Causal Inference: An Introduction to Counterfactuals,https://www.reddit.com/r/MachineLearning/comments/ajdf33/d_blog_post_on_causal_inference_an_introduction/,fhuszar,1548343197,,0,1,False,default,,,,,
1354,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,0,ajdihd,self.MachineLearning,RBFN implementation in tensorflow. Comparative analysis of clustering algorithms and variable cluster centers.,https://www.reddit.com/r/MachineLearning/comments/ajdihd/rbfn_implementation_in_tensorflow_comparative/,leviackerman76,1548343773,[removed],0,1,False,self,,,,,
1355,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,0,ajdmfb,towardsdatascience.com,Hyperparameter Optimisation with Flair - How to Beat Googles AutoML,https://www.reddit.com/r/MachineLearning/comments/ajdmfb/hyperparameter_optimisation_with_flair_how_to/,vitalitylife,1548344440,,0,1,False,default,,,,,
1356,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,0,ajdsa8,ashutoshtripathi.com,What is the Coefficient of Determination | R Square,https://www.reddit.com/r/MachineLearning/comments/ajdsa8/what_is_the_coefficient_of_determination_r_square/,imashutoshtr,1548345412,,0,1,False,default,,,,,
1357,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,1,ajdw71,arxiv.org,[R] Geometric Understanding of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/ajdw71/r_geometric_understanding_of_deep_learning/,dimber-damber,1548346045,,12,1,False,default,,,,,
1358,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,1,ajef2i,self.MachineLearning,[R] ML-tool to distinguish fake news from real,https://www.reddit.com/r/MachineLearning/comments/ajef2i/r_mltool_to_distinguish_fake_news_from_real/,MathiasBack,1548349042," In a class project at MIT we're looking into detection of fake news, an important challenge of our time. Our research looked at whether a specific tool based on ML can help us distinguish fake news by highlighting certain words in both fake and real news. 

If some of you have 5 minutes to spare during the day, I would really appreciate it if you could use them to help me gathering data for said project using the following link:   
[https://mit.co1.qualtrics.com/jfe/form/SV\_ebyzk4FJzUgIY3H](https://mit.co1.qualtrics.com/jfe/form/SV_ebyzk4FJzUgIY3H?fbclid=IwAR2OGwNBVS1KiTOxrmS_yvMYbltG8AwImrTEk4g4EBM0u8q9sFG5Wzg4qfY)

\[Note: it seems to work only on a computer, not using a phone/tablet\]

You will get to a survey where you will read 2 (long or short) article texts (with or without some words highlighted) and answer some questions.",2,1,False,self,,,,,
1359,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,2,ajepyt,self.MachineLearning,AI for Games,https://www.reddit.com/r/MachineLearning/comments/ajepyt/ai_for_games/,g-x91,1548350746,[removed],0,1,False,self,,,,,
1360,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,2,ajesvs,youtube.com,[LIVE] DeepMind StarCraft II Demonstration,https://www.reddit.com/r/MachineLearning/comments/ajesvs/live_deepmind_starcraft_ii_demonstration/,Cock-tail,1548351202,,0,1,False,https://b.thumbs.redditmedia.com/wYUOO2qZLXqDlG7cPYe8_qfkrLKghAFvH7d6IE7znbE.jpg,,,,,
1361,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,2,ajetns,medium.com,Introducing Community Products: making crowdselling your data a reality from any application or gadget,https://www.reddit.com/r/MachineLearning/comments/ajetns/introducing_community_products_making/,thamilton5,1548351321,,0,1,False,https://b.thumbs.redditmedia.com/t8geC9FSjj3HiU1IzIjkftnLG9XI7IVyn_2nz1ChAaQ.jpg,,,,,
1362,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,2,ajew93,edyoda.com,"We have made 20 free Machine Learning tutorials and wanted to share them with you guys for the feedback. It's free, just pass on your feedback here.",https://www.reddit.com/r/MachineLearning/comments/ajew93/we_have_made_20_free_machine_learning_tutorials/,iamarmaan,1548351730,,0,1,False,default,,,,,
1363,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,2,ajezj1,self.MachineLearning,[D] MIT 6.S091: Introduction to Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/ajezj1/d_mit_6s091_introduction_to_deep_reinforcement/,UltraMarathonMan,1548352235,"First lecture of course 6.S091: Deep Reinforcement Learning introducing deep RL. This is my favorite subfield of AI as it asks fundamental questions about what it takes to build safe &amp; intelligent robots that operate in the real world. 

Video: [https://www.youtube.com/watch?v=zR11FLZ-O9M](https://www.youtube.com/watch?v=zR11FLZ-O9M)

Lecture Slides: [PDF on Dropbox](https://www.dropbox.com/s/wekmlv45omd266o/deep_rl_intro.pdf?dl=0)

GitHub: [https://github.com/lexfridman/mit-deep-learning](https://github.com/lexfridman/mit-deep-learning)

Lecture outline: 

* Introduction
* Types of learning
* Reinforcement learning in humans
* What can be learned from data?
* Reinforcement learning framework
* Challenge for RL in real-world applications
* Component of an RL agent
* Example: robot in a room
* AI safety and unintended consequences
* Examples of RL systems
* Takeaways for real-world impact
* 3 types of RL: model-based, value-based, policy-based
* Q-learning
* Deep Q-Networks (DQN)
* Policy Gradient (PG)
* Advantage Actor-Critic (A2C &amp; A3C)
* Deep Deterministic Policy Gradient (DDPG)
* Policy Optimization (TRPO and PPO)
* AlphaZero
* Deep RL in real-world applications 
* Closing the RL simulation gap
* Next step in Deep RL

Let me know if you have suggestions for topics I should cover (in depth) in upcoming lectures.",0,1,False,self,,,,,
1364,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,2,ajf137,self.MachineLearning,[D] MIT 6.S091: Introduction to Deep Reinforcement Learning (Deep RL),https://www.reddit.com/r/MachineLearning/comments/ajf137/d_mit_6s091_introduction_to_deep_reinforcement/,UltraMarathonMan,1548352485,"First lecture of course 6.S091: Deep Reinforcement Learning introducing deep RL. This is my favorite subfield of AI as it asks fundamental questions about what it takes to build safe &amp; intelligent robots that operate in the real world.

Video: [https://www.youtube.com/watch?v=zR11FLZ-O9M](https://www.youtube.com/watch?v=zR11FLZ-O9M)

Lecture Slides: [PDF on Dropbox](https://www.dropbox.com/s/wekmlv45omd266o/deep_rl_intro.pdf?dl=0)

GitHub: [https://github.com/lexfridman/mit-deep-learning](https://github.com/lexfridman/mit-deep-learning)

&amp;#x200B;

![img](l5smjlr6tec21 ""First lecture of 6.S091: Deep Reinforcement Learning"")

Lecture outline:

* Introduction
* Types of learning
* Reinforcement learning in humans
* What can be learned from data?
* Reinforcement learning framework
* Challenge for RL in real-world applications
* Component of an RL agent
* Example: robot in a room
* AI safety and unintended consequences
* Examples of RL systems
* Takeaways for real-world impact
* 3 types of RL: model-based, value-based, policy-based
* Q-learning
* Deep Q-Networks (DQN)
* Policy Gradient (PG)
* Advantage Actor-Critic (A2C &amp; A3C)
* Deep Deterministic Policy Gradient (DDPG)
* Policy Optimization (TRPO and PPO)
* AlphaZero
* Deep RL in real-world applications
* Closing the RL simulation gap
* Next step in Deep RL

Let me know if you have suggestions for topics I should cover (in depth) in upcoming lectures.",4,1,False,self,,,,,
1365,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,3,ajfn1g,self.MachineLearning,Help building a new rig for Tensorflow,https://www.reddit.com/r/MachineLearning/comments/ajfn1g/help_building_a_new_rig_for_tensorflow/,talipini,1548355869,[removed],0,1,False,self,,,,,
1366,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,3,ajfpgt,self.MachineLearning,[N] DeepMind's AlphaStar wins 5-0 against LiquidTLO on StarCraft II,https://www.reddit.com/r/MachineLearning/comments/ajfpgt/n_deepminds_alphastar_wins_50_against_liquidtlo/,gohu_cd,1548356261,"Any ML and StarCraft expert can provide details on how much the results are impressive?  


Let's have a thread where we can analyze the results.",292,1,False,self,,,,,
1367,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,4,ajfv52,youtu.be,DeepMind StarCraft II Demonstration Video,https://www.reddit.com/r/MachineLearning/comments/ajfv52/deepmind_starcraft_ii_demonstration_video/,vackosar,1548357132,,0,1,False,https://a.thumbs.redditmedia.com/q_fW85tes55SKn4UZWwLs15To89ueNRdG7pCxHzFwU4.jpg,,,,,
1368,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,4,ajfxzy,self.MachineLearning,Deepminds Breakthrough AlphaStar Signifies Unprecedented Progress Towards AGI,https://www.reddit.com/r/MachineLearning/comments/ajfxzy/deepminds_breakthrough_alphastar_signifies/,red-starman,1548357573,[removed],0,1,False,self,,,,,
1369,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,4,ajfyqx,self.MachineLearning,How well can tensorflow probability be integrated in tensorflow code?,https://www.reddit.com/r/MachineLearning/comments/ajfyqx/how_well_can_tensorflow_probability_be_integrated/,captaincyypher,1548357692,"Is anybody out there with experience combining tensorflow and tensorflow probability?

I have tried to combine them and have experience a lot of issues due to tensorflow probability having many different object types. For example, I tried to sample points using a MCMC method and using a neural network as my target probability density function but I has not worked out yet.

&amp;#x200B;",0,1,False,self,,,,,
1370,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,4,ajg9hm,youtube.com,Artificial intelligence in the dining room,https://www.reddit.com/r/MachineLearning/comments/ajg9hm/artificial_intelligence_in_the_dining_room/,cmillionaire9,1548359396,,0,1,False,default,,,,,
1371,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,4,ajga62,self.MachineLearning,Short-range skip connections,https://www.reddit.com/r/MachineLearning/comments/ajga62/shortrange_skip_connections/,ceps1986,1548359498,"I've been putting some thought into this, and haven't quite convinced myself if the following is necessary...

&amp;#x200B;

When creating short-range skip connections from input and convolution output, should one use batchnorm prior to concatinating these (i.e. concat\[conv\_output, input\])? I'm not sure if this necessary, but just wondering if anyone has looked into this...

&amp;#x200B;",0,1,False,self,,,,,
1372,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,4,ajgd1x,self.MachineLearning,[D] What to ask for after a job offer in Canada?,https://www.reddit.com/r/MachineLearning/comments/ajgd1x/d_what_to_ask_for_after_a_job_offer_in_canada/,GeekMonolith,1548359957,I don't know much about the market in Canada and recently got a few offers from startups in Canada. What sort of benefits and other things should you ask for when you have a job offer in tech companies? How do you negotiate this effectively?,0,1,False,self,,,,,
1373,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,5,ajgj2c,youtu.be,[D] Artificial intelligence in the dining room,https://www.reddit.com/r/MachineLearning/comments/ajgj2c/d_artificial_intelligence_in_the_dining_room/,cmillionaire9,1548360904,,0,1,False,https://b.thumbs.redditmedia.com/BuXHWdhj2MXexwnMpTW6OmkykQC4-2clxos-J01kodE.jpg,,,,,
1374,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,5,ajgjto,self.MachineLearning,[N] The latest version of AlphaStar loses to Liquid MaNa,https://www.reddit.com/r/MachineLearning/comments/ajgjto/n_the_latest_version_of_alphastar_loses_to_liquid/,Felix_Tholomyes,1548361027,"The newest version of AlphaStar has to control the camera as well, MaNa just beat it live after losing to the previous version 0-5",66,1,False,self,,,,,
1375,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,5,ajgmyn,self.MachineLearning,[D] Artificial intelligence in the dining room,https://www.reddit.com/r/MachineLearning/comments/ajgmyn/d_artificial_intelligence_in_the_dining_room/,cmillionaire9,1548361534," Neural networks are trained to identify food. Money is instantly debited from a credit card.   
[https://www.youtube.com/watch?v=LwaV-gI5krA&amp;feature=youtu.be](https://www.youtube.com/watch?v=LwaV-gI5krA&amp;feature=youtu.be)",3,1,False,self,,,,,
1376,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,5,ajgpu4,towardsdatascience.com,Computer Vision: An Intuitive Explanation,https://www.reddit.com/r/MachineLearning/comments/ajgpu4/computer_vision_an_intuitive_explanation/,danimex,1548361977,,0,1,False,default,,,,,
1377,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,5,ajgt42,deepmind.com,AlphaStar: Mastering the Real-Time Strategy Game StarCraft II | DeepMind,https://www.reddit.com/r/MachineLearning/comments/ajgt42/alphastar_mastering_the_realtime_strategy_game/,Cock-tail,1548362476,,0,1,False,default,,,,,
1378,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,5,ajgzoc,self.MachineLearning,"We are Oriol Vinyals and David Silver from DeepMinds AlphaStar team, joined by StarCraft II pro players TLO and MaNa! Ask us anything",https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/,OriolVinyals,1548363323,[removed],1083,1,False,self,,,,,
1379,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,5,ajh27z,self.MachineLearning,AMA Request UberAI team behind Paired Open-Ended Trailblazer(POET) paper,https://www.reddit.com/r/MachineLearning/comments/ajh27z/ama_request_uberai_team_behind_paired_openended/,Leader_of_Internet,1548363461,[removed],0,1,False,self,,,,,
1380,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,6,ajh6ch,youtu.be,I made an AI that lands on the moon in 2D (using neuroevolution),https://www.reddit.com/r/MachineLearning/comments/ajh6ch/i_made_an_ai_that_lands_on_the_moon_in_2d_using/,DJ_Roby,1548363786,,0,1,False,https://b.thumbs.redditmedia.com/lFV7wDJGnD6Qqn6YJ7q2tjMTZmKwNa-YX5-ecXpDNKE.jpg,,,,,
1381,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,6,ajh89t,self.MachineLearning,[R] AlphaStar: Mastering the Real-Time Strategy Game StarCraft II,https://www.reddit.com/r/MachineLearning/comments/ajh89t/r_alphastar_mastering_the_realtime_strategy_game/,P4TR10T_TR41T0R,1548363997,After today's stream ([https://www.youtube.com/watch?v=cUTMhmVh1qs](https://www.youtube.com/watch?v=cUTMhmVh1qs)) DeepMind has released the blog post about AlphaStar! here is the link: https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/,5,1,False,self,,,,,
1382,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,6,ajhej1,medium.com,Filling Holes: Adobe Proposes Foreground-Aware Image Inpainting,https://www.reddit.com/r/MachineLearning/comments/ajhej1/filling_holes_adobe_proposes_foregroundaware/,gwen0927,1548364886,,0,1,False,https://b.thumbs.redditmedia.com/tCq14vzuHQreTMI1jF2iIk2bMcOaWtKr9hJ1If9SIKg.jpg,,,,,
1383,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,6,ajhn0h,youtube.com,"A journey through neural networks - Self Driving, simulations and education",https://www.reddit.com/r/MachineLearning/comments/ajhn0h/a_journey_through_neural_networks_self_driving/,DevTechRetopall,1548366212,,0,1,False,https://b.thumbs.redditmedia.com/g5pij9nYL7r_Yq75c0Xi1kzujQ4qfSGoD0dnObmYP5s.jpg,,,,,
1384,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,6,ajhqyj,self.MachineLearning,How to use deep learning as a general optimiser (non-differentiable loss function),https://www.reddit.com/r/MachineLearning/comments/ajhqyj/how_to_use_deep_learning_as_a_general_optimiser/,fadg345,1548366847,[removed],1,1,False,self,,,,,
1385,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,7,aji78j,self.MachineLearning,"So far, what are the biggest impacts from Machine Learning?",https://www.reddit.com/r/MachineLearning/comments/aji78j/so_far_what_are_the_biggest_impacts_from_machine/,letstryusingreddit,1548369476,"Aside from better search results and better shopping recommedations, what are the things that were already changed/improved tremendously by machine learning? 
(I wouldn't count self driving car since its not commercially available yet. I wouldn't count language translation software either, since they are not widespread and inconsistent.)",0,1,False,self,,,,,
1386,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,8,ajijyy,self.MachineLearning,AMA test,https://www.reddit.com/r/MachineLearning/comments/ajijyy/ama_test/,testaccount44ml,1548371614,[removed],0,1,False,self,,,,,
1387,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,9,ajj6jp,github.com,"Graph:Note PGDB -- A Graph Database of Programmers, built by YOU and owned by YOU",https://www.reddit.com/r/MachineLearning/comments/ajj6jp/graphnote_pgdb_a_graph_database_of_programmers/,wengchunkn,1548375617,,0,1,False,https://b.thumbs.redditmedia.com/Uqzz5WtTmFvhKN_wJnLb0-ojrmU-RSy5FcI6t9zuRpE.jpg,,,,,
1388,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,9,ajj9p1,go.thoughtspot.com,SpotIQ AI-Driven Analytics - White Paper,https://www.reddit.com/r/MachineLearning/comments/ajj9p1/spotiq_aidriven_analytics_white_paper/,aryapowerpuff,1548376201,,0,1,False,default,,,,,
1389,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,9,ajjhz0,blog.paperspace.com,[R] Physics control tasks with Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/ajjhz0/r_physics_control_tasks_with_deep_reinforcement/,coffeepants87,1548377698,,0,1,False,default,,,,,
1390,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,10,ajjnen,self.MachineLearning,[D] Finding static images inside of a larger image,https://www.reddit.com/r/MachineLearning/comments/ajjnen/d_finding_static_images_inside_of_a_larger_image/,KoltBruh,1548378664,"Hello! To elaborate a bit more on the title, I'm attempting to find a way to detect if an image exists inside of another image. I will give an example to be more specific: The goal of my work is to create a system where the player of a card game (for example Hearthstone) can look at a single screen of their collection and be notified of all of the cards that are currently visible on the screen. Note: I'm not actually using Hearthstone, so I can't just tap into the available local game data.  I figured I'd post here as I've tried just about everything that I can think of with no success. The things that I have tried are listed below.

* Looking for relevant log files or local game data didn't reveal anything useful.
* I attempted to use the Tesseract library to use OCR to find the names of the cards. I think that the fancy fonts made this a bad option.
* I tried to use image diff tools like pixelmatch, resemble.js, and blink-diff. These seem to require both images to be the same size, so this didn't work out either.
* I tried to use template matching using OpenCV to check if a match could be found, but the confidence scores between cards that weren't on the screen and cards that were were either way too close or even flipped backwards, so I abandoned that. This was after attempting to crop the card images enough to where all of the not-unique parts of the cards (ex. the borders and number values were removed), as well as attempting to apply edge detection filters on the images to simplify them.

My current thought process is that I would need to do some sort of feature extraction or object extraction to do this. I've considered researching how to make a training dataset based on the card images that I have, but that seems like a bit overkill as I'm only really looking for one static, unchanging image every time. I've seen that projects such as \[card\_scan\]([https://github.com/YenTheFirst/card\_scan](https://github.com/YenTheFirst/card_scan)) exist, but even those seem more complicated than what I'm attempting to do. Any help or discussion is greatly appreciated. Thank you again!",10,1,False,self,,,,,
1391,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,10,ajjy6o,medium.com,Data Augmentation by fastai v1,https://www.reddit.com/r/MachineLearning/comments/ajjy6o/data_augmentation_by_fastai_v1/,dronecub,1548380706,,0,1,False,default,,,,,
1392,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,12,ajl2xh,self.MachineLearning,help me understand what deep learning is or what it is used for?,https://www.reddit.com/r/MachineLearning/comments/ajl2xh/help_me_understand_what_deep_learning_is_or_what/,stratohaze,1548388455,[removed],0,1,False,self,,,,,
1393,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,12,ajl4er,self.MachineLearning,Ubuntu 17.04 vs Ubuntu 18.04/18.10 for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ajl4er/ubuntu_1704_vs_ubuntu_18041810_for_machine/,mgavaudan,1548388746,[removed],0,1,False,self,,,,,
1394,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,13,ajlg7p,challenger.io,TensorFlow vs PyTorch - Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/ajlg7p/tensorflow_vs_pytorch_artificial_intelligence/,aaronscat,1548391122,,1,1,False,default,,,,,
1395,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,13,ajlkti,challenger.io,Is China going to dominate AI?,https://www.reddit.com/r/MachineLearning/comments/ajlkti/is_china_going_to_dominate_ai/,aaronscat,1548392038,,0,1,False,default,,,,,
1396,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,15,ajm7uo,self.MachineLearning,[D] Where do you draw the boundaries on 'what is machine learning?',https://www.reddit.com/r/MachineLearning/comments/ajm7uo/d_where_do_you_draw_the_boundaries_on_what_is/,abcdeathburger,1548396962,"Possibly stupid question: does a machine learning model need to be something generalizable to many different problems (by adjusting hyperparameters and so on)? Or does it simply need to be able to make (strong) predictions based on previous data?

&amp;#x200B;

I normally think of ML algorithms as those where you import a library (say for SVMs), throw your data into a matrix (after doing any necessary feature engineering), and playing around with hyperparameters to make accurate predictions.

&amp;#x200B;

But suppose you come up with a clever linear program that takes in previous data and creates fantastic predictions. This is more complicated than just saying ""if x1 &lt; 5, do this, else if x2 &gt; 10, do this,"" etc. But it is particular to the specific problem you're trying to solve. When you need to make predictions for another problem, you may come up with another algorithm motivated by some of your linear programming ideas, but you won't be able to package your algorithm into a library to solve a wide array of problems. Is it still machine learning? Does it even matter what we call it, as long as it reliably solves the problem?",14,1,False,self,,,,,
1397,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,15,ajm9ed,self.MachineLearning,Aseptic Processing Market Insights and Global Industry Forecast to 2024,https://www.reddit.com/r/MachineLearning/comments/ajm9ed/aseptic_processing_market_insights_and_global/,parag_bir,1548397326,[removed],1,1,False,self,,,,,
1398,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,15,ajmfhu,linkedin.com,Things High School Students Must Know About Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/ajmfhu/things_high_school_students_must_know_about/,vermarajan,1548398799,,0,1,False,default,,,,,
1399,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,16,ajmkr7,self.MachineLearning,"Interview with The Creator of Keras, AI Researcher: Franois Chollet",https://www.reddit.com/r/MachineLearning/comments/ajmkr7/interview_with_the_creator_of_keras_ai_researcher/,init__27,1548400148,[removed],0,1,False,self,,,,,
1400,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,16,ajmrke,/r/MachineLearning/comments/ajmrke/what_kind_of_model_could_we_train_with_historical/,What kind of model could we train with historical &amp; realtime traffic data?,https://www.reddit.com/r/MachineLearning/comments/ajmrke/what_kind_of_model_could_we_train_with_historical/,data_entertainment,1548401962,,1,1,False,default,,,,,
1401,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,16,ajmtyv,/r/MachineLearning/comments/ajmtyv/d_what_kind_of_model_could_we_train_with/,[D] What kind of model could we train with historical &amp; realtime traffic and road surface condition data?,https://www.reddit.com/r/MachineLearning/comments/ajmtyv/d_what_kind_of_model_could_we_train_with/,data_entertainment,1548402615,,1,1,False,default,,,,,
1402,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,17,ajmyr4,self.MachineLearning,[D] What kind of model could we train with historical + realtime traffic and road surface condition data?,https://www.reddit.com/r/MachineLearning/comments/ajmyr4/d_what_kind_of_model_could_we_train_with/,data_entertainment,1548403917,"Have a look at the [video](https://www.whereos.com/business-insights/roadcloud-and-whereos-collaborate-on-automotive-data-collection-and-processing/) of hourly traffic speed analysis in Helsinki Finland for 2018. [RoadCloud](https://www.roadcloud.fi/) has collected a large amount of sensors data, and we have processed analyzed the data using [WhereOS](https://www.whereos.com/) into a video which shows hour-by-hour speed changes for each road segment. 

The question is what kind of model could we train with the data? One obvious one would be to take past weather conditions and train a model that can predict the friction level or road segment average speed with the future weather forecasts. But, how about other ideas? The data is collected from a fleet of commercial cars, each car having attributes such as velocity, heading, friction (wrt. road surface wet/snow/ice/normal) and so forth. The difference to the usual traffic data is the high precision road surface data collected through the optical RoadCloud sensors installed in the cars.",0,1,False,self,,,,,
1403,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,17,ajn0hp,self.MachineLearning,How do I come up with novel ideas for research in machine learning?,https://www.reddit.com/r/MachineLearning/comments/ajn0hp/how_do_i_come_up_with_novel_ideas_for_research_in/,surya-k,1548404410,[removed],0,1,False,self,,,,,
1404,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,18,ajn96k,self.MachineLearning,Good degrees other than CompSci for AI/ML?,https://www.reddit.com/r/MachineLearning/comments/ajn96k/good_degrees_other_than_compsci_for_aiml/,ShadowViking47,1548406974,[removed],0,1,False,self,,,,,
1405,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,18,ajn99s,self.MachineLearning,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/ajn99s/global_machine_learning_market_size_outlook/,BetterTraffic8,1548406999,[removed],0,1,False,self,,,,,
1406,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,18,ajnaoi,self.MachineLearning,Looking Adviser for startup or somebody who can coach me 2 times per week $$$,https://www.reddit.com/r/MachineLearning/comments/ajnaoi/looking_adviser_for_startup_or_somebody_who_can/,Andrius_Blazaitis,1548407405,[removed],0,1,False,self,,,,,
1407,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,18,ajnjhy,medium.com,How Machine Learning is Enabling New Cost Levers in Post-Trade Operations,https://www.reddit.com/r/MachineLearning/comments/ajnjhy/how_machine_learning_is_enabling_new_cost_levers/,Marksfik,1548409971,,0,1,False,default,,,,,
1408,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,19,ajnldo,arxiv.org,[R] Unsupervised Image-to-Image Translation with Self-Attention Networks,https://www.reddit.com/r/MachineLearning/comments/ajnldo/r_unsupervised_imagetoimage_translation_with/,kh22l22,1548410513,,11,1,False,default,,,,,
1409,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,19,ajnshw,channels.theinnovationenterprise.com,Why do you need machine learning strategy for marketing purpose?,https://www.reddit.com/r/MachineLearning/comments/ajnshw/why_do_you_need_machine_learning_strategy_for/,james-warner,1548412415,,0,1,False,default,,,,,
1410,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,21,ajom26,arxiv.org,[1901.08184] Learning How to Autonomously Race a Car: a Predictive Control Approach,https://www.reddit.com/r/MachineLearning/comments/ajom26/190108184_learning_how_to_autonomously_race_a_car/,ihaphleas,1548419797,,1,1,False,default,,,,,
1411,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,21,ajonjq,technologyreview.com,"We analyzed 16,625 papers to figure out where AI is headed next",https://www.reddit.com/r/MachineLearning/comments/ajonjq/we_analyzed_16625_papers_to_figure_out_where_ai/,saadmrb,1548420128,,0,1,False,default,,,,,
1412,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,22,ajp6db,self.MachineLearning,[D] Research Project Advice Needed!,https://www.reddit.com/r/MachineLearning/comments/ajp6db/d_research_project_advice_needed/,raichet,1548424061,"Hi guys, I need some advices for my school research project. I cannot post the actual data or talk about the project detail itself, so I will be modifying the project context a bit. Please bear with me!

I am given images similar to [this](https://pixabay.com/en/eucalyptus-flower-buds-buds-blossom-1234009/), except there will be multiple stems with their respective group of flower buds. **My task is to segment the image into individual components (stems), and measure the number of flower buds belonging to each component, and the length and diameter of each flower bud on a component.** Each picture is guaranteed to have stems. Most stems will have many flower buds on them, but some stems may not have any (highly unlikely). There will likely be a LOT of measurements per image.


My questions:

1. I would like to have some mental clarity on the project's pipeline. Since each picture is guaranteed to have stems, I am thinking instance segmentation method to segment the picture into individual components consisting of a stem and its flower buds. Then for each component, since some component may have no flower buds, I would use object detection methods to draw a bounding box around each bud, then use semantic/instance segmentation to segment each bud from its box. Is this a correct way to be tackling the segmentation portion of the project?

2. What type of task would measuring length/diameter/angle of each bud be?

3. What is the best way to label the given pictures? The dataset is pretty small, which will pose a challenge.  

I am brainstorming ideas, and trying to come up with a clear mental picture of exactly what needs to be done. Thank you for reading through this long post. Any input will be appreciated!


",0,1,False,self,,,,,
1413,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,23,ajphco,self.MachineLearning,[D] The best of GAN papers in the year 2018 (part 2),https://www.reddit.com/r/MachineLearning/comments/ajphco/d_the_best_of_gan_papers_in_the_year_2018_part_2/,dtransposed,1548426152," 

Hi!  
As a follow-up to my [older post](https://dtransposed.github.io/blog/Best-of-GANs-2018-(Part-1-out-of-2).html)  I would like to share a second part of the summary concerning my favourite research papers in 2018 with the focus on the deep learning models - Generative Adversarial Networks.

I am certain there is a substantial number of passionates who would enjoy this short read.

Have fun!

[https://dtransposed.github.io/blog/Best-of-GANs-2018-(Part-1-out-of-2).html](https://dtransposed.github.io/blog/Best-of-GANs-2018-(Part-1-out-of-2).html)",1,1,False,self,,,,,
1414,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,23,ajphyc,self.MachineLearning,"[D] Basic Research vs. Applied Research, Journals and Conferences",https://www.reddit.com/r/MachineLearning/comments/ajphyc/d_basic_research_vs_applied_research_journals_and/,novice__researcher,1548426263,"Hello everyone.

I've been reading this sub for a while and it seems like common knowledge that the best place to publish papers nowadays are for example the NIPS and ICML conferences and JMLR journal.

The thing is, are this mainly for basic research, i.e., research carried out to increase understanding of fundamental principles?

If one is writing a more ""applied research"" kind of paper, would it be better to try and publish it elsewhere, of more specifically in the field in which we are applying the ML tools?

This question arises because I would like to search for the best recent papers that use neural networks in the field of wireless communications/signal processing . So knowing where people from this fields publish when they make ML related findings would help me a lot.

I don't know if this is the correct sub to start this discussion/ ask this question. Please feel free to send me to another place :D
",9,1,False,self,,,,,
1415,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,23,ajpivc,self.MachineLearning,[Discussion] Do Bayes Algorithms still plays an important role in AI?,https://www.reddit.com/r/MachineLearning/comments/ajpivc/discussion_do_bayes_algorithms_still_plays_an/,HouseLTN,1548426435,"As per title, do bayes algorithms still play a critical role in AI applications given the rise of deep learning? 
If yes, please provide a few ELI5 examples for better understanding. ",18,1,False,self,,,,,
1416,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,23,ajpk7i,self.MachineLearning,Resources needed to become a data scientist,https://www.reddit.com/r/MachineLearning/comments/ajpk7i/resources_needed_to_become_a_data_scientist/,b14cksh4d0w369,1548426691,Can you guys suggest me open science data science courses . Along with all kind of books. To become a fully fledged data science. I can afford books but not courses,0,1,False,self,,,,,
1417,MachineLearning,t5_2r3gv,2019-1-25,2019,1,25,23,ajpng1,julialang.org,DiffEqFlux.jl  A Julia Library for Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/ajpng1/diffeqfluxjl_a_julia_library_for_neural/,cafedude,1548427286,,0,1,False,default,,,,,
1418,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,0,ajpxmt,self.MachineLearning,Low-Level Tensorflow or Estimator Implementation?,https://www.reddit.com/r/MachineLearning/comments/ajpxmt/lowlevel_tensorflow_or_estimator_implementation/,kiunthmo,1548429069,[removed],0,1,False,self,,,,,
1419,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,0,ajqeui,developereconomics.com,The battle: Tensorflow vs Pytorch,https://www.reddit.com/r/MachineLearning/comments/ajqeui/the_battle_tensorflow_vs_pytorch/,vjmde,1548431978,,0,1,False,default,,,,,
1420,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,1,ajqgpn,edyoda.com,"[Tutorial] Step by step guide detailing how to get started with learning Machine Learning for complete beginners. It covers most of the getting started video and github repo link, simple explanations, and a project. It's free, feel free to share if you find useful.",https://www.reddit.com/r/MachineLearning/comments/ajqgpn/tutorial_step_by_step_guide_detailing_how_to_get/,iamarmaan,1548432282,,0,1,False,default,,,,,
1421,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,1,ajqhxh,self.MachineLearning,Tensorflow Keras Dropout Layer is Broken,https://www.reddit.com/r/MachineLearning/comments/ajqhxh/tensorflow_keras_dropout_layer_is_broken/,r-scholz,1548432478,[removed],0,1,False,self,,,,,
1422,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,1,ajqmq9,self.MachineLearning,[D] tf.keras Dropout layer is broken,https://www.reddit.com/r/MachineLearning/comments/ajqmq9/d_tfkeras_dropout_layer_is_broken/,r-scholz,1548433249,"Hey  just a warning to all of you out there using tf.keras: In version 1.11  or 1.12, it appears that the Dropout layer is broken. When calling  \`model.fit\`, it acts as if it was in the testing phase.

You can reproduce the bug live on the tutorial script [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)

Increment the dropout to 99.9% and be amazed that the network still achieves &gt;95% test acc.

Apparantly the  issue will be fixed in version 1.13 [https://github.com/tensorflow/tensorflow/issues/25175](https://github.com/tensorflow/tensorflow/issues/25175)",63,1,False,self,,,,,
1423,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,1,ajqqc9,self.MachineLearning,AlphaStar - What AI research can achieve with unlimited resources,https://www.reddit.com/r/MachineLearning/comments/ajqqc9/alphastar_what_ai_research_can_achieve_with/,The_Great_Boberto,1548433830,[removed],0,1,False,self,,,,,
1424,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,1,ajqs66,quantamagazine.org,Building a Translator for Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/ajqs66/building_a_translator_for_artificial_intelligence/,jonfla,1548434119,,0,1,False,default,,,,,
1425,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,1,ajr094,self.MachineLearning,"What is the best nature-inspired word, idiom, or method in the fields of Artificial Intelligence, deep learning and machine learning",https://www.reddit.com/r/MachineLearning/comments/ajr094/what_is_the_best_natureinspired_word_idiom_or/,Doctor_who1,1548435442,[removed],0,1,False,self,,,,,
1426,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,2,ajrks1,self.MachineLearning,[P] ML for text formatting,https://www.reddit.com/r/MachineLearning/comments/ajrks1/p_ml_for_text_formatting/,384534,1548438617,"Sup Bois, 

  


Suggest framework for correct text formatting as in sample

  


For ex.

I give database with thousands of proper text formats, neural network training its classificator , and when next time I load unformatted text, it will format it correctly

  


In my company there are some format standards for official documents , with correct Gramma, spaces, etc

  


I want to test it, to optimize format process for co-workers 

  


Thanks. Regards",1,1,False,self,,,,,
1427,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,2,ajrmly,self.MachineLearning,Recent Surveys on Active Learning for CV or NLP?,https://www.reddit.com/r/MachineLearning/comments/ajrmly/recent_surveys_on_active_learning_for_cv_or_nlp/,pigdogsheep,1548438917,[removed],0,1,False,self,,,,,
1428,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,2,ajrnbz,self.MachineLearning,Would using linux sacrifice GPU computing performance?,https://www.reddit.com/r/MachineLearning/comments/ajrnbz/would_using_linux_sacrifice_gpu_computing/,dewhard,1548439029,[removed],0,1,False,self,,,,,
1429,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,3,ajrro3,self.MachineLearning,Advice/guidelines for ML work environment/workflow process,https://www.reddit.com/r/MachineLearning/comments/ajrro3/adviceguidelines_for_ml_work_environmentworkflow/,bandalorian,1548439674,[removed],0,1,False,self,,,,,
1430,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,3,ajrx50,arxiv.org,[1807.02581] The Goldilocks zone: Towards better understanding of neural network loss landscapes,https://www.reddit.com/r/MachineLearning/comments/ajrx50/180702581_the_goldilocks_zone_towards_better/,i-heart-turtles,1548440526,,9,1,False,default,,,,,
1431,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,3,ajryfp,arxiv.org,[R] Adversarial Robustness May Be at Odds With Simplicity,https://www.reddit.com/r/MachineLearning/comments/ajryfp/r_adversarial_robustness_may_be_at_odds_with/,preetum,1548440741,,8,1,False,default,,,,,
1432,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,3,ajs20t,self.MachineLearning,[Q] Why does some times cross entropy loss become -ln(1/NumClass) and gradient norm of the whole network become zero?,https://www.reddit.com/r/MachineLearning/comments/ajs20t/q_why_does_some_times_cross_entropy_loss_become/,dchatterjee172,1548441290,[removed],0,1,False,self,,,,,
1433,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,3,ajs7vk,self.MachineLearning,Cutting Edge Deep Learning: Fast.ai 2019 just launched!,https://www.reddit.com/r/MachineLearning/comments/ajs7vk/cutting_edge_deep_learning_fastai_2019_just/,init__27,1548442196,[removed],0,1,False,self,,,,,
1434,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,3,ajs9w3,self.MachineLearning,What features of Tensorboard/TensorboardX are you actually using?,https://www.reddit.com/r/MachineLearning/comments/ajs9w3/what_features_of_tensorboardtensorboardx_are_you/,ai_yoda,1548442518,[removed],0,1,False,self,,,,,
1435,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,4,ajsd1r,self.MachineLearning,"Deepmind on AlphaStar: ""weve also learnt a lot about how to scale up RL to really large problems with huge action spaces and long time horizons.""",https://www.reddit.com/r/MachineLearning/comments/ajsd1r/deepmind_on_alphastar_weve_also_learnt_a_lot/,Mangalaiii,1548443028,"https://old.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/eexwqup/

Sounds like we're beginning to open the door to AI solving truly open-ended problems.",0,1,False,self,,,,,
1436,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,4,ajsdpx,self.MachineLearning,Reinforcement Learning for Distinct but Dependant Tasks,https://www.reddit.com/r/MachineLearning/comments/ajsdpx/reinforcement_learning_for_distinct_but_dependant/,lantern_lol,1548443128,"Im attempting to write reinforcement learning task where I want to solve two problems which are dependant but not necessarily related.

Let us consider I train a number of agents to play tic-tac-toe. I then want to determine if I should select X or O for each agent; some may perform better with X, and some with O.

I could do some form of black box optimization once each agent is trained, but that seems inefficient is it doesn't make use of information gained in training. 

I have tried multi-task reinforcement learning by sharing network parameters and using a single network to predict both this decision and the game-playing decisions made by the agent. I do this by simply including a binary input feature that tells the network whether this is the first step in the episode; if so, the prediciton of which player to select will be used; otherwise, the predicted actions will be taken.

I think this is an incorrect approach as (1) I do not get convergence and (2) the tasks have different action + observation spaces.

However I also think that simply performing an optimization task on top of a learned agent is not efficient at all as it does not exploit the context of the agent during training. Note that tic-tac-toe is just analagous to my actual problem; both agents will have much more complex action spaces.

Does anyone have any ideas as to how I can approach such a problem?",0,1,False,self,,,,,
1437,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,4,ajsii2,self.MachineLearning,[D] Reinforcement Learning for Distinct but Dependant Tasks,https://www.reddit.com/r/MachineLearning/comments/ajsii2/d_reinforcement_learning_for_distinct_but/,lantern_lol,1548443929,"Im attempting to write reinforcement learning task where I want to solve two problems which are dependant but not necessarily related.

Let us consider I train a number of agents to play tic-tac-toe. I then want to determine if I should select X or O for each agent; some may perform better with X, and some with O, and so the action of the player predictor inherently depends on the agent.

I could do some form of black box optimization once each agent is trained, but that seems inefficient is it doesn't make use of information gained in training.

I have also tried multi-task reinforcement learning by sharing network parameters and using a single network to predict both this decision and the game-playing decisions made by the agent. I do this by simply including a binary input feature that tells the network whether this is the first step in the episode; if so, the prediction of which player to select will be used; otherwise, the predicted actions will be taken.

I think this is an incorrect approach as (1) I do not get convergence and (2) the tasks have different action + observation spaces.

However I also think that simply performing an optimization task on top of a learned agent is not efficient at all as it does not exploit the context of the agent during training. Note that tic-tac-toe is just analagous to my actual problem; both agents will have much more complex action spaces.

Another approach may be to form a supervised learning task trained in parallel, using results of training to make the decision as to whether X or O should be selected; however the learned distribution would undoubtedly change as the agent learns, and I am concerned this would cause issues.

Does anyone have any ideas as to how I can approach such a problem?",0,1,False,self,,,,,
1438,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,4,ajswes,self.MachineLearning,Any information about Function Saturation?,https://www.reddit.com/r/MachineLearning/comments/ajswes/any_information_about_function_saturation/,Jandevries101,1548446141,[removed],0,1,False,self,,,,,
1439,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,4,ajsxac,julialang.org,[P] DiffEqFlux.jl  A Julia Library for Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/ajsxac/p_diffeqfluxjl_a_julia_library_for_neural/,UncleOxidant,1548446287,,0,1,False,default,,,,,
1440,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,5,ajszqs,self.MachineLearning,"Awesome Deep Learning, Machine Learning, and Reinforcement Learning Lectures !!!",https://www.reddit.com/r/MachineLearning/comments/ajszqs/awesome_deep_learning_machine_learning_and/,kmario23,1548446666,[removed],0,1,False,self,,,,,
1441,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,5,ajt3bx,self.MachineLearning,Seeking advice in regards to ML and Quantitative Finance,https://www.reddit.com/r/MachineLearning/comments/ajt3bx/seeking_advice_in_regards_to_ml_and_quantitative/,GrandMastaJayy,1548447236,[removed],0,1,False,self,,,,,
1442,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,5,ajt4hi,self.MachineLearning,FPGA vs GPU for fast inference,https://www.reddit.com/r/MachineLearning/comments/ajt4hi/fpga_vs_gpu_for_fast_inference/,abeecrombie,1548447420,[removed],0,1,False,self,,,,,
1443,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,5,ajt90m,self.MachineLearning,Is the FPGA SOC fast enough for a CNN running in a bare metal environment using NEON intrinsics?,https://www.reddit.com/r/MachineLearning/comments/ajt90m/is_the_fpga_soc_fast_enough_for_a_cnn_running_in/,leepa2018,1548448167,[removed],0,1,False,self,,,,,
1444,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,5,ajtac3,self.smalltalk,Looking for ideas for using reflection/introspection for statistical programming [X-post r/smalltalk][Discussion],https://www.reddit.com/r/MachineLearning/comments/ajtac3/looking_for_ideas_for_using/,Raoul314,1548448384,,0,1,False,default,,,,,
1445,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,5,ajtdyo,arxiv.org,[Research] Interpretable Neural Networks for Predicting Mortality Risk using Multi-modal Electronic Health Records,https://www.reddit.com/r/MachineLearning/comments/ajtdyo/research_interpretable_neural_networks_for/,aulloa,1548448967,,1,1,False,default,,,,,
1446,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,8,ajv0cf,self.MachineLearning,Anyone working on application of GANs to infer demographics from street view?,https://www.reddit.com/r/MachineLearning/comments/ajv0cf/anyone_working_on_application_of_gans_to_infer/,Many_Consideration,1548458143,[removed],0,1,False,self,,,,,
1447,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,8,ajv7wp,self.MachineLearning,"[D] Deepening your theoretical knowledge of DL, ML, and RL",https://www.reddit.com/r/MachineLearning/comments/ajv7wp/d_deepening_your_theoretical_knowledge_of_dl_ml/,kmario23,1548459416,"Are you guys interested in making your theoretical foundations of Deep Learning, Machine Learning, and Reinforcement Learning? Then look no further! Here's the good news. I have compiled list of awesome lectures starting from 2012-till  date, and the list is continuously evolving.

Please find the courses in my GitHub repo [Deep Learning Drizzle](https://github.com/kmario23/deep-learning-drizzle). You're welcome to share it with anyone who might be curious to know the techniques in depth. Feel free to send a PR if you have some suggestions!

GitHub repo: [Deep Learning Drizzle](https://github.com/kmario23/deep-learning-drizzle)

Have a fun &amp; nice weekend!",19,1,False,self,,,,,
1448,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,9,ajvu7g,self.MachineLearning,Python demo implementation of Automatic Differentiation,https://www.reddit.com/r/MachineLearning/comments/ajvu7g/python_demo_implementation_of_automatic/,Ikuyas,1548463306,"Do any of you have any good idea to demonstrate the automatic differentiation in python for the teaching purpose or giving as a homework assignment for the college math senior class? I found two ways using annotation and using a data type which is a class with two fields, value and the derivative. I think what Tensorflow does is source code transformation but I want to do it mathematical way and using a graph is too much foreign to math students. ",0,1,False,self,,,,,
1449,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,12,ajx8te,self.MachineLearning,Uber AI Residency challenge,https://www.reddit.com/r/MachineLearning/comments/ajx8te/uber_ai_residency_challenge/,stateless_,1548473318,[removed],0,1,False,self,,,,,
1450,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,14,ajyc2z,self.MachineLearning,[P] ENet - Real Time Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/ajyc2z/p_enet_real_time_semantic_segmentation/,iArunava,1548482246,"ENet - Real Time Semantic Segmentation. Paper Reproduced. And Github structured w/ Pretrained model

With a colab notebook to reproduce it in one go!

Repo: [https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation](https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation)

Colab: [https://colab.research.google.com/github/iArunava/ENet-Real-Time-Semantic-Segmentation/blob/master/ENet-Real%20Time%20Semantic%20Segmentation.ipynb](https://colab.research.google.com/github/iArunava/ENet-Real-Time-Semantic-Segmentation/blob/master/ENet-Real%20Time%20Semantic%20Segmentation.ipynb)

Paper: [https://arxiv.org/pdf/1606.02147.pdf](https://arxiv.org/pdf/1606.02147.pdf)",20,1,False,self,,,,,
1451,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,15,ajyiqp,self.MachineLearning,Mutilayer sequence to sequence LSTM is Broken,https://www.reddit.com/r/MachineLearning/comments/ajyiqp/mutilayer_sequence_to_sequence_lstm_is_broken/,jowanga,1548483961,[removed],0,1,False,self,,,,,
1452,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,15,ajypm5,self.MachineLearning,which laptop should i purchase macbook air or acer predator,https://www.reddit.com/r/MachineLearning/comments/ajypm5/which_laptop_should_i_purchase_macbook_air_or/,cudanexus,1548485838,[removed],0,1,False,self,,,,,
1453,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,16,ajyrn7,self.MachineLearning,How to set up a Q function approximator using neural net for DDPG?,https://www.reddit.com/r/MachineLearning/comments/ajyrn7/how_to_set_up_a_q_function_approximator_using/,qudcjf7928,1548486401,"&amp;#x200B;

*Processing img 6b5vy36bspc21...*

&amp;#x200B;

For discrete action space, I thought the ""conventional method"" is to set up the neural network in such a way that the inputs are the states and each of the output node represents possible action with its value being the Q-value for that action, hence the optimal action is the action with the highest Q-value, that is, output node with the biggest value. 

&amp;#x200B;

But for deep deterministic policy gradient, how to set up the Q function network in such a way that it is a function of the current state (or next state...etc) AND is also a function of the continuous output of the policy network? (like in the eqn 12 ,  14?) 

&amp;#x200B;

Judging from the algorithm, I would need additional input node that takes in the continuous output from the policy network and 1 output node for the Q value for its given state and action. Is that a correct interpretation? 

&amp;#x200B;

Also, what about ""multi-dimensional"" action? So instead of the policy network having 1 output node with its continuous output, but rather a multiple output nodes ( which is useful for a robotic arm's angular velocity in X-axis, Y-axis.....etc) 

&amp;#x200B;

Could I do the same but just extend the dimensions of the neural net? 

&amp;#x200B;

Has anyone had experience with DDPG? Since I would need to store the replay buffer and have 2 additional network for target policy and target Q, I'd assume it would require a lot of memory. How is the overall training speed as well? 

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1454,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,16,ajyuiw,self.MachineLearning,Can anybody suggest Machine Learning courses/books which include all the complex mathematics involved?,https://www.reddit.com/r/MachineLearning/comments/ajyuiw/can_anybody_suggest_machine_learning_coursesbooks/,Somil112,1548487238,[removed],0,1,False,self,,,,,
1455,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,16,ajyy28,self.MachineLearning,Gender Estimation from profile picture,https://www.reddit.com/r/MachineLearning/comments/ajyy28/gender_estimation_from_profile_picture/,R4rathee,1548488312,"Hi Friends,

This is my first post on Reddit. Please bear with me :)

I've around 6k photos from Linkedin profiles and I want to guess the gender based on the profile photos.

Can anybody help me with python package or any other efficient method to determine the gender with good accuracy for all images in one go?

Regards,
Ombir",0,1,False,self,,,,,
1456,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,16,ajyypj,self.MachineLearning,[D] Better then adam?,https://www.reddit.com/r/MachineLearning/comments/ajyypj/d_better_then_adam/,hadaev,1548488515,"Did humanity invented some noticeable better optimizers then adam?

&amp;#x200B;

I tried AdafactorOptimizer from tensor2tensor and my gan show worse results.",9,1,False,self,,,,,
1457,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,17,ajz27x,self.MachineLearning,Article on markov decision process,https://www.reddit.com/r/MachineLearning/comments/ajz27x/article_on_markov_decision_process/,sanchit2843,1548489616,[removed],0,1,False,self,,,,,
1458,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,18,ajzje3,self.MachineLearning,[D] Analysis: The Era of Deep Learning Is Coming to an End,https://www.reddit.com/r/MachineLearning/comments/ajzje3/d_analysis_the_era_of_deep_learning_is_coming_to/,RedEyed__,1548495136,[removed],0,1,False,self,,,,,
1459,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,18,ajzkyp,arxiv.org,"[R] [1901.08164] ""Decoupled Greedy Learning of CNNs"" &lt;-- The Overthrower of Backprop?",https://www.reddit.com/r/MachineLearning/comments/ajzkyp/r_190108164_decoupled_greedy_learning_of_cnns_the/,evc123,1548495662,,10,1,False,default,,,,,
1460,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,18,ajzkzx,self.MachineLearning,Training vgg19 on larger images (eg. 1024x1024px),https://www.reddit.com/r/MachineLearning/comments/ajzkzx/training_vgg19_on_larger_images_eg_1024x1024px/,8solutions,1548495674,[removed],0,1,False,self,,,,,
1461,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,19,ajzugx,self.MachineLearning,How to interpret perplexity in language modelling,https://www.reddit.com/r/MachineLearning/comments/ajzugx/how_to_interpret_perplexity_in_language_modelling/,csnlpstudent,1548498752,[removed],0,1,False,self,,,,,
1462,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,19,ajzwxy,icra2019.org,[D] ICRA decisions are out,https://www.reddit.com/r/MachineLearning/comments/ajzwxy/d_icra_decisions_are_out/,jeasinema,1548499559,,0,1,False,default,,,,,
1463,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,20,ajzzrw,self.MachineLearning,Must read books to study for ML,https://www.reddit.com/r/MachineLearning/comments/ajzzrw/must_read_books_to_study_for_ml/,dchasani,1548500488,[removed],0,1,False,self,,,,,
1464,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,20,ak00wq,self.MachineLearning,Segmentation for detecting defects ?,https://www.reddit.com/r/MachineLearning/comments/ak00wq/segmentation_for_detecting_defects/,emfisabitch,1548500856,[removed],0,1,False,self,,,,,
1465,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,20,ak04le,self.MachineLearning,Introduction to Tensor for Deeplearning,https://www.reddit.com/r/MachineLearning/comments/ak04le/introduction_to_tensor_for_deeplearning/,imHarin,1548502063,[removed],0,1,False,self,,,,,
1466,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,21,ak0b4w,self.MachineLearning,Machine learning,https://www.reddit.com/r/MachineLearning/comments/ak0b4w/machine_learning/,AkshayGB00123,1548504075,"I want to learn Machine learning from scratch ? 
Can you suggest me good papers to learn what is machine learning ? And all technologies ? ",0,1,False,self,,,,,
1467,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,21,ak0c2u,journals.aps.org,Creating the most accurate model of black hole mergers with the aid of supercomputers and artificial intelligence.,https://www.reddit.com/r/MachineLearning/comments/ak0c2u/creating_the_most_accurate_model_of_black_hole/,MistWeaver80,1548504327,,0,1,False,https://b.thumbs.redditmedia.com/NnbGNkSjUJzUt13fnr6aEJR4HSz5ceYUWjcIL4VpDJQ.jpg,,,,,
1468,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,21,ak0eqk,self.MachineLearning,Book on Operations research,https://www.reddit.com/r/MachineLearning/comments/ak0eqk/book_on_operations_research/,PyWarrior,1548505100,[removed],0,1,False,self,,,,,
1469,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,21,ak0m6c,self.MachineLearning,An idea about rewarding a neural net for predicting other neural nets,https://www.reddit.com/r/MachineLearning/comments/ak0m6c/an_idea_about_rewarding_a_neural_net_for/,antor_thegreat,1548507266,[removed],0,1,False,self,,,,,
1470,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,22,ak0puj,self.MachineLearning,query in text mining. pls help,https://www.reddit.com/r/MachineLearning/comments/ak0puj/query_in_text_mining_pls_help/,SakshiRajput041,1548508273,[removed],0,1,False,self,,,,,
1471,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,22,ak0yqk,self.MachineLearning,Help with Novelty Detection and Binary Classification for Emotion Recognition,https://www.reddit.com/r/MachineLearning/comments/ak0yqk/help_with_novelty_detection_and_binary/,titogannon,1548510450,[removed],0,1,False,self,,,,,
1472,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,23,ak188g,self.MachineLearning,[R] Help with Novelty Recognition and Binary Classification for Emotion Recognition,https://www.reddit.com/r/MachineLearning/comments/ak188g/r_help_with_novelty_recognition_and_binary/,titogannon,1548512538,"Hello /r/MLQuestions!

&amp;#x200B;

Im looking for advice regarding my ML project.

&amp;#x200B;

Using a special wristband, I am able to collect a bunch of physiological data from human subjects. I want to develop an application to recognize when these physiological signals change in a meaningful way and only then ask the user how he/she is feeling. This data will later be used for machine learning testing. The problem is, that I am struggling to find appropriate ways to classify current data input as meaningful and ask for information only when relevant user input is to be gathered, not more and not less.

&amp;#x200B;

For me, this seems to be a [novelty detection](https://en.wikipedia.org/wiki/Novelty_detection) problem, combined with a [binary classification](https://en.wikipedia.org/wiki/Binary_classification) problem. I have to recognize what values coming from the data stream are to be considered *normal*, and therefore not bother the user with unnecessary input requests. I would also use novelty detection to recognize the data coming out of the normal zone and ask the user about it. This new data is then not considered novelty anymore, and binary classification will tell if the user is to be asked about his emotions when getting the same data in the future.

&amp;#x200B;

So, these are my questions:

\-	What do you think about my reasoning of the problem? Do you have other perspectives on how to handle these problems? I have been told this could also be considered an [anomaly detection](https://en.wikipedia.org/wiki/Anomaly_detection) problem, for example.

\-	What algorithms would you use to separate normal from more meaningful physiological data? [Support Vector Machines](https://en.wikipedia.org/wiki/Support-vector_machine) perhaps? Maybe some [decision theory](https://en.wikipedia.org/wiki/Decision_theory)?

\-	Do you know any books or papers on similar matters? Even if I have found some after hours and hours of research, you may be able to point me to something different than those I have.

&amp;#x200B;

It is worth noting that data collection is supposed to be done when no other factors are messing with signal readings, such as sport.

&amp;#x200B;

Any help would be much appreciated.

&amp;#x200B;

Best regards,

Titogannon

&amp;#x200B;

&amp;#x200B;",2,1,False,self,,,,,
1473,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,23,ak1gxc,arxiv.org,"""Training Neural Networks with Local Error Signals""",https://www.reddit.com/r/MachineLearning/comments/ak1gxc/training_neural_networks_with_local_error_signals/,PuzzleheadedReality9,1548514408,,11,1,False,default,,,,,
1474,MachineLearning,t5_2r3gv,2019-1-26,2019,1,26,23,ak1if1,arxiv.org,[R] Maximum Entropy Generators for Energy-Based Models,https://www.reddit.com/r/MachineLearning/comments/ak1if1/r_maximum_entropy_generators_for_energybased/,EquivalentFoundation,1548514733,,2,1,False,default,,,,,
1475,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,0,ak1rkq,self.MachineLearning,"Is there a paper or research from Doctor Who 's AI series? If yes, please send the article link.",https://www.reddit.com/r/MachineLearning/comments/ak1rkq/is_there_a_paper_or_research_from_doctor_who_s_ai/,Doctor_who1,1548516554,[removed],0,1,False,self,,,,,
1476,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,0,ak1so6,self.MachineLearning,Suggestions for approaching Autocomplete in ecommerce,https://www.reddit.com/r/MachineLearning/comments/ak1so6/suggestions_for_approaching_autocomplete_in/,dileep31,1548516764,"Hello everyone

&amp;#x200B;

I am looking at improving the autocomplete feature in search box of our ecommerce site. In our current approach, we  identify important ""queries"" that our users enter (based on how popular they are) and create a static index (using a datastructure called TRIE). In real-time, there is a look up done on this TRIE and suggestions are shown to the user.

&amp;#x200B;

The current model relies heavily on offline computation and we see typically only around 40% of our queries coming from type ahead. I am wondering if there are any standard approaches that follows real-time computation in typeahead/autocomplete. 

&amp;#x200B;

Can someone who worked on similar work earlier point to some direction? 

&amp;#x200B;

&amp;#x200B;",0,1,False,self,,,,,
1477,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,0,ak1vfj,self.MachineLearning,[D]How large dataset is required for Speech-To-Text recognition ?,https://www.reddit.com/r/MachineLearning/comments/ak1vfj/dhow_large_dataset_is_required_for_speechtotext/,SaltyStackSmasher,1548517290,"Hello,

&amp;#x200B;

I am planning to build a Speech-To-Text for my language using deep learning.

Looking at Mozilla's DeepSpeech implementation, I was curious just how large of a dataset will I need for such end-to-end approach ? I have found a dataset  with around \~40000 labeled terms in my language. Is that large enough for DeepSpeech ? Or should I switch to some other method like HTK  ? Which models may work better for such datasets ?

&amp;#x200B;

Thanks in advanced",7,1,False,self,,,,,
1478,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,0,ak1xnc,arxiv.org,[1901.08162] Causal Reasoning from Meta-reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/ak1xnc/190108162_causal_reasoning_from_metareinforcement/,joshuacpeterson,1548517689,,1,1,False,default,,,,,
1479,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,2,ak2s6q,self.MachineLearning,Timeseries prediction,https://www.reddit.com/r/MachineLearning/comments/ak2s6q/timeseries_prediction/,khizanov,1548523050,[removed],0,1,False,self,,,,,
1480,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,2,ak30kp,self.MachineLearning,Is there an English transcript for Lex Fridman 's podcast in English?,https://www.reddit.com/r/MachineLearning/comments/ak30kp/is_there_an_english_transcript_for_lex_fridman_s/,Doctor_who1,1548524441,[removed],0,1,False,self,,,,,
1481,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,3,ak39mj,self.MachineLearning,"If you wanted to make a long term bet on Machine Learning, what companies would you invest in?",https://www.reddit.com/r/MachineLearning/comments/ak39mj/if_you_wanted_to_make_a_long_term_bet_on_machine/,SuchRush,1548525950,[removed],0,1,False,self,,,,,
1482,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,3,ak3ov6,self.MachineLearning,Reinforcement Learning Research Frontier,https://www.reddit.com/r/MachineLearning/comments/ak3ov6/reinforcement_learning_research_frontier/,Zizanno,1548528452,[removed],0,1,False,self,,,,,
1483,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,4,ak3v4i,self.MachineLearning,[D] An analysis on how AlphaStar's superhuman speed is a band-aid fix for the limitations of imitation learning.,https://www.reddit.com/r/MachineLearning/comments/ak3v4i/d_an_analysis_on_how_alphastars_superhuman_speed/,SoulDrivenOlives,1548529476,"I will try to make a convincing argument for the following:

1. AlphaStar played with superhuman speed and precision. 
2. Deepmind claimed to have restricted the AI from performing actions that would be physically impossible to a human. They have not succeeded in this and most likely are aware of it.
3. **The reason why AlphaStar is performing at superhuman speeds is most likely due to it's inability to unlearn the human players tendency to spam click. I suspect Deepmind wanted to restrict it to a more human like performance but are simply not able to.** It's going to take us some time to work our way to this point but it is the whole reason why I'm writing this so I ask you to have patience.

First of all I want to clarify that I am a layman. I've been following AI and the Starcraft 2 scene for years but I do not claim to be an expert in either topic. If you notice any misconceptions in what I'm about write please do point it out. I'm only a fanboy and all this is fascinating to me. This essay contains a lot of speculation and I will admit that I can't prove all of my core claims definitively. Having said that, if you are so kind to read all of this and disagree with me, please argue in good faith. I would love to be proven wrong. Ok, here we go.

# The Superhuman Speed of AlphaStar

&gt;David Silver*:* [*""AlphaStar can't react faster than a human player can, nor can it execute more clicks than a human player"".*](https://www.youtube.com/watch?v=UuhECwm31dM&amp;t=99s)

The Starcraft 2 scene was dominated in 2018 by a player called Serral. He is the current world champion and won 7 out of 9 major tournaments he attended that year resulting in the single most dominant run of any Starcraft 2 player in the history of the game. This guy is fast. Maybe the fastest player in the world. 

[Here's a first person view of him playing.](https://www.youtube.com/watch?v=HRsDAX8DfBw&amp;t=780) Serral is the pinkish white player. Take a look at his APM displayed in the upper left corner of the screen. APM is short for actions per minute. Basically it is a number that represents how fast the player is clicking on his mouse and keyboard. At no point is he able to sustain more than 500 APM for long. There are burst of 800 but they last only fractions of a second and are most likely resulted from spam clicking, which I will be discussing shortly.

While arguably the fastest human player is able to sustain an impressive 500 APM, AlphaStar had bursts going up to 1500+. These inhumane 1000+ APM bursts sometimes lasted for 5 seconds stretches and were full of meaningful actions. 1500 actions in a minute translates to 25 actions a second. This is physically impossible to do. I also want you to take into account that in a game of Starcraft 5 seconds is a long time, especially at the very beginning of a big battle. If the superhuman execution during the first 5 seconds gives the AI an upper hand it will win the engagement by a large margin because of the snowball effect. [An engagement in game 3 vs Mana. AlphaStar sustain a 1000+ APM over a period of 5 seconds.](https://www.youtube.com/watch?v=H3MCb4W7-kM&amp;t=59m30s) Another one in game 4 had bursts that go up to [1500+ APM.](https://www.youtube.com/watch?v=cUTMhmVh1qs&amp;t=2h11m32s) One of the commentators points out how the average APM is still acceptable but it is quite clear that the sustained bursts are way higher than what a human could do. 

# Spam-clicks, APM and the Surgical Precision of a Robot

Most human players have a tendency to spam click. Spam-clicks are exactly what they sound like. Meaningless clicks that don't have an effect on anything. For example, a human being might be moving his army and when they click to where they want the army to go, they click more than once. What effect does this have? Nothing. The army won't walk any faster. A single click would have been enough. Why do they do it then? There are two reasons:

1. Spam-clicking is the natural by-product of a human being trying to click around as fast as possible.
2. It helps to warm up finger muscles.

Remember that player Serral we talked about earlier? The impressive thing about him is actually not how fast he is clicking but how precise he is. Serral not only has a really high APM (the total clicks per minute, including spam-clicks) but also a ridiculously high effective-APM (the total clicks per minute, excluding spam-clicks). 

[Here's a former proplayer losing his mind on twitter after discovering the effective-APM of Serral](https://twitter.com/retjah/status/959047573930696704?lang=fi). An E-APM that high is practically unheard of. 344 is so much that I have hard time believing it. But wait. If AlphaStar can potentially play without spam, wouldn't this mean that it's peak E-APM could be equal to it's peak APM? This makes the 1000+ spikes even more inhumane. Now also take into consideration that AlphaStar plays with perfect accuracy. It always clicks exactly where it intends to. Humans missclick. 

There is a clear, almost unanimous consensus among the Starcraft 2 scene that AlphaStar performed sequences that no human could ever hope to replicate. It was faster and more precise than what is physically possible. The most mechanically impressive human pro in the world is several times slower. The accuracy can't even be compared. 

**David Silver's claim that AlphaStar can't execute anything human player a human being can't replicate is simply not true.** 

# Doing Things the Right Way VS Doing Things The Fast Way

&gt;OriolVinyals:  It is important that we play the games that we created and collectively agreed on by the community as grand challenges . We are trying to build intelligent systems that develop the amazing learning capabilities that we possess, so it is indeed desirable to make our systems learn in a way thats as human-like as possible. As cool as it may sound to push a game to its limits by, for example, playing at very high APMs, that doesnt really help us measure our agents capabilities and progress, making the benchmark useless. 

Why is Deepmind interested in restricting the agent to play like a human? Why not just let it run wild with no limitations? The reason is that Starcraft 2 is a game that can be broken by mechanical perfection. [In this video](https://www.youtube.com/watch?v=IKVFZ28ybQs) a bot attacks a group of tanks with perfect micro. When the unit control is this good, the AI doesn't even need to learn strategy. The purpose of this exercise is not lost on me. Deepmind is not necessarily interested in creating an AI that can beat Starcraft but advancing AI research as a whole.  It is deeply unsatisfying to have prominent members of this research project make claims of human-like mechanical limitations when the agent is very obviously breaking them and winning it's games specifically because it is demonstrating superhuman execution. 

AlphaStar is able to outperform human players with unit control that was not taken into consideration when the game developer was carefully balancing the game. This can obfuscate any strategic thinking the AI has learned. It can even make the strategic thinking completely unnecessary. This is not the same thing as being stuck in a local maxima. **Abusing superior control is very likely to be** **the best and most effective way to play the game** as disappointing as that sounds. 

This is what one of the pros who played AlphaStar had to say about it's strengths and weaknesses:

&gt; Mana: I would say that clearly the best aspect of its game is the unit control. In all of the games when we had a similar unit count, AlphaStar came victorious. The worst aspect from the few games that we were able to play was its stubbornness to tech up. It was so convinced to win with basic units that it barely made anything else and eventually in the exhibition match that did not work out. There werent many crucial decision making moments so I would say its mechanics were the reason for victory. 

There's almost unanimous consensus among Starcraft fans that AlphaStar won almost purely because of it's superhuman speed, reaction times and accuracy. The pro who played against seems to agree. There was a member of the Deepmind team who played against AlphaStar before they let the pros test it. Most likely he would agree with the assesment. David Silver and Oriol Vinyal keep repeating the mantra of how the bot is only able to do things that a human could do as well. 

This does not sound like doing things the right way. Something about this is really sketchy.

# Why Does Deepmind Allow AlphaStar to Have Super-Human Mechanical Ability?

Now we finally get to the meat and potatoes of this essay. First let's recap.

* We know what APM, E-APM and spam-clicking are. 
* We have a rudimentary understanding of what the upper limits of human play looks like. 
* We understand that AlphaStars gameplay is in direct contradiction to what the developers claim it was allowed to be able to execute. 
* We understand that the consensus among Starcraft scene is that the unit control won the games and that strategic thinking wasn't even needed. 
* We understand that the goal of Deepmind is not to create a microbot or abuse the game in ways it was never meant to be played like. 
* It is incredibly unlikely that no one in Deepmind's Starcraft AI team thought burst APM of 1500+ were unrealistic. Their Starcraft guy knows more about the game than I do. They are close with Blizzard. It is in their interest (see the previous bullet point) to make the bot act as close to a human as possible.  


Thank you for sticking with me for this long. Here's what I suspect happened:

1) At the very start of the project Deepmind agrees upon heavy APM restrictions on AlphaStar. At this point the agent is not allowed to have superhuman bursts of speed we saw in the demonstration. These restrictions most likely take into account.

* **The average APM across the whole game.**
* **Maximum burst APM over a short period of time.** I think capping it around 4-6 clicks per second would be reasonable. Remember Serral and his 344 E-APM that was head and shoulders above the competition? That is less than 6 clicks per second. The AlphaStar that played against Mana was able to perform 25 clicks per second over sustained periods of time. This is so much faster than even the fastest spam clicks a human can do that I don't think the original restrictions allowed for it. 
* **Minimum time between clicks.** Even if the speed bursts of the bot were capped, it could still perform almost instantaneous actions at the beginning of the time slice in an inhuman way. A human being obviously could not do this.  

Something like that. Some people would argue for adding a random element on accuracy as well but I suspect that would hinder the training way too much.

2) Next Deepmind downloads thousands of high-ranking amateur games and begins imitation learning. At this stage the agent is simply trying to imitate what humans do in games. 

3) The agent adopts a behaviorof spam-clicking. This is highly likely becausehuman players spam click so much during games. It is almost certainly the single most repeated pattern of action that humans perform and thus would most likely root itself very deeply into the behavior of the agent.

4) AlphaStars maximum burst APM is restricted close to how fast a human spam clicks. **Because most of the actions Alphastar is executing are spam clicks, it does not have the APM available to experiment in fights.**If the agent doesn't experiment, it won'tlearn. Here's what one of the developers said in an AMA yesterday, I think he tipped his hand: 

&gt;OriolVinyals: Training an AI to play with low APM is quite interesting. In the early days, we had agents trained with very low APMs, but they did not micro at all.  

5) In order to speed up development they change APM restrictions to allow high bursts. Here are the APM restrictions that AlphaStar was playing in the demonstration:

&gt;OriolVinyals:  In particular, we set a maximum of 600 APMs over 5 second periods, 400 over 15 second periods, 320 over 30 second periods, and 300 over 60 second period. If the agent issues more actions in such periods, we drop / ignore the actions. These were values taken from human statistics. 

This is the equivalent of lying through statistics. It looks reasonable, but it allows for superhuman execution.

There's a limit to how fast a human can spam click. The most typical form of spam clicking is issuing a movement or attack commands to a unit. This is done by clicking with your mouse. Try clicking your mouse as fast as you can. The agentlearnt that kind ofspam clicking. It would not be clicking faster because the humans it is imitating are not clicking faster. **The extra APM that allows itto go to superhuman speedscan be considered ""free""APMwich it can experiment with.**  

6) The free APMis used to experiment in engagements. This kind of interaction would happen often while training.AlphaStar starts to learn new kind of behavior that leads to better outcomes and it starts to break away from constant spam clicking.   

7) If the agent learned actual useful actions why then didn't Deepmind go back to the speculated initial harsher, humanlike limitations on APM? It couldn't be because they didn't realize that what the bot was doing was superhuman. The Starcraft community hasalmost unanimous consensus that AlphaStar had superhuman micro. The human pros said in the ama that AlphaStars greatest strength was it's unit control and greatest weakness it's strategic thinking. The Starcraft people within Deepmind's team must have been thinking the same. The reason is probably because the agent still occasionally displays spam clicking. Even though based on analyzing the games it seems to be able to execute crisply with very little spam, it still regularly engage in it. This is apparent in game 1 against Mana when Alphastar is moving up the ramp. The agent was spamclicking movement commands. It still had not unlearned it. The spam clicking would hurt the agent most during big engagements and the APM cap was tinkered to allow it to perform well even in those. 

# You Wrote a Damn Book, Why Do You Even Care OP?

So there you have it. The agent was not able to un-learn spam clicking and Deepmind had to tinker with the APM cap to allow experimentation. This had unfortunate side effect of superhuman execution wich resulted in the agent breaking the game by being able to execute strategies that were never intended to be possible in the game. 

What leaves the sourest taste in my mouth is this image: 

https://i.redd.it/ctuungv1dtc21.png

It seems to be designed to mislead people unfamiliar with Starcraft 2. It seems to be designed to show how the APM of AlphaStar was reasonable. Look at Mana's APM and compare that to AlphaStar. While the mean of Mana is higher, the tail of AlphaStar goes way above what any human is capable of doing with any kind of intent of precision. Notice how Mana's peak APM is around 750 while AlphaStar is above 1500. Now take into account that Mana's 750 is almost 50% spamclicks while AlphaStars is a lot less spammy. 

Now take a look at TLO's APM. The tail goes up to 2000's. Think about that for a second. How is that even possible? It is made possible by a trick called [rapid fire](https://www.youtube.com/watch?v=N2dNK-mIWkQ&amp;t=1m25s). TLO is not clicking super fast. He is holding down a button and the game is registering this as 2000 APM. The only thing you can do with rapid fire is to spam a spell. That's it. TLO just over-uses it for some reason. The neat little effect is that this is masking AlphaStars burst APM and making it look reasonable to people who are not familiar with Starcraft.

This is literally lying through statistics. ",305,1,False,self,,,,,
1484,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,4,ak3vz2,youtu.be,I made a 2D car game AI that evades incoming cars,https://www.reddit.com/r/MachineLearning/comments/ak3vz2/i_made_a_2d_car_game_ai_that_evades_incoming_cars/,DJ_Roby,1548529603,,0,1,False,https://b.thumbs.redditmedia.com/CDObILJuO5fzawNTEucm6WFhUeFN7HVm7rrMYxeY-PU.jpg,,,,,
1485,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,4,ak3xu4,self.MachineLearning,[D] Match-ups between DeepMind's AI's and humans have been quite limited. Human opponents have not had the opportunity to play with or study their DeepMind opponents prior to competition. What are the benefits and disadvantages of DeepMind's approach to outside competitors?,https://www.reddit.com/r/MachineLearning/comments/ak3xu4/d_matchups_between_deepminds_ais_and_humans_have/,ILikeBigButtss,1548529911,"What DeepMind has done with AlphaGo and it's successors is certainly been quite impressive. AlphaGo Zero and Alpha Zero should be quite a bit stronger than any human opponent based on its results against other AI's. However it's hard for me to consider that computers have surpassed humans in go in the way that they have surpassed humans in chess due to the lack of high level games between top humans and the DeepMind programs. AlphaGo Zero is supposedly far stronger than human players despite no exposure to human play during it's training. However, AlphaGo Zero's only opponents seem to be other DeepMind AI's. It would seem to me that play against humans who have had time to study and learn it's potential weaknesses would be a critical part of its testing. It's quite possible that all of the DeepMind AI's have some common shortcomings that could be found and exploited if humans were given more opportunity to study them before the match.

The competitions that have occurred between DeepMind and professional go players feel much more like the [Deep Blue vs Kasparov](https://en.m.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov) matches in 1996 and 1997 than the 2002 [Brains in Bahrain] (https://en.m.wikipedia.org/wiki/Brains_in_Bahrain) matches between Vladimir Kramnik and Deep Fritz or the 2006 rematch between Kramnik and Deep Fritz. DeepBlue showed that the computers were reaching the level of top humans, but it wasn't until later that computers surpassed the top humans.

Deep Blue was a completely new and unfamiliar opponent for Kasparov and ran on [a purpose built super computer] (https://www.chessprogramming.org/Deep_Blue). The 1997 version of Deep Blue was one of the [500 fastest computers in the world at the time.](https://www.top500.org/list/1997/06/300/?page=3) AlphaGo was similarly unfamiliar to is human opponents and ran on a multi-million dollar custom built machine. The purpose of these matches was to demonstrate that a computer could indeed compete with top humans, thus massive hardware and the human's unfamiliarity with the program were perfectly fine.

In later human vs computer matches, the computers hardware became much more limited, the human opponent was given significant time to analyze and practice against the AI, and the human was given some specific special treatment in the match rules. For Brains in Bahrain, [Deep Fritz ran on a machine powered by 8 Pentium CPUs](https://www.baltimoresun.com/news/bs-xpm-2001-08-01-0108010132-story,amp.html). This was certainly a very powerful computer at the time, but was significantly less expensive and more commercially available than the behemoth that was DeepBlue. Months before the match, Kramnik was given a copy of the software and hardware that he would be playing against. He had the ability to become very familiar with his opponent and develop strategies to exploit weaknesses. [The rules for the match provided Kramnik with special advantages] (https://www.theguardian.com/technology/2002/oct/24/games.onlinesupplement2), including the ability to pause the match until the next day at the 56th move and use his own copy of the software to analyze the position and develop his strategy going forward.

DeepMind hasn't given any human the opportunity to extensively study and probe their machine challenger before competition. It is quite possible that these AI's have glaring holes in their game and play poorly in certain situations that humans are better at.

For an AI that claims to have surpassed human intelligence with no exposure to human play, it seems odd that it has never even matched up against a top human. Beating the best humans with an AI that is completely self taught would be one of the most important achievements in AI research to date, but no actual games have been played to demonstrate that this is indeed the case. DeepMind seems to just treat it as a forgone conclusion based on performance against AI's that have beaten humans in limited play.

This could very well be due to relative lack of direct business competitors for DeepMind. There were several strong chess engines developed by different teams and organizations that all wanted to demonstrate their strength. The only AI competitors that can compete with DeepMind's AIs seem to be other DeepMind AIs.",8,1,False,self,,,,,
1486,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,4,ak45jd,self.MachineLearning,Is it correct or not? why?,https://www.reddit.com/r/MachineLearning/comments/ak45jd/is_it_correct_or_not_why/,Doctor_who1,1548531214,[removed],0,1,False,self,,,,,
1487,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,5,ak4lbw,self.MachineLearning,"[D] Anyone have any ML hypothesis, theories, or general project ideas that they'll probably never get around to pursuing, but would happily give small and occasional chucks of advice/guidance to anyone who would pursue it?",https://www.reddit.com/r/MachineLearning/comments/ak4lbw/d_anyone_have_any_ml_hypothesis_theories_or/,BatmantoshReturns,1548533879,"I have a ton of things I'm curious about but don't have time to pursue them all. 

I posted here and here if anyone wanted to take them up. I don't mind that they end up being someone else's project, as I don't have time to do them anyway, and also you gain insights in areas that you're really interested. 

https://www.reddit.com/r/MachinesLearn/comments/agaevz/newer_people_anyone_interested_in_a_beginner/

https://www.reddit.com/r/MachinesLearn/comments/aa64p0/anyone_looking_for_a_project_to_work_on_and_a_bit/

And was impressed by the amount of people willing to collaborate. 

I decided to spread this idea here, perhaps see if mentor/mentee pairs and groups can form. 

Also, my old offers are still good, if anyone is interested in applying NLP for information retrieval and recommendation towards research papers, pms are welcome ",3,1,False,self,,,,,
1488,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,5,ak4o0a,nintyzeros.com,A Deep Intro to Machine Learning - A Beginners Guide,https://www.reddit.com/r/MachineLearning/comments/ak4o0a/a_deep_intro_to_machine_learning_a_beginners_guide/,magicram,1548534336,,2,1,False,default,,,,,
1489,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,7,ak5vrg,self.MachineLearning,Is 38 too old for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/ak5vrg/is_38_too_old_for_machine_learning/,nntuan7,1548541843,[removed],0,1,False,self,,,,,
1490,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,10,ak7jh9,self.MachineLearning,[D] Are there any news letters that provide news or latest advancements in ML?,https://www.reddit.com/r/MachineLearning/comments/ak7jh9/d_are_there_any_news_letters_that_provide_news_or/,vlanins,1548553083,"Are there any good newsletters that provide news about machine learning and the latest advancements? I think it would be a great way to be knowledgeable about new things popping up in the field and could lead to interesting side projects.

If this doesn't exist, how do you guys stay up to date? Just dive right into arxiv and read papers?
",9,1,False,self,,,,,
1491,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,11,ak7tab,self.MachineLearning,"Problem with Keras multi-gpu model. I am just built my machine with a threadripper 1950x on an msi X399 gaming pro, using two nvidia gtx 1080ti.",https://www.reddit.com/r/MachineLearning/comments/ak7tab/problem_with_keras_multigpu_model_i_am_just_built/,Drgoldsz22,1548555146,[removed],0,1,False,self,,,,,
1492,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,11,ak7tew,github.com,Speech to text | Google Cloud Speech API,https://www.reddit.com/r/MachineLearning/comments/ak7tew/speech_to_text_google_cloud_speech_api/,mudza100,1548555176,,0,1,False,default,,,,,
1493,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,12,ak86n7,i.imgur.com,Neural network for handwritten digit recognition in Minecraft. I think I've seen it all now...,https://www.reddit.com/r/MachineLearning/comments/ak86n7/neural_network_for_handwritten_digit_recognition/,SMZero,1548558022,,0,1,False,https://a.thumbs.redditmedia.com/40XejqX8bMhciIwdw8g5kJLOENy8Ja36rECEWfbdlE8.jpg,,,,,
1494,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,12,ak881c,course.fast.ai,"[N] fast.ai's 2019 ""Pratical Deep Learning Tutorial"" is out!",https://www.reddit.com/r/MachineLearning/comments/ak881c/n_fastais_2019_pratical_deep_learning_tutorial_is/,dwlsalmeida,1548558317,,0,1,False,default,,,,,
1495,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,12,ak8kw4,nature.com,"Guidelines for reinforcement learning in healthcare (Nature Medicine, 2019)",https://www.reddit.com/r/MachineLearning/comments/ak8kw4/guidelines_for_reinforcement_learning_in/,serghiou,1548561032,,0,1,False,default,,,,,
1496,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,13,ak8sqq,nikunjbhoraniya.com,Types of Variation in Machining and Measurement,https://www.reddit.com/r/MachineLearning/comments/ak8sqq/types_of_variation_in_machining_and_measurement/,nikunjbhoraniya,1548562780,,2,1,False,https://b.thumbs.redditmedia.com/5oAH6rlFphCSPUjrB3YU8CY5yQvVDB5OzeoRePFAU0k.jpg,,,,,
1497,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,13,ak8wpr,self.MachineLearning,[D] Has anyone used video instead of GPS for navigation?,https://www.reddit.com/r/MachineLearning/comments/ak8wpr/d_has_anyone_used_video_instead_of_gps_for/,fuckinghelldad,1548563685,,4,1,False,self,,,,,
1498,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,14,ak98k8,self.MachineLearning,A good source for deep hidden Markove Model,https://www.reddit.com/r/MachineLearning/comments/ak98k8/a_good_source_for_deep_hidden_markove_model/,amiya_mandal,1548566445,[removed],0,1,False,self,,,,,
1499,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,15,ak9i6e,self.MachineLearning,DeepFaceLab training,https://www.reddit.com/r/MachineLearning/comments/ak9i6e/deepfacelab_training/,CtrlSodapop,1548568871,So recently I've been using DeepFaceLab to create funny videos however I have had one major problem. Whenever I begin to train the AI it will stop working randomly. I am using the train H64.bat file to train it. If anyone can help me fix this problem it would be greatly appreciated.,0,1,False,self,,,,,
1500,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,15,ak9kkr,self.MachineLearning,[D] DeepFaceLab training,https://www.reddit.com/r/MachineLearning/comments/ak9kkr/d_deepfacelab_training/,CtrlSodapop,1548569471,So recently I've been using DeepFaceLab to create funny videos however I have had one major problem. Whenever I begin to train the AI it will stop working randomly. I am using the train H64.bat file to train it. If anyone can help me fix this problem it would be greatly appreciated. ,0,1,False,self,,,,,
1501,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,15,ak9p0p,technologyreview.com,"We analyzed 16,625 papers to figure out where AI is headed next",https://www.reddit.com/r/MachineLearning/comments/ak9p0p/we_analyzed_16625_papers_to_figure_out_where_ai/,maphar,1548570674,,0,1,False,default,,,,,
1502,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,15,ak9rav,self.MachineLearning,[D] What can DeepMinds AlphaStar tell us about imitation learning for self-driving cars?,https://www.reddit.com/r/MachineLearning/comments/ak9rav/d_what_can_deepminds_alphastar_tell_us_about/,strangecosmos,1548571307,"Using just supervised imitation learning, DeepMinds AlphaStar achieved an estimated MMR (matchmaking ranking) in the [2500 to 3000 range](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/) for StarCraft II. If this estimated MMR is accurate (in terms of how that version of AlphaStar would stack up against human players), then AlphaStar would have placed either in the Gold league or in Silver 1 when playing against human players in [Europe and the Americas](https://burnysc2.github.io/MMRranges/).  Assuming 1/3 of Silver players are in Silver 1, that would put AlphaStar approximately in the 23rd to 50th percentile (or roughly the second quartile). 

To achieve this level of performance, AlphaStar was trained on a dataset of 500,000+ games played by humans. 

I find this an intriguing proof of concept for supervised imitation learning. It was recently [reported by The Information](https://www.theinformation.com/articles/what-makes-teslas-autopilot-different) that Tesla is attempting to do supervised imitation learning with autonomous driving: 

&gt;Teslas cars collect so much camera and other sensor data as they drive around, even when Autopilot isnt turned on, that the Autopilot team can examine what traditional human driving looks like in various driving scenarios and mimic it, said the person familiar with the system.  But Teslas engineers believe that by putting enough data from good human driving through a neural network, that network can learn how to directly predict the correct steering, braking and acceleration in most situations. You dont need anything else to teach the system how to drive autonomously, said a person who has been involved with the team.

I wonder if anyone has thoughts on this comparison between supervised imitation learning for StarCraft and for autonomous driving. Should the headway that AlphaStar made purely with imitation learning make us feel more optimistic about the same approach applied to autonomous driving? Or are StarCraft and driving too dissimilar for AlphaStar to provide relevant information? ",19,1,False,self,,,,,
1503,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,16,aka0yd,i.redd.it,"My thoughts on Text Classifcation after getting dissapointing results with BERT, ELMO and ULMFiT",https://www.reddit.com/r/MachineLearning/comments/aka0yd/my_thoughts_on_text_classifcation_after_getting/,youllnevercatchmeNSA,1548574058,,0,1,False,https://b.thumbs.redditmedia.com/4UEX9-CuuvMvR2xIiQpTn21qOKo89KxrIg0AXoMYdCA.jpg,,,,,
1504,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,18,akamtd,self.tensorflow,Convert HTML Table into JSON data,https://www.reddit.com/r/MachineLearning/comments/akamtd/convert_html_table_into_json_data/,avejack,1548580995,,0,1,False,https://b.thumbs.redditmedia.com/ejegoeWG0FaxbaVpldIp9WmES3zcurd1ryDRk3zLk3o.jpg,,,,,
1505,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,18,akany7,self.MachineLearning,"[D] Most commercial chatbot still relies on hand written dialogue flow and rules, is there any research on overcoming this restrictions ?",https://www.reddit.com/r/MachineLearning/comments/akany7/d_most_commercial_chatbot_still_relies_on_hand/,RavlaAlvar,1548581380,,8,1,False,self,,,,,
1506,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,18,akasbu,self.MachineLearning,What do you make of YouTube's Recommendation Algorithm?,https://www.reddit.com/r/MachineLearning/comments/akasbu/what_do_you_make_of_youtubes_recommendation/,bigboyparpa,1548582798,[removed],0,1,False,self,,,,,
1507,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,19,akatzd,self.MachineLearning,[D] Is there any app that enhances writing in images?,https://www.reddit.com/r/MachineLearning/comments/akatzd/d_is_there_any_app_that_enhances_writing_in_images/,TheSkipRow,1548583355,"So I have some stuff to read for my Uni and the image quality is kinda shit. Problem is, the guy lost the original documents, so the images is all we have. It's handwriting if that makes a difference.",0,1,False,self,,,,,
1508,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,20,akbc11,self.MachineLearning,[P] Tag estimation for anime-style girl image,https://www.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/,KichangKim,1548589018,"Hi. I created tag estimation system for anime-style girl image. This is my hobby project and any of feedbacks are welcome.

Web : http://kanotype.iptime.org:8003/deepdanbooru/
Telegram Bot : https://telegram.me/deepdanbooru_bot

Here is breif information for this system.
- Used library : CNTK
- Network : resnet based (299x299 image -&gt; about 6000 tags)
- Training time : about 28 days (Ryzen 7 1800X + Geforce 1070)

I hope you enjoy this.",19,1,False,self,,,,,
1509,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,20,akbcec,self.MachineLearning,[D] Does machine learning help us understand biological minds better?,https://www.reddit.com/r/MachineLearning/comments/akbcec/d_does_machine_learning_help_us_understand/,BatBast,1548589135,,54,1,False,self,,,,,
1510,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,20,akbe39,self.MachineLearning,[D] Best approach to variable image sizes for Image Classification?,https://www.reddit.com/r/MachineLearning/comments/akbe39/d_best_approach_to_variable_image_sizes_for_image/,MrKotia,1548589644,"I am working on classification of brain tumors. The dataset is comprised of brain images from various angles, with a border and mask of the tumor position. I have cropped the rectangle that contains the tumor, as the other parts of the image are irrelevant and vary due to the different angles from which the image is taken. Now I am left with a series of images of the tumor, each belonging to 1 of 3 possible cases of tumor. But to train this data for classification, I need to have the 2d image array in a uniform shape I believe.

&amp;#x200B;

Possible approaches:

1) Crop each image to a fixed size (say 100x100). But this would result in loss of data and also depending on position of the tumor in image, I could be facing uneven crops due to reaching the edge of the image.

2) Pad the image to a fixed shape, bigger than the biggest cropped image shape (say 350x350). But again this would introduce noise in the data I suppose, and I'm not sure how can I pad the image uniformly across all 4 sides of the rectangle.

&amp;#x200B;

Any responses are welcome, as I couldn't find viable solutions to this problem elsewhere.",26,1,False,self,,,,,
1511,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,21,akbj26,self.MachineLearning,"What are the best books about machine learning , deep learning for 2019 ?",https://www.reddit.com/r/MachineLearning/comments/akbj26/what_are_the_best_books_about_machine_learning/,Doctor_who1,1548591143,[removed],0,1,False,self,,,,,
1512,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,21,akboac,self.MachineLearning,"Which are the best magazines MachineLearning , Deep Learnig to read?",https://www.reddit.com/r/MachineLearning/comments/akboac/which_are_the_best_magazines_machinelearning_deep/,Doctor_who1,1548592688,[removed],0,1,False,self,,,,,
1513,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,22,akc5rc,self.MachineLearning,Associating Physical Quantities in a sentence with subject,https://www.reddit.com/r/MachineLearning/comments/akc5rc/associating_physical_quantities_in_a_sentence/,A4KA5H,1548597042,[removed],0,1,False,self,,,,,
1514,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,23,akclcw,course.fast.ai,"[N] fast.ai's 2019 ""Practical Deep Learning for Coders, v3"" is out!",https://www.reddit.com/r/MachineLearning/comments/akclcw/n_fastais_2019_practical_deep_learning_for_coders/,dwlsalmeida,1548600517,,0,1,False,default,,,,,
1515,MachineLearning,t5_2r3gv,2019-1-27,2019,1,27,23,akcnru,self.MachineLearning,[D] Do you prefer the layout of machine learning based research papers to have two columns or one column?,https://www.reddit.com/r/MachineLearning/comments/akcnru/d_do_you_prefer_the_layout_of_machine_learning/,bobdammit,1548601046,"I have noticed that quite a few machine learning research papers only use 1 column, while research in other areas frequently use two columns. Personally, I enjoy two columns because I can speed read it easier when performing my first initial scan of a paper, but I have also spoken to others that find two column papers hard to follow.

",22,1,False,self,,,,,
1516,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,0,akcons,self.MachineLearning,Looking for data analysts to beta test a web app we've been working on!,https://www.reddit.com/r/MachineLearning/comments/akcons/looking_for_data_analysts_to_beta_test_a_web_app/,rab657,1548601232,[removed],0,1,False,self,,,,,
1517,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,0,akcv6u,self.MachineLearning,What is the difference between LDI and word2vec?,https://www.reddit.com/r/MachineLearning/comments/akcv6u/what_is_the_difference_between_ldi_and_word2vec/,fruttibar,1548602502,[removed],0,1,False,self,,,,,
1518,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,0,akcz1m,self.MachineLearning,CNN for classification Question,https://www.reddit.com/r/MachineLearning/comments/akcz1m/cnn_for_classification_question/,jtan354,1548603250,[removed],0,1,False,self,,,,,
1519,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,2,akeb2r,self.MachineLearning,"Transfer learning - TERRIBLE RESULTS?! Help, please.",https://www.reddit.com/r/MachineLearning/comments/akeb2r/transfer_learning_terrible_results_help_please/,GantMan,1548611412,[removed],0,1,False,self,,,,,
1520,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,3,akeis7,self.MachineLearning,How to practice machine learning?,https://www.reddit.com/r/MachineLearning/comments/akeis7/how_to_practice_machine_learning/,Chrisboyd88,1548612881,[removed],0,1,False,self,,,,,
1521,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,3,aker6n,self.MachineLearning,"Any positive feedback would be appreciated, I am new to youtube",https://www.reddit.com/r/MachineLearning/comments/aker6n/any_positive_feedback_would_be_appreciated_i_am/,pointless-ai,1548615213,[removed],0,1,False,self,,,,,
1522,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,4,akeype,self.MachineLearning,Is there any suggestions on how can I make this video more interesting ?,https://www.reddit.com/r/MachineLearning/comments/akeype/is_there_any_suggestions_on_how_can_i_make_this/,pointless-ai,1548616798,[removed],0,1,False,self,,,,,
1523,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,4,akf88j,self.MachineLearning,Interesting Machine Learning DIY projects?,https://www.reddit.com/r/MachineLearning/comments/akf88j/interesting_machine_learning_diy_projects/,Zalonio,1548618648,[removed],0,1,False,self,,,,,
1524,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,4,akf9ka,self.MachineLearning,[D] Training two AI's to communicate with each other to solve a problem?,https://www.reddit.com/r/MachineLearning/comments/akf9ka/d_training_two_ais_to_communicate_with_each_other/,AxeLond,1548619000,"Has similar to this been done before? 

My idea of setup would be something similar to AlphaStar playing SC2 but it's 2 versus 2. Similar to normal matchmaking you would take 2 random agents and team them up vs two other random agents. Every individual agent would have no knowledge of how the other 3 will act but they access to in game chat where they could choose to send one of 10 standard messages to their teammate like ""Attack"", ""Retreat"", ""Defend"", ""Me"", ""You"".

They could of course just choose to play the game silently like low level players do but at the high level you have to communicate what your plans are and what both should do. Maybe the best strategy is that one agent stays home and defends while the other attacks. Would they learn to communicate this with each other and listen to what the other is saying. If both are exact copies of each other could they for example decide which one of them should defend and who goes for an attack?",32,4,False,self,,,,,
1525,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,5,akfcul,self.MachineLearning,Machine Learning and Biology,https://www.reddit.com/r/MachineLearning/comments/akfcul/machine_learning_and_biology/,thereader82,1548619561,[removed],0,1,False,self,,,,,
1526,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,6,akfttp,self.MachineLearning,"Don't know math, should I quit learning ML ?",https://www.reddit.com/r/MachineLearning/comments/akfttp/dont_know_math_should_i_quit_learning_ml/,MataTop,1548623254,[removed],0,1,False,self,,,,,
1527,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,6,akg3p2,arxiv.org,[1901.08256] Large-Batch Training for LSTM and Beyond,https://www.reddit.com/r/MachineLearning/comments/akg3p2/190108256_largebatch_training_for_lstm_and_beyond/,sidsig,1548624829,,3,3,False,default,,,,,
1528,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,8,akhhxn,self.MachineLearning,[P] Rendsolve GPU Cloud,https://www.reddit.com/r/MachineLearning/comments/akhhxn/p_rendsolve_gpu_cloud/,rendsolve,1548633413,"Hey /MachineLearning

My name is Phillip, Founder of Rendsolve! Rendsolve is a GPU focused cloud computing platform, Our target is Engineers and Creatives who are dependant on GPU compute. We have RTX , Quadro and Tesla P100 systems. Access is easy, We have a web console for SSH, VNC or RDP. After configuration you can access your VM over the browser with a Jupyter Notebook also.

What are Capsules? Simply there just another word for One Click Apps (DO) or Images/Templates (GCP/AWS). Our capsules allow engineers to have a fully featured GPU accelerated ML system up and running. You can read more about our ML Capsules [here](https://blog.rendsolve.com/machine-learning-capsules/).

Until the end of January we are offering to users a 1 to 1 on credits. Essentially putting the platform at half price, Credits dont expire so no need to use them at a given time. Minimum is $10.

If anyone wants a demo system you can DM me here or open a support ticket [here](https://rendsolve.zendesk.com/hc/en-us/requests/new) .

I would love to hear your feedback and Im happy to answer any questions you have!

Ill leave some link here:

[https://rendsolve.com](https://rendsolve.com/)

[https://cloud.rendsolve.com](https://cloud.rendsolve.com/)

[Getting Started with Rendsolve. ](https://rendsolve.zendesk.com/hc/en-us/articles/360000110915)

[Setting up you Jupyter Notebook Portal. ](https://rendsolve.zendesk.com/hc/en-us/articles/360000688715-Setting-up-your-Jupyter-Notebook-environment-on-Rendsolve-)

[Fast.Ai Datasets.](https://rendsolve.zendesk.com/hc/en-us/articles/360000688715-Setting-up-your-Jupyter-Notebook-environment-on-Rendsolve-)

[Security &amp; Data Privacy](https://rendsolve.zendesk.com/hc/en-us/sections/360000155415-Security-Data-Privacy)

[Billing](https://rendsolve.zendesk.com/hc/en-us/sections/360000003016-General-Billing)",9,1,False,self,,,,,
1529,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,9,akhkqt,medium.com,Gradient boosting machine: will performance drop if a single tree is removed?,https://www.reddit.com/r/MachineLearning/comments/akhkqt/gradient_boosting_machine_will_performance_drop/,DapperAverage,1548633936,,0,1,False,default,,,,,
1530,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,9,akhl2q,self.MachineLearning,Is there a graphic with all the different ML Gates,https://www.reddit.com/r/MachineLearning/comments/akhl2q/is_there_a_graphic_with_all_the_different_ml_gates/,CSS_Programmer,1548633995,[removed],0,1,False,self,,,,,
1531,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,10,aki6rj,self.MachineLearning,Books/articles where it's described how to build something from scratch *and without frameworks*?,https://www.reddit.com/r/MachineLearning/comments/aki6rj/booksarticles_where_its_described_how_to_build/,xuhiw,1548637894,[removed],0,1,False,self,,,,,
1532,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,10,akick1,arxiv.org,[1901.08708] Almost Boltzmann Exploration,https://www.reddit.com/r/MachineLearning/comments/akick1/190108708_almost_boltzmann_exploration/,bulletbolt,1548639002,,5,1,False,default,,,,,
1533,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,11,akj0j3,towardsdatascience.com,[D] What follows AlphaStar for Academic AI Researchers?,https://www.reddit.com/r/MachineLearning/comments/akj0j3/d_what_follows_alphastar_for_academic_ai/,bweber,1548643701,,0,1,False,default,,,,,
1534,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,12,akjaoe,arxiv.org,[1901.08746] BioBERT: pre-trained biomedical language representation model for biomedical text mining,https://www.reddit.com/r/MachineLearning/comments/akjaoe/190108746_biobert_pretrained_biomedical_language/,63coldnoodle,1548645636,,12,1,False,default,,,,,
1535,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,12,akjk08,newyorker.com,[D] A Study on Driverless-Car Ethics Offers a Troubling Look Into Our Values,https://www.reddit.com/r/MachineLearning/comments/akjk08/d_a_study_on_driverlesscar_ethics_offers_a/,phobrain,1548647455,,0,1,False,default,,,,,
1536,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,13,akjxy1,9spl.com,Machine Learning (ML) Development Services Company India | Hire AI Developers USA,https://www.reddit.com/r/MachineLearning/comments/akjxy1/machine_learning_ml_development_services_company/,jamielannisters,1548650372,,0,1,False,default,,,,,
1537,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,13,akk3bi,self.MachineLearning,RTX 2070 or Tesla K80 (24GB)?,https://www.reddit.com/r/MachineLearning/comments/akk3bi/rtx_2070_or_tesla_k80_24gb/,Ash3nBlue,1548651563,[removed],0,1,False,self,,,,,
1538,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,14,akk9z3,self.MachineLearning,RTX 2080Ti Vs GTX 1080Ti: Mixed Precision training comparisions,https://www.reddit.com/r/MachineLearning/comments/akk9z3/rtx_2080ti_vs_gtx_1080ti_mixed_precision_training/,init__27,1548652972,[https://hackernoon.com/rtx-2080ti-vs-gtx-1080ti-fastai-mixed-precision-training-comparisons-on-cifar-100-761d8f615d7f](https://hackernoon.com/rtx-2080ti-vs-gtx-1080ti-fastai-mixed-precision-training-comparisons-on-cifar-100-761d8f615d7f),0,1,False,self,,,,,
1539,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,15,akkn3j,hexaware.com,Enhancing Straight Through Processing in KYC operations with Robotics Automation and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/akkn3j/enhancing_straight_through_processing_in_kyc/,kirankarthick2019,1548655876,,0,1,False,default,,,,,
1540,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,15,akknsr,self.MachineLearning,[Project] How do I train a network that outputs parameters for a black-box function?,https://www.reddit.com/r/MachineLearning/comments/akknsr/project_how_do_i_train_a_network_that_outputs/,avrock123,1548656038,"Have a compelling idea for a project but not sure how the training would work, here's the abstracted problem statement:

I have an input X that I want to mimic with a given function g(p), where p is a set of parameters that results closest possible mimic of X. I want to find a function f(X) which outputs these optimal parameters. I'd use supervised learning, but back-prop through g(p) isn't feasible. Mathematically:

    min E[ L[ X_mimic, X] ] where X_mimic = g(f(X)), L is a loss function

The input X for my particular application is a 2D time-series, and p is \~50 dimensional. Any methods that get around this problem? My knee-jerk reaction was to apply unsupervised methods like auto-encoders, but if I can't backprop through the generator won't work. GAN's don't really make sense either for the same reason, and there's no obvious thing to discriminate as 'real' vs 'fake'.

The only way I could think of was an RL approach, since you can think of X and X\_mimic as the environment state, and the f(X) as a policy function that generates a set of actions 'p', provided a good reward function. It feels a little weird to use RL though since ideally the algorithm should predict 'p' after one look at the environment, although I guess it could also iteratively converge to a solution. Another thought is showing the algorithm only the beginning of a time series, having it predict the parameters and generate X\_mimic, and iteratively showing more and more of the time-series till convergence.   My guess is this will not be data-efficient, though I'm not sure.

&amp;#x200B;

Anything I'm missing? I feel like this type of problem must have some literature, especially pertaining to control systems. I'm also a beginner to RL so some guidance would be appreciated. ",6,1,False,self,,,,,
1541,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,17,akljrv,self.MachineLearning,Multiple Tries NN,https://www.reddit.com/r/MachineLearning/comments/akljrv/multiple_tries_nn/,cayala10,1548664308,[removed],0,1,False,self,,,,,
1542,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,17,aklmmt,github.com,Recommender Systems Flowchart,https://www.reddit.com/r/MachineLearning/comments/aklmmt/recommender_systems_flowchart/,Fil727,1548665170,,0,1,False,https://b.thumbs.redditmedia.com/MyBYfcoxBcNhQBMc7wtd-39hClzVkIZa4J19VNKgw-Y.jpg,,,,,
1543,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,17,akln03,self.MachineLearning,Benchmarks for ARMA/ARCH-like time series analysis models  predicting probability distributions for successive times especially for financial time series?,https://www.reddit.com/r/MachineLearning/comments/akln03/benchmarks_for_armaarchlike_time_series_analysis/,jarekduda,1548665296,[removed],0,1,False,self,,,,,
1544,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,18,aklswz,self.MachineLearning,[D] Benchmarks for ARMA/ARCH-like time series analysis models  predicting probability distributions for successive times especially for financial time series?,https://www.reddit.com/r/MachineLearning/comments/aklswz/d_benchmarks_for_armaarchlike_time_series/,jarekduda,1548667016,"There are probably hundreds of time series analysis models, especially [ARMA/ARCH](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity) family, so there are needed some good benchmarks to choose the best one for a given task, to test new methods - check if it is a good direction... but I couldnt find them (?)

Do anybody know such benchmarks  public datasets especially financial time series, and concrete evaluation criteria for various tasks?

If not, maybe lets try to discuss how it should be done based on machine learning experience? 

Regarding public datasets, daily data can be downloaded e.g. from [Reuters](https://www.reuters.com/finance/global-market-data), here are [10 years daily data for 29 companies and a century for Dow Jones]( https://www.dropbox.com/sh/aijejoofuwbulo5/AABAHh3lEBnsmhRSCpHDvADna). Are there public samples for other types of data, e.g. intraday?

**The real question are evaluation criteria for the best prediction power.** In econometrics literature e.g. [Elements of financial risk menagement](http://fanarco.net/books/risk/Elements.of.Financial.Risk.Management.pdf) book, there are tests like **unconditional coverage**: if chosen quantile predicted by our distribution agrees with population percentage. It is usually tested for just a few quantiles, we can test it for all of them by just plotting sorted CDF(x) values: it would be a line for the real CDF.

But this coverage test is only a question about calibration and is relatively easy to achieve, standard real evaluation in probability/statistics/machine learning is **log-likelihood: average lg(rho_t (x_t))** over the series, where the used distribution can vary e.g. as Gaussian of varying width in ARCH. This MLE evaluation is kind of logarithm of probability (normalized to 1) of getting the observed sequence, its difference can be translated to average improvement of predicted density, it has information theoretic interpretation (asymptotically as minus cross entropy) ... but log-likelihood seems forgotten for time series model evaluation in economics literature (?)

For example [I have recently tested](https://arxiv.org/pdf/1807.04119) ([slides](https://www.dropbox.com/s/7u6f2zpreph6j8o/rapid.pdf)) daily log returns for 29 Dow Jones companies ... and while ARMA/ARCH are mostly based on Gaussian distribution, it turns out terrible looking at log-likelihood (also unconditional coverage), e.g. pure i.i.d. Laplace distribution (or other rho(x) ~ exp(-|x|^p ) [generalized normal distributions](https://en.wikipedia.org/wiki/Generalized_normal_distribution)) turns out much better than (context dependent) ARCH here: https://i.imgur.com/ldKRB3S.png

Beside choosing evaluation score, there remain a few difficult question to choose:

* there should be separate categories for different orders of methods - how many previous values it uses for prediction,

* also separate categories for stationary and non-stationary models: with parameters evolving in time,

* splitting into training and evaluation set,

* some regularization criteria - penalty for size of model (?)

Any thought about designing good benchmarks for financial time series models? Maybe there are already some? 

Should they use log-likelihood or a different main evaluation score?",6,1,False,self,,,,,
1545,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,18,aklwv8,self.MachineLearning,[N] Report: Tesla is using behavior cloning (i.e. supervised imitation learning) for Autopilot and full self-driving,https://www.reddit.com/r/MachineLearning/comments/aklwv8/n_report_tesla_is_using_behavior_cloning_ie/,strangecosmos,1548668205,"The full story is reported by [Amir Efrati in The Information](https://news.ycombinator.com/from?site=theinformation.com). (The caveat is that this has been reported based on information from unnamed sources, and as far as I know only one reporter has confirmed this story.)

Heres the key excerpt from the article: 

&gt;Teslas cars collect so much camera and other sensor data as they drive around, even when Autopilot isnt turned on, that the Autopilot team can examine what traditional human driving looks like in various driving scenarios and mimic it, said the person familiar with the system. It uses this information as an additional factor to plan how a car will drive in specific situationsfor example, how to steer a curve on a road or avoid an object. Such an approach has its limits, of course: behavior cloning, as the method is sometimes called 
&gt;
&gt;But Teslas engineers believe that by putting enough data from good human driving through a neural network, that network can learn how to directly predict the correct steering, braking and acceleration in most situations. You dont need anything else to teach the system how to drive autonomously, said a person who has been involved with the team. They envision a future in which humans wont need to write code to tell the car what to do when it encounters a particular scenario; it will know what to do on its own.",96,1,False,self,,,,,
1546,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,18,aklyj9,youtu.be,This is how you train match-boxes to play Tic Tac toe #ReinforcementLearning,https://www.reddit.com/r/MachineLearning/comments/aklyj9/this_is_how_you_train_matchboxes_to_play_tic_tac/,Riturajkaushik,1548668706,,0,1,False,default,,,,,
1547,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,18,akm14e,self.MachineLearning,[D] How to treat different measurements of the same variable?,https://www.reddit.com/r/MachineLearning/comments/akm14e/d_how_to_treat_different_measurements_of_the_same/,Mimsyy,1548669454,"Please forgive (and enlighten) me about any statistical misunderstandings, it was never my strong suit.

Let's say we are measuring a quantity X on a large group of samples, using k different devices. Different devices are (most of the time) used for different samples. The output distributions for the different devices will have different means and variances, but we know that the underlying distribution should be same for all measurements. For example, measurements with two different devices may have the following distributions:

https://i.redd.it/hegnyvgjy4d21.png

Now, what is the best way to turn measurements on X into a feature for an ML algorithm (GBDT is what I'm currently using for the task)? I was thinking it would be reasonable to group the measurements by device, subtract their individual means and divide by the individual variances, then keeping it as a single feature. This does not always turn out well however, so it is probably not the best way to do it. Having them all as separate features also doesn't seem like the best possible method.

I'm grateful for any insights!",4,1,False,self,,,,,
1548,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,19,akm5q9,self.MachineLearning,[1807.04067] MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network,https://www.reddit.com/r/MachineLearning/comments/akm5q9/180704067_multiposenet_fast_multiperson_pose/,salihkaragoz,1548670728,[removed],0,1,False,self,,,,,
1549,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,19,akm80f,self.MachineLearning,White Coal Machine,https://www.reddit.com/r/MachineLearning/comments/akm80f/white_coal_machine/,ecostan1,1548671368,"The raw material is collected at one place and compressed under high pressure using [white coal machine](https://www.ecostan.com/) (also known as briquetting press) to turn it into green bio-fuel. Its calorific value depends on the waste that is used to make it. Also, the moisture content is nil and ash is very low. ",0,1,False,self,,,,,
1550,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,19,akm963,eng.uber.com,[N] Manifold: A Model-Agnostic Visual Debugging Tool for Machine Learning at Uber,https://www.reddit.com/r/MachineLearning/comments/akm963/n_manifold_a_modelagnostic_visual_debugging_tool/,pp314159,1548671707,,0,1,False,default,,,,,
1551,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,19,akma16,self.MachineLearning,VAE implementation in Keras - what's wrong here?,https://www.reddit.com/r/MachineLearning/comments/akma16/vae_implementation_in_keras_whats_wrong_here/,daniel451,1548671951,[removed],0,1,False,self,,,,,
1552,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,19,akmavj,self.MachineLearning,Local Python based Server for serving a CNN Model or Any ML Model?,https://www.reddit.com/r/MachineLearning/comments/akmavj/local_python_based_server_for_serving_a_cnn_model/,ysfxali,1548672172,"I am currently using CherryPy on Windows(Usual working environment in my scenario) and it is pretty good.

&amp;#x200B;

Just want to know what server setup other people use.",0,1,False,self,,,,,
1553,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,20,akmhou,self.MachineLearning,Training neural word embeddings,https://www.reddit.com/r/MachineLearning/comments/akmhou/training_neural_word_embeddings/,boredhumen101,1548674047,[removed],0,1,False,self,,,,,
1554,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,22,aknf78,self.MachineLearning,Book: Machine Learning with R - Second Edition,https://www.reddit.com/r/MachineLearning/comments/aknf78/book_machine_learning_with_r_second_edition/,andrea_manero,1548682176,[removed],0,1,False,self,,,,,
1555,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,22,aknfwd,self.MachineLearning,[P] Teaching agents to use a painting program with World Models.,https://www.reddit.com/r/MachineLearning/comments/aknfwd/p_teaching_agents_to_use_a_painting_program_with/,Reiinakano,1548682319,"Hello Reddit, sharing my latest project where I try using World Models to teach an agent how to use a paint program and replicate SPIRAL's results.

I wrote a blog post about it: [https://reiinakano.github.io/2019/01/27/world-painters.html](https://reiinakano.github.io/2019/01/27/world-painters.html)

&amp;#x200B;

https://i.redd.it/40q9ptqy16d21.gif

Hope some of you will find it useful!",0,1,False,https://b.thumbs.redditmedia.com/pJA5yITfrFESyb0Y_FTCSBZHHkyDd3wB4mv9LCQAjiI.jpg,,,,,
1556,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,22,aknhoi,self.MachineLearning,Good place for agriculture data?,https://www.reddit.com/r/MachineLearning/comments/aknhoi/good_place_for_agriculture_data/,crazy_davinci,1548682708,[removed],0,1,False,self,,,,,
1557,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,22,aknjx8,developereconomics.com,The battle: Tensorflow vs Pytorch,https://www.reddit.com/r/MachineLearning/comments/aknjx8/the_battle_tensorflow_vs_pytorch/,vjmde,1548683160,,0,1,False,default,,,,,
1558,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,23,akns16,i.imgur.com,Question about assumption,https://www.reddit.com/r/MachineLearning/comments/akns16/question_about_assumption/,Matschreiner,1548684723,,0,1,False,default,,,,,
1559,MachineLearning,t5_2r3gv,2019-1-28,2019,1,28,23,ako51k,self.MachineLearning,Pronunciation matching,https://www.reddit.com/r/MachineLearning/comments/ako51k/pronunciation_matching/,tghv2011,1548687230,[removed],0,1,False,self,,,,,
1560,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,0,akob53,self.MachineLearning,Reparameterization Trick instead of Log-Derivative in Policy Gradients?,https://www.reddit.com/r/MachineLearning/comments/akob53/reparameterization_trick_instead_of_logderivative/,ultron777,1548688315,[removed],0,1,False,self,,,,,
1561,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,0,akofit,theappsolutions.com,4 Types of Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/akofit/4_types_of_machine_learning_algorithms/,noer_noer,1548689082,,0,1,False,default,,,,,
1562,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,0,akonst,docs.google.com,"[D] If Not Notebooks, Then What? - AAAI 2019",https://www.reddit.com/r/MachineLearning/comments/akonst/d_if_not_notebooks_then_what_aaai_2019/,rasmii,1548690469,,0,1,False,default,,,,,
1563,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akot24,self.MachineLearning,"[Nodejs] Now, the easy-to-use Mozilla / DeepSpeech STT has been created.",https://www.reddit.com/r/MachineLearning/comments/akot24/nodejs_now_the_easytouse_mozilla_deepspeech_stt/,rjs1197,1548691331,[removed],0,1,False,self,,,,,
1564,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akotnn,theappsolutions.com,Data Analytics,https://www.reddit.com/r/MachineLearning/comments/akotnn/data_analytics/,mishannon,1548691434,,0,1,False,default,,,,,
1565,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akourh,self.MachineLearning,Predicting mature face based on baby face?,https://www.reddit.com/r/MachineLearning/comments/akourh/predicting_mature_face_based_on_baby_face/,jamqdlaty,1548691612,[removed],0,1,False,self,,,,,
1566,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akov99,self.MachineLearning,[D] Reparameterization Trick instead of Log-Derivative in Policy Gradients?,https://www.reddit.com/r/MachineLearning/comments/akov99/d_reparameterization_trick_instead_of/,ultron777,1548691689,"Is there anything that stops me from trying to estimate the gradient through the expectations over actions drawn from my policy in the standard policy gradient setup with the reparameterization trick, instead of using log-derivative (REINFORCE)?

Seems like it's pretty straightforward to do assuming continuous actions, but even in a discrete action setup, one can use the Gumble Softmax reparameterization. Would this give me a lower variance estimator than standard REINFORCE?",8,1,False,self,,,,,
1567,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akoytn,theappsolutions.com,Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/akoytn/machine_learning_algorithms/,noer_noer,1548692292,,0,1,False,https://b.thumbs.redditmedia.com/g95RTVC-kBI7Xq0q5pmoQ0rT47mg8dTY_JrFvRCtaMg.jpg,,,,,
1568,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akp1vm,self.MachineLearning,[D] The ML Community should Standardize its Style Sheets,https://www.reddit.com/r/MachineLearning/comments/akp1vm/d_the_ml_community_should_standardize_its_style/,alexmlamb,1548692792,"Common situation: you've just submitted to ICML and you want to post to arxiv, but you want your arxiv version to be single column (ICML is double column).  

What do you have to do?  Find style sheets from a single column conference, switch your imports, fix the random latex bugs, etc.  Remove any references to the name of the other conference.  Very annoying.  

Imagine if we had one style sheet for our main conferences with parameters to pick the conference and the format: 

&amp;#x200B;

\\usepackage\[icml,doublecolumn\]{ml\_style}

or

\\usepackage\[preprint,singlecolumn\]{ml\_style}

&amp;#x200B;

And then you'd need to make sure that everything else is written the same way, so you can change just by changing that one line.  ",7,1,False,self,,,,,
1569,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akpc8i,self.MachineLearning,Using active learning to polish the accuracy after learning from a dataset,https://www.reddit.com/r/MachineLearning/comments/akpc8i/using_active_learning_to_polish_the_accuracy/,smndoo,1548694466,[removed],0,1,False,self,,,,,
1570,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,1,akpckv,self.MachineLearning,[D] Need advice for new GPU purchase. RTX 2070 or Tesla K80?,https://www.reddit.com/r/MachineLearning/comments/akpckv/d_need_advice_for_new_gpu_purchase_rtx_2070_or/,Ash3nBlue,1548694521,Both will cost the same and passive cooling isn't an issue. I intend to use it as a second GPU (currently have a 1080 TI) for general deep learning research. Is the much larger (24GB) memory pool of the K80 worth the tradeoff in speed and tensor cores?,8,1,False,self,,,,,
1571,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,2,akpkbt,arxiv.org,[1802.10501] Predictive Uncertainty Estimation via Prior Networks,https://www.reddit.com/r/MachineLearning/comments/akpkbt/180210501_predictive_uncertainty_estimation_via/,sidsig,1548695725,,1,1,False,default,,,,,
1572,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,2,akpy02,self.MachineLearning,Help with Machine learning applied to Forex Trading,https://www.reddit.com/r/MachineLearning/comments/akpy02/help_with_machine_learning_applied_to_forex/,0cn421,1548697877,[removed],0,1,False,self,,,,,
1573,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,2,akpz4j,kite.com,The Smartest AI Code Completions Just Got Smarter,https://www.reddit.com/r/MachineLearning/comments/akpz4j/the_smartest_ai_code_completions_just_got_smarter/,brendanmcd96,1548698060,,1,1,False,default,,,,,
1574,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,3,akqajt,self.MachineLearning,"[R] Obstacle Tower Environment: inspired by Montezuma's Revenge, a benchmark for hard problems in DeepRL",https://www.reddit.com/r/MachineLearning/comments/akqajt/r_obstacle_tower_environment_inspired_by/,downtownslim,1548699852,"A new challenge for Deep RL. Requires vision, control, planning, and generalization in order for agents to perform well.

https://github.com/Unity-Technologies/obstacle-tower-env",2,1,False,self,,,,,
1575,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,3,akqatg,self.MachineLearning,[P] Videos for the First TVM and Deep Learning Compiler Conference,https://www.reddit.com/r/MachineLearning/comments/akqatg/p_videos_for_the_first_tvm_and_deep_learning/,crowwork,1548699897,"TVM is an open-source deep learning compiler stack for CPUs, GPUs, and specialized accelerators. It aims to close the gap between the productivity-focused deep learning frameworks, and the performance- or efficiency-oriented hardware backends.

The conference contained 20+ talks covering deep learning compilation, specialized accelerators, IoT, ML for systems, privacy/security and more

Link: https://www.youtube.com/playlist?list=PLR4pm7mU3ROmrk9rimv6nRinKb0YKBx0f",0,1,False,self,,,,,
1576,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,3,akqcb3,heartbeat.fritz.ai,"[D] We have created a list of ML newsletters to follow, let us know about others you follow.",https://www.reddit.com/r/MachineLearning/comments/akqcb3/d_we_have_created_a_list_of_ml_newsletters_to/,zsajjad,1548700127,,0,1,False,default,,,,,
1577,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,4,akqooq,self.MachineLearning,[D] Alternatives to averaging embeddings of tokens in sequences of different length.,https://www.reddit.com/r/MachineLearning/comments/akqooq/d_alternatives_to_averaging_embeddings_of_tokens/,gsk694,1548702032,"Without doubt embeddings are very useful, if not the pretrained ots w2v results directly than as initial weights. My main intention is to use these token level embeddings (contextual or otherwise) to represent a sentence to measure similarity between sentences with little to no dependency on rules/POS. For this, I need to represent variable length sequences into some fixed space representation. I am currently surveying for existing literature on this note.

At token level so to speak these are easy to use but when it comes to sequence level (sentence per se) one of the simplest,  
over-used method is to take an average of the tokens in sequence to represent the sequence as a vector.

On doing a quick research I've come across interesting yet simple ideas like averaging with tf-idf weightage and DL methods like [InferSent](https://github.com/facebookresearch/InferSent), [Microsoft's method](https://openreview.net/pdf?id=B18WgG-CZ), [an old classic](https://pdfs.semanticscholar.org/1374/617e135eaa772e52c9a2e8253f49483676d6.pdf) and google's [thing](https://arxiv.org/pdf/1803.11175.pdf) (universal sentence encoder). 

&amp;#x200B;

Is there any current work or evaluation methods for this? What could be an ideal way to achieve this when simple averaging is too simple and when there is no way (at least I do not know where to look) to compare the methods.   


&amp;#x200B;",13,1,False,self,,,,,
1578,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,4,akqqt6,self.MachineLearning,Which technologies/frameworks should I use for image recognition on a mobile device?,https://www.reddit.com/r/MachineLearning/comments/akqqt6/which_technologiesframeworks_should_i_use_for/,stracki,1548702360,"Hi guys,

I want to use image recognition in the context of search in an image database (more specifically like ""this picture contains XYZ, here are similar pictures also containing XYZ""). The application should be lightweight enough to work on a mobile device (like e.g. a Hololens).

I looked for frameworks regarding image recognition and found the following:

* Apache MxNet with Gluon API
* TensorFlow Slim
* PyTorch (maybe using fastai)
* Caffe2

Do you think, one of these would be suited for what I want to achieve? If so, which are specific advantages against the other frameworks. Are there any other recommendations? It should preferably be possible to use the framework with Python, it should be well-documented and it should already come with pre-trained models for image recognition.

Another possibility that I found, are Cloud services like Azure Computer Vision or Google Cloud Vision. Any opinions on those? Their free plans sounded a bit restricting, but those looked like a good option for an early app prototype.

Thanks in advance",0,1,False,self,,,,,
1579,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,4,akquw3,self.MachineLearning,[P] Neural network racing cars around a track,https://www.reddit.com/r/MachineLearning/comments/akquw3/p_neural_network_racing_cars_around_a_track/,hides_dirty_secrets,1548702979,"After Christmas I wanted to play around with some machine learning, so I read up a bit on the subject and created a little car racing game. I made this video to show it off and to help explain to some friends what I was working on.

https://www.youtube.com/watch?v=wL7tSgUpy8w

Well, go ahead and take a look if you like! ;)",93,1,False,self,,,,,
1580,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,4,akqx92,reddit.com,[P] Neural network racing cars around a track,https://www.reddit.com/r/MachineLearning/comments/akqx92/p_neural_network_racing_cars_around_a_track/,Kinglens311,1548703338,,0,1,False,default,,,,,
1581,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,5,akrdyu,youtube.com,FUN chat with google home,https://www.reddit.com/r/MachineLearning/comments/akrdyu/fun_chat_with_google_home/,aryapowerpuff,1548705898,,0,1,False,https://a.thumbs.redditmedia.com/xS_x4MBpeziKlFko4w82TQmgM0icPIFSxMl59ro6_D8.jpg,,,,,
1582,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,5,akrkf4,self.MachineLearning,[R] [ICLR 2019] Learning when to communicate at scale in multiagent cooperative and competitive tasks: Code and Paper,https://www.reddit.com/r/MachineLearning/comments/akrkf4/r_iclr_2019_learning_when_to_communicate_at_scale/,apsdehal,1548706895,"**arXiV**: [https://arxiv.org/abs/1812.09755](https://arxiv.org/abs/1812.09755)

**GitHub**:  [https://github.com/IC3Net/IC3Net](https://github.com/IC3Net/IC3Net)

&amp;#x200B;

**Takeaway Points:**

\- Agents can learn only to communicate when it is **profitable** for them.

\- **Like humans**, IC3Net, **a single model** can be used in **any scenario** whether competitive, cooperative or mixed.

\- Individualized rewards and selective communication help IC3Net **scale** better than the baselines.

&amp;#x200B;

**Summary:**

We introduce **IC3Net**, a single model which can be used in any multiagent scenario, whether cooperative, competitive or mixed. IC3Ne's performance gap from baselines increases as the size of environment increases. We run experiments on three different environment including **StarCraft**. We also show that agents can actually learn to communicate only when it is profitable from them. We also provide a gym environment for StarCraft.

&amp;#x200B;

First author here, happy to answer any questions.",2,1,False,self,,,,,
1583,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,5,akrp25,medium.com,Article on markov decision process,https://www.reddit.com/r/MachineLearning/comments/akrp25/article_on_markov_decision_process/,sanchit2843,1548707601,,0,1,False,https://b.thumbs.redditmedia.com/HH4Oo_dHM8rraoLwU6L9Sx88-Ss9R_o1eRPWebuqGMk.jpg,,,,,
1584,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,5,akrrjj,self.MachineLearning,Build highly valuable and marketable analytics skills - Big Data Hadoop Certification Training,https://www.reddit.com/r/MachineLearning/comments/akrrjj/build_highly_valuable_and_marketable_analytics/,internetdigitalentre,1548707972,[removed],0,1,False,https://b.thumbs.redditmedia.com/mMdCtr4Ib36LKsOugJeN7LhtYarf00UbX2Apa5MPADM.jpg,,,,,
1585,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,6,aks4w3,self.MachineLearning,Inverse of reLU,https://www.reddit.com/r/MachineLearning/comments/aks4w3/inverse_of_relu/,jgibson_37,1548709945,"I'm planning on modifying my neural network library to have the ability to perform a backwards pass (**not** backpropagation...) which will use the inverse of the activation function (**not** derivative) i.e. for tanh activation function, it would change to tanh^(-1) not just the derivative. I am struggling to find the inverse of the reLU function, hoping someone here could help.

Thanks!",0,1,False,self,,,,,
1586,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,6,akscrp,self.MachineLearning,Ideas for PhD Thesis in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/akscrp/ideas_for_phd_thesis_in_machine_learning/,Terriermon9,1548711131,[removed],0,1,False,self,,,,,
1587,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,6,aksds4,self.MachineLearning,[R] Using active learning to polish the accuracy after learning from a dataset,https://www.reddit.com/r/MachineLearning/comments/aksds4/r_using_active_learning_to_polish_the_accuracy/,smndoo,1548711275,"The original post was removed due to missing tag I guess...

&amp;#x200B;

I am trying to create a prediction model which would return a numerical score upon receiving a set of representative numbers. 

The purpose was to replicate human quality assessment procedure upon facing visual and textual data. In my prototype, I've made a python script to extract representative data from the raw data (video and text) and trained with the human assessment score. However, due to the small assessment data size (\~60 pts), it was impossible to account for every possible variation (i.e. having low accuracy on predicting what was not trained). 

Therefore, my solution was to train using a set of statistical distribution curve/function from the existing dataset, then the trained machine accuracy would be polished by having a second round of active learning from a small number of real humans.

 I've developed the first prototype on Keras, but am willing to create a more serious version this round. What are your suggestions in terms of implementation? Also, What are your thoughts on using a statistical distribution curve to train the NN and polishing with active learning? ",0,1,False,self,,,,,
1588,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,6,aksilm,self.MachineLearning,How could ML improve this solution? (record linkage / data matching),https://www.reddit.com/r/MachineLearning/comments/aksilm/how_could_ml_improve_this_solution_record_linkage/,Shoddy_Researcher,1548711980,[removed],0,1,False,self,,,,,
1589,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,7,akss38,self.MachineLearning,[R] Ideas for PhD Thesis in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/akss38/r_ideas_for_phd_thesis_in_machine_learning/,Terriermon9,1548713407,"Hi! I'm a Master in Mathematics and Applications and I will start a PhD in the same area soon, with focus in Statistical and Machine Learning.

&amp;#x200B;

I would like to ask you some good ideas for me to propose for a PhD thesis. I've done anomaly detection in my MSc thesis and I am looking for a different field. I prefer supervised learning problems. I am also interested in NLP and Computer Vision. In your opinion, which are the greatest trends in that areas (I am opened to other fields also)?

&amp;#x200B;

Thank you very much!",4,1,False,self,,,,,
1590,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,8,akthku,self.MachineLearning,"Am I missing something with batchnorm, is tensorboard bugged out or am I going crazy?",https://www.reddit.com/r/MachineLearning/comments/akthku/am_i_missing_something_with_batchnorm_is/,MoBizziness,1548717418,[removed],0,1,False,self,,,,,
1591,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,8,aktleb,self.MachineLearning,"(Keras/tensorflow) Am I missing something here with batch norm, is tensorboard bugged out or am I just going crazy?",https://www.reddit.com/r/MachineLearning/comments/aktleb/kerastensorflow_am_i_missing_something_here_with/,MoBizziness,1548718056,[removed],0,1,False,self,,,,,
1592,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,8,aktpvi,self.MachineLearning,"[P] (Keras/tensorflow) Am I missing something here with batch norm, is tensorboard bugged out or am I just going crazy?",https://www.reddit.com/r/MachineLearning/comments/aktpvi/p_kerastensorflow_am_i_missing_something_here/,MoBizziness,1548718787,"Sorry if this is something obvious that I'm missing, I've tried /r/learnmachinelearning and they're of no help so I thought I'd try here.

&amp;#x200B;

I am using this:

&amp;#x200B;

&amp;#x200B;

        def cnn_block_1(inp, filt, kernal, b_num):
            # Layer Names
            c1_left, b1_left, a1_left, c2_left, b2_left, a2_left, c3_left, b3_left, a3_left, p_left = \
            ""c1_left_"" + b_num, ""b1_left_"" + b_num, ""a1_left_"" + b_num, ""c2_left_"" + b_num, \
            ""b2_left_"" + b_num, ""a2_left_"" + b_num, ""c3_left_"" + b_num, ""b3_left_"" + b_num, \
            ""a3_left_"" + b_num, ""p_left_"" + b_num,
    
            # Block
            c1_l = Conv2D(filters=filt, kernel_size=kernal, padding=""same"", name=c1_left)(inp)
            bn1_l = BatchNormalization(name=b1_left)(c1_l)
            a1_l = Activation(""relu"", name=a1_left)(bn1_l)
            c2_l = Conv2D(filters=filt, kernel_size=kernal, padding=""same"", name=c2_left)(a1_l)
            bn2_l = BatchNormalization(name=b2_left)(c2_l)
            a2_l = Activation(""relu"", name=a2_left)(bn2_l)
            c3_l = Conv2D(filters=filt, kernel_size=kernal, padding=""same"", name=c3_left)(a2_l)
            bn3_l = BatchNormalization(name=b3_left)(c3_l)
            a3_l = Activation(""relu"", name=a3_left)(bn3_l)
            p_l = MaxPooling2D(padding=""same"", name=p_left)(a3_l)
    
            return p_l
    
        block_number = 6
        b_num = block_number
        filter_value = 2
    
        x1 = Sequential()
        x1.add(Embedding(vocab_char_size, embedding_char_size, input_length=maxlen, mask_zero=True,         weights=[e_char], name='embedding_1', trainable=False))
        x1.add(Lambda(lambda xo: K.expand_dims(xo, axis=3)))
    
        x2 = Sequential()
        x2.add(Embedding(vocab_word_size, embedding_word_size, input_length=maxlen, mask_zero=True,     weights=[e_word], name='embedding_2', trainable=False))
        x2.add(Lambda(lambda xo: K.expand_dims(xo, axis=3)))
    
        c = Concatenate(axis=3)([x1.output, x2.output])
        left_string = list()
        left_string.append(c)
        f_value = filter_value
        for i in range(left_arm_blocks):
            c = cnn_block_1(left_string[-1], f_value, kernal_value, str(i))
            left_string.append(c)
            f_value *= 2
    
        x = Lambda(lambda xq: xq, output_shape=lambda s: s)(left_string[-1])
        flat1 = Flatten()(x)

&amp;#x200B;

To string together a bunch of predefined CNN blocks. I am saving the output of each call to the function in a list and using the last output in the list as the input for the next layer etc.

&amp;#x200B;

When I load the model on tensorboard to have a look at the architecture though, something bizarre is happening: [https://i.imgur.com/H3KCz4q.png](https://i.imgur.com/H3KCz4q.png)

&amp;#x200B;

For some reason it shows the first batch norm layer or ""b1\_left\_0"" is being used  as an input to every other batch norm layer in my entire model, including an entire other ""right"" arm of the model that is only connected to this via a concatenate layer much later on.

&amp;#x200B;

I'm assuming I'm missing something obvious here and am being dumb but I'm at a loss for how to try to work this out further as everything in my code seems to be working as intended.

&amp;#x200B;

Thanks in advance for any advice.",0,1,False,self,,,,,
1593,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,9,aku3n9,self.MachineLearning,Datasets on counseling conversations,https://www.reddit.com/r/MachineLearning/comments/aku3n9/datasets_on_counseling_conversations/,ndo3,1548721165,[removed],0,1,False,self,,,,,
1594,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,10,akuim5,arxiv.org,[R] Self-Supervised Generalisation with Meta Auxiliary Learning,https://www.reddit.com/r/MachineLearning/comments/akuim5/r_selfsupervised_generalisation_with_meta/,lorenmontez,1548723774,,6,1,False,default,,,,,
1595,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,10,akuqrf,blogs.unity3d.com,Unity announced the upcoming launch of the Obstacle Tower Challenge to test the limits of intelligence systems.,https://www.reddit.com/r/MachineLearning/comments/akuqrf/unity_announced_the_upcoming_launch_of_the/,leonchenzhy,1548725281,,0,1,False,https://b.thumbs.redditmedia.com/k6pC5L5pEcCZhQObOVF37njSnAHFCHH0TNqjHQ9wDbc.jpg,,,,,
1596,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,11,akvagw,self.MachineLearning,"CVPR 2019 reviews are out. Sad to say I got ""weak rejects"". Congratulations to all those who had positive reviews. :)",https://www.reddit.com/r/MachineLearning/comments/akvagw/cvpr_2019_reviews_are_out_sad_to_say_i_got_weak/,cdrfrk,1548728837,[removed],0,1,False,self,,,,,
1597,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,11,akvjjl,self.MachineLearning,UI Automation and ML,https://www.reddit.com/r/MachineLearning/comments/akvjjl/ui_automation_and_ml/,relia7,1548730470,[removed],0,1,False,self,,,,,
1598,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,12,akvzg0,self.MachineLearning,Where to begin in ML?,https://www.reddit.com/r/MachineLearning/comments/akvzg0/where_to_begin_in_ml/,NikosSQRT144,1548733383,[removed],0,1,False,self,,,,,
1599,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,12,akvzki,self.MachineLearning,"[P] Information Theory, Entropy and Intelligence Lecture Series",https://www.reddit.com/r/MachineLearning/comments/akvzki/p_information_theory_entropy_and_intelligence/,hiconcep,1548733407,"The most basic theories related to information and intelligence is information theory. This lecture series is to explain how entropy and information theory has to do with intelligence and story of Claude Shannon. It is produced in Korean, but all the slides are in English and English subtitle is available.

&amp;#x200B;

[Information Theory, Entropy and Intelligence (1)](https://www.youtube.com/watch?v=DZBqd0giIbk)

[Information Theory, Entropy and Intelligence (2)](https://www.youtube.com/watch?v=mBMc_3sNa9w)

[Information Theory, Entropy and Intelligence (3)](https://www.youtube.com/watch?v=5JJYqYuPIwE)",3,1,False,self,,,,,
1600,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,13,akw8tv,self.MachineLearning,Need advice on CPU choice for converting crypto mining rig into deep learning rig,https://www.reddit.com/r/MachineLearning/comments/akw8tv/need_advice_on_cpu_choice_for_converting_crypto/,asdfasdf128128,1548735143,[removed],0,1,False,self,,,,,
1601,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,13,akwbs8,self.MachineLearning,[D] Need advice on CPU choice for converting crypto mining rig into deep learning rig,https://www.reddit.com/r/MachineLearning/comments/akwbs8/d_need_advice_on_cpu_choice_for_converting_crypto/,asdfasdf128128,1548735743,"Hi,
I'm a student that's going to be taking a deep learning course this semester, and I want to use my mining rig for training. Right now, I have:
4x 1080ti, 2x 1070, Pentium Dual-Core CPU

I'm asking for advice about buying a CPU to accommodate these GPUs. From online research, I understand that clock speed isn't that important, but number of total threads is very important.

I was wondering if anyone has any recommendations on a budget CPU that will allow me to harness at least the majority of the benefit of having so many nice GPUs. I'm thinking about the Intel 7600K, which can be found here: https://www.amazon.com/Intel-i5-7600K-Desktop-Processors-BX80677I57600K/dp/B01MRRPPQS

Any advice on this? Any links to resources where I can learn about what I need? Anybody have any quantitative take on this (e.g., how  fast will the rig run if using an 8-core CPU vs 4-core vs current shitty Pentium?)

Advice would be very much appreciated!",10,1,False,self,,,,,
1602,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,13,akwgmn,self.MachineLearning,Could AlphaStar potentially be implemented wrong?,https://www.reddit.com/r/MachineLearning/comments/akwgmn/could_alphastar_potentially_be_implemented_wrong/,qudcjf7928,1548736741,[removed],0,1,False,self,,,,,
1603,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,14,akwmvc,youtu.be,Changing race of the faces in facial images using cycle generative adversarial networks.(my solo hackathon project),https://www.reddit.com/r/MachineLearning/comments/akwmvc/changing_race_of_the_faces_in_facial_images_using/,charan_1996,1548738035,,0,1,False,default,,,,,
1604,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,14,akwwe7,arxiv.org,Imitation Learning from Imperfect Demonstration,https://www.reddit.com/r/MachineLearning/comments/akwwe7/imitation_learning_from_imperfect_demonstration/,Kristery,1548740065,,1,1,False,default,,,,,
1605,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,14,akwzmh,fififummobileapps.over-blog.com,How Machine Learning Improves The Capability of Mobile Apps?,https://www.reddit.com/r/MachineLearning/comments/akwzmh/how_machine_learning_improves_the_capability_of/,fifiummobileapps,1548740804,,0,1,False,default,,,,,
1606,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,14,akwzmu,self.MachineLearning,Does the output layer of a NN need bias if it only has 1 node?,https://www.reddit.com/r/MachineLearning/comments/akwzmu/does_the_output_layer_of_a_nn_need_bias_if_it/,qudcjf7928,1548740807,[removed],0,1,False,self,,,,,
1607,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,16,akxjfc,self.MachineLearning,getting NAN using Residual Neural Network,https://www.reddit.com/r/MachineLearning/comments/akxjfc/getting_nan_using_residual_neural_network/,qudcjf7928,1548745529,[removed],1,1,False,self,,,,,
1608,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,16,akxmy2,self.MachineLearning,What is the best way to go about implementing personalization for app users (using recommendation engine)?,https://www.reddit.com/r/MachineLearning/comments/akxmy2/what_is_the_best_way_to_go_about_implementing/,FarisAi,1548746429,[removed],0,1,False,self,,,,,
1609,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,16,akxpn3,smarten.com,Improve Data Literacy and Create Citizen Data Scientists!,https://www.reddit.com/r/MachineLearning/comments/akxpn3/improve_data_literacy_and_create_citizen_data/,ElegantMicroWebIndia,1548747152,,0,1,False,default,,,,,
1610,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,16,akxrwi,self.MachineLearning,"I want to learn AI, to parse style sheets, and return a refactored version of the code. Possible?",https://www.reddit.com/r/MachineLearning/comments/akxrwi/i_want_to_learn_ai_to_parse_style_sheets_and/,lawandordercandidate,1548747798,[removed],0,1,False,self,,,,,
1611,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,17,akxzof,self.MachineLearning,Yes!,https://www.reddit.com/r/MachineLearning/comments/akxzof/yes/,csdoru,1548749971,[removed],0,1,False,self,,,,,
1612,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,17,aky09u,self.MachineLearning,Practical Deep Learning for Coders 2019,https://www.reddit.com/r/MachineLearning/comments/aky09u/practical_deep_learning_for_coders_2019/,worldwide__master,1548750154,[removed],0,1,False,self,,,,,
1613,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,17,aky13c,youtube.com,Servo conversation framework,https://www.reddit.com/r/MachineLearning/comments/aky13c/servo_conversation_framework/,liormessinger,1548750382,,0,1,False,default,,,,,
1614,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,17,aky5fw,self.MachineLearning,J.P. Morgan's Comprehensive Guide on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/aky5fw/jp_morgans_comprehensive_guide_on_machine_learning/,andrea_manero,1548751673,[removed],0,1,False,self,,,,,
1615,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,18,akyatf,youtube.com,Introduction to character level CNN in text classification with PyTorch Implementation + code + models,https://www.reddit.com/r/MachineLearning/comments/akyatf/introduction_to_character_level_cnn_in_text/,ahmedbesbes,1548753234,,0,1,False,https://b.thumbs.redditmedia.com/u6POCLHXPvOaw3ZQM4dTkZetbWyXGBH6lFyJ19WKbNM.jpg,,,,,
1616,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,18,akyd4s,github.com,Character Based CNN for text classification in PyTorch // Detailed code + Visualization + Video Demo,https://www.reddit.com/r/MachineLearning/comments/akyd4s/character_based_cnn_for_text_classification_in/,ahmedbesbes,1548753895,,0,1,False,default,,,,,
1617,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,18,akyeb7,self.MachineLearning,What're strategies of studying machine learning by reading Hands-on Machine Learning with Scikit-Learn?,https://www.reddit.com/r/MachineLearning/comments/akyeb7/whatre_strategies_of_studying_machine_learning_by/,homchange,1548754235,[removed],0,1,False,self,,,,,
1618,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,18,akyfkr,arxiv.org,[1901.09437] block coordinate descent is far more efficient than SGD,https://www.reddit.com/r/MachineLearning/comments/akyfkr/190109437_block_coordinate_descent_is_far_more/,statmlsn,1548754571,,9,1,False,default,,,,,
1619,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,18,akyiu8,digit.fyi,"Stop Saying ""Machine Learning Algorithm"". Please.",https://www.reddit.com/r/MachineLearning/comments/akyiu8/stop_saying_machine_learning_algorithm_please/,WhoopDeFreakinDo,1548755451,,0,1,False,default,,,,,
1620,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,18,akyj8y,self.MachineLearning,[P] Neural network for car recognition,https://www.reddit.com/r/MachineLearning/comments/akyj8y/p_neural_network_for_car_recognition/,behindthedash,1548755566,"The last year I attended the field of machine learning and at the end of 2018 achieved some results that I want to share. I trained a neural network to recognize a car by a photo and created this simple [**demo**](http://68.183.4.242/) for illustration.

# # The data

The starting point for the task was the [**Stanford Cars Dataset**](https://www.kaggle.com/jutrera/stanford-car-dataset-by-classes-folder). Some classes of this dataset contain quite a lot of errors (e.g. models of Audi or Aston Martin are often difficult to tell apart for a human being). So I took only 48 classes from the dataset and cleaned them up. During training, the data was augmented by rotations, reflections and messing around with colors.

# # Base models

Sure enough, I applied transfer learning for the task. I used as feature extractors the models that are among the best performing on the ImageNet dataset (ResNet50, Inception V3 and Exception). First, I simply concatenated their outputs and stacked some dense layers on the top. This approach didnt work particularly well - about 55% accuracy on the test set.

# # Approach

I tried to analyze how I recognize a car that I see in the street. The easiest thing to tell about a car is its body type. There are only a few of them and usually, they have distinct features. As a rule, a manufacturer is also not a problem. And recognizing the particular model almost always takes another moment or two. Sometimes it may take a minute if a model is rare. In this case, I look at the features like the shape of the radiator grill or of lamps. These features are decisive, I think. The shape of the doors and windows is also important. And the information about the body type and the manufacturer is certainly also helpful.

It looked like a sequence to me, so I decided to try an LSTM. It should take the concatenated feature vector yielded by pretrained models and predict body type on the first step, manufacturer on the second and model on the third. And it should be bidirectional because the sequence probably works in the opposite direction as well. At the top of that, another LSTM should gather these predictions together and yield the final prediction of make and model. This approach raised the test accuracy significantly, we got about 72% on the test set.

I have to admit that LSTM is a bit of overkill here. The sequence is short, so an ordinary RNN should do just as well. But LSTM is just as easy to implement in Keras and barely adds a significant lag to the overall evaluation time, so it wasnt of the highest priority to try and replace the LSTM.

# # Tweaks and tricks

Next, I tried to get rid of the redundant features the network might overfit to, like the characteristics of the background. Dropout layers didnt help a lot, so I used another ready solution provided by the amazing [**ImageAI**](https://github.com/OlafenwaMoses/ImageAI) library. I applied the yolo model to detect a car on the image and crop it accordingly on the preprocessing step. This gave us other 10% of test accuracy.

The last step was made by removing all dropout layers which gave us 5 more percent. It surprised me first, but now I think that since we got rid of almost all background after the detection step, the network could infer almost always only relevant features from the input. So the dropouts probably became more harmful than helpful after that.

# # Architecture

I think there is no reason to describe the architecture we used any further, so Ill just provide a diagram

https://i.redd.it/33lwf11j1cd21.png

# # Analysis

The final result, for now, is 87,5% of top 1 accuracy and 98,5% of top 5 accuracy. Top 1 accuracy means that a prediction with the highest score is correct and top 5 accuracy means that the correct answer is among the predictions with top 5 scores.

Most of the errors the network makes are easy to make. For example, this Audi S4 is confused with an Audi A4 of the previous generation:

https://i.redd.it/jwdh485q2cd21.png

This Bentley Continental is classified as Audi A5:

https://i.redd.it/o2zi56ur2cd21.png

Some errors suggest that the network still takes color into account too much. This Bentley is recognized as Chevrolet Cobalt SS many of which are of red color in the training set:

https://i.redd.it/zyiryn6t2cd21.png

It is interesting how the network behaves when it sees a model that wasnt in the training set. This Porsche 911 was classified as Chevrolet Corvette which is also a sports car of a similar kind:

https://i.redd.it/h9k6kk8v2cd21.png

And this Honda CRV is classified as Acura ZDX:

https://i.redd.it/yvzrk3vw2cd21.png

Some results are funny though:

https://i.redd.it/uneeg93z2cd21.png

In general, it looks like the network gets the idea of a car on a photo correctly. A sports car is rarely confused with an SUV and a convertible most likely may be confused with another convertible of the same kind:

https://i.redd.it/wfqe39h03cd21.png

I think that many problems may go away if we use a bigger dataset. And that is what we are working on right now.

Also, I cant help thinking that maybe the features we get from the last layers of the pretrained models are too general. It may happen that if we use the earlier layers we get the better results. It was difficult to check while the development was happening on a local machine because the earlier layers are much bigger than the last one and demand much more memory. Now that I moved to [**Google Cloud Platform**](https://cloud.google.com/)and to [**FloydHub**](https://www.floydhub.com/)I think, we can give it a try

# # Stack

All development was done in Python Notebooks and [**Keras**](https://keras.io/)and deployed using [**Flask**](http://flask.pocoo.org/)and [**Docker**](https://www.docker.com/).

If you are interested, you can find the code on [**GitHub**](https://github.com/iPhaeton/car_identification).",29,1,False,https://b.thumbs.redditmedia.com/GrnB1oIz_PZ1GiLvVHzG6QBfLfwsfyvMsOBSIUsD8ZM.jpg,,,,,
1621,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,18,akyk82,self.MachineLearning,"[P] ""Learning to Drive Smoothly in Minutes"" - Reinforcement Learning on a Small Racing Car (SAC and VAE features)",https://www.reddit.com/r/MachineLearning/comments/akyk82/p_learning_to_drive_smoothly_in_minutes/,araffin2,1548755844,"Blog post: [https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4](https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4)

Github repo to reproduce the results: [https://github.com/araffin/learning-to-drive-in-5-minutes](https://github.com/araffin/learning-to-drive-in-5-minutes)

&amp;#x200B;

In short: it learns a control policy in \~25 trials (8000 steps, \~5-10 minutes) using Soft Actor-Critic (SAC) on VAE features.

For smoothing the control, the method adds a constrain on the steering rate and augment the input with the command history.",2,1,False,self,,,,,
1622,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,19,akym2n,self.MachineLearning,What is the current and previous SOTA in domain specific entity extraction?,https://www.reddit.com/r/MachineLearning/comments/akym2n/what_is_the_current_and_previous_sota_in_domain/,boredhumen101,1548756343,[removed],0,1,False,self,,,,,
1623,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,19,akyrv7,self.MachineLearning,Which profession you can acquire after Machine Learning training?,https://www.reddit.com/r/MachineLearning/comments/akyrv7/which_profession_you_can_acquire_after_machine/,techethans,1548757947,"Nowadays machine learning uses with python to provide learning training to the students and will ready for corporate work. Anyone get [machine learning training in Pune](https://www.sooperarticles.com/education-articles/get-benefits-pursuing-machine-learning-python-1706651.html) with python can acquire profession as a Data Scientist, Business Analysts, Robotics Engineer and Hadoop Developers.",0,1,False,self,,,,,
1624,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,19,akys1e,self.MachineLearning,Panoptic Segmentation Introductory Article,https://www.reddit.com/r/MachineLearning/comments/akys1e/panoptic_segmentation_introductory_article/,Jt_dm1,1548757991,[removed],0,1,False,self,,,,,
1625,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,20,akyzwu,self.MachineLearning,[P] Can agents learn to paint inside their own dreams?,https://www.reddit.com/r/MachineLearning/comments/akyzwu/p_can_agents_learn_to_paint_inside_their_own/,Reiinakano,1548760111,"Shared this on Twitter previously. This is mostly a copy paste. I link to a more detailed blog post below.

&amp;#x200B;

Sharing my winter break project. I tried combining [World Models](http://worldmodels.github.io)and [SPIRAL](https://deepmind.com/blog/learning-to-generate-images/) to see if an agent can learn to paint inside its own dream. It can! These strokes are generated purely inside a world model, yet transfer seamlessly to a real paint program.

&amp;#x200B;

![video](8vxs4l13gcd21)

&amp;#x200B;

The approach is simple. I train a world model that can predict the output stroke of a real paint program for a corresponding action, in effect creating a fully differentiable paint program. I then train an agent to paint using the same adversarial method detailed in SPIRAL.

I find the most amazing thing about this is how quickly it converges. The original SPIRAL paper shows it converges in 50M steps (using multiple multi-CPU/GPU instances). The MNIST-trained agent I show here used 11K steps and took \~10 hours to train on a free Colaboratory GPU.

&amp;#x200B;

&amp;#x200B;

![gif](1jczq9zrgcd21)

&amp;#x200B;

MNIST was easy, so I tried KMNIST. Simple chars like  and  were easy but overall, the dataset is much harder. I'm confident it can be improved with more strokes + training steps, though. Stroke order is also all over the place. 

&amp;#x200B;

&amp;#x200B;

![gif](hd0zg28vgcd21)

&amp;#x200B;

I also trained a full color 15-stroke agent on CelebA. The outputs seem on par with SPIRAL (orig. version). Unlike SPIRAL, my agent doesn't make strokes that are fully covered by future strokes. I attribute this to the ease of credit assignment when full gradients are available.

&amp;#x200B;

&amp;#x200B;

![video](paw7iv7xgcd21)

&amp;#x200B;

A problem I found with this approach was how it reacts to discrete actions (eg. binary 0 or 1 to lift a brush or not. What happens at 0.5?). The gradients in this case do not model the real world properly and can cause weird behavior. In the worst case, it can kill training.

&amp;#x200B;

Perhaps in these cases, we can find a way to use gradient-free optimization (ES) to learn to explore these discrete actions, while keeping the power of regular gradient descent for continuous actions.  Open to ideas

&amp;#x200B;

Anyway, I have to state here that I was not the first to think of this. While planning this tweet thread, I decided to look at some citations of the World Models paper and saw [https://openreview.net/forum?id=HJxwDiActX ](https://t.co/WKNoSXmfJM). Apparently, I am some months late to the party.

&amp;#x200B;

I use a different painter, training method, and agent architecture, but the core idea remains the same: fully differentiable world models work well for generation. Painful lesson in doing proper literature review before working. Guess that's why I'm not a researcher ;)

&amp;#x200B;

In any case, I'm very excited about exploring the possibilities of world models for creative ML in general. Some ideas: Style transfer, DeepDream w/ a painter parameterization, going beyond brush strokes and changing the world model itself as a way to produce art.

&amp;#x200B;

Link to a more detailed blog post: [https://reiinakano.github.io/2019/01/27/world-painters.html](https://reiinakano.github.io/2019/01/27/world-painters.html)

Link to Twitter thread: [https://twitter.com/ReiiYoda/status/1083772843920318464](https://twitter.com/ReiiYoda/status/1083772843920318464)",0,1,False,self,,,,,
1626,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,20,akz42t,youtu.be,[D] COCO Dataset Format - Complete Walkthrough Tutorial,https://www.reddit.com/r/MachineLearning/comments/akz42t/d_coco_dataset_format_complete_walkthrough/,akTwelve,1548761211,,1,1,False,https://a.thumbs.redditmedia.com/JZdIIbdy_NZYHR7wFsiOHLChMHm_WvimgDz2StKXjq0.jpg,,,,,
1627,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,20,akz8xc,self.MachineLearning,[R] Learning Role-based Graph Embeddings,https://www.reddit.com/r/MachineLearning/comments/akz8xc/r_learning_rolebased_graph_embeddings/,benitorosenberg,1548762478,"&amp;#x200B;

&amp;#x200B;

https://i.redd.it/bihqk27gocd21.png

Paper: [https://arxiv.org/abs/1802.02896](https://arxiv.org/abs/1802.02896)

Github: [https://github.com/benedekrozemberczki/role2vec](https://github.com/benedekrozemberczki/role2vec)

Abstract:

Random walks are at the heart of many existing network embedding methods. However, such algorithms have many limitations that arise from the use of random walks, e.g., the features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to vertex identity. In this work, we introduce the Role2Vec framework which uses the flexible notion of attributed random walks, and serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many others that leverage random walks. Our proposed framework enables these methods to be more widely applicable for both transductive and inductive learning as well as for use on graphs with attributes (if available). This is achieved by learning functions that generalize to new nodes and graphs. We show that our proposed framework is effective with an average AUC improvement of 16.55% while requiring on average 853x less space than existing methods on a variety of graphs.",0,1,False,https://b.thumbs.redditmedia.com/OhYt0-mi0rAKDcqf9DAwNyqoo674gWBAgGp0oeyn6bM.jpg,,,,,
1628,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,20,akzaur,self.MachineLearning,Images of Aphids,https://www.reddit.com/r/MachineLearning/comments/akzaur/images_of_aphids/,operationsspatial,1548762974,[removed],0,1,False,self,,,,,
1629,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,21,akzf3o,self.MachineLearning,"wich GPU , price-capacity [gtx 1070 or rtx 2060] ?",https://www.reddit.com/r/MachineLearning/comments/akzf3o/wich_gpu_pricecapacity_gtx_1070_or_rtx_2060/,Don_Celino,1548763973,[removed],0,1,False,self,,,,,
1630,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,21,akzfrf,analyticsindiamag.com,Does Machine Learning Share The Same Fate Of Mathematical Unsolvability?,https://www.reddit.com/r/MachineLearning/comments/akzfrf/does_machine_learning_share_the_same_fate_of/,sunps,1548764133,,0,1,False,default,,,,,
1631,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,21,akzodj,self.MachineLearning,"Machine Vision Market Share, Industry Analysis, Share, Countries, Growth, Top Key Players and Future Insights Report 2025",https://www.reddit.com/r/MachineLearning/comments/akzodj/machine_vision_market_share_industry_analysis/,AcceptableExplorer0,1548766083,"  

growing at a CAGR of 5.0% from 2018 to 2025.

What is Machine Vision?

Machine vision is a term that is used to describe the methods as well as technology that is used to extract information from an image in a way that is automated. This process differs from image processing due to the fact that image processing results in another image while machine vision involves the extraction of the data from an image. Machine vision involves the methods as well as technology that is used to provide services such as image-based automatic analysis as well as inspection. These services are provided to areas such as process control, robotic guidance, and automatic inspection. Machine vision involves the incorporation of software, technology as well as hardware products. The process begins with the planning of the overall requirements which is subsequently followed by developing a solution.

**Get || FREE Sample Report @** [**https://www.verifiedmarketresearch.com/download-sample/?rid=3983**](https://www.verifiedmarketresearch.com/download-sample/?rid=3983)

Machine Vision Market Outlook

Rising need for automation and quality inspection, increasing demand for vision-guided robotic systems and increment in adoption rate of 3d machine vision systems have been driving the global machine vision market. On the other hand, changing requirements of end-user and lacking flexible machine vision solutions might hinder the overall market at a global level.The Global Substation Automation Market study report will provide a valuable insight with an emphasis on global market including some of the major players such as ABB Ltd., Siemens AG, General Electric, Cisco Systems, Inc., Schneider Electric, Eaton Corporation PLC, Honeywell International, Inc., Schweitzer Engineering Laboratories, Inc., Novatech LLC, Crompton Greaves. Our market analysis also entails a section solely dedicated to such major players wherein our analysts provide an insight into the financial statements of all the major players, along with its product benchmarking and SWOT analysis.

**Full Report is Available @** [**https://www.verifiedmarketresearch.com/product/global-machine-vision-market-size-and-forecast-to-2025/**](https://www.verifiedmarketresearch.com/product/global-machine-vision-market-size-and-forecast-to-2025/)

**About Us:**

 Verified Market Research' has been providing Research Reports, with up to date information, and in-depth analysis, for several years now, to individuals and companies alike that are looking for accurate Research Data. Our aim is to save your Time and Resources, providing you with the required Research Data, so you can only concentrate on Progress and Growth. Our Data includes research from various industries, along with all necessary statistics like Market Trends, or Forecasts from reliable sources.  
 

**Contact Us:**

Mr. Ankush

Call: +1 (650) 781 4080

Email:sales@verifiedmarketresearch.com",0,1,False,self,,,,,
1632,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,22,akztta,self.MachineLearning,[R] Learning Role-based Graph Embeddings (IJCAI 2018),https://www.reddit.com/r/MachineLearning/comments/akztta/r_learning_rolebased_graph_embeddings_ijcai_2018/,benitorosenberg,1548767237,"&amp;#x200B;

&amp;#x200B;

https://i.redd.it/7ygs7gof2dd21.png

Python: [https://github.com/benedekrozemberczki/role2vec](https://github.com/benedekrozemberczki/role2vec)

Paper: [https://arxiv.org/pdf/1802.02896.pdf](https://arxiv.org/pdf/1802.02896.pdf)

Abstract:

Random walks are at the heart of many existing network embedding methods. However, such algorithms have many limitations that arise from the use of random walks, e.g., the features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to vertex identity. In this work, we introduce the Role2Vec framework which uses the flexible notion of attributed random walks, and serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many others that leverage random walks. Our proposed framework enables these methods to be more widely applicable for both transductive and inductive learning as well as for use on graphs with attributes (if available). This is achieved by learning functions that generalize to new nodes and graphs. We show that our proposed framework is effective with an average AUC improvement of 16.55% while requiring on average 853x less space than existing methods on a variety of graphs.

&amp;#x200B;",5,1,False,https://b.thumbs.redditmedia.com/mD8z09cvuxezm-a_BFHp6LNMFJCtxpGBzOX7T6drDKs.jpg,,,,,
1633,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,22,akzuhh,self.MachineLearning,Intelligent Quiz Tool using ML and AI,https://www.reddit.com/r/MachineLearning/comments/akzuhh/intelligent_quiz_tool_using_ml_and_ai/,drocobeth,1548767374,[removed],0,1,False,self,,,,,
1634,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,22,akzzo0,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/akzzo0/global_machine_learning_market_size_outlook/,vardhan1020,1548768470,,0,1,False,default,,,,,
1635,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,22,akzzz3,self.MachineLearning,[Project] Can agents learn to paint inside their own dreams?,https://www.reddit.com/r/MachineLearning/comments/akzzz3/project_can_agents_learn_to_paint_inside_their/,Reiinakano,1548768533,"Shared this on Twitter previously. Here is a more detailed [blog post](https://reiinakano.github.io/2019/01/27/world-painters.html)

Sharing my winter break project. I tried combining [World Models](http://worldmodels.github.io/)and [SPIRAL](https://deepmind.com/blog/learning-to-generate-images/) to see if an agent can learn to paint inside its own dream. It can! These strokes are generated purely inside a world model, yet transfer seamlessly to a real paint program.

&amp;#x200B;

![video](lgpmzutw5dd21 ""GIF showing target, what the network dreams vs what it's actually drawing"")

The approach is simple. I train a world model that can predict the output stroke of a real paint program for a corresponding action, in effect creating a fully differentiable paint program. I then train an agent to paint using the same adversarial method detailed in SPIRAL.

I find the most amazing thing about this is how quickly it converges. The original SPIRAL paper shows it converges in 50M steps (using multiple multi-CPU/GPU instances). The MNIST-trained agent I show here used 11K steps and took \~10 hours to train on a free Colaboratory GPU.

&amp;#x200B;

![gif](mrgyk6q26dd21 ""MNIST reconstructions"")

MNIST was easy, so I tried KMNIST. Simple chars like  and  were easy but overall, the dataset is much harder. I'm confident it can be improved with more strokes + training steps, though. Stroke order is also all over the place.

&amp;#x200B;

![gif](pzwo0eg46dd21 ""KMNIST reconstructions"")

I also trained a full color 15-stroke agent on CelebA. The outputs seem on par with SPIRAL (orig. version). Unlike SPIRAL, my agent doesn't make strokes that are fully covered by future strokes. I attribute this to the ease of credit assignment when full gradients are available.

&amp;#x200B;

![gif](mn13v6q56dd21 ""CelebA reconstructions"")

A problem I found with this approach was how it reacts to discrete actions (eg. binary 0 or 1 to lift a brush or not. What happens at 0.5?). The gradients in this case do not model the real world properly and can cause weird behavior. In the worst case, it can kill training.

Perhaps in these cases, we can find a way to use gradient-free optimization (ES) to learn to explore these discrete actions, while keeping the power of regular gradient descent for continuous actions. Open to ideas

Anyway, I have to state here that I was not the first to think of this. While planning this tweet thread, I decided to look at some citations of the World Models paper and saw [https://openreview.net/forum?id=HJxwDiActX ](https://t.co/WKNoSXmfJM). Apparently, I am some months late to the party.

I use a different painter, training method, and agent architecture, but the core idea remains the same: fully differentiable world models work well for generation. Painful lesson in doing proper literature review before working. Guess that's why I'm not a researcher ;)

In any case, I'm very excited about exploring the possibilities of world models for creative ML in general. Some ideas: Style transfer, DeepDream w/ a painter parameterization, going beyond brush strokes and changing the world model itself as a way to produce art.

Link to a more detailed blog post: [https://reiinakano.github.io/2019/01/27/world-painters.html](https://reiinakano.github.io/2019/01/27/world-painters.html)

Link to Twitter thread: [https://twitter.com/ReiiYoda/status/1083772843920318464](https://twitter.com/ReiiYoda/status/1083772843920318464)",0,1,False,https://b.thumbs.redditmedia.com/WY2sD8-5jtsDsbjSwq_XJ3ZaXaFzPtQoLJa168cu_LI.jpg,,,,,
1636,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,22,al03ec,self.MachineLearning,[D] Make your Python data processing workflow communicate with AWS,https://www.reddit.com/r/MachineLearning/comments/al03ec/d_make_your_python_data_processing_workflow/,arnauda9,1548769239,"Airflow is a platform to easily declare data processing workflows in Python. Here is an article I wrote about how Airflow connections work. Any feedback would be much appreciated!

[https://blog.sicara.com/automate-aws-tasks-boto3-airflow-hooks-593c3120e8fc](https://blog.sicara.com/automate-aws-tasks-boto3-airflow-hooks-593c3120e8fc)",2,1,False,self,,,,,
1637,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,23,al0i9z,self.MachineLearning,[P] Can agents learn to paint inside their own dreams?,https://www.reddit.com/r/MachineLearning/comments/al0i9z/p_can_agents_learn_to_paint_inside_their_own/,Reiinakano,1548772091,"Shared this on Twitter previously. Here is a more detailed [blog post](https://reiinakano.github.io/2019/01/27/world-painters.html)

Sharing my winter break project. I tried combining [World Models](http://worldmodels.github.io/)and [SPIRAL](https://deepmind.com/blog/learning-to-generate-images/) to see if an agent can learn to paint inside its own dream. It can! These strokes are generated purely inside a world model, yet transfer seamlessly to a real paint program.

&amp;#x200B;

*Processing gif tpc6m2srgdd21...*

The approach is simple. I train a world model that can predict the output stroke of a real paint program for a corresponding action, in effect creating a fully differentiable paint program. I then train an agent to paint using the same adversarial method detailed in SPIRAL.

I find the most amazing thing about this is how quickly it converges. The original SPIRAL paper shows it converges in 50M steps (using multiple multi-CPU/GPU instances). The MNIST-trained agent I show here used 11K steps and took \~10 hours to train on a free Colaboratory GPU.

&amp;#x200B;

[MNIST reconstructions](https://i.redd.it/g7t8gr6vgdd21.gif)

MNIST was easy, so I tried KMNIST. Simple chars like  and  were easy but overall, the dataset is much harder. I'm confident it can be improved with more strokes + training steps, though. Stroke order is also all over the place.

&amp;#x200B;

*Processing gif ta7ornuwgdd21...*

I also trained a full color 15-stroke agent on CelebA. The outputs seem on par with SPIRAL (orig. version). Unlike SPIRAL, my agent doesn't make strokes that are fully covered by future strokes. I attribute this to the ease of credit assignment when full gradients are available.

&amp;#x200B;

*Processing gif ksamaj5ygdd21...*

A problem I found with this approach was how it reacts to discrete actions (eg. binary 0 or 1 to lift a brush or not. What happens at 0.5?). The gradients in this case do not model the real world properly and can cause weird behavior. In the worst case, it can kill training.

Perhaps in these cases, we can find a way to use gradient-free optimization (ES) to learn to explore these discrete actions, while keeping the power of regular gradient descent for continuous actions. Open to ideas

Anyway, I have to state here that I was not the first to think of this. While planning this tweet thread, I decided to look at some citations of the World Models paper and saw [https://openreview.net/forum?id=HJxwDiActX ](https://t.co/WKNoSXmfJM). Apparently, I am some months late to the party.

I use a different painter, training method, and agent architecture, but the core idea remains the same: fully differentiable world models work well for generation. Painful lesson in doing proper literature review before working. Guess that's why I'm not a researcher ;)

In any case, I'm very excited about exploring the possibilities of world models for creative ML in general. Some ideas: Style transfer, DeepDream w/ a painter parameterization, going beyond brush strokes and changing the world model itself as a way to produce art.

Link to a more detailed blog post: [https://reiinakano.github.io/2019/01/27/world-painters.html](https://reiinakano.github.io/2019/01/27/world-painters.html)

Link to Twitter thread: [https://twitter.com/ReiiYoda/status/1083772843920318464](https://twitter.com/ReiiYoda/status/1083772843920318464)",0,1,False,self,,,,,
1638,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,23,al0j7l,self.MachineLearning,How to generate simple descriptions based on groups of data as inputs.,https://www.reddit.com/r/MachineLearning/comments/al0j7l/how_to_generate_simple_descriptions_based_on/,redditpertu,1548772266,[removed],0,1,False,self,,,,,
1639,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,23,al0jd2,self.MachineLearning,[Project] Using Mask R-CNN to detect parking spaces from webcam feed,https://www.reddit.com/r/MachineLearning/comments/al0jd2/project_using_mask_rcnn_to_detect_parking_spaces/,ceceshao1,1548772295,"Pretty fun project to build a parking space detector! 

&amp;#x200B;

Here's the link to the full tutorial:

[https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400](https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnn-and-python-955f2231c400)

https://i.redd.it/ryths2mrgdd21.gif",3,1,False,https://b.thumbs.redditmedia.com/qwqDGB8j4KeuBJGCLczKuQRKUuU42meMQ4vrxUaIq5E.jpg,,,,,
1640,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,23,al0l60,self.MachineLearning,Possible? - AI Youtube Star,https://www.reddit.com/r/MachineLearning/comments/al0l60/possible_ai_youtube_star/,Jandevries101,1548772627,"Hi Everyone,

&amp;#x200B;

So something came to my mind surounding, Youtube. It is based on that AI that controlled a Twitter Account,  Since Youtube has an api where you can upload and edit video's etc, it would be somewhat possible to make a Youtube account and then have an ai upload content to it and based on the likes or dislikes he adapts his video's content, whiles still following Youtube Guidelines on what you can post.

&amp;#x200B;

What are the options for making such thing possible?

&amp;#x200B;

Jan",0,1,False,self,,,,,
1641,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,23,al0lvl,self.MachineLearning,[D] Variational Autoencoders are not autoencoders,https://www.reddit.com/r/MachineLearning/comments/al0lvl/d_variational_autoencoders_are_not_autoencoders/,PaulRubenstein,1548772758,"I've come across many researchers who have encountered the following problem: you train a Variational Autoencoder with a flexible decoder, get good log-likelihoods, but then realise that your model has learned to completely ignore the latent code. 

Although it's not complicated to see why this happens, the fact that this behaviour is omitted from most intro courses/turorials on VAEs makes it very unexpected to most people. In this blog post I explain why it happens.

[http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/](http://paulrubenstein.co.uk/variational-autoencoders-are-not-autoencoders/)",79,1,False,self,,,,,
1642,MachineLearning,t5_2r3gv,2019-1-29,2019,1,29,23,al0plj,self.MachineLearning,"I need advice, Do you have any idea for a good ML project.",https://www.reddit.com/r/MachineLearning/comments/al0plj/i_need_advice_do_you_have_any_idea_for_a_good_ml/,AlphaProgrammer007,1548773406,"I am a first year computer science (engineering) student. And we have to choose an ML project for the end of year project. Well, I truly don't know anything about ML, so I'd like you to help me brain-storm some project ideas. Preferably a project that'll teach me much about ML, haves some data available online(or easy to gather it's data), and that's a good addition to my CV. Sidenote: There' no problem with Math (French System).",0,1,False,self,,,,,
1643,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,0,al0v4r,self.MachineLearning,[P] Deploying Pytorch models using Tensorflow Serving,https://www.reddit.com/r/MachineLearning/comments/al0v4r/p_deploying_pytorch_models_using_tensorflow/,barty777,1548774366,"Our team has recently started developing models in Pytorch instead of Tensorflow which was the standard so far. The Pytorch is usually praised for its simplicity and elegancy, but Tensorflow excels at efficient model deployment because of the dedicated Tensorflow Serving component. In the blog post on the link below, we explain how to port Pytorch models to the Tensorflow Serving format, thus combining the best features from both Pytorch and Tensorflow. Full conversion code is also available in the blog!

  
Hope you'll find it useful!  
[https://medium.com/styria-data-science-tech-blog/running-pytorch-models-in-production-fa09bebca622](https://medium.com/styria-data-science-tech-blog/running-pytorch-models-in-production-fa09bebca622)",5,1,False,self,,,,,
1644,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,0,al16vr,self.MachineLearning,Using Keras (python) and 1000 images how can I find 10 images similar to one?,https://www.reddit.com/r/MachineLearning/comments/al16vr/using_keras_python_and_1000_images_how_can_i_find/,Giusepo,1548776344,[removed],0,1,False,self,,,,,
1645,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,1,al1fjf,self.MachineLearning,[P] Sharing my winter break project. Can agents learn to paint inside their own dreams?,https://www.reddit.com/r/MachineLearning/comments/al1fjf/p_sharing_my_winter_break_project_can_agents/,Reiinakano,1548777745,[removed],3,1,False,self,,,,,
1646,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,1,al1i3s,towardsdatascience.com,[P] Learning NLP Language Models with Real Data - With Complete Code Notebook,https://www.reddit.com/r/MachineLearning/comments/al1i3s/p_learning_nlp_language_models_with_real_data/,osbornep,1548778148,,1,1,False,https://a.thumbs.redditmedia.com/2OH6lLC41JuqHgw8wBSyFVZ5340w9tZ86L1opor-5m4.jpg,,,,,
1647,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,1,al1pe0,self.MachineLearning,[D] First TextWorld Problems - Microsoft Research Montreals latest AI competition,https://www.reddit.com/r/MachineLearning/comments/al1pe0/d_first_textworld_problems_microsoft_research/,tavianator,1548779331,"https://www.microsoft.com/en-us/research/blog/first-textworld-problems-microsoft-research-montreals-latest-ai-competition-is-really-cooking/

We launched this competition last month at NeurIPS.  I figured I'd post it here too in case anyone's interested.  The goal is to make an RL agent that can learn to play a class of text based games.  AMA about the competition or the TextWorld framework itself!

Direct link to the competition: http://aka.ms/textworld-challenge

Try out a game in your browser: https://www.microsoft.com/en-us/research/project/textworld/#!try-it",4,1,False,self,,,,,
1648,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,1,al1u0n,self.MachineLearning,PyTorch tutorial on google colaboratory,https://www.reddit.com/r/MachineLearning/comments/al1u0n/pytorch_tutorial_on_google_colaboratory/,param87,1548780059,[removed],0,1,False,self,,,,,
1649,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,1,al1vvk,self.MachineLearning,[D] Resources for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/al1vvk/d_resources_for_machine_learning/,bradleykins,1548780356,"Hey guys, im looking for some free resources for machine learning, specifically neural networks etc.

&amp;#x200B;

I have been googling around and found a few things, mainly people suggesting subsacribing / paying to do Andrew Ng's coursera or buy his book. Does anyone know of Free resources that come in the form of projects?   


This is the best way i learn and would love if someone has some. FYI i dont have the time to commit to a paid service due to having a real job and it being time consuming so i fear paying $400 for a course only to not complete it by the deadline to be quite worrying. :(

&amp;#x200B;

Thanks In Advance!",6,1,False,self,,,,,
1650,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,1,al1xjn,self.MachineLearning,[D] How to learn design experiments ?,https://www.reddit.com/r/MachineLearning/comments/al1xjn/d_how_to_learn_design_experiments/,mayguntr,1548780623,"After two rejection in a row for different papers with similar comments like ""novel and interesting idea, but experimental evaluation is kind a turnoff"", I am wondering how can I improve my skills about designing, executing and interpreting the experiments ? ",3,1,False,self,,,,,
1651,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,2,al28ox,self.MachineLearning,[D] How to motivate DNN architectural design?,https://www.reddit.com/r/MachineLearning/comments/al28ox/d_how_to_motivate_dnn_architectural_design/,SirM2X,1548782388,"Disclaimer: When it comes to publishing DNN papers, I'm a noob. So bear with me please.

&amp;#x200B;

I recently submitted my first DNN paper for review, which achieves state-of-the-art results on multiple datasets. The architecture of the network is minimal and effective.

Starting from a well-known model, I ran lots and lots of experiments and came up with an architecture that worked particularly well for the task at hand. The model I found does not closely resemble the original model much.

&amp;#x200B;

Issue is that two reviewers have picked on this an have said that I did not really motivate my network architecture. Truth is, my architecture ""just works"", and I cannot think of any reasonable deep discussion as to why it works well.  How can I respond to these reviews? How can I motivate an architecture? I thought iterative design was inherent to every DNN research...

&amp;#x200B;

Thanks for your help in advance.",6,1,False,self,,,,,
1652,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,2,al2acu,medium.com,[N] AAAI-19 Announces Best Papers,https://www.reddit.com/r/MachineLearning/comments/al2acu/n_aaai19_announces_best_papers/,gwen0927,1548782659,,0,1,False,https://b.thumbs.redditmedia.com/KCvsU5oeyrqGVQz7xo0sJhiMgC6kdy6hJ48jNspEMto.jpg,,,,,
1653,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,2,al2ak7,self.MachineLearning,Should I go straight to learning Tensorflow or other popular ML libraries?,https://www.reddit.com/r/MachineLearning/comments/al2ak7/should_i_go_straight_to_learning_tensorflow_or/,SenseDeletion,1548782695,[removed],0,1,False,self,,,,,
1654,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,2,al2cgu,self.MachineLearning,"Let's learn programming together: Hackathon in Germany. February 14 -17th. Challenges from John Deere, SAP, Bilfinger and more. Free Hotel + Travel Expense + Beer&amp;Pizza! 9k in prizes. Apply at hack-days.de",https://www.reddit.com/r/MachineLearning/comments/al2cgu/lets_learn_programming_together_hackathon_in/,HackdaysGermany,1548782987,[removed],1,1,False,https://b.thumbs.redditmedia.com/cYNMZ7y6typGumOTHYmQOLhfV7dIn50Z8gDi12rfMNs.jpg,,,,,
1655,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,2,al2dxp,self.MachineLearning,Making Sense of the Machine,https://www.reddit.com/r/MachineLearning/comments/al2dxp/making_sense_of_the_machine/,TMStratusGrid,1548783204,[removed],0,1,False,self,,,,,
1656,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,3,al2qi9,self.MachineLearning,[D] Using statistical distribution to fill in the data gap and polishing with active leaning,https://www.reddit.com/r/MachineLearning/comments/al2qi9/d_using_statistical_distribution_to_fill_in_the/,smndoo,1548785149,"Third time trying as I put the wrong tag before...

&amp;#x200B;

I am trying to create a prediction model which would return a numerical score upon receiving a set of representative numbers.

The purpose was to replicate human quality assessment procedure upon facing visual and textual data. In my prototype, I've made a python script to extract representative data from the raw data (video and text) and trained with the human assessment score. However, due to the small assessment data size (\~60 pts), it was impossible to account for every possible variation (i.e. having low accuracy on predicting what was not trained).

Therefore, my solution was to train using a set of statistical distribution curve/function from the existing dataset, then the trained machine accuracy would be polished by having a second round of active learning from a small number of real humans.

I've developed the first prototype on Keras, but am willing to create a more serious version this round. What are your suggestions in terms of implementation? Also, What are your thoughts on using a statistical distribution curve to train the NN and polishing with active learning?",3,1,False,self,,,,,
1657,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,3,al2rc5,medium.com,[N] German-Canadian AI Startup TwentyBN: Intelligent Avatars is AI's Next Adjacent Possible,https://www.reddit.com/r/MachineLearning/comments/al2rc5/n_germancanadian_ai_startup_twentybn_intelligent/,nahuak,1548785263,,0,1,False,https://b.thumbs.redditmedia.com/km1Z-hEOu4450edXeStovCzm0LZw4H6osawomxmPkxw.jpg,,,,,
1658,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,3,al2y6r,self.MachineLearning,Where do I begin?,https://www.reddit.com/r/MachineLearning/comments/al2y6r/where_do_i_begin/,LoneWolf0936,1548786283,[removed],0,1,False,self,,,,,
1659,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,3,al3aou,self.MachineLearning,making a new podcast specializing in Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/al3aou/making_a_new_podcast_specializing_in_artificial/,Doctor_who1,1548788155,[removed],0,1,False,self,,,,,
1660,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,4,al3g5b,self.MachineLearning,"Newbie here, need advice",https://www.reddit.com/r/MachineLearning/comments/al3g5b/newbie_here_need_advice/,AdiSunder27,1548788949,[removed],0,1,False,self,,,,,
1661,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,4,al3kfh,self.deeplearning,Black Swans Journal Club,https://www.reddit.com/r/MachineLearning/comments/al3kfh/black_swans_journal_club/,jdyr1729,1548789598,,0,1,False,https://b.thumbs.redditmedia.com/4RlSD4mDxovrZ277Y4H5ibVUZTA9APvsFtCwMWgh2JA.jpg,,,,,
1662,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,4,al3shg,ai.googleblog.com,Transformer-XL: Unleashing the Potential of Attention Models,https://www.reddit.com/r/MachineLearning/comments/al3shg/transformerxl_unleashing_the_potential_of/,sjoerdapp,1548790811,,0,1,False,https://b.thumbs.redditmedia.com/6vQx7JPvwRfp0Fr8uD8Xx0_qmnixhzh6qOIEozv85WQ.jpg,,,,,
1663,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,4,al3u4x,arxiv.org,[1901.08974] Rescaling and other forms of unsupervised preprocessing introduce bias into cross-validation,https://www.reddit.com/r/MachineLearning/comments/al3u4x/190108974_rescaling_and_other_forms_of/,Amit_Moscovich,1548791055,,45,1,False,default,,,,,
1664,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,5,al4c5t,self.MachineLearning,[D] Lastest machine learning methods applications to self-replicating machines (Von Neumann probe),https://www.reddit.com/r/MachineLearning/comments/al4c5t/d_lastest_machine_learning_methods_applications/,Naeph,1548793799,"I wonder why none of the recent methods in robotics and machine learning (deep learning, reinforcement learning, deep reinforcement learning) have been applied in any way to Von Neumann's problem of building self-replicating machines, which, if solved, could generate enormous advances in space industry: asteroid mining, self-replicating infrasctructures and spaceships, and so on.

Thank you very much by advance for your consideration regarding my questions.",2,1,False,self,,,,,
1665,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,5,al4e8k,self.MachineLearning,"Machine Learning, '04, Cornell Univ",https://www.reddit.com/r/MachineLearning/comments/al4e8k/machine_learning_04_cornell_univ/,ithacasnowman,1548794112,[removed],0,1,False,self,,,,,
1666,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,6,al4sms,blog.floydhub.com,[N] On Building an Instagram Street Art Dataset and Detection Model,https://www.reddit.com/r/MachineLearning/comments/al4sms/n_on_building_an_instagram_street_art_dataset_and/,pirate7777777,1548796277,,0,1,False,https://b.thumbs.redditmedia.com/FdgwqfVYt3OxbRXAFJ8KrNMmtfQfyQwgRo-pgBBA7sA.jpg,,,,,
1667,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,6,al4xr7,self.MachineLearning,"[D] ML researchers, what are some of your favorite resources that have helped you succeed in your work?",https://www.reddit.com/r/MachineLearning/comments/al4xr7/d_ml_researchers_what_are_some_of_your_favorite/,mitare,1548797082,"Research is hard. What are some of your favorite websites, tools, people, resources, etc. that have helped you to be more successful and productive in your research? These could be tools for finding relevant papers, workshopping ideas, programming more effectively, mental health, identifying high-impact research questions, or whatever you'd like. What tricks of the trade would you give to a new student or coworker?",49,1,False,self,,,,,
1668,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,6,al59qw,self.MachineLearning,"Where can I publish ideas, if my supervisor doesn't want me to work on these topics?",https://www.reddit.com/r/MachineLearning/comments/al59qw/where_can_i_publish_ideas_if_my_supervisor_doesnt/,mlstu,1548798927,"I am a student assistant at a machine learning department. We work in a field where I have ideas that don't fit the line of research of our department. I find it fair that I cannot freely choose what to work on, but I would still like to do it in my free time. I thought about publishing them on a private blog, but I don't think it would get attention there. Has someone been in a similar situation?

&amp;#x200B;",0,1,False,self,,,,,
1669,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,8,al60on,soundcloud.com,A Collection Of Dance Tracks Made With ML Integration,https://www.reddit.com/r/MachineLearning/comments/al60on/a_collection_of_dance_tracks_made_with_ml/,deadseankennedy,1548803123,,1,1,False,default,,,,,
1670,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,9,al70rx,self.MachineLearning,[D] 3D convolution/ RNN over video frames captured at random time steps.,https://www.reddit.com/r/MachineLearning/comments/al70rx/d_3d_convolution_rnn_over_video_frames_captured/,WillingCucumber,1548809330,"Hi all,

I have video frames captured at random time steps. Since the frames are captured at ordered but random time steps, I cannot apply 3D convolution directly over them. Reason being unequal distance in terms of time values b/w different frames.

Does anyone know if any work exists on applying 3D convolution or some other deep learning technique over such data. This problem seems close to the problem of unstructured data, but I am not able to come up with a solution.

&amp;#x200B;

Looking forward to some interesting methods.

Thanks !!",7,1,False,self,,,,,
1671,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,10,al7il6,self.MachineLearning,I'm doing a project that involves using a Gaussian Mixture Model,https://www.reddit.com/r/MachineLearning/comments/al7il6/im_doing_a_project_that_involves_using_a_gaussian/,drax-tic,1548812560,[removed],0,1,False,self,,,,,
1672,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,10,al7iot,self.MachineLearning,[D] Salary discussion,https://www.reddit.com/r/MachineLearning/comments/al7iot/d_salary_discussion/,sneakysneakynoleaky,1548812579,"Shall we have a salary open forum?

&amp;#x200B;

The more we know, the better we can collectively bargain.

&amp;#x200B;

Also I keep getting ridiculous lowball offers and I want to complain   


&amp;#x200B;

&amp;#x200B;

Experience:  lotsa research w reputable groups and significant software eng/real world ml experience 

Location: High cost of living, US 

Offers:   
  \- startup 1 opened with 90;'90 is interesting but not quite where we need to be...'; immediately raised to 110  + 20 sign on.  
  \-  startup 2 opened with 90. share disappointment and invite them to try again. they hold firm and say they've scoped the market. funniest part, ad is still up and the range on the ad is well above 90",37,1,False,self,,,,,
1673,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,11,al7xoz,[1] https,[R] Multigrid Backprojection Super-Resolution and Deep Filter Visualization,https://www.reddit.com/r/MachineLearning/comments/al7xoz/r_multigrid_backprojection_superresolution_and/,pnavarre,1548815325,,0,1,False,default,,,,,
1674,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,12,al8c4j,self.MachineLearning,Comparing sets - Finding the most similar set,https://www.reddit.com/r/MachineLearning/comments/al8c4j/comparing_sets_finding_the_most_similar_set/,PyroIAm,1548817964,[removed],0,1,False,self,,,,,
1675,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,12,al8cyn,self.MachineLearning,[R] MGBP : Multi-Grid Back-Projection super-resolution,https://www.reddit.com/r/MachineLearning/comments/al8cyn/r_mgbp_multigrid_backprojection_superresolution/,pnavarre,1548818120,"&amp;#x200B;

https://i.redd.it/i2g3akxx4hd21.png

Paper at AAAI 2019: [https://arxiv.org/abs/1809.09326](https://arxiv.org/abs/1809.09326)

Paper at ECCV PIRM 2018: [https://arxiv.org/abs/1809.10711](https://arxiv.org/abs/1809.10711) (2nd best perceptual quality)

Github: [https://github.com/pnavarre/pirm-sr-2018](https://github.com/pnavarre/pirm-sr-2018)

&amp;#x200B;

Abstract (AAAI-2019):

We introduce a novel deep-learning architecture for image upscaling by large factors (e.g. 4x, 8x) based on examples of pristine high-resolution images. Our target is to reconstruct high-resolution images from their downscale versions. The proposed system performs a multi-level progressive upscaling, starting from small factors (2x) and updating for higher factors (4x and 8x). The system is recursive as it repeats the same procedure at each level. It is also residual since we use the network to update the outputs of a classic upscaler. The network residuals are improved by Iterative Back-Projections (IBP) computed in the features of a convolutional network. To work in multiple levels we extend the standard back-projection algorithm using a recursion analogous to Multi-Grid algorithms commonly used as solvers of large systems of linear equations. We finally show how the network can be interpreted as a standard upsampling-and-filter upscaler with a space-variant filter that adapts to the geometry. This approach allows us to visualize how the network learns to upscale. Finally, our system reaches state of the art quality for models with relatively few number of parameters.

&amp;#x200B;

Abstract (ECCV-PIRM 2018):

We describe our solution for the PIRM Super-Resolution Challenge 2018 where we achieved the 2nd best perceptual quality for average RMSE&lt;=16, 5th best for RMSE&lt;=12.5, and 7th best for RMSE&lt;=11.5. We modify a recently proposed Multi-Grid Back-Projection (MGBP) architecture to work as a generative system with an input parameter that can control the amount of artificial details in the output. We propose a discriminator for adversarial training with the following novel properties: it is multi-scale that resembles a progressive-GAN; it is recursive that balances the architecture of the generator; and it includes a new layer to capture significant statistics of natural images. Finally, we propose a training strategy that avoids conflicts between reconstruction and perceptual losses. Our configuration uses only 281k parameters and upscales each image of the competition in 0.2s in average.

&amp;#x200B;

Also see:

PIRM-SR 2018 Report: [https://arxiv.org/pdf/1809.07517.pdf](https://arxiv.org/pdf/1809.07517.pdf)

PIRM Dataset: [https://pirm.github.io/](https://pirm.github.io/)",2,1,False,self,,,,,
1676,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,12,al8enr,self.MachineLearning,Pedestrian detection on SBC,https://www.reddit.com/r/MachineLearning/comments/al8enr/pedestrian_detection_on_sbc/,mack_30,1548818424,[removed],0,1,False,self,,,,,
1677,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,12,al8fxt,self.MachineLearning,[D] Comparing Sets - Finding the closest set,https://www.reddit.com/r/MachineLearning/comments/al8fxt/d_comparing_sets_finding_the_closest_set/,PyroIAm,1548818668,"*Note: Re-posted with an updated title to adhere to this sub's posting rules.

Hi ML people,

This is my first time posting here and, to be honest, I'm not sure if this is exactly a machine learning problem at all, but I couldn't think of any other subs to ask this question in. I know it may be out of place, so if you know of any better places for me to ask about this sort of thing, I'd really appreciate it if you could point me in the right direction.

I'm trying to build a very simple question-answer system where a user can enter a question and, based on the words or keywords in their question, return a result from a set consisting of keywords. Ideally, the returned set of keywords would be the most accurate and representative of all possible stored sets that was searched against.

Currently I'm doing it rather naively and essentially giving each set a ""score"", where a point is assigned for each word that is shared between both the query and the candidate results. 

An example of the current system for you (very basic, would be larger in production):

Query: ""best new phone of ces 2019""

Candidate results:

* top, **best**, **phone**, **ces**, **2019** (score: 4)
* **best**, student, **phone** (score: 2)
* **new**, dell, laptop, **2019**, gaming (score: 2)

Now this works well for somewhat explicit searches like I have outlined above (the first candidate result would be returned as it has the highest score), but when either the searches are more vague or the candidate results overlap heavily, there exists the case where either two candidate results can tie (i.e. same scores), or where the wrong result is returned. I can't remember the exact scenario that I had where it returned the wrong results.

Another method I tried for the score was assigning one point for each matching keyword and subtracting one for each keyword that wasn't shared, but that ended up punishing larger candidate sets.

Anyways, this is the question I have for you. Ideally without involving some heavy algorithms or training sets, what other methods could I use to try to predict the most accurate set? Again, I feel this isn't exactly the largest or most intense problem to solve, but I've been thinking it over for a few days and I'd love some feedback or insight, if possible.

Many thanks in advance!
",13,1,False,self,,,,,
1678,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,12,al8pz8,imgur.com,[P] A tfidf based Twitter Sentiment Analyzer,https://www.reddit.com/r/MachineLearning/comments/al8pz8/p_a_tfidf_based_twitter_sentiment_analyzer/,R0rshrk,1548820600,,1,1,False,default,,,,,
1679,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,13,al8svl,self.MachineLearning,[D] Motion Amplification,https://www.reddit.com/r/MachineLearning/comments/al8svl/d_motion_amplification/,Kenny_smash,1548821146,"Recently I became very interested in motion amplification techniques. I know that this is made by video processing and there is a lot of papers on it, but there is a machine learning technique that could reproduce this results?

&amp;#x200B;

[https://www.youtube.com/watch?v=n0rjfeleEgE](https://www.youtube.com/watch?v=n0rjfeleEgE)",3,1,False,self,,,,,
1680,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,14,al9966,i.redd.it,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/al9966/global_machine_learning_market_size_outlook/,vardhan1020,1548824496,,0,1,False,https://a.thumbs.redditmedia.com/69MuYyxoabyhIbMo4H67dtxiGCMD75bVFwid4i6Zja4.jpg,,,,,
1681,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,14,al9cos,envisioninteligence.com,"Global Machine Learning Market  Size, Outlook, Trends and Forecasts (2019  2025)",https://www.reddit.com/r/MachineLearning/comments/al9cos/global_machine_learning_market_size_outlook/,vardhan1020,1548825183,,0,1,False,default,,,,,
1682,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,15,al9r4c,self.MachineLearning,Logo Detection Using Deep Networks,https://www.reddit.com/r/MachineLearning/comments/al9r4c/logo_detection_using_deep_networks/,mittal_neha,1548828326,[removed],0,1,False,self,,,,,
1683,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,15,al9uam,self.MachineLearning,PyTorch tutorial on google colaboratory,https://www.reddit.com/r/MachineLearning/comments/al9uam/pytorch_tutorial_on_google_colaboratory/,param87,1548829033,[removed],0,1,False,self,,,,,
1684,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,15,al9ylp,self.MachineLearning,Machine Learning Bangalore | Machine Learning Course Bangalore,https://www.reddit.com/r/MachineLearning/comments/al9ylp/machine_learning_bangalore_machine_learning/,vardana12321,1548830034,[removed],0,1,False,self,,,,,
1685,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,15,ala13w,self.MachineLearning,"Using MATLAB i have created NEURAL NETWORK ,im using MATLABs inbuilt 'fminunc' for optimization of cost function.But cost function is not reducing with iteration .Errors says 'Optimization stopped because the norm of the current step is less than options.TolX = 1.000000e-06.'' Any idea anyone?",https://www.reddit.com/r/MachineLearning/comments/ala13w/using_matlab_i_have_created_neural_network_im/,JINU8,1548830619,[removed],0,1,False,self,,,,,
1686,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,15,ala190,self.MachineLearning,What it would take to become the next Bill Gates/Mark Zuckerberg,https://www.reddit.com/r/MachineLearning/comments/ala190/what_it_would_take_to_become_the_next_bill/,ultimaterr,1548830654,[removed],0,1,False,self,,,,,
1687,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,16,alac34,goo.gl,trying to create a data set to predict age given screen time,https://www.reddit.com/r/MachineLearning/comments/alac34/trying_to_create_a_data_set_to_predict_age_given/,Cashewcandlestick,1548833276,,0,1,False,default,,,,,
1688,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,17,alalvb,self.MachineLearning,[P] Looking for datasets of therapist conversations...,https://www.reddit.com/r/MachineLearning/comments/alalvb/p_looking_for_datasets_of_therapist_conversations/,TimeLordTim,1548835873,"I'm working on a project to create an online therapist chatbot, and I wanted to use some fun Machine Learning to make the conversation more fluid. However, I'm having trouble finding a training set for my model to learn on... I've tried looking on some data sites(kaggle, UCI Repository, skymind, [deeplearning.net](https://deeplearning.net), etc), but I haven't found anything...

I know most of this would be locked behind doctor/patient confidentiality, but I was hoping I might find some generic conversations. I am willing to do some data scraping as well, but I don't know where I might something to scrape.

&amp;#x200B;

Any suggestions for finding data, either in a set or scrapeable? Or should I just buckle down and build one myself?",30,1,False,self,,,,,
1689,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,17,alan67,self.MachineLearning,Need help to find use cases of my ML project.[Human path tracing using CCTV camera],https://www.reddit.com/r/MachineLearning/comments/alan67/need_help_to_find_use_cases_of_my_ml_projecthuman/,psuzn,1548836226,[removed],0,1,False,self,,,,,
1690,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,17,alarda,self.MachineLearning,Searching for some interesting research paper,https://www.reddit.com/r/MachineLearning/comments/alarda/searching_for_some_interesting_research_paper/,Kurogami_neko,1548837449,[removed],0,1,False,self,,,,,
1691,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,17,alatqi,self.MachineLearning,[1901.10314]Trust Region-Guided Proximal Policy Optimization,https://www.reddit.com/r/MachineLearning/comments/alatqi/190110314trust_regionguided_proximal_policy/,RebornHugo,1548838145,[removed],0,1,False,self,,,,,
1692,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,17,alauqt,self.MachineLearning,Share some interething research paper on ML and AI to keep your outdated friends to be up to date :),https://www.reddit.com/r/MachineLearning/comments/alauqt/share_some_interething_research_paper_on_ml_and/,Kurogami_neko,1548838450,[removed],0,1,False,self,,,,,
1693,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,17,alavtf,arxiv.org,Trust Region-Guided Proximal Policy Optimization,https://www.reddit.com/r/MachineLearning/comments/alavtf/trust_regionguided_proximal_policy_optimization/,RebornHugo,1548838771,,1,1,False,default,,,,,
1694,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,18,alb2jj,self.MachineLearning,ELI5: What are the basics of building something like DisplayPurposes?,https://www.reddit.com/r/MachineLearning/comments/alb2jj/eli5_what_are_the_basics_of_building_something/,fanopolis,1548840698,[removed],0,1,False,self,,,,,
1695,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,18,alb44d,self.MachineLearning,Leverage The Hidden Potent Of Machine Learning For Mobile Apps,https://www.reddit.com/r/MachineLearning/comments/alb44d/leverage_the_hidden_potent_of_machine_learning/,shelinawilliam,1548841138,[removed],0,1,False,self,,,,,
1696,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,18,alb4w4,self.MachineLearning,Who can answer the challenging question on selenium webdriver automation testing and machine learning ?,https://www.reddit.com/r/MachineLearning/comments/alb4w4/who_can_answer_the_challenging_question_on/,Blogercup,1548841364,[removed],0,1,False,self,,,,,
1697,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,19,albd3e,youtu.be,[D] Jennifer Lawrence with Steve Buscemi face,https://www.reddit.com/r/MachineLearning/comments/albd3e/d_jennifer_lawrence_with_steve_buscemi_face/,cmillionaire9,1548843589,,0,1,False,default,,,,,
1698,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,19,albfse,self.MachineLearning,[D] Jennifer Lawrence with Steve Buscemi face,https://www.reddit.com/r/MachineLearning/comments/albfse/d_jennifer_lawrence_with_steve_buscemi_face/,cmillionaire9,1548844340,[https://youtu.be/64PAnrsH4vk](https://youtu.be/64PAnrsH4vk),0,1,False,self,,,,,
1699,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,19,albgnx,self.MachineLearning,[P] Jennifer Lawrence with Steve Buscemi face,https://www.reddit.com/r/MachineLearning/comments/albgnx/p_jennifer_lawrence_with_steve_buscemi_face/,PastTechnician4,1548844576,[removed],0,1,False,self,,,,,
1700,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,19,albi7l,self.MachineLearning,[Project] Jennifer Lawrence with Steve Buscemi face,https://www.reddit.com/r/MachineLearning/comments/albi7l/project_jennifer_lawrence_with_steve_buscemi_face/,cmillionaire9,1548845006," The guy combined the faces of Jennifer Lawrence and Steve Buscemi!   
youtu.be/64PAnrsH4vk",1,1,False,self,,,,,
1701,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,20,albpv8,houseofbots.com,Get easy steps to master machine learning with the advanced programming language: Python,https://www.reddit.com/r/MachineLearning/comments/albpv8/get_easy_steps_to_master_machine_learning_with/,sunps,1548847051,,0,1,False,default,,,,,
1702,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,20,albrsg,self.MachineLearning,Snake Game with Genetic Algorithm,https://www.reddit.com/r/MachineLearning/comments/albrsg/snake_game_with_genetic_algorithm/,atulkrishna50,1548847571,[removed],0,1,False,self,,,,,
1703,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,20,albye6,self.MachineLearning,[Discussion] What is the right course of action when someone does not cite you?,https://www.reddit.com/r/MachineLearning/comments/albye6/discussion_what_is_the_right_course_of_action/,astonished_crofty,1548849227,"I am a junior researcher in the field. I came across a (NeurIPS accepted) paper on arXiv today that is proposing ideas very similar to what I did in one of my papers. They do not cite my work. What is the right course of action here? 

Options:

1. Trust author's literature awareness and integrity: understand that you don't deserve citation. trust their authority. if they did not cite, that means your work had no influence on their research. don't be ""petty"".
2. Ignorance: for some reason, they did not cite. just ignore and move on.
3. Schmidhuber: ask for a citation and do not budge

As I am a junior researcher, I do not know if such citations will add up to something. But at the same time, I am guilty of taking researchers more legitimately if their scholar metrics are good. Maybe everyone does this and taking all opportunities to boost them is a good idea. Maybe such bargains do not offer big improvements and I should just focus on research and the world will come to me if and when I make a major contribution. I am not sure.

What is your take on this issue? I would really appreciate some responses from senior people in the field.",81,1,False,self,,,,,
1704,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,21,alc4uw,self.tensorflow,Tensorflow prediction,https://www.reddit.com/r/MachineLearning/comments/alc4uw/tensorflow_prediction/,zinngg,1548850682,,0,1,False,default,,,,,
1705,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,21,alc53l,self.MachineLearning,Methods for finding optimum input parameters,https://www.reddit.com/r/MachineLearning/comments/alc53l/methods_for_finding_optimum_input_parameters/,ChiroNika,1548850738,"Hi everyone,

I just started working with DeepLearning / Machine Learning and so far I can train a model and use it to make predictions. So please excuse my amateurish question:

What methods can I use to find the optimal set of input parameters? 

Stupid simplified example: I have a range of products consisting of different materials with different properites. I want to predict the earnings a product makes. I train a model and now I can predict earnings of a new product depending on its materials and properties.

But how can I find out the ""perfect"" set of input parameters (materials and properties) in order to create a perfect product and find the optimum of earnings of the trained model?

  
Thanks in advance and again, please excuse my laymans terms.  
",0,1,False,self,,,,,
1706,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,22,alcgqj,i.redd.it,"Looking for #Compute power, #Database storage, #Content delivery, or other functionality? #AWS has the services to help you build sophisticated #Applications with increased flexibility, #Scalability and reliability",https://www.reddit.com/r/MachineLearning/comments/alcgqj/looking_for_compute_power_database_storage/,AWSTrainingInHouston,1548853343,,0,1,False,https://b.thumbs.redditmedia.com/xuIKmYXFhjcDBb2OU-AIll9Q1JQDw6ZEiG3lbIzGnus.jpg,,,,,
1707,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,22,alcstg,pplonski.github.io,[D] Automatic Machine Learning is broken,https://www.reddit.com/r/MachineLearning/comments/alcstg/d_automatic_machine_learning_is_broken/,pp314159,1548855846,,0,1,False,https://b.thumbs.redditmedia.com/0JVi3qGHXL8RjyLGu2If42ujt7rsZyeAX9VJwtRNUuc.jpg,,,,,
1708,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,23,alcyqw,medium.com,The Evolution of AlphaStar,https://www.reddit.com/r/MachineLearning/comments/alcyqw/the_evolution_of_alphastar/,Fewthp,1548857003,,0,1,False,https://b.thumbs.redditmedia.com/66S8nPKYvwrrfuLg4PBfMN6g4QejJUnQPK8yD1VnOaY.jpg,,,,,
1709,MachineLearning,t5_2r3gv,2019-1-30,2019,1,30,23,ald7yf,self.MachineLearning,Spanish Text Classification,https://www.reddit.com/r/MachineLearning/comments/ald7yf/spanish_text_classification/,ShaikKamaal,1548858765,"Hi All, I am new to data science. I have Spanish text for classification. I am using Python language. I am using below code for Spanish text classification. But it is giving less accuracy. 

\# For pre-prosessing  below code

cls\_txt = re.sub('\[\^a-zA-Z\]', ' ', text)

cls\_txt = cls\_txt.lower()

cls\_txt = cls\_txt.split()

cls\_txt = \[word for word in cls\_txt if not word in set(stopwords.words('spanish'))\]

cls\_txt = ' '.join(cls\_txt)

stemmer = SnowballStemmer('spanish')

cls\_txt = \[stemmer.stem(word) for word in word\_tokenize(cls\_txt)\]

cls\_txt = ' '.join(cls\_txt)

&amp;#x200B;

\# For Classification below code

from sklearn.model\_selection import train\_test\_split

X\_train, X\_test, y\_train, y\_test = train\_test\_split(X, y, test\_size=0.20, random\_state=0)

from sklearn.naive\_bayes import GaussianNB

classifier = GaussianNB()

[classifier.fit](https://classifier.fit)(X\_train, y\_train)

y\_pred = classifier.predict(X\_test)

&amp;#x200B;

Please suggest me better code and approach for Spanish text classification.

&amp;#x200B;

Thanks for advance.

&amp;#x200B;",0,1,False,self,,,,,
1710,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,0,aldvy4,self.MachineLearning,What is the state of the art with regards to machine learning in object tracking for video?,https://www.reddit.com/r/MachineLearning/comments/aldvy4/what_is_the_state_of_the_art_with_regards_to/,nived90,1548862925,[removed],0,1,False,self,,,,,
1711,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,0,aldwv0,self.MachineLearning,PLDA,https://www.reddit.com/r/MachineLearning/comments/aldwv0/plda/,deveid,1548863071,[removed],0,1,False,self,,,,,
1712,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,0,ale027,self.MachineLearning,"Simple Questions Thread January 30, 2019",https://www.reddit.com/r/MachineLearning/comments/ale027/simple_questions_thread_january_30_2019/,AutoModerator,1548863570,[removed],0,1,False,self,,,,,
1713,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,1,ale678,medium.com,Tsinghua University Proves Quantum Supremacy on GANs,https://www.reddit.com/r/MachineLearning/comments/ale678/tsinghua_university_proves_quantum_supremacy_on/,Yuqing7,1548864531,,0,1,False,https://b.thumbs.redditmedia.com/ltVLZFiAfg6eP9T6GEaXjg_BhkVnoP9Ewqyon_nXt1M.jpg,,,,,
1714,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,1,alegwq,self.MachineLearning,newbie question from (Python) developer,https://www.reddit.com/r/MachineLearning/comments/alegwq/newbie_question_from_python_developer/,kittie_thrower,1548866201,[removed],0,1,False,self,,,,,
1715,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,1,alehoq,self.MachineLearning,Hyperparameter Results Analysis,https://www.reddit.com/r/MachineLearning/comments/alehoq/hyperparameter_results_analysis/,Tzeny15,1548866319,[removed],0,1,False,self,,,,,
1716,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,1,alej2v,self.MachineLearning,L2 Regularization and Batch Norm,https://www.reddit.com/r/MachineLearning/comments/alej2v/l2_regularization_and_batch_norm/,icosaplex,1548866527,"[https://blog.janestreet.com/l2-regularization-and-batch-norm/](https://blog.janestreet.com/l2-regularization-and-batch-norm/)

I recently uploaded this blog post about a major interaction between L2 and batch normalization. It's a straightforward interaction that has been already known and studied before, but I posted about it since at least in the small circles I've been in, I've found not always part of the common knowledge.

Basically, if you're using batch norm, then (with some conditions and assumptions, but not particularly strenuous ones) an L2 penalty or weight decay on model weights doesn't generally act as a regularizer preventing overfitting, rather it ensures your gradients don't decay too fast preventing underfitting.

Hope you find it interesting!",0,1,False,self,,,,,
1717,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,2,aler62,self.MachineLearning,[D] L2 Regularization and Batch Norm,https://www.reddit.com/r/MachineLearning/comments/aler62/d_l2_regularization_and_batch_norm/,icosaplex,1548867719,"[https://blog.janestreet.com/l2-regularization-and-batch-norm/](https://blog.janestreet.com/l2-regularization-and-batch-norm/)

I recently uploaded this blog post about a major interaction between L2 and batch normalization. It's a straightforward interaction that has been already known and studied before, but I posted about it since at least in the small circles I've been in, I've found it not always part of the common knowledge.

Basically, if you're using batch norm, then (with some conditions and assumptions, but not particularly strenuous ones) an L2 penalty or weight decay on model weights doesn't generally act as a regularizer preventing overfitting for layers being batch-normed, rather it ensures your gradients don't decay too fast preventing underfitting.

Hope you find it interesting!",12,1,False,self,,,,,
1718,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,2,alet7o,medium.com,"Google et al demo model cards at ACM Fairness, Accountability, and Transparency* Conference",https://www.reddit.com/r/MachineLearning/comments/alet7o/google_et_al_demo_model_cards_at_acm_fairness/,Runner1928,1548868009,,0,1,False,https://b.thumbs.redditmedia.com/2xqZk47n1OShZ5W1RIk5DZogSzXOccA18BWsfS5RCtg.jpg,,,,,
1719,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,2,alf1l0,self.MachineLearning,[P] Good Research Papers for Location inference from Tweets?,https://www.reddit.com/r/MachineLearning/comments/alf1l0/p_good_research_papers_for_location_inference/,Zealousideal_Honey,1548869263,"I am working on a project to predict Geo-Location from untagged tweets and was looking for some good and insightful papers to read to know what work has been done on it in the past.  I found some paper which I really liked but can't find any more such papers.

Can someone suggest good papers in this field or maybe point out on how should I proceed(any good conferences I can look up)?

I am an undergraduate student and it's my first research project, and I am scared of screwing things up or not keeping up to my advisor's expectations, so any help would be appreciated a lot.",12,1,False,self,,,,,
1720,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,2,alf1zv,self.MachineLearning,what is the best way to create Ros node c++ to run Tensorflow model.,https://www.reddit.com/r/MachineLearning/comments/alf1zv/what_is_the_best_way_to_create_ros_node_c_to_run/,zaher88abd,1548869326,[removed],0,1,False,self,,,,,
1721,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,2,alfa9g,self.MachineLearning,Why is medium so popular for machine learning?,https://www.reddit.com/r/MachineLearning/comments/alfa9g/why_is_medium_so_popular_for_machine_learning/,bandalorian,1548870568,[removed],0,1,False,self,,,,,
1722,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,2,alfdu5,beta.aifiddle.io,Visual Tool To Build And Train Models On The Web,https://www.reddit.com/r/MachineLearning/comments/alfdu5/visual_tool_to_build_and_train_models_on_the_web/,Zlass,1548871107,,0,1,False,https://b.thumbs.redditmedia.com/pjlrm-VW-0XA6FUu8t5I90jIyWTKqFq16OX-z-N_VhQ.jpg,,,,,
1723,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,3,alfn81,self.MachineLearning,FastAI: All lesson notebooks now available to use with free GPU(s) as Kaggle kernels,https://www.reddit.com/r/MachineLearning/comments/alfn81/fastai_all_lesson_notebooks_now_available_to_use/,init__27,1548872488,[removed],0,1,False,self,,,,,
1724,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,3,alfyy6,self.MachineLearning,Catching the Most Notorious Spy in US History and Why James Bonds Just a Hacker | Eric ONeill,https://www.reddit.com/r/MachineLearning/comments/alfyy6/catching_the_most_notorious_spy_in_us_history_and/,The_Syndicate_VC,1548874281,[removed],0,1,False,self,,,,,
1725,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,4,algjvk,self.MachineLearning,[D] How to guide research direction?,https://www.reddit.com/r/MachineLearning/comments/algjvk/d_how_to_guide_research_direction/,chebyshev3,1548877325,"I'm confused about how to guide my own research focus so that I'm using my time efficiently.

I typically follow [this guide] (https://www.eecs.harvard.edu/~michaelm/postscripts/ReadPaper.pdf) on reviewing papers. My work is applied ML. My team has not done any novel work, although we'll occasionally talk about novel solutions.

I've tried doing research in my free time, and I'm working on non-euclidean neural networks right now. I wanted to see if a complete non-euclidean network was feasible in an NLP task (which would be novel), and after a few weeks of combining other existing ideas to work on my chosen task, my work has not led to any novel insights besides sub-SOTA results. this alone doesn't seem publishable, and now I'm working on my non-euclidean math to try to understand how to move forward without guessing at changes.

I don't really know if I'm *doing* research correctly, at least not research that's publishable. I wanted to take a step back and ask the community how they go about their research in an intelligent way.",4,1,False,self,,,,,
1726,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,4,algmq0,self.MachineLearning,Where to begin to understand how random first works?,https://www.reddit.com/r/MachineLearning/comments/algmq0/where_to_begin_to_understand_how_random_first/,ehossain,1548877746,[removed],0,1,False,self,,,,,
1727,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,5,alh98d,self.MachineLearning,[Q] Fast sampling of the solution space for convolutional layer,https://www.reddit.com/r/MachineLearning/comments/alh98d/q_fast_sampling_of_the_solution_space_for/,glockenspielcello,1548881126,[removed],0,1,False,self,,,,,
1728,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,6,alhx31,self.MachineLearning,Random sampling of variable space,https://www.reddit.com/r/MachineLearning/comments/alhx31/random_sampling_of_variable_space/,ARak101,1548884775,"I think I remember reading somewhere that given a large variable space (i.e x1, x2, x3, ... xn where n is large and each variable can take a large range of values) it is more efficient to randomly sample by assigning each variable a random value and collecting the outcomes and using ML algorithms like PCA and NMF to get out the trends, rather than sequentially sweeping the variables to and getting the trends by producing huge datasets. I was wondering if firstly someone could confrim this is correct and secondly point me in the direction of a journal or textbook reference. 

&amp;#x200B;

For context, I'm trying to explore a complex synthetic pathway and want to explore the relationship between synthetic conditions and formation outcomes. Problem is that there numerous variables that need to be swept, so there variable space is too large to explore practically. 

&amp;#x200B;",0,1,False,self,,,,,
1729,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,7,ali5vf,self.MachineLearning,How do you manage and schedule your GPU resources on a multi-user system?,https://www.reddit.com/r/MachineLearning/comments/ali5vf/how_do_you_manage_and_schedule_your_gpu_resources/,ameasure,1548886127,[removed],0,1,False,self,,,,,
1730,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,7,alia5g,self.MachineLearning,"Question about AlphaStar, possible deficiency of pure neuro-networks?",https://www.reddit.com/r/MachineLearning/comments/alia5g/question_about_alphastar_possible_deficiency_of/,1096bimu,1548886796,"I'm not something for this field, I'm just curious after watching the AlphaStar games. I'm going to try to be as precise as possible but again, not an expert and I don't know much about how these things work.

So I've been thinking, using humans as an analogy, there is this clear distinction between the faculty of reason and instincts, or muscle memory. Quite often when we try to learn something what we are really doing is moving something we know in reason, into what we know in instinct. For example you can tell a kid to turn the bicycle handle toward the fall to correct it, the kid could understand it if he/she is smart, but that alone won't make him/her ride a bicycle, you have to actually go and practice riding it.

In my understanding, training a neuro-network is analogous to training muscle memory in people, but it isn't analogous to teaching new knowledge to a human. And I'm thinking that might be a big weakness of AlphaStar.

To give you another example, for a human SC2 player, it is very straightforward to tell him/her for example, the next patch nerfed Phoenix to hell, don't use it. And the human will be very successful at not using it because the human easily gets the reasoning that Phoenix is now inefficient, perhaps under all circumstances. Now can you do the same thing to AlphaStar? I'm suspecting it's not possible because AlphaStar doesn't have a reasoning part to it, it will only learn that after playing like 10 yers worth of games against itself and finding out the hard way.

Again hopefully I'm correct at this, but I think AlphaGo on the other hand, did have what I would call reasoning capability. It actually does enumerate some future states for its evaluation network to look at and see which ones results in the most amount of winning. It's not as complex as human reasoning, it's very narrow in application but very effective. In principle AlphaGo can see potential strategies of its opponent coming many moves ahead (if its selection networks don't filter them out first) and deal with them at the present.

In contrast, if I am correct in saying that AlphaStar does not have this capability, I'm guessing it has no or very limited ability to respond to long-term strategies from its opponents. I'm quite certain that's exactly what we have been seeing in the games. AlphaStar obviously has godlike micro in battles, it can move units in relation to enemy units very well. But you'll see some very strange behaviour:

It does not respect force fields at all, I mean any human can see it coming but apparently AlphaStar does not. I'm guessing its neuro-networks didn't establish a connection between chokepoints, sentinels and loosing units. Or more specifically, reward and punishment over a couple of second of delay aren't working. And it has no ability to enumerate possible enemy strategies or movements. 

AphaStar repeatedly move its army back and forth in response to Mana's drop ship exploit. Any human should be able to see what's going on but clearly AlphaStar did not. It is immediately obvious to any human that you can't catch those two units, so you can just leave a few units to defend while keeping your main army on more important tasks. I'm thinking AlphaStar didn't think of it because it doesn't think, it doesn't reason, it just reacts to the present.

AlphaStar not upgrading tech that benefits the only unit its army is comprised of. Again, I think it's just because AlphaStar doesn't actually know what techs do, it basically just upgrades tech whenever it feels like it.

I also think the fact that DeepMind had to use different agents to compete is an indication that even they knew AlphaStar really does not have any strategy, an agent just has one strategy it can execute well and with some flexibility. So instead what you need is a bunch of agents, each with a different strategy, and someone else has to decide which one takes the wheel. I mean sure the distinction between AlphaStar the project, and the agents it produced is quite arbitrary. But the ability to decide which strategy to play should really be a part of the player. So AlphaStar should really be able to pick agents to represent itself, even if that's just a simple random select. However keep in mind while this may not have changed the results of the TLO and Mana matches, such a 5-agent random select AlphaStar player would almost certainly loose to top humans in a large number of matches. Because the humans will easily figure out that there are only 5 strategies, and what to do against each of them.

Clearly, AlphaStar is missing some crucial component of what makes a top StarCraft player, and I think it's the capability to reason. While trained neuro-networks are extremely fast in reaction, and more flexible than hard-coded algorithms, it's still not as flexible as general purpose reasoning. I'm not being an AI pessimist, I am absolutely certain AI will crack StarCraft completely in near-future. I'm just saying AlphaStar clearly isn't the same kind of success as AlphaGo. I'm pretty sure top SC2 players can easily beat AlphaStar if playing in for example 5x slow motion. But clearly nobody can beat AlphaGo in any kind of slow-motion scenario.",1,1,False,self,,,,,
1731,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,8,alj4l1,gited.io,"Teachers, share your Jupyter notebooks, share your lessons",https://www.reddit.com/r/MachineLearning/comments/alj4l1/teachers_share_your_jupyter_notebooks_share_your/,babalbox,1548891560,,0,1,False,https://b.thumbs.redditmedia.com/pUVIZpBNe6hr_Fo88JS2lotRp1tT_H4zPfzdyy7R1aA.jpg,,,,,
1732,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,8,alj5rx,self.MachineLearning,[HELP] How to approach anomaly detection with correlated fields?,https://www.reddit.com/r/MachineLearning/comments/alj5rx/help_how_to_approach_anomaly_detection_with/,shseham,1548891757,[removed],0,1,False,self,,,,,
1733,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,8,alj8id,gited.io,"[P] Teachers, share your Jupyter notebooks, share your lessons",https://www.reddit.com/r/MachineLearning/comments/alj8id/p_teachers_share_your_jupyter_notebooks_share/,babalbox,1548892177,,0,1,False,https://b.thumbs.redditmedia.com/pUVIZpBNe6hr_Fo88JS2lotRp1tT_H4zPfzdyy7R1aA.jpg,,,,,
1734,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,9,aljdcz,self.MachineLearning,[D] How to calculate perplexity between unequal length sequences from LogSoftmax?,https://www.reddit.com/r/MachineLearning/comments/aljdcz/d_how_to_calculate_perplexity_between_unequal/,trias10,1548892973,"I'm using Pytorch for a classic seq2seq model, but am unsure how to correctly calculate perplexity for a single sample, when your generated decoder sentence is different length from the true sentence. If someone could please provide some guidance, I would greatly appreciate it.

For example, the correct true seq is maybe:

[&lt;SOS&gt;, the, boy, went, home, after, school, &lt;EOS&gt;]

But the decoder generates something absurd:

[&lt;SOS&gt;, hello, planet, earth, &lt;EOS&gt;]

The terminal layer is a LogSoftmax for inference/decoding.

For the decoder, I have a tensor which saves all vocab_size logSoftmax outputs for each time step, so for the above example, I would end up with a tensor which is 4 x 10000 (assuming vocab size is 10k), and dim 0 is 4 because the &lt;SOS&gt; is not generated since it's forcibly always provided first. Once an EOS token is hit, the inference loop breaks, which is why that tensor is only 4 rows long.

I have read that what you should do in this case is index into that tensor such that for each row, you return the column value corresponding to the y_true index (so the logsoftmax value the model generated for the true word, even if that wasn't the winning word), but how would that work if the two sequences are unequal length? Do you basically just add padding to force them to be equal? If so, what would be the final formula then to calculate perplexity? I know it's close to pow(2, -sum(indexed_vector) / ?) where ? is some length, but not sure if it's decoded length or the true length, or possibly some padded length?",1,1,False,self,,,,,
1735,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,9,aljqib,self.MachineLearning,[D] How to approach anomaly detection with correlated fields?,https://www.reddit.com/r/MachineLearning/comments/aljqib/d_how_to_approach_anomaly_detection_with/,shseham,1548895238,"I am new to machine learning and I wanted to try solving an anomaly detection problem. Any pointers as to how to approach this would be helpful.

The dataset has a lot of columns and two of them are - country and region. For values could be (US, Texas), (Italy, Sicily), (Sonora, Mexico). There could be anomalies in the source data where a wrong state is paired with a country (Italy, Texas). How would I approach this type anomaly detection?",1,1,False,self,,,,,
1736,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,9,aljr6p,medium.com,Activity Classification for iOS how to,https://www.reddit.com/r/MachineLearning/comments/aljr6p/activity_classification_for_ios_how_to/,heybluez,1548895355,,0,1,False,https://b.thumbs.redditmedia.com/gtmdW1D02MRt5Eziu4nUMAzLAaO-EtnR_L4uywNR-Do.jpg,,,,,
1737,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,9,aljsqe,self.MachineLearning,Deep Learning Research Review: Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/aljsqe/deep_learning_research_review_natural_language/,andrea_manero,1548895636,"http://www.datasciencecentral.com/profiles/blogs/deep-learning-research-review-week-3-natural-language-processin-1
",0,1,False,self,,,,,
1738,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,9,aljt0y,self.MachineLearning,[D] Wrong stress in some words in TTS models,https://www.reddit.com/r/MachineLearning/comments/aljt0y/d_wrong_stress_in_some_words_in_tts_models/,anagapetyan,1548895689,"Hi!

I have test some pertained TTS models and found that they have some problems with the pronunciations of names(first/last). I mean wrong stress. 

How to deal with it without adding new training data? Maybe in some TTS papers mentioned some interesting regularisations or techniques which can help me

 ",4,1,False,self,,,,,
1739,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,10,aljx9n,i.redd.it,"Nothing crazy, just little project I did to help visualize how a simple MLP network classified clusters of data over time.",https://www.reddit.com/r/MachineLearning/comments/aljx9n/nothing_crazy_just_little_project_i_did_to_help/,TheTitanOfWar,1548896469,,1,1,False,default,,,,,
1740,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,10,alk7ye,youtu.be,Energy Saving Electro-hydraulic Servo CNC Press Brake Test Running Youtube Video,https://www.reddit.com/r/MachineLearning/comments/alk7ye/energy_saving_electrohydraulic_servo_cnc_press/,CNCPressBrakeChina,1548898389,,0,1,False,https://a.thumbs.redditmedia.com/2xPNNqycQmBaU0ZKVCEWQkQ6HZRqVrNRlpRhCsFRd10.jpg,,,,,
1741,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,11,alkk2v,arxiv.org,Separable Effects for Causal Inference in the Presence of Competing Risks,https://www.reddit.com/r/MachineLearning/comments/alkk2v/separable_effects_for_causal_inference_in_the/,AlfonzoKaizerKok,1548900619,,1,1,False,default,,,,,
1742,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,11,alkwzo,self.MachineLearning,Can you help me use decision tree correctly in weka?,https://www.reddit.com/r/MachineLearning/comments/alkwzo/can_you_help_me_use_decision_tree_correctly_in/,croxcrocodile,1548903047,[removed],0,1,False,self,,,,,
1743,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,11,alkx4w,self.MachineLearning,What Machine Learning Software do you recommend?,https://www.reddit.com/r/MachineLearning/comments/alkx4w/what_machine_learning_software_do_you_recommend/,Frank1789,1548903073,[removed],0,1,False,self,,,,,
1744,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,13,allng0,arxiv.org,[R] Go-Explore: a New Approach for Hard-Exploration Problems,https://www.reddit.com/r/MachineLearning/comments/allng0/r_goexplore_a_new_approach_for_hardexploration/,baylearn,1548908212,,25,1,False,default,,,,,
1745,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,13,allpbj,self.MachineLearning,Have any of you guys/gals tried Comet.ml?,https://www.reddit.com/r/MachineLearning/comments/allpbj/have_any_of_you_guysgals_tried_cometml/,ky0ung25,1548908593,[removed],0,1,False,self,,,,,
1746,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,14,alm0vs,computerworld.com.au,Coates Hire pilots computer vision safety spotter (nice use case),https://www.reddit.com/r/MachineLearning/comments/alm0vs/coates_hire_pilots_computer_vision_safety_spotter/,GtothePtotheN,1548910931,,0,1,False,default,,,,,
1747,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,15,almwfg,self.MachineLearning,Bellman Equation and dynamic programming article,https://www.reddit.com/r/MachineLearning/comments/almwfg/bellman_equation_and_dynamic_programming_article/,sanchit2843,1548917965,[removed],0,1,False,self,,,,,
1748,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,16,almxy6,youtube.com,R vs Python - What should I learn in 2019? | R and Python Comparison | I...,https://www.reddit.com/r/MachineLearning/comments/almxy6/r_vs_python_what_should_i_learn_in_2019_r_and/,Debashis__,1548918323,,0,1,False,default,,,,,
1749,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,16,aln08r,medium.com,The 50 Best Public Datasets for Machine Learning  Data Driven Investor  Medium,https://www.reddit.com/r/MachineLearning/comments/aln08r/the_50_best_public_datasets_for_machine_learning/,lady_monsoon,1548918879,,0,1,False,default,,,,,
1750,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,16,aln3a1,self.MachineLearning,PyTorch or Keras/TensorFlow for tool in Jupyter lab,https://www.reddit.com/r/MachineLearning/comments/aln3a1/pytorch_or_kerastensorflow_for_tool_in_jupyter_lab/,baidodo,1548919679,[removed],0,1,False,self,,,,,
1751,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,16,aln3vg,fast.ai,Five Things That Scare Me About AI,https://www.reddit.com/r/MachineLearning/comments/aln3vg/five_things_that_scare_me_about_ai/,benevolent001,1548919839,,0,1,False,default,,,,,
1752,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,17,alnftq,self.MachineLearning,Logit expression,https://www.reddit.com/r/MachineLearning/comments/alnftq/logit_expression/,eriksonis6,1548923002,[removed],0,1,False,self,,,,,
1753,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,17,alni1p,self.MachineLearning,[D] How does an inception module generate sparse matrices?,https://www.reddit.com/r/MachineLearning/comments/alni1p/d_how_does_an_inception_module_generate_sparse/,Eoncarry,1548923643,"I've been reading the paper and I understand that having sparse representations is important for computational efficiency. But how does having an inception module help in producing such sparse representations? How is having convolution kernels of different sizes (as in inception), stronger/ better in producing sparsity, than compared to having kernels of all same size?",9,1,False,self,,,,,
1754,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,17,alnk5y,intelegain.com,30+ Powerful Machine Learning - AI Examples,https://www.reddit.com/r/MachineLearning/comments/alnk5y/30_powerful_machine_learning_ai_examples/,intelegain_inc,1548924252,,0,1,False,default,,,,,
1755,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,19,alo7b9,trainings.internshala.com,Internships and Interested ML Courses . I love this. Have a look !! ,https://www.reddit.com/r/MachineLearning/comments/alo7b9/internships_and_interested_ml_courses_i_love_this/,Ranvijay02,1548930744,,0,1,False,default,,,,,
1756,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,19,alo7hv,youtube.com,Wanna Kicks - Explore new sneakers in AR!,https://www.reddit.com/r/MachineLearning/comments/alo7hv/wanna_kicks_explore_new_sneakers_in_ar/,darya_sesitskaya,1548930792,,0,1,False,https://b.thumbs.redditmedia.com/xJodQqTgQ1FBZEdIYyf2-Yy4fkJM17pKNspg0MS_Mnk.jpg,,,,,
1757,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,19,alo9z9,rod-keras-multi-label.blogbranches.godatadriven.com,Keras: multi-label classification with ImageDataGenerator,https://www.reddit.com/r/MachineLearning/comments/alo9z9/keras_multilabel_classification_with/,rragundez,1548931446,,0,1,False,default,,,,,
1758,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,19,alobbt,blog.godatadriven.com,[P] Keras: multi-label classification with ImageDataGenerator,https://www.reddit.com/r/MachineLearning/comments/alobbt/p_keras_multilabel_classification_with/,rragundez,1548931796,,0,1,False,https://b.thumbs.redditmedia.com/5GsQ26J_RPh8RMBWHOWZEMy7h5_tylC-X4iz8qWp-Dk.jpg,,,,,
1759,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,19,alod54,lhd.co.com,LHD S.p.A. | Telescopic Forks | Stacker Cranes | Automated Warehouse AS/RS,https://www.reddit.com/r/MachineLearning/comments/alod54/lhd_spa_telescopic_forks_stacker_cranes_automated/,lhd121,1548932282,,0,1,False,default,,,,,
1760,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,20,aloere,blog.ntrlab.com,US Roadshow: NTR @ the 2019 API Inspection and Mechanical Integrity Summit,https://www.reddit.com/r/MachineLearning/comments/aloere/us_roadshow_ntr_the_2019_api_inspection_and/,Batareika_1,1548932675,,0,1,False,https://b.thumbs.redditmedia.com/DISEMa2TJ2k3To9C7N5PIZH_-a9Of51rG0t38rQm4bM.jpg,,,,,
1761,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,20,alogh8,medium.com,Open Project - Author Name Disambiguation using Self-citation,https://www.reddit.com/r/MachineLearning/comments/alogh8/open_project_author_name_disambiguation_using/,yo__on,1548933128,,0,1,False,https://b.thumbs.redditmedia.com/Qht_K_4He7Kra_7YEOrG-4YKo9gPaa_9Skv72eEAAbI.jpg,,,,,
1762,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,21,alottk,self.MachineLearning,Can anyone recommend a good trouble shooting guide/lesson?,https://www.reddit.com/r/MachineLearning/comments/alottk/can_anyone_recommend_a_good_trouble_shooting/,niazipan,1548936484,[removed],0,1,False,self,,,,,
1763,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,21,alouwi,arxiv.org,[R] Constant Time Graph Neural Networks,https://www.reddit.com/r/MachineLearning/comments/alouwi/r_constant_time_graph_neural_networks/,joisino,1548936743,,20,1,False,default,,,,,
1764,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,21,alp1d1,self.MachineLearning,Reinforcement Learning (RL) Tutorial with Sample Python Codes,https://www.reddit.com/r/MachineLearning/comments/alp1d1/reinforcement_learning_rl_tutorial_with_sample/,obsezer,1548938249,[removed],0,1,False,self,,,,,
1765,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,21,alp5sg,self.MachineLearning,Persisted machine learning pipeline framework?,https://www.reddit.com/r/MachineLearning/comments/alp5sg/persisted_machine_learning_pipeline_framework/,kalabele,1548939220,"Hi,

My current workflow looks like this: Have an idea how to improve a model. Copy an existing Jupyter notebook. Make some adjustments. Run it. Wait. A. Really. Long. Time.

Part of the problem is that I prepare the data and run the hyperparameter optimization in the same notebook. So even if the data hasn't changed, it still has to prepare all data again. Also, in the hyperparameter optimization, I may only have changed how the models are scored. However, it still has to train all models again.

I think that this duplicated work could be mostly avoided if I used a framework which knows exactly what has been done previously. For each step, it would have to know the dependencies and would have to persist the results. When some work has to be done again in the future, it would simply load these intermediate results.

This would be especially useful if the framework integrated with the libraries I use. For example, if it supports my hyperparameter optimization library, it could whenever I change the score function, first re-evaluate the previously best results before doing whatever it usually does. Same is true when some features change or some new hyperparameters are added (anywhere in the pipeline). Instead of running the hyperparameter optimization from scratch, it could just try first what has worked best before. 

Is there already such a pipeline framework for Python?",0,1,False,self,,,,,
1766,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,21,alp6c1,self.MachineLearning,Can deep Reinforcement Learning benefit from feature extraction techniques?,https://www.reddit.com/r/MachineLearning/comments/alp6c1/can_deep_reinforcement_learning_benefit_from/,qudcjf7928,1548939348,[removed],0,1,False,self,,,,,
1767,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,22,alpa12,self.MachineLearning,Question: What should I start with ?,https://www.reddit.com/r/MachineLearning/comments/alpa12/question_what_should_i_start_with/,yourcreativehorses,1548940134,[removed],0,1,False,self,,,,,
1768,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,22,alpa36,youtube.com,Drone Weaponry a growing threat?,https://www.reddit.com/r/MachineLearning/comments/alpa36/drone_weaponry_a_growing_threat/,wtf1001,1548940149,,0,1,False,https://b.thumbs.redditmedia.com/t3r0AnxhGTIO1q6n0IIz9U-LxOFnHUB65mxaHozzeTQ.jpg,,,,,
1769,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,22,alpm2a,arxiv.org,[1901.10861] A Simple Explanation for the Existence of Adversarial Examples with Small Hamming Distance,https://www.reddit.com/r/MachineLearning/comments/alpm2a/190110861_a_simple_explanation_for_the_existence/,ihaphleas,1548942570,,11,1,False,default,,,,,
1770,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,22,alpnqt,arxiv.org,[1901.10951] Distant Vehicle Detection Using Radar and Vision,https://www.reddit.com/r/MachineLearning/comments/alpnqt/190110951_distant_vehicle_detection_using_radar/,ihaphleas,1548942897,,1,1,False,default,,,,,
1771,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,23,alppkp,arxiv.org,[1901.10513] Adversarial Examples Are a Natural Consequence of Test Error in Noise,https://www.reddit.com/r/MachineLearning/comments/alppkp/190110513_adversarial_examples_are_a_natural/,ihaphleas,1548943255,,12,1,False,default,,,,,
1772,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,23,alpql6,self.MachineLearning,DATASET: Indian political news articles,https://www.reddit.com/r/MachineLearning/comments/alpql6/dataset_indian_political_news_articles/,xen-m-rph,1548943430,[removed],0,1,False,self,,,,,
1773,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,23,alps8n,arxiv.org,[1705.02498] A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions,https://www.reddit.com/r/MachineLearning/comments/alps8n/170502498_a_study_and_comparison_of_human_and/,ihaphleas,1548943722,,2,1,False,default,,,,,
1774,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,23,alpvdm,self.MachineLearning,[P] Activity Classification for watchOS: Part 1  Skafos.ai  Medium,https://www.reddit.com/r/MachineLearning/comments/alpvdm/p_activity_classification_for_watchos_part_1/,Davlucmac,1548944304,"I want to know how to collect data from an Apple Watch to develop an ML-driven app. This article series is just what I need - thought the applied ML folks might like it!

[https://medium.com/metis-machine/activity-classification-for-watchos-part-1-542d44388c40](https://medium.com/metis-machine/activity-classification-for-watchos-part-1-542d44388c40)

*Note: Digging into Apple's Turicreate framework, I was surprised the activity classification example model included data from a competing smartphone platform. This article is aimed at collecting from an apple watch, in order to develop an app for the same platform.*",10,1,False,self,,,,,
1775,MachineLearning,t5_2r3gv,2019-1-31,2019,1,31,23,alq64z,journals.plos.org,Machine learning will improve cancer immunotherapy success.,https://www.reddit.com/r/MachineLearning/comments/alq64z/machine_learning_will_improve_cancer/,MistWeaver80,1548946251,,0,1,False,default,,,,,
