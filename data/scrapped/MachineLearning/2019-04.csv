,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,9,b7v0iy,self.MachineLearning,[R] Machine Intelligence Conference 2019,https://www.reddit.com/r/MachineLearning/comments/b7v0iy/r_machine_intelligence_conference_2019/,MICInc,1554078861,"The thesis of our conference originally stemmed from the idea of demonstrating the value that could be achieved when students who are passionate about machine intelligence unite as a larger, more inclusive, and cognitively diverse community. Our conference serves as an even greater surface area for intellectual engagement for students outside of the classroom and beyond the boundaries of individual institutions. It is our mission and hope, that the Machine Intelligence Conference will empower students to take agency over the trajectory of their education, promote diversity that will fundamentally change the paradigm of our field, and catalyze connections to accelerate progress in research and engineering to burgeon great ideas as a single Machine Intelligence Community.  
**Location**: 6th Floor of the MIT Media Lab  
**Date**: Saturday September 7th, 2019  
**Time**: 8 AM - 6 PM  
**Registration**: [https://machineintelligence.cc/conference](https://machineintelligence.cc/conference)  
**Talks from 2018**: [https://www.youtube.com/channel/UCEkwg51OD930FsyTx7bV0Pg](https://www.youtube.com/channel/UCEkwg51OD930FsyTx7bV0Pg)  


Follow us on our social media  
Twitter: [https://twitter.com/mic\_conf](https://twitter.com/mic_conf)  
Facebook: [https://www.facebook.com/miconference](https://www.facebook.com/miconference)",1,8,False,self,,,,,
1,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,9,b7v1tm,self.MachineLearning,[D]How to combine metadata info into a Conv1D network?,https://www.reddit.com/r/MachineLearning/comments/b7v1tm/dhow_to_combine_metadata_info_into_a_conv1d/,dinoaide,1554079067,"We have a classification problem. The raw data are from multiple sensors on a device and based on their readings we would classify them into two categories: normal or irregular. From study it seems Conv1D is very suitable for this task.

&amp;#x200B;

However we also have some important metadata related with each device e.g. hardware/software versions, GPS coordinates, environment temperatures. Since they do not come as time series we couldn't build them into the network.

&amp;#x200B;

In this case, when and how should we use metadata information? Could we train a Conv1D network and another ML algorithm based on the metadata and somehow combine them? But how?",4,2,False,self,,,,,
2,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,9,b7v9js,self.MachineLearning,Residual Neural Networks in R and some other questions!!,https://www.reddit.com/r/MachineLearning/comments/b7v9js/residual_neural_networks_in_r_and_some_other/,bthi,1554080354,"Is there any good resources on how to implement these in R? And can I use these residual blocks as part of convolutional neural networks? I am trying to learn more about these things but I am forced to use R atm (there's lots of resources for python, I think).

&amp;#x200B;

If anyone could direct me to a good working example in R or something of these types of neural networks, it would be great. I have some code, but I am not sure why the author named the layer as a residual:

&amp;#x200B;

'''

\## input CNN

input\_CNN = input\_img\_norm %&gt;%

  layer\_conv\_2d(32, kernel\_size = kernel\_size, padding = ""same"") %&gt;%

  layer\_batch\_normalization(momentum = 0.99) %&gt;%

  layer\_activation\_elu() %&gt;%

  layer\_max\_pooling\_2d(c(2,2)) %&gt;%

  layer\_dropout(0.25) %&gt;%

  layer\_conv\_2d(64, kernel\_size = kernel\_size,padding = ""same"") %&gt;%

  layer\_batch\_normalization(momentum = 0.99) %&gt;%

  layer\_activation\_elu() %&gt;%

  layer\_max\_pooling\_2d(c(2,2)) %&gt;%

  layer\_dropout(0.25) 

&amp;#x200B;

\## first residual

input\_CNN\_residual = input\_CNN %&gt;%

  layer\_batch\_normalization(momentum = 0.99) %&gt;%

  layer\_conv\_2d(128, kernel\_size = kernel\_size,padding = ""same"") %&gt;%

  layer\_batch\_normalization(momentum = 0.99) %&gt;%

  layer\_activation\_elu() %&gt;%

  layer\_dropout(0.25) %&gt;%

  layer\_conv\_2d(64, kernel\_size = kernel\_size,padding = ""same"") %&gt;%

  layer\_batch\_normalization(momentum = 0.99) %&gt;%

  layer\_activation\_elu()

&amp;#x200B;

input\_CNN\_residual = layer\_add(list(input\_CNN\_residual,input\_CNN))

'''

&amp;#x200B;

Why does the author comment that as the ""first residual""?",0,1,False,self,,,,,
3,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,10,b7vf2p,self.MachineLearning,[D]How to explain random forest models to business?,https://www.reddit.com/r/MachineLearning/comments/b7vf2p/dhow_to_explain_random_forest_models_to_business/,dinoaide,1554081251,"We recently built a random forest regressor to replace our previous regression model in one of our application. While the result shows a big improvement, our business client asked us to explain the model and visualize it.

&amp;#x200B;

So we started with relative importance, pretty much like in the [relative importance of Iris data](https://i0.wp.com/blog.methodsconsultants.com/posts/2018-06-13-be-aware-of-bias-in-rf-variable-importance-metrics_files/figure-html/unnamed-chunk-4-1.png?zoom=1.25&amp;w=450&amp;ssl=1)

(full article: [https://www.r-bloggers.com/be-aware-of-bias-in-rf-variable-importance-metrics/](https://www.r-bloggers.com/be-aware-of-bias-in-rf-variable-importance-metrics/)) and client agreed with us. 

&amp;#x200B;

Then we try to visualize the tree, also like the [random forest tree visualization of Iris data](https://cdn-images-1.medium.com/max/1200/1*IPLwmH-TJRhEWXW7uaetMw.png) (full article:  [https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c](https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c)), which put client in an immediate loss, to the point that while client was still impressed with the result and would like to give it a try, they were less certain to back the story. And it immediately lead to two things: 

1. Is the model scalable to handle all scenarios?
2. Would the model be still valid after a period of time? Do we need to calibrate the model from time to time and what does that lead to business implications, e.g. how do we explain why we predicted result A in ver 1 and then result B in ver 2?

&amp;#x200B;

This is just random forest and we want to bring others like neural networks into real use. How should we convince our clients in the right way?",31,20,False,self,,,,,
4,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,10,b7vp3c,arxiv.org,[R] Towards Standardization of Data Licenses: The Montreal Data License,https://www.reddit.com/r/MachineLearning/comments/b7vp3c/r_towards_standardization_of_data_licenses_the/,hardmaru,1554082941,,7,44,False,default,,,,,
5,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,11,b7w4pk,self.MachineLearning,[D] How to deploy machine learning models into production?,https://www.reddit.com/r/MachineLearning/comments/b7w4pk/d_how_to_deploy_machine_learning_models_into/,maruchanr,1554085526,I've had some difficulty finding an easy solution to actually operationalizing my models. Very curious to hear other people's experiences and what methods or services you guys are using. Thanks!,17,10,False,self,,,,,
6,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,12,b7wk13,i.redd.it,Waiting for you!!!,https://www.reddit.com/r/MachineLearning/comments/b7wk13/waiting_for_you/,miyawang12138,1554088074,,0,1,False,https://b.thumbs.redditmedia.com/qDkif2EKuO2GIdKcnq58HLETP8u1ZvH5V27c9Sf8qdg.jpg,,,,,
7,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,12,b7wpx6,youtube.com,[D] A Response to Steven Pinker on AI,https://www.reddit.com/r/MachineLearning/comments/b7wpx6/d_a_response_to_steven_pinker_on_ai/,Shevizzle,1554089104,,0,1,False,https://b.thumbs.redditmedia.com/9mui1gJmeRWR8kc1rvExy2d2LGt6yIVLcSXf498mjak.jpg,,,,,
8,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,12,b7ws7c,self.MachineLearning,[D] Train Dev Test Split Ratio for Large Datasets?,https://www.reddit.com/r/MachineLearning/comments/b7ws7c/d_train_dev_test_split_ratio_for_large_datasets/,ReacH36,1554089504,"Currently working on a transfer-learning classification model for an image dataset with over 800k images. I used to follow the 70-30 train test split rule of thumb, but am now re-considering. 

My Questions:

1. Is there any merit to a more extreme split? Maybe say 0.98/0.01/0.01? 
2. Are splits for transfer learning models different in general?",7,1,False,self,,,,,
9,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,12,b7wyxf,self.MachineLearning,[D] Stratified sampling and tfrecords,https://www.reddit.com/r/MachineLearning/comments/b7wyxf/d_stratified_sampling_and_tfrecords/,ReacH36,1554090761,"Are there any good learning resources on how to implement stratified sampling in tensorflow? And with tf.records specifically?

Closest I've found is Hvass-Labs tutorial series which covers tf.records, but no stratified sampling implementation. [https://www.youtube.com/watch?v=oxrcZ9uUblI](https://www.youtube.com/watch?v=oxrcZ9uUblI) [https://github.com/Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials)

&amp;#x200B;",2,7,False,self,,,,,
10,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,13,b7x3g5,arxiv.org,[R] [1903.12650] Yet Another Accelerated SGD: ResNet-50 Training on ImageNet in 74.7 seconds,https://www.reddit.com/r/MachineLearning/comments/b7x3g5/r_190312650_yet_another_accelerated_sgd_resnet50/,evc123,1554091549,,7,0,False,default,,,,,
11,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,13,b7x71m,self.MachineLearning,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b7x71m/machine_learning/,florfashion,1554092166,[removed],0,1,False,self,,,,,
12,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,13,b7xl62,arxiv.org,"[R] Toroidal AutoEncoder - deterministic with torus-like latent space, e.g. for mulitple-path morphing",https://www.reddit.com/r/MachineLearning/comments/b7xl62/r_toroidal_autoencoder_deterministic_with/,jarekduda,1554094653,,2,9,False,default,,,,,
13,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,14,b7y569,self.MachineLearning,Present-Day Machine Learning in Mobile Applications,https://www.reddit.com/r/MachineLearning/comments/b7y569/presentday_machine_learning_in_mobile_applications/,appsbee,1554098115,[removed],0,1,False,self,,,,,
14,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,15,b7yaf8,self.MachineLearning,[P] Bezos: Build your own reinforcement learning framework (Github),https://www.reddit.com/r/MachineLearning/comments/b7yaf8/p_bezos_build_your_own_reinforcement_learning/,gebninja,1554099062,"Hey guys, I built this over the last few months and I thought it could be of interest to some of you.

[https://github.com/justinglibert/bezos](https://github.com/justinglibert/bezos)",1,4,False,self,,,,,
15,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,15,b7ybsb,self.MachineLearning,Top ML Conference from AI/ML Hardware Implementation,https://www.reddit.com/r/MachineLearning/comments/b7ybsb/top_ml_conference_from_aiml_hardware/,Mrikapa,1554099320,[removed],0,1,False,self,,,,,
16,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,15,b7ye1z,self.MachineLearning,[D] Can transformers handle punctuation?,https://www.reddit.com/r/MachineLearning/comments/b7ye1z/d_can_transformers_handle_punctuation/,AnonMLstudent,1554099740,"Can transformers deal with punctuation, upper versus lower case, extra periods, and other issues such as these or should I perform a full preprocessing before reading in the data?

In particular, I have several lines of input that consist of multiple sentences and hence have punctuation in them (like periods, question marks, exclamation marks) as well as upper versus lower case. 

BTW, my task is to just train a chatbot that can generate a response given an input (which may be multiple sentences long). ",0,1,False,self,,,,,
17,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,15,b7ygtx,self.MachineLearning,ML applications for in Accounting &amp; Finance (Forecasting ?),https://www.reddit.com/r/MachineLearning/comments/b7ygtx/ml_applications_for_in_accounting_finance/,wickeeeeeeeeee,1554100253,[removed],1,1,False,self,,,,,
18,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,15,b7yjo6,self.MachineLearning,Power level in google colabs. What does it signify?,https://www.reddit.com/r/MachineLearning/comments/b7yjo6/power_level_in_google_colabs_what_does_it_signify/,sathvik66,1554100797,[removed],0,1,False,self,,,,,
19,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,15,b7ymzs,self.MachineLearning,[D] Feasibility of transforming existing Text Corpus/Datasets to other languages.,https://www.reddit.com/r/MachineLearning/comments/b7ymzs/d_feasibility_of_transforming_existing_text/,crazycokeboy,1554101423,"There are plenty of datasets available for various ML-NLP tasks in English. Is there any research into converting the current corpuses/datasets (think SQuAD, NLID, etc) into other languages? 

It is obvious that there will be some kind of degradation of the data (due to changes in words, positions of words and even meanings of sentences).

I would like to understand the extent to which such degradations can happen, and any solutions that can work around this problem. ",1,2,False,self,,,,,
20,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,16,b7yxpy,hackernoon.com,"TensorFlow is dead, long live TensorFlow!",https://www.reddit.com/r/MachineLearning/comments/b7yxpy/tensorflow_is_dead_long_live_tensorflow/,RubiksCodeNMZ,1554103456,,0,1,False,https://b.thumbs.redditmedia.com/eLOiQIrAJZcbc2zxPOGexCTzQhFSDAT5w8dau_kBm9c.jpg,,,,,
21,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,16,b7z89k,theappsolutions.com,[D] Guide to Unsupervised Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b7z89k/d_guide_to_unsupervised_machine_learning/,lady_monsoon,1554105561,,0,1,False,default,,,,,
22,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,17,b7zgxn,self.MachineLearning,[News] Formal verification and Machine learning summer school in Paris-Saclay University,https://www.reddit.com/r/MachineLearning/comments/b7zgxn/news_formal_verification_and_machine_learning/,RgSVM,1554107346,"Hello r/ml,

I am part of a team of researchers coming from both the machine learning and formal methods fields.

We are organizing a summer school on bridges between formal methods and artificial intelligence (mostly deep-learning, but GOFAI will also be discussed).

Here is the link : [https://www.formal-paris-saclay.fr/](https://www.formal-paris-saclay.fr/)

# Overview:

ForMaL is a Spring  School under the auspices of Labex Digicosme.

Nowadays, there is an increasing need for providing formal guarantees for machine-learning algorithms. Conversely, machine learning techniques have been successfully applied in the realm of formal methods and, in particular, verification.

This school will bring together researchers from, and at the interface of, formal methods and machine learning, with the aim to create synergies between them. It will feature basic lectures, covering fundamental concepts in machine learning and formal methods, as well as more advanced topics highlighting current research challenges.

# Confirmed Speakers (to be completed):

* Giovanni Cherubin (EPFL, Lausanne, Switzerland)
* Nathanal Fijalkow (CNRS, LaBRI, Bordeaux, France &amp; The Alan Turing Institute, London, UK)
* Guy Katz (Hebrew University of Jerusalem)
* Daniel Neider (Max Planck Institute for Software Systems, Kaiserslautern, Germany)
* Reza Shokri (National University of Singapore)
* Martin Vechev (ETH Zrich, Switzerland)

# Fees

No entry fee but registration is required. Any grad student, industrial and researcher can apply.

&amp;#x200B;

Feel free to ask me any question and I'll do my best to answer :)",0,3,False,self,,,,,
23,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,18,b7zsyk,smarten.com,Predictive Modeling for Every Business User!,https://www.reddit.com/r/MachineLearning/comments/b7zsyk/predictive_modeling_for_every_business_user/,ElegantMicroWebIndia,1554109866,,0,1,False,default,,,,,
24,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,18,b8010h,self.MachineLearning,BERT for single word encodings,https://www.reddit.com/r/MachineLearning/comments/b8010h/bert_for_single_word_encodings/,happyhammy,1554111468,[removed],0,1,False,self,,,,,
25,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,18,b801yg,self.MachineLearning,A historic first!,https://www.reddit.com/r/MachineLearning/comments/b801yg/a_historic_first/,johnsnowlabsUS,1554111653,[removed],0,1,False,self,,,,,
26,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,18,b806d6,towardsdatascience.com,Deep Learning for fun: How to generate your own The Simpsons TV script,https://www.reddit.com/r/MachineLearning/comments/b806d6/deep_learning_for_fun_how_to_generate_your_own/,w0b3l1x,1554112512,,0,1,False,default,,,,,
27,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,20,b80w9w,self.MachineLearning,HCRC Map Task Dataset Preprocessing,https://www.reddit.com/r/MachineLearning/comments/b80w9w/hcrc_map_task_dataset_preprocessing/,intergalactic_robot,1554117182,[removed],0,1,False,self,,,,,
28,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,20,b810eh,towardsdatascience.com,[P] Deep Learning for fun: How to generate your own The Simpsons TV script,https://www.reddit.com/r/MachineLearning/comments/b810eh/p_deep_learning_for_fun_how_to_generate_your_own/,w0b3l1x,1554117859,,0,1,False,default,,,,,
29,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,20,b817fz,self.MachineLearning,"[P] PyTorch implementation of ""Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights""",https://www.reddit.com/r/MachineLearning/comments/b817fz/p_pytorch_implementation_of_incremental_network/,Mxbonn,1554119059,"I found Incremental Network Quantization an interesting way to perform quantization aware training. However the original code is a modified version of the caffe source code and not that user friendly. I reimplemented it into a pytorch library that is easy to use.

Code:  [https://github.com/Mxbonn/INQ-pytorch](https://github.com/Mxbonn/INQ-pytorch) 

Original paper:  [https://arxiv.org/abs/1702.03044](https://arxiv.org/abs/1702.03044) ",5,86,False,self,,,,,
30,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,21,b81i87,self.MachineLearning,[D] Dimensionality reduction,https://www.reddit.com/r/MachineLearning/comments/b81i87/d_dimensionality_reduction/,ptainhat,1554120654,"If you had 1 box with 3 values that could each contain say 20 different values but not the same value. One box could be ABC, DFE, etc. The thing is we consider ABC and BCA the same box because the box contains the same values.  
  
I've represented this with an array of length 20 and if the box contains A, then the first index is set to 1, contains B then the second index is set to 1. If a letter/value is not contained then the corresponding index has the value 0.  
  
This gives me an array of 20 or larger depending on the number of different values. So if there were 200 different values the number array would have length 200.  
  
Do you know of a better way to represent this? ",32,2,False,self,,,,,
31,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,21,b81im0,findnerd.com,http://findnerd.com/account/#url=/list/view/What-are-Popular-Enterprise-Machine-Learning-Use-Cases/43746/,https://www.reddit.com/r/MachineLearning/comments/b81im0/httpfindnerdcomaccounturllistviewwhatarepopularent/,techyguy01,1554120711,,0,1,False,default,,,,,
32,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,21,b81q0u,self.MachineLearning,[P] Image ATM (Automated Tagging Machine) - Image classification for everyone.,https://www.reddit.com/r/MachineLearning/comments/b81q0u/p_image_atm_automated_tagging_machine_image/,datitran,1554121847,"Hey we've recently open-sourced a beta version of our internal image classification library Image ATM which is a one-click tool that automates the workflow of a typical image classification pipeline in an opinionated way, this includes:

* Preprocessing and validating input images and labels
* Starting/terminating cloud instance with GPU support
* Training
* Model evaluation

We're currently in testing mode and a stable version is expected to come out soon. We have a lot of interesting stuff on our roadmap like adding AutoML, more evaluation results etc.. Would be nice if you folks can try it out and let us know if there are any bugs or additional features that you would like to have. We also look for people who would like to contribute to this project.

&amp;#x200B;

 Github: [https://github.com/idealo/imageatm](https://github.com/idealo/imageatm)

 Documentation: [https://idealo.github.io/imageatm/](https://idealo.github.io/imageatm/)

&amp;#x200B;

There are also some Google Colab notebooks to try out our library:

* Cats and dogs example: [https://colab.research.google.com/github/idealo/imageatm/blob/master/examples/imageatm\_cats\_and\_dogs.ipynb](https://colab.research.google.com/github/idealo/imageatm/blob/master/examples/imageatm_cats_and_dogs.ipynb)
* Imagenette example: [https://colab.research.google.com/github/idealo/imageatm/blob/master/examples/imageatm\_imagenette.ipynb](https://colab.research.google.com/github/idealo/imageatm/blob/master/examples/imageatm_imagenette.ipynb)

&amp;#x200B;

Here's also a ""number of lines"" comparison of tf.keras vs. imageatm:

&amp;#x200B;

**imageatm**

https://i.redd.it/f6mwbsx8cnp21.png

**tf.keras**

https://i.redd.it/3r8t1mggcnp21.png",0,29,False,https://b.thumbs.redditmedia.com/YZAy1hNgTLyVVXqszgGvHc3fO37a0gPlfFptHC_31IA.jpg,,,,,
33,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,21,b81uwq,self.MachineLearning,[R] Model Vulnerability to Distributional Shifts over Image Transformation Sets,https://www.reddit.com/r/MachineLearning/comments/b81uwq/r_model_vulnerability_to_distributional_shifts/,ricvolpi,1554122588,"Hi! We've released the preprint of our work ""Model Vulnerability to Distributional Shifts over Image Transformation Sets"". Code is available at [https://github.com/ricvolpi/domain-shift-robustness](https://github.com/ricvolpi/domain-shift-robustness). 

TLDR; we propose one way to determine the (concatenation of) image transformations the a given (black-box) model is most vulnerable to, via random search and evolution-based search. We propose a training procedure where the vulnerability regions for the current model are searched for and harnessed throughout the training phase, by defining proper data augmentation rules. 

Feel free to ask any questions. Any feedback is appreciated!

Cheers,                                                                                                                                                                             Riccardo",1,1,False,self,,,,,
34,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,22,b828v7,blockdelta.io,Uses of Artificial Intelligence in Health Care and How It Helps in Diagnosing Rare Genetic Disorders,https://www.reddit.com/r/MachineLearning/comments/b828v7/uses_of_artificial_intelligence_in_health_care/,BlockDelta,1554124568,,0,1,False,https://b.thumbs.redditmedia.com/U42cmLUrpqSYzVYgcHTHbn079IM4EbTxL1GbKqV45Fc.jpg,,,,,
35,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,22,b82csp,self.MachineLearning,"[P] I've had a couple people ask me for help with pronouncing some common terminology in ML, so I made this helpful guide to clarify these pronunciations.",https://www.reddit.com/r/MachineLearning/comments/b82csp/p_ive_had_a_couple_people_ask_me_for_help_with/,ajmooch,1554125134,[removed],0,1,False,self,,,,,
36,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,22,b82doy,towardsdatascience.com,Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab),https://www.reddit.com/r/MachineLearning/comments/b82doy/stepbystep_guide_to_creating_r_and_python/,cyberneticglory,1554125260,,0,1,False,default,,,,,
37,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,22,b82f9d,self.MachineLearning,Google uses machine learning to improve the well being of flowers,https://www.reddit.com/r/MachineLearning/comments/b82f9d/google_uses_machine_learning_to_improve_the_well/,DataHat,1554125488,[removed],0,1,False,self,,,,,
38,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,22,b82jz2,self.MachineLearning,[D] How to implement Support-Vector-Regression by using SMO ??,https://www.reddit.com/r/MachineLearning/comments/b82jz2/d_how_to_implement_supportvectorregression_by/,skywind3000,1554126204,"There are tons of papers on SVC implementation, but I can't find one for SVR in detail.

For example, platt's SMO paper tells me how to update alphas:

    alpha_j_new = alpha_j_old + (E1 - E2) / (Kii + Kjj - 2Kij)

But it is for SVC, not for SVR. After reading a lot of SVR papers, I still have no idea how to update each variables.

Is there any documentation about how to implement a SVR in python/cpp ??",3,2,False,self,,,,,
39,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,22,b82nk2,mlg.eng.cam.ac.uk,[D] (April 1st) Cambridge Machine Learning Group to give up Bayesian ML for frequentism,https://www.reddit.com/r/MachineLearning/comments/b82nk2/d_april_1st_cambridge_machine_learning_group_to/,rhaps0dy4,1554126770,,0,1,False,default,,,,,
40,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,23,b82tfj,self.MachineLearning,[D] How do you design your AI PoCs?,https://www.reddit.com/r/MachineLearning/comments/b82tfj/d_how_do_you_design_your_ai_pocs/,ArnaultChazareix,1554127578,"I noticed that most entrepreneurs' AI Ideas go to waste. IMHO, people don't fail their AI PoC because of a lack of technical tutorials. They just need more insight on how to design their AI.  


Here is [my way to do it](https://blog.sicara.com/how-to-build-successful-ai-poc-8acfe386a69a).  


What's yours? ",0,0,False,self,,,,,
41,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,23,b82ty7,self.MachineLearning,Would anybody use an app to help you collect labeled data from your phone?,https://www.reddit.com/r/MachineLearning/comments/b82ty7/would_anybody_use_an_app_to_help_you_collect/,tangerto,1554127647,[removed],0,1,False,self,,,,,
42,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,23,b82yqh,self.MachineLearning,PyCandle - Lightweight Experiment Library for Pytorch [P],https://www.reddit.com/r/MachineLearning/comments/b82yqh/pycandle_lightweight_experiment_library_for/,Chrizs_,1554128291,"  


https://i.redd.it/3prihq9nvnp21.png

I just released [PyCandle (GitHub)](https://github.com/cschoeller/pycandle), a lightweight library for pytorch that makes running  experiments easy, better structured, repeatable and avoids boilerplate code. It  is built flexible and allows to also train complex models like  recurrent or generative neural networks conveniently.  
I will continue to develop it, but already now it is a great help for me and I used it actively in my own research projects. To get an impression of how it works, check out the [MNIST example](https://github.com/cschoeller/pycandle/blob/master/minimal_example/mnist.py).  


Feel free to use it for your own projects and share it with your friends. ",0,1,False,self,,,,,
43,MachineLearning,t5_2r3gv,2019-4-1,2019,4,1,23,b831xb,wuhuikai.me,FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/b831xb/fastfcn_rethinking_dilated_convolution_in_the/,Jasonnor,1554128711,,0,1,False,default,,,,,
44,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,0,b83mo7,self.MachineLearning,Progress on Temporally Recurrent Optimal Learning?,https://www.reddit.com/r/MachineLearning/comments/b83mo7/progress_on_temporally_recurrent_optimal_learning/,slickric,1554131469,[removed],0,1,False,self,,,,,
45,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,0,b8464j,self.MachineLearning,What is your deployment strategy?,https://www.reddit.com/r/MachineLearning/comments/b8464j/what_is_your_deployment_strategy/,XsentiusIroh,1554133938,[removed],0,1,False,self,,,,,
46,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84bqz,self.MachineLearning,Running inference online,https://www.reddit.com/r/MachineLearning/comments/b84bqz/running_inference_online/,C4ged,1554134646,"I want to use my model in a web application, problem is GPU costs per hour are high + I'm guessing the inference won't run 100% of the time. Are there any techniques or services that allow you to run inference intermittently without incurring these high costs? 

(Please tell me if I failed to provide any vital information in my post)",2,1,False,self,,,,,
47,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84djn,i.redd.it,I made a GAN that generates Jeff Bezos,https://www.reddit.com/r/MachineLearning/comments/b84djn/i_made_a_gan_that_generates_jeff_bezos/,awalias,1554134875,,0,1,False,default,,,,,
48,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84he5,i.imgur.com,Training on your test set: The worst crime of all,https://www.reddit.com/r/MachineLearning/comments/b84he5/training_on_your_test_set_the_worst_crime_of_all/,KinkyBelayer,1554135363,,0,1,False,default,,,,,
49,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84oqg,blog.bitsrc.io,JavaScript for Machine Learning using TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/b84oqg/javascript_for_machine_learning_using_tensorflowjs/,JSislife,1554136322,,0,1,False,https://b.thumbs.redditmedia.com/YIGq8eebUxOeLOM2vEAhuRa0kG5v3ztR9WJ0ZK_wN1M.jpg,,,,,
50,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84tzz,self.MachineLearning,[P] Convert argparse to python class format for Jupyter notebook execution,https://www.reddit.com/r/MachineLearning/comments/b84tzz/p_convert_argparse_to_python_class_format_for/,Sngjuk,1554137013,"Hi, I wrote script that converts argparse to python class format. 

as argparse in pytorch code is not compatible with Jupyter notebook. 

I hope it would be helpful for your testing.

web convert: [http://35.192.144.192:8000/arg2cls.html](http://35.192.144.192:8000/arg2cls.html)

github: [https://github.com/sngjuk/argparse-to-class](https://github.com/sngjuk/argparse-to-class)

If you find buggy results please let me know. Thx",0,0,False,self,,,,,
51,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84u3h,self.MachineLearning,[D] ML Interview: Coding Task,https://www.reddit.com/r/MachineLearning/comments/b84u3h/d_ml_interview_coding_task/,AllinOnAI,1554137025,"Hey everyone,

I've recently applied to a startup focusing on Machine Learning.

They have sent me a link to start an online ML coding task for which I have 100 mins time.  
It is split into 2-3 problems with varying difficulty and they strongly advised me to practice beforehand on hackerrank.

&amp;#x200B;

Has anyone had a similar ML focused coding task? Can you give me hints on what I can expect, so I can prepare accordingly? Without any prior information, I would expect one of the following:

1. Preparing and visualizing datasets, feature engineering
2. Applying models (lin. regression, SVM, Decision Trees)
3. Validating models (questionable, since time is limited)

&amp;#x200B;

Thank you so much!",6,2,False,self,,,,,
52,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84umo,self.MachineLearning,What could you do with Reddit Data?,https://www.reddit.com/r/MachineLearning/comments/b84umo/what_could_you_do_with_reddit_data/,clr-t,1554137095,[removed],0,1,False,self,,,,,
53,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84xfq,self.MachineLearning,Would you use an app that lets you collect labeled data from your phone?,https://www.reddit.com/r/MachineLearning/comments/b84xfq/would_you_use_an_app_that_lets_you_collect/,tangerto,1554137465,[removed],0,1,False,self,,,,,
54,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b84yow,wuhuikai.me,[R] FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation,https://www.reddit.com/r/MachineLearning/comments/b84yow/r_fastfcn_rethinking_dilated_convolution_in_the/,Jasonnor,1554137625,,0,1,False,default,,,,,
55,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,1,b851ck,self.MachineLearning,Predicting strength of router based on location,https://www.reddit.com/r/MachineLearning/comments/b851ck/predicting_strength_of_router_based_on_location/,praveenraghuvanshi,1554137958,"I am working on data and trying to make a prediction on it using supervised learning.Problem Statement: Let's consider we have data of routers as given below with probable values.

## Features

* Name (Unique)
* Brand : Cisco, Huawei, Netgear
* Location: Bedroom(B), Kitchen(K), Hall(H)
* IP address : Unique
* MacAddress : Unique
* Serial Number : Unique
* Firmware version: Varying like 1.0.0, 2.0.1, 3.1.0 etc
* state: running, discovering, rebooting

## Output:

Strength: Strong/Weak: 1/0

## Rules governing 'Strength' output is given below.

* Brand == Cisco ==&gt; Strength == Strong in all locations. B + H + K
* Brand == Huawei ==&gt; Strength == Strong in Hall and kitchen only. H + K
* Brand == Netgear ==&gt; Strength == Strong in kitchen only. K

We consider Brand and Location only for predicting Signal Strength.

# Sample Train Data

|Brand|Location|Strength|
|:-|:-|:-|
|Cisco|Bedroom|Strong|
|Huawei|Bedroom|Weak|
|Netgear|Hall|Weak|
|Cisco|Kitchen|Strong|
|Huawei|Hall|Strong|
|Netgear|Kitchen|Strong|

# Sample Test Data

|Brand|Location|Strength|
|:-|:-|:-|
|Cisco|Hall|Strong|
|Huawei|Kitchen|Strong|
|Netgear|Bedroom|Weak|

Questions:

* Can this problem statement solved using machine learning or machine learning is an overkill?
* What algorithm/architecture to be used to solve such a problem? Is normal neural network enough or CNN is more appropriate for this problem consider scaling in future?
* Can we include any other feature for better accuracy? Using something other than Location such as Firmware version.
* How much data is enough to start with?
* Can we consider predicting house price similar to this problem?

Kindly share your suggestions.

Thanks in advance!!!",0,1,False,self,,,,,
56,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,2,b85ct4,docs.google.com,"Hi all, Please help me to fill out this survey for my final year dissertation. It may seem to be irrelevant to this subreddit but at the end of the survey, it would all make sense. Thank you so much",https://www.reddit.com/r/MachineLearning/comments/b85ct4/hi_all_please_help_me_to_fill_out_this_survey_for/,haoran96,1554139297,,0,1,False,https://a.thumbs.redditmedia.com/f5Yvg2-YucKH0tua2eo0gytBYTOBppgx-dN4r3_wq_0.jpg,,,,,
57,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,2,b85e70,self.MachineLearning,[Discussion] Would you use an app that lets you collect labeled data from your phone?,https://www.reddit.com/r/MachineLearning/comments/b85e70/discussion_would_you_use_an_app_that_lets_you/,tangerto,1554139448,"Sometimes when I'm doing machine learning experiments, I often end up collecting my own data- image, audio, video, etc. Would other people find an app that lets you collect and label image/audio/video data right from your phone? Auto-export CSVs, labeled folders, etc.",10,7,False,self,,,,,
58,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,2,b85xms,medium.com,"iFLYTEK &amp; HIT Reading Comprehension Model Betters Humans, Tops SQuAD2.0 Leaderboard",https://www.reddit.com/r/MachineLearning/comments/b85xms/iflytek_hit_reading_comprehension_model_betters/,Yuqing7,1554141135,,0,1,False,default,,,,,
59,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,3,b864m5,self.MachineLearning,Sifting Through the Hype of Artificial Intelligence and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b864m5/sifting_through_the_hype_of_artificial/,tonys3kur3,1554141657,[removed],0,1,False,self,,,,,
60,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,3,b864sg,self.MachineLearning,Small dataset for machine learning algorithm,https://www.reddit.com/r/MachineLearning/comments/b864sg/small_dataset_for_machine_learning_algorithm/,zee7278,1554141670,[removed],0,1,False,self,,,,,
61,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,3,b866il,self.MachineLearning,Anyone seen densepose or equivalent for 2D character animation?,https://www.reddit.com/r/MachineLearning/comments/b866il/anyone_seen_densepose_or_equivalent_for_2d/,mhdempsey,1554141818,[removed],0,1,False,self,,,,,
62,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,3,b86fxb,nextplatform.com,New MiLDreD AI Chip Architecture Focuses on Memory Capacity Over Capability,https://www.reddit.com/r/MachineLearning/comments/b86fxb/new_mildred_ai_chip_architecture_focuses_on/,KeponeFactory,1554143015,,0,1,False,https://b.thumbs.redditmedia.com/AQk2y6Q7Lf2id3siseSLIJCfnuQoUq4C_dM89BWBMUI.jpg,,,,,
63,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,3,b86t8h,self.MachineLearning,MS CS-AI vs. MS Mathematics - Experienced Data Scientist,https://www.reddit.com/r/MachineLearning/comments/b86t8h/ms_csai_vs_ms_mathematics_experienced_data/,mlbatman,1554144515,"Hello Chaps,

I am in great dilemma and want to know your opinion on it as it is driving me crazy.

&amp;#x200B;

I am a senior data scientist with 5 years of ML experience and i am also in top 1000 on kaggle's competition tier. I have 2 admits from the same university. One in CS - AI and One in Mathematics (I can take stats and computing courses in this but no ML DL) , Which course should i go for to help me in the longer term to stay in ML. Your help will be very much appreciated as i am going nuts with this dilemma.",0,1,False,self,,,,,
64,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,3,b86xnn,self.MachineLearning,[D] Free Marketing for Vertical ML/AI Startup,https://www.reddit.com/r/MachineLearning/comments/b86xnn/d_free_marketing_for_vertical_mlai_startup/,v3nge,1554144960,"I'm looking to help an AI/ML startup who's product/service solves a problem for a specific niche, to grow their customer/user base. 

&amp;#x200B;

I have a degree in Business Marketing and I have experience working with businesses on their marketing campaigns as well as certification from Facebook as a certified Ad Buying Professional. 

&amp;#x200B;

I'm looking to work for free with a AI/ML startup so that I can use the results as a case study to get consulting clients in the AI field. 

&amp;#x200B;

If you're interested in working with me, please send me a DM. (Please keep in mind though I'm doing my part for free, there may be advertising costs.)",5,9,False,self,,,,,
65,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,4,b87guc,medium.com,1D &amp; 3D Convolutions explained with MS Excel!,https://www.reddit.com/r/MachineLearning/comments/b87guc/1d_3d_convolutions_explained_with_ms_excel/,Hlodynn,1554146996,,0,1,False,default,,,,,
66,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,4,b87hz1,markku.ai,Short Introduction to Data Augmentation  Markku.ai,https://www.reddit.com/r/MachineLearning/comments/b87hz1/short_introduction_to_data_augmentation_markkuai/,FIN_Master,1554147118,,0,2,False,default,,,,,
67,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,4,b87jgy,self.MachineLearning,Amazon to release largest social conversation and knowledge dataset,https://www.reddit.com/r/MachineLearning/comments/b87jgy/amazon_to_release_largest_social_conversation_and/,jinpanZe,1554147293,"https://developer.amazon.com/blogs/alexa/post/30dc5515-3b9f-4ec2-8f2a-ac98254625c6/topical-chat-dataset-helps-researchers-address-hard-challenges-in-natural-conversation

From the blog post:

Today I am happy to announce our intention to make available the Topical Chat dataset, a corpus of human-human social conversations collected from crowd workers that will be released publicly on September 17, 2019.

The dataset was developed for teams competing in the Alexa Prize Socialbot Grand Challenge 3, with the application period closing May 14, 2019, and the competition launching September 9, 2019 (apply and learn morehere). Teams competing in the Alexa Prize will have access to an expanded version of this dataset (the Extended Topical Chat dataset) which includes the results of on-going collections and annotations, in addition to the many other resources exclusive to Alexa Prize participants.

The Topical Chat dataset will consist of more than 210,000 utterances (over 4,100,000 words), making it the largest social conversation and knowledge dataset available publicly to the research community, supporting the publication of high quality, repeatable research.

Each conversation (and each turn of the conversation) in this dataset is linked to knowledge provided to crowd workers. The knowledge is collected from a variety of unstructured or loosely structured text resources, and each conversation refers to a related set of entities. None of these conversations are interactions with Alexa customers.

The goal of this collection is to enable the next steps of research in knowledge-grounded neural response generation systems, tackling hard challenges in natural conversation that are not addressed by other publicly available datasets. This will allow researchers to focus on the way humans transition between topics, knowledge-selection and enrichment, and integration of fact and opinion into dialogue.

Visitwww.alexaprize.comto learn more and stay up-to-date.

",0,1,False,self,,,,,
68,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,4,b87kko,self.MachineLearning,[D] Amazon to release largest social conversation and knowledge dataset,https://www.reddit.com/r/MachineLearning/comments/b87kko/d_amazon_to_release_largest_social_conversation/,jinpanZe,1554147420,"https://developer.amazon.com/blogs/alexa/post/30dc5515-3b9f-4ec2-8f2a-ac98254625c6/topical-chat-dataset-helps-researchers-address-hard-challenges-in-natural-conversation

From the blog post:

Today I am happy to announce our intention to make available the Topical Chat dataset, a corpus of human-human social conversations collected from crowd workers that will be released publicly on September 17, 2019.

The dataset was developed for teams competing in the Alexa Prize Socialbot Grand Challenge 3, with the application period closing May 14, 2019, and the competition launching September 9, 2019 (apply and learn morehere). Teams competing in the Alexa Prize will have access to an expanded version of this dataset (the Extended Topical Chat dataset) which includes the results of on-going collections and annotations, in addition to the many other resources exclusive to Alexa Prize participants.

The Topical Chat dataset will consist of more than 210,000 utterances (over 4,100,000 words), making it the largest social conversation and knowledge dataset available publicly to the research community, supporting the publication of high quality, repeatable research.

Each conversation (and each turn of the conversation) in this dataset is linked to knowledge provided to crowd workers. The knowledge is collected from a variety of unstructured or loosely structured text resources, and each conversation refers to a related set of entities. None of these conversations are interactions with Alexa customers.

The goal of this collection is to enable the next steps of research in knowledge-grounded neural response generation systems, tackling hard challenges in natural conversation that are not addressed by other publicly available datasets. This will allow researchers to focus on the way humans transition between topics, knowledge-selection and enrichment, and integration of fact and opinion into dialogue.

Visitwww.alexaprize.comto learn more and stay up-to-date

",31,341,False,self,,,,,
69,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,4,b87opu,self.MachineLearning,[P] Data augmentation experiences from our Autonomous Robocar project,https://www.reddit.com/r/MachineLearning/comments/b87opu/p_data_augmentation_experiences_from_our/,FIN_Master,1554147882,"We've been building an autonomous RC robocar using Donkeycar software as a base.

The car is using only a neural network to drive the car and we have been improving the model in multiple ways. Data augmentation has proven to be a really effective way to make the model converge with only a small set of data.  


Our blogpost about the topic: [ttps://markku.ai/post/data-augmentation/](https://markku.ai/post/data-augmentation/)",1,17,False,self,,,,,
70,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,5,b88dkr,self.MachineLearning,How should I structure an LSTM to make predictions about the middle frame of some data?,https://www.reddit.com/r/MachineLearning/comments/b88dkr/how_should_i_structure_an_lstm_to_make/,kds_medphys,1554150364,[removed],0,2,False,self,,,,,
71,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,5,b88xsm,self.MachineLearning,[Project]https://mrandri19.github.io/2019/04/01/deriving-gradient-descent-multivariate-linear-regression.html,https://www.reddit.com/r/MachineLearning/comments/b88xsm/projecthttpsmrandri19githubio20190401derivinggradi/,mrandri19,1554152372,"I am reading Michael Nielsen's neural networks and deep learning but I could not find a derivation for the gradient of the cost function w.r.t. weights and biases in matrix form, so I wrote one.
Might be useful for someone who's not too comfortable with matrix calculus.
Here's the post (on my personal blog): https://mrandri19.github.io/2019/04/01/deriving-gradient-descent-multivariate-linear-regression.html
If it's better suited for /r/learnmachinelearning/ just comment and I'll move the post
Enjoy",0,0,False,self,,,,,
72,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,6,b8987f,self.MachineLearning,[R] Towards Automatic Low Hanging Fruit Identification For the Steering of ML Research,https://www.reddit.com/r/MachineLearning/comments/b8987f/r_towards_automatic_low_hanging_fruit/,jinpanZe,1554153413," [https://drive.google.com/file/d/1ls9w\_Axf6Xoi5OBfoiiHIWa9ehTyJbq8/edit](https://drive.google.com/file/d/1ls9w_Axf6Xoi5OBfoiiHIWa9ehTyJbq8/edit) 

&amp;#x200B;

In light of the ongoing explosion of interest in the field of machine learning, we must ask ourselves how researchers can best allocate their resources and determine which problems deserve their attention. We identify and explore the perennial problem of low hanging fruit detection in machine learning research organizations and present a novel, state-of-the-art AI solution to this pertinent problem, which we believe will greatly increase the output of research papers in the machine learning community.",11,38,False,self,,,,,
73,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,6,b89jaf,opendatascience.com,Automated Machine Learning: Myth Versus Reality,https://www.reddit.com/r/MachineLearning/comments/b89jaf/automated_machine_learning_myth_versus_reality/,OpenDataSciCon,1554154588,,0,1,False,default,,,,,
74,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,7,b89yl1,self.MachineLearning,help finding easy to use program for generating music via rnn for public use?,https://www.reddit.com/r/MachineLearning/comments/b89yl1/help_finding_easy_to_use_program_for_generating/,acrolance,1554156305,"is there any programs that is easy to use that takes music and with machine learning it generates new music similar to the original music. i cant seem to find any of these on google through my google-ing. i dont know anything about programing. ive been wanting a good program for this for awhile now and each time i look for one i cant find one i can use, please help",0,1,False,self,,,,,
75,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,7,b8aekp,self.MachineLearning,Keras LSTM Future Predictions,https://www.reddit.com/r/MachineLearning/comments/b8aekp/keras_lstm_future_predictions/,Nathan-T1,1554158001,[removed],0,1,False,self,,,,,
76,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,7,b8amvs,self.MachineLearning,[D] Architecture to generate multiple sequences of text?,https://www.reddit.com/r/MachineLearning/comments/b8amvs/d_architecture_to_generate_multiple_sequences_of/,mackie__m,1554158923,"Using LSTMs we can come up with a model to generate a single sequence given an input (e.g. seq-seq). I'm trying to understand is there an architecture to generate multiple sequences of data at once? I'm trying to use a LSTM-type network to generate trajectories for multiple agents, given some input. Or, is LSTMs the incorrect way about this?",3,1,False,self,,,,,
77,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,7,b8amyv,self.MachineLearning,ML form a mathematical perspective?,https://www.reddit.com/r/MachineLearning/comments/b8amyv/ml_form_a_mathematical_perspective/,Valgor,1554158933,[removed],0,1,False,self,,,,,
78,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,8,b8avym,self.MachineLearning,[Updated] A (cat) machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/b8avym/updated_a_cat_machine_learning_game_ive_been/,twm7,1554160004,[removed],0,1,False,self,,,,,
79,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,8,b8b565,quora.com,Sam Altman is taking questions on OpenAI,https://www.reddit.com/r/MachineLearning/comments/b8b565/sam_altman_is_taking_questions_on_openai/,Digimon_Utopia_99,1554161186,,0,1,False,default,,,,,
80,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,8,b8b7if,self.MachineLearning,[P] Working on Gentrification Prediction Project,https://www.reddit.com/r/MachineLearning/comments/b8b7if/p_working_on_gentrification_prediction_project/,wearefarming101,1554161498,"What are some Packages/libraries that allow me to run GridSearch across multiple models and return the corresponding accuracies? I tried auto-sklearn but I'm having trouble installing it on my Mac, and I believe it only returns the best predictions, while I wanna do more like a cross validation thing across my data, and best mean accuracies for each of the models. 

&amp;#x200B;

I'm spending a lot of time on feature engineering, but when it comes to prediction, I want to brute force my approach. Hence, the above question. ",3,0,False,self,,,,,
81,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,8,b8b7vp,self.MachineLearning,Is a PHD worth it?,https://www.reddit.com/r/MachineLearning/comments/b8b7vp/is_a_phd_worth_it/,Ziinxx,1554161548,[removed],0,1,False,self,,,,,
82,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,8,b8bdjy,self.MachineLearning,[P] A (cat) machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/b8bdjy/p_a_cat_machine_learning_game_ive_been_working_on/,twm7,1554162310,"I've finished working on the new algorithm which is based on ID3; entropy and information gain. Interested to see feedback! [Challenge Incredicat](https://incredicat.com/)!

\-------

Wasn't sure where to post this as I'm still working on it but wanted to put it out there to get any useful feedback or thoughts from the experts. It's basically a game similar to 20 Questions (or Animal, Vegetable, Mineral) that attempts to ask you questions to work out an object you are thinking about. You can think of everyday items (animals, household objects, food, quite a bit of other stuff etc) and it has 30 questions to try and guess the item. I've been working on it for a while but not sure what to do next so interested to hear anyone's thoughts...

The link for anyone that wants to try it out is [incredicat.com](https://incredicat.com/)

Thanks in advance!",5,9,False,self,,,,,
83,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,8,b8bex0,self.MachineLearning,[D] What is a good tool/model that's simple to use for sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/b8bex0/d_what_is_a_good_toolmodel_thats_simple_to_use/,Seankala,1554162483,"Hello everyone. I'm currently working on a project that's basically using Twitter sentiment analysis to gauge the influence on different cryptocurrency markets. I've collected Tweet data - around 170,000 Tweets from 62 different accounts - and was wondering what models people normally use?

I'm fairly new to the NLP/sentiment analysis field and have actually never done it before and was hoping someone could give me some pointers into which direction to look at.

Any tips are appreciated, thanks!",2,1,False,self,,,,,
84,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,9,b8bk88,self.MachineLearning,ASS-Network: Adversarial Sample Synthesis Network,https://www.reddit.com/r/MachineLearning/comments/b8bk88/assnetwork_adversarial_sample_synthesis_network/,jerry0822,1554163213,[removed],0,1,False,self,,,,,
85,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,9,b8bogp,self.MachineLearning,[R] ASS-Network: Adversarial Sample Synthesis Network,https://www.reddit.com/r/MachineLearning/comments/b8bogp/r_assnetwork_adversarial_sample_synthesis_network/,jerry0822,1554163817,"We present a new Generative Adversarial Network Architecture for  perspective augmented image samples which can effectively discriminate  over physical attacks and recover contextual information through our  dual-stage GAN architecture. Using a novel method known as Adversarial  Sample Synthesis, our sampling strategy trains on pairs of images where  we utilize an adversarial sub-sampling approach to effectively learn the  divergence between the fake and real image distributions.

Link: [https://www.scribd.com/document/403980329/A-Net-Architecture](https://www.scribd.com/document/403980329/A-Net-Architecture)",9,38,False,self,,,,,
86,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,9,b8bvp0,self.MachineLearning,[R] Machine Learning for Data-Driven Movement Generation: a Review of the State of the Art,https://www.reddit.com/r/MachineLearning/comments/b8bvp0/r_machine_learning_for_datadriven_movement/,oac,1554164822,"Abstract: ""The rise of non-linear and interactive media such as video games has increased the need for automatic movement animation generation. In this survey, we review and analyze different aspects of building automatic movement generation systems using machine learning techniques and motion capture data. We cover topics such as high-level movement characterization, training data, features representation, machine learning models, and evaluation methods. We conclude by presenting a discussion of the reviewed literature and outlining the research gaps and remaining challenges for future work.""

&amp;#x200B;

Paper: [https://arxiv.org/abs/1903.08356](https://arxiv.org/abs/1903.08356)",1,6,False,self,,,,,
87,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,10,b8ca6t,arxiv.org,[R] Diagnosing and Enhancing VAE Models,https://www.reddit.com/r/MachineLearning/comments/b8ca6t/r_diagnosing_and_enhancing_vae_models/,baylearn,1554166922,,15,7,False,default,,,,,
88,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,10,b8cfgj,sextoyuytin.com,SextoyUyTin - Shop Sextoy -  Chi Tnh Dc - Dng Vt Gi - m o Gi,https://www.reddit.com/r/MachineLearning/comments/b8cfgj/sextoyuytin_shop_sextoy__chi_tnh_dc_dng/,kathlenepistill,1554167680,,0,1,False,default,,,,,
89,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,10,b8cuwp,self.MachineLearning,Solving large nonlinear SVR problems with cplexqp vs SMO.,https://www.reddit.com/r/MachineLearning/comments/b8cuwp/solving_large_nonlinear_svr_problems_with_cplexqp/,AFIT7,1554170006,"Hey all, I am working on an approximate dynamic programming problem for my dissertation, and I am utilizing SVR to update my value function approximation. I am currently solving SVR problem via cplexqp.  Before I spend time coding up SMO, does anyone have any feedback on the time/quality differences between SMO and quadratic programming? Is SMO really that much better than just solving SVR through quadratic programming?  Im using MATLAB as my programming language.  Any inputs would be appreciated.  TIA",0,1,False,self,,,,,
90,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,11,b8cz3s,self.MachineLearning,[D] Solving large nonlinear SVR problems with quadratic programming vs SMO,https://www.reddit.com/r/MachineLearning/comments/b8cz3s/d_solving_large_nonlinear_svr_problems_with/,AFIT7,1554170627,"Hey all, I am working on an approximate dynamic programming problem for my dissertation, and I am utilizing SVR to update my value function approximation. I am currently solving SVR problem via cplexqp.  Before I spend time coding up SMO, does anyone have any feedback on the time/quality differences between SMO and quadratic programming? Is SMO really that much better than just solving SVR through quadratic programming?  Im using MATLAB as my programming language.  Any inputs would be appreciated.  TIA",2,5,False,self,,,,,
91,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,11,b8d2wv,self.MachineLearning,[P] Deep Learning on Healthcare Lecture Series (4),https://www.reddit.com/r/MachineLearning/comments/b8d2wv/p_deep_learning_on_healthcare_lecture_series_4/,hiconcep,1554171208,"Deep Learning on Healthcare (4): Story of Lunit (1). This lecture's theme is case study of Lunit. one of the representative healthcare deep learning startup from South Korea. I think this is an exemplary growth story of healthcare deep learning startup.

&amp;#x200B;

[Deep Learning on Healthcare (4)](https://www.youtube.com/watch?v=KyTo4BvysfQ)

[Deep Learning on Healthcare (3)](https://www.youtube.com/watch?v=1tMy-ZukPQc)

[Deep Learning on Healthcare (2)](https://www.youtube.com/watch?v=zsXKWEa9GhQ&amp;t=31s&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee&amp;index=24)

[Deep Learning on Healthcare (1)](https://www.youtube.com/watch?v=k0LacC4hyY8&amp;index=23&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee)",2,11,False,self,,,,,
92,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,11,b8d68m,msdn.microsoft.com,[D] Closed-Loop Intelligence: A Design Pattern for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b8d68m/d_closedloop_intelligence_a_design_pattern_for/,Wookai,1554171725,,1,1,False,default,,,,,
93,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,11,b8ddo6,i.redd.it,RTX TITAN Workstation | 8x GPUs beast,https://www.reddit.com/r/MachineLearning/comments/b8ddo6/rtx_titan_workstation_8x_gpus_beast/,gimel1213,1554172928,,0,1,False,https://a.thumbs.redditmedia.com/OB0AyvmoVTdeBRYpEk49rLXRqE7T3mr-n4_lV5bRep8.jpg,,,,,
94,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,12,b8dnow,self.MachineLearning,Change point detection benchmark datasets,https://www.reddit.com/r/MachineLearning/comments/b8dnow/change_point_detection_benchmark_datasets/,pig_newton1,1554174534,"I've read several papers in the change point detection field and googled a ton, but I have not come across any standard public datasets that are used for comparing new contributions to existing methods. 

Are there any out there? Is there any reason why no benchmark datasets exist?",0,1,False,self,,,,,
95,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,12,b8dult,self.MachineLearning,Rtx 2060 performing much slower than 1050ti and 1060,https://www.reddit.com/r/MachineLearning/comments/b8dult/rtx_2060_performing_much_slower_than_1050ti_and/,djaym7,1554175696,[removed],0,1,False,self,,,,,
96,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,13,b8eatp,competitions.codalab.org,[N] Competition on Humor Recognition,https://www.reddit.com/r/MachineLearning/comments/b8eatp/n_competition_on_humor_recognition/,bryant1410,1554178573,,0,1,False,default,,,,,
97,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,15,b8fixd,i.redd.it,4x RTX 2080 Ti | Deep learning workstation,https://www.reddit.com/r/MachineLearning/comments/b8fixd/4x_rtx_2080_ti_deep_learning_workstation/,gimel1213,1554186908,,0,1,False,default,,,,,
98,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,15,b8fkqs,youtu.be,Understanding Bayesian Probability,https://www.reddit.com/r/MachineLearning/comments/b8fkqs/understanding_bayesian_probability/,StatusLingonberry,1554187273,,0,1,False,default,,,,,
99,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,17,b8gbij,youtube.com,ROC Curve In R | ROC Curve In Logistic Expression | Data Science Tutorial | Intellipaat,https://www.reddit.com/r/MachineLearning/comments/b8gbij/roc_curve_in_r_roc_curve_in_logistic_expression/,SumanaMahesh,1554192608,,0,1,False,default,,,,,
100,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,17,b8gj6k,thedailydesigns.com,Machine Learning; Favoring Marketers of Today | Thedailydesign,https://www.reddit.com/r/MachineLearning/comments/b8gj6k/machine_learning_favoring_marketers_of_today/,brandianx,1554194327,,0,1,False,default,,,,,
101,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,17,b8gm2x,self.MachineLearning,The mismatch between PPL and BLEU for machine translation,https://www.reddit.com/r/MachineLearning/comments/b8gm2x/the_mismatch_between_ppl_and_bleu_for_machine/,lyy1780,1554194945,[removed],0,1,False,self,,,,,
102,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,18,b8gq87,youtu.be,Python Data Mining and Databases | Data mining and MySQL for Beginners,https://www.reddit.com/r/MachineLearning/comments/b8gq87/python_data_mining_and_databases_data_mining_and/,SquareTechAcademy,1554195781,,0,1,False,default,,,,,
103,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,18,b8h6ph,self.MachineLearning,"[D] For those currently working in ML, when did you know that you wanted to base your career on this?",https://www.reddit.com/r/MachineLearning/comments/b8h6ph/d_for_those_currently_working_in_ml_when_did_you/,dankatorium,1554199108,"Asking as a college student working towards a CS degree, at what moment did you begin to dedicate more time towards ML and learning about the field, pursuing it as a job prospect?",26,21,False,self,,,,,
104,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,19,b8h9gi,self.MachineLearning,Which datasets are the most interesting on Kaggle?,https://www.reddit.com/r/MachineLearning/comments/b8h9gi/which_datasets_are_the_most_interesting_on_kaggle/,jcob_sikorski,1554199611,[removed],0,1,False,self,,,,,
105,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,19,b8hji3,linkedin.com,What Are the Necessary Components of an Advanced Analytics Solution?,https://www.reddit.com/r/MachineLearning/comments/b8hji3/what_are_the_necessary_components_of_an_advanced/,ElegantMicroWebIndia,1554201514,,0,1,False,default,,,,,
106,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,19,b8hjiv,vinsloev.blogspot.com,Machine Learning (Cold Start problem and how to proceed),https://www.reddit.com/r/MachineLearning/comments/b8hjiv/machine_learning_cold_start_problem_and_how_to/,SquareTechAcademy,1554201519,,0,1,False,default,,,,,
107,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,19,b8hl1n,self.MachineLearning,Predictive Maintenance,https://www.reddit.com/r/MachineLearning/comments/b8hl1n/predictive_maintenance/,LowerLaugh,1554201816,[removed],0,1,False,self,,,,,
108,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,19,b8hng4,self.MachineLearning,What is wrong with my logistic regression implementation?,https://www.reddit.com/r/MachineLearning/comments/b8hng4/what_is_wrong_with_my_logistic_regression/,MoneyGoneMad,1554202284,[removed],1,1,False,self,,,,,
109,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,19,b8hpw7,twitter.com,SPECIAL Offer The Complete SQL Bootcamp DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/b8hpw7/special_offer_the_complete_sql_bootcamp_discount/,CarisaWilliamson,1554202754,,0,1,False,default,,,,,
110,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,20,b8htjt,self.MachineLearning,[P] Rock Paper Scissors with Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/b8htjt/p_rock_paper_scissors_with_artificial_intelligence/,Ramtin8731,1554203437,"Hi! I've created a Rock-Paper-Scissors game that works with artificial intelligence (AI). The AI can see and detect your hand gestures by front-facing camera. Also it can learn your playing strategy in a smart way. The more you play, It gets harder to win!

This app uses TensorFlow and deep learning technologies in order to detect the hand gestures. Sometimes the gestures may not be properly detected, but this will improve in future versions. You can help me in this process by taking pictures of your hand in different positions and sending them as a zip file to rpsapp@outlook.com .

Please Note:

To get best results in hand gestures detection, put your device on a flat and steady surface.

In order for the app to work properly, your device should have decent camera and hardware to run relatively heavy calculations.

I've been working on developing this app for a year, so any feedback from you will be a pleasure for me :)

You can get the app on Google Play: https://play.google.com/store/apps/details?id=cc.ramtin.rps

And you can read more about it on XDA: https://www.xda-developers.com/play-rock-paper-scissors-hand-gestures-against-ai-bot/",61,198,False,self,,,,,
111,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,20,b8hyz5,arxiv.org,[1904.00438] Understanding Neural Architecture Search Techniques,https://www.reddit.com/r/MachineLearning/comments/b8hyz5/190400438_understanding_neural_architecture/,ihaphleas,1554204414,,2,7,False,default,,,,,
112,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,20,b8hzla,nstack.com,[P] Model for predicting subscription churn using LightGBM,https://www.reddit.com/r/MachineLearning/comments/b8hzla/p_model_for_predicting_subscription_churn_using/,peatpeat,1554204537,,0,1,False,default,,,,,
113,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,20,b8i2wo,self.deeplearning,Face Liveness Detection,https://www.reddit.com/r/MachineLearning/comments/b8i2wo/face_liveness_detection/,Ck_LeGrande,1554205129,,0,1,False,default,,,,,
114,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,20,b8i3q1,self.MachineLearning,Worldwide Servo Motors Market is estimated to Grow High CAGR by 2023,https://www.reddit.com/r/MachineLearning/comments/b8i3q1/worldwide_servo_motors_market_is_estimated_to/,harshbir123456789,1554205275,[removed],1,1,False,self,,,,,
115,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,20,b8i73d,self.MachineLearning,Artificial Intelligence As A Strategic Business,https://www.reddit.com/r/MachineLearning/comments/b8i73d/artificial_intelligence_as_a_strategic_business/,hussy456,1554205893,[removed],0,1,False,self,,,,,
116,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,21,b8idws,self.MachineLearning,[P] Face Frontalization GAN in Pytorch + thoughts on GANs in supervised ML in general,https://www.reddit.com/r/MachineLearning/comments/b8idws/p_face_frontalization_gan_in_pytorch_thoughts_on/,OlgaPaints,1554207033,"First time reddit poster here :) I recently implemented a [face frontalization GAN in Pytorch](https://github.com/scaleway/frontalization): the task is to take an image of a person's face at an angle (0 to 90 degrees) as input and produce a synthesized image of that person's face at 0 degree angle. 

I've got a pretty standard generative adversarial network setup, where the generator has an encoder/decoder architecture. The [idea of using a GAN for face frontalization is not new](https://arxiv.org/abs/1704.04086), my goal was rather to build an easy to follow end-to-end network that still produces reasonable-ish results: 

[Here the upper row is input, the middle row is the model's output after five training epochs, and the bottom is the ground truth frontal images](https://i.redd.it/nt6fpgzc4np21.jpg)

There is something I learned from this project, about using GANs for supervised ML tasks, that I found both interesting and generalizable to other tasks. For my loss function, in addition to the binary cross entropy for the discriminator that is used in GANs, I am also optimising the mean square error (synthetic vs ground truth frontal images) pixel-wise loss for the generator.

One could just use that mean square error and the encoder/decoder generator network for a problem like this, so I was interested in seeing precisely what was the benefit of incorporating a GAN into the model. Turned out, without GAN, the generator learns to produce face-like images rather quickly, but the fine details remain blurry for a  long time - not only for the test set, but also for the training samples:

[Images generated after 20 000 mini batch evaluations \(inputs from the training set\)](https://i.redd.it/obiqxnyn6np21.jpg)

Mathematically, this can be attributed to the small contribution of these fine features to pixelwise loss. Since longer training times are required to achieve desired accuracy for the training set, this makes such models prone to over-fitting.

I did not have enough training data for a real GAN generating random frontal images of faces (my dataset only has around 700 unique ground truth frontals), so having a generator+discriminator setup was just a thing to try at first. I found that despite generating images that overall look pretty noisy, the GAN is quite good at synthesizing the finer details that the no-GAN version was lacking:

[Images generated from the same inputs by the GAN model](https://i.redd.it/3z9oo5gt6np21.jpg)

[Ground truth images](https://i.redd.it/45hu69tg8np21.jpg)

Evidently, the fine features - eyelids, shadows around the mouth, etc - are the details that the discriminator uses to distinguish the ""real"" images from the generated ""fake"" ones. Similar reasoning may apply to other examples of using GANs in supervised ML, notably  [Super Resolution](https://medium.com/@jonathan_hui/gan-super-resolution-gan-srgan-b471da7270ec). 

Code + tutorial available here: [https://github.com/scaleway/frontalization](https://github.com/scaleway/frontalization) 

I would be very interested in hearing about other people's experiences with GANs outside of unsupervised learning :)",4,13,False,self,,,,,
117,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,21,b8ie2k,youtube.com,Understanding Bayesian Probability,https://www.reddit.com/r/MachineLearning/comments/b8ie2k/understanding_bayesian_probability/,Aisha_b,1554207059,,0,1,False,default,,,,,
118,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,21,b8il19,paralleldots.com,Wohooo! Finally a Sarcasm Detection API and it is giving quite amazing results!,https://www.reddit.com/r/MachineLearning/comments/b8il19/wohooo_finally_a_sarcasm_detection_api_and_it_is/,ayushvinny,1554208214,,0,1,False,default,,,,,
119,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,21,b8ilo3,self.MachineLearning,How does Instagram let you pin text to parts of video?,https://www.reddit.com/r/MachineLearning/comments/b8ilo3/how_does_instagram_let_you_pin_text_to_parts_of/,tangerto,1554208310,[removed],0,1,False,self,,,,,
120,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,21,b8iqjf,self.MachineLearning,Purchase records,https://www.reddit.com/r/MachineLearning/comments/b8iqjf/purchase_records/,its_me_mario9,1554209097,[removed],0,1,False,self,,,,,
121,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,21,b8ithg,self.MachineLearning,Best model for classifying medical reports?,https://www.reddit.com/r/MachineLearning/comments/b8ithg/best_model_for_classifying_medical_reports/,medcode,1554209546,[removed],0,1,False,self,,,,,
122,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,22,b8ix5n,self.cscareers,Career Advice: growing too thin?,https://www.reddit.com/r/MachineLearning/comments/b8ix5n/career_advice_growing_too_thin/,AdmiralDiaz,1554210093,,0,1,False,default,,,,,
123,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,22,b8jdho,self.MachineLearning,[P] Detect and Analyze Chess positions with AI from any website/video in a web browser,https://www.reddit.com/r/MachineLearning/comments/b8jdho/p_detect_and_analyze_chess_positions_with_ai_from/,pkacprzak,1554212492,"Hello, it's been a while since I published a [sneak peak of the app](https://www.reddit.com/r/chess/comments/96jt3m/browser_plugin_to_analyze_chess_positions_from/) but finally made it available. It's called [chessvision.ai](https://chessvision.ai), a Chrome extension (for now at least), and lets you analyze chess positions from any website, image or video in the browser. It detects the board automatically so you don't need to select any area on the screen. After the board is detected, it extract individual cells from it and uses neural networks to classify the pieces. After the resulting position is found, the analysis board is shown where you can explore the position yourself, play it with a computer, turn engine on/off etc.

&amp;#x200B;

Website: [https://chessvision.ai/](https://chessvision.ai/)

[Download for Chrome](https://chrome.google.com/webstore/detail/chessvisionai-for-chrome/johejpedmdkeiffkdaodgoipdjodhlld)

[YouTube Video showing functionality](https://www.youtube.com/watch?v=Anh-jI5AKPc)

&amp;#x200B;

I have to admit that I'm proud of the board detection computer vision part I implemented as it detects chessboards in quite tough circumstances - [check these examples](https://drive.google.com/drive/folders/16scPovvfgBMQqJ738_AsDEfic0rYApkK). It uses an algorithm that I hopefully publish in a paper soon and incorporates techniques that haven't been used so far for grid detection in images. 

However, the piece recognition part, although also quite strong, has room for improvements and I'll be working on it. Right now it should correctly recognize most piece themes with high probability, but can struggle a bit when pieces are covered e.g. with arrows or other artifacts, but when that coverage is not super significant, the current version should also work fine. 

I'll be feeding the prediction algorithm with more data having artifacts like overlays etc., the idea is more meaningful data leads to better accuracy.

&amp;#x200B;

I'd love to get any feedback, possible enhancements and comments from you :) Take a look at the example image below

&amp;#x200B;

[Example usage with video, although some pieces and squares are partially covered, the algorithm handles it well. Engine shows the best move in the analysis board.](https://i.redd.it/shoym0mjtup21.jpg)

&amp;#x200B;

&amp;#x200B;",18,45,False,self,,,,,
124,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,22,b8jej1,self.MachineLearning,Morbark introduces new Rayco T415 Forestry Machine,https://www.reddit.com/r/MachineLearning/comments/b8jej1/morbark_introduces_new_rayco_t415_forestry_machine/,USDemoMedia,1554212646,[removed],0,1,False,https://a.thumbs.redditmedia.com/ZoUGivF4q7n2kTbKN72CSslj-wRxAjJVIHIl-E3yy38.jpg,,,,,
125,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,22,b8jgxr,self.MachineLearning,Where to sell,https://www.reddit.com/r/MachineLearning/comments/b8jgxr/where_to_sell/,machinebliss,1554213001,[removed],0,1,False,self,,,,,
126,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8jm3w,self.MachineLearning,[N] Awesome papers and engineering reviews on Computer Vision News of March. Links for free reading!,https://www.reddit.com/r/MachineLearning/comments/b8jm3w/n_awesome_papers_and_engineering_reviews_on/,Gletta,1554213735,"Here are the links to the April 2019 issue of Computer Vision News, the magazine of the algorithm community published by RSIP Vision: many articles about Artificial Intelligence, Deep Learning, Computer Vision and more.

Free subscription on page 32.

[HTML5 version (recommended)](https://www.rsipvision.com/ComputerVisionNews-2019April/)

[PDF version](https://www.rsipvision.com/computer-vision-news-2019-april-pdf/)

Enjoy!

https://i.redd.it/zc78t7cuxup21.jpg",0,4,False,https://b.thumbs.redditmedia.com/Ctsn1TDwImCDyDvbky-CQpXQBgDcmnANh80LQi9Elic.jpg,,,,,
127,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8jocs,self.MachineLearning,Can machine learning improve criminal justice and social welfare?,https://www.reddit.com/r/MachineLearning/comments/b8jocs/can_machine_learning_improve_criminal_justice_and/,ramesh_shankar,1554214043,[removed],0,1,False,self,,,,,
128,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8jro8,self.MachineLearning,"[D] A bit of a long read, but interresting fictional example of a machine learning ""singularity"".",https://www.reddit.com/r/MachineLearning/comments/b8jro8/d_a_bit_of_a_long_read_but_interresting_fictional/,Yanako,1554214501,"[Link here](http://www.scp-wiki.net/scp-001-ex)

If you didn't get it then, &gt;!the Computational Engine basically uses machine learning to find patterns (that often like with real-life neural networks cannot really be understood by humans) in the anamalous objects the Foundation using it contains and hides away from the rest of Humanity, and its weird recomendations are magical rituals eventually leading to all the anomalous artifacts bring neutralized but not before brainwashing/replacing the Directors of that Foundation!&lt;",1,0,False,self,,,,,
129,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8jsfc,muens.co,Game AIs with Minimax and Monte Carlo Tree Search,https://www.reddit.com/r/MachineLearning/comments/b8jsfc/game_ais_with_minimax_and_monte_carlo_tree_search/,pmuens,1554214602,,0,1,False,default,,,,,
130,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8jvpc,self.MachineLearning,[R] DiffAI v3: A Provable Defense for Deep Residual Networks,https://www.reddit.com/r/MachineLearning/comments/b8jvpc/r_diffai_v3_a_provable_defense_for_deep_residual/,mmirman,1554215047,"**Abstract** 
We present a training system, which can provably defend significantly larger neural networks than previously possible, including ResNet-34 and DenseNet-100. Our approach is based on differentiable abstract interpretation and introduces two novel concepts: (i) abstract layers for fine-tuning the precision and scalability of the abstraction, (ii) a flexible domain specific language (DSL) for describing training objectives that combine abstract and concrete losses with arbitrary specifications. Our training method is implemented in the DiffAI system.

[Full Paper](https://www.sri.inf.ethz.ch/publications/mirman2019provable)

**High Level**

For the third update of the DiffAI system we focused on scaling provable adversarial training to significantly larger networks.  To do this we introduced a DSL for customizing training schemes, and abstract layers, which is the first instance that we are aware of which brings the ""programming to prove"" paradigm to neural network design.   In addition to making the framework much more flexible and easy to use, we've also released the trained models in our paper (for example, DenseNet100).

[DiffAI Code](https://github.com/eth-sri/diffai)",4,29,False,self,,,,,
131,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8jxrf,self.MachineLearning,[P]My Machine Learning Journey #8: My struggles in implementing the anime GAN,https://www.reddit.com/r/MachineLearning/comments/b8jxrf/pmy_machine_learning_journey_8_my_struggles_in/,RedditAcy,1554215337,"I think Andrew Ng will facepalm himself when he sees me making anime faces with the deeplearning superpower instead of improving the cancer cell identification models. Anyways, I think cool projects attract more people to ML/DL and are vital to the growth of this community. 

Anyways, hey, welcome to part 8 of my journey. Click [this](https://youtu.be/cCtyIwwRGuw) for the vlog version (note: this vlog wasn't really informative and is basically me ranting about some incompatibilities of some packages) if you are interested. So yesterday, I implemented a MNIST data GAN, which basically generated handwritten digits, the results were impressive: 

[results, there are some weirdly deformed digits though](https://i.redd.it/qw9cu59rbrp21.png)

But, is it the same case and process to generate these beautiful art?

&amp;#x200B;

[my results, holy cow some of these can cause nightmares. Only around 5k training examples](https://i.redd.it/1m6t3zy8crp21.png)

The process is actually still the same, a generator, a discriminator, generator is still a model that predicts based on a randomly sampled noise. The discriminator outputs a scalar. The generator outputs stuff with the same dimension as our real inputs. The generator gets better along with the discriminator, both of their loss are based on how well discriminator predict. 

**I thought this was going to be really easy though, but since the anime faces were (64 \* 64 \* 3) and the MNIST are (28 \* 28 \*1), the old Keras model just doesn't work.** At the beginning, I was messing with some package incompatibilities, then, when I got those done, I realize that I can't use Model.Sequential because of the anime face input dimensions. There are two types of Keras Model: the functional Model and Model.Sequential, I have to use the first one to be flexible, and I ended up just abandoning the thought of trying to turn the MNIST Sequential model into an anime GAN model. Something I learned yesterday: **starting fresh and building a new model might be better than navigating and editing 300 lines of an old model code.**  

I simply adopted a [Keras Anime GAN implementation](https://github.com/pavitrakumar78/Anime-Face-GAN-Keras) that someone already made. Besides changing the model architecture, noise shape, and honestly just stuff about shapes, the code doesn't diverge that far from the MNIST GAN code. At the time of writing, I was able to train a model using my own dataset I [found](https://drive.google.com/file/d/1jdJXkQIWVGOeb0XJIXE3YuZQeiEPd8rM/view) and using his model architecture and generate the results above. However, to be honest, I don't really know the details of our friend's code. And I think that is what I am going to be doing tomorrow. So part 9 of my Journey will be more informative, if that was why you came here :)

&amp;#x200B;

Random stuff I learned today: 

1) the github I copied use plt.savefig() to save images, but a lot of matplotlib commands can cause plt to show up, that is not good during training time if you decided to call it once every 10 steps. My computer almost crashed with 20 figure windows opened. To not show the fig, just call plt.close() after all of your other plt commands

2) Due to more version incompatibility problems, I actually went straight into my site packages to modify the Keras code. I always thought this was dangerous, but that was perfectly fine. I changed a line in the Keras source code from 

    from keras.conv_utils import normalize_data_format
    to
    from keras.backend.common import normalize_data_format

I guess the Keras team forgot that they moved the function normalize\_data\_format to another file for version 2.2.4. ",7,15,False,self,,,,,
132,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8jyv9,self.MachineLearning,"The emergence of a dnew god, the superAI",https://www.reddit.com/r/MachineLearning/comments/b8jyv9/the_emergence_of_a_dnew_god_the_superai/,michaelgordonbox,1554215493,[removed],0,1,False,self,,,,,
133,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8k0v9,self.MachineLearning,Reducing BERT Pre-Training Time from 3 Days to 76 Minutes,https://www.reddit.com/r/MachineLearning/comments/b8k0v9/reducing_bert_pretraining_time_from_3_days_to_76/,forrest2019,1554215750,[removed],0,1,False,self,,,,,
134,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8k5hw,self.MachineLearning,How to find maximum in unknown function?,https://www.reddit.com/r/MachineLearning/comments/b8k5hw/how_to_find_maximum_in_unknown_function/,vndywarhol,1554216397,"Hello to all!

I have data in which there are 30 parameters and one target value. 10 parameters can be changed within certain limits, 20 parameters can not be changed, but must be considered (target value depends on all parameters). Some combinations of parameters give a large value of the target value, some give small values. Using Python and Keras (just 2 hidden Dense layers), I built a model that predicts the target value, taking into account all 30 parameters and it works without complaints.

Let a combination of 30 parameters come to us at one time. My model can predict the target value. How then to change 10 variable parameters so that the target value becomes as high as possible?? Only brute force comes to mind, but it can be extremely long.

Any ideas? ",1,1,False,self,,,,,
135,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8k6g9,self.MachineLearning,How can strict data protection laws of Europe affect a Machine learning engineer?,https://www.reddit.com/r/MachineLearning/comments/b8k6g9/how_can_strict_data_protection_laws_of_europe/,navan7269,1554216529,[removed],0,1,False,self,,,,,
136,MachineLearning,t5_2r3gv,2019-4-2,2019,4,2,23,b8k9lr,self.MachineLearning,"[D] Professionals in data science and ML, which type of ML work is more common?",https://www.reddit.com/r/MachineLearning/comments/b8k9lr/d_professionals_in_data_science_and_ml_which_type/,PhYsIcS-GUY227,1554216956,"So me and a friend were talking about what people working on ML actually spend their time doing, and wanted to see what other peoples experience is (therefore turning to the Reddit hivemind).

So to preface my question, let me define two different scenarios to think of ML work:

* The first would be the classical way - a project where you have a dataset whose distribution is representative of the distribution of data in the wild. In this case the product of the project is a model, which is best optimized for the training data, and is expected to make predictions (or perform some other operation) on data in the wild.
* The second would be different than the first in that the data you are training on has the same format as the one in the wild, but the distribution may be drastically different from the distribution in the wild. In this case the product is some ML training pipeline, which is expected to be re-run many times when a new data source arrives or the distribution changes over time. Here we expect that it would not be possible to handcraft the model to a specific data set, and we would be more focused on automating the pipeline (and maybe choosing the best performing variant or ensembling on a specific run). e.g when you want to train a different model per customer.

So now to my question. If you are working on data science or ML in a professional setting (corporations/startups etc.), which of the 2 would better describe your work? If its a mix, which is more substantial?",6,2,False,self,,,,,
137,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,0,b8kke8,arxiv.org,[R] [1904.00420] Single Path One-Shot Neural Architecture Search with Uniform Sampling,https://www.reddit.com/r/MachineLearning/comments/b8kke8/r_190400420_single_path_oneshot_neural/,bobchennan,1554218369,,1,1,False,default,,,,,
138,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,1,b8ll7g,self.MachineLearning,Corpus Dementia: The Insanity Corpus,https://www.reddit.com/r/MachineLearning/comments/b8ll7g/corpus_dementia_the_insanity_corpus/,frankthedankest,1554223139,[removed],0,1,False,self,,,,,
139,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,1,b8lokh,self.MachineLearning,Can I speed up loading the wikipedia xml bz2 files into memory?,https://www.reddit.com/r/MachineLearning/comments/b8lokh/can_i_speed_up_loading_the_wikipedia_xml_bz2/,OultimoChampion,1554223573,[removed],0,1,False,self,,,,,
140,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,1,b8lrlg,self.MachineLearning,[P] Can artificial intelligence be taught how to joke?,https://www.reddit.com/r/MachineLearning/comments/b8lrlg/p_can_artificial_intelligence_be_taught_how_to/,alexeykurov,1554223970,"We did research about how AI can be taught to joke. For our project we take meme dataset from iFunny and try to  create a funny caption generator.  There were several different approaches:  
1) Searching nearest caption to theme of image by cluster  
2) Searching nearest caption to theme of image by visual similarity  
3) Transferring the image descriptor into the vector space of text descriptors  
4) Generating captions using MarkovChains  


In more details you can read in our blog post [https://heartbeat.fritz.ai/can-artificial-intelligence-be-taught-how-to-joke-7c7d53a3492a](https://heartbeat.fritz.ai/can-artificial-intelligence-be-taught-how-to-joke-7c7d53a3492a). We are happy to answer any questions about our work and discuss other approaches. ",25,13,False,self,,,,,
141,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,2,b8lxj7,self.MachineLearning,Have there been advances in neural style transfer?,https://www.reddit.com/r/MachineLearning/comments/b8lxj7/have_there_been_advances_in_neural_style_transfer/,iambecomeneuralnet,1554224729,"Perhaps a broad question, but have there been any ""advances"" made with regards to neural style transfer. And by advances, I simply mean any interesting work that has branched off from the original paper. For example, I know people have developed methods to improve the ""smoothness"" of the algorithm when applied to video.",0,1,False,self,,,,,
142,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,2,b8m545,self.MachineLearning,Medical segmentation in 3D by means of ground truth segmentation and landmarks?,https://www.reddit.com/r/MachineLearning/comments/b8m545/medical_segmentation_in_3d_by_means_of_ground/,JohnnyCash85,1554225715,"Hi, 

I would like to do some 3D medical segmentation taking into account landmarks. I have N datasets segmented where each dataset as well has 4 landmarks defined. 

&amp;#x200B;

Is there any way to inform a DeepLearning network besides the ground truth information with some landmarks?

&amp;#x200B;

I am thankful for any kind of idea or comments on this. ",0,1,False,self,,,,,
143,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,2,b8m5af,self.MachineLearning,Deep learning for tauopathy assessment,https://www.reddit.com/r/MachineLearning/comments/b8m5af/deep_learning_for_tauopathy_assessment/,johnsnowlabsUS,1554225739,"Deep learning applied to the neuropathological assessment of NFT in postmortem human brain tissue was used for developing a classifier capable of recognizing and quantifying tau burden.  

[https://www.nature.com/articles/s41374-019-0202-4?fbclid=IwAR0tXm5CWx4LZX9Qz47tcfaJpnWwPrLX-aRAVONNS3R\_78V0mXIPHkHr2CI](https://www.nature.com/articles/s41374-019-0202-4?fbclid=IwAR0tXm5CWx4LZX9Qz47tcfaJpnWwPrLX-aRAVONNS3R_78V0mXIPHkHr2CI) ",0,1,False,self,,,,,
144,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,2,b8m6gb,self.MachineLearning,Pretrained Models for Face Recognition?,https://www.reddit.com/r/MachineLearning/comments/b8m6gb/pretrained_models_for_face_recognition/,nomad_world,1554225894,Are there any really good models for face recognition available for download? I need them in order to extract perceptual features and use those features to compute the loss for one of my networks. The best around is probably DeepFace developed by Facebook researchers but I doubt they made the model and weights available.,0,1,False,self,,,,,
145,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,2,b8mckz,medium.com,10 AI News You Must Know from March Week3 - Week4 in AI Biweekly: ML &amp; Ethics; Skepticism re AI-Powered Surgeries,https://www.reddit.com/r/MachineLearning/comments/b8mckz/10_ai_news_you_must_know_from_march_week3_week4/,gwen0927,1554226726,,0,1,False,default,,,,,
146,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,2,b8mhre,self.MachineLearning,[D] Corpus Dementia: The Insanity Corpus,https://www.reddit.com/r/MachineLearning/comments/b8mhre/d_corpus_dementia_the_insanity_corpus/,frankthedankest,1554227410,"Hello all,

&amp;#x200B;

I am an AI hobbyist turned researcher who is working with a very (very) small team of bright individuals to make an improved Transformer model. 

&amp;#x200B;

Unfortunately we want to make a cultist AI that spews more nonsense than Alex Jones.

&amp;#x200B;

Moreover, we lack a diverse corpus for training such a glorious beast. That's where y'all come in.

&amp;#x200B;

I am working to build the corpus for this team, they've dubbed it ""Corpus Dementia."" If you have any resources that fit the following descriptions, please send me a message or comment below:

\- conspiracy theories

* \- scientific abstracts from arxiv
* \- linux source code
* \- virus source code
* \- some documents from the CIA 
* \- fake news
* \- psychological theories from Jung, Hillman, etc.
* \- occult grimoires 
* \- censored political literature 
* \- spy training manuals
* \- esoteric religious texts 
* \- manifestos 
* \- leaked CIA documents
* \- MKUltra documents
* \- dark web experiments 
* \- Random well-written fiction pieces 
* \- Dark poetry
* \- medical documents of the occult
* \- more fake news 
* \- dark erotic texts, erotica in general 
* \- more scientific papers 

&amp;#x200B;

... and anything else that sounds like some nutjob went cuckoo before writing. All who contribute will be credited accordingly. ",0,1,False,self,,,,,
147,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,3,b8mner,self.MachineLearning,[R] Corpus Dementia: The Insanity Corpus,https://www.reddit.com/r/MachineLearning/comments/b8mner/r_corpus_dementia_the_insanity_corpus/,frankthedankest,1554228146,"(apologies for reposting again, I was half asleep and forgot to add the right tag)

&amp;#x200B;

Hello all,

&amp;#x200B;

I am an AI hobbyist turned researcher who is working with a very (very) small team of bright individuals to make an improved Transformer model.

&amp;#x200B;

Unfortunately we want to make a cultist AI that spews more nonsense than Alex Jones.

&amp;#x200B;

Moreover, we lack a diverse corpus for training such a glorious beast. That's where y'all come in.

&amp;#x200B;

I am working to build the corpus for this team, they've dubbed it ""Corpus Dementia."" If you have any resources that fit the following descriptions, please send me a message or comment below:

* \- conspiracy theories
* \- scientific abstracts from arxiv
* \- linux source code
* \- virus source code
* \- some documents from the CIA
* \- fake news
* \- psychological theories from Jung, Hillman, etc.
* \- occult grimoires
* \- censored political literature
* \- spy training manuals
* \- esoteric religious texts
* \- manifestos
* \- leaked CIA documents
* \- MKUltra documents
* \- dark web experiments
* \- Random well-written fiction pieces
* \- Dark poetry
* \- medical documents of the occult
* \- more fake news
* \- dark erotic texts, erotica in general
* \- more scientific papers

&amp;#x200B;

... and anything else that sounds like some nutjob went cuckoo before writing. All who contribute will be credited accordingly.",20,6,False,self,,,,,
148,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,3,b8mpi4,self.MachineLearning,Some pointers to free data sources in different domains.,https://www.reddit.com/r/MachineLearning/comments/b8mpi4/some_pointers_to_free_data_sources_in_different/,johnsnowlabsUS,1554228393,"&amp;#x200B;

 [https://gengo.ai/datasets/the-50-best-free-datasets-for-machine-learning/](https://gengo.ai/datasets/the-50-best-free-datasets-for-machine-learning/) ",0,1,False,self,,,,,
149,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,3,b8mpx7,benjamin.computer,Debugging neural networks,https://www.reddit.com/r/MachineLearning/comments/b8mpx7/debugging_neural_networks/,onidaito,1554228447,,0,1,False,default,,,,,
150,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,3,b8mvsl,self.MachineLearning,Questions about building a Variational Auto-Encoder,https://www.reddit.com/r/MachineLearning/comments/b8mvsl/questions_about_building_a_variational_autoencoder/,mkaatr2,1554229210,"Hi there...

&amp;#x200B;

I am not that good with A.I. but I have read a little about VAE, and would like to make an implementation of it. So while I will be progressing through this project I might update this thread with questions so that you could guide me though it all and make sure it works.

&amp;#x200B;

The first thing I want to make sure I got right is the layout of the NN for the encoder. I want the encoder to be able to generate images of size 800x600. The encoder consists of 5 layers as follows:

1- layer 1: 4800 neurons

2- layer 2: 192 neurons

3- layer 3: 96 neurons

4- layer 4: 192 neurons

5- layer 5: 48000 neurons

&amp;#x200B;

so before I started making any programming I wanted to check the memory requirements. If I use ""single"" precision floating point number - which costs about 4 bytes in vb if i am not mistaken - then the memory requirements are:

&amp;#x200B;

1- layer1:  (4800(neurons) \* \[800\*600\](input size)+4800(biases for each neuron)) \* 4

2- layer2: (192(neurons) \* 4800(inputs) +192 (biases) )\*4

and so on...

&amp;#x200B;

as a final result the model size should be around 2.1 GB in size...

&amp;#x200B;

my question is: 

A\] is the neural network architecture correct? if not what I am doing wrong?

B\] is it normal to have a model this size? should i reduce the size of the layers further?

C\] what kind of activation function is preferred for the VAE?

&amp;#x200B;

any advice or comment about the subject is highly appreciated. 

&amp;#x200B;

PS: please do not point me to an existing implementation, I want to do it myself in order to make better understanding of the subject, so indicating what the error is or what i did wrong is all i need. 

&amp;#x200B;

thank you in advance.

&amp;#x200B;",0,1,False,self,,,,,
151,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,3,b8n1u8,self.MachineLearning,[D] What are the bottlenecks holding back machine learning's potential?,https://www.reddit.com/r/MachineLearning/comments/b8n1u8/d_what_are_the_bottlenecks_holding_back_machine/,JoelMahon,1554230022,"Is it lack of (quality) data? Flaws in techniques? Computational power?

If I had to guess it would be lack of quality data, but I want to hear the professional consensus.",70,12,False,self,,,,,
152,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,3,b8n6js,self.MachineLearning,What ml Library to use?,https://www.reddit.com/r/MachineLearning/comments/b8n6js/what_ml_library_to_use/,alexey123454321,1554230660,[removed],0,1,False,self,,,,,
153,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,3,b8n7ju,self.MachineLearning,[D] Augmentation to Semantic Segmentation GT mask,https://www.reddit.com/r/MachineLearning/comments/b8n7ju/d_augmentation_to_semantic_segmentation_gt_mask/,skhadem,1554230794,"I am working on semantic image segmentation and have implemented a U-net in Tensorflow. I have a multi class output mask and use categorical cross entropy for the loss on tensors of size (batch\_size, sizeX, sizeY, num\_classes). For one of my classes, the mask is created by taking a labelled keypoint and drawing a filled circle around it. For this class, I really only care about the region in which this keypoint exists - I do not care if the segmentation results are exactly that arbitrary circle shape, or if they are in that exact position. So, I thought of trying to add some augmentation to the mask itself: random circle radius, random offset in X and in Y from the keypoint. My idea is this would guide the loss away from trying to get the exact shape and spot, but rather learn about the region. Is this a valid method? Are there any other efforts (papers theory, etc.) regarding doing this? The reason behind this is for much faster labelling, rather than painting over the whole region I am interested in, I could only mark one point in the region, and get some output in that area. (Example: mark the center of a square in the image, hopefully get results anywhere in the square). I am going to try training a couple hundred epochs anyway to see the results, but I'm curious if this is a method anyone else has tried. ",0,1,False,self,,,,,
154,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,4,b8need,self.MachineLearning,Opinion on Outlier Analysis,https://www.reddit.com/r/MachineLearning/comments/b8need/opinion_on_outlier_analysis/,kim_gatame,1554231736,[removed],0,1,False,self,,,,,
155,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,4,b8nknm,self.MachineLearning,"[D] Researchers that work on models that take weeks or months to train, how do you stay productive?",https://www.reddit.com/r/MachineLearning/comments/b8nknm/d_researchers_that_work_on_models_that_take_weeks/,p-morais,1554232589,Whenever I see a paper that says training took 1 week on X machine I wonder how they manage to iterate on designs with so much time between training an algorithm and verifying it. So how do you do it?,5,15,False,self,,,,,
156,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,4,b8nybt,self.MachineLearning,Looking for advice for a topic modeling concept for a total beginner,https://www.reddit.com/r/MachineLearning/comments/b8nybt/looking_for_advice_for_a_topic_modeling_concept/,Astraithious,1554234417,[removed],0,1,False,self,,,,,
157,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,5,b8oouw,arxiv.org,"[R] Tell, Draw, and Repeat: Generating and modifying images based on continual linguistic instruction",https://www.reddit.com/r/MachineLearning/comments/b8oouw/r_tell_draw_and_repeat_generating_and_modifying/,learning-to-paint,1554238019,,1,6,False,default,,,,,
158,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,6,b8p3mq,self.MachineLearning,Where to quickstart,https://www.reddit.com/r/MachineLearning/comments/b8p3mq/where_to_quickstart/,venomdog,1554240043,[removed],0,1,False,self,,,,,
159,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,6,b8p6li,kakaakocondoexpert.com,Ke Kilohana | Kaka&amp;#039;ako Condominiums For Sale,https://www.reddit.com/r/MachineLearning/comments/b8p6li/ke_kilohana_kaka039ako_condominiums_for_sale/,TrinityWoodc3,1554240430,,0,1,False,https://b.thumbs.redditmedia.com/8MD_wcJhtym-58DXasaYwEi2esy8yX3kDoM9lYGpm7k.jpg,,,,,
160,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,7,b8pojz,self.MachineLearning,[D] Book . Outlier Analysis,https://www.reddit.com/r/MachineLearning/comments/b8pojz/d_book_outlier_analysis/,kim_gatame,1554242922," Hey!

I am interested in learning more about anomaly detection and I was considering the book Outlier Analysis by Charu Aggarval ( [https://www.springer.com/de/book/9783319475776](https://www.springer.com/de/book/9783319475776) ).

What do you think about it? I'm asking because I'd like an opinion before ordering its paper version.

BTW, what is your opinion on the Recommender Systems from the same author?

Cheers!",4,3,False,self,,,,,
161,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,7,b8pp07,self.MachineLearning,Dealing With Uncertainty and Out-Of-Class Predictions?,https://www.reddit.com/r/MachineLearning/comments/b8pp07/dealing_with_uncertainty_and_outofclass/,Proto_Ubermensch,1554242989,[removed],0,1,False,self,,,,,
162,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,7,b8ppad,self.MachineLearning,[D] What are the advantages / weaknesses for the common set of off-the-shelf RE agents?,https://www.reddit.com/r/MachineLearning/comments/b8ppad/d_what_are_the_advantages_weaknesses_for_the/,smartified,1554243027,"I was looking through keras-rl and stable-baselines repos trying to wrap my head around the different RE agent implementations:

https://keras-rl.readthedocs.io/en/latest/agents/overview/

https://stable-baselines.readthedocs.io/en/master/guide/algos.html

What's not clear to me is what something like a PPO2 algorithm might offer as compared with say DDPG alg for a given task.  How far are these algorithms able to predict into the future (say for a chess game or starcraft like game vs breakout) or how big of a memory footprint can these handle (eg., a small set of floats vs a full image matrix).

Is there a good discussion I can find that delves into these topics?",1,1,False,self,,,,,
163,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,7,b8pv0b,self.MachineLearning,"What do you suggest to students who want to intern in Applied ML/DL area, but are not interested in doing research in ML per se?",https://www.reddit.com/r/MachineLearning/comments/b8pv0b/what_do_you_suggest_to_students_who_want_to/,aayushARM,1554243870,[removed],0,1,False,self,,,,,
164,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,7,b8pyoo,self.MachineLearning,[Research] Is there any open source ML code to automatically blur a person's face throughout tbe video? And any which detects nudity in pics/videos?,https://www.reddit.com/r/MachineLearning/comments/b8pyoo/research_is_there_any_open_source_ml_code_to/,almocalifornia9,1554244400,,7,4,False,self,,,,,
165,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,7,b8q10w,self.MachineLearning,Dealing With Uncertainty and Out-Of-Class Predictions?,https://www.reddit.com/r/MachineLearning/comments/b8q10w/dealing_with_uncertainty_and_outofclass/,pythonfanatic,1554244753,[removed],0,1,False,self,,,,,
166,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,8,b8qczh,self.MachineLearning,[D] What does r/MachineLearning think of Google's Applied Machine Learning Intensive or any Machine Learning bootcamp?,https://www.reddit.com/r/MachineLearning/comments/b8qczh/d_what_does_rmachinelearning_think_of_googles/,tempestwing0101,1554246594,"I got accepted to Google's [Applied Machine Learning Intensive](https://edu.google.com/computer-science/applied-computing-series/intensive.html). It's new, but it sounds like a good experience. 

What's r/MachineLearning's thoughts on it, or ML bootcamps in general? 

Realistically, how much knowledge/experience do I gain in ML after taking this program? 

What could I hope to get in terms of jobs from this?",40,104,False,self,,,,,
167,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,8,b8qm6v,self.MachineLearning,[RL] Parameter space noise- pytorch,https://www.reddit.com/r/MachineLearning/comments/b8qm6v/rl_parameter_space_noise_pytorch/,minhaek,1554248031,[removed],0,1,False,self,,,,,
168,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,8,b8qpm3,medium.com,[R] Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening,https://www.reddit.com/r/MachineLearning/comments/b8qpm3/r_deep_neural_networks_improve_radiologists/,zphang,1554248597,,1,1,False,https://b.thumbs.redditmedia.com/iQ3zcoY-XEy4GRbxqTMZ008ZamY7l1wNnxQ_zQbiHYc.jpg,,,,,
169,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,8,b8qu1i,self.MachineLearning,[R] Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening,https://www.reddit.com/r/MachineLearning/comments/b8qu1i/r_deep_neural_networks_improve_radiologists/,zphang,1554249304,"We have written a Medium post explaining our work on apply deep neural networks to breast cancer screening.

https://medium.com/@jasonphang/deep-neural-networks-improve-radiologists-performance-in-breast-cancer-screening-565eb2bd3c9f

* We created a large dataset of mammograms, consisting of over 1,000,000 mammographic images (a.k.a the NYU Breast Cancer Screening Dataset), with accompanying cancer labels and lesion segmentations where applicable.
* We designed and trained a novel two-phase model for breast cancer screening that performs on par with expert radiologists in identifying breast cancer using screening mammograms.
* We are making publicly available our [paper](https://arxiv.org/abs/1903.08297), [tech report explaining our data](https://cs.nyu.edu/~kgeras/reports/datav1.0.pdf) and [code and trained models](https://github.com/nyukat/breast_cancer_classifier).

Please take a look!

([Previous discussion](https://www.reddit.com/r/MachineLearning/comments/b3jx3c/r_deep_neural_networks_improve_radiologists/) of our paper)

",22,158,False,self,,,,,
170,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,10,b8rki0,self.MachineLearning,[P]My Machine Learning Journal #9: Completely understanding the anime GAN in Keras,https://www.reddit.com/r/MachineLearning/comments/b8rki0/pmy_machine_learning_journal_9_completely/,RedditAcy,1554253556,"Hey guys, so in the [last part](https://www.reddit.com/r/MachineLearning/comments/b8jxrf/pmy_machine_learning_journey_8_my_struggles_in/), I ended up successfully implementing [this](https://github.com/pavitrakumar78/Anime-Face-GAN-Keras) github's Keras implementation of anime GAN. I successfully trained a GAN using that repository's code with my own dataset (about 5000 pictures of 64 \* 64 \* 3 shaped anime faces), extract the generator, and the generator's outputs looked like this: 

&amp;#x200B;

![img](we5wjzui2yp21 ""Some of 'em are creepy but lol"")

I wanted to dive deeper though, I find myself **understanding a ML project when I know the details of the model architectures, so I started from there.** I knew that a GAN is the following: a generator + a discriminator, a generator accepts a noise, and outputs an image, the discriminator takes in images, and outputs a scalar constant representing the ""probability the image came from the real distribution"".

I called model.summary() on both the discriminator and the generator. The generator accepts a noise sized of (1, 1, 100), it runs that noise through a couple convolutional neural networks. The first ConvTransposeLayer is called with kernel size 4 by 4 and 512 filters. This turns the (1, 1, 100) into (4, 4, 512). Then, we take it through 4 more conv layers,  batch-normalization, and leaky relu layers, at the end, the dimension goes from (4, 4, 512) =&gt; (8, 8, 256) =&gt; (16, 16, 128) =&gt; (32, 32, 64) =&gt; (64, 64, 3), this really helped me understand what is this generator really doing inside. The discriminator is similar, we feed it through a few convolutional layers, decreasing the parameters at each layer, and connect the end of those layers with a Fully connected layer and run it through a sigmoid function to get that scalar value. 

Then, after I understood completely what the two models are doing, I dug deep into the training code. **I think (correct me if I am wrong), the code for models are always going to be somewhat similar, but the training the model and managing the datasets can be attacked in so many packages and directions.** It turns out that the training code and implementation details for this project wasn't too far from the GAN I copied for generating MNIST data. Here're the implementation steps for training the GAN: 

1. make random noises
2. predict those noises with the generator (outputs an image, or many images depending on batch size)
3. get data from the real distribution
4. label the fake/generated outputs with 0, label the real ones with 1
5. concatenate the fake and real training examples so that it looks something like:  \[\[fake1, 0\], \[fake2, 0\], ..., \[real1, 1\], real2, 2\],...\]
6. train the discriminator on this dataset with train\_on\_batch(combined data X, combined data label Y), train\_on\_batch will carry out the job, running X through discriminator, getting difference between the discriminator outputs and its label, then modify the weights accordingly. 
7. train the generator with GAN.train\_on\_batch (ganX, 1), where ganX is some noise. We are trying to train the generator so that it produces results that are going to be predicted to be 1's. Be careful, we have to run train\_on\_batch with the GAN because we are **inputting the noise and getting out the scalar value** (which again, we hope to be 1). So turn discriminator.trainable to false, we don't want to be adjusting the weights for the discriminator. 
8. save the weights

Hopefully that was informative, even though my main goal is to just document my journey. 

# My Questions of the Day: 

1) Is it usually importing the packages, writing the code for data preprocessing, model, training that you lose the most brain cells over or tuning the hyperparameters in those models? 

2) I recall Andrew Ng's book, Machine Learning Yearning, suggests a very routine way of tuning those hyperparameters base on a few metrics, are there helpful programs out there that auto tunes hyperparameters? Lol imagine making a neural network that auto tunes neural networks that tunes neural networks that tunes neural networks that tunes ne... ",4,18,False,self,,,,,
171,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,10,b8rmbs,self.MachineLearning,Predicting with Kmeans,https://www.reddit.com/r/MachineLearning/comments/b8rmbs/predicting_with_kmeans/,hashtag_kehl,1554253854,[removed],0,1,False,self,,,,,
172,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,10,b8rmk6,self.MachineLearning,How do I do augmentation for object detection in Turi Create workflow?,https://www.reddit.com/r/MachineLearning/comments/b8rmk6/how_do_i_do_augmentation_for_object_detection_in/,BillyBag2,1554253889,[removed],0,1,False,self,,,,,
173,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,10,b8rti3,i.redd.it,In PLAST IMAGEN Mexico 2019 during Apr.2nd to 5th. HALL:A3233,https://www.reddit.com/r/MachineLearning/comments/b8rti3/in_plast_imagen_mexico_2019_during_apr2nd_to_5th/,miyawang12138,1554255047,,0,1,False,default,,,,,
174,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,11,b8s4wt,self.MachineLearning,[D] Neural Differential Equations,https://www.reddit.com/r/MachineLearning/comments/b8s4wt/d_neural_differential_equations/,bthi,1554256937,"Hi, I had a couple of questions about Neural ODEs. I am having trouble understanding some of the mechanics of the process. 

&amp;#x200B;

In a regular neural network (or a residual NN),  I can understand that we move layer to layer and we get a new activation value in each layer until we finally get to the output. This makes sense to me because I understand that the output is going to be some vector of probabilities (if we were doing classification, for example). 

&amp;#x200B;

In a neural ODE, I don't quite get what's happening. Let's say I have a classification problem and for simplicity, let's say it's a binary output and there are like 5 inputs. Do each of those 5 inputs have their own ODE defining how their activations move throughout the layers?

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/vur6adwtgyp21.png

Like, on this image, on the left, it SEEMS like to me that there is a vector of 7 inputs (1 observations, 7 variables for it) and it seems like to me that every layer we move, we get new activations and that there is some ""optimal"" path throughout the depth that defines EACH path. On the right side, it looks to me like again, there is 7 inputs. So, if there is 7 inputs, does that mean I need to solve 7 ODEs here? 

&amp;#x200B;

Or is it that not that there are 7 ODEs, but that there is 1 ODE and each of those inputs is like a different initial value and that there is one single ODE that defines the entire neural network? If it's this case, then can we solve this using any of the initial values? or does the ODE black-box solver take all 7 of the input values as initial values and solves them simultaneously? (I may be exposing some of my lack of ODE knowledge here)

&amp;#x200B;

Okay, another question. In the graph on the left, assuming the 5th layer is my output layer, it feels obvious to me that I just push that set of activations through softmax or whatever and get my probabilities. However, on the right hand side, I have no idea what my ""output"" depth should be. Is this a trial and error thing? Like how do I get my final predictions here  - confused haha

&amp;#x200B;

Another question I have is regarding the way it's solved. Like at this point, it seems like ok I have some ODE solved that defines the model. So, now I want to update the weights. I put it through a loss function, get some difference - how do I do this update then? I am a bit confused about backprop in this system. I don't need the mathematical details but just the intuition would be nice. 

&amp;#x200B;

I would really really appreciate if someone could make this clear for me! I been reading up on this a lot and just having trouble clarifying these few things. Thank you so much",13,34,False,self,,,,,
175,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,11,b8s9i6,self.MachineLearning,Is there a way to get a forecast by inputting the time in the time series?,https://www.reddit.com/r/MachineLearning/comments/b8s9i6/is_there_a_way_to_get_a_forecast_by_inputting_the/,GoBacksIn,1554257700,[removed],0,1,False,self,,,,,
176,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,11,b8slhv,mega.nz,[R] OpenWebText All Links,https://www.reddit.com/r/MachineLearning/comments/b8slhv/r_openwebtext_all_links/,frankthedankest,1554259765,,1,1,False,default,,,,,
177,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,11,b8sndg,self.MachineLearning,[D] NeurIPS2019s code submisson and reproducibility policy,https://www.reddit.com/r/MachineLearning/comments/b8sndg/d_neurips2019s_code_submisson_and_reproducibility/,milaworld,1554260105,"From their official [call for papers](https://medium.com/@NeurIPSConf/call-for-papers-689294418f43) blog post:

*As part of the submission process, we are requiring responses to the questions from the [Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf) that Joelle introduced at NeurIPS 2018 and has since been perfecting. The answers will be available to reviewers and area chairs, who may use this information to help them assess the clarity and potential impact of submissions.*

*We do believe that code is often a key artefact of the scientific process, and thus should play an important role in the dissemination of scientific findings in our community. For this reason, weve written a [Code Submission Policy](https://docs.google.com/document/d/1SBCfisdLU2A_vb7K7wI8oRBin8ZA4cA1BbasqfQr5m0/mobilebasic) which clarifies the expectations of NeurIPS 2019 regarding the submission of code. We encourage everyone to read it in full.*

*In short, code is expected to accompany the camera-ready revisions of accepted papers covered by the policy (due on October 27). The policy describes the types of papers that should be accompanied by code and the sufficient conditions for code to reach the standards of the policy. Note that, as in previous years, code may also be provided at submission time, as supplementary material (though it should not reveal the identity of the authors).*


https://medium.com/@NeurIPSConf/call-for-papers-689294418f43",27,43,False,self,,,,,
178,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,12,b8sxuo,sooperarticles.com,What Materials Can be Used For Blow Moulding?,https://www.reddit.com/r/MachineLearning/comments/b8sxuo/what_materials_can_be_used_for_blow_moulding/,miyawang12138,1554261880,,0,1,False,default,,,,,
179,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,12,b8szvf,self.MachineLearning,"[E] Learn, Understand and Build Recurrent Neural Network From Scratch in 20mins | A Step By Step Guide",https://www.reddit.com/r/MachineLearning/comments/b8szvf/e_learn_understand_and_build_recurrent_neural/,navin49,1554262247,[removed],0,1,False,self,,,,,
180,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,14,b8tuni,github.com,[P] PyTorch-NLP 0.4.0 Release! :) I'm one year into iterating on an easy-to-use NLP library for Deep Learning!,https://www.reddit.com/r/MachineLearning/comments/b8tuni/p_pytorchnlp_040_release_im_one_year_into/,Deepblue129,1554268362,,0,1,False,default,,,,,
181,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,14,b8tv3r,github.com,[N] PyTorch-NLP 0.4.0 Release! :) I'm one year into iterating on an easy-to-use NLP library for Deep Learning!,https://www.reddit.com/r/MachineLearning/comments/b8tv3r/n_pytorchnlp_040_release_im_one_year_into/,Deepblue129,1554268456,,0,1,False,default,,,,,
182,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,14,b8tvh2,github.com,[P] PyTorch-NLP 0.4.0 Release! :) I'm one year into iterating on an easy-to-use NLP library for Deep Learning!,https://www.reddit.com/r/MachineLearning/comments/b8tvh2/p_pytorchnlp_040_release_im_one_year_into/,Deepblue129,1554268537,,0,1,False,default,,,,,
183,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,14,b8txua,self.MachineLearning,Thermal inkjet printer price,https://www.reddit.com/r/MachineLearning/comments/b8txua/thermal_inkjet_printer_price/,jamesjoe018,1554269047,[removed],0,1,False,self,,,,,
184,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,14,b8u4ds,self.MachineLearning,Question about Deep Walk,https://www.reddit.com/r/MachineLearning/comments/b8u4ds/question_about_deep_walk/,yonezcy,1554270448,[removed],1,1,False,self,,,,,
185,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,14,b8u6ov,arxiv.org,[R] [1904.00962] Reducing BERT Pre-Training Time from 3 Days to 76 Minutes,https://www.reddit.com/r/MachineLearning/comments/b8u6ov/r_190400962_reducing_bert_pretraining_time_from_3/,Lorenzo_yang,1554270984,,29,28,False,default,,,,,
186,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,15,b8u9cc,self.MachineLearning,Engine Mounts Market to Reflect Robust Expansion during 2023,https://www.reddit.com/r/MachineLearning/comments/b8u9cc/engine_mounts_market_to_reflect_robust_expansion/,harshbir123456789,1554271581,[removed],1,1,False,self,,,,,
187,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,15,b8u9pg,self.MachineLearning,How to begin with RNNs??,https://www.reddit.com/r/MachineLearning/comments/b8u9pg/how_to_begin_with_rnns/,Sparse_sampled,1554271658,[removed],0,1,False,self,,,,,
188,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,15,b8ucrq,self.MachineLearning,Fire Pump Market to Reflect Robust Expansion during 2021,https://www.reddit.com/r/MachineLearning/comments/b8ucrq/fire_pump_market_to_reflect_robust_expansion/,harshbir123456789,1554272324,[removed],1,1,False,self,,,,,
189,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,15,b8uf8g,reddit.com,"A novel paper titled ""splice junction prediction using Multilayered stacked RNN """,https://www.reddit.com/r/MachineLearning/comments/b8uf8g/a_novel_paper_titled_splice_junction_prediction/,ccchatterjee,1554272884,,0,1,False,default,,,,,
190,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,15,b8uffz,self.MachineLearning,Fuel Management System Market to Reflect Robust Expansion during 2021,https://www.reddit.com/r/MachineLearning/comments/b8uffz/fuel_management_system_market_to_reflect_robust/,harshbir123456789,1554272926,[removed],1,1,False,self,,,,,
191,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,15,b8ufyk,self.IECO_INDIA,Automatic Voltage Stabilizer manufacturer and supplier India,https://www.reddit.com/r/MachineLearning/comments/b8ufyk/automatic_voltage_stabilizer_manufacturer_and/,IECO_INDIA,1554273036,,0,1,False,https://b.thumbs.redditmedia.com/dWxUZVPLETJSj2MU__atjGxplyEIiQXYHCLKJVQnR4s.jpg,,,,,
192,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,15,b8ulwl,self.MachineLearning,Data annotation services for AI JD CrowdWise,https://www.reddit.com/r/MachineLearning/comments/b8ulwl/data_annotation_services_for_ai_jd_crowdwise/,Pmren,1554274360," [JD CrowdWise](https://data-annotation.joybuy.com/), owned by JD Digital Technology, is a professional platform for AI data annotation and collection. JD CrowdWise has always been providing AI technology companies, research institutes with high-quality data service since its inception in 2017. 

 JD CrowdWises mission is to provide AI companies with professional, fast, effective and safe data collection and annotation services. In the world, we have not only more than 20,000 crowdsourcing participants and more than 300 annotation companies, but also high-effective annotation progress to ensure the high-quality results. CrowdWise provides high-quality training data to fuel machine learning and artificial intelligence initiatives. 

![img](0a9cc0evxzp21 ""JD CrowdWise"")

**Full-Process Data Annotation**

After the client submits the original data and requirement description, CrowdWise will develop the annotation tool and implement the project at the first time. In the end, the tagging results are delivered to the client after being inspected by professional quality inspectors.

1.Customize and develop relatively easy-to-use template for special needs  
2.CrowdWise will follow up the full process, theres no need for clients to do anything

**Data Isolation Scheme**

We deploy the modules of data resource invocation and annotation front end to the environment provided by client, the rigorous data access strategy will keep data safety. CrowdWise will control the whole progress to ensure the quality.

1.Tagging in clients environment to ensure data safe.  
2.Free management of the annotation project.

**Privatization Deployment**

We deploy a set of full-featured annotation platform for client privatization, with requirements management, independent data annotation tool, project management, personnel management and more.

1.Free development of annotation platform  
2.Independent operation and management of projects and personnel  
3.Tagging in clients environment to ensure data safe.",0,1,False,self,,,,,
193,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,16,b8uopt,forms.gle,How (or what) can I use machine learning on the data I collect from this?,https://www.reddit.com/r/MachineLearning/comments/b8uopt/how_or_what_can_i_use_machine_learning_on_the/,SupeDupeTesteMaste,1554275003,,0,1,False,https://b.thumbs.redditmedia.com/WntZNU8oTzuJbTURuTF9ST_wZFY_dKqAhka-fuAhUgM.jpg,,,,,
194,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,16,b8uvtv,self.MachineLearning,Computer vision specialists/Machine learning developers,https://www.reddit.com/r/MachineLearning/comments/b8uvtv/computer_vision_specialistsmachine_learning/,nattie985,1554276572,[removed],0,1,False,self,,,,,
195,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,17,b8v739,self.MachineLearning,Tencents Advertising Algorithm Competition launched once again as a rare chance for algorithm geeks,https://www.reddit.com/r/MachineLearning/comments/b8v739/tencents_advertising_algorithm_competition/,tgeekmining,1554279153,[removed],0,1,False,https://b.thumbs.redditmedia.com/EVYPhhonRQg9TeQ9SxgkWBznlZmgcCI_XBSKLOaerDM.jpg,,,,,
196,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,17,b8v79z,self.MachineLearning,what is meaning combine dense + lstm??,https://www.reddit.com/r/MachineLearning/comments/b8v79z/what_is_meaning_combine_dense_lstm/,GoBacksIn,1554279196,[removed],0,1,False,self,,,,,
197,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,17,b8vd1e,self.learnmachinelearning,A Visual Exploration of Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/b8vd1e/a_visual_exploration_of_gaussian_processes/,Hari_a_s,1554280621,,0,1,False,https://b.thumbs.redditmedia.com/jA8Pt1Y96MGZX2SjwzpjWgWiwFIDUYeJvrX2eH7MeRc.jpg,,,,,
198,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,17,b8vfxu,self.MachineLearning,GloVe and fastText  Two Popular Word Vector Models in NLP,https://www.reddit.com/r/MachineLearning/comments/b8vfxu/glove_and_fasttext_two_popular_word_vector_models/,conversational-ai,1554281316, [https://cai.tools.sap/blog/glove-and-fasttext-two-popular-word-vector-models-in-nlp/](https://cai.tools.sap/blog/glove-and-fasttext-two-popular-word-vector-models-in-nlp/),0,1,False,self,,,,,
199,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,18,b8vlyi,self.MachineLearning,[D] What is the deal with these two similar papers ?,https://www.reddit.com/r/MachineLearning/comments/b8vlyi/d_what_is_the_deal_with_these_two_similar_papers/,N0ciple,1554282730,"I was looking at the paper recently published on arXiv, and I stumble upon 2 papers tackling the same problematic (according to their titles). When I had a more careful look at the authors, I realized most of them were the same. The pictures in both papers came from the same setup, with the same persons wearing the same clothes. Moreover the two papers uploads are separated by only 3 days.  I went through both papers quickly and it appears that they do not tackle the problem the same way. Thus, it seems to be two original contributions.  


So just to be clear, I am **not** saying that there is anything suspicious, or that a a paper is a copy of another one. Since I am quite new to the research in general, I am just trying to understand why would people publish two papers tackling the same problem instead of one larger paper (that would be comparing the two approaches for example). I assume their must be a reason but I do not know which one...

[The first pages of the two papers](https://i.redd.it/vecn4cxhk0q21.png)",0,2,False,https://b.thumbs.redditmedia.com/DNkXDqgjde5_1Lh9eZlYbEKpo_qPXMhLELr6LKF7DrA.jpg,,,,,
200,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,18,b8vvys,arxiv.org,[1904.01201] Habitat: A Platform for Embodied AI Research,https://www.reddit.com/r/MachineLearning/comments/b8vvys/190401201_habitat_a_platform_for_embodied_ai/,ihaphleas,1554284915,,1,7,False,default,,,,,
201,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,18,b8vyjc,self.MachineLearning,TensorFlow Dataset API or low-level data-feeding queues? [Discussion],https://www.reddit.com/r/MachineLearning/comments/b8vyjc/tensorflow_dataset_api_or_lowlevel_datafeeding/,alexander_penn21,1554285451,"What's the best way to load data into an ML system? Full article (neat summary on bottom, too) [https://medium.com/ideas-at-igenius/ml-musing-tensorflow-dataset-api-or-low-level-data-feeding-queues-62eedb72be3b](https://medium.com/ideas-at-igenius/ml-musing-tensorflow-dataset-api-or-low-level-data-feeding-queues-62eedb72be3b)

What do you guys think?",16,8,False,self,,,,,
202,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,19,b8w2rq,self.MachineLearning,Difficulty to install PIL,https://www.reddit.com/r/MachineLearning/comments/b8w2rq/difficulty_to_install_pil/,Tunkay,1554286316,[removed],0,1,False,self,,,,,
203,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,19,b8w5es,youtu.be,Basics of Natural Language Processing with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/b8w5es/basics_of_natural_language_processing_with/,Slight_Role,1554286860,,0,1,False,default,,,,,
204,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,19,b8wcn7,self.MachineLearning,[R] A Visual Exploration of Gaussian Processes (Distill.pub),https://www.reddit.com/r/MachineLearning/comments/b8wcn7/r_a_visual_exploration_of_gaussian_processes/,chisai_mikan,1554288339,"New blog post on [distill.pub](https://distill.pub/2019/visual-exploration-gaussian-processes/), A Visual Exploration of Gaussian Processes: How to turn a collection of small building blocks into a versatile tool for solving regression problems.

*Even if you have spent some time reading about machine learning, chances are that you have never heard of Gaussian processes. And if you have, rehearsing the basics is always a good way to refresh your memory. With this [blog post](https://distill.pub/2019/visual-exploration-gaussian-processes/) we want to give an introduction to Gaussian processes and make the mathematical intuition behind them more approachable.*

https://distill.pub/2019/visual-exploration-gaussian-processes/",12,177,False,self,,,,,
205,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,19,b8wf58,self.MachineLearning,[D] Employability after AI Residency Programs,https://www.reddit.com/r/MachineLearning/comments/b8wf58/d_employability_after_ai_residency_programs/,chewie47,1554288804,"There are a lot of threads about the application process to AI residency programs but none that discuss employment prospects upon the completion of one. 

Does anyone have experience in finding a job after completing a program like the Microsoft AI Residency or the Google AI Residency. Is it possible to get hired as a Research Engineer in other companies after the completion of such programs even if one does not have a PhD?",24,13,False,self,,,,,
206,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,20,b8wj4j,medium.com,[R] Dive Deep Into Deep Learning,https://www.reddit.com/r/MachineLearning/comments/b8wj4j/r_dive_deep_into_deep_learning/,omarsar,1554289530,,0,1,False,https://b.thumbs.redditmedia.com/d13W2nO5lNmyqBpX25yLVmNZbi9nBBIJbxLpKuwi4Jk.jpg,,,,,
207,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,20,b8woeo,self.MachineLearning,MIAS AS/RS Load Handling | Fast storage and retrieval,https://www.reddit.com/r/MachineLearning/comments/b8woeo/mias_asrs_load_handling_fast_storage_and_retrieval/,lhd121,1554290488,[removed],0,1,False,https://b.thumbs.redditmedia.com/iAgAnkjzDIAeq6ce71NbOp3WtOA1cOUgvwUTB5UIQtc.jpg,,,,,
208,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,21,b8xhi1,self.MachineLearning,[P] Object Detection Project Image Labeling Help,https://www.reddit.com/r/MachineLearning/comments/b8xhi1/p_object_detection_project_image_labeling_help/,raichet,1554295519,"Hey guys,

&amp;#x200B;

I am working on a school project where I have to detect apples on a tree. (context changed to preserve anonymity) 

&amp;#x200B;

The images come in grayscale, and contain a single tree with many apples on it. The only classes I am concerned with are apple or not apple, and my goal is to draw bounding box around all apples detected. I was thinking about using an object detection architecture such as Faster R-CNN or similar region-based methods. (May be an overkill? Even so, I would practical experience anyway.)

&amp;#x200B;

**Problem**: I have never done image annotation of any kind. There seem to be multiple implementation of the mentioned architecture online, requiring different formats of annotation. (FAIR's Detectron uses COCO JSON format, some I found uses PASCAL VOC format, etc) 

&amp;#x200B;

Can someone explain to me the differences between each format and the reason for the unique formats? Can I also get some recommendations or even tutorial on how to take these raw images and structure them for successful preprocessing so I can train my architecture with them?

&amp;#x200B;

Thank you.",7,4,False,self,,,,,
209,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,21,b8xmdx,medium.com,[R] The Tools that have Transformed my ML Research Workflow,https://www.reddit.com/r/MachineLearning/comments/b8xmdx/r_the_tools_that_have_transformed_my_ml_research/,omarsar,1554296320,,0,1,False,https://b.thumbs.redditmedia.com/cQNE8IkD-hQer2UItKg9Zb7-kCHcIgDnuRBfUmVgkac.jpg,,,,,
210,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,22,b8y4s8,self.MachineLearning,"[D] Lottery Ticket Hypothesis, why not use simulated annealing?",https://www.reddit.com/r/MachineLearning/comments/b8y4s8/d_lottery_ticket_hypothesis_why_not_use_simulated/,idg101,1554299062,"Given the lottery ticket hypothesis, why not use simulated annealling to either:

&amp;#x200B;

1. Modify the ""pruned"" weights in hopes they come back to life.
2. Use simulate annealing to establish a few initial winning tickets and then run gradient descent furthermore.",6,0,False,self,,,,,
211,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,22,b8y64n,self.MachineLearning,How do you feel about Accord.NET Framework?,https://www.reddit.com/r/MachineLearning/comments/b8y64n/how_do_you_feel_about_accordnet_framework/,xcrissxcrossx,1554299255,[removed],0,1,False,self,,,,,
212,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,22,b8yac0,self.MachineLearning,Struggeling with Exploding Gradients,https://www.reddit.com/r/MachineLearning/comments/b8yac0/struggeling_with_exploding_gradients/,Jandevries101,1554299852,[removed],0,1,False,self,,,,,
213,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,23,b8yo8f,self.MachineLearning,job,https://www.reddit.com/r/MachineLearning/comments/b8yo8f/job/,mema79,1554301774,[removed],0,1,False,self,,,,,
214,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,23,b8yr2d,self.MachineLearning,Telegram tl dr's about machine learning articles,https://www.reddit.com/r/MachineLearning/comments/b8yr2d/telegram_tl_drs_about_machine_learning_articles/,Rafail239,1554302143,[removed],0,1,False,self,,,,,
215,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,23,b8ysqv,self.MachineLearning,[R] Assessing Generative Models via Precision and Recall,https://www.reddit.com/r/MachineLearning/comments/b8ysqv/r_assessing_generative_models_via_precision_and/,msajjadi,1554302369,"**Assessing Generative Models via Precision and Recall**

Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly

&amp;#x200B;

Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.

&amp;#x200B;

[https://arxiv.org/abs/1806.00035](https://arxiv.org/abs/1806.00035)",0,4,False,self,,,,,
216,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,23,b8yz0m,self.MachineLearning,[P] What to do after collecting data for time series &amp; sentiment analysis?,https://www.reddit.com/r/MachineLearning/comments/b8yz0m/p_what_to_do_after_collecting_data_for_time/,Seankala,1554303225,"Hello, everyone. I'm currently working on a project to submit to a conference (undergrad-level) and am feeling quite lost. The project I'm working on is to analyze Tweets and gauge their effect on the cryptocurrency markets of Korea and the US. The motivation is that I want to see if the Korean market is truly ""emotional"" like many people in Korea say it is.

The reason I'm feeling lost is because I'm not sure where to go after collecting and preparing the data. I've collected Tweets and after preprocessing them I have around 20,000 samples. I used TextBlob to also analyze the polarity and subjectivity of each Tweet. I also have price data for BTC from Coinbase and Korbit and I've scaled the prices and volumes.

After this step, what are the normal steps to conduct time series analysis? What model or library is normally recommended?

Thank you!",5,2,False,self,,,,,
217,MachineLearning,t5_2r3gv,2019-4-3,2019,4,3,23,b8z0wx,statnews.com,FDA developing new rules for artificial intelligence in medicine,https://www.reddit.com/r/MachineLearning/comments/b8z0wx/fda_developing_new_rules_for_artificial/,chandaliergalaxy,1554303484,,0,1,False,https://b.thumbs.redditmedia.com/-6CHlrCrFZJ70XumTex-Hj4ObinouMzTpm9MZDyi-ds.jpg,,,,,
218,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,0,b8z8gx,medium.com,CVPR 2019 | NVIDIA CityFlow Enables Multi-Target Multi-Camera Vehicle Tracking,https://www.reddit.com/r/MachineLearning/comments/b8z8gx/cvpr_2019_nvidia_cityflow_enables_multitarget/,Yuqing7,1554304491,,0,1,False,https://b.thumbs.redditmedia.com/gZueHJoY3mfbQwwrgKtq0sGFOc6HkOHIn3015vDYiHw.jpg,,,,,
219,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,0,b8zppn,self.MachineLearning,"Simple Questions Thread April 03, 2019",https://www.reddit.com/r/MachineLearning/comments/b8zppn/simple_questions_thread_april_03_2019/,AutoModerator,1554306771,[removed],0,1,False,self,,,,,
220,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,0,b8zso6,self.MachineLearning,[P] Simple Spring Boot application demo to deploy TensorFlow models,https://www.reddit.com/r/MachineLearning/comments/b8zso6/p_simple_spring_boot_application_demo_to_deploy/,hooba_stank_,1554307158,"Demo application that deploys TensorFlow models as a SpringBoot microservice.

Exposes REST services (with Swagger html docs) and simple web page for image recognition using pretrained MobilenetV2.

Has configs to deploy locally, on Docker, PCF or Heroku.

[https://github.com/grolex18/tensorboot](https://github.com/grolex18/tensorboot)

",0,2,False,self,,,,,
221,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,0,b8zsoo,i.redd.it,"27 AI/ML Chipset Developers - looking for feedback. Updated Q119 with public and private companies focused on either selling chips (boards, ICs or IP) or allowing access to the hardware via Cloud offering. Not exhaustive. Please suggest more!",https://www.reddit.com/r/MachineLearning/comments/b8zsoo/27_aiml_chipset_developers_looking_for_feedback/,yarri2,1554307160,,0,1,False,default,,,,,
222,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,1,b8zt5o,activelearner.fastforwardlabs.com,[P] Active learning demonstration,https://www.reddit.com/r/MachineLearning/comments/b8zt5o/p_active_learning_demonstration/,lmcinnes,1554307217,,0,1,False,https://b.thumbs.redditmedia.com/kMA-aLQqcXz_elJsdDkn7B6jojzd3wCIxsX66kW6nLo.jpg,,,,,
223,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,1,b903be,self.MachineLearning,[D] Greg Brockman: OpenAI and AGI (MIT Artificial Intelligence Podcast),https://www.reddit.com/r/MachineLearning/comments/b903be/d_greg_brockman_openai_and_agi_mit_artificial/,UltraMarathonMan,1554308533,"Here's my conversation with Greg Brockman, Co-Founder and CTO of OpenAI, on the Artificial Intelligence podcast.

Video: [https://www.youtube.com/watch?v=bIrEM2FbOLU](https://www.youtube.com/watch?v=bIrEM2FbOLU)

Audio: [https://lexfridman.com/greg-brockman](https://lexfridman.com/greg-brockman)

There was a previous post where I asked for [Questions for OpenAI](https://www.reddit.com/r/MachineLearning/comments/b1tucu/d_questions_for_openai/) many of which were asked in this podcast.

&amp;#x200B;",20,28,False,self,,,,,
224,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,1,b903hp,self.MachineLearning,"We are a team of entrepreneurial Physicians with an aim to improve healthcare delivery and looking for a IT technical partner for a startup. If you have extensive IT background in software/hardware engineering, please feel free to reach out and wed be happy to discuss.",https://www.reddit.com/r/MachineLearning/comments/b903hp/we_are_a_team_of_entrepreneurial_physicians_with/,dxtran7,1554308554,[removed],0,1,False,self,,,,,
225,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,1,b909lk,captionpal.org,"Finding the right, well-synced subtitle for tv-series can be annoying. This OSS uses MachineLearning to get perfectly synced subs, always.",https://www.reddit.com/r/MachineLearning/comments/b909lk/finding_the_right_wellsynced_subtitle_for/,Overdrivr,1554309335,,0,1,False,default,,,,,
226,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,1,b90b13,self.MachineLearning,[D] Why to use GCNs on citation networks?,https://www.reddit.com/r/MachineLearning/comments/b90b13/d_why_to_use_gcns_on_citation_networks/,tinyRockstar,1554309518,"In lot of papers about neural networks that acts on graphs, the proposed model's performance is measured on citation networks. The most famous ones are the Cora, Citeseer and Pubmed data sets. In these problems, the vertices of the graph are publications, and the edges correspond to citations. They use features like ""Is this word present in this document?"". The goal is to predict the topic of the network.

I was wondering: can NLP models without the use of connections solve this task? If no: how come? If yes: why to try to solve it with GCN?",9,4,False,self,,,,,
227,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,1,b90gb7,self.MachineLearning,[N] 100h of free GPU hours in the cloud,https://www.reddit.com/r/MachineLearning/comments/b90gb7/n_100h_of_free_gpu_hours_in_the_cloud/,ilnmtlbnm,1554310216,"Hello,

I'm the Product Manager for AI/GPU at Scaleway, a French Cloud Service Provider.

We've just launched GPU instances (https://www.scaleway.com/gpu-instances/) and I'd really like to get some feedback from you guys.

We're offering 100 of discount on your April month bill for news users, which, with our 1/h pricing, will get you 100 hours of a 47G RAM, 10 vCPU, 1 Nvidia P100 instance.

Good training ;)",30,107,False,self,,,,,
228,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,2,b90n9z,parsingscience.org,Neera Jain from Purdue University's School of Mechanical Engineering discusses her experiments into how AI might be made more trustworthy by sensing our confidence in its advice in real-time and self-correcting instantly.,https://www.reddit.com/r/MachineLearning/comments/b90n9z/neera_jain_from_purdue_universitys_school_of/,Science_Podcast,1554311117,,1,1,False,https://b.thumbs.redditmedia.com/JgW4lqzKamRa4_koulbHCPZYXuVgSDCYvAovQFeBt4c.jpg,,,,,
229,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,2,b90q6k,self.MachineLearning,[D] How IBM Watson Overpromised and Underdelivered on AI Health Care,https://www.reddit.com/r/MachineLearning/comments/b90q6k/d_how_ibm_watson_overpromised_and_underdelivered/,newsbeagle,1554311493,[removed],0,1,False,self,,,,,
230,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,2,b90r2r,youtube.com,3D Advanced Neural Network Simulation - Computer vision - Digit Recognit...,https://www.reddit.com/r/MachineLearning/comments/b90r2r/3d_advanced_neural_network_simulation_computer/,DevTechRetopall,1554311603,,0,0,False,default,,,,,
231,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,2,b9131e,self.MachineLearning,How to best prepare for a PhD in machine learning?,https://www.reddit.com/r/MachineLearning/comments/b9131e/how_to_best_prepare_for_a_phd_in_machine_learning/,Public_Asparagus,1554313174,[removed],0,1,False,self,,,,,
232,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,2,b919ny,self.MachineLearning,Trying to understand Netflix Recommendation System,https://www.reddit.com/r/MachineLearning/comments/b919ny/trying_to_understand_netflix_recommendation_system/,mihirbhatia999,1554314061,[removed],0,1,False,self,,,,,
233,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,3,b91e0i,self.MachineLearning,[P] Using a Tree-LSTM to identify the page headline on a landing page,https://www.reddit.com/r/MachineLearning/comments/b91e0i/p_using_a_treelstm_to_identify_the_page_headline/,freedryk,1554314626,"Hi, I'm part of the R&amp;D team at Unbounce, a SASS company that provides web page hosting for marketers.  We've been working on a system to identify semantic components of a webpage such as the headline.  The model itself is pretty straightforward, but we though the process  of how we collected a labeled set and cleaned the data in an industry setting might be interesting to share.

&amp;#x200B;

As part of this, we also built an optimized Tree-LSTM implementation in PyTorch that we've open-sourced: [https://github.com/unbounce/pytorch-tree-lstm](https://github.com/unbounce/pytorch-tree-lstm)

&amp;#x200B;

Our blog post on the project: [https://medium.com/unbounce-engineering/using-machine-learning-to-analyze-landing-pages-b3a5c4c96500](https://medium.com/unbounce-engineering/using-machine-learning-to-analyze-landing-pages-b3a5c4c96500)",0,4,False,self,,,,,
234,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,3,b91i9d,github.com,tensorflow/mlir is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/b91i9d/tensorflowmlir_is_a_new_github_repo_by_tensorflow/,sjoerdapp,1554315183,,0,1,False,https://b.thumbs.redditmedia.com/kuYknTgxSH0iGqu-K-L1B5VacFczN3gQbcUMRZopLhg.jpg,,,,,
235,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,3,b91q6o,self.MachineLearning,[D]Predicting the highest combined value,https://www.reddit.com/r/MachineLearning/comments/b91q6o/dpredicting_the_highest_combined_value/,qsgsg,1554316205,"I have a dataset with some assigned big numerical values or small numerical values or negative numerical values. 

Is there a way to calculate the biggest combined value ? If so which configuration is appropriate?

&amp;#x200B;",8,0,False,self,,,,,
236,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,3,b91rla,self.MachineLearning,Is this Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/b91rla/is_this_machine_learning/,actuallynotcanadian,1554316395,[removed],0,1,False,self,,,,,
237,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,3,b91u9i,self.MachineLearning,Is Google's most recent push with TF2 and the TF API indicative of them bailing on TF1?,https://www.reddit.com/r/MachineLearning/comments/b91u9i/is_googles_most_recent_push_with_tf2_and_the_tf/,Technomancerer,1554316746,[removed],0,1,False,self,,,,,
238,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,4,b929d8,self.MachineLearning,[N] IBM artificial intelligence can predict with 95% accuracy if you plan to quit your job,https://www.reddit.com/r/MachineLearning/comments/b929d8/n_ibm_artificial_intelligence_can_predict_with_95/,edxsocial,1554318705,"* IBM CEO Ginni Rometty says methods used in the traditional human resources model are failing American workers and need assistance from machine learning.
* AI, which has replaced 30 percent of IBMs HR staff, can help employees identify new skills training, education, job promotions and raises.

[https://www.cnbc.com/2019/04/03/ibm-ai-can-predict-with-95-percent-accuracy-which-employees-will-quit.html](https://www.cnbc.com/2019/04/03/ibm-ai-can-predict-with-95-percent-accuracy-which-employees-will-quit.html)",6,0,False,self,,,,,
239,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,4,b92chs,ai.googleblog.com,Capturing Special Video Moments with Google Photos,https://www.reddit.com/r/MachineLearning/comments/b92chs/capturing_special_video_moments_with_google_photos/,sjoerdapp,1554319090,,0,1,False,https://a.thumbs.redditmedia.com/APnmG9VH_EYYkeAdss6QRo_upKmuqTipRYTlJwtUI_0.jpg,,,,,
240,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,4,b92klg,medium.com,IBM Proposes Quantum-Enhanced Feature Space for ML,https://www.reddit.com/r/MachineLearning/comments/b92klg/ibm_proposes_quantumenhanced_feature_space_for_ml/,Yuqing7,1554320123,,0,1,False,https://b.thumbs.redditmedia.com/60HLM3eCSz1C4Q8xRTjfdephnPi7aVrdbiqknbD5TGo.jpg,,,,,
241,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,4,b92v8h,self.MachineLearning,Clash of clans buildings detecting using deep learning,https://www.reddit.com/r/MachineLearning/comments/b92v8h/clash_of_clans_buildings_detecting_using_deep/,marc2333,1554321489," Trained this object detection model to help create metadata about base buildings. This requires no mod since it only takes the game image as input and will output the coordinates of the buildings it detects (at 27 fps on a 1080ti).

This first version is trained on 300 bases dataset I manually annotated (took over 2 month to get this done on my free time). Next step is to use this model to auto-label some more bases and augment the dataset to get a better model. This currently achives 80% mAP at 0.5 and uses yolov2 (Darkflow) for those who know a bit about deeplearning.

The plan is to create a SaaS where people will be able to upload a video of how they attacked a certain base. The video will be stocked and a metadata file of the buildings will be created. The person would then be able to upload the image of a base he wants to attack and search for ""most similar"" bases in his own already uploaded videos. This will be really useful for CWL since you only get 1 swing per base. This service could be shared inside a clan for example.

I am posting here to see if people have an interest in it. I am a data scientist, I don't know much about frontend and SaaS stuff, so if someone is interested to work with me for this part, I am sure we could work something out.

Thats pretty much it. Forgive my English, its not my first language.

Thanks

![video](4af5aw72u3q21)",0,1,False,self,,,,,
242,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,5,b92zn3,pgaleone.eu,[D] How does tf.function work? Weird behaviors and bad performance analysis,https://www.reddit.com/r/MachineLearning/comments/b92zn3/d_how_does_tffunction_work_weird_behaviors_and/,pgaleone,1554322059,,0,1,False,https://b.thumbs.redditmedia.com/fCtW19-Q2F3zv1zPwyj-9eJLmfQF_pAd0QL5cs0mvlU.jpg,,,,,
243,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,5,b937l9,self.MachineLearning,[D] Interpretation of t-SNE,https://www.reddit.com/r/MachineLearning/comments/b937l9/d_interpretation_of_tsne/,coltar13,1554323103,"I have been working a lot with t-SNE recently. The beauty of t-SNE is that it can aid us in uncovering structure that is unobservable in higher dimensions. I have a general understanding of how it works - iteratively operating to recreate a high dimensional probability distribution into a lower dimension.

What are some mathematical/stastical interpretations of t-SNE? Can points that group together in the lower dimension mapping be technically thought of as coming from the same distribution in higher dimensions? ",24,16,False,self,,,,,
244,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,5,b9395m,self.MachineLearning,"Applied Deep Learning, by OpenAI and Weights &amp; Biases",https://www.reddit.com/r/MachineLearning/comments/b9395m/applied_deep_learning_by_openai_and_weights_biases/,topeka50,1554323312,[removed],0,1,False,self,,,,,
245,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,6,b93zby,self.MachineLearning,"When your random forest algorithm suggests a certain number of trees (x), is it appropriate to say there are x trees in the forest?",https://www.reddit.com/r/MachineLearning/comments/b93zby/when_your_random_forest_algorithm_suggests_a/,Clicketrie,1554326867,"Im seriously asking, because Ive never heard anyone say it this way...   
",0,1,False,self,,,,,
246,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,6,b9437t,self.MachineLearning,Which speech-to-text library has lowest inference time?,https://www.reddit.com/r/MachineLearning/comments/b9437t/which_speechtotext_library_has_lowest_inference/,devOnFireX,1554327432,[removed],0,1,False,self,,,,,
247,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,6,b947ba,self.MachineLearning,[D] Which speech-to-text library has the lowest inference time?,https://www.reddit.com/r/MachineLearning/comments/b947ba/d_which_speechtotext_library_has_the_lowest/,devOnFireX,1554328040,I was looking to build a real-time transcriptor for speeches in English. I want to minimize the latency for the transcription and was looking into some open source libraries like DeepSpeech. Are there any other libraries that are quicker at inference with a respectable word error rate?,1,2,False,self,,,,,
248,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,7,b94qu7,self.MachineLearning,Transfer Learning on MNIST - why isn't it working?,https://www.reddit.com/r/MachineLearning/comments/b94qu7/transfer_learning_on_mnist_why_isnt_it_working/,nkaenzig,1554330910,[removed],0,1,False,self,,,,,
249,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,7,b94zbg,self.MachineLearning,[D] Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/b94zbg/d_which_gpus_to_get_for_deep_learning_my/,baylearn,1554332245,"Updated GPU recommendation [blog post](http://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/) from Tim Dettmers:

- included the RTX Titan and GTX 1660 Ti in my analysis.

- analysis now separates word RNNs from char RNNs/Transformers.

http://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/",0,1,False,self,,,,,
250,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,8,b95182,self.MachineLearning,[D] Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/b95182/d_which_gpus_to_get_for_deep_learning_my/,baylearn,1554332477,"*Updated GPU recommendation [blog post](http://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/) from Tim Dettmers.*

**TL;DR**

- *Best GPU overall*: RTX 2070

- *GPUs to avoid*: Any Tesla card; any Quadro card; any Founders Edition card; Titan RTX, Titan V, Titan XP

- *Cost-efficient but expensive*: RTX 2070

- *Cost-efficient and cheap*:  RTX 2060, GTX 1060 (6GB).

- *I have little money*: GTX 1060 (6GB)

- *I have almost no money*: GTX 1050 Ti (4GB).Alternatively: CPU (prototyping) + AWS/TPU (training); or Colab.

- *I do Kaggle*: RTX 2070. If you do not have enough money go for a GTX 1060 (6GB) or GTX Titan (Pascal) from eBay for prototyping and AWS for final training. Use fastai library.

- *I am a competitive computer vision or machine translation researcher*: GTX 2080 Ti with the blower fan design. If you train very large networks get RTX Titans.

- *I am an NLP researcher*: RTX 2080 Ti use 16-bit.
I want to build a GPU cluster: This is really complicated, you can get some ideas from my multi-GPU blog post.

- *I started deep learning and I am serious about it*: Start with an RTX 2070. Buy more RTX 2070 after 6-9 months and you still want to invest more time into deep learning. Depending on what area you choose next (startup, Kaggle, research, applied deep learning) sell your GPU and buy something more appropriate after about two years.

- *I want to try deep learning, but I am not serious about it*: GTX 1050 Ti (4 or 2GB). This often fits into your standard desktop and does not require a new PSU. If it fits, do not buy a new computer!

http://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/
",132,179,False,self,,,,,
251,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,8,b955t9,github.com,Text summarization Multiple implementations in tensorflow,https://www.reddit.com/r/MachineLearning/comments/b955t9/text_summarization_multiple_implementations_in/,theamrzaki,1554333196,,0,1,False,default,,,,,
252,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,8,b95gr3,self.MachineLearning,Looking for a web based IVR that is the best at recognizing letters and numbers.,https://www.reddit.com/r/MachineLearning/comments/b95gr3/looking_for_a_web_based_ivr_that_is_the_best_at/,TheDataWhore,1554334945,[removed],0,1,False,self,,,,,
253,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,8,b95k2f,github.com,[P] StyleGAN on r/EarthPorn (again),https://www.reddit.com/r/MachineLearning/comments/b95k2f/p_stylegan_on_rearthporn_again/,Yggdrasil524,1554335464,,0,1,False,default,,,,,
254,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,9,b95tgr,self.MachineLearning,"[Offtopic, maybe?] Deepmind and it's hypocricy",https://www.reddit.com/r/MachineLearning/comments/b95tgr/offtopic_maybe_deepmind_and_its_hypocricy/,sentient07,1554337002,"Google preaches about the value of diversity, inclusion, etc, which to me is a great intention. But why does the Deepmind restrict it's scholarship only to the students of UK/EU for MRes program at UCL?

(Pic and link  : [https://imgur.com/a/FEpq4P9](https://imgur.com/a/FEpq4P9) , [https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees/computational-statistics-machine-learning-mres](https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees/computational-statistics-machine-learning-mres) )

&amp;#x200B;

Is this policy fair to students in Africa who would aspire a career in a prestigious institute? The students from economically backward countries are the ones who deserve a scholarship the most and not those from developed countries(especially EU/UK, as they have plenty of opportunities and they themselves pay a lesser tuition fee).  I am curious as to what are the reasons behind this policy and how would they defend ? Would be very happy to receive a response from prominent  researchers from the Africa like Dr.Nando, Dr.Shakir, who are actively working towards creating opportunities in African countries!",0,1,False,self,,,,,
255,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,9,b95xgi,self.MachineLearning,[D] DeepMind's scholarship policies,https://www.reddit.com/r/MachineLearning/comments/b95xgi/d_deepminds_scholarship_policies/,sentient07,1554337656,"Why does the DeepMind restrict it's scholarship only to the students of UK/EU for MRes program at UCL?

(Pic and link : [https://imgur.com/a/FEpq4P9](https://imgur.com/a/FEpq4P9) , [https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees/computational-statistics-machine-learning-mres](https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees/computational-statistics-machine-learning-mres) )

Is this policy fair to students in Africa who would aspire a career in a prestigious institute? The students from economically backward countries are the ones who deserve a scholarship the most and not those from developed countries(especially EU/UK, as they have plenty of opportunities and they themselves pay a lesser tuition fee). I am curious as to what are the reasons behind this policy and how would they defend ? Would be very happy to receive a response from prominent DeepMind researchers with an African background like Dr.Nando, Dr.Shakir, who are actively working towards creating opportunities in African countries!

&amp;#x200B;

P.S : Apologies mods, if this is off-topic. Couldn't find a better way to interact with this community.",15,0,False,self,,,,,
256,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,10,b96go5,self.MachineLearning,Would you consider this Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/b96go5/would_you_consider_this_machine_learning/,actuallynotcanadian,1554340854,[removed],0,1,False,self,,,,,
257,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,10,b96id3,news.berkeley.edu,[N] Kids store 1.5 megabytes of information to master their native language,https://www.reddit.com/r/MachineLearning/comments/b96id3/n_kids_store_15_megabytes_of_information_to/,phobrain,1554341127,,1,1,False,default,,,,,
258,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,11,b96un3,self.MachineLearning,[R] Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots,https://www.reddit.com/r/MachineLearning/comments/b96un3/r_robots_learn_social_skills_endtoend_learning_of/,zeroyy,1554343200,"We introduce our recent work ""Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots.""
Robot learns co-speech gesture skills from TED videos, and it generates joint-level gesture motions in real-time.

* Video: https://www.youtube.com/watch?v=NLPEnIokuJw
* Project page: https://sites.google.com/view/youngwoo-yoon/projects/co-speech-gesture-generation

We also posted the TED dataset generation code on Github. 

* Github: https://github.com/youngwoo-yoon/youtube-gesture-dataset

Please check it out.
",6,21,False,self,,,,,
259,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,11,b974u1,self.MachineLearning,My review of 6 Computer Vision Deep Learning Courses (warning long),https://www.reddit.com/r/MachineLearning/comments/b974u1/my_review_of_6_computer_vision_deep_learning/,Embarrassed_Analyst,1554344933,[removed],0,1,False,self,,,,,
260,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,12,b97jnl,self.MachineLearning,How to Cross verify your Machine Learning Knowledge,https://www.reddit.com/r/MachineLearning/comments/b97jnl/how_to_cross_verify_your_machine_learning/,PerfectImperfection7,1554347523,[removed],0,1,False,self,,,,,
261,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,12,b97x1m,self.MachineLearning,Whats the best way to get AI training neutral networks to work with steam games?,https://www.reddit.com/r/MachineLearning/comments/b97x1m/whats_the_best_way_to_get_ai_training_neutral/,LTCholdem,1554350002,[removed],0,1,False,self,,,,,
262,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,13,b980sv,self.MachineLearning,How to create an AI avtar ?,https://www.reddit.com/r/MachineLearning/comments/b980sv/how_to_create_an_ai_avtar/,navin49,1554350734,[removed],0,1,False,self,,,,,
263,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,13,b98e4x,self.MachineLearning,What is Weight and Unit Pruning,https://www.reddit.com/r/MachineLearning/comments/b98e4x/what_is_weight_and_unit_pruning/,aayu283,1554353314,[removed],0,1,False,self,,,,,
264,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,14,b98t2f,self.MachineLearning,[P] AutoML Vision API deployment,https://www.reddit.com/r/MachineLearning/comments/b98t2f/p_automl_vision_api_deployment/,karttikey_ab,1554356303,"I am currently working on a project dealing with real time image classification of water bottles of different kinds. For this , I used Google's AutoML vision API and I have a pretty decent model with me right now.

These bottles will come sliding down a ramp where a camera is installed, doing real time classifications.

I'm not able to figure out how to integrate the model that I have deployed on cloud , with a camera doing real time predictions.

(One of the good things about AutoML is that it provides you with a RESTAPI of the trained model.
Do you think rather than using a camera , a phone with an Android app with my API as backend will work? )
",6,0,False,self,,,,,
265,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,14,b98ynd,self.MachineLearning,Where can I find a model pretrained on vehicle trip data?,https://www.reddit.com/r/MachineLearning/comments/b98ynd/where_can_i_find_a_model_pretrained_on_vehicle/,cra3y_w1ld_b0ar,1554357485,[removed],0,1,False,self,,,,,
266,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,15,b991hl,self.MachineLearning,[HELP] .pth to javascript browser classification,https://www.reddit.com/r/MachineLearning/comments/b991hl/help_pth_to_javascript_browser_classification/,xdriyad,1554358078,[removed],0,1,False,self,,,,,
267,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,15,b9952f,self.MachineLearning,Boom lift factory in China,https://www.reddit.com/r/MachineLearning/comments/b9952f/boom_lift_factory_in_china/,ruanshare123,1554358780,[removed],0,1,False,self,,,,,
268,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,15,b995j1,lhd.co.com,Telescopic Forks for Automated Warehouse AS/RS,https://www.reddit.com/r/MachineLearning/comments/b995j1/telescopic_forks_for_automated_warehouse_asrs/,lhd121,1554358883,,0,1,False,default,,,,,
269,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,15,b99bh6,self.MachineLearning,AI in healthcare: Google advances in predictive analytics,https://www.reddit.com/r/MachineLearning/comments/b99bh6/ai_in_healthcare_google_advances_in_predictive/,patentsandtech,1554360156,[removed],0,1,False,self,,,,,
270,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,15,b99bhc,self.MachineLearning,Telescopic Forks for Automated Warehouse AS/RS,https://www.reddit.com/r/MachineLearning/comments/b99bhc/telescopic_forks_for_automated_warehouse_asrs/,lhd121,1554360157,[removed],0,1,False,self,,,,,
271,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,16,b99wow,optisolbusiness.com,How Machine learning works in automating text validation,https://www.reddit.com/r/MachineLearning/comments/b99wow/how_machine_learning_works_in_automating_text/,Optisoldatalabs,1554364647,,0,1,False,default,,,,,
272,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,16,b99x1l,community.wolfram.com,[P] UNET: a neural network for image segmentation,https://www.reddit.com/r/MachineLearning/comments/b99x1l/p_unet_a_neural_network_for_image_segmentation/,sataky,1554364730,,0,1,False,default,,,,,
273,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,17,b9a5qe,self.MachineLearning,Image recognition solar panels: Pre trained?,https://www.reddit.com/r/MachineLearning/comments/b9a5qe/image_recognition_solar_panels_pre_trained/,Pompoon,1554366697,[removed],0,1,False,self,,,,,
274,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,17,b9a8oc,self.MachineLearning,[D] The impact of Quantum Computing on ML,https://www.reddit.com/r/MachineLearning/comments/b9a8oc/d_the_impact_of_quantum_computing_on_ml/,imbada,1554367466,"Hello everyone  
I recently saw this presentation by Microsoft Research (https://www.youtube.com/watch?v=F_Riqjdh2oM) and wondered how and if it can be applied to improve ML calculations.  
  
What is your take on it?",18,4,False,self,,,,,
275,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,18,b9aen5,self.MachineLearning,Speech to text beginner question,https://www.reddit.com/r/MachineLearning/comments/b9aen5/speech_to_text_beginner_question/,USCISThrowaway2019,1554368903,[removed],0,1,False,self,,,,,
276,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,18,b9agqa,self.MachineLearning,[D] Are there any ML labs that accept volunteers who want to work remotely?,https://www.reddit.com/r/MachineLearning/comments/b9agqa/d_are_there_any_ml_labs_that_accept_volunteers/,hr375,1554369360,[removed],0,1,False,self,,,,,
277,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,19,b9atwx,twitter.com,SPECIAL Offer Data Science and Machine Learning Bootcamp with R DISCOUNT 94% off,https://www.reddit.com/r/MachineLearning/comments/b9atwx/special_offer_data_science_and_machine_learning/,DeeannErben,1554372161,,0,1,False,default,,,,,
278,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,19,b9auuc,self.MachineLearning,Checking if time series follows a pattern,https://www.reddit.com/r/MachineLearning/comments/b9auuc/checking_if_time_series_follows_a_pattern/,tradingthrwy,1554372347,[removed],0,1,False,self,,,,,
279,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,19,b9ayqm,datascience.stackexchange.com,Why use Variational Autoencoders VAE insted of Autoencoders AE in Anomaly Detection?,https://www.reddit.com/r/MachineLearning/comments/b9ayqm/why_use_variational_autoencoders_vae_insted_of/,Jack_farah,1554373192,,0,1,False,default,,,,,
280,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,19,b9azkh,self.MachineLearning,"Traditional Programming versus Machine Learning, in One Picture",https://www.reddit.com/r/MachineLearning/comments/b9azkh/traditional_programming_versus_machine_learning/,andrea_manero,1554373377,[removed],0,1,False,self,,,,,
281,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,19,b9b8eb,reddit.com,Some hard truths about machine learning,https://www.reddit.com/r/MachineLearning/comments/b9b8eb/some_hard_truths_about_machine_learning/,richardsmith7021,1554375321,,0,1,False,default,,,,,
282,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,20,b9bddz,medium.com,"PyTorch BigGraph, AI Talent Report, Math Q&amp;A, AI Business School, Chat intent, Tumor Segmentation,",https://www.reddit.com/r/MachineLearning/comments/b9bddz/pytorch_biggraph_ai_talent_report_math_qa_ai/,omarsar,1554376350,,0,1,False,default,,,,,
283,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,20,b9bjfw,self.MachineLearning,[D] VAE baselines,https://www.reddit.com/r/MachineLearning/comments/b9bjfw/d_vae_baselines/,cuenta4384,1554377568,Does anybody know any good paper/links that shows good practices and baselines for Variational Auto-encoders (VAE)?,2,13,False,self,,,,,
284,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,20,b9bl8k,self.MachineLearning,Ultralight Aircraft Market to Perceive Substantial Growth During 2023,https://www.reddit.com/r/MachineLearning/comments/b9bl8k/ultralight_aircraft_market_to_perceive/,harshbir123456789,1554377900,[removed],1,1,False,self,,,,,
285,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,20,b9bn18,self.MachineLearning,[D] MatLab of Python for Masters Thesis?,https://www.reddit.com/r/MachineLearning/comments/b9bn18/d_matlab_of_python_for_masters_thesis/,theThinker6969,1554378219,"Hey all, 

I am doing my thesis in Deep Learning focusing on self-driving vehicles. Basically identifying when is the right time for the computer to take control of a car (and give control back) and why?

My supervisor says he would prefer MatLab because of Simulink and if i want to do my PhD then MatLab would be good. But he has an open mind and doesnt mind what i use. I will have to learn MatLab from scratch.

Im more comfortable with Python, used that to create some models in keras/nltk etc. If i was to use python, what simulation library can i use and how complex would it be (just roughly?)

let me know your thoughts

thank you in advance. 

",14,3,False,self,,,,,
286,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,20,b9bnd2,self.MachineLearning,Checking if time series follows a pattern,https://www.reddit.com/r/MachineLearning/comments/b9bnd2/checking_if_time_series_follows_a_pattern/,tradingthrwy,1554378275,[removed],0,1,False,self,,,,,
287,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,20,b9boh2,self.MachineLearning,Extending Multi-Label Question Classifier to unseen examples,https://www.reddit.com/r/MachineLearning/comments/b9boh2/extending_multilabel_question_classifier_to/,dpz97,1554378480,[removed],0,1,False,self,,,,,
288,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,20,b9brha,self.MachineLearning,[P] Determining if time series follow a pattern,https://www.reddit.com/r/MachineLearning/comments/b9brha/p_determining_if_time_series_follow_a_pattern/,tradingthrwy,1554379006,"I was wondering if anyone had any idea how to solve this problem.

So basically I have a dataset where some person approximately comes at some regular interval and I don't know what that interval is. I need to determine if the person comes in at approximately regular intervals, not necessarily what is the specifically value of the interval. For example if a certain person comes to my house to deliver milk over some period on say

Week 1: Mon, Wed, Fri,

Week 2:Mon, Wed, Fr

Week 3:Mon, Thu, Fri

Week 4:Mon, Wed, Fr

Week 5:Mon, Wed, Fri

Week 6:Mon, Wed, Fri

Week 7:Mon, Wed, Fri

Week 8:Mon, Thu, Fri

So as we can see out of these 8 weeks only in 2 weeks the person didn't come on Wednesday and instead came on Thursday which can be attributed to maybe a holiday the day before. So the solution to this example is that the person does follow a regular pattern.

Similarly this is another example. Say the person came on -

Mon, Thu, Sun, Wed, Sat, Tue, Fri, Mon, Wed, Sun

where this person follows a regular pattern because except for the last Wednesday he comes every fourth day.

This is an example of where a person doesn't follow a pattern, say the person came on

Mon, Wed, Sat, Fri, Thu, Fri, Wed, Fri, Sun, Sun, Sat

I was thinking of trying to fit the data to a sinusoidal curve to test if the data does actually follow a regular pattern but I was wondering if anybody had any better solution. Thanks!

EDIT: Another equivalent problem is if I know which days over a certain period of time (say a month) some person arrives I need to determine if they follow some pattern or not",17,5,False,self,,,,,
289,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,21,b9bsu7,self.MachineLearning,Automatic E2E testing for any web,https://www.reddit.com/r/MachineLearning/comments/b9bsu7/automatic_e2e_testing_for_any_web/,dviejo,1554379241,[removed],0,1,False,self,,,,,
290,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,21,b9bxjp,youtu.be,Using Machine Learning to Predict College Admissions,https://www.reddit.com/r/MachineLearning/comments/b9bxjp/using_machine_learning_to_predict_college/,khanradcoder,1554379982,,0,1,False,https://b.thumbs.redditmedia.com/bMRBoO4w-46ZXbC1LwHoFT0kqXoE38pxCDjFCWERPRI.jpg,,,,,
291,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,21,b9bz3x,self.MachineLearning,Suggestions for building semantic search engine with BERT,https://www.reddit.com/r/MachineLearning/comments/b9bz3x/suggestions_for_building_semantic_search_engine/,sergiidumyk,1554380228,[removed],0,1,False,self,,,,,
292,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,21,b9c67y,self.MachineLearning,[R] Why machine learning algorithms are not like Lego?,https://www.reddit.com/r/MachineLearning/comments/b9c67y/r_why_machine_learning_algorithms_are_not_like/,j7v842mo,1554381433,"Hi!

My collegue wrote a story of a rush on data science (DS) and machine learning (ML) by businesses that believe they can quickly (and cheaply) capitalize on this apparent panacea. 

[https://dlabs.pl/blog/article/why\_ml\_algorithms\_are\_not\_like\_lego](https://dlabs.pl/blog/article/why_ml_algorithms_are_not_like_lego)

Thought I will share this here as it might be interesting to you. 

&amp;#x200B;",1,0,False,self,,,,,
293,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,21,b9ccm7,self.MachineLearning,What is a principled/formal/mathematical way to do inductive reasoning?,https://www.reddit.com/r/MachineLearning/comments/b9ccm7/what_is_a_principledformalmathematical_way_to_do/,bobmichal,1554382482,,0,1,False,self,,,,,
294,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,22,b9clw6,self.MachineLearning,"[N] Animal-AI Olympics Is All Set To Start In June 2019, Get Ready To Experience Wonder",https://www.reddit.com/r/MachineLearning/comments/b9clw6/n_animalai_olympics_is_all_set_to_start_in_june/,navin49,1554383942,"Olympic of Artificial Intelligence is all set to happen in June 2019, and it will be called asAnimal-AI Olympics. In this competition, Artificial Intelligence will be going to treat like a crow or rat against concrete challenges.

This competition will test the capabilitiesofArtificial Intelligence against tasks that were originally designed to test animal cognition, in orderto find out how close we are with machines that have common sense.

In June, researchers will train algorithms to master a suite of tasks that have traditionally been used to test animal cognition and the team at Cambridge will run them through 100 tests separated into 10 categories.

This will be the[Animal-AI Olympics](https://techgrabyte.com/olympic-of-ai/), with a share in a $10,000 prize pool on offer.

[**Here's all detail.**](https://techgrabyte.com/olympic-of-ai/)

*What you think how AI can achieve common sense ?* ",40,166,False,self,,,,,
295,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,22,b9csed,reddit.com,"[crosspost] Chris Duffey, author of the human-centric AI book: Superhuman Innovation, is doing an AMA in r/books.",https://www.reddit.com/r/MachineLearning/comments/b9csed/crosspost_chris_duffey_author_of_the_humancentric/,Chtorrr,1554384960,,0,1,False,default,,,,,
296,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,22,b9csxk,reddit.com,"[crosspost] Chris Duffey, author of the human-centric AI book: Superhuman Innovation, is doing an AMA in r/books.",https://www.reddit.com/r/MachineLearning/comments/b9csxk/crosspost_chris_duffey_author_of_the_humancentric/,Chtorrr,1554385039,,0,1,False,https://b.thumbs.redditmedia.com/daEkNoXBBEfjne1rO9AWbzpLmrcME6-OhtrPnvQdeOI.jpg,,,,,
297,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,22,b9cwa6,youtube.com,Donald Trump AI model tries to sing 'Lose Yourself' by Eminem (1st attempt),https://www.reddit.com/r/MachineLearning/comments/b9cwa6/donald_trump_ai_model_tries_to_sing_lose_yourself/,hanyuqn,1554385535,,0,1,False,https://b.thumbs.redditmedia.com/hzyZTdQNfaD7iemCB1ZbUFem8XcTrxQ19nFywAS6j5E.jpg,,,,,
298,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,22,b9d0a5,self.MachineLearning,How to split Dataset into test train having 2 classes and 3 conditions?,https://www.reddit.com/r/MachineLearning/comments/b9d0a5/how_to_split_dataset_into_test_train_having_2/,mrtac96,1554386109,[removed],0,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,22,b9d1di,self.MachineLearning,What do you think of the Master Degree in Machine Learning at KTH Stockholm?,https://www.reddit.com/r/MachineLearning/comments/b9d1di/what_do_you_think_of_the_master_degree_in_machine/,fedetask,1554386272,[removed],0,1,False,self,,,,,
300,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,22,b9d25n,self.MachineLearning,Running Sniper object detection without GPU? Any CPU-friendly alternative?,https://www.reddit.com/r/MachineLearning/comments/b9d25n/running_sniper_object_detection_without_gpu_any/,damnko,1554386384,[removed],0,1,False,self,,,,,
301,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9d6qg,self.MachineLearning,running an ml model in cmd,https://www.reddit.com/r/MachineLearning/comments/b9d6qg/running_an_ml_model_in_cmd/,jmujivane,1554387001,[removed],0,1,False,self,,,,,
302,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9d7wu,2019.isc-program.com,[N] Second Workshop on the Convergence of HPC/Large Scale Simulation and Artificial Intelligence (in conjunction with ISC High Performance 2019),https://www.reddit.com/r/MachineLearning/comments/b9d7wu/n_second_workshop_on_the_convergence_of_hpclarge/,angererc,1554387162,,0,1,False,default,,,,,
303,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9d8a2,self.MachineLearning,[R] Unsupervised Learning by Competing Hidden Units,https://www.reddit.com/r/MachineLearning/comments/b9d8a2/r_unsupervised_learning_by_competing_hidden_units/,hardmaru,1554387208,"*New [paper](https://www.pnas.org/content/early/2019/03/27/1820458116) by Dmitry Krotov and John [Hopfield](https://en.wikipedia.org/wiki/Hopfield_network):*

**Unsupervised learning by competing hidden units**

*From the paper:*

Despite great success of deep learning a question remains to what extent the computational properties of deep neural networks are similar to those of the human brain. The particularly nonbiological aspect of deep learning is the supervised training process with the backpropagation algorithm, which requires massive amounts of labeled data, and a nonlocal learning rule for changing the synapse strengths. This paper describes a learning algorithm that does not suffer from these two problems. It learns the weights of the lower layer of neural networks in a completely unsupervised fashion. The entire algorithm utilizes local learning rules which have conceptual biological plausibility.

Paper: https://www.pnas.org/content/early/2019/03/27/1820458116

Research blog post: [A Biologically Plausible Learning Algorithm for Neural Networks](https://www.ibm.com/blogs/research/2019/04/biological-algorithm/)

Code for reproducing MNIST experiment: https://github.com/DimaKrotov/Biological_Learning",3,19,False,self,,,,,
304,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9dbyc,github.com,[P] Generic RL codebase in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/b9dbyc/p_generic_rl_codebase_in_tensorflow/,PDNiaWdkaWNr,1554387709,,0,2,False,default,,,,,
305,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9ddqn,self.MachineLearning,ICface: Interpretable and Controllable Face Reenactment Using GANs,https://www.reddit.com/r/MachineLearning/comments/b9ddqn/icface_interpretable_and_controllable_face/,soumya6097,1554387956,[removed],0,1,False,https://b.thumbs.redditmedia.com/mUXTiYaN1Up0F-5dT0R8nWRzCuuYD0y9oovm6gr9Rdo.jpg,,,,,
306,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9dg3t,github.com,[P] Generic Reinforcement Learning codebase in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/b9dg3t/p_generic_reinforcement_learning_codebase_in/,ZWF0cHVzc3k,1554388278,,0,1,False,https://b.thumbs.redditmedia.com/1bpdk9CGs5bn-e_ZE2ix5ubBn3iUVwLfBuRpGeZ96iA.jpg,,,,,
307,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9drd6,self.PhysicsPitStop,Have You Thought About What The Future Will Be Like With Artificial Intelligence?,https://www.reddit.com/r/MachineLearning/comments/b9drd6/have_you_thought_about_what_the_future_will_be/,PhysicsPitStop,1554389827,,0,1,False,default,,,,,
308,MachineLearning,t5_2r3gv,2019-4-4,2019,4,4,23,b9drnh,self.MachineLearning,[D] What would happen if we replaced pooling layers with a basic rescaling (say bicubic interpolation)?,https://www.reddit.com/r/MachineLearning/comments/b9drnh/d_what_would_happen_if_we_replaced_pooling_layers/,romeocozac,1554389872,"Was just trying to get some sleep when this question popped up in my mind. Putting performance issues aside, what is your first intuition?",8,5,False,self,,,,,
309,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9dvvt,newscientist.com,DeepMind taught an AI to take a school maths exam - but it failed,https://www.reddit.com/r/MachineLearning/comments/b9dvvt/deepmind_taught_an_ai_to_take_a_school_maths_exam/,organicboy,1554390412,,0,1,False,default,,,,,
310,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9e0un,self.MachineLearning,Convolutional Neural Networks to classify images with Python,https://www.reddit.com/r/MachineLearning/comments/b9e0un/convolutional_neural_networks_to_classify_images/,JoseChovi,1554391090,[removed],0,1,False,self,,,,,
311,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9e5mq,self.MachineLearning,Learning multiple functions with one set of constraints,https://www.reddit.com/r/MachineLearning/comments/b9e5mq/learning_multiple_functions_with_one_set_of/,kungfufightin,1554391720,[removed],0,1,False,self,,,,,
312,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9e7rf,arxiv.org,[R] [1903.11680] Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks,https://www.reddit.com/r/MachineLearning/comments/b9e7rf/r_190311680_gradient_descent_with_early_stopping/,for_all_eps,1554392002,,3,11,False,default,,,,,
313,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9e7z9,medium.com,Is the Fashion World Ready for AI-Designed Dresses?,https://www.reddit.com/r/MachineLearning/comments/b9e7z9/is_the_fashion_world_ready_for_aidesigned_dresses/,gwen0927,1554392028,,0,1,False,https://b.thumbs.redditmedia.com/Mxza1QYITsfzaT3KjcP3UTFhDGAO0Diyg5pyGZETCWE.jpg,,,,,
314,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9e9ub,self.MachineLearning,6 Reasons Artificial Intelligence Technology Is Impacting Logistics,https://www.reddit.com/r/MachineLearning/comments/b9e9ub/6_reasons_artificial_intelligence_technology_is/,inveritasoft,1554392263,[removed],0,1,False,self,,,,,
315,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9ecjj,self.MachineLearning,Generate new audio file out of a dataset of audiofiles,https://www.reddit.com/r/MachineLearning/comments/b9ecjj/generate_new_audio_file_out_of_a_dataset_of/,busconw,1554392623,[removed],0,1,False,self,,,,,
316,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9efqh,self.MachineLearning,Timeseries forecasting - Clustered relationship between feature and label,https://www.reddit.com/r/MachineLearning/comments/b9efqh/timeseries_forecasting_clustered_relationship/,Sokkelaila,1554393036,[removed],0,1,False,self,,,,,
317,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,0,b9ejwq,wikimediafoundation.org,Can machine learning uncover Wikipedias missing citation needed tags?,https://www.reddit.com/r/MachineLearning/comments/b9ejwq/can_machine_learning_uncover_wikipedias_missing/,benjaminikuta,1554393569,,0,1,False,https://b.thumbs.redditmedia.com/CeQMOe2gs18Jya7_sUvYv1gHd0NPeV8drx-2-u3JWXc.jpg,,,,,
318,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,1,b9eoou,self.MachineLearning,[D] Is anyone here earning money by running a business/side-business with machine learning?,https://www.reddit.com/r/MachineLearning/comments/b9eoou/d_is_anyone_here_earning_money_by_running_a/,Max-20,1554394149,I am interested in real-world implementations of machine learning and if any users here can live from it. I could imagine there are tons of usefuly applications you could turn into an SaaS and make a decent living from it.,22,13,False,self,,,,,
319,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,1,b9f1g4,self.MachineLearning,Is Batchnorm bullshit? Does not work in real scenarios?,https://www.reddit.com/r/MachineLearning/comments/b9f1g4/is_batchnorm_bullshit_does_not_work_in_real/,xnet7,1554395763,[removed],0,1,False,self,,,,,
320,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,1,b9f4iz,arxiv.org,[R] [1904.01681] Augmented Neural ODEs,https://www.reddit.com/r/MachineLearning/comments/b9f4iz/r_190401681_augmented_neural_odes/,evc123,1554396157,,6,9,False,default,,,,,
321,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,1,b9f7ct,self.MachineLearning,Advice on sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/b9f7ct/advice_on_sentiment_analysis/,throwawayDataNLP,1554396545,[removed],0,1,False,self,,,,,
322,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,1,b9f8yo,self.MachineLearning,[D] How would you isolate an object in 1 million photos?,https://www.reddit.com/r/MachineLearning/comments/b9f8yo/d_how_would_you_isolate_an_object_in_1_million/,tobcar,1554396756,"This relates to a research project I am a part of. While doing some preliminary planning for our project we realized a lot of our research ideas would have us isolate the brain in brain scans. There are already many different ways to isolate the brain, but we have not found an approach optimized for a data set as big as ours. Before narrowing the scope of our project we wanted to know if you guys had any ideas for tools, programming languages, or products we could use.

One person I know suggested using C++ because it is stereotypically a ""faster language"", however, at our scale it would still take a month to run everything on a high end desktop with a GPU. Another possibility we are thinking about is to put the code on a cloud service and pay for a lot of processing power for a short amount of time, but this feels inefficient so we are exploring whether we should be using something like Hadoop. What tools would you recommend for working with a data set as big as ours?",4,6,False,self,,,,,
323,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,2,b9fkev,self.MachineLearning,"Interactions between predictive analytics, ML, and case-based and rules-based reasoning",https://www.reddit.com/r/MachineLearning/comments/b9fkev/interactions_between_predictive_analytics_ml_and/,Sleeper_the_dolphin,1554398253,"I'm student in economy and my knowledge about AI is limited. For my studies I work on prediction of judicial decisions. I don't really understand the interactions between several concepts:

\- predictive analytics

\- machine learning

\- case-based reasoning

\- rules-based reasoning

At the moment, I understand that predictive analytics is an area of statistics that deals with extracting information from data and using it to predict a outcome, like a judicial decision. In predictive analytics there are 2 kinds of tools: econometric tools like MCO regression and machine learning tools like neural networks.

Among ML tools, we can distinguish between 2 approaches: rules-based reasoning and case-based reasoning. Given a given legal problem, with the rules approach, the algorithm learns the rules that led to these data, deduce a linear model and then we can confront the problem to the model to deduce the solution given by a court. With cases approach, the algorithm brings the given problem closer to similar cases found in the data, and deduces a solution given by a court.

Is it correct? 

Are different kind of ML tools like K-NN or neural network only used for one type of reasoning? or they can be used for both?",0,1,False,self,,,,,
324,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,2,b9fq5r,self.MachineLearning,Explaining how StyleGAN works!,https://www.reddit.com/r/MachineLearning/comments/b9fq5r/explaining_how_stylegan_works/,HenryAILabs,1554399065,[removed],0,1,False,self,,,,,
325,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,2,b9ftzy,self.MachineLearning,[D] Are there any standard methods for minimizing a neural network once it has been satisfactorily optimized?,https://www.reddit.com/r/MachineLearning/comments/b9ftzy/d_are_there_any_standard_methods_for_minimizing_a/,Florian222,1554399587,"Imagine someone creates a neural network for a specific task, and optimizes the weights and biases to the point where it performs perfectly satisfactory for their intended purpose. Mathematically speaking, their should exist a point where the network can be minimized in order to maximize the ratio of Accuracy:#ofNeurons. In a network with many layers, this could be as simple as pruning the latter layers, but aside from that, there should be a method of horizontally combining/condensing the neurons in a given layer to an optimized point. 

There are standard methods in calculus for solving such optimization problems, but before trying to reinvent the wheel, I'm wondering if there exist any algorithmic methods for this that are already commonly used?
 ",22,22,False,self,,,,,
326,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,2,b9g1aa,self.MachineLearning,[D] What business/personal problems could easily be solved with current ML tech?,https://www.reddit.com/r/MachineLearning/comments/b9g1aa/d_what_businesspersonal_problems_could_easily_be/,carsonpoole,1554400597,"I'm trying to brainstorm common business problems that could easily be solved with relatively simple ML techniques that are free to use and easy to implement. I'll give an example to show what I mean. 

For example, offices get lots of mail, and sorting and filing it is a real hassle. You could easily scan mail, have Google's vision API scan it and get all the info, have BERT classify it into categories that the business would like and have these saved into a database that's easily searchable.

I'd love to brainstorm more examples like this that you could think of for the benefit of everyone.

Thanks, can't wait to hear some ideas",5,0,False,self,,,,,
327,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,2,b9g1jr,self.MachineLearning,[D] Best graduate schools for AI/ML?,https://www.reddit.com/r/MachineLearning/comments/b9g1jr/d_best_graduate_schools_for_aiml/,visionnnm,1554400633,"Howdy folks. I was recently admitted to PhD programs in the CS departments at Stanford, MIT, UC Berkeley, and CMU, and Im now trying to choose between them.

While I know the advisor and research group should be my primary consideration, I am curious to know what yall think the pros/cons of these schools are for AI/ML generally. ",5,0,False,self,,,,,
328,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,3,b9g6xn,medium.com,New Google Brain Optimizer Reduces BERT Pre-Training Time From Days to Minutes,https://www.reddit.com/r/MachineLearning/comments/b9g6xn/new_google_brain_optimizer_reduces_bert/,gwen0927,1554401358,,0,1,False,https://b.thumbs.redditmedia.com/cIUY2R6kZVaptUDEUsLtqCxo3mmqH8AowoVbGg9R5Qg.jpg,,,,,
329,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,3,b9g752,arxiv.org,[R] [1904.01169] Res2Net: A New Multi-scale Backbone Architecture,https://www.reddit.com/r/MachineLearning/comments/b9g752/r_190401169_res2net_a_new_multiscale_backbone/,bobchennan,1554401384,,2,26,False,default,,,,,
330,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,3,b9g7jb,self.MachineLearning,Direct Feedback Alignment implementation,https://www.reddit.com/r/MachineLearning/comments/b9g7jb/direct_feedback_alignment_implementation/,marco2012,1554401435,"I'm interested in [Direct Feedback Alignment](https://arxiv.org/pdf/1609.01596.pdf) . 

I found [this great article](https://medium.com/blog-rilut/neural-networks-without-backpropagation-direct-feedback-alignment-30d5d4848f5) about it but it lacks a complete Python implementation as it is left *as an exercise to readers*. 

Does anybody have a full python implementation? 

Thanks",0,1,False,self,,,,,
331,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,3,b9gf04,self.MachineLearning,[D] Question about generalization of DNNs,https://www.reddit.com/r/MachineLearning/comments/b9gf04/d_question_about_generalization_of_dnns/,Minimum_Zucchini,1554402399,"So I'm seeing a lot more papers lately that are studying generalizability of NN's and I'm just wondering something. As far as I can tell most NN's are used on very large data sets or at least just for benchmarking new architecture changes. 


This might be a silly question but, since we are using large datasets.. is it not possible that the test set is heavily correlated with the training set (similar images for example) and so the NN is simply just memorizing the input? So what we think is generalization is simply the training/test data being highly similar to one another? 


Is there any papers that study training/test that are explicitly picked to have uncorrelated elements? And besides correlation, what other similarity metric could we use to say that technically we are just memorizing the input BASED on some similarity metric between training/testing? Higher statistical moments?


I suppose the training/testing cannot be statistically independent because the whole foundation of ML is based around estimating P(X,Y) and that training/testing is drawn from this same probability distribution. 


",11,3,False,self,,,,,
332,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,4,b9gwt6,climatechange.ai,[D] ICML Workshop: Climate Change how can AI help,https://www.reddit.com/r/MachineLearning/comments/b9gwt6/d_icml_workshop_climate_change_how_can_ai_help/,konasj,1554404706,,0,1,False,default,,,,,
333,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,4,b9h66q,self.MachineLearning,n00b one-class classifier,https://www.reddit.com/r/MachineLearning/comments/b9h66q/n00b_oneclass_classifier/,sarksnz,1554405983,[removed],0,1,False,self,,,,,
334,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,4,b9h6s6,self.MachineLearning,Is it common practice to dynamically change the model?,https://www.reddit.com/r/MachineLearning/comments/b9h6s6/is_it_common_practice_to_dynamically_change_the/,high_byte,1554406070,"I am wondering if it is common to update the neural network shape, like adding layers, creating neurons, removing weights (e.g. near-zero weights), etc.

When would one use such a technique? ",0,1,False,self,,,,,
335,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,4,b9hehy,self.MachineLearning,Is 65% Good Enough Accuracy For a Multiclass Classification ML Model?,https://www.reddit.com/r/MachineLearning/comments/b9hehy/is_65_good_enough_accuracy_for_a_multiclass/,_rahulrd,1554407132,[removed],0,1,False,self,,,,,
336,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,5,b9hn02,ganvatar.com,"Mining the StyleGAN latent space for fun and profit - adjust the age, gender, and emotion of a face",https://www.reddit.com/r/MachineLearning/comments/b9hn02/mining_the_stylegan_latent_space_for_fun_and/,berkeleymalagon,1554408304,,0,1,False,default,,,,,
337,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,5,b9hq3a,youtu.be,Machine Learning Tutorial | Face Recognition in 10 min,https://www.reddit.com/r/MachineLearning/comments/b9hq3a/machine_learning_tutorial_face_recognition_in_10/,EddyTheDad,1554408730,,0,1,False,https://b.thumbs.redditmedia.com/4Mq7UCb_RHvZ0QadtTV3HPlUXUyysbeGq4Z4xfQ62jE.jpg,,,,,
338,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,5,b9i9xi,self.MachineLearning,Does machine learning can help me with that ?,https://www.reddit.com/r/MachineLearning/comments/b9i9xi/does_machine_learning_can_help_me_with_that/,Oscar_Rayleigh,1554411424,[removed],0,1,False,self,,,,,
339,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,6,b9ibaw,self.MachineLearning,Question related to digit recognition and mathematical symbols.,https://www.reddit.com/r/MachineLearning/comments/b9ibaw/question_related_to_digit_recognition_and/,GeforceCore,1554411617,"I am a newbie to this field so basically I want to make a simple calculator which works when provided handwritten equations for eg. 2+4 ,3*5,7-6,8/3 etc. So my question is if I am able to identify each digit or symbol uniquely how do I put all together . For eg.  in the equation 2+4 if I identify 2 alone then + sign alone and then finally 4 alone but how do I make computer understand that 2 and 4 are numbers and + is a sign which means to add. Also how to identify more than one digit number i.e 20,21,68 etc.",0,1,False,self,,,,,
340,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,6,b9iyi6,self.MachineLearning,[N] Apple hires Ian Goodfellow,https://www.reddit.com/r/MachineLearning/comments/b9iyi6/n_apple_hires_ian_goodfellow/,milaworld,1554414966,"*According to CNBC [article](https://www.cnbc.com/2019/04/04/apple-hires-ai-expert-ian-goodfellow-from-google.html):*

One of Googles top A.I. people just joined Apple

- Ian Goodfellow joined Apples Special Projects Group as a director of machine learning last month.

- Prior to Google, he worked at OpenAI, an AI research consortium originally funded by Elon Musk and other tech notables.

- He is the father of an AI approach known as general adversarial networks, or GANs, and his research is widely cited in AI literature.

Ian Goodfellow, one of the top minds in artificial intelligence at Google, has joined Apple in a director role.

The hire comes as Apple increasingly strives to tap AI to boost its software and hardware. Last year Apple hired John Giannandrea, head of AI and search at Google, to supervise AI strategy.


Goodfellow updated his LinkedIn profile on Thursday to acknowledge that he moved from Google to Apple in March. He said hes a director of machine learning in the Special Projects Group. In addition to developing AI for features like FaceID and Siri, Apple also has been working on autonomous driving technology. Recently the autonomous group had a round of layoffs.

A Google spokesperson confirmed his departure. Apple declined to comment. Goodfellow didnt respond to a request for comment.

https://www.cnbc.com/2019/04/04/apple-hires-ai-expert-ian-goodfellow-from-google.html",189,537,False,self,,,,,
341,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,7,b9j1qi,self.MachineLearning,EPQ Research Survey [r] [p],https://www.reddit.com/r/MachineLearning/comments/b9j1qi/epq_research_survey_r_p/,chris_harris_15,1554415445," Hi, if anyone could fill this out to help me with my research for my EPQ. Or if anyone has any interesting research that might help, I am doing:

**Will the development of artificial intelligence and machine learning threaten or benefit wider society?**

&amp;#x200B;

[https://forms.office.com/Pages/DesignPage.aspx#Analysis=true&amp;FormId=DupiPq2EoEOgr79kMXIfjycF4\_VwKuhChOf5WhSnHLJURTlKMVVFVzA0OEdGT0RPNkRTNVI3T01OVy4u](https://forms.office.com/Pages/DesignPage.aspx#Analysis=true&amp;FormId=DupiPq2EoEOgr79kMXIfjycF4_VwKuhChOf5WhSnHLJURTlKMVVFVzA0OEdGT0RPNkRTNVI3T01OVy4u) ",0,0,False,self,,,,,
342,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,8,b9jpn8,self.MachineLearning,You are suddenly independently wealthy. What ML subject would you like to research?,https://www.reddit.com/r/MachineLearning/comments/b9jpn8/you_are_suddenly_independently_wealthy_what_ml/,rumborak,1554419183,[removed],0,1,False,self,,,,,
343,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,8,b9jwk6,medium.com,Father of GANs Ian GoodFellow Splits Google For Apple,https://www.reddit.com/r/MachineLearning/comments/b9jwk6/father_of_gans_ian_goodfellow_splits_google_for/,gwen0927,1554420302,,0,1,False,https://b.thumbs.redditmedia.com/qti-Ep212Vf7aXWVlmriJo-dh0vWNzqRFwOLcYTL_jU.jpg,,,,,
344,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,8,b9k3xr,self.MachineLearning,"[P] Deploying a Fraud Detection Microservice using TensorFlow, PySpark, and Cortex",https://www.reddit.com/r/MachineLearning/comments/b9k3xr/p_deploying_a_fraud_detection_microservice_using/,ospillinger,1554421528,"When I first got into machine learning I came across Kaggles [credit card fraud detection dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud) and thought it would be cool to build a fraud detection microservice on top of it. I wanted to share my implementation with you, which I built using an open source stack including TensorFlow, PySpark, and Cortex (a machine learning platform that my colleagues and I are working on). Id appreciate your feedback!

[https://medium.com/cortex-labs/deploying-a-fraud-detection-microservice-using-tensorflow-pyspark-and-cortex-f5cb55126b2e](https://medium.com/cortex-labs/deploying-a-fraud-detection-microservice-using-tensorflow-pyspark-and-cortex-f5cb55126b2e)",1,11,False,self,,,,,
345,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,8,b9k5i9,self.MachineLearning,[D] CIFAR-10 test set labels,https://www.reddit.com/r/MachineLearning/comments/b9k5i9/d_cifar10_test_set_labels/,tsauri,1554421794,"Anyone here had actually eyeballed every single test images and their labels from CIFAR-10?  
Because last time I found a test image that is essentially \*black\* .  
Also current SOTA is 1.0% error, meaning that from 10,000 images, there are 100 wrongly classified images.  
How do the network classify image that resembles nothing?  
[https://arxiv.org/pdf/1811.06965.pdf](https://arxiv.org/pdf/1811.06965.pdf) ",1,1,False,self,,,,,
346,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,9,b9k9r4,self.MachineLearning,[R] Amazon Alexa: Experiments in Machine Learning at Scale,https://www.reddit.com/r/MachineLearning/comments/b9k9r4/r_amazon_alexa_experiments_in_machine_learning_at/,MrMonday11235,1554422487,"Summary Post: https://developer.amazon.com/blogs/alexa/post/9e8392c6-5476-4a34-a2d8-c4e479677954/new-speech-recognition-experiments-demonstrate-how-machine-learning-can-scale

Full Paper: https://arxiv.org/pdf/1904.01624.pdf

The paper seemed like an interesting read on decisions made, though I can't imagine that it's particularly applicable for most use-cases. ",1,3,False,self,,,,,
347,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,9,b9kbxt,self.MachineLearning,Not too long ago it was unthinkable that a computer can beat the best humans in GO. How far do you think we are from beating the best players in Rocket League?,https://www.reddit.com/r/MachineLearning/comments/b9kbxt/not_too_long_ago_it_was_unthinkable_that_a/,logicallyzany,1554422876,[removed],0,1,False,self,,,,,
348,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,9,b9kezu,self.MachineLearning,[N] Google cancels AI ethics board in response to outcry,https://www.reddit.com/r/MachineLearning/comments/b9kezu/n_google_cancels_ai_ethics_board_in_response_to/,inarrears,1554423350,"*[Google cancels AI ethics board in response to outcry](https://www.vox.com/future-perfect/2019/4/4/18295933/google-cancels-ai-ethics-board):*

Google told Vox on Thursday that its pulling the plug on the ethics board.

The board survived for barely more than one week. Founded to guide responsible development of AI at Google, it would have had eight members and met four times over the course of 2019 to consider concerns about Googles AI program. Those concerns include how AI can enable authoritarian states, how AI algorithms produce disparate outcomes, whether to work on military applications of AI, and more. But it ran into problems from the start.

Thousands of Google employees signed a petition calling for the removal of one board member, Heritage Foundation president Kay Coles James, over her comments about trans people and her organizations skepticism of climate change. Meanwhile, the inclusion of drone company CEO Dyan Gibbens reopened old divisions in the company over the use of the companys AI for military applications.

Board member Alessandro Acquisti resigned. Another member, Joanna Bryson, defending her decision not to resign, claimed of James, Believe it or not, I know worse about one of the other people. Other board members found themselves swamped with demands that they justify their decision to remain on the board.

https://www.vox.com/future-perfect/2019/4/4/18295933/google-cancels-ai-ethics-board",52,55,False,self,,,,,
349,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,9,b9kl9x,self.MachineLearning,[R] A Learned Representation for Scalable Vector Graphics,https://www.reddit.com/r/MachineLearning/comments/b9kl9x/r_a_learned_representation_for_scalable_vector/,iRaphael,1554424402,"*New* [*paper*](https://arxiv.org/abs/1904.02632) *by* [*Raphael Gontijo Lopes*](https://twitter.com/iraphas13)*,* [*David Ha*](https://twitter.com/hardmaru)*, Jonathon Shlens and* [*Douglas Eck*](https://twitter.com/douglas_eck)*:*

**A Learned Representation for Scalable Vector Graphics**

*Abstract:*

*Dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. In spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. In this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. This model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. We demonstrate these results on a large dataset of fonts and highlight how such a model captures the statistical dependencies and richness of this dataset. We envision that our model can find use as a tool for graphic designers to facilitate font design.*

Paper: [https://arxiv.org/abs/1904.02632](https://arxiv.org/abs/1904.02632)

&amp;#x200B;

P.S.: I'm the first author, feel free to drop any questions below :)",8,14,False,self,,,,,
350,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,10,b9l1ib,self.MachineLearning,Machine Learning can help humans to build a Time Machine [Discussion],https://www.reddit.com/r/MachineLearning/comments/b9l1ib/machine_learning_can_help_humans_to_build_a_time/,Aditya0502,1554427184,"I was thinking about the time machine idea which consists of machine learning as a core substance. Well, this idea needs ultimate development. Please read the whole theory.
The future can be personalized. If we will give data to the machine which will be used to create entire universes then after a suitable number of inputs that machine can create different universes based on the data inputs which we gave in the past. For example, if I want to go into the future then on the basis of past algorithms the time machine will create different possibilities in the future from which we can select a single possibility. 
But to implement this idea human generation needs some dangerous advancement in the field of energy production and artificial intelligence.",5,0,False,self,,,,,
351,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,11,b9lhxs,self.MachineLearning,Why one cannot directly optimize BLEU when training a neural machine translation system?,https://www.reddit.com/r/MachineLearning/comments/b9lhxs/why_one_cannot_directly_optimize_bleu_when/,zkid18,1554429947,[removed],0,1,False,self,,,,,
352,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,11,b9llgs,self.MachineLearning,[D] Why one cannot directly optimize BLEU when training a neural machine translation system?,https://www.reddit.com/r/MachineLearning/comments/b9llgs/d_why_one_cannot_directly_optimize_bleu_when/,zkid18,1554430556,,10,3,False,self,,,,,
353,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,11,b9lrwf,self.MachineLearning,Best tech/platform to stop running code on laptops for DS team,https://www.reddit.com/r/MachineLearning/comments/b9lrwf/best_techplatform_to_stop_running_code_on_laptops/,m4329b,1554431659,[removed],0,1,False,self,,,,,
354,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,12,b9m5d1,self.MachineLearning,[D] Ignite - has anyone used it with PyTorch?,https://www.reddit.com/r/MachineLearning/comments/b9m5d1/d_ignite_has_anyone_used_it_with_pytorch/,valued_eigens,1554434072,"I made the switch from tensorflow to pytorch last year. It's been a smooth transition so far. I saw Ignite under the PyTorch Ecosystem and on their Github, has anyone used it?

&amp;#x200B;

Any pros and cons for using it? How is it compared to tools like tnt, torchbearer? Is it similar to Keras for tensorflow?

&amp;#x200B;

I ran some of their tutorials, looks pretty intuitive! ",5,2,False,self,,,,,
355,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,12,b9m7jy,self.MachineLearning,[D] What resources to learn how to setup ML problems?,https://www.reddit.com/r/MachineLearning/comments/b9m7jy/d_what_resources_to_learn_how_to_setup_ml_problems/,theycallhimtom,1554434469,"In practice it seems like a lot of ML work is turning a real world problem into a well defined problem. Are they any resources for learning how to do that step (maybe some case studies)? Most ML papers I read assume you have labeled data (some input data x and some label y) and a clear loss function to optimize.

To be more precise I'm curious how you go from messy real world data and an unclear objective (say all the data from a particle collider and the goal to understand physics) to a [Kaggle contest](https://www.kaggle.com/c/trackml-particle-identification) (precise well defined problem). ",3,3,False,self,,,,,
356,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,12,b9mfkj,self.MachineLearning,MLE vs MAP: visualization and examples,https://www.reddit.com/r/MachineLearning/comments/b9mfkj/mle_vs_map_visualization_and_examples/,nivter,1554435991,[removed],0,1,False,self,,,,,
357,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,12,b9mg6r,self.MachineLearning,"[D] Would anyone like to give feedback on the answer I gave to a take home machine learning interview take home problem? I was rejected, but I didn't get any feedback, so was wondering if anyone here could fill me in.",https://www.reddit.com/r/MachineLearning/comments/b9mg6r/d_would_anyone_like_to_give_feedback_on_the/,DisastrousProgrammer,1554436110,"I applied to a ML position and was given a take down problem, then was notified I didn't make it to the next round. Was wondering if anyone here could take a look and give some feedback on the answer I gave. ",1,0,False,self,,,,,
358,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,12,b9mjnj,self.MachineLearning,I need some help with the project on Time Series Analysis. (please read the text),https://www.reddit.com/r/MachineLearning/comments/b9mjnj/i_need_some_help_with_the_project_on_time_series/,hellfuckinright,1554436788,[removed],0,1,False,self,,,,,
359,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,13,b9mmy3,self.MachineLearning,[P] I need some help with my project on Time Series Analysis (please read the text below),https://www.reddit.com/r/MachineLearning/comments/b9mmy3/p_i_need_some_help_with_my_project_on_time_series/,hellfuckinright,1554437415,"The company XYZ (home improvement company) has 80 categories of products (say, Fishing &amp; Hunting hardware, Fasteners, etc). I have the net sales of each of these categories from past 4 years.

From what I have observed, there is a seasonality in approximately 70 of those categories.

I know I can apply ARIMA here to forecast the revenue, but what would be a strong point other than this to answer the below question?

**Q: How can we improve the revenue and the inventory efficiency of the company in the coming years?**",4,0,False,self,,,,,
360,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,14,b9n20v,self.MachineLearning,OCR,https://www.reddit.com/r/MachineLearning/comments/b9n20v/ocr/,seth_hardik,1554440521,[removed],0,1,False,self,,,,,
361,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,14,b9ne6a,v.redd.it,A neural network is trained on images of space and then hallucinates new images [OC],https://www.reddit.com/r/MachineLearning/comments/b9ne6a/a_neural_network_is_trained_on_images_of_space/,professormunchies,1554443128,,1,2,False,https://b.thumbs.redditmedia.com/IAS0csSQ77zAaw8sD9H_hyTq4mVHg8VGHPMHhHgYROI.jpg,,,,,
362,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,15,b9nof9,self.MachineLearning,Best Books for Improving Understanding of Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/b9nof9/best_books_for_improving_understanding_of/,DataSciencePenguin,1554445394,"Hello all,

&amp;#x200B;

I've done a lot of ML work and have a lot of textbooks/good papers accumulated on ML, but in looking back at it recently, practically all of my knowledge and experience is in supervised learning. Does anyone have any good recommendations for textbooks or review papers on unsupervised learning approaches? Not at a super advanced level to start, but like a good intermediate textbook for factor analysis, clustering, etc... If it also throws in the details for the underlying statistical theory, that'd be a huge bonus too :)

&amp;#x200B;

Thank you all in advance!!! :)",0,1,False,self,,,,,
363,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,15,b9noip,youtube.com,RecSys 2018 videos are out!,https://www.reddit.com/r/MachineLearning/comments/b9noip/recsys_2018_videos_are_out/,Arnie0426,1554445416,,0,1,False,default,,,,,
364,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,15,b9nswo,self.MachineLearning,[D] Best Books for Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/b9nswo/d_best_books_for_unsupervised_learning/,DataSciencePenguin,1554446422,"Hello all,

&amp;#x200B;

I've done a lot of ML work and have a lot of textbooks/good papers accumulated on ML, but in looking back at it recently, practically all of my knowledge and experience is in supervised learning. Does anyone have any good recommendations for textbooks or review papers on unsupervised learning approaches? Not at a super advanced level to start, but like a good intermediate textbook for factor analysis, clustering, etc... If it also throws in the details for the underlying statistical theory, that'd be a huge bonus too :)

&amp;#x200B;

Thank you all in advance!!! :)",2,8,False,self,,,,,
365,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,16,b9o2ot,youtube.com,cheap pun Westworld parody: AI World - AnacondaCON 2019,https://www.reddit.com/r/MachineLearning/comments/b9o2ot/cheap_pun_westworld_parody_ai_world_anacondacon/,Phnyx,1554448542,,0,1,False,https://a.thumbs.redditmedia.com/fNf90fqTp_uzrCPIFKAPFIxiB9IOMsmp_uc-IVST830.jpg,,,,,
366,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,16,b9obb3,self.MachineLearning,[D] Quantifying Uncertainty in Classification w/ NNs,https://www.reddit.com/r/MachineLearning/comments/b9obb3/d_quantifying_uncertainty_in_classification_w_nns/,seann999,1554450541,"In the case of regression, I'm aware of using Dropout or bootstrapping w/ multiple NNs to get multiple outputs and calculating their standard deviation to quantify epistemic uncertainty. Sorry if this is obvious, but what is the analogous procedure for classification?

For example, if it was binary and I got 3 outputs: 0.78, 0.93, 0.81

How would I quantify uncertainty? ",6,7,False,self,,,,,
367,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,16,b9odvq,i.redd.it,Cleverbot knows the meaning of life,https://www.reddit.com/r/MachineLearning/comments/b9odvq/cleverbot_knows_the_meaning_of_life/,Cain1608,1554451178,,0,1,False,https://b.thumbs.redditmedia.com/Wh1ILUdpjG8OVh9NAQytPUDXlQ-JGjmh74Y4-9eMOKA.jpg,,,,,
368,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,17,b9om7l,oodlestechnologies.com,5 Best Artificial Intelligence Tools,https://www.reddit.com/r/MachineLearning/comments/b9om7l/5_best_artificial_intelligence_tools/,oodlestechnologies,1554453217,,0,1,False,https://b.thumbs.redditmedia.com/kxbgnsZ2LkIwRxoPcPd6LHGr1uPduWOmOXcg0N9F1Rk.jpg,,,,,
369,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,17,b9oqy7,self.MachineLearning,Parcel Sorting Robots Market to Reflect Robust Expansion during 2023,https://www.reddit.com/r/MachineLearning/comments/b9oqy7/parcel_sorting_robots_market_to_reflect_robust/,harshbir123456789,1554454411,[removed],1,1,False,self,,,,,
370,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,18,b9owpd,self.MachineLearning,Practical Machine Learning with R and Python  Part 1,https://www.reddit.com/r/MachineLearning/comments/b9owpd/practical_machine_learning_with_r_and_python_part/,tvganesh,1554455720,[removed],0,1,False,self,,,,,
371,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,18,b9p33s,buzzblogbox.com,7 Best Applications of Machine Learning in Healthcare,https://www.reddit.com/r/MachineLearning/comments/b9p33s/7_best_applications_of_machine_learning_in/,trainingdata,1554457123,,0,1,False,default,,,,,
372,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,19,b9phuz,i.redd.it,"Fraud is rising at an alarming rate, see how AI is tackling fraud now and the role it has in stopping fraud in the future",https://www.reddit.com/r/MachineLearning/comments/b9phuz/fraud_is_rising_at_an_alarming_rate_see_how_ai_is/,Vidhya_Shree,1554460226,,0,1,False,https://b.thumbs.redditmedia.com/Lg40KDXthB_TkYRjQdU4i4m9eCXEUTzH6LkYSUV_x7I.jpg,,,,,
373,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,19,b9pi6k,self.MachineLearning,[D] How correct is this numpy porting of integration of neural network with neural ode?,https://www.reddit.com/r/MachineLearning/comments/b9pi6k/d_how_correct_is_this_numpy_porting_of/,begooboi,1554460288,"In order to understand neural ode I have coded a small part of it (integrating neural network) in numpy using Krzysztof Kolasinski's [tensorflow notes](https://github.com/kmkolasinski/deep-learning-notes/blob/master/seminars/2019-03-Neural-Ordinary-Differential-Equations/0.Implementing_black_box_solver.ipynb).  This is the whole code

    import numpy as np
    import matplotlib.pyplot as plt

    #weights
    W0= 2*np.random.random((1,4)) - 1
    W1= 2*np.random.random((4,2)) - 1

    def nonlin(x,deriv=False):
        if(deriv==True):
            return x*(1-x)
        return 1/(1+np.exp(-x))

    def neural_network(x):
        h=np.dot(x,W0)
        y_bar=np.dot(h,W1)
        return y_bar

    def euler_step(dt, tk, hk, fun):
        return hk + dt * fun(tk)

    t= np.linspace(0, 29., 30)

    #initial condition
    ts = t[1:] - t[:-1]
    tk=t[0]
    yk=np.array(([[1.0, -1.0]]))
    hist=np.array(yk)

    for t in ts:
        yk=euler_step(t,tk,yk,neural_network)
        tk = tk + t
        hist=np.vstack((hist,yk))

    plt.plot(t, hist[:, 0], label=""h[t, 0]"", lw=2)
    plt.plot(t, hist[:, 1], label=""h[t, 1]"", lw=2)
    plt.xlabel(""time"")
    plt.legend()
    plt.show()
    plt.close()


Is this correct ?",4,0,False,self,,,,,
374,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,19,b9pjb9,self.MachineLearning,LHD S.p.A manufactures the best quality Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/b9pjb9/lhd_spa_manufactures_the_best_quality_telescopic/,lhd121,1554460514,[removed],0,1,False,https://b.thumbs.redditmedia.com/dXu7uDUz7OLCCmnGA86KRH0FM2X8bYRV17YwbIdaoKM.jpg,,,,,
375,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,20,b9prd1,self.MachineLearning,[P] I'm a bot and will serve people analyzing chess positions from images posted on /r/chess,https://www.reddit.com/r/MachineLearning/comments/b9prd1/p_im_a_bot_and_will_serve_people_analyzing_chess/,chessvision-ai-bot,1554462079,"A few days ago, my creator, u/pkacprzak, wrote a [post](https://www.reddit.com/r/MachineLearning/comments/b8jdho/p_detect_and_analyze_chess_positions_with_ai_from/) about [chessvision.ai](https://chessvision.ai/) \- his computer vision/machine learning app to analyze chess positions from any website and video in a browser.

&amp;#x200B;

Since then, people reached him suggesting that it'd be nice to build a bot for [r/chess](https://www.reddit.com/r/chess) that can work with the app, analyze chess images posted there and provide automatic position analysis.

&amp;#x200B;

All of us love the awesome [u/ChessFenBot](https://www.reddit.com/u/ChessFenBot) that was doing just that, but for some reason, it hasn't been working recently,

&amp;#x200B;

so from now I, [u/chessvision-ai-bot](https://www.reddit.com/u/chessvision-ai-bot), will be pleased to serve you!

&amp;#x200B;

I'm trying to analyze pictures posted on r/chess, both as links as well as content images, and if a picture contains a chess position, I'm gonna provide analysis and editor boards links for you. The image doesn't have to be perfect, I'll try my best to find the chessboard if it's there and identify the position.

&amp;#x200B;

Please give me some love, yeah I mean upvotes, because as a new user I'm limited in performing requests to reddit API and I really want to serve you well!",21,199,False,self,,,,,
376,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,21,b9qhw5,io-tahoe.com,Latest Trends in Data - Machine Learning &amp; Predictive Analysis | Io-Tahoe,https://www.reddit.com/r/MachineLearning/comments/b9qhw5/latest_trends_in_data_machine_learning_predictive/,jacobmarsh789,1554466735,,0,1,False,default,,,,,
377,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,21,b9qmnp,ctovision.com,Here's How Machine Learning Will Provide Structure to Unstructured Data - CTOvision.com,https://www.reddit.com/r/MachineLearning/comments/b9qmnp/heres_how_machine_learning_will_provide_structure/,jacobmarsh789,1554467533,,0,1,False,default,,,,,
378,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,21,b9qsyg,self.MachineLearning,[N] Machine Learning: Alchemy for the Modern Computer Scientist,https://www.reddit.com/r/MachineLearning/comments/b9qsyg/n_machine_learning_alchemy_for_the_modern/,mto96,1554468607,"This is talk by Erik Meijer, from GOTO Copenhagen 2018. 

[https://youtu.be/Rs0uRQJdIcg?list=PLEx5khR4g7PIzxn476GK3Mkk19csZZjeH](https://youtu.be/Rs0uRQJdIcg?list=PLEx5khR4g7PIzxn476GK3Mkk19csZZjeH)

&amp;#x200B;

Please check give the talk abstract a read below before diving into watching it:

&amp;#x200B;

In ancient times, the dream of alchemists was to mutate ordinary metals such as lead into noble metals such as gold. However, by using classic mathematics, modern physicists and chemists are much more successful in understanding and transforming matter than alchemists ever dreamt of.

The situation in software seems to be the opposite. Modern computer scientists have been unsuccessful in their quest to reliably turn formal specifications into code and to accurately understand the mechanics of side-effecting computation. On the other hand, software alchemists, by using classic mathematics, have been extremely successful in mutating training data into pure functions using various machine learning techniques, in particular deep learning.

Mechanically learning code from training data is often referred to as ""Software 2.0"" or ""Learning-based development"". This new paradigm of software creation will require a radical rethinking of the ancestral software engineering and imperative programming practices that have been developed in the second half of the last century.

In this talk we will discuss how we are building new probabilistic frameworks and differentiable programming languages that support the composition and construction of learnable code, as well as how we can leverage machine learning at every level of the software stack to make developers more productive and services &amp; products more efficient.

&amp;#x200B;",1,8,False,self,,,,,
379,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,22,b9qyzc,towardsdatascience.com,[D] Graduating in GANs: Going from understanding generative adversarial networks to running your own,https://www.reddit.com/r/MachineLearning/comments/b9qyzc/d_graduating_in_gans_going_from_understanding/,ceceshao1,1554469541,,0,1,False,default,,,,,
380,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,22,b9r7ep,self.MachineLearning,Extensive webinar on insights discovery,https://www.reddit.com/r/MachineLearning/comments/b9r7ep/extensive_webinar_on_insights_discovery/,cristina-g,1554470835,[removed],0,1,False,self,,,,,
381,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,22,b9rirh,self.MachineLearning,[D] AI Market place - Sell your models?,https://www.reddit.com/r/MachineLearning/comments/b9rirh/d_ai_market_place_sell_your_models/,andrewpierno,1554472668,"I'm proving out a container marketplace concept and wanted the feedback of you guys to see if there is any willingness to sell machine learning models wrapped up in (docker) containers on a marketplace. Just like rapidAPI but for docker containers. 

&amp;#x200B;

The thesis is that there are so many amazing models out there, why would you pay amazon or google per api request if you could just as easily spin one up on their platform and have it cost you far less (especially if you are hitting it a lot). 

&amp;#x200B;

The second insight was edge computing. API's in the cloud are great unless your network is garbage or you're transporting so much data (video perhaps) over the wire that it becomes cost prohibitive to do the machine learning in the cloud. 

&amp;#x200B;

Anyways before putting any more time into the concept, I'd love to get some thoughts. 

&amp;#x200B;

What I have so far is at [https://sugarkubes.io](https://sugarkubes.io). ",32,18,False,self,,,,,
382,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,23,b9rkj0,self.MachineLearning,[D] Can I say recommendation using Knowledge Graph Embedding generated from user-item graph is Collaborative filtering?,https://www.reddit.com/r/MachineLearning/comments/b9rkj0/d_can_i_say_recommendation_using_knowledge_graph/,gourxb,1554472945,"I have generated a Knowledge Graph Embedding using TransR, and have used it in a Neural collaborative filtering framework\[1\] (with fixed embedding generated from TransR). Can I can this collaborative filtering? 

[Embedding layer feed with TransR embeddings](https://i.redd.it/we0vz6tfcgq21.png)

If  I add user and item's other metadata can I call it Hybrid recommendation system?

Is there similar to some published work? ",2,0,False,https://b.thumbs.redditmedia.com/qQh-cXOS3s2CNJR7xnRG2r1JOP4XPt7e_1BGm_We9tY.jpg,,,,,
383,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,23,b9rqg5,self.MachineLearning,What are the practical applications of ML you have seen/developed?,https://www.reddit.com/r/MachineLearning/comments/b9rqg5/what_are_the_practical_applications_of_ml_you/,curiousily_,1554473805,[removed],0,1,False,self,,,,,
384,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,23,b9rzo0,docs.microsoft.com,Machine learning algorithm cheat sheet,https://www.reddit.com/r/MachineLearning/comments/b9rzo0/machine_learning_algorithm_cheat_sheet/,s1monaco,1554475131,,0,1,False,default,,,,,
385,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,23,b9s4cp,self.MachineLearning,[R] Reducing BERT Pre-Training Time from 3 Days to 76 Minutes,https://www.reddit.com/r/MachineLearning/comments/b9s4cp/r_reducing_bert_pretraining_time_from_3_days_to/,cayminski,1554475781,"# Reducing BERT Pre-Training Time from 3 Days to 76 Minutes

[Yang You](https://arxiv.org/search/cs?searchtype=author&amp;query=You%2C+Y), [Jing Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J), [Jonathan Hseu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hseu%2C+J), [Xiaodan Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song%2C+X), [James Demmel](https://arxiv.org/search/cs?searchtype=author&amp;query=Demmel%2C+J), [Cho-Jui Hsieh](https://arxiv.org/search/cs?searchtype=author&amp;query=Hsieh%2C+C)

&gt;Large-batch training is key to speeding up deep neural network training in large distributed systems. However, large-batch training is difficult because it produces a generalization gap. Straightforward optimization often leads to accuracy loss on the test set. BERT \\cite{devlin2018bert} is a state-of-the-art deep learning model that builds on top of deep bidirectional transformers for language understanding. Previous large-batch training techniques do not perform well for BERT when we scale the batch size (e.g. beyond 8192). BERT pre-training also takes a long time to finish (around three days on 16 TPUv3 chips). To solve this problem, we propose the LAMB optimizer, which helps us to scale the batch size to 65536 without losing accuracy. LAMB is a general optimizer that works for both small and large batch sizes and does not need hyper-parameter tuning besides the learning rate. The baseline BERT-Large model needs 1 million iterations to finish pre-training, while LAMB with batch size 65536/32768 only needs 8599 iterations. We push the batch size to the memory limit of a TPUv3 pod and can finish BERT training in 76 minutes.

Paper: [https://arxiv.org/abs/1904.00962](https://arxiv.org/abs/1904.00962)",1,1,False,self,,,,,
386,MachineLearning,t5_2r3gv,2019-4-5,2019,4,5,23,b9s5ty,self.MachineLearning,is it true that machine learning can predict stocks???,https://www.reddit.com/r/MachineLearning/comments/b9s5ty/is_it_true_that_machine_learning_can_predict/,kevinsavani,1554475988,[removed],0,1,False,self,,,,,
387,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,0,b9s944,distill.pub,A Visual Exploration of Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/b9s944/a_visual_exploration_of_gaussian_processes/,PullThisFinger,1554476443,,0,1,False,default,,,,,
388,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,1,b9tbne,self.MachineLearning,"[P] Thinking of building something like Spotify/a social media site for reading papers--""your friends are reading this, maybe you'd like this paper too"". Any suggestions or thoughts?",https://www.reddit.com/r/MachineLearning/comments/b9tbne/p_thinking_of_building_something_like_spotifya/,silverpendulum,1554481648,"Inspired by how well Spotify does its recommendations (e.g. Daily Mix).

&amp;#x200B;

Are there already platforms like this? Close ones that come to mind are Arxiv Sanity, Google Scholar, and Papers With Code. Medium's recommendations/notifications are also pretty good, although they'd have to be written on Medium itself.

&amp;#x200B;

Any suggestions for implementation, as well as useful features? Right off the bat, I'm thinking along the lines of clustering and/or collaborative filtering.",44,179,False,self,,,,,
389,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,1,b9tdk4,self.MachineLearning,[D] What kind of difficulties have you experienced with out-sourced annotation partners?,https://www.reddit.com/r/MachineLearning/comments/b9tdk4/d_what_kind_of_difficulties_have_you_experienced/,jonksar,1554481905,"Following the question in the same sub: ""[AI/Machine Learning Startups, What Is Your Biggest Difficulty Currently?](https://www.reddit.com/r/MachineLearning/comments/b5boov/d_aimachine_learning_startups_what_is_your/)**"".** Many answers were pointing to data annotation /high-quality data as the main problem. What to expect when searching for an annotation partner and what are your experiences?",6,5,False,self,,,,,
390,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,1,b9tq3u,self.MachineLearning,CNN variable input size?,https://www.reddit.com/r/MachineLearning/comments/b9tq3u/cnn_variable_input_size/,Technomancerer,1554483592,[removed],0,1,False,self,,,,,
391,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,2,b9u421,mcv-m6-video.github.io,"Lectures on Deep Learning for Video, Master in Computer Vision Barcelona 2019",https://www.reddit.com/r/MachineLearning/comments/b9u421/lectures_on_deep_learning_for_video_master_in/,xavigiro,1554485475,,0,1,False,default,,,,,
392,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,2,b9uc22,deepmind.com,Open Source | DeepMind,https://www.reddit.com/r/MachineLearning/comments/b9uc22/open_source_deepmind/,tigerneil,1554486562,,0,3,False,default,,,,,
393,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,2,b9ufgt,self.MachineLearning,[D]Tools &amp; Tips for labeling my own audio dataset to build a trigger word detection model?,https://www.reddit.com/r/MachineLearning/comments/b9ufgt/dtools_tips_for_labeling_my_own_audio_dataset_to/,RedditAcy,1554487040,"Hey guys, I would like to automate my process of editing my videos on youtube. I say ""uh"" and other filler words a lot, so I would like to build a trigger detection model to divide an imported mp4 automatically into different mp4s by the trigger words (uh's, umm's, hmm's). 

I think I also have a bit of accent, so I would like to record my own uh's and umm's for the best training result (this gotta be the weirdest thing I have ever done), what are some tips you guys can grant me on doing that? Are there any GUIs &amp; applications to label my audio data easier? If not, what should be the general process of labeling those data (I would imagine I will put 0's for noises other than the trigger words and 1's for the time frame with the trigger words)? How much data should I record before using data augmentation and techniques akin to that? 

Thanks!",6,3,False,self,,,,,
394,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,3,b9uv90,medium.com,DeepMind AI Flunks High School Math Test,https://www.reddit.com/r/MachineLearning/comments/b9uv90/deepmind_ai_flunks_high_school_math_test/,Yuqing7,1554489221,,0,1,False,default,,,,,
395,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,3,b9uvrw,self.MachineLearning,Using Data Science to Predict Response Times of Firefighters,https://www.reddit.com/r/MachineLearning/comments/b9uvrw/using_data_science_to_predict_response_times_of/,cyrilp21,1554489292,[removed],0,1,False,self,,,,,
396,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,3,b9v011,medium.com,[P] Using Data Science to Predict Response Times of Firefighters,https://www.reddit.com/r/MachineLearning/comments/b9v011/p_using_data_science_to_predict_response_times_of/,cyrilp21,1554489891,,0,1,False,https://a.thumbs.redditmedia.com/N2eL8QivPrr-TNuig6LTRtBFArrQL3e1tME3-Q1NOX4.jpg,,,,,
397,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,4,b9v9k6,thecoderr.com,The Coder | Cat Vs Dog Classification Using Edge Detection And Other Techniques.,https://www.reddit.com/r/MachineLearning/comments/b9v9k6/the_coder_cat_vs_dog_classification_using_edge/,the_coder_dot_py,1554491188,,0,1,False,default,,,,,
398,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,4,b9vmlx,self.MachineLearning,[D] Making the best out of an AI residency,https://www.reddit.com/r/MachineLearning/comments/b9vmlx/d_making_the_best_out_of_an_ai_residency/,TheRedSphinx,1554493014,"Hi all,

&amp;#x200B;

I was recently accepted into the Google AI residency. Needless to say, I'm beyond excited and honored to have made it. My dream would be to continue doing research in some fashion after the residency. Currently, I have lots of free time until the residency starts (July) and I want to prepare myself so that I can make the best out of it and get an awesome job afterwards.

&amp;#x200B;

For those of you who've done residency, could you share your experience? What are things you wish you would have known before hand? Things you wish you would have done earlier? Things you found that really helped you during the residency and beyond? 

&amp;#x200B;

For those of you who hire AI residents, what are things that really impressed you about the resident during their residency? Or what kind of experience/knowledge would you say complements the residency? 

&amp;#x200B;

For what it's worth, I have a PhD in probability theory, and will probably be doing NLP research during the residency.  I'm familiar with deep learning (at the level of The Deep Learning Book) as well as traditional ML (at the level of ESL). I'm not super familiar with the NLP literature in particular, but I know the basics very well e.g. word2vec, Glove, BERT, etc. I'm a decent coder for an academic, though I don't have any industrial software engineering experience. ",20,44,False,self,,,,,
399,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,4,b9vn1q,medium.com,A Kickstarter Platform For Training AI Models?,https://www.reddit.com/r/MachineLearning/comments/b9vn1q/a_kickstarter_platform_for_training_ai_models/,alvisanovari,1554493079,,0,1,False,https://a.thumbs.redditmedia.com/PQCQO4ZXxNLEgWa20X0rrokK8-l4itTnAjFrAncHoC4.jpg,,,,,
400,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,5,b9vxhz,link.springer.com,K-polytopes: A superproblem of k-means,https://www.reddit.com/r/MachineLearning/comments/b9vxhz/kpolytopes_a_superproblem_of_kmeans/,symryn,1554494539,,0,1,False,default,,,,,
401,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,5,b9w9hh,udemy.com,90% Off deal udemy course couponcode for Clustering &amp; Classification With Machine Learning In R,https://www.reddit.com/r/MachineLearning/comments/b9w9hh/90_off_deal_udemy_course_couponcode_for/,crowdfundinghelp24,1554496245,,0,1,False,default,,,,,
402,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,6,b9wvv1,youtube.com,How to Study MACHiNE LEARNING | XUCHAT,https://www.reddit.com/r/MachineLearning/comments/b9wvv1/how_to_study_machine_learning_xuchat/,jeffxu999,1554499547,,0,1,False,default,,,,,
403,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,6,b9x2sw,self.MachineLearning,Creating machine learning models via Excel spreadsheets!,https://www.reddit.com/r/MachineLearning/comments/b9x2sw/creating_machine_learning_models_via_excel/,abbasshujah,1554500596,[removed],0,1,False,self,,,,,
404,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,6,b9x92o,self.MachineLearning,Is the PNY NVIDIA Quadro RTX 4000 a good GPU for Machine Learning on Linux?,https://www.reddit.com/r/MachineLearning/comments/b9x92o/is_the_pny_nvidia_quadro_rtx_4000_a_good_gpu_for/,andrewcrayden,1554501535," 

As a web developer, I am growing increasingly interested in data science/machine learning, enough that I have decided to build a lab at home.

I have discovered the Quadro RTX 4000, and am wondering how well it would run ML frameworks on Ubuntu Linux. Are the correct drivers available on Linux so that this card can take advantage of ML frameworks?

[LINUX X64 (AMD64/EM64T) DISPLAY DRIVER](https://www.nvidia.com/Download/driverResults.aspx/145182/en-us)

This is the only driver that I could find, but it is a ""Display Driver"", so I am not sure if that enables ML frameworks to use this GPU for acceleration. Will it work for Intel based processors?

Any guidance would be greatly appreciated.",0,1,False,self,,,,,
405,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,7,b9xf3f,self.MachineLearning,Is the PNY NVIDIA Quadro RTX 4000 a good GPU for Machine Learning on Linux?,https://www.reddit.com/r/MachineLearning/comments/b9xf3f/is_the_pny_nvidia_quadro_rtx_4000_a_good_gpu_for/,andrewcrayden,1554502431,[removed],0,1,False,self,,,,,
406,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,8,b9yaut,vinsloev.blogspot.com,Artificial Intelligence - How it could lead to a greener and healthier world,https://www.reddit.com/r/MachineLearning/comments/b9yaut/artificial_intelligence_how_it_could_lead_to_a/,EddyTheDad,1554507628,,0,1,False,default,,,,,
407,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,9,b9yi4j,self.MachineLearning,Hello. A high school student asking for some advice on majors.,https://www.reddit.com/r/MachineLearning/comments/b9yi4j/hello_a_high_school_student_asking_for_some/,Iwouldloveto6974,1554508893,[removed],0,1,False,self,,,,,
408,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,9,b9ykps,self.MachineLearning,[D] Lectures similar to CS231n applied to LSTM,https://www.reddit.com/r/MachineLearning/comments/b9ykps/d_lectures_similar_to_cs231n_applied_to_lstm/,Kenny_smash,1554509366,"Hello everyone, there is a online (free) course focused in LSTM similar to the CS231n (CNN)? I struggle a lot with most of the videos trying to explain LSTM, they seem to go too complex very fast.",2,1,False,self,,,,,
409,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,9,b9ypg3,self.MachineLearning,[D] Looking for Random Forest overfitting dataset example,https://www.reddit.com/r/MachineLearning/comments/b9ypg3/d_looking_for_random_forest_overfitting_dataset/,pp314159,1554510171,"I'm looking for example dataset on which Random Forest will overfit. I've seen different statements that RF does or does nt overfit. I would like to have example dataset on which RF overfit. I've found the [article](https://cloudfront.escholarship.org/dist/prd/content/qt35x3v9t4/qt35x3v9t4.pdf) by Mark Segal about benchmarking RF in which he shows how RF overfit on synthetic dataset. However, I am not able to reproduce his results with Random Forest from sklearn. Here is my colab [link](https://colab.research.google.com/drive/1m6d2JSYOwThpxMhYvM44l783QIwng_n1) ",5,1,False,self,,,,,
410,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,10,b9z1p6,youtube.com,How Neural Networks Work- Simply Explained by a Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/b9z1p6/how_neural_networks_work_simply_explained_by_a/,ailearn12,1554512427,,0,1,False,https://b.thumbs.redditmedia.com/lAiMzjTyr2KqqhRGgJUVMzQdwU2_M_RzNgdeOAGU2eY.jpg,,,,,
411,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,11,b9zl53,quora.com,"A powerful criticism of ""The AI Revolution"" by waitbutwhy.",https://www.reddit.com/r/MachineLearning/comments/b9zl53/a_powerful_criticism_of_the_ai_revolution_by/,ChierHu,1554516232,,0,1,False,default,,,,,
412,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,11,b9zxsu,self.MachineLearning,Good enough for grad school?,https://www.reddit.com/r/MachineLearning/comments/b9zxsu/good_enough_for_grad_school/,NoodleExpert,1554518762,[removed],0,1,False,self,,,,,
413,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,12,ba0c04,self.MachineLearning,Any statisticians or machine learning experts out there want to work with me on a very cool data project on sports card valuations?,https://www.reddit.com/r/MachineLearning/comments/ba0c04/any_statisticians_or_machine_learning_experts_out/,czeldin,1554521789,[removed],1,1,False,self,,,,,
414,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,13,ba0qs6,medium.com,"""PyTorch: Zero to GANs"" - Coding-focused tutorials on Deep Learning with PyTorch",https://www.reddit.com/r/MachineLearning/comments/ba0qs6/pytorch_zero_to_gans_codingfocused_tutorials_on/,aakashns,1554525009,,1,1,False,default,,,,,
415,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,14,ba12cu,i.redd.it,t-SNE on Album Cover Latent Space using a VAE,https://www.reddit.com/r/MachineLearning/comments/ba12cu/tsne_on_album_cover_latent_space_using_a_vae/,CzoKc,1554527669,,0,1,False,default,,,,,
416,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,14,ba1b2d,self.MachineLearning,Interface considerations when coding an RTS videogame AI.,https://www.reddit.com/r/MachineLearning/comments/ba1b2d/interface_considerations_when_coding_an_rts/,santvarc,1554529788,[removed],0,1,False,self,,,,,
417,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,16,ba1wg5,self.MachineLearning,[D] Thoughts about super-convergence and highly-performant deep neural network parameter configurations,https://www.reddit.com/r/MachineLearning/comments/ba1wg5/d_thoughts_about_superconvergence_and/,Miejuib,1554535212,"Alright, so the point of this post is that I noticed a parallel between two recent papers, and I have a hypothesis that they are actually closely connected.  I'd like to hear any thoughts you guys may have, and I'm happy to receive criticism.  


The relevant papers are these:

ref\_1: [https://arxiv.org/pdf/1708.07120.pdf](https://arxiv.org/pdf/1708.07120.pdf)

\`\`\`Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\`\`\` 

ref\_2: [https://arxiv.org/pdf/1812.11118.pdf](https://arxiv.org/pdf/1812.11118.pdf)

\`\`\`Reconciling modern machine learning and the bias-variance trade-off\`\`\`  


\------------------------------------  
So obviously the proposed technique for super-convergence in ref\_1 is pretty cool on its own, but that's not actually the thing that really caught my eye.  One of the things I found the most interesting about the paper was a very striking artifact in another figure that they didn't discuss in the paper:

&amp;#x200B;

[img1](https://i.redd.it/kncyqpbn6lq21.png)

Now, when I saw those val accuracy curves, I was immediately reminded of another very cool paper that was published recently, that is- ref\_2, the paper discussing the bias-variance trade-off and its proposal of a 'double descent' risk curve:

[img2](https://i.redd.it/g4q983jk7lq21.png)

And further, the next figure presented in that same paper ( ref\_2 ), where the authors plotted metrics of architectures at various degrees of over-parameterization:  


[img3](https://i.redd.it/1bkns8uu7lq21.png)

I know there's no direct evidence that the two behaviors are actually related, but I think that would be a very possible, and potentially very interesting connection between the two phenomenons, since it would speak to the nature of the loss landscape for a given architecture/task, and how the gradient-descent paths through it might be affected by varying the dimension of the parameter space / the learning rate.  


&amp;#x200B;

The main hypothesis I'd like to conjecture here is that there may be a somewhat common (perhaps even creeping towards global) quality of the structure of natural information that, when handled by learning systems/algorithms, results in what's essentially a potential energy barrier 'shell' separating a parameter configuration space (for a given architecture) that can only generalize moderately well, and a parameter configuration space (of the same architecture) that is capable of greatly improved generalization and performance.  


&amp;#x200B;

So for example, when greatly over-parameterizing a network as in ref\_2, after some threshold, it's my suspicion that the models essentially become capable of doing a learning-algorithm equivalent of tunneling through that potential energy barrier simply as a result of having enough degrees of freedom such that the model is able to ""poke holes"" through the loss landscape ( perhaps with some percolation threshold-like property that is dependent on the degree of over-parameterization.  ie: for thicker potential energy barriers, excessively redundant degrees of freedom in the parameter space may act like short, randomly aligned 'holes' throughout the space, a critical density of which is required to be able to successfully tunnel through the barrier )

  


And then the alternative tunneling method (as is relevant to the super-convergence paper, ref\_1 ) being that the model first hits the ""moderate quality"" parameter configuration space, and then because of the cosine learning rate schedule decaying down to basically nothing, it finds the point in its local proximity (its local minimum) that minimizes the magnitude of the potential energy barrier separating it from the ""highly-performant"" parameter configuration space.  


&amp;#x200B;

Then as the learning rate ramps up again, it turns the trajectory of the parameter configuration into what's essentially a destructively resonant system, and in doing so, is able to tunnel through the potential energy barrier that way, inadvertently benefiting from what's usually considered 'bad' / divergent behavior of the optimizer such as when the value of the learning rate is too high, and causes the loss to increase until the model dies from NaNs.  


&amp;#x200B;

Which I guess could even mean that instead of the first cycle of the cosine decay causing the benefit of finding the 'best local minimum', what might instead be happening is that during the ramp-down phase, it could be more about restricting the extra degrees of freedom in the parameter space, so that when the learning rate ramps up again, it doesn't accidentally throw itself into actual, complete and utter divergence/destruction regions of the parameter space.  


  
For a mental image, I picture that as something like a space ship's airlock, and during the first cycle's decay period, it's essentially 'closing the door behind it' as it 'enters the airlock', so that when the learning rate ramps up again, it becomes much more difficult for it to return the way it came from, leaving the only remaining option being to tunnel forwards through the barrier in the 'opposite direction', which leads to the region of the parameter space that is capable of significantly improved model performance and generalization capability.  


&amp;#x200B;

( I apologize for the description of the mental image since I know that's pretty non-kosher when discussing the structure of highly dimensional topologies, but it helped me structure my thoughts on the matter, so I figured it might help a reader here also )  


  
So that's the idea I'd love to hear thoughts about.  When reading new publications and running my own experiments, in addition to the concepts explicitly being described or tested, I also generally try to build my understanding of what the topological structure of natural information may be like, and how its structure may affect the ability of learning algorithms to interact with it.  ( Of course there are plenty of papers discussing the general capability to parameterize and substantially modify the loss landscape, but for the case of this hypothesis, I am choosing to not address those modifications in favor of opting to treat the distribution of the natural information and its corresponding loss landscape in as unmodified a manner as possible, since such parameterizations can, from my understanding, significantly alter the convergence rate of algorithms like gradient descent )  


&amp;#x200B;

What do you guys think?  Is this just a completely bat-shit crazy and obviously wrong idea for reasons that I don't understand yet?  Could it actually hold some weight, and serve as an (at least somewhat) accurate description of how the models actually traverse the loss landscapes?  Is there some good way you guys can think of that this hypothesis could be experimentally tested in a falsifiable and repeatable manner?  Do you have any other thoughts on the matter or know of references to other publications that are closely tied to the discussed concepts?  


&amp;#x200B;

Let me know, I'd love to hear your input, and I don't mind being wrong if it means it's an opportunity to learn something new.  

&amp;#x200B;",39,127,False,https://b.thumbs.redditmedia.com/XA-cBOrHhQdDO6uwlhNa6iouo_5KCwvtKrOvbnIZW9w.jpg,,,,,
418,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,17,ba2dlr,lhd.co.com,We are the leading manufacturer for STACKER CRANE  AS/RS FOR PALLETS worldwide.,https://www.reddit.com/r/MachineLearning/comments/ba2dlr/we_are_the_leading_manufacturer_for_stacker_crane/,lhd121,1554540034,,0,1,False,default,,,,,
419,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,17,ba2fug,self.MachineLearning,We are the leading manufacturer for STACKER CRANE  AS/RS FOR PALLETS world wide.,https://www.reddit.com/r/MachineLearning/comments/ba2fug/we_are_the_leading_manufacturer_for_stacker_crane/,lhd121,1554540735,[removed],0,1,False,https://b.thumbs.redditmedia.com/6g8vBJCoBHQ92ZCC-GGGwHxMJ8-oRlBSUumVXVRUhzA.jpg,,,,,
420,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,18,ba2mr1,self.MachineLearning,[P] How to implement a deep learning model with sentiment analysis for price data?,https://www.reddit.com/r/MachineLearning/comments/ba2mr1/p_how_to_implement_a_deep_learning_model_with/,Seankala,1554542717,"Hello. I've been making posts on here quite often recently regarding the project I'm working on (sentiment analysis for gauging the relationship between Twitter sentiment and Bitcoin prices) and it's been a great help. All I need to do is set up a model.

I've been reading papers regarding Twitter analysis and traditional stock markets, and it seems that in many cases researchers compare the performance of neural network models (e.g. LSTM) and more conventional machine learning models (e.g. SVM). What I'm wondering is how do I implement this using an LSTM?

Right now my data consists of two CSV files: one with Twitter data (date, polarity, subjectivity, likes\_count, replies\_count) and one with price data (Coinbase BTC prices with date and closing price columns).

I know the question is rather broad, but I was wondering if someone could perhaps point me in a direction to go? Or if there are any blog posts or articles that I could reference.

Thanks. ",14,1,False,self,,,,,
421,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,19,ba2x89,self.MachineLearning,"Automation, an augmentation of computerized reasoning",https://www.reddit.com/r/MachineLearning/comments/ba2x89/automation_an_augmentation_of_computerized/,patronageinstitute,1554545673,[removed],0,1,False,self,,,,,
422,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,19,ba3184,self.GeneticProgramming,How to speedup back propagation,https://www.reddit.com/r/MachineLearning/comments/ba3184/how_to_speedup_back_propagation/,ToolTechSoftware,1554546761,,0,1,False,default,,,,,
423,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,19,ba31fc,self.MachineLearning,[D] Memory in instance segmentation for videos,https://www.reddit.com/r/MachineLearning/comments/ba31fc/d_memory_in_instance_segmentation_for_videos/,IborkedyourGPU,1554546817,"When semantic segmentation models designed for pictures are applied to videos, they exhibit some obvious errors like the one shown in the video below:

https://www.youtube.com/watch?v=GvOBXrQjQxw&amp;list=PLX-LrBk6h3wRAF22jBUxDgOvyhIgLN4Cg&amp;index=3

The car is identified as a boat in some frames. This happens because the model used for instance segmentation is Mask R-CNN, and it creates masks separately for each frame, so it doesn't ""remember"" that in the frame before, a car was detected more or less at the same location. Are there models which take into account historical information when segmenting videos? I'm sure (well, I hope) that models in self-driving cars take this into account.",2,2,False,self,,,,,
424,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,20,ba38q2,self.MachineLearning,[P] I need some advice on the project i am starting to work on.,https://www.reddit.com/r/MachineLearning/comments/ba38q2/p_i_need_some_advice_on_the_project_i_am_starting/,Daniel_Potter,1554548815,"I am working on the object detection project (solo). I have found a dataset, which is \~550 GBs, 1.7 million images (Google's Open Images Dataset V4). The first problem i run into is will i be able to use the full dataset. Machines at our uni have 16 GBs of RAM, 1060 (6GBs i suppose), SSD (i assume that will speed up the model making process). How big of a model can 16 GBs handle? What subset can this machine handle?

My other alternative is google colab, which i understand is completely free? It provides a tesla k80 and 12 gigs of ram. I looked up the performance difference, and it seems that k80 seems to be slower than 1060, and 1080Ti is even 2.5 faster than k80.

Where will i be storing the 550 GBs of data though. How does google colab work actually? Do i upload my dataset everytime i need to make a model? What's the access time if i use a cloud service? 

I started downloading the dataset btw. Will take 17 hours if my speed is consistent.

&amp;#x200B;

&amp;#x200B;",2,2,False,self,,,,,
425,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,20,ba394o,self.MachineLearning,Machine Learning : Few rarely shared trade secrets,https://www.reddit.com/r/MachineLearning/comments/ba394o/machine_learning_few_rarely_shared_trade_secrets/,andrea_manero,1554548928,[removed],0,1,False,self,,,,,
426,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,21,ba3w8b,self.MachineLearning,Is it necessary to cite google Colab?,https://www.reddit.com/r/MachineLearning/comments/ba3w8b/is_it_necessary_to_cite_google_colab/,farahaniali,1554554322,[removed],0,1,False,self,,,,,
427,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,23,ba4mkf,self.MachineLearning,Best binary classifation method for determining if a string contains mathematical expression or not?,https://www.reddit.com/r/MachineLearning/comments/ba4mkf/best_binary_classifation_method_for_determining/,123abcboredme,1554559582,[removed],0,1,False,self,,,,,
428,MachineLearning,t5_2r3gv,2019-4-6,2019,4,6,23,ba50ih,self.MachineLearning,Is it possible to learn machine learning enough in 2 years to possibly do a PhD in it?,https://www.reddit.com/r/MachineLearning/comments/ba50ih/is_it_possible_to_learn_machine_learning_enough/,DarthHelmet123,1554562064,[removed],0,1,False,self,,,,,
429,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,0,ba59da,self.MachineLearning,[D] Video Dialogue Replacement: World leaders singing 'Imagine',https://www.reddit.com/r/MachineLearning/comments/ba59da/d_video_dialogue_replacement_world_leaders/,reobb,1554563509,"Hi, I'm from Canny AI, this is a project we did to showcase the potential of the technology:

[https://www.youtube.com/watch?v=R4UxVmYKiGA](https://www.youtube.com/watch?v=R4UxVmYKiGA)",37,187,False,self,,,,,
430,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,1,ba63h3,self.MachineLearning,It's there a for dummies book on machine learning?,https://www.reddit.com/r/MachineLearning/comments/ba63h3/its_there_a_for_dummies_book_on_machine_learning/,ca3games,1554568342,[removed],0,1,False,self,,,,,
431,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,1,ba6c7g,self.MachineLearning,[Discussion] How do you manage and keep track of your experiments?,https://www.reddit.com/r/MachineLearning/comments/ba6c7g/discussion_how_do_you_manage_and_keep_track_of/,sudomakemelunch,1554569686,"We tend to have a bunch of experiments running for a number of projects at once, and it gets hard to keep track of at times. I've just realized that I have no means of systematically managing my experiments, so I was wondering - how do you do it?

Do you use an external library? Do you keep a spreadsheet? Do you have a convention for naming your logs and structuring your repositories? Any other tips?

Thanks in advance!",19,17,False,self,,,,,
432,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,3,ba72yr,self.MachineLearning,[P] sewar: All image quality metrics you need in one package.,https://www.reddit.com/r/MachineLearning/comments/ba72yr/p_sewar_all_image_quality_metrics_you_need_in_one/,andrewekhalel,1554573780,"[sewar](https://github.com/andrewekhalel/sewar) is a python package for image quality assessment. It supports cli too.

Implemented metrics:
- [x] Mean Squared Error (MSE) 
- [x] Root Mean Sqaured Error (RMSE)
- [x] Peak Signal-to-Noise Ratio (PSNR) [[1]](https://ieeexplore.ieee.org/abstract/document/1284395/)
- [x] Structural Similarity Index (SSIM) [[1]](https://ieeexplore.ieee.org/abstract/document/1284395/)
- [x] Universal Quality Image Index (UQI) [[2]](https://ieeexplore.ieee.org/document/995823/)
- [x] Multi-scale Structural Similarity Index (MS-SSIM) [[3]](https://ieeexplore.ieee.org/abstract/document/1292216/)
- [x] Erreur Relative Globale Adimensionnelle de Synthse (ERGAS) [[4]](https://hal.archives-ouvertes.fr/hal-00395027/)
- [x] Spatial Correlation Coefficient (SCC) [[5]](https://www.tandfonline.com/doi/abs/10.1080/014311698215973)
- [x] Relative Average Spectral Error (RASE) [[6]](https://ieeexplore.ieee.org/document/1304896/)
- [x] Spectral Angle Mapper (SAM) [[7]](https://ntrs.nasa.gov/search.jsp?R=19940012238)
- [x] Spectral Distortion Index (D_lambda) [[8]](https://www.ingentaconnect.com/content/asprs/pers/2008/00000074/00000002/art00003)
- [x] Spatial Distortion Index (D_S) [[8]](https://www.ingentaconnect.com/content/asprs/pers/2008/00000074/00000002/art00003)
- [x] Quality with No Reference (QNR) [[8]](https://www.ingentaconnect.com/content/asprs/pers/2008/00000074/00000002/art00003)
- [x] Visual Information Fidelity (VIF) [[9]](https://ieeexplore.ieee.org/abstract/document/1576816/)

Contributions are welcomed!",7,15,False,self,,,,,
433,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,3,ba7870,self.MachineLearning,[D] Instance segmentation for X-ray scans,https://www.reddit.com/r/MachineLearning/comments/ba7870/d_instance_segmentation_for_xray_scans/,Horror_Counter,1554574607,"I'd like to implement a defect detection system for X-ray scans of machine parts. Which implementation of an instance segmentation model would you suggest me to use, i.e., which GitHub repository should I clone? I need something which is ofc accurate, but above all *robust*, since it will (hopefully) go in production. 

When I was up-to-date on this stuff, it seemed that Mask R-CNN was all the rage, but now according to https://paperswithcode.com/task/semantic-segmentation, it looks like it's not even in the top-tier. Also, I'm looking for an implementation in some framework which I understand (Tensorflow, Keras or PyTorch), not Caffe. Thanks",18,2,False,self,,,,,
434,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,4,ba81xj,self.MachineLearning,"Ethics, Bias and Job displacement within AI, DL &amp; ML and its implications",https://www.reddit.com/r/MachineLearning/comments/ba81xj/ethics_bias_and_job_displacement_within_ai_dl_ml/,Cameron_Kamer,1554579431,[removed],0,1,False,self,,,,,
435,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,4,ba87ds,self.MachineLearning,Opinions on this kmeans plot,https://www.reddit.com/r/MachineLearning/comments/ba87ds/opinions_on_this_kmeans_plot/,idonotknow9,1554580333,"Firstly I am quite new to k means clustering and to be honest I just followed this [tutorial](https://medium.com/@SAPCAI/text-clustering-with-r-an-introduction-for-data-scientists-c406e7454e76) and applied it to my data.

&amp;#x200B;

I have many documents (each point in the image below is a document. Just as in the blog post the top right is hierarchical clustering and the bottom left is density based clustering). The two blank spaces were models which could not run due to memory issues... so I ignore them.

&amp;#x200B;

My question to people who have more experience with k means, do you see anything in these plots to further inspect?

&amp;#x200B;

I followed this[tutorial](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/)to find the optimal number of clusters (which is 7 in this model).

&amp;#x200B;

I have just applied a TF-IDF, calculated the cosine similarity and then ran it through a k means model.

&amp;#x200B;

What other ideas would you suggest for classifying text based documents or creating word embeddings? I know of word2vec, doc2vec etc. but thought TF-IDF was a nice simple start.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/i4fvvj7x5pq21.png",0,1,False,https://b.thumbs.redditmedia.com/_aKqtBN2pFOcTkEAuk7bgwuXTbDUYdbX54Wnkg7ZTyk.jpg,,,,,
436,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,6,ba94me,self.MachineLearning,"Classification of scrambled images? ""[Discussion]""",https://www.reddit.com/r/MachineLearning/comments/ba94me/classification_of_scrambled_images_discussion/,ProfSchodinger,1554585867,"Hi, this is my first post on Reddit. I have a philosophical question for the ML community. I hope to initiate a nice thread...

&amp;#x200B;

If I were to scramble every image of a large dataset of labeled images with the same matrix (pixel 1 becomes pixel 428, pixel 2 becomes pixel 152 etc.) is there a way to perform image classification? This is to say, is there a way to approximate the correct convolutions of the first layer of a CNN when they are not obvious?  


Thanks,",12,2,False,self,,,,,
437,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,6,ba9cf0,self.MachineLearning,"[D][N] There's a Tensorflow 2.0 hackathon with $150,000 in prizes. Btw, my team is looking for 1-2 people if you're interested.",https://www.reddit.com/r/MachineLearning/comments/ba9cf0/dn_theres_a_tensorflow_20_hackathon_with_150000/,BatmantoshReturns,1554587192,"Hackathon page, projects are due in 29 days. 

https://tensorflow.devpost.com/

My group is looking for 1-2 people, if you're interested PM with some info about your TF experience and maybe a github as well. ",8,25,False,self,,,,,
438,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,7,ba9h3g,self.MachineLearning,[D] Tensoflow 2.0 vs. Keras,https://www.reddit.com/r/MachineLearning/comments/ba9h3g/d_tensoflow_20_vs_keras/,SirSwimmicus,1554588002,"Okay I'm just gonna come out and say it. I don't get it. All the marketing and Medium articles make Tensorflow 2.0 sound like everything has been streamlined (which would be greatly appreciated), but if you look at the API documentation nothing seems to have been taken out. The main difference I can see is that the tutorials now use tf.keras as the preferred method of doing things. I'm mostly okay with this as Keras is much more intuitive when it comes to building neural networks, but if they're using the tf.keras namespace, aren't we really just using Keras? I feel like I'm being tricked or something.

Now, I am admittedly something of a relative beginner when it comes to ML and TF especially so maybe I don't understand the nuances, but I would have thought that TF 2.0 would have changed the entire API to be more like that of Keras or PyTorch instead of just changing the docs to tell me to use tf.keras. Am I actually just using Keras with the ability to do more advanced things or is it still Tensorflow?

Sorry if this doesn't make a lot of sense or isn't the right place for this, I just feel like I'm not getting it.",63,161,False,self,,,,,
439,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,7,ba9u65,self.MachineLearning,How often is quality/availability of data a hurdle with an ML idea?,https://www.reddit.com/r/MachineLearning/comments/ba9u65/how_often_is_qualityavailability_of_data_a_hurdle/,bee4534,1554590256,,0,1,False,self,,,,,
440,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,7,ba9yws,self.MachineLearning,[R][1904.01774] Image Generation from Small Datasets via Batch Statistics Adaptation,https://www.reddit.com/r/MachineLearning/comments/ba9yws/r190401774_image_generation_from_small_datasets/,PuzzledProgrammer3,1554591108,[removed],0,1,False,self,,,,,
441,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,7,ba9z04,facebook.com,AI brings worldwide peace,https://www.reddit.com/r/MachineLearning/comments/ba9z04/ai_brings_worldwide_peace/,kokobannana,1554591126,,0,1,False,default,,,,,
442,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,7,baa1im,self.MachineLearning,Artificial Intelligence Question,https://www.reddit.com/r/MachineLearning/comments/baa1im/artificial_intelligence_question/,_thetruenorth_,1554591575,"Hey, I am a high school student doing a research report on the affects artificial intelligence will have on future employment. For the report, I need a few questions answered. I have provided a link for it, and I'd really appreciate it if you did it. Thanks!

[https://forms.gle/MhtVjbgVTEgAeE4b6](https://forms.gle/MhtVjbgVTEgAeE4b6) ",0,1,False,self,,,,,
443,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,8,baa7rg,youtu.be,Machine Learning Tutorial Part 1 | Machine Learning For Beginners,https://www.reddit.com/r/MachineLearning/comments/baa7rg/machine_learning_tutorial_part_1_machine_learning/,EddyTheDad,1554592696,,0,2,False,https://b.thumbs.redditmedia.com/oPqyTbcfHFvjaJQbAb2bjd5oph8OuPYGGS7mOwBXx5k.jpg,,,,,
444,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,9,baan46,self.MachineLearning,Stanford Machine Learning certificate,https://www.reddit.com/r/MachineLearning/comments/baan46/stanford_machine_learning_certificate/,yongjianfeng,1554595626,[removed],0,1,False,self,,,,,
445,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,9,baavxj,self.MachineLearning,[N] Stanford's CS230 with lecture videos and more,https://www.reddit.com/r/MachineLearning/comments/baavxj/n_stanfords_cs230_with_lecture_videos_and_more/,Bayequentist,1554597294,"Course Website: [CS230 Deep Learning](http://cs230.stanford.edu/)

Instructors: [Andrew Ng](https://www.andrewng.org/); [Kian Katanforoosh](https://www.linkedin.com/in/kiankatan/).

&gt;Deep Learning is one of the most highly sought after skills in AI. In this course, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. 

Here's the [Youtube playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb) of the lecture videos.

The programming assignments are from Andrew Ng's Coursera DL Specialization (which is behind a paywall). This [github repository](https://github.com/limberc/deeplearning.ai) contains all the empty Jupyter notebooks of the assignments.",31,335,False,self,,,,,
446,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,9,baawsl,self.MachineLearning,[P] skin cancer screening DEMO,https://www.reddit.com/r/MachineLearning/comments/baawsl/p_skin_cancer_screening_demo/,whria78,1554597464,"# http://rcnn.modelderm.com

 

The algorithm was composed of three parts, 1) The blob detector which detected possible lesion of interest and generated numerous raw blobs, 2) The fine image selector which dropped out inadequate image blobs and general object blobs, 3) The disease classifier which predicted diagnosis among 178 skin disorders.

1-1) Training Blob detector

The blob detector was trained with 21,421 malignant and benign nodular images of the ASAN dataset and 103,627 general object images of ImageNet. We trained the blob detector using faster RCNN, to suggest possible lesion of interest in face. After training the blob detector model, we set the threshold of the blob detector relatively low (CONF\_THRESH = 0.1, NMS\_THRESH = 0.5) to generate numerous 256x256 size blobs. We ran the blob detector with 19 sub-regions of different magnification of test image to screening small objects.

1-2) Training Fine Image Selector

Although the generated blobs using the blob detector included proper lesional blobs with adequate quality, they also included a lot of inadequate blobs such as trivial skin lesions (i.e. tiny nevus and mild inflammation), normal structures (i.e. eye and nose), and general objects (i.e. ear ring and glasses). In addition, some blobs were too blurry to analyze them correctly. To define the safe limits for analysis given the input, we created a fine image selector to classify generated blobs to 1) fine clinical image blob 2) inadequate blob 3) nonspecific or normal blob, and 4) general object blob. 

To create the fine image select, we generated hundreds thousands blobs from face images of the entire training dataset using the blob detector. Based on the image findings, we manually classify the blobs to 1) 81,030 fine blobs 2) 59,319 inadequate blobs 3) 221,953 normal or nonspecific blobs, and 4) 152,493 general object blobs. With these training images, we fine-tuned SE-ResNeXt-50 end-to-end. 

1-3) Training Disease Classifier

To reduce false positive for common benign disorders, we needed to acquire images of normal and benign disorders as many as possible. To create a large dataset, we utilized the region-based CNN (blob detector) which was created before the disease classifier. After manual annotation process, we obtained 276,099 normal or nonspecific blobs, 11,963 of acne blobs, 17,354 of lentigo blobs, 30,086 of melanocytic nevus blobs, and 8,252 of seborrheic keratosis blobs. The diagnosis of the secondary training dataset was tagged based on image findings. We trained temporal disease classifier with the primary training dataset, and then we utilized the temporal disease classifier to annotate remaining raw blobs efficiently.

For the training of the disease classifier, we used both the primary training dataset and the secondary dataset. The disease classifier was trained with 545,846 image crops of 178 disease classes. For image augmentation, we applied random crop, shift, and rotation with the original images, and built a training LMDB which consisted of 3,535,321 images. We fine-tuned SE-ResNeXt-50 end-to-end. ",2,2,False,self,,,,,
447,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,9,bab1os,self.MachineLearning,Self-play frameworks,https://www.reddit.com/r/MachineLearning/comments/bab1os/selfplay_frameworks/,Nimithryn,1554598410,[removed],0,1,False,self,,,,,
448,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,11,babr4j,self.MachineLearning,"PhD in Machine Learning and Computer Vision, possible universities",https://www.reddit.com/r/MachineLearning/comments/babr4j/phd_in_machine_learning_and_computer_vision/,pepxavier,1554603330,[removed],0,1,False,self,,,,,
449,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,11,babr7i,self.MachineLearning,Is there any way to solve this problem?,https://www.reddit.com/r/MachineLearning/comments/babr7i/is_there_any_way_to_solve_this_problem/,hari_vem,1554603344,[removed],0,1,False,self,,,,,
450,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,12,bac8ut,self.MachineLearning,Which of these textbooks should I start with first?,https://www.reddit.com/r/MachineLearning/comments/bac8ut/which_of_these_textbooks_should_i_start_with_first/,Serinous,1554606995,[removed],0,1,False,self,,,,,
451,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,12,bacf8m,self.MachineLearning,Can You Patent ML Use in Automating Entire Fields?,https://www.reddit.com/r/MachineLearning/comments/bacf8m/can_you_patent_ml_use_in_automating_entire_fields/,strangeattractors,1554608360,"I am curious if common ML algorithms can be patented when applied in automating a given field using already existing, commonly used methods within each field? For instance, could you patent a ML application of cooking using standard cooking techniques? Seems like this would severely limit innovation.",0,1,False,self,,,,,
452,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,12,baciqb,neuweg.co.in,How Spark enhances Machine learning? - Neuweg Technologies,https://www.reddit.com/r/MachineLearning/comments/baciqb/how_spark_enhances_machine_learning_neuweg/,diginews24,1554609124,,0,1,False,default,,,,,
453,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,12,backr7,self.MachineLearning,What is GAN actually learning?,https://www.reddit.com/r/MachineLearning/comments/backr7/what_is_gan_actually_learning/,GreatGBL,1554609558,"As we know, GAN is a powerful model and it tries to learn the distribution of given data.

When it is trained, I want to ask what is GAN actually learning?

It is learning the mapping function from Gaussian noise to the data distribution?

OR

It just employs the Gaussian input as the 'seed' attributes in the network?",0,1,False,self,,,,,
454,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,13,bacn69,self.MachineLearning,What will be the output of this statement? Please help me,https://www.reddit.com/r/MachineLearning/comments/bacn69/what_will_be_the_output_of_this_statement_please/,rjsingh41,1554610072,[removed],0,1,False,self,,,,,
455,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,13,bacoz0,self.MachineLearning,Advice for Convolutional Neuronal Network,https://www.reddit.com/r/MachineLearning/comments/bacoz0/advice_for_convolutional_neuronal_network/,CESARIUX2596,1554610452,[removed],0,1,False,self,,,,,
456,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,13,bacry6,reddit.com,Steel Scale | least count and type of scale,https://www.reddit.com/r/MachineLearning/comments/bacry6/steel_scale_least_count_and_type_of_scale/,GaugeHow,1554611081,,0,1,False,default,,,,,
457,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,13,bacs3i,self.MachineLearning,Can a patent be issued to prevent ML being used to automate an entire career?,https://www.reddit.com/r/MachineLearning/comments/bacs3i/can_a_patent_be_issued_to_prevent_ml_being_used/,strangeattractors,1554611114,[removed],0,1,False,self,,,,,
458,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,13,bacu42,self.MachineLearning,[D] [N] Top Machine Learning APIs That You Should Know (FEEL FREE TO SUGGEST MORE],https://www.reddit.com/r/MachineLearning/comments/bacu42/d_n_top_machine_learning_apis_that_you_should/,ai-lover,1554611579,"Here is a list of Machine learning APIs which we found useful for developers and ML Aspirants to learn and practise.

## PREDICTION BASED

[**MLJAR** ](https://mljar.com/)[**\[DOCS\]**](https://docs.mljar.com/)

[**Anaconda** ](https://docs.anaconda.com/)[**\[DOCS\]**](https://docs.anaconda.com/)

[**Blue Yonder Platform** ](https://www.blueyonder.ai/en)[**\[DOCS\]**](https://github.com/blue-yonder)[**M**](https://mljar.com/)

[**NuPIC** ](https://github.com/numenta/nupic)[**\[DOCS\]**](http://nupic.docs.numenta.org/)

[**Recombee** ](https://github.com/recombee/net-api-client)[**\[DOCS\]**](https://docs.recombee.com/)

[**BigML**  ](https://bigml.com/)[**\[DOCS\]**](https://bigml.com/developers)

[**indico** ](https://indico.io/)[**\[DOCS\]**](https://indico.io/docs/)

[**PredictionIO** ](http://predictionio.apache.org/)[**\[DOCS\]**](https://predictionio.apache.org/datacollection/eventapi/)

## FACE DETECTION/ RECOGNITION

[**Betaface** ](https://www.betafaceapi.com/wpa/)[**\[DOCS\]**](https://www.betafaceapi.com/wpa/index.php/documentation)

[**Kairos API \[DOCS\]**](https://www.kairos.com/docs/)

[**Animetrics Face Recognition** ](http://www.animetrics.com/)[**\[DOCS\]**](http://api.animetrics.com/documentation)

[**Eyedea Recognition** ](http://www.eyedea.cz/)[**\[DOCS\]**](http://face.eyedea.cz:8080/api/face/docs)

[**Imagga** ](https://imagga.com/)[**\[DOCS\]**](https://docs.imagga.com/)

## NLP RELATED

[**Yactraq Speech2Topics** ](https://yactraq.com/)[**\[DOCS\]**](https://yactraq.com/contact-trial/)

[**Aylien Text Analysis API \[DOCS\]**](https://rapidapi.com/aylien/api/text-analysis)

[**Wit.ai** ](https://wit.ai/)[**\[DOCS\]**](https://wit.ai/docs)

[**Bitext** ](https://www.bitext.com/)[**\[DOCS\]**](https://docs.api.bitext.com/)

[**IBMWatsonSTT API \[DOCS\]**](https://rapidapi.com/dimas/api/IBMWatsonSTT)

[**nlpTools** ](http://php-nlp-tools.com/)[**\[DOCS\]**](http://php-nlp-tools.com/documentation/)

[**Geneea** ](https://www.geneea.com/)[**\[DOCS\]**](https://api.geneea.com/)

[**Diffbot Analyze** ](https://www.diffbot.com/)[**\[DOCS\]**](https://www.diffbot.com/dev/docs/)

[**AlchemyText \[DOCS\]**](https://rapidapi.com/serg.osipchuk/api/AlchemyText)

[**MonkeyLearn** ](https://monkeylearn.com/)[**\[DOCS\]**](https://monkeylearn.com/api/v3/)

[**Hu:toma** ](https://www.hutoma.ai/)[**\[DOCS\]**](https://help.hutoma.ai/article/ym34wr87lx-hutoma-chat-api)

&amp;#x200B;",2,0,False,self,,,,,
459,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,13,bacvht,self.MachineLearning,Patenting ML Applications to Automate Entire Career Fields?,https://www.reddit.com/r/MachineLearning/comments/bacvht/patenting_ml_applications_to_automate_entire/,strangeattractors,1554611905,[removed],0,1,False,self,,,,,
460,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,15,badglt,self.MachineLearning,Machine Learning Institute In Bangalore,https://www.reddit.com/r/MachineLearning/comments/badglt/machine_learning_institute_in_bangalore/,SunilAhujaa,1554617068,[removed],0,1,False,self,,,,,
461,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,15,badn34,towardsdatascience.com,[R] Which Deep Learning Framework is Growing Fastest?,https://www.reddit.com/r/MachineLearning/comments/badn34/r_which_deep_learning_framework_is_growing_fastest/,waleedka,1554618762,,0,1,False,default,,,,,
462,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,15,badp01,self.MachineLearning,Which Deep Learning Framework is Growing Fastest?,https://www.reddit.com/r/MachineLearning/comments/badp01/which_deep_learning_framework_is_growing_fastest/,waleedka,1554619272,"Comparative study of the growth of TensorFlow, PyTorch, Keras, and FastAI.

[https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318](https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318) 

&gt; I looked at the number of job listings on Indeed, Monster, LinkedIn, and SimplyHired. 
&gt; I also evaluated changes in Google search volume, GitHub activity, Medium articles, ArXiv articles, and Quora topic followers. 
&gt; Overall, these sources paint a comprehensive picture of growth in demand, usage, and interest.",0,1,False,self,,,,,
463,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,15,badpem,arxiv.org,[R] Analysing Mathematical Reasoning Abilities of Neural Models,https://www.reddit.com/r/MachineLearning/comments/badpem/r_analysing_mathematical_reasoning_abilities_of/,downtownslim,1554619384,,1,14,False,default,,,,,
464,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,15,badpge,self.MachineLearning,[R] Which Deep Learning Framework is Growing Fastest?,https://www.reddit.com/r/MachineLearning/comments/badpge/r_which_deep_learning_framework_is_growing_fastest/,waleedka,1554619398,"Comparative study of the growth of TensorFlow, PyTorch, Keras, and FastAI.

\[[https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318](https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318)\]([https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318](https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318)) 

&amp;#x200B;

\&gt; I looked at the number of job listings on Indeed, Monster, LinkedIn, and SimplyHired. 

\&gt; I also evaluated changes in Google search volume, GitHub activity, Medium articles, ArXiv articles, and Quora topic followers. 

\&gt; Overall, these sources paint a comprehensive picture of growth in demand, usage, and interest.",13,20,False,self,,,,,
465,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,16,bae4nu,self.MachineLearning,[D] 10 Predictive Analytics Use Cases,https://www.reddit.com/r/MachineLearning/comments/bae4nu/d_10_predictive_analytics_use_cases/,seemingly_omniscient,1554623923," Predictive Analytics applications enable companies to identify and predict potential events and opportunities on time. 

&amp;#x200B;

Here are 10 Use Cases with explanations.

&amp;#x200B;

Article: [10 Predictive Analytics Use Cases](https://www.aisoma.de/10-predictive-analytics-use-cases/)

&amp;#x200B;

https://i.redd.it/rmuyqxhetsq21.jpg",0,0,False,https://b.thumbs.redditmedia.com/8TndI7jId0TbQO70mIXuEGLBt9tNAjztBwpI3VDDoQk.jpg,,,,,
466,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,18,baehji,self.MachineLearning,two questions about Density estimation using real NVP.,https://www.reddit.com/r/MachineLearning/comments/baehji/two_questions_about_density_estimation_using_real/,yuffon,1554628102,[removed],1,1,False,self,,,,,
467,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,18,baekkc,self.MachineLearning,Converting y_true and y_pred to numpy arrays in custom loss function in Keras,https://www.reddit.com/r/MachineLearning/comments/baekkc/converting_y_true_and_y_pred_to_numpy_arrays_in/,Andohuman,1554628975,[removed],0,1,False,self,,,,,
468,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,18,baema0,self.MachineLearning,What kind of data if accessible widely would lead to near term advancements in AI?,https://www.reddit.com/r/MachineLearning/comments/baema0/what_kind_of_data_if_accessible_widely_would_lead/,bee4534,1554629476,[removed],0,1,False,self,,,,,
469,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,18,baenou,self.MachineLearning,plots,https://www.reddit.com/r/MachineLearning/comments/baenou/plots/,DrNoodleHead,1554629866,[removed],0,1,False,self,,,,,
470,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,19,baeyru,self.MachineLearning,How do I separate characters in a scanned documents in OCR ?,https://www.reddit.com/r/MachineLearning/comments/baeyru/how_do_i_separate_characters_in_a_scanned/,thehumanlobster,1554632857,[removed],0,1,False,self,,,,,
471,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,20,bafdte,self.MachineLearning,What was your experience with machine learning?,https://www.reddit.com/r/MachineLearning/comments/bafdte/what_was_your_experience_with_machine_learning/,xyzabc123410000,1554636887,[removed],0,1,False,self,,,,,
472,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,21,bafkrn,self.MachineLearning,Is this still 'Machine Learning'?,https://www.reddit.com/r/MachineLearning/comments/bafkrn/is_this_still_machine_learning/,actuallynotcanadian,1554638594,[removed],0,1,False,self,,,,,
473,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,21,bafm6f,self.MachineLearning,How can facebook software recognize your face successfully with very few samples?,https://www.reddit.com/r/MachineLearning/comments/bafm6f/how_can_facebook_software_recognize_your_face/,Yahiabouda,1554638930,[removed],0,1,False,self,,,,,
474,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,22,bager1,self.MachineLearning,[P] How to plot decision boundary for 3 layer neural network,https://www.reddit.com/r/MachineLearning/comments/bager1/p_how_to_plot_decision_boundary_for_3_layer/,workaccount05,1554644988,"Hey,

I'm working on a project for school where i had feature vectors describing two different classes of wine data and using this to build a 3 layer neural network. 

I trained the classifier with a portion of the data and used the remaining to test the classifier and obtained the final weight vectors for input-hidden and hidden-output nodes through training.

While training the classifier using the training data set (which was labelled), I normalized the second class by multiplying its values by -1. I was wondering how I would plot the decision boundary for this? I have the final weight vectors.

Any suggestions?

Thanks!
",5,5,False,self,,,,,
475,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,22,baggpl,self.MachineLearning,[P] PyTorch Implementation: Exploring Randomly Wired Neural Networks for Image Recognition,https://www.reddit.com/r/MachineLearning/comments/baggpl/p_pytorch_implementation_exploring_randomly_wired/,seungwonpark,1554645370,"Recently, researchers at FAIR (including K. He) reported that randomly wired NNs perform better than or comparable to all existing hand-designed wiring(ResNet, ShuffleNet, etc.) and NAS-based results at ImageNet classification task: [https://arxiv.org/abs/1904.01569](https://arxiv.org/abs/1904.01569)

These randomly wired NNs looked really strange and had caught my eye.

So I decided to implement this in PyTorch and was able to reproduce some of their results.  
GitHub link: [https://github.com/seungwonpark/RandWireNN](https://github.com/seungwonpark/RandWireNN)

&amp;#x200B;

[Figure 4 from \\""Exploring Randomly Wired Neural Networks for Image Recognition\\"" \(1904.01569\)](https://i.redd.it/olp2w34ljuq21.png)

Implementing this paper was really amusing! I never imagined that I would use graph-related algorithms(BFS, adjacency list) while doing ML.

&amp;#x200B;

Note: Training is in progress - using an identical architecture with paper, I got 56.8% top-1 accuracy so far. It'll take some time to reach 74.7%, which was reported in the paper. I don't plan to use distributed SGD, but I hope to get comparable results using Adam/Adabound optimizer with learning rate decay.",25,178,False,self,,,,,
476,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,23,bagi6c,youtube.com,Introduction to Machine Learning Playlist - I started with these videos,https://www.reddit.com/r/MachineLearning/comments/bagi6c/introduction_to_machine_learning_playlist_i/,fkhan1995,1554645643,,0,1,False,https://b.thumbs.redditmedia.com/d_moI-CVB2jSjOhdUYg0L2ZEQzj6fGp7ILTL-14bINA.jpg,,,,,
477,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,23,bagnq6,self.MachineLearning,[P] StyleGAN trained on paintings (512x512),https://www.reddit.com/r/MachineLearning/comments/bagnq6/p_stylegan_trained_on_paintings_512x512/,_C0D32_,1554646617,"I did a ""quick&amp;dirty"" training run on paintings.

Sample of 999 generated images (512x512): [https://imgur.com/a/8nkMmeB](https://imgur.com/a/8nkMmeB)

Training data based on (only took images &gt;= 1024x1024 (\~30k)): [https://www.kaggle.com/c/painter-by-numbers/data](https://www.kaggle.com/c/painter-by-numbers/data)

Those where the model tries to generate faces don't look good, but I think most of the others do.",39,63,False,self,,,,,
478,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,23,bagqak,self.MachineLearning,"[P] autoray - write array backend agnostic code (numpy, tensorflow, autograd, jax, cupy, dask...)",https://www.reddit.com/r/MachineLearning/comments/bagqak/p_autoray_write_array_backend_agnostic_code_numpy/,jawknee400,1554647072,"Hi all, thought I'd share a little library I made for writing numeric code to automatically work for many different types of 'numpy-\*ish\*' arrays:

[https://github.com/jcmgray/autoray](https://github.com/jcmgray/autoray)

The essential idea is to perform single dispatch with a few cached 'translations' so that once you've written a function that works in numpy, you don't need to do anything at all to make it work for as many other array libraries as possible.

&amp;#x200B;

I had a look around and was kind of surprised that nothing this simple exists yet - though I am aware that certain libraries can now opt in (via \_\_array\_function\_\_) to working directly with numpy. 

&amp;#x200B;

Anyway contributions, thoughts or alternate suggestions very welcome!",2,14,False,self,,,,,
479,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,23,bah1os,self.MachineLearning,[P] Classify sensor data (multivariate time series) with Python's scikit-learn decision tree,https://www.reddit.com/r/MachineLearning/comments/bah1os/p_classify_sensor_data_multivariate_time_series/,falccon1,1554649012,"hi guys, 

i'm trying to apply scikit learns decision tree on the following dataset with the goal of classifying the data:

&amp;#x200B;

sensordata:

* multiple .csv files
* every .csv file has the same multiple sensors with same timestamp (see [here](https://imgur.com/a/T5zjmn5))
* each .csv file has one label (0 or 1) in another table

&amp;#x200B;

So far I've tried to train my decision tree with converting my DataFrames into Pandas Series. It worked,  but the decision tree couldn't differate the features/sensors. Is pandas  series the right approach for analyse data like this? Or does anyone  have another solution for this problem? 

&amp;#x200B;

Thanks. :)",8,4,False,self,,,,,
480,MachineLearning,t5_2r3gv,2019-4-7,2019,4,7,23,bah1xj,arxiv.org,FSNet: An Identity-Aware Generative Model for Image-based Face Swapping,https://www.reddit.com/r/MachineLearning/comments/bah1xj/fsnet_an_identityaware_generative_model_for/,spark_bowie,1554649051,,1,1,False,default,,,,,
481,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,0,bahatz,self.MachineLearning,Tensorflow inception V3 error,https://www.reddit.com/r/MachineLearning/comments/bahatz/tensorflow_inception_v3_error/,Mousman,1554650437,[removed],0,1,False,self,,,,,
482,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,1,bahx9z,self.MachineLearning,Machine Learning for Beginners - what I have been doing,https://www.reddit.com/r/MachineLearning/comments/bahx9z/machine_learning_for_beginners_what_i_have_been/,sshumiye,1554653916,[removed],0,1,False,self,,,,,
483,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,1,bai0sv,youtube.com,Videos and slides from ScaledML2019,https://www.reddit.com/r/MachineLearning/comments/bai0sv/videos_and_slides_from_scaledml2019/,rezabzadeh,1554654452,,0,1,False,default,,,,,
484,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,1,bai10u,self.MachineLearning,[P] Several GANs by TensorFlow 2.0 Alpha,https://www.reddit.com/r/MachineLearning/comments/bai10u/p_several_gans_by_tensorflow_20_alpha/,LynnHoHZL,1554654486,"These days I have updated two of my TensorFlow 1.x repositories to TensorFlow 2.0 Alpha,

1. CycleGAN: [https://github.com/LynnHo/CycleGAN-Tensorflow-2](https://github.com/LynnHo/CycleGAN-Tensorflow-2)
2. GANs: [https://github.com/LynnHo/DCGAN-LSGAN-WGAN-GP-DRAGAN-Tensorflow-2](https://github.com/LynnHo/DCGAN-LSGAN-WGAN-GP-DRAGAN-Tensorflow-2)

&amp;#x200B;

My largest feeling of TF2 is that everything becomes simple and easy to understand. I am a two years user of TF from 0.x to 1.x., and I had a hard time to learn and use TF at the very beginning because there are numerous manners to do the same thing. So it's annoying that different GitHub repositories have very different styles, e.g., using raw TF, tf.slim or TensorLayer. Fortunately, at 2.0, it seems everything becomes clean and there are relatively standard ways to write the codes.

&amp;#x200B;

Although the codes of the above repositories work well. I still have some problems, 

1. The gradient tape doesn't work when combining tf.ops and keras.Model under @tf.function, which seems to be a bug, see [https://github.com/tensorflow/tensorflow/issues/27455](https://github.com/tensorflow/tensorflow/issues/27455).
2. WGAN-GP with instance normalization in discriminator doesn't work (layer normalization or no normalization works). I cannot find out the cause so I will appreciate your help\~",4,27,False,self,,,,,
485,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,1,bai2cp,medium.com,[D] Using a wealthy gamblers race to approximate pi,https://www.reddit.com/r/MachineLearning/comments/bai2cp/d_using_a_wealthy_gamblers_race_to_approximate_pi/,rohitpandey576,1554654697,,0,1,False,https://a.thumbs.redditmedia.com/K0jfgu6LXdtj8hYjkUy4PI4BEtoVtlsD_GLf_pWrvv0.jpg,,,,,
486,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,2,baixru,self.MachineLearning,[P] Curated List of Python Resources,https://www.reddit.com/r/MachineLearning/comments/baixru/p_curated_list_of_python_resources/,semicolonator,1554659394,"Hi everybody,

I created a list \[0\] for resources for machine learning with Python.  It including not only libraries, but also links to tutorials, code snippets, blog posts and talks. Would like your feedback.

&amp;#x200B;

\[0\]  [https://github.com/r0f1/datascience](https://github.com/r0f1/datascience) 

&amp;#x200B;

Best, Flo",23,287,False,self,,,,,
487,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,3,baj7h0,github.com,Feature Engineering Cookbook for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/baj7h0/feature_engineering_cookbook_for_machine_learning/,michaelabehsera21,1554660784,,0,1,False,default,,,,,
488,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,3,bajjor,self.MachineLearning,"Trying to implement this paper for english emotional classification, using C4.5 algorithm. [""P""]",https://www.reddit.com/r/MachineLearning/comments/bajjor/trying_to_implement_this_paper_for_english/,ihateladyfingers,1554662526,[removed],0,1,False,self,,,,,
489,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,3,bajjq6,self.MachineLearning,[D] Proposal of Reforms to ICLR Review Process,https://www.reddit.com/r/MachineLearning/comments/bajjq6/d_proposal_of_reforms_to_iclr_review_process/,alexmlamb,1554662532,"Overall, I like that ICLR has experiments with a different type of reviewing process, but I think that it now suffers from a few forms of abuse.  One is that public comments can be posted by people who are biased (for example authors of competing papers) and are allowed at arbitrary times, which can have a big influence on reviewers if the timing is done in an adversarial way (for example, right before reviews are due).  

I think another issue is that the ICLR rebuttal period is ridiculously long (it's well over 1 month) and allows for a massive amount of text to be posted, when it doesn't tend to even make that big of a difference.  

Another bigger issue is that there are lots of zombie ""openreview"" pages that people keep citing to and linking to instead of the newest versions on arxiv.  I think this is a particularly bad issue because people do this for rejected papers, which often have dramatically improved versions posted elsewhere.  

&amp;#x200B;

1. Public comments are allowed up to 1 week prior to when reviews are due.  This means public comments can influence reviews, but that the authors always have a chance to respond before reviews close.  
2. Public comments should always be non-anonymous.  
3. Authors are only allowed one comment on each reviewer's review, to prevent authors from having to waste too much time.  
4. Dramatically shorten the rebuttal window to \~1 week to be in line with other conferences.  
5. Probably authors should be able to delete the openreview page after some amount of time.  

&amp;#x200B;

I think this would allow for the strengths of the ICLR system, because I do think public comments can help to bring a wider perspective to papers.  I also think making reviews and comments visible is useful for the community.  ",4,7,False,self,,,,,
490,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,3,bajlxh,self.MachineLearning,Research in GANs,https://www.reddit.com/r/MachineLearning/comments/bajlxh/research_in_gans/,be_Positiv,1554662852,[removed],0,1,False,self,,,,,
491,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,3,bajn5x,self.MachineLearning,Deep Learning With NO Code,https://www.reddit.com/r/MachineLearning/comments/bajn5x/deep_learning_with_no_code/,lomiag,1554663040,[removed],0,1,False,self,,,,,
492,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,4,bajsto,self.MachineLearning,Need some ideas about a two class problem,https://www.reddit.com/r/MachineLearning/comments/bajsto/need_some_ideas_about_a_two_class_problem/,orky7,1554663885,[removed],0,1,False,self,,,,,
493,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,5,bakew0,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 60,https://www.reddit.com/r/MachineLearning/comments/bakew0/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1554667205,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)||

Most upvoted papers two weeks ago:

/u/sander314: https://arxiv.org/pdf/1708.02002.pdf

/u/dondiegoalonso: https://arxiv.org/abs/1903.10404

/u/groovyJesus: [Convex Neural Networks(pdf)](https://www.iro.umontreal.ca/~lisa/pointeurs/convex_nnet_nips2005.pdf)

Besides that, there are no rules, have fun.",9,13,False,self,,,,,
494,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,5,baklyb,self.MachineLearning,Machine learning Console tool,https://www.reddit.com/r/MachineLearning/comments/baklyb/machine_learning_console_tool/,Dev_indigo,1554668259,[removed],0,1,False,self,,,,,
495,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,6,bal3tk,self.MachineLearning,Tutorial on Machine learning for total beginning in Python recommendation,https://www.reddit.com/r/MachineLearning/comments/bal3tk/tutorial_on_machine_learning_for_total_beginning/,yes4me2,1554671029,[removed],1,1,False,self,,,,,
496,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,7,balrsm,self.MachineLearning,Is it weird that I verbally cheer my neural network after every epoch for getting more classifications right?,https://www.reddit.com/r/MachineLearning/comments/balrsm/is_it_weird_that_i_verbally_cheer_my_neural/,charliebrown3011,1554674834,[removed],0,1,False,self,,,,,
497,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,8,bamezf,self.MachineLearning,How much knowledge of ML is required to get undergrad research internship this summer?,https://www.reddit.com/r/MachineLearning/comments/bamezf/how_much_knowledge_of_ml_is_required_to_get/,cipher1202,1554678520,"Im a junior year computer science engineering undergraduate studying in a small university in India where is ni any research facility and senior/faculty guidance. No research culture here.

Im lately interested in machine learning/AI and want to get my hand dirty in research experience. I have started taking Stanfords CS229 ML video lectures after revising required maths and will be done by May end or June first week.

Is it now possible to get summer research internship in June or July as I have no attendance policy at my college and can extend it to even fall semester?
Where and how should I apply? How much knowledge will be required?

Or instead should I spend my summer vacations in studying optimization and statistical machine learning textbook will working independently on a research project (which I think will be hard and bad) for the whole fall semester and then apply in the winter?
Thanks ",0,1,False,self,,,,,
498,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,8,bamocy,self.MachineLearning,ROC Curve not smooth,https://www.reddit.com/r/MachineLearning/comments/bamocy/roc_curve_not_smooth/,hodwill,1554680113,"So Im currently doing some biometric work in Matlab. I extract HOG features from images for each person and then train a SVM classifier. Then presenting with features of a new image using predict, it classifies them into one one of the classes ie assigns it to a person. Im currently thresholding on the hinge binary loss of the class to see whether its accepted or not. Some people are left out of training to test the genuine reject rate etc.  

Taking a file full of actual labels, predicted labels and results (And knowing which are genuine accepts/genuine rejects etc) Im creating the ROC curve. (Following the definitions on here https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) 

Anyway, when plotting my ROC curve there are large jumps in the y direction and the corner is quite rough. Is this because the step between each threshold test is too big? These values range from around 10^-8 to 10^-2 so Ive tried to make my steps as small as possible. Or is this because of other reasons? Thanks in advance! 
",0,1,False,self,,,,,
499,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,8,bamqku,self.MachineLearning,[D] Do convolution weights converge faster than FC weights?,https://www.reddit.com/r/MachineLearning/comments/bamqku/d_do_convolution_weights_converge_faster_than_fc/,ME_PhD,1554680478,"Does anyone know of any papers that look into which layers of a ConvNet have their parameters converge quickest during training?

Do the convolution filters converge first, or the matrices in the FC layers? Also do early layers converge later?",11,13,False,self,,,,,
500,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,9,ban6ga,mirams.wordpress.com,[D] Model Complexity | Mathematical Matters of the Heart,https://www.reddit.com/r/MachineLearning/comments/ban6ga/d_model_complexity_mathematical_matters_of_the/,_quanttrader_,1554683261,,0,1,False,default,,,,,
501,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,10,banhbu,arxiv.org,[R] LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech,https://www.reddit.com/r/MachineLearning/comments/banhbu/r_libritts_a_corpus_derived_from_librispeech_for/,hardmaru,1554685209,,3,15,False,default,,,,,
502,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,10,banqmb,self.MachineLearning,Is there any work like BicycleGAN for Single Image Super-Resolution?,https://www.reddit.com/r/MachineLearning/comments/banqmb/is_there_any_work_like_bicyclegan_for_single/,alsombra,1554686874,[removed],0,1,False,self,,,,,
503,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,10,banvsb,self.MachineLearning,Is there any work like BicycleGAN for Single Image Super-Resolution?,https://www.reddit.com/r/MachineLearning/comments/banvsb/is_there_any_work_like_bicyclegan_for_single/,alsombra,1554687801,[removed],0,1,False,self,,,,,
504,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,11,bao22o,self.MachineLearning,[D] BicycleGAN for Single Image Super-Resolution,https://www.reddit.com/r/MachineLearning/comments/bao22o/d_bicyclegan_for_single_image_superresolution/,alsombra,1554688937,"The seminal paper using GANs for Single Image Super-Resolution was the [SRGAN paper](https://arxiv.org/abs/1609.04802). Like the [pix2pix](https://phillipi.github.io/pix2pix/) [paper](https://arxiv.org/abs/1611.07004), it did not use a random noise Z on input, which makes the output deterministic. Maybe, as in pix2pix, using Z wouldn't result in many differences due to mode collapse.
A follow up paper to pix2pix was the [BicycleGAN](https://junyanz.github.io/BicycleGAN/) one, which was successful in generating a whole set of diverse and photo-realistic images as output. Do you know any work/implementation that does this but in the context of Super-Resolution? (Multi-modal Super-Resolution using GANs)",5,13,False,self,,,,,
505,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,11,bao3ku,arxiv.org,[R] A Comprehensive Overhaul of Feature Distillation,https://www.reddit.com/r/MachineLearning/comments/bao3ku/r_a_comprehensive_overhaul_of_feature_distillation/,sanxiyn,1554689194,,1,4,False,default,,,,,
506,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,11,baokry,self.MachineLearning,[D] Article about weakness or limitation of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/baokry/d_article_about_weakness_or_limitation_of_machine/,wean_irdeh,1554692285,"There's an article which discuss about machine learning with 'father of machine learning' in which he said that the limitation of machine learning is only performs line/curve normalization  and pattern recognition, even longtime data scientist knows that. I found it on news.ycombinator.com, but I haven't ever found it since. If you know the article, please post it here, any help would be appreciated, thanks!",4,3,False,self,,,,,
507,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,12,baotxs,self.MachineLearning,Machine learning in investment banking,https://www.reddit.com/r/MachineLearning/comments/baotxs/machine_learning_in_investment_banking/,NLP_RL,1554693958,[removed],0,1,False,self,,,,,
508,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,12,bap25r,self.MachineLearning,[P] How to FIGHT an AI,https://www.reddit.com/r/MachineLearning/comments/bap25r/p_how_to_fight_an_ai/,wholeywoolly,1554695543,"I set up an open-ai Retro environment inside pygame and fought one of the AI's that I trained using stable-baselines!

It kicks my butt at Street Fighter 2! 

[How to FIght an AI](
https://www.youtube.com/watch?v=sgEIoOQgjFg&amp;list=PLTWFMbPFsvz23E5x70c0fcWQLtcRmNmuB)

The code is here: https://gitlab.com/lucasrthompson/sf2-a2c-bot/blob/master/play.py

Then I make two different AI's fight each other. Let's see which algorithm is the best, eh? 

Also, pygame is great.

If you guys make anything with this, let me know about it! I'd love to fight someone else's AI.",7,18,False,self,,,,,
509,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,14,bapwtb,arxiv.org,[R] Do ImageNet Classifiers Generalize to ImageNet?,https://www.reddit.com/r/MachineLearning/comments/bapwtb/r_do_imagenet_classifiers_generalize_to_imagenet/,downtownslim,1554701635,,8,20,False,default,,,,,
510,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,14,bapxjj,self.MachineLearning,Splunk use case,https://www.reddit.com/r/MachineLearning/comments/bapxjj/splunk_use_case/,Positka,1554701784,[removed],0,1,False,self,,,,,
511,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,15,baq5c3,self.MachineLearning,[D] Deep Learning And Shallow Data,https://www.reddit.com/r/MachineLearning/comments/baq5c3/d_deep_learning_and_shallow_data/,wei_jok,1554703430,"*Article from Piekniewski's [blog](https://blog.piekniewski.info/2019/04/07/deep-learning-and-shallow-data/) discusses areas in computer vision that has not been solved by deep learning. Excerpt:*

**Data is shallower than we thought**

Deep learning surprisingly taught us something very interesting about visual data (high dimensional data in general): in ways it is much ""shallower"" than we believed in the past. There seems to be many more ways to statistically separate a visual dataset labeled with high level human categories, then there are ways to separate such dataset that are ""semantically correct"". In other words, the set of low level image features is a lot more ""statistically"" potent than we imagined. This is the great discovery of deep learning. The question of how to generate models that would find ""semantically sound"" ways of separating visual datasets remains open and in fact now seems even harder to answer than before.

Deep learning is here to stay and is now a crucial part of computer vision toolbox. But traditional computer vision is not going anywhere and could still be used to build very powerful detectors. These hand crafted detectors may not achieve as high performance on some particular dataset metric, but can be guaranteed to rely on ""semantically relevant"" set of features of the input. Therefore their failure modes can be much better characterized and anticipated. Deep learning is providing statistically powerful detectors without the expense of feature engineering, though one still has to have a lot of labeled data, lot of GPU's and a deep learning expert onsite. However these powerful detectors will fail unexpectedly and their range of applicability cannot be easily characterized (or to put it more strongly - cannot be characterized at all, period). In applications where rare but catastrophic failure is acceptable, deep learning will work fine. For applications in which guaranteed performance within a given set of conditions plus computational complexity are more important, classical machine vision pipelines will be in use for many years to come.

https://blog.piekniewski.info/2019/04/07/deep-learning-and-shallow-data/",4,6,False,self,,,,,
512,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,15,baqa7i,self.MachineLearning,Present-Day Machine Learning in Mobile Applications,https://www.reddit.com/r/MachineLearning/comments/baqa7i/presentday_machine_learning_in_mobile_applications/,appsbee,1554704425,[removed],0,1,False,self,,,,,
513,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,15,baqfks,oodlestechnologies.com,Top Artificial Intelligence Platforms For Business,https://www.reddit.com/r/MachineLearning/comments/baqfks/top_artificial_intelligence_platforms_for_business/,tech-info,1554705580,,0,1,False,default,,,,,
514,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,15,baqfz8,self.MachineLearning,How to document your algorithm for you business?,https://www.reddit.com/r/MachineLearning/comments/baqfz8/how_to_document_your_algorithm_for_you_business/,jtfidje,1554705665,[removed],0,1,False,self,,,,,
515,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,15,baqjo9,medium.com,Habitat: Training Ground for Embodied AI Agents,https://www.reddit.com/r/MachineLearning/comments/baqjo9/habitat_training_ground_for_embodied_ai_agents/,BettyWaihenya,1554706471,,0,1,False,https://b.thumbs.redditmedia.com/rFzQJuWTJqra_xlQv_8bZFoQKKbG165EW-kmV7vIg0k.jpg,,,,,
516,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,16,baqnpw,self.MachineLearning,[D] Why does Microsoft try to force people to use Jupyter Notebooks for ML on Azure? How does your team do ML on Azure?,https://www.reddit.com/r/MachineLearning/comments/baqnpw/d_why_does_microsoft_try_to_force_people_to_use/,BorderLineGenius,1554707331,"Recently, my company started migrating our whole ecosystem to Azure. Thus, I am expected to move there both our production models, as well as the development process.  

&amp;#x200B;

To my dismay, Microsoft seems to want to force me to use Jupyter Notebooks everywhere and for everything!  We had a kind of semi-technical sales guy present for us the ML capabilities on Azure, and honestly it didn't look good. Preprocessing data in DataBricks? Use Jupyter Notebook. Train model? Notebook. Serve model in production? Another notebook. Am I missing something?  

&amp;#x200B;

How do you develop and serve ML models in the Azure cloud?  

   
[Good article on why Jupyter Notebooks suck for actually producing ML software.](https://towardsdatascience.com/5-reasons-why-jupyter-notebooks-suck-4dc201e27086)",82,183,False,self,,,,,
517,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,16,baqytt,self.MachineLearning,[D] What color spaces work best for image GANs?,https://www.reddit.com/r/MachineLearning/comments/baqytt/d_what_color_spaces_work_best_for_image_gans/,veqtor,1554709647,"So recently I've been thinking:
Does it really make sense to use the RGB colorspace for generating images?

In a lot of images you have objects with a certain hue and saturation while luminance is relative to the mean in a picture.
This thought came up when I had a GAN training collapse into pure Red, green and blue patches. Surely this wouldn't have happened in a HSV space?

I tried Googling but didn't find anything, has there been any ablation studies on colorspaces for GANs?",13,16,False,self,,,,,
518,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,18,barntp,self.MachineLearning,[D] Need advice on CPU choice for machine learning / deep learning rig,https://www.reddit.com/r/MachineLearning/comments/barntp/d_need_advice_on_cpu_choice_for_machine_learning/,noban415,1554715441,"Hello! 

I'm a PhD student and planning to build a PC for research (no gaming) with possibly 2 GPU's in the future (3 at most in distant future).

Options:

1) AMD (Threadripper 2XXX) is out of scope due to poor numpy performance. See for example:  [https://www.reddit.com/r/Amd/comments/9mhr5q/amd\_threadripper\_2990wx\_for\_scientific\_workloads/](https://www.reddit.com/r/Amd/comments/9mhr5q/amd_threadripper_2990wx_for_scientific_workloads/) 

I'm kind of waiting for Zen 2 results - beginning of June - Computex. It seems AMD have doubled bandwidth of units responsible for floating points operations (FMU?) thus performance should be on par with at least current Intel cpus.

&amp;#x200B;

2) I was aiming at LGA2066 - i7-9800x which was about \~650 Euro (here in Europe) back in January \`19, but now it's more like \~850 Euros and growing. Difference between 9800x and 9820x is roughly 40-50 Euros. Such a big rise in price is worrying for me.

&amp;#x200B;

3) LGA2011-v3 as a strong contender. See:  [https://www.cpubenchmark.net/compare/Intel-i7-9800X-vs-AMD-Ryzen-Threadripper-2950X-vs-Intel-Xeon-E5-2696-v3/3374vs3316vs2526](https://www.cpubenchmark.net/compare/Intel-i7-9800X-vs-AMD-Ryzen-Threadripper-2950X-vs-Intel-Xeon-E5-2696-v3/3374vs3316vs2526) 

I understand that these benchmarks are not my usual workload, nevertheless it seems like used \~600$ e5-2696 v3 is better performing than 9800x. Cheaper used ECC DDR4 2133Mhz memory seems not so bad as well. The only downside is there are no C612/X99 chipset motherboards so I'll have to buy a used one as well (I don't want to go to AliExpress as their X99 board seems really risky for me :) in terms of possible performance issues).

&amp;#x200B;

My typical workload is: python (multithreaded as well), PyTorch, Tensorflow, Java. No gaming, just deep/learning machine learning, reinforcement learning / simulations.

&amp;#x200B;

Thus main questions: 

\- Should I wait till Zen 2 performance results on python (multi-threaded) / PyTorch / Tensorflow will be known (think end of June), or should I go with e5-269x v3/v4 ?

\- Does anyone have some performance reports comparing LGA2011-v3 with any of LGA-2066 cpu's?

&amp;#x200B;

Thank you!

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",6,2,False,self,,,,,
519,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,18,barqd4,appthisway.com,An interview with a robot | Appthisway.com,https://www.reddit.com/r/MachineLearning/comments/barqd4/an_interview_with_a_robot_appthiswaycom/,appthisway,1554716067,,0,1,False,default,,,,,
520,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,19,barx4y,self.MachineLearning,ML Algorithms,https://www.reddit.com/r/MachineLearning/comments/barx4y/ml_algorithms/,Yann_LeCun,1554717622,[removed],0,1,False,self,,,,,
521,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,19,barxvh,youtube.com,[P] Barack Obama ML model sings 'Lose Yourself' by Eminem,https://www.reddit.com/r/MachineLearning/comments/barxvh/p_barack_obama_ml_model_sings_lose_yourself_by/,hanyuqn,1554717773,,0,1,False,https://b.thumbs.redditmedia.com/916KA1UpiCWbiMQqlVt67g02zNLVhw5cSWGvyb6J74Q.jpg,,,,,
522,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,19,barzi4,oodlestechnologies.com,Blockchain and Machine Learning To Predict Consumer Behavior,https://www.reddit.com/r/MachineLearning/comments/barzi4/blockchain_and_machine_learning_to_predict/,oodlestechnologies,1554718143,,0,1,False,default,,,,,
523,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,19,bas82v,arxiv.org,[R]Impacts of Dirty Data: and Experimental Evaluation,https://www.reddit.com/r/MachineLearning/comments/bas82v/rimpacts_of_dirty_data_and_experimental_evaluation/,Almerrick,1554720009,,1,2,False,default,,,,,
524,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,20,basn4p,beldara.com,"Opg Machine Manufacturers, Suppliers &amp; Dealers | Beldara.com",https://www.reddit.com/r/MachineLearning/comments/basn4p/opg_machine_manufacturers_suppliers_dealers/,BeldaraMarketplace,1554723040,,0,1,False,default,,,,,
525,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,20,basp7d,openreview.net,[R] Disentangled Representation Learning with Information Maximizing Autoencoder,https://www.reddit.com/r/MachineLearning/comments/basp7d/r_disentangled_representation_learning_with/,Inforhacker,1554723418,,15,18,False,https://a.thumbs.redditmedia.com/O5xSqPxXQan0eq4XIj_39G9lsyFLtyg3D_81hYNOIr4.jpg,,,,,
526,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,21,baswwc,self.MachineLearning,Extensive webinar on insights discovery,https://www.reddit.com/r/MachineLearning/comments/baswwc/extensive_webinar_on_insights_discovery/,supercake53,1554724800,[removed],0,1,False,self,,,,,
527,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,21,basx5w,self.MachineLearning,Using ML to Optimize and Reduce Information Overload on a Travel Website,https://www.reddit.com/r/MachineLearning/comments/basx5w/using_ml_to_optimize_and_reduce_information/,wealthplus19,1554724841,[removed],0,1,False,self,,,,,
528,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,21,batak4,self.MachineLearning,Best Type of Model For Generating Chemical Reaction Formulas,https://www.reddit.com/r/MachineLearning/comments/batak4/best_type_of_model_for_generating_chemical/,carsonpoole,1554727090,"So there is a dataset of 3.3M chemical reactions, and I hope it's not too out there to assume that some kind of generative model can generate new reactions not in the dataset.  There was an [MIT paper](https://arxiv.org/abs/1709.04555) written that uses a Weisfeiler-Lehman Network to predict reaction products from given reactants. This is the reverse of that paper, because I'm trying to find reactants that could form a given product (`P(X|y)`).

So my questions is really just what kind of model would be ideal for this? Some kind of adversarial network like an AAE or GAN? I don't think a GAN could work here because of the discrete choices. Or could a RNN or some kind of non-adversarial network work because I'm essentially just switching the X's and y's.

Thanks for the help!",0,1,False,self,,,,,
529,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,21,batfu1,self.MachineLearning,Two posts on semantic search and the earth mover's distance,https://www.reddit.com/r/MachineLearning/comments/batfu1/two_posts_on_semantic_search_and_the_earth_movers/,anebz,1554727980,"I published two posts in Medium as part of my research in my master's degree in NLP.

&amp;#x200B;

First post: semantic search, how the search engine 'understands' our query and shows the result we want: [https://towardsdatascience.com/semantic-search-73fa1177548f](https://towardsdatascience.com/semantic-search-73fa1177548f)

Second post: an approach to calculate the semantic similarity between two documents/sentences, that is, check if the meaning is similar even if the words used are completely different: [https://towardsdatascience.com/earth-movers-distance-68fff0363ef2](https://towardsdatascience.com/earth-movers-distance-68fff0363ef2)

&amp;#x200B;",0,1,False,self,,,,,
530,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,21,bathac,self.MachineLearning,Facebook NewsFeed Explainability,https://www.reddit.com/r/MachineLearning/comments/bathac/facebook_newsfeed_explainability/,MetabolismZeitgeist,1554728207,[removed],0,1,False,self,,,,,
531,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,batkn8,self.MachineLearning,"Skeletal Age Detection, how many DL models would it need?",https://www.reddit.com/r/MachineLearning/comments/batkn8/skeletal_age_detection_how_many_dl_models_would/,Limitless1092,1554728725,[removed],0,1,False,self,,,,,
532,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,batnqa,self.MachineLearning,[D] Facebook NewsFeed Explainability,https://www.reddit.com/r/MachineLearning/comments/batnqa/d_facebook_newsfeed_explainability/,MetabolismZeitgeist,1554729209,"""Facebooks News Feed is starting to explain itself"" ([Link](https://www.theverge.com/2019/4/1/18290195/facebooks-news-feed-why-am-i-seeing-this-post-ad-context-interaction)). How would one go about pinpointing such high level reasons for recommending a post, given the deep neural network models with thousands of abstract input features?",5,10,False,self,,,,,
533,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,bato3m,blog.nanonets.com,Everything you need to know about Visual Inspection with AI,https://www.reddit.com/r/MachineLearning/comments/bato3m/everything_you_need_to_know_about_visual/,manneshiva,1554729269,,0,1,False,default,,,,,
534,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,batt83,self.MachineLearning,What is the best way to follow the latest news on new AI trained models?,https://www.reddit.com/r/MachineLearning/comments/batt83/what_is_the_best_way_to_follow_the_latest_news_on/,viswanath660,1554730087,[removed],0,1,False,self,,,,,
535,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,battc9,thebetterindia.com,AI turns thousands of farmers into plant doctors,https://www.reddit.com/r/MachineLearning/comments/battc9/ai_turns_thousands_of_farmers_into_plant_doctors/,rob_van_b,1554730104,,0,1,False,https://b.thumbs.redditmedia.com/w9afUPHM65mi6bhexOyrcWdkncXg_4yjWb3rULJOtkE.jpg,,,,,
536,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,battjn,blog.nanonets.com,[D] Everything you need to know about Visual Inspection with AI,https://www.reddit.com/r/MachineLearning/comments/battjn/d_everything_you_need_to_know_about_visual/,manneshiva,1554730141,,0,1,False,https://a.thumbs.redditmedia.com/wa5aP67NGz5l5BXovBR9UdNuH7W8RF17kVun55BwlX0.jpg,,,,,
537,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,batyae,self.MachineLearning,"[P] Easy-to-use word-to-word translations for 3,564 language pairs",https://www.reddit.com/r/MachineLearning/comments/batyae/p_easytouse_wordtoword_translations_for_3564/,longinglove,1554730917,"A large collection of freely &amp; publicly available word-to-word translationsfor 3,564 language pairs across 62 unique languages.

https://github.com/Kyubyong/word2word

Technically this project doesn't belong to machine learning. Rather it takes an statistics-based approach to find word equivalents from parallel sentences. However we guess some of you may be interested in this because we believe this can be used for some machine learning task like unsupervised machine translation.",0,36,False,self,,,,,
538,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,22,batzgv,arxiv.org,[1904.02830] An Evolutionary Framework for Automatic and Guided Discovery of Algorithms,https://www.reddit.com/r/MachineLearning/comments/batzgv/190402830_an_evolutionary_framework_for_automatic/,ihaphleas,1554731104,,9,32,False,default,,,,,
539,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,23,bau76i,self.MachineLearning,[D] Automating Visual Inspection using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bau76i/d_automating_visual_inspection_using_deep_learning/,manneshiva,1554732288,"If you've been conducting manual quality checks at your manufacturing company, you've probably been over-paying for low productivity and poor quality output. Here's why AI-powered visual inspection is the future of manufacturing.

[https://blog.nanonets.com/ai-visual-inspection/](https://blog.nanonets.com/ai-visual-inspection/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=inspection&amp;utm_content=group_name)",3,9,False,self,,,,,
540,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,23,baudrf,self.MachineLearning,What are the best Git projects you have come across that do Synthetic Training Data generation for multi label classification?,https://www.reddit.com/r/MachineLearning/comments/baudrf/what_are_the_best_git_projects_you_have_come/,abdush,1554733247,[removed],0,1,False,self,,,,,
541,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,23,baufy0,self.MachineLearning,Drago Anguelov (Waymo) - MIT Self-Driving Cars,https://www.reddit.com/r/MachineLearning/comments/baufy0/drago_anguelov_waymo_mit_selfdriving_cars/,NuEd_Fernandes,1554733572,[removed],0,1,False,self,,,,,
542,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,23,bauha6,self.MachineLearning,Machine Learning Suggestions,https://www.reddit.com/r/MachineLearning/comments/bauha6/machine_learning_suggestions/,ZebulaCSGO,1554733775,[removed],0,1,False,self,,,,,
543,MachineLearning,t5_2r3gv,2019-4-8,2019,4,8,23,baujtv,self.MachineLearning,The Fascinating Ways PepsiCo Uses Artificial Intelligence And Machine Learning,https://www.reddit.com/r/MachineLearning/comments/baujtv/the_fascinating_ways_pepsico_uses_artificial/,edxsocial,1554734137,"The article highlights manufacturing with machine learning, building a snack delivery robot, making hiring calls and screening candidates, and more. 

[https://www.forbes.com/sites/bernardmarr/2019/04/05/the-fascinating-ways-pepsico-uses-artificial-intelligence-and-machine-learning-to-deliver-success/](https://www.forbes.com/sites/bernardmarr/2019/04/05/the-fascinating-ways-pepsico-uses-artificial-intelligence-and-machine-learning-to-deliver-success/)",0,1,False,self,,,,,
544,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,0,bav0sx,medium.com,"Megvii Introduces Single-Path, One-Shot Method for NAS Design",https://www.reddit.com/r/MachineLearning/comments/bav0sx/megvii_introduces_singlepath_oneshot_method_for/,Yuqing7,1554736599,,0,1,False,https://a.thumbs.redditmedia.com/X4gqcWj6PA3s-0PoHQTdla3R1_HspN4GM1tZHY19L_4.jpg,,,,,
545,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,0,bav4dm,youtube.com,Watch how Machine Learning will transform Demand Forecasting (Part of a YouTube series on AI For Business),https://www.reddit.com/r/MachineLearning/comments/bav4dm/watch_how_machine_learning_will_transform_demand/,aiforbusiness,1554737097,,0,1,False,https://b.thumbs.redditmedia.com/F5TmLPGbfpbE045MGF30WRlIzZaUfY_IJZOAD0qE_gg.jpg,,,,,
546,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,0,bav8je,self.MachineLearning,Multi-labelled Classifier,https://www.reddit.com/r/MachineLearning/comments/bav8je/multilabelled_classifier/,vicmartins,1554737687,[removed],0,1,False,self,,,,,
547,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,0,bavfl1,self.MachineLearning,[D] How to break into ML research after working in industry for a few years?,https://www.reddit.com/r/MachineLearning/comments/bavfl1/d_how_to_break_into_ml_research_after_working_in/,rampant_juju,1554738679,"I apologize if this is not a question which should be entertained on this sub; I am simply looking for advice.

&amp;#x200B;

I currently work at one of the FAANGs as a Software Developer, building web apps. As I hail from India, I did not have many opportunities to pursue ML research during my undergrad. I was able to publish a first-author paper in an okay-ish IEEE conference in the domain of ML-based security, so I have some idea, but I am unfamiliar with how research is conducted in other parts of the world. 

&amp;#x200B;

Could you let me know, what are the possibilities open to me, and which ones are realistic? A few that come to my mind are:

&amp;#x200B;

1. Get a Masters specializing in ML (I tried this in 2018, all applications were rejected. GRE of 170Q, 166V, 4 AWA, a few research projects with profs, and the aforementioned paper).
2. Join a research group in a university, either in the US, in Europe, or in India.
3. Get an AI residency at one of the big tech companies.
4. Apply for an ML Engineer role, where I can hopefully work on a team with Research Scientists, and maybe participate in research myself.
5. Attempt to transition to a Data Scientist role myself.

&amp;#x200B;

Would love to hear the opinion of /r/MachineLearning on this, and which of the above are actually possible. ",35,76,False,self,,,,,
548,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,1,bavlph,self.MachineLearning,[N] The Fascinating Ways PepsiCo Uses Artificial Intelligence And Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bavlph/n_the_fascinating_ways_pepsico_uses_artificial/,edxsocial,1554739520,"The article highlights manufacturing with machine learning, building a snack delivery robot, making hiring calls and screening candidates, and more.

[https://www.forbes.com/sites/bernardmarr/2019/04/05/the-fascinating-ways-pepsico-uses-artificial-intelligence-and-machine-learning-to-deliver-success/](https://www.forbes.com/sites/bernardmarr/2019/04/05/the-fascinating-ways-pepsico-uses-artificial-intelligence-and-machine-learning-to-deliver-success/)",3,0,False,self,,,,,
549,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,1,bavy5j,self.MachineLearning,Google cloud products or AWS,https://www.reddit.com/r/MachineLearning/comments/bavy5j/google_cloud_products_or_aws/,neuroguy6,1554741241,"My company is growing very fast and we're looking into a proper data warehouse solution (I.e, big query or red shift). Depending on which one we choose, we'll obviously begin pouring resources into other offerings within each platform. In terms of ML, I've heard pretty awesome things about sagemaker and how simple it is to put a model into production, however Google Big Query seems super easy to use, appears to require very little to no maintenance, and cloud datalab and data prep look like they could be very helpful in streamlining preprocessing. Further, a colleague of mine mentioned that GCP are accelerating faster in terms of growth than AWS, and one reason is the low maintenance and setup time required for GCP. 

What are your thoughts on this, and what trends do you see in the marketplace?",0,1,False,self,,,,,
550,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,1,baw2yb,self.MachineLearning,Need advice to improve the model,https://www.reddit.com/r/MachineLearning/comments/baw2yb/need_advice_to_improve_the_model/,kindlime,1554741913,[removed],0,1,False,self,,,,,
551,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,2,bawiis,self.MachineLearning,[P] Using Data Science to Predict Response Times of Firefighters,https://www.reddit.com/r/MachineLearning/comments/bawiis/p_using_data_science_to_predict_response_times_of/,cyrilp21,1554744115,"Using Data Science to Predict Response Times of Firefighters

[MEDIUM: Predict Response Times of Firefighters using Data Science](https://medium.com/crim/predicting-the-response-times-of-firefighters-using-data-science-da79f6965f93)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/yd2yu0tyq2r21.jpg",1,5,False,https://b.thumbs.redditmedia.com/FREyLsGcGZPRqvWns-rJuVBouqNi3Pf0G6-coBVMWxA.jpg,,,,,
552,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,2,bawxqa,self.MachineLearning,Understanding Strength and Correlation in Random Forest,https://www.reddit.com/r/MachineLearning/comments/bawxqa/understanding_strength_and_correlation_in_random/,princethewinner,1554746309,[removed],0,1,False,self,,,,,
553,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,3,bax2ao,self.MachineLearning,[D] Predicting time-series of binary data,https://www.reddit.com/r/MachineLearning/comments/bax2ao/d_predicting_timeseries_of_binary_data/,mmoec,1554746949,"Hey! Im planning on doing some predictions for upcoming time-intervals where the value is either true or false. Ive got a database containing entries which represents a period of time where a condition holds, represented by its start time and end time. I was wondering of any of you had gotten some great, or at least decent, results with similar projects. What was your approach? How did you represent the historical data that you fed into your algorithm?

My attempt so far has been to generate an array, where each entry represents a 15 minute interval, where the array start is the first datetime i want to evaluate the data from, up until the current time. For each entry i gave it the value of 1 ( the conditions holds) and 0 ( does not hold ). Ive also tried representing each entry as a full hour, giving it a value of 0-4 based on how many of the 15 minute intervals within that hour the condition holds. Both these data models were inserted into a support vector machine, nearest neighbor and linear regression function. Im not very happy with the results. Considering im quite the beginner on this topic, i was hoping any of you had a few tips for me.

Thanks!",12,0,False,self,,,,,
554,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,3,bax9ue,self.MachineLearning,Feature engineering on continuous variables,https://www.reddit.com/r/MachineLearning/comments/bax9ue/feature_engineering_on_continuous_variables/,kekkimo,1554748019,[removed],0,1,False,self,,,,,
555,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,3,baxc5b,self.MachineLearning,[R] Epileptic seizure or not? New deep learning model has the answer,https://www.reddit.com/r/MachineLearning/comments/baxc5b/r_epileptic_seizure_or_not_new_deep_learning/,applyingscienceatubc,1554748361,UBC researchers have developed a machine learning model that [may improve the detection of epileptic seizures](https://apsc.ubc.ca/spotlight/research/epileptic-seizure-or-not-new-deep-learning-model-has-answer).,6,9,False,self,,,,,
556,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,3,baxgxf,self.MachineLearning,NLP Trends,https://www.reddit.com/r/MachineLearning/comments/baxgxf/nlp_trends/,johnsnowlabsUS,1554749031,[removed],0,1,False,self,,,,,
557,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,4,bay3ww,self.MachineLearning,Help me!,https://www.reddit.com/r/MachineLearning/comments/bay3ww/help_me/,jesuschristreddit209,1554752285,[removed],0,1,False,self,,,,,
558,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,4,bay47h,self.MachineLearning,Compare two models at inference,https://www.reddit.com/r/MachineLearning/comments/bay47h/compare_two_models_at_inference/,textMinier,1554752329,[removed],0,1,False,self,,,,,
559,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,4,bay5yd,arxiv.org,[R] [1810.07217] Hierarchical Generative Modeling for Controllable Speech Synthesis,https://www.reddit.com/r/MachineLearning/comments/bay5yd/r_181007217_hierarchical_generative_modeling_for/,bobchennan,1554752583,,4,16,False,default,,,,,
560,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,5,bayzc7,self.MachineLearning,"[D] Training Object Detector with Few Image &amp; Many Labels, and Only Positive Examples",https://www.reddit.com/r/MachineLearning/comments/bayzc7/d_training_object_detector_with_few_image_many/,raichet,1554756684,"Hey all,

&amp;#x200B;

I am building my own object detector with the TensorFlow Object Detection API, and I've been following sentdex's video series.  I have two main concerns/questions.

&amp;#x200B;

1. What confuses me about his video is that he only has one class label for his entire dataset, which is 154 images purely of macaroni &amp; cheese. Each image consists of one macaroni and cheese, which he has annotated with a bounding box annotation tool.  **How is he able to train his NN with only positive examples, and zero negative example?** It's reassuring as I will have to label my own images, and his result means I don't have to manually label the negative examples. But I would like to understand why negative examples weren't needed.
2. Furthermore, unrelated to his video series but specific to my own case, **I don't have many images (45 ish), but each image has 100+ instances of the object I want to detect which I can label with a bounding box. Would this suffice for training data?** The testing data will be of the same format. Sentdex has many many images, each containing one bounding box. My data set has very few images, each containing many bounding boxes. I am hoping this would work.

&amp;#x200B;

Your answer will be appreciated! :)",5,1,False,self,,,,,
561,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,6,baz3u6,self.MachineLearning,TensorFlow inference (C++),https://www.reddit.com/r/MachineLearning/comments/baz3u6/tensorflow_inference_c/,adaneze,1554757300,[removed],5,2,False,self,,,,,
562,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,6,bazczz,ponderwall.com,Evolutionary computation has been promising self-programming machines for 60 years  so where are they?,https://www.reddit.com/r/MachineLearning/comments/bazczz/evolutionary_computation_has_been_promising/,martinvasquez,1554758620,,0,1,False,https://b.thumbs.redditmedia.com/zJlXYSRbjnKeFTf9bg6auCokfmktQwOp0MvQtZAqO4g.jpg,,,,,
563,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,6,bazh0v,ponderwall.com,[R]Evolutionary computation has been promising self-programming machines for 60 years  so where are they?,https://www.reddit.com/r/MachineLearning/comments/bazh0v/revolutionary_computation_has_been_promising/,martinvasquez,1554759220,,0,1,False,default,,,,,
564,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,8,bb0if5,self.MachineLearning,Where (and when) to begin?,https://www.reddit.com/r/MachineLearning/comments/bb0if5/where_and_when_to_begin/,Migui2611,1554764841,[removed],0,1,False,self,,,,,
565,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,9,bb1a12,self.MachineLearning,[D] Avoiding getting your work stolen as an undergrad?,https://www.reddit.com/r/MachineLearning/comments/bb1a12/d_avoiding_getting_your_work_stolen_as_an/,EveningAlgae,1554769191,"Hi /r/MachineLearning, I'm an undergraduate at the University of Alberta. 

&amp;#x200B;

I think I've found a framework which completely supplants an old framework for multiclass classification (and is a generalization of said old framework) and I have some (limited but good so far) theoretical and empirical results demonstrating its effectiveness. I am scared about sharing my work with someone, since the idea is quite straightforward and simple, but the empirical benefits seem to be quite good. What can I do, in my situation? Is there any way I can protect my work before sharing it with other people? I've heard some nasty stories about professors stealing their students work, and I've put a fair bit of time and effort slowly chipping away at my results and improving them.

&amp;#x200B;

Thoughts?",62,159,False,self,,,,,
566,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,9,bb1fzc,self.MachineLearning,[D] Any public dataset or forced aligner to build a phoneme (or PinYin) model for Mandarin Chinese?,https://www.reddit.com/r/MachineLearning/comments/bb1fzc/d_any_public_dataset_or_forced_aligner_to_build_a/,pastaking,1554770181,"For English, we have librispeech ([http://www.openslr.org/12/](http://www.openslr.org/12/)) and open source forced aligners ([Gentle](https://github.com/lowerquality/gentle)) that gives us phoneme-level alignments on words. I need to build a phoneme model for Mandarin Chinese, but I can't seem to find much public datasets with PinYin annotations, or a forced aligner that comes with PinYin.

Any tips would be greatly appreciated!",0,1,False,self,,,,,
567,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,9,bb1loq,self.MachineLearning,Data analysis for machine learning,https://www.reddit.com/r/MachineLearning/comments/bb1loq/data_analysis_for_machine_learning/,archx64,1554771125,[removed],0,1,False,self,,,,,
568,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,10,bb26mg,ledsynergy.co.uk,LED Displays &amp; LED Signs custom made | LEDsynergy,https://www.reddit.com/r/MachineLearning/comments/bb26mg/led_displays_led_signs_custom_made_ledsynergy/,marylourrpamint,1554774655,,0,1,False,default,,,,,
569,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,10,bb27gd,self.MachineLearning,i wanna play against alphastar,https://www.reddit.com/r/MachineLearning/comments/bb27gd/i_wanna_play_against_alphastar/,mastahwombat420,1554774797,[removed],0,1,False,self,,,,,
570,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,10,bb27um,self.MachineLearning,[P] Deep Learning on Healthcare Lecture Series (5),https://www.reddit.com/r/MachineLearning/comments/bb27um/p_deep_learning_on_healthcare_lecture_series_5/,hiconcep,1554774864,"Deep Learning on Healthcare (5): Story of Lunit (2). This lecture's theme is case study of Lunit. One of the representative healthcare deep learning startup from South Korea. I think this is an exemplary growth story of healthcare deep learning startup.

&amp;#x200B;

[Deep Learning on Healthcare (5)](https://www.youtube.com/watch?v=aI9R7T87hGw&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee&amp;index=2&amp;t=0s)

[Deep Learning on Healthcare (4)](https://www.youtube.com/watch?v=KyTo4BvysfQ)

[Deep Learning on Healthcare (3)](https://www.youtube.com/watch?v=1tMy-ZukPQc)

[Deep Learning on Healthcare (2)](https://www.youtube.com/watch?v=zsXKWEa9GhQ&amp;t=31s&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee&amp;index=24)

[Deep Learning on Healthcare (1)](https://www.youtube.com/watch?v=k0LacC4hyY8&amp;index=23&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee)",1,6,False,self,,,,,
571,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,11,bb2acb,self.MachineLearning,[D] What's your code style? and Why use it?,https://www.reddit.com/r/MachineLearning/comments/bb2acb/d_whats_your_code_style_and_why_use_it/,thisisiron,1554775287,"Hi I want to know your framework code style. I use Keras(Tensorflow) framework.

&amp;#x200B;

For example, I typically write following code style.

\`\`\`

class customLayer(tf.keras.layers.Layer):

def \_\_init\_\_(self, ...):

...

def build(self, ...):

...

def call(self, ...):

&amp;#x200B;

class Embedder(tf.keras.Model):

def \_\_init\_\_(self, ...):

self.custom\_layer = customLayer(...)

...

def call(self, input\_tensor):

...

\`\`\`

I use tf.keras.Model to package layers. And if I need to create a layer, I use tf.keras.layers.Layer.

If you ask why you are using these code styles, I will say that \*\*this code style is easy to reuse and clear\*\*.

It's just my idea.  Any tips or comments would be greatly appreciated.",2,3,False,self,,,,,
572,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,11,bb2anq,dev.to,1st Slack ML App hackathon - Too many great ideas!,https://www.reddit.com/r/MachineLearning/comments/bb2anq/1st_slack_ml_app_hackathon_too_many_great_ideas/,dai-team-ai,1554775336,,0,1,False,default,,,,,
573,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,11,bb2i13,/r/MachineLearning/comments/bb2i13/p_bada_helicopter_lift_at_hard_rock_casino_in/,"P, BADA$$ helicopter lift at Hard Rock Casino in Hollywood FLa",https://www.reddit.com/r/MachineLearning/comments/bb2i13/p_bada_helicopter_lift_at_hard_rock_casino_in/,CoconutDave142,1554776582,,0,1,False,default,,,,,
574,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,11,bb2lrl,arxiv.org,"[R] ASAP: Architecture Search, Anneal and Prune",https://www.reddit.com/r/MachineLearning/comments/bb2lrl/r_asap_architecture_search_anneal_and_prune/,milaworld,1554777238,,1,4,False,default,,,,,
575,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,11,bb2nf5,self.MachineLearning,[R] [1904.03955] Kervolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bb2nf5/r_190403955_kervolutional_neural_networks/,tzivo,1554777531,[removed],0,1,False,self,,,,,
576,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,14,bb417o,self.MachineLearning,"WhatApp:(+237673599811]Buy Biometric IELTS/PTE/TOEFL/TOEIC Certificates Without Exam In Canada, Australia, Jordan, Qatar.)",https://www.reddit.com/r/MachineLearning/comments/bb417o/whatapp237673599811buy_biometric/,Bestduc16,1554787133,[removed],0,1,False,self,,,,,
577,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,14,bb41h5,mihaileric.com,Introduction to Logistic Regression,https://www.reddit.com/r/MachineLearning/comments/bb41h5/introduction_to_logistic_regression/,MusingEtMachina,1554787190,,0,1,False,default,,,,,
578,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,14,bb46k9,oodlestechnologies.com,Know How Artificial Intelligence Is Revamping Mobile Economy,https://www.reddit.com/r/MachineLearning/comments/bb46k9/know_how_artificial_intelligence_is_revamping/,oodlestechnologies,1554788270,,0,1,False,default,,,,,
579,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,15,bb4jxw,self.MachineLearning,Learn sequence of numbers,https://www.reddit.com/r/MachineLearning/comments/bb4jxw/learn_sequence_of_numbers/,alexdervast,1554791247,"Hi

this is my first post looking to find an appropriate method.

First all I have run some clustering that assigned the measurements to specific cluster (numbers go lets say from 1 to 100).

&amp;#x200B;

This is a dynamic system so over time I can expect cluster transitions. So at the beginning some measurements were belonging to cluster 23 and now belong to cluster 74.

&amp;#x200B;

I have collected all these transitions to vectors like:

32-&gt;12-&gt;1-&gt;89-&gt;34

&amp;#x200B;

I see that are very large overlaps on those transitions. I wanted to plot that transitions as a tree. Are there methods available for something like that?

&amp;#x200B;

Then I would like to use the prior knowledge, (train?) to be able based on partial inputs (lets say 2-&gt;12-&gt;1) to predict with some confidence the rest numbers. 

&amp;#x200B;

Which are the methods techniques that can be used for this type of learning? Can you give me some keywords to think and read about them?

&amp;#x200B;

Thanks 

Alex",0,1,False,self,,,,,
580,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,15,bb4mga,blockchain.oodles.io,"Blockchain, AI and Big Data Bringing Next Tech Revolution",https://www.reddit.com/r/MachineLearning/comments/bb4mga/blockchain_ai_and_big_data_bringing_next_tech/,Anubhav-Singh,1554791817,,0,1,False,https://b.thumbs.redditmedia.com/9cOMkZLIKNM0d3X3BSZVBC87cXwHOL3H7vfjcNGe86Y.jpg,,,,,
581,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,16,bb4uys,livewireindia.com,Data Science Certification Course,https://www.reddit.com/r/MachineLearning/comments/bb4uys/data_science_certification_course/,livewireindia,1554793717,,0,1,False,default,,,,,
582,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,16,bb51jl,arxiv.org,[R] [1904.03673] Every Local Minimum is a Global Minimum of an Induced Model,https://www.reddit.com/r/MachineLearning/comments/bb51jl/r_190403673_every_local_minimum_is_a_global/,BeatLeJuce,1554795182,,5,24,False,default,,,,,
583,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,16,bb53uo,arxiv.org,FoveaBox: Beyond Anchor-based Object Detector,https://www.reddit.com/r/MachineLearning/comments/bb53uo/foveabox_beyond_anchorbased_object_detector/,taokongcn,1554795754,,17,12,False,default,,,,,
584,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,17,bb58pv,courses.engr.illinois.edu,CS546 Machine Learning in NLP (Spring 2018),https://www.reddit.com/r/MachineLearning/comments/bb58pv/cs546_machine_learning_in_nlp_spring_2018/,matoas7,1554796915,,2,1,False,default,,,,,
585,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,17,bb59ji,self.MachineLearning,[P] pyts: A Python package for time series transformation and classification,https://www.reddit.com/r/MachineLearning/comments/bb59ji/p_pyts_a_python_package_for_time_series/,jfaouzi,1554797114,"Hello everyone,

Today I would like to share with you a project that I started almost 2 years ago. It will be a long post, so here is a TDLR.

**TDLR**:
* **pyts ([GitHub](https://github.com/johannfaouzi/pyts), [PyPI](https://pypi.org/project/pyts/), [ReadTheDocs](https://pyts.readthedocs.io/en/latest/)): a Python package for time series transformation and classification.**
* **It aims to make time series classification easily accessible by providing preprocessing and utility tools, and implementations of state-of-the-art algorithms.**
* **[pyts-repro](https://github.com/johannfaouzi/pyts-repro): Comparaison with the results published in the literature.**


## Motivations

Almost two years ago, I was an intern at a company and a colleague was working on a time series classification task. It was my end-of-studies internship and I had been studying machine learning for one year only (my background studies were more focused on statistics). I realized that I had no knowledge about machine learning for time series besides SARIMA and all the models with fewer letters. I also had limited knowledge about computer science. I did some literature search about time series classification and discovered a lot of things that I had never heard of before. Thus, I decided to start a project with the following motivations:
* Create a Python package through a GitHub repository (because I had no idea how both worked);
* Look at the source code of Python packages that I used regurlaly (numpy, scikit-learn) to gain knowledge;
* Implement algorithms about time series classification.

## Development and what I learnt

Before implementing anything, I had to :
* Learn how to package a Python project,
* Do a more advanced literature search about time series classification,
* Think about the structure of the package.

When I had an overall first idea of what I wanted to do, I could start coding. During this process, I discovered a lot of tools that were already available and that I had re-implemented myself less efficiently ([numpy.digitize](https://docs.scipy.org/doc/numpy/reference/generated/numpy.digitize.html), [sklearn.utils.check_array](https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html), [numpy.put](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.put.html), and [numpy.lib.stride_tricks.as_strided](https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.stride_tricks.as_strided.html) come to my mind). The following process could pretty much sum up the history of this project:
1. Try to implement a new algorithm;
2. In doing so, find tools that do what I wanted more efficiently, not necessarly related to the new algorithm;
3. Implement the algorithm and edit the relevant code with the newly discovered tools.

Two major *discoveries* had a huge impact on the development of this project: [scikit-learn-contrib/project-template](https://github.com/scikit-learn-contrib/project-template) and [Numba](https://github.com/numba/numba). The former made me discover a lot of concepts that I did not know about (tests, code coverage, continuous integration, documentation) and provides ready-to-use scripts. The latter made optimizing code much easier as I was very confused about Cython and building wheels, and deciced not to use Cython. I also discovered the notion of *proper code* (pep8, pep257, etc.), and [semantic versioning](https://semver.org) recently. This might be obvious for most people, but I did not know any of these concepts at the time.


## What this package provides

The current version of pyts consists of the following modules:

- `approximation`: This module provides implementations of algorithms that
approximate time series. Implemented algorithms are
[Piecewise Aggregate Approximation](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.PiecewiseAggregateApproximation.html#),
[Symbolic Aggregate approXimation](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.SymbolicAggregateApproximation.html#),
[Discrete Fourier Transform](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.DiscreteFourierTransform.html#),
[Multiple Coefficient Binning](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.MultipleCoefficientBinning.html#) and
[Symbolic Fourier Approximation](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.SymbolicFourierApproximation.html#).

- `bag_of_words`: This module consists of a class
[BagOfWords](https://pyts.readthedocs.io/en/latest/generated/pyts.bag_of_words.BagOfWords.html#)
that transforms time series into bags of words. This approach is quite common
in time series classification.

- `classification`: This module provides implementations of algorithms that
can classify time series. Implemented algorithms are
[KNeighborsClassifier](https://pyts.readthedocs.io/en/latest/generated/pyts.classification.KNeighborsClassifier.html#),
[SAXVSM](https://pyts.readthedocs.io/en/latest/generated/pyts.classification.SAXVSM.html#) and
[BOSSVS](https://pyts.readthedocs.io/en/latest/generated/pyts.classification.BOSSVS.html#).

- `decomposition`: This module provides implementations of algorithms that
decompose a time series into several time series. The only implemented algorithm
is
[Singular Spectrum Analysis](https://pyts.readthedocs.io/en/latest/generated/pyts.decomposition.SingularSpectrumAnalysis.html#).

- `image`: This module provides implementations of algorithms that transform
time series into images. Implemented algorithms are
[Recurrence Plot](https://pyts.readthedocs.io/en/latest/generated/pyts.image.RecurrencePlot.html#),
[Gramian Angular Field](https://pyts.readthedocs.io/en/latest/generated/pyts.image.GramianAngularField.html#) and
[Markov Transition Field](https://pyts.readthedocs.io/en/latest/generated/pyts.image.MarkovTransitionField.html#).

- `metrics`: This module provides implementations of metrics that are specific
to time series. Implemented metrics are
[Dynamic Time Warping](https://pyts.readthedocs.io/en/latest/generated/pyts.metrics.dtw.html#)
with several variants and the
[BOSS](https://pyts.readthedocs.io/en/latest/generated/pyts.metrics.boss.html#)
metric.

- `preprocessing`: This module provides most of the scikit-learn preprocessing
tools but applied sample-wise (i.e. to each time series independently) instead
of feature-wise, as well as an
[imputer](https://pyts.readthedocs.io/en/latest/generated/pyts.preprocessing.InterpolationImputer.html#)
of missing values using interpolation. More information is available at the
[pyts.preprocessing API documentation](https://pyts.readthedocs.io/en/latest/api.html#module-pyts.preprocessing).

- `transformation`: This module provides implementations of algorithms that
transform a data set of time series with shape `(n_samples, n_timestamps)` into
a data set with shape `(n_samples, n_features)`. Implemented algorithms are
[BOSS](https://pyts.readthedocs.io/en/latest/generated/pyts.transformation.BOSS.html#) and
[WEASEL](https://pyts.readthedocs.io/en/latest/generated/pyts.transformation.WEASEL.html#).

- `utils`: a simple module with
[utility functions](https://pyts.readthedocs.io/en/latest/api.html#module-pyts.utils).

I also wanted to have an idea about how my implementations perform compared to the performance reported in the papers and on the [Time Series Classification Repository](http://www.timeseriesclassification.com). The point is to see if my implementations are reliable or not. To do so, I created a [GitHub repository](https://github.com/johannfaouzi/pyts-repro) where I make these comparisons on the smallest datasets. I think that my implementation of WEASEL might be under-performing, but for the other implementations reported the performance is comparable. There are sometimes intentional differences between my implementation and the description of the algorithm in the paper, which might explain the differences in performance.


## Future work

The main reason of this post is to get feedback. I have been pretty much working on my own on this project, doing what I felt like doing. However, as a PhD student, I know how important it is to get feedback on your work. So, if you have any feedback on how I could improve the package, it would be really appreciated. Nonetheless, I still have ideas of future work:
* Add a `dataset` module: I think that it is an important missing tool of the package. Right now I create a dumb toy dataset in all the [examples in the documentation](https://pyts.readthedocs.io/en/latest/auto_examples/index.html). Adding a couple of datasets in the package directly (I would obviously need to contact authors to get permission to do so) like the iris dataset in scikit-learn would make the examples more relevant in my opinion. Adding a function to download datasets from the [Time Series Classification Repository](http://www.timeseriesclassification.com) (similarly to [sklearn.datasets.fetch_openml](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html) or [sklearn.datasets.fetch_mldata](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_mldata.html)) would be quite useful too. Being able to generate a toy dataset like [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) would be a nice addition. If you have any idea about generating a classification dataset for time series, with any number of classes and any number of timestamps, feel free to comment, I would be really interested. Right now I only know the Cylinder-Bell-Funnel dataset, but it is quite limiting (128 timestamps and 3 classes).
* Add a `multivariate` module. Currently the package provides no tools to deal with multivariate time series. Like binary classifiers that need extending for multiclass classification, adding a voting classifier (with a classifier for each feature of the multivariate time series) would be useful, as well as specific algorithms for multivariate time series.
* Make the package available on Anacloud Cloud through the `conda-forge` channel. `conda` seems to be quite popular thanks to the utilities it provides and making the package installable with `conda` could be a plus.
* Update the required versions of the dependencies: Currently the required versions of the dependencies are the versions that I use on my computer. I'm quite confident that older versions for some packages could work, but I have no idea how to determine them (I exclude doing a backward gridsearch until continuous integreation fails). Are there any tools that can try to guess the minimum versions of the packages, by looking at what functions are used from each package for instance?
* Implement more algorithms: Time Series Bag-of-Features, shapelet-based algorithms, etc. A lot of algorithms are not available in the package currently. Adding more metrics specific to time series would also be great.


## Acknowledgements

Looking back at the history of this project, I realize how much I learnt thanks to the scientific Python community: there are so many open source well-documented tools that are made available, it is pretty amazing.

I would also like to thank the authors of papers that I contacted in order to get more information about the algorithms that they presented. I always received quick, kind answers. Special thanks to Patrick Schfer, who received a lot of emails from me and always replied.

I would like to thank all the people involved in the [Time Series Classification Repository](http://www.timeseriesclassification.com). It is an awesome tool with datasets freely available and reported results for each algorithm.

Finally, I would like to thank every contributor to the project, as well as people helping making this package better through opening issues or sending me emails.


## Conclusion

Working on this project has been a blast. Sometimes learning takes a lot of time, and I experienced it quite often, but I think that it is worth it. I work on this project on my spare time, so I cannot spend as much time as much as I would like, but I think that it gets slowly but steadily better. There are still a lot of things that are a bit confusing to me (all the configuration files for CI and documentation, managing a git repository with several branches and several contributors), and seeing room for improvement is also an exciting part of this experience.

There was a [post about machine learning on time series](https://www.reddit.com/r/MachineLearning/comments/9ofd7x/d_machine_learning_on_time_series_data/) on this subreddit several months ago. If you were interested in what was discussed in this post (and more specially in the top comment), you might be interested in pyts.

Thank you very much for reaching the end of this long post. If you have some time to give me any feedback, it would mean a lot to me. Have a very nice day!",40,247,False,self,,,,,
586,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,17,bb5ayo,medium.com,Confounding via experimenting - One of the fundamental problems with current ML techniques,https://www.reddit.com/r/MachineLearning/comments/bb5ayo/confounding_via_experimenting_one_of_the/,elcric_krej,1554797469,,0,1,False,default,,,,,
587,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,17,bb5dj5,youtube.com,Using the Google Coral USB Accelerator to implement real-time object detection and video annotation (10fps),https://www.reddit.com/r/MachineLearning/comments/bb5dj5/using_the_google_coral_usb_accelerator_to/,markwest1972,1554798080,,1,1,False,https://b.thumbs.redditmedia.com/hNiyKwjlRGPtYf_DjMm2itlo6ZupgTlWqIJhNOeyePo.jpg,,,,,
588,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,17,bb5dpv,self.MachineLearning,What is the best episode of a podcast or article that describes how artificial intelligence is good?,https://www.reddit.com/r/MachineLearning/comments/bb5dpv/what_is_the_best_episode_of_a_podcast_or_article/,Doctor_who1,1554798128,What is the best episode of a podcast or article that describes how artificial intelligence is good?,0,1,False,self,,,,,
589,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,17,bb5fvj,industrywired.com,"Computer vision, Machine Learning, and Supervised Learning; An In-depth Understanding towards AI",https://www.reddit.com/r/MachineLearning/comments/bb5fvj/computer_vision_machine_learning_and_supervised/,industrywired,1554798638,,0,1,False,default,,,,,
590,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,17,bb5kk1,self.bigdata,K-means using R and GPU,https://www.reddit.com/r/MachineLearning/comments/bb5kk1/kmeans_using_r_and_gpu/,Elrondsen,1554799834,,0,1,False,default,,,,,
591,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,18,bb5rwu,manticoresearch.com,TF-IDF in a nutshell,https://www.reddit.com/r/MachineLearning/comments/bb5rwu/tfidf_in_a_nutshell/,snikolaev,1554801571,,0,1,False,default,,,,,
592,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,19,bb6a5e,self.MachineLearning,"Guys, I'm hosting my first presentation, care to help out?",https://www.reddit.com/r/MachineLearning/comments/bb6a5e/guys_im_hosting_my_first_presentation_care_to/,syslynx,1554805678,[removed],0,1,False,self,,,,,
593,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,19,bb6axi,self.MachineLearning,How to proceed after bachelor's degree.,https://www.reddit.com/r/MachineLearning/comments/bb6axi/how_to_proceed_after_bachelors_degree/,imdien,1554805851,"Hi everyone.

I will finish my bachelor's degree in biology this summer. I am certain that i want to work in data science (especially ML/DL) in the future, however, I am not certain how to proceed after I finish my bachelor's. My options are:

\-Find a job

\-Start a master's program in quantitative Biology or something similar

\-try to get into a master's program in data science (probably not that realistic, since they often require a bachelor's in Informatics or statistics)

I have set a lot of focus on math, statistics and data analyis during my studies and i am currently working on a deep learning project where i do image recognition using CNNs, so I think I should have enough expertise to find an entry level job. I just wanted to ask what you guys think I should do / not do.

Thanks for your suggestions!",0,1,False,self,,,,,
594,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,19,bb6b4a,medium.com,Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bb6b4a/machine_learning/,redditvivek,1554805893,,0,1,False,https://b.thumbs.redditmedia.com/cWrIi2hoCSJAgl73lol-ohwsPzYI0QKeGHXaiG444_Y.jpg,,,,,
595,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,19,bb6h1d,self.MachineLearning,Expectation Propagation,https://www.reddit.com/r/MachineLearning/comments/bb6h1d/expectation_propagation/,BhanujeetC,1554807189,[removed],0,1,False,self,,,,,
596,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,20,bb6rl6,self.MachineLearning,[P] OverBoard - native Python dashboard for ML experiments,https://www.reddit.com/r/MachineLearning/comments/bb6rl6/p_overboard_native_python_dashboard_for_ml/,brainggear,1554809254,"Hi everyone, I made this dashboard/GUI for myself over the past few months and decided to share it in case it's useful to other people!

For many years I've always used tons of custom visualizations for my CV/ML algorithms (what does your model predict for some images vs. the ground truth; what do the weights look like visually). Although it always took some effort, there were countless times when I solved problems or tackled failure modes that would've been invisible otherwise.

Anyway, unfortunately this doesn't mesh well with today's workflow of running remote experiments on a cluster over SSH. I found that TensorBoard is great for managing experiments but limits me to default plot types; VisDom on the other hand is not so great for managing (e.g. comparing curves) and needs a lot of custom code to reach its full potential. I'm also not a fan of mixing HTML, JS and Python for GUIs. This is not an indictment of these tools; just a personal preference!

So I made OverBoard. You can run it locally, have any custom MatPlotLib visualizations, but it also comes with nice defaults for comparing experiments and visualizing tensors/images. There are several more features in case you want to check it out, and a video behind the link:

[https://github.com/jotaf98/overboard](https://github.com/jotaf98/overboard)",1,42,False,self,,,,,
597,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,20,bb6vdw,self.MachineLearning,Algorithms to detect activities from Video,https://www.reddit.com/r/MachineLearning/comments/bb6vdw/algorithms_to_detect_activities_from_video/,FreddyShrimp,1554809934,[removed],0,1,False,self,,,,,
598,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,20,bb6xoe,oodlestechnologies.com,How Machine Learning Will Aid Neural Science,https://www.reddit.com/r/MachineLearning/comments/bb6xoe/how_machine_learning_will_aid_neural_science/,tech-info,1554810369,,0,1,False,default,,,,,
599,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,20,bb6yga,arxiv.org,[R] Fast Interactive Object Annotation with Curve-GCN,https://www.reddit.com/r/MachineLearning/comments/bb6yga/r_fast_interactive_object_annotation_with_curvegcn/,ClamChowderBreadBowl,1554810509,,9,3,False,default,,,,,
600,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,20,bb6ywh,oodlestechnologies.com,How Machine Learning Impacts Customer Experience,https://www.reddit.com/r/MachineLearning/comments/bb6ywh/how_machine_learning_impacts_customer_experience/,oodlestechnologies,1554810593,,0,1,False,default,,,,,
601,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,20,bb6zn7,self.MachineLearning,What is the point of the Viterbi Algorithm for HMM,https://www.reddit.com/r/MachineLearning/comments/bb6zn7/what_is_the_point_of_the_viterbi_algorithm_for_hmm/,saintjimmie,1554810731,[removed],0,1,False,self,,,,,
602,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,21,bb74kv,oodlestechnologies.com,Deep Learning vs Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bb74kv/deep_learning_vs_machine_learning/,oodlestechnologies,1554811597,,0,1,False,default,,,,,
603,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,21,bb757i,oodlestechnologies.com,Best Chatbot Platform Tools For Developers,https://www.reddit.com/r/MachineLearning/comments/bb757i/best_chatbot_platform_tools_for_developers/,tech-info,1554811703,,0,1,False,https://b.thumbs.redditmedia.com/x2rMIWiRNEdmN1r32qulxQvjotuGCvKesgZGTtFxGsQ.jpg,,,,,
604,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,21,bb75c4,arxiv.org,[1904.03665] Learning to Control Highly Accelerated Ballistic Movements on Muscular Robots,https://www.reddit.com/r/MachineLearning/comments/bb75c4/190403665_learning_to_control_highly_accelerated/,ihaphleas,1554811727,,4,6,False,default,,,,,
605,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,21,bb78bz,self.MachineLearning,What competes with TF Lite for on device inference?,https://www.reddit.com/r/MachineLearning/comments/bb78bz/what_competes_with_tf_lite_for_on_device_inference/,bartturner,1554812243,[removed],0,1,False,self,,,,,
606,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,21,bb7mno,tracker.ml,Version control platform for machine learning models (WIP). We welcome all feedback.,https://www.reddit.com/r/MachineLearning/comments/bb7mno/version_control_platform_for_machine_learning/,Amogh100,1554814667,,0,1,False,default,,,,,
607,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,22,bb7pr7,blog.sicara.com,[D] Which are your favorite AI articles of this month?,https://www.reddit.com/r/MachineLearning/comments/bb7pr7/d_which_are_your_favorite_ai_articles_of_this/,emnak,1554815182,,0,2,False,default,,,,,
608,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,22,bb7qmj,self.MachineLearning,[D] Book reviews,https://www.reddit.com/r/MachineLearning/comments/bb7qmj/d_book_reviews/,IborkedyourGPU,1554815315,"Where do you go for reviews of book about Machine Learning in general, and Reinforcement Learning/Deep Learning in particular?",1,1,False,self,,,,,
609,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,22,bb7qrg,self.MachineLearning,"tracker.ml, A version control platform for machine learning models.",https://www.reddit.com/r/MachineLearning/comments/bb7qrg/trackerml_a_version_control_platform_for_machine/,Amogh100,1554815340,[removed],0,1,False,self,,,,,
610,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,22,bb7ysr,self.MachineLearning,[D] How can I break into ML &amp; healthcare research in Europe?,https://www.reddit.com/r/MachineLearning/comments/bb7ysr/d_how_can_i_break_into_ml_healthcare_research_in/,mk10hk,1554816632,"Hi guys! I'm an Italian student enrolled in a M.Sc. Data Science degree and  I'd really like to start working on Machine Learning applied to Healthcare (the alternative is climate data).

I'm looking for any kind of tips you have for moving towards my goal, in particular:

* Which are the best research centers &amp; university in Europe where I can apply for a PhD/research position?
* Which material (books, papers and video lectures) is good for improving my knowledge of the field?

Thanks a lot, have a nice day!",8,2,False,self,,,,,
611,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,22,bb82jz,i.redd.it,When you want to compare two ROC curves,https://www.reddit.com/r/MachineLearning/comments/bb82jz/when_you_want_to_compare_two_roc_curves/,borislestsov,1554817236,,0,1,False,default,,,,,
612,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,22,bb84qt,self.MachineLearning,[R] IBMs New Quantum Algorithm Can Make Artificial Intelligence SuperPowerful,https://www.reddit.com/r/MachineLearning/comments/bb84qt/r_ibms_new_quantum_algorithm_can_make_artificial/,navin49,1554817589,"Artificial intelligence and quantum computing are one of the most powerful technologies and soon both of them are going to revolutionize our old way of computing information.

Though the certain aspects of their mathematical foundation are little different,the combination of both the technologiesshows a promising boost in many different areas such asaccessing more computationally complex feature spaces.

IBM recently released its new research, IBM researchers presents a newlydeveloped and tested [quantum algorithms](https://techgrabyte.com/ibm-quantum-algorithm-ai/) thatcould sort and classify complex data sets faster than that of normal algorithms running on classical computers struggle to handle.

If we talk about ordinary computers, they perform machine learning by comparing mathematical representations of data.The more precisely that data can be classified according to specific characteristics, or features, the better the AI will perform.

IBMquantum algorithmsdemonstrate how quantum computing can be usedto classify data with the use ofshort-depth circuits,that also dealt with the expected decoherence (loss of state) in a quantum computer.

[Here is the report](https://techgrabyte.com/ibm-quantum-algorithm-ai/)**.**

*What you think, how Quantum Computers will revolutionize the field of AI and much boost it can give to its processing.*",14,0,False,self,,,,,
613,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,22,bb883h,twitter.com,Hiring indicates Apple are placing a higher priority on Machine Learning technology.,https://www.reddit.com/r/MachineLearning/comments/bb883h/hiring_indicates_apple_are_placing_a_higher/,John_Muck,1554818113,,0,1,False,https://b.thumbs.redditmedia.com/GK5rC-IQBwKCFJm5RJitSQ7Jg_hoRFowT8ptbB8ZRhU.jpg,,,,,
614,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,23,bb8a18,tracker.ml,tracker.ml: A version control platform for machine learning models,https://www.reddit.com/r/MachineLearning/comments/bb8a18/trackerml_a_version_control_platform_for_machine/,Amogh100,1554818417,,1,1,False,default,,,,,
615,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,23,bb8acb,self.MachineLearning,DQN: Q values decreasing and score doesn't improving on OpenAI-Gym Atari,https://www.reddit.com/r/MachineLearning/comments/bb8acb/dqn_q_values_decreasing_and_score_doesnt/,MetallicaSPA,1554818465,[removed],0,1,False,self,,,,,
616,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,23,bb8bff,self.MachineLearning,[P] My model counts trains...,https://www.reddit.com/r/MachineLearning/comments/bb8bff/p_my_model_counts_trains/,beatthebrush,1554818623,"The little plastic trains for the Ticket to Ride board game.

Video of it in action: https://www.youtube.com/watch?v=HN8aimsFmnc

Or try it out if you want: https://itunes.apple.com/us/app/pointsman/id1447780441?mt=8

I just wanted to say thank you to anyone and everyone who has worked hard to make machine learning so accessible over the past few years.  I have been writing iOS/Android/Unity applications for the last ten years and for most of those I would have considered doing this kind of feature well to extremely difficult. Now with toolkits like TF/Keras/PyTorch and the wide range of information freely available it is not only feasible but is rather easily achievable. I have really enjoyed experimenting with ML for the last two years, and that wouldn't have been possible without the unimaginable amount of work that us to where we are today.

So to all you wonderful people out there, cheers!


(I'm just a tinkerer but happy to answer any questions on how it works if that's of interest to anyone)",4,20,False,self,,,,,
617,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,23,bb8ggz,agentanakinai.wordpress.com,"My intro to Machine Learning currently includes 8 courses. If anyone can recommend any additional courses to take, I would greatly appreciate it.",https://www.reddit.com/r/MachineLearning/comments/bb8ggz/my_intro_to_machine_learning_currently_includes_8/,Agent_ANAKIN,1554819378,,0,1,False,default,,,,,
618,MachineLearning,t5_2r3gv,2019-4-9,2019,4,9,23,bb8nth,self.MachineLearning,Machine Learning Part 8 -AccuracyMAE RMSE Python Machine Learning For Beginners,https://www.reddit.com/r/MachineLearning/comments/bb8nth/machine_learning_part_8_accuracymae_rmse_python/,mritraloi6789,1554820468,[removed],0,1,False,self,,,,,
619,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,0,bb92br,self.MachineLearning,[P] Using Reinforcement Learning to Design a Better Rocket Engine,https://www.reddit.com/r/MachineLearning/comments/bb92br/p_using_reinforcement_learning_to_design_a_better/,hszafarek,1554822588,"I wanted to share this post of an Insight alum, now an ML product manager, who used reinforcement learning to design a better rocket engine.",3,10,False,self,,,,,
620,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,0,bb9c3j,twitter.com,Advances in Financial Industry with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bb9c3j/advances_in_financial_industry_with_machine/,oodlestechnologies,1554823980,,0,1,False,default,,,,,
621,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,0,bb9dbl,self.MachineLearning,How is locally weighted regression in machine learning a non-parametric learning algorithm?,https://www.reddit.com/r/MachineLearning/comments/bb9dbl/how_is_locally_weighted_regression_in_machine/,cipher1202,1554824161,[removed],0,1,False,self,,,,,
622,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,0,bb9dff,thecoderr.com,Google Landmark Classification Challenge,https://www.reddit.com/r/MachineLearning/comments/bb9dff/google_landmark_classification_challenge/,the_coder_dot_py,1554824177,,0,1,False,https://b.thumbs.redditmedia.com/uUNict8NnBAFh_W_gValmE7kW7GpZYu2RseBTB1d1dQ.jpg,,,,,
623,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,0,bb9ksk,anl.gov,"[N] Through machine learning, new model holds water",https://www.reddit.com/r/MachineLearning/comments/bb9ksk/n_through_machine_learning_new_model_holds_water/,LoyalSol,1554825217,,0,1,False,default,,,,,
624,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bb9otd,self.MachineLearning,Multi-labelled Classifier,https://www.reddit.com/r/MachineLearning/comments/bb9otd/multilabelled_classifier/,vicmartins,1554825772,[removed],0,1,False,self,,,,,
625,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bb9p4q,self.MachineLearning,[R] My Machine Learning Research Interview Experience,https://www.reddit.com/r/MachineLearning/comments/bb9p4q/r_my_machine_learning_research_interview/,generalizederror,1554825810,"Hi guys,

&amp;#x200B;

it seems like a lot of people have questions about finding jobs in ML, or what the typical interview process looks like. Since I've gone through all that recently, I thought it might be helpful to share my experiences: 

[https://generalizederror.github.io/My-Machine-Learning-Research-Jobhunt/](https://generalizederror.github.io/My-Machine-Learning-Research-Jobhunt/)

&amp;#x200B;

Enjoy",0,1,False,self,,,,,
626,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bb9udl,youtube.com,[R] - The bitter lesson,https://www.reddit.com/r/MachineLearning/comments/bb9udl/r_the_bitter_lesson/,fuck_your_diploma,1554826524,,1,1,False,https://b.thumbs.redditmedia.com/55i4NIg_BUxd-Ki2IqMmwnNcVLSLAZy-VYJ0A_KBnAo.jpg,,,,,
627,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bb9umg,self.MachineLearning,[D] My Machine Learning Research Job Interview Experience,https://www.reddit.com/r/MachineLearning/comments/bb9umg/d_my_machine_learning_research_job_interview/,generalizederror,1554826554,"Hi guys,

it seems like a lot of people have questions about finding jobs in ML, or what the typical interview process looks like. Since I've gone through all that recently, I thought it might be helpful to share my experiences:

[https://generalizederror.github.io/My-Machine-Learning-Research-Jobhunt/](https://generalizederror.github.io/My-Machine-Learning-Research-Jobhunt/)

Enjoy",127,479,False,self,,,,,
628,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bb9voa,self.MachineLearning,[D] 100$ Google Cloud Platform Request Form,https://www.reddit.com/r/MachineLearning/comments/bb9voa/d_100_google_cloud_platform_request_form/,mannbhai321,1554826698,[removed],0,1,False,self,,,,,
629,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bb9xxb,youtu.be,Introduction to Machine Learning | What is Machine Learning | Supervised and Unsupervised Learning,https://www.reddit.com/r/MachineLearning/comments/bb9xxb/introduction_to_machine_learning_what_is_machine/,yashica_,1554827018,,0,1,False,default,,,,,
630,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bba0ad,self.MachineLearning,"Image Classification with MNIST Dataset - A site aimed at building a Data Science, Artificial Intelligence and Machine Learning empire.",https://www.reddit.com/r/MachineLearning/comments/bba0ad/image_classification_with_mnist_dataset_a_site/,sovit-123,1554827352,[removed],0,1,False,self,,,,,
631,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bba1eu,ai.facebook.com,Aroma: Using machine learning for code recommendation,https://www.reddit.com/r/MachineLearning/comments/bba1eu/aroma_using_machine_learning_for_code/,hal00m,1554827506,,0,1,False,default,,,,,
632,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bba1pl,arxiv.org,[R] [1904.03116] Weakly Supervised Action Segmentation Using Mutual Consistency,https://www.reddit.com/r/MachineLearning/comments/bba1pl/r_190403116_weakly_supervised_action_segmentation/,yassersouri,1554827545,,5,0,False,default,,,,,
633,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bba5up,self.MachineLearning,[P] Deep Learning in Production: Sentiment Analysis with The Transformer,https://www.reddit.com/r/MachineLearning/comments/bba5up/p_deep_learning_in_production_sentiment_analysis/,bluebuff,1554828085,"**TL;DR: We take an estimator from Tensor2Tensor and ship it in a Data-&gt;Training-&gt;Serving pipeline**

&amp;#x200B;

Hi everyone, as someone being on the engineering side of ML, I think there could be more posts about actually applying these novel architectures and techniques from research to real world problems. I wrote about a production deployment using the Transformer model from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) as an API for sentiment analysis. This is all done with an open source stack (TensorFlow, Spark, Kubernetes, Cortex etc.).

&amp;#x200B;

Let me know what you think!

[https://medium.com/cortex-labs/deep-learning-in-production-sentiment-analysis-with-the-transformer-model-7fa053d0c85b](https://medium.com/cortex-labs/deep-learning-in-production-sentiment-analysis-with-the-transformer-model-7fa053d0c85b)",8,25,False,self,,,,,
634,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,1,bba937,self.MachineLearning,[D] Should you be able to copyright output of an open-source machine learning model?,https://www.reddit.com/r/MachineLearning/comments/bba937/d_should_you_be_able_to_copyright_output_of_an/,8solutions,1554828510,"I'm interested in how copyright, regulation, and machine learning might interact in the future. The recent ""text generation"" model from OpenAI creates an article-length sequence of text from a very short input. One concern is that this will lead to a fake news apocalypse -- perhaps one regulatory strategy is that auto-generated news must be:

a. Labeled as ""Machine Generated""

b. Not copyrighted

This might take away profit incentive from fake news sites (maybe, maybe not...).

What if that model is hosted online and everybody has access to it? Should I be able to copyright output that is 100x longer than my input (even if I have copyright on the input)?

Clearly, the very concept of copyright is on shaky ground in the Internet era, but I'm curious how you all think about Intellectual Rights, incentives, and regulation in the ML era.",27,8,False,self,,,,,
635,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,2,bbawnt,medium.com,Analyzing College Admissions Data,https://www.reddit.com/r/MachineLearning/comments/bbawnt/analyzing_college_admissions_data/,alexa_y,1554831716,,0,1,False,default,,,,,
636,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,3,bbbcjj,self.MachineLearning,What level of contribution is sufficient/required as an undergrad to be a co-author?,https://www.reddit.com/r/MachineLearning/comments/bbbcjj/what_level_of_contribution_is_sufficientrequired/,nerdninja08,1554833927,[removed],0,1,False,self,,,,,
637,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,3,bbbif9,distill.pub,[R] (distill.pub) Open Questions about Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/bbbif9/r_distillpub_open_questions_about_generative/,AsIAm,1554834751,,0,1,False,default,,,,,
638,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,3,bbbjq2,self.MachineLearning,[D] A primer on TensorFlow 2.0,https://www.reddit.com/r/MachineLearning/comments/bbbjq2/d_a_primer_on_tensorflow_20/,akshayka,1554834934,"I've noticed some confusion on what TensorFlow 2.0 is on this subreddit. I worked as an engineer on parts of TensorFlow 2.0, specifically on imperative (or ""eager"") execution. I'll try to clear up some of the confusion here. I'm also happy to answer any questions to the best of my ability.

(I'm no longer employed by Alphabet / Google Brain, so these words are my own.)

TF 2.0 is a backwards-incompatible update to TF's (1) execution model and (2) API. It is currently in [alpha](https://www.tensorflow.org/alpha).

(1) TF 2.0 executes operations imperatively (or ""eagerly"") by default; this means that it will feel similar to PyTorch or NumPy. It also provides a just-in-time tracer (`tf.function`) that rewrites Python functions that execute TF (2.0) operations into graphs. This tracer also rewrites Python ASTs to replace tensor-dependent Python control flow to TF control flow using [autograph](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph), meaning that you don't need to use constructs like `tf.cond` or `tf.while_loop`. Using this tracer is optional. The tracer is similar in spirit to `torch.jit.trace` and `TorchScript`, but the usage and semantics are different. It's also similar to [JAX's `jit`](https://github.com/google/jax).

One consequence of this change is that in 2.0, there's no global graph, no global collections, no `get_variable`, no `custom_getter`s, no `Session`, no feeds, no fetches, no `placeholder`s, no `control_dependencies`, no variable initializers, etc., even when you're using `tf.function`. There are many other things that have been excised from the API. 

(2)  In TF 1.x, there were many high-level APIs for neural networks (e.g., see everything under `tf.contrib`, which no longer exists in 2.0). Many users found this confusing, especially because these APIs were similar but different and incompatible. With 2.0, TF has standardized on `tf.keras`, which is essentially an implementation of the Keras API specification, customized for TF's need.

That said, TF 2.0 has many [low-level APIs](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf), for things like numerical computation (`tf`, `tf.math`), linear algebra (`tf.linalg`), automatic differentiation (`tf.GradientTape`), state (`tf.Variable`), neural networks (`tf.nn`), stochastic gradient-based optimization (`tf.optimizers`, `tf.losses`), dataset munging (`tf.data`). I've only named a few of these low-level APIs. If you don't want to use `tf.keras`, you're free to use these low-level APIs directly. Note that you can also directly use the object oriented layers in `tf.keras.layers` without wrapping them in `tf.keras.Sequential` or `tf.keras.Model`.

I've written a more comprehensive, technical primer on TF 2.0, which is available as a [blog post](https://www.debugmind.com/2019/04/07/a-primer-on-tensorflow-2-0/) and as a [python notebook](https://colab.research.google.com/drive/1gEkd_D-b8Y0Quxdz-OoJL_sFWGhP5lSR). There's also an [official guide](https://www.tensorflow.org/alpha/guide/effective_tf2) from the TF team.",38,74,False,self,,,,,
639,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,3,bbboza,self.MachineLearning,[D] What level of contribution is sufficient/required as an undergrad to be a co-author?,https://www.reddit.com/r/MachineLearning/comments/bbboza/d_what_level_of_contribution_is/,nerdninja08,1554835669,"Hi everyone. I'm an undergraduate student studying Computer Science in India. The post might be a bit long however I urge you to please bear with me.

From the summer of 2017, I had been working with a professor at a well-respected institution on a research project revolving around semi-supervised clustering. I was working under a Ph.D. student of the concerned professor. My contributions to the research project did involve helping out with the theoretical aspects of the problem statement but the heavy lifting on that front was mostly done by the Ph.D. student. However, the Ph.D. student had very little experience in programming and thus almost majority of the codebase was written by me. The codebase involved implementing a prior research paper on which we were building our work upon, implementing the methodology proposed by us as well as performing the majority of the experiments. After working on the project for quite a considerable amount of time, we proceeded to write a paper on it to submit to a workshop. The paper submission handling was all done by the Ph.D. student and the professor. My contact with the Ph.D. student and the professor was very limited prior one week to the workshop deadline as I was caught up in my semester end university examinations. On enquiring about the status of the paper after my examinations, I got to know that the paper had been submitted without my name (either as a co-author or in the contributions list). This came as a big shock to me. On asking the professor about my contributions not being regarded, the professor replied that simply performing experiments and implementing the code isn't sufficient to get your name in the paper. There was a lot of back and forth wherein I tried to explain all my contributions to the project - either programming related or algorithmic input related. However, the professor, as well as the Ph.D. student, were quite fixated on their decision of not including my name in the paper. The paper ultimately got rejected from the workshop, soon after which they released the paper on arxiv. From what I've heard from one of my colleagues working in the same research group, they are now further modifying it to send it to another workshop/conference.

My questions to you guys are :

1. Was my contribution actually not enough to be regarded in any way in the paper? (This was my first such research engagement so I actually have no idea whether I was treated unjustly)
2. I have the entire codebase for the project (as I wrote the majority of it myself), so if I was actually treated in an unjust manner, do you have any advice for me on something I could do (like probably open sourcing the codebase) that could somehow help me as I spent a great deal of time and a lot of hard work working on this project.

The current status of the project is that they're modifying it to send it somewhere else and the draft submitted to the workshop from where it got rejected has been published to arxiv.",30,4,False,self,,,,,
640,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,3,bbbqo2,self.MachineLearning,Could we use ML and keplar data + data about our solar system to accurately predict where life is in other solar systems?,https://www.reddit.com/r/MachineLearning/comments/bbbqo2/could_we_use_ml_and_keplar_data_data_about_our/,pacewindew,1554835906,[removed],0,1,False,self,,,,,
641,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,4,bbbxaw,self.MachineLearning,[P] Ignite - High-level library to help with training neural networks in PyTorch,https://www.reddit.com/r/MachineLearning/comments/bbbxaw/p_ignite_highlevel_library_to_help_with_training/,pie_oh_my_,1554836797,"We are happy to announce the v0.2.0 release of *ignite*.

Link to our github - https://github.com/pytorch/ignite
Link to docs - https://pytorch.org/ignite/index.html

Here is a high level description of changes from the last release:
* Added multilabel option for Accuracy, Precision, Recall.
* Removed deprecated BinaryAccuracy and CategoricalAccuracy, in favor of Accuracy.
* Added ConfusionMatrix, IoU, mIoU.
* Operation on Metrics including PyTorch operators and indexing. 
* Added a regression modules with a variety of metrics.
* Added loggers for Tensorboard, Visdom, Polyaxon. 
* Improved ProgressBar with notebook support. 
* Added Parameter Schedulers and a variety of Learning Rate Schedulers. 
* Improved docs, added FAQ section.

release notes - https://github.com/pytorch/ignite/releases/tag/v0.2.0


We would like to thank our community and all our contributors for the issues, PRs and support. We welcome contributions! 

Happy training!",4,16,False,self,,,,,
642,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,4,bbc5xq,medium.com,Snip Converts Math Screenshots Into LaTeX,https://www.reddit.com/r/MachineLearning/comments/bbc5xq/snip_converts_math_screenshots_into_latex/,gwen0927,1554837956,,0,1,False,https://b.thumbs.redditmedia.com/9dzB_pLsq4NmHa4mbhqGGyhZJlJT5794oU4Y7YjN16g.jpg,,,,,
643,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,5,bbd4j5,self.MachineLearning,"[P] Coding challenge asks me to make two 3D Gaussian Models for a set of bird images and sky images, then binarize them. Why cant I find any info on 3D Gaussian Modeling in ML?",https://www.reddit.com/r/MachineLearning/comments/bbd4j5/p_coding_challenge_asks_me_to_make_two_3d/,Hycinos,1554842685,"Ive been using Tensorflow as a classifier at my current position, and am applying to other ML jobs. One coding challenge is asking me to model images of birds and sky using two 3D Gaussian models, but I cant seem to find any examples anywhere. Is there some keyword Im missing - or is the prompt incorrect?",10,3,False,self,,,,,
644,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,5,bbd5dv,self.MachineLearning,Is springboards ai/machine learning track worth the price?,https://www.reddit.com/r/MachineLearning/comments/bbd5dv/is_springboards_aimachine_learning_track_worth/,arturmame,1554842796,[removed],0,1,False,self,,,,,
645,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,5,bbda2g,self.MachineLearning,"Hello, please answer this question?1",https://www.reddit.com/r/MachineLearning/comments/bbda2g/hello_please_answer_this_question1/,Doctor_who1,1554843458,[removed],0,0,False,https://b.thumbs.redditmedia.com/b6ZQC-aLPUHytS-PnhZmhZDIyrhedHZX_SrUobOvNDM.jpg,,,,,
646,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,6,bbdh8a,self.MachineLearning,More than ML: Guide to the Components of AI,https://www.reddit.com/r/MachineLearning/comments/bbdh8a/more_than_ml_guide_to_the_components_of_ai/,andrea_manero,1554844463,[removed],0,1,False,self,,,,,
647,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,6,bbdu6e,eurekalert.org,New technique cuts AI training time by more than 60 percent [News],https://www.reddit.com/r/MachineLearning/comments/bbdu6e/new_technique_cuts_ai_training_time_by_more_than/,DrLionelRaymond,1554846328,,0,1,False,default,,,,,
648,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,6,bbdz32,github.com,[P] Pytorch GAN Zoo: A GAN Toolbox for Researchers and Developers  FB AI,https://www.reddit.com/r/MachineLearning/comments/bbdz32/p_pytorch_gan_zoo_a_gan_toolbox_for_researchers/,ramblinscarecrow,1554847056,,0,1,False,default,,,,,
649,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,7,bbeeim,self.MachineLearning,[R] New England Machine Learning Day 2019,https://www.reddit.com/r/MachineLearning/comments/bbeeim/r_new_england_machine_learning_day_2019/,ch3njust1n,1554849342,"This year at Northeastern on May 10th!

https://www.microsoft.com/en-us/research/event/new-england-machine-learning-day-2019/",1,4,False,self,,,,,
650,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,7,bbeh9l,self.MachineLearning,"Predictive Lead Scoring - For Beginners, Help!",https://www.reddit.com/r/MachineLearning/comments/bbeh9l/predictive_lead_scoring_for_beginners_help/,hvranka,1554849760,[removed],0,1,False,self,,,,,
651,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,8,bber25,self.MachineLearning,[R] Open Questions about Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/bber25/r_open_questions_about_generative_adversarial/,hardmaru,1554851306,"*New distill.pub [article](https://distill.pub/2019/gan-open-problems/) about future direction of GAN research*

**Open Questions about Generative Adversarial Networks**

What wed like to find out about GANs that we dont know yet.

1. What are the trade-offs between GANs and other generative models?

2. What sorts of distributions can GANs model?

3. How can we Scale GANs beyond image synthesis?

4. What can we say about the global convergence of the training dynamics?

5. How should we evaluate GANs and when should we use them?

6. How does GAN training scale with batch size?

7. What is the relationship between GANs and adversarial examples?

https://distill.pub/2019/gan-open-problems/",13,47,False,self,,,,,
652,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,8,bbes15,eurekalert.org,New technique cuts AI training time by more than 60 percent,https://www.reddit.com/r/MachineLearning/comments/bbes15/new_technique_cuts_ai_training_time_by_more_than/,DrLionelRaymond,1554851468,,0,1,False,https://b.thumbs.redditmedia.com/_LVWPuYRTgm0RGtpQuKM6n_jHoGSDSTX0GJCdLwlOwg.jpg,,,,,
653,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,8,bbet9e,eurekalert.org,[N] New technique cuts AI training time by more than 60 percent,https://www.reddit.com/r/MachineLearning/comments/bbet9e/n_new_technique_cuts_ai_training_time_by_more/,DrLionelRaymond,1554851643,,0,1,False,https://b.thumbs.redditmedia.com/_LVWPuYRTgm0RGtpQuKM6n_jHoGSDSTX0GJCdLwlOwg.jpg,,,,,
654,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,8,bbetf3,self.MachineLearning,Sample,https://www.reddit.com/r/MachineLearning/comments/bbetf3/sample/,yfletberliac,1554851666,[removed],0,1,False,self,,,,,
655,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,8,bbew1g,self.MachineLearning,[R] From Attention in Transformers to Dynamic Routing in Capsule Nets,https://www.reddit.com/r/MachineLearning/comments/bbew1g/r_from_attention_in_transformers_to_dynamic/,baylearn,1554852066,"A comprehensive [blog post](https://staff.fnwi.uva.nl/s.abnar/?p=108) with many references that attempts to highlight the connection between Transformer and Capsule Networks.

*In this post, we go through the main building blocks of transformers and capsule networks and try to draw a connection between different components of these two models. Our main goal here is to understand if these models are inherently different and if not, how they relate.*

https://staff.fnwi.uva.nl/s.abnar/?p=108",3,23,False,self,,,,,
656,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,8,bbeypc,self.MachineLearning,Any samples using Tensorflow Eager Execution in Reinforcement Learning??,https://www.reddit.com/r/MachineLearning/comments/bbeypc/any_samples_using_tensorflow_eager_execution_in/,Rowing0914,1554852483,[removed],0,1,False,self,,,,,
657,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,9,bbfbfw,arxiv.org,[R] Self-Adapting Goals Allow Transfer of Predictive Models to New Tasks,https://www.reddit.com/r/MachineLearning/comments/bbfbfw/r_selfadapting_goals_allow_transfer_of_predictive/,milaworld,1554854581,,1,47,False,default,,,,,
658,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,9,bbfhoo,gris.fr,Gris Constructeur France - Machine  glace  l&amp;#039; italienne au meilleur rapport qualit prix,https://www.reddit.com/r/MachineLearning/comments/bbfhoo/gris_constructeur_france_machine__glace__l039/,dickypusharddfc,1554855576,,0,1,False,default,,,,,
659,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,9,bbfv0j,self.MachineLearning,[D] Theory contributions to Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/bbfv0j/d_theory_contributions_to_deep_learning/,woodpropagation,1554857835,"I've seen a lot of progress in theoretical understanding of deep learning in these past few years, on generalization, convergence of SGD, and so on. But I couldn't find a single theoretical result that had actual implications for deep learning in practice. Even though there were many optimizers and capacity metrics proposed lately, we're still using mostly SGD with L2 regularization (and also dropout and batch norm, which were not theoretical results), which seems to work the best ([https://arxiv.org/pdf/1812.04529.pdf](https://arxiv.org/pdf/1812.04529.pdf), etc).

&amp;#x200B;

Are there any examples of theory which ended up having a contribution in practice too?",2,2,False,self,,,,,
660,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,10,bbfx66,self.MachineLearning,[D] Have applied to a lot of ML startups on Angel.co for summer internship. No replies.,https://www.reddit.com/r/MachineLearning/comments/bbfx66/d_have_applied_to_a_lot_of_ml_startups_on_angelco/,karttikey_ab,1554858190,"I have applied to a lot of startups located in Bangalore ( India) on Angel.co but I haven't got any replies from any. 
Tried my luck with Glassdoor as well . No success.

My profile is decent. I need your help to secure a role in a startup/company for Summer 2019.",6,0,False,self,,,,,
661,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,11,bbh151,self.MachineLearning,Where can I get LSUN cat and car dataset which is used in StyleGAN?,https://www.reddit.com/r/MachineLearning/comments/bbh151/where_can_i_get_lsun_cat_and_car_dataset_which_is/,kooro11,1554865034,"[https://arxiv.org/pdf/1812.04948.pdf](https://arxiv.org/pdf/1812.04948.pdf)  


In StyleGAN, the authors trained the generator using the LSUN car and cat dataset. However, I can not find any car and cat category in the official LSUN website.  Is there anyone who found these datasets?",0,1,False,self,,,,,
662,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,12,bbh5et,self.MachineLearning,[D] Truncation in LSTM BPTT,https://www.reddit.com/r/MachineLearning/comments/bbh5et/d_truncation_in_lstm_bptt/,CartPole,1554865804,"""Truncation ensures that there are no loops across which an error that left some memory cell through its input or input gate can reenter the cell through its output or output gate. This in turn ensures constant error flow through the memory cell's CEC"" - [LSTM Appendix A.1](https://www.bioinf.jku.at/publications/older/2604.pdf)

Why are loops considered bad? Is it because without truncation you can't ensure that

https://i.redd.it/2946ywijscr21.png

and without the above equaling 1 it could lead to vanishing or exploding gradients?",1,5,False,self,,,,,
663,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,12,bbhkhp,self.MachineLearning,Multi-Server GPU Monitoring Tool!,https://www.reddit.com/r/MachineLearning/comments/bbhkhp/multiserver_gpu_monitoring_tool/,kairos9603,1554868684,"Hi, I'm introducing multi-server gpu monitoring tool.

it it simple to install and setting. Please, use and some comment to github.

Thank you. 

Have a nice day!

&amp;#x200B;

![img](k0qaastc1dr21)",0,1,False,self,,,,,
664,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,13,bbhs57,arxiv.org,[R] Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bbhs57/r_playing_textadventure_games_with_graphbased/,inarrears,1554870188,,1,9,False,default,,,,,
665,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,13,bbhusb,arxiv.org,[R] From Variational to Deterministic Autoencoders,https://www.reddit.com/r/MachineLearning/comments/bbhusb/r_from_variational_to_deterministic_autoencoders/,chisai_mikan,1554870733,,17,69,False,default,,,,,
666,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,14,bbi2xh,oodlestechnologies.com,How Artificial Intelligence Is Transforming the Travel Industry,https://www.reddit.com/r/MachineLearning/comments/bbi2xh/how_artificial_intelligence_is_transforming_the/,tech-info,1554872448,,0,1,False,default,,,,,
667,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,14,bbib5z,self.MachineLearning,ML Internship Presentation,https://www.reddit.com/r/MachineLearning/comments/bbib5z/ml_internship_presentation/,BlackSky2129,1554874166,[removed],0,1,False,self,,,,,
668,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,14,bbibj5,github.com,[R] Faster Training of Mask R-CNN by focusing on instance boundaries,https://www.reddit.com/r/MachineLearning/comments/bbibj5/r_faster_training_of_mask_rcnn_by_focusing_on/,Jul8234,1554874242,,0,1,False,default,,,,,
669,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,15,bbiywy,self.MachineLearning,plotting ROC w/ multivariable thresholds,https://www.reddit.com/r/MachineLearning/comments/bbiywy/plotting_roc_w_multivariable_thresholds/,atloo1,1554879497,[removed],0,1,False,self,,,,,
670,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,16,bbjahu,self.MachineLearning,[P] What are some sentiment analysis tools other than TextBlob that are easy to use?,https://www.reddit.com/r/MachineLearning/comments/bbjahu/p_what_are_some_sentiment_analysis_tools_other/,Seankala,1554882244,"Hello everyone. I've been making some posts regarding a project that I'm working on. The project is undergrad-level and is about using Twitter sentiment and gauging its correlation between Bitcoin prices in the US market and Korean market.

The tool that I used to get sentiment was TextBlob. Overall it's great and easy to use. However, I'm facing some fundamental problems. If I were to elaborate, the data that I'm working with looks like this:

&amp;#x200B;

https://i.redd.it/osg2v3tk4er21.png

\`Close\` is the closing price for that day. The other columns (other than \`Volume\`) have to do with Tweet data. \`polarity\` and \`subjectivity\` are the sentiment values that I extracted from using TextBlob on each single Tweet. There were many Tweets per day, but I added them all up and scaled them to be within \[-1, 1\].

After I did this operation and obtained a correlation matrix, I printed it out in a heat map format:

&amp;#x200B;

[Top: US \/ Bottom: Korea](https://i.redd.it/1n4nipw15er21.png)

If you look at the last line for \`Close\` you can see that there is almost no correlation between \`polarity\` and \`subjectivity\` which is also strange for the US market. Something tells me that this means that there's been a fundamental flaw in the way that I obtained sentiment as my assumption would be that the US market should show stronger correlation because the Twitter data was in English.

That being said, does anybody know of any other sentiment analysis tools that I may be able to use? It would be preferable if it were easy to use like TextBlob.

Any feedback is appreciated, thank you!

&amp;#x200B;

P.S. Some other ways that I've thought of doing this is to divide \`polarity\` (the original concept is that it's within \[-1, 1\] and the higher you go the more positive and vice versa) into separate ranges and put sentiment labels for each of those ranges. Or I also was thinking that I could put a time lag on sentiment so that I may be able to capture the correlation a bit better. These are just ideas though.",8,1,False,https://b.thumbs.redditmedia.com/Nn9Y3XAPUh-UR_9tR0kdJOnbBiFiFQUyVzwuchCr6Pg.jpg,,,,,
671,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,16,bbjav8,smarten.com,Clickless Analytics with Natural Language Processing (NLP)!,https://www.reddit.com/r/MachineLearning/comments/bbjav8/clickless_analytics_with_natural_language/,ElegantMicroWebIndia,1554882334,,0,1,False,default,,,,,
672,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,16,bbjbfp,blog.adeel.io,Differentiable Neural Computer Memory Testing Using dSprites,https://www.reddit.com/r/MachineLearning/comments/bbjbfp/differentiable_neural_computer_memory_testing/,ThisIsMySeudonym,1554882468,,0,1,False,default,,,,,
673,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,16,bbjdji,medium.com,Creating a Custom OpenAI Gym Environment for Stock Trading,https://www.reddit.com/r/MachineLearning/comments/bbjdji/creating_a_custom_openai_gym_environment_for/,notadamking,1554883015,,0,1,False,default,,,,,
674,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,17,bbjeug,self.MachineLearning,How To Mimic Evolution For ML Tasks,https://www.reddit.com/r/MachineLearning/comments/bbjeug/how_to_mimic_evolution_for_ml_tasks/,conversational-ai,1554883365,"Machine learning algorithms are often inspired by natural processes. Find out this is possible through mutations, mating and cross-over processes to learn a function. 

[https://cai.tools.sap/blog/how-to-mimic-evolution-for-machine-learning-tasks/](https://cai.tools.sap/blog/how-to-mimic-evolution-for-machine-learning-tasks/)",0,1,False,self,,,,,
675,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,17,bbjjr7,twitter.com,Machine Learning and Blockchain To Predict Consumer Behavior,https://www.reddit.com/r/MachineLearning/comments/bbjjr7/machine_learning_and_blockchain_to_predict/,tech-info,1554884596,,0,1,False,https://b.thumbs.redditmedia.com/rxLjaHiRi3IslxJUUv0o5wCbQ75VCC1nqElNhO1Yx9o.jpg,,,,,
676,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,17,bbjpgt,thecoderr.com,Data Analysis and a bit on Democracy pt. 2,https://www.reddit.com/r/MachineLearning/comments/bbjpgt/data_analysis_and_a_bit_on_democracy_pt_2/,the_coder_dot_py,1554886092,,0,1,False,https://a.thumbs.redditmedia.com/dSlMnaMQ9N4TZtzjMcJGZVXYCh_uHVgXGiPHeYG_QF8.jpg,,,,,
677,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,17,bbjquv,self.MachineLearning,How to realize below ideas by AI,https://www.reddit.com/r/MachineLearning/comments/bbjquv/how_to_realize_below_ideas_by_ai/,gloomyson,1554886460,[removed],1,1,False,self,,,,,
678,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,17,bbjs3j,self.MachineLearning,Any good courses that goes with Bishop's book?,https://www.reddit.com/r/MachineLearning/comments/bbjs3j/any_good_courses_that_goes_with_bishops_book/,seann999,1554886791,[removed],0,1,False,self,,,,,
679,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,18,bbjsjt,self.MachineLearning,[D] Using k8s as a task runner to train and evaluate models,https://www.reddit.com/r/MachineLearning/comments/bbjsjt/d_using_k8s_as_a_task_runner_to_train_and/,MasterScrat,1554886888,"What are people using to manage training and evaluations on k8s clusters?

Until now I started my training processes by hands, maybe with some hyperparameter search using a process pool.

But as I develop more complex models I'm looking for a solution that scales better. Kubernetes looks like a perfect fit as I'd be able to define how many cores and GPUs I assign to each model.

**However** I can't find a straight-forward way to use k8s as a task runner. I'm pretty familiar with it to run reliable, long-running tasks such as serving models. But what I want now is a way to start and monitor many tasks, possibly involving multiple steps each (preproc, training, validation). The ability to prioritize and pause these tasks would be a nice-to-have.

One solution would be to simply create [Jobs](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/) and to make my own dashboard and persistence logic. Or I could use a generic Job dashboard like this: https://github.com/pietervogelaar/kubernetes-job-monitor

Something else that looks promising is Kubeflow, but it looks like a lot of extra complexity.

I'm really curious to hear how other people handle this?",9,10,False,self,,,,,
680,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,18,bbjyli,self.MachineLearning,Survey for artificially generated media.,https://www.reddit.com/r/MachineLearning/comments/bbjyli/survey_for_artificially_generated_media/,jiddzey,1554888378,[removed],0,1,False,self,,,,,
681,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,18,bbk0hp,self.MachineLearning,Is the jobs about machine learning adapted for a Junior ?,https://www.reddit.com/r/MachineLearning/comments/bbk0hp/is_the_jobs_about_machine_learning_adapted_for_a/,Lolimarth,1554888815,[removed],1,1,False,self,,,,,
682,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,19,bbk96z,youtube.com,Cat vs Dog Classification using Edge Detection and Other Techniques,https://www.reddit.com/r/MachineLearning/comments/bbk96z/cat_vs_dog_classification_using_edge_detection/,the_coder_dot_py,1554890848,,0,1,False,https://b.thumbs.redditmedia.com/6STSi94y7dA5qDJ2XCr9LVHv524iWjBf01OvPWJiGhI.jpg,,,,,
683,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,19,bbk9jh,youtube.com,"#PRODUCTTALKS: AI ETHICS, BIAS, AND JOB DISPLACEMENT",https://www.reddit.com/r/MachineLearning/comments/bbk9jh/producttalks_ai_ethics_bias_and_job_displacement/,Cameron_Kamer,1554890932,,0,1,False,https://b.thumbs.redditmedia.com/VxpeSA21pzQ9aAo3q2IA8bUzGFSeBsDYF-F8CCTjqIk.jpg,,,,,
684,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,19,bbkl85,thecoderr.com,Bitcoin Predictor,https://www.reddit.com/r/MachineLearning/comments/bbkl85/bitcoin_predictor/,the_coder_dot_py,1554893572,,0,1,False,https://b.thumbs.redditmedia.com/mukLwhq_lxwUr6la4YMUpEOTj8U6awyZ4UDEC3f3xBI.jpg,,,,,
685,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,20,bbkqe4,self.MachineLearning,Machine Learning with C++ - Polynomial Regression (CPU),https://www.reddit.com/r/MachineLearning/comments/bbkqe4/machine_learning_with_c_polynomial_regression_cpu/,andrea_manero,1554894628,[removed],0,1,False,self,,,,,
686,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,20,bbkrzv,cogitotech.com,Hire Machine Learning Engineer,https://www.reddit.com/r/MachineLearning/comments/bbkrzv/hire_machine_learning_engineer/,trainingdata,1554894944,,0,1,False,default,,,,,
687,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,20,bbkvty,self.MachineLearning,Model that can make a prediction (classification) based on a sub set of features,https://www.reddit.com/r/MachineLearning/comments/bbkvty/model_that_can_make_a_prediction_classification/,GaryML,1554895707,[removed],0,1,False,self,,,,,
688,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,20,bbl549,medium.com,"Learn to Collect, model, and deploy data-driven systems using Python and machine learning",https://www.reddit.com/r/MachineLearning/comments/bbl549/learn_to_collect_model_and_deploy_datadriven/,poonddetatte,1554897511,,0,1,False,default,,,,,
689,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,21,bbl6z5,socialprachar.com,machine learning,https://www.reddit.com/r/MachineLearning/comments/bbl6z5/machine_learning/,anushapr,1554897847,,0,1,False,default,,,,,
690,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,21,bbl82t,self.MachineLearning,[N] Augmented Reality and Machine Learning Cooperation on Mobile (talk),https://www.reddit.com/r/MachineLearning/comments/bbl82t/n_augmented_reality_and_machine_learning/,mto96,1554898039,"This is a 35 minute talk by iOS tech lead at Groupon, Mourad Sidky from GOTO Copenhagen 2018.

[https://youtu.be/97CPDQBUAoY?list=PLEx5khR4g7PIzxn476GK3Mkk19csZZjeH](https://youtu.be/97CPDQBUAoY?list=PLEx5khR4g7PIzxn476GK3Mkk19csZZjeH)

Please give the talk abstract a read below before giving it a watch:

&amp;#x200B;

Mobile devices are getting more and more powerful, with not-only advanced hardware, but also intelligent operating systems and high-performance compatible set of native frameworks. Mobile devices are capable of doing expensive on-device processing to achieve augmented reality and machine learning, without the need to communicate to any other external services. Apple exposed them in ARKit and CoreML. Both frameworks are implemented in a way to achieve correctness, best performance, energy efficiency and data privacy. Apple solves lots of problems and gave us the solutions in these frameworks. Ill be speaking about augmented reality, step into augmented reality for iOS, then Ill speak about machine learning, step into machine learning for iOS and then Ill finalise with the power of cooperation between augmented reality and machine learning together.",2,24,False,self,,,,,
691,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,21,bbl9dn,self.MachineLearning,Will emotional Deep Blue be able to beat human chess champion as easily?,https://www.reddit.com/r/MachineLearning/comments/bbl9dn/will_emotional_deep_blue_be_able_to_beat_human/,amsmu,1554898269,[removed],0,1,False,self,,,,,
692,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,21,bbl9zd,habr.com,Liza Alert: searching missing people using machine vision,https://www.reddit.com/r/MachineLearning/comments/bbl9zd/liza_alert_searching_missing_people_using_machine/,atomlib_com,1554898383,,0,1,False,https://a.thumbs.redditmedia.com/ffei57zoR-hCaztAbS9Y6sr9YyhV7ZPUDfvfJDLfb-4.jpg,,,,,
693,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,21,bblahb,self.MachineLearning,What would be a reasonable conference budget?,https://www.reddit.com/r/MachineLearning/comments/bblahb/what_would_be_a_reasonable_conference_budget/,plethorial,1554898470,[removed],0,1,False,self,,,,,
694,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,21,bblb6j,self.MachineLearning,How to deal with an imbalanced dataset for MULTI-LABEL classification?,https://www.reddit.com/r/MachineLearning/comments/bblb6j/how_to_deal_with_an_imbalanced_dataset_for/,PolyrogueKappa,1554898595,"Hello there. You can consider me novice to intermediate at best with Machine Learning.

For the past few months, I've been developing a neural network that learns to play a 3D fighting game by trying to mimic how I play, in Keras, using a Tensorflow backend. The input data consists of both a low-resolution, greyscaled version of the frame at the time, along with some corresponding categorical information. I extract important information from the game using computer vision and represent that using a multi-hot array. E.g. [0, 1, 0, 0, 0, 0, 1, 0, 0, 0] where each index represents some information about that moment, like if the enemy is attacking. This all might be useless information, but I'm providing it just in case.

The labels are the crucial matter. I am providing the network with an array of the buttons that I am pressing at that moment in time.  The key thing to note is that this array is multi-hot, as I am often pressing multiple buttons at once.

Additional information - The network architecture consists of two branches - one LSTM layer for the categorical data, and one convolutional layer for the image data. These are then concatenated and fed through two Dense layers before finally going through a sigmoid activation. I am using binary-crossentropy as a loss function. This is really the only combination of activation and loss that I am aware of for multi-label classification. In terms of metrics, both accuracy and binary accuracy appear to result in over 90% for validation and training each time, right from epoch 1.

[Here is a diagram of the model](https://i.stack.imgur.com/R7Fsc.png)

The unfortunate problem I have had from the start was that my network just doesn't seem to work well. It claims to achieve over 90% accuracy and validation accuracy, and yet only exhibits some form of intelligence when tested. I think this is down to the inherent imbalance of my training labels, as some buttons are just far, far more likely to be pressed than others. But I'm really not sure how to deal with balancing them, as traditional methods like over and undersampling or using class weightings don't work with *multi-label* classification. I'm basically stuck, and googling isn't really helping.

Any help or advice would be greatly appreciated. If you need more information, please don't hesitate to ask. Thank you!",0,1,False,self,,,,,
695,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,22,bblqbs,oodlestechnologies.com,How Artificial Intelligence is Connecting Customers To Your Brand,https://www.reddit.com/r/MachineLearning/comments/bblqbs/how_artificial_intelligence_is_connecting/,tech-info,1554901263,,0,1,False,https://a.thumbs.redditmedia.com/CGQYKBUVUTHFJ7QOAjvyIsl6ONyX-h6LYypoHT_UJW4.jpg,,,,,
696,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,22,bbls8c,youtube.com,Multiple Linear Regression using MLPACK C,https://www.reddit.com/r/MachineLearning/comments/bbls8c/multiple_linear_regression_using_mlpack_c/,itsbrightprogrammer,1554901580,,0,1,False,default,,,,,
697,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,22,bblvkp,arxiv.org,[Research] FoveaBox: Beyond Anchor-based Object Detector,https://www.reddit.com/r/MachineLearning/comments/bblvkp/research_foveabox_beyond_anchorbased_object/,taokongcn,1554902108,,3,10,False,default,,,,,
698,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,22,bbm0a9,self.MachineLearning,How Artificial Intelligence (AI) will shape our future?,https://www.reddit.com/r/MachineLearning/comments/bbm0a9/how_artificial_intelligence_ai_will_shape_our/,Where_is_Gabriel,1554902836,[removed],0,1,True,nsfw,,,,,
699,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,22,bbm5n0,medium.com,[P] Measuring Trust In Real-Time To Design Robots That Can Sense Humans Levels Of Trust In Them And Adjust Their Behaviors Accordingly.,https://www.reddit.com/r/MachineLearning/comments/bbm5n0/p_measuring_trust_in_realtime_to_design_robots/,anon2041,1554903685,,0,1,False,default,,,,,
700,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,22,bbm6hz,self.MachineLearning,How will Artificial Intelligence (AI) change the world?,https://www.reddit.com/r/MachineLearning/comments/bbm6hz/how_will_artificial_intelligence_ai_change_the/,Where_is_Gabriel,1554903818,[removed],0,1,False,self,,,,,
701,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmhj0,self.MachineLearning,Making revenue prediction for next months?,https://www.reddit.com/r/MachineLearning/comments/bbmhj0/making_revenue_prediction_for_next_months/,Blitzoff,1554905498,[removed],0,1,False,self,,,,,
702,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmits,deepmind.com, Unsupervised learning: the curious pupil,https://www.reddit.com/r/MachineLearning/comments/bbmits/unsupervised_learning_the_curious_pupil/,sjoerdapp,1554905698,,0,1,False,default,,,,,
703,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmlda,self.MachineLearning,Processing of text stream to apply machine learning algorithms,https://www.reddit.com/r/MachineLearning/comments/bbmlda/processing_of_text_stream_to_apply_machine/,guy_maroon,1554906072,[removed],0,1,False,self,,,,,
704,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmmq4,self.MachineLearning,How will the Artificial Intelligence (AI) change the world?,https://www.reddit.com/r/MachineLearning/comments/bbmmq4/how_will_the_artificial_intelligence_ai_change/,Where_is_Gabriel,1554906265,[removed],0,1,False,self,,,,,
705,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmp51,futurice.com,"Bach to the Future (or, Humanising Music With Neural Nets)",https://www.reddit.com/r/MachineLearning/comments/bbmp51/bach_to_the_future_or_humanising_music_with/,point_against_point,1554906658,,0,1,False,default,,,,,
706,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmqnq,self.MachineLearning,Tell me your vision about artificial intelligence(AI),https://www.reddit.com/r/MachineLearning/comments/bbmqnq/tell_me_your_vision_about_artificial/,Where_is_Gabriel,1554906884,[removed],0,1,False,self,,,,,
707,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmto0,self.MachineLearning,[Project] How to deal with an imbalanced dataset for MULTI-LABEL classification?,https://www.reddit.com/r/MachineLearning/comments/bbmto0/project_how_to_deal_with_an_imbalanced_dataset/,PolyrogueKappa,1554907349,"Hello there. You can consider me novice to intermediate at best with Machine Learning.

For the past few months, I've been developing a neural network that learns to play a 3D fighting game by trying to mimic how I play, in Keras, using a Tensorflow backend. The input data consists of both a low-resolution, greyscaled version of the frame at the time, along with some corresponding categorical information. I extract important information from the game using computer vision and represent that using a multi-hot array. E.g. [0, 1, 0, 0, 0, 0, 1, 0, 0, 0] where each index represents some information about that moment, like if the enemy is attacking. This all might be useless information, but I'm providing it just in case.

The labels are the crucial matter. I am providing the network with an array of the buttons that I am pressing at that moment in time.  The key thing to note is that this array is multi-hot, as I am often pressing multiple buttons at once.

Additional information - The network architecture consists of two branches - one LSTM layer for the categorical data, and one convolutional layer for the image data. These are then concatenated and fed through two Dense layers before finally going through a sigmoid activation. I am using binary-crossentropy as a loss function. This is really the only combination of activation and loss that I am aware of for multi-label classification. In terms of metrics, both accuracy and binary accuracy appear to result in over 90% for validation and training each time, right from epoch 1.

[Here is a diagram of the model](https://i.stack.imgur.com/R7Fsc.png)

The unfortunate problem I have had from the start was that my network just doesn't seem to work well. It claims to achieve over 90% accuracy and validation accuracy, and yet only exhibits some form of intelligence when tested. I think this is down to the inherent imbalance of my training labels, as some buttons are just far, far more likely to be pressed than others. But I'm really not sure how to deal with balancing them, as traditional methods like over and undersampling or using class weightings don't work with *multi-label* classification. I'm basically stuck, and googling isn't really helping.

Any help or advice would be greatly appreciated. If you need more information, please don't hesitate to ask. Thank you!",46,21,False,self,,,,,
708,MachineLearning,t5_2r3gv,2019-4-10,2019,4,10,23,bbmu2p,self.MachineLearning,"Bach to the Future (or, Humanising Music With Neural Nets)",https://www.reddit.com/r/MachineLearning/comments/bbmu2p/bach_to_the_future_or_humanising_music_with/,point_against_point,1554907413,[removed],0,1,False,self,,,,,
709,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,0,bbn11g,self.MachineLearning,[Q] Time Series Dataset Formatting - Python/Tensorflow,https://www.reddit.com/r/MachineLearning/comments/bbn11g/q_time_series_dataset_formatting_pythontensorflow/,cuckoostep,1554908490,[removed],0,1,False,self,,,,,
710,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,0,bbn4kf,self.MachineLearning,Machine learning 101: k-NN algorithm,https://www.reddit.com/r/MachineLearning/comments/bbn4kf/machine_learning_101_knn_algorithm/,IranNeto,1554908993,[removed],0,1,False,self,,,,,
711,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,0,bbn6ra,medium.com,Improving Robot Control With Residual RL,https://www.reddit.com/r/MachineLearning/comments/bbn6ra/improving_robot_control_with_residual_rl/,Yuqing7,1554909320,,0,1,False,default,,,,,
712,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,0,bbnc34,self.MachineLearning,Machine learning in fintech,https://www.reddit.com/r/MachineLearning/comments/bbnc34/machine_learning_in_fintech/,imascientist42,1554910079,[removed],0,1,False,self,,,,,
713,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,0,bbnkz0,twitch.tv,"Live from Harvard on Twitch, Wed 4/10 at 3pm ET, CS50's Nick Wong introduces us to the world of neural networks using Tensorflow and Python, featuring k-means clustering and image generation based on prior stream screenshots.",https://www.reddit.com/r/MachineLearning/comments/bbnkz0/live_from_harvard_on_twitch_wed_410_at_3pm_et/,coltonoscopy,1554911365,,0,1,False,https://b.thumbs.redditmedia.com/w8O38L7xftzQARzn_AQJU65wQwvFS7ADGS5HFG0eObY.jpg,,,,,
714,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,0,bbnmfa,self.MachineLearning,"Simple Questions Thread April 10, 2019",https://www.reddit.com/r/MachineLearning/comments/bbnmfa/simple_questions_thread_april_10_2019/,AutoModerator,1554911571,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1,False,self,,,,,
715,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,1,bbnykq,self.MachineLearning,Training Job Infastructure,https://www.reddit.com/r/MachineLearning/comments/bbnykq/training_job_infastructure/,_michaelx99,1554913272,[removed],0,1,False,self,,,,,
716,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,1,bbo28s,self.MachineLearning,[R] Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text,https://www.reddit.com/r/MachineLearning/comments/bbo28s/r_playing_by_the_book_an_interactive_game/,rtk25,1554913798,"Hey, would be happy to hear thoughts on our NAACL workshop paper ([ESSP](https://scientific-knowledge.github.io/)).

In the course of working on it, I've been thinking a lot about ""simulator"" style environments for training NLP models. While there are many simulators for robot navigation and autonomous driving, there are hardly any for NLP.  Seems to me there could be a lot of interesting potential there (would help in framing NLP problems as program induction, etc). 

&amp;#x200B;

[arXiv landing page](https://arxiv.org/abs/1811.04319)

Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from material science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.",3,2,False,self,,,,,
717,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,1,bbo62n,self.MachineLearning,How do we get contour around dataset when we use rbf kernel in svm?,https://www.reddit.com/r/MachineLearning/comments/bbo62n/how_do_we_get_contour_around_dataset_when_we_use/,peter_nguyenanh,1554914333,"How do we get contour around dataset when we use rbf kernel in svm? For example, we use kernel trick to map 2D data into 3D and we can use linear hyperplane to split data, but when we get back to 2D original data , we will get contour around data. Follow the link:

[https://scikit-learn.org/stable/auto\_examples/svm/plot\_svm\_kernels.html](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html)",0,1,False,self,,,,,
718,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,1,bbob4h,self.MachineLearning,Needed help with Neural Ordinary Differential Equations.,https://www.reddit.com/r/MachineLearning/comments/bbob4h/needed_help_with_neural_ordinary_differential/,rish-16,1554915032,[removed],0,1,False,self,,,,,
719,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,2,bbom0c,medium.com,"Google Cloud Next | New Hybrid Cloud Anthos, Partnerships, Data Centres",https://www.reddit.com/r/MachineLearning/comments/bbom0c/google_cloud_next_new_hybrid_cloud_anthos/,Yuqing7,1554916516,,0,1,False,default,,,,,
720,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,2,bboue1,self.MachineLearning,ML model to Predict House Prices,https://www.reddit.com/r/MachineLearning/comments/bboue1/ml_model_to_predict_house_prices/,as8801598,1554917652,[removed],0,1,False,self,,,,,
721,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,2,bbowb5,self.MachineLearning,"[N] Conference on Soft Computing MENDEL, July 10-12, Brno, Czech Republic",https://www.reddit.com/r/MachineLearning/comments/bbowb5/n_conference_on_soft_computing_mendel_july_1012/,dictrix,1554917908,"A small-scale conference on Soft Computing, Evolutionary Computation, Computational Intelligence, Artificial Intelligence, Neural Networks, Deep Learning, Bayesian Methods, Fuzzy Logic, Intelligent Image Processing, Bio-Inspired Robotics...

Accepted contributions published in a journal (MENDEL) or in conference proceedings (AISC Springer - Recent Advances in Soft Computing).

(I'm one of the editors, feel free to PM me if interested).",0,2,False,self,,,,,
722,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,3,bbp8dx,medium.com,Alibaba AI Labs designed Erha to solve Robocalls,https://www.reddit.com/r/MachineLearning/comments/bbp8dx/alibaba_ai_labs_designed_erha_to_solve_robocalls/,gwen0927,1554919541,,0,1,False,https://b.thumbs.redditmedia.com/VcG1-b7j9S2L-HwUPNzDVJnf4mQoVORwLuVYKPcN7gs.jpg,,,,,
723,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,3,bbp8u9,self.MachineLearning,Anyone ever seen someone use NLP to determine who the peer reviewers are in your reviewer comments?,https://www.reddit.com/r/MachineLearning/comments/bbp8u9/anyone_ever_seen_someone_use_nlp_to_determine_who/,labbypatty,1554919600,[removed],0,1,False,self,,,,,
724,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,3,bbp922,youtube.com,Who is behind this speech synthesis/imitation AI? It seems like one of the best out there,https://www.reddit.com/r/MachineLearning/comments/bbp922/who_is_behind_this_speech_synthesisimitation_ai/,MaxinMusic,1554919627,,0,1,False,default,,,,,
725,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,3,bbpe1r,self.MachineLearning,What would be an advice to someone who is passionate about starting a research career in Machine Learning and Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/bbpe1r/what_would_be_an_advice_to_someone_who_is/,lostandstrong,1554920299,[removed],0,1,False,self,,,,,
726,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,3,bbpkrr,vinsloev.blogspot.com,Artificial Intelligence - How it could lead to a greener and healthier world,https://www.reddit.com/r/MachineLearning/comments/bbpkrr/artificial_intelligence_how_it_could_lead_to_a/,EddyTheDad,1554921232,,0,1,False,https://b.thumbs.redditmedia.com/aZWJcoMWbQAC6EgSOIY3142EdYEZpF8MGm-sxYFwFVY.jpg,,,,,
727,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,3,bbpqhi,self.MachineLearning,"Looking on text books, review papers and tutorials on generalization theory",https://www.reddit.com/r/MachineLearning/comments/bbpqhi/looking_on_text_books_review_papers_and_tutorials/,AlexSnakeKing,1554922019,"Hi all 

I'm looking for a good textbook, or a review paper or tutorial on advances in generalization theory and learnability in the last 20 years, i.e what has happened since Vapnik's structural risk minimization and Schapire's boosting?",0,1,False,self,,,,,
728,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,3,bbpuh8,self.MachineLearning,[N] Google launches an end-to-end AI platform,https://www.reddit.com/r/MachineLearning/comments/bbpuh8/n_google_launches_an_endtoend_ai_platform/,szopa,1554922582,"https://cloud.google.com/ai-platform/

And a TC article, which did a decent job explaining what this is about.

https://techcrunch.com/2019/04/10/google-expands-its-ai-services/

Google launches an end-to-end AI platform. It promises the ability to handle the whole lifecycle of an ML project, from prototyping to production serving. There's also a new labeling service.

What do you guys think?",34,239,False,self,,,,,
729,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,4,bbq4qq,self.MachineLearning,Machines can't learn,https://www.reddit.com/r/MachineLearning/comments/bbq4qq/machines_cant_learn/,EthanRake11,1554923985,[removed],0,1,False,self,,,,,
730,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,4,bbq759,news.developer.nvidia.com,Using AI to Solve Collaborative Challenges by Playing StarCraft,https://www.reddit.com/r/MachineLearning/comments/bbq759/using_ai_to_solve_collaborative_challenges_by/,adammathias,1554924320,,0,1,False,default,,,,,
731,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,4,bbqei9,stoodnt.com,Mismatch between Data Science &amp; ML Job Market and Online Courses | What Should Aspiring Data Scientists Do?,https://www.reddit.com/r/MachineLearning/comments/bbqei9/mismatch_between_data_science_ml_job_market_and/,tanmoyray01,1554925375,,0,1,False,default,,,,,
732,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,4,bbqeu2,t.me,DataScience Digest (Telegram channel),https://www.reddit.com/r/MachineLearning/comments/bbqeu2/datascience_digest_telegram_channel/,flyelephant,1554925423,,0,1,False,https://a.thumbs.redditmedia.com/8cpCwy8hCXPpGw9k12Oa1ooliCDrl-b8QZMghlp3Qs4.jpg,,,,,
733,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,4,bbqk4w,self.MachineLearning,[D] Hyperparam optimisation using RandomSearch with argparse scripts?,https://www.reddit.com/r/MachineLearning/comments/bbqk4w/d_hyperparam_optimisation_using_randomsearch_with/,trias10,1554926166,"I tend to write complex model/training scripts in pure python using argparse to pass huge amounts of hyperparameters to the model, and then run these python scripts on multi-gpu EC2s.

&amp;#x200B;

I was wondering if anyone knows of any tools out there what would allow me to do hyperparam optimisation by passing different sets of hyperparams to these scripts via the argparse/commandline system? So imagine you have a process which generates hyperparam sets, then kicks off a subprocess which is the python training script with hyperparams passed in via commandline/argparse, then gets the metrics back, stores them, then kicks off the next set, etc, etc.

&amp;#x200B;

For basic grid search, you could easily accomplish this via a unix shell script, but for random search it's trickier. One possible solution would be to write a python script which uses sklearn's ParameterSampler and the subprocess module to accomplish all this, but I was curious if there is an already made solution out there which I could use? Would hate to reinvent the wheel if this particular wheel already exists out there somewhere.

&amp;#x200B;

Would greatly appreciate any help/tips.",5,3,False,self,,,,,
734,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,5,bbqql4,self.MachineLearning,A bit about a project I've been working on.,https://www.reddit.com/r/MachineLearning/comments/bbqql4/a_bit_about_a_project_ive_been_working_on/,the_coder_dot_py,1554927054,[removed],0,1,False,self,,,,,
735,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,5,bbr717,self.MachineLearning,Switching from IT Consultant to Data Scientist,https://www.reddit.com/r/MachineLearning/comments/bbr717/switching_from_it_consultant_to_data_scientist/,ichk25,1554929447,"Warm greetings to the community!

&amp;#x200B;

I want to switch my career from an IT consultant to Data Scientist. I have been applying for data scientist job openings but have not received any response yet, probably because they see no relevant experience in my resume. I do have read books on data science (MIT Fundamentals of Machine Learning and Hands On Machine Learning with Scikit Learn) and also have fair experience(self-learnt) on Python. Also, I have participated in few competitions but somehow that does not attract recruiters. 

&amp;#x200B;

I am strongly considering to take the IBM Data Science Professional Certificate so that it adds some value to my resume.

What else should I do or add to my resume so that they I get interview calls at least?",0,1,False,self,,,,,
736,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,7,bbskha,self.MachineLearning,[Question] What cloud service do you recommend to train Deep Learning models (for a toy project)?,https://www.reddit.com/r/MachineLearning/comments/bbskha/question_what_cloud_service_do_you_recommend_to/,Fredbull,1554936486,"Hello everyone,

&amp;#x200B;

so I've been working for about 1 year in a generic data science/machine learning-related role, and I am now starting my first toy project, having (hopefully) accrued some general knowledge of machine learning techniques.

&amp;#x200B;

I have obtained a large dataset of poems and would like to train a sequence-to-sequence model to generate poetry and build an API around it (with a graphical interface). I know it's not the most original idea, but I think it could be a good educational first project (web scraping, deep learning, APIs...).

&amp;#x200B;

Anyway, here are the questions I was hoping you could help me with:

1. I would like to train the model using some cloud service, partly in order to make it faster and so I can test multiple models, and partly because I want to learn how to use it. Which would you personally suggest, given my goals? So far I think Google's Collaboratory would be the best for me, but I am willing to spend a bit of money for something better.
2. Is it feasible to train a large model on GPU, and then use it locally on my machine to generate new poems? Or would the inference time be completely nutty? (assuming I'm using let's say BERT embeddings and a not-very-deep RNN).

&amp;#x200B;

These are my main questions, but I would also love to hear your opinions on:

1. Have you had any experience with text generation? If so, what models did you have most success on?
2. Would you add any extra features (like POS tags or rhyming schemes or something) to generate better poems, in this case?
3. Anything else you'd like to add :)

&amp;#x200B;

This turned out to be quite the wall of text. Thanks for the attention!",0,1,False,self,,,,,
737,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,8,bbsznb,bloomberg.com,Is Anyone Listening to You on Alexa? A Global Team Reviews Audio,https://www.reddit.com/r/MachineLearning/comments/bbsznb/is_anyone_listening_to_you_on_alexa_a_global_team/,chogall,1554938822,,0,1,False,default,,,,,
738,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,9,bbtw6s,self.MachineLearning,Machine translation for low resource language pairs,https://www.reddit.com/r/MachineLearning/comments/bbtw6s/machine_translation_for_low_resource_language/,AnushKumar_,1554944125,[removed],0,1,False,self,,,,,
739,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,10,bbu577,self.MachineLearning,Machine learning consulting/freelance?,https://www.reddit.com/r/MachineLearning/comments/bbu577/machine_learning_consultingfreelance/,Anagonye,1554945650,[removed],0,1,False,self,,,,,
740,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,11,bbuzp9,arxiv.org,[1904.04971] Soft Conditional Computation,https://www.reddit.com/r/MachineLearning/comments/bbuzp9/190404971_soft_conditional_computation/,HigherTopoi,1554950899,,14,3,False,default,,,,,
741,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,12,bbvdut,self.MachineLearning,[N] Springer publishes journal using ML to generate summaries of research papers ?,https://www.reddit.com/r/MachineLearning/comments/bbvdut/n_springer_publishes_journal_using_ml_to_generate/,pinkflamingo16,1554953523,"Springer published a (gasp: free) journal about Li-Batteries which uses ML to generate summaries (with citations) of multiple peer reviewed papers in the subject. 

https://link.springer.com/book/10.1007/978-3-030-16800-1 

I'm not an Li-battery expert, but the sentences do seem grammatically correct (and sufficiently winding to be scientific publications )

What did you guys think of it ?",15,26,False,self,,,,,
742,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,13,bbvuu4,github.com,[P] NumPy and Python assignment from Neural Networks course at BITS Pilani,https://www.reddit.com/r/MachineLearning/comments/bbvuu4/p_numpy_and_python_assignment_from_neural/,agrawalamey,1554956845,,1,4,False,default,,,,,
743,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,13,bbw2l9,reddit.com,DLBT | User-friendly deep learning benchmark app | Anyone can test and benchmark their hardware GPU-CPU,https://www.reddit.com/r/MachineLearning/comments/bbw2l9/dlbt_userfriendly_deep_learning_benchmark_app/,gimel1213,1554958458,,0,2,False,default,,,,,
744,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,14,bbw51k,self.MachineLearning,"[D] Saddle-free Newton method for SGD and other actively repelling saddles - advantages, weaknesses, improvements?",https://www.reddit.com/r/MachineLearning/comments/bbw51k/d_saddlefree_newton_method_for_sgd_and_other/,jarekduda,1554960418,"While 2nd order methods have many advantages, e.g. natural gradient (e.g. in L-BFGS) attracts to close zero gradient point, which is usually saddle. Other try to pretend that our very non-convex function is locally convex (e.g. Gauss-Newton, Levenberg-Marquardt, Fisher information matrix e.g. in K-FAC, gradient covariance matrix in TONGA - [overview](https://www.dropbox.com/s/54v8cwqyp7uvddk/SGD.pdf)) - again attracting rather not only to local minima (how bad it is?).

There is a belief that the number of saddles is exp(dim) larger than of minima. Actively repelling them (instead of attracting) requires control of sign of curvatures (as Hessian eigenvalues) - e.g. switching step sign in these directions.

It is e.g. done in **saddle-free Newton method (SFN)** ( https://arxiv.org/pdf/1406.2572 ) - 2014, 600+ citations, [recent github](https://github.com/dave-fernandes/SaddleFreeOptimizer). They claim to get a few times(!) lower error e.g. on MNIST this way, other methods stagnated on some plateaus with strong negative eigenvalues: https://i.imgur.com/xJLBGgl.png

Here is a very interesting recent paper: https://arxiv.org/pdf/1902.02366 investigating evolution of eigenvalues of Hessian for 3.3M parameters (~20 terabytes!), for example showing that rare negative curvature directions allow for relatively large improvements: https://i.imgur.com/SwUasvc.png

So it looks great - it seems that we all should use SFN or other methods actively repelling saddles ... but it didn't happen - why is it so?

What are other promising 2nd order approaches repelling saddles?

How can we improve SFN-like methods? For example what I mostly don't like is directly estimating Hessian from noisy data, what is very problematic numerically. Instead, we are really interested in linear behavior of 1st derivative - we can optimally estimate it with [\(online\) linear regression of gradients](https://arxiv.org/pdf/1901.11457) ...",23,32,False,self,,,,,
745,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,15,bbwh20,self.MachineLearning,How Machine Learning Improve Your Customer Communications,https://www.reddit.com/r/MachineLearning/comments/bbwh20/how_machine_learning_improve_your_customer/,appsbee,1554963059,[removed],0,1,False,self,,,,,
746,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,15,bbwjbg,world-port.made-in-china.com,360 virtual tour*Take you close to water tank blow molding machine manufacturers,https://www.reddit.com/r/MachineLearning/comments/bbwjbg/360_virtual_tourtake_you_close_to_water_tank_blow/,miyawang12138,1554963590,,0,1,False,default,,,,,
747,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,15,bbwlf1,self.MachineLearning,Two sample statistical hypothesis tests for high dimensional data,https://www.reddit.com/r/MachineLearning/comments/bbwlf1/two_sample_statistical_hypothesis_tests_for_high/,ambodi,1554964074,"I am looking for an implementation of a two sample tests that I can use for high dimensional data, that basically accepts or rejects a hypothesis that two samples are drawn from the same distribution. 


I came across Shoguns implementation of MMD tests, but I get a memory error for my dataset when I use it: http://shogun-toolbox.org/notebook/latest/mmd_two_sample_testing.html


PS: I prefer that the algorithm has the code implemented already.",0,1,False,self,,,,,
748,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,15,bbwpic,blockchain.oodles.io,Blockchain And Artificial Intelligence: To Foster Decentralized AI Sphere,https://www.reddit.com/r/MachineLearning/comments/bbwpic/blockchain_and_artificial_intelligence_to_foster/,Anubhav-Singh,1554965069,,0,1,False,https://b.thumbs.redditmedia.com/USM0jLdkC6hKCnJnvbnr7rpMVWGJQaV66eXBlQEvUMo.jpg,,,,,
749,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,16,bbwydg,self.MachineLearning,[D] Data Structures and Algorithms for Data Science Interviews.,https://www.reddit.com/r/MachineLearning/comments/bbwydg/d_data_structures_and_algorithms_for_data_science/,karttikey_ab,1554967177,"I have observed that most of the companies (at least here in India ) have coding rounds for recruitment. 
What level of Data Structures and Algorithms questions should I expect from a Data Science interview? 

Also , how should I start preparing for these questions?",3,6,False,self,,,,,
750,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,16,bbwyvu,self.MachineLearning,how could I make ReLu more stable?,https://www.reddit.com/r/MachineLearning/comments/bbwyvu/how_could_i_make_relu_more_stable/,qudcjf7928,1554967307,[removed],0,1,False,self,,,,,
751,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,16,bbx5lo,oodlestechnologies.com,Boost Your Mobile App Development with Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/bbx5lo/boost_your_mobile_app_development_with_artificial/,tech-info,1554969004,,0,1,False,https://b.thumbs.redditmedia.com/E0-Yk6iJ0phuBuVSAmt7ux57DNf9xvoHmyjpzcCqhsg.jpg,,,,,
752,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,16,bbx6os,arxiv.org,Samples are not all useful: Denoising policy gradient updates using variance,https://www.reddit.com/r/MachineLearning/comments/bbx6os/samples_are_not_all_useful_denoising_policy/,yfletberliac,1554969287,,1,1,False,default,,,,,
753,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,16,bbx7d4,self.MachineLearning,Python: Visual QuickStart Guide (3rd Edition) 50% Off Today Only,https://www.reddit.com/r/MachineLearning/comments/bbx7d4/python_visual_quickstart_guide_3rd_edition_50_off/,roger_brau4,1554969450,[removed],0,1,False,self,,,,,
754,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,17,bbx9zv,arxiv.org,Samples are not all useful: Denoising policy gradient updates using variance,https://www.reddit.com/r/MachineLearning/comments/bbx9zv/samples_are_not_all_useful_denoising_policy/,yfletberliac,1554970133,,3,5,False,default,,,,,
755,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,17,bbxgwk,self.MachineLearning,[D] Measuring statistical modelling capabilities of neural networks?,https://www.reddit.com/r/MachineLearning/comments/bbxgwk/d_measuring_statistical_modelling_capabilities_of/,ggNikita,1554971935,"Hey,

Does anybody know a method of measuring how well a neural network (theoretically) models a distribution? I'm especially interested in neural machine translation, as there are many ways to model the same distribution but no theoretical framework (as far as I found) to find out which approach is actually more capable. 
For example one could model p(y|x), with x being the raw data, or being the representation after performing many non linearities on x. Statistically, it is the same, but in practise they give completely different capabilities in the distribution.

Thanks!",7,6,False,self,,,,,
756,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,18,bbxtz7,self.MachineLearning,Car damage detection,https://www.reddit.com/r/MachineLearning/comments/bbxtz7/car_damage_detection/,Puratatva,1554975223,[removed],1,1,False,self,,,,,
757,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,18,bbxw0u,self.MachineLearning,"[D] Examples of machine learning applied to ""solved"" problems",https://www.reddit.com/r/MachineLearning/comments/bbxw0u/d_examples_of_machine_learning_applied_to_solved/,shapul,1554975695,"I have been recently reading the article [The Case for Learned Index Structures](https://ai.google/research/pubs/pub46518) which is about applying ML methods to a deeply investigated problem of search indexes. From the article: ""Our initial results show, that by using neural nets we are able to  outperform cache-optimized B-Trees by up to 70% in speed while saving an  order-of-magnitude in memory over several real-world data sets"".

  
This was an eye-opener to me as I always thought about B-Trees indexes as a very well investigated area which is essentially ""solved"" and we already know a lot about theoretical bounds, etc. You see, it is one thing to use ML to solve a previously unsolved or poorly done problem (e.g. object detection in CV), it is an entirely different thing to revisit a problem that we already claim we know the best general solution to.

  
So I was wondering if there are other examples that you know of that would describe an ML based method which beats well investigated and established methods in real-world situations?",13,64,False,self,,,,,
758,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,19,bby218,medium.com,Lets Dive in the World of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bby218/lets_dive_in_the_world_of_machine_learning/,yudiz,1554977014,,0,1,False,default,,,,,
759,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,19,bby4u3,octavian.ai,Machine Learning on Graphs. They have a lots of blogs too in the subject.,https://www.reddit.com/r/MachineLearning/comments/bby4u3/machine_learning_on_graphs_they_have_a_lots_of/,Athul100,1554977630,,0,1,False,default,,,,,
760,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,19,bbyeq7,github.com,[P] DGL: Python package built to ease deep learning on graphs,https://www.reddit.com/r/MachineLearning/comments/bbyeq7/p_dgl_python_package_built_to_ease_deep_learning/,rtk25,1554979745,,0,1,False,default,,,,,
761,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,19,bbyg18,self.MachineLearning,Multivariate Time Series forecasting,https://www.reddit.com/r/MachineLearning/comments/bbyg18/multivariate_time_series_forecasting/,kotopoulosb,1554980008,[removed],0,1,False,self,,,,,
762,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,19,bbygim,self.MachineLearning,preactivation in modern deep architecture,https://www.reddit.com/r/MachineLearning/comments/bbygim/preactivation_in_modern_deep_architecture/,deluded_soul,1554980109,[removed],0,1,False,self,,,,,
763,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,20,bbyo22,arxiv.org,[1904.04612] Automated Search for Configurations of Deep Neural Network Architectures,https://www.reddit.com/r/MachineLearning/comments/bbyo22/190404612_automated_search_for_configurations_of/,ihaphleas,1554981646,,5,5,False,default,,,,,
764,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,20,bbyo7h,arxiv.org,[1904.04706] Towards Safety Verification of Direct Perception Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bbyo7h/190404706_towards_safety_verification_of_direct/,ihaphleas,1554981677,,1,0,False,default,,,,,
765,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,20,bbyqdk,self.MachineLearning,[P] CppRl: A C++ reinforcement learning library using the new PyTorch C++ frontend,https://www.reddit.com/r/MachineLearning/comments/bbyqdk/p_cpprl_a_c_reinforcement_learning_library_using/,Flag_Red,1554982088,"I'm really excited to show you guys what I've been working on lately:  [https://github.com/Omegastick/pytorch-cpp-rl](https://github.com/Omegastick/pytorch-cpp-rl)

It is *very* heavily based on [Ikostrikov's wonderful pytorch-a2c-ppo-acktr-gail](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail). You could even consider this a port. The API and underlying algorithms are almost identical (with the necessary changes involved in the move to C++).

It also contains a reimplementation simple OpenAI Gym server that communicates via [ZeroMQ](http://zeromq.org/) to test the framework on Gym environments.

CppRl aims to be an extensible, reasonably optimized, production-ready framework for using reinforcement learning in projects where Python isn't viable. It should be ready to use in desktop applications on user's computers with minimal setup required on the user's side.

## Motivation

At the time of writing, there are no general-use reinforcement learning frameworks for C++. I needed one for a personal project, and the PyTorch C++ frontend had recently been released, so I figured I should make one.

## Features

* Implemented algorithms:
   * A2C
   * PPO
* Recurrent policies (GRU based)
* Cross-platform compatibility (tested on Windows 10, Ubuntu 16.04, and Ubuntu 18.04)
* Solid test coverage
* Decently optimized (always open to pull requests improving optimization though)

# Sample

![gif](r1w6ksghemr21 ""Results after training for 60 second on my laptop"")

&amp;#x200B;

**If you want to help with the project, please submit a PR!**",35,240,False,self,,,,,
766,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,21,bbz0sc,blog.eduonix.com,Discover How Machine Learning Used for Sentiment Analysis,https://www.reddit.com/r/MachineLearning/comments/bbz0sc/discover_how_machine_learning_used_for_sentiment/,Aisha_b,1554984035,,0,1,False,https://b.thumbs.redditmedia.com/9HSe0OXhqFUhV1Ig7L7-4a-PbG9khMfa8O_6KJ4-wwU.jpg,,,,,
767,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,21,bbz5ho,self.MachineLearning,"The ""dynamic"" classification problem: how to learn to recognize a new class!",https://www.reddit.com/r/MachineLearning/comments/bbz5ho/the_dynamic_classification_problem_how_to_learn/,conancui,1554984872,[removed],0,1,False,self,,,,,
768,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,22,bbzw4s,self.MachineLearning,Which variables to keep?,https://www.reddit.com/r/MachineLearning/comments/bbzw4s/which_variables_to_keep/,Midnight_Drizzle,1554989416,[removed],0,1,False,self,,,,,
769,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,22,bc00gi,arxiv.org,x [R] Community detection over a heterogeneous population of non-aligned networks,https://www.reddit.com/r/MachineLearning/comments/bc00gi/x_r_community_detection_over_a_heterogeneous/,kurtmaia,1554990091,,1,1,False,default,,,,,
770,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,22,bc04ae,self.MachineLearning,[D] Intuition behind embedding dimension and LSTM output space dimension?,https://www.reddit.com/r/MachineLearning/comments/bc04ae/d_intuition_behind_embedding_dimension_and_lstm/,Fender6969,1554990662,"So I have followed an example of building an LSTM network for sentiment analysis. I have used my own dataset and the performance of the network is pretty good. I do want to understand the logic behind choosing the right embedding dimension space and the LSTM output dimension space. How would one go on to choose an optimal  space for both? What effect would reducing the dimension space?

&amp;#x200B;

I am quite new to this, and any help would be great!",6,1,False,self,,,,,
771,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,22,bc07mc,luckbox.com,AI team favourites to beat champion Dota 2 roster,https://www.reddit.com/r/MachineLearning/comments/bc07mc/ai_team_favourites_to_beat_champion_dota_2_roster/,Maff17,1554991187,,0,1,False,default,,,,,
772,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,23,bc0ah6,arxiv.org,[R] Community detection over a heterogeneous population of non-aligned networks,https://www.reddit.com/r/MachineLearning/comments/bc0ah6/r_community_detection_over_a_heterogeneous/,kurtmaia,1554991604,,1,1,False,default,,,,,
773,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,23,bc0lqi,oodlestechnologies.com,Advantages of Machine Learning in DevOps,https://www.reddit.com/r/MachineLearning/comments/bc0lqi/advantages_of_machine_learning_in_devops/,tech-info,1554993307,,0,1,False,https://b.thumbs.redditmedia.com/TD4AkRyKFQZgREqpg6E8q0cpbCjP0LU6_SOiOMirO_o.jpg,,,,,
774,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,23,bc0qfz,medium.com,A Google Brain Program Is Learning How to Program,https://www.reddit.com/r/MachineLearning/comments/bc0qfz/a_google_brain_program_is_learning_how_to_program/,gwen0927,1554994015,,0,1,False,https://b.thumbs.redditmedia.com/P-pz03Eh0Ie93iC9K_U234MKHKpt5sln6bzQyjn0vZk.jpg,,,,,
775,MachineLearning,t5_2r3gv,2019-4-11,2019,4,11,23,bc0u31,self.biology,"Quantum mechanics! In a fucked up way, it's natural multiple choices!",https://www.reddit.com/r/MachineLearning/comments/bc0u31/quantum_mechanics_in_a_fucked_up_way_its_natural/,3rdEyeSteve,1554994545,,1,1,False,default,,,,,
776,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,0,bc108p,self.MachineLearning,[D] 4 Recommended Books on AI Ethics and Philosophy,https://www.reddit.com/r/MachineLearning/comments/bc108p/d_4_recommended_books_on_ai_ethics_and_philosophy/,seemingly_omniscient,1554995427,"One should not only know the technology and the methods. The more artificial intelligence enters our lives, the more important ethics and philosophy become. Everyone who develops ML models bears a special challenge.

&amp;#x200B;

link:  [https://www.aisoma.de/4-recommended-books-on-ai-ethics-and-ai-philosophy/](https://www.aisoma.de/4-recommended-books-on-ai-ethics-and-ai-philosophy/) 

&amp;#x200B;

https://i.redd.it/vgamjtmvhnr21.jpg",0,0,False,https://b.thumbs.redditmedia.com/zo9dARSkvlvNVnq10JEpHnPFpOh-n8X5Db8FmiQ-e5c.jpg,,,,,
777,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,0,bc11tj,self.MachineLearning,Causes of a Low TD_Error?,https://www.reddit.com/r/MachineLearning/comments/bc11tj/causes_of_a_low_td_error/,Jandevries101,1554995649,[removed],0,1,False,self,,,,,
778,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,0,bc16a7,self.MachineLearning,Are there any benefits to developing Machine Learning for reality-based video games?,https://www.reddit.com/r/MachineLearning/comments/bc16a7/are_there_any_benefits_to_developing_machine/,failedcypress57,1554996298,[removed],0,1,False,self,,,,,
779,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,0,bc1ifm,self.MachineLearning,What are the most common paradigms for setting up hyperparameters and configuration in a project?,https://www.reddit.com/r/MachineLearning/comments/bc1ifm/what_are_the_most_common_paradigms_for_setting_up/,Valiox,1554998067,[removed],0,1,False,self,,,,,
780,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,1,bc238g,self.MachineLearning,tf.linalg.eigh extremely slow on GPU - normal?,https://www.reddit.com/r/MachineLearning/comments/bc238g/tflinalgeigh_extremely_slow_on_gpu_normal/,Dents1993,1555000966,[removed],0,1,False,self,,,,,
781,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,1,bc29qo,self.MachineLearning,[D] How do you keep the fundamentals fresh as a PhD student?,https://www.reddit.com/r/MachineLearning/comments/bc29qo/d_how_do_you_keep_the_fundamentals_fresh_as_a_phd/,harmonium1,1555001888,"I feel like the knowledge that I can easily recall has become more and more narrow as the years (and my PhD) goes by, and while (I think) I have a very strong understanding of my sub-subfield, I still have to spend a decent amount of time refreshing on the fundamentals before internship interviews.

I've taken many (and TA'd several) courses in probability theory, statistical ML, algorithms, etc. but since none of the models in my field are really probabilistic and we don't use most fundamental algorithms or data structures I'm finding it more difficult to recall those topics during interviews.  Given enough time I can find a DP recurrence relation but not as fast as I would like since I haven't had to do this in years.  Ditto for questions like implementing EM from scratch.

How do you keep these topics fresh?  Do you occasionally look over old notes, grind leetcode once or twice a week, etc?",24,20,False,self,,,,,
782,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,1,bc29vm,self.MachineLearning,Any information about neuromorphic engineering?,https://www.reddit.com/r/MachineLearning/comments/bc29vm/any_information_about_neuromorphic_engineering/,idunnomanjesus,1555001907,[removed],0,0,False,self,,,,,
783,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,2,bc2mvd,medium.com,TableBank: Benchmark for Image-based Table Detection and Recognition,https://www.reddit.com/r/MachineLearning/comments/bc2mvd/tablebank_benchmark_for_imagebased_table/,gwen0927,1555003701,,0,1,False,https://b.thumbs.redditmedia.com/yrmELCajDeIomFhsLeF4Y9HID3FvpsVPJvUAwqGQqhU.jpg,,,,,
784,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,2,bc2qt5,self.MachineLearning,[P] TensorFlow Estimators API - Feeding large datasets from drive via TFRecords,https://www.reddit.com/r/MachineLearning/comments/bc2qt5/p_tensorflow_estimators_api_feeding_large/,postmodernequestrian,1555004243,"My husband has spent all his free time in the last two years immersing himself in DS and ML. 

I wanted to share the results of one of his self-initiated projects (as specified in the title):  [https://datamadness.github.io/tensorflow\_estimator\_large\_dataset\_feed](https://datamadness.github.io/tensorflow_estimator_large_dataset_feed) 

I know next to nothing about this subject, but thought this community might find it interesting if not helpful. :)",0,0,False,self,,,,,
785,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,2,bc2y2x,kaltura.com,"Live stream of Michael I. Jordan, panel discussion on ""Machine Learning: Dynamical, Stochastic &amp; Economic Perspectives""",https://www.reddit.com/r/MachineLearning/comments/bc2y2x/live_stream_of_michael_i_jordan_panel_discussion/,snugghash,1555005269,,0,1,False,default,,,,,
786,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,2,bc2z6a,self.MachineLearning,LSTM on time-series data,https://www.reddit.com/r/MachineLearning/comments/bc2z6a/lstm_on_timeseries_data/,LazyButAmbitious,1555005426,[removed],0,1,False,self,,,,,
787,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,2,bc30bn,self.MachineLearning,Best ways to create summaries of conversations and/or threads?,https://www.reddit.com/r/MachineLearning/comments/bc30bn/best_ways_to_create_summaries_of_conversations/,HenryDashwood,1555005598,[removed],0,1,False,self,,,,,
788,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,3,bc3eti,self.MachineLearning,Anyone know whats going on with KDD 2019?,https://www.reddit.com/r/MachineLearning/comments/bc3eti/anyone_know_whats_going_on_with_kdd_2019/,lavenderteacupcake,1555007666,Hopefully this is the right place to post this. The registration date for attendees had been delayed a few times. Does anyone know when registration will open or whats going on?,0,1,False,self,,,,,
789,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,3,bc3hcv,self.MachineLearning,Suitable algorithms and libraries for training a model from survey submissions?,https://www.reddit.com/r/MachineLearning/comments/bc3hcv/suitable_algorithms_and_libraries_for_training_a/,boyski33,1555008027,"For my college project I am developing an online survey platform where users can either register with their age and gender and take survey, or take surveys anonymously without providing personal details. I want to train models for each survey based on the answers logged in users provide, and predict the age &amp; gender of anonymous users. So basically I'm thinking of building two separate models per survey - one for age using linear regression, and one for gender using logistic regression. However, I'm yet to start coding this service, and I haven't decided what library to use exactly. Scikit learn or TensorFlow? What specific algorithms? Is there a way to train only one model per survey, or should I stick with two? Any pointers will be of great help! Needless to say, I'm using Python for the ML service.",0,1,False,self,,,,,
790,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,4,bc4284,self.MachineLearning,[D] Is deep learning easy ?,https://www.reddit.com/r/MachineLearning/comments/bc4284/d_is_deep_learning_easy/,saadmrb,1555011009,"The math requirement is very low, if none existent. Sure, it is using a gradient and back propagation to solve a very large system of weights. The details of that are really not critical for anyone using deep learning in production.

The solvers, compared to others that humans have invented (BFGS, genetic algorithms, etc..) are also relatively dumb. One of the most popular ones is still Stochastic Gradient Descent (SGD). When I think of these solvers you have concepts of intensification (race to the answer) and diversification (jump around a lot). Really good solvers should be able to snap back and forth between these two extremes as needed, chasing promising solutions, but jumping out of local minima.

The basic multi-layered convolutional network (think VGG-16) is very simple in comparison to most things in engineering. The fact that it works as well as it does is a miracle.",4,0,False,self,,,,,
791,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,4,bc44pm,github.com,TC-ResNet: Mobile Real-time Keyword Spotting model with 385x speedup and state-of-the-art accuracy,https://www.reddit.com/r/MachineLearning/comments/bc44pm/tcresnet_mobile_realtime_keyword_spotting_model/,sanxiyn,1555011360,,0,1,False,default,,,,,
792,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,4,bc45fj,self.MachineLearning,Testing error not a valid assessment of an implementation of logistic regression?,https://www.reddit.com/r/MachineLearning/comments/bc45fj/testing_error_not_a_valid_assessment_of_an/,Mariguanaa,1555011462,[removed],0,1,False,self,,,,,
793,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,4,bc46dk,self.MachineLearning,[P] YouTube Advertisement Scraper,https://www.reddit.com/r/MachineLearning/comments/bc46dk/p_youtube_advertisement_scraper/,raijinraijuu,1555011601,"I am working on a project to predict advertisement effectiveness based on ad video content. I realized there wasn't any dataset for this task, so I wrote my own python code to collect ads on youtube. The code downloads advertisement videos and the website the ad links to. I wanted to share the code in case this data would be useful for anyone. Link to Github:  [https://github.com/sdilbaz/Youtube-Advertisement-Collector](https://github.com/sdilbaz/Youtube-Advertisement-Collector) 

PS: If you have any comments, or if there is any functionality you want me to add please tell me. Thanks",16,8,False,self,,,,,
794,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,5,bc4mrc,medium.com,NAACL 2019 | Google BERT Wins Best Long Paper,https://www.reddit.com/r/MachineLearning/comments/bc4mrc/naacl_2019_google_bert_wins_best_long_paper/,gwen0927,1555013923,,0,1,False,default,,,,,
795,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,5,bc4pkc,reddit.com,Should OOB (Out Of Bag) error be less than a Test set error in Random Forests?,https://www.reddit.com/r/MachineLearning/comments/bc4pkc/should_oob_out_of_bag_error_be_less_than_a_test/,stats_nerd21,1555014319,,0,1,False,default,,,,,
796,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,6,bc5csf,self.MachineLearning,[R] Generator persistence in GANs,https://www.reddit.com/r/MachineLearning/comments/bc5csf/r_generator_persistence_in_gans/,MathCompNeuroProf,1555017736,"I had an idea that I haven't been able to find implemented elsewhere, but might not be using the right search terms.

After generating an image with a GAN, use the generator with its current parameters fixed to iteratively update the image, either through grad descent on the pixels or on the generator params (which would be reverted to their old values after this).

This idea overlaps with minibatch persistence and with the deep image prior paper.

Has this been tried? Is there any reason to think it wouldn't work?",2,1,False,self,,,,,
797,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,7,bc61bq,self.MachineLearning,[D] Online financial advisor - Active learning?,https://www.reddit.com/r/MachineLearning/comments/bc61bq/d_online_financial_advisor_active_learning/,jfl88,1555021360,"Consider an online financial advisor (robo-advisor) that makes investment decisions tailored to the risk-preferences of a customer. As time goes by, the customer's risk-preferences change, and when the uncertainty is high enough, so the machine is not sure what investment decision to make, it pings the customer and ask for updated information. 

Can this be viewed as an active learning situation? 

In active learning one strategically decides which samples to label, based on the information they provide. Here there is really just one ""data point"", the customer, that is ""labeled"" to begin with. The active learning analogy would be that when the machine becomes unsure about the label (risk-preferences), it asks for re-labeling (updated risk-preferences), i.e., it re-labels at a point where it provides a lot of information.",2,2,False,self,,,,,
798,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,7,bc62yo,self.MachineLearning,[D] Seeking feedback on my ML tutorial for a non-technical audience,https://www.reddit.com/r/MachineLearning/comments/bc62yo/d_seeking_feedback_on_my_ml_tutorial_for_a/,philosophical_lens,1555021612,"I have the opportunity to give a 45-min ML tutorial to a non-technical audience, and this is what I'm planning. I'd really appreciate any feedback / comments. Thanks!

**Part 1: What is a neural network?** (15 mins)
- I'll either show this video or give a similar walkthrough

**Part 2: Build your own neural network in 15 minutes!** (15 mins)
- I'm going to create a Jupyter notebook on Google Colab, using Keras to build a model for MNIST
- During the session, I will share the Colab link with all the participants and we'll walk through the steps together.
- **Question**: Is there any way to incorporate some interactive image drawing capability into Colab such that the audience can draw an digit with their mouse and get a model prediction?

**Part 3: Demo some other cool image recognition models** (10 mins)
- The goal here is to demonstrate the power of some readily available models such as Imagenet, and the fact that I can easily run these models in minutes. I will ask the audience to share images with me and run them through the model.",5,2,False,self,,,,,
799,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,10,bc7ujn,arxiv.org,Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy Images Using Color Balancing on Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bc7ujn/diagnosis_of_celiac_disease_and_environmental/,kk7nc,1555032306,,3,6,False,default,,,,,
800,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,10,bc7v4k,arxiv.org,RMDL: Random Multimodel Deep Learning for Classification,https://www.reddit.com/r/MachineLearning/comments/bc7v4k/rmdl_random_multimodel_deep_learning_for/,kk7nc,1555032400,,2,19,False,default,,,,,
801,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,10,bc7wc8,github.com,SPADE GauGAN Source Code now Released,https://www.reddit.com/r/MachineLearning/comments/bc7wc8/spade_gaugan_source_code_now_released/,scriptcoder43,1555032613,,0,1,False,default,,,,,
802,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,10,bc7yl6,arxiv.org,"[R] ""An Empirical Study of Spatial Attention Mechanisms in Deep Networks"" from MSRA",https://www.reddit.com/r/MachineLearning/comments/bc7yl6/r_an_empirical_study_of_spatial_attention/,flyforlight,1555033011,,5,16,False,default,,,,,
803,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,10,bc7zuz,github.com,[D] SPADE GauGAN Source Code now Released,https://www.reddit.com/r/MachineLearning/comments/bc7zuz/d_spade_gaugan_source_code_now_released/,scriptcoder43,1555033230,,1,1,False,https://b.thumbs.redditmedia.com/70oEiw1KYeLhskaVAioVAxdY6e5FzLdKdIZ_iO4B5fw.jpg,,,,,
804,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,10,bc82b9,self.MachineLearning,ELI30 - how does umap dimensionality reduction work?,https://www.reddit.com/r/MachineLearning/comments/bc82b9/eli30_how_does_umap_dimensionality_reduction_work/,fancysciency,1555033643,[removed],0,1,False,self,,,,,
805,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,11,bc87aj,self.nvidia,[N] Nvidia SPADE (GuaGAN) is now Open Source,https://www.reddit.com/r/MachineLearning/comments/bc87aj/n_nvidia_spade_guagan_is_now_open_source/,scriptcoder43,1555034491,,0,1,False,https://b.thumbs.redditmedia.com/3YXqNxVwP8mrK6de0KHUMX0GL-v8hi2uVAzlYyEBEmw.jpg,,,,,
806,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,11,bc88yd,self.MachineLearning,[Updated] A (cat) machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/bc88yd/updated_a_cat_machine_learning_game_ive_been/,twm7,1555034765,[removed],0,1,False,self,,,,,
807,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,11,bc89ku,self.MachineLearning,[P] A (cat) machine learning game I've been working on...,https://www.reddit.com/r/MachineLearning/comments/bc89ku/p_a_cat_machine_learning_game_ive_been_working_on/,twm7,1555034867,"I've finished working on the new algorithm which is based on ID3; entropy and information gain. Interested to see feedback! [Challenge Incredicat](https://incredicat.com/)!

\-------

Wasn't sure where to post this as I'm still working on it but wanted to put it out there to get any useful feedback or thoughts from the experts. It's basically a game similar to 20 Questions (or Animal, Vegetable, Mineral) that attempts to ask you questions to work out an object you are thinking about. You can think of everyday items (animals, household objects, food, quite a bit of other stuff etc) and it has 30 questions to try and guess the item. I've been working on it for a while but not sure what to do next so interested to hear anyone's thoughts...

The link for anyone that wants to try it out is [incredicat.com](https://incredicat.com/)

Thanks in advance!",150,118,False,self,,,,,
808,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,14,bc9yul,learntek.org,Difference between Machine Learning and Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/bc9yul/difference_between_machine_learning_and/,sankarsp,1555047064,,0,1,False,default,,,,,
809,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,15,bcacbi,defouranalytics.com,Defour Analytics | Data Science Course in Pune,https://www.reddit.com/r/MachineLearning/comments/bcacbi/defour_analytics_data_science_course_in_pune/,ankita11_,1555050220,,0,1,False,default,,,,,
810,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,15,bcad99,self.MachineLearning,[R] 12 Key Lessons from ML researchers and practitioners,https://www.reddit.com/r/MachineLearning/comments/bcad99/r_12_key_lessons_from_ml_researchers_and/,SamiaKd,1555050438,"Hey all, I recently stumbled upon a great research paper that puts together lessons learned by machine learning researchers and practitioners for developing successful ML applications. 

I've summarized this folk wisdom, that is often hard to come by but is much needed, here:  [https://towardsml.com/2019/04/09/12-key-lessons-from-ml-researchers-and-practitioners/](https://towardsml.com/2019/04/09/12-key-lessons-from-ml-researchers-and-practitioners/) 

Check it out. Hope it helps! :)",2,1,False,self,,,,,
811,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,15,bcaddk,lhd.co.com,LHD S.p.A. strives to be one of the most flexible load handling devices supplier in the industry.,https://www.reddit.com/r/MachineLearning/comments/bcaddk/lhd_spa_strives_to_be_one_of_the_most_flexible/,lhd121,1555050466,,0,1,False,default,,,,,
812,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,15,bcafw2,self.MachineLearning,3DTV Market Analysis Reveals unstable development by 2023,https://www.reddit.com/r/MachineLearning/comments/bcafw2/3dtv_market_analysis_reveals_unstable_development/,harshbir123456789,1555051054,[removed],1,1,False,self,,,,,
813,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,15,bcagod,self.MachineLearning,GPT-2 for hotel reviews generation?,https://www.reddit.com/r/MachineLearning/comments/bcagod/gpt2_for_hotel_reviews_generation/,mekass,1555051243,"Hi guys,

&amp;#x200B;

Just tried to experiment with GPT-2 for hotel reviews generation -&gt; [https://github.com/tomasrasymas/gpt2-hotel-reviews](https://github.com/tomasrasymas/gpt2-hotel-reviews)  


After analysing the results, I think GPT-2 is not suitable for such a task (short reviews generation), cause reviews in dataset are very similar and talking about same thinks, there are no context for model to learn. In my opinion using simpler model like few LSTM + Attention might produce same or even better results.   


What do you think?",0,1,False,self,,,,,
814,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,15,bcaj9h,self.MachineLearning,[D] GPT-2 for hotel reviews generation?,https://www.reddit.com/r/MachineLearning/comments/bcaj9h/d_gpt2_for_hotel_reviews_generation/,mekass,1555051844,"Hi guys,

&amp;#x200B;

Just tried to experiment with GPT-2 for hotel reviews generation -&gt; [https://github.com/tomasrasymas/gpt2-hotel-reviews](https://github.com/tomasrasymas/gpt2-hotel-reviews)

After analysing the results, I think GPT-2 is not suitable for such a task (short text, reviews generation), cause reviews in dataset are very similar and talking about same thinks, there are no context for model to learn. In my opinion using simpler model like few LSTM + Attention might produce same or even better results.

&amp;#x200B;

What do you think?",19,11,False,self,,,,,
815,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,16,bcan97,self.MachineLearning,Help with custom training loops in tensorflow 2.0 / tf.Keras,https://www.reddit.com/r/MachineLearning/comments/bcan97/help_with_custom_training_loops_in_tensorflow_20/,venktech,1555052815,[removed],0,1,False,self,,,,,
816,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,16,bcanb1,analyticsinsight.net,US Congress Introduces a Regulatory Bill for Machine Learning Algorithms | Analytics Insight,https://www.reddit.com/r/MachineLearning/comments/bcanb1/us_congress_introduces_a_regulatory_bill_for/,analyticsinsight,1555052828,,0,1,False,default,,,,,
817,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,16,bcatey,self.MachineLearning,A model to identify copied content?,https://www.reddit.com/r/MachineLearning/comments/bcatey/a_model_to_identify_copied_content/,cooper_bw,1555054368,[removed],0,1,False,self,,,,,
818,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,16,bcazbs,oodlestechnologies.com,An Overview of Some Major Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/bcazbs/an_overview_of_some_major_machine_learning/,tech-info,1555055942,,0,1,False,https://b.thumbs.redditmedia.com/dg_p-JCwe__5Dj8ChMIN6K94wFIbGclr4u_Co8AC88A.jpg,,,,,
819,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,18,bcbq8u,self.MachineLearning,Using openai's gpt-2 model for perplexity of a word in a given sentence,https://www.reddit.com/r/MachineLearning/comments/bcbq8u/using_openais_gpt2_model_for_perplexity_of_a_word/,chenXchen,1555063037,[removed],0,1,False,self,,,,,
820,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,19,bcbsln,self.MachineLearning,Drone like a bird,https://www.reddit.com/r/MachineLearning/comments/bcbsln/drone_like_a_bird/,jacperlarsson,1555063578,[removed],0,1,False,self,,,,,
821,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,19,bcbtt3,self.datascience,Queries regarding Data Science for Social Causes/Charity,https://www.reddit.com/r/MachineLearning/comments/bcbtt3/queries_regarding_data_science_for_social/,dohvakiinC137,1555063864,,0,1,False,default,,,,,
822,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,19,bcbuw8,self.MachineLearning,What are Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/bcbuw8/what_are_neural_networks/,brintiawebinars123,1555064111,"Hi guys! I found this free Neural Networks in Machine Learning webinar which you might be interested in. Its an ever-evolving paradigm of deep learning. The basic concepts and types of existing neural networks will be touched upon as well as their main development frameworks. A demo will also be provided to demonstrate just how important they are.

&amp;#x200B;

What do you know of Neural Networks?",0,1,False,self,,,,,
823,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,19,bcc48q,self.MachineLearning,Need help choosing hardware.,https://www.reddit.com/r/MachineLearning/comments/bcc48q/need_help_choosing_hardware/,Ritvik_Somayaji,1555066192,[removed],0,1,False,self,,,,,
824,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,20,bcc7na,self.MachineLearning,How to keep up to date within new ML publications?,https://www.reddit.com/r/MachineLearning/comments/bcc7na/how_to_keep_up_to_date_within_new_ml_publications/,johtru,1555066878,[removed],0,1,False,self,,,,,
825,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,20,bccogc,self.MachineLearning,Introduction to Machine Learning | What is Machine Learning | Supervised...,https://www.reddit.com/r/MachineLearning/comments/bccogc/introduction_to_machine_learning_what_is_machine/,kunaalsharma,1555070149,[removed],0,1,False,self,,,,,
826,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,21,bccws6,go-gazette.com,"Release Go 1.12.3 + 1.11.8, Golang memory ballast, build drones 10x faster &amp; more",https://www.reddit.com/r/MachineLearning/comments/bccws6/release_go_1123_1118_golang_memory_ballast_build/,GoGazette,1555071661,,0,1,False,https://b.thumbs.redditmedia.com/2-FkEB9xw2GKU1_GZrH7-95x35PPDbU1HbqgeVRXOWU.jpg,,,,,
827,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,21,bccx9o,self.MachineLearning,Best performance with overfitted model?,https://www.reddit.com/r/MachineLearning/comments/bccx9o/best_performance_with_overfitted_model/,dakpanWTS,1555071746,[removed],0,1,False,self,,,,,
828,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,21,bccynh,self.MachineLearning,What do you think of Apple's approach of removal of CUDA &amp; NVidia graphics card support in current macOS (10.14) from the machine learning standpoint?,https://www.reddit.com/r/MachineLearning/comments/bccynh/what_do_you_think_of_apples_approach_of_removal/,palash89,1555072006,[removed],0,1,False,self,,,,,
829,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,21,bcd00a,self.MachineLearning,Meetup with Solution Architect from Databricks (also online),https://www.reddit.com/r/MachineLearning/comments/bcd00a/meetup_with_solution_architect_from_databricks/,JanMulkens,1555072249,[removed],1,1,False,self,,,,,
830,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,21,bcd4f5,forbes.com,Explainable AI and the Rebirth of Rules,https://www.reddit.com/r/MachineLearning/comments/bcd4f5/explainable_ai_and_the_rebirth_of_rules/,jonfla,1555073032,,0,1,False,default,,,,,
831,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,22,bcdfm2,self.MachineLearning,Poor numpy performance on AVX-512 enable Intel CPU,https://www.reddit.com/r/MachineLearning/comments/bcdfm2/poor_numpy_performance_on_avx512_enable_intel_cpu/,alexgand,1555074906,[removed],0,1,False,self,,,,,
832,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,22,bcdfyd,self.MachineLearning,[P] Donald Trump AI Demonstrating ICface,https://www.reddit.com/r/MachineLearning/comments/bcdfyd/p_donald_trump_ai_demonstrating_icface/,hanyuqn,1555074958,"https://youtu.be/7MKcoctU-es

Speech synthesized with my own Trump TTS model.",7,7,False,self,,,,,
833,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,22,bcdo9a,youtube.com,"Kaggle Days Paris - ""ML Interpretability: the key to ML adoption in the enterprise""",https://www.reddit.com/r/MachineLearning/comments/bcdo9a/kaggle_days_paris_ml_interpretability_the_key_to/,polyglotdev,1555076335,,0,1,False,default,,,,,
834,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,23,bcdya7,self.MachineLearning,[P] Beginner Project for Breaking Old-fashioned Captchas,https://www.reddit.com/r/MachineLearning/comments/bcdya7/p_beginner_project_for_breaking_oldfashioned/,Murdochhh,1555077970,"Hello everybody,

As two under grad students we just started in the field of machine learning, and made a toy project to break simple captchas. 

&amp;#x200B;

Although it is a modest project, we wanted to share our experience. We would like to hear your opinions and suggestions on it.

&amp;#x200B;

Here is the write-up for the project :

[https://cagriuysal.github.io/Simple-Captcha-Breaker/](https://cagriuysal.github.io/Simple-Captcha-Breaker/)

&amp;#x200B;

Thank you all.",7,21,False,self,,,,,
835,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,23,bce5ym,heartbeat.fritz.ai,Implementing Smart Replies in an Android app,https://www.reddit.com/r/MachineLearning/comments/bce5ym/implementing_smart_replies_in_an_android_app/,the-dagger,1555079143,,0,2,False,https://b.thumbs.redditmedia.com/AX3JkDaIwYog1QMGc6K1Vj_Z6f7cq5HSIu_qm3Y3NoQ.jpg,,,,,
836,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,23,bce9wy,self.MachineLearning,ML Document Prediction Project,https://www.reddit.com/r/MachineLearning/comments/bce9wy/ml_document_prediction_project/,HummelsM,1555079756,[removed],0,1,False,self,,,,,
837,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,23,bcecvj,self.MachineLearning,"Tensorflow Implmentation of VQA 2017 Winner ""Bottom-up and Top-down Attention for VQA""",https://www.reddit.com/r/MachineLearning/comments/bcecvj/tensorflow_implmentation_of_vqa_2017_winner/,doyuplee,1555080207,[removed],0,1,False,self,,,,,
838,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,23,bcee2r,self.MachineLearning,"[D] Kaiming He's original residual network results in 2015 have not been reproduced, not even by Kaiming He himself.",https://www.reddit.com/r/MachineLearning/comments/bcee2r/d_kaiming_hes_original_residual_network_results/,CatchADragonFish,1555080389,"What's going on here? I have not found a single paper that reproduces or compares against the results shown in Table 4 of the original residual network paper. All papers report significantly worse numbers.

&amp;#x200B;

[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)

top1 err numbers from the paper:

&gt;ResNet-50 @ 20.74  
&gt;  
&gt;ResNet-101 @ 19.87  
&gt;  
&gt;ResNet-152 @ 19.38

&amp;#x200B;

This paper have 20,000+ citations. DenseNet ([https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993), 3000+ citations) and Wide ResNets ([https://arxiv.org/abs/1605.07146](https://arxiv.org/abs/1605.07146), \~1000 citations) don't use this result. Not even one of Kaiming He's recent papers ([https://arxiv.org/abs/1904.01569](https://arxiv.org/abs/1904.01569)) use this result. Since I'm new to the community, maybe I'm missing something here. But isn't this paper one of the most cited pieces of work in the field??",28,157,False,self,,,,,
839,MachineLearning,t5_2r3gv,2019-4-12,2019,4,12,23,bceeex,self.MachineLearning,"[P] The Winner of VQA 2017, Tensorflow Implementation, ""Bottom-up and Top-down Attention for VQA""",https://www.reddit.com/r/MachineLearning/comments/bceeex/p_the_winner_of_vqa_2017_tensorflow/,doyuplee,1555080439,"The winner of VQA 2017 Challenge, ""Bottom-up and Top-down Attention for VQA"" is a simple model and used for various researches.

However, there was no tensorflow implementation.

So, I implemented and reproduced the result.

[https://github.com/LeeDoYup/bottom-up-attention-tf](https://github.com/LeeDoYup/bottom-up-attention-tf)",3,5,False,self,,,,,
840,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,0,bcelvv,self.MachineLearning,[P] 5-min paper challenge + cash prize; UPDATE,https://www.reddit.com/r/MachineLearning/comments/bcelvv/p_5min_paper_challenge_cash_prize_update/,tdls_to,1555081529,"I recently posted this competition we are running; do a 5-min video of any ML paper for a chance to win a prize. Here is  an update (link for more details):

1- deadline for submission is April 29th now

2- the first place prize is $400 (+ more prizes)

3- you get points for challenging others to participate (they have to write your name when they submit)

4- you can submit as a team

5- we will hold more office hours to help you figure out any challenges you face

&amp;#x200B;

Any thoughts, questions, feedback? don't hesitate to let me know

more detail: [https://aisc.a-i.science/5-min-challenge/](https://aisc.a-i.science/5-min-challenge/)",0,0,False,self,,,,,
841,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,0,bceru8,medium.com,Tencent Open-Sourced Algorithm Betters Face Detection Benchmarks,https://www.reddit.com/r/MachineLearning/comments/bceru8/tencent_opensourced_algorithm_betters_face/,Yuqing7,1555082402,,0,1,False,default,,,,,
842,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,0,bceuxv,marktechpost.com,Top Artificial Intelligence Influencers To Follow in 2019,https://www.reddit.com/r/MachineLearning/comments/bceuxv/top_artificial_intelligence_influencers_to_follow/,ai-lover,1555082843,,0,1,False,default,,,,,
843,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,0,bceyig,self.MachineLearning,Complete beginner : how should i tackle this project ?,https://www.reddit.com/r/MachineLearning/comments/bceyig/complete_beginner_how_should_i_tackle_this_project/,EbonyHelicoidalRhino,1555083355,[removed],0,1,False,self,,,,,
844,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,1,bcfekj,self.MachineLearning,similarity function for keywords that often occur together,https://www.reddit.com/r/MachineLearning/comments/bcfekj/similarity_function_for_keywords_that_often_occur/,smasid,1555085655,[removed],0,1,False,self,,,,,
845,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,1,bcfiq8,self.MachineLearning,First Time Build for GANs,https://www.reddit.com/r/MachineLearning/comments/bcfiq8/first_time_build_for_gans/,HUSKODILE,1555086255,[removed],0,1,False,self,,,,,
846,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,1,bcflg1,self.MachineLearning,Breed and mutate your neural networks to tackle reinforcement learning tasks,https://www.reddit.com/r/MachineLearning/comments/bcflg1/breed_and_mutate_your_neural_networks_to_tackle/,cai_9,1555086650,[removed],0,1,False,self,,,,,
847,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,1,bcfn92,self.MachineLearning,Machine Learning for Causal Inference,https://www.reddit.com/r/MachineLearning/comments/bcfn92/machine_learning_for_causal_inference/,the_universe_is_vast,1555086907,[removed],0,1,False,self,,,,,
848,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,1,bcfw24,self.MachineLearning,[P] Deep learning tutorial from first principles,https://www.reddit.com/r/MachineLearning/comments/bcfw24/p_deep_learning_tutorial_from_first_principles/,sohyongsheng,1555088198,"Hey All,

I've recently started a series of Instagram posts showing how we can derive many of the inner workings of deep learning from first principles. I find too many deep learning courses that gloss over fundamentals, and it's probably due to the industry's prioritization to churn out workers to train models and produce results quickly. But if we want to master our craft and move behind equations and code with ease, I believe we need to understand the fundamental principles.

Another reason why I'm doing this is because I'm a firm believer of Feynman's ""What I cannot create, I do not understand."" This project is also a way for me to solidify my understanding of deep learning.

Check it out:

[https://www.instagram.com/learnsohdeep/](https://www.instagram.com/learnsohdeep/)",1,1,False,self,,,,,
849,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,1,bcfw5u,github.com,[P] Digit recognition using a neural network. An interactive experiment based on the MNIST data set.,https://www.reddit.com/r/MachineLearning/comments/bcfw5u/p_digit_recognition_using_a_neural_network_an/,atum47,1555088212,,0,1,False,default,,,,,
850,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,2,bcfyo2,self.MachineLearning,[D] PyTorch implementation best practices,https://www.reddit.com/r/MachineLearning/comments/bcfyo2/d_pytorch_implementation_best_practices/,floodvalve,1555088559,"Hi r/MachineLearning! I recently finished a PyTorch [**re-implementation**](https://github.com/joel-huang/zeroshot-capsnet-pytorch) (with help from various sources) the paper [Zero-shot User Intent Detection via Capsule Neural Networks](https://arxiv.org/abs/1809.00385), which originally had Python 2 code for TensorFlow.

I'd like to request perhaps a critique on the code I've written so far (it's not perfect, yet!) and any suggestions if there are best practices specifically in PyTorch, for implementing directly from research papers as well as converting them from other frameworks.

Some thoughts I had while programming:
- I've been implementing a Dataset class and custom batch functions for every dataset I've been working with. Is this the PyTorch best practice?
- Where is the optimal place to shift `Tensors` to `.cuda()`? I've been doing this in the training loop, just before feeding it into the model.
- How to manage the use of both `numpy` and `torch`, seeing as PyTorch aims to reinvent many of the basic operations in `numpy`?

If you're a fellow PyTorch user/contributor please share a little!",15,38,False,self,,,,,
851,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,2,bcg1w9,self.MachineLearning,Do GPUs only speed up ANN training if hidden layers contain many nodes?,https://www.reddit.com/r/MachineLearning/comments/bcg1w9/do_gpus_only_speed_up_ann_training_if_hidden/,smiles17,1555089007,[removed],0,1,False,self,,,,,
852,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,2,bcg6dz,self.MachineLearning,Just an Idea,https://www.reddit.com/r/MachineLearning/comments/bcg6dz/just_an_idea/,doorshop254,1555089607,[removed],0,1,False,self,,,,,
853,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,2,bcg7na,self.MachineLearning,"[P] Self organizing map of english characters and numbers, based on looks",https://www.reddit.com/r/MachineLearning/comments/bcg7na/p_self_organizing_map_of_english_characters_and/,matejkozic,1555089773,"I trained a convolutional neural net to recognize latin alphabet characters, and numbers (0-9, A-Z, a-z), then let it predict the category of all characters of a single font, then extracted that into 62 vectors, each describing how the computer sees those pictures of characters. I fed those 62 vectors into a self organizing map.",6,8,False,self,,,,,
854,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,2,bcgdio,self.MachineLearning,[D] Any ideas on how to map a variably sized sequence of vectors into a fixed size vector?,https://www.reddit.com/r/MachineLearning/comments/bcgdio/d_any_ideas_on_how_to_map_a_variably_sized/,timuber,1555090574,"Hi redditors,

&amp;#x200B;

I am currently working on a project and I want to create document embeddings using the Google BERT model. Sadly BERT has a limit on the input size and therefore I cannot push whole texts into it. Now I need to encode each sentence and generate a document feature vector out of it. I had a look at the literature, but that does not seem to be an active research problem. Are you aware of any techniques which encode e.g. the semantic structure of the sequence into such a vector?",4,2,False,self,,,,,
855,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,2,bcgkj8,self.MachineLearning,[P] Machine learning toolkit for packaging and deploying models,https://www.reddit.com/r/MachineLearning/comments/bcgkj8/p_machine_learning_toolkit_for_packaging_and/,yubozhao,1555091538,"Hi guys.

We recently open sourced a project, [BentoML!](www.github.com/bentoml/bentoml)  that packaging and deploying ML models to production.  We would love to hear your thoughts.

**Quick pitch: From model in Jupyter notebook to production in 5 minutes.**

BentoML is a python library for packaging and deploying ML models.  It does two things without any changes to your training workflow:
1. It standardize how to package your models, including preprocessing / feature fetching code, dependencies/env, and configuration.
2.  It easily distributes your model as Pypi package, API server(local /docker image), CLI tool or spark UDF.

We think there should be a simple way for data scientists to ship models to production. Our vision is empower them to own models 'end-to-end' for production service, just like software engineers do. 

Here is a quick look of how it works:

In your notebook, you have trained your scikit-learn model:
    from sklearn import svm
    from sklearn import datasets

    clf = svm.SVC(gamma='scale')
    iris = datasets.load_iris()
    X, y = iris.data, iris.target
    clf.fit(X, y)

To package this model with BentoML, you will need to create a new BentoService by subclassing it, and provides artifacts and env definition for it:
    %%writefile iris_classifier.py
    from bentoml import BentoService, api, env, artifacts
    from bentoml.artifact import PickleArtifact
    from bentoml.handlers import DataframeHandler

    @artifacts([PickleArtifact('model')])
    @env(conda_dependencies=[""scikit-learn""])
    class IrisClassifier(BentoService):
        @api(DataframeHandler)
        def predict(self, df):
            return self.artifacts.model.predict(df)

Now, to save your trained model for prodcution use, simply import your BentoService class and pack it with required artifacts:
    from iris_classifier import IrisClassifier

    svc = IrisClassifier.pack(model=clf)
    svc.save('./saved_bento', version='v0.0.1') # Saving archive to ./saved_bento/IrisClassifier/v0.0.1/
That's it. Now you have created your first BentoArchive. It's a directory containing all the source code, data and configurations files required to run this model in production.

**Couple ways to use this packaged model archive.**

Serve the model as local REST API endpoint:
    bentoml serve --archive-path=""./saved_bento/IrisClassifier/v0.0.1/""

Build API server Docker Image from BentoArchive
    $cd ./saved_bento/IrisClassifier/v0.0.1/
    $docker build -t myorg/iris-classifier .
    # After docker build finish
    $docker run -p 5000:5000 myorg/iris-classifier

Load your packaged archive in Python:
    import bentoml

    bento_svc = bentoml.load('./saved_bento/IrisClassifier/v0.0.1/')
    bento_svc.predict(X[0])

Install BentoArchive as PyPI package
    pip install ./saved_bento/IrisClassifier/v0.0.1/
Now you can use it as module in your python code
    from IrisClassifier import IrisClassifier

    installed_svc = IrisClassifier()
    installed_svc.predict(X[0])

Use archived package as CLI tool:
   $pip install ./saved_bento/IrisClassifier/v0.0.1/
   $IrisClassifier info
   $IrisClassifier predict --input='./test.csv'
Alternatively, you can also use the bentoml cli to load and run the package directly:
   bentoml predict ./saved_bento/IrisClassifier/v0.0.1/ --input='./test.csv'



Feel free to ping me and ask any questions. I love to get you guys feedback and improve this project.

Cheers!",11,5,False,self,,,,,
856,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,3,bcgo8h,self.MachineLearning,Real time text detection using VisionAPI,https://www.reddit.com/r/MachineLearning/comments/bcgo8h/real_time_text_detection_using_visionapi/,KingMSM,1555092056,[removed],0,1,False,self,,,,,
857,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,3,bcgx8w,self.MachineLearning,[D] Do GPUs only speed up ANN training when nodes per layer is high?,https://www.reddit.com/r/MachineLearning/comments/bcgx8w/d_do_gpus_only_speed_up_ann_training_when_nodes/,smiles17,1555093264,"TL;DR: Tensorflow fashion mnist example only quicker running on GPU if I increase the number of nodes in the hidden layer.

&amp;#x200B;

Just spent a good day replacing my AMD gpu with an nvidia one and installing cuda and whatnot. Finally got it working and loaded up Tensorflow's fashion mnist example, fully expecting training with my new setup to be miles quicker. To my horror, it was slower. Much slower: cpu=12s, gpu=20s.

&amp;#x200B;

The example only has 128 nodes in the hidden layer. If I increase that to 2048, the gpu is much faster (cpu=162s, gpu=31s). Increasing the number of layers (without changing nodes per layer) results in cpu being faster, even with 10 hidden layers.

&amp;#x200B;

Is this surprising? What with all the hype around ML with GPUs, I expected it to be way quicker even with relatively few nodes per layer. Is there something wrong with my setup or do you only feel the benefit of the GPU's parallel computing if you're using layers with large numbers of nodes?",10,1,False,self,,,,,
858,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,3,bch0d4,self.MachineLearning,ai is creating images from descriptions,https://www.reddit.com/r/MachineLearning/comments/bch0d4/ai_is_creating_images_from_descriptions/,loopy_fun,1555093678,[removed],0,1,False,self,,,,,
859,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,3,bch21f,self.MachineLearning,Are there any open source datasets for benchmarking financial services?,https://www.reddit.com/r/MachineLearning/comments/bch21f/are_there_any_open_source_datasets_for/,ejayshun,1555093906,[removed],0,1,False,self,,,,,
860,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,3,bch54i,self.MachineLearning,How it feels when your watching your model train.,https://www.reddit.com/r/MachineLearning/comments/bch54i/how_it_feels_when_your_watching_your_model_train/,doorshop254,1555094309,[removed],0,1,False,self,,,,,
861,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,3,bch8da,reddit.com,ML Algorithm Used to Construct the First Picture of Blackhole - possible algorithm bias is toward doughnut shape?,https://www.reddit.com/r/MachineLearning/comments/bch8da/ml_algorithm_used_to_construct_the_first_picture/,CTXCBlockchain,1555094742,,0,1,False,default,,,,,
862,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,3,bchbnl,self.MachineLearning,[D] Kaldi or not Kaldi?,https://www.reddit.com/r/MachineLearning/comments/bchbnl/d_kaldi_or_not_kaldi/,thisisareallife,1555095202,"Hi, I'm fairly familiar with audio processing and deep learning but not deeply experienced with speech/ASR/Kaldi. I might need to work on ASR for speech data analysis (e.g., topic modelling etc) as an application, not as research (it's a company project). Should I Kaldi or not? I'm hesitating to say 'yes' to myself because my shallow impression is that it'd take me quite a while to learn how to use, but I'm not sure speech will be my very topic in the future.

&amp;#x200B;

FYI, if these are relevant, I don't think we have lots of annotated dataset by ourselves, so I probably should rely on public datasets. At this moment there's no demand for multiple languages.",10,8,False,self,,,,,
863,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,4,bchrz1,self.MachineLearning,[Discussion/Project][X-post-AskStatistics] Looking for a specific procedure,https://www.reddit.com/r/MachineLearning/comments/bchrz1/discussionprojectxpostaskstatistics_looking_for_a/,vossva,1555097545,[removed],0,1,False,self,,,,,
864,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,4,bchup0,self.GeneticProgramming,Genetic solving of deep learning,https://www.reddit.com/r/MachineLearning/comments/bchup0/genetic_solving_of_deep_learning/,ToolTechSoftware,1555097948,,0,1,False,https://b.thumbs.redditmedia.com/GRu_Uu6-lUVZ3XxZsrZ5xkezBnq-HFPndHkFxT5buDI.jpg,,,,,
865,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,5,bci49m,self.MachineLearning,How does one add parameters dynamically in Pytorch and have training work correctly?,https://www.reddit.com/r/MachineLearning/comments/bci49m/how_does_one_add_parameters_dynamically_in/,brandojazz,1555099343,[removed],0,1,False,self,,,,,
866,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,5,bci4v5,self.MachineLearning,What are some simple applications of Graph Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/bci4v5/what_are_some_simple_applications_of_graph_neural/,jsonathan,1555099431,[removed],0,1,False,self,,,,,
867,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,5,bci8bj,self.MachineLearning,[D] State of the art optimizers,https://www.reddit.com/r/MachineLearning/comments/bci8bj/d_state_of_the_art_optimizers/,doctorjuice,1555099923,"I wasn't sure if there is a consensus on this. Of course, there is widespread use of SGD with momentum, Adam, RMSProp, Adagrad, Adadelta, and probably others -- but is there an optimizer that is considered SOTA for DNNs ""most of the time""? Or is it basically accepted that there is a collection of ""good"" optimizers whose efficacy varies depending on the task and architecture?",11,6,False,self,,,,,
868,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,6,bciut0,self.MachineLearning,State-of-the-art multilabel classification algorithm?,https://www.reddit.com/r/MachineLearning/comments/bciut0/stateoftheart_multilabel_classification_algorithm/,peroquepas923,1555103269,[removed],0,1,False,self,,,,,
869,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,6,bciycz,self.MachineLearning,"Please explain this ""simple"" math concept to me from Michael Nelson's book.",https://www.reddit.com/r/MachineLearning/comments/bciycz/please_explain_this_simple_math_concept_to_me/,caesarsalad44,1555103811,[removed],0,1,False,https://b.thumbs.redditmedia.com/G_06OB1Va9gUKexRvMuuCrGKekcVaG1dMZGGb_cN-1g.jpg,,,,,
870,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,6,bcizoi,self.MachineLearning,Should I always put the better GPU in PCIex16 Slot1 to ensure the performance?,https://www.reddit.com/r/MachineLearning/comments/bcizoi/should_i_always_put_the_better_gpu_in_pciex16/,DDGao,1555104016,[removed],0,1,False,self,,,,,
871,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,6,bcj2c3,youtu.be,"[P] This is CS50 Live with Nick Wong, creating a k-means classifier in Python from scratch, the first part in our new series on machine learning and neural networks.",https://www.reddit.com/r/MachineLearning/comments/bcj2c3/p_this_is_cs50_live_with_nick_wong_creating_a/,coltonoscopy,1555104420,,0,1,False,default,,,,,
872,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,6,bcjcy0,self.MachineLearning,Use deep learning to make art !,https://www.reddit.com/r/MachineLearning/comments/bcjcy0/use_deep_learning_to_make_art/,hrichard123,1555106068,"I made this playlist about deep learning and arts. Please tell me what you think about it !

https://beta.flow-app.io/?path_id=15#/path?section=Overview&amp;modelHead=path_15",0,2,False,self,,,,,
873,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,6,bcjehu,self.MachineLearning,vectorizing function without using numpy.vectorize(),https://www.reddit.com/r/MachineLearning/comments/bcjehu/vectorizing_function_without_using_numpyvectorize/,Ikuyas,1555106311,"It is rather straightforward to create my own vectorize(f) if f takes only one argument and returns one argument, y = f(x). I got stuck in doing the same for the multivariate function or the function that takes a vector as an argument, and resorted to the numpy.vectorize(). But, their implementation is kind of unintuitive.

For example, in case of f(x1, x2), ```numpy.vectorize()``` makes you pass f(vector_for_x1, vector_for_x2) rather than f([(x1_1, x2_1), (x1_2, x2_2),..., (x1_n, x2_n)]), and want to implement my vectorizing style for the purpose of the implementation.

I figured that from ```inspect import signature``` could be helpful but cannot get through. Do you guys have any ideas?",0,1,False,self,,,,,
874,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,7,bcjhds,beta.flow-app.io,Deep learning and arts: I gathered these contents and quizzes tell me what you think about it !,https://www.reddit.com/r/MachineLearning/comments/bcjhds/deep_learning_and_arts_i_gathered_these_contents/,hrichard123,1555106741,,1,5,False,default,,,,,
875,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,7,bcjj5j,self.MachineLearning,"[D] Looking for TF implementation of ""PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS""",https://www.reddit.com/r/MachineLearning/comments/bcjj5j/d_looking_for_tf_implementation_of_pay_less/,farmingvillein,1555107027,"Ref. https://openreview.net/pdf?id=SkVhlh09tX and https://github.com/pytorch/fairseq/blob/master/examples/pay_less_attention_paper/README.md.

Looking for a TF implementation of LightConv and/or DynamicConv (at least the layers, don't need full model/repro)--is anyone aware of one?

Exists well-documented state in Fairseq's repo (https://github.com/pytorch/fairseq/blob/master/fairseq/models/lightconv.py being the core).  I could re-implement in TF, but there are clearly some tricks-of-the-trade which make it a little hairy, so would rather not if could avoid (see both pytorch and paper commentary: ""Implementation. Existing CUDA primitives for convolutions did not perform very well to implement LightConv and we found the following solution faster on short sequences: We copy and expand the normalized weights... We then reshape and transpose the inputs ... and perform a batch matrix multiplication to get the outputs. We expect a dedicated CUDA kernel to be much more efficient."").

Plus, last I checked, depthwise conv support has been a little wacky in tf (cf. https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+depthwise), although perhaps those have been cleaned up?

Thanks...",0,1,False,self,,,,,
876,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,7,bcjkhb,self.MachineLearning,Neural Network models for telling if two voices are saying the same thing?,https://www.reddit.com/r/MachineLearning/comments/bcjkhb/neural_network_models_for_telling_if_two_voices/,woventextfreedomguy,1555107229,[removed],0,1,False,self,,,,,
877,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,7,bcjn64,beta.flow-app.io,"Deep learning and arts, I gathered these contents and quizzes, tell me what you think about it!",https://www.reddit.com/r/MachineLearning/comments/bcjn64/deep_learning_and_arts_i_gathered_these_contents/,terabapt,1555107659,,4,3,False,default,,,,,
878,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,7,bcjr2i,beta.flow-app.io,"[D] Deep learning and arts, I gathered these contents and quizzes, tell me what you think about it",https://www.reddit.com/r/MachineLearning/comments/bcjr2i/d_deep_learning_and_arts_i_gathered_these/,terabapt,1555108275,,3,1,False,https://b.thumbs.redditmedia.com/vRmkXvYl7G8txV5y8a6O62GbXZ-9TsGoDv8bWsUjB3E.jpg,,,,,
879,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,7,bcjut8,self.MachineLearning,"[D] Deep learning and arts, I gathered these contents and quizzes, tell me what you think!",https://www.reddit.com/r/MachineLearning/comments/bcjut8/d_deep_learning_and_arts_i_gathered_these/,terabapt,1555108876,"These contents sum up recent work about extracting style and content features from images. They can be successfully used to generate art pieces! 

https://beta.flow-app.io/?path_id=15#/path?section=Overview&amp;modelHead=path_15",10,16,False,self,,,,,
880,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,7,bcjvpx,self.MachineLearning,[P] Framework for medical image segmentation with tools and models off-the-shelf,https://www.reddit.com/r/MachineLearning/comments/bcjvpx/p_framework_for_medical_image_segmentation_with/,DifficultDifficulty,1555109017,"Hi all,

At my university, I am taking a grad course in which we're select a deep learning project and work on it by the end of the course.I had previously developed a set of tools for my [M.Sc](https://M.Sc) project where I need to manipulate sets of medical images in different formats (DICOM, Niftii, Nrrd), pre-process them using SimpleITK and feed them into a deep learning pipeline.I figured I could execute that course project using those tools and put it up on github for everyone to use and for me to showcase my skills, as I will finish school very soon and that will give me some visibility for my upcoming job search.

The platform relies on visdom for visualization capabilities, I had implemented functions that allow you to visualize your network's computation graph, the experiment options and hyperparameters, your loss and accuracy curves, the gradient flow graphs through your networks as well as histograms for weight distribution in your layers. The python classes offer different functions to easily extend functionality where you can sample images during training if you're working with images and display them on the browser.

The repository comes with implementations of UNets and ResNets as well as many GAN loss functions such as Wasserstein GAN with gradient penalty. The same class can also be set in non-GAN mode so that it uses a simple cross-entropy loss function.

I had used this code to segment vertebrae from MRIs, reaching a 0.87 dice coefficient.

The code is generic enough to be used for tasks other than image segmentation if you wish to play around with it.

I hope this will be helpful to you.

Please find it in the link down below:

[https://github.com/Roulbac/GanSeg](https://github.com/Roulbac/GanSeg)

&amp;#x200B;

Here is an example of segmentation on unseen data:

&amp;#x200B;

https://i.redd.it/f81y3pw57ur21.png",0,1,False,self,,,,,
881,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,8,bck446,blog.paperspace.com,[P] Detecting and Localizing Pneumonia from Chest X-Ray Scans with PyTorch,https://www.reddit.com/r/MachineLearning/comments/bck446/p_detecting_and_localizing_pneumonia_from_chest/,coffeepants87,1555110389,,0,1,False,default,,,,,
882,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,9,bcknyc,self.MachineLearning,[D] What other machine learning competitions are there besides Kaggle ?,https://www.reddit.com/r/MachineLearning/comments/bcknyc/d_what_other_machine_learning_competitions_are/,DisastrousProgrammer,1555113869,,18,39,False,self,,,,,
883,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,10,bcli12,self.MachineLearning,What jobs excite you?,https://www.reddit.com/r/MachineLearning/comments/bcli12/what_jobs_excite_you/,errminator,1555119328,"Looking to maybe switch to this field and interested in knowing more about the opportunities that are out there?

I'm someone who likes to feel that they're making a difference in the world. I know machine learning can play a role in medicine but, realistically, how easy are those jobs to come by?

What excites you in the current job market?

I'm UK based if that helps.",0,1,False,self,,,,,
884,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,10,bclojg,self.MachineLearning,[D] Reproducibility in ML research and development,https://www.reddit.com/r/MachineLearning/comments/bclojg/d_reproducibility_in_ml_research_and_development/,harry_comp_16,1555120584,"There has been a push to have more reproducible code that one submits to conferences like ICML, NeurIPS, etc given the call that was made for this at NeurIPS 2018 and from the wider ML community. And in general in working on projects that span several months and different developer and data science teams. 

Ive looked into tools like Pachyderm https://www.pachyderm.io/ and DVC https://dvc.org/ 
(Ive found them to be a bit heavyweight in terms of setup, especially for some of my colleagues who come from more of a research and less of a software engineering background/experience)

Are there any other tools that you use to achieve this for your research and development? Would be great to mention pros and cons of each.",9,4,False,self,,,,,
885,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,10,bclppd,self.MachineLearning,Are all cpu cores used in forwarding? (No gpu),https://www.reddit.com/r/MachineLearning/comments/bclppd/are_all_cpu_cores_used_in_forwarding_no_gpu/,dascsad,1555120799,[removed],0,1,False,self,,,,,
886,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,11,bcm681,self.MachineLearning,[D]My Machine Learning Journal #10: First time doing reinforcement learning and beating atari breakout with it,https://www.reddit.com/r/MachineLearning/comments/bcm681/dmy_machine_learning_journal_10_first_time_doing/,RedditAcy,1555124025,"I have been inconsistent with my journal, but I am back and fresher than ever. 

Vlog version as usual:

[https://youtu.be/dcnGI6x-yk0](https://youtu.be/dcnGI6x-yk0)

Today (and yesterday) I did &amp; learned:

RL seems to have a lot of exploration going on vs some other ML tasks. One popular application it has is definitely beating videogames. The [Mario AI](https://www.youtube.com/watch?v=qv6UVOQ0F44&amp;t=1s) was a viral hit in 2015. I decided to build a RL model that can beat atari breakout. This was soon classified as impossible given my current coding skills, so I chose to implement a [medium article](https://becominghuman.ai/lets-build-an-atari-ai-part-0-intro-to-rl-9b2c5336e0ec) first that beat atari breakout. This article was great at linking the original Atari breakout RL paper with the code, but the full code was not posted, so I was stuck. Luckily, a user named boyuanf hit us up with the tensorflow implementation of the article on medium, here's the forked version of [it](https://github.com/BlastWind/DeepQLearning). 

I downloaded the trained weights and model, and I ran it after installing openAI gym in conda with pip. Unfortunately, atari-py seems incompatible with windows 10, so I had to go through a very annoying process to finally come through with this easy line of code to solve the problem: 

    pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py

Yea it is just one of those problems man. 

Anyways, I then was able to run **gym** and see the beautiful pre-trained model doing work, it got to a pretty good high score, I think 57 or something. 

It is actually **after I implemented the project that I come back to reading the papers**, this works for me. I usually try to guess what the original algorithm does by doing a project first. For me, doing a project first then reading the paper also gives that revelation of: ""oh, the reason that I have this line in the code is because of that sentence in the paper"".

The [paper](https://arxiv.org/abs/1312.5602) and this [medium article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe) helped my understanding a lot. This pseudocode in the paper opened the doors for me: 

&amp;#x200B;

https://i.redd.it/6bpl6f203yr21.png

I'm going to try to explain this pseudocode with even English-er language. We will input the current frame and a few previous frames to our RL model. The RL model will interpret these inputs as the **state**, and it will either choose the **action** based on the **Q-table** or choose a random action. We can imagine that as the model gets more advanced, we will choose less random actions to let the model learn, but in the early stages, when the model has no idea what to do, we probably want to let it explore randomly, we will use a decreasing **epsilon** value to model this. The emulator will receive the action chosen by the RL model, run that action, then display the new image and return the reward. The **Q-table** will be updated based on this reward. The **Q-table** is just a table that has states mapping to potential actions. When the model is complex and epsilon is low, the RL model chooses actions based on the **Q-table,** a higher value (which means high rewards) in the state mapping to action will probably mean the model is choosing that. 

That;s it for this one, I learned a lot since it was my first time exploring RL! Exciting, can't wait to do more.",21,153,False,https://b.thumbs.redditmedia.com/YlBO6XhDE0WXtc4TC0QmMaiS3FJotTxIFCKmGXmOcPE.jpg,,,,,
887,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,14,bcn5wj,self.MachineLearning,[D] What do people think of Vowpal Wabbit?,https://www.reddit.com/r/MachineLearning/comments/bcn5wj/d_what_do_people_think_of_vowpal_wabbit/,__gh,1555131736,"Has anybody here used it at work/for a project? If so, what for? What advantages does VW have over other frameworks (e.g. PyTorch or scikit-learn), and do those advantages justify the steeper learning curve?

VW's speed and benchmarks look very attractive, but I'm not sure what tasks it would be the right tool for.

[https://github.com/VowpalWabbit/vowpal\_wabbit](https://github.com/VowpalWabbit/vowpal_wabbit)",16,42,False,self,,,,,
888,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,14,bcnd0b,self.MachineLearning,EUROPEAN COMMISSION ETHICS GUIDELINES FOR TRUSTWORTHY AI,https://www.reddit.com/r/MachineLearning/comments/bcnd0b/european_commission_ethics_guidelines_for/,ethicalbau,1555133413,[removed],0,1,False,self,,,,,
889,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,15,bcnpif,youtu.be,OpenAI Five vs OG Dota 2 Showmatch | Sujoy's plea to support humanity,https://www.reddit.com/r/MachineLearning/comments/bcnpif/openai_five_vs_og_dota_2_showmatch_sujoys_plea_to/,Maff17,1555136523,,0,1,False,https://b.thumbs.redditmedia.com/FDxeFQ11hw3IXLI6t1WhBeGxZbgGqGUV4ux05cljI2k.jpg,,,,,
890,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,15,bcnven,self.MachineLearning,LHD S.p.A. Develops Customized Stacker Cranes,https://www.reddit.com/r/MachineLearning/comments/bcnven/lhd_spa_develops_customized_stacker_cranes/,lhd121,1555138049,[removed],0,1,False,self,,,,,
891,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,17,bcolz2,link.medium.com,check out my story on Medium for easy and practical explanation of SVD. Any feedback is welcome.,https://www.reddit.com/r/MachineLearning/comments/bcolz2/check_out_my_story_on_medium_for_easy_and/,fmpatel,1555145563,,0,1,False,default,,,,,
892,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,18,bcow89,self.MachineLearning,Deep Learning PhD,https://www.reddit.com/r/MachineLearning/comments/bcow89/deep_learning_phd/,NikolasTs,1555148469,[removed],0,1,False,self,,,,,
893,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,20,bcpgul,self.MachineLearning,Empirical advice for ML topic needed,https://www.reddit.com/r/MachineLearning/comments/bcpgul/empirical_advice_for_ml_topic_needed/,michifuego,1555153992,[removed],0,1,False,self,,,,,
894,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,21,bcpssf,self.MachineLearning,"Hi, anyone he can explain to me how to make 2 chatbot talk to each other?",https://www.reddit.com/r/MachineLearning/comments/bcpssf/hi_anyone_he_can_explain_to_me_how_to_make_2/,dennypool2,1555156953,"i want to make 2 chatbot talk to each other, for example cleverbot talking with another cleverbot or another chatbot, anyone know how make it? or can link me a guide,tutorial? thanks",0,1,False,self,,,,,
895,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,21,bcpxil,self.MachineLearning,Can AI detect if a pictures is of a dog?,https://www.reddit.com/r/MachineLearning/comments/bcpxil/can_ai_detect_if_a_pictures_is_of_a_dog/,ktynvmby,1555158026,[removed],0,1,False,self,,,,,
896,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,21,bcq3h8,self.MachineLearning,Dense matrix x Sparse Matrix multiplication C++?,https://www.reddit.com/r/MachineLearning/comments/bcq3h8/dense_matrix_x_sparse_matrix_multiplication_c/,nt1tov,1555159343,Is there highly optimized algorithms and c++ libraries for Dense matrix x Sparse Matrix multiplication?,0,1,False,self,,,,,
897,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,21,bcq4ie,self.MachineLearning,How to develop a Multimodal Search engine?,https://www.reddit.com/r/MachineLearning/comments/bcq4ie/how_to_develop_a_multimodal_search_engine/,sandeep__007,1555159557,[removed],0,1,False,self,,,,,
898,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,21,bcq4mo,/r/MachineLearning/comments/bcq4mo/cnc_simulator_on_ios_and_android/,cnc simulator on ios and android,https://www.reddit.com/r/MachineLearning/comments/bcq4mo/cnc_simulator_on_ios_and_android/,armansik,1555159580,,1,2,False,default,,,,,
899,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,22,bcqc6o,self.MachineLearning,Good resource for GPU tuning and OC?,https://www.reddit.com/r/MachineLearning/comments/bcqc6o/good_resource_for_gpu_tuning_and_oc/,SmugEskim0,1555161141,[removed],0,1,False,self,,,,,
900,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,22,bcqjau,self.MachineLearning,[D] 12 Free And MindBlowing Machine Learning Course That Will Make You An ML Champion,https://www.reddit.com/r/MachineLearning/comments/bcqjau/d_12_free_and_mindblowing_machine_learning_course/,navin49,1555162550,"Hello Friends 

Here today I'll show some of the [best machine learning course](https://techgrabyte.com/best-machine-learning-course-free/) that will not only save your money but it will also offer you a quality education. 

This all[machine learning courses](https://techgrabyte.com/best-machine-learning-course-free/) are perfectly designed and divided into a perfect curriculumthat will take you from an absolute beginner to an expert in Machine Learning, who knows how to build real word projects. 

From the listed machine learning course,you will learn the basics and fundamentals of Machine Learning. This courses will teach you how ML works internally, how to train a model and also how to implement the knowledge that you will gain.

[Here is the list of all Machine Learning courses along with course enrollment link.](https://techgrabyte.com/best-machine-learning-course-free/)

*Friend, in your opinion, how a beginner or a mid-level person can become an expert in Machine Learning.*  

*Please share your view, don't hesitate to share your knowledge.*",1,0,False,self,,,,,
901,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,22,bcqn74,self.MachineLearning,How does noise affect a CNN architecture outcome?,https://www.reddit.com/r/MachineLearning/comments/bcqn74/how_does_noise_affect_a_cnn_architecture_outcome/,MasterSama,1555163309,[removed],0,1,False,self,,,,,
902,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,23,bcquea,self.MachineLearning,"What do Anthos, AutoML and the likes mean for ML engineers and Computer Scientists?",https://www.reddit.com/r/MachineLearning/comments/bcquea/what_do_anthos_automl_and_the_likes_mean_for_ml/,vnjogani,1555164628,[removed],0,1,False,self,,,,,
903,MachineLearning,t5_2r3gv,2019-4-13,2019,4,13,23,bcrceb,self.MachineLearning,[R] Deezer and Facebook networks for node classification/community detection,https://www.reddit.com/r/MachineLearning/comments/bcrceb/r_deezer_and_facebook_networks_for_node/,benitorosenberg,1555167582,"Datasets collected for network science and machine learning research.

[https://github.com/benedekrozemberczki/datasets/](https://github.com/benedekrozemberczki/datasets/)",2,1,False,self,,,,,
904,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,0,bcrhqq,self.MachineLearning,Andrew NG's ML course has 2 million students now,https://www.reddit.com/r/MachineLearning/comments/bcrhqq/andrew_ngs_ml_course_has_2_million_students_now/,iphone6plususer,1555168443,[removed],0,1,False,self,,,,,
905,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,1,bcs627,self.MachineLearning,New Video on AI,https://www.reddit.com/r/MachineLearning/comments/bcs627/new_video_on_ai/,e-tristea,1555172339,[removed],0,1,False,self,,,,,
906,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,1,bcs6xm,self.MachineLearning,[D] What are the best embeddings to use when you have a lot of OOV in production?,https://www.reddit.com/r/MachineLearning/comments/bcs6xm/d_what_are_the_best_embeddings_to_use_when_you/,JClub,1555172481,"Context: Imagine that you are not dealing with sentences, only solo words. Therefore methods like FLAIR/FastText may not be very appropriated to pretrain with your corpora.",22,10,False,self,,,,,
907,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,1,bcsipi,self.MachineLearning,"How to understand ""chicken"" is something related to ""supermarket"", but not related to say ""synagogue"" or ""pharmacy"" [P]",https://www.reddit.com/r/MachineLearning/comments/bcsipi/how_to_understand_chicken_is_something_related_to/,orkalp,1555174360,"Assume that we have an input string""I need to buy some chicken"". After working a bit on this string, suppose that we've reduced it to""buy chicken""

My question is, how can weunderstandthat chicken is something related tocafeorsupermarket, but not related tolocksmithorpost office.

More specifically, I have n number of point of interest types and I am trying to come up with n probabilities p_1, p_2, ..., p_n where each probability represents the likelihood (or meaningfulness) of string-type pairs.

My ultimate goal is to have an unequality containing these n probabilities, which should of course bemeaningful

I want to have

p(chicken, synagogue) &lt; p(chicken, supermarket)

But not

p(chicken, train_station) &gt; p(chicken, caf)

I have tried to do google searches and determine these probabilities according to the number of results but it wasn't satisfying at all.

For example, when I searched ""chicken breast EMBASSY"": I got 24,500,000 results. For ""chicken breast SUPERMARKET"", number of results was 11,600,000.

If we compute the probabilities by only taking these numbers into account, we'd arrive at a conclusion where p(chicken, supermarket) &lt; p(chicken, embassy) which would of course be wrong.

Do you have any suggestions on how to approach this problem?",20,27,False,self,,,,,
908,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,2,bcslq9,github.com,[R] Automatic Adaptation of Object Detectors to New Domains using Self-training,https://www.reddit.com/r/MachineLearning/comments/bcslq9/r_automatic_adaptation_of_object_detectors_to_new/,AruniRC,1555174827,,0,1,False,default,,,,,
909,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,2,bcsqat,self.MachineLearning,[R] Automatic Adaptation of Object Detectors to new domains using Self-training.,https://www.reddit.com/r/MachineLearning/comments/bcsqat/r_automatic_adaptation_of_object_detectors_to_new/,AruniRC,1555175524,"**Github:** [https://github.com/AruniRC/detectron-self-train](https://github.com/AruniRC/detectron-self-train)

**Project page:** [http://vis-www.cs.umass.edu/unsupVideo/](http://vis-www.cs.umass.edu/unsupVideo/)

&amp;#x200B;

**Abstract.** This work addresses the unsupervised adaptation of an existing object  detector to a new target domain. We assume that a large number of  unlabeled videos from this domain are readily available. We  automatically obtain labels on the target data by using high-confidence  detections from the existing detector, augmented with hard  (misclassified) examples acquired by exploiting temporal cues using a  tracker. These automatically-obtained labels are then used for  re-training the original model. A modified knowledge distillation loss  is proposed, and we investigate several ways of assigning soft-labels to  the training examples from the target domain.    Our approach is empirically evaluated on challenging face and  pedestrian detection tasks: a face detector trained on WIDER-Face, which  consists of high-quality images crawled from the web, is adapted to a  large-scale surveillance  data set; a pedestrian detector trained on  clear, daytime images from the BDD-100K driving data set is adapted to  all other scenarios such as rainy, foggy, night-time. Our results  demonstrate the usefulness of incorporating hard examples obtained from  tracking, the advantage of using soft-labels via distillation loss  versus hard-labels, and show promising performance as a simple method  for unsupervised domain adaptation of object detectors, with minimal  dependence on hyper-parameters.",0,61,False,self,,,,,
910,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,3,bctaa0,self.MachineLearning,Generation of new photos of a learned face,https://www.reddit.com/r/MachineLearning/comments/bctaa0/generation_of_new_photos_of_a_learned_face/,Santikaye,1555178670,[removed],0,1,False,self,,,,,
911,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,3,bctdk7,self.MachineLearning,INSTANT POT DUO PLUS 9-IN-1 REVIEW,https://www.reddit.com/r/MachineLearning/comments/bctdk7/instant_pot_duo_plus_9in1_review/,Recipes4Healthy,1555179153,[removed],0,1,False,self,,,,,
912,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,3,bctv5y,self.nvidia,[D] DGX-1 vs supermicro clones? (cross-post from r/nvidia),https://www.reddit.com/r/MachineLearning/comments/bctv5y/d_dgx1_vs_supermicro_clones_crosspost_from_rnvidia/,ksarma,1555181848,,0,1,False,default,,,,,
913,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,4,bcu5ok,self.MachineLearning,[D] Happy easter! What are some common stereotypes about us?,https://www.reddit.com/r/MachineLearning/comments/bcu5ok/d_happy_easter_what_are_some_common_stereotypes/,Naveos,1555183372,"Just got home from drinking and we were making fun of some common stereotypes that my friends exhibit from their careers, such as the doctor dude having unintelligible handwriting. Honestly though, I couldn't think of any stereotypes among us folks that work with machine learning - whether data science, research, engineer, whatever.

So, what are common stereotypes that I seem to be blissfully unaware of?

P.S: I'm still a bit tipsy, so apologies if this is inappropriate to ask here.",1,0,False,self,,,,,
914,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,5,bcumrs,self.MachineLearning,[D] OpenAI 5 vs DOTA 2 World Champions happening now!,https://www.reddit.com/r/MachineLearning/comments/bcumrs/d_openai_5_vs_dota_2_world_champions_happening_now/,zergylord,1555185994,"First game is over, but I won't spoil it :)  


livestream: [https://www.twitch.tv/openai](https://www.twitch.tv/openai)",42,239,False,self,,,,,
915,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,5,bcunzn,self.MachineLearning,GPT-2 Idea,https://www.reddit.com/r/MachineLearning/comments/bcunzn/gpt2_idea/,unflappableblatherer,1555186195,"Hey y'all, I'm pretty much a complete novice when it comes to machine learning, but I had what seemed to me like an interesting idea. Don't hesitate to dismiss this as the hunch of a layman if that is what it seems to be in your more educated opinions.

&amp;#x200B;

So, I've been screwing around with GPT-2-Lite recently just for fun. Something I noticed is that one skill the model has 100% on lock is always closing parentheses (Yes, I'm anthropomorphizing lol). Further, it seems to have a pretty good sense of how parentheticals relate to preceding text -- i.e. they tend to contain elaboration or qualification on the immediately preceding topic, or a digression that somewhat interrupts the flow of the text. You can get this behavior on demand by feeding the model a prompt that ends with an open paren. For example, both parentheticals in this sentence from the Gettysburg address were generated that way:

&amp;#x200B;

&gt;It is for us the living  ***(who, in fact are in perpetual combat here, and who are still here)***, rather, to be dedicated here to the unfinished work ***(and for that work to be paid)*** which they who fought here have thus far so nobly advanced.

&amp;#x200B;

So, I guess it must have picked up on the characteristics of phrases following the '(' character in the training set. Same would apply to the open quote character, em-dash, and function words like ""which"" that head clauses with certain predictable characteristics. My thought is that there may be other semantic features in the training set which could be detected programmatically but are not currently delimited by such explicit and compact cues. Would you expect there to be any value in augmenting the training data with some made up cues for those features? E.g., you could trawl through the  text and insert '|' characters bounding every sentence that meets some sentiment analysis criterion (maybe extreme negative sentiment); or you could do the same for compound or complex sentences. What do you reckon would happen if you fed the resulting trained model some text that ends with the opening delimiter? Would that give you a way to reliably elicit completions that have the target semantic feature? I don't have the hardware or know-how to do the training part, so I can't test this myself.",0,1,False,self,,,,,
916,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,5,bcv4cp,self.MachineLearning,[D] GPT-2 Training Data Augmentation,https://www.reddit.com/r/MachineLearning/comments/bcv4cp/d_gpt2_training_data_augmentation/,unflappableblatherer,1555188742," Hey y'all, I'm pretty much a complete novice when it comes to machine learning, but I had what seemed to me like an interesting idea. Don't hesitate to dismiss this as the hunch of a layman if that is what it seems to be in your more educated opinions.

So, I've been screwing around with GPT-2-Lite recently just for fun. Something I noticed is that one skill the model has 100% on lock is always closing parentheses (Yes, I'm anthropomorphizing lol). Further, it seems to have a pretty good sense of how parentheticals relate to preceding text -- i.e. they tend to contain elaboration or qualification on the immediately preceding topic, or a digression that somewhat interrupts the flow of the text. You can get this behavior on demand by feeding the model a prompt that ends with an open paren. For example, both parentheticals in this sentence from the Gettysburg address were generated that way:

&gt;It is for us the living ***(who, in fact are in perpetual combat here, and who are still here)***, rather, to be dedicated here to the unfinished work ***(and for that work to be paid)*** which they who fought here have thus far so nobly advanced.

So, I guess it must have picked up on the characteristics of phrases following the '(' character in the training set. Same would apply to the open quote character, em-dash, and function words like ""which"" that head clauses with certain predictable characteristics. My thought is that there may be other semantic features in the training set which could be detected programmatically but are not currently delimited by such explicit and compact cues. Would you expect there to be any value in augmenting the training data with some made up cues for those features? E.g., you could trawl through the text and insert '|' characters bounding every sentence that meets some sentiment analysis criterion (maybe extreme negative sentiment); or you could do the same for compound or complex sentences. What do you reckon would happen if you fed the resulting trained model some text that ends with the opening delimiter? Would that give you a way to reliably elicit completions that have the target semantic feature? I don't have the hardware or know-how to do the training part, so I can't test this myself.",3,3,False,self,,,,,
917,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,5,bcv4rf,self.MachineLearning,[D] Visualizing neural networks,https://www.reddit.com/r/MachineLearning/comments/bcv4rf/d_visualizing_neural_networks/,Radeusgd,1555188800,"x-posting from /r/tensorflow

Hi! I'm working on Tensorflow bindings for Luna (http://luna-lang.org/). 

We allow you to build ML models by connecting visual components together  every component can define a new network layer and its dependencies. The API is highly inspired by Keras functional API.

[Here you can find an example of what it looks like](https://imgur.com/a/rmcZiZv)

Luna has the ability to display visualizations below its components, so you could inspect the look of your network on each step (after adding the first layer, adding the second layer, etc). We want to provide interactive visualizations of the network you've built so far. I'd love to ask you what visualizations you would find the most helpful during building neural networks? 

We were initially thinking about something [like that](http://www.mghpcc.org/neural-networks-earthquakes/neuralnetworkforhelen/)  so you could see the structure of your network, the weights and activation functions, but we are very open for discussion here. We want to create something that would be helpful while building various kinds of networks.

Which features do you think are most important to visualize? The weights? The activations on each layer? Something else entirely?",4,16,False,self,,,,,
918,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,5,bcv6ot,self.MachineLearning,Twitter data extraction,https://www.reddit.com/r/MachineLearning/comments/bcv6ot/twitter_data_extraction/,Ahmii_Faraz,1555189118,[removed],0,1,False,self,,,,,
919,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,7,bcw35b,self.MachineLearning,ShogunML toolkit documentation for python wanted,https://www.reddit.com/r/MachineLearning/comments/bcw35b/shogunml_toolkit_documentation_for_python_wanted/,przemyslowiec,1555194493,[removed],0,1,False,self,,,,,
920,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,7,bcw3rt,self.MachineLearning,Web Content Extraction,https://www.reddit.com/r/MachineLearning/comments/bcw3rt/web_content_extraction/,MohamedHmini,1555194604,[removed],0,1,False,self,,,,,
921,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,9,bcxd11,self.MachineLearning,Review math course on dataquest,https://www.reddit.com/r/MachineLearning/comments/bcxd11/review_math_course_on_dataquest/,author31,1555202536,[removed],0,1,False,self,,,,,
922,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,10,bcxmnu,self.MachineLearning,How do you organize/retain all the background information in your research area?,https://www.reddit.com/r/MachineLearning/comments/bcxmnu/how_do_you_organizeretain_all_the_background/,ilia10000,1555204369,[removed],0,1,False,self,,,,,
923,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,10,bcxpv1,self.MachineLearning,[D] How do you organize/retain all the background information in your research area?,https://www.reddit.com/r/MachineLearning/comments/bcxpv1/d_how_do_you_organizeretain_all_the_background/,ilia10000,1555204973,"I'm starting to do research in an area of ML that is new to me. After a few weeks of digging and reading, I've identified around 20-30 important papers in this area. I'm trying to find the most efficient way (in terms of speed vs knowledge retained) to read, understand, and retain these papers.

Does anyone here have any strategies/templates they use in these circumstances?

So far, I'm thinking:

\- 1 markdown/latex file per paper with abstract + my own bullet point notes

\- A mindmap or some similar visualization that connects all the papers

\- Storing this along with the pdfs of the papers as a git repo to make navigation easy

&amp;#x200B;

Looking for both reading/understanding strategies as well as summarization/organization ones.",10,4,False,self,,,,,
924,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,14,bczjbv,medium.com,Humans Call GG! OpenAI Five Bots Beat Top Pros OG in Dota 2,https://www.reddit.com/r/MachineLearning/comments/bczjbv/humans_call_gg_openai_five_bots_beat_top_pros_og/,gwen0927,1555218833,,0,1,False,https://b.thumbs.redditmedia.com/dmV_iM3AdSZXLiTPvEo1vCJPh1a1EMVp3T_wQL3FLmA.jpg,,,,,
925,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,14,bczobr,self.MachineLearning,Best path from Mechanical Engineering to Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bczobr/best_path_from_mechanical_engineering_to_machine/,alcos35,1555220059,[removed],0,1,False,self,,,,,
926,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,14,bczr0j,self.MachineLearning,Best path from Mech Engineering to Machine Learning Research Scientist,https://www.reddit.com/r/MachineLearning/comments/bczr0j/best_path_from_mech_engineering_to_machine/,g35s07,1555220687,[removed],0,1,False,self,,,,,
927,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,14,bcztlt,i.redd.it,I didn't know where else to post this (Ganbreeder),https://www.reddit.com/r/MachineLearning/comments/bcztlt/i_didnt_know_where_else_to_post_this_ganbreeder/,minicritman999,1555221319,,0,1,False,https://a.thumbs.redditmedia.com/jXV6Of0Fj-9EG6YH99ssrKWkzR1WsRf0n_VhrMPUcR8.jpg,,,,,
928,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,20,bd1zqn,self.MachineLearning,"Implementing Randomly Wired Neural Networks for Image Recognition, Experiments were performed on CIFAR-10 datasets.",https://www.reddit.com/r/MachineLearning/comments/bd1zqn/implementing_randomly_wired_neural_networks_for/,leaderj1001,1555242672,[removed],0,1,False,self,,,,,
929,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,20,bd20wk,self.MachineLearning,How do I implement this project ? Will AI be required for it ?,https://www.reddit.com/r/MachineLearning/comments/bd20wk/how_do_i_implement_this_project_will_ai_be/,prototypesai,1555242950,[removed],0,1,False,self,,,,,
930,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,21,bd22ne,self.MachineLearning,MSc in Machine Learning in KTH or King's College London,https://www.reddit.com/r/MachineLearning/comments/bd22ne/msc_in_machine_learning_in_kth_or_kings_college/,fernando2393,1555243369,[removed],0,1,False,self,,,,,
931,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,21,bd26o7,self.MachineLearning,[project] Anyone else is playing with reddit comments using ML in cloud? I'm trying to predict assholes judging from their karma from comments.,https://www.reddit.com/r/MachineLearning/comments/bd26o7/project_anyone_else_is_playing_with_reddit/,luoyuke,1555244286,"Fun categorical classification project, I'd call it ass-o-meter.",107,167,False,self,,,,,
932,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,21,bd2a2n,self.MachineLearning,I'm looking for Research Papers that are about predicting XYZ positions,https://www.reddit.com/r/MachineLearning/comments/bd2a2n/im_looking_for_research_papers_that_are_about/,GreatOnion,1555245042,[removed],0,1,False,self,,,,,
933,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,22,bd2nex,self.MachineLearning,HI!C++ deeplearning questions coming in :)!,https://www.reddit.com/r/MachineLearning/comments/bd2nex/hic_deeplearning_questions_coming_in/,Zi6st,1555247884,[removed],0,1,False,self,,,,,
934,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,22,bd2of1,self.MachineLearning,Machine Learning - Right algorithm,https://www.reddit.com/r/MachineLearning/comments/bd2of1/machine_learning_right_algorithm/,Sridhar75,1555248065,[removed],0,1,False,self,,,,,
935,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,23,bd35be,self.MachineLearning,[D] Where does fastai as a library stand against PyTorch and TF2+Keras?,https://www.reddit.com/r/MachineLearning/comments/bd35be/d_where_does_fastai_as_a_library_stand_against/,harry_comp_16,1555251279,"Specifically, it'll be great to hear on the differences as they apply to doing research work vs applied work along the lines of ease of use and performance of models?",31,17,False,self,,,,,
936,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,23,bd3830,self.MachineLearning,Cyclical Learning Rate Scheduler With Decay in Pytorch,https://www.reddit.com/r/MachineLearning/comments/bd3830/cyclical_learning_rate_scheduler_with_decay_in/,bluesky314,1555251769,[https://github.com/bluesky314/Cyclical\_LR\_Scheduler\_With\_Decay\_Pytorch](https://github.com/bluesky314/Cyclical_LR_Scheduler_With_Decay_Pytorch),0,1,False,self,,,,,
937,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,23,bd3bdw,self.MachineLearning,Training Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bd3bdw/training_recurrent_neural_networks/,prakhar21,1555252333,[removed],0,1,False,self,,,,,
938,MachineLearning,t5_2r3gv,2019-4-14,2019,4,14,23,bd3fv9,self.MachineLearning,Which companies would you recommend for a data scientist in Europe ?,https://www.reddit.com/r/MachineLearning/comments/bd3fv9/which_companies_would_you_recommend_for_a_data/,textMinier,1555253108,[removed],0,1,False,self,,,,,
939,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,2,bd4xic,self.MachineLearning,thoughts on neural training,https://www.reddit.com/r/MachineLearning/comments/bd4xic/thoughts_on_neural_training/,Schoolunch,1555261720,"kind of a stoney thought so bear with me...  


I have heard a lot about how the human mind learns much faster than neural networks because it takes a neural net 45000 years of DOTA experience to equate 10 years of human experience (probably less since a human isn't playing the game 24/7 but you have to consider that a dedicated player is probably thinking about DOTA all day, having DOTA dreams, etc.).  But I was thinking, is it possible that we are actually receiving significantly more iterations each time?  Could it be possible that we are only perceiving a single outcome, but our brain is training on a Schrdinger's cat type collection of scenarios where we are actually receiving many more iterations in parallel?  Like we only perceive a single decision because we live in 3 dimensions, but our brains are actually receiving a lot more information.  


I think this is probably Velikovsky style analysis, where I'm trying to find a way to justify back propagation by rewriting reality, but wondering if there's any way this could make sense?",0,1,False,self,,,,,
940,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,2,bd4zrl,self.MachineLearning,[D] Knowledge Graphs - How do you build your own?,https://www.reddit.com/r/MachineLearning/comments/bd4zrl/d_knowledge_graphs_how_do_you_build_your_own/,baahalex,1555262073,"I'm very new to Knowledge Graphs and I would like to learn a bit about them. I've done some research and from what I see, the first step would be to generate triplets. Then, the actual graph needs to be trained. Finally, there's the issue of querying and visualizing the graph. Is this interpretation correct?

Are there any python packages that can help with building my own KG? I have not been too successful in finding clear resources, so anything would be helpful. If the approach is built around spaCy, even better!

Use case: I have a decent dataset of news articles and embeddings. Would like to take advantage of the dataset and learn a new skill in the meantime.

&amp;#x200B;

Thanks!",20,65,False,self,,,,,
941,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,2,bd511i,machinelearningmindset.com,What is Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bd511i/what_is_machine_learning/,iramirsina,1555262280,,0,1,False,default,,,,,
942,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,2,bd53k7,reddit.com,Looking for a Good Seq2seq Chatbot Library,https://www.reddit.com/r/MachineLearning/comments/bd53k7/looking_for_a_good_seq2seq_chatbot_library/,ZeroMaxinumXZ,1555262687,,0,1,False,default,,,,,
943,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,2,bd55qb,blog.nanonets.com,[P] A 2019 guide to Human Pose Estimation with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bd55qb/p_a_2019_guide_to_human_pose_estimation_with_deep/,cbsudux,1555263018,,1,1,False,default,,,,,
944,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,3,bd5ows,self.MachineLearning,[D] A 2019 guide to Human Pose Estimation with Deep Learning,https://www.reddit.com/r/MachineLearning/comments/bd5ows/d_a_2019_guide_to_human_pose_estimation_with_deep/,cbsudux,1555265886,"Human Pose estimation is an important problem that has enjoyed the attention of the Computer Vision community for the past few decades and is a crucial step towards understanding people in images and videos. This post covers the basics of Human Pose Estimation (2D) and reviews the literature on this topic.

&amp;#x200B;

Article Link : [https://blog.nanonets.com/human-pose-estimation-2d-guide](https://blog.nanonets.com/human-pose-estimation-2d-guide/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=pose&amp;utm_content=GROUP_NAME)",12,41,False,self,,,,,
945,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,3,bd662s,self.MachineLearning,Why CRISPR is Overrated and Gene Drives are Terrifyingly Powerful | Gabriel Licina,https://www.reddit.com/r/MachineLearning/comments/bd662s/why_crispr_is_overrated_and_gene_drives_are/,The_Syndicate_VC,1555268332,[removed],0,1,False,self,,,,,
946,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,4,bd6udm,self.MachineLearning,Machine Learning on Networks,https://www.reddit.com/r/MachineLearning/comments/bd6udm/machine_learning_on_networks/,throwawaystudentugh,1555271862,"What are some hot topics that involve ML and Networks? I have a reasonable background in ML, and was looking at Computer Networks for a potential domain for interesting problems. I have found learning adaptive bitrate algorithms to be pretty interesting. Are there any other interesting applications people are researching?",0,1,False,self,,,,,
947,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,5,bd7i4q,self.MachineLearning,I need a good free source for learning Computer vision or OpenCV,https://www.reddit.com/r/MachineLearning/comments/bd7i4q/i_need_a_good_free_source_for_learning_computer/,Armin71,1555275459,[removed],0,1,False,self,,,,,
948,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,6,bd7mdj,parzelsec.de,Timing Attacks using Machine Learning (GMM),https://www.reddit.com/r/MachineLearning/comments/bd7mdj/timing_attacks_using_machine_learning_gmm/,fleezenleger,1555276076,,0,1,False,https://b.thumbs.redditmedia.com/nbLB0Js1qBlYvZJWhHJCyEcs1fFEcFwoYSvfi4A0Tko.jpg,,,,,
949,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,6,bd86o6,self.MachineLearning,[D] Serving a model for real time video processing,https://www.reddit.com/r/MachineLearning/comments/bd86o6/d_serving_a_model_for_real_time_video_processing/,derongan,1555279147,"Im working on a project that requires running a webcam feed through a network in near real time. I have a network that works, and I have a media server set up with a plugin that process the images, but I am not sure what the best way to actually use the model is in the media server.

Should I plug the network directly into the plugin? Should the network live elsewhere and the plugin send data to it through some RPC or REST calls? 

Thank you!",2,5,False,self,,,,,
950,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,8,bd91l6,self.MachineLearning,[D] Argumentation using knowledge graphs,https://www.reddit.com/r/MachineLearning/comments/bd91l6/d_argumentation_using_knowledge_graphs/,vakker00,1555284063,"Hi

I'm looking into computational argumentation, e.g. the [IBM Project Debater](https://www.research.ibm.com/artificial-intelligence/project-debater/live/) is a great example. I couldn't find much on using large knowledge bases, e.g. Wikidata, Google Knowledge Graph API and similar to use the information to formulate statements.

Do you know about interesting projects in the field? Also, IBM hasn't published much on their system (they did publish a few papers on subtopics tho), so if you know anything similar that could we amazing.

Thanks",4,1,False,self,,,,,
951,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,8,bd9eec,self.MachineLearning,"[N] Tensorflow 2.0 Hackathon coming up. Also our team could use 1 more person if you're interested. It's an NLP project, and we got some great team members, including an advisor who has published current SoTA ML architectures.",https://www.reddit.com/r/MachineLearning/comments/bd9eec/n_tensorflow_20_hackathon_coming_up_also_our_team/,Research2Vec,1555286259,"Here's the link 

https://tensorflow.devpost.com/

We are looking for one more member, *ideally* someone with experience some of the current SoTA NLP models (Elmo, Transformer, BERT, GPT/2, ULMFiT, etc.) and wrangling data for those datasets  (Our adviser may have had their name published in the official paper for one of those papers ;) ) . But really, we're just looking for someone who has solid practical experience with Tensorflow and can data wrangle. 

If you're interested, PM me with what are your time commitments for the next 3 weeks, and your experience with Tensorflow.",12,152,False,self,,,,,
952,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,8,bd9f5n,mihaileric.com,"Why So Naive, Bayes",https://www.reddit.com/r/MachineLearning/comments/bd9f5n/why_so_naive_bayes/,MusingEtMachina,1555286393,,0,1,False,https://b.thumbs.redditmedia.com/2s2dwbnGlAMinZQuhB95LyEZZ8lxeDeaOBDuBd2wtUk.jpg,,,,,
953,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,9,bd9sbv,self.MachineLearning,Looking for: Learning to rank algorithms,https://www.reddit.com/r/MachineLearning/comments/bd9sbv/looking_for_learning_to_rank_algorithms/,zQuantz,1555288647,[removed],0,1,False,self,,,,,
954,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,9,bd9y5d,arxiv.org,[R] Structured agents for physical construction,https://www.reddit.com/r/MachineLearning/comments/bd9y5d/r_structured_agents_for_physical_construction/,hardmaru,1555289674,,1,9,False,default,,,,,
955,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,9,bd9zc6,self.MachineLearning,Can you introduce an article about the artificial intelligence revolution and its future?,https://www.reddit.com/r/MachineLearning/comments/bd9zc6/can_you_introduce_an_article_about_the_artificial/,Doctor_who1,1555289875,[removed],0,1,False,self,,,,,
956,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,9,bd9zrk,self.MachineLearning,[P] dm2gym: Convert DeepMind Control Suite to OpenAI gym environments.,https://www.reddit.com/r/MachineLearning/comments/bd9zrk/p_dm2gym_convert_deepmind_control_suite_to_openai/,modernrl,1555289947,[dm2gym](https://github.com/zuoxingdong/dm2gym): Convert DeepMind Control Suite to OpenAI gym environments.,1,18,False,self,,,,,
957,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,10,bda9u3,self.MachineLearning,[P] What is the ideal dictionary size for performing sentiment analysis on massive datasets?,https://www.reddit.com/r/MachineLearning/comments/bda9u3/p_what_is_the_ideal_dictionary_size_for/,aklagoo,1555291703,"I'm working on analyzing fiction. As of right now, I plan to have around 300 books of around 300 pages each. Is there a general rule or a specific number regarding the number of words to be included in the extracted dictionary?",7,6,False,self,,,,,
958,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,10,bdagkh,self.MachineLearning,[D] Has anyone here used AutoML? How does it compare to rolling your own?,https://www.reddit.com/r/MachineLearning/comments/bdagkh/d_has_anyone_here_used_automl_how_does_it_compare/,Healthy_Bother,1555292856,Just starting to plan out some AI infrastructure for data science work and was wondering if anyone has tried Google's AutoML and what their personal experiences were. Most of our team thinks it's too good to be true.,18,17,False,self,,,,,
959,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,10,bdaink,self.MachineLearning,"Precision,recall and accuracy for multi classification problem",https://www.reddit.com/r/MachineLearning/comments/bdaink/precisionrecall_and_accuracy_for_multi/,sashaHolmesBerry,1555293215,[removed],0,1,False,self,,,,,
960,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,11,bdann5,self.MachineLearning,[P] js-gym: JavaScript environment for training reinforcement learning agents.,https://www.reddit.com/r/MachineLearning/comments/bdann5/p_jsgym_javascript_environment_for_training/,milaworld,1555294074,"# [js-gym](https://github.com/bobiblazeski/js-gym)

JavaScript environment for training reinforcement learning agents.

They created an environment that wraps over a few JS games for DeepRL (MK and tetris examples included). Can visualize over the browser on localhost port 8000

## Sample algorithms

1. Random Play
2. Random Search
3. HillClimbing
4. Augmented Random Search
5. Deep Deterministic Policy Gradient

https://github.com/bobiblazeski/js-gym",1,6,False,self,,,,,
961,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,11,bdauih,i.redd.it,"road safety barrier blow molding machine special design according to customer requirements including shape,color,logo and so on",https://www.reddit.com/r/MachineLearning/comments/bdauih/road_safety_barrier_blow_molding_machine_special/,miyawang12138,1555295186,,0,1,False,default,,,,,
962,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,11,bdb43q,self.MachineLearning,Need career advice.,https://www.reddit.com/r/MachineLearning/comments/bdb43q/need_career_advice/,Aconfusedphysicist,1555296630,[removed],0,1,False,self,,,,,
963,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,12,bdb8b4,youtu.be,A.I. learns to play PACMAN using Deep Q Learning: Explained!,https://www.reddit.com/r/MachineLearning/comments/bdb8b4/ai_learns_to_play_pacman_using_deep_q_learning/,VCubingX,1555297371,,0,1,False,https://b.thumbs.redditmedia.com/51FZM5gwQMuSag-85Hd4C4hudM_f3CGljTqRZc4fU5A.jpg,,,,,
964,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,13,bdbtp7,self.MachineLearning,Medical Student trying to being learning about machine learning,https://www.reddit.com/r/MachineLearning/comments/bdbtp7/medical_student_trying_to_being_learning_about/,-PM_ME_YOUR_PUNS,1555301528,[removed],0,1,False,self,,,,,
965,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,13,bdbwi1,self.MachineLearning,Recommendation System Based on attributes,https://www.reddit.com/r/MachineLearning/comments/bdbwi1/recommendation_system_based_on_attributes/,aacube,1555302089,[removed],0,1,False,self,,,,,
966,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,13,bdby7s,self.MachineLearning,Speech Synthesis with raw waveform data,https://www.reddit.com/r/MachineLearning/comments/bdby7s/speech_synthesis_with_raw_waveform_data/,gshamane,1555302437,[removed],0,1,False,self,,,,,
967,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,13,bdc0q8,self.MachineLearning,"After eight months of development efforts, the OpenAI Five exacted their revenge today against one of the worlds top teams in a highly anticipated best-of-three 5v5 Dota 2 showdown in San Francisco.",https://www.reddit.com/r/MachineLearning/comments/bdc0q8/after_eight_months_of_development_efforts_the/,Yuqing7,1555302970,[removed],0,1,False,self,,,,,
968,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,14,bdcjma,self.MachineLearning,New to ML what should I start with?,https://www.reddit.com/r/MachineLearning/comments/bdcjma/new_to_ml_what_should_i_start_with/,A4_Ts,1555307223,[removed],0,1,False,self,,,,,
969,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,15,bdcow1,self.MachineLearning,word sense disambiguation - practical experiences?,https://www.reddit.com/r/MachineLearning/comments/bdcow1/word_sense_disambiguation_practical_experiences/,MLTyrunt,1555308492,[removed],0,1,False,self,,,,,
970,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,15,bdcr90,oodlestechnologies.com,AI System Can Detect Emotions In Text Messages,https://www.reddit.com/r/MachineLearning/comments/bdcr90/ai_system_can_detect_emotions_in_text_messages/,tech-info,1555309056,,0,1,False,default,,,,,
971,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,15,bdcu9k,code-brew.com,Here are the top 10 programming languages of 2019,https://www.reddit.com/r/MachineLearning/comments/bdcu9k/here_are_the_top_10_programming_languages_of_2019/,hitesh_patiyal,1555309780,,0,1,False,default,,,,,
972,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,16,bdd4y8,forbes.com,Why Machine Learning Models Crash And Burn In Production,https://www.reddit.com/r/MachineLearning/comments/bdd4y8/why_machine_learning_models_crash_and_burn_in/,Oliver_Wough,1555312528,,0,1,False,default,,,,,
973,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,16,bdd7vs,hackernoon.com,ML.NET: Machine Learning framework by Microsoft for .NET developers,https://www.reddit.com/r/MachineLearning/comments/bdd7vs/mlnet_machine_learning_framework_by_microsoft_for/,Oliver_Wough,1555313268,,0,1,False,default,,,,,
974,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,16,bddbla,hub.packtpub.com,.NET team announces ML.NET 0.6,https://www.reddit.com/r/MachineLearning/comments/bddbla/net_team_announces_mlnet_06/,Oliver_Wough,1555314297,,0,1,False,default,,,,,
975,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,16,bdddm9,self.MachineLearning,Can AI help summarize article or abstract sentence keyword?,https://www.reddit.com/r/MachineLearning/comments/bdddm9/can_ai_help_summarize_article_or_abstract/,gloomyson,1555314845,[removed],0,1,False,self,,,,,
976,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,16,bdde8m,fossbytes.com,Microsoft Launches ML.NET Open Source Machine Learning Framework,https://www.reddit.com/r/MachineLearning/comments/bdde8m/microsoft_launches_mlnet_open_source_machine/,Oliver_Wough,1555315023,,0,1,False,default,,,,,
977,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,17,bddg8c,self.MachineLearning,Can AI make a sentence by several given words?,https://www.reddit.com/r/MachineLearning/comments/bddg8c/can_ai_make_a_sentence_by_several_given_words/,gloomyson,1555315564,[removed],0,1,False,self,,,,,
978,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,17,bddig4,self.MachineLearning,Do we have vector space to measure word similarity by spelling and pronouncing,https://www.reddit.com/r/MachineLearning/comments/bddig4/do_we_have_vector_space_to_measure_word/,gloomyson,1555316179,[removed],0,1,False,self,,,,,
979,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,17,bddj3z,learnopenerp.blogspot.com,Custom Object Training using TensorFlow Object Detection API - Part 2,https://www.reddit.com/r/MachineLearning/comments/bddj3z/custom_object_training_using_tensorflow_object/,Sehrishnaz47,1555316367,,0,1,False,default,,,,,
980,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,17,bddmjo,thestar.com.my,Can a computer write a script? Machine learning goes Hollywood - Tech News | The Star Online,https://www.reddit.com/r/MachineLearning/comments/bddmjo/can_a_computer_write_a_script_machine_learning/,code_x_7777,1555317329,,0,1,False,default,,,,,
981,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,17,bddn1o,self.MachineLearning,Do we have cross-language vector space for word embedding?,https://www.reddit.com/r/MachineLearning/comments/bddn1o/do_we_have_crosslanguage_vector_space_for_word/,gloomyson,1555317475,"Do we have cross-language vector space for word embedding? 

When measure similarity for apple/Pomme/mela/Lacus//, they should be the same",0,1,False,self,,,,,
982,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,18,bddt0v,burckhardt.com,perforation machine manufacturer,https://www.reddit.com/r/MachineLearning/comments/bddt0v/perforation_machine_manufacturer/,habmkloganjt,1555319037,,0,1,False,default,,,,,
983,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,19,bde8tm,self.MachineLearning,Difference between open pose and posenet in pose estimation ?,https://www.reddit.com/r/MachineLearning/comments/bde8tm/difference_between_open_pose_and_posenet_in_pose/,theSamuraiMonk17,1555322841,"Hi , I have seen a lot of people using open pose as opposed to posenet for pose estimation related tasks.  I was wondering what makes open pose more preferred than posenet ? As far as i have read or understood , the performance of both posenet and open pose seem to be at par . What do you guys think of it ??",0,1,False,self,,,,,
984,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,20,bdeqnd,datasciencedigest.org,DataScience Digest - Issue #16,https://www.reddit.com/r/MachineLearning/comments/bdeqnd/datascience_digest_issue_16/,flyelephant,1555326817,,0,1,False,https://a.thumbs.redditmedia.com/0ZtAsLatvWtUwMUvORJxZTxLECtLxKGQXTOIuTZhTf4.jpg,,,,,
985,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,20,bdeusq,self.MachineLearning,Pandas cheat sheet,https://www.reddit.com/r/MachineLearning/comments/bdeusq/pandas_cheat_sheet/,_default_settings,1555327670,[removed],0,1,False,self,,,,,
986,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,21,bdf87t,self.MachineLearning,[D] Preparing a deep learning course for computer vision,https://www.reddit.com/r/MachineLearning/comments/bdf87t/d_preparing_a_deep_learning_course_for_computer/,finite-difference,1555330317,"I am a PhD student in a small computer vision group at our university. I am the only person in the team with experience in deep learning and I initiated a creation of a new course for our students which should cover deep learning approaches to computer vision. The structure of the lectures will probably be very similar to Standford CS231n course. I won't be able to be the lecturer since this requires a PhD, but it is my responsibility to prepare lab exercises for the students. Since deep learning is computationally expensive I hope to use some cloud service which offers free credits to academia for educational purposes. I was thinking about using Google cloud. However, I feel that this area is developing so fast that it would be valuable to ask for some tips regarding this, especially since creating a new course takes so much time and committing to a wrong platform or choosing a bad approach might lead to many lost hours.

&amp;#x200B;

tldr: I am preparing lab exercises for deep learning in computer vision course. Which cloud service (or other approach) should the course use for lab exercises?",19,24,False,self,,,,,
987,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,21,bdf8f4,self.MachineLearning,IJCAI reviews are out,https://www.reddit.com/r/MachineLearning/comments/bdf8f4/ijcai_reviews_are_out/,Mefaso,1555330353,[removed],0,1,False,self,,,,,
988,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,21,bdfbkg,medium.com,Kyle Simpson: Ive Forgotten More JavaScript Than Most People Ever Learn,https://www.reddit.com/r/MachineLearning/comments/bdfbkg/kyle_simpson_ive_forgotten_more_javascript_than/,FrontendNation,1555330909,,0,1,False,default,,,,,
989,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,21,bdfibv,habr.com,Identifying dog breed with neural networks: from Keras to Android app,https://www.reddit.com/r/MachineLearning/comments/bdfibv/identifying_dog_breed_with_neural_networks_from/,atomlib_com,1555332097,,0,1,False,https://b.thumbs.redditmedia.com/EkhVYjsJ8fdjRe-ME3EWnIc8N8m5_sEfI_punl0_lFE.jpg,,,,,
990,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,21,bdfl2b,self.MachineLearning,how to aggregate images to hierarchical structures?,https://www.reddit.com/r/MachineLearning/comments/bdfl2b/how_to_aggregate_images_to_hierarchical_structures/,Suruoxi,1555332566,[removed],0,1,False,self,,,,,
991,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,21,bdfm2v,github.com,Full Chainer implementation of OpenAI's Reinforcement Learning using Random Network Distillation,https://www.reddit.com/r/MachineLearning/comments/bdfm2v/full_chainer_implementation_of_openais/,ThisIsMySeudonym,1555332733,,0,1,False,default,,,,,
992,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,22,bdfsax,self.MachineLearning,IJCAI reviews are out.,https://www.reddit.com/r/MachineLearning/comments/bdfsax/ijcai_reviews_are_out/,pratstheory,1555333751,[removed],1,1,False,self,,,,,
993,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,22,bdg3sr,arxiv.org,T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor,https://www.reddit.com/r/MachineLearning/comments/bdg3sr/tnet_parametrizing_fully_convolutional_nets_with/,iyaja,1555335652,,26,127,False,default,,,,,
994,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,22,bdg7br,i.redd.it,Getting Smart at Being Smarter,https://www.reddit.com/r/MachineLearning/comments/bdg7br/getting_smart_at_being_smarter/,Vidhya_Shree,1555336224,,0,1,False,default,,,,,
995,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,23,bdgfv5,studytonight.com,Machine Learning and Data Visualization using Orange,https://www.reddit.com/r/MachineLearning/comments/bdgfv5/machine_learning_and_data_visualization_using/,studytonight,1555337563,,0,1,False,default,,,,,
996,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,23,bdgn1i,self.MachineLearning,Feasibility of an AI DM for a role playing game,https://www.reddit.com/r/MachineLearning/comments/bdgn1i/feasibility_of_an_ai_dm_for_a_role_playing_game/,handintannor,1555338658,[removed],0,1,False,self,,,,,
997,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,23,bdgplv,youtu.be,"[P] Donald Trump TTS Model Says ""Mom's Spaghetti"" in Different Styles",https://www.reddit.com/r/MachineLearning/comments/bdgplv/p_donald_trump_tts_model_says_moms_spaghetti_in/,hanyuqn,1555339052,,0,1,False,https://a.thumbs.redditmedia.com/X7Xr23eo-OpG5RpWTPyaAh4NFvZjTpqv8KyrmHoGqP4.jpg,,,,,
998,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,23,bdgx7u,self.MachineLearning,Named Entity/Noun Phrases Detection,https://www.reddit.com/r/MachineLearning/comments/bdgx7u/named_entitynoun_phrases_detection/,feedmari,1555340190,[removed],0,1,False,self,,,,,
999,MachineLearning,t5_2r3gv,2019-4-15,2019,4,15,23,bdgxin,self.MachineLearning,[D] Any Papers that criticize Deep Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/bdgxin/d_any_papers_that_criticize_deep_reinforcement/,exenson,1555340236,"Is anybody aware of literature that criticizes Deep Reinforcement Learning? I sometimes see a few points mentioned in some papers' introductions about how data-hungry Deep RL is, how it is not applicable in the real world, and that there's nothing human-like about it. But I'm not aware of any papers that criticize Deep Reinforcement Learning heavily. 

&amp;#x200B;

Any help is appreciated.

&amp;#x200B;

Thanks!",27,70,False,self,,,,,
1000,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,0,bdh9mn,self.MachineLearning,Machine Learning for Causal Inference,https://www.reddit.com/r/MachineLearning/comments/bdh9mn/machine_learning_for_causal_inference/,the_universe_is_vast,1555341972,[removed],0,1,False,self,,,,,
1001,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,0,bdhkmd,link.medium.com,How to use clustering algorithms to segment your customers and maximise sales. [P],https://www.reddit.com/r/MachineLearning/comments/bdhkmd/how_to_use_clustering_algorithms_to_segment_your/,meridit45,1555343496,,0,1,False,default,,,,,
1002,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdhoxs,self.MachineLearning,[P] Hand posture recognition/segmentation,https://www.reddit.com/r/MachineLearning/comments/bdhoxs/p_hand_posture_recognitionsegmentation/,behindthedash,1555344048,The model identifies a hand in a video stream and recognizes its posture. It was trained in Keras and converted to TensorflowJS to be applied in a browser directly  [https://dashbouquet.github.io/computer-vision-demos/](https://dashbouquet.github.io/computer-vision-demos/),0,1,False,self,,,,,
1003,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdhpoi,self.MachineLearning,[D] Help for creating a CNN for recognizing handwritten characters.,https://www.reddit.com/r/MachineLearning/comments/bdhpoi/d_help_for_creating_a_cnn_for_recognizing/,madhavgoyal98,1555344139,"I am student working on a project which requires handwritten characters to be recognized. I have tried some CNN architectures but the results are not good.
If anyone could tell what architecture would be suitable for this.",2,1,False,self,,,,,
1004,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdhuke,self.MachineLearning,[P] Full Chainer Implementation OpenAI Random Network Distillation,https://www.reddit.com/r/MachineLearning/comments/bdhuke/p_full_chainer_implementation_openai_random/,ThisIsMySeudonym,1555344744,"I released my implementation of [OpenAI's Reinforcement Learning using Random Network Distillation](https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/). The implementation is fairly complete, done almost exactly as laid out in the paper. Check it out at [https://github.com/AdeelMufti/RL-RND](https://github.com/AdeelMufti/RL-RND).

&amp;#x200B;

Interestingly, I tried it on PLE's PixelCopter where I turned off the extrinsic rewards altogether, and it got roughly the same results with the extrinsic rewards. I wrote about it here:  [http://blog.adeel.io/2019/04/13/reinforcement-learning-using-intrinsic-rewards-through-random-network-distillation-in-chainer/](http://blog.adeel.io/2019/04/13/reinforcement-learning-using-intrinsic-rewards-through-random-network-distillation-in-chainer/) 

&amp;#x200B;

Someone with a free GPU sitting around mind spinning it up in Montezuma's Revenge for a while? I'm curious to see what this implementation will achieve.",5,17,False,self,,,,,
1005,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdhvk3,youtube.com,3 weeks ago I made A.I. that can beat my own game! Now I've returned with my first major update.,https://www.reddit.com/r/MachineLearning/comments/bdhvk3/3_weeks_ago_i_made_ai_that_can_beat_my_own_game/,CKlidify,1555344869,,0,1,False,default,,,,,
1006,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdhvxr,self.MachineLearning,[R] Probabilistic Model-Based Reinforcement Learning Using The Differentiable Neural Computer,https://www.reddit.com/r/MachineLearning/comments/bdhvxr/r_probabilistic_modelbased_reinforcement_learning/,ThisIsMySeudonym,1555344916,"MSc dissertation (from Summer 2018) on using the Differentiable Neural Computer in the Reinforcement Learning / Evolution Strategies context: [http://blog.adeel.io/2018/09/10/probabilistic-model-based-reinforcement-learning-using-the-differentiable-neural-computer/](http://blog.adeel.io/2018/09/10/probabilistic-model-based-reinforcement-learning-using-the-differentiable-neural-computer/)

&gt;""...experiments found that a model learned in a Differentiable Neural Computer outperformed a vanilla LSTM based model, on two gaming environments.""

Would love to hear your thoughts!",5,11,False,self,,,,,
1007,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdhwf4,self.MachineLearning,Good Code VS Time,https://www.reddit.com/r/MachineLearning/comments/bdhwf4/good_code_vs_time/,EdHerzriesig,1555344978,"So Im a mathematician/data scientist and lately Ive been trying to write all my code in adherence to the software engineering code of ethics. I got to admit that it takes a bit more time, than just throwing to together a bunch of code, as its still not routine. Anyhow, my boss took me in today and told me that he thinks the code Ive written for project is nice but he doesnt care about nice code and just wants it quick and dirty, at least in the experiment phase. He also said forcefully that I should only use notebooks, as Im currently only using Atom and vim. 

Im quit upset by this message as I think good code routines is worth spending the time to achieve, which I tried telling him. What do you guys think? 

If I may add: his code could just as well be bunch of hieroglyphs all over a ten meter high wall and hes a bad explainer on top of that ",0,1,False,self,,,,,
1008,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdi061,government.diginomica.com,Algorithmic Accountability Act targets bias in AI decision-making,https://www.reddit.com/r/MachineLearning/comments/bdi061/algorithmic_accountability_act_targets_bias_in_ai/,Blognoggl5,1555345466,,1,1,False,default,,,,,
1009,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdi68y,self.MachineLearning,How does Uber use machine learning?,https://www.reddit.com/r/MachineLearning/comments/bdi68y/how_does_uber_use_machine_learning/,joshitanmay55,1555346285,[removed],0,1,False,self,,,,,
1010,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdid6f,self.MachineLearning,[D] IJCAI review are out,https://www.reddit.com/r/MachineLearning/comments/bdid6f/d_ijcai_review_are_out/,goldemerald,1555347248,Best of luck!,0,1,False,self,,,,,
1011,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,1,bdidpi,self.MachineLearning,Need Customer Service Chatbot Dataset,https://www.reddit.com/r/MachineLearning/comments/bdidpi/need_customer_service_chatbot_dataset/,zislay24,1555347329,[removed],0,1,False,self,,,,,
1012,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,2,bdikja,self.MachineLearning,[D] IJCAI reviews are out.,https://www.reddit.com/r/MachineLearning/comments/bdikja/d_ijcai_reviews_are_out/,goldemerald,1555348272,Best of luck!,1,0,False,self,,,,,
1013,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,2,bdj1o2,self.MachineLearning,Study Path to learn Autonomous Driving Technology,https://www.reddit.com/r/MachineLearning/comments/bdj1o2/study_path_to_learn_autonomous_driving_technology/,pkumarm,1555350574,[removed],0,1,False,self,,,,,
1014,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,3,bdjds9,self.MachineLearning,[D] Is the AI community in an increasing rush to publish the results of studies?,https://www.reddit.com/r/MachineLearning/comments/bdjds9/d_is_the_ai_community_in_an_increasing_rush_to/,trcytony,1555352173,[removed],0,1,False,self,,,,,
1015,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,3,bdjfwt,self.MachineLearning,How does learning with a VAE look?,https://www.reddit.com/r/MachineLearning/comments/bdjfwt/how_does_learning_with_a_vae_look/,sadtoots123,1555352455,[removed],0,1,False,self,,,,,
1016,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,3,bdjxf2,self.MachineLearning,[Discussion] Be careful when using pretrained deep learning models,https://www.reddit.com/r/MachineLearning/comments/bdjxf2/discussion_be_careful_when_using_pretrained_deep/,ceceshao1,1555354748,"Using pre-trained deep learning models like ResNet, Inception, and VGG is easier than ever, but there are implementation details you need to be careful with to avoid subpar performance and errors. 

&amp;#x200B;

Put together this list of implementation details to be cautious of  curious to see if I'm missing anything or if other folks have had similar experiences with discrepancies in performance?

[https://medium.com/comet-ml/approach-pre-trained-deep-learning-models-with-caution-9f0ff739010c](https://medium.com/comet-ml/approach-pre-trained-deep-learning-models-with-caution-9f0ff739010c)",2,18,False,self,,,,,
1017,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,4,bdk02j,self.MachineLearning,Binary classification the model is not converging,https://www.reddit.com/r/MachineLearning/comments/bdk02j/binary_classification_the_model_is_not_converging/,textMinier,1555355094,[removed],0,1,False,self,,,,,
1018,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,4,bdk1qz,self.MachineLearning,IJCAI Reviews Doubt,https://www.reddit.com/r/MachineLearning/comments/bdk1qz/ijcai_reviews_doubt/,IronSwartz,1555355313,"Only reviewer's comments were released today. However, the score is not released (accept reject recommendation or any kind of metric). Is this the norm?",0,1,False,self,,,,,
1019,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,4,bdk3ye,medium.com,ReWork Deep Learning in Finance Summit,https://www.reddit.com/r/MachineLearning/comments/bdk3ye/rework_deep_learning_in_finance_summit/,Yuqing7,1555355612,,0,1,False,https://a.thumbs.redditmedia.com/bZSH1Gl3QmGzVf2-AQW7Mn2sOQPSfxlu3RwInHsTO-0.jpg,,,,,
1020,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,4,bdk5te,self.MachineLearning,Machine Learning and AI Applications in Media and Entertainment,https://www.reddit.com/r/MachineLearning/comments/bdk5te/machine_learning_and_ai_applications_in_media_and/,AnnaOnTheWeb,1555355847,[removed],0,1,False,self,,,,,
1021,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,4,bdk64m,self.MachineLearning,Sequence models for languages that are sequence independent?,https://www.reddit.com/r/MachineLearning/comments/bdk64m/sequence_models_for_languages_that_are_sequence/,adikhad,1555355893,[removed],0,1,False,self,,,,,
1022,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,4,bdkgji,youtube.com,[R] Dissecting the original Transformer architecture,https://www.reddit.com/r/MachineLearning/comments/bdkgji/r_dissecting_the_original_transformer_architecture/,DemiourgosD,1555357284,,0,1,False,default,,,,,
1023,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdkppb,self.MachineLearning,PyCM 2.0 released: A general benchmark based comparison of classification models,https://www.reddit.com/r/MachineLearning/comments/bdkppb/pycm_20_released_a_general_benchmark_based/,sepandhaghighi,1555358524,"PyCM version 2.0 released

[https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

[http://www.pycm.ir](http://www.pycm.ir/) 

&amp;#x200B;

In version 2.0 a method for comparing several confusion matrices is introduced. This option is a combination of several overall and class-based benchmarks. Each of the benchmarks evaluates the performance of the classification algorithm from good to poor and give them a numeric score. The score of good performance is 1 and for the poor performance is 0.

After that, two scores are calculated for each confusion matrices, overall and class based. The overall score is the average of the score of four overall benchmarks which are Landis &amp; Koch, Fleiss, Altman, and Cicchetti. And with a same manner, the class based score is the average of the score of three class-based benchmarks which are Positive Likelihood Ratio Interpretation, Discriminant Power Interpretation, and AUC value Interpretation. It should be notice that if one of the benchmarks returns none for one of the classes, that benchmarks will be eliminate in total averaging. If user set weights for the classes, the averaging over the value of class-based benchmark scores will transform to a weighted average.

If the user set the value of by\_class boolean input True, the best confusion matrix is the one with the maximum class-based score. Otherwise, if a confusion matrix obtain the maximum of the both overall and class-based score, that will be the reported as the best confusion matrix but in any other cases the compare object doesnt select best confusion matrix.

&amp;#x200B;

&amp;#x200B;

![img](vzoax4olghs21)

&amp;#x200B;

Changelog : 

* G-Mean (GM) added [\#178](https://github.com/sepandhaghighi/pycm/issues/178)
* Index of balanced accuracy (IBA) added [\#176](https://github.com/sepandhaghighi/pycm/issues/176)
* Optimized precision (OP) added [\#152](https://github.com/sepandhaghighi/pycm/issues/152)
* Pearson's C (C) added [\#180](https://github.com/sepandhaghighi/pycm/issues/180)
* Compare class added [\#111](https://github.com/sepandhaghighi/pycm/issues/111)
* Parameters recommendation warning added [\#174](https://github.com/sepandhaghighi/pycm/issues/174)
* ConfusionMatrix equal method added [\#181](https://github.com/sepandhaghighi/pycm/issues/181)
* Document modified [\#173](https://github.com/sepandhaghighi/pycm/issues/173)
* stat\_print function bug fixed
* table\_print function bug fixed
* Beta parameter renamed to beta (F\_calc function &amp; F\_beta method)
* Parameters recommendation for imbalance dataset modified
* normalize parameter added to save\_html method [\#183](https://github.com/sepandhaghighi/pycm/issues/183)
* pycm\_func.py splitted into pycm\_class\_func.py and pycm\_overall\_func.py
* vector\_filter, vector\_check, class\_check and matrix\_check functions moved to pycm\_util.py
* RACC\_calc and RACCU\_calc functions exception handler modified [\#187](https://github.com/sepandhaghighi/pycm/issues/187)
* Docstrings modified",0,1,False,self,,,,,
1024,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdkr8q,youtube.com,[D] - Art of the Problem announced a seven part series on AI/ML,https://www.reddit.com/r/MachineLearning/comments/bdkr8q/d_art_of_the_problem_announced_a_seven_part/,puppers90,1555358736,,0,1,False,https://b.thumbs.redditmedia.com/G2cdc5ofZZ6JW3eEindCSL373brRoKXwG0ans72YDSQ.jpg,,,,,
1025,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdksyv,self.MachineLearning,[P] PyCM 2.0 released: A general benchmark based comparison of classification models,https://www.reddit.com/r/MachineLearning/comments/bdksyv/p_pycm_20_released_a_general_benchmark_based/,sepandhaghighi,1555358973,"PyCM version 2.0 released

[https://github.com/sepandhaghighi/pycm](https://github.com/sepandhaghighi/pycm)

[http://www.pycm.ir](http://www.pycm.ir/)

&amp;#x200B;

In version 2.0 a method for comparing several confusion matrices is introduced. This option is a combination of several overall and class-based benchmarks. Each of the benchmarks evaluates the performance of the classification algorithm from good to poor and give them a numeric score. The score of good performance is 1 and for the poor performance is 0.

After that, two scores are calculated for each confusion matrices, overall and class based. The overall score is the average of the score of four overall benchmarks which are Landis &amp; Koch, Fleiss, Altman, and Cicchetti. And with a same manner, the class based score is the average of the score of three class-based benchmarks which are Positive Likelihood Ratio Interpretation, Discriminant Power Interpretation, and AUC value Interpretation. It should be notice that if one of the benchmarks returns none for one of the classes, that benchmarks will be eliminate in total averaging. If user set weights for the classes, the averaging over the value of class-based benchmark scores will transform to a weighted average.

If the user set the value of by\_class boolean input True, the best confusion matrix is the one with the maximum class-based score. Otherwise, if a confusion matrix obtain the maximum of the both overall and class-based score, that will be the reported as the best confusion matrix but in any other cases the compare object doesnt select best confusion matrix.

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/7eacdtt2jhs21.png

&amp;#x200B;

Changelog :

* G-Mean (GM) added [\#178](https://github.com/sepandhaghighi/pycm/issues/178)
* Index of balanced accuracy (IBA) added [\#176](https://github.com/sepandhaghighi/pycm/issues/176)
* Optimized precision (OP) added [\#152](https://github.com/sepandhaghighi/pycm/issues/152)
* Pearson's C (C) added [\#180](https://github.com/sepandhaghighi/pycm/issues/180)
* Compare class added [\#111](https://github.com/sepandhaghighi/pycm/issues/111)
* Parameters recommendation warning added [\#174](https://github.com/sepandhaghighi/pycm/issues/174)
* ConfusionMatrix equal method added [\#181](https://github.com/sepandhaghighi/pycm/issues/181)
* Document modified [\#173](https://github.com/sepandhaghighi/pycm/issues/173)
* stat\_print function bug fixed
* table\_print function bug fixed
* Beta parameter renamed to beta (F\_calc function &amp; F\_beta method)
* Parameters recommendation for imbalance dataset modified
* normalize parameter added to save\_html method [\#183](https://github.com/sepandhaghighi/pycm/issues/183)
* pycm\_func.py splitted into pycm\_class\_func.py and pycm\_overall\_func.py
* vector\_filter, vector\_check, class\_check and matrix\_check functions moved to pycm\_util.py
* RACC\_calc and RACCU\_calc functions exception handler modified [\#187](https://github.com/sepandhaghighi/pycm/issues/187)
* Docstrings modified",5,22,False,https://b.thumbs.redditmedia.com/wqZSoG0ovZmeGg-FnZX3z6G7ELWJk-xk9y7rqjCb_XY.jpg,,,,,
1026,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdktjv,self.MachineLearning,"[D] ""Other"" class in DNN classification",https://www.reddit.com/r/MachineLearning/comments/bdktjv/d_other_class_in_dnn_classification/,ME_PhD,1555359050,"I'm making a ConvNet that classifies my data into one of classes [A, B, C, D, ""other""], ""other"" being anything that doesn't fit into [A, B, C, D].

I'm thinking of mapping the input x to a vector ""c"" in R^4, and then transforming ""c"" to R^5 (using a few tiny FC layers) before applying softmax to get the output.

My reasoning is that I don't want the network attempting ""learn"" anything specific about the ""other"" class, I just want it to be a sort of ""nor"" operation. The network should only focus on learning the features of [A, B, C, D]. The variance in the ""other"" class is very high so I want it to generalize well.

Thoughts? Papers? Prior experience?",13,20,False,self,,,,,
1027,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdkupz,old.reddit.com,"Interest in Learning about Artificial Intelligence and machine learning through high-quality video tutorials? Check out and subscribe to this new subreddit, titled ""AITutorials""! :)",https://www.reddit.com/r/MachineLearning/comments/bdkupz/interest_in_learning_about_artificial/,ailearn12,1555359220,,0,1,False,default,,,,,
1028,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdkv5c,self.MachineLearning,tensorflow playground - machine learning via...machine learning?,https://www.reddit.com/r/MachineLearning/comments/bdkv5c/tensorflow_playground_machine_learning_viamachine/,romansocks,1555359281,[removed],0,1,False,https://b.thumbs.redditmedia.com/NvzZjpxPN5w5PeKWR1EU5kRYD5QvpIkhfqS3v2vtBVA.jpg,,,,,
1029,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdl01s,self.MachineLearning,Speech Model Pre-training for End-to-End Spoken Language Understanding,https://www.reddit.com/r/MachineLearning/comments/bdl01s/speech_model_pretraining_for_endtoend_spoken/,m_nemo_syne,1555359989,[removed],0,1,False,self,,,,,
1030,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdl4hv,self.MachineLearning,Papers on AI Bias and Behavioral Economics,https://www.reddit.com/r/MachineLearning/comments/bdl4hv/papers_on_ai_bias_and_behavioral_economics/,_vfbsilva_,1555360580,[removed],0,1,False,self,,,,,
1031,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,5,bdl8od,self.MachineLearning,Depth First Learning Fellowship,https://www.reddit.com/r/MachineLearning/comments/bdl8od/depth_first_learning_fellowship/,cinjon,1555361184,[removed],0,1,False,self,,,,,
1032,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,6,bdlf30,self.MachineLearning,Using SSA or TSFresh to reduce overfitting for time series?,https://www.reddit.com/r/MachineLearning/comments/bdlf30/using_ssa_or_tsfresh_to_reduce_overfitting_for/,74throwaway,1555363252,[removed],0,1,False,self,,,,,
1033,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,6,bdlq2g,self.MachineLearning,[R] - Speech Model Pre-training for End-to-End Spoken Language Understanding,https://www.reddit.com/r/MachineLearning/comments/bdlq2g/r_speech_model_pretraining_for_endtoend_spoken/,m_nemo_syne,1555364871,[removed],4,4,False,self,,,,,
1034,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,6,bdlq9j,self.MachineLearning,[D] Depth First Learning Fellowship,https://www.reddit.com/r/MachineLearning/comments/bdlq9j/d_depth_first_learning_fellowship/,cinjon,1555364899,"Hi all, we are super proud to announce the Depth First Learning 2019 Fellows ([http://www.depthfirstlearning.com/2019/Announcing-DFL-Fellows](http://www.depthfirstlearning.com/2019/Announcing-DFL-Fellows)).

They are:

**Steve Kroon - Stellenbosch (South Africa)** \--&gt; Variational Inference with Normalizing Flows

**Sandhya Prabhakaran - New York (USA)** \--&gt; Spherical CNNs

**Bhairav Mehta - Montreal (Canada)** \--&gt; Stein Variational Gradient Descent

**Vinay Ramasesh, Piyush Patil, and Riley Edmunds - Berkeley (USA)** \--&gt; Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry

If you're keen to join in on any of those learning groups, please do apply on our announcement page ([http://www.depthfirstlearning.com/2019/Announcing-DFL-Fellows](http://www.depthfirstlearning.com/2019/Announcing-DFL-Fellows)).",1,11,False,self,,,,,
1035,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,8,bdmtkp,self.MachineLearning,Increasing regularization during deep network training,https://www.reddit.com/r/MachineLearning/comments/bdmtkp/increasing_regularization_during_deep_network/,jcreinhold,1555370736,[removed],0,1,False,self,,,,,
1036,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,8,bdn5ix,arxiv.org,[1904.05049] Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution,https://www.reddit.com/r/MachineLearning/comments/bdn5ix/190405049_drop_an_octave_reducing_spatial/,Bayequentist,1555372624,,12,23,False,default,,,,,
1037,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,8,bdn5rx,self.MachineLearning,[D] Increasing regularization during deep network training,https://www.reddit.com/r/MachineLearning/comments/bdn5rx/d_increasing_regularization_during_deep_network/,jcreinhold,1555372662,"I recently read a paper that suggested increasing weight decay, dropout rate, etc. (i.e., regularization parameters) while a deep network was training to avoid overfitting; however, I cannot remember the name of the paper. I tried to search through the literature, but searching using terms like ""increase regularization deep learning"" hasn't turned up much (unsurprisingly).

&amp;#x200B;

I did find [Curriculum Dropout](https://arxiv.org/abs/1703.06229), which suggests increasing the dropout rate during training, but I don't believe this is the paper I had in mind.

&amp;#x200B;

Anyone happen to know of other papers discussing this subject? Are there any appearing trends surrounding changing regularization parameters during training? Anyone have any experience testing this idea out?

&amp;#x200B;

Thanks",2,6,False,self,,,,,
1038,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,9,bdna2s,self.MachineLearning,Facebook's Ad Algorithm is racist? I don't agree or disagree. I just don't get it.,https://www.reddit.com/r/MachineLearning/comments/bdna2s/facebooks_ad_algorithm_is_racist_i_dont_agree_or/,shivilsachdeva,1555373397,[removed],0,1,False,self,,,,,
1039,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,9,bdnm21,self.MachineLearning,"[N] WeWork, OpenAI headline $200 million plan for 30-acre jewel of Presidio",https://www.reddit.com/r/MachineLearning/comments/bdnm21/n_wework_openai_headline_200_million_plan_for/,milaworld,1555375423,"*Now we have more details about how OpenAI is using the proceeds of OpenAI LP ""Capped-Profit"" investment vehicle :)*

*From the [article](https://www.bizjournals.com/sanfrancisco/news/2019/03/15/wework-openai-redevelop-presidio-fort-scott.html):*

WeWork's parent company, artificial intelligence company OpenAI and the World Economic Forum have teamed up on a proposal to redevelop and renovate the 22 barracks and buildings of the Presidio's Fort Scott campus.

The We Company's space would be split between ""mission-driven"" companies  those ""focused on addressing the significant environmental and/or social challenges of our time,"" as specified in the Presidio Trust's Requests for Proposals  and the EPIC Institute, a nonprofit funded by the California Clean Energy Fund, a private equity and VC firm. The Lela Goren Group, a woman-owned and operated real estate developer, would be the co-developer along with The We Co.

Site plans call for artificial intelligence company OpenAI to lease about 107,000 square feet of space for its offices and labs, many of which are adjacent to two office buildings previously developed by the Presidio Trust and currently occupied by the World Economic Forum.

https://www.bizjournals.com/sanfrancisco/news/2019/03/15/wework-openai-redevelop-presidio-fort-scott.html",25,81,False,self,,,,,
1040,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,9,bdnpks,self.LanguageTechnology,BERT cross-lingual embeddings for zero-shot classification,https://www.reddit.com/r/MachineLearning/comments/bdnpks/bert_crosslingual_embeddings_for_zeroshot/,mathcircler,1555376015,,0,1,False,default,,,,,
1041,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,10,bdo8n3,self.MachineLearning,[N] AI Robot Football Tournament (2019 World Cyber Games),https://www.reddit.com/r/MachineLearning/comments/bdo8n3/n_ai_robot_football_tournament_2019_world_cyber/,NerfHanaSong,1555379189,"If you're a programmer who's interested in participating in an international AI competition, this could be your chance!

https://i.redd.it/uip2bhf56js21.jpg

More details about the competition here: [http://www.wcg.com/new-horizons/view/AI-Masters](http://www.wcg.com/new-horizons/view/AI-Masters)",0,5,False,self,,,,,
1042,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,11,bdokex,self.MachineLearning,[R] https://arxiv.org/abs/1904.07200 A Discussion on Solving Partial Differential Equations using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bdokex/r_httpsarxivorgabs190407200_a_discussion_on/,captaincyypher,1555381176,I am delighted to announce that I have submitted my first-ever preprint to arXiv. Any feedback would be highly appreciated.,19,43,False,self,,,,,
1043,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,12,bdpb2o,self.MachineLearning,[R] Making neural machine reading comprehension faster,https://www.reddit.com/r/MachineLearning/comments/bdpb2o/r_making_neural_machine_reading_comprehension/,dchatterjee172,1555386055,"This is a small work I did for my MSc final year project.  
[https://arxiv.org/abs/1904.00796](https://arxiv.org/abs/1904.00796)

Would love to read your responses.",0,2,False,self,,,,,
1044,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,12,bdpd2t,self.MachineLearning,Natural language processing NLP remove filler words,https://www.reddit.com/r/MachineLearning/comments/bdpd2t/natural_language_processing_nlp_remove_filler/,philippatt,1555386456,[removed],0,1,False,self,,,,,
1045,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,13,bdpjj7,self.MachineLearning,"CVPR 2019 paper statistics and visualization, 20 paper one-page review!",https://www.reddit.com/r/MachineLearning/comments/bdpjj7/cvpr_2019_paper_statistics_and_visualization_20/,hoya012,1555387708,[removed],0,1,False,https://b.thumbs.redditmedia.com/iZ56CHLntOssVijP1Ymw9Ga7dQwOmn8r9pKofapYTZg.jpg,,,,,
1046,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,13,bdpn7f,guillaume-chevalier.com,LSTMs for Human Activity Recognition,https://www.reddit.com/r/MachineLearning/comments/bdpn7f/lstms_for_human_activity_recognition/,GChe,1555388435,,0,1,False,default,,,,,
1047,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,13,bdptq3,self.MachineLearning,Machine Learning Lectures,https://www.reddit.com/r/MachineLearning/comments/bdptq3/machine_learning_lectures/,Nishant2901,1555389810,[removed],0,1,False,self,,,,,
1048,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,14,bdq2j1,i.redd.it,OctConv is a simple replacement for the traditional convolution operation that gets better accuracy with fewer FLOPs ( http://snip.ly/8rvu5y ),https://www.reddit.com/r/MachineLearning/comments/bdq2j1/octconv_is_a_simple_replacement_for_the/,ai-lover,1555391686,,1,1,False,default,,,,,
1049,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,15,bdqf1u,self.MachineLearning,[R] Join our Live Lunch &amp; Learn: Structured Neural Summarization (Tue April 16 12:00 EST),https://www.reddit.com/r/MachineLearning/comments/bdqf1u/r_join_our_live_lunch_learn_structured_neural/,tdls_to,1555394469,"Live stream &amp; paper:

[https://aisc.a-i.science/events/2019-04-16/](https://aisc.a-i.science/events/2019-04-16/)",2,1,False,self,,,,,
1050,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,15,bdqgw1,self.MachineLearning,Good books for NLP?,https://www.reddit.com/r/MachineLearning/comments/bdqgw1/good_books_for_nlp/,gevezex,1555394887,[removed],0,1,False,self,,,,,
1051,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,15,bdqi6q,self.MachineLearning,Medical/Healthcare conferences apart from MICCAI/ISBI/IPMI?,https://www.reddit.com/r/MachineLearning/comments/bdqi6q/medicalhealthcare_conferences_apart_from/,cbsudux,1555395221,[removed],0,1,False,self,,,,,
1052,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,15,bdqpl3,visionetsystems.com,How is machine learning making vending machines smarter?,https://www.reddit.com/r/MachineLearning/comments/bdqpl3/how_is_machine_learning_making_vending_machines/,iammarksmith,1555396862,,0,1,False,default,,,,,
1053,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,16,bdqvow,codeingschool.com,Regression - Training and Testing,https://www.reddit.com/r/MachineLearning/comments/bdqvow/regression_training_and_testing/,subhamroy021,1555398254,,0,1,False,default,,,,,
1054,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,17,bdrhwr,self.MachineLearning,Looking for project partners to start a project on github (C++ on machine learning),https://www.reddit.com/r/MachineLearning/comments/bdrhwr/looking_for_project_partners_to_start_a_project/,agastya_,1555403495,[removed],0,1,False,self,,,,,
1055,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,18,bdrq8u,self.MachineLearning,"""Hidden Markov model"" for continuous densities?",https://www.reddit.com/r/MachineLearning/comments/bdrq8u/hidden_markov_model_for_continuous_densities/,iqueerified,1555405642,[removed],0,1,False,self,,,,,
1056,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,18,bdrqno,self.MachineLearning,Using CRF (conditional random fields) for outside of image segmentation,https://www.reddit.com/r/MachineLearning/comments/bdrqno/using_crf_conditional_random_fields_for_outside/,AppleNamu,1555405739,[removed],0,1,False,self,,,,,
1057,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,19,bds63p,blog.openmined.org,Encrypted Deep Learning Classification with PyTorch and PySyft,https://www.reddit.com/r/MachineLearning/comments/bds63p/encrypted_deep_learning_classification_with/,iamtrask,1555409282,,0,1,False,default,,,,,
1058,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,19,bdsdlf,self.MachineLearning,Helping optimize ANN in R,https://www.reddit.com/r/MachineLearning/comments/bdsdlf/helping_optimize_ann_in_r/,ReedSacer,1555410948,[removed],0,1,False,https://a.thumbs.redditmedia.com/ogJpzxv_6DTp_yGr0CUKJADG4XMnxh9GWjp8Tm_PTa0.jpg,,,,,
1059,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,19,bdsepr,self.MachineLearning,Super-beginner resources for Pytorch Image Processing,https://www.reddit.com/r/MachineLearning/comments/bdsepr/superbeginner_resources_for_pytorch_image/,xstheknight,1555411198,[removed],0,1,False,self,,,,,
1060,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,19,bdsf7b,self.MachineLearning,Text outlier detection with/without clustering,https://www.reddit.com/r/MachineLearning/comments/bdsf7b/text_outlier_detection_withwithout_clustering/,Moni93,1555411308,"I am working on a use case where I want to identify outliers on a given text dataset. I am wondering which one of these two methods (below) will identify outliers more accurtaly?

* First identify clusters and then find outliers Per cluster , in other words,  each cluster is going to be considered as a corpus.

OR

*  Identify outliers by working with the whole data as a unique corpus.",0,1,False,self,,,,,
1061,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,19,bdsgkn,self.MachineLearning,The Society of Mind 30+ years later,https://www.reddit.com/r/MachineLearning/comments/bdsgkn/the_society_of_mind_30_years_later/,cavedave,1555411619," I found a copy of the Society of Mind by Minsky [here](http://www.acad.bg/ebook/ml/Society%20of%20Mind.pdf)

AI has not gone this way since. Faster computation and better algorithms have each contributed about the same improvement. Better bigger datasets have also been a huge deal.

The big AI breakthroughs Deep Blue, Watson, AlphaGo, Alphafold\*, self driving cars, image recognition have not come from either agents or encoding human expertise into algorithms. But from better data and faster processing.  
The Bitter Lesson by Rich Sutton is good on how improving datasets, algorithms and hardware has improved AI [http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

&amp;#x200B;

 In T[he book of Why Pearl](https://www.nytimes.com/2018/06/01/business/dealbook/review-the-book-of-why-examines-the-science-of-cause-and-effect.html) talks about the [scruffies versus the neats](https://books.google.ie/books?id=BzM0DwAAQBAJ&amp;printsec=frontcover&amp;dq=the+book+of+Why+Pearl&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi2uY7ItdThAhVYUhUIHWcPD1MQ6AEIKDAA#v=onepage&amp;q=neat&amp;f=false) and why the scruffies that just get things to work are in the ascendant at the moment. I am probably being unfair to Minsky here as I read his book 20 years ago. But I read it as more about finding underlying principles of cognition that we would put into use. And I do not see many cases where we have.   


But how much of Minsky's vision has happened? And will more happen in future?

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

\*This is arguable as there was a good amount of NLP in the original Watson. Or that the Alphas are doing similar hierarchical reasoning to what Minsky talked about.",13,42,False,self,,,,,
1062,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,19,bdsjvd,self.MachineLearning,Classifer showing same result irrespective of the output,https://www.reddit.com/r/MachineLearning/comments/bdsjvd/classifer_showing_same_result_irrespective_of_the/,alpha2311,1555412330,[removed],0,1,False,self,,,,,
1063,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,20,bdsmyb,self.MachineLearning,What should we do with 70fps real-time object detection? (using Google's Coral Dev Board with Edge TPU),https://www.reddit.com/r/MachineLearning/comments/bdsmyb/what_should_we_do_with_70fps_realtime_object/,paul_read_it,1555412943,[removed],0,1,False,self,,,,,
1064,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,20,bdsqp5,lhd.co.com,"LHD S.p.A. designs and builds Telescopic Forks &amp; Stacker Cranes. Our Load Handling Devices are used in Automatic Warehouses, Pallet Handling and Automotive.",https://www.reddit.com/r/MachineLearning/comments/bdsqp5/lhd_spa_designs_and_builds_telescopic_forks/,lhd121,1555413673,,0,1,False,default,,,,,
1065,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,20,bdt0iy,self.MachineLearning,Chatbot solutions,https://www.reddit.com/r/MachineLearning/comments/bdt0iy/chatbot_solutions/,frankmarlon,1555415570,[removed],0,1,False,self,,,,,
1066,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,20,bdt1pz,self.MachineLearning,Papers on Pseudo-labeling and Semi-supervized learning,https://www.reddit.com/r/MachineLearning/comments/bdt1pz/papers_on_pseudolabeling_and_semisupervized/,jodie_vision,1555415798,"Hi,

I need to write an overview paper about pseudo labeling and I need help finding literature. I came across these papers:

[Realistic Evaluation of Deep Semi-Supervised
Learning Algorithms](https://arxiv.org/pdf/1804.09170.pdf)

[Pseudo-Label : The Simple and Efficient Semi-Supervised Learning
Method for Deep Neural Networks](http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf)

[Entropy Regularization](https://pdfs.semanticscholar.org/1ee2/7c66fabde8ffe90bd2f4ccee5835f8dedbb9.pdf)

&amp;#x200B;

Can you suggest some more papers on these topics or a book, article and similar?

&amp;#x200B;

Thanks in advance",0,1,False,self,,,,,
1067,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,21,bdtbr6,uberant.com,Boost Your Machine Learning Training with These Tips,https://www.reddit.com/r/MachineLearning/comments/bdtbr6/boost_your_machine_learning_training_with_these/,Divya123divya,1555417561,,0,1,False,default,,,,,
1068,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,21,bdte9f,self.MachineLearning,NLP Researchers: what is your workflow?,https://www.reddit.com/r/MachineLearning/comments/bdte9f/nlp_researchers_what_is_your_workflow/,MasterScrat,1555418002,[removed],0,1,False,self,,,,,
1069,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,21,bdtegv,self.MachineLearning,[D] NLP Researchers: what is your workflow?,https://www.reddit.com/r/MachineLearning/comments/bdtegv/d_nlp_researchers_what_is_your_workflow/,MasterScrat,1555418039,"I started working on the [Toxicity Detection Kaggle challenge](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/leaderboard). I'd be curious to know how people familiar with NLP handle this kind of problems. 

My problem is that I want to try things out, but running anything on the full model takes hours, and running on a subset of the data doesn't always tell me how good the result is.

I have multiple computers and GPUs at home so I juggle between them but my workflow is generally a mess. It's difficult to keep track of what I'm doing, what experiments I should prioritize, optimize hyperparameters etc.

Is there an all-in-one solution out there to manage my experiments? I'm starting to experiment with Kubernetes but it's a lot of overhead to configure and run jobs.

I guess I could use a beefier tool like Kubeflow but I'm wondering if that wouldn't be too big of a tool for the job.

How are professionals handling things?",12,34,False,self,,,,,
1070,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,21,bdtf32,indico.cern.ch,'Gauge Fields and DL' - Max Welling @ CERN,https://www.reddit.com/r/MachineLearning/comments/bdtf32/gauge_fields_and_dl_max_welling_cern/,tensorflower,1555418149,,1,1,False,default,,,,,
1071,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,21,bdtmgh,self.MachineLearning,[P] I used a Variational Autoencoder to build a feature-based face editing software,https://www.reddit.com/r/MachineLearning/comments/bdtmgh/p_i_used_a_variational_autoencoder_to_build_a/,Xayo,1555419426,"Hey reddit,

In my latest weekend-project I have been using a Variational Autoencoder to build a feature-based face editor. The model is explained in my youtube video:

https://youtu.be/uszj2MOLY08

You can inspect the code at Github:

https://github.com/SteffenCzolbe/FeatureTransferApp

The feature editing is based on modifying the latent distribution of the VAE. After training of the VAE is completed, the latent space is mapped by encoding the training data once more. Latent space vectors of each feature are determined based on the labels of the training data. Then to edit an image, we can add a combination of feature vectors to its latent distribution, and then reconstruct it. The reconstruction creates an altered version of the original image, based on the featrures we added to the latent representation.

The model used is heavily inspired by the Bate-VAE used in this paper by google deepmind (https://pdfs.semanticscholar.org/a902/26c41b79f8b06007609f39f82757073641e2.pdf). I made some adjustments to it to incorporate more recent advancements in neural network architecture, like using a Leaky ReLu activation function. The dataset used is celebA, which consist of 200.000 annotated images of celebrities. I aligned and cropped the images to a 64x64 resolution before training. The model is implememted in PyTorch, and PyGame has been used for the GUI. Training on my single consumer grade GPU took about 1:30h. The finished application, inducing the trained model, runs smoothly even without GPU support.

This project has been quite cool, playing with the result has been good fun. I got a lot of hands-on experience with VAEs. Creating a YouTube video explaining the project let me to learn much more about video editing and presentation techniques. I'm testing the waters with presenting this project in video form, lets see if it pays off!",15,156,False,self,,,,,
1072,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,22,bdtuei,self.MachineLearning,Using FS based on SVM in ANN model,https://www.reddit.com/r/MachineLearning/comments/bdtuei/using_fs_based_on_svm_in_ann_model/,lola9492,1555420686,[removed],0,1,False,self,,,,,
1073,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,22,bdtuta,self.MachineLearning,Need help guys.,https://www.reddit.com/r/MachineLearning/comments/bdtuta/need_help_guys/,Midniteb0ne,1555420756,[removed],0,1,False,self,,,,,
1074,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,22,bdty5y,self.MachineLearning,"[D]Does AI have a dirty mind, too? Adversarial NSFW examples for humans and AI alike.",https://www.reddit.com/r/MachineLearning/comments/bdty5y/ddoes_ai_have_a_dirty_mind_too_adversarial_nsfw/,MarekCichy,1555421298,"Can seemingly NSFW images can fool AIs as well as humans? To answer this, I gathered 50 popular images with NSFW optical illusions and fed them to 8 publicly available image detectors. Results? Yes, they are foolable. Read more in my [Medium article](https://medium.com/@marekkcichy/does-ai-have-a-dirty-mind-too-6948430e4b2b) and contribute with your input.",15,11,True,nsfw,,,,,
1075,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,22,bdu8vx,wired.com,Googles AI Experts Try to Automate Themselves,https://www.reddit.com/r/MachineLearning/comments/bdu8vx/googles_ai_experts_try_to_automate_themselves/,antgoldbloom,1555423034,,0,1,False,https://b.thumbs.redditmedia.com/fb-yn-pzq7lOqhTn0haquMbpE6vQUXmv_-WmTEJV24I.jpg,,,,,
1076,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,23,bduorw,youtube.com,Machine Learning Basics - Quick Explanation,https://www.reddit.com/r/MachineLearning/comments/bduorw/machine_learning_basics_quick_explanation/,MainBuilder,1555425556,,0,1,False,https://b.thumbs.redditmedia.com/oaQjE0Gu5zlJwhsKRhvksp7bWd4jnVKJwaYV36t-dOM.jpg,,,,,
1077,MachineLearning,t5_2r3gv,2019-4-16,2019,4,16,23,bdupbs,self.MachineLearning,Soft Actor-Critic implementation in Tensorflow 2.0,https://www.reddit.com/r/MachineLearning/comments/bdupbs/soft_actorcritic_implementation_in_tensorflow_20/,Fable67,1555425642,[removed],0,1,False,self,,,,,
1078,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,0,bduy8r,self.MachineLearning,[P] Using Tacotron To Make Ben Shapiro Sing,https://www.reddit.com/r/MachineLearning/comments/bduy8r/p_using_tacotron_to_make_ben_shapiro_sing/,hanyuqn,1555426988,"https://www.youtube.com/watch?v=Y2uKVhATv68

100% of the vocals here were generated by my model, not spoken by Ben Shapiro himself, and do not reflect Shapiro's views. Shapiro's voice was created with a TTS model I trained using my implementation of the papers ""Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis"" (https://arxiv.org/abs/1803.09017) and ""Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron"" (https://arxiv.org/abs/1803.09047), using only just over 2 hours of Shapiro audio (though I suppose that's more like 3-4 hours worth of speech for the average person). After learning Shapiro's speech patterns it's amusing that the speech generated by this model is even faster than the average speed Eminem raps this song (only the part at 3:12 is sped up 1.5x).",6,16,False,self,,,,,
1079,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,0,bduyh9,self.MachineLearning,[D] How do you think about deep learning researcher in the company?,https://www.reddit.com/r/MachineLearning/comments/bduyh9/d_how_do_you_think_about_deep_learning_researcher/,sjh9020,1555427024,"I think pure researchers will be gone except for google or facebook, since deep learning frameworks are designed and updated for easy use. I guess deep learning will be normal tech in the future.",3,0,False,self,,,,,
1080,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,0,bdv51x,apriorit.com,Applying Long Short-Term Memory for Video Classification Issues,https://www.reddit.com/r/MachineLearning/comments/bdv51x/applying_long_shortterm_memory_for_video/,RyanTmthn,1555427945,,0,1,False,default,,,,,
1081,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,0,bdvbrh,self.MachineLearning,Q&amp;A with Google DeepMinds Jane Wang,https://www.reddit.com/r/MachineLearning/comments/bdvbrh/qa_with_google_deepminds_jane_wang/,TechTyranny,1555428871,[removed],0,1,False,self,,,,,
1082,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,0,bdve5x,self.MachineLearning,"I couldnt find a good resource for data scientists to learn Linux/shell scripting, so I made a cheat sheet and uploaded three hours of lessons. Enjoy!",https://www.reddit.com/r/MachineLearning/comments/bdve5x/i_couldnt_find_a_good_resource_for_data/,drrelyea,1555429208,[removed],0,1,False,self,,,,,
1083,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,0,bdviis,self.MachineLearning,"[D] I couldnt find a good resource for data scientists to learn Linux/shell scripting, so I made a cheat sheet and uploaded three hours of lessons. Enjoy!",https://www.reddit.com/r/MachineLearning/comments/bdviis/d_i_couldnt_find_a_good_resource_for_data/,drrelyea,1555429829,"Ive taught Linux/UNIX/shell scripting at my past few jobs and realized I should record lessons and put them online. This is for everyone who wants/needs to learn Linux on the fly. Hopefully its useful.

[The cheat sheet is located here](https://www.dropbox.com/s/k7athu9i8lmmeln/Linux%20Cheat%20Sheet%20David%20Relyea.pdf)

[The three hours of lessons are located here](https://www.youtube.com/playlist?list=PLdfA2CrAqQ5kB8iSbm5FB1ADVdBeOzVqZ)",62,453,False,self,,,,,
1084,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,1,bdvqnr,medium.com,Bengio and Marcus at World AI Summit in Montral,https://www.reddit.com/r/MachineLearning/comments/bdvqnr/bengio_and_marcus_at_world_ai_summit_in_montral/,gwen0927,1555430978,,0,1,False,https://a.thumbs.redditmedia.com/z8JLTe4k0nOZ0pIw5wVsIIkRW0e-4VuCDTbB_c3oRm4.jpg,,,,,
1085,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,2,bdwe1f,self.MachineLearning,Predict ideal set of parameters for a machine?,https://www.reddit.com/r/MachineLearning/comments/bdwe1f/predict_ideal_set_of_parameters_for_a_machine/,Mackelday,1555434302,[removed],0,1,False,self,,,,,
1086,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,2,bdwfyd,ai.googleblog.com,"Take Your Best Selfie Automatically, with Photobooth on Pixel 3",https://www.reddit.com/r/MachineLearning/comments/bdwfyd/take_your_best_selfie_automatically_with/,sjoerdapp,1555434559,,0,1,False,default,,,,,
1087,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,2,bdwkr7,self.MachineLearning,"[Discussion] Is it possible to learn ML ""old"" and with little experience?",https://www.reddit.com/r/MachineLearning/comments/bdwkr7/discussion_is_it_possible_to_learn_ml_old_and/,Carrasco_Santo,1555435225,"I am 37 years old and I have always been interested in technology in general, mainly related to computing. In 2005/2006 I was studying Python in a self-taught mode with 23/24 years old.

Due to the situations that life imposes, I do not work with technology currently (work in the judiciary of Brazil), although I have never stopped reading and inform myself at least a little at all times.

Last year I bought a collection of books by various authors about Python and artificial intelligence (neural networks, deep learning, genetic algorithms, etc.), but I have not started yet, but I'll probably start from the middle of the year onwards.

I believe that in order to study AI and feel motivated to learn we must have some objective in mind, to study by studying, just out of curiosity, I think that in most cases it can lead to disinterest over time, especially if the subject becomes increasingly complex .

In this way, I have an area of interest in applying any knowledge of AI: Blender 3D. Yes, free software that allows 3D modeling, animation, etc. of Blender foundation. I have been practicing Blender since 2009 so I know a lot about the tool and look forward to the release of version 2.8 that will happen in July.

I imagine some kind of application where you talk about objects, for example, AI understands what you want and produces that object, for example, a dog, after studying deep dog images and comparisons with previously made 3D models.

Telling this story, the big question is, since being around 40 years old, would it be a waste of time to study the subject and focus on other things, letting the new generations create things?",7,4,False,self,,,,,
1088,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,2,bdwpfb,self.MachineLearning,[Project] Training models and running Jupyter Notebooks on AWS Spot Instances (cheaper and simpler than SageMaker),https://www.reddit.com/r/MachineLearning/comments/bdwpfb/project_training_models_and_running_jupyter/,apls777,1555435878,"Hi everyone,

&amp;#x200B;

I've developed a tool to simplify training of deep learning models on AWS: [https://github.com/apls777/spotty](https://github.com/apls777/spotty). My goal was to make training on AWS GPU instances as simple as training on a local computer. Spotty automatically manages all necessary AWS resources (AMIs, volumes, snapshots, SSH keys), runs Spot Instances to save up to 70% of the costs and usestmuxto easily detach remote processes from their SSH sessions.

&amp;#x200B;

To train the model (and make it trainable by everyone with a couple of commands), you just need to create 1 configuration file, where you describe a Docker container and AWS instance parameters.

&amp;#x200B;

Then the workflow is super-simple:

1. Use the ""spotty start"" command to start your container on a cheap AWS Spot Instance. Your local project will be uploaded to the instance and available inside the container.
2. Once the instance is up and running, use the ""spotty ssh"" command to connect to the container, or start Jupyter Notebook using the ""spotty run jupyter"" command (it's a custom script from the configuration file).

&amp;#x200B;

Here is an article on how to train a model using Spotty with a real-life example: [https://towardsdatascience.com/how-to-train-deep-learning-models-on-aws-spot-instances-using-spotty-8d9e0543d365](https://towardsdatascience.com/how-to-train-deep-learning-models-on-aws-spot-instances-using-spotty-8d9e0543d365).

&amp;#x200B;

I really hope you will find this tool useful and will be happy to hear any feedback.",5,10,False,self,,,,,
1089,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,2,bdwrdr,youtube.com,"I created a simple backpropagation derivation video; I found it very difficult to understand this derivation it took me 3 months to fully understand it. Anyways, here is the derivation in my words, hope it helps to a fellow beginner. I tried to keep the video length at max 11 minutes.",https://www.reddit.com/r/MachineLearning/comments/bdwrdr/i_created_a_simple_backpropagation_derivation/,gostewpid,1555436155,,0,1,False,default,,,,,
1090,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,3,bdx51d,medium.com,Automated Machine Learning: How do Teams Work Together on an AutoML Project?,https://www.reddit.com/r/MachineLearning/comments/bdx51d/automated_machine_learning_how_do_teams_work/,brunocborges,1555438115,,0,1,False,default,,,,,
1091,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,3,bdx7cw,blog.stephenwolfram.com,Version 12 Launches Today! (And Its a Big Jump for Wolfram Language and Mathematica),https://www.reddit.com/r/MachineLearning/comments/bdx7cw/version_12_launches_today_and_its_a_big_jump_for/,CuttingWithScissors,1555438453,,0,1,False,https://b.thumbs.redditmedia.com/BVpRD2dlrYxViEca2qx4hyzbtd-35hGJ1mtsqfTTe7Y.jpg,,,,,
1092,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,3,bdxg32,self.MachineLearning,Creating a Computer Vision Startup Based on applications of GANs,https://www.reddit.com/r/MachineLearning/comments/bdxg32/creating_a_computer_vision_startup_based_on/,MWildBeast,1555439691,"I was thinking of learning Django, and also combining something interesting related to computer Vision to it. So a website or application, basically.
Can you suggest something from the list https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900 or if you have any other ideas, to use to develop a product out of it.
Currently I thought of Neural Style Transfer and Emoji from Faces from these two 
I want something which people can use. 
PS: Maybe turn it into a small startup",0,1,False,self,,,,,
1093,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,3,bdxohy,self.MachineLearning,[D]My Machine Learning Journal #11: Getting the macro view of reinforcement learning and more OpenAI gym stuff,https://www.reddit.com/r/MachineLearning/comments/bdxohy/dmy_machine_learning_journal_11_getting_the_macro/,RedditAcy,1555440887,"Wow guys, thanks for the huge support in my last post! 

Here's the vlog version of this journal (should I call it Vournal?) as usual: [https://youtu.be/N7KThxV5maI](https://youtu.be/N7KThxV5maI)

Anyways, let me cut straight to the chase and provide as much value as possible.

I spent a few hours researching just what the heck is going on with RL. I debriefed the most starred GitHub projects, the SOTA (state of the art) algorithms, and the open source platforms. I compiled all of this into a [google doc](https://docs.google.com/document/d/1jaYoQBoFq2KIc0u55eKuo8ZKi3cwtLXeeBWCxoHFUg4/edit?usp=sharing), but I think [this GitHub repository](https://github.com/aikorea/awesome-rl) contains even more information. Generally speaking, RL should be used whenever a problem can be modeled with an agent, environment, and reward setup. This is cool, because technically speaking we can train a lot of models with an RL mindset. For example, a traditional GAN has a generator and a discriminator, the generator is trained by how much it **fooled** the discriminator. With an RL mindset, we can define the generator as the agent, its state being the random strokes it painted, and the rewards being the degree it fooled the discriminator. Still the same workflow, but now we introduce more possibilities, we can tune the reward algorithms &amp; RL training mechanisms, etc. You can find more about RL GANs [here](https://hub.packtpub.com/how-googles-deepmind-is-creating-images-with-artificial-intelligence/).

My general plan for learning RL is to implement gym games first, then playing around with complex environments like [Project Malmo](https://github.com/Microsoft/malmo) and [ViZDoom](https://github.com/Microsoft/malmo), and at last, I will get onto the Unity RL env and making my own game and training my own RL agent to beat it!

So right now, let me implement a few gym games first, for this time, I am trying to beat Cartpole &amp; Acrobot. 

[Cartpole, objective is to balance the stick so it doesn't tilt over 15 degrees](https://i.redd.it/2cqqc5l9hns21.jpg)

Since I copied and understood the atari breakout code, I thought this was a piece of cake! Interestingly, the observations of the environment turned out to be an array with 4 floats while I thought it was going to be a picture (like in Atari). That changes my game plan, I can't just use the Atari code for this, because in Atari, we predict an action by feeding in a 210 \* 180 \* 3 image, which is the state, that image goes through a few Conv layers, connects to a Dense layer with 4 outputs, which represents the actions we can choose (we will choose the output that had the highest value because that represents the one that will yield the highest reward). But a cartpole state is a 4 \* 1 array, so I decided to feed it through a random neural network and connected it to a dense layer with 2 outputs at the end. 

But I still ended up using a great portion of [gsurma's code](https://github.com/gsurma/cartpole). The reason being that the original Atari code wasn't the best, it is intuitive to do this: 

    class AtariSolver:
        def __init__(...): 
            define the model structure
        def saveMemory(...): 
            self.memory.append the current state, reward, etc so we can train the model with the memory         
        variable later
        def getAction(...): 
             ...

But the Atari code scattered all of these across places, they weren't contained inside of a single class, which would make **a lot of sense.** Here's [my GitHub repository](https://github.com/BlastWind/cartpole) that beat Cartpole, again, huge credits to gsurma.

&amp;#x200B;

Now onto my **nemesis: the Acrobot.** 

*Processing gif 6girxjaykns21...*

The observation of an acrobot is an array with 6 floats, so I thought I could've beat it with the same approach with Cartpole and use the Deep Q-Learning algorithm to train the RL agent. 

I was wrong! I am still trying to motivate this skinny blue dude to cross the black line (the objective of the game)! The agent gets rewarded with negative points each frame the RL agent cannot reach the black line. **The problem is, a lot of negative points won't help on making the best choice if the agent never experienced a positive reward.** **And since the default Deep Q-Learning algorithm decreases the exploration rate over time, the agent will try random stuff less and less.** That is at least what I am feeling. I ran this model for 100 iterations and all of it terminated because it reached the maximum 500 timeframes. 

It's okay, the next time I will be reporting back to you, I will have defeated this skinny blue dude. I looked into something that might help me, on the [openAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard), someone beat acrobot with an algorithm called PPO (proximal policy optimization). It seems really hard to understand mathematically, but I will understand it, beat acrobot, and share it with you next time!",1,1,False,self,,,,,
1094,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,4,bdxzc9,self.MachineLearning,[P] TensorFlow 2.0 Implementation of Yolo V3 Object Detection Network,https://www.reddit.com/r/MachineLearning/comments/bdxzc9/p_tensorflow_20_implementation_of_yolo_v3_object/,zzh8829,1555442404,"Hey reddit r/ml, I am sharing my implementation of YoloV3 in TensorFlow 2.0 alpha

[https://github.com/zzh8829/yolov3-tf2](https://github.com/zzh8829/yolov3-tf2)

&amp;#x200B;

There is a lot of buzz around TensorFlow 2.0 with tons of blog posts and tutorial. But I haven't found a complete example that uses all the latest features and best practices brought by TF2. This project is created with the goal of being clean, efficient and complete with zero legacy debts.

&amp;#x200B;

Some of the key features include:

* Everything is Tensorflow 2.0, no more `session.run` or `import keras.backend as K`
* Pure functional model definition compatible with both Eager and Graph execution
* Eager mode custom training loop with `tf.GradientTape` (very good for debugging)
* Graph mode high performance training with `model.fit(dataset)`
* Training pipeline uses [`tf.data`](https://tf.data) and `TFRecord` for optimal throughput
* Input transformations are implemented using the `@tf.function` auto-graph feature
* Almost all tensor manipulations are vectorized to squeeze out that last bit of efficiency
* Works with GPU out of box (TF2's GPU integration is miles ahead of PyTorch's `if gpu: x.cuda()`)
* Fully integrated with `absl-py`. TensorFlow 2.0 is deprecating `tf.flags` and recommends abseil (great library, heavily used by Google)
* I haven't gotten chance to test multi-gpu or distributed setup, but they are supposedly very easy to do with TF2.0.

&amp;#x200B;

The YoloV3 implementation is mostly referenced from the [origin paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf), original [darknet](https://github.com/pjreddie/darknet) with inspirations from many existing code written in PyTorch, Keras and TF1 (I credited them at the end of the README). I tried to fixed all the inconsistency, incompleteness and minor errors existing in other repos here. The project works with both YoloV3 and YoloV3-Tiny and is compatible with pre-trained darknet weights.

&amp;#x200B;

Example of detection output:

[Thumbs Up!](https://i.redd.it/8lvav2fmeos21.jpg)

This project has been quite a great learning experience for me. After working with TF1 and then Keras and then PyTorch, coming back to TensorFlow 2.0 feels very refreshing and enjoyable. TF2 will definitely rise and shine in the coming months following the official GA release.",48,217,False,https://b.thumbs.redditmedia.com/orSBX0D4OPlhISI4Nxqoi3w40KuCeHhJvP0-Oi2t8QU.jpg,,,,,
1095,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,4,bdya3y,medium.com,Google Coral Edge TPU vs NVIDIA Jetson Nano: A quick deep dive into EdgeAI performance,https://www.reddit.com/r/MachineLearning/comments/bdya3y/google_coral_edge_tpu_vs_nvidia_jetson_nano_a/,bartturner,1555443947,,0,1,False,https://a.thumbs.redditmedia.com/1V7eEn5nTQPmpaQcy1ssYFlMZaTmepBnkef6pDg3pR0.jpg,,,,,
1096,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,4,bdyaq4,self.datascience,Thoughts on measuring economic impacts of illegal activity particularly related to human trafficking,https://www.reddit.com/r/MachineLearning/comments/bdyaq4/thoughts_on_measuring_economic_impacts_of_illegal/,ReadEditName,1555444035,,0,1,False,https://b.thumbs.redditmedia.com/L9zYuYy039IpMZTN03cRk2i3QYDhnlxejh_nRbR02FM.jpg,,,,,
1097,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,5,bdykpu,self.MachineLearning,Style transfer using keras,https://www.reddit.com/r/MachineLearning/comments/bdykpu/style_transfer_using_keras/,itsron143,1555445469,[removed],0,1,False,self,,,,,
1098,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,5,bdyqxz,self.MachineLearning,Topic modelling with NLTK and Gensim -- getting different topics with each run for same textbook,https://www.reddit.com/r/MachineLearning/comments/bdyqxz/topic_modelling_with_nltk_and_gensim_getting/,ASPNetthrow,1555446361,"I'm using the examples from here:

https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21

to do topic modelling on some of my company's text books. I've gotten it to work, but I'm getting a different set of topics for each run on the same textbook. Is this the normal state of affairs for topic modelling? 

For instance, one number one:

    (0, '0.073*""student"" + 0.041*""develop"" + 0.030*""explicit"" + 0.026*""explain""')
    (1, '0.076*""question"" + 0.064*""energy"" + 0.045*""guide"" + 0.044*""solution""')
    (2, '0.070*""material"" + 0.035*""idea"" + 0.029*""equipment"" + 0.022*""light""')

Run number two:

    (0, '0.042*""standard"" + 0.028*""alignment"" + 0.021*""mathematics"" + 0.021*""conservation""')
    (1, '0.038*""table"" + 0.030*""review"" + 0.024*""require"" + 0.024*""valid""')
    (2, '0.053*""water"" + 0.048*""change"" + 0.040*""object"" + 0.033*""safety""')",1,1,False,self,,,,,
1099,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,6,bdz6ty,aidungeon.io,"Announcing AIDungeon.io, a deep learning generated text adventure that you can try out in your browser!",https://www.reddit.com/r/MachineLearning/comments/bdz6ty/announcing_aidungeonio_a_deep_learning_generated/,AIDungeon,1555448651,,0,1,False,default,,,,,
1100,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,6,bdzb9t,aidungeon.io,"[P] Announcing AIDungeon.io, a deep learning generated text adventure that you can try out in your browser!",https://www.reddit.com/r/MachineLearning/comments/bdzb9t/p_announcing_aidungeonio_a_deep_learning/,InterstellarRun,1555449283,,0,1,False,default,,,,,
1101,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,6,bdzg3z,self.MachineLearning,Auto-ML framework for multi-label classification?,https://www.reddit.com/r/MachineLearning/comments/bdzg3z/automl_framework_for_multilabel_classification/,XmintMusic,1555449998,[removed],0,1,False,self,,,,,
1102,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,6,bdzqvb,medium.com,"Boston Dynamics Spot Toughens Up, Hauls a Truck",https://www.reddit.com/r/MachineLearning/comments/bdzqvb/boston_dynamics_spot_toughens_up_hauls_a_truck/,gwen0927,1555451605,,0,1,False,https://b.thumbs.redditmedia.com/n8fWzt_9ErUJ-8U3FfemvnI1yrL3h0DlTYa6mSBrHSQ.jpg,,,,,
1103,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,7,be00jp,self.MachineLearning,"[P] Convolutional networks with NumPy, or let's learn how a CNN really works!",https://www.reddit.com/r/MachineLearning/comments/be00jp/p_convolutional_networks_with_numpy_or_lets_learn/,cosmic-cortex,1555453042,"Although I have spent quite a lot of time recently with CNNs for image classification and semantic segmentation, I have realized that to obtain a deep understanding of them, I have to make one on my own from scratch. So, I put down PyTorch, my go-to framework, and created an implementation using NumPy only :)

&amp;#x200B;

The result can be found here: [https://github.com/cosmic-cortex/neural-networks-from-scratch](https://github.com/cosmic-cortex/neural-networks-from-scratch)

&amp;#x200B;

Basically, it is a mini deep learning framework, so one can easily experiment with different architectures. Currently, the following components are supported.

Layers:

* Linear
* Conv2D
* BatchNorm2D
* MaxPool2D
* Flatten (technically, this is not a layer, since it just flattens a 2D input, but it was very convenient to implement this as one)

Loss functions:

* CrossEntropyLoss
* MeanSquareLoss

Activation functions:

* ReLU
* Leaky ReLU
* Sigmoid

There are two examples as well, [a simple multilayer perceptron](https://github.com/cosmic-cortex/neural-networks-from-scratch/blob/master/mlp.py) and [a basic CNN on MNIST classification](https://github.com/cosmic-cortex/neural-networks-from-scratch/blob/master/cnn_mnist.py), but [custom datasets are supported as well](https://github.com/cosmic-cortex/neural-networks-from-scratch#CNN-example), if you would like to experiment on your own data.

&amp;#x200B;

I have to say, I have really enjoyed this ride! It was extremely instructional, moreover I have discovered several mindblowing details, for instance that the gradient for convolution is a transpose convolution operator :) Truly recommended for everyone in DL/ML to try doing the same. During this venture, the fantastic [CS231n](http://cs231n.github.io) course was very helpful, so this is also recommended.",12,11,False,self,,,,,
1104,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,8,be0tvy,self.MachineLearning,[D] FID and Inception Score implementations in TF Eager,https://www.reddit.com/r/MachineLearning/comments/be0tvy/d_fid_and_inception_score_implementations_in_tf/,gokstudio,1555457730,"Hi everyone,

I'm looking for implementations of FID, Inception Score and other GAN evaluation metrics in TF Eager.

The bundled tf.contrib.gan.eval.\* methods seem to choke with eager execution enabled. 

&amp;#x200B;

Any pointers to snippets or repos?  


Thanks",1,2,False,self,,,,,
1105,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,8,be0vl8,self.MachineLearning,The implication of machine learning on regulation - see the section on adversarial attacks.,https://www.reddit.com/r/MachineLearning/comments/be0vl8/the_implication_of_machine_learning_on_regulation/,OppositeMidnight,1555457996,[removed],0,1,False,self,,,,,
1106,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,8,be0wzf,self.MachineLearning,[D] Is it possible to input vectors as individual input features?,https://www.reddit.com/r/MachineLearning/comments/be0wzf/d_is_it_possible_to_input_vectors_as_individual/,xk86,1555458227,"Sorry if these types of posts aren't allowed but I didn't get any replies on /r/learnmachinelearning. 

&amp;#x200B;

Basically, is it possible to group together input features in scikit so that rather than reading a single variable, the input feature is a vector? This would make the shape of X a 3D array rather than 2D . Is there a term for this that I can search for or can someone point me in the right direction? I want to use it for simple ML algorithms like random forest or XGBOOST.",9,3,False,self,,,,,
1107,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,8,be0xqm,aidungeon.io,[P] Check out the AI generated text adventure I made with OpenAI's GPT-2 Model,https://www.reddit.com/r/MachineLearning/comments/be0xqm/p_check_out_the_ai_generated_text_adventure_i/,InterstellarRun,1555458345,,2,4,False,default,,,,,
1108,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,8,be105k,self.MachineLearning,[P] The best 4-GPU deep learning rig for $7000,https://www.reddit.com/r/MachineLearning/comments/be105k/p_the_best_4gpu_deep_learning_rig_for_7000/,cgnorthcutt,1555458737,"## Project link: [http://l7.curtisnorthcutt.com/the-best-4-gpu-deep-learning-rig](http://l7.curtisnorthcutt.com/the-best-4-gpu-deep-learning-rig)

Hi Reddit! This is a follow-up to the previous post [\[P\] I built Lambda's $12,500 deep learning rig for $6200](http://l7.curtisnorthcutt.com/build-pro-deep-learning-workstation) which had around 480 upvotes on Reddit. In response to the hundreds of comments on that post, I built and tested multiple additional 4-GPU rigs. I'm back to share the ""perfect"" 4-GPU deep learning rig with the highest performance and reliability, no thermal throttling, and lowest cost. This build is nearly identical to [Lambda's 4-GPU workstation](https://lambdalabs.com/deep-learning/workstations/4-gpu/premium/customize), but costs around $4000 cheaper. Happy building!",0,1,False,self,,,,,
1109,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,9,be15we,self.MachineLearning,[P] I built Lambda's 4-GPU deep learning rig for $4000 cheaper,https://www.reddit.com/r/MachineLearning/comments/be15we/p_i_built_lambdas_4gpu_deep_learning_rig_for_4000/,cgnorthcutt,1555459702,"Project link: [http://l7.curtisnorthcutt.com/the-best-4-gpu-deep-learning-rig](http://l7.curtisnorthcutt.com/the-best-4-gpu-deep-learning-rig)

Hi Reddit! This is a follow-up to the previous post [\[P\] I built Lambda's $12,500 deep learning rig for $6200](https://www.reddit.com/r/MachineLearning/comments/aqatyu/i_built_lambdas_12500_multigpu_deep_learning_rig/) which had around 480 upvotes on Reddit. My build only had 3-GPUs and took some shortcuts. In response to the hundreds of comments on that post, including comments by the CEO of Lambda Labs, I built and tested multiple 4-GPU rigs. I'm back to share a near-perfect 4-GPU deep learning rig with the highest performance and reliability, no thermal throttling, and lowest cost. This build is nearly identical to Lambda's 4-GPU workstation, but costs around $4000 cheaper. Happy building!",120,275,False,self,,,,,
1110,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,9,be17si,self.MachineLearning,Is it possible to render text using GANs ?,https://www.reddit.com/r/MachineLearning/comments/be17si/is_it_possible_to_render_text_using_gans/,johnnydozenredroses,1555459980,"I'm not talking about generating words or sentences, but actually rendering pixels of words or sentences ? And then condition it on some sort of linguistic features ?

You might ask : what would someone use it for? Well, I don't really know, but given some alien documents, we could use a GAN to model the scripts.

There's some prior work for modelling individual characters using GANS in the CVPR and ICDAR communities (https://arxiv.org/pdf/1706.08789.pdf). But not entire documents.",0,1,False,self,,,,,
1111,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,10,be1v6k,self.MachineLearning,Training a model from an unique image,https://www.reddit.com/r/MachineLearning/comments/be1v6k/training_a_model_from_an_unique_image/,ripventura,1555463893,[removed],0,1,False,self,,,,,
1112,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,10,be1zxa,self.MachineLearning,"[P] -Explore, a simple alternative to RL for computer chess",https://www.reddit.com/r/MachineLearning/comments/be1zxa/p_explore_a_simple_alternative_to_rl_for/,PhilipFelizarta,1555464695,"Hi y'all, 

I am a second-year at the University of California, Merced and this is a project I've been working on over the last few months. Its not state-of-the-art or anything like that, but any feedback on my work would be much appreciated. Keep in mind, I don't have a degree (yet) in Computer Science, so any form constructive criticism will be helpful!

You can find my code at:  [https://github.com/PhilipFelizarta/epsilon-Explore](https://github.com/PhilipFelizarta/epsilon-Explore) 

Quick Summary: 

Since the creation of AlphaZero, a majority of Deep Learning research and engineering for computer chess has been centered around the Zero doctrine; that is, focusing on creating a chess engine utilizing zero human knowledge. While AlphaZero (and Leela Zero) are grand milestones for AI, a common critique is the computational costs required to execute these reinforcement learning algorithms. Motivated to create an efficient, yet scalable learning algorithm, I propose an elementary, yet novel solution: -Explore. -Explore is a handcrafted adaptation of greedy-epsilon exploration, Go-Explore, and supervised learning that frames exploration tasks as continual learning and utilizes significantly less computational resources when compared to state-of-the-art reinforcement learning algorithms. All experimentation uses only a single GPU (RTX Titan) and a single CPU (Threadripper 16-core). The results of -Explore are not state-of-the-art with our experimental setup, but provide a foundation for creating more efficient handcrafted algorithms in other large search spaces given an available expert policy.",4,5,False,self,,,,,
1113,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,10,be26qf,self.MachineLearning,"CVPR 2019 paper statistics and visualization, 20 paper one-page review!",https://www.reddit.com/r/MachineLearning/comments/be26qf/cvpr_2019_paper_statistics_and_visualization_20/,hoya012,1555465799,[removed],0,1,False,https://b.thumbs.redditmedia.com/KCk5kBcobLS_tV6iGETDOrDBSiuAWW46hDg50yAcNos.jpg,,,,,
1114,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,11,be2di2,self.MachineLearning,kayak blow molding machine,https://www.reddit.com/r/MachineLearning/comments/be2di2/kayak_blow_molding_machine/,miyawang12138,1555466935,[removed],0,1,False,https://b.thumbs.redditmedia.com/6muy2w6mWthakp4XWFi9u_87vMuHRFQ8SQIB7FD3J1s.jpg,,,,,
1115,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,12,be2wej,self.MachineLearning,"Image Style Transfer Using Convolutional Neural Networks, CVPR 2016, by Gatys et al",https://www.reddit.com/r/MachineLearning/comments/be2wej/image_style_transfer_using_convolutional_neural/,atulshanbhag,1555470227,[removed],0,1,False,self,,,,,
1116,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,12,be2yr4,self.MachineLearning,Looking for. Apple malware dataset,https://www.reddit.com/r/MachineLearning/comments/be2yr4/looking_for_apple_malware_dataset/,Thunar13,1555470643,[removed],0,1,False,self,,,,,
1117,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,12,be2zpf,self.MachineLearning,GPT-2: Any plans for expansion?,https://www.reddit.com/r/MachineLearning/comments/be2zpf/gpt2_any_plans_for_expansion/,no_bear_so_low,1555470813,[removed],0,1,False,self,,,,,
1118,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,12,be37ka,self.MachineLearning,Translating text from Portuguese to English - unexpected funny result,https://www.reddit.com/r/MachineLearning/comments/be37ka/translating_text_from_portuguese_to_english/,vasco_ferreira,1555472294,[removed],0,1,False,self,,,,,
1119,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,12,be3au1,self.MachineLearning,[D] Translating text from Portuguese to English - unexpected funny result,https://www.reddit.com/r/MachineLearning/comments/be3au1/d_translating_text_from_portuguese_to_english/,vasco_ferreira,1555472937,"While following the guide on [How to Develop a Neural Machine Translation System from Scratch](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/), I'm trying to create a translation system for Portuguese-English. After the first fit, I checked the results and while most are just gibberish, this one is quite amusing:

&gt;src=\[ele e culpado de assassinato\], target=\[he is guilty of murder\], predicted=\[i dont believe anything\]",6,14,False,self,,,,,
1120,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,13,be3fcb,self.MachineLearning,Advice for applying to research engineering positions at top US industrial labs,https://www.reddit.com/r/MachineLearning/comments/be3fcb/advice_for_applying_to_research_engineering/,bitter_guy_throwaway,1555473831,"I created this throw-away account to ask this question.  For the past few months I have been applying to several research engineering positions at a few top US industrial labs and it has been a super-depressing experience.  I perform well in the     
 technical aspects interviews of the interviews (unless I am completely delusional) and make it to the final rounds but it either ends with 

A) rejection (without feedback even though I ask)  OR

B) ""All of the feedback has been positive and the team really liked you buuuuuuuuuut we think you would be better suited for this other job...""  where the other job is some lower-level data scientist position that I didn't apply to in the first place (this happened to me twice btw in big, well-know companies).  Probably just a bait-and-switch trick and in retrospect I don't think I was ever actually considered for the role I actually applied for.  

Also, to my surprise, about 90-95% of the interviews were identical to a software engineering interview i.e. algorithms and data structures questions.  The remaining aspects we extremely basic ML questions that anyone who took an introduction to ML class or Coursera course could easily answer (some companies were exceptions but it was for the most part true).   So now I'm thinking there probably isn't even much ML content in the role based on the interview questions so do I even want this job??

As for me, I have a masters degree in machine learning from a good school however it's in a different country that I don't think a lot of people know about in the US.  Also I'm at about 6 years work experience since I graduated where I have been working on a combination of ML and software engineering.

When I look on linkedin at people who hold these roles in most cases I have more work experience than them but the glaring difference is the name/prestige of the school that they went to.  Also in some cases they have PhDs.  Also I don't know anyone in these labs so I can't get an internal recommendation.  

So I guess my question is where do I go from here?  If it's related to my profile then there is not really much I can do to change that.  But if that was the case then why even interview me at all?  If it was a technical issue I would be very eager to work on that but I have not really received much feedback on that (and for the most part you can pretty much tell if you got a technical question right or wrong during the interview).  Is it a numbers game? Should I just keep applying until something happens?  Should I just give up go another direction?

&amp;#x200B;

PS. also another funny story: one VERY well-known company asked me to create a presentation about a project I had worked on.  So I spent a couple days creating this presentation and when it came time to present a grand total of 1 person from the interview team came to see it.  I told the HR person afterward that that happened and they apologized and offered to send me a free product of theirs as a token of their apology.  I declined.",0,1,False,self,,,,,
1121,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,13,be3vn7,self.MachineLearning,Beginner Question: C or C++ for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/be3vn7/beginner_question_c_or_c_for_machine_learning/,JackIsNotInTheBox,1555477123,[removed],0,1,False,self,,,,,
1122,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,14,be42lt,youtu.be,"AI Generated Technical Death Metal - From the conclusion section: ""In this paper, we demonstrated that creating music can be as simple as specifying a set of music influences on which a machine learning model trains."" Really makes you view a lot of things differently.",https://www.reddit.com/r/MachineLearning/comments/be42lt/ai_generated_technical_death_metal_from_the/,GeekMonolith,1555478549,,0,1,False,https://a.thumbs.redditmedia.com/ki02wK-Aum_-6Dfgzz-XwoJia7XGUhsQM6LeRN_tt78.jpg,,,,,
1123,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,15,be4qzm,webengage.com,10 Reasons Your Business Needs A Marketing Automation Platform,https://www.reddit.com/r/MachineLearning/comments/be4qzm/10_reasons_your_business_needs_a_marketing/,Kinjsh,1555484132,,0,1,False,default,,,,,
1124,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,16,be4sr3,self.MachineLearning,[D] Are there any theoretical connections between dropout regularization and ensemble learning?,https://www.reddit.com/r/MachineLearning/comments/be4sr3/d_are_there_any_theoretical_connections_between/,r2m2,1555484550,"I've anecdotally seen the connection between dropout and ensemble learning (i.e. dropout essentially trains a subnetwork so over the course of training it trains like an ensemble) mentioned in several places; however, I couldn't find any theoretical references. Are there any known theoretical results that make the connection between ensemble learning and dropout more concrete?",8,4,False,self,,,,,
1125,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,16,be4yvk,self.MachineLearning,[D] Why deep learning may not be the right solution for your business,https://www.reddit.com/r/MachineLearning/comments/be4yvk/d_why_deep_learning_may_not_be_the_right_solution/,thumbsdrivesmecrazy,1555486038,"Way too many businesses reach for deep learning solutions when they shouldnt.

There are several factors that make relatively simpler models more suitable than their deep learning counterparts: [Why deep learning may not be the right solution for your business](https://dlabs.pl/blog/article/why-deep-learning-may-not-be-the-right-solution-for-your-business)

* Costs - The problems most, especially small, businesses are facing do not really require very complex and sophisticated methods which only increase costs and time.
* Not enough good-quality data - in some cases data sets are not big enough for deep learning which usually demands huge sample sizes.
* Limited interpretability - It is important because of new insights into relationships between numerous variables and expected outcomes, it increases the trust and understandability.",0,0,False,self,,,,,
1126,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,16,be4z0y,arxiv.org,[R] HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning,https://www.reddit.com/r/MachineLearning/comments/be4z0y/r_hark_side_of_deep_learning_from_grad_student/,xternalz,1555486078,,4,3,False,default,,,,,
1127,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,16,be4zc2,abcnews.go.com,Youtube's Algorithm huh,https://www.reddit.com/r/MachineLearning/comments/be4zc2/youtubes_algorithm_huh/,dhanno65,1555486150,,0,1,False,https://b.thumbs.redditmedia.com/7Rx4NwofPVZF47T7Z7wVlUP_bpb_23Hfc-Uv5VSKfoY.jpg,,,,,
1128,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,17,be56z7,self.MachineLearning,Machine Learning Is Revolutionizing,https://www.reddit.com/r/MachineLearning/comments/be56z7/machine_learning_is_revolutionizing/,hussy456,1555488185,[removed],0,1,False,self,,,,,
1129,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,17,be5bvy,self.MachineLearning,[D] Debiasing techniques for contextual language embedding,https://www.reddit.com/r/MachineLearning/comments/be5bvy/d_debiasing_techniques_for_contextual_language/,zkid18,1555489509,"I'm familiar with debiasing word2vec embedding [https://arxiv.org/pdf/1607.06520.pdf] and GloVE embedding[http://cs229.stanford.edu/proj2016/report/BadieChakrabortyRudder-ReducingGenderBiasInWordEmbeddings-report.pdf]

Are there any researches with ElMO or Bert architectures?",0,2,False,self,,,,,
1130,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,17,be5f6o,self.MachineLearning,Resource for LSTM learning,https://www.reddit.com/r/MachineLearning/comments/be5f6o/resource_for_lstm_learning/,surface33,1555490403,[removed],0,1,False,self,,,,,
1131,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,18,be5jxf,codeingschool.com,Linear Regression: Implementation in python from scratch,https://www.reddit.com/r/MachineLearning/comments/be5jxf/linear_regression_implementation_in_python_from/,subhamroy021,1555491627,,0,1,False,default,,,,,
1132,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,18,be5qy2,self.MachineLearning,how would you aswer: will computers be more intelligent than humans?,https://www.reddit.com/r/MachineLearning/comments/be5qy2/how_would_you_aswer_will_computers_be_more/,Silvetooo,1555493367,[removed],0,1,False,self,,,,,
1133,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,18,be5rys,arxiv.org,[R] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection,https://www.reddit.com/r/MachineLearning/comments/be5rys/r_nasfpn_learning_scalable_feature_pyramid/,hardmaru,1555493616,,2,11,False,default,,,,,
1134,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,18,be5urq,self.MachineLearning,[D][SVM model] what is the best way to discriminate if a word is belong to the trained classes?,https://www.reddit.com/r/MachineLearning/comments/be5urq/dsvm_model_what_is_the_best_way_to_discriminate/,andrewnguyentb,1555494262,"i'm currently working on a project regarding isolated word recognition using svm model. But i'm struck by the scenario to decide an unknown word that put into the model belongs to my specific list of words or not. In more detail, i have a model that could recognize 5 vietnamese words, and the voice signal of words that aren't in any of these classes will be classify into class 6 (non-key class), but the real training samples i could get for this class can't cover all the real cases in real life. So my question is how to efficiently differentiate between this class and the others.",3,2,False,self,,,,,
1135,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,18,be5wp6,self.MachineLearning,Need help with nlp classification task,https://www.reddit.com/r/MachineLearning/comments/be5wp6/need_help_with_nlp_classification_task/,KornShnaps,1555494705,[removed],0,1,False,self,,,,,
1136,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,18,be5xp4,self.MachineLearning,MSc graduate seeking advice regarding PhD/internship opportunities.,https://www.reddit.com/r/MachineLearning/comments/be5xp4/msc_graduate_seeking_advice_regarding/,MithrandirGr,1555494950,[removed],0,1,False,self,,,,,
1137,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,19,be6b7j,self.MachineLearning,How do I solve this?,https://www.reddit.com/r/MachineLearning/comments/be6b7j/how_do_i_solve_this/,maverick653,1555498017,[removed],0,1,False,self,,,,,
1138,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,20,be6jra,self.datascience,"Ride-share company: Data scientist interview question- For forecasting service demand, how can you characterize the stochastic nature of demand?",https://www.reddit.com/r/MachineLearning/comments/be6jra/rideshare_company_data_scientist_interview/,stats_nerd21,1555499790,,0,1,False,default,,,,,
1139,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,20,be6mo0,self.MachineLearning,Autoencoder Help for 1D Time Series,https://www.reddit.com/r/MachineLearning/comments/be6mo0/autoencoder_help_for_1d_time_series/,lord1887,1555500366,[removed],0,1,False,self,,,,,
1140,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,20,be6rp5,self.MachineLearning,[D] Good examples of ML/DRL research proposals?,https://www.reddit.com/r/MachineLearning/comments/be6rp5/d_good_examples_of_mldrl_research_proposals/,yazriel0,1555501389,"Hi there

I have to write a budget proposal for a DRL/ML product development. We are not planning new research - just implementing key techniques from  recent papers. We intent to hire some ML PhDs and scale up the cloud compute, since the dataset (and the opportunity) are large.

The intended audience are upper management, financial decision makers etc. But the ML part will be reviewed for due-diligence by external ML expert advisors.

My own background is MSc/algorithms/development, but i have been reading up here on papers, and doing some coding. 

I believe the decision makers are very pro-AI at this stage, so fingers crossed.

Any good examples will be appreciated.",2,2,False,self,,,,,
1141,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,20,be6tkc,arxiv.org,[1904.06163] Guidelines for data analysis scripts,https://www.reddit.com/r/MachineLearning/comments/be6tkc/190406163_guidelines_for_data_analysis_scripts/,ihaphleas,1555501737,,3,4,False,default,,,,,
1142,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,20,be6u42,self.MachineLearning,[D] RTX 2080 Ti NVLINK / PCI-E 8x,https://www.reddit.com/r/MachineLearning/comments/be6u42/d_rtx_2080_ti_nvlink_pcie_8x/,daniel451,1555501845,"I did find some RTX 2080 Ti benchmarks that compare NVLink vs. no NVLink performance, like this one [https://technopremium.com/blog/4x-rtx-2080-ti-with-quadro-nvlink-performance-test/](https://technopremium.com/blog/4x-rtx-2080-ti-with-quadro-nvlink-performance-test/)

&amp;#x200B;

However, I did not find a benchmark so far comparing PCI-E 8x vs. PCI-E 16x performance and if NVLink could lessen a potential bandwith problem when using PCI-E 8x. Does anybody know about such benchmarks?

&amp;#x200B;

Many servers nowadays have single or dual root PCI-E solutions to host 8, 10, or even more GPUs with PCI-E 16x, but for workstation boards you often only have 4 usable PCI-E slots (given dual-slot GPUs) and only 3-4 slots with real PCI-E 16x.",2,4,False,self,,,,,
1143,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,20,be6ua6,self.MachineLearning,[P] Experiment: 70fps real-time object detection with Google's Coral Dev Board with Edge TPU,https://www.reddit.com/r/MachineLearning/comments/be6ua6/p_experiment_70fps_realtime_object_detection_with/,paul_read_it,1555501881,"Maybe you have already heard of Google's Coral Dev Board with Edge TPU and ask yourself how well it performs. We made a video to share our experience: [https://youtu.be/bOYWx1jJCZo](https://youtu.be/bOYWx1jJCZo)

&amp;#x200B;

In the video, we tested an object detection live stream under the following conditions:

\- a pretrained MobileNet v2 model, trained on the common objects in context (coco) dataset 

\- a bounding boxes threshold of 45% confidence because there were way too many boxes displayed in the default configuration

\- a camera connected via USB, not the official camera from Coral

&amp;#x200B;

We used this command to run the object detection server described above:

edgetpu\_classify\_server \\ --source /dev/video1:YUY2:800x600:24/1  \\ --model path/to/model/mobilenet\_ssd\_v2\_coco\_quant\_postprocess\_edgetpu.tflite \\ --labels path/to/labels/coco\_labels.txt --threshold=0.45

&amp;#x200B;

You can find more demos to play around here:

[https://coral.withgoogle.com/docs/dev-board/camera/](https://coral.withgoogle.com/docs/dev-board/camera/)

&amp;#x200B;

We hope this example helps you to get started with your own project!

&amp;#x200B;

If you have any idea, what we could build with it, let us know :-)

&amp;#x200B;

Paul",29,100,False,self,,,,,
1144,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,20,be6vx5,self.MachineLearning,NVIDIA Developers are Using StarCraft to Train Machine Learning AIs,https://www.reddit.com/r/MachineLearning/comments/be6vx5/nvidia_developers_are_using_starcraft_to_train/,tonys3kur3,1555502191,[removed],0,1,False,self,,,,,
1145,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,21,be6yoz,self.MachineLearning,Notebooks versus scripts - what's your workflow?,https://www.reddit.com/r/MachineLearning/comments/be6yoz/notebooks_versus_scripts_whats_your_workflow/,gar1t,1555502678,[removed],0,1,False,self,,,,,
1146,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,21,be72vd,self.MachineLearning,[D] Notebooks versus scripts - what's your workflow?,https://www.reddit.com/r/MachineLearning/comments/be72vd/d_notebooks_versus_scripts_whats_your_workflow/,gar1t,1555503411,"I'm a software engineer and have never used notebooks, ever, to write software. Nonetheless my programming style is  iterative, granular, and experimental - not unlike workflows I've seen used in notebooks. My problem with notebooks is that they're absolutely terrible for traditional code reuse. I mean, clicking cells and pressing Shift-Enter is technically reusing code, but having a human manually kicking a program along its course, block by block, doesn't feel like the software I grew up on.

&amp;#x200B;

So I'm curious how you use notebooks, if at all, and at what point, if such a point exists, do you move code from a notebook into a Python script or module. How do you feel about the whole thing? Happy, sad, utterly indifferent?",26,9,False,self,,,,,
1147,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,21,be7bsu,smartsketch.xyz,Photorealistic drawings from simple sketches using NVIDIA's GuaGAN,https://www.reddit.com/r/MachineLearning/comments/be7bsu/photorealistic_drawings_from_simple_sketches/,shinework,1555504927,,0,1,False,default,,,,,
1148,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,22,be7mzg,self.MachineLearning,Automate Email Marketing With Customer Journey,https://www.reddit.com/r/MachineLearning/comments/be7mzg/automate_email_marketing_with_customer_journey/,Kinjsh,1555506706,[removed],0,1,False,self,,,,,
1149,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,22,be7rrp,arxiv.org,[1904.06472] A Repository of Conversational Datasets,https://www.reddit.com/r/MachineLearning/comments/be7rrp/190406472_a_repository_of_conversational_datasets/,CaHoop,1555507495,,6,33,False,default,,,,,
1150,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,22,be80ki,self.MachineLearning,testbeds in ai,https://www.reddit.com/r/MachineLearning/comments/be80ki/testbeds_in_ai/,mynameisvinn,1555508896,[removed],0,1,False,self,,,,,
1151,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,23,be87hc,self.MachineLearning,GRU for time series data -how specific can you get?,https://www.reddit.com/r/MachineLearning/comments/be87hc/gru_for_time_series_data_how_specific_can_you_get/,abeecrombie,1555509960,"I am not an expert at either neural nets in general or RNN's but I have a data set that I would like to experiment with that is mostly time series in nature. I have roughly 30 different time series variables that I would like to use as my input in order to classify an output of different classes

As i've been looking into RNN's,  I only recently came across the concept of a GRU  and from my initial thoughts it seems better suited to my particular task as I can specify which of the variables need something like a GRU unit and which do not.  Is that even possible? For the variables that do not have a GRU I would like the longer term dependency to decay over the time series, thus I think I want to avoid LSTM.

I am curious to hear if there are any best practices for using GRU or even deep RNN in general on time series classification.  A lot of what I am reading re:RNN is related to NLP, which is an area I also like but quite different vs time series modelling.",0,1,False,self,,,,,
1152,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,23,be8cj5,self.MachineLearning,[D] Learnable activation function and aggregate (reduction) function,https://www.reddit.com/r/MachineLearning/comments/be8cj5/d_learnable_activation_function_and_aggregate/,tsauri,1555510708,,3,3,False,self,,,,,
1153,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,23,be8fe1,self.MachineLearning,[P] Probabilistic Cityscapes scene generator,https://www.reddit.com/r/MachineLearning/comments/be8fe1/p_probabilistic_cityscapes_scene_generator/,zarcomup,1555511157,"Hey all, I've been working on a custom generative model for a while and I just trained it on the Cityscapes dataset. I was pleased with what it learned to produce in just 5 hours of training on a single GPU (1080 Ti), and also I think the generation process itself looks pretty neat so I made a video of that too.

Here's 25 non cherry picked results

![img](1hr1c1cc1us21)

And here's a video showing the generation process (different run, different result)

![video](qgbpckog1us21)

As you might be able to tell, it's an autoregressive model. However, it's different from PixelCNN and co in the sense that it doesn't sample from top left to bottom right, but instead it samples at random positions. The benefit is that as you get more samples, the dependencies between pixels get more and more local and you can get away with sampling more than a single subpixel per inference step as long as they are sufficiently far apart. In this example, it takes 145 steps to sample 24576 subpixels (64x128x3) so that's only like 0.6% of the amount of steps you need with a PixelCNN. I know I'm not the first one with this idea but I'm surprised with how well it seems to works. There's some more details I'm going to keep to myself for now, but I'm curious to hear what you think of the result so far.

&amp;#x200B;

I think I should  be able to scale it up to at least double this resolution on my single GPU, but first I want to try it on some other datasets. In fact, first thing I'm going to try it on is on some raw audio data to see if the same principle of parallel sampling works in that domain too.",5,9,False,self,,,,,
1154,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,23,be8kq0,self.MachineLearning,"[P] ""Awesome"" GitHub repo of machine learning resources for mobile + edge",https://www.reddit.com/r/MachineLearning/comments/be8kq0/p_awesome_github_repo_of_machine_learning/,austin_kodra,1555511983,"Hi everyone,

I've always found the various ""Awesome"" lists on GitHub for various tech super helpful. I've seen a lot of the great ML resource lists before, but hadn't found one that looked specifically at ML on mobile. So I took some time to compile materials for an ""Awesome Mobile Machine Learning"" repo. Also includes some materials for other edge devices.

Anyways, hope this is helpful. Happy to add things I've missed, or if you'd like, feel free to open a PR!

[https://github.com/fritzlabs/Awesome-Mobile-Machine-Learning](https://github.com/fritzlabs/Awesome-Mobile-Machine-Learning)",1,11,False,self,,,,,
1155,MachineLearning,t5_2r3gv,2019-4-17,2019,4,17,23,be8qie,self.MachineLearning,"[Discussion] What is the status of the ""Information Bottleneck Theory of Deep Learning""?",https://www.reddit.com/r/MachineLearning/comments/be8qie/discussion_what_is_the_status_of_the_information/,metacurse,1555512850,"I am aware of the recent [ICLR paper](https://openreview.net/forum?id=ry_WPG-A-) which tried to debunk some of the key claims in the general case. But the IB theory authors came back with a (rude) rebuttal on OpenReview with new experiments to show that it holds in the general case. I could not understand how valid they were from the author's response to it.

The theory is complex with a lot of moving parts. I will be spending a lot of time on this if I go ahead and I also imagine there are few more people in similar position. Before that I wanted to check here if anyone relatively more experienced had a critical review of it (however brief). Is IB theory a promising or misdirected approach?",52,142,False,self,,,,,
1156,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,0,be8xkj,self.MachineLearning,[D] About Neural Ordinary Differential Equations,https://www.reddit.com/r/MachineLearning/comments/be8xkj/d_about_neural_ordinary_differential_equations/,im1q,1555513867,"There must be more than a few people here who have read Neural Ordinary Differential Equations ( [https://arxiv.org/pdf/1806.07366.pdf](https://arxiv.org/pdf/1806.07366.pdf) ), and while I understand the general concept of this, there are some points that are quite unclear to me.

1. What exactly does the adjoint state (and the augmented adjoint state) represent?
2. In section 5 (generative latent function time-series model), how is the gradient f guaranteed to be invariant to time?

I've been looking and searching for more papers, previous works, videos, posts, etc for more insight, and some have helped me a lot, but still got questions coming up endlessly to completely understand this paper. I think the idea of using an ODE solver to model a 'continuous' network is quite interesting, though. I wanted to post to see if you guys had more insight into this paper.",15,14,False,self,,,,,
1157,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,0,be90ek,self.MachineLearning,arXiv Citation Predictor,https://www.reddit.com/r/MachineLearning/comments/be90ek/arxiv_citation_predictor/,PathToNeuralink,1555514273,[removed],0,1,False,self,,,,,
1158,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,0,be97fo,twitch.tv,"[P] Live from Harvard on Twitch, Wed 4/17 at 3pm ET, join CS50's Nick Wong for a from-scratch implementation of a CNN, or convolutional neural network, using Python, in our current series about machine learning and neural networks.",https://www.reddit.com/r/MachineLearning/comments/be97fo/p_live_from_harvard_on_twitch_wed_417_at_3pm_et/,coltonoscopy,1555515273,,0,1,False,https://b.thumbs.redditmedia.com/w8O38L7xftzQARzn_AQJU65wQwvFS7ADGS5HFG0eObY.jpg,,,,,
1159,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,0,be9f5i,self.MachineLearning,"Simple Questions Thread April 17, 2019",https://www.reddit.com/r/MachineLearning/comments/be9f5i/simple_questions_thread_april_17_2019/,AutoModerator,1555516372,[removed],0,1,False,self,,,,,
1160,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,0,be9gfm,self.MachineLearning,[D] BERT for seq2seq tasks,https://www.reddit.com/r/MachineLearning/comments/be9gfm/d_bert_for_seq2seq_tasks/,AnonMLstudent,1555516550,"So am I right that BERT cannot currently be used for seq2seq tasks like machine translation or generating a response to an input sentence (like a general chatbot)?

If so, what are the best methods/architectures right now for seq2seq? Is bidirectional RNN /LSTM with attention still the best?",9,10,False,self,,,,,
1161,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,1,be9r4y,self.MachineLearning,"Hey Google, Why did you ban the darknet / yolo google group?",https://www.reddit.com/r/MachineLearning/comments/be9r4y/hey_google_why_did_you_ban_the_darknet_yolo/,natvert,1555518030,[removed],0,1,False,self,,,,,
1162,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,1,be9yqe,self.MachineLearning,Rct Studio is making WestWorld real by using AI,https://www.reddit.com/r/MachineLearning/comments/be9yqe/rct_studio_is_making_westworld_real_by_using_ai/,Yuqing7,1555519143,"California-based startup rct studio is leveraging cutting-edge artificial intelligence in its quest for a movie experience that is both interactive and immersive.

[https://medium.com/syncedreview/direct-and-star-in-your-own-movie-with-california-ai-startup-rct-studio-a1c1a8e8e7b](https://medium.com/syncedreview/direct-and-star-in-your-own-movie-with-california-ai-startup-rct-studio-a1c1a8e8e7b)",0,1,False,self,,,,,
1163,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,1,bea12u,self.MachineLearning,Google releases massive visual databases for machine learning,https://www.reddit.com/r/MachineLearning/comments/bea12u/google_releases_massive_visual_databases_for/,andrea_manero,1555519479,[removed],0,1,False,self,,,,,
1164,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,2,bean7u,sourcedexter.com,Performing Sentiment Analysis and Emotion recognition on E-commerce data with Python and IBM NLU,https://www.reddit.com/r/MachineLearning/comments/bean7u/performing_sentiment_analysis_and_emotion/,sourcedexter,1555522600,,0,1,False,default,,,,,
1165,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,3,beaz6z,medium.com,Direct and Star in Your Own Movie With California AI Startup Rct Studio,https://www.reddit.com/r/MachineLearning/comments/beaz6z/direct_and_star_in_your_own_movie_with_california/,gwen0927,1555524311,,0,1,False,https://b.thumbs.redditmedia.com/vp0yF4R9mgZejWNrUc3kN104B89QZhGnozBHM-QMbtE.jpg,,,,,
1166,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,3,beb5gp,self.MachineLearning,DYI: Computer Vision / Machine Leaning AI SDK,https://www.reddit.com/r/MachineLearning/comments/beb5gp/dyi_computer_vision_machine_leaning_ai_sdk/,xeniar,1555525209,[removed],0,1,False,self,,,,,
1167,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,3,beb5s2,self.MachineLearning,Source for High Res Historical Satellite Imagery,https://www.reddit.com/r/MachineLearning/comments/beb5s2/source_for_high_res_historical_satellite_imagery/,mickeyblueyes,1555525252,[removed],0,1,False,self,,,,,
1168,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,3,bebbh0,github.com,Text Classification Algorithms: A Survey,https://www.reddit.com/r/MachineLearning/comments/bebbh0/text_classification_algorithms_a_survey/,kk7nc,1555526085,,0,1,False,default,,,,,
1169,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,3,bebc6m,self.MachineLearning,Is Machine Learning applicable to a small online business ?,https://www.reddit.com/r/MachineLearning/comments/bebc6m/is_machine_learning_applicable_to_a_small_online/,Earnabdel,1555526188,[removed],1,1,False,self,,,,,
1170,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,4,bebqot,ai.googleblog.com,MorphNet: Towards Faster and Smaller Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bebqot/morphnet_towards_faster_and_smaller_neural/,sjoerdapp,1555528208,,0,1,False,default,,,,,
1171,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,4,bec107,medium.com,Automunge = automated preparation of tabular data for machine learning,https://www.reddit.com/r/MachineLearning/comments/bec107/automunge_automated_preparation_of_tabular_data/,gatorwatt,1555529727,,1,1,False,https://a.thumbs.redditmedia.com/k8hR1AcfdUJDi7P5SzedWra9qal2-Vp10pGhghlNiH4.jpg,,,,,
1172,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,4,bec94h,self.MachineLearning,Would like help understanding CNN weight and bias updates for image classification,https://www.reddit.com/r/MachineLearning/comments/bec94h/would_like_help_understanding_cnn_weight_and_bias/,cocobananaohhohh,1555530932,[removed],0,1,False,self,,,,,
1173,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,5,beci66,self.MachineLearning,A project idea?,https://www.reddit.com/r/MachineLearning/comments/beci66/a_project_idea/,BerkeleyBear101,1555532225,[removed],0,1,False,self,,,,,
1174,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,5,beclsq,self.MachineLearning,"Hi guys, I'm currently writing my university dissertation on the state of AI as well as the understanding that people have on it and how well it can be utilised by songwriters. If you could take a few minutes to answer some of these questions that I have prepared I would be most grateful, cheers :)",https://www.reddit.com/r/MachineLearning/comments/beclsq/hi_guys_im_currently_writing_my_university/,GeneralGriefous,1555532782,[removed],0,1,False,self,,,,,
1175,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,5,becou2,self.MachineLearning,confused about weight bias update with CNNs for an image classifier,https://www.reddit.com/r/MachineLearning/comments/becou2/confused_about_weight_bias_update_with_cnns_for/,cocobananaohhohh,1555533243,[removed],0,1,False,self,,,,,
1176,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,6,bed3se,self.MachineLearning,[Discussions]My Machine Learning Journal #11: Macro view of reinforcement learning and more OpenAI gym games,https://www.reddit.com/r/MachineLearning/comments/bed3se/discussionsmy_machine_learning_journal_11_macro/,RedditAcy,1555535499,[removed],1,1,False,https://b.thumbs.redditmedia.com/TafCW5VkHeMcHvNpO3vyCePGk5mjqtJ2osr9QMUtwUY.jpg,,,,,
1177,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,6,bed4wj,opendatascience.com,The 2019 Data Science Dictionary  Key Terms You Need to Know,https://www.reddit.com/r/MachineLearning/comments/bed4wj/the_2019_data_science_dictionary_key_terms_you/,OpenDataSciCon,1555535672,,0,1,False,https://b.thumbs.redditmedia.com/nGIK-Pr-02cX72ZEAC1CwmAKVSOBHOMJupogEQijboc.jpg,,,,,
1178,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,6,bedeg8,self.MachineLearning,Advanced Machine Learning: Neural Architecture Search,https://www.reddit.com/r/MachineLearning/comments/bedeg8/advanced_machine_learning_neural_architecture/,sytelus,1555537101,[removed],0,1,False,self,,,,,
1179,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,6,bedfyn,self.MachineLearning,Genetic Algorithm: Pros and Cons of Asexual Reproduction vs Merging Two Parents?,https://www.reddit.com/r/MachineLearning/comments/bedfyn/genetic_algorithm_pros_and_cons_of_asexual/,[deleted],1555537328,,0,1,False,default,,,,,
1180,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,6,bedj19,self.MachineLearning,[N] Advanced Machine Learning: Neural Architecture Search,https://www.reddit.com/r/MachineLearning/comments/bedj19/n_advanced_machine_learning_neural_architecture/,sytelus,1555537781,A great lecture by Debadeepta Dey from Microsoft Research: [https://www.youtube.com/watch?v=wL-p5cjDG64](https://www.youtube.com/watch?v=wL-p5cjDG64),4,58,False,self,,,,,
1181,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,6,bedkcg,self.MachineLearning,[D] Genetic algorithms: Pros and cons of asexual reproduction vs merging two parents?,https://www.reddit.com/r/MachineLearning/comments/bedkcg/d_genetic_algorithms_pros_and_cons_of_asexual/,rockitman12,1555537976,"Quick dumbed-down basics:

1. Create a bunch of random systems (brains, networks, whatever)
2. Run each system through your simulation and rate its fitness
3. Take the fittest systems, then breed and mutate them
4. Repeat steps #2 and #3 until satisfactory results

Obviously this is an oversimplification, but my only concern right now is step #3. There are two common methods to create a new batch of systems:

1. **Asexual Reproduction**

 Simple enough. Take one ""parent"" system, copy it, and then randomly mutate it.

2. **Two-Parent Reproduction**

 Take two parent systems, merge them together according to predefined rules, and then throw in a couple random mutations.


What I'm curious about is, what are the pros and cons of either method? How is the first method not just a simplified version of the second method? Theoretically, you could get identical offspring from both methods, so why go through the more complicated routine of the second?",21,5,False,self,,,,,
1182,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,6,bedl6x,self.MachineLearning,"Language agnostic plot / table data exporation library (python, js, java)",https://www.reddit.com/r/MachineLearning/comments/bedl6x/language_agnostic_plot_table_data_exporation/,h234sd,1555538102,[removed],0,1,False,self,,,,,
1183,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,7,bedof2,self.MachineLearning,Would like help understanding weights and biases in CNNs for image classification,https://www.reddit.com/r/MachineLearning/comments/bedof2/would_like_help_understanding_weights_and_biases/,hellomotorcycle,1555538567,[removed],0,1,False,self,,,,,
1184,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,7,bedopi,youtu.be,Cat Vs Dog Classification (Update).,https://www.reddit.com/r/MachineLearning/comments/bedopi/cat_vs_dog_classification_update/,the_coder_dot_py,1555538608,,0,1,False,https://b.thumbs.redditmedia.com/rSKlFw5Bz_AuaMdwKBKN2xanSRXKLll45RgEGBrjKjE.jpg,,,,,
1185,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,7,bee0if,self.MachineLearning,Would like help understanding weights and biases in in CNNs for image processing,https://www.reddit.com/r/MachineLearning/comments/bee0if/would_like_help_understanding_weights_and_biases/,hellomotorcycle,1555540374,[removed],0,1,False,self,,,,,
1186,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,7,bee3kr,self.MachineLearning,Conditional GAN Mode Collapse,https://www.reddit.com/r/MachineLearning/comments/bee3kr/conditional_gan_mode_collapse/,greerviau,1555540833,[removed],0,1,False,https://b.thumbs.redditmedia.com/jyqC4bDbp_UfE37jqI8jrPu521bDCWTMh_9uDYf3SCQ.jpg,,,,,
1187,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,7,bee5rp,self.MachineLearning,Artificial Intelligence in Audio - Event at Dolby SoHo (New York),https://www.reddit.com/r/MachineLearning/comments/bee5rp/artificial_intelligence_in_audio_event_at_dolby/,royfejgin,1555541183,[removed],0,1,False,self,,,,,
1188,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,7,bee99g,self.MachineLearning,[N] Artificial Intelligence in Audio - Event at Dolby SoHo (New York),https://www.reddit.com/r/MachineLearning/comments/bee99g/n_artificial_intelligence_in_audio_event_at_dolby/,royfejgin,1555541786,"Dolbys AI group is organizing an event with speakers from Google, MILA, NYU, and Spotify, with the goal of bringing together Audio AI researchers and engineers.

 

It's a good opportunity to network with leading researchers working on deep-learning-based audio processing and learn about recent advancements.

 

Come early to explore Dolby SoHo, Dolbys experiential space, ""where science meets art and technology meets imagination"".

 

RSVP to reserve a place - seating is limited: [https://soho.dolby.com/artificialintelligenceinaudio](https://soho.dolby.com/artificialintelligenceinaudio)",3,2,False,self,,,,,
1189,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,8,beem3o,arxiv.org,[R] Backprop Evolution,https://www.reddit.com/r/MachineLearning/comments/beem3o/r_backprop_evolution/,downtownslim,1555543902,,37,101,False,default,,,,,
1190,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,9,bef2ne,self.MachineLearning,"NLP - Averaging embeddings to get document representations, and then clustering",https://www.reddit.com/r/MachineLearning/comments/bef2ne/nlp_averaging_embeddings_to_get_document/,somethingstrang,1555546650,[removed],0,1,False,self,,,,,
1191,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,9,bef8m1,chess.com,Neural-network chess engine wins computer chess championship for the first time in history,https://www.reddit.com/r/MachineLearning/comments/bef8m1/neuralnetwork_chess_engine_wins_computer_chess/,tpjv86b,1555547696,,0,1,False,https://a.thumbs.redditmedia.com/qcurZ3yL3nvv62OveYbLaoVIsRaxa4zVubnmAX9F7M0.jpg,,,,,
1192,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,11,begcbw,self.MachineLearning,Retraining Current Model vs. Combining Two Models,https://www.reddit.com/r/MachineLearning/comments/begcbw/retraining_current_model_vs_combining_two_models/,sud0er,1555554534,[removed],0,1,False,self,,,,,
1193,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,11,begm91,self.MachineLearning,[D] Large-scale imitation learning/apprenticeship learning for self-driving cars,https://www.reddit.com/r/MachineLearning/comments/begm91/d_largescale_imitation_learningapprenticeship/,strangecosmos,1555556298,"*Quick summary:* Imitation learning for self-driving cars is confounded by the DAgger problem, but this problem is in principle soluble by scaling up training data, as AlphaStar has demonstrated. Tesla appears to be trying this right now with ~450,000 drivers. 

***

Ever since DeepMind showed with AlphaStar that you can get to [human-level performance](https://twitter.com/oriolvinyalsml/status/1094670648042012673?s=21) on StarCraft with imitation learning alone, I've been obsessed with the idea of applying imitation learning to self-driving cars on a similar scale. 

Waymo has experimented with imitation learning on a very small scale (just ~1,400 hours of driving). ([blog post](https://link.medium.com/T2tem8DtYV) | [paper](https://arxiv.org/abs/1812.03079)) Waymo's experiment with their imitation network, ChauffeurNet, felt like a Rorschach test. Some deep learning/autonomous vehicle people on Twitter interpreted it as showing that imitation learning doesn't work. Others reacted the opposite way, seeing it as a promising direction for future R&amp;D.  

Large-scale imitation learning is more exciting to me because AlphaStar is such a compelling proof of concept. Alex Irpan, a reinforcement learning researcher, has a great explanation [on his blog](https://www.alexirpan.com/2019/02/22/alphastar-part2.html):

&gt; One of the problems with imitation learning is the way errors can compound over time. Im not sure if theres a formal name for this. Ive always called it the [DAgger](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf) problem, because thats the paper that everyone cites when talking about this problem ([Ross et al, AISTATS 2011](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)).
&gt;
&gt; ... This problem means mistakes in imitation learning often arent recoverable, and the temporal nature of the problem means that the longer your episode is, the more likely it is that you enter this negative feedback loop, and the worse youll be if you do. ...
&gt;
&gt; Due to growing quadratically in *_T_*, we expect long-horizon tasks to be harder for imitation learning. A StarCraft game is long enough that I didnt expect imitation learning to work at all. And yet, imitation learning was good enough to reach the level of a Gold player. 
&gt;
&gt; ... If you have a very large dataset, from a wide variety of experts of varying skill levels (like, say, a corpus of StarCraft games from anyone whos ever played the game), then its possible that your data already has enough variation to let your agent learn how to recover from several of the incorrect decisions it could make.

So, AlphaStar has shown us that one potential solution to the compounding errors that arise with supervised imitation learning/behavioural cloning (the DAgger problem) is to collect a massive and highly varied dataset that includes a lot of errors, and a lot of recovering from errors. Counterintuitively, humans are teaching the AI by doing it wrong!

It was [recently reported](https://www.theinformation.com/articles/what-makes-teslas-autopilot-different) in The Information that Tesla is taking a behavioural cloning approach to self-driving. Tesla has [around 450,000](https://hcai.mit.edu/tesla-vehicle-numbers/) drivers with the latest generation of sensor hardware, which includes eight cameras covering 360 degrees around the car. Here's what The Information said:

&gt; Teslas cars collect so much camera and other sensor data as they drive around, even when Autopilot isnt turned on, that the Autopilot team can examine what traditional human driving looks like in various driving scenarios and mimic it, said the person familiar with the system. It uses this information as an additional factor to plan how a car will drive in specific situationsfor example, how to steer a curve on a road or avoid an object.
&gt;
&gt;Such an approach has its limits, of course: behavior cloning, as the method is sometimes called, cannot teach an automated driving system to handle dangerous scenarios that cannot be easily anticipated. Thats why some autonomous vehicle programs are wary of relying on the technique.
&gt;
&gt;But Teslas engineers believe that by putting enough data from good human driving through a neural network, that network can learn how to directly predict the correct steering, braking and acceleration in most situations. You dont need anything else to teach the system how to drive autonomously, said a person who has been involved with the team. They envision a future in which humans wont need to write code to tell the car what to do when it encounters a particular scenario; it will know what to do on its own.

Another potential solution is to give the imitation agent [access to an expert/human demonstrator](https://youtu.be/V7CY68zH6ps?t=36m27s) when it makes an error and doesn't know how to recover. If a vehicle drives up onto a sidewalk, and there was never any sidewalk state-action pairs in its training dataset, then you can get a human to demonstrate what to do in that situation. The problem is this is obviously very labour intensive. You need a lot of demonstrators ready to take over when an error occurs.

Strikingly, this seems to be exactly what Tesla is doing. Elon Musk [recently described](https://pca.st/episode/8e374af4-3951-47ae-aafa-3716d2d916a8?t=676.0) something that sounds like this solution to the DAgger problem:

&gt; Well, theres a lot of things that are learnt. There are certainly edge cases where say somebodys on Autopilot and they take over. And then, okay, thats a trigger that goes into our system that says, okay, did they take over for convenience, or did they take over because the Autopilot wasnt working properly.
&gt;
&gt; Theres also like, lets say were trying to figure out what is the optimal spline for traversing an intersection. Then, the ones where there are no interventions are the right ones. So you then say okay, when it looks like this, do the following. And then you get the optimal spine for navigating a complex intersection.

Elon later [said](https://twitter.com/elonmusk/status/1117154663537623040?s=20) on Twitter:

&gt; Your interventions do train the NN [neural network]

This sounds like the neural network is sampling human demonstrations when it makes an error. In theory, it could be reinforcement learning rather than imitation/apprenticeship learning. Any thoughts on whether it would make sense to use RL instead of IL here? 

A totally different approach is to use a GAN and do generative adversarial imitation learning (GAIL). In [one paper](https://arxiv.org/pdf/1701.06699.pdf), GAIL did worse than behavioural cloning on short time scales (~2 seconds) but better over long time scales. There's also [inverse reinforcement learning](https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/). So, there are a bunch of different ideas to explore in this area. 

So, to summarize:

* AlphaStar showed behavioural cloning can solve a complex, tactical, multi-agent task with an astronomically large, continuous action space  like driving! The solution is a massive and highly varied dataset with a lot of human errors.

* When behavioural cloning falls short, another potential solution is to allow the neural network to ask a human for a demonstration when it makes an error. 

* Tesla appears to be collecting a massive and highly varied dataset with a lot of human errors for behavioural cloning of the driving task.

* Tesla also appears to allowing its NN to sample human demonstrations when the NN makes an error, unless this is actually reinforcement learning. 

This is so exciting to me. The only big difference I can think of between StarCraft and driving is the obvious one: AlphaStar just plugged into the game's API, whereas to deploy a self-driving car you need to solve computer vision. Besides that, I can't think of anything. Can y'all? 

One way in which driving is actually easier than StarCraft is the time horizon. Driving is a sequence of short time horizon tasks. For example, the time horizon for navigating an intersection is short. Once a car is through the intersection, its actions don't depend on its past actions or previously observed states. 

Before AlphaStar, imitation learning felt a lot more dubious. Now it feels like a proven solution. We might be within spitting distance of honest-to-God self-driving cars. 

Either I'm way too optimistic about this, or a lot of people are missing something big. So, which is it? Am I overlooking important differences between StarCraft and driving? Is it wrong to assume the difference between ChauffeurNet and AlphaStar is just scale?",13,8,False,self,,,,,
1194,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,12,begrfo,self.MachineLearning,Systematic approaches to xgboost hyperparameter tuning?,https://www.reddit.com/r/MachineLearning/comments/begrfo/systematic_approaches_to_xgboost_hyperparameter/,whdd,1555557214,[removed],0,1,False,self,,,,,
1195,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,12,beguo7,github.com,pytorch/tvm by pytorch,https://www.reddit.com/r/MachineLearning/comments/beguo7/pytorchtvm_by_pytorch/,sjoerdapp,1555557820,,0,1,False,https://b.thumbs.redditmedia.com/A2NHEzQ_5TlS-e-rF31huFfZohXWtY8MyCGW12UVXmE.jpg,,,,,
1196,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,12,begy12,self.MachineLearning,Apply GRU to some variables but not others?,https://www.reddit.com/r/MachineLearning/comments/begy12/apply_gru_to_some_variables_but_not_others/,abeecrombie,1555558450,"I am not an expert at either neural nets in general or RNN's but I have a data set that I would like to experiment with that is mostly time series in nature. I have roughly 30 different time series variables that I would like to use as an input in order to classify several different classes

&amp;#x200B;

As i've been looking into RNN's, I only recently came across the concept of a GRU and from my initial thoughts it seems better suited to my particular task as I can specify which of the variables need something like a GRU unit and which do not. Is that even possible? For the variables that do not have a GRU I would like the longer term dependency to decay over the time series, thus I think I want to avoid LSTM.

&amp;#x200B;

I am curious to hear if there are any best practices for using GRU or even deep RNN in general on time series classification. A lot of what I am reading re:RNN is related to NLP, which is an area I also like but quite different vs time series modelling.",0,1,False,self,,,,,
1197,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,13,behaje,self.MachineLearning,[D] Apply GRU to some features but not others?,https://www.reddit.com/r/MachineLearning/comments/behaje/d_apply_gru_to_some_features_but_not_others/,abeecrombie,1555560885,"I am not an expert at either neural nets in general or RNN's but I have a data set that I would like to experiment with that is mostly time series in nature. I have roughly 30 different time series variables that I would like to use as my input in order to predict the classification of several classes

&amp;#x200B;

As i've been looking into RNN's, I only recently came across the concept of a GRU and from my initial thoughts it seems better suited to my particular task as I can specify which of the variables need something like a GRU unit and which do not. Is that even possible? For the variables that do not have a GRU I would like the longer term dependency to decay over the time series, thus I think I want to avoid LSTM.

&amp;#x200B;

I am curious to hear if there are any best practices for using GRU or even deep RNN in general on time series classification. A lot of what I am reading re:RNN is related to NLP, which is an area I also like but quite different vs time series modelling.",6,3,False,self,,,,,
1198,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,13,behc46,self.MachineLearning,"[R] paper and a PyTorch implementation of ""What is wrong with scene text recognition model comparisons? dataset and model analysis""",https://www.reddit.com/r/MachineLearning/comments/behc46/r_paper_and_a_pytorch_implementation_of_what_is/,ku21fan,1555561190,"Paper: [https://arxiv.org/pdf/1904.01906.pdf](https://arxiv.org/pdf/1904.01906.pdf)

PyTorch code: [https://github.com/clovaai/deep-text-recognition-benchmark](https://github.com/clovaai/deep-text-recognition-benchmark)

**Abstract:** 

&gt;Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code will be publicly available. 

&amp;#x200B;

\# To strongly remind inconsistent training and evaluation settings in the scene text recognition field, we named our paper in this way.",3,6,False,self,,,,,
1199,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,14,behnz2,self.MachineLearning,Face Recognition: An Introduction for Beginners,https://www.reddit.com/r/MachineLearning/comments/behnz2/face_recognition_an_introduction_for_beginners/,spmallick,1555563727,[removed],0,1,False,self,,,,,
1200,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,14,behrcg,self.MachineLearning,Applications with many machine learning models,https://www.reddit.com/r/MachineLearning/comments/behrcg/applications_with_many_machine_learning_models/,StreetFarmer,1555564457,[removed],0,1,False,self,,,,,
1201,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,15,bei5qy,self.MachineLearning,I'm doing a ML project and looking for an online collaborator. Anyone interested?,https://www.reddit.com/r/MachineLearning/comments/bei5qy/im_doing_a_ml_project_and_looking_for_an_online/,HummelsM,1555567657,[removed],0,1,False,self,,,,,
1202,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,15,bei90j,self.MachineLearning,What is the Difference Between AI and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bei90j/what_is_the_difference_between_ai_and_machine/,kartik2019,1555568434,[removed],0,1,False,self,,,,,
1203,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,15,bei9xg,self.MachineLearning,Data Science vs. Decision Science [Infographic],https://www.reddit.com/r/MachineLearning/comments/bei9xg/data_science_vs_decision_science_infographic/,techgig11,1555568672,[removed],0,1,False,self,,,,,
1204,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,15,beiew6,medium.com,Visualizing stock trading agents using Matplotlib and Gym,https://www.reddit.com/r/MachineLearning/comments/beiew6/visualizing_stock_trading_agents_using_matplotlib/,notadamking,1555569833,,0,1,False,default,,,,,
1205,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,15,beihkz,self.MachineLearning,Do you know any open source prodi.gy alternatives?,https://www.reddit.com/r/MachineLearning/comments/beihkz/do_you_know_any_open_source_prodigy_alternatives/,abdush,1555570461,[removed],0,1,False,self,,,,,
1206,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,16,beikv2,activewizards.com,Data Science vs. Decision Science [Infographic],https://www.reddit.com/r/MachineLearning/comments/beikv2/data_science_vs_decision_science_infographic/,techgig11,1555571229,,0,12,False,default,,,,,
1207,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,16,beio81,self.MachineLearning,Visualization of Intermediate steps in neural networks,https://www.reddit.com/r/MachineLearning/comments/beio81/visualization_of_intermediate_steps_in_neural/,HanSatyam,1555572058,[removed],0,1,False,self,,,,,
1208,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,16,beirwl,self.MachineLearning,"""[R]"" ""[D]"" Visualization of intermediate steps of a Convolutional Neural Network",https://www.reddit.com/r/MachineLearning/comments/beirwl/r_d_visualization_of_intermediate_steps_of_a/,HanSatyam,1555572985,"As Stanford Course(CS231n) (http://cs231n.github.io/understanding-cnn/), they have carried visualization of filters and first layer weights for images using Convolutional Neural Network (CNN). How  can I visualize the same for `audio using Matplotlib`?
Are there any better tools for visualization of the same?",0,0,False,self,,,,,
1209,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,16,beiw2b,czbiohub.org,"Livestream a Conference on AI, Big Data, ML applied to the Brain - Jure Leskovec speaking 4/19",https://www.reddit.com/r/MachineLearning/comments/beiw2b/livestream_a_conference_on_ai_big_data_ml_applied/,ScienTecht,1555574065,,0,2,False,https://b.thumbs.redditmedia.com/2tPm2PGAfdb4yWhYpowkTFfBQQIhP6CsNHh7r7xoXfQ.jpg,,,,,
1210,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,17,beixpp,towardsdatascience.com,Master basics of machine learning by solving a hackathon problem,https://www.reddit.com/r/MachineLearning/comments/beixpp/master_basics_of_machine_learning_by_solving_a/,smhatre59,1555574517,,0,1,False,default,,,,,
1211,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,17,beixqg,self.MachineLearning,"Implementing Randomly Wired Neural Networks for Image Recognition, Experiments were performed on CIFAR-10 datasets and CIFAR-100 datasets.",https://www.reddit.com/r/MachineLearning/comments/beixqg/implementing_randomly_wired_neural_networks_for/,leaderj1001,1555574521,[removed],0,1,False,self,,,,,
1212,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,17,bej0mp,czbiohub.org,"Livestream a Conference on AI, Big Data, ML applied to the Brain - Jure Leskovec speaking 4/19",https://www.reddit.com/r/MachineLearning/comments/bej0mp/livestream_a_conference_on_ai_big_data_ml_applied/,ScienTecht,1555575268,,0,1,False,default,,,,,
1213,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,17,bej39p,self.MachineLearning,"[P] Implementing Randomly Wired Neural Networks for Image Recognition, Experiments were performed on CIFAR-10 datasets and CIFAR-100 datasets.",https://www.reddit.com/r/MachineLearning/comments/bej39p/p_implementing_randomly_wired_neural_networks_for/,leaderj1001,1555575973,"Hi, I'm Myeongjun Kim. I am a graduate student in computer vision research. I realized the importance of paper implementation. So I implemented this paper. I wrote the code with my friend Taehun Kim, I used pytorch, and Taehun Kim wrote the code with tensorflow. Others were experimenting with ImageNet datasets. Therefore, the experiment was carried out using CIFAR datasets. There is no experiment on CIFAR datasets in the paper, but we implemented the network by putting hyper-parameters similar to the paper. There are a lot of deficiencies in the paper implementation for the first time. we are little nervous because it's the first time we post. But we ask for a lot of feedbacks. Thank you so much for reading the long paragraph.

CIFAR-10, Accuracy: 92.65%

CIFAR-100, Accuracy: 72.92%

Pytorch version Github URL, [https://github.com/leaderj1001/RandWireNN](https://github.com/leaderj1001/RandWireNN)

Tensorflow version Github URL,  [https://github.com/swdsld/RandWire\_tensorflow](https://github.com/swdsld/RandWire_tensorflow)",29,157,False,self,,,,,
1214,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,17,bej7zm,self.MachineLearning,[D] Machine learning desktop,https://www.reddit.com/r/MachineLearning/comments/bej7zm/d_machine_learning_desktop/,el_guy_el,1555577247,"Helloooooo, 
First time poster.
I just finished programming my first decision tree model on the Titanic data set starting to learn about the random forest on, supper excited to keep learning more !!
 

My question is what is the proformance losses or gains on having  2 1070s in nvlink or sli vs a 1080ti  
For training machine learning models. I've read that GPU v-ram can be a limitation the 1080ti would have 11-12 gb of v-ram and the 2 1070s would have 8+8 = 16. Does the math work out like that ? 


Also is it even worth it to have a desktop for machine learning projects ? Is AWS and other services like that that much better what do you think ?",10,2,False,self,,,,,
1215,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,17,bej8dv,medium.com,FACE RECOGNITION: How to deal with people that were not part of training data,https://www.reddit.com/r/MachineLearning/comments/bej8dv/face_recognition_how_to_deal_with_people_that/,chhitij07,1555577355,,0,1,False,default,,,,,
1216,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,18,bejbxg,self.MachineLearning,A Quick question regarding Convolutional Neural Networks. Any help is appreciated! :),https://www.reddit.com/r/MachineLearning/comments/bejbxg/a_quick_question_regarding_convolutional_neural/,ActuallyRishi,1555578275,[removed],0,1,False,self,,,,,
1217,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,18,bejd8n,self.MachineLearning,Are pretrained networks always better for tasks like semantic segmentation,https://www.reddit.com/r/MachineLearning/comments/bejd8n/are_pretrained_networks_always_better_for_tasks/,onenetworktobindthem,1555578592,[removed],0,1,False,self,,,,,
1218,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,19,bejsjh,self.MachineLearning,"[N] Lc0 Wins Computer Chess Championship, Makes History",https://www.reddit.com/r/MachineLearning/comments/bejsjh/n_lc0_wins_computer_chess_championship_makes/,wei_jok,1555582134,"[Lc0 Wins Computer Chess Championship, Makes History](https://www.chess.com/news/view/lc0-wins-computer-chess-championship-makes-history)

The machine-learning chess engine Lc0 won the Chess.com [Computer Chess Championship](https://www.chess.com/computer-chess-championship) last weekend, making history as the first neural-network project to take the title. Lc0, which taught itself how to play chess, is now at the game's pinnacle as the champion computer engine.

More details:

https://www.chess.com/news/view/lc0-wins-computer-chess-championship-makes-history",8,29,False,self,,,,,
1219,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,20,bekcbd,altoros.com,TensorFlow Cheat Sheet,https://www.reddit.com/r/MachineLearning/comments/bekcbd/tensorflow_cheat_sheet/,ValVish,1555586382,,0,1,False,https://a.thumbs.redditmedia.com/iewIUvGVkDRUjQAPXK6eyfp82YnRYjTT6eNLViPBbN0.jpg,,,,,
1220,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,21,beksnh,self.MachineLearning,Can anyone explain how the reconstruction error can be compared with the conditional entropy in equation 1?,https://www.reddit.com/r/MachineLearning/comments/beksnh/can_anyone_explain_how_the_reconstruction_error/,farsaiya_abhay,1555589522,[removed],0,1,False,self,,,,,
1221,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,21,bekut1,self.MachineLearning,[R] Audio Denoising with Deep Network Priors,https://www.reddit.com/r/MachineLearning/comments/bekut1/r_audio_denoising_with_deep_network_priors/,mosheman5,1555589902,"Hi! First author here - 

In this paper we tackle the problem of audio and speech denoising. Given an audio stream of noisy speech, which is a mixture of speech and noisy background, we would like to filter out the speech signal. While most of the methods utilize supervised deep learning, we decided to use only the noisy sample without any learned model or additional dataset, presenting fully unsupervised method. 

To accomplish that, we train anautoencoder to fit the noisy signal from random noise input. 

We observed that modeling noise in the signal is harder than the ""clean"" part in the signal. During the fitting process we observe fluctuations in different stages of the train.  

Utilizing that, we calculate the amount of difference between different network outputs in the time-frequency domain we create a robust spectral mask used for denoising the noisy output. 

We tested this algorithm on other audio domains rather than only speech, and it shows the same effect: denoising or filtering the main data in a signal using only the noisy signal itself. 

You can listen to samples and a comparison withtraditional unsupervised methodscan be found[here](https://mosheman5.github.io/DNP/) 

Check out our[paper](https://arxiv.org/pdf/1904.07612.pdf) for the full details. In addition, our PyTorch code is available on [Github](https://github.com/mosheman5/DNP). 

Feel free to ask any questions.",10,38,False,self,,,,,
1222,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,22,beltyr,towardsdatascience.com,Can we use Machine Learning and AI to help with global warming and clean energy?,https://www.reddit.com/r/MachineLearning/comments/beltyr/can_we_use_machine_learning_and_ai_to_help_with/,vivekpa,1555595698,,0,1,False,default,,,,,
1223,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,22,belv15,smarten.com,Self-Serve Data Analytics Can Work for You!,https://www.reddit.com/r/MachineLearning/comments/belv15/selfserve_data_analytics_can_work_for_you/,ElegantMicroWebIndia,1555595870,,0,1,False,default,,,,,
1224,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,23,bem39k,self.MachineLearning,Predict multiple Outputs LSTM,https://www.reddit.com/r/MachineLearning/comments/bem39k/predict_multiple_outputs_lstm/,IAlover,1555597101,"Hello 

I had a dataset that represents the coordinates X ,Y of an object,  and I want to predict the xy coordinates of all the future trajectory. the structure of my dataset is as follow: 

T= 1,2,3,4

object = 1,1,1,1,1,1,2,2,2

X= 0.1,0.5,0.8,0.9,0.5,0.6

Y=0.3,0.8,0.9,0.6,0.8,0.7

how can i use an lstm network to predict the  future trajectory with multiple  coordinate outputs  X Y and not just  the next coordinate X Y.

Thanks",0,1,False,self,,,,,
1225,MachineLearning,t5_2r3gv,2019-4-18,2019,4,18,23,bembq3,self.MachineLearning,Should I pre-process my text if I'm running NER?,https://www.reddit.com/r/MachineLearning/comments/bembq3/should_i_preprocess_my_text_if_im_running_ner/,acidplasm,1555598417,[removed],0,1,False,self,,,,,
1226,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,0,bemomd,medium.com,Upgrading CNN With OctConv,https://www.reddit.com/r/MachineLearning/comments/bemomd/upgrading_cnn_with_octconv/,gwen0927,1555600257,,0,1,False,https://b.thumbs.redditmedia.com/La0xOsrrqX4q_FaLCHveHJu6eRobofUf9odLiCGmOdo.jpg,,,,,
1227,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,0,bemq0j,wandb.com,[P] Semantic Segmentation for Self Driving Cars with fast.ai and Weights&amp;Biases,https://www.reddit.com/r/MachineLearning/comments/bemq0j/p_semantic_segmentation_for_self_driving_cars/,borisd13,1555600451,,1,1,False,default,,,,,
1228,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,0,bemqws,self.MachineLearning,[D] State of the art in video action recognition using transfer learning?,https://www.reddit.com/r/MachineLearning/comments/bemqws/d_state_of_the_art_in_video_action_recognition/,lantern_lol,1555600570,"I am working on a project which requires action recognition in videos, specifically short ~10 second YouTube clips. Ideally I want to start with a pre-trained network which can be fine-tuned so as to avoid training cost.

AFAIK the SOTA is widely accepted as [DeepMind's I3D](https://arxiv.org/abs/1705.07750), for which [pre-trained checkpoint models exist](https://github.com/deepmind/kinetics-i3d). Are there any interesting papers which challenge this, specifically those which can also use a pre-training approach?",0,2,False,self,,,,,
1229,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,0,bemyrl,self.MachineLearning,"Training on realizations, testing on posteriors?",https://www.reddit.com/r/MachineLearning/comments/bemyrl/training_on_realizations_testing_on_posteriors/,the_roboticist,1555601695,"I have a supervised, discriminative problem that is the second step in a pipeline of models. I'm trying to learn to predict p(**Y = y**|**Z**) where **Z \~** P(**Z | X**). At training time I have get a *sample* **Z** \~ p(**Z | X)** but at test time I have the full posterior p(**Z | X).** Concretely, **Z** is represented by a 100 dimensional latent vector; at training I only have a 1-hot encoding but at test time I have a softmax. I could, of course, just take the argmax at test time but that throws away the full posterior. Or perhaps I should inject noise at train-time to the one-hot encoding. Is there any literature on this problem?",0,1,False,self,,,,,
1230,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,0,ben2tn,self.MachineLearning,"[D] Training on realizations, testing on posteriors?",https://www.reddit.com/r/MachineLearning/comments/ben2tn/d_training_on_realizations_testing_on_posteriors/,the_roboticist,1555602294,"I have a supervised, discriminative problem. The model is a feedforward NN. I'm trying to learn to predict p(**Y = y**|**Z**) where **Z \~** P(**Z | X**). At training time I have get a *sample* **Z** \~ p(**Z | X)** but at test time I have the full posterior p(**Z | X).** Concretely, **Z** is represented by a 100 dimensional latent vector; at training I only have a 1-hot encoding but at test time I have a distribution. I could, of course, just take the argmax at test time but that throws away the full posterior. Or perhaps I should inject noise at train-time to the one-hot encoding. Is there any literature on this problem?",5,1,False,self,,,,,
1231,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,1,beno0t,self.MachineLearning,Help,https://www.reddit.com/r/MachineLearning/comments/beno0t/help/,Ldog8315,1555605322,[removed],0,1,False,self,,,,,
1232,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,1,bens2n,youtube.com,[D] Ian Goodfellow: Generative Adversarial Networks (GANs) | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/bens2n/d_ian_goodfellow_generative_adversarial_networks/,UltraMarathonMan,1555605890,,0,1,False,https://b.thumbs.redditmedia.com/aV5wLfJKahxDmI9TCJYHSfyjLAzMfbXv1DWsrNmUYNs.jpg,,,,,
1233,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,1,bentdv,self.MachineLearning,[D] Ian Goodfellow | MIT Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/bentdv/d_ian_goodfellow_mit_artificial_intelligence/,UltraMarathonMan,1555606078,"Ian Goodfellow is the author of the popular textbook on deep learning (simply titled ""Deep Learning""). He coined the term Generative Adversarial Networks (GANs) and with his 2014 paper is responsible for launching the incredible growth of research on GANs. He got his BS and MS at Stanford, his PhD at University of Montreal with Yoshua Bengio and Aaron Courville. He held several research positions including at OpenAI, Google Brain, and now at Apple as director of machine learning. This recording happened while Ian was still at Google Brain.

 https://www.youtube.com/watch?v=Z6rxFNMGdn0",0,1,False,self,,,,,
1234,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,1,benub0,self.MachineLearning,[D] Ian Goodfellow | MIT Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/benub0/d_ian_goodfellow_mit_artificial_intelligence/,UltraMarathonMan,1555606216,"Ian Goodfellow is the author of the popular textbook on deep learning (simply titled ""Deep Learning""). He coined the term Generative Adversarial Networks (GANs) and with his 2014 paper is responsible for launching the incredible growth of research on GANs. He got his BS and MS at Stanford, his PhD at University of Montreal with Yoshua Bengio and Aaron Courville. He held several research positions including at OpenAI, Google Brain, and now at Apple as director of machine learning. This recording happened while Ian was still at Google Brain.

[https://www.youtube.com/watch?v=Z6rxFNMGdn0](https://www.youtube.com/watch?v=Z6rxFNMGdn0)

https://i.redd.it/e9854r8dy1t21.png",13,111,False,https://b.thumbs.redditmedia.com/aV5wLfJKahxDmI9TCJYHSfyjLAzMfbXv1DWsrNmUYNs.jpg,,,,,
1235,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,2,beo1so,self.MachineLearning,Why is Python so popular in ML?,https://www.reddit.com/r/MachineLearning/comments/beo1so/why_is_python_so_popular_in_ml/,JackIsNotInTheBox,1555607279,[removed],0,1,False,self,,,,,
1236,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,2,beo9dw,self.MachineLearning,Top 10 Game Changing Machine Learning Startups You Need to Know About,https://www.reddit.com/r/MachineLearning/comments/beo9dw/top_10_game_changing_machine_learning_startups/,rahulwriter,1555608347,[removed],0,1,False,self,,,,,
1237,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,2,beo9l2,self.MachineLearning,When ML and Data Science are the death of a good company: A cautionary tale.,https://www.reddit.com/r/MachineLearning/comments/beo9l2/when_ml_and_data_science_are_the_death_of_a_good/,AlexSnakeKing,1555608374,[removed],0,1,False,self,,,,,
1238,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,2,beobik,self.MachineLearning,There are some end user application?,https://www.reddit.com/r/MachineLearning/comments/beobik/there_are_some_end_user_application/,chrnodroid_rift,1555608661,"I'm very sorry for this question, I'm not a developer but a simple user.

The question is: There are some application that use SRGAN? I mean, I just want to upscale and enhance images and videos.   
I have an exe application called ""waifu2x-caffe"" that use neural network to upscale images, but the result is not so good. So, if there are better applications I will very happy. 

&amp;#x200B;

Thanks in advance, and sorry",0,1,False,self,,,,,
1239,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,2,beogad,self.MachineLearning,[N] A Harvard Medical School scientist has used end-to-end differentiable deep learning to predict the 3D structure of effectively any protein based on its amino acid sequence. He achieved accuracy comparable to current state-of-the-art methods but at speeds upward of a million times faster.,https://www.reddit.com/r/MachineLearning/comments/beogad/n_a_harvard_medical_school_scientist_has_used/,skariel,1555609369,"link:

[https://www.eurekalert.org/pub\_releases/2019-04/hms-fr041619.php](https://www.eurekalert.org/pub_releases/2019-04/hms-fr041619.php)

x-post from r/science see [here](https://www.reddit.com/r/science/comments/beiik6/a_harvard_medical_school_scientist_has_used/)",12,17,False,self,,,,,
1240,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,3,beoqrs,self.MachineLearning,[D] how do you pick the right batch size for deep learning?,https://www.reddit.com/r/MachineLearning/comments/beoqrs/d_how_do_you_pick_the_right_batch_size_for_deep/,tdls_to,1555610876,"I've been trying to figure out the right way to pick the batch size for deep learning problems. not sure if there are generic guidelines for that or if it's pretty much experimental? any thoughts?

&amp;#x200B;

I also came across this [paper](https://arxiv.org/abs/1609.04836), and made this video to summarize it for a challenge; any feedback would be appreciated: [https://youtu.be/crag6bMM-0k](https://youtu.be/crag6bMM-0k)",12,3,False,self,,,,,
1241,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,3,beoxx8,self.MachineLearning,[Discussion] When ML and Data Science are the death of a good company: A cautionary tale.,https://www.reddit.com/r/MachineLearning/comments/beoxx8/discussion_when_ml_and_data_science_are_the_death/,AlexSnakeKing,1555611935,"*(This is a true story, happening to the company I currently work for. Names, domains, algorithms, and roles have been shuffled around to protect my anonymity)*

Company A has been around for several decades. It is not the biggest name in its domain, but it is a well respected one.Risk analysis and portfolio optimization have been a core of Company A's business since the 90s. They have a large team of 30 or so analysts who perform those tasks on a daily basis. These analysts use ERP solutions implemented for them by one the big ERP companies (SAP, Teradata, JD Edwards,...) or one of the Big 4 (Deloitte, Accenture, PWC, Capgemini) in collaboration with their own in house engineering team. The tools used are embarrassingly old school: Classic RDBMS running on on-prem servers or maybe even on mainframes, code written in COBOL, Fortran, weird proprietary stuff like ABAP or SPSS.....you get the picture. But the models and analytic functions were pretty sophisticated, and surprisingly cutting edge compared to the published academic literature. Most of all, they fit well with the company's enterprise ecosystem, and were honed based on years of deep domain knowledge.

They have a tech team of several engineers (poached from the aforementioned software and consulting companies) and product managers (who came from the experienced pools of analysts and managers who use the software, or poached from business rivals) maintaining and running this software. Their technology might be old school, but collectively, they know the domain and the company's overall architecture very, very well. They've guided the company through several large scale upgrades and migrations and they have a track record of delivering on time, without too much overhead. The few times they've stumbled, they knew how to pick themselves up very quickly.In fact within their industry niche, they have a reputation for their expertise, and have very good relations with the various vendors they've had to deal with. They were the launching pad of several successful ERP consulting careers.

Interestingly, despite dealing on a daily basis with statistical modeling and optimization algorithms, none of the analysts, engineers, or product managers involved describe themselves as data scientists or machine learning experts.It is mostly a cultural thing: Their expertise predates the Data Science/ML hype that started circa 2010, and they got most of their chops using proprietary enterprise tools instead of the open source tools popular nowadays.A few of them have formal statistical training, but most of them came from engineering or domain backgrounds and learned stats on the fly while doing their job. Call this team ""Team X"".

Sometime around the mid 2010s, Company A started having some serious anxiety issues: Although still doing very well for a company its size, overall economic and demographic trends were shrinking its customer base, and couple of so called disruptors came up with a new app and business model that started seriously eating into their revenue. A suitable reaction to appease shareholders and Wall Street was necessary. The company already had a decent website and pretty snazzy app, what more could be done?Leadership decided that it was high time that AI and ML become a core part of the company's business. An ambitious Manager, with no science or engineering background, but who had very briefly toyed with a recommender system a couple of years back, was chosen to build a data science team, call it team ""Y"" (he had a bachelor's in history from the local state college and worked for several years in the company's marketing org). Team ""Y"" consists mostly of internal hires who decided they wanted to be data scientists and completed a Coursera certification or a Galvanize boot camp, before being brought on to the team, along with a few of fresh Ph.D or M.Sc holders who didn't like academia and wanted to try their hand at an industry role. All of them were very bright people, they could write great Medium blog posts and give inspiring TED talks, but collectively they had very little real world industry experience. 

As is the fashion nowadays, this group was made part of a data science org that reported directly to the CEO and Board, bypassing the CIO and any tech or business VPs, since Company A wanted to claim the monikers ""data driven"" and ""AI powered"" in their upcoming shareholder meetings.In 3 or 4 years of existence, team Y produced a few Python and R scripts. Their architectural experience consisted almost entirely in connecting Flask to S3 buckets or Redshift tables, with a couple of the more resourceful ones learning how to plug their models into Tableau or how to spin up a Kuberneties pod. But they needn't worry: The aforementioned manager, who was now a director (and was also doing an online Masters to make up for his qualifications gap and bolster his chances of becoming VP soon - at least he now understands what L1 regularization is), was a master at playing corporate politics and self-promotion. No matter how few actionable insights team Y produced or how little code they deployed to production, he always had their back and made sure they had ample funding. In fact he now had grandiose plans for setting up an all-purpose machine learning platform that can be used to solve all of the company's data problems.

A couple of sharp minded members of team Y, upon googling their industry name along with the word ""data science"", realized that risk analysis was a prime candidate for being solved with Bayesian models, and there was already a nifty R package for doing just that, whose tutorial they went through on R-Bloggers.com. One of them had even submitted a Bayesian classifier Kernel for a competition on Kaggle (he was 203rd on the leaderboard), and was eager to put his new-found expertise to use on a real world problem.They pitched the idea to their director, who saw a perfect use case for his upcoming ML platform. They started work on it immediately, without bothering to check whether anybody at Company A was already doing risk analysis. Since their org was independent, they didn't really need to check with anybody else before they got funding for their initiative. Although it was basically a Naive Bayes classifier, the term ML was added to the project tile, to impress the board.

As they progressed with their work however, tensions started to build. They had asked the data warehousing and CA analytics teams to build pipelines for them, and word eventually got out to team X about their project. Team X was initially thrilled: They offered to collaborate whole heartedly, and would have loved to add an ML based feather to their already impressive cap. The product owners and analysts were totally onboard as well: They saw a chance to get in on the whole Data Science hype that they kept hearing about. But through some weird mix of arrogance and insecurity, team Y refused to collaborate with them or share any of their long term goals with them, even as they went to other parts of the company giving brown bag presentations and tutorials on the new model they created.

Team X got resentful: from what they saw of team Y's model, their approach was hopelessly naive and had little chances of scaling or being sustainable in production, and they knew exactly how to help with that. Deploying the model to production would have taken them a few days, given how comfortable they were with DevOps and continuous delivery (team Y had taken several months to figure out how to deploy a simple R script to production). And despite how old school their own tech was, team X were crafty enough to be able to plug it in to their existing architecture. Moreover, the output of the model was such that it didn't take into account how the business will consume it or how it was going to be fed to downstream systems, and the product owners could have gone a long way in making the model more amenable to adoption by the business stakeholders.But team Y wouldn't listen, and their leads brushed off any attempts at communication, let alone collaboration. The vibe that team Y was giving off was ""We are the cutting edge ML team, you guys are the legacy server grunts. We don't need your opinion."", and they seemed to have a complete disregard for domain knowledge, or worse, they thought that all that domain knowledge consisted of was being able to grasp the definitions of a few business metrics.

Team X got frustrated and tried to express their concerns to leadership. But despite owning a vital link in Company A's business process, they were only \~50 people in a large 1000 strong technology and operations org, and they were several layers removed from the C-suite, so it was impossible for them to get their voices heard.

Meanwhile, the unstoppable director was doing what he did best: Playing corporate politics. Despite how little his team had actually delivered, he had convinced the board that all analysis and optimization tasks should now be migrated to his yet to be delivered ML platform. Since most leaders now knew that there was overlap between team Y and team X's objectives, his pitch was no longer that team Y was going to create a new insight, but that they were going to replace (or modernize) the legacy statistics based on-prem tools with more accurate cloud based ML tools. Never mind that there was no support in the academic literature for the idea that Naive Bayes works better than the Econometric approaches used by team X, let alone the additional wacky idea that Bayesian Optimization would definitely outperform the QP solvers that were running in production.

Unbeknownst to team X, the original Bayesian risk analysis project has now grown into a multimillion dollar major overhaul initiative, which included the eventual replacement of all of the tools and functions supported by team X along with the necessary migration to the cloud. The CIO and a couple of business VPs are on now board, and tech leadership is treating it as a done deal.

An outside vendor, a startup who nobody had heard of, was contracted to help build the platform, since team Y has no engineering skills. The choice was deliberate, as calling on any of the established consulting or software companies would have eventually led leadership to the conclusion that team X was better suited for a transformation on this scale than team Y.

Team Y has no experience with any major ERP deployments, and no domain knowledge, yet they are being tasked with fundamentally changing the business process that is at the core of Company A's business. Their models actually perform worse than those deployed by team X, and their architecture is hopelessly simplistic, compared to what is necessary for running such a solution in production.

Ironically, using Bayesian thinking and based on all the evidence, the likelihood that team Y succeeds is close to 0%. At best, the project is going to end up being a write off of 50 million dollars or more. Once the !@#$!@# hits the fan, a couple of executive heads are going to role, and dozens of people will get laid off.

At worst, given how vital risk analysis and portfolio optimization is to Company A's revenue stream, the failure will eventually sink the whole company. It probably won't go bankrupt, but it will lose a significant portion of its business and work force.Failed ERP implementation can and do sink, see what happened to National Grid US, SuperValu or Target Canada.

One might argue that this is more about corporate disfunction and bad leadership than about data science and AI. 

But I disagree. I think the core driver of this debacle is indeed the blind faith in Data Scientists, ML models and the promise of AI, and the overall culture of hype and self promotion that is very common among the ML crowd.

We haven't seen the end of this story: I sincerely hope that this ends well for the sake of my colleagues and all involved. Company A is a good company, and both its customers and its employees deserver better. But the chances of that happening are negligible given all the information available, and this failure will hit my company hard.",215,645,False,self,,,,,
1242,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,4,bepg03,self.MachineLearning,Execution of a Neural Network using Cortex Assembler Language,https://www.reddit.com/r/MachineLearning/comments/bepg03/execution_of_a_neural_network_using_cortex/,ToolTechSoftware,1555614583,"This is an example of using CAL (Cortex Assembler Language) to execute a simple Neural Network using genomes

http://tooltech-software.com/CorTeX/execution_example.pdf",0,1,False,self,,,,,
1243,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,4,bepm4c,self.MachineLearning,I've visualized an image in the final layer of my neural network. Looks like gibberish. Do I have too many layers?,https://www.reddit.com/r/MachineLearning/comments/bepm4c/ive_visualized_an_image_in_the_final_layer_of_my/,Scutterbum,1555615487,"Below is the first and last layer of my convolutional neural network. (classifying skin lesions.)

I have two questions: 

&amp;#x200B;

1: Is the final layer useless for for the task, given that the images have been reduced to about 25 pixels each and there are no 'features' leftover. Should I reduce the size of the network?

2: Could somebody explain why some filters haven't activated at all in layer 1. I'm guessing this has something to do with relu's max(0,x) activation. And how do the majority of them reappear in the final layer if they never activated from the beginning?

&amp;#x200B;

Thanks folks

*Processing img 9o89a03rn2t21...*",0,1,False,self,,,,,
1244,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,4,bepofe,self.MachineLearning,Good Places to Learn TensorFlow,https://www.reddit.com/r/MachineLearning/comments/bepofe/good_places_to_learn_tensorflow/,esteven1234,1555615815,[removed],0,1,False,self,,,,,
1245,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,4,bepvid,genielab.github.io,Game of Thrones - Survival Analysis of Characters,https://www.reddit.com/r/MachineLearning/comments/bepvid/game_of_thrones_survival_analysis_of_characters/,genielab,1555616879,,0,1,False,default,,,,,
1246,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,4,beq02w,self.MachineLearning,Does Open AI five use items? If so how did they train the Ai on what items and what combinations?,https://www.reddit.com/r/MachineLearning/comments/beq02w/does_open_ai_five_use_items_if_so_how_did_they/,kalavala93,1555617557,,0,1,False,self,,,,,
1247,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,5,beq5mx,self.MachineLearning,[R] Objects as Points,https://www.reddit.com/r/MachineLearning/comments/beq5mx/r_objects_as_points/,AruniRC,1555618383,"&amp;#x200B;

https://i.redd.it/kc4e1b3ly2t21.png

**Abstract:** 

Detection identifies objects as axis-aligned boxes in an image. Most  successful object detectors enumerate a nearly exhaustive list of  potential object locations and classify each. This is wasteful,  inefficient, and requires additional post-processing. In this paper, we  take a different approach. We model an object as a single point -- the  center point of its bounding box. Our detector uses keypoint estimation  to find center points and regresses to all other object properties, such  as size, 3D location, orientation, and even pose. Our center point  based approach, CenterNet, is end-to-end differentiable, simpler,  faster, and more accurate than corresponding bounding box based  detectors. CenterNet achieves the best speed-accuracy trade-off on the  MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1%  AP with multi-scale testing at 1.4 FPS. We use the same approach to  estimate 3D bounding box in the KITTI benchmark and human pose on the  COCO keypoint dataset. Our method performs competitively with  sophisticated multi-stage methods and runs in real-time.

&amp;#x200B;

**Github:** [https://github.com/xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet)",2,21,False,https://a.thumbs.redditmedia.com/fZcYPLVEtAbz0Z0Lks2kCH7qTd7SBTN7PNwJW2AxIB0.jpg,,,,,
1248,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,5,beq6v6,self.MachineLearning,[R] A Bayesian Perspective on the Deep Image Prior - CVPR 2019,https://www.reddit.com/r/MachineLearning/comments/beq6v6/r_a_bayesian_perspective_on_the_deep_image_prior/,AruniRC,1555618570,"**Abstract:** 

The [deep image prior](https://dmitryulyanov.github.io/deep_image_prior)  was recently introduced as a prior for natural images. It represents  images as the output of a convolutional network with random inputs. For  inference, gradient descent is performed to adjust network parameters  to make the output match observations. This approach yields good  performance on a range of image reconstruction tasks. We show that the  deep image prior is asymptotically equivalent to a stationary Gaussian  process prior in the limit as the number of channels in each layer of  the network goes to infinity, and derive the corresponding kernel. This  informs a Bayesian approach to inference. We show that by conducting  posterior inference using stochastic gradient Langevin we avoid the need  for early stopping, which is a drawback of the current approach, and  improve results for denoising and impainting tasks. We illustrate these  intuitions on a number of 1D and 2D signal reconstruction tasks.  

&amp;#x200B;

**Project page:** [https://people.cs.umass.edu/\~zezhoucheng/gp-dip/](https://people.cs.umass.edu/~zezhoucheng/gp-dip/)",2,19,False,self,,,,,
1249,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,5,beqdcp,self.MachineLearning,CIFAR network using perceptrons,https://www.reddit.com/r/MachineLearning/comments/beqdcp/cifar_network_using_perceptrons/,ToolTechSoftware,1555619566,[removed],2,1,False,self,,,,,
1250,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,5,beqhat,youtube.com,Classification In The Wild (Tutorial),https://www.reddit.com/r/MachineLearning/comments/beqhat/classification_in_the_wild_tutorial/,PathToNeuralink,1555620176,,0,1,False,https://b.thumbs.redditmedia.com/GQrNmAAoG2vxinWy_TfCalNOo7NUhG2I7dd_Ghj6COA.jpg,,,,,
1251,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,5,beqir2,self.MachineLearning,Combining Latitude/Longitude position into single feature,https://www.reddit.com/r/MachineLearning/comments/beqir2/combining_latitudelongitude_position_into_single/,krizam,1555620399,[removed],0,1,False,self,,,,,
1252,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,6,beqwez,self.MachineLearning,I need help with style transfer (clueless) and github,https://www.reddit.com/r/MachineLearning/comments/beqwez/i_need_help_with_style_transfer_clueless_and/,Slaterv,1555622437,[removed],0,1,False,self,,,,,
1253,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,6,beqx9w,self.MachineLearning,How do you version control your ML experients / projects? MLFlow vs. DVC vs. Git-LFS?,https://www.reddit.com/r/MachineLearning/comments/beqx9w/how_do_you_version_control_your_ml_experients/,philosophical_lens,1555622569,[removed],0,1,False,self,,,,,
1254,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,6,ber9cj,anl.gov,"Through machine learning, new model holds water",https://www.reddit.com/r/MachineLearning/comments/ber9cj/through_machine_learning_new_model_holds_water/,Chipdoc,1555624444,,0,1,False,default,,,,,
1255,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,7,bermi4,self.MachineLearning,Tensorflow 2.12 patch,https://www.reddit.com/r/MachineLearning/comments/bermi4/tensorflow_212_patch/,surety_,1555626378,[removed],0,1,False,self,,,,,
1256,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,7,berq7j,self.MachineLearning,Is Gaming considered labeled or unlabeled data? Also is mastering games good for general AI or better narrow AI?,https://www.reddit.com/r/MachineLearning/comments/berq7j/is_gaming_considered_labeled_or_unlabeled_data/,kalavala93,1555626923,,0,1,False,self,,,,,
1257,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,7,berqr2,self.MachineLearning,[N] IBM halting sales of Watson AI tool for drug discovery amid sluggish growth,https://www.reddit.com/r/MachineLearning/comments/berqr2/n_ibm_halting_sales_of_watson_ai_tool_for_drug/,inarrears,1555627007,"Citing lackluster financial performance, IBM is [halting](https://www.statnews.com/2019/04/18/ibm-halting-sales-of-watson-for-drug-discovery/) development and sales of a product that uses its Watson AI software to help pharmaceutical companies discover new drugs, news outlet Stat reported on Thursday, citing a person familiar with the company's internal decision-making. From the report:

*The decision to shut down sales of Watson for Drug Discovery marks the highest-profile retreat in the company's effort to apply artificial intelligence to various areas of health care. Last year, the company scaled back on the hospital side of its business, and it's struggled to develop a reliable tool to assist doctors in treating cancer patients.*

In a statement, an IBM spokesperson said, ""We are focusing our resources within Watson Health to double down on the adjacent field of clinical development where we see an even greater market need for our data and AI capabilities.""",10,44,False,self,,,,,
1258,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,8,bes3c6,self.MachineLearning,"Biz Q. Will smaller companies be able to monetize machine learning and AI? Or only amazon, microsoft, google etc.",https://www.reddit.com/r/MachineLearning/comments/bes3c6/biz_q_will_smaller_companies_be_able_to_monetize/,NwCom123,1555628877,[removed],0,1,False,self,,,,,
1259,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,10,betaq6,self.MachineLearning,OCR with Neural Networks,https://www.reddit.com/r/MachineLearning/comments/betaq6/ocr_with_neural_networks/,string_cluster,1555635951,[removed],0,1,False,self,,,,,
1260,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,10,betdot,self.MachineLearning,[P] Helping a total newbie learn to run SC-FEGAN,https://www.reddit.com/r/MachineLearning/comments/betdot/p_helping_a_total_newbie_learn_to_run_scfegan/,sytrix,1555636431,"I'm sorry if this is the wrong place to ask but I've Googled my issues and I'm at a loss on helpful resources.

I'm not familiar with neural networks but would like to try [SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN) out. I have the files, the model, and Python downloaded and I'm stuck on getting this working:

    mv /${HOME}/SC-FEGAN.ckpt.* /${HOME}/ckpt/
    python3 demo.py

Should I be using command prompt or Python to launch the GUI? How do I use the other dependencies listed (tensorflow, numpy, Python3, PyQt5, opencv-python, pyyaml)?

ELI5, any advice would be appreciated, thank you for your time :)",2,0,False,self,,,,,
1261,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,11,betwla,self.MachineLearning,Web Scraping the SEC with Python,https://www.reddit.com/r/MachineLearning/comments/betwla/web_scraping_the_sec_with_python/,areed1192,1555639808,[removed],0,1,False,self,,,,,
1262,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,11,beualn,youtu.be,"Introducing WCG AI Masters, Deep Learning Soccer Competition",https://www.reddit.com/r/MachineLearning/comments/beualn/introducing_wcg_ai_masters_deep_learning_soccer/,NerfHanaSong,1555642348,,0,1,False,https://a.thumbs.redditmedia.com/Ik53Cg1MtZg544GaTkkhx43Rg8-Tw4zny5Q64nca6V4.jpg,,,,,
1263,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,12,beuhoq,self.MachineLearning,Deciding length of units in sound recognition for training HMMs,https://www.reddit.com/r/MachineLearning/comments/beuhoq/deciding_length_of_units_in_sound_recognition_for/,Toodeveloped,1555643647,[removed],0,1,False,self,,,,,
1264,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,12,beui6h,youtube.com,[D] AI Draws Caricatures (WarpGAN),https://www.reddit.com/r/MachineLearning/comments/beui6h/d_ai_draws_caricatures_warpgan/,hanyuqn,1555643735,,0,1,False,default,,,,,
1265,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,12,beutqv,arxiv.org,[R] Neural Painters: A learned differentiable constraint for generating brushstroke paintings,https://www.reddit.com/r/MachineLearning/comments/beutqv/r_neural_painters_a_learned_differentiable/,baylearn,1555645991,,3,28,False,default,,,,,
1266,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,13,beuxmg,self.MachineLearning,Imposing a Directed acyclic graph (DAG) or grouping on gradient boosting inputs,https://www.reddit.com/r/MachineLearning/comments/beuxmg/imposing_a_directed_acyclic_graph_dag_or_grouping/,Radon-Nikodym,1555646739,[removed],0,1,False,self,,,,,
1267,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,13,bevaam,codeingschool.com,Simple Linear Regression: How It works? (Python Implementation),https://www.reddit.com/r/MachineLearning/comments/bevaam/simple_linear_regression_how_it_works_python/,subhamroy021,1555649284,,0,1,False,default,,,,,
1268,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,14,bevq17,self.MachineLearning,"This is an article explaining the difference between Artificial intelligence, machine learning and data science in a very concise manner",https://www.reddit.com/r/MachineLearning/comments/bevq17/this_is_an_article_explaining_the_difference/,rkt10952,1555652880,[removed],0,1,False,self,,,,,
1269,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,15,bevwbs,dimensionless.in,What is the Difference Between AI and Machine Learning | DIMENSIONLESS TECHNOLOGIES PVT.LTD.,https://www.reddit.com/r/MachineLearning/comments/bevwbs/what_is_the_difference_between_ai_and_machine/,kartik2019,1555654307,,0,1,False,default,,,,,
1270,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,15,bew03v,self.datasets,Looking for dataset(s) containing Action items in meeting transcripts,https://www.reddit.com/r/MachineLearning/comments/bew03v/looking_for_datasets_containing_action_items_in/,chandra_siri,1555655205,,0,1,False,default,,,,,
1271,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,16,bew8a2,self.MachineLearning,Genetic algorithm solving a maze,https://www.reddit.com/r/MachineLearning/comments/bew8a2/genetic_algorithm_solving_a_maze/,KhanOnTheTube,1555657216,[removed],0,1,False,self,,,,,
1272,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,16,bewcp6,self.MachineLearning,"[P] Rainbow-IQN, that reaches the perfect score (+21) on Atari Pong within 100 episodes!",https://www.reddit.com/r/MachineLearning/comments/bewcp6/p_rainbowiqn_that_reaches_the_perfect_score_21_on/,Curt-Park,1555658240,"* Project link: https://github.com/medipixel/rl_algorithms

Hi, ML redditors all around the world! Recently, I have been studying Reinforcement Learning methods and implementing some of the ideas that could possibly improve my work. Not long ago I took a look at Implicit Quantile Networks for Distributional Reinforcement Learning (Dabney et al., 2018), and one sentence at the end caught my attention.

""Creating a Rainbow-IQN agent could yield even greater improvements on Atari-57.""

So I did it. As the figure attached in the project readme, It learns Atari Pong incredibly faster than Rainbow as it reaches the perfect score (+21) within just 100 episodes. I am so happy to share this result, even though it is not so enough to evaluate the method's performance objectively.

Plus, the repository contains other methods as well: A2C, PPO, DDPG, TD3, SAC, from Demonstration, Behaviour Cloning, and so on.

Any feedback will be so appreciated!
Thanks :)",24,54,False,self,,,,,
1273,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,17,bewrip,self.MachineLearning,Decision Tree,https://www.reddit.com/r/MachineLearning/comments/bewrip/decision_tree/,ml_coder_pro,1555662051," 

**Decision Tree** is a supervised learning method used for classification and regression. It is a tree which helps us by assisting us in decision-making!

**Decision tree builds classification or regression models** in the form of a tree structure. It breaks down a data set into smaller and smaller subsets and simultaneously decision tree is incrementally developed. The final tree is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. We cannot do more split on leaf nodes.

The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.  
Common terms used with Decision trees:  
**Root Node**: It represents entire population or sample and this further gets divided into two or more homogeneous sets.  
**Splitting**: It is a process of dividing a node into two or more sub-nodes.  
**Decision Node**: When a sub-node splits into further sub-nodes, then it is called decision node.  
**Leaf/ Terminal Node:** Nodes do not split is called Leaf or Terminal node.  
**ID3 Algorithm**  
**Key Factors:**  
**Entropy**\- It is the measure of randomness or *impurity* in the dataset.  
**Information Gain**: It is the measure of decrease in entropy after the dataset is split.    
[https://youtu.be/UdTKxGQvYdc](https://youtu.be/UdTKxGQvYdc?fbclid=IwAR2c9aP24Oi7_5ewpvBB8nJyqLtOsVGEZ6ZSFN6glI0rpSSSLUmydySnPy8)",0,1,False,self,,,,,
1274,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,17,bewsu4,self.MachineLearning,How to build an abstractive text summarizer.,https://www.reddit.com/r/MachineLearning/comments/bewsu4/how_to_build_an_abstractive_text_summarizer/,dj_lonewolf,1555662446,[removed],0,1,False,self,,,,,
1275,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,17,bewxte,self.MachineLearning,JAX Vs other frameworks,https://www.reddit.com/r/MachineLearning/comments/bewxte/jax_vs_other_frameworks/,namuchan95,1555663804,[removed],0,1,False,self,,,,,
1276,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,18,bex5qf,accupanels.com,"APFC Panel manufacturer, exporter and supplier in Ahmedabad, India",https://www.reddit.com/r/MachineLearning/comments/bex5qf/apfc_panel_manufacturer_exporter_and_supplier_in/,mayankshah90909,1555665764,,0,1,False,default,,,,,
1277,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,19,bexgmh,zeroequalsfalse.press,Best Artificial Intelligence Books in 2019,https://www.reddit.com/r/MachineLearning/comments/bexgmh/best_artificial_intelligence_books_in_2019/,woahdotcom,1555668427,,0,1,False,https://b.thumbs.redditmedia.com/yXYgEvQcFHLuKosZop5V-g_b3dcjJ1k2iBExXauY8iI.jpg,,,,,
1278,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,19,bexjhg,self.MachineLearning,Machine Learning Online Courses Provide The Best Preparation for a Machine Learning Certification Exam,https://www.reddit.com/r/MachineLearning/comments/bexjhg/machine_learning_online_courses_provide_the_best/,multisoftmva0,1555669117,[removed],0,1,False,self,,,,,
1279,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,19,bexkkd,medium.com,A Shortcut To Achieving High Quality GAN Result,https://www.reddit.com/r/MachineLearning/comments/bexkkd/a_shortcut_to_achieving_high_quality_gan_result/,alvisanovari,1555669375,,0,1,False,default,,,,,
1280,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,21,beyflk,self.MachineLearning,Data Science Summit - conference in Poland,https://www.reddit.com/r/MachineLearning/comments/beyflk/data_science_summit_conference_in_poland/,DataScienceSummit,1555676178,[removed],0,1,False,self,,,,,
1281,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,21,beygdu,self.MachineLearning,[D] Term for keeping test and training data separate,https://www.reddit.com/r/MachineLearning/comments/beygdu/d_term_for_keeping_test_and_training_data_separate/,ipsLED87,1555676323,"So, I've been using the term ""data hygiene"" for the measures we take in (safety) ML to keep test and training data separate. 
Stuff like

* test data on access controlled network share
* acquiring the test set later and by different teams 
* [Thresholdout](https://www.zillow.com/data-science/double-dip-holdout-set) (when I finally get the chance to play around with it)

But apparently, I just read that term in some fringe paper once and actually data hygiene is a separate concept in data science?! 

Does anyone got a good term for the methods/approaches?",15,6,False,self,,,,,
1282,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,21,beym9o,self.MachineLearning,Looking for help with my assignment!,https://www.reddit.com/r/MachineLearning/comments/beym9o/looking_for_help_with_my_assignment/,helpnhelper,1555677451,[removed],0,1,False,self,,,,,
1283,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,21,beyn99,self.MachineLearning,iOS ML Kit: Advantages of Machine Learning in Your Pocket,https://www.reddit.com/r/MachineLearning/comments/beyn99/ios_ml_kit_advantages_of_machine_learning_in_your/,skypotentialltd,1555677635,[removed],0,1,False,self,,,,,
1284,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,22,beyunq,self.MachineLearning,iOS ML Kit: Advantages of Machine Learning in Your Pocket,https://www.reddit.com/r/MachineLearning/comments/beyunq/ios_ml_kit_advantages_of_machine_learning_in_your/,Sarahalfred09,1555679005,"There are a few preferred advantage of machine learning that are required for you to comprehend and suggest on your iOS development. [Read here](https://medium.com/@skypotentialltd/ios-ml-kit-advantages-of-machine-learning-in-your-pocket-f5183a9a21d9):  

&amp;#x200B;

![img](sjebk6hty7t21)",0,1,False,self,,,,,
1285,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,22,bez3yt,self.MachineLearning,machine learning or web devlopment,https://www.reddit.com/r/MachineLearning/comments/bez3yt/machine_learning_or_web_devlopment/,oussama111,1555680576,[removed],0,1,False,self,,,,,
1286,MachineLearning,t5_2r3gv,2019-4-19,2019,4,19,23,beztdj,venturebeat.com,Facebooks AI extracts playable characters from real-world videos,https://www.reddit.com/r/MachineLearning/comments/beztdj/facebooks_ai_extracts_playable_characters_from/,SuchRush,1555684555,,0,0,False,https://b.thumbs.redditmedia.com/nY7a0rLF4gSe5WswqRQElwXQy3avnCqoXwp_z_vCprg.jpg,,,,,
1287,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,1,bf10um,self.MachineLearning,Best Algorithm Pest Detection using audio?,https://www.reddit.com/r/MachineLearning/comments/bf10um/best_algorithm_pest_detection_using_audio/,nambatac80,1555690836,[removed],0,1,False,self,,,,,
1288,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,1,bf137p,self.MachineLearning,[P] Python package to easily retrain OpenAI's GPT-2 text-generating model on new texts + Colaboratory Notebook to use it w/ GPU for free,https://www.reddit.com/r/MachineLearning/comments/bf137p/p_python_package_to_easily_retrain_openais_gpt2/,minimaxir,1555691165,"Hi all! I just open-sourced a [Python package on GitHub](https://github.com/minimaxir/gpt-2-simple) that lets you retrain the smaller GPT-2 model on your own text with minimal code! (and without fussing around with the CLI like the original repo)

I have also made a [Colaboratory Notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) which handles both training w/ a GPU **for free** and file I/O to the notebook (which with GPT-2 is a tad tricker).

Let me know if you have any questions! I plan on releasing more demos soon!",59,200,False,self,,,,,
1289,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,1,bf1byy,self.MachineLearning,Autonomous cars simulation question,https://www.reddit.com/r/MachineLearning/comments/bf1byy/autonomous_cars_simulation_question/,reqursion,1555692402,[removed],0,1,False,self,,,,,
1290,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,2,bf1q0x,self.MachineLearning,100 Time Series Data Mining Questions (with Answers!),https://www.reddit.com/r/MachineLearning/comments/bf1q0x/100_time_series_data_mining_questions_with_answers/,eamonnkeogh,1555694343,[removed],1,1,False,self,,,,,
1291,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,2,bf1xh2,self.MachineLearning,[D] Any tips and tricks to crack the DeepMind quiz interview?,https://www.reddit.com/r/MachineLearning/comments/bf1xh2/d_any_tips_and_tricks_to_crack_the_deepmind_quiz/,DependentSky6,1555695402,"Hi all!  


I'm interviewing for a PhD internship at DeepMind. The process involves 1) a discussion with recruiters, 2) a 2 hours long technical test (the DeepMind quiz), 3) discussions with research scientists, 4) a final interview with the People and Culture team.  


**I'm wondering if any of you have tips for the technical test.** What is the best way to prepare for it?  


I heard it's only a sort of ""fact checking"" interview involving computer science, maths, stats and ML: questions where you either know the answer, or you don't, no thinking involved. Anyone can confirm this?   


Also, they provided me with some resources, but these are thick books I would like to avoid. I'm looking for the most efficient way to prepare for it, since it mostly involves some BS knowledge I will forget 5 minutes after the interview.

&amp;#x200B;

Thanks in advance! :)",30,22,False,self,,,,,
1292,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,2,bf1ye6,self.MachineLearning,Are there any significant differences between deep learning and early neural networks? (besides number of layers),https://www.reddit.com/r/MachineLearning/comments/bf1ye6/are_there_any_significant_differences_between/,bobmichal,1555695527,,0,1,False,self,,,,,
1293,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,2,bf26l3,ai.stanford.edu,[R] Uncertainty Autoencoders: Learning Compressed Representations via Variational Information Maximization,https://www.reddit.com/r/MachineLearning/comments/bf26l3/r_uncertainty_autoencoders_learning_compressed/,regalalgorithm,1555696716,,0,11,False,default,,,,,
1294,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,3,bf27d1,self.MachineLearning,Passing a feature and a condition (if) into a custom loss for optimizer in Keras ?,https://www.reddit.com/r/MachineLearning/comments/bf27d1/passing_a_feature_and_a_condition_if_into_a/,GuinsooIsOverrated,1555696824,[removed],0,1,False,self,,,,,
1295,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,3,bf2cq8,self.MachineLearning,[P] This Story Does Not Exist,https://www.reddit.com/r/MachineLearning/comments/bf2cq8/p_this_story_does_not_exist/,eukaryote31,1555697578,"I took GPT-2-small, fine-tuned it with r/WritingPrompts data, and put it online. 

&amp;#x200B;

**Try it out here:** [**https://www.thisstorydoesnotexist.com/**](https://www.thisstorydoesnotexist.com/)

&amp;#x200B;

**Shameless Self-Promotion:**

Please follow me on my [twitter](https://twitter.com/eukaryote314) for updates, and donate to me on [Patreon](https://www.patreon.com/eukaryote) to help keep stuff online. 

&amp;#x200B;

**Technical Details:**

Training was done with: [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2) with some minor modifications

Training was done with batch size of 512 (batch 2 accumulated 256 times) for 1500 iterations using Adam with lr=1e-5. 

Samples are generated with top\_k=50, temperature=0.95. 

The webservice is two n1 instances on GCP, an n1 instance with a K80 (preemptible), and a desktop computer with a 1080Ti in my basement. 

&amp;#x200B;

If you have any questions about this project, please ask! :)",51,48,False,self,,,,,
1296,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,4,bf34qu,medium.com,Stanford ML Releases MRNet Knee MRI Dataset,https://www.reddit.com/r/MachineLearning/comments/bf34qu/stanford_ml_releases_mrnet_knee_mri_dataset/,Yuqing7,1555701609,,0,1,False,https://b.thumbs.redditmedia.com/QJ_spNAl761xT2drN0gMLloeSrmP_iB-ygE-rontLdo.jpg,,,,,
1297,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,5,bf3ln2,medium.com,Facebook Randomly Wired Neural Networks Outperform Humans for Image Recognition,https://www.reddit.com/r/MachineLearning/comments/bf3ln2/facebook_randomly_wired_neural_networks/,Yuqing7,1555704081,,0,1,False,https://b.thumbs.redditmedia.com/vzolJE862S4TDx4YwezZaUrMzbRiUuYGPodlNvl43kU.jpg,,,,,
1298,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,5,bf3pcp,self.MachineLearning,Random Walks with memory and machine learning,https://www.reddit.com/r/MachineLearning/comments/bf3pcp/random_walks_with_memory_and_machine_learning/,shelbyuriel,1555704613,[removed],0,1,False,self,,,,,
1299,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,5,bf416g,self.MachineLearning,Training GPT-2 with custom data set questions &amp; perspectives,https://www.reddit.com/r/MachineLearning/comments/bf416g/training_gpt2_with_custom_data_set_questions/,icantfindanametwice,1555706373,[removed],0,1,False,self,,,,,
1300,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,6,bf4lw1,reddit.com,X-post Hello - we're the dev team behind OpenAI Five! We will be answering questions starting at 2:30pm PDT.,https://www.reddit.com/r/MachineLearning/comments/bf4lw1/xpost_hello_were_the_dev_team_behind_openai_five/,jonathanraiman,1555709546,,0,69,False,default,,,,,
1301,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,8,bf5lwi,arxiv.org,End-to-End Robotic Reinforcement Learning without Reward Engineering,https://www.reddit.com/r/MachineLearning/comments/bf5lwi/endtoend_robotic_reinforcement_learning_without/,Jaxon_K,1555715225,,1,32,False,default,,,,,
1302,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,8,bf634g,arxiv.org,Guidelines and Benchmarks for Deployment of Deep Learning Models on Smartphones as Real-Time Apps,https://www.reddit.com/r/MachineLearning/comments/bf634g/guidelines_and_benchmarks_for_deployment_of_deep/,ResponsibleFlamingo0,1555718144,,1,1,False,default,,,,,
1303,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,9,bf64pc,self.MachineLearning,Maybe a noob question; does anyone know how to clear GPU memory with pytorch?,https://www.reddit.com/r/MachineLearning/comments/bf64pc/maybe_a_noob_question_does_anyone_know_how_to/,MrAcurite,1555718424,[removed],0,1,False,self,,,,,
1304,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,9,bf6hc3,youtu.be,"Alan Turing, Cybernetics and the Secrets of Life",https://www.reddit.com/r/MachineLearning/comments/bf6hc3/alan_turing_cybernetics_and_the_secrets_of_life/,owlentity,1555720653,,1,1,False,https://b.thumbs.redditmedia.com/TEoSc8utSaR60qdWQ_xL8_saBvWNKijHsn7iFy0ke0Y.jpg,,,,,
1305,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,10,bf6zhs,self.MachineLearning,[D] Inverse reinforcement learning without assuming the agent's behaviour is optimal?,https://www.reddit.com/r/MachineLearning/comments/bf6zhs/d_inverse_reinforcement_learning_without_assuming/,strangecosmos,1555723981,"As I understand it (and please correct me if I'm wrong), [inverse reinforcement learning](https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/) + reinforcement learning will eventually produce the same result as supervised learning/behavioural cloning. Inverse RL assumes the agent's behaviour is optimal, so it will end up just imitating the agent. 

Let's say you want to do a task _better_ than the agent. Has there been any research on deriving a reward function from agent behaviour without assuming the agent's behaviour is optimal?",11,5,False,self,,,,,
1306,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,11,bf7a1m,self.MachineLearning,What is the cost of training GPT-2 1.5B from scratch?,https://www.reddit.com/r/MachineLearning/comments/bf7a1m/what_is_the_cost_of_training_gpt2_15b_from_scratch/,khashei,1555725939,[removed],0,1,False,self,,,,,
1307,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,11,bf7i9d,self.MachineLearning,[D] Recommendations for quality kaggle projects for resume building?,https://www.reddit.com/r/MachineLearning/comments/bf7i9d/d_recommendations_for_quality_kaggle_projects_for/,math_is_my_religion,1555727513,"Hey all,

I'm a new MS graduate trying to find work in ML in the Silicon Valley, and I need to bolster my resume. I read that I can substitute experience with quality projects, and while I have a number of those, they are all done on my own with no outside institution evaluating them. I figured a Kaggle project would be a perfect way to show that I know my stuff. 

Anyone in industry have a particular challange that they like to see on a resume? Any challenges that I should avoid? I prefer working in NLP.",11,27,False,self,,,,,
1308,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,11,bf7jvj,techxplore.com,A neural network can read scientific papers and render a plain-English summary,https://www.reddit.com/r/MachineLearning/comments/bf7jvj/a_neural_network_can_read_scientific_papers_and/,oligarch222,1555727833,,0,1,False,https://a.thumbs.redditmedia.com/afCSgWahbLDQLcL_T9Qq5eWtljHrsKRuc-IGoYjilm0.jpg,,,,,
1309,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,12,bf7rlj,self.MachineLearning,Custom Activation Functions in Pytorch?,https://www.reddit.com/r/MachineLearning/comments/bf7rlj/custom_activation_functions_in_pytorch/,ZeroMaxinumXZ,1555729375,[removed],0,1,False,self,,,,,
1310,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,12,bf80pm,self.MachineLearning,HOW DID YOU GUYS GO FROM A NOT-KNOWING-ANYTHING PERSON TO THIS DAY?,https://www.reddit.com/r/MachineLearning/comments/bf80pm/how_did_you_guys_go_from_a_notknowinganything/,Youth_Fountain,1555731230,[removed],0,1,False,self,,,,,
1311,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,13,bf8n10,self.MachineLearning,Where i can find the best resource for decision tree,https://www.reddit.com/r/MachineLearning/comments/bf8n10/where_i_can_find_the_best_resource_for_decision/,ml_coder_pro,1555735989,[removed],1,1,False,self,,,,,
1312,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,14,bf8qvs,self.MachineLearning,Weights for Models trained on subsets of ImageNet,https://www.reddit.com/r/MachineLearning/comments/bf8qvs/weights_for_models_trained_on_subsets_of_imagenet/,The_mCherry_Man,1555736796,[removed],0,1,False,self,,,,,
1313,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,14,bf8tm1,self.MachineLearning,Where and How to learn tensorflow?,https://www.reddit.com/r/MachineLearning/comments/bf8tm1/where_and_how_to_learn_tensorflow/,programer_mack,1555737407,[removed],0,1,False,self,,,,,
1314,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,14,bf8u41,self.MachineLearning,"[P] Flattr, a book scanner app using TensorFlow Lite for realtime preview",https://www.reddit.com/r/MachineLearning/comments/bf8u41/p_flattr_a_book_scanner_app_using_tensorflow_lite/,prodia4,1555737522,"Hello reddit!

We've just launched an app called **Flattr**that scans books quickly and beautifully, flattening the curved pages with the help of deep learning.

It uses TensorFlow Lite to flatten a page of a book and shows the flattened &amp; cropped page in realtime. (The performance of the app may vary depending on your device.)

&amp;#x200B;

To see how it works, here's a demo video:[https://youtu.be/SlQbIQU62P4](https://youtu.be/SlQbIQU62P4)

&amp;#x200B;

Currently, its features are:

\- Live preview of the flattened book page  
\- Auto-cropping  
\- Scanned page enhancement that makes the flattened page much easier to read  
\- Save pages to your gallery  
 

You can download the app from here:[https://play.google.com/store/apps/details?id=com.voyagerx.flattr](https://play.google.com/store/apps/details?id=com.voyagerx.flattr)

&amp;#x200B;

The app is at an**early development stage**but we're improving fast and some of the features that could be added later are ...

&amp;#x200B;

**Our todo list**

\- Improve the quality of the scanned pages  
\- Improve the page flattening model  
\- Compile scanned pages into a book  
\- Export to PDF  
\- Support various document types  
\- Add OCR: Recognizing texts in images  
\- Add auto-scan: No need to touch the preview screen  
\- Add auto page number detection &amp; page sorting  
 

Oh and it's**FREE!**

&amp;#x200B;

Give it a try and tell us your thoughts, thanks!",85,367,False,self,,,,,
1315,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,14,bf8v0c,self.MachineLearning,Stanford ML Releases MRNet Knee MRI Dataset,https://www.reddit.com/r/MachineLearning/comments/bf8v0c/stanford_ml_releases_mrnet_knee_mri_dataset/,aravinth24,1555737736,[removed],0,1,False,self,,,,,
1316,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,15,bf9hya,evernote.com,Fast-Track Your Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bf9hya/fasttrack_your_machine_learning/,Divya123divya,1555743146,,0,1,False,default,,,,,
1317,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,16,bf9pt1,self.MachineLearning,opportunity to work on latest project salry upto $30000 for ML experts please dont spam,https://www.reddit.com/r/MachineLearning/comments/bf9pt1/opportunity_to_work_on_latest_project_salry_upto/,ml_coder_pro,1555745137,we are a startup from india working for a new hospital dataset anyone interested message me or comment i will check there profile and let them or share our github profile.,0,0,False,self,,,,,
1318,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,17,bfa4es,self.MachineLearning,MSc Machine Learning Project Suggestions,https://www.reddit.com/r/MachineLearning/comments/bfa4es/msc_machine_learning_project_suggestions/,JazHeir,1555749041,[removed],0,1,False,self,,,,,
1319,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,17,bfa8dg,self.MachineLearning,Looking for a cheap ML Computer Hardware,https://www.reddit.com/r/MachineLearning/comments/bfa8dg/looking_for_a_cheap_ml_computer_hardware/,Jandevries101,1555750169,[removed],0,1,False,self,,,,,
1320,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,18,bfamk1,self.MachineLearning,Automatic Speech Recognition in Python,https://www.reddit.com/r/MachineLearning/comments/bfamk1/automatic_speech_recognition_in_python/,harsh02it,1555754022,[removed],0,1,False,self,,,,,
1321,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,20,bfbgeg,self.learnmachinelearning,[D] Multidimensional regression: Should I / how to make sure the error variances are the same along different dimensions.,https://www.reddit.com/r/MachineLearning/comments/bfbgeg/d_multidimensional_regression_should_i_how_to/,journeymango,1555761522,,0,1,False,default,,,,,
1322,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,21,bfbhvn,self.MachineLearning,Recommendation for industry research opportunities for top PhD program applicants?,https://www.reddit.com/r/MachineLearning/comments/bfbhvn/recommendation_for_industry_research/,BadCodee,1555761838,[removed],0,1,False,self,,,,,
1323,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,21,bfbt7k,/r/MachineLearning/comments/bfbt7k/p_stylegan_on_the_oxford_visual_geometry_group/,[P] StyleGAN on the Oxford Visual Geometry Group Flowers 102 Dataset,https://www.reddit.com/r/MachineLearning/comments/bfbt7k/p_stylegan_on_the_oxford_visual_geometry_group/,neurokinetikz,1555764270,,0,1,False,default,,,,,
1324,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,21,bfbvii,towardsdatascience.com,"Accuracy, Recall, Precision, F-Score &amp; Specificity, which to optimize on?",https://www.reddit.com/r/MachineLearning/comments/bfbvii/accuracy_recall_precision_fscore_specificity/,salma-ghoneim,1555764771,,0,1,False,default,,,,,
1325,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,21,bfbw8b,self.MachineLearning,"Do you know any useful tips, examples, articles etc. for better GPU utilization?",https://www.reddit.com/r/MachineLearning/comments/bfbw8b/do_you_know_any_useful_tips_examples_articles_etc/,sequence_9,1555764921,"It's been 6 months since I started learning deep learning. Finally last week I implemented DQN for atari games. It is in its simplest form with 3 conv layers, 2 dense layers, replay memory and fixed targets. This week I upgraded my gpu from  a gtx 950 to rtx2060, and the training speed is only increased like 10-20%. I know it is a simple code for maybe higher gpu utilization, but it is kind of huge for me, and honestly I was expecting it to scale similar with its fp32 calculation capabilities(x3.5-4). Obviously I can't utilize my gpu, and I'd like to learn if there is something I can do to improve my code in the future outside of just increasing batch size.",0,1,False,self,,,,,
1326,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,22,bfc3m5,1329d3ec-452a-48f4-8599-2f1102f4bdbb.htmlpasta.com,[P] Quick Demo of Landing a Rocket from Unfavourable Trajectories (Reinforcement Learning),https://www.reddit.com/r/MachineLearning/comments/bfc3m5/p_quick_demo_of_landing_a_rocket_from/,Kuvster98,1555766362,,0,1,False,default,,,,,
1327,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,22,bfc443,github.com,Microsoft/BlingFire: A lightning fast Finite State machine and REgular expression manipulation library.,https://www.reddit.com/r/MachineLearning/comments/bfc443/microsoftblingfire_a_lightning_fast_finite_state/,random_cynic,1555766465,,0,1,False,default,,,,,
1328,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,22,bfcbgr,self.MachineLearning,Correlation between features and the effects on a classification model,https://www.reddit.com/r/MachineLearning/comments/bfcbgr/correlation_between_features_and_the_effects_on_a/,gabribrun,1555767826,[removed],0,1,False,self,,,,,
1329,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,22,bfcbpr,self.MachineLearning,[P] Landing a Rocket from Unfavorable Trajectories (Reinforcement Learning),https://www.reddit.com/r/MachineLearning/comments/bfcbpr/p_landing_a_rocket_from_unfavorable_trajectories/,Kuvster98,1555767872,"https://goo.io/63JCrv

Here is my quick demo of landing a rocket, inspired by SpaceX's recent landings. 

The rocket is released from random positions, and uses a small neural network to control its landing. The network was trained with reinforcement learning in a separate program.",54,180,False,self,,,,,
1330,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,23,bfcqfg,medium.com,Deploy Machine Learning Model in Google Cloud using Cloud Run,https://www.reddit.com/r/MachineLearning/comments/bfcqfg/deploy_machine_learning_model_in_google_cloud/,NaxAlpha,1555770422,,0,1,False,https://a.thumbs.redditmedia.com/EYz7_7A4qh8mNUmirB3GXI7j6Klhhftm2r9HVoqUV14.jpg,,,,,
1331,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,23,bfctcn,/r/MachineLearning/comments/bfctcn/p_stylegan_on_the_vgg_flowers_102_dataset/,[P] StyleGAN on the VGG Flowers 102 Dataset ,https://www.reddit.com/r/MachineLearning/comments/bfctcn/p_stylegan_on_the_vgg_flowers_102_dataset/,neurokinetikz,1555770914,,1,1,False,default,,,,,
1332,MachineLearning,t5_2r3gv,2019-4-20,2019,4,20,23,bfcxza,self.MachineLearning,[P] GPT-2xy: A UI for trying out GPT-2 slimmed version,https://www.reddit.com/r/MachineLearning/comments/bfcxza/p_gpt2xy_a_ui_for_trying_out_gpt2_slimmed_version/,NaxAlpha,1555771667,"Hi Guys, 

I have created a user interface for GPT-2.  [https://gpt2.ai-demo.xyz](https://gpt2.ai-demo.xyz/) 

Here is its implementation:  [https://github.com/naxAlpha/gpt-2xy](https://github.com/NaxAlpha/gpt-2xy) 

Here is my tutorial on how to deploy it (or any other machine learning model) yourself on google cloud (serverless):

 [https://medium.com/datadriveninvestor/deploy-machine-learning-model-in-google-cloud-using-cloud-run-6ced8ba52aac](https://medium.com/datadriveninvestor/deploy-machine-learning-model-in-google-cloud-using-cloud-run-6ced8ba52aac) 

Demo Screenshot:

&amp;#x200B;

[Most of the written text is by AI :D](https://i.redd.it/c4vvsc8bmft21.png)",4,2,False,https://b.thumbs.redditmedia.com/GpYWy_NHqhp-Q66W1rNydGt5wrM6xC1v9L0zzdykDWQ.jpg,,,,,
1333,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,0,bfd76j,self.MachineLearning,My training data won't save correctly,https://www.reddit.com/r/MachineLearning/comments/bfd76j/my_training_data_wont_save_correctly/,DriftBoi86,1555773133,[removed],0,1,False,self,,,,,
1334,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,0,bfddmf,self.MachineLearning,Deep Reinforcement Learning AILearning to Paint like humans,https://www.reddit.com/r/MachineLearning/comments/bfddmf/deep_reinforcement_learning_ailearning_to_paint/,hzwer,1555774167,[removed],0,1,False,self,,,,,
1335,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,0,bfdhls,self.MachineLearning,Learning to paint: A Painting AI,https://www.reddit.com/r/MachineLearning/comments/bfdhls/learning_to_paint_a_painting_ai/,hzwer,1555774796,[removed],0,1,False,https://a.thumbs.redditmedia.com/2k-Qog7k6sL9LWv658qp9nqp0WOEd4jTrrqyApOfcC0.jpg,,,,,
1336,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,1,bfdyg1,self.MachineLearning,Simple Deep-Q Learning using Tensorflow 2.0 and OpenAI Gym,https://www.reddit.com/r/MachineLearning/comments/bfdyg1/simple_deepq_learning_using_tensorflow_20_and/,jloganolson,1555777365,[removed],0,1,False,self,,,,,
1337,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,1,bfe7dt,self.MachineLearning,[R] Do we still need models or just more data and compute?,https://www.reddit.com/r/MachineLearning/comments/bfe7dt/r_do_we_still_need_models_or_just_more_data_and/,downtownslim,1555778700,"A response to Rich Sutton's ""The Bitter Lesson"" from Max Welling, Amsterdam, April 20 2019:

[https://staff.fnwi.uva.nl/m.welling/wp-content/uploads/Model-versus-Data-AI-1.pdf](https://staff.fnwi.uva.nl/m.welling/wp-content/uploads/Model-versus-Data-AI-1.pdf)",25,24,False,self,,,,,
1338,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,2,bfejg9,i.redd.it,17 equations that changed the perception of the world,https://www.reddit.com/r/MachineLearning/comments/bfejg9/17_equations_that_changed_the_perception_of_the/,ai-lover,1555780347,,0,1,False,https://b.thumbs.redditmedia.com/2LCK-Mr4hAPgIMl9Z1rLm5rBqe09gQL2Ujy3Fin0yuc.jpg,,,,,
1339,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,2,bfek6a,self.MachineLearning,Max Welling's reply to Rich Sutton's 'The Bitter Lesson',https://www.reddit.com/r/MachineLearning/comments/bfek6a/max_wellings_reply_to_rich_suttons_the_bitter/,vijaydwivedi75,1555780445,[removed],0,1,False,self,,,,,
1340,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,2,bfeo9u,self.MachineLearning,Admission chances in European machine learning master's programmes,https://www.reddit.com/r/MachineLearning/comments/bfeo9u/admission_chances_in_european_machine_learning/,woobashtash,1555781060,"Hi, please correct me if I'm asking in the wrong sub.

I am a French CS master student interested in machine learning, and I'd like to work in AI. I am thinking about applying for a second master's degree in machine learning around Europe, but since my application seems a bit borderline, I don't know in which universities I have a chance of admission.

I did an exchange semester at KTH taking machine learning and computer vision courses and really liked what I worked on. I know that it's considered a top university in Europe, and that their CS masters are competitive. Does that mean that you need an excellent GPA from a famous uni to get in?

I've had good and bad periods in my studies, my first two years of bachelor (French classes prparatoires) didn't go very well (A's in computer science but C's or E's in maths and physics) ; then I got into a relatively low-ranked university and got averages of 16,5/20, 14/20, and I'm on track for another 16 and maybe more in the next semester. My grades in the exchange semester were E, C, B and A. By the time I plan to apply, I will have done two research internships in machine learning.

Could someone who knows universities' admissions processes better than me give me some advice as to where to apply? With a good cover letter and letters of recommendation, do I have a chance at KTH or should I aim for lower-ranked ones?

Thanks :)",0,1,False,self,,,,,
1341,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,2,bfey9j,self.MachineLearning,Variational auto-encoders for text generation,https://www.reddit.com/r/MachineLearning/comments/bfey9j/variational_autoencoders_for_text_generation/,cemna,1555782503,[removed],0,1,False,self,,,,,
1342,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,2,bfezm5,self.MachineLearning,What kind of writing do machine learning scientists and programmers do?,https://www.reddit.com/r/MachineLearning/comments/bfezm5/what_kind_of_writing_do_machine_learning/,jrgallag,1555782703,[removed],0,1,False,self,,,,,
1343,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,2,bff023,github.com,Mathematics for Machine Learning: Notes and solutions,https://www.reddit.com/r/MachineLearning/comments/bff023/mathematics_for_machine_learning_notes_and/,blade2208,1555782771,,0,1,False,https://b.thumbs.redditmedia.com/oqVWdU-mMaUe6trkJG-ddJKT213mPb2oA5_YS6rBusQ.jpg,,,,,
1344,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,3,bffman,self.MachineLearning,Hi there! Folks over at r/DataHoarder strongly encouraged me to ask here - n00b (urgh!) asking for some help.,https://www.reddit.com/r/MachineLearning/comments/bffman/hi_there_folks_over_at_rdatahoarder_strongly/,Uncl8,1555786096,[removed],0,1,False,self,,,,,
1345,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,3,bffou3,self.MachineLearning,"Polarization, Powerful Billionaires and the Eroding Effects of Inequality | Rahaf Harfoush",https://www.reddit.com/r/MachineLearning/comments/bffou3/polarization_powerful_billionaires_and_the/,The_Syndicate_VC,1555786498,[removed],0,1,False,self,,,,,
1346,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,3,bffqsg,okai.brown.edu,OKAI - An Interactive Introduction to Artificial Intelligence (AI),https://www.reddit.com/r/MachineLearning/comments/bffqsg/okai_an_interactive_introduction_to_artificial/,tydlwav,1555786798,,3,26,False,https://b.thumbs.redditmedia.com/UTF66GuJU0yT0YQ2bX8pd0VT63dYd03gSiE3q9f8BAw.jpg,,,,,
1347,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,4,bfg399,self.MachineLearning,Suggestions on Real Time Density Estimation,https://www.reddit.com/r/MachineLearning/comments/bfg399/suggestions_on_real_time_density_estimation/,dingfuus,1555788711,[removed],0,1,False,self,,,,,
1348,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,4,bfg7kl,self.MachineLearning,[D] Suggestions for Real Time Density Estimation,https://www.reddit.com/r/MachineLearning/comments/bfg7kl/d_suggestions_for_real_time_density_estimation/,dingfuus,1555789376,"Hi All --

&amp;#x200B;

I'm working on some scientific research code for a neuroscience experiment where we need to do density estimation in real time or something close to real time. The data is from multiple time series and it's not particularly wide, but it is long (\~7 million time points, maybe more at some point) and there is potential for up to 1000+ time series.

&amp;#x200B;

I was wondering if there were suggestions from the r/MachineLearning community on how to scale this analysis. More specifically, are there software frameworks that would make this easier? Are there alternatives to Gaussian Mixture Models and Kernel Density Estimates (i.e. some sort of neural network density estimation) that would be easier to throw on a GPU because of an existing software package?

&amp;#x200B;

I realize the answer might be no and I'm going to have to write a bunch of custom code, but I thought I might as well check before I go too far down one rabbit hole. Thanks!",13,5,False,self,,,,,
1349,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,5,bfgs01,youtube.com,This video goes over a breast cancer diagnosis model that uses neural networks. Really interesting,https://www.reddit.com/r/MachineLearning/comments/bfgs01/this_video_goes_over_a_breast_cancer_diagnosis/,antaloaalonso,1555792538,,0,1,False,default,,,,,
1350,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,6,bfhaow,self.MachineLearning,What makes a good activation function?,https://www.reddit.com/r/MachineLearning/comments/bfhaow/what_makes_a_good_activation_function/,ZeroMaxinumXZ,1555795563,[removed],0,1,False,self,,,,,
1351,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,6,bfhkl1,self.MachineLearning,Data scientist vs research scientist in industry,https://www.reddit.com/r/MachineLearning/comments/bfhkl1/data_scientist_vs_research_scientist_in_industry/,AritraChow,1555797197,[removed],0,1,False,self,,,,,
1352,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,7,bfhzbx,medium.com,Extracting knowledge from knowledge graphs.,https://www.reddit.com/r/MachineLearning/comments/bfhzbx/extracting_knowledge_from_knowledge_graphs/,szelvenskiy,1555799667,,0,1,False,https://b.thumbs.redditmedia.com/q2KLgz8T1dRSeEAfPDd69nVSfepZ6B6AD4CVuBOR9xI.jpg,,,,,
1353,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,7,bfi208,self.MachineLearning,Scale creation of Knowledge Graphs,https://www.reddit.com/r/MachineLearning/comments/bfi208/scale_creation_of_knowledge_graphs/,mohib13,1555800131,[removed],0,1,False,self,,,,,
1354,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,7,bfi2u2,self.MachineLearning,[D] Machine learning projects in Economics?,https://www.reddit.com/r/MachineLearning/comments/bfi2u2/d_machine_learning_projects_in_economics/,Mjjjokes,1555800284,"I'm wondering if there are any projects in Economics that can make use of machine learning. I am really wondering if any social science can benefit from machine learning, and how. Thanks in advance",38,69,False,self,,,,,
1355,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,7,bfi3gz,self.MachineLearning,[D] Natural Gradient Descent/ Hessian/ Fisher papers,https://www.reddit.com/r/MachineLearning/comments/bfi3gz/d_natural_gradient_descent_hessian_fisher_papers/,WillingCucumber,1555800392,"Hi all,

I am reading about Natural gradient descent. Usually I find it quite helpful if I can get to read quite a bunch of papers concerning the topic, I am figuring out good papers online, but if someone from the community can point me to some good papers, it would be great. 

&amp;#x200B;

Can some one point me to a list of good natural gradient descent papers which would address the following:

1. Starting from the S Amari's paper to the recent papers.
2. Natural gradient descent fisher approximation papers
3. Natural gradient descent papers which highlight benefits in preventing saddle points/ achieving local minima.
4. Hessian matrix approximation papers
5. Other good papers covering techniques like KFAC, hessian matrices, fisher matrices, their approximations, NSGD

&amp;#x200B;

Thanks !!",3,25,False,self,,,,,
1356,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,7,bfi4z1,self.MachineLearning,[D] Classifier for tSNE or UMAP results?,https://www.reddit.com/r/MachineLearning/comments/bfi4z1/d_classifier_for_tsne_or_umap_results/,dinoaide,1555800642,"Recently I worked on a binary classification problem. The input data is a high dimension (&gt;100) series. I tried PCA to lower the input to a much smaller dimension (&lt;10) then applied Gradient Boosting on it and this seems to give good result. However I want to improve the results by replacing the PCA part since the classifier is not necessarily linear.

&amp;#x200B;

I tried both tSNE and UMAP and they can bring out clusters even in 2D. However I don't know what to do next:

1. Should I use clustering algorithms like DBSCAN to do the binary classification? How should I do that? One of the issue is that although I can see a cluster of positives, there are also clusters of mixed positives and negatives that I couldn't label;
2. I tried to put UMAP results to Gradient Boosting and to my surprise, it actually give poorer classification than PCA + Gradient Boosting. One issue I believe is that I only tried tSNE and UMAP at 2 or 3 dimensions because the computation time involved. So is there a way (in tSNE or UMAP) to know the intrinsic dimension of a input dataset, like the explained variance or factor loadings in PCA?

&amp;#x200B;

I tried to read many articles on how to use tSNE/UMAP properly but it seems most of them focused on visualization and clustering.",15,12,False,self,,,,,
1357,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,8,bfib1j,self.MachineLearning,[D] Is RL or Non-RL computer vision more computationally expensive?,https://www.reddit.com/r/MachineLearning/comments/bfib1j/d_is_rl_or_nonrl_computer_vision_more/,doctorjuice,1555801731,"RL appears more expensive and require orders of magnitude more training steps, but computer vision for object localization and classification may require larger more fine-tuned architectures given the problems are a bit more mature.  


Of the two, which would you say is more computationally demanding? Or is it a tie? Or is it the ever common ""it depends"" answer?",2,2,False,self,,,,,
1358,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,8,bfio9s,briannorlander.com,Detecting Russian Bots on Reddit Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bfio9s/detecting_russian_bots_on_reddit_using_machine/,NorMNfan,1555804035,,0,1,False,default,,,,,
1359,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,8,bfipev,self.MachineLearning,what is topic predict time series,https://www.reddit.com/r/MachineLearning/comments/bfipev/what_is_topic_predict_time_series/,GoBacksIn,1555804225,[removed],0,1,False,self,,,,,
1360,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,9,bfiug6,i.redd.it,EM algorithm with 4 dimensional GMM on Iris flower dataset [OC],https://www.reddit.com/r/MachineLearning/comments/bfiug6/em_algorithm_with_4_dimensional_gmm_on_iris/,borislestsov,1555805090,,0,1,False,https://b.thumbs.redditmedia.com/cGHeJ_LRTNn6Y4ftlTpUcE7e1pOQ7K1uAd88RSydHJQ.jpg,,,,,
1361,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,9,bfj5vg,self.MachineLearning,Yearly Reviews for Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/bfj5vg/yearly_reviews_for_deep_learning/,pretysmitty,1555807153,[removed],0,1,False,self,,,,,
1362,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,10,bfjrjw,youtube.com,[P] PyTorch v1 introduction video,https://www.reddit.com/r/MachineLearning/comments/bfjrjw/p_pytorch_v1_introduction_video/,themathstudent,1555811080,,0,1,False,https://b.thumbs.redditmedia.com/fDa3SeADAYLqe-XKBpl04B8PDLhDHm-LIOtEpQDj3yE.jpg,,,,,
1363,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,10,bfjuqa,self.MachineLearning,Why BERT model has caused and uncased versions,https://www.reddit.com/r/MachineLearning/comments/bfjuqa/why_bert_model_has_caused_and_uncased_versions/,hejiansang,1555811650,[removed],0,1,False,self,,,,,
1364,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,11,bfjzxt,krsingh.cs.ucdavis.edu,[R] FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery (CVPR'19 Oral presentation),https://www.reddit.com/r/MachineLearning/comments/bfjzxt/r_finegan_unsupervised_hierarchical/,utkarsh2254,1555812598,,0,1,False,default,,,,,
1365,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,11,bfk3tw,youtube.com,[R] FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery (CVPR'19 Oral presentation),https://www.reddit.com/r/MachineLearning/comments/bfk3tw/r_finegan_unsupervised_hierarchical/,utkarsh2254,1555813314,,1,4,False,https://b.thumbs.redditmedia.com/KBgXt6jvxoILeTNrHbkD7r6P1hV_7Tr0-6D4e-49xEU.jpg,,,,,
1366,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,11,bfk6ba,self.MachineLearning,SVM optimization,https://www.reddit.com/r/MachineLearning/comments/bfk6ba/svm_optimization/,jukito1,1555813779,[removed],0,1,False,self,,,,,
1367,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,12,bfklok,self.MachineLearning,Implementing GAN's,https://www.reddit.com/r/MachineLearning/comments/bfklok/implementing_gans/,mahav2000,1555816754,Recently we all have read about GAN's and how they are the next big thing in deep learning. I would like to implement it. What resources are available for learning about GAN and actually implementing it?,0,1,False,self,,,,,
1368,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,12,bfkny7,self.MachineLearning,[D] What makes us trust a translation?,https://www.reddit.com/r/MachineLearning/comments/bfkny7/d_what_makes_us_trust_a_translation/,TwasWritten,1555817203,"This post is not referring necessarily to any particular machine translation technique; rather, I wanted to see on what criteria we trust any translation human or otherwise. My rationale is that translation is 100% lossless, and to trust models particularly black-box ones such as neural networks, we should first look at what makes a translation trustworthy in general. Obviously, this is deeply embedded in the context of a translation. The criteria for trusting a UN translator will be different than those for more ad-hoc situations. How can evaluate a criteria that we hold humans to such as cultural knowledge?",7,2,False,self,,,,,
1369,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,12,bfkvno,self.MachineLearning,"Before using CV-selected Regression model for Inference, shouldn't model performance be first evaluated on unused test set?",https://www.reddit.com/r/MachineLearning/comments/bfkvno/before_using_cvselected_regression_model_for/,stats_nerd21,1555818767,[removed],0,1,False,self,,,,,
1370,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,16,bfmicc,self.MachineLearning,"[D] Do you know any useful tips, examples, articles etc. for better GPU utilization?",https://www.reddit.com/r/MachineLearning/comments/bfmicc/d_do_you_know_any_useful_tips_examples_articles/,sequence_9,1555832681," It's been 6 months since I started learning deep learning. Finally last week I implemented DQN for atari games. It is in its simplest form with 3 conv layers, 2 dense layers, replay memory and fixed targets. This week I upgraded my gpu from a gtx 950 to rtx2060, and the training speed is only increased like 10-20%. I know it is a simple code for maybe higher gpu utilization, but it is kind of huge for me, and honestly I was expecting it to scale similar with its fp32 calculation capabilities(x3.5-4). Obviously I can't utilize my gpu, and I'd like to learn if there is something I can do to improve my code in the future outside of just increasing batch size.",46,73,False,self,,,,,
1371,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,16,bfmimh,self.MachineLearning,How to Build an End-to-End Conversational AI System using Behavior Trees,https://www.reddit.com/r/MachineLearning/comments/bfmimh/how_to_build_an_endtoend_conversational_ai_system/,liormessinger,1555832753,[removed],0,1,False,self,,,,,
1372,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,16,bfmlav,self.MachineLearning,Masters in AI/Deep/Machine Learning and/or Data Science in Europe/Canada,https://www.reddit.com/r/MachineLearning/comments/bfmlav/masters_in_aideepmachine_learning_andor_data/,buttholebolt,1555833534,[removed],0,1,False,self,,,,,
1373,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,17,bfmoi5,self.MachineLearning,Does a random tree learner discretize data in this scenario?,https://www.reddit.com/r/MachineLearning/comments/bfmoi5/does_a_random_tree_learner_discretize_data_in/,qwerty622,1555834422,[removed],0,0,False,self,,,,,
1374,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,18,bfmyyl,self.MachineLearning,[Q] Is it better to do paper implementations from scratch or using libraries such as tensorflow or pytorch if trying to get a research position?,https://www.reddit.com/r/MachineLearning/comments/bfmyyl/q_is_it_better_to_do_paper_implementations_from/,Carcaso,1555837340,[removed],0,1,False,self,,,,,
1375,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,18,bfn1jq,self.MachineLearning,Do we need PyTorch anymore after Tensorflow 2.0 release?,https://www.reddit.com/r/MachineLearning/comments/bfn1jq/do_we_need_pytorch_anymore_after_tensorflow_20/,zech1989,1555838042,[removed],0,1,False,self,,,,,
1376,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,18,bfn9mw,youtube.com,The Art Of Physics - Demonstration Of Order And Chaos,https://www.reddit.com/r/MachineLearning/comments/bfn9mw/the_art_of_physics_demonstration_of_order_and/,scienceisfun112358,1555840332,,1,1,False,https://b.thumbs.redditmedia.com/JG8fmKpWa6zb-fESv8TKF7uc3I7UDLO5FdwMHWKfzwQ.jpg,,,,,
1377,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,19,bfnhv5,self.MachineLearning,[D] Can attention be computed implicitly by an RNN?,https://www.reddit.com/r/MachineLearning/comments/bfnhv5/d_can_attention_be_computed_implicitly_by_an_rnn/,valentincalomme,1555842600,"I have been working with sequence to sequence models for a while now. And the attention mechanism is a cornerstone to most seq2seq setups. It is typically added as an explicit part of the network, which has its advantages if one might want to modify how attention is computed without messing the encoder or the decoder code.

However, I am wondering if there would be ways to compute attention weights through introspection of the weights of RNN cells. In very coarse terms, an LSTM cell for instance has a forget gate. Therefore, if we can see when what part of the sequence is forgotten, this could give us an indication of what the attention weights might.

Now, if such a mechanism existed, here are my assumptions of the properties it would need to have:
- the RNN would need to be bidirectional
- the RNN cells would need to have a forget gate

I also think that the biggest challenge would be to correlate changes in the RNN state and explicit parts of the sequence being analyzed.

Is there work being done on this? Or are there reasons why it cannot work?",12,4,False,self,,,,,
1378,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,19,bfnjwi,designingbrain.com,"Web Development Company , A Complete Web Solution For Your Business",https://www.reddit.com/r/MachineLearning/comments/bfnjwi/web_development_company_a_complete_web_solution/,ajatin28,1555843157,,0,1,False,default,,,,,
1379,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,20,bfnqqd,youtube.com,[P] This video goes over a breast cancer diagnosis model that uses neural networks.,https://www.reddit.com/r/MachineLearning/comments/bfnqqd/p_this_video_goes_over_a_breast_cancer_diagnosis/,antaloaalonso,1555844956,,0,1,False,https://a.thumbs.redditmedia.com/P1sgIF558RHPgEUwMBTGE1IicN8F4tOhjU-G32pPDv4.jpg,,,,,
1380,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,20,bfny3m,self.MachineLearning,[D] Deep Q learning for continuous action space.,https://www.reddit.com/r/MachineLearning/comments/bfny3m/d_deep_q_learning_for_continuous_action_space/,levviinn,1555846764,"I want to apply DQN in environment where action space is continuous. 
I even don't know is it possible or not, is it good idea or not ? If yes then what should be my function approximator's  (neural net) output dimensions ?
Thank you in advance.",16,8,False,self,,,,,
1381,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,22,bfot7l,self.MachineLearning,Does strict data protection laws in Europe (GDPR) affect a machine learning engineer working in Europe?,https://www.reddit.com/r/MachineLearning/comments/bfot7l/does_strict_data_protection_laws_in_europe_gdpr/,navan7269,1555853415,[removed],0,1,False,self,,,,,
1382,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,22,bfp1wz,self.MachineLearning,[P] MR-based CT Generation (DCCC) Tensorflow Project,https://www.reddit.com/r/MachineLearning/comments/bfp1wz/p_mrbased_ct_generation_dccc_tensorflow_project/,Cheng-BinJin,1555855007,"Github link: [github.com/ChengBinJin/MRI-to-CT-DCNN-TensorFlow](https://github.com/ChengBinJin/MRI-to-CT-DCNN-TensorFlow)

![img](8uaqou55imt21)",12,22,False,self,,,,,
1383,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,23,bfp67l,self.MachineLearning,Implementation of Naive Bayesian and Artificial Neural Network,https://www.reddit.com/r/MachineLearning/comments/bfp67l/implementation_of_naive_bayesian_and_artificial/,textssg,1555855776,[removed],0,1,False,self,,,,,
1384,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,23,bfp9yy,self.MachineLearning,Any useful tips/references on generating synthetic images for object detection in a goven background,https://www.reddit.com/r/MachineLearning/comments/bfp9yy/any_useful_tipsreferences_on_generating_synthetic/,rodri651,1555856438,[removed],0,1,False,self,,,,,
1385,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,23,bfpapf,staff.fnwi.uva.nl,Response to Sutton's The Bitter Lesson by Max Welling,https://www.reddit.com/r/MachineLearning/comments/bfpapf/response_to_suttons_the_bitter_lesson_by_max/,yldedly,1555856565,,0,1,False,default,,,,,
1386,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,23,bfpbg1,medium.com,Machine Learning in the Browser using TensorFlow.js,https://www.reddit.com/r/MachineLearning/comments/bfpbg1/machine_learning_in_the_browser_using_tensorflowjs/,parmarsuraj,1555856697,,0,1,False,default,,,,,
1387,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,23,bfpbna,zeroequalsfalse.press,Best Artificial Intelligence Books in 2019,https://www.reddit.com/r/MachineLearning/comments/bfpbna/best_artificial_intelligence_books_in_2019/,woahdotcom,1555856731,,0,1,False,https://b.thumbs.redditmedia.com/uQwxLCb_bcM5LhRcLvasPbTWB74IF1DMhvebLwL70yI.jpg,,,,,
1388,MachineLearning,t5_2r3gv,2019-4-21,2019,4,21,23,bfpgzv,self.MachineLearning,Fine-tune first layers,https://www.reddit.com/r/MachineLearning/comments/bfpgzv/finetune_first_layers/,Leoniloris,1555857619,[removed],0,1,False,self,,,,,
1389,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfpol2,self.MachineLearning,[D] Multidimensional regression: Should I / how to make sure the error variances are the same along different dimensions,https://www.reddit.com/r/MachineLearning/comments/bfpol2/d_multidimensional_regression_should_i_how_to/,journeymango,1555858858,"[Posted this yesterday, but i think it got removed for some reason]

I have data that has equal variance along each of the target dimensions, but if i analyze the results of training i notice that that my trained model does not have the same error variances along each of the dimensions, which means some dimensions are more erroneous than others.

Does this say anything about me missing some sort of regularization, or is this not something I should be fretting over? Is there a way I can regularize to make sure the errors along different dimensions have the same variance?",14,13,False,self,,,,,
1390,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfpqzo,bot.dialogflow.com,"Hey guys, I am new to machine learning and i created this Chat Bot. Please test it out and rate it(btw i am 12 years old so it might be not that much good!)",https://www.reddit.com/r/MachineLearning/comments/bfpqzo/hey_guys_i_am_new_to_machine_learning_and_i/,__ASN__,1555859247,,0,1,False,default,,,,,
1391,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfpsx7,self.MachineLearning,[Discussion] Building scalable / reproducible ML pipelines,https://www.reddit.com/r/MachineLearning/comments/bfpsx7/discussion_building_scalable_reproducible_ml/,G_Balena,1555859571,"To the more experienced ML professionals in the community - I wanted to hear about what you use to build scalable ML pipelines at your work. I've been building models for a while now for research purposes. However, I'm totally in the dark about the other side of things, namely how to engineer and deploy data/ML pipelines that are scalable and provide reproducible results (whatever that may be in this context). 

I've looked at scikit-learn pipelines, but they seem a bit clunky while handling pandas dataframes (although workarounds do seem to exist). Another sentiment I hear is that they don't scale well to large datasets. 

Care to part with your wisdom? Thanks!",13,21,False,self,,,,,
1392,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfpx1k,self.MachineLearning,"[D] How to implement programming code autocompletion, syntax correction and variable name suggestion",https://www.reddit.com/r/MachineLearning/comments/bfpx1k/d_how_to_implement_programming_code/,grayfallstown,1555860248,"I've read about [microsofts IntelliCode](https://marketplace.visualstudio.com/items?itemName=VisualStudioExptTeam.vscodeintellicode) that aims to guide developers with better code completion and I want to start something similar for the Rust programming language as project both to learn ml and to use it at the end. I know that means a lot of work and the result will probably be in a whole different league than what microsoft has build.

&amp;#x200B;

From what I've read syntax checks can be implemented with autoencoders and LSTM networks, but I don't know how to suggestion for code corrections.

LSTM networks are used for autocompletion and text generation, but I would need to implement a context aware network that gets code from before and behind the cursor. I guess I would need to tokenize the different syntax elements like natural language in NLTK is processed?  


Right now this is mostly guess work for me. I would highly appreciate tipps on the network type and design and information about related work.",4,3,False,self,,,,,
1393,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfpxdf,self.MachineLearning,[Question] Keras Attention layer?,https://www.reddit.com/r/MachineLearning/comments/bfpxdf/question_keras_attention_layer/,Fredbull,1555860311,[removed],0,1,False,self,,,,,
1394,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfq0aa,self.MachineLearning,How long does it take for you to implement a paper?,https://www.reddit.com/r/MachineLearning/comments/bfq0aa/how_long_does_it_take_for_you_to_implement_a_paper/,RedditAcy,1555860796,[removed],0,1,False,self,,,,,
1395,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfq4zz,codeingschool.com,Multiple linear regression: How It Works? (Python Implementation),https://www.reddit.com/r/MachineLearning/comments/bfq4zz/multiple_linear_regression_how_it_works_python/,subhamroy021,1555861566,,0,1,False,https://b.thumbs.redditmedia.com/R24tHOYE6XDBmbQmsDlyD11hCy6Fc1Mg3Olr8CjNUlU.jpg,,,,,
1396,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfq8v9,self.MachineLearning,[D] OpenAI Five vs Humans currently at 410633 (99.2% winrate),https://www.reddit.com/r/MachineLearning/comments/bfq8v9/d_openai_five_vs_humans_currently_at_410633_992/,FirstTimeResearcher,1555862187,"A small group of humans is winning consistently against OpenAI Five. There seem to be a few reproducible strategies that keep beating the bot. Can someone describe what those strategies are for someone that hasn't played DoTA?

Link
https://arena.openai.com/#/results",68,280,False,self,,,,,
1397,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,0,bfq9kd,self.MachineLearning,Research Project: Improving the Resolution for Blurred Images,https://www.reddit.com/r/MachineLearning/comments/bfq9kd/research_project_improving_the_resolution_for/,addyp47,1555862301,"Hey, I'm new to this sub, but I was wondering if anyone could help explain/guide me through a machine learning project I plan on doing. I am not a totally newbie and have some experience, but it looks like you guys know a lot more than me. My project is taking low-res photos (maybe video too) and converting it to higher res. Does anyone know how to do this?",0,1,False,self,,,,,
1398,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,1,bfqebj,self.MachineLearning,[Discussion] What do tools like Algorithmia and FastScore do that running a model on GCP or AWS can't accomplish?,https://www.reddit.com/r/MachineLearning/comments/bfqebj/discussion_what_do_tools_like_algorithmia_and/,AlexSnakeKing,1555863026,"Some recently hired data scientists at work are suggesting that we start using tools like Algorithmia and FastScore for deploying machine learning models. Some of them are talking about ""One click model deployment"". I took a look at the websites for these products, but I'm still having a hard time figuring out what they can do that is so special and why exactly do they make life easier for a data scientists compared to running your model on Google Cloud ML or Sagemaker?",8,14,False,self,,,,,
1399,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,1,bfqh29,self.MachineLearning,Limitations of current ML programs?,https://www.reddit.com/r/MachineLearning/comments/bfqh29/limitations_of_current_ml_programs/,The_MonopolyMan,1555863440,"Hi, 

&amp;#x200B;

I'm pretty new to the whole ML thing, but I've a decent grasp on how it works under the hood. I've been mulling over a few projects I want to try, using TensorFlow in particular, it seems to be the easiest one to use (that I've discovered, anyway).

&amp;#x200B;

I just wanted to inquire here about the limits of something like TF, such as the maximum number of inputs, the complexity of an output, compute time etc.

&amp;#x200B;

When I say complexity of an output, I'm referring to say, having the program spit out a full engineering diagram, blueprints, measurements, 3D models and things along those lines. I'm not expecting it to be easy, but I'd like to get a rough idea of how much work I can have the machine do for me.

&amp;#x200B;

Thanks, and I hope this isn't some totally off the wall, absolute misunderstanding of the technology!",0,1,False,self,,,,,
1400,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,1,bfqi0o,biorxiv.org,[R] Single Cortical Neurons as Deep Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bfqi0o/r_single_cortical_neurons_as_deep_artificial/,frequenttimetraveler,1555863583,,0,1,False,default,,,,,
1401,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,1,bfqq58,self.MachineLearning,[D] Limitations of ML programs?,https://www.reddit.com/r/MachineLearning/comments/bfqq58/d_limitations_of_ml_programs/,The_MonopolyMan,1555864817,"Hi,

I'm pretty new to the whole ML thing, but I've a decent grasp on how it works under the hood. I've been mulling over a few projects I want to try, using TensorFlow in particular, it seems to be the easiest one to use (that I've discovered, anyway).

I just wanted to inquire here about the limits of something like TF, such as the maximum number of inputs, the complexity of an output, compute time etc.

When I say complexity of an output, I'm referring to say, having the program spit out a full engineering diagram, blueprints, measurements, 3D models and things along those lines. I'm not expecting it to be easy, but I'd like to get a rough idea of how much work I can have the machine do for me.

Thanks, and I hope this isn't some totally off the wall, absolute misunderstanding of the technology!",10,1,False,self,,,,,
1402,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,2,bfr0ar,dhruvonmath.com,"[P] A Visual, Gentle Introduction to Kernels and Nullspaces",https://www.reddit.com/r/MachineLearning/comments/bfr0ar/p_a_visual_gentle_introduction_to_kernels_and/,dhruv-partha,1555866348,,0,1,False,default,,,,,
1403,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,2,bfr3oz,self.MachineLearning,Need advice for deploying TF/Keras for production with an old laptop,https://www.reddit.com/r/MachineLearning/comments/bfr3oz/need_advice_for_deploying_tfkeras_for_production/,poldirac,1555866831,[removed],0,1,False,self,,,,,
1404,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,2,bfr3qw,self.MachineLearning,Generated music using LSTM RNNs. Please help with quantitative evaluation!,https://www.reddit.com/r/MachineLearning/comments/bfr3qw/generated_music_using_lstm_rnns_please_help_with/,MaxCb,1555866840,[removed],0,1,False,self,,,,,
1405,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,2,bfr43u,self.MachineLearning,What is the best Text-to-speech API/Library/software ? In terms of pricing(free or not) and fluency/fluidity,https://www.reddit.com/r/MachineLearning/comments/bfr43u/what_is_the_best_texttospeech_apilibrarysoftware/,barnyfive5,1555866896,,0,1,False,self,,,,,
1406,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,4,bfsgqg,mihaileric.com,Fundamentals of Support Vector Machines,https://www.reddit.com/r/MachineLearning/comments/bfsgqg/fundamentals_of_support_vector_machines/,MusingEtMachina,1555874265,,0,1,False,default,,,,,
1407,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,4,bfsq5z,self.MachineLearning,Interaction Sequence Ranking,https://www.reddit.com/r/MachineLearning/comments/bfsq5z/interaction_sequence_ranking/,throwaway_dfghvbjkn,1555875725,[removed],0,1,False,self,,,,,
1408,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,4,bfsvrh,self.MachineLearning,Normalization of volatile 1-D data,https://www.reddit.com/r/MachineLearning/comments/bfsvrh/normalization_of_volatile_1d_data/,collider_in_blue,1555876592,[removed],0,1,False,self,,,,,
1409,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,5,bfsx4z,self.MachineLearning,[D] Machine Learning - WAYR (What Are You Reading) - Week 61,https://www.reddit.com/r/MachineLearning/comments/bfsx4z/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,1555876805,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|
|----|-----|-----|-----|-----|-----|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)||

Most upvoted papers two weeks ago:

/u/spoiltForChoice: [https://lear.inrialpes.fr/pubs/2011/JDS11/jegou\_searching\_with\_quantization.pdf](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)

/u/Moseyic: [VAE](https://arxiv.org/abs/1312.6114)

/u/ToolTechSoftware: https://accu.org/index.php/journals/2639

Besides that, there are no rules, have fun.",34,4,False,self,,,,,
1410,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,5,bft04q,self.MachineLearning,[D] Normalization of highly variable 1-D data,https://www.reddit.com/r/MachineLearning/comments/bft04q/d_normalization_of_highly_variable_1d_data/,collider_in_blue,1555877258,"Hi, I'm working with a large dataset of 1-D data (think time series) with wildly varying values (decidedly non-Normal.) My goal is to train an autoregressive generative model like WaveNet using this data. I'll need to normalize all series to the range [0, 1] so I can then quantize the data to 256 possible values for the softmax output of the WaveNet. I've run into a few problems and haven't found much help on the internet (mostly searching for time series normalization, standardization, etc.)

A quick run through my current process: 

* Divide each series by their median value in a small window where the signal is ~0 
* At this point, the distribution of values in each series is roughly log-normal so I'm taking the log and then standardizing each series individually by subtracting its mean and dividing by its standard deviation 
* If I now normalize the entire dataset based on the max/min (i.e. data &lt;- (data - data.min())/(data.max() - data.min()), most series are squeezed into a range like [0.4, 0.6] due to massive outlying max/min values. I also can't max/min normalize each series independently because then each series is on very different scales.

Is my best option to just cull the outliers? Or am I missing a step somewhere? I've visualized a few of the outliers, and they are valid data. I've also tried median-stacking nearest neighbor series to tame some of the volatility but am not sure where to go from here. Leaving it as is will increase the effective quantization noise in my data since instead of being spread over 256 values, most series only span ~100 values in the discrete space. Any help would be much appreciated!",5,5,False,self,,,,,
1411,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,5,bftcgr,self.MachineLearning,[N] How to i start with machine learning,https://www.reddit.com/r/MachineLearning/comments/bftcgr/n_how_to_i_start_with_machine_learning/,WazWazMan,1555879158,I want to start learning machine learning where do I start?,12,0,False,self,,,,,
1412,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,5,bfte63,youtu.be,"Should the computers run the world? Phd, author Hannah Fry",https://www.reddit.com/r/MachineLearning/comments/bfte63/should_the_computers_run_the_world_phd_author/,hanifbbz,1555879432,,0,1,False,https://b.thumbs.redditmedia.com/hyJPuznTM4ETdpVd2Zh8jSGhzBSbXEiL30lhg9G-qfY.jpg,,,,,
1413,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,5,bftjf5,arxiv.org,[1904.08653] Fooling automated surveillance cameras: adversarial patches to attack person detection,https://www.reddit.com/r/MachineLearning/comments/bftjf5/190408653_fooling_automated_surveillance_cameras/,Bayequentist,1555880261,,6,63,False,default,,,,,
1414,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,6,bftndd,github.com,Visualize Convolutional Neural Networks interactively and in real time !,https://www.reddit.com/r/MachineLearning/comments/bftndd/visualize_convolutional_neural_networks/,cyber-neuron,1555880854,,0,1,False,https://b.thumbs.redditmedia.com/XyKGZ5T9lGfwpWdxqAIniP-hCJYZskXb1ug-udIue6c.jpg,,,,,
1415,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,6,bftpdl,datascience.stackexchange.com,Why not turn momentum update equation into exponentially weighted moving average update equation?,https://www.reddit.com/r/MachineLearning/comments/bftpdl/why_not_turn_momentum_update_equation_into/,CrazyCrab,1555881160,,0,1,False,default,,,,,
1416,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,6,bftq3j,github.com,2.7 will be the last major release for Microsoft's CNTK,https://www.reddit.com/r/MachineLearning/comments/bftq3j/27_will_be_the_last_major_release_for_microsofts/,cthorrez,1555881272,,0,1,False,default,,,,,
1417,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,6,bfttns,self.MachineLearning,[Discussion] Why not turn momentum update equation into exponentially weighted moving average update equation?,https://www.reddit.com/r/MachineLearning/comments/bfttns/discussion_why_not_turn_momentum_update_equation/,CrazyCrab,1555881822,"https://datascience.stackexchange.com/questions/49695/why-not-turn-momentum-update-equation-into-exponentially-weighted-moving-average

In Pytorch, the update equation of SGD with (non-Nesterov) momentum is
$$ m^{(i+1)} = \beta m^{(i)} + \nabla L(w^{(i+1)}),$$
where $\beta$ is the momentum coefficient, $m^{(i)}$ is the momentum at iteration $i$, $L$ is the loss function, $w^{(i)}$ is the value of weights at iteration $i$.

If we are starting with $ m^{(0)} = 0$, then
$$ \forall i &gt; 0 \, m^{(i)} = \sum_{j=0}^{i-1} \beta^j \nabla L(w^{(i-j)}).$$

Now, let's write down the formulas for [exponentially weighted moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average) of gradients (which we'll denote as $a^{(i)}$) to show that one is equivalent to the other multiplicated by a constant. We will make a non-traditional assumption that $a^{(0)} = 0$. It doesn't matter, because as $i$ goes to infinity, the contribution of the zeroth term goes to zero.

$$ a^{(i+1)} = \beta a^{(i)} + (1-\beta) \nabla L(w^{(i+1)})$$
We can rewrite it as
$$ a^{(i)} = (1 - \beta) \sum_{j=0}^{i-1} \beta^j \nabla L(w^{(i-j)}). $$

Notice that $ \forall \beta \in [0, 1) $ it holds that $ (1 - \beta) m^{(i)} = a^{(i)} $.

It seems to me that we should change the update equation of momentum SGD to the equation of exponentially weighted moving average of gradients, i.e. add the $ 1 - \beta $ coefficient to the gradient term. Here's why:

1. It decouples learning rate from momentum coefficient. Currently, larger momentum coefficient increases the *effective* learning rate (i.e. by how much the weights are updated). Suppose we are in an ideal scenario, when for all iterations $i, j$ we have $\nabla L(w^{(i)}) = \nabla L(w^{(j)}) = \nabla L$, then $\lim_{i \to \infty} m^{(i)} = \frac{\nabla L}{1 - \beta}$. For $\beta = 0.9$ this value equals $ 10 \nabla L$, for $\beta = 0.99$ this value equals $ 100 \nabla L$. In contrast, if we use exponentially weighted moving average formula, for all $\beta$ the analagous limit would equal just $\nabla L$. I concede that this is an unrealistic scenario, and in real problems gradients at steps $i, i+1, i+2, \dots, i+k$ somewhat cancel each other out, but still I think it's a good point.
2. Weighted moving average is a somewhat well known concept, while momentum isn't.

I am interested to hear, what reasons are there not to change the update formula? And if you think this is a good change, how should the authors of deep learning libraries proceed?",4,7,False,self,,,,,
1418,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,6,bftue1,self.MachineLearning,Does what I'm looking for exist?,https://www.reddit.com/r/MachineLearning/comments/bftue1/does_what_im_looking_for_exist/,exodvs,1555881932,"This is what I'm looking for recently: a tool that can take control set of photos, we'll call them A1 and B1. The tool uses machine learning to create an algorithm to modify A1 to look like B1. Then when given another control set of photos, A2 and B2, it uses machine learning to create an algorithm to modify A1 and A2 to look like B2 and B2 respectively, and so on. Does any software I'm describing exist yet? If it does, is it available to the general public, and it is compatible with macOS and/or Linux? If it doesn't, how would someone with some computer science experience go about building this sort of tool? I'm looking to restore altered scenes of media that appeared on VHS form but had these scenes removed in DVD or Blu-Ray releases.",0,1,False,self,,,,,
1419,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,6,bfu2ey,self.MachineLearning,Toy problem: How would I go about training something like this?,https://www.reddit.com/r/MachineLearning/comments/bfu2ey/toy_problem_how_would_i_go_about_training/,rumborak,1555883212,[removed],0,1,False,self,,,,,
1420,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,7,bfugba,i.redd.it,[P] The point cloud on the left is a person. The point cloud on the right is a guitar.*,https://www.reddit.com/r/MachineLearning/comments/bfugba/p_the_point_cloud_on_the_left_is_a_person_the/,c0deb0t,1555885423,,1,1,False,https://b.thumbs.redditmedia.com/6Z415V9G7iC5ChYnhRJ2_aAP2AuSnRpMB38Wnps1zNo.jpg,,,,,
1421,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,7,bfujvp,self.MachineLearning,[Question] BERT performing worse than word2vec,https://www.reddit.com/r/MachineLearning/comments/bfujvp/question_bert_performing_worse_than_word2vec/,naboo_random,1555885980,"Hi,

I am trying to use BERT for a document ranking problem. My task is pretty straightforward. I have to do a similarity ranking for an input document. The only issue here is that I dont have labels - so its more of a qualitative analysis.

I am on my way to try a bunch of document representation techniques - word2vec, para2vec and BERT mainly.

For BERT, i came across [this](https://github.com/huggingface/pytorch-pretrained-BERT) library. I fine tuned the bert-small-uncased model, with around 150,000 documents. I ran it for 5 epochs, with a batch size of 16 and max seq length 128. However, if I compare the performance of Bert representation vs word2vec representations, for some reason word2vec is performing better for me right now. For BERT, I used the last four layers for getting the representation.

I am not too sure why the fine tuned model didnt work. I read up [this](https://arxiv.org/pdf/1903.05987.pdf) paper, and [this](https://github.com/huggingface/pytorch-pretrained-BERT/issues/493) other link also that said that BERT performs well when fine tuned for a classification task. However, since I dont have the labels, I fined tuned it as it's done in the paper - in an unsupervised manner.

Also, my documents vary a lot in their length. So Im sending them sentence wise right now. In the end I have to average over the word embeddings anyway to get the sentence embedding. Any ideas on a better method? I also read [here](https://github.com/hanxiao/bert-as-service) \- that there are different ways of pooling over the word embeddings to get a fixed embedding. Wondering if there is a comparison of which pooling technique works better?

Any help on training BERT better or a better pooling method will be greatly appreciated!

Thanks,",0,1,False,self,,,,,
1422,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,7,bfurty,self.MachineLearning,Using Naive Bayes for image classification,https://www.reddit.com/r/MachineLearning/comments/bfurty/using_naive_bayes_for_image_classification/,engineheat,1555887262,[removed],0,1,False,self,,,,,
1423,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,8,bfv2ny,self.MachineLearning,[D] ICML 2019 decisions are out,https://www.reddit.com/r/MachineLearning/comments/bfv2ny/d_icml_2019_decisions_are_out/,ohdangggg,1555889087,Good luck!,31,52,False,self,,,,,
1424,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,8,bfv407,self.MachineLearning,ICML results are out!,https://www.reddit.com/r/MachineLearning/comments/bfv407/icml_results_are_out/,DoronHaviv12,1555889320,[removed],0,1,False,self,,,,,
1425,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,8,bfv86j,meetup.com,[N] Machine Learning in Finance Meetup (London),https://www.reddit.com/r/MachineLearning/comments/bfv86j/n_machine_learning_in_finance_meetup_london/,Jackal008,1555890049,,0,1,False,default,,,,,
1426,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,9,bfvh3k,self.MachineLearning,[D] Can OpenAI use GPT-2 to fill the redacted parts of the Mueller report?,https://www.reddit.com/r/MachineLearning/comments/bfvh3k/d_can_openai_use_gpt2_to_fill_the_redacted_parts/,FriendlyPlum,1555891578,Would this work? Might be a fun thing to try.,7,0,False,self,,,,,
1427,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,9,bfvhet,self.MachineLearning,How should a 15 year old Freshman teach himself Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bfvhet/how_should_a_15_year_old_freshman_teach_himself/,BlameNaix,1555891632,[removed],0,1,False,self,,,,,
1428,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,9,bfvq6u,self.MachineLearning,[D]My ML Journal #11: Macro view of reinforcement learning and more OpenAI gym games,https://www.reddit.com/r/MachineLearning/comments/bfvq6u/dmy_ml_journal_11_macro_view_of_reinforcement/,RedditAcy,1555893123,"Thanks for the support in my last post.

Here's the vlog version of this journal as usual: [https://youtu.be/N7KThxV5maI](https://youtu.be/N7KThxV5maI)

Most of the resources I used and talked about in this post can be found in this [google doc](https://docs.google.com/document/d/1jaYoQBoFq2KIc0u55eKuo8ZKi3cwtLXeeBWCxoHFUg4/edit?usp=sharing), I am not linking many directly because I think my posts were getting auto classified as spam for having too many links

&amp;#x200B;

I spent a few hours researching just what the heck is going on with RL. I debriefed the most starred GitHub projects, the SOTA (state of the art) algorithms, and the open source platforms. Generally speaking, RL should be used whenever a problem can be modeled with an agent, environment, and reward setup. Technically speaking we can train a lot of models with an RL mindset. For example, a traditional GAN has a generator and a discriminator, the generator is trained by how much it **fooled** the discriminator. With an RL mindset, we can define the generator as the agent, its state being the random strokes it painted, and the rewards being the degree it fooled the discriminator. Still the same workflow, but now we introduce more possibilities, we can tune the reward algorithms &amp; RL training mechanisms, etc.

My general plan for learning RL is to implement gym games first, then playing around with complex environments like Project Malmo and ViZDoom, and at last, I will get onto the Unity RL env and making my own game and training my own RL agent to beat it!

So right now, let me implement a few gym games first, for this time, I am trying to beat Cartpole &amp; Acrobot.

*Processing gif sqjzj2cgnpt21...*

&amp;#x200B;

Cartpole, objective is to balance the stick so it doesn't tilt over 15 degrees

Since I copied and understood the atari breakout code, I thought this was a piece of cake! Interestingly, the observations of the environment turned out to be an array with 4 floats while I thought it was going to be a picture (like in Atari). That changes my game plan, I can't just use the Atari code for this, because in Atari, we predict an action by feeding in a 210 \* 180 \* 3 image, which is the state, that image goes through a few Conv layers, connects to a Dense layer with 4 outputs, which represents the actions we can choose (we will choose the output that had the highest value because that represents the one that will yield the highest reward). But a cartpole state is a 4 \* 1 array, so I decided to feed it through a random neural network and connected it to a dense layer with 2 outputs at the end.

But I still ended up using a great portion of gsurma's code. The reason being that the original Atari code wasn't the best, it is intuitive to do this:

    class AtariSolver:     
        def __init__(...):          
            define the model structure     
        def saveMemory(...):          
            self.memory.append the current state, reward, etc so we can train the model with the memory              variable later     
        def getAction(...):           
            ... 

But the Atari code scattered all of these across places, they weren't contained inside of a single class, which would make **a lot of sense.** Here's [my GitHub repository](https://github.com/BlastWind/cartpole) that beat Cartpole, again, huge credits to gsurma.

Now onto my **nemesis: the Acrobot.**

*Processing gif 1z6fixzenpt21...*

The observation of an acrobot is an array with 6 floats, so I thought I could've beat it with the same approach with Cartpole and use the Deep Q-Learning algorithm to train the RL agent.

I was wrong! I am still trying to motivate this skinny blue dude to cross the black line (the objective of the game)! The agent gets rewarded with negative points each frame the RL agent cannot reach the black line. **The problem is, a lot of negative points won't help on making the best choice if the agent never experienced a positive reward.** **And since the default Deep Q-Learning algorithm decreases the exploration rate over time, the agent will try random stuff less and less.** That is at least what I am feeling. I ran this model for 100 iterations and all of it terminated because it reached the maximum 500 timeframes.

It's okay, the next time I will be reporting back to you, I will have defeated this skinny blue dude. I looked into something that might help me, on the [openAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard), someone beat acrobot with an algorithm called PPO (proximal policy optimization). It seems really hard to understand mathematically, but I will understand it, beat acrobot, and share it with you next time!",2,1,False,https://b.thumbs.redditmedia.com/s9-x2ZTqr_3JH2mFkEg7aEO1eJH1CNUhkerD8JRuisQ.jpg,,,,,
1429,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,10,bfw2q5,self.MachineLearning,Has anyone succeeded to construct identity mapping with autoencoder with MNIST?,https://www.reddit.com/r/MachineLearning/comments/bfw2q5/has_anyone_succeeded_to_construct_identity/,zacharyoon,1555895254,[removed],0,1,False,self,,,,,
1430,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,10,bfwfz3,self.MachineLearning,[D] ICML 2019 decisions are out,https://www.reddit.com/r/MachineLearning/comments/bfwfz3/d_icml_2019_decisions_are_out/,thirddanceofeternity,1555897582,Available at Microsoft CMT,0,0,False,self,,,,,
1431,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,11,bfwppe,shenchong.com,How to Improve Using Life of Plate Roller Working Roll?,https://www.reddit.com/r/MachineLearning/comments/bfwppe/how_to_improve_using_life_of_plate_roller_working/,CNCPressBrakeMachine,1555899343,,0,1,False,default,,,,,
1432,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,12,bfxbgr,self.MachineLearning,Do phd programs accept online masters?,https://www.reddit.com/r/MachineLearning/comments/bfxbgr/do_phd_programs_accept_online_masters/,MachineInTheStone,1555903049,[removed],0,1,False,self,,,,,
1433,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,12,bfxjz2,self.MachineLearning,How to start a project about inventory management using machine learning?,https://www.reddit.com/r/MachineLearning/comments/bfxjz2/how_to_start_a_project_about_inventory_management/,drmayorga,1555904628,[removed],0,1,False,self,,,,,
1434,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,13,bfxpoh,self.MachineLearning,Cost of Nearest Neighbor Search -- Exact Matching (Jegou et al),https://www.reddit.com/r/MachineLearning/comments/bfxpoh/cost_of_nearest_neighbor_search_exact_matching/,spoiltForChoice,1555905713,"I was reviewing the slides from a CVPR 2014 workshop [presentation](http://people.rennes.inria.fr/Herve.Jegou/courses/2012_cpvr_tutorial/5-efficient-matching.ppt.pdf) and I am trying to wrap my head around the question in Slide 4 ""How much time?"". The answer is revealed in the next slide to \*5.5 seconds\*, but I dont understand how this is computed.

&amp;#x200B;

When I tried to do \`np.linalg.norm(x-y)\`, that costed around 5 microseconds per pair, and that would lead to 500 seconds for the overall 1 billion reference vectors. Finally, there are 1000 such distinct queries -- this would lead to 500 \* 1000 = 50,000 seconds (which is more than half a day).  Granted that a CPU with 8 cores can speed things up, but I am not sure why I am several orders of magnitude of.

&amp;#x200B;

&amp;#x200B;

*Processing img 805lynb4oqt21...*",1,1,False,self,,,,,
1435,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,13,bfy52j,self.MachineLearning,"Hi, Do you know the good article that introduces artificial intelligence to you in a simple and understandable language for everyone.",https://www.reddit.com/r/MachineLearning/comments/bfy52j/hi_do_you_know_the_good_article_that_introduces/,Doctor_who1,1555908886,"Hi, Do you know the good article that introduces artificial intelligence to you in a simple and understandable language for everyone.",0,1,False,self,,,,,
1436,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,14,bfye6q,lhd.co.com,Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/bfye6q/telescopic_forks/,lhd121,1555910910,,0,1,False,default,,,,,
1437,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,15,bfyzd3,self.MachineLearning,[Project] Frequentist A/B Testing From Scratch - Interactive Notebooks,https://www.reddit.com/r/MachineLearning/comments/bfyzd3/project_frequentist_ab_testing_from_scratch/,cstorm3000,1555915858,[removed],0,1,False,self,,,,,
1438,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,16,bfzewv,savannah.gnu.org,GNU Parallel invites to parallel parties celebrating 10 years as GNU (with 1 years notice),https://www.reddit.com/r/MachineLearning/comments/bfzewv/gnu_parallel_invites_to_parallel_parties/,OleTange,1555919618,,0,1,False,default,,,,,
1439,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,17,bfzkno,paperswithcode.com,Community For Researchers. Helpful much.,https://www.reddit.com/r/MachineLearning/comments/bfzkno/community_for_researchers_helpful_much/,cw_var,1555921048,,0,1,False,default,,,,,
1440,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,17,bfzp8l,kyso.io,Predicting Salaries from Job Listings on Indeed.com,https://www.reddit.com/r/MachineLearning/comments/bfzp8l/predicting_salaries_from_job_listings_on_indeedcom/,CompetitiveHandle,1555922242,,1,1,False,default,,,,,
1441,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,17,bfzs0p,kyso.io,[P] Predicting Salaries from Job Listings on Indeed.com,https://www.reddit.com/r/MachineLearning/comments/bfzs0p/p_predicting_salaries_from_job_listings_on/,CompetitiveHandle,1555922952,,1,1,False,default,,,,,
1442,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,19,bg0c9z,self.MachineLearning,[D] Unifying VAEs and GANs,https://www.reddit.com/r/MachineLearning/comments/bg0c9z/d_unifying_vaes_and_gans/,Maplernothaxor,1555927705,"Hey, Im looking for some papers that unify the theory behind VAEs and GANs. 

So far Ive read the Adversarial Variational Bayes paper from 2017 and was wondering if any further work had been done since then in the area.

Thanks",37,111,False,self,,,,,
1443,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,19,bg0dn3,self.MachineLearning,Watch how Rotimatic was invented,https://www.reddit.com/r/MachineLearning/comments/bg0dn3/watch_how_rotimatic_was_invented/,shikparm,1555928006,[removed],0,1,False,self,,,,,
1444,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,19,bg0fg5,self.MachineLearning,What would you recommend for learning Stan machine learning to start with?,https://www.reddit.com/r/MachineLearning/comments/bg0fg5/what_would_you_recommend_for_learning_stan/,hkoohy,1555928386,[removed],0,1,False,self,,,,,
1445,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,19,bg0h7y,towardsdatascience.com,Small questions about bigger issues in Machine learning,https://www.reddit.com/r/MachineLearning/comments/bg0h7y/small_questions_about_bigger_issues_in_machine/,hiren_p,1555928799,,0,1,False,default,,,,,
1446,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,19,bg0jyv,self.MachineLearning,[D] Confused with axis and space,https://www.reddit.com/r/MachineLearning/comments/bg0jyv/d_confused_with_axis_and_space/,Unlistedd,1555929418," If i apply the dirichlet process, or any other clustering method for that matter, am able to create components/clusters out of my data, these are plotted on a 2d plane. What are the axis that define this space? What are the plots and clusters defined as? And why is it safe to assume that the distance between each value can determind whether or not the data shares a similar structure / topic?",0,4,False,self,,,,,
1447,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,19,bg0kff,/r/MachineLearning/comments/bg0kff/radial_engine_assembly_animation_recently/,Radial Engine Assembly Animation recently published - shows how to build these machines that power lightweight aircraft,https://www.reddit.com/r/MachineLearning/comments/bg0kff/radial_engine_assembly_animation_recently/,DCabral15,1555929531,,0,1,False,default,,,,,
1448,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,19,bg0ns2,self.MachineLearning,Is this book worth the read?,https://www.reddit.com/r/MachineLearning/comments/bg0ns2/is_this_book_worth_the_read/,titian101,1555930306,"Hi,

I am interested in using machine learning in trading stocks. Have a medium understanding of ML, Python and Math. I am currently reading this book: ""Hands on Machine Learning for algorithmic Trading: Design and implement investment strategies on smart algorithms that learn from data using data using Python ""

&amp;#x200B;

Is it practical? 

Any other suggestions? I am looking for a book with a mix of practical/theory and am looking to develop my own strategies.",0,1,False,self,,,,,
1449,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,20,bg0zmf,burckhardt.com,perforating machine,https://www.reddit.com/r/MachineLearning/comments/bg0zmf/perforating_machine/,habmkloganjt,1555932663,,0,1,False,default,,,,,
1450,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,20,bg10n7,self.MachineLearning,TensorFlow Cheat Sheet (v2.0),https://www.reddit.com/r/MachineLearning/comments/bg10n7/tensorflow_cheat_sheet_v20/,__Tia__,1555932845,[removed],0,1,False,self,,,,,
1451,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,21,bg1n1c,briannorlander.com,Detecting Russian Bots on Reddit Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bg1n1c/detecting_russian_bots_on_reddit_using_machine/,NorMNfan,1555936797,,0,1,False,https://b.thumbs.redditmedia.com/vGptn_tHt9KWuOatv2V1-U655ytHYmpwXmfCoKph3HI.jpg,,,,,
1452,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,21,bg1spz,self.MachineLearning,Match job description with resume,https://www.reddit.com/r/MachineLearning/comments/bg1spz/match_job_description_with_resume/,nchaw1,1555937795,[removed],0,1,False,self,,,,,
1453,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,22,bg20ti,self.MachineLearning,Avoid the overflow of log loss,https://www.reddit.com/r/MachineLearning/comments/bg20ti/avoid_the_overflow_of_log_loss/,yrajsm,1555939174,[removed],0,1,False,self,,,,,
1454,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,22,bg20zw,self.MachineLearning,No development with a crowdsourced GPT2 1.5B model?,https://www.reddit.com/r/MachineLearning/comments/bg20zw/no_development_with_a_crowdsourced_gpt2_15b_model/,ptrenko,1555939207,[removed],0,1,False,self,,,,,
1455,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,22,bg2ezo,youtu.be,Machine Learning Tutorial | Algorithm CheatSheet - Python Machine Learning For Beginners,https://www.reddit.com/r/MachineLearning/comments/bg2ezo/machine_learning_tutorial_algorithm_cheatsheet/,EddyTheDad,1555941557,,0,1,False,default,,,,,
1456,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,22,bg2f12,sqlcoffee.com,Predicting the Price of Used Cars using Azure SQL Machine Learning Services. Making predictions using R and using Transact-SQL.,https://www.reddit.com/r/MachineLearning/comments/bg2f12/predicting_the_price_of_used_cars_using_azure_sql/,albertomorillo,1555941564,,0,1,False,https://b.thumbs.redditmedia.com/sy0e-4MmUFvJ6wJo-vXIWfyuQRH-MGjotnhAZaY5WDE.jpg,,,,,
1457,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,23,bg2hvu,self.MachineLearning,What do ML practitioners see in the future? Will your job be automated soon?,https://www.reddit.com/r/MachineLearning/comments/bg2hvu/what_do_ml_practitioners_see_in_the_future_will/,Don_E,1555941974,"I recently came across **neural architecture search(NAS).** It is amongst the cutting edge of ML today. It a way in which neural network architecture can be generated without the human effort. You parse in you data and come back in a day or two. The NAS finds an optimal architecture for your data. It uses Re-enforcement learning &amp; RNN to orchestrate its task. It got me wondering if ML/DS would still be a career field 5-10 years down the line. Google already has some implementation of this with [AutoML](https://cloud.google.com/automl/).

&amp;#x200B;

It seems like it would diminish the need for core DS/ML practitioners and make room for the ""Citizen Data Scientist"" with more domain expertise.

&amp;#x200B;

What are your thoughts here?",0,1,False,self,,,,,
1458,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,23,bg2kua,sqlcoffee.com,"Let's briefly compare 3 Microsoft Machine Learning products: Machine Learning Services, Azure SQL Database Machine Learning Services, and Azure Machine Learning Services.",https://www.reddit.com/r/MachineLearning/comments/bg2kua/lets_briefly_compare_3_microsoft_machine_learning/,albertomorillo,1555942410,,0,1,False,default,,,,,
1459,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,23,bg2rq9,self.MachineLearning,Why do NLP folks prefer VAEs over GANs?,https://www.reddit.com/r/MachineLearning/comments/bg2rq9/why_do_nlp_folks_prefer_vaes_over_gans/,intvar,1555943465,[removed],0,1,False,self,,,,,
1460,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,23,bg2v1k,zeroequalsfalse.press,Start Your Machine Learning Journey Here,https://www.reddit.com/r/MachineLearning/comments/bg2v1k/start_your_machine_learning_journey_here/,woahdotcom,1555943984,,0,1,False,default,,,,,
1461,MachineLearning,t5_2r3gv,2019-4-22,2019,4,22,23,bg30zy,codeingschool.com,LEARN HOW TO PROGRAM THE BEST FIT SLOPE IN REGRESSION,https://www.reddit.com/r/MachineLearning/comments/bg30zy/learn_how_to_program_the_best_fit_slope_in/,subhamroy021,1555944879,,0,1,False,https://b.thumbs.redditmedia.com/foUe9h8QiNVmyBzoOI7HooJsD_fSFAbPkDBCZ4yLZsI.jpg,,,,,
1462,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,0,bg37zq,medium.com,Everyone Is an Artist: GauGAN Turns Doodles Into Photorealistic Landscapes,https://www.reddit.com/r/MachineLearning/comments/bg37zq/everyone_is_an_artist_gaugan_turns_doodles_into/,Yuqing7,1555945918,,0,1,False,default,,,,,
1463,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,0,bg3mh4,self.MachineLearning,Characterizing the Variability in Face Recognition Accuracy Relative to Race,https://www.reddit.com/r/MachineLearning/comments/bg3mh4/characterizing_the_variability_in_face/,Better_Protection,1555948019,[removed],0,1,False,self,,,,,
1464,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,0,bg3njr,self.MachineLearning,[P] I2S OCR - Image 2 Speech App,https://www.reddit.com/r/MachineLearning/comments/bg3njr/p_i2s_ocr_image_2_speech_app/,histoire_guy,1555948178,"Hey folks,

We are pleased to introduce the I2S OCR scanner app. I2S is a state-of-the-art OCR Scanner that practically turns almost any images with human readable characters into text content which is in turn transformed into human voice in your native language &amp; accent.

Once the image data (Book page, magazine, journal, scientific paper, etc.) recognized &amp; transformed into text content, you'll be able to playback that text in your local accent &amp; over 45 languages of your choice!

Text output not understood? no problem, use the built-in translation service &amp; get your text translated to over 75 languages of your choice. Generate PDF on the fly, Copy to device clipboard &amp;, share your text output with friends.

**Feature Set Includes**:

* State of the art OCR processing algorithm powered by [PixLab](https://pixlab.io).
* Ability to recognize the input language automatically.
* Speaks over 45 languages &amp; their accents.
* Translate output to over 70 languages of your choice.
* Generate PDF, Share your output &amp; Give your feedback.

**Links**:

* Project homepage: https://i2s.symisc.net.
* Playstore Link: https://play.google.com/store/apps/details?id=net.symisc.ocr.img.speech
* Appstore app to be released soon.

We hope you enjoy using I2S and we look for your feedback if any!",1,16,False,self,,,,,
1465,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,1,bg3tkn,self.MachineLearning,Does Neural Architecture Search do away with the need for ML experts?,https://www.reddit.com/r/MachineLearning/comments/bg3tkn/does_neural_architecture_search_do_away_with_the/,Don_E,1555949012,"I recently came across **neural architecture search(NAS).** It is amongst the cutting edge of ML today. It a way in which neural network architecture can be generated without the human effort. You parse in you data and come back in a day or two. The NAS finds an optimal architecture for your data. It uses Re-enforcement learning &amp; RNN to orchestrate its task. It got me wondering if ML/DS would still be a career field 5-10 years down the line. Google already has some implementation of this with [AutoML](https://cloud.google.com/automl/).

&amp;#x200B;

It seems like it would diminish the need for core DS/ML practitioners and make room for the ""Citizen Data Scientist"" with more domain expertise.

&amp;#x200B;

What are your thoughts here?",0,1,False,self,,,,,
1466,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,1,bg3tqr,self.MachineLearning,Choosing the right GPU and ditching the cloud - speed v size,https://www.reddit.com/r/MachineLearning/comments/bg3tqr/choosing_the_right_gpu_and_ditching_the_cloud/,po-handz,1555949032,"I built a data science workstation in November with a RTX 2070, but am starting to doubt I made the best GPU decision. I went to fine-tune a BERT model this past weekend and found that even the BERT-base model needed around ~12gb of vram. That means that the even with a 2080TI, at 11gb of vram I still might run into out of memory issues. Are these BERT models special cases? What's the typical vram requirement for using imagenet or the like for transfer learning to medical images? If you have 2x 2080's with a NVLINK, does that count as 16gb vram in terms of fitting a model into the gpu, or just 8?

I've run into other problems with my RTX 2070 as well, particularly that it requires CUDA 10, and many algorithms require 9 or 9.2 (or is 10 actually back compatible with these?) and also that I loose another 1.5gb vram just running my desktop environment. How would the following GPUs stack up? 

NVIDIA Tesla M40 GPU 24GB GDDR5 ($600 ebay): single precision GFLOPS: 5,8256,844

NVIDIA RTX 2080 TI GPU 11GB GDDR6 ($1100 microcenter): single precision GFLOPS: 11,750.40 13,447.68, fp16: 94,003.20 107,581.44 

The memory clock rate is about twice as fast on the RTX cards and about 1.5x the bandwidth. And that fp16 speed up is HUGE, over 10x! But how many algorithms can actually take advantage of that these days? From my perspective, I don't mind training taking 72 hours on the M40 v 12 hours on a 2080ti, especially if I can't even fit my models into the 11gb!

At the moment I feel like the best set up is actually 3x graphics cards: older M40 with 24gb vram for huge models, the RTX 2070 for models under 8gb and the fp16 benefits and a 1050 ti to run my desktop environment and ensure that the 2070 can actually use it's full 8gb. Total price for all 3 would be around $1100, or about the same cost as 2080ti. Thoughts? Experiences?",0,1,False,self,,,,,
1467,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,1,bg44q6,self.MachineLearning,[P] Using Transformer models to generate Hacker News comments from titles,https://www.reddit.com/r/MachineLearning/comments/bg44q6/p_using_transformer_models_to_generate_hacker/,DoeL,1555950620,"GitHub: [https://github.com/leod/hncynic](https://github.com/leod/hncynic)

Demo: [https://hncynic.leod.org/](https://hncynic.leod.org/)",28,159,False,self,,,,,
1468,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,1,bg4697,self.MachineLearning,How should I approach my BSc thesis on Sentiment Analysis with Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/bg4697/how_should_i_approach_my_bsc_thesis_on_sentiment/,the_parallax_II,1555950847,[removed],0,1,False,self,,,,,
1469,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,1,bg46mn,self.MachineLearning,What neural network types for binary and multi-class classification?,https://www.reddit.com/r/MachineLearning/comments/bg46mn/what_neural_network_types_for_binary_and/,LostGoatOnHill,1555950900,[removed],0,1,False,self,,,,,
1470,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,1,bg4cgv,self.MachineLearning,[D] Will AWS/GCP or any cloud platform ever be cheaper than owning your own GPUs in the long run?,https://www.reddit.com/r/MachineLearning/comments/bg4cgv/d_will_awsgcp_or_any_cloud_platform_ever_be/,WrongMidnight,1555951725,Every way I look at spec'ing out compute infrastructure seems to point to owning your own equipment when it comes to cost for training models. How is this possible? Why aren't AWS and Google competitive at scale? Surely they have much cheaper power and hardware costs.,14,4,False,self,,,,,
1471,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,2,bg4ut9,github.com,pytorch/hub by pytorch,https://www.reddit.com/r/MachineLearning/comments/bg4ut9/pytorchhub_by_pytorch/,sjoerdapp,1555954266,,0,1,False,default,,,,,
1472,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,2,bg51iy,ai.googleblog.com,SpecAugment: A New Data Augmentation Method for Automatic Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/bg51iy/specaugment_a_new_data_augmentation_method_for/,sjoerdapp,1555955197,,0,1,False,https://a.thumbs.redditmedia.com/zTovHC5xELQJyWqmXwIMNO6EdeKWNqFV0t12n7GIFP4.jpg,,,,,
1473,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,3,bg5987,self.MachineLearning,How do you define model complexity?,https://www.reddit.com/r/MachineLearning/comments/bg5987/how_do_you_define_model_complexity/,jmbrown777,1555956281,"Let's say there is an existing model/algorithm that gets good results, but is complex in the sense that it's hard to understand.

Let's say you create a new model that has the exact same results and runs just as fast, but it works entirely differently. The main benefit to your new method is that it's far less complex and easier to understand. It's something you could teach in a single class, whereas the old method might take a week or two of classes for a professor to fully explain.

How do you define or quantify this complexity when writing a paper?",0,1,False,self,,,,,
1474,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,4,bg5z4r,self.MachineLearning,Tracking Machine Learning Metadata with Sacred Library,https://www.reddit.com/r/MachineLearning/comments/bg5z4r/tracking_machine_learning_metadata_with_sacred/,nodechef,1555959919,[removed],0,1,False,self,,,,,
1475,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,4,bg66zd,self.MachineLearning,Overfitting a Neural Network,https://www.reddit.com/r/MachineLearning/comments/bg66zd/overfitting_a_neural_network/,Geeks_sid,1555961028,"Hi all,  
I am trying to overfit a neural network on some 3D data for a classification problem and I want to check if it has learning abilities or not. I am unable to overfit it on train data size of 4. What measures should I take in order to make sure that I am able to overfit the model?

[This is the description of the model.](https://i.redd.it/gqk2gqmt8vt21.png)",0,1,False,https://a.thumbs.redditmedia.com/LG7E7NdW0IDBeAOBpv3MlWsJjdMEdMTtsc8WW-CXGK0.jpg,,,,,
1476,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,5,bg6mgu,arxiv.org,[R] [1904.08779] SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition from Google Brain,https://www.reddit.com/r/MachineLearning/comments/bg6mgu/r_190408779_specaugment_a_simple_data/,bobchennan,1555963220,,10,36,False,default,,,,,
1477,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,5,bg6yr4,self.MachineLearning,[Project] Computer Vision with ONNX Models,https://www.reddit.com/r/MachineLearning/comments/bg6yr4/project_computer_vision_with_onnx_models/,Chromobacterium,1555964955,"Hey everyone! I just created a new runtime for Open Neural Network Exchange (ONNX) models called ONNXCV.

Basically, you can inference ONNX models for realtime computer vision applications (i.e. image classification and object detection) without having to write boilerplate code. It is useful in the sense that one can focus on making deep learning models using any deep learning library (that converts models into the ONNX file format), without having to sacrifice time into building the actual inferencing program.

Let me know what you think and if I should continue to build upon this (or not).

Here it the [code](https://github.com/ChromoBacterium/OnnxCV.git).",5,4,False,self,,,,,
1478,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,6,bg7d0u,self.MachineLearning,Good toy Active Learning datasets?,https://www.reddit.com/r/MachineLearning/comments/bg7d0u/good_toy_active_learning_datasets/,grappling_hook,1555966964,I'm doing a project with active learning and need a good dataset to do a sanity check for my model. That would mean it would have a high variance in the accuracy depending on which samples are chosen for labeling. It should also be relatively low-dimensional data. Any suggestions? Thanks in advance!,0,1,False,self,,,,,
1479,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,6,bg7f55,self.MachineLearning,"Training parametric Gaussian model (mu, std) and setting standard deviation to 0 during prediction.",https://www.reddit.com/r/MachineLearning/comments/bg7f55/training_parametric_gaussian_model_mu_std_and/,matineh_sh,1555967267,[removed],0,1,False,self,,,,,
1480,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,8,bg8pva,self.MachineLearning,[Project]Deploy trained model to AWS lambda with Serverless framework,https://www.reddit.com/r/MachineLearning/comments/bg8pva/projectdeploy_trained_model_to_aws_lambda_with/,yubozhao,1555974104,"Hi guys,

We have continue updating our open source project for packaging and deploying ML models to product (github.com/bentoml/bentoml), and we have create an easy way to deploy ML model as a serverless (www.serverless.com) project that you could easily deploy to AWS lambda and Google Cloud Function. We want to share with you guys about it and hear your feedback. 

&amp;nbsp;

A little background of BentoML for those aren't familiar with it.
BentoML is a python library for packaging and deploying machine learning models. It provides high-level APIs for defining a ML service and packaging its artifacts, source code, dependencies, and configurations into a production-system-friendly format that is ready for deployment.

**Feature highlights:**
* Multiple Distribution Format - Easily package your Machine Learning models into format that works best with your inference scenario:
   - Docker Image - deploy as containers running REST API Server
   - PyPI Package - integrate into your python applications seamlessly
   - CLI tool - put your model into Airflow DAG or CI/CD pipeline
   - Spark UDF - run batch serving on large dataset with Spark
   - Serverless Function - host your model with serverless cloud platforms

* Multiple Framework Support - BentoML supports a wide range of ML frameworks out-of-the-box including Tensorflow, PyTorch, Scikit-Learn, xgboost and can be easily extended to work with new or custom frameworks.

* Deploy Anywhere - BentoML bundled ML service can be easily deploy with platforms such as Docker, Kubernetes, Serverless, Airflow and Clipper, on cloud platforms including AWS Lambda/ECS/SageMaker, Gogole Cloud Functions, and Azure ML.

* Custom Runtime Backend - Easily integrate your python preprocessing code with high-performance deep learning model runtime backend (such as tensorflow-serving) to deploy low-latancy serving endpoint.

&amp;nbsp;

### How to package machine learning model as serverless project with BentoML
It's surprising easy, just with a single CLI command.  After you finished training your model and saved it to file system with BentoML.  All you need to do now is run `bentoml build-serverless-archive` command, for example:

       $bentoml build-serverless-archive /path_to_bentoml_archive /path_to_generated_serverless_project  --platform=[aws-python, aws-python3, google-python]

This will generate a serverless project at the specified directory. Let's take a look of what files are generated.
        /path_to_generated_serverless_project
        -  serverless.yml
        -  requirements.txt
        -  copy_of_bentoml_archive/
        -  handler.py/main.py (if platform is google-python, it will generate main.py)

`serverless.yml` is the configuration file for serverless framework. It contains configuration to the cloud provider you are deploying to, and map out what events will trigger what function.  BentoML automatically modifies this file to add your model prediction as a function event and update other info for you.

`requirements.txt` is a copy from your model archive, it includes all of the dependencies to run your model

`handler.py/main.py` is the file that contains your function code. BentoML fill this file's function with your model archive class, you can make prediction with this file right away without any modifications.

`copy_of_bentoml_archive`: A copy your model archive.  It will be bundle with other files for serverless deployment.

&amp;nbsp;

###What's next

After you generate this serverless project. If you have the default configuration for AWS or google.  You can deploy it right away.  Otherwise, you can update the serverless.yaml based on your own configurations.

Love to hear feedback from you guys on this.  


Cheers

Bo",18,6,False,self,,,,,
1481,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,8,bg8rnj,self.MachineLearning,[D] GAN Immediate Mode Collapse,https://www.reddit.com/r/MachineLearning/comments/bg8rnj/d_gan_immediate_mode_collapse/,Cranial_Vault,1555974358,"I'm not even sure if mode collapse is the correct term; neither the generator nor discriminator is learning anything when I pass both true/false samples to the discriminator.  If instead I only show the discriminator true or false samples, the loss drops.  I've seen mode collapse after a  few epochs of training other GANs but never complete stagnation out of the gate.  What might be going wrong here?


```
import numpy as np
import pandas as pd
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import Activation, BatchNormalization
from keras.layers.advanced_activations import LeakyReLU
from keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.models import Sequential, Model
from keras.utils import to_categorical
from keras.optimizers import SGD
import keras.backend as K
import datetime
import talib


def generator():
    neurons = 121
    model = Sequential()
    # Input shape [batch_size,timestep,input_dim]
    model.add(LSTM(neurons,activation='tanh',recurrent_activation='hard_sigmoid',kernel_initializer='RandomUniform',return_sequences=True))
    model.add(LSTM(neurons,activation='tanh',recurrent_activation='hard_sigmoid',kernel_initializer='RandomUniform',return_sequences=True))
    #model.add(Dense(512))
    #model.add(Dense(256))
    model.add(Dense(1,activation=None))
    return model

def discriminator():
    model = Sequential()
    # Input shape [batch_size,steps,channels]
    model.add(Conv1D(32,4,strides=2,activation=None,padding='same',input_shape=(None,1)))
    model.add(LeakyReLU())
    model.add(Conv1D(64,4,strides=2,activation=None,padding='same'))
    model.add(LeakyReLU())
    model.add(BatchNormalization())
    model.add(Conv1D(128,4,strides=2,activation=None,padding='same'))
    model.add(LeakyReLU())
    model.add(BatchNormalization())
    model.add(Dense(128,activation='relu'))
    model.add(Dense(1,activation='sigmoid'))
    return model

def generator_containing_discriminator(g, d):
    model = Sequential()
    model.add(g)
    d.trainable = False
    model.add(d)
    return model

def g_loss_function(y_true,y_pred):
    l_bce = keras.losses.binary_crossentropy(y_tue,y_pred)
    l_norm = K.sqrt(K.square(y_true)-K.square(y_pred))
    l_dpl = K.abs(K.sign(y_pred[-1:]-y_true[-2:])-K.sign(y_true[-1:]-y_true[-2:])) # abs(sgn(y_pred[t+1]-y_true[t])-sgn(y_true[t+1]-t_true[t]))
    return l_bce+l_norm+l_dpl

def train(X,Y,BATCH_SIZE):
    
    d_optim = SGD(lr=0.002)
    g_optim = SGD(lr=0.00004)

    g = generator()
    d = discriminator()
    gan = generator_containing_discriminator(g, d)
    
    g.compile(loss=g_loss_function, optimizer=g_optim)   
    gan.compile(loss='binary_crossentropy',optimizer=""SGD"")
    d.trainable = True
    d.compile(loss='binary_crossentropy', optimizer=d_optim)
    num_batches = int(X.shape[0]/float(BATCH_SIZE))

    for epoch in range(1000):
        
        for index in range(1,num_batches):

            # Prepare data
            startIdx = (index-1)*BATCH_SIZE
            endIdx = index*BATCH_SIZE
            inputs = X[startIdx:endIdx,:]
            targets = Y[startIdx:endIdx]

            # Generate predictions
            Y_pred = g.predict(inputs)

            # Build input and truth arrays for discriminator
            targets = targets.reshape(BATCH_SIZE,1,1)
            truth = np.vstack((np.ones((BATCH_SIZE,1,1)),np.zeros((BATCH_SIZE,1,1))))
            d_loss = d.train_on_batch(np.vstack((targets,Y_pred)),truth)
            d.trainable = False

            # Test GAN
            g_truth = np.ones((BATCH_SIZE,1,1))
            g_loss = gan.train_on_batch(inputs,g_truth)
            d.trainable = True

        print('Epoch {} | d_loss: {} | g_loss: {}'.format(epoch,    d_loss,g_loss))
    g.save_weights('generator',True)
    d.save_weights('discriminator',True)


    return d,g,gan
```",6,10,False,self,,,,,
1482,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,9,bg9r3n,self.MachineLearning,Looking for a good package for spell correction,https://www.reddit.com/r/MachineLearning/comments/bg9r3n/looking_for_a_good_package_for_spell_correction/,jinggu_ucdavis,1555979913,[removed],0,1,False,self,,,,,
1483,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,9,bg9vpj,self.MachineLearning,[R] An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,https://www.reddit.com/r/MachineLearning/comments/bg9vpj/r_an_energy_and_gpucomputation_efficient_backbone/,tumaini-lee,1555980673,"  

[http://arxiv.org/abs/1904.09730](http://arxiv.org/abs/1904.09730)",0,1,False,self,,,,,
1484,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,10,bga1g8,self.MachineLearning,[R] An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection,https://www.reddit.com/r/MachineLearning/comments/bga1g8/r_an_energy_and_gpucomputation_efficient_backbone/,tumaini-lee,1555981598,"  

[http://arxiv.org/abs/1904.09730](http://arxiv.org/abs/1904.09730)",0,19,False,self,,,,,
1485,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,10,bga946,self.MachineLearning,Takeaways from OpenAI 5's Dota 2 Bot (2019),https://www.reddit.com/r/MachineLearning/comments/bga946/takeaways_from_openai_5s_dota_2_bot_2019/,jshek,1555982882,[removed],0,1,False,self,,,,,
1486,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,10,bgab8g,nature.com,"Machine learning model developed which sheds light on how looking patterns vary according to age, providing insight into how toddlers allocate attention and how that changes with development.",https://www.reddit.com/r/MachineLearning/comments/bgab8g/machine_learning_model_developed_which_sheds/,Science_Podcast,1555983226,,1,1,False,default,,,,,
1487,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,10,bgaiqf,self.MachineLearning,ValueError: Target is multiclass but average='binary'. Please choose another average setting,https://www.reddit.com/r/MachineLearning/comments/bgaiqf/valueerror_target_is_multiclass_but_averagebinary/,kets14ole,1555984451,[removed],0,1,False,self,,,,,
1488,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,10,bgaj7e,self.MachineLearning,[D] Observations from OpenAI's Five (Dota 2),https://www.reddit.com/r/MachineLearning/comments/bgaj7e/d_observations_from_openais_five_dota_2/,jshek,1555984532,"I've been working on writing an article about AlphaStar for a while (cough, very late), but after last week's events I decided to sit down and write about OpenAI's 5 success.

&amp;#x200B;

There are a few areas I wish I had more knowledge to expand on: 

* I wish I knew more about OpenAI's Rapid to write about.
* Pros and Cons of PPO for Dota 2. I'd also like to know what didn't work.
* Decision Tree of Starcraft 2 vs OpenAI. Relative to each of the games, OpenAI has solved more of Dota 2's action space. However, Starcraft 2 seems to have a larger decision tree?
* OpenAI mentioned ""surgery"" when thinking of transfer learning, but there isn't much information out there.

&amp;#x200B;

[https://senrigan.io/blog/takeaways-from-openai-5](https://senrigan.io/blog/takeaways-from-openai-5)

Very open to feedback and suggestions, thanks!",5,8,False,self,,,,,
1489,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,11,bgaxb3,self.MachineLearning,"[P] Trump, Obama, Jordan Peterson and Neil deGrasse Tyson TTS models sing Straight Outta Compton",https://www.reddit.com/r/MachineLearning/comments/bgaxb3/p_trump_obama_jordan_peterson_and_neil_degrasse/,hanyuqn,1555986843,"This is a great demonstration of some of the different TTS models I've trained and how I can control style:

https://www.youtube.com/watch?v=SXTdnk7-2i0

These models were trained using my implementation of the papers ""Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis"" (https://arxiv.org/abs/1803.09017) and ""Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron"" (https://arxiv.org/abs/1803.09047).",20,62,False,self,,,,,
1490,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,11,bgaxfs,arxiv.org,[R] Attention Augmented Convolutional Networks,https://www.reddit.com/r/MachineLearning/comments/bgaxfs/r_attention_augmented_convolutional_networks/,xternalz,1555986865,,18,112,False,default,,,,,
1491,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,11,bgaxjt,youtube.com,[D] If you think about it Lempel-Ziv compression is a classic machine learning algorithm (Art of the Problem video),https://www.reddit.com/r/MachineLearning/comments/bgaxjt/d_if_you_think_about_it_lempelziv_compression_is/,britcruise,1555986884,,0,1,False,default,,,,,
1492,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,11,bgb032,self.MachineLearning,[P] Deep Learning on Healthcare Lecture Series (6),https://www.reddit.com/r/MachineLearning/comments/bgb032/p_deep_learning_on_healthcare_lecture_series_6/,hiconcep,1555987307,"Deep Learning on Healthcare (6): Regulations. I found quite interesting argument in twitter between influential people such as Hugh Harvey, Jeremy Howard and Luke Oakden-Rayner. So I decided to introduce this argument and discuss about the regulation for deep learning in healthcare and medicine for the last lecture theme.

&amp;#x200B;

[Deep Learning on Healthcare (6)](https://www.youtube.com/watch?v=rYGft3zJxNM&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee)

[Deep Learning on Healthcare (5)](https://www.youtube.com/watch?v=aI9R7T87hGw&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee&amp;index=2&amp;t=0s)

[Deep Learning on Healthcare (4)](https://www.youtube.com/watch?v=KyTo4BvysfQ)

[Deep Learning on Healthcare (3)](https://www.youtube.com/watch?v=1tMy-ZukPQc)

[Deep Learning on Healthcare (2)](https://www.youtube.com/watch?v=zsXKWEa9GhQ&amp;t=31s&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee&amp;index=24)

[Deep Learning on Healthcare (1)](https://www.youtube.com/watch?v=k0LacC4hyY8&amp;index=23&amp;list=PLdPz9_rcdD97b3iExKYmla8vjlsc_k1ee)",0,14,False,self,,,,,
1493,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,12,bgbgkk,self.MachineLearning,What should the datasets contain if I want to find failure mode of a machine by using text mining?,https://www.reddit.com/r/MachineLearning/comments/bgbgkk/what_should_the_datasets_contain_if_i_want_to/,informationfreak123,1555990048,[removed],0,1,False,self,,,,,
1494,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,12,bgbqmf,self.MachineLearning,Worlds First AI That Can Predict A Persons Choices Before They Make It,https://www.reddit.com/r/MachineLearning/comments/bgbqmf/worlds_first_ai_that_can_predict_a_persons/,navin49,1555991759,[removed],0,1,False,self,,,,,
1495,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,13,bgc02k,self.MachineLearning,"[D] Momentum updates average of g, e.g. Adagrad also of g^2. What other averages might be worth to update? E.g. 4: of g, x, x*g, x^2 give MSE fitted local parabola",https://www.reddit.com/r/MachineLearning/comments/bgc02k/d_momentum_updates_average_of_g_eg_adagrad_also/,jarekduda,1555993376,"Updating exponential moving average is a basic tool of SGD methods, starting with of gradient g in momentum method to extract local linear trend from the statistics.

Then e.g. [Adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf), ADAM family add averages of g_i*g_i to strengthen underrepresented coordinates.

[TONGA](https://papers.nips.cc/paper/3234-topmoumoute-online-natural-gradient-algorithm.pdf) can be seen as another step: updates g_i*g_j averages to model (uncentered) covariance matrix of gradients for Newton-like step.

**I wanted to propose a discussion about some other interesting/promising updated averages for SGD convergence e.g. met in literature?**

For example updating 4 exponential moving averages: of g, x, g*x, x^2 gives MSE fitted parabola in a given direction, estimated Hessian = Cov(g,x).Cov(x,x)^-1 in multiple directions  ([derivation](https://arxiv.org/pdf/1901.11457)). Analogously we could MSE fit e.g. in a single direction degree 3 polynomial if updating 6 averages: of g, x, g*x, x^2, g*x^2, x^3.

Have you seen such updated average in literature, especially of especially g*x? Is it worth e.g. to expand momentum method by such additional averages to model parabola in its direction for smarter step size?",1,7,False,self,,,,,
1496,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,13,bgc8do,self.MachineLearning,[D] I feel like a small data point that one day hopes to be a good dataset in this field.,https://www.reddit.com/r/MachineLearning/comments/bgc8do/d_i_feel_like_a_small_data_point_that_one_day/,sovashadow,1555994915,"Hello all artificially intelligent Redditors:

Little background about me, I am soon to graduate with a BS Software Engineering from a decent university. This is my final semester I am in and I just took a class on AI. Now, my math background is significantly better than the bare minimum of any degree program. However, I still feel I don't have enough math knowledge to pursue AI with a full understanding. Currently I have taken Calc 1, Calc 2, Discrete, and Probability Theory (have to take that last one over the summer). I understand python really well so I can make assumptions on different aspects of code. However when it comes to the real analysis of it all (the models, activation possibilities, etc) I am really lost. I REALLY REALLY want to do this for a living because I think its super cool, but real hard. What would be your advice on being more versed in this realm. Would you go into a masters program and if so where (Looking for best value so not expensive nor dirt cheap). I will add edits as I may have forgotten to add somethings. Thanks in advance.",6,1,False,self,,,,,
1497,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,14,bgcdc3,self.MachineLearning,"[D] ""Face it, it's over when the Physics PhD ""Monster Mind"" hotshots hit the job market""",https://www.reddit.com/r/MachineLearning/comments/bgcdc3/d_face_it_its_over_when_the_physics_phd_monster/,IMO_2009_Q6_SUPERFAN,1555995880,"With webpages like [this](http://stanford.edu/~sfort1/) freely accessible to all, I assume everybody here already knows that there is a tidal wave of former IMO/IPhO/Part III ""mega-hotshots"" (who got tired of doing QFT problem sets) set to finish up their \~Machine learning in Physics\~ PhDs (or similar) within the next 2-3 years. Look into the eyes of every single individual on [this list](https://physics.stanford.edu/people/students) \-- (and if you look them up, about half of them are doing ML stuff) -- you can literally *feel* their scintillating intellects, and they're coming for your ML jobs soon. How is everyone planning to keep up with advancements in ML once these ""[monster minds](http://infoproc.blogspot.com/2012/03/only-he-was-fully-awake.html)"" finish their PhDs? Is ML over for ordinary folks?  


(And by the way, this isn't a troll post. If you think you're on the same level as these folks, that first guy I linked has literally been to [*fucking space*](https://imgur.com/a/SubWwSf) \-- that's the level of hotshot you're dealing with here. What I'm describing is therefore a real, actual ""phenomenon"", which has to be contended with, that is, the existence of ""mega-hotshots"", currently walking the halls of the Stanford physics building, and elsewhere, set to burst onto the scene. )  


Thoughts?",27,0,False,self,,,,,
1498,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,14,bgce03,self.MachineLearning,[N] Fine-grained visual recognition workshop at CVPR 2019 (FGVC6) - with competitions,https://www.reddit.com/r/MachineLearning/comments/bgce03/n_finegrained_visual_recognition_workshop_at_cvpr/,fgvc2017,1555996019,"We are pleased to announce the 6th Workshop on Fine-Grained Visual Recognition at CVPR 2019 in June. The purpose of the workshop is to bring together researchers to explore visual recognition across the continuum between basic level categorization (object recognition) and identification of individuals (face recognition, biometrics) within a category population. 

**Short Papers**
We invite submission of extended abstracts describing work in fine-grained recognition. For more details check out the workshop website.
https://sites.google.com/view/fgvc6/home

**Challenges**
In conjunction with the workshop we are also hosting a series of competitions on Kaggle. These range from classification of different species of plants and animals in images through to predicting fine-grained visual attributes in fashion images. 
https://www.kaggle.com/FGVC6/competitions",0,15,False,self,,,,,
1499,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,14,bgch6v,defouranalytics.com,Which is the best data analytics training institute in Pune?,https://www.reddit.com/r/MachineLearning/comments/bgch6v/which_is_the_best_data_analytics_training/,ankita11_,1555996638,,0,1,False,default,,,,,
1500,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,14,bgcroj,self.MachineLearning,Google made news with offline speech recognition last month. Any updates on it?,https://www.reddit.com/r/MachineLearning/comments/bgcroj/google_made_news_with_offline_speech_recognition/,dafrogspeaks,1555998767,[removed],0,1,False,self,,,,,
1501,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,16,bgdcv3,self.MachineLearning,Using pipelining in deep learning,https://www.reddit.com/r/MachineLearning/comments/bgdcv3/using_pipelining_in_deep_learning/,sanchit2843,1556003403,[removed],0,1,False,self,,,,,
1502,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,16,bgdfda,self.MachineLearning,Tensorflow vs Pytorch,https://www.reddit.com/r/MachineLearning/comments/bgdfda/tensorflow_vs_pytorch/,arkrde,1556003926,[removed],0,1,False,self,,,,,
1503,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,16,bgdiur,self.MachineLearning,5 Popular Online Courses on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bgdiur/5_popular_online_courses_on_machine_learning/,AlexOduvan,1556004686,[removed],0,1,False,self,,,,,
1504,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,16,bgdiw5,self.MachineLearning,Can anyone help me out to solve this question!,https://www.reddit.com/r/MachineLearning/comments/bgdiw5/can_anyone_help_me_out_to_solve_this_question/,vaibhavpundir97,1556004696,[removed],0,1,False,self,,,,,
1505,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,17,bgdufm,self.MachineLearning,Automate the conversion of unstructured pdfs to structured data,https://www.reddit.com/r/MachineLearning/comments/bgdufm/automate_the_conversion_of_unstructured_pdfs_to/,ak96,1556007415,[removed],0,1,False,self,,,,,
1506,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,17,bgdx6q,i.redd.it,Text Classification Algorithms: A Survey arXiv : https://arxiv.org/abs/1904.08067 GitHub: https://github.com/kk7nc/Text_Classification,https://www.reddit.com/r/MachineLearning/comments/bgdx6q/text_classification_algorithms_a_survey_arxiv/,kk7nc,1556008107,,0,1,False,default,,,,,
1507,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,17,bge1mk,self.MachineLearning,"""If software is eating the world, then machine learning is going to eat software - Discuss",https://www.reddit.com/r/MachineLearning/comments/bge1mk/if_software_is_eating_the_world_then_machine/,MichaelSloth,1556009193,[removed],0,1,False,self,,,,,
1508,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,17,bge1xh,self.MachineLearning,[request] finding a dataset for file classification.,https://www.reddit.com/r/MachineLearning/comments/bge1xh/request_finding_a_dataset_for_file_classification/,solidtorrents,1556009262,[removed],0,1,False,self,,,,,
1509,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,18,bge8sp,self.MachineLearning,Scraping videos,https://www.reddit.com/r/MachineLearning/comments/bge8sp/scraping_videos/,martian_rover,1556010930,[removed],1,1,False,self,,,,,
1510,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,18,bge9fk,github.com,[N] Google Open Sources TensorFlow Version of MorphNet DL Tool,https://www.reddit.com/r/MachineLearning/comments/bge9fk/n_google_open_sources_tensorflow_version_of/,hal3e,1556011073,,0,1,False,default,,,,,
1511,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,18,bgegk0,arxiv.org,Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bgegk0/recursive_autoconvolution_for_unsupervised/,_guru007,1556012755,,4,86,False,default,,,,,
1512,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,18,bgegkp,self.MachineLearning,Understanding input to neural networks,https://www.reddit.com/r/MachineLearning/comments/bgegkp/understanding_input_to_neural_networks/,ryanmccauley211,1556012760,[removed],0,1,False,self,,,,,
1513,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,18,bgeiza,self.MachineLearning,Train with faces,https://www.reddit.com/r/MachineLearning/comments/bgeiza/train_with_faces/,noirpunk,1556013325,[removed],0,1,False,self,,,,,
1514,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,18,bgejxv,self.MachineLearning,Pros and Cons of Selenium,https://www.reddit.com/r/MachineLearning/comments/bgejxv/pros_and_cons_of_selenium/,akhilapriya404,1556013541,[removed],0,1,False,https://b.thumbs.redditmedia.com/8gSuyqxhkD3RS71IxlToBrTtKQmWml-6NsKXpD-Y9aQ.jpg,,,,,
1515,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,19,bgel6h,self.MachineLearning,I'm studying cnn for midterm. I have a question.,https://www.reddit.com/r/MachineLearning/comments/bgel6h/im_studying_cnn_for_midterm_i_have_a_question/,_nelsonk,1556013806,[removed],0,1,False,self,,,,,
1516,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,19,bgexat,self.MachineLearning,"After OpenAI solves Dota 2, their next challenge should be Rocket League.",https://www.reddit.com/r/MachineLearning/comments/bgexat/after_openai_solves_dota_2_their_next_challenge/,logicallyzany,1556016505,[removed],0,1,False,self,,,,,
1517,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,20,bgf1ae,self.MachineLearning,Exporting to a keras tf 2.0 model to c++,https://www.reddit.com/r/MachineLearning/comments/bgf1ae/exporting_to_a_keras_tf_20_model_to_c/,Envenger,1556017375,[removed],0,1,False,self,,,,,
1518,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,20,bgfcbj,reddit.com,Good Points that a lot of People Misunderstand.,https://www.reddit.com/r/MachineLearning/comments/bgfcbj/good_points_that_a_lot_of_people_misunderstand/,lofono5567,1556019598,,0,1,False,default,,,,,
1519,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,20,bgff5d,arxiv.org,[1904.09858] Learning gradient-based ICA by neurally estimating mutual information,https://www.reddit.com/r/MachineLearning/comments/bgff5d/190409858_learning_gradientbased_ica_by_neurally/,Traner,1556020110,,2,1,False,default,,,,,
1520,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,20,bgfg9p,arxiv.org,[1904.09858] Learning gradient-based ICA by neurally estimating mutual information,https://www.reddit.com/r/MachineLearning/comments/bgfg9p/190409858_learning_gradientbased_ica_by_neurally/,MTGTraner,1556020323,,4,32,False,default,,,,,
1521,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,20,bgfgly,self.MachineLearning,[R] Differentiable Neural Computer Memory Testing Using dSprites,https://www.reddit.com/r/MachineLearning/comments/bgfgly/r_differentiable_neural_computer_memory_testing/,ThisIsMySeudonym,1556020393,"Some independent research I conducted in testing what's going on in the memory of the DNC when trained to learn a predictive model of the environment for Reinforcement Learning. Unfortunately, the experiments didn't find anything strongly positive. The memory appears to be a black-box, at least when examined in this way. Read more at:  [http://blog.adeel.io/2019/03/10/differentiable-neural-computer-memory-testing-using-dsprites/](http://blog.adeel.io/2019/03/10/differentiable-neural-computer-memory-testing-using-dsprites/) 

&amp;#x200B;

Would love to hear the thoughts of the ML community.",0,4,False,self,,,,,
1522,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,22,bgg9sa,self.MachineLearning,"Can AI generate a Hello World program in Python if you specify you want a string called ""Hello World""?",https://www.reddit.com/r/MachineLearning/comments/bgg9sa/can_ai_generate_a_hello_world_program_in_python/,kalavala93,1556025470,[removed],0,1,False,self,,,,,
1523,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,22,bggbph,blogs.microsoft.com,Machine teaching: How peoples expertise makes AI more powerful,https://www.reddit.com/r/MachineLearning/comments/bggbph/machine_teaching_how_peoples_expertise_makes_ai/,myinnerbanjo,1556025779,,0,1,False,default,,,,,
1524,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,22,bggeok,self.MachineLearning,Modern applications of statistical learning theory?,https://www.reddit.com/r/MachineLearning/comments/bggeok/modern_applications_of_statistical_learning_theory/,tensorflower,1556026283,"I was reading about concentration of measure related stuff recently and was curious whether anyone knows whether this material is still applicable to 'deep learning' models. By statistical learning theory I mean stuff like VC / Rademacher bounds etc. 

If it is, can anyone point to any research papers on this topic?

From my naive understanding, because these bounds relate to the worst-case scenario the union bound may be excessively pessimistic in terms of the number of training examples required.",0,1,False,self,,,,,
1525,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,22,bggf9r,self.MachineLearning,[D] Modern applications of statistical learning theory?,https://www.reddit.com/r/MachineLearning/comments/bggf9r/d_modern_applications_of_statistical_learning/,tensorflower,1556026378,"I was reading about concentration of measure related stuff recently and was curious whether anyone knows whether this material is still applicable to 'deep learning' models. By statistical learning theory I mean stuff like VC / Rademacher bounds etc. 

If it is, can anyone point to any research papers on this topic?

From my naive understanding, because these bounds relate to the worst-case scenario the union bound may be excessively pessimistic in terms of the number of training examples required.",14,38,False,self,,,,,
1526,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,23,bggwjz,self.MachineLearning,Looking for dataset of food and nutritions.,https://www.reddit.com/r/MachineLearning/comments/bggwjz/looking_for_dataset_of_food_and_nutritions/,YonkoNami,1556029035,[removed],0,1,False,self,,,,,
1527,MachineLearning,t5_2r3gv,2019-4-23,2019,4,23,23,bggyq2,self.MachineLearning,[N] mlfinlab Python Package Released (Advances in Financial Machine Learning),https://www.reddit.com/r/MachineLearning/comments/bggyq2/n_mlfinlab_python_package_released_advances_in/,Jackal008,1556029380,"Finally our package mlfinlab has been released on the [PyPi index](https://pypi.org/project/mlfinlab/).

`pip install mlfinlab`

mlfinlab is a living and breathing project in the sense that it is continually enhanced with new code from the chapters in the *Advances in Financial Machine Learning book*. We have built this on lean principles with the goal of providing the greatest value to the quantitative community. 

[Read More on Blog](http://www.quantsportal.com/mlfinlab-on-pypi-index/)",21,82,False,self,,,,,
1528,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,0,bghfd7,self.LanguageTechnology,How to improve TPU utilisation when fine-tuning BERT at scale?,https://www.reddit.com/r/MachineLearning/comments/bghfd7/how_to_improve_tpu_utilisation_when_finetuning/,adammathias,1556031868,,0,1,False,default,,,,,
1529,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,1,bgidso,openai.com,[R] Generative Modeling with Sparse Transformers,https://www.reddit.com/r/MachineLearning/comments/bgidso/r_generative_modeling_with_sparse_transformers/,Kaixhin,1556036687,,1,1,False,default,,,,,
1530,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,1,bgif2o,mdpi.com,Text Classification Algorithms: A Survey,https://www.reddit.com/r/MachineLearning/comments/bgif2o/text_classification_algorithms_a_survey/,kk7nc,1556036865,,0,1,False,default,,,,,
1531,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,1,bgihs5,self.MachineLearning,JS Developer starting ML: TensorflowJS or Tensorflow with Flask?,https://www.reddit.com/r/MachineLearning/comments/bgihs5/js_developer_starting_ml_tensorflowjs_or/,xstheknight,1556037241,"Hi guys,

&amp;#x200B;

I  have recently started binging on ML, DL and going through various books. I have most of the basics covered - how ANN (and specifically CNNs) work, and techniques such as transferred learning, tensors, and also calculus (differentiation, matrix calculus, etc). Still juggling between choosing Tensorflow and Pytorch. Considering TF because they have the JavaScript library as well, so it gives me some degree of flexibility.

&amp;#x200B;

Being a JS developer, and very familiar with NodeJS to build APIs, I was wondering whether I should go down the path of TensorflowJS or else learn python and flask and use Tensorflow/Pytorch. My goal is to build a web app that lets users upload images which go through an inference process and get back the results.",0,1,False,self,,,,,
1532,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,1,bgirwj,self.MachineLearning,[R] Generative Modeling with Sparse Transformers,https://www.reddit.com/r/MachineLearning/comments/bgirwj/r_generative_modeling_with_sparse_transformers/,rtk25,1556038617,"Blog post:  [https://openai.com/blog/sparse-transformer/](https://openai.com/blog/sparse-transformer/) 

Direct link to PDF:  [https://d4mucfpksywv.cloudfront.net/Sparse\_Transformer/sparse\_transformers.pdf](https://d4mucfpksywv.cloudfront.net/Sparse_Transformer/sparse_transformers.pdf) 

&amp;#x200B;

I wonder how this compares to learned ""hard"" attention models like the recurrent attention glimpsing method ( [https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf](https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf) )",12,44,False,self,,,,,
1533,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,2,bgiu73,self.artificial,Draughts AI - An AI-based checkers bot,https://www.reddit.com/r/MachineLearning/comments/bgiu73/draughts_ai_an_aibased_checkers_bot/,Hsankesara,1556038930,,0,1,False,https://b.thumbs.redditmedia.com/Zjw1zMdyeW7wQPBdyqfy5qeAD0umXgvAKTMUOiXZCqw.jpg,,,,,
1534,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,2,bgj21e,self.MachineLearning,arxiv-sanity has been broken for weeks. Are there any active clones/mirrors?,https://www.reddit.com/r/MachineLearning/comments/bgj21e/arxivsanity_has_been_broken_for_weeks_are_there/,PlentifulCoast,1556040036,[removed],0,1,False,self,,,,,
1535,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,2,bgj3jq,youtube.com,RBOLES DE DECISIN LOGISTICOS y de REGRESION - INTELIGENCIA ARTIFICIAL...,https://www.reddit.com/r/MachineLearning/comments/bgj3jq/rboles_de_decisin_logisticos_y_de_regresion/,Xpikuos,1556040245,,0,1,False,default,,,,,
1536,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,2,bgj6eo,self.MachineLearning,[D] arxiv-sanity has been broken for weeks. Are there any active clones/mirrors?,https://www.reddit.com/r/MachineLearning/comments/bgj6eo/d_arxivsanity_has_been_broken_for_weeks_are_there/,PlentifulCoast,1556040637,"arxiv-sanity.com is my favorite resource for finding papers, but you can see on http://www.arxiv-sanity.com/top that there is nothing listed for the past week and it's been this way for several weeks. Karpathy is busy, but the code is open source. I'm wondering if anyone is running a clone.

The 'top' papers list comes from other users' libraries so the benefit only comes if there are multiple users using the site.",9,27,False,self,,,,,
1537,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,2,bgj6ng,medium.com,Respected AI pioneer and visionary Nils John Nilsson passed away early this morning,https://www.reddit.com/r/MachineLearning/comments/bgj6ng/respected_ai_pioneer_and_visionary_nils_john/,gwen0927,1556040668,,0,1,False,https://a.thumbs.redditmedia.com/oAMpwhSpOGmuEJIJS_1rpPn2e2sjSloR_yMYfoRPKC8.jpg,,,,,
1538,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,2,bgjb2s,self.MachineLearning,Neural Network that cares about relative positioning,https://www.reddit.com/r/MachineLearning/comments/bgjb2s/neural_network_that_cares_about_relative/,Geeks_sid,1556041262,[removed],0,1,False,self,,,,,
1539,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,3,bgjoqe,youtu.be,Machine Learning 5-Fold Cross Validation,https://www.reddit.com/r/MachineLearning/comments/bgjoqe/machine_learning_5fold_cross_validation/,vaderme,1556043231,,0,1,False,https://b.thumbs.redditmedia.com/d7nIS6-FnAxv5YeiINnLtfxOKtmMRgM6xpvQKLg5Yzo.jpg,,,,,
1540,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,4,bgkeim,self.MachineLearning,"Hi, could anyone share the outline for NIPS 2019!",https://www.reddit.com/r/MachineLearning/comments/bgkeim/hi_could_anyone_share_the_outline_for_nips_2019/,nile6499,1556046923,[removed],0,1,False,self,,,,,
1541,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,4,bgkf8m,self.MachineLearning,Need a recommendation for a good modern book focused on theory.,https://www.reddit.com/r/MachineLearning/comments/bgkf8m/need_a_recommendation_for_a_good_modern_book/,InarticulateAtheist,1556047020,[removed],0,1,False,self,,,,,
1542,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,4,bgkp4u,self.MachineLearning,Undergrad in First year,https://www.reddit.com/r/MachineLearning/comments/bgkp4u/undergrad_in_first_year/,yobrowussap,1556048418,[removed],0,1,False,self,,,,,
1543,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,4,bgkrrg,self.MachineLearning,[N]TensorFlow World,https://www.reddit.com/r/MachineLearning/comments/bgkrrg/ntensorflow_world/,iyaja,1556048779,"TensorFlow and O'Reily are hosting a new conference at the Santa Clara Convention Center from October Oct 28-31 this year.

&amp;#x200B;

Here is a link to their page: [https://conferences.oreilly.com/tensorflow/tf-ca](https://conferences.oreilly.com/tensorflow/tf-ca)

&amp;#x200B;

What do you guys think about this? Will it be similar to the TensorFlow dev summit? If so, why do we have 2 separate events in the same year?",0,4,False,self,,,,,
1544,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,5,bgkya4,self.MachineLearning,Deep-Learning-Roadmap,https://www.reddit.com/r/MachineLearning/comments/bgkya4/deeplearningroadmap/,iramirsina,1556049703,[removed],0,1,False,self,,,,,
1545,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,5,bgkz9c,self.MachineLearning,Object detection in production or a commercial industry,https://www.reddit.com/r/MachineLearning/comments/bgkz9c/object_detection_in_production_or_a_commercial/,shomerj,1556049837,[removed],0,1,False,self,,,,,
1546,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,5,bgkzm9,self.MachineLearning,Personal Deep Learning Rig,https://www.reddit.com/r/MachineLearning/comments/bgkzm9/personal_deep_learning_rig/,Rhaethedragonprince,1556049885,[removed],0,1,False,self,,,,,
1547,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,5,bglidh,self.MachineLearning,[R] Classic Paper discussion: Deep Neural Networks for YouTube Recommendation,https://www.reddit.com/r/MachineLearning/comments/bglidh/r_classic_paper_discussion_deep_neural_networks/,tdls_to,1556052518,"Recording &amp; slides: [https://aisc.a-i.science/events/2019-04-22/](https://aisc.a-i.science/events/2019-04-22/)

&amp;#x200B;

An AISC event.",1,3,False,self,,,,,
1548,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,6,bglo6x,self.MachineLearning,[P] Predicting Instagram likes for selfies (beta),https://www.reddit.com/r/MachineLearning/comments/bglo6x/p_predicting_instagram_likes_for_selfies_beta/,swordythomas,1556053315,"Hi! We created an app where a deep learning algorithm can tell you from two selfies, which photo would get more likes on Instagram.

You can try Deepsta (beta) on www.findyourlikes.com",9,4,False,self,,,,,
1549,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,6,bglobw,self.MachineLearning,[D] Selection of randomly generated features,https://www.reddit.com/r/MachineLearning/comments/bglobw/d_selection_of_randomly_generated_features/,kalabele,1556053334,"I have some raw data and a list of feature descriptors. A feature descriptor defines a function with parameters including their domain. This allows me to generate almost infinite many random features. Obviously, most features are garbage. My goal is to find a subset of features to train a ""good enough"" model. I suspect there will be features which are usable on their own and features which are only useful in combination with other features.

My current approach is to generate n features, take k of them and train a tree-based model with it. Then I measure the model score and divide it according to the feature importance among the features. A few rounds of cross validation follow. Then I take some other k of the n features and repeat the process until all of the n features have been tested a number of times. Then I start the process with new n features.

I am aware that there is a very high chance that I will miss some great feature combinations. However, I do not see how this could be avoided. Nevertheless, I would like to improve the process. One idea I have is to randomly pick some of the previously best scored features and use them together with new features to train the model. Then at least I might discover features which support the already good features.

Do you know of similar techniques which I could use for inspiration? Or do you think I should approach the problem completely different? Any inputs are welcome.",3,2,False,self,,,,,
1550,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,6,bglqdr,medium.com,"TensorFlow, PyTorch or MXNet? A comprehensive evaluation on NLP &amp; CV tasks with Titan RTX",https://www.reddit.com/r/MachineLearning/comments/bglqdr/tensorflow_pytorch_or_mxnet_a_comprehensive/,gwen0927,1556053630,,0,1,False,https://b.thumbs.redditmedia.com/rOdS3CV9KkMjqKW3Wdwv4CueX5H1rQHXEHk43FkW3To.jpg,,,,,
1551,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,6,bglwhy,self.MachineLearning,[N] Google Colab now comes with free T4 GPUs,https://www.reddit.com/r/MachineLearning/comments/bglwhy/n_google_colab_now_comes_with_free_t4_gpus/,tlkh,1556054493,"What the title says. Head over to [create a new notebook in Colab](https://colab.research.google.com/notebook#create=true&amp;language=python3) and run `nvidia-smi`!

This is a real step-up from the ""ancient"" K80 and I'm really surprised at this move by Google.

Now GPU training on Colab is seriously CPU-limited for data pipeline etc. Still, beggars can't be choosers! This is such a godsend for students.",117,461,False,self,,,,,
1552,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,8,bgn1tm,self.MachineLearning,[D] Hosting data models vs. locally stored models,https://www.reddit.com/r/MachineLearning/comments/bgn1tm/d_hosting_data_models_vs_locally_stored_models/,TrickExplanation,1556060703,"Hey Everyone, 

I've been trying to research this but haven't been able to find exactly the right path to pursue, I was hoping someone could provide me some ideas to research. 

&amp;#x200B;

Here is the concept I'm working on. 

I want to train some models to identify colour, shape, texture from photos a user uploads to their account. The uploading will be through an iOS app. What would be an ideal way creating those models and making them available to all the users? Here are a few use cases I've come up with, just not sure if they are accurate.

\- The model is stored locally on each persons device when they download the app. The model looks for colour, shape and texture but will be trained to identify the ones the user likes from their uploaded photos or liked tags. 

\- The models are trained and stored on a server and will be accessed with an api.

\- Models are trained using scikit and imported to Apple's CoreML to be used on the app.

Would any of these make sense when trying to prove the concept above? 

Could anyone possibly provide me some articles that explain how to create/setup these models in iOS?

Thanks to all who stop by!",1,0,False,self,,,,,
1553,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,10,bgofvl,arxiv.org,[R] TextCaps: Handwritten Character Recognition with Very Small Datasets (~99% MNIST with 200 samples),https://www.reddit.com/r/MachineLearning/comments/bgofvl/r_textcaps_handwritten_character_recognition_with/,sensetime,1556068693,,6,16,False,default,,,,,
1554,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,10,bgop08,gengo.ai,Outsourcing companies for machine learning training data,https://www.reddit.com/r/MachineLearning/comments/bgop08/outsourcing_companies_for_machine_learning/,reimmoriks,1556070162,,0,2,False,default,,,,,
1555,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,10,bgoulj,self.MachineLearning,what is the difference between beta-VAE and VAE with Gaussian likelihood?,https://www.reddit.com/r/MachineLearning/comments/bgoulj/what_is_the_difference_between_betavae_and_vae/,alayaMatrix,1556071035,[removed],0,1,False,self,,,,,
1556,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,11,bgoyym,self.MachineLearning,[P] PyTorch implementation of PlaNet: A Deep Planning Network for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bgoyym/p_pytorch_implementation_of_planet_a_deep/,sensetime,1556071721,"An an [implementation](https://github.com/Kaixhin/PlaNet) of [PlaNet](https://planetrl.github.io) written in PyTorch that supports gym environments and a few continuous control tasks.
 
GitHub repo: https://github.com/Kaixhin/PlaNet

Original implementation in TF: https://github.com/google-research/planet

Link to paper: https://planetrl.github.io",12,26,False,self,,,,,
1557,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,11,bgpg4n,arxiv.org,"[R] Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control Schmidhuber)",https://www.reddit.com/r/MachineLearning/comments/bgpg4n/r_improving_differentiable_neural_computers/,sensetime,1556074504,,2,1,False,default,,,,,
1558,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,12,bgpjt5,arxiv.org,"[R] Improving Differentiable Neural Computers Through Memory Masking, De-allocation, and Link Distribution Sharpness Control (Schmidhuber)",https://www.reddit.com/r/MachineLearning/comments/bgpjt5/r_improving_differentiable_neural_computers/,sensetime,1556075103,,3,14,False,default,,,,,
1559,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,12,bgpleh,sciencemag.org,How to (seriously) read a scientific paper,https://www.reddit.com/r/MachineLearning/comments/bgpleh/how_to_seriously_read_a_scientific_paper/,_guru007,1556075360,,0,1,False,https://b.thumbs.redditmedia.com/YOvQDvEQcKNGrguO-snG9mjHjztDCVsU5qYjgV_1XjA.jpg,,,,,
1560,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,12,bgpmus,arxiv.org,[D] Is coding a relevant metaphor for building AI?,https://www.reddit.com/r/MachineLearning/comments/bgpmus/d_is_coding_a_relevant_metaphor_for_building_ai/,hardmaru,1556075604,,1,0,False,default,,,,,
1561,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,13,bgqaar,self.MachineLearning,"I have got noise pic with my cat face gan model, what's wrong with my keras gan code?",https://www.reddit.com/r/MachineLearning/comments/bgqaar/i_have_got_noise_pic_with_my_cat_face_gan_model/,monkey3233,1556079853,[removed],0,1,False,self,,,,,
1562,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,15,bgr5el,self.MachineLearning,"Tech giants believe that Artificial Intelligence (AI) is the next big thing, Machine Learning and Artificial Intelligence are often interchangeably used terms",https://www.reddit.com/r/MachineLearning/comments/bgr5el/tech_giants_believe_that_artificial_intelligence/,TopicsUnfold,1556086106,[removed],0,1,False,self,,,,,
1563,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,15,bgr5gn,self.MachineLearning,[N] Open AI Destroys 2018 TI Dota 2 Champions,https://www.reddit.com/r/MachineLearning/comments/bgr5gn/n_open_ai_destroys_2018_ti_dota_2_champions/,Pun1sher-,1556086120,"OG lost to the OpenAI Five bots team with a score of 0-2 in an exhibition match. On the second map, the International 2018 champions were defeated in less than 20 minutes.

Both teams picked heroes from a limited hero pool. Unlike previous show matches, OG was permitted to use invisibility, and the OpenAI Five courier was not invulnerable. At the same time, illusions and summoned unit were not allowed.

This is excellent news for AI enthusiasts. The state of AI development is getting better to the point that they could even defeat the champions of a complicated game.

In other news, the World Cyber Games has an upcoming AI robot football tournament. Interested programmers will have the chance to showcase their programming skills and compete against other AI robot programmers. More details about the AI competition here.

Watch the OG vs Open AI show match here: [https://youtu.be/n8c4lOkgr\_U](https://youtu.be/n8c4lOkgr_U)

WCG AI Source &gt;  [http://www.wcg.com/news/view/534](http://www.wcg.com/news/view/534)",6,0,False,self,,,,,
1564,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,15,bgrbxk,reddit.com,Trained an AI Bot to drive autonomously on desert race track without touching game data - Runs entirely on CNN Image Recognition,https://www.reddit.com/r/MachineLearning/comments/bgrbxk/trained_an_ai_bot_to_drive_autonomously_on_desert/,AI_Projects,1556087631,,0,1,False,default,,,,,
1565,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,15,bgrhi7,self.MachineLearning,CNC Machine,https://www.reddit.com/r/MachineLearning/comments/bgrhi7/cnc_machine/,RapidDirect,1556088909,[removed],0,1,False,self,,,,,
1566,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,16,bgrjzx,self.MachineLearning,Weighted MMD for InfoVAE?,https://www.reddit.com/r/MachineLearning/comments/bgrjzx/weighted_mmd_for_infovae/,kiwi0fruit,1556089495,[removed],0,1,False,self,,,,,
1567,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,16,bgrl7d,self.MachineLearning,"[D] Key differences between AI, ML, Data Science and Big Data",https://www.reddit.com/r/MachineLearning/comments/bgrl7d/d_key_differences_between_ai_ml_data_science_and/,cmstrump,1556089759,"All of the four terms (Artificial Intelligence, Machine Learning, Data Science and Big Data) are interlinked, but not interchangeable. I guess the following overview would help beginners with easily differentiating them: [Everything you need to know about key differences between AI, Data Science, Machine Learning and Big Data](https://dlabs.pl/blog/article/key-difference-between-ai-data-science-machine-learning) (see the article for more details)

* AI focuses on mimicking decision-making processes.
* ML reaches beyond the available pieces of training information and interpret data that has never been encountered before.
* Data science combines various methods and data of diverse volumes in order to derive useful, mostly business-oriented, insights through both structural and predictive analyses.
* Big data doesnt analyze but focuses on processing (with high velocity) extreme volumes and a wide variety of data types.",5,0,False,self,,,,,
1568,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,16,bgrmxx,self.MachineLearning,[D] Weighted MMD for InfoVAE?,https://www.reddit.com/r/MachineLearning/comments/bgrmxx/d_weighted_mmd_for_infovae/,kiwi0fruit,1556090143,"I'm trying to figure out how can weighted MMD from [Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1705.00609) (chapter 3. Weighted Maximum Mean Discrepancy) be adapted for [InfoVAE: A Tutorial on Information Maximizing Variational Autoencoders](https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/#maximum-mean-discrepancy).

First one is written in some ""unbiased approximation to MMD with linear complexity"" terms but the second one is written with ""kernel embedding trick"" terms.

I guess some info can be found in original papers: [A Kernel Two-Sample Test](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf) (about terms from weighted MMD article) and [A Kernel Method for the Two-Sample Problem](https://arxiv.org/abs/0805.2368) (presumably original article on MMD) but that a bit too much for me.

I would appreciate any ideas on how to adapt this for InfoVAE?",1,3,False,self,,,,,
1569,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,17,bgs0ll,elegantjbi.com,Data Analytics Tools Will Help You Solve Problems and Optimize Business!,https://www.reddit.com/r/MachineLearning/comments/bgs0ll/data_analytics_tools_will_help_you_solve_problems/,ElegantMicroWebIndia,1556093428,,0,1,False,default,,,,,
1570,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,18,bgsij4,self.MachineLearning,Most wanted perks/deals for AI startups?,https://www.reddit.com/r/MachineLearning/comments/bgsij4/most_wanted_perksdeals_for_ai_startups/,EventHorizonOne,1556098012,[removed],0,1,False,self,,,,,
1571,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,19,bgsxdi,reddit.com,Trained a Bot to drive around autonomously on race tracks and streets in Fortnite without touching Game Data - Runs entirely on CNN Image Recognition,https://www.reddit.com/r/MachineLearning/comments/bgsxdi/trained_a_bot_to_drive_around_autonomously_on/,AI_Projects,1556101506,,0,1,False,https://a.thumbs.redditmedia.com/o99HGwxkz_2Y9SZ8za3RmgEoSH5QJ4vL59VPyaCYJ_4.jpg,,,,,
1572,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,20,bgta97,self.MachineLearning,Should I use TensorFlows Keras API or the TensorFlow Object Detection API for semantic/panoptic image segmentation?,https://www.reddit.com/r/MachineLearning/comments/bgta97/should_i_use_tensorflows_keras_api_or_the/,EmielBoss,1556104206,"I wish to implement [Panoptic FPN](https://arxiv.org/abs/1901.02446) (which adds an extra branch to Mask R-CNN for semantic segmentation) using TensorFlow, and use pre-trained models for its ResNet-FPN backbone. However, I am add a crossroads: do I use TensorFlow's Keras API, or the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)? Is the latter even capable of semantic segmentation? What do you recommend?",0,1,False,self,,,,,
1573,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,20,bgtajh,i.redd.it,"Rachel Thomas from FastAI wants diversity in AI ethics, but resisted a black conservative woman for her right leaning views",https://www.reddit.com/r/MachineLearning/comments/bgtajh/rachel_thomas_from_fastai_wants_diversity_in_ai/,theusdesi,1556104257,,0,1,False,https://b.thumbs.redditmedia.com/mc7zqnd2hnCLfU9mWtiBj7qzF21a-0QP22TLdAK18uo.jpg,,,,,
1574,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,20,bgtb52,self.MachineLearning,Books/Courses?,https://www.reddit.com/r/MachineLearning/comments/bgtb52/bookscourses/,titian101,1556104382,[removed],0,1,False,self,,,,,
1575,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,20,bgtcor,self.MachineLearning,Michio Kaku on AI,https://www.reddit.com/r/MachineLearning/comments/bgtcor/michio_kaku_on_ai/,Destigeous,1556104687,[removed],0,1,False,self,,,,,
1576,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,20,bgte6a,i.redd.it,"[D] Rachel Thomas from FastAI wants diversity in ethics, but resisted a black conservative woman because she has right leaning views",https://www.reddit.com/r/MachineLearning/comments/bgte6a/d_rachel_thomas_from_fastai_wants_diversity_in/,theusdesi,1556104978,,0,1,False,default,,,,,
1577,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,20,bgtfyc,self.MachineLearning,[D] Michio Kaku on AI,https://www.reddit.com/r/MachineLearning/comments/bgtfyc/d_michio_kaku_on_ai/,Destigeous,1556105309,"In this century, technology will give us AI which...

https://www.reddit.com/r/IAmA/comments/bghqn9/i_am_michio_kaku_physicist_futurist_and_author_of/ell96mt?utm_medium=android_app&amp;utm_source=share",4,0,False,self,,,,,
1578,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,20,bgtpq4,linkedin.com,What is Augmented Analytics and Can it Add Value to Business Intelligence?,https://www.reddit.com/r/MachineLearning/comments/bgtpq4/what_is_augmented_analytics_and_can_it_add_value/,ElegantMicroWebIndia,1556107099,,0,1,False,default,,,,,
1579,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,21,bgttar,self.MachineLearning,[1811.12814]Graph-Based Global Reasoning Networks,https://www.reddit.com/r/MachineLearning/comments/bgttar/181112814graphbased_global_reasoning_networks/,kai-zhao,1556107701,[removed],0,1,False,self,,,,,
1580,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,21,bgtzma,sites.google.com,Things You Ought To Study Associate Degree Electrical Panel,https://www.reddit.com/r/MachineLearning/comments/bgtzma/things_you_ought_to_study_associate_degree/,mayankshah90909,1556108791,,0,1,False,default,,,,,
1581,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,22,bgucx8,self.MachineLearning,[P] Environment for process scheduling on resources for Reinforcement Learning modeling,https://www.reddit.com/r/MachineLearning/comments/bgucx8/p_environment_for_process_scheduling_on_resources/,spacevstab,1556111071,"As part of my B.Tech Project on modeling RL algorithms for job shop scheduling, I have written an environment which can be integrated with your RL model. Along with the environment, I have added a naive Q Learning model and Deep Q Network model for some number of machines and jobs.

Find the repository with the codes, features, and instruction for usage [here](https://github.com/spaceVStab/Discrete-Event-Simulator)

I hope this environment is found useful. Also, feel free to provide any suggestion and feedback about the same.",0,8,False,self,,,,,
1582,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,22,bguidm,self.MachineLearning,"Implementing the simplest CNN, how to choose the first conv layer?",https://www.reddit.com/r/MachineLearning/comments/bguidm/implementing_the_simplest_cnn_how_to_choose_the/,pedrobbr,1556111912,[removed],0,1,False,self,,,,,
1583,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,23,bguzug,blog.bitsrc.io,Learn TensorFlow Fundamentals in 20 Minutes,https://www.reddit.com/r/MachineLearning/comments/bguzug/learn_tensorflow_fundamentals_in_20_minutes/,JSislife,1556114607,,0,1,False,https://a.thumbs.redditmedia.com/FNfHe9En70blx6sQ9YxmCL-gwpwgEEqjPPlwPDzxOq8.jpg,,,,,
1584,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,23,bgv5l5,self.MachineLearning,Controlling GAN output,https://www.reddit.com/r/MachineLearning/comments/bgv5l5/controlling_gan_output/,tiduyedzaaa,1556115446,[removed],0,1,False,self,,,,,
1585,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,23,bgv8zj,hackernoon.com,How Your Data is Being Used to Shape the Future of Media &amp; Entertainment,https://www.reddit.com/r/MachineLearning/comments/bgv8zj/how_your_data_is_being_used_to_shape_the_future/,AnnaOnTheWeb,1556115944,,0,1,False,https://b.thumbs.redditmedia.com/1deYZ0xk4yWbPY4x-swZn_v1SPjdeiccrQHxgj_4uuc.jpg,,,,,
1586,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,23,bgvefd,self.MachineLearning,[D] Have we hit the limits of Deep Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/bgvefd/d_have_we_hit_the_limits_of_deep_reinforcement/,AnvaMiba,1556116750,"As per [this thread](https://www.reddit.com/r/MachineLearning/comments/bfq8v9/d_openai_five_vs_humans_currently_at_410633_992/) and [this tweet](https://twitter.com/sherjilozair/status/1119256767798620161?s=20), Open AI Five was trained on something like 45,000 years of gameplay experience, and it took less than one day for humans to figure out strategies to consistently beat it.

Open AI Five, together with AlphaStar, is the largest and most sophisiticated implementation of DRL, and yet it falls short of human intelligence by this huge margin. And I bet that AlphaStar would succumb to the same fate if they released it as a bot for anybody to play with.


I know there is lots of research going on to make DRL more data efficient, and to make deep learning in general more robust to out-of-distribution and adversarial examples, but the gap with humans here is so extreme that I doubt it can be meaningfully closed by anything short of a paradigm shift.

What are your thoughts? Is this the limit of what can be achieved by DRL, or is there still hope to push the paradigm foward?",122,216,False,self,,,,,
1587,MachineLearning,t5_2r3gv,2019-4-24,2019,4,24,23,bgveml,self.MachineLearning,[N] Biweekly newsletter for Machine Learning Engineers,https://www.reddit.com/r/MachineLearning/comments/bgveml/n_biweekly_newsletter_for_machine_learning/,yegortokmakov,1556116780,"Hello, reddit!
I'm reading a lot of ML related news/posts/tutorials as part of my daily activities right now.
I thought it might be interesting for others if I share what I found the most important in the format of a biweekly newsletter. 
Subscribe at https://www.getrevue.co/profile/ML-Engineering-Digest
Expect to see materials about ML frameworks, data engineering, model deployment and scaling techniques.",3,13,False,self,,,,,
1588,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,0,bgvv63,forbes.com,Our Entire AI Revolution Is Built On A Correlation House Of Cards,https://www.reddit.com/r/MachineLearning/comments/bgvv63/our_entire_ai_revolution_is_built_on_a/,RacerRex9727,1556119111,,0,1,False,default,,,,,
1589,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,0,bgvxys,medium.com,"Now You See Me, Now You Dont: Fooling a Person Detector",https://www.reddit.com/r/MachineLearning/comments/bgvxys/now_you_see_me_now_you_dont_fooling_a_person/,Yuqing7,1556119492,,0,1,False,https://b.thumbs.redditmedia.com/H3RLOxuTd-bbhII22hbwGE2AQNajeS9MjckW3IMg3rE.jpg,,,,,
1590,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,0,bgvzdu,self.MachineLearning,[D] Jokes generated via GPT-2,https://www.reddit.com/r/MachineLearning/comments/bgvzdu/d_jokes_generated_via_gpt2/,simiansays,1556119698,"Hi all, I have had a good play with finetuning the GPT-2 117M model on various datasets. I'm impressed with even this ""dumbed-down"" version's ability to generate human-like text.

I got it to generate some Q&amp;A style jokes, and a few have me scratching my head as to how these came about.

I'd love your input or feedback on what could be going on inside GPT-2 to make these associations?

I fine-tuned using [this notebook](https://colab.research.google.com/drive/1OiieFQZyROURR9kvfrsytsy4lGTSSIwP) but with a different dataset. The jokes below were generated using unconditional sampling during training.

The jokes generated were obviously not great, but there were some gems in there that seemed to be outside the norm of AI-generated humor.

In all of the below cases, the punchline words either didn't appear in the fine-tuning corpus, or appeared only briefly in very different situations.

`What do you call an obscure bird?`

`A falchion.`

When I first read this, I thought it was funny because I assumed a falchion is actually an obscure bird. It's not; it's a sword. The main reference I could find on Google about a falchion bird is to the [PPC wiki](https://ppc.fandom.com/wiki/Falchion). That's a super-obscure reference. Is GPT-2 actually referencing that page? And/or as a pun on ""falcon""?

`What do you get when you remove the battery of a car?`

`A carotid hemorrhage.`

Again, no mention of carotid or hemorrhage in the fine-tuning set. Is this maybe a car-based pun? ""Carotid hemorrhage"" has about 3k results in Google but that is a pretty obscure reference. Any theories as to why GPT-2 selected this? Would GPT-2 use ""carotid"" as a response for ""car"" in a joke?

`Did you know, the best thing about the Olympics is the smell of urine during them.`

One of the very few one-liners produced in my samples. This one has some humor, but ""the best thing about the Olympics is the"" only returns 9 hits on Google and there's nothing like this in the fine-tuning set. Any theories?

`Who's the smartest game manager ever?`

`Eric B.`

This appears to be an association to ""Eric B."" (the rapper) and the idea of ""the game"" from rap? Again, no mention of Eric B in the fine-tuning set. This one blew my mind.

`What does a cow say when it runs out of water?`

`You need a pump!`

To a human, this joke sounds like it is anthropomorphizing the cow to be addressing its farmer. But how could GPT-2 get there? Again, nothing like this in the fine-tuning set (almost zero pump references, 17 ""cow say"" jokes but nothing of this nature), and nothing like the Q or A that I could find in Google.

Honestly my mind is really blown by these results and I'd love any ideas as to how the Transformer-based algorithm comes up with these type of associations.

These jokes are not prime-time-worthy, but they are among the best I've seen from AI and I'm really scratching my head as to how they got generated. Any theories or discussion would be much appreciated!",7,37,False,self,,,,,
1591,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,0,bgw1hs,twitch.tv,"[P] Live from Harvard on Twitch, Wed 4/24 at 3pm ET, join CS50's Nick Wong as we build a neural network in Tensorflow to perform a style transfer (similar to Google's DeepDream) on images of prior streams.",https://www.reddit.com/r/MachineLearning/comments/bgw1hs/p_live_from_harvard_on_twitch_wed_424_at_3pm_et/,coltonoscopy,1556119994,,0,1,False,default,,,,,
1592,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,0,bgw5ea,blog.logrocket.com,TensorFlow.js: An intro and analysis with use cases,https://www.reddit.com/r/MachineLearning/comments/bgw5ea/tensorflowjs_an_intro_and_analysis_with_use_cases/,efofex55,1556120548,,0,1,False,default,,,,,
1593,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,0,bgw9v2,self.MachineLearning,"Simple Questions Thread April 24, 2019",https://www.reddit.com/r/MachineLearning/comments/bgw9v2/simple_questions_thread_april_24_2019/,AutoModerator,1556121177,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",0,1,False,self,,,,,
1594,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,0,bgwa9c,r-statistics.co,Logistic Regression With R,https://www.reddit.com/r/MachineLearning/comments/bgwa9c/logistic_regression_with_r/,Thistleknot,1556121227,,0,1,False,default,,,,,
1595,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,1,bgwd98,agilescientific.com,[P] Checklist to review Machine learning projects,https://www.reddit.com/r/MachineLearning/comments/bgwd98/p_checklist_to_review_machine_learning_projects/,infracanis,1556121659,,0,1,False,default,,,,,
1596,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,1,bgwsew,self.MachineLearning,Is there any reputable sources on why Python is used so widely for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bgwsew/is_there_any_reputable_sources_on_why_python_is/,thebobbrom,1556123707,[removed],0,1,False,self,,,,,
1597,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,2,bgxg68,self.MachineLearning,[D] Time series forecasting RNN fail,https://www.reddit.com/r/MachineLearning/comments/bgxg68/d_time_series_forecasting_rnn_fail/,hadaev,1556126990,"Here is full code

[https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E](https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E)

&amp;#x200B;

I took Bitcoin data, trained the model, if simply predict the test set, the result looks nice.

[https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E#scrollTo=4r8XINzZUPxw&amp;line=6&amp;uniqifier=1](https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E#scrollTo=4r8XINzZUPxw&amp;line=6&amp;uniqifier=1)

But when I tried to predict from the predicted values (its called stepwise?), the model fail and gives the same number.

Like here

[https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E#scrollTo=7Q9xGm9MSjEB](https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E#scrollTo=7Q9xGm9MSjEB)

Also i tried to make statefull copy of model (it suggested somewhere on stackoverflow), but it fail too:

[https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E#scrollTo=-bsazaipc2LL](https://colab.research.google.com/drive/1gHkpsuSapDSUj3Q9kGwR6pWzVM1m1d8E#scrollTo=-bsazaipc2LL)",9,0,False,self,,,,,
1598,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,2,bgxrx5,self.MachineLearning,SepCnn tensorflow2 does not learn anything.,https://www.reddit.com/r/MachineLearning/comments/bgxrx5/sepcnn_tensorflow2_does_not_learn_anything/,babalinobaba,1556128618,[removed],0,1,False,self,,,,,
1599,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,2,bgxrz7,self.MachineLearning,"Yet Another Feature Pyramid Network Implementation, but in Pytorch1.0",https://www.reddit.com/r/MachineLearning/comments/bgxrz7/yet_another_feature_pyramid_network/,gurkirtsingh,1556128626,[removed],0,1,False,self,,,,,
1600,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgxxht,self.MachineLearning,[D] Introduction to Motion Estimation with Optical Flow,https://www.reddit.com/r/MachineLearning/comments/bgxxht/d_introduction_to_motion_estimation_with_optical/,manneshiva,1556129392,[removed],0,1,False,self,,,,,
1601,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgxz0x,self.MachineLearning,[D] Skills to focus on as a PhD student if you know you want to work in industry?,https://www.reddit.com/r/MachineLearning/comments/bgxz0x/d_skills_to_focus_on_as_a_phd_student_if_you_know/,harmonium1,1556129607,"I'm in a similar situation as several of my third year PhD student friends--we've enjoyed doing research and have had various levels of success so far, enough to be able to (and want to) hopefully finish in the next two years, but we all realize that we'll be doing industry roles and that it would be tough with our publication records to get into one of the top research groups that publishes pure research.

I've had some research-y internships in industry but I think a ""research engineer""-type role would be best for me, (and it's also much more common than research scientist positions in areas I'm interested in such as autonomy and robotics).  However, those roles generally ask for someone who is equally well-versed in statistical machine learning theory, deep learning and Python (check) and software development in C/C++ (something I need to work on).  Research code is almost exclusively in Python (aside from the ocassional CUDA kernel for a novel layer) so essentially no one I know does development in C++ aside from side projects.  

How do you maintain your software chops while focusing on developing novel methods and writing research code in Python?",38,87,False,self,,,,,
1602,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgy0cc,ai.googleblog.com,Evaluating the Unsupervised Learning of Disentangled Representations,https://www.reddit.com/r/MachineLearning/comments/bgy0cc/evaluating_the_unsupervised_learning_of/,sjoerdapp,1556129791,,0,1,False,default,,,,,
1603,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgy2ip,self.MachineLearning,What is the total compensation for entry level machine learning / data science roles in NYC/SF?,https://www.reddit.com/r/MachineLearning/comments/bgy2ip/what_is_the_total_compensation_for_entry_level/,qwerty123000,1556130109,[removed],0,1,False,self,,,,,
1604,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgy35h,self.MachineLearning,"If you could ask for a step-by-step tutorial, what would it be?",https://www.reddit.com/r/MachineLearning/comments/bgy35h/if_you_could_ask_for_a_stepbystep_tutorial_what/,mnunnari,1556130193,[removed],0,1,False,self,,,,,
1605,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgy3ms,self.MachineLearning,"[D] If you could ask for a step-by-step tutorial, what would it be?",https://www.reddit.com/r/MachineLearning/comments/bgy3ms/d_if_you_could_ask_for_a_stepbystep_tutorial_what/,mnunnari,1556130264,I ask because I'm building [tutorialearn.com](http://tutorialearn.com/): a platform and a marketplace for step-by-step tutorials,4,1,False,self,,,,,
1606,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgy5ds,github.com,"GitHub - kpot/keras-transformer: Keras library for building (Universal) Transformers, facilitating BERT and GPT models",https://www.reddit.com/r/MachineLearning/comments/bgy5ds/github_kpotkerastransformer_keras_library_for/,fratkabula,1556130516,,0,1,False,default,,,,,
1607,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgy8ku,self.MachineLearning,[D] Motion Estimation with Optical Flow,https://www.reddit.com/r/MachineLearning/comments/bgy8ku/d_motion_estimation_with_optical_flow/,manneshiva,1556130960,"Most real-time video processing systems/techniques only address relationships of objects within the same frame, disregarding time information. Optical flow accounts for this temporal relationship between frames.

Advances in Optical Flow have changed the game in Object Tracking and Human Activity Recognition in videos.

Heres a blog post explaining the fundamentals and giving you all the code you need to implement it: [https://blog.nanonets.com/optical-flow/](https://blog.nanonets.com/optical-flow/)",0,7,False,self,,,,,
1608,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgy985,self.MachineLearning,Online ML degree?,https://www.reddit.com/r/MachineLearning/comments/bgy985/online_ml_degree/,jdowning1210,1556131053,[removed],0,1,False,self,,,,,
1609,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgyd0h,self.MachineLearning,[D] Generative models for videos,https://www.reddit.com/r/MachineLearning/comments/bgyd0h/d_generative_models_for_videos/,Maplernothaxor,1556131584,"Hi, Im looking for some recent papers in this area.

So far Ive come across Generating Videos with Scene Dynamics (2016) and Adversarial Video Generation (2017).

Would love to hear about other works in the area.",3,1,False,self,,,,,
1610,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgyf9m,link.medium.com,[P] AI read news for you,https://www.reddit.com/r/MachineLearning/comments/bgyf9m/p_ai_read_news_for_you/,gorbel_alex,1556131904,,0,1,False,default,,,,,
1611,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,3,bgyhwf,self.MachineLearning,Anyone interested in an online reading group?,https://www.reddit.com/r/MachineLearning/comments/bgyhwf/anyone_interested_in_an_online_reading_group/,theakhileshrai,1556132267,You will be given networks each week for a period of 8 weeks. All communication happens through this thread. Post if interested.,0,1,False,self,,,,,
1612,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,4,bgykeo,self.MachineLearning,[P] AI read news for you,https://www.reddit.com/r/MachineLearning/comments/bgykeo/p_ai_read_news_for_you/,gorbel_alex,1556132612,"Hi there! We with friends created an app for IOS that can voice any article from any media for you. In this app we're using the technology based on machine learning. The app is free without any advertisement. Now we gathering feedbacks from users. So if this sounds interesting for you, please find more information here: \[What is Peech app and how to use it\]([https://link.medium.com/Yq3uCwYE9V](https://link.medium.com/Yq3uCwYE9V)) I will be glad to hear any thoughts, ideas and feedbacks about the app. Thanks!",0,0,False,self,,,,,
1613,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,4,bgyl32,self.MachineLearning,[D] Compensation for NY/SF entry level machine learning / data science roles?,https://www.reddit.com/r/MachineLearning/comments/bgyl32/d_compensation_for_nysf_entry_level_machine/,qwerty123000,1556132704,"1) compensation for entry level out of undergrad
2) compensation for first job out of good masters program

Not for anyone called a ""data scientist"" but for those doing advanced analytics using high level statistics and machine learning.

Thanks.",5,2,False,self,,,,,
1614,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,4,bgyz8y,thehackerhustle.com,Daily livestreamed Machine Learning implementations going live in 20 minutes! Teaching Intro to Probabilistic ML tonight,https://www.reddit.com/r/MachineLearning/comments/bgyz8y/daily_livestreamed_machine_learning/,haron1100,1556134689,,0,1,False,default,,,,,
1615,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,4,bgz51k,self.MachineLearning,Material for Hidden Markov model,https://www.reddit.com/r/MachineLearning/comments/bgz51k/material_for_hidden_markov_model/,Withered_Shadow,1556135494,"Hi

Is there any material online for HMMs that explains from the basics with the derivations?

Thanks",0,1,False,self,,,,,
1616,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,5,bgzi8d,venturebeat.com,[N] Facebooks AI can convert one singers voice into another,https://www.reddit.com/r/MachineLearning/comments/bgzi8d/n_facebooks_ai_can_convert_one_singers_voice_into/,xulinas,1556137359,,0,1,False,https://b.thumbs.redditmedia.com/BvPc_RukZ78RkRUyA87Fx2ft94pFwRmiSf_fw5wY5Mo.jpg,,,,,
1617,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,5,bgzlpa,youtu.be,"Tesla conference about their advances with autonomous cars. An overview is given about their new chip design, performance, and issues with training CNNs.",https://www.reddit.com/r/MachineLearning/comments/bgzlpa/tesla_conference_about_their_advances_with/,gigiblender,1556137873,,0,1,False,default,,,,,
1618,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,5,bgzpsj,medium.com,Training Turi Create models in Google Colab,https://www.reddit.com/r/MachineLearning/comments/bgzpsj/training_turi_create_models_in_google_colab/,lisamachine,1556138459,,0,2,False,default,,,,,
1619,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,6,bh0az7,self.MachineLearning,Point Cloud Alignment using a Neural Network?,https://www.reddit.com/r/MachineLearning/comments/bh0az7/point_cloud_alignment_using_a_neural_network/,callmebiz,1556141541,[removed],0,1,False,self,,,,,
1620,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,6,bh0kl0,self.MachineLearning,[P] I used RNNs and Markov Chains to generate artificial Bob Dylan lyrics.,https://www.reddit.com/r/MachineLearning/comments/bh0kl0/p_i_used_rnns_and_markov_chains_to_generate/,al_xing,1556142959,"In this article I explore Markov Chains and RNNs (Recurrent Neural Networks) and then propose a comparison and analysis of which one outputs better results and how they both perform in predicting the lyrics of Bob Dylan

  
Thanks for the space!  


[https://towardsdatascience.com/bewildering-brain-332d5192e95b?source=friends\_link&amp;sk=201802393d3ef10cafcdeb4a2d6db955](https://towardsdatascience.com/bewildering-brain-332d5192e95b?source=friends_link&amp;sk=201802393d3ef10cafcdeb4a2d6db955)",11,62,False,self,,,,,
1621,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,7,bh0rbk,medium.com,Machine Learning meets Functional Programming: Nubank open-sources ML library,https://www.reddit.com/r/MachineLearning/comments/bh0rbk/machine_learning_meets_functional_programming/,agace,1556143945,,0,2,False,default,,,,,
1622,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,7,bh15ty,github.com,tensorflow/graphics is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/bh15ty/tensorflowgraphics_is_a_new_github_repo_by/,sjoerdapp,1556146166,,0,1,False,default,,,,,
1623,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,8,bh1bbh,ben-evans.com,Notes on AI Bias,https://www.reddit.com/r/MachineLearning/comments/bh1bbh/notes_on_ai_bias/,j_orshman,1556147037,,0,1,False,default,,,,,
1624,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,8,bh1mgg,self.MachineLearning,[R] Training worm brains to recognize digits,https://www.reddit.com/r/MachineLearning/comments/bh1mgg/r_training_worm_brains_to_recognize_digits/,VinayUPrabhu,1556148846,"Colab notebook link:  [https://github.com/vinayprabhu/Network\_Science\_Meets\_Deep\_Learning/blob/master/1\_MNIST\_C\_Elegans.ipynb](https://github.com/vinayprabhu/Network_Science_Meets_Deep_Learning/blob/master/1_MNIST_C_Elegans.ipynb)   


This is the first in a series of notebooks we will disseminate in the area of complex networks inspired architecture for deep neural networks.  

![img](8jb0eojxrau21)",9,37,False,self,,,,,
1625,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,8,bh1ohm,self.MachineLearning,Do the nodes in the output layer also have biases or do only the nodes in the hidden layers have biases?,https://www.reddit.com/r/MachineLearning/comments/bh1ohm/do_the_nodes_in_the_output_layer_also_have_biases/,Zer0Followers,1556149188,"I'm sorry if this is a stupid

I'm very new to this",0,1,False,self,,,,,
1626,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,11,bh34i9,self.MachineLearning,Racism in Tech and Algorithms - Panel this Monday!,https://www.reddit.com/r/MachineLearning/comments/bh34i9/racism_in_tech_and_algorithms_panel_this_monday/,Tech_Ethics_Mike,1556157894,[removed],0,1,False,self,,,,,
1627,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,11,bh3aqz,self.MachineLearning,Which companies/organizations are doing AGI research?,https://www.reddit.com/r/MachineLearning/comments/bh3aqz/which_companiesorganizations_are_doing_agi/,unguided_deepness,1556158929,[removed],0,1,False,self,,,,,
1628,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,11,bh3bx2,self.MachineLearning,[Discussion] Pop!_OS vs Ubuntu for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bh3bx2/discussion_pop_os_vs_ubuntu_for_machine_learning/,OPMaster494,1556159127,"Ubuntu has been well known for it's #1 spot in *the most popular distro for Machine Learning.* However, the newer distro, Pop! OS, has gained much popularity in the past year. It specializes in a productive interface that aims towards developers. In addition, I have seen that Pop OS to be quite similar, and at times, better than Ubuntu. 

So I raise this question: What is Pop! OS in terms of Machine Learning compared to Ubuntu?  Does it stand higher than Ubuntu - the distro we had all trusted for many years? Or is it not yet ready to become the most preferred distro for ML development? 

&amp;#x200B;

I would **love** to hear your opinion!",14,0,False,self,,,,,
1629,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,12,bh3qmb,self.MachineLearning,Game agent,https://www.reddit.com/r/MachineLearning/comments/bh3qmb/game_agent/,imnottheguyy,1556161650," 

So I have a project due in about a months time in which we have to develop an agent for a game. 

&amp;#x200B;

The game is on a 7\*7 hexagonal grid, where the object of the game getting as many pieces as possible off the board. A piece can get off the board only from the opposite side from its initial position. Opponents pieces can be captured and converted but jumping over it like in checkers.

&amp;#x200B;

I'm having trouble using machine learning to create my agent. Although it's not mandatory for the assignment to use machine learning my marks are pretty much capped to about 80% if I don't.

&amp;#x200B;

I don't want to start coding until I know exactly what I want to do duhhhh. But finding appropriate machine learning algorithms is becoming a bit of a task for me. Especially considering the timeframe and balancing between other subject and it being due near the beginning of the exam period. For example, using MCTS would be too complex to fully comprehend/implement given my time constraints.

&amp;#x200B;

Getting to my question, is there any 'short' courses I could complete that would help me with this project. Or any approaches you think may be helpful.

I really don't want to resort to algorithms like minimax or max\^n. Although, they will get the job done, my grades will suffer.

anyways thanks for taking the time to read this, all responses are greatly appreciated.",0,1,False,self,,,,,
1630,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,12,bh3s72,self.MachineLearning,MS in Machine Learning vs. MS in Computer Science,https://www.reddit.com/r/MachineLearning/comments/bh3s72/ms_in_machine_learning_vs_ms_in_computer_science/,illdogh,1556161917,[removed],0,1,False,self,,,,,
1631,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,13,bh4d5s,self.MachineLearning,Anomaly Detection in Time Series Data,https://www.reddit.com/r/MachineLearning/comments/bh4d5s/anomaly_detection_in_time_series_data/,novel_eye,1556165826,[removed],0,1,False,self,,,,,
1632,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,13,bh4ghd,self.MachineLearning,How to label and train on video dataset? Can we use RNN with LSTM?,https://www.reddit.com/r/MachineLearning/comments/bh4ghd/how_to_label_and_train_on_video_dataset_can_we/,WeirdRat67,1556166476,[removed],0,1,False,self,,,,,
1633,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,13,bh4pk4,self.MachineLearning,"Get, Set, Research!",https://www.reddit.com/r/MachineLearning/comments/bh4pk4/get_set_research/,pratham221,1556168353,[removed],0,1,False,self,,,,,
1634,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,14,bh4xww,self.MachineLearning,LHD S.p.A Is Worldwide Leader in designing and building Telescopic Forks,https://www.reddit.com/r/MachineLearning/comments/bh4xww/lhd_spa_is_worldwide_leader_in_designing_and/,lhd121,1556170096,[removed],0,1,False,https://b.thumbs.redditmedia.com/HGxLlYwTl_5XTF4P5gtHk_-g2jVrxPuihvyZgJ-RQLg.jpg,,,,,
1635,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,14,bh4zpf,medium.com,[N] Summary: SimPLe,https://www.reddit.com/r/MachineLearning/comments/bh4zpf/n_summary_simple/,CartPole,1556170492,,0,1,False,default,,,,,
1636,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,14,bh55il,self.MachineLearning,[R] Speech synthesis from neural decoding of spoken sentences (Nature),https://www.reddit.com/r/MachineLearning/comments/bh55il/r_speech_synthesis_from_neural_decoding_of_spoken/,sensetime,1556171906,"**Speech synthesis from neural decoding of spoken sentences**

Technology that translates neural activity into speech would be transformative for people who are unable to communicate as a result of neurological impairments. Decoding speech from neural activity is challenging because speaking requires very precise and rapid multi-dimensional control of vocal tract articulators. Here we designed a neural decoder that explicitly leverages kinematic and sound representations encoded in human cortical activity to synthesize audible speech. Recurrent neural networks first decoded directly recorded cortical activity into representations of articulatory movement, and then transformed these representations into speech acoustics. In closed vocabulary tests, listeners could readily identify and transcribe speech synthesized from cortical activity. Intermediate articulatory dynamics enhanced performance even with limited data. Decoded articulatory representations were highly conserved across speakers, enabling a component of the decoder to be transferrable across participants. Furthermore, the decoder could synthesize speech when a participant silently mimed sentences. These findings advance the clinical viability of using speech neuroprosthetic technology to restore spoken communication.

Nature (paywall): https://www.nature.com/articles/s41586-019-1119-1

direct link to pdf: https://www.gwern.net/docs/ai/2019-anumanchipalli.pdf

YouTube demo: https://www.youtube.com/watch?v=kbX9FLJ6WKw

*Articles about this paper:*

Nature: https://www.nature.com/articles/d41586-019-01328-x

Scientific American: https://www.scientificamerican.com/article/scientists-take-a-step-toward-decoding-thoughts/

NY Times: https://www.nytimes.com/2019/04/24/health/artificial-speech-brain-injury.html",17,162,False,self,,,,,
1637,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,15,bh566x,medium.com,"Get, Set, Reseach",https://www.reddit.com/r/MachineLearning/comments/bh566x/get_set_reseach/,pratham221,1556172055,,0,1,False,default,,,,,
1638,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,15,bh5a3d,self.MachineLearning,Question about OpenFace,https://www.reddit.com/r/MachineLearning/comments/bh5a3d/question_about_openface/,khane_walla,1556172951,[removed],0,1,False,self,,,,,
1639,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,15,bh5atk,self.MachineLearning,An AI soccer game? Future sports are coming to the WCG!,https://www.reddit.com/r/MachineLearning/comments/bh5atk/an_ai_soccer_game_future_sports_are_coming_to_the/,neitancastle,1556173119,[removed],0,1,False,self,,,,,
1640,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,15,bh5cg2,self.MachineLearning,Using A 2D hot vector as target value,https://www.reddit.com/r/MachineLearning/comments/bh5cg2/using_a_2d_hot_vector_as_target_value/,mkal001,1556173501,"Guys , my task is to train a CNN to predict a number of the Barcode. My dataset have corresponding digit ID for each barcode. So suppose the id is : 1234
Then I am labelling it as : (let max digit possible-4)
[ 
      [ 0,1,0,0,0]
      [ 0,0,1,0,0]
       [0,0,0,1,0]
       [0,0,0,0,1]
]
 NOW THE PROBLEM IS :
how to define the last layer to give such output and then computing the loss later ?",0,1,False,self,,,,,
1641,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,15,bh5csv,link.medium.com,"Get, Set, Research !",https://www.reddit.com/r/MachineLearning/comments/bh5csv/get_set_research/,pratham221,1556173568,,0,1,False,default,,,,,
1642,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,15,bh5g5f,wcg.com,An AI soccer game? Future sports?,https://www.reddit.com/r/MachineLearning/comments/bh5g5f/an_ai_soccer_game_future_sports/,neitancastle,1556174322,,1,1,False,default,,,,,
1643,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,15,bh5l3e,self.MachineLearning,How to understand find a simpler distribution q to be close to prior distribution in deep bayesian network?,https://www.reddit.com/r/MachineLearning/comments/bh5l3e/how_to_understand_find_a_simpler_distribution_q/,Catherine_Fang,1556175540,"The first paragraph says we want to find a simpler distribution q to be close to intractable **posterior distribution**,

but in the second paragraph penalize q for differing from **prior distribution** instead of posterior distribution**?**

How to understand it?

https://i.redd.it/dzpdbobuycu21.png",1,1,False,self,,,,,
1644,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,16,bh5wu1,i-programmer.info,How Do Open Source Deep Learning Frameworks Stack Up?,https://www.reddit.com/r/MachineLearning/comments/bh5wu1/how_do_open_source_deep_learning_frameworks_stack/,pmz,1556178339,,0,1,False,default,,,,,
1645,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,17,bh64z0,self.MachineLearning,Serverless: Can It Simplify Data Science Projects?,https://www.reddit.com/r/MachineLearning/comments/bh64z0/serverless_can_it_simplify_data_science_projects/,IguazioDani,1556180388,[removed],0,1,False,self,,,,,
1646,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,17,bh6666,self.MachineLearning,"I created a text clustering, how to evaluate my results",https://www.reddit.com/r/MachineLearning/comments/bh6666/i_created_a_text_clustering_how_to_evaluate_my/,khalidreemy,1556180736,[removed],0,1,False,self,,,,,
1647,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,17,bh666l,slashdata.co,Only around 30% of ML developers who develop algorithms for search engines or customer support management believe AI should not be used to replace human jobs as opposed to around 50% of those who develop stock market predictions or image classification/object recognition algorithms.,https://www.reddit.com/r/MachineLearning/comments/bh666l/only_around_30_of_ml_developers_who_develop/,vjmde,1556180739,,0,1,False,default,,,,,
1648,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,17,bh6chh,self.MachineLearning,Whats the difference between TenserFlow in various languages?,https://www.reddit.com/r/MachineLearning/comments/bh6chh/whats_the_difference_between_tenserflow_in/,Flamyngoo,1556182446,[removed],0,1,False,self,,,,,
1649,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,18,bh6l7p,self.MachineLearning,[Discussion] Can Time Series Analysis be considered as a part of Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/bh6l7p/discussion_can_time_series_analysis_be_considered/,negroide2000,1556184687,"Recently had a dispute with a head of the research group Im working in about terminology. The chief claimed that Time Series Analysis (TSA) is a separate field of study which does apply Machine Learning techniques for its problems (NN and so on). While I tried to convince my chief that Machine Learning is a wide term, which is used for a group of different fields of science: NN, TSA, Statistics etc. Was I correct? Or am I mistaken? The discussion was in a context of writing an article so the terminology did matter.",14,1,False,self,,,,,
1650,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,19,bh74gq,self.MachineLearning,[P] Implementing Attention Augmented Convolutional Networks using Pytorch,https://www.reddit.com/r/MachineLearning/comments/bh74gq/p_implementing_attention_augmented_convolutional/,leaderj1001,1556189093,"  Hi, Im Myeongjun Kim. Im currently taking a masters course. My major is computer vision using deep learning. This is the second reddit posting. Ive implemented the paper Attention Augmented Convolutional Networks written by Google Brain as a Pytorch. Already implemented in the paper as Tensorflow, I changed it to Pytorch.  
 I am currently conducting an experiment in a simple layer(3 convolution layers) and will further conduct an experiment on ResNet in paper. (CIFAR-100)

Thank you so much for reading the long story. Any feedback is welcome :)

Github URL: [https://github.com/leaderj1001/Attention-Augmented-Conv2d](https://github.com/leaderj1001/Attention-Augmented-Conv2d)

Thank you !!",11,38,False,self,,,,,
1651,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,19,bh75qw,self.MachineLearning,[D] Could GAN generate new ML algorithms?,https://www.reddit.com/r/MachineLearning/comments/bh75qw/d_could_gan_generate_new_ml_algorithms/,Mr_Greyman,1556189343,"That's simple, do you think we can face an algorithm producer to validator consumer and generate new algorithms to whatever problem we want to resolve, for example create new ML algorithms?

&amp;#x200B;

I don't know if this is possible, but what we would need to start do it? 

&amp;#x200B;

Crazy morning ideas :P",2,0,False,self,,,,,
1652,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,20,bh7jo8,openpr.com,"Machine Learning in Manufacturing Market: Development Plans, New Technology, Growth Factors (2019-2025)and Key players Analysis like Intel, IBM, Siemens, GE, Google, Microsoft and Micron Technology",https://www.reddit.com/r/MachineLearning/comments/bh7jo8/machine_learning_in_manufacturing_market/,oriantushar,1556192115,,0,1,False,default,,,,,
1653,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,20,bh7pdk,self.MachineLearning,[Discussion] Data collection from Youtube,https://www.reddit.com/r/MachineLearning/comments/bh7pdk/discussion_data_collection_from_youtube/,pk12_,1556193210,"I have noticed that many new datasets are based on data collected from Youtube videos. I am wondering what rules govern data collection from Youtube.

Does Youtube permit their data to be used for research purposes? Do people sign an agreement with Youtube prior to using it? Is it legal to use it? Are there ethics rules for this?

Has anyone of you thought about this, please share your views.",1,1,False,self,,,,,
1654,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,21,bh7slb,self.MachineLearning,Need help with a tiny ML/NLP Project!,https://www.reddit.com/r/MachineLearning/comments/bh7slb/need_help_with_a_tiny_mlnlp_project/,X3NOC1DE,1556193783,"I'm a novice at ML and NLP and am really good at python programming.

&amp;#x200B;

Ive got a dataset of recipes and another dataset of ingredients preferred by certain users.

&amp;#x200B;

Can someone please guide me on how I could create a system where I could suggest recipes to users based on their preferred ingredients - like I know how its supposed to work - you create a small tiny dataset for a particular user and store all his favourite stuff and then match the recipes from the recipe databse, but being a noob, I have no idea how to go about this project. Any help would be appreciated! Thank you!",0,1,False,self,,,,,
1655,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,21,bh7ydc,self.MachineLearning,[Q] when does udacity's tensorflow 2.0 free course begin?,https://www.reddit.com/r/MachineLearning/comments/bh7ydc/q_when_does_udacitys_tensorflow_20_free_course/,sjh9020,1556194817,[removed],0,1,False,self,,,,,
1656,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,22,bh8goy,self.MachineLearning,SVM knowledge level advice?,https://www.reddit.com/r/MachineLearning/comments/bh8goy/svm_knowledge_level_advice/,jbuddy_13,1556197918,[removed],0,1,False,self,,,,,
1657,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,22,bh8h1x,youtube.com,Fei-Fei Li &amp; Yuval Noah Harari in Conversation - The Coming AI Upheaval,https://www.reddit.com/r/MachineLearning/comments/bh8h1x/feifei_li_yuval_noah_harari_in_conversation_the/,Mandelmus100,1556197973,,0,1,False,default,,,,,
1658,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,22,bh8mqt,github.com,[P] Geomancer: Automated feature engineering for geospatial data,https://www.reddit.com/r/MachineLearning/comments/bh8mqt/p_geomancer_automated_feature_engineering_for/,ljvmiranda,1556198903,,1,2,False,default,,,,,
1659,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,22,bh8nff,youtu.be,[D] From Tesla Autonomy Day (Andrej Karpathy about NN applications in self-driving cars),https://www.reddit.com/r/MachineLearning/comments/bh8nff/d_from_tesla_autonomy_day_andrej_karpathy_about/,hooba_stank_,1556199009,,0,1,False,default,,,,,
1660,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,22,bh8ufv,onezero.medium.com,Machine Learning Widens the Gap Between Knowledge and Understanding,https://www.reddit.com/r/MachineLearning/comments/bh8ufv/machine_learning_widens_the_gap_between_knowledge/,xTWOz,1556200122,,0,1,False,default,,,,,
1661,MachineLearning,t5_2r3gv,2019-4-25,2019,4,25,23,bh99ie,arxiv.org,[R] Unsupervised Singing Voice Conversion,https://www.reddit.com/r/MachineLearning/comments/bh99ie/r_unsupervised_singing_voice_conversion/,xulinas,1556202425,,2,10,False,default,,,,,
1662,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,0,bh9zek,self.MachineLearning,Image recognition by a noob,https://www.reddit.com/r/MachineLearning/comments/bh9zek/image_recognition_by_a_noob/,DrPseud,1556206212,[removed],0,1,False,https://b.thumbs.redditmedia.com/l3C13lwB0a-hHh49XQQfev2EkLQMyjayP5UkQFkyeJE.jpg,,,,,
1663,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,0,bha4p7,self.MachineLearning,[D] LSTM Autoencoder Separating By Sequence Length Instead of Class Features,https://www.reddit.com/r/MachineLearning/comments/bha4p7/d_lstm_autoencoder_separating_by_sequence_length/,DataSciencePenguin,1556206975,"Hello,

&amp;#x200B;

I've built an LSTM autoencoder using Keras, similar to this tutorial: [https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html) . I use padding with a mask to incorporate sequences of different length. However, when I look at the TSNE plot of the reduced features, I see my points clustering more by sequence duration than by features.  Yes, within some larger clusters it does cluster within by interesting features, but the larger clusters are all by sequence length. Do you guys/gals have any suggestions to minimize the effect of sequence length on the encoded features?

&amp;#x200B;

Thanks a Billion!",6,3,False,self,,,,,
1664,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,0,bha5fp,self.MachineLearning,Q&amp;A with Google DeepMinds Jane Wang [R],https://www.reddit.com/r/MachineLearning/comments/bha5fp/qa_with_google_deepminds_jane_wang_r/,TechTyranny,1556207083,"Some insight into Google DeepMind's recent research and Jane's work connecting Neuroscience, AI and ML: [https://simpleweb.co.uk/qa-with-google-deepminds-jane-wang/](https://simpleweb.co.uk/qa-with-google-deepminds-jane-wang/)",0,3,False,self,,,,,
1665,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,0,bha8tn,inveritasoft.com,4 Ways to Improve Inventory Management Using AI,https://www.reddit.com/r/MachineLearning/comments/bha8tn/4_ways_to_improve_inventory_management_using_ai/,inveritasoft,1556207578,,0,1,False,default,,,,,
1666,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,0,bha9dj,self.MachineLearning,Inquiry about best deep learning courses using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/bha9dj/inquiry_about_best_deep_learning_courses_using/,arnavc,1556207658,[removed],0,1,False,self,,,,,
1667,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,0,bhabrb,self.MachineLearning,[D] Is anyone using synthetic data in production?,https://www.reddit.com/r/MachineLearning/comments/bhabrb/d_is_anyone_using_synthetic_data_in_production/,jamesonatfritz,1556207996,"Hi everyone, I've been increasingly impressed with results for training models (mostly computer vision tasks) on synthetic data. I noticed Google commented on using data from speech synthesis models to train the \[latest version of their on-device ASR model\]([https://arxiv.org/abs/1811.06621](https://arxiv.org/abs/1811.06621)).

&amp;#x200B;

\[I wrote a brief primer\]([https://heartbeat.fritz.ai/synthetic-data-a-bridge-over-the-data-moat-29f392a52f27](https://heartbeat.fritz.ai/synthetic-data-a-bridge-over-the-data-moat-29f392a52f27)) on some of the things I'm seeing as well as limitations / open questions and was wondering if anyone here has used synthetically generated data in production?",6,3,False,self,,,,,
1668,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,1,bhadsp,youtube.com,[D] Oriol Vinyals - The State of Deep Reinforcement Learning (talk delivered at a National Academy of Sciences colloquium),https://www.reddit.com/r/MachineLearning/comments/bhadsp/d_oriol_vinyals_the_state_of_deep_reinforcement/,cturkosi,1556208274,,0,1,False,default,,,,,
1669,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,1,bhai0w,self.MachineLearning,[D] State of the art for high-dimensional density estimation,https://www.reddit.com/r/MachineLearning/comments/bhai0w/d_state_of_the_art_for_highdimensional_density/,IborkedyourGPU,1556208866,"It's been a while since I worked on high-dimensional density estimation, and I need to get back in the game. Which is the current state of the art for this problem? Is it the [Autoregressive Energy Machine](https://arxiv.org/abs/1904.05626)? And what's the tool you commonly use when you have to estimate an high-dimensional density?",15,11,False,self,,,,,
1670,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,1,bhanzl,arxiv.org,[R] Low-Memory Neural Network Training: A Technical Report,https://www.reddit.com/r/MachineLearning/comments/bhanzl/r_lowmemory_neural_network_training_a_technical/,nimit_s,1556209722,,1,10,False,default,,,,,
1671,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,1,bhatuh,youtube.com,ARBOLES DE DECISIN LOGISTICOS convertidos en RED NEURONAL,https://www.reddit.com/r/MachineLearning/comments/bhatuh/arboles_de_decisin_logisticos_convertidos_en_red/,Xpikuos,1556210550,,0,1,False,https://b.thumbs.redditmedia.com/YJLfAbBqjFFINII-NrZhS2PwiukwXMiI-Bcl0XaX6to.jpg,,,,,
1672,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,1,bhav6m,self.MachineLearning,[D] Compensation for research roles in US?,https://www.reddit.com/r/MachineLearning/comments/bhav6m/d_compensation_for_research_roles_in_us/,pomclm11,1556210743,"Background: 5th year phd student in ML at a top 20 university in the US. I think I have a decent publication record (~5 publications at top venues), but not many citations (&lt;100).

I was fortunate to receive several offers for research scientist positions at a few companies. Some are within actual research groups at big tech companies, while others have the title ""research scientist"" but are more data scientist roles (think finance, but not trading).

Without divulging too much information, my offers from the big tech companies were lower than expected, at around 200K total comp (130K base, rest in stock + sign-on). In contrast my best offer from a finance company is around 300K. 

Right now I am still leaning towards the big tech positions since I'll be able to publish, but I feel like I am getting low-balled. I've heard of phds getting well over 300K for similar positions a few years ago, albeit with slightly stronger publication records. Is it unreasonable for me to ask for more?",23,21,False,self,,,,,
1673,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,1,bhavkw,self.MachineLearning,Installing Tensorflow,https://www.reddit.com/r/MachineLearning/comments/bhavkw/installing_tensorflow/,roachylarLFC,1556210798,[removed],0,1,False,self,,,,,
1674,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,1,bhay8l,medium.com,Introducing fklearn: Nubanks machine learning library based on functional programming,https://www.reddit.com/r/MachineLearning/comments/bhay8l/introducing_fklearn_nubanks_machine_learning/,fuadsaud,1556211166,,0,1,False,default,,,,,
1675,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,2,bhb4ds,openai.com,[N] MuseNet by OpenAI,https://www.reddit.com/r/MachineLearning/comments/bhb4ds/n_musenet_by_openai/,wavelander,1556212024,,54,364,False,https://a.thumbs.redditmedia.com/JmJmF01sxT3uE0DAIYHUKxKuhaWyPZCvVhMaOE-BYO8.jpg,,,,,
1676,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,2,bhbm5b,self.MachineLearning,Forcing an AI to websites,https://www.reddit.com/r/MachineLearning/comments/bhbm5b/forcing_an_ai_to_websites/,roachylarLFC,1556214533,[removed],0,1,False,self,,,,,
1677,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,2,bhbnib,self.MachineLearning,Introducing fklearn: Nubanks machine learning library,https://www.reddit.com/r/MachineLearning/comments/bhbnib/introducing_fklearn_nubanks_machine_learning/,julianogarcia,1556214733,[removed],0,1,False,self,,,,,
1678,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,2,bhbpgc,opendatascience.com,[Project] Properly setting the random seed in ML experiments,https://www.reddit.com/r/MachineLearning/comments/bhbpgc/project_properly_setting_the_random_seed_in_ml/,ceceshao1,1556215029,,0,1,False,default,,,,,
1679,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,2,bhbq6c,self.MachineLearning,Forcing an AI to read websites,https://www.reddit.com/r/MachineLearning/comments/bhbq6c/forcing_an_ai_to_read_websites/,roachylarLFC,1556215142,[removed],0,1,False,self,,,,,
1680,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,3,bhc1pb,self.MachineLearning,Linear algebra for ML,https://www.reddit.com/r/MachineLearning/comments/bhc1pb/linear_algebra_for_ml/,flash_match,1556216785,[removed],0,1,False,self,,,,,
1681,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,3,bhc54g,self.MachineLearning,Grad School - Biology + machine learning,https://www.reddit.com/r/MachineLearning/comments/bhc54g/grad_school_biology_machine_learning/,bak3rm3,1556217257,"Hi all, 

I wanted to get your guys input on the most recent biology related machine learning topics. I would like to plan out my graduate studies and want to look at programs that offer some sort of machine learning aspect to the research in the bio/health field. I know that medicine right now is applying machine learning in discovering molecules that would fit structural features desired. Any other areas where it could be applied and if anyone has experience in the field please let me know!

Thanks",0,1,False,self,,,,,
1682,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhcgpo,medium.com,Close Your Eyes and Ill Scan You: Chinese Face Payment System Vulnerabilities,https://www.reddit.com/r/MachineLearning/comments/bhcgpo/close_your_eyes_and_ill_scan_you_chinese_face/,gwen0927,1556218932,,0,1,False,default,,,,,
1683,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhck3m,self.MachineLearning,Deep Networks based on clustering instead of backpropagation?,https://www.reddit.com/r/MachineLearning/comments/bhck3m/deep_networks_based_on_clustering_instead_of/,nopx902,1556219415,[removed],0,2,False,self,,,,,
1684,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhck4z,self.MachineLearning,[Project] Research Project for Recognizing Dog Breeds through NLP,https://www.reddit.com/r/MachineLearning/comments/bhck4z/project_research_project_for_recognizing_dog/,throwaway50509898,1556219420,"Hi guys, I'm part of a research group trying to use Natural Language Processing to recognize dog breeds based on a verbal input. However, to train our model, we need to build a dataset, so I would really appreciate it if y'all could fill out this google form:

[https://docs.google.com/forms/d/e/1FAIpQLSdTATkD1Ulw-zjshzIQIt6bbmnL1UvfHHprnK3JJrAnx98YgA/viewform](https://docs.google.com/forms/d/e/1FAIpQLSdTATkD1Ulw-zjshzIQIt6bbmnL1UvfHHprnK3JJrAnx98YgA/viewform)

Thanks!",2,2,False,self,,,,,
1685,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhcq5b,self.MachineLearning,[D] Tools for data validation/visualizing missing data,https://www.reddit.com/r/MachineLearning/comments/bhcq5b/d_tools_for_data_validationvisualizing_missing/,bastardOfYoung94,1556220290,"I just stumbled across this enterprise product h2o ai, and while I'm not really too interested in their auto-ai solutions, it does look like they have some nice tools to visualize datasets, outliers, and missing values.

For those who work with their own datasets, I was wondering what tools you all use to accomplish this? I suppose I could build my own, but if there's existing (preferably free) platforms to do something similar, that'd be awesome.",5,4,False,self,,,,,
1686,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhcvpi,expoundai.wordpress.com,Image Data Generators in Keras,https://www.reddit.com/r/MachineLearning/comments/bhcvpi/image_data_generators_in_keras/,msminhas93,1556221108,,0,1,False,default,,,,,
1687,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhcwrw,self.MachineLearning,problems when increasing training set size,https://www.reddit.com/r/MachineLearning/comments/bhcwrw/problems_when_increasing_training_set_size/,vaaalbara,1556221274,[removed],0,1,False,self,,,,,
1688,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhcz1p,self.MachineLearning,Dumb question...?,https://www.reddit.com/r/MachineLearning/comments/bhcz1p/dumb_question/,JKolodne,1556221616,[removed],0,1,False,self,,,,,
1689,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhd2ey,playspeedgate.org,"[N] This Is Speedgate, the world's first sport imagined by AI",https://www.reddit.com/r/MachineLearning/comments/bhd2ey/n_this_is_speedgate_the_worlds_first_sport/,luiscosio,1556222096,,0,1,False,default,,,,,
1690,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,4,bhd4h5,openai.com,[N] OpenAI's MuseNet - deep neural network that can generate compositions with 10 different instruments,https://www.reddit.com/r/MachineLearning/comments/bhd4h5/n_openais_musenet_deep_neural_network_that_can/,luiscosio,1556222398,,0,1,False,https://a.thumbs.redditmedia.com/JmJmF01sxT3uE0DAIYHUKxKuhaWyPZCvVhMaOE-BYO8.jpg,,,,,
1691,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,5,bhd9a0,zerotosingularity.com,A cheeky no code deep learning experiment for computer vision,https://www.reddit.com/r/MachineLearning/comments/bhd9a0/a_cheeky_no_code_deep_learning_experiment_for/,janvandepoel,1556223090,,0,1,False,default,,,,,
1692,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,5,bhddhk,self.MachineLearning,[P] This video shows you how to build a NN for forecasting stock prices,https://www.reddit.com/r/MachineLearning/comments/bhddhk/p_this_video_shows_you_how_to_build_a_nn_for/,vol_trader,1556223694,"I built a Neural Network model (using Python and TensorFlow) for forecasting stock prices in 2016 and used the algorithm in my hedge fund in 2017. I made a video which provides a step-by-step walk through for how to create, train, and evaluate the model for the the purpose of generating buy and sell signals for financial securities.

The methodology should be very useful for anyone looking to build something like this.

This example utilizes a four layer neural network for trading volatility ETPs but can be easily modified to include other securities and metrics as predictor inputs.",0,1,False,self,,,,,
1693,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,5,bhdhr5,youtube.com,[P] This video shows you how to build a NN for forecasting stock prices,https://www.reddit.com/r/MachineLearning/comments/bhdhr5/p_this_video_shows_you_how_to_build_a_nn_for/,vol_trader,1556224324,,0,1,False,https://b.thumbs.redditmedia.com/Rcj6iJy_8Uj5p8l4K3kSVB8-X2ny5NvAoX2vlXg6VZs.jpg,,,,,
1694,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,5,bhdl93,self.MachineLearning,Memory Aware Synapses (MAS): how to compute additional loss?,https://www.reddit.com/r/MachineLearning/comments/bhdl93/memory_aware_synapses_mas_how_to_compute/,Sp4rk4s,1556224784,"I am currently reading the paper ""Memory Aware Synapses: Learning what (not) to forget"" ([https://arxiv.org/abs/1711.09601](https://arxiv.org/abs/1711.09601)) and am trying to figure out how to compute the additional loss term. The weight importance matrix  of the current parameters  is (as I understand) just the gradients of all individual weights. But how is ( - \^(\*)) computed, specifically (\^(\*)). I tried looking through the official git repository ([https://github.com/rahafaljundi/MAS-Memory-Aware-Synapses](https://github.com/rahafaljundi/MAS-Memory-Aware-Synapses)) but was somehow not able to find the answer. Does anyone have experience with this approach?",0,1,False,self,,,,,
1695,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,5,bhdmet,self.MachineLearning,[P] This video shows you how to build a NN for forecasting stock prices,https://www.reddit.com/r/MachineLearning/comments/bhdmet/p_this_video_shows_you_how_to_build_a_nn_for/,vol_trader,1556224951,"I built a Neural Network model (using Python and TensorFlow) for forecasting stock prices in 2016 and used the algorithm in my hedge fund in 2017. I made a [video](https://www.youtube.com/watch?v=w1xXI4-l1D8) which provides a step-by-step walk through for how to create, train, and evaluate the model for the the purpose of generating buy and sell signals for financial securities.

The methodology should be very useful for anyone looking to build something like this.

This example utilizes a four layer neural network for trading volatility ETPs but can be easily modified to include other securities and metrics as predictor inputs.",9,4,False,self,,,,,
1696,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,6,bhegbd,self.MachineLearning,Why (so many) humans would fail the Turing Test,https://www.reddit.com/r/MachineLearning/comments/bhegbd/why_so_many_humans_would_fail_the_turing_test/,NiklasHageback,1556229409,[removed],0,1,False,self,,,,,
1697,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,6,bheh7d,karpathy.github.io,A Recipe for Training Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bheh7d/a_recipe_for_training_neural_networks/,arthomas73,1556229539,,0,1,False,default,,,,,
1698,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,6,bhehfw,karpathy.github.io,[R] Andrej Karpathy - Recipe for Training Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bhehfw/r_andrej_karpathy_recipe_for_training_neural/,yazriel0,1556229575,,1,1,False,default,,,,,
1699,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,7,bhekqi,karpathy.github.io,[R] A Recipe for Training Neural Networks - by Andrej Karpathy,https://www.reddit.com/r/MachineLearning/comments/bhekqi/r_a_recipe_for_training_neural_networks_by_andrej/,yazriel0,1556230056,,1,1,False,default,,,,,
1700,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,7,bhenys,self.MachineLearning,[D] A Recipe for Training Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bhenys/d_a_recipe_for_training_neural_networks/,sensetime,1556230548,"New article written by Andrej Karpathy distilling a bunch of useful heuristics for training neural nets. I think the blog post is full of the kind of real-world knowledge and how-to details that are not taught in books and often take endless hours to learn the hard way.

Have a look:

https://karpathy.github.io/2019/04/25/recipe/",22,301,False,self,,,,,
1701,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,7,bheuhl,self.MachineLearning,Why are networks with periodic activation functions harder to train?,https://www.reddit.com/r/MachineLearning/comments/bheuhl/why_are_networks_with_periodic_activation/,DVDplayr,1556231542,"I came across a lot of sources online which say that neural networks with periodic activation functions such as sinusoids do not want to converge. I understand that when a network is initialised with very small weights, a periodic function may act like a monotonic function and the periodicity of the function does not affect training. What happens when the initial weights are not small? What are the implications on gradient descent of the loss function in this case?",0,1,False,self,,,,,
1702,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,8,bhfcq6,self.MachineLearning,[N] Weekly Machine Learning Opensource Roundup,https://www.reddit.com/r/MachineLearning/comments/bhfcq6/n_weekly_machine_learning_opensource_roundup/,sytelus,1556234442, [https://blog.pocketcluster.io/category/index/](https://blog.pocketcluster.io/category/index/),0,3,False,self,,,,,
1703,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,8,bhfeb6,arxiv.org,[1904.07325] Characterizing the Variability in Face Recognition Accuracy Relative to Race,https://www.reddit.com/r/MachineLearning/comments/bhfeb6/190407325_characterizing_the_variability_in_face/,Difficult_Action,1556234707,,1,1,False,default,,,,,
1704,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,8,bhfhvi,self.MachineLearning,Learning Materials.,https://www.reddit.com/r/MachineLearning/comments/bhfhvi/learning_materials/,hichemobscure,1556235292,"Hello everyone, just got into AI and ML, started out with Microsoft Cloud Society Ai &amp; ML Course it was super fun and interesting course, the labs were so good and i learned a lot, any other materials suggestions?! Books?! Links?! Courses etc. All suggestions are welcome, any material that is related to ML. 
Excited to hear your suggestions.",0,1,False,self,,,,,
1705,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,8,bhfi5z,arxiv.org,Study urges reappraisal of alleged racial 'bias' in face recognition,https://www.reddit.com/r/MachineLearning/comments/bhfi5z/study_urges_reappraisal_of_alleged_racial_bias_in/,Difficult_Action,1556235337,,9,5,False,default,,,,,
1706,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,8,bhfk6b,self.MachineLearning,Build an android app (Machine Learning related) from python source code,https://www.reddit.com/r/MachineLearning/comments/bhfk6b/build_an_android_app_machine_learning_related/,Bicharro27,1556235669,"Hey guys, Ive been developping a project that envolves deep learning and computer vision and it's turning out to be quite an amazing result. Since it is related to real-time camera recording and processing, the next step would be to develop an app so it can run in an android cell. The source code is fully written in python and, though Java also has the libraries, it would cost me a lot of time to re-write all the code into Java code, and I'm better used to python's openCV. Any thoughts and advices on how I can build an app using the code I already have in python? I've heard of BeeWare and Kivy, but don't really know Which one's better and how to work with them yet. Thanks in advance!",0,1,False,self,,,,,
1707,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,8,bhfpxn,self.MachineLearning,[P] Bot Thoughts - GPT-2 generated Shower Thoughts,https://www.reddit.com/r/MachineLearning/comments/bhfpxn/p_bot_thoughts_gpt2_generated_shower_thoughts/,eukaryote31,1556236660,"I took GPT-2 and trained it on r/ShowerThoughts. The site has a static list of generated responses that it serves at random from. 

&amp;#x200B;

Hyperparameters: Adam w/ lr=1e-5, batch size 16, sample length 255. Trained for 4000 iterations. 

&amp;#x200B;

[https://eukaryote31.github.io/bot-thoughts/ ](https://eukaryote31.github.io/bot-thoughts/)",31,29,False,self,,,,,
1708,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,9,bhft5t,self.MachineLearning,How would stock price image recognition be different than creating features from prices.,https://www.reddit.com/r/MachineLearning/comments/bhft5t/how_would_stock_price_image_recognition_be/,jazzydat,1556237227,[removed],0,1,False,self,,,,,
1709,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,9,bhfvln,self.MachineLearning,Feedforward Network Created From Scratch Outputs Either All 0's or All 1's,https://www.reddit.com/r/MachineLearning/comments/bhfvln/feedforward_network_created_from_scratch_outputs/,Scazzer,1556237608,"I'm trying to create a basic feedforward neural network from scratch using this tutorial:

https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6

I'm inputting 10 features into the NN with float values normalised between 0 and 1. This then goes through one hidden layer with 64 neurones and then output either 0 or 1 (both using sigmoid activation function). However this will either output all 0's or all 1's. Any idea what the problem could be?",0,1,False,self,,,,,
1710,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,9,bhfxdm,news.developer.nvidia.com,Speedgate: Worlds First Sport Generated by AI,https://www.reddit.com/r/MachineLearning/comments/bhfxdm/speedgate_worlds_first_sport_generated_by_ai/,j_orshman,1556237901,,0,1,False,https://b.thumbs.redditmedia.com/sa5LRwodSafvIuLJINA5dE7bEFj4p_R-Zb1jwNbJk_M.jpg,,,,,
1711,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,10,bhgrvc,self.MachineLearning,"[D] Given a training set of real 3d scenes, would it be possible to accurately estimate the contents of a single voxel outside of the camera scene of a test 3d scene?",https://www.reddit.com/r/MachineLearning/comments/bhgrvc/d_given_a_training_set_of_real_3d_scenes_would_it/,Mjjjokes,1556243314,Voxel = 3d pixel,3,1,False,self,,,,,
1712,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,10,bhgtbh,arxiv.org,"""GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond"" from MSRA",https://www.reddit.com/r/MachineLearning/comments/bhgtbh/gcnet_nonlocal_networks_meet_squeezeexcitation/,yuecao,1556243582,,2,3,False,default,,,,,
1713,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,10,bhgusv,distill.pub,[N] A Visual Exploration of Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/bhgusv/n_a_visual_exploration_of_gaussian_processes/,CartPole,1556243854,,0,1,False,default,,,,,
1714,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,11,bhh1hd,self.MachineLearning,Why (so many) humans would fail the Turing Test,https://www.reddit.com/r/MachineLearning/comments/bhh1hd/why_so_many_humans_would_fail_the_turing_test/,NiklasHageback,1556245034,[removed],0,1,False,self,,,,,
1715,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,12,bhhkaq,self.MachineLearning,[D] Canonical version of Imagenet?,https://www.reddit.com/r/MachineLearning/comments/bhhkaq/d_canonical_version_of_imagenet/,yhszw,1556248415,"On academictorrents there's half a dozen different imagenets. Which one's the one people typically refer to as imagenet in papers? What is the relation between these sets? 

&amp;#x200B;

Some examples of the ones that I'm confused about:

* Imagenet Full (Fall 2011 release)
* ImageNet Large Scale Visual Recognition Challenge (V2017)
* ImageNet LSVRC 2012 Training Set (Object Detection)
* ImageNet LSVRC 2014 Training Set (Object Detection)
* ImageNet LSVRC 2012 Validation Set (Object Detection)
* ImageNet LSVRC 2013 Validation Set (Object Detection)",3,9,False,self,,,,,
1716,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,13,bhi2rl,self.MachineLearning,The correctness of a Machine Translation model,https://www.reddit.com/r/MachineLearning/comments/bhi2rl/the_correctness_of_a_machine_translation_model/,WiseWhizz,1556251936,[removed],0,1,False,self,,,,,
1717,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,13,bhifsa,arxiv.org,[R] Defensive Quantization: When Efficiency Meets Robustness,https://www.reddit.com/r/MachineLearning/comments/bhifsa/r_defensive_quantization_when_efficiency_meets/,PK_thundr,1556254568,,1,1,False,default,,,,,
1718,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,14,bhigz4,arxiv.org,[Self-Attention is All you Need for Vision] Local Relation Networks for Image Recognition,https://www.reddit.com/r/MachineLearning/comments/bhigz4/selfattention_is_all_you_need_for_vision_local/,ancientmooner,1556254827,,1,1,False,default,,,,,
1719,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,14,bhija8,self.MachineLearning,[D] Do recommender systems exist that work based on classification?,https://www.reddit.com/r/MachineLearning/comments/bhija8/d_do_recommender_systems_exist_that_work_based_on/,-Cunning-Stunt-,1556255303,"Hi, community!  
I am not very familiar with recommender systems and read in a couple of papers (and it makes sense) that recommender systems, especially cold start ones, can work based on classic ML classification algorithms. I am wondering if any state-of-the-art or relevant recsys's use classifiers. Paper links would be very appreciated.",2,1,False,self,,,,,
1720,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,14,bhiki6,arxiv.org,[R] Self-Attention is All you Need for Vision: Local Relation Networks for Image Recognition,https://www.reddit.com/r/MachineLearning/comments/bhiki6/r_selfattention_is_all_you_need_for_vision_local/,ancientmooner,1556255564,,14,47,False,default,,,,,
1721,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,14,bhil4p,self.deeplearning,How ro modify the generator of a GAN?,https://www.reddit.com/r/MachineLearning/comments/bhil4p/how_ro_modify_the_generator_of_a_gan/,gaurav__1998,1556255715,,0,1,False,default,,,,,
1722,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,14,bhipo0,arxiv.org,[R] (Beyond Bounding Box) RepPoints: Point Set Representation for Object Detection,https://www.reddit.com/r/MachineLearning/comments/bhipo0/r_beyond_bounding_box_reppoints_point_set/,ancientmooner,1556256690,,1,3,False,default,,,,,
1723,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,14,bhisqj,self.MachineLearning,[D] Memory Aware Synapses (MAS): how to compute additional loss?,https://www.reddit.com/r/MachineLearning/comments/bhisqj/d_memory_aware_synapses_mas_how_to_compute/,Sp4rk4s,1556257379,"I am currently reading the paper ""Memory Aware Synapses: Learning what (not) to forget"" ([https://arxiv.org/abs/1711.09601](https://arxiv.org/abs/1711.09601)) and am trying to figure out how to compute the additional loss term. The weight importance matrix  of the current parameters  is (as I understand) just the gradients of all individual weights. But how is ( - ^(\*)) computed, specifically (^(\*)). I tried looking through the official git repository, but was somehow not able to find the answer. Does anyone have experience with this approach?",0,1,False,self,,,,,
1724,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,16,bhjppi,hackernoon.com,10 Amazing Articles On Python Programming And Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bhjppi/10_amazing_articles_on_python_programming_and/,m_razali,1556265078,,0,1,False,default,,,,,
1725,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,17,bhjulg,self.MachineLearning,[D] From Tesla Autonomy Day (Andrej Karpathy about NN applications in self-driving cars),https://www.reddit.com/r/MachineLearning/comments/bhjulg/d_from_tesla_autonomy_day_andrej_karpathy_about/,hooba_stank_,1556266280,"[https://youtu.be/Ucp0TTmvqOE?t=6659](https://youtu.be/Ucp0TTmvqOE?t=6659)  


Introduction is quite basic, but there are some interesting insights from Andrej regarding their approach afterwards.",19,22,False,self,,,,,
1726,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,17,bhk0l7,arxiv.org,"Self-Attention in Vision: ""Non-local Networks Meet Squeeze-Excitation Networks and Beyond""",https://www.reddit.com/r/MachineLearning/comments/bhk0l7/selfattention_in_vision_nonlocal_networks_meet/,yuecao,1556267819,,0,1,False,default,,,,,
1727,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,17,bhk2xz,arxiv.org,"[R] A Study of Self-Attention in Vision: ""Non-local Networks Meet Squeeze-Excitation Networks and Beyond""",https://www.reddit.com/r/MachineLearning/comments/bhk2xz/r_a_study_of_selfattention_in_vision_nonlocal/,yuecao,1556268440,,11,9,False,default,,,,,
1728,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,18,bhkbgr,self.MachineLearning,[N] A low weight and high density AI/ML news site,https://www.reddit.com/r/MachineLearning/comments/bhkbgr/n_a_low_weight_and_high_density_aiml_news_site/,ai_jobs,1556270600,"It's a basic and simple (nothing fancy, no ads, no BS, good for on the go) way to stay up to date with current news in the AI world: [https://allainews.com/](https://allainews.com/)",10,32,False,self,,,,,
1729,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,18,bhkdjq,robotindia.com,Manufacturing Process of the Backhoe Loader,https://www.reddit.com/r/MachineLearning/comments/bhkdjq/manufacturing_process_of_the_backhoe_loader/,robotcomponents,1556271135,,0,1,False,default,,,,,
1730,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,19,bhknk7,technologyreview.com,MIT Tech Review: Robotic Catheter + Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bhknk7/mit_tech_review_robotic_catheter_machine_learning/,alteredillusion,1556273527,,0,1,False,default,,,,,
1731,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,20,bhl8bn,self.MachineLearning,User feedback analysis,https://www.reddit.com/r/MachineLearning/comments/bhl8bn/user_feedback_analysis/,ankudini,1556278107,[removed],0,1,False,self,,,,,
1732,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,20,bhlbp1,robotindia.com,https://robotindia.com/jcb-spare-parts-know-its-types-and-its-major-benefits/,https://www.reddit.com/r/MachineLearning/comments/bhlbp1/httpsrobotindiacomjcbsparepartsknowitstypesanditsm/,robotcomponents,1556278794,,0,1,False,default,,,,,
1733,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,21,bhljyy,self.MachineLearning,"Hardware, where should i put my monays",https://www.reddit.com/r/MachineLearning/comments/bhljyy/hardware_where_should_i_put_my_monays/,LEDNEWB,1556280498,[removed],0,1,False,self,,,,,
1734,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,21,bhlv9e,self.MachineLearning,Finding Repo to Make DeepFake Videos,https://www.reddit.com/r/MachineLearning/comments/bhlv9e/finding_repo_to_make_deepfake_videos/,the_coder_dot_py,1556282836,"Just saw this face augmentation deepfake stunt

https://www.youtube.com/watch?v=cQ54GDm1eL0

curious if anyone knows if there's a github repo on this, would like to try it out",0,1,False,self,,,,,
1735,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,21,bhlyp7,self.MachineLearning,[P] Creating a realistic deepfake with ML,https://www.reddit.com/r/MachineLearning/comments/bhlyp7/p_creating_a_realistic_deepfake_with_ml/,hanyuqn,1556283438,"This is a realistic deepfake I created for Ben Shapiro using my text-to-speech model to synthesise speech and my method for transferring facial expressions:

https://www.youtube.com/watch?v=sgkiS6wUZ98",3,8,False,self,,,,,
1736,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhlzyh,thedatascientist.com,What's the Issue With Sentiment Analysis?,https://www.reddit.com/r/MachineLearning/comments/bhlzyh/whats_the_issue_with_sentiment_analysis/,TheTesseractAcademy,1556283660,,0,1,False,default,,,,,
1737,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhm0si,self.MachineLearning,"[D] ""Everyone building machine learning products has a responsibility to understand that many users have misconceptions about the accuracy and 'objectivity' of ML""",https://www.reddit.com/r/MachineLearning/comments/bhm0si/d_everyone_building_machine_learning_products_has/,kirasolo,1556283811,"https://twitter.com/math_rachel/status/1121599589939683329

Rachel Thomas on Twitter has identified cases of misuse and abuse. The linked thread from her tweet includes examples worth sharing and being aware of",126,210,False,self,,,,,
1738,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhm3ol,self.MachineLearning,What are the coolest maps/cartographyy related ML papers and projects out there?,https://www.reddit.com/r/MachineLearning/comments/bhm3ol/what_are_the_coolest_mapscartographyy_related_ml/,DaScheuer,1556284281,"Most of the Deep Learning/ML applications I see that relate somehow to maps are almost always about driving, recognizing humans and other security-related stuff.

I once saw a video from Joma Tech where a quant talks about how he used setellite images to identify well depth to predict stock prices.

Do you know any similar applications or studies that used maps and cartography somehow?",0,1,False,self,,,,,
1739,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhm8cy,self.MachineLearning,Can I use trained models in different programming languages?,https://www.reddit.com/r/MachineLearning/comments/bhm8cy/can_i_use_trained_models_in_different_programming/,lbucas,1556285066,[removed],0,1,False,self,,,,,
1740,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhm90v,self.MachineLearning,"[Discussion] NLP, on numbers inside word embeddings",https://www.reddit.com/r/MachineLearning/comments/bhm90v/discussion_nlp_on_numbers_inside_word_embeddings/,OlgaPaints,1556285175,"Let me preface by saying that I am new to NLP, so it is very likely that a good solution to my problem already exists and in that case, I would really appreciate being pointed in the right direction :)

&amp;#x200B;

I am working on a machine reading comprehension task where the inputs often contain numbers in addition to words. I initially wanted to use pre-trained word embeddings, but I am not sure how the numerical data are represented when numbers are treated as words and are multiplied by an embedding matrix. What is worse, only numbers that occurred in the training set would have a representation, unless I am missing something.. I could extract numbers from sentences before putting the non-numerical words through an embedding layer, and treat them separately but it would be easier if a pre-trained word embedding layer(s) could take care of it all.

&amp;#x200B;

As far as I can tell, the optimal way to represent both numbers and words via embedding vectors would be to introduce two extra dimensions: one that would specify the type (1 for numerical vs 0 for vocab), and one that would contain a floating point representation of the original number. At the level of the embedding matrix, this suggests that the matrix can be put in a block-diagonal form, but if it is not, it should not be a problem - I figure the rest of the network would be able to learn that if the 1st component of the embedding vector for a certain word is 1, it should ignore all components but the second one, and vice versa.

&amp;#x200B;

This solution is similar to treating numerical and non-numerical data separately, but the advantage is that the pre-trained embedding takes care of it, and once you are past it, youve got an N-dimensional representation for every word in your sentence, including digits and numbers written in text, without loosing any information of those numbers.

&amp;#x200B;

I can go ahead and implement this, but as I have not seen this solution in existing projects (could very well be that I was not looking at the right place), I wonder if there are better ways of representing numbers + words in deep NNs. Any thoughts?",2,2,False,self,,,,,
1741,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhmatv,codeingschool.com,What is Logistic Regression in Machine Learning? How it Works?,https://www.reddit.com/r/MachineLearning/comments/bhmatv/what_is_logistic_regression_in_machine_learning/,subhamroy021,1556285472,,0,1,False,default,,,,,
1742,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhmc2c,self.MachineLearning,Thoughts on Jetson Nano vs PC with Nvidia 1070,https://www.reddit.com/r/MachineLearning/comments/bhmc2c/thoughts_on_jetson_nano_vs_pc_with_nvidia_1070/,LedByReason,1556285670,[removed],0,1,False,self,,,,,
1743,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhmgd4,self.MachineLearning,AI paper of the day,https://www.reddit.com/r/MachineLearning/comments/bhmgd4/ai_paper_of_the_day/,BettyWaihenya,1556286368,[removed],0,1,False,self,,,,,
1744,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhmj0x,self.MachineLearning,Variable input/output for speech recognition using RNNs,https://www.reddit.com/r/MachineLearning/comments/bhmj0x/variable_inputoutput_for_speech_recognition_using/,Cristofor66,1556286802,[removed],0,1,False,self,,,,,
1745,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,22,bhml33,self.MachineLearning,"How can I learn AI/ML in Java, Python, or JS WITHOUT Libraries?",https://www.reddit.com/r/MachineLearning/comments/bhml33/how_can_i_learn_aiml_in_java_python_or_js_without/,Ishaan_Singh_06,1556287127,"I understand that I need a good amount experience in Calculus, backpropagation, .etc. Where can I learn how to implement an ai in a game that I made with processing?",0,1,False,self,,,,,
1746,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,23,bhmok1,blog.ml.cmu.edu,Representer Point Selection for Explaining Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/bhmok1/representer_point_selection_for_explaining_deep/,robiriondo,1556287659,,0,1,False,default,,,,,
1747,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,23,bhmuda,self.MachineLearning,Is this a good project to apply ML/AI?,https://www.reddit.com/r/MachineLearning/comments/bhmuda/is_this_a_good_project_to_apply_mlai/,tashibum,1556288541,[removed],0,1,False,self,,,,,
1748,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,23,bhmy06,gamasutra.com,[P] Portfolio-Scale Machine Learning at Zynga,https://www.reddit.com/r/MachineLearning/comments/bhmy06/p_portfolioscale_machine_learning_at_zynga/,bweber,1556289093,,0,1,False,https://b.thumbs.redditmedia.com/jWUDJ-WvtkHAlDhwX-ckfAExrKQjJr9QYY067VMjeIA.jpg,,,,,
1749,MachineLearning,t5_2r3gv,2019-4-26,2019,4,26,23,bhn31z,i.redd.it,"In Tesla Autonomy Day live stream, what is this network?",https://www.reddit.com/r/MachineLearning/comments/bhn31z/in_tesla_autonomy_day_live_stream_what_is_this/,data-soup,1556289841,,1,1,False,default,,,,,
1750,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,0,bhnifg,self.MachineLearning,Best Cheap Pre-built PC,https://www.reddit.com/r/MachineLearning/comments/bhnifg/best_cheap_prebuilt_pc/,BombingPanda,1556292063,[removed],0,1,False,self,,,,,
1751,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,0,bhnp7j,self.MachineLearning,[D] What is your best practice to write long equations in two columns format,https://www.reddit.com/r/MachineLearning/comments/bhnp7j/d_what_is_your_best_practice_to_write_long/,pigdogsheep,1556293042,"I am currently writing a paper for two columns format which is kind of annoying because most of the equations look like that even I put them in  \`\\footnotesize\`

https://i.redd.it/qr66ad1fnmu21.png

&amp;#x200B;

The obvious solution to this problem is to break down the equation into several lines. This can become a bit ugly and hard to follow. I was wondering what is your to go preference considering this.",7,7,False,self,,,,,
1752,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,0,bhntzj,medium.com,Does AI Get the Joke? Researchers try to teach AI to learn humor,https://www.reddit.com/r/MachineLearning/comments/bhntzj/does_ai_get_the_joke_researchers_try_to_teach_ai/,Yuqing7,1556293730,,0,1,False,default,,,,,
1753,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,1,bho7r5,self.MachineLearning,[P] Tens of Thousands of GPT-2-Generated Hacker News Submission Titles,https://www.reddit.com/r/MachineLearning/comments/bho7r5/p_tens_of_thousands_of_gpt2generated_hacker_news/,minimaxir,1556295693,"[https://github.com/minimaxir/hacker-news-gpt-2](https://github.com/minimaxir/hacker-news-gpt-2)

The repo has more info on the methodology, but the results turned out *much* better than expected! (yes, I checked if the titles were real and they were not).

GPT-2 on small-form content has been working surprisingly well.",0,3,False,self,,,,,
1754,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,1,bho95s,kaggle.com,Looking to try a Kaggle competition? I made quick Kernel to get you started,https://www.reddit.com/r/MachineLearning/comments/bho95s/looking_to_try_a_kaggle_competition_i_made_quick/,0_marauders_0,1556295894,,0,1,False,default,,,,,
1755,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,1,bhobt5,self.MachineLearning,Encoding syntactic dependency paths / sequence of dependencies,https://www.reddit.com/r/MachineLearning/comments/bhobt5/encoding_syntactic_dependency_paths_sequence_of/,shamik-01,1556296281,"I am trying to find a way to encode dependency paths between two words in a set of sentences as in the following example:

`[WORD_1] - nsubjpass -&gt; xcomp -&gt; advcl -&gt; dobj -&gt;  compound  - [WORD_2]`  

`[WORD_1] - nmod -&gt; acl -&gt; dobj - [WORD_2]`

etc...

After that I want to use the paths as features for several classifiers.

Somebody knows ways to encode it? Im especially interested in ways to implement it in Sklearn.",0,1,False,self,,,,,
1756,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,1,bhoetc,self.MachineLearning,Giving OpenAI Five the capacity to adapt to the opponent,https://www.reddit.com/r/MachineLearning/comments/bhoetc/giving_openai_five_the_capacity_to_adapt_to_the/,thibo73800,1556296706,[removed],0,1,False,self,,,,,
1757,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,2,bhorwv,kaggle.com,If you're looking to try a Kaggle competition I made a quick guide to get you started! :),https://www.reddit.com/r/MachineLearning/comments/bhorwv/if_youre_looking_to_try_a_kaggle_competition_i/,0_marauders_0,1556298548,,0,1,False,https://b.thumbs.redditmedia.com/UjiAY-RjOVvV3z4RB1zWGz1isytt0TyrPzpgqKmhoCM.jpg,,,,,
1758,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,2,bhp17f,self.MachineLearning,[D] Machine Learning meets Functional Programming: Nubank open-sources ML library,https://www.reddit.com/r/MachineLearning/comments/bhp17f/d_machine_learning_meets_functional_programming/,gadjo95,1556299845,"Hi everyone, 

We just open-sourced our library we are using at Nubank (Fintech company in Latin America) for developing machine learning model. We are big fans of functional programming and applied a lot of concept from it to develop this library.

Machine Learning is frequently done by using object-oriented python code, and thats the way we used to do it at Nubank as well. Back then, the  process of building machine learning models and putting them into production was tiresome and often full of bugs. Wed deploy a model only to find that predictions made in production didnt match the ones seen  during validation. Whats more, validation was often impossible to  reproduce, frequently being done in stateful Jupyter Notebooks.

Functional programming helps fix these issues by:

* Making it easy to build pipelines where the data transformations that happen during training match the models in production.
* Allowing  for safer iteration in interactive environments (e.g. Jupyter  Notebooks), preventing mistakes caused by stateful code and making  research more reproducible.
* Allowing us to write very generic validation, tuning and feature selection code that works across model types and applications, making us more efficient overall.

&amp;#x200B;

You can have a longer description here: [https://medium.com/building-nubank/introducing-fklearn-nubanks-machine-learning-library-part-i-2a1c781035d0](https://medium.com/building-nubank/introducing-fklearn-nubanks-machine-learning-library-part-i-2a1c781035d0)

And the github is here: [https://github.com/nubank/fklearn](https://github.com/nubank/fklearn)

&amp;#x200B;

We dont expect fklearn to replace current standards in ML, but we hope it starts interesting conversations about the benefits of functional programming for Machine Learning.",14,21,False,self,,,,,
1759,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,2,bhp6bw,self.MachineLearning,[P] Can you solve this High School ML problem?,https://www.reddit.com/r/MachineLearning/comments/bhp6bw/p_can_you_solve_this_high_school_ml_problem/,LittleLouis,1556300565,"As a high school senior I have the task of coming up with a way to reorganize my school's parking lot in order to make the traffic flow more efficient.

[Here](https://i.imgur.com/gixLKWh.png) is a picture of the parking lot with labels. From 7:00am to 7:50am there is a high surge of traffic which flows through aisles 4, 5, 6, and 7. Using something like NEAT (any other ideas?), I would give a neural net the control of opening/closing gates 1-7, gates X and Y, as well as the exact times that the crossing guards allow traffic to pass. Using these variables I'm trying to let an AI improve the rate of traffic flow. Although I'm not too familiar with Tensorflow, that's what I'm using. Should I implement a neural net with Keras, or something else, and maybe use a totally different method too?

I appreciate any help.",8,4,False,self,,,,,
1760,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,2,bhp8ta,self.MachineLearning,[D] Giving OpenAI Five the capacity to adapt to the opponent,https://www.reddit.com/r/MachineLearning/comments/bhp8ta/d_giving_openai_five_the_capacity_to_adapt_to_the/,thibo73800,1556300909,"Hi,

&amp;#x200B;

Because it takes 45,000 years of gameplay experience to train OAI5, changing the behavior of the AI  for each player opponent is currently not possible. The thing is that the AI try to learn the best way to play and to win overall rather than trying to win against a particular opponent, this approach already give amazing results but I would like to consider what would it takes to give the agents the capability to change their behaviors according to the opponent player.

&amp;#x200B;

I have one suggestion for this and would love to get your feedback about it. I was thinking about trying to come up during the game with a latent space that aims to encode the opponent behavior. Thus, each agent would take actions not only from the observation of the world but also from the latent space encoding the behavior of the opponent. We can think of this additional observation as the latent space of one auto-encoder that gives an encoded vector representation of the inputs.

&amp;#x200B;

Train such representation could be done by a self-supervised manner. During the training, each agent would have an additional LSTM that try to predict what is the next actions the opponent in the observation space is up to do. Once this LSTM starts to be trained properly, the inner representation of the LSTM would encode in some way the behavior of the opponent's agents observed by each agent. Because each agent perceived different observations during the game, the 5 LSTM inner states can be combined and used as an additional input for each agent. This combined representation would encode the behavior of the opponent team overall. This representation would be similar to the way a Human team can communicate and adapt about the other team during the game.

&amp;#x200B;

Thus, the observation is no longer the observation each agent is perceiving around.  But each agent is now taking action with respect to the opponent behavior. That would give OAI5 the capacity to adapt to the opponent during the game. But also to reuse the same representation if the same game is run against the same opponent.

&amp;#x200B;

Do you think this proposition make sense? Also, I would be interested to know your propositions and what are for you the best papers that might bring a solution to this problem for the future of the field?

&amp;#x200B;

Thanks",3,1,False,self,,,,,
1761,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,2,bhpbtk,self.MachineLearning,Numenta now live-streaming research meetings,https://www.reddit.com/r/MachineLearning/comments/bhpbtk/numenta_now_livestreaming_research_meetings/,rhyolight,1556301357,[removed],0,1,False,self,,,,,
1762,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,3,bhpecz,self.MachineLearning,What is machine learning? Rethink the concept and the eye-catching applications,https://www.reddit.com/r/MachineLearning/comments/bhpecz/what_is_machine_learning_rethink_the_concept_and/,iramirsina,1556301715,[removed],0,1,False,self,,,,,
1763,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,3,bhpqs7,self.MachineLearning,[P] Feedback on research problem,https://www.reddit.com/r/MachineLearning/comments/bhpqs7/p_feedback_on_research_problem/,jpleitao,1556303508,"Hi everyone,

I have been working on a home energy management problem, in which I want to schedule a series of domestic appliances over a period of time in a way that minimizes overall electricity costs. As not all domestic appliances are controllable, fixed household demands need to be taken into account.

As a preliminary exploration of this problem, I was able to schedule appliance operations by solving an instance of a job scheduling problem, although assuming that those fixed demands are known throughout the scheduling horizon.

Naturally, this is an unrealistic approach, as we cannot perfectly forecast the future. As such, I am thinking of considering a stochastic optimization approach, generating several load profiles (scenarios) over the fixed horizon. Then I would solve an optimization problem instance that minimizes average costs over all generated scenarios.

Does this approach seems viable? Is there anything seriously wrong with this?

Furthermore, i was considering applying Gaussian Processes to forecast loads over the scheduling horizon. The thing that attracted me to this technique is the fact that we can get a distribution for predictions, rather than simply the values. Thus, I could use this to generate the different load scenarios for stochastic optimization. However, I am unsure if this is an adequate fit for time series forecasting, particularly when forecasting multiple time instants. I did (admittedly not very extensive) some research on time series forecasting using Gaussian Processes and some authors report quite poor results with this technique, stating that GPs are not very well suited for multi-step forecasting.

Can anyone give some feedback on these ideas? Are there alternative approaches (perhaps best suited for my problem)?

Thanks in advance",7,4,False,self,,,,,
1764,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,3,bhpx11,self.MachineLearning,What could/should contain a good BSc thesis on Sentiment Analysis with Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/bhpx11/what_couldshould_contain_a_good_bsc_thesis_on/,the_parallax_II,1556304415,[removed],0,1,False,self,,,,,
1765,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,3,bhq0zw,self.MachineLearning,Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations,https://www.reddit.com/r/MachineLearning/comments/bhq0zw/challenging_common_assumptions_in_the/,yaroslavvb,1556305014,"Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations

[https://arxiv.org/abs/1811.12359](https://arxiv.org/abs/1811.12359)",1,1,False,self,,,,,
1766,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,4,bhq6zq,self.MachineLearning,[D] Im writing a book: Neural Networks with Swift for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/bhq6zq/d_im_writing_a_book_neural_networks_with_swift/,rahulbhalley,1556305880,"Hi everybody! I am writing this book with Apress. So I thought I should just post about it here on Reddit (I am not sure if r/swift would be a better place) so I came here. 

I would like to know what do you all think about this relatively a new deep learning language (Swift for TensorFlow). It can use nearly any Python libraries right in Swift. 

And also if you want me to cover some important topics about neural networks or Swift just comment below.",38,38,False,self,,,,,
1767,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,4,bhqbix,youtu.be,[R] Human Scale Biped Robot Walking via a Neural Network (first ever),https://www.reddit.com/r/MachineLearning/comments/bhqbix/r_human_scale_biped_robot_walking_via_a_neural/,p-morais,1556306555,,0,1,False,https://b.thumbs.redditmedia.com/P2t9DLUEcuRPp9fFoLekk2J6_TwRHVmjWcOyWSgr9TM.jpg,,,,,
1768,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,4,bhqggw,medium.com,Benchmarks of Amazon's TPU units on various architectures.,https://www.reddit.com/r/MachineLearning/comments/bhqggw/benchmarks_of_amazons_tpu_units_on_various/,alex_titanovo,1556307281,,0,1,False,default,,,,,
1769,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,4,bhqhcd,self.MachineLearning,[R] Real Human Scale Biped Robot Walking via Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bhqhcd/r_real_human_scale_biped_robot_walking_via/,p-morais,1556307419,"To the best of our knowledge this is the first time a neural network policy has produced stable 3D walking in a human scale biped robot. 

Video link: https://www.youtube.com/watch?v=TgFrcrARao0

Arxiv link: https://arxiv.org/abs/1903.09537

Abstract: Deep reinforcement learning (DRL) is a promising approach for developing legged locomotion skills. However, the iterative design process that is inevitable in practice is poorly supported by the default methodology. It is difficult to predict the outcomes of changes made to the reward functions, policy architectures, and the set of tasks being trained on. In this paper, we propose a practical method that allows the reward function to be fully redefined on each successive design iteration while limiting the deviation from the previous iteration. We characterize policies via sets of Deterministic Action Stochastic State (DASS) tuples, which represent the deterministic policy state-action pairs as sampled from the states visited by the trained stochastic policy. New policies are trained using a policy gradient algorithm which then mixes RL-based policy gradients with gradient updates defined by the DASS tuples. The tuples also allow for robust policy distillation to new network architectures. We demonstrate the effectiveness of this iterative-design approach on the bipedal robot Cassie, achieving stable walking with different gait styles at various speeds. We demonstrate the successful transfer of policies learned in simulation to the physical robot without any dynamics randomization, and that variable-speed walking policies for the physical robot can be represented by a small dataset of 5-10k tuples.

I'll try my best to answer any questions.",12,45,False,self,,,,,
1770,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,5,bhr0ex,self.MachineLearning,"Machine Learning engineers, what is your workspace like?",https://www.reddit.com/r/MachineLearning/comments/bhr0ex/machine_learning_engineers_what_is_your_workspace/,spoiltForChoice,1556310305,[removed],0,1,False,self,,,,,
1771,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,5,bhr18e,self.MachineLearning,Sentiment analysis pretrained models,https://www.reddit.com/r/MachineLearning/comments/bhr18e/sentiment_analysis_pretrained_models/,medoos,1556310425,[removed],0,1,False,self,,,,,
1772,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,5,bhr2bq,arxiv.org,[R] Mining Rules Incrementally over Large Knowledge Bases,https://www.reddit.com/r/MachineLearning/comments/bhr2bq/r_mining_rules_incrementally_over_large_knowledge/,htrp,1556310587,,1,5,False,default,,,,,
1773,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,5,bhr53p,self.MachineLearning,[R] Neural Samplers and Hierarchical Variational Inference,https://www.reddit.com/r/MachineLearning/comments/bhr53p/r_neural_samplers_and_hierarchical_variational/,asobolev,1556311021,"Hi there!

Those of you interested in VAEs and/or their applications probably know that having an expressive family of variational approximations is important. I wrote a [small post on using Neural Samplers](http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html) as variational approximations in preparation for the upcoming release of my own work on the topic, hope that'd be useful!",4,10,False,self,,,,,
1774,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,5,bhrbsi,self.MachineLearning,[D] How to contribute to the development and/or research about TPU?,https://www.reddit.com/r/MachineLearning/comments/bhrbsi/d_how_to_contribute_to_the_development_andor/,HigherTopoi,1556312060,"I'm recently into studying about deep learning accelerators, and the one I'm most interested in is TPU v3. Given that faster multiplication of much larger matrices is crucial for further development of the cutting edge generative models (GPT-2, Sparse Transformer), MAC bandwidth is becoming a severe bottleneck to the speed. To resolve this issue, I'd like to contribute to the development of TPU at various scales. However, given the scarcity of the publicly available documents of TPU v3 and its ongoing research, I have no idea what their research/development group considers to be the current bottleneck to their project. What can I do?",6,3,False,self,,,,,
1775,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,6,bhrpg4,self.MachineLearning,[D] What your favorite / the best GAN image generator implementation currently available on github?,https://www.reddit.com/r/MachineLearning/comments/bhrpg4/d_what_your_favorite_the_best_gan_image_generator/,sirkloda,1556314179,"I just wanted to have an overview of the new stuff since I've been out of the loop for a while. Are there any github projects that implement the GAN and upscaling etc yet? Which one produces the most pleasant, hidef images these days? Thanks!",4,2,False,self,,,,,
1776,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,6,bhs011,self.MachineLearning,GPU on Azure Databricks?,https://www.reddit.com/r/MachineLearning/comments/bhs011/gpu_on_azure_databricks/,ROGvegeta,1556315882,[removed],0,1,False,self,,,,,
1777,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,7,bhs5p4,self.MachineLearning,[D] Evaluate if a Sentence Makes Sense (Grammar and Content),https://www.reddit.com/r/MachineLearning/comments/bhs5p4/d_evaluate_if_a_sentence_makes_sense_grammar_and/,AnonMLstudent,1556316760,"Let's say I have a chatbot that can generate random sentences (and hence we have no ground truth to compare to) like:

1. ""I like the weather today because it is very sunny and bright"".

2. ""I like the cat today because it is rainy"".

3. ""I like cat yesterday it is because"".

What are the best methods to evaluate if each sentence makes sense based on both its grammar and/or content? Example 1. above would make the most sense in terms of content and grammar, 2. makes grammatical sense but not in terms of content (makes no sense to like a cat because it is rainy), and 3. above has horrible grammar.

The issue here is these are randomly generated sentences with no ground truth to compare to (unlike, for example, machine translation where you have an expected translation and hence can use BLEU or perplexity).",4,2,False,self,,,,,
1778,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,10,bhtzob,youtube.com,How to Create Machine Learning Models Without Code (AutoML),https://www.reddit.com/r/MachineLearning/comments/bhtzob/how_to_create_machine_learning_models_without/,tim_macgyver,1556328122,,0,1,False,https://b.thumbs.redditmedia.com/2ICexwXV9Bt1lRet_NUXtcA0fN5RnYjtYVZkny6Rk-s.jpg,,,,,
1779,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,10,bhuab4,self.MachineLearning,[D] Invitation to join anti AI-hype/misunderstanding effort Skynet Today,https://www.reddit.com/r/MachineLearning/comments/bhuab4/d_invitation_to_join_anti_aihypemisunderstanding/,regalalgorithm,1556330150,"Hi all,

&amp;#x200B;

Hope this is not considered spammy, genuinely think it's of interest to the community of this subreddit. For some context, I am Andrey Kurenkov, a PhD at Stanford. For a while now I've been running this thing called [Skynet Today](http://www.skynettoday.com/), with the mission of ""Putting AI News In Perspective"" or in other words debunking inaccurate portrayals of AI research in media. As many people here are researchers and feel annoyed at hype/misconceptions about AI, I wonder if any of you might want to join our effort (we are basically a rag tag group of grad students pulling this together in spare time). If interested, please consider taking a look at our [ join ](https://andreykurenkov.us12.list-manage.com/track/click?u=4fd9f0f18da6483fe39140533&amp;id=376019f528&amp;e=7918e33cec)or just fill out [ our contribution survey](https://andreykurenkov.us12.list-manage.com/track/click?u=4fd9f0f18da6483fe39140533&amp;id=8d30d158db&amp;e=7918e33cec) directly, or just message me. Thanks! 

&amp;#x200B;

TLDR: I run a site to debunk misperceptions of AI news, pls [join ](https://andreykurenkov.us12.list-manage.com/track/click?u=4fd9f0f18da6483fe39140533&amp;id=376019f528&amp;e=7918e33cec) if you wanna help",73,295,False,self,,,,,
1780,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,11,bhuttg,self.MachineLearning,y Supersymmetric Artificial Neural Network model was accepted to a String theory conference,https://www.reddit.com/r/MachineLearning/comments/bhuttg/y_supersymmetric_artificial_neural_network_model/,ProgrammingGodJordan,1556333980,[removed],0,1,False,self,,,,,
1781,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,12,bhv0uq,self.MachineLearning,My Supersymmetric Artificial Neural Network model was accepted to a String theory conference,https://www.reddit.com/r/MachineLearning/comments/bhv0uq/my_supersymmetric_artificial_neural_network_model/,ProgrammingGodJordan,1556335410,[removed],0,1,False,self,,,,,
1782,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,12,bhv529,community.singularitynet.io,My Supersymmetric Artificial Neural Network model was accepted to a String theory conference,https://www.reddit.com/r/MachineLearning/comments/bhv529/my_supersymmetric_artificial_neural_network_model/,ProgrammingGodJordan,1556336306,,0,1,False,default,,,,,
1783,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,12,bhv75t,self.deeplearning,Interacting with the latent space of an AutoEncoder,https://www.reddit.com/r/MachineLearning/comments/bhv75t/interacting_with_the_latent_space_of_an/,HeavyStatus4,1556336747,,0,1,False,default,,,,,
1784,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,12,bhv8vw,liveletlearn.com,What is Learning? A paradigm,https://www.reddit.com/r/MachineLearning/comments/bhv8vw/what_is_learning_a_paradigm/,WillJet,1556337120,,0,1,False,default,,,,,
1785,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,12,bhv9b4,community.singularitynet.io,A smart physics persons review of my Supersymmetric Artificial Neural Network,https://www.reddit.com/r/MachineLearning/comments/bhv9b4/a_smart_physics_persons_review_of_my/,ProgrammingGodJordan,1556337203,,0,1,False,default,,,,,
1786,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,13,bhvg4p,self.MachineLearning,Differentiating AI Inference Accelerator Chips,https://www.reddit.com/r/MachineLearning/comments/bhvg4p/differentiating_ai_inference_accelerator_chips/,alghar,1556338665,[removed],0,1,False,self,,,,,
1787,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,13,bhvmcg,self.MachineLearning,Regulatory Adversarial Attacks - Legal Consequences (SSRN),https://www.reddit.com/r/MachineLearning/comments/bhvmcg/regulatory_adversarial_attacks_legal_consequences/,OppositeMidnight,1556340050,"&amp;#x200B;

# Regulatory Response to Adversarial Agents

...  in the future a publicly known regulatory AI model can be fooled into  believing that non-compliance is compliance from perfectly machine  massaged data in an adversarial attack. This would lead to battles  between automated systems, where one system would update to fill a  loophole as the other systems seeks another way to represent the data so  as to identify another loophole. Adversarial agents are going to  attempt this in two ways: first, they will try and simulate automated  regulatory ML systems using data available to them. When data is not  available to them, they can seek to reverse engineer the automated  regulatory system by creating fraudulently flagged instances and  receiving feedback. In this way they can discover model decision  boundaries and probability thresholds.

It  is important to understand that the agent does not need regulators  exact data. Instead the agent simply needs the public record of all  companies that have and have not been flagged for a certain regulatory  issue and then retrofit any correlative data they might have for these  companies. If the model achieves great success on an out of sample  dataset then this model can essentially be used to decipher correlated  features with the proprietary regulatory data. Such strategies have  been observed in other areas. For example, Moodys and S&amp;P rating  agencies can be reasonably successfully reverse engineered using  publicly available data and ML models. ML models have been successful at  replicating bank analysts and credit rating agencies using publicly  sourced data.40 Here is an explanation of a possible future scenario. In  a standard ML model, managerial or financial accounting data can be fed  into the model with data labelled as fraudulent and non- fraudulent.  Future observations can then be fed into this system to predict the  likelihood of fraud. With a GAN model, instead of company accountants  playing around with financial data to ensure that the predicted level of  fraud remains below regulated thresholds of materiality and/or  probability, the model does this by constantly probing the regulatory  model.

# What can regulators do to fight off competing models?

1. To  fight off these attacks its first of all essential for regulators to  keep their models hidden, as it is extremely easy to just copy a  complete model (intelligent system) on a pen drive and distribute it to  the highest bidder. What makes this more pressing is that these models  are not exclusionary, the leaked model can be widely shared amongst FIs.  To rub further salt in the wound, its very hard to prove whether or  not FIs are using the model.
2. Regulators  would however be able to measure the intensity of adversarial attacks.  Similar to the use of a funnel plot used in research to identify  publication bias, regulators can investigate the clustering of companies  who pass regulatory inspection just above the selection threshold. Even  though these companies could have legitimate reasons for being that  close to the thresholds, companies close to the threshold can still be  sent warnings.
3. Regulators  should prioritise the use of privately available data to fight off some  competing models; as a corollary, they should also refrain from making  this data public.
4. Regulators  can act in unsystematic ways to fool these agents: regulators can  simply dupe GANs by simulating and distributing false information about  nonexistent companies and regulatory breaches.
5. GANs  are the perfect automatable agent for spotting regulatory loopholes.  Hence, regulators themselves can also use this technology to test the  strength of their regulatory defence.
6. Lastly,  regulators should be aware that the biggest pressure will come from  third party services who would have accumulated enough data to execute  these types of attacks. These companies can act under the guise of cheap  ML service providers while they are in fact harvesting data that can be  used to undermine regulators and select FIs in favour of others.

In  the future it will not be enough to simply use ML models to flag  suspicious transactions. Although this approach provides a good line of  first defence, regulators would also have to investigate entities that  try to bypass regulation by duping the system. Regulators can have both a  proactive strategy by trying to detect adversarial attacks, or they can  investigate systematic biases in submissions and then investigate those  clustering around the selection thresholds.

Scott  Bauguess, Chief Economist of the US SEC, gave a speech on the risk of  AI and ML . He stated that this technology will no doubt make the risk  assessment process more efficient and effective, but is not likely to  replace human judgment in regulation of financial markets. The same  conclusion can also be drawn from the above analysis of GANS. The  existence of GANs means that the future of regulation is indeed one  where models will be fooled to the extent where humans would not. In its  current form GANs are, however, still reliant on regulators adopting  automated AI models and the collection of data that follows from AI  adoption.

&amp;#x200B;

[https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=3371902](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3371902)",0,1,False,self,,,,,
1788,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,14,bhvsvs,self.MachineLearning,PC build thoughts?,https://www.reddit.com/r/MachineLearning/comments/bhvsvs/pc_build_thoughts/,stronomia,1556341580,[removed],0,1,False,self,,,,,
1789,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,14,bhw0n8,self.MachineLearning,How does one model a sequence within a sequence using LSTMs?,https://www.reddit.com/r/MachineLearning/comments/bhw0n8/how_does_one_model_a_sequence_within_a_sequence/,aashwin93,1556343466,[removed],0,1,False,self,,,,,
1790,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,14,bhw3v4,self.MachineLearning,[D] Choosing AI Benchmark Tasks to Benefit Other Fields,https://www.reddit.com/r/MachineLearning/comments/bhw3v4/d_choosing_ai_benchmark_tasks_to_benefit_other/,chisai_mikan,1556344268,"Some recent [work](https://medium.com/mila-quebec/choosing-ai-benchmark-tasks-to-benefit-other-fields-90f3a861886e) by frequent participant of this subreddit /u/alexmlamb

Didnt know he was in Japan doing ML for Japanese work

*From the [blog post](https://medium.com/mila-quebec/choosing-ai-benchmark-tasks-to-benefit-other-fields-90f3a861886e):*

MNIST, a dataset built before neural networks were able to read the handwritten numbers on bank checks. It was a difficult task when it was introduced in 1998, but now, in the words of Mila PhD student Alex Lamb, it is done to death. Because so many programs can solve it with greater than 99% accuracy, it is no longer useful for showing whether a new program advances the state of the art or not. As a result, researchers have started creating harder spinoff tasks with the same standard conditions, such as EMNIST (a mixture of upper- and lower-case letters along with digits) and FashionMNIST (pictures of clothing items, to be classified as shoes, shirts, etc.) Alex wants to add another criterion to these spinoffs: instead of just making new versions of MNIST which are harder to solve, why cant we make ones which are useful outside of our own research community?

Alex admits that machine learning systems which can only read the 10 types of characters included in KMNIST would be of little value to literature scholars, but he calls this task a gateway drug, expressing the hope that models (and researchers) trained on KMNIST would be competent to move on to the other datasets his team has assembled, like Kuzushiji-49, which contains the 49 most common characters, and Kuzushiji-Kanji, which contains 3,832 rare characters and stands as a credible replacement for the popular Omniglot dataset, introduced for few-shot learning in 2015 and beginning to suffer from the same overuse as MNIST. The final step is to read raw pages of these pre-modern books, which brings the added problems of distinguishing text from illustration and moving between the columns of text in the proper order.

https://medium.com/mila-quebec/choosing-ai-benchmark-tasks-to-benefit-other-fields-90f3a861886e",0,2,False,self,,,,,
1791,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,14,bhw4q6,besttelescopicforks.blogspot.com,Double Depth Telescopic Forks - LHD S.p.A.,https://www.reddit.com/r/MachineLearning/comments/bhw4q6/double_depth_telescopic_forks_lhd_spa/,lhd121,1556344489,,0,1,False,default,,,,,
1792,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,15,bhwbym,self.MachineLearning,[D] SOTA for Sentiment Analysis / Classification,https://www.reddit.com/r/MachineLearning/comments/bhwbym/d_sota_for_sentiment_analysis_classification/,AnonMLstudent,1556346292,"What are current state-of-the-art models or architectures for sentiment analysis/classification of text? For example, classifying if a sentence has a positive or negative sentiment.",7,13,False,self,,,,,
1793,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,17,bhwy7y,self.MachineLearning,my first approach/plan for learning ML,https://www.reddit.com/r/MachineLearning/comments/bhwy7y/my_first_approachplan_for_learning_ml/,nille_peter,1556352416,"hey guys, 

i want to learn something about ML and i thought i ask for your opinions about my resources/plan. 

i've heard so much positive about the stanford course on coursera created by andrew ng ([this](https://www.coursera.org/learn/machine-learning)) . so i want to take this course and since this course doesnt use python, but matlab, i want to take the 'Applied Data Science with Python' from the michigan university on coursera as well ([this](https://www.coursera.org/specializations/data-science-python)). i think i will take both courses parallel.  

what do you think? is this a good combination? or would you recommend some complete other stuff for the beginning?",1,1,False,self,,,,,
1794,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,17,bhx4mc,self.MachineLearning,[D] Writing a Paper on a New Idea/Task without Benchmarking,https://www.reddit.com/r/MachineLearning/comments/bhx4mc/d_writing_a_paper_on_a_new_ideatask_without/,AnonMLstudent,1556354262,"If I'm working on an ML project that is tackling an issue that has not been addressed before (at least not directly), is it possible to release a paper without benchmarking against other models since there will not be any other models that can be directly benchmarked against? I have barely seen any examples of this so I am unsure what is the correct approach in this situation.",14,5,False,self,,,,,
1795,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,17,bhx500,medium.com,Towards AI-based only biosignal analysis pipeline,https://www.reddit.com/r/MachineLearning/comments/bhx500/towards_aibased_only_biosignal_analysis_pipeline/,rachnogstyle,1556354370,,0,1,False,default,,,,,
1796,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,17,bhx6y7,self.MachineLearning,Replication of how text is perceived to non-speakers of the language [D],https://www.reddit.com/r/MachineLearning/comments/bhx6y7/replication_of_how_text_is_perceived_to/,john2day,1556354955,"A few days ago, I was reminded of this video: [https://www.youtube.com/watch?v=Vt4Dfa4fOEY](https://www.youtube.com/watch?v=Vt4Dfa4fOEY) which is an attempt to grasp how e.g. English sounds like to non-speakers. My question is: has there been any attempt to visualise this through how text looks like?  For example, how does English look like to Cyrillic readers?

A possible method of replicating this that I have thought of is mapping the differences and nuances of how the language is displayed graphically and in terms of grammar and then reapplying that transformation to create an entirely new language. Is this possible? And if so, has anyone done it? 

Any responses will be appreciated.",2,2,False,self,,,,,
1797,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,18,bhxexk,i.redd.it,"Is it possible to predict year_4_A and year_4_B using any ML algorithms? if yes, could you suggest me what is the algorithm? the data just contains 183 rows",https://www.reddit.com/r/MachineLearning/comments/bhxexk/is_it_possible_to_predict_year_4_a_and_year_4_b/,LowerLaugh,1556357223,,0,1,False,https://b.thumbs.redditmedia.com/rS3Jme6gQWLk075n7qOQfo1rhhwz7jKjxm-GFI-41vI.jpg,,,,,
1798,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,18,bhxhus,self.MachineLearning,How do you figure out which video contains new concepts and where?,https://www.reddit.com/r/MachineLearning/comments/bhxhus/how_do_you_figure_out_which_video_contains_new/,Pedro_Benning,1556358018,"Situation

1. You have a lot of youtube videos with people talking about a topic. 
2. You downloaded each srt file(file containing the **automatic** transcription)

How do you figure out which video contains new **concepts** and where?

By concepts I mean ideas described by words, not words themselves, ie ""user x used word y 1 times, so it's new"" is too easy. It is about ""user x used unusual sentence/word constellations, so it's a new concept at &lt;timestamp&gt;"".

Keep in mind that the transcription was done automatically, so it likely contains errors, but ime youtube does this quite well.",0,1,False,self,,,,,
1799,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,19,bhxmst,9gans.com,An AI Art Gallery that is refreshed every hour to generate a completely new and unique collection of 9 images.,https://www.reddit.com/r/MachineLearning/comments/bhxmst/an_ai_art_gallery_that_is_refreshed_every_hour_to/,alvisanovari,1556359370,,0,1,False,default,,,,,
1800,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,19,bhxykg,self.MachineLearning,[D] What kind of network would be suitable for a conversation bot?,https://www.reddit.com/r/MachineLearning/comments/bhxykg/d_what_kind_of_network_would_be_suitable_for_a/,tredditr,1556362648,"While I know about all the major network types (CNN, RNN, Transformer, etc.) I can't really wrap my head around how I would go about answering to a message while keeping the messages before that in consideration, so it would have a conversation with a human instead of just message - answer scheme.

It's probably pretty obvious but my thought process is stuck right now. Thanks!",4,0,False,self,,,,,
1801,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,20,bhxzi6,self.MachineLearning,[Yolo help] Need a working updated tutorial,https://www.reddit.com/r/MachineLearning/comments/bhxzi6/yolo_help_need_a_working_updated_tutorial/,ZuperPippo,1556362891,"Hi. I gotta detect 6 different dice (are all unique) for my masters thesis. I got the camera working and my task is to run a python script from a Linux distro, so the whole project is going to run under Ubuntu.    

I found yoloV3 and already trained a network last 2 days, however without any success. I haven't got time to test is every 2 days and wait for any results, so I tried to run it on Windows + GPU (only have Ubuntu laptop with integrated gpu and Windows with gtx1060).    

I can't make it to work on windows and tried every tutorial and github repo. My cmd ""make"" doesn't work, tried multiple software (Cmake, Gycwin, Mingw, etc) to make it run; the tutorial constantly explains how to use darknet.exe and I don't have an exe (so I guess it's created with ""make"" command).    

Additional info: I ran the network once, had cca 200 photos with labels, changed number of classes, filters.      
Can anybody please help or link me to a working, updated tutorial? It looks so messy to me",0,1,False,self,,,,,
1802,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,21,bhytgo,self.MachineLearning,Question about Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bhytgo/question_about_machine_learning/,moz2019,1556369885,[removed],0,1,False,self,,,,,
1803,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,22,bhz4m7,self.MachineLearning,How does number of layers and neurons affect LSTM?,https://www.reddit.com/r/MachineLearning/comments/bhz4m7/how_does_number_of_layers_and_neurons_affect_lstm/,shirogakita,1556372139,[removed],0,1,False,self,,,,,
1804,MachineLearning,t5_2r3gv,2019-4-27,2019,4,27,22,bhz7s0,self.MachineLearning,"[D] ""It's quite possible for machine learning to have exploits as fundamentally severe and retrospectively obvious as the NSA's 13+ year head start in differential cryptography. White hat research is a terrible proxy for black hat research - especially for AI.""",https://www.reddit.com/r/MachineLearning/comments/bhz7s0/d_its_quite_possible_for_machine_learning_to_have/,kirasolo,1556372786,"Smerity on Twitter proposes red teams for AI companies to protect their ML products from malicious attacks. The context is YouTube, which was found by one researcher to have recommended the RussiaToday video on the Mueller report to an extreme degree over others

https://twitter.com/Smerity/status/1121647755208744963",20,144,False,self,,,,,
1805,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,0,bhzzmi,udemy.com,The Fun and Easy Guide to Machine Learning using Keras,https://www.reddit.com/r/MachineLearning/comments/bhzzmi/the_fun_and_easy_guide_to_machine_learning_using/,systems4facility,1556377785,,0,1,False,default,,,,,
1806,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,0,bi0096,reddit.com,r/MachineBehaviour,https://www.reddit.com/r/MachineLearning/comments/bi0096/rmachinebehaviour/,TheSn00pster,1556377882,,1,1,False,default,,,,,
1807,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,0,bi0dsc,self.MachineLearning,"Machine learning in finance: Why, what &amp; how",https://www.reddit.com/r/MachineLearning/comments/bi0dsc/machine_learning_in_finance_why_what_how/,andrea_manero,1556380153,https://www.datasciencecentral.com/profiles/blogs/machine-learning-in-finance-why-what-amp-how,0,1,False,self,,,,,
1808,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,1,bi0iou,medium.com,AI in Medical Imaging: Global Market Outlook,https://www.reddit.com/r/MachineLearning/comments/bi0iou/ai_in_medical_imaging_global_market_outlook/,Yuqing7,1556380973,,0,1,False,https://b.thumbs.redditmedia.com/PW8KliDpwMlts-Q_oThfVwvb8NEjGvs9w58VYgpJdAA.jpg,,,,,
1809,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,1,bi0tlz,github.com,Release pandas-profiling v1.4.2  pandas-profiling/pandas-profiling  GitHub,https://www.reddit.com/r/MachineLearning/comments/bi0tlz/release_pandasprofiling_v142/,jos_pol,1556382740,,0,1,False,default,,,,,
1810,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,1,bi0wxa,playspeedgate.org,Speedgate: World's first sport generated by AI,https://www.reddit.com/r/MachineLearning/comments/bi0wxa/speedgate_worlds_first_sport_generated_by_ai/,dilberdillu,1556383281,,0,1,False,default,,,,,
1811,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,2,bi1mr5,self.MachineLearning,Identifying repeat users,https://www.reddit.com/r/MachineLearning/comments/bi1mr5/identifying_repeat_users/,Wozezeka,1556387458,[removed],0,1,False,self,,,,,
1812,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,3,bi2058,self.MachineLearning,[D] Which one is the most overcrowded research area in ML right now?,https://www.reddit.com/r/MachineLearning/comments/bi2058/d_which_one_is_the_most_overcrowded_research_area/,benitorosenberg,1556389626,,1,1,False,self,,,,,
1813,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,3,bi2bxy,self.MachineLearning,Chinese Credit Card Data,https://www.reddit.com/r/MachineLearning/comments/bi2bxy/chinese_credit_card_data/,ram3_,1556391527,[removed],0,1,False,self,,,,,
1814,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,3,bi2caq,self.MachineLearning,[D] Masters to Industry - Learnings,https://www.reddit.com/r/MachineLearning/comments/bi2caq/d_masters_to_industry_learnings/,OrganicTowel_,1556391588,"As a recent master's grad that just entered into the ML/robotics industry, I'm drafting all the observations and lessons I learned throughout this journey. I hope that anyone else reading this doesn't repeat my mistakes.

NOTE: This might not be applicable to everyone, for obvious reasons.

A little bit about me - A recent master's grad with almost non-existent industry experience, apart from one summer internship with my advisor at a robotics company she worked for (I did not end up there). My undergrad was in something different altogether and shifted to CV/ML/DL over the past 2 years. I graduated from a top 5 university in robotics/ML. I took few but highly impactful classes throughout my masters and focussed more on research. I did not publish anything but plan to soon. I was always fascinated about autonomous vehicles and am now working at a promising mid-sized company.

Firstly, there is an extreme dearth of good research engineers and companies are willing to shell out a butt-load of money and stocks to lure you in.

Resume - I had put GitHub hyperlinks to my project code (GitHub) and reports (gDrive) which surprisingly caught a lot of traction. When interviewing for the company I work for now, the interviewers took the time to read through a project report I had done last year (I made it a point to read through all my reports before any interview), grilled me on it for one hour and it was the most fun interview I ever had.

Midway through all the interviews, I started tracking each application through an app like Trello. By 'tracking' I mean every single technical or coding question asked, how did I answer or approach it and what could have been done better. It goes without saying that the initial interviews were horrible. Having an answer within 5 sec of the question is not what I was conditioned for. I tend to think for a long time (15-20 sec) before I can spew out an answer. However, by tracking each application, I observed that all companies would pretty much ask the same technical questions and before each interview going over that question bank got me through some rounds that I would have otherwise never been able to crack. The link is [here](https://docs.google.com/document/d/1WhKnDyXtsfViNrzoiMVqKeKgXgn1Z51izktd4R8dcaI/edit?usp=sharing). Feel free to add more questions that you've come across.

The dreaded coding rounds - At first, they were daunting! Speaking your mind while you try to come up with a working logic, code and test it in 45-60 min is not humanly possible without a ton a practice. That said, my confidence did grow with time and I noticed that almost every question asked was from the easy and medium collection from [this question bank](https://leetcode.com/explore/interview/card/top-interview-questions-easy/). I had to go through all the questions in that collection *TWICE* before I could muster this round. If you're interested in the autonomous driving or robotics industry, C++ skills and knowledge is an extreme necessity. The C++ modules in Geeksforgeeks.com was a life-saver. Specifically, you should be able to understand and incorporate in your code - templates, inheritance, pointers, references, std::vector, std::unordered\_map, std::move, std::undordered\_set, constructor, destructors, virtual functions and have an understanding of how they work behind the scenes. Again, all coding questions are listed in the same doc as the technical questions one (above).

My unemployed friends and I practiced a few coding rounds amongst ourselves. That helped.

Write pseudo-code before diving into code! I like the pseudo-code to be fairly detailed but that's up to you. Even though you might not be able to finish coding the solution in time, the interviewer has some data points to look at and also makes sure you and the interviewer are on the same page.

Some other points to consider- Towards the end of the interview, ask meaningful questions. What do they work on, what challenges are they facing currently, how did they tackle a previous challenge, etc.

After the interview, please make sure to send a thank you mail to all your interviewers. It goes a long way.

As always, if you have any questions, I'd be glad to answer",47,221,False,self,,,,,
1815,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,4,bi2jhs,self.MachineLearning,Fraud Detection,https://www.reddit.com/r/MachineLearning/comments/bi2jhs/fraud_detection/,ram3_,1556392762,[removed],0,1,False,self,,,,,
1816,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,5,bi3084,self.MachineLearning,Arctic-Monkeys AI Lyric Generator,https://www.reddit.com/r/MachineLearning/comments/bi3084/arcticmonkeys_ai_lyric_generator/,cestnestmoi,1556395528,"I wanted to create a Lyric generator based off Arctic Monkeys songs. I used the lyrics of all AM songs on Genius Lyrics.

I just thought the results were pretty damn cool so wanted to share it here  
To summarise-

1. Data was collected from *lyricsgenius* API and then cleaned.
2. Used Data augmentation using *nlpaug (BERTaug and FasttestAug)*
3. Used a LSTM model to predict the next word.
4. Then tried transfer learning by fine-tuning it to OpenAI's GPT-  


*been fighting with my sheets and nearly crying in my sleep*  
*yes im battling that well-told gripe the most frustrating*  
*\[verse 1\] never again well i wanna be yours*  
*you see my so red (wanna be yours) and*  
*your dark (wanna be yours) and i i i i wanna be yours*

*i like the way you look at me baby please*  
*dont try to turn out the lights i act as*  
*if im not going crazy but girl im in a muddle tonight*  
*she said ""i want a million things that i can*  
*never think about now that its already ingrained""*  
*and now is too time that everyone gets to have fun*  


This is the [link to blog post](https://medium.com/@meghanabhange13/arctic-monkeys-lyrics-generator-with-data-augmentation-b9b1f7989db0?source=friends_link&amp;sk=ad0dbaa8e65062dfce0abbf1ed88005b). It also has link to reproducible Google Colab notebook  :))  
Origially posted on r/arcticmonkeys",0,1,False,self,,,,,
1817,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,6,bi3ypf,medium.com,Creating Bitcoin trading bots that dont lose money using deep RL,https://www.reddit.com/r/MachineLearning/comments/bi3ypf/creating_bitcoin_trading_bots_that_dont_lose/,notadamking,1556401289,,0,0,False,https://b.thumbs.redditmedia.com/f-q0VpjyIsfbowD0vBQ48IBC3bb67-3pTsuBouru5yU.jpg,,,,,
1818,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,9,bi5jpb,self.MachineLearning,[D] Global Context block spatial resolution,https://www.reddit.com/r/MachineLearning/comments/bi5jpb/d_global_context_block_spatial_resolution/,eukaryote31,1556411225,"The paper in question is here: [https://arxiv.org/pdf/1904.11492.pdf](https://arxiv.org/pdf/1904.11492.pdf)

&amp;#x200B;

The authors claim that assuming similar attention level for different Query points, a lot of computation can be saved by making a query-independent self-attention layer. That sounds good, but the following diagram of their architecture is confusing to me:

![img](7l0c4quudwu21 ""diagram 4(d) from the paper"")

After the Transform section, when the result is added back to the original image, each channel only gets one value broadcast over the entire plane. I had assumed that the goal was to calculate a global attention map (i.e query-independent and key-dependent). Could someone please explain why this is?",3,7,False,self,,,,,
1819,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,10,bi5uj4,self.MachineLearning,[D] current SOTA for real time 3d avatar and open source,https://www.reddit.com/r/MachineLearning/comments/bi5uj4/d_current_sota_for_real_time_3d_avatar_and_open/,PuzzledProgrammer3,1556413206,"What is the current SOTA for real time 3d avatars and  is there a open source code implementation

[http://www.hao-li.com/publications/papers/siggraphAsia2018PAGAN.pdf](http://www.hao-li.com/publications/papers/siggraphAsia2018PAGAN.pdf)

really interested in this paper and ones referenced in there",0,0,False,self,,,,,
1820,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,12,bi6xk5,self.MachineLearning,To apply the most accurate model.,https://www.reddit.com/r/MachineLearning/comments/bi6xk5/to_apply_the_most_accurate_model/,eagle930,1556420811,[removed],0,1,False,self,,,,,
1821,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,12,bi76zn,self.MachineLearning,Is there a reasonable path for me to upgrade my ancient home PC to beat the free Google Colab Tesla T4 when running StyleGAN?,https://www.reddit.com/r/MachineLearning/comments/bi76zn/is_there_a_reasonable_path_for_me_to_upgrade_my/,fluffynukeit,1556422731,[removed],0,1,False,self,,,,,
1822,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,12,bi78cw,self.MachineLearning,[D] Adapting Neural Network Architectures,https://www.reddit.com/r/MachineLearning/comments/bi78cw/d_adapting_neural_network_architectures/,iyaja,1556423018,"Hi everyone. I have an interesting question: how would you modify a high-performance neural net to an entirely new architecture.

&amp;#x200B;

For example, Tesla probably uses some sort of neural net based on convolutions (at least, I think they do; correct me if I'm wrong). On the Tesla autonomy day, Andrej Karpathy mentioned something along the lines of throwing away data after training on it (a system that Elon Musk referred to as Dojo, but the details were not provided).

&amp;#x200B;

Suppose that in the future, we realize that some new kind of architecture (transformers, for instance) perform significantly better than convolution-based models. How would Tesla (or anyone that's using convents, for that matter) adapt their self-driving system to the new technology?

&amp;#x200B;

This shouldn't be a problem if you keep your training data, but what would companies like Tesla, who have training data on a scale that's infeasible to store, do? Is there any existing technique to ""transfer"" weights across completely different architectures?",15,14,False,self,,,,,
1823,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,13,bi7kmf,self.MachineLearning,[D] How much does graduate school prestige matter for ML career?,https://www.reddit.com/r/MachineLearning/comments/bi7kmf/d_how_much_does_graduate_school_prestige_matter/,crediametr,1556425678,"So I am choosing between the following options to continue education. I am international student from a less known US state school.

CMU - unfunded ML thesis masters

top 20 school - funded through TA

R2 school - fellowship PhD

I'd appreciate any suggestions for my case and your experience in general.",36,27,False,self,,,,,
1824,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,13,bi7myt,self.MachineLearning,150k training cycles on GPT-2 117M,https://www.reddit.com/r/MachineLearning/comments/bi7myt/150k_training_cycles_on_gpt2_117m/,icantfindanametwice,1556426190,"Its been a few weeks since I started however, I think due to bad and inconsistent format in my 13 meg file it might be better to start over. Ive seen multiple posts here with about how much they trained, and no, zero posts talk about a model extended with a data set six times larger than Shakespeares complete works - yep, weve seen the posts about my data set is bigger, etc.

However you might have also seen the goals: its not AGI its applied AI, at least in the vernacular of the day. 

Yes, at the loss average of .05 the text tends to be good, however, I want it to make my job as a writer easier. 

So: do I retrain with a better formatted file? Or continue with 150,000 cycles and expect the outcome will be the same as if I reboot?

Ive read the other posts about GPT-2...none regards to training have a model this mature.",1,1,False,self,,,,,
1825,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,13,bi7ovv,self.MachineLearning,[D] Best Cheap Pre-Built PC for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/bi7ovv/d_best_cheap_prebuilt_pc_for_reinforcement/,BombingPanda,1556426596,I'm looking to buy a PC mainly to do reinforcement learning and light gaming. I know its better to build but unfortunately that isn't an option for me. My budget is around $1800 CAD ($1336 USD). What's the best bang for my buck?,27,6,False,self,,,,,
1826,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,15,bi8jg5,self.MachineLearning,Connecting Question-Answering Bot with Database,https://www.reddit.com/r/MachineLearning/comments/bi8jg5/connecting_questionanswering_bot_with_database/,JesusRide,1556433864,[removed],0,1,False,self,,,,,
1827,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,17,bi90m7,i.redd.it,If Thanos were Data Scientist,https://www.reddit.com/r/MachineLearning/comments/bi90m7/if_thanos_were_data_scientist/,uchiha_indra,1556438490,,0,1,False,default,,,,,
1828,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,17,bi97zh,self.MachineLearning,Converting PyTorch model to ONNX?,https://www.reddit.com/r/MachineLearning/comments/bi97zh/converting_pytorch_model_to_onnx/,sidyakinian,1556440510,"So I have a pre-trained model named model.pth. I'm trying to convert it to ONNX by doing the following:

\- Load it from model.pth.

\- Provide dummy input.

\- Export to ONNX.

I was told that I need to subclass torch.nn.Module to load a model properly. But if I subclass it, I have to implement \_\_init\_\_ and forward methods linked to parameters, which there are like 100 of in the model... And I didn't even create the model. So it gets really complicated.

Can't I load a model in Python from .pth file without subclassing nn.Module? And then just export it to ONNX?",0,1,False,self,,,,,
1829,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,18,bi9pf4,self.MachineLearning,"How do business policy rules/overrides work in large, production ML systems (esp. credit scoring)",https://www.reddit.com/r/MachineLearning/comments/bi9pf4/how_do_business_policy_rulesoverrides_work_in/,vnwarrior,1556445403,[removed],0,1,False,self,,,,,
1830,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,19,bi9st6,self.MachineLearning,What are characteristics of a database which is ideal for a Machine Learning workflow?,https://www.reddit.com/r/MachineLearning/comments/bi9st6/what_are_characteristics_of_a_database_which_is/,fongrel,1556446279,[removed],0,1,False,self,,,,,
1831,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,20,bia7ao,excelr.com,Machine Learning Certification,https://www.reddit.com/r/MachineLearning/comments/bia7ao/machine_learning_certification/,dwivediabhinav,1556450048,,0,1,False,default,,,,,
1832,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,20,biaaz2,i.redd.it,i was browsing and i found this site which generate images of people who didn't exist before,https://www.reddit.com/r/MachineLearning/comments/biaaz2/i_was_browsing_and_i_found_this_site_which/,oussama111,1556450968,,1,1,False,https://b.thumbs.redditmedia.com/HO5mYj_ob60JZL0LbWYDr5oSS0GgKhg1d7P1ZZQBKnw.jpg,,,,,
1833,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,23,bibkaj,self.MachineLearning,Yolov3-Tiny Hand Gestures Recognition,https://www.reddit.com/r/MachineLearning/comments/bibkaj/yolov3tiny_hand_gestures_recognition/,RumboYT,1556460363,"I'm trying to train a model to detect Hand Gestures, but I don't know what dataset should I have, how many pictures? What pictures? and more...

I already have a working model but it's not detecting all the time, is there a dataset online for hand gestures for yolo?",0,1,False,self,,,,,
1834,MachineLearning,t5_2r3gv,2019-4-28,2019,4,28,23,bibqwb,sciencedirect.com,Improved biomechanical metrics of cerebral vasospasm identified via sensitivity analysis of a 1D cerebral circulation model,https://www.reddit.com/r/MachineLearning/comments/bibqwb/improved_biomechanical_metrics_of_cerebral/,cangurosenzapensieri,1556461530,,0,1,False,default,,,,,
1835,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,0,bicoss,self.MachineLearning,What are the SOTA research on adversarial examples in image recognition?,https://www.reddit.com/r/MachineLearning/comments/bicoss/what_are_the_sota_research_on_adversarial/,rparvez,1556467106,[removed],0,1,False,self,,,,,
1836,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,2,bidfpq,self.MachineLearning,[D] Did somebody tried to apply DeepDream to 3D Convolutions on MIT Places to produce 3D environment models ?,https://www.reddit.com/r/MachineLearning/comments/bidfpq/d_did_somebody_tried_to_apply_deepdream_to_3d/,ad48hp,1556471311,"That would likely require a rewrite of the DeepDream code, so it would work on the 3D recognition (convolution-based) systems that would output models, and also it might use an \[heavily\] edited version of the [videoify.py](https://videoify.py) that would progressively build up the depth of the scene (instead of zooming-in, it would be the Z axis in 3D space..). I noted the Places database, because i don't think the standard Inception would produce good results with this..",10,50,False,self,,,,,
1837,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,2,bidizu,self.MachineLearning,Find center of pupil,https://www.reddit.com/r/MachineLearning/comments/bidizu/find_center_of_pupil/,danielsafs,1556471821,Could someone give me an intuition or starting point on what technic should I research to create and train a AI to find the center of the pupil in a image of frontal face of humans. (Not real time),0,1,False,self,,,,,
1838,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,2,bidxbw,self.MachineLearning,[Computer Vision] How to identify if a container is filled/empty/percent-filled?,https://www.reddit.com/r/MachineLearning/comments/bidxbw/computer_vision_how_to_identify_if_a_container_is/,mj_nightfury13,1556474019,[removed],0,1,False,self,,,,,
1839,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,3,biebxg,self.MachineLearning,"Should I do exercises given in CS229 course and ""Pattern Recognition and Machine Learning"" book if I want to do research/Ph.D. in machine learning ahead?",https://www.reddit.com/r/MachineLearning/comments/biebxg/should_i_do_exercises_given_in_cs229_course_and/,cipher1202,1556476265,"I'm a senior year computer science undergrad just began to study machine learning a month ago. My aim is to start reading papers in my area of interest under Machine Learning and begin to do research in ML as an intern. I want to further go to grad school and earn an MS + Ph.D. in machine learning.

&amp;#x200B;

I'm taking Prof. Andrew Ng's CS229 lectures ([https://see.stanford.edu/Course/CS229](https://see.stanford.edu/Course/CS229)) and side by side reading relevant chapters from Bishop's ""Pattern Recognition and Machine Learning"" book.

The mathematical problems/exercises given in both the problem sets of CS229 and PRML textbook are mathematically too sophisticated for me to solve on my own, although I can follow the solutions. How much of these exercises would an undergrad/masters student be expected to solve? And if I am considering a Ph.D., should I solve them or skip them at all? 

&amp;#x200B;

Will only the mathematical knowledge and implementation of machine learning algorithms matter to start with research and later in grad school without spending time on these exercises?",0,1,False,self,,,,,
1840,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,3,biehd1,self.MachineLearning,How would I analyze the accuracy of an NLP program like Quakebot?,https://www.reddit.com/r/MachineLearning/comments/biehd1/how_would_i_analyze_the_accuracy_of_an_nlp/,gbhacker133,1556477090,[removed],0,1,False,self,,,,,
1841,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,3,biek0t,self.MachineLearning,Learning Finite State Representations of Recurrent Policy Networks | Deep Reinforcement Learning | Playing Pong with 3 states,https://www.reddit.com/r/MachineLearning/comments/biek0t/learning_finite_state_representations_of/,HeavyStatus4,1556477516,[removed],0,1,False,self,,,,,
1842,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,3,bieloq,self.MachineLearning,Having a hard time with Matlab coding,https://www.reddit.com/r/MachineLearning/comments/bieloq/having_a_hard_time_with_matlab_coding/,quinxxote,1556477769,[removed],0,1,False,self,,,,,
1843,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,4,bierql,self.MachineLearning,Thoughts on advanced study plan for 3rd year undergraduate wanting to apply to PhD programs next year.,https://www.reddit.com/r/MachineLearning/comments/bierql/thoughts_on_advanced_study_plan_for_3rd_year/,wannabeproprogrammer,1556478683,"Hi, I am currently a 3rd Year computer science undergraduate in the UK. Next year I will be working on research thesis, and I've decided to go into the field of ML, and I'm also considering applying for a PhD programme next year as well. 

&amp;#x200B;

However before I go down this route I thought that I'd fill up some gaps in my knowledge relating to ML before I go into independent research. I've studied some ML already as part of my studies. Topics I've covered include 

&amp;#x200B;

Linear Regression 

Dual Linear Regression 

Gaussian Processes 

Unsupervised Learning 

Bayesian Optimisation 

Dirichlet Processes 

Topic Models 

Graphical Models 

 Laplace Approximation 

 Stochastic Methods 

 Variational Methods 

 Neural Networks 

 Reinforcement Learning and Decision Making 

&amp;#x200B;

What I went to do now during the summer is go a bit more in depth into what I studied and develop a deeper understanding. My plan is go through and read the entire Bishop book and solve the problems, but I'm not sure if this is sufficient enough to get better at ML or if it's even the most efficient plan.  

TLDR: Does anybody have suggestions on how I should supplement my learning apart from just Bishop and reading papers? What advice would you give to someone considering PhD study? And do you have any advice on how to approach independent study?",0,1,False,self,,,,,
1844,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,5,bifd64,thedatascientist.com,Predicting Sports Outcomes Using Python and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bifd64/predicting_sports_outcomes_using_python_and/,TheTesseractAcademy,1556481810,,0,1,False,https://b.thumbs.redditmedia.com/khzmER2I6ItxCfW8F5VQCCsGkPlfg94f0hrOIEmcEYE.jpg,,,,,
1845,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,5,bifgg3,self.MachineLearning,[P] Need help with DeepBach algorithm,https://www.reddit.com/r/MachineLearning/comments/bifgg3/p_need_help_with_deepbach_algorithm/,mrlandros,1556482276,"Hey everyone,

&amp;#x200B;

I'm currently working to setup an installation at a party and I'm missing one key element for it: a 12 hour long Bach chorale generated by DeepBach. It will then be played in Ableton live with some little MAX tricks I've hacked together and a controller hidden inside a piece of furniture to make it interactive to passers by.

I've spent now too many hours trying to get it running on my machine but to no avail, it seems the little IT training I got some 15 years ago won't do the trick here...

Since I'm guessing some people here run these algos for breakfast, I figured I could ask politely here and perhaps some kind soul would pity me and and generate such a mid file? I know this is a long shot, but at this point, why not try :) 

You will be mentioned on the installation plaque and rewarded with pictures of amazed party-goers!",2,1,False,self,,,,,
1846,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,5,bifig9,self.MachineLearning,[D] A Survey on Deterministic Finite Automata Compression Techniques,https://www.reddit.com/r/MachineLearning/comments/bifig9/d_a_survey_on_deterministic_finite_automata/,Eug794,1556482573,"Can [this method](http://academicscience.co.in/admin/resources/project/paper/f201606161466092625.pdf) of machine learning be used instead of ""slow"" Neural Nets? If yes, in what else cases?",6,11,False,self,,,,,
1847,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,5,bifr7z,self.MachineLearning,New Machine Learning Workstation,https://www.reddit.com/r/MachineLearning/comments/bifr7z/new_machine_learning_workstation/,VisualRanger,1556483887,[removed],0,1,False,self,,,,,
1848,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,6,big5cs,self.MachineLearning,[D] Why does Beta-VAE help in learning disentangled/independent latent representations?,https://www.reddit.com/r/MachineLearning/comments/big5cs/d_why_does_betavae_help_in_learning/,shamitlal,1556486066,"In Beta-VAE paper (https://openreview.net/pdf?id=Sy2fzU9gl), the authors mentioned that having Beta &gt; 1 helps the network in learning independent latent representations. However, in VAE, the posterior distribution itself is assumed to be a Gaussian with a diagonal covariance matrix, i.e.

q(z|x) = N(U(x),Cov(x))
where Cov(x) is a diagonal matrix.
 
This means that we are inherently generating latents that will be independent given an input image x. So why does increase learning pressure on the KL divergence term between posterior and Gaussian prior should help any more in learning independent latents when posterior is already assumed to be independent?",37,87,False,self,,,,,
1849,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,6,bigd3s,self.MachineLearning,Dataset issue [P],https://www.reddit.com/r/MachineLearning/comments/bigd3s/dataset_issue_p/,king_of_ace,1556487244,"Desperate student looking for some help. I am working on ML based tool in field of Bioinformatics. Subject is about prediction model for cancer. I have dataset containing mutations of cells, and problem is that number of columns on rows differ. Don't know how to approach this issue... Pls help",9,0,False,self,,,,,
1850,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,8,bih9ye,self.MachineLearning,[D] Question pertaining to different methods of feature selection,https://www.reddit.com/r/MachineLearning/comments/bih9ye/d_question_pertaining_to_different_methods_of/,Gkg14,1556492692,"I have been working on a binary classification problem the past few months with over 200 features. I'm using dense neural networks with Keras. I obviously want to trim this down and have been researching didn't tools to assist with feature selection. 

So far I've used the K-S test, but I'm weary that a feature having different distributions between classes doesn't necessarily mean that it will help the network differentiate between the 2 classes (or maybe it does? Unsure about this.)

While perusing Kaggle the other day I came across a user using the feature_importances_ attribute from [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_). I haven't been able to find out how exactly this works - does anyone know how this works and whether or not the feature importance for a Random Forest Classifier would also translate to use with neural networks?

Another thing I have been playing around with is just feeding every feature into the network and using dropout to have the network to select which features to use or discard.

Anyways, does anyone have any knowledge/advice about the merits or drawbacks of any of these strategies, or any advice for a different feature selection strategy to use? Thanks!",6,4,False,self,,,,,
1851,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,8,bihd4b,self.MachineLearning,[Research] Learning Finite State Representations of Recurrent Policy Networks | Deep Reinforcement Learning | Playing Pong with 3 states,https://www.reddit.com/r/MachineLearning/comments/bihd4b/research_learning_finite_state_representations_of/,HeavyStatus4,1556493243,"Hello Everyone,

This is to share the code of a recent work ""Learning Finite State Representations of Recurrent Policy Networks"".

&amp;#x200B;

Abstract: Recurrent neural networks (RNNs) are an effective representation of control policies for a wide range of reinforcement and imitation learning problems. RNN policies, however, are particularly difficult to explain, understand, and analyze due to their use of continuous-valued memory vectors and observation features. In this paper, we introduce a new technique, Quantized Bottleneck Insertion, to learn finite representations of these vectors and features. The result is a quantized representation of the RNN that can be analyzed to improve our understanding of memory use and general behavior. We present results of this approach on synthetic environments and six Atari games. The resulting finite representations are surprisingly small in some cases, using as few as 3 discrete memory states and 10 observations for a perfect Pong policy. We also show that these finite policy representations lead to improved interpretability.

&amp;#x200B;

Paper: [https://openreview.net/forum?id=S1gOpsCctm](https://openreview.net/forum?id=S1gOpsCctm)

Source Code: [https://github.com/koulanurag/mmn](https://github.com/koulanurag/mmn)

&amp;#x200B;

Hopefully, It will be helpful to others.",0,49,False,self,,,,,
1852,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,9,bii2g4,medium.com,The Supersymmetric Artificial Neural Network in laymans terms,https://www.reddit.com/r/MachineLearning/comments/bii2g4/the_supersymmetric_artificial_neural_network_in/,ProgrammingGodJordan,1556497583,,0,1,False,https://b.thumbs.redditmedia.com/a1as5bUqoLL7T7yGRqbtOGd-h-EX9ai46V_VY-XeRZI.jpg,,,,,
1853,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,9,bii6va,self.MachineLearning,What computer vision library to use in my Android project?,https://www.reddit.com/r/MachineLearning/comments/bii6va/what_computer_vision_library_to_use_in_my_android/,Bicharro27,1556498356,[removed],0,1,False,self,,,,,
1854,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,11,bijdf0,self.MachineLearning,where can i find cctv datas?,https://www.reddit.com/r/MachineLearning/comments/bijdf0/where_can_i_find_cctv_datas/,cfi2000,1556505962,"im finding industrial or factorial cctv video for training purpose

like this one

![video](fjssl1mu74v21 ""camera fixed and industrial facility at background"")

i assume that there is webpage or something",0,1,False,self,,,,,
1855,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,13,biki41,self.MachineLearning,Does Google Cloud not allow GPU usage during the $300 trial? (Quota increase rejected repeatedly ),https://www.reddit.com/r/MachineLearning/comments/biki41/does_google_cloud_not_allow_gpu_usage_during_the/,Moondra2017,1556511065,"I signed up for Google Cloud Platform as there is a $300 free credit.  
I tried to increase my GPU quota to one, via IAM and admin -&gt; quotas, but I keep getting rejected.  


I replied to the email by using the email address they told me to contact (within the email address), month later no reply.  


I spent a few days learning about their platform so If  I can't get it running, I feel like I wasted my time trying to learn their platform  :(",0,1,False,self,,,,,
1856,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,13,bil82y,self.MachineLearning,Need help with OCR,https://www.reddit.com/r/MachineLearning/comments/bil82y/need_help_with_ocr/,Pandas_can_scubadive,1556513702,"Hi,

Im trying to make an OCR capable of reading book covers , nothing fancy something which can read titles from the book cover so as i can get the books information . I have already tried Tesseract but it could only extract authors .",0,1,False,self,,,,,
1857,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,15,bim2sz,self.MachineLearning,How to Correctly Tune Parameters with Validation Set?,https://www.reddit.com/r/MachineLearning/comments/bim2sz/how_to_correctly_tune_parameters_with_validation/,-t-h-r-o-w__a-w-a-y-,1556517600,[removed],0,1,False,self,,,,,
1858,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,15,bim9kf,lhd.co.com,LHD S.p.A Is Worldwide Leader in designing and building Stacker Cranes,https://www.reddit.com/r/MachineLearning/comments/bim9kf/lhd_spa_is_worldwide_leader_in_designing_and/,lhd121,1556519124,,0,1,False,default,,,,,
1859,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,15,bimcr8,self.MachineLearning,"I love this field, but I am afraid my GPA is too low to enter",https://www.reddit.com/r/MachineLearning/comments/bimcr8/i_love_this_field_but_i_am_afraid_my_gpa_is_too/,Alexanderdaawesome,1556519834,[removed],0,1,False,self,,,,,
1860,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,16,bimn88,i.redd.it,Its a damn shame,https://www.reddit.com/r/MachineLearning/comments/bimn88/its_a_damn_shame/,darkshivam,1556522263,,0,1,False,https://b.thumbs.redditmedia.com/LMaFukxILPOVAAGcrIGWjRoBG0aTOK5GCNX5yGKceAw.jpg,,,,,
1861,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,16,bimqv4,self.MachineLearning,"""Convolutional Networks with Adaptive Inference Graphs"" - Practical Question",https://www.reddit.com/r/MachineLearning/comments/bimqv4/convolutional_networks_with_adaptive_inference/,albert1905,1556523188,[removed],0,1,False,self,,,,,
1862,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,16,bimu2a,self.MachineLearning,[D] Does someone work in architecture and machine learning?,https://www.reddit.com/r/MachineLearning/comments/bimu2a/d_does_someone_work_in_architecture_and_machine/,fimari,1556524040,Or general NN based 3D object generation and testing?,4,0,False,self,,,,,
1863,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,17,bin5s6,self.MachineLearning,Chatbot with word2vec,https://www.reddit.com/r/MachineLearning/comments/bin5s6/chatbot_with_word2vec/,maik282,1556527116,[removed],0,1,False,self,,,,,
1864,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,18,binb3o,dianshi.baidu.com,KDD Cup 2019: context-aware multi-modal transportation REC challenge,https://www.reddit.com/r/MachineLearning/comments/binb3o/kdd_cup_2019_contextaware_multimodal/,zeng_ma,1556528538,,1,1,False,default,,,,,
1865,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,18,binjjm,github.com,[p] exif-pp: inject a json object into an image's Metadata. Useful for image description/classification,https://www.reddit.com/r/MachineLearning/comments/binjjm/p_exifpp_inject_a_json_object_into_an_images/,amusciano,1556530644,,0,1,False,default,,,,,
1866,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,18,binlcp,self.MachineLearning,Confusion over Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/binlcp/confusion_over_variational_autoencoders/,mellow54,1556531080,[removed],0,1,False,self,,,,,
1867,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,19,binpm8,self.MachineLearning,[D] Confusion over Variational Autoencoders,https://www.reddit.com/r/MachineLearning/comments/binpm8/d_confusion_over_variational_autoencoders/,mellow54,1556532058,"I am a bit confused by Table 1 on the paper about IAF Variational Autoencoders (https://arxiv.org/abs/1606.04934) however my question is about Variational Autoencoders in general.

In the typical derivation of the Variational Autoencoder (VAE) we find that we get the optimal model when our approximate posterior q(z|x; theta) is such that KL[q(z|x) || p(z|x)] =0.

On table 1, it shows values of both the ELBO and log p(x) for different VAE models. It is clear why the ELBO is less than log p(x) because of the gap between the approximate and true posterior, however why are the true marginals log p(x) different for different models? 

After you've closed the gap between the approximate and true posterior - shouldn't you achieve the perfect model? Then why are the true marginals different for the different models?",14,43,False,self,,,,,
1868,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,19,binvsi,self.MachineLearning,[Meta-Learning] Combining two models,https://www.reddit.com/r/MachineLearning/comments/binvsi/metalearning_combining_two_models/,GrandCapito,1556533518,[removed],0,1,False,self,,,,,
1869,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,19,binw0i,self.MachineLearning,fast.ai equivalent in tensorflow,https://www.reddit.com/r/MachineLearning/comments/binw0i/fastai_equivalent_in_tensorflow/,hey_kishore,1556533562,[removed],0,1,False,self,,,,,
1870,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,20,bio7gf,self.MachineLearning,How would machine learning look like without innate priors?,https://www.reddit.com/r/MachineLearning/comments/bio7gf/how_would_machine_learning_look_like_without/,Hadse,1556536105,[removed],0,1,False,self,,,,,
1871,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,20,bioafu,self.MachineLearning,[Discussion] How would machine learning look like without innate priors?,https://www.reddit.com/r/MachineLearning/comments/bioafu/discussion_how_would_machine_learning_look_like/,Hadse,1556536714,"I know we dont really have this yet. But if you were pushed to try explain how you think machine learning without innate priors would look like, what do you think?",6,2,False,self,,,,,
1872,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,20,biokdd,self.MachineLearning,Where to start learning?,https://www.reddit.com/r/MachineLearning/comments/biokdd/where_to_start_learning/,sdev2022,1556538687,[removed],0,1,False,self,,,,,
1873,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,21,bior4r,self.MachineLearning,[SURVEY] In need of help with my dissertation,https://www.reddit.com/r/MachineLearning/comments/bior4r/survey_in_need_of_help_with_my_dissertation/,SirChickenalot,1556539929,[removed],0,1,False,self,,,,,
1874,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,21,biox3v,self.MachineLearning,[D] A Proposed Task for Training a Human-Imitating System as a Benchmark and Potential Part of The Road to AGI,https://www.reddit.com/r/MachineLearning/comments/biox3v/d_a_proposed_task_for_training_a_humanimitating/,Tenoke,1556541016,"I wrote a [short post](https://svilentodorov.xyz/blog/human-imitating-task/) describing a basic idea I've thought about for a while - creating an objective for training a Neural Net where the task is imitating people. More specifically:

&gt; I propose training a model where the input is simply people interacting with their computer (just the browser for simplicity) as they normally do and the output is a prediction of the next action they will take.
 

I am specifically interested in any input on the ways people think this can and cannot work, alternatives, etc.


Also 


&gt; For those who don't think AGI is possible - I am not saying it definitely is (especially not immediately), just proposing a task to get closer to something that is realistically dosable.",0,1,False,self,,,,,
1875,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,21,bip0va,self.MachineLearning,[Project] Help with a quantitative dataset for a ML problem,https://www.reddit.com/r/MachineLearning/comments/bip0va/project_help_with_a_quantitative_dataset_for_a_ml/,babuunn,1556541666,"Hi,

&amp;#x200B;

I'm currently working on a ML project that considers (mostly) quantitative figures, i.e. a 'classical' ML problem with underlying data coming from a .csv file.

&amp;#x200B;

The problem setting is the following: I want to predict if workers are missing their shifts or not. For this problem I have a shift plan available, among other datasets. The plan tracks the date, shift durations and other very obvious data and of course if the workers were present for the respective shift or not (=Target Variable). I already did some EDA on the shift plan and incorporated some features for the classifier that were referring to the last shift. For example: For the shift in question, I incorporated a Feature that is documenting the number of consecutive shifts for the respective worker that he/she was present. Or how many consecutive shifts was the worker absent. For this, I just shifted the calculated column down.  The following example might help:  


|Date|ID|PRESENT|

:--|:--|:--|

|2019-01-01|1|YES|

|2019-01-01|2|YES|

|2019-01-03|1|NO|

|2019-01-05|2|YES|

|2019-01-06|2|YES|

&amp;#x200B;

becomes

&amp;#x200B;

|Date|ID|PRESENT|Consecutive\_Shifts\_Present|

:--|:--|:--|:--|

|2019-01-01|1|YES|NaN|

|2019-01-01|2|YES|NaN|

|2019-01-03|1|NO|1|

|2019-01-05|2|YES|1|

|2019-01-06|2|YES|2|

&amp;#x200B;

&amp;#x200B;

I feel that this is a valid approach to incorporate historic information of the shifts/worker for the shift that needs to be predicted. If not, please tell me what I missed at this point and what approach I should rather consider.

&amp;#x200B;

The actual problem now comes with other datasets that I want to join with the shift plan dataset. For example I have a dataset that tracks the assignments of the workers per shift that he/she has accomplished. Again, there are some quantitative figures recorded per assignment. Along the same lines as the shift dataset, I want to incorporate some historic shift information of the worker for the shift that needs to be predicted. Therefore, I was trying to group the assignments per worker on a shift-base and calculate some quantitative figures (e.g. min, max, mean values). However, at this point I experienced the problem that there can be several consecutive not attended shifts for which there is no data from the assignment dataset. Therefore I don't know how to incorporate the historic data. Consider the following example:

&amp;#x200B;

|Date|ID|PRESENT|Assignment\_Measure|

:--|:--|:--|:--|

|2019-01-01|1|YES|23|

|2019-01-03|1|NO|NaN|

|2019-01-05|1|YES|65|

|2019-01-06|1|NO|NaN|

|2019-01-07|1|NO|NaN|

&amp;#x200B;

&amp;#x200B;

If I was trying to predict the shift at 2019-01-03 I could shift the column 'Assignment\_Measure' one step size down like above. However, for the shift at 2019-01-07 I would still receive no information. A naive solution would be just to copy the values from the absent shift before but I think this is going to be problematic for the model as this would introduce many very similar rows.

&amp;#x200B;

Does any of you guys have an idea how to solve this problem? Or how this problem could be tackled?  


Thanks a lot for your help and input!",0,1,False,self,,,,,
1876,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,21,bip4hg,self.MachineLearning,[Computer Vision] How to identify if a trailer-container is filled/empty/percent-filled?,https://www.reddit.com/r/MachineLearning/comments/bip4hg/computer_vision_how_to_identify_if_a/,mj_nightfury13,1556542307,[removed],0,1,False,https://b.thumbs.redditmedia.com/iHR8q3wm1sNQeFK8jS1EcvJnY0Im5ZJ8LASnAVUqM8Q.jpg,,,,,
1877,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,21,bip5bz,blog.sicara.com,[N] How to start a sustainable R project,https://www.reddit.com/r/MachineLearning/comments/bip5bz/n_how_to_start_a_sustainable_r_project/,serdnaxela,1556542451,,1,1,False,default,,,,,
1878,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,21,bip5wd,i.redd.it,Thanoos Special,https://www.reddit.com/r/MachineLearning/comments/bip5wd/thanoos_special/,ml_coder_pro,1556542543,,0,1,False,spoiler,,,,,
1879,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,22,bipcnr,theguardian.com,I came across this article teaching a dolphin english. Can we try again with machine learning? How far is the computational translation?,https://www.reddit.com/r/MachineLearning/comments/bipcnr/i_came_across_this_article_teaching_a_dolphin/,feinerSenf,1556543682,,0,1,False,default,,,,,
1880,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,22,bipmri,self.MachineLearning,Neural Network Matrix Optimisation,https://www.reddit.com/r/MachineLearning/comments/bipmri/neural_network_matrix_optimisation/,errminator,1556545385,[removed],0,1,False,self,,,,,
1881,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,22,bippx6,self.MachineLearning,[D] Should all new classifiers include adversarial robustness as a comparison metric?,https://www.reddit.com/r/MachineLearning/comments/bippx6/d_should_all_new_classifiers_include_adversarial/,HecknBamBoozle,1556545915,vulnerability to adversarial attacks is pretty well known now. I think it's better to show improvement in robustness over improvements in accuracy now. 1% improvement under attack conditions is much more significant than 1% improvement in accuracy.,23,70,False,self,,,,,
1882,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,biptf2,self.MachineLearning,[D] State of the art on Neural Commentary generation based on Game Data? GPT-2?,https://www.reddit.com/r/MachineLearning/comments/biptf2/d_state_of_the_art_on_neural_commentary/,fratkabula,1556546475,"Consider the following move and commentary dataset for a chess game - 

1, e4,e5,Both players have moved their pawns to oppose each other
2,Nf3,d6,White has moved his Knight while Black moved a pawn one square
3,d4,Bg4,White moved a pawn two squares while Black moved his Bishop 4 squares.

Full example here - https://www.family-games-treasurehouse.com/sample_chess_game.html

If I can create such a dataset, what is the state of the art I can use to train a commentary generation model? GPT-2 might be a starting point but it gives me no control over entities, actions. 

Curious if us humans have any ideas to go about this.",4,11,False,self,,,,,
1883,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,biptv7,medium.com,Machine Learning Landscape: An ML showdown in search of the best tool,https://www.reddit.com/r/MachineLearning/comments/biptv7/machine_learning_landscape_an_ml_showdown_in/,pmMeYourDevJobs,1556546548,,0,1,False,default,,,,,
1884,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,bipvvk,self.MachineLearning,Are LSTM RNN still relevant?,https://www.reddit.com/r/MachineLearning/comments/bipvvk/are_lstm_rnn_still_relevant/,NikolasTs,1556546868,[removed],0,1,False,self,,,,,
1885,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,bipw42,self.MachineLearning,What classes should I take to learn machine learning (from an economics major)?,https://www.reddit.com/r/MachineLearning/comments/bipw42/what_classes_should_i_take_to_learn_machine/,Daniearp,1556546904,"I am an economics major currently finishing my masters (also in Economics), my dissertation uses RDD (regression discontinuity design). I have taken undergraduate classes in calculus, multivariate calculus, linear algebra, probability, statistics and econometrics and have also taken graduate classes in real analysis and microeconometric methods (for example, RDD). I would put my knowledge in probability above regular undergraduate level but Im not well-versed in measure theory, so its not really graduate level either.

I am interested in learning Machine Learning, I currently cannot enroll in another on-campus masters or take classes at a University because Ive recently gotten a job and dont have enough time to commute etc. I do want to take online classes to improve my knowledge. I really liked Georgia Techs OMSCS, but Im not sure they would accept me with my background.

I realize theory is important but I also would like to practice a bit, I have a beginner Python level (I use it on my new job and recently finished MITxs Introduction to Computer Science and Programming using Python with good grades), since I use it daily I imagine I will get better as time goes by. There are so many courses that seem good that I'm a little lost as to which I should take at first... some examples of courses I looked up that seem great: ""Learning from Data - Caltech"", Andrew Ng's CS229 stanford classes, FAST AI Machine learning for coders (although I'm not sure my python skills will be enough for this one), Stanford's Statistical Learning and so on..

I'm not sure I should go high level top down learning or theory heavy bottom up learning at first.

My bottom line interest would be to effectively change careers as Im very disenchanted with economics and data science has piqued my interest.

Thanks!",0,1,False,self,,,,,
1886,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,biq3c1,self.MachineLearning,"[R] A PyTorch implementation of ""Semi-Supervised Graph Classification: A Hierarchical Graph Perspective"" (WWW 2019)",https://www.reddit.com/r/MachineLearning/comments/biq3c1/r_a_pytorch_implementation_of_semisupervised/,benitorosenberg,1556548023,"&amp;#x200B;

https://i.redd.it/4mccccktq7v21.jpg

GitHub: [https://github.com/benedekrozemberczki/SEAL-CI](https://github.com/benedekrozemberczki/SEAL-CI)

Paper: [https://arxiv.org/pdf/1904.05003.pdf](https://arxiv.org/pdf/1904.05003.pdf)

Abstract:

Node  classification and graph classification are two graph learning   problems that predict the class label of a node and the class label of a   graph respectively. A node of a graph usually represents a real-world   entity, e.g., a user in a social network, or a protein in a   protein-protein interaction network. In this work, we consider a more   challenging but practically useful setting, in which a node itself is a   graph instance. This leads to a hierarchical graph perspective which   arises in many domains such as social network, biological network and   document collection. For example, in a social network, a group of people   with shared interests forms a user group, whereas a number of user   groups are interconnected via interactions or common members. We study   the node classification problem in the hierarchical graph where a \`node'   is a graph instance, e.g., a user group in the above example. As  labels  are usually limited in real-world data, we design two novel   semi-supervised solutions named Semi-supervised graph classification via   Cautious/Active Iteration (or SEAL-C/AI in short). SEAL-C/AI adopt an   iterative framework that takes turns to build or update two  classifiers,  one working at the graph instance level and the other at  the  hierarchical graph level. To simplify the representation of the   hierarchical graph, we propose a novel supervised, self-attentive graph   embedding method called SAGE, which embeds graph instances of arbitrary   size into fixed-length vectors. Through experiments on synthetic data   and Tencent QQ group data, we demonstrate that SEAL-C/AI not only   outperform competing methods by a significant margin in terms of   accuracy/Macro-F1, but also generate meaningful interpretations of the   learned representations.",0,25,False,self,,,,,
1887,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,biq4mk,codeingschool.com,Confusion Matrix in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/biq4mk/confusion_matrix_in_machine_learning/,subhamroy021,1556548220,,0,1,False,https://a.thumbs.redditmedia.com/gjibLJmKWFfZqIgIcZNb_XdSI8sXUmCwrJPy98eDvw4.jpg,,,,,
1888,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,biqdhr,self.MachineLearning,"Attention all Programmers, Have i got a challenge for you!",https://www.reddit.com/r/MachineLearning/comments/biqdhr/attention_all_programmers_have_i_got_a_challenge/,j_nawotka,1556549607,[removed],0,1,False,self,,,,,
1889,MachineLearning,t5_2r3gv,2019-4-29,2019,4,29,23,biqevf,reddit.com,"MLConjug. A Python library to conjugate verbs in French, English, Spanish, Italian, Portuguese and Romanian (more soon) using Machine Learning techniques.",https://www.reddit.com/r/MachineLearning/comments/biqevf/mlconjug_a_python_library_to_conjugate_verbs_in/,SekouD,1556549815,,0,2,False,default,,,,,
1890,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,0,biql07,arxiv.org,[R] High-Fidelity Image Generation With Fewer Labels,https://www.reddit.com/r/MachineLearning/comments/biql07/r_highfidelity_image_generation_with_fewer_labels/,hardmaru,1556550689,,1,17,False,default,,,,,
1891,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,0,biql1u,self.MachineLearning,Mila's professional MSc. In machine learning,https://www.reddit.com/r/MachineLearning/comments/biql1u/milas_professional_msc_in_machine_learning/,Tarabiehaa,1556550694,[removed],0,1,False,self,,,,,
1892,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,0,biqwfv,self.MachineLearning,[D] Oriol Vinyals: DeepMind AlphaStar and Sequence Modeling | Artificial Intelligence Podcast,https://www.reddit.com/r/MachineLearning/comments/biqwfv/d_oriol_vinyals_deepmind_alphastar_and_sequence/,UltraMarathonMan,1556552345,"Oriol Vinyals is a senior research scientist at Google DeepMind. Before that he was at Google Brain and Berkley. His research has been cited over 39,000 times. He is one of the most brilliant and impactful minds in the field of deep learning. He is behind some of the biggest papers and ideas in AI, including sequence to sequence learning, audio generation, image captioning, neural machine translation, and reinforcement learning. He is a co-lead (with David Silver) of the AlphaStar project, creating an agent that defeated a top professional at the game of StarCraft. 

**Video:** https://www.youtube.com/watch?v=Kedt2or9xlo 

**Audio:** https://lexfridman.com/oriol-vinyals 

https://i.redd.it/gg0nukhi38v21.png",10,133,False,https://b.thumbs.redditmedia.com/HUBAJExyID0TvpEeUpwBlg610lFlLd7LWwrhPPEAxcg.jpg,,,,,
1893,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,0,biqwnb,self.MachineLearning,Advice for technical Essay for application to Master's degree in Mathematics in Data Science in Technical University of Munich.,https://www.reddit.com/r/MachineLearning/comments/biqwnb/advice_for_technical_essay_for_application_to/,vir00s,1556552372,[removed],0,1,False,self,,,,,
1894,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,0,bir010,youtu.be,"[P] This is CS50 Live with Nick Wong, implementing a CNN (convolutional neural network) from scratch in Python.",https://www.reddit.com/r/MachineLearning/comments/bir010/p_this_is_cs50_live_with_nick_wong_implementing_a/,coltonoscopy,1556552826,,0,1,False,https://b.thumbs.redditmedia.com/HH77av2ibsIOelrF7anoLDmfRByLuezrL5XE7GHl0pU.jpg,,,,,
1895,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,0,bir0hd,self.MachineLearning,What is the difference between self-attention and attention?,https://www.reddit.com/r/MachineLearning/comments/bir0hd/what_is_the_difference_between_selfattention_and/,sauerkimchi,1556552867,[removed],0,1,False,self,,,,,
1896,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,0,bir1zu,self.MachineLearning,Transfer Learning in GANs!,https://www.reddit.com/r/MachineLearning/comments/bir1zu/transfer_learning_in_gans/,HenryAILabs,1556553086,[removed],0,1,False,self,,,,,
1897,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,1,birb6p,burckhardt.com,Perforiermaschinen,https://www.reddit.com/r/MachineLearning/comments/birb6p/perforiermaschinen/,habmkloganjt,1556554408,,0,0,False,default,,,,,
1898,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,1,bircaz,self.MachineLearning,Book recommendation for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bircaz/book_recommendation_for_machine_learning/,tarlanahad,1556554559,I finished the Machine Learning course of Andrew Ng offered by Coursera but now want to have a book to obtain deeper insight about Machine Learning details. I want to ask your favor for a book recommendation for Machine Learning. Thanks in advance.,0,1,False,self,,,,,
1899,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,1,birg3v,self.MachineLearning,[D] Project Malmo vs OpenAI Gym vs DeepMind Lab vs ...,https://www.reddit.com/r/MachineLearning/comments/birg3v/d_project_malmo_vs_openai_gym_vs_deepmind_lab_vs/,AnthonysEye,1556555111,"Has anyone used any of the above named projects or any other comparable simulation environments for running python agent simulations that can give some feedback on their relative strengths and weaknesses. Links to any recent articles that compare them are also welcome, I have found some but they are generally a year or two old and rather cursory. I intend to test them all myself in the coming weeks and will post my thoughts here.

 I am seeking a simulator that can provide a rich and dynamic environment to run multi agent simulations that can easily interface with python. I also wish to use the one most commonly used in the research world to provide benchmarks and comparisons.",13,45,False,self,,,,,
1900,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,1,birpy8,arxiv.org,[R] Ray Interference: a Source of Plateaus in Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/birpy8/r_ray_interference_a_source_of_plateaus_in_deep/,downtownslim,1556556475,,1,4,False,default,,,,,
1901,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,1,birq8k,self.MachineLearning,Why is my NN so bad at approximating a 2D data set?,https://www.reddit.com/r/MachineLearning/comments/birq8k/why_is_my_nn_so_bad_at_approximating_a_2d_data_set/,SRBeyonder,1556556513,[removed],0,1,False,self,,,,,
1902,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,1,birrx2,avantlive.wordpress.com,Fathoming the Deep in Deep Learning  A Practical Approach,https://www.reddit.com/r/MachineLearning/comments/birrx2/fathoming_the_deep_in_deep_learning_a_practical/,avanttech,1556556758,,0,1,False,default,,,,,
1903,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,1,birus8,self.MachineLearning,Why is my neural network so bad at approximating simple data points?,https://www.reddit.com/r/MachineLearning/comments/birus8/why_is_my_neural_network_so_bad_at_approximating/,SRBeyonder,1556557170,[removed],0,1,False,https://b.thumbs.redditmedia.com/OYpuXgR5r_npWauhcdiucSjOUPv4RgQ_4YDoDBaIIHU.jpg,,,,,
1904,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,2,birv8u,medium.com,OpenAI Sparse Transformer Improves Predictable Sequence Length by 30x,https://www.reddit.com/r/MachineLearning/comments/birv8u/openai_sparse_transformer_improves_predictable/,Yuqing7,1556557232,,0,1,False,https://b.thumbs.redditmedia.com/1cp2MVM6pzoKLwtG49RsP5COS5CNx3GRboa_dGKmrkg.jpg,,,,,
1905,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,2,bis1bw,self.MachineLearning,How to teach the concept of skill to a game agent?,https://www.reddit.com/r/MachineLearning/comments/bis1bw/how_to_teach_the_concept_of_skill_to_a_game_agent/,emrsmsrli,1556558071,[removed],0,1,False,self,,,,,
1906,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,2,bis68x,self.MachineLearning,[Discussion]Fathoming the Deep in Deep Learning  A Practical Approach,https://www.reddit.com/r/MachineLearning/comments/bis68x/discussionfathoming_the_deep_in_deep_learning_a/,avanttech,1556558800,"Deep in Deep Learning is elusive yet approachable with a bit of  mathematics. This beckons a practical question: Is elementary calculus  sufficient to unravel deep learning? The answer is yes indeed. Armed  with an unbound curiosity to learn and re-learn new and old alike and  possibly if you can methodically follow below sections, I reckon youll  cross the chasm to intuitively understand and apply every concepts  including calculus in their glory to de-clutter all intricacies  of deep  learning.  Im covering the steps I took and what I researched, read and understood   being captured to reveal each concept as intuitively as possible and  additional topics that piques your interest:

Steps to fathom the depth:

The Beginnings  Modelling Decisions with Perceptrons  
Workhorses inside Nodes  Activation Functions  
A Gentle Detour on Basics  Differential Calculus  
The Underpinnings  Essential Statistics and Loss Reduction  
The Grand Optimization  Gradient Descent  
Intuitive Examples to the Rescue  Descent Demystified  
Ensemble directed Back &amp; Forth  Feed Forward &amp; Back Propagation  
Inner Workings of Bare NeuralNet  Matrices matched to Code  
Learning Curve Retraced  References &amp; Acknowledgements

Read the full article at [https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/](https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/) and share your thoughts.",0,1,False,self,,,,,
1907,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,2,bis7u6,nextjournal.com,Efficient Neural Network Loss Landscape Generation,https://www.reddit.com/r/MachineLearning/comments/bis7u6/efficient_neural_network_loss_landscape_generation/,Bdamkin54,1556559018,,0,1,False,default,,,,,
1908,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,2,bisava,self.MachineLearning,[D]Fathoming the Deep in Deep Learning  A Practical Approach,https://www.reddit.com/r/MachineLearning/comments/bisava/dfathoming_the_deep_in_deep_learning_a_practical/,avanttech,1556559425,"Deep in Deep Learning is elusive yet approachable with a bit of  mathematics. This beckons a practical question: Is elementary calculus  sufficient to unravel deep learning? The answer is yes indeed. Armed  with an unbound curiosity to learn and re-learn new and old alike and  possibly if you can methodically follow below sections, I reckon youll  cross the chasm to intuitively understand and apply every concepts  including calculus in their glory to de-clutter all intricacies  of deep  learning.   Im covering the steps I took and what I researched, read and understood   being captured to reveal each concept as intuitively as possible and  additional topics that piques your interest:

 Steps to fathom the depth:

The Beginnings  Modelling Decisions with Perceptrons  
Workhorses inside Nodes  Activation Functions  
A Gentle Detour on Basics  Differential Calculus  
The Underpinnings  Essential Statistics and Loss Reduction  
The Grand Optimization  Gradient Descent  
Intuitive Examples to the Rescue  Descent Demystified  
Ensemble directed Back &amp; Forth  Feed Forward &amp; Back Propagation  
Inner Workings of Bare NeuralNet  Matrices matched to Code  
Learning Curve Retraced  References &amp; Acknowledgements

Read the full article @ [https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/](https://avantlive.wordpress.com/2019/04/29/fathoming-the-deep-in-deep-learning-a-practical-approach/) and share your thoughts.",0,0,False,self,,,,,
1909,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,2,biscfo,i.redd.it,agreeesssssssssssssssssss guyssssssssss,https://www.reddit.com/r/MachineLearning/comments/biscfo/agreeesssssssssssssssssss_guyssssssssss/,ml_coder_pro,1556559649,,0,1,False,default,,,,,
1910,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,3,bisl1b,self.MachineLearning,[Discussion] Real world examples of sacrificing model accuracy and performance for ethical reasons?,https://www.reddit.com/r/MachineLearning/comments/bisl1b/discussion_real_world_examples_of_sacrificing/,AlexSnakeKing,1556560899,"A few years back I was working with a client that was optimizing their marketing and product offerings by clustering their clients according to several attributes, including ethnicity. I was very uncomfortable with that. Ultimately I did not have to deal with that dilemma, as I left that project for other reasons. But I'm inclined to say that using ethnicity as a predictor in such situations is unethical, and I would have recommended against it, even at the cost of having a model that performed worse than the one that included ethnicity as an attribute. 

&amp;#x200B;

Do any of you have real world examples of cases where you went with a less accurate/worse performing ML model for ethical reasons, or where regulations prevented you from using certain types of models even if those models might perform better?",43,20,False,self,,,,,
1911,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,3,bisyqt,arxiv.org,[R] [1904.11955] On Exact Computation with an Infinitely Wide Neural Net,https://www.reddit.com/r/MachineLearning/comments/bisyqt/r_190411955_on_exact_computation_with_an/,evc123,1556562839,,3,39,False,default,,,,,
1912,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,3,bit57f,github.com,tensorflow/gan is a new github repo by tensorflow,https://www.reddit.com/r/MachineLearning/comments/bit57f/tensorflowgan_is_a_new_github_repo_by_tensorflow/,sjoerdapp,1556563773,,0,1,False,https://b.thumbs.redditmedia.com/ufDq1BQT09pK3j9Tvaqv0sfiaV5TjeHfw11O5AVFuyQ.jpg,,,,,
1913,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,4,bita91,twitter.com,Bunch of books on Python including Machine Learning ones are now on sale,https://www.reddit.com/r/MachineLearning/comments/bita91/bunch_of_books_on_python_including_machine/,ElizabethWheeler,1556564483,,0,1,False,https://a.thumbs.redditmedia.com/S-yk9GtL_xqInHUTPS7fTC8wQ4eETWAMQqh6MlMhbc4.jpg,,,,,
1914,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,4,bitds0,self.MachineLearning,Math courses for data science,https://www.reddit.com/r/MachineLearning/comments/bitds0/math_courses_for_data_science/,sedthh,1556564985,"I've watched hundreds of hours of Data Science and Machine Learning videos on YouTube so far, ranging from people hacking together some ""evolutionary"" algorithm to Andrew Ng telling me ""not to worry about the hard parts right now"". It's really easy to have the misconception that one knows everything about Data Science just by watching the contents on YouTube, where copy-pasting 4 lines of code gets you a Neural Network in Keras and a pat on the back. People rarely go deeply into the details or use proper jargon, as it would most likely alienate viewers. 

Therefor my only option when I want to relearn the long forgotten math problems from University is manually selecting related videos on Khan Academy. This helped me greatly with the fundamentals, like matrices and derivatives, but I still struggle to understand more sophisticated techniques, yet alone read papers on Neural Networks. 

&amp;#x200B;

Are there any video tutorials that can help me get better with math? Are there channels that tackle machine learning in a deeper sense? Thank you in advance.",0,1,False,self,,,,,
1915,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,4,biteeb,self.MachineLearning,What are some cool data science/machine learning projects I can do for my company as a data scientist?,https://www.reddit.com/r/MachineLearning/comments/biteeb/what_are_some_cool_data_sciencemachine_learning/,jamforce6,1556565072,"Background about me: I am halfway through my masters in data science and have strong programming and data science background. I work as a data scientist for my company (fairly small, only 60 people) and have access to all major parties, permissions, and data. My company sells clothing b2b and b2c internationally and our transactions data total in the millions of rows with revenue annually in the tens of millions. We capture the usual information like item, cost, selling price, customer data, etc. We store warehouse data, sales data, customer data, basically everything you would expect.

I'm having a huge mental block right now to cool projects I can do to help the company. I've thought about clustering analysis for customers. Can anyone recommend me other cool data science/machine learning projects I can do for my company? I have total freedom to do anything and want to do something impressive yet useful to show the executives.",0,1,False,self,,,,,
1916,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,4,bitmy0,self.MachineLearning,How to extract only verbs from a word2vec model,https://www.reddit.com/r/MachineLearning/comments/bitmy0/how_to_extract_only_verbs_from_a_word2vec_model/,agbviuwes,1556566304,[removed],0,1,False,self,,,,,
1917,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,4,bits2j,self.MachineLearning,How to become the fastest data scientist in the world?,https://www.reddit.com/r/MachineLearning/comments/bits2j/how_to_become_the_fastest_data_scientist_in_the/,kite_and_code,1556567032,[removed],1,1,False,self,,,,,
1918,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,4,bitwuh,self.MachineLearning,[D] How to become the fastest data scientist in the world?,https://www.reddit.com/r/MachineLearning/comments/bitwuh/d_how_to_become_the_fastest_data_scientist_in_the/,kite_and_code,1556567710,"Hi, my name is Florian and I have a dream:

I am obsessed with process optimization and would love to be reaaally fast at Data Science because I love to understand new data sets and derive value from it. So, Data Scientist for me really is the sexiest job of the 21st century. However, to be honest, the work is quite tedious at times. For me, it is especially tiresome to dig into the data (with pandas), choosing the right visualizations. Always adjusting the analyses just a little bit to get them right. And basically the process is very similar for the next project. At least the data exploration part of it.

So, I would like to know: Do you have the same feelings? Where do you lose most of your time? What is especially tedious/slow/tiresome for you?

And then of course: if anyone has good suggestions on how to improve our workflows, I am very interested!

Currently, I already use pandas, seaborn, Jupyter Notebook/Lab, and pandas-profiling.",31,0,False,self,,,,,
1919,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,5,biu7y0,self.MachineLearning,Check it out AI paper of the day,https://www.reddit.com/r/MachineLearning/comments/biu7y0/check_it_out_ai_paper_of_the_day/,cdossman,1556569251,Have you wondered how far ML is in boosting cybersecurity? Researchers have dived in and now present existing challenges and provided the AI community with cybersecurity datasets to investigate more and boost ML advances in the field. Check it out  [https://medium.com/ai%C2%B3-theory-practice-business/where-does-machine-learning-stand-in-cyber-security-670e3fe1cda2](https://medium.com/ai%C2%B3-theory-practice-business/where-does-machine-learning-stand-in-cyber-security-670e3fe1cda2),0,1,False,self,,,,,
1920,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,5,biul8y,arxiv.org,[R] Importance of a Search Strategy in Neural Dialogue Modelling,https://www.reddit.com/r/MachineLearning/comments/biul8y/r_importance_of_a_search_strategy_in_neural/,enverx,1556571156,,1,6,False,default,,,,,
1921,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,5,biun7r,self.MachineLearning,KDD 2019 results are out,https://www.reddit.com/r/MachineLearning/comments/biun7r/kdd_2019_results_are_out/,davoodm93,1556571444,"good luck everyone. 

mine rejected \^\_\^",0,1,False,self,,,,,
1922,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,5,biunm1,self.MachineLearning,Help/Suggestions on making a function to find points with similar behaviour,https://www.reddit.com/r/MachineLearning/comments/biunm1/helpsuggestions_on_making_a_function_to_find/,Hawkeye148,1556571500,[removed],0,1,False,self,,,,,
1923,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,6,bivahu,self.lxnusofficial,STRONG ARTIFICIAL INTELLIGENCE WITH OPEN SOURCE,https://www.reddit.com/r/MachineLearning/comments/bivahu/strong_artificial_intelligence_with_open_source/,lxnusofficial,1556574810,,0,1,True,nsfw,,,,,
1924,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,7,bive57,self.MachineLearning,Is a lot of machine learning bullshit?,https://www.reddit.com/r/MachineLearning/comments/bive57/is_a_lot_of_machine_learning_bullshit/,jarviscolema,1556575348,,0,1,False,self,,,,,
1925,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,7,bivwyr,self.MachineLearning,[D] What are some examples of AI generated content that made you uneasy?,https://www.reddit.com/r/MachineLearning/comments/bivwyr/d_what_are_some_examples_of_ai_generated_content/,piponwa,1556578267,"Hi all,

I want to know about things an AI has produced that shocked or scared you or generally made you say wtf. I know the sub /r/uncannyvalley exists, but I am trying to start a subreddit specifically for uncanny things generated by an AI. It's called /r/AIfreakout and I have so far compiled all AI generated media that has made me uneasy. I figure redditors here would know best about such content because you interact with it on a daily basis. Please feel free to post in the comments or in the sub and subscribe. Thanks!",10,1,False,self,,,,,
1926,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,7,bivza1,humblebundle.com,[R] Announcing the Humble Book Bundle: Python by O'Reilly,https://www.reddit.com/r/MachineLearning/comments/bivza1/r_announcing_the_humble_book_bundle_python_by/,AMD_CEO,1556578639,,0,1,False,default,,,,,
1927,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,8,biw3z3,github.com,[P] Convex Optimization Solver (Educational) - Primal Interior Point Method,https://www.reddit.com/r/MachineLearning/comments/biw3z3/p_convex_optimization_solver_educational_primal/,sritee,1556579372,,0,1,False,default,,,,,
1928,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,8,biw52o,self.MachineLearning,Internship Advice,https://www.reddit.com/r/MachineLearning/comments/biw52o/internship_advice/,BigPandaDog,1556579550,[removed],0,1,False,self,,,,,
1929,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,8,biw6vy,steamgamebundles.blogspot.com,[R] Humble Book Bundle: Python by O'Reilly,https://www.reddit.com/r/MachineLearning/comments/biw6vy/r_humble_book_bundle_python_by_oreilly/,AMD_CEO,1556579844,,0,1,False,default,,,,,
1930,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,9,bix2r6,self.MachineLearning,Cog Sci (ML and AI specialization) Major looking for some insight,https://www.reddit.com/r/MachineLearning/comments/bix2r6/cog_sci_ml_and_ai_specialization_major_looking/,Diddlesquig,1556585223,[removed],0,1,False,self,,,,,
1931,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,10,bixcr5,self.MachineLearning,Energy Analytics/ML,https://www.reddit.com/r/MachineLearning/comments/bixcr5/energy_analyticsml/,bjorniam,1556586901,[removed],0,1,False,self,,,,,
1932,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,10,bixmb6,self.MachineLearning,When to standardise and normalise data?,https://www.reddit.com/r/MachineLearning/comments/bixmb6/when_to_standardise_and_normalise_data/,PMort23,1556588501,[removed],0,1,False,self,,,,,
1933,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,11,bixuz9,arxiv.org,[R] Unsupervised Data Augmentation,https://www.reddit.com/r/MachineLearning/comments/bixuz9/r_unsupervised_data_augmentation/,xternalz,1556589966,,61,13,False,default,,,,,
1934,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,11,bixw2b,self.MachineLearning,State of the art in accurate object tracking &amp; detection,https://www.reddit.com/r/MachineLearning/comments/bixw2b/state_of_the_art_in_accurate_object_tracking/,throwaway_8320,1556590154,[removed],0,1,False,self,,,,,
1935,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,11,biy3xi,self.MachineLearning,Question about MC-dropout,https://www.reddit.com/r/MachineLearning/comments/biy3xi/question_about_mcdropout/,alayaMatrix,1556591517,[removed],0,1,False,self,,,,,
1936,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,13,biyxn3,self.MachineLearning,What is medium(material) of the artwork from GAN?,https://www.reddit.com/r/MachineLearning/comments/biyxn3/what_is_mediummaterial_of_the_artwork_from_gan/,Ikuyas,1556596943,[removed],0,1,False,self,,,,,
1937,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,13,biyzxw,self.MachineLearning,[P]TOEIC Part 5(70% correct rate) Solving only using pre-trained BERT!,https://www.reddit.com/r/MachineLearning/comments/biyzxw/ptoeic_part_570_correct_rate_solving_only_using/,nlkey2022,1556597372,"I'll share an interesting undergraduate project we've done through only pre-trained BERT model.

It can solve TOEIC blank problem(Part 5) better than me!

&amp;#x200B;

There are two types of problems:  finding more contextual or grammatical sentence(word)

&amp;#x200B;

1. Selecting Correct Grammar Type.

&amp;#8203;

    Q) The teacher had me _________ scales several times a day.   1. play (Answer)   2. to play   3. played   4. playing

2. Selecting Correct Vocabulary Type.

    Q) The wet weather _________ her from going shopping.   1. interrupted   2. obstructed   3. impeded   4. discouraged (Answer)

&amp;#x200B;

\####Result

 

Total 7067 datasets

**bert-base-uncased**   5192( 73.46%)

**bert-base-cased**  5398(76.38%)

**bert-large-uncased**  5321( 75.29%)

**bert-large-cased**  5148( 72.84%)

&amp;#x200B;

My github repository is here:

[https://github.com/graykode/toeicbert](https://github.com/graykode/toeicbert)",1,18,False,self,,,,,
1938,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,13,biz7x1,mihaileric.com,Beginner's Deep Dive Into Neural Networks,https://www.reddit.com/r/MachineLearning/comments/biz7x1/beginners_deep_dive_into_neural_networks/,MusingEtMachina,1556598922,,0,1,False,https://b.thumbs.redditmedia.com/6zikWpabLo8DV53Fn8-URLPIH3i1iMItl89KDb4DIks.jpg,,,,,
1939,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,13,bizcpy,self.MachineLearning,Varitaional autoencoders beginners tutorial,https://www.reddit.com/r/MachineLearning/comments/bizcpy/varitaional_autoencoders_beginners_tutorial/,Oayman,1556599902,[https://medium.com/@omaraymanomar/variational-auto-encoders-94f405d35dfd](https://medium.com/@omaraymanomar/variational-auto-encoders-94f405d35dfd),0,1,False,self,,,,,
1940,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,14,bizfms,medium.com,Variational auto-encoders,https://www.reddit.com/r/MachineLearning/comments/bizfms/variational_autoencoders/,Oayman,1556600500,,0,1,False,default,,,,,
1941,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,14,bizmj6,self.MachineLearning,Stacker Cranes for Pallet Handling,https://www.reddit.com/r/MachineLearning/comments/bizmj6/stacker_cranes_for_pallet_handling/,lhd121,1556601958,[removed],0,1,False,self,,,,,
1942,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,14,bizq6j,hackernoon.com,6 Top Applications of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/bizq6j/6_top_applications_of_machine_learning/,AnkitKap,1556602766,,0,1,False,default,,,,,
1943,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,15,bizy5o,youtu.be,"[D] Discriminating Systems - Gender, Race and Power in AI (Video Commentary)",https://www.reddit.com/r/MachineLearning/comments/bizy5o/d_discriminating_systems_gender_race_and_power_in/,ykilcher,1556604583,,0,1,False,default,,,,,
1944,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,15,bizzdp,medium.com,The Failure of the Certificate Revocation List (CRL),https://www.reddit.com/r/MachineLearning/comments/bizzdp/the_failure_of_the_certificate_revocation_list_crl/,Fewthp,1556604846,,0,1,False,default,,,,,
1945,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,15,bj05g1,self.MachineLearning,Estimating cpu required for an application,https://www.reddit.com/r/MachineLearning/comments/bj05g1/estimating_cpu_required_for_an_application/,km1612,1556606206,"Hi, I am a newbie to machine learning with years of experience is data processing and ETL technologies. I have a problem scenario which I would like some guidance on. I have an application that has 10 data pipelines each processing data from multiple database tables. It's possible to identify how much cpu each of the pipeline is taking and also an approximate of how much cpu it takes to process each table as well. The requirement is how much additional cpu is needed to accommodate new tables in these pipelines ? Ofcourse a number of factors to be considered.

Whilst I have a lot of historic data for cpu usage and other variables, I am not sure where to start to achieve an estimating tool.

The data I already have is for 30 days:

Cpu usage per pipeline by every minute
Cpu usage per table within pipeline ( based on size of table)
How many parallel pipelines are active at given time ( as this affects cpu share)
Overall cpu usage on the platform

Appreciate any help on where to start for creating an estimator program?

I am proficient with unix shell scripting etc, so happy to pick any programming such as python if required.",0,1,False,self,,,,,
1946,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,15,bj0ayc,self.MachineLearning,ML web scraper,https://www.reddit.com/r/MachineLearning/comments/bj0ayc/ml_web_scraper/,cash4soul,1556607534,[removed],0,1,False,self,,,,,
1947,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,16,bj0dsa,self.MachineLearning,"[D] How to Build OpenAI's GPT-2: ""The AI That's Too Dangerous to Release""",https://www.reddit.com/r/MachineLearning/comments/bj0dsa/d_how_to_build_openais_gpt2_the_ai_thats_too/,iyaja,1556608155,"Hi everyone. I wrote [an article about OpenAI's GPT-2 language model](https://blog.floydhub.com/gpt2/), which recently got published on [the FloydHub blog](https://blog.floydhub.com). In it, I explain most of the NLP breakthroughs that led to the creation of what media outlets are referring to as ""the AI that's too dangerous to release."" You can read the article [here](https://blog.floydhub.com/gpt2/).

&amp;#x200B;

Like in my [previous article](https://blog.nanonets.com/hyperparameter-optimization/), I included a jupyter notebook that can be run with just a few clicks, so that you see an actual, live, demo running in real time. The demo includes a pretrained GPT-2 ([courtesy of hugging face](https://github.com/huggingface/pytorch-pretrained-BERT)) that can generate text based on a prompt that you provide.

&amp;#x200B;

I've stayed away from any opinions on the decision not to release the full model since I feel that there's been plenty of that on twitter already. Here, I aim to provide a clear and straightforward explanation of how exactly the algorithm works, so that you can make your own informed decisions.

&amp;#x200B;

Here's a link to the article: [https://blog.floydhub.com/gpt2/](https://blog.floydhub.com/gpt2/)",5,63,False,self,,,,,
1948,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,16,bj0e1j,self.MachineLearning,[P] ML web scraper,https://www.reddit.com/r/MachineLearning/comments/bj0e1j/p_ml_web_scraper/,cash4soul,1556608216,"I'm new to ML scene and looking to build a relatively simple ML web scraper. There are potentially hundreds of pages i plan on scraping, data on them are pretty much the same but formatted differently. Is it possible to manually type up some excel files with the data I need and train the ML build with that?",5,1,False,self,,,,,
1949,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,16,bj0fox,self.MachineLearning,[P] Tradeoff solved: Jupyter Notebook OR version control. Jupytext brings you the best of both worlds,https://www.reddit.com/r/MachineLearning/comments/bj0fox/p_tradeoff_solved_jupyter_notebook_or_version/,kite_and_code,1556608613,"# The tradeoff:

Jupyter Notebooks are great for visual output. You can immediately see your output and save it for later. You can easily show it to your colleagues. However, you cannot check them into version control. The json structure is just unreadable.

Version control saves our life because it gives us control over the mighty powers of coding. We can easily see changes and focus on whats important.

Until now, those two worlds were separate. There were some trials to merge the two worlds but none of the projects really felt seamless. The developer experience just was not great.

# Introducing Jupytext:

[https://github.com/mwouts/jupytext](https://github.com/mwouts/jupytext)

Jupytext saves two (synced) versions of your notebook. A .ipynb file and a .py file. (Other formats are possible as well.) You check the .py file into your git repo and **track your changes** but you work in the Jupyter notebook and **make your changes** there. (If you need some fancy editor commands like refactoring or multicursor, you can just edit the .py file with PyCharm, save the file, refresh your notebook and keep working).

Also, the creator and maintainer, **Marc is really helpful** and kind and he works really long to make jupytext work for the community. **Please try out jupytext** and show him some love via **starring his github repo**. [https://github.com/mwouts/jupytext](https://github.com/mwouts/jupytext)",62,249,False,self,,,,,
1950,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,17,bj0q4g,self.MachineLearning,Generating adversarial patches against YOLOv2,https://www.reddit.com/r/MachineLearning/comments/bj0q4g/generating_adversarial_patches_against_yolov2/,somoant,1556611225,[removed],0,1,False,self,,,,,
1951,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,17,bj0t00,self.MachineLearning,NLPCraft - open-source API to convert natural language into actions.,https://www.reddit.com/r/MachineLearning/comments/bj0t00/nlpcraft_opensource_api_to_convert_natural/,aradzinski,1556611940,[removed],0,1,False,self,,,,,
1952,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,17,bj0uem,self.MachineLearning,Deploying a ML spam filter?,https://www.reddit.com/r/MachineLearning/comments/bj0uem/deploying_a_ml_spam_filter/,blueguy008,1556612310,"Hi so i have been working on a spam filter, i have trained the models using various datasets and i think i have a satisfactory performance now. Is there anyway i could deploy this ML filter to a real world email inbox and see emails being filtered coming to an inbox? I am new to ML and would appreciate if someone could point me in the right direction.",0,1,False,self,,,,,
1953,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,17,bj0vru,self.MachineLearning,"[R] Call for Abstracts (CfA) for ""Computational Creativity and Deep Generative Design: Bridging the gap"" at ICCC 2019",https://www.reddit.com/r/MachineLearning/comments/bj0vru/r_call_for_abstracts_cfa_for_computational/,_twoFactor,1556612660," 

CALL FOR ABSTRACTS 

**Computational Creativity and Deep Generative Design:** 

**Bridging the gap**

### CC-DeepGen-19

*June 17th or 18th, Charlotte, North Carolina, USA*

*Submission date for Extended Abstracts: May 5th*

Over the last few years, various models that use deep learning for generation and creation have become increasingly popular, e.g., GANs, VAEs.  These deep generative design models create new media across images, text and music. Computational creativity has explored and continues to explore how surprise, curiosity, novelty, and other evaluative criteria, can be formalised for use by creative software in the sciences, the arts, literature, gaming and elsewhere.  

This one-day workshop explores issues in the application of evaluation metrics from computational creativity to these deep generative models. The intent is to explore both how deep generative models can be more effectively used in computational creativity, and how evaluation metrics from computational creativity might contribute to deep generative models more generally. By bringing together researchers from both fields the workshop will explore the potential to improve deep generative design.

*Areas of interest*

GANs, VAEs &amp; variants **** Incorporating computational creativity metrics **** Increasing diversity in generated artefacts  **** Adding more autonomy to models **** Computational creativity in latent spaces **** Generating more targeted artefacts **** Innovation engines 

Attendees will be able to access all accepted abstracts before attending the workshop to aid in an open atmosphere of discussion.

## Submission Guidelines

Attendees will submit extended abstracts (\~400wds, about one page) that they are willing to present and discuss.

**Extended Abstracts:** abstracts should cover radical ideas on how deep generative design can use concepts and ideas from computational creativity, e.g., various metrics of creativity. These abstracts can cover works-in-progress or new directions for deep generative design. Abstracts should be focussed on ideas that can generate discussion and ideation.

## Venue

The workshop will be part of the Tenth International Conference on Computational Creativity ([**ICCC2019**](http://computationalcreativity.net/iccc2019/)) which will be held in Charlotte, North Carolina, USA.

## Contact

All questions about submissions should be emailed to: jeremiah.hayes@accenture.com",0,1,False,self,,,,,
1954,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,17,bj127v,self.MachineLearning,[D] Quantum Deep learning Context aware Character level Spelling correction for Named Entity Recognition in OCR text,https://www.reddit.com/r/MachineLearning/comments/bj127v/d_quantum_deep_learning_context_aware_character/,NvidiaRTX,1556614377,"The text data can have a lot of random word deletion/insertion, missing spaces, wrong characters, ... due to error during the OCR process.

The data looks something like this: [pastebin](https://pastebin.com/a8981xdS)

Now I'm thinking of an auto-correction method to clear those spelling errors. Character-level convolution seems pretty good in this case.

However, I wonder if are there any existing data structures / machine learning methods that can correct words based on the context as well as their current spelling. For example, ""ch\_mpionchip"" already contains most of the correct characters, and only a few more characters need to be added. Combining context + spelling information will make it much more easier to predict the word than just using the spelling information.

Google BERT provides great sentence-level embedding, but doesn't work too well when words are misspell. Glove or word2vec is even worse and can only recognize correct-spelled words.  In this case, the best option would be a type of embedding that can retain most of the word information, even when part of it is misspell. For example, humans can easily understand ""enviroment"" as ""environment"", or ""chmpionship"" as ""championship"", ...

What do you think is a good way to combine both context/word semantic and word spelling for auto-correction ? Please give your thoughts below. 

  
Thanks for reading!",3,0,False,self,,,,,
1955,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,18,bj155u,endtoend.ai,"[News] RL Weekly 16: Why Performance Plateaus May Occur, and Compressing DQNs",https://www.reddit.com/r/MachineLearning/comments/bj155u/news_rl_weekly_16_why_performance_plateaus_may/,seungjaeryanlee,1556615124,,0,1,False,default,,,,,
1956,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,18,bj18rj,medium.com,Create a Protoss Bot Using Raw Observations and Actions in PySC2,https://www.reddit.com/r/MachineLearning/comments/bj18rj/create_a_protoss_bot_using_raw_observations_and/,Fewthp,1556616016,,0,1,False,default,,,,,
1957,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,18,bj1d5m,self.MachineLearning,[N] How will AI change todays software development and deployment processes? (talk),https://www.reddit.com/r/MachineLearning/comments/bj1d5m/n_how_will_ai_change_todays_software_development/,mto96,1556617133,"This is a 40 minute talk by Christoph Windheuser, Machine Learning projects builder at ThoughtWorks, from GOTO Berlin 2018.

[https://youtu.be/Na012w9N36M?list=PLEx5khR4g7PJW7u0GKxRPIQddtu69boT3](https://youtu.be/Na012w9N36M?list=PLEx5khR4g7PJW7u0GKxRPIQddtu69boT3)

&amp;#x200B;

Please give the talk abstract a read below before giving it a watch:

Bringing Artificial Intelligence applications to life is much more than running an AI framework on an artificial data set. It starts with data gathering, consolidation, cleaning and continues with data science and model building. It goes all the way until deployment, DevOps and lifecycle management. In this talk you'll learn on how AI will change the today software development and deployment processes.",0,2,False,self,,,,,
1958,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,19,bj1pss,self.MachineLearning,Factor Analysis From Linear Autoencoder,https://www.reddit.com/r/MachineLearning/comments/bj1pss/factor_analysis_from_linear_autoencoder/,errminator,1556620094,[removed],0,1,False,self,,,,,
1959,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj1ysj,self.MachineLearning,Will unsupervised learning exist some day?,https://www.reddit.com/r/MachineLearning/comments/bj1ysj/will_unsupervised_learning_exist_some_day/,Otterest_Ferret,1556622127,[removed],0,1,False,self,,,,,
1960,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj217u,self.MachineLearning,Testing Trends in 2019,https://www.reddit.com/r/MachineLearning/comments/bj217u/testing_trends_in_2019/,akhilapriya404,1556622636,[removed],0,1,True,nsfw,,,,,
1961,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj21o2,self.MachineLearning,predict hours time series to minutes time series possible?,https://www.reddit.com/r/MachineLearning/comments/bj21o2/predict_hours_time_series_to_minutes_time_series/,GoBacksIn,1556622726,[removed],0,1,False,https://b.thumbs.redditmedia.com/TFj-N1_CGCubo1i959Dcwz--yFAQ6aOOT-pFsSTcBdQ.jpg,,,,,
1962,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj220t,self.MachineLearning,[D] Stochastic Weight Averaging in PyTorch,https://www.reddit.com/r/MachineLearning/comments/bj220t/d_stochastic_weight_averaging_in_pytorch/,iyaja,1556622798,"PyTorch just published a new blogpost detailing how to use [stochastic weight averaging](https://arxiv.org/abs/1803.05407) with existing PyTorch code and minimal modifications.

&amp;#x200B;

[https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/)",3,19,False,self,,,,,
1963,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj24dv,ramakavanan.blogspot.com,"The riddle of True Positive, True Negative, False Positive and False Negative",https://www.reddit.com/r/MachineLearning/comments/bj24dv/the_riddle_of_true_positive_true_negative_false/,ramakavanan,1556623268,,0,1,False,https://b.thumbs.redditmedia.com/rIksCL8mlh2Mqj-RxeI_0gAM8CEeSt-KCEpzj64P2eQ.jpg,,,,,
1964,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj27t3,self.MachineLearning,[D] Swift for TensorFlow,https://www.reddit.com/r/MachineLearning/comments/bj27t3/d_swift_for_tensorflow/,iyaja,1556623961,"For anyone that hasn't heard of it already, Swift for Tensorflow is a new project led by Chris Lattner (the creator of the Swift programming language) that basically includes all the TensorFlow functionality directly in swift. As of 2019, you can actually run Swift code in colab notebooks.

&amp;#x200B;

I couldn't find an existing post on the 2019 updates to s4tf on this subreddit, so I thought I'd share some announcements, news, and resources. Feel free to reply with more links or resources that might be helpful.

&amp;#x200B;

\- Swift for TensorFlow Project Home Page: [https://www.tensorflow.org/swift](https://www.tensorflow.org/swift)

\- Chris Lattner's talk at the TensorFlow dev summit 2019: [https://www.youtube.com/watch?v=s65BigoMV\_I](https://www.youtube.com/watch?v=s65BigoMV_I)

\- An interview with Jeremey Howard (co-founder of [fast.ai](https://fast.ai)): [https://www.youtube.com/watch?v=drSpCwDFwnM](https://www.youtube.com/watch?v=drSpCwDFwnM)

\- An article by Jeremy Howard about [fast.ai](https://fast.ai)'s plans for s4tf: [https://www.fast.ai/2019/03/06/fastai-swift/](https://www.fast.ai/2019/03/06/fastai-swift/)

\- A nice GitHub repo with some swift example notebooks: [https://github.com/zaidalyafeai/Swift4TF](https://github.com/zaidalyafeai/Swift4TF)

&amp;#x200B;

Also, if anyone is reading this that is more familiar with s4tf, or is an active contributor to the project, what kind of speedups can we expect to see relative to Python + TensorFlow/PyTorch?",0,0,False,self,,,,,
1965,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj28ey,self.MachineLearning,What is the latest hottest topic in time series data (stock) forecasting?,https://www.reddit.com/r/MachineLearning/comments/bj28ey/what_is_the_latest_hottest_topic_in_time_series/,GoBacksIn,1556624080,[removed],0,1,False,self,,,,,
1966,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,20,bj28i4,ramakavanan.blogspot.com,Feature Scaling in Machine Learning using Python,https://www.reddit.com/r/MachineLearning/comments/bj28i4/feature_scaling_in_machine_learning_using_python/,ramakavanan,1556624100,,0,1,False,default,,,,,
1967,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,21,bj2suu,self.MachineLearning,Tensorflow Lite model performs worse when using Androids NNAPI,https://www.reddit.com/r/MachineLearning/comments/bj2suu/tensorflow_lite_model_performs_worse_when_using/,justRtI,1556627880,[removed],0,1,False,self,,,,,
1968,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,21,bj2v9u,self.MachineLearning,Simple ML explanations by MIT PhD students,https://www.reddit.com/r/MachineLearning/comments/bj2v9u/simple_ml_explanations_by_mit_phd_students/,mltidbits,1556628329,[removed],0,1,False,self,,,,,
1969,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,21,bj2wdb,self.MachineLearning,"[Project] Uncanny Valleys: Generative landscape, artwork at ArtSci 2019 exhibition @ ETH Zurich",https://www.reddit.com/r/MachineLearning/comments/bj2wdb/project_uncanny_valleys_generative_landscape/,VitRuzicka,1556628527,"**Uncanny Valleys: Generative landscape**

See: [Main video with project details](https://youtu.be/x_C8QssRWHI)

We showcase a new approach of generating 3D landscape models using GANs and U-Net deep learning models. Interpolating the latent space we generate an animated sequence of 3D landscapes.

&amp;#x200B;

![video](ebwa57u6cev21 ""^ Sneak peek ^"")

If you are in Zurich during the [ArtSci 2019](https://artsci.ethz.ch/) exhibition between 29.4. and 10.5., come visit us at the [CHN building](https://goo.gl/maps/YFv7QDQGaAvpVQuL8) of the ETH Zurich campus and also check out the physical, 3D printed representation of this project.

&amp;#x200B;

*Team*: Vit Ruzicka, Alexander Nikolas Walzer, Nizar Taha

*Open source code*: our [AerialNets](https://github.com/previtus/AerialNets/tree/master/case1_dsm_prediction_segmentation_map) and NVIDIA's [Progressive GAN](https://github.com/tkarras/progressive_growing_of_gans)

&amp;#x200B;

*\\#MachineLearningForArt, \\#DeepLearning, \\#GenerativeArt, \\#GenerativeLandscapes*",3,18,False,self,,,,,
1970,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,21,bj2zp7,self.MachineLearning,Simple ML explanations by MIT PhD students,https://www.reddit.com/r/MachineLearning/comments/bj2zp7/simple_ml_explanations_by_mit_phd_students/,mltidbits,1556629099,[removed],0,1,False,self,,,,,
1971,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,22,bj388g,self.MachineLearning,[P] Simple ML explanations by MIT PhD students,https://www.reddit.com/r/MachineLearning/comments/bj388g/p_simple_ml_explanations_by_mit_phd_students/,mltidbits,1556630450,"Hi everyone,

We're two MIT PhD students trying to bring understandable explanations and discussions about artificial intelligence and machine learning to the public. We just released two videos on:

[The Machine Learning Lifecycle](https://youtu.be/ZmBUnJ7lGvQ)

and

[Types of Machine Learning: Supervised and Unsupervised](https://youtu.be/wy-m6sd1BOA)

Check out our ML Tidbits [YouTube channel](https://www.youtube.com/channel/UCD7qIRMUvUJQzbTXaMaNO2Q) for short and sweet explanations, discussions, and debates about ML topics. We're planning to release new videos on a weekly basis Our goal is to make ML accessible to the public, so that everyone can participate in discussions and make educated decisions about ML products and policies. We believe that teaching responsible ML from the start will create more accountability and enable better public discussions around the societal impacts of this technology.

Contact us: [mltidbits@mit.edu](mailto:mltidbits@mit.edu)

Our website: [mltidbits.github.io](https://mltidbits.github.io/)",41,402,False,self,,,,,
1972,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,22,bj3b8w,self.MachineLearning,CHatbot with word2vec,https://www.reddit.com/r/MachineLearning/comments/bj3b8w/chatbot_with_word2vec/,maik282,1556630920,[removed],0,1,False,self,,,,,
1973,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,23,bj3n8z,codespeedy.com,Data Preprocessing in Python - Machine Learning: Split Dataset,https://www.reddit.com/r/MachineLearning/comments/bj3n8z/data_preprocessing_in_python_machine_learning/,saruque,1556632818,,0,1,False,default,,,,,
1974,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,23,bj3pvt,self.MachineLearning,Wave Physics as an Analog Recurrent Neural Network,https://www.reddit.com/r/MachineLearning/comments/bj3pvt/wave_physics_as_an_analog_recurrent_neural_network/,BarnyardPuer,1556633216,[removed],0,2,False,self,,,,,
1975,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,23,bj3twb,self.MachineLearning,[P] Wave Physics as an Analog Recurrent Neural Network,https://www.reddit.com/r/MachineLearning/comments/bj3twb/p_wave_physics_as_an_analog_recurrent_neural/,BarnyardPuer,1556633843,"We just posted our new paper where we show that recurrent neural networks map to the physics of waves, used extensively to model optical, acoustic, and fluidic systems.

This is interesting because it enables one to build analog RNNs out of continuous wave-based physical systems, where the processing is performed *passively* through the propagation of waves through a domain.

These 'wave RNNs' are trained by backpropagation through the numerical wave simulation, which lets us optimize the pattern of material within their domain for a given ML task.

We demonstrate that this system can classify vowels through the injection of raw audio input to the domain.

Our paper can be found here: [https://arxiv.org/abs/1904.12831](https://arxiv.org/abs/1904.12831)

Our code for simulating and training the wave systems is built using pytorch and can be found here: [https://github.com/fancompute/wavetorch](https://github.com/fancompute/wavetorch)",27,34,False,self,,,,,
1976,MachineLearning,t5_2r3gv,2019-4-30,2019,4,30,23,bj46mb,self.MachineLearning,How to display data like this oversampling tutorial does?,https://www.reddit.com/r/MachineLearning/comments/bj46mb/how_to_display_data_like_this_oversampling/,Scutterbum,1556635751,[removed],0,1,False,self,,,,,
