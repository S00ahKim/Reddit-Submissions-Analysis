,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2015-12-1,2015,12,1,9,3uxbw7,Deep Spatial Autoencoders for Visuomotor Learning,https://www.reddit.com/r/MachineLearning/comments/3uxbw7/deep_spatial_autoencoders_for_visuomotor_learning/,SometimesGood,1448928006,,0,13
1,2015-12-1,2015,12,1,9,3uxd14,"Towards Universal Paraphrastic Sentence Embeddings: ""simple compositional architectures based on updated vector averaging vastly outperform long short-term memory (LSTM) recurrent neural networks"" [arXiv]",https://www.reddit.com/r/MachineLearning/comments/3uxd14/towards_universal_paraphrastic_sentence/,onewugtwowugs,1448928441,,6,28
2,2015-12-1,2015,12,1,9,3uxfx4,A tutorial and methodology for topic modeling of tweets in R,https://www.reddit.com/r/MachineLearning/comments/3uxfx4/a_tutorial_and_methodology_for_topic_modeling_of/,[deleted],1448929588,[deleted],0,0
3,2015-12-1,2015,12,1,9,3uxhx7,GPU advice for matlab NN toolbox,https://www.reddit.com/r/MachineLearning/comments/3uxhx7/gpu_advice_for_matlab_nn_toolbox/,retardedocelot,1448930426,"Hi! I'm playing with a machine learning project for a cognitive science class using matlab's NN toolbox. I am adding a 980ti in addition to the current GTX580 I have, is it worth using both and breaking up the training sets nonsymetricaly between the two, or should I just use the 980ti? Would adding 780ti aswell be worthwhile?",1,0
4,2015-12-1,2015,12,1,9,3uxka2,Topic modeling tweets in R (methods paper + code),https://www.reddit.com/r/MachineLearning/comments/3uxka2/topic_modeling_tweets_in_r_methods_paper_code/,timmaeus,1448931395,,0,1
5,2015-12-1,2015,12,1,9,3uxkde,What you wanted to know about TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3uxkde/what_you_wanted_to_know_about_tensorflow/,clbam8,1448931431,,0,0
6,2015-12-1,2015,12,1,10,3uxqvz,Neural GPUs Learn Algorithms,https://www.reddit.com/r/MachineLearning/comments/3uxqvz/neural_gpus_learn_algorithms/,cesarsalgado,1448934097,,4,8
7,2015-12-1,2015,12,1,10,3uxska,Deep Learning or Machine Learning Lab in Canada,https://www.reddit.com/r/MachineLearning/comments/3uxska/deep_learning_or_machine_learning_lab_in_canada/,Dragonsonepiece,1448934800,"Hey Guys, I am going to graduate soon. I am looking for grad schools in CS in Canada. Can you tell me if there are any other Machine Learning Labs (including any new promising prof looking to set up lab ) apart from UdeM, UAlberta, UBC, McGill or UoT in Canada which are doing good research?

I need to add more to the list to be safe side of admissions. 

Thanks

Edit: Apart from Graham Taylor and Larochelle , any other machine learning Lab in Canada? BTW how is SFU for machine learning?  ",8,0
8,2015-12-1,2015,12,1,11,3uxxw9,[1511.08228] Neural GPUs Learn Algorithms (Google),https://www.reddit.com/r/MachineLearning/comments/3uxxw9/151108228_neural_gpus_learn_algorithms_google/,[deleted],1448936943,[deleted],0,2
9,2015-12-1,2015,12,1,12,3uy79n,What's your opinion on Keras?,https://www.reddit.com/r/MachineLearning/comments/3uy79n/whats_your_opinion_on_keras/,MasterEpictetus,1448940683,"I recently learned about Keras (http://keras.io/), a neural network library built on top of theano. I saw some pretty complex examples written in just a few lines and was intrigued (e.g., http://avisingh599.github.io/deeplearning/visual-qa/). With so many deep learning libraries out there, I was wondering what people in this subreddit think of this one.
 ",11,7
10,2015-12-1,2015,12,1,13,3uycc2,On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models,https://www.reddit.com/r/MachineLearning/comments/3uycc2/on_learning_to_think_algorithmic_information/,InaneMembrane,1448942814,,20,54
11,2015-12-1,2015,12,1,15,3uywrk,Need an spark.mllib algorithm for finding best recommendation of a given set of data,https://www.reddit.com/r/MachineLearning/comments/3uywrk/need_an_sparkmllib_algorithm_for_finding_best/,Ande_Ka_Funda,1448952650,"I'm using echonest's music data to build a music recommendation system on Apache spark. For each song I have the following attribute :

""danceability"",""energy"",""key"",""loudness"",""tempo"",""time_signature""

For a user, I have a set of songs(say a set of 10 songs) he has listened to along with the play count. So now for a user I want to build a profile that takes in account of the above said parameter and prioritize them against the play_count (higher play_count = more priority).

Using this, I want an mlib algorithm that can predict the closest 100 songs based on that user's profile and treating the play_count as a priority.

List of mlib algos are here: http://spark.apache.org/docs/latest/mllib-guide.html

Since I'm weak in mathematics, I would appreciate some suggestions.

EDIT:
After hours of research yesterday I found that there are two major ways to build a recommendation system

1) Collaborative filtering
2) Content-based recommender system

What I'm looking for is the Content-based recommender system. Matrix factorization are usually useful for the former.  I'm not looking for empty attributes to be filled. I'm looking for recommendation on new set of (complete)data which is based on old set of data.

I found that Naive Bayes classifiers is a great way to do the later. Luckily Mllib does have an [implementation](http://spark.apache.org/docs/latest/mllib-naive-bayes.html) of the same.

I'm passing each of my song attributes(""danceability"",""energy"",""key"",""loudness"",""tempo"",""time_signature"") as a dense vector. Training the NaiveBayes on new set of data and testing it with the old set of data did the trick for me.",0,0
12,2015-12-1,2015,12,1,15,3uywru,Naive question: Can you train a neural net 1 neuron at a time?,https://www.reddit.com/r/MachineLearning/comments/3uywru/naive_question_can_you_train_a_neural_net_1/,omniron,1448952655,"Basically, rather than creating a network with fixed parameters (a set number of layers and nodes per layer) could you just construct the network as it was being trained, and only add nodes/layers as training fails?",17,7
13,2015-12-1,2015,12,1,16,3uyzd8,"Binding via Reconstruction Clustering :1511.06418 ""We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. ...""",https://www.reddit.com/r/MachineLearning/comments/3uyzd8/binding_via_reconstruction_clustering_151106418/,pranv,1448954211,,1,8
14,2015-12-1,2015,12,1,16,3uyzzv,Deep Learning at Scholarpedia by Dr. Juergen Schmidhuber,https://www.reddit.com/r/MachineLearning/comments/3uyzzv/deep_learning_at_scholarpedia_by_dr_juergen/,pranv,1448954616,,4,7
15,2015-12-1,2015,12,1,19,3uze5x,How does IBM's personality insights work? How would you go about building similar functionality to run locally?,https://www.reddit.com/r/MachineLearning/comments/3uze5x/how_does_ibms_personality_insights_work_how_would/,mild_redditor,1448965608,,0,0
16,2015-12-1,2015,12,1,19,3uzfgi,Question on unitary matrices and RNN,https://www.reddit.com/r/MachineLearning/comments/3uzfgi/question_on_unitary_matrices_and_rnn/,schmook,1448966687,"Recently there's been an article linked here (sorry, I can't find the link, and I'm on mobile) that suggested a parametrization for weights in recurrent neural networks that kept the weight matrix in the subset of unitary matrices, thus keeping the eigenvalues aways equals to 1, thus keeping training more stable. 

My question is: unitary matrices form a Lie group and we know that small variations in this space of unitary matrices are generated by the elements of its Lie algebra. So, to keep the matrices very close to unitary, it would seem that it's enough to calculate the gradients only in the directions of the u(n) algebra generators and using small enough learning rates. 

Do you guys know of any work in this direction?  Am I missing something important here? This is just a shower thought so there's probably some loophole in it. Can you spot any obvious reasons why this wouldn't work?",2,10
17,2015-12-1,2015,12,1,19,3uzfz2,Should You Leave The Task Of Lubrication to The New Mechanics?,https://www.reddit.com/r/MachineLearning/comments/3uzfz2/should_you_leave_the_task_of_lubrication_to_the/,jackerfrinandis,1448967084,,0,1
18,2015-12-1,2015,12,1,21,3uzpey,Task Loss Estimation for Sequence Prediction,https://www.reddit.com/r/MachineLearning/comments/3uzpey/task_loss_estimation_for_sequence_prediction/,cesarsalgado,1448973960,,0,1
19,2015-12-1,2015,12,1,21,3uzq2s,Dynamic Capacity Networks,https://www.reddit.com/r/MachineLearning/comments/3uzq2s/dynamic_capacity_networks/,cesarsalgado,1448974392,,0,6
20,2015-12-1,2015,12,1,22,3uzvd2,Beyond the Fence: the world's first machine-generated musical to debut in London. (RNNs generating screenplay and stuff),https://www.reddit.com/r/MachineLearning/comments/3uzvd2/beyond_the_fence_the_worlds_first/,fhuszar,1448977429,,0,4
21,2015-12-2,2015,12,2,0,3v0938,Discussion: What next in Computer Vision and Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/3v0938/discussion_what_next_in_computer_vision_and_deep/,code2hell,1448983789,I have been going through and experimenting with deep learning concepts. I would really appreciate if anyone could suggest some interesting projects to follow so as to make my understanding better. Also a discussion on what next in Computer Vision and Deep Learning would be welcome.,15,4
22,2015-12-2,2015,12,2,1,3v0kvd,A curated list of resources dedicated to Natural Language Processing,https://www.reddit.com/r/MachineLearning/comments/3v0kvd/a_curated_list_of_resources_dedicated_to_natural/,kwk236,1448988224,"I am starting a new awesomeness list about NLP (there is already a list about speech recognition and language processing, but I want to focus more on Natural Language Processing itself), feel free to join!

https://github.com/keonkim/awesome-nlp",10,25
23,2015-12-2,2015,12,2,2,3v0p63,Free excerpt from Spark GraphX In Action: Graph-Based Semi-Supervised Learning [OC],https://www.reddit.com/r/MachineLearning/comments/3v0p63/free_excerpt_from_spark_graphx_in_action/,michaelmalak,1448989761,,0,0
24,2015-12-2,2015,12,2,2,3v0wmf,"Keras, now running on TensorFlow",https://www.reddit.com/r/MachineLearning/comments/3v0wmf/keras_now_running_on_tensorflow/,DX89B,1448992486,,32,140
25,2015-12-2,2015,12,2,2,3v0wq1,Heavy Duty Post &amp; Rock Augers For Sale - FREE Quote,https://www.reddit.com/r/MachineLearning/comments/3v0wq1/heavy_duty_post_rock_augers_for_sale_free_quote/,xinger3988,1448992520,,0,1
26,2015-12-2,2015,12,2,3,3v0zff,Sequence to Sequence  Video to Text: Generate a description of events in a video with LSTMs,https://www.reddit.com/r/MachineLearning/comments/3v0zff/sequence_to_sequence_video_to_text_generate_a/,alxndrkalinin,1448993497,,0,7
27,2015-12-2,2015,12,2,3,3v10qq,Artificial intelligence service gives Wikipedians X-ray specs to see through bad edits [x-post from /r/artificial],https://www.reddit.com/r/MachineLearning/comments/3v10qq/artificial_intelligence_service_gives_wikipedians/,CleverSurveys,1448993973,,2,0
28,2015-12-2,2015,12,2,3,3v133p,M2M communication,https://www.reddit.com/r/MachineLearning/comments/3v133p/m2m_communication/,tigerbro94,1448994800,Any good resources out there on M2M communication? Thanks!,0,0
29,2015-12-2,2015,12,2,7,3v24df,Best development environment for Torch?,https://www.reddit.com/r/MachineLearning/comments/3v24df/best_development_environment_for_torch/,[deleted],1449008029,[deleted],2,0
30,2015-12-2,2015,12,2,8,3v2dvx,Learning to link images with their descriptions,https://www.reddit.com/r/MachineLearning/comments/3v2dvx/learning_to_link_images_with_their_descriptions/,alxndrkalinin,1449011799,,0,0
31,2015-12-2,2015,12,2,8,3v2eid,Machine Learning training time in R,https://www.reddit.com/r/MachineLearning/comments/3v2eid/machine_learning_training_time_in_r/,tehsandvich,1449012063,I'm starting to get more experience training machine learning model on large data sets in R but the problem is that training time for some algorithms takes incredibly long. Is python faster in this regards? Or is this something  I just have to accept.,5,0
32,2015-12-2,2015,12,2,8,3v2eqz,Tutorial: Building TensorFlow for Jetson TK1,https://www.reddit.com/r/MachineLearning/comments/3v2eqz/tutorial_building_tensorflow_for_jetson_tk1/,alxndrkalinin,1449012168,,0,1
33,2015-12-2,2015,12,2,8,3v2hts,CrossPost from MLQuestions: Random Forest from scratch. Any help is appreciated.,https://www.reddit.com/r/MachineLearning/comments/3v2hts/crosspost_from_mlquestions_random_forest_from/,vanoccupanther,1449013432,,0,0
34,2015-12-2,2015,12,2,9,3v2qvh,I just built a synthetic neural network but I don't know how to train it,https://www.reddit.com/r/MachineLearning/comments/3v2qvh/i_just_built_a_synthetic_neural_network_but_i/,quicksilv3rflash,1449017263,"Hello, [I have just finished constructing a synthetic neural network](http://www.instructables.com/id/Construct-a-Synthetic-Neural-Network/) on an FPGA. [The github repository is here.](https://github.com/quicksilv3rflash/Synthetic_neural_network_XenoluX) It consists of 36 *asynchronous* neurons in six layers, each neuron of which can be ordered to wire their 6 synapses in any pattern. What types of tasks does one test neural networks on? What algorithms are used to update synaptic connection weights in a synthetic neural network?",7,0
35,2015-12-2,2015,12,2,10,3v2zr3,Gradient descent: why additive cost functions are used commonly instead of multiplicative?,https://www.reddit.com/r/MachineLearning/comments/3v2zr3/gradient_descent_why_additive_cost_functions_are/,hungry_for_knowledge,1449021172,"In numerical optimization (e.g. gradient descent): when we want make sure two criteria are met (e.g. maximized) simultaneously, people often use an additive cost function of: C = argmax( C1(x) + C2(x) ) instead of a multiplicative one: C = argmax( C1(x) x C2(x) ).

Could someone please shed light on this question for me?
Greatly appreciated!",17,11
36,2015-12-2,2015,12,2,11,3v31by,Cifar-10 with k-nearest?,https://www.reddit.com/r/MachineLearning/comments/3v31by/cifar10_with_knearest/,bourbondog,1449021819,"Hello!

I'm trying to classify the cifar-10 with a very basic k-nearest neighbour algorithm. I'm using HOG features. It seems like I'm maxing out on ~54% accuracy (which, tbh, is surprising for such a lame algorithm).

I'm trying to figure out if I can push the algorithm a bit more. Here's what the confusion matrix for the results looks like at the moment.

http://imgur.com/MAMRDnP

On the left is the actual image label. On the bottom is the prediction. It seems like classes 3 and 4 are performing terribly. They correspond to cats and deer. Any suggestions on why HOG is performing bad on these classes?",5,0
37,2015-12-2,2015,12,2,13,3v3jfu,Looking for guidance: why might these clustering algorithms be producing only one cluster?,https://www.reddit.com/r/MachineLearning/comments/3v3jfu/looking_for_guidance_why_might_these_clustering/,[deleted],1449029940,[deleted],0,1
38,2015-12-2,2015,12,2,16,3v45wt,Dealing with varying image size?,https://www.reddit.com/r/MachineLearning/comments/3v45wt/dealing_with_varying_image_size/,shakedzy,1449041684,"Hi all,
I'm trying to make an OCR using a neural network (if anyone here did the Coursera course of Andrew Ng, something like that). 
Now, assume that all images used are 20x20, so I got 400 input units (excluding the biased), each representing a certain specific pixel. 
Now how do I create the same thing but for images of varying size? Even those with different height and width?

Thanks!",8,3
39,2015-12-2,2015,12,2,17,3v4b49,Machined Learnings: Sample Variance Penalization,https://www.reddit.com/r/MachineLearning/comments/3v4b49/machined_learnings_sample_variance_penalization/,iori42,1449045083,,0,1
40,2015-12-2,2015,12,2,17,3v4blb,[1512.00103] Multilingual Language Processing From Bytes,https://www.reddit.com/r/MachineLearning/comments/3v4blb/151200103_multilingual_language_processing_from/,downtownslim,1449045423,,6,19
41,2015-12-2,2015,12,2,18,3v4i8k,"Take a course in ""Image recognition and classification"" or ""Modern Database Systems""",https://www.reddit.com/r/MachineLearning/comments/3v4i8k/take_a_course_in_image_recognition_and/,foresthump1,1449050345,"I am kinda new to the computer science / machine learning field and I have to make a choice between these two courses. At first I wanted to go with Image recognition because I will learn more machine learning, but since my database skills are confined to SQL queries, maybe I should take that course and learn more about databases since that is something that all data science / machine learning jobs will use and not only specific to ""image recognition""?

Any thoughts and input on this would be greatly appreciated!",1,0
42,2015-12-2,2015,12,2,19,3v4kwb,Quick &amp; dirty Bayesian Optimization of 1- and 2d-functions,https://www.reddit.com/r/MachineLearning/comments/3v4kwb/quick_dirty_bayesian_optimization_of_1_and/,-TrustyDwarf-,1449052166,Quick &amp; dirty Bayesian Optimization in [30 lines of code](https://gist.github.com/anonymous/1d08758dc712ac65568d) and a [visualization](http://imgur.com/a/yg0V4) of it optimizing a 1- and 2d function.,8,34
43,2015-12-2,2015,12,2,20,3v4qle,Learning Simple Algorithms from Examples,https://www.reddit.com/r/MachineLearning/comments/3v4qle/learning_simple_algorithms_from_examples/,bubaonaruba,1449056280,,2,12
44,2015-12-2,2015,12,2,21,3v4ued,[question] Sum-Product Nets,https://www.reddit.com/r/MachineLearning/comments/3v4ued/question_sumproduct_nets/,Kiuhnm,1449058743,"I don't see any posts/links about sum-product deep nets on this forum. 

The (first and only) [Deep Learning Book](http://goodfeli.github.io/dlbook/) doesn't even mention them (according to the analytic index).

Why is that? I thought they were promising.",7,5
45,2015-12-2,2015,12,2,23,3v57ur,Which Deep Learning professor is the nicest advisor?,https://www.reddit.com/r/MachineLearning/comments/3v57ur/which_deep_learning_professor_is_the_nicest/,andrewbarto28,1449066048,"I know in general I should choose the one I identify most of their research. But what if he is an asshole? I'd rather be advised by a professor that maybe doesn't do my favorite research (but still I like it very much) and is known to be a very nice person than to be advised by an asshole that will make me cry :)

What do you guys know about LeCun, Bengio, and Schmidhuber? Are they nice with their student? Do they give attention to them?

I have heard that Hinton is a very nice advisor, but he doesn't accept students anymore :(

Maybe I am being too optimist that I can afford to choose between them, but just in case one day I need to make this decision... ;)

Edit: Cross post in quora in case someone want to answer anonymously: https://www.quora.com/Which-Deep-Learning-professor-is-the-nicest-advisor",9,0
46,2015-12-3,2015,12,3,0,3v5jle,Generating Images from Captions with Attention (U Toronto),https://www.reddit.com/r/MachineLearning/comments/3v5jle/generating_images_from_captions_with_attention_u/,[deleted],1449071166,[deleted],0,1
47,2015-12-3,2015,12,3,0,3v5kl1,Text Classification Problmes,https://www.reddit.com/r/MachineLearning/comments/3v5kl1/text_classification_problmes/,alphamystic007,1449071596,"Hi ML

My first post here, I have been investigating some NLP problems recently and I was wondering if I could get your opinions on what are the major problems or headaches in Text Classification at the moment.

I am particularly interested in how cheap and accurate human tagging can help.

Any input is greatly appreciated.",0,0
48,2015-12-3,2015,12,3,2,3v5whv,I taught a computer to write like Engadget,https://www.reddit.com/r/MachineLearning/comments/3v5whv/i_taught_a_computer_to_write_like_engadget/,mrkaiser,1449076021,,5,55
49,2015-12-3,2015,12,3,2,3v5wkq,"Illustration2Vec: A Semantic Vector Representation of Illustrations (paper, code, pre-trained models)",https://www.reddit.com/r/MachineLearning/comments/3v5wkq/illustration2vec_a_semantic_vector_representation/,alxndrkalinin,1449076050,,0,35
50,2015-12-3,2015,12,3,2,3v63dq,Any good blocks tutorial?,https://www.reddit.com/r/MachineLearning/comments/3v63dq/any_good_blocks_tutorial/,[deleted],1449078569,[deleted],2,0
51,2015-12-3,2015,12,3,3,3v6b07,Machine Translation Market to Reach $983.3 Million by 2022,https://www.reddit.com/r/MachineLearning/comments/3v6b07/machine_translation_market_to_reach_9833_million/,deenafeegley,1449081326,,0,1
52,2015-12-3,2015,12,3,3,3v6bz2,Google Cloud Vision API changes the way applications understand images,https://www.reddit.com/r/MachineLearning/comments/3v6bz2/google_cloud_vision_api_changes_the_way/,[deleted],1449081675,[deleted],0,0
53,2015-12-3,2015,12,3,4,3v6lfn,Learning Theano,https://www.reddit.com/r/MachineLearning/comments/3v6lfn/learning_theano/,djc1000,1449085107,"I'm a new redditer so I hope I'm consistent with reddiquette...

I'm getting started with Theano and looking for resources I can go to for advice and discussion more detailed than the tutorials.  I don't want to use the mailing list because I feel like I'd probably waste people's time with too many naive questions.

The issue that started this, is that I'm trying to adapt the deeplearning.net RBM and SdA examples to train on a large high-dimensionality dataset.  Because the data has to be stored as a sparse matrix, when I do is divide it up into chunks of a bunch of multibatches, convert it to dense, and then try to copy it to GPU. 

But, instead of consuming the expected 80% of GPU, its only consuming 3%, and running very, very slowly.  On the theano graph, I see a few GPUFromHost operations and I can't figure out what they are or why they're there. 

So what's the best place to go with questions like this?  Is there an IRC channel?  A theano-oriented discussion board?  

What's the best strategy for going about this?",14,0
54,2015-12-3,2015,12,3,5,3v6qxy,Large-scale Simple Question Answering with Memory Networks (Facebook),https://www.reddit.com/r/MachineLearning/comments/3v6qxy/largescale_simple_question_answering_with_memory/,[deleted],1449087178,[deleted],0,0
55,2015-12-3,2015,12,3,5,3v6s1q,"First technical interview tomorrow, so screwed",https://www.reddit.com/r/MachineLearning/comments/3v6s1q/first_technical_interview_tomorrow_so_screwed/,DancesWithPizzas,1449087561,"I've got my first technical (ever) interview tomorrow. I've been so busy with work and grad school that I have not been able to prepare very much. Yes, I'm very much screwed. Can anyone provided any advice on what to look at if you only have one night! AHHHH, help!

The interviewer said that basic algorithms and machine learning methods will be reviewed. I've taken classes in both.",17,2
56,2015-12-3,2015,12,3,6,3v76ah,16 free e-books on Machine Learning!,https://www.reddit.com/r/MachineLearning/comments/3v76ah/16_free_ebooks_on_machine_learning/,Shoresh92,1449092981,,0,0
57,2015-12-3,2015,12,3,7,3v79wu,Multi GPU For Multi RNNs in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3v79wu/multi_gpu_for_multi_rnns_in_tensorflow/,LeavesBreathe,1449094391,,7,22
58,2015-12-3,2015,12,3,7,3v7gec,"[question] How does the given algorithm incorporate machine learning techniques??? (no code included, but there is a description)",https://www.reddit.com/r/MachineLearning/comments/3v7gec/question_how_does_the_given_algorithm_incorporate/,wtfisml,1449097014,"   For a report (the final, big report we have in our ""big data"" class at my uni), we had to use Hadoop with a machine learning algorithm of our choice (random forest, L-SVM, etc). He had provided code for what he calls ""random forest with hadoop,"" and we were essentially supposed to simply modify this code to run with our dataset, and compare it to a version of the same code that doesn't incorporate Hadoop. 

   That being said, he gave us some R code with a mapreduce call in it and said basically, ""here is random forest with Hadoop, compare with a Hadoop-less version."" Since the class is moreso about Hadoop and HDFS than machine learning, I can't exactly tell whether this code actually does what random forest does. Without posting the code itself (afraid of copyright issues since it's in his textbook), I want to describe what some of this code does. 

   Random Forest (mapper):
The mapper seems to be creating two bootstrap samples, combining them with rbind(), assigning the bootstrap sample index to the key and the combined samples themselves as the val.

   RF (reducer): The reducer is where the code takes one feature--specified by the coder--and compares each entry in the  feature against the feature mean, assigning a class to that feature (either a 0 or 1 for his example), which I have modified to a feature in question for my dataset, and introduced one more possible class. The original feature values are then replaced with their assigned classes, and... That's it. 

   The rest of the code: The rest of the code is mine, and is only for the purpose of debugging: it takes the output, compares those assigned classes against the actual classes, and calculates ""information gain."" 

   The bootstrap samples are never used anywhere else in his code, no bagging is apparent, only one feature is used, and the metric by which that feature is classified is hard-coded and, seemingly, specific to that particular example dataset. No ""trees"" are made, and there is certainly no vote tally function for said trees' outputs. 

   The code that performs this function isn't supposed to be some incomplete ""fill-in-the-blanks"" type of situation; rather, it's straight out of a textbook he wrote on the subject that he is using as an example of code for random forest, to help describe how it works. I'm at a loss--I don't understand how this at all accomplishes what random forest is supposed to accomplish, seeing as there is no ""training, validation, and testing""--the algorithm simply uses mapreduce to compare observation attributes against the feature mean. 

   His decision tree example seems to be ridden with manual coder-specified decisions as well, such as a mapper with comments like   
""#split needed on left side""    
(code)    
""#split not needed on the right side""    
(code)

   
   Shouldn't these algorithms be doing that work for me? (Thereby making the algorithms themselves much more complex?) When I use a package like Rpart, it generates a decision tree from my data, with little else to go on that I specify! 

   Once again, sorry for my ineptitude, I am not a machine learning programmer, though I *would* like to learn how these approaches work, instead of scratching my head at how a couple of manual comparisons qualify as ML. 

Tl;dr: My professor gave us code for ""machine learning"" (random forest, decision tree) with hadoop that is suspiciously riddled with manually made decisions about the data/no apparent tree/forest structure (though I am not a machine learning programmer, so my scrutiny is nil).

Edit: tldr, formatting    
Edit 2: I realized that the bootstrapping code was actually making multiple bootstrap samples, but the code just isn't going any further with them.",1,0
59,2015-12-3,2015,12,3,9,3v7qmr,"A biologist, a mathematician, and a computer scientist walk into a footer - Ryan Adams on collaborations between biology and ML",https://www.reddit.com/r/MachineLearning/comments/3v7qmr/a_biologist_a_mathematician_and_a_computer/,[deleted],1449101600,[deleted],0,1
60,2015-12-3,2015,12,3,9,3v7qro,"A biologist, a mathematician, and a computer scientist walk into a foobar - Ryan Adams on collaborations between biology and ML",https://www.reddit.com/r/MachineLearning/comments/3v7qro/a_biologist_a_mathematician_and_a_computer/,beamsearch,1449101659,,0,51
61,2015-12-3,2015,12,3,11,3v88ej,relative layer sizes for lstm networks question,https://www.reddit.com/r/MachineLearning/comments/3v88ej/relative_layer_sizes_for_lstm_networks_question/,butWhoWasBee,1449109674,"For lstm networks with multiple hidden layers of lstm nodes, do you generally find it is better for all the layers to be the same size, or to to decrease in size, or some other configuration? I understand the answer will vary with the data being trained on, but any insight is appreciated! 

Edit: some background, I am predicting a discrete variable that can have 40 states. I am using one hot encoding, so my network has 40 inputs and 40 outputs",0,2
62,2015-12-3,2015,12,3,11,3v8al3,Best Way to Learn?,https://www.reddit.com/r/MachineLearning/comments/3v8al3/best_way_to_learn/,lavabender,1449110667,"I'm a freshman CS major (please hold your chuckles), and I've been looking into the theory (bayesian probability, gradient descent, loss functions, etc.) behind some of the main elementary machine learning models like kNN, naive Bayes, linear SVM, and ANNs, but I've been waiting to implement anything with them because I always feel like I don't fully grasp the theory yet. I've done Calc 3 as well as Linear Algebra, so I think I've got the fundamentals down, so my question is: do you recommend getting into the more intricate aspects of theory before implementing anything, or would it be more useful to get some practical experience as long as I have a rough understanding and delve into the specifics as I need to?",3,2
63,2015-12-3,2015,12,3,12,3v8gj6,Is decision tree gradient boosting just an average of weak classifiers?,https://www.reddit.com/r/MachineLearning/comments/3v8gj6/is_decision_tree_gradient_boosting_just_an/,[deleted],1449113417,[deleted],0,1
64,2015-12-3,2015,12,3,12,3v8h25,Actor-Mimic Deep Multitask and Transfer Reinforcement Learning (U Toronto),https://www.reddit.com/r/MachineLearning/comments/3v8h25/actormimic_deep_multitask_and_transfer/,[deleted],1449113682,[deleted],0,1
65,2015-12-3,2015,12,3,12,3v8ipy,Rethinking the Inception Architecture for Computer Vision,https://www.reddit.com/r/MachineLearning/comments/3v8ipy/rethinking_the_inception_architecture_for/,cesarsalgado,1449114473,,12,27
66,2015-12-3,2015,12,3,13,3v8mr9,PAKDD 2016 Contest,https://www.reddit.com/r/MachineLearning/comments/3v8mr9/pakdd_2016_contest/,DM_walker,1449116424,"Sarcasm detection on Reddit comments
http://www.parrotanalytics.com/pacific-asia-knowledge-discovery-and-data-mining-conference-2016-contest/

Duration: from 25th November 2015 to 15th February 2016

Particular features:

1) participants may submit solutions as teams made up of one or more persons; 

2) each team needs to designate a leader responsible for communication with the Organizers;

3) a single person can be a leader of only one team;

4) one person may be incorporated in maximally 3 teams;

5) each team needs to be composed of a different set of persons.
",6,1
67,2015-12-3,2015,12,3,13,3v8nz3,Machine learning and big data know it wasn't you who just swiped your credit card,https://www.reddit.com/r/MachineLearning/comments/3v8nz3/machine_learning_and_big_data_know_it_wasnt_you/,john_philip,1449117005,,0,1
68,2015-12-3,2015,12,3,20,3v9q99,Chemical structure extraction from images with deep learning : xpost /r/deeplearning,https://www.reddit.com/r/MachineLearning/comments/3v9q99/chemical_structure_extraction_from_images_with/,eleitl,1449142670,,0,1
69,2015-12-3,2015,12,3,20,3v9qmb,"My extensive thesis from last year about using deep learning for facial analysis. Basic, but nice experiments with network hyperparameters. Maybe some of you will find this useful/interesting in some ways. [Warning: Super pretty figures and plots]",https://www.reddit.com/r/MachineLearning/comments/3v9qmb/my_extensive_thesis_from_last_year_about_using/,nomaderx,1449142934,,11,40
70,2015-12-3,2015,12,3,22,3v9zsw,"MNIST digit classifier (python based) - a lot of improvements on the way, neural network implemented using numpy arrays, no usage of readymade objects from libraries like sklearn or theano.",https://www.reddit.com/r/MachineLearning/comments/3v9zsw/mnist_digit_classifier_python_based_a_lot_of/,[deleted],1449149004,[deleted],1,1
71,2015-12-3,2015,12,3,22,3va12e,Check out my Deep RNN pet project,https://www.reddit.com/r/MachineLearning/comments/3va12e/check_out_my_deep_rnn_pet_project/,harponen,1449149699,"Just published my little Deep Recurrent Neural Network module. It has an RBM layer on top! 


I used it to generate some text and polyphonic music. Works quite well!


Please read the post for details here:
http://www.heikkiarponen.net/2015/12/introducing-derprnn.html


Also listen to some example on soundcloud:
https://soundcloud.com/omega-dais/sample1


WTF can't link to github?!?! OK it's at github.com/harpone/DerpRNN
",8,4
72,2015-12-3,2015,12,3,22,3va2b6,"[Help] What are some useful tips for coping up with a large number of research papers or, How to weed out superfluous claims and results?",https://www.reddit.com/r/MachineLearning/comments/3va2b6/help_what_are_some_useful_tips_for_coping_up_with/,loopnn,1449150367,"I don't know about you, but I'm inundated in a plethora of papers on Deep Learning coming out left and right. Even if I constrict myself to my small domain only (NLP), still there a lot of them. And there are all sorts of new things. Some work here, some work there.

I really am losing my mind in deciding what to read and review, what to try out, what to spend time on..


Everyone wants to beat, outperform, state the state of the art... I'm lost..

So, the pros out there, how the hell do you manage to get the signal out of this much noise? Or, am I just a loser and actually there is less noise than I think there is?


Thanks. 
",18,20
73,2015-12-4,2015,12,4,0,3vagrh,Unclearness on A Neural Algorithm of Artistic Style,https://www.reddit.com/r/MachineLearning/comments/3vagrh/unclearness_on_a_neural_algorithm_of_artistic/,carpedm20,1449157257,"I recently read this paper and code implementation but I can't fully understanding of the meaning of this sentence ""The five style reconstructions in Fig 1 were generated by matching the style representations on layer conv1 1 (a), conv1 1 and conv2 1 (b), conv1 1, conv2 1 and conv3 1 (c), conv1 1, conv2 1, conv3 1 and conv4 1 (d), conv1 1, conv2 1, conv3 1, conv4 1 and conv5 1 (e)."". What I understand from equation 3 is that we use inner product of feature activation in a single layer but the mentioned sentence seems to explaining how they calculate the style loss between(?) different layers. Am I understand this sentence in a wrong way or ... I need a clear explanation about the meaning of the sentence",1,0
74,2015-12-4,2015,12,4,3,3vb1hs,Towards Understandable Neural Networks for High-Level AI Tasks,https://www.reddit.com/r/MachineLearning/comments/3vb1hs/towards_understandable_neural_networks_for/,KeponeFactory,1449165626,,0,4
75,2015-12-4,2015,12,4,3,3vb9wz,Machine Learning Trick of the Day (6): Tricks with Sticks,https://www.reddit.com/r/MachineLearning/comments/3vb9wz/machine_learning_trick_of_the_day_6_tricks_with/,skrza,1449168872,,0,6
76,2015-12-4,2015,12,4,4,3vbefp,Two online sequence to sequence learning algorithms submitted to ICLR 2016,https://www.reddit.com/r/MachineLearning/comments/3vbefp/two_online_sequence_to_sequence_learning/,kaiser_nkf,1449170639,"CTC-based one: Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification

http://arxiv.org/abs/1511.06841


Attention-based one: An Online Sequence-to-Sequence Model Using Partial Conditioning

http://arxiv.org/abs/1511.04868

Pros and cons?",1,8
77,2015-12-4,2015,12,4,4,3vbg0o,Machine Learning for Image Analysis Basics to Advanced,https://www.reddit.com/r/MachineLearning/comments/3vbg0o/machine_learning_for_image_analysis_basics_to/,chuckatx,1449171254,[removed],0,1
78,2015-12-4,2015,12,4,5,3vbkxk,Novice question: what to do when my autoencoder doesn't learn useful features?,https://www.reddit.com/r/MachineLearning/comments/3vbkxk/novice_question_what_to_do_when_my_autoencoder/,reddit_tl,1449173178,"My setup: I have around 7000 images of size 64X64. They are black and white images.I am using the Theano stacked denoised autoencoder to train it. I have tried different size of the first layer (from 100 to 2000 neurons) and I have tried to train it with 1000 epochs. The end result is that the mean cross entropy loss if above 2000, which is very bad I assume. I added some momentum in hope of improving the optimization. It helped, just a little. At this point, I don't know how to improve the training further.

Example images (original and reconstructed) are shown in the links.

https://groups.google.com/d/msg/theano-users/ncLi5Gj8WUo/gm1zi8B3DAAJ
Any tips on how I may improve the situation?",8,3
79,2015-12-4,2015,12,4,5,3vbnfx,Brainstorming machine learning opportunities for consumer good part failures?,https://www.reddit.com/r/MachineLearning/comments/3vbnfx/brainstorming_machine_learning_opportunities_for/,gary_feesher,1449174152,"Let's say that my friend works at a company that manufacturers electric hair clippers.

This company generates data from testing stations at the end of the assembly line, generates data from quality assurance testing, and generates data in the reliability testing lab which runs the hair clippers until they fail.

We also have customer return data and customer complaint data that warrants a replacement hair clipper or to have the customer send in their clipper for repair.

The common denominator (like, a primary key) across all of these databases is the defective component within the clipper and why that component had to be replaced or addressed (think, the condition of the component).  

There is some pressure from our upper management to develop a model that ""predicts potential failures in the customer's home, based on signals seen in all of these labs.""  It is definitely easier said than done, but I really have no place to start.

Based on what I have provided, what would be some first steps toward the objective laid out by the upper management?

",2,0
80,2015-12-4,2015,12,4,5,3vbo6y,"Dato Winners' Interview: 1st place, Mad Professors (4 level StackNet, XGBoost)",https://www.reddit.com/r/MachineLearning/comments/3vbo6y/dato_winners_interview_1st_place_mad_professors_4/,alxndrkalinin,1449174458,,7,39
81,2015-12-4,2015,12,4,5,3vbouq,[HELP] Image ML from regular ML,https://www.reddit.com/r/MachineLearning/comments/3vbouq/help_image_ml_from_regular_ml/,chuckatx,1449174720,"I am a graduate student in ATX and my research is based on application of machine learning in the energy industry. I cannot delve into the specifics but I have been using traditional machine learning i.e. NN, RFs, SVms etc. and I am proficient in R .
One subject matter that I wish to delve into is the application of machine learning to rock core samples for detection of various attributes (porosity, fractures etc.). I've never dealt with images before.
I wanted a good tutorial to begin analysis with images in machine learning. The problem with rock core's are that they generally BW.
On another note, I currently use R for such analysis should I start looking into python or tensor flow?
Cheers",1,1
82,2015-12-4,2015,12,4,6,3vbwnp,Counseling and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3vbwnp/counseling_and_machine_learning/,[deleted],1449177713,[deleted],0,1
83,2015-12-4,2015,12,4,6,3vbxsg,Counseling and machine learning,https://www.reddit.com/r/MachineLearning/comments/3vbxsg/counseling_and_machine_learning/,arthomas73,1449178162,,1,0
84,2015-12-4,2015,12,4,7,3vc68l,Deep Visual Analogy-Making,https://www.reddit.com/r/MachineLearning/comments/3vc68l/deep_visual_analogymaking/,vonnik,1449181476,,0,15
85,2015-12-4,2015,12,4,7,3vc7gw,Has anybody here installed Caffe on windows?? I just need some help.,https://www.reddit.com/r/MachineLearning/comments/3vc7gw/has_anybody_here_installed_caffe_on_windows_i/,truthseeker1990,1449182004,,8,2
86,2015-12-4,2015,12,4,8,3vch3q,Tutorial  Getting Started with GraphLab For Machine Learning in Python,https://www.reddit.com/r/MachineLearning/comments/3vch3q/tutorial_getting_started_with_graphlab_for/,pacmanisfun,1449186090,,0,0
87,2015-12-4,2015,12,4,10,3vcsjg,Machine Learning: Feedback on the approach described below to evaluate recommend engines (quality of recommendations) based on implicit feedback?,https://www.reddit.com/r/MachineLearning/comments/3vcsjg/machine_learning_feedback_on_the_approach/,nikunjbjj,1449191082,"Context: Something like Amazon's ""You may also like"" widget which usually means people who liked this product also liked products x, y, z. 


Proposed Evaluation Approach: Obtain a list of products browsed by users in a small time frame (say half an hour) and compute observed co-visitations for pairs of products and compare it with predicted co-visitations from pre-generated recommendations.


For instance if users u1, u2 viewed the following products- 



u1: [p1, p2, p3],


u2: [p2, p3]


Then the co-visitation counts are:


(p1, p2): 1,


(p1, p3): 1,


(p2, p1): 1,


(p2, p3): 2,


(p3, p1): 1,


(p3, p2): 2


Assume the following recommendations were generated prior to the observed timeframe.


p1: [p2, p4, p5, p6],


p2: [p1, p3, p8],


p3: [p9, p10, p11],


From the above observed counts and predictions, we compute two numbers- 


count_success and count_total. 


count_total = 1+1+1+2+1+2 = 8 product pairs considered


Lets iterate over the observed co-visits: 


p2 is in the recommendation list of p1 so count_sucess = 1,


p3 not in recommendation list of p1 so count_success remains 1,


p1 is in the recommendation list of p2 so count_sucess = 2,


p3 is in the recommendation list of p2 so count_sucess = 4 (because viewed twice),


p1 and p2 not in recommendation list of p3 so count_success remains 4. 


Therefore co-visitation recall accuracy = count_success/count_total = 0.5.


This is the basic idea, further we can consider other factors like - position in the recommendation list, normalizing for trending products' views, pruning anomalies etc.",1,5
88,2015-12-4,2015,12,4,13,3vdm98,2 in 1 shrink packing machine,https://www.reddit.com/r/MachineLearning/comments/3vdm98/2_in_1_shrink_packing_machine/,dongfengpacking,1449204739,,1,1
89,2015-12-4,2015,12,4,15,3vduiy,Help with TimeSeries Modeling / ARMA methods,https://www.reddit.com/r/MachineLearning/comments/3vduiy/help_with_timeseries_modeling_arma_methods/,chain20,1449209038,"Hi r/machinelearning,

I am trying to model a timeseries generated by a temperature sensor that looks like [this](http://i.imgur.com/QPUGHkG.png).

I am very new to timeseries modeling and after reading a bunch of articles, tutorials and some papers, I generated the [ACF](http://i.imgur.com/HW184b6.png) and [PACF](http://i.imgur.com/GCGMAxd.png) for analysis.

For my first attempt, I fit an ARMA(2,2) model using the statstools python package. The mean and variance of the residuals are -0.0019 and 3.535 respectively. The variance seems to be very high compared to what is expected (most texts say the residuals should follow a unit normal distribution). This is what the plot of [fitted values vs residuals](http://i.imgur.com/G0euCFq.png) and [residual histogram](http://i.imgur.com/b0iIQwr.png) looked like.

Here are my questions:

1. How do I interpret such an ACF and PACF? I guess the PACF is simple, but the ACF has high values for a large number of lags.

2. How do I select the best model to fit the timeseries? Is there any hope of ARMA (or ARIMA) fitting this series well? Is there anything better?

3. Is the residual variance too high? How do I interpret the residuals given the scatter plot and histogram.

I apologize for the long-ish post, but any help would be appreciated.",12,3
90,2015-12-4,2015,12,4,15,3vdvi1,MXNet.JS: MXNet Javascript Package for Deep Learning Prediction API in Browser,https://www.reddit.com/r/MachineLearning/comments/3vdvi1/mxnetjs_mxnet_javascript_package_for_deep/,antinucleon,1449209587,,8,16
91,2015-12-4,2015,12,4,17,3ve8s5,Advice on modelling xml documents with recursive recurrent NNs,https://www.reddit.com/r/MachineLearning/comments/3ve8s5/advice_on_modelling_xml_documents_with_recursive/,robclouth,1449218648,"Hi folks. I have a bunch of xml documents that I'd like to model with the aim of generating new ones, like the amazing recurrent NN experiments by Andrej Karpathy. But since they are tree-structured documents that can be quite deep, I don't think character-level recurrent NNs are the best way because recurrent NNs are designed for linear prediction. Reading up on the web suggests that recursive recurrent NNs might be the way to go. They have recently been used in language modelling for capturing the tree like structure of syntax. Can anyone offer any suggestions (and hopefully implementations) to get me started? Thanks ",10,5
92,2015-12-4,2015,12,4,19,3vego7,What's the difference between loading mnist.hdf5 manually and with MNIST() in blocks?,https://www.reddit.com/r/MachineLearning/comments/3vego7/whats_the_difference_between_loading_mnisthdf5/,[deleted],1449224981,[deleted],1,0
93,2015-12-4,2015,12,4,21,3veny1,EMLI5 support vector machines,https://www.reddit.com/r/MachineLearning/comments/3veny1/emli5_support_vector_machines/,[deleted],1449230495,,7,19
94,2015-12-4,2015,12,4,21,3vepq8,3 Reasons Why Machine Learning is Not a Cybersecurity Pipe Dream,https://www.reddit.com/r/MachineLearning/comments/3vepq8/3_reasons_why_machine_learning_is_not_a/,Brad1Mav5,1449231707,,0,1
95,2015-12-4,2015,12,4,21,3veqbp,machine learning with digital circuit,https://www.reddit.com/r/MachineLearning/comments/3veqbp/machine_learning_with_digital_circuit/,godspeed_china,1449232131,"Hi, I'm testing a new model: a recurrent network whose unit is a logical gate with two inputs. There are 16 types of such gate according truth table. The input to the circuit is the binary representation of IEEE float. For a sample with 3 features, the network receives 32*3=96 bit, which turns on/off 96 gates. Then the network runs according to logical rule. After certain iterations, I collect the output from 32 gates forming an IEEE float output. The loss function is the squared loss. I use genetic algorithm to optimize the network and have some successes. The cool thing is that everything is bit and Boolean logic. The model can be physically burnt onto silicon. What do you think about this type of models?",6,3
96,2015-12-4,2015,12,4,23,3vf24b,(Deepmind) Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions,https://www.reddit.com/r/MachineLearning/comments/3vf24b/deepmind_deep_reinforcement_learning_with/,InaneMembrane,1449238831,,0,22
97,2015-12-4,2015,12,4,23,3vf5rb,Newbie question: Why does char RNN use one dimension per character type rather than a single dimension to encode all characters?,https://www.reddit.com/r/MachineLearning/comments/3vf5rb/newbie_question_why_does_char_rnn_use_one/,robclouth,1449240571,"Why a=[1, 0, 0, ...], b=[0, 1, 0, ...], c=[0, 0, 1, ...] instead of a=[0], b=[0.1], c=[0.2]?
Sorry this might seem like a silly question. I'm just trying to figure some stuff out in my head.
",15,7
98,2015-12-4,2015,12,4,23,3vf6rn,"Machine learning works spectacularly well on big data, but mathematicians arent quite sure why",https://www.reddit.com/r/MachineLearning/comments/3vf6rn/machine_learning_works_spectacularly_well_on_big/,mtnchkn,1449241051,,0,0
99,2015-12-5,2015,12,5,2,3vfo5k,Full solutions for Bishop's Pattern Recognition and Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3vfo5k/full_solutions_for_bishops_pattern_recognition/,Nimitz14,1449248540,Can't access them online without some code that I don't have. There are some derivations I'm not following.,6,5
100,2015-12-5,2015,12,5,5,3vgirs,Google releases Cloud Vision API,https://www.reddit.com/r/MachineLearning/comments/3vgirs/google_releases_cloud_vision_api/,Thesandlord,1449260847,,44,183
101,2015-12-5,2015,12,5,5,3vgjau,Switching from statistics to ML?,https://www.reddit.com/r/MachineLearning/comments/3vgjau/switching_from_statistics_to_ml/,[deleted],1449261068,[deleted],2,1
102,2015-12-5,2015,12,5,6,3vgp8m,Hyper-parameter Ranges for Sparse and Denoising Autoencoders,https://www.reddit.com/r/MachineLearning/comments/3vgp8m/hyperparameter_ranges_for_sparse_and_denoising/,tyrannouswalnut,1449263406,"I'm attempting to determine how sensitive sparse autoencoders and denoising autoencoders are to their hyperparameter values. I want to do this by doing a grid search of various hyperparameter values and measuring their effects on accuracy of classification on the MNIST data set. The trouble I'm having is with finding the ranges of values (as well as the increments for these values) that I should use for the hyperparameters.

For example, in the UFLDL tutorial for sparse autoencoders there's rho, lambda, and beta for the major hyperparameters. The values they give for these are 0.01, 0.0001, and 3 respectively. My instinct is to use these as sorts of middle values and take 5 or so values in each direction linearly (i.e. 0.005, 0.006, 0.007, 0.008, 0.009, 0.01, 0.011, 0.012, 0.013, 0.014, and 0.015 for rho) but I know that I shouldn't just use my instinct for this. There's also the possibility for log scales for any of these hyperparameters as well.

Essentially, my question is does anyone know where I could look to find reasonable ranges of hyperparameter values and their corresponding increment values for sparse autoencoders and denoising autoencoders?",6,1
103,2015-12-5,2015,12,5,7,3vgy2b,[Question] Why are most published DL algorithms using a single input representation?,https://www.reddit.com/r/MachineLearning/comments/3vgy2b/question_why_are_most_published_dl_algorithms/,bubaonaruba,1449267053,"For instance let's take simple 1D time series data. One can imagine many transformations/representations of the input that could be simultaneously fed into a NN. E.g.:

* simple 1d vector with values
* 1d vector of deltas
* 1d vector of log deltas
* 2D plot of the series
* Markov Transition Matrix
* Fourier Transform etc.
* Polar coordinates etc.
* and many more...

Are we counting that the NN can discover features as well or better from a single representation than when trained on multiple? Are there any inherent problems to multi-representation input?

For research purposes, when the goal is automatic feature engineering, multiple input representations don't make sense. But what about practical purposes?",8,2
104,2015-12-5,2015,12,5,9,3vhgkc,"In a multi-class classification, what do you do when one of your classes perform poorly",https://www.reddit.com/r/MachineLearning/comments/3vhgkc/in_a_multiclass_classification_what_do_you_do/,fireboltkills,1449275377,"In a multi-class classification, what do you do when one of your classes perform poorly when compared to the other classes. 
By performance, I mean all metrics namely:

* Precision
* Recall
* F1-Score
* AUC",2,3
105,2015-12-5,2015,12,5,13,3viapy,Regression instead of classification ?,https://www.reddit.com/r/MachineLearning/comments/3viapy/regression_instead_of_classification/,PyBet,1449291524,"I am reading this thesis, in which numerical features are used to train and predict 3 classes: A, B, and C. 

Instead of using classification, a SVR is used and he assigns 0, 0.5 and 1 to A, B, and C.

Then he takes the results from the regression and derives the ranges for A, B and C.

Example: 0 &lt;= A &lt; 0.6, 0.6&lt;= B &lt;0.7, 0.7&lt;= C

Why does he uses regression instead of classification ?
Are there any advantages ?",5,1
106,2015-12-5,2015,12,5,23,3vjhik,Getting a perceptron to pass XOR,https://www.reddit.com/r/MachineLearning/comments/3vjhik/getting_a_perceptron_to_pass_xor/,josephottinger,1449324301,"I've been playing with writing an AI library for Java, focusing on classification with bayesian algorithms and a perceptron, largely going off of Toby Segaran's ""Programming Collective Intelligence.""

However, I have yet to get the perceptron to pass the XOR problem successfully.

Right now, the library is available on github (http://github.com/jottinger/ml, branch v5). It's not in final form; I'm mostly trying to get the classification to work.

So what's happening, you ask.

Well, I have it being trained with the following data:

ITRUE, XTRUE = OFALSE
ITRUE, XFALSE = OTRUE
IFALSE, XTRUE = OTRUE

And what I'm expecting is that when I query with IFALSE, XFALSE, I should get OFALSE.

I *think* the problem is that there's no hidden node being generated for IFALSE and XFALSE, because the strengths aren't being set right -- but the truth is, I don't know. My understanding is that a node with new information (the two ""false"" inputs) would have a ""neutral"" result, whereas the perceptron should already know about a series of negative results for the other queries.

Can anyone shed light on what I'm doing wrong or what I'm not understanding?",22,0
107,2015-12-6,2015,12,6,1,3vjx1b,How do you measure distance between vectors with components with differing orders of magnitude?,https://www.reddit.com/r/MachineLearning/comments/3vjx1b/how_do_you_measure_distance_between_vectors_with/,[deleted],1449332665,[deleted],0,1
108,2015-12-6,2015,12,6,3,3vkcz5,Recurrent Neural Networks basics,https://www.reddit.com/r/MachineLearning/comments/3vkcz5/recurrent_neural_networks_basics/,[deleted],1449339871,[deleted],0,1
109,2015-12-6,2015,12,6,3,3vkg2f,[1511.06410] Better Computer Go Player with Neural Network and Long-term Prediction,https://www.reddit.com/r/MachineLearning/comments/3vkg2f/151106410_better_computer_go_player_with_neural/,downtownslim,1449341200,,8,35
110,2015-12-6,2015,12,6,4,3vkl1w,calculating new information content in document compared to collection of documents,https://www.reddit.com/r/MachineLearning/comments/3vkl1w/calculating_new_information_content_in_document/,stockManstockMan,1449343318,"Say I have a large collection of text including multiple documents. I also have a ""new"" document, but I'm lazy and I don't want to read it unless it contains some significant information I haven't already read. Is there a way to calculate the extent to which this document contains new information that is not included in the collection?",2,0
111,2015-12-6,2015,12,6,5,3vktp4,Seq2Seq Generative Adversarial Net,https://www.reddit.com/r/MachineLearning/comments/3vktp4/seq2seq_generative_adversarial_net/,LeavesBreathe,1449346926,"Hey Guys,

Skip Thoughts Seq2seq and GAN's are great, but I'm wondering: Has anyone combined these two together? It seems like a logical next step to take, but I can't find any papers on it. 

The [conditional adversarial net](http://arxiv.org/pdf/1411.1784v1.pdf) talks about how this can be applied to NLP. 

I'm fairly new to GAN's but I'm wondering if they can be conditioned with attention (like seq2seq is currently). In summary, the conditional GAN would have:

1. Attention
2. Sequence to Sequence Generator (sentence to sentence)
3. LSTM/GRU/uRNN for the Generator and Discriminator
4. [Stabilizing Activations](http://arxiv.org/pdf/1511.08400.pdf) in generator

Is this possible? Is there something I'm missing? Before trying all of this out, I thought I would get some opinions on this. Thanks!


",9,7
112,2015-12-6,2015,12,6,6,3vl2gq,What is the state of the art in computing semantic similarity between passages?,https://www.reddit.com/r/MachineLearning/comments/3vl2gq/what_is_the_state_of_the_art_in_computing/,stockManstockMan,1449350686,"I asked a previous question on this topic, this is how I should have phrased it.",4,1
113,2015-12-6,2015,12,6,6,3vl4kz,Attribute2Image: Conditional Image Generation from Visual Attributes,https://www.reddit.com/r/MachineLearning/comments/3vl4kz/attribute2image_conditional_image_generation_from/,certainly_not_jesus,1449351646,,1,8
114,2015-12-6,2015,12,6,7,3vl9wz,Masters in Europe,https://www.reddit.com/r/MachineLearning/comments/3vl9wz/masters_in_europe/,forloopsarebad,1449354029,"Hello reddit,

So I'm in the final bachelor year now and looking for a masters. My diploma will probably be on generating music using rnn's to give you some background on my interests.

Does anybody have advice for the top ML programmes in Europe? Ideally, I'd love to go some place where they combine said ML degree with applications in music. What I've found so far is Edinburgh which seems rather interesting. They seem to have done something with deep learning and music before but it might get too expensive.

I've seen QMUL has some music related programmes where ML could be applied, but it's more geared towards music itself + it's in London so extra expensive.

If I don't take UK into account because of expenses and I also have to take out Switzerland for the same reason. Germany as well, mostly because I don't see myself spending two years there. So what I'm left with is Holland or one of the nordic countries. University of Amsterdam seems to have a relatively good AI programme, but there's nothing related to music around there, KTH in Stockholm seems to also have ML, but with a big emphasis on speech. Aalto seems to also have decent ML, but  again nothing special regarding music. Didn't really see much in Denmark besides a Sound and Music computing programme at Aalborg which has an interesting content but the university itself is pretty poorly rated.

Does anybody have any experience with any of these? Or just anything ML and/or music in Europe? I'm kinda torn between choosing one, but I'm leaning towards ML. If I do a music one I'd still want to apply ML to what I learn there, but it would improve my DSP background.

P.S. US is ruled out just because of the sheer cost of it, I may consider it for a PhD later, but right now I'm not ready to commit to it.",10,0
115,2015-12-6,2015,12,6,7,3vl9yy,A Practical Guide to Planning and Executing ML Projects in Industry,https://www.reddit.com/r/MachineLearning/comments/3vl9yy/a_practical_guide_to_planning_and_executing_ml/,blowjobtransistor,1449354057,,9,77
116,2015-12-6,2015,12,6,10,3vlwix,Variational Autoencoder questions,https://www.reddit.com/r/MachineLearning/comments/3vlwix/variational_autoencoder_questions/,LyExpo,1449364573,"I'm struggling to understand the training process for the case when p(x|z) is continuous, in particular p(x|z) is N( mu(z), sigma(z) ). Here is an example architecture:

x
h = tanh(W1x + b1)
mu = W2h + b2
log_sigma = 0.5 * (W3h +b3)
z = mu + log_sigma * noise
d = tanh(W4z + b4)
mu_d = W5d + b5
log_sigma_d = 0.5 * (W6d + b6)

So p(x|z) is normally distributed with mean mu_d and standard deviation exp{log_sigma_d} ? I *think* I'm ok up to here. If so, this gives us the approximation log p(x|z) for the reconstruction part of the cost, and I'm ok with calculating the the KL regularization term. 

Now what about actually getting a reconstructed version of the input x? Do we input x into the network and get back mu_d and log_sgima_d, then generate a sample from the N(mu_d, sigma_d) distribution? I think I'm a bit confused at this step because in the Bernoulli case the network will actually give us the reconstructed input directly, not just the parameters of some distribution. Is this correct?

Anything else wrong here?

",21,3
117,2015-12-6,2015,12,6,10,3vm0ws,[Question/Help] First Supervised Classification Program,https://www.reddit.com/r/MachineLearning/comments/3vm0ws/questionhelp_first_supervised_classification/,fullfademan,1449366744,"Im working on my first ML project; I am trying to build a really simple web analytics tool. I want to use apache web logs to determine whether or not to issue a coupon to a customer. The web logs contain ip address and browsing history, so I can determine what pages a customer went to and whether or not they bought the product. Using this data as a training set, I hope to build a model that can report to me who I should issue new coupons to, in order to improve sales.

My question is: how can I implement this? Any details I can get will be really helpful. I already have a java program that can take the web logs and split them into individual pieces of data. How do I get this data into a form that is acceptable, and how do I make the program? Its hard for me to find tutorials that are low-level and implementation focused.
Thanks!
",1,1
118,2015-12-6,2015,12,6,13,3vmfvj,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,https://www.reddit.com/r/MachineLearning/comments/3vmfvj/incentivizing_exploration_in_reinforcement/,MrTwiggy,1449374456,,1,2
119,2015-12-6,2015,12,6,15,3vmtyz,Principal component analysis,https://www.reddit.com/r/MachineLearning/comments/3vmtyz/principal_component_analysis/,efavdb,1449383003,,8,13
120,2015-12-6,2015,12,6,15,3vmvsz,Meet up for NIPS 2015,https://www.reddit.com/r/MachineLearning/comments/3vmvsz/meet_up_for_nips_2015/,Turing_Machinegun,1449384291,"EDIT:
New NIPS day has started. Maybe we could do lunch together later. Reddit is not exactly the best platform for real-time stuff so here's my twitter handle: @GCarmantini

EDIT4:
N Y K S Bistro pub for food and beer, welcome to join us! 

EDIT3:
Red scarf in front of 210c fellow redditors, that's the signal ! 

EDIT2:
Running late because of poor spatial skills, but I think I'm here! 

EDIT: 
Just arrived at hotel, finally I have internet access. Ok guys, so the plan is: 7pm at the convention centre, once there we can decide where to go, I'll update this post for the people who want to join us later. 

Hi guys, NIPS is about to start and I am a PhD student who doesn't know (almost) anybody there. I already did this once with cogsci and it was boring in the first few days of the conference before I got to meet some nice people.
Anybody in the same situation? We could meet somewhere on Sunday or Monday and speed this networking thing up. Thought it could be interesting. ",23,32
121,2015-12-6,2015,12,6,16,3vmx1s,Extended Conditional Independence and Applications in Causal Inference,https://www.reddit.com/r/MachineLearning/comments/3vmx1s/extended_conditional_independence_and/,InaneMembrane,1449385200,,0,0
122,2015-12-6,2015,12,6,16,3vn0rn,Using Support Vector Machines to replace OLS? [In R?],https://www.reddit.com/r/MachineLearning/comments/3vn0rn/using_support_vector_machines_to_replace_ols_in_r/,newtorstats,1449387800,"Does anyone have an example of what R code for doing this would look like/a link to an example? Been doing ordinary least squares but it's been suggested to me that gradient boosting, random forest and support vector machines is the ""more right"" way to go about this.

    # Add one column of ""1"" to X
    one.col &lt;- matrix(1, nrow(X), 1)
    X &lt;- cbind(X, one.col)
    # transposing X
    X.t = t(X)
    # X'X, X'Y
    X.t.X &lt;- X.t %*% X
    X.t.Y &lt;- X.t %*% Y
    # (X'X^-1)X'Y
    betahat = solve(X.t.X) %*% X.t.Y",2,0
123,2015-12-6,2015,12,6,19,3vnbkg,Machine Learning for Money,https://www.reddit.com/r/MachineLearning/comments/3vnbkg/machine_learning_for_money/,MLflumadiddle,1449397548,"I was wondering if any of you guys do this sort of stuff for money or have any suggestions/ideas on interesting, niche, independent projects?  

I have a particular interest in games where machine learning and data modeling would give one an advantage.  I recently experimented with Fantasy NBA, but my models (currently) aren't up to par, scoring around 230 when about a minimum of 260 is needed for consistent profits.  

In the past I've also worked on independent projects related to the stock market and poker, with varying degrees of success.  ",5,0
124,2015-12-6,2015,12,6,19,3vndcd,Writing Back Propagation on the Parallella,https://www.reddit.com/r/MachineLearning/comments/3vndcd/writing_back_propagation_on_the_parallella/,nickoppen,1449399285,,2,7
125,2015-12-6,2015,12,6,22,3vnodj,NIPS 2015: What are you most looking forward to?,https://www.reddit.com/r/MachineLearning/comments/3vnodj/nips_2015_what_are_you_most_looking_forward_to/,[deleted],1449408783,[removed],0,1
126,2015-12-6,2015,12,6,23,3vnwum,RNN vs LSTM: Vanishing Gradients,https://www.reddit.com/r/MachineLearning/comments/3vnwum/rnn_vs_lstm_vanishing_gradients/,pranv,1449413849,,22,85
127,2015-12-7,2015,12,7,1,3voaxf,Backpropagation for dummies,https://www.reddit.com/r/MachineLearning/comments/3voaxf/backpropagation_for_dummies/,sachinrjoglekar,1449420827,,0,1
128,2015-12-7,2015,12,7,4,3vowdq,"Hidden Technical Debt in Machine Learning System, Google Inc",https://www.reddit.com/r/MachineLearning/comments/3vowdq/hidden_technical_debt_in_machine_learning_system/,based2,1449429361,,3,38
129,2015-12-7,2015,12,7,4,3vowhz,"Would Tensor Flow be a good tool for this? If not, what would?",https://www.reddit.com/r/MachineLearning/comments/3vowhz/would_tensor_flow_be_a_good_tool_for_this_if_not/,lift_0ff,1449429403,"I'm quite interested in machine learning and artificial neural networks. But I've only done some reading. I've yet to actually try anything.

As a first, hopefully simple, learning project, I'd like to use an ANN to help me buy an airplane. I'm a pilot, and have been wanting to buy one for a while. Simple, old, slow (but safe) airplanes can be bought for under $25,000. I want to use an ANN to find an airplane that presents a good value, one that's priced below market average for it's features.

Part of the problem is that the features aren't clearly defined in the data. The data would contain price, location, make and model name (where make and model names are often non-standard, misspelled, or incorrectly punctuated). The rest of the information you'd use to provide a valuation is in the description, just a big block of non-standardized text.

So, I'd like to feed that data (price, location, make and model, and description paragraph) into an ANN, and have it pop out the records that seemed to be priced far below average, to help me focus my search.

Is this something TensorFlow would be good at? Is there a better tool?",9,0
130,2015-12-7,2015,12,7,5,3vp387,What do the following symbols mean,https://www.reddit.com/r/MachineLearning/comments/3vp387/what_do_the_following_symbols_mean/,FutureIsMine,1449432167,"I'm encountering the following symbols ``N(mk, I/5)`` from a Bayesian Mixture.  The book can be found at [this](https://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf) on page 35 of the pdf (17 in the book). Im wondering what are the following symbols representing, especially the I term. ",2,0
131,2015-12-7,2015,12,7,5,3vp5dv,Chatbot question,https://www.reddit.com/r/MachineLearning/comments/3vp5dv/chatbot_question/,Guybrush1882,1449433038,"I'm looking for some social validation (or lack thereof)...I might have an opportunity to work on an AI chatbot that generates sales leads, mostly for auto dealerships.  The chatbot is called Ava and it's made by Conversica.  Thoughts?",1,0
132,2015-12-7,2015,12,7,5,3vp5fr,Topological Data Analysis  Open Source Implementations,https://www.reddit.com/r/MachineLearning/comments/3vp5fr/topological_data_analysis_open_source/,mmmayo13,1449433058,,0,4
133,2015-12-7,2015,12,7,6,3vpg3l,"Anyone know what happened to the q-learning attention code repository for ""Learning Simple Algorithms from Examples""? It disappeared.",https://www.reddit.com/r/MachineLearning/comments/3vpg3l/anyone_know_what_happened_to_the_qlearning/,[deleted],1449437147,[deleted],0,1
134,2015-12-7,2015,12,7,6,3vpix4,Medicine Related Machine Learning in the Bay Area?,https://www.reddit.com/r/MachineLearning/comments/3vpix4/medicine_related_machine_learning_in_the_bay_area/,freelyread,1449438277,"Are there any groups focusing on how ML can help with diagnosis or other aspects of medicine near San Francisco?  
  
For example, who is doing work on training machines to detect/predict oncology?  
  
Are there any suggestions for those who wish to transfer skills from medicine into ML?  
  
Who has experience with introducing ML into a medical setting? How was the experience and what advice is there on this?",17,6
135,2015-12-7,2015,12,7,7,3vpo0w,How to create a live interactive net?,https://www.reddit.com/r/MachineLearning/comments/3vpo0w/how_to_create_a_live_interactive_net/,dogedickguy,1449440356,"So I'm interested in machine learning, specifically neural nets, and I am curious about how I could use torch or any other framework (preferably torch as ive been working with it mostly) to make a net that is training as it is running, is that possible with torch?  I would really be interested in something like having an image you could interact with in some way, while the neural net interacts with it.",0,0
136,2015-12-7,2015,12,7,8,3vpv4g,backprop through matrix-matrix multiplication,https://www.reddit.com/r/MachineLearning/comments/3vpv4g/backprop_through_matrixmatrix_multiplication/,lioru,1449443417,"In his course Andrej Karpathy gives a simple example to explain back propagation through a matrix-matrix multiplication layer in a neural network: 
http://cs231n.github.io/optimization-2/#mat .

tl;dr:
W.shape = (5,10)
X.shape = (10,3)
W.dot(X).shape = (5,3)

Assuming we're deriving some 1 dimensional loss function L, I worked the example out on paper, and found the dimensions of the derivative of L by X is (1,30) and by W (1,50).

here's my question:

1. Why in back propagation the derivatives end up with the dimension of the variable we're deriving by (like he writes in his example) 
instead of the way we were taught in calculus class?
To derive MM multiplication one needs to treat one of the matrices as a vector, and then the dimensions come out the way i did it.
I assume this is some sort of optimization technique but I haven't seen any proof of this anywhere.

2. In the case of MM multiplication, how is the derivative in the article like the ""classic calculus"" derivative? Why do they both give the same result? 
AFAIK this is the only way to derive matrix-matrix multiplication: http://www4.ncsu.edu/~pfackler/MatCalc.pdf, 
so deriving WX by X, for example, will give a 15x50 matrix, that becomes 1x50 after chain rule (because we're trying to derive by L).

1x50 doesn't equal 5x10...
",1,0
137,2015-12-7,2015,12,7,8,3vpy0o,Top 10 Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/3vpy0o/top_10_machine_learning_algorithms/,vincentg64,1449444666,,0,1
138,2015-12-7,2015,12,7,8,3vpygi,A Statistical View of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3vpygi/a_statistical_view_of_deep_learning/,mmmayo13,1449444862,,5,15
139,2015-12-7,2015,12,7,10,3vqcdr,Deep neural network written from scratch in Julia,https://www.reddit.com/r/MachineLearning/comments/3vqcdr/deep_neural_network_written_from_scratch_in_julia/,jostmey,1449451094,,15,43
140,2015-12-7,2015,12,7,10,3vqdxq,Application of the Laminar Navier-Stokes Equations for Solving 2D and 3D Pathfinding Problems with Static and Dynamic Spatial Constraints. Implementation and validation with Finite Element Methods.,https://www.reddit.com/r/MachineLearning/comments/3vqdxq/application_of_the_laminar_navierstokes_equations/,tatitomate,1449451816,,0,2
141,2015-12-7,2015,12,7,11,3vqibp,HP 130 coding machine,https://www.reddit.com/r/MachineLearning/comments/3vqibp/hp_130_coding_machine/,dongfengpacking,1449453860,,1,1
142,2015-12-7,2015,12,7,11,3vqjf8,How Much Memory Does A Data Scientist Need?,https://www.reddit.com/r/MachineLearning/comments/3vqjf8/how_much_memory_does_a_data_scientist_need/,dmpetrov,1449454397,,0,2
143,2015-12-7,2015,12,7,11,3vqm6h,"Batch Normalization ""representation power""",https://www.reddit.com/r/MachineLearning/comments/3vqm6h/batch_normalization_representation_power/,rd11235,1449455621,"""we make sure that the transformation inserted in the network can represent the identity transform...

we introduce, for each activation x, a pair of parameters gamma, beta, which scale and shift the normalized value...

Indeed, by setting gamma = ... and beta = ..., we could recover the original activations, if that were the optimal thing to do.""

This seems off to me. Wouldn't we need separate gamma, beta values for each batch for this statement to be true, which is not what's done? What am I missing?",7,0
144,2015-12-7,2015,12,7,11,3vqot7,HP 241 coding machine,https://www.reddit.com/r/MachineLearning/comments/3vqot7/hp_241_coding_machine/,dongfengpacking,1449456833,,1,1
145,2015-12-7,2015,12,7,12,3vqwl3,"Hinton ""kinship"" network in Keras doesn't converge?",https://www.reddit.com/r/MachineLearning/comments/3vqwl3/hinton_kinship_network_in_keras_doesnt_converge/,dcgrigsby,1449460429,"I'm attempting to re-implement Hinton's ""kinship"" network from (1986!) [this paper](http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs202_sp12/Readings/hinton86.pdf)

When implemented correctly, the network learns relationships between people in two isomorphic family trees. The network's inputs are a person and relationship -- e.g., ""Arthur"" ""Father"" and the output is expected to be people that match the relationship e.g.,  ""Christopher"".   

The paper shows neurons for the person encoding, six for relationship encoding - these are fully connected to a 12 neuron layer - which is in-turn fully connected to a layer with 6 neurons to represent the encoding of the output person, and finally connected to the output layer. 

I've attempted to duplicate his network exactly using [Keras](http://keras.io/):    

    graph = Graph()

    graph.add_input(name='person1', input_shape=(X_person1.shape[1],))
    graph.add_node(Dense(6, activation='sigmoid', init='normal'), name='person1_learned_encoding', input='person1') 

    graph.add_input(name='relationship', input_shape=(X_relationship.shape[1],))
    graph.add_node(Dense(6, activation='sigmoid', init='normal'), name='relationship_learned_encoding', input='relationship')

    graph.add_node(Dense(12, activation='sigmoid', init='normal'), name='predicted_features', inputs=['person1_learned_encoding', 'relationship_learned_encoding'])

    graph.add_node(Dense(6, activation='sigmoid', init='normal'), name='person2_learned_encoding', input='predicted_features') 

    graph.add_node(Dense(y_person2.shape[1], activation='softmax', init='normal'), name='person2', input='person2_learned_encoding')
    graph.add_output(name='output', input='person2')

    graph.compile('sgd', {'output': 'mse'})
    history = graph.fit({'person1':X_person1, 'relationship':X_relationship, 'output':y_person2}, verbose=0, batch_size=n, validation_split=0.035398, nb_epoch=1500, callbacks=[Peek()])

 The Hinton paper was written in 1986, so it predates everything but SGD, which I've used.  Also, like the paper, my implementation uses Sigmoid activation.  I've tried a variety of different initializations.  

Hinton's paper he says the network converges within 1500 iterations.  My implementation does not converge at 1500 or, for that matter, 1,500,000.  

Hinton's paper shows a visualization, which I've also implemented.  The code and visualizations are available in [this iPython notebook]( http://nbviewer.ipython.org/github/dcgrigsby/kinship/blob/master/kinship_net.ipynb).  Looking at the visualizations, I have two competing hypothesis about what's wrong:  (1) it's Vanishing Gradients, (2) I've made a newbie mistake because this is my first Graph network with Keras.  The visualizations make me think I've wired up the network right, but maybe not the inputs or outputs.

I'd be grateful for someone's practiced eyes to take a look.  

Thanks.


",1,4
146,2015-12-7,2015,12,7,12,3vqwot,Neural Topic Modelling Implementation?,https://www.reddit.com/r/MachineLearning/comments/3vqwot/neural_topic_modelling_implementation/,djc1000,1449460474,"I recently read this paper http://nlp.cs.rpi.edu/paper/AAAI15.pdf on using word2vec embeddings for generative topic modeling with a neural net. 

The algorithm looks like it would be difficult to implement efficiently, and very easy to make a mistake that would be hard to track down.

Is anyone aware of an existing implementation, reference or otherwise? 

Thanks.",6,5
147,2015-12-7,2015,12,7,13,3vr08e,"CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data",https://www.reddit.com/r/MachineLearning/comments/3vr08e/crosscat_a_fully_bayesian_nonparametric_method/,InaneMembrane,1449462224,,0,3
148,2015-12-7,2015,12,7,14,3vr68h,HP 450 coding machine,https://www.reddit.com/r/MachineLearning/comments/3vr68h/hp_450_coding_machine/,dongfengpacking,1449465215,,1,1
149,2015-12-7,2015,12,7,15,3vrg1i,I created an open-source client-side neural-learning chatbot in less than 500 lines of javascript that keeps its memory in browser storage and can build relationships with different people over time. I need help explaining how it works. Is anyone here good with evolutionary computation?,https://www.reddit.com/r/MachineLearning/comments/3vrg1i/i_created_an_opensource_clientside_neurallearning/,[deleted],1449470785,[deleted],4,0
150,2015-12-7,2015,12,7,17,3vrnl1,Best way to identify a ball being passed,https://www.reddit.com/r/MachineLearning/comments/3vrnl1/best_way_to_identify_a_ball_being_passed/,senortripleD,1449475709,Hi I'm trying to track the passing of a basketball using the data in the video here http://on.nba.com/1NQqQGy so far I've had limited sucess using a hidden markov model on the ball's xyz data but it often misidentifies short slow passes. since I don't have a collection of already classified plays what would be the best way(s) to identify a pass.,1,1
151,2015-12-7,2015,12,7,17,3vrops,Deep Learning advice for a novice,https://www.reddit.com/r/MachineLearning/comments/3vrops/deep_learning_advice_for_a_novice/,xristos_forokolomvos,1449476573,"Hello everyone, Computer Engineering undergraduate here!

I've been trying to implement a Deep Learning Neural Network for handwritten capital English letter recognition. I've done so in the past in Matlab with SVMs (too simplistic I know) and know I'm trying to implement the same thing in python. I use NoLearn and Lasagne. So far I get 0% accuracy which leads me to the assumption that I'm doing something fundamentally wrong.

Steps:
1) Preprocess the image to only keep sub-image that contains the letter, then resize to 25*25
2)Feed to NN with label vector Y having values [0,25], 0-&gt;A, 1-&gt;B etc

What seems to be the problem here? Also when I change the last layers activation function from softmax to something else like sigmoid,rectify in the messages printed during training process I get Nans or Zeros.

code: https://github.com/stavskal/DeepLearning-OCR/blob/master/eikoniko/cv.py

Thanks in advance!",8,1
152,2015-12-7,2015,12,7,17,3vrp7r,How machine learning is changing the way we live and work,https://www.reddit.com/r/MachineLearning/comments/3vrp7r/how_machine_learning_is_changing_the_way_we_live/,michealjury,1449476948,,0,1
153,2015-12-7,2015,12,7,18,3vrrso,cnc spare parts-cncrouter,https://www.reddit.com/r/MachineLearning/comments/3vrrso/cnc_spare_partscncrouter/,apexcnctech,1449478987,[removed],1,0
154,2015-12-7,2015,12,7,19,3vrx9v,What's the best way to test sequence to sequence learning?,https://www.reddit.com/r/MachineLearning/comments/3vrx9v/whats_the_best_way_to_test_sequence_to_sequence/,[deleted],1449483318,[deleted],2,0
155,2015-12-7,2015,12,7,20,3vs35u,Viability of using Machine Learning on predicting Climate Change?,https://www.reddit.com/r/MachineLearning/comments/3vs35u/viability_of_using_machine_learning_on_predicting/,moimitou,1449487910,"I have been thinking about the possibility of inputing the various time series that describe the climate (temperature, CO2/CH4 concentrations, solar radiation, etc) into a machine learning algorithm for time series prediction.

This would be interesting because we would be able to learn the relationships between the different time series from the data itself, thus confirming, contradicting (and eventually correcting) the current climate models, which usually apply the basic laws of physics to a 3d grid.

1/ So far, I did not find anyone who has tried this? Is there any basic reason why this would not work? (That being said, I understand it may be difficult! :P )

2/ I want to read up as much as I can on time series prediction with this application in mind. Can anyone point me to the best resources they have found on the subject?",8,2
156,2015-12-7,2015,12,7,22,3vsgy5,[Question] Semi-supervised learning with VAE,https://www.reddit.com/r/MachineLearning/comments/3vsgy5/question_semisupervised_learning_with_vae/,0entr0py,1449496751,"I am referring [this paper](http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf) by Kingma et. al in NIPS'14 on extending the VAE for classification. I have some difficulty understanding the M2 model proposed in this paper.   

From what I understand  they handle the case of labeled data and unlabeled data separately. For labeled data they simply treat it as input to both the encoder and the decoder (along with x and z respectively). The confusion I have is regarding the unlabeled case,specifically:  

Is the label for unlabeled data calculated using a classifier before feeding into the model similar to the labeled case - if so how is the classifier trained since we have so little labeled data ?  And why do they add the classifier loss to the error term separately (eq.9 in paper) ?    

My understanding is that the unknown y are treated as latent (similar to z) and we hope that q(y|x) will be a good classifier.  But i am not sure if this is enough to train a good model.

Thanks for any inputs

",4,2
157,2015-12-8,2015,12,8,0,3vsop4,"Googles ""Incept-v3"" model released",https://www.reddit.com/r/MachineLearning/comments/3vsop4/googles_inceptv3_model_released/,[deleted],1449500572,[deleted],0,1
158,2015-12-8,2015,12,8,0,3vsosz,"Googles ""Inception-v3"" model released",https://www.reddit.com/r/MachineLearning/comments/3vsosz/googles_inceptionv3_model_released/,[deleted],1449500611,[deleted],0,2
159,2015-12-8,2015,12,8,0,3vsrc5,GPU Platforms Extending Reach for Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3vsrc5/gpu_platforms_extending_reach_for_deep_learning/,[deleted],1449501756,[deleted],0,5
160,2015-12-8,2015,12,8,0,3vstxu,"Googles ""Inception v3"" model released",https://www.reddit.com/r/MachineLearning/comments/3vstxu/googles_inception_v3_model_released/,samim23,1449502923,,16,108
161,2015-12-8,2015,12,8,0,3vsu7v,Adversarial Machines - Fooling A.Is (and turn everyone into a Manga),https://www.reddit.com/r/MachineLearning/comments/3vsu7v/adversarial_machines_fooling_ais_and_turn/,samim23,1449503032,,4,1
162,2015-12-8,2015,12,8,0,3vsvr5,Deep Neural Networks Are Easily Fooled [pdf],https://www.reddit.com/r/MachineLearning/comments/3vsvr5/deep_neural_networks_are_easily_fooled_pdf/,naroom,1449503676,,8,0
163,2015-12-8,2015,12,8,0,3vsw2x,Google and Facebook Race to Solve the Ancient Game of Go With AI,https://www.reddit.com/r/MachineLearning/comments/3vsw2x/google_and_facebook_race_to_solve_the_ancient/,farful,1449503811,,3,37
164,2015-12-8,2015,12,8,1,3vt02w,"Ask ML: Help with fact checking a blog post on CNN/RNN, thanks",https://www.reddit.com/r/MachineLearning/comments/3vt02w/ask_ml_help_with_fact_checking_a_blog_post_on/,[deleted],1449505434,[deleted],0,2
165,2015-12-8,2015,12,8,1,3vt4mq,Bidirectional RNN pooling over multiple time steps,https://www.reddit.com/r/MachineLearning/comments/3vt4mq/bidirectional_rnn_pooling_over_multiple_time_steps/,theclothingguy,1449507265,"Hi, I'm creating a bi-directional recurrent neural networks for text classification. At each time step I feed in a word of the text I am trying to classify. I was wondering if there was a way to take a max-pool over all of the time steps required in order to feed the text in?
Thanks!

Edit: I am trying to create something like figure 3 in the image http://i.stack.imgur.com/WSOie.png",3,3
166,2015-12-8,2015,12,8,2,3vt9h5,Clearest intro/tutorial on CNNs?,https://www.reddit.com/r/MachineLearning/comments/3vt9h5/clearest_introtutorial_on_cnns/,[deleted],1449509139,[deleted],2,0
167,2015-12-8,2015,12,8,2,3vtcvz,[Question] What algorithm is best for this?,https://www.reddit.com/r/MachineLearning/comments/3vtcvz/question_what_algorithm_is_best_for_this/,zazaalaza,1449510502,"Hello, I am a beginner programmer and I've taken interest in machine learning. I was wondering which algorithm is the best to dealing with the following problem:

I input different files that have all sorts of data in them.
It outputs specific data that I need from those files, even though that data could be in different places in each file.

I would like to implement something like this in the future, which algorithm should I start with?",4,0
168,2015-12-8,2015,12,8,4,3vtrkl,"ASK /r/machinelearning: Working with large image dataset, infrastructure and pipeline",https://www.reddit.com/r/MachineLearning/comments/3vtrkl/ask_rmachinelearning_working_with_large_image/,eigendiego,1449516075,"I work with a small (4) but growing number of data scientists. Our working data is generated by a fairly high traffic website in the e-commerce space. I need advice for how to design and spec out a reasonable workflow for our team, specifically around working with a growing image dataset. (Text and structured data we've wranged locally and with spark, it's not the primary pain point right now)

For our images dataset I bought a single 1TB SSD and packed it chock full with resized images (scaled to 1024x1024). The idea here was to have a local working set we could feed to a local TitanX GPU (what our personal towers are geared with) to train a neural network on a first shot at classification on images. The results were very encouraging, so we want to extend our work in that direction (deep conv nets and relatives)

Sadly our 1TB SSD lived too short a life. It was fried before we could train a second net (probably some TRIM issues?)

My questions for those of you working with image datasets:

+ Do you have RAID setup? SSD? Spinning disk? How many terabytes do you work with?


One of our ops guys (with more expertise on the hardware end of things) outlined the following solutions:

Platinum Option:

+ $18,000 crazy beefy shared work station
+ 12 cores, 128 GB, one or multiple GPUs
+ 10 TB of storage in RAID6

Gold Option:

+ 16 cores 2.6GHz
+ 64GB DDR4
+ 10TB of Storage in RAID6

Bronze Option:

+ $2000 for 2 Samsung 850 Pro 2.5, this is 1TB in RAID1 attached to one of our current workstations, it will then become more of a shared workstation when working with the larger datasets.

Here's how I see things evolving:

+ I think it would be most convenient if we're able to play with a large subset of our images locally. We have decent memory on our local stations, and beefy cards as well, so really we're just underprovisioned on storage at the moment.

+ So going with Bronze, we're able to meet above and bring solutions to the prototype stage. We would then need to work on scaling training to a more massive dataset, say using multiple GPUs on one machine, or across machines.
++ I do think we need to go by baby steps here though. I think that we should prove our prototypes locally first, and in the next step tackle the scaling beyond a single node. This is part of why the Bronze option is appealing.

+ Should we go with the SAN setup and bring the whole shebang locally?

+ Should we forget about training on our towers and move to AWS's seemingly wimpy GPUs?
++ My only holdback against this is the iteration cycle. Locally, our Data Scientists are able to iterate quickly on developing a method. The AWS move feels like jumping to solve the scalability problem before we're proven our methods.

tl/dr: any and all comments around your multiterabyte workflow are appreciated. Thanks to everyone in advance!
",2,0
169,2015-12-8,2015,12,8,5,3vtzbx,How should i attack a data mining problem?,https://www.reddit.com/r/MachineLearning/comments/3vtzbx/how_should_i_attack_a_data_mining_problem/,johnadler557,1449518992,"Hi, 
I have dataset (150 features , 300 observations) which is very clean ! no noise, no missing values, normal distributions of features. the accuracy that i am getting with different methods is very high (98%). However, the professor is saying that : "" I'm mostly looking for a coherent explanation of how you would attack the specified problem, and what you would expect to find at each stage. You might want to use data mining tools to validate your assumptions and to check that your reasoning does indeed seem to agree with the models you generate."" 

the way that i attack a datamining problem is like this : 
1) dealing with missing values 
2) seeing distribution of features 
3) using random forest to look deeper into attributes, in a way of importance
4) using different typologies for my deep neural networks and comparing results 
5) getting the deep features and use them to train my network. 

Do you have any comments ( or additional step )? am i doing it right? 
what does he mean by ""validate your assumption ... ""?

Thanks in advance ",2,1
170,2015-12-8,2015,12,8,5,3vu0gs,Non compete agreement,https://www.reddit.com/r/MachineLearning/comments/3vu0gs/non_compete_agreement/,DancesWithPizzas,1449519411,"Hi all. I was wondering if anyone can share their experience with this. I'm about to sign on with a tech company (first one) and have been asked to sign a non compete agreement. I'm a bit concerned because the language is some what 'comprehensive' on the non compete part. Is this type of language normal for tech companies. This is for a company that operates within the realm of this subreddit and my role would also would be within the realm of this subreddit. Should I be concerned or is this pretty normal. The length is 12 month, from what I've heard is normal.",17,5
171,2015-12-8,2015,12,8,6,3vuei5,Detecting genres based on Rythem Patterns . Which algorithm should I use for it.,https://www.reddit.com/r/MachineLearning/comments/3vuei5/detecting_genres_based_on_rythem_patterns_which/,Ande_Ka_Funda,1449524841,"Hi

The first set of database I have is this:
http://www.ifs.tuwien.ac.at/mir/msd/download.html

This is a collection of various Rhythm pattern for a song. This data-set contains details of 100000 songs.

The second data-set is of Genre's of this songs in 15 genres.

What I want is to use these audio features to predict the genres of new songs.

Till now I tried Logistic regression, decision trees, random forests, naive Bayes (provided by Mllib) but the success rate was less than 10%.

What should I do or which algo should I use to get it to atleast 50 %.",4,0
172,2015-12-8,2015,12,8,9,3vuzdq,50 Useful Machine Learning &amp; Prediction APIs,https://www.reddit.com/r/MachineLearning/comments/3vuzdq/50_useful_machine_learning_prediction_apis/,dabshitty,1449533136,,0,1
173,2015-12-8,2015,12,8,10,3vv7it,Code: Neural machine translation between the writings of Shakespeare and modern English using TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3vv7it/code_neural_machine_translation_between_the/,[deleted],1449536657,[deleted],1,18
174,2015-12-8,2015,12,8,10,3vvbzj,Is this a suitable problem for a neural network?,https://www.reddit.com/r/MachineLearning/comments/3vvbzj/is_this_a_suitable_problem_for_a_neural_network/,kaworu1986,1449538624,"I am evaluating whether it would be feasible to apply machine learning to the field of animation.

The result Id like to achieve is: given two key frames of an animation and how many in-between frames there are between them, have a computer generate those.

To a developer without data science or machine learning experience like me, the steps to achieve this look like:

- Build a neural network
- Use existing animation material for supervised training
- Have the network generate new in-between frames from new input key frames

Supervised training would take two key frames and a numeric value indicating how close the in-between frame is to the first key frame as inputs, and the existing in-between frame as reference output.

Is this a suitable approach to tackle such a problem? If so, are there any particular types (deep, convolutional, etc.) of neural networks that I should look into over others?",3,0
175,2015-12-8,2015,12,8,10,3vvdly,Keras Text Generation Question,https://www.reddit.com/r/MachineLearning/comments/3vvdly/keras_text_generation_question/,butWhoWasBee,1449539358,"https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py

So I see the data is getting cut into sentences = maxlen

The model's input is defined below. The dimensions seem to be the length of a single time step * the size of the maxlen window

model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))

So lets say there were 5 chars in the dictionary and the window size was 10. Is it passing 50 inputs in each time, or is it passing them 5 at a time, and then resetting the state every 50? 


I am aware that Keras is stateless, so you could call model.predict(x) 10 times and it would always return the same result. Given this, is this example script really doing anything recurrent at all? Is there any reason to use an LSTM over a simple non recurrent sigmoid layer in this example, or am I misunderstanding Keras? This just seems like a sliding window to me, which doesn't even require recurrence. 

edit: If it is the case that this isn't really doing anything recurrent and is just doing a sliding window, does anybody know how to do truly recurrent text generation on Keras. Suggestions for other theano using libraries that can do this would be appreciated as well (LSTM and dropout is a huge plus) ",16,2
176,2015-12-8,2015,12,8,11,3vvkei,NeoRL - Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3vvkei/neorl_reinforcement_learning/,CireNeikual,1449542209,,7,8
177,2015-12-8,2015,12,8,12,3vvt37,Alternatives to gradient descent for back prop?,https://www.reddit.com/r/MachineLearning/comments/3vvt37/alternatives_to_gradient_descent_for_back_prop/,cafedude,1449546007,"Given the tendency of gradient descent to easily get stuck in local minima, why is it used pretty much universally for back propagation? Would alternative optimization algorithms be worth exploring, or is it just that practitioners feel like the local minima problem isn't actually all that problematic for these kinds of problems? Most likely alternative optimization strategies to gradient descent would be more computationally expensive and thus it's not considered to be worth the extra computational effort?",17,3
178,2015-12-8,2015,12,8,15,3vwdz0,NLP project: Which candidate's speeches are most like Hitler's?,https://www.reddit.com/r/MachineLearning/comments/3vwdz0/nlp_project_which_candidates_speeches_are_most/,sleepicat,1449556758,How would one go about using machine learning to match words or phrases used by the US 2016 Presidential candidates' speeches to those of Hitler and see which candidate's speeches are most like Hitler's?  ,11,13
179,2015-12-8,2015,12,8,18,3vwtxy,Friendly challenge for Facebook and Google: Play each other in a game of Go,https://www.reddit.com/r/MachineLearning/comments/3vwtxy/friendly_challenge_for_facebook_and_google_play/,feedthecreed,1449568004,"Both companies have claimed strong improvements in their AI for the game of Go [1], so it seems logical to have an AI vs AI match of Go. Maybe if this gets enough support from the ML community here, we can get this to happen!

[1] http://www.wired.com/2015/12/google-and-facebook-race-to-solve-the-ancient-game-of-go/",6,277
180,2015-12-8,2015,12,8,18,3vwu4h,Lube Pumps: What Is The Need To Use Them?,https://www.reddit.com/r/MachineLearning/comments/3vwu4h/lube_pumps_what_is_the_need_to_use_them/,jackerfrinandis,1449568130,,0,1
181,2015-12-8,2015,12,8,19,3vwz4e,Transitioning into ML.,https://www.reddit.com/r/MachineLearning/comments/3vwz4e/transitioning_into_ml/,siriusblackprat,1449571843,"I've been working in a manufacturing plant as a production engg. for slightly more than an year. Although the work is ok, it doesn't terribly fascinate me. What does though, at least superficially, is AI. I do have a bachelor's degree in Mechatronics, but the ""effectiveness"" is largely gone (it's been 3 years now). The field looks technically daunting to a newbie and I have few ideas on how to effectively begin. I've been thinking about taking Udacity's ""Machine Learning Nanodegree"". Is it worth doing that? How would you guys suggest I proceed?",1,0
182,2015-12-8,2015,12,8,20,3vx1qh,Simple Baseline for Visual Question Answering: Bag of words and visual features as accurate as neural visual question answering models,https://www.reddit.com/r/MachineLearning/comments/3vx1qh/simple_baseline_for_visual_question_answering_bag/,onewugtwowugs,1449573823,,0,4
183,2015-12-8,2015,12,8,22,3vxal4,What happened at NIPS 2015 Deep Learning Session?,https://www.reddit.com/r/MachineLearning/comments/3vxal4/what_happened_at_nips_2015_deep_learning_session/,metacurse,1449579676,"I read tidbits about it, nothing complete. Someone tell",8,25
184,2015-12-8,2015,12,8,22,3vxdlm,[Question] Not sure how to get started with genetic algorithm programming,https://www.reddit.com/r/MachineLearning/comments/3vxdlm/question_not_sure_how_to_get_started_with_genetic/,Martijnie,1449581352,"Hi guys,

After watching [this](https://www.youtube.com/watch?v=p_H2TLG1cMo) and [this](https://www.youtube.com/watch?v=u2t77mQmJiY) video I read a lot about neural networks and genetic algorithms. I really would like to get started now with programming my own experiments like those in the videos but I'm not sure where to start and I couldn't really find any in-depth (preferably javascript or python) guides/tutorials.

Any help to get this newbie started is greatly appreciated!",4,0
185,2015-12-8,2015,12,8,22,3vxe6i,Investing in Artificial Intelligence: a VC perspective,https://www.reddit.com/r/MachineLearning/comments/3vxe6i/investing_in_artificial_intelligence_a_vc/,ahousley,1449581663,,0,1
186,2015-12-8,2015,12,8,22,3vxgb0,Why people does not use Einstein notation?,https://www.reddit.com/r/MachineLearning/comments/3vxgb0/why_people_does_not_use_einstein_notation/,Tokukawa,1449582809,"I often struggling in reading papers of machine learning. Actually not for complexity of the math, that it is actually very basic, but because of the poor formalism  that is messing the concepts even more.  And I bet it is a common feeling.
Why people does not use Einstein notation in machine learning? Life would be easier for everyone.",12,5
187,2015-12-8,2015,12,8,23,3vxht4,NIPS 2015: Speech Processing (Yoshua Bengio),https://www.reddit.com/r/MachineLearning/comments/3vxht4/nips_2015_speech_processing_yoshua_bengio/,alexcaps,1449583549,,1,3
188,2015-12-9,2015,12,9,0,3vxw78,Strategies of the Artificial: The Machine View of Strategy and its Consequences | Kings of War,https://www.reddit.com/r/MachineLearning/comments/3vxw78/strategies_of_the_artificial_the_machine_view_of/,[deleted],1449589762,[deleted],0,0
189,2015-12-9,2015,12,9,1,3vxyb2,Help. I'm new and I am hunger for machine learning! Help me please!,https://www.reddit.com/r/MachineLearning/comments/3vxyb2/help_im_new_and_i_am_hunger_for_machine_learning/,jennifercqcq,1449590576,"Hey guys,

I have engineering background. I'm doing growth hacking, and I've been amazed by the power of data. I can't wait to join a community like this. I want to use the power of machine learning and help e-commerce companies with personalization. 
I have front end coding skills(html,JS), and experiences with data mining.
To start, where would be a good resource to learn deep machine learning? both online and offline. Any recommendations how I should start this journey? ",3,0
190,2015-12-9,2015,12,9,1,3vy0hx,Are linear models sufficient in practice?,https://www.reddit.com/r/MachineLearning/comments/3vy0hx/are_linear_models_sufficient_in_practice/,j_lyf,1449591403,"For all the deep learning hype and state of the art results, I'm just curious to know what percentage of folks are leveraging linear models (i.e. logistic regression and friends) in production for their ML pipelines. Any anecdotes would be good.",4,0
191,2015-12-9,2015,12,9,2,3vy7cf,"Check out the Deep Learning playlist: 79 presentations available for free - includes speakers from Baidu, Stanford University, Google, DeepMind, Twitter, Flickr, MIT, U of Toronto &amp; more",https://www.reddit.com/r/MachineLearning/comments/3vy7cf/check_out_the_deep_learning_playlist_79/,reworksophie,1449594065,,0,1
192,2015-12-9,2015,12,9,3,3vyjpw,Hardware specs for convnet in Tesla car?,https://www.reddit.com/r/MachineLearning/comments/3vyjpw/hardware_specs_for_convnet_in_tesla_car/,akshayxyz,1449598618,"Somewhere I read that Tesla's autopilot uses convnets. I was wondering what are the hardware specs of the 'online serving' convnet in the Tesla Car.

Edit1:
Looks like it uses Tegra
https://www.reddit.com/r/teslamotors/comments/3vappw/wccftech_a_look_at_teslas_autopilot/",0,0
193,2015-12-9,2015,12,9,3,3vykj1,Sparkit-learn,https://www.reddit.com/r/MachineLearning/comments/3vykj1/sparkitlearn/,spimby,1449598907,"I came across this library that aims to integrate spark and scikit-learn. Do any of you have experience working with this? Reviews? Gotchas? Alternatives? More details than the README gives?

https://github.com/lensacom/sparkit-learn",0,2
194,2015-12-9,2015,12,9,3,3vyopw,Deep Neural Decision Forests Explained,https://www.reddit.com/r/MachineLearning/comments/3vyopw/deep_neural_decision_forests_explained/,CompleteSkeptic,1449600446,,0,11
195,2015-12-9,2015,12,9,3,3vypul,[Question] What to learn first to make this project?,https://www.reddit.com/r/MachineLearning/comments/3vypul/question_what_to_learn_first_to_make_this_project/,iamgoku204,1449600848,"I am self learning ML. I have done Andrew Ng's course on coursera which was a good introduction and want to learn further. Because this field is too vast I planned that am going to do it as project based learning. The project I came up with was this: A program which writes in my handwriting. The idea is that I will first process the images of my writing to turn into training data, then feed it to some form of ML system( I was thinking of using neural networks as in class ). After successful completion of training , I give the program a line of text written by keyboard and it turns it into an image as if I have written it by my hand. I chose this project because I have lots of notes from my previous classes . The problem is that I don't know what to learn next and would like to get your opinion on that. Thanks.",2,0
196,2015-12-9,2015,12,9,5,3vz12j,Practical applications of the game-playing learning algorithms?,https://www.reddit.com/r/MachineLearning/comments/3vz12j/practical_applications_of_the_gameplaying/,[deleted],1449604849,[deleted],2,2
197,2015-12-9,2015,12,9,5,3vz2sc,Neural Programmer-Interpreters,https://www.reddit.com/r/MachineLearning/comments/3vz2sc/neural_programmerinterpreters/,kzf_,1449605469,,0,4
198,2015-12-9,2015,12,9,5,3vz6ez,Whats the Difference Between Data Engineering and Data Science?,https://www.reddit.com/r/MachineLearning/comments/3vz6ez/whats_the_difference_between_data_engineering_and/,dnabeyta,1449606798,,1,1
199,2015-12-9,2015,12,9,7,3vzlzz,"Reproducing ""A Neural Conversational Model"" in Torch",https://www.reddit.com/r/MachineLearning/comments/3vzlzz/reproducing_a_neural_conversational_model_in_torch/,macournoyer,1449612305,"Like many, I was very impressed by the results in the [Neural Conversational Model](http://arxiv.org/abs/1506.05869) (Google chatbot) paper. Here's my attempt at reproducing it with very limited resources and knowledge:

https://github.com/macournoyer/neuralconvo

I did manage to get some fun conversations:

&gt; **me:** Hi  
&gt; **bot:** Hey.
&gt; 
&gt; **me:** What's your name?  
&gt; **bot:** Pris.
&gt; 
&gt; **me:** How old are you?  
&gt; **bot:** Thirty five eleven.
&gt; 
&gt; **me:** What's 2 + 2?  
&gt; **bot:** Nothing.
&gt; 
&gt; **me:** That's funny.  
&gt; **bot:** No.
&gt; 
&gt; **me:** Where are you from?  
&gt; **bot:** Helsinki, there!
&gt; 
&gt; **me:** That's a nice place.  
&gt; **bot:** Yes.
&gt; 
&gt; **me:** How long have you been living in Helsinki?  
&gt; **bot:** Thirty years.
&gt; 
&gt; **me:** Talk to you later.  
&gt; **bot:** What?
&gt; 
&gt; **me:** I'm leaving.  
&gt; **bot:** Leaving what?
&gt; 
&gt; **me:** Leaving this conversation.  
&gt; **bot:** Yes.

However, most of the answers are pretty bad, eg.: *&gt; It would, it would, it would...*.

I could only get some decent results on a small dataset of ~5000 examples. As soon as I increase the size of the training set I get bad results. I think this is because I can't use networks (LSTM x 2) larger then 1000 nodes on my 4GB graphic card (is this normal?).

I've implemented a few papers (in other fields) in the past, but this is my first serious ML project. I would appreciate any advice or pointer on how to improve this. Specifically:

- Is there any way to make large/deep networks fit on my 4GB card? 
- How should I measure the quality of the training? Perplexity?
- Would switching to word2vec for word embedding improve the results?

Thanks!",17,16
200,2015-12-9,2015,12,9,7,3vzsik,Pretty Tensor - Fluent Neural Networks in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3vzsik/pretty_tensor_fluent_neural_networks_in_tensorflow/,eiderman,1449614777,,12,80
201,2015-12-9,2015,12,9,8,3w02pp,A desktop for deep learning - Ubuntu + M2090 + ??,https://www.reddit.com/r/MachineLearning/comments/3w02pp/a_desktop_for_deep_learning_ubuntu_m2090/,keidouleyoucee,1449618784,"I'm going to build my own machine for deep learning. I got an abandoned PC from the school and found out a reasonable-priced Tesla M2090. 

Do I need to - or am I recommend to - get a Nvidia Quadro for the display? That's what the seller on ebay recommends to do, but he is also not 100% sure on other options - e.g. use a geforce or on-board one. I found some links saying it is available to use Tesla with on-board on, but I'd like to spend some more money to avoid painful settings. 

Please give me any advice! Thanks.
",9,2
202,2015-12-9,2015,12,9,9,3w04lj,Machine Learning in FP&amp;A,https://www.reddit.com/r/MachineLearning/comments/3w04lj/machine_learning_in_fpa/,juicemoney,1449619610,"I work for a Fortune 500 company in the financial planning &amp; analysis team (FP&amp;A). For those that don't know, FP&amp;A is responsible for creating revenue and expense budgets, recurring monthly forecasts, and other financial analysis.

I am just about done with Andrew Ng's machine learning class and want to apply it to problems within FP&amp;A. We have a little over 100 members in FP&amp;A globally and nobody has applied machine learning to any problem. I'm reaching out to see if anybody has any examples of FP&amp;A problems that have been tackled using machine learning? Also, how does one identify a problem that can be solved using machine learning?

Thanks in advance for the help!",2,0
203,2015-12-9,2015,12,9,9,3w06p8,The gap between an amateur and a professional in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3w06p8/the_gap_between_an_amateur_and_a_professional_in/,PyBet,1449620458,"I was just wondering if an amateur, self-learning person can reach the levels and the results of a professional in the field.

In reality how much is the difference when it comes to actual problems ?",7,0
204,2015-12-9,2015,12,9,9,3w0c9u,Tools for Data Science Pairing?,https://www.reddit.com/r/MachineLearning/comments/3w0c9u/tools_for_data_science_pairing/,cavaunpeu,1449622671,"Tools for remote pairing on Jupyter notebooks, and the Sublime text editor. How do you and your team do this?",0,0
205,2015-12-9,2015,12,9,9,3w0ck6,Build Online Image Classification Service with Shiny and MXNetR,https://www.reddit.com/r/MachineLearning/comments/3w0ck6/build_online_image_classification_service_with/,phunter_lau,1449622787,,0,1
206,2015-12-9,2015,12,9,12,3w0te1,"Top 5 arXiv Deep Learning Papers, Explained by Hugo Larochelle",https://www.reddit.com/r/MachineLearning/comments/3w0te1/top_5_arxiv_deep_learning_papers_explained_by/,mmmayo13,1449630020,,3,9
207,2015-12-9,2015,12,9,12,3w0xi0,Finding sparse solutions to linear systems,https://www.reddit.com/r/MachineLearning/comments/3w0xi0/finding_sparse_solutions_to_linear_systems/,skier_scott,1449631803,,0,6
208,2015-12-9,2015,12,9,13,3w17ec,How To Choose the Right Lubricant For Motor,https://www.reddit.com/r/MachineLearning/comments/3w17ec/how_to_choose_the_right_lubricant_for_motor/,jackerfrinandis,1449636387,,0,1
209,2015-12-9,2015,12,9,13,3w17jt,Methods for Staying Current on Research?,https://www.reddit.com/r/MachineLearning/comments/3w17jt/methods_for_staying_current_on_research/,p01ym47h,1449636457,"I am curious what tools, websites, and methods people use to stay current on (ML) research. I often hear of and read papers that get major media coverage but I'm failing to see how researchers perpetually stay current. Where do they go to easily see what papers are getting a lot of attention? On what forums do they communicate? Are there sites/tools I don't know about?  It's never been straight forward to me how real researchers do this. I've always just stayed in the mediocrity zone and only read the most popular papers. Scanning the daily arXiv paper submissions seems like overkill since there are so many across a lot of categories. Any direction/wisdom is be appreciated!",3,0
210,2015-12-9,2015,12,9,15,3w1m73,Neuroscientists find new support for Chomskys internal grammar thesis,https://www.reddit.com/r/MachineLearning/comments/3w1m73/neuroscientists_find_new_support_for_chomskys/,Atupis,1449644274,,12,25
211,2015-12-9,2015,12,9,17,3w1wck,Rapid Mixer Granulator - Anchormark,https://www.reddit.com/r/MachineLearning/comments/3w1wck/rapid_mixer_granulator_anchormark/,Anchormark1,1449651263,,0,1
212,2015-12-9,2015,12,9,20,3w2896,Tomasz Malisiewicz's selection of ICCV papers,https://www.reddit.com/r/MachineLearning/comments/3w2896/tomasz_malisiewiczs_selection_of_iccv_papers/,alito,1449659873,,1,21
213,2015-12-9,2015,12,9,21,3w2dcb,How important is visualization for industry?,https://www.reddit.com/r/MachineLearning/comments/3w2dcb/how_important_is_visualization_for_industry/,unsureifcatordog,1449663406,"Hi ml,
I am choosing between two courses, one in ""Information visualization"" and one in ""Artificial neural networks"". I'm pursuing a data science / statistics career and was wondering what you guys think would be most useful in industry? Seems to me like ANN is very niched and I can't even see a job posting anywhere that even mentions it, but visualization is something that is always needed, right?

Any input is highly appreciated!
Thanks.",4,0
214,2015-12-9,2015,12,9,21,3w2e9y,"Haskell: no benefits, good for nothing",https://www.reddit.com/r/MachineLearning/comments/3w2e9y/haskell_no_benefits_good_for_nothing/,realtalkintech,1449664013,,5,0
215,2015-12-9,2015,12,9,21,3w2ejv,"Which Machine Learning book to choose (APM, MLAP or ISL)?",https://www.reddit.com/r/MachineLearning/comments/3w2ejv/which_machine_learning_book_to_choose_apm_mlap_or/,Machinery86,1449664178,"

I'm searching a book as a referesher in machine learning (I have taken a lecture in machine learning some times ago). I will be applying machine learning in a project.

I have searched a lot of books and found the following three the one which best fits to me:

    Applied Predictive Modeling by Kuhn and Johnson
    Machine learning an Algorithmic Perspective (second edition) by Marsland
    An Introduction to Statistical Learning by Hastie

Applied Predictive Modeling and An Introduction to Statistical Learning uses R as examples and Machine learning an Algorithmic Perspective uses Python. The used language does not matter that much because I'm not really interested in language examples (I'm using Matlab) and I will probably skip such examples.

The advantage of Applied Predictive Modeling seems to be that it covers the whole machine learning procedure (feature selection etc.) and seems to be very well written. The advantage of Machine learning an Algorithmic Perspective (second edition) is that it covers more topics (ensemble learning, graphical models, gaussian processes) and has perhaps a bit more math.

Which one of these three books would you recommend (and why)?",8,5
216,2015-12-9,2015,12,9,21,3w2em7,How To Find Cheap Brochure Printing Services,https://www.reddit.com/r/MachineLearning/comments/3w2em7/how_to_find_cheap_brochure_printing_services/,printbrookprinting,1449664223,,0,0
217,2015-12-9,2015,12,9,22,3w2m6k,Create unique music for your videos in seconds using artificial intelligence,https://www.reddit.com/r/MachineLearning/comments/3w2m6k/create_unique_music_for_your_videos_in_seconds/,nb410,1449668317,,2,8
218,2015-12-9,2015,12,9,23,3w2qf3,Overfitting on protein folding data,https://www.reddit.com/r/MachineLearning/comments/3w2qf3/overfitting_on_protein_folding_data/,acgtgagcgta,1449670281,,1,0
219,2015-12-9,2015,12,9,23,3w2trg,Is there any information available about Google's Smart Ad Selection System (SmartASS)?,https://www.reddit.com/r/MachineLearning/comments/3w2trg/is_there_any_information_available_about_googles/,AudioManiac,1449671786,"I'm doing research for college, and I came across Google SmartASS in Machine Learning: A Probabilistic Approach (2012) by Kevin P. Murphy.

The book doesn't cover much more about it, other than using it as an example of probabilistic prediction in industry. Part of my research is on the area of probabilistic prediction, so I would love to be able to find more information about SmartASS.

I looked up the reference used in that book, which was from http://www.theregister.co.uk/2010/02/02/google_smartass_server/

I'm wondering if there was more written on the topic since then, as well as maybe more reliable sources such as academic publications or something along those lines. But I'll take anything at the moment. I can't seem to find much on it at all",2,0
220,2015-12-9,2015,12,9,23,3w2trz,"Simple Baseline for Visual Question Answering (Demo, Code, and Report)",https://www.reddit.com/r/MachineLearning/comments/3w2trz/simple_baseline_for_visual_question_answering/,cesarsalgado,1449671795,,2,3
221,2015-12-9,2015,12,9,23,3w2vfl,Learning Simple Algorithms from Examples (code is up again on github),https://www.reddit.com/r/MachineLearning/comments/3w2vfl/learning_simple_algorithms_from_examples_code_is/,cesarsalgado,1449672517,,1,85
222,2015-12-10,2015,12,10,0,3w2xq7,Simple question on Scikit-learn and KNN,https://www.reddit.com/r/MachineLearning/comments/3w2xq7/simple_question_on_scikitlearn_and_knn/,[deleted],1449673472,[deleted],2,0
223,2015-12-10,2015,12,10,0,3w33lt,"A Look into ""Machine Learning's First Cheating Scandal""",https://www.reddit.com/r/MachineLearning/comments/3w33lt/a_look_into_machine_learnings_first_cheating/,dunnowhattoputhere,1449675862,,9,27
224,2015-12-10,2015,12,10,0,3w33qk,The Quest for Cross-lingual Systems,https://www.reddit.com/r/MachineLearning/comments/3w33qk/the_quest_for_crosslingual_systems/,orangejaipur,1449675920,,1,1
225,2015-12-10,2015,12,10,1,3w39fn,Deep Learning Startup Launches New Photos App with Custom Training,https://www.reddit.com/r/MachineLearning/comments/3w39fn/deep_learning_startup_launches_new_photos_app/,[deleted],1449678257,[deleted],0,1
226,2015-12-10,2015,12,10,1,3w3bmj,New deep learning photos app with custom training,https://www.reddit.com/r/MachineLearning/comments/3w3bmj/new_deep_learning_photos_app_with_custom_training/,flippzz,1449679138,,5,7
227,2015-12-10,2015,12,10,1,3w3c34,Ten Free Machine Learning Textbooks,https://www.reddit.com/r/MachineLearning/comments/3w3c34/ten_free_machine_learning_textbooks/,[deleted],1449679324,[deleted],0,0
228,2015-12-10,2015,12,10,1,3w3egh,"The Ladder Network results extremely impressive, but has anyone had luck with the network?",https://www.reddit.com/r/MachineLearning/comments/3w3egh/the_ladder_network_results_extremely_impressive/,____BOLD____,1449680244,"Semi-Supervised Learning with Ladder Network
http://arxiv.org/abs/1507.02672

On MNIST, with only 100 labeled examples, it achieves 1.06% error

That paper reports very amazing semi-supervised learning performance, but I haven't heard of any work that has been derived from that paper for any new datasets. Does anyone have experience with this architecture? Does it only work for CIFAR and MNIST?",10,0
229,2015-12-10,2015,12,10,2,3w3fay,Machine Learning: An overview of national top employers.,https://www.reddit.com/r/MachineLearning/comments/3w3fay/machine_learning_an_overview_of_national_top/,shugert,1449680570,,1,1
230,2015-12-10,2015,12,10,2,3w3j3v,Playing against Deep Go,https://www.reddit.com/r/MachineLearning/comments/3w3j3v/playing_against_deep_go/,fspeech,1449681972,,1,0
231,2015-12-10,2015,12,10,2,3w3jda,"When using cross-entropy error to train neural network for multi-label classification, do we assume each output unit models particular class score?",https://www.reddit.com/r/MachineLearning/comments/3w3jda/when_using_crossentropy_error_to_train_neural/,zibenmoka,1449682068,"Most probably the answer is just ""yes"". I got confused when implementing a network myself. It seems like I have to choose which output neuron will be responsible for modeling which class and stick to it while backpropagation. So in fact I might want to have a vector that holds that information (or of course assume some given ordering as convention)

Please confirm or explain why not. Thanks a lot! ",5,0
232,2015-12-10,2015,12,10,3,3w3ppz,Confusion about sequence lengths and truncated backprop,https://www.reddit.com/r/MachineLearning/comments/3w3ppz/confusion_about_sequence_lengths_and_truncated/,robclouth,1449684373,"Hey folks. I'm trying to use a LSTM net to model the XML-based project files for some software, with the aim of generating new ones. The project files have a very specific structure and a clear beginning and end (the &lt;root&gt; and &lt;/root&gt; tags), so the so-called 'sequence length' should be the whole project file right? I have hundreds of these files, so should I treat them individually as training examples with variable sequence length? If so, then clearly I can't fit that all into memory since the files contain up to 1000000 elements. Internet seems to suggest truncating the backpropagation. Since I'm using lasagne, does that mean setting the gradient_steps argument to something reasonable? 

Or the other way is to concatenate all of them into a single mega long file and go through bit by bit. But this doesn't make sense because each file is a distinct unit that doesn't 'wrap around' like that.


Any help would be so much appreciated. I've jumped in the deep end here considering I have little background in machine learning...but that's the way I learn best and I'm finding this whole field fascinating.",10,1
233,2015-12-10,2015,12,10,3,3w3qv2,Introduction to Chainer: Neural Networks in Python,https://www.reddit.com/r/MachineLearning/comments/3w3qv2/introduction_to_chainer_neural_networks_in_python/,Kombutini,1449684778,,2,3
234,2015-12-10,2015,12,10,3,3w3roa,GPU ram limitations matlab feedforward neural network,https://www.reddit.com/r/MachineLearning/comments/3w3roa/gpu_ram_limitations_matlab_feedforward_neural/,[deleted],1449685060,[deleted],1,0
235,2015-12-10,2015,12,10,3,3w3x14,"Are there any examples of Machine Learning being to improve computer graphics, computer vision, or any other artistic tasks?",https://www.reddit.com/r/MachineLearning/comments/3w3x14/are_there_any_examples_of_machine_learning_being/,Seddit55,1449687030,,6,1
236,2015-12-10,2015,12,10,3,3w3yba,Slides of the NIPS'2015 Deep Learning tutorial by Yann Lecun and Yoshua Bengio,https://www.reddit.com/r/MachineLearning/comments/3w3yba/slides_of_the_nips2015_deep_learning_tutorial_by/,clbam8,1449687481,,1,17
237,2015-12-10,2015,12,10,4,3w41lf,[1512.01563] State of the Art Control of Atari Games Using Shallow Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3w41lf/151201563_state_of_the_art_control_of_atari_games/,flangles,1449688620,,2,31
238,2015-12-10,2015,12,10,4,3w45wa,Breakfast at NIPS,https://www.reddit.com/r/MachineLearning/comments/3w45wa/breakfast_at_nips/,NIPSthrowaway,1449690185,"It isn't made with love. It is made with the urge to leave work ASAP. I could actually taste it in my mouth. :(

",2,3
239,2015-12-10,2015,12,10,5,3w4fu3,VAE code variance,https://www.reddit.com/r/MachineLearning/comments/3w4fu3/vae_code_variance/,[deleted],1449693874,[deleted],1,2
240,2015-12-10,2015,12,10,6,3w4jl3,Machine Learning Supports Production Uptime,https://www.reddit.com/r/MachineLearning/comments/3w4jl3/machine_learning_supports_production_uptime/,shugert,1449695220,,0,0
241,2015-12-10,2015,12,10,6,3w4qlj,Baidu's speech recognition system benefits from BatchNorm when used with very deep RNNs. (Section 3.2),https://www.reddit.com/r/MachineLearning/comments/3w4qlj/baidus_speech_recognition_system_benefits_from/,sherjilozair,1449697837,,10,14
242,2015-12-10,2015,12,10,8,3w58nm,TensorFlow 0.6 released - now supports Python 3,https://www.reddit.com/r/MachineLearning/comments/3w58nm/tensorflow_06_released_now_supports_python_3/,outlacedev,1449705061,,20,108
243,2015-12-10,2015,12,10,10,3w5otn,Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis,https://www.reddit.com/r/MachineLearning/comments/3w5otn/weaklysupervised_disentangling_with_recurrent/,downtownslim,1449711778,,0,1
244,2015-12-10,2015,12,10,15,3w6p7v,"Getting into machine learning, would love advice on how to pursue it?",https://www.reddit.com/r/MachineLearning/comments/3w6p7v/getting_into_machine_learning_would_love_advice/,[deleted],1449727444,[deleted],4,1
245,2015-12-10,2015,12,10,15,3w6rl8,Thinking required; opinion on Machine Intelligence from IBM Research,https://www.reddit.com/r/MachineLearning/comments/3w6rl8/thinking_required_opinion_on_machine_intelligence/,kmrocki,1449728642,,0,0
246,2015-12-10,2015,12,10,16,3w6wqk,[1512.02728] Distributed Training of Deep Neural Networks with Theoretical Analysis: Under SSP Setting,https://www.reddit.com/r/MachineLearning/comments/3w6wqk/151202728_distributed_training_of_deep_neural/,iori42,1449731281,,0,2
247,2015-12-10,2015,12,10,17,3w72xz,Vertical food vacuum sealer,https://www.reddit.com/r/MachineLearning/comments/3w72xz/vertical_food_vacuum_sealer/,dongfengpacking,1449734662,,1,1
248,2015-12-10,2015,12,10,17,3w7490,Nice tutorial notebook on Tensorflow,https://www.reddit.com/r/MachineLearning/comments/3w7490/nice_tutorial_notebook_on_tensorflow/,cast42,1449735473,,0,15
249,2015-12-10,2015,12,10,18,3w79uj,SSD: Single Shot MultiBox Detector,https://www.reddit.com/r/MachineLearning/comments/3w79uj/ssd_single_shot_multibox_detector/,cesarsalgado,1449738954,,0,10
250,2015-12-10,2015,12,10,18,3w7d5v,Idea on a simple word representation learning technique?,https://www.reddit.com/r/MachineLearning/comments/3w7d5v/idea_on_a_simple_word_representation_learning/,swentso,1449740960,"I'm trying to find a simple (unsupervised maybe) way of learning word representation for Arabic language. 

I'm stating ""simple"" because (1) the Arabic language itself is pretty complicated, so including any semantics would be problematic. (2) I only have the weekend to implement a POS tagger, so I do need to easily learn a word representation to start with.

For the size, i've got about about 2K sentences, and my data is already tokenized.",5,0
251,2015-12-10,2015,12,10,19,3w7gfi,My ct st 40 Trung Quc,https://www.reddit.com/r/MachineLearning/comments/3w7gfi/my_ct_st_40_trung_quc/,lamnq0514,1449742998,,0,1
252,2015-12-10,2015,12,10,21,3w7tjt,What algorithms want,https://www.reddit.com/r/MachineLearning/comments/3w7tjt/what_algorithms_want/,stormforce7916,1449751147,,0,0
253,2015-12-10,2015,12,10,21,3w7vks,"Hadn't Seen a Post on the D-Wave Quantum Annealing Yet, Thoughts?",https://www.reddit.com/r/MachineLearning/comments/3w7vks/hadnt_seen_a_post_on_the_dwave_quantum_annealing/,Saedeas,1449752272,,3,3
254,2015-12-10,2015,12,10,22,3w80jr,Very deep rnn,https://www.reddit.com/r/MachineLearning/comments/3w80jr/very_deep_rnn/,pedromnasc,1449754877,"Is there any paper about very deep rnn? I mean instead of using a neural network with 2 or 3 recurrent layers, it is using 5, 6, ... recurrent layers (of course, diminishing the number of neurons in each layer). I can think as the main advantage of this architecture the ability to learning many different time scales. Anyone can say the advantages or disadvantages of using this kind of architecture? Is there a tradeoff between the number of layers and the number of neurons in each layer? (in the case of rnn)",26,8
255,2015-12-10,2015,12,10,23,3w88it,Imagenet ILSVRC 2015 results,https://www.reddit.com/r/MachineLearning/comments/3w88it/imagenet_ilsvrc_2015_results/,matsiyatzy,1449758528,,33,85
256,2015-12-11,2015,12,11,0,3w8i4v,Easy way to get TensorFlow notebook on Azure,https://www.reddit.com/r/MachineLearning/comments/3w8i4v/easy_way_to_get_tensorflow_notebook_on_azure/,noodlefrenzy,1449762394,,1,3
257,2015-12-11,2015,12,11,0,3w8jxs,Camelyon16 - Challenge in cancer metastases detection,https://www.reddit.com/r/MachineLearning/comments/3w8jxs/camelyon16_challenge_in_cancer_metastases/,Babak-Ehteshami,1449763065,"Hello /r/MachineLearning,

We are challenging strong groups in the Machine Learning and Image Analysis field to take on the difficult task of detecting metastases in microscopic images of lymph node tissue sections. Presence of metastasis in lymph nodes of cancer patients is a poor prognostic sign and prompts for more intense treatment. Automated detection of metastases is highly meaningful and holds a great promise to improve the diagnostic process.

We are providing two large datasets (400 whole slide microscopic images of size 218,000 * 95,000 pixels) from the Radboud University Medical Center (Nijmegen, the Netherlands), as well as the University Medical Center Utrecht (Utrecht, the Netherlands). This will be the first challenge using whole-slide images in histopathology giving the participants huge amounts of data to train their systems. To see an example image with annotated tumor region have a look at the link below:

[Data description](http://camelyon16.grand-challenge.org/data/)

Please check out our website for further information about this challenge.

[camelyon16.grand-challenge.org](http://camelyon16.grand-challenge.org/)",32,20
258,2015-12-11,2015,12,11,1,3w8otx,What is wrong with data challenges,https://www.reddit.com/r/MachineLearning/comments/3w8otx/what_is_wrong_with_data_challenges/,bonoboTP,1449764848,,4,10
259,2015-12-11,2015,12,11,1,3w8sue,Super-Resolution using Residual-Learning Network,https://www.reddit.com/r/MachineLearning/comments/3w8sue/superresolution_using_residuallearning_network/,kjw0612,1449766217,,17,50
260,2015-12-11,2015,12,11,2,3w8xxu,How to extract words that identify the author's gender in a given text? (x-post from /r/learnprogramming),https://www.reddit.com/r/MachineLearning/comments/3w8xxu/how_to_extract_words_that_identify_the_authors/,redhotchiliguy,1449768088,"Here's the problem:

I have a repository with 150+ texts, created by random people. This repository will be bigger later, with more than thousands entries. They are all in .docx and I need to run all the texts to identify author's gender in each text; use the words that identified to throw in a IA so the machine can learn how to later identify more preciselly the author's gender.

I know this may be not the simplest of the questions here but I already searched a lot how to do it ,with no succes.

Edit: I want to create a gender classifier, because I already have the documents. Some are already sort by gender. But before getting into learning and testing, I'd like to know how to retrieve info of each document that allows me to know the features that would give me 'tips' regarding gender.

Thanks in advance!",4,1
261,2015-12-11,2015,12,11,2,3w8y4l,Alternating Least Squares,https://www.reddit.com/r/MachineLearning/comments/3w8y4l/alternating_least_squares/,Otternonsnse,1449768157,"Hi,

I have a good understanding of how a matrix factorisation algorithm works, namely with Gradient descent. 

I also ""think"" that I have a good enough understanding of how ALS works. However, does anyone have any good video material with a concrete explanation/walk through of a practical case?

If so, that would be fantastic.",0,0
262,2015-12-11,2015,12,11,3,3w9cv3,Profile Likelihood vs. Kriging Variance in Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/3w9cv3/profile_likelihood_vs_kriging_variance_in/,Zephyr314,1449773353,,0,1
263,2015-12-11,2015,12,11,3,3w9cz8,Likelihood for Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/3w9cz8/likelihood_for_gaussian_processes/,Zephyr314,1449773395,,1,3
264,2015-12-11,2015,12,11,4,3w9g8u,Proving that gradient descent is effective.,https://www.reddit.com/r/MachineLearning/comments/3w9g8u/proving_that_gradient_descent_is_effective/,alexgotyou,1449774526,"Hi! I'm a high school student currently writing a paper about neural networks and the backpropogation algorithm.

I've understood pretty much all the basics of neural networks and gradient descent(I am still wrapping my head around backpropogation though). 

One of the tasks i have been given is to explain why stochastic gradient descent is a much more effective approach than other minimization approaches.

I noticed that one of the e-books I've been reading(http://neuralnetworksanddeeplearning.com/chap1.html) mentions that you can prove that gradient descent is the optimal strategy for minimizing a function, and that this can be proved using the Cauchy-Schwarz inequality(search for ""Cauchy-Schwarz"" on the page to get to where it's talked about).

I would love to include such a proof, but I'm having trouble finding out where to start. I would appreciate any help/tips :)

",9,3
265,2015-12-11,2015,12,11,5,3w9qmy,pre-start health &amp; safety racking inspection,https://www.reddit.com/r/MachineLearning/comments/3w9qmy/prestart_health_safety_racking_inspection/,Avayatelecom,1449778153,,0,1
266,2015-12-11,2015,12,11,5,3w9r4e,Recommend papers website used at NIPS DL workshop,https://www.reddit.com/r/MachineLearning/comments/3w9r4e/recommend_papers_website_used_at_nips_dl_workshop/,SuperFX,1449778335,,0,5
267,2015-12-11,2015,12,11,5,3w9tr4,Facebook to Publish Designs for `Big Sur' AI Computer Hardware,https://www.reddit.com/r/MachineLearning/comments/3w9tr4/facebook_to_publish_designs_for_big_sur_ai/,julian88888888,1449779286,,5,15
268,2015-12-11,2015,12,11,5,3w9xh6,Human-level concept learning through probabilistic program induction,https://www.reddit.com/r/MachineLearning/comments/3w9xh6/humanlevel_concept_learning_through_probabilistic/,bitcczzzvv,1449780582,,13,26
269,2015-12-11,2015,12,11,10,3wb6p9,MSRA's Deep Residual Learning for Image Recognition,https://www.reddit.com/r/MachineLearning/comments/3wb6p9/msras_deep_residual_learning_for_image_recognition/,r-sync,1449797678,,80,99
270,2015-12-11,2015,12,11,11,3wbbs6,rap2vec: finding good rhymes,https://www.reddit.com/r/MachineLearning/comments/3wbbs6/rap2vec_finding_good_rhymes/,Jxieeducation,1449799754,"Hey,

I don't have it working ... However, would love some thoughts.

So I have a dataset of rap lyrics. I trained a word2vec model on top. 

What I did was:

1. train word2vec model on the words in lyrics

2. given a word, calculate top 200 suggestions. For each suggestion, use nltk's cmudict to get the syllables for the original and suggested word. score it base on overlap of syllable

However, the results were pretty terrible. Would love some tips.

Src: https://github.com/jxieeducation/Quick-Data-Science-Experiments-2015/tree/master/rap2vec



Example of bad rhymes...

time: [('waits', 0.08333333333333333), (""that've"", 0.06666666666666667), ('stagecoach', 0.047619047619047616), (""sign's"", 0.08333333333333333), ('retracts', 0.041666666666666664), ('nonprofit', 0.037037037037037035), (""night's"", 0.3333333333333333), ('night', 0.4444444444444444), ('month', 0.08333333333333333), ('first', 0.08333333333333333), ('elapsed', 0.05555555555555555), ('daya', 0.1111111111111111), ('date', 0.1111111111111111), ('damianos', 0.041666666666666664), ('continuum', 0.13333333333333333), ('boutin', 0.06666666666666667), ('awhile', 0.08333333333333333)]

sex: [('welles', 0.0625), ('wealth', 0.0625), ('unprotected', 0.09090909090909091), ('unconditional', 0.020833333333333332), ('tiddlywinks', 0.1), ('threesome', 0.041666666666666664), ('talks', 0.25), ('surreal', 0.0625), ('stutters', 0.041666666666666664), ('stroke', 0.2), ('stampeding', 0.027777777777777776), ('sequels', 0.14285714285714285), ('seductive', 0.125), ('seafood', 0.05), ('satisfaction', 0.09090909090909091), ('regrets', 0.14285714285714285), ('protection', 0.125), ('premarital', 0.025), ('pleasure', 0.05), ('pistachios', 0.027777777777777776), ('perplexed', 0.28125), ('objecting', 0.125), ('necessary', 0.125), ('masturbate', 0.03125),  ('affection', 0.14285714285714285), ('acquainted', 0.03571428571428571)]

 


",11,6
271,2015-12-11,2015,12,11,12,3wbmee,What job's are involved with machine learning.,https://www.reddit.com/r/MachineLearning/comments/3wbmee/what_jobs_are_involved_with_machine_learning/,Highfivesghost,1449804015,I'm taking a computer science class right now. I've learned about machine learning and nearul networks and I think it's really cool. So what job's can be involved with machine learning.,6,0
272,2015-12-11,2015,12,11,13,3wbyo3,How was the NIPS Deep Learning Symposium?,https://www.reddit.com/r/MachineLearning/comments/3wbyo3/how_was_the_nips_deep_learning_symposium/,20150831,1449809437,"Couldn't make the symposium but looks like there were a lot of interesting papers and speakers:

https://sites.google.com/site/nips2015deeplearnings/home/schedule

Anyone have a review of the symposium? Was attendance huge?",4,6
273,2015-12-11,2015,12,11,17,3wcoqi,"[Question] Any information on exactly how Facebook's M, human assisted AI, maps semantic meanings of questions to their associated answers (white papers, code libraries, frameworks)?",https://www.reddit.com/r/MachineLearning/comments/3wcoqi/question_any_information_on_exactly_how_facebooks/,buttocks_of_stalin,1449823576,"I ask this question because I am wondering how does one exactly go about building the basic backend structure of a human-assisted AI that understands text/verbal based questions and statements? I'm just a bit lost and don't even know where to start googling for more answers. Sorry if this is a newbie question or not applicable to this sub. 

I know M primarily uses Wit.ai for its language processing which Facebook acquired. Wit is decently open so theoretically I can build an MVP of M that can have voice/text processing of input.

Is there any information on how facebook goes about building the connections between the questions/inputs and answers? Is there a standard machine learning library that maps these relations? For example, if I was asking this question about picture/visual object recognition, I would be looking for someone to point me to Theano and Keras type libraries that build the backpropagation of the neural networks. Basically, I'm wondering what the equivalent to speech is and likely what facebook is using. 
",3,1
274,2015-12-11,2015,12,11,18,3wcqkc,5 Free Sources Of Geo DataSet,https://www.reddit.com/r/MachineLearning/comments/3wcqkc/5_free_sources_of_geo_dataset/,jonsabata,1449824831,,0,1
275,2015-12-11,2015,12,11,18,3wcudh,Another self-driving car hits the road in China,https://www.reddit.com/r/MachineLearning/comments/3wcudh/another_selfdriving_car_hits_the_road_in_china/,john_philip,1449827218,,0,0
276,2015-12-11,2015,12,11,19,3wcz1y,A Learning Advance in Artificial Intelligence Rivals Human Abilities,https://www.reddit.com/r/MachineLearning/comments/3wcz1y/a_learning_advance_in_artificial_intelligence/,mikkom,1449830310,,1,1
277,2015-12-11,2015,12,11,21,3wdcib,Using AMD GPU's To Double Your Memory &amp; Comp Power,https://www.reddit.com/r/MachineLearning/comments/3wdcib/using_amd_gpus_to_double_your_memory_comp_power/,LeavesBreathe,1449838440,"Hey Guys,

Simple question: Why do we not use AMD cards vs Nvidia? The [Radeon R9 390X](http://www.newegg.com/Product/Product.aspx?Item=N82E16814121973) has 8gb of memory, 6TFlops, and 384gb/s transfer.

And Yet it is $430 new?!?!? The 980 Ti, which has only 6gb of memory and 334gb/s transfer costs $650 new. 

You can get two R9 390x for less than one titan x. How does this make any sense? 

Deep Learning aside, wouldn't AMD just destroy NVIDIA in the gamer's market?

Clearly, I'm missing something big because I can't be the first to notice this. ",34,3
278,2015-12-11,2015,12,11,23,3wdl9t,What can be done if the classes are sparse when training a neural network?,https://www.reddit.com/r/MachineLearning/comments/3wdl9t/what_can_be_done_if_the_classes_are_sparse_when/,eygrr,1449842907,"Hi,

I'm training a Neural Network that has a large amount of negative classifications (0) and a small amount of positive classifications (1) in the data. I'm wondering if there is a cost function or activation function (or something else) that can accommodate this. All thoughts appreciated.",11,5
279,2015-12-11,2015,12,11,23,3wdlui,Implementing a CNN for Text Classification in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/3wdlui/implementing_a_cnn_for_text_classification_in/,pogopuschel_,1449843160,,0,11
280,2015-12-11,2015,12,11,23,3wdmtu,Machine Learning Inspired by Human Learning | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/3wdmtu/machine_learning_inspired_by_human_learning_mit/,not-much,1449843607,,0,9
281,2015-12-11,2015,12,11,23,3wdr9e,Numerai  A global AI tournament to predict the stock market,https://www.reddit.com/r/MachineLearning/comments/3wdr9e/numerai_a_global_ai_tournament_to_predict_the/,dsernst,1449845562,,78,81
282,2015-12-12,2015,12,12,0,3wdtxt,A Learning Advance in Artificial Intelligence Rivals Human Abilities,https://www.reddit.com/r/MachineLearning/comments/3wdtxt/a_learning_advance_in_artificial_intelligence/,[deleted],1449846728,[deleted],0,1
283,2015-12-12,2015,12,12,0,3wdxyk,Generative Recipes Model ?,https://www.reddit.com/r/MachineLearning/comments/3wdxyk/generative_recipes_model/,Buanderie,1449848375,"Hello everyone.
I was wondering... If I build a regression model that maps cooking recipes from a website (for example) to their ratings (taken from the same website)... How could I use this model to generate cooking recipes whose ratings would at least as good as ""x"" ?",3,2
284,2015-12-12,2015,12,12,1,3we38e,Sensitivity analysis of machine learning models,https://www.reddit.com/r/MachineLearning/comments/3we38e/sensitivity_analysis_of_machine_learning_models/,fabhan,1449850371,"I want to study the gradient of some of the common non-neural-net ML models (e.g. gradient boosted trees) with respect to the input features. Are there any prior works in this space?

Thanks.",1,1
285,2015-12-12,2015,12,12,3,3wemlj,Facebook's End-To-End Memory Networks using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/3wemlj/facebooks_endtoend_memory_networks_using/,[deleted],1449857417,[deleted],0,1
286,2015-12-12,2015,12,12,3,3wens0,Facebook's End-To-End Memory Networks using Tensorflow,https://www.reddit.com/r/MachineLearning/comments/3wens0/facebooks_endtoend_memory_networks_using/,carpedm20,1449857829,,0,12
287,2015-12-12,2015,12,12,4,3wexm6,Sliding Window code and polling for suggestions [C#],https://www.reddit.com/r/MachineLearning/comments/3wexm6/sliding_window_code_and_polling_for_suggestions_c/,Thistleknot,1449861413,"I have some code I'm using for neural network sliding windows.

I was wondering if there is an easier way to do it, maybe using a Dictionary file.

Code:
http://pastebin.com/6hrczUkq",0,0
288,2015-12-12,2015,12,12,5,3wfajp,Why my VAE for toy dateset doesn't learn?,https://www.reddit.com/r/MachineLearning/comments/3wfajp/why_my_vae_for_toy_dateset_doesnt_learn/,colinfang,1449866230,"I played with VAE (Variational Auto-encoder) for a couple of days. It works pretty well on the demo dataset like mnist or frayface. However, when I apply it to my own dataset, it doesn't work at all.

I cannot find any reference regarding the potential difficulties in training VAE, so I made a tiny test case.


My toy dataset is as follows:

    np.random.seed(1)
    X = np.random.choice([0.05, 0.95], n)


And my VAE structure is like:


    X -&gt; hidden layer 1 (50 units)
      -&gt; hidden layer 2 (50 units)
      -&gt; code layer (1 units)
      -&gt; sampling layer (1 sampling)
      -&gt; hidden layer 3 (50 units)
      -&gt; hidden layer 4 (50 units)
      -&gt; X (with Bernoulli loss + KL)


The input is a probability, I hope after learning, VAE can figure out that the latent variable is actually X itself.

However, I tried Bernoulli as well as Gaussian loss for the output layer, neither works.

The code layer is stuck with the prior N(0,1), hence the prediction is always around 0.5.

The toy script is here, https://gist.github.com/colinfang/20abd925ad3140236e63. The majority of the code comes from https://jmetzen.github.io/2015-11-27/vae.html

Any comment is appreciated.",2,3
289,2015-12-12,2015,12,12,5,3wfak4,The most important aspects of deep networks I learned from NIPS,https://www.reddit.com/r/MachineLearning/comments/3wfak4/the_most_important_aspects_of_deep_networks_i/,NIPSthrowaway,1449866234,"Fixed points.

Correlations.

Conference would have been a lot better if they provide food that is actually edible. But to be fair. The desserts were awesome. That's how you get people fat.",8,0
290,2015-12-12,2015,12,12,6,3wfkov,Inside the GPU Clusters That Power Baidu's Neural Network,https://www.reddit.com/r/MachineLearning/comments/3wfkov/inside_the_gpu_clusters_that_power_baidus_neural/,[deleted],1449870197,[deleted],0,5
291,2015-12-12,2015,12,12,7,3wfnd6,"open AI launches with 1 billion dollars of funding, poaching Ilya Sutskever as research director",https://www.reddit.com/r/MachineLearning/comments/3wfnd6/open_ai_launches_with_1_billion_dollars_of/,fhuszar,1449871298,,2,7
292,2015-12-12,2015,12,12,7,3wfqip,"OpenAI - a non-profit AI research company, launched by Sutskever, Musk, Bengio, Zaremba and many others",https://www.reddit.com/r/MachineLearning/comments/3wfqip/openai_a_nonprofit_ai_research_company_launched/,elanmart,1449872559,,101,310
293,2015-12-12,2015,12,12,8,3wfw8s,Baidu's Deep Speech Poster from NIPS,https://www.reddit.com/r/MachineLearning/comments/3wfw8s/baidus_deep_speech_poster_from_nips/,ekelsen,1449874903,,2,7
294,2015-12-12,2015,12,12,8,3wfxky,Basic question - What is a frame?,https://www.reddit.com/r/MachineLearning/comments/3wfxky/basic_question_what_is_a_frame/,[deleted],1449875494,[deleted],0,2
295,2015-12-12,2015,12,12,8,3wfyeu,Human-level concept learning through probabilistic program induction (Non neural net based learning),https://www.reddit.com/r/MachineLearning/comments/3wfyeu/humanlevel_concept_learning_through_probabilistic/,omniron,1449875851,,2,3
296,2015-12-12,2015,12,12,17,3whqc5,Deep Learning: where is it in the hype cycle?,https://www.reddit.com/r/MachineLearning/comments/3whqc5/deep_learning_where_is_it_in_the_hype_cycle/,oneweirdkerneltrick,1449907312,"Every year I feel like ""this is the year where interest in deep learning will plateau"", but it seems like each year there are more big announcements (LeCun+FAIR, Ng+Baidu, DeepMind, now OpenAI), record NIPS attendance, and inevitably, more stupid articles. What are people's thoughts on where deep learning is in the hype cycle? (As a deep learning researcher it's fun to have public interest in our work, but the hype articles have been particularly frustrating. I guess you can't have your cake and eat it too.)",41,20
297,2015-12-12,2015,12,12,19,3wi1fc,can arctan improve deep network?,https://www.reddit.com/r/MachineLearning/comments/3wi1fc/can_arctan_improve_deep_network/,godspeed_china,1449915234,"I found that arctan function is less saturated compared to logistic function. Thus the back-propagated gradient is not vanishing. I trained my first deep network (&lt;20 layers) without pre-training, using lbfgs, and it seems to converge to a good local optima. Anyone tried or confirmed this phenomenon?",3,2
298,2015-12-12,2015,12,12,20,3wi6ea,NYT: A Learning Advance in Artificial Intelligence Rivals Human Abilities,https://www.reddit.com/r/MachineLearning/comments/3wi6ea/nyt_a_learning_advance_in_artificial_intelligence/,charloBravie,1449919010,,0,0
299,2015-12-12,2015,12,12,21,3wiagk,"the CAT signal :3 (volume warning, sorry)",https://www.reddit.com/r/MachineLearning/comments/3wiagk/the_cat_signal_3_volume_warning_sorry/,learnin_no_bully_pls,1449922218,,3,3
300,2015-12-12,2015,12,12,23,3wipt8,"Data Science for Losers, Part 6  Azure ML",https://www.reddit.com/r/MachineLearning/comments/3wipt8/data_science_for_losers_part_6_azure_ml/,[deleted],1449931919,[deleted],0,1
301,2015-12-13,2015,12,13,0,3wivqi,Google released inception v3. With 3.5% Top-5 error.,https://www.reddit.com/r/MachineLearning/comments/3wivqi/google_released_inception_v3_with_35_top5_error/,oddjob_,1449934927,,9,73
302,2015-12-13,2015,12,13,0,3wixqs,a little help with tensorflow,https://www.reddit.com/r/MachineLearning/comments/3wixqs/a_little_help_with_tensorflow/,s_kar,1449935926,"I was trying out tensorflow. I could not understand what **tf.nn.softmax_cross_entropy_with_logits** does. The api docs are here - https://www.tensorflow.org/versions/master/api_docs/python/nn.html#softmax_cross_entropy_with_logits.


I posted a question on stackoverflow regarding this. No response.
Link to SO - 

http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with

can anyone help me out?",1,1
303,2015-12-13,2015,12,13,3,3wjjcs,Machine Learning books for self taught software engineers?,https://www.reddit.com/r/MachineLearning/comments/3wjjcs/machine_learning_books_for_self_taught_software/,oismail91,1449944894,"I am a self taught software engineer. I have been programming profesisonally for two years, primarily in Ruby and JavaScript (web development). I have recently learned Python.

Anyone know any books that are a good introduction to machine learning and the math behind it. I only took calculus in college, so the matrix stuff and mathematical notation i see in a lot of tutorials throws me off. I need a book that is code first and then theory. 

Any suggestions? I prefer books than audio/video. ",10,3
304,2015-12-13,2015,12,13,4,3wjtcg,The current state of machine intelligence 2.0,https://www.reddit.com/r/MachineLearning/comments/3wjtcg/the_current_state_of_machine_intelligence_20/,speckz,1449948758,,5,30
305,2015-12-13,2015,12,13,4,3wjwun,Projects or topics for 4th year math/stat major student who want to pursue a MS in ML.,https://www.reddit.com/r/MachineLearning/comments/3wjwun/projects_or_topics_for_4th_year_mathstat_major/,bot_cereal,1449950133,"TL;DR...Looking for project on machine learning that involve a lot of math (pure math/statistics) for 4th year Math student.

Hi,
I am in my final year of my undergraduate in math and stat. I am thinking about taking a project/research courses. (I am tired of taking courses that I have no interested in and I believe it will improve my chance of get accepting to top school) Since I want to pursue MS in ML/STAT, I want to find the topic that relate to machine learning but ,at the same time, involve a lot of math.

I have a pretty solid background in mathematics and statistics. I am familiar with python(taught myself), SAS, SQL and maple.
 

Some topics that I find interested are
-manifold learning
-Fourier Transform and its application in ML
-Dimension reduction
-Fuzzy Logic
-Density Estimation
-NN
-GA (Not sure if there's any math involve here)
-TDA

pardon for my poor english",2,4
306,2015-12-13,2015,12,13,6,3wk6wl,Tips on buying/building a computer with a GPU for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3wk6wl/tips_on_buyingbuilding_a_computer_with_a_gpu_for/,ShittyAlgorithms,1449954173,"Hey everyone,

So I've become increasingly interested in machine learning, and especially deep learning, within the past year or so. I keep building neural networks on my evenings and weekends and then purchasing AWS GPU instances to train them. It's at the point where I think I should consider purchasing a GPU to save money in the long run (and also allow me to train longer, with less hassle).

Does anyone have any tips or know of any good guides for purchasing a GPU or building a computer with a GPU in it? I wouldn't use the computer for anything other than machine learning.

Thanks!",26,12
307,2015-12-13,2015,12,13,7,3wkniw,What is the standard practice to train a deep networks for multiple train/test splits?,https://www.reddit.com/r/MachineLearning/comments/3wkniw/what_is_the_standard_practice_to_train_a_deep/,slack4ever,1449960597,"Hi all. I am new to deep learning, so pardon me for my ignorance. I am training a small network that takes about 10 hours (about 40K epoch). I am reading some papers (SVR on a dataset) and they say that they train their models 50-100 times with different train/test splits and then they take the mean or median of all the runs to get the final score. So, I guess that their training takes less time. What should one do to a deep network? Should one train each train/test split from scratch? Can one 'cheat' by using a 'pretty good' network as an initial start?",4,2
308,2015-12-13,2015,12,13,10,3wl90l,libraries usable for streaming data,https://www.reddit.com/r/MachineLearning/comments/3wl90l/libraries_usable_for_streaming_data/,NightFantom,1449969809,"Hi there, 

Because of reasons I have a C++ interface that can access data in a file in small chunks, but it is infeasible to convert this to files for input for another program. The final result has to run on windows too (and for portability I'd like to avoid visual c++, so I'm currently using Qt with Mingw on windows and with gcc on linux).

Are there any libraries / programs out there that either allow data to be ""sent"" to them through the use of pipes (e.g. 

    myprogram.exe inputfile.dat | learn.exe 

producing learned.model) and/or are c++-based /interfaceable from c++ that compile on windows? At this point anything remotely machine learning-related that actually works would be a godsend.

Thanks!",7,2
309,2015-12-13,2015,12,13,12,3wlpqz,On the integrity of the reviewing process,https://www.reddit.com/r/MachineLearning/comments/3wlpqz/on_the_integrity_of_the_reviewing_process/,cedricdb,1449977497,,2,3
310,2015-12-13,2015,12,13,13,3wlx9g,my-own-machine-voice?,https://www.reddit.com/r/MachineLearning/comments/3wlx9g/myownmachinevoice/,JakeTheMaster,1449981130,"If computer can automatically generate your own voice, and talk to you, will that be cool?

I read the article from [here](http://www.acapela-group.com/meet-eric-and-garmt-my-own-voice-users/). 

*There are several procedures of doing this*:

&gt; 1. You read some word samples
&gt; 2. Machine learns your voices
&gt; 3. Generate you-own-voice by machine

    So, are there some projects like this in public?
    
    Can you list some opensource my-own-voice alike projects?

If I want to have my-own-machine-voice.

Appreciate your comments!

Many thanks!",21,7
311,2015-12-13,2015,12,13,15,3wmd8h,"Books on Neural Networks, Deep Learning",https://www.reddit.com/r/MachineLearning/comments/3wmd8h/books_on_neural_networks_deep_learning/,VagueRequiem,1449989909,"I am currently looking for books on neural networks and deep learning structures.  I've seen Bishop's Neural Networks for Pattern Recognition recommended quite a lot, but it is a relatively older book.  Are there any other (preferably newer) books on neural networks and/or deep learning that are recommended?  

EDIT: In addition, if there are any ""must have"" books on the topic of machine learning in general, please feel free to recommend them as well.",9,8
312,2015-12-13,2015,12,13,16,3wmhkm,On the spirit of NIPS 2015 and OpenAI,https://www.reddit.com/r/MachineLearning/comments/3wmhkm/on_the_spirit_of_nips_2015_and_openai/,gwulfs,1449992615,,1,14
313,2015-12-13,2015,12,13,20,3wn0hq,Better Optimization with Repeated Cross Validation and the XGBoost model - Machine Learning with R,https://www.reddit.com/r/MachineLearning/comments/3wn0hq/better_optimization_with_repeated_cross/,cast42,1450007086,,0,0
314,2015-12-13,2015,12,13,21,3wn1z8,"NIPS 2015, or ""How modern conference poster sessions aren't very modern""",https://www.reddit.com/r/MachineLearning/comments/3wn1z8/nips_2015_or_how_modern_conference_poster/,thatguydr,1450008284,"I attended NIPS this year, and it was a fun time. Lots of great talks, lots of good food (catered!), lots of people. **Lots** of people. Almost 4000.

There were four poster sessions, each with 100 posters. If you're reading this subreddit, you can do the math in your head. If you can picture what happens when you combine math and rugby, you have a good approximation of the NIPS poster sessions.

Did I mention that conference attendance is growing exponentially? The organizers made sure to point this out in the opening session. Next year, there will likely be 5000 or 6000 people. Making the problem worse: the number of papers is only growing by 10% year-over-year.

The issue is simple: **Modern conference-based poster sessions don't scale.**

Deep learning is a hot topic, the attention it's getting will not diminish any time soon, and the solution I heard multiple people floating at the conference was, ""Let's cap attendance next year."" And that's a terrible solution. We have an amazing field that's making revolutionary leaps and bounds, and rather than finding a scalable process for knowledge dissemination, we're thinking of limiting it?

The only (obvious) solution I can think of is to have modern math papers come with a 4-6 minute video of a ""poster presentation"" where the motivation, analysis/methodology/algorithm, and results are laid out. Doing so would facilitate much faster transmission of results across a much larger swath of the math community. It would also allow the NIPS organizers to avoid having to find helmets for people for next year. It would also bring math into the 21st century, where I can find any manner of how-to information on YouTube, but no descriptions of recently published math results.

I'd like to know from the people here who were at NIPS this year (or similarly large conferences, like ICML) what your feelings were on the poster sessions and whether you think videotaped poster sessions (or anything similar) sound reasonable. They could always have actual poster sessions as well, but allowing attendees to focus on Q&amp;A rather than dissemination of technique and results might be really useful.

**tl;dr** The NIPS poster sessions this year were 70% math and 30% rugby, and we should try to find a long-term solution for the problem of efficient knowledge dissemination at large mathematics conferences.",26,17
315,2015-12-13,2015,12,13,21,3wn2l5,"Is a graduate degree needed to get a position at a place like openAI, deepmind, etc? How about a bachelors?",https://www.reddit.com/r/MachineLearning/comments/3wn2l5/is_a_graduate_degree_needed_to_get_a_position_at/,[deleted],1450008733,[deleted],12,0
316,2015-12-13,2015,12,13,21,3wn3m9,Keras IndexError: One of the index value is out of bounds,https://www.reddit.com/r/MachineLearning/comments/3wn3m9/keras_indexerror_one_of_the_index_value_is_out_of/,srepho,1450009514,"I am playing around with Keras for NLP and unfortunately am stuck on the simple part of converting text into a format I can use. I am starting with the imdb cnn example located in the example folder [link](https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py). Instead I am using the data from the Kaggle Movie competition [link](https://www.kaggle.com/c/word2vec-nlp-tutorial). So I read in the dataset to a pandas data frame. From here I need to use the keras.preprocessing.text utils

    train_df = pd.read_csv(""/home/ubuntu/kaggle/Movie/labeledTrainData.tsv"", sep=""\t"")
    Reviews = tuple(train_df.review)
    
    Sentiment = tuple(train_df.sentiment)

    TKN = text.Tokenizer(nb_words=10000)
    TKN.fit_on_texts(Reviews)
    Seq_Rev = TKN.texts_to_sequences(Reviews)

Then I just follow the code from the example. Unfortunately I end up getting an error

    IndexError: One of the index value is out of bound. Error code: 65535.\n",2,0
317,2015-12-13,2015,12,13,22,3wn7c8,Code for Generative Handwriting using LSTM Mixture Density Networks with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3wn7c8/code_for_generative_handwriting_using_lstm/,cryptocerous,1450011931,,7,26
318,2015-12-13,2015,12,13,23,3wnhqa,"Python labeling framework/software, don't remember the name and can't find it",https://www.reddit.com/r/MachineLearning/comments/3wnhqa/python_labeling_frameworksoftware_dont_remember/,learnin_no_bully_pls,1450018138,"I read about a labeling framework/software in python, extensible to any task you need to label for. I sadly can't find it anymore or remember the name. Has anyone heard about it?

By the way, can I suggest restoring the simple questions thread? I think it would be very useful for beginners (like me!).",4,2
319,2015-12-14,2015,12,14,0,3wno5e,"In your experience, which machine learning course on Coursera (or other MOOC web site) was the best?",https://www.reddit.com/r/MachineLearning/comments/3wno5e/in_your_experience_which_machine_learning_course/,sircharles---,1450021344,"There's a lot of options out there - Stanford, Wesleyan, University of Washington, and Johns Hopkins all have their own machine learning courses. 

In your experience, which was the most practical/useful?",61,128
320,2015-12-14,2015,12,14,0,3wnppy,Is knowledge gained simply by (correctly) assuming a relationship exists?,https://www.reddit.com/r/MachineLearning/comments/3wnppy/is_knowledge_gained_simply_by_correctly_assuming/,[deleted],1450022097,[deleted],0,0
321,2015-12-14,2015,12,14,0,3wnq3p,"Career Advice Needed, also Vent. [Very Long]",https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/,LoSpooky,1450022304,"Hi /r/machinelearning!

I'm currently very lost and confused regarding my professional future and I could really use some advice.
I have no idea where to begin from as there's no concise way to summarize it all and I also feel like writing it all down will help me get through the situation... 

&amp;nbsp;

So I will start from the beginning and write you the whole story.

&amp;nbsp;

This is going to be very long so **Warning: Walls of Text Incoming!**

[**Part I**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxxjpta)

[**Part II**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxxjrda)

[**Part III**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxxjsd5)

[**Part IV**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxxjt4v)

[**Part V**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxxju2f)

[**Part VI**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxxjuun)

&amp;nbsp;

[**TL;DR Edition**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxy5yzq)

[**TL;DR(TL;DR) Edition**](https://www.reddit.com/r/MachineLearning/comments/3wnq3p/career_advice_needed_also_vent_very_long/cxy63aj)

&amp;nbsp;

Thanks in advance to all those who'll take the time to read this and to comment. Much appreciated.

**13/12 17:10 :** As a born and raised Milanese I will now go prepare myself for Aperitivo later tonight and I will be checking on the thread later tonight, but more likely tomorrow.

**14/12 02:47:** Tl;dr version and Tl;dr(Tl;dr) versions up
",19,0
322,2015-12-14,2015,12,14,2,3wnyyz,Is it possible to inject domain knowledge into neural networks?,https://www.reddit.com/r/MachineLearning/comments/3wnyyz/is_it_possible_to_inject_domain_knowledge_into/,thefuckisth4t,1450026192,"I have data where I know the a lot about the form of the underlying signal I'm interested in, but there's a lot of noise and different signals can be entangled. 
In [this podcast](http://www.thetalkingmachines.com/blog/) at around 28.00, the speaker mentions [this paper](http://iopscience.iop.org/article/10.1088/2041-8205/776/2/L25). He then goes on to talk about 'big models' (that is, using domain knowledge) has become more normal for ANNs. Can any one point me in the direction of these things? It seems very tempting when you have little data but quite a bit of domain knowledge.",6,4
323,2015-12-14,2015,12,14,2,3wo5tp,Sampling Nearly-Equidistant Points,https://www.reddit.com/r/MachineLearning/comments/3wo5tp/sampling_nearlyequidistant_points/,TheSwitchBlade,1450028992,,0,2
324,2015-12-14,2015,12,14,3,3wobwa,What did you learn at NIPS ?,https://www.reddit.com/r/MachineLearning/comments/3wobwa/what_did_you_learn_at_nips/,guillaumedehaene,1450031486,"This was my first NIPS, and it was absolutely wonderful ! I feel like I learned quite a lot about approximate inference (my focus right now), MCMC methods, practical challenges of deep learning. It was a super motivating experience

I'd say the highlights for me were:

- the deep art result from the Betghe group (which was presented both a the deep learning symposium and on a poster)

- Vapnik's talk, though I got lost at some point along the way

- The ""Bayesian quadrature"" workshop organized by P. Henning: so many interesting questions to tackle there

- Meeting a bunch of very interesting and intelligent colleagues

What were yours ?",6,1
325,2015-12-14,2015,12,14,3,3woe0m,How do I visualize the weights of a multilayer perceptron?,https://www.reddit.com/r/MachineLearning/comments/3woe0m/how_do_i_visualize_the_weights_of_a_multilayer/,bourbondog,1450032268,"I have a 3 layer mlp (one input, one hidden one output) and want to visualize the weights. What are some terms I should search for or techniques I can use?

The input is a 4000 element feature vector generated from a 32*32 image. The hidden layer is 512 nodes and the output is 10 nodes.",5,2
326,2015-12-14,2015,12,14,4,3wojjp,BT traversal,https://www.reddit.com/r/MachineLearning/comments/3wojjp/bt_traversal/,Kmoneytric,1450034570,"Sup guys, I have a problem that I've been working on for a while and I'm stuck. I have a Binary tree with a depth of 3 (8 leaves). One of the leaves is the goal state. There is a blob at the root whose goal is to reach the goal state. It can take the left or right path at each node OR it can traverse both paths with a higher cost (lets say speed decrease or something). If it reaches a leaf which is not the goal state, it can traverse back up from where it came and chose another path. I would like to create some algorithm that maps out a possible behavioral pattern given certain search strategies (like is it better to traverse left and right with the added cost or chose one path over the other. I guess this is kind of like A*). What is the best way to model something like this? I just finished my intro to AI course and it looks like a Markov Decision Process would be the best approach to this. But I'm new to machine learning and statistics so what other approaches are there? ",1,0
327,2015-12-14,2015,12,14,4,3wojxa,Collection of notes from selected NIPS 2015 talks,https://www.reddit.com/r/MachineLearning/comments/3wojxa/collection_of_notes_from_selected_nips_2015_talks/,abracaradbra,1450034724,,0,22
328,2015-12-14,2015,12,14,5,3woqqj,Prismatic is shutting down. Alternatives ?,https://www.reddit.com/r/MachineLearning/comments/3woqqj/prismatic_is_shutting_down_alternatives/,quan27081982,1450037263,"""Prismatic is a social news curation and discovery application for various Web browsers and mobile devices running iOS. It combines machine learning, user experience design, and interaction design to create a new way to discover, consume, and share media. Prismatic software uses social network aggregation and machine learning algorithms to filter the content that aligns with the interests of a specific user. ""

But now they are shutting down:
 http://blog.getprismatic.com/next-chapter/

Are they any other alternatives there?

",6,0
329,2015-12-14,2015,12,14,5,3wowoz,"Just completed Coursera's ""Machine Learning"" course -- here's my review",https://www.reddit.com/r/MachineLearning/comments/3wowoz/just_completed_courseras_machine_learning_course/,dailymorn,1450039449,,0,3
330,2015-12-14,2015,12,14,5,3woxgd,Can you recommend a good book on statistical reasoning?,https://www.reddit.com/r/MachineLearning/comments/3woxgd/can_you_recommend_a_good_book_on_statistical/,rumborak,1450039748,"I'm looking for a good, technical (no proofs though) book on statistical reasoning to start reading over the holidays. Is there a classic ""bible"" on the topic?",2,0
331,2015-12-14,2015,12,14,6,3wp5pc,Results on SVHN with Vanilla VAE?,https://www.reddit.com/r/MachineLearning/comments/3wp5pc/results_on_svhn_with_vanilla_vae/,alexmlamb,1450042985,Does anyone know of results on generating SVHN images with vanilla variational autoencoders?  ,7,1
332,2015-12-14,2015,12,14,9,3wpvs6,First impressions of Google's Tensorflow?,https://www.reddit.com/r/MachineLearning/comments/3wpvs6/first_impressions_of_googles_tensorflow/,herejust4this,1450053856,Having just finished a semester of school and having my winter break begin I was thinking of trying out Tensorflow. Since it's been out for a little while now does anybody have any first impressions of it? Is it living up to the hype or should I go down another path if I'm attempting to learn more about machine learning?,10,3
333,2015-12-14,2015,12,14,10,3wq0g0,Orthogonal Initialization of RNN's,https://www.reddit.com/r/MachineLearning/comments/3wq0g0/orthogonal_initialization_of_rnns/,LeavesBreathe,1450055943,"Hey Guys,

After reading many reddit threads and this paper: http://arxiv.org/pdf/1312.6120v3.pdf, I have come to a strong belief that orthogonal initialization of RNNs is the way to go. 

I'm talking specifically GRU's and LSTM's but in general it seems that everyone has benefited from orthogonal initialization. In the paper above, they discuss that orthogonal initialization is not as [kurtotic](https://en.wikipedia.org/wiki/Kurtosis) when dealing with multiple layers.

My main question is: What is the verdict on this? Do you guys go with orthogonal most of the time? 

If anyone wants to shed light on why orthogonal weights are so powerful, please do so. I'm still struggling to understand why they lead to improvements. ",10,1
334,2015-12-14,2015,12,14,11,3wqct4,RNN Easy Example with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3wqct4/rnn_easy_example_with_tensorflow/,dpmms,1450061298,,0,5
335,2015-12-14,2015,12,14,13,3wqsnu,Which machine learning and statistics (or related) books should I read next?,https://www.reddit.com/r/MachineLearning/comments/3wqsnu/which_machine_learning_and_statistics_or_related/,reddit_turtle_,1450068610,"I've been wondering what I should read next and thus came across [Michael Jordan's and Bradford Cross's suggestions](http://nzhiltsov.blogspot.com/2014/09/highly-recommended-books-for-machine-learning-researchers.html). Those unfortunately are huge lists and each text is a huge commitment of time. So I'd like some opinions on what I should prioritize to read next.

* I will be picking two texts, one more practical and one more theoretical. Alternating between the two daily to mix things up. Otherwise, I'd only focus on practical stuff since my current job is in applications in industry.
* My related course background: I took an undergraduate course in statistics (using Part I and II of Wasserman's All of Statistics), a graduate pattern recognition course (using Duda et al.'s Pattern Classification), and a graduate machine learning course (no text, selection of papers).
* Beyond that I did read Casella &amp; Berger's Statistical Inference (the one and only book in Jordan's list I can check off!) and Bishop's Pattern Recognition and Machine Learning. Using Bishop as the reference, my knowledge is mostly in the first half of the book (linear, basic ANNs, SVM) and the second half of the book I'm less familiar with (graphical models). I recall trying to read Robert's Bayesian Choice some years ago and it was beyond my level for the most part. Not sure if I could read it now, since I've sold it before my last move.
* My current reading is reviewing Wasserman's All of Statistics (Part I and II) and I'm about a third through Shalev-Shwartz &amp; Ben-David's Understanding Machine Learning. I'm enjoying the latter quite a bit since I glossed over the learning theory in school and so the perspective in this text is refreshing.
* For my next ""practical"" text, I'm kind of settling down on Aggarwal's Data Mining. Covers a wide range of topics I'm not familiar with (association, outlier, network analysis, etc.) that I need would be good to have in my toolbox. But I'm welcome to other suggestions.
* For my next ""theory"" text, I'm less certain. Should I dive into one of Jordan's suggested statistics books (probably Lehmann's Elements of Large Sample Theory is the reasonable one for my level) or should I start his sequence of probability texts (starting with Grimmett &amp; Stirzaker's Probability and Random Processes) to build up to measure theoretic probability because that will make the statistics texts more understandable? Or should I follow Cross's suggestions and maybe read some linear models text (e.g. Faraway's Linear Models with R), robust statistics (e.g. Huber &amp; Ronchetti's Robust Statistics), or Bayesian statistics (e.g. Gelman et al.'s Bayesian Data Analysis)? Sooo many choices.

Maybe in 10 years I could get through Jordan's and Cross's lists haha",9,14
336,2015-12-14,2015,12,14,14,3wqzaf,How to handle high dimensional time series with a hidden variable count order(s) of magnitude higher than the dimensionality?,https://www.reddit.com/r/MachineLearning/comments/3wqzaf/how_to_handle_high_dimensional_time_series_with_a/,[deleted],1450071825,[deleted],1,1
337,2015-12-14,2015,12,14,15,3wr2qx,How does the EM algorithm work for discriminative models?,https://www.reddit.com/r/MachineLearning/comments/3wr2qx/how_does_the_em_algorithm_work_for_discriminative/,nnnoob123,1450073733,"Most EM tutorials you can find online always put it in context of a generative model, i.e P(x|z) where the z's are hidden. In this case, as i understand it, in the E step you estimate P(z|x) using bayes rule, and then in the M step use the expectations of P(z) to update P(x|z).

It's hard to wrap my head around how this works, even for generative models, but its even harder for a discriminative model -- say you have a model for P(z|x), and the z's are hidden. EM would say, estimate P(z|x) given your current model for P(z|x), then in the M step, update your model of P(z|x) -- but your very estimates for P(z|x) came from your model, so how can the M step do anything? haven't you picked the exact estimates for P(z|x) that would mean your current model is correct?

Could someone help me out?",5,4
338,2015-12-14,2015,12,14,15,3wr4p7,Finished a few online courses about ML. Where do I go from here?,https://www.reddit.com/r/MachineLearning/comments/3wr4p7/finished_a_few_online_courses_about_ml_where_do_i/,picardo,1450074852,"I completed 3-4 online courses about machine learning in the last couple of years. The first one was Andrew Ng's course. When I started taking that, I had no plans on how I might use that knowledge. I thought I liked it, but needed to fill in some gaps, so I took a few courses on linear algebra, statistics, and then a few more data science courses from Johns Hopkins, also on Coursera. And finally this year I completed a course on edx, which focused on using Spark to do ML. 

All through this, I never made any plans about how I could make use of this knowledge. I always followed my curiosity, and figured it would be cool to have these courses on my resumee when applying for jobs. But I don't think I'm hirable in the field necessarily; all I've done is watch a few videos, read some books, and run a few scripts. In other words, I haven't worked on an ML project that requires me to constantly use my knowledge. 

My dayjob is as a freelance developer, building web applications. It doesn't require me to use ML, but I like what I do.

I'm curious to read what this community would recommend an amateur like me to do with what he's learned about ML through online courses. I thought I could build an open source project or an app, but all the ideas I come up with are either far beyond my ability or are someone else is doing them already. ",9,18
339,2015-12-14,2015,12,14,15,3wr6ea,Topic modeling on English and Swahili newspaper coverage of the Tanzanian general election,https://www.reddit.com/r/MachineLearning/comments/3wr6ea/topic_modeling_on_english_and_swahili_newspaper/,thebenedict,1450075717,,2,0
340,2015-12-14,2015,12,14,15,3wr6x2,Microsoft Face API Implementation,https://www.reddit.com/r/MachineLearning/comments/3wr6x2/microsoft_face_api_implementation/,[deleted],1450076014,[deleted],0,1
341,2015-12-14,2015,12,14,16,3wr82y,How do you deal with array-based categorical data?,https://www.reddit.com/r/MachineLearning/comments/3wr82y/how_do_you_deal_with_arraybased_categorical_data/,Zachary1a2b3c,1450076717,"Imagine that I have the user data that I want to ""vectorize"" for a machine learning task. It comes in the following format:

{
  subjects: [String (subject id)],
  education: [
    school: String (school id),
    concentration: String (conc. id),
    degree: String (degree id)
  ],
  age: Integer,
  gender: String
}

If I had only one subject and one education record, I would use one-hot encoding for this. However, in this case I can have an arbitrary number of subjects and an arbitrary number of education records. Not only this, but the education records are also nested and are actually a composition of three types of data: school, concentration, and degree.

What would be your recommendation for ""numericalizing"" this kind of data?",5,0
342,2015-12-14,2015,12,14,18,3wrnrl,"A curated list of Machine Learning and Deep Learning tutorials, articles and other resources",https://www.reddit.com/r/MachineLearning/comments/3wrnrl/a_curated_list_of_machine_learning_and_deep/,hX3S,1450087075,,10,289
343,2015-12-14,2015,12,14,20,3wrumh,Full MatLab vs. R Code Comparison,https://www.reddit.com/r/MachineLearning/comments/3wrumh/full_matlab_vs_r_code_comparison/,shakedzy,1450091957,"Hi! 
I ran into [this](http://math.umaine.edu/~hiebeler/comp/matlabR.pdf) when looking for MatLab vs. R info on the web. 
As a MatLab programmer learning R now, this is very handy, and I'm more than sure some of you will find it useful too.",0,0
344,2015-12-14,2015,12,14,22,3ws7dx,"Local Linear embedding (LLE) in python,GPU, estimation of final dimension, computation overhead - dimensionality reduction.",https://www.reddit.com/r/MachineLearning/comments/3ws7dx/local_linear_embedding_lle_in_pythongpu/,SnowRipple,1450099896,"Hey,
  
    I have binary target images (a pixel is a vessel or not,(0,1) discrete range) and I would like to perform dimensionality reduction on them. I used PCA and the results were okayish, so I am trying to find some better method. We can assume that data contains no noise.

    Data in real world is inherently non-linear and following success of manifold learning methods I am researching them. I like the idea behind LLE however using sklearn implementations it's very computationally intensive (after almost 2 days still didn't finish for 280k of 34x34 patches of target images). 

Is there some GPU implementation of LLE, Isomap or Spectral embedding(laplacian eigenmap)? Is it possible to implement them e.g. in tensorflow?

I read that LLE is very sensitive to parameters, is it possible to ""estimate"" final dimension? Sklearn documentation says:
http://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding

""The reconstruction error computed by each routine can be used to choose the optimal output dimension. For a d-dimensional manifold embedded in a D-dimensional parameter space, the reconstruction error will decrease as n_components is increased until n_components == d."" 

What does it mean exactly? I thought that n_components is always equal to d .

Can someone propose some other dimensionality reduction/manifold learning methods for this type of data please?

Many Thanks!
",6,0
345,2015-12-15,2015,12,15,0,3wslsj,Trending feature to predict number of views,https://www.reddit.com/r/MachineLearning/comments/3wslsj/trending_feature_to_predict_number_of_views/,moimitou,1450106793,"I am working on a problem where I have access to a database with news articles, their publication date and the number of views they got 24hrs after they got published.

**The objective is to be able to predict the number of views for any new article.**

I want to add a feature that, for a given article, gives an indication of the number of views / publications of all articles with a similar theme that happened a short time before the publication of this article. This would capture the ""buzz"" phenomenon. (If there has been a lot of articles on elections, then it is quite likely that a new article on elections will get lots of views.)

**Here is my strategy :**

We classify all articles among X themes.

We segment the timeline into Y timesteps.

For each timestep:

For each theme:

* we count the number of views of articles of this theme published in this timestep
* ditto for the number of publications
* we add these two values as features for all articles published in the next timestep

**So, I was wondering if this violates the i.i.d. assumption?** (Because, if I randomly separate the data into train/test, this injects information from the target values of the test set as a feature of the training set.) Is this ""allowed""?

Am I missing something? Do you see another strategy?",4,0
346,2015-12-15,2015,12,15,1,3wsv63,LSTM peephole connections question. LSTM Cell vs. LSTM Block?,https://www.reddit.com/r/MachineLearning/comments/3wsv63/lstm_peephole_connections_question_lstm_cell_vs/,Mattoss,1450110634,"In studying the LSTM equations (4.2) - (4.10) in Alex Graves PhD thesis I was irritated by the formulation of the peephole connections. I understand that these equations explain one particular ""block"" in an LSTM layer. By summing over the C ""memory cells"", each cell in a block influences the activation of the gates to all other cells inside this block.

I cannot recall having seen this ""grouping"" of LSTM cells into blocks in other literature and was wondering if I either misunderstood these equations, I didn't find other literature because I didn't look closely enough or if it this grouping of cells is just not done. 

What is your understanding?",7,3
347,2015-12-15,2015,12,15,1,3wsxv7,Whats Wrong with Convolution Neural Nets? (video request),https://www.reddit.com/r/MachineLearning/comments/3wsxv7/whats_wrong_with_convolution_neural_nets_video/,perunBujo,1450111614,"Hello, I once sawed video with Geoff Hinton called ""Whats Wrong with Convolution Neural Nets?"" It would really help me to watch it again, but its not online anymore. So I am asking whether somebody has an copy of it. I don't expect so, but nevertheless I ask.
It was here:
http://techtv.mit.edu/collections/bcs/videos/30698-what-s-wrong-with-convolutional-nets

Thanks",4,13
348,2015-12-15,2015,12,15,2,3wt6xn,any of you have ml-004 exercises without completed answers?,https://www.reddit.com/r/MachineLearning/comments/3wt6xn/any_of_you_have_ml004_exercises_without_completed/,ghostcheck,1450115066,"Hi,
was wondering if anyone got the original source files for ml-004 exercises on cousera.org. Most github repositories that contains exercise files are already solved/completed by the uploader. If anyone have it do send a link.",0,0
349,2015-12-15,2015,12,15,2,3wt8ho,What do computers see when we talk to them?,https://www.reddit.com/r/MachineLearning/comments/3wt8ho/what_do_computers_see_when_we_talk_to_them/,alexcaps,1450115709,,0,0
350,2015-12-15,2015,12,15,3,3wtbyg,How do I speed up K-Means?,https://www.reddit.com/r/MachineLearning/comments/3wtbyg/how_do_i_speed_up_kmeans/,[deleted],1450117084,[deleted],0,1
351,2015-12-15,2015,12,15,3,3wtfmr,How to speed up K-Means?,https://www.reddit.com/r/MachineLearning/comments/3wtfmr/how_to_speed_up_kmeans/,Lefttown,1450118714,"I've noticed the BOW methods mentioned in several papers. For example, Fei-Fei Li; Perona, P. (2005).

However, I'm having difficulty implementing it even on a small dataset (5000 images).

The clustering phase with K-Means has taken way too long time -- 12 hours for a vocabulary of 20k words. My target is 1-2M word, which would take several days using my current approach.

Right now, I'm using SIFT detectors and Spark with two nodes. I will add nodes when increasing the dtaset from 5k to 50k images.

Can you guys sugget approaches to speed this up?",12,4
352,2015-12-15,2015,12,15,3,3wth69,NeoRL - Self-Sustaining Predictions,https://www.reddit.com/r/MachineLearning/comments/3wth69/neorl_selfsustaining_predictions/,CireNeikual,1450119333,,2,1
353,2015-12-15,2015,12,15,6,3wu3sc,[Question] Why do shallow embeddings work better?,https://www.reddit.com/r/MachineLearning/comments/3wu3sc/question_why_do_shallow_embeddings_work_better/,bubaonaruba,1450127768,As above. What's the intuition behind that?,2,0
354,2015-12-15,2015,12,15,6,3wu7di,How Does Doc2Vec work?,https://www.reddit.com/r/MachineLearning/comments/3wu7di/how_does_doc2vec_work/,aashus18,1450129101,"Since it's an unsupervised algorithm - how does one assign label while training it on Google News corpus ?  Further is it possible to obtain a vector representation of new sentence based on the trained model ?

My objective is to get similar paragraphs or sentences corresponding to a query one makes on the news dataset.",4,4
355,2015-12-15,2015,12,15,7,3wuejx,My takeaways from NIPS 2015,https://www.reddit.com/r/MachineLearning/comments/3wuejx/my_takeaways_from_nips_2015/,danvk,1450131735,,4,17
356,2015-12-15,2015,12,15,7,3wufie,Optimizing Warehouse Operations with Machine Learning on GPUs,https://www.reddit.com/r/MachineLearning/comments/3wufie/optimizing_warehouse_operations_with_machine/,harrism,1450132096,,0,0
357,2015-12-15,2015,12,15,7,3wugo0,Learn about Text Classification and Sentiment Analysis,https://www.reddit.com/r/MachineLearning/comments/3wugo0/learn_about_text_classification_and_sentiment/,ataspinar,1450132530,,0,0
358,2015-12-15,2015,12,15,8,3wulsl,[Q] Train 1 NN to classify 1/10 digits vs. Train 10 NNs to each classify 1 digit?,https://www.reddit.com/r/MachineLearning/comments/3wulsl/q_train_1_nn_to_classify_110_digits_vs_train_10/,jaba0,1450134434,"Hey folks,

I'd like to train a NN to classify audio samples of me saying the digits 0-9. I plan to use TensorFlow. This is mostly a learning exercise. 

Do you think it would be faster to train one NN with 10 outputs, directly? Or, would it be faster to train 10 NNs, each with one output unit where the output of NN i means digit i recognized or digit i not recognized. Is there any advantage to training the 10 and then using those weights as input for 1 network? 

I've got a small dataset of me saying about 4 takes of each digits. I plan to generate more data by mixing in various levels of noise, and pitch-shifting and amplitude-scaling my source recordings.

Thanks for any hints. Also, if I'm making any conceptual errors here, please point them out.",4,2
359,2015-12-15,2015,12,15,8,3wun92,How do you pick your algorithm?,https://www.reddit.com/r/MachineLearning/comments/3wun92/how_do_you_pick_your_algorithm/,fitnessdl,1450134966,"I'm a noob with ML so please bare with me if this is an obvious question. My approach to analysis has always been to pick what works best with the data. I was talking to some friends and we discussed the idea of ""let the data speak for itself"". In ML, does this mean running all sorts of algorithms on the data and seeing which predicts the best? Then what is the purpose of learning the theory behind ML? Basically, when you get a new dataset and need to work on it, do you prep it and then run all kinds of algorithms instead of first studying which would work best?",1,0
360,2015-12-15,2015,12,15,8,3wupcs,Why do networks sometimes suddenly diverge?,https://www.reddit.com/r/MachineLearning/comments/3wupcs/why_do_networks_sometimes_suddenly_diverge/,londons_explorer,1450135771,"When training simple classification models (eg. Imagenet) I understand why, with the wrong choice of learning rate, a model can diverge.

But why do models sometimes suddenly diverge after hours of training?

Even when monitoring all the weights min and max values I don't see anything happening beforehand - all the weights just become NaN in the same iteration.",4,0
361,2015-12-15,2015,12,15,8,3wupq5,5 important Python Data Science advancements of 2015,https://www.reddit.com/r/MachineLearning/comments/3wupq5/5_important_python_data_science_advancements_of/,elgehelge,1450135909,,31,98
362,2015-12-15,2015,12,15,9,3wuw9w,Time series with partial future information,https://www.reddit.com/r/MachineLearning/comments/3wuw9w/time_series_with_partial_future_information/,[deleted],1450138478,[deleted],0,0
363,2015-12-15,2015,12,15,9,3wv25j,Anybody know of a free database of conversations?,https://www.reddit.com/r/MachineLearning/comments/3wv25j/anybody_know_of_a_free_database_of_conversations/,butWhoWasBee,1450140771,I would like to train a LSTM network to return responses to statements. Does anybody know of a free resource of conversations or questions and answers or something like that?,2,6
364,2015-12-15,2015,12,15,9,3wv2oh,Deploying Deep Neural Networks Efficiently,https://www.reddit.com/r/MachineLearning/comments/3wv2oh/deploying_deep_neural_networks_efficiently/,etzmor,1450141004,,0,4
365,2015-12-15,2015,12,15,10,3wv7et,"New Tool - Interactive, node-by-node debugging and visualization for TensorFlow",https://www.reddit.com/r/MachineLearning/comments/3wv7et/new_tool_interactive_nodebynode_debugging_and/,[deleted],1450143127,[deleted],0,1
366,2015-12-15,2015,12,15,10,3wv7wz,"New Tool - Interactive, node-by-node debugging and visualization for TensorFlow",https://www.reddit.com/r/MachineLearning/comments/3wv7wz/new_tool_interactive_nodebynode_debugging_and/,cryptocerous,1450143368,,0,5
367,2015-12-15,2015,12,15,11,3wvbfo,Understanding the generative nature of LDA with R,https://www.reddit.com/r/MachineLearning/comments/3wvbfo/understanding_the_generative_nature_of_lda_with_r/,spuriouscorrelations,1450144915,,0,1
368,2015-12-15,2015,12,15,11,3wvcx3,New Time Series Regression Tool for Python,https://www.reddit.com/r/MachineLearning/comments/3wvcx3/new_time_series_regression_tool_for_python/,mhamilton723,1450145640,"Hey,

I have been working on creating a time series regression class that transforms any scikit-learn regressor object into a time series / sequence to sequence regressor. I'm currently using it to predict the stock market, and forecast ahead an arbitrary number of days. Check out my repo here:

https://github.com/mhamilton723/TimeSeriesRegressor/tree/master

Feel free to leave comments and suggestions. I encourage pull requests and documentation additions!",13,19
369,2015-12-15,2015,12,15,13,3wvyxn,"Interactive, node-by-node debugging and visualization for TensorFlow",https://www.reddit.com/r/MachineLearning/comments/3wvyxn/interactive_nodebynode_debugging_and/,cherianthomas,1450153438,,0,1
370,2015-12-15,2015,12,15,15,3wwje3,Stanford Machine Learning course projects,https://www.reddit.com/r/MachineLearning/comments/3wwje3/stanford_machine_learning_course_projects/,PM_YOUR_NIPS_PAPERS,1450159235,,3,10
371,2015-12-15,2015,12,15,15,3wwl6o,Scientists teach machines to learn like humans,https://www.reddit.com/r/MachineLearning/comments/3wwl6o/scientists_teach_machines_to_learn_like_humans/,john_philip,1450159988,,0,0
372,2015-12-15,2015,12,15,16,3wwrzz,"""Scaling Machine Learning"" by Alex Smola, NIPS 2015 talk (mxnet, parameter server, recommender system etc)",https://www.reddit.com/r/MachineLearning/comments/3wwrzz/scaling_machine_learning_by_alex_smola_nips_2015/,phunter_lau,1450163184,,2,22
373,2015-12-15,2015,12,15,16,3wwvht,[1512.03965] The Power of Depth for Feedforward Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3wwvht/151203965_the_power_of_depth_for_feedforward/,iori42,1450165164,,3,11
374,2015-12-15,2015,12,15,16,3wwvqk,Downhill: Optimization Algorithms for Theano,https://www.reddit.com/r/MachineLearning/comments/3wwvqk/downhill_optimization_algorithms_for_theano/,iori42,1450165290,,7,33
375,2015-12-15,2015,12,15,16,3wwx7r,Inside Deep Dreams: How Google Made Its Computers Go Crazy,https://www.reddit.com/r/MachineLearning/comments/3wwx7r/inside_deep_dreams_how_google_made_its_computers/,iori42,1450166145,,0,4
376,2015-12-15,2015,12,15,17,3wwy2o,Seems like CuDNN v4 is out in RC,https://www.reddit.com/r/MachineLearning/comments/3wwy2o/seems_like_cudnn_v4_is_out_in_rc/,pilooch,1450166739,,8,5
377,2015-12-15,2015,12,15,18,3wx4sr,Why havent ReLUs helped vanishing gradients in RNNs the way it did in Feed Forward Deep Nets?,https://www.reddit.com/r/MachineLearning/comments/3wx4sr/why_havent_relus_helped_vanishing_gradients_in/,[deleted],1450171934,[deleted],34,13
378,2015-12-15,2015,12,15,18,3wx5m2,What is the difference between RBF neural network and sigmoid neural network?,https://www.reddit.com/r/MachineLearning/comments/3wx5m2/what_is_the_difference_between_rbf_neural_network/,[deleted],1450172538,[deleted],0,4
379,2015-12-15,2015,12,15,19,3wxakz,A New Object-Recognition Algorithm Could Change the Face of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3wxakz/a_new_objectrecognition_algorithm_could_change/,john_philip,1450176233,,1,0
380,2015-12-15,2015,12,15,22,3wxqgx,iOS devs,https://www.reddit.com/r/MachineLearning/comments/3wxqgx/ios_devs/,saethius,1450186245,"Hello. Is anyone here experience in both iOS development and machine learning?

Regards",0,0
381,2015-12-15,2015,12,15,22,3wxshg,Facebooks open-sourcing of AI hardware is the start of the deep learning revolution,https://www.reddit.com/r/MachineLearning/comments/3wxshg/facebooks_opensourcing_of_ai_hardware_is_the/,stevep2007,1450187211,,1,1
382,2015-12-15,2015,12,15,23,3wxy32,Seek advice/suggestions for applying machine-learning in my research,https://www.reddit.com/r/MachineLearning/comments/3wxy32/seek_advicesuggestions_for_applying/,saguaro648,1450189842,"I am quite new to the machine learning field. The only experience I have is the machine learning course offered by Andrew Ng on Coursera. I am interested in applying what I learned from that class to my research. 
My research deals with using a set of meteorological variables
(such as wind speed, temperature, humidity, etc) to predict the day when extreme precipitation will occur. Here is my problem set:

for example, I have daily observations of meteorological variables and corresponding extreme precipitation dates and non-extreme dates from 1979-2014. I also have modeled daily data of meteorological variables from 1979-2005. My purpose is to build a binary classification model (extreme versus non-extreme) based on observations, then apply the classification model to the climate model data. Not only I want to see if the constructed model works for the climate model data, but also I want to compare how model performs compared with the observations.

Here are my questions:

1. Suppose I have to do some-processing to the observation data, for example, normalization by subtracting mean and dividing with standard deviation, then divide the data of 1979-2014 into training data and test data.  There are two ways: 1)  Normalize the entire time series (1979-2014) first, then divide into training data (1979-2005) and test data (2006-2014). In this way, all the time series use the same mean and standard deviation to normalize; 2) Divide the the data into training data (1979-2005) and test data (2006-2014) first, then normalize each data set separately. In this way, two data sets use their own mean and standard deviation. Which is the normal way in terms of machine learning? If I use the first way to normalize the data, when I apply the constructed classification model to the model data which spans from 1979-2005, is there any inconsistency issue since I have to normalize the climate model data based on its 1979-2005 mean and standard deviation. I mean consistency mostly because I want to compare performance of applying the classification model for the climate model data with the performance of the observations (training data results).

2. Since each meteorological variable in my case is a spatial field (x-y dimension), and what I am dealing with is an unbalanced classification (few
extreme dates, but much more non-extreme dates). What I did is not to construct a linear or non-linear regression model, but to perform the Fourier transform for each day of a particular meteorological field first to reduce the data dimensionality. I extracted the phases at 20 center locations for each extreme and non-extreme date, and then build 1-D and 2-D empirical distributions for each center location using all the extreme and non-extreme dates, respectively. Next, for each day at each center location, I calculate the log-likelihood ratio of its phase belonging to extreme and non-extreme empirical distribution and sum all the ratios across the center locations. Then I construct two ratio distributions: one based on all extreme dates and the other based on all non-extreme dates. The decision threshold for extreme and non-extreme can then be decided based on the two ratio distributions. Currently I tested this method using the second way to normalize the data. But I found that the method works really well for the observational training data set (1979-2005) with very good accuracy, precision and recall, but works really poor for the observational test data (2006-2014). I assume this could be related to the overfitting problem. But I am not sure how to correct such problem with my method as I cannot simply apply regularization as normally done in linear regression.  So any
suggestion on solve the overfitting problem in my problem set would be highly appreciated.",2,0
383,2015-12-16,2015,12,16,0,3wy4ft,10 Machine Learning Algorithms Explained to an 'Army Soldier',https://www.reddit.com/r/MachineLearning/comments/3wy4ft/10_machine_learning_algorithms_explained_to_an/,john_philip,1450192589,,1,0
384,2015-12-16,2015,12,16,0,3wy8c5,Can silicon based lifeforms suffer? (serious),https://www.reddit.com/r/MachineLearning/comments/3wy8c5/can_silicon_based_lifeforms_suffer_serious/,[deleted],1450194150,[deleted],0,1
385,2015-12-16,2015,12,16,0,3wy9yt,Can AI suffer? (serious),https://www.reddit.com/r/MachineLearning/comments/3wy9yt/can_ai_suffer_serious/,Alyte,1450194781,"Read before posting: http://petrl.org/

This was posted here a while ago (by somebody else), but was dismissed as a joke. However I want to make it clear that this isn't one.
If you're immature: Instead of posting about how funny you think the possibility of suffering is, how about giving a decent counter argument?

For everybody else: what are your thoughts? What can we do about this? It may become an issue in the coming decades. Heck, it might already be happening!",23,0
386,2015-12-16,2015,12,16,1,3wyb3b,Word2vec Tutorial and an online app to play around,https://www.reddit.com/r/MachineLearning/comments/3wyb3b/word2vec_tutorial_and_an_online_app_to_play_around/,dreampulse,1450195229,,1,3
387,2015-12-16,2015,12,16,1,3wyddz,PhD Opening in the Machine Perception and Cognitive Robotics Laboratory,https://www.reddit.com/r/MachineLearning/comments/3wyddz/phd_opening_in_the_machine_perception_and/,johnson_lindenstraus,1450196123,"PhD Opening in the Machine Perception and Cognitive Robotics Laboratory (MPCR) at the Center for Complex Systems and Brain Sciences in Boca Raton, Florida. 

We are pleased to announce two new fully-funded PhD openings in our lab where we are conducting research in the emerging field of brain-inspired, machine learning techniques including deep-learning neural networks, sparse modeling and computational reinforcement learning. We are using an experimental platform consisting of wireless, neural-network controlled robotic agents, in order to test theories of perception, cognition, and functional brain architecture in an embedded context. 

Successful applicants will receive full tuition waiver and a competitive stipend for five years and will have the opportunity to earn a Ph.D.in a cutting edge, interdisciplinary research center. 

Ideal candidates will have a degree in Computer Science/Engineering, Mathematics, Data Science or a related field and be familiar with one or more of the following:

Deep Learning

Neural Networks

Machine Learning

Computational Neuroscience

Computational Psychology

Sparse Modelling

Compressed Sensing

High-Performance Computing

Robotics

Computer Vision


For more information visit www.mpcrlab.com or contact the lab director, Dr. Elan Barenholtz, Ph.D at elan.barenholtz@fau.edu

For information about the Center for Complex Systems visit:http://www.ccs.fau.edu/phd_overview.php

Application Information can be found at http://www.ccs.fau.edu/requirements.php
",10,13
388,2015-12-16,2015,12,16,1,3wyhi7,Now Available: Speaker &amp; Video APIs from Microsoft Project Oxford,https://www.reddit.com/r/MachineLearning/comments/3wyhi7/now_available_speaker_video_apis_from_microsoft/,Alkasai,1450197700,,0,0
389,2015-12-16,2015,12,16,2,3wyp6f,"Boston area interest in CS-181 Machine Learning, Harvard Extension",https://www.reddit.com/r/MachineLearning/comments/3wyp6f/boston_area_interest_in_cs181_machine_learning/,brentjoseph,1450200624,"Is anyone in the Boston area interested in taking CS-181 (Machine Learning) or Applied Math 207 (Monte Carlo Methods, Stochastic Optimization) through the Harvard extension school? 

More information can be found here: 


AM207: http://vcg.seas.harvard.edu/classes/am-207-monte-carlo-methods-stochastic-optimization

CS181: https://www.extension.harvard.edu/academics/courses/machine-learning/24603
last year's class: https://canvas.harvard.edu/courses/1862 (note different professor, so likely substantially different from the current offering). ",2,1
390,2015-12-16,2015,12,16,2,3wyt33,Scalable Machine Learning and Complex Data Analysis at Scale,https://www.reddit.com/r/MachineLearning/comments/3wyt33/scalable_machine_learning_and_complex_data/,gradientflow,1450202146,,0,1
391,2015-12-16,2015,12,16,3,3wyusq,EmptyHeaded: SIMD join-processing engine,https://www.reddit.com/r/MachineLearning/comments/3wyusq/emptyheaded_simd_joinprocessing_engine/,improbabble,1450202789,,0,1
392,2015-12-16,2015,12,16,4,3wz3et,Understanding Variational Inference,https://www.reddit.com/r/MachineLearning/comments/3wz3et/understanding_variational_inference/,kronosapiens,1450206018,,5,13
393,2015-12-16,2015,12,16,4,3wz61m,Using Circular Self-Organizing Maps to solve the Symmetric Traveling Salesman Problem,https://www.reddit.com/r/MachineLearning/comments/3wz61m/using_circular_selforganizing_maps_to_solve_the/,sachinrjoglekar,1450207028,,1,6
394,2015-12-16,2015,12,16,4,3wz6ig,Microsoft researchers advance deep learning,https://www.reddit.com/r/MachineLearning/comments/3wz6ig/microsoft_researchers_advance_deep_learning/,InaneMembrane,1450207207,,1,0
395,2015-12-16,2015,12,16,5,3wzczw,Elon Musk And Co Pledge $1 Billion To OpenAI,https://www.reddit.com/r/MachineLearning/comments/3wzczw/elon_musk_and_co_pledge_1_billion_to_openai/,[deleted],1450209642,[deleted],1,0
396,2015-12-16,2015,12,16,5,3wzjpv,10 ways you might be able to tell when an area of research is undergoing rapid expansion and society's expectations may be somewhat unrealistic [Neil Lawrence blog],https://www.reddit.com/r/MachineLearning/comments/3wzjpv/10_ways_you_might_be_able_to_tell_when_an_area_of/,urish,1450212080,,0,14
397,2015-12-16,2015,12,16,5,3wzkko,OpenAI won't benefit humanity without data-sharing,https://www.reddit.com/r/MachineLearning/comments/3wzkko/openai_wont_benefit_humanity_without_datasharing/,urish,1450212395,,6,20
398,2015-12-16,2015,12,16,5,3wzl0r,Hi I am a newbi in this reddit machine learning cite.,https://www.reddit.com/r/MachineLearning/comments/3wzl0r/hi_i_am_a_newbi_in_this_reddit_machine_learning/,machine-learning,1450212548,I am very exciting now because in my country really few people are interested in machine learning field. Finally I found the awesome guys who have the same interesting with me. ,3,0
399,2015-12-16,2015,12,16,5,3wzmkx,"Why did Google open-source their core machine learning algorithms? ""Its simple. Machine learning algorithms arent the secret sauce. The data is the secret sauce.""",https://www.reddit.com/r/MachineLearning/comments/3wzmkx/why_did_google_opensource_their_core_machine/,wkilpan,1450213123,,78,345
400,2015-12-16,2015,12,16,6,3wzq1i,NIPS 2015 review,https://www.reddit.com/r/MachineLearning/comments/3wzq1i/nips_2015_review/,cast42,1450214425,,3,12
401,2015-12-16,2015,12,16,6,3wzvbv,Deep Learning SIMPLIFIED - YouTube Series,https://www.reddit.com/r/MachineLearning/comments/3wzvbv/deep_learning_simplified_youtube_series/,jrajagopal,1450216495,"Hi everyone! I am new to this sub-reddit and wanted to introduce myself. I have been working on a YouTube series for Deep Learning that you may like. If you are new to Deep learning, or need to explain Deep Learning to a newbie, you may like this series. Content you'll typically find online on the topic is highly mathematical/technical, which is great! But if you're like me, you probably want to just understand the models and the intuition. Thats what this series is about!

Here is the link to the series intro. Please take a look and let me know what you think!
https://www.youtube.com/watch?v=b99UVkWzYTQ",10,30
402,2015-12-16,2015,12,16,7,3wzz5l,Parallel library for simulated annealing,https://www.reddit.com/r/MachineLearning/comments/3wzz5l/parallel_library_for_simulated_annealing/,hassanzadeh,1450217951,"Hi everyone,
I wonder if you know a good parallel library for simulated annealing?",0,3
403,2015-12-16,2015,12,16,9,3x0j5w,What libraries are available for Multi-Label classification using Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/3x0j5w/what_libraries_are_available_for_multilabel/,eygrr,1450226000,"Hi, I'm looking to create a network that is capable of supporting multi-label learning with an output vector that contains multiple classes. Below is an example of that.

&gt;[0 1 1 1 0 0 0 1 ... 1 1 0 0 1 1 0 0]

I have found that [scikit-learn] (http://scikit-learn.org/stable/modules/multiclass.html) can support this, but very little information from other libraries. Is this perhaps something natively available rather than explicitly defined, or is it that libraries like Keras and Lasagne don't support this kind of network? 

Appreciate any information on developing a solution that can handle this, and I am particularly interested in libraries that have built in functionality for algorithms like ML-BP [[1]] (http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkde06a.pdf)  and a cross-entropy with AdaGrad [[2]] (http://link.springer.com/chapter/10.1007%2F978-3-662-44851-9_28#page-1) type solution.

[[1]] (http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkde06a.pdf) Multi-Label Neural Networks with Applications to Functional Genomics and Text Categorization

[[2]] (http://link.springer.com/chapter/10.1007%2F978-3-662-44851-9_28#page-1) Large-Scale Multi-label Text Classification 
Revisiting Neural Networks",12,4
404,2015-12-16,2015,12,16,10,3x0o3w,[1512.04143] Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3x0o3w/151204143_insideoutside_net_detecting_objects_in/,shmel39,1450228220,,1,11
405,2015-12-16,2015,12,16,13,3x1du7,Minimum Risk Training for Neural Machine Translation,https://www.reddit.com/r/MachineLearning/comments/3x1du7/minimum_risk_training_for_neural_machine/,[deleted],1450239925,[deleted],0,1
406,2015-12-16,2015,12,16,13,3x1eli,College Student,https://www.reddit.com/r/MachineLearning/comments/3x1eli/college_student/,zechariahc,1450240323,"What science should I take in college that would benefit me most for Machine Learning(if any do), my choices are: Biology, Gen Chem, Physics. Thanks!",10,0
407,2015-12-16,2015,12,16,13,3x1gd6,7 BLEU improvement: Minimum Risk Training for Neural Machine Translation,https://www.reddit.com/r/MachineLearning/comments/3x1gd6/7_bleu_improvement_minimum_risk_training_for/,ihsgnef,1450241185,,10,17
408,2015-12-16,2015,12,16,14,3x1i2u,From One Point to A Manifold: Orbit Models for Knowledge Graph Embedding,https://www.reddit.com/r/MachineLearning/comments/3x1i2u/from_one_point_to_a_manifold_orbit_models_for/,InaneMembrane,1450242028,,0,2
409,2015-12-16,2015,12,16,16,3x1vhh,VisualGenome dataset: 100k images with more than 4 million region descriptions,https://www.reddit.com/r/MachineLearning/comments/3x1vhh/visualgenome_dataset_100k_images_with_more_than_4/,iori42,1450249466,,1,24
410,2015-12-16,2015,12,16,17,3x22n4,MY MI LI CA CD T NG,https://www.reddit.com/r/MachineLearning/comments/3x22n4/my_mi_li_ca_cd_t_ng/,hoa-posts,1450254432,,0,0
411,2015-12-16,2015,12,16,17,3x23y0,The Devnagari Character Dataset,https://www.reddit.com/r/MachineLearning/comments/3x23y0/the_devnagari_character_dataset/,ddofer,1450255458,,4,5
412,2015-12-16,2015,12,16,18,3x295e,[Question] Does anybody know about efforts in tree transduction and/or result chaining,https://www.reddit.com/r/MachineLearning/comments/3x295e/question_does_anybody_know_about_efforts_in_tree/,bluemason,1450259510,"I'm looking into the possibility of combining different prediction trees. 

That is, given a set of trees that are trained on the same data, make a prediction based on a mapping for which no single tree was expressedly trained.

A very simple example of this would be:

- Given Tree{A-&gt;B} and Tree{B-&gt;C}
- Predict C from A.

More complicated examples would involve predictor sets with overlapping trained-on attributes, multiple ""routes"" throughout the treeset, missing attributes, etc.

&amp;nbsp;

To this end, I've been looking into prior research involving tree transduction, probability trees, etc.

I haven't been able to find much related research. (Most of my progress so far involves probabilistic trees &amp; general statistics.) I expect there isn't any, but maybe I'm searching on the wrong keywords. 

&amp;nbsp;

**Q:** Does anyone else have some related knowledge (or knowledge of knowledge)?",1,4
413,2015-12-16,2015,12,16,22,3x2ueg,NIPS 2015 Overviews Collection,https://www.reddit.com/r/MachineLearning/comments/3x2ueg/nips_2015_overviews_collection/,barmaley_exe,1450274017,"Okay, NIPS 2015 is over and for those of us who couldn't make it the internet is now full of overviews / summaries / takeaways / etc. This is a list of those I personally stumbled upon:

* Dan Vanderkam: [My takeaways from NIPS 2015](http://www.danvk.org/2015/12/12/nips-2015.html)
* Brad Neuberg: [10 Deep Learning Trends at NIPS 2015](http://codinginparadise.org/ebooks/html/blog/ten_deep_learning_trends_at_nips_2015.html)
* Paul Mineiro: [NIPS 2015 Review](http://www.machinedlearnings.com/2015/12/nips-2015-review.html)
* Stephanie Hyland: [NIPS 2015 by the day](https://corcra.github.io/ml/2015/12/14/NIPS2015.html)
* Il Memming Park: [NIPS 2015 Part 1](https://memming.wordpress.com/2015/12/07/nips-2015-part-1/) + [NIPS 2015 Part 2](https://memming.wordpress.com/2015/12/09/nips-2015-part-2/) + [NIPS 2015 Part 3](https://memming.wordpress.com/2015/12/15/nips-2015-part-3/) + [NIPS 2015 workshops](https://memming.wordpress.com/2015/12/15/nips-2015-workshops/)
* Alex Williams: [Highlights of NIPS2015](http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/)
* Sbastien Bubeck: [On the spirit of NIPS 2015 and OpenAI](https://blogs.princeton.edu/imabandit/2015/12/13/on-the-spirit-of-nips-2015-and-openai/)
* Marc Deisenroth: [NIPS 2015  Deep RL Workshop](https://gridworld.wordpress.com/2015/12/13/nips-2015-deep-rl-workshop/) + [NIPS 2015  Deep Learning Tutorial](https://gridworld.wordpress.com/2015/12/14/nips-2015-deep-learning-tutorial/) + [NIPS 2015  Bayesian Optimization Workshop](https://gridworld.wordpress.com/2015/12/13/bayesian-optimization-workshop-at-nips-2015/) + [NIPS 2015 Posner Lecture  Zoubin Ghahramani: Probabilistic Machine Learning](https://gridworld.wordpress.com/2015/12/08/nips-2015-posner-lecture-zoubin-ghahramani/) + [NIPS 2015  RL Tutorial](https://gridworld.wordpress.com/2015/12/08/nips-2015-rl-tutorial/)
* John Langford: [Interesting things at NIPS 2015](http://hunch.net/?p=3001617)

**EDIT 18.12**:

* Tim Head: [A Physicist at NIPS2015](http://betatim.github.io/posts/physics-does-nips/)
* Cinjon: [NIPS 2015](http://www.cinjon.com/nips-2015/)
* Shakir Mohamed: [A Year of Approximate Inference: Review of the NIPS 2015 Workshop](http://blog.shakirm.com/2015/12/a-year-of-approximate-inference/)
* Libby Kinsey: [My NIPS 2015 highlights](https://medium.com/@libbykinsey/my-nips-2015-highlights-810e23c9da13#.yyjtevf1q)
* Jayani Withanawasam: [WiML and the NIPS conference Montreal 2015](http://www.zaizi.com/blog/wiml-and-nips-conference-montreal-2015)

**EDIT 25.12**:

* Victoria Krakovna: [Highlights and impressions from NIPS conference on machine learning](https://vkrakovna.wordpress.com/2015/12/24/highlights-and-impressions-from-nips-conference-on-machine-learning/)
* Sandhya Prabhakaran: [NIPS Montreal 2015 - notes](https://docs.google.com/document/d/1Ox4nK4GlxUo0WA-vXPmLjaIhgrggMxcGJGva_6k8-F0/edit)
* Roy Hyunjin Han: [NIPS 2015 Selected Recap](http://slides.com/invisibleroads/nips-2015-recap/fullscreen#/)
* James Atwood: [NIPS 2015 Deep Learning Tutorial Notes](http://jatwood.org/blog/nips-deep-learning-tutorial.html) + [Notes on Jon Kleinberg's NIPS Talk](http://jatwood.org/blog/kleinberg-nips-talk.html)

**EDIT 1.01**:

* Yisong Yue: [Thoughts on NIPS 2015 and OpenAI](http://yyue.blogspot.ru/2015/12/thoughts-on-nips-2015-and-openai.html)

**EDIT 10.01**:

* /u/jimfleming: [NIPS 2015 Deep Learning Symposium](https://paper.dropbox.com/doc/NIPS-2015-Deep-Learning-Symposium-I9GcRsCAU46bdzfr0mv85)
* /u/jareality : [NIPS Deep Learning Symposium](https://www.evernote.com/shard/s267/sh/64195d10-53b4-4312-8c5a-d10ab5138c36/22ce804ec6c8ab2b9ceb3096b8cd929e)

Did I miss anything? Probably so! Feel free to post relevant links in the comments.",10,63
414,2015-12-16,2015,12,16,23,3x329m,"k-Fold cross validation question... Do I evaluate each ""fold"" model, or the final model?",https://www.reddit.com/r/MachineLearning/comments/3x329m/kfold_cross_validation_question_do_i_evaluate/,holidaytie,1450277871,"Let's say I do 10-fold cross validation. I will have 10 training sets and 10 corresponding hold-out sets (all from my single overall dataset).

So, I make 10 models and evaluate them against the 10 hold-outs. Then I have have 10 performance metrics (I could average) to get a better ""sense"" of future model performance.

My question is why not use the overall model from all the data to evaluate the 10 hold-out sets and get a ""sense"" of expected model performance that way? After all, the ""overall"" model is the one going into production. Why is evaluating 10 ""similar"" models the better method?

Thanks.",15,3
415,2015-12-17,2015,12,17,0,3x35qf,Designing for AI // an educated guess,https://www.reddit.com/r/MachineLearning/comments/3x35qf/designing_for_ai_an_educated_guess/,byedit,1450279415,,0,1
416,2015-12-17,2015,12,17,1,3x3ceu,What ML algorithm(s) should I learn in order to predict based on multiple features the likelihood of a binary event occurring in the next week?,https://www.reddit.com/r/MachineLearning/comments/3x3ceu/what_ml_algorithms_should_i_learn_in_order_to/,natemi,1450282183,"Newbie question here: Can someone point me to what ML algorithms I should learn in order to predict the likelihood of an event occurring in a specified time period from the present forward based on multiple features in historical data I've collected, including past occurrences of the event I'm trying to predict? I'm trying to predict the likelihood of a binary event occurring in the next week.",9,1
417,2015-12-17,2015,12,17,1,3x3een,"Robo-Trump: Generating Donald Trump Speeches with Dictionary Embedding and LSTM, and Words Clustering",https://www.reddit.com/r/MachineLearning/comments/3x3een/robotrump_generating_donald_trump_speeches_with/,ThatWillNeverShake,1450282985,"----
*Disclaimer: This is not meant as a political analysis or opinion*

Based on Andrej Karpathy's [article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) on text generation, I built a model for text generation in Keras. However, instead of using character-level input data, I instead use words Donald Trump uses in his speeches as the input, and feed it to an Embedding layer. His speeches contain ~50k words with a vocabulary of ~3k words. I picked Trump because the transcripts for his speeches are more readily available, and more organic because they are not scripted (to the best of my knowledge). However, because his speeches are stream of consciousness, he tends to repeat phrases over and over again, which sometimes cause the model to get stuck in a loop when generating them, although I'm not sure that's 100% correct.

Nonetheless, I was able to get Donald Trump's speech transcripts since the beginning of his campaign from [what the folly](http://www.whatthefolly.com/). And I think the model is able to generate speeches that have...uh...Trumpian flavors?

For example:
With feed of ""mexico and iran are"" the model generates:
&gt; taking over the next day, he doesn't win, he doesn't win, he doesn't win, he doesn't win, he doesn't win, he doesn't win, he doesn't win, he didn't know. i said, what's wrong? the art of the deal. they did not read that i spend less. and i have to tell you. i have a friend, bill zanker, who's a very smart guy. he said, if you don't have great people. we have a lot of big news conference. and let's talk about it. and you know, i was the king. i was the ultimate. i think the deal is done, a lot of the pundits on television, as usual, saying that should you know, i was a little weak on, people were interviewed. believe me, i was the blonde. i say, that's a hundred percent. but you know, they say, although,, you have to go out, we have to go to japan. did you hear this? iran. they let her out. he gave them $150 billion. it's called amateur night, we had a situation in california very recently where somebody said, ""you shouldn't probably against as the one. she was a horrible negotiator. these people are babies. they're babies. do you remember when they said they want to impeach, do you remember, no, they're wrong. very stupid people. and every country in the world, i mean, i think. but i listened to them today. some politicians, but you know, i was in manassas, virginia, wherever i mean, look at this. somebody said, do you have thin skin when i'm right. we can make this country great again. the ladies and gentlemen. i am officially going to happen. that i can tell you that. i'll bring back our jobs from china, from mexico, from japan, from so they standpoint of spending more money on from the country. i think he'd be to be at $20 trillion. according to the economists, because i'm totally against the trade bill doral, we had a situation in california very recently where somebody said, ""you shouldn't you're not protect. what are we doing? and you know she's been going on. corporate million. it's a great assets. and they said i was the only one that's self funding. i don't want it. so we take it down. so they are going to take it down. so they are going to take it down. they couldn't believe it. i believe it. i believe in 

and with feed of ""I love America because"", the model generate:
&gt; we have a $2 trillion. we have nothing. and every time we have to help those people, we have to straighten out, we have to be strong, we have to go to japan. did you hear this? iran. they want to work. they can't. they don't have to develop, i can tell you. i will immediately terminate president are leaving our country. and then we have to get the greatest people that are sitting back like in the can give him millions. he has $2 million sitting in a super pac. i want to see christmas. no, i want to see christmas. no, i want to see him by himself. he's a bad negotiator. he's actually in a race. it wasn't just casual, it could be, but i doubt it. he knew about it. they didn't mention at $250 million you right? so we decapitate one. them trump is calling for a total and complete victory. gen. douglas dollars. i've turned down so much money. i have a friend, bill zanker, who's a very smart guy. he said, if you don't have great people. we have a lot of foolish people. we have great people. we have to be strong. we have to get strong, and we have to do something to clean up this country. so i ordered 4000. that's the difference between china's leaders and hell what they're doing. we have a lot of foolish people. we have a 24 day period, not a bad story. i won't say that. so, if you have some dumb office on television, as usual, saying that who is getting the oil, most of the oil? the stuff. it's natural. it's a great poll. it's true. it's true. it's true. they'll pay me anything. they have nothing. don't worry about it. but obama is talking about the ukraine. i think that's terrible. does anyone know this is a great audience. but i have more, i won't use the candidates' name because i don't want to run. they never thought of him as this as the people that are running that super pac there, they'd go to the chinese, a lot of the pundits on television, as usual, saying that who should get the oil, and what they don't do that. so, if you have some dumb office. i'm building as you know, afghanistan for $43 million, can you imagine this is a great poll. it's true. it's true. it's true. it's true. it's true.

But more interestingly, because I use embedding layer, each word is represented as a vector. So if I feed it to a clustering algorithm (I used KNN), [this](http://pastebin.com/k16wqVuF) is what came out. Although most of the clusters are useless, there are clusters with clear subject (e.g. the words ""is"", ""that's"", ""it's"", ""be"", ""was"", ""are"", ""by"", ""they're"", ""we're"", ""only"", ""what's"", ""i'm"", ""you're"", ""wasn't"", ""family"", ""negotiate"", ""nobody's"", ""everyone's"", ""she's"", ""funding's"", ""neighbor"", ""something's"", ""damn"", ""understandhe's"", ""everything's"", and ""i'm"" all belong to the same cluster). 

*edit:
After some fine tuning I was able to make the speech a little bit more coherent.
With the seed ""make the country"", it outputs
&gt;rich. it was terrible. but saudi arabia, they make a billion dollars a day. i was the king. i was the king. i was the smartest person in yemen, we have to get to business. i have to say, but he was an early starter. and we have to get to business. i have to say, donald, i can't believe it. it's so important. but this is what they're saying, dad, you're going to be really pro, growth, blah, blah. it's disgusting what's disgusting., and it was a 19 set washer and to go to japan. did you hear that character. we are going to have to think about it. but you know, i've been nice to me. something happens in washington. the beauty of washington. not a good deal. we have to get to business. but i've always said qatar so what i do well. i think we're going to get it back. no, it's true. you know, it's about time that on iraq. he couldn't answer the question, is iraq. think of it. trillion. trillion dollars to the horrible thing he just said, ""oh, give you an example. and they say, ""we beat china""? you know, people say that's such a hard time even though you know i'm going to do it. remember the poll numbers are horrible. i mean, i can tell you is going to be really pro, growth, seven, 000 people. it was terrible. but saudi arabia, they make a billion dollars a day. i was the smartest person in yemen, we have to get to business. but i've always said qatar so what i do is i build a great wall. and it's got to stop and it's got to stop and it's got to stop and it's got to stop and it's got to stop and it's got to stop and it's got to stop and it's got to stop and it's got to stop and it's got to stop it. and we have to get through these 15 people to another country. and believe me, to the women, it's all about jobs. we have to get to business. i have to tell you, i love people. but i know how to go about making the deal. all of my life. like this. and it's very unfair. i feel it's very sad. now we owe $19 trillion. the budget is so bad. the imbalance is what he should. and you know, we had a really good.

Also, keep in mind that Trump's speech can be pretty incoherent at times because he would jump from one topic to another abruptly.",10,15
418,2015-12-17,2015,12,17,1,3x3g75,Anyone competing in the Data Science Bowl?,https://www.reddit.com/r/MachineLearning/comments/3x3g75/anyone_competing_in_the_data_science_bowl/,datawhat,1450283698,"https://www.kaggle.com/c/second-annual-data-science-bowl

Seems like a competition where deep learning should thrive!  Also, your solution actually could make a difference to lots of people.

Personally I know nothing about the field, but I'll be investing quite a bit of time doing some research so that I can compete in this one!",3,16
419,2015-12-17,2015,12,17,2,3x3rvc,Add and Subtract Words Like Vectors with Word2Vec,https://www.reddit.com/r/MachineLearning/comments/3x3rvc/add_and_subtract_words_like_vectors_with_word2vec/,dnabeyta,1450288428,,0,0
420,2015-12-17,2015,12,17,3,3x3urc,"Tips on publishing in NIPS, ICML or any top tier conferences for ML",https://www.reddit.com/r/MachineLearning/comments/3x3urc/tips_on_publishing_in_nips_icml_or_any_top_tier/,mr_robot_elliot,1450289509,"As you must have already known, this year NIPS has seen an exponential growth in registrations. In the face of overwhelming competition, i think it would be great to log the tips for publishing in a top tier conferences. It would be great if someone can comment on the special care for the papers inorder to get published. This is a very subjective question in nature, so any comments are welcome.",24,13
421,2015-12-17,2015,12,17,3,3x3wia,How can I use ML to classify decisions after the fact,https://www.reddit.com/r/MachineLearning/comments/3x3wia/how_can_i_use_ml_to_classify_decisions_after_the/,oldmacjunk,1450290215,"An analyst conducts an interview and during the interview they collect notes. At the end of the interview they make a binary decision. I want to be able to group analyst decisions based on how similar they are to one another. All notes are natural langyage and may contain typos and other errors. Dataset &lt; 10,000 notes. How should I attempt to solve this?",5,2
422,2015-12-17,2015,12,17,4,3x470a,why are bayesian methods (considered) more elegant?,https://www.reddit.com/r/MachineLearning/comments/3x470a/why_are_bayesian_methods_considered_more_elegant/,r-sync,1450294302,"I was chatting with a few folks at NIPS, and one common theme was that their papers on bayesian methods were more elegant, but got less attention. 

As a bayesian n00b, dont most bayesian methods approximate the partition function anyways? Doesn't all the elegance go away when one does that?

Anyone who can give a bit more perspective from the bayesian side.

p.s.: I ride the energy based learning bandwagon.
",44,58
423,2015-12-17,2015,12,17,4,3x47l4,How to use Gaussian Mixture Model (GMM) in motion detection?,https://www.reddit.com/r/MachineLearning/comments/3x47l4/how_to_use_gaussian_mixture_model_gmm_in_motion/,dinoucs,1450294538,"Hi,
I'm working on a graduate project about stationary objects with GMM.
Any information would be so helpful.
Thanks",2,0
424,2015-12-17,2015,12,17,6,3x4ml0,"With the focus on deep learning, what problems still motivate graphical models?",https://www.reddit.com/r/MachineLearning/comments/3x4ml0/with_the_focus_on_deep_learning_what_problems/,hongloumeng,1450300482,"What real world problems still motivate the use of graphical models, such as Bayesian networks, Markov networks, HMMs, etc?  My experience with these models is both learning network structure, and doing inference (generating probabilities about different combinations of states of variables in the network) don't scale well at all.  How are they faring in a landscape now dominated by neural net based approached?",6,9
425,2015-12-17,2015,12,17,6,3x4t7u,How difficult is it to train a self-driving car?,https://www.reddit.com/r/MachineLearning/comments/3x4t7u/how_difficult_is_it_to_train_a_selfdriving_car/,solarpoweredbiscuit,1450303073,,34,33
426,2015-12-17,2015,12,17,8,3x562c,Choosing points for a gaussian process subset of regressors?,https://www.reddit.com/r/MachineLearning/comments/3x562c/choosing_points_for_a_gaussian_process_subset_of/,FTL_Diesel,1450308347,"I have a large dataset that I'm trying to treat via a gaussian process model, and it is large enough that I need to use approximations methods to have the calculations complete in a reasonable time, and to prevent the computer from barfing on the matrix inversions.

Anyways, I'm reading up on using the Subset of Regressors technique (so called by Rasmussen and Williams), and I'm not quite clear on how one goes about selecting the subset I of data points that you work with. Do they all have to be sequential? My impression is yes, so that you can partition the covariance matrix into discrete sections - but then it seems hard for me to believe that this method gives you much information about what is happening in other parts of the data set. Perhaps this skepticism is unwarranted, however.",4,4
427,2015-12-17,2015,12,17,9,3x5hal,Good intro on computational graphs?,https://www.reddit.com/r/MachineLearning/comments/3x5hal/good_intro_on_computational_graphs/,letoseldon,1450313335,"I'm trying to understand TensorFlow architecture, but the idea of a computational graph is opaque to me (this was always a black box to me in Theano since I relied on tools like Keras).",4,7
428,2015-12-17,2015,12,17,12,3x62kb,Deep Learning in a Nutshell part 2: History and Training,https://www.reddit.com/r/MachineLearning/comments/3x62kb/deep_learning_in_a_nutshell_part_2_history_and/,harrism,1450323063,,0,70
429,2015-12-17,2015,12,17,12,3x63hc,Machine learning is driving the next self-driving car,https://www.reddit.com/r/MachineLearning/comments/3x63hc/machine_learning_is_driving_the_next_selfdriving/,reallifeted,1450323519,,0,2
430,2015-12-17,2015,12,17,13,3x6cnn,Intuition for RNN learning rate?,https://www.reddit.com/r/MachineLearning/comments/3x6cnn/intuition_for_rnn_learning_rate/,jstaker7,1450328239,"In papers I've been reading about RNN/LSTMs, I often see learning rates between 0.5-0.7, but in my experience with deep image recognition this is absurdly high. Learning rates for deep CNNs are often between 0.00001-0.001. Is there a reason for this? I'm having a hard time wrapping my head around processing 12M sentences with such a fast learning rate. ",14,1
431,2015-12-17,2015,12,17,14,3x6inb,[Question/Discussion] Cloockwork RNNs: Are they better than LSTMs as described? Ay one has applied it to practical problems?,https://www.reddit.com/r/MachineLearning/comments/3x6inb/questiondiscussion_cloockwork_rnns_are_they/,[deleted],1450331524,[deleted],19,9
432,2015-12-17,2015,12,17,15,3x6jrh,Nice animation of how quickly RNNs forget compared to LSTMs,https://www.reddit.com/r/MachineLearning/comments/3x6jrh/nice_animation_of_how_quickly_rnns_forget/,[deleted],1450332166,[deleted],4,1
433,2015-12-17,2015,12,17,17,3x6ypq,Symphony from Synapses: Neocortex as a Universal Dynamical Systems Modeller using Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/3x6ypq/symphony_from_synapses_neocortex_as_a_universal/,fergbyrne,1450342197,,5,0
434,2015-12-17,2015,12,17,18,3x6zxt,BrainChip Provides Details of Neural Network Architecture,https://www.reddit.com/r/MachineLearning/comments/3x6zxt/brainchip_provides_details_of_neural_network/,Yax42,1450343034,,4,1
435,2015-12-17,2015,12,17,19,3x75w5,Google Patent Application: System And Method For Parallelizing Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3x75w5/google_patent_application_system_and_method_for/,pogopuschel_,1450347462,,5,15
436,2015-12-17,2015,12,17,22,3x7poc,Generating sound with recurrent neural nets,https://www.reddit.com/r/MachineLearning/comments/3x7poc/generating_sound_with_recurrent_neural_nets/,anotherjohng,1450360290,,25,50
437,2015-12-17,2015,12,17,23,3x7uzy,Advice for designing neural network,https://www.reddit.com/r/MachineLearning/comments/3x7uzy/advice_for_designing_neural_network/,michal_sustr,1450362938,"I just got into deep learning and read some papers about the math behind it. I would like to try to use it on a task that I've previously solved using Hidden Markov Models, to try to compare if a NN is able to do it better :)

As input I have time series in multiple axis (12 different time series) in which I'm trying to find specific subsequences for which I have imprecise segmentation marks that denote the start and end of the subsequence, see http://s30.postimg.org/rsu8bncb5/Selection_196.png

I would like to train the NN to find these segmentation marks by itself. The marks are not necessarily placed in a precise positions. What would you suggest to do in this case? 

I thought about creating an LSTM network with couple of dense layers after it, and make a small window around the segmentation marks instead of one point which designates the separation. Train the network using binary crossentropy loss function. To augment the available data set I can introduce some small gaussian noise.

Any further suggestions?
",0,0
438,2015-12-17,2015,12,17,23,3x7wyh,"what are the different classes of classifier models? Mane if maxent/exponential family model is one of the types, what are the others?",https://www.reddit.com/r/MachineLearning/comments/3x7wyh/what_are_the_different_classes_of_classifier/,anonCuriousReditter,1450363854,[removed],0,1
439,2015-12-18,2015,12,18,0,3x7ynp,Rise of Fully Automated ML - Code and Benchmarks,https://www.reddit.com/r/MachineLearning/comments/3x7ynp/rise_of_fully_automated_ml_code_and_benchmarks/,stevea,1450364594,,9,7
440,2015-12-18,2015,12,18,0,3x84mf,"Gary Marcus, A Deep Learning Dissenter, Thinks He Has a More Powerful AI Approach | MIT Technology Review",https://www.reddit.com/r/MachineLearning/comments/3x84mf/gary_marcus_a_deep_learning_dissenter_thinks_he/,555x,1450367239,,14,6
441,2015-12-18,2015,12,18,1,3x8cn2,6 GPU PC Build -- Is SLI Necessary?,https://www.reddit.com/r/MachineLearning/comments/3x8cn2/6_gpu_pc_build_is_sli_necessary/,LeavesBreathe,1450370478,"Hey Guys, Looking to build a pc with 6 Titan X's. Few questions:

1. Do you need to have SLI compatibility? Since we don't link our GPUs with SLI, then do we really need to have SLI compatibility in the motherboard?

2. You will only notice 4 Titan X's...but the motherboard has six 16 PCIe 3.0 slots. This is really what we need right? 

3. Any recommendations about setups is very appreciated. I currently have a two 980Ti setup with a i7-920. 

Thanks!

[PCPartPicker part list](http://pcpartpicker.com/p/Z6Rx8d) / [Price breakdown by merchant](http://pcpartpicker.com/p/Z6Rx8d/by_merchant/)

Type|Item|Price
:----|:----|:----
**CPU** | [Intel Core i7-2600 3.4GHz Quad-Core Processor](http://pcpartpicker.com/part/intel-cpu-bx80623i72600) |-
**Motherboard** | [ASRock Z77 WS ATX LGA1155 Motherboard](http://pcpartpicker.com/part/asrock-motherboard-z77ws) |-
**Memory** | [G.Skill Ripjaws X Series 16GB (2 x 8GB) DDR3-1600 Memory](http://pcpartpicker.com/part/gskill-memory-f312800cl10d16gbxl) | $59.99 @ Newegg 
**Memory** | [G.Skill Ripjaws X Series 16GB (2 x 8GB) DDR3-1600 Memory](http://pcpartpicker.com/part/gskill-memory-f312800cl10d16gbxl) | $59.99 @ Newegg 
**Video Card** | [Asus GeForce GTX Titan X 12GB Video Card](http://pcpartpicker.com/part/asus-video-card-gtxtitanx12gd5) (4-Way SLI) | $1014.98 @ Newegg 
**Video Card** | [Asus GeForce GTX Titan X 12GB Video Card](http://pcpartpicker.com/part/asus-video-card-gtxtitanx12gd5) (4-Way SLI) | $1014.98 @ Newegg 
**Video Card** | [Asus GeForce GTX Titan X 12GB Video Card](http://pcpartpicker.com/part/asus-video-card-gtxtitanx12gd5) (4-Way SLI) | $1014.98 @ Newegg 
**Video Card** | [Asus GeForce GTX Titan X 12GB Video Card](http://pcpartpicker.com/part/asus-video-card-gtxtitanx12gd5) (4-Way SLI) | $1014.98 @ Newegg 
**Power Supply** | [Thermaltake Toughpower Grand 1200W 80+ Gold Certified ATX Power Supply](http://pcpartpicker.com/part/thermaltake-power-supply-tpg1200m) | $217.98 @ Newegg 
 | *Prices include shipping, taxes, rebates, and discounts* |
 | **Total** | **$4397.88**
 | Generated by [PCPartPicker](http://pcpartpicker.com) 2015-12-17 11:34 EST-0500 |",41,7
442,2015-12-18,2015,12,18,2,3x8jj3,Survey of Modern Questions and Challenges in Feature Extraction,https://www.reddit.com/r/MachineLearning/comments/3x8jj3/survey_of_modern_questions_and_challenges_in/,iidealized,1450373226,,0,7
443,2015-12-18,2015,12,18,2,3x8m73,Recommendations for reporting top features,https://www.reddit.com/r/MachineLearning/comments/3x8m73/recommendations_for_reporting_top_features/,[deleted],1450374225,"I have a wide data set, which has something like 3x as many features as data points, and I am tasked with writing a report describing which features are most predictive for binary classification. I'm wondering what would be the most methodologically sound strategy for doing so.

Including every single feature in a random forest with leave-1-out cross validation gives decent accuracy, precision, and recall; so I proposed reporting those metrics as well as the top-k features as determined by random forest feature importance over the entire data set, but my colleague said that approach rubs him the wrong way, and he advised trying a nested cross-validation approach. The problem is that, if I were to do so, I would have multiple lists of feature rankings and then the issue would be combining those sets into a single list of important features. Can anybody offer some advice?",5,5
444,2015-12-18,2015,12,18,3,3x8r8s,"Deep Learning with Ilya Sutskever, Research Director at OpenAI",https://www.reddit.com/r/MachineLearning/comments/3x8r8s/deep_learning_with_ilya_sutskever_research/,reworksophie,1450376192,,0,1
445,2015-12-18,2015,12,18,3,3x8te8,I just achieved a factor of 8 to 10 improvement. Should I try a top-tier journal?,https://www.reddit.com/r/MachineLearning/comments/3x8te8/i_just_achieved_a_factor_of_8_to_10_improvement/,[deleted],1450377083,[deleted],3,4
446,2015-12-18,2015,12,18,4,3x92e1,Machine learning in algorithmic music composition - demo video of a prototype breakbeat generator I made based on Numenta's Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/3x92e1/machine_learning_in_algorithmic_music_composition/,HybridSystem,1450380632,,2,2
447,2015-12-18,2015,12,18,5,3x97ew,Deep Learning SIMPLIFIED: Episode 2 - What is a neural network?,https://www.reddit.com/r/MachineLearning/comments/3x97ew/deep_learning_simplified_episode_2_what_is_a/,jrajagopal,1450382684,"Ladies and gentlemen, its time for Episode 2. If you are the sort of beginner that I was back when I was new to this topic, neural nets may seem intimidating - they are complex methods that are difficult to train. However, it soon becomes apparent what a neural net is and how it works, we start to what they are really about. Here is a video that really simplifies neural nets and explains the intuition behind this method.
https://www.youtube.com/watch?v=P2HPcj8lRJE",2,10
448,2015-12-18,2015,12,18,5,3x9a8n,Voice controlled home automation,https://www.reddit.com/r/MachineLearning/comments/3x9a8n/voice_controlled_home_automation/,alexcaps,1450383845,,1,4
449,2015-12-18,2015,12,18,5,3x9cng,"Exploring 250,000+ Movies with Association Discovery",https://www.reddit.com/r/MachineLearning/comments/3x9cng/exploring_250000_movies_with_association_discovery/,czuriaga,1450384867,,0,1
450,2015-12-18,2015,12,18,6,3x9gom,"30,000 Ways To Say I Love You",https://www.reddit.com/r/MachineLearning/comments/3x9gom/30000_ways_to_say_i_love_you/,tvrevdotcom,1450386540,,0,0
451,2015-12-18,2015,12,18,6,3x9jqr,[Question] - Calculating the recall and precision values of multilayer neural network with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3x9jqr/question_calculating_the_recall_and_precision/,anilguzelankara,1450387833,"Hello everyone! How can i calculate the recall and precision values of this example?

Actually in the code, there are naturally y and pred tensors.

I can't get the values of y and pred as a numpy array. The real problem is that actually.

https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/multilayer_perceptron.py",2,1
452,2015-12-18,2015,12,18,6,3x9kld,tanh or sigmoid for simple NN in MNIST,https://www.reddit.com/r/MachineLearning/comments/3x9kld/tanh_or_sigmoid_for_simple_nn_in_mnist/,yhg0112,1450388141,"hello, 

last day i've implemented simple neural network with Theano for MNIST dataset at my first time (on course homework).


Because i know that even very simple(1-layer) NN with only dozens of neurons performs very good on MNIST set, i've configured my networks as

  * activate function : sigmoid,

  * 28*28, [500,50], 10 2-layer hidden nodes network,

  * output function : softmax,

  * with L2 weight decay regularization.


However at the first time, my network converged with accuracy about 30%, which i cannot endure. 


i thought that my hyperparameter - (learning rate &amp; etc.) was seriously wrong; i'd used simple SGD with initial learning rate 0.01, and i'd halfed the learning rate whenever valid_error goes up.


i'd searched my parameter about 8 hours straight, till i'm exhausted( checking if there is any mistake in my code ), and i'd gave up and submitted my poor working NN.


Just after submitting, i've changed my activation function to tanh from sigmoid.


and it works as great as expected. The accuracy on testset gets vertically up to about 90% in few starting epochs, and it seems to converge as good. 


is there anyone have any idea about my result? i've always thought that sigmoid and tanh acts very very similar way and i'd just chosen the sigmoid because i prefer exponential function to log function..i have no idea why sigmoid worked poor on my result.

 ",4,0
453,2015-12-18,2015,12,18,7,3x9v48,Machine Learning  ExplainedSimply,https://www.reddit.com/r/MachineLearning/comments/3x9v48/machine_learning_explainedsimply/,Friars1993,1450392618,,0,1
454,2015-12-18,2015,12,18,7,3x9vuv,And yet another graph autodiff. But why did I do this.,https://www.reddit.com/r/MachineLearning/comments/3x9vuv/and_yet_another_graph_autodiff_but_why_did_i_do/,bbsome,1450392940,"Hi,
So I started writing yet another tool for autodiff, which is based on symbolic backward autodifferentiation. Currently it lacks any documentations at all. However it pretty much resembles almost identically Theano. Why did I do this:

1. I wanted automatic shape inference at compile time. This is achieved by using the fact that we know that all shapes of the graph are a polynomial over the integers of the inputs. Requires a bit more verbosity on specifying ""dynamic"" shapes if they are the same. However, this means that any shape mistake will be inferred before the graph is compiled. As a consequence of this Broadcasting is an explicit operation (similar to CGT). However, there are flags on the graph which for some operations you can let it do it automatically without complaining (implicit broadcasting)

2. For Recurrent models if you do not have a scan(which I currently don't and don't know if I will, but probably should) graphs can get very large and Theano compilation time is slow. That's why I think the optimization procedures should be done more efficient and would be the main effort on my side of tool. 

3. Most important for me! The fact that the graph manipulation and source code generation should be 2 separate things. I wanted to make something that other people can reuse, so my main goal is to make it fully just a graph autodiff. Soon I will add something like ""backends"" which would generate source code based on the graph. Potentially, I will have and something like ""execution graph"" which to sort of assign variables to each node (e.g. for memory to be able to reuse memory) and inlining of different operations. My first look is to do it with Arrayfire (https://github.com/arrayfire/arrayfire) as it does all of the lowlevel stuff with very little programming effort and works on both CPU and GPU. 

4. For later I think maybe people who can optimize more can do their own backends of how to generate the source code, while the graph part to remain the same so it is easily reusable. Also in much further future maybe doing distributed stuff would be helpfull as well or multi devices, however that is further down the path.

The repo is here:https://github.com/Botev/metadiff.
Any criticism and comments are most welcome. I just wanted to share it here if someone have more thoughts about it or wanted to try something like this.",1,2
455,2015-12-18,2015,12,18,8,3x9zm1,what's the state of the art in classifying text into questions vs non questions?,https://www.reddit.com/r/MachineLearning/comments/3x9zm1/whats_the_state_of_the_art_in_classifying_text/,hlpmewmyrelationship,1450394595,I will post links and possibly discussion here as I gather information,5,1
456,2015-12-18,2015,12,18,8,3xa0qb,Handling a set of varying resolution time series,https://www.reddit.com/r/MachineLearning/comments/3xa0qb/handling_a_set_of_varying_resolution_time_series/,trevinstein,1450395089,"You have a high dimensional time series in which not all the dimensions match up in timing, that is, some remain temporarily constant as others change.


Suppose the resolution varies from second-by-second to day-to-day, what is the best way to make them line up? do you just leave the temporary constants? Do you use some sort of smoothing of values based on the nature of the variable? (ex. if you only have daily temperature readings you assume that temperature fluctuates during the day in some fashion and insert the fake in between values)



Also another problem, how do you compress a time series while trying to retain as much information about the overall time series? Suppose you want to transform second-by-second to hourly, how do you do it such that you don't destroy interesting events that may happen in between? Do you add to the  dimensionality with feature engineering?",3,1
457,2015-12-18,2015,12,18,10,3xaf8p,L vi sng MWS22EGL,https://www.reddit.com/r/MachineLearning/comments/3xaf8p/l_vi_sng_mws22egl/,lamnq0514,1450402019,,0,1
458,2015-12-18,2015,12,18,11,3xal3c,Tensorflow notebooks for Udacity course,https://www.reddit.com/r/MachineLearning/comments/3xal3c/tensorflow_notebooks_for_udacity_course/,cesarsalgado,1450405001,,4,1
459,2015-12-18,2015,12,18,11,3xalry,Tensorflow now accepts pull requests on Github,https://www.reddit.com/r/MachineLearning/comments/3xalry/tensorflow_now_accepts_pull_requests_on_github/,cesarsalgado,1450405366,,16,122
460,2015-12-18,2015,12,18,16,3xbkb9,Just thinking out loud: NN based planetary surface/heightmap generation...,https://www.reddit.com/r/MachineLearning/comments/3xbkb9/just_thinking_out_loud_nn_based_planetary/,SeveQStorm,1450425480,"I've just watched the trailer of a space sim game (Star Citizen) in which they show procedurally generated planetary surfaces. 

Now, I'm wondering if it's possible to get a NN (guess it's best to use a conv NN in this case) to learn how planetary surfaces look like by using actual heightmaps of for example our own space rock called Earth and sample, random and naturally looking heightmaps... maybe even including oceans and greens and mountains, and the corresponding textures...

Any comments on this thought?",18,3
461,2015-12-18,2015,12,18,18,3xbqel,Dry Powder Filling Machine - Anchormark,https://www.reddit.com/r/MachineLearning/comments/3xbqel/dry_powder_filling_machine_anchormark/,Anchormark1,1450430002,,0,1
462,2015-12-18,2015,12,18,19,3xbun5,The Science of Deep Learning | Yarin Gal - Blog,https://www.reddit.com/r/MachineLearning/comments/3xbun5/the_science_of_deep_learning_yarin_gal_blog/,petrux,1450433273,,1,17
463,2015-12-18,2015,12,18,20,3xbyva,I'm new to ML and this seemed like a nice intro - Architecting Predictive Algorithms for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3xbyva/im_new_to_ml_and_this_seemed_like_a_nice_intro/,boner_freelove,1450436428,,0,0
464,2015-12-18,2015,12,18,20,3xc3an,Confused on torch code,https://www.reddit.com/r/MachineLearning/comments/3xc3an/confused_on_torch_code/,[deleted],1450439538,[deleted],1,1
465,2015-12-18,2015,12,18,21,3xc7lq,Tensor Methods in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3xc7lq/tensor_methods_in_machine_learning/,cast42,1450442400,,0,48
466,2015-12-18,2015,12,18,23,3xcgjb,New to Machine Learning (but highly interested). Did some experiments on MLlib algorithms but got poor results. Need advice.,https://www.reddit.com/r/MachineLearning/comments/3xcgjb/new_to_machine_learning_but_highly_interested_did/,Ande_Ka_Funda,1450447630,"Hello

I used this dataset and split the data 70/30 to train models.
http://mlr.cs.umass.edu/ml/machine-learning-databases/wine/

I used the experiments on 5 algorithms. The error rate was around 30-40%. You can see the complete results and projects here:
http://mlr.cs.umass.edu/ml/machine-learning-databases/wine/

Or in this [image](http://imgur.com/hj4tvqC)

(As a validation I did do a 100/30 split and the error rates were around 0-2 %).

So now my question is : 

-&gt;Is the result expected?

-&gt;How can I improve my models?

-&gt; What other cool things can I do with this or any other data.

",0,0
467,2015-12-18,2015,12,18,23,3xclt9,A Python libraries for building Recommender systems,https://www.reddit.com/r/MachineLearning/comments/3xclt9/a_python_libraries_for_building_recommender/,[deleted],1450450306,[deleted],0,0
468,2015-12-19,2015,12,19,0,3xcqiy,Georgia Tech Researchers Demonstrate How the Brain Can Handle So Much Data,https://www.reddit.com/r/MachineLearning/comments/3xcqiy/georgia_tech_researchers_demonstrate_how_the/,Articulated-rage,1450452471,,4,16
469,2015-12-19,2015,12,19,2,3xd6rc,"Inside Google's Quantum Computing Lab - ""In 10 years theres nothing but quantum machine learning"" (x-post from /r/futurology)",https://www.reddit.com/r/MachineLearning/comments/3xd6rc/inside_googles_quantum_computing_lab_in_10_years/,racoonear,1450459294,http://www.technologyreview.com/news/544421/googles-quantum-dream-machine/,5,0
470,2015-12-19,2015,12,19,3,3xdilh,Deep Learning Robot,https://www.reddit.com/r/MachineLearning/comments/3xdilh/deep_learning_robot/,pilooch,1450464213,,19,15
471,2015-12-19,2015,12,19,3,3xdkl8,First use of Reinforcement Learning to solve Tower of Hanoi puzzle,https://www.reddit.com/r/MachineLearning/comments/3xdkl8/first_use_of_reinforcement_learning_to_solve/,[deleted],1450465020,[deleted],1,0
472,2015-12-19,2015,12,19,4,3xdles,RNN's Power Abstract Thought In the Brain?,https://www.reddit.com/r/MachineLearning/comments/3xdles/rnns_power_abstract_thought_in_the_brain/,LeavesBreathe,1450465358,"http://www.umass.edu/newsoffice/article/how-brain-architecture-leads-abstract

This is obviously not proof, but nonetheless, I thought it was interesting. I feel like we could use the paper to form inspiration for RNN designs.

I think this addresses the question of: Should we use stacked layers of RNNs? Clearly it looks like a solid yes to me, given that we have skip connections between all layers. ",18,2
473,2015-12-19,2015,12,19,4,3xdmwx,Machine learning,https://www.reddit.com/r/MachineLearning/comments/3xdmwx/machine_learning/,demon_dev007,1450465975,[removed],2,0
474,2015-12-19,2015,12,19,4,3xdrdy,Python libraries for building recommender systems,https://www.reddit.com/r/MachineLearning/comments/3xdrdy/python_libraries_for_building_recommender_systems/,minimonger,1450467877,,0,0
475,2015-12-19,2015,12,19,5,3xduwi,Story of a decent amen break generated by LSTM! [own blog post],https://www.reddit.com/r/MachineLearning/comments/3xduwi/story_of_a_decent_amen_break_generated_by_lstm/,fiala__,1450469426,,7,1
476,2015-12-19,2015,12,19,5,3xdv26,Help debugging tensorflow program,https://www.reddit.com/r/MachineLearning/comments/3xdv26/help_debugging_tensorflow_program/,[deleted],1450469483,[deleted],0,1
477,2015-12-19,2015,12,19,5,3xdxg2,Strategies for Training Large Vocabulary Neural Language Models: Kneser-Ney n-gram model still beat neural network based LMs at modeling large vocabularies.,https://www.reddit.com/r/MachineLearning/comments/3xdxg2/strategies_for_training_large_vocabulary_neural/,onewugtwowugs,1450470518,,5,7
478,2015-12-19,2015,12,19,5,3xdzui,Deep Learning SIMPLIFIED: Episode 3 - Why Deep?,https://www.reddit.com/r/MachineLearning/comments/3xdzui/deep_learning_simplified_episode_3_why_deep/,jrajagopal,1450471575,"What would be the reason to use neural networks, while there are so many other machine learning methods available? When it comes to working with data that has complex patterns, neural nets do a better job at deciphering them than any other method. Further, high performance GPUs can make a huge difference in training them. In this video, the motivation behind using deep neural networks is explained in detail.
https://www.youtube.com/watch?v=CEv_0r5huTY",8,16
479,2015-12-19,2015,12,19,5,3xe1t4,Prototype multi-GPU training in Theano - LSTM EASGD example,https://www.reddit.com/r/MachineLearning/comments/3xe1t4/prototype_multigpu_training_in_theano_lstm_easgd/,kkastner,1450472389,,2,33
480,2015-12-19,2015,12,19,6,3xe3q9,Question - Recognizing document structure.,https://www.reddit.com/r/MachineLearning/comments/3xe3q9/question_recognizing_document_structure/,zazaalaza,1450473253,"I found [this tool](https://www.youtube.com/watch?v=dqaKn-B8mtA), Demo starts at 1:40, and was wondering that besides the OCR is there any machine learning involved?

I'm guessing there is some machine learning library for recognizing future incoices, and if so, what?",0,0
481,2015-12-19,2015,12,19,6,3xe9ge,Increasing Diversity in Seq2Seq Outputs,https://www.reddit.com/r/MachineLearning/comments/3xe9ge/increasing_diversity_in_seq2seq_outputs/,nick_courier,1450475705,"Hello everyone. I've been using seq2seq to research conversation systems for psychology. I have a model with similar parameters to this paper: http://arxiv.org/abs/1510.03055

I'm somewhat confused as to how changing the objective function only during testing (without backprop) can change the output of the neural net. 

They write ""We generate N-best lists based on P(T|S) and
then rerank the list by linearly combining p(T|S)
with p(S|T) and LT . ""

This makes me think that they first generate all of the possible outputs, and then simply apply a reranking to this. But the output of the neural net has not changed at all. Is this understanding correct?
",3,2
482,2015-12-19,2015,12,19,8,3xejdw,Kernel SVM's are 2layer neural nets!,https://www.reddit.com/r/MachineLearning/comments/3xejdw/kernel_svms_are_2layer_neural_nets/,mr_robot_elliot,1450480267,,18,21
483,2015-12-19,2015,12,19,10,3xf1su,I'm going on a 2-week road trip. What 1 ML or AI book should I take with me?,https://www.reddit.com/r/MachineLearning/comments/3xf1su/im_going_on_a_2week_road_trip_what_1_ml_or_ai/,ritperson,1450489807,"I've already got a fiction book (Erewhon by Sam Butler), now I need a technical book for when I'm in the passenger seat.

I've been doing web and data engineering for a while, but I'd *really* like to get a better grasp on the math behind machine learning and AI. I find it's actually easier to understand the code first, and the math notation second, so books that combine the two appeal the most to me. I'd also like to get to the cutting edge, deep learning techniques, etc (I haven't studied this area much at all yet). 

What do you recommend?
",16,21
484,2015-12-19,2015,12,19,15,3xfto9,Visualisation of the layers of a deep Q network playing breakout,https://www.reddit.com/r/MachineLearning/comments/3xfto9/visualisation_of_the_layers_of_a_deep_q_network/,alito,1450506052,,1,4
485,2015-12-19,2015,12,19,15,3xfwil,I'm looking for an approximate clustering algorithm for a paper I'm working on.,https://www.reddit.com/r/MachineLearning/comments/3xfwil/im_looking_for_an_approximate_clustering/,bleindB,1450507942,"I have the main algorithm developed but that is applied at a later stage in the experiment. Before that, however, I need to find an *approximate set* of clusters for my data set. Moreover, I already have my set of desired cluster centers so I want the clustering to be done around these centers. I'm only familiar with K-Means and Fuzzy C-Means and neither of these will do the job as they will not ""grow"" the clusters around the given center points. Any help would be very much appreciated.",11,1
486,2015-12-19,2015,12,19,16,3xfxy5,Machine learning Engineer vs Data Scientist,https://www.reddit.com/r/MachineLearning/comments/3xfxy5/machine_learning_engineer_vs_data_scientist/,letoseldon,1450508990,"When looking at job postings that don't require a PhD (non-research), it seems that there is some overlap between these two job titles, but the ""data scientist"" category is extremely broad.  

Most jobs that specifically have ""machine learning"" in the title seem to be looking for CS people with some experience in ML (usually specifically saying ""MS in CS with experience in ML""). 

""Data scientist"" jobs seem to fall into one of two categories: 
(1) rebranded ""data analyst"" jobs that are looking for people with some background in data analysis, often looking for R/SAS/SPSS. Seems like the majority of data scientist jobs.
(2) ""computational statistician"" - Python and databases experience with good statistics background.  I see a lot of grad students in statistics gravitate toward these jobs. Not likely to involve much ML (you might use lasso but no SVM/deep learning).

Despite being a non-CS guy (grad student in statistics), I find the ""ML engineer""-type job a lot more attractive.  I have a stronger programming background that stats students (strong Python, low-intermediate C/C++, Unix, etc.) and ML background (took grad classes in the CS department that involved good measure of implementation and theory) but no CS fundamentals (algorithms &amp; data structures, software design). 
I'm afraid that most ML engineer interviews will involve an equal measure of ML/statistics questions and generic algorithm theory questions.

Is it the case that you basically need at least an undergrad CS degree level of CS before getting a job in ML?",18,19
487,2015-12-19,2015,12,19,22,3xgq7x,Any book for ML mathematics that can be understood by a high school student ?,https://www.reddit.com/r/MachineLearning/comments/3xgq7x/any_book_for_ml_mathematics_that_can_be/,3eyedravens,1450532833,"Am a high school student that wants to get into machine learning.
I am already proficient in python and want to be competent with the math prerequisites (probability,Analysis,Linear Algebra,Statistics)  that concern it. I have tried reading some Math books but  find them rather  ""cloudy"". Please help :/",7,1
488,2015-12-19,2015,12,19,22,3xgq8s,"Request for information (Stochastic Neural Networks, Bayesian Probabilities, Heuristics and Fuzzy Hilbert Spaces)",https://www.reddit.com/r/MachineLearning/comments/3xgq8s/request_for_information_stochastic_neural/,skyleach,1450532847,"I'm attempting to modify my existing sources to include perceptual context hilbert spaces as metadata attached to linguistic consumption. I have the general idea of assigning a context-sensitive 4D vertices into the space relative the ID of the rational context and using bayesian probabilities (with the data points serving as a heuristic modification vector for efficiency over time) feeding the life cycle and process priority of the network mapping algorithms.
This will be a time-sensitive use of stochastic processing where headless consumers will generate 4D coordinate space matrices for a given block of time and each vertices will map to a tagged block of content being consumed by the other parsers focused on other types of data.
Does anyone know of any other projects working on this at the moment with which I could collaborate?

If you don't understand what I'm talking about, ask.  Be helpful towards the question/discussion.  There are a significant number of cross-discipline approaches here that most people don't run into in combinations.  However my model works damned well so far so you may not *understand* how I'm using these ideas together, but if you understand what each discipline is I can explain how they relate... just ask.",9,0
489,2015-12-19,2015,12,19,22,3xgqj2,How to implement a NN??,https://www.reddit.com/r/MachineLearning/comments/3xgqj2/how_to_implement_a_nn/,[deleted],1450533061,[deleted],0,0
490,2015-12-19,2015,12,19,23,3xgrir,What is the best programming language to implement neural networks?,https://www.reddit.com/r/MachineLearning/comments/3xgrir/what_is_the_best_programming_language_to/,pescu23,1450533725,"Languages like Oz or Erlang seem more adapted, but they don't have many libraries, and they are harder to master (it's easy to play with them, but is it OK to create complete software ?).

What would you suggest ?",8,0
491,2015-12-20,2015,12,20,0,3xgzec,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,https://www.reddit.com/r/MachineLearning/comments/3xgzec/deep_speech_2_endtoend_speech_recognition_in/,alexeyr,1450538465,,5,14
492,2015-12-20,2015,12,20,0,3xh264,"Which stochastic method has empirically faster convergence among Adam,AdaDelta,AdaGrad,RMSprop etc",https://www.reddit.com/r/MachineLearning/comments/3xh264/which_stochastic_method_has_empirically_faster/,mr_robot_elliot,1450539996,,8,10
493,2015-12-20,2015,12,20,1,3xh9sv,Nando de Freitas will be doing an AMA in /r/MachineLearning on December 26,https://www.reddit.com/r/MachineLearning/comments/3xh9sv/nando_de_freitas_will_be_doing_an_ama_in/,olaf_nij,1450543699,"After a bit of a break, I'm happy to announce the next AMA. [Nando de Freitas](https://www.cs.ox.ac.uk/people/nando.defreitas/), Professor at Oxford University and a senior staff research scientist at Google DeepMind will be stopping by /r/MachineLearning on December 26 for an AMA.

As usual, a thread will be created before the official AMA time for those who won't be able to attend on that day.",9,166
494,2015-12-20,2015,12,20,1,3xha23,Is this a Hidden Markov Model?,https://www.reddit.com/r/MachineLearning/comments/3xha23/is_this_a_hidden_markov_model/,[deleted],1450543817,[deleted],0,0
495,2015-12-20,2015,12,20,2,3xhc7b,Cross entropy error for neural networks,https://www.reddit.com/r/MachineLearning/comments/3xhc7b/cross_entropy_error_for_neural_networks/,zibenmoka,1450544809,"Hi there.

I think I am confused about cross entropy error function formulation for multi-class classification problem. From what I can see here:

https://www-i6.informatik.rwth-aachen.de/publications/download/861/GolikPavelDoetschPatrickNeyHermann--Cross-Entropyvs.SquaredErrorTrainingaTheoreticalExperimentalComparison--2013.pdf

I conclude cross-entropy formulation should not be averaged over number of training examples, but on the other hand some guy here

http://neuralnetworksanddeeplearning.com/chap3.html
uses average of cross entropies as an error 


Which one people tend to use and why? 
",6,1
496,2015-12-20,2015,12,20,2,3xhg04,"I posted this back propagation tutorial a while back and it seems to have helped a few people, so I thought I should post it here!",https://www.reddit.com/r/MachineLearning/comments/3xhg04/i_posted_this_back_propagation_tutorial_a_while/,Weihua99,1450546578,,4,6
497,2015-12-20,2015,12,20,3,3xhj58,Deep Reinforcement Learning Zoo - looking for trained models,https://www.reddit.com/r/MachineLearning/comments/3xhj58/deep_reinforcement_learning_zoo_looking_for/,GeneralIntelligence,1450548015,"In my master's thesis I am focusing on knowledge transfer in deep Q learning, specifically on Arcade Learning Environment. For that I need several fully trained models. However, I don't really have access to a fast machine; training base models for several games alone would take a couple of weeks. So I was looking for some pretrained models, with no luck.

If you have trained models for some of the games, doesn't matter what's the implementation, I would be very grateful if you could share. I've created a github repo at https://github.com/michalkoziarski/Deep-RL-Zoo. Ultimately, I'd hope to maintain something similar to the model zoo, so people could easily get trained models for different implementations of the algorithm.

If you have models and are willing to share, either submit pull request, post here or contact me directly. For now there is no specific format for submissions, I will sort that out later if there will be enough models shared for it to even make sense. If possible please submit implementation used, training parameters and score history.

TL;DR: I am looking for trained models, if people are willing to share I will also maintain repository for others to use.",7,16
498,2015-12-20,2015,12,20,3,3xhmb3,Using Azure ML,https://www.reddit.com/r/MachineLearning/comments/3xhmb3/using_azure_ml/,brakmic,1450549477,,0,3
499,2015-12-20,2015,12,20,4,3xhr3r,CudNN TGZ download link?,https://www.reddit.com/r/MachineLearning/comments/3xhr3r/cudnn_tgz_download_link/,miserlou,1450551685,"I'm looking for ""cudnn-7.0-linux-x64-v3.0-prod.tgz"" - I need to have an account approved by nVidia to do this, and I need the file _today_ for an urgent project.

Can anybody DM me a link? Thanks very much!

edit, found it!
edit2: removed direct link at host's request",4,1
500,2015-12-20,2015,12,20,4,3xhtoh,What are the best alternatives of Prismatic?,https://www.reddit.com/r/MachineLearning/comments/3xhtoh/what_are_the_best_alternatives_of_prismatic/,5h0m4,1450552894,"Prismatic  will be close on the 20th of december. Please help me to find out the news application for various Web browsers and mobile devices running iOS which combines machine learning, user experience design, and interaction design to create a new way to discover, consume, and share media. I want to use the software uses social network aggregation and machine learning algorithms to filter the content that aligns with the interests of a specific user. Also it should integrates with Facebook, Twitter, and Pocket to gather information about user's interests and suggest the most relevant stories to read.
",2,0
501,2015-12-20,2015,12,20,5,3xi096,Screwing Around with Models: Music Generation Using Weird Autoencoder and LSTM Architecture in Keras,https://www.reddit.com/r/MachineLearning/comments/3xi096/screwing_around_with_models_music_generation/,ThatWillNeverShake,1450555942,"Following my last [attempt](https://www.reddit.com/r/MachineLearning/comments/3td04a/music_generation_using_stacked_denoising/) at music generation, I was able to generate another music ""extension"" by training a NN model over Joanna Newsom's song ""Sapokanikan"" (here's the video for context and comparison, although in my opinion you should really listen to it anyways because it's gorgeous):

https://www.youtube.com/watch?v=ky9Ro9pP2gc

A 90 second sample with rate of 16000 hz from the song is overlappingly sliced into 2048 samples and then STFT'd, and use the frequency phase and magnitude as input. Then a 16 layer Denoising Autoencoder is trained with ELU activation followed by dropout layers for the encoder and linear activation for the decoder layers. I then add 2 LSTM layers in between the last encoder and first decoder (i.e. the deepest layer in the autoencoder). Each Autoencoder layer reduces the dimension by 250. I then train the LSTM model with overlapping encoded data. Unsurprisingly the LSTM layers add a lot more capacity into the autoencoder. 

To generate the model that will create new data, I feed the encoded data output from last encoder layer (i.e. before LSTM layers) onto layers of LSTM's, and then top it off with the LSTM layers from the autoencoder with frozen weights (i.e. untrainable). Instead of shifted blocks of input like I did [last time](https://www.reddit.com/r/MachineLearning/comments/3td04a/music_generation_using_stacked_denoising/), I instead train it with the shifted blocks of the output of the autoencoder after the LSTM layers.

I use 2 methods of generation: appended parts and whole generation. Appended parts appends the last block of each predictions into the results, while whole generation just use the whole output as the next input.

Part Generated:

https://www.youtube.com/watch?v=zMD04EPm0mU

Whole Generated:

https://www.youtube.com/watch?v=q0ZdSAkGo48

I also tried different generation sequence length for comparison (listen with headphones):

https://www.youtube.com/watch?v=_dwfxuLGsPA

The whole generated sounds closer to the source material, but I'm not sure if it's overfitting the model or what. There is no clicking noise anymore like I did last time because now it's properly windowed.

Any thoughts?

edit: [github](https://github.com/ppramesi/JOANNA)",18,5
502,2015-12-20,2015,12,20,7,3xigbx,Highway networks?,https://www.reddit.com/r/MachineLearning/comments/3xigbx/highway_networks/,djc1000,1450563466,"So keras now supports ""highway networks.""  There was discussion of the concept on /r/machinelearning seven months ago when the article came out.  Here's the article:  http://arxiv.org/pdf/1505.00387v2.pdf

What kind of results have people gotten?  On the one hand the authors are training up to 900 layers.  On the other hand, the performance at 100 layers is about the same as 10 normal dense layers, so what's the point?  

Anyone have experience to share?",14,14
503,2015-12-20,2015,12,20,7,3xihbd,Word2Vec on,https://www.reddit.com/r/MachineLearning/comments/3xihbd/word2vec_on/,Jxieeducation,1450563935,"com.facebook.katana + picture =&gt; co.vine.android, com.instagram.android, com.snapchat.android

com.google.android.apps.translate + chinese =&gt; com.google.android.inputmethod.pinyin, com.voicetranslator.SpeakAndTranslateFree, com.croquis.biscuit

anime + wallpaper =&gt; com.animepictures.stalkerg.ap, com.jb.gosms.theme.heartscandyzebra4scs , com.hola.launcher.theme.zc14982

code:
https://github.com/jxieeducation/Quick-Data-Science-Experiments-2015/tree/master/appstore-data-analysis

data:
https://github.com/MarcelloLins/GooglePlayAppsCrawler

post:
http://jxieeducation.com/2015-12-19/Appstore-Review-With-Word-Embedding/",1,0
504,2015-12-20,2015,12,20,8,3xioux,What amount of compositional parameters require distributed machine learning?,https://www.reddit.com/r/MachineLearning/comments/3xioux/what_amount_of_compositional_parameters_require/,[deleted],1450567545,[deleted],1,0
505,2015-12-20,2015,12,20,9,3xiz8g,Generating Audio with NeoRLs Predictive Hierarchy,https://www.reddit.com/r/MachineLearning/comments/3xiz8g/generating_audio_with_neorls_predictive_hierarchy/,CireNeikual,1450572707,,0,3
506,2015-12-20,2015,12,20,12,3xjfod,Deep belief network for small data sets,https://www.reddit.com/r/MachineLearning/comments/3xjfod/deep_belief_network_for_small_data_sets/,hassanzadeh,1450581788,"Hi everyone,
I have a small data set less than 100 samples. I wonder if deep learning can applied to my case.  I thought deep belief network can be a good choice because of the unsupervised tuning. Can you please share with me your thought on this?
Thanks",7,1
507,2015-12-20,2015,12,20,14,3xju90,Thinking out loud: theoretical basis for eg vgg_e network architecture?,https://www.reddit.com/r/MachineLearning/comments/3xju90/thinking_out_loud_theoretical_basis_for_eg_vgg_e/,hughperkins,1450590464,"deep neural nets are recently producing phenomenal successes.  In many fields, be it images, text, sound, and even starting to write programs (ie NTM et al).

However, I get the uncomfortable feeling that a lot of the network architectures, be it alexnet, vgg_e, inception, and so on, are basically very empirical in nature, ie 'look it works, here are the results'.  I'm wondering what options could be open to providing some kind of theoretical underpinnings to such architectures?  Some kind of theoretical justification for why we have eg 32 feature planes in layer 1, and for several layers no maxpooling, and then throw in a maxpooling, and so on?

Perhaps there might be 'something' we could measure for each layer, maybe some measure of abstraction, or, throwing out buzzwords for now really :-P, information/entropy, or ... something?

Alternatively, perhaps some way of being able to measure, at least, 'ok, for layer 3, we dont know how many filter planes we should have, but we do know that the current number is too [few|many] because .... [overtraining|undertraining|entropy|something]'?
",4,5
508,2015-12-20,2015,12,20,16,3xk13d,What framework will OpenAI use?,https://www.reddit.com/r/MachineLearning/comments/3xk13d/what_framework_will_openai_use/,cerberusd,1450595486,"What kind of framework will OpenAI publish with? As far as language goes, given their stated purpose, Python seems obvious, but more importantly, will they develop a proprietary framework, or will individual OpenAI researchers use languages/frameworks of their choosing like theano/tensorflow/torch?",2,1
509,2015-12-20,2015,12,20,16,3xk1ji,Wagic (MTG) and deep learning,https://www.reddit.com/r/MachineLearning/comments/3xk1ji/wagic_mtg_and_deep_learning/,Tom_Neverwinter,1450595830,"I feel like running a experiment on MTG using wagic. (
As many cards are already implemented) A input system, and ocr to get my weights. This is a just for fun project, but I'm interested on feedback and any input. I will be building it on a vsphere or hyper-v server so it is easily moved around and shared.",2,7
510,2015-12-20,2015,12,20,17,3xk6a9,Is there code for Google's inceptionism generation of class images?,https://www.reddit.com/r/MachineLearning/comments/3xk6a9/is_there_code_for_googles_inceptionism_generation/,dudel,1450600002,"I'm looking for an implementation or paper on the procedure for generating images from trained classes as in [Google's inceptionism post](http://googleresearch.blogspot.se/2015/06/inceptionism-going-deeper-into-neural.html). Some papers are cited but frankly I can't make sense of them for this purpose. The code they released soon after is great but it doesn't include this visualizing of ""platonic ideals"" that I find really intruiging. 

They mention pixel correlation which reminds me of [Gatys's paper about ""style"" images](http://arxiv.org/abs/1508.06576), but I don't have enough experience with CNNs to connect the dots myself. Does anyone have any suggestions on how to go about?",2,1
511,2015-12-20,2015,12,20,18,3xkbq5,[help] how to represent text data to representative features using python/matlab?,https://www.reddit.com/r/MachineLearning/comments/3xkbq5/help_how_to_represent_text_data_to_representative/,poporing88,1450605023,I have a set of html with text data. I would like to convert these articles to representative features to perform some multimedia-retrieval with audio/video/images. My knowledge on this is very basic (BoW-&gt; LDA) but I am not familiar with the state-of-the-art methods and the correct tools to do this. Many thanks for knowledge sharing :),12,0
512,2015-12-20,2015,12,20,21,3xkmrp,"[help] html + machine learning. Is it possible to create a NN which can identify comments in a web page? If yes, can you guys give me some starting points? Thanks!",https://www.reddit.com/r/MachineLearning/comments/3xkmrp/help_html_machine_learning_is_it_possible_to/,yodeah,1450615068,,3,0
513,2015-12-21,2015,12,21,0,3xl5o4,"75% prediction rate of the housing market [study] using Multiple Regression Analysis contrasted with Neural Networks, methods could be applied to other markets by finding correlated variables",https://www.reddit.com/r/MachineLearning/comments/3xl5o4/75_prediction_rate_of_the_housing_market_study/,CitizenJosh,1450626976,"&gt; The neural network seems to be able to predict up to 4 months out with no issue.

* [article](https://docs.google.com/document/d/1KnAlssQ4k7ownsh4ig6KlE2d5CTph5BO7_e1NzNqgDo/edit)
* [discussion on r/RealEstateTechnology](https://www.reddit.com/r/RealEstateTechnology/comments/3xl3o2/75_prediction_rate_of_the_housing_market_study/)",0,0
514,2015-12-21,2015,12,21,1,3xl93v,Keras: Has anybody been able to train a stateful LSTM yet?,https://www.reddit.com/r/MachineLearning/comments/3xl93v/keras_has_anybody_been_able_to_train_a_stateful/,butWhoWasBee,1450628648,"It looks like Keras very recently finished stateful LSTMs (https://github.com/fchollet/keras/issues/98) . Has anybody been able to train one of these successfully? I would love to try a truly stateful lstm for text generation as opposed to only passing in the last X. If you look at their example text generation script ( https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py ), which was made before the stateful lstm, you have to select a maxlen, which is defaulted to 20. The network only looks at the last 20 during training and generation. ",5,7
515,2015-12-21,2015,12,21,1,3xl9kw,Visual Question Answering Online Demo: Ask any question about any image,https://www.reddit.com/r/MachineLearning/comments/3xl9kw/visual_question_answering_online_demo_ask_any/,dexter1691,1450628860,,14,9
516,2015-12-21,2015,12,21,2,3xlgeb,Assisted Drawing: Exploring Augmented Creativity,https://www.reddit.com/r/MachineLearning/comments/3xlgeb/assisted_drawing_exploring_augmented_creativity/,alexjc,1450631980,,3,39
517,2015-12-21,2015,12,21,4,3xlxiw,what is the best open source library for sentiment analysis? is it Stanford CoreNLP?,https://www.reddit.com/r/MachineLearning/comments/3xlxiw/what_is_the_best_open_source_library_for/,hlpmewmyrelationship,1450639531,"And how do the various free, freemium, and paid apis on mashape compare to the performance of the best open source one?

My understanding is that Stanford CoreNLP's sentiment analysis is based on this 2013 paper: http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf

which states: 
When trained on the new treebank, this model outperforms all previous methods on several metrics.

Is this model in fact the best in the business? and has it remained so since 2013?",10,14
518,2015-12-21,2015,12,21,4,3xlyrn,cross-validated confusion matrix,https://www.reddit.com/r/MachineLearning/comments/3xlyrn/crossvalidated_confusion_matrix/,[deleted],1450640087,[deleted],0,1
519,2015-12-21,2015,12,21,5,3xm56r,Inside OpenAI: Will Transparency Protect Us From Artificial Intelligence Run Amok? Interview with founding member of OpenAI,https://www.reddit.com/r/MachineLearning/comments/3xm56r/inside_openai_will_transparency_protect_us_from/,SkaiFox,1450642909,,13,15
520,2015-12-21,2015,12,21,5,3xm8ry,parametric vs non parametric training,https://www.reddit.com/r/MachineLearning/comments/3xm8ry/parametric_vs_non_parametric_training/,chajohari,1450644485,"Where to find good comparison between parametric vs non parametric training method, from whatever I have read so far it's the later in demand/has bright future. Curious to know what's in store for other going ahead",1,2
521,2015-12-21,2015,12,21,10,3xnbe3,Is there a text processing system that integrate all repetitive questions on the thread in unsupervised-way?,https://www.reddit.com/r/MachineLearning/comments/3xnbe3/is_there_a_text_processing_system_that_integrate/,inctrljinee,1450662151,"Hi everyone.
I am curious about is there any web-based text processing system that integrates repetitive questions(comments) on the thread by applying machine learning scheme. For example, like AMA session in reddit, people often ask very similar questions but their literal expressions vary through each person. (ex. When did you join the band? When did you first bump into the band? When was your first meeting with the band? all the repetitive questions)  I believe that the state-of-the-art text processing technique has reached the level to build a system that process text in a collaborative way. I think this kind of service is very imperative because it is nearly impossible to answer the millions of questions rushing through thread but it would be more efficient to answer the question that has been mostly questioned about.

If you guys know any similar research or corporation which is working on the similar problem, I'll be glad to see sharing them with people here.",2,0
522,2015-12-21,2015,12,21,11,3xnf8g,AI: Robots that can learn by viewing 'How-To' videos,https://www.reddit.com/r/MachineLearning/comments/3xnf8g/ai_robots_that_can_learn_by_viewing_howto_videos/,[deleted],1450664103,[deleted],0,1
523,2015-12-21,2015,12,21,11,3xnfe0,Seeking advice for hardware configuration for doing deep learning experiments.,https://www.reddit.com/r/MachineLearning/comments/3xnfe0/seeking_advice_for_hardware_configuration_for/,koormoosh,1450664173,"I am looking to assemble myself a desktop to run some deep learning experiments and have a budget of around ~$3k. Can you share your opinion on what graphic card, cpu and memory size I should aim for to squeeze the most power out of my budget?",2,0
524,2015-12-21,2015,12,21,13,3xntt6,I want to learn deep learning,https://www.reddit.com/r/MachineLearning/comments/3xntt6/i_want_to_learn_deep_learning/,Lt_Snuffles,1450671409,"I am fairly good in python. I want learn deep learning and apply it in python. There is so many resources lying around in internet. 
I am trying to find a systematic way to learn deep learning. How should I proceed?",9,3
525,2015-12-21,2015,12,21,13,3xnwxr,My PI is losing my credibility.,https://www.reddit.com/r/MachineLearning/comments/3xnwxr/my_pi_is_losing_my_credibility/,[deleted],1450673063,[deleted],4,5
526,2015-12-21,2015,12,21,14,3xo1c1,Question about word2vec modeling and effect of sentence loading order,https://www.reddit.com/r/MachineLearning/comments/3xo1c1/question_about_word2vec_modeling_and_effect_of/,sleepicat,1450675464,"My word2vec model (gensim python version) seems to be very sensitive to the order in which data is presented to it. For example, if it sees one example of ""orange cat"" and later sees 20+ examples of ""orange juice,"" it doesn't seem to adjust to the 20+ examples of ""orange juice"" and continues to show ""cat"" (the vector) being closer than ""juice"" to the word ""orange."" But if I force some ""orange juice"" examples to load before the ""orange cat"" example, the distance between ""cat"" and ""orange"" becomes much greater than the distance between ""orange"" and ""juice"" (like one would expect).

I've tried adjusting things like the learning rate, choice of hierarchical or negative sampling, and these seem to have little effect. 

But then I came across a [forum comment by Mikolov](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/kH1Ch0sWJwMJ)  saying that the sentences need to be shuffled--I assume he means during the iterative training (i.e., looping through the sentences multiple times), but am not certain.

Sentence shuffling seems like a hacky and potentially computationally costly work-around.

What would be the best approach to make the model less sensitive to sentence order?  My initial guess would be to adjust the learning rate, but I thought I tried that already. ",8,7
527,2015-12-21,2015,12,21,15,3xoaoh,What is a natural strongly convex function over the unit ball for the L-infinity norm?,https://www.reddit.com/r/MachineLearning/comments/3xoaoh/what_is_a_natural_strongly_convex_function_over/,U1F4A9,1450680930,"i.e. for L1 it would be negative entropy, Euclidean distance for the L2 ball, what is a good choice for the unit ball in L-infinity?",4,8
528,2015-12-21,2015,12,21,16,3xob7t,Calculating Receptive Fields in CNNs,https://www.reddit.com/r/MachineLearning/comments/3xob7t/calculating_receptive_fields_in_cnns/,coltsfan2604,1450681275,How do people calculate the receptive fields at each layer in a CNN? Things get complicated following pooling layers and strides but this is not mentioned in any papers but a very important aspect of understand these beasts. ,8,0
529,2015-12-21,2015,12,21,18,3xom8a,[Discussion] What would you like to see in a Theano/Python-Based Deep Learning Library?,https://www.reddit.com/r/MachineLearning/comments/3xom8a/discussion_what_would_you_like_to_see_in_a/,nasimrahaman,1450688795,"Hello /r/MachineLearning, 

We here in [HCI](http://hci.iwr.uni-heidelberg.de) are working on a brand new Theano-based deep learning library geared towards image processing. In that context, I'd love to know what is it you're missing in the existing libraries (Lasagne/Keras/Pylearn2) that you would love to see implemented.

Thanks! ",38,28
530,2015-12-21,2015,12,21,20,3xoxbl,Is logistic regression the most suitable for binary classification ?,https://www.reddit.com/r/MachineLearning/comments/3xoxbl/is_logistic_regression_the_most_suitable_for/,PyBet,1450697024,"I am rather new to machine learning. I want to classify a binary class, 1 or 0, True or False. After reading about it I think that logistic regression might be suitable for this task.

Should I start there, and what should I try next ?",12,1
531,2015-12-21,2015,12,21,20,3xoyz9,Convert Your Text into 3D Images with WordsEye Tool,https://www.reddit.com/r/MachineLearning/comments/3xoyz9/convert_your_text_into_3d_images_with_wordseye/,[deleted],1450698238,[deleted],0,1
532,2015-12-21,2015,12,21,21,3xp3pv,Gradient descent step size question,https://www.reddit.com/r/MachineLearning/comments/3xp3pv/gradient_descent_step_size_question/,Letitgo123456,1450701412,"This is prolly a silly question but here goes :-

In general, step size is decreased as the # of iterations increases. However what this means is that the step size is small when the gradient is small and large when the gradient is large. Shouldn't it be the opposite (the only way I can see it making sense is if the curvature is small in the beginning but increases towards the end but I don't see why that'd be the case in general).",9,1
533,2015-12-21,2015,12,21,23,3xpfsj,Idea: weekly advice megathread and ban advice questions,https://www.reddit.com/r/MachineLearning/comments/3xpfsj/idea_weekly_advice_megathread_and_ban_advice/,InaneMembrane,1450708323,I see tons of questions that that get down voted and no responses. Might as well ban them so they don't clog up the sub. Plus people would probably get better advice in with a weekly or semi weekly megathread.,7,0
534,2015-12-22,2015,12,22,0,3xpl91,Side Projects for Interships,https://www.reddit.com/r/MachineLearning/comments/3xpl91/side_projects_for_interships/,Vainsingr,1450710956,"Hello, I'm looking for CS summer internship mostly in CA. I have finished my first semester in the MS CS and have 3.0. Planning to build my resume so that my chances in top companies in data mining/Machine learning domain increase using side projects.

So where do I start? Looking for some idea how to start and some1 to guide 
",4,2
535,2015-12-22,2015,12,22,0,3xpldq,"Numerai - like Kaggle, but with a clean dataset, top ten in the money, and recurring payouts",https://www.reddit.com/r/MachineLearning/comments/3xpldq/numerai_like_kaggle_but_with_a_clean_dataset_top/,improbabble,1450711009,,18,16
536,2015-12-22,2015,12,22,0,3xpp9e,[Reference request] State of the art on MNIST,https://www.reddit.com/r/MachineLearning/comments/3xpp9e/reference_request_state_of_the_art_on_mnist/,AnvaMiba,1450712769,"What is the state of the art on MNIST without preprocessing, without dataset augmentation and without using convolutions?
",8,1
537,2015-12-22,2015,12,22,1,3xprd1,Creating a Data Science Blog,https://www.reddit.com/r/MachineLearning/comments/3xprd1/creating_a_data_science_blog/,TheDataScientist,1450713678,"Hi friends,

I'm looking to create a blog on Data Science and trying to assess what people want to learn or know -- but perhaps have not found a good resource. 

Some ideas include:

* Assessing new software (e.g. if you're a data analytics manager, do you hire more data scientists or bring on a product like KNIME, RapidMiner, DataRobot, Trifacta, etc.)

* Programming walkthroughs from start to finish (e.g. how to scrape a website, clean the data, perform sentiment analysis, etc.)

* How to break into data science (focus on what programming skills, statistics skills, and communication skills are necessary to get in AND be successful)

* [Let me know what you're interested in here]


Also, if you don't mind, would you let me know you're level of expertise (1 being absolute beginner/never touched and 10 being expert) and knowledge in following domains. Example below:

**Programming:** [8] R, Python, Java

**Statistics:** [10] linear, logistic, multivariate, data cleaning, etc.

**Machine Learning:** [7]: SVM, Boosted Trees, Neural Networks, etc.

**Years in field:** 8

**Areas of weakness/things you want to learn better:** [anything]

Many thanks and look forward to helping the community grow.",12,0
538,2015-12-22,2015,12,22,1,3xptxu,Constrained optimization using SGD?,https://www.reddit.com/r/MachineLearning/comments/3xptxu/constrained_optimization_using_sgd/,cypherx,1450714747,"I'm interested in implementing the original Candes formulation of matrix completion using a SGD-based optimization algorithm. The problem formulation is roughly `minimize(nuclear_norm(Complete)) s.t. Complete[i,j] = Original[i,j] where Missing[i,j] = 0`. 

There are two things about this that aren't obvious to me: how to express the nuclear norm objective one sample at a time and even more confusing to me: how to incorporate the equality constraint. 

I know that this can be reorganized into an unconstrained problem like `minimize(||Complete[~Missing] - Original[~Missing]||^2 + lambda * nuclear_norm(Complete))`. 

Are there other ways to incorporate constraints into SGD? Also, with the lagrange multiplier form above, is there a schedule for `lambda` that best approximates solving the original constrained problem? 

",5,3
539,2015-12-22,2015,12,22,2,3xq4tm,EM estimation of a mixture of logistic function,https://www.reddit.com/r/MachineLearning/comments/3xq4tm/em_estimation_of_a_mixture_of_logistic_function/,em_ericzhao,1450719244,"Suppose we have a 1 dimension random variable X, and N observations, x1, x2, ..., xn. I want to model the pdf of X by a mixture of two logistic functions, say, p(x|\theta) = \alpha * logi(x|\theta_1) - \beta * logi(x|theta_2), where \theta={\theta_1, \theta_2, alpha, beta}. Now given the N observations, how can I estimate \theta using EM? Can anybody provide the detailed solution, e.g. equations of E and M step? Thanks a lot.",4,0
540,2015-12-22,2015,12,22,2,3xq7e0,Does arxiv's computer_science.learning require endorsement for first timers?,https://www.reddit.com/r/MachineLearning/comments/3xq7e0/does_arxivs_computer_sciencelearning_require/,Jxieeducation,1450720268,"Does arxiv's computer_science.learning require endorsement for first timers?

Just wondering if I can publish some original LSTM research without endorsement",5,0
541,2015-12-22,2015,12,22,3,3xqdzh,"[x-post] AI Prof Ken Stanley IAmA, Neuroevolution / large-scale ANNs / ML+Video Games",https://www.reddit.com/r/MachineLearning/comments/3xqdzh/xpost_ai_prof_ken_stanley_iama_neuroevolution/,joelbot2000,1450722863,,0,25
542,2015-12-22,2015,12,22,3,3xqfwf,Deep learning for hackers with MXnet (2): Neural art,https://www.reddit.com/r/MachineLearning/comments/3xqfwf/deep_learning_for_hackers_with_mxnet_2_neural_art/,phunter_lau,1450723591,,29,42
543,2015-12-22,2015,12,22,4,3xqi4n,Pseudo Reinforcement Learning? Supervised signal at end of variable length run,https://www.reddit.com/r/MachineLearning/comments/3xqi4n/pseudo_reinforcement_learning_supervised_signal/,Eruditass,1450724498,"I need to learn a regression from n inputs to an output at each timestep, but my ground truth is only on an aggregated basis (e.g. for 300 samples for one session, the summed total should be Y.  The next session may be 3252 samples).  What is the best way to formulate this?  

It sounds like reinforcement learning a bit, something I'm not familiar with, but most applications of RL seem to be learning policies and RNN's, while my regression weights should be constant for each sample and not dependent on previous samples.  Is RL only for RNN's? ",5,4
544,2015-12-22,2015,12,22,4,3xqjv9,Deep Learning SIMPLIFIED: Episode 4 - How to Choose,https://www.reddit.com/r/MachineLearning/comments/3xqjv9/deep_learning_simplified_episode_4_how_to_choose/,jrajagopal,1450725179,"Deep Learning as a field has developed quite a bit in the last decade, and we now have a variety of models to pick from with new models and improvements arriving frequently. The flip side to this is, the burden of choice now falls on you to figure out which model to pick for what application. Here is a clip that gives you some guidelines to help you decide.
https://www.youtube.com/watch?v=JjZDoojyzXQ",1,7
545,2015-12-22,2015,12,22,5,3xqtuo,"24 Data Science, R, Python, Excel, and Machine Learning Cheat Sheets",https://www.reddit.com/r/MachineLearning/comments/3xqtuo/24_data_science_r_python_excel_and_machine/,vincentg64,1450729220,,0,1
546,2015-12-22,2015,12,22,5,3xqvgy,Artificial Intelligence - The Best Worst Thing Ever,https://www.reddit.com/r/MachineLearning/comments/3xqvgy/artificial_intelligence_the_best_worst_thing_ever/,farzyness,1450729883,,2,0
547,2015-12-22,2015,12,22,6,3xr21g,Question about RNN Feature enginering (fixed sliding window vs variable sliding window),https://www.reddit.com/r/MachineLearning/comments/3xr21g/question_about_rnn_feature_enginering_fixed/,rnnnnr,1450732491,"I am using lasagne to train a LSTM rnn for time series prediction of a given sequence. My training data is sequences of length 30. I am currently using a fixed length sliding window to create batches for training -- a tensor of (25,5,15). I then want to use this model to predict sequences of variable length. For this use case, would it be better to train the model on variable length sequences ie show it sequences longer and shorter than 5? It looks like this is possible in las using a mask. Curious if anyone has any thoughts. Current accuracy is sitting around 60% 

",2,4
548,2015-12-22,2015,12,22,6,3xr5uo,TensorFlow LSTM for time series?,https://www.reddit.com/r/MachineLearning/comments/3xr5uo/tensorflow_lstm_for_time_series/,Qwerty10110,1450734079,"Has anyone been able to get a LSTM in TensorFlow working for time series? I'm new to LSTMs, and the examples I can find are for MNIST classification and sequence prediction. Even a working example on the sunspots data set would be hugely helpful. Thanks in advance!",5,1
549,2015-12-22,2015,12,22,7,3xrbfs,I want to get a PhD in Machine Learning -- Major advice,https://www.reddit.com/r/MachineLearning/comments/3xrbfs/i_want_to_get_a_phd_in_machine_learning_major/,numbersloth,1450736469,"Hi all. I've taken an intro to machine learning class and really really like it and I'm excited about the opportunities to apply ML to fields such as computational biology, AI/CogSci, and clinical decision making. 

Right now, I am a Computer Science Major with minors in Math and Biology. I am wondering if I should drop the Bio minor to take up a double major in math and cs? Should I switch my major to mathematics and change cs to a minor? 

I'm reluctant to drop the biology minor since I know that I want to go into research that has applications to health/medicine/biology, but I know that a lot of researchers pick up biology skills on the fly. ",24,5
550,2015-12-22,2015,12,22,9,3xrq1u,Apache Hadoop for Beginners,https://www.reddit.com/r/MachineLearning/comments/3xrq1u/apache_hadoop_for_beginners/,aaron_shugert,1450742974,,0,0
551,2015-12-22,2015,12,22,9,3xrt8w,Predicting the News Youll Read - using ensemble methods against Pocket's data,https://www.reddit.com/r/MachineLearning/comments/3xrt8w/predicting_the_news_youll_read_using_ensemble/,innuhnet,1450744395,,1,19
552,2015-12-22,2015,12,22,10,3xrx72,Multistep methods,https://www.reddit.com/r/MachineLearning/comments/3xrx72/multistep_methods/,Letitgo123456,1450746313,"Polyak in his 'Introduction to Optimization' text mentions multistep methods in which the next point is a function of multiple previous points. He discusses momentum (and some other method) which are functions of only 2 previous points (so 2-step) but doesn't have any discussion on multiple steps.

I was wondering if there were any papers/books discussing this.",0,1
553,2015-12-22,2015,12,22,11,3xs5h5,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/3xs5h5/a_theoretically_grounded_application_of_dropout/,SuperFX,1450750216,,6,21
554,2015-12-22,2015,12,22,11,3xs5nc,Is it reasonable for a random forest to have .7 oob_score but a tiny fraction of a percentage training error?,https://www.reddit.com/r/MachineLearning/comments/3xs5nc/is_it_reasonable_for_a_random_forest_to_have_7/,randomForestConfusio,1450750304,"I'm using sklearn RFClassifier to classify text. With an oob_score of .56 I get less than 1 % training error. adding some additional features to get oob_Score of .7 I get less than .001 % training error. Are these reasonable results? It suprises me that the training error is so low, I only have about 15 features. At least it scales with oob_score so it seems possible that everything is working correctly. For the unfamiliar, oob_score in scikit learn is basically a prediction of expected accuracy on a test set, and is generally quite accurate for large training and test sets. That is, if you get an oob_score of .65 on a 20k sample size training set, and then predict a 20k sample size test set, you will correctly classify within 1 % of 65 % of the test samples.",5,2
555,2015-12-22,2015,12,22,12,3xseu5,(xpost from r/statistics) What level of statistics Master's program could I get into with a 3.5 gpa chemical engineering background and 2 years engineering work experience?,https://www.reddit.com/r/MachineLearning/comments/3xseu5/xpost_from_rstatistics_what_level_of_statistics/,curious95,1450754756,"I haven't studied math beyond differential equations and linear algebra, and I don't have research experience, so I assume I'd be a no go on the top programs. But how far down the rankings would I have to settle for? I scored a 2190 on the SAT and I think I could do comparably well on the GRE. I've done the machine learning coursera course and a few other programming and statistics ones, and I attended a two week machine learning workshop during my PTO. My work deals with databases, programming, and basic analysis, but no significant statistics.

While I'd really like to apply last minute for the fall 2016 cycle (I hate my job), I think it'd be better for me to build up my profile and go for fall 2017. I don't have research experience but I knew two professors well that I think would write positive letters of recommendation.

I love the ideas and applications of machine learning and would like to work in this area. I know it's rather advanced, though, so I'd be happy with any sort of predictive statistical work. Long term I'd still shoot for science focused data... lots of fields interest me so I hope I could learn common techniques and change it up my focus every now and again.

I went to an average state school and I think my job options have been limited by that (also my degree, of course). I'd like to go to a well respected school with strong connections for better chances at the interesting jobs (right now audio interests me the most, though I have more ""expertise"" in other areas).

So I'm curious about the following:
* What ranking of statistics program could I get into?
* What ranking of data analytics or related master's program could I get into?
* Would I have any chance at getting funded, and what would I have to do?
* Could I quit my job now, start taking classes and somehow transfer in for the fall 2016 or 2017 application cycles? I have a little savings and figure I could get by if I worked part time.
",3,0
556,2015-12-22,2015,12,22,14,3xss4h,What are some differences between motion segmentation and background subtraction?,https://www.reddit.com/r/MachineLearning/comments/3xss4h/what_are_some_differences_between_motion/,mattmilos,1450761512,,0,0
557,2015-12-22,2015,12,22,15,3xt1gb,Machine Learning in Javascript- A compilation of Resources,https://www.reddit.com/r/MachineLearning/comments/3xt1gb/machine_learning_in_javascript_a_compilation_of/,mattmilos,1450766751,,3,1
558,2015-12-22,2015,12,22,15,3xt2hd,Clojure Machine Learning &amp; Statistics Libraries,https://www.reddit.com/r/MachineLearning/comments/3xt2hd/clojure_machine_learning_statistics_libraries/,mattmilos,1450767358,,0,0
559,2015-12-22,2015,12,22,16,3xt5k0,Scalable recommendation engines in Python?,https://www.reddit.com/r/MachineLearning/comments/3xt5k0/scalable_recommendation_engines_in_python/,pddpro,1450769287,"As the title suggests, I am looking for formidable recommendation engines that can be scaled. My primary language is python, thus, I am looking for engines that are either written in python are have python bindings available. I have already looked at CRAB (which is memory-based and doesn't appear to be scalable) and some other small libraries but none appear to fulfill my needs.

Honestly, I am quite new to this field so I may not even have an idea as to what I require. I would be thankful for any insights.",9,2
560,2015-12-22,2015,12,22,18,3xtfta,[Request] Could some kind soul please provide a run down of the different ML frameworks/pros/cons? Or provide a link to a rundown?,https://www.reddit.com/r/MachineLearning/comments/3xtfta/request_could_some_kind_soul_please_provide_a_run/,Xirious,1450776858,"Hey all,

I'd like to get started with some DL and I thought I'd come here to ask for some advice. I've seen tons of different posts about the various frameworks and would like to compare between them before I delve in. These are the one's I'm currently considering: 

Caffe

Theano

TensorFlow

Lasagne

Keras 

Torch

CuDNN(?) (I have registered/installed it).

If there are any missing please let me know. I am proficient in Python, significantly better in C/C++ and do a ton of my research in MATLAB. From what I've seen 90% of the popular frameworks are in Python and I have no problem using it. What would you say are the major differences and which is the best to begin using? My main development is on Mint 17.2 on a CUDA-enabled system so I'd prefer solutions related to using that but I also have two other PC's I develop from that aren't necessarily CUDA-enabled (one is Windows 10 and has a compatible GPU the second is Mint 17.3 with no CUDA GPU). 

For anyone interested: I'm busy with my PhD in Synthetic Aperture Radar in the Maritime Domain performing ship detection and fusing data from various SAR sensors, including transponder based systems. I have a background Image Processing/Computer Vision/Machine Learning and am close to finishing up with the PhD. I have an idea or two for ",2,3
561,2015-12-22,2015,12,22,19,3xtlyt,Goddamnit.,https://www.reddit.com/r/MachineLearning/comments/3xtlyt/goddamnit/,actuallyaspacepanda,1450781670,,71,95
562,2015-12-22,2015,12,22,20,3xtp2v,Fabrication Companies and Their Effect on Different Industries,https://www.reddit.com/r/MachineLearning/comments/3xtp2v/fabrication_companies_and_their_effect_on/,alexgender27,1450784003,,0,1
563,2015-12-22,2015,12,22,21,3xtt1m,This Artificial Intelligence Company Helps to build targeted Follower base on Twitter,https://www.reddit.com/r/MachineLearning/comments/3xtt1m/this_artificial_intelligence_company_helps_to/,klend1,1450786860,,1,0
564,2015-12-22,2015,12,22,21,3xtua1,Deep Learning: How will it change our lives?,https://www.reddit.com/r/MachineLearning/comments/3xtua1/deep_learning_how_will_it_change_our_lives/,cybrbeast,1450787724,,0,0
565,2015-12-22,2015,12,22,22,3xu1x5,Why Google and Facebook are opensourcing AI hardware,https://www.reddit.com/r/MachineLearning/comments/3xu1x5/why_google_and_facebook_are_opensourcing_ai/,jonfla,1450792410,,0,0
566,2015-12-23,2015,12,23,0,3xubov,From Frequency to Meaning: Vector Space Models of Semantics (2010),https://www.reddit.com/r/MachineLearning/comments/3xubov/from_frequency_to_meaning_vector_space_models_of/,mtrn,1450797467,,3,12
567,2015-12-23,2015,12,23,0,3xucre,XGBoost explained,https://www.reddit.com/r/MachineLearning/comments/3xucre/xgboost_explained/,xristos_forokolomvos,1450797967,"Since this model seems to pop up everywhere in Kaggle competitions, is anyone kind enough to explain why it is so powerful and what methods are used for the ensembles that keep on bashing the scoreboards?",16,40
568,2015-12-23,2015,12,23,1,3xund8,Accuracy vs Explainability of Machine Learning Models [NIPS workshop poster],https://www.reddit.com/r/MachineLearning/comments/3xund8/accuracy_vs_explainability_of_machine_learning/,fhuszar,1450802584,,3,37
569,2015-12-23,2015,12,23,1,3xunrf,TensorFlow Regression with Categoricals,https://www.reddit.com/r/MachineLearning/comments/3xunrf/tensorflow_regression_with_categoricals/,linux1975,1450802757,"Has anyone seen a good example on how to implement regression with numerical scaling and categoricals inside tensorflow, without having to onehot or pandas?",7,0
570,2015-12-23,2015,12,23,2,3xuskv,Driverseat: AI Machine Learns to Drive Using Crowdteaching | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/3xuskv/driverseat_ai_machine_learns_to_drive_using/,mttd,1450804760,,1,7
571,2015-12-23,2015,12,23,3,3xuz1a,ImageNet scavenger hunt,https://www.reddit.com/r/MachineLearning/comments/3xuz1a/imagenet_scavenger_hunt/,FreakTheMight,1450807426,,2,3
572,2015-12-23,2015,12,23,3,3xv5lo,What are some ideas for a machine learning project which is related to a social problem?,https://www.reddit.com/r/MachineLearning/comments/3xv5lo/what_are_some_ideas_for_a_machine_learning/,alivenotions,1450810014,"As a third year Computer Science student, we are assigned to work on a project which is related to a social problem. The time assigned is approximately three months. What could be some interesting ideas to work on that should take three months?
I am fairly comfortable with Python and intend to do the project in it.",7,7
573,2015-12-23,2015,12,23,4,3xvaxw,How Random Forest Modeling Solves Seattle's Bikesharing Problem,https://www.reddit.com/r/MachineLearning/comments/3xvaxw/how_random_forest_modeling_solves_seattles/,dnabeyta,1450812231,,1,0
574,2015-12-23,2015,12,23,4,3xveta,Deep Learning SIMPLIFIED: Episode 5 - An Old Problem,https://www.reddit.com/r/MachineLearning/comments/3xveta/deep_learning_simplified_episode_5_an_old_problem/,jrajagopal,1450813800,"While Deep Neural Nets are the state of the art in machine learning, the flipside is that they are really hard to train. Up until 2006, there was no way to train them satisfactorily. Here is a clip that explains further. 
https://www.youtube.com/watch?v=SKMpmAOUa2Q",3,0
575,2015-12-23,2015,12,23,5,3xvo21,Transfer Learning for ConvNets in Torch?,https://www.reddit.com/r/MachineLearning/comments/3xvo21/transfer_learning_for_convnets_in_torch/,iwantedthisusername,1450817554,Does anyone know where I can find an example of transfer learning in Torch?,1,5
576,2015-12-23,2015,12,23,6,3xvx6d,"Which of the NIPS 2015 papers are most significant/useful, and why?",https://www.reddit.com/r/MachineLearning/comments/3xvx6d/which_of_the_nips_2015_papers_are_most/,Trepeneur,1450821313,"The NIPS (Advances in Neural Information Processing Systems) 28 papers are interesting to read, but as a newcomer to the field it is hard to decide which ones represent significant improvement on the state of the art:

http://papers.nips.cc/book/advances-in-neural-information-processing-systems-28-2015

Which would be your picks, and why?",3,18
577,2015-12-23,2015,12,23,7,3xw5ir,Learning ML,https://www.reddit.com/r/MachineLearning/comments/3xw5ir/learning_ml/,[deleted],1450824906,[deleted],2,0
578,2015-12-23,2015,12,23,11,3xwxhk,LSTM generating animation?,https://www.reddit.com/r/MachineLearning/comments/3xwxhk/lstm_generating_animation/,reddit_tl,1450838165,"Is there work that uses LSTM or other memory based techniques to generate animation sequences, and therefore animation video, based on existing video?
If not, is this even possible?",6,1
579,2015-12-23,2015,12,23,13,3xx9cw,Compress and Control (Use gzip to play Pong),https://www.reddit.com/r/MachineLearning/comments/3xx9cw/compress_and_control_use_gzip_to_play_pong/,gogogadgetlegz,1450844296,,3,13
580,2015-12-23,2015,12,23,13,3xx9ig,"[1512.05463] Continuous online sequence learning with an unsupervised neural network model (Numenta, Inc)",https://www.reddit.com/r/MachineLearning/comments/3xx9ig/151205463_continuous_online_sequence_learning/,ihsgnef,1450844387,,9,13
581,2015-12-23,2015,12,23,15,3xxkgn,Constraints on the depth of the Deep CNNs,https://www.reddit.com/r/MachineLearning/comments/3xxkgn/constraints_on_the_depth_of_the_deep_cnns/,coltsfan2604,1450850574,Is there a constraint on how many layers you can have in a deep cnns? It seems that the last convolutional layer's receptive field should not be greater than the size of the input image. Is that correct? ,1,2
582,2015-12-23,2015,12,23,15,3xxm6z,NIPS 2015 Videos (just scroll down a bit),https://www.reddit.com/r/MachineLearning/comments/3xxm6z/nips_2015_videos_just_scroll_down_a_bit/,barmaley_exe,1450851609,,4,44
583,2015-12-23,2015,12,23,16,3xxsvz,"Dry Powder Filling Machine, Automatic Bottle Washing Machine",https://www.reddit.com/r/MachineLearning/comments/3xxsvz/dry_powder_filling_machine_automatic_bottle/,Anchormark1,1450855985,,0,1
584,2015-12-23,2015,12,23,16,3xxth1,Training Neural Networks with small/tiny training sets,https://www.reddit.com/r/MachineLearning/comments/3xxth1/training_neural_networks_with_smalltiny_training/,quantumloophole,1450856425,"Hi,

I started to study NN's recently and stepped on a question that was not easily answered by google, actually I didn't find a answer.

Every material I've read so far relies on huge databases, for example Nielsen's online book http://neuralnetworksanddeeplearning.com/, uses MNIST with 60,000 training data. 

But I was wondering about situations where you may have only a small amount of training inputs, like person recognition ( given 2 photos, determine if they are the same person), that I imagine have &lt; 10 photos to use as train set.

My question is, given a small number of training data, how to train a NN, is that a special cost function for small sets or it's something else?",17,7
585,2015-12-23,2015,12,23,18,3xy2tb,What makes a model generalise well to new data?,https://www.reddit.com/r/MachineLearning/comments/3xy2tb/what_makes_a_model_generalise_well_to_new_data/,fuckinghelldad,1450863678,,3,2
586,2015-12-23,2015,12,23,20,3xy9gq,Dr. Jrgen Schmidhuber: Microsoft Wins ImageNet 2015 through Feedforward LSTM without Gates,https://www.reddit.com/r/MachineLearning/comments/3xy9gq/dr_jrgen_schmidhuber_microsoft_wins_imagenet/,metacurse,1450868762,,38,67
587,2015-12-23,2015,12,23,20,3xycux,Tools for preprocessing speech before feeding to an LSTM?,https://www.reddit.com/r/MachineLearning/comments/3xycux/tools_for_preprocessing_speech_before_feeding_to/,wildtales,1450871388,Are there any freely available tools that can preprocess speech before I feed it to an LSTM?,6,4
588,2015-12-23,2015,12,23,21,3xyiap,A Year of Approximate Inference: Review of the NIPS 2015 Workshop,https://www.reddit.com/r/MachineLearning/comments/3xyiap/a_year_of_approximate_inference_review_of_the/,mttd,1450875115,,0,5
589,2015-12-23,2015,12,23,23,3xyv3l,"wooden bead machine,want to CNC beads machine,check below",https://www.reddit.com/r/MachineLearning/comments/3xyv3l/wooden_bead_machinewant_to_cnc_beads_machinecheck/,Arterki-tools,1450882518,,5,0
590,2015-12-24,2015,12,24,0,3xz0d7,Eric Schmidt and Jared Cohen on Products of AI,https://www.reddit.com/r/MachineLearning/comments/3xz0d7/eric_schmidt_and_jared_cohen_on_products_of_ai/,byedit,1450885105,,0,1
591,2015-12-24,2015,12,24,0,3xz10c,Logo detection in photos: distorted and resized images,https://www.reddit.com/r/MachineLearning/comments/3xz10c/logo_detection_in_photos_distorted_and_resized/,niujin,1450885406,"I am trying to develop a system that would identify company logos in photos. The photos can be any size, and the logo could be varying sizes and also distorted (imagine if the logo is on a surface at an angle, or on the side of a curved bottle).
I am thinking that maybe this could be handled with neural networks. I've followed some of Andrew Ng's lecture notes and they sound very powerful, however I am struggling to understand how NNs can handle the target image located in a different part of the main image, or distorted in some way.

It's easy for me to understand if the input image and the image you're trying to recognise are the same size: every pixel maps to a weight, over several layers, that eventually lead to a similarity metric between two images.
However how can this approach cope with the translations and resizings that I mentioned?

I'm looking at a Java library called Neuroph, which sounds interesting. Basically I'd like to hear from anyone who solved a similar problem before, what ML approach you would recommend. I've also collected some sample photos with logos in photos of varying quality which I could use as training data. However I'm unsure what to do next.

Also if neural networks really are the best or most practical way of solving this? For example, are they slow or do they use a lot of memory? What about a cascade classifier?

Any advice welcome, especially if you've had experience with a particular approach.
thanks a lot!",9,4
592,2015-12-24,2015,12,24,0,3xz18g,Backprop for (R)BMs and energy based models?,https://www.reddit.com/r/MachineLearning/comments/3xz18g/backprop_for_rbms_and_energy_based_models/,Ghostlike4331,1450885503,"I know now that it should be possible as they have a single cost function at the end. Has backprop been tried on these more complex models directly? If so, how did it work?",3,1
593,2015-12-24,2015,12,24,1,3xz74p,Browser-based 2D wheeled-vehicle evolution simulator using genetic algorithm,https://www.reddit.com/r/MachineLearning/comments/3xz74p/browserbased_2d_wheeledvehicle_evolution/,natemi,1450888314,,25,33
594,2015-12-24,2015,12,24,3,3xznel,get stuck in feature extraction on message,https://www.reddit.com/r/MachineLearning/comments/3xznel/get_stuck_in_feature_extraction_on_message/,xiaogea01,1450895424,"I get many different types of messagesnearly 1GB,but they have different length with no human labelshow to transform each message into a vector for model learning,could some one sharing some link for this step? I do not how to chose the number  of  dimensions for the vector :)",2,1
595,2015-12-24,2015,12,24,3,3xzoe1,Global Mean Pooling in DRL?,https://www.reddit.com/r/MachineLearning/comments/3xzoe1/global_mean_pooling_in_drl/,Refefer,1450895858,"One of the big take aways from MSRA's new deep residual learning paper was the complete lack of fully connected layers (outside of the softmax mapping).  However, unless I read over it, they didn't go into details as to motivating factors, especially since they've long used spatial pooling layers in their networks.

My only hunch is that it has to do with making segmenting easier ala the Segmentation For Free paper.  Anyone have any additional insight?",1,1
596,2015-12-24,2015,12,24,3,3xzqva,Wearables Gesture recognition,https://www.reddit.com/r/MachineLearning/comments/3xzqva/wearables_gesture_recognition/,SYCarrot,1450896927,"Hello all,

I'm on my last year before graduation and my thesis involves **inferring gestures with the wearables' accelerometer**. What is a good resource to learn/use to be able to infer direction of movement of a smartwatch?

I do not have previous knowledge about machine learning, but I am willing to learn. I have briefly used AzureML which doesn't really help to understand how machine learning works other than the pipeline.",4,3
597,2015-12-24,2015,12,24,3,3xzr3d,UK Royal Society wants to hear your thoughts on machine learning,https://www.reddit.com/r/MachineLearning/comments/3xzr3d/uk_royal_society_wants_to_hear_your_thoughts_on/,shauert,1450897023,,0,1
598,2015-12-24,2015,12,24,7,3y0m3t,"Recognizing Hand/Arm-Gestures in 3D space, how do i get started?",https://www.reddit.com/r/MachineLearning/comments/3y0m3t/recognizing_handarmgestures_in_3d_space_how_do_i/,learnjava,1450911099,"As a more or less complete beginner in the ML field I have no idea how to get started searching for information on the topic of gesture recognition with the focus being on 3D space.

A video showing 3D gestures in a new system from BMW can be found [here](https://www.youtube.com/watch?v=NqyCGTzTKio)

What i found so far seems to indicate that RNNs would be a good place to start with this because of their capability for sequence detection.

As a long term goal i would like to work with more ""dynamic"" gestures, reacting programmatically *while* the gesture is being executed vs. having an on/off-binary result if a gesture was recognized or not *after* it has been made.

I assume the reaction could either be made outside the ML part in normal code, e.g. 

    if (gestureRecognizedFromNetwork) 
    do checkProgressOfGesture
    then do somethingWithProgress(progressIn%)

Are RNNs still useful for this? I'm not sure how i would handle this with sequences and it sounds like a lot of one-on-one case based scripting would be necessary to get it to work the way i want.

Another way i imagine it to work would be to ""bind"" the progress logic directly to the network output which could then control it.

After reading a bit this sounds like it could use reinforcement learning, letting the system control the output and rewarding/penalizing it based on the correctness of the action it takes.

Any thoughts on that? I have lots of time to learn more about this.

Links/papers or keywords for further investigation would be really helpful.
",1,0
599,2015-12-24,2015,12,24,10,3y13c3,How to trick a neural network into thinking a panda is a vulture,https://www.reddit.com/r/MachineLearning/comments/3y13c3/how_to_trick_a_neural_network_into_thinking_a/,bork,1450919581,,9,6
600,2015-12-24,2015,12,24,10,3y16d1,How does training an auto-encoder using RBM with random Z work?,https://www.reddit.com/r/MachineLearning/comments/3y16d1/how_does_training_an_autoencoder_using_rbm_with/,[deleted],1450921168,[deleted],5,1
601,2015-12-24,2015,12,24,10,3y16pa,I trained a Deep Q Network built in TensorFlow to play Atari Pong. These are the results after 25 hours of training (link to github in video description).,https://www.reddit.com/r/MachineLearning/comments/3y16pa/i_trained_a_deep_q_network_built_in_tensorflow_to/,Aks95,1450921354,,105,266
602,2015-12-24,2015,12,24,10,3y16vh,(How) can I do simultaneous one hot encoding in R?,https://www.reddit.com/r/MachineLearning/comments/3y16vh/how_can_i_do_simultaneous_one_hot_encoding_in_r/,ManDrone,1450921450,"Say I have 5 variables, each with 3 levels. I need to one-hot encode each one (V11, V12, V13; V21,V22,V23...V51, V52, V53). I am familiar with the model.matrix method for one-hot encoding a single variable but was wondering about a way that converts each variable into its one-hot set separately and then merges all the data together into a single matrix for output at the end. ",2,0
603,2015-12-24,2015,12,24,10,3y17if,Plotting decision boundary with more than 3 features?,https://www.reddit.com/r/MachineLearning/comments/3y17if/plotting_decision_boundary_with_more_than_3/,Gay_Hat_On_Nun,1450921778,"I am using logistic regression and I have a data set of 1000 instances with 80 features a piece and a 1 or a 0. The 1s and 0s can be separated by different colors, but how would I place 1000 points on a graph and show all 80 features to visualize the decision boundary?

Thanks.",8,5
604,2015-12-24,2015,12,24,10,3y18jx,I'm experimenting with predicting number of comments on reddit self posts -- feature ideas?,https://www.reddit.com/r/MachineLearning/comments/3y18jx/im_experimenting_with_predicting_number_of/,[deleted],1450922366,[deleted],2,1
605,2015-12-24,2015,12,24,16,3y28wy,Demystifying Deep Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/3y28wy/demystifying_deep_reinforcement_learning/,downtownslim,1450943223,,0,44
606,2015-12-24,2015,12,24,18,3y2fgc,Global Charging Pile Sales 2015 Market Research Report,https://www.reddit.com/r/MachineLearning/comments/3y2fgc/global_charging_pile_sales_2015_market_research/,naincyjorge,1450948700,,0,1
607,2015-12-24,2015,12,24,20,3y2n87,Backpropagation from scratch,https://www.reddit.com/r/MachineLearning/comments/3y2n87/backpropagation_from_scratch/,int8blog,1450955517,"Hi Reddit, I managed to finish a post about backpropagation. I know it has been talked about too many times already. The purpose of this one is my education mainly. Anyways. I'll be happy though if you find it anyhow helpful (and not confusing!)

http://int8.io/backpropagation-from-scratch-in-julia-part-ii-derivation-and-implementation/",2,5
608,2015-12-24,2015,12,24,21,3y2t5h,Gradient decent in R?,https://www.reddit.com/r/MachineLearning/comments/3y2t5h/gradient_decent_in_r/,shakedzy,1450960885,"Hi all,
I'm switching to R from MatLab, and trying to translate functions I wrote.
I have my neural-network functions that use fminunc (or fmincg) for minimizing cost functions. Is there an equivalent function in R?
Thanks!",1,0
609,2015-12-24,2015,12,24,21,3y2ucj,ML to predict tennis results in Python,https://www.reddit.com/r/MachineLearning/comments/3y2ucj/ml_to_predict_tennis_results_in_python/,pescu23,1450961883,"Hi guys, I've found this very interesting final project from the Imperial College of London 

http://www.doc.ic.ac.uk/teaching/distinguished-projects/2015/m.sipko.pdf

I would be very interesting to replicate this project with Python but I don't understand a few things on it.

For example, how does he built the DataSet, for what I've seen in OnCourt they don't give that features so... Do I have to calculate first?

What do you think about it? I find the project very interesting. **Any help to write the code with python and pybrain will be highly appreciated.**",5,7
610,2015-12-25,2015,12,25,3,3y3ufh,"How do you estimate ""bias"" ?",https://www.reddit.com/r/MachineLearning/comments/3y3ufh/how_do_you_estimate_bias/,kickinurnuts,1450983337,"So, I built a classifier for a 0-1 problem. But now I'm supposed to estimate the bias. There are 2 types of bias to be estimated: additive, and multiplicative. Any ideas how to go about doing this for an arbitrary classifier?",0,1
611,2015-12-25,2015,12,25,4,3y3w5a,Machine Learning Chats,https://www.reddit.com/r/MachineLearning/comments/3y3w5a/machine_learning_chats/,Dwe_Nivek,1450984242,Hey sorry that this is no real discussion but I was wondering if there are any machine learning group chats (like slack.com or gitter.im)?,2,1
612,2015-12-25,2015,12,25,4,3y3z7r,How to find an internship,https://www.reddit.com/r/MachineLearning/comments/3y3z7r/how_to_find_an_internship/,zechariahc,1450985832,So I have no idea where to look or how to get an internship in Machine Learning as an undergrad.,12,0
613,2015-12-25,2015,12,25,4,3y41si,Are all of the Embedding layers in current frameworks broken?,https://www.reddit.com/r/MachineLearning/comments/3y41si/are_all_of_the_embedding_layers_in_current/,djc1000,1450987148,"I've been having a discussion with someone on here, and he's convinced that the embedding layers in keras, lasagne, and probably every framework, are broken with regularization. 

I don't think that can be right, but he's sure enough of himself, and his argument isn't bad, that now I'm nervous. 

His argument is that the major deep learning frameworks implement embeddings as dense matrices where the input is converted to one-hot format.  Applying l1 or l2 regularization to the output (or, apparently, to any subsequent layer), means a non-zero gradient will be calculated for all of the weights in the embedding matrix, including ones that weren't used.

Is he right?",16,3
614,2015-12-25,2015,12,25,5,3y4344,[Discuss] Synthetic datasets all the new rage now?,https://www.reddit.com/r/MachineLearning/comments/3y4344/discuss_synthetic_datasets_all_the_new_rage_now/,rantana,1450987794,"Has anyone noticed that a lot of papers coming out in the couple years only demonstrate results on synthetic (computer generated) datasets?

These include things like Atari, BABI, Chairs, etc. As someone who works on applying these things to real data, I worry that these papers might be creating the illusion of progress. Real data is way more nuanced than synthetic. ",5,3
615,2015-12-25,2015,12,25,5,3y43h7,Benchmark results for clustering of images,https://www.reddit.com/r/MachineLearning/comments/3y43h7/benchmark_results_for_clustering_of_images/,s_daptardar,1450987991,"What are the state-of-the-art algorithms and features for clustering (unsupervised learning) on images as benchmarked on popular computer vision datasets like MNIST, Caltech-101, Caltech-256, CIFAR, ImageNet, COCO etc ?",0,0
616,2015-12-25,2015,12,25,5,3y44yr,Code for MSR Residual Networks,https://www.reddit.com/r/MachineLearning/comments/3y44yr/code_for_msr_residual_networks/,alexmlamb,1450988735,"Hello, 

Does anyone have code for this, preferably in Theano?  

For fully connected layers it's fairly trivial, but I could see multiple ways to do what he's suggesting with convolutional layers.  

For example, I'm wondering if just doing mean-pooling on the input layer and adding it to the output from the convolution suffices.  ",2,1
617,2015-12-25,2015,12,25,8,3y4ols,Python for Linear Regression on Mac OSX,https://www.reddit.com/r/MachineLearning/comments/3y4ols/python_for_linear_regression_on_mac_osx/,snazrul,1450999476,https://www.youtube.com/watch?v=dSYJVbj4Eew,0,0
618,2015-12-25,2015,12,25,10,3y4zai,AMA: Nando de Freitas,https://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/,nandodefreitas,1451006049,"I am a scientist at Google DeepMind and a [professor at Oxford University](https://www.cs.ox.ac.uk/people/nando.defreitas/).

One day I woke up very hungry after having experienced vivid visual dreams of delicious food. This is when I realised there was hope in understanding intelligence, thinking, and perhaps even consciousness. The homunculus was gone. 

I believe in (i) innovation -- creating what was not there, and eventually seeing what was there all along, (ii) formalising intelligence in mathematical terms to relate it to computation, entropy and other ideas that form our understanding of the universe, (iii) engineering intelligent machines, (iv) using these machines to improve the lives of humans and save the environment that shaped who we are.

This holiday season, I'd like to engage with you and answer your questions -- The actual date will be December 26th, 2015, but I am creating this thread in advance so people can post questions ahead of time. ",266,268
619,2015-12-25,2015,12,25,11,3y56pf,How is progress in machine learning measured?,https://www.reddit.com/r/MachineLearning/comments/3y56pf/how_is_progress_in_machine_learning_measured/,Whitey_Knightey,1451011029,"With chess playing computers, you can measure their chess  rating to accurately track their progress. How do we track the progress of computers that use machine learning? I have used the ""human level"" performance of things like image classification as a metric before, but are there other metrics? 

Ray Kurzweil was able to accurately predict the approximate year computers would beat the best humans at chess by tracking their yearly improvements in chess ratings. 

https://www.cs.purdue.edu/homes/dgleich/publications/Gleich%202003%20-%20Machine%20Learning%20in%20Computer%20Chess.pdf",3,0
620,2015-12-25,2015,12,25,12,3y5ble,What are some of the top papers from 2014 - 2015?,https://www.reddit.com/r/MachineLearning/comments/3y5ble/what_are_some_of_the_top_papers_from_2014_2015/,dharma-kaya,1451014388,,5,12
621,2015-12-25,2015,12,25,15,3y5pme,RL Course by David Silver - Lecture 7: Policy Gradient Methods,https://www.reddit.com/r/MachineLearning/comments/3y5pme/rl_course_by_david_silver_lecture_7_policy/,captcompile,1451023937,,5,9
622,2015-12-25,2015,12,25,15,3y5t8t,What open source framework would this community be interested in seeing developed?,https://www.reddit.com/r/MachineLearning/comments/3y5t8t/what_open_source_framework_would_this_community/,ucsdbound15,1451026673,My colleagues and I are proponents of OSS and want to spend some of our time developing a framework in the likes of http://jmlr.csail.mit.edu/mloss/. Does this community have anything they would like to see in a framework? It will be in either Python or C++.,12,5
623,2015-12-25,2015,12,25,18,3y65dy,Undergraduate summer internships in machine learning?,https://www.reddit.com/r/MachineLearning/comments/3y65dy/undergraduate_summer_internships_in_machine/,neural_not,1451036804,"Does anyone have any recommendations of companies offering good machine learning internships? I am a junior with a major in applied mathematics, with significant ML experience, and I'm really trying to pursue the path of ML.",8,8
624,2015-12-25,2015,12,25,19,3y67fl,[Python users] How do you go about setting up your (clean) work environment with different ML frameworks?,https://www.reddit.com/r/MachineLearning/comments/3y67fl/python_users_how_do_you_go_about_setting_up_your/,swentso,1451038938,"I do have some problems sometimes with projects especially when I upgrade a package. Or with packages like *numpy*, that are shared through multiple frameworks, I find myself installing them in each *virtualenv*.

If you guys frameworks (blocks, pylearn, caffe ...) how do you arrange them?",9,3
625,2015-12-25,2015,12,25,20,3y6c8z,Improving machine learning with an old approach,https://www.reddit.com/r/MachineLearning/comments/3y6c8z/improving_machine_learning_with_an_old_approach/,lokator9,1451044053,,6,6
626,2015-12-25,2015,12,25,21,3y6f8t,Is there any technique to know in advance the amount of training examples you need to make deep learning get good performance?,https://www.reddit.com/r/MachineLearning/comments/3y6f8t/is_there_any_technique_to_know_in_advance_the/,hoaphumanoid,1451047071,"Deep learning has been a revolution recently and its success is related with the huge amount of data that we can currently manage and the generalization of the GPUs.

So here is the problem I'm facing. I know that deep neural nets have the best performance, there is no doubt about it. However, they have a good performance when the number of training examples is huge. If the number of training examples is low it is better to use a SVM or decision trees.

But what is huge? what is low? In a paper of face recognition (FaceNet by Google) they show the performance vs the flops (which can be related with the number of training examples). They used between 100M and 200M training examples, which is huge.

My question is: Is there any method to predict in advance the number of training examples I need to have a good performance in deep learning??? The reason I ask this is because it is a waste of time to manually classify a dataset if the performance is not going to be good.",2,3
627,2015-12-25,2015,12,25,23,3y6m8d,Data science with R Course Overview,https://www.reddit.com/r/MachineLearning/comments/3y6m8d/data_science_with_r_course_overview/,padmajatamada,1451052900,,0,0
628,2015-12-26,2015,12,26,1,3y6wwf,Sequence to sequence backprop for really long sequences?,https://www.reddit.com/r/MachineLearning/comments/3y6wwf/sequence_to_sequence_backprop_for_really_long/,imh,1451060454,"I'm testing some sequence to sequence stuff on longer sequecnes, and I'm trying to figure out how to compute gradients for longer input ([standard setup](https://www.tensorflow.org/versions/master/images/basic_seq2seq.png)).

I can see how truncating backprop on the decoder will still learn, but how does that work for the encoder? Can anyone tell me how it works, or point me towards relevant papers? Thanks ahead of time!",1,13
629,2015-12-26,2015,12,26,2,3y74hg,Many to One LSTM Network,https://www.reddit.com/r/MachineLearning/comments/3y74hg/many_to_one_lstm_network/,ShoaibAhmed,1451064766,My task is human activity recognition using raw accelerometer readings. I would like to implement this many to one LSTM architecture for sequence classification in either Torch7 or TensorFlow. Can anyone provide me with a snippet in any of the above mentioned frameworks regarding how to implement this many to one LSTM architecture.,2,5
630,2015-12-26,2015,12,26,2,3y75vk,Deep Residual Learning for Image Recognition (paper+code ),https://www.reddit.com/r/MachineLearning/comments/3y75vk/deep_residual_learning_for_image_recognition/,[deleted],1451065546,[deleted],0,1
631,2015-12-26,2015,12,26,2,3y765n,Deep Residual Learning Lasagne implementation,https://www.reddit.com/r/MachineLearning/comments/3y765n/deep_residual_learning_lasagne_implementation/,samim23,1451065701,,0,26
632,2015-12-26,2015,12,26,3,3y7dso,What was your experience learning your favorite deeplearning framework?,https://www.reddit.com/r/MachineLearning/comments/3y7dso/what_was_your_experience_learning_your_favorite/,swentso,1451069896,"What was your experience learning your favorite deeplearning framework?

What steps did you follow? ( going through Tutorials (there is not much of them), reading the code, the doc ...etc) ",5,2
633,2015-12-26,2015,12,26,4,3y7jfm,Text Classification using Multinomial Naive Bayes,https://www.reddit.com/r/MachineLearning/comments/3y7jfm/text_classification_using_multinomial_naive_bayes/,akshayk0406,1451073002,"Hello Folks,

I am having trouble implementing text-classification of 20newsgroups data using multinomial naive bayes approach. Following is the approach I am taking:-

1) Divide Data into balanced train and test-set.

2) for each tuple(documentid,featureid) in training set I am computing tfidf score using only training documents.

3) For each tuple(documentid,featureid) in test-set I am computing tfidf score. For idf i am using Idf score from training set and feature is not present in training I am using log(N) where N is number of documents in training set as my idf score.

4) Term-frequency is normalized.

5) I have computed sum of tfidf score for each class and also computed tfidf score of feature grouped by class.

Now I am confused how to do classification, how do i compute P(X|C).

P(X|C) = P(X1|C)*P(X2|C)*.....*P(Xn|C)
what should be P(X1|C) ? should it be sum of tfidf scores for feature in X1 for class C? If this is the case, then I am ignoring tfidf score for X1 in the current document ?

Can someone please tell me where I am wrong ?

Thanks",2,1
634,2015-12-26,2015,12,26,6,3y7wn5,Applied math course for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3y7wn5/applied_math_course_for_machine_learning/,ph3rn,1451079800,"Would this course, called [Methods of Applied Mathematics](http://imgur.com/xXsQrlU) be useful for Machine Learning in any sense? I am a CS/math double major freshman and  I'm going through math electives to see which ones I should take that would be useful to ML. ",10,2
635,2015-12-26,2015,12,26,7,3y81sc,Investing In Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/3y81sc/investing_in_artificial_intelligence/,nb410,1451082538,,6,10
636,2015-12-26,2015,12,26,7,3y8439,What i need to know from linear algebra to start with ML?,https://www.reddit.com/r/MachineLearning/comments/3y8439/what_i_need_to_know_from_linear_algebra_to_start/,glatjas,1451083758,"which topics from here i need to know ?
https://www.khanacademy.org/math/linear-algebra

Should i learn all subtopics from this link like for example 

""Null space 2: Calculating the null space of a matrix""
""Null space 3: Relation to linear independence""
etc? 

Or only some of those topics are important in ML ?
",6,6
637,2015-12-26,2015,12,26,7,3y84hr,How does Adam compare to Adadelta?,https://www.reddit.com/r/MachineLearning/comments/3y84hr/how_does_adam_compare_to_adadelta/,fabhan,1451083985,"What are their major differences? In your experience, which one is more effective and any intuition why?

Adadelta had been around for 2 years and was hugely popular when Adam was published. It's weird that Adam's authors didn't include Adadelta in their comparisons.",9,11
638,2015-12-26,2015,12,26,11,3y8qr6,Have you ever tried Eureqa ?,https://www.reddit.com/r/MachineLearning/comments/3y8qr6/have_you_ever_tried_eureqa/,PyBet,1451096625,"I found out about Eureqa from Morgan Freeman's show ""Through the Wormhole"", a few years ago.

It takes a lot more time than something like scikit-learn so I have tried it only a few times.

Have you used it ? Can you share what did you use it for ?",1,6
639,2015-12-26,2015,12,26,11,3y8s01,Good open source projects to get involved in?,https://www.reddit.com/r/MachineLearning/comments/3y8s01/good_open_source_projects_to_get_involved_in/,asymptotics,1451097298,"I'm interested in contributing to open source ML projects.  My theoretical background is a lot stronger than my (non-R) programming experience so far, but I'm comfortable in Python and fairly comfortable (but looking to improve!) in C++.

I've worked on R packages for computational statistics but I'm looking to expand to more general purpose programming languages.",6,22
640,2015-12-26,2015,12,26,14,3y97fj,Do we always have a goal?,https://www.reddit.com/r/MachineLearning/comments/3y97fj/do_we_always_have_a_goal/,blackcapcoder,1451106035,"I'm not sure if this is the right sub, but do we always have a goal?
This could be as simple as becoming entertained or actually finding a goal (boredom?), but there is never a moment where we do not have a goal, right?",3,0
641,2015-12-26,2015,12,26,14,3y9d2l,What is the best books to learn about ml?,https://www.reddit.com/r/MachineLearning/comments/3y9d2l/what_is_the_best_books_to_learn_about_ml/,[deleted],1451109481,[deleted],0,1
642,2015-12-26,2015,12,26,15,3y9fga,How can I avoid osill,https://www.reddit.com/r/MachineLearning/comments/3y9fga/how_can_i_avoid_osill/,[deleted],1451110956,[deleted],0,1
643,2015-12-26,2015,12,26,15,3y9gkj,How can I avoid oscillations in gradient descent?,https://www.reddit.com/r/MachineLearning/comments/3y9gkj/how_can_i_avoid_oscillations_in_gradient_descent/,Qanari,1451111722,"I have a vector field `V = [Vx, Vy]` which has a global minimum at [;p=(x_g, y_g) ;], where `[;||V(p)||} \approx 0;]`. I am using gradient descent to move toward this global minimum like so:

`[; q_{n+1} = q_{n} - V(q_{n}) * h ;]`

But some times I get stuck in some oscillations like [this](http://imgur.com/0vqqVUc), which due to the problem I am working with are far from that global minimum. Is there a simple way to avoid this problem?

I don't have an explicit formula for the vector field. I am calculating the gradient numerically at each point. 

Please let me know if this is not the right sub.",9,3
644,2015-12-26,2015,12,26,21,3ya3qb,"I just finished an introductory course in machine learning. I want to learn more, what's something I could try over the break.",https://www.reddit.com/r/MachineLearning/comments/3ya3qb/i_just_finished_an_introductory_course_in_machine/,amirhd91,1451132385,"Hi everyone,

So last semester I took an introductory course in machine learning in my university, and I enjoyed it a lot.
I was wondering what would be an interesting next step for me to try over the winter break. Should I take an online course or read a book or research papers, or is it better for me to try finish a small project, and if so how to find such a project. (I find websites like Kaggle confusing for beginners.)",5,6
645,2015-12-26,2015,12,26,21,3ya6fy,Training LSTM to recognize sequence,https://www.reddit.com/r/MachineLearning/comments/3ya6fy/training_lstm_to_recognize_sequence/,michal_sustr,1451134597,"I'm trying to train LSTM to recognize there has been occurence of a certain pattern in a time-series. 

https://github.com/michalsustr/DeepLearning/blob/master/lstm_genseq.py

Basically, there is an occurence of a value 100 that drops down to 1. When it drops, I would like to output ""1"", otherwise ""0"". 

Illustrative figure: https://github.com/michalsustr/DeepLearning/blob/master/inout.png
with a zoom in
https://github.com/michalsustr/DeepLearning/blob/master/inoutzoom.png

However, training yields only ""1"" for the entire time series. Any thoughts on how to set the code up?

Thanks.",11,2
646,2015-12-26,2015,12,26,22,3ya7o7,"Machine learning frees up data scientists' time, simplifies smart applications",https://www.reddit.com/r/MachineLearning/comments/3ya7o7/machine_learning_frees_up_data_scientists_time/,[deleted],1451135639,[deleted],0,1
647,2015-12-26,2015,12,26,22,3ya8iw,Analyzing networks of characters in 'Love Actually',https://www.reddit.com/r/MachineLearning/comments/3ya8iw/analyzing_networks_of_characters_in_love_actually/,lokator9,1451136322,,9,56
648,2015-12-26,2015,12,26,23,3yac4r,Using Data to Look at Legislative Leaders - Follow the Leader [OC],https://www.reddit.com/r/MachineLearning/comments/3yac4r/using_data_to_look_at_legislative_leaders_follow/,DontVoteForMe,1451138900,,0,2
649,2015-12-27,2015,12,27,0,3yampp,Fuzzy Speciation in Genetic Algorithms using k-d Trees,https://www.reddit.com/r/MachineLearning/comments/3yampp/fuzzy_speciation_in_genetic_algorithms_using_kd/,sachinrjoglekar,1451145381,,0,6
650,2015-12-27,2015,12,27,2,3yatzr,using machine learning to talk to dead people,https://www.reddit.com/r/MachineLearning/comments/3yatzr/using_machine_learning_to_talk_to_dead_people/,toisanji,1451149242,,15,9
651,2015-12-27,2015,12,27,2,3yazla,Is it common to use LSTM for reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/3yazla/is_it_common_to_use_lstm_for_reinforcement/,zibenmoka,1451152054,"Hi there, 

First of all my question is a question of a beginner. I have some knowledge of LSTM and very basic knowledge of RL. Please then treat this question as a need for a guidance rather than arrogance coming from lack of knowledge. 



Anyways, I wonder if people use LSTM for reinforcement learning. I can imagine environment state to be input, with action as output. Whenever action is chosen it is executed and reward is calculated. Our error function then could depend on that reward. Instead of modeling error itself one could model its derivative - that would indicate a need for parameter change.",9,3
652,2015-12-27,2015,12,27,3,3yb72x,Doubt in Adam's update rule,https://www.reddit.com/r/MachineLearning/comments/3yb72x/doubt_in_adams_update_rule/,[deleted],1451155572,[deleted],0,1
653,2015-12-27,2015,12,27,5,3ybj4d,[1511.05644] Adversarial Autoencoders,https://www.reddit.com/r/MachineLearning/comments/3ybj4d/151105644_adversarial_autoencoders/,__ishaan,1451161412,,18,12
654,2015-12-27,2015,12,27,10,3ycksu,Sequential context/question/answer dataset,https://www.reddit.com/r/MachineLearning/comments/3ycksu/sequential_contextquestionanswer_dataset/,[deleted],1451180115,[deleted],0,1
655,2015-12-27,2015,12,27,10,3ycn4g,Machine Intelligence In The Real World,https://www.reddit.com/r/MachineLearning/comments/3ycn4g/machine_intelligence_in_the_real_world/,j_juggernaut,1451181307,,0,8
656,2015-12-27,2015,12,27,10,3ycn5b,Does anybody know of any sequential context/question/answer datasets?,https://www.reddit.com/r/MachineLearning/comments/3ycn5b/does_anybody_know_of_any_sequential/,butWhoWasBee,1451181321,"I've seen several datasets that use a context/question/answer format. Sometimes the context is text and sometimes it is an image.

Example: 

context : sally went to the mall 

question: where is sally? 

answer: mall


However, each trio of context/question/answer seems to be independent, whether the context is an image or text. Are there any datasets like this but data is arranged in order, so that the AI would have to remember context/images from previous questions in order to correctly answer it?
Example of what I am looking for:

context : today is 90 degrees 

question: is today hot or cold? 

answer: hot


context : yesterday was 80 degrees 

question: is yesterday warmer than today? 

answer: no",2,1
657,2015-12-27,2015,12,27,11,3yctfq,What do deep architectures actually learn?,https://www.reddit.com/r/MachineLearning/comments/3yctfq/what_do_deep_architectures_actually_learn/,burn_heart,1451184611,"What work has been done on **scientifically** studying what deep architectures - and specifically deep convolutional networks with pooling layers such as LeNet - actually learn when trained on various images? When trained on 'natural' images, in the first layer or two they *seem* to learn lines and edges, but what they actually learn in the higher levels seems opaque to me. I've seen reconstructions showing, say, faces and animals and whatever emerging from random noise but those seem to be merely 'maximum likelihood' estimates.",4,0
658,2015-12-27,2015,12,27,13,3yd1zc,What are some good systematic and scientific approaches for documenting ML model development?,https://www.reddit.com/r/MachineLearning/comments/3yd1zc/what_are_some_good_systematic_and_scientific/,sleepicat,1451189161,"I'm curious to know how people track ML model (or algorithm) development, assuming you don't want to repeat prior mistakes during development and eventually want to write it up and present it to others.  

Do you keep a notebook (paper/Word/LaTeX)?  

Do you save many versions of your code (by date, by significant number of changes)?  
",9,9
659,2015-12-27,2015,12,27,18,3ydvj2,"[Quick Question] Does Oxford, IDSIA etc take bachelors degree holders in for PhD programmes?",https://www.reddit.com/r/MachineLearning/comments/3ydvj2/quick_question_does_oxford_idsia_etc_take/,ShortLongTermMemory,1451209843,"Hello All,

Hope you are enjoying holidays.

A friend told me that he read on this community that many universities would take in students with just Bachelors for PhD programs. I looked up and found that it the answer was only about US universities. I tried looking up on some of the current students, but couldn't find anything concrete.

Does anyone know if any European institutes, particularly if IDSIA does this as well. ( I have mailed them and haven't gotten a reply yet and in their website, I mostly found Post Docs :)

Also, great community here - learnt a couple of things in my very first visit.

Thanks in Advance!",6,2
660,2015-12-27,2015,12,27,19,3ydy8s,Text classification with an imbalanced dataset,https://www.reddit.com/r/MachineLearning/comments/3ydy8s/text_classification_with_an_imbalanced_dataset/,sknepal,1451212496,"Hi, I'm a beginner trying my luck on classifying social media posts. What I am planning to do is classify the posts on different Facebook Groups (related to the local developer communities of PHP, Python etc.) into Tech events (meetup, workshops etc) and Job announcements.

I have used facepy and grabbed recent posts from a list of groups. I then manually labeled some of the posts into three labels: 0, 1 (tech event) and 2 (job). The number of 0 labeled posts is always way higher than the other two combined. I have an imbalanced dataset. Can somebody please tell me if it will cause any problem, and how should I approach it? Thank you.",1,2
661,2015-12-27,2015,12,27,20,3ydzyl,[AMA Request] David Silver,https://www.reddit.com/r/MachineLearning/comments/3ydzyl/ama_request_david_silver/,true-randomness,1451214192,,0,68
662,2015-12-27,2015,12,27,21,3ye4ev,Does autoencoder suitable for missing data imputation?,https://www.reddit.com/r/MachineLearning/comments/3ye4ev/does_autoencoder_suitable_for_missing_data/,[deleted],1451218326,[deleted],0,1
663,2015-12-27,2015,12,27,22,3ye8lb,"Announcing /r/datascienceblogs, a place for medium-sized, non-institutionally-funded data science ideas of a more casual, not causal, nature.",https://www.reddit.com/r/MachineLearning/comments/3ye8lb/announcing_rdatascienceblogs_a_place_for/,warisaracket1,1451221662,,1,5
664,2015-12-27,2015,12,27,22,3ye8oi,[AMA Request] Ilya Sutskever,https://www.reddit.com/r/MachineLearning/comments/3ye8oi/ama_request_ilya_sutskever/,brylevkirill,1451221744,,9,57
665,2015-12-27,2015,12,27,22,3yeb15,Do you have any datasets to share with the world?,https://www.reddit.com/r/MachineLearning/comments/3yeb15/do_you_have_any_datasets_to_share_with_the_world/,mrborgen86,1451223509,"I'm building an open source platform for sharing machine learning datasets, as I'd like to encourage more people and businesses to share more of their data, plus make it easier to browse through datasets and get a quick sense of their qualities.
Which leads to my question:
Do you have any datasets you'd be willing to share? If so, I would be very happy to feature them on the front page of Datasets.co.
Site: http://www.datasets.co/
Github repo: https://github.com/perborgen/data_hub
All other feedback is highly appreciated :)",2,4
666,2015-12-27,2015,12,27,23,3yeepn,Does autoencoder have advantages in data imputation?,https://www.reddit.com/r/MachineLearning/comments/3yeepn/does_autoencoder_have_advantages_in_data/,veinpy,1451226336,"First of all, my question is a in a beginner's point. And I hope get some instruction about autoencoder because I recently study the usage of autoencoder in missing data imputation subject.
As I know, the autoencoder take the MLP's advantages which could regress any non-linear function. And the AE utilize all of other features to prediction the missing one.
So I was wonder whether the performance of AE is outperformed with other NA fill methods, like zero/mean substitution, K-nearest neighbor, logistic regression? 
How to measure the performance? Is putting auto-encoder and other methods into some classification problem a good choice?
I will update here about my experiment of  prediction accuracy of missing data as well as classification accuracy.",6,0
667,2015-12-28,2015,12,28,2,3yewee,Stuck at implementing conjugate gradient descent,https://www.reddit.com/r/MachineLearning/comments/3yewee/stuck_at_implementing_conjugate_gradient_descent/,Qanari,1451236228,"I am trying to implement conjugate gradient descent numerically but I'm stuck. The update rule in normal gradient descent algorithm is: 

[; x_{n+1} = x_n + \alpha \nabla f(x_n) ;]

My problem is that I don't have the explicit defention of function f or its gradient, but I do have a table where I can look up the value of its gradient at any point. I'm also using some fixed [; \alpha ;] here.

Now that I'm trying to implement the conjugate gradient descent I'm stuck and don't know what I should do. As mentioned on the Wikipedia article this algorithm can be used to solve a system of linear equations Ax = b. But I don't have a matrix A or vector b. I just have a numeric value of the gradient of a function at a point. So, when in the algorithm I have to update the coefficients alpha and beta I don't know how I should proceed because I don't have the matrix A. 

I also have the same problem in normal gradient descent method to have a dynamic alpha. 

What is it that I don't understand here?",2,0
668,2015-12-28,2015,12,28,2,3yf2cc,Which of these math electives would be useful for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3yf2cc/which_of_these_math_electives_would_be_useful_for/,ph3rn,1451239069,[removed],2,1
669,2015-12-28,2015,12,28,4,3yfgvr,ACT-R is a cognitive architecture: a theory for simulating and understanding human cognition.,https://www.reddit.com/r/MachineLearning/comments/3yfgvr/actr_is_a_cognitive_architecture_a_theory_for/,[deleted],1451245802,[deleted],1,0
670,2015-12-28,2015,12,28,4,3yfhg6,Neural Net using rectifier only fit a piecewise linear function from which the output activation function is computed :O,https://www.reddit.com/r/MachineLearning/comments/3yfhg6/neural_net_using_rectifier_only_fit_a_piecewise/,Setepenre,1451246041,"Well, if you think about it.

A rectifier (max(xw + b, 0)) is an hidden 'if' statement which select which linear function should be applied.
Even if you have multiple hidden layers those are only linear combination of piecewise linear function.
At the end of the day the estimated function is piecewise linear.

which makes a neural net, once trained, a simple piecewise linear function.

i.e if I am not mistaken, nnet are non linear in function of the parameter theta 
but piecewise linear in function of the input x.

Considered model:
     
      # hidden layer
      a0 = x
      a{n+1} = w{n} * a{n} + b{n}
      z{n+1} = max(a{n+1}, 0)
      
      # output activation function
      z{d} = a{d}

      # cost function
      (z{d} - y) ^ 2
     ",12,0
671,2015-12-28,2015,12,28,4,3yfhq9,What ML papers have been major influences in your academic writing style?,https://www.reddit.com/r/MachineLearning/comments/3yfhq9/what_ml_papers_have_been_major_influences_in_your/,onewugtwowugs,1451246155,"Sometimes we like a paper due to its content, sometimes due to its style. Or in the best of times, both. For me, a good writing style makes reading a joy and almost effortless, while a badly structured paper can make even the easiest of topics difficult to parse. 

What ML papers have helped you achieve your current style of writing?",4,11
672,2015-12-28,2015,12,28,6,3yfvd6,CNN increasing array rank doubt,https://www.reddit.com/r/MachineLearning/comments/3yfvd6/cnn_increasing_array_rank_doubt/,MarcoROG-SG,1451251916,"Hello!
I'm trying to implement a convolutional neural network in Python using numpy and scipy and i came across an issue.
The first time i apply convolution on a single image, i start with a rank-2 array(the image widthxheight) and a rank 3 array(the weights: weightNo x width x height). By applying 2D convolution i get a rank-3 array : width, height and filter number.

Having this as an output, i then pass it through a pooling layer, while the size of the dimesions decreases, the array is still rank-3.

When i apply convolution again in another layer, i start with a rank-3 array input (the same as before) and a rank-3 array weights.
So this would result in a rank-4 array because each new filter is applied to each of the old filters.

By stacking many convolutional layers one after another, the rank of the array continues increasing. Is this fine or it is an issue?
Is there any fix to this?

I tought of using rank-4 filters (width,height, depth and filter number), this way i'd have a costant rank in all the network.
Is this an effective fix? Any tip on how to implement a 3d convolution?",9,2
673,2015-12-28,2015,12,28,7,3yg1k8,"get the max arg of a [X,Y,32] tensor into [32,2]?",https://www.reddit.com/r/MachineLearning/comments/3yg1k8/get_the_max_arg_of_a_xy32_tensor_into_322/,hapliniste,1451254617,"Hello! 

I'm a beginner in ML and am currently trying to do some DRL in tensorflow (to play simple games). The model I'm using right now is multiple CNN followed by fully connected layers, but I think this is very inefficient (because I don't want max-pooling that would cause position information to be lost).

I was thinking about transforming the output of a convolutional layer [20x20x32] (20x20xkernelSize) into another tensor of shape [32x2] (KernelSizex2, for having 2 values, X and Y, linked to each kernel).

I just don't know how to do it :(

I'm using Tensorflow and found the OP tf.nn.max_pool_with_argmax() wich seem to do something similar to what I want, but it put it into a single value (so the X and Y value are not distinct).

Does anyone have some insight for me on how's the best way to do it?

TLDR: I want a pooling operation that transform [X,Y,K] into [K,2]

([[K1maxArgX,K1maxArgY],[K2maxArgX,K2maxArgY],...]",1,1
674,2015-12-28,2015,12,28,7,3yg3be,What's next after CS 229?,https://www.reddit.com/r/MachineLearning/comments/3yg3be/whats_next_after_cs_229/,MusicIsLife1995,1451255380,"I finished the grad class http://cs229.stanford.edu/

What should I focus on now to improve my Machine Learning skills? I also have about 4 or 5 ML projects under my belt, which is awesome. ",5,2
675,2015-12-28,2015,12,28,7,3yg4bz,Learning ML,https://www.reddit.com/r/MachineLearning/comments/3yg4bz/learning_ml/,apexpredator457,1451255785,"So, i'm interested into learning ML and i started a course on coursera, but it's extremely boring to study by myself and i have no idea when i will have enough knowledge to start on a project. There is no ML course offered in my undergrad university so i'm pretty much sure that ""if"" i do something related to ML in my final project i will be the only one :). Anyway i found a project about ML in one of my teachers webpage (ML is her research field) so i was wondering how much should i know before i start that project (just for fun). 
Thanks for your time. 
(sorry if it's too long)

 Learning how to Translate Locative Preposition from English into French from Large Text Corpora

Machine translation of locative prepositions is not straightforward, even between closely related languages. For example, while English speakers will say: ""The boy is ON the bus"", French speakers will say: ``Le garcon est DANS le bus"" (Literally, ""The boy is IN the bus""). Similarly, English speakers will say: ""The picture is ON the wall"" while French Speakers will say ""Le tableau est AU mur"" (Literally, ""The picture is AT the wall""). The purpose of this project is to use statistical techniques and Reference tools such as Dictionnaries, Thesauri, or WordNet to browse through large corpora of bilingual texts and attempt to learn translation rules that will associate the correct French translation to the English locative preposition used in a given context. 

Since this project can be managed in two phases (a French and an English phase) separately, although preferable, knowledge of both English and French is not required. After learning the various text processing tools, the student will be given the freedom to choose the direction for his/her project that interests him/her most. Alternatively, s/he will be given more specific guidance.
  ",4,1
676,2015-12-28,2015,12,28,8,3ygbuv,Ilya Sutskever presenting Neural GPU at NIPS 2015,https://www.reddit.com/r/MachineLearning/comments/3ygbuv/ilya_sutskever_presenting_neural_gpu_at_nips_2015/,jordn,1451259153,,2,80
677,2015-12-28,2015,12,28,10,3ygnyg,Machine learning on biological data.,https://www.reddit.com/r/MachineLearning/comments/3ygnyg/machine_learning_on_biological_data/,bionerd2,1451264691,"So, I've taken CS229 (the full course, not coursera, at Stanford), as well as looked through Learning from data (caltech MOOC) and I'm wondering what would be some good tools/datasets/other MOOCs/books to get an idea of what the real questions/applications/problems are at the intersection of ML and biological/clinical data. I've heard that genomics is an area of interest. I'm really interested -- could anyone point me to some good resources?",6,5
678,2015-12-28,2015,12,28,13,3yhfhs,Any good python code for augmenting image training data?,https://www.reddit.com/r/MachineLearning/comments/3yhfhs/any_good_python_code_for_augmenting_image/,EdwardRaff,1451277858,"As the title asks, for training neural networks on image based datasets - is anyone aware of some good easy to use code for generating lots of noisy variants of an image? ",5,12
679,2015-12-28,2015,12,28,13,3yhft8,What/where should I do/go from an intro to ML class?,https://www.reddit.com/r/MachineLearning/comments/3yhft8/whatwhere_should_i_dogo_from_an_intro_to_ml_class/,asljkdfhg,1451278023,"I've finished an intro to ML class this past semester, and I was hoping I could build some kind of small project or work on something over this winter break. I have about 3-4 weeks, but I don't know what to try out. My only exposure to ML was this class, which is based on the ciml textbook. We've gone over only some of the basics until unsupervised learning, so I only know things like dts, perceptron, knn, kmeans++, ova/ava, pca, kernels, neural networks, and gradient descent. I tried looking into open source projects but I've never worked on any before, and I have no idea what I'm looking at when I'm checking out the code on github. I looked into HackerRank's AI domain as well, since I practice algos with it, but its machine learning stuff is largely statistics based, and even though I know a bit, I don't know if I know enough to work on that. What are some of the stuff I can do with my level of knowledge in ML?",6,3
680,2015-12-28,2015,12,28,14,3yhhnv,Having trouble getting mnist training data on a single hidden layer neural network to generalize to handwriting.,https://www.reddit.com/r/MachineLearning/comments/3yhhnv/having_trouble_getting_mnist_training_data_on_a/,motioncuty,1451278970,"I am reaching about 88% correct on training and testing mnist data but I am having significant failures when I draw my own 28 x 28 digits. Would anyone know good routes for improving generalization to help classify these self made digits. I am thinking scale, skew, centering and line thickness are making it hard for the network to classify correctly.",6,2
681,2015-12-28,2015,12,28,14,3yhjry,TensorFlow: a new LSTM RNN based Morse decoder experiment,https://www.reddit.com/r/MachineLearning/comments/3yhjry/tensorflow_a_new_lstm_rnn_based_morse_decoder/,ag1le,1451280075,"A new LSTM RNN based Morse decoder was built using Google's TensorFlow open source library. This decoder learns Morse code from bit patterns and is able to translate noisy patterns directly to characters. Without any specific optimization the decoder achieved 96.3% accuracy with a training set of 114,000 samples.

For details see: 
http://ag1le.blogspot.com/2015/12/tensorflow-new-lstm-rnn-based-morse.html",4,9
682,2015-12-28,2015,12,28,19,3yiebb,Any experience with NVIDIA Jetson TK1/TX1?,https://www.reddit.com/r/MachineLearning/comments/3yiebb/any_experience_with_nvidia_jetson_tk1tx1/,egrefen,1451299846,"At NIPS this year, NVIDIA very kindly gave away some [Jetson TX1 Development Boards](http://www.nvidia.com/object/jetson-tx1-module.html) (and a lot of free booze) to attendees. It's a cool gift, but I'm exactly sure where to get started with it, and so I turn to our little community.

Has anyone here played around with a TX1, or the TK1 (which I understand is its predecessor)? What sort of projects have/would you run on it? Do you just let it sit on your desk naked, or do you have an enclosure for it (and if so, what sort)?

I hope people don't think this is off-topic for this sub, but since the device is pretty much designed for low-power machine learning, I assume it's vaguely relevant.",26,17
683,2015-12-28,2015,12,28,21,3yil7w,"Recurrent Net Dreams Up Fake Chinese Characters (SVG, TensorFlow)",https://www.reddit.com/r/MachineLearning/comments/3yil7w/recurrent_net_dreams_up_fake_chinese_characters/,hardmaru,1451305028,,20,110
684,2015-12-28,2015,12,28,21,3yio39,Some help with Facebook prediction,https://www.reddit.com/r/MachineLearning/comments/3yio39/some_help_with_facebook_prediction/,deurdemeur,1451307012,"Hi
For my master dissertation I am trying to predict the change whether a certain Facebook user will like a certain page. I am using SAS and I have a pretty large Facebook data set (with a lot of variables per user such as: statuses, groups, likes, books read, movies watched, demographical data, inspirational people, sports,tv shows, interests, education, activities,...)
I don't actually have an idea how to start doing this. Can anyone help me?
Many thanks in advance!",2,0
685,2015-12-28,2015,12,28,22,3yittl,"Continually updated Data Science IPython Notebooks: Deep learning, Spark, Hadoop MapReduce, Kaggle, scikit-learn, matplotlib, pandas, NumPy, AWS, Python essentials, and various command lines",https://www.reddit.com/r/MachineLearning/comments/3yittl/continually_updated_data_science_ipython/,donnemartin,1451310502,,12,122
686,2015-12-29,2015,12,29,0,3yj2jt,Everything you need to know about artificial neural networks,https://www.reddit.com/r/MachineLearning/comments/3yj2jt/everything_you_need_to_know_about_artificial/,joshdotai,1451315207,,2,0
687,2015-12-29,2015,12,29,0,3yj6fi,"DeepLearningKit  Open Source Deep Learning Framework for Apples iOS, OS X and tvOS",https://www.reddit.com/r/MachineLearning/comments/3yj6fi/deeplearningkit_open_source_deep_learning/,transhumanist_,1451317068,,2,12
688,2015-12-29,2015,12,29,0,3yj6i1,Which of the math courses on this list would be useful for Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/3yj6i1/which_of_the_math_courses_on_this_list_would_be/,ph3rn,1451317100,"[This](http://pdfsr.com/pdf/pages-1-.pdf) is the list of 300 and 400 level math courses offered at my uni. I am already taking Intro to Probability, Probability and Stochastic Processes, Intermediate Analysis and Numerical Analysis 1, as they are requirements for math majors. I was told I should pick courses that cover real analysis and convex optimization, but I don't see anything with the word 'convex' or 'real anaylsis' here. Is the Intermediate Analysis course here the same thing?",0,1
689,2015-12-29,2015,12,29,0,3yj77g,How to get started with machine learning?,https://www.reddit.com/r/MachineLearning/comments/3yj77g/how_to_get_started_with_machine_learning/,natemi,1451317423,"I'm a programmer who's super interested in machine learning but often feels at a loss to know how to break into the field.

Questions:

1. What do you feel helped you the most in going from interested to slightly-OK at using machine learning? 

2. What resources do you know about now that you think would make that process easier for you then had you known about them / had they existed?

3. What is your advice to someone new to the field on how to go from zero to competent with as little pain as possible? 

A little about me:

I'm a business-major web developer with a serious interest in machine learning who got through half of a basic online course on machine learning, loves learning by doing, and routinely plays around with python in my spare time.

Thanks!",2,2
690,2015-12-29,2015,12,29,1,3yjhku,Buried comment by Nando De Freitas about growing up under apartheid.,https://www.reddit.com/r/MachineLearning/comments/3yjhku/buried_comment_by_nando_de_freitas_about_growing/,SuperFX,1451321945,,0,29
691,2015-12-29,2015,12,29,3,3yjqkz,Questions about Grid LSTM paper,https://www.reddit.com/r/MachineLearning/comments/3yjqkz/questions_about_grid_lstm_paper/,leopd,1451325730,"I'm wondering if anybody here can help me understand the Grid LSTM paper: http://arxiv.org/pdf/1507.01526v2.pdf

In particular, section 4.4 describing a novel model for machine translation seems particularly important as an alternative to encoder-decoder networks, but is confusing to me.  It says the network ""shares weights across the source and target dimensions.""  But properly two of the three dimensions follow the target sentence -- I'm guessing the third dimension which the paper refers to as the ""hierarchy"" is not sharing weights with the other two, but I'd love to get confirmation on this.  Also, do I understand correctly that there's a softmax output with loss+supervision only on every other layer of the hierarchy -- the ones that are scanning the source sentence backwards?

Also, in the parity experiment (5.1) what does it mean to have a 1-LSTM with tied weights?  With only a single dimension, if the weights aren't tied, then it's not even an RNN any more, right?  Which would open up all sorts of ambiguity in the other experiments about what ""untied"" means.  I'm assuming that within a single dimension the weights are always shared, so I'm confused why it even says ""tied"" for a 1-LSTM.  I think that terminology is included to reinforce the similarity with the tied ReLU/tanh networks, but it's confusing.

This seems like important work, presenting a general framework for multi-dimensional networks that  perform quite well on a wide variety of tasks.  But IMHO it could use some cleanup to make it easier to understand.  I'm looking forwards to v3.  :)",5,2
692,2015-12-29,2015,12,29,3,3yjrex,Machine Learning Tools,https://www.reddit.com/r/MachineLearning/comments/3yjrex/machine_learning_tools/,PyBet,1451326088,,0,0
693,2015-12-29,2015,12,29,5,3ykber,Profile of Founders of Deepart.io: Agile for Academics,https://www.reddit.com/r/MachineLearning/comments/3ykber/profile_of_founders_of_deepartio_agile_for/,analyst_mike,1451334309,,0,0
694,2015-12-29,2015,12,29,6,3ykj83,What's your non-NN workflow like?,https://www.reddit.com/r/MachineLearning/comments/3ykj83/whats_your_nonnn_workflow_like/,asymptotics,1451337540,"Most people use one of the few popular libraries for deep learning, but I would guess that there's more variability in the tools people choose to use for more traditional tools like random forests or logistic regression.

I generally use R (I've tried scikit-learn but the syntax seems so verbose compared to R) and I've been waiting for more libraries to come out for Julia before I invest time into it. ",8,7
695,2015-12-29,2015,12,29,6,3yklpr,"On a lighter note, introducing /r/badML!",https://www.reddit.com/r/MachineLearning/comments/3yklpr/on_a_lighter_note_introducing_rbadml/,say_wot_again,1451338566,"For when you find discussions of machine learning or AI (either on Reddit or on the Internet as a whole) that fundamentally misunderstand what the point is, overhype the technology well beyond what's plausible in the foreseeable future, or introduce apocalyptic concerns that are extremely unwarranted. Let me know what you think of this idea!",1,1
696,2015-12-29,2015,12,29,12,3ylwyv,Toward a Research Agenda in Adversarial Reasoning: Computational Approaches to Anticipating the Opponent's Intent and Actions (DARPA),https://www.reddit.com/r/MachineLearning/comments/3ylwyv/toward_a_research_agenda_in_adversarial_reasoning/,InaneMembrane,1451359270,,2,5
697,2015-12-29,2015,12,29,13,3ym5h7,"10 years later, how do these various arguments (mostly in favor of SVMs and against neural nets) stand up?",https://www.reddit.com/r/MachineLearning/comments/3ym5h7/10_years_later_how_do_these_various_arguments/,sanity,1451363203,,24,78
698,2015-12-29,2015,12,29,13,3ym6dp,An unusually detailed description of how machine learning is used in online advertising,https://www.reddit.com/r/MachineLearning/comments/3ym6dp/an_unusually_detailed_description_of_how_machine/,sanity,1451363610,,0,5
699,2015-12-29,2015,12,29,14,3ymdlo,Will Tensorflow ever be available for Windows?,https://www.reddit.com/r/MachineLearning/comments/3ymdlo/will_tensorflow_ever_be_available_for_windows/,Whitey_Knightey,1451367034,,10,1
700,2015-12-29,2015,12,29,15,3ymkie,[ML Ignorant] See and say?,https://www.reddit.com/r/MachineLearning/comments/3ymkie/ml_ignorant_see_and_say/,LHoT10820,1451370506,"It seems to me like a neutral net trained in numeric string pattern recognition would still find great difficulty in recognizing the ""see and say"" pattern.

If you are unacquainted, see and say is simple. You say how many of each digit you see right to left, with two variations.

3 - 13 - 1113 - 3113 - 132113

3 - 13 - 1113 - 3113 - 2321

I did a quick search and couldn't find anything regarding the see and say pattern.

Rabbit hole: What resources are available to me to put the effort forth and calm my mind on my own?",1,1
701,2015-12-29,2015,12,29,15,3ymmca,What are some examples of learning algorithms that are embarrassingly parallelizable?,https://www.reddit.com/r/MachineLearning/comments/3ymmca/what_are_some_examples_of_learning_algorithms/,dharma-kaya,1451371460,(or near embarrassingly parallelizable). Random forests might be one good example.,19,6
702,2015-12-29,2015,12,29,16,3ymrec,[Question/Discussion] How do we backprop through Gaussian Distributions?,https://www.reddit.com/r/MachineLearning/comments/3ymrec/questiondiscussion_how_do_we_backprop_through/,[deleted],1451374416,[deleted],4,5
703,2015-12-29,2015,12,29,16,3ymscq,"Great, Simple Description of MCMC (Markov Chain Monte Carlo)",https://www.reddit.com/r/MachineLearning/comments/3ymscq/great_simple_description_of_mcmc_markov_chain/,blowjobtransistor,1451375071,,2,29
704,2015-12-29,2015,12,29,18,3yn2na,How Microsoft beat Google at understanding images with machine learning,https://www.reddit.com/r/MachineLearning/comments/3yn2na/how_microsoft_beat_google_at_understanding_images/,whitebeatle,1451382944,,26,87
705,2015-12-29,2015,12,29,20,3yn95k,PAPIs Connect 2016  Call for Proposals on track!,https://www.reddit.com/r/MachineLearning/comments/3yn95k/papis_connect_2016_call_for_proposals_on_track/,czuriaga,1451388167,,0,1
706,2015-12-29,2015,12,29,21,3yne9w,How do I interpret this error from Orange,https://www.reddit.com/r/MachineLearning/comments/3yne9w/how_do_i_interpret_this_error_from_orange/,fruitheart,1451392378,"I literally know no programming or machine learning, other than the concepts behind the main techniques and traditional regression stuff. 

All I did was connect a clean .csv dataset to a SVM icon and clicked on the output view in the taskbar.


C:\Python34\lib\site-packages\sklearn\svm\base.py:209: ConvergenceWarning: Solver terminated early 
(max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

C:\Python34\lib\site-packages\sklearn\svm\base.py:209: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)

C:\Python34\lib\site-packages\sklearn\svm\base.py:209: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.
  % self.max_iter, ConvergenceWarning)",3,0
707,2015-12-29,2015,12,29,21,3yng7t,Need to get started with Machine Learning. Where to start from? What else do I need to learn before starting with it?,https://www.reddit.com/r/MachineLearning/comments/3yng7t/need_to_get_started_with_machine_learning_where/,dumb-dodo,1451393825,,2,0
708,2015-12-29,2015,12,29,22,3ynikc,Question about reinforcement learning application in business,https://www.reddit.com/r/MachineLearning/comments/3ynikc/question_about_reinforcement_learning_application/,Bal_Thockeray,1451395479,"I've just started reading up on reinforcement learning and I am still trying to figure out its potential applications in business. I have thought about an example and if someone here could tell me whether I am on the right track, I will highly appreciate it. Here is my potential application:

On Twitter, brands tweet with the expectations that more people will retweet and ""favorite"" them. Once a brand observes the response to its tweet, which can be a reward, for the sake of simplicity, if many people retweeted or can be a penalty if only a few retweeted (it could be valence of replies as well). Accordingly, the brand will decide the next tweet and then observe the response from people. This will go on.

Can this situation be formulated as a reinforcement learning problem? Why or why not?",5,3
709,2015-12-30,2015,12,30,0,3ynu44,"What is the technology stack behind a self driving car? I keep hearing deep learning come up, can someone explain?",https://www.reddit.com/r/MachineLearning/comments/3ynu44/what_is_the_technology_stack_behind_a_self/,mmishu,1451402193,,16,15
710,2015-12-30,2015,12,30,0,3ynw44,"Search Engine Relevance, Computerphile",https://www.reddit.com/r/MachineLearning/comments/3ynw44/search_engine_relevance_computerphile/,[deleted],1451403202,[deleted],0,1
711,2015-12-30,2015,12,30,1,3yo3s5,"Position paper by Max Welling: ""Are ML and Statistics Complementary?"" [pdf]",https://www.reddit.com/r/MachineLearning/comments/3yo3s5/position_paper_by_max_welling_are_ml_and/,urish,1451406727,,15,27
712,2015-12-30,2015,12,30,2,3yod6c,How is the AI program at the university of Lugano ?,https://www.reddit.com/r/MachineLearning/comments/3yod6c/how_is_the_ai_program_at_the_university_of_lugano/,har777,1451410780,"I found the program on the homepage of Jrgen Schmidhuber and was wondering if anyone has any insight about how good the program is. Do students actually get a chance to do research at IDSIA ?

Link: http://people.idsia.ch/~juergen/aimaster.html",1,1
713,2015-12-30,2015,12,30,3,3yoi15,Highway Networks with TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3yoi15/highway_networks_with_tensorflow/,jimfleming,1451412896,,4,39
714,2015-12-30,2015,12,30,3,3yoiy3,How to run learning agents against PyGame,https://www.reddit.com/r/MachineLearning/comments/3yoiy3/how_to_run_learning_agents_against_pygame/,DanielSlater8,1451413300,,1,58
715,2015-12-30,2015,12,30,3,3yom5g,why is 'SVM' called 'SVM'?,https://www.reddit.com/r/MachineLearning/comments/3yom5g/why_is_svm_called_svm/,[deleted],1451414673,[deleted],12,0
716,2015-12-30,2015,12,30,4,3you3d,Here's What We Can Expect from Deep Learning in 2016 and Beyond (Roundtable Q&amp;A with 14 Experts),https://www.reddit.com/r/MachineLearning/comments/3you3d/heres_what_we_can_expect_from_deep_learning_in/,reworksophie,1451417960,,0,1
717,2015-12-30,2015,12,30,4,3yovfy,Online reading groups for advanced machine learning,https://www.reddit.com/r/MachineLearning/comments/3yovfy/online_reading_groups_for_advanced_machine/,centau1,1451418500,"Do you know any online reading groups for advanced machine learning? If not, are you willing to start one?",16,15
718,2015-12-30,2015,12,30,5,3yozr7,Use for (Yet) Another Theoretical Model of Thinking?,https://www.reddit.com/r/MachineLearning/comments/3yozr7/use_for_yet_another_theoretical_model_of_thinking/,canttouchmypingas,1451420298,"http://arxiv.org/abs/1511.02455

I like keeping up with what I'm interested in with machine learning by browsing some keywords in Google's scholarly articles. It leads me to some cutting edge stuff I can use in my own development. But I was curious about modeling emotion and I came across this. 

How would I use this for natural language processing? I can't even begin to think of an example to use this with, I work well off examples.",0,4
719,2015-12-30,2015,12,30,6,3yp5sm,"""Even if we don't achieve the Holy Grail of human level cognition within the next five years..."" :).",https://www.reddit.com/r/MachineLearning/comments/3yp5sm/even_if_we_dont_achieve_the_holy_grail_of_human/,bkaz,1451422849,,3,10
720,2015-12-30,2015,12,30,8,3ypmvz,Book most similar to Andrew Ng's course?,https://www.reddit.com/r/MachineLearning/comments/3ypmvz/book_most_similar_to_andrew_ngs_course/,YourWelcomeOrMine,1451430106,"I really enjoyed the content of Andrew Ng's course, but would prefer a textbook with lessons and exercises. Can anyone recommend a similar book to work on building machine learning programming skills and writing classifiers?",6,3
721,2015-12-30,2015,12,30,8,3ypot9,Modern Retail Analytics: Machine Learning and Big Data,https://www.reddit.com/r/MachineLearning/comments/3ypot9/modern_retail_analytics_machine_learning_and_big/,preinventedwheel,1451430985,,0,1
722,2015-12-30,2015,12,30,9,3ypyx7,Interesting recent papers/videos/talks?,https://www.reddit.com/r/MachineLearning/comments/3ypyx7/interesting_recent_papersvideostalks/,spurious_recollectio,1451435481,"With the holidays one has a bit of free time and I was thinking of catching up on recent interesting papers or videos/talks?  Preferably videos I guess.  I'm interested in Deep Learning, in particularly NLP and QA, etc... (but other cool stuff is great too).  Could anyone recommend interesting talks or papers I might have missed from NIPs or some other workshop which is freely available online?  I know its a vague question but the answers might be useful for others as well just as a list of nice recent things to check out.

EDIT: As I mentioned in a comment below I guess a more focused question would be ""what are the interesting videos from NIPs or other recent workshops that people can recommend?""
",2,6
723,2015-12-30,2015,12,30,10,3yq9fd,[Meta]Base of girls for seexxx! My personal list! Link activ: 1 hours.,https://www.reddit.com/r/MachineLearning/comments/3yq9fd/metabase_of_girls_for_seexxx_my_personal_list/,munkins11749,1451440453,,0,1
724,2015-12-30,2015,12,30,11,3yqbb0,[1512.07030] Deep Learning with S-shaped Rectified Linear Activation Units,https://www.reddit.com/r/MachineLearning/comments/3yqbb0/151207030_deep_learning_with_sshaped_rectified/,downtownslim,1451441321,,6,20
725,2015-12-30,2015,12,30,12,3yqk30,An interesting blog about Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3yqk30/an_interesting_blog_about_machine_learning/,tarpus,1451445556,,0,1
726,2015-12-30,2015,12,30,13,3yqshp,I made an app for the blind with an mxnet CNN that speaks what their phone sees in realtime!,https://www.reddit.com/r/MachineLearning/comments/3yqshp/i_made_an_app_for_the_blind_with_an_mxnet_cnn/,ieee8023,1451449833,,39,165
727,2015-12-30,2015,12,30,15,3yr907,[Meta] This subreddit is overwhelming.,https://www.reddit.com/r/MachineLearning/comments/3yr907/meta_this_subreddit_is_overwhelming/,cuban_CIFAR,1451458792,"The membership that contributes to this board is very talented, knowledgeable, and involved. Props to those guys.  
  
However. Sometimes, if there are beginner tier questions asked here they might be downvoted due to their relative triviality, if they're not clearly relatable to content we see here or if they aren't phrased appropriately.  
This among troves and troves of high level research papers, or , conversely, just extremely mushy elementary talks/tutorials. The middle ground is something that is hard to recognize, isolate, and promote.  
  
It also seems like the board enjoys ""digesting"" material more than it does playing around with it. Which makes the board more like a live reference page with commentary.    
  
Right now I'm polling for opinions on starting r/ml_experiments or r/ml_light board for a more free-form ""say and do stupid things"" style for discourse.  Is it naive to expect this sort of thing to work?
",30,38
728,2015-12-30,2015,12,30,18,3yrknj,Plotting four dependent variables,https://www.reddit.com/r/MachineLearning/comments/3yrknj/plotting_four_dependent_variables/,[deleted],1451466906,[deleted],0,1
729,2015-12-30,2015,12,30,19,3yrpeg,Talk: Memory-based Bayesian Reasoning and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3yrpeg/talk_memorybased_bayesian_reasoning_and_deep/,HelmsmanRobertson,1451470521,,3,12
730,2015-12-30,2015,12,30,20,3yruvo,Future of Life Institute: The Top A.I. Breakthroughs of 2015,https://www.reddit.com/r/MachineLearning/comments/3yruvo/future_of_life_institute_the_top_ai_breakthroughs/,egrefen,1451474733,,4,17
731,2015-12-30,2015,12,30,20,3yrv8p,Massive Deep Learning,https://www.reddit.com/r/MachineLearning/comments/3yrv8p/massive_deep_learning/,alrojo,1451474991,"Which libraries are optimal for deploying massive deep learning?(Neural networks)
By massive deep learning is meant tasks that would require exaflops, computed on several GPU's across several nodes of GPU's (preferably in the 100s)

By best is meant how many types of technologies supported(CNN, RNN, BN, SPN, Semi-supervised etc.), how fast is the community to develop new technologies and how fast is the computation of the library.

Currently I am aware of the following libraries that support multi gpu on multi node(however, unfamiliar with their relative speed):

MXNET: https://github.com/dmlc/mxnet
2000 stars on github, seems healthy with plenty of community collaboration.
supports multi gpu and multi node

yjxiong/Caffe: https://github.com/yjxiong/caffe
Fork of caffe, 30 stars, supports multi gpu and multi node.

Petuum: http://petuum.github.io/
Tools for caffe(with poseidon), 700 stars on github, barely any community support, made by the sailing lab at CMU, supports both data and model parallelization for non-synchronous SGD to multiple gpus/nodes.",2,4
732,2015-12-30,2015,12,30,21,3yrzks,ELI5: The Reparameterization Trick,https://www.reddit.com/r/MachineLearning/comments/3yrzks/eli5_the_reparameterization_trick/,1212015,1451478181,"Any one has a simple, intuitive explanation of the reparameterization trick[1]? There seems to be a wide application of this[2][3], but every explanation I have found is mathematically super dense.

Can we have a ""Gaussian Sampler Layer"" just like other layers in a Deep Net, with forward and backward equations? If yes, what would the exact equations be?

[1] Auto-Encoding Variational Bayes. Diederik P. Kingma and Max Welling, arXiv: 1312.6114v10

[2] Variational Dropout and the Local Reparameterization Trick. Diederik P. Kingma, Tim Salimans and Max Welling

[3] DRAW: A Recurrent Neural Network For Image Generation. Karol Gregor, Ivo Danihelka, Alex Graves, Daan Wierstra.",3,15
733,2015-12-30,2015,12,30,21,3ys1am,Data Format for Scikit learn,https://www.reddit.com/r/MachineLearning/comments/3ys1am/data_format_for_scikit_learn/,[deleted],1451479381,[deleted],0,1
734,2015-12-30,2015,12,30,22,3ys2v4,Neural Turing Machine in Tensorflow,https://www.reddit.com/r/MachineLearning/comments/3ys2v4/neural_turing_machine_in_tensorflow/,carpedm20,1451480475,,1,13
735,2015-12-30,2015,12,30,22,3ys57d,"Joscha Bach: Computational Meta-Psychology [video, 1:01:39]",https://www.reddit.com/r/MachineLearning/comments/3ys57d/joscha_bach_computational_metapsychology_video/,[deleted],1451481888,[deleted],0,1
736,2015-12-30,2015,12,30,23,3ysc7z,Developed a Cost Sensitive Neural Network. Would like to discuss if anyone is into it.,https://www.reddit.com/r/MachineLearning/comments/3ysc7z/developed_a_cost_sensitive_neural_network_would/,gubbanub,1451485857,Major use of this algorithm is when the data is highly imbalanced and inter variance between classes is less.,1,1
737,2015-12-31,2015,12,31,0,3ysmsn,10 Great Datasets on Movies,https://www.reddit.com/r/MachineLearning/comments/3ysmsn/10_great_datasets_on_movies/,ninacertain,1451491085,,1,20
738,2015-12-31,2015,12,31,1,3ysp2m,Why are movie subtitles and elementary-school reading comprehension assignments not used as NLP/QA datasets?,https://www.reddit.com/r/MachineLearning/comments/3ysp2m/why_are_movie_subtitles_and_elementaryschool/,spurious_recollectio,1451492064,"Two very natural datasets that would seem to provide a huge amount of data are:

- Movie subtitles
- Elementary school reading comprehension questions

For movies there's a lot one can do.  There are often subtitles in various languages which provides a nice translation dataset (more relevant for many contexts that e.g. EU documents).  One can also use the fact that the sound and video are synced with the text to do multi-modal unsupervised training (e.g. train embeddings that map simultanuous image/audo/text data into vectors that are nearby).  There's a lot one can imagine doing with this.  Has anyone tried?  Are there datasets available?

The second thing is even more puzzling.  I don't know where to get them but I imagine there must be giant troves of reading comprehension questions from middle or elementary schoolers.  These can either be standardized tests with answers or even just graded questions.  They can be multiple choice or even freeform graded text.  One can imagine building really interesting datasets with any of these but I've never seen it mentioned.  I would do it myself if I had any idea where to get these questions.

Has anyone thought about either of these?  Does anyone have insights on how to get started getting the data in either case (for films its easy to see where to get the films but are there nice tools to grab synced video/audio/text)?",1,4
739,2015-12-31,2015,12,31,3,3yt4o5,"Deep Learning in a Single File for Smart Devices with MXnet (Python, Java and Go, with Demo)",https://www.reddit.com/r/MachineLearning/comments/3yt4o5/deep_learning_in_a_single_file_for_smart_devices/,phunter_lau,1451498547,,5,43
740,2015-12-31,2015,12,31,4,3ytlsk,Generative model over joint distribution with LSTMs (see PDF link).,https://www.reddit.com/r/MachineLearning/comments/3ytlsk/generative_model_over_joint_distribution_with/,jostmey,1451505427,"Just finished looking at this paper [(link)](http://papers.nips.cc/paper/5637-generative-image-modeling-using-spatial-lstms.pdf). They propose a method for training a generative model over a joint distribution using LSTM units. What caught my attention is that the approach does not require the use of hidden stochastic variables. This means that no inference is required, so a model could be trained using the Likelihood function as the *direct* objective function, unlike other generative models like DAEs, Adversarial nets, ect.

It looks like to me that the model will scale well. Any thoughts?",0,3
741,2015-12-31,2015,12,31,5,3ytnqc,"Deep Q learning with continuous actions (question about paper ""Continuous control with deep reinforcement learning"")",https://www.reddit.com/r/MachineLearning/comments/3ytnqc/deep_q_learning_with_continuous_actions_question/,nivwusquorum,1451506204,"Hi all,

Some time ago, I gave you DeepQ learning implemented in Tensorflow. Now I am trying to implement a version of the algorithm that allows for continuous actions according to this paper:

http://arxiv.org/pdf/1509.02971v2.pdf

I am trying to understand how one should train the actor... Am I correct in saying that to train the actor we just optimize the following:

maximimize critic_score(observation, actor(observation))
w.r.t actor variables

They provide a formula for the gradient explicitly, but I think it's equivalent the above optimization problem. Nevertheless I implement the code and it does not optimize for the simplest problem in the world. It is most probably a software bug, but I also would like to make sure that I understand the theory.",16,6
742,2015-12-31,2015,12,31,5,3yttol,Any recommendations for an easy to use regression supervised learner (numeric inputs and outputs) in Java?,https://www.reddit.com/r/MachineLearning/comments/3yttol/any_recommendations_for_an_easy_to_use_regression/,sanity,1451508685,"I'm trying to do a quick experiment to compare two different ways to represent the target variables in a supervised learning problem, where the inputs are several numeric values (there may be 2-6 target variables depending on the representations).

I have limited time for the project, so my hope is to find a library for doing the regression part of this in Java relatively easily.  Predictive accuracy isn't too important because I'm really only interested in the relative performance of the two different target variable representations (I know this is a little sloppy, but it's adequate for the task).

Can anyone point me to a suitable library that won't have a significant learning-curve, or a bunch of tweaking, just to achieve this (hopefully) simple task?",3,1
743,2015-12-31,2015,12,31,6,3yty41,[Beginner] Which of the math courses in this list would be useful for machine learning?,https://www.reddit.com/r/MachineLearning/comments/3yty41/beginner_which_of_the_math_courses_in_this_list/,ph3rn,1451510580,"[This](http://pdfsr.com/pdf/pages-1-.pdf) is the list of 300 and 400 level math courses offered at my uni. I am already taking Intro to Probability, Probability and Stochastic Processes, Intermediate Analysis and Numerical Analysis 1, as they are requirements for math majors. I was told I should pick courses that cover real analysis and convex optimization, but I don't see anything with the word 'convex' or 'real anaylsis' here. Is the Intermediate Analysis course here the same thing?",5,1
744,2015-12-31,2015,12,31,7,3yu7b7,"Finished my first ML project, an evolutionary algorithm engine. Any advice?",https://www.reddit.com/r/MachineLearning/comments/3yu7b7/finished_my_first_ml_project_an_evolutionary/,adkyriazis,1451514535,"Hi everyone,

I'm relatively new to the machine learning scene, and wanted to take a stab at evolutionary algorithms. I developed a primitive evolutionary algorithm engine in Java, and I was wondering if anyone had any advice they could give me on how to improve the engine, or expand the project in a meaningful way. 

https://github.com/akyriazis/evolutionary-algorithm

Any advice would be greatly appreciated.",2,3
745,2015-12-31,2015,12,31,8,3yuexa,How to export data-frame from Apache Spark,https://www.reddit.com/r/MachineLearning/comments/3yuexa/how_to_export_dataframe_from_apache_spark/,[deleted],1451517938,[deleted],0,9
746,2015-12-31,2015,12,31,9,3yuk7m,Train Google's dinosaur using simple neural network,https://www.reddit.com/r/MachineLearning/comments/3yuk7m/train_googles_dinosaur_using_simple_neural_network/,tngan,1451520343,,4,37
747,2015-12-31,2015,12,31,10,3yut5t,"With 1 day to go in first Numerai tournament, user WSW jumps to first place, improving previous best AUC of 0.55 to 0.65",https://www.reddit.com/r/MachineLearning/comments/3yut5t/with_1_day_to_go_in_first_numerai_tournament_user/,klihu,1451524463,,1,0
748,2015-12-31,2015,12,31,10,3yuulr,Where to get data to train Markov chain chatbot?,https://www.reddit.com/r/MachineLearning/comments/3yuulr/where_to_get_data_to_train_markov_chain_chatbot/,[deleted],1451525179,[deleted],1,1
749,2015-12-31,2015,12,31,10,3yuvep,Tutorial on Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/3yuvep/tutorial_on_generative_adversarial_networks/,[deleted],1451525578,[deleted],0,1
750,2015-12-31,2015,12,31,10,3yuwyj,Tutorial on Generative Adversarial Networks in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/3yuwyj/tutorial_on_generative_adversarial_networks_in/,ejang,1451526338,,7,26
751,2015-12-31,2015,12,31,11,3yuzxv,Panda challenge for CNN,https://www.reddit.com/r/MachineLearning/comments/3yuzxv/panda_challenge_for_cnn/,reddit_tl,1451527790,"Pretty sure Cnn can handle this and pick out the panda. Anyone want to take a crack?
https://i.imgur.com/r1MpjUe.jpg",3,1
752,2015-12-31,2015,12,31,11,3yv1nm,This is exactly what I think when reading almost any ML/AI paper.,https://www.reddit.com/r/MachineLearning/comments/3yv1nm/this_is_exactly_what_i_think_when_reading_almost/,mattway,1451528644,,9,0
753,2015-12-31,2015,12,31,12,3yv9pw,Startup.ML Deep Learning Conference Videos,https://www.reddit.com/r/MachineLearning/comments/3yv9pw/startupml_deep_learning_conference_videos/,arshakn,1451532711,,0,1
754,2015-12-31,2015,12,31,13,3yvhnh,Top Reasons to Why Should Engineering Students Look for 6 Months Industrial Training in Noida,https://www.reddit.com/r/MachineLearning/comments/3yvhnh/top_reasons_to_why_should_engineering_students/,aeabdotin,1451536747,,0,1
755,2015-12-31,2015,12,31,13,3yvj2p,Best Programming Language for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/3yvj2p/best_programming_language_for_machine_learning/,[deleted],1451537536,[deleted],2,0
756,2015-12-31,2015,12,31,14,3yvle1,Summer Internships,https://www.reddit.com/r/MachineLearning/comments/3yvle1/summer_internships/,walter109,1451538711,"I'm doing a PhD in statistics and am looking for a machine learning summer internship. Can anyone recommend interesting companies to look at (other than the ones that immediately come to mind like Google, Microsoft, IBM, etc)? ",4,0
757,2015-12-31,2015,12,31,15,3yvv28,Is negative step size an issue in conjugate gradient descent?,https://www.reddit.com/r/MachineLearning/comments/3yvv28/is_negative_step_size_an_issue_in_conjugate/,Qanari,1451544225,"I have followed [this](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) tutorial and implemented conjugate gradient descent. It works fairly good for my problems. I started plotting the step size '[; \alpha ;]`, and noticed for some initial conditions it is negative. Although it solves the problem it is negative. Is this an issue? I mean mathematically, does this violate any assumption for stability of the algorithm or some thing like that?

If I use this condition `[; \alpha = max(0, \alpha) ;]` it gets stuck in local minima and can not converge to the solution.",2,0
758,2015-12-31,2015,12,31,18,3yw73i,Identical network converging in Theano but saturating in TensorFlow?,https://www.reddit.com/r/MachineLearning/comments/3yw73i/identical_network_converging_in_theano_but/,pinmelon,1451552751,"Quick intro, I'm relatively new to machine learning, been playing around with Theano and TensorFlow for a project. Still unfamiliar with much of the ML concepts though.

I've been trying to use deep NN for a **regression** problem. I first got it working in Theano, where I got nice convergence. I then tried to replicate the network in TensorFlow, but there the network simply saturates within an epoch and gets stuck (~1mil datasets per epoch).

I've tried to simplify things:

* 2 hidden layers, nothing fancy (size 200 each)

* softsign activations

* single linear output

* mean-squared-error loss function

* standard stochastic gradient descent

* identical learning rate

* identical mini-batch sizes

* identical weight/bias initialization (identical prng seeds)

* run on CPU to avoid GPU complications

Even with all that, TensorFlow gets stuck at loss values ~40. I'm trying to hit values &lt;1 (which I can reach using Theano). It's driving me crazy!

Sample Theano losses @iteration: #1: 376.82, #2: 158.92, #3: 76.00, #4: 61.49 ... #297: 9.34, #298: 8.65, #299: 7.91, #300: 7.48

Sample TensorFlow losses @iteration: #1: 376.60, #2: 160.73, #3: 79.07, #4: 64.56 ... #297: 42.19, #298: 41.43, #299: 41.08, #300: 41.26

Some extra info:

* For some reason the results of matmul (TensorFlow) and dot (Theano) are not identical. Even numpy.sum gets different results. The differences are small though, on the order of 0.002%. Different numerical libraries maybe?

* In case anyone suggests to *""forget it and just use Theano""*, I'm reimplementing in TensorFlow because I want to be sure I'm getting the concepts right before extending usage further into the problem domain.",6,6
759,2015-12-31,2015,12,31,20,3ywgv3,bayes.js: A Small Library for Doing MCMC in the Browser,https://www.reddit.com/r/MachineLearning/comments/3ywgv3/bayesjs_a_small_library_for_doing_mcmc_in_the/,rasmusab,1451560605,,0,49
760,2015-12-31,2015,12,31,20,3ywhch,10 most popular Deep Learning libraries started in 2015,https://www.reddit.com/r/MachineLearning/comments/3ywhch/10_most_popular_deep_learning_libraries_started/,fruiapps,1451561005,,7,0
761,2015-12-31,2015,12,31,20,3ywi6x,Deep Residual Learning: The bottleneck,https://www.reddit.com/r/MachineLearning/comments/3ywi6x/deep_residual_learning_the_bottleneck/,alrojo,1451561642,"In the deep residual network winning the ImageNet2015 by Microsoft (http://arxiv.org/abs/1512.03385), they used a bottleneck architecture to reduce computational time.

psuedo-code (ConvLayer and BatchNorm layer are layer classes like in lasagne or mxnet):

inputLayer(num_filters=256)

convLayer1 = Batch_norm(Convlayer(inputLayer, filter_size=(1, 1), num_filters=64, stride=(?, ?)))

convLayer2 = Batch_norm(Convlayer(convLayer1, filter_size=(3, 3), num_filters=64, stride=(?, ?)))

convLayer3 = Batch_norm(Convlayer(convLayer2, filter_size=(1, 1), num_filters=256, stride=(1, 1)))

Then followed by a transform of the inputLayer and a sumlayer.

I had both the convLayer1 and convLayer2 having (?, ?) with their strides. This is because I cannot seem to get a confirmation from the article in which part of the bottleneck it does the striding. Intuitively it would make the most sense to do it in convLayer2 as it has 3x3 filter_size. If it is done in convLayer1 the bottleneck would lose 75% of the information already computed in the image as it has 1x1 filter_size.

What is your intuition from reading the article?",9,11
762,2015-12-31,2015,12,31,23,3ywwv0,Adversarial Examples: ensemblea as antidote?,https://www.reddit.com/r/MachineLearning/comments/3ywwv0/adversarial_examples_ensemblea_as_antidote/,mcostalba,1451572188,"DISCLAIMER: I am really a noob here, but I was impressed by this Karpathy 's [blog]( http://karpathy.github.io/2015/03/30/breaking-convnets/) and the original Goodfellow's [paper](http://arxiv.org/abs/1412.6572).

They carefully craft an adversarial example that fools the network, leveraging the exact sequence of weight perturbations: this is a very specific key that can open only a very specific locker. The same perturbation vector possibly will not work for another set of weights. In particularly (possibly) will not work for another ConvNet instance.

So here the question: Is an ensemble of nets prone to adversarial example attack?
",10,4
