,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2012-3-2,2012,3,2,8,qdltz,Review: Machine Learning: An Algorithmic Perspective by Stephen Marsland,https://www.reddit.com/r/MachineLearning/comments/qdltz/review_machine_learning_an_algorithmic/,robintw,1330642909,,7,17
1,2012-3-2,2012,3,2,11,qdv2s,any one can explain what's the difference between NS(nearest subspace) method and K-means? ,https://www.reddit.com/r/MachineLearning/comments/qdv2s/any_one_can_explain_whats_the_difference_between/,katahe,1330654516,"In NS, we cluster new vector to the group with nearest weighted distance which i think is the same as in K-means. Anyone can explain a little or recommend some reading materials. Thank you!",3,6
2,2012-3-2,2012,3,2,11,qdvsh,"Does anyone have any experience with Segaran's ""Programming Collective Intelligence"" ?",https://www.reddit.com/r/MachineLearning/comments/qdvsh/does_anyone_have_any_experience_with_segarans/,Wonnk13,1330655433,"All of my coursework is with Bishop's pattern matching book and other more theoretical texts. Although they're well written i'm finding it easy to get lost in all of the proofs and struggling to learn how implement things with real world data. 

Segaran's book looks to be more of an applied ""cookbook"" style, however it is nearly five years old. Are the techniques still applicable, and is any of the code (Python 2.4) broken or replaced by better libraries? Any input appreciated :) ",5,3
3,2012-3-2,2012,3,2,14,qe36h,The people who believe we are living in a simulation...,https://www.reddit.com/r/MachineLearning/comments/qe36h/the_people_who_believe_we_are_living_in_a/,[deleted],1330665328,"I didnt know where to post this but I am looking for links or videos or names related to the following quote...if you can help....or tell me which subreddit to post in:

""Because of the rate computing power is increasing there is actually a small but growing fringe religion/spiritual belief out there that believes that we as humans live within a simulation and that ""god"" is the one who programmed us. A lot of it is based on the simulation argument/hypothesis. I've seen a few interviews about it but I'm not sure what it's called.""",7,0
4,2012-3-3,2012,3,3,2,qenw0,The Spelling Corrector that Got Me Interested in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/qenw0/the_spelling_corrector_that_got_me_interested_in/,tdh3m,1330708400,,1,21
5,2012-3-3,2012,3,3,5,qexeq,new subreddit for reinforcement learning!,https://www.reddit.com/r/MachineLearning/comments/qexeq/new_subreddit_for_reinforcement_learning/,lpiloto,1330720242,,3,2
6,2012-3-3,2012,3,3,16,qfo82,What is Mahalanobis Distance? An accessible and intuitive introduction.,https://www.reddit.com/r/MachineLearning/comments/qfo82/what_is_mahalanobis_distance_an_accessible_and/,lpiloto,1330758978,,2,17
7,2012-3-5,2012,3,5,5,qhj1n,Does anyone know of any data sets where I can apply data mining techniques towards customer relationship management?,https://www.reddit.com/r/MachineLearning/comments/qhj1n/does_anyone_know_of_any_data_sets_where_i_can/,aguyfromucdavis,1330892924,I'm looking for data sets that show customer behavior or activity and whether or not they bought a certain product. I'm looking to apply techniques like k-means clustering or decision trees to profile the customer population and learn more about their propensity to purchase a product. Thanks! (x-post from r/datasets),1,8
8,2012-3-6,2012,3,6,2,qitet,The data science debate: domain expertise or machine learning?,https://www.reddit.com/r/MachineLearning/comments/qitet/the_data_science_debate_domain_expertise_or/,agconway,1330970057,,1,21
9,2012-3-6,2012,3,6,6,qj529,how does computer vision use machine learning?,https://www.reddit.com/r/MachineLearning/comments/qj529/how_does_computer_vision_use_machine_learning/,waspinator,1330984269,"I'm reading [Szeliski's book](http://szeliski.org/Book/) about computer vision, and I can't always see a connection between computer vision and machine learning in many places. Especially in instance recognition. Does it exist? 

is there any machine learning going on in instance recognition? Is [SIFT](http://en.wikipedia.org/wiki/Scale-invariant_feature_transform) the only thing required to accomplish this? Does any learning take place?

Is there a paper or other document explaining how a machine learning approach is used in finding objects in an image?",13,5
10,2012-3-6,2012,3,6,9,qjc26,Why generic machine learning fails.,https://www.reddit.com/r/MachineLearning/comments/qjc26/why_generic_machine_learning_fails/,qkdhfjdjdhd,1330992322,,6,15
11,2012-3-6,2012,3,6,17,qjx7z,Confusion matrix with leave-one-out cross validation,https://www.reddit.com/r/MachineLearning/comments/qjx7z/confusion_matrix_with_leaveoneout_cross_validation/,thrope,1331021311,"I have started working on a project where we are using a nearest mean classifier on a noisy data set to evaluate different features. 

We do leave one out - where we leave out one sample, fit the classifier to the remaining data, and then predict for the left out sample. We repeat this with each data point and build the confusion matrix in this way. 

I am trying to get a deeper understanding of the theory and I get the impression the usually the confusion matrix is generated from a whole set of test data (not used for fitting) and is for a single classifier - whereas here we are building the matrix from all the many different (leave-one-out) classifiers. 

Is this still called a confusion matrix or is there any other terms for this procedure (I am a beginner in the ML field). Is there any reference I could read for more details of the theory behind this kind of procedure or how it could be improved. 

",3,3
12,2012-3-6,2012,3,6,19,qjzxq,I need your help finding a suitable algorithm for an incremental learning task. (details inside),https://www.reddit.com/r/MachineLearning/comments/qjzxq/i_need_your_help_finding_a_suitable_algorithm_for/,huepfburg,1331029040,"Dear reddit,

I am a compsci student from Germany and currently working on a project that will most likely be part of my bachelors thesis. I am having troubles finding an algorithm/method that allows me to do the following:

There will be a set of objects (up to 5,000 per Set) which will be presented to the user. The user will select exactly one of these objects from the set depending on the context  he will choose the object which fits best into the current context.

The system should learn over time what object the user selected within what context and later present the most suitable object for the current context at the top of the list of available objects.

Now here is an example: The user has loaded a resource containing numerous items (Strings) into his the system (a texteditor). After the user initiated a special auto-complete mode, the system should present a list of these items (Strings) and should order them depending on how suitable the Strings are for the current context (before and after the cursor position). The system has learned what Strings are most suitable for the context.

I was first thinking of using an nave bayes approach by using the tokenized context as featureset and the items from the resource as available classes. I am not sure if this is a valid approach. Here are the problems I am facing:

* The number of available classes (items) can be very large.
* The context will be different every time. It is most unlikely, that the exact same featureset will occure more than once.
* The algorithm will have to be suitable for incremental learning: The system will start off without any data about the user's preferences and learn them over time, while the user is working with the system.

Do you have any suggestions as to what methods would be applicable to the task I've just described? Obviously performance is an issue  the system should not take too long for the user to notice, before the ordered list of items is presented.

Thanks in advance",7,6
13,2012-3-7,2012,3,7,4,qkif3,Has any one tried torch7? How does it compare to matlab or numpy?,https://www.reddit.com/r/MachineLearning/comments/qkif3/has_any_one_tried_torch7_how_does_it_compare_to/,DoorsofPerceptron,1331061129,,12,8
14,2012-3-7,2012,3,7,5,qkmd0,Stanford NLP class is live!,https://www.reddit.com/r/MachineLearning/comments/qkmd0/stanford_nlp_class_is_live/,HughJorgan1986,1331066016,,7,40
15,2012-3-7,2012,3,7,13,qlash,What Lang/Platforms to use for working with datasets?,https://www.reddit.com/r/MachineLearning/comments/qlash/what_langplatforms_to_use_for_working_with/,zigzagp,1331096204,"Hi, I'm a final year student in CS and is on the verge of completing the lectures of Artificial Intelligence (Stanford Online) and i have already covered Machine Learning (Stanford Online). Now, i want to work with some data sets and from what i read from quora or from here, i see that people generally recommend either R or NumpY or specific libraries for specific algorithms. What i would like to know, is if there are different situations/purposes for which each of them are used and if there are, please let know. Also, if you would like to recommend new platforms, pls do so but also tell us as to why it is being used?",15,5
16,2012-3-8,2012,3,8,2,qlykm,"Reduce excise on textile machinery, TMMA tells FinMin",https://www.reddit.com/r/MachineLearning/comments/qlykm/reduce_excise_on_textile_machinery_tmma_tells/,hhind,1331141355,,0,0
17,2012-3-8,2012,3,8,13,qmw75,core bit,https://www.reddit.com/r/MachineLearning/comments/qmw75/core_bit/,ginopartez,1331181806,,1,1
18,2012-3-9,2012,3,9,2,qninc,Library to Strip Wiki Markup from Wikipedia?,https://www.reddit.com/r/MachineLearning/comments/qninc/library_to_strip_wiki_markup_from_wikipedia/,ltltltlt,1331226623,"I was wondering if there's a better resource for stripping the natural language text, doing appropriate substitutions of things like
    {{convert|3|lbs|kg}}
but ignoring things like
    {{cite book|bla|bla|bla}}
and replacing [[Cat|Cat]] style links but removing [[fr:Chat]] style links, preferably in Ruby, Python, or even C.

All I can find is the PHP that is part of Mediawiki itself and a number of sources that strip the marked-up text from XML, which is trivial.

If not, would anyone be interested in collaborating on writing (a solid begining to) such a parser over the weekend? I have been doing some experimenting and the difficulty is really just recognizing what templates are syntactically relevant (convert, et al) and which are information that doesn't belong/I don't want in the natural language portion (see, cite, et al)",19,6
19,2012-3-10,2012,3,10,1,qp1hj,Irfans Taxonomy of Predictive Modeling,https://www.reddit.com/r/MachineLearning/comments/qp1hj/irfans_taxonomy_of_predictive_modeling/,willis77,1331311125,,0,6
20,2012-3-10,2012,3,10,5,qpb0w,Peter Norvig - The Unreasonable Effectiveness of Data,https://www.reddit.com/r/MachineLearning/comments/qpb0w/peter_norvig_the_unreasonable_effectiveness_of/,vrld,1331323293,,0,5
21,2012-3-13,2012,3,13,1,qt3ak,Personalization vs Privacy,https://www.reddit.com/r/MachineLearning/comments/qt3ak/personalization_vs_privacy/,towaway1234,1331570882,"
I use AdBlock in Chrome and I'm subscribed to the following lists:

Fanboy tracking list, Fanboy annoyance list, Easylist privacy, the main Fanboy list, the main Easylist.

Am I still able to receive customized and personalized content when I use various web apps, such as Google Search? Or does Adblock block webapp attempts to collect data about me, therefore are they unable to offer customized and personalized content to me?

Google Search personalizes content for every user. They do this by collecting as much data about the user as possible. After that they run machine learning algorithms on the data which in the end outputs the user's preferences. In the end, they feed this into their search algorithm that modifies the displayed results in order to suit the preferences of the user more, therefore the user will find the search results more relevant to his interest.

Example: I regularly visits my university's homepage. For example, the university uses Google Analytics to track page views and visitors. The data is sent to the university Analytics account and it is used by Google too. This way, Google can compare my IP address to the IP address that I use to log into their services. Therefore, they can see that I visit my university's homepage a lot, so I must attend it. Then next time I do a search on Computer Programming 101, Google Search will return many results from many universities. However, because Google now knows that I attend that one university, it will modify the search results and display that one on top. Therefore, I get more relevant result for my interests.

Adblock blocks the Google Analytics tracker plugin, which means Google cannot collect data about me. Therefore, they won't know that I go to that one university, so when I search for Computer Programming 101, they will display a lot of results that I'm not looking for. They won't be able to personalize and customize the search results to my interests, since they don't have enough data about me, because Adblock blocks their data collection attempts.

I hope it is clearer now what I meant. There may be a myriad of different scenarios where this happens, I just wrote one off the top of my head.",2,0
22,2012-3-13,2012,3,13,7,qtl95,Rise of the crossword robots,https://www.reddit.com/r/MachineLearning/comments/qtl95/rise_of_the_crossword_robots/,cavedave,1331591677,,2,17
23,2012-3-13,2012,3,13,9,qtrp9,Ask /r/ML: NLP system for laws relevant to given activity?,https://www.reddit.com/r/MachineLearning/comments/qtrp9/ask_rml_nlp_system_for_laws_relevant_to_given/,astebbin,1331599245,"Does anyone know whether a natural language-processing system exists to list all laws relevant to an activity specified in a textual query, such as buying a car, owning a pet, going fishing, etc.? I know that a lot of research has done in legal NLP recently, specifically in the field of patent redundancy checking, but I can't seem to find a system fitting this particular description. Thanks!",1,3
24,2012-3-13,2012,3,13,12,qu14e,Wondering if someone could point me in the right direction...,https://www.reddit.com/r/MachineLearning/comments/qu14e/wondering_if_someone_could_point_me_in_the_right/,Rotten194,1331610540,"Not sure if this is the right place for this, but here goes...

I'm not very experienced with ML, never taken a class or anything, but am currently working on a project that I wanted to use it in to learn some more about it.

In this project I'm looking to take a smallish set of numbers (~10-20) and tweak them based on binary good/bad feedback. I was wondering if this is possible, and if so if someone could point me at a good book/paper/website to work on something like this? Thanks!",8,4
25,2012-3-14,2012,3,14,3,qux3f,"Looking to scrape contact info from mixed raw HTML pages, for indexing into a central database. Anybody faced this problem (or similar) before and have a lead?",https://www.reddit.com/r/MachineLearning/comments/qux3f/looking_to_scrape_contact_info_from_mixed_raw/,[deleted],1331665185,"Title is a pretty good summary. I've got a large number of files I'm responsible for extracting name, email, address, phone number from to build a database for work, and data scraping is a little out of my wheelhouse. The layout of the pages is not all the same, but there are indicators to help locate the pertinent data - the name or an abbreviation of the name is part of the filename, and the phone numbers seem to universally be in the xxx-xxx-xxxx or (xxx) xxx-xxxx format. I wouldn't presume to barge in here and ask y'all to do my work for me - just a point in the right direction would be a blessing. Without prior knowledge, I'm using the shotgun research approach so far, and several hours of Googling has thusfar produced... bupkiss. So, any ideas?
",15,6
26,2012-3-14,2012,3,14,5,qv1dn,[AskML] Finding tv show name and season/episode number in unpredictable filenames -- is ML right for me?,https://www.reddit.com/r/MachineLearning/comments/qv1dn/askml_finding_tv_show_name_and_seasonepisode/,omginternets,1331669965,"Hello /r/ML!  This is my first post here, so please be gentle!

I'm writing a python script that cleans up file names of TV shows based on information scraped from thetvdb.com.

In the overwhelming majority of cases, the name of the show, along with the season/episode information is right in the filename.  When this is the case, I'd like for my scraper to be able to pick it out of the other crap in the filename so that I can use it to look up information on thetvdb.com.

So again, in a nutshell, I'd like to :

- Pick out tv show name, season and episode information from an unpredictable string of characters (filename)

- Detect a state in which no tv and/or season/episode information is present in the filename.

Does this sound like something a naive baysean classifer could do?  I ask because this is the only type of classifier I think I'd be able to implement straight away.  If not, could you please point me in the right direction?

As always, tutorials and code snippets are quite welcome!

Thanks in advance!",10,1
27,2012-3-14,2012,3,14,11,qviyh,Machine learning and object recognition...,https://www.reddit.com/r/MachineLearning/comments/qviyh/machine_learning_and_object_recognition/,cosmobaud,1331690849,"I keep being continually amazed at how apparently effortless object recognition is for humans and how difficult it is currently for AI.


For example look at a random photo like this http://essentialurbanism.files.wordpress.com/2010/09/sukkah_city_panorama1.jpg


Most of us will have no problem identifying and categorizing the objects featured in the photo even though they have never seen those specific objects before.
My thought our abilities comes from us ""assuming"" and generalizing objects very efficiently.


For example when you look at the photo above I don't believe that we actually recognize the objects but assume from ""top down"" if you will what we're perceiving.

For example if you look at a photo on this article
http://www.sciencedaily.com/releases/2010/09/100922121937.htm
Our brain assumed one thing and then proceeded to make other assumptions about the scene based on that knowledge.


I'm really curious about learning how we determine our assumption is good enough so we can move forward and how to we correct the assumption.",5,6
28,2012-3-14,2012,3,14,16,qvvkt,[AskML] ML algorithm to grade transcripts of counseling sessions?,https://www.reddit.com/r/MachineLearning/comments/qvvkt/askml_ml_algorithm_to_grade_transcripts_of/,hillset,1331708543,"I've recently been wondering what it would take to create some sort of program to grade transcripts of students practicing a specific counseling technique in a video taped counseling session.  Basically, I have ~150 videos of students practicing a 10 minute counseling session with a practice client, and I also have a structured form we use to grade these taped sessions.  The form asks the evaluator to rate the student on 14 items (such as expressing empathy or addressing the client's concerns adequately).  Each of these items can be graded as a + (if the student says something that reflects appropriate use of that item, for ex. if the student appropriately expresses empathy), or a - (if the student says something that reflects inappropriate use of that item).  These final + and - counts for each item are then assessed by the evaluator to arrive at an overall score (out of 100) for the video.  I am interested in designing an NLP program that could predict + and - counts for each item on the evaluation form based on transcripts of these counseling sessions, and also in an ML algorithm that could predict an overall score for the video based on the distribution of counts over the 14 items.  I was wondering if anyone on /r/machinelearning could recommend any good books/papers/websites to point me in the right direction or if anyone on reddit was interested in working on this project with me?  Since the data is from students I cant share it but I would love the opportunity to collaborate on the code with a fellow redditor.  Thanks for the help /r/machinelearning!",6,1
29,2012-3-15,2012,3,15,0,qw9q0,I'm new to ML and looking to group strings of keywords into similar sets. ,https://www.reddit.com/r/MachineLearning/comments/qw9q0/im_new_to_ml_and_looking_to_group_strings_of/,kainolophobia,1331739202,"Basically, I want to map the similarities between various keywords pools such that I can group the various pools as being a part of one larger pool. This process would ideally ""learn"" and make more specific pools as more data is acquired. I've come across clustering algorithms and the like, but was wondering what everyone here had to think about it before I dive into a certain direction. Thanks!",4,4
30,2012-3-15,2012,3,15,6,qwr8o,Learning From Data - Online Course from Caltech,https://www.reddit.com/r/MachineLearning/comments/qwr8o/learning_from_data_online_course_from_caltech/,lightcatcher,1331759996,,5,45
31,2012-3-15,2012,3,15,22,qxpzt,Bayesian statistics tutorial: video from PyCon 2012,https://www.reddit.com/r/MachineLearning/comments/qxpzt/bayesian_statistics_tutorial_video_from_pycon_2012/,AllenDowney,1331817114,,0,24
32,2012-3-15,2012,3,15,23,qxsrb,Judea Pearl wins Turing Award,https://www.reddit.com/r/MachineLearning/comments/qxsrb/judea_pearl_wins_turing_award/,DoorsofPerceptron,1331821677,,3,53
33,2012-3-16,2012,3,16,22,qzbet,"University researchers are developing a system to help identify people who are behind offensive comments posted on the internet.
",https://www.reddit.com/r/MachineLearning/comments/qzbet/university_researchers_are_developing_a_system_to/,fishandchips,1331903297,,6,15
34,2012-3-17,2012,3,17,5,qzwuu,Computer matching wits with humans in crossword tournament,https://www.reddit.com/r/MachineLearning/comments/qzwuu/computer_matching_wits_with_humans_in_crossword/,joshdick,1331931327,,0,10
35,2012-3-17,2012,3,17,7,r01an,Ask ML: Mean parameterization of exponential families,https://www.reddit.com/r/MachineLearning/comments/r01an/ask_ml_mean_parameterization_of_exponential/,the_mind_is_a_sponge,1331936830,"Hello all, I was wondering if someone could help me understand a concept that I just came across when reading a graphical model text by Wainwright and Jordan: http://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf

What I'm confused about is how the mean parameters paramaterize a distribution. Do they just replace the canonical parameters in the regular exponential family density function? This concept is introduced in section 3.4 of the text

",7,11
36,2012-3-18,2012,3,18,12,r1lhj,Why are derived features used in neural networks?,https://www.reddit.com/r/MachineLearning/comments/r1lhj/why_are_derived_features_used_in_neural_networks/,towaway1234,1332040049,"For example, one wants to predict house prices and have two input features the length and width of the house. Sometimes, one also includes 'derived' polynomial input features, such as area, which is length * width.

1) What is the point of including derived features? Shouldn't a neural network learn the connection between length, width and price during the training? Why isn't the third feature, area, redundant?

In addition, sometimes I also see that people run genetic selection algorithms on the input features in order to reduce their number.

2) What is the point of reducing the input features if they all contain useful information? Shouldn't the neural network assign appropriate weights to each input feature according to its importance? What is the point of running genetic selection algorithms?",20,12
37,2012-3-18,2012,3,18,22,r1zjm,Attempt #2: Want to help reddit build a recommender? -- A public dump of voting data that our users have donated for research : redditdev,https://www.reddit.com/r/MachineLearning/comments/r1zjm/attempt_2_want_to_help_reddit_build_a_recommender/,TheFrigginArchitect,1332076644,,9,33
38,2012-3-20,2012,3,20,6,r41ho,"Solving ""find the next number in the sequence"" problem using R and stats [r-bloggers]",https://www.reddit.com/r/MachineLearning/comments/r41ho/solving_find_the_next_number_in_the_sequence/,talgalili,1332192433,,0,1
39,2012-3-20,2012,3,20,19,r4wtj,linear regression,https://www.reddit.com/r/MachineLearning/comments/r4wtj/linear_regression/,[deleted],1332239596,,0,1
40,2012-3-20,2012,3,20,19,r4x2l,linear regression,https://www.reddit.com/r/MachineLearning/comments/r4x2l/linear_regression/,galotalp,1332240406,,0,0
41,2012-3-20,2012,3,20,22,r50kp,Physicists simulate strongly correlated fermions,https://www.reddit.com/r/MachineLearning/comments/r50kp/physicists_simulate_strongly_correlated_fermions/,nazgand,1332248798,,0,0
42,2012-3-21,2012,3,21,5,r5keg,Modeling and Reasoning with Bayesian Networks,https://www.reddit.com/r/MachineLearning/comments/r5keg/modeling_and_reasoning_with_bayesian_networks/,chris_nh82,1332274178,"I just purchased Modeling and Reasoning with Bayesian Networks by Adnan Darwiche. So far I'm finding it very well written and readable. I'd like to do some of the exercises to confirm my understanding, but I can't seem to find solutions so I can check my answers. Anyone happen to know if they are available anywhere? A teacher's guide perhaps?

Thanks",5,8
43,2012-3-21,2012,3,21,10,r62q7,"""Second Chance"" Machine March Madness Competition",https://www.reddit.com/r/MachineLearning/comments/r62q7/second_chance_machine_march_madness_competition/,danger_t,1332294761,,0,0
44,2012-3-21,2012,3,21,14,r6e6j,Industry seeks removal of excise duty on synthetic fibre,https://www.reddit.com/r/MachineLearning/comments/r6e6j/industry_seeks_removal_of_excise_duty_on/,hhrotery,1332309414,,1,0
45,2012-3-21,2012,3,21,17,r6i99,Proper method for Calculating Similarity Between Two Similarity Matrices?,https://www.reddit.com/r/MachineLearning/comments/r6i99/proper_method_for_calculating_similarity_between/,thornpyros,1332318882,"We want to calculate how similar two *similarity* matrices are. First of all I clarify some points. We have two similarity matrices which contain same set of items for which we gather information from two different sources.

You can assume that similarity matrices contain pairwise closeness of two people. These closeness values come from two different sources such as Facebook and MySpace.

Let's give you an example to illustrate what I want to say.

We have 4 people: John, Paul, George, Ringo. All they have Facebook and MySpace accounts.

From their Facebook profiles, we made some processing and somehow we evaluated the closeness of these people as follows.

* Paul - John = 0.5
* Paul - George = 0.4
* Paul - Ringo = 0.6
* John - George = 0.6
* John - Ringo = 0.5
* George - Ringo = 0.6

From MySpace,

* Paul - John = 0.2
* Paul - George = 0.3
* Paul - Ringo = 0.4
* John - George = 0.2
* John - Ringo = 0.1
* George - Ringo = 0.15

From informations above, we can easily create two matrices both of which are symmetric with respect to the main diagonal. Our ultimate goal is to calculate how similar these two matrices are.

1- What if I align all those values (of course only one triangular for each matrix) in one row (like below) and simply evaluate Pearson Correlation Coefficient between
these rows?

* facebook = [0.5, 0.4, 0.6, 0.6, 0.5, 0.6]
* myspace  = [0.2, 0.3, 0.4, 0.2, 0.1, 0.15]

You may have noticed that these similarities are **not independent** of each other. So, is it enough or appropriate method to calculate similarity between two matrices?
According to an article named Mandel Test on [Wikipedia](http://en.wikipedia.org/wiki/Mantel_test) this simple procedure is not appropriate.

&gt;Because distances are not independent of each other  since changing the ""position"" of one object would change n  1 of these distances (the distance from that object to each of the others)  we can't assess the relationship between the two matrices by simply evaluating the correlation coefficient between the two sets of distances and testing its statistical significance. The Mantel test deals with this problem.

2- How about the rank correlation techniques such as Kendall?

3- Is there more specific or proper technique to evaluate this? 

4- Mantel Test calculates correlation between two matrices. It seems it is proper for our purpose. I will read [Spatial Analysis in Ecology - Mantel's Test](http://www.nceas.ucsb.edu/scicomp/Dloads/SpatialAnalysisEcologists/SpatialEcologyMantelTest.pdf) which can be reached by Wikipedia but could anyone who use this method for same reason share his/her experience?",8,19
46,2012-3-22,2012,3,22,3,r72vp,Question about gradient descent,https://www.reddit.com/r/MachineLearning/comments/r72vp/question_about_gradient_descent/,radicality,1332354101,"Hi guys! I have an exam in ML tomorrow, and thought you guys maybe might help me with a question. It's not a specific problem from an exercise sheet, but I'm a bit confused nevertheless. I've already typed it out, [here it is](http://math.stackexchange.com/questions/122977/batch-vs-incremental-gradient-descent) Thanks",5,0
47,2012-3-22,2012,3,22,16,r83rv,The Rupp Report: Successful Two Product Lines Strategy For Karl Mayer,https://www.reddit.com/r/MachineLearning/comments/r83rv/the_rupp_report_successful_two_product_lines/,hhrotery,1332402234,,0,1
48,2012-3-23,2012,3,23,3,r8qmq,What's the driving math principle behind this manifold projection paper?,https://www.reddit.com/r/MachineLearning/comments/r8qmq/whats_the_driving_math_principle_behind_this/,mdraugelis,1332442422,"Data collected in high dimensions often resides on much lower dimensional space.  The lower dimension can be efficiently discovered if it's a linear mapping with a PCA projection.  However it's been shown that for natural signals (images, eeg, music, etc) the lower dimensional mapping is non-linear .

In manifold generation there are three primary steps 
1. The local neighborhood of each point is found.
2. A description of these neighborhoods is computed.
3. A low-dimensional output is computed by solving some convex optimization problem under some normalization constraints.

The paper, ""Proximity graphs for clustering and manifold learning"", attached presents a simple and intuitive approach based on Minimum Spanning Trees to adaptively selecting the neighborhood.  However, I've been at a loss to mathematically explain why their approach is valid.

I've thought about using entropy as a method to show the neighborhood enables a projection that has minimum entropy on the new space...which implies maximum concentration of data....which would be a good thing.

Any ideas??",5,9
49,2012-3-23,2012,3,23,12,r9gkh,Free Stanford online class:  Probabilistic Graphical Models (registration ends Fri.),https://www.reddit.com/r/MachineLearning/comments/r9gkh/free_stanford_online_class_probabilistic/,contrarianism,1332473105,,8,14
50,2012-3-23,2012,3,23,13,r9it2,"Ask ML: any (realtime) background removal (or motion segmentation, generally) algorithms which actually works for real world applications?",https://www.reddit.com/r/MachineLearning/comments/r9it2/ask_ml_any_realtime_background_removal_or_motion/,[deleted],1332476191,"opencv has a couple of algorithms for background removal, butt they are not of high performance. which algorithms do you know which would work decently for background removal of a typical webcam video, excluding the naive methods like the use of chroma key.",1,0
51,2012-3-23,2012,3,23,19,r9t7j,Under what conditions would it be logical to choose SVM over Boosted Regression Trees (BRT) or CART?,https://www.reddit.com/r/MachineLearning/comments/r9t7j/under_what_conditions_would_it_be_logical_to/,duckandcover,1332499752,"I've studied both to some degree and to the extent I've used them I always seem to end up using BRT or Boosted Cart.   

The subspace and and subset techniques, and a small learning rate, seem to make what would seem a recipe for overfitting, due to univariate partitioning, work well(1) and I don't have to normalize data or worry to much about missing features like I do for SVM(2) nor do I have try and divine the optimal C or the best Kernal etc.  I imagine that SVM is better when the number of observations are really small but beyond that I just haven't seen it yet.  

I imagine that there's a whole world of specialized kernels for special problems but for I don't see the point of SVM for typical generic supervised learning problems. 

Flame away.


1) I had thousands of features.  I used the max squares stat used to select the variable at each node to show me which features BRT had deemed important.  Then I took just the top ones (e.g. 20 to 50 of features) and retrained getting almost the same test set ROC.  In fact, most of the time the original kitcken sink model worked better (insignificantly but it was consistent)

2) - Example: One of my features was a standard deviation of the size of some found defect but sometimes, i.e. for some observations, there were no such defects so there was no std. dev.  For BRT I just put in a flag value (e.g. -1) but for SVM I'm not sure what I'd do.

**EDIT**

I guess my ultimate desire from this post is to get a rule of thumb as to when it might be preferable to use SVM (besides really small data sets).  Some of my best friends use SVM but when I ask them why the don't seem to be able to articulate any benefits and yet I know lots of people use them so I assume I must be missing something.",17,21
52,2012-3-24,2012,3,24,0,ra3cj,Question about linear SVMs and libsvm,https://www.reddit.com/r/MachineLearning/comments/ra3cj/question_about_linear_svms_and_libsvm/,[deleted],1332517934,"Hi guys! I have a quick question about linear SVMs. 

I was always under the impression that a linear SVM splits the original feature space into two parts using a hyperplane in the original feature space (i.e. does not need to be mapped to higher dimensional space). If this is true, then why would I need support vectors or a kernel at all? Shouldn't my output just be a vector of weights (one for each feature dimension) and a bias term to define the hyperplane? When I try to use a linear SVM in libsvm, I'm getting lots of support vectors out and support vector weights. Something like 9000 support vectors, with a weight coefficient for each. I should really only need 1100 weights (the number of dimensions of my feature space) plus a bias term, right? Am I missing something obvious?

Thanks!",3,5
53,2012-3-24,2012,3,24,21,rbbie,A geek with a hat  ml-class.org vs. real world ML class,https://www.reddit.com/r/MachineLearning/comments/rbbie/a_geek_with_a_hat_mlclassorg_vs_real_world_ml/,epyuiy,1332591516,,4,0
54,2012-3-25,2012,3,25,20,rcl89,"Stanley Chisels                        

Stanley Planes

Stanley Surform
",https://www.reddit.com/r/MachineLearning/comments/rcl89/stanley_chisels_stanley_planes_stanley_surform/,seolinkexpert89,1332675872,,0,1
55,2012-3-26,2012,3,26,2,rcwee,Probabilistic Topic Models | CACM,https://www.reddit.com/r/MachineLearning/comments/rcwee/probabilistic_topic_models_cacm/,tyrial,1332698255,,0,22
56,2012-3-27,2012,3,27,0,re9o5,Factuals Gil Elbaz Wants to Gather the Data Universe,https://www.reddit.com/r/MachineLearning/comments/re9o5/factuals_gil_elbaz_wants_to_gather_the_data/,jdw25,1332774522,,0,22
57,2012-3-27,2012,3,27,3,rejn0,Question about Radial Basis Function Kernels for SVM,https://www.reddit.com/r/MachineLearning/comments/rejn0/question_about_radial_basis_function_kernels_for/,[deleted],1332786757,"There are a couple things about kernels (specifically the RBF kernel) that I am wondering about.

First, I know that for a kernel to be valid, it most be symmetric and positive semi-definite. How can you prove that the RBF kernel has those qualities?

Second, for the RBF in form: 

K(xi,xj) = exp(-B||xi-xj||^2) 

How can you prove that any dataset is separable for large enough values of B?

Thanks for any help or insights on these inquiries!",5,6
58,2012-3-27,2012,3,27,5,reom6,Model to predict drop-in attendance at a tutoring center?,https://www.reddit.com/r/MachineLearning/comments/reom6/model_to_predict_dropin_attendance_at_a_tutoring/,esg43,1332792511,"Hello, r/MachineLearning.  
I am undertaking a project to try to predict attendance at a tutoring center (unlimited drop-in).  I have about 1.5 years of data with the following fields: Student ID, Arrival Time, Duration of stay, Grade, School.

My ultimate goal is to predict the days and times of attendance of each student.  I have trained a model which estimates a probability of attendance on a particular day based on

1. What percentage of times the student has been present that day of the week.

and

2. What percentage of the time the student has been present on a particular day in the previous 2 weeks.

The model then comes up with a weighted probability of attendance based on the aforementioned statistics, and then estimates the time of arrival as the first time interval that we have seen the student more than 50 percent of the times on that day.  What I mean here is that we maybe see the student 50 percent of the time between 3PM and 3:30PM on Wednesday, and so we estimate that they will arrive at that time.

What other models might work in such a situation?  Is there any literature on this that I am missing?  I have been considering Neural Networks, Logistic regression (by deriving some more features about student behavior).  
Thank you in advance for your input!",3,0
59,2012-3-27,2012,3,27,5,rer18,High Performance machine learning ... the numbers are impressive.,https://www.reddit.com/r/MachineLearning/comments/rer18/high_performance_machine_learning_the_numbers_are/,gfuzzball,1332795133,,6,0
60,2012-3-27,2012,3,27,6,ret81,Google summer of code 2012  and R  a call for students (please help to share),https://www.reddit.com/r/MachineLearning/comments/ret81/google_summer_of_code_2012_and_r_a_call_for/,talgalili,1332797523,,0,1
61,2012-3-27,2012,3,27,15,rfj0h,Packaging for the Pharmaceutical Industry,https://www.reddit.com/r/MachineLearning/comments/rfj0h/packaging_for_the_pharmaceutical_industry/,gustavoduhamel,1332829509,,1,1
62,2012-3-27,2012,3,27,17,rflq7,Looking for efficient LOO implementation of linear discriminant classifier,https://www.reddit.com/r/MachineLearning/comments/rflq7/looking_for_efficient_loo_implementation_of/,thrope,1332835819,"I am looking to do leave-one-out crossvalidation with a basic linear or diagonal linear classifier (assume normal distribution etc.)

I have an efficient version for template matching (means only), but I am sure there must be some tricks for updating the (co)variances efficiently for each leave out sample (and maybe updating the inverse of the covariance matrix - I saw Sherman-Morrison mentioned).

Any implementation I could look at to get a head start or any other tips would be great.",0,2
63,2012-3-27,2012,3,27,22,rft3d,The sunrise problem: a classic Bayes's theorem problem (from 1763).,https://www.reddit.com/r/MachineLearning/comments/rft3d/the_sunrise_problem_a_classic_bayess_theorem/,AllenDowney,1332854748,,0,28
64,2012-3-28,2012,3,28,11,rgxvj,Probabilistic Counting,https://www.reddit.com/r/MachineLearning/comments/rgxvj/probabilistic_counting/,willis77,1332903316,,1,8
65,2012-3-28,2012,3,28,21,rhfk4,BAYES NET BY EXAMPLE USING PYTHON AND KHAN ACADEMY DATA,https://www.reddit.com/r/MachineLearning/comments/rhfk4/bayes_net_by_example_using_python_and_khan/,cavedave,1332936741,,5,8
66,2012-3-29,2012,3,29,12,rinw9,Choosing a Machine Learning Classifier,https://www.reddit.com/r/MachineLearning/comments/rinw9/choosing_a_machine_learning_classifier/,mycall,1332990048,,25,23
67,2012-3-29,2012,3,29,14,riucn,Diamond Blade: A Tool For Perfection And Elegance,https://www.reddit.com/r/MachineLearning/comments/riucn/diamond_blade_a_tool_for_perfection_and_elegance/,dimit0948,1332998683,,1,1
68,2012-3-29,2012,3,29,21,rj4a2,Book on the implementation of ML algorithms?,https://www.reddit.com/r/MachineLearning/comments/rj4a2/book_on_the_implementation_of_ml_algorithms/,cpa,1333022849,"Hi there,

I'm quite familiar with machine learning and I'm looking for a book that covers all the implementation details of ML algorithms (conditioning, numerical stability, scalability, parallel implementations...)

Does such a book exist?",19,15
69,2012-3-30,2012,3,30,0,rjbic,The Drivetrain Approach: A four-step process for building data products,https://www.reddit.com/r/MachineLearning/comments/rjbic/the_drivetrain_approach_a_fourstep_process_for/,willis77,1333034230,,0,12
70,2012-3-30,2012,3,30,7,rjzp0,Validating data?,https://www.reddit.com/r/MachineLearning/comments/rjzp0/validating_data/,[deleted],1333061262,"Let's say I have a large dataset. For example, 100,000 messages, put in 12 different categories. These messages are scraped from an online source and  someone has manually put them into these categories over the course of many years. 

What can I do to validate whether these messages have indeed been placed into the correct categories?

Is there anything machine learning can do for me?",9,6
71,2012-3-30,2012,3,30,21,rkuak,R 2.15.0 is released,https://www.reddit.com/r/MachineLearning/comments/rkuak/r_2150_is_released/,talgalili,1333112003,,0,0
72,2012-3-30,2012,3,30,22,rkw02,"What defines the ""size"" of a dataset?",https://www.reddit.com/r/MachineLearning/comments/rkw02/what_defines_the_size_of_a_dataset/,clarle,1333114797,"This might be a trivial question, but I was reading this:

http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/

I'm an undergraduate student working on a Kaggle classifying competition for a school project.  Our dataset has 2500 items, each with 5000 identifying parameters.  

My question is, what is used to define the size of a dataset?  How much data is needed before a dataset is considered ""large"", for example?  

Also, would you need to take into consideration the number of parameters that each item has, as well as possibly the statistical properties of each of those parameters?  If you have 100 parameters, but 99 of them are either too spaced out or too close together (so they're effectively useless in classification), would you consider the dataset to be small?

Thanks a lot for your help!",4,2
73,2012-3-30,2012,3,30,23,rkwvq,What is Support Vector Machine? : xpost from explainlikeimfive,https://www.reddit.com/r/MachineLearning/comments/rkwvq/what_is_support_vector_machine_xpost_from/,imissyourmusk,1333116114,,7,25
74,2012-3-31,2012,3,31,16,rm5ir,Does it make sense to perform canonical correlation analysis on two covariance matrices?,https://www.reddit.com/r/MachineLearning/comments/rm5ir/does_it_make_sense_to_perform_canonical/,[deleted],1333178603,"I'm doing basic classification and I'd like to make my training data as similar as possible to my testing data.  The idea being that I would transform my training data and train my classifier on the transformed data and then predict on data that was similarly transformed.  A thought I had, which may or may not make sense when you get down to the details, was to perform CCA on the covariance matrices of my training and testing data and then generate new data according to the result of my CCA which I'd like to interpret as a covariance matrix which maximizes the correlation between the training and testing data.  There are several potential problems with this, but I figured I'd ask here before trying to find the answers in the mathematics myself.  However, here are some things I think may be a problem:

It's quite likely that the CCA projection of two symmetric matrices is not symmetric itself and thus I wouldn't be able to interpret the result as a covariance matrix.  Also, how should I use the covariance matrix to modify my training data?  Would it even be meaningful/helpful to do so?",0,1
75,2012-3-31,2012,3,31,16,rm5k7,AskML: Does it make sense to perform canonical correlation analysis on two covariance matrices?,https://www.reddit.com/r/MachineLearning/comments/rm5k7/askml_does_it_make_sense_to_perform_canonical/,lpiloto,1333178706,"I'm doing basic classification and I'd like to make my training data as similar as possible to my testing data.  The idea being that I would transform my training data and train my classifier on the transformed data and then predict on data that was similarly transformed.  A thought I had, which may or may not make sense when you get down to the details, was to perform CCA on the covariance matrices of my training and testing data and then generate new data according to the result of my CCA which I'd like to interpret as a covariance matrix which maximizes the correlation between the training and testing data.  There are several potential problems with this, but I figured I'd ask here before trying to find the answers in the mathematics myself.  However, here are some things I think may be a problem:

It's quite likely that the CCA projection of two symmetric matrices is not symmetric itself and thus I wouldn't be able to interpret the result as a covariance matrix.  Also, how should I use the covariance matrix to modify my training data?  Would it even be meaningful/helpful to do so?",5,6
76,2012-3-31,2012,3,31,17,rm6p1,TN textile industry unhappy with power tariff hike,https://www.reddit.com/r/MachineLearning/comments/rm6p1/tn_textile_industry_unhappy_with_power_tariff_hike/,hhrotery,1333182049,,0,1
