,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,10,4ct4sk,self.MachineLearning,Alex Lamb will be doing an AMA in /r/MachineLearning on April 1,https://www.reddit.com/r/MachineLearning/comments/4ct4sk/alex_lamb_will_be_doing_an_ama_in/,olaf_nij,1459473181,"Because why not....

A thread will **not** be created before the official AMA time for those who won't be able to attend on that day.

Maybe /u/alexmlamb will answer question posted here.

[Special thanks goes to /u/NasenSpray](https://www.reddit.com/r/MachineLearning/comments/4bqeuh/meta_should_this_sub_be_split_in_two/d1bntjb)",82,70,False,self,,,,,
1,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,10,4ct8hs,self.MachineLearning,[Question] Tag Chunks in Text using CRF,https://www.reddit.com/r/MachineLearning/comments/4ct8hs/question_tag_chunks_in_text_using_crf/,brunoalano,1459474708,"Hello, I've a Sentence, where each Token `t` of this Sentence has lemma and stem. I apply a set of preprocessing tools, like Dependency Parser and Constituency Parser and I'm trying to identify some ""special"" chunks, that is, chunks that apply some information to the main action.

For example: ""I want to buy a new car, giving 3000 as entrance and paying in 30 parcels"" as sentence, should: ""giving 3000 as entrance"", I want to tag this part of text as ""ENTRANCE"" category. ""paying in 30 parcels"", should be tagged as ""PARCEL"".

Sadly I should manually do that, because in my language I can't apply something like SPF, to knowledge representation and extraction.

So, should I use CRF for this task? HMM? Is ""structured learning"" the best pratice for that, because I aim to process all the tokens, not only the chunks, since the Chunker can fail.",0,1,False,self,,,,,
2,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,10,4ctb44,yoavz.com,Music Language Modeling with Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4ctb44/music_language_modeling_with_recurrent_neural/,[deleted],1459475802,[deleted],0,1,False,default,,,,,
3,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,11,4ctd55,yoavz.com,Music Language Modeling with Recurrent Neural Networks in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4ctd55/music_language_modeling_with_recurrent_neural/,yoavz,1459476593,,2,34,False,http://b.thumbs.redditmedia.com/13GDaxybNzHVCIPf6QMLbW0riZ4dV17dde5yJ_TAvjg.jpg,,,,,
4,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,11,4cthdy,clean-equipment.com,My phun p lc cao Nilfisk CHNH HNG | CLEAN,https://www.reddit.com/r/MachineLearning/comments/4cthdy/my_phun_p_lc_cao_nilfisk_chnh_hng_clean/,tkmailseo,1459478282,,1,0,False,default,,,,,
5,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,13,4ctti4,microsoft.com,Microsoft's Face and Emotion Recognition App,https://www.reddit.com/r/MachineLearning/comments/4ctti4/microsofts_face_and_emotion_recognition_app/,[deleted],1459483356,[deleted],0,1,False,default,,,,,
6,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,13,4ctyqv,self.MachineLearning,Help regarding implementing and visualizing autoencoders,https://www.reddit.com/r/MachineLearning/comments/4ctyqv/help_regarding_implementing_and_visualizing/,Alirezag,1459485649,"I am very new to all deep learning as well as using python.
I apologize if my question is very simple or not smart.
So I decide to give a try to auto-encoders (AE) and see how they work.
I completely understand the concept and theory behind it btw.

The best and neat ae code that I find was here: http://deeplearning.net/tutorial/dA.html and the code (dA.py) provided: http://deeplearning.net/tutorial/code/dA.py
it is a denoising AE, and I am fine with that.

So I run the dA.py code and it works fine.
I am trying to run the dA on just a single image (my image is not a binary image and its size is 28*28) and visualize my hidden layer (weights) and see the reconstructed image as well.
first I'd like to run it of dA and then on sda (stacked AE) and visualize all the layers.
I really appreciate it someone can help me. 

I try to write some lines of code and I did, However, I dont know the next step.
So In the following, I am importing the autoencoder and define a grayscale image. i call the dA fuction and my code works fine. but the next step that I want to see the reconstructed image and visualize the weights of my layer (or layers for SAE), i dont know how to do that.

Here is the code that I write so far:

'''import PIL.Image as Image

import dA

rng = dA.numpy.random.RandomState(123)

theano_rng = dA.RandomStreams(rng.randint(2 ** 30))

Img2 = dA.Image.open(""fruits.jpg"").convert('LA')

index = dA.T.lscalar()  # index to a [mini]batch

x = dA.T.matrix('x')


da = dA.dA(
    numpy_rng=rng,
    theano_rng=theano_rng,
    input=Img2,
    n_visible=28 * 28,
    n_hidden=500
)'''

so it works fine without any error, but I dont know what to do next.

I'd be so grateful If you can help me out, also please feel free to mention any related cm or link that you think can be a good example.
",0,0,False,self,,,,,
7,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,13,4cu04l,somatic.io,Check out the potential for Deep learning! (Somatic.io),https://www.reddit.com/r/MachineLearning/comments/4cu04l/check_out_the_potential_for_deep_learning/,DemBones85,1459486301,,0,0,False,http://b.thumbs.redditmedia.com/BWvQNcpZVj1JjKlxvOyvRBnAZ_Nhzn_2wCBKQS0Vzns.jpg,,,,,
8,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,17,4cumef,youtube.com,The next revolution in photo search.,https://www.reddit.com/r/MachineLearning/comments/4cumef/the_next_revolution_in_photo_search/,charlie0_o,1459498264,,5,0,False,http://b.thumbs.redditmedia.com/8FZgNCrhRcCKivnISneeIiObTZ-DYzlVA8GCqGm42dM.jpg,,,,,
9,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,17,4cunnu,esa.int,Machine Learning Competition to prolong ESA's Mars Express mission life,https://www.reddit.com/r/MachineLearning/comments/4cunnu/machine_learning_competition_to_prolong_esas_mars/,Cesans,1459499081,,6,13,False,http://b.thumbs.redditmedia.com/V6QLesvwcs_-HnVD5SJoV4ZAKA_8D594BlXlviv5Ato.jpg,,,,,
10,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,18,4curk5,arxiv.org,[1603.09382v1] Deep Networks with Stochastic Depth,https://www.reddit.com/r/MachineLearning/comments/4curk5/160309382v1_deep_networks_with_stochastic_depth/,Inori,1459501618,,16,53,False,default,,,,,
11,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,19,4cuyum,self.MachineLearning,Question: Neural Networks for Time Series Data Classification,https://www.reddit.com/r/MachineLearning/comments/4cuyum/question_neural_networks_for_time_series_data/,Newti,1459506154,"Ill make a very concrete, simplified example of the kind of data I would like to explore. It does not have to be financial data at all, but the structure is important.

* Yearly key financial figures from companies (assets, liabilities, cashflows, etc).
* 2 labels: Financial Distress (yes/no)
* Every period (year) has one of the two labels associated
* Assume 10 periods

The obvious question to ask: ""Looking at the last X years of a company with label NO, what is the chance it will change to YES in the next Y years?""

Traditional methods use some form of feature extraction (dynamic time warping, shapelet transform, etc) and then apply ""regular"" classifiers. However this falls short on really ""connecting"" subsequent periods.

RNNs and LSTMs have this property of being able to directly incorporate previous periods into the evaluation of the following periods. I was looking for research being published in this area but I could not dig up much (short of maybe http://arxiv.org/abs/1603.06995v1)

If anyone could refer me to recent articles on this topic (LSTM for time series, not necessarily financial data) or if anyone has ideas on the topic I would be very glad to hear them!

",10,1,False,self,,,,,
12,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,19,4cuz36,self.MachineLearning,Neural topic models,https://www.reddit.com/r/MachineLearning/comments/4cuz36/neural_topic_models/,cjmcmurtrie,1459506285,"I have been surveying a number of papers on various topic models that can be trained with backprop, including the below.

Are you aware of any further research in this area?

Are there any memorable repositories that have implemented these or similar models? Bonus upvotes if they are in Lua/Torch!

****

[A Neural Autoregressive Topic Model](http://www.dmi.usherb.ca/~larocheh/publications/nips_2012_camera_ready.pdf)

[A Novel Neural Topic Model and Its Supervised Extension](https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiToP60j-3LAhXMtBQKHTHQC-sQFgghMAA&amp;url=https%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI15%2Fpaper%2Fdownload%2F9303%2F9544&amp;usg=AFQjCNGDgYlifcy4DgP1NsnRxWy2TpQZuA&amp;sig2=HzHqP3ZeHeYvT-1Jy05LMw&amp;bvm=bv.118443451,d.ZWU)

[A Hybrid Neural Network-Latent Topic Model](https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiToP60j-3LAhXMtBQKHTHQC-sQFggpMAE&amp;url=https%3A%2F%2Fwww.cs.nyu.edu%2F~wanli%2Fwan-zhu-fergus12.pdf&amp;usg=AFQjCNFKloLzVJxwnJtMFXLVDOZvodb7HQ&amp;sig2=fBQouYe7fBdwtaSpObpa9w&amp;bvm=bv.118443451,d.ZWU)

[Modeling Documents with a Deep Boltzmann Machine](https://www.cs.toronto.edu/~hinton/absps/deepBMdocs.pdf)",3,3,False,self,,,,,
13,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,19,4cuz5c,economist.com,"As Silicon Valley fights for talent, universities struggle to hold on to their stars",https://www.reddit.com/r/MachineLearning/comments/4cuz5c/as_silicon_valley_fights_for_talent_universities/,pmigdal,1459506325,,21,57,False,http://b.thumbs.redditmedia.com/AWuGv5I60CfkxpPKARaSF2yQLjvmwcdLXPDvKyCPitE.jpg,,,,,
14,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,19,4cv176,self.MachineLearning,[Question] Calculate trigram Probabilities from Language Model,https://www.reddit.com/r/MachineLearning/comments/4cv176/question_calculate_trigram_probabilities_from/,Mugiwara_Luffy,1459507520,"I am trying to generate trigrams using a Language Model toolkit.

Given any three words, can i calculate their trigram probabilities using the [CMU Language Model Toolkit](http://www.speech.cs.cmu.edu/SLM/toolkit.html) by using a large corpus as a training set.

I am working on Windows with Python 2.7",0,0,False,self,,,,,
15,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,20,4cv3af,machiningpartner.com,Machining Partner  The Bridge Between Buyers and Suppliers,https://www.reddit.com/r/MachineLearning/comments/4cv3af/machining_partner_the_bridge_between_buyers_and/,masumnetcox,1459508741,,0,1,False,default,,,,,
16,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,20,4cv3v8,self.MachineLearning,Anyone knows of a public dataset of videos/audio with people showing natural emotions while speaking for an emotion classifier from speech?,https://www.reddit.com/r/MachineLearning/comments/4cv3v8/anyone_knows_of_a_public_dataset_of_videosaudio/,carlos_argueta,1459509077,"Hi all,
Anyone knows of a public dataset of videos/audio with people showing natural emotions while speaking? I have found a few but mostly acted or induced and they are very bad. Others are only available for academic research but I m not in academia. Annotated dataset preferred but not annotated is also fine. The goal is to train an emotion classifier from speech.

Thanks a lot",2,1,False,self,,,,,
17,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,21,4cv9ef,xkcd.com,This xkcd was released less than 2 years ago..,https://www.reddit.com/r/MachineLearning/comments/4cv9ef/this_xkcd_was_released_less_than_2_years_ago/,testic,1459512009,,88,353,False,http://b.thumbs.redditmedia.com/xW3PqlFZDW_x_BguR3cVfo5wwyeGPbUdgb-Ut34JmKU.jpg,,,,,
18,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,21,4cv9mr,self.MachineLearning,[Question] Scaling behavior of LSTM model complexity with data set size,https://www.reddit.com/r/MachineLearning/comments/4cv9mr/question_scaling_behavior_of_lstm_model/,SuperFX,1459512118,"Im observing somewhat strange behavior in terms of how model accuracy is changing with respect to model size in a RNN. My problem is simple: Im mapping one sequence into another, both of equal length, and so its a classification problem for each time step. Inputs are ~50D (one-hot and continuous) and outputs are ~10 classes. Im using a single-layer bidirectional LSTM with peepholes. Data set is around 5000 sequences, each of which is about 300 time steps long, and so given that Im making a classification for each time step and normalizing the loss so that the weight of each sequence is proportional to its length, it would _seem_ that I have something like 5000 x 300 = 1,500,000 data points.

I initially started out with medium-sized LSTM layers (~500 units each direction) and got OK results (about 70% accuracy for 10-way classification). The results are actually good in relative terms in that SOTA is around there, but not so great in absolute terms, in that one wants to get much closer to 100%. Whats surprising however is that as I made the net smaller and smaller, performance more or less stayed constant, all the way down to 50 units, with which Im still getting ~70% accuracy. Taking it down to 25 units shaves off a few percentage points, and taking it further down to 5 units shaves off less than 10% percentage points, to around 63%! The output classes are not uniformly distributed with the most popular occurring about 30% of the time, but still 63% is no slouch. At 5 units I have about 1000 parameters when all is said and done, which is less than the number of _sequences_, let alone the number of time steps for which Im making predictions. (At 25 units I have ~10,000 parameters). It would seem that the data set size should permit much larger model capacities, no? Is there something wrong with my reasoning? it seems like Im maxing out way too early in terms of model complexity.

Incidentally, the mapping from one sequence to another is deterministic, and while I dont know the underlying function, I do know that one exists even though its likely very complex. So its not a problem of the inputs not being predictive of the outputs.",10,3,False,self,,,,,
19,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,21,4cvcae,self.MachineLearning,[Question] CNTK. Evaluate performance with code?,https://www.reddit.com/r/MachineLearning/comments/4cvcae/question_cntk_evaluate_performance_with_code/,Blixtbob,1459513333,"In an attempt to learn I'm trying to train a neural network to solve rubiks cube. My idea was to present a state for the network and let him choose next move. Scoring simple by how many same colors on each side he manages to do. 

The problem here is that I can't pregenerate training data. Sure I can generate the first state but in order to evaluate the performance I actually need to simulate the cube. 

It looks like I can only train on pregenerated static data? Isnt it possible to write code for the evaluation? Is there any suggestions for any other framework than CNTK where I can do something like this?",0,0,False,self,,,,,
20,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,21,4cvdz6,re-work.co,"We asked deep learning and healthcare experts for their predictions for the next 5 years, the risks involved with AI integration &amp; areas for disruption",https://www.reddit.com/r/MachineLearning/comments/4cvdz6/we_asked_deep_learning_and_healthcare_experts_for/,reworksophie,1459513998,,0,1,False,default,,,,,
21,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,21,4cvfkh,self.MachineLearning,[Q] Recurrent autoencoder?,https://www.reddit.com/r/MachineLearning/comments/4cvfkh/q_recurrent_autoencoder/,TamisAchilles,1459514668,I was wondering if such a thing exists?? For example in order to encode a time series into a fixed size vector representation. ,7,6,False,self,,,,,
22,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,21,4cvg7c,nextplatform.com,"Baidus Chief Scientist on Intersection of Supercomputing, Machine Learning",https://www.reddit.com/r/MachineLearning/comments/4cvg7c/baidus_chief_scientist_on_intersection_of/,[deleted],1459514940,[deleted],0,0,False,default,,,,,
23,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,22,4cvj39,self.MachineLearning,Artworks related to machine learning?,https://www.reddit.com/r/MachineLearning/comments/4cvj39/artworks_related_to_machine_learning/,Professional_123,1459516065,"Hello,

I just moved into a new place and I want to hang some pictures up on the wall! Do you where I can find ML related pictures?

I think deep dream pics are too creepy. But I'm willing to put one up so I can explain to people what they are :)",6,0,False,self,,,,,
24,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,23,4cvvf4,self.MachineLearning,Where to study machine learning summer camp around US,https://www.reddit.com/r/MachineLearning/comments/4cvvf4/where_to_study_machine_learning_summer_camp/,unkn0wn12345,1459520636,,0,0,False,self,,,,,
25,MachineLearning,t5_2r3gv,2016-4-1,2016,4,1,23,4cvx97,self.MachineLearning,what's a nice dataset to train and test a deep neural network for classification?,https://www.reddit.com/r/MachineLearning/comments/4cvx97/whats_a_nice_dataset_to_train_and_test_a_deep/,Aumanidol,1459521301,"I want to implement a residual deep neural network (http://arxiv.org/pdf/1512.03385v1.pdf ) in theano, and mess around with it a bit.

Do you know any cool dataset to test it without having too much trouble in analizing it? my goal is classification.

I was looking in the audio field, but most music ones I found require a lot of operation related to managing and extracting the data, but I'm most interested in working with the NN (this is the first time I use them).

",7,2,False,self,,,,,
26,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,0,4cw9oj,merl.com,"MERL Researchers Create ""Deep Psychic"" Neural Network That Predicts the Future",https://www.reddit.com/r/MachineLearning/comments/4cw9oj/merl_researchers_create_deep_psychic_neural/,CptnCat,1459525471,,1,8,False,default,,,,,
27,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,0,4cwatn,cloudplatform.googleblog.com,"Google Style Detection: Using millions of hours of deep learning, convolutional neural networks and petabytes of source data, Vision API can now not just identify clothing, but evaluate the nuances of style to a relative degree of uncertainty.",https://www.reddit.com/r/MachineLearning/comments/4cwatn/google_style_detection_using_millions_of_hours_of/,__ML__,1459525850,,0,1,False,default,,,,,
28,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,1,4cwcqx,self.MachineLearning,"Machine Learning blogs in Spanish, Dutch, or Polish",https://www.reddit.com/r/MachineLearning/comments/4cwcqx/machine_learning_blogs_in_spanish_dutch_or_polish/,iforgot120,1459526480,"I want to practice my Dutch, Spanish, and Polish by reading blogs that are about subjects I'm interested in, including machine learning (or tech in general, really). Does anyone know of any good blogs in any of these languages that they can recommend?",1,0,False,self,,,,,
29,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,1,4cwdtd,qu.edu.sa,Qassim University,https://www.reddit.com/r/MachineLearning/comments/4cwdtd/qassim_university/,qassim11,1459526806,,1,1,False,default,,,,,
30,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,1,4cwh1d,self.MachineLearning,classification and clustering,https://www.reddit.com/r/MachineLearning/comments/4cwh1d/classification_and_clustering/,ikennarene,1459527751,[removed],0,1,False,default,,,,,
31,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,1,4cwhri,self.MachineLearning,Stateful conversational bot,https://www.reddit.com/r/MachineLearning/comments/4cwhri/stateful_conversational_bot/,ReLeVaNtUs_ername,1459527957,"Hey /r/MachineLearning!

I've seen a lot of interest recently in chatbots, and I'm interested by them a lot too. 

I've looked at all the AIML bots like ALICE et al, and I don't think a pattern matching chatbot is the best thing. 
I also looked at ""A Neural Conversational Model"", and while I thought that bot was better, there was still no real flow in the conversation. 

I'm curious. If I wanted to build a chatbot that would be better than the one described in ""A Neural Conversational Model'"", what would I need to do?

For instance, how would I store little tidbits about the person I'm chatting to? Within the neural network for instance, or in a database externally? How would you base current responses while keeping the previous state of the conversation in mind? 

I know all of this is still very much a research topic, but if someone could give me some information and insight into the topic, and even point me to some research papers, I'd be incredibly thankful.",5,4,False,self,,,,,
32,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,2,4cwpk1,blog.otoro.net,Generating Large Images from Latent Vectors,https://www.reddit.com/r/MachineLearning/comments/4cwpk1/generating_large_images_from_latent_vectors/,hardmaru,1459530301,,2,77,False,http://b.thumbs.redditmedia.com/tke4skZHUvybaCmjzCZuytJQj90GngAJg7T673TGf1o.jpg,,,,,
33,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,2,4cwwg3,media.bemyapp.com,7 Tips to Get Your Machine Learning Startup Off the Ground,https://www.reddit.com/r/MachineLearning/comments/4cwwg3/7_tips_to_get_your_machine_learning_startup_off/,tony_sf,1459532249,,0,2,False,default,,,,,
34,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,2,4cwwox,self.MachineLearning,Why isn't Apple investing more in ML?,https://www.reddit.com/r/MachineLearning/comments/4cwwox/why_isnt_apple_investing_more_in_ml/,[deleted],1459532316,[deleted],10,16,False,default,,,,,
35,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,2,4cwy7s,github.com,GloVe Word Embedding in theano,https://www.reddit.com/r/MachineLearning/comments/4cwy7s/glove_word_embedding_in_theano/,shash273,1459532760,,0,10,False,http://b.thumbs.redditmedia.com/ndQJIhZSyYRWKE1OYorOSifwf_2eEAfzf_rOYtpThPs.jpg,,,,,
36,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,2,4cx0k9,youtube.com,Starcraft simple Neural Network testing,https://www.reddit.com/r/MachineLearning/comments/4cx0k9/starcraft_simple_neural_network_testing/,Sportinger,1459533447,,3,15,False,http://a.thumbs.redditmedia.com/w8jU0WDltRuBg6fBVIM9sAmx_OCtCATgKeWM0NXb2m0.jpg,,,,,
37,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,4,4cxfmq,i.imgur.com,Kaggle's April Fool's page,https://www.reddit.com/r/MachineLearning/comments/4cxfmq/kaggles_april_fools_page/,iamaaditya,1459537707,,2,21,False,http://b.thumbs.redditmedia.com/rL8ecFqlIRxNtzRtnC3sU7rVu2jSLq-9xDI9jL_8Q9w.jpg,,,,,
38,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,5,4cxst7,arxiv.org,[1603.09727] Neural Language Correction with Character-Based Attention (Stanford),https://www.reddit.com/r/MachineLearning/comments/4cxst7/160309727_neural_language_correction_with/,InaneMembrane,1459541415,,0,14,False,default,,,,,
39,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,5,4cxyly,self.MachineLearning,Modelling two sets of features using Deep Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4cxyly/modelling_two_sets_of_features_using_deep_neural/,utkarshsimha,1459543084,"I am using Deep Neural Networks to try and perform classification (On speech dataset). I'm getting above average accuracies and would like to improve it further by providing another set of features (say length of each phoneme or something). AFAIK, adding this to the existing MFCC features would be skewing the pattern trying to be recognised. 
I'm a bit weak on my theory - So I wanted to know if it is viable to have two DNNs trained on these two sets of features separately, and their output (which is the same) is multiplied to get the resultant output?
out = out_dnn1 * out_dnn2 (As they are probabilities [softamax output layer])",2,2,False,self,,,,,
40,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,5,4cxyp8,arxiv.org,"Johnson, Karpathy and FeiFei on image captioning CNN-RNN",https://www.reddit.com/r/MachineLearning/comments/4cxyp8/johnson_karpathy_and_feifei_on_image_captioning/,Tamazy,1459543108,,1,18,False,default,,,,,
41,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,5,4cxzld,x.ai,An AI assistant that schedules meetings for you,https://www.reddit.com/r/MachineLearning/comments/4cxzld/an_ai_assistant_that_schedules_meetings_for_you/,szalonymjut,1459543379,,0,0,False,http://a.thumbs.redditmedia.com/E3Sg1VaTNbyLDALyKHJ4DdwvDFRcnTYvM-Ouj0_BI50.jpg,,,,,
42,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,6,4cyecm,youtube.com,DanDoesData: A special look at Excel97,https://www.reddit.com/r/MachineLearning/comments/4cyecm/dandoesdata_a_special_look_at_excel97/,vanboxel,1459547963,,0,0,False,default,,,,,
43,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,7,4cygfm,self.MachineLearning,How much does quality of grad school matter in being able to do industry research after your PhD?,https://www.reddit.com/r/MachineLearning/comments/4cygfm/how_much_does_quality_of_grad_school_matter_in/,imdirtysocks45,1459548646,"I'm still a sophomore in college and will probably do a MS in CS first unless I get really lucky, but would it still be worth it to do a PhD at a lesser known institution and research on ML if my goal is industry research? I really don't want to go into academia. ",14,7,False,self,,,,,
44,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,8,4cytf8,petapixel.com,Convolutional neural network used to create automatic colorizations of BW pictures,https://www.reddit.com/r/MachineLearning/comments/4cytf8/convolutional_neural_network_used_to_create/,[deleted],1459553040,[deleted],0,1,False,default,,,,,
45,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,12,4czwnh,bitfusion.io,Bitfusion Boost 16 GPU Caffe Cluster - Quick Start on Amazon AWS with CloudFormation,https://www.reddit.com/r/MachineLearning/comments/4czwnh/bitfusion_boost_16_gpu_caffe_cluster_quick_start/,mtweak,1459568720,,3,10,False,http://b.thumbs.redditmedia.com/xMBgiXnIlJdEwTVahfxmImumJOgrXyxVcep9UxHeSwI.jpg,,,,,
46,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,16,4d0kt7,gist.github.com,"Notes for ""Large Scale Distributed Deep Networks"" (DisBelief) paper.",https://www.reddit.com/r/MachineLearning/comments/4d0kt7/notes_for_large_scale_distributed_deep_networks/,shagunsodhani,1459581237,,1,8,False,http://b.thumbs.redditmedia.com/lvJHflIAg4deAzTfFTD0okJa1RZ1OQ5VAGKhaVWo8OQ.jpg,,,,,
47,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,19,4d10no,self.MachineLearning,Why my text generator doesn't work?,https://www.reddit.com/r/MachineLearning/comments/4d10no/why_my_text_generator_doesnt_work/,machine-learning,1459592709,"Hi guys, I am a newbie in RNN field.

I want to train a LSTM RNN for generating text sequences.

However, it seems to generate only one sentence...

More strictly, the first sentence with the start character ""A"" was ""An apple is two."". However after that sentence (after second one), RNN always generated ""The sign was accepted by Morris."".

Anyone knows the reason?? 

Help me plz.... I spend 8 days to solve this problem...  ",6,0,False,self,,,,,
48,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,22,4d1gv5,self.MachineLearning,Input dimensions,https://www.reddit.com/r/MachineLearning/comments/4d1gv5/input_dimensions/,Nalamotse,1459603261,[removed],0,1,False,default,,,,,
49,MachineLearning,t5_2r3gv,2016-4-2,2016,4,2,23,4d1s2l,self.MachineLearning,"Tentative novel unsupervised learning algorithm, I need help implementing it, willing to pay.",https://www.reddit.com/r/MachineLearning/comments/4d1s2l/tentative_novel_unsupervised_learning_algorithm_i/,bkaz,1459608814,[removed],16,0,False,default,,,,,
50,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,1,4d29jd,self.MachineLearning,Please help me find a good book on neural networks?,https://www.reddit.com/r/MachineLearning/comments/4d29jd/please_help_me_find_a_good_book_on_neural_networks/,Smileynator,1459616123,"So i am trying to implement my first neural network. Went for Tanh implementation (so all nodes go from -1 to +1) because most documentation stated that it is superior to linear 0-1 or the same thing as tanh with 0-1 output.

I am having a hard time grasping most documentation because it generates questions i can't resolve with the documentation, and all the big algebra stuff you usually find on wikipedia is voodoo to me most of the times.

I found 1 document that is clear as day, this one:
https://web.archive.org/web/20150317210621/https://www4.rgu.ac.uk/files/chapter3%20-%20bp.pdf
It explained backprop and error calculation well.
However the math doesn't seem to add up on it. If i somewhere end up with an output or an error being 0, it starts messing up because of their output * (1-output) thing, a 0 output would cause the error to be 0 always (even when 0 output isn't what i want at all!) It also does not explain how to add/use/calculate bias. And i understand that those are important for various reasons (which aren't all that clear either)

So please help me find a book that explains stuff like this one, i haven't been able to find any myself. Most are really complex, or assume i know a ton about neural networks and algebra from the get go. I need something to teach me back to front. I'd be willing to buy a good book if it is formatted like the one i linked here, but with the better type of algorithms in it.

Thanks in advance!",15,12,False,self,,,,,
51,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,2,4d2c92,self.MachineLearning,How to get ideal number of clusters while doing text clustering with kmeans?,https://www.reddit.com/r/MachineLearning/comments/4d2c92/how_to_get_ideal_number_of_clusters_while_doing/,mln00b13,1459617173,"I know I can do cross validation and try with certain possible values of clusters, but I do not have a defined training data set. I tried with mean shift, but it is not as accurate as kmeans. What I want is fully unsupervised, without me needing it to tell number of clusters to form, with the accuracy of kmeans. Is it possible? Currently I am using Scikit to do this in Python.",4,0,False,self,,,,,
52,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,2,4d2cqq,govtech.com,The Pentagon Eyes Militarisation of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4d2cqq/the_pentagon_eyes_militarisation_of_deep_learning/,cjmcmurtrie,1459617383,,35,75,False,http://a.thumbs.redditmedia.com/hmK5jsORH64lQu9GSMNgqtydXYp7jMQe2dtvECIDDx4.jpg,,,,,
53,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,2,4d2esu,self.MachineLearning,Methods for learning complex motor skills?,https://www.reddit.com/r/MachineLearning/comments/4d2esu/methods_for_learning_complex_motor_skills/,omphalos,1459618196,"What are some good resources for learning about this topic?  Human beings have over 600 muscles in the body, the ability to coordinate actions, learn fine motor skills, compose them together to create other motor skills (playing the piano) and so on.  I'm only familiar with reinforcement learning algorithms where decisions are typically modeled as simple multi-arm bandits as far as I know.  Is there anything more sophisticated out there?",6,4,False,self,,,,,
54,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,2,4d2ghp,self.MachineLearning,Viewing attention outputs over an image?,https://www.reddit.com/r/MachineLearning/comments/4d2ghp/viewing_attention_outputs_over_an_image/,logrech,1459618867,"My attention model computes probabilities over the filter outputs of the last convolutional layer. 

After multiple convolution and max pooling layers, the original image starts out as 224x224 and ends up being 14x14. As such, the attention model outputs 196 probabilities. 

I'm wondering if there's an easy way to upscale the probabilities to the original image so I can view the attention outputs or would you simply have to undo all the layers. 

What do most researchers do? ",4,3,False,self,,,,,
55,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,2,4d2isx,self.MachineLearning,Theano Question: Looping through tensor,https://www.reddit.com/r/MachineLearning/comments/4d2isx/theano_question_looping_through_tensor/,aiisfordummies,1459619787,"Does anyone know of a good way to loop through a transformed array in Theano?

For example:
train_x = Shared(mydataset_x)

z = encode(train_x)

I now want to do something like this:

new_z = []

for i in z :

     Do stuff with i

     new_z.append(new_i)

Thanks!",14,1,False,self,,,,,
56,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,5,4d331k,arxiv.org,"Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing",https://www.reddit.com/r/MachineLearning/comments/4d331k/convolutional_networks_for_fast_energyefficient/,mere_mortise,1459627762,,20,11,False,default,,,,,
57,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,5,4d3a6r,self.MachineLearning,Is the universe a recursive neural network?,https://www.reddit.com/r/MachineLearning/comments/4d3a6r/is_the_universe_a_recursive_neural_network/,syncoPete,1459630730,"I was reading about Nick Bostrom's simulation hypothesis (the hypothesis that we and the universe are part of a computer simulation). It has led me down another line of thought.

1.  My brain (a neural network) is (as far as I know) the source of everything I experience. What it generates may be prompted by external stimuli (when I'm awake), or not (e.g. when I'm asleep). The brain is able to produce a universe on demand, whether in dream or in reality.

2.  The brain is reproducible as a computational system. In the future, we will produce computers that are like the brain, computers that therefore experience internal universes like mine. These internal worlds may be connected to the outside world (like mine is when I'm awake), or not (like when I'm asleep).

3.  Those computers will all exist as self-contained universes.

You see? The point is that if a conscious experience is reproducible as a computer program, this means that a universe can be contained inside one - hence, my universe is likely to be a computer program. Why would I assume that my universe is the one true universe, if it is so easy to create millions of them in the computer software of 2050-2100 or beyond? As the computer program can be parametrised, it exists independent of time restrictions. So long as there is one at the beginning of the recursion, the next ones can all have their own relationship with time.

It leads me to think that if we assume we are able to reproduce the brain and its experiences with computers at any time in the future, then we immediately have to accept that we ourselves are probably a piece of software. I don't see any other way it could be.",11,0,False,self,,,,,
58,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,6,4d3h3m,self.MachineLearning,Help developing project with ML and Bio?,https://www.reddit.com/r/MachineLearning/comments/4d3h3m/help_developing_project_with_ml_and_bio/,bionerd2,1459633576,"Hi everyone! I'm taking an introductory graduate course on ML this quarter and there's a final project. I'm hoping to do something kinda substantive (if that's possible) relating to computational bio/genomics. I'm wondering where to resources to find open questions in this area. Would NIPS/ICML's workshops on comp bio be the place to look? Where else? Thanks!
",3,1,False,self,,,,,
59,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,9,4d41u6,self.MachineLearning,How is ILSVRC Dataset organized ?,https://www.reddit.com/r/MachineLearning/comments/4d41u6/how_is_ilsvrc_dataset_organized/,[deleted],1459642660,[deleted],0,0,False,default,,,,,
60,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,13,4d4yi3,johngustafson.net,A Radical Approach to Computation with Real Numbers [pdf],https://www.reddit.com/r/MachineLearning/comments/4d4yi3/a_radical_approach_to_computation_with_real/,InaneMembrane,1459658985,,3,24,False,default,,,,,
61,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,14,4d543t,self.MachineLearning,"Engineer with solid math background and a MS, what are my chances of getting a ML/Big data job?",https://www.reddit.com/r/MachineLearning/comments/4d543t/engineer_with_solid_math_background_and_a_ms_what/,SpiderFan,1459662326,"I have a BS and MS from top engineering schools, and developed a strong math background on topics that seem to be precursors to machine learning (linear algebra, non-linear optimization, stats, multivariable calc, numerical computation ) and even took an intro to neural nets while in grad school. 

I was wondering what are my chances of breaking into ML given my background. If it's possible, what's the best way to learn ML for someone with an already decent math background for it?

Then, what should a side-project include which would require me to learn everything that a professional ML person would know. ",7,0,False,self,,,,,
62,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,15,4d590m,self.MachineLearning,lstm rnn example for time series data in tensorflow,https://www.reddit.com/r/MachineLearning/comments/4d590m/lstm_rnn_example_for_time_series_data_in/,vinbigdata,1459665595,[removed],0,1,False,default,,,,,
63,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,18,4d5lgm,arxiv.org,Leveraging full Imagenet hierarchy (trained models available),https://www.reddit.com/r/MachineLearning/comments/4d5lgm/leveraging_full_imagenet_hierarchy_trained_models/,wltrt1c3m,1459675573,,1,1,False,default,,,,,
64,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,19,4d5ror,self.MachineLearning,"Learning curves for ImageNet or CIFAR using major convnet architectures (Inception, etc)?",https://www.reddit.com/r/MachineLearning/comments/4d5ror/learning_curves_for_imagenet_or_cifar_using_major/,SuperFX,1459680625,"Has anyone come across learning curves (performance as a function of data set size, not number of epochs) for some of the SOTA (or at least competitive) methods on ImageNet or CIFAR? I'm thinking of experiments that intentionally leave out parts of the image training data to see how the test performance changes.",0,6,False,self,,,,,
65,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,19,4d5s5b,self.MachineLearning,Chess position evaluation with convolutional neural network in Julia,https://www.reddit.com/r/MachineLearning/comments/4d5s5b/chess_position_evaluation_with_convolutional/,int8blog,1459680984,"Hello reddit,

http://int8.io/chess-position-evaluation-with-convolutional-neural-networks-in-julia/

I just finished a post about convolutional neural networks applied to chess positions evaluation. Results still need major improvements - and I would appreciate any suggestions on that matter

",8,36,False,self,,,,,
66,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,20,4d5tv4,self.MachineLearning,Policy Gradient methods for Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/4d5tv4/policy_gradient_methods_for_reinforcement_learning/,iseebeerpeople,1459682418,"I've been working on reinforcement learning problems for a while now, and recently, I've come across a problem with a continuous action space. I understand that one way to solve such problems is by using policy gradient methods, but I don't seem to get them as they are not as intuitive as other methods like Q-learning or Temporal Difference Learning. I was wondering if anybody on this sub could point me to some good resources. TIA!",3,14,False,self,,,,,
67,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,20,4d5tz8,self.MachineLearning,PLease suggest good beginner resources (with datasets) for Named Entity Recognition using deep learning,https://www.reddit.com/r/MachineLearning/comments/4d5tz8/please_suggest_good_beginner_resources_with/,[deleted],1459682496,[removed],0,0,False,default,,,,,
68,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,20,4d5tzt,self.MachineLearning,Machine Learning for Sentiment Analysis,https://www.reddit.com/r/MachineLearning/comments/4d5tzt/machine_learning_for_sentiment_analysis/,harwee,1459682508,"I have been trying to use ML for sentiment analysis of sentences, I have been successful with Naive Bayes and SVM but I would like to implement Neural Networks for Sentiment Analysis but couldn't find a way to convert words as input for neural networks. I know that representing word as a numerical is not efficient. How is nlpnet implemented, I tried to understand that but that flew over my head.",4,13,False,self,,,,,
69,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,20,4d5wp0,self.MachineLearning,Class bias question - balancing the data,https://www.reddit.com/r/MachineLearning/comments/4d5wp0/class_bias_question_balancing_the_data/,joefromlondon,1459684601,"I have a question regarding standard procedure (if there is one) or how other people deal with classes of uneven size when training. 

I am currently using a deep convolutional neural network for the segmentation of images, which classifies each pixel (or voxel) into a certain class. A with any segmentation problem, and many problems in machine learning, the ""background"" of the image, has many more samples than the regions to be segmented causing a bias towards this. I am about to use some stratified sampling but due to the way I've implemented this, the training data will change slightly each epoch. Is this OK? I wondered if anyone had any other suggestions/ successes with different methods?

Thanks!",3,1,False,self,,,,,
70,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,22,4d64i0,self.MachineLearning,Where is Deep Learning used in practice besides Google and Facebook ?,https://www.reddit.com/r/MachineLearning/comments/4d64i0/where_is_deep_learning_used_in_practice_besides/,Patatatarte,1459689819,"I'm trying to understand if deep learning is really used in practice besides image recognition, translation or playing Go by internet Giants such as Google or Facebook. 

Is there's any other real application of deep learning in other domain and other industries ?
",8,0,False,self,,,,,
71,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,23,4d6c5i,annalyzin.wordpress.com,"How to discover purchasing patterns using association rules, a layman's tutorial",https://www.reddit.com/r/MachineLearning/comments/4d6c5i/how_to_discover_purchasing_patterns_using/,inxurgence,1459693952,,0,8,False,http://b.thumbs.redditmedia.com/C_scH3XuoWXYQsdT7-_DKQcLGr2GfLSkfGOAVTcU_eo.jpg,,,,,
72,MachineLearning,t5_2r3gv,2016-4-3,2016,4,3,23,4d6f5p,self.MachineLearning,Evaluate performance of word2vec independently of final classification task,https://www.reddit.com/r/MachineLearning/comments/4d6f5p/evaluate_performance_of_word2vec_independently_of/,Pieranha,1459695433,"I have a complex pipeline for sentiment prediction, where training a word2vec representation is one step. I would like to evaluate the performance of the word2vec representation independently of the LSTM model that it is fed into afterwards. There's several reasons for doing this - most importantly, training the LSTM model takes quite a while. Preferably, I would like to evaluate the word2vec representation in an unsupervised manner without any labels.

The performance evaluation would be used to tune the hyperparameters for the word2vec representation task (see https://radimrehurek.com/gensim/models/word2vec.html).",9,0,False,self,,,,,
73,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,0,4d6fnd,deliprao.com,The idea is ridiculously simple (perhaps why it is effective?): randomly skip layers while training,https://www.reddit.com/r/MachineLearning/comments/4d6fnd/the_idea_is_ridiculously_simple_perhaps_why_it_is/,quirm,1459695650,,46,101,False,http://a.thumbs.redditmedia.com/G94NpLLg1Ywnz6O8JNswnHa2PLF2dXYlo5z7rQUTNu4.jpg,,,,,
74,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,0,4d6k14,medium.com,Machine Intelligence Begins: How We Got Here,https://www.reddit.com/r/MachineLearning/comments/4d6k14/machine_intelligence_begins_how_we_got_here/,paramaggarwal,1459697632,,0,0,False,http://b.thumbs.redditmedia.com/nBalxhpV8xODvPOuFzaoRzNu1CaVfrezlyAJAfPRIAw.jpg,,,,,
75,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,1,4d6rxy,academictorrents.com,How to download ImageNet 2012!,https://www.reddit.com/r/MachineLearning/comments/4d6rxy/how_to_download_imagenet_2012/,uhllkaushdflaksjdhf,1459701036,,0,1,False,default,,,,,
76,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,1,4d6uc4,blog.cambridgecoding.com,Scanning hyperspace: how to tune machine learning models,https://www.reddit.com/r/MachineLearning/comments/4d6uc4/scanning_hyperspace_how_to_tune_machine_learning/,DrLegend,1459702077,,8,5,False,http://a.thumbs.redditmedia.com/YEmLZ-X4tYxhCc61Ipf19WNzTa64BBI_VBDTjS8gb28.jpg,,,,,
77,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,3,4d78uz,github.com,lda2vec: Tools for interpreting natural language,https://www.reddit.com/r/MachineLearning/comments/4d78uz/lda2vec_tools_for_interpreting_natural_language/,blowjobtransistor,1459707804,,19,29,False,http://b.thumbs.redditmedia.com/nqbR21XA9LD7MA7Tb9flSxiWJRAjMz77z9FnsMdHHoI.jpg,,,,,
78,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,4,4d7kay,aioptify.com,Top Machine Learning &amp; Data Mining Books for Machine Learning Engineers and Data Scientists,https://www.reddit.com/r/MachineLearning/comments/4d7kay/top_machine_learning_data_mining_books_for/,kjahan,1459712290,,2,0,False,http://b.thumbs.redditmedia.com/eXuYk3dT6uUc8DoFOeLbf1CLf1SeYBlUfofAHwJQSoc.jpg,,,,,
79,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,5,4d7t1h,self.MachineLearning,Panama Papers dataset?,https://www.reddit.com/r/MachineLearning/comments/4d7t1h/panama_papers_dataset/,wonkypedia,1459715825,"I'm looking hard online, and I can't find the panama papers for download. But journalists across the world have access to it. Anyone have any idea how to obtain it? 

It's a few terabytes, which seems intimidating, but it takes about a day to process petabytes on AWS. To find the names of people involved, we would need to run a PoS tagger in a map job. Might not take all that long to find names in 11.6mil files. 

It's also possible to dump it all into AWS elastic search, and make it more easily searchable. 

It feels to me like this is the kind of problem big data was made to solve... and something like this is too precious to be left to journalists alone. 


EDIT: I posted this yesterday just as the story was breaking. I have newer perspectives, thanks to the comments and reading more news. 

They didn't give the data as such to journalists, they just made a search interface. It was tedious for the journalists to try retrieving info over 8 months. (A video from the Indian Express explained how they did the analysis, i can't seem to find that now). 

Wikileaks has started releasing some documents: https://www.documentcloud.org/public/search/Source:%20%22Internal%20documents%20from%20Mossack%20Fonseca%20%28Panama%20Papers%29%22/p2

That said, there are very good reasons to not make all of it public as said by people with better knowledge than me in the comments:

* Data Privacy. Innocent people can be targets for identity thieves and others with malicious intent, as private addresses, bank details, passports and other such sensitive documents are in the cluster. 

* Releasing it little by little allows to keep this news current and make sure the effects last. 

* The guilty might be able to take preventative action to ensure they are protected (though, if someone has the right connections, they dont need a public dataset). 

* It's quite frankly dangerous! people can die for digging too deep. (But if everyone has the data, we can't all be in danger?). 

That said, this is the sort of problem we should be working towards solving. people should be able to put out insights from a data dump without having to require deep knowledge of ML/big data. I feel now like the kind of work we all do matters in a real world tangible sense, in combating corruption. ",132,255,False,self,,,,,
80,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,5,4d7tc3,self.MachineLearning,Any work on getting Deep networks to learn for themselves?,https://www.reddit.com/r/MachineLearning/comments/4d7tc3/any_work_on_getting_deep_networks_to_learn_for/,ding_bong_bing_dong,1459715935,"I have seen some papers in the past that have focused on evolving local learning rules for neural networks in order to solve control problems that static networks can't solve. It is certainly an interesting problem, because these plastic networks could potentially learn to solve problems they weren't exposed too, much in the same way a static classifier correctly identifies objects it wasn't specifically trained on.

Has anyone done any work on how this might work within the context of deep neural networks?",1,0,False,self,,,,,
81,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,5,4d7u6q,cloudtweaks.com,"How Netflix Is Using Big Data To Create Better, Successful Shows",https://www.reddit.com/r/MachineLearning/comments/4d7u6q/how_netflix_is_using_big_data_to_create_better/,mthemove,1459716259,,6,38,False,http://b.thumbs.redditmedia.com/UHAlOLdn2rYiOFi4TZMcWS5m-TkgKLtxjLDKHil3sMY.jpg,,,,,
82,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,6,4d7yey,bearbullexaminer.com,A Support Vector Machine Model for Stock Market Direction (x-post from /r/stockmarket).,https://www.reddit.com/r/MachineLearning/comments/4d7yey/a_support_vector_machine_model_for_stock_market/,gindc,1459717918,,6,6,False,http://b.thumbs.redditmedia.com/kjSoipKXGANwXTNHTfRiJ_KY1U2LPurjaDHcUu52Pwc.jpg,,,,,
83,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,6,4d80ay,self.MachineLearning,Finding Missing Pressure with lm() - a first serious attempt.,https://www.reddit.com/r/MachineLearning/comments/4d80ay/finding_missing_pressure_with_lm_a_first_serious/,[deleted],1459718702,[deleted],0,2,False,default,,,,,
84,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,7,4d89nf,self.MachineLearning,Where is research in distributed learning being done?,https://www.reddit.com/r/MachineLearning/comments/4d89nf/where_is_research_in_distributed_learning_being/,FourthHead,1459722638,"I'm interested in learning that is distributed across a robotic swarm. Essentially, I would be interested in situations where you have multiple robotic agents and they have to cooperatively learn a problem.

I want to go to grad school but wasn't able to find a lot of information about the field. I was hoping someone working in the field (or a related one) would point me towards a good school. I would strongly prefer distributed learning that is directly being applied to robots, but would be willing to settle for distributed learning in general.",2,3,False,self,,,,,
85,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,8,4d8fjw,technologyreview.com,"""In our view reinforcement learning is going to be as big as deep learning in the next two or three years. - Demis Hassabis in Technology Review portrait",https://www.reddit.com/r/MachineLearning/comments/4d8fjw/in_our_view_reinforcement_learning_is_going_to_be/,jakn,1459725133,,5,8,False,http://b.thumbs.redditmedia.com/SdWacC7dxaRFbvILZAc-73SnfHRB-AmV-CyzTFazW6Q.jpg,,,,,
86,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,9,4d8p3e,self.MachineLearning,Expressing CNN back propagation as convolutions,https://www.reddit.com/r/MachineLearning/comments/4d8p3e/expressing_cnn_back_propagation_as_convolutions/,Blammar,1459729381,"In the code I've been able to find, the computation of the gradient appears as a convolution (e.g., DeepLearnToolbox CNN/cnnbp.m):

    net.layers{l}.dk{i}{j} = convn(flipall(net.layers{l - 1}.a{i}), net.layers{l}.d{j}, 'valid') / size(net.layers{l}.d{j}, 3);

Looking at equation 9.12 in http://www.deeplearningbook.org/contents/convnets.html, though,
I do not see how to express the whole thing as a single convolution. (9.12) can be simplified to:

    dK(krow, kcol) = sum(G(row, col) * V(row+krow, col+kcol); row, col)    (*)

if we assume 1 input and 1 output channel for clarity. G and V are the same size, say MxN, and row and col extend over that. krow and kcol extend over the kernel size PxQ. Going outside V gives you a 0 value.

For example, in one dimension, if G is [a b c d], V is [w x y z], and M is 3, then the first sum is dot (G, [0 w x y]), the second sum is dot (G, [w x y z]), and the third sum is dot (G, [x y z 0]).

How do I express (*) as a single convolution? A Matlab example is fine, or even ArrayFire or just an explicit sum. I'm clearly missing something obvious.

",2,4,False,self,,,,,
87,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,9,4d8suc,self.MachineLearning,Multiclass generation,https://www.reddit.com/r/MachineLearning/comments/4d8suc/multiclass_generation/,forloopsarebad,1459731114,"Maybe I'm using the wrong term for this, since I couldn't find anything online after searching for a while. 

What I'm interested in here is exploring whether we need different networks for different classes of generation(for example in text, we could take prose to be different from code, or in music we could take different genres of music). Basically, could I train one single network that'll learn multiple such classes and choose one to generate output from based on a priming input, or do I absolutely need different networks. 

I'm mostly talking about generating using RNN's, but I guess the concept would apply to others as well.

I imagine there'll be some issues, like the size of the networks, the training methods, maybe architectural changes. Has this been explored at all?",2,1,False,self,,,,,
88,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,10,4d8xvm,homes.cs.washington.edu,LIME - Local Interpretable Model-Agnostic Explanations -- Explains any machine learning models. Find Bug in 20 newsgroup classifier and understand the behavior of deep neural networks,https://www.reddit.com/r/MachineLearning/comments/4d8xvm/lime_local_interpretable_modelagnostic/,crowwork,1459733435,,1,49,False,http://a.thumbs.redditmedia.com/Lkkdmq64Ea6NWu9vn9GKKui1p_iha5c1UnbBfXQRyk4.jpg,,,,,
89,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,11,4d95gl,self.MachineLearning,External validation of a group's ML process,https://www.reddit.com/r/MachineLearning/comments/4d95gl/external_validation_of_a_groups_ml_process/,people_are_robots,1459736950,"Say there's a group (that is opaque to you) that's heavily using ML methods to produce some outputs. You don't have access to their input data or code but can ask high level questions about what their data is, what modeling techniques they use, how they think about generalization, where their features come from, gross aggregate statistics, etc.

What questions do you think would be useful to distinguish if the group is effectively using the tools they claim to or are just producing snake oil (whether they mean to or not)?",2,6,False,self,,,,,
90,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,11,4d96f3,self.MachineLearning,Deep Neural Network Application Needed,https://www.reddit.com/r/MachineLearning/comments/4d96f3/deep_neural_network_application_needed/,TopGunSnake,1459737400,"I am looking for an application for deep neural network development. Any suggestions (aside from the stock market) would be appreciated. Ideally, the application could use simulated data or reinforcement learning.",5,2,False,self,,,,,
91,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,11,4d97kv,youtube.com,Hello World - Machine Learning Recipes #1,https://www.reddit.com/r/MachineLearning/comments/4d97kv/hello_world_machine_learning_recipes_1/,fantompower,1459737914,,2,9,False,http://b.thumbs.redditmedia.com/QhtHtiZPceoVh17djIW2miHKTMv16xaW8rDTlAShhTo.jpg,,,,,
92,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,11,4d98at,twitter.com,Franois Chollet on Twitter - Rewriting Keras: job done. Next will be rewriting the documentation.,https://www.reddit.com/r/MachineLearning/comments/4d98at/franois_chollet_on_twitter_rewriting_keras_job/,j_lyf,1459738268,,7,27,False,http://b.thumbs.redditmedia.com/fTX8XKujQ_YZWGghKFcS7to8UQ6aWzkUqqrHDXxfgOE.jpg,,,,,
93,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,15,4d9vm9,self.MachineLearning,"In Kingma &amp; Welling (2014)'s VAE paper, how does the closed-form expression for the variational lower bound come about?",https://www.reddit.com/r/MachineLearning/comments/4d9vm9/in_kingma_welling_2014s_vae_paper_how_does_the/,jogloran,1459750594,"I'm reading through [Auto-encoding Variational Bayes](http://arxiv.org/pdf/1312.6114v10.pdf) by Kingma &amp; Welling (2015) -- in Appendix B on page 10 there's the expression for the terms in the variational lower bound in the special case of a Gaussian prior with zero mean and diagonal covariance matrix.

How do I evaluate the below integral? 

 N(z; , ^2 ) log N(z; 0, I) dz

It's clear how 1/sqrt(2) in log N(z; 0,I) yields a term -log(2)/2, but how do the ^2, ^2 terms come about? Also what happens to the z terms in the integral? I imagine they get integrated out or cancel with something else but I can't see how right now...",5,19,False,self,,,,,
94,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,15,4d9xcq,self.MachineLearning,Finding a needle in the hay?,https://www.reddit.com/r/MachineLearning/comments/4d9xcq/finding_a_needle_in_the_hay/,Atma-n,1459751671,"Hi! I have a problem I thought I might get some help from machine learning. I have thought clustering might be a way but please feel free to throw any suggestions my way.
I have a set of data with fault events. At the event time a snapshot of parameters is taken and put in a list as seen below. In this table you see User 1 getting fault code 1 and after that some sampled data at that moment. I have a lot of more parameters such as time and many more sensor values.

Can you see any way that can help me sort the data to understand why the different Fault Codes happen? Maybe some codes happen due to others, maybe some happens only when a sensor has a specific value etc.

What do you say, do I have any way of getting a clearer picture of this?

Thanks!


User|Fault Code Name|Sensor 1 value, analogue|Sensor 2 value|Calculated Value 1|
:--|:--|:--|:--|:--|
User 1|Code ID 1|100.000000|.780000|45.882353|
User 2|Code ID 1|7.058824|6.130000|22.745098|
User 2|Code ID 2|7.058824|6.140000|22.745098|
User 2|Code ID 3|7.058824|5.630000|20.392157|
User 3|Code ID 3|7.058824|5.790000|21.176471|
User 3|Code ID 4|6.666667|33.260000|45.490196|
",6,3,False,self,,,,,
95,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,16,4da4iu,self.MachineLearning,multi-label loss function,https://www.reddit.com/r/MachineLearning/comments/4da4iu/multilabel_loss_function/,fengmudan,1459756277,"Hi there, I'm doing a multi-label task, ie, the output is [0,1,1,1,0,1]
via an ANN. The top layer I picked is the sigmoid activation.

The question is, should I use binary cross-entropy or multiclass cross entropy??

Note this problem is different than the multiclass problem, where the latter case the output is an one hot vector.
THanks!!",0,1,False,self,,,,,
96,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,17,4da5z9,tastehit.com,Neural networks with Van Gogh's artistic talent,https://www.reddit.com/r/MachineLearning/comments/4da5z9/neural_networks_with_van_goghs_artistic_talent/,HCthegreat,1459757246,,0,1,False,http://b.thumbs.redditmedia.com/Ykp1VX6Tw6ixetZF1Pdkv4KFxbmQ5HnX5mV3NFdUurk.jpg,,,,,
97,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,17,4da71w,self.MachineLearning,What are some interesting problems in Medical domain where deep learning methods can be used?,https://www.reddit.com/r/MachineLearning/comments/4da71w/what_are_some_interesting_problems_in_medical/,[deleted],1459758065,[deleted],0,1,False,default,,,,,
98,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,17,4da77v,self.MachineLearning,What are some interesting problems in Medical domain where deep learning methods can be used?,https://www.reddit.com/r/MachineLearning/comments/4da77v/what_are_some_interesting_problems_in_medical/,shash273,1459758182,,13,3,False,self,,,,,
99,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,17,4da7mo,issuu.com,Catlogo de punzonadoras de ocasin,https://www.reddit.com/r/MachineLearning/comments/4da7mo/catlogo_de_punzonadoras_de_ocasin/,Barriuso,1459758517,,0,1,False,default,,,,,
100,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,18,4daan8,youtube.com,Build a Neural Net in 4 Minutes,https://www.reddit.com/r/MachineLearning/comments/4daan8/build_a_neural_net_in_4_minutes/,llSourcell,1459760647,,0,2,False,http://b.thumbs.redditmedia.com/H_YFqK6PC5FIoncRJYPIBB7F5NxUf-jpGm6xvXHmteI.jpg,,,,,
101,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,18,4dab85,self.MachineLearning,Can someone help me debug my word2vec implementation?,https://www.reddit.com/r/MachineLearning/comments/4dab85/can_someone_help_me_debug_my_word2vec/,[deleted],1459761080,[deleted],0,1,False,default,,,,,
102,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,18,4dacfg,self.MachineLearning,TensorFlow RNN implementation from scratch?,https://www.reddit.com/r/MachineLearning/comments/4dacfg/tensorflow_rnn_implementation_from_scratch/,polytop3,1459761938,"Hi All,

I have recently delved into tensorflow (coming from theano). It seems TensorFlow provides prebuilt LSTMs, GRUs, etc.

However, I would like to build one from scratch.

In theano, it is relatively straightforward with theano.scan. However, the [LSTM implementation in TensorFlow](https://github.com/tensorflow/tensorflow/blob/1cd108ae5dff363be4a031f3d9ced4330100ca8f/tensorflow/python/ops/rnn_cell.py#L159) seems a bit more elusive. I don't see any equivalent of scan there, so I am unsure how the recurrence occurs.

Is there a tutorial that clearly shows how to create an LSTM in TensorFlow from scratch? (even a vanilla RNN would do, since once I get that I should be able to readily adapt that to LSTMs)",8,0,False,self,,,,,
103,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,18,4dacqu,self.MachineLearning,Self Driving Car Wining the World Rally Championship,https://www.reddit.com/r/MachineLearning/comments/4dacqu/self_driving_car_wining_the_world_rally/,gabriel1983,1459762175,When do you estimate this might happen? 10-20 years from now?,7,2,False,self,,,,,
104,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,18,4dadld,self.MachineLearning,Can someone help me debug my word2vec implementation?,https://www.reddit.com/r/MachineLearning/comments/4dadld/can_someone_help_me_debug_my_word2vec/,[deleted],1459762801,[removed],0,1,False,default,,,,,
105,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,19,4dafrh,self.MachineLearning,Can someone help me debug my word2vec implementation?,https://www.reddit.com/r/MachineLearning/comments/4dafrh/can_someone_help_me_debug_my_word2vec/,FuschiaKnight,1459764243,"Hey there!

I'm an undergrad whose been trying to implement word2vec for myself to get a better feel for it. I'm just focusing on Skip-Gram with Negative Sampling.

I've done extensive reading on the subject (Mikolov's papers, Socher's papers - the most relevant of which is obviously GloVe, Levy &amp; Goldberg's papers all the way up to their TACL 2015 one). I also watched Socher's appropriate lectures from cs224d. The paper I found most helpful for the gradient updates was

http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf

I thought I implemented the algorithm correctly, but when I train on a small subset of the text8 data (the first 40,00,000 bytes of it), my vectors do not do as well as they should on the analogy dataset. Here is a link to my code

https://github.com/wboag/my_word2vec

When I say that my vectors aren't as good as they should be, what I mean is that I am comparing them against both the Mikolov vectors and gensim vectors trained with the same hyperparameters (and one thread each) on the same dataset. I've included both of those in the github repo for comparison. They each get about 13% accuracy, wheras mine only get 2-3% accuracy.

**Installation Notes:** I'm using a C++ library called Eigen, but I put the necesarry dirs in my repo, so you wouldn't need to install anything new to run it. For the gensim code, you'd need the standard numpy, genim, ... packages (installable via pip). The official word2vec is in C and has the makefile, so that will be run-able out-of-the-box.

**Running It:** I have a script named run.sh which builds vectors for each of the 3 methods and evaluates each one. The file also contains expected runtimes for training and accuracy for evaluation.

----------------------------------------------------------

Okay, so that's the backstory. Now for my dilemma - I cannot for the life of me figure out what I'm doing wrong. The main function is organized in a very simple manner, I think. And my function for computing the gradients for a (w,c) pair also looks good to me - I closely followed the paper I linked.

And it's not like I'm getting 0%. The 3% accuracy means that SOMETHING semi-relevant is getting learned. If I tweak the gradient formulas a little, it sets me to 0% accuracy, as expected.

I tried following Mikolov's C code, but I got lost trying to understand it. I even had trouble following the specifics of Radim's gensim code because he vectorized everything inside of train_sg_pair() so I can't see why his works and mine doesn't.

I hope one of you will be kind enough to take the time to figure this darn thing out, because I've been wrestling with it forever and still haven't fixed it.

I suspect that the gradient formula is correct but maybe I'm somehow not applying every update for whatever reason (i'd had a bug early where my for loop over text tokens was accidentally only iterating for |V| number of times instead of |N|). Maybe it's something like that.",26,0,False,self,,,,,
106,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,21,4darh6,self.MachineLearning,"In RNN, can evaluation be mini-batched?",https://www.reddit.com/r/MachineLearning/comments/4darh6/in_rnn_can_evaluation_be_minibatched/,wtfalreadyinuse,1459771468,"The typical approach to apply mini-batch methods in training RNN is adding padding values to make sequences in the same batch to have equal length.
Can this idea be applied to evaluation? (or prediction?)
I found that it takes a lot of time to predict entire data one-by-one, in some cases it takes even longer than training time.",0,1,False,self,,,,,
107,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,21,4darro,self.MachineLearning,"What are the top conferences and journals for data mining, outlier detection from text or attribute data?",https://www.reddit.com/r/MachineLearning/comments/4darro/what_are_the_top_conferences_and_journals_for/,poporing88,1459771613,"In computer vision, we have tier1: CVPR ICCV and ECCV. tier2: ACCV ICME ICIP. (for journal it is TPAMI &gt;TIP&gt; TIFS &gt;TMM ) I wonder if there is such hierarchies for outlier detection research from emails/attributes? i am guessing KDD but I need a better grasp. This would help.for my RRL. Thanks!!!",4,6,False,self,,,,,
108,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,21,4dasqg,ibm.com,Cognitive Storage: Teaching Computers What to Learn and What to Forget,https://www.reddit.com/r/MachineLearning/comments/4dasqg/cognitive_storage_teaching_computers_what_to/,ibmzrl,1459772078,,8,5,False,default,,,,,
109,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,22,4db2lv,185.100.87.84,Turkish Citizenship Database Dumped,https://www.reddit.com/r/MachineLearning/comments/4db2lv/turkish_citizenship_database_dumped/,pmigdal,1459776732,,2,0,False,default,,,,,
110,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,23,4dbb2v,arxiv.org,Building Machines That Learn and Think Like People,https://www.reddit.com/r/MachineLearning/comments/4dbb2v/building_machines_that_learn_and_think_like_people/,pgay,1459780330,,19,17,False,default,,,,,
111,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,23,4dbbe1,self.MachineLearning,"Has anyone discovered linear substructures in a sentence embedding vector space, similar to the relationships demonstrated by glove/word2vec?",https://www.reddit.com/r/MachineLearning/comments/4dbbe1/has_anyone_discovered_linear_substructures_in_a/,throwaway1849430,1459780476,I'm curious if a contradictory sentence can be found in the space or something along those lines. ,1,1,False,self,,,,,
112,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,23,4dbdf5,gettopical.com,"Microsoft's machine learning vision includes security, too",https://www.reddit.com/r/MachineLearning/comments/4dbdf5/microsofts_machine_learning_vision_includes/,Barry_Bird,1459781265,,0,5,False,http://b.thumbs.redditmedia.com/daHjFKARF8CCwIMUybGKCX1lFXJ3co1UOJk-32_Xw6o.jpg,,,,,
113,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,23,4dbeeg,self.MachineLearning,Frontiers in graphical models research?,https://www.reddit.com/r/MachineLearning/comments/4dbeeg/frontiers_in_graphical_models_research/,adagrad,1459781646,What are some interesting PGM papers from the past year? ,9,47,False,self,,,,,
114,MachineLearning,t5_2r3gv,2016-4-4,2016,4,4,23,4dber4,textkernel.com,Conference Intelligent Machines,https://www.reddit.com/r/MachineLearning/comments/4dber4/conference_intelligent_machines/,grumpybusinesscat,1459781787,,0,0,False,http://b.thumbs.redditmedia.com/k8DWh5GGQz5Ze9sSrKQ7KfTxPVDr3HtXlr9x-__xyuc.jpg,,,,,
115,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,0,4dbfw6,self.MachineLearning,ML &amp; AI podcasts to reccommend?,https://www.reddit.com/r/MachineLearning/comments/4dbfw6/ml_ai_podcasts_to_reccommend/,thesameoldstories,1459782209,"Hi, seeking for some advice in cool ML &amp; AI podcasts. Big fan of TalkingMachines but I'm seeking to get to know some more. Long train journeys awaiting. 

Thanks
",8,21,False,self,,,,,
116,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,0,4dbi3v,self.MachineLearning,Worth it to leave Bay Area for better job elsewhere?,https://www.reddit.com/r/MachineLearning/comments/4dbi3v/worth_it_to_leave_bay_area_for_better_job/,[deleted],1459783027,[deleted],14,1,False,default,,,,,
117,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,0,4dbp4h,medium.com,Before AlphaGo there was TD-Gammon,https://www.reddit.com/r/MachineLearning/comments/4dbp4h/before_alphago_there_was_tdgammon/,jimfleming,1459785551,,8,7,False,http://b.thumbs.redditmedia.com/82SV-_QLxBnzZFMpjX_JXJlccILWOpY7ndhcgj0I31s.jpg,,,,,
118,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,2,4dc8jt,self.MachineLearning,Does the statistical definition of 'variance' refer to the same 'variance' used in the discussion of bias vs. variance?,https://www.reddit.com/r/MachineLearning/comments/4dc8jt/does_the_statistical_definition_of_variance_refer/,relganz,1459792506,"I am familiar with using and understanding variance in both settings, but I can't figure out if they really mean the same thing.  For example, in the context of bias vs. variance, I hear that a model has too much variance, so it performs better on training data than on test data, and more regularization is needed, etc.  But does the 'variance' there actually refer to statistical definition (square of standard deviation) ?  And if so how exactly does that work? Thanks.",3,18,False,self,,,,,
119,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,4,4dcls3,openculture.com,"The museum of modern art moma puts online 65,000 works",https://www.reddit.com/r/MachineLearning/comments/4dcls3/the_museum_of_modern_art_moma_puts_online_65000/,3eyedravens,1459797227,,1,53,False,http://b.thumbs.redditmedia.com/t8DgPzh6LLZWxspLDxBtvZT-dSILymHuyDH3YrRnhVo.jpg,,,,,
120,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,4,4dctdm,self.MachineLearning,Defining Reward for Deep Reinforcement Learning?,https://www.reddit.com/r/MachineLearning/comments/4dctdm/defining_reward_for_deep_reinforcement_learning/,doctorjuice,1459799863,"Hi all,

I am designing a neural network in Lasagne, a Theano based Deep Learning Library.

I am trying to program a simple, Reinforcement Learning network, but am running into a road block in defining the loss function.

Here is a description of the problem:

**Input:** Each datapoint is 103 x 3

**Output:** Float scalar

**Reward:** Distance calculated using input.

Basically, the input can be thought of as a location of the AI. The AI needs to get closer to a fixed destination point. The distance can be calculated by the input alone. The output (alhpa) affects the movement of the AI over time.

How should I define the **loss function** in lasagne? I'm at a loss (no pun intended). ",4,0,False,self,,,,,
121,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,5,4dcwl0,arxiv.org,Building Machines That Learn and Think Like People,https://www.reddit.com/r/MachineLearning/comments/4dcwl0/building_machines_that_learn_and_think_like_people/,aleph__one,1459800967,,5,1,False,default,,,,,
122,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,6,4dd66x,iknowfirst.com,Stock Market Forecast: Creating a Model for Chaos Mapping and Predictions,https://www.reddit.com/r/MachineLearning/comments/4dd66x/stock_market_forecast_creating_a_model_for_chaos/,elen777,1459804399,,0,0,False,http://a.thumbs.redditmedia.com/MY6HAhKpfgr13Om4-udNG5LOBWeOTVPistdIIgFNgm8.jpg,,,,,
123,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,6,4dd6r2,media.bemyapp.com,Nervana Systems co-founder on deep learning &amp; startup launch,https://www.reddit.com/r/MachineLearning/comments/4dd6r2/nervana_systems_cofounder_on_deep_learning/,tony_sf,1459804599,,0,0,False,http://b.thumbs.redditmedia.com/tM1xC1fW0S6HdZUIT-H1n-jyi1yygSZvrOsnUV5si8A.jpg,,,,,
124,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,6,4ddcwk,metamind.io,MetaMind acquired by Salesforce,https://www.reddit.com/r/MachineLearning/comments/4ddcwk/metamind_acquired_by_salesforce/,fhuszar,1459806945,,16,30,False,http://b.thumbs.redditmedia.com/c76N2Hb1Bx8UBsuVlFKVo7rf6havPrZ4KrByGWxpFbA.jpg,,,,,
125,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,12,4desm1,self.MachineLearning,How much are researchers / coders in ML making?,https://www.reddit.com/r/MachineLearning/comments/4desm1/how_much_are_researchers_coders_in_ml_making/,[deleted],1459828566,[deleted],5,0,False,default,,,,,
126,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,14,4df19g,self.MachineLearning,Could Neural Networks be used for Object Size/Orientation Detection? (Read below!),https://www.reddit.com/r/MachineLearning/comments/4df19g/could_neural_networks_be_used_for_object/,soulslicer0,1459832991,"Good day,

I have an idea and I am not sure if it has been implemented in any papers. Basically, this is an image classification and size detection problem. Normally, we train  a classifier to detect objects, say a box.

Okay, now in the detection stage, we have a moving Kernel over the image, of varying sizes and maybe even rotation (we rotate back and resize down to 32x32 or whatever our NN structure is). Basically, I could use a evolutionary approach, where I have a matrix of say [x y length width rotation] vectors. And I add some noise to them, and try them on the image. If a kernel (represented by the vector above) is perfectly fitting, the NN should output a higher weight. I repopulate the ones with higher weight. Eventually, I should converge on a perfect kernel size. This should represent the object rotation and size in 2D.

What do you think?",4,3,False,self,,,,,
127,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,14,4df1hm,dmlc.ml,Deep3D: Automatic 2D-to-3D Video Conversion with CNNs,https://www.reddit.com/r/MachineLearning/comments/4df1hm/deep3d_automatic_2dto3d_video_conversion_with_cnns/,antinucleon,1459833117,,24,99,False,default,,,,,
128,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,14,4df332,datasciencecentral.com,Machine Learning : Few rarely shared trade secrets,https://www.reddit.com/r/MachineLearning/comments/4df332/machine_learning_few_rarely_shared_trade_secrets/,vincentg64,1459834059,,0,1,False,default,,,,,
129,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,14,4df4vc,self.MachineLearning,What is the percentage of computer engineers/researchers who know/work in Machine Learning/AI?,https://www.reddit.com/r/MachineLearning/comments/4df4vc/what_is_the_percentage_of_computer/,anantzoid,1459835050,"Lately, the media is all showered with Deep Learning and it's achievements, thus creating an image that the only significant work going on in computing in Deep Learning.
That makes me wonder, what is the percentage of computer scientists/engineers who actually work in these areas? 
Most of my colleagues are not very familiar with anything ranging from linear regression to ConvNet.",4,4,False,self,,,,,
130,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,15,4df7a3,blog.ayoungprogrammer.com,Determining Gender of a Name with 80% Accuracy Using Only Three Features,https://www.reddit.com/r/MachineLearning/comments/4df7a3/determining_gender_of_a_name_with_80_accuracy/,3eyedravens,1459836482,,3,12,False,default,,,,,
131,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,17,4dflgf,self.MachineLearning,Training RNNs: different possibilities,https://www.reddit.com/r/MachineLearning/comments/4dflgf/training_rnns_different_possibilities/,cedricdb,1459846345,"Reading about RNNs (and variants) and wading through GitHub code to observe their training procedures, I noticed that there are a bunch of different ways to train RNNs. Authors not always point out how exactly their RNNs are trained, which does not allow for reproducibility of the experiments.

For example, in Karpathy's Char-RNN (I studied the Python gist code, not the Torch version) the RNN is trained on fixed subsequent batches of 25 characters every epoch. That is, the first 25 characters are taken as an input after which we backpropagate, then the next 25 characters, then the following 25 etc. When all data is seen, we start over again. I also found a variant of this, in which the batches get an offset every new epoch. That is, we start with an offset of 0 (which is the case above); the next epoch the offset is increased by one, so that now batches are formed of characters [1-26], then [27-52], then [53-78] etc. I have seen other persons taking the 'sliding window' approach, i.e. first taking characters [0-25] and backpropagate, then characters [1-26], after that [2-27] etc. (which, btw, seems to me the most logical way to train an RNN).

Any thoughts on this?
Thanks!",7,15,False,self,,,,,
132,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,18,4dfns1,8bitvision.com,8bitvision: Upload an image to classify the content,https://www.reddit.com/r/MachineLearning/comments/4dfns1/8bitvision_upload_an_image_to_classify_the_content/,themoosemind,1459847982,,1,0,False,http://b.thumbs.redditmedia.com/j-GCI9Nx77yKrL5ZqvR9dsVh35PXzFlWMX20kQbPHmU.jpg,,,,,
133,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,19,4dfuvt,self.MachineLearning,'2nd-degree' LSTMs with an LSTM for the state?,https://www.reddit.com/r/MachineLearning/comments/4dfuvt/2nddegree_lstms_with_an_lstm_for_the_state/,sachinrjoglekar,1459852863,"Some time back, I wrote a blog post(https://codesachin.wordpress.com/2016/01/23/predicting-trigonometric-waves-few-steps-ahead-with-lstms-in-tensorflow/) describing how an LSTM could be used to predict the value in a trigonometric wave few steps ahead (predicting f(x + 20) from f(x)). 

The wave equation was something like A*sin(a1) + B*sin(a2) + C, with a1 and a2 both having different initial phases and frequencies. 

The LSTM alone couldn't figure out the wave (normalized), but when I fed in x, x' and x'' (first and second derivatives of the value), the model could understandably learn the pattern faster.

Now, I have been trying a new technique where I have a 'front' LSTM. Now at every iteration, this LSTM's total state (h:c) go through another 'meta' LSTM (whose state is 2*state of the front LSTM), and then gets fed back into the front LSTM. The intuition for this, is that the meta-LSTM will provide a higher degree of abstraction/understanding over the trends, and front-LSTM will use the understanding of these predicted trends, to compute the future value.

Programmatically, this is what I mean (in TensorFlow):



lstm_layer = rnn_cell.BasicLSTMCell(input_dim)

meta_lstm_layer = rnn_cell.BasicLSTMCell(lstm_layer.state_size)

lstm_state = tf.Variable(tf.zeros([1, lstm_layer.state_size]))

meta_lstm_state = tf.Variable(tf.zeros([1, meta_lstm_layer.state_size]))

meta_lstm_output, meta_lstm_state_output = meta_lstm_layer(
    lstm_state, meta_lstm_state, scope=""METALSTM"")

lstm_output, lstm_state_output = lstm_layer(
    input_layer, meta_lstm_output, scope=""LSTM"")

lstm_update = lstm_state.assign(lstm_state_output)

meta_lstm_update = meta_lstm_state.assign(meta_lstm_state_output)

Does this make sense? Or am I just adding an unnecessary level of complexity? I am not an expert with the math behind LSTMs, and I am trying to work it out. But are there any obvious pitfalls here?",4,1,False,self,,,,,
134,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,22,4dgb6j,github.com,Panama Papers dataset 2016,https://www.reddit.com/r/MachineLearning/comments/4dgb6j/panama_papers_dataset_2016/,amaboura,1459861446,,23,123,False,http://b.thumbs.redditmedia.com/PZ_0ACWfrC1Sif2jL82H5ZyTNlnG8Sccmr_LBb44OWI.jpg,,,,,
135,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,22,4dgb8s,self.MachineLearning,Standardization with mean/std or median/IQR?,https://www.reddit.com/r/MachineLearning/comments/4dgb8s/standardization_with_meanstd_or_medianiqr/,BlackHawk90,1459861475,"Hello

I have a dataset with 10000 data points and 20 features. The features are not normal distribution (most of them have a generalized extreme value or burr distribution and all values are greater or equal to zero). Of course some classifiers requires standardized/normalized features so that the features have similar scale. Because I have some outliers in my data, I think I have to do standardization (and not normalization). Currently I'm subtracting the mean of each feature and divide by the standard deviation. Another option would be to subtract the median and divide by the IQR.

Which of this two options is better or does it not matter?",15,1,False,self,,,,,
136,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,22,4dgdfq,self.MachineLearning,Need for removing correlated and near-zero variance features despite feature selection?,https://www.reddit.com/r/MachineLearning/comments/4dgdfq/need_for_removing_correlated_and_nearzero/,BlackHawk90,1459862417,"Hello

I'm doing classification with two classes. Before I apply a classifier, I'm doing some preprocessing steps like removing near-zero variance features or highly correlated features (for those classifiers which are sensitive to it).

Now I will also add a feature selection step (ReliefF and genetic algorithms). Do I still have to do the above preprocessing steps before or after the feature selection or is this already incorporated in the feature selection? I think the feature selection process should already eliminate correlated and near-zero variance features but I'm not completely sure. Of course standardization and missing value imputation I have to do before the feature selection.",0,2,False,self,,,,,
137,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,22,4dge5h,woobox.com,CUNY Center for Student Entrepreneurship. Early Stage Machine learning company.,https://www.reddit.com/r/MachineLearning/comments/4dge5h/cuny_center_for_student_entrepreneurship_early/,AppliedMotions,1459862738,,0,1,False,default,,,,,
138,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,22,4dgeix,self.MachineLearning,Creating Spell Checker That Takes Surrounding Words Into Context,https://www.reddit.com/r/MachineLearning/comments/4dgeix/creating_spell_checker_that_takes_surrounding/,LeavesBreathe,1459862897,"Hey Guys,

The goal here is correct simple mispellings from words randomly being separated. 

For example, you'll have a sentence:

Learning a new language is an important com ponent to developing your brain. 

I found this python spelling corrector http://pythonhosted.org/pyenchant/tutorial.html but the problem is that it doesn't join two fragments of words. Same story with this checker: https://pypi.python.org/pypi/language-check


Is there a simple python script that can be written to address these split words? I feel that it would have to take the context around the word to decide whether to join them. 

I thought of doing some sort of sequence to sequence RNN set up, but it seems like overkill for this type of task. Let me know what you guys think!",4,2,False,self,,,,,
139,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,22,4dgiva,self.MachineLearning,Machine Learning Research Internship/Assistantship ?,https://www.reddit.com/r/MachineLearning/comments/4dgiva/machine_learning_research_internshipassistantship/,sitarwars,1459864665,"Hi, I will be graduating after completing my undegrad studies in CS this May. I have a lot of previous research as well as industry experience. Could you lead me to some university that is hiring international interns/assistants in the field of ML. Am not looking for any sort of funding. Am a student from a top reputed college in India.",4,0,False,self,,,,,
140,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,23,4dgkie,self.MachineLearning,CTC Alternatives?,https://www.reddit.com/r/MachineLearning/comments/4dgkie/ctc_alternatives/,Bardelaz,1459865319,"Seq2Seq (encoder-decoder) is a too general in the sense that the output time is not required to be monotonic with the input. Also, it is not suitable for streaming applications. 
Why not CTC?  it is a bit complicated to implement and compute heavy. 

CTC paper: http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf",4,3,False,self,,,,,
141,MachineLearning,t5_2r3gv,2016-4-5,2016,4,5,23,4dgrhs,self.MachineLearning,How to do make sense of tensorflow tensorboard Histograms?,https://www.reddit.com/r/MachineLearning/comments/4dgrhs/how_to_do_make_sense_of_tensorflow_tensorboard/,illiterate_gorillas,1459868110,"I would like to know how to make sense of the tensor flow graphs/Histograms generated.  This graph is easy to understand Accuracy and loss are straight forward to understand.

The code for this can be found [here](https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py).

In this [image](http://i.stack.imgur.com/LVAWR.png).

* Accuracy- Accuracy of current state of network for given train data.
Higher is better

* Accuracy/Validation -  Accuracy of current state of network for given Validation data which is not seen by network before. Higher is better

* Loss- Loss of network on train data. Lower is better.

* Loss/Valadation - Loss of network on test data. Lower is better.
If loss increases it's a sign of over-fitting.

* Conv2d/L2-Loss - Loss of particular layer wrt train data. 

Basically I would like to know what the graph signifies and how i could use it to understand my network and if possible what changes i can make to improve it.

**How Do I interpret the histograms?**
[This is what I Don't understand](http://i.stack.imgur.com/P5cMY.png).
",2,9,False,self,,,,,
142,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,0,4dgxg0,arxiv.org,Low-Rank Factorization of Determinantal Point Processes for Recommendation,https://www.reddit.com/r/MachineLearning/comments/4dgxg0/lowrank_factorization_of_determinantal_point/,strategist922,1459870254,,1,4,False,default,,,,,
143,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,0,4dgysh,self.MachineLearning,Pretraining for residual networks,https://www.reddit.com/r/MachineLearning/comments/4dgysh/pretraining_for_residual_networks/,ntak,1459870750,"Hello!

I am using a residual net to do some binary classification of images (Does the image contains a bird or not?). I am looking to speed up a bit the convergence of the net and, if possible, enhance performance a bit.

I am using Lasagne to code the net and Blocks as a training framework.

Thanks",7,4,False,self,,,,,
144,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,0,4dgyvu,self.MachineLearning,Should I give up on ML?,https://www.reddit.com/r/MachineLearning/comments/4dgyvu/should_i_give_up_on_ml/,anotherMLguy,1459870788,[removed],0,1,False,default,,,,,
145,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,0,4dh1fh,ustream.tv,live stream GTC keynote by nvidia CEO Jen-Hsun Huang: Pascal GPU + other ML related announcements expected,https://www.reddit.com/r/MachineLearning/comments/4dh1fh/live_stream_gtc_keynote_by_nvidia_ceo_jenhsun/,fhuszar,1459871694,,34,38,False,http://b.thumbs.redditmedia.com/qNyS9v6zzRa9ZWMXkX91LE7EqUxQ-r8II4XjXr5tDtI.jpg,,,,,
146,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,1,4dh3ga,self.MachineLearning,Any ML or DL Summer Schools?,https://www.reddit.com/r/MachineLearning/comments/4dh3ga/any_ml_or_dl_summer_schools/,stevofolife,1459872399,What are some of the well-known Machine Learning and Deep Learning Summer Schools to attend?,11,5,False,self,,,,,
147,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,1,4dh3h3,iamaaditya.github.io,[Keras] Simple tutorial on Visual Question Answering on Jupyter Notebook - data/model included [OC],https://www.reddit.com/r/MachineLearning/comments/4dh3h3/keras_simple_tutorial_on_visual_question/,iamaaditya,1459872409,,1,14,False,http://b.thumbs.redditmedia.com/SpbCJdEd-YiPMuwa4M-9Rek_qBeV35-qzNZWWV4YUgw.jpg,,,,,
148,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,1,4dh4r1,blog.recast.ai,How can a developer work with audio files?,https://www.reddit.com/r/MachineLearning/comments/4dh4r1/how_can_a_developer_work_with_audio_files/,recastai,1459872826,,0,1,False,default,,,,,
149,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,2,4dhl4x,bytecoin.org,What the Blockchain Technology Can Do and What Could Be Overstretched Expectations,https://www.reddit.com/r/MachineLearning/comments/4dhl4x/what_the_blockchain_technology_can_do_and_what/,professorXY,1459878602,,1,0,False,default,,,,,
150,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,3,4dhwrg,self.MachineLearning,Learning to Search Papers?,https://www.reddit.com/r/MachineLearning/comments/4dhwrg/learning_to_search_papers/,Refefer,1459882675,"I'm trying to track down some of the papers used in the Vowpal Wabbit Learning to Search algorithms and am having a hard time finding them.  I've been able to find the slides via (this)[http://hunch.net/~l2s/] link, but they don't list references.  Could anyone point me in the right direction?",3,7,False,self,,,,,
151,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,4,4dhy7h,venturebeat.com,"Nvidia creates a 15B-transistor chip for deep learning [This is a beast of a machine, the densest computer ever made, Huang said.]",https://www.reddit.com/r/MachineLearning/comments/4dhy7h/nvidia_creates_a_15btransistor_chip_for_deep/,Yuli-Ban,1459883174,,53,135,False,http://a.thumbs.redditmedia.com/c9ezEs-6xyAwq7XFa5a2MUqNC2VXKAi421uj43HQgf0.jpg,,,,,
152,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,4,4di0ij,youtube.com,Microsoft Cognitive Services: Introducing the Seeing AI app,https://www.reddit.com/r/MachineLearning/comments/4di0ij/microsoft_cognitive_services_introducing_the/,bahidev,1459883998,,1,1,False,http://b.thumbs.redditmedia.com/F74Qq86vrfWcecd9a6ae2lWSIXuZhkbvE9KNrkCssIo.jpg,,,,,
153,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,4,4di0ma,nextplatform.com,"NVIDIA CEO Outlines ""New Model of Computing"" With Deep Learning at the Center",https://www.reddit.com/r/MachineLearning/comments/4di0ma/nvidia_ceo_outlines_new_model_of_computing_with/,[deleted],1459884032,[deleted],0,6,False,default,,,,,
154,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,4,4di4os,self.MachineLearning,Deep learning is so dependent on nVidia. Are there any alternatives even on the horizon?,https://www.reddit.com/r/MachineLearning/comments/4di4os/deep_learning_is_so_dependent_on_nvidia_are_there/,thecity2,1459885487,"Just don't like this aspect of deep learning that is so dependent on proprietary technology, but it seems like AMD is not even attempting to mount a challenge. Is there any hope for more open GPU alternatives?",88,39,False,self,,,,,
155,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,4,4di553,engadget.com,The gap is increasing between big companies and the individuals with this type of ultra expensive Deep Learning Machines,https://www.reddit.com/r/MachineLearning/comments/4di553/the_gap_is_increasing_between_big_companies_and/,cagbal,1459885649,,4,1,False,http://b.thumbs.redditmedia.com/ZrkoKiyHY7RAxbVDz53CFVHysDrISsRrRLtsr12q2kU.jpg,,,,,
156,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,5,4di79h,self.MachineLearning,Stupid question: how do I write my own dataset?,https://www.reddit.com/r/MachineLearning/comments/4di79h/stupid_question_how_do_i_write_my_own_dataset/,[deleted],1459886414,[deleted],5,0,False,default,,,,,
157,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,5,4di85e,youtube.com,The Next Rembrandt,https://www.reddit.com/r/MachineLearning/comments/4di85e/the_next_rembrandt/,gogglygogol,1459886718,,2,20,False,http://a.thumbs.redditmedia.com/tlxG4k8wICVpWoXWtHFZfbNMb4z9_endz-0YmAqMXs8.jpg,,,,,
158,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,5,4di8zx,self.MachineLearning,Expanding the number of things that are dynamically adjusted with neural nets,https://www.reddit.com/r/MachineLearning/comments/4di8zx/expanding_the_number_of_things_that_are/,BinaryAlgorithm,1459887014,"I've been trying to take in the state of the art on neural networks. Broadly:
* LIF neurons have a time encoding component with interesting properties and capabilities
* The Blue Brain project showed me that there are over 100 neuron sub-types (firing properties, LTP learning rules)
* NEAT demonstrates GA applied to NN's as well as the benefits of dynamic topology

With all of these in mind I am thinking that when implementing digital organisms (each with their own NN) inside of a simulation environment I should allow for variation in as many things as possible so that they can generate the necessary network complexity. This means adding/removing neurons, adding/removing connections, creating different neuron types and modifying neuron parameters (threshold, value decay rate, absolute refractory period, relative refractory start value and decay rate to base, etc). This is instead of trying to ""guess"" what the correct topography or neuron structure is. Similarly, instead of trying to guess at the best set of LTP shapes, allow the network to incrementally evolve its LTP curve for each neuron (as opposed to the widely used symmetric/asymmetric curve shapes, although they might be a good starting point). This means each neuron's learning pattern can also be dynamic as well during the organism's life cycle (another degree of freedom). Perhaps the idea could be expanded to GA or reproduction method as well; let organisms or species pools define their own mutation rates or other things that might normally have been hyper-parameters of the simulation environment; those with values that work well will tend to evolve fitness faster and those parameters may mutate values over time as well.

One of the big questions I have is whether to try to get most of these changes between generations or to try to adjust factors and topography during the organism's lifespan (that is what humans do during brain development and learning). I am used to running a full life cycle, calculating fitness, then making changes. However, that makes it hard to detect improvements in fitness except after running the entire cycle. I have read about other reward approaches to try to get functionality to improve during lifespan by making adjustments based off immediate reward input, but I also want the capability of encouraging multi-step behaviors which requires memory and some way of associating past actions towards the reward at the end (deep recurrent network with spike delays over long or arbitrary periods relative to the rest of the net? dynamic values for spike propagation delay? saved and triggered spike buffers?)

It has been harder to find guides applicable to the digital organism simulations, as opposed to many on specific domains where there is a training set. In order to get beyond ""move, search for food, and eat it"" level simulations, I think that communication between organisms and social grouping is necessary. I have yet to find anything addressing approaches to this level of complexity. That includes problems like communication encoding (such as ""speaking"" within proximity or leaving ""written"" artifacts on the ground to be found later), organism unique identities, and some memory mechanisms to store and act on the information now or later. I am not sure if NN's can get beyond ""programmed instincts"" without adjusting the ""mind"" during the organism's lifespan, and I am curious how to do that.",3,1,False,self,,,,,
159,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,6,4dilcg,sciencevsdeath.com,An optimistic review of state-of-the-arts in AI. Is it really possible to have human level AI in 2021?,https://www.reddit.com/r/MachineLearning/comments/4dilcg/an_optimistic_review_of_stateofthearts_in_ai_is/,[deleted],1459891590,[deleted],5,0,False,default,,,,,
160,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,6,4dipoz,self.MachineLearning,Use Data Light,https://www.reddit.com/r/MachineLearning/comments/4dipoz/use_data_light/,tthomsonc1,1459893156,[removed],0,1,False,default,,,,,
161,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,6,4diqkb,github.com,"Pastalog: Simple, realtime visualization of neural network training performance for Python and more",https://www.reddit.com/r/MachineLearning/comments/4diqkb/pastalog_simple_realtime_visualization_of_neural/,rewonc,1459893469,,10,30,False,http://b.thumbs.redditmedia.com/MFpewh_6D1XLO1-Bez2D4EUtswQNRz9zKxNEIqQSbNY.jpg,,,,,
162,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,7,4divyw,nvidia.com,"The NVIDIA DGX-1 Deep Learning System, Built for AI",https://www.reddit.com/r/MachineLearning/comments/4divyw/the_nvidia_dgx1_deep_learning_system_built_for_ai/,maaku7,1459895493,,3,2,False,default,,,,,
163,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,8,4dj15c,youtube.com,DanDoesData: Recurrent Neural Nets in Keras,https://www.reddit.com/r/MachineLearning/comments/4dj15c/dandoesdata_recurrent_neural_nets_in_keras/,vanboxel,1459897533,,0,0,False,default,,,,,
164,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,8,4dj3kc,usa.baidu.com,Baidus Silicon Valley AI Lab Announces Collaboration with Peel at GPU Tech Conference,https://www.reddit.com/r/MachineLearning/comments/4dj3kc/baidus_silicon_valley_ai_lab_announces/,etzmor,1459898494,,0,1,False,http://b.thumbs.redditmedia.com/VuS09bIO73nzxVtqyhvoFYtlpmgaWK8bOwemEaN5EGc.jpg,,,,,
165,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,8,4dj8q4,self.MachineLearning,How to transform feature with peak at zero to normal distribution?,https://www.reddit.com/r/MachineLearning/comments/4dj8q4/how_to_transform_feature_with_peak_at_zero_to/,Bohemian90,1459900608,[removed],0,1,False,default,,,,,
166,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,9,4djazh,self.MachineLearning,Implementing a Custom Convolution Kernel?,https://www.reddit.com/r/MachineLearning/comments/4djazh/implementing_a_custom_convolution_kernel/,[deleted],1459901541,[removed],0,1,False,default,,,,,
167,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,9,4djb01,arxiv.org,Revisiting Distributed Synchronous SGD,https://www.reddit.com/r/MachineLearning/comments/4djb01/revisiting_distributed_synchronous_sgd/,mttd,1459901545,,1,14,False,default,,,,,
168,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,9,4djbks,reddit.com,machinelearning subreddits curated by /u/pfcohen  /r/machinelearning,https://www.reddit.com/r/MachineLearning/comments/4djbks/machinelearning_subreddits_curated_by_upfcohen/,pfcohen,1459901805,,0,3,False,http://b.thumbs.redditmedia.com/yK_6r3YinYWZqCLBZCiwLylDjFyKmXXOmjQXXC6-vIA.jpg,,,,,
169,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,9,4djc66,self.MachineLearning,Best way to model documents?,https://www.reddit.com/r/MachineLearning/comments/4djc66/best_way_to_model_documents/,slushi236,1459902065,"I have a system as follows:

* large historical set of documents with more added continuously (hundreds/second).
* large number of historical user actions, e.g. user 1 read article A at time t, then read article B at time t+1. Again, more actions are added over time.

Using this data, I'd like to be able to quickly model the meaning of each document as a vector -- quickly meaning something I can run immediately while ingesting new documents. So I don't think something like Doc2Vec wouldn't work for me. 

 I know I can do simple BoW modeling but I am wondering if something better can be done given I have some user activity information. 

I have been kicking around the idea that I could use the same rough idea as Word2Vec: if a user reads a few articles in a row, those documents are probably similar and the context of the ""surrounding"" documents could be used in the loss function while training the ""center"" document vector. 

But I am not sure how to model the generation of the document vectors. RNNs/LSTMs seem like a natural way to model documents, but here there could be longish articles (2000-3000 words), and I am not sure what the loss function should even produce until the end of the article is reached. 

This seems like a fairly common problem, are there papers out there that address this? I'm a bit of a newbie so sorry if I am missing something obvious here.",5,7,False,self,,,,,
170,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,9,4dje1z,mediastream.cern.ch,Deep Learning and the Future of AI - Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/4dje1z/deep_learning_and_the_future_of_ai_yann_lecun/,elisee,1459902869,,17,40,False,default,,,,,
171,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,9,4djgfy,datasciencecentral.com,Training a Computer to Recognize Your Handwriting,https://www.reddit.com/r/MachineLearning/comments/4djgfy/training_a_computer_to_recognize_your_handwriting/,vincentg64,1459903861,,0,1,False,default,,,,,
172,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,9,4djhdf,self.MachineLearning,Question on how I should go about this. (kinda long),https://www.reddit.com/r/MachineLearning/comments/4djhdf/question_on_how_i_should_go_about_this_kinda_long/,elitepker,1459904282,"Hey, guys, first post on this subreddit, I have always been a lurker on this Reddit and interested in Machine Learning, however, have never had a need for it till now. So pretty much I have a question on how to implement in a game. The game has a stock market; I like to make my money buying bulk item low, and selling high, and making bank off the margin (flipping). I have a list of items, on a file, each saying if the ""flip"" was successful or not, and how long it took to flip. I want to develop a way, so it takes all the items, and calculates and sorts the items in which items are fastest flips on average, and which items are most successful. Do you think I can do this without machine learning, and if so is it harder then machine learning, and if you believe machine learning would be good for this task how would you go about it. Thanks s much in advance; I can't wait to read the responses!",2,3,False,self,,,,,
173,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,12,4dk3uw,self.MachineLearning,Scaling classifiers for a website... One model per user?,https://www.reddit.com/r/MachineLearning/comments/4dk3uw/scaling_classifiers_for_a_website_one_model_per/,foozaboop,1459914071,"I am trying to build a simple web app that will aggregate book/movie reviews for a few users, and make recommendations based on those.

The way this is modeled is that given a matrix X and a set of labels Y, each user would have their own Y for the same X. Aka, Tom's review of movies (A, B, C) can be Y_tom = (1,0,1) while Bob's reviews for (A,B,C) are (0,1,1).

Assuming I can build X and Y_i for every user i, is there a smarter way to build this or will I have to learn N models, one per user? Is there a smarter way?",2,0,False,self,,,,,
174,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,13,4dk9ri,fdjohnson.full-design.com,How Lubrication is Effective for your Machinery?,https://www.reddit.com/r/MachineLearning/comments/4dk9ri/how_lubrication_is_effective_for_your_machinery/,jackerfrinandis,1459917222,,0,1,False,default,,,,,
175,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,16,4dkpkj,youtube.com,Bng ti pvc bn thao tc inox,https://www.reddit.com/r/MachineLearning/comments/4dkpkj/bng_ti_pvc_bn_thao_tc_inox/,bangtaitruongtho,1459926651,,0,0,False,http://b.thumbs.redditmedia.com/NRrUnFXnT9wBqC8430Jl_nXcijIIkh1U-gw7wteSSAo.jpg,,,,,
176,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,16,4dkqt7,self.MachineLearning,Some state-of-the-arts in natural language processing and their discussion,https://www.reddit.com/r/MachineLearning/comments/4dkqt7/some_stateofthearts_in_natural_language/,[deleted],1459927473,[deleted],1,1,False,default,,,,,
177,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,16,4dkrw1,self.MachineLearning,Some state-of-the-arts in natural language processing and their discussion,https://www.reddit.com/r/MachineLearning/comments/4dkrw1/some_stateofthearts_in_natural_language/,Sergej_Shegurin,1459928190,"Current state-of-the-arts in terms of perplexity:

dataset |  perplexity | link
---------|----------|----------
Wikipedia english corpus snapshot 2014/09/17 (1.5B words)  | 27.1  | [1]
1B word benchmark (shuffled sentences)  | 24.2 | [2]
OpenSubtitles (923M words)   | 17 | [17]
IT Helpdesk Troubleshooting (30M words)   | 8 | [17]
Movie Triplets (1M words)   | 27 | [18]
PTB (1M words)   | 62.34 | [19]

Why perplexity matters?
If neural net says smth inconsistent (in **any** sense:  logical, syntactic, pragmatic) then it means that it gives too much probability to some inappropriate words i.e, it's perplexity isn't optimized yet.  When hierarchical neural chat bots would achieve low enough perplexity, they would likely to write coherent stories, to answer intelligibly with common sense, to reason in a consistent and logical way etc.  Just as in [141] we would be able to adjust a conversational model to imitate style and opinions of a distinct person.

What are reasonable predictions for perplexity improvements in the nearest future?
Let's take two impressive recent works both submitted to arxiv in February 2016.  They are:  ""contextual
LSTM models for large scale NLP tasks"" [1] and ""Exploring the limits of language modeling"" [2].

Table 2: Best perplexity scores, **single** model:

number of hidden neurons |  perplexity in [1]  | perplexity in [2] 
---------|----------|----------
256 | 38  | --
512  | -- | 54
1024  | 27 | --
2048 | -- | 44
4096  | -- | --
8192 | -- | 30 (ensemble gives 24.2)

From table 2, it's quite reasonable to predict that for [1] hs=4096 might give pplx &lt; 20, hs=8192 might give pplx &lt; 15 and ensemble of models with hs=8192 trained on 10B words might give **perplexity well below 10.** Nobody can tell now what kind of common sense reasoning would such a neural net have.

Shannon estimated lower and upper bounds of human perplexity to be 0.6 and 1.3 bits per character [34]. Strictly speaking, applying formula (17) in his work gives lower bound equal 0.648 which he rounded. Given average word length of 4.5 symbols and including spaces (as Shannon included them in his game)
gives us **an estimate for lower bound on human-level word perplexity as 11.8** = 2 ^ (0.648 * 5.5) or better to say 10 plus something.  This lower bound might be much less than real human perplexity [30]. Another source of improvement may come from solving some discrepancy in what kind of perplexity is
optimized [35] [36].  However, both KL(P||Q) and KL(Q||P) have optimum when P=Q. Also, [35] [36] propose  ways  to  partially  solve  that  problem  using  adversarial  learning.   ""Generating  sequences  from continuous space""[97] demonstrates impressive advantages of adversarial paradigm.

Table 3. Best BLEU scores

lang pair |  BLEU  | dataset | link
---------|----------|----------|----------
en=&gt;fr | 37.5  | WMT'14 | [3]
fr=&gt;en  | 35.8 | WMT'14 | [4]
ar=&gt;en  | 56.4 | NIST OpenMT'12 | [5]
ch=&gt;en | 40.06 | MT03 | [6]
ge=&gt;en  | 29.3 | WMT'15 | [7]
en=&gt;ge | 26.5 | WMT'15 | [7]
en=&gt;ru | 29.37 | newstest-14 | [8]
en=&gt;ja | 36.21 | WAT'15 | [9]

Human  BLEU  score  for  chinese=&gt;english  translation  on  MT03  dataset  is  35.76  [15].   
In  recent  article  [6]  neural  network  gets  40.06  BLEU  on  the  same  task  and  dataset.

----------------
I've read ~ several thousand articles on deep learning in recent several years.

The product of my work is a review of current state-of-the-arts in many areas of AI which also contains references for links in this post:

http://sciencevsdeath.com/review-of-state-of-the-arts.pdf",30,31,False,self,,,,,
178,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,17,4dkvdk,self.MachineLearning,How do you train/validate neural networks?,https://www.reddit.com/r/MachineLearning/comments/4dkvdk/how_do_you_trainvalidate_neural_networks/,wtfalreadyinuse,1459930724,[removed],2,0,False,default,,,,,
179,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,17,4dkw87,self.MachineLearning,Question About Random Forests,https://www.reddit.com/r/MachineLearning/comments/4dkw87/question_about_random_forests/,Icesliced,1459931375,"Hey guys,

So I'm trying to grasp my head around Machine Learning, but I'm hearing different things left and right and just plain confused at this point.

If I want to create a Random Forest Classifier that predicts future data based off of historical data what should my features be/work best? I'm trying this for NBA Modelling so here's what I've read/seen

- Averages of every game over 15 games + Averages of every home/away game over 10 games (Lots of Features here, like close to 3000 features (95% sure this is wrong))

- ARIMA with 9 key variables and use data from earlier in the season to predict these 9 values using this ARIMA for each value and tuning the variables with the training set

I'm new to this. Probably in over my head. I have nothing but time. If someone could explain what is the best way for me to predict future data and use it in a random forest classifier or if I'm being stupid and need to be using a different Learning Machine I'd appreciate any help I can get. I think I'm on the right path, but I can't be sure. I've heard more features is good and that you only want a small set of features because you get too much noise. Hopefully someone can tell me if I'm on the right track or looking at this the wrong way. 

I'm new to this. I didn't even know machine learning existed two weeks ago. Just a bored College Student with too much time. I think I have the basic concept of everything but I could be wrong. 

",4,1,False,self,,,,,
180,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,17,4dkxm3,self.MachineLearning,What is better PCA or SVD,https://www.reddit.com/r/MachineLearning/comments/4dkxm3/what_is_better_pca_or_svd/,realhamster,1459932456,"If I understand them correctly, they are both trying to reduce the rank of a matrix, so that we are left with a reduced amount of features which capture most of the information in our dataset. So do they have any difference in the results they give?",11,9,False,self,,,,,
181,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,19,4dl42j,self.MachineLearning,Gradients with respect to multiple samples in parallel?,https://www.reddit.com/r/MachineLearning/comments/4dl42j/gradients_with_respect_to_multiple_samples_in/,MeAlonePlz,1459937008,"Hey all,

I tried to post the question in the theano user google group but got no answer, and it really bugs me so im gonna try here.

so I'm currently trying to implement variational inference where I have discrete latent variables ( http://arxiv.org/abs/1602.06725 ).
The important part is that I have gradients that depend on a sampling procedure and I have to calculate one gradient for every sample and only after that can take the mean.

In the method described in the paper I take multiple samples for every data point, and then additionally use SGD on minibatches.
Now I can put the costs into a tensor but in Theano I can only take the gradients with respect to scalars.
I have an implementation using a loop (scan) and it works, but it is very slow.

I don't really see why it shouldn't be possible to take the gradients in parallel as my network is very small. 

So does anybody know if this is possible in Theano or any other Framework (I'd be willing to change if it fixes the issue)
",12,4,False,self,,,,,
182,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,19,4dl5i0,self.MachineLearning,Feature map to positions [HELP],https://www.reddit.com/r/MachineLearning/comments/4dl5i0/feature_map_to_positions_help/,hapliniste,1459937885,"Hi everyone, I'm trying to convert a feature map [F,64,64] to a list of positions [F,2].

I have some intuitions on how to do it but now need a bit of help for the real implementation.

This Fmap to Pos model will go after some CNN that takes an image and make it to the shape [16,64,64] (16 features).

I will also use a position matrix [2,64,64] that store the position at the given point (0,0;0,1;0,2 _ 1,0;1,1;...).

Here's a quick conceptual model of what I wanna do:
___
(INPUT) [16,64,64] 

-&gt;Binarize (set to 1 or 0 based on a treshold (lets say 0.5)): to create a mask

(MASK)[16,64,64]

-&gt;Mask (PosMat) with (MASK): we now get a map with positions informations scattered on it (for each F:16)

(PosMap)[16,2,64,64]

-&gt;Pooling? : I need to transform the (PosMap) to a [16,2] tensor. (I don't think using max pooling would work)
___

So, maybe you guys can help me on what operations I need to use. I work on TF/Keras, but some insight from others libs or even pure math could help me too!

Thank for reading! :) Any comment is welcome. Tell me if this is full of flaws or something.",2,0,False,self,,,,,
183,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,20,4dlbky,alanzucconi.com,Evolutionary Computation - Part 1,https://www.reddit.com/r/MachineLearning/comments/4dlbky/evolutionary_computation_part_1/,Chuckytah,1459941874,,48,91,False,http://b.thumbs.redditmedia.com/js2BFvSkT8y_1gKzf2QKALglX_c2piKgIFmGnIDiOvE.jpg,,,,,
184,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,21,4dlj5p,self.MachineLearning,DGX-1 vs 2 x quad TitanX boxes,https://www.reddit.com/r/MachineLearning/comments/4dlj5p/dgx1_vs_2_x_quad_titanx_boxes/,smith2008,1459946077,"I don't think anyone around here cares how DGX-1 compares to CPUs so I hope someone could shed some light for us. They say they can train AlexNet in 2 hours but how much faster this would be than training it on two quad TitanX systems? If anyone has done that (or done it one one) please share what timings you've got.

Can't wait to see a consumer version of P100.

EDIT: found it [here](https://blogs.nvidia.com/blog/2015/03/17/digits-devbox/) . ""Training AlexNet can be completed in only 13 hours with the DIGITS DevBox"". So I guest in ~7 hours with two of those.",10,5,False,self,,,,,
185,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,22,4dloc2,technologyreview.com,Siri - and friends - for business,https://www.reddit.com/r/MachineLearning/comments/4dloc2/siri_and_friends_for_business/,jonfla,1459948483,,0,0,False,http://a.thumbs.redditmedia.com/gNtebRV0EpHqtQi3K3Mmrj9pn8TnEcIG7Nb-cdRAZp0.jpg,,,,,
186,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,22,4dlptb,self.MachineLearning,What are you working on?,https://www.reddit.com/r/MachineLearning/comments/4dlptb/what_are_you_working_on/,weirdML,1459949117,,59,53,False,self,,,,,
187,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,23,4dm0x2,self.MachineLearning,What exhibits or talks at GTC 2016 do you find interesting?,https://www.reddit.com/r/MachineLearning/comments/4dm0x2/what_exhibits_or_talks_at_gtc_2016_do_you_find/,fabhan,1459953756,,0,0,False,self,,,,,
188,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,23,4dm2e8,developer.nvidia.com,cuDNN v5 RC just came out,https://www.reddit.com/r/MachineLearning/comments/4dm2e8/cudnn_v5_rc_just_came_out/,abstractcontrol,1459954312,,16,45,False,http://b.thumbs.redditmedia.com/zXTBksiX1bUFQoyXSFVIu1M91UdT8WuZonqu6QhQHck.jpg,,,,,
189,MachineLearning,t5_2r3gv,2016-4-6,2016,4,6,23,4dm3jy,people.idsia.ch,On the Visual Perception of Forest Trails,https://www.reddit.com/r/MachineLearning/comments/4dm3jy/on_the_visual_perception_of_forest_trails/,MjrK,1459954773,,1,2,False,default,,,,,
190,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,0,4dmblu,self.MachineLearning,I'm interested in deep learning and I've finished Andrew Ng's course. Which of these 3 online courses would be best to try next?,https://www.reddit.com/r/MachineLearning/comments/4dmblu/im_interested_in_deep_learning_and_ive_finished/,relganz,1459957705,"1. [Deep Learning at Udacity](https://www.udacity.com/course/deep-learning--ud730)

2. [Deep Learning in Python at Udemy](https://www.udemy.com/data-science-deep-learning-in-python/)

3. [Practical Deep Learning in Theano + TensorFlow at Udemy](https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow/)

4. [Neural Nets at Coursera](https://www.coursera.org/course/neuralnets)

5. [Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/syllabus.html)

I'm aware that Kaggle and personal projects are important as well, but this question is just about the courses.  Does anyone who has completed any of these have feedback?  Thanks.

Edit: Added a 4th candidate that I found

Edit 2: Since people are recommending Karpathy's course, adding it here as a reference.  It's not a 'native online' course, but who cares?
",26,8,False,self,,,,,
191,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,0,4dmcdw,youtu.be,Video Pydata Amsterdam: Finding relations in documents with IEPY,https://www.reddit.com/r/MachineLearning/comments/4dmcdw/video_pydata_amsterdam_finding_relations_in/,copybin,1459957998,,0,1,False,http://b.thumbs.redditmedia.com/1cNHON896k1VH2EkU03yKfV76XfvdFDvpVjvT1LFyKo.jpg,,,,,
192,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,0,4dmd2g,self.MachineLearning,Have DeepMind's results generally been reproducible?,https://www.reddit.com/r/MachineLearning/comments/4dmd2g/have_deepminds_results_generally_been_reproducible/,cjmcmurtrie,1459958235,"It seems like anything DeepMind attempts achieves state-of-the-art on some task. Their output as a group is pretty prolific. I was very excited about their [incorporation of variational inference into topic modelling](http://arxiv.org/pdf/1511.06038v3.pdf). However, someone [on this sub](https://www.reddit.com/r/MachineLearning/comments/4b7dvx/neural_variational_inference_for_text_processing/d18ew29) implemented the model and could not reproduce their SOTA perplexity (actually, the user reported perplexity worse than even LDA, which has been surpassed many times).

Is there a general sense that DeepMind's results have been reproducible? I'm not being cynical or suspicious, I'm just curious about the consensus.",6,19,False,self,,,,,
193,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,1,4dmdon,turing.deepart.io,Online turing test for 'DeepStyle',https://www.reddit.com/r/MachineLearning/comments/4dmdon/online_turing_test_for_deepstyle/,[deleted],1459958456,[deleted],0,1,False,default,,,,,
194,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,1,4dmhfd,blog.exabeam.com,Expert-driven statistical modeling is a key and core component of an anomaly detection system,https://www.reddit.com/r/MachineLearning/comments/4dmhfd/expertdriven_statistical_modeling_is_a_key_and/,ChandCurly5,1459959804,,0,0,False,http://a.thumbs.redditmedia.com/zDscdWRKUaHO6BNTIERBwIbUN4XIzaJX7MNC2STL_O0.jpg,,,,,
195,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,1,4dmhh4,spectrum.ieee.org,Speech recognition to make diet-tracking easier: just tell your phone what you're eating.,https://www.reddit.com/r/MachineLearning/comments/4dmhh4/speech_recognition_to_make_diettracking_easier/,newsbeagle,1459959823,,0,1,False,default,,,,,
196,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,1,4dmm2t,self.MachineLearning,"In Multilayer Perceptrons, what is the point of a sigmoid function?",https://www.reddit.com/r/MachineLearning/comments/4dmm2t/in_multilayer_perceptrons_what_is_the_point_of_a/,sjalfurstaralfur,1459961438,I'm currently studying MLPs and I dont get why it suddenly switches from a simple threshold function to a sigmoid function. What's wrong with the simple threshold function? Why do you need a sigmoid? Is it to simply have more variety in the output rather than just 1 or 0?,12,1,False,self,,,,,
197,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,1,4dmmm4,self.MachineLearning,Is there a name for linear regression where you add parameters for every combination of products of the input variables to the hypothesis function?,https://www.reddit.com/r/MachineLearning/comments/4dmmm4/is_there_a_name_for_linear_regression_where_you/,relganz,1459961657,"Let's say you have 3 features X1, X2, and X3 and are predicting h.  The variables T1, T2, etc. are the parameters/weights.

A normal linear regression hypothesis would be roughly this:

h = X1T1 + X2T2 + X3T3

What is the name for a version that looks like this?

h = X1T1 + X2T2 + X3T3 + X1X2T4 +X1X3T5 + X2X3T6 + X1X2X3T7

Are there are cases where this is useful?  Thanks.",11,0,False,self,,,,,
198,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,2,4dmr9e,self.MachineLearning,Beginner dev who wants to start attending ML-focused conferences. Any advice?,https://www.reddit.com/r/MachineLearning/comments/4dmr9e/beginner_dev_who_wants_to_start_attending/,Sanctimonia13,1459963286,"I became a Software Engineer through an accelerated bootcamp I finished almost a year ago. Since then I've narrowed down my long term interest to Machine Learning and AI development. Because of the high ROI I've noticed on my previous endeavors, I like to target immersive approaches for learning about subjects of interest.

That, the value of learning firsthand from experts in the field, and the availability of introductory tutorials makes ML conferences very appealing. 

I'd love to hear advice from anyone who has attended these for a dev who's new to the field. I was considering ODSC first (I'm based in Boston), maybe the NYC ML conference, and then (if not too impossible) maybe NIPS in December.

Thanks!",2,1,False,self,,,,,
199,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,3,4dn5lw,self.MachineLearning,PhysNet implementation?,https://www.reddit.com/r/MachineLearning/comments/4dn5lw/physnet_implementation/,[deleted],1459968327,[deleted],0,1,False,default,,,,,
200,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,4,4dnco3,medium.com,Why Algorithms are Worthless,https://www.reddit.com/r/MachineLearning/comments/4dnco3/why_algorithms_are_worthless/,kumquatz,1459970876,,6,0,False,http://b.thumbs.redditmedia.com/we8n-5FEFGX5qSsqJjMRy3w2-YmRAL_UyY78Ufqdlhc.jpg,,,,,
201,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,4,4dnefj,quantamagazine.org,Mapping the Brain to Build Better Machines | Quanta Magazine,https://www.reddit.com/r/MachineLearning/comments/4dnefj/mapping_the_brain_to_build_better_machines_quanta/,pedromnasc,1459971502,,3,0,False,http://b.thumbs.redditmedia.com/bw6jmUK_sIGSv0A1Yboeol8K0aNBsutiGx4R8A7LmZs.jpg,,,,,
202,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,4,4dnfon,news.meta.com,"Meta Science, a free scientific literature discovery engine, is now live",https://www.reddit.com/r/MachineLearning/comments/4dnfon/meta_science_a_free_scientific_literature/,Aprioribox,1459971945,,2,15,False,http://b.thumbs.redditmedia.com/NCahprD1agpevJxBOkNnbn5d2Bbm1oWhyOteQwVVRSY.jpg,,,,,
203,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,4,4dngtc,i.imgur.com,The Gradient Warrior!,https://www.reddit.com/r/MachineLearning/comments/4dngtc/the_gradient_warrior/,[deleted],1459972381,[deleted],0,0,False,default,,,,,
204,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,4,4dnh3d,self.MachineLearning,"Easy to understand explanation of reinforcement learning ""eligibility traces?""",https://www.reddit.com/r/MachineLearning/comments/4dnh3d/easy_to_understand_explanation_of_reinforcement/,student_of_rl,1459972486,[removed],0,1,False,default,,,,,
205,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,5,4dnnx7,self.MachineLearning,Anyone use contour based segmentation for region proposal?,https://www.reddit.com/r/MachineLearning/comments/4dnnx7/anyone_use_contour_based_segmentation_for_region/,mikos,1459974975,"I wonder why approaches such as Selective Search are so popular, especially given their time consuming nature, when simpler older approaches such as:
    Image-&gt;canny-&gt;contour-&gt;poly approx (region) 

seem a much simpler and gives comparable results IMHO.
Am i missing something?",0,1,False,self,,,,,
206,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,6,4dnvio,devblogs.nvidia.com,Optimizing Recurrent Neural Networks In CuDNN 5,https://www.reddit.com/r/MachineLearning/comments/4dnvio/optimizing_recurrent_neural_networks_in_cudnn_5/,harrism,1459977778,,14,35,False,http://b.thumbs.redditmedia.com/Q31wgExbDfgtw_2QKI1lPSQdwZBc7b9dUNZ6claCv-Q.jpg,,,,,
207,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,6,4dnyiz,self.MachineLearning,Question about loss clipping on DeepMind's DQN,https://www.reddit.com/r/MachineLearning/comments/4dnyiz/question_about_loss_clipping_on_deepminds_dqn/,n00bkilla555,1459978863,"I am trying my own implementation of the DQN paper by Deepmind in tensor flow and am running into difficulty with clipping of the loss function. Here is an excerpt from the nature paper describing the loss clipping:
&gt; We also found it helpful to clip the error term from the update   to be between 1 and 1. Because the absolute value loss function |x| has a derivative of 1 for all negative values of x and a derivative of 1 for all positive values of x, clipping the squared error to be between 1 and 1 corresponds to using an absolute value loss function for errors outside of the (1,1) interval. This form of error clipping further improved the stability of the algorithm.


(link to full paper: http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)

What I have tried so far is using tf.clip_by_value to clip the loss I calculate between -1 and +1. The agent is not learning the proper policy in this case. I printed out the gradients of the network and realized that if the loss falls below -1, the gradients all suddenly turn to 0! 

My reasoning for this happening is that the clipped loss is a constant function in (-inf,-1) U (1,inf), which means it has zero gradient in those regions. This in turn ensures that the gradients throughout the network are zero (think of it as, whatever input image I provide the network, the loss stays at -1 in the local neighborhood because it has been clipped).

So, my question is two parts:
1. What exactly did Deepmind mean in the excerpt? Did they mean that the loss below -1 is clipped to -1 and above +1 is clipped to +1. If so, how did they deal with the gradients (i.e. what is all that part about absolute value functions?)
2. (bonus question) How should I implement the answer to above in tensor flow?

Thanks! ",5,6,False,self,,,,,
208,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,7,4do336,marketintelreports.com,Does Axial Piston Hydraulic Motors Consumption market Really Matter !!,https://www.reddit.com/r/MachineLearning/comments/4do336/does_axial_piston_hydraulic_motors_consumption/,tharunvicky,1459980653,,0,1,False,default,,,,,
209,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,7,4do8ff,self.MachineLearning,Getting Started with Machine Learning (Not Understanding the Subreddit FAQ/Wiki),https://www.reddit.com/r/MachineLearning/comments/4do8ff/getting_started_with_machine_learning_not/,DarkLord987,1459982767,"Hi!

So I just looked over the beginners FAQ/Wiki and I am horribly confused. I have been interested in machine learning, but I have no idea where to start. I am still having a hard time after watching some tutorials because most don't make any sense even though they are geared towards ""beginners."" If this information helps, I am a high school student who is knowledgeable in computer science and has never worked with anything related to machine learning. Any links, tutorials, videos, articles etc. for a beginner like me would be great. I would like to get started with machine learning as soon as possible, but I will need your help to get me off my feet.

Thanks, Matt",10,3,False,self,,,,,
210,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,8,4doeh0,self.MachineLearning,Can anyone recommend a good course or articles on handling text (short and long form) as features for classification problems,https://www.reddit.com/r/MachineLearning/comments/4doeh0/can_anyone_recommend_a_good_course_or_articles_on/,Visibleone,1459985184,Bonus points for torch(Lua),0,1,False,self,,,,,
211,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,8,4dog09,youtu.be,"Neural Network ""Learning"" Images (and extrapolating)",https://www.reddit.com/r/MachineLearning/comments/4dog09/neural_network_learning_images_and_extrapolating/,tannerbohn,1459985826,,0,0,False,http://b.thumbs.redditmedia.com/gb9oRDkyk5nnTZUly5A-ysiIUURyT6VmFn-eU7Rr8bg.jpg,,,,,
212,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,9,4dolxb,xn--qi8hyak86eybv6bfos5e.xn--vi8hiv.ws,"[Supervised Learning:] This tool was fed ""labeled-data"" from tens of thousands of ""polite and impolite"" sentences. It grades the ""politeness"" of any sentence.",https://www.reddit.com/r/MachineLearning/comments/4dolxb/supervised_learning_this_tool_was_fed_labeleddata/,[deleted],1459988306,[deleted],31,77,False,default,,,,,
213,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,9,4domnk,facebook.com,The Deep Learning textbook is now complete,https://www.reddit.com/r/MachineLearning/comments/4domnk/the_deep_learning_textbook_is_now_complete/,clbam8,1459988637,,118,413,False,default,,,,,
214,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,9,4dor5l,arxiv.org,[1604.01662] Towards Bayesian Deep Learning: A Survey,https://www.reddit.com/r/MachineLearning/comments/4dor5l/160401662_towards_bayesian_deep_learning_a_survey/,InaneMembrane,1459990600,,1,13,False,default,,,,,
215,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,9,4dor5q,self.MachineLearning,What's the reason Facebook stuck to Lua for Torch?,https://www.reddit.com/r/MachineLearning/comments/4dor5q/whats_the_reason_facebook_stuck_to_lua_for_torch/,bourbondog,1459990603,The language in general is terrible. I don't understand why a software development company would endorse such a language for math intensive purposes. Can someone shed some light?,15,3,False,self,,,,,
216,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,11,4dp33q,self.MachineLearning,"Holistic/Reductionist AI, Maths required for certain machine learning?...",https://www.reddit.com/r/MachineLearning/comments/4dp33q/holisticreductionist_ai_maths_required_for/,Guthien123,1459995736,"I'm not experienced in AI or machine learning much at all, but currently want to get into it. I've heard of reductionist AI and holistic AI. Reductionist AI seems to be the kind in which solves problems based on reasoning, (e.g. a problem that a human would need to think hard on with pencil and paper). And for holistic AI, are their actions based on pattern? Such as adapting over time? Again, I'm new to this so please forgive me if I'm confusing.. Because I'm confused myself. Do tell me if I have those two wrong or out of order.

Anyways, for a reliable, human-like AI, it would need holistic learning alongside reductionist learning, correct? What are the pros and cons for holistic vs reductionist AI? 

Also, when going into the path of AI, are they ever separated into holistic / reductionist courses? I've always loved AI and want to get into it, and I mean, AI that can do human-like functions. I realize that there are a lot of machine learning, AI, programming, mathematics, philosophy, etc classes needed / recommended for creating AI. What courses are required and recommended for going into the field of creating human-like functioning AI? Thank you so much in advanced, and sorry if that was poorly worded. 

If this subreddit disregards teenager 'noobs' like me into the uneducated pile, I'm sorry for coming.. it's just been hard trying to educate myself and find more info.. Thanks in advanced for responses.",3,0,False,self,,,,,
217,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,12,4dpafn,wildml.com,"Deep Learning for Chatbots, Part 1  Introduction",https://www.reddit.com/r/MachineLearning/comments/4dpafn/deep_learning_for_chatbots_part_1_introduction/,pogopuschel_,1459998979,,0,13,False,http://b.thumbs.redditmedia.com/CPmAOpXrfLa9ImQ-deA53F4awAtFRe6P3myIqe1215g.jpg,,,,,
218,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,13,4dpj2z,jayantj.github.io,Analysing classic literature with word2vec,https://www.reddit.com/r/MachineLearning/comments/4dpj2z/analysing_classic_literature_with_word2vec/,jayantjain12,1460003275,,0,10,False,http://a.thumbs.redditmedia.com/jOSxXQ2AZYBFV2aYj5rV-j-QhezKDcses9mR5o66DG8.jpg,,,,,
219,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,16,4dq0aa,self.MachineLearning,Master's thesis about NILM applied to IT-Security,https://www.reddit.com/r/MachineLearning/comments/4dq0aa/masters_thesis_about_nilm_applied_to_itsecurity/,[deleted],1460013632,[deleted],0,0,False,default,,,,,
220,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,18,4dqclf,self.MachineLearning,Reinforcement Learning: The dilemma of discretization and performance metrics in continuous actions and continuous states space.,https://www.reddit.com/r/MachineLearning/comments/4dqclf/reinforcement_learning_the_dilemma_of/,siddkotwal,1460022435,"I am trying to write an adaptive controller for a control system, namely a power management system using Q-learning. I recently implemented a toy RL problem for the cart-pole system and worked out the formulation of the helicopter control problem from Andrew NG's notes. I appreciate how value function approximation is imperative in such situations. However both these popular examples have very small number of possible discrete actions. I have three questions:

1) What is the correct way to handle such problems *if you don't have a small number of discrete actions*? The dimensionality of my actions and states seems to have blown up and the learning looks very poor, which brings me to my next question.

3) *How do I measure the performance of my agent?* Since the reward changes in conjunction with the dynamic environment, at every time-step I can't decide the performance metrics for my continuous RL agent. Also unlike gridworld problems, I can't check the Q-value table due to huge state-action pairs, how do I know my actions are optimal? 

3) Since I have a model for the evoluation of states through time. 
States = [Y, U].
Y[t+1] = aY[t] + bA, where A is an action. 

*Choosing discretization step for actions A will also affect how finely I have to discretize my state variable Y.* How do I choose my discretization steps?



Thanks a lot!",10,3,False,self,,,,,
221,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,19,4dqeh8,self.MachineLearning,Research Topic to work on ?,https://www.reddit.com/r/MachineLearning/comments/4dqeh8/research_topic_to_work_on/,sitarwars,1460023713,I am a beginner in the area of Machine Learning. Can you suggest a research project idea that is new and might lead to a publication or so,7,0,False,self,,,,,
222,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,19,4dqgey,self.MachineLearning,[Career Advice] Should I go for an advanced degree?,https://www.reddit.com/r/MachineLearning/comments/4dqgey/career_advice_should_i_go_for_an_advanced_degree/,InterestedInML,1460024999,[removed],0,3,False,default,,,,,
223,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,19,4dqhay,self.MachineLearning,RNN resources,https://www.reddit.com/r/MachineLearning/comments/4dqhay/rnn_resources/,tw2927,1460025555,"What is a good way to learn to implement and some theory behind LSTMs. So far I have found papers and blogs, but is there a more comprehensive resource available?",2,0,False,self,,,,,
224,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,21,4dqr5f,re-work.co,"Deep Learning: Modular in Theory, Inflexible in Practice - Q&amp;A with Enlitic's Senior Data Scientist #reworkDL#WHD2016",https://www.reddit.com/r/MachineLearning/comments/4dqr5f/deep_learning_modular_in_theory_inflexible_in/,reworksophie,1460031273,,0,1,False,default,,,,,
225,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,21,4dqu4w,self.MachineLearning,First time building a text classier.,https://www.reddit.com/r/MachineLearning/comments/4dqu4w/first_time_building_a_text_classier/,ZloyeZlo,1460032732,"Hi all,

This may get a bit too long, but thanks for reading.
I've recently got interested in Machine Learning, and started reading/watching videos on the subject. But still I have a very limited grasp of the whole idea. 

As my first project, I would like to build a text classifier. The data are sentence level, it's a sample of hand coded evaluations of actors in a text. It has two variables
'Sentence' and 'Evaluation'.
The sentence variable looks something like this: [Beginning of sentence][Actor X][Rest of the sentence]

The sentences are actual text, and 'Evaluation' is categorical variable with 'positive', 'negative' or 'neutral' values.

I want to train a model to eventually provide actor evaluations in sentences it never seen before.

Is it feasible/doable in R or Python (that's what the only languages I know). And where would I need to start? 

Any info would be greatly appreciated. Thanks.
",1,0,False,self,,,,,
226,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,22,4dr2aq,pocketcluster.wordpress.com,"BigData frameworks, libraries and toolsets weekly roundup  Apr. 7, 2016",https://www.reddit.com/r/MachineLearning/comments/4dr2aq/bigdata_frameworks_libraries_and_toolsets_weekly/,stkim1,1460036292,,0,1,False,default,,,,,
227,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,22,4dr3ty,self.MachineLearning,Help improving my Twitter rumor detection research,https://www.reddit.com/r/MachineLearning/comments/4dr3ty/help_improving_my_twitter_rumor_detection_research/,bootyapplause,1460036930,"I'm fairly new to machine learning and nlp but my current workflow is:

**Zika Tweet Collection** (completed) - Have Tweet ID, User ID, User location, Tweet contents

**Rumor Detection** - identify Zika virus related rumors

* *Training Set* - manually classify 2500 Tweets as rumor-like or not rumor-like and give reasons why
* *Classifier* - Use training set to create list of properties. Give properties weights based on how often they were associated with a rumor-like tweet. Use this to classify remaining Tweets. 
* *Supervision* - Use bagging to smooth classifier, use confusion matrix to weed out properties, and update training set whenever new properties emerge.

Does this workflow look appropriate? I've currently started manually classifying Tweets and identifying rumor-like properties.

Thanks! I got a lot of help defining my workflow from /r/languagetechnology.",5,5,False,self,,,,,
228,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,23,4dr94s,self.MachineLearning,Optimized Twiddle Algorithm,https://www.reddit.com/r/MachineLearning/comments/4dr94s/optimized_twiddle_algorithm/,criticalcontext,1460039058,"So, I have been using [Twiddle](https://martin-thoma.com/twiddle/) to do some basic parameter optimization for my code. However, I am increasingly aware of it's limitations. For one, my dataset is a slow simulation which must run on all data each iteration. It would be good if I could reliably take a sample of this data without having to worry about twiddle overfitting to a specific ""easy"" subset of the data. Also, it would be good if, after it reaches a tolerance, it quits and restarts to begin looking for a new ""local minimum"". And then, after it has found a few of these, it would be good of it to lower it's tolerance and begin gradient finding even better local minima within those local minima. Any ideas?

Are there any other good machine learning algorithms for parameter optimization that are function agnostic and don't require all the data upfront?",1,0,False,self,,,,,
229,MachineLearning,t5_2r3gv,2016-4-7,2016,4,7,23,4dr998,self.MachineLearning,AskReddit: has anyone implemented ResNets with stochastic depth in Theano? Need advice!,https://www.reddit.com/r/MachineLearning/comments/4dr998/askreddit_has_anyone_implemented_resnets_with/,AlfonzoKaizerKok,1460039103,"I'm referring to the following paper: http://arxiv.org/abs/1603.09382.

I'm thinking of implementing it in Theano, and would really like some pointers. For instance, what would be the best way to block off the output of a layer with some probability? Should I draw a scalar from `rng.binomial` and  and multiply it by output of the layer? ",5,5,False,self,,,,,
230,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,0,4drij1,sharpsightlabs.com,A very quick introduction to machine learning in R with caret,https://www.reddit.com/r/MachineLearning/comments/4drij1/a_very_quick_introduction_to_machine_learning_in/,SharpSightLabs,1460042458,,0,1,False,http://b.thumbs.redditmedia.com/zaXn1U-fhtuaZmdZPiDx8zvhH0fMlZ04sMrP2oOx3dw.jpg,,,,,
231,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,0,4drjfm,cnet.com,Machine learning goes for baroque and paints 'brand new' Rembrandt,https://www.reddit.com/r/MachineLearning/comments/4drjfm/machine_learning_goes_for_baroque_and_paints/,emmatoday,1460042794,,1,0,False,http://b.thumbs.redditmedia.com/4nL_lzI5D2uTjqpCCLkNHHACRlm72GddzaxSX7OoJfs.jpg,,,,,
232,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,0,4drkd0,devblogs.nvidia.com,Fast Multi-GPU Collectives With NCCL,https://www.reddit.com/r/MachineLearning/comments/4drkd0/fast_multigpu_collectives_with_nccl/,harrism,1460043121,,2,11,False,http://a.thumbs.redditmedia.com/m1noQavQZttahteiuqnfUmlXHzA3lazlRyuV2t1yub8.jpg,,,,,
233,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,2,4ds6mu,facebook.com,A mission to build sentience and make it accessible to all.,https://www.reddit.com/r/MachineLearning/comments/4ds6mu/a_mission_to_build_sentience_and_make_it/,rajarsheem,1460050872,,2,0,False,default,,,,,
234,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,3,4dsila,github.com,scikit-feature: python implementation of a ton of feature selection algorithms,https://www.reddit.com/r/MachineLearning/comments/4dsila/scikitfeature_python_implementation_of_a_ton_of/,Botekin,1460054954,,16,73,False,http://b.thumbs.redditmedia.com/ttyWIU9tc_1EiD1lu-eCPMyTaiRLNc_wVY2zPNfG3ZY.jpg,,,,,
235,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,4,4dskya,datasciencecentral.com,15 Deep Learning Libraries,https://www.reddit.com/r/MachineLearning/comments/4dskya/15_deep_learning_libraries/,vincentg64,1460055782,,0,1,False,default,,,,,
236,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,4,4dsoqd,marketintelreports.com,Key Factors Behind the Growth of Hydraulic Gearmotors of North America,https://www.reddit.com/r/MachineLearning/comments/4dsoqd/key_factors_behind_the_growth_of_hydraulic/,tharunvicky,1460057101,,0,1,False,default,,,,,
237,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,4,4dsrmt,marketintelreports.com,Single-acting Pneumatic Cylinders Market - Disruptive Innovations Leads the Global Market,https://www.reddit.com/r/MachineLearning/comments/4dsrmt/singleacting_pneumatic_cylinders_market/,tharunvicky,1460058113,,0,1,False,default,,,,,
238,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,5,4dsxri,self.MachineLearning,AskReddit: Help with a guidance for my graduation thesis,https://www.reddit.com/r/MachineLearning/comments/4dsxri/askreddit_help_with_a_guidance_for_my_graduation/,diohadhasuh,1460060320,"Hello, I'm a computer scientist student, I will finish CS this year so I already started my graduation thesis. I work on a Computer Vision - Robotics lab here on my university and my main field of interest and that I want to pursue as an academic field is machine learning / deep learning, so I thought about mixing robotics with machine learning which is something very common.

My main idea is Outdoor Autonomous Navigation , I want my robot to know what a grass is, what a tree is, what people and cars are so he can avoid it or do the things I will set it to do, my approach to the problem so far and what I already did is:
For every image frame I slice the image into subImages and for each subImage I calculate it's histogram and compare with a huge data base containing tons of histograms of grass/sky/trees (for example) and run a knn/svm to classify the subImage into one of the closest histograms, and if everything goes by the script I will have a full labeled system for the robot, but I'm facing some problems and I'm not a really expert on the field yet so I really wan't some guidance because I don't know what to do, my professor told me this will be kinda hard to do this way and for a graduation thesis, I have implemented a LBP descriptor to classificate some textures like grass and asphalt but I can't use LBP for everything, I don't even know if the LBP will be accurate for grass and asphalt (if my dataset is huge enough), anyways, sorry for the long text, I just don't know what path to seek now, I don't even know if my current approach is a good one or I'm doing something silly.",4,0,False,self,,,,,
239,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,5,4dt1ti,self.MachineLearning,Books/resources to help me decide if artificial intelligence is right for me,https://www.reddit.com/r/MachineLearning/comments/4dt1ti/booksresources_to_help_me_decide_if_artificial/,Celective,1460061751,"I'm currently looking at computer science degrees (in the UK), and have found myself particularly drawn to computer science and artificial intelligence courses, any books you'd recommend to read to help me gauge if this is something I'd like to spend the next few years studying?",6,0,False,self,,,,,
240,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,5,4dt4gt,deeplearningbook.org,"Deep Learning book by Ian Goodfellow, Yoshua Bengio and Aaron Courville; online version now complete",https://www.reddit.com/r/MachineLearning/comments/4dt4gt/deep_learning_book_by_ian_goodfellow_yoshua/,alexeyr,1460062763,,7,108,False,default,,,,,
241,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,6,4dt4nk,marketintelreports.com,An Analytical Insight on Linear Displacement Sensors Market!,https://www.reddit.com/r/MachineLearning/comments/4dt4nk/an_analytical_insight_on_linear_displacement/,tharunvicky,1460062830,,0,1,False,default,,,,,
242,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,6,4dtbks,nbviewer.jupyter.org,"TensorFlow now has an unofficial scan function! Here are some examples, including a vanilla RNN from scratch.",https://www.reddit.com/r/MachineLearning/comments/4dtbks/tensorflow_now_has_an_unofficial_scan_function/,rd11235,1460065332,,16,24,False,default,,,,,
243,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,7,4dthzx,self.MachineLearning,Questions thread #3 - 2016.04.07,https://www.reddit.com/r/MachineLearning/comments/4dthzx/questions_thread_3_20160407/,feedtheaimbot,1460067766,"**Please post your questions here instead of creating a new thread. Helps keep the sub clean :)**

Thread will stay alive until next one so keep posting after the date in the title. 

Thanks to everyone for answering questions in the previous thread!

Previous threads:

* [Simple Questions Thread #2 + Meta - 2016.03.23](https://www.reddit.com/r/MachineLearning/comments/4bp1ck/simple_questions_thread_2_meta_20160323/)

* [Simple Questions Thread #1 - 2016.03.08](https://www.reddit.com/r/MachineLearning/comments/49k54u/simple_questions_thread_20160308/)",292,19,False,self,,,,,
244,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,9,4du34z,randomekek.github.io,Deep learning notes for beginners by a beginner.,https://www.reddit.com/r/MachineLearning/comments/4du34z/deep_learning_notes_for_beginners_by_a_beginner/,windoze,1460076327,,27,233,False,http://b.thumbs.redditmedia.com/L9-MZDxo3kxfRXxgq7Gt_xyfGMGmtwUguzfg1cMg0lU.jpg,,,,,
245,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,9,4du3xm,arxiv.org,[1604.01348] Bayesian Optimization with Exponential Convergence,https://www.reddit.com/r/MachineLearning/comments/4du3xm/160401348_bayesian_optimization_with_exponential/,bbsome,1460076685,,4,15,False,default,,,,,
246,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,10,4du5pq,self.MachineLearning,Feature selection with ReliefF algorithm,https://www.reddit.com/r/MachineLearning/comments/4du5pq/feature_selection_with_relieff_algorithm/,Bohemian90,1460077436,"

I have a dataset consisting of around 10000 data points and 20 features. I'm using nested cross-validation for estimating the performance. Now, I want to do feature selection.

Due to the nested cross-validation I think genectic algorithm or simulated annealing is computationally infeasibly (for SVM, Boosting etc.). So I plan to use ReliefF.

I think I have to do a grid search for the number of nearest neighbours of ReliefF. What values should I search over?

ReliefF gives me a ranking and weights. So my idea is to first exclude the feature with the lowest weight and compute the accuracy. Then I exclude the feature with the second lowest weight etc. until there is no improvement in accuracy.

Is this a correct approach?",12,0,False,self,,,,,
247,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,10,4du7gv,russellsstewart.com,Introduction to debugging neural networks,https://www.reddit.com/r/MachineLearning/comments/4du7gv/introduction_to_debugging_neural_networks/,singularai,1460078208,,2,27,False,default,,,,,
248,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,11,4duh7g,self.MachineLearning,Xesto - Machine Learning and Gestural Recognition,https://www.reddit.com/r/MachineLearning/comments/4duh7g/xesto_machine_learning_and_gestural_recognition/,Xesto,1460082539,"Hey everyone!

My name is Sophie, and I am the co-founder and CEO of Xesto, a small early-stage start-up in Canada developing a device agnostic cloud-based gestural recognition platform (with other hugely exciting things planned), solving precision and latency problems.

We are conducting a closed Beta for our platform, starting April 17. If you are interested in participating, all that is needed is a Leap Motion. Otherwise, Xesto will launch around mid-May, so stay posted!

If gestures and getting involved in cool new technologies early on are up your alley - please send me an email or we can discuss here :) 

Thank you! 
Email: sophie@xesto.io 
Website: http://xesto.io 
A few minutes to talk tech? https://www.surveymonkey.com/r/xestotesting",2,0,False,self,,,,,
249,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,15,4dvba0,blog.shakirm.com,Learning in Brains and Machines: The Dogma of Sparsity,https://www.reddit.com/r/MachineLearning/comments/4dvba0/learning_in_brains_and_machines_the_dogma_of/,iori42,1460098729,,0,38,False,http://b.thumbs.redditmedia.com/OHBMhx5EYNVQLrSRxdo4Q7v8NJ8KN-2tij0gH3_tAJA.jpg,,,,,
250,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,16,4dvc7b,cs.cmu.edu,MXNet GTC Talk,https://www.reddit.com/r/MachineLearning/comments/4dvc7b/mxnet_gtc_talk/,antinucleon,1460099315,,4,12,False,default,,,,,
251,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,16,4dveie,iflscience.com,Six Platoons Of Self-Driving Trucks Just Drove Thousands Of Kilometers Across Europe,https://www.reddit.com/r/MachineLearning/comments/4dveie/six_platoons_of_selfdriving_trucks_just_drove/,swentso,1460100906,,0,2,False,http://b.thumbs.redditmedia.com/yaIXCDsgvAM6XFmMFzvgFeq3MB1eeLuKuZJvHwur1Wo.jpg,,,,,
252,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,16,4dvfa8,accenture.com,Artificial Intelligence coming of age Accenture,https://www.reddit.com/r/MachineLearning/comments/4dvfa8/artificial_intelligence_coming_of_age_accenture/,Martin81,1460101466,,3,0,False,http://b.thumbs.redditmedia.com/FSJ-sCl3rWtdLt5sIEVBupK_LAlaTm5RZVyatK0rFTw.jpg,,,,,
253,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,18,4dvme6,self.MachineLearning,Master's thesis about NILM applied to IT-Security,https://www.reddit.com/r/MachineLearning/comments/4dvme6/masters_thesis_about_nilm_applied_to_itsecurity/,wederer42,1460106732,"Hi!

I am a master's student in business informatics (or information systems) and researching how to apply NILM (non intrusive appliance load monitoring) to the field of IT-security.

My experience in machine learning could be rounded off to 0, so I wanted to get some input on how best to approach this topic.

The basic idea would be for example: Monitor the power consumption of an office and look for permanent increases in power consumption or decreases in idle times. This could point to a malware infection or ongoing attack. Some work has been done on smartphones on this topic, but none on desktop computers.

 

My idea was to look into deep learning via TensorFlow or Theano with Keras/Lasagne.

* Are there any databases for office/enterprise power consumptions?
* Is deep learning the way to go or would you rather start with something else?

 
Thank you in advance!",0,0,False,self,,,,,
254,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,18,4dvo1w,i.imgur.com,Ice cream truck MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4dvo1w/ice_cream_truck_machinelearning/,lmelhawiqaa,1460107941,,0,1,False,default,,,,,
255,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,20,4dw1q6,reddit.com,I tried training a deep neural network to generate TNG scripts. The results were ... interesting.  /r/startrek,https://www.reddit.com/r/MachineLearning/comments/4dw1q6/i_tried_training_a_deep_neural_network_to/,[deleted],1460116718,[deleted],0,1,False,default,,,,,
256,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,22,4dwa6q,self.MachineLearning,"Is it better to have more training data which are close to the decision boundary, or more data which are ""typical"" of their class?",https://www.reddit.com/r/MachineLearning/comments/4dwa6q/is_it_better_to_have_more_training_data_which_are/,whymitchellwhy,1460120666,"Specifically I'm interested in image classification. 

I'm developing a program which uses a multi step classification process, with the idea being that after an initial classification is done, a new set of pixels are chosen by the program to be classed as training data for another iteration. 

I'm trying to figure out which classifiers do better with training data which is closer to the decision boundary, and which do better with training data which is more typical of the class it represents",7,21,False,self,,,,,
257,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,22,4dwej6,pieroit.org,How much does machine learning cost?,https://www.reddit.com/r/MachineLearning/comments/4dwej6/how_much_does_machine_learning_cost/,pieroit,1460122574,,0,0,False,http://b.thumbs.redditmedia.com/Bvlbb78euYLLTSckECdixktLugCNMQ_FUQpqrQfI6Rs.jpg,,,,,
258,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,22,4dwg8f,github.com,Stochastic Depth in Keras,https://www.reddit.com/r/MachineLearning/comments/4dwg8f/stochastic_depth_in_keras/,nhutatsu,1460123273,,1,34,False,http://b.thumbs.redditmedia.com/X19CdD2UBzdNmfLwwuIeTQtgVF7ZiqNu5r9DVOcM1YY.jpg,,,,,
259,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,23,4dwlwm,self.MachineLearning,Mapping Problem Types to Algorithms,https://www.reddit.com/r/MachineLearning/comments/4dwlwm/mapping_problem_types_to_algorithms/,miserlou,1460125503,"In terms of publicly available information on practical machine learning, I generally see two types of posts: the first, explaining the mathematics of a particular algorithm, the second, explaining how to set up and use a specific toolkit.

The one thing that I think is missing is the space in the middle - how do you fit your problem to the right type of algorithm? How do you fine tune it? Maybe I'll write this guide myself, but I was wondering if anybody out there had seen anything like that in terms of the publicly available literature/resources, especially something with real world examples.",2,0,False,self,,,,,
260,MachineLearning,t5_2r3gv,2016-4-8,2016,4,8,23,4dwram,self.MachineLearning,Matlab Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/4dwram/matlab_deep_learning/,[deleted],1460127579,"A collaborating is asking me about using matlab, instead of one of the implementations getting heavy use. Does anything good exist?  Do the matlab implementations use the GPU? How hard should I push for theano/torch instead?
",11,4,False,self,,,,,
261,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,0,4dwrwa,self.MachineLearning,Applications of RNNs and LSTMs on GPS time series?,https://www.reddit.com/r/MachineLearning/comments/4dwrwa/applications_of_rnns_and_lstms_on_gps_time_series/,xristos_forokolomvos,1460127776,Can anyone point me to relevant works?,1,7,False,self,,,,,
262,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,0,4dwtm8,self.MachineLearning,Introduction to Machine Learning with portraits,https://www.reddit.com/r/MachineLearning/comments/4dwtm8/introduction_to_machine_learning_with_portraits/,smgigi,1460128369,"Hello! I am new to this area. But, I think this explanation help you understand machine learning better. I compare Monet and Gogh to help you understand how machine learning works. This article gives you a broader sense of machine learning.

Specific R code is followed. Examples include KNN, Classification Tree, and Naive Bayes.

http://www.mbaprogrammer.com/2016/04/introduction-to-machine-learning-data_7.html",1,0,False,self,,,,,
263,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,0,4dx0qv,blog.exabeam.com,Machine learning is a method that is used to devise complex models and algorithms for the purpose of learning or making predictions from data.,https://www.reddit.com/r/MachineLearning/comments/4dx0qv/machine_learning_is_a_method_that_is_used_to/,Millerr5Bentlee,1460131031,,0,0,False,http://a.thumbs.redditmedia.com/zDscdWRKUaHO6BNTIERBwIbUN4XIzaJX7MNC2STL_O0.jpg,,,,,
264,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,1,4dx1bw,popsnip.com,AI and Machine Learning Interview Resource,https://www.reddit.com/r/MachineLearning/comments/4dx1bw/ai_and_machine_learning_interview_resource/,[deleted],1460131235,[deleted],0,1,False,default,,,,,
265,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,1,4dx81b,deeplearningskysthelimit.blogspot.nl,Will this subsist as the AI Match of the 21st century: Deep Learning AlphaGo vs. Lee Sedol (9p)? What worldwide is regarded as the most outstanding grand challenge for artificial intelligence may have become a closed down chapter with this match.,https://www.reddit.com/r/MachineLearning/comments/4dx81b/will_this_subsist_as_the_ai_match_of_the_21st/,DeepLearningBob,1460133677,,1,0,False,http://b.thumbs.redditmedia.com/fcnF8jLxmdAPvNnEloEvlcjHzJJPU53rUq3-IHGz5Sg.jpg,,,,,
266,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,2,4dxewq,self.MachineLearning,"Can deep learning models ""forget"" their training?",https://www.reddit.com/r/MachineLearning/comments/4dxewq/can_deep_learning_models_forget_their_training/,thecity2,1460136111,"So I have a CNN that was trained (using Caffe and the google inception model from Model Zoo) on a set of images (profile photos that users upload to our app). We have a community of mods that approve or reject those photos based on a set of image moderation rules. The initial model I created is so good that we can now auto-approve over 50% of all images being uploaded (images that have a reject score &lt; 0.01). All images that are not auto-approved go to a human moderator (yay humans!). 

My thought was that I could take the 50% of human moderated images and improve the model by continuing to train it on those images. But I'm noticing that during this ""re-training"" run, the results are quite a bit worse than before, with the model being able to classify less than 50% of images correctly.

Now I'm questioning whether this is the right approach. Does it make sense to continue to train the model with what amounts to essentially a ""censored"" data set (compared to the original)? Would you expect that the model will somehow ""forget"" how to classify the 50% of images that are now being auto-approved (with very low reject scores) or will those parts of the model network remain intact during re-training?

I guess the general question is can you improve a model like this without giving it an entirely new dataset, by simply continuing to iteratively train on new images that weren't part of the original data set.",7,0,False,self,,,,,
267,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,2,4dxgd5,self.MachineLearning,Resources for GPU programming?,https://www.reddit.com/r/MachineLearning/comments/4dxgd5/resources_for_gpu_programming/,[deleted],1460136637,[deleted],21,44,False,default,,,,,
268,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,2,4dxhxn,medium.com,Recent developments in artificial intelligence survey,https://www.reddit.com/r/MachineLearning/comments/4dxhxn/recent_developments_in_artificial_intelligence/,kordikp,1460137198,,0,0,False,http://b.thumbs.redditmedia.com/JprtfRVM3rRKJVoGXAFqoeS4ma1Y10KcE6ecn8f-x0o.jpg,,,,,
269,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,3,4dxr1m,self.MachineLearning,Options for CNN to tag images,https://www.reddit.com/r/MachineLearning/comments/4dxr1m/options_for_cnn_to_tag_images/,[deleted],1460140508,[deleted],4,0,False,default,,,,,
270,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,4,4dxxal,self.MachineLearning,Ideas for Highly Concurrent Problems to Be Solved With Quantum Computing,https://www.reddit.com/r/MachineLearning/comments/4dxxal/ideas_for_highly_concurrent_problems_to_be_solved/,Schoolunch,1460142876,"I have access to a quantum computer for a research project, and one issue with quantum computers is they can only take in about 1000 variables total, so they are extremely limited in scope.  Outside of this though, they are very capable of solving global minimum for these variables at an extremely fast rate.  What kind of problems would be interesting to explore that utilizes this concept?

As a gopher I feel that there are some very interesting machine learning problems that could be solved concurrently, but would love some outside insights into this.",5,0,False,self,,,,,
271,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,4,4dxz8j,self.MachineLearning,How do I learn to build machine learning apps at scale?,https://www.reddit.com/r/MachineLearning/comments/4dxz8j/how_do_i_learn_to_build_machine_learning_apps_at/,jarins,1460143558,"Over the last year, in my spare time, I've built several small machine learning apps. For example, [one](http://fogornot.com) that predicts the San Francisco fog at a neighborhood by neighborhood level.

I have ideas for other things that I want to build, but at a much larger scale, for example large recommendation systems off Twitter's API. I've tried looking for tutorials or courses on how to build scalable machine learning apps, but I feel like I'm drowning in a lot of stuff I don't understand -- distributed architecture, MapReduce, Spark to name a few. Suggestions for the best way for a beginner to scale to get started?",7,3,False,self,,,,,
272,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,5,4dybk9,self.MachineLearning,State-of-the-art Automatic Signature Falisification,https://www.reddit.com/r/MachineLearning/comments/4dybk9/stateoftheart_automatic_signature_falisification/,joekr07,1460148274,"So with all the deep learning tools we have today, what is the current best practice for generating a false signature?

I wonder if a expert could see a difference anymore?

Is there any works on this out there?",0,0,False,self,,,,,
273,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,6,4dyf62,self.MachineLearning,My python solutions to Andrew Ng's Coursera ML course,https://www.reddit.com/r/MachineLearning/comments/4dyf62/my_python_solutions_to_andrew_ngs_coursera_ml/,n3utrino,1460149665,"I'm not sure if this worth posting, but I've just completed all of the homeworks in Andrew Ng's Coursera Machine Learning course (which I _loved_). I did the homeworks in python/numpy/scipy in ipython notebooks so they're easy to view. I thought I would post them here in case someone else is trying to complete them in python and needs some inspiration. I don't think there are any ethical problems with posting them publicly because to actually get credit for the course solutions must be done in Octave or Matlab.

Hope this helps someone out in the future, I spent a lot of time on it!

https://github.com/kaleko/CourseraML",61,193,False,self,,,,,
274,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,6,4dyhd3,self.MachineLearning,Help with a neural network in Matlab,https://www.reddit.com/r/MachineLearning/comments/4dyhd3/help_with_a_neural_network_in_matlab/,Frigorifico,1460150502,"Hello, this is what I am trying to do:


I have five sets of points who represent functions, each is made by multiplying a constant by a vector of 10,000 numbers between 0 and 1, but I don't know what the constants are and I don't know what the vectors where in each case, I suspect they are the same linearly spaced vector because that's what my profesor would do.

I have to use a neural network to find the constants. My idea was to make one neuron with five inputs between 0 and 1 and train it.


It would multiply the numbers by the weights, add them up and see if it is the same as the sum of the five functions I have.


When it ended it's training I would see what the final weights are and those are my constants.

However I am not sure how to use the matlab functions to do this.

I know that I can create a neural network using this:

* net=newfit([0 1], [1,1], {'tansig','purelin','purelin'},'trainlm');

which if I understand is a single neuron in line with another one (because I don't know how to declare just one) who uses numbers between 0 and 1.

And I trained it using a single vector of points between 0 and 1 and the vector of the sum of the other five, and it displayed the correct function using this code:

* net1 = train(net, x, y1); 
* a1= sim(net1,x);% Plot result and compare 
* plot(x,a1,'o',x,y1); grid;

When I looked at the weights however there were only four of them, and it occurred to me that I never specified how many inputs the neuron has, and looking in documentation I see no reference to this, only reference to the number of layers and neurons and how to train them.

Thank you a lot",0,0,False,self,,,,,
275,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,6,4dym6s,self.MachineLearning,Explaining the triumph of AlphaGo to non-go players,https://www.reddit.com/r/MachineLearning/comments/4dym6s/explaining_the_triumph_of_alphago_to_nongo_players/,BradysBlunders,1460152406,"Almost all the videos about AlphaGo are very oriented towards go players. I really wanted to share why the recent Google Deepmind AlphaGo Challenge was so significant and amazing to a broader audience. Although I made the following videos to appeal to both go and AI enthusiasts, I hope they will be approachable and informative to those who know nothing of either. I welcome your feedback. 

[Episode 1](https://youtu.be/7f47A7Pd11k) - A brief introduction to go, its rules, and its history, as well as the history of computer go, and a not too technical introduction to AlphaGo

[Episode 2](https://youtu.be/5FNOouojjOQ) - A low-level discussion of the innovative and amazing moves of AlphaGo, the adaptive play of Lee Sedol, consideration of AlphaGo's true rating, and then some thoughts on Game 4 ""bugs""

[Episode 3](https://youtu.be/x4iJmdoBG_0) - Discussion of the impact of the AlphaGo on the go community, the future of DeepMind and AlphaGo, and final reflection on the Challenge",3,15,False,self,,,,,
276,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,7,4dyq6s,self.MachineLearning,What's your aim as a machine learning researcher/technologist?,https://www.reddit.com/r/MachineLearning/comments/4dyq6s/whats_your_aim_as_a_machine_learning/,[deleted],1460153986,[removed],0,1,False,default,,,,,
277,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,8,4dz1we,self.MachineLearning,Zelda 30th (AI)versary,https://www.reddit.com/r/MachineLearning/comments/4dz1we/zelda_30th_aiversary/,pipkinsoup,1460158998,"I want to help a computer beat Zelda for its 30th anniversary.  However,  I'm still pretty new at this and after reading this thread [https://www.reddit.com/r/MachineLearning/comments/4b3vz8/i_want_to_make_a_neural_network_that_learns_how/] I've decided that might be unrealistic. What might be a better problem to start with? ",1,0,False,self,,,,,
278,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,9,4dz72b,self.MachineLearning,Simulated Annealing vs. Genetic Algorithms for feature selection,https://www.reddit.com/r/MachineLearning/comments/4dz72b/simulated_annealing_vs_genetic_algorithms_for/,[deleted],1460161282,[deleted],1,0,False,default,,,,,
279,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,9,4dz794,self.MachineLearning,what's a basic speech dataset for a deep neural networks (with an example)?,https://www.reddit.com/r/MachineLearning/comments/4dz794/whats_a_basic_speech_dataset_for_a_deep_neural/,Aumanidol,1460161361,"I'd really like to test speech recognition a bit.

do you know of any dataset that has an example of its implementation around? I've never worked with audio data so I really have no idea on how to normalize it and feed it to my network, an example on github would be great, but its search engine returns nothing.

I'm using theano and lasagne, but I just want to learn how to actually extract features from the speech samples.

I found LITIS on the sub, but I heard it takes 16+gb of ram to process and I haven't found any implemented example.
Thank you all.",4,1,False,self,,,,,
280,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,13,4dzxs3,self.MachineLearning,Best way to deal with time series data,https://www.reddit.com/r/MachineLearning/comments/4dzxs3/best_way_to_deal_with_time_series_data/,gmo517,1460174701,"Curious to hear different approaches with time series data?

What type of model(s) do you all like to use when we have multiple features (some time series, some numerical/categorical)?

And also consider that the same time series variable may be of different lengths (one data point has 80 instances for that one feature whereas another data point may have 20 instances for that same feature) What do you do then?",18,4,False,self,,,,,
281,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,15,4e0aya,self.MachineLearning,RNN Online Demos,https://www.reddit.com/r/MachineLearning/comments/4e0aya/rnn_online_demos/,Jxieeducation,1460183123,"Some great demos of RNNs:

[Hand writing generation] (http://www.cs.toronto.edu/~graves/handwriting.html)
[Sticky note writting generation] (http://www.inkposter.com/)

[Image captioning](http://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html)

[Music generation] (https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/)

[Language translation](http://104.131.78.120/)

[Sentence generation] (http://www.cs.toronto.edu/~ilya/fourth.cgi?prefix=The+meaning+of+life+is+&amp;numChars=50)


**Please let me know if there are any impressive demos that I missed!
",2,21,False,self,,,,,
282,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,16,4e0i4h,self.MachineLearning,How do you organize / manage your pdf/paper collection?,https://www.reddit.com/r/MachineLearning/comments/4e0i4h/how_do_you_organize_manage_your_pdfpaper/,hughperkins,1460188630,"I have 832 pdf files now, which I was storing in a normal file system, like directories like 'nlp', 'image', 'memory', etc.  But I dont think this is ideal, because lots of papers cross categories, and trying to shoe-horn them in doesnt work very well, and, in the worst case, is a block to creativity, because means some relevant paper is classified under say 'lstm' instead of 'nlp', and I forgot about it, cos looking in 'nlp' (say).

I'm thinking maybe what I should be using instead is perhaps a tagging system, where I can assign multiple tags to each paper?  Thoughts?  What system(s) do you like to use?  (I'm on Ubuntu by the way)",38,70,False,self,,,,,
283,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,17,4e0jhk,udemy.com,"Everything you need to know about Big Data, and Data Processing and Hadoop",https://www.reddit.com/r/MachineLearning/comments/4e0jhk/everything_you_need_to_know_about_big_data_and/,andalib_ansari,1460189806,,0,1,False,default,,,,,
284,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,18,4e0mms,self.MachineLearning,Something wrong with my AP graphs,https://www.reddit.com/r/MachineLearning/comments/4e0mms/something_wrong_with_my_ap_graphs/,sunshineatnoon,1460192576,"I have this two method and they give me different detection results on images. So I use PASCAL devkit to test their accuracy and got these to AP(Average Precision) graphs:

[method1!](https://raw.githubusercontent.com/sunshineatnoon/Paper-Collection/master/%5B%22images%22%5D/train.jpg)
[method2!](https://raw.githubusercontent.com/sunshineatnoon/Paper-Collection/master/%5B%22images%22%5D/train2.jpg)

But I think there's something wrong with these two graphs. Clearly, the method with higher AP score has lower accuracy and comparable recall with the method with lower AP score. Did I misunderstand something here? Thanks!
",0,1,False,self,,,,,
285,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,19,4e0tv6,self.MachineLearning,Good resources for using LSTM in Sentiment Analysis task.,https://www.reddit.com/r/MachineLearning/comments/4e0tv6/good_resources_for_using_lstm_in_sentiment/,shash273,1460198442,I am trying to perform sentiment analysis task using LSTM. What are some resources (beginner to advanced) which I can refer to? ,8,9,False,self,,,,,
286,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,22,4e16fw,self.MachineLearning,Research or work experience for MS in ml,https://www.reddit.com/r/MachineLearning/comments/4e16fw/research_or_work_experience_for_ms_in_ml/,cvikasreddy,1460207234,"I am a third year UG(4year course) student in mechanical engineering at IIT Kharagpur.

Iam very interested in data science and want to do MS in ml.

I can stay one more year(total 5 years) in the same course to get a dual degree and I can do research projects while here and publish a research paper.

Or I can get a job as a data analyst after 4 years and gain experience.

Which path do you think might help me into a top tier university for MS.

During my fourth year whether or not I go for job, I will take Machine Learning, advanced ml, NLP courses.",2,0,False,self,,,,,
287,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,23,4e1cgq,self.MachineLearning,How to classify time series with different length?,https://www.reddit.com/r/MachineLearning/comments/4e1cgq/how_to_classify_time_series_with_different_length/,valikund,1460210465,"Hi, I am trying to solve a classification problem, where each class instance has a time series as features. The problem is that the time series have a length between 187 and 220 values. What can I do?
",6,0,False,self,,,,,
288,MachineLearning,t5_2r3gv,2016-4-9,2016,4,9,23,4e1grg,medium.com,How blockchain can be used in ML/AI tasks (opinion),https://www.reddit.com/r/MachineLearning/comments/4e1grg/how_blockchain_can_be_used_in_mlai_tasks_opinion/,macx0r,1460212554,,2,0,False,http://a.thumbs.redditmedia.com/hKrr2WUYBnnDEYgQRs2bDuZol4Juom5aMjucRi01cH0.jpg,,,,,
289,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,3,4e2c3q,arxiv.org,[1511.06292] Foveation-based Mechanisms Alleviate Adversarial Examples,https://www.reddit.com/r/MachineLearning/comments/4e2c3q/151106292_foveationbased_mechanisms_alleviate/,eaturbrainz,1460225752,,11,30,False,default,,,,,
290,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,3,4e2get,self.MachineLearning,argmax differentiable?,https://www.reddit.com/r/MachineLearning/comments/4e2get/argmax_differentiable/,yield22,1460227437,"Argmax operation, and also max pooling operation, they are not even continuous function (or are they?), but somehow they are ""differentiable"" in neural networks (e.g. by only backprop via max element)... Can anyone provides some justification and insight for me, how can one get away with the underlying discontinuity of the function? What's the function we really minimize at the end?",19,14,False,self,,,,,
291,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,4,4e2l2p,self.MachineLearning,Best place to get help with neural net code,https://www.reddit.com/r/MachineLearning/comments/4e2l2p/best_place_to_get_help_with_neural_net_code/,[deleted],1460229367,[deleted],2,0,False,default,,,,,
292,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,4,4e2mlr,self.MachineLearning,What are the primary differences in research that a professor does at a university vs. what an industry researcher would do? (Assume the places with the most resources.),https://www.reddit.com/r/MachineLearning/comments/4e2mlr/what_are_the_primary_differences_in_research_that/,imdirtysocks45,1460230003,Could an industry researcher work on more theoretical stuff? Or is that primarily done in universities?,31,30,False,self,,,,,
293,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,4,4e2nvn,udemy.com,Machine Learning for Recommender Systems: A Beginner's Guide - Udemy Course,https://www.reddit.com/r/MachineLearning/comments/4e2nvn/machine_learning_for_recommender_systems_a/,Sroy20,1460230508,,0,1,False,default,,,,,
294,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,6,4e30br,self.MachineLearning,Training a CNN with the same data but different labels,https://www.reddit.com/r/MachineLearning/comments/4e30br/training_a_cnn_with_the_same_data_but_different/,ClayStep,1460235669,"I apologize for the ambiguous title, but it was difficult to compile my question into a sentence. 
I'm currently fine-tuning a CNN using Caffe. I have a large data-set of paintings, and corresponding class labels generated from their medium.
I'm not interested in the output class, but rather the 9216 dimension feature vector generated from the Pool5 layer of the network (I'm using AlexNet).

Now, when I generate the class labels from the meta-data assosiated with the painting I'm using the least frequent term as it tends to be more telling. 

As an example, a painting's medium meta-data may be ""Oil and Chalk on Paper""; currently, the least frequent term would have been applied as the target label, in this case ""Oil"". However; if this painting was again trained this time with ""Chalk"", given enough iterations and samples could the network's parameters be tuned to generate a feature vector somewhere between Chalk and Oil paintings for those which contained both?

Thanks",4,6,False,self,,,,,
295,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,6,4e32ll,self.MachineLearning,Working before applying to masters?,https://www.reddit.com/r/MachineLearning/comments/4e32ll/working_before_applying_to_masters/,minglee23,1460236618,[removed],0,1,False,default,,,,,
296,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,7,4e3cx2,self.MachineLearning,Analyzing large amounts of categorical data?,https://www.reddit.com/r/MachineLearning/comments/4e3cx2/analyzing_large_amounts_of_categorical_data/,coffeecoffeecoffeee,1460241001,"Imagine I have a list of cell phones with no way to connect them to a particular geographic region.  I want to use ""Device"" as an input in a model consisting entirely of categorical inputs.  The only issue is that cell phone has thousands of levels, and it would lead to a uselessly sparse matrix to include all of them.  Most multivariate dimension reduction techniques, like PCA and factor analysis, only work on quantitative inputs.

How would I reduce the number of these factors in a meaningful way?  It seems like most of machine learning is heavily focused on quantitative-only inputs (and not just because scikit-learn can't handle categorical features).  Additionally, are there any good resources or books on mining and modeling massive amounts of categorical data?  Agresti is a great book in general, but it seems like you end up with trivial statistical significance with enough data for the kinds of problems he focuses on.",2,1,False,self,,,,,
297,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,7,4e3dy7,self.MachineLearning,Reading Ian Goodfellow's new deep learning book and can't figure out how to derive a conditional probability. Can someone help?,https://www.reddit.com/r/MachineLearning/comments/4e3dy7/reading_ian_goodfellows_new_deep_learning_book/,Coneylake,1460241463,"I posted [the question on statistics stack exchange](http://stats.stackexchange.com/questions/206442/how-to-derive-this-conditional-distribution-function-for-a-restricted-boltzmann), but it hasn't gotten much attention. I think my confusion is pretty simple, so please take a look. ",9,3,False,self,,,,,
298,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,8,4e3kx2,self.MachineLearning,Need some help: Stacking 2 conv2d layers together,https://www.reddit.com/r/MachineLearning/comments/4e3kx2/need_some_help_stacking_2_conv2d_layers_together/,Jxieeducation,1460244561,"Hey! 

Really appreciate your time reading this! What happens when I try to stack 2 convolutional layers right after another?

People very frequently do Input &gt;&gt; conv2d &gt;&gt; relu &gt;&gt; conv2d &gt;&gt; relu &gt;&gt; max pooling

Convolution makes sense to me when the input is an image / input. What happens when u try to get a conv2d layer from the outputs of a conv2d layer?

Say both layers are 256x256x64. What would be the inputs to the cells on the first filter on the second conv2d? (is the input only the first filter on the first conv2d?

Ty!",5,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,8,4e3nnd,self.MachineLearning,I trained a neural network to play connect four!,https://www.reddit.com/r/MachineLearning/comments/4e3nnd/i_trained_a_neural_network_to_play_connect_four/,efmo,1460245850,[removed],0,1,False,default,,,,,
300,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,9,4e3pgi,amazon.co.uk,"A gentle intro to neural networks - aimed at complete beginners - written in plain English, with loads of diagrams and examples. No expertise or experience assumed. Only school maths (age 11-16). Python is also introduced for beginners. http://makeyourownneuralnetwork.blogspot.co.uk",https://www.reddit.com/r/MachineLearning/comments/4e3pgi/a_gentle_intro_to_neural_networks_aimed_at/,myoneuralnetwork,1460246667,,0,1,False,default,,,,,
301,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,9,4e3qh5,self.MachineLearning,Adding embedding layers to keras siamese net.,https://www.reddit.com/r/MachineLearning/comments/4e3qh5/adding_embedding_layers_to_keras_siamese_net/,[deleted],1460247113,[deleted],0,1,False,default,,,,,
302,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,12,4e4h06,self.MachineLearning,Keypoint Localization Using Cross Entropy,https://www.reddit.com/r/MachineLearning/comments/4e4h06/keypoint_localization_using_cross_entropy/,spidey-fan,1460260406,[removed],0,1,False,default,,,,,
303,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,14,4e4qqh,self.MachineLearning,Using the gamma distribution for REINFORCE,https://www.reddit.com/r/MachineLearning/comments/4e4qqh/using_the_gamma_distribution_for_reinforce/,ShakespearePoop,1460266173,"Hi guys,

I'm working on time-series data, and I'd like my model to only be able to jump forward in time. Furthermore, the events I want it to attend to are usually spaced apart pretty regularly (heartbeats on an ECG). What the RAM model currently does is

* interpret the input to the reinforce module as locations in the image with range [-1,1] such that the center of the image is [0,0] 
* use the input location as the mean, and sample from a normal distribution with that mean and a fixed variance

The changes I'd like to make are:

* interpret the input as the distance to jump forward in time, so the range would be [0,1] and 0 would correspond to the minimum possible distance between two heartbeats, and 1 would be the maximum
* use the gamma distribution instead, as it fits more nicely with need to always jump forward in time.

What I need help with is understanding how to go about this mathematically. How do I compute the gradient of a probability distribution? It doesn't make sense to me on a conceptual level. Do I just take the derivative of the PDF function? I tried to follow the logic of [this code](http://torch.ch/blog/2015/09/21/rmva.html), but the calculation of the derivative isn't explained there (the derivative is just stated), and what's more confusing is that I can't find the derivative in that post stated on the wikipedia page for the normal distribution.",2,1,False,self,,,,,
304,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,16,4e50kh,self.MachineLearning,Q. Disadvantages of distributed representations,https://www.reddit.com/r/MachineLearning/comments/4e50kh/q_disadvantages_of_distributed_representations/,0entr0py,1460273476,"The advantages generally cited are distributed representations allow exponentially more data to be represented and allow sharing of common causal factors (if the representations are untangled).  

But are there any disadvantages of this way of representing data? One issue I have come across is with large knowledge banks as in QA tasks. Is there a more general outline of where distributed representations fail ?",6,9,False,self,,,,,
305,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,17,4e55k8,self.MachineLearning,Alternatives to LSTM/RNN ?,https://www.reddit.com/r/MachineLearning/comments/4e55k8/alternatives_to_lstmrnn/,illiterate_gorillas,1460277780,"Hi,
I working on time series data.
And I failed in building a LSTM [model](https://github.com/tensorflow/tensorflow/issues/1821) for this data.(NAN loss and 0 accuracy always).
I would like to know what other methods are available to make use of this feature with time data. 
Such that information from this time based/ Features at different time steps would be effectively utilized.",17,3,False,self,,,,,
306,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,18,4e57ab,self.MachineLearning,Question about experience replay in deep q learning,https://www.reddit.com/r/MachineLearning/comments/4e57ab/question_about_experience_replay_in_deep_q/,ming0308,1460279282,"I am not sure did I understand it correctly.
In each state, we update the score of chosen action to be the best Q-Value of the next state and keep the score of other action to be unchanged. Moreover, we put &lt;state, updated scores of all actions&gt;
into memory. We sample N pairs in the memory (needed to be in the same game??) and train them altogether. So we only calculate the new score of the transition that we just take and reuse the calculated scores of previous transition stored in memory?

If I am correct, I doubt why it will be convergent. Let's say the game is won by the AI, and got a reward 1 in the terminal state. But it never propagated to the previous transition as previous scores are stored in the memory.  Should I update the scores in relay memory as well?

Thanks",5,2,False,self,,,,,
307,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,18,4e5aay,self.MachineLearning,Oxford's VGG page down (Hosts VGG and VGGFace networks),https://www.reddit.com/r/MachineLearning/comments/4e5aay/oxfords_vgg_page_down_hosts_vgg_and_vggface/,gitarg,1460281815,"Hi. This might be a long shot, but I'd like to see if there is anyone who has contact with the Visual Geometry Group (VGG) at Oxford here. I'm in a computer vision class, at a Norwegian university, and I'd like to use the VGGFace (pretrained) network for a project, but the hosting page is down.

Does anybody have a local copy of it which could be made available? Or does anyone know if VGG's webpages have switched addresses?

Their original site (which at the moment is down): http://www.robots.ox.ac.uk/~vgg/

Thanks.",3,0,False,self,,,,,
308,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,20,4e5g6z,self.MachineLearning,Implementing Custom Convolution Kernels?,https://www.reddit.com/r/MachineLearning/comments/4e5g6z/implementing_custom_convolution_kernels/,ajmooch,1460286731,"I've been reading a bunch recently on Nervana/Neon conv implementations of the Winograd kernels (i.e. http://arxiv.org/abs/1509.09308), and I'm interested in trying something similar myself, for either Theano or Tensorflow. I've got an application where having a very specific, highly optimized kernel would be useful, and I'm interested in getting my hands dirty with some lower-level stuff.

Assuming I wanted to do this for a kernel that doesn't already exist (let's say a 3x7 kernel, as an arbitrary example) and had already derived the ""optimal"" ops, where would I start? Do I need to be writing CUDA code, would I be better off hooking into some preexisting library with nice Python wrappers (pycuda?), and just specifying the exact ops I want to do, or do I need to be writing Assembly to directly program my GPU of choice? Or, am I completely off the mark, and thinking along the wrong lines entirely? 

A quick pointer in the write direction would be helpful--I'm aware that even among the things I've suggested, there will be differences in performance and behavior, and also aware that this is not a trivial thing to implement.

Thanks!",17,13,False,self,,,,,
309,MachineLearning,t5_2r3gv,2016-4-10,2016,4,10,23,4e5wj4,self.MachineLearning,Dipping toes in again!,https://www.reddit.com/r/MachineLearning/comments/4e5wj4/dipping_toes_in_again/,python_J,1460297255,"Looking for some advice:
Regarding predicting star ratings for products/ratings etc.
Is it more appropriate to use regression?even though its generally categorical?thanks!",2,0,False,self,,,,,
310,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,0,4e6a1j,self.MachineLearning,Elon Musk vs. George Hotz,https://www.reddit.com/r/MachineLearning/comments/4e6a1j/elon_musk_vs_george_hotz/,SYNA3STH3T1K,1460303658,"This link has been posted before on another subreddit but I wanted to get r/MachineLearning's take on it. 

http://www.theverge.com/2015/12/17/10374422/elon-musk-george-hotz-self-driving-car-tech-criticism

My question is, how is George Hotz doing this? I've always been under the impression that it requires massive google-sized computers to be able to build a CNN (assuming that's what it is) capable of learning that quickly. 
And is Elon Musk right? Does that 0.99% really matter? Humans aren't even at 99% 
Regardless of what Musk says, through his actions he does seem threatened a bit.  ",18,0,False,self,,,,,
311,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,0,4e6aex,subhrajitroy.com,Free ebook on Machine Learning for Recommender Systems,https://www.reddit.com/r/MachineLearning/comments/4e6aex/free_ebook_on_machine_learning_for_recommender/,Sroy20,1460303832,,10,75,False,http://b.thumbs.redditmedia.com/hx8kClDuiu1hjnINJJlFn7P5OU_QxU-pnwnYmGvVv1E.jpg,,,,,
312,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,1,4e6ewj,self.MachineLearning,Reinforcement learning programming implementations,https://www.reddit.com/r/MachineLearning/comments/4e6ewj/reinforcement_learning_programming_implementations/,simontemplar_,1460305630,"I guess this is half ML and half programming, but I was looking at [this Pybrain tutorial](http://pybrain.org/docs/tutorial/reinforcement-learning.html):

It's a cool opener on the concepts, but leaves the actual implementations very hazy. For instance, I would love to understand how to create my own environment (or task, for that matter). Instead, this tutorial just throws ready-made stuff at you which, I reckon, isn't very helpful in actually understanding.

If there exists good explanations *involving programming* I would be very keen in looking into them. Thanks folks!
",11,5,False,self,,,,,
313,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,3,4e6tka,self.MachineLearning,What's the easiest way for a non-programmer mathematician/neuroscientist to implement deep learning?,https://www.reddit.com/r/MachineLearning/comments/4e6tka/whats_the_easiest_way_for_a_nonprogrammer/,jpsky,1460311478,"I have a decent conceptual understanding of deep learning, but I'm not much of a programmer (read: I know enough code to create graphs but not much else). I'd like to start *doing* deep learning rather than just reading/thinking about it. What's the quickest way for me to get started with deep learning?",45,10,False,self,,,,,
314,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,3,4e6v3p,self.MachineLearning,RNN Question from the deep learning book,https://www.reddit.com/r/MachineLearning/comments/4e6v3p/rnn_question_from_the_deep_learning_book/,Jxieeducation,1460312083,"RNN Question,

Can someone explain why training [this network](http://s9.postimg.org/4iky2cnpr/13007193_1179801832054400_8088994446460993211_n.jpg) doesn't require backpropagation through time?

Isn't h(t) still dependent on h(t-1)",6,0,False,self,,,,,
315,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,3,4e719e,self.MachineLearning,SMOTE for high dimensional inputs?,https://www.reddit.com/r/MachineLearning/comments/4e719e/smote_for_high_dimensional_inputs/,nharada,1460314433,Is anyone aware of synthetic oversampling techniques that can be applied to high dimensional data (~15K features)? SMOTE works great in lower dimensions but since KNN scales with feature size it's too slow for my application.,2,7,False,self,,,,,
316,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,4,4e72q9,arxiv.org,Information Theoretic-Learning Auto-Encoder,https://www.reddit.com/r/MachineLearning/comments/4e72q9/information_theoreticlearning_autoencoder/,muktabh,1460314978,,7,19,False,default,,,,,
317,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,4,4e79sp,aioptify.com,The Ultimate List of Machine Learning and Data Mining Books,https://www.reddit.com/r/MachineLearning/comments/4e79sp/the_ultimate_list_of_machine_learning_and_data/,kjahan,1460317761,,0,0,False,http://b.thumbs.redditmedia.com/eXuYk3dT6uUc8DoFOeLbf1CLf1SeYBlUfofAHwJQSoc.jpg,,,,,
318,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,6,4e7s4a,self.MachineLearning,What Math is required to learn Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/4e7s4a/what_math_is_required_to_learn_machine_learning/,RedWine32,1460324998,"I am a High School student and I am currently taking Algebra 2 which is fairly easy. However after ""trying"" to take Andrew Ng Machine Learning coursera course, I did not understand anything, especially cost function. 

A lot of foreign notations were used such as theta,summation, etc, so I realized if I am going to benefit from this course or any course at all,  I really need to invest my abundant free time into to learning Math, which I am 100% dedicated to do. If someone could outline which Math topics are the essential for Machine Learning that would be awesome. 


Thanks!",26,28,False,self,,,,,
319,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,6,4e7sm4,self.MachineLearning,what is the reason to marginalize over latent variables,https://www.reddit.com/r/MachineLearning/comments/4e7sm4/what_is_the_reason_to_marginalize_over_latent/,John_Smith111,1460325223,"hello all

What is the reason to marginalize over latent  variables - what is the result of that ? 

Please provide some intuition about the intent and results of marginalizing over latent variables ?",14,0,False,self,,,,,
320,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,7,4e7uam,self.MachineLearning,Machine learning competition submission tactics. When to submit your best solution?,https://www.reddit.com/r/MachineLearning/comments/4e7uam/machine_learning_competition_submission_tactics/,lo1201,1460325927,"Maybe it's a wrong sub for this question, but I hope someone with experience in ML/programming competitions will have something to say.

So, if you are in a hypothetical competition with a leaderboard, how to best behave yourself?

For example, leaderboard is stale and there is no good results, but it's two month till the end of the competition(it's already been going for two months). And if you happen to find what you think is a good solution, when do you submit it? Right away or wait till the end?

Or if leaderboard is full of good results, you are at the third place and there is little difference between you and second/first, and you think that you've improved your solution considerably and sure that it will beat those in front of you, but it's a (a)month (b)week till the end, do you submit right away or do you wait?",5,5,False,self,,,,,
321,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,7,4e7uhv,self.MachineLearning,"What's a good process to predict the probability of an event, independently of the characteristics of it?",https://www.reddit.com/r/MachineLearning/comments/4e7uhv/whats_a_good_process_to_predict_the_probability/,elemur,1460326005,"For example, how can you predict how likely something is to happen as opposed to what the characteristics of values of the event that may occur? For example how likely a sale might be to occur, as compared to how much a sale might be?",2,0,False,self,,,,,
322,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,7,4e7vn5,self.MachineLearning,tight bound,https://www.reddit.com/r/MachineLearning/comments/4e7vn5/tight_bound/,John_Smith111,1460326481,"Hello all 

i saw quaora post 

https://www.quora.com/Why-do-expectation-maximization-Why-distinguish-between-latent-variables-and-model-parameters

i read that text: 

"" thus maximizing the complete-data likelihood with respect to  z  and    can serve as a proxy to improve your model, even though the likelihood is not guaranteed to increase since the bound isn't tight ""

what is tight bound in the context  of likelihood functions  ?

10x all ",13,0,False,self,,,,,
323,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,7,4e81ne,self.MachineLearning,Hadoop or AWS more useful for Machine Learning careers?,https://www.reddit.com/r/MachineLearning/comments/4e81ne/hadoop_or_aws_more_useful_for_machine_learning/,[deleted],1460329051,[deleted],6,2,False,default,,,,,
324,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,10,4e8iii,self.MachineLearning,I'm young but successful US equities trader. I also have a fairly good understanding of machine learning and have done a lot of work with text classification. What is the best strategy to find a cofounder w excellent machine learning skills for my stock market info startup?,https://www.reddit.com/r/MachineLearning/comments/4e8iii/im_young_but_successful_us_equities_trader_i_also/,[deleted],1460336465,[removed],0,1,False,default,,,,,
325,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,10,4e8k9w,self.MachineLearning,I'm young but successful US equities trader. I also have a fairly good understanding of machine learning and have done a lot of work with text classification. What is the best strategy to find a cofounder w excellent machine learning skills for my stock market info startup?,https://www.reddit.com/r/MachineLearning/comments/4e8k9w/im_young_but_successful_us_equities_trader_i_also/,[deleted],1460337289,[removed],0,1,False,default,,,,,
326,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,11,4e8uee,jvns.ca,Looking inside machine learning black boxes,https://www.reddit.com/r/MachineLearning/comments/4e8uee/looking_inside_machine_learning_black_boxes/,pierrelux,1460341908,,2,20,False,default,,,,,
327,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,12,4e93s1,self.MachineLearning,How do RNNs sort a sequence?,https://www.reddit.com/r/MachineLearning/comments/4e93s1/how_do_rnns_sort_a_sequence/,yield22,1460346337,"Regular RNNs (e.g. LSTMs without attention) can sort short sequence quite well, but how exactly can they do it? Do they count all symbols and output them sequentially or? Seems really hard to understand that.",8,27,False,self,,,,,
328,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,13,4e969r,self.MachineLearning,Help with MNIST SVM Assignment #NOOB,https://www.reddit.com/r/MachineLearning/comments/4e969r/help_with_mnist_svm_assignment_noob/,KingLancer,1460347646,"Hey everyone,
I have an assignment to do dealing with the MNIST dataset and creating and SVM with a non-linear kernel in MatLab, and I'm so lost. I have to:

1)Make an SVM classifier with nonlinear kernel without using an SVM library. Then using that implementation of the SVM classifier, compare one versus the rest and one versus one.

2)Next I have to use the same one versus one classifiers from the previous problem in a DAGSVM approach. 

I have no idea what's going on with this assignment! And I have no idea where to look for help. I tried looking on github but most of that is written in C or uses SVM libraries. Does anyone on here have any information that could help me learn what to do. I'd super appreciate it. ",4,0,False,self,,,,,
329,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,13,4e9a8t,gist.github.com,Notes for Distributed GraphLab paper.,https://www.reddit.com/r/MachineLearning/comments/4e9a8t/notes_for_distributed_graphlab_paper/,shagunsodhani,1460349850,,0,2,False,http://b.thumbs.redditmedia.com/lvJHflIAg4deAzTfFTD0okJa1RZ1OQ5VAGKhaVWo8OQ.jpg,,,,,
330,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,14,4e9c9x,self.MachineLearning,Is this a good lab to research for (I'm an undergrad) if my interests are in psychology?,https://www.reddit.com/r/MachineLearning/comments/4e9c9x/is_this_a_good_lab_to_research_for_im_an/,imdirtysocks45,1460350998,"I want to do ML for the pay lololol, but for real, I like psychology and would like to get into this ""cognitive computing"" world. I want to stay THE FUCK out of academia though. Getting an ML will open doors for me I'm sure, but I want to specifically work on something that improves AI in terms of not just learning but deeper things like personality or motivation. Here's the lab http://dilab.gatech.edu. If I want to continue on this path for my career, will I need a PhD. or could I just get a Master's in CS with an ML concentration while doing research about ""cognitive computing?"" I really wish getting a PhD was like med school where you were *guaranteed* 300k+ afterwards :( ",5,0,False,self,,,,,
331,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,16,4e9s5d,self.MachineLearning,How can I repeat the experiments DeepMind did with beating Atari games?,https://www.reddit.com/r/MachineLearning/comments/4e9s5d/how_can_i_repeat_the_experiments_deepmind_did/,dsocma,1460361455,"I'd like to do a project, as motivation to learn this stuff deep learning, reinforcement learning ect where I repeat DeepMind's Atari experiments but I'll use different games.

&gt;If you aren't familiar with DeepMind's Atari game experiments: https://www.youtube.com/watch?v=V1eYniJ0Rnk

I have some questions:

1. How possible would it be for someone with some basic coding skills and good understanding of machine learning, to repeat DeepMind's game experiment for a game like this: [Geometry Wars II: Pacifism](https://www.youtube.com/watch?v=rhWfAOaJnFg&amp;nohtml5=False). &lt; this is the world record score, so it might be an example of the style of play that the system will have to discover.

2. How should one go about doing this?  I'm assuming I would need supercomputer access?  OR if doing it with that particular game is not possible, how would I go about doing it with a game already done by DeepMind?",15,7,False,self,,,,,
332,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,17,4e9upo,self.MachineLearning,Building a spam filtering system using python,https://www.reddit.com/r/MachineLearning/comments/4e9upo/building_a_spam_filtering_system_using_python/,madboy1995,1460363338,"**EDIT**

I made a pip package for the spamfiltering system which can be used to classify your emails after training

    $ pip install nltk==3.2.1, beautifulsoup4==4.4.1
    $ pip install spammy

Github link: [https://github.com/prodicus/spammy](https://github.com/prodicus/spammy)

***

Hey everyone, 

I tried my hand at making a custom **spam filtering system**,  where you can train the classifier against the dataset and then cross validate the accuracy. You can even use my classifier to classify your emails into ham or spam. [Refer the API](https://github.com/prodicus/spamfilter#api-usage) for the how to

**Features of my classifier**

- **You can used the trained classifier in pickled form so that you don't have to train it on test dataset**
- classifier Trained against close to 33K emails from enron dataset (www.cs.cmu.edu/~enron/)
- You can even train it on your own dataset.
- **Accuracy**: 

I ran it one too many times apparantly and the accuracy is generally between 

|          | Accuracy  |
|:--------:|:---:|
| **Spam** | _80 to 94%_|
| **Ham**  | _70 to 80%_ |

This is my first machine learning project (hopefully not the last!), your reviews and suggestions would be more than welcome!

***

**Links**

- github page: [https://github.com/prodicus/spamfilter](https://github.com/prodicus/spamfilter)
- Pickled classifiers: [https://github.com/prodicus/spamfilter/releases/tag/v0.0.3](https://github.com/prodicus/spamfilter/releases/tag/v0.0.3)
- API usage: [https://github.com/prodicus/spamfilter#api-usage](https://github.com/prodicus/spamfilter#api-usage)
- Project wiki: [https://github.com/prodicus/spamfilter/wiki](https://github.com/prodicus/spamfilter/wiki)",1,1,False,self,,,,,
333,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,18,4ea0m7,self.MachineLearning,Audio normalization/preprocessing before classification,https://www.reddit.com/r/MachineLearning/comments/4ea0m7/audio_normalizationpreprocessing_before/,JustARandomNoob165,1460367412,"Hello!

I have a labelled collection of audio files, that were recorded in totally different conditions. For instance, the loudness can vary a lot(in one file the signal can be very loud, in the other it might be much less loud, but still clearly distinctive). How is it possible to normalize such collection of audio samples, before doing feature extraction(like MFCC) and later classification to achieve robustness?",4,2,False,self,,,,,
334,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,20,4eaa61,factorie.cs.umass.edu,Toolkit for deployable probabilistic modeling: Factorie 1.2 released,https://www.reddit.com/r/MachineLearning/comments/4eaa61/toolkit_for_deployable_probabilistic_modeling/,dafcok,1460373715,,1,10,False,default,,,,,
335,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,20,4ead6r,self.MachineLearning,[Q] Machine Learning &amp; CUDA programming,https://www.reddit.com/r/MachineLearning/comments/4ead6r/q_machine_learning_cuda_programming/,Kiuhnm,1460375555,Do you think it's important for a researcher in Machine Learning and Deep Learning to know how to program GPUs by using CUDA or pyCUDA directly?,6,1,False,self,,,,,
336,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,21,4eaja7,blackboxchallenge.com,BlackBox machine learning competition,https://www.reddit.com/r/MachineLearning/comments/4eaja7/blackbox_machine_learning_competition/,alexperrier,1460378775,,5,1,False,default,,,,,
337,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,23,4eavtq,jisungk.github.io,"I built deepjazz, a prototype for deep learning driven jazz generation (SoundCloud, GitHub inside)",https://www.reddit.com/r/MachineLearning/comments/4eavtq/i_built_deepjazz_a_prototype_for_deep_learning/,turingexam,1460384425,,31,152,False,http://b.thumbs.redditmedia.com/ETx3jkSgdN3xfruoqojDE7XCTUVE9UY8ys1WwHBkSho.jpg,,,,,
338,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,23,4eaxsp,self.MachineLearning,[Question] LSTM Classification with Small Training Set,https://www.reddit.com/r/MachineLearning/comments/4eaxsp/question_lstm_classification_with_small_training/,brunoalano,1460385200,"Hello, I have a problem. I'm trying to use LSTM using Semantic Vectors (word2vec) as Input, so, for each word, I've a 100 dimensional vector that represents that word. I want to use that because it improves the accuracy, providing a inner ""disambiguation"" module, since LSTM receives Sequences as input.

The problem is that I've a small training set, with about 200 classified sentences.

What should I do, invest more time to create a bigger training set (how much sentences I need more or less?) or to use a simpler algorithm, like Logistic Regression or some Naive Bayes classifier, using only TF-IDF and forgoting about the Semantic Vectors?

Thanks",6,7,False,self,,,,,
339,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,23,4eaz4s,quora.com,David Barber Answering Questions on Quora Tomorrow,https://www.reddit.com/r/MachineLearning/comments/4eaz4s/david_barber_answering_questions_on_quora_tomorrow/,tangerinemike,1460385693,,0,9,False,default,,,,,
340,MachineLearning,t5_2r3gv,2016-4-11,2016,4,11,23,4eb0r2,blog.alluviate.com,Microglia: A Biological Plausible Basis for Back-propagation,https://www.reddit.com/r/MachineLearning/comments/4eb0r2/microglia_a_biological_plausible_basis_for/,ceperez,1460386333,,1,1,False,default,,,,,
341,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,0,4eb2et,youtube.com,"2.16.17 Telsa model S P85D ""Fear the reaper"" mario Cart mode",https://www.reddit.com/r/MachineLearning/comments/4eb2et/21617_telsa_model_s_p85d_fear_the_reaper_mario/,spaceballjedi,1460386976,,6,0,False,default,,,,,
342,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,0,4eb5uq,self.MachineLearning,I'm a young but successful US equities trader. I also have a fairly good understanding of machine learning and have done a lot of work with text classification. What is the best strategy to find a cofounder w excellent machine learning skills for my stock market info startup?,https://www.reddit.com/r/MachineLearning/comments/4eb5uq/im_a_young_but_successful_us_equities_trader_i/,mlCofounder,1460388253,,4,0,False,self,,,,,
343,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,0,4ebbqh,self.MachineLearning,With the amount of data that Google and the NSA collect. How do you think they are using machine learning to go through the data? How will they use this data?,https://www.reddit.com/r/MachineLearning/comments/4ebbqh/with_the_amount_of_data_that_google_and_the_nsa/,[deleted],1460390395,[deleted],0,0,False,default,,,,,
344,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,1,4ebh0f,self.MachineLearning,[Question] Neural networks for regression,https://www.reddit.com/r/MachineLearning/comments/4ebh0f/question_neural_networks_for_regression/,centau1,1460392241,"what are some good references about using modern neural networks for regression problems? I know they are extensively used in classification, but not sure how to use them for regression.

Thanks",16,19,False,self,,,,,
345,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,2,4ebrqu,argmin.net,Ben Recht starts a blog,https://www.reddit.com/r/MachineLearning/comments/4ebrqu/ben_recht_starts_a_blog/,elexhobby,1460396047,,13,17,False,default,,,,,
346,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,3,4ebxed,self.MachineLearning,Question for Soft targets (Dark knowledge written by Hinton).,https://www.reddit.com/r/MachineLearning/comments/4ebxed/question_for_soft_targets_dark_knowledge_written/,machine-learning,1460398019,"HI all, I am now trying to train my network using soft targets as hinton's paper (Dark knowledge). However I just wonder that, for hard targets, the softmax layer with cross-entropy is enough for get the error using t_j - y_j, where t_j means the j-th hard target and y_j means the j-th ouput class of the network. Therefore, the derivative of the softmax layer will be just 1. However when I want to use the soft targets for training, I think the softmax layer derivative has to be also changed... Is it right? Please help me... I almost spend one week with this problem.. ",1,1,False,self,,,,,
347,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,3,4ec4fm,blog.keras.io,"Keras 1.0 - the major update of Deep Learning library just released. New features include functional API, arbitrary metrics for arbitrary endpoints, etc.",https://www.reddit.com/r/MachineLearning/comments/4ec4fm/keras_10_the_major_update_of_deep_learning/,lukovkin,1460400457,,21,169,False,default,,,,,
348,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,5,4ecmob,self.MachineLearning,How does AlphaGo's victory over Lee Sedol affect concepts outside of AI?,https://www.reddit.com/r/MachineLearning/comments/4ecmob/how_does_alphagos_victory_over_lee_sedol_affect/,[deleted],1460406754,[deleted],3,0,False,default,,,,,
349,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,8,4edbn0,self.MachineLearning,[Question] Machine Learning Job without Formal Education,https://www.reddit.com/r/MachineLearning/comments/4edbn0/question_machine_learning_job_without_formal/,machinelearningthrow,1460416089,[removed],0,1,False,default,,,,,
350,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,8,4edcw1,self.MachineLearning,[Question] Do I need to manually annotate more of my data so that I can train a system to automatically annotate it?,https://www.reddit.com/r/MachineLearning/comments/4edcw1/question_do_i_need_to_manually_annotate_more_of/,egshef,1460416585,"Hey /r/MachineLearning sorry if this seems like a dumb question but I've run into a bit of trouble with a task I'm working on and advice would be appreciated. (I also will post something over on /r/LanguageTechnology since my question is NLP and ML related if nobody here can really help me). 

For all of the machine learning tasks I've had in the past my data has been fully classified (usually with a numeric value or True/False associated with a string of text). I'm a total noob to machine learning so all I've done is really cleaned up data, put it into arff files, and run it through Weka (I work primarily in Java and Python).

The project I'm working on now is a corpus 50,000 words of text divided into strings of sentences. The basis of my corpus is 4 .txt files of speech from commentators of the video game CS:GO. I used the UAM Corpus to manually annotate events (single words that are ""game-changing"" events in the game) for one of the 4 .txt files, which then turns the .txt file into an XML file. I've never done anything with XML before, nor have I have done any machine learning work where only a portion of the data is annotated. I want to do feature extraction and POS tagging (also available with the UAM Corpus tool as a level of annotation) to see if a system could (I guess) extract events from a text.

My question is what would be the best way to go about training a system with a limited amount of annotated data either using NLTK or Weka, and how would I format the data to do so. I do plan on manually annotating everything at some point but as of now I can't since time is an issue. Sorry if I didn't really explain this that well and thanks in advance!",2,3,False,self,,,,,
351,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,8,4edh2n,self.MachineLearning,Upsampling with Missing Values,https://www.reddit.com/r/MachineLearning/comments/4edh2n/upsampling_with_missing_values/,Bohemian90,1460418301,"Hello

I want to apply ADASYN (Matlab [link](http://www.mathworks.com/matlabcentral/fileexchange/50541-adasyn--improves-class-balance--extension-of-smote-)) which is an extension of the Synthetic Minority Over-Sampling Technique.

First, does this also work with missing data or do I have to impute the missing data first?

Second, if I have to do imputation of missing values anyway (because some classifiers require it), is it better to do the imputation before or after upsampling?",6,3,False,self,,,,,
352,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,8,4edin5,self.MachineLearning,Which shade of BLEU are you using for MT?,https://www.reddit.com/r/MachineLearning/comments/4edin5/which_shade_of_bleu_are_you_using_for_mt/,alvas_,1460418919,[removed],0,1,False,default,,,,,
353,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,9,4edjyu,self.MachineLearning,When is Nvidia's New Titan Coming Out?,https://www.reddit.com/r/MachineLearning/comments/4edjyu/when_is_nvidias_new_titan_coming_out/,LeavesBreathe,1460419432,"Hey Guys,

Just wondering when Nvidia's new pascal Titan GPU is coming out. I've read alot online and I've seen 16gb ram rumors and others. 

The 1080 seems to be coming out in June-ish, but any word on the new Titan?

Also, some people are saying that they are going to purposely cripple fp32? Really hope this is not the case! Thanks",14,2,False,self,,,,,
354,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,11,4ee2sy,self.MachineLearning,Cross validation with different x-values,https://www.reddit.com/r/MachineLearning/comments/4ee2sy/cross_validation_with_different_xvalues/,wdlearn,1460427298,[removed],0,1,False,default,,,,,
355,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,11,4ee6vm,self.MachineLearning,[Question] help in Ridge regression,https://www.reddit.com/r/MachineLearning/comments/4ee6vm/question_help_in_ridge_regression/,Vainsingr,1460429011,"I am learning to implement ridge regression manually without using the sklearn package.

To find theta I am using this equation -

 = (X^T. X + I)^1.X^T .y

this is used to fit a model.

Now for prediction . what equation should I use.??
Getting very confused in these.

Thanks",6,2,False,self,,,,,
356,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,12,4eeetj,self.MachineLearning,Bayesian Belief Networks,https://www.reddit.com/r/MachineLearning/comments/4eeetj/bayesian_belief_networks/,Schoolunch,1460432591,Is there a reason these are not employed more often in Machine Learning problems and more often in scientific computing?,2,7,False,self,,,,,
357,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,14,4eeq9v,thenextweb.com,"Data science sexiness: Your guide to Python and R, and which one is best",https://www.reddit.com/r/MachineLearning/comments/4eeq9v/data_science_sexiness_your_guide_to_python_and_r/,john_philip,1460438545,,0,0,False,default,,,,,
358,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,14,4ees9n,fastcodesign.com,"Neural Networks: What Are They, And Why Is The Tech Industry Obsessed With Them?",https://www.reddit.com/r/MachineLearning/comments/4ees9n/neural_networks_what_are_they_and_why_is_the_tech/,john_philip,1460439707,,0,0,False,default,,,,,
359,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,17,4ef9vq,github.com,deepy: A highly extensible deep learning framework based on Theano,https://www.reddit.com/r/MachineLearning/comments/4ef9vq/deepy_a_highly_extensible_deep_learning_framework/,galapag0,1460451572,,1,4,False,http://b.thumbs.redditmedia.com/LHcEzIgXX1ua2BTqCZsQKJbvDSbASI3tkWrL7cSsllg.jpg,,,,,
360,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,19,4efi2k,self.MachineLearning,"Extracting keywords from a paragraph- how should I approach this problem (NLP, python)?",https://www.reddit.com/r/MachineLearning/comments/4efi2k/extracting_keywords_from_a_paragraph_how_should_i/,hyped22,1460457183,"I am working on e-learning software where end users are asked questions that require free text responses. Each question also has an answer guide (not visible to users responding to the question and generally a max of 2-3 paragraphs). A simplified example of a question could be: ""What are the 3 pillars of our company?"" and the answer guide could be: ""The 3 pillars that our company has are: do no evil, be happy, respect your coworkers"". An example of an answer left by an end user could be: ""The pillars are happiness and respecting coworkers"".

I want to facilitate the feedback given to the end user on their free text responses. In the example above we could automatically give the following feedback: ""You failed to mention anything about 'do no evil'"". One approach would be to simply extract keywords from the answer guide: 'no evil', 'happy', 'respect coworkers' and then checking whether end users' answers contain these keywords.

I am new to NLP and my experience with machine learning has been simply with linear/logistic regression and classification. Can someone just point me in the right direction for me to get started with?",2,3,False,self,,,,,
361,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,21,4efw2p,self.MachineLearning,Neural Network for Reverberation Decay Rate Prediction (Keras),https://www.reddit.com/r/MachineLearning/comments/4efw2p/neural_network_for_reverberation_decay_rate/,shpongledspores,1460464777,"Hi, I asked a question yesterday on Cross-Validated (Stackexchange) but have had no response, and just wondered if anyone here might have any ideas?

http://stats.stackexchange.com/questions/206694/neural-network-for-reverberation-decay-rate-prediction-keras",3,1,False,self,,,,,
362,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,21,4efwz0,dezyre.com,Top 10 Machine Learning Algorithms,https://www.reddit.com/r/MachineLearning/comments/4efwz0/top_10_machine_learning_algorithms/,margarettecrystal,1460465157,,0,1,False,default,,,,,
363,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,21,4efx2h,self.MachineLearning,[Question] Where can one take their ML skills besides a F500/tech company?,https://www.reddit.com/r/MachineLearning/comments/4efx2h/question_where_can_one_take_their_ml_skills/,christianpalestinian,1460465199,"I am an entrepreneur in the Middle East and am intrigued by big data, ML and similar areas. I am considering learning ML skills but am having trouble understanding what benefit I can bring to organizations or markets that do not already have large tech/software companies in place. 
",7,0,False,self,,,,,
364,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,22,4efyy9,self.MachineLearning,Would multidimensional/Grid LSTMs be able to get 99% on permuted pixel-by-pixel Mnist?,https://www.reddit.com/r/MachineLearning/comments/4efyy9/would_multidimensionalgrid_lstms_be_able_to_get/,abstractcontrol,1460466001,"Reading the [recurrent batch optimization](http://arxiv.org/abs/1603.09025) paper I see that standard LSTMs get 90% on that problem. I know that Grid LSTMs are better than regular ones, but I am wondering whether anybody ran the particular experiment above ? If I had to guess, I would say yes as the absolute (Manhattan) depth of the network would be 56 for a Grid LSTM compared to 784 for the regular LSTM. I would guess that a Edit: (3D) Grid LSTM would be an easier target for the optimizer.",2,16,False,self,,,,,
365,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,22,4efz37,github.com,A tensorflow and numpy implementation of the HMM viterbi and forward/backward algorithms,https://www.reddit.com/r/MachineLearning/comments/4efz37/a_tensorflow_and_numpy_implementation_of_the_hmm/,zdwiel,1460466052,,11,38,False,http://b.thumbs.redditmedia.com/HywFlHdyFUeWU6FClD-lC0jsQwSMJuWfe8IFe8oZdmc.jpg,,,,,
366,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,22,4eg27g,i.imgur.com,Two nets. At once.,https://www.reddit.com/r/MachineLearning/comments/4eg27g/two_nets_at_once/,BadGoyWithAGun,1460467405,,4,0,False,http://b.thumbs.redditmedia.com/L6KZZEdpZiWftu2_T-bY2HOoPRpqJiyPXBTylvKzbbA.jpg,,,,,
367,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,22,4eg4hq,self.MachineLearning,How much more data (in %) do I need to train an LSTM compared to a HMM?,https://www.reddit.com/r/MachineLearning/comments/4eg4hq/how_much_more_data_in_do_i_need_to_train_an_lstm/,ml_question,1460468387,,8,0,False,self,,,,,
368,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,23,4eg7ts,medium.com,News in artificial intelligence and machine learning you should know about,https://www.reddit.com/r/MachineLearning/comments/4eg7ts/news_in_artificial_intelligence_and_machine/,nb410,1460469797,,0,7,False,http://b.thumbs.redditmedia.com/UWyflv63CxKVPN1Jm4WDztZrv946kn7noHS0BKYmw4U.jpg,,,,,
369,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,23,4eg9eh,cachestocaches.com,Have You Tried Using a 'Nearest Neighbor Search'?,https://www.reddit.com/r/MachineLearning/comments/4eg9eh/have_you_tried_using_a_nearest_neighbor_search/,CharlieDarwin2,1460470420,,3,24,False,http://b.thumbs.redditmedia.com/IertLUspRmIgmZwMm-YohYn9GUeS6xgJF4mzDXSwsCA.jpg,,,,,
370,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,23,4eg9wy,self.MachineLearning,Neural Machine Translation Code ???,https://www.reddit.com/r/MachineLearning/comments/4eg9wy/neural_machine_translation_code/,november07,1460470600,"Hi, I wanted to actually train the state of the art neural translation code, and get the results as in the paper, by Cho et al, but can't find the pretrained model or the parameters to train with, does anyone know where I can get these??? ",8,0,False,self,,,,,
371,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,23,4egcgt,self.MachineLearning,"[Question] Neural Machine Translation, Char2Char - articles and/or code",https://www.reddit.com/r/MachineLearning/comments/4egcgt/question_neural_machine_translation_char2char/,alrojo,1460471601,"**Neural Machine Translation (NMT)** is a task currently performed by encoding and decoding on a word level.

**Key articles** within the field of **Word2Word NMT.**

http://arxiv.org/abs/1409.3215 - Sequence to Sequence Learning with Neural Networks, Sutskever et al., 2014

http://arxiv.org/abs/1409.0473 - Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau et al., 2014

With implementations of the articles available at the TensorFlow (TF) tutorial websites.

https://www.tensorflow.org/versions/r0.7/tutorials/seq2seq/index.html

Char2Char NMT does not seem as well studied or code represented - possibly because it is difficult to make it work. However, as emphasised by Kyunghyun Cho https://www.youtube.com/watch?v=zwYKaq9RG9w&amp;t=0h9m38s, char level NMT proposes many benefits, why it is of research interest.

What are the **key articles** for **Char2Char NMT?** and where can I find **code examples?**

The best NMT char2char articles I have been able to find are both published on arxiv within the past few weeks/months, however I could not find any implementations of these articles.

http://arxiv.org/abs/1511.04586 - Character-based Neural Machine Translation, Ling et al., 2015

Where the authors uses some tricks to build chars into the encoder and decoder of the Bahdanau, 2014 model, giving a successful model.

http://arxiv.org/pdf/1603.06147.pdf - A Character-level Decoder without Explicit Segmentation for Neural Machine Translation, Chung et al., 2016 - 

Seems like (close to) naive use of Bahdanau, 2014 with BPE-encoder_to_Char-decoder gave a succesful BLEU in the range of 24 on WMT En-De.

**What other key articles exists in the field of Char2Char NMT and where can I find code?** I have tried naively replacing words for chars in the TF Sutskever 2014 model, but with the results deteriorated from 18 validation BLEU to 2 validation BLEU on the Europarl en-fr.",2,8,False,self,,,,,
372,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,23,4egejz,arxiv.org,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier",https://www.reddit.com/r/MachineLearning/comments/4egejz/why_should_i_trust_you_explaining_the_predictions/,x2342,1460472396,,1,23,False,default,,,,,
373,MachineLearning,t5_2r3gv,2016-4-12,2016,4,12,23,4egggp,self.MachineLearning,Does it make any sense to apply convolution to inputs which have no order/distance between them?,https://www.reddit.com/r/MachineLearning/comments/4egggp/does_it_make_any_sense_to_apply_convolution_to/,thai_tong,1460473090,"I know that CNN's are used a lot for computer vision where we want to deal with local features of the image. This makes sense because one pixel influences how we interpret its neighbours.

If we had data for, say, medical decisions and we recorded many variables like age, weight, and existing medical conditions, these inputs have no distance between them and no sense of order which we could use to identify nearest neighbours.

Having said that I could imagine that it might be useful to use a CNN for the inputs because it groups together inputs in ways which are unlikely to occur by chance if we just trained a NN by stochastic gradient descent.",6,1,False,self,,,,,
374,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,0,4egltq,annalyzin.wordpress.com,Analyzing Weapons Trade using Network Graphs to Uncover Alliances between Countries,https://www.reddit.com/r/MachineLearning/comments/4egltq/analyzing_weapons_trade_using_network_graphs_to/,inxurgence,1460475041,,4,31,False,http://b.thumbs.redditmedia.com/HNmwFE7Y9yV3mzUmOwfUDZ0iHYx640Ngeh2mO9rDOQg.jpg,,,,,
375,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,0,4egnx8,armlessjohn404.github.io,Dumb calculator with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4egnx8/dumb_calculator_with_machine_learning/,ArmlessJohn404,1460475819,,21,89,False,http://b.thumbs.redditmedia.com/RWJiRnQdFlnDPDvfsovCzjBMbnc-r1h2nrkvpoI-v5k.jpg,,,,,
376,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,0,4egozk,analyticsvidhya.com,A Complete Tutorial on Tree Based Modeling (in R and Python),https://www.reddit.com/r/MachineLearning/comments/4egozk/a_complete_tutorial_on_tree_based_modeling_in_r/,john_philip,1460476217,,0,10,False,http://b.thumbs.redditmedia.com/FdIxLEUVBx1g0iLGgaF4hPbUpCaN2K2Jikv0rSnTQgk.jpg,,,,,
377,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,1,4egr3v,self.MachineLearning,"In RNN/LSTM, why passing the states across mini-batch can help learn longer dependencies?",https://www.reddit.com/r/MachineLearning/comments/4egr3v/in_rnnlstm_why_passing_the_states_across/,yield22,1460476983,"One trick for training with long sequence (that can't fit in one mini-batch) is to pass the states across multiple mini-batch, so that later batch can continue on the previous last states.

Although this trick passes the states in the forward pass over multiple batches, but it cannot passes the gradients from the later batches to previous ones. So I am curious how it can help learn longer dependencies?

(Some code using this trick can be found in keras, or andrej's blog http://karpathy.github.io/2015/05/21/rnn-effectiveness/)",2,8,False,self,,,,,
378,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,1,4egrqa,kdnuggets.com,"From Science to Data Science, a Comprehensive Guide for Transition",https://www.reddit.com/r/MachineLearning/comments/4egrqa/from_science_to_data_science_a_comprehensive/,pmigdal,1460477195,,1,9,False,http://a.thumbs.redditmedia.com/Fjv2zBsel0E6Wl2J9gTBDMcPbGjbvY7gno3wXaZk_S4.jpg,,,,,
379,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,1,4egum0,self.MachineLearning,Q. Pre-training with VAE,https://www.reddit.com/r/MachineLearning/comments/4egum0/q_pretraining_with_vae/,0entr0py,1460478223,"I am curious about the performance of unsupervised pre-training for classification with VAE. According to the paper by Kingma, pre-trained VAE + TSVM outperforms Contractive Autoencoder and Manifold tangent classifier in semi-supervised classification.  

My question is -  Since the model is forced to learn the representations distributed close to a prior ~ Gaussian with diagonal covariance, shouldn't this act as a strong regularizer making the representations less expressive and make classification more difficult on these learned representations ? The upside is that we are able to sample from the network because the distribution is close. How does this procedure (of forcing a prior) really not affect the quality of representations for downstream tasks ?",2,10,False,self,,,,,
380,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,2,4eh1fa,reddit.com,I had some fun playing around with torch-rnn to generate some TV scripts.,https://www.reddit.com/r/MachineLearning/comments/4eh1fa/i_had_some_fun_playing_around_with_torchrnn_to/,[deleted],1460480657,[deleted],0,0,False,default,,,,,
381,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,2,4eh24w,self.MachineLearning,what are the most popular Machine Learning strategies for multi-step forecasting ?,https://www.reddit.com/r/MachineLearning/comments/4eh24w/what_are_the_most_popular_machine_learning/,KarimaTouati,1460480898,[removed],0,1,False,default,,,,,
382,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,2,4eh61l,self.MachineLearning,State of the art methods for text classification?,https://www.reddit.com/r/MachineLearning/comments/4eh61l/state_of_the_art_methods_for_text_classification/,jkquijas,1460482245,What are some of the more effective recent methods for classification of documents?,2,3,False,self,,,,,
383,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,3,4ehgzg,self.MachineLearning,Is it okay to use cross entropy loss function with soft labels?,https://www.reddit.com/r/MachineLearning/comments/4ehgzg/is_it_okay_to_use_cross_entropy_loss_function/,deepbasu007,1460486035,"I have a classification problem where pixels will be labeled with soft labels (which denote probabilities) rather than hard 0,1 labels. Earlier with hard 0,1 pixel labeling the cross entropy loss function (sigmoidCross entropyLossLayer from Caffe) was giving decent results. Is it okay to use the sigmoid cross entropy loss layer (from Caffe) for this soft classification problem?",4,2,False,self,,,,,
384,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,3,4ehjgv,medium.com,Chatbots: Igniting Division of Labour in AI,https://www.reddit.com/r/MachineLearning/comments/4ehjgv/chatbots_igniting_division_of_labour_in_ai/,pelumiy,1460486906,,0,0,False,http://b.thumbs.redditmedia.com/PEGrGiUyX5j2g24Vgx2_ykrGwZR9m7d6bBC0L_QV5AI.jpg,,,,,
385,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,4,4ehrqc,brettromero.com,Data Science: A Kaggle Walkthrough  Adding New Data,https://www.reddit.com/r/MachineLearning/comments/4ehrqc/data_science_a_kaggle_walkthrough_adding_new_data/,mrbrettromero,1460489771,,1,7,False,http://b.thumbs.redditmedia.com/IyOatykVd_woUhP-zWdBOzE6-myRjVLLNAdQejH6yjA.jpg,,,,,
386,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,6,4eicmh,self.MachineLearning,Adversarial Autoenocder - Dependence on Dimensionality of Z?,https://www.reddit.com/r/MachineLearning/comments/4eicmh/adversarial_autoenocder_dependence_on/,alexmlamb,1460497147,"Hello, 

Has anyone tried running the adversarial autoencoder with different dimensions for the latent space?  The paper shows results for 2 dimensions.  

Have people observed if it works well with more dimensions?  ",12,1,False,self,,,,,
387,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,7,4eigl1,youtube.com,DanDoesData: Keras 1.0 upgrading and more RNNs,https://www.reddit.com/r/MachineLearning/comments/4eigl1/dandoesdata_keras_10_upgrading_and_more_rnns/,vanboxel,1460498635,,0,4,False,default,,,,,
388,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,7,4eijvj,self.MachineLearning,PLE: A Reinforcement Learning Environment,https://www.reddit.com/r/MachineLearning/comments/4eijvj/ple_a_reinforcement_learning_environment/,feedtheaimbot,1460499855,"Hey!

I thought I'd share a library I've been using with my personal work and projects. Its an interface around PyGame which makes it painless to start doing RL based work. Some things it gives you:

 * Growing library of games
 * Full control of games
 * Non-visual state representations
 * Easy to swap ALE out with PLE. The interface is almost the same.
 * Python (big+ if all tooling is in it)


Deciding to share it now as I'd like to get feedback from others on how to adjust the library and make it useful. Plus I was afraid of developing ""just-one-more-thing"" syndrome causing further delays. 


Current plans:

* write tutorials using it (using Keras and General Deep Q RL) which should be completed within the next few weeks. 

* add pre-trained DQN models for each game along with training curves etc. to save others time (not sure if this would be helpful/important. let me know).

* clean up docs. make them clearer etc.

* add more games?

* refactor structure slightly.


I eventually plan to extend support other libraries (PyGlet) and perhaps wrap complicated games like Quake 1 (or 2?) with the goal of keeping the interface consistent. 

Would love feedback!


[Github repo.](https://github.com/ntasfi/PyGame-Learning-Environment)

[Docs.](http://pygame-learning-environment.readthedocs.org/en/latest/index.html)

Edit: [General Deep Q RL](https://github.com/VinF/General_Deep_Q_RL) currently supports PLE out of the box.",8,35,False,self,,,,,
389,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,7,4eila2,playground.tensorflow.org,Tensorflow Playground,https://www.reddit.com/r/MachineLearning/comments/4eila2/tensorflow_playground/,gwulfs,1460500383,,30,459,False,http://b.thumbs.redditmedia.com/5HHxepPyEwftVnp7qYhOWpBSMoBqid0Fk5oZQYp55Pg.jpg,,,,,
390,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,8,4eiqep,self.MachineLearning,"In state of the art deep-learning classifiers, what is the order of these in terms of size: M(# of training examples), N(# of features), W (# of weights).",https://www.reddit.com/r/MachineLearning/comments/4eiqep/in_state_of_the_art_deeplearning_classifiers_what/,relganz,1460502341,"I know W doesn't normally mean # of weights, but I'm defining it that way for simplicity.

It seems like all of these numbers can get quite large. For example, in a ConvNet for computer vision, if you have 100x100 color images, you already have 100x100x256x256x256 features, which is so large that I assume some type of reduction must be used.  But all of those would need 1 weight at a minimum for the input nodes (and probably have far more), so W would be even larger.

Similarly, the number of training examples can be huge (e.g. ImageNet), especially if multiple mutations are used.

So what do you think is realistic?

M &gt; W &gt; N?

W &gt; N&gt; M?

Any of the above?
",4,2,False,self,,,,,
391,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,8,4eiwnd,self.MachineLearning,Building a PC with two Titan X GPUs,https://www.reddit.com/r/MachineLearning/comments/4eiwnd/building_a_pc_with_two_titan_x_gpus/,muop,1460504819,[removed],0,1,False,default,,,,,
392,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,8,4eixyv,self.MachineLearning,Logistic regression and unbalanced classes,https://www.reddit.com/r/MachineLearning/comments/4eixyv/logistic_regression_and_unbalanced_classes/,cptdoyle,1460505353,[removed],0,1,False,default,,,,,
393,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,9,4ej40g,smithsonianmag.com,Machine Learning May Help Determine When The Old Testament Was Written,https://www.reddit.com/r/MachineLearning/comments/4ej40g/machine_learning_may_help_determine_when_the_old/,[deleted],1460507887,[deleted],0,0,False,default,,,,,
394,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,10,4ej8v1,youtu.be,Stanford Machine Learning Lecture 1,https://www.reddit.com/r/MachineLearning/comments/4ej8v1/stanford_machine_learning_lecture_1/,gyeonggi,1460509909,,2,3,False,http://b.thumbs.redditmedia.com/jKYxnevc8s3rEFPi9xjdwcnmHQwdWldfzC-0dSjiHFY.jpg,,,,,
395,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,10,4ej9zm,stayclassy.io,Classy: A web-based supervised learning service written in Python,https://www.reddit.com/r/MachineLearning/comments/4ej9zm/classy_a_webbased_supervised_learning_service/,Davismj,1460510367,,0,0,False,http://b.thumbs.redditmedia.com/I0wjbirQjk_LCFNSH35kf2KN8JhFXL6DGBidWDoZmss.jpg,,,,,
396,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,12,4ejugc,playground.tensorflow.org,Tinker with a Neural Network in Your Browser,https://www.reddit.com/r/MachineLearning/comments/4ejugc/tinker_with_a_neural_network_in_your_browser/,iamkeyur,1460519174,,1,10,False,http://b.thumbs.redditmedia.com/5HHxepPyEwftVnp7qYhOWpBSMoBqid0Fk5oZQYp55Pg.jpg,,,,,
397,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,16,4ekj4m,techcrunch.com,Google Calendars newest feature uses machine learning to help you actually accomplish your goals,https://www.reddit.com/r/MachineLearning/comments/4ekj4m/google_calendars_newest_feature_uses_machine/,kraakf,1460532941,,0,3,False,http://a.thumbs.redditmedia.com/hJFidMGyuSaYTTiWbXh7tIjMiS9VVd9ecnktb1ii5g8.jpg,,,,,
398,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,16,4ekk47,self.MachineLearning,Does it make sense to use AWS or a similar service to train neural networks?,https://www.reddit.com/r/MachineLearning/comments/4ekk47/does_it_make_sense_to_use_aws_or_a_similar/,realhamster,1460533594,"I am currently going through cs231n online, and training these neural networks on my slow chromebook is not really optimal. Ideally I would buy a desktop pc with a good video card but I dont have enough money atm.

Is there a relatively cheap online service in which I can ssh into an ubuntu machine, install some software and let it train?",14,1,False,self,,,,,
399,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,17,4ekmth,homes.cs.washington.edu,The 8 page document I wish I had read when I started to learn about machine learning.,https://www.reddit.com/r/MachineLearning/comments/4ekmth/the_8_page_document_i_wish_i_had_read_when_i/,bayeslaw,1460535492,,7,116,False,default,,,,,
400,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,17,4ekop7,numenta.com,"Numenta Researchers Discover How The Brain Learns Sequences, A Key To Intelligent Systems",https://www.reddit.com/r/MachineLearning/comments/4ekop7/numenta_researchers_discover_how_the_brain_learns/,dharma-1,1460536877,,22,4,False,http://b.thumbs.redditmedia.com/jY4XqBF6EwGViX59hKUevo73Tia5iRqqA6Aza1aNEKA.jpg,,,,,
401,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,18,4ekulq,self.MachineLearning,Machine learning,https://www.reddit.com/r/MachineLearning/comments/4ekulq/machine_learning/,mgabere,1460540845,[removed],0,1,False,default,,,,,
402,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,19,4ekwkj,github.com,Generating diagrams of distributions for Bayesian hierarchical models,https://www.reddit.com/r/MachineLearning/comments/4ekwkj/generating_diagrams_of_distributions_for_bayesian/,pmigdal,1460542158,,3,6,False,http://b.thumbs.redditmedia.com/7bxbuoDodaN7XY6gYb6fMof5VoC50FJaah4R4BErA-U.jpg,,,,,
403,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,19,4ekywt,self.MachineLearning,Tensorflow vs. Theano- Which to learn,https://www.reddit.com/r/MachineLearning/comments/4ekywt/tensorflow_vs_theano_which_to_learn/,EdmondRR,1460543735,"Let's talk about the two big python based libraries for deep learning. 
I'm not sure which one to learn, as TF has BAD documentation and it looks like a pain to develop in Theano.

What's your opinion?",27,12,False,self,,,,,
404,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,20,4el2xf,somatic.io,"Docker and Deep Learning, a bad match",https://www.reddit.com/r/MachineLearning/comments/4el2xf/docker_and_deep_learning_a_bad_match/,toisanji,1460546199,,5,0,False,default,,,,,
405,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,20,4el597,self.MachineLearning,"Is r/MachineLearning ""Deep Learning News""?",https://www.reddit.com/r/MachineLearning/comments/4el597/is_rmachinelearning_deep_learning_news/,pmigdal,1460547489,"I am new here (a few weeks). While this subreddit claims to be related to various topics related to machine learning (the right bar, [FAQ](https://www.reddit.com/r/MachineLearning/wiki/index)), the practice shows that is mostly ""Deep Learning News"" (judging by links on the front page).

Moreover, when I post something I get quite a few downvotes for posts:

* without neural networks (just plain ML or a related topic)
* not being news (e.g. a 1-year old text/research paper, which I consider insightful)

Sure, deep learning is now super cool (and I share this sentiment) and it rightfully gets a lot of attention. But why downvotes for not-news or not-DL? (It's not ""nobody likes my submissions"" - I get a lot of upvotes for ""Deep Learning News"".)

So, the question for seasoned users - *Is r/MachineLearning ""Deep Learning News""?*
(I am a newcomer, so I am asking for expectations (to adjust to). I am not implying that it shouldn't be DLN.)",68,296,False,self,,,,,
406,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,22,4elnfl,self.MachineLearning,plino: Spam filtering made easy,https://www.reddit.com/r/MachineLearning/comments/4elnfl/plino_spam_filtering_made_easy/,madboy1995,1460555552,"Hey everyone, 

[Plino](https://plino.herokuapp.com/) is an intelligent spam filtering system built on top of [spammy](https://github.com/prodicus/spammy) (a custom naive bayes spam classifer)

**Features**

- REST API provided. 
- High accuracy
- Classifier trained over 33,000 from the [enron dataset](www.cs.cmu.edu/~enron/)
- Minimal downtime for service (thanks to the guys @ [heroku](https://heroku.com))
- built on top of the giant shoulders of [`nltk`](http://nltk.org)
- Completely opensourced under GPLv3

**Usage**

    $ curl -H ""Content-Type: application/json"" -X \
    POST -d \
    '{""email_text"":""SAMPLE EMAIL TEXT""}' \
    https://plino.herokuapp.com/api/v1/classify/


I would be more than happy for your valuable suggestions :)

***

**Links**

- URL: [https://plino.herokuapp.com/](https://plino.herokuapp.com/)
- Github: [https://github.com/prodicus/plino](https://github.com/prodicus/plino)
- spammy: [https://github.com/prodicus/spammy](https://github.com/prodicus/spammy)
- API usage: [https://github.com/prodicus/plino#rest-api-usage](https://github.com/prodicus/plino#rest-api-usage)
",0,0,False,self,,,,,
407,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,22,4elnqs,self.MachineLearning,Overfitting vs Convergence question [ANN],https://www.reddit.com/r/MachineLearning/comments/4elnqs/overfitting_vs_convergence_question_ann/,apple-sauce,1460555675,"I am a little bit confused. Suppose you are overfitting on your training data (for example due to not having enough training data), and your test accuracy is decreasing. Is this because the weights are stuck in a local minima, and thus will not converge?",10,2,False,self,,,,,
408,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,22,4elo37,re-work.co,"Deep Learning at Scale: Q&amp;A with Naveen Rao, Nervana Systems",https://www.reddit.com/r/MachineLearning/comments/4elo37/deep_learning_at_scale_qa_with_naveen_rao_nervana/,reworksophie,1460555800,,0,1,False,default,,,,,
409,MachineLearning,t5_2r3gv,2016-4-13,2016,4,13,23,4elsni,imgur.com,Likeliest reason train and test error would begin slowly increasing after some training?,https://www.reddit.com/r/MachineLearning/comments/4elsni/likeliest_reason_train_and_test_error_would_begin/,[deleted],1460557544,[deleted],15,2,False,default,,,,,
410,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,0,4ely4r,self.MachineLearning,Evaluating data set in pairs? How to?,https://www.reddit.com/r/MachineLearning/comments/4ely4r/evaluating_data_set_in_pairs_how_to/,amateurstatsgeek,1460559605,"I'm trying to predict the playoff results for hockey. It occurred to me that I hadn't taken into account that the playoff format isn't 16 teams all playing each other simultaneously. It's 16 teams playing in pairs and tiers until eliminated. 

In training my data I had just put in the features and playoff teams and trained it to how many wins each team had in that playoff year. 

I would probably have better results if I trained it to analyze matchups in pairs but I have no idea how to do that. Anyone got some advice?",4,0,False,self,,,,,
411,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,0,4em72a,self.MachineLearning,"In Caffe, is there any degradation in accuracy of the model when doing multi-gpu training?",https://www.reddit.com/r/MachineLearning/comments/4em72a/in_caffe_is_there_any_degradation_in_accuracy_of/,deepbasu007,1460562847," I am trying to train a VGG-16 model. When I use a single GPU with batch size 80 and learning rate 0.0002, the validation error drops to ~35% after 200k iterations. However, when I try to train the same model with the same dataset on 2 GPU's with batch size 40 and learning rate 0.0002, the error after 200k iterations is much higher ~45%. So, everything is same in the two settings. The only difference is that I adjust the batch size for the multi-GPU case to keep everything constant. Why is there a drop in validation accuracy? Am I missing something obvious here? Any help would be highly appreciated. ",4,3,False,self,,,,,
412,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,2,4emn7p,blog.liip.ch,"A short post on how to use ml to predict the bg, which is a Zrich tradition.",https://www.reddit.com/r/MachineLearning/comments/4emn7p/a_short_post_on_how_to_use_ml_to_predict_the_bg/,plotti,1460568625,,0,8,False,http://b.thumbs.redditmedia.com/b52TOQCxLBfQnjOFUlYJwBDwjWBRdTzxTkieNVGTFbo.jpg,,,,,
413,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,2,4emon6,smartcritique.com,Google Calendar's Goals (made through machine learning) will help you accomplish all your tasks,https://www.reddit.com/r/MachineLearning/comments/4emon6/google_calendars_goals_made_through_machine/,CharliVance,1460569134,,1,1,False,default,,,,,
414,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,2,4emrm8,googleresearch.blogspot.com,Announcing TensorFlow 0.8  now with distributed computing support!,https://www.reddit.com/r/MachineLearning/comments/4emrm8/announcing_tensorflow_08_now_with_distributed/,fhoffa,1460570133,,14,101,False,http://b.thumbs.redditmedia.com/nRAgpgJk_xERtqfQ07n5WU2LaWl1CUQ1G7JL-xIMPfk.jpg,,,,,
415,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,4,4en4ej,github.com,bayes.js - MCMC and Bayes in the browser,https://www.reddit.com/r/MachineLearning/comments/4en4ej/bayesjs_mcmc_and_bayes_in_the_browser/,pmigdal,1460574392,,3,15,False,http://b.thumbs.redditmedia.com/7bxbuoDodaN7XY6gYb6fMof5VoC50FJaah4R4BErA-U.jpg,,,,,
416,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,4,4en8o0,self.MachineLearning,Anyone have an issue with keras/theano on ubuntu 16.04?,https://www.reddit.com/r/MachineLearning/comments/4en8o0/anyone_have_an_issue_with_kerastheano_on_ubuntu/,klop2031,1460575873,"Has anyone seen this issue where keras seems to want to run an epoch and just sits there? I pulled theano and keras from git. I am getting something along the lines of:

    python Documents/psetTest.py 
    Using Theano backend.
    Train on 1000 samples, validate on 200 samples
    Epoch 1/1
    
and it just sits there and does nothing. Has anyone seen this?
Ubuntu 16.04
keras 1.0
theano 0.8.1",9,0,False,self,,,,,
417,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,4,4enb0o,self.MachineLearning,Deep Learning Limitation as an approach to AGI,https://www.reddit.com/r/MachineLearning/comments/4enb0o/deep_learning_limitation_as_an_approach_to_agi/,pReddit7,1460576672,[removed],0,1,False,default,,,,,
418,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,6,4enpys,self.MachineLearning,Precision and recall explained for 5-year old,https://www.reddit.com/r/MachineLearning/comments/4enpys/precision_and_recall_explained_for_5year_old/,kiote_the_one,1460581716,"Hi! I was thinking how to explain this therms for myself basically, but what if my article could help to someone else? https://kiote.io/2016/04/01/precision-and-recall-explained-for-5-year-old

Or maybe you can find some error in my ""explanation"" - would be happy to hear that as well.",3,0,False,self,,,,,
419,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,7,4enyok,self.MachineLearning,What algorithm can be used to predict future value of some thing?,https://www.reddit.com/r/MachineLearning/comments/4enyok/what_algorithm_can_be_used_to_predict_future/,brijeshdankhara,1460584916,[removed],0,1,False,default,,,,,
420,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,7,4eo0fe,self.MachineLearning,Alternative test to XOR for Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4eo0fe/alternative_test_to_xor_for_artificial_neural/,nate1421m,1460585546,"I'm looking for an alternative test to XOR for testing and evaluating the performance of artificial feed forward neural networks.

I don't want to simply select a new random test as I fear I may select  a test which is too easy for a network accidentally.

For example if I were to just select a random different logic gates to emulate, If I had naively selected AND. It would be a poor choice as AND can be created with a single neuron and therefore is a poor test for a network.

I also do not want to select a very complicated test like an Auto-encoder with MNIST training data as If I did not implement the test correctly the network would fail and I would confuse it with poor performance.

TLDR: Looking for a test which can be implemented easily and requires a ""decently"" sized network to compute it.


EDIT:
Thanks everyone for all the ideas :)",12,1,False,self,,,,,
421,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,7,4eo178,self.MachineLearning,"Search relevance for Home-Depot, academic purposes",https://www.reddit.com/r/MachineLearning/comments/4eo178/search_relevance_for_homedepot_academic_purposes/,Calcoes,1460585851,"I have taken the product search relevance challenge for a university subject. [Link here](https://www.kaggle.com/c/home-depot-product-search-relevance)

I used a python script to extract features from the dataset which includes a query, product title, product description and relevance( float from 1 to 3, which is the value we want to determinate) and i have extracted many features but selected only 7 as seen below(all numeric values):

1.     Common words between title and query
2.     Percentage of query words found in title
4.     Ratcliff/Obershelp Query and Title Similarity
5.     Levenshtein distance between Query and Title
14.   Common words between description and query
15.   Percentage of query words found in description
23. Ratcliff/Obershelp query and description similarity

Then i used weka with discretized data and j48 algorithm to assess the percentage of correctly classified instances but only obtained 27%, any idea on good features to extract from the text that can wield a better result?

",3,0,False,self,,,,,
422,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,8,4eoa0r,self.MachineLearning,Performance: Sparse Vectorization vs for loops?,https://www.reddit.com/r/MachineLearning/comments/4eoa0r/performance_sparse_vectorization_vs_for_loops/,soul_me,1460589306,"I run into this type of problem frequently:

You have a matrix **M** where each row is a vector of features of an item. You have one vector **a** representing one of the rows in **M**. 


You want to find the average similarity between **a** and a specific set of rows in **M**. Similary is defined by L1 (linear) distances.


Vectorized implementation:

    Vector w = [0,1,0,1,0] represents the indexes of rows in M (0 = ignore row, 1 = get similarity)
    C = M - a                       (row by row subtraction)
    s = sum_rows( abs_val(C)  )     (absolute value of all elements in matrix and sum horizontally)
    distance_sum = dot(s, w)        (dot product of two vectors)
    average similarity = distance_sum / sum (w) 
 

For loop implementation:

    q = list of indexes of rows to get similarity of
    for index in q:
        distance_sum += sum( absolute_value(a - M[index,:]) )
    average_similarity = distance_sum / q.length()


When would you use one method over the other?

",3,1,False,self,,,,,
423,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,8,4eoech,self.MachineLearning,How to apply LSTM for question answer selection problem?,https://www.reddit.com/r/MachineLearning/comments/4eoech/how_to_apply_lstm_for_question_answer_selection/,Pdksock,1460591022,"For input to the LSTM I have data in the form of triplets &lt;Qi, Aij, Lij&gt; where every Question Qi has a list of Answers where each answer is labelled. I want to train this model over a number of triplets like this and during testing I want to predict the label of an answer given its question. What I am confused about is how to go about implementing many to one lstm? 

What I can think of is:


1) Pass the word vectors of question followed by &lt;Q&gt; tag.


2) Pass the word vectors of answers followed by &lt;A&gt; tag.


3) Pass the label and observe the output. Use loss function only at this step. 

Is this approach correct?",7,1,False,self,,,,,
424,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,8,4eoffy,self.MachineLearning,"What variable name do you prefer for number of records, M or N? What do you prefer for number of features, N or P?",https://www.reddit.com/r/MachineLearning/comments/4eoffy/what_variable_name_do_you_prefer_for_number_of/,relganz,1460591478,,4,0,False,self,,,,,
425,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,9,4eojzv,self.MachineLearning,Interesting job requirement - must have experience in time travel,https://www.reddit.com/r/MachineLearning/comments/4eojzv/interesting_job_requirement_must_have_experience/,CaffeineJag,1460593315,"""10+ years of hands on experience with machine learning applied to big data""",7,0,False,self,,,,,
426,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,12,4ep9o0,youtube.com,Let water come out of air receiver AEOMACHINE,https://www.reddit.com/r/MachineLearning/comments/4ep9o0/let_water_come_out_of_air_receiver_aeomachine/,AEOMACHINE,1460603404,,1,1,False,default,,,,,
427,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,14,4eprzb,youtu.be,Visualizing a Decision Tree - Machine Learning Recipes #2,https://www.reddit.com/r/MachineLearning/comments/4eprzb/visualizing_a_decision_tree_machine_learning/,Akatchi,1460612133,,0,21,False,http://b.thumbs.redditmedia.com/tqD6B1bj6a5eNLs9epie3prt54hsPhwBfUmMPI9gt0I.jpg,,,,,
428,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,15,4epzcg,haifengl.github.io,Smile - Statistical Machine Intelligence and Learning Engine,https://www.reddit.com/r/MachineLearning/comments/4epzcg/smile_statistical_machine_intelligence_and/,alexeyr,1460616567,,3,16,False,http://a.thumbs.redditmedia.com/XcKMmrgyMEF5iFEkyRm-tP0sJv6d6C_oDhr0tmy1CF8.jpg,,,,,
429,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,16,4eq2z7,codeschool.com,Machine Learning: an introduction from Code School using text learning,https://www.reddit.com/r/MachineLearning/comments/4eq2z7/machine_learning_an_introduction_from_code_school/,jiabanster,1460618975,,0,7,False,http://a.thumbs.redditmedia.com/OcLLhYpSiilqseNCGeZXxCRVt3K32_N8-KuphREJiL4.jpg,,,,,
430,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,18,4eqedi,wired.com,Google Is About to Supercharge Its TensorFlow Open Source AI,https://www.reddit.com/r/MachineLearning/comments/4eqedi/google_is_about_to_supercharge_its_tensorflow/,Martin81,1460626791,,0,0,False,http://b.thumbs.redditmedia.com/KhWU7IhiBoFzxH3SWV7YJzgo0p1-zCiaraNtoXGWnGQ.jpg,,,,,
431,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,18,4eqfd3,self.MachineLearning,Inputs and Output of Gradient Boosting Decision Trees? (related to item recommendations),https://www.reddit.com/r/MachineLearning/comments/4eqfd3/inputs_and_output_of_gradient_boosting_decision/,[deleted],1460627441,[removed],0,1,False,default,,,,,
432,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,19,4eqifs,self.MachineLearning,Gaussian observation VAE,https://www.reddit.com/r/MachineLearning/comments/4eqifs/gaussian_observation_vae/,disentangle,1460629410,"I'm trying to model real-valued data with a VAE, for which the typical thing (afaik) is to use a diagonal covariance Gaussian observation model p(x|z).
One problem I'm having fairly consistently is that after only a few epochs (say 5~10) the means of p(x|z) (with z ~ q(z|x)) are very close to x and after a while the learned variances of p(x|z) become very small (e.g. 0.04).
At this point the reconstruction term of the loss dominates the KL divergence term of the loss; the net cost of lowering KLD term becomes very high (because it would decrease the reconstruction term a lot).
So eventually I end up with a very weakly regularized VAE which is quite similar to a regular AE, and the learned features are not very meaningful.

I tried a couple of things that are supposed to help with this kind of issue, such as batch normalization, PCA transforming inputs, etc., but so-far the results have not been very encouraging.

Fixing variances to a constant value seems like a solution, but I'm not sure which constant makes sense (and it seems to affect the reconstruction/KLD balance significantly).
So I'm wondering if there's some kind of ""less hacky"" solution to this issue.",13,27,False,self,,,,,
433,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,20,4eqqu6,self.MachineLearning,Churn Model: Including features/attributes that is not specific to the customer behaviour and its impact.,https://www.reddit.com/r/MachineLearning/comments/4eqqu6/churn_model_including_featuresattributes_that_is/,rajivhere2016,1460634261,[removed],0,1,False,default,,,,,
434,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,22,4er229,self.MachineLearning,What are you working on?,https://www.reddit.com/r/MachineLearning/comments/4er229/what_are_you_working_on/,weirdML,1460639595,,16,4,False,self,,,,,
435,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,22,4er5ev,self.MachineLearning,How to add constraints over variables to a Q-Network learning agent?,https://www.reddit.com/r/MachineLearning/comments/4er5ev/how_to_add_constraints_over_variables_to_a/,siddkotwal,1460641003,"Since my tabular Q-learning approach failed to generalize action-values, I moved to function approximation. With the help of everyone here at /r/MachineLearning I used a neural network for estimating the best action-value for a given state and this worked well for a toy problem. I also recently read the DeepMind paper on Atari and checked out a few implementations, so I'm hooked to writing my own sometime!

My question is, if I have certain constraint(s) involving my state continuous variables for choosing an action, how do I factor them in before feeding it to the Q-network? Currently I was doing something like this - 

(1) Feed the state to the Q-network and get action-values. 

(2) Select the action corresponding to maximum action-value and test if the constraint holds true. IF NOT select the next best action, repeat. 

This somehow doesn't lead me anywhere close to optimality! What am I doing wrong? 

* Another issue might be the size of my dataset, which might not be large enough for Q-network to be trained well. However, I tried to extend it by copying the same data again (1 year extended to 2 years and so on). And I somehow started seeing repeating patterns in my accumulated reward! Which is useless.

(3) How are problems that involve Q-network as function approximators with small datasets solved?

Thanks!",0,0,False,self,,,,,
436,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,23,4er9cl,self.MachineLearning,Multiclass classification based on a matrix,https://www.reddit.com/r/MachineLearning/comments/4er9cl/multiclass_classification_based_on_a_matrix/,valikund,1460642538,"HI. I have a problem where I have to classify time series. I created a matrix, which contains the DTW distances of the time series. So at index I,J the DTW distance of sample I and J is located. 
There are 12 different classes. For the evaluation I get a test time series with a supposed class label, and I have to decide, whether the label is right or wrong. Can anybody give me an advice which classifiaction algorithm to use (knn, random forest..)?",0,0,False,self,,,,,
437,MachineLearning,t5_2r3gv,2016-4-14,2016,4,14,23,4ergy1,gitxiv.com,Deep Networks with Stochastic Depth stochastic depth training procedure trains short networks and obtains deep networks,https://www.reddit.com/r/MachineLearning/comments/4ergy1/deep_networks_with_stochastic_depth_stochastic/,[deleted],1460645337,[deleted],0,1,False,default,,,,,
438,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,0,4erjkm,self.MachineLearning,AI for Smart Home,https://www.reddit.com/r/MachineLearning/comments/4erjkm/ai_for_smart_home/,SmartAll,1460646219,[removed],1,1,False,default,,,,,
439,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,0,4err6c,self.MachineLearning,CNN for binary classification,https://www.reddit.com/r/MachineLearning/comments/4err6c/cnn_for_binary_classification/,azraelxii,1460648912,"I have read a few books and constantly see CNNs brought up when trying to classify images. Can they be used for binary classification, or is it over kill? ",5,1,False,self,,,,,
440,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,1,4ervmf,self.MachineLearning,Tensorflow RNN time series prediction,https://www.reddit.com/r/MachineLearning/comments/4ervmf/tensorflow_rnn_time_series_prediction/,haskkk,1460650440,"There are lots of examples using tensorflow rnns to do text generation or prediction on MNIST, however I am looking to do prediction on continuous data. For example. Feed an RNN a sign wave and have it predict, similar to this example: http://www.fuzihao.org/blog/2016/02/29/Predict-Time-Sequence-with-LSTM/
",14,12,False,self,,,,,
441,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,1,4erwbh,self.MachineLearning,How do I interpret and debug my NN model?,https://www.reddit.com/r/MachineLearning/comments/4erwbh/how_do_i_interpret_and_debug_my_nn_model/,mln00b13,1460650674,"I am using Tensorflow's ImageNet model to train my own images. I have also used Keras to buld my models, but I can't figure out how to debug it next. I want to see what each layer does to my image, as well as what the whole model does. For example, given a test image, is it possible to plot on the image what parts of the image made it pick that particular class? Also, for my test image, plot on the image what each layer outputs, as well as for each class, plot the most important features on an image? Or am I asking too much?",4,1,False,self,,,,,
442,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,1,4erxe5,self.MachineLearning,Noob question about Tensorflow Playground,https://www.reddit.com/r/MachineLearning/comments/4erxe5/noob_question_about_tensorflow_playground/,The_Fifth_Henry,1460651021,"What is the meaning of ""Which properties do you want to feed in?"" in the input layer?

I understand the effect of choosing one or multiple, but can't fully grasp what I'm doing.",7,4,False,self,,,,,
443,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,1,4es0w8,self.MachineLearning,Very strange binary classifier results,https://www.reddit.com/r/MachineLearning/comments/4es0w8/very_strange_binary_classifier_results/,eth-dev,1460652151,"I'm doing binary classification via logistic regression (about 2.500 observations and 30 predictor variables). Overall, the predictor variables are modestly predictive of the output variable (AUC = 0.55). 

However, it's possible to partition the observations into two subsets with the following properties:
- in each data subset, the binary classifier achieves a near-perfect test performance (AUC ~= 1).
- the model effects in one data subset are exactly the opposite of the model effects on the other data set, with the magnitude being similar. For instance, a very significant *positive* effect in one subset is a very significant *negative* effect in another subset. 

The whole data set is balanced (about 45% zeros and 55% ones), and so are both data subsets, so this is not a trivial outcome of placing all ""ones"" into one subset and ""zeros"" into another subset. 

My initial thought was that there were encoding problems with the binary output variable, but the persons who curated the data told me everything is OK with the data. 


Any thoughts about alternative explanations?",6,0,False,self,,,,,
444,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,2,4esdia,itila.blogspot.fr,"in memory of David MacKay FRS, a remarkable post of how a pioneer in Machine Learning dealt with his illness",https://www.reddit.com/r/MachineLearning/comments/4esdia/in_memory_of_david_mackay_frs_a_remarkable_post/,olaf_nij,1460656380,,14,147,False,http://a.thumbs.redditmedia.com/YrQF0PEFlsUpAVEH3kduugEAtjAAPcTJpMUcRGvaPX0.jpg,,,,,
445,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,3,4esfnn,self.MachineLearning,Which of these math courses will be useful in Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/4esfnn/which_of_these_math_courses_will_be_useful_in/,[deleted],1460657101,[removed],0,1,False,default,,,,,
446,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,6,4ethu5,self.MachineLearning,Why we need to map to lower-dimensionality space,https://www.reddit.com/r/MachineLearning/comments/4ethu5/why_we_need_to_map_to_lowerdimensionality_space/,John_Smith111,1460670243,"
Hello all
I would like to ask why we want to map data to lower-dimensionality space ?

As example self organizing maps can be used to produce lower-dimensionality data sets: 

http://www.turingfinance.com/misconceptions-about-neural-networks/

What outcome will  provide that to us ? Maybe we can achieve easy clustering ? 
As far as i know pulling to high dimension will provide us easy regresion - kernel trick ... what about dimension reduction?

10x all

",8,1,False,self,,,,,
447,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,6,4etk4d,thoughtly.co,Deep Learning Tutorial Series for NLP: Lesson 3 Simple Networks and Code  Thoughtly,https://www.reddit.com/r/MachineLearning/comments/4etk4d/deep_learning_tutorial_series_for_nlp_lesson_3/,mercurialkitten,1460671026,,0,2,False,http://a.thumbs.redditmedia.com/fTmNYFUr362Nf8SakYBzLozNbzqYhuCau8kKn17NZ34.jpg,,,,,
448,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,7,4etmod,self.MachineLearning,Do and how deep belief net achieves adaptive kernel,https://www.reddit.com/r/MachineLearning/comments/4etmod/do_and_how_deep_belief_net_achieves_adaptive/,John_Smith111,1460671982,"Do deep belief nets achieves adaptive kernel ? if yes - how it achived - some property of RBMs? 
",0,0,False,self,,,,,
449,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,8,4etxwv,cmry.github.io,Scikit-learn Pipeline Persistence and JSON Serialization,https://www.reddit.com/r/MachineLearning/comments/4etxwv/scikitlearn_pipeline_persistence_and_json/,[deleted],1460676329,[deleted],0,0,False,default,,,,,
450,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,8,4eu2vd,self.MachineLearning,Can pre-trained networks be used in commercial applications?,https://www.reddit.com/r/MachineLearning/comments/4eu2vd/can_pretrained_networks_be_used_in_commercial/,gauss256,1460678354,"Suppose you want to use a VGG pre-trained network in a commercial application. That seems legit because the VGG models are [made available under a Creative Commons Attribution license](http://www.robots.ox.ac.uk/~vgg/research/very_deep/).

But the models were trained on ImageNet so I guess they are a derivative work of ImageNet. I can't find information about what kind of license ImageNet has and how it would affect the licensing of a model trained on it.

Can anyone clarify the situation?",24,11,False,self,,,,,
451,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,9,4eu9qd,self.MachineLearning,Pre-extracted fc7 CNN representations for large image sets,https://www.reddit.com/r/MachineLearning/comments/4eu9qd/preextracted_fc7_cnn_representations_for_large/,anonDogeLover,1460681208,Does anyone know if there are datasets available with pre-extracted AlexNet fc7 representations from a large and diverse set of images (like a random imagenet sample)?,5,1,False,self,,,,,
452,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,10,4eucf1,self.MachineLearning,Tuning priors/weights/costs to counteract class imbalance,https://www.reddit.com/r/MachineLearning/comments/4eucf1/tuning_priorsweightscosts_to_counteract_class/,Bohemian90,1460682353,"Hello

I have a classification problem which consists of two classes. It has high class imbalance. There are around 85% data points for the negative class and only 15% for the positive class.

One option is upsampling but I would also like to try using priors/weights/costs of the classifiers.

How should the priors, weights or costs be chosen (or the range to search)?",6,0,False,self,,,,,
453,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,10,4eug5z,github.com,DeepMark: Next Generation of Deep Learning benchmarks,https://www.reddit.com/r/MachineLearning/comments/4eug5z/deepmark_next_generation_of_deep_learning/,pranv,1460683914,,4,32,False,http://b.thumbs.redditmedia.com/m2LJoJ033T9SvszQ3VTiDrLfM5C4dFGq8obfZ06TqFg.jpg,,,,,
454,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,10,4euhaa,self.MachineLearning,Gradient Descent for Elastic net Regression,https://www.reddit.com/r/MachineLearning/comments/4euhaa/gradient_descent_for_elastic_net_regression/,Vainsingr,1460684388,"I am using the from the wikipedia page to find the gradient descent.
https://en.wikipedia.org/wiki/Elastic_net_regularization.

What will be gradient descent equation for this. ?

And as for ridge regression if i am using very large data sets. Instead of calculating the inverse is there another way to calculate it.

Ridge Regression eqn -  = (X^T. X + I)^1.X^T .y

Thanks",6,0,False,self,,,,,
455,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,12,4euzmk,self.MachineLearning,log-sum-exp for logistic regression,https://www.reddit.com/r/MachineLearning/comments/4euzmk/logsumexp_for_logistic_regression/,snickerdoodol,1460692226,"As mentioned here: https://hips.seas.harvard.edu/blog/2013/01/09/computing-log-sum-exp/, the log-sum-exp trick is often used to prevent overflow / underflow for multiclass logistic regression, specifically when calculating the negative log-likelihood. However, if the argument to exp(w^T x) is large enough to cause overflow, wouldn't that also be the case for standard binary logistic regression as well, since negative-log-likelihood in that case contains the sigmoid function, which also has exp(w^T x)? However, I don't think log-sum-exp can be applied to binary logistic regression, right?",8,2,False,self,,,,,
456,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,12,4ev04j,pocketcluster.wordpress.com,"BigData examples, libraries and toolsets weekly roundup  Apr. 15, 2016",https://www.reddit.com/r/MachineLearning/comments/4ev04j/bigdata_examples_libraries_and_toolsets_weekly/,stkim1,1460692458,,0,1,False,default,,,,,
457,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,12,4ev0lb,arxiv.org,Deep Residual Networks with Exponential Linear Unit,https://www.reddit.com/r/MachineLearning/comments/4ev0lb/deep_residual_networks_with_exponential_linear/,[deleted],1460692672,[deleted],13,3,False,default,,,,,
458,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,13,4ev5fh,crowdcourse.stanford.edu,Join Stanford Crowd Course Initiative to collaborate and create open courses on machine learning and related topics.,https://www.reddit.com/r/MachineLearning/comments/4ev5fh/join_stanford_crowd_course_initiative_to/,rajcalif,1460695095,,0,16,False,http://a.thumbs.redditmedia.com/dRAaDRK3RsmX2F0jJeYz0StWDrzzSwvndE-IEk8Uy50.jpg,,,,,
459,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,13,4ev5yj,github.com,BetaGo - Create Go bots with Keras Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4ev5yj/betago_create_go_bots_with_keras_deep_learning/,maxpumperla,1460695371,,0,33,False,http://b.thumbs.redditmedia.com/CZQUJccG8xI6FlWdiOBeOl_79GE43YITfb6dS4--ejU.jpg,,,,,
460,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,16,4evlps,self.MachineLearning,Does TensorFlow's image retraining example support multi label classification?,https://www.reddit.com/r/MachineLearning/comments/4evlps/does_tensorflows_image_retraining_example_support/,n00bto1337,1460704325,"Also, instead of the final_result layer, is it possible to get the feature vector, like the pool3 features for this model?",0,0,False,self,,,,,
461,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,17,4evt1b,inverse.com,How the Multi-Armed Bandit Determines What You See Online,https://www.reddit.com/r/MachineLearning/comments/4evt1b/how_the_multiarmed_bandit_determines_what_you_see/,visarga,1460708896,,1,5,False,http://b.thumbs.redditmedia.com/74q1GzoUhqxGnMkd8tlr1m-OWyhds4Z9giCedrVs5NA.jpg,,,,,
462,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,19,4ew3sv,self.MachineLearning,How do I show images instead of plot points while doing tsne in Python?,https://www.reddit.com/r/MachineLearning/comments/4ew3sv/how_do_i_show_images_instead_of_plot_points_while/,mln00b13,1460715995,"I'm using the Barnes Hut tsne python implementation from here http://lvdmaaten.github.io/tsne/, but it plots points on a scatter plot. How can I get it to plot/show original images instead of points? How do I generate something like http://cs.stanford.edu/people/karpathy/cnnembed/?",5,1,False,self,,,,,
463,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,19,4ew4qe,livestream.com,Watch Today: Free Live Streaming from The Machine Learning Conference in NYC,https://www.reddit.com/r/MachineLearning/comments/4ew4qe/watch_today_free_live_streaming_from_the_machine/,shonburton,1460716580,,4,18,False,http://b.thumbs.redditmedia.com/YSDsCUOdd4M5JW7h4Wc3BQG0aaphEhAmxkRN39Qj0JU.jpg,,,,,
464,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,20,4ew7la,itila.blogspot.com,"David J.C. MacKay, Machine Learning pioneer, dies",https://www.reddit.com/r/MachineLearning/comments/4ew7la/david_jc_mackay_machine_learning_pioneer_dies/,iamkeyur,1460718335,,6,125,False,http://b.thumbs.redditmedia.com/Y1fnkjWSuQbBijnVfNx9QW7eLc6nS6wvLuz8UxWS1HM.jpg,,,,,
465,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,20,4ew96j,self.MachineLearning,[Question] Is machine learning actually used to detect oil-spills or is it only been researched?,https://www.reddit.com/r/MachineLearning/comments/4ew96j/question_is_machine_learning_actually_used_to/,Zahand,1460719260,"Hello, so I am writing a paper on machine learning and I found this very interesting articale from 1998 where they researched on how machine learning could be used to detect oil spills from radar satellite images. 

I was wondering if these are actually used today, or if it still just something people have researched. 

I have tried searching for it, but i cannot find any reliable source confirming that machine learning is infact used to detect oil spills. 

The article mentioned: http://link.springer.com/article/10.1023%2FA%3A1007452223027

Thanks in advance. ",1,1,False,self,,,,,
466,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,21,4eweho,self.MachineLearning,Online Semi-supervised regressor,https://www.reddit.com/r/MachineLearning/comments/4eweho/online_semisupervised_regressor/,richardsutherland198,1460722068,[removed],0,1,False,default,,,,,
467,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,21,4ewflv,re-work.co,"The intersection of machine learning &amp; IoT for a healthier, smarter home",https://www.reddit.com/r/MachineLearning/comments/4ewflv/the_intersection_of_machine_learning_iot_for_a/,reworksophie,1460722601,,0,1,False,default,,,,,
468,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,21,4ewhv9,youtube.com,Proof you don't need to know calculus very well to be a ML researcher,https://www.reddit.com/r/MachineLearning/comments/4ewhv9/proof_you_dont_need_to_know_calculus_very_well_to/,mean_MLer,1460723605,,0,1,False,default,,,,,
469,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,22,4ewpmq,deeplearningskysthelimit.blogspot.nl,Deep Learning AlphaGo under a Magnifying Glass: how does the AI program manage to play superior Go?,https://www.reddit.com/r/MachineLearning/comments/4ewpmq/deep_learning_alphago_under_a_magnifying_glass/,DeepLearningBob,1460726971,,9,34,False,http://b.thumbs.redditmedia.com/sgc2qhFh-sUCwCvA9J2dsmuKxcHwCOubXsUQZ1wSWLc.jpg,,,,,
470,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,22,4ewtmt,self.MachineLearning,Looking for 1 on 1 Machine Learning Tutor,https://www.reddit.com/r/MachineLearning/comments/4ewtmt/looking_for_1_on_1_machine_learning_tutor/,rikeen,1460728656,"Like the title states, I'm looking for an experienced Machine Learning practitioner who has the time and teaching aptitude to tutor one on one. I have a need to become interview ready in the next few months.

I'm looking for a very quick turnaround, hence why I would prefer someone who has insider knowledge. 

Things I want to learn/have some experience in:

* R
* Statistical Learning
* Python
* Basic linear algebra (only as needed)

About me:
I have an IT background, mostly consisting of grid/server maintenance and evaluation at an enterprise level. I have worked in preventative measures for the past 2-3 years. Just out of college.

Please PM if you think you are capable. Willing to pay and put in the requisite time/effort.",16,2,False,self,,,,,
471,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,23,4ewywu,self.MachineLearning,Is my understanding of REINFORCE correct (Alphago)?,https://www.reddit.com/r/MachineLearning/comments/4ewywu/is_my_understanding_of_reinforce_correct_alphago/,re_enforce,1460730688,[removed],0,1,False,default,,,,,
472,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,23,4ewyyr,youtube.com,Mario Becomes Social! Cognitive Modeling,https://www.reddit.com/r/MachineLearning/comments/4ewyyr/mario_becomes_social_cognitive_modeling/,com2mentator,1460730704,,7,31,False,http://a.thumbs.redditmedia.com/c_8pgg1hzxCrET-vU4uYI4NB0LRQczEGzIwnu2Flgl8.jpg,,,,,
473,MachineLearning,t5_2r3gv,2016-4-15,2016,4,15,23,4ex1wt,self.MachineLearning,Which of these math courses would be useful in machine learning?,https://www.reddit.com/r/MachineLearning/comments/4ex1wt/which_of_these_math_courses_would_be_useful_in/,[deleted],1460731871,[removed],0,1,False,default,,,,,
474,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,0,4ex9kk,youtube.com,HTM School Episode 2: SDR Capacity &amp; Comparison,https://www.reddit.com/r/MachineLearning/comments/4ex9kk/htm_school_episode_2_sdr_capacity_comparison/,numenta,1460734631,,1,1,False,http://b.thumbs.redditmedia.com/L9OdeWHzbIwE5Gff0fDB9mvbSPaqSZHlfDxaEaKydPY.jpg,,,,,
475,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,0,4exbmy,arxiv.org,Inference of Plant Diseases from Leaf Images through Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4exbmy/inference_of_plant_diseases_from_leaf_images/,panisson,1460735380,,4,16,False,default,,,,,
476,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,1,4exk3u,arxiv.org,DCNNs are more sensitive to blur and noise than JPEG compression or low-contrast,https://www.reddit.com/r/MachineLearning/comments/4exk3u/dcnns_are_more_sensitive_to_blur_and_noise_than/,kcimc,1460738384,,8,7,False,default,,,,,
477,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,2,4exv36,pbs.twimg.com,This Week in Machine Learning: April 15th,https://www.reddit.com/r/MachineLearning/comments/4exv36/this_week_in_machine_learning_april_15th/,DavidAJoyner,1460742170,,3,1,False,http://b.thumbs.redditmedia.com/KIMnafx4_y0kWq7d6p8GZrmZAEym-BnKOhyehUq59Gk.jpg,,,,,
478,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,3,4ey08m,datasciencecentral.com,"11 Deep Learning Articles, Tutorials and Resources",https://www.reddit.com/r/MachineLearning/comments/4ey08m/11_deep_learning_articles_tutorials_and_resources/,vincentg64,1460743937,,0,1,False,default,,,,,
479,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,4,4eyb9y,self.MachineLearning,How to do anomaly/abnormality detection in images using deep learning?,https://www.reddit.com/r/MachineLearning/comments/4eyb9y/how_to_do_anomalyabnormality_detection_in_images/,SnowRipple,1460747767,"Hi there!

   I have medical images in which there are different artefacts/local entities. I would like my system to be able to tell me if a given local entity (after localization which can be a separate step which I think I have figured out) is of unusual shape/texture/colour, shortly speaking if it is abnormal/anomalous .

There is no dataset with annotated abnormalities so it  have to be unsupervised. The obvious way would be to use an autoencoder, but which one? Or RNN somehow? 

I can't find any leads related to deep learning and anomaly detection.

What would you propose to look at/research?",10,7,False,self,,,,,
480,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,4,4eydcb,self.MachineLearning,Music-RNN Samples and a Request for Advice,https://www.reddit.com/r/MachineLearning/comments/4eydcb/musicrnn_samples_and_a_request_for_advice/,JosephLChu,1460748453,"So, [here are some samples generated with Music-RNN](https://www.youtube.com/playlist?list=PL-Ewp2FNJeNJp1K1PF_7NCjt2ZdmsoOiB).

This is basically a Recurrent Neural Network trained on raw audio data.  It uses LSTM neurons right now, though I tried implementing the Clockwork RNN architecture and also the Gated Feedback RNN  architecture, but found their performance to actually be worse than just a straightforward LSTM-based network (though I don't know if my implementation might have just been wrong).

It does use Dropout, as well as the technique for scaling the norm of the gradient that Bengio's team came up with.  I've spent countless hours running various hyperparameter settings and yet I find myself unable to significantly improve the performance of the network further.

Does anyone have any advice on what to do with this project?  It seems like it has potential, but I don't really know if I'm just deluding myself at this point to justify the effort put into it.",26,1,False,self,,,,,
481,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,4,4eyeqt,blog.monkeylearn.com,New MonkeyLearn integration with Scrapinghub,https://www.reddit.com/r/MachineLearning/comments/4eyeqt/new_monkeylearn_integration_with_scrapinghub/,wildcodegowrong,1460748945,,0,0,False,http://b.thumbs.redditmedia.com/t3UdIiRE4GJMiytGYXScK9wMkp4BhYfLCZZku8-X5uo.jpg,,,,,
482,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,9,4ezq3h,self.MachineLearning,Advanced Data Science Program Teaches Scala for Free,https://www.reddit.com/r/MachineLearning/comments/4ezq3h/advanced_data_science_program_teaches_scala_for/,DS12Residency,1460767729,"DataScience, Inc. will host a residency program delivering functional data science education-- for free. Top candidates earn a seat in the highly competitive program run by Chris McKinlay, known for hacking OkCupid: https://www.reddit.com/r/Damnthatsinteresting/comments/1vua25/how_a_math_genius_hacked_okcupid_to_find_true_love/

Learn more at education.datascience.com",0,3,False,self,,,,,
483,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,10,4ezvhn,self.MachineLearning,Need a partner to make the first general AI ever?,https://www.reddit.com/r/MachineLearning/comments/4ezvhn/need_a_partner_to_make_the_first_general_ai_ever/,ocdthrowaway958,1460770200,"I have decided to begin the quest to make the first general AI in existence. I am more than capable of doing it on my own, but would rather not waste time doing the tedious, boring stuff (i.e. figuring out how to use some stupid API).

I am willing to share (partial credit) with someone who agrees to work with me. I would be giving you weekly tasks and I would expect them done by the end of the week, no matter what. You won't get an opportunity to contribute ideas unfortunately. I am not interested in what you have to say, to be honest.

I do not have much experience in the field, but I have an IQ of a 156 and the ability to learn EXTREMELY fast.

Message me if you'd like to go down in history as an assistant for possibly the most significant project in human history",51,0,False,self,,,,,
484,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,10,4ezxun,self.MachineLearning,Is Spearman's rho a good metric for evaluating the results of regression problem?,https://www.reddit.com/r/MachineLearning/comments/4ezxun/is_spearmans_rho_a_good_metric_for_evaluating_the/,just_anhduc,1460771288,[removed],0,1,False,default,,,,,
485,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,11,4f06te,self.MachineLearning,Survey for Deep Learning Algorithm that Identifies Extraversion Through Twitter,https://www.reddit.com/r/MachineLearning/comments/4f06te/survey_for_deep_learning_algorithm_that/,danielcanadia,1460775493,"Hi everyone.

I am currently creating an algorithm that is capable of identifying personality traits such as extraversion through people's Twitter profiles. It uses a Word2vec-like neural network and Deep Learning with Long Short-Term Memory layers to accomplish this task. The accuracy for extraversion is currently at 87-91% but I need more data.

If you have Twitter, I would really appreciate if you could take this survey and input your Twitter profile. Also retweeting it would be awesome. https://docs.google.com/forms/d/1jsTgBU49BhbExC2bPEGwemT3LlZWFpoW8gZsjax5Cm4/viewform#responses

Glad to also answer any questions :)",2,0,False,self,,,,,
486,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,12,4f07rp,youtube.com,Google has started a new video series teaching machine learning and I can actually understand it.,https://www.reddit.com/r/MachineLearning/comments/4f07rp/google_has_started_a_new_video_series_teaching/,iamkeyur,1460775996,,155,700,False,http://b.thumbs.redditmedia.com/QhtHtiZPceoVh17djIW2miHKTMv16xaW8rDTlAShhTo.jpg,,,,,
487,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,17,4f13z5,self.MachineLearning,Machine learning and Control Algorithms (Career Guidance),https://www.reddit.com/r/MachineLearning/comments/4f13z5/machine_learning_and_control_algorithms_career/,lukewarmasian,1460795702,[removed],0,1,False,default,,,,,
488,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,17,4f15hu,reddit.com,[Topic on /r/computervision] How the Computer Vision community fits with the Deep Learning landscape,https://www.reddit.com/r/MachineLearning/comments/4f15hu/topic_on_rcomputervision_how_the_computer_vision/,[deleted],1460796829,[deleted],0,0,False,default,,,,,
489,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,20,4f1gjk,self.MachineLearning,How do I run the tensorflow docker image without jupyter notebook?!?!,https://www.reddit.com/r/MachineLearning/comments/4f1gjk/how_do_i_run_the_tensorflow_docker_image_without/,homestead_cyborg,1460804734,"Sorry if noob or wrong forum, 

I am struggling a bit with docker. The official tensorflow image automatically runs the jupyter notebook upon start. I don't want that, since I'd like to do some additional installs of tools I am using. And when I do Ctrl-C to shut down the notebook, the entire container shuts down... ",3,0,False,self,,,,,
490,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,21,4f1r8c,self.MachineLearning,"I want to learn machine learning, but I need to learn the required basics, like linear regression. Any tutorial out there which focuses on having a solid base to use ML ?",https://www.reddit.com/r/MachineLearning/comments/4f1r8c/i_want_to_learn_machine_learning_but_i_need_to/,jokoon,1460810867,"I'm not ignorant in math, but it seems most ML course often skips those basics. I guess I mostly needs some parts on statistics, although not most of it ?",21,0,False,self,,,,,
491,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,22,4f1t9f,self.MachineLearning,5 Popular Applications of Metal Bellow Seals,https://www.reddit.com/r/MachineLearning/comments/4f1t9f/5_popular_applications_of_metal_bellow_seals/,fbumechanical,1460811861,[removed],0,1,False,default,,,,,
492,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,22,4f1uhg,self.MachineLearning,Functions and Physical Properties of Metal Bellow Seals,https://www.reddit.com/r/MachineLearning/comments/4f1uhg/functions_and_physical_properties_of_metal_bellow/,fbumechanical,1460812498,[removed],0,1,False,default,,,,,
493,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,23,4f26g5,gist.github.com,A small script to download all UCI datasets,https://www.reddit.com/r/MachineLearning/comments/4f26g5/a_small_script_to_download_all_uci_datasets/,thewhitetulip,1460818206,,8,13,False,http://b.thumbs.redditmedia.com/a_loIVLxru8wR0dIv6kBCnqHqJXj2gyokNwGVGzmLKQ.jpg,,,,,
494,MachineLearning,t5_2r3gv,2016-4-16,2016,4,16,23,4f26la,self.MachineLearning,"How much does GPA matter for top PhD programs? This one ""manual"" made by a CMU professor says you only need at least a 3.5.",https://www.reddit.com/r/MachineLearning/comments/4f26la/how_much_does_gpa_matter_for_top_phd_programs/,imdirtysocks45,1460818261,"I was reading [this](https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf) which gave me hope because although right now I have a 3.77, after a bad semester, it could drop to a 3.60. I want to go to like Stanford, Berkeley or MIT. I'm currently at Georgia Tech, and he says they're more lenient on GPA for top CS programs. I'm a rising junior, so I'll basically be doing research for two years hopefully with a well known professor. If I can impress him maybe I can get in based on that alone with good publishing. But is a 3.5+ good enough? I'll obviously have a chance to bring it up. I may also do an MS with a research thesis in between if I have to. 

EDIT: Although I haven't yet, I'll take some grad classes throughout my bachelor's. ",11,0,False,self,,,,,
495,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,0,4f29lv,oneweirdkerneltrick.com,Visually Identifying Rank,https://www.reddit.com/r/MachineLearning/comments/4f29lv/visually_identifying_rank/,galapag0,1460819552,,7,42,False,default,,,,,
496,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,0,4f2dqr,nature.com,Deep Learning in Label-free Cell Classification,https://www.reddit.com/r/MachineLearning/comments/4f2dqr/deep_learning_in_labelfree_cell_classification/,[deleted],1460821300,[deleted],0,0,False,default,,,,,
497,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,1,4f2lnj,sciencenewsjournal.com,Scientists developed a microscope that uses AI in order to locate cancer cells more efficiently. The device uses photonic time stretch and deep learning to analyze 36 million images every second without damaging the blood samples (x-post from /r/science),https://www.reddit.com/r/MachineLearning/comments/4f2lnj/scientists_developed_a_microscope_that_uses_ai_in/,Lost4468,1460824448,,2,13,False,http://b.thumbs.redditmedia.com/_qe9M_ALzp0KLFfLAUDn1YAzbzQ6NDo0iaV2X9rVVxI.jpg,,,,,
498,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,2,4f2qp0,self.MachineLearning,State of the art of modeling physics with ML?,https://www.reddit.com/r/MachineLearning/comments/4f2qp0/state_of_the_art_of_modeling_physics_with_ml/,rasputin48,1460826438,Are there any examples of ML based techniques being competitive with modeling the underlying physics of systems (say fluids for example)?,14,5,False,self,,,,,
499,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,2,4f2r9v,ianozsvald.com,Allergic Rhinitis Research Project Using Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4f2r9v/allergic_rhinitis_research_project_using_machine/,putd,1460826679,,0,1,False,default,,,,,
500,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,2,4f2xma,self.MachineLearning,3 day Machine Learning &amp; Data Science Competition for Community Members - Invitation,https://www.reddit.com/r/MachineLearning/comments/4f2xma/3_day_machine_learning_data_science_competition/,manish_saraswat,1460829268,[removed],0,1,False,default,,,,,
501,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,3,4f37bs,self.MachineLearning,learning rate schedule and regularization for embedding layer?,https://www.reddit.com/r/MachineLearning/comments/4f37bs/learning_rate_schedule_and_regularization_for/,yield22,1460833179,"with SGD training, every mini-batch the embedding matrix will only get partial gradient (for those used words), hence most word embeddings got update every several iterations. This is very different from other parameters like conv filters that got updated much more frequent, e.g. every iteration.

There are two questions:
(1) is it better to use different learning rate schedule for embedding layer that is different from other frequently-updated layers?
(2) is it better to regularize over only used words every mini-batch, or is it better to regularize over the whole embedding matrix every mini-batch?",1,3,False,self,,,,,
502,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,4,4f39w9,cloudburst.indico.io,"Introducing Cloudburst, a dataset sharing hub. We need your ideas, your criticism, and your data to improve its usability and relevance to the ML community.",https://www.reddit.com/r/MachineLearning/comments/4f39w9/introducing_cloudburst_a_dataset_sharing_hub_we/,Cloudburst-Scope,1460834203,,1,4,False,default,,,,,
503,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,5,4f3ncy,self.MachineLearning,Skflow/Tensorflow confusion,https://www.reddit.com/r/MachineLearning/comments/4f3ncy/skflowtensorflow_confusion/,haskkk,1460839650,"There is a minimal example of an RNN in the Skflow documentation. The input data is a matrix with shape (4,5). Why is the data split according to the following function for input?: 

    def input_fn(X):
        return tf.split(1, 5, X)

This function returns a list of 5 arrays with shape 4,1 

    [array([[ 2.],
           [ 2.],
           [ 3.],
           [ 2.]], dtype=float32), array([[ 1.],
           [ 2.],
           [ 3.],
           [ 4.]], dtype=float32), array([[ 2.],
           [ 3.],
           [ 1.],
           [ 5.]], dtype=float32), array([[ 2.],
           [ 4.],
           [ 2.],
           [ 4.]], dtype=float32), array([[ 3.],
           [ 5.],
           [ 1.],
           [ 1.]], dtype=f


and, what is the difference/impact on the RNN between the above function, or defining the function like this? As both input functions run

    def input_fn(X):
        return tf.split(1, 1, X)

Which returns the following: 

    [[[ 1.,  3.,  3.,  2.,  1.],
            [ 2.,  3.,  4.,  5.,  6.]]

Presented here: 

    testRNN(self):
            random.seed(42)
            import numpy as np
            data = np.array(list([[2, 1, 2, 2, 3],
                                  [2, 2, 3, 4, 5],
                                  [3, 3, 1, 2, 1],
                                  [2, 4, 5, 4, 1]]), dtype=np.float32)
            # labels for classification
            labels = np.array(list([1, 0, 1, 0]), dtype=np.float32)
            # targets for regression
            targets = np.array(list([10, 16, 10, 16]), dtype=np.float32)
            test_data = np.array(list([[1, 3, 3, 2, 1], [2, 3, 4, 5, 6]]))
            def input_fn(X):
                return tf.split(1, 5, X)
    
            # Classification
            classifier = skflow.TensorFlowRNNClassifier(
                rnn_size=2, cell_type='lstm', n_classes=2, input_op_fn=input_fn)
            classifier.fit(data, labels)
            classifier.weights_
            classifier.bias_
            predictions = classifier.predict(test_data)
            self.assertAllClose(predictions, np.array([1, 0]))",1,0,False,self,,,,,
504,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,6,4f3pvr,self.MachineLearning,Caffe Installation Question,https://www.reddit.com/r/MachineLearning/comments/4f3pvr/caffe_installation_question/,grim-grime,1460840730,"I'm running into the following issue when installing Caffe.

I followed all the rules in [this](http://caffe.berkeleyvision.org/install_osx.html) guide (downloading all the dependencies with brew), installed Nvidia drivers/toolkit, downloaded the latest caffe release, and tried to run make, but I get an error message:

    $ make all -j8    

    LD -o .build_release/lib/libcaffe.so.1.0.0-rc3    

    clang: warning: argument unused during compilation: '-pthread'    

    Undefined symbols for architecture x86_64:    

      ""cv::String::deallocate()"", referenced from:    

          caffe::WindowDataLayer&lt;float&gt;::load_batch(caffe::Batch&lt;float&gt;*) in window_data_layer.o    

          caffe::WindowDataLayer&lt;double&gt;::load_batch(caffe::Batch&lt;double&gt;*) in window_data_layer.o    

          caffe::ReadImageToCVMat(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int, bool) in io.o    

          caffe::ReadImageToDatum(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int, int, bool, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, caffe::Datum*) in io.o    

      ""cv::String::allocate(unsigned long)"", referenced from:    

          caffe::WindowDataLayer&lt;float&gt;::load_batch(caffe::Batch&lt;float&gt;*) in window_data_layer.o    

          caffe::WindowDataLayer&lt;double&gt;::load_batch(caffe::Batch&lt;double&gt;*) in window_data_layer.o    

          caffe::ReadImageToCVMat(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int, bool) in io.o    

          caffe::ReadImageToDatum(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, int, int, int, bool, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, caffe::Datum*) in io.o    

    ld: symbol(s) not found for architecture x86_64    

    clang: error: linker command failed with exit code 1 (use -v to see invocation)    

    make: *** [.build_release/lib/libcaffe.so.1.0.0-rc3] Error 1


Any ideas what might be causing this?",9,0,False,self,,,,,
505,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,7,4f3xvc,self.MachineLearning,Want to start a startup that automatically MOOCifies Textbooks with help of AR/VR. Need feasability reviews and Suggestions for next step..,https://www.reddit.com/r/MachineLearning/comments/4f3xvc/want_to_start_a_startup_that_automatically/,Kal-Ky,1460844225,[Deleted],8,0,False,self,,,,,
506,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,9,4f4h9d,self.MachineLearning,Deep Learning Classics,https://www.reddit.com/r/MachineLearning/comments/4f4h9d/deep_learning_classics/,[deleted],1460853038,[removed],0,1,False,default,,,,,
507,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,13,4f588m,self.MachineLearning,What is Multitask elastic ?,https://www.reddit.com/r/MachineLearning/comments/4f588m/what_is_multitask_elastic/,Vainsingr,1460867008,"I have seen the sklearn function for multitask elastic net is sklearn.linear_model.MultiTaskElasticNet. 

What is difference between this and the normal elastic net. I have googled and dint not find any on multitaskElastic net",4,0,False,self,,,,,
508,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,13,4f5898,download.tensorflow.org,TensorFlow: Large-Scale Machine Learning on Distributed Systems (2015) [pdf],https://www.reddit.com/r/MachineLearning/comments/4f5898/tensorflow_largescale_machine_learning_on/,iamkeyur,1460867021,,1,0,False,default,,,,,
509,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,14,4f5gpn,thenextweb.com,Now you can power any app with Google's speech recognition s,https://www.reddit.com/r/MachineLearning/comments/4f5gpn/now_you_can_power_any_app_with_googles_speech/,jo_kruger,1460872090,,24,61,False,http://b.thumbs.redditmedia.com/IyhaYLLaVnd_UEDQl7YDBFESrlDAFi8t7bPw-pSlmpo.jpg,,,,,
510,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,21,4f6bum,youtu.be,Mobileye: End-end DNN not possible for self driving cars,https://www.reddit.com/r/MachineLearning/comments/4f6bum/mobileye_endend_dnn_not_possible_for_self_driving/,heltok,1460894458,,69,90,False,http://b.thumbs.redditmedia.com/TzMfXRcMNrFcXRbyDDGTjqW7WcbYcMJIEXPPAl73nvI.jpg,,,,,
511,MachineLearning,t5_2r3gv,2016-4-17,2016,4,17,21,4f6de7,twitch.tv,"Super Mario Bros, Neural Network with Genetic Algorithm [check comments for more info]",https://www.reddit.com/r/MachineLearning/comments/4f6de7/super_mario_bros_neural_network_with_genetic/,Givemeallyourcats,1460895432,,16,7,False,http://b.thumbs.redditmedia.com/W9eIOtb7xSAPQU77tCNUbhskxhypMK1Zr6nn_lPbN3s.jpg,,,,,
512,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,0,4f6wpz,kastnerkyle.github.io,Bad Speech Synthesis Made Simple,https://www.reddit.com/r/MachineLearning/comments/4f6wpz/bad_speech_synthesis_made_simple/,kkastner,1460905316,,3,30,False,http://b.thumbs.redditmedia.com/dXgoj0kZE42eKOOFr-35h6HfBxNAryERJ4itupD6MBo.jpg,,,,,
513,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,0,4f709b,self.MachineLearning,(Question) For computing output error in neural network,https://www.reddit.com/r/MachineLearning/comments/4f709b/question_for_computing_output_error_in_neural/,machine-learning,1460906829,[removed],3,0,False,default,,,,,
514,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,1,4f7ar0,self.MachineLearning,Installing scikit-learn doesn't work. Could anyone help?,https://www.reddit.com/r/MachineLearning/comments/4f7ar0/installing_scikitlearn_doesnt_work_could_anyone/,[deleted],1460911024,[deleted],6,1,False,default,,,,,
515,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,1,4f7bew,self.MachineLearning,Which of these math courses would be useful to take?,https://www.reddit.com/r/MachineLearning/comments/4f7bew/which_of_these_math_courses_would_be_useful_to/,ph3rn,1460911275,[removed],0,1,False,default,,,,,
516,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,2,4f7i9z,self.MachineLearning,Plot a Bayes posterior probability in 3D - Matlab,https://www.reddit.com/r/MachineLearning/comments/4f7i9z/plot_a_bayes_posterior_probability_in_3d_matlab/,[deleted],1460913865,[deleted],0,0,False,default,,,,,
517,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,3,4f7oue,self.MachineLearning,PDF for Introduction to Machine Learning in Python?,https://www.reddit.com/r/MachineLearning/comments/4f7oue/pdf_for_introduction_to_machine_learning_in_python/,o_rka,1460916283,"There's an early release version floating around somewhere that I could REALLY REALLY use.  The next edition isn't published until July and I need the content before that.  I have some other PDFs of similar stature if anyone cares to trade. 

http://www.amazon.com/Introduction-Machine-Learning-Python-Sarah/dp/1449369413
ISBN-10: 1449369413",2,0,False,self,,,,,
518,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,4,4f83o0,self.MachineLearning,"Using priors, weights or costs for mitigating class imbalance?",https://www.reddit.com/r/MachineLearning/comments/4f83o0/using_priors_weights_or_costs_for_mitigating/,Bohemian90,1460921845,"Hello

A plethora of Matlab classifiers (e.g. tree-based or svm) allow to set priors, costs or weights for the data points. This can help dealing with imbalanced data. Unfortunately, none does support setting weights for classes directly.

Which one (prior/costs/weighting data points) is best to use for an imbalanced dataset and what values (or range of value) should be used? For priors I could use 0.5 for each class. For weighting I could assign each data point a value of numSamples/(numClasses + numSamplesClass) where numSamplesClass is the number of samples of the corresponding class.",1,1,False,self,,,,,
519,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,5,4f88ev,self.MachineLearning,Tuning Parameters for Boosting/Bagging/Random Forest,https://www.reddit.com/r/MachineLearning/comments/4f88ev/tuning_parameters_for_boostingbaggingrandom_forest/,BlackHawk90,1460923753,"Hello

I want to use tree-based classifiers for my classifiaction problem. I'm thinking about bagging, boosting (AdaBoost, LogitBoost, RUSBoost) and Random Forest but I'm unsure about the tuning parameters, i.e. which range I should search.

I'm using the [TreeBagger](http://ch.mathworks.com/help/stats/treebagger.html) and [fitensemble](http://ch.mathworks.com/help/stats/fitensemble.html) method from Matlab. I'm unsure about the following parameters:

- Number of iterations / Trees
- Sampling with or without replacement? If without replacement what in bag fraction to take?
- Minimum Leaf Size
- Minimum Parent Size
- Maximum number of decision splits
- Learning rate for shrinkage
- RatioToSmallest (Every element of this vector is the sampling proportion for this class with respect to the class with fewest observations). I have highly imbalanced classes.
- MarginPrecision
- (The level of pruning and value of the pruning cost the tree should pruned to (alpha))

I would be very happy if somebody could give a quick help.",6,2,False,self,,,,,
520,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,8,4f8zuo,self.MachineLearning,Guided sampling of space outside of training data?,https://www.reddit.com/r/MachineLearning/comments/4f8zuo/guided_sampling_of_space_outside_of_training_data/,nnarehard,1460934540,"im working on a relatively simple supervised regression problem of approximately 30 properties onto a list of labeled data. However, the power of this network is entirely dependent on it making predictions near the edge of my data, or outside out it (all the data here is continuous numerical values). 

As such, I would like to iteratively make predictions that are outside my domain of applicability, make the actual measurement (all of my data is associated with physical experiments in a lab), and then add that data to my training data. Instead of making random guesses as to which measurements to make, is there any process to understanding what kind of space you should be exploring in order to maximize your return on your training data (to increase the domain of applicability)? ",2,0,False,self,,,,,
521,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,8,4f94sm,self.MachineLearning,Variable dimension data?,https://www.reddit.com/r/MachineLearning/comments/4f94sm/variable_dimension_data/,Icko_,1460936559,"In a problem im solving,  the data has ~400 dimensions, and I am to predict a very biased binary label. For most of the dimensions, &gt;90% of the training data is zeros, meaning there is no useful information. Usually, a user either has data either in most columns or in almost none. From this point of view the data is with variable dimensions. What are some models or ways to handle this?",5,1,False,self,,,,,
522,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,9,4f9dpj,self.MachineLearning,Looking for an updated AMI to run theano/keras on AWS.,https://www.reddit.com/r/MachineLearning/comments/4f9dpj/looking_for_an_updated_ami_to_run_theanokeras_on/,MasterEpictetus,1460940360,All versions I tried are somewhat outdated. Has anyone successfully tried a somewhat recent version? Thanks!,9,7,False,self,,,,,
523,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,10,4f9ney,youtube.com,Stanford Machine Learning Lecture 2,https://www.reddit.com/r/MachineLearning/comments/4f9ney/stanford_machine_learning_lecture_2/,gyeonggi,1460944626,,2,39,False,http://b.thumbs.redditmedia.com/4CRyrqbSsAGl0Xx0eXGc-50nKYDuO3rq_xpvIAVs9fM.jpg,,,,,
524,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,11,4f9q94,self.MachineLearning,A doubt with the derivation of back propagation equation shown in this page.,https://www.reddit.com/r/MachineLearning/comments/4f9q94/a_doubt_with_the_derivation_of_back_propagation/,z-petal,1460945860,"[This](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) is the page I am referring.

It says 

[deo1/dneto1 = deo1/douto1 * douto1/dneto1 = 0.74136507 * 0.186815602](https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+E_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+%5Cfrac%7B%5Cpartial+E_%7Bo1%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%2A+%5Cfrac%7B%5Cpartial+out_%7Bo1%7D%7D%7B%5Cpartial+net_%7Bo1%7D%7D+%3D+0.74136507+%2A+0.186815602+%3D+0.138498562&amp;bg=ffffff&amp;fg=404040&amp;s=0)

(search for 'using values we calculated earlier' to go to this location in page)

implying that deo1/dout1 = 0.74136507 because it is calculated during an earlier step, while calculating the output of the forward propagation step.

But in that earlier step, it says

[deTotal/dOutO1 = 0.74136507](https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+E_%7Btotal%7D%7D%7B%5Cpartial+out_%7Bo1%7D%7D+%3D+-%28target_%7Bo1%7D+-+out_%7Bo1%7D%29+%3D+-%280.01+-+0.75136507%29+%3D+0.74136507&amp;bg=ffffff&amp;fg=404040&amp;s=0)

(search for 'how much does the total error change with respect to the output' to go to this location in the page)

0.74136507 is the calculated value of the expression deTotal/douto1.

Is this correct? How is deTotal/douto1 == deo1/douto1 ?",3,3,False,self,,,,,
525,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,12,4fa3hz,christophvel.wordpress.com,"Hey guys, been creeping on this sub for a while, I decided to make a machine learning blog, check out on my first posts looking at predicting the Democratic race using Google Trends.",https://www.reddit.com/r/MachineLearning/comments/4fa3hz/hey_guys_been_creeping_on_this_sub_for_a_while_i/,christophvel,1460951843,,0,1,False,default,,,,,
526,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,15,4fai6n,youtu.be,I've been working on a self driving car that runs off a single webcam and a Macbook Air. Check out the simulated results!,https://www.reddit.com/r/MachineLearning/comments/4fai6n/ive_been_working_on_a_self_driving_car_that_runs/,Weihua99,1460959534,,64,147,False,http://b.thumbs.redditmedia.com/Xcj1Evc_PJpNAQlDXz2JqISVG4FX8mh8UV6-Vj_2kaA.jpg,,,,,
527,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,16,4farpc,youtube.com,Flooring laminating hot press machine,https://www.reddit.com/r/MachineLearning/comments/4farpc/flooring_laminating_hot_press_machine/,qiangtong-suzy,1460965138,,0,0,False,default,,,,,
528,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,17,4fav8a,self.MachineLearning,Do deep neural networks have more local minimums?,https://www.reddit.com/r/MachineLearning/comments/4fav8a/do_deep_neural_networks_have_more_local_minimums/,nate1421m,1460967329,"I have heard that training deep networks can be difficult due to local minima.
If you are training two neural networks with the same data. Where one of the networks is deeper (more hidden layers) than the other. Will the deeper network contain more local minima or is it impossible to say when only considering how deep the network is?
",25,8,False,self,,,,,
529,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,17,4favu2,youtube.com,Flooring production line China,https://www.reddit.com/r/MachineLearning/comments/4favu2/flooring_production_line_china/,qiangtong-suzy,1460967730,,0,0,False,default,,,,,
530,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,17,4fawdw,arxiv.org,"Precomputed Real-Time Texture Synthesis with ""Markovian"" Generative Adversarial Networks",https://www.reddit.com/r/MachineLearning/comments/4fawdw/precomputed_realtime_texture_synthesis_with/,alexjc,1460968132,,6,15,False,default,,,,,
531,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,17,4fawjd,self.MachineLearning,Question about learning with gradient descent,https://www.reddit.com/r/MachineLearning/comments/4fawjd/question_about_learning_with_gradient_descent/,bangingit,1460968217,"I'm having a hard time understanding gradient descent. Once one has computed all derivatives of the cost with respect to weights and biases / errors, what, exactly, does one do? 

How does one update the weights/biases? I can think of a simple way of doing it for a net with no hidden layer, but I have no idea how one would do it on one with more layers, since changing any weight or bias will change the value of the error of neurons in earlier and later layers - hence my question.

Anyone care to help me out?",5,3,False,self,,,,,
532,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,19,4fb641,self.MachineLearning,Tesorflow playgound,https://www.reddit.com/r/MachineLearning/comments/4fb641/tesorflow_playgound/,delijati,1460974347,[removed],2,0,False,default,,,,,
533,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,19,4fb9kh,aisociety.kr,Deconvolutions in Convolutional Neural Networks (slides),https://www.reddit.com/r/MachineLearning/comments/4fb9kh/deconvolutions_in_convolutional_neural_networks/,pmigdal,1460976414,,3,12,False,default,,,,,
534,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,20,4fbeiz,github.com,Logistic Regression from the Scratch,https://www.reddit.com/r/MachineLearning/comments/4fbeiz/logistic_regression_from_the_scratch/,upulbandara,1460979155,,1,0,False,http://b.thumbs.redditmedia.com/qQp5j0jva4ZRLxKoQwqRV6QhPoV11N0cjqe2ifGZxIo.jpg,,,,,
535,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,23,4fc5bl,self.MachineLearning,Will online poker die?,https://www.reddit.com/r/MachineLearning/comments/4fc5bl/will_online_poker_die/,maestron,1460990458,It doesn't seem entirely unreasonable to imagine that computers will become better than humans at 10-man no limit hold-em at some point in the not too distant future. Would this spell the end of online poker?  Certainly it would become impossible to be an online poker pro anymore? ,8,0,False,self,,,,,
536,MachineLearning,t5_2r3gv,2016-4-18,2016,4,18,23,4fc8kc,self.MachineLearning,Why is the HMDB51 dataset so hard ?,https://www.reddit.com/r/MachineLearning/comments/4fc8kc/why_is_the_hmdb51_dataset_so_hard/,illiterate_gorillas,1460991573,"This is the [HMDB 51 dataset](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/).

The dataset contains 6849 clips divided into 51 action categories, each containing a minimum of 101 clips.
Going through a few papers.
I had observed that this dataset is particularly hard to train and get a good prediction on. 

My question is why is this dataset so hard?(Action Recognition Task)
I have a few theories. Which I'm sure might not be true.

So please do let me know what you think. And your theories as well.

1) The train split consists of  3570 videos which is a small sample size to generalize actions.

2) The Video quality is worse in comparison to the [UCF101](http://crcv.ucf.edu/data/UCF101.php) dataset.

3)Few of the videos are too small and not useful for models which is successive motion to predict scores.


Papers:-
[1](http://arxiv.org/abs/1406.2199)
[2](https://hal.inria.fr/hal-01241518/file/VarolCVPR16.pdf)
[2](http://www.di.ens.fr/willow/pdfscurrent/kantorov14cvpr.pdf)
[4](http://www.cse.buffalo.edu/~jcorso/pubs/jcorso_CVPR2012_actionbank.pdf)",0,1,False,self,,,,,
537,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,0,4fca3x,self.MachineLearning,decouple mini-batch size and learning rate,https://www.reddit.com/r/MachineLearning/comments/4fca3x/decouple_minibatch_size_and_learning_rate/,yield22,1460992086,"In many cases, the optimal learning rates (or related parameters) are coupled with mini-batch size,  meaning when you change mini-batch size, you have to find a new set of learning rates (or related parameters, like learning rate schedule). 

Are there any ways to decouple them, so you can freely change mini-batch size while not affecting performance? or if not, in practice, what do people do? first tune mini-batch size? just random search all?",6,0,False,self,,,,,
538,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,0,4fcc52,self.MachineLearning,What is the difference between a single-layer restricted boltzmann machine and PCA or SVD?,https://www.reddit.com/r/MachineLearning/comments/4fcc52/what_is_the_difference_between_a_singlelayer/,sanity,1460992768,"They both find a way to encode higher dimensionality data in lower dimensions - is a RBM somehow more powerful in how it does this?

Could you stack PCA or SVD to obtain a more powerful dimensionality reduction?",6,1,False,self,,,,,
539,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,0,4fcebx,youtube.com,Microscope uses Artificial Intelligence to find cancer cells more efficiently,https://www.reddit.com/r/MachineLearning/comments/4fcebx/microscope_uses_artificial_intelligence_to_find/,themoosemind,1460993504,,4,5,False,http://b.thumbs.redditmedia.com/cCHhFReAr59BRLpE70wNuJgFs1zb0PfjnVbiXDjAiOo.jpg,,,,,
540,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,0,4fcfce,control.kylemcdonald.net,"Images from ArXiv CS.* papers, updated daily",https://www.reddit.com/r/MachineLearning/comments/4fcfce/images_from_arxiv_cs_papers_updated_daily/,kcimc,1460993834,,15,24,False,default,,,,,
541,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,0,4fcinu,youtube.com,A Neural Network Groups Movie Scenes By Setting,https://www.reddit.com/r/MachineLearning/comments/4fcinu/a_neural_network_groups_movie_scenes_by_setting/,icandoitbetter,1460994924,,1,15,False,http://b.thumbs.redditmedia.com/S90pzQJsREQ6W9E25g02fr41XjVEB8Xv1cGU0WxVMBQ.jpg,,,,,
542,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,1,4fctfk,spectrum.ieee.org,"Ban or No Ban, Hard Questions Remain on Autonomous Weapons",https://www.reddit.com/r/MachineLearning/comments/4fctfk/ban_or_no_ban_hard_questions_remain_on_autonomous/,dendisuhubdy,1460998565,,0,1,False,default,,,,,
543,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,1,4fctvl,featureselection.asu.edu,scikit-feature: an open-source feature selection repository,https://www.reddit.com/r/MachineLearning/comments/4fctvl/scikitfeature_an_opensource_feature_selection/,galapag0,1460998722,,0,6,False,default,,,,,
544,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,2,4fcvzd,scs.ryerson.ca,An Interactive Node-Link Visualization of Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4fcvzd/an_interactive_nodelink_visualization_of/,charlie0_o,1460999426,,4,8,False,http://b.thumbs.redditmedia.com/TdacN-6yF4V-alQdMOwO3h5PEnepO3F5o9eWyDU8Trk.jpg,,,,,
545,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,2,4fcxag,self.MachineLearning,Using other people's result images in your paper,https://www.reddit.com/r/MachineLearning/comments/4fcxag/using_other_peoples_result_images_in_your_paper/,alexmlamb,1460999864,"Hello, 

If you're writing a paper (say on generative models) and you want to show a head-to-head comparison of your method with another method, what are the community standards around showing images/samples from both models in your paper?  

If you lifted the images directly from their paper, do you need to ask permission?  Or is it enough just to cite them?  ",8,3,False,self,,,,,
546,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,2,4fd47h,people.csail.mit.edu,MIT uses 3 unsupervised-learning methods to detect cyber-attacks in real-time,https://www.reddit.com/r/MachineLearning/comments/4fd47h/mit_uses_3_unsupervisedlearning_methods_to_detect/,amc22004,1461002205,,9,151,False,default,,,,,
547,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,2,4fd4e9,futhark-lang.org,The Futhark Programming Language : High-performance purely functional data-parallel array programming on the GPU,https://www.reddit.com/r/MachineLearning/comments/4fd4e9/the_futhark_programming_language_highperformance/,muktabh,1461002276,,4,44,False,default,,,,,
548,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,3,4fdab8,self.MachineLearning,Figaro vs PyMC,https://www.reddit.com/r/MachineLearning/comments/4fdab8/figaro_vs_pymc/,rightname,1461004247,"I am trying to decide between Figaro &amp; PyMC to learn Probabilistic Programming. I have used both python and Java extensively, so ease of use is not an issue. 

I would like to get an understanding of what are the major differences between the two libraries, based on features, exhaustiveness and so on. ",2,4,False,self,,,,,
549,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,3,4fdaw1,self.MachineLearning,UberPool and Lyft Line Matching models,https://www.reddit.com/r/MachineLearning/comments/4fdaw1/uberpool_and_lyft_line_matching_models/,Tylermd1,1461004435,"Anyone have an idea how UberPool and Lyft Line's ride share matching system works/ is being modelled?

If you are not familiar with UberPool or Lyft Line here's a quick overview: https://www.quora.com/How-do-the-UberPOOL-and-Lyft-Line-matching-algorithms-compare",1,3,False,self,,,,,
550,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,3,4fdc53,self.MachineLearning,"Novice with a ""wikipedia"" understanding of ML, how long will it take to change that?",https://www.reddit.com/r/MachineLearning/comments/4fdc53/novice_with_a_wikipedia_understanding_of_ml_how/,ronbbot,1461004840,"How long would it take to go from just knowing linear algebra and python to being able to develop something that can for example tell me how huggable something is? What I really want to know is would it be practical to learn all of this in three months?
What I'm referencing -&gt; http://imgur.com/a/T1QNL ",3,0,False,self,,,,,
551,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,3,4fdcax,ft.com,"AI hedge fund, Numerai raises $1.5m from cofounder of Renaissance Technologies",https://www.reddit.com/r/MachineLearning/comments/4fdcax/ai_hedge_fund_numerai_raises_15m_from_cofounder/,dsernst,1461004894,,6,26,False,http://b.thumbs.redditmedia.com/_sJpEBZVwiWaZxGMBq04e89ek_OZ9hcMwmsq7KXO-rM.jpg,,,,,
552,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,3,4fdf9y,youtube.com,Sentiment Analysis in 4 Minutes,https://www.reddit.com/r/MachineLearning/comments/4fdf9y/sentiment_analysis_in_4_minutes/,llSourcell,1461005889,,0,5,False,http://b.thumbs.redditmedia.com/RHytQV1Xv8hlUZVSs8363Fly4uYDSf4AK3FuWgFaTxQ.jpg,,,,,
553,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,4,4fdi9h,self.MachineLearning,Modern LSTM Architectures?,https://www.reddit.com/r/MachineLearning/comments/4fdi9h/modern_lstm_architectures/,hazard02,1461006879,"Is there a paper that describes modern LSTM architectures, in the same way that the GoogLeNet paper is representative of modern CNNs?",6,0,False,self,,,,,
554,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,4,4fdm50,self.MachineLearning,Demographics of r/machinelearning,https://www.reddit.com/r/MachineLearning/comments/4fdm50/demographics_of_rmachinelearning/,[deleted],1461008158,[deleted],16,3,False,default,,,,,
555,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,4,4fdoog,x.ai,An AI assistant that schedules meetings for you,https://www.reddit.com/r/MachineLearning/comments/4fdoog/an_ai_assistant_that_schedules_meetings_for_you/,dgarbvt,1461008983,,0,1,False,default,,,,,
556,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,5,4fdtkf,self.MachineLearning,Deduplication/Entity resolution business evaluation,https://www.reddit.com/r/MachineLearning/comments/4fdtkf/deduplicationentity_resolution_business_evaluation/,faizanj,1461010597,[removed],1,1,False,default,,,,,
557,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,6,4fe5ho,self.MachineLearning,attempting to get a dataset into AWS ML results in failure,https://www.reddit.com/r/MachineLearning/comments/4fe5ho/attempting_to_get_a_dataset_into_aws_ml_results/,frankilla44,1461014719,"I keep on getting errors that there are empty values for my some of my data.  I know the data I'm putting in isn't really that clean yet, I was hoping to do that in the next step.  Do I need to thoroughly clean my data first to use amazon ML? I use Azure ML studio and it doesn't reject the data..",1,0,False,self,,,,,
558,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,6,4fe5l1,self.MachineLearning,Can someone explain how this would work? Using probabilistic outputs of one classifier as feature vector of another,https://www.reddit.com/r/MachineLearning/comments/4fe5l1/can_someone_explain_how_this_would_work_using/,subyng,1461014745,"This is from a paper titled ""Automatic segmentation of the preterm neonatal brain with MRI using supervised classication"". They are doing image segmentation. After using kNN classifier on the data (binary classification), they obtain probabilities. They take the pixels with probabilities between 0.2 and 0.8 for a second stage of classification. The excerpt of interest from the paper itself reads:  

""Visual inspection of posterior probabilities, generated by the three classiers employed in the rst stage, suggested that these can provide useful information. Therefore, an additional set of features, obtained by ltering the probabilistic segmentation of each tissue that was generated by the rst classication stage, was constructed""

What I don't understand is, if they create a new set of features from the output of the testing data, how do they get the same features from the training data, which would be necessary for training the new classifier?


",9,0,False,self,,,,,
559,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,6,4fe6qa,arxiv.org,[1604.04326] Improving the Robustness of Deep Neural Networks via Stability Training,https://www.reddit.com/r/MachineLearning/comments/4fe6qa/160404326_improving_the_robustness_of_deep_neural/,x2342,1461015156,,5,22,False,default,,,,,
560,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,6,4fearz,self.MachineLearning,Does anyone know why the Paulus/Hornegger book: Applied Pattern Recognition is so expensive?,https://www.reddit.com/r/MachineLearning/comments/4fearz/does_anyone_know_why_the_paulushornegger_book/,HoneyVortex,1461016601,"I actually have [a copy](http://i.sli.mg/you9bz.jpg) of the second edition. Looking through it, I'm not too pleased with it's implementation examples of C++ which are very C-like. (At least in the beginning of the book and the book assumes that it is teaching C++ to people with a background in C.) What I do like about it is that it doesn't try to dumb anything down and it includes the mathematics.

Looking on Amazon it's gone down in price in the last 6 years but at one point people were selling used copies for $700.

http://www.amazon.com/Applied-Pattern-Recognition-Introduction-Processing/dp/3528155582?ie=UTF8&amp;psc=1&amp;redirect=true&amp;ref_=oh_aui_detailpage_o00_s00

Now, these used books are going for 33 bucks but the newer edition (4th Edition) is selling for 200 dollars:


http://www.amazon.com/Applied-Pattern-Recognition-Fourth-Edition/dp/3528355581?ie=UTF8&amp;keywords=applied%20pattern%20recognition%20paulus&amp;qid=1461013315&amp;ref_=sr_1_2&amp;sr=8-2

Does anyone have an opinion on this book? Should I get the newer edition, does it have more information in it?

Why is the demand so high and why haven't they published an updated manual recently?

Here's some selected pages from the chapter on ""Hierarchy of Picture Processing Objects""

https://sli.mg/a/evRjaV

Enjoy.






",2,0,False,self,,,,,
561,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,7,4fedom,self.MachineLearning,metaphor detection tool,https://www.reddit.com/r/MachineLearning/comments/4fedom/metaphor_detection_tool/,[deleted],1461017668,[deleted],0,0,False,default,,,,,
562,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,8,4fes8c,self.MachineLearning,ELI5: Why is higher dimensionality bad in data analytics?,https://www.reddit.com/r/MachineLearning/comments/4fes8c/eli5_why_is_higher_dimensionality_bad_in_data/,phenkdo02139,1461023215,"My understanding of dimensionality is that they capture different aspects of a study subject (e.g. age, gender, weight, height etc.). But wouldn't that be an asset by performing an n-way comparison of dimensions to learn which dimension(s) differ? Why is dimensionality reduction a thing?",4,0,False,self,,,,,
563,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,13,4ffwl5,self.MachineLearning,"90% of stock trades are through algorithmic trading, most of which is machine learning...",https://www.reddit.com/r/MachineLearning/comments/4ffwl5/90_of_stock_trades_are_through_algorithmic/,magnora7,1461039175,"and if machine learning algorithms are to perform well, they must use training data. So if one were to train a neural net or some algorithm to trade the stock market and make money, one would train it with existing stock market data.

After training, they lock in the weights, and that's the neural net that is used as an input-output machine and given money to play in the market. How many different machine learning algorithms do you think are actually used, that actually have enough money to sway the market? There are likely some bots that are essentially throwing around billions of dollars an hour for the largest algorithmic investing firms, and these bots are precisely trained and tested against real-world scenarios. 

The bots learn from the past. So the more the stock market is traded with algorithms ([which has been the trend](http://www.zerohedge.com/contributed/2012-17-26/84-all-stock-trades-are-high-frequency-computers-%E2%80%A6-only-16-are-done-human-tra)) the more the future will look like the past, but averaged out across those few dozen or hundred different algorithm designs that essentially drive the stock market today. 

I'm sorry if this is the wrong place to post this, but I think this sub is the only place that can really appreciate what I'm getting at.

TL;DR: The more the market is traded with algorithms that are trained from the past behavior of the S&amp;P 500, the more the future market behavior will begin to look like an averaged-out version of the past. 

[Reference chart of the S&amp;P 500, note the 3 humps of around 2000, 2008, and today](https://www.google.com/finance?q=INDEXSP%3A.INX&amp;ei=q6oVV8CFE4O2mAGz8qjYAQ)",8,0,False,self,,,,,
564,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,15,4fgdrf,self.MachineLearning,Linear Regression with 50+ inputs.,https://www.reddit.com/r/MachineLearning/comments/4fgdrf/linear_regression_with_50_inputs/,NotTheSoberRussian,1461048533,"    model = Sequential()
    model.add(Dense(32,input_dim=54, activation='linear'))
    model.add(Dense(32,activation='linear'))
    model.add(Dense(1,activation='linear'))
    
    model.compile(loss='mse', optimizer='rmsprop')
    
    model.fit(x_train, y_train, nb_epoch=1500, batch_size=16,verbose=0)
    model.fit(x_train, y_train, nb_epoch=1, batch_size=16,verbose=1)
    score = model.evaluate(x_test, y_test, batch_size=48, show_accuracy=True)


I'm using Keras+Tensorflow and I'm trying to get make essentially a linear regression model, and I'm trying to find ways to improve it. I have the output so far, and my loss is crazy ridiculous

    Epoch 1/1
    449/449 [==============================] - 0s - loss: 81.6287     
    99/99 [==============================] - 0s     
    ('Test score: ', [93.47659532951586, 1.0])

Here's a sample input (18 x,y,z coordinates per input)
            
    55.469984,53.04649,51.612565,52.552165,55.413436,55.664798,54.798754,52.541975,52.390782,54.629238,53.602161,52.098404,55.608484,54.109282,51.912223,55.88926,52.312918,52.334785,55.193273,51.972516,51.518203,57.428772,50.256732,55.959344,51.557814,51.316556,56.507795,52.36109,56.439443,55.253294,52.987678,55.717568,54.676167,54.3663,55.717837,55.871671,55.537976,53.31533,52.696573,54.419804,52.776422,51.367988,53.374243,56.169188,55.633965,54.078762,55.914484,54.805297,51.095756,50.514384,57.142574,53.582711,55.090078,55.383082
            
Here's the sample output

    -36.2458094817

Am I going about this the right way? I think my loss is too high and my accuracy shouldn't be 1?",7,0,False,self,,,,,
565,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,16,4fgk8k,arxiv.org,"[1604.03640v1] Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex",https://www.reddit.com/r/MachineLearning/comments/4fgk8k/160403640v1_bridging_the_gaps_between_residual/,x2342,1461052496,,44,43,False,default,,,,,
566,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,17,4fgnns,self.MachineLearning,Beginner-&gt;genetic algo tetris confusion?,https://www.reddit.com/r/MachineLearning/comments/4fgnns/beginnergenetic_algo_tetris_confusion/,maximus12793,1461054703,"https://codemyroad.wordpress.com/2013/04/14/tetris-ai-the-near-perfect-player/
I have read this and have made the standard hello world GA but am confused about how to actually generate guesses or the gene pool? In the case of a game it seems inefficient to have to evaluate after losing a game so I get that you want to check the fitness after every piece moves. However I am not sure what should be a generation or population etc. in a game situation?

In the case of hello world it was simple in terms of how to breed... half string A half string B, but am not sure what to do with tetris (1 arbitrary move vs another?) and then reproduce from here? In the end it seems like the GA would only be very good for that single run-through of the game (the one that it eventually reached out of thousands of iterations and finally hit one that by chance worked well) and hasn't really become good at tetris at all, or am I missing something? Also I see on alot of the more complicated GA applications people train the model with a simple goal first and then progressively make it more challenging, any idea how I would save the model in between iterations? (I dont mean ""write the model to a hdf5 or json, but what exactly would I be saving, attribute wise?)

**tldr;**

- How to breed (population/individual) given the game state?

- Does GA actually make our program good outside of that single run? 

- What is the best way to save the model if I wanted to train in steps? (ie 1 line then 10 lines etc.)? ",5,0,False,self,,,,,
567,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,18,4fgqac,self.MachineLearning,Stock trading with web text analisys?,https://www.reddit.com/r/MachineLearning/comments/4fgqac/stock_trading_with_web_text_analisys/,hapliniste,1461056438,"Trading with ML is hard and tend to not work well for long period of time. Is it plausible to add a network similar to sentiment analysis that go around the web and read texts linked to stock names (like, for GOOG, search for the terms ""Google"",...) and use it as features for the prediction model?

I think predicting stock based on previous data AND sentiment analysis could give interesting results?

Like if Google announce something big, it will pop up on the media fast, generating tons of reactions (good or bad). Maybe we could use it to predict how the stock will be affected by this announcement?

Do you know if someone tried it? ",6,2,False,self,,,,,
568,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,19,4fgwx6,self.MachineLearning,Machine learning approach to decrypting cetacean language,https://www.reddit.com/r/MachineLearning/comments/4fgwx6/machine_learning_approach_to_decrypting_cetacean/,gabriel1983,1461060727,How would one proceed?,7,2,False,self,,,,,
569,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,21,4fhb6q,cloudacademy.com,Google Vision API: Image Analysis as a Service,https://www.reddit.com/r/MachineLearning/comments/4fhb6q/google_vision_api_image_analysis_as_a_service/,[deleted],1461068648,[deleted],0,0,False,default,,,,,
570,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,22,4fhg7s,self.MachineLearning,Building a Recommendation Engine,https://www.reddit.com/r/MachineLearning/comments/4fhg7s/building_a_recommendation_engine/,hoodsy,1461070913,"I'm just beginning to learn about ML, and have been working through Andrew Ng's coursera course to establish some fundamentals. 

I'm trying to learn the basics of ML to create a recommendation system for a product I am building - but it's hard to know where to start. I know that I will need a recommendation engine, but what steps should I take to be able to create something like this?",23,9,False,self,,,,,
571,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,22,4fhjxv,self.MachineLearning,"Denoising auto encoders, using the mean reconstruction error to determine model capacity",https://www.reddit.com/r/MachineLearning/comments/4fhjxv/denoising_auto_encoders_using_the_mean/,DanielSlater8,1461072412,"On this page, it mentions a few advantages of denoising autoencoders vs ordinary autoencoders:
http://www.iro.umontreal.ca/~bengioy/ift6266/H12/html/dae_en.html

In particular using mean reconstruction error to determine capacity and classification error after fine tuning for choosing the amount of corruption. Does anyone know of a good source for more detail on these 2 issues(or want to share their knowledge here). I tried reading the linked to papers but couldn't find anything on those 2 subjects.
Thanks in advance.",2,5,False,self,,,,,
572,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,22,4fhl3v,gist.github.com,"Notes for ""Learning To Execute"" paper",https://www.reddit.com/r/MachineLearning/comments/4fhl3v/notes_for_learning_to_execute_paper/,[deleted],1461072881,[deleted],1,0,False,default,,,,,
573,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,23,4fhpcy,arxiv.org,Towards Vulnerability Discovery Using Staged Program Analysis,https://www.reddit.com/r/MachineLearning/comments/4fhpcy/towards_vulnerability_discovery_using_staged/,galapag0,1461074506,,0,1,False,default,,,,,
574,MachineLearning,t5_2r3gv,2016-4-19,2016,4,19,23,4fhypg,andreykurenkov.com,A 'Brief' History of Game AI Up To AlphaGo,https://www.reddit.com/r/MachineLearning/comments/4fhypg/a_brief_history_of_game_ai_up_to_alphago/,regalalgorithm,1461077854,,11,24,False,default,,,,,
575,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,0,4fi290,self.MachineLearning,keras code to theano,https://www.reddit.com/r/MachineLearning/comments/4fi290/keras_code_to_theano/,[deleted],1461079060,[deleted],7,0,False,default,,,,,
576,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,0,4fi47w,self.MachineLearning,How to choose an unsupervised classification algo? Food macronutrients dataset,https://www.reddit.com/r/MachineLearning/comments/4fi47w/how_to_choose_an_unsupervised_classification_algo/,thiskevin,1461079732,"I have a dataset with macronutrient information for ~8000 foods. For each food I have the number of grams of protein, carbs, fat that are contained in 100g of that food.

When I plot the data, I can identify a few clusters visually: http://imgur.com/Cl1koTd

How do I go about choosing a unsupervised classification algo for this problem?

Using the DBScan algorithm I was able to get this result: http://imgur.com/1e7ALyy

Is there a diff algo that would allow me to do better?",8,1,False,self,,,,,
577,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,0,4fi4b7,self.MachineLearning,global optimization library,https://www.reddit.com/r/MachineLearning/comments/4fi4b7/global_optimization_library/,EINHmusic,1461079761,"Premise: I'm a math and ML noob. Any term you throw at me, I'll have to google it, just to give you an idea of my background. This might not be the best place to ask.

This is the scenario (it comes from a pretty dumb experiment of which I'd prefer not to give details of): I have a function, really noisy, non linear, weird, full of trivial and non helpful local minimums. It takes as input a 100 to 1000 (depending on the particular experiment) dimensional vector of both continuous and discrete variables. Evaluation is costly: I can evaluate the function a couple of to a dozen times per second depending on what computer I'm running on. The function is non differentiable (it  actually involves calling external programs to get the result). I can't easily implement it on the GPU, but I can use the cores I have available in parallel.

I'm looking for a pointer towards a library that allows me to do global optimization on this kind of function. Am I looking for something like https://github.com/fmfn/BayesianOptimization ?",12,2,False,self,,,,,
578,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,0,4fi7h4,self.MachineLearning,why weight between layers are summed in infinite sigmoid belief network,https://www.reddit.com/r/MachineLearning/comments/4fi7h4/why_weight_between_layers_are_summed_in_infinite/,John_Smith111,1461080812,"hello all 

 i read paper of Geoff Hinton on Deep Belif Nets - i would like to ask why he sum the wight between different layers in infinite belief net with tights weights when computes the d log(p)/dw 

https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf

He expose that: 

"" Since the weights are replicated, the full derivative for a generative
weight is obtained by summing the derivative of the generative weight between all pairs of the layers ""

i cant follow why the replicated wights lead to to summing the derivatives between layers ?

10x all


",5,0,False,self,,,,,
579,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,1,4fia4o,self.MachineLearning,How do I go about building a NN model to detect texture/pattern in clothing images?,https://www.reddit.com/r/MachineLearning/comments/4fia4o/how_do_i_go_about_building_a_nn_model_to_detect/,n00bto1337,1461081708,"The image data set is of people wearing clothes. What sort of pre processing do I have to do before feeding it into any model? Also, is it possible to use Tensorflow's Inception retraining model to achieve this?",3,0,False,self,,,,,
580,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,1,4fic0i,nextplatform.com,Cray Retools Storm Supercomputers for Deep Learning on the (Relative) Cheap,https://www.reddit.com/r/MachineLearning/comments/4fic0i/cray_retools_storm_supercomputers_for_deep/,[deleted],1461082336,[deleted],0,7,False,default,,,,,
581,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,1,4fihpp,quora.com,John Langford is doing a Quora session today,https://www.reddit.com/r/MachineLearning/comments/4fihpp/john_langford_is_doing_a_quora_session_today/,vodkagoodmeatrotten,1461084241,,2,4,False,default,,,,,
582,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,2,4fiqqn,self.MachineLearning,Has any ML researcher released Clojure code to accompany a published paper? Ever?,https://www.reddit.com/r/MachineLearning/comments/4fiqqn/has_any_ml_researcher_released_clojure_code_to/,[deleted],1461087359,[deleted],0,0,False,default,,,,,
583,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,2,4fisfw,shortscience.org,I made this site called Short Science to make research more accessible! Researchers can publish and read paper summaries that are voted on and ranked! The goal is making research more accessible by cutting through the jargon and added complication in many papers so you can understand the point! [OC],https://www.reddit.com/r/MachineLearning/comments/4fisfw/i_made_this_site_called_short_science_to_make/,ieee8023,1461087936,,28,216,False,http://b.thumbs.redditmedia.com/NlDGhez24oyJc-mYcKaYTMwbiu7ZrpY8-0UlSerVDpI.jpg,,,,,
584,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,2,4fit2p,captchasolutions.com,OCR Captcha Solver,https://www.reddit.com/r/MachineLearning/comments/4fit2p/ocr_captcha_solver/,articlefr,1461088154,,3,0,False,default,,,,,
585,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,2,4fiug0,aigameresearch.org,"Created this several years ago, thought /r/MachineLearning might enjoy it.",https://www.reddit.com/r/MachineLearning/comments/4fiug0/created_this_several_years_ago_thought/,CireNeikual,1461088631,,3,3,False,default,,,,,
586,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,3,4fiyzu,blog.yhat.com,Building a (semi) Autonomous Drone with Python,https://www.reddit.com/r/MachineLearning/comments/4fiyzu/building_a_semi_autonomous_drone_with_python/,elisebreda,1461090165,,0,4,False,default,,,,,
587,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,3,4fj06m,jorditorres.org,FIRST CONTACT WITH TENSORFLOW book | Prof. Jordi Torres,https://www.reddit.com/r/MachineLearning/comments/4fj06m/first_contact_with_tensorflow_book_prof_jordi/,coskunh,1461090557,,0,27,False,http://b.thumbs.redditmedia.com/J1BVfSiYlf72a2D2AeClWksy_N_20owmanj8uxLoIWs.jpg,,,,,
588,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,4,4fj7kn,self.MachineLearning,Pretrain seq2seq?,https://www.reddit.com/r/MachineLearning/comments/4fj7kn/pretrain_seq2seq/,pumpkin105,1461093058,"Hi,

I am training a seq2seq network to recreate a seen text sequence. I have four GRUs whereas the second one returns its last state as input to the third one. Thus I have two encoder networks and two decoder networks.

I try some kind of pretraining by starting training with very short sequences and enlarging them over time: `t`, `th`, `the`, `the `, `the d`, `the do`, `the dog`. The length of sequence is a uniform distribution, but the upper bound is increased over time.

From my experience, training is very slow and results are not as good as expected. After one day of training, my network can reconstruct a 20 letter sequence with letter accuracy of 80%.

Has anyone tried something similar and had more success with a different approach?",0,0,False,self,,,,,
589,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,4,4fja2p,github.com,One line Bayesian optimization of scikit_learn model hyperparameters,https://www.reddit.com/r/MachineLearning/comments/4fja2p/one_line_bayesian_optimization_of_scikit_learn/,Zephyr314,1461093882,,14,0,False,http://b.thumbs.redditmedia.com/Ib9gtwFJVfrO50JifLtV8RgEqkxev7iEEO7YOF4YTnE.jpg,,,,,
590,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,4,4fjdxy,self.MachineLearning,ELI5: Bias in Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4fjdxy/eli5_bias_in_neural_networks/,bakmanthetitan329,1461095189,"I understand the core concept, that an offset might be necessary to meet certain conditions, but what are the practical applications of this?",10,0,False,self,,,,,
591,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,5,4fjl1c,blog.clarifai.com,What Convnets see when they see nudity,https://www.reddit.com/r/MachineLearning/comments/4fjl1c/what_convnets_see_when_they_see_nudity/,flippzz,1461097636,,38,91,False,http://b.thumbs.redditmedia.com/ovDLWyMbVGfHwtuwnvY-7s31usGpZW2ILhs06vq_Zeg.jpg,,,,,
592,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,6,4fjtuj,blogs.microsoft.com,Teaching computers to describe images as people would - Next at Microsoft,https://www.reddit.com/r/MachineLearning/comments/4fjtuj/teaching_computers_to_describe_images_as_people/,whitebeatle,1461100628,,0,0,False,http://b.thumbs.redditmedia.com/XmNr8Zma_SEtI0_62rfNP-aDw5KUMnc_18-cL7Toj_Y.jpg,,,,,
593,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,7,4fk165,youtube.com,DanDoesData: Simple RNN examples in Keras,https://www.reddit.com/r/MachineLearning/comments/4fk165/dandoesdata_simple_rnn_examples_in_keras/,vanboxel,1461103233,,0,0,False,default,,,,,
594,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,7,4fk2uh,self.MachineLearning,Colorize your photo or video with convolutional neural network,https://www.reddit.com/r/MachineLearning/comments/4fk2uh/colorize_your_photo_or_video_with_convolutional/,tassadar_ha,1461103869,"I've just finished a small enhancement to Ryan Dahls work on CNN for image colorization. Now you can colorize full-sized photos and submit folders for colorization.
https://github.com/shelpuk/image-colorization

Here are some results. Old photo of the city of Lviv:
https://github.com/shelpuk/image-colorization/tree/master/old_lviv/color

And a song from the movie Wcgi (1939):
http://youtu.be/RNQdbYaxag0
",0,1,False,self,,,,,
595,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,7,4fk3rs,self.MachineLearning,[AMA Request] Jeff Hawkins,https://www.reddit.com/r/MachineLearning/comments/4fk3rs/ama_request_jeff_hawkins/,carlthome,1461104195,"# Downvote me to oblivion, sure, but before you do read my motivation.

~~**EDIT: So this got to zero faster than I expected, you bunch of goons. My supervisor kinda likes the HTM so now I'll have to explain to him that I can't put Hawkins into my Discussion because reddit will downvote me.**~~

Hawkins is one of the most notorious people in ""machine learning"". I even feel I have to put machine learning in quotes because I'm sure many of you, and maybe even he himself, would not consider him a student of machine learning. Yet, he is a very vocal and prominent spokesperson for AI and he inspires a lot of people, despite the controversy.

Many of the previous AMAs on this subreddit have asked about Hawkins' and Numenta's research. Schmidhuber wrote _""At the moment I dont see any evidence that Hawkins system can contribute towards more powerful AI systems._ and LeCun _""I think Jeff Hawkins [...] greatly underestimated the difficulty of reducing these conceptual ideas to practice.""_.

I think it would be particularly interesting to ask him of his views on the progress of ANNs, and **especially his view on *recurrent* NNs** and if they might be unified with Numenta's HTM. What does he think of Alex Graves' work with the NTM for example? The whole philosophy of using *hierarchical models* and the importance of *modelling sequences* for making accurate predictions is all there, after all.",18,21,False,self,,,,,
596,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,9,4fktfe,self.MachineLearning,"Unsupervised learning with Geospacial &amp; Location data, best place to start?",https://www.reddit.com/r/MachineLearning/comments/4fktfe/unsupervised_learning_with_geospacial_location/,Rich700000000000,1461113975,"Say I have a dataset of thousands of events and their location &amp; time. I want to predict the location of future events.

1. What toolkit would I use?
2. What algorithm would give me the best results? Unsupervised, of course. 
3. How would I incorporate associated metadata? For example, the temperature or proximity to a train station?",2,0,False,self,,,,,
597,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,11,4fl6kk,self.MachineLearning,Trying to understand when NN/CNN/LSTM/etc... go wrong.,https://www.reddit.com/r/MachineLearning/comments/4fl6kk/trying_to_understand_when_nncnnlstmetc_go_wrong/,thephysberry,1461119125,"The achievements of neural nets have been truly astounding. They seem to be setting the bar in terms of performance in many/all ML challenges. I am, however, curious about where they fail. Specifically:

* What types of problems do other algorithms (SVM, bayes, decision trees, etc.) outperform NNs?
* Is there much research into artificially constructing decision boundaries which thwart NNs?
* Are there some known problems where back-propagation is outperformed by different optimisation algorithms?

I am trying to understand a sort of meta decision boundary in ""problem space"" as to which type of algorithm to select for a given problem. I am aware that things like logistic regression are far easier to implement and may perform equally on simple enough tasks. But that sort of comparison is not what I am looking for. 

To give a more concrete question: 
I have read that the many local minima in the NN optimisation are not an issue and it is actually saddle points which really trip up NNs. Are there problem types that are prone to developing more saddle points in the parameter space (but don't trip up some other algorithm)?

Thanks!",7,11,False,self,,,,,
598,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,15,4flzsk,self.MachineLearning,Topic classification from text,https://www.reddit.com/r/MachineLearning/comments/4flzsk/topic_classification_from_text/,anantzoid,1461132524,"Hi, 
I've recently just gone through the basics of RNNs and was amazed what Sequence to Sequence models can do. I had an idea for a project in mind, but couldn't find any related approaches online.

I want to train summares of books with the book name and want the system to predict the book name if I gave a small text context. For example, ""magic in London"" should give me results as ""Harry Potter"", even though the phrase may not be mentioned in the training data,  the system should be able to find the similar word vectors (that were in the summary) and thus give the result. How feasible is this? And it'd be really helpful if someone could point me to relevant resources.

I guess similar kind of systems are made for conversational bots, where similar patters are matched against the user's question to provide the answer.",5,3,False,self,,,,,
599,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,15,4fm2di,somatic.io,"On AlphaGo, intuition, and the master objective function",https://www.reddit.com/r/MachineLearning/comments/4fm2di/on_alphago_intuition_and_the_master_objective/,WikiBrain8,1461134038,,0,0,False,http://b.thumbs.redditmedia.com/NJIYvKryz0THYbksIff90w1v01YqIqRo2CVg-ERUm9I.jpg,,,,,
600,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,16,4fma8i,medium.com,Adventures in Narrated Reality: New forms &amp; interfaces for written language,https://www.reddit.com/r/MachineLearning/comments/4fma8i/adventures_in_narrated_reality_new_forms/,self,1461138630,,0,7,False,http://b.thumbs.redditmedia.com/yFrjK5kwgyW3uLYxFaOmCOL_d2fDy3ZlvGavuLen0lI.jpg,,,,,
601,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,16,4fmaqt,self.MachineLearning,Optimize features in order to reclassify item.,https://www.reddit.com/r/MachineLearning/comments/4fmaqt/optimize_features_in_order_to_reclassify_item/,dinodingo,1461138959,"I have a (large) dataset of items in a store, each with a range of features (price, size, weight, color, ratings, customer demand, etc). I also know how well each of these items are selling.

There are a lot of information out there on how to create a classifier which can predict how much a new item will sell, based on it's features[1]. However, assuming my goal is not to introduce new (successful) items in my store, but optimize the current items.

If I have an item with some fixed properties which sells poorly, how can I find which features I need to change (and how much) for the item to sell better. The typical classifier will be able to predict that an item wont sell, but how can you make it tell you ""why"" it won't sell, and what you need to change?

[1] Example: An item with a low price and high demand will probably sell a lot, while an item with no demand will never sell.",4,0,False,self,,,,,
602,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,17,4fmgac,joanbruna.github.io,Stat212b: Topics Course on Deep Learning by joanbruna,https://www.reddit.com/r/MachineLearning/comments/4fmgac/stat212b_topics_course_on_deep_learning_by/,TheInvisibleHand89,1461142227,,3,36,False,default,,,,,
603,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,19,4fmpxp,self.MachineLearning,Speed and optimization in Torch?,https://www.reddit.com/r/MachineLearning/comments/4fmpxp/speed_and_optimization_in_torch/,cjmcmurtrie,1461148423,"Has anyone put together any documentation/tips for writing faster Torch code?

There are many subtle features I am discovering in Torch that have quite a different effect on memory/processing overhead, e.g. torch.expand versus torch.repeatTensor.",5,8,False,self,,,,,
604,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,20,4fmsxn,self.MachineLearning,Machine learning and Ontologies?,https://www.reddit.com/r/MachineLearning/comments/4fmsxn/machine_learning_and_ontologies/,Jadeyard,1461150266,"Hello,

I am curious about the split between the machine learning (or rather deep learning) and the ontology communities.  Can you elaborate on the gap between them? Some people from the one community seem to be outright allergic to the other topic.

Do both approaches have different uses and combinations during the development process can be beneficial to finished products?",6,11,False,self,,,,,
605,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,20,4fmvdr,self.MachineLearning,Which model to use for multiclass classification with vectors?,https://www.reddit.com/r/MachineLearning/comments/4fmvdr/which_model_to_use_for_multiclass_classification/,valikund,1461151649,"Hi, I am working on a classification problem, but I have trouble with finding the right algorithm. My data consists of integer vectors, which contain 60 values. Each vector belongs to a class, there are 12 classes altogether. I want an algorithm where i can dump my training vectors and class labels, and later input new vectors, and the algorithm would tell which class they are. ",8,7,False,self,,,,,
606,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,22,4fna8e,self.MachineLearning,Machine learning in maze solving?,https://www.reddit.com/r/MachineLearning/comments/4fna8e/machine_learning_in_maze_solving/,ToKickAHoneyBadger,1461158550,"I am currently in the early phases of an engineering bachelor's thesis on using machine learning in maze solving.

The main focus of the thesis is actually agent based simulation, but this is heavily supported by the machine learning component.

Machine learning is so broad at this point that I don't really know where to start. Does anyone have an experience in machine learning and mazes? Having a reference point to work off or some direction would be great.

Thanks in advance reddit.",13,1,False,self,,,,,
607,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,22,4fnbdr,haifengl.github.io,Classification algorithms visual comparison,https://www.reddit.com/r/MachineLearning/comments/4fnbdr/classification_algorithms_visual_comparison/,[deleted],1461159003,[deleted],0,56,False,default,,,,,
608,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,23,4fngm6,blog.mldb.ai,"Machine Learning Meets Economics, Part 2: the Case of the Augmented Humans",https://www.reddit.com/r/MachineLearning/comments/4fngm6/machine_learning_meets_economics_part_2_the_case/,nkruchten,1461160954,,4,64,False,http://b.thumbs.redditmedia.com/89S4CS2ozYjavQwAG2fuNBkhEII4rmqSwyIxGqZLvjU.jpg,,,,,
609,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,23,4fni9o,issuu.com,Maquinaria de segunda mano: corte y deformacin,https://www.reddit.com/r/MachineLearning/comments/4fni9o/maquinaria_de_segunda_mano_corte_y_deformacin/,Barriuso,1461161577,,0,1,False,default,,,,,
610,MachineLearning,t5_2r3gv,2016-4-20,2016,4,20,23,4fnj4q,self.MachineLearning,Choosing a Linix Operating System for ML,https://www.reddit.com/r/MachineLearning/comments/4fnj4q/choosing_a_linix_operating_system_for_ml/,deeayecee,1461161880,"I'm planning on building a Linux machine for performing ML. My hope is to leverage many of the Python based frameworks available -- scikit, theano, Caffe, lasagne, Tensorflow, gensim, NLTK, etc. I have an older machine running CentOS 6.4 which has ruled out certain modules at times (it runs an older version of glibc, there is direct TensorFlow support for CentOS, etc), so my fear is picking an OS that will prevent me from exploring and utilizing some of the newest frameworks. Of the relatively mainstream Linux distributions (RedHat, Fedora, CentOS, Ubuntu, etc), will I be ruling out certain frameworks?

TIA for your help!",7,2,False,self,,,,,
611,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,0,4fntfr,blog.aidangomez.ca,Backpropogating an LSTM: A Numerical Example,https://www.reddit.com/r/MachineLearning/comments/4fntfr/backpropogating_an_lstm_a_numerical_example/,alxndrkalinin,1461165365,,1,67,False,http://a.thumbs.redditmedia.com/Dd-5Jvm4YoCNvGDTX2iYiZSVcdc9z0_1mct4m6XAff8.jpg,,,,,
612,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,0,4fnznr,quora.com,Traits of a good machine learning engineer,https://www.reddit.com/r/MachineLearning/comments/4fnznr/traits_of_a_good_machine_learning_engineer/,nikhilbd,1461167502,,0,1,False,default,,,,,
613,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,1,4fo8j2,self.MachineLearning,Developing a Machine Learning Model to QA human meta data attribution,https://www.reddit.com/r/MachineLearning/comments/4fo8j2/developing_a_machine_learning_model_to_qa_human/,JC08,1461170477,"## The Goal
I want to develop a machine learning model in R that I can deploy in Java. I want to describe what I've tried, and how it failed, and what my next iteration is so every one here can help guide me.

## The Problem
We have a system where a human will examine raw text and assign it meta-data where each piece of meta-data is a separate category. For example, say we had String 1 and available categories A-Z and a human assigned String 1 categories A, B, C, and F. There is a large amount of human QA that happens afterwards to ensure that String 1 either received all the categories it should have and didn't receive any categories it shouldn't have, for example String 1 should not have received the f category. I am tasked with developing a way to automatically detect if a String needs QA after meta-data has been assigned to it. To summarize, a string can have multiple categories assigned to it.
 
To get more specific I have 3000ish categories and I know that 2700 of them are currently in use. Currently in use means there exists Strings that have been assigned that category. So that implies there are 300ish categories that have not even been assigned to a String.

## What I've tried
I hypothesized that we could use Solr's default Vector Space Model to generate a similarity score to predict if a String should or should not have received the category. I utilized Solr 5.2.1 and indexed to it each category where each document contained a field that contained all strings that received that category for that particular document joined together with a single space separating each String. Since I had 2700ish categories in use, then in Solr I had 2700ish documents.

Next, I took a set of Strings that I did not index and got their similarity scores for all documents that had a similarity score greater than zero. Now, all the categories returned did not necessarily correspond to to categories the String actually had. So my data set looked liked the following


score | has

.7444|  1

.234  | 0

Where the score is the similarity score from Solr for that String and the has variable is a factor variable where the values that can be taken are 0 and 1, 0 for the String is not assigned that category and 1 for that String is assigned that category. As I mentioned before I built the data set in R and used caret to train a logistic Regression Model. I chose a Logistic Regression Model because it's easy to take the trained parameters and create a Java Method out of them. The model performed Ok on the test Data. It was very good at predicting absolutely when a String should not have received a category and did poorly for when a String should receive a category and ok when the category had similarities to other categories. The score was a good predictor, but it wasn't enough to help differentiate the differences between categories that have similarities with important differences.

I then ran a third set of untouched data through the logistic regression model and quite frankly, the results were shit.

## The Next Iteration
The big takeaway from the first iteration was that similarity score, while being an ok predictor, did not encapsulate enough to differentiate categories that were similar. For my next iteration I want to try using the R tm package to generate the TF-IDF document matrix. So I am still going for binary classification for has and should not have and I am thinking of two possible experimental set-ups which I describe below.

First, since there are 2700 active categories, then we will have 2700 models where each model will be composed of two categories. The first category are the Strings that have that category and the second are the Strings that do not have that category. To build that data set I would take all our Strings with the category and without and split it into a 70% training set and 30% testing set. Then, I will train a Support Vector Machine (SVM) on the training set. I am choosing a SVM because research indicates that SVM models perform well on text classification using TD-IDF matrices. I would perform the indicated steps for each category. Then I would utilize the r2pmml package, or some other such package, to save all the SVM models into a format I can try implementing in Java.

The biggest issues with the previously described approach are that with such a large amount of models it will be difficult to evaluate the quality of each and every one of them, training such a large amount of models will take quite a bit of time as I have to demonstrate the previously described approach will work before I am allowed access to enterprise cloud resources, and even if I can save the models to PMML for java there will still be 2700ish models to manage and there are a lot of unknowns at the moment for using PMML. 

The second approach is to take the documents generated from Solr and use the R tm package to generate a TD-IDF matrix on all 2700 categories using the Solr Document Field that contains all the strings. Next I would train a SVM model on that TF-IDF matrix. Finally I would would explore the results using 70% of our total Strings to see if there is a correlation between category probability and assignment of having that category. 

The problems with that approach are that I am again relying on a single predictor, probability, to determine when a String should receive a category. I am relying on the category probabilities and it is unknown if I can get those in PMML where I am planning to export the model too.

The advantage of the approach over the 2700 models is that I have a single model to deal with.

## Feedback
Please give me feed back on my thoughts. I am asking because I want to create something that will work for the task and I want to rely on those that have been down that road before me for assistance.",2,2,False,self,,,,,
614,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,2,4fofd0,self.MachineLearning,Parameters for CART tree,https://www.reddit.com/r/MachineLearning/comments/4fofd0/parameters_for_cart_tree/,BlackHawk90,1461172772,"Hello

I'm using fitctree function from Matlab to fit a CART tree to my dataset (10'000 data points, 17 features).

First, how can I exactly prune it and to what level should the tree be pruned? I think I have to use the prune method but I'm confused by the 'MergeLeaves' and 'Prune' flag of fitctree.

Second, what values of 'MinLeafSize', 'MinParentSize' and 'MaxNumSplits' are reasonable to try?",0,0,False,self,,,,,
615,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,3,4for64,projecteuclid.org,A survey of cross-validation procedures for model selection,https://www.reddit.com/r/MachineLearning/comments/4for64/a_survey_of_crossvalidation_procedures_for_model/,carmichael561,1461176658,,0,6,False,http://b.thumbs.redditmedia.com/u2nmAflT-XSJ3Zu64npzl3IJsLUR8h7I30JVwHZjHGw.jpg,,,,,
616,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,4,4foxr9,cs.stanford.edu,DenseCap: Fully Convolutional Localization Networks for Dense Captioning,https://www.reddit.com/r/MachineLearning/comments/4foxr9/densecap_fully_convolutional_localization/,vkhuc,1461178820,,15,108,False,http://b.thumbs.redditmedia.com/-AJNVqylSz-g_N2SvybxGBRYMy7ej08_ElPf2iVC0hg.jpg,,,,,
617,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,4,4fp6hd,self.MachineLearning,Show Attend and tell?,https://www.reddit.com/r/MachineLearning/comments/4fp6hd/show_attend_and_tell/,KrisSingh,1461181717,was reading the paper Show attend and tell for genrating captions i coudn't understand the need for E embedding matrix please refer fig 4 also is z_t and what is y_t-1 ?,3,2,False,self,,,,,
618,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,5,4fpbdk,tnw.to,Machine learning to guess who will die next in Game of thrones,https://www.reddit.com/r/MachineLearning/comments/4fpbdk/machine_learning_to_guess_who_will_die_next_in/,himalayanSpider,1461183387,,0,0,False,http://b.thumbs.redditmedia.com/9zYJRsInwXJNjmuGTeHkW5QjmKoQwkOYk5ZWrhH5pCA.jpg,,,,,
619,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,6,4fpohi,youtube.com,"UMassTTS: ""Intro to Machine Learning"" by Cassidy Williams, Clarifai",https://www.reddit.com/r/MachineLearning/comments/4fpohi/umasstts_intro_to_machine_learning_by_cassidy/,jikjordan,1461187922,,2,3,False,http://b.thumbs.redditmedia.com/I7Iwk6qJpT_qAbmd5oGoHN4CxrquNoDN-E0rR4px6AM.jpg,,,,,
620,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,6,4fpow3,self.MachineLearning,Question about a particular paper on conversion of categorical data to numeric form,https://www.reddit.com/r/MachineLearning/comments/4fpow3/question_about_a_particular_paper_on_conversion/,thestudent2006,1461188065,"I am not sure this is the right forum for this -- I have a question about a particular paper:

http://www-users.cs.umn.edu/~sboriah/PDFs/ChandolaCBK2009.pdf

The authors describe 4 heuristics that can be derived from categorical data -- this is in order to map categorical data to numerical. These heuristics are d_m, f_m, n_x, f_x. They also provide two examples y and z and the values of the quantities above computed with respect to dataset in table 3. I am able to lock into their values exactly for d_m and f_m but I cannot reproduce n_x and f_x.

Could someone read this paper and try to derive these values? I basically take it their equation (3.3) shows summation of reciprocals of arity for A_x set (i.e. the set of mismatching attributes) -- I can't reproduce -5.45 and -7.90.

Please note I already contacted the authors -- one responded that Dr. Boriah is the person responsible for these calculations but he is apparently not reacheable.",0,0,False,self,,,,,
621,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,7,4fq2im,self.MachineLearning,Using machine learning libraries to make game AI?,https://www.reddit.com/r/MachineLearning/comments/4fq2im/using_machine_learning_libraries_to_make_game_ai/,Another_moose,1461192939,"Hey everyone, sorry if this is off-topic/ irrelevant.

I've been working on a game in my spare time, it's basically a simplified version of Civ 5, and I've got to the point of starting the AI. I had been thinking of using some kind of ML/evolution before, but recently, I came across some really cool demos of tensorflow and sklearn.

What I'm wondering though, is how could I take a model trained in an external library, and use it in code? Or maybe I could take the raw data from the network and write my own handler for it? Would I be better off trying to roll my own, even as a learning experience?

I'd really appreciate some help! Thanks!
Moose.",6,1,False,self,,,,,
622,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,7,4fq2lk,self.MachineLearning,[Question]: What is the best place to start reviewing work around traffic modeling ?,https://www.reddit.com/r/MachineLearning/comments/4fq2lk/question_what_is_the_best_place_to_start/,__bee,1461192967,"In case there is anyone working on these problems, what's the best place to start reviewing papers around traffic modeling ? ",5,0,False,self,,,,,
623,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,8,4fq900,arxiv.org,Deep Residual Networks with Exponential Linear Unit,https://www.reddit.com/r/MachineLearning/comments/4fq900/deep_residual_networks_with_exponential_linear/,[deleted],1461195424,[deleted],0,1,False,default,,,,,
624,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,8,4fqaos,arxiv.org,[1604.04112v1] Deep Residual Networks with Exponential Linear Unit,https://www.reddit.com/r/MachineLearning/comments/4fqaos/160404112v1_deep_residual_networks_with/,dmitritkatch,1461196052,,6,4,False,default,,,,,
625,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,8,4fqat5,arxiv.org,Churn analysis using deep convolutional neural networks and autoencoders,https://www.reddit.com/r/MachineLearning/comments/4fqat5/churn_analysis_using_deep_convolutional_neural/,_panw,1461196101,,8,0,False,default,,,,,
626,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,10,4fqnjb,self.MachineLearning,[Question] Reduced Error Logistic Regression (RELR),https://www.reddit.com/r/MachineLearning/comments/4fqnjb/question_reduced_error_logistic_regression_relr/,Kiuhnm,1461201407,"I came across a book titled [Calculus of Thought: Neuromorphic Logistic Regression in Cognitive Machines](http://www.amazon.com//dp/012410407X).

It introduces a method called **reduced error logistic regression (RELR)**. Does anyone know anything about it?

---

**Please don't downvote the messenger!**",7,4,False,self,,,,,
627,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,13,4frcan,self.MachineLearning,Where can I find a trained machine translation seq2seq model?,https://www.reddit.com/r/MachineLearning/comments/4frcan/where_can_i_find_a_trained_machine_translation/,rettis,1461211951,"Title says it all - I'd like to play around with a well trained LSTM sequence to sequence MT model, but I'd rather not futz around with training one.",5,3,False,self,,,,,
628,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,14,4fritg,self.MachineLearning,RNN language models: No cross validation?,https://www.reddit.com/r/MachineLearning/comments/4fritg/rnn_language_models_no_cross_validation/,rescue11,1461215454,"I always see language model datasets having a train set and a dev set (validation). What are the reasons why this is done, rather than doing n-cross validation on the data?",2,0,False,self,,,,,
629,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,14,4frjss,buffer-pictures.s3.amazonaws.com,"Easily work out Machine learning experiments using Azure MLStudio. This can scale nicely, easy to create quick experiments, easy to visualize stuff and great integration with other Azure services.",https://www.reddit.com/r/MachineLearning/comments/4frjss/easily_work_out_machine_learning_experiments/,Goot1965,1461215996,,0,0,False,http://b.thumbs.redditmedia.com/9NWQEmUcQNbot1rASwEzO_Ly455DAXgQDObDmM8U4JQ.jpg,,,,,
630,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,14,4frlqk,youtube.com,HONMAKSAN Fren Kampana Disk Tornas -- Disc Rotors Resurface and Brake D...,https://www.reddit.com/r/MachineLearning/comments/4frlqk/honmaksan_fren_kampana_disk_tornas_disc_rotors/,honmaksan,1461217120,,1,0,False,default,,,,,
631,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,14,4frm32,arxiv.org,[1604.06057] Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,https://www.reddit.com/r/MachineLearning/comments/4frm32/160406057_hierarchical_deep_reinforcement/,iori42,1461217322,,21,60,False,default,,,,,
632,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,16,4frz5d,self.MachineLearning,Tips on training RNNs for binary classification with very few true positives,https://www.reddit.com/r/MachineLearning/comments/4frz5d/tips_on_training_rnns_for_binary_classification/,habitats,1461225190,"I'm trying to solve a multilabel problem with n-binary RNN classifiers, i.e. one classifier for every class. However, some of the classes in my dataset have very few true positives, e.g. 1/100 or even 1/1000. 

My current approach works pretty good with classes that are 1/10, but it breaks down with the skewed ones.

Any tips on how to tackle this? Is it a bad idea to manufacture different datasets to force the true positives to become higher? 

For the record; the RNN I'm using is a two layer LSTM-based network with 333 hidden nodes, and binary output. Training is backprog with SGD. Minibatch size is 100.",13,10,False,self,,,,,
633,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,17,4fs4ji,self.MachineLearning,Neural Network on an Android Smartphone,https://www.reddit.com/r/MachineLearning/comments/4fs4ji/neural_network_on_an_android_smartphone/,shadow12348,1461228752,"Hey,

I'm trying to make a simple app that can basically be trained to recognize a particular object. So the user would point his camera at an object for a couple of seconds and it gets sampled thousands of times in these seconds that serves as the training data for the neural network. I have the following questions, any help would be appreciated-

1. Are there any API's or android libraries that already do this or should I just write the entire neural network by hand? I ask because, I would like to compare the effectiveness of different types of neural networks.
2. I am no expert in android programming but is it possible to use the GPU on a smartphone for training, rather than the CPU? It has been proven to be much more effective.

Any feedback would help. Thanks in advance!",4,0,False,self,,,,,
634,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,20,4fshrs,self.MachineLearning,Stanford CoreNLP sentiment analysis different results,https://www.reddit.com/r/MachineLearning/comments/4fshrs/stanford_corenlp_sentiment_analysis_different/,Faceman1208,1461237480,"Is there anybody who was able to archive the same results like in the Socher paper (http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf). I used the pretrained model from the website http://nlp.stanford.edu/sentiment/code.html and got the following results:

Paper results

* Fine-grained 80.7 (All)  45.7 (Root) 
* Positive/Negative 87.6 (All) 85.4 (Root)

The results which I got

* Fine-grained 0.802155 (All)  0.441629 (Root) 
* Positive/Negative 0.695110 (All) 0.785832 (Root) 

The whole results:

		EVALUATION SUMMARY
	Tested 82600 labels
	  66258 correct
	  16342 incorrect
	  0.802155 accuracy
	Tested 2210 roots
	  976 correct
	  1234 incorrect
	  0.441629 accuracy
	Label confusion matrix
		  Guess/Gold       0       1       2       3       4    Marg. (Guess)
				   0     323     161      27       3       3     517
				   1    1294    5498    2245     652     148    9837
				   2     292    2993   51972    2868     282   58407
				   3      99     602    2283    7247    2140   12371
				   4       0       1      21     228    1218    1468
		Marg. (Gold)    2008    9255   56548   10998    3791

				   0        prec=0.62476, recall=0.16086, spec=0.99759, f1=0.25584
				   1        prec=0.55891, recall=0.59406, spec=0.94084, f1=0.57595
				   2        prec=0.88982, recall=0.91908, spec=0.75299, f1=0.90421
				   3        prec=0.58581, recall=0.65894, spec=0.92844, f1=0.62022
				   4        prec=0.8297, recall=0.32129, spec=0.99683, f1=0.46321
	Root label confusion matrix
		  Guess/Gold       0       1       2       3       4    Marg. (Guess)
				   0      44      39       9       0       0      92
				   1     193     451     190     131      36    1001
				   2      23      62      82      30       8     205
				   3      19      81     101     299     255     755
				   4       0       0       7      50     100     157
		Marg. (Gold)     279     633     389     510     399

				   0        prec=0.47826, recall=0.15771, spec=0.97514, f1=0.2372
				   1        prec=0.45055, recall=0.71248, spec=0.65124, f1=0.55202
				   2        prec=0.4, recall=0.2108, spec=0.93245, f1=0.27609
				   3        prec=0.39603, recall=0.58627, spec=0.73176, f1=0.47273
				   4        prec=0.63694, recall=0.25063, spec=0.96853, f1=0.35971

	Approximate Negative label accuracy: 0.646009
	Approximate Positive label accuracy: 0.732504
	Combined approximate label accuracy: 0.695110
	Approximate Negative root label accuracy: 0.797149
	Approximate Positive root label accuracy: 0.774477
	Combined approximate root label accuracy: 0.785832 

But I was not able to get the same results like in the paper.",0,1,False,self,,,,,
635,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,20,4fsiyj,blog.acolyer.org,The amazing power of word vectors,https://www.reddit.com/r/MachineLearning/comments/4fsiyj/the_amazing_power_of_word_vectors/,mttd,1461238106,,24,111,False,http://b.thumbs.redditmedia.com/wYq5k4suSzeH3nrMuv0-3Yr65ze5DnDWmGqV0B6aeZQ.jpg,,,,,
636,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,20,4fskzu,self.MachineLearning,2-class pre-trained sentiment model,https://www.reddit.com/r/MachineLearning/comments/4fskzu/2class_pretrained_sentiment_model/,Faceman1208,1461239194,[removed],0,1,False,default,,,,,
637,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,20,4fsl7l,github.com,TensorFlow Simplified Interface Examples: /r/MachineLearning's thoughts?,https://www.reddit.com/r/MachineLearning/comments/4fsl7l/tensorflow_simplified_interface_examples/,alexanderjmartin,1461239312,,7,14,False,http://b.thumbs.redditmedia.com/_24W0V7dMzXx1JhHLLxPQRWuW1Xn_hWqasy3XkOTj8E.jpg,,,,,
638,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,22,4ft1ns,github.com,ColorNet: open source colorizing grayscale images,https://www.reddit.com/r/MachineLearning/comments/4ft1ns/colornet_open_source_colorizing_grayscale_images/,shauchenka,1461246714,,0,1,False,default,,,,,
639,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,22,4ft28q,autumnai.com,How to build Machine Learning applications with Leaf 0.2.1 and Rust,https://www.reddit.com/r/MachineLearning/comments/4ft28q/how_to_build_machine_learning_applications_with/,mjhirn,1461246942,,0,0,False,default,,,,,
640,MachineLearning,t5_2r3gv,2016-4-21,2016,4,21,23,4ft9ry,csoonline.com,Machine Learning and Social Engineering Attacks,https://www.reddit.com/r/MachineLearning/comments/4ft9ry/machine_learning_and_social_engineering_attacks/,putd,1461249830,,0,0,False,http://b.thumbs.redditmedia.com/ISrA7rPvYgq_jP292I5yjZgXdPkehh5tuyGrgrgxAqQ.jpg,,,,,
641,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,0,4ftimd,self.MachineLearning,[QUESTION] Trained models for neural networks?,https://www.reddit.com/r/MachineLearning/comments/4ftimd/question_trained_models_for_neural_networks/,deluded_soul,1461252970,"I am making an app running on the Nvidia Jetson TK1 board, which has support for CUDA and cuDNN. I am trying to implement face detection and full body tracking that can track a single person. I would like to use deep learning for this but my problem is that I do not have much training/labelled data. I wonder if someone already has already developed such models and if the trained model might already be available?",4,0,False,self,,,,,
642,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,1,4ftugf,self.MachineLearning,Photo person attractiveness datasets? Preferably one that controls for clothing / makeup / environment.,https://www.reddit.com/r/MachineLearning/comments/4ftugf/photo_person_attractiveness_datasets_preferably/,[deleted],1461257106,[deleted],5,0,False,default,,,,,
643,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,1,4ftvnr,self.MachineLearning,[Q] ELI5: Why *not* have a royal rumble between all Supervised Learning techniques?,https://www.reddit.com/r/MachineLearning/comments/4ftvnr/q_eli5_why_not_have_a_royal_rumble_between_all/,firesalamander,1461257524,"(Warning for ML PhDs and professionals: This has caused indignant sputtering from people that know far more than me, which is why I'm asking!) 

*I asked a friend: which algorithm is the best?*  And was promptly told that question wasn't valid, and it all depends, and spend more time curating your data, and get an expert involved to tune your model, and it is an art, and...

No, I want to try it!  I used the excellent [JSAT ML Library](https://github.com/EdwardRaff/JSAT) that has academically-sound implementations of many supervised learning algorithms though a normalized interface.  But I didn't know which algo to pick, or which inputs to use.  So I picked them all.

    Reflection to find all classes that implement ""Classifier""
    x all constructors of each class
    x all parameters of each constructor with a range of guesses.  
        Integer?  Sure, we got 3, 13, and 50!  
        Double?  We got those too! 
        Weak learners?  Why not DecisionStumps?  Anything else?
        KernelTricks?  No idea, I'll pick one from the test cases!
    x a bunch of data sets off of www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

Then run them all in parallel, let your desktop overheat for a few days, ignore a ton of errors, and **[boom - a visualization of how they all compare](http://ml-allthethings.appspot.com/).**  Please click around and try the filters on the left.

So what?  

My goal is to produce a short list of ""2-5 good starting algorithms (with parameters) that newcomers should try first *because they are fast and work on a wide range of datasets because they break under very different conditions*.  

After clicking around, it seemed safe to pick Multinomial Naive Bayes, NewGLMNET, and OnlineAMM - they all fast, relatively modern, outperform their close relatives, and don't seem to be as fragile.

Finally my question tree: **Is this at all reasonable?  possible?  Are the three I came up with a good starter set?  Is there a better set?**",50,9,False,self,,,,,
644,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,3,4fua7l,self.MachineLearning,Entity recognition in 3d meshes,https://www.reddit.com/r/MachineLearning/comments/4fua7l/entity_recognition_in_3d_meshes/,HipHomelessHomie,1461262213,"Hi friends,

I am wondering wether any of you are aware of work being done on trying to recognize an entity given 3d mesh data. 

Obviously humans can often tell what a certain mesh is approximating... Humans and giraffes have very distinct features... But I imagine teaching those to a machine might prove quite difficult for several reasons. One would have to find a good method of encoding the metric of the mesh.

I want to do some work in that direction and am looking for existing papers and datasets. Do any of those exist?

Thanks.",7,7,False,self,,,,,
645,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,4,4fumh9,self.MachineLearning,Any torch implementation of deconvolution?,https://www.reddit.com/r/MachineLearning/comments/4fumh9/any_torch_implementation_of_deconvolution/,code2hell,1461266412,Or any idea of how to use caffe pre trained weights in torch.. Planning to implement semantic segmentation in torch.. Need help! Thanks! ,4,4,False,self,,,,,
646,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,5,4fuzj9,self.MachineLearning,Webinar - How we use AWS for Machine Learning and data collection in our company,https://www.reddit.com/r/MachineLearning/comments/4fuzj9/webinar_how_we_use_aws_for_machine_learning_and/,caligolae,1461270898,"I'm here to announce our first webinar on how we internally use AWS for Machine Learning and Data Collection. 


In this webinar you'll speak directly with **Alex Casalboni** *Sr. Software Engineer*, **Roberto Turrin** *Sr. Data Scientist* and **Luca Baroffio** *Data Scientist* in our Engineering team at Cloud Academy. They will show you how to use Amazon Web Services to manage daily challenges and build a complex machine learning system.

[Register here to the webinar.](https://attendee.gotowebinar.com/register/4150636977507618561)
---

Specifically, you'll learn:


**1. How to deploy machine learning models in the Cloud**

**2. Tips on serverless computing on AWS**

**3. How machine learning can improve your learning experience**


At the end of the webinar we'll send you slides + github repo + recorded webinar. 
So, in case you can't attend you should still register.  (**Jingle Keys**)

----

Some of our articles you liked:

1. [Google Vision API: Image Analysis as a Service](http://cloudacademy.com/blog/google-vision-api-image-analysis/)
2. [BigML: Machine Learning made easy](http://cloudacademy.com/blog/bigml-machine-learning/)
3. [Google Prediction API: a Machine Learning black box for developers]
(http://cloudacademy.com/blog/google-prediction-api/)
4. [Azure Machine Learning: simplified predictive analytics](http://cloudacademy.com/blog/azure-machine-learning/)
5. [Amazon Machine Learning: use cases and a real example in Python](http://cloudacademy.com/blog/aws-machine-learning/)",1,9,False,self,,,,,
647,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,5,4fv1tx,essex.ac.uk,Funded PhDs in AI / ML (in the UK),https://www.reddit.com/r/MachineLearning/comments/4fv1tx/funded_phds_in_ai_ml_in_the_uk/,mikef22,1461271698,,17,6,False,default,,,,,
648,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,6,4fv956,bitfusion.io,Run the latest Nvidia Digits 3 and Cuda 7.5 in minutes,https://www.reddit.com/r/MachineLearning/comments/4fv956/run_the_latest_nvidia_digits_3_and_cuda_75_in/,mtweak,1461274369,,0,1,False,http://b.thumbs.redditmedia.com/HzU1DOVITQzI_HoV6-phZVj2wbR1UnaF4IDz1j9034g.jpg,,,,,
649,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,6,4fv9aj,vizdoom.cs.put.edu.pl,Visual Doom AI Competition @ CIG 2016 [x-post from /r/computervision ],https://www.reddit.com/r/MachineLearning/comments/4fv9aj/visual_doom_ai_competition_cig_2016_xpost_from/,LetaBot,1461274418,,18,66,False,http://b.thumbs.redditmedia.com/vvSlvt4Igh9lL08ODjdUSMz5NuUR4G7yGYkoz3xMnHo.jpg,,,,,
650,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,7,4fvl4s,self.MachineLearning,Automatic Webpage Classification,https://www.reddit.com/r/MachineLearning/comments/4fvl4s/automatic_webpage_classification/,hack777,1461278905,"I'm trying to create a document classifier but I'm not able to think of features to use. Anybody has experience with this? I'm classifying webpages. I used beautiful soup to remove the tags. I know tf-idf can be used, but not exactly sure how.
Suggestions on how to 'clean' the data better (eg removing stop words, stemming, etc) are also welcome. Also, how to reduce the size of the feature vector and any recent publications would of great help.

Edit: I'm using this dataset: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/",3,0,False,self,,,,,
651,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,8,4fvsqw,self.MachineLearning,"ELI5: MXNET sounds like a great library, but no one uses it. Why?",https://www.reddit.com/r/MachineLearning/comments/4fvsqw/eli5_mxnet_sounds_like_a_great_library_but_no_one/,metacurse,1461281871,"By ""no one uses it"" I mean based on what I've heard from people and activity on Github. Keras, which is just a wrapper kind of thing has more success in DL community? Why? Is there something wrong with MXNet?

Also, what is the deal with mixing imperative and declarative paradigms? It seems more like having both in same library. 

Please share your experiences with this library that seems like the ultimate thing but no one uses",24,9,False,self,,,,,
652,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,9,4fvxix,arxiv.org,[1604.06174] Training Deep Nets with Sublinear Memory Cost,https://www.reddit.com/r/MachineLearning/comments/4fvxix/160406174_training_deep_nets_with_sublinear/,antinucleon,1461283743,,21,18,False,default,,,,,
653,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,11,4fwdx3,self.MachineLearning,Deep Neural Net Grammar Checker,https://www.reddit.com/r/MachineLearning/comments/4fwdx3/deep_neural_net_grammar_checker/,LeavesBreathe,1461290410,"Hey Guys,

I'm trying to grammar check a huge corpus of text. There are a few online solutions like grammarly and after the deadline, but the problem with them is that they are way too slow. For 1gb of text, it would take about 10 weeks. I don't mind paying, but it would take way too long.  

Instead, a much faster solution would be to train a deep neural net to grammar check. You could potentially batch sentences (256 or 512 batch size) and could do it all in a few days. 

I was thinking about doing it with a seq2seq approach. The idea is to split up words into subwords, and you would process these subwords within your grammar checker. 

I'm trying to figure out how to do break up these words to subwords. Is there a good approach to breaking up words? Syllables might work, but I feel this requires a basic HMM to do it. Wanted to know people's thoughts on this whole grammar problem!
",6,1,False,self,,,,,
654,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,11,4fwhwc,thoughtly.co,Deep Learning Lesson 4: Multilayer Networks and Booleans  Thoughtly,https://www.reddit.com/r/MachineLearning/comments/4fwhwc/deep_learning_lesson_4_multilayer_networks_and/,mercurialkitten,1461292085,,0,3,False,http://a.thumbs.redditmedia.com/j7eod9aOS8LT5lxJlmCa9rV721kzfOGqZr1ElWV1Cs8.jpg,,,,,
655,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,12,4fwnjf,self.MachineLearning,TensorFlow Introductory Lecture,https://www.reddit.com/r/MachineLearning/comments/4fwnjf/tensorflow_introductory_lecture/,rbharath,1461294553,"We've put together an introductory lecture on TensorFlow as part of [CS 224D](http://cs224d.stanford.edu/), Stanford's deep-learning for NLP class. As far as we can tell, this is one of the first academic lectures on TensorFlow (aside from Google's official docs of course). We hope it'll prove useful to the ML community.

Here's a link to the [video](https://www.youtube.com/watch?v=L8Y2_Cq2X5s&amp;feature=youtu.be) and [slides](http://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf). Feel free to ask us questions on this thread, and we'll answer to the best of our ability.

Edit: Here's a link to the [problem set](http://cs224d.stanford.edu/assignment2/index.html) associated with the lecture. The pset has you implement softmax classifiers, simple deep-networks, and recurrent networks in tensorflow. You should be able to do the pset after watching the lecture (assuming you have a background in ML, say at the level of the Stanford Coursera ML course).",27,292,False,self,,,,,
656,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,13,4fx08v,self.MachineLearning,Does a ML GPU benchmark exist?,https://www.reddit.com/r/MachineLearning/comments/4fx08v/does_a_ml_gpu_benchmark_exist/,themoosemind,1461301079,It seems to me that nowadays it is almost impossible to compare different GPUs without running a benchmark on the kind of task one wants to use it for. So I wondered if there is a GPU benchmark with public results available? I thought of something like training a state-of-the-art MNIST network and giving the time for doing so.,2,0,False,self,,,,,
657,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,14,4fx1xy,self.MachineLearning,How do you model the prediction of upcoming point events from signatures in features?,https://www.reddit.com/r/MachineLearning/comments/4fx1xy/how_do_you_model_the_prediction_of_upcoming_point/,Gere1,1461302038,"I know supervised learning methods and basics of time series analysis, but what is a natural way to model prediction of events?

Let's say I have created a lot time-dependent features. Every now and then, a particular event occurs, which is just a point event and not a value in my features. But I expect that before those events a signature in my features builds up, which indicates that the event is likely to occur some time in the future.

How can you model this naturally?

The most basic thing would be to pick time steps, and at each step use supervised learning predict the probability that the event will occur within some time. But that doesn't take into account when exactly the event will occur. HMMs don't seem like a natural fit either, since I don't have a good state representation.

It seems a bit tricky to make this fit well. Do you have advice or know good references?",4,1,False,self,,,,,
658,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,14,4fx6ac,self.MachineLearning,Getting into ML and I want to build stuff. What are reasonable expectations to have when starting out?,https://www.reddit.com/r/MachineLearning/comments/4fx6ac/getting_into_ml_and_i_want_to_build_stuff_what/,[deleted],1461304489,[deleted],4,0,False,default,,,,,
659,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,15,4fxc8i,self.MachineLearning,What is causing the dip in learning performance in this graph?,https://www.reddit.com/r/MachineLearning/comments/4fxc8i/what_is_causing_the_dip_in_learning_performance/,2_bit_encryption,1461308105,"I'm training a neural network to play Reversi/Othello via Q-learning with experience replay (full details at bottom of post).  Currently using a board size of 6x6 instead of 8x8, for more rapid testing and training.

[Here's a graph of the results I'm getting.](https://imgur.com/j4CNJpC)  The x-axis is the amount of epochs trained (from 1 to 500,000) and the y-axis is the percentage of wins against an opponent who plays totally randomly (tested through 1000 games).

E.g., if at epoch 50,000 the graph is at 80%, then at epoch 50,000 the network won 800/1000 games against a random opponent. (sorry for the poorly created graph image, did it hastily in Google Sheets)

Now, for the most part this graph actually looks how I expected it to look after training.  However, I have a few questions:

What might be causing the 'dip' in the very beginning, where it sinks down to around 30% winrate before climbing back up to a maximum of 93%?  This really surprised me, because it seems like to do *worse* than a random opponent, you would need to actually *try* to lose.  It performs consistently worse than if it were to have just played random moves, which baffles me.

Second, I noticed that after around half of the 500,000 epochs, the graph becomes much more disjointed.  I assume that I might have reached the upper bound of training.  This is a vague question, but are there things I can do to raise my upper bound, and achieve better results?

Here are the details of my setup:
Keras network.  Input layer of 36 (6x6), hidden layer of 44, output layer of 36.  Hidden layer is relu activation.  Output is linear.  I'm using classic Q-Learning with experience replay.  Experience has a memory of 2000 actions long.  I don't do any updates to the neural network until after the 2000th action, so the memory is saturated.  I'm using epsilon-greedy policy, which lowers linearly from 1.0 to 0.01 (reaches 0.01 after 60% of epochs, and stays there).  Q-Learning rate is 0.01.  Every turn I pull 20 experiences from the replay memory at random and train on them (so batch size of 20).  Optimization is RMSprop.

I have a feeling the answer might just be ""that's just the way it is.""  If that's the case, I'm still curious to know why!  Thanks everyone.",12,8,False,self,,,,,
660,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,15,4fxcm2,russellsstewart.com,"Maximum likelihood decoding with RNNs - the good, the bad, and the ugly.",https://www.reddit.com/r/MachineLearning/comments/4fxcm2/maximum_likelihood_decoding_with_rnns_the_good/,[deleted],1461308340,[deleted],0,2,False,default,,,,,
661,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,16,4fxe08,self.MachineLearning,[Q] Computing classifier score for generating ROC curve,https://www.reddit.com/r/MachineLearning/comments/4fxe08/q_computing_classifier_score_for_generating_roc/,rv77ax,1461309146,"Assume that I have training and test set with two class (0, 1) where 1 is positive and 0 is negative. Training set contain 5 positive and 5 negative samples: `[1,1,1,0,1,0,0,0,1,0]`.

 I Run my test set on model and got predictions: `[1,1,0,0,0,0,0,0,1,0]`.

Prior probability of class 1, Pr(1) : 5/10 = 0.5

Prior probability of class 0, Pr(0) : 5/10 = 0.5

So, in order to create ROC curve I compute the score using naive bayes with the following formula: `P(c) * P(pred|nclass)`, where c is class values 0 or 1.

Sample|Actuals | Predictions | Score
---|---|---|---------------|
1 | 1 | 1 | Pr(1) * 1/3 = 0,1666666667 |  
2 | 1 | 1 | Pr(1) * 2/3 = 0,3333333333|
3 | 1 | 0 | Pr(0) * 1/7 = 0,0714285714|
4 | 0 | 0 | Pr(0) * 2/7 = 0,1428571429|
5 | 1 | 0 | Pr(0) * 3/7 = 0,2142857143|
6 | 0 | 0 | Pr(0) * 4/7 = 0,2857142857|
7 | 0 | 0 | Pr(0) * 5/7 = 0,3571428571|
8 | 0 | 0 | Pr(0) * 6/7 = 0,4285714286|
9 | 1 | 1 | Pr(1) * 3/3 = 0,5|
10 | 0 | 0 | Pr(0) * 7/7 = 0,5|

I then sort the probabilities descending by score and compute TPR and FPR, which result in,

Sample|Actuals | Predictions | Score | tp			|fp		|TPR	|FPR
-|-|-|-|-|-|-|-
  ||||0|0|0|0
9|	1		|1	|0,5|1|0|1 / 3 = 0,3333333333|0/7 = 0
10|	0		|0	|0,5|1|1|1 / 3 = 0,3333333333|1/7 = 0,1428571429
8|	0		|0	|0,4285714286|1|2	|1/3 = 0,3333333333	|2/7=0,2857142857
7|	0		|0	|0,3571428571|1|3	|1/3 = 0,3333333333	|3/7=0,4285714286
2|	1		|1	|0,3333333333|2|3	|2/3 = 0,6666666667	|3/7=0,4285714286
6|	0		|0	|0,2857142857|2|4	|2/3 = 0,6666666667	|4/7=0,5714285714
5|	1		|0	|0,2142857143|2|5	|2/3 = 0,6666666667	|5/7=0,7142857143
1|	1		|1	|0,1666666667|3|5	|3/3 = 1	|5/7=0,7142857143
4|	0		|0	|0,1428571429|3|6	|3/3 = 1	|6/7=0,8571428571
3|	1		|0	|0,0714285714|3|7	|3/3 = 1	|7/7=1


Am I doing this right?",9,0,False,self,,,,,
662,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,16,4fxfce,ijcai-16.org,IJCAI-16 accepted papers (list),https://www.reddit.com/r/MachineLearning/comments/4fxfce/ijcai16_accepted_papers_list/,ciolaamotore,1461309953,,1,9,False,http://b.thumbs.redditmedia.com/XHc3qPnFpetZ5HKxi2L__lqOIxCg55qjr2RI_Uct2Aw.jpg,,,,,
663,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,17,4fxiup,yomguithereal.github.io,"Talisman - straightforward &amp; modular NLP, machine learning &amp; fuzzy matching library for JS [x-post r/javascript]",https://www.reddit.com/r/MachineLearning/comments/4fxiup/talisman_straightforward_modular_nlp_machine/,sixtine,1461312127,,0,6,False,default,,,,,
664,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,17,4fxjt8,aetros.com,AETROS TRAINER - deep neural network GUI - beta registration just started,https://www.reddit.com/r/MachineLearning/comments/4fxjt8/aetros_trainer_deep_neural_network_gui_beta/,neoteat,1461312777,,1,0,False,http://b.thumbs.redditmedia.com/Kr0EX1BJqzPkkyyoX8U0owBwaSUpBugBGNHY6frG8MM.jpg,,,,,
665,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,17,4fxjuj,self.MachineLearning,Best libraries for working with seq2seq,https://www.reddit.com/r/MachineLearning/comments/4fxjuj/best_libraries_for_working_with_seq2seq/,Pryther,1461312809,"I am looking for a good library that implements the seq2seq framework to test around with, are there any opinions which are the fastest/best/easiest to use? Specifically, I am looking for a seq2seq model with attention, without any other bells and whistles (like embedding, which the TensorFlow model with attention has).

TensorFlow seems to have the most extensive selection, or are there any libraries I am not aware of?",7,1,False,self,,,,,
666,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,17,4fxk9u,self.MachineLearning,State of the art sentiment prediction,https://www.reddit.com/r/MachineLearning/comments/4fxk9u/state_of_the_art_sentiment_prediction/,Pieranha,1461313130,What is the current state of the art within sentiment prediction and which benchmark datasets are used?,4,1,False,self,,,,,
667,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,19,4fxvot,self.MachineLearning,What are the preprocessing steps required before feeding images into a CNN?,https://www.reddit.com/r/MachineLearning/comments/4fxvot/what_are_the_preprocessing_steps_required_before/,n00bto1337,1461321112,"I need to identify very specific features in an image, like the neckline of a dress,  as well as the color and other specific features. The images are also of various sizes.
I was thinking of resizing all images to same size, perhaps highlighting the edges. What other things do I need to do?",6,3,False,self,,,,,
668,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,19,4fxwwq,self.MachineLearning,Sports and Machine learning,https://www.reddit.com/r/MachineLearning/comments/4fxwwq/sports_and_machine_learning/,Joostjansenn,1461321910,"Hi all,
for my thesis I am in contact with a company that has loads of data on sports. For all olympic games they have all results for the last 12 years and for football also data on the last 12 years, but not only the results but only a lot of statistics on the games.

Now is my assignment for my thesis that I have to use a machine learning technique and was wondering if you guys could help me a bit with inspiration on a research. 

Currently I am thinking about:
- building a predictive model on football results
- try to build a model on number of visitors of a game and attractiveness of the matches (need to request more data for that)

the company was thinking about these topics, where I doubt if these are applicable for machine learning:
- building a model for a player index on the football matches. Meaning that the players get ratings during the game on how well they perform 
- Making a world ranking for other sports using the ELO-system. Which is the system like in online chess where you get a lot of points if you win from a higher ranked person and not that much when you win from a lower ranked person.

I hope you guys can share your ideas on especially the topics the company thought of and can give me some inspiration on a research question.",3,0,False,self,,,,,
669,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,19,4fxxon,self.MachineLearning,Affordable and dedicated physical compute node,https://www.reddit.com/r/MachineLearning/comments/4fxxon/affordable_and_dedicated_physical_compute_node/,jaxonpolak,1461322398,[removed],0,1,False,default,,,,,
670,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,20,4fy1rx,self.MachineLearning,[Question] Decoding engineering jargon.,https://www.reddit.com/r/MachineLearning/comments/4fy1rx/question_decoding_engineering_jargon/,slugsnot,1461324891,"I have a column in my data which is ""description"". It is filled with natural/unnatural language and obscure terminology. For instance AS might mean ""air speed"", and it might appear as a range ""AS: 400mph to 475mph"". It may also contain multiple parameters comma delineated, or all of the variety of technical jargon. 

There is a tremendous amount of variability, but there is simultaneously a lot of pattern - it's hard to describe, but it is clear that there is a lot of meaning in the descriptions. I don't think bag-of-words will cut it.

I would like to extract relations ie ""air speed"":""400-475"" using entity recognition. 

As a start, I have found iepy [gazettes](http://iepy.readthedocs.org/en/latest/gazettes.html) which allow me to custom define some of that lingo. This is a great start!

I am open to any advice this community might have. Pointers, tips, everything is appreciated.

Thanks!",3,0,False,self,,,,,
671,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,21,4fy4t5,rowingmachinescanada.com,Buy Rowing Machines Canada | Best Rowing Machine For Sale | Indoor Rowers Canada,https://www.reddit.com/r/MachineLearning/comments/4fy4t5/buy_rowing_machines_canada_best_rowing_machine/,Lanedanil,1461326591,,0,0,False,default,,,,,
672,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,21,4fy5m7,self.MachineLearning,[help] Project Mini-Siri,https://www.reddit.com/r/MachineLearning/comments/4fy5m7/help_project_minisiri/,sa7es,1461327006,"Hey Folks!
My first post here. I decided to learn ML in a practical way now. I got an idea for a big project and would like some ideas and suggestion in how to go.

I would like to create a mini-Siri/Cortana. An AI capable of listen an audio and classifying it, and future even completely understand it. 
I am am engineer undergrad in Brazil, and I am thinking in doing it my final project. Here  engineer Bachelor takes 5 years, almost like an bachelor and Master together in some countries in Europe. Because of it my final project should be Master level, but i have not done anything like that before. I got two years to prepare, so I am reading a lot and starting early.
Could you help me with some good datasets (audio/text) for this problem and books/thesis/articles to read about it?

tl;dr -&gt; project mini-siri : Ai classifier for audio. Suggestion in:
* how to go with the project
* books in ML/AI for this area
* articles/thesis for reference
* Audio/Text datasets for ML

Thank you Folks!",2,0,False,self,,,,,
673,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,21,4fyabc,spacemachine.net,Datasets over algorithms,https://www.reddit.com/r/MachineLearning/comments/4fyabc/datasets_over_algorithms/,cavedave,1461329264,,6,7,False,http://b.thumbs.redditmedia.com/lyYQhvZWTaF306sCXFhC5632IXdeEInKUTfqzhFJghM.jpg,,,,,
674,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,21,4fyb0w,self.MachineLearning,Freshman at an average university not in the US; is there no hope for me?,https://www.reddit.com/r/MachineLearning/comments/4fyb0w/freshman_at_an_average_university_not_in_the_us/,ph3rn,1461329597,"I'm at a university in the middle east, majoring in computer science and math. There aren't many professors here doing research in machine learning, and I doubt the UAE would have an companies offering internships in the field. What can I do to make myself stand out, if I ever want to do research in ML or work in industry? 

EDIT: Is it really true that you need papers in top-tier conferences to have ANY chance to go to a top school for a PhD? I have no idea how that's going to work. ",38,2,False,self,,,,,
675,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,22,4fyibt,deeplearningskysthelimit.blogspot.nl,Review of game 1 of the AI match of the 21st century: Lee Sedol underestimates AlphaGo's incredible fighting power,https://www.reddit.com/r/MachineLearning/comments/4fyibt/review_of_game_1_of_the_ai_match_of_the_21st/,DeepLearningBob,1461332776,,1,4,False,http://b.thumbs.redditmedia.com/Z8NvemsWwE_wBxQyouXSJ4RF-lhMd1x9D127Kk8yExw.jpg,,,,,
676,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,23,4fyl7w,self.MachineLearning,"Tensorflow based ""Trump Deep BS Quote RNN Generator""",https://www.reddit.com/r/MachineLearning/comments/4fyl7w/tensorflow_based_trump_deep_bs_quote_rnn_generator/,killianlevacher,1461333983,"Hi Lads, in the process of experimenting with Tensorflow in the last few weeks, I thought I'd share a Tensorflow implementation of Karpathy's excellent pure NumPy character-based RNN. Also just for the fun, I trained it instead on Trump Tweets collected by the NYT. Some of the automatic quotes generated are gas! lol :P http://killianlevacher.github.io/blog/posts/post-2016-03-01/post.html",5,1,False,self,,,,,
677,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,23,4fyqhx,chrome.google.com,ML-enhanced Wikipedia browsing (Chrome extension),https://www.reddit.com/r/MachineLearning/comments/4fyqhx/mlenhanced_wikipedia_browsing_chrome_extension/,benjaminwilson,1461336027,,3,30,False,http://b.thumbs.redditmedia.com/Bxa1AslC9dyY669BGzpD_0fvr5FjphxJGNspgX8tM7A.jpg,,,,,
678,MachineLearning,t5_2r3gv,2016-4-22,2016,4,22,23,4fytfp,self.MachineLearning,Questions thread #4 2016.04.22,https://www.reddit.com/r/MachineLearning/comments/4fytfp/questions_thread_4_20160422/,feedtheaimbot,1461337178,"**Please post your questions here instead of creating a new thread. Helps keep the sub clean. :) Encourage others who create new posts for questions to post here instead!**

Thread will stay alive until next one so keep posting after the date in the title. 

Thanks to everyone for answering questions in the previous thread!

Previous threads:

* [Questions Thread #3 2016.04.07](https://www.reddit.com/r/MachineLearning/comments/4dthzx/questions_thread_3_20160407/)

* [Simple Questions Thread #2 + Meta - 2016.03.23](https://www.reddit.com/r/MachineLearning/comments/4bp1ck/simple_questions_thread_2_meta_20160323/)

* [Simple Questions Thread #1 - 2016.03.08](https://www.reddit.com/r/MachineLearning/comments/49k54u/simple_questions_thread_20160308/)",206,26,False,self,,,,,
679,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,0,4fyuxc,quora.com,Upcoming Quora Session with Daphne Koller (4/27),https://www.reddit.com/r/MachineLearning/comments/4fyuxc/upcoming_quora_session_with_daphne_koller_427/,alxndrkalinin,1461337730,,1,24,False,default,,,,,
680,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,0,4fyzzg,russellsstewart.com,"Maximum likelihood decoding with RNNs - the good, the bad, and the ugly",https://www.reddit.com/r/MachineLearning/comments/4fyzzg/maximum_likelihood_decoding_with_rnns_the_good/,singularai,1461339622,,0,5,False,default,,,,,
681,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,0,4fz1wy,self.MachineLearning,Google TensorFlow interactive online tutorial,https://www.reddit.com/r/MachineLearning/comments/4fz1wy/google_tensorflow_interactive_online_tutorial/,dockker,1461340335,[removed],0,1,False,default,,,,,
682,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,1,4fz5cd,self.MachineLearning,Machine Learning Brainstorming Questions?,https://www.reddit.com/r/MachineLearning/comments/4fz5cd/machine_learning_brainstorming_questions/,coffeecoffeecoffeee,1461341571,"After bombing a bunch of interviews, I'm realizing that I'm not good at quickly looking at a problem and data, deciding on variables to look at, and building a classifier or a regressor.  Given a week or so, I can easily build something useful and people like it, but I'm not good at working on a crunch.  I'm also terrible at verbally expressing my brainstorming process.

Is there a book or a website with a bunch of data analysis questions where I can brainstorm solutions?

I'm looking at questions along the lines of ""Given a list of perfectly-spelled character names from fantasy books, build a model to predict whether or not new names are missing apostrophes."" and ""Given a bunch of celebrities and their dates of death, build a model to predict how many celebrities will die in 2017.""",2,1,False,self,,,,,
683,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,1,4fz6l3,self.MachineLearning,TUPAC16: ML challenge on predicting breast cancer aggressiveness from microscopy images,https://www.reddit.com/r/MachineLearning/comments/4fz6l3/tupac16_ml_challenge_on_predicting_breast_cancer/,mitkovetta,1461341996,"Hey /r/MachineLearning,

We've just launched a challenge on predicting the proliferation (growth) speed of breast cancer tumors from microscopy whole-slide images, which is predictive of breast cancer patients' outcome. The assessment of this biomarker influences the decisions for the treatment plan of the patient (patients with fast growing, aggressive tumors are justifiably treated with more aggressive therapy). Currently this assessment is done manually by pathologists, but we think that it can be done more reliably by automatic image analysis. The training dataset consists of 500 whole-slide histology images (the size of one image can easily exceed 50,000 by 50,000 pixels). We also provide two auxiliary datasets to help with the design of the method.

Details can be found on the challenge website http://tupac.tue-image.nl/ and I'll be happy to answer any questions you might have.",8,25,False,self,,,,,
684,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,1,4fz85d,nextplatform.com,Nvidias Tesla P100 Steals Machine Learning From The CPU,https://www.reddit.com/r/MachineLearning/comments/4fz85d/nvidias_tesla_p100_steals_machine_learning_from/,[deleted],1461342569,[deleted],0,1,False,default,,,,,
685,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,2,4fzilw,self.MachineLearning,"How do you train a model to learn a single concept (such as ""dog"")?",https://www.reddit.com/r/MachineLearning/comments/4fzilw/how_do_you_train_a_model_to_learn_a_single/,thecity2,1461346200,"A human child can learn to recognize a new class or concept (""Johnny, this is called a dog.""). What is the paradigm for training machines to learn new concepts like this? Yes, we can build classifiers to distinguish ""dog"" from ""cat"", but you would need to input data for a discriminatory class. Obviously, you can't build a classifier if you only have one class, but a lot of human learning takes place this way, and I'm trying to find an analogy in the ML/AI world for this idea of concept learning.",10,1,False,self,,,,,
686,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,2,4fzli1,self.MachineLearning,How to prevent SVMs from overfitting,https://www.reddit.com/r/MachineLearning/comments/4fzli1/how_to_prevent_svms_from_overfitting/,[deleted],1461347193,[deleted],7,11,False,default,,,,,
687,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,2,4fzmzj,semanticscholar.org,Semantic Scholar - a search engine on computer science research driven by AI,https://www.reddit.com/r/MachineLearning/comments/4fzmzj/semantic_scholar_a_search_engine_on_computer/,murakamifanboy,1461347697,,3,12,False,http://b.thumbs.redditmedia.com/rQHSMHmhpdjM1uzDkbXLgpC7TIRxZGxmUzzF3lgkZCo.jpg,,,,,
688,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,3,4fzqbo,github.com,Sequence-to-Sequence with Attention in Torch,https://www.reddit.com/r/MachineLearning/comments/4fzqbo/sequencetosequence_with_attention_in_torch/,VAEShmAE,1461348892,,0,5,False,http://a.thumbs.redditmedia.com/apksrnnRAt3a_nUAu6NANrk8LvbkxhN4IHMfCXiSlr0.jpg,,,,,
689,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,3,4fzt53,self.MachineLearning,What hyper-parameters do you usually tune in xgboost?,https://www.reddit.com/r/MachineLearning/comments/4fzt53/what_hyperparameters_do_you_usually_tune_in/,andrewbarto28,1461349885,"I want to publish a paper where I plan to use xgboost as one of the baselines. I want to make a grid search in some of the hyper-parameters. Which ones should I use in the grid search?

Some examples of possible hyper-parameters to tune:

- max_depth
- eta
- num_boost_round",4,2,False,self,,,,,
690,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,3,4fzt7l,i.imgur.com,"This Week in Machine Learning -- April 22nd, 2016",https://www.reddit.com/r/MachineLearning/comments/4fzt7l/this_week_in_machine_learning_april_22nd_2016/,DavidAJoyner,1461349904,,12,59,False,http://b.thumbs.redditmedia.com/ne4JqwJEVFQFARYUMUif-y51J3bEJntINC34Q23JdUs.jpg,,,,,
691,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,3,4fzwup,kdnuggets.com,Top 10 IPython Notebook Tutorials for Data Science and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4fzwup/top_10_ipython_notebook_tutorials_for_data/,[deleted],1461351203,[deleted],0,1,False,default,,,,,
692,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,4,4g05qb,medium.com,"A Machine Learning API to rule them all: Caffe, XGBoost and Tensorflow are in a boat",https://www.reddit.com/r/MachineLearning/comments/4g05qb/a_machine_learning_api_to_rule_them_all_caffe/,pilooch,1461354375,,2,0,False,http://b.thumbs.redditmedia.com/1HFk6cLpYXl33n3WS9rpwPsB3jMbJAmeRY853GPRnhg.jpg,,,,,
693,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,4,4g06p5,self.MachineLearning,Question about Adaptive Computation Time?,https://www.reddit.com/r/MachineLearning/comments/4g06p5/question_about_adaptive_computation_time/,[deleted],1461354750,[deleted],0,0,False,default,,,,,
694,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,6,4g0l9w,self.MachineLearning,"I'm interested in detecting when a speaker says ""uhh"" in audio files. I have some ML experience, but not enough here Where do I start?",https://www.reddit.com/r/MachineLearning/comments/4g0l9w/im_interested_in_detecting_when_a_speaker_says/,[deleted],1461360398,[deleted],0,1,False,default,,,,,
695,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,6,4g0n8j,self.MachineLearning,"I'm interested in detecting when a speaker says ""uhh"" in audio files. I have some ML experience, but not enough here. Where do I start?",https://www.reddit.com/r/MachineLearning/comments/4g0n8j/im_interested_in_detecting_when_a_speaker_says/,Zulban,1461361127,"Here's [the prime minister of Canada](https://www.youtube.com/watch?v=rvNDGoxxfSI). He says ""uhh"" a lot. I think it would be funny to train a classifier on his ""uhhh""s, then download a few hours of footage and automatically splice together a Trudeau ""uhh uhh uhh uhh uhh uhh uhh uhh uhh"" video.

I've done ML before, but never audio/signal processing. I usually work with Python, C# and Linux. Do you recommend any tools or tutorials?

Mostly I'm just not sure how to extract features from audio files. There's a lot of material online and I don't know enough to guide myself.

Thanks!",6,10,False,self,,,,,
696,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,10,4g1ic1,self.MachineLearning,Is there pattern recognition software development tutorial?,https://www.reddit.com/r/MachineLearning/comments/4g1ic1/is_there_pattern_recognition_software_development/,rousse101,1461374983,with online coding ,0,0,False,self,,,,,
697,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,11,4g1q1e,self.MachineLearning,Keras Variable Length Sequence-to-Sequence Learning with TimeDistributed Embeddings.,https://www.reddit.com/r/MachineLearning/comments/4g1q1e/keras_variable_length_sequencetosequence_learning/,darfs,1461378751,"I'm trying to do some sequence - to - sequence learning with Keras, but I'm having trouble figuring out how to encode variable-length sequences.

Essentially, I'm feeding my network a ""story"" (series of sentences), and i'm trying to learn key words at each level (each sentence).

My input data is a 3D Tensor of shape (Samples, Story_Size, Sentence_Size), where Story_Size is of variable length (stories can vary in size from 2 sentences to 12 sentences). My labels are also a 3D Tensor of shape (Samples, Story_Size, Vocab_Size), where the key word is encoded one-hot.

Ideally, my model would look something like this:

    model = Sequential()
    # Input Shape: (_, STORY_SIZE, SENTENCE_SIZE)
    model.add(TimeDistributed(Embedding(input_dim=self.V, output_dim=self.EMBEDDING_SIZE,
                                            input_length=self.SENTENCE_SIZE, mask_zero=True),
                     input_shape=(self.STORY_SIZE, self.SENTENCE_SIZE)))
    # Shape: (_, STORY_SIZE, SENTENCE_SIZE, EMBEDDING_SIZE)
    model.add(TimeDistributed(Flatten()))
    # Shape: (_, STORY_SIZE, SENTENCE_SIZE * EMBEDDING_SIZE)
    model.add(LSTM(self.MEMORY_SIZE, return_sequences=True))
    # Shape: (_, STORY_SIZE, MEMORY_SIZE)
    model.add(TimeDistributed(Dense(self.V, activation='softmax')))
    # Shape: (_, STORY_SIZE, VOCAB_SIZE) --&gt; Softmax

However, this doesn't work as there isn't a way to do TimeDistributed Embeddings, nor do I know how to get a variable-length input encoded.

If anyone knows a way to do this with keras, your input would be very much appreciated. Thanks!",4,7,False,self,,,,,
698,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,12,4g1tnw,self.MachineLearning,GPU Server OS Selection,https://www.reddit.com/r/MachineLearning/comments/4g1tnw/gpu_server_os_selection/,TopGunSnake,1461380525,"I am in the process of setting up a GPU Server, the first on a rack, and I am trying to figure out which OS would work best. I could go down the route of client OS, and simply use a KVM over IP, or utilize a server OS to run jobs. Which OS would you all suggest?",4,1,False,self,,,,,
699,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,12,4g1w8n,self.MachineLearning,5-Day Applied Rationality Workshop for Machine Learning Students &amp; Researchers,https://www.reddit.com/r/MachineLearning/comments/4g1w8n/5day_applied_rationality_workshop_for_machine/,AnnaSalamon,1461381873,"The Center for Applied Rationality is a Berkeley-based nonprofit that runs immersive workshops for entrepreneurs, researchers, students, and other ambitious, analytical, practically-minded people. The practice of applied rationality, which the workshops aim towards, involves noticing what cognitive algorithms you seem to be running, checking whether those algorithms seem to be helping you form accurate beliefs and achieve your goals, and looking for ways to improve them.

A typical 4-day CFAR workshop costs $3900 to attend, but thanks to a generous grant from the Future of Life Institute this fall we will be running a free five-day workshop for students and researchers in the fields of machine learning and artificial intelligence. All costs are covered by this grant, including room, board, and flights.

The workshop will take place this Aug 30 through Sep 4 in the San Francisco Bay Area and will include:  

* 2 days focused on learning models and skills, such as how habits develop and how to redesign your habits.  
* 2 days focused on practicing skills and applying them to whichever areas of your life you would like to make improvements on, such as how to make faster progress on projects or how to have more productive collaborations with colleagues.  
* 1 day (special to this workshop) focused on discussion of the long-term impact of artificial intelligence, and on what reasoning habits  if spread across the relevant research communities  may increase the probability of positive long-term AI outcomes.  

Go [here](http://rationality.org/cfar-for-ml-researchers/) to read more or to apply, or ask questions here.",1,0,False,self,,,,,
700,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,12,4g1ylw,arxiv.org,State of the art facial attribute classification by cross referencing 40 hours of bodycam footage with weather and neighborhood census data,https://www.reddit.com/r/MachineLearning/comments/4g1ylw/state_of_the_art_facial_attribute_classification/,kcimc,1461383106,,0,2,False,default,,,,,
701,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,13,4g20m8,blog.computationalcomplexity.org,"Review of Pedro Domingos' ""The Master Algorithm"": ""does a nice job giving the landscape of machine learning algorithms and putting them in a common text from their philosophical underpinnings to the models that they build on, all in a mostly non-technical way.""",https://www.reddit.com/r/MachineLearning/comments/4g20m8/review_of_pedro_domingos_the_master_algorithm/,DevFRus,1461384203,,3,74,False,http://a.thumbs.redditmedia.com/UIE9c6hZE7erYgs3_VlPdKPtGqq4Qdbz7rOKwzIMz28.jpg,,,,,
702,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,17,4g2rnu,self.MachineLearning,Bayesian Optimization for Python,https://www.reddit.com/r/MachineLearning/comments/4g2rnu/bayesian_optimization_for_python/,_alphamaximus_,1461401384,"I'm trying to solve a one arm bandit problem where the target is a stochastic function.

For Python users there are a lot of options in terms of software:

1. [sigopt](https://sigopt.com/)
2. [spearmint](https://github.com/HIPS/Spearmint)
3. [moe](https://github.com/yelp/MOE)
4. [hyperopt](http://hyperopt.github.io/hyperopt/)
5. [BayesianOptimization](https://github.com/fmfn/BayesianOptimization)
6. [pybo](https://github.com/mwhoffman/pybo)
7. [gpyopt](https://github.com/SheffieldML/GPyOpt)

Does anyone know the difference? Are there any that I'm missing?

From what I've been gathering, Sigopt is too expensive. Spearmint is probably the best open-source option, but has a terrible interface (config files, etc). Moe is similar to Spearmint and I believe Spearmint is a derivative of Moe (?). Hyperopt is not Bayesian Optimization, but an alternative.

BayesianOptimization, pybo, gpyopt are all Python friendly, but I'm not sure how they compare to Spearmint and to each other.

Any thoughts would be great. Thanks!",10,5,False,self,,,,,
703,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,20,4g35fe,github.com,Univariate Gradient Descent algorithm not working properly,https://www.reddit.com/r/MachineLearning/comments/4g35fe/univariate_gradient_descent_algorithm_not_working/,[deleted],1461411631,[deleted],0,1,False,default,,,,,
704,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,21,4g3b0n,self.MachineLearning,Neuron saturation occurs only in last layer or all layers?,https://www.reddit.com/r/MachineLearning/comments/4g3b0n/neuron_saturation_occurs_only_in_last_layer_or/,kennysong,1461414948,"In [Chapter 3](http://neuralnetworksanddeeplearning.com/chap3.html) of the Neural Networks and Deep Learning book, the text repeatedly states that neuron saturation depends only on the activation function of the output layer and the cost function, such as:

""When should we use the cross-entropy instead of the quadratic cost? In fact, the cross-entropy is nearly always the better choice, provided the output neurons are sigmoid neurons.""

""This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic cost is, in fact, an appropriate cost function to use.""

However, it's unclear to me why saturation is only a problem for the output layer. If there are previous hidden layers with sigmoid activations and a quadratic cost function, wouldn't the gradient for those previous layers also have a problem with saturation?",3,3,False,self,,,,,
705,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,22,4g3eca,fortunesky-tech.com,Wire Stripping Machine For Car Charging Column Cable,https://www.reddit.com/r/MachineLearning/comments/4g3eca/wire_stripping_machine_for_car_charging_column/,yueyueniao1112,1461416791,,0,0,False,default,,,,,
706,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,22,4g3elk,peltarion.com,Chor-RNN: Generative Choreography using Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4g3elk/chorrnn_generative_choreography_using_deep/,samim23,1461416924,,9,29,False,default,,,,,
707,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,22,4g3k16,deeplearningskysthelimit.blogspot.nl,Review of Game 2 of the Match of the 21st century: AlphaGo's new move and devastating aggression,https://www.reddit.com/r/MachineLearning/comments/4g3k16/review_of_game_2_of_the_match_of_the_21st_century/,DeepLearningBob,1461419736,,2,7,False,http://b.thumbs.redditmedia.com/6PIHUbWu75XLcFXnKz2hEM3XJik4w0wIs44-o1ndk1k.jpg,,,,,
708,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,23,4g3ktr,multithreaded.stitchfix.com,"Sorry ARIMA, but Im Going Bayesian",https://www.reddit.com/r/MachineLearning/comments/4g3ktr/sorry_arima_but_im_going_bayesian/,pmigdal,1461420121,,3,145,False,http://b.thumbs.redditmedia.com/xQWsEHtsdC-c7Bq8BzKrVY0YqGV9Gjxdzk2fRnE1w4w.jpg,,,,,
709,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,23,4g3kxr,fortunesky-tech.com,Automatic Both-ends-wire Cut Strip Strand Soldering Machine,https://www.reddit.com/r/MachineLearning/comments/4g3kxr/automatic_bothendswire_cut_strip_strand_soldering/,yueyueniao1112,1461420170,,0,0,False,default,,,,,
710,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,23,4g3n72,fortunesky-tech.com,Automatic cut-to-length strip flat ribbon cable terminal crimping machine,https://www.reddit.com/r/MachineLearning/comments/4g3n72/automatic_cuttolength_strip_flat_ribbon_cable/,yueyueniao1112,1461421213,,1,0,False,default,,,,,
711,MachineLearning,t5_2r3gv,2016-4-23,2016,4,23,23,4g3sb9,codingame.com,Machine Learning comes to CodinGame,https://www.reddit.com/r/MachineLearning/comments/4g3sb9/machine_learning_comes_to_codingame/,steven2358,1461423464,,3,8,False,default,,,,,
712,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,0,4g3v77,self.MachineLearning,Parameters for J48 Tree (WEKA),https://www.reddit.com/r/MachineLearning/comments/4g3v77/parameters_for_j48_tree_weka/,Bohemian90,1461424723,"Hello

I'm using the [J48 Tree](http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/J48.html) of WEKA for classification. It support lots of parameters.

Which parameter ranges should I search for? I'm especially unsure about the following settings:

- runing confidence
- minimum number of instances",3,2,False,self,,,,,
713,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,0,4g3vlr,self.MachineLearning,"PEC posted a method for prediction the GOP primaries with Google Correlate, could machine learning be used to make more accurate predictions than polls?",https://www.reddit.com/r/MachineLearning/comments/4g3vlr/pec_posted_a_method_for_prediction_the_gop/,[deleted],1461424900,[deleted],0,0,False,default,,,,,
714,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,0,4g3xww,fortunesky-tech.com,Automatic wire single-end terminal crimping single-end soldering machine,https://www.reddit.com/r/MachineLearning/comments/4g3xww/automatic_wire_singleend_terminal_crimping/,yueyueniao1112,1461425885,,0,1,False,default,,,,,
715,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,1,4g42kn,fortunesky-tech.com,Pneumatic Jacket Multicore Wire Stripping Machine,https://www.reddit.com/r/MachineLearning/comments/4g42kn/pneumatic_jacket_multicore_wire_stripping_machine/,yueyueniao1112,1461427794,,0,1,False,default,,,,,
716,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,1,4g443t,self.MachineLearning,"Measuring performance in the real world, outside of the lab",https://www.reddit.com/r/MachineLearning/comments/4g443t/measuring_performance_in_the_real_world_outside/,Gavekort,1461428419,"I am looking to do fitness-evaluation of a four-legged robot that uses an open loop passive gait pattern. Fitness is measured in terms of how far it can move in a straight line in a certain amount of time or iterations. Evaluating the performance is fairly easy in a simulator or in a laboratory with motion capture equipment, but are there any researched methods of doing fitness-evaluation in the real world, without having motion capture equipment? Like for instance using computer vision, GPS or other localization-methods?",2,3,False,self,,,,,
717,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,1,4g4510,nucl.ai,"Minecraft, ENHANCE! Neural Networks to Upscale &amp; Stylize Pixel Art",https://www.reddit.com/r/MachineLearning/comments/4g4510/minecraft_enhance_neural_networks_to_upscale/,linuxjava,1461428807,,15,76,False,http://b.thumbs.redditmedia.com/rH5jeTXc4xqgNcP1GEQpnt3vs9RFmw1l4LaU_bn_3HA.jpg,,,,,
718,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,2,4g4bkr,self.MachineLearning,Optical Character Recognization,https://www.reddit.com/r/MachineLearning/comments/4g4bkr/optical_character_recognization/,tehtacolord,1461431488,"Hi guys, 

I have about 4000 pictures with some letters and numbers in them I need to extract to textfiles. I've read a lot about OCR recently and found H&amp;P 's Tesseract as well as a lot other OCR libraries.
Since I don't want to simply ""use"" a library without understanding what my algorithm does (and Tesseract is hell for me since it's nearly undocumented and not so easy to use, besides there's no decent API for C#), is there any possiblity to implement a kind of ""AI"" to determine which character I've read in?

I've already written a part of my program in C#, limiting the box (as a .bmp-file) which contains the character that needs to be recognized. Should I just compare the chosen box to a set of given characters or would it be more efficient to implement a kind of ""AI"" to determine the letter given in the box? (The question of efficiency refers to run-time primarily).


Thanks in advance
",0,0,False,self,,,,,
719,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,2,4g4ffr,translate.google.com,End of anonymity: Identification of random passengers [Translated from Russian],https://www.reddit.com/r/MachineLearning/comments/4g4ffr/end_of_anonymity_identification_of_random/,vodkagoodmeatrotten,1461433055,,1,4,False,default,,,,,
720,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,4,4g4xa4,cms.web.cern.ch,CMS releases new batch of research data(300 TB) from LHC | CMS Experiment,https://www.reddit.com/r/MachineLearning/comments/4g4xa4/cms_releases_new_batch_of_research_data300_tb/,InaneMembrane,1461440247,,0,11,False,default,,,,,
721,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,8,4g5t1e,github.com,Cool XGBoost Guide (Gradient Boosted Trees),https://www.reddit.com/r/MachineLearning/comments/4g5t1e/cool_xgboost_guide_gradient_boosted_trees/,Jxieeducation,1461453681,,2,21,False,http://a.thumbs.redditmedia.com/7XXiMJDpkLWovGbmVOKsrz0GOOHWzrP4ZPO7IgjkDr4.jpg,,,,,
722,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,9,4g64dz,wsj.com,"To beat humans at StarCraft, AI needs to learn how to lie",https://www.reddit.com/r/MachineLearning/comments/4g64dz/to_beat_humans_at_starcraft_ai_needs_to_learn_how/,jarins,1461458710,,18,8,False,http://b.thumbs.redditmedia.com/7dDpk8iuhh0CN6yZjRfYQ-Z0BDv1YWGQ4vEVuIHk8MA.jpg,,,,,
723,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,13,4g6t6k,self.MachineLearning,Any internships in Machine Learning I can apply for?,https://www.reddit.com/r/MachineLearning/comments/4g6t6k/any_internships_in_machine_learning_i_can_apply/,ameya_shanbhag,1461471427,"I am in my penultimate year of CS undergrad program from India.
I was planning for an internship in ML so any good companies where I can apply?",5,1,False,self,,,,,
724,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,14,4g712d,self.MachineLearning,Some short ML/DL project in theano?,https://www.reddit.com/r/MachineLearning/comments/4g712d/some_short_mldl_project_in_theano/,shash273,1461476426,I am planning to learn theano for my research work in deep learning in text mining. Can anyone suggest some short projects in this area which I can code up in theano?,2,0,False,self,,,,,
725,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,15,4g740n,youtube.com,Nice Introductory lecture on Dirichlet Processes,https://www.reddit.com/r/MachineLearning/comments/4g740n/nice_introductory_lecture_on_dirichlet_processes/,hyphypants,1461478427,,1,77,False,http://b.thumbs.redditmedia.com/qCPDlAl__Eim21bBkqmjjSCSOq-lU1iVjLEPIV0Wi9M.jpg,,,,,
726,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,15,4g768n,self.MachineLearning,I am attempting to make a 16-H-16 Autoencoder in Python using PyBrain.,https://www.reddit.com/r/MachineLearning/comments/4g768n/i_am_attempting_to_make_a_16h16_autoencoder_in/,[deleted],1461479992,[removed],0,1,False,default,,,,,
727,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,16,4g78q0,self.MachineLearning,I am attempting to make a 16-H-16 Autoencoder in Python using PyBrain. But I've run into a problem.,https://www.reddit.com/r/MachineLearning/comments/4g78q0/i_am_attempting_to_make_a_16h16_autoencoder_in/,[deleted],1461481776,[removed],0,1,False,default,,,,,
728,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,16,4g798i,self.MachineLearning,I am attempting to make a 16-H-16 Autoencoder in Python using PyBrain. But I've run into a bit of a problem.,https://www.reddit.com/r/MachineLearning/comments/4g798i/i_am_attempting_to_make_a_16h16_autoencoder_in/,FindingANicePlace,1461482140,"I am attempting to make a 16-H-16 Autoencoder in Python using PyBrain. It is supposed be on the identity function that maps 0 to 0, 1 to 1, ..., 15 to 15 in the following form:

0 = (1,0,0,...,0) 1 = (0,1,0,...,0) ... 15 = (0,0,0,..., 1)

I'm currently stuck on successfully training it, mostly on how to tell if it trained or not. I was reading hidden layers activation values using print(nnet['hidden0'].outputbuffer[nnet['hidden0'].offset]) but it doens't seem to be helping me, the outputs also may as well be jibberish to me at this point.
What would be a good way to tell if it is trained correctly?",5,2,False,self,,,,,
729,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,17,4g7gi6,bandieuhoanhietdo.blogspot.com,i nt hay ho nht ca iu ha Nagakawa ti Vit Nam | Bn iu ha nhit  gi r,https://www.reddit.com/r/MachineLearning/comments/4g7gi6/i_nt_hay_ho_nht_ca_iu_ha_nagakawa_ti/,quaninet2688,1461487399,,0,1,False,default,,,,,
730,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,18,4g7in8,ml4a.github.io,"Machine Learning for Artists course lectures, ITP-NYU - Spring 2016",https://www.reddit.com/r/MachineLearning/comments/4g7in8/machine_learning_for_artists_course_lectures/,sharno,1461488992,,0,32,False,http://b.thumbs.redditmedia.com/W3js7ZQDBf8aFHphnJgkBaslZVlBOOawlrTTvjGRSAo.jpg,,,,,
731,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,18,4g7kog,self.MachineLearning,Any Caffe Tutorials for python programmers?[Note:Have very little knowledge in C/C++],https://www.reddit.com/r/MachineLearning/comments/4g7kog/any_caffe_tutorials_for_python/,code2hell,1461490531,"Can anyone suggest some good caffe tutorials for:
1) Creating own model
2) Using pre-trained weights and partially training an existing model

Thanks!",8,2,False,self,,,,,
732,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,20,4g7s8z,self.MachineLearning,Stacked Learning limitations,https://www.reddit.com/r/MachineLearning/comments/4g7s8z/stacked_learning_limitations/,xristos_forokolomvos,1461496452,"When I read articles about Kaggle solutions it almost always involves stacked learning. But what I've noticed is that it usually means 1 or 2 layers of stacking. 

What stops them from adding more?",7,2,False,self,,,,,
733,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,22,4g87tn,self.MachineLearning,How to use Machine Learning in practice?,https://www.reddit.com/r/MachineLearning/comments/4g87tn/how_to_use_machine_learning_in_practice/,voider1,1461506020,"I'd like to know how to use ML in practice, let's say I use Scikit Learn, and I fit it, and then I use it to make predictions. Do I just take the prediction, check what it is, and then do something?

    if preds == x:
        print(""Hello"")
    elif preds == y:
        print(""Bye"")
    else:
        print(""What?"")

Like this?",3,0,False,self,,,,,
734,MachineLearning,t5_2r3gv,2016-4-24,2016,4,24,23,4g8ezs,deeplearningskysthelimit.blogspot.nl,Review of Game 3 of the Match of the 21st Century: Lee Sedol's opening mistakes due to enormous mental pressure,https://www.reddit.com/r/MachineLearning/comments/4g8ezs/review_of_game_3_of_the_match_of_the_21st_century/,DeepLearningBob,1461509381,,6,28,False,http://b.thumbs.redditmedia.com/Ngq3iHyNLIjUqn7gpMJ5Z7L6rWBZ62kX7s__s5sH9qY.jpg,,,,,
735,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,3,4g98be,linkedin.com,"""No Match for Machine Learning: How the Future of Computing is Solving Difficult Problems from Terrorism to Cancer to Climate Change"" by @ericmileymba",https://www.reddit.com/r/MachineLearning/comments/4g98be/no_match_for_machine_learning_how_the_future_of/,ericmileymba,1461521447,,2,0,False,default,,,,,
736,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,3,4g9a8w,mucmd.org,MUCMD - Machine Learning in Health Care,https://www.reddit.com/r/MachineLearning/comments/4g9a8w/mucmd_machine_learning_in_health_care/,iidealized,1461522105,,1,3,False,default,,,,,
737,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,3,4g9akd,intelligence.org,New papers dividing logical uncertainty into two subproblems - Machine Intelligence Research Institute,https://www.reddit.com/r/MachineLearning/comments/4g9akd/new_papers_dividing_logical_uncertainty_into_two/,InaneMembrane,1461522219,,14,51,False,http://b.thumbs.redditmedia.com/wCYidZTpljKPKULFtb5P9DjcZVPhPOkZqn-uSf8i2nE.jpg,,,,,
738,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,3,4g9b1o,self.MachineLearning,How is Edinburgh for a MSc in Machine Learning/AI compared to others in UK/Europe? (x-post from /r/cscareerquestions),https://www.reddit.com/r/MachineLearning/comments/4g9b1o/how_is_edinburgh_for_a_msc_in_machine_learningai/,BigMakondo,1461522388,"I've heard Master's in Edinburgh are more research oriented. How does this affect your chances to work in companies/start-ups?

Are the teachers good, compared to other places?

Please, share your experiences (not necessarily about Edinburgh)!",8,16,False,self,,,,,
739,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,3,4g9bik,self.MachineLearning,Multi-input Tensorflow LSTM,https://www.reddit.com/r/MachineLearning/comments/4g9bik/multiinput_tensorflow_lstm/,[deleted],1461522547,[deleted],7,5,False,default,,,,,
740,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,4,4g9h3e,self.MachineLearning,How to get remote jobs in machine learning?,https://www.reddit.com/r/MachineLearning/comments/4g9h3e/how_to_get_remote_jobs_in_machine_learning/,74throwaway,1461524568,"Although I currently work in data analytics and wanted to work in machine learning, I heard that data scientists and machine learning engineers have to give lots of presentations and be at the office to explain machine learning concepts to management and non-technical people, so remote opportunities are scarce. By working remotely, I don't want to just work from home. I was hoping I could live abroad for periods of at least a few months. 

Is this true? Are there remote jobs for machine learning engineers and data scientists?  What skills are most benificial for a remote career? Would knowing Hadoop and Spark well, as opposed to R and Python, lead to remote positions?
",16,23,False,self,,,,,
741,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,4,4g9oeq,self.MachineLearning,What is your opinion on Coursera 'Statistics with R' specialization ?,https://www.reddit.com/r/MachineLearning/comments/4g9oeq/what_is_your_opinion_on_coursera_statistics_with/,zibenmoka,1461527313,Do you have any thoughts to share on that one? thanks ,1,1,False,self,,,,,
742,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,5,4g9rcx,self.MachineLearning,"what is ""posterior distribution of the latent variables in hidden layer"" in RBM",https://www.reddit.com/r/MachineLearning/comments/4g9rcx/what_is_posterior_distribution_of_the_latent/,John_Smith111,1461528392,"I have a  infinite deep belief net with light weights   (not RBM )
what is the "" posterior distribution of the latent variables in hidden layer"" is it prior * likelihood terms?

how can we express it with formula?

",10,0,False,self,,,,,
743,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,5,4g9xjl,self.MachineLearning,Halving weights of hidden layers during testing,https://www.reddit.com/r/MachineLearning/comments/4g9xjl/halving_weights_of_hidden_layers_during_testing/,akgoel,1461530764,"I saw G. Hinton's UBC Techtalk, in which he mentions the merits of using dropout for training - https://youtu.be/vShMxxqtDDs?t=36m

He mentions that we halve the weights of hidden layers, as it effectively approximates the Geometric Mean of 2^h possible models (from h hidden units in a layer).

Is this an accepted practice, to divide layer weights by half during testing? Or am I understanding wrong?",6,0,False,self,,,,,
744,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,9,4gawog,self.MachineLearning,How to explain Gradient Boosting in an interview?,https://www.reddit.com/r/MachineLearning/comments/4gawog/how_to_explain_gradient_boosting_in_an_interview/,74throwaway,1461544686,"I'm having trouble preparing for a possible interview question in which I could be asked to explain Gradient Boosting. I think I can explain other machine learning algorithms relatively concisely and well enough so that I talk for no longer than a minute. 

I did some google searching on gradient boosting and all I could find were overly technical explanations. For example, in the link below, the first answer gives a good concise answer for adaboost but not gradient boosting. The second answer seems way too long and technical to explain to an interviewer:
https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting

Any suggestions?
",9,17,False,self,,,,,
745,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,10,4gb3x2,self.MachineLearning,Easiest practical way to assign a probability to a phrase based on a small corpus?,https://www.reddit.com/r/MachineLearning/comments/4gb3x2/easiest_practical_way_to_assign_a_probability_to/,carlos_argueta,1461547800,"Hi all,
I am looking for a practical and simple way to give a phrase a score based on the similarity to a corpus. For instance, let's say I have a small chat log as a corpus. Given two phrases ""i am doing fine"" and ""car breaks tomorrow"", the first sentence should have a higher score (or probability) than the second. 
I am interested in something easy to implement (or already implemented) solution, I m not doing research or trying to beat the state-of-the-art so deep learning and other complex solutions are probably not what I want.

Thanks a lot for your help.",8,0,False,self,,,,,
746,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,12,4gblw9,self.MachineLearning,Why is the unit of gradient descent in y instead of in x?,https://www.reddit.com/r/MachineLearning/comments/4gblw9/why_is_the_unit_of_gradient_descent_in_y_instead/,leaugleg,1461555958,"Consider the following cost function 

    J = x^2 + m  (m in Real)

this is a parabola facing upwards.

The gradient of this is `2x`, the unit of the gradient is `y/x` if we apply gradient descent on some random point `x1`

    x1 = x1 - gradient(J)(x1)

the unit doesn't match

since we have `x1`'s value represent in x unit and `gradient(J)(x1)`'s value represent in y unit.

The direction of the gradient make sense, but the value seems to have the wrong unit.

Am I getting something wrong?",4,0,False,self,,,,,
747,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,13,4gbpjd,self.MachineLearning,Best resources for Machine Learning on Windows 10,https://www.reddit.com/r/MachineLearning/comments/4gbpjd/best_resources_for_machine_learning_on_windows_10/,[deleted],1461557723,[deleted],1,0,False,default,,,,,
748,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,13,4gbum4,self.MachineLearning,Could a New Deep Learning Religion Achieve Tax Exempt Status?,https://www.reddit.com/r/MachineLearning/comments/4gbum4/could_a_new_deep_learning_religion_achieve_tax/,alexmlamb,1461560381,[removed],1,0,False,default,,,,,
749,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,16,4gcacr,self.MachineLearning,"Help me pls: Stochastic gradient descent for online learning of binary, non-stationary problem?",https://www.reddit.com/r/MachineLearning/comments/4gcacr/help_me_pls_stochastic_gradient_descent_for/,kit_hod_jao,1461569970,"Hi

I want to use a simple 3 layer (1 input, 2 hidden) feed forward network with training by backprop.

I'm a bit concerned about the input data posed by the problem - specifically that the data will cause problems with convergence. 

The problem is: 

Given an input matrix of sparse binary values (i.e. either 0 or 1), produce another matrix of equal dimension containing another set of sparse binary values. 

The input and (correct) output matrices will be very orthogonal over time -i.e. if a bit equals 1, it will almost certainly be 0 for all other inputs.

Now for the nasty bit:

a) the training has to be online, without batches: It's online time-series data, so only one input and output pair is available at any time. 
b) Although there will be patterns in the sequences, over time the patterns will drift. So, bits that were '1's (at a particular point in a sequence) will become '0's and vice-versa. (I think this is called a non-stationary problem).

I'm worried that the learned weights will saturate while patterns are stable, causing problems with convergence when the observed sequences of input and output change. I know this is basically stochastic gradient descent, but the conditions above are a bit extreme.

Would these issues be mitigated using weight regularization? (if the weights don't get too big or too small it would be easier to change them) 

Also, what cost and activation functions are most appropriate for this type of problem?

Many thanks in advance for advice.
",0,0,False,self,,,,,
750,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,17,4gccgi,haifengl.github.io,Clustering algorithms visual comparison,https://www.reddit.com/r/MachineLearning/comments/4gccgi/clustering_algorithms_visual_comparison/,pdsminer,1461571378,,1,22,False,http://a.thumbs.redditmedia.com/XcKMmrgyMEF5iFEkyRm-tP0sJv6d6C_oDhr0tmy1CF8.jpg,,,,,
751,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,17,4gcek1,github.com,Evolutionary Computation Tool and Framework - checkout demos in the readme file,https://www.reddit.com/r/MachineLearning/comments/4gcek1/evolutionary_computation_tool_and_framework/,cstefanache,1461572871,,0,2,False,http://a.thumbs.redditmedia.com/CC2C1XWSEYG7HBYb_wifJ9Usluq1ZgE9MIQjpE7AZ64.jpg,,,,,
752,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,18,4gcjal,self.MachineLearning,Using one or multiple optimizers,https://www.reddit.com/r/MachineLearning/comments/4gcjal/using_one_or_multiple_optimizers/,Pukkiepukkert,1461576141,[removed],1,1,False,default,,,,,
753,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,19,4gcqd4,building-babylon.net,Explanation of softmax parameterisation and optimisation,https://www.reddit.com/r/MachineLearning/comments/4gcqd4/explanation_of_softmax_parameterisation_and/,[deleted],1461580569,[deleted],0,2,False,default,,,,,
754,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,19,4gcs95,self.MachineLearning,post-doc and phd posts,https://www.reddit.com/r/MachineLearning/comments/4gcs95/postdoc_and_phd_posts/,jeffreysbowers,1461581667,[removed],0,1,False,default,,,,,
755,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,21,4gd4o5,twitter.com,"Reverse OCR, a bot which generates squiggles until an OCR word recognition recognises a word.",https://www.reddit.com/r/MachineLearning/comments/4gd4o5/reverse_ocr_a_bot_which_generates_squiggles_until/,TheGeorge,1461588160,,25,219,False,http://b.thumbs.redditmedia.com/_Jk9b2VVu6Xi_cARbEPdBPa54SzbVpKnFuq7K0BebmY.jpg,,,,,
756,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,22,4gd8uu,datasciencecentral.com,21 data science systems used by Amazon to operate its business,https://www.reddit.com/r/MachineLearning/comments/4gd8uu/21_data_science_systems_used_by_amazon_to_operate/,omkay256,1461589954,,0,0,False,default,,,,,
757,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,22,4gde8c,terryum.io,The basic workflow of the TensorFlow codes,https://www.reddit.com/r/MachineLearning/comments/4gde8c/the_basic_workflow_of_the_tensorflow_codes/,terryum,1461592329,,1,6,False,http://b.thumbs.redditmedia.com/B0m1WIj99imulQjt63cpVbhxufCBqZDuEkS-a7Kiiaw.jpg,,,,,
758,MachineLearning,t5_2r3gv,2016-4-25,2016,4,25,23,4gdmce,self.MachineLearning,Bayesian Data Analysis [video lectures],https://www.reddit.com/r/MachineLearning/comments/4gdmce/bayesian_data_analysis_video_lectures/,Kiuhnm,1461595564,"I've just found out that the author of [Statistical Rethinking: A Bayesian Course with Examples in R and Stan](http://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445/) has released 26+ hours of [lectures](https://www.youtube.com/playlist?list=PLDcUM9US4XdMdZOhJWJJD4mDBMnbTWw_z) on the same topic.

I believe that every Machine Learning enthusiast would benefit from knowing a little bit of Bayesian Data Analysis (BDA). The book and video lectures are just at the right level for a gentle (but serious) introduction to BDA.

Enjoy!",0,14,False,self,,,,,
759,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,0,4gdr84,self.MachineLearning,[Survey] To everyone who is working or has worked with large-scale data analysis.,https://www.reddit.com/r/MachineLearning/comments/4gdr84/survey_to_everyone_who_is_working_or_has_worked/,quesoburguesa,1461597336,"Good day. 

I am currently doing research with an NSF program. We are aiming to assess the current situation of data analysts from all fields and backgrounds, focusing on discovering their current access to resources, struggles, limitations, tool preferences and overall thoughts. 

No personal information will be collected, and if the answer to any question involves disclosing any sensitive information, please do not hesitate to indicate this and continue with the rest of the survey. 

Looking forward to your input! You can access the survey [here](https://docs.google.com/forms/d/1TkPhAXUL01EsbGqZF_s8J4XXA-9ngYyR5pgEMA890gM/viewform?ths=true&amp;edit_requested=true).",0,0,False,self,,,,,
760,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,0,4gdxf1,medium.com,A look at autoencoders and word embeddings!,https://www.reddit.com/r/MachineLearning/comments/4gdxf1/a_look_at_autoencoders_and_word_embeddings/,[deleted],1461599550,[deleted],0,1,False,default,,,,,
761,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,1,4ge112,self.MachineLearning,3D Convolutional Deep Neural Network,https://www.reddit.com/r/MachineLearning/comments/4ge112/3d_convolutional_deep_neural_network/,criticalcontext,1461600838,"I am trying to do some independent research on computational creativity, applying concepts like those found in alexjc/neural-doodle and jcjohnson/neural-style to create artistic 3D voxel structures. For this, I believe I need to run convolutional layers, only in a 3D fashion, along each tensor. I have seen some do this by using a 2D conv layer, with the 3rd dimension being defined as channels, however I am unsure as to the effectiveness of this. Others most likely have resized the 3D tensor into a 2D matrix of some channel height N, and run NxN 2D conv on that matrix. Have any of you heard of doing raw 3D convolution in theano or tensorflow?",2,0,False,self,,,,,
762,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,1,4ge2vc,youtube.com,Bayes Impact Machine Learning Hackathon,https://www.reddit.com/r/MachineLearning/comments/4ge2vc/bayes_impact_machine_learning_hackathon/,llSourcell,1461601482,,1,0,False,http://b.thumbs.redditmedia.com/IyEe5Js5OQy197WG0ZRr7he6G9hiq3NLlHBdZrcYkEo.jpg,,,,,
763,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,1,4ge4cj,medium.com,A look at autencoders - one of the coolest applications of neural networks,https://www.reddit.com/r/MachineLearning/comments/4ge4cj/a_look_at_autencoders_one_of_the_coolest/,sup6978,1461601973,,4,31,False,http://b.thumbs.redditmedia.com/7fYgL_LVMUR30YmZteCYXbaiFmmz-qGGUyxIQ5JqkXc.jpg,,,,,
764,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,1,4ge924,youtube.com,Two+ Minute Papers - No Such Thing As Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/4ge924/two_minute_papers_no_such_thing_as_artificial/,com2mentator,1461603584,,4,9,False,http://b.thumbs.redditmedia.com/gSq44-JQqM7iay07RS8LOrNryyCm-W7slqdFOy7OadY.jpg,,,,,
765,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,2,4gee9r,self.MachineLearning,What is min_child_weight in xgboost?,https://www.reddit.com/r/MachineLearning/comments/4gee9r/what_is_min_child_weight_in_xgboost/,Tokukawa,1461605382,"I find a good [tutorial on xgboost](http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/), but I can't understand the min_child_weight parameter of xgboost. Actually I can't understand of which weights the parameter is referencing to.",3,0,False,self,,,,,
766,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,3,4geq93,blog.keras.io,Keras as a simplified interface to TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4geq93/keras_as_a_simplified_interface_to_tensorflow/,fchollet,1461609524,,4,51,False,default,,,,,
767,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,3,4getbv,research.google.com,GPUCC - An Open-Source GPGPU Compiler,https://www.reddit.com/r/MachineLearning/comments/4getbv/gpucc_an_opensource_gpgpu_compiler/,m_ke,1461610641,,33,74,False,http://b.thumbs.redditmedia.com/V_a_XdWkMfB43Z1j5kW47yVOJnIX6VjJBlDBiLqiqwU.jpg,,,,,
768,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,5,4gf81o,self.MachineLearning,Confusion Matrix of RBFNN only guessing positive,https://www.reddit.com/r/MachineLearning/comments/4gf81o/confusion_matrix_of_rbfnn_only_guessing_positive/,[deleted],1461615637,[deleted],0,1,False,default,,,,,
769,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,6,4gffdf,untapt.com,A Neural Network that Dreams in Resumes,https://www.reddit.com/r/MachineLearning/comments/4gffdf/a_neural_network_that_dreams_in_resumes/,[deleted],1461618177,[deleted],0,0,False,default,,,,,
770,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,6,4gfmoh,self.MachineLearning,Many-GPUs setup for machine learning?,https://www.reddit.com/r/MachineLearning/comments/4gfmoh/manygpus_setup_for_machine_learning/,MasterScrat,1461620722,"If I had a system with multiple GPUs (think 4 to 6), would it be usable to scale up machine learning algo?

I'm thinking of such systems, from the bitcoin mining era: http://www.rigs.ch/72-thickbox_default/chassis-pour-6-gpu-basse-consommation.jpg ",2,1,False,self,,,,,
771,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,7,4gfpsn,self.MachineLearning,Is there any research on (or a field name for) machine learning approached to generating stories?,https://www.reddit.com/r/MachineLearning/comments/4gfpsn/is_there_any_research_on_or_a_field_name_for/,BrassTeacup,1461621868,"I'd like to look into generating stories using machine learning techniques, does anyone have any links or search areas I should look into?

Specifically, I mean generating stories with characters that interact.

Thanks!",2,2,False,self,,,,,
772,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,7,4gfq4z,arxiv.org,[1511.05212] Binary embeddings with structured hashed projections,https://www.reddit.com/r/MachineLearning/comments/4gfq4z/151105212_binary_embeddings_with_structured/,FluxSeeds,1461621986,,0,7,False,default,,,,,
773,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,8,4gfyqt,self.MachineLearning,[Conspiracy] Why did Alex Graves leave IDSIA and go to Hinton's lab who is Schmidhuber's staunch rival?,https://www.reddit.com/r/MachineLearning/comments/4gfyqt/conspiracy_why_did_alex_graves_leave_idsia_and_go/,[deleted],1461625395,[deleted],0,0,False,default,,,,,
774,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,8,4gg1f9,self.MachineLearning,"Is this an ARMA(2,2) or ARMA(2,1)?",https://www.reddit.com/r/MachineLearning/comments/4gg1f9/is_this_an_arma22_or_arma21/,[deleted],1461626454,[deleted],1,1,False,default,,,,,
775,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,8,4gg5hf,youtube.com,[video] Acceleration of Deep Learning,https://www.reddit.com/r/MachineLearning/comments/4gg5hf/video_acceleration_of_deep_learning/,S__OBrien,1461628123,,0,1,False,http://b.thumbs.redditmedia.com/gsDRY8IOq9R2KXbqMFQGw3_P4koWyF7ahUDKNYBN5Mc.jpg,,,,,
776,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,9,4ggb0r,self.MachineLearning,LSTM for time series forecasting?,https://www.reddit.com/r/MachineLearning/comments/4ggb0r/lstm_for_time_series_forecasting/,butWhoWasBee,1461630392,"How well do LSTM networks perform in time series forecasting compared to standard RNN or sigmoid networks being used on a sliding window? I understand that in theory they should have longer memory than RNN, but have you found them sucesful in practice?",4,15,False,self,,,,,
777,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,9,4ggdnj,arxiv.org,[1604.06778] Benchmarking Deep Reinforcement Learning for Continuous Control,https://www.reddit.com/r/MachineLearning/comments/4ggdnj/160406778_benchmarking_deep_reinforcement/,dementrock,1461631490,,10,39,False,default,,,,,
778,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,12,4ggyjj,arxiv.org,[1604.07316] End to End Learning for Self-Driving Cars,https://www.reddit.com/r/MachineLearning/comments/4ggyjj/160407316_end_to_end_learning_for_selfdriving_cars/,m_ke,1461640057,,19,21,False,default,,,,,
779,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,14,4ghdy7,machinemonitor.com,"Asset &amp; Maintenance Management Systems | Electrical Engineering, Inspection &amp; Testing Services",https://www.reddit.com/r/MachineLearning/comments/4ghdy7/asset_maintenance_management_systems_electrical/,Romanora,1461647536,,0,1,False,default,,,,,
780,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,16,4ghpjl,self.MachineLearning,One vs multiple optimizers,https://www.reddit.com/r/MachineLearning/comments/4ghpjl/one_vs_multiple_optimizers/,Pukkiepukkert,1461654366,"We are building a neural network with some LSTM layers and we are having mixed results when investigating different optimizers. Some are very fast in the begining, some are fast but jump all over the place and some make slow but steady improvements.
Our idea was, would it be beneficial to just switch between optimizers. For example, learn 1000 epochs with a SGD optimizer, then 1000 epochs with RMSprop, 1000 with an Adam optimizer and repeat this? Any ideas or experiences with this?",9,8,False,self,,,,,
781,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,16,4ghtuj,self.MachineLearning,Can machines think as Humans?? What do you think guys? Is it possible to make machines think as humans in near future??,https://www.reddit.com/r/MachineLearning/comments/4ghtuj/can_machines_think_as_humans_what_do_you_think/,niderfan,1461657277,,10,0,False,self,,,,,
782,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,16,4ghtvj,self.MachineLearning,Semantic Hashing Implementation,https://www.reddit.com/r/MachineLearning/comments/4ghtvj/semantic_hashing_implementation/,usmanshaahid,1461657295,"Does anyone know where to find implementation of this paper by Ruslan Salakhudinov http://www.cs.toronto.edu/~fritz/absps/sh.pdf

EDIT: I have tried using this code https://github.com/gynnash/AutoEncoder but it doesn't work out of the box and also a bit jumbled up. There is no explanation.",0,1,False,self,,,,,
783,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,18,4gi297,self.MachineLearning,"Training error of my feed forward nn is not reduced, is anybody help? with DeepLearning4J",https://www.reddit.com/r/MachineLearning/comments/4gi297/training_error_of_my_feed_forward_nn_is_not/,ktsh0133,1461663312,"I have a lot of training sets, but training error is all same.

 
(new NeuralNetConfiguration.Builder().seed(seed)
              .iterations(iterations)              .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
              .useDropConnect(true)
              .l1(0.001).regularization(true).l2(1e-3)
             .updater(Updater.ADADELTA).weightInit(WeightInit.XAVIER).learningRate(0.05).momentum(0.5).dropOut(0.5)
                .list(3)
              .layer(layerInd.getAndIncrement(), new DenseLayer.Builder().nIn(170).nOut(100)
                .activation(""relu"")
                .build())
              .layer(layerInd.getAndIncrement(), new DenseLayer.Builder().nIn(100).nOut(30)
                .activation(""relu"")
                .build())
              .layer(layerInd.getAndIncrement(), new OutputLayer.Builder(LossFunctions.LossFunction.RMSE_XENT)
                .activation(""softmax"")
                .nIn(30).nOut(outputNum).build())
              .backprop(true).pretrain(false)
              .build())

18:32:56.201 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 10 is 1.3304473154396808
18:32:59.156 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 15 is 1.3343679404264677
18:33:02.317 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 20 is 1.3290371591628944
18:33:05.330 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 25 is 1.3320109091534436
18:33:23.862 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 0 is 1.333015909141921
18:33:26.722 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 5 is 1.3329320028800808
18:33:29.853 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 10 is 1.3328699716132333
18:33:32.974 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 15 is 1.3324134090990687
18:33:36.140 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 20 is 1.3327987215904784
18:33:39.211 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 25 is 1.3261973153299833
18:33:58.562 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 0 is 1.334705596567981
18:34:01.669 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 5 is 1.3347549715537286
18:34:04.715 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 10 is 1.3332185652945088
18:34:07.694 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 15 is 1.3340670027809967
18:34:10.952 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 20 is 1.3335301277726423
18:34:14.307 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 25 is 1.3306999715086663
18:34:33.743 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 0 is 1.3339762214954995
18:34:36.628 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 5 is 1.332874346479157
18:34:39.616 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 10 is 1.332707471465336
18:34:42.728 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 15 is 1.3306195026992704
18:34:46.036 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 20 is 1.3331201276886189
18:34:49.189 [main] INFO  o.d.o.l.ScoreIterationListener - Score at iteration 25 is 1.332836846424285
",3,0,False,self,,,,,
784,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,18,4gi2ej,mixmachinery.com,About Foshan vacuum kneader,https://www.reddit.com/r/MachineLearning/comments/4gi2ej/about_foshan_vacuum_kneader/,mixmachinery,1461663411,,1,1,False,default,,,,,
785,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,18,4gi4i0,nbviewer.jupyter.org,"DeepDream tutorial from TensorFlow, new tricks included.",https://www.reddit.com/r/MachineLearning/comments/4gi4i0/deepdream_tutorial_from_tensorflow_new_tricks/,znah,1461664755,,3,10,False,default,,,,,
786,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,20,4gide8,self.MachineLearning,Simple neural network: Huge gap between training and validation errors,https://www.reddit.com/r/MachineLearning/comments/4gide8/simple_neural_network_huge_gap_between_training/,upulbandara,1461670077,"I'm working on  the ""Implementing a Neural Network"" exercise of the ""CS231n: Convolutional Neural Networks for Visual Recognition"" class. During the hyper-parameter optimization process, I was able to see a huge gap between training and  cross-validation datasets as given below.

lr 1.000000e-04 reg 1.000000e-04 train accuracy: 0.449633 val accuracy: 0.460000

lr 1.000000e-04 reg 1.000000e-03 train accuracy: 0.449245 val accuracy: 0.463000

lr 1.000000e-03 reg 1.000000e-04 train accuracy: 0.755694 val accuracy: 0.556000

lr 1.000000e-03 reg 1.000000e-03 train accuracy: 0.751469 val accuracy: 0.566000

best validation accuracy achieved during cross-validation: 0.566000

test accuracy:  0.543


Should I consider this a sign of an overfit?  or is it safe to accept the performance of the testing dataset.?

Complete Ipython notebook is given in [1].

[1]. http://nbviewer.jupyter.org/github/upul/scratch/blob/master/two_layer_net.ipynb",6,0,False,self,,,,,
787,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,20,4gifr0,plus.google.com,TensorSlow no longer,https://www.reddit.com/r/MachineLearning/comments/4gifr0/tensorslow_no_longer/,andrewbarto28,1461671365,,38,123,False,http://b.thumbs.redditmedia.com/OdF6GOY-SolEJUBhbhsAN_hpyXYMNd9d5tm_ZtOMdiE.jpg,,,,,
788,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,20,4gig6u,self.MachineLearning,Dry Syrup Powder Filling Machine - Automatic High Speed Rotary Dry Syrup 12x8 Powder Filling &amp; Capping Machine,https://www.reddit.com/r/MachineLearning/comments/4gig6u/dry_syrup_powder_filling_machine_automatic_high/,machinefilling,1461671608,[removed],0,0,False,default,,,,,
789,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,21,4gikgr,self.MachineLearning,Which would be your general purpose machine learning library of choice (python)?,https://www.reddit.com/r/MachineLearning/comments/4gikgr/which_would_be_your_general_purpose_machine/,TheJCPT,1461673672,"If you know you would have to perform machine learning, both for classification and clustering, and didn't want to lose too much time mastering libraries, which library would you pick? And why?

(of course some dependencies like numpy can be used, besides the machine library itself)

I'm asking this because I need to perform some machine learning for research, but the research isn't focused on the machine learning itself, so I don't want to ""waste"" too much time on that. I already have some background on the theory behind machine learning and have used it in R before.",7,0,False,self,,,,,
790,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,21,4gio6c,self.MachineLearning,Is it possible computers to think and make decisions?,https://www.reddit.com/r/MachineLearning/comments/4gio6c/is_it_possible_computers_to_think_and_make/,rousse101,1461675384,[removed],0,0,False,default,,,,,
791,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,22,4giou3,github.com,"A keras implementation of CNN (AlexNet, VGG16, VGG19) modified for object localisation, with pre-trained weights.",https://www.reddit.com/r/MachineLearning/comments/4giou3/a_keras_implementation_of_cnn_alexnet_vgg16_vgg19/,leonardblier,1461675693,,0,1,False,default,,,,,
792,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,22,4giw1l,self.MachineLearning,Minibatch neural network training in MATLAB,https://www.reddit.com/r/MachineLearning/comments/4giw1l/minibatch_neural_network_training_in_matlab/,scatterbrain333,1461678702,[removed],0,1,False,default,,,,,
793,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,23,4gj57r,drivendata.github.io,An Opinionated Project Template for Data Science in Python,https://www.reddit.com/r/MachineLearning/comments/4gj57r/an_opinionated_project_template_for_data_science/,dat-um,1461682175,,1,14,False,default,,,,,
794,MachineLearning,t5_2r3gv,2016-4-26,2016,4,26,23,4gj58r,github.com,"A keras implementation of CNN (AlexNet, VGG16, VGG19) modified for object localisation, with pre-trained weights.",https://www.reddit.com/r/MachineLearning/comments/4gj58r/a_keras_implementation_of_cnn_alexnet_vgg16_vgg19/,leonardblier,1461682185,,1,6,False,http://b.thumbs.redditmedia.com/baK3OeGE92MuUcttIMgr4W0P73ZS0iYHlqWGTihanZc.jpg,,,,,
795,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,0,4gj7xg,youtu.be,"Deep Learning Overview &amp; Visualizing What Deep Neural Networks Learn (minute 51:00, great new results!!)",https://www.reddit.com/r/MachineLearning/comments/4gj7xg/deep_learning_overview_visualizing_what_deep/,burlapScholar,1461683163,,3,32,False,http://b.thumbs.redditmedia.com/TmdzywDFEWlBb_JVuf5f3y47MAReYCY1wxPN8wH8DXE.jpg,,,,,
796,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,0,4gjbmz,self.MachineLearning,RNN-based modeling of the relationship between 2 or more time-series signals?,https://www.reddit.com/r/MachineLearning/comments/4gjbmz/rnnbased_modeling_of_the_relationship_between_2/,impairment,1461684462,"What modern approaches are there for classifying interactions between time-series? Consider toy problems such as:

- Model looks at usage history of 2 user accounts, and classifies whether those 2 accounts are being used by the same person.

- Movement tracking sensor data from someone's arm, and movement data from someone's leg. Predict if a pair of arm / leg sensors are from the same walking person.

- 10,000 time-series signals. Generally these signals, as a whole, are following a power law distribution, but that ranking is evolving over time. Predict which of the 10 are most likely to have a higher value in the next time-step.",6,5,False,self,,,,,
797,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,2,4gjy6s,arxiv.org,Neural Random Forests,https://www.reddit.com/r/MachineLearning/comments/4gjy6s/neural_random_forests/,jcannell,1461692075,,5,6,False,default,,,,,
798,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,2,4gk1u6,github.com,rllab: a framework for developing and evaluating reinforcement learning algorithms,https://www.reddit.com/r/MachineLearning/comments/4gk1u6/rllab_a_framework_for_developing_and_evaluating/,alxndrkalinin,1461693301,,0,26,False,http://b.thumbs.redditmedia.com/REcHVwLxNWS1RQgMlFxz9sxbXHFhfmbkbajnk5rF_nM.jpg,,,,,
799,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,4,4gkimu,highdimensionality.com,A Different Approach to Low-Rank Matrix Completion: Part 1,https://www.reddit.com/r/MachineLearning/comments/4gkimu/a_different_approach_to_lowrank_matrix_completion/,pmigdal,1461698991,,0,23,False,http://b.thumbs.redditmedia.com/M6Y0FizHuZ6o6xlkqy1Z1PSn-D40XB3qthEim6wCtFc.jpg,,,,,
800,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,4,4gkkor,engadget.com,AI will frag each other with rocket launchers in 'Doom',https://www.reddit.com/r/MachineLearning/comments/4gkkor/ai_will_frag_each_other_with_rocket_launchers_in/,DerThes,1461699657,,0,0,False,http://b.thumbs.redditmedia.com/ZxElspAckCm5NYwHfyNN7G9DhAtbUvJzzkXrkKP12RY.jpg,,,,,
801,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,5,4gkpkg,blog.docker.com,Docker Cloud to the Rescue,https://www.reddit.com/r/MachineLearning/comments/4gkpkg/docker_cloud_to_the_rescue/,shugert,1461701317,,0,0,False,http://b.thumbs.redditmedia.com/iT7O76jNjV3Ap_WZry9weQnveXnAfzSU7ZRgLIx9vuY.jpg,,,,,
802,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,5,4gkryx,arxiv.org,End-to-End Tracking and Semantic Segmentation Using Recurrent Neural Networks,https://www.reddit.com/r/MachineLearning/comments/4gkryx/endtoend_tracking_and_semantic_segmentation_using/,senorstallone,1461702124,,1,4,False,default,,,,,
803,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,5,4gktav,self.MachineLearning,Career Advice (machine learning engineer)?,https://www.reddit.com/r/MachineLearning/comments/4gktav/career_advice_machine_learning_engineer/,FatSoccerMan,1461702577,"I am a data scientist turned software engineer(backend, non-ml) who is looking to eventually transition into doing machine learning engineering full time. What technical skills do I need to acquire in order to make this transition? For example, I have heard knowing C++ is very useful. What other things are important? Assume I have a sufficient mathematical/statistical background.",2,0,False,self,,,,,
804,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,6,4gkylx,self.MachineLearning,"Are there any iPython/Jupyter-like programs that have tabs (or equivalent), amongst which I can share global variables? I just want to load &amp; transform my data once and run experiments without repetition",https://www.reddit.com/r/MachineLearning/comments/4gkylx/are_there_any_ipythonjupyterlike_programs_that/,sweet_trash_scent,1461704401,"I find it annoying to have to constantly re-load and re-transform data if I want to run a new experiment. If, conceptually, two things are different (EDA vs fitting a predictive model for example), I want to run them in different contexts",3,0,False,self,,,,,
805,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,6,4gkzxm,elitetrader.com,Machine Learning is the new C++,https://www.reddit.com/r/MachineLearning/comments/4gkzxm/machine_learning_is_the_new_c/,Markjack99,1461704845,,0,0,False,default,,,,,
806,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,6,4gl697,docs.google.com,Causal Inference Challenge: 2016 Atlantic Causal Inference Conference [statistics],https://www.reddit.com/r/MachineLearning/comments/4gl697/causal_inference_challenge_2016_atlantic_causal/,urish,1461707075,,0,11,False,http://b.thumbs.redditmedia.com/raHu5Ev7hHjZMux8yG1N1ImoLl12v5CEg-cdWlePwBA.jpg,,,,,
807,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,7,4glg5w,self.MachineLearning,What's a god way to represent known relationships between parameters in a machine learning model?,https://www.reddit.com/r/MachineLearning/comments/4glg5w/whats_a_god_way_to_represent_known_relationships/,[deleted],1461710691,[deleted],0,1,False,default,,,,,
808,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,8,4gliwd,youtube.com,Line By Line: Walkthru of TensorFlow fully_connected_feed.py,https://www.reddit.com/r/MachineLearning/comments/4gliwd/line_by_line_walkthru_of_tensorflow_fully/,vanboxel,1461711723,,2,2,False,default,,,,,
809,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,8,4gljk2,self.MachineLearning,What's a good way to represent known relationships between parameters in a machine learning model?,https://www.reddit.com/r/MachineLearning/comments/4gljk2/whats_a_good_way_to_represent_known_relationships/,FuzziCat,1461711982,"For example, I know that Monday always comes after Sunday and that days of the week repeat in a cycle.  How should I represent the days of the week so that the model can learn from the temporal ordering and cyclicity of the days of the week?  ",1,1,False,self,,,,,
810,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,8,4gljtb,github.com,My Keras implementation of a deep semantic similarity/convolutional latent semantic model,https://www.reddit.com/r/MachineLearning/comments/4gljtb/my_keras_implementation_of_a_deep_semantic/,[deleted],1461712078,[deleted],2,2,False,default,,,,,
811,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,8,4gllw7,self.MachineLearning,Extensions of Support Vector Machines (Papers),https://www.reddit.com/r/MachineLearning/comments/4gllw7/extensions_of_support_vector_machines_papers/,Bohemian90,1461712873,"Hello

I'm searching for papers which provide different SVM formulations/extensions and also provide Matlab code. A google search did not bring good results. I'm especially interested in SVM algorithms which can handle missing data or imbalanced datasets.

Does anybody know such papers?",0,2,False,self,,,,,
812,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,8,4glm4l,aws.amazon.com,"Machine Learning, Recommendation Systems, and Data Analysis at Cloud Academy",https://www.reddit.com/r/MachineLearning/comments/4glm4l/machine_learning_recommendation_systems_and_data/,caligolae,1461712966,,0,2,False,http://b.thumbs.redditmedia.com/gosdyLq0oQKkHF9adr8My6IDnPjJ7spJ5EgvxtBad8I.jpg,,,,,
813,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,9,4glzcj,self.MachineLearning,Is it a good time to do Masters in Stats/ML?,https://www.reddit.com/r/MachineLearning/comments/4glzcj/is_it_a_good_time_to_do_masters_in_statsml/,slingshot_wolf,1461717937,"I would soon be completing my undergrad and I have two options now: Go for a Masters or take up a job. 

I have been dabbling a lot in research during my undergrad (have a few 2nd author publications in good conferences) and would really love to do more by doing a Masters, but I am a little apprehensive. There is too much hype in this field and I am afraid interest in the field is on the verge of losing momentum. Would spending 2 more years be worth it?

All the greatly experienced people in here, what would you recommend? 

Thanks!",4,0,False,self,,,,,
814,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,10,4gm42n,self.MachineLearning,"Unsupervised clustering in a biological, beta distributed data set",https://www.reddit.com/r/MachineLearning/comments/4gm42n/unsupervised_clustering_in_a_biological_beta/,[deleted],1461719710,[deleted],6,2,False,default,,,,,
815,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,11,4gmihl,self.MachineLearning,"Looking for old link from here, it relates cyber security and ML.",https://www.reddit.com/r/MachineLearning/comments/4gmihl/looking_for_old_link_from_here_it_relates_cyber/,final-getsuga,1461725352,"I could have sworn I saw a post here from over a week ago where it was an image with the words ""This week in Machine Learning"" and the Udacity logo was at the top right. Bottom right was a cybersecurity section. Does anyone have that link? I thought I saved it but I can't find it. 

Thank you ",2,0,False,self,,,,,
816,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,13,4gmut4,self.MachineLearning,Amateur trying to get started with MachineLearning,https://www.reddit.com/r/MachineLearning/comments/4gmut4/amateur_trying_to_get_started_with_machinelearning/,kattenfreja,1461730605,"Hello everybody.

I've been following this subreddit for some time, as well as read articles and watched videos on machine learning trying to get started with ML. I understand the basic concepts and have some very basic knowledge of python. The reason for this is that I believe I could apply machine learning to a work project I'm working on which involves image analysis. The problem is that I just can't seem to really get the hang ofsome of the practical basics, as I'm quite new to programming. 

What I'm trying to accomplish is basically an algorithm which automatically identifies certain objects in larger images, and returns the amount of detected items. I'm thinking of using Tensorflow to do this.

The question is basically how to set up the environment on a Windows 10 computer.

 1. What IDE do you recommend which is compatible with tensorflow and runs on Win10? Would I need to set up anything else for the development environment for TF?

2. Ideally I would eventually want to create a stand alone executable, capable of automatically analyzing images from a certain folder which is regularly updated; preferrably offline - is this possible to accomplish with tensorflow?

3. If anyone could recommend a tutorial, or explain the basic concepts of importing images into tensorflow I would also be really grateful.

4. I've considered installing Ubuntu as a dual boot os, is working with machine learning significantly easier with Ubuntu compared to Win or OSX? 

Thanks a lot in advance, I'm grateful for all help and thoughts.
",1,0,False,self,,,,,
817,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,14,4gn08x,arxiv.org,[1506.03877] Training Bidirectional Helmholtz Machines,https://www.reddit.com/r/MachineLearning/comments/4gn08x/150603877_training_bidirectional_helmholtz/,adagrad,1461733364,,1,16,False,default,,,,,
818,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,15,4gn77y,self.MachineLearning,TensorFlow: simple recurrent neural network?,https://www.reddit.com/r/MachineLearning/comments/4gn77y/tensorflow_simple_recurrent_neural_network/,ascenator,1461737184,"Hey there,

I've implemented some MLPs and CNNs in TensorFlow and would like to go ahead to recurrent neural networks. Unfortunately I'm not skilled in natural language processing, hence, it is difficult for me to follow the TensorFlow RNN tutorials, since they are about complex RNN models (like LSTMs) for NLP.

I found no tutorials for simple recurrent neural networks in TensorFlow, just the ""RNNBasicCell"" of TensorFlow itself. Is this thing like an Elman network (SRN)?

Does someone know an easy tutorial for this?

Or can someone supply an easy example of a simple recurrent neural network in TensorFlow?

Is it mandatory to use the RNN Cells of TensorFlow?
If not, how would one implement his own RNN in TensorFlow?
Any tips?

thanks!

",3,1,False,self,,,,,
819,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,15,4gn8xp,self.MachineLearning,What are your thoughts on DeepMark?,https://www.reddit.com/r/MachineLearning/comments/4gn8xp/what_are_your_thoughts_on_deepmark/,laura1222,1461738190,,2,0,False,self,,,,,
820,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,15,4gnacj,self.MachineLearning,Where to get advice on errors with a TensorFlow program I'm writing?,https://www.reddit.com/r/MachineLearning/comments/4gnacj/where_to_get_advice_on_errors_with_a_tensorflow/,[deleted],1461739089,[deleted],0,0,False,default,,,,,
821,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,15,4gncgt,arxiv.org,A Deep Hierarchical Approach to Lifelong Learning in Minecraft,https://www.reddit.com/r/MachineLearning/comments/4gncgt/a_deep_hierarchical_approach_to_lifelong_learning/,TomZahavy,1461740389,,0,1,False,default,,,,,
822,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,17,4gnign,falconn-lib.org,FALCONN: Similarity Search Over High-Dimensional Data,https://www.reddit.com/r/MachineLearning/comments/4gnign/falconn_similarity_search_over_highdimensional/,galapag0,1461744348,,5,21,False,http://b.thumbs.redditmedia.com/h7NLe-hwfeimARcHFn01anVG0SX35uy3VKiPMa7j9hQ.jpg,,,,,
823,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,17,4gnjfc,mixmachinery.com,The constructure of kneader,https://www.reddit.com/r/MachineLearning/comments/4gnjfc/the_constructure_of_kneader/,mixmachinery,1461745001,,1,1,False,default,,,,,
824,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,17,4gnk2o,self.MachineLearning,A Deep Hierarchical Approach to Lifelong Learning in Minecraft,https://www.reddit.com/r/MachineLearning/comments/4gnk2o/a_deep_hierarchical_approach_to_lifelong_learning/,TomZahavy,1461745447,https://arxiv.org/abs/1604.07255,2,0,False,self,,,,,
825,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,17,4gnkhh,hi.cs.waseda.ac.jp,Automatic Image Colorization with Simultaneous Classification,https://www.reddit.com/r/MachineLearning/comments/4gnkhh/automatic_image_colorization_with_simultaneous/,smith2008,1461745740,,22,135,False,http://b.thumbs.redditmedia.com/9AyrIPbJTAVN3ITBZHsN2TqHYiDUxjBVCk4qGuzG5Ds.jpg,,,,,
826,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,17,4gnn35,self.MachineLearning,Predict the base form of word,https://www.reddit.com/r/MachineLearning/comments/4gnn35/predict_the_base_form_of_word/,ming0308,1461747594,"I am thinking is it possible to train the model that take any word as input and output the base form of it. 
For example,  loves - &gt; love
It seems feasible to me because I believe there are some hidden rules,  eg remove s,  ed suffix. But of cause,  some are more difficult,  such as wrote and write. 

I am considering neural network, but I am not sure how to encode the input and output.  So if I just simply  feed the alphabets of word into neutral net,  the number of input nodes varies,  and the same problem appears in the output layer.

Any guidance is appreciated. 




",8,0,False,self,,,,,
827,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,19,4gnvio,self.MachineLearning,Evolutionary Computation and Genetic Programming for Dummies,https://www.reddit.com/r/MachineLearning/comments/4gnvio/evolutionary_computation_and_genetic_programming/,AlanZucconi,1461753095,"Understanding Machine Learning can be quite hard, especially without a solid knowledge of Maths of Statistics. There are few techniques, however, which can be understood with little to no Maths at all. Evolutionary Computation falls into this category. This tutorial, divided in four parts, introduces genetic programming and hows how it can be used to find solution to very practical problems.

* Part 1. [Theory](http://www.alanzucconi.com/?p=4730)
* Part 2. [The Creature](http://www.alanzucconi.com/?p=4774)
* Part 3. [The Genome](http://www.alanzucconi.com/?p=4736)
* Part 4. [The Loop](http://www.alanzucconi.com/?p=4765)

A working package is provided in the last part of the tutorial, with examples to evolve the AI of a ragdoll, a horse and a simple walker.

If you have any question, please do not hesitate do contact me. ",3,1,False,self,,,,,
828,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,20,4go0vb,coursecatalogue.uva.nl,I have applied to University of Amsterdam masters in AI.Could you look at the link and tell me how is the program?,https://www.reddit.com/r/MachineLearning/comments/4go0vb/i_have_applied_to_university_of_amsterdam_masters/,rohanpota,1461756364,,6,0,False,default,,,,,
829,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,21,4go9jo,self.MachineLearning,Multiple CNN Layers for Sentence Classification,https://www.reddit.com/r/MachineLearning/comments/4go9jo/multiple_cnn_layers_for_sentence_classification/,LeavesBreathe,1461760652,"Hey Guys,

http://arxiv.org/abs/1510.03820
http://arxiv.org/abs/1408.5882

Above are two great papers on using CNNs for sentence classification. In most of the papers I come across on this topic, it doesn't seem that many individuals have tried multiple layers. 

Is there a reason for this? I would think that having a greater level of abstraction would only serve to benefit and its not that much more work. I'll be testing this shortly, but I just wanted to run it by you guys.

Naturally, this also brings up the question of attention. There has been much reported success with RNN based attention mechanisms. With multiple CNN layers, would it be best to just apply attention to the very last CNN layer? 

I found it strange that attention helps when your task is classification, and there is only one timestep output. I always thought that attention was made for multiple timestep outputs such as seq2seq.

Normally, with multiple RNN layers, shortcut connections can help. Is there an equivalent for this with multiple CNN layers? 

I have also found that CNN -&gt; MLP helps my sentence classification task. As MLP's are cheap, I'm confused why more papers don't have them. Is there something I'm missing here?

Thanks!",6,0,False,self,,,,,
830,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,21,4goay6,wiseguyreports.com,United States Industrial Security Systems Industry 2016 Market Research Report,https://www.reddit.com/r/MachineLearning/comments/4goay6/united_states_industrial_security_systems/,naincyjorge,1461761303,,0,1,False,default,,,,,
831,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,22,4gociw,datacamp.com,The importance of preprocessing in data science and the machine learning pipeline,https://www.reddit.com/r/MachineLearning/comments/4gociw/the_importance_of_preprocessing_in_data_science/,theuwmt,1461762003,,2,0,False,http://b.thumbs.redditmedia.com/2_q8qUVgFxois58b35XKHl2ecAE5vU1E99ZruV9FGAQ.jpg,,,,,
832,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,22,4goehb,self.MachineLearning,mind uploading,https://www.reddit.com/r/MachineLearning/comments/4goehb/mind_uploading/,Loki7295,1461762840,[removed],1,1,False,default,,,,,
833,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,22,4gogvb,self.MachineLearning,Can stochastic gradient descent be used with Multiple Adaptive Regression Spline (MARS)?,https://www.reddit.com/r/MachineLearning/comments/4gogvb/can_stochastic_gradient_descent_be_used_with/,datasciguy-aaay,1461763889,[removed],0,1,False,default,,,,,
834,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,22,4goip1,self.MachineLearning,Question about Deep Belief Networks.,https://www.reddit.com/r/MachineLearning/comments/4goip1/question_about_deep_belief_networks/,[deleted],1461764639,[deleted],0,1,False,default,,,,,
835,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,23,4goqxq,self.MachineLearning,Can an incremental-learning or online-learning variant algorithm of artificial neural networks possibly be designed?,https://www.reddit.com/r/MachineLearning/comments/4goqxq/can_an_incrementallearning_or_onlinelearning/,datasciguy-aaay,1461767759,Is there some obstacle in particular in the design of all neural network algorithms that absolutely forces them to always use batch learning?  Can an incremental-learning or online-learning variant algorithm of neural networks possibly be designed?,11,0,False,self,,,,,
836,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,23,4gorro,self.MachineLearning,How to stitch together two models?,https://www.reddit.com/r/MachineLearning/comments/4gorro/how_to_stitch_together_two_models/,shakedzy,1461768065,"Let's assume I have a ML model which consists of y(x)=a+bx+cx^2 and z(x)=i+jx+kx^2 +lx^3. The model is y(x) for x&lt;=g and z(x) fo x&gt;=g. The thing is - I need to find g and all the constants a,b,c,i,j,k,l. How do I do that?",7,0,False,self,,,,,
837,MachineLearning,t5_2r3gv,2016-4-27,2016,4,27,23,4gosiy,self.MachineLearning,Can stochastic gradient descent (SGD) optimization be used with Multiple Adaptive Regression Spline (MARS)?,https://www.reddit.com/r/MachineLearning/comments/4gosiy/can_stochastic_gradient_descent_sgd_optimization/,datasciguy-aaay,1461768367,"
I would like to know if someone has strong arguments for, or against, the technical compatibility of stochastic gradient descent specifically for optimizing the learned parameters of the learning algorithm MARS. I am looking into it. My goal is to hear from others who already looked into it too.

I would love to entertain tangential discussions in another thread. Thanks for your consideration of staying directly on topic if possible. You are the best.",1,0,False,self,,,,,
838,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,0,4govaa,youtube.com,"Neural Nets (12a) and Deep Neural Nets (12b)- An updated, 2015 edition to the wonderful 2010 lecture by MIT's professor Patrick Winston",https://www.reddit.com/r/MachineLearning/comments/4govaa/neural_nets_12a_and_deep_neural_nets_12b_an/,Eagle-Eye-Smith,1461769348,,5,34,False,http://b.thumbs.redditmedia.com/XRiP7mBOK8MTAgEDXvNVYJc1X41jbd2Y-nMtxf-1DyU.jpg,,,,,
839,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,1,4gpdg2,self.MachineLearning,Speeding up Caffe on mobile,https://www.reddit.com/r/MachineLearning/comments/4gpdg2/speeding_up_caffe_on_mobile/,niujin,1461775680,"Hi

I am developing some applications using a convolutional neural network that I trained, which uses the architecture of GoogLeNet in Caffe. I would like to run them on mobile platforms. Input image size is 256x256.

Ive done some testing and I find that HTC One processes an image in 5 seconds, and Samsung S6 in 1 second. In both cases these are too slow for my purposes. I would like to increase the speed by 2x to 5x.

I would like to know, has anyone tried to run GoogLeNet on mobile platforms? Any tips on how to speed things up? In particular I am looking at Android at the moment but will also look at iPhone soon. Maybe theres an alternative architecture to GoogLeNet thats even faster? I am open to any new ideas!

I found a paper, *Compressing Neural Networks with the Hashing Trick*, which gives me an idea that involves modifying Caffe. Do you have any other suggestions?

thanks very much!",11,0,False,self,,,,,
840,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,1,4gpe2r,re-work.co,"Deep Learning with Hugo Larochelle, Research Scientist at Twitter Cortex",https://www.reddit.com/r/MachineLearning/comments/4gpe2r/deep_learning_with_hugo_larochelle_research/,reworksophie,1461775906,,0,1,False,default,,,,,
841,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,1,4gpero,self.MachineLearning,Anomaly detection where CLT doesn't apply?,https://www.reddit.com/r/MachineLearning/comments/4gpero/anomaly_detection_where_clt_doesnt_apply/,shakedzy,1461776153,"When trying to detect anomalies in data, it is usually helpful to use the Central Limit Theory, and see which data points are left far from the bell curve.
But CLT doesn't always apply, for various reasons. How can I detect anomalies given a huge dataset of this type?",1,0,False,self,,,,,
842,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,1,4gpevv,self.MachineLearning,State of the art models for anaphora/ coreference resolution?,https://www.reddit.com/r/MachineLearning/comments/4gpevv/state_of_the_art_models_for_anaphora_coreference/,rushimg,1461776203,[removed],0,1,False,default,,,,,
843,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gphqc,self.MachineLearning,"Bayesian treatment of ""outliers""",https://www.reddit.com/r/MachineLearning/comments/4gphqc/bayesian_treatment_of_outliers/,bayesianclassifier,1461777171,[removed],0,1,False,default,,,,,
844,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gphxs,gym.openai.com,OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms,https://www.reddit.com/r/MachineLearning/comments/4gphxs/openai_gym_a_toolkit_for_developing_and_comparing/,nharada,1461777240,,3,53,False,http://b.thumbs.redditmedia.com/-Qy5lFRc4heEup0XEZjYn-GVBb6BHeHyfH8lD7Sk-us.jpg,,,,,
845,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gpi84,openai.com,"OpenAI ""Gym"" - Reinforcement Learning Library and Service just revealed &amp; released!",https://www.reddit.com/r/MachineLearning/comments/4gpi84/openai_gym_reinforcement_learning_library_and/,locrawl,1461777327,,46,206,False,default,,,,,
846,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gpimm,nervanasys.com,Deep Reinforcement Learning with OpenAI Gym,https://www.reddit.com/r/MachineLearning/comments/4gpimm/deep_reinforcement_learning_with_openai_gym/,alxndrkalinin,1461777467,,0,8,False,http://b.thumbs.redditmedia.com/gNvLDoU1i5YORfcl28gQ9NWhTw_qsiftZuYEnhCaAxA.jpg,,,,,
847,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gpk1b,youtube.com,Google Machine Learning Recipes #3 - What Makes a Good Feature?,https://www.reddit.com/r/MachineLearning/comments/4gpk1b/google_machine_learning_recipes_3_what_makes_a/,sbjf,1461777929,,0,10,False,http://b.thumbs.redditmedia.com/-2g8FNbOtPVFmRmZuT7vMSwIGuAF5eFry72tizyBFVk.jpg,,,,,
848,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gpm0w,self.MachineLearning,Newbie Question re. RNN topology,https://www.reddit.com/r/MachineLearning/comments/4gpm0w/newbie_question_re_rnn_topology/,[deleted],1461778574,[removed],0,1,False,default,,,,,
849,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gpof7,self.MachineLearning,Simple Question about Using Pre-Existing CNNs or Deriving Your Own,https://www.reddit.com/r/MachineLearning/comments/4gpof7/simple_question_about_using_preexisting_cnns_or/,IndividualCarnival,1461779370,"Hi everyone!

I've been entering my way into deep learning and have covered most of CS231n, but I wanted to clarify something quickly about CNNs before I consider playing around with any of it.

Watching the lectures it seemed as though the lecturers hinted that we can always start by just ""downloading"" one of these architectures (VGGNet, GoogleNet, ResNet, whatever) and then start building upon such architectures for our problem. Is this a common thing that everyone does? 

I had assumed walking in that this may be similar to other more basic supervised problems where we could (for the most part) get a way with making some things from scratch. For the purposes of CNNs, is it, for sake of practicality, common to simply use other successful architectures and not try to derive our own? I was hoping to try to play with different orientations for CNNs and explore it's effectiveness but if that is not just a practical thing, then I might diverge otherwise from such idea.

Any clarification on how I can go about starting any practice with CNNs would be really appreciated!",0,0,False,self,,,,,
850,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gpoic,self.MachineLearning,Bayesian treatment of outliers,https://www.reddit.com/r/MachineLearning/comments/4gpoic/bayesian_treatment_of_outliers/,bayesianclassifier,1461779399,[removed],0,1,False,default,,,,,
851,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,2,4gppdl,wired.com,"Inside OpenAI, Elon Musks Wild Plan to Set Artificial Intelligence Free",https://www.reddit.com/r/MachineLearning/comments/4gppdl/inside_openai_elon_musks_wild_plan_to_set/,vonnik,1461779680,,6,0,False,http://b.thumbs.redditmedia.com/GPq5gTr6wVZXkhaWT_T8jDpejr0nYADbxlwT3vx-haE.jpg,,,,,
852,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,3,4gptkp,self.MachineLearning,State of the art models for anaphora/ coreference resolution?,https://www.reddit.com/r/MachineLearning/comments/4gptkp/state_of_the_art_models_for_anaphora_coreference/,rushimg,1461781104,Was wondering if any one could provide me with papers describing the state of the art model for anaphora/corefrence resolution. Both neural and non-neural approaches are appreciated. Thanks.,2,7,False,self,,,,,
853,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,3,4gpxbz,self.MachineLearning,Neural Network Question,https://www.reddit.com/r/MachineLearning/comments/4gpxbz/neural_network_question/,DarthDovahkiin5,1461782408,"I'm totally new at this. Recently I became interested in Neural Networks. But I'm confused on one part. 

So your input into the first neuron has a weight. But then when you input into the hidden layer what would the weights be? 
",3,0,False,self,,,,,
854,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,4,4gq116,self.MachineLearning,I built a food-recognition meal logging app with deep learning,https://www.reddit.com/r/MachineLearning/comments/4gq116/i_built_a_foodrecognition_meal_logging_app_with/,subcosmos,1461783714,"[Android download link](https://play.google.com/apps/testing/infinome.app)

[Typical result](https://www.infino.me/food/share/RKP-Jo-WAysTacDjnsYQnQ/)

iOS hopefully coming soon!

Hi all. I used deep learning networks to put together a food logging app. Take a photo of what is on your plate, and the app will classify about ~1k food labels. The app also supports nutrition facts lookup with either a barcode or keyword, based on what the neural net predicts. Fatsecret has been kind enough to let me use their massive food database.

You can also checkin with foursquare to indicate if you are eating out. For major restauraunt chains this helps with nutrition facts lookups.

Anyone want to alpha test? Id love some initial gut reactions.",21,41,False,self,,,,,
855,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,5,4gqewq,self.MachineLearning,Udacity's Machine Learning Engineer Nanodegree: Where are the projects?,https://www.reddit.com/r/MachineLearning/comments/4gqewq/udacitys_machine_learning_engineer_nanodegree/,sonnysfo,1461788640,"I would like to look at detailed description of the projects of the ML Engineer nanodegree in the link below, how do I see the details of requirements, dataset etc required in order to finish the projects while auditing the courses, but without registering for the program?

https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009#",2,0,False,self,,,,,
856,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,5,4gqezy,self.MachineLearning,CNN Filters Question,https://www.reddit.com/r/MachineLearning/comments/4gqezy/cnn_filters_question/,haskkk,1461788670,"What prevents various layers in the convolution from learning to activate on the same features? Or maybe better yet, how do they learn to activate on different features? ",4,0,False,self,,,,,
857,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,5,4gqkv6,self.MachineLearning,Estimating the number of neurons required to model a dataset [question],https://www.reddit.com/r/MachineLearning/comments/4gqkv6/estimating_the_number_of_neurons_required_to/,gindc,1461790640,"I have been playing around with TensorFlow and music data.

The model I am using has 100 inputs and 1 output.  The inputs involve statistics about a song, and the output is whether hate it or like it (0 or 1).

I have about 15,000 examples and new examples are added all the time.  So the network is updated often.

**Here is the question:**  How do I determine how many layers and how many neurons are required to model the data?  It seems like if I use 2 or 3 layers of 100 neurons each, it will under-fit the data.  However, if I use 7 or 8 layers, the data will be significantly over-fit.

I've read about early training termination.  But is there a way to slightly under-fit the data by choosing the correct number of layers/neurons?  Is there a heuristic for determine optimal network geometry?",13,0,False,self,,,,,
858,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,6,4gqlhn,self.MachineLearning,Convnets for minecraft biome generation from arbitrary input,https://www.reddit.com/r/MachineLearning/comments/4gqlhn/convnets_for_minecraft_biome_generation_from/,NootropicJoe,1461790853,"So I'm envisioning the minecraft world like there's a color(block type) and coordinate value for each of the blocks.

My idea, is you run a recurrent convnet as you would for image feature recognition to discover the features which are shared between a set of input chunks. Then you use a neural-style/deepdream type algorithm hallucinate the features on top of something.

Would be cool, I'm thinking to generate cities mainly.",0,0,False,self,,,,,
859,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,6,4gqogp,wired.com,"Inside OpenAI, Elon Musks Wild Plan to Set Artificial Intelligence Free",https://www.reddit.com/r/MachineLearning/comments/4gqogp/inside_openai_elon_musks_wild_plan_to_set/,[deleted],1461791887,[deleted],0,1,False,default,,,,,
860,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,8,4gr6pr,self.MachineLearning,Personalized recommendations project for Reddit,https://www.reddit.com/r/MachineLearning/comments/4gr6pr/personalized_recommendations_project_for_reddit/,Park-jae-sang,1461798883,"Hello. Currently, I have over 2000 unread posts in my RSS feed from MachineLearning and CS related subreddits. Given the information overload I am thinking about creating a personalized posts recommender for Reddit. But first I would like to check if there are active projects working on similar problem. I'd rather contribute to an existing project than start from scratch. I found one project but it recommends new subreddits. Quite different task in my opinion. Please feel free to post relevant projects.

If there is nothing, I am planning to use word2vec + RNN. I am not an expert in NLP and the choice is influenced by CS224d course from Stanford.      ",0,0,False,self,,,,,
861,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,8,4gr98r,self.MachineLearning,Passing an Integer into Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4gr98r/passing_an_integer_into_tensorflow/,[deleted],1461799879,[deleted],9,0,False,default,,,,,
862,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,8,4gradp,devblogs.nvidia.com,Train Your Reinforcement Learning Agents At The OpenAI Gym,https://www.reddit.com/r/MachineLearning/comments/4gradp/train_your_reinforcement_learning_agents_at_the/,harrism,1461800341,,0,9,False,http://a.thumbs.redditmedia.com/BilJGQnULVr_GeBf7wEBD9pxlacAeibBvL4IGcN_vU0.jpg,,,,,
863,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,8,4grccw,self.MachineLearning,Implementation of triplet-based distance learning network such as FaceNet?,https://www.reddit.com/r/MachineLearning/comments/4grccw/implementation_of_tripletbased_distance_learning/,Pieranha,1461801181,"Triplet-based distance learning for face recognition etc. seems very effective, but I don't see any open source implementations. This may be due to the huge requirement in terms of training data (see the FaceNet paper - http://arxiv.org/abs/1503.03832), but that amount of data may be feasible to obtain for other tasks than face recognition. I'm therefore wondering why I can't seem to find a proper open source implementation.

Has anyone found an implementation of FaceNet or a similar triplet-based network in Keras/Theano/ThensorFlow/Lasagne?",5,2,False,self,,,,,
864,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,9,4grhq9,blog.dominodatalab.com,How Machine Learning Amplifies Inequality in Society,https://www.reddit.com/r/MachineLearning/comments/4grhq9/how_machine_learning_amplifies_inequality_in/,gregory_k,1461803356,,0,0,False,http://b.thumbs.redditmedia.com/A0X5D6gBWaX-zpD_76x9eiOAyh4V6vm3mo2L_J6Baak.jpg,,,,,
865,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,11,4grz9q,github.com,Top deep learning projects on GitHub,https://www.reddit.com/r/MachineLearning/comments/4grz9q/top_deep_learning_projects_on_github/,[deleted],1461810602,[deleted],1,0,False,default,,,,,
866,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,12,4gsb2l,medium.com,Let Me Hear Your Voice and I Will Tell You How You Feel,https://www.reddit.com/r/MachineLearning/comments/4gsb2l/let_me_hear_your_voice_and_i_will_tell_you_how/,carlos_argueta,1461815852,,0,0,False,http://b.thumbs.redditmedia.com/dGuzyGALXZCrDaSkB9LhB2AhhZVhVmDKwLnUI35MZIc.jpg,,,,,
867,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,13,4gsew8,analyticbridge.com,Machine Learning with Python- Why do they form the best combination,https://www.reddit.com/r/MachineLearning/comments/4gsew8/machine_learning_with_python_why_do_they_form_the/,vincentg64,1461817690,,0,1,False,default,,,,,
868,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,13,4gshjy,self.MachineLearning,What dataset do you wish was made publicly available?,https://www.reddit.com/r/MachineLearning/comments/4gshjy/what_dataset_do_you_wish_was_made_publicly/,[deleted],1461819129,[deleted],12,2,False,default,,,,,
869,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,13,4gsi7k,self.MachineLearning,Recommandations of relatively advanced/useful documentaries/YT talks/lectures (roughly ~60min or more) of ML to watch?,https://www.reddit.com/r/MachineLearning/comments/4gsi7k/recommandations_of_relatively_advanceduseful/,scriptkittieswagger,1461819478,,2,1,False,self,,,,,
870,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,15,4gsrkm,gym.openai.com,OpenAI Gym's Secret Reinforcement Learning Tutorial (A work in progress),https://www.reddit.com/r/MachineLearning/comments/4gsrkm/openai_gyms_secret_reinforcement_learning/,jasonheh,1461824654,,2,10,False,default,,,,,
871,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,16,4gswio,self.MachineLearning,Pixel Recurrent Neural Networks Theano Code?,https://www.reddit.com/r/MachineLearning/comments/4gswio/pixel_recurrent_neural_networks_theano_code/,alexmlamb,1461827596,Has anyone implemented Pixel Recurrent Neural Networks in Theano?  ,0,5,False,self,,,,,
872,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,16,4gsys2,creativeai.net,Sketch Simplification: Fully Convolutional Networks for Rough Sketch Cleanup,https://www.reddit.com/r/MachineLearning/comments/4gsys2/sketch_simplification_fully_convolutional/,olalonde,1461829067,,2,18,False,default,,,,,
873,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,17,4gt1u5,self.MachineLearning,Building a model to (try to) predict future based on learning historical data,https://www.reddit.com/r/MachineLearning/comments/4gt1u5/building_a_model_to_try_to_predict_future_based/,andulrv,1461831168,"Hi Reddit!

I have an upcoming data mining project and we've also had couple of session about Machine Learning. For the project I've come up with an idea and I am looking forward to feedback, if this is even viable or can this be done to a degree of certainty?

The data we plan on using: https://www.kaggle.com/worldbank/world-development-indicators

My questions is, would it be possible, by choosing and combining different significant features (indicators) and then training the machine based on yearly data (let's say 1960-2015) and try to predict values for 2016. Once we have some results, go and look around official reports/public newspapers to see if any of the predictions match or are close to ones reported in the public web.

Thanks in advance and I even appreciate feedback saying ""this is not possible"".",9,5,False,self,,,,,
874,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,17,4gt3ye,youtube.com,"Movie colorization using the paper from yesterday ""Colorization with Simultaneous Classification""",https://www.reddit.com/r/MachineLearning/comments/4gt3ye/movie_colorization_using_the_paper_from_yesterday/,mar_cnu,1461832631,,25,90,False,http://b.thumbs.redditmedia.com/ARdO8f__ouKoQ8oK4wti7TZLW7SNasH02k4Usx1kkRI.jpg,,,,,
875,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,18,4gt7t7,self.MachineLearning,Using Word2Vec document vectors as features in Naive Bayes,https://www.reddit.com/r/MachineLearning/comments/4gt7t7/using_word2vec_document_vectors_as_features_in/,habitats,1461835270,"I have a bunch of Word2Vec features, that I've added together and normalized in order to create document vectors for my examples. However, Word2Vec vectors sometimes contain negative values, whereas Naive Bayes is only compatible with positive values (it assumes document frequencies).

Any recommendations as to how to get this working? ",19,4,False,self,,,,,
876,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,22,4gtyy7,kratzert.github.io,Understanding the backward pass through Batch Normalization Layer,https://www.reddit.com/r/MachineLearning/comments/4gtyy7/understanding_the_backward_pass_through_batch/,changingourworld,1461850120,,2,27,False,http://b.thumbs.redditmedia.com/Eynmcbd-XHqL4B04AriOw-oAhlS9rLPV-U82YEKhXcw.jpg,,,,,
877,MachineLearning,t5_2r3gv,2016-4-28,2016,4,28,23,4gu3u9,inference.vc,"Notes on ""Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles""",https://www.reddit.com/r/MachineLearning/comments/4gu3u9/notes_on_unsupervised_learning_of_visual/,fhuszar,1461852149,,13,49,False,http://b.thumbs.redditmedia.com/_a_ohwHG1wku1ZijYQ_Q7ZiWDLb_DB8ub-v1m2wPNAw.jpg,,,,,
878,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,0,4guelt,self.MachineLearning,What branches of ML exist and which is more marketable from a career perspective?,https://www.reddit.com/r/MachineLearning/comments/4guelt/what_branches_of_ml_exist_and_which_is_more/,vizuals,1461856210,"Can anyone give (or point to) a layout of ML general topics (e.g. recommender systems, classification, etc.) and comment on which there is/will be a lot of interest for?

I think I have a beginner's overview of the ML landscape--I know enough to know I don't know much. (FWIW, I have an MS in Stats and am very comfortable with Python and Data).",1,0,False,self,,,,,
879,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,0,4gujfk,arxiv.org,Context Encoders: Feature Learning by Inpainting,https://www.reddit.com/r/MachineLearning/comments/4gujfk/context_encoders_feature_learning_by_inpainting/,vodkagoodmeatrotten,1461857920,,1,4,False,default,,,,,
880,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,0,4guku8,self.MachineLearning,"TF: Done training, now what?",https://www.reddit.com/r/MachineLearning/comments/4guku8/tf_done_training_now_what/,olafurjon,1461858431,"I am implementing a speech recognizer using an RNN in tensorflow, now that I am done training a model and I'm happy with my test results, what is the general procedure for utilizing the system in a real world application?",9,2,False,self,,,,,
881,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,0,4gumhg,self.MachineLearning,What is the classification of model that uses convolutiona filters with SVM/Bayes classifier,https://www.reddit.com/r/MachineLearning/comments/4gumhg/what_is_the_classification_of_model_that_uses/,Asnen,1461859015,"Might be a stupid question, but i'll go for it:

Can a model that uses convolutional filters and SVM/Bayes classifier instead of fully connected layer be still considered a neural network?

Convolutional layers doen't seem like a classic NN, and with most common way of organization CNN having fully connected layer of neurons to produce an output i get how it could be still considered a NN, but with that layer gone im not entirely sure it is still a NN.

Im researching about NN application for text classification, and thinking about making a comparassion of using Conv-filters with different classifiers(like one fully-connected, two fully-connected, Naive Bayes, SVM), but since theme is already set i'd like to keep it in boundaries of NN.",5,1,False,self,,,,,
882,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,2,4gv2oy,self.MachineLearning,[Question] Training set bounds for (deep) NNs to approximate function (classes)?,https://www.reddit.com/r/MachineLearning/comments/4gv2oy/question_training_set_bounds_for_deep_nns_to/,aTadConfusedAlien,1461864596,"Hi all,

I am looking for papers that give bounds on the amount of data needed to approximate a given function or class of functions (up to a given uncertainty) with a NN, possibly depending on the type of nonlinearity or architecture chosen.

I know that Cybenko and Kolomogorov's theorem can give some insight into that. Are there papers the specifically focus on the data needed part of them?

Thank you!",0,1,False,self,,,,,
883,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,2,4gv4n5,kickstarter.com,"Hey, I'm using GMMs and RL to control signal lights for bikes!",https://www.reddit.com/r/MachineLearning/comments/4gv4n5/hey_im_using_gmms_and_rl_to_control_signal_lights/,amasterblaster,1461865232,,7,0,False,http://b.thumbs.redditmedia.com/cjAyB_4Yet3nBj2-26YVsVjAGPbhoClyRbL-UTwIO9g.jpg,,,,,
884,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,2,4gv4ug,datasciencecentral.com,11 Important Model Evaluation Techniques Everyone Should Know,https://www.reddit.com/r/MachineLearning/comments/4gv4ug/11_important_model_evaluation_techniques_everyone/,vincentg64,1461865303,,0,1,False,default,,,,,
885,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,2,4gv6gh,self.MachineLearning,Do Deep Neural Nets always outperform other methods?,https://www.reddit.com/r/MachineLearning/comments/4gv6gh/do_deep_neural_nets_always_outperform_other/,llSourcell,1461865875,"I'm thinking about a bunch of possible applications of machine learning

-recommendations

-fraud detection

-object recognition

-game AI

-generating music/art/stories

-sentiment analysis

-speech recognition

etc.

and basically it feels like deep learning can be applied to pretty much everything and it will outperform every other ML model . 

Is that correct? Can someone give me an example where this is not the case?",23,0,False,self,,,,,
886,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,2,4gv7cc,techcrunch.com,"Movidius ""Fathom"" - A sub $100 Neural USB Stick just announced!",https://www.reddit.com/r/MachineLearning/comments/4gv7cc/movidius_fathom_a_sub_100_neural_usb_stick_just/,locrawl,1461866189,,5,10,False,http://b.thumbs.redditmedia.com/49R98Wp-jANVxpmDX6fVorCBTtKpUw4n5WOA35RgIbE.jpg,,,,,
887,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,3,4gvfqg,self.MachineLearning,formatting my data for using t-SNE .py script (lvdmaaten),https://www.reddit.com/r/MachineLearning/comments/4gvfqg/formatting_my_data_for_using_tsne_py_script/,apple-sauce,1461869038,"ML newbie here. I have a dataset that I want to run through the t-SNE alglorithm [1]. I downloaded the .py script from [here](https://lvdmaaten.github.io/tsne/). The MNIST data is formatted in a particular way:

    1.0000000e+00   0.0000000e+00 ...
    1.0000000e+00   1.0000000e+00 ...
    ....                      ....
    1.0000000e+00   0.0000000e+00 ...

My question is, how can I convert my (colour) images into that same format?


[1] Van der Maaten, Laurens, and Geoffrey Hinton. ""Visualizing data using t-SNE."" *Journal of Machine Learning Research* 9.2579-2605 (2008): 85.
",7,2,False,self,,,,,
888,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,3,4gvgrw,arxiv.org,Dialog-based Language Learning [arXiv:1604.06045] (by Jason Weston),https://www.reddit.com/r/MachineLearning/comments/4gvgrw/dialogbased_language_learning_arxiv160406045_by/,evc123,1461869415,,0,19,False,default,,,,,
889,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,4,4gvlfg,self.MachineLearning,Any still have Andrej Karpathy's fc7 dataset?,https://www.reddit.com/r/MachineLearning/comments/4gvlfg/any_still_have_andrej_karpathys_fc7_dataset/,anonDogeLover,1461870975,His link is dead: http://cs.stanford.edu/people/karpathy/cnnembed/cnncodes.mat,3,1,False,self,,,,,
890,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,5,4gvvzz,self.MachineLearning,Q-learning algorithms. Can anyone explain the difference between these two versions?,https://www.reddit.com/r/MachineLearning/comments/4gvvzz/qlearning_algorithms_can_anyone_explain_the/,solololol,1461874676,"The most common I see, including on [wikipedia](https://en.wikipedia.org/wiki/Q-learning#Algorithm), is:

   Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))

(there may be a `gamma` multiplying `max(Q(s')` to discount future rewards)

I don't understand two things: what is the role of the second `Q(s,a)`, the one in parentheses, and why is it sometimes omitted, like in [this tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)

Edit: format",6,2,False,self,,,,,
891,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,6,4gw565,self.MachineLearning,Deep learning groups @ MSR?,https://www.reddit.com/r/MachineLearning/comments/4gw565/deep_learning_groups_msr/,alrojo,1461877929,"Which are the leading deep learning groups at Microsoft Research (MSR)?

On the top of my head I can only think of:

- Kaiming He's group (ResNet), Visual Computing Group, Beijing 

Do they also have NLP and American/European departments with leading research in deep learning?",5,7,False,self,,,,,
892,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,6,4gwcyk,blog.init.ai,Latest deep learning research on residual neural networks,https://www.reddit.com/r/MachineLearning/comments/4gwcyk/latest_deep_learning_research_on_residual_neural/,Init_ai,1461880764,,17,58,False,http://b.thumbs.redditmedia.com/C98ZDduJV940NaJ9T8Rw2IevUirSBA6JU_RdIoUsd-o.jpg,,,,,
893,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,7,4gwfp1,fdjohnson.com,Hazards of Inappropriate Motor Lubrication and the Role of Oil Lubricators,https://www.reddit.com/r/MachineLearning/comments/4gwfp1/hazards_of_inappropriate_motor_lubrication_and/,jackerfrinandis,1461881771,,0,1,False,default,,,,,
894,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,9,4gx185,self.MachineLearning,Intro to NN,https://www.reddit.com/r/MachineLearning/comments/4gx185/intro_to_nn/,[deleted],1461890663,[deleted],0,0,False,default,,,,,
895,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,13,4gxtwd,self.MachineLearning,Proof of Softmax derivative,https://www.reddit.com/r/MachineLearning/comments/4gxtwd/proof_of_softmax_derivative/,FutureIsMine,1461903483,Are there any great resources that give an in depth proof of the derivative of the softmax when used within the cross-entropy loss function? I've been struggling to fully derive the softmax and looking for some guidance here. ,11,4,False,self,,,,,
896,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,15,4gy7fe,self.MachineLearning,[Question] Clustering on a self-similarity matrix,https://www.reddit.com/r/MachineLearning/comments/4gy7fe/question_clustering_on_a_selfsimilarity_matrix/,slugsnot,1461911309,"I have a sparse item-item matrix. Some 1's sprinkled about showing that two items have a high degree of similarity. The rest are NaN.

1 N N

N 1 N

N N N

We also have rules about the data that help us impute many of these Nan values with a 0 (there is no similarity between those items).

1 0 0

0 1 N

N N 0

For completion, let's impute the remaining NaN values with row/column means.

1    0    0

0   1   .25

.33 .33 0

Something like that.

My question is this: Does imputing the 0's in step 2 poison the ability to cluster?

My concern is this: If I apply a rule, thus filling in many 0's, I worry that they may begin to form their own cluster. Is this a valid concern?

Similar discussions are taking place on forums: [here](http://stackoverflow.com/questions/10086551/effective-clustering-of-a-similarity-matrix) and [here](https://www.researchgate.net/post/efficient_clustering_algorithms_for_a_similarity_matrix) where people are clustering on similarity (or distance) matrices. But nothing really addressing the idea of imputing chunks of known dissimilarities by known rules.

edit:
I see that Affinity Propagation in sklearn, [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) has an option for ""precomputed"" affinity. Could this be useful?",4,2,False,self,,,,,
897,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,16,4gycmx,engadget.com,Artificial intelligence now fits inside a USB stick,https://www.reddit.com/r/MachineLearning/comments/4gycmx/artificial_intelligence_now_fits_inside_a_usb/,poporing88,1461914715,,9,0,False,http://b.thumbs.redditmedia.com/vi2Izo08WNk7Fj8fgtemHTdTveIT1T7lCx_zMRyU2iM.jpg,,,,,
898,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,20,4gyvil,self.MachineLearning,LSTM: Presetting the internal state? (X-Post from /r/deeplearning),https://www.reddit.com/r/MachineLearning/comments/4gyvil/lstm_presetting_the_internal_state_xpost_from/,flaschenpost-de,1461927826,"Following problem: I want to learn a prediction for a big set of different persons. Let's say it would be text prediction. Each person has their own style, but of course there are similarities between many of them. LSTMs keep their internal state. So from what I understand it would be reasonable to initialize that hidden state with the individual parameters of the single person that is speaking. Does anybody know an example or a tutorial about problems like that? When a new person enters, I do not know much about them, but some kind of group they belong to. So this person should inherit the default settings of his group but that parameter set should be adopted fast to his own style.",8,0,False,self,,,,,
899,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,20,4gywju,cbonnett.github.io,Mixture Density Networks for Galaxy distance determination in TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4gywju/mixture_density_networks_for_galaxy_distance/,pl47,1461928429,,1,11,False,http://b.thumbs.redditmedia.com/QsOvVXT-ZrOGNmNt79dvEWRCMMqcEEKWq_YWhLEdSSU.jpg,,,,,
900,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,20,4gywt9,self.MachineLearning,Optimize Elo-ranking with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/4gywt9/optimize_eloranking_with_machine_learning/,[deleted],1461928589,[deleted],3,0,False,default,,,,,
901,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,20,4gyzna,movidius.com,Deep Learning Accelerator on a USB Stick,https://www.reddit.com/r/MachineLearning/comments/4gyzna/deep_learning_accelerator_on_a_usb_stick/,sigma914,1461930210,,17,26,False,http://a.thumbs.redditmedia.com/XheAAyAMJYRd5CtgyF7lExq3_h3SYkrtnfHirZIzeu8.jpg,,,,,
902,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,21,4gz3tm,youtube.com,Stanford Seminar - Geoffrey Hinton of Google &amp; University of Toronto,https://www.reddit.com/r/MachineLearning/comments/4gz3tm/stanford_seminar_geoffrey_hinton_of_google/,fergbyrne,1461932329,,30,72,False,http://a.thumbs.redditmedia.com/dHxRcjqxLOlVZem-E1g5JOjK7Lupbi4yDIu4buTweA0.jpg,,,,,
903,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,22,4gzarx,reddit.com,"[x-post from /r/Science] ""Science AMA Series: Im Aleix Martinez, a Professor of Cognitive Science and Machine Learning at The Ohio State University. My main areas of expertise are face perception, emotion, and language. Im available today to answer your questions. AMA!""",https://www.reddit.com/r/MachineLearning/comments/4gzarx/xpost_from_rscience_science_ama_series_im_aleix/,rhiever,1461935487,,1,3,False,http://b.thumbs.redditmedia.com/yK_6r3YinYWZqCLBZCiwLylDjFyKmXXOmjQXXC6-vIA.jpg,,,,,
904,MachineLearning,t5_2r3gv,2016-4-29,2016,4,29,22,4gzhqt,arxiv.org,The IBM 2016 English Conversational Telephone Speech Recognition System,https://www.reddit.com/r/MachineLearning/comments/4gzhqt/the_ibm_2016_english_conversational_telephone/,jrmuizel,1461938287,,0,11,False,default,,,,,
905,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,0,4gzw7l,youtube.com,"A Primer on Recommendation Systems (intro talk i gave on recsys, please dont be too harsh :))",https://www.reddit.com/r/MachineLearning/comments/4gzw7l/a_primer_on_recommendation_systems_intro_talk_i/,manueslapera,1461943596,,0,7,False,http://b.thumbs.redditmedia.com/HT6KHCfmCqjroVf9uQrJQCx-hPgc1dQmPeFjlcWSMXc.jpg,,,,,
906,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,1,4h030r,googleresearch.blogspot.co.uk,DeepMind moves to TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4h030r/deepmind_moves_to_tensorflow/,confused00-,1461945862,,69,284,False,http://b.thumbs.redditmedia.com/X5DMPBgn68GEue92aQVTGAWHSXhu_cfTyuqqdACRF1w.jpg,,,,,
907,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,1,4h098d,lab41.org,Deep Learning Generative Models for Handwriting Analysis,https://www.reddit.com/r/MachineLearning/comments/4h098d/deep_learning_generative_models_for_handwriting/,amplifier_khan,1461948016,,0,9,False,http://b.thumbs.redditmedia.com/2UeWcogzoidgQZT2lFCKFoxf28aJ56Ls5roTFn54YCw.jpg,,,,,
908,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,2,4h0ejn,datasciencecentral.com,Deep Learning Demystified,https://www.reddit.com/r/MachineLearning/comments/4h0ejn/deep_learning_demystified/,vincentg64,1461949881,,0,1,False,default,,,,,
909,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,2,4h0n6v,deeplearningskysthelimit.blogspot.nl,Review of Game 4 of the Match of the 21st Century: Lee Sedol's brilliant move reveals weaknesses AlphaGo,https://www.reddit.com/r/MachineLearning/comments/4h0n6v/review_of_game_4_of_the_match_of_the_21st_century/,[deleted],1461952764,[deleted],1,0,False,default,,,,,
910,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,3,4h0r33,gitxiv.com,Associative Long Short-Term Memory (paper+code),https://www.reddit.com/r/MachineLearning/comments/4h0r33/associative_long_shortterm_memory_papercode/,samim23,1461954043,,2,13,False,default,,,,,
911,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,3,4h0s0i,youtube.com,HTM School Episode 3: SDR Overlap Sets and Subsampling,https://www.reddit.com/r/MachineLearning/comments/4h0s0i/htm_school_episode_3_sdr_overlap_sets_and/,numenta,1461954346,,0,0,False,http://b.thumbs.redditmedia.com/dvXYGpiHU7MFY8gLPuIlnzHsZSst3FdoH7YpYz394kI.jpg,,,,,
912,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,3,4h0vqp,self.MachineLearning,How to convert text to vector?,https://www.reddit.com/r/MachineLearning/comments/4h0vqp/how_to_convert_text_to_vector/,[deleted],1461955588,[deleted],4,0,False,default,,,,,
913,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,3,4h0x37,infolab.stanford.edu,"Great intro for some methods of doing conten recommendations (training decision trees, collaborative filtering, etc.)",https://www.reddit.com/r/MachineLearning/comments/4h0x37/great_intro_for_some_methods_of_doing_conten/,[deleted],1461956034,[deleted],0,1,False,default,,,,,
914,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,4,4h108a,infolab.stanford.edu,"Great intro on content recommendations (training decision trees, collaborative filtering, etc.)",https://www.reddit.com/r/MachineLearning/comments/4h108a/great_intro_on_content_recommendations_training/,jeiting,1461957140,,0,1,False,default,,,,,
915,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,5,4h1azq,self.MachineLearning,"Lose Yourself, as misheard by a Deep Network",https://www.reddit.com/r/MachineLearning/comments/4h1azq/lose_yourself_as_misheard_by_a_deep_network/,[deleted],1461961051,[deleted],3,0,False,default,,,,,
916,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,5,4h1ck1,self.MachineLearning,Deep Learning Development on Windows?,https://www.reddit.com/r/MachineLearning/comments/4h1ck1/deep_learning_development_on_windows/,IndividualCarnival,1461961630,"I was just wondering what you guys use for development? I wanted to use TensorFlow but everywhere I look it seems generally incompatible ):. Will I have to switch to Torch or Theano then?

In addition, I am completely new to coding with python (but not coding altogether). I have everything installed but I'm only writing code via NotePad++ right now. Is there an IDE or a better environment that people usually use to code? Thanks!",26,0,False,self,,,,,
917,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,6,4h1mhe,i.imgur.com,"This Week in Machine Learning -- April 29th, 2016",https://www.reddit.com/r/MachineLearning/comments/4h1mhe/this_week_in_machine_learning_april_29th_2016/,DavidAJoyner,1461965414,,8,8,False,http://b.thumbs.redditmedia.com/Ae9MHNPD2vmzJj5Hx8smhZvqXQashDOF0QhTusXC8Vc.jpg,,,,,
918,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,8,4h23nr,self.MachineLearning,TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4h23nr/tensorflow/,carlthome,1461972190,"I just want to find out if having TensorFlow in the title can make a post hit the front page.

EDIT: This was fun while it lasted. This post actually peaked at 20 upvotes initially (who are you guys?) and then took a nose dive into oblivion, securely affirming my future as an unemployed machine learning research engineer.",13,0,False,self,,,,,
919,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,9,4h29od,self.MachineLearning,TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4h29od/tensorflow/,etiquettebot,1461974823,I was wondering if a post with the words TensorFlow would automatically make it to the frontpage.,1,0,False,self,,,,,
920,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,9,4h2ef3,deeplearningskysthelimit.blogspot.nl,Review of Game 4 of the Artificial Intelligence Match of the 21st Century: Lee Sedol's brilliant move reveals weaknesses AlphaGo REVISED,https://www.reddit.com/r/MachineLearning/comments/4h2ef3/review_of_game_4_of_the_artificial_intelligence/,DeepLearningBob,1461976985,,3,16,False,http://a.thumbs.redditmedia.com/uXaRL4euG1eKOepHWsH3PJK1MRA-lTeygXHM8b0JyG8.jpg,,,,,
921,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,9,4h2ehk,self.MachineLearning,Is anyone here running Tensorflow on Google Cloud Platform?,https://www.reddit.com/r/MachineLearning/comments/4h2ehk/is_anyone_here_running_tensorflow_on_google_cloud/,dufu,1461977015,,2,0,False,self,,,,,
922,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,9,4h2eii,self.MachineLearning,Tensorflow,https://www.reddit.com/r/MachineLearning/comments/4h2eii/tensorflow/,siblbombs,1461977025,I'm just wondering if people other than /u/alexmlamb can shitpost.,3,0,False,self,,,,,
923,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,10,4h2j95,self.MachineLearning,How to upload data in tensorflow,https://www.reddit.com/r/MachineLearning/comments/4h2j95/how_to_upload_data_in_tensorflow/,orwellissimo,1461979251,"I'm trying to learn how to use tensorflow in python. I have CSV files with train and test data. I'm searching for a step by step tutorial for someone who is doing this for the first time.

Thank you,
orwellissimo :)",3,0,False,self,,,,,
924,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,10,4h2jc9,github.com,"Chainer Guide - Intuitive Deep Learning Framework - with Many Cool Demos like Deep Dream, Seq2Seq and Variational Autoencoder",https://www.reddit.com/r/MachineLearning/comments/4h2jc9/chainer_guide_intuitive_deep_learning_framework/,Jxieeducation,1461979294,,10,17,False,http://a.thumbs.redditmedia.com/7XXiMJDpkLWovGbmVOKsrz0GOOHWzrP4ZPO7IgjkDr4.jpg,,,,,
925,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,10,4h2lie,self.MachineLearning,Solving The Vanishing Gradient and Exploding Gradient Problem With One Line Of Code?,https://www.reddit.com/r/MachineLearning/comments/4h2lie/solving_the_vanishing_gradient_and_exploding/,JosephLChu,1461980329,"So...  I've been working with Andrej Karpathy's Char-RNN and my Music-RNN, and I found the whole idea of just clipping the gradients to solve the exploding gradient problem be terribly inelegant.  Thus, began a whole series of experiments to see if I could improve upon the method...

Basically the best existing method I could find in the literature involves thresholding the norm of the gradients and then scaling the gradients by the threshold divided by the gradient norm, as suggested by Bengio's lab.  This worked better, but I found the threshold hyperparameter to be clunky, and sought a way to do away with it...

Long story short, I figured out a very simple alternative that seems to work astonishingly well, at least with RMSProp.  Also, the way I am scaling the gradients by the inverse of the norm of the gradients, seems like it would solve not only the exploding gradients, but also the vanishing gradients?  I'm not particularly good at math, so I'm not sure if this would be the case or not.  Though there is a scaling factor parameter that it is rather sensitive to in there that took a long time to discover is actually related to the number of timesteps and the batch size (I think?  I'm not 100% sure right now but the equation seems to work?).

The question now becomes... what should I do now?  Share the details of my implementation right away, or wait and try and publish it somewhere?  For the record, I'm not affiliated with an academic institution anymore, so I'm not even sure if I can submit a paper anywhere.

The thought also occurs to me that this could be some kind of trade secret that is already known by some people, because it seems astonishingly, trivially simple.

Anyways, have something neat:

A clip of my Music-RNN with the old thresholding method:
https://www.youtube.com/watch?v=v_esVXQPGS4

A clip of my Music-RNN with the new method:
https://www.youtube.com/watch?v=CyEtlFRe0jI

I'm not really sure if it's clear from those clips but the validation error numbers are noticeably better with the new method.

Edit:  Here's the formula...

g = tensor of gradients, t = number of timesteps aka sequence length of this pass, b = batch size, n = norm of gradients

g = g * (t * (t / b) / n)

Edit 2:

So, it appears things are complicated by the fact that my version of Char-RNN was modified to implement a Stochastic Timeskip algorithm, a modification of [Stochastic Depth](https://arxiv.org/abs/1603.09382).  I've been testing the Scale Gradient Norm algorithm with the probability that a timestep will be present set to 1, but it's very possible that the way in which I've implemented this means that on extremely rare occasions it will skip a timestep.  This may explain the apparent performance increase.

Edit 3:

So, I based my theory on a run with the Music-RNN in which I actually used the formula:

g = g * (t * 5 / n)

I had assumed that t * 5 = 1250 and so thought that that meant that 250 * 250 / 50 was the correct scaling factor.  However, I neglected to consider that t is variable based on the Stochastic Timeskip probability.  Thus, I realized that t * 5 occasionally actually is less than 1250.  From further experiments with Char-RNN, I believe the proper formula is actually...

g = tensor of gradients, tcount = variable number of timesteps counted this pass, tmax = total number of timesteps aka sequence length of full network, n = norm of gradients

g = g * (tcount^2 / tmax / n)

Edit 4:

Alright, so after much experimenting I think I finally have this figured out...  it has to do with... wait for it... The Golden Ratio!

Basically for the Stochastic Timeskip implementation I was using, the formula is something like (still figuring out where the timesteps fits into this):

g = g * ((1 + sqrt(5)) / 2  * t / n)

For those who are just implementing a regular RNN with fixed timesteps you can leave them out and get a good approximation with the simplified equation:

g = g * ((1 + sqrt(5)) / 2  / n)

where g = gradients, n = norm of gradients

For those of you who aren't aware (1 + sqrt(5)) / 2 is the Golden Ratio, or approximately 1.618.  Don't use 1.618 though, because that's not precise enough.  I recommend using the actual irrational number via the above equation.",25,6,False,self,,,,,
926,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,12,4h2vpt,rpubs.com,Data Science Interview Questions with Detailed Answers,https://www.reddit.com/r/MachineLearning/comments/4h2vpt/data_science_interview_questions_with_detailed/,iamstp,1461985238,,2,4,False,default,,,,,
927,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,13,4h385f,self.MachineLearning,How do you find machine learning conventions/meetups/etc close to you?,https://www.reddit.com/r/MachineLearning/comments/4h385f/how_do_you_find_machine_learning/,fuckinghelldad,1461992016,,5,0,False,self,,,,,
928,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,15,4h3iob,self.MachineLearning,"What kind of product/service can be developed, based on unsupervised (statistical) machine learning, unsupervised natural language processing and computer vision?",https://www.reddit.com/r/MachineLearning/comments/4h3iob/what_kind_of_productservice_can_be_developed/,rousse101,1461999000,,6,0,False,self,,,,,
929,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,16,4h3moa,benjaminbolte.com,Deep Language Modeling for Question Answering using Keras,https://www.reddit.com/r/MachineLearning/comments/4h3moa/deep_language_modeling_for_question_answering/,code_kansas,1462001897,,8,17,False,http://b.thumbs.redditmedia.com/JvtNlixvxYl3ox2EsVor-wm0xW--kPaTu_OmF0lAyMI.jpg,,,,,
930,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,20,4h438z,googleresearch.blogspot.com,DeepMind moves to TensorFlow,https://www.reddit.com/r/MachineLearning/comments/4h438z/deepmind_moves_to_tensorflow/,iamkeyur,1462014952,,1,0,False,http://b.thumbs.redditmedia.com/X5DMPBgn68GEue92aQVTGAWHSXhu_cfTyuqqdACRF1w.jpg,,,,,
931,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,21,4h49nf,self.MachineLearning,Which Deep Learning library has the least GPU memory use ?,https://www.reddit.com/r/MachineLearning/comments/4h49nf/which_deep_learning_library_has_the_least_gpu/,erogol,1462019294,"For my experience, I see MxNet has very moderate compared to Caffe and Theano based libraries. For instance, I can train 48 batch size InceptionV2 with BN in 4GB card with MxNet but not Caffe or Theano. I never tried Torch for such large scales (I always intent but since the language weirdness I scared :). I expect to hear your comments for other libraries as well.   ",14,13,False,self,,,,,
932,MachineLearning,t5_2r3gv,2016-4-30,2016,4,30,23,4h4qdd,self.MachineLearning,NN demo in NetLogo,https://www.reddit.com/r/MachineLearning/comments/4h4qdd/nn_demo_in_netlogo/,_cr_55_,1462028130,"Just a simple NN demo, but it is pretty cool how easy it is done with NetLogo.

It tries to fit data generated from one of two functions:

- Majority (it returns 1 if the input has more 1's than 00's)

- Even (it returns 1 if there are an even number of 11's).

The result (click ""Setup"" first):
http://www.cs.us.es/~fsancho/Modelos/ANN.html

And how it is done:
http://www.cs.us.es/~fsancho/?e=135",0,0,False,self,,,,,
