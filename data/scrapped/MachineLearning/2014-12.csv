,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2014-12-1,2014,12,1,10,2nw9u8,It seems like the first step toward good machine vision is scale/distance logic,https://www.reddit.com/r/MachineLearning/comments/2nw9u8/it_seems_like_the_first_step_toward_good_machine/,[deleted],1417397464,"Is this the current favored technique?

Background: Natural or man made (Color, texture, pattern)         

Judging Distance to camera: Visible texture, Atmospheric diffusion        

Judging distance to background: Shading, lighting  
         
Texture: Metallic - shiny, machine scraped texture, silvery or coppery. Particulates? Fibrous?         

Big or small: Diffusion, visible texture pattern, lighting/shading, blurred close-up effect, using logic relative to size of background

Man Made: Perfect/straight lines + Metallic. Perfect curved surface

It seems like judging scale and distance is the very first step to determining what it is you're looking at. And it seems the best way to do that is to analyze diffusion, texture, camera effects (Blurring), and reasoning out the distance of the background and the object.

Is this currently done?

",3,1
1,2014-12-1,2014,12,1,13,2nwrmu,Interest in machine learning reading group meetup following this course http://www.seas.harvard.edu/courses/cs281/,https://www.reddit.com/r/MachineLearning/comments/2nwrmu/interest_in_machine_learning_reading_group_meetup/,[deleted],1417407851,"How many people would be interested in machine learning reading group meetup following this course http://www.seas.harvard.edu/courses/cs281/.
We can meet weekly and read the book ""Kevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press."". We can also do the assignment from the course.
This is a tough course with heavy mathematics. The prerequisites are:

1) any intro to machine learning

2) statistical inference berger and cassella

3) r programming (the book suggest matlab code, but we can easily translate it to r)

P.S.: reading this book won't guarantee you a data science job. please see it as learning something complex for fun.",5,13
2,2014-12-1,2014,12,1,14,2nx0eg,Dimensionality Reduction for non numerical data?,https://www.reddit.com/r/MachineLearning/comments/2nx0eg/dimensionality_reduction_for_non_numerical_data/,IllSc,1417413511,"If I have datasets with lot of features, some of them are non numerical (symbolic), is it possible to apply Dimensionality Reduction like with PCA and/or using Feature Extraction like RBM ?",11,6
3,2014-12-1,2014,12,1,16,2nx8hr,"""Deep Learning"" with non-Image/Audio/Text data. (Proteins)",https://www.reddit.com/r/MachineLearning/comments/2nx8hr/deep_learning_with_nonimageaudiotext_data_proteins/,ddofer,1417419993,"Hi,
I work with data that isn't Vision/images, audio or natural text, (Protein sequences). 
I've done a fair bit of work on extracting features from the sequences using a variety of representations (K-mer frequencies, discrete and continuous properties, time-signal representations, etc' ), and have worked with this using mainly SVMs and Random Forests for the best results up until now. 

I'm interested in applying some more ""advanced"" methods to this, mainly:
A) techniques which can take advantage of huge amounts of unlabelled (incredibly heterogenous) samples (i.e. some of the 80 million protein sequences known). 
 From what I know, Deep belief networks, and the various autoencoders + layers of thereof would be a good starting point for this? 
B) Use of multi-layer networks (EG multilayer perceptrons) and other techniques to better classify the data. (In conjunction with A presumably). 

The features are mostly partially linear but weak, (i.e. individually relevant strong features, - a linear SVM does work, but with mediocre performance); And dimensionality of the ""raw"" features is moderately high - ~hundreds to tens of thousands (depending on what type of N-Gram length I extract). 
(I've tried applying PCA but performance dropped). 

I frame this as a binary classification task usually, In most cases my positive set is a few hundred to a few thousand samples, while my negative set is a random sampling from the ""unknown"" background of many millions of proteins not annotated as having the properties of the positive set, (i.e P-U learning framed as a binary case). 
(So - overfitting is a very big issue, even with Cross validation and relatively simple models such as RF/ensembles). 

All my work so far has been with scikit learn (Python 3+). 
 Nolearn and Breze look good, but I haven't managed to get them running on my home Windows PC, while the Lab Linux PC is limited in what I can install. 
Ease of use is a major concern for me, I'm a neophyte with no rigorous background in ML, and I completely lack intuitions when it comes to nets. EG: How many layers to start with? Should they be larger or smaller than the input dimensionality [the literature is conflicted], should the layers go : [Large, medium, small, medium, large], or something else? Does it really make sense to try stacked sparse filtering and to have the first layer be larger than the input dimensionality? (Example from the FastML blog). etc' 

Almost all the literature I've seen really focuses on Images, or audio text, and not ""Here's an unknown dataset, get better results than a RF with it""..  (Again, a sole exception is the FastML ""Kaggle BlackBox challenge"" post).
Thank you very much!


PS -
Here's a basic form of the features and datasets I have in mind:
http://www.ncbi.nlm.nih.gov/pubmed/24336809

PPS -

 If it'll make it easier, imagine I'm giving as an example the Kaggle ""Forest Cover Type Prediction"" challenge. Same issues apply in terms of lack of intuition.

Thanks very much for any tips/advice/folk wisdom!

",17,31
4,2014-12-1,2014,12,1,17,2nxc6p,Undefined function 'binornd' for input arguments of type 'double',https://www.reddit.com/r/MachineLearning/comments/2nxc6p/undefined_function_binornd_for_input_arguments_of/,[deleted],1417423786,"my matlab is not finding binornd can anyone upload their source code for the file????
(matlab R2014b)",0,0
5,2014-12-1,2014,12,1,18,2nxdi5,LSTM-network and multivariate Time-Series,https://www.reddit.com/r/MachineLearning/comments/2nxdi5/lstmnetwork_and_multivariate_timeseries/,[deleted],1417425216,"I'm currently working on a project with multivariate time-series data. Hence, each time point *t* of an input sequence consists of a vector of *n* values. I need to use LSTM networks for predicting a single scalar output value at each time point *t* from the input sequence. 

My problem is, I don't think there is existing software which is able to deal with such a input data-structure. Therefore, I'm trying to mitigate this fact by adjusting the network topology to make standard approaches work. 

The only idea which came to my mind is to train *n* seperate networks (with a standard univariate time-series) and combine their output at the last stage with another neuronal net. The problem is that this approach leads to sideeffects which I would like to omit.

Do you have some other ideas/hints?

Edit: typos/grammar

",0,1
6,2014-12-1,2014,12,1,19,2nxgfs,Robustness to wrong labels,https://www.reddit.com/r/MachineLearning/comments/2nxgfs/robustness_to_wrong_labels/,pl47,1417428377,"I was wondering if someone cold refer to some literature on the robustness to wrongly labelled learning data. I currently have a problem where I have a limited amount of examples for which I have some estimate of the robustness of the label. 
So which algorithms are best suited for this ? 
I  know adaboost is particularly bad for this due to the exponential loss function, and hence a Huber loss function would be more robust. 
But what about classification ? There the loss function tends to be deviance or cross-entropy, would one just up-weight the training examples for which you the labels are more robust ? 
Is there a  robust-loss function you can use for classification ? 
Is there any consensus if NN or RF are more robust to mislabeled data ?   

Many thanks for your help.


 




        ",9,3
7,2014-12-2,2014,12,2,0,2ny3vd,Upcoming workshop combining Machine Learning with Security: Looking for papers,https://www.reddit.com/r/MachineLearning/comments/2ny3vd/upcoming_workshop_combining_machine_learning_with/,caedin8,1417448580,"Hi I am a research assistant to Dr. Verma at the University of Houston, 

Dr. Verma and 4 other professors from the University of Texas, University of Houston, and the University of Massachusetts are organizing the ""International Workshop on Security and Privacy Analytics (SPA 2015)"" co-located with ACM CODASPY 2015.

The workshop is looking to publish new research in the combined fields of security and analytics including Machine Learning and NLP. This is the official statement:


&gt; The workshop will focus on, but is not limited to, the following areas:

&gt; * Natural Language Processing for security/privacy;
&gt; * Data Mining techniques for security/privacy;
&gt; * Machine learning for security/privacy;
&gt; * Statistics for security/privacy;
&gt; * Inference Control;
&gt; * Privacy-preserving data mining;
&gt; * Security of machine learning;
&gt; * Security of data mining;
&gt; * Security of natural language processing; 
&gt; * Case studies;
&gt; * Educational topics and courses.


If any of you are are working on new research that would apply to this workshop and are interested, please let me know or follow the information at http://capex.cs.uh.edu/?q=secanalysis2015. I will be at the workshop and if any one from here decides to go or submit work please feel free to contact me!

Thank you!",2,0
8,2014-12-2,2014,12,2,1,2nybef,measuring classifier concordance,https://www.reddit.com/r/MachineLearning/comments/2nybef/measuring_classifier_concordance/,[deleted],1417452609,"Suppose I have a collection of binary classifiers, each of which outputs a ""probability"" value given a test vector. Quotes because outputs are not calibrated or necessarily between 0 and 1, but correlated in some way with label probability. 

How would you quantify the similarity between these classifier's predictions, lacking any knowledge of the true labels?",0,1
9,2014-12-2,2014,12,2,2,2nyfsh,Allreduce (or MPI) vs. Parameter server approaches,https://www.reddit.com/r/MachineLearning/comments/2nyfsh/allreduce_or_mpi_vs_parameter_server_approaches/,vkhuc,1417454864,,0,7
10,2014-12-2,2014,12,2,9,2nzwbi,What's the best way to deploy a complicated machine learning model into a simple environment that has no libraries?,https://www.reddit.com/r/MachineLearning/comments/2nzwbi/whats_the_best_way_to_deploy_a_complicated/,inoddy,1417481683,"For example, I can develop a model (say a random forest) in R but then have to deploy it where I can't use R and have to write all the code myself.

A possible way is to use R to generate all possible predictions and then fit or even over fit a linear regression model to my predictions aiming for 100% R squared. This could be difficult to achieve with linear models.

Any better methods?
",7,0
11,2014-12-2,2014,12,2,12,2o0amd,How does a single neuron!!!,https://www.reddit.com/r/MachineLearning/comments/2o0amd/how_does_a_single_neuron/,[deleted],1417489277,"I am not able to understand how a single neuron learns, the specific problem that I am trying to solve is that I have been given a single neuron which obeys the Gradient Descent Optimization and there are n training samples {Xi} where each Xi is a 2D vector and I have to use a logistic sigmoid activation function for the neuron, along with n input vectors I have been given an output vector Tn which consists of the expected output values for the input vectors, we perform feed forward propagation technique on the neuron using ""an = w0 + w1*x1n + w2*x2n""  and the output of the neuron is yn = 1/(1+exp(-an)) now I need to write a formula for sum of squares error function with weight decay regularizer for the neuron(firstly I don't understand what this weight decay regularizer does and how does it help), secondly I need to perform the gradient descent and derive the weight update iteration for w0, w1 and w2 (I don't understand this sadly :( ).

Would be great if someone could help?",0,0
12,2014-12-2,2014,12,2,12,2o0bbm,What metaheuristics have the smallest time complexity w.r.t. number of variables?,https://www.reddit.com/r/MachineLearning/comments/2o0bbm/what_metaheuristics_have_the_smallest_time/,[deleted],1417489645,"Maybe there isn't a definite answer to this, as I can't find one when looking around. I'm talking about algorithms which would optimize a function which is given (for example) a binary string as the parameters. 
",0,1
13,2014-12-2,2014,12,2,12,2o0btc,Will expectation maximization always improve upon the last iteration until convergence?,https://www.reddit.com/r/MachineLearning/comments/2o0btc/will_expectation_maximization_always_improve_upon/,[deleted],1417489926,"I am trying to implement an expectation maximization algorithm, and my implementation tends towards the optimal solution, but the tedancy is not consistent (calculated from the kullback leibler divergence). Is that evidence that my algorithm is not correctly implemented?",4,1
14,2014-12-2,2014,12,2,16,2o10zs,Deep learning for chess,https://www.reddit.com/r/MachineLearning/comments/2o10zs/deep_learning_for_chess/,benanne,1417506869,,24,101
15,2014-12-2,2014,12,2,17,2o12uv,How to get a job at DeepMind. And Elon Musk's comments.,https://www.reddit.com/r/MachineLearning/comments/2o12uv/how_to_get_a_job_at_deepmind_and_elon_musks/,[deleted],1417508785,"What would someone without a PhD need to do to get a job at a top 10 research group on deep learning / artificial intelligence?  Would you get a job offer if you broke some important machine learning benchmark?  Or won a competition?  Would that be enough?

Do you really need to be at one of these places like DeepMind to be at the cutting edge of the field?  I mean, a lot of the job openings for DeepMind seem to be more mundane stuff, i.e. stuff where on the day to day, you might just be coding on a need to know basis, and be kept in the dark about the bigger picture stuff.

The reason I want to get a job at DeepMind, is after hearing Elon Musk's comment about how within 5 years A.I. is going to be dangerous.  I know what a true A.I. could do, and basically, if someone had a true A.I. with the IQ of a human, they could take over the world within 2 years.  That is not a joke.  Once it reached human level intelligence it could boostrap itself up to 100x or 1,000x a human, and then we are talking about a machine that could invent things we can't even imagine and do it in a matter of hours or minutes.

So, do you think that DeepMind is so much ahead of the rest of the field that they could do this?  I mean, we already heard from a very rational person who claims they have ""Superintelligences"".  Seeing how close deep learning is to the way the human brain works, I think it is very plausible, especially with the money and talent Google is undoubtedly throwing at it.

Another bad scenario would be that the government is going to realize what a weapon A.I. is and they will make it completely illegal.
",0,0
16,2014-12-2,2014,12,2,21,2o1fq6,"Lego robot with the ""brain"" of a real worm",https://www.reddit.com/r/MachineLearning/comments/2o1fq6/lego_robot_with_the_brain_of_a_real_worm/,rumpelstilzien,1417522405,,1,8
17,2014-12-2,2014,12,2,23,2o1puh,Stephen Hawking warns artificial intelligence could end mankind,https://www.reddit.com/r/MachineLearning/comments/2o1puh/stephen_hawking_warns_artificial_intelligence/,katchoovanski,1417530614,,1,0
18,2014-12-3,2014,12,3,0,2o1une,Advice on self-study for my winter break?,https://www.reddit.com/r/MachineLearning/comments/2o1une/advice_on_selfstudy_for_my_winter_break/,Nixonite,1417533689,"Hello everyone,

I'm a math major interested in getting into data science. I took a seminar on data science this semester (ending in 2 weeks) and it was entirely on data mining/python/data cleaning/visualization etc.

I want to spend my winter break of 6 weeks studying machine learning and just getting a deeper feel of the subject. I don't like video lectures and I kinda poked my head into ""The Elements of Statistical Learning"" but I feel like it may be just a little bit over my head (only on page 18). 

I'm an experienced python programmer (with understanding of basic data structures/algorithms/complexity) and have taken the following math courses:{calc 3, intro linear algebra, intro probability, intro real analysis}. Is there a book or pdf that I can read and finish in 6 weeks that will give me the *theory* of machine learning in some sort of nutshell?

Should I just put my face into The Elements of Statistical Learning for 6 weeks (it's not undoable, just seems kinda slow) or do you have a better book recommendation? 

Oh and I think I can only manage 400-500 pages in 6 weeks (Elements is like 700+). 

I have the practical side covered with the book ""Building Machine Learning Systems with Python"" so only theory recommendations please and thank you!",6,9
19,2014-12-3,2014,12,3,1,2o1z5c,Getting the index of a leaf where an example belongs in scikit learn,https://www.reddit.com/r/MachineLearning/comments/2o1z5c/getting_the_index_of_a_leaf_where_an_example/,[deleted],1417536214,"Hi everyone,

Is there any way to get the leaf to which an example belongs in scikit-learn's trees ?
Actually I am trying out the method suggested in this article 
[practical lessons facebook from Predicting Clicks on Ads at Facebook](https://www.facebook.com/publications/329190253909587/)

If you guys have any suggestions, or have tried out the approach yourselves, I'll be glad to hear your advice.

Thank you !",1,0
20,2014-12-3,2014,12,3,1,2o22v3,Cognitive algortihm,https://www.reddit.com/r/MachineLearning/comments/2o22v3/cognitive_algortihm/,bkaz,1417538121,"Lots of people think that main problem in AI is a lack of funding. I disagree, Einstein didn't need a pay to work on his theory. GI is the most theoretical problem ever, &amp; such work must be driven by curiosity. Anyway, I offer prizes of up to ~$500K for relevant ideas or stimulating questions &amp; objections. $1600 is paid out so far, mostly for stimulation. This wont work unless my intro (below) rings a lot of bells for you.

My approach is unsupervised learning, vaguely similar to hierarchical fuzzy clustering. But no method that I know of is close enough. This field is largely experimental (especially ANNs), while I believe in theoretical integrity. So, I had to start from the scratch. I am not interested in combining disparate methods that somehow work, any suggestion must fit into strictly incremental-complexity search hierarchy outlined in the intro. Unless you can find a flaw in my reasoning, which would be even more valuable:

I define intelligence as ability to predict &amp; plan, which can only be done by discovering &amp; projecting patterns. This perspective is well established: pattern recognition is a core of any IQ test. A pattern is a set of matching inputs or lower-level patterns. Partial matches &amp; misses are produced by comparison among input patterns, over selectively extended range of search. I see no alternative to these definitions, &amp; expand them to derive operations on the inputs. This deduction is uniquely incremental, hence a singular in cognitive algorithm...:

www.cognitivealgorithm.info
",11,0
21,2014-12-3,2014,12,3,2,2o280e,Python Tools for Visual Studio now integrates with Azure Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2o280e/python_tools_for_visual_studio_now_integrates/,MLBlogTeam,1417540773,,0,1
22,2014-12-3,2014,12,3,2,2o28ct,Libraries to get calibrated probabilities for ML outlier detection algorithm ?,https://www.reddit.com/r/MachineLearning/comments/2o28ct/libraries_to_get_calibrated_probabilities_for_ml/,ighbal,1417540957,"I am currently developping a 2 class classification algorithm. However, as the dataset is at the moment really small (&lt;50 observations) and imbalanced (~1/10 ratio), I decided to rather first concentrate on developing a one class, i.e. outlier detection ML algorithm. Moreover, I'll need to get from this ML algorithm, an estimate of the conditional probability P(Outlier|X).
As I have read that conditional probabilites for strong imbalanced dataset, needs correct calibration, and that Platt scaling is not recommended in that case,
I'd like to ask to the community here if it exist some libraries in R, Python, ... that would facilitate this calibration exercise.",1,0
23,2014-12-3,2014,12,3,4,2o2ll4,Deep Learning RNNaissance with Dr. Juergen Schmidhuber (NYC-ML Meetup),https://www.reddit.com/r/MachineLearning/comments/2o2ll4/deep_learning_rnnaissance_with_dr_juergen/,xamdam,1417547475,,3,7
24,2014-12-3,2014,12,3,5,2o2s8a,Need help with approaching enrollment prediction,https://www.reddit.com/r/MachineLearning/comments/2o2s8a/need_help_with_approaching_enrollment_prediction/,[deleted],1417550670,"Hello! So, we've broken the greater enrollment problem into a smaller one that just looks at the past 5 years worth of transcripts and sees if it can use that information to predict whether or not a student will take a specific class in the next quarter. I have a function that converts a ""transcript"" into a bit vector - 1 if the class has been taken and 0 if the class has not been taken. Each vector is ordered exactly the same and each vector is followed by True or False indicating whether or not (in the next quarter) they took the class we're interested in (can easily be turned to 0 or 1). I'm just wondering how I would use logistic regression (scikit-learn) to approach this problem, I'm a little stuck/lost. 
Also if there are any things you would change or want to clarify, ask away.
Thanks!",1,0
25,2014-12-3,2014,12,3,5,2o2us0,"How to use the ""little bag of bootstraps"" methodology to compute ""error bounds"" on machine learning tasks.",https://www.reddit.com/r/MachineLearning/comments/2o2us0/how_to_use_the_little_bag_of_bootstraps/,AnnaOnTheWeb,1417551880,,0,8
26,2014-12-3,2014,12,3,6,2o31a5,Help/Resources with/for Bayesian Optimization,https://www.reddit.com/r/MachineLearning/comments/2o31a5/helpresources_withfor_bayesian_optimization/,genix2011,1417554889,"Hello all,

I want to implement Bayesian Optimization like described in the paper ""Active Preference Learning with Discrete Choice Data"" - by Eric Brochu, Nando de Freitas and Abhijeet Ghosh (http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_902.pdf). 
Unfortunately it seems like it is too much for me right now, I seem to miss a lot of knowledge in GP and Statistics.

Does anyone have any resources that could help understanding Bayesian Optimization better? Or, perhaps even an implementation of the referenced paper?

Thank you very much.",1,2
27,2014-12-3,2014,12,3,8,2o3l2x,How to deploy a model from scikit-learn to android?,https://www.reddit.com/r/MachineLearning/comments/2o3l2x/how_to_deploy_a_model_from_scikitlearn_to_android/,[deleted],1417564460,"I'm not sure if this is the best place to be asking this question, but I'm not sure where else I would ask it. I have made a model using support vector machines in scikit-learn and I would like to use it in an android app. Are there anyways to easily port this model? After doing some searching I was unable to find anything, so any references would be of help.",5,3
28,2014-12-3,2014,12,3,10,2o3vu3,Is there any ML technique that relies on long term memories?,https://www.reddit.com/r/MachineLearning/comments/2o3vu3/is_there_any_ml_technique_that_relies_on_long/,SrPeixinho,1417569914,"I've been looking through ML techniques and most of them, if I get it right, are kinda ""flat"" in that they search through some space to solve some specific problem. After the training phase, it doesn't really change much. Is there any kind of technique that targets the long term? That is, teaching something many different things, and allowing it to improve its memory and gain new experiences with time, never stopping to learn (including when it is computing something)?

I've also never heard of an ML algorithm that learns about the problem **while** it computes it.",5,0
29,2014-12-3,2014,12,3,13,2o4fxx,SciKit-Learn Laboratory (SKLL),https://www.reddit.com/r/MachineLearning/comments/2o4fxx/scikitlearn_laboratory_skll/,danielblanchard,1417580752,,10,45
30,2014-12-3,2014,12,3,15,2o4q0f,Learning algorithms CheatSheet?,https://www.reddit.com/r/MachineLearning/comments/2o4q0f/learning_algorithms_cheatsheet/,deathstone,1417587073,"Hello r/MachineLearning

I have my finals coming up and I'm enrolled in a Machine Learning course at my university. The course is not very well taught and I had been doing the Andrew NG course on Coursera before the semester began. Due to certain other responsibilities, I was unable to complete the Coursera course. My finals are in 2 days and the finals has a complete section with Modelling problems.
 
The course has been taught in such a way that it is completely theoretical and less application focused. I messed up in this regards and didn't pay much attention to the application area of ML.
The professor has focused on Decision trees, ANN's, SVMs, Clustering, Bayesian Networks, Reinforcement Learning, Ensemble Classifiers, Active learning and Dimensionality reduction methodologies. 

Is there any resource available which gives a side by side comparison of the application of these algorithms to various problems or just a repository of Machine learning modelling problems with their solutions so that I can get into that particular mindset which would be helpful to me at this point of time.

I know I messed up this semester, I would really appreciate it if you guys could help me out at this late hour.",2,0
31,2014-12-3,2014,12,3,16,2o4wpv,Deep Learning Face Representation by Joint Identification-Verification -- Paper that reports better accuracy than Deepface on LFW,https://www.reddit.com/r/MachineLearning/comments/2o4wpv/deep_learning_face_representation_by_joint/,rorschach122,1417592323,,0,2
32,2014-12-3,2014,12,3,17,2o50kt,Artificial Intelligence for Games!!!,https://www.reddit.com/r/MachineLearning/comments/2o50kt/artificial_intelligence_for_games/,Sergiointelnics,1417596155,,0,0
33,2014-12-3,2014,12,3,19,2o554p,Pylearn2: a machine learning research library,https://www.reddit.com/r/MachineLearning/comments/2o554p/pylearn2_a_machine_learning_research_library/,iwansyahp,1417601080,,0,0
34,2014-12-3,2014,12,3,20,2o59ud,Understanding Big Data: What is unsupervised learning,https://www.reddit.com/r/MachineLearning/comments/2o59ud/understanding_big_data_what_is_unsupervised/,soniyaiyer,1417606240,,0,0
35,2014-12-3,2014,12,3,23,2o5mmw,[1412.1058] Effective Use of Word Order for Text Categorization with Convolutional Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2o5mmw/14121058_effective_use_of_word_order_for_text/,improbabble,1417617151,,7,4
36,2014-12-4,2014,12,4,1,2o5zp3,HTM in Java!,https://www.reddit.com/r/MachineLearning/comments/2o5zp3/htm_in_java/,numenta,1417624737,,12,17
37,2014-12-4,2014,12,4,2,2o682s,Online masters in ML?,https://www.reddit.com/r/MachineLearning/comments/2o682s/online_masters_in_ml/,ai_noob,1417629089,Are there any valid all online options out there for getting a master's degree in machine learning?  I did a quick search myself and couldn't get past all the ads for random online programs.  I currently have a bachelor's in CS from MIT so I think I should be able to jump right in but there are no programs near where I live. Thanks.,12,3
38,2014-12-4,2014,12,4,3,2o6d51,"How do I choose the ""right"" or ""optimal"" random forest model?",https://www.reddit.com/r/MachineLearning/comments/2o6d51/how_do_i_choose_the_right_or_optimal_random/,TissueReligion,1417631604,"Hey guys. I have a practical concern about the stochasticity of random forest generation.

So lets say I have a dataset that I'm training a random forest model on, and I do 100 simulations, generating a new random forest each time, and I get a new model with a new associated classification accuracy each time.

What is a rational approach to selecting the ""optimal"" random forest model? Just selecting the one with the highest accuracy on a particular set of training data doesn't seem like a good metric.

Would this approach make sense?
-Generate 100 random forests on 100 different partitions of the test data
-Measure the classification accuracy of each of the random forests on 100 partitions of the test data (ie, 100 random forests * 100 tests each = 10,000 tests total)
-Pick the forest with the highest total accuracy.

Eh?",19,7
39,2014-12-4,2014,12,4,6,2o6w9p,Astronomical data mining?,https://www.reddit.com/r/MachineLearning/comments/2o6w9p/astronomical_data_mining/,kinofpumps,1417640828,"I just heard about this field today, and it's got me really interested. I'm applying to graduate schools for a master's is statistics, and have a strong programming background. Does anybody know anything about this field? What jobs there are, and where? It seems like a great way to combine my love of space with programming and stats. ",7,5
40,2014-12-4,2014,12,4,6,2o6z5n,Images to Text demo - Toronto Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2o6z5n/images_to_text_demo_toronto_deep_learning/,benanne,1417642216,,9,15
41,2014-12-4,2014,12,4,6,2o72mk,Taming Latency Variability and Scaling Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2o72mk/taming_latency_variability_and_scaling_deep/,oneAngrySonOfaBitch,1417643971,"Great talk on how google is implementing Deep Learning to solve real problems in production.

https://plus.google.com/+ResearchatGoogle/posts/C1dPhQhcDRv",0,1
42,2014-12-4,2014,12,4,8,2o7a76,Scikit vs R for random forest,https://www.reddit.com/r/MachineLearning/comments/2o7a76/scikit_vs_r_for_random_forest/,neelshiv,1417647892,"I am looking into a random forest model that will use both continuous and categorical variables. I have used scikit for forests in the past, but I've never had to use categorical variables. Based on what I've read, these seem to be the immediately obvious pros and cons for the two...

* R can handle categorical variables without any transformation
* R can't handle categorical variables that have more than 32 levels (and I'm using state). The ability to use categorical variables might not matter
* R can choose a multiple levels from a category at a specific branch. If I have a dummy variable for each label in a category, this is not possible
* Creating a single numeric variable for categories in scikit implies ordinality. Creating a dummy variable for each level of each categorical variable could drastically increase data size and dimensionality
It seems like there are pros and cons to each.

Unless my understanding of ordinality in integer labeled categories is wrong, it seems like a branch in a scikit can't accomplish the same level of interaction that one in R could because R could split on multiple values of a categorical variable instead of one value at a time. At the same time, I wouldn't even be able to pass a few of my categories into R because of the label limit. With sufficiently deep trees and sufficiently large forests, would this difference in implementation even matter?

Any thoughts would be appreciated. Thanks.",7,10
43,2014-12-4,2014,12,4,11,2o7xgp,"63 Machine Learning, Data Science, Big Data Resources and Articles",https://www.reddit.com/r/MachineLearning/comments/2o7xgp/63_machine_learning_data_science_big_data/,urinec,1417660213,,0,0
44,2014-12-4,2014,12,4,15,2o8j56,Ways to improve dynamic time warping word recognition system?,https://www.reddit.com/r/MachineLearning/comments/2o8j56/ways_to_improve_dynamic_time_warping_word/,kaustest,1417673094,"I recently got interested in speech recognition and have implemented a simple dynamic time warp system for word recognition for my own learning purpose. However after testing a bit I believe that I might made a mistake somewhere in the implementation, as the least distance value in traversing the dtw matrix does not accurately separate different word from each other. Here is the step I followed in the implementation.

1. Compute mfcc for two wav samples using https://github.com/jameslyons/python_speech_features (I will eventually replace it with my own mfcc algorithm)

2. Compute l2 norm for the top 13 mfcc in order

3. Traverse through the dtw matrix and find the least distance.


Below are my results

Comparing 1 of the kiwi file to all other file I get the following average

    kiwi
    53.5627956541
    apple
    52.8226506157
    banana
    57.885524018
    lime
    48.5113003162
    orange
    63.9675030969

Here is my code https://gist.github.com/anonymous/1323692feea2a2bcfba4

I feel I am missing some crucial steps, any advice will be appreciated.

Thanks

Edit:

I am using audio file from

https://dl.dropboxusercontent.com/u/15378192/audio.tar.gz

and 

http://www.forvo.com/word/apple/#en",8,5
45,2014-12-4,2014,12,4,17,2o8s44,Possible Commercial-friendly Caffe Pre-trained Nets,https://www.reddit.com/r/MachineLearning/comments/2o8s44/possible_commercialfriendly_caffe_pretrained_nets/,CompleteSkeptic,1417680741,"A friend of mine told me that during an Nvidia webinar today (""DIY Deep Learning for Vision: A Tutorial with Caffe""), someone mentioned that the pre-trained nets will be released with a new commercial-friendly license. I know quite a few companies that would be pleased with the news, and I assume others would be as well. (:

Has anyone else heard this though or have any news on it?",7,3
46,2014-12-4,2014,12,4,18,2o8wbt,Data Science @ ESIEE Paris - Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/2o8wbt/data_science_esiee_paris_yann_lecun/,paralax77,1417685330,,1,2
47,2014-12-4,2014,12,4,22,2o99r2,"From G+, Andrej Karpathy: ""I rendered the NIPS 2014 papers in pretty format again.""",https://www.reddit.com/r/MachineLearning/comments/2o99r2/from_g_andrej_karpathy_i_rendered_the_nips_2014/,test3545,1417699146,,6,25
48,2014-12-5,2014,12,5,2,2o9vmm,How to WEKA the living daylights outta my data with a real life scenario???,https://www.reddit.com/r/MachineLearning/comments/2o9vmm/how_to_weka_the_living_daylights_outta_my_data/,shugakayne,1417712709,"I must say this is exciting, i absolutely have no foundation in computer science or programming and neither was i very good at mathematics but somehow i am in love with the idea of machine learning, probably because i have a real life scenario i want to experiment with.

I have up to 20 weekends and more of historical data of matches played and i would like to see how weka can predict the outcome of matches played within that 20 week period.

My data is in tabular form and it is stored in microsoft word.
It is a forecast of football matches played in the past.

Pattern detection is the key, By poring over historical data of matches played in the past, patterns begin to emerge and i use this to forecast what the outcome of matches will be for the next game.

I use the following attributes for detecting patterns and making predictions which on paper is always 80-100% accurate but when i make a bet, it fails.
(results, team names, codes, week's color, row number) 

Results= Matches that result in DRAWS

Team names = Believe it or not, teams names are used as parameters to make predictions, HOW? They begin with Alphabets.

Codes= These are 3-4 strings either digits or a combo of letters and digits, depending on where they are strategically placed in the table, they offer insight into detecting patterns.

Weeks Color= In the football forecasting world, there are 4 colours used to represent each week in a month. RED, BLUE, BROWN and PURPLE. These also allows the forecaster to see emerging patterns.

Row Number= Each week, the data is presented in a table form with two competing teams occupying a row and a number is associated with that row. These numbers are used to make preditions. 

So i would like to TEACH WEKA how i detect these patterns so that my task can be automated and tweaked anyhow i like it.

In plain english, how do i write out my ""pattern detecting style"" for weka to understand and how do i get this information loaded into weka for processing into my desired results.
Going by my scenario, What will be my attributes? 
What will be my instances? 
What will be the claasifiers? 
What algorithms do i use to achieve my aim or will i need to write new algorithms?

I sincerely hope someone will come to my rescue.

Thanks
 ",1,0
49,2014-12-5,2014,12,5,3,2oa8ev,Question about Neural Networks with directed weights,https://www.reddit.com/r/MachineLearning/comments/2oa8ev/question_about_neural_networks_with_directed/,jostmey,1417719194,"A characteristic feature of biological neural networks is that the connections between neurons are directed. Backpropagation as well as Boltzmann machines rely on bidirectional connections that allow information to flow in either direction ~~unidirectional connections~~. In backpropagation the error signal is propagated backwards through the connections to the pre-synaptic neuron, while in Boltzmann machines the weights are assumed to be tied. However, I came across this slide in a talk by Geoffery Hinton, in which he claims that the weights in a Boltzmann machine do not necessarily need to be tied (see Reference). Can anyone refer me to a paper on this? Has someone derived the equations for a Boltzmann machine with purely directed connections?

Reference: http://www.cs.toronto.edu/~hinton/backpropincortex2007.pdf#page=12

Thanks!",6,1
50,2014-12-5,2014,12,5,4,2oaca6,A Dozen Good Videos on Data Science,https://www.reddit.com/r/MachineLearning/comments/2oaca6/a_dozen_good_videos_on_data_science/,rscottking73,1417721078,,0,0
51,2014-12-5,2014,12,5,7,2oawar,"Scalable, High-Quality Object Detection",https://www.reddit.com/r/MachineLearning/comments/2oawar/scalable_highquality_object_detection/,vkhuc,1417731022,,0,5
52,2014-12-5,2014,12,5,8,2ob5lg,Huge drop in quality of neural network when using tanh instead of sigmoid as an activation function?,https://www.reddit.com/r/MachineLearning/comments/2ob5lg/huge_drop_in_quality_of_neural_network_when_using/,mbuffett1,1417735804,"I'm playing around with neural nets in Haskell, and I've got this code :

    import AI.HNN.FF.Network
    import Numeric.LinearAlgebra


    main :: IO ()
    main = do
        net &lt;- createNetwork 2 [5] 1 :: IO (Network Double)
        print samples
        let betterNet = trainNTimes 100 0.8 tanh tanh' net samples
        print (output betterNet tanh (fromList [11,10]))
        print ""hi""


    samples :: Samples Double
    samples = [(fromList [a,b], fromList [out])  | a &lt;- [1..25], b &lt;- [1..25], let out = if a &gt; b then 1 else 0]


Basically I give it input of the form [a, b], and the output is [c], where c is 1 when a &gt; b, and 0 when a &lt;= b. Using tanh as the activation function gives terrible results, when b &gt;= a, the result is usually around -.9, and when a &gt; b, the result is ~.85, but varies wildly, sometimes as low as 0.3. Then, I just swap out every instance of 'tanh' with 'sigmoid', and suddenly it works flawlessly, giving .9999 when a &gt; b, and something like 3*10^-5 when b &gt;= a.

I really don't have much of an idea what I'm doing, so I'm hoping someone can explain the difference in performance, since everything I've been reading on the difference between the two makes it seem like they're almost interchangeable.",5,4
53,2014-12-5,2014,12,5,9,2obcxe,Transforming Auto-encoders,https://www.reddit.com/r/MachineLearning/comments/2obcxe/transforming_autoencoders/,madisonmay,1417739634,,2,7
54,2014-12-5,2014,12,5,9,2obd8a,Train network by comparing 2 set of entries,https://www.reddit.com/r/MachineLearning/comments/2obd8a/train_network_by_comparing_2_set_of_entries/,fawar,1417739813,"I'm wondering if there is an actual method to do the following :


Having 2 set of Identical features which are not the same training exemple as a single training exemple.

A1,A2...A10, B1,B2....B10. =&gt; A is better/ B is better

(Supervised learning)  would it be possible to train a NN pr anything else that way? 
And then use it to. Evaluate a single. Set of feature (once training is done)?",8,0
55,2014-12-5,2014,12,5,13,2obzgp,Machine Learning: The High Interest Credit Card of Technical Debt,https://www.reddit.com/r/MachineLearning/comments/2obzgp/machine_learning_the_high_interest_credit_card_of/,improbabble,1417752553,,12,60
56,2014-12-5,2014,12,5,13,2oc3gk,"I'd love to read a primer on ""Breeding"" in machine learning",https://www.reddit.com/r/MachineLearning/comments/2oc3gk/id_love_to_read_a_primer_on_breeding_in_machine/,[deleted],1417755065,"test 1000 permutations of a program for desired characteristics     
breed the two highest scoring programs (into 1000 **randomized** permutations)       
repeat      

",0,1
57,2014-12-5,2014,12,5,18,2ocnnx,Translating randomForest (R) back into actionable items,https://www.reddit.com/r/MachineLearning/comments/2ocnnx/translating_randomforest_r_back_into_actionable/,kaaswagen,1417772150,"I'm having a hard time translating my random forest back into actionable results. I'm trying to predict a bit flip based on a number of variables using R's excellent randomForest package. Let's say one of my fields is an integer; 'duration' with a value from 0 to 5000. It turns out that in varImpPlot it is by far the best indication of the flip. How do I plot the chance that a bit will be flipped for duration x using the random forest I've made? partialPlot only gets me so far, the axes are basically unlabeled and I can't 'action' that graph. Is there something I'm overlooking?",1,2
58,2014-12-5,2014,12,5,21,2ocvoe,11 open source tools to make the most of machine learning,https://www.reddit.com/r/MachineLearning/comments/2ocvoe/11_open_source_tools_to_make_the_most_of_machine/,futureisdata,1417781277,,1,0
59,2014-12-5,2014,12,5,21,2ocxp9,Neural Machine Translation demo,https://www.reddit.com/r/MachineLearning/comments/2ocxp9/neural_machine_translation_demo/,benanne,1417783230,,6,11
60,2014-12-5,2014,12,5,22,2od10m,Machine Learning in MATLAB?,https://www.reddit.com/r/MachineLearning/comments/2od10m/machine_learning_in_matlab/,[deleted],1417786216,Does anyone here do their machine learning in MATLAB? What do people think about various packages available in matlab? I'm a total beginner and would love to hear people's opinions,6,0
61,2014-12-6,2014,12,6,1,2odkgt,Data science in Python: A practical tutorial,https://www.reddit.com/r/MachineLearning/comments/2odkgt/data_science_in_python_a_practical_tutorial/,piskvorky,1417798335,,20,86
62,2014-12-6,2014,12,6,2,2odnq4,Question on auto-association with multilayer perceptrons,https://www.reddit.com/r/MachineLearning/comments/2odnq4/question_on_autoassociation_with_multilayer/,fascz,1417800049,"Hi!

I am trying to teach a multilayer perceptron to do an auto-association task but I can't get it to work properly. I have little experience with neural networks so I thought that the solution to my problem could be obvious for you guys. I tried many sets of parameters (size of hidden layer and learning rate) on my relatively simple implementation of the backprop rule, but without success. My inputs are images of black and white fish. The problem that I have is that the cost always seems to plateau after a while, and instead of giving me the same output as my input after the training, I get a kind of prototype fish for all inputs. I tried to lower my learning rate but it doesn't seem to be helping.

I have a sigmoid activation for my hidden and output units. My inputs are all vector with 8064 binary features. My cost function is the following:

    def cost(pred,y,_lambda,w1,w2):
        m = pred.shape[0]
        J = np.sum(np.power(pred - y,2))/(2*m)
    if _lambda != 0:
        regularization = (_lambda/(2*m)) * (np.sum(np.power(w1[:,1:],2)) + np.sum(np.power(w2[:,1:],2)))
        J = J + regularization
    return J   

Thank you!",7,1
63,2014-12-6,2014,12,6,4,2oe2cn,Richard Socher's new startup has just been unveiled,https://www.reddit.com/r/MachineLearning/comments/2oe2cn/richard_sochers_new_startup_has_just_been_unveiled/,notarowboat,1417807722,,4,17
64,2014-12-6,2014,12,6,5,2oe7rn,"[homework] trouble with nltk python NaiveBayesClassifier, I keep getting same probabilities",https://www.reddit.com/r/MachineLearning/comments/2oe7rn/homework_trouble_with_nltk_python/,sayan5678,1417810594,"Hey /r/machinelearning--

 I don't see too many [homework] posts here, so I hope I'm not in the wrong sub. If so, please point me to a better option.

 so I'm working on a project in which i take in anime names and genres and if they are relevant or irrelevant 
I am trying to build a NaiveBayesClassifier with that and then I want to pass in genres and for it to tell me if it is relevant or irrelevant
I currently have the following:

    import nltk
    trainingdata =[({'drama': True, 'mystery': True, 'horror': True, 'psychological': True}, 'relevant'), ({'drama': True, 'fantasy': True, 'romance': True, 'adventure': True, 'science fiction': True}, 'unrelevant')]
    classifier = nltk.classify.naivebayes.NaiveBayesClassifier.train(trainingdata)
    classifier.classify({'Fantasy': True, 'Comedy': True, 'Supernatural': True})
    prob_dist = classifier.prob_classify(anime)
    print ""relevant "" + str(prob_dist.prob(""relevant""))
    print ""unrelevant "" + str(prob_dist.prob(""unrelevant""))

I currently have :
    
    size of training array:110
    the relevant length 57
    the unrelevant length 53

Some results I receive :

    relevant Tantei Opera Milky Holmes TD
    {'Mystery': True, 'Comedy': True, 'Super': True, 'Power': True}
    relevant 0.518018018018
    unrelevant 0.481981981982

    relevant Juuou Mujin no Fafnir
    {'Romance': True, 'Fantasy': True, 'School': True}
    relevant 0.518018018018
    unrelevant 0.481981981982

I was wondering if that makes sense... Since I am getting the same relevant probability for each classification it makes.. 
From my understanding of Naive Bayes it shouldn't be doing that... 

using python, nltk
Thanks!",5,0
65,2014-12-6,2014,12,6,5,2oe98q,Help with Generative Adversarial Networks,https://www.reddit.com/r/MachineLearning/comments/2oe98q/help_with_generative_adversarial_networks/,alexmlamb,1417811374,"I just finished implementing Generative Adversarial Networks.  There are a few things that I'm confused about.  

1.  In my experiments, training diverges if I use any type of distribution where regions with no density touch regions with high density.  An extreme example of this is discrete distributions where all of the mass is concentrated at single points.  

I think that this happens because there's a strong penalty for the generator assigning probability density to places where there shouldn't be any mass.  

In the paper, the authors consider two different losses for the generator:  

a: -1.0 * log(D(G(z)))

b: log(1.0 - D(G(z)))

As D(G(z)) -&gt; 0.0, the first loss approaches infinity and the second loss approaches 0.0.  

As D(G(z)) -&gt; 1.0, the first loss approaches 0.0 and the second loss approaches negative infinity.  

I think that the second loss is much better if the true distribution has discontinuities, because it allows the network to put some amount of mass in places that shouldn't have any mass without facing a severe penalty.  I did a simple experiment where I trained the model to reproduce a mixture of a normal distribution and a gamma distribution.  The gamma distribution has a discontinuity which the first loss has a hard time handling: 

Loss a: 

http://imgur.com/bo7TYfJ

Loss b: 

http://imgur.com/MoIZIx4

In both cases green is the true distribution, blue is the generated distributed, and the dots are D(G(z)) from the discriminator.  As you can see, the second loss performs a lot better.  Nonetheless, the paper advocates for the use of the first loss in section 3.  

Perhaps there's another loss for the generator that gets the benefits of both?  

2.  I've found that the generator is a lot harder to optimize than the discriminator.  

3.  The experimental results in the paper are for real data.  Has anyone done experiments showing that the model is calibrated for synthetic data?  ",8,3
66,2014-12-6,2014,12,6,5,2oeawh,Listing Kaggle projects on resume,https://www.reddit.com/r/MachineLearning/comments/2oeawh/listing_kaggle_projects_on_resume/,bthrill,1417812211,I just completed a few tutorials on analyzing the Kaggle Titanic dataset using R and Python from various websites. Should I list these completed submissions on my resume?,8,1
67,2014-12-6,2014,12,6,6,2oeg5t,"Backpropagation as simple as possible, but no simpler",https://www.reddit.com/r/MachineLearning/comments/2oeg5t/backpropagation_as_simple_as_possible_but_no/,stephencwelch,1417815002,,15,24
68,2014-12-6,2014,12,6,7,2oensf,DIY Deep Learning for Vision: A Tutorial with Caffe,https://www.reddit.com/r/MachineLearning/comments/2oensf/diy_deep_learning_for_vision_a_tutorial_with_caffe/,CompleteSkeptic,1417819013,,1,4
69,2014-12-6,2014,12,6,8,2oesw5,Weekend reading - 3 recent stories about Microsoft Machine Learning and Advanced Analytics,https://www.reddit.com/r/MachineLearning/comments/2oesw5/weekend_reading_3_recent_stories_about_microsoft/,MLBlogTeam,1417821779,,0,0
70,2014-12-6,2014,12,6,13,2ofncf,What do you want to hear from a prospect? How to interview for machine learning?,https://www.reddit.com/r/MachineLearning/comments/2ofncf/what_do_you_want_to_hear_from_a_prospect_how_to/,poundcakejumpsuit,1417840817,"I got a technical interview at a machine learning company, and have no idea how to prepare. I'm skilled at C++ and MATLAB, and I've fooled around in Python, but is this gonna be your standard ""balance this node, explain a hash table"" interview? I have a little experience in machine learning but expect to be picking up skills on the fly.",12,2
71,2014-12-6,2014,12,6,19,2og95b,Can machine learning be used to translate complicated concepts into simple terms?,https://www.reddit.com/r/MachineLearning/comments/2og95b/can_machine_learning_be_used_to_translate/,uptnapishtim,1417861522,For example lets say I typed the word trabecilae would machine learning be used to retrieve pictures of the eiffel tower and simplify the physics?,1,0
72,2014-12-7,2014,12,7,1,2ogvut,Namely [NYC] is looking for a Data Science Intern,https://www.reddit.com/r/MachineLearning/comments/2ogvut/namely_nyc_is_looking_for_a_data_science_intern/,manueslapera,1417883657,"Come! It will be fun :)

http://namely.workable.com/jobs/27793",14,0
73,2014-12-7,2014,12,7,3,2oh5wg,Experts divided on Stephen Hawkings claim that Artificial Intelligence could end Humanity,https://www.reddit.com/r/MachineLearning/comments/2oh5wg/experts_divided_on_stephen_hawkings_claim_that/,sheriner,1417889869,,10,0
74,2014-12-7,2014,12,7,4,2ohdq2,Linear combination of weak estimators over fuzzy classifiers?,https://www.reddit.com/r/MachineLearning/comments/2ohdq2/linear_combination_of_weak_estimators_over_fuzzy/,bubaonaruba,1417894372,"Having:

* set of soft fuzzy classifiers (classification onto overlapping sets) C_i(x) -&gt; [0,1]
* corresponding set of weak estimators R_i(z) of the form R_i(z) = EX(y|z).


The estimators R_i are just some kind of regression, kalman or particle filters. The classifiers C_i are fixed and static.

How to make a strong estimator out of a weighted combination of the form L(x, z) = SUM_i: C_i(x)*R_i(z)*Q_i ?


In other words how to choose the weights Q_i?


Is there some kind of online approach to this problem?",0,0
75,2014-12-7,2014,12,7,4,2ohei0,Databricks to run free Scalable Machine Learning MOOC,https://www.reddit.com/r/MachineLearning/comments/2ohei0/databricks_to_run_free_scalable_machine_learning/,edXbecky,1417894841,,8,54
76,2014-12-7,2014,12,7,6,2oho7j,"How to choose linkage in hidden layers (Fully, cluster .. etc)",https://www.reddit.com/r/MachineLearning/comments/2oho7j/how_to_choose_linkage_in_hidden_layers_fully/,fawar,1417900246,"Why should I choose to fully link?

Input to first Hidden layer?
Hidden Layer to Hidden Layer?
Hidden Layer to ouput?

If I don't fully link those, what are my options and how do I choose it?

Since the knowledge in those units is ""hidden"", machine representation, how should I expect to know which linkage would fit the most? 

I'm asking because, I normally see Fully linked everywhere, but Convolutional just use subsets
",7,0
77,2014-12-7,2014,12,7,6,2ohoe1,Neural Representation of Language Learning - Tom Mitchell,https://www.reddit.com/r/MachineLearning/comments/2ohoe1/neural_representation_of_language_learning_tom/,alexleavitt,1417900352,,3,21
78,2014-12-7,2014,12,7,14,2oiznd,$1M prize for digitization and identification of insect collections,https://www.reddit.com/r/MachineLearning/comments/2oiznd/1m_prize_for_digitization_and_identification_of/,aihardin,1417930060,,13,9
79,2014-12-7,2014,12,7,14,2oj1jb,"What are the benefits and disadvantages to Lasso, Ridge, Elastic Net, and Non Negative Garrotte Regularization techniques?",https://www.reddit.com/r/MachineLearning/comments/2oj1jb/what_are_the_benefits_and_disadvantages_to_lasso/,TrashQuestion,1417931398,"I am implementing these four regularization techniques for linear regression of stock data in MATLAB but i noticed elastic net is just the sum of Ridge and Lasso, and i dont full understand how exactly Non Negative Garrotte Works as a regularization technique.

How does Garrotte work and why wouldnt you just always use elastic net over lasso and ridge? (Aside from computation complexity)
",3,0
80,2014-12-7,2014,12,7,15,2oj47j,[Question]Training RBF with LMS,https://www.reddit.com/r/MachineLearning/comments/2oj47j/questiontraining_rbf_with_lms/,[deleted],1417933399,"I am training my RBF with LMS and wanted to know if I am on the right track. Are my weights updated with the product of the learning rate parameter, the error between the desired and actual response, and the output of the respective activation function? I'm unsure if I'm getting the right values. How would you plot a learning curve for this? It's MSE vs training points used from what I understand. If having trained with n number of epochs, is my MSE at that point based on the error within epoch n or based on epoch n and all the epochs beforehand?

I've been debugging my code all day and have probably confused myself in the process. Any insight will be much appreciated!",0,0
81,2014-12-7,2014,12,7,16,2ojain,Best Applied ML Course / Tutorial?,https://www.reddit.com/r/MachineLearning/comments/2ojain/best_applied_ml_course_tutorial/,benkitty,1417939012,"I'd like to learn more modern machine learning techniques in the context of an actual data set and code base. For example, there are theoretical books on convolutional neural networks, but I don't want to hunt down small/medium training data sets that I can try in under a week. I'm mostly language agnostic, but it would be nice if the language was Java-like, Python-like, or R-like.

Basically I'm looking for is a collection of ""Hello World"" programs demonstrating modern neural nets, SVMs, and other classification algos.",4,0
82,2014-12-7,2014,12,7,19,2ojjbr,Hyperparameter Optimization Routines,https://www.reddit.com/r/MachineLearning/comments/2ojjbr/hyperparameter_optimization_routines/,ml_man,1417949683,"What algorithms do other people use for hyperparameter optimisation and how long do you spend on it?

I quite like random sampling of hyperparams as it is so simple to implement and will typically get a good spread of samples.

With Gaussian Process I find it will often beat random samples; but they can be sensitive to parameters being specified in the wrong space (e.g. learning rate vs. log(learning rate)), among other difficulties.

I haven't tried TPE (http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) but it looks respectable.
 
I haven't experimented much with manual tuning.

I am currently working on a problem where my models just aren't training nicely and would be grateful to hear any advice or opinions.",7,1
83,2014-12-7,2014,12,7,21,2ojp01,An exact mapping between the Variational Renormalization Group and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2ojp01/an_exact_mapping_between_the_variational/,[deleted],1417956756,,1,3
84,2014-12-7,2014,12,7,22,2ojsrq,RNN in generative mode with Theano (Scan op),https://www.reddit.com/r/MachineLearning/comments/2ojsrq/rnn_in_generative_mode_with_theano_scan_op/,TheAlienDude,1417960724,,4,2
85,2014-12-7,2014,12,7,23,2ojtfd,Jeremy Howard - The wonderful and terrifying implications of computers that can learn,https://www.reddit.com/r/MachineLearning/comments/2ojtfd/jeremy_howard_the_wonderful_and_terrifying/,cybrbeast,1417961348,,26,38
86,2014-12-7,2014,12,7,23,2ojvmu,"Stephen Hawking right about dangers of AI... but for the wrong reasons, says eminent computer expert",https://www.reddit.com/r/MachineLearning/comments/2ojvmu/stephen_hawking_right_about_dangers_of_ai_but_for/,netkrow,1417963308,,6,0
87,2014-12-8,2014,12,8,5,2oksyb,[NoobQuestion] How to calculate the angle between an SVM solution with kernel and a vector?,https://www.reddit.com/r/MachineLearning/comments/2oksyb/noobquestion_how_to_calculate_the_angle_between/,Chobeat,1417983774,"As the title say i have my svm solution with kernel in the form of a few vector of lagrangian variables (alpha,gamma,delta) specific for the svm i'm working with, i have a vector (generated by a regressor but that's not important) and i need to calculate the angle between them.

Obviously i don't have the explicit vector of weights because i would need to know the mapping phi(x) but i don't know it due to the kernel trick. 

How should i proceed?",1,0
88,2014-12-8,2014,12,8,5,2okvcs,LightLDA: Big Topic Models on Modest Compute Clusters,https://www.reddit.com/r/MachineLearning/comments/2okvcs/lightlda_big_topic_models_on_modest_compute/,[deleted],1417985011,,0,1
89,2014-12-8,2014,12,8,5,2okwij,Implement Naive Bayes From Scratch in Python,https://www.reddit.com/r/MachineLearning/comments/2okwij/implement_naive_bayes_from_scratch_in_python/,jasonb,1417985642,,0,26
90,2014-12-8,2014,12,8,8,2olaym,"Why isn't there a ""Machine Learning"" Stack Exchange site?",https://www.reddit.com/r/MachineLearning/comments/2olaym/why_isnt_there_a_machine_learning_stack_exchange/,[deleted],1417993459,It seems like there's enough interest (even technical interest) in the field to warrant one.,5,2
91,2014-12-8,2014,12,8,11,2ols0y,Finding good parameters and interpreting the results (in python),https://www.reddit.com/r/MachineLearning/comments/2ols0y/finding_good_parameters_and_interpreting_the/,Azsu,1418004156,"I have a data set that I'm playing with, but I'm not sure if there is a pattern to be found so I'm being really hesitant with my results. As such I also don't have a good feel for tweaking my parameters so I decided to generate a product of parameters, calculate an error rate against the test data and add the abs sum of values across the training set and see what kind of parameters give me a good fit to my error rate.

So far it appears that as my [iterations](http://pastebin.com/RSx7wU7D) go up the over fitting gets really bad and the best configurations tend to land pretty close to the defaults.

Here are all the parameters that I am calculating a product for:

        l_l1_ratio = (0, 0.05, 0.1, 0.15, 0.25, 0.5, 0.75, 1)
        l_penalty = ('l1', 'l2', 'elasticnet')
        l_alpha = (0.00001, 0.0001, 0.001, 0.01, 0.1)
        l_loss = ('squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive')
        l_n_iter = (5, 50, 500, 5000)
        l_eta0 = (0.01, 0.001, 0.0001) # 0.1 crashes the fit!



I'm using [sklearn.linear_model.SGDRegressor](http://scikit-learn.sourceforge.net/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)

I also have plotted a scatter of expected values X against actual values Y then in a second graph performed a fill of expected against actual. [1752 error](http://imgur.com/7oeubki) vs [15360 error](http://imgur.com/6iC9dNz).

There are 30 attributes and ~400k samples broken up 80/20 but after rejecting bad data the useable rows end up being around ~40k.

A low score seems to be highly accurate but higher scores tend to jump around quite a bit. But my main question is about strange gap centered around 0,0 are these caused by NaN rows or are they artifacts of the SGDRegressor and am I correct in assuming that high error rates with large n_iter counts are a sign of strong over fitting?",5,1
92,2014-12-8,2014,12,8,14,2omcre,Large Scale Deep Learning  Jeff Dean,https://www.reddit.com/r/MachineLearning/comments/2omcre/large_scale_deep_learning_jeff_dean/,benkitty,1418016726,,3,12
93,2014-12-8,2014,12,8,14,2omdt8,Looking for a good book on nonparametric methods for machine learning.,https://www.reddit.com/r/MachineLearning/comments/2omdt8/looking_for_a_good_book_on_nonparametric_methods/,[deleted],1418017453,"Just wondering if anyone here had come across any good resources for this sort of stuff at an intermediate level. I couldn't see any in the subreddit FAQ, but maybe there just aren't a great deal of textbooks that are completely dedicated to the topic. Any help would be much appreciated.

e: Also the more application-centred the better, but this is not a major concern.",0,1
94,2014-12-8,2014,12,8,15,2omhql,Please suggest resources for learning about Generalized Linear models.,https://www.reddit.com/r/MachineLearning/comments/2omhql/please_suggest_resources_for_learning_about/,[deleted],1418020275,"The best youtube videos, courses, books, articles etc. Could you please suggest a few. I am overwhelmed on where to start.",0,2
95,2014-12-8,2014,12,8,17,2omqpn,Different types of Radial Drilling Machine from polykraftmachines.com,https://www.reddit.com/r/MachineLearning/comments/2omqpn/different_types_of_radial_drilling_machine_from/,polykraftmachines,1418028601,,0,1
96,2014-12-8,2014,12,8,22,2on58p,Response to Yann LeCun's Questions on the Brain,https://www.reddit.com/r/MachineLearning/comments/2on58p/response_to_yann_lecuns_questions_on_the_brain/,fergbyrne,1418043655,,1,0
97,2014-12-8,2014,12,8,22,2on6k2,MusicMood - Classify Music by Mood Based on Song Lyrics,https://www.reddit.com/r/MachineLearning/comments/2on6k2/musicmood_classify_music_by_mood_based_on_song/,[deleted],1418044708,,0,0
98,2014-12-8,2014,12,8,22,2on7rw,MusicMood. My experiences with a building a classification model based on song lyrics,https://www.reddit.com/r/MachineLearning/comments/2on7rw/musicmood_my_experiences_with_a_building_a/,[deleted],1418045700,,0,1
99,2014-12-8,2014,12,8,22,2on7y7,My experiences with training a classifier based on song lyrics and turning it into a webapp,https://www.reddit.com/r/MachineLearning/comments/2on7y7/my_experiences_with_training_a_classifier_based/,rasbt,1418045808,,11,25
100,2014-12-8,2014,12,8,23,2onc08,High Quality Object Detection at Scale,https://www.reddit.com/r/MachineLearning/comments/2onc08/high_quality_object_detection_at_scale/,[deleted],1418048834,,0,10
101,2014-12-9,2014,12,9,0,2onh8b,/r/MachineLearning hits 30K subscribers,https://www.reddit.com/r/MachineLearning/comments/2onh8b/rmachinelearning_hits_30k_subscribers/,TrendingBot,1418052239,,3,12
102,2014-12-9,2014,12,9,0,2oniop,Common Problems with Data,https://www.reddit.com/r/MachineLearning/comments/2oniop/common_problems_with_data/,rscottking73,1418053062,,1,0
103,2014-12-9,2014,12,9,1,2onnq0,Who's in Montreal right now (@ NIPS)?,https://www.reddit.com/r/MachineLearning/comments/2onnq0/whos_in_montreal_right_now_nips/,cypherx,1418055807,Which posters are you excited about? Which workshops are you going to?,4,11
104,2014-12-9,2014,12,9,3,2onzmd,Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images,https://www.reddit.com/r/MachineLearning/comments/2onzmd/deep_neural_networks_are_easily_fooled_high/,downtownslim,1418061889,,48,83
105,2014-12-9,2014,12,9,4,2oo6tq,Accepted Papers - Software Engineering for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2oo6tq/accepted_papers_software_engineering_for_machine/,xamdam,1418065362,,0,3
106,2014-12-9,2014,12,9,9,2op7uz,Who are some of the leading researchers working in the field of machine learning and working at Google or Facebook or other well known companies?,https://www.reddit.com/r/MachineLearning/comments/2op7uz/who_are_some_of_the_leading_researchers_working/,ankitsablok89,1418083308,"I am quite curious to know who are some of the machine learning experts working at some of the cream companies in US mainly like Google, Facebook, Palantir etc and startups if any.",13,3
107,2014-12-9,2014,12,9,12,2optld,Time Series Question,https://www.reddit.com/r/MachineLearning/comments/2optld/time_series_question/,DinosaurChemist,1418095151,"So I am playing with a pet project.

I have collected several time series (dates) which can take one of four different categorical values on a given day (A,B,C,D).  I would like to train a model to forecast what future what the next week's outcomes will be. Could you please suggest any appropriate approaches?  Thanks in advance.   ",3,3
108,2014-12-9,2014,12,9,14,2oq8rj,Anyone worked with Sonar Data?,https://www.reddit.com/r/MachineLearning/comments/2oq8rj/anyone_worked_with_sonar_data/,soulslicer0,1418103913,"Has anyone one here worked with sonar data? I am hoping to do some kind of object recognition on sonar data, and am curious to know some of the pre-processing techniques, and/or features extracted from the final image. References to papers would be great too",3,0
109,2014-12-9,2014,12,9,14,2oq9gu,How to get contribute to ongoing developments in AI?,https://www.reddit.com/r/MachineLearning/comments/2oq9gu/how_to_get_contribute_to_ongoing_developments_in/,[deleted],1418104384,"Hey everyone, I'm about to finish a course in AI and I wanted to know:
1.  What are some cool new open-source softwares around that I can contribute to? 2. What are some hot areas in AI now? 3. What are some skills that can get me into AI-related companies?",0,1
110,2014-12-9,2014,12,9,15,2oqdjo,Building a deeper understanding of images,https://www.reddit.com/r/MachineLearning/comments/2oqdjo/building_a_deeper_understanding_of_images/,larsga,1418107243,,0,2
111,2014-12-9,2014,12,9,16,2oqgn8,Took a course in AI. Next?,https://www.reddit.com/r/MachineLearning/comments/2oqgn8/took_a_course_in_ai_next/,n0uveau,1418109709,"Hey everyone, I'm about to finish a course in AI and I wanted to know: 1. What are some cool new open-source softwares around that I can contribute to? 2. What are some hot areas in AI now? 3. What are some skills that can get me into AI-related companies?",13,0
112,2014-12-9,2014,12,9,17,2oqmjm,NIPS '14 Deep Learning Workshop - Accepted Papers,https://www.reddit.com/r/MachineLearning/comments/2oqmjm/nips_14_deep_learning_workshop_accepted_papers/,MohamedO,1418115336,,0,24
113,2014-12-9,2014,12,9,18,2oqoit,course recommendations,https://www.reddit.com/r/MachineLearning/comments/2oqoit/course_recommendations/,nsaul,1418117289,"I'm currently figuring out which courses to take next semester, and I was hoping some internet wisdom from this relevant subreddit could help me decide.  

This will be my final semester of an undergraduate math degree at a midtear state school.  I have to decide between taking Analysis II, or a grad level Machine Learning course.  

My goals are to go to grad school studying statistics and ML and eventually work in industry.  I will probably take a pitstop in industry before going to grad school.  

The main points I've been debating with myself are:

* *Analysis is a critical course for future math study and having more will help with grad school applications.*

* *ML is my fix and I want to study it in grad school.*

* *Studying ML seriously will require more Analysis, and the sooner the better.*

* *MLwill help land an interesting job if I never make it to grad school.*

Do you have any thoughts?  

**tl;dr** more analysis courses, or machine learning courses?  ",14,3
114,2014-12-9,2014,12,9,19,2oqrnp,Jeff Hawkins on why his Approach to AI will be the approach to AI,https://www.reddit.com/r/MachineLearning/comments/2oqrnp/jeff_hawkins_on_why_his_approach_to_ai_will_be/,[deleted],1418120479,,4,0
115,2014-12-9,2014,12,9,21,2or0or,The fastest convolutions in Theano with meta-optimization,https://www.reddit.com/r/MachineLearning/comments/2or0or/the_fastest_convolutions_in_theano_with/,benanne,1418129129,,13,31
116,2014-12-9,2014,12,9,22,2or4j0,Do we know enough about the brains to build intelligent machines?,https://www.reddit.com/r/MachineLearning/comments/2or4j0/do_we_know_enough_about_the_brains_to_build/,[deleted],1418132288,"Should we prioritize learning about how brains work first, or do you think that we already know enough and the rest is just about implementing algorithms etc,",4,4
117,2014-12-10,2014,12,10,0,2orgr8,What SVM does the R package 'caret' use? / Alternative SVM suggestions wanted,https://www.reddit.com/r/MachineLearning/comments/2orgr8/what_svm_does_the_r_package_caret_use_alternative/,AlexDiru,1418139804,"I need to use the SVM (used by the caret package [1] in R) in Java. One way would be to use rJava [2] and call R from Java but I will need to do this over 100,000 times so I can imagine that will take a while. Thus if I can access the SVM it uses without the overhead of R, everything will be much quicker!

Or an alternative would be to use an SVM which has a similar parameter training method to the one used in caret [3] as that seems to work relatively well. Other SVMs seem to favour one class on the prediction. The data used is class balanced, min-max normalised, though I haven't done too much investigation into parameter searching - RBF kernel, I tend to set the cost to between 1 and 100, and gamma as 1/num_features.

Cheers for any help :)

[1] http://topepo.github.io/caret/index.html

[2] http://rforge.net/JRI/

[3] http://topepo.github.io/caret/training.html#control",6,0
118,2014-12-10,2014,12,10,1,2orn58,Learn Immersive Teaches Languages in Virtual Reality,https://www.reddit.com/r/MachineLearning/comments/2orn58/learn_immersive_teaches_languages_in_virtual/,tonydiv,1418143177,,0,1
119,2014-12-10,2014,12,10,1,2orpil,New to ML,https://www.reddit.com/r/MachineLearning/comments/2orpil/new_to_ml/,coreybenny,1418144382,"Hi Everyone, 
I'm new to machine learning but am looking to integrate it into some of my research (public health). Anyway I have already found some potentially good resources (books, online classes etc) to help me learn about it but was hoping to hear some recommendations from the community as well. Two resources that I am very interested in finding would be:
1. A machine learning cookbook style book
2. A top down approach to learning 

Thanks for your help!",7,2
120,2014-12-10,2014,12,10,2,2orr4g,Machine Learning  Hype or Reality? Microsoft ML Experts Weigh In,https://www.reddit.com/r/MachineLearning/comments/2orr4g/machine_learning_hype_or_reality_microsoft_ml/,MLBlogTeam,1418145203,,0,7
121,2014-12-10,2014,12,10,2,2oru2f,Training data size in deep learning,https://www.reddit.com/r/MachineLearning/comments/2oru2f/training_data_size_in_deep_learning/,pnambiar,1418146595,"Hello, I am trying to find out the effect of training dataset size on the success rate of overfeat classifier. I would like to deremine the effect of pose of the objects in the training on the classification results,

Any published work?",3,0
122,2014-12-10,2014,12,10,3,2orzb5,Status and/or thoughts on ELMs? (Extreme Learning Machines),https://www.reddit.com/r/MachineLearning/comments/2orzb5/status_andor_thoughts_on_elms_extreme_learning/,cafedude,1418149165,"I've been looking into various NN training options and stumbled across Extreme Learning Machines which, the creators purport, supposedly train much faster than deep networks using more traditional back prop training methods. 

I notice this paper from 2004: http://www.ntu.edu.sg/home/egbhuang/pdf/ELM_IJCNN2004.PDF
 
And an article in IEEE IntelligentSystems from 2013:
http://www.ntu.edu.sg/home/egbhuang/pdf/IEEE-IS-ELM.pdf

Does anyone have any experience with ELMs? Impressions, opinions?",5,1
123,2014-12-10,2014,12,10,5,2osf7q,Is there a good free paper available which compares the performance of classifiers given a relatively small sample size (give the FV dimensionality)?,https://www.reddit.com/r/MachineLearning/comments/2osf7q/is_there_a_good_free_paper_available_which/,duckandcover,1418156746,"I gather large margin classifiers are the way to go which would suggest SVM or perhaps some tree boosting methods which, as I understand it, are also large margin.  I thought I saw a helpful paper but it's not free....damn it.",1,1
124,2014-12-10,2014,12,10,5,2osimj,Tips To Get The Most From The Naive Bayes,https://www.reddit.com/r/MachineLearning/comments/2osimj/tips_to_get_the_most_from_the_naive_bayes/,jasonb,1418158442,,0,3
125,2014-12-10,2014,12,10,7,2oss6l,Need benchmark data sets to test my Evolutionary RNN.,https://www.reddit.com/r/MachineLearning/comments/2oss6l/need_benchmark_data_sets_to_test_my_evolutionary/,y05f,1418162892,"I am working on Evolutionary recurrent neural networks and i want to test and compare the performance of my method. For this, i need recent benchmark data sets which are used by the most researchers. What are your suggestions?",5,1
126,2014-12-10,2014,12,10,8,2ot5ke,"[Question] Metrics to compare between two cities, using their demographic data ?",https://www.reddit.com/r/MachineLearning/comments/2ot5ke/question_metrics_to_compare_between_two_cities/,[deleted],1418169552,"I made a histogram-based visualization to compare between two states (collected data from different sources like: census .. etc).

What is the best way to compare between two states, with metrics, using demographic data?. Is there any recommended metrics that I need to use.

Is there any similar project/paper/website/book that I need to check ? ",1,0
127,2014-12-10,2014,12,10,9,2ot9c4,Automatically labeling facts in a never-ending language learning system,https://www.reddit.com/r/MachineLearning/comments/2ot9c4/automatically_labeling_facts_in_a_neverending/,wordsnerd,1418171534,,0,16
128,2014-12-10,2014,12,10,18,2oum56,Real-Time Predictive Analytics: Are Companies Ready to Take Full Advantage?,https://www.reddit.com/r/MachineLearning/comments/2oum56/realtime_predictive_analytics_are_companies_ready/,Sergiointelnics,1418202908,,0,0
129,2014-12-11,2014,12,11,1,2ovgea,Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? [pdf],https://www.reddit.com/r/MachineLearning/comments/2ovgea/do_we_need_hundreds_of_classifiers_to_solve_real/,improbabble,1418227487,,12,31
130,2014-12-11,2014,12,11,1,2ovgvp,L_1 regularization in Machine Learning: Deep Convolutional Networks and Kernel Machines (x-post r/CompressiveSensing),https://www.reddit.com/r/MachineLearning/comments/2ovgvp/l_1_regularization_in_machine_learning_deep/,compsens,1418227750,,0,2
131,2014-12-11,2014,12,11,1,2ovk24,[Question] Formula for number of parameters in an undirected graphical model,https://www.reddit.com/r/MachineLearning/comments/2ovk24/question_formula_for_number_of_parameters_in_an/,jmite,1418229392,"Disclaimer: crossposted [here](http://cs.stackexchange.com/questions/34109/formula-for-number-of-parameters-in-an-undirected-graphical-probability-model).

I have googled endlessly, and I cannot find it. Can anyone point me to a reference that gives a way to calculate the number of parameters in an undirected Graphical Model?

Adapting from the similar formula for Bayes nets, this is my guess: if we have variables $1 \ldots k$ where variable $i$ has $d_i$ possible values, and $n(i)$ is the set of vertices adjacent to $i$ in the independence graph, then the number of parameters would be

$\Sigma_{i=1}^{k}(d_i - 1) \Pi_{j \in n(i)}d_j$.

Can anyone verify this, and if it's wrong, point me to a correct solution?

I'm trying to calculate the AIC and BIC scores for graphical models, but to do that, I need the number of parameters each has.",0,0
132,2014-12-11,2014,12,11,3,2ovxnu,A Crowd Of Scientists Finds A Better Way To Predict Seizures,https://www.reddit.com/r/MachineLearning/comments/2ovxnu/a_crowd_of_scientists_finds_a_better_way_to/,cavedave,1418236165,,9,15
133,2014-12-11,2014,12,11,4,2ow60h,What kind of computer setup do I need in order to be able to train deep neural network on imagenet database in reasonable time (say couple of hours)?,https://www.reddit.com/r/MachineLearning/comments/2ow60h/what_kind_of_computer_setup_do_i_need_in_order_to/,SunnyJapan,1418240074,,15,3
134,2014-12-11,2014,12,11,6,2owk09,Jeremy Howard (Recent TEDxBrussels on ML) AMA @ /r/futurology - 1PM EST Dec 13th,https://www.reddit.com/r/MachineLearning/comments/2owk09/jeremy_howard_recent_tedxbrussels_on_ml_ama/,[deleted],1418246625,"Jeremy Howard discussed ML and its implications at the recent TEDxBrussels and will be hosting an AMA over at /r/futurology this coming Saturday at 1PM EST.

Feel free to join us!",0,0
135,2014-12-11,2014,12,11,6,2owk8k,DARPA Offers Free Watson-Like Artificial Intelligence,https://www.reddit.com/r/MachineLearning/comments/2owk8k/darpa_offers_free_watsonlike_artificial/,mrprint,1418246732,,0,2
136,2014-12-11,2014,12,11,7,2owpri,Request: Clever Things to do with Naive Bayes,https://www.reddit.com/r/MachineLearning/comments/2owpri/request_clever_things_to_do_with_naive_bayes/,[deleted],1418249284,,0,1
137,2014-12-11,2014,12,11,7,2owrdf,Request: Clever Things to do with Naive Bayes,https://www.reddit.com/r/MachineLearning/comments/2owrdf/request_clever_things_to_do_with_naive_bayes/,PlexiglassPelican,1418250034,,1,0
138,2014-12-11,2014,12,11,8,2owy6j,Kaggle introduces a new deep learning tutorial for sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/2owy6j/kaggle_introduces_a_new_deep_learning_tutorial/,notarowboat,1418253404,,15,65
139,2014-12-11,2014,12,11,10,2oxeor,Good news from the Hawkman,https://www.reddit.com/r/MachineLearning/comments/2oxeor/good_news_from_the_hawkman/,[deleted],1418262191,,0,1
140,2014-12-11,2014,12,11,10,2oxfhb,[Question] How can I combine multiple neural networks into one?,https://www.reddit.com/r/MachineLearning/comments/2oxfhb/question_how_can_i_combine_multiple_neural/,maemre,1418262642,"Hi!

I'm implementing Cooperative Q-Learning with multiple agents [as described by Ahmadabadi](http://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;citation_for_view=QlwWxmoAAAAJ:u5HHmVD_uO8C). However, there is a communication overhead (agents communicate over a network and trying to be energy efficient, so the data sent over network is costly) and I'm planning to implement Q function using a neural network instead of a matrix to reduce communication overhead.

But, I could not find a method to combine the neural networks of several agents into one to send them back to agents. Is there a method that does that efficiently? Do you have any ideas?

Or, alternatively, do you have any ideas to minimize the overhead?

In the worst case, I'm planning to reconstruct Q function as a matrix and train a neural network with all of the the extracted Q functions.",3,4
141,2014-12-11,2014,12,11,17,2oyga8,word2vec is an implicit shifted PMI matrix factorization,https://www.reddit.com/r/MachineLearning/comments/2oyga8/word2vec_is_an_implicit_shifted_pmi_matrix/,fasterthanlite,1418286839,,1,11
142,2014-12-11,2014,12,11,21,2oysym,How a Math Genius Hacked OkCupid to Find True Love | WIRED,https://www.reddit.com/r/MachineLearning/comments/2oysym/how_a_math_genius_hacked_okcupid_to_find_true/,ArchieIndian,1418300046,,0,0
143,2014-12-11,2014,12,11,21,2oyug8,The Current State of Machine Intellignece,https://www.reddit.com/r/MachineLearning/comments/2oyug8/the_current_state_of_machine_intellignece/,digitron,1418301477,,44,94
144,2014-12-11,2014,12,11,22,2oyymj,Is Deep Learning a Revolution in Artificial Intelligence?,https://www.reddit.com/r/MachineLearning/comments/2oyymj/is_deep_learning_a_revolution_in_artificial/,[deleted],1418304925,,0,1
145,2014-12-12,2014,12,12,1,2ozd7p,Label Propagation experience,https://www.reddit.com/r/MachineLearning/comments/2ozd7p/label_propagation_experience/,improbabble,1418314125,"Does anyone have experience/tips they'd be willing to share on label propagation?

In particular I'm interested if you've used it successfully for regression problems particularly over large-ish graphs. I have 90MM nodes and 1.2B edges so I'm also curious if anyone has pointers for data of that size and larger.

I've looked at these packages:
* https://github.com/parthatalukdar/junto
* https://github.com/smly/label-propagation
* Sklearn (http://scikit-learn.org/stable/modules/label_propagation.html)
* Graphchi (http://bickson.blogspot.com/2013/02/label-propagation-in-graphchi.html)

",1,1
146,2014-12-12,2014,12,12,2,2ozly7,Microsoft working with research community to advance sign language recognition using Xbox Kinect,https://www.reddit.com/r/MachineLearning/comments/2ozly7/microsoft_working_with_research_community_to/,MLBlogTeam,1418318655,,0,2
147,2014-12-12,2014,12,12,4,2ozxw1,The long foggy race to General-Purpose Superintelligence-An explosion will take place in AI investments. Deep learning is the first step.,https://www.reddit.com/r/MachineLearning/comments/2ozxw1/the_long_foggy_race_to_generalpurpose/,apmTech,1418324571,"short article on the future of AI, deep learning and funding- do let me know your thoughts =)

http://www.planettechnews.com/reviews/climbing-the-mountain-the-long-foggy-race-to-general-purpose-superintelligence

www.planettechnews.com for your daily dose of innovation news",0,0
148,2014-12-12,2014,12,12,4,2p00t3,Is Master in Applied Statistics a good program to get myself into Machine Learning field? If this is the wrong subreddit sorry!,https://www.reddit.com/r/MachineLearning/comments/2p00t3/is_master_in_applied_statistics_a_good_program_to/,urmyheartBeatStopR,1418325951,,2,1
149,2014-12-12,2014,12,12,4,2p04rq,Greedy BFS multi-variate optimization?,https://www.reddit.com/r/MachineLearning/comments/2p04rq/greedy_bfs_multivariate_optimization/,[deleted],1418327829,"I'm trying to come up with a fast way to optimize a function which takes a large number of variables, each of which can take a finite set of values. 

I have an idea which seems like, if polished, would give a good greedy solution, but I know something like this is sure to have been done already or there's a better way to do it; so what I'm looking for is some suggestion on papers to look at to accomplish what I'm trying to do. 

The algorithm I have in mind goes as follows:

&amp;nbsp;

The objective is to maximize function f(a,b,c), where a,b,c can take on values in some finite sets A,B,C. 

Base solution = f(a0,b0,c0)

Depth one branches are f(a1,b0,c0), f(a0,b1,c0), f(a0,b0,c1)

Depth two branches are f(a2,b0,c0), f(a0,b2,c0), f(a0,b0,c2)

Depth n branches are f(an, b0, c0), f(a0, bn, c0), f(a0, b0, cn)  
&amp;nbsp;

So you have a tree like:

                       f(a0,b0,c0)
            /                |                \
    f(a1,b0,c0)         f(a0,b1,c0)        f(a0,b0,c1)    
          |                  |                  |
    f(a2,b0,c0)         f(a0,b2,c0)       f(a0,b0,c2)
          |                  |                  |
         ...                ...                ...

If the first depth branches has a better solution than the base, that branch becomes the new base; next round.

Otherwise, move to the next depth and search the branches.

Each depth's branches are just the previous root's parameter index + 1. So for example if f(a1,b0,c0) is a better solution, it becomes the base and the next tree is


                       f(a1,b0,c0)
            /                |                \
    f(a2,b0,c0)         f(a1,b1,c0)        f(a1,b0,c1)    
          |                  |                  |
    f(a3,b0,c0)         f(a1,b2,c0)       f(a1,b0,c2)
          |                  |                  |
         ...                ...                ...


The algorithm ends when reaching a leaf node, at which point the root is the returned solution.",1,1
150,2014-12-12,2014,12,12,11,2p1hc6,Methods for training convolutional neural networks on varying size test images?,https://www.reddit.com/r/MachineLearning/comments/2p1hc6/methods_for_training_convolutional_neural/,Vystril,1418352793,It seems like all the literature I come across has uniform sizes for the training images.  Are there any techniques for training a CNN on varying size input images?,11,3
151,2014-12-12,2014,12,12,16,2p25gt,Can anyone suggest a Java ML lib that supports Logistic Regression + L2 reg?,https://www.reddit.com/r/MachineLearning/comments/2p25gt/can_anyone_suggest_a_java_ml_lib_that_supports/,isep,1418368634,"So far I've been playing with [LAML](https://sites.google.com/site/qianmingjie/home/toolkits/laml) which has been really nice as far as efficiency goes but doesn't seem to support any sort of regularization.

Worst case I can just export my data and use a different language/lib in my pipeline but I figured I should see what's out there.

Cheers!",11,1
152,2014-12-13,2014,12,13,2,2p3ebm,"ML in Bing brings the worlds knowledge into your Office docs - take ""Insights for Office"" for a free spin",https://www.reddit.com/r/MachineLearning/comments/2p3ebm/ml_in_bing_brings_the_worlds_knowledge_into_your/,MLBlogTeam,1418404910,,8,0
153,2014-12-13,2014,12,13,7,2p4cuq,[github] Logistic Matrix Factorization for Implicit Feedback Data,https://www.reddit.com/r/MachineLearning/comments/2p4cuq/github_logistic_matrix_factorization_for_implicit/,improbabble,1418423627,,7,14
154,2014-12-13,2014,12,13,9,2p4m7m,Creating a universal standard/schema for text resources rather than using natural launguage processing.,https://www.reddit.com/r/MachineLearning/comments/2p4m7m/creating_a_universal_standardschema_for_text/,mrapogee,1418428981,,2,1
155,2014-12-13,2014,12,13,10,2p4u9a,"Data Elixir, Issue 13 - machine learning, data wrangling, D3, gift list for data geeks",https://www.reddit.com/r/MachineLearning/comments/2p4u9a/data_elixir_issue_13_machine_learning_data/,lonriesberg,1418434045,,0,1
156,2014-12-13,2014,12,13,14,2p5dko,Data Scientists Alessandro Gagliardi and Amit Kapoor talk about their learnings in the field.,https://www.reddit.com/r/MachineLearning/comments/2p5dko/data_scientists_alessandro_gagliardi_and_amit/,barmalade,1418447275,,0,0
157,2014-12-13,2014,12,13,20,2p60ty,Machine Learning Best of 2014 Nomination thread,https://www.reddit.com/r/MachineLearning/comments/2p60ty/machine_learning_best_of_2014_nomination_thread/,cavedave,1418471417,"I am going to start a comment threads on each nomination category. The categories at the moment will be

Most influential Variable (Best commenter/contributor to the community)

Best discussion

Highest AUC (Best submission)


To nominate something, find the top comment of mine that is the category, and reply to it with a link and a brief description of why you think it is Best Of material. If you want another category PM the mods with a suggestion.
Take a moment to think back and see what sticks out in your own memory as good AMA's, threads and submissions",22,61
158,2014-12-13,2014,12,13,22,2p6518,"ML Problem: ""Will this compile?""",https://www.reddit.com/r/MachineLearning/comments/2p6518/ml_problem_will_this_compile/,noel___,1418476530,"*Given ample training data of compiling and non-compiling strings of source code in a single language (consider the language to be fully unknown, albeit consistent), construct a self configuring solution for classifying new strings of source code from that same language as either compiling or non-compiling.*

The solution to this problem could theoretically comprehend (or tractably index) the foundational structure of language (programming, spoken, written, mathematical, scientific, genetic, viral, chemical, or otherwise).

**If anyone else is actively working in this problem domain, I'm looking for you.**
",10,4
159,2014-12-14,2014,12,14,1,2p6l99,NIPS2014 Poster papers (x-post r/CompressiveSesning ),https://www.reddit.com/r/MachineLearning/comments/2p6l99/nips2014_poster_papers_xpost_rcompressivesesning/,compsens,1418489713,,0,2
160,2014-12-14,2014,12,14,2,2p6oc5,Can anyone give some more insight into this quote?,https://www.reddit.com/r/MachineLearning/comments/2p6oc5/can_anyone_give_some_more_insight_into_this_quote/,nytalmx,1418491654,"http://youtu.be/LZnAFO5gkOQ?t=37m12s

Specifically the the stuff he says about it being well understood that there are high dimensional functions which cannot be approximated, and about neural nets approximating a ""class"" of function and that we know nothing about the functional spaces they can approximate.

And then the panelist to his right says that we do know, but he gets cut off.

Can anyone can give some more insight about these questions?",3,1
161,2014-12-14,2014,12,14,6,2p7fgr,"Link to Futurology -&gt;AMA I'm Jeremy Howard, Enlitic CEO, Kaggle Past President, Singularity U Faculty. Ask me anything about machine learning, future of medicine, technological unemployment, startups, VC, or programming",https://www.reddit.com/r/MachineLearning/comments/2p7fgr/link_to_futurology_ama_im_jeremy_howard_enlitic/,hajshin,1418507609,,3,6
162,2014-12-14,2014,12,14,12,2p89gb,Is it possible to automatically determine the value of parameters for this type of image processing?,https://www.reddit.com/r/MachineLearning/comments/2p89gb/is_it_possible_to_automatically_determine_the/,cvnovice,1418526431,"I've been working on this project for a while and am still struggling to  get my automatic detection algorithms to work every time.

Given  a set of images like [this](http://imgur.com/4xBwedk) I need to extract the coloured region inside of the border of each square. 

It seemed relatively simple at first but I can't get my code to work on every image. 

The problem is the images may have been taken under different lighting conditions or slightly rotated, etc, so some times some of the squares are missed. 

I currently apply a Gaussian blur to the grey-scale image, then use otzu threshold to biniarize the image for contour detection. The contours are then sorted by size, shape ,and centroid to get the positions of the regions of interest which is used to extract the colour information.

The problem is sometimes contour detection fails to get the outline for each square. I can play with the parameters of the blur and threshold step to get it to work, but I would like it to automatically determine optimal parameters to get a 10x10 grid of ""perfect"" square contours.

Is there a machine learning or computer vision method to automatically determine the proper parameters for this type of image processing?  ",5,0
163,2014-12-14,2014,12,14,13,2p8epv,Is machine learning closely related to artificial intelligence?,https://www.reddit.com/r/MachineLearning/comments/2p8epv/is_machine_learning_closely_related_to_artificial/,[deleted],1418529986,"Both of these fields of study interest me greatly, but I'm honestly not sure if there is much of a difference between the two. Would machine learning also be 'artificial intelligence'?

Or are they different because in the case of machine learning, we can technically have unsupervised learning in which intelligence is gained without any support. Whereas in artificial intelligence, I suppose we would support our programs with a variety of rules and decision making abilities which were defined by us.

What's the difference?",6,0
164,2014-12-14,2014,12,14,17,2p8wv2,True art is samples from function spaces which can only be reached via a generative process which involves intelligence.,https://www.reddit.com/r/MachineLearning/comments/2p8wv2/true_art_is_samples_from_function_spaces_which/,nytalmx,1418545132,"What is art? Maybe machine learning can provide an answer.  Sorry to veer from hardcore machine learning, this is more of a philosophical question.  I think that the statement in the title is true.  For example, a non-intelligent program couldn't create a cartoon that satrizes something, that would require intelligence.  Cartoons are a type of art.  It applies to other art as well, I just chose cartoons because it seemed easier to explain by starting from there because some paintings do appear that they could be created by non-intelligent generative processes, (jackson pollack) which brings the question of whether that is true art.

It could be that ""function spaces"" of things/images/audio that can only be generated by human level intelligence, may be of interest to our brain and make us feel awe or curiousity precisely because they are from a different class of functions which look completely different from the patterns we see in nature. So since our visual system and auditory system is so attuened to compressing and processing the information in natural systems, when we see something which breaks many rules of structure that we are used to, it is very exciting for our perception.

edit: also, I dont want to get into an argument on whether science and math can be art as well.  I personally think that science and math is more art than art, however, if you asked a wide survey, I think most people would define art as needing to be created by humans.  On the other hand, now that I think about it, science and math WERE created by humans, so maybe they are in fact art even by my prior defintion.",6,0
165,2014-12-14,2014,12,14,17,2p8yhb,Who went to NIPS 2014 and what were the best sessions you attended?,https://www.reddit.com/r/MachineLearning/comments/2p8yhb/who_went_to_nips_2014_and_what_were_the_best/,evc123,1418546900,What were the best sessions you attended at NIPS 2014?,15,26
166,2014-12-14,2014,12,14,18,2p8z3e,Applying machine learning for identifying driver behavior,https://www.reddit.com/r/MachineLearning/comments/2p8z3e/applying_machine_learning_for_identifying_driver/,umeedu,1418547641,"I am a grad student in transportation engineering in Canada. I wrote some code to identify different stages of driving (uninfluenced driving, approaching, following, braking) as described in Wiedemann car following model. Because driving behavior varies for the same driver, among different vehicle types and driving styles, my code was a crude approximation of reality. 
I want to repeat this exercise using machine learning (ML)/ data science in R but have no background in ML so far. 
What do you think I should start learning first in ML in context of the research problem?",8,4
167,2014-12-14,2014,12,14,19,2p93s9,"Nice visualization of stochastic optimizers by Alec Radford: (SGD, momentum, Nesterov, AdaGrad, AdaDelta, RMSProp)",https://www.reddit.com/r/MachineLearning/comments/2p93s9/nice_visualization_of_stochastic_optimizers_by/,exellentpossum,1418553676,,24,228
168,2014-12-15,2014,12,15,0,2p9lz0,Suggestions for a beginner..,https://www.reddit.com/r/MachineLearning/comments/2p9lz0/suggestions_for_a_beginner/,pddpro,1418572294,"Hello r/machinelearning !

I am a beginner in the wonderful field of Machine Learning. I took Prof. Andrew Ng's class in coursera a year ago,
so I know somewhat about regressions, optimization algorithms, basic neural nets, SVMs, and the likes.

However, I have not yet used any of those knowledge in any practical areas. With the new era of Deep Learning and the 
exciting prospects it entails, I am extremely eager to dive my hands into it, only I feel that I am not ready. My knowledge
of ML is a limited one that I gathered from Prof Andrew's course and my mathematics isn't at the level where I could be called an expert.

Thus, I'd like to inquire of good resources which could fill the gaps in my knowledge of ML and prepare me for a dive into the world of Deep Learning.
I'd really appreciate if you guys could point me in the right direction.

Regards.",6,0
169,2014-12-15,2014,12,15,0,2p9mn0,"How to get into a Ph.D program in Machine Learning at institutions like MIT, Stanford, Caltech, UCB etc..",https://www.reddit.com/r/MachineLearning/comments/2p9mn0/how_to_get_into_a_phd_program_in_machine_learning/,ankitsablok89,1418572752,,4,0
170,2014-12-15,2014,12,15,3,2pa3nl,"HR article, why your data scientist sucks",https://www.reddit.com/r/MachineLearning/comments/2pa3nl/hr_article_why_your_data_scientist_sucks/,[deleted],1418583130,,3,0
171,2014-12-15,2014,12,15,5,2pag0q,Great discussion about initializing neural networks on G+,https://www.reddit.com/r/MachineLearning/comments/2pag0q/great_discussion_about_initializing_neural/,benanne,1418589752,,7,15
172,2014-12-15,2014,12,15,7,2paowz,"Identification of ""long running"" patterns",https://www.reddit.com/r/MachineLearning/comments/2paowz/identification_of_long_running_patterns/,piscoster,1418594452,"Hello guys,

I have changed my job into the security niche and I am currently researching the question about ""long running attacks"". Currently, I haven`t found much. However, I believe it is possible to use machine learning to extract such long running patterns on log files from different sources.

* Is there any research done on ""long term anomaly detection""? Any ideas how to realize such a method? 
* Are there still some open problem in this niche?

I really appreciate your answer!
",1,0
173,2014-12-15,2014,12,15,8,2pavv8,How should I deal with categorical feature vector with feature groups that have growing number of categories,https://www.reddit.com/r/MachineLearning/comments/2pavv8/how_should_i_deal_with_categorical_feature_vector/,[deleted],1418598213,"So, what the title says: How should I deal with categorical feature vector with feature groups that have growing number of categories

Background:
As an exercise, I would like to do a binary classification on URLs. To figure out if they are spam or benign.  
(Some people say this may not be the best way to detect spam... but thats another issue. This is an exercise more than anything)


So, given a URIs like this:

Ftp://foobarhost:123/some/path

Http://1234Host.com/some/other/path?some=query&amp;string=true


One of the feature groups will be the  Scheme (Protocol)  i.e  FTP | HTTP | HTTPS  etc..


I am representing this as a binary categorical feature vector where the value is either 0 or 1:

&lt;FTP, HTTP, HTTPS&gt;


then we have other feature vectors based on the other parts of the URI  (host, path, query etc)


Note: The reason we keep separate feature vectors for all of the feature groups is that for example the word Foo means completely different things depending on where in the URI we see it

.
.
.
When we have fully built all of these feature group vectors, we would like to combine all of them to become our input. This works well


BUT, what can we do if for example we come across a new Scheme/Protocol  FooBar:// and grow the  Scheme feature group?  If we grow that feature vector, then when we combine  the index where the next feature group started would mean something completely different.


Example: Before growing this is our categorical input vector:

 [Scheme][Host bag of words   ]

&lt;Http, Ftp, foo, bar, moo, woof,....&gt;


After growing our Scheme feature group this is what it will look like:

 [       Scheme      ][Host bag of words   ]

&lt;Http, Ftp,**HTTPS** foo, bar, moo, woof,....&gt;

So everything after index 2 means something completely different than it used to before growing the feature vector



You might say, just add it to the end of the end of the combined feature vector... but how do you keep track of that particular feature being at that index?


Sorry for the wall of text.. trying to explain the issue the best I can
",1,1
174,2014-12-15,2014,12,15,8,2pb1a7,Train time layer resizing,https://www.reddit.com/r/MachineLearning/comments/2pb1a7/train_time_layer_resizing/,londons_explorer,1418601151,"When designing a neural net, depth and sizes of each layer matter a lot.   All papers I've seen so far seem to manually choose sizes.

I'm considering a system which (imagine all layers are fully connected for now) can dynamically add and remove nodes and layers at training time to try to put parameters only where they are valuable.

Every epoch, the ""least useful"" 1% of nodes in a fully connected neural net are removed, and a new 1% of nodes are added randomly and randomly initialised.

""Least useful"" nodes are determined by looking at d(Node output value)/d(Loss Function) averaged across a large number of training examples.

Adding layers can be done by inserting a new ""unity"" layer between two existing layers, which has all parameters set to just pass through data unmodified (and no nonlinear elements), but it can then have new non-unity nodes added through the above mechanism.

One can imagine there are (sometimes non-trivial) extensions to this idea to apply to other layer types too.

Has this been done before, and is there any literature on the topic?",4,4
175,2014-12-15,2014,12,15,9,2pb6wm,A Common Logic to Seeing Cats and Cosmos | Quanta Magazine,https://www.reddit.com/r/MachineLearning/comments/2pb6wm/a_common_logic_to_seeing_cats_and_cosmos_quanta/,masharpe,1418604216,,0,1
176,2014-12-15,2014,12,15,11,2pbfyf,My experience with using a linear model for Tic Tac Toe learning,https://www.reddit.com/r/MachineLearning/comments/2pbfyf/my_experience_with_using_a_linear_model_for_tic/,[deleted],1418609332,"I have put up my results here:
http://skothawala.com/2014/12/15/my-experience-with-using-a-linear-model-for-tic-tac-toe-learning/

The results, in a nutshell, were that given a set of features, the learned linear model failed to show any significant improvement over one using random weights. It did, however, show a significant improvement over a player making moves at random. The credit, I conclude, is due to the algorithm used by the learning player to make a move.

I don't have much formal training in ML and am trying to learn on my own. I would very much like to learn about any bugs or flaws in my approach, and any suggestions from more experienced practitioners will be much appreciated. 

Thanks for checking it out. ",0,0
177,2014-12-15,2014,12,15,12,2pbpe9,How do I get started with Abstractive Text Summarization?,https://www.reddit.com/r/MachineLearning/comments/2pbpe9/how_do_i_get_started_with_abstractive_text/,fiveblinks,1418614696,"Hello, I've been working on automatic extractive text summarization where the program picks the most important sentences of a text and forms its summary. 
What I would like to do now is more challenging. I would like to generate summaries in natural language, and not just pick important sentences. The computer should generate abstracts that are grammatically correct and content wise coherent. 
How do I go about this problem? What machine learning algorithms would help? What kind of features am I supposed to identify?
Sorry for the bad formatting, I'm on my phone. 
Thanks ",4,7
178,2014-12-15,2014,12,15,18,2pcfzc,Where do you think machine learning/AI will be in 5 years?,https://www.reddit.com/r/MachineLearning/comments/2pcfzc/where_do_you_think_machine_learningai_will_be_in/,[deleted],1418634742,,0,1
179,2014-12-15,2014,12,15,19,2pck7y,Autotuning PID controller?,https://www.reddit.com/r/MachineLearning/comments/2pck7y/autotuning_pid_controller/,joeflux,1418639177,"I've been looking for weeks, and can't find out how to autotune a PID algorithm in c++.  All I find is abstract mathematical equations that I struggle to interpret.

It's for a quad-copter.  I want to autotune the PID parameters for a quadcopter.",12,15
180,2014-12-15,2014,12,15,21,2pcr52,ML algorithm parameters in Python,https://www.reddit.com/r/MachineLearning/comments/2pcr52/ml_algorithm_parameters_in_python/,InfinityCoffee,1418646329,"I was wondering what the preferred way to handle (often large-dimensional) parameter sets in Python is, as you'd find in neural networks and Bayesian variational methods (particularly the latter). I am especially wondering how you make your code interface well with gradient methods.

Numpy functionality seems a must, but storing parameters in separate named arrays has two weaknesses as I see it.

* Operating on the entire parameter set, e.g. adding a gradient, is impossible, so you have to write the operation for each array of parameters explicitly.
* If you have variations of the model with only a subset of the parameters inheritance becomes difficult to work with.

Storing everything in a single array/vector seems like a terrible solution as the code would become unintelligible. So what is a better way to do it? A dictionary so you can loop over the parameters? A Pandas Panel frame? A parameter class with its own update method?

Would there be any significant overhead on using a dictionary of a few large arrays and then looping over the keys, as opposed to writing each operation explicitly? ",5,2
181,2014-12-15,2014,12,15,22,2pcvs6,I need someone to run a few topic modeling algorithms for me,https://www.reddit.com/r/MachineLearning/comments/2pcvs6/i_need_someone_to_run_a_few_topic_modeling/,acrossthepond925,1418650383,"Relatively new to machine learning, but decently well read on the particular applications I'm interested in for my research (topic modeling applications mostly).

I'm learning slowly on the technical side (languages) but really need to start making just a little basic headway. Does anyone have any suggestions of where I might go to hire someone to do some basic work topic modeling a couple of corpora for me? ",2,0
182,2014-12-15,2014,12,15,23,2pd0rv,The Four V's of Big Data,https://www.reddit.com/r/MachineLearning/comments/2pd0rv/the_four_vs_of_big_data/,Quantizedz,1418654081,,0,0
183,2014-12-15,2014,12,15,23,2pd2zk,Questions about Q-Learning using Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2pd2zk/questions_about_qlearning_using_neural_networks/,yerlikayahamza,1418655452,"I have implemented Q-Learning as described in,

http://web.cs.swarthmore.edu/~meeden/cs81/s12/papers/MarkStevePaper.pdf

In order to approx. Q(S,A) I use a neural network structure like the following,

 - Activation sigmoid
 - Inputs, number of inputs + 1 extra for the Action (All Inputs Scaled 0-1)
 - Outputs, single output. Q-Value
 - N number of M Hidden Layers.
 - Exploration method random 0 &lt; rand() &lt; propExplore

At each learning iteration I run all action/state pairs through the neural network either pick one at random or choose the one with the highest Q-Value then using the following formula,

http://i.stack.imgur.com/e3hgc.png

I calculate a Q-Target value then calculate an error using,

    error = QTarget - LastQValueReturnedFromNN

and the neural network using this error.

Q1, Am I on the right track? I have seen some papers that implement a NN with one output neuron for each action.

Q2, My reward function returns a number between -1 and 1. Is it ok to return a number between -1 and 1 when the activation function is sigmoid (0 1)

Q3, From my understanding of this method given enough training instances it should be quarantined to find an optimal policy wight? When training for XOR sometimes it learns it after 2k iterations sometimes it won't learn even after 40k 50k iterations. Is this because random is random or am I missing something. (I've tried using boltzmann exploration instead of pure random but still iteration count needed for xor fluctuates.)",6,2
184,2014-12-16,2014,12,16,0,2pd8t9,Using a linear model for tic tac toe learning,https://www.reddit.com/r/MachineLearning/comments/2pd8t9/using_a_linear_model_for_tic_tac_toe_learning/,[deleted],1418658942,,0,0
185,2014-12-16,2014,12,16,1,2pde8q,Why Neural Networks Look Set To Thrash The Best Human Go Players For The First Time | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/2pde8q/why_neural_networks_look_set_to_thrash_the_best/,ezetter,1418661887,,41,64
186,2014-12-16,2014,12,16,2,2pdl43,A Parallel and Efficient Algorithm for Learning to Match [github],https://www.reddit.com/r/MachineLearning/comments/2pdl43/a_parallel_and_efficient_algorithm_for_learning/,improbabble,1418665373,,1,2
187,2014-12-16,2014,12,16,4,2pdz9n,Discovering Structure in High-Dimensional Data Through Correlation Explanation,https://www.reddit.com/r/MachineLearning/comments/2pdz9n/discovering_structure_in_highdimensional_data/,Foxtr0t,1418672330,"The paper:
http://papers.nips.cc/paper/5580-discovering-structure-in-high-dimensional-data-through-correlation-explanation

Code on GitHub:
https://github.com/gregversteeg/CorEx

Some demos:
http://www.isi.edu/people/gregv/preliminary_corex_visualizations",1,16
188,2014-12-16,2014,12,16,4,2pe154,The NIPS Experiment: Half the papers appearing at NIPS would be rejected if the review process were rerun,https://www.reddit.com/r/MachineLearning/comments/2pe154/the_nips_experiment_half_the_papers_appearing_at/,urish,1418673184,,20,75
189,2014-12-16,2014,12,16,5,2pe7f2,Machine Learning introduction question,https://www.reddit.com/r/MachineLearning/comments/2pe7f2/machine_learning_introduction_question/,[deleted],1418676149,I've only just started learning Python but it's going to be some time before I can really produce anything with it.  I was wondering if anyone has a downloadable program already that maybe I could feed it some images or something see something like Deeplearning in action on our own systems?  I'm just super excited about eventually learning how to teach algorithms like this.  So something I can use to spark ideas would be really cool! ,0,1
190,2014-12-16,2014,12,16,6,2pedw5,Any expertise in adaptive resonance theory (ART)?,https://www.reddit.com/r/MachineLearning/comments/2pedw5/any_expertise_in_adaptive_resonance_theory_art/,ownallogist,1418679261,"Hi all,

Beginning to work on a project with a professor regarding the Adaptive Resonance Theory, particularly ART 3. My role is to develop a model using python as the main language.

Aside from wikipedia and a few research papers here and there, I am having difficulty:

1) Completely understanding the concept
2) Finding anything code related

Would anyone happen to have some expertise in this area? 


Thanks in advance!
",0,0
191,2014-12-16,2014,12,16,7,2penzj,Text Analytics Sandbox - Like JSfiddlefor Text Analytics,https://www.reddit.com/r/MachineLearning/comments/2penzj/text_analytics_sandbox_like_jsfiddlefor_text/,[deleted],1418684089,,0,1
192,2014-12-16,2014,12,16,8,2peovb,AYLIEN Text Analytics Sandbox - Like JSFiddle for Text Analytics,https://www.reddit.com/r/MachineLearning/comments/2peovb/aylien_text_analytics_sandbox_like_jsfiddle_for/,MikeWally,1418684477,,0,0
193,2014-12-16,2014,12,16,8,2pevfe,Profile: Mohammad Sabah - Director of Data Science at Workday,https://www.reddit.com/r/MachineLearning/comments/2pevfe/profile_mohammad_sabah_director_of_data_science/,jubalince,1418687778,,1,0
194,2014-12-16,2014,12,16,11,2pffna,Where can I find implementation/tutorial of Knowledge based recommender system?,https://www.reddit.com/r/MachineLearning/comments/2pffna/where_can_i_find_implementationtutorial_of/,schmity,1418698439,"Hey,
I'm trying to find an implementation of a knowledge based or demographic recommender but all I have found so far are descriptions of what they are, not an example or tutorial which is what I'm looking for.  Any help?  Thanks.",0,2
195,2014-12-16,2014,12,16,16,2pg77r,Complex event processing and Machine Learning...,https://www.reddit.com/r/MachineLearning/comments/2pg77r/complex_event_processing_and_machine_learning/,__null__,1418715771,"Hi, I would like to learn wether there is an overlap between the fileds of complex event processing and machine learning",3,1
196,2014-12-16,2014,12,16,17,2pgb8k,Do you think that present-day machine learning will lead to Strong AI or is completely new paradigm needed?,https://www.reddit.com/r/MachineLearning/comments/2pgb8k/do_you_think_that_presentday_machine_learning/,[deleted],1418719573,"What I mean is that even if say Elon Musk develops liquid rockets rest of his life, that paradigm will never lead to faster than light travel. To accomplish that you need something completely different like warp drive or wormhole technology.

So do you think that just developing better deep learning algorithms etc will lead to Strong AI or do we need something completely new like from going from liquid rockets to warp drive?",42,4
197,2014-12-16,2014,12,16,17,2pgbl9,Error when using multiple taps using Scan in Theano,https://www.reddit.com/r/MachineLearning/comments/2pgbl9/error_when_using_multiple_taps_using_scan_in/,TheAlienDude,1418719959,,1,0
198,2014-12-16,2014,12,16,22,2pgurl,Conference report NIPS'14,https://www.reddit.com/r/MachineLearning/comments/2pgurl/conference_report_nips14/,cast42,1418737796,,3,16
199,2014-12-17,2014,12,17,0,2ph5tm,What every machine learning package can learn from Vowpal Wabbit,https://www.reddit.com/r/MachineLearning/comments/2ph5tm/what_every_machine_learning_package_can_learn/,gwulfs,1418744706,,8,12
200,2014-12-17,2014,12,17,2,2phjfq,Video for paper: Deep Neural Networks are Easily Fooled.,https://www.reddit.com/r/MachineLearning/comments/2phjfq/video_for_paper_deep_neural_networks_are_easily/,[deleted],1418751631,"Anh Nguyen made a video introducing a paper that was [discussed](https://www.reddit.com/r/MachineLearning/comments/2onzmd/deep_neural_networks_are_easily_fooled_high/) on r/MachineLearning a few days ago.

Here's the paper link in case anyone is interested:
http://arxiv.org/abs/1412.1897",0,1
201,2014-12-17,2014,12,17,2,2phkrd,Video explaining paper: Deep Neural Networks are Easily Fooled,https://www.reddit.com/r/MachineLearning/comments/2phkrd/video_explaining_paper_deep_neural_networks_are/,JasonYosinski,1418752259,,36,93
202,2014-12-17,2014,12,17,2,2phlk9,Read John Platt's view on key ML trends seen at NIPS 2014 at Montreal,https://www.reddit.com/r/MachineLearning/comments/2phlk9/read_john_platts_view_on_key_ml_trends_seen_at/,MLBlogTeam,1418752641,,0,3
203,2014-12-17,2014,12,17,9,2piwp2,[FP-Growth process] Help! Rapidminer is driving me mental,https://www.reddit.com/r/MachineLearning/comments/2piwp2/fpgrowth_process_help_rapidminer_is_driving_me/,D-Hex,1418775768,"Alright so I'm trying to do this simple FP-Growth process on Rapidminer 5.3. 

It's really straight forward copy of this:

https://www.youtube.com/watch?v=oXrUz5CWM4E

My source files are txt files - average length 24 pages. Around 173 KB.

I'm doing this on just one text file. Now I'm running an i7 six core processor with 16GB of RAM on Win 8.1

What it does is - start the process and then keeps running until it runs out of memory.  Has anyone had this problem and how have you solved it? 

PS- I went on the forum, tried all the solutions, no joy. 

PPS - not an advanced user so I haven't gone too far beyond this video aside from trying to use ""Memory Materialise"" and ""Memory Clear""  which does exactly the same. 

Apologies for being a klutz and thanks in advance. ",0,0
204,2014-12-17,2014,12,17,16,2pk137,Favorite Machine Learning Books?,https://www.reddit.com/r/MachineLearning/comments/2pk137/favorite_machine_learning_books/,matlab484,1418799942,"I'm looking to pick up some hard copies of machine learning resources, do you guys have any favorites in mind? Since the field moves so fast, I doubt there's any good hard cover books for the newest deep learning techniques, but fundamentals in statistics are always good. Also, if anyone has a favorite resource book for image classification that'd be great too.",13,18
205,2014-12-17,2014,12,17,19,2pkeao,A reliable source of comments on Machine Learning papers?,https://www.reddit.com/r/MachineLearning/comments/2pkeao/a_reliable_source_of_comments_on_machine_learning/,galapag0,1418812904,"I'm sure we all faced the challenge of going thought the same classic (and not so classic) papers on Machine Learning and read them, fail to understand them because there are some bits of information missing: comments. Some papers are difficult to understand (maybe the explanation is not exactly clear), some can have small (or even big!) mistakes, some can have missing information to understand them.
My question is: is there any reliable collaborative source of comments on Machine Learning papers (even maybe with comments per sentence or formula). It can be a blog, a wiki or a subreddit.",1,11
206,2014-12-17,2014,12,17,21,2pkjpp,"Hey guys, I'm working on an intelligent bot that runs on your browser. I'd love to hear what you think about it so far.",https://www.reddit.com/r/MachineLearning/comments/2pkjpp/hey_guys_im_working_on_an_intelligent_bot_that/,jonGomez,1418818460,,4,1
207,2014-12-17,2014,12,17,21,2pkkf4,Is adaboost in sklearn just gradient boosting but with an exponential loss function ?,https://www.reddit.com/r/MachineLearning/comments/2pkkf4/is_adaboost_in_sklearn_just_gradient_boosting_but/,pl47,1418819122,"Hi,


Reading about adaboost it says that it has an exponential loss function.
The sklearn version one can edit the loss functions, 
this is something only gradient boosting could do. 
So what is going on ? 

",5,2
208,2014-12-17,2014,12,17,22,2pknvo,Tutorial: Using convolutional neural nets to detect facial keypoints (based on Python and theano).,https://www.reddit.com/r/MachineLearning/comments/2pknvo/tutorial_using_convolutional_neural_nets_to/,ogrisel,1418822078,,9,67
209,2014-12-17,2014,12,17,23,2pkssu,A New Multilayer Model for Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/2pkssu/a_new_multilayer_model_for_hierarchical_temporal/,fergbyrne,1418825847,,18,3
210,2014-12-17,2014,12,17,23,2pkvic,Jeremy Howard: The wonderful and terrifying implications of computers that can learn,https://www.reddit.com/r/MachineLearning/comments/2pkvic/jeremy_howard_the_wonderful_and_terrifying/,CaptainHoek,1418827637,,0,0
211,2014-12-18,2014,12,18,0,2pky4b,What do you enjoy about machine learning?,https://www.reddit.com/r/MachineLearning/comments/2pky4b/what_do_you_enjoy_about_machine_learning/,AfraidOfToasters,1418829222,"I'm a student and have just been introduced to the basic concepts of machine learning. I have taken a CS course on machine learning as well as a Cognitive Science course about neural models.

I can really tell that I'm just barely scratching the surface right now. My only motivation to keep learning about it is basically that I want to understand it at a higher level. Curiosities I have about the subject are usually left unexplored because of my inability to approach them. The first thing that got me interested in machine learning was [this video](https://www.youtube.com/watch?v=bBt0imn77Zg).

So these are my questions for you (and myself):


1. Do you have anything that you would like to explore with machine learning? What is it that you are curious about?


2. What do you enjoy the most about machine learning?

I know to some people my questions may come across as childish but I really find these questions important. I don't like having my curiosity constrained by a syllabus. Even if you think I shouldn't consider these questions I would value even a brief explanation. 

My Answers:

1. I find that stochastic exploration for solutions yields interesting results. I would also like to know if there is any application for having several unique perspectives on how to solve a problem.

2. I don't know much about it yet but I feel that I will be able to use it to gain a better understanding of the world.",0,2
212,2014-12-18,2014,12,18,1,2pl5ty,Deep neural nets discovering novel features?,https://www.reddit.com/r/MachineLearning/comments/2pl5ty/deep_neural_nets_discovering_novel_features/,GibbsSamplePlatter,1418833500,"Do we have any good examples of deep neural networks capturing novel features that are then fed back into the science community?  

I vaguely recall something about cancer detection net finding out that surrounding healthy cells are just as important for detection, but can't find the link.  

Any others are appreciated too. ",4,3
213,2014-12-18,2014,12,18,2,2plb4d,"Towards Deep Neural Network Architectures Robust to Adversarial Examples [it's amusing how many people are selling ""adversarial examples"" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure]",https://www.reddit.com/r/MachineLearning/comments/2plb4d/towards_deep_neural_network_architectures_robust/,[deleted],1418836254,,0,1
214,2014-12-18,2014,12,18,2,2plbe6,How to best visualize breast cancer data,https://www.reddit.com/r/MachineLearning/comments/2plbe6/how_to_best_visualize_breast_cancer_data/,omnipresent101,1418836392,"I'm playing around with the wisconsin breast cancer data that predicts malignant or benign. 

I'm can't figure out which visualization to use to form my Hypothesis. I want my hypothesis to be something like ""which attribute would have the most effect on predicting the class variable"". 

I made a scatterplot matrix for this data but I think it doesn't show good visualization from which I can make manual predictions by just looking at the image. 

Which graph/visualization should I use to extract some meaningful information from the data by just visually looking at it?

https://dl.dropboxusercontent.com/s/d6jg7jwpb75erq3/Rplot14.png?dl=0


     &gt; library(foreign)

     &gt; breast &lt;- read.arff(""http://www.cs.iastate.edu/~cs573x/labs/lab1/breast-cancer-   
           wisconsin.arff"")

      &gt; breast$class &lt;- as.numeric(as.character(breast$class))
",0,0
215,2014-12-18,2014,12,18,2,2plelx,Headline = Machine Learning + Hackernews + Reddit + Pocket,https://www.reddit.com/r/MachineLearning/comments/2plelx/headline_machine_learning_hackernews_reddit_pocket/,therealadammenges,1418838024,,1,0
216,2014-12-18,2014,12,18,4,2plofg,"""Impossibru"" meme is a face in the Toronto Faces Dataset",https://www.reddit.com/r/MachineLearning/comments/2plofg/impossibru_meme_is_a_face_in_the_toronto_faces/,impossibru_ML,1418842865,"I was looking through the 0-fold test set of the Toronto Faces Dataset and saw [this face](http://i.imgur.com/NDY6W4K.jpg) looking at me. His expression is ""disgust.""

The image on the right is from the TFD. How did this happen?",7,44
217,2014-12-18,2014,12,18,4,2plpgq,"Machine Learning Trends from NIPS 2014, blog post by John Platt",https://www.reddit.com/r/MachineLearning/comments/2plpgq/machine_learning_trends_from_nips_2014_blog_post/,[deleted],1418843413,,0,1
218,2014-12-18,2014,12,18,4,2plrfo,"Towards Deep Neural Network Architectures Robust to Adversarial Examples [it's amusing how many people are selling ""adversarial examples"" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure]",https://www.reddit.com/r/MachineLearning/comments/2plrfo/towards_deep_neural_network_architectures_robust/,[deleted],1418844381,,0,1
219,2014-12-18,2014,12,18,4,2pls8q,"Towards DNN Architectures Robust to Adversarial Examples - it's amusing how many people are selling ""adversarial examples"" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure",https://www.reddit.com/r/MachineLearning/comments/2pls8q/towards_dnn_architectures_robust_to_adversarial/,[deleted],1418844796,,0,1
220,2014-12-18,2014,12,18,4,2plsn7,"Towards DNN Architectures Robust to Adversarial Examples - it's amusing how many people are using ""adversarial examples"" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure",https://www.reddit.com/r/MachineLearning/comments/2plsn7/towards_dnn_architectures_robust_to_adversarial/,[deleted],1418845006,,0,1
221,2014-12-18,2014,12,18,4,2plt23,"Towards DNN Architectures Robust to Adversarial Examples - there are some who are exploiting adversarial examples as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure",https://www.reddit.com/r/MachineLearning/comments/2plt23/towards_dnn_architectures_robust_to_adversarial/,test3545,1418845217,,1,5
222,2014-12-18,2014,12,18,6,2pm51i,Reducing your R memory footprint by 7000x,https://www.reddit.com/r/MachineLearning/comments/2pm51i/reducing_your_r_memory_footprint_by_7000x/,hernamesbarbara,1418850978,,1,0
223,2014-12-18,2014,12,18,19,2po61b,Cluster Validity Indices (CVI) in Python,https://www.reddit.com/r/MachineLearning/comments/2po61b/cluster_validity_indices_cvi_in_python/,actjqm,1418896950,"Hi.

I've been working on the implementation of some CVIs that I needed for a project because sklearn only has Silhouete score implemented. I currently have Dunn and Davis Bouldin indices.

https://github.com/actjqm/jqm_cvi

I used this opportunity to try to learn Cython. I would greatly appreciate tips and comments on my .pyx.

Tell me if i should post this to /r/python instead. Posted here because I have the impression there are a lot o Python enthusiasts around here.

(sorry for functions with different inputs)",0,0
224,2014-12-18,2014,12,18,19,2po7tw,"How to do pixel accurate analysis of a user interface (classification of ""clickable"" for example) ?",https://www.reddit.com/r/MachineLearning/comments/2po7tw/how_to_do_pixel_accurate_analysis_of_a_user/,Schlagv,1418898779,"How can we do ""segmentation"" on a UI to find buttons or clickable areas ?

The traditional sliding window is great to find if the picture contains an item of interest.

How do we do this for pixel accurate detection of a button ? We use a sliding sliding window that only says ""true"" if the pixel at the center of the picture is ""clickable"" ?

Is this done with several levels of zoom, to first find large areas that contains clickable parts, then with smaller sliding windows in the clickable areas to find smaller areas and ultimately pixel accurate classification ?",4,0
225,2014-12-18,2014,12,18,19,2po8ww,"r/ML, i've hit a wall trying to code a simple RNN, please help!",https://www.reddit.com/r/MachineLearning/comments/2po8ww/rml_ive_hit_a_wall_trying_to_code_a_simple_rnn/,blkorcut,1418899899,"Hey guys,

I've come across a really crazy bug happening to me in coding backpropagation for an RNN in pure python. I'm comparing my implementation's results against an implementation from theano (which is much easier to code as there is no backpropagation to figure out). The weirdest bug is that I'm getting the last row of weights correct for the input -&gt; hidden layer connections, but the rest are incorrect. I.e, I'm correctly figuring out the derivative of the weights from the 5th unit of the first layer to each unit in the hidden layer, but none of the rest. All info is here:

http://stackoverflow.com/questions/27544698/pure-python-rnn-and-theano-rnn-computing-different-gradients-code-and-results

(and note that in the code df is the derivative of the hidden activation function f)

I'm totally blocked here, can't figure it out for the life of me. I really thought my backpropagation code is correct, but there must be an issue there. I'm doing gradient checks too and theano's implementation is computing the correct gradient. Really, really not sure what's happening with the last row of weights for input-&gt;hidden, though, that really makes no sense to me.

Youre my last hope r/ML!",1,11
226,2014-12-19,2014,12,19,1,2poxpx,DeepSpeech: Scaling up end-to-end speech recognition,https://www.reddit.com/r/MachineLearning/comments/2poxpx/deepspeech_scaling_up_endtoend_speech_recognition/,jesuslop,1418918728,"Research at Baidu for a best of its class spectrum-to-text RNN speech recognition highlighted also [here](http://venturebeat.com/2014/12/18/baidu-researchers-beef-up-the-search-giants-speech-recognition-savvy/).

EDIT actual [link](http://arxiv-web3.library.cornell.edu/abs/1412.5567).
EDIT2 allegedly better results reported by ibm watson group, see comments.",19,50
227,2014-12-19,2014,12,19,1,2pp1ib,Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?,https://www.reddit.com/r/MachineLearning/comments/2pp1ib/do_we_need_hundreds_of_classifiers_to_solve_real/,[deleted],1418920805,,0,1
228,2014-12-19,2014,12,19,4,2ppojv,"Implementing a deep LSTM/RNN  la ""Sequence to sequence learning...""",https://www.reddit.com/r/MachineLearning/comments/2ppojv/implementing_a_deep_lstmrnn__la_sequence_to/,qurun,1418932212,"I've been looking into ""Sequence to Sequence Learning with Neural Networks"" by Sutskever, Vinyals and Le ([arXiv:1409.3215](http://arxiv.org/abs/1409.3215)), and would like to understand how/why they implement a deep (4-layer) Long-Short Term Memory (LSTM) network.  

They do not explain the details of how they tie together the network, but from following the references I get the impression that they are using a connection structure like this: http://i.imgur.com/J3DwxSF.png (from ""Hybrid speech recognition with deep bidirectional LSTM"" http://www.cs.toronto.edu/~graves/asru_2013.pdf ), except using LSTM units.  

If they were using a deep RNN, though, I would think that a connection like this would work better: http://i.imgur.com/9txOrbN.png .  The difference is that I feed the output of the whole network into the input at the next time step, instead of feeding each layer's outputs into the same layer's inputs at the next time step.  

Why do they do it this way?  

Also, is there a good, modern reference that compares and contrasts approaches to recurrent neural networks?  For example, I would like to know: 

* Why are these special memory units better than a simple (possibly deep) recurrent neural network (RNN)?  

* Is there any good understanding of the difference between LSTM units and the simpler memory units introduced in ""Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation"" ([arXiv:1406.1078](http://arxiv.org/abs/1406.1078))?  It seems like this paper gets good results even with a shallow network (they use only one layer of hidden units). 

* Do any neural network packages have good built-in support for RNNs and these variants?  
So far, I have found LISA Groundhog https://github.com/lisa-groundhog/GroundHog and some simpler theano examples (like https://github.com/gwtaylor/theano-rnn).  
In torch7, nnx has a simple recurrent module (https://github.com/clementfarabet/lua---nnx#nnx.Recurrent).  I don't know whether/how it works with deep RNNs, and haven't found support for LSTMs or other memory modules.  
I haven't found anything in Caffe.  ",19,13
229,2014-12-19,2014,12,19,6,2pq4dq,Landscaping the relationship of Machine Learning with other academic fields,https://www.reddit.com/r/MachineLearning/comments/2pq4dq/landscaping_the_relationship_of_machine_learning/,bharatkhatri,1418939864,"Let's try to figure out how the following academic fields relate to each other :
1. Machine Learning
2. Data Mining
3. Artificial Intelligence
4. Computational Intelligence
5. Operations Research",1,0
230,2014-12-19,2014,12,19,7,2pqa9k,What version of Python are you all using?,https://www.reddit.com/r/MachineLearning/comments/2pqa9k/what_version_of_python_are_you_all_using/,greatluck,1418942776,"I'm following some video tutorials to learn the language and many of them still use version 2, even recent ones.  Just curious if it's worthwhile to still learn using ver. 2.",16,0
231,2014-12-19,2014,12,19,9,2pqkrh,derrivative of MSE (t-y)!=(y-t),https://www.reddit.com/r/MachineLearning/comments/2pqkrh/derrivative_of_mse_tyyt/,nevemkwa,1418948408,"I've only just scratched the surface of ML and I've implemented a NN which seems to work, but I don't understand completely why and how, and I've become obsessed with this which is strange as I never cared about math which is a problem I guess.

Anyway, I kindof got the ""aha"" moment watching this video https://www.youtube.com/watch?v=p1-FiWjThs8&amp;spfreload=10 and I wanted to plot E/w just for me to se how the derivative is telling me if I need to increase or decrease w.


My code is quite straight forward, i set a particular weight in the output layer in a loop and then assign:


err=((target[0]-output[0])*(target[0]-output[0]))/2;

dtErr=(target[0]-output[0])*NeuralNetwork::derivativeSigmoid(output[0], 1);


When plotted for w [-1,1] i should get the MSE and its derivative, right?
Wrong.

I get MSE, but instead of derivative I get the inverse derivative, meaning it's positive when MSE is climbing. Now what. MSE is quadratic, so switching (t-y)^2 to (y-t)^2 would give the same result, but the derivative would then be (y-t), which would actually plot the correct graph. But of course, making that change in the neural net corrupts it.

What am I missing? Why is that derivative positive when it should be negative and why is that ok??
		",3,1
232,2014-12-19,2014,12,19,10,2pqp6h,Bill Gates briefly discusses machine learning with David Letterman in 1995,https://www.reddit.com/r/MachineLearning/comments/2pqp6h/bill_gates_briefly_discusses_machine_learning/,b4xt3r,1418950972,The entire interview is interesting from a historical perspective but I found [Bill Gates' brief thought about machine learning](https://www.youtube.com/watch?v=lz6IQX7uDk4&amp;t=6m40s) to be the most interesting.  The ML part of the interview is all of 15 seconds long but gives some historical perspective to the field back in 1995.,11,8
233,2014-12-19,2014,12,19,12,2pr5gf,Learning object classification with pose,https://www.reddit.com/r/MachineLearning/comments/2pr5gf/learning_object_classification_with_pose/,mackie__m,1418960548,"I'm trying to understand how to model a learner, that can understand objects and the pose of the object (This is an idea for a research problem). The output I'd want would be a 'cup on it side' (90 degrees tilted), 'a cup upright' (normal position), 'a cup inverted' (180 degree tilted), 'a knife laying flat' (on a table), 'a knife horizontal' (while being used by someone). It would be okay to give output like 'a knife - 180 degrees'. I used natural language to get the point across. 

I think the subtlety here would be that the algorithm learns through looking at objects and the natural pose of it. How can you guide a learner, (maybe a deep net using sparse auto encoders as an unsupervised learning method would be great!), to learn pose as well as the objects?",4,0
234,2014-12-19,2014,12,19,13,2prbnm,The Deep Learning Saga,https://www.reddit.com/r/MachineLearning/comments/2prbnm/the_deep_learning_saga/,[deleted],1418964487,,0,1
235,2014-12-19,2014,12,19,15,2prmje,"The Story of Siri, by its founder Adam Cheyer",https://www.reddit.com/r/MachineLearning/comments/2prmje/the_story_of_siri_by_its_founder_adam_cheyer/,[deleted],1418971839,,0,0
236,2014-12-19,2014,12,19,17,2prsqw,How would you approach this problem?,https://www.reddit.com/r/MachineLearning/comments/2prsqw/how_would_you_approach_this_problem/,harfharf11,1418977556,"Hey all,

Let's say I'm faced with this problem: A user has to choose among 10 options. Each option draws from a 5D random variable, let's say X = [is_black, is_sedan, is_cheap, is_fast, is_efficient] (pretty sure that's not standard notation, sorry). Each dimension is binary. If you are given 100 users, how would you model this problem in order to make good predictions on what future users would choose? This is what I've tried so far:

Combining the data from all of the observations (where one observation is one user selecting among 10 options), and unrolling each observation such that 10 rows were produced per observation, each containing X's outcome, and an additional output column y where y = 1 if the user chose the car and 0 otherwise, I split the data into a training and test set and fed the training set into a random forest, logistic regression, and SVM. The test results were pitiful.

I then figured that I was discarding important information by ""unrolling"" each user choice into independent rows. So then I processed the test data into chunks of 10 (since we know that a user must make a choice), and picked the highest raw scoring row as the predicted choice using SVM and logreg. This too performed poorly. For fun I also tried just using the raw counts of each possible outcome of X seen in the training data and using that as a ""probability"" of each option being chosen, and selecting the highest. This too did poorly.

I don't know how I would represent each observation without unrolling it as I had done to make it suitable for the algorithms I have chosen. I.e., training in a way that preserves the idea that a choice was made among each observation. My first thought is, if that were done, you would be training multiple models on tiny batches of 10, and that doesn't seem like a good idea.

Am I on the wrong track here? Does anyone have any better ideas or suggestions?

Also, if this is the wrong subreddit for this type of question I apologize. Thanks!",1,0
237,2014-12-19,2014,12,19,19,2pryt4,Communication-Efficient Distributed Dual Coordinate Ascent,https://www.reddit.com/r/MachineLearning/comments/2pryt4/communicationefficient_distributed_dual/,jellchou,1418983897,,0,2
238,2014-12-19,2014,12,19,19,2ps08a,Buy And Sell Used Construction Equipment,https://www.reddit.com/r/MachineLearning/comments/2ps08a/buy_and_sell_used_construction_equipment/,infrabazaarpvt,1418985521,,0,1
239,2014-12-19,2014,12,19,23,2psg5c,mlpack: A scalable C++ machine learning library,https://www.reddit.com/r/MachineLearning/comments/2psg5c/mlpack_a_scalable_c_machine_learning_library/,kraakf,1418999732,,6,17
240,2014-12-20,2014,12,20,1,2pst73,[Academic Research] Machine learning in computer security,https://www.reddit.com/r/MachineLearning/comments/2pst73/academic_research_machine_learning_in_computer/,piscoster,1419007309,"Hello guys,

I am looking for academic research papers about the usage of machine learning in the computer security. At the moment I haven`t found much about this topic on google scholar and [dblp](http://www.informatik.uni-trier.de/~ley/db/). Most of the security research focuses on for example.: detection of malware with signature/rule based patterns. So usually complex event processing and no use of machine learning.

It seems to me that computer security does not apply machine learning that much, for example in IDS systems.

Where is machine learning applied in computer security heavily?

Please recommend me some papers and topics, where I can do further research on?

I really appreciate your answers!
",4,4
241,2014-12-20,2014,12,20,2,2psw2e,Increase Efficiency of Distillation Columns through Maleta Distillation Trays,https://www.reddit.com/r/MachineLearning/comments/2psw2e/increase_efficiency_of_distillation_columns/,maletacdcom,1419008881,,0,1
242,2014-12-20,2014,12,20,3,2pt2bl,Andrew Ng and Baidu Announces Breakthrough In Speech Recognition,https://www.reddit.com/r/MachineLearning/comments/2pt2bl/andrew_ng_and_baidu_announces_breakthrough_in/,TangerineX,1419012224,,27,73
243,2014-12-20,2014,12,20,3,2pt79u,Any existing work on supervised learning of features from audio waveform input?,https://www.reddit.com/r/MachineLearning/comments/2pt79u/any_existing_work_on_supervised_learning_of/,animus144,1419014753,,0,0
244,2014-12-20,2014,12,20,4,2ptbdp,convolution without summation?,https://www.reddit.com/r/MachineLearning/comments/2ptbdp/convolution_without_summation/,sshidy,1419016837,"Hi, I'm new to deep learning. Here's the question about Convolutional Net:
So a m*n feature map can be mapped to a (m-k+1)*(n-k+1) feature map by a k*k filter. But I don't think it's a (complete) convolution. It seems that a complete convolution computing should further sum the (m-k+1)*(n-k+1) features. However, the name and the mathematical function of convolution appeared in almost every materials make me confusing. I just want to know whether my understanding of Conv-Net is correct, that it's actually convolutions without summations.",2,1
245,2014-12-20,2014,12,20,4,2ptddn,The Geometry of Classifiers,https://www.reddit.com/r/MachineLearning/comments/2ptddn/the_geometry_of_classifiers/,rrenaud,1419017871,,1,1
246,2014-12-20,2014,12,20,4,2ptf26,When will unsupervised learning be as good as supervised learning?,https://www.reddit.com/r/MachineLearning/comments/2ptf26/when_will_unsupervised_learning_be_as_good_as/,sixwings,1419018742,"Andrew Ng has said many times that the holy grail of deep learning is unsupervised learning, i.e., the use of unlabeled training data. Why has there been no real progress in unsupervised learning?",6,0
247,2014-12-20,2014,12,20,5,2pthgf,SVM w/ RBF in OpenCL?,https://www.reddit.com/r/MachineLearning/comments/2pthgf/svm_w_rbf_in_opencl/,georgeo,1419019906,I saw that libsvm offers a CUDA implementation. Is there anything similar for OpenCL?,2,0
248,2014-12-20,2014,12,20,6,2ptt09,Learning about Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2ptt09/learning_about_machine_learning/,brianmannmath,1419025920,,1,0
249,2014-12-20,2014,12,20,7,2pty3w,fastest way to get off the ground with convnets,https://www.reddit.com/r/MachineLearning/comments/2pty3w/fastest_way_to_get_off_the_ground_with_convnets/,spurious_recollectio,1419028592,"I've been building a neural network library as a learning tool for myself (so I'm not expecting to beat state-of-the-art systems but still want reasonable performance).  I've gotten quite far with vanilla feed-forward nets but am now trying to implements conv-nets and am finding it quite hard to find decent resources.  

Since I actually want to use this system for something interesting like image classification I'm looking for a GPU based backend and I'd like to reduce the dependencies I need.  Currently I'm looking at incorporating [cudarray](https://github.com/andersbll/cudarray) but am having problems getting it to compile so I'd also like to consider alternatives.  My main requirements are something reasonably fast (GPU based on nvidia GPU), with a python interface or simple enough that I could write a cython wrapper, and low level enough that it just does convolutions and maybe pooling (I don't want to call another NN library).

I've also found a general dirth of good CNN resources so would appreciate any tutorials links, etc... on implementing a CNN.

thanks!",12,2
250,2014-12-20,2014,12,20,8,2pu1hx,"Received interesting but limited data-set, where to begin with probabilistic modeling?",https://www.reddit.com/r/MachineLearning/comments/2pu1hx/received_interesting_but_limited_dataset_where_to/,Ogi010,1419030487,"Hello /r/machinelearning,


I play in an recreational/upper skill level basketball league in Portland.  The people that run the league track games by recording the final score (as well as what week/season/year, and the team names obviously).  I thought it might be neat to try and develop an algorithm that can determine most ideal team match-ups based on past performance, so I asked the league if they had historical data, and if they would be willing to share.

The league got back to me right away and gave me a CSV file of data going back 4 years for approximately ~200 teams over a number of different leagues/skill levels.  

Given that I don't have many parameters to work with (final score, date and team names), and that I'm just starting to learn about machine learning (I'm a mechanical engineer by trade, just dipping my toes into the water regarding software, I know a little R, but I'm more familiar with Python), I was hoping that /r/machinelearning would give me some ideas on where to start.

Any input would be greatly appreciated!",2,1
251,2014-12-20,2014,12,20,8,2pu6ez,[Academic Research] Domain Transfer of knowledge in ML,https://www.reddit.com/r/MachineLearning/comments/2pu6ez/academic_research_domain_transfer_of_knowledge_in/,mackie__m,1419033308,"I'm looking at research topics for ML, and this seems to be something that regularly comes up. One paper I found is [this](http://www.machinelearning.org/proceedings/icml2007/papers/329.pdf), where it uses a phenomenon called Rule Transfer to speed up domain transfer of knowledge. 

Some questions I now have are:
With popularity of deep nets and auto encoders, is this problem interesting anymore? 
Given that we can automatically learn interesting characteristics, does it make sense to look at this problem? Why not just learn everything? 

It would be great to get some insight about this topic. I would appreciate some help. Thanks!",2,2
252,2014-12-20,2014,12,20,17,2pvc1c,Hands on tutorial for creating new classifier with Caffe?,https://www.reddit.com/r/MachineLearning/comments/2pvc1c/hands_on_tutorial_for_creating_new_classifier/,matlab484,1419062657,"I checked the fine tuning tutorial on Caffe's site, but I'm still confused. Lets say you have a data set of dogs and cats, and want to use one of the prebuilt networks like AlexNet with Caffe. How does this work? Do you just switch out the last layer with something else? Right now the prebuilt ones will make predictions with the 1000 categories from Imagenet, how do you make it predict on your set, like 'cat' or 'dog'?  

Has anyone seen a practical tutorial for something like this, with code? Thanks!",4,1
253,2014-12-20,2014,12,20,19,2pvl7t,Another paper researching ways to combat misclassification of adversarial examples by neural nets.,https://www.reddit.com/r/MachineLearning/comments/2pvl7t/another_paper_researching_ways_to_combat/,test3545,1419072769,,18,17
254,2014-12-20,2014,12,20,23,2pvz1c,Seminal papers on machine learning,https://www.reddit.com/r/MachineLearning/comments/2pvz1c/seminal_papers_on_machine_learning/,mostly_complaints,1419086879,What papers would you consider groundbreaking and a must read for students of the field?,15,17
255,2014-12-21,2014,12,21,0,2pw11t,The Maleta cyclic distillation LLC at the conference Distillation and Absorption 2014 showed pilot distillation column in cyclic operation,https://www.reddit.com/r/MachineLearning/comments/2pw11t/the_maleta_cyclic_distillation_llc_at_the/,maletacdcom,1419088428,,0,1
256,2014-12-21,2014,12,21,1,2pw56g,Ng's course.,https://www.reddit.com/r/MachineLearning/comments/2pw56g/ngs_course/,NicolasGuacamole,1419091383,,6,5
257,2014-12-21,2014,12,21,1,2pw5od,"Data Elixir, Issue 14: AI developments, interviews, data creeps, ""best of 2014"" collections",https://www.reddit.com/r/MachineLearning/comments/2pw5od/data_elixir_issue_14_ai_developments_interviews/,[deleted],1419091714,,0,0
258,2014-12-21,2014,12,21,6,2px2y1,v18.12 of the dlib C++ machine learning library was just released,https://www.reddit.com/r/MachineLearning/comments/2px2y1/v1812_of_the_dlib_c_machine_learning_library_was/,davis685,1419111937,,0,12
259,2014-12-21,2014,12,21,7,2pxa54,200 machine learning and data science resources,https://www.reddit.com/r/MachineLearning/comments/2pxa54/200_machine_learning_and_data_science_resources/,urinec,1419116194,,1,0
260,2014-12-21,2014,12,21,17,2pykro,[Question] Which model to predict air cleanness (Ozone/CO2) ?,https://www.reddit.com/r/MachineLearning/comments/2pykro/question_which_model_to_predict_air_cleanness/,[deleted],1419150605,"How hard it is to predict the air pollution?

I am an agronomist, doing some research on some small plants. The plants are very sensitive to air pollution in urban area (need deep explanation here, but it's not the problem.).

The Problem: We want to predict the ozone and particulate concentration in the air. Specifically, we have a dataset of collected ozone and particulate measurements in the last three months with hour-frequency. We could get other information (such: weather/wind/ ..).

What is the best model to predict and sense the cleanness of the air?

Self-learner here, can anyone recommend paper/research project to check?",4,1
261,2014-12-21,2014,12,21,18,2pynza,Machine learning vs Reasoning and knowledge base: When to use what?,https://www.reddit.com/r/MachineLearning/comments/2pynza/machine_learning_vs_reasoning_and_knowledge_base/,piscoster,1419154235,"Hello guys,

in a lot of papers scientists use a knowledge base to check or represent data.

I am comparing machine learning versus reasoning systems, which use a knowledge base as a backeend. In my opinion machine learning wins over logic, when:

* Performance is an issue
* What is not in the knowledge base is not known to the reasoning system
* Unknown anomalies are completely invisible to the creator of the knowledge base

What do you think is an argument against/for using logic and a knowlege base versus machine learning?

I appreciate your reply!",9,13
262,2014-12-22,2014,12,22,0,2pz8c3,What Programming Language to choose out of C / C++ and Fortran ?,https://www.reddit.com/r/MachineLearning/comments/2pz8c3/what_programming_language_to_choose_out_of_c_c/,muktabh,1419176595,"Hi,
Which of these programming languages makes more sense to implement a Neural Net like http://research.microsoft.com/apps/pubs/?id=226584 or http://nips.cc/Conferences/2014/Program/event.php?ID=4554 ? 
I know Python well and have written algorithms using theano , but these papers have some elements which are hard to write in theano. In case, theano doesnt suffice to write them, what would be best alternative out of these 3 ?
I have never worked in C++ and Fortran and used C in some courses in my college. All three seem to be having support for GPU and interoperability with Python . 
Is there any other factor I should take into consideration while choosing one ?",42,3
263,2014-12-22,2014,12,22,5,2q01z5,Difference between invariance and equivariance(in terms of convolutional neural networks),https://www.reddit.com/r/MachineLearning/comments/2q01z5/difference_between_invariance_and_equivariancein/,nilspin,1419194844,"This might be more suitable for /r/statistics but I am trying to learn about convolutional neural networks so this seemed like a better place.

I was watching Geoffery Hinton's coursera lecture on convolutional neural nets and at one point he says ""replicated feature detectors achieve 'tranlational equivariance' rather than 'translational invariance' "".

I do not have background in statistics so was wondering if someone could give an explanation in layman terms.",3,7
264,2014-12-22,2014,12,22,6,2q08ll,[Academic research] Machine learning and complex event processing - Correlation of streamed events,https://www.reddit.com/r/MachineLearning/comments/2q08ll/academic_research_machine_learning_and_complex/,Regentag,1419198541,"Hello guys,

I am looking for academic research papers about the usage of machine learning with complex event processing. I plan to focus much more on the topic machine learning, because even though my background is in statistics and computer science, I haven`t done much research in this field.

Specifically I want to focus, how to determine if an event depends on an other event. At the moment I haven`t found much about this topic on google scholar and dblp. 

Most of the complex event processing research focuses on reasoning and system execution. So usually these systems do not use machine learning.

* Where is machine learning applied in complex event processing?
* Is it possible to find correlation of events, or even patterns, in an event stream via machine learning?

Please recommend me some papers, where I can do further research on?

I really appreciate your answers!
",7,3
265,2014-12-22,2014,12,22,8,2q0hg9,How to train Auto-Encoders with Tied Weights?,https://www.reddit.com/r/MachineLearning/comments/2q0hg9/how_to_train_autoencoders_with_tied_weights/,sungiv,1419203589,"Hi there.

I started to read about auto-encoders a short time ago and I am trying to imagine how I could employ an under-complete AE (I'm considering the simplest scenario possible, no denoising, only with a hidden layer).

The idea of reconstructing the input in the output layer seems trivial to me, but I just can't understand how it is possible to apply the backpropagation algorithm in the training phase if one considers W in the encoder and W' the decoder. Furthermore, what advantages or disadvantages will I have if I use tied weights and why that simple property assures me those advantages? And how do I assure that both matrices will continue to be transpose when I backpropagate the error?

I am pretty sure that something is wring in my brain and I can't see what is going on. I hope someone can help :)

Thank you in advance.",7,8
266,2014-12-22,2014,12,22,9,2q0oau,Introducing Seldon: the Open Predictive Platform,https://www.reddit.com/r/MachineLearning/comments/2q0oau/introducing_seldon_the_open_predictive_platform/,ahousley,1419207423,,0,1
267,2014-12-22,2014,12,22,9,2q0pzk,Continuous Hierarchical Temporal Memory (CHTM) Part 1,https://www.reddit.com/r/MachineLearning/comments/2q0pzk/continuous_hierarchical_temporal_memory_chtm_part/,CireNeikual,1419208399,,0,9
268,2014-12-22,2014,12,22,11,2q14r0,"Naive Bayes SVM: Simple, Good Sentiment and Topic Classification",https://www.reddit.com/r/MachineLearning/comments/2q14r0/naive_bayes_svm_simple_good_sentiment_and_topic/,rantana,1419217098,,1,10
269,2014-12-22,2014,12,22,13,2q1gb4,Image a circle of neurons. Each neuron is (or can) be connected to any other neuron. The weights between them can only be binary (0 or 1). What is this architecture called?,https://www.reddit.com/r/MachineLearning/comments/2q1gb4/image_a_circle_of_neurons_each_neuron_is_or_can/,Ayakalam,1419224072,"Edit: That should real ""imagine"", not ""image"".

Title says it all. 

Let's imagine we have, say, 10 neurons, all in a circle. Now, each neuron can be connected to any other one, (except itself of course). However also crucially, the weights associated between every neurons can only ever be binary, that is to say, either 0 or 1. 

What might this structure be called? As an aside, would the modeling of such a structure be possible with something like Theano, or Caffe? 

Thanks!",28,0
270,2014-12-22,2014,12,22,16,2q1v2x,What's wrong with convolutional nets?,https://www.reddit.com/r/MachineLearning/comments/2q1v2x/whats_wrong_with_convolutional_nets/,downtownslim,1419234708,,20,25
271,2014-12-22,2014,12,22,18,2q2185,Improving Naive Bayes accuracy for text classification?,https://www.reddit.com/r/MachineLearning/comments/2q2185/improving_naive_bayes_accuracy_for_text/,trininxs,1419240976,"Hi everyone,

I am performing document (text) classification on the category of websites, and use the website content (tokenized, stemmed and lowercased) as the feature set for my data.

My problem is that I have two over-represented categories which has vastly more data points than any other (roughly 70% or 4000~ of my data points are of his one of these two categories, while about 20 other categories make up the last 30%, some of which have fewer than 50 data points). I had a couple questions as to how I could improve on this:

1. What could I do to improve the accuracy of my classifier in this case of sparse data for some of the labels? Should I simply discard a certain proportion of the data points in the category which is over-represented? Should I use something other than Gaussian Naive Bayes with tf-idf?

2. After I perform the classification, I save the tfidf vector as well as the classifier to disk. However, when I re-rerun the classification on the same data, I sometimes get different results from what I initially got (for example, if previously a data point was classified as ""Entertainment"", it might receive ""News"" now). Is this indicative of an error in my implementation, or expected?",14,8
272,2014-12-22,2014,12,22,20,2q276r,Plastic Bag Making Machine,https://www.reddit.com/r/MachineLearning/comments/2q276r/plastic_bag_making_machine/,ramylemon,1419247269,,0,0
273,2014-12-22,2014,12,22,21,2q2bbx,How to Grow and Prune a Classification Tree,https://www.reddit.com/r/MachineLearning/comments/2q2bbx/how_to_grow_and_prune_a_classification_tree/,cavedave,1419251376,,0,22
274,2014-12-23,2014,12,23,0,2q2r1i,Interactive in-browser 3D visualization of datasets,https://www.reddit.com/r/MachineLearning/comments/2q2r1i/interactive_inbrowser_3d_visualization_of_datasets/,Foxtr0t,1419263063,,0,8
275,2014-12-23,2014,12,23,2,2q301e,Convolutional Nets and CIFAR-10: An Interview with Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/2q301e/convolutional_nets_and_cifar10_an_interview_with/,vodkagoodmeatrotten,1419268034,,0,25
276,2014-12-23,2014,12,23,5,2q3rn2,"How can I get ""Train time"" and ""Test time"" in WEKA?",https://www.reddit.com/r/MachineLearning/comments/2q3rn2/how_can_i_get_train_time_and_test_time_in_weka/,omnipresent101,1419281612,"I'm using weka J48 on my dataset to predict the quality of the wine: http://zangsir.weebly.com/uploads/3/1/3/8/3138983/wine2.arff

When I classify the data in Weka using J48 and a 66% percentage split, I get the following Classifier output. But nowhere in this output do I see the ""Train time"" and ""Test time"".

How can I get the ""Train time"" and ""Test time""? Do I need to run with option ""use training set"" to get the ""train time"" and then run again with 66% split to get the ""test time""?


        === Run information ===
        
        Scheme:weka.classifiers.trees.J48 -C 0.25 -M 2
        Relation:     wines
        Instances:    6497
        Attributes:   13
                      fixed.acidity
                      volatile.acidity
                      citric.acid
                      residual.sugar
                      chlorides
                      free.sulfur.dioxide
                      total.sulfur.dioxide
                      density
                      pH
                      sulphates
                      alcohol
                      quality
                      kind
        Test mode:split 66.0% train, remainder test
        
        === Classifier model (full training set) ===
        
        J48 pruned tree
        ------------------
        
        total.sulfur.dioxide &lt;= 67
        |   chlorides &lt;= 0.049
        |   |   sulphates &lt;= 0.55
        |   |   |   density &lt;= 0.99455: white (108.0/1.0)
        |   |   |   density &gt; 0.99455
        |   |   |   |   residual.sugar &lt;= 2.65: red (5.0)
        |   |   |   |   residual.sugar &gt; 2.65: white (7.0)
        |   |   sulphates &gt; 0.55
        |   |   |   chlorides &lt;= 0.037: white (15.0/1.0)
        |   |   |   chlorides &gt; 0.037
        |   |   |   |   alcohol &lt;= 10.6: white (4.0/1.0)
        |   |   |   |   alcohol &gt; 10.6: red (22.0)
        |   chlorides &gt; 0.049
        |   |   density &lt;= 0.99442
        |   |   |   pH &lt;= 3.19
        |   |   |   |   total.sulfur.dioxide &lt;= 36: red (6.0)
        |   |   |   |   total.sulfur.dioxide &gt; 36: white (10.0)
        |   |   |   pH &gt; 3.19: red (98.0/1.0)
        |   |   density &gt; 0.99442: red (1129.0)
        total.sulfur.dioxide &gt; 67
        |   chlorides &lt;= 0.067
        |   |   citric.acid &lt;= 0.11
        |   |   |   pH &lt;= 3.49: white (80.0/1.0)
        |   |   |   pH &gt; 3.49
        |   |   |   |   total.sulfur.dioxide &lt;= 100: red (11.0)
        |   |   |   |   total.sulfur.dioxide &gt; 100: white (7.0/1.0)
        |   |   citric.acid &gt; 0.11
        |   |   |   chlorides &lt;= 0.059
        |   |   |   |   volatile.acidity &lt;= 0.575: white (4254.0/14.0)
        |   |   |   |   volatile.acidity &gt; 0.575
        |   |   |   |   |   pH &lt;= 3.45: white (37.0)
        |   |   |   |   |   pH &gt; 3.45: red (5.0/1.0)
        |   |   |   chlorides &gt; 0.059
        |   |   |   |   total.sulfur.dioxide &lt;= 89
        |   |   |   |   |   fixed.acidity &lt;= 8.4: white (5.0/1.0)
        |   |   |   |   |   fixed.acidity &gt; 8.4: red (4.0)
        |   |   |   |   total.sulfur.dioxide &gt; 89
        |   |   |   |   |   sulphates &lt;= 0.53: white (123.0)
        |   |   |   |   |   sulphates &gt; 0.53
        |   |   |   |   |   |   volatile.acidity &lt;= 0.47
        |   |   |   |   |   |   |   fixed.acidity &lt;= 6.15: red (3.0/1.0)
        |   |   |   |   |   |   |   fixed.acidity &gt; 6.15: white (35.0)
        |   |   |   |   |   |   volatile.acidity &gt; 0.47: red (2.0)
        |   chlorides &gt; 0.067
        |   |   total.sulfur.dioxide &lt;= 153
        |   |   |   density &lt;= 0.99498
        |   |   |   |   alcohol &lt;= 11.1: white (75.0/1.0)
        |   |   |   |   alcohol &gt; 11.1
        |   |   |   |   |   density &lt;= 0.992: white (11.0)
        |   |   |   |   |   density &gt; 0.992: red (5.0)
        |   |   |   density &gt; 0.99498
        |   |   |   |   volatile.acidity &lt;= 0.305
        |   |   |   |   |   fixed.acidity &lt;= 7.8: white (16.0)
        |   |   |   |   |   fixed.acidity &gt; 7.8: red (3.0)
        |   |   |   |   volatile.acidity &gt; 0.305
        |   |   |   |   |   residual.sugar &lt;= 8.2: red (282.0/1.0)
        |   |   |   |   |   residual.sugar &gt; 8.2
        |   |   |   |   |   |   pH &lt;= 3.14: white (11.0)
        |   |   |   |   |   |   pH &gt; 3.14: red (6.0)
        |   |   total.sulfur.dioxide &gt; 153: white (118.0/1.0)
        
        Number of Leaves  : 	31
        
        Size of the tree : 	61
        
        
        Time taken to build model: 0.1 seconds
        
        === Evaluation on test split ===
        === Summary ===
        
        Correctly Classified Instances        2166               98.0534 %
        Incorrectly Classified Instances        43                1.9466 %
        Kappa statistic                          0.9461
        Mean absolute error                      0.0217
        Root mean squared error                  0.1357
        Relative absolute error                  5.8797 %
        Root relative squared error             31.8558 %
        Total Number of Instances             2209     
        
        === Detailed Accuracy By Class ===
        
                       TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class
                         0.954     0.011      0.963     0.954     0.959      0.975    red
                         0.989     0.046      0.986     0.989     0.987      0.975    white
        Weighted Avg.    0.981     0.038      0.98      0.981     0.981      0.975
        
        === Confusion Matrix ===
        
            a    b   &lt;-- classified as
          501   24 |    a = red
           19 1665 |    b = white
        
        

",1,1
277,2014-12-23,2014,12,23,5,2q3s9p,Interpreting random forests,https://www.reddit.com/r/MachineLearning/comments/2q3s9p/interpreting_random_forests/,themelink,1419281910,,21,3
278,2014-12-23,2014,12,23,6,2q3swd,can Multilayer perceptrons and standard Backpropagation algorithm take revenge against Deep learning architectures and methods?,https://www.reddit.com/r/MachineLearning/comments/2q3swd/can_multilayer_perceptrons_and_standard/,[deleted],1419282225,,2,0
279,2014-12-23,2014,12,23,8,2q4cai,Any advice?,https://www.reddit.com/r/MachineLearning/comments/2q4cai/any_advice/,irwt,1419292263,"Greetings,
I am a first year bioinformatics bachelor student and I got interested in machine learning. So far, I have experience only with python, and was wondering if I should continue with python, or start learn a new language. 
I want to know what programming language(s) you use for machine learning, and why? What are the pros and cons of different languages for machine learning? 
",5,0
280,2014-12-23,2014,12,23,9,2q4fou,Machine Learning Lab - Regex Generator - Automatic Generation of Text Extraction Patterns from Examples with Genetic Programming,https://www.reddit.com/r/MachineLearning/comments/2q4fou/machine_learning_lab_regex_generator_automatic/,bboyjkang,1419294028,,3,7
281,2014-12-23,2014,12,23,9,2q4j8x,Convolutional neural network morphing pictures of chairs,https://www.reddit.com/r/MachineLearning/comments/2q4j8x/convolutional_neural_network_morphing_pictures_of/,Noncomment,1419295982,,30,40
282,2014-12-23,2014,12,23,10,2q4n8l,Machine Learning Resources,https://www.reddit.com/r/MachineLearning/comments/2q4n8l/machine_learning_resources/,gwulfs,1419298266,,0,3
283,2014-12-23,2014,12,23,11,2q4qhq,Looking for simple machine learning projects that deal with real world data to get started!!!,https://www.reddit.com/r/MachineLearning/comments/2q4qhq/looking_for_simple_machine_learning_projects_that/,ankitsablok89,1419300120,Now that I am over with my exams I want to get my hands dirty with some machine learning and I plan to learn some machine learning algorithms by coding them myself either in Python or Java or C++ and I am looking for some projects that I accomplish in a span of 2 months. Any project suggestions will be helpful. Thanks a lot in advance :).,1,0
284,2014-12-23,2014,12,23,14,2q5az5,Can someone ELI5? What can I actually do (research-wise) by deriving hidden topic models?,https://www.reddit.com/r/MachineLearning/comments/2q5az5/can_someone_eli5_what_can_i_actually_do/,the14thd0ct0r,1419312124,What can actually be done from there with the information?,3,1
285,2014-12-24,2014,12,24,2,2q6viy,"See what WIRED mag calls ""a new Microsoft technology that seems borrowed from the world of Star Trek",https://www.reddit.com/r/MachineLearning/comments/2q6viy/see_what_wired_mag_calls_a_new_microsoft/,MLBlogTeam,1419355348,,1,0
286,2014-12-24,2014,12,24,3,2q7152,Linear Algebra for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2q7152/linear_algebra_for_machine_learning/,jasonb,1419358164,,8,39
287,2014-12-24,2014,12,24,5,2q7fm1,Deep Fried Convnets (x-post r/CompressiveSensing),https://www.reddit.com/r/MachineLearning/comments/2q7fm1/deep_fried_convnets_xpost_rcompressivesensing/,compsens,1419365334,,1,7
288,2014-12-24,2014,12,24,6,2q7mxb,Semantic Hash for newbs,https://www.reddit.com/r/MachineLearning/comments/2q7mxb/semantic_hash_for_newbs/,[deleted],1419368992,"Hi,

So I have some math background but I'm having a bit of trouble understanding what I need to do.

I get the impression that I have a a multi layer neural network with SDAs in the layers. 

My goal is to find a semantic hash for documents. Maybe there's a pybrains implementation somewhere I could look at when I want to make my own.



Here are some resources I found.
http://deeplearning.net/tutorial/code/SdA.py
http://www.cs.toronto.edu/~rsalakhu/papers/semantic_final.pdf
",0,2
289,2014-12-24,2014,12,24,6,2q7ppm,Yann LeCun: Training convolutional features on unlabeled video data,https://www.reddit.com/r/MachineLearning/comments/2q7ppm/yann_lecun_training_convolutional_features_on/,[deleted],1419370432,,1,4
290,2014-12-24,2014,12,24,7,2q7tu3,How can I cluster strings with multiple words?,https://www.reddit.com/r/MachineLearning/comments/2q7tu3/how_can_i_cluster_strings_with_multiple_words/,DemonKingWart,1419372486,"I have a set of about 10^5 short strings (1-6 words each) and I want to cluster them into about 10^3 groups based on the presence of similar words.  All I can think of now is creating a TDM after stemming and then doing hierarchical clustering.  Any better ideas?

The purpose of this is for regression, so I could use fuzzy clustering or a numeric vector for each string as well.  I just would like to reduce the dimensionality.  ",3,2
291,2014-12-24,2014,12,24,8,2q85pn,Backprop gradient twice as big as finite difference suggests on neural network. Any advice?,https://www.reddit.com/r/MachineLearning/comments/2q85pn/backprop_gradient_twice_as_big_as_finite/,ledmmaster,1419378880,"Hey guys,

I am programming a neural network from scratch in Python, and when I do a numerical check of the backpropagation gradient, I see numbers like the following example: 

Gradient: -0.0375543629722
Finite difference: 0.0187042723576 

The analytical gradient is around twice as big (ignoring the sign) as the finite difference.

I am using the following formula for the numerical gradient:

f(x + eps) - f(x - eps) / 2*eps

For the single unit sigmoid output layer I am using the following analytical gradient: (target - prediction) * activation_of_hidden_unit

It's based on the cross-entropy loss, and uses Stochastic Gradient Descent (maybe it has something to do with it?).

The network trains and generalizes, but I am curious and a little bit worried about this gradient problem.

If you need more details, please tell.

Thank you for your help!",4,2
292,2014-12-24,2014,12,24,12,2q8t59,Continuous Hierarchical Temporal Memory Part 2,https://www.reddit.com/r/MachineLearning/comments/2q8t59/continuous_hierarchical_temporal_memory_part_2/,CireNeikual,1419392828,,0,0
293,2014-12-24,2014,12,24,16,2q9d18,Forklift Training,https://www.reddit.com/r/MachineLearning/comments/2q9d18/forklift_training/,aus_andr,1419405946,,0,0
294,2014-12-24,2014,12,24,20,2q9t41,Buy Self Priming Mud Pumps online,https://www.reddit.com/r/MachineLearning/comments/2q9t41/buy_self_priming_mud_pumps_online/,reliableengineers1,1419422026,,4,0
295,2014-12-24,2014,12,24,22,2q9zfp,Making Sense of Word2vec Extensions [with code],https://www.reddit.com/r/MachineLearning/comments/2q9zfp/making_sense_of_word2vec_extensions_with_code/,piskvorky,1419428622,,0,38
296,2014-12-24,2014,12,24,22,2q9zh9,Who is interested in working on Open Source Data-Science Projects [As a mentor or contributor].,https://www.reddit.com/r/MachineLearning/comments/2q9zh9/who_is_interested_in_working_on_open_source/,SomeoneisWondering,1419428671,"I'd like to work on data-driven open source projects in my spare time. If you are interested in working as a group of people to build large-scale projects during the weekends, exchange ideas, chatting and discussing different models. Please, comment on this post (you can help us learn new things, or we can build things together )

Let's focus on tackling different problems.


**UPDATE**

If you are interested in this initiative, we have started a new subreddit [r/OpenDataScience][1]. We strongly encourage you to join this subreddit and add comments with your ideas, suggestions, and interest!

[1]: http://www.reddit.com/r/OpenDataScience/",57,23
297,2014-12-24,2014,12,24,22,2qa0i2,Enthusiasts and Skeptics Debate Artificial Intelligence - Vanity Fair,https://www.reddit.com/r/MachineLearning/comments/2qa0i2/enthusiasts_and_skeptics_debate_artificial/,[deleted],1419429593,,0,0
298,2014-12-25,2014,12,25,0,2qa5t7,Feature Extraction in Deep Neural Nets and Renormalization in Quantum Mechanics characterized by same mathematics,https://www.reddit.com/r/MachineLearning/comments/2qa5t7/feature_extraction_in_deep_neural_nets_and/,blindConjecture,1419433668,,5,15
299,2014-12-25,2014,12,25,0,2qa919,Python Neural Net Fails to Train. Gives roughly the same output values on every output neuron.,https://www.reddit.com/r/MachineLearning/comments/2qa919/python_neural_net_fails_to_train_gives_roughly/,[deleted],1419435917,"Hi all,

Just wrote up my first Neural Network Class in python. Everything as far as I can tell should work, but there is some bug in it that I can't seem to find(Probably staring me right in the face). 

I first tried it on 10,000 examples of the MNIST data, then again when trying to replicate the sign function, and again when trying to replicate a XOR Gate. Every time, regardless of the # of epochs, it always produces output from all the output neurons(regardless of how many there may be) that are all roughly the same value, but the cost function seems to be going down. 

I am using batch gradient descent, all done using vectors(no loop for each training example).  

    #Neural Network Class
    
    import numpy as np
    
    
    
    class NeuralNetwork:
        
        #methods
        def __init__(self,layer_shape):
            #Useful Network Info
            self.__layer_shape = layer_shape
            self.__layers = len(layer_shape)
            
            #Initialize Random Weights
            self.__weights = [] 
            self.__weight_sizes = []
            for i in range(len(layer_shape)-1):
                current_weight_size = (layer_shape[i+1],layer_shape[i]+1)
                self.__weight_sizes.append(current_weight_size)
                self.__weights.append(np.random.normal(loc=0.1,scale=0.1,size=current_weight_size))
            
        def sigmoid(self,z):
            return (1/(1+np.exp(-z)))
        
        def sig_prime(self,z):
            return np.multiply(self.sigmoid(z),(1-self.sigmoid(z)))
        
        
        def Feedforward(self,input,Train=False):
            self.__input_cases = np.shape(input)[0]
            
            #Empty list to hold the output of every layer.
            output_list = []
            #Appends the output of the the 1st input layer.
            output_list.append(input)
            
            for i in range(self.__layers-1):
                if i == 0:
                    output = self.sigmoid(np.dot(np.concatenate((np.ones((self.__input_cases,1)),input),1),self.__weights[0].T))
                    output_list.append(output)
                else:
                    output = self.sigmoid(np.dot(np.concatenate((np.ones((self.__input_cases,1)),output),1),self.__weights[i].T))                 
                    output_list.append(output)
            
            #Returns the final output if not training.         
            if Train == False:
                return output_list[-1]
            #Returns the entire output_list if need for training
            else:
                return output_list
        
        def CostFunction(self,input,target,error_func=1):
            """"""Gives the cost of using a particular weight matrix 
            based off of the input and targeted output""""""
            
            #Run the network to get output using current theta matrices.
            output = self.Feedforward(input)
            
            
            #####Allows user to choose Cost Functions.##### 
            
            #
            #Log Based Error Function
            #
            if error_func == 0:
                error = np.multiply(-target,np.log(output))-np.multiply((1-target),np.log(1-output))
                total_error = np.sum(np.sum(error))
            #    
            #Squared Error Cost Function
            #
            elif error_func == 1:
                error = (target - output)**2
                total_error = 0.5 * np.sum(np.sum(error))
                
            return total_error
        
        def Weight_Grad(self,input,target,output_list):
            
                    #Finds the Deltas for Each Layer
                    # 
                    deltas = []
                    for i in range(self.__layers - 1):
                        #Finds Error Delta for the last layer
                        if i == 0:
                            
                            error = (target-output_list[-1])
                            
                            error_delta = -1*np.multiply(error,np.multiply(output_list[-1],(1-output_list[-1])))
                            deltas.append(error_delta)
                        #Finds Error Delta for the hidden layers   
                        else:
                            #Weight matrices have bias values removed
                            error_delta = np.multiply(np.dot(deltas[-1],self.__weights[-i][:,1:]),output_list[-i-1]*(1-output_list[-i-1]))
                            deltas.append(error_delta)
                            
                    #
                    #Finds the Deltas for each Weight Matrix
                    #
                    Weight_Delta_List = []
                    deltas.reverse()
                    for i in range(len(self.__weights)):
                         
                        current_weight_delta = (1/self.__input_cases) * np.dot(deltas[i].T,np.concatenate((np.ones((self.__input_cases,1)),output_list[i]),1))
                        Weight_Delta_List.append(current_weight_delta)
                        #print(""Weight"",i,""Delta:"",""\n"",current_weight_delta)
                        #print()
                         
                    #
                    #Combines all Weight Deltas into a single row vector
                    #
                    Weight_Delta_Vector = np.array([[]])
                    for i in Weight_Delta_List:
                        
                        Weight_Delta_Vector = np.concatenate((Weight_Delta_Vector,np.reshape(i,(1,-1))),1)
                    return Weight_Delta_List        
            
        def Train(self,input_data,target):
            #
            #Gradient Checking:
            #
            
            #First Get Gradients from first iteration of Back Propagation 
            output_list = self.Feedforward(input_data,Train=True)
            self.__input_cases = np.shape(input_data)[0]
            
            Weight_Delta_List = self.Weight_Grad(input_data,target,output_list)  
            
            #Creates List of Gradient Approx arrays set to zero.
            grad_approx_list = []
            for i in self.__weight_sizes:
                current_grad_approx = np.zeros(i)
                grad_approx_list.append(current_grad_approx)
            
            
            #Compute Approx. Gradient for every Weight Change
            for W in range(len(self.__weights)):
                for index,value in np.ndenumerate(self.__weights[W]):
                    orig_value = self.__weights[W][index]      #Saves the Original Value
                    print(""Orig Value:"", orig_value)
                    
                    #Sets weight to  weight +/- epsilon
                    self.__weights[W][index] = orig_value+.00001
                    cost_plusE = self.CostFunction(input_data, target)
                    
                    self.__weights[W][index] = orig_value-.00001
                    cost_minusE = self.CostFunction(input_data, target)
                    
                    #Solves for grad approx:
                    grad_approx = (cost_plusE-cost_minusE)/(2*.00001)
                    grad_approx_list[W][index] = grad_approx
                    
                    #Sets Weight Value back to its original value
                    self.__weights[W][index] = orig_value
            
               
            #
            #Print Gradients from Back Prop. and Grad Approx. side-by-side:
            #
            
            print(""Back Prop. Grad"",""\t"",""Grad. Approx"")
            print(""-""*15,""\t"",""-""*15)
            for W in range(len(self.__weights)):
                for index, value in np.ndenumerate(self.__weights[W]):
                    print(self.__weights[W][index],""\t""*3,grad_approx_list[W][index])
            
            print(""\n""*3)
            input_ = input(""Press Enter to continue:"")
           
            
            #
            #Perform Weight Updates for X number of Iterations
            #
            for i in range(10000):
            #Run the network
                output_list = self.Feedforward(input_data,Train=True)
                self.__input_cases = np.shape(input_data)[0]
                
                Weight_Delta_List = self.Weight_Grad(input_data,target,output_list)
                           
                    
                for w in range(len(self.__weights)):
                    #print(self.__weights[w])
                    #print(Weight_Delta_List[w])
                    self.__weights[w] = self.__weights[w] - (.01*Weight_Delta_List[w]) 
            
                   
            print(""Done"")

I even implememented Gradient Checking and the values are different, and I thought I would try replacing the Back Propagation updates with the Approx. Gradient Checking values, but that gave the same results, causing me to doubt even my Gradient Checking code. 

Here are some of the values being produced when training for the XOR Gate:
Back Prop. Grad: 0.0756102610697 	0.261814503398 	     0.0292734023876 			

Grad Approx:      0.05302210631166       0.0416095559674      0.0246847342122 

Cost:
Before Training: 0.508019225507
After Training 0.50007095103 (After 10000 Epochs)

Output for 4 different examples(after training):

[ 0.49317733]
 [ 0.49294556]
 [ 0.50489004]
 [ 0.50465824]

So my question is, is there any obvious problem with my Back Propagation, or my gradient checking? Are there any usual problems when a ANN shows these symptoms(Outputs are all roughly the same/Cost is going down)? ",1,0
300,2014-12-25,2014,12,25,1,2qabc3,Regression without negative examples,https://www.reddit.com/r/MachineLearning/comments/2qabc3/regression_without_negative_examples/,efavdb,1419437369,,0,4
301,2014-12-25,2014,12,25,4,2qayxv,Machine Learning Trends from NIPS 2014,https://www.reddit.com/r/MachineLearning/comments/2qayxv/machine_learning_trends_from_nips_2014/,mttd,1419450559,,0,56
302,2014-12-25,2014,12,25,8,2qbht2,The wonderful and terrifying implications of computers that can learn,https://www.reddit.com/r/MachineLearning/comments/2qbht2/the_wonderful_and_terrifying_implications_of/,pateras,1419462124,,0,4
303,2014-12-25,2014,12,25,10,2qbv4z,Wash car machine --32kw Industrial steam cleaner,https://www.reddit.com/r/MachineLearning/comments/2qbv4z/wash_car_machine_32kw_industrial_steam_cleaner/,zzaixcarwash,1419471518,,2,0
304,2014-12-25,2014,12,25,15,2qcj69,Roll Forming Machine,https://www.reddit.com/r/MachineLearning/comments/2qcj69/roll_forming_machine/,rollformingmachinery,1419490744,,1,0
305,2014-12-26,2014,12,26,2,2qdjz8,Machine Learning With Statistical And Causal Methods,https://www.reddit.com/r/MachineLearning/comments/2qdjz8/machine_learning_with_statistical_and_causal/,alexeyr,1419529297,,0,17
306,2014-12-26,2014,12,26,6,2qe3qa,Question about what computer to buy if I'm going to do a project in machine learning,https://www.reddit.com/r/MachineLearning/comments/2qe3qa/question_about_what_computer_to_buy_if_im_going/,[deleted],1419542222,"I'm halfway into Andrew ng coursera course ( currently watching previous videos . I started late so I'll be getting the SOA by joining the next batch) 

I'm hoping that after doing this course and reading some more , I'll be in a position to start attempting kaggle challenges ? Or be in a position to make a publishable paper ( nothing grand ) in a local conference. 

I think I'll start with deep learning after finishing the coursera programme. 

Now my noob question is , can I accomplish the above goals with a normal laptop ? 

I don't have access to any labs and I cannot pay dollars for any cloud services. So please advice me which areas from machine learning I can focus on with my limited hardware.

Also be brutally honest if I'm being over ambitious in thinking that I will be in any position to start solving problems after a coursera programme. 

Thank you",36,5
307,2014-12-26,2014,12,26,12,2qey2p,Regression problems with extremely right skewed targets,https://www.reddit.com/r/MachineLearning/comments/2qey2p/regression_problems_with_extremely_right_skewed/,AWKWARD_HANDS_GUY,1419563661,"Hey /r/MachineLearning ,

I'm starting a project soon that will require some supervised machine learning. the target variable is severely right skewed. The target looks like it almost follows a Poisson distribution, even though its not count data. My features will be almost 100% categorical. 

This is the first time I've really worked on a problem such as this. My background is in basic econometrics where assumptions of normally distributed everything is really never violated. 

Are there any sources out there for techniques or algorithms that work particularly well in a case such as mine? ",8,3
308,2014-12-26,2014,12,26,15,2qfd1c,"How to use AdaBoost for more ""complex"" classifications?",https://www.reddit.com/r/MachineLearning/comments/2qfd1c/how_to_use_adaboost_for_more_complex/,odkken,1419574658,"In [this](http://www.wired.com/2014/01/how-to-hack-okcupid/all/) article, Chris McKinlay says he used AdaBoost to choose the proper ""importances"" of questions he answered on okcupid.

If you haven't read and don't want to read the article, or are unfamiliar with okcupid and the question system, here's the data and problem he had:

The goal is to ""match"" as highly as possible with as many users as possible, each of whom may have answered an arbitrary number of questions.  These questions may have between 2 and 4 answers each, and for the sake of simplicity, let's pretend that the formula for a match% between you and another user is the sum of your matching answers, divided by the total number of matching questions (denominator = # of questions you have in common, numerator = # of those questions you answered the same way).  The real formula is slightly more complex, but the approach would be the same regarding ""picking"" a correct answer (he actually used boosting to find the ideal ""importance"" to place on a given question, rather than the right answer).

In any case, the point is you want to pick a certain value for each question, such that you maximize your match% with as many users as possible - something you might quantify by the sum of match% over all users.

Now I've watched the [MIT course on AI](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi) up to and including the lecture on boosting, but I don't understand how you would apply it to a problem like this.  Honestly I don't even know where to begin with choosing rules for the weak learners.  I don't have any ""rules"" about what values to choose for each question (if the user is under 5'5, choose A, etc) - I'm just trying to fit the data I have.

Is this not the way boosting is supposed to be used?  Is there likely some other optimization left out of how he figured this out?",4,6
309,2014-12-26,2014,12,26,17,2qfmfv,Regrinding and fine grinding equipment-tower mill,https://www.reddit.com/r/MachineLearning/comments/2qfmfv/regrinding_and_fine_grinding_equipmenttower_mill/,hnbljq,1419583576,,0,0
310,2014-12-27,2014,12,27,0,2qg77o,Videos from Convex Optimization @CMU,https://www.reddit.com/r/MachineLearning/comments/2qg77o/videos_from_convex_optimization_cmu/,matiskay,1419607094,,5,39
311,2014-12-27,2014,12,27,4,2qgrqd,"Data Elixir, Issue 15: machine learning in the real-world, how to trick a neural net, data science resources, best data viz tools of 2014",https://www.reddit.com/r/MachineLearning/comments/2qgrqd/data_elixir_issue_15_machine_learning_in_the/,lonriesberg,1419620774,,1,0
312,2014-12-27,2014,12,27,4,2qgtfu,"64 new external resources and articles about machine learning, data science, and big data",https://www.reddit.com/r/MachineLearning/comments/2qgtfu/64_new_external_resources_and_articles_about/,vincentg64,1419621821,http://www.datascienceworld.com/profiles/blogs/55-new-external-resources-and-articles-about-data-science-big,0,0
313,2014-12-27,2014,12,27,7,2qhcgc,Jeremy Howard TED talk: The wonderful and terrifying implications of computers that can learn,https://www.reddit.com/r/MachineLearning/comments/2qhcgc/jeremy_howard_ted_talk_the_wonderful_and/,whyoy,1419633561,,2,0
314,2014-12-27,2014,12,27,10,2qhtze,Algebraic approach to classification?,https://www.reddit.com/r/MachineLearning/comments/2qhtze/algebraic_approach_to_classification/,[deleted],1419645155,"So I was reading [this interview with a Kaggle winner](http://blog.kaggle.com/2014/07/28/11th-hour-win-of-greek-media-monitoring-challenge/). This chap is a Russian professor who used an approach based on what he calls ""the algebraic approach to classification"".    
&gt;My algorithm consisted of two parts: linear combinations of regressors for each label and a binary decision rule. Such algorithms are very popular in Russia, for example in the algebraic approach to classification. This technique had been developing by academician Yuri Zhuravlev and his scientific school since 1978 and is unknown in Europe and USA.   
     
The bloke he's talking about is a Russian mathematician named [Yuri Zhuravlev](https://en.wikipedia.org/wiki/Yuri_Zhuravlev). I've tried to look this up, but I tend to run into really obscure mathematical papers that were seemingly translated from Russian using Google translate so haven't mustered the courage to take them on yet. Does anyone know what he's talking about here?",7,38
315,2014-12-27,2014,12,27,16,2qilk3,Has anyone worked on a machine learning project and towards the end realised that there was a better way ? What do you usually do in such situations ?,https://www.reddit.com/r/MachineLearning/comments/2qilk3/has_anyone_worked_on_a_machine_learning_project/,[deleted],1419665453,"For example you realised that your choice of classifier was not the best , you could have chosen a better way for dimensionality reduction. Just wanted to hear some stories.

I'm looking to start a project and since I'm new to this field , I'm mostly looking to solve existing problems with a better approach. 

",17,6
316,2014-12-27,2014,12,27,17,2qipux,"A fully connected, recurrent autoencoder; or: ""I connected everything, and it worked!""",https://www.reddit.com/r/MachineLearning/comments/2qipux/a_fully_connected_recurrent_autoencoder_or_i/,[deleted],1419670028,,0,1
317,2014-12-28,2014,12,28,1,2qjf0z,A First Kit of Machine Learning Resources,https://www.reddit.com/r/MachineLearning/comments/2qjf0z/a_first_kit_of_machine_learning_resources/,[deleted],1419697073,"The Machine Learning Salon Kit contains 150 pages of useful websites to start machine learning. It's free from any comment, registration or advertising.
[MachineLearningSalonKit](http://www.machinelearningsalon.org/download.html)

The Machine Learning Salon is collecting worldwide information from websites such as Reddit, DataTau, LinkedIn, Google Scolar, University websites, ... and sometimes we are posting new information too (PyData NYC 2014, Royal Society, etc.).",5,22
318,2014-12-28,2014,12,28,2,2qjl7k,FTRL proximal questions (theory and implementation),https://www.reddit.com/r/MachineLearning/comments/2qjl7k/ftrl_proximal_questions_theory_and_implementation/,[deleted],1419701233,"Hi,  
I've been trying to implement FTRL proximal for the avuzu competition(ads click rate prediction) on Kaggle. However I'm a begginer in ML and I'm unsure about some aspects of this algorithm.  
  
&amp;nbsp;

I've been using this [paper](http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf).  
  
&amp;nbsp;

Am I justified in using FTRL proximal instead of simple online gradient descent in this particular contest considering there are only about 30 features? From what I've understood the advantage over OGD is that it introduces sparsity and reduces memory usage when storing the weights. But since there are only 30 features and we are only storing 2 weight vectors at any time in OGD (unless I'm mistaken about that part) why go through the trouble of using FTRL?  
Am I missing another advantage or did I misunderstand something?  
   
&amp;nbsp;

On a side note the paper talks about feature vectors with billions of coefficient and that really boggles my mind. How do they even find that many features in the behavior of online users... even using high polynomial features...  
  
&amp;nbsp;

My second problem is the implementation itself, there is a nice one in the kaggle forum but I figured I'd try to do my own before looking at it. Here is [mine](https://github.com/EtienneDesticourt/Kaggle-Avuzu/blob/master/FTRLProx.py), lines 50 to 80 is ftrl proximal.  
I can't seem to pinpoint the problem, I've tried with up to 100 000 training examples, many different parameters( alpha, beta, l1, l2), with as few as one feature but nothing seems to make it converge and I have an accuracy of 0.5 whereas simple batch logistic regression gets ~0.94. So all I can think of is an error in my implementation. I've copied the Algorithm 1 from the paper page 2 but before (and after) doing so I've read multiple times the theory(page 2-3) to try and understand what I was doing, yet it still doesn't work.
I wish I could pinpoint more precisely my problem before asking but I just don't know what's wrong, I'm sorry.  
  
&amp;nbsp;

I'd be very grateful for any help answering these two questions.

&amp;nbsp;

Cheers

",0,1
319,2014-12-28,2014,12,28,3,2qjtck,Thesis: Quantum Algorithms for Linear Algebra and Machine Learning; Anupam Prakash (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/2qjtck/thesis_quantum_algorithms_for_linear_algebra_and/,compsens,1419706429,,2,8
320,2014-12-28,2014,12,28,4,2qjwf7,Adaboost on svm,https://www.reddit.com/r/MachineLearning/comments/2qjwf7/adaboost_on_svm/,nukich74,1419708308,"Adaboost is recommened to use with weak classifiers. I also heard, that it is not good idea to use it with SVM. Looking on algo i cant guess why it is bad idea. Can anyone explain?",7,2
321,2014-12-28,2014,12,28,8,2qkjd5,How to cluster text sentences unsupervised?,https://www.reddit.com/r/MachineLearning/comments/2qkjd5/how_to_cluster_text_sentences_unsupervised/,desegel,1419722332,"Hi everyone.

I need to take about 200k sentences and cluster them to groups based on text similarity.  
I don't want to specify a constant number of clusters - I want it to just figure out groups based on a ""tolerance"" variable that i could play with.

I read about word2vec/doc2vec that pays attention to the context of the words and not just as a random bag of words and I think it could help me, but i couldn't figure out how to use it for clustering purposes.

Any help is welcome, thanks !",7,5
322,2014-12-28,2014,12,28,14,2qlhu6,"#Future #MIT #CarnegieMellon #MachineLearning #AI #2015 Eric Miley, #MBA ""On Machine Learning and the Next Wave of Innovation""",https://www.reddit.com/r/MachineLearning/comments/2qlhu6/future_mit_carnegiemellon_machinelearning_ai_2015/,[deleted],1419745715,,1,0
323,2014-12-29,2014,12,29,0,2qmdoj,WEKA question on Naive Bayes classifiers,https://www.reddit.com/r/MachineLearning/comments/2qmdoj/weka_question_on_naive_bayes_classifiers/,[deleted],1419780483,x,6,1
324,2014-12-29,2014,12,29,1,2qmh6a,"The cheat sheet for MATLAB, Python NumPy, R, and Julia",https://www.reddit.com/r/MachineLearning/comments/2qmh6a/the_cheat_sheet_for_matlab_python_numpy_r_and/,soulslicer0,1419783162,,8,124
325,2014-12-29,2014,12,29,3,2qmvt6,"""On Machine Learning and the Next Wave of Innovation"" - Eric Miley, Carnegie Mellon University",https://www.reddit.com/r/MachineLearning/comments/2qmvt6/on_machine_learning_and_the_next_wave_of/,ericmileymba,1419792645,,0,1
326,2014-12-29,2014,12,29,4,2qmybu,Do you need an academic background for machine learning and data science?,https://www.reddit.com/r/MachineLearning/comments/2qmybu/do_you_need_an_academic_background_for_machine/,charlesbukowksi,1419794173,"If you're a self-taught programmer and avid autodidact, is machine learning or a subset of it (like neural networks) something you could develop expertise in, compared to say mobile programming, or is it something that would be best with an academic background and connections?  I'm speaking with a mind to study it and potentially use it entrepreneurially.

What kind of math background do you need, or does it depend on the specific field?  I never studied advanced maths like multivariate calculus or linear algebra but I would be willing to learn.",27,4
327,2014-12-29,2014,12,29,4,2qn0m5,Machine learning-based NBA predictions visualized,https://www.reddit.com/r/MachineLearning/comments/2qn0m5/machine_learningbased_nba_predictions_visualized/,efavdb,1419795477,,1,1
328,2014-12-29,2014,12,29,7,2qnla2,"Big Data, Machine Learning, and the Social Sciences: Fairness, Accountability, and Transparency",https://www.reddit.com/r/MachineLearning/comments/2qnla2/big_data_machine_learning_and_the_social_sciences/,onewugtwowugs,1419807186,,0,0
329,2014-12-29,2014,12,29,11,2qo93i,Stochastic Gradient Boosting: Choosing the Best Number of Iterations,https://www.reddit.com/r/MachineLearning/comments/2qo93i/stochastic_gradient_boosting_choosing_the_best/,yanirse,1419821076,,0,8
330,2014-12-29,2014,12,29,13,2qojmx,15 Biggest Unsolved Mysteries in the World,https://www.reddit.com/r/MachineLearning/comments/2qojmx/15_biggest_unsolved_mysteries_in_the_world/,JosephCox123,1419827573,,0,1
331,2014-12-29,2014,12,29,15,2qowgz,Packaging Machine,https://www.reddit.com/r/MachineLearning/comments/2qowgz/packaging_machine/,ramylemon,1419836189,,0,1
332,2014-12-29,2014,12,29,17,2qp2rh,With wax/foam/water cleaning! Car wash equipment--Self service car wash,https://www.reddit.com/r/MachineLearning/comments/2qp2rh/with_waxfoamwater_cleaning_car_wash_equipmentself/,zzaixcarwash,1419841775,,0,1
333,2014-12-29,2014,12,29,19,2qp9l4,Slitting Machine,https://www.reddit.com/r/MachineLearning/comments/2qp9l4/slitting_machine/,ramylemon,1419849084,,0,0
334,2014-12-29,2014,12,29,19,2qpaph,Awesome CS courses,https://www.reddit.com/r/MachineLearning/comments/2qpaph/awesome_cs_courses/,psamarj,1419850293,,0,14
335,2014-12-29,2014,12,29,19,2qpaue,Latent Dirichlet allocation - how am I supposed to pronounce it?,https://www.reddit.com/r/MachineLearning/comments/2qpaue/latent_dirichlet_allocation_how_am_i_supposed_to/,fawkesdotbe,1419850469,"Dear all,

I am currently busy with Topic Modeling and thus use the term Latent Dirichlet allocation (LDA) quite often. Not being a native English speaker, I am not sure how to pronounce the ""Dirichlet"" part. Could a native English speaker use [Vocaroo](http://vocaroo.com/) (or other) to show it to me? 

I am aware the name actually comes from [Mr Dirichlet](http://en.wikipedia.org/wiki/Peter_Gustav_Lejeune_Dirichlet), a German scientist, and that the name should be pronounced as such -- but I'd like to have a US/UK version of the pronunciation too.

Thanks!
",7,0
336,2014-12-29,2014,12,29,21,2qpf9x,Why is the natural gradient not used more in machine learning?,https://www.reddit.com/r/MachineLearning/comments/2qpf9x/why_is_the_natural_gradient_not_used_more_in/,remington_steele,1419855445,"I am reading the Paisley et. al. [Stochastic Variational Inference](http://arxiv.org/abs/1206.7051) paper and saw the concept of a natural gradient for the first time. For the uninitiated, it does gradient descent using a more intrinsic  distance for probability distributions (the symmetrized KL divergence). It makes sense intuitively that this would speed up stochastic optimization, and results confirm it. So I guess my question is, why is this not more popular for doing things like ordinary least-squares regression with SGD? What are the drawbacks to using it? Is the primary impediment the calculation of the Fisher info matrix?",10,38
337,2014-12-29,2014,12,29,21,2qpfux,From Computation to Consciousness [31c3],https://www.reddit.com/r/MachineLearning/comments/2qpfux/from_computation_to_consciousness_31c3/,vrld,1419856130,,1,3
338,2014-12-29,2014,12,29,21,2qpg6h,Ask an expert: how soon until human-level A.I.,https://www.reddit.com/r/MachineLearning/comments/2qpg6h/ask_an_expert_how_soon_until_humanlevel_ai/,maccl,1419856449,"I know this is an impossible question, but with a group of experts, maybe the mean guess will actually be close to the truth.

First question: what are some important aspects of human cognition that we cannot replicate to some degree. For example, I would say the following can be replicated: sensory processing (deep learning), 
memory (comptuers are very good at memory), searching for information, ect, and you could also say that computers are good at stuff like planning, since a computer can combinatorically plan.  So what specific ""tasks"" can you name that a human brain can do, but we have no idea how to do it in a computer?  I would say ""understanding"" or even feeling or emotion are things that would be at the top of my list.

Second question: How many years do you think it will be until we we have a computer system that can 
perform all the functions of a human brain?  I would guess 10 years.",11,0
339,2014-12-29,2014,12,29,23,2qpljv,Anyone familiar with the minimum description length (MDL) principle? Which codes to choose for which data structures?,https://www.reddit.com/r/MachineLearning/comments/2qpljv/anyone_familiar_with_the_minimum_description/,0-n1,1419861803,"I've recently found myself quite fascinated with the minimum description length principle and I'm trying to apply it to my own area of research, which is statistical natural language modeling. It seems to me that while one is, in principle, free to choose arbitrary encoding schemes for model and data, the codes can/do introduce bias towards certain structures. Of course, the relevant structures should be reflected by the encoding scheme -- but I am not fully clear on how one should describe each data structure. I am aware that this can drastically depend on the task at hand, but I am wondering whether any of you have any experience with this and would be willing to discuss.",1,7
340,2014-12-30,2014,12,30,2,2qq8lh,Applied physics/physics to ML?,https://www.reddit.com/r/MachineLearning/comments/2qq8lh/applied_physicsphysics_to_ml/,e808,1419875760,"I'm a rising junior studying applied physics/engineering physics with a minor in computer science. I'm interested in potentially going into ML. I have a pretty strong math background from all the physics, and have experience in software development as well (worked at a software startup this last summer). I'm currently in the process of finding an ML professor to work with at my university. I wanted to know if anyone has experience made the jump from physics to machine learning? Based on my CS minor, I have and will be taking courses in algorithms, AI, etc. The applied physics degree is a combination of mostly physics courses, some core EE/Mech Eng courses, and the remaining electives for the degree are very flexible.

For the electives, I'd figure that I would focus on computational stuff- modelling and simulation. Would this provide me with a solid skillset for pursuing ML in industry or graduate school? ",4,1
341,2014-12-30,2014,12,30,3,2qqb60,Tech 2015: Deep Learning And Machine Intelligence Will Eat The World,https://www.reddit.com/r/MachineLearning/comments/2qqb60/tech_2015_deep_learning_and_machine_intelligence/,yudlejoza,1419877106,,0,0
342,2014-12-30,2014,12,30,4,2qqjur,Machine Intelligence Cracks Genetic Controls,https://www.reddit.com/r/MachineLearning/comments/2qqjur/machine_intelligence_cracks_genetic_controls/,digitron,1419881461,,5,20
343,2014-12-30,2014,12,30,5,2qqnq3,Why are humans designing neural networks?,https://www.reddit.com/r/MachineLearning/comments/2qqnq3/why_are_humans_designing_neural_networks/,magwo,1419883328,"It seems to me that a significant bottleneck in the development of ML and NNs is the idea that we as humans should be designing the neural networks - that is - to select the number of hidden layers, the number of neurons in those hidden layers, and the degree of connectivity between them.

Why are we not letting computers - algorithms or neural networks design the neural networks?

The human brain did not have its features designed, and the ability of the brain to adapt and relocate functionality suggests that it has the ability to layout/allocate/design new networks.

One apparent thing that hints at the adaptability of the human NNs is its ability to work with alphabets of varying lengths. It does not appear to be pre-programmed for an alphabet with 20 symbols or whatever - it can work with large alphabets.

Does state-of-the-art of NN research still involve humans designing networks?

Forgive me if this is a common or irrelevant question. I did a search but did not find anything apparent discussing this subject.",25,18
344,2014-12-30,2014,12,30,7,2qr47h,Diagnosing problems with autoencoders,https://www.reddit.com/r/MachineLearning/comments/2qr47h/diagnosing_problems_with_autoencoders/,paralax77,1419891565,"So I have tried a few autoencoders:

basic autoencoder, contractive autoencoder and denoising autoencoder

they all work nice when I train them on MNIST dataset. I get nice filters. The problem is, when I feed these autoencoders my images, of the same size as the mnist images, the autoencoders seem to learn one filter only. It looks like superposition of all images.

Basically, it looks like it mashed together every of my images and shows the result as each filter. Each single one of these filters is almost identical.

I've tried different configurations, changing hidden unit sizes, training epochs - by a few different orders of magnitute but the result is always the same.

Can anyone tell me why would this happen?

Can it have something to do with the size of my data? For comparison, MNIST has 50000 examples while my dataset has 500 - could this be the reason?

I would love feedback from someone experienced with autoencoders.",2,2
345,2014-12-30,2014,12,30,7,2qr8m1,Exponential-Family Harmoniums vs standard RBMs [Question],https://www.reddit.com/r/MachineLearning/comments/2qr8m1/exponentialfamily_harmoniums_vs_standard_rbms/,M_Bus,1419893789,"I'm experimenting with Restricted Boltzmann Machines to do some unsupervised learning so I can have an actual generative model of some data I'm working with. Also useful for pre-training of deep belief nets or things like that.

Anyway, a lot of the data I'm working with is real-valued data. Sometimes it's integers between, say, 1 and 20, and sometimes it's real values between 0 and 1,000,000 that are approximately exponentially distributed. I say ""real values,"" but really it's rounded to the nearest 100th. It's distributed according to a real distribution, for all intents and purposes, but it doesn't require a great deal of precision in terms of decimal points.

So I was toying with the idea of setting up a so-called ""Exponential-Family Harmonium,"" but I would need to use some kind of weird Energy formulation with mixed data types, since some of my input vector coordinates are binary values, some are binomially distributed, and some are maybe exponential or Gaussian. But it occurred to me that I could convert all of my real-valued entries into binary digits and use a standard RBM.

My question: would that actually work? That is, would converting all non-binary numbers into binary (and then just using a normal Restricted Boltzmann Machine trained with Contrastive Divergence) work? What kind of internal representation of the data would my algorithm come up with? Would it actually make sense when I run the program backwards to generate new examples of data?

Has anyone tried this? If so, how did it work out?",4,2
346,2014-12-30,2014,12,30,8,2qrbf8,What algorithms should I be looking to learn?,https://www.reddit.com/r/MachineLearning/comments/2qrbf8/what_algorithms_should_i_be_looking_to_learn/,beaverteeth92,1419895225,"I have a math major completed, some graduate statistics work under my belt, and have done a decent amount of computer science coursework, but I've forgotten a good bit of it.  Most of that work was in Java.  I'm not an amazing programmer by any means, even though I'm comfortable doing statistical analysis with generalized linear models.

What algorithms and data structures should I get really comfortable with if I want to get more comfortable with implementing machine learning algorithms (and I guess general ""data science"")?  I went through some of Andrew Ng's course but found it was more technique-oriented than programming-oriented.  I figure some kind of tree would make a lot of sense (especially for decision trees and random forests), but I don't know what algorithms tend to be used most commonly.  I also get the impression that Bloom filters are used a lot but can't find much information on implementations.

Additionally, should I try implementing them in a lower-level language?  I'm not a fan of Java, but it seems like a lot of newer good linear algebra libraries are written in C++.  I don't know C++, but would it be worth learning if I wanted to get involved in machine learning?  Also is it worth going through SICP if my eventual goal is to get comfortable enough with programming for machine learning?

**EDIT:  To be clear, I already am comfortable with Java, Matlab, Python, and R.  I'm comfortable with data analysis because of my statistics background.  Here, I'm asking specifically about implementing machine learning algorithms down to the data structures they will be used on.  Not about how to use and interpret them.**",20,7
347,2014-12-30,2014,12,30,9,2qrje1,"Did anyone here use ""no more pesky learning rates"" algorithm? need some help",https://www.reddit.com/r/MachineLearning/comments/2qrje1/did_anyone_here_use_no_more_pesky_learning_rates/,lo1201,1419899536,"I just started to learn neural nets, completed Andrew Ng course and in the middle of Geoffrey Hinton 2012 year course about neural networks on Coursera. He mentions this paper [no more pesky learning rates](http://arxiv.org/pdf/1206.1106.pdf) It's almost 3 years old now. I found only one post about it in this subreddit and it's not much there. There are some implementations on the Internet, ok. But what is hard is to find code that computes Hessian of the network. I couldn't find clear implementation of bbprop mentioned in this paper.. I found this paper [Estimating the Hessian by Back-propagating Curvature](http://arxiv.org/pdf/1206.6464v2.pdf) but I couldn't implement it so far. Should I continue trying or it's not worth it? Can someone share a link or code for bbprop or some other algorithm that can efficiently compute Hessian diagonal?

Edit: At least maybe someone knows what is this V(v_i) variable is? And all those indices, after two days now I almost understand what is happening(not how it works) but I still can't understand what is V stands for. I even made it just random valued vector, but I guess it's wrong. [Here's](http://techtalks.tv/talks/estimating-the-hessian-by-back-propagating-curvature/57305/) a short video about the second paper.",9,2
348,2014-12-30,2014,12,30,11,2qrver,C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning,https://www.reddit.com/r/MachineLearning/comments/2qrver/csvddnet_an_effective_singlelayer_network_for/,rantana,1419906297,,0,1
349,2014-12-30,2014,12,30,13,2qs8xk,Deepmind type results against humans?,https://www.reddit.com/r/MachineLearning/comments/2qs8xk/deepmind_type_results_against_humans/,rasputin48,1419914367,Deepmind famously got a computer to master atari games using only visual input.  What is the state of the art for a similar setup against humans.  Can a computer learn to beat a human at pong using visual input and the difference of scores as the objective function?,5,1
350,2014-12-30,2014,12,30,14,2qscld,LambdaNet - A functional neural network library written in Haskell,https://www.reddit.com/r/MachineLearning/comments/2qscld/lambdanet_a_functional_neural_network_library/,diamondium,1419916528,,1,7
351,2014-12-30,2014,12,30,14,2qsfmk,Sparse coding resources,https://www.reddit.com/r/MachineLearning/comments/2qsfmk/sparse_coding_resources/,chchan,1419918373,"Are there any good resources on understanding Sparse coding. Also how come there are so few people using sparse coding? is it not as good as RBMs or autoencoders?

Right now I am using videos from Hugo Larochelle
https://www.youtube.com/watch?v=7a0_iEruGoM

But I want other sources so I understand the process better.",3,2
352,2014-12-30,2014,12,30,15,2qsiae,Attention please!!!Car wash equipment---mobile car wash,https://www.reddit.com/r/MachineLearning/comments/2qsiae/attention_pleasecar_wash_equipmentmobile_car_wash/,zzaixcarwash,1419920092,,0,1
353,2014-12-30,2014,12,30,15,2qsje7,How do you initialize your neural network weights?,https://www.reddit.com/r/MachineLearning/comments/2qsje7/how_do_you_initialize_your_neural_network_weights/,rantana,1419920827,"Has anyone found any success beyond initializing weights randomly from an alpha*N(0,1) distribution?",32,27
354,2014-12-30,2014,12,30,19,2qszgg,How feasible is freelancing in machine learning?,https://www.reddit.com/r/MachineLearning/comments/2qszgg/how_feasible_is_freelancing_in_machine_learning/,[deleted],1419935289,"I'm not sure if this is an appropriate question for this subreddit - if it isn't, my apologies - but I was wondering if anyone here has any experience doing freelance machine learning work (especially remotely) and could tell me a little bit about potential pitfalls and so on. 

Some background: I'm a mathematician by training - I got my PhD a few years ago - and after a couple of postdocs I'm getting a little fed up with the academic career.

I'm currently studying to get another Master's degree, with elements of machine learning, neural networks and computational neuroscience: perhaps it's a little unnecessary, but I like to learn and could afford it and so I decided to treat myself a little. Furthermore, I'm supplementing it by MOOCs and individual study (I'm currently going through Murphy's book). My personal - and, admittedly, possibly quite flawed - impression is that, if I maintain the current pace, at the end of the next semester I should have a pretty solid (if still imperfect, obviously) understanding of the most common elements of machine learning.

Lately, I've been also thinking that the freedom and variety of freelance work might suit me better than working for a single business for the foreseeable future; and that, ideally, I would prefer to be able to work remotely. 

Is this reasonably feasible, as a longish-term objective, or am I talking nonsense? Also, am I correct in thinking that if I want to do that, I might want to *first* aim for temporary positions, and attempt to transition to full-time, freelance remote work *after* I have some experience and reputation? 

Thanks a lot!",20,23
355,2014-12-30,2014,12,30,20,2qt4lx,Breakthroughs in Artificial Intelligence from 2014 | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/2qt4lx/breakthroughs_in_artificial_intelligence_from/,mttd,1419940727,,0,7
356,2014-12-30,2014,12,30,21,2qt7zw,"A Short Review of 2014 on Nuit Blanche: On Compressive Sensing, Machine Learning, Advanced Matrix Factorization and many beautiful things.",https://www.reddit.com/r/MachineLearning/comments/2qt7zw/a_short_review_of_2014_on_nuit_blanche_on/,[deleted],1419944221,,0,1
357,2014-12-30,2014,12,30,21,2qt81z,"A Short Review of 2014 on Nuit Blanche: On Compressive Sensing, Machine Learning, Advanced Matrix Factorization and many beautiful things (x-post r/CompressiveSensing)",https://www.reddit.com/r/MachineLearning/comments/2qt81z/a_short_review_of_2014_on_nuit_blanche_on/,compsens,1419944273,,0,4
358,2014-12-30,2014,12,30,22,2qt86u,"How to model this ""un predicatability"" problem?",https://www.reddit.com/r/MachineLearning/comments/2qt86u/how_to_model_this_un_predicatability_problem/,sashankdvk,1419944410,,1,0
359,2014-12-31,2014,12,31,0,2qtk28,Statistical Learning Theory Lecture Notes,https://www.reddit.com/r/MachineLearning/comments/2qtk28/statistical_learning_theory_lecture_notes/,mmahesh,1419953457,,3,25
360,2014-12-31,2014,12,31,2,2qtxrq,Most important tools to learn,https://www.reddit.com/r/MachineLearning/comments/2qtxrq/most_important_tools_to_learn/,letoseldon,1419960875,"For someone who has a background in applied math and Python programming experience (incl. numpy and scipy), how would you rank the following tools for ML in terms of importance:

- Matlab
- R
- Theano 
- SKlearn
- Others?
",16,17
361,2014-12-31,2014,12,31,3,2qu3bi,Readers Choice  10 Most Popular Microsoft Machine Learning Blog Posts of 2014,https://www.reddit.com/r/MachineLearning/comments/2qu3bi/readers_choice_10_most_popular_microsoft_machine/,MLBlogTeam,1419963678,,0,0
362,2014-12-31,2014,12,31,10,2qveuz,A minibatch learning (using SGD) 101,https://www.reddit.com/r/MachineLearning/comments/2qveuz/a_minibatch_learning_using_sgd_101/,[deleted],1419987963,,4,1
363,2014-12-31,2014,12,31,16,2qwilv,bias term in online SGD,https://www.reddit.com/r/MachineLearning/comments/2qwilv/bias_term_in_online_sgd/,jdxyw,1420012632,"In this paper https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0CCcQFjAB&amp;url=http%3A%2F%2Fresearch.google.com%2Fpubs%2Fpub41159.html&amp;ei=cqujVKaDA9LV8gWGm4CwBg&amp;usg=AFQjCNHeQ_C152a89Rax1FozSDxnT8W1_w Google present a online SGD algorithm. I try to implement it and find that the bias term is not as usual as the bias term which generated by LBFGS. In my opinio, the bias term should represent the average output of a LR model. However, the bias term LR model trained by the online SGD is very different from that trained by LBFGS. Anyone knows it?",6,0
364,2014-12-31,2014,12,31,23,2qx56v,Why does Deep Learning work? - A perspective from Group Theory,https://www.reddit.com/r/MachineLearning/comments/2qx56v/why_does_deep_learning_work_a_perspective_from/,vkhuc,1420035753,,0,1
