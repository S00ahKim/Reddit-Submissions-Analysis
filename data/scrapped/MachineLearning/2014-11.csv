,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2014-11-1,2014,11,1,11,2ky0q7,self.MachineLearning,Parallelize Main Process in RapidMiner?,https://www.reddit.com/r/MachineLearning/comments/2ky0q7/parallelize_main_process_in_rapidminer/,srt19170,1414807523,"I have the kind of odd situation that one some machines where I have RapidMiner installed I have the option to ""Parallelize the main process"" and on others I don't.  In all cases I have 5.2.008 installed, and I'm running on Windows.  Any clues or help would be appreciated!",7,0,False,self,,,,,
1,MachineLearning,t5_2r3gv,2014-11-1,2014,11,1,11,2ky36r,channel9.msdn.com,Video: Introducing Microsoft Azure Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2ky36r/video_introducing_microsoft_azure_machine_learning/,bartczernicki,1414809361,,6,1,False,http://b.thumbs.redditmedia.com/gmheQOqGZlVM251tSBdW1ilMbf1CUpwoMhmeaxVvRMs.jpg,,,,,
2,MachineLearning,t5_2r3gv,2014-11-1,2014,11,1,13,2kydbj,self.MachineLearning,Fine-tuning on neural nets?,https://www.reddit.com/r/MachineLearning/comments/2kydbj/finetuning_on_neural_nets/,matlab484,1414817700,"For some reason I still don't understand the concept of fine tuning on neural networks. My main interest is in image classifcation, and when I read tutorials they mention that to make training easier, just use a architecture pretrained from imagenet, then just finetune using the existing architecture for your specific classification problem. Could someone explain what this means to me? Its probably really basic, I just can't get it. Thanks!",6,3,False,self,,,,,
3,MachineLearning,t5_2r3gv,2014-11-1,2014,11,1,16,2kyl9b,blog.shriphani.com,A description and implementation of metric multidimensional scaling (manifold learning + dimension reduction).,https://www.reddit.com/r/MachineLearning/comments/2kyl9b/a_description_and_implementation_of_metric/,shriphani,1414825801,,0,1,False,default,,,,,
4,MachineLearning,t5_2r3gv,2014-11-2,2014,11,2,4,2kzzpk,self.MachineLearning,DRBM Implementation,https://www.reddit.com/r/MachineLearning/comments/2kzzpk/drbm_implementation/,Evolutis,1414869338,"Hey Everyone,

I am implementing a DRBM following this paper: http://machinelearning.org/archive/icml2008/papers/601.pdf

I am stuck on p(y | h), the difficulty I am having is with dy and dy^, what would be examples of those values? I know that d represents the biases of the class units, but what exactly would dy^ be? I speculate that dy could simply be the product of d and c , where c is the desired class and I am thinking dy^ could be the sum of the products of d and all possible c.

All that aside, I have also started looking at this now: https://dslpitt.org/uai/papers/11/p463-louradour.pdf

And in this paper, p(y=ec | h) has a different form. Which would you recommend? And why are they different?

Thank you!",3,6,False,self,,,,,
5,MachineLearning,t5_2r3gv,2014-11-2,2014,11,2,6,2l0axm,self.MachineLearning,Frequency/Chord Detection in JS? [Suggested X-Post from /r/webdev],https://www.reddit.com/r/MachineLearning/comments/2l0axm/frequencychord_detection_in_js_suggested_xpost/,Ajnin123,1414875833,"Hi all,  
I was just curious if there was any good frequency detection libraries/tutorials in JS, which would be easily linked to a microphone input and be able to return the length of each subsequent frequency. Using FFT it would be really good if there was any clear tutorials for calculating multiple notes simultaneously in JS. I am a beginner, so sorry if this seems quite basic.
Thanks,  
Aj.",4,0,False,self,,,,,
6,MachineLearning,t5_2r3gv,2014-11-2,2014,11,2,9,2l0w6y,self.MachineLearning,Visualising the hyperplane from SVM?,https://www.reddit.com/r/MachineLearning/comments/2l0w6y/visualising_the_hyperplane_from_svm/,thechickenmannnnn,1414888827,"Using Matlab, I've used SVM to find the support vectors of my dataset.

To make predictions, I say: 
     p_hat = dot(alpha.*y_train, K(i, :)) + b

Where K is my Gaussian kernel matrix. It performs with 86% accuracy.

I now want to visualise the hyperplane back in the primal space (37 dimensions, and I plot each dimension individually). But I can't seem to find a way to do this after endless searching on the internet.

Any experienced MLers know what to do?",3,5,False,self,,,,,
7,MachineLearning,t5_2r3gv,2014-11-2,2014,11,2,9,2l0xkk,homes.cs.washington.edu,Deep Symmetry Networks (Pedro Domingos),https://www.reddit.com/r/MachineLearning/comments/2l0xkk/deep_symmetry_networks_pedro_domingos/,alexmlamb,1414889746,,7,15,False,default,,,,,
8,MachineLearning,t5_2r3gv,2014-11-2,2014,11,2,16,2l1szt,thinkingmachineblog.net,Jeff Hawkins on the Limitations of Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/2l1szt/jeff_hawkins_on_the_limitations_of_artificial/,slacka123,1414914347,,7,0,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
9,MachineLearning,t5_2r3gv,2014-11-2,2014,11,2,20,2l21cb,self.MachineLearning,[Question]: Semantic Text Recommendation Engine,https://www.reddit.com/r/MachineLearning/comments/2l21cb/question_semantic_text_recommendation_engine/,[deleted],1414926245,"What are the best models, Projects or research papers to check that helps building Semantic content Recommendation Engine using any approach  Text Mining/NLP/Semantic Text?",0,1,False,default,,,,,
10,MachineLearning,t5_2r3gv,2014-11-2,2014,11,2,21,2l25kx,self.MachineLearning,Question about denoising autoencoders.,https://www.reddit.com/r/MachineLearning/comments/2l25kx/question_about_denoising_autoencoders/,[deleted],1414931691,"Not sure if this sub is for questions like this but I'm at a loss.

Am I understanding this correctly: if I have a noisy signal, I cannot denoise it using a denoising autoencoder? I'm quite confused...",13,6,False,self,,,,,
11,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,0,2l2i6x,self.MachineLearning,[Question] Semantic Text Recommendation Engine,https://www.reddit.com/r/MachineLearning/comments/2l2i6x/question_semantic_text_recommendation_engine/,[deleted],1414942333,"Do you recommend any titles, models, Projects or research papers to check that helps building semantic tagging &amp; recommendation of text-based content using any approach [Text Mining/NLP/Semantic Text]?",2,0,False,default,,,,,
12,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,3,2l301k,self.MachineLearning,What is the best loss function to use when comparing discriminative predictive (probabilistic) models that apply to severely imbalanced data sets?,https://www.reddit.com/r/MachineLearning/comments/2l301k/what_is_the_best_loss_function_to_use_when/,AlexTHawk,1414953537,"Hi, 

I am try to identify the best predictive model that yields accurate probabilities of positives (the data's labels are binary) on a severely imbalanced data set.  

I have 2 main problems.

1. Different loss functions tell me different models are better. In particular I get a 2-5% better ROC-AUC loss if I go with model 1 over model 2.  However, I get a 7 to 20% better Log loss if I go with model 2 over model 1.  The RMSE is 1% better for model 2.  I realize log-loss and rmse are ""well calibrated"", however because the data is so imbalanced, I am concerned that these loss functions will be more impacted by noise in the data.  Also, I do care to some degree about how well the predictions are ordered (another reason to favor AUC).

2. I am not sure I am doing Log and RMSE loss calculations correctly.  Because the data is severely imbalanced (the ratio of positives to negatives is about 1 to 1000), the data i work with is pre-filtered before I get it to remove 99 out of every 100 negative training instances (and I can't get the removed instances later). During training time, I give all instances equal weight, but when I calculate model losses during cross validation, I give negative instances a compensatory weight of 100 and positive instances a weight of 1. My concern is that this weighting scheme is wrong for a number of reasons.  For one I am concerned it will encourage overfitting to the few negative examples that didn't get removed by downsampling.  Second, I worry that it only makes sense to evaluate a model on a test data set that uses the same weighting scheme as the training data.  

Any thoughts and advice would be greatly Appreciated.
Alex",5,4,False,self,,,,,
13,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,5,2l3ezi,self.MachineLearning,Variable-performance ML algorithms?,https://www.reddit.com/r/MachineLearning/comments/2l3ezi/variableperformance_ml_algorithms/,fuzzysingularity,1414961543,"I'm curious if there exists proven-algorithms that can perform with variable performance/accuracy. One can think of an example, where an objective function that depends on a parameter that defines the accuracy vs. speed tradeoff. i.e. At certain instances, the algorithm is required to evaluate rapidly, in which case, the accuracy of the result is affected. However, in certain other instances, the algorithm is required to evaluate with maximum accuracy, in which case, it can afford to take long enough to evaluate the result. 

I'm aware of anytime-performance algorithms, but I'm curious if there are any others that I haven't stumbled across. Seems fairly useful in a robotics/computer-vision context. 

Thanks in advance!",6,0,False,self,,,,,
14,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,7,2l3mhe,self.MachineLearning,Using discrete Fourier series to train ANN's,https://www.reddit.com/r/MachineLearning/comments/2l3mhe/using_discrete_fourier_series_to_train_anns/,Linkazzz,1414965684,"Greetings, r/MachineLearning,

I'm a self-taught non-math/cs undergrad, quite new to the field, but trying to get better at it, because it fascinates me. So I had this idea for my term paper: use EEG, transform the recordings using STFT and trying to use that as a more computation friendly representation of the data in training the network. However I ran into a problem: discrete Fourier series of a single STFT are [n by 3] vectors (where n is the number of harmonics) and as far as I know, that can't be used to train ANN's. You need one training example to be a row/column vector. Has anyone else encountered such a problem and/or has any idea how to come up with a solution, a suitable data representation, if any?

Cheers!",5,3,False,self,,,,,
15,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,10,2l46k7,self.MachineLearning,How to perform feature creation from multivariate time series data?,https://www.reddit.com/r/MachineLearning/comments/2l46k7/how_to_perform_feature_creation_from_multivariate/,gabjuasfijwee,1414977480,How could one create features for multivariate time series data in a way that the created features are time-independent? Is this possible with recurrent neural networks in some unsupervised fashion?,16,8,False,self,,,,,
16,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,11,2l4gcr,self.MachineLearning,"Suggestions for talk, tutorials, seminars to watch in a reading group?",https://www.reddit.com/r/MachineLearning/comments/2l4gcr/suggestions_for_talk_tutorials_seminars_to_watch/,discretemathematics,1414983349,"Folks,
I host a computer vision and machine learning reading group at my work place. Its mostly people presenting their own work or the group watching an interesting talk or tutorial together lasting 45 mins to an hour. I am looking for suggestions that we could watch for the upcoming week that might be interesting to an audience of mostly Masters and PhDs in machine learning related areas.",3,0,False,self,,,,,
17,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,19,2l5bmj,self.MachineLearning,[META] State of the Journal Club,https://www.reddit.com/r/MachineLearning/comments/2l5bmj/meta_state_of_the_journal_club/,BeatLeJuce,1415010671,"Hi there!

As some of you might have noticed, there were no new Journal Club posts this week. This was mainly due to myself being ill for most of last week. Sorry for not updating the threads this week.

With that said, there was a lot less participation in the club as I would've expected given previous discussions about it. While the club-posts did get a fair number of upvotes each time, there were very few (if any) people actually discussing the papers or proposing new ones (and no-one seemed to mind that we skipped last week).

So the question is whether the journal club should continue to exist in its current form. Right now it seems like the posts are merely taking up space on the front page. Is there still interest in having a journal club? And if so, in which format would you like the club to take place? ",9,9,False,self,,,,,
18,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,21,2l5i7t,blog.applysci.com,AI system mimics human short term memory,https://www.reddit.com/r/MachineLearning/comments/2l5i7t/ai_system_mimics_human_short_term_memory/,ApplySci,1415018175,,0,1,False,http://b.thumbs.redditmedia.com/Hs-XnzjYJV-_3yf5x4-twSqByCEmcrqcaP1SHQKDAQc.jpg,,,,,
19,MachineLearning,t5_2r3gv,2014-11-3,2014,11,3,22,2l5ksb,jamesmcmurray.me,An introduction to the EM algorithm,https://www.reddit.com/r/MachineLearning/comments/2l5ksb/an_introduction_to_the_em_algorithm/,[deleted],1415020566,,2,22,False,default,,,,,
20,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,1,2l624a,self.MachineLearning,Sports betting: regression to predict odds,https://www.reddit.com/r/MachineLearning/comments/2l624a/sports_betting_regression_to_predict_odds/,joaoroque,1415032042,"Hi.

I'm a hobbyist at machine learning with a limited experience and I have a question that I hope you can help me with!

I'm trying to predict the odds of a player being the winner in a tennis match. My problem is that this is not a simple supervised learning problem where you give the algorithm the desired output and tell it to train. No one knows what the correct odds are.

In my dataset I have the bookmaker odds, but my goal is to beat those odds. So I thought the best approach would be to train a estimator using a custom score function that would return the money won/lost on a bet on the cases where the predicted odds where lower than bookmaker odds. I tried it with scikit-learn but it seems that this is not possible ( http://stackoverflow.com/questions/26706314/scikit-learn-custom-score-function-needs-values-from-dataset-other-than-x-and-y ).

So my question is, what kind of algorithm should I use to predict a value that I don't know before hand and can only check how good the prediction is with a cost function?",7,1,False,self,,,,,
21,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,2,2l66dk,grigory.us,Massively Parallel Clustering in MapReduce,https://www.reddit.com/r/MachineLearning/comments/2l66dk/massively_parallel_clustering_in_mapreduce/,yaroslavtsev,1415034234,,0,4,False,http://b.thumbs.redditmedia.com/vU6VBDZyEcTWswGX61jUYXwUye4QQH6NZzQfBG7mphI.jpg,,,,,
22,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,3,2l6es7,nuit-blanche.blogspot.dk,Convex Optimization in Julia (x-post with r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/2l6es7/convex_optimization_in_julia_xpost_with/,compsens,1415038553,,6,22,False,http://b.thumbs.redditmedia.com/CSQZTiyeEnl1tsIZi5000-sJoK9a_1qeZfLx-Kl2BUQ.jpg,,,,,
23,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,4,2l6loi,yseam.com,Cool post on word2vec for skill similarity,https://www.reddit.com/r/MachineLearning/comments/2l6loi/cool_post_on_word2vec_for_skill_similarity/,kindofbutnotreally,1415041938,,0,7,False,default,,,,,
24,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,4,2l6mm7,self.MachineLearning,Learning multiple tasks using Manifold regularization: Question about the paper.,https://www.reddit.com/r/MachineLearning/comments/2l6mm7/learning_multiple_tasks_using_manifold/,speechMachine,1415042391,"I was reading the paper ""Learning multiple tasks using Manifold regularization""(http://www.umiacs.umd.edu/~arvinda/mysite/papers/nipsMTL.pdf). This paper appeared in NIPS 2010. I was wondering if someone in the community has already read this. I tried to read the background reference paper, ""Dimensionality reduction using Kernel Map Methods"" (http://www.cs.utah.edu/~sgerber/research/kmm_iccv09.pdf) to gain some intuition.

Specifically I am trying to justify (to myself) how the gradient of the projection distance reduces to the more simplified form from Equation 8 to Equation 9 in Section 3.1. Any suggestions in the geometric interpretations would be helpful.",1,2,False,self,,,,,
25,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,4,2l6oul,self.MachineLearning,Collecting data for a music recommendation system class project,https://www.reddit.com/r/MachineLearning/comments/2l6oul/collecting_data_for_a_music_recommendation_system/,trank_,1415043466,"Hello,

I'm taking a class that's related to machine learning and Big Data, and the project is a music recommendation system. Part of the assignment is collecting some data, and I was hoping I'd be able to do that here. I looked in /r/datasets, but this is particular to the class for demonstration--not too many variables and not very detailed.

Here is the Google forms survey I created: http://goo.gl/forms/HUKhNhGPnk

Sorry for the grammar/ambiguity issues if you find any, and just answer as best as possible. I didn't want to deviate from what was provided by the class.

Thanks!",2,0,False,self,,,,,
26,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,4,2l6oz1,bitbootcamp.com,New Data Science School - NYC,https://www.reddit.com/r/MachineLearning/comments/2l6oz1/new_data_science_school_nyc/,[deleted],1415043524,,3,0,False,default,,,,,
27,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,6,2l735d,self.MachineLearning,Feature Similarity Measures,https://www.reddit.com/r/MachineLearning/comments/2l735d/feature_similarity_measures/,warriortux,1415050559,"Hello,

I have a question to ask you guys. Its regarding similarity measures that can be used to find similar features.

The features are text based. e.g.

    sample 1-&gt; word1 word2 word3 word4....

    sample 2-&gt; word2 word1 word4 word5....

    sample 3-&gt; word2 word1 word4 word5....


As we can see, the similarity should be in terms of the words and **their order is very important**. From the example, sample-2 and sample-3 are the most similar because of the order of the words. Any suggestions are much appreciated.

Is Cosine similarity good enough?",4,0,False,self,,,,,
28,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,6,2l74gk,self.MachineLearning,We probably all know that computers could generate an awesome music... But today I come upon something that blew my mind:,https://www.reddit.com/r/MachineLearning/comments/2l74gk/we_probably_all_know_that_computers_could/,test3545,1415051200,"Ok, it could be nothing - mine Korean is infinitely worse than mine Chinese. Zero phrases vs ~100. Anyhow here is something called *""new virtual singer""*. And quick Google search reveals that there are indeed ""virtual singers"" out there. 

Could anyone shed more light on this - is it what I dream it is, something computer generated? Or I'm just making a grave mistake that would affect mine hard earned karma points? :( 

Thanx in advance! :)



**Main link:** http://youtu.be/N3itdmpY4tk

**Awesome music** we all know about: http://youtu.be/7Pq-S557XQU

And I totally admit, I'm personally a huge fan of **K-pop:** http://youtu.be/3pYziw7QGOI",1,0,False,self,,,,,
29,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,10,2l7twt,danielphadley.com,"Sorry if it's too basic for this sub, but this was my first application of ML for text analysis (of Gone Girl)",https://www.reddit.com/r/MachineLearning/comments/2l7twt/sorry_if_its_too_basic_for_this_sub_but_this_was/,oreo_fanboy,1415064589,,4,35,False,http://b.thumbs.redditmedia.com/DrVyHKVa9xKNa0u3AgZDjLSzzL1LhjMChnMXfbqUO7c.jpg,,,,,
30,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,12,2l86sk,mathway.com,Math Simplified,https://www.reddit.com/r/MachineLearning/comments/2l86sk/math_simplified/,[deleted],1415071840,,1,0,False,default,,,,,
31,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,12,2l87k7,blog.shriphani.com,Powerful Ideas in Manifold Learning,https://www.reddit.com/r/MachineLearning/comments/2l87k7/powerful_ideas_in_manifold_learning/,shriphani,1415072280,,0,1,False,default,,,,,
32,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,17,2l8vac,rt.com,DeepMind Technologies (Google) builds a so called Neural Turing Machine (arXiv paper link inside),https://www.reddit.com/r/MachineLearning/comments/2l8vac/deepmind_technologies_google_builds_a_so_called/,petrux,1415090977,,1,0,False,http://b.thumbs.redditmedia.com/EhuHfwUpF44OeOpv2HkisRb9Rvf8jtM9jAgF4F8prPU.jpg,,,,,
33,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,20,2l94vw,github.com,Sentence2vec: an implementation of Paragraph Vector,https://www.reddit.com/r/MachineLearning/comments/2l94vw/sentence2vec_an_implementation_of_paragraph_vector/,galapag0,1415101589,,7,29,False,http://b.thumbs.redditmedia.com/yWU1JhFkBJl9jCePjj0AhrPtokzciHmSCQCa5P7_XiE.jpg,,,,,
34,MachineLearning,t5_2r3gv,2014-11-4,2014,11,4,21,2l95pc,self.MachineLearning,What do think about Ray Kurzweil's predictions regarding AI?,https://www.reddit.com/r/MachineLearning/comments/2l95pc/what_do_think_about_ray_kurzweils_predictions/,[deleted],1415102456,,9,3,False,default,,,,,
35,MachineLearning,t5_2r3gv,2014-11-5,2014,11,5,5,2laq2l,gigaom.com,Applications are driving investment in deep learning startups,https://www.reddit.com/r/MachineLearning/comments/2laq2l/applications_are_driving_investment_in_deep/,agrman,1415134310,,2,0,False,http://a.thumbs.redditmedia.com/AJjDFXPASy9n-zZrYNkAnYCDMA-xfjoZaj1hAI9uT24.jpg,,,,,
36,MachineLearning,t5_2r3gv,2014-11-5,2014,11,5,6,2lav6t,self.MachineLearning,O(N) approximate connected graph components,https://www.reddit.com/r/MachineLearning/comments/2lav6t/on_approximate_connected_graph_components/,alexmlamb,1415136938,"So I have a problem where I have a graph with N nodes and a distance function defined over any two pairs of nodes (x,y).  Computing this distance function is reasonably expensive.  Note that I do not have a vector representation for each node, I only have a distance function.  

My interest is in getting something like normalized graph cuts.  I.e. I want a way of cutting my graph into roughly connected components.  

One strategy that I have tried is picking a cutoff k and then doing a search for connected components.  In practice this scales in close to linear time, as it quickly puts many of the nodes into a few clusters and they don't need to be searched multiple times (so in practice I only need to compute a tiny fraction of the whole distance matrix over all nodes).  The results are also reasonably good for my task, but it is brittle because of the hard threshold.  

Alternatively, I've tried doing spectral clustering and DBSCAN.  These also work well and are much less britle than a hard search.  However they require me to compute the full distance matrix, which is prohibitively expensive.  

One way I could do it is by first running a search for connected components at threshold k1.  Then try to merge the clusters using a different (and lower) threshold k2, but requiring at least two matches to merge the clusters.  

Does anyone have any ideas on better approaches for this problem?  ",5,2,False,self,,,,,
37,MachineLearning,t5_2r3gv,2014-11-5,2014,11,5,6,2lawdd,youtube.com,Neural Networks Demystified [Part 1: Data and Architecture],https://www.reddit.com/r/MachineLearning/comments/2lawdd/neural_networks_demystified_part_1_data_and/,enigmakthx,1415137553,,16,102,False,http://b.thumbs.redditmedia.com/rhIuuA4JwABKOYacYH7uwIM-07JJE7GFhF8YjYofp4c.jpg,,,,,
38,MachineLearning,t5_2r3gv,2014-11-5,2014,11,5,15,2lcexa,self.MachineLearning,Job Candidate Question - Coursera Certificates Worth Anything?,https://www.reddit.com/r/MachineLearning/comments/2lcexa/job_candidate_question_coursera_certificates/,[deleted],1415168318,"Question for you all.  Coursera is offering a data science specialization certificate, [here](https://www.coursera.org/specialization/jhudatascience/1?utm_source=email&amp;utm_medium=growth&amp;utm_campaign=spnds).  It would cost just over $400.  If a job candidate listed this somewhere on a resume, would you take them more or less seriously?  What do people think of applicants who list this on their resumes?  It seems to be more and more common.",10,6,False,default,,,,,
39,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,2,2ldrtc,self.MachineLearning,Difference between stream reasoning and machine learning,https://www.reddit.com/r/MachineLearning/comments/2ldrtc/difference_between_stream_reasoning_and_machine/,piscoster,1415207385,"Hello guys,

I am trying to grasp the difference between machine learning(ML) and stream reasoning(SR)[1][2][3]. 

As I undestand it, machine learning is much more precise when it comes down to analysing data streams. Furthermore, it can deal with much more data than stream reasoning engines, like ETALIS, CQELS etc. .

However, basically ML and SR are basically doing the same, right? Hence, I was wondering which is much more appropriate in which situation to use. So when should someone consider to use stream reasoning and when machine learning?

I appreciate your replies!

* [1] [http://www.w3.org/community/rsp/wiki/RDF_Stream_Processors_Implementation](http://www.w3.org/community/rsp/wiki/RDF_Stream_Processors_Implementation)
* [2] [http://wasp.cs.vu.nl/larkc/nefors10/paper/nefors10_paper_0.pdf](http://wasp.cs.vu.nl/larkc/nefors10/paper/nefors10_paper_0.pdf)
* [3] [http://streamreasoning.org/](http://streamreasoning.org/)",3,0,False,self,,,,,
40,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,3,2le541,self.MachineLearning,"Is there collection of very simple, well written, commented implementations of machine learning algorithms?",https://www.reddit.com/r/MachineLearning/comments/2le541/is_there_collection_of_very_simple_well_written/,SrPeixinho,1415213892,"I want to learn more about machine learning, but, considering how big the field is, instead of buying a pile of long books and diving into oceans of words, I'd really prefer something more akin to browsing some well written code in a simple programming language (Python, Scheme, Haskell, JavaScript...), with comments explaining what it does and why it works, and play with that code. That would be such a productive learning experience. Is there anything like that? A git repo?",22,24,False,self,,,,,
41,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,4,2le67y,nlpers.blogspot.com,Hal Daume's EMNLP 2014 paper list with mini-reviews,https://www.reddit.com/r/MachineLearning/comments/2le67y/hal_daumes_emnlp_2014_paper_list_with_minireviews/,xamdam,1415214412,,1,26,False,http://b.thumbs.redditmedia.com/EFdUSdQysh0mL1TEDy-w6JeBxUcuFp9imr83rfxn2Qk.jpg,,,,,
42,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,5,2lee31,self.MachineLearning,"What is the future of Mathematic and mathematicians on an automatic, computerized and digital world?",https://www.reddit.com/r/MachineLearning/comments/2lee31/what_is_the_future_of_mathematic_and/,Gaussiez,1415218341,"I try to explain my question and its importance. Today our society lives a moment full (in a good sense) of fast changes and convulsive advances related with technology. Other times in the History, these types of technological changes affected to our daily live and some professional jobs, like when calculators and computer softwares substituted calculists, accountants or draftsmen some years ago. In some sense the Mankind appears to be replaceable by machines in many fields, but I do not pose this question in a luddist sense, because I think that if machines can do better that job, they should do it instead of us and it will be better for all.
This question is because I thought a few weeks ago that Mathematic could be relatively safe from all these futurible changes in society and that the job of mathematicians would be one of those which could be saved, like other artistic and creative jobs like writer, poet or, maybe, philosopher. But my thoughts changed suddenly a few days ago and I become depressed because I hoped that pure Mathematic belongs to those disciplines which would constitute the last redoubt of Mankind-necessity, although helped by computers and calculators. However, my latest readings on the last advances on Information Theory, Automata Theory, Computability Theory, Computational Complexity Theory, Theoretical Computer Science, Theory of Computation and Logic have shown me the possibility of a Mathematic done only by computers working syntactically or even semantically with axioms and models in first-order or higher-order predicative, deductive or modal logics developed. I thought, like many other mathematicians, that this almost Hilbertian statement could not be achieved because of Gdel's incompleteness theorems, but this asseveration which seems to save Mathematic from a full computerization it's not true because these theorems are true in the first-order axiomatization of arithmetics (Peano) which is not complete, but some (if not all) the second-order or higher-order axiomatizations of arithmetics are complete and these theorems do not apply and, although our computers nowadays only are advanced in first-order automatic deduction, nothing impedes the existence of future computers, which could work in higher-order logics or exotic axiomatizations, solve the most important open problems in Mathematic and prove theorems which Mankind never could imagine; and, in this case, what would be the role of human mathematicians? In fact, would they have any role? Can we save Mathematic for humans? Is there any theoretical or technical impediment which avoids this terrible final? If not, How could we help the machines in do Mathematic? What is, finally, the last role of Mankind in developing Mathematic?
I'm not searching only in terms of time of computation or deduction. I do not care if a computer takes longer than human being to prove a theorem or to solve a problem, but if it can do all things that we do or even more. If we can save Mathematic for Mankind, what have us that computers have not and vice versa?
I will accept and thank all transversally or not philosophical or mathematical answers and opinions but well reasoned and founded. I repeat this is not a luddist question or manifestation.",2,0,False,self,,,,,
43,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,5,2leglv,self.MachineLearning,What are the current problems and status in automatic theorem proving?,https://www.reddit.com/r/MachineLearning/comments/2leglv/what_are_the_current_problems_and_status_in/,Gaussiez,1415219563,"Why can't we see yet computers trying to resolve or prove some of the Milenium Prize Problems or the Smale's list?
Can nowadays our computers hop from theory to metatheory in a formalized language in order to discover possibly undecidable propositions?
Could Riemann's Hypothesis, for example, always be undecidable for a computer?
Could a computer working in first(or some other nth)-order arithmetic recognise the incompleteness of the system finding an undecidable proposition and proving, thus, the Gdel incompleteness theorem? If not, what is that it haven't but Gdel (or other human) yes? How can we equip the computer with this ability? Or we can't?",2,0,False,self,,,,,
44,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,19,2lgovr,nuit-blanche.blogspot.ch,Convex Optimization for Big Data (x-post r/Compressive Sensing ),https://www.reddit.com/r/MachineLearning/comments/2lgovr/convex_optimization_for_big_data_xpost/,compsens,1415270978,,0,10,False,http://b.thumbs.redditmedia.com/7arYGqkBwtHJiqea7ZojIkJLwHCFv206W7XgTPUMS-A.jpg,,,,,
45,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,19,2lgpdo,reddit.com,Machine Learning Journal Club  /r/mlpapers,https://www.reddit.com/r/MachineLearning/comments/2lgpdo/machine_learning_journal_club_rmlpapers/,CreativePunch,1415271511,,5,12,False,default,,,,,
46,MachineLearning,t5_2r3gv,2014-11-6,2014,11,6,21,2lgwaj,self.MachineLearning,Recognition of hate speech,https://www.reddit.com/r/MachineLearning/comments/2lgwaj/recognition_of_hate_speech/,rawcuban77,1415278187,"For my college project i have to do some work in R-studio. Goal of the work is to recognize if texts in database are hate speech or not.....instructor told me that i can use TM package for R. I found a lot of papers for text classification,but i will try to ask here too,maybe you people have done something similar. For any hint or idea i will be very thankfull! ",21,0,False,self,,,,,
47,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,1,2lhghr,self.MachineLearning,Multi-Class MLP; is it better to split into N separate binary class MLPs?,https://www.reddit.com/r/MachineLearning/comments/2lhghr/multiclass_mlp_is_it_better_to_split_into_n/,ml_man,1415290561,"Does anyone have any experience/good sources on whether its better to  transform multiple class classification problems into a set of binary sub-problems?

For anyone unfamiliar with the terms; a multi-class MLP may predict whether an example is a cat or dog or human; a binary MLP may predict whether an example is a car or not.

When is it better to train a network which predicts if an example is a cat/dog/human vs. training 3 networks which individually predict if an example is cat/not, dog/not, human/not?",7,3,False,self,,,,,
48,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,2,2lhm6f,engineeringblog.yelp.com,Yelp Dataset Challenge Round 3 Winners and Dataset Tools for Round 4,https://www.reddit.com/r/MachineLearning/comments/2lhm6f/yelp_dataset_challenge_round_3_winners_and/,Zephyr314,1415293485,,0,5,False,default,,,,,
49,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,2,2lhnfc,rzagabe.com,Artificial Neural Networks  Introduction,https://www.reddit.com/r/MachineLearning/comments/2lhnfc/artificial_neural_networks_introduction/,monsieurlu,1415294065,,4,20,False,http://b.thumbs.redditmedia.com/ey8q_Cr3xaEyxaj_ajWyCw1C7TLEuc8gpz5gF-Wya2U.jpg,,,,,
50,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,4,2li7ba,mlcsf.eventbrite.com,MLconf is next week - Check out the presenters here,https://www.reddit.com/r/MachineLearning/comments/2li7ba/mlconf_is_next_week_check_out_the_presenters_here/,shonburton,1415303680,,3,0,False,http://b.thumbs.redditmedia.com/JlbWHTa7g8LYTZrY8SnFZaY-eN09NEueAkmbd7gt8vE.jpg,,,,,
51,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,6,2lihu2,self.MachineLearning,Benchmarks for massive scalability and realtime machine learning open door to new class of machine intelligence,https://www.reddit.com/r/MachineLearning/comments/2lihu2/benchmarks_for_massive_scalability_and_realtime/,xyggy,1415308732,"Benchmarks demonstrate massive scalablilty and realtime machine learning on a distributed (hpc) system by timing two operations as the number of cores increase: query execution and adding new items.

The results open the door to a new class of machine intelligence grounded in realtime-learning and massive scalability ... leaving behind batch-learning and poor scalability.

Details at www.xyggy.com.",0,0,False,self,,,,,
52,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,12,2ljnm8,self.MachineLearning,What's with the trend towards deeper neural nets?,https://www.reddit.com/r/MachineLearning/comments/2ljnm8/whats_with_the_trend_towards_deeper_neural_nets/,CarbonAvatar,1415331244,"I remember from my Russell / Norvig AI textbook that any neural net that can be expressed in N hidden layers can also be expressed in a single hidden layer.

Yet, the trend seems to be towards more layers lately. Why is this? Is it a question of efficiency? Learning speed?",24,12,False,self,,,,,
53,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,12,2ljnzr,self.MachineLearning,How to handle highly imbalanced label uncertainty (if that's the right thing to call it)?,https://www.reddit.com/r/MachineLearning/comments/2ljnzr/how_to_handle_highly_imbalanced_label_uncertainty/,Omega037,1415331466,"I'm currently working with a dataset of about 1,500 samples in which the labels/response variables take the form of a number of attempts made for that sample and the subsequent number of successes it had.  

In other words, you might have Sample 1 be ""attempted 25 times and succeeded five times"", while Sample 2 is ""attempted 12 times and succeeded once"".

Unfortunately, the number of times the attempts are made varies greatly.  For some samples it may be as high as 300 attempts, while for others it is only a single attempt.  For those larger numbers we have a decent amount of confidence in the overall performance of that sample, but for the single attempt samples we have almost none.

When I try to use this data to build a model, the model becomes heavily biased by samples that have single attempts that are successful.  So far my solution has just been to drop any samples with less than a minimum number of attempts (I have arbitrarily chosen 10), but this results in a significant loss of samples (around 30%).

Any ideas on how I can better handle this problem?",3,1,False,self,,,,,
54,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,14,2ljwn3,self.MachineLearning,Top universities to pursue a PhD in any of the following ML research areas?,https://www.reddit.com/r/MachineLearning/comments/2ljwn3/top_universities_to_pursue_a_phd_in_any_of_the/,vamsilg,1415336884,"I will be finishing my bachelor's degree soon and I am willing to pursue a PhD in any of the following ML Research areas. 
1) Deep Learning
2) Tensor factorization for ML
3) Machine learning for NLP

Please help me with the list of universities that pursue good research in these areas. 
",12,1,False,self,,,,,
55,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,20,2lkjgv,mashable.com,'...they claim can predict the effectiveness of new medicines 150 times faster than current methods.'-- Even with a super computer how are they brute forcing combinatorial explosion?,https://www.reddit.com/r/MachineLearning/comments/2lkjgv/they_claim_can_predict_the_effectiveness_of_new/,SmoothCB,1415358664,,10,3,False,http://b.thumbs.redditmedia.com/FykJ3UvRwIi-tEpiw0A2jv5YramsHxRpGsM6-iuxp6A.jpg,,,,,
56,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,20,2lkklj,self.MachineLearning,Machine Learning and Software Analysis,https://www.reddit.com/r/MachineLearning/comments/2lkklj/machine_learning_and_software_analysis/,perincertus,1415359807,"Hello /MachineLearning!

I'm a masters student in computer science with a strong focus on software engineering. However, I'm getting more and more interested in machine learning and was wondering if there exist applications for machine learning in the field of software analysis and testing. Any ideas?",7,9,False,self,,,,,
57,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,22,2lkufm,arxiv.org,Convolutional Neural Network-based Place Recognition,https://www.reddit.com/r/MachineLearning/comments/2lkufm/convolutional_neural_networkbased_place/,keghn,1415368534,,0,1,False,default,,,,,
58,MachineLearning,t5_2r3gv,2014-11-7,2014,11,7,22,2lkuiu,arxiv.org,A Hybrid Recurrent Neural Network For Music Transcription,https://www.reddit.com/r/MachineLearning/comments/2lkuiu/a_hybrid_recurrent_neural_network_for_music/,keghn,1415368618,,0,1,False,default,,,,,
59,MachineLearning,t5_2r3gv,2014-11-8,2014,11,8,0,2ll19w,phys.org,New way to predict how traffic will flow,https://www.reddit.com/r/MachineLearning/comments/2ll19w/new_way_to_predict_how_traffic_will_flow/,keghn,1415372962,,6,29,False,http://a.thumbs.redditmedia.com/9_j0q5-VtqzQJ2QRTmFZN58KE4LsLWwlt5lvVaBiq-8.jpg,,,,,
60,MachineLearning,t5_2r3gv,2014-11-8,2014,11,8,1,2ll97o,slideshare.net,Introduction to Deep Learning with Python's Theano by indico Data Solutions,https://www.reddit.com/r/MachineLearning/comments/2ll97o/introduction_to_deep_learning_with_pythons_theano/,just-anotherguy,1415377443,,2,8,False,http://b.thumbs.redditmedia.com/FvtfSWJxh6tAt2TC5g0Ly3PXUNas8vu3nzUuMwxetzQ.jpg,,,,,
61,MachineLearning,t5_2r3gv,2014-11-8,2014,11,8,5,2lm1o6,self.MachineLearning,What are the pro's and con's of AdaBoost? How useful is it?,https://www.reddit.com/r/MachineLearning/comments/2lm1o6/what_are_the_pros_and_cons_of_adaboost_how_useful/,Master_Rux,1415392156,,4,3,False,self,,,,,
62,MachineLearning,t5_2r3gv,2014-11-8,2014,11,8,8,2lmo0l,self.MachineLearning,AMA Geoffrey Hinton,https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/,geoffhinton,1415404545,"I design learning algorithms for neural networks. My aim is to discover a learning procedure that is efficient at finding complex structure in large, high-dimensional datasets and to show that this is how the brain learns to see. I was one of the researchers who introduced the back-propagation algorithm that has been widely used for practical applications. My other contributions to neural network research include Boltzmann machines, distributed representations, time-delay neural nets, mixtures of experts, variational learning, contrastive divergence learning, dropout, and deep belief nets.   My students have changed the way in which speech recognition and object recognition are done. 

I now work part-time at Google and part-time at the University of Toronto. ",262,374,False,self,,,,,
63,MachineLearning,t5_2r3gv,2014-11-8,2014,11,8,14,2lnf6b,self.MachineLearning,Question: How to build image recommender system for nearest similar images in database?,https://www.reddit.com/r/MachineLearning/comments/2lnf6b/question_how_to_build_image_recommender_system/,plzdontsetfault,1415423167,"Hi, sorry if this is a novice question, I'm a junior who is new to machine learning but is super interested. For a school project I am trying to recommend clothing images, I get how to tell if a image is a shirt or shoe, take a dataset of images with each image labeled shirt or shoe, then train a model and use it to classify a new image. I don't get how to do this when there's no specific label, like if you want to show more shoes that are similar in color. I am thinking to use nearest neighbor, but how would this work and what metric? Can neural networks do this?",10,2,False,self,,,,,
64,MachineLearning,t5_2r3gv,2014-11-8,2014,11,8,16,2lnoou,self.MachineLearning,Getting data from forums,https://www.reddit.com/r/MachineLearning/comments/2lnoou/getting_data_from_forums/,rawcuban77,1415431520,,3,0,False,default,,,,,
65,MachineLearning,t5_2r3gv,2014-11-8,2014,11,8,19,2lnz2t,youtube.com,Dark Knowledge - TTIC Distinguished Lecture Series - Geoffrey Hinton,https://www.reddit.com/r/MachineLearning/comments/2lnz2t/dark_knowledge_ttic_distinguished_lecture_series/,__Joker,1415443899,,9,34,False,http://a.thumbs.redditmedia.com/iD2geHc0SchFPLYf0K0FZKYZc_DYnf6u6DXnjvva474.jpg,,,,,
66,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,0,2loeuk,self.MachineLearning,What are some breakthroughs in recent years which machine learning has enabled?,https://www.reddit.com/r/MachineLearning/comments/2loeuk/what_are_some_breakthroughs_in_recent_years_which/,hellojarvis,1415459448,"I recently came across ML, and I have been seeing so much hype about it. Can you please list why it is such a big deal. What are the breakthroughs it has caused?",16,15,False,self,,,,,
67,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,4,2lp6mm,self.MachineLearning,Splitting 8GB dataset into train/test split in Python,https://www.reddit.com/r/MachineLearning/comments/2lp6mm/splitting_8gb_dataset_into_traintest_split_in/,acpigeon,1415476367,"I'm working on the Kaggle [Avazu CTR Prediction](https://www.kaggle.com/c/avazu-ctr-prediction) competition. The training dataset is much larger than what I'm used to dealing with, i.e. bigger than my machine limits. I'm looking for some pointers on how to approach the problem and this forum seems like a better place to pose this than StackOverflow (where I can go for implementation details). I've cross posted this on the Kaggle forums as well, so we'll see if that helps.

Conceptually, here's what I think I have to do
1) Split the training csv into two files using a random 70/30 split, ideally a different split for each training run
2) Use a generator to yield the lines in the resulting files for one-by-one encoding
3) Use another generator to feed the output of the previous generator into sklearn for training / xvalidation.
4) Use another dual generator setup on the test data set to write out predictions

Is there a Pythonic way to handle this problem at scale? Does this put me outside the realm of sklearn and into hand rolled model implementations? Would love any advice that I could get for this problem.",6,3,False,self,,,,,
68,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,5,2lpa4i,reddit.com,"[Journal Club] The journal club has tentatively moved to /r/mlpapers , new voting/discussion threads are online there. Tell us what you think of it :)",https://www.reddit.com/r/MachineLearning/comments/2lpa4i/journal_club_the_journal_club_has_tentatively/,BeatLeJuce,1415478477,,0,7,False,default,,,,,
69,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,11,2lqa62,cireneikual.wordpress.com,My Attempt at Outperforming Deepmind's Atari Results - UPDATE 11,https://www.reddit.com/r/MachineLearning/comments/2lqa62/my_attempt_at_outperforming_deepminds_atari/,CireNeikual,1415501017,,3,7,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
70,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,12,2lqd62,nbviewer.ipython.org,Simple speech recognition using dynamic time warping,https://www.reddit.com/r/MachineLearning/comments/2lqd62/simple_speech_recognition_using_dynamic_time/,[deleted],1415503205,,0,29,False,default,,,,,
71,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,13,2lqkbp,dataelixir.com,"Data Elixir - News and resources for developers who work with data | Issue 9: Neural nets, big ideas, and data viz.",https://www.reddit.com/r/MachineLearning/comments/2lqkbp/data_elixir_news_and_resources_for_developers_who/,lonriesberg,1415508352,,1,0,False,http://b.thumbs.redditmedia.com/0pxgAlEylDGCCAgCFe6fHMET4KrK-5Q_bWHhydd3uVI.jpg,,,,,
72,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,14,2lqoif,datasciencecentral.com,22 tips for better data science,https://www.reddit.com/r/MachineLearning/comments/2lqoif/22_tips_for_better_data_science/,urinec,1415511677,,3,0,False,http://b.thumbs.redditmedia.com/F4Dl53_A6XRNl830uKsuhEUjGIpAbQVO6vfatFYZaug.jpg,,,,,
73,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,15,2lqqn0,ntu.edu.sg,Comparison between ELM and Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2lqqn0/comparison_between_elm_and_deep_learning/,rantana,1415513475,,20,10,False,default,,,,,
74,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,20,2lr5ky,memkite.com,Added 152 new Deep Learning papers to the Deeplearning.University Bibliography,https://www.reddit.com/r/MachineLearning/comments/2lr5ky/added_152_new_deep_learning_papers_to_the/,atveit,1415531029,,0,0,False,default,,,,,
75,MachineLearning,t5_2r3gv,2014-11-9,2014,11,9,23,2lri8v,bbc.co.uk,Interview with DeepMind founder Demis Hassabis (BBC Radio 4),https://www.reddit.com/r/MachineLearning/comments/2lri8v/interview_with_deepmind_founder_demis_hassabis/,benanne,1415544577,,16,45,False,http://b.thumbs.redditmedia.com/Mtj1qV6o0KiFXGr-LTP4gTL2VkKzsc0iBl2ZUwPFq1c.jpg,,,,,
76,MachineLearning,t5_2r3gv,2014-11-10,2014,11,10,5,2lsgnx,blog.datumbox.com,How to install and use the Datumbox Machine Learning Framework,https://www.reddit.com/r/MachineLearning/comments/2lsgnx/how_to_install_and_use_the_datumbox_machine/,datumbox,1415564357,,0,0,False,http://b.thumbs.redditmedia.com/QCroK0ipgBeejWgJUAF0tA0FCtuSNa9jTbFwUpRGVfM.jpg,,,,,
77,MachineLearning,t5_2r3gv,2014-11-10,2014,11,10,5,2lsibc,self.MachineLearning,Explaining Differences between Clusters : What search term am I looking for?,https://www.reddit.com/r/MachineLearning/comments/2lsibc/explaining_differences_between_clusters_what/,[deleted],1415565290,"I'm working on a problem where I want to explain the differences between clusters of features based on sets of latent variables that could have generated the data (i.e. cluster A is cluster B plus signal X).  I'm assuming this is a common problem, with a few well-researched solutions.  Can anyone point me towards the names of some algorithms / what to read?  ",0,1,False,default,,,,,
78,MachineLearning,t5_2r3gv,2014-11-10,2014,11,10,6,2lsn2q,eng.kifi.com,From word2vec to doc2vec: an approach driven by Chinese restaurant process,https://www.reddit.com/r/MachineLearning/comments/2lsn2q/from_word2vec_to_doc2vec_an_approach_driven_by/,galapag0,1415568001,,4,30,False,http://b.thumbs.redditmedia.com/jWZ4ZtQIN57H2dRDbjzPBn5suksBBh5bEZFObF9wenM.jpg,,,,,
79,MachineLearning,t5_2r3gv,2014-11-10,2014,11,10,14,2ltyg9,self.MachineLearning,[Neural Networks] How to diagnose lack of sharp/clear filters?,https://www.reddit.com/r/MachineLearning/comments/2ltyg9/neural_networks_how_to_diagnose_lack_of/,donnaprima,1415595623,"I was trying out a weird 2-hidden layer Autoencoder, where instead of 1 hidden layer on an typical AutoEncoder, I have 2. Note, this is not a  stacking up of 2 autoencoders, but rather training the 2 hidden layer parameters at one go. I wanted to try this just to compare the results with typical autoencoders, no other reason.

I found that my filters (between the visible and the 1st hidden layer) were not ""sharp"". As with Zeiler's paper on deconvolutional networks for visualizing the Krizhevsky-net, sharp filters are often a good sign for the network.

So I want to diagnose what the reason behind blurry filters could be. Would anyone have an idea of  typical reasons for this? like a lack of data, training epochs, or a learning rule (Adagrad/Adadelta..)? I'll be trying out these but advice from someone who already has explored this would be very helpful.",4,4,False,self,,,,,
80,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,0,2lv5dz,pyimagesearch.com,6 Steps to Detecting Objects in Images (with HOG + Linear SVM),https://www.reddit.com/r/MachineLearning/comments/2lv5dz/6_steps_to_detecting_objects_in_images_with_hog/,zionsrogue,1415633203,,21,12,False,http://b.thumbs.redditmedia.com/6jOQECBffDjFcdMZsV9DUvzfsRAqbWGbI4Vc3QSsInc.jpg,,,,,
81,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,0,2lv8n3,self.MachineLearning,CUDA-based neural networks in Python,https://www.reddit.com/r/MachineLearning/comments/2lv8n3/cudabased_neural_networks_in_python/,abll,1415635027,"I have spent the last couple of weeks coding on two projects: [CUDArray](http://compute.dtu.dk/~abll/blog/cudarray) is a CUDA-based subset of NumPy and [deeppy](http://github.com/andersbll/deeppy/) is a neural network framework built on top of CUDArray.

While there is still a lot to be done on both projects, I think they have reached a point where they start to look promising. Highlights include:

- Since CUDArray imitates NumPy, its user interface is well-known to NumPy-programmers.
- CUDArray has CPU fall-back when no GPU is available.
- CUDArray relies on cuBLAS, cuRAND and cuDNN for fast array operations.
- deeppy has a pythonic user interface (that is, no YAML configuration files).
- In terms of speed, CUDArray/deeppy is very competitive with other popular neural network frameworks. At least, that's my experience so far.

Be sure to check out some deeppy examples:

[Multi-layer perceptron with dropout on MNIST](http://github.com/andersbll/deeppy/blob/master/examples/mlp_dropout_mnist.py)

[Convnet on CIFAR-10](http://github.com/andersbll/deeppy/blob/master/examples/convnet_cifar.py)",39,85,False,self,,,,,
82,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,2,2lvgqn,hakkalabs.co,Deep Learning: Machine Perception and Its Applications,https://www.reddit.com/r/MachineLearning/comments/2lvgqn/deep_learning_machine_perception_and_its/,dot_2,1415639223,,0,0,False,http://b.thumbs.redditmedia.com/PhToLWNy_LHdMOjDPD9LcGMOahU8EUKoGm1wGIVHu_I.jpg,,,,,
83,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,2,2lvmnt,nuit-blanche.blogspot.com,"LRSLibrary : 64 Matrix Factorizations (Robust PCA, NMF, PCP...) Algorithms for Low-Rank and Sparse for Background Modeling and Subtraction in Videos",https://www.reddit.com/r/MachineLearning/comments/2lvmnt/lrslibrary_64_matrix_factorizations_robust_pca/,compsens,1415642200,,0,5,False,http://b.thumbs.redditmedia.com/hu3uLq3jP4_sv1tzbYCgS8tO4Yadi6z-27HeifjCQwo.jpg,,,,,
84,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,5,2lw8ez,self.MachineLearning,Question: What algorithms are used to explain differences between clusters?,https://www.reddit.com/r/MachineLearning/comments/2lw8ez/question_what_algorithms_are_used_to_explain/,[deleted],1415652944,I'm working on a problem where I'd like to explain differences between clusters based on underlying signals in the data (ie Cluster A is Cluster B plus signal X). Can someone point me towards the right search terms / papers on this topic?  Thanks (I can provide a lot more detail if that would be helpful). ,1,0,False,default,,,,,
85,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,8,2lwovs,self.MachineLearning,Free database of food ingredients compounds?,https://www.reddit.com/r/MachineLearning/comments/2lwovs/free_database_of_food_ingredients_compounds/,UannaFF,1415660993,"Hi guys! can anybody tell me if there's such thing as a free database of food ingredients compounds? I am working on a project that is based in the idea of combining ingredients based on their compatibility by the amount of shared compounds to create new meals, it's a university thing and i haven't found a free database. I found this one http://www.leffingwell.com/flavbase.htm and other two, but all of them are paid. I don't need something as complete as that, can be of liquids only, or of some kind of ingredients only... i just need something, even if it is a subset to get me started, otherwise it would be too much work to finish it because i can't pay 2000$ to gain access to a database.",3,0,False,self,,,,,
86,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,10,2lx40m,self.MachineLearning,"What does ""CRP is a marginalized version of PYP"" mean?",https://www.reddit.com/r/MachineLearning/comments/2lx40m/what_does_crp_is_a_marginalized_version_of_pyp/,koormoosh,1415669176,"I've been reading this phrase and I don't know what it means ""CRP is a marginalized version of PYP"". What are the parameters/latent-variables we are marginalizing out to drive CRP from PYP?
",4,3,False,self,,,,,
87,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,10,2lx5i2,self.MachineLearning,Weka vs scikit-learn [for natural language processing]?,https://www.reddit.com/r/MachineLearning/comments/2lx5i2/weka_vs_scikitlearn_for_natural_language/,ml_student,1415669986,"Im with my bachelor thesis and my advisor let me do a comparison between Weka and scikit-learn libraries. Could anybody tell me why scikit is better than weka?, what are some advantages between each one?.

Thanks!",20,0,False,self,,,,,
88,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,12,2lxf0e,self.MachineLearning,The Features and Benefits of Concrete Pump,https://www.reddit.com/r/MachineLearning/comments/2lxf0e/the_features_and_benefits_of_concrete_pump/,haomeibatchingplant,1415675282,,2,0,False,default,,,,,
89,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,17,2ly87n,self.MachineLearning,The Maintenance Tips of Concrete Batching Plant For Sale,https://www.reddit.com/r/MachineLearning/comments/2ly87n/the_maintenance_tips_of_concrete_batching_plant/,haomeibatchingplant,1415696021,,0,0,False,default,,,,,
90,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,22,2lyno1,arxiv.org,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,https://www.reddit.com/r/MachineLearning/comments/2lyno1/unifying_visualsemantic_embeddings_with/,[deleted],1415711425,,0,1,False,default,,,,,
91,MachineLearning,t5_2r3gv,2014-11-11,2014,11,11,22,2lyoot,arxiv.org,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,https://www.reddit.com/r/MachineLearning/comments/2lyoot/unifying_visualsemantic_embeddings_with/,sergii_gavrylov,1415712195,,1,1,False,default,,,,,
92,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,1,2lzalp,blog.jatwood.org,4chan Feelometer,https://www.reddit.com/r/MachineLearning/comments/2lzalp/4chan_feelometer/,yacobus,1415725197,,0,0,False,default,,,,,
93,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,2,2lzec3,blog.datacamp.com,New Infographic: How to become a data scientist in 8 steps,https://www.reddit.com/r/MachineLearning/comments/2lzec3/new_infographic_how_to_become_a_data_scientist_in/,martijnT,1415727020,,7,0,False,default,,,,,
94,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,4,2lzqh5,self.MachineLearning,Expectation Maximization C++ code review,https://www.reddit.com/r/MachineLearning/comments/2lzqh5/expectation_maximization_c_code_review/,[deleted],1415732956,"As a side project to get into machine learning I am trying to implement an expectation maximization algorithm in C++ but I am going wrong somewhere.

Would anyone be willing to look over my code and help me understand where I am going wrong? ",0,1,False,default,,,,,
95,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,4,2lzsw8,self.MachineLearning,Expectation Maximization C++ code review,https://www.reddit.com/r/MachineLearning/comments/2lzsw8/expectation_maximization_c_code_review/,[deleted],1415734097,"As a side project to get into machine learning I am trying to implement an expectation maximization algorithm in C++ but I am going wrong somewhere.

Would anyone be willing to look over my code and help me understand where I am going wrong?

edit: here is a link to the code
https://bitbucket.org/MathanCompBioakes/em/src/56e1489b7a7dbe7060c8b39efacdac58c4c297c7/em.cpp?at=master",2,2,False,default,,,,,
96,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,4,2lztll,static.googleusercontent.com,The Learning Behind Gmail Priority Inbox [Google - PDF],https://www.reddit.com/r/MachineLearning/comments/2lztll/the_learning_behind_gmail_priority_inbox_google/,[deleted],1415734428,,9,31,False,default,,,,,
97,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,4,2lzvez,self.MachineLearning,Weighting labeled points over unlabeled points in semi-supervised learning.,https://www.reddit.com/r/MachineLearning/comments/2lzvez/weighting_labeled_points_over_unlabeled_points_in/,SymphMeta,1415735288,"I am currently working on a [semi-supervised generative model](http://people.cs.umass.edu/~mccallum/papers/semisup-em.pdf) with about 330 labeled nodes and 18,000 unlabeled nodes.  There are about 300 different variables I'm using based on n-grams of varying sizes, each of which can have a non-negative integer value.  There are 5 different classes I am trying to sort points into, with prior probabilities of 0.08 to 0.5.  (If there are any more questions needed about the dataset to answer the following questions, let me know.)

However, when I run the algorithm, it will shrink some of the smaller groups down to a small number (much smaller than what one would expect based on the standard error of the prior estimates) unless I decide to weight the algorithm more heavily in favor of the labeled data for all iterations (not just the first).

Is there any basis for maintaining a higher weight for the data that is originally labeled?  Is it likely that one of the issues is the sparsity of each group's training set with respect to the number of variables?

",0,2,False,self,,,,,
98,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,5,2m03rw,datasciencecentral.com,13 New Trends in Big Data and Data Science,https://www.reddit.com/r/MachineLearning/comments/2m03rw/13_new_trends_in_big_data_and_data_science/,vincentg64,1415739511,,3,0,False,http://b.thumbs.redditmedia.com/AiBfBQ5CqOqqs8Dzt9LcDnsgd_RL82-D5bish6icdEk.jpg,,,,,
99,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,6,2m06xr,self.MachineLearning,Naive Bayes vs. Simple Formula,https://www.reddit.com/r/MachineLearning/comments/2m06xr/naive_bayes_vs_simple_formula/,lovestowritecode,1415741015,"I'm working on a program that classifies whether or not a web page is related to Movies or not using Naive Bayes. I got a fairly good result of 85% accuracy but I felt this wasn't high enough and had an idea to just come up with my own simple formula.

My formula has a list of good keywords and bad keywords, and each keyword has a weighted number value. So if the word **movie** is on a web page it has a high number value of 5 and if a negative word appears like **medical** it has a negative number value of -5. 

All the values are added up and if it is a positive number it's a web page about movies, negative number would mean it is not related to movies. 

This may seem over simplified but I tested this on over 300,000 web pages and got a 99% accuracy rate, it basically destroyed the naive bayes on actual data. This way also uses very little memory and can process incredible fast. It seems almost too good to be true and have fears that this may not scale properly.

[EDIT] Thanks everyone for their much appreciated help and responses! Liking the community on /r/MachineLearning, hope to contribute at some point.",20,5,False,self,,,,,
100,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,8,2m0m5k,self.MachineLearning,Which classes to take for ML grad school?,https://www.reddit.com/r/MachineLearning/comments/2m0m5k/which_classes_to_take_for_ml_grad_school/,letoseldon,1415748452,"I have an opportunity to take 1-2 evening classes starting in January, and the local college offers the usual set of masters and PhD-level grad classes (probability theory, mathematical stats, real analysis, etc.) 

My background is in applied math, with only a limited background in statistics (one math stats class and one prob. theory class) and a weaker background in real analysis than pure math majors.  What class would help me the most for grad school in ML? 

",13,0,False,self,,,,,
101,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,10,2m127c,self.MachineLearning,Seeking advice on possible next steps in working on ML.,https://www.reddit.com/r/MachineLearning/comments/2m127c/seeking_advice_on_possible_next_steps_in_working/,hyperion_agent3011,1415757090,"Sorry if this is long winded, but I figured maybe someone on here could provide some advice.

So I am currently a senior in college who has a job lined up in the Bay Area starting in August doing Business Analytics. I interned at the company this previous summer and most of the work involved writing SQL queries and doing visualizations in Tableau. My manager told me that if I came back I would have room to do more interesting/more complicated things than what I worked on this summer. Some more background on me is that I'm studying Statistics and Computer Science in School and am currently taking the grad level ML course offered at my college. 

Now while I am studying statistics and computer science, I feel like I still do not know enough about ML as I would like, and I would like to really practice ML in industry in the future. So my question is where do I go from here? Its too late to apply to grad school at this point as I have already signed an offer, and have never made getting into graduate school the focus of my undergraduate years. Ideally I would like to work in industry implementing ML algorithms in useful ways that have an impact for the business or the customer. I feel like I need to know more though, despite the fact that I have studied both statistics and comp sci. What would be some practical steps for me to take? I've always figured I could learn more ML on my own and participate in Kaggle competitions to build up my credentials, which would then let me transition into a more formal ML role, is that possible? I guess maybe my question boils down to do I need to at some point go to grad school? I'm not against it, but would rather learn on my own and make it that way, is that feasible?",6,2,False,self,,,,,
102,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,11,2m17vr,self.MachineLearning,Some Advanced Things Badly Needed Due To The Society Development Trend,https://www.reddit.com/r/MachineLearning/comments/2m17vr/some_advanced_things_badly_needed_due_to_the/,haomeibatchingplant,1415760177,,0,1,False,default,,,,,
103,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,12,2m1cad,self.MachineLearning,Experiences with bayesian hyperparameter optimization?,https://www.reddit.com/r/MachineLearning/comments/2m1cad/experiences_with_bayesian_hyperparameter/,galapag0,1415762639,"I was checking the paper, [Practical Bayesian Optimization of Machine Learning](http://arxiv.org/pdf/1206.2944) and i was wondering if anyone here had some experience (good or bad) with it..",10,12,False,self,,,,,
104,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,17,2m23d0,self.MachineLearning,Is Azure Machine Learning tool useful at all ?,https://www.reddit.com/r/MachineLearning/comments/2m23d0/is_azure_machine_learning_tool_useful_at_all/,chiragdhull,1415782159,,12,11,False,self,,,,,
105,MachineLearning,t5_2r3gv,2014-11-12,2014,11,12,19,2m29t9,self.MachineLearning,"Comparison of Non-linearities in terms of convergence dynamics,limitations and strong points?",https://www.reddit.com/r/MachineLearning/comments/2m29t9/comparison_of_nonlinearities_in_terms_of/,[deleted],1415789339,"Nowadays, linear and piecewise linear activations functions are preferred for neural networks. Could you please recommend any works that compare the suitability, and the convergence behavior of these activation functions? Some specific questions: 

1. use of Rectified Linear Units/maxout for autoencoders. Especially when using autoencoders as denoisers, where we care about the image reconstruction rather than as feature learners for subsequent classification. I wasn't convinced with the approach taken in Glorot's Deep Sparse rectifier Networks, maybe some other work has taken up this analysis?


3. Recommendations of Learning rates when confronted with any of these units. Especially in heterogenous networks, which we might end up using when we have outputs in 0-1 range and have to use saturating i.e. sigmoid non-linearities.

2. comparison of saturating piecewise linear functions (http://yann.lecun.com/exdb/publis/pdf/goroshin-lecun-iclr-13.pdf) vs saturating nonlinearities like tanh and sigmoid. Where do we still prefer prefer tanh and sigmoid?",0,1,False,default,,,,,
106,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,1,2m34kp,levyomer.files.wordpress.com,word2vec is actually just matrix factorization,https://www.reddit.com/r/MachineLearning/comments/2m34kp/word2vec_is_actually_just_matrix_factorization/,gabjuasfijwee,1415811312,,6,26,False,default,,,,,
107,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,2,2m36a4,self.MachineLearning,A question on dropout regularization in neural networks,https://www.reddit.com/r/MachineLearning/comments/2m36a4/a_question_on_dropout_regularization_in_neural/,Devilsbabe,1415812187,"I was watching [this presentation](http://youtu.be/vShMxxqtDDs?t=35m26s) by Geoff Hinton today and did not fully understand a point he made about dropout. He states that dropout pulls weights towards what other models want, whereas L1 or L2 regularization just pulls the weights toward 0. 

Does someone have some insight into what he means? I understand the L1/L2 part, but not the pulling of weights towards other models. Thanks.",1,3,False,self,,,,,
108,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,2,2m3blh,blog.iriomk.com,Attending 2nd week of Couseras Machine Learning course,https://www.reddit.com/r/MachineLearning/comments/2m3blh/attending_2nd_week_of_couseras_machine_learning/,IrioMk,1415814789,,0,0,False,http://b.thumbs.redditmedia.com/sGFevTdH-cqwIPu3brviaUKCJ9GnVjadbi-2CEgMY4Y.jpg,,,,,
109,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,6,2m42em,deeplearning.net,Theano Tutorial: RNN with Word Embeddings,https://www.reddit.com/r/MachineLearning/comments/2m42em/theano_tutorial_rnn_with_word_embeddings/,siblbombs,1415827613,,0,26,False,default,,,,,
110,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,11,2m546l,self.MachineLearning,High Efficiency Operation Improves The Productivity of concrete,https://www.reddit.com/r/MachineLearning/comments/2m546l/high_efficiency_operation_improves_the/,haomeibatchingplant,1415846092,,0,1,False,default,,,,,
111,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,13,2m5g4s,blog.shriphani.com,Intuition and implementation of Isomap [also posted to /r/clojure],https://www.reddit.com/r/MachineLearning/comments/2m5g4s/intuition_and_implementation_of_isomap_also/,shriphani,1415852892,,3,2,False,http://b.thumbs.redditmedia.com/mjpdVG8IIc1RN9AeGiK9OWqsAtWFsZI6SWI12qCSZPs.jpg,,,,,
112,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,16,2m5vvt,self.MachineLearning,"Comparison of Non-linearities in terms of convergence dynamics,limitations and strong points?",https://www.reddit.com/r/MachineLearning/comments/2m5vvt/comparison_of_nonlinearities_in_terms_of/,budhdub,1415864699,"Nowadays, linear and piecewise linear activations functions are preferred for neural networks. Could you please recommend any works that compare the suitability, and the convergence behavior of these activation functions? Some specific questions:

1. use of Rectified Linear Units/maxout for autoencoders. Especially when using autoencoders as denoisers, where we care about the image reconstruction rather than as feature learners for subsequent classification. I wasn't convinced with the approach taken in Glorot's Deep Sparse rectifier Networks, maybe some other work has taken up this analysis?

2. Recommendations of Learning rates when confronted with any of these units. Especially in heterogenous networks, which we might end up using when we have outputs in 0-1 range and have to use saturating i.e. sigmoid non-linearities.

3. comparison of saturating piecewise linear functions (http://yann.lecun.com/exdb/publis/pdf/goroshin-lecun-iclr-13.pdf) vs saturating nonlinearities like tanh and sigmoid. Where do we still prefer prefer tanh and sigmoid?",0,2,False,self,,,,,
113,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,18,2m6337,dippl.org,The Design and Implementation of Probabilistic Programming Languages,https://www.reddit.com/r/MachineLearning/comments/2m6337/the_design_and_implementation_of_probabilistic/,eaturbrainz,1415872281,,0,29,False,default,,,,,
114,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,18,2m635q,self.MachineLearning,What is the relationship between Linear SVMs and logistic regression?,https://www.reddit.com/r/MachineLearning/comments/2m635q/what_is_the_relationship_between_linear_svms_and/,[deleted],1415872360,"Clearly they are optimising for different objective functions, but what is the significance of this in the interpretation of the results?

It seems that in both cases you obtain an interpretable weighting on the features used for the classification, but how do these differ?",4,6,False,default,,,,,
115,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,18,2m63fa,self.MachineLearning,Dictionary learning for extremely small dataset of samples?,https://www.reddit.com/r/MachineLearning/comments/2m63fa/dictionary_learning_for_extremely_small_dataset/,BeijingChina,1415872649,"I have around 100 samples, on which I'd like to do some sort of ""dictionary learning"". These are essentially medical images of a particular ethnicity and I cant really augment the data. I am looking to try a dictionary learning approach where I can use the learnt dictionaries on ""healthy specimens"" to reconstruct test data and discover abnormalities.

I am wrangling with the question of which method could make the best use of this small amount of data when learning to do reconstruction? 
Normally for classification tasks people take the ""tendency towards overfitting"" as a deciding factor between schemes. Is there an analogue for reconstruction type problems? 

Apart from getting our hands dirty and trying out many different methods (sparse coding, NMF, autoencoders etc.) which I expect I will have to, what would be a good direction to think about this? 

thanks guys. ",0,1,False,self,,,,,
116,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,20,2m67mi,github.com,"ArrayFire, a general-purpose GPU library, goes open source",https://www.reddit.com/r/MachineLearning/comments/2m67mi/arrayfire_a_generalpurpose_gpu_library_goes_open/,galapag0,1415877345,,10,22,False,http://b.thumbs.redditmedia.com/AgDLdCbA4YnoUhwLAhqzKACoqnTk8zgDW7oLn53rRXs.jpg,,,,,
117,MachineLearning,t5_2r3gv,2014-11-13,2014,11,13,22,2m6gdf,self.MachineLearning,Where can i get the lenet5(lecun 98) configuration file?,https://www.reddit.com/r/MachineLearning/comments/2m6gdf/where_can_i_get_the_lenet5lecun_98_configuration/,huangfengzi,1415885784,"For now ,i'm trying to implement the lenet 5 in c++ . While i successfully set up the network structure and for a single input image the output would converge after 30 iteration . The network can't outperform a random guess on the whole mnist data .Longer training time makes the performance worse.  I have spent whole week to figure it out but achieved nothing. So ,i need the lenet5 configuration file(about the weight and bias) to check if i had constructed a wrong network . Any help? Thank you very much for your attention.
",2,1,False,self,,,,,
118,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,0,2m6pwc,inbits.com,Efficiency of Predicted Sparseness as a Motivating Model for Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/2m6pwc/efficiency_of_predicted_sparseness_as_a/,fergbyrne,1415892196,,20,2,False,http://b.thumbs.redditmedia.com/aTMHgwYDdB6gKJxwweGQOnHl_fAnebZEuQhx7lbPBtY.jpg,,,,,
119,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,1,2m6wbi,self.MachineLearning,Question: finding associations in disparate data,https://www.reddit.com/r/MachineLearning/comments/2m6wbi/question_finding_associations_in_disparate_data/,sempernullus,1415895857,"I am in the beginning stages of a project that will hopefully find associations between a wide variety of seemingly disparate data.  I had originally wanted to use scikit-learn because of support &amp; documentation.  

However, it seems that Weka is more suited for association rules.  If finding links in data is the goal am I correct in moving to Weka as opposed to scikit-learn?

I appreciate the feedback and help.",4,1,False,self,,,,,
120,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,2,2m72hv,self.MachineLearning,Hellinger Distance Decision Trees,https://www.reddit.com/r/MachineLearning/comments/2m72hv/hellinger_distance_decision_trees/,improbabble,1415899154,"Does anyone have experience with this technique or know if they've been extended to forests?

I deal with lots of imbalanced data in my work so I'm curious if anyone has experience here.

edit: Forgot to include link: 
http://www3.nd.edu/~dial/hddt/
https://www3.nd.edu/~dial/papers/DMKD11.pdf",5,2,False,self,,,,,
121,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,3,2m7bnl,blog.aylien.com,Text and Image Analysis: From pixels to characters and back - A Hybrid Approach (Blog Post),https://www.reddit.com/r/MachineLearning/comments/2m7bnl/text_and_image_analysis_from_pixels_to_characters/,MikeWally,1415903817,,0,13,False,http://b.thumbs.redditmedia.com/RNa_QM8kMFMCSVlMIIpJ2n7F2BpefBuDjr5v2AQHcxQ.jpg,,,,,
122,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,7,2m814h,self.MachineLearning,Clustering by Maximizing the Difference between Feature Covariance?,https://www.reddit.com/r/MachineLearning/comments/2m814h/clustering_by_maximizing_the_difference_between/,Ibarea,1415916710,Is there any clustering algorithm that attempts to find groups of samples that have maximally different feature covariances? Want to make sure I'm not reinventing the wheel here (or doing something really dumb),5,2,False,self,,,,,
123,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,9,2m8gk7,github.com,Do something awesome: mentor the next generation of data scientists,https://www.reddit.com/r/MachineLearning/comments/2m8gk7/do_something_awesome_mentor_the_next_generation/,chrisvoncsefalvay,1415924614,,10,30,False,http://b.thumbs.redditmedia.com/HPRjm9Faj5wzBhiMpMrJnPFqH0ArlITZDsVS0Sm1dRM.jpg,,,,,
124,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,9,2m8i52,self.MachineLearning,Why does my stacked linear SVM perform better than one big model?,https://www.reddit.com/r/MachineLearning/comments/2m8i52/why_does_my_stacked_linear_svm_perform_better/,paralax77,1415925425,"So I am trying to classify data. I have two major groups of features. So I made two models - each for one group.

Then, I use the output of these two models as features for another ( stacked ) model above them.

When I merge these two groups of features into one model, I get lower accuracy. 

Why is it so?",8,3,False,self,,,,,
125,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,9,2m8jm7,github.com,CUDA implementation of word2vec (CBOW),https://www.reddit.com/r/MachineLearning/comments/2m8jm7/cuda_implementation_of_word2vec_cbow/,galapag0,1415926226,,0,9,False,http://a.thumbs.redditmedia.com/FcWKf30jcgJsV1YOowt0R6kwQCGvCAZS9pMWiRfjrr0.jpg,,,,,
126,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,10,2m8q4a,docs.google.com,Enter to win free tickets to www.next.ml (Scalable Machine Learning Conference &amp; Workshops),https://www.reddit.com/r/MachineLearning/comments/2m8q4a/enter_to_win_free_tickets_to_wwwnextml_scalable/,gwulfs,1415929808,,0,14,False,http://b.thumbs.redditmedia.com/L0Jmtqwod0d54PmClWSs3Mr_E9BW7lKuuotUwFO2aiw.jpg,,,,,
127,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,10,2m8r11,devblogs.nvidia.com,Embedded Machine Learning with the cuDNN Deep Neural Network Library and Jetson TK1,https://www.reddit.com/r/MachineLearning/comments/2m8r11/embedded_machine_learning_with_the_cudnn_deep/,harrism,1415930329,,0,6,False,http://b.thumbs.redditmedia.com/U4CqO7jwmwRqt8cxJ8lGauARkCK2gfIiIJl7ayTULEQ.jpg,,,,,
128,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,14,2m9dmr,github.com,node.js neural networks including generalized lstm,https://www.reddit.com/r/MachineLearning/comments/2m9dmr/nodejs_neural_networks_including_generalized_lstm/,Zen_X,1415944022,,4,12,False,http://a.thumbs.redditmedia.com/em1RcgFHZl7g758nCT4JejUsgJ3kLWlsD-ydrbrBvG4.jpg,,,,,
129,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,15,2m9g9a,youtube.com,Deep Learning vs. Neural Networks video (7:52 min),https://www.reddit.com/r/MachineLearning/comments/2m9g9a/deep_learning_vs_neural_networks_video_752_min/,neuromorphics,1415945966,,2,0,False,http://a.thumbs.redditmedia.com/Zy6PJV0WOAL586b94k7XtA7oUGfAy41A8_AnNzfFXY4.jpg,,,,,
130,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,16,2m9mga,github.com,Implementation of Neural Turing Machines,https://www.reddit.com/r/MachineLearning/comments/2m9mga/implementation_of_neural_turing_machines/,mlalma,1415951405,,54,37,False,http://b.thumbs.redditmedia.com/5MXo7RKglAz5F_qFk6j3TWk5W37loUx-VjCDQfa31zI.jpg,,,,,
131,MachineLearning,t5_2r3gv,2014-11-14,2014,11,14,23,2macgy,nuit-blanche.blogspot.fr,"Yes, We Can Locate Philae and Here Is How ...",https://www.reddit.com/r/MachineLearning/comments/2macgy/yes_we_can_locate_philae_and_here_is_how/,compsens,1415976172,,15,30,False,http://b.thumbs.redditmedia.com/hHnJs-swuz4yoD3eRPqP7kXrpy7Gl0hyYNljLfqA5uk.jpg,,,,,
132,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,5,2mbbgn,new.livestream.com,Free Live Streaming from The Machine Learning conference @mlconf,https://www.reddit.com/r/MachineLearning/comments/2mbbgn/free_live_streaming_from_the_machine_learning/,shonburton,1415995200,,0,19,False,http://b.thumbs.redditmedia.com/tRu-aU_otrSEpVHIQJYjP18eQjojkGDnecH6LcJDAog.jpg,,,,,
133,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,6,2mbj2r,self.MachineLearning,"Many experts on artificial intelligence state that when computational power exceeds that of the human brain, we will have computers smarter than humans. But how can we program something smarter than ourselves?",https://www.reddit.com/r/MachineLearning/comments/2mbj2r/many_experts_on_artificial_intelligence_state/,vuarterrr,1415999220,"This is something I've never been able to get my head around. I've heard that currently our supercomputers are about as smart as a ""mentally chellenged cockroach"" (quoting Michio Kaku here) and that some day in the future (if we figure out molecular/quantum computing) our computers will be as smart as a human.

This notion seems backed by many credible people such as Ray Kurzweil, Steven Hawking and the aforemention Michio Kaku, but I don't understand how we could do this. A computer has to programmed, so surely despite its raw computational power it is still is limited by its programming (which is done by a human).

So basically my question is: Even if we have enormous computational power, how can we program a computer to be smarter than ourselves?

Thanks, hope this is clear
",9,0,False,self,,,,,
134,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,9,2mc3kq,self.MachineLearning,Question on using real biological network as artifical neural network architecture,https://www.reddit.com/r/MachineLearning/comments/2mc3kq/question_on_using_real_biological_network_as/,osazuwa,1416010820,"I am trying to use the topology of a real life cell signaling pathway as the architecture in an ANN and then train the model on bioinformatics data.  R is my tool of choice, but I can't find any R package that allows me to actually provide the network structure as an argument to the neural network fitting function, they only let you choose from commonly used models.  The models I am using are typically feed forward, but in some cases have cycles.  The constraint is the R packages are not flexible enough.  I am not new to machine learning, but I am to neural networks and deep learning and all that.  What software tools allow you to pick your own topology?",10,2,False,self,,,,,
135,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,9,2mc6yc,self.MachineLearning,Does self adaptive parameter control exist outside evolutive algorithms?,https://www.reddit.com/r/MachineLearning/comments/2mc6yc/does_self_adaptive_parameter_control_exist/,UannaFF,1416012884,"Hello! i am researching about adaptive and self adaptive parameter control and i found some readings about adaptive control in fields outside evolutive algorithms but i've had no such luck with self adaptive control, the only references i get are applied to evolutive algorithms. Self adaptive control, like i read, enters itself into the individuals of the algorithm, to make the parameter change as they change. My question is, is there some kind of self adaptive parameter control outside evolutive algorithms or is it restricted to them?",2,0,False,self,,,,,
136,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,15,2mcybh,github.com,A Java implementation of ListRank-MF,https://www.reddit.com/r/MachineLearning/comments/2mcybh/a_java_implementation_of_listrankmf/,ammsa,1416032067,,0,4,False,http://b.thumbs.redditmedia.com/GiDEK8MQosTUVy6nmtkJum1dOkTZsa5BV8nWqcVL-uA.jpg,,,,,
137,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,16,2md2yi,imgflip.com,Dynemech AntiVibration Technology for Machines,https://www.reddit.com/r/MachineLearning/comments/2md2yi/dynemech_antivibration_technology_for_machines/,Dynemech,1416036173,,0,1,False,default,,,,,
138,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,17,2md6hz,poudro.com,A simple take on building a personalized radio using curated data,https://www.reddit.com/r/MachineLearning/comments/2md6hz/a_simple_take_on_building_a_personalized_radio/,poudro,1416039962,,0,1,False,http://b.thumbs.redditmedia.com/JPu9ONuNCC01zUtSbyCI0WF2l9Zz5psnfVj6KG5bPiI.jpg,,,,,
139,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,18,2md8u0,gifpal.com,Dynemech #antivibration solutions - VIBRATIONMOUNTSINDIA.COM,https://www.reddit.com/r/MachineLearning/comments/2md8u0/dynemech_antivibration_solutions/,Dynemech,1416042749,,0,1,False,default,,,,,
140,MachineLearning,t5_2r3gv,2014-11-15,2014,11,15,22,2mdn20,self.MachineLearning,What direction ought a math grad student take?,https://www.reddit.com/r/MachineLearning/comments/2mdn20/what_direction_ought_a_math_grad_student_take/,sillypantstoan,1416059363,"Hi everyone. I was hoping for a little advice. I'm a math grad student so this post is a little beyond the ""what maths do I need"" posts that I see, as I've done graduate linear, numerical, graph theory, etc. I need to pick a specific direction/adviser and I'd like it to be relevant to machine learning. My choice is between algorithms and functional analysis (the functional analysis prof was great to talk to when I was working through the SVM algorithm). Any thoughts?",11,3,False,self,,,,,
141,MachineLearning,t5_2r3gv,2014-11-16,2014,11,16,1,2me1na,self.MachineLearning,does anyone have some papers about the rosetta mission regarding machine learning and optimal control?,https://www.reddit.com/r/MachineLearning/comments/2me1na/does_anyone_have_some_papers_about_the_rosetta/,Aumanidol,1416070453,"I want to learn about how they figured out stuff like the spacial trajectory,gravity assists,the motor control of the lander.... I found some minor stuff about curiosity, but none about rosetta jet.

any help?",6,9,False,self,,,,,
142,MachineLearning,t5_2r3gv,2014-11-16,2014,11,16,4,2meezv,self.MachineLearning,Anyone interested in working on machine learning projects as a group?,https://www.reddit.com/r/MachineLearning/comments/2meezv/anyone_interested_in_working_on_machine_learning/,segfault343,1416078427,"I think it would be cool if people from this subreddit could work together on side projects and learn from eachother. I've been working on a fashion recommendation system using visual search and I'd love to have people work on it with me. I'm sure other people have projects they are passionate about too and would love teamates, what do you guys think?

Edit: Hey so it looks like I need your emails to invite you to the slack channel, not your slack usernames, could you please pm me that, thanks!",21,12,False,self,,,,,
143,MachineLearning,t5_2r3gv,2014-11-16,2014,11,16,8,2mf2s9,datascienceworld.com,Unicorn Data Scientist Shares his Secrets with You,https://www.reddit.com/r/MachineLearning/comments/2mf2s9/unicorn_data_scientist_shares_his_secrets_with_you/,namurtupues,1416092769,,2,0,False,default,,,,,
144,MachineLearning,t5_2r3gv,2014-11-16,2014,11,16,18,2mge8f,youtube.com,"Automatic bottle blow molding machine, 7000 bottles/hour",https://www.reddit.com/r/MachineLearning/comments/2mge8f/automatic_bottle_blow_molding_machine_7000/,cntic,1416129962,,0,0,False,default,,,,,
145,MachineLearning,t5_2r3gv,2014-11-16,2014,11,16,23,2mgu8m,nlpers.blogspot.com,The myth of a strong baseline,https://www.reddit.com/r/MachineLearning/comments/2mgu8m/the_myth_of_a_strong_baseline/,rrenaud,1416149028,,7,33,False,http://b.thumbs.redditmedia.com/EFdUSdQysh0mL1TEDy-w6JeBxUcuFp9imr83rfxn2Qk.jpg,,,,,
146,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,5,2mhoan,self.MachineLearning,Decision Trees,https://www.reddit.com/r/MachineLearning/comments/2mhoan/decision_trees/,shuchat,1416168252,Why is it possible for a decision tree to have a root of null?,1,1,False,self,,,,,
147,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,6,2mhx65,self.MachineLearning,I have an OpenCL/Python environment set up. What is the ML 'beginner's program' I should attempt to build?,https://www.reddit.com/r/MachineLearning/comments/2mhx65/i_have_an_openclpython_environment_set_up_what_is/,slackermanz,1416173168,"Just a small, easy first attempt at anything ML. Happy to do all the research etc myself, I just need a target to point at.

Any ideas?",10,11,False,self,,,,,
148,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,8,2micx6,self.MachineLearning,"What's the difference between Reinforcement Learning, Online Learning and Active Learning?",https://www.reddit.com/r/MachineLearning/comments/2micx6/whats_the_difference_between_reinforcement/,eshenxian,1416182246,"I am new to Reinforcement Learning.

It seems like they shares some similarities. Online Learning might be a more general idea, and reinforcement learning trying to deal with environment when active learning trying to solve traditional supervised learning problem with human in the loop.

Can anyone help me to distinguish between these concepts?",8,4,False,self,,,,,
149,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,15,2mjdzr,arxiv.org,How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets,https://www.reddit.com/r/MachineLearning/comments/2mjdzr/how_to_scale_up_kernel_methods_to_be_as_good_as/,urish,1416205742,,0,6,False,default,,,,,
150,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,16,2mjje0,vimeo.com,The Data Science Revolution (Jeremy Howard) - Exponential Finance 2014,https://www.reddit.com/r/MachineLearning/comments/2mjje0/the_data_science_revolution_jeremy_howard/,noMotif,1416210808,,6,8,False,http://a.thumbs.redditmedia.com/H8cT3S4ZFMmfFlkyx5bY24FKt6CEpbS92EHVXMc4Nq4.jpg,,,,,
151,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,17,2mjlt9,self.MachineLearning,Is backpropagation algorithm different for both full-connected and local-connected neural network?,https://www.reddit.com/r/MachineLearning/comments/2mjlt9/is_backpropagation_algorithm_different_for_both/,jdxyw,1416213388,"Hi reddit, is backpropagation algorithm different for both full-connected and local-connected neural network? I know how to use the BP for a full-connected network, but I don't know how to use the BP for a local connect network.Or you can call partial-connected network . How to calculate the derivative for those links which are not connected. Any documentation for it?",1,1,False,self,,,,,
152,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,18,2mjnkt,reddit.com,[Journal Club Discussion] Automating music composition and melody generation,https://www.reddit.com/r/MachineLearning/comments/2mjnkt/journal_club_discussion_automating_music/,BeatLeJuce,1416215435,,1,5,False,default,,,,,
153,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,18,2mjnmc,reddit.com,[Journal Club Vote] week 47 / 2014 voting thread,https://www.reddit.com/r/MachineLearning/comments/2mjnmc/journal_club_vote_week_47_2014_voting_thread/,BeatLeJuce,1416215490,,1,2,False,default,,,,,
154,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,20,2mjuyk,toppersworld.com,Cool article: Top 5 Open Source Data Mining Tools,https://www.reddit.com/r/MachineLearning/comments/2mjuyk/cool_article_top_5_open_source_data_mining_tools/,Sergiointelnics,1416223804,,0,0,False,http://a.thumbs.redditmedia.com/fzc57ZbzubtyW5Y62wyRRNZ8lrvvS_MDuKVru_tfsw0.jpg,,,,,
155,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,21,2mjzla,self.MachineLearning,Data collection for handedness classifier using HMMs,https://www.reddit.com/r/MachineLearning/comments/2mjzla/data_collection_for_handedness_classifier_using/,serious_scientist,1416228468,"Hi. I'm working on a paper as part of a school project, where I've built a classifier that uses Hidden Markov Models to determine whether an individual is right or left-hand dominant, based on how they type (through keystroke dynamics). It's all going pretty well, except that I have too little data from left-handed individuals to work with. So, if you're left handed and feel like helping out, I'd appreciate it if you could follow the link below and complete the test (only takes ~1 minute to complete).

https://handednesstest.herokuapp.com/

",14,8,False,self,,,,,
156,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,21,2mk066,self.MachineLearning,How do speech recognition algorithms initially separate a continuous sound wave into discrete chunks?,https://www.reddit.com/r/MachineLearning/comments/2mk066/how_do_speech_recognition_algorithms_initially/,shotbythought,1416228996,"I'm a beginner in the middle of Andrew Ng's Machine Learning class, and its got me thinking about some machine learning applications. Particularly, it's application in speech recognition. I figure that once you have the sound wave in discrete chunks, it becomes like a classification problem we discussed in class. 

However, that's not the case, at least not initially. I'm wondering what types of algorithms are out there for handling continuous streams of data, as this doesn't seem to be covered in the class.",2,3,False,self,,,,,
157,MachineLearning,t5_2r3gv,2014-11-17,2014,11,17,23,2mk4zn,blog.manugarri.com,Article about how to build a Recommendation Engine for Reddit [OC],https://www.reddit.com/r/MachineLearning/comments/2mk4zn/article_about_how_to_build_a_recommendation/,manueslapera,1416233051,,9,46,False,http://b.thumbs.redditmedia.com/DauhlI3lB4en3ikdcEgHml9UpnlY-d92r6QUq51vhjQ.jpg,,,,,
158,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,0,2mkevo,arxiv.org,Detecting Linguistic change across time using change point detection,https://www.reddit.com/r/MachineLearning/comments/2mkevo/detecting_linguistic_change_across_time_using/,viveksck,1416239372,,1,2,False,default,,,,,
159,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,1,2mkkfq,jvns.ca,Fun with machine learning: logistic regression!,https://www.reddit.com/r/MachineLearning/comments/2mkkfq/fun_with_machine_learning_logistic_regression/,bork,1416242187,,0,2,False,http://b.thumbs.redditmedia.com/RHWZ0nz-VG3jFtszBBoUpXPVCquP47l4Qm-Awc_DTpY.jpg,,,,,
160,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,1,2mklum,self.MachineLearning,Identification vs Classification: How to do 1:1 Face Verification,https://www.reddit.com/r/MachineLearning/comments/2mklum/identification_vs_classification_how_to_do_11/,joe-murray,1416242875,"Hey guys, I'm having an issue with an ML problem. Essentially I'm trying to code a facial ""verification"" system. Typically face recognition models use 1:N matching which means;

- We have a set of N identities
- Given image, which identity is represented in the image?

I'm trying to do a 1:1 verification system, which works like this:

- We have one identity
- Given image, does this image depict the identity?

You can see that the first system *assumes* a match occurs and tries to pin it down to the correct person, whereas the second system is checking *if* a match exists.

The difficulty is this -- ML does not work very well for this problem. Typically a classification algorithm uses training data which attempts to describe the difference between classes (e.g. this information represents THIS class, that information represents THAT class, etc), and this is analogous to the 1:N face recognition task. Face verification, on the other hand, only uses a training set with one individual class (there's no class split to discern) and attempts to ""learn"" to identify that class, but since it doesn't have contrasting information it's not very good at doing that. 

My question is this: **what kinds of techniques, if any, exist for identification instead of classification?** If I can't find any good techniques my tentative idea is to mask the 1:1 verification as a 1:N matching problem (so essentially I'll just get a bunch of random face images and collectively describe them as the ""other class""), but this requires a very large data set that I don't currently have, so I'd like to avoid that option if possible.",6,1,False,self,,,,,
161,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,2,2mkq47,self.MachineLearning,Ordered classification?(small medium large),https://www.reddit.com/r/MachineLearning/comments/2mkq47/ordered_classificationsmall_medium_large/,Divided_Pi,1416244955,"Hey guys,

I know I've read about this type of problem before but my google-fu is failing me. Anyone know what the formal statement is of a problem like this? Preferably without defining before hand what falls into each category.

And can this be extended to problems where you want to classify something as ""good, bad, ugly""?",1,1,False,self,,,,,
162,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,4,2ml8pw,stickywinner.com,End-to-End Predictive Model in AzureML using Linear Regression,https://www.reddit.com/r/MachineLearning/comments/2ml8pw/endtoend_predictive_model_in_azureml_using_linear/,tredkar,1416254247,,0,2,False,http://b.thumbs.redditmedia.com/w-XzwP0REygLCn5J-IlOvUuEtfaUC1S4vQp4PPnpbXM.jpg,,,,,
163,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,5,2ml977,self.MachineLearning,"1 CSV, 30 stories",https://www.reddit.com/r/MachineLearning/comments/2ml977/1_csv_30_stories/,cast42,1416254468,,3,0,False,default,,,,,
164,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,5,2mlg57,self.MachineLearning,"Detecting duplicate posts, comments, etc.",https://www.reddit.com/r/MachineLearning/comments/2mlg57/detecting_duplicate_posts_comments_etc/,[deleted],1416257887,"https://github.com/emmjaykay/reddit-faqizer

It's a prototype of a feature I think would be neat. I put it together after the Amy Poelher AMA.

Maybe I'll turn it into a bot?

Any pointers or suggestions for improvement would be appreciated.",1,4,False,default,,,,,
165,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,7,2mlo8b,self.MachineLearning,Need help getting dropout to work,https://www.reddit.com/r/MachineLearning/comments/2mlo8b/need_help_getting_dropout_to_work/,spurious_recollectio,1416261886,"I've been building my own NN library simply because I find this is the easiest way to learn about things.  My approach has been to add every feature under the sun (dropout, momentum, adaptive learning, regularization, GPU, etc...).  I've been using/testing it primarily on a Kaggle competition where there is a relatively small amount of labelled data (15k).

So far things work quite well for standard training but I've never successfully trained a network using dropout.  With the latter I either get:

1 - Stuck at around 60-70% training/validation error.

2 - Numerical instabilities (NaN's)

3 - Stuck in a bias-dominated regime.

By (3) I mean the network will always select a particular output (from a softmax layer) for all validation cases.  The actual choice of output will change each epoch but the network somehow gets stuck being dominated by bias.

I've tried a whole bunch of things:

- Increase learning rate: leads either to (2) or (3).  this happens already at smallish rates like 0.5.
- Reclu or channelout layers: problem happens for both
- Momentum: usually makes things worse
- Fan-in regulator: makes some weights vanishingly small (1e-100) and tends to lead to bias domination (but avoids numerical instabilities)
- L2 regulator: doesn't do too much.
- Adaptive per-weights learning rate: tends to lead to more numerical instabilities.  this really speeds up normal learning but with dropout it seems to exacerbate (2) and (3).
- Random amount of dropout: between e.g. 0.3-0.5 still gives all the same problems.

I realize that the best thing to do would probably be to retreat to a more standard dataset like MNIST (which I will probably do) but the thing is that without dropout I can acheive 80% accuracy on the validation set (and 100% on the training set) so its clear that there's enough data to do better.

To give more details I'm using a relatively large network (4-5 layers of size 200 with inputs of size ~50 and output ~10) using either Reclu or channelout and without any pretraining.  

I refresh the choice of dropout units every minibatch (size 10-150) and I also do 0.2 dropout on the input layer.

I do intend to go back to MNIST and compare to some published results but if anyone can provide some thoughts or inspirations I would really appreciate it!
",19,6,False,self,,,,,
166,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,7,2mlqiq,arxiv.org,[arxiv] A Multiplicative Model for Learning Distributed Text-Based Attribute Representations,https://www.reddit.com/r/MachineLearning/comments/2mlqiq/arxiv_a_multiplicative_model_for_learning/,improbabble,1416262983,,1,2,False,default,,,,,
167,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,7,2mlqza,self.MachineLearning,TWOO NEW ML/ANN ALGHORITMS,https://www.reddit.com/r/MachineLearning/comments/2mlqza/twoo_new_mlann_alghoritms/,BojanPLOJ,1416263209,"Please see my two new algorithms: ""bipropagation"" and ""Border pairs method"".

https://www.novapublishers.com/catalog/product_info.php?products_id=50224

What do you think about it? Thanks, Bojan PLOJ PhD",0,0,False,self,,,,,
168,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,12,2mmmh4,self.MachineLearning,Hierarchical Classification for POS tagging,https://www.reddit.com/r/MachineLearning/comments/2mmmh4/hierarchical_classification_for_pos_tagging/,tysonwil,1416280361,"I'm working with classification algorithms, especially in the context of multiple tags and hierarchically structured tags (where coarse-grained predictions are desired when the model cannot confidently support finer-grained decisions). I'm just starting to read about Hierarchical classification - does it seem like an appropriate approach for POS tagging?",2,1,False,self,,,,,
169,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,13,2mmtxh,arxiv.org,Show and Tell: A Neural Image Caption Generator,https://www.reddit.com/r/MachineLearning/comments/2mmtxh/show_and_tell_a_neural_image_caption_generator/,downtownslim,1416284674,,2,30,False,default,,,,,
170,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,17,2mndld,self.MachineLearning,Please give me your insight! Does BeyondCore utilize some kind of magic or just a bunch of neatly collected 'simple' models and analyses converted to CxO people? (x-post from datascience),https://www.reddit.com/r/MachineLearning/comments/2mndld/please_give_me_your_insight_does_beyondcore/,sloby,1416299843,"The site: http://www.beyondcore.com/

IMO you can't simply automate completely a data scientist's job but for a company that's considering hiring a smaller team, it might reduce the size with a pair or two.

I'm pretty sure it has a couple of hidden and clever solutions but overall it seems that it 'just' utilizes the basic models and statistical analyses that most of us already know. 

The other thing I'm nto sure is whether it requires some kind of preconfiguration or you just let it run in the wild? IMO certain business areas have the typical patterns (accounting, manufacture, etc) that they're prepared for and they tailor the system for the specific database of the customer as the system itself is very flexible. Under the hood it has to have some kind of knowledge that it's dealing with products and product categories and not employees and their evaluation data.

I really like the concept of the whole package and I'm really curious about the opinion of yours! What do you think?",0,0,False,self,,,,,
171,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,18,2mnheb,self.MachineLearning,Why don't people use video game engines to generate labelled datasets ?,https://www.reddit.com/r/MachineLearning/comments/2mnheb/why_dont_people_use_video_game_engines_to/,Schlagv,1416303948,"Scientists in the ML community always use real world pictures and find very hard to label data.

With video game engines we can generate infinite amounts of data, with pictures in different angles, light, shading, noise, shader effects, occlusion, transparency, ...

With shaders, it is also easy to generate pixel accurate labels for segmentation tasks, face recognition in scenes with people occluding each other, ...

Why don't we see ML scientists using Unity or Unreal Engine to create infinite amounts of data ?",19,25,False,self,,,,,
172,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,21,2mnqj8,googleresearch.blogspot.co.uk,A picture is worth a thousand (coherent) words: building a natural description of images. (Google Research Blog),https://www.reddit.com/r/MachineLearning/comments/2mnqj8/a_picture_is_worth_a_thousand_coherent_words/,abashinyan,1416313528,,20,84,False,http://a.thumbs.redditmedia.com/bqOdmGanhnujvTVn3XfmadXr0qe8WFsFy2p1FifhOE8.jpg,,,,,
173,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,23,2mo144,self.MachineLearning,smartphone ML algorithm to prevent texting while driving,https://www.reddit.com/r/MachineLearning/comments/2mo144/smartphone_ml_algorithm_to_prevent_texting_while/,pseudocoder1,1416321612,"Hi,
  the recent google patent on ~""any and all ML algorithms on smartphones"" gave me an idea for the texting while driving problem.  I work in telecom and have thought about this problem.  We can detect that the phone is in motion with GPS at the phone or radio strength information at the switching center that the towers connect to.  The problem is, someone can be riding in a car and they still need to be able to text.

  One difference between the scenario of a driver texting vs a passenger texting is that the driver is likely not typing as they normally do while not driving.  In particular the timing information between keystrokes could be used to detect one-handed typing which is characteristic of texting while driving.  The phone screen could then be made to go blank until the phone's GPS shows that it has stopped. ",9,0,False,self,,,,,
174,MachineLearning,t5_2r3gv,2014-11-18,2014,11,18,23,2mo2uk,vitalflux.com,Can Software Developers Productivity be Predicted using Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/2mo2uk/can_software_developers_productivity_be_predicted/,svyas,1416322727,,1,0,False,http://b.thumbs.redditmedia.com/baTKztvD6S28aibsBB85Pep1geJ9xISf84Nv6qdZuUQ.jpg,,,,,
175,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,1,2mob42,self.MachineLearning,Can anyone give me an intuitive reason why random kitchen sinks work?,https://www.reddit.com/r/MachineLearning/comments/2mob42/can_anyone_give_me_an_intuitive_reason_why_random/,meandtree,1416327492,"http://www.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf

The math in this paper is a little bit over my head, can anyone give an intuition as to why RKS approaches the minimum risk.",3,2,False,self,,,,,
176,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,3,2mop7b,self.MachineLearning,What are the best ways for text classification if the provided training set is very small?,https://www.reddit.com/r/MachineLearning/comments/2mop7b/what_are_the_best_ways_for_text_classification_if/,morgandix,1416334543,"I have 3 categories (low,mid,high), and i have 75;27;12 documents respectively. The documents are fairly short (think around 80-150 words)

I am using WEKA and what I tried to do was the following:

* reformat using *StringToWordVector* filter
* cut down 3-3-3 samples for testing purposes
* *SMOTE* the remaining documents to have same number of instances from all three categories (72-72-72 from 72-24-9) 

after classifying with several algorithms (iBK, SMO, j48, bayes) i had very little success with testing on the previously extracted set. 

The performance was around 20-40%, only because the classifiers tended to classify the documents as ""low"". I am pretty sure because there simply wasn't enough initial instances of ""mid"" and ""high"" so the classifiers couldn't learn a wide enough range of those categories.

Is there anything i could try to improve the performance?

Thanks",11,3,False,self,,,,,
177,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,4,2mowzd,youtube.com,European ML Summer School videos (Iceland 2014),https://www.reddit.com/r/MachineLearning/comments/2mowzd/european_ml_summer_school_videos_iceland_2014/,xamdam,1416338305,,0,15,False,http://b.thumbs.redditmedia.com/YVAFkP08K7SH1E3lfe9apz_3ZQ5y2kBdym9mW5XBetA.jpg,,,,,
178,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,4,2mp03f,hakkalabs.co,Scaling Language Understanding via Joint Multilingual Learning,https://www.reddit.com/r/MachineLearning/comments/2mp03f/scaling_language_understanding_via_joint/,dot_2,1416339786,,0,3,False,http://b.thumbs.redditmedia.com/Xd9mdeaG6rxmJxJPeR5lkY8hUjNUXSfKdNDB0im-pEo.jpg,,,,,
179,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,5,2mp52q,self.MachineLearning,Pole balancer works,https://www.reddit.com/r/MachineLearning/comments/2mp52q/pole_balancer_works/,toki78,1416342255,"Hi !
I coded a Q(lambda) and SARSA(lambda) agent.

http://toki78.github.io/
https://github.com/toki78/JuRLs/tree/master/JuRLs/src/jurls

-Thorsten",0,0,False,self,,,,,
180,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,6,2mpbix,self.MachineLearning,HHMM in R or Python,https://www.reddit.com/r/MachineLearning/comments/2mpbix/hhmm_in_r_or_python/,[deleted],1416345344,"Does anyone know of any examples of HHMM in R or Python.

I've googled but didn't have much luck. It may be that HHMMs have fallen out of favor, can anyone point me towards more reading on why?

Thanks!",4,1,False,default,,,,,
181,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,8,2mprmt,self.MachineLearning,Hierarchical Hidden Markov Model in R or Python,https://www.reddit.com/r/MachineLearning/comments/2mprmt/hierarchical_hidden_markov_model_in_r_or_python/,jonathan881,1416353363,"Does anyone know of any examples of HHMM in R or Python.
I've googled but didn't have much luck. It may be that HHMMs have fallen out of favor, can anyone point me towards more reading on why?

Thanks!

I deleted my last question regarding hhmm because the title was causing confusion. 
",6,8,False,self,,,,,
182,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,9,2mq15j,blogs.technet.com,Rapid Progress in Automatic Image Captioning,https://www.reddit.com/r/MachineLearning/comments/2mq15j/rapid_progress_in_automatic_image_captioning/,vkhuc,1416358413,,0,4,False,http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg,,,,,
183,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,9,2mq1vu,gidi-chain.com,"Zhejiang Gidi industrial chain Co.,Ltd.",https://www.reddit.com/r/MachineLearning/comments/2mq1vu/zhejiang_gidi_industrial_chain_coltd/,gidichain,1416358775,,0,0,False,default,,,,,
184,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,14,2mqu7v,self.MachineLearning,Dirichlet process mixture models for clustering using R?,https://www.reddit.com/r/MachineLearning/comments/2mqu7v/dirichlet_process_mixture_models_for_clustering/,[deleted],1416375011,"I was following along with this blog post - http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/ where he explains clustering through the used of DPMM's, and at the bottom he has a cool example of clustering the McDonald's menu.

He just simply uses scikit-learns Dirichlet Process Gaussian Mixture Model in Python to perform this clustering. I was checking to see if there was a package available in CRAN to perform similar function in R, and the closest I found was dpmixsim, which only works on a vector, so we would not be able to give it a data frame.


Does anyone know of a similar function in R that would handle data frames? (If not, I'll just export my data from R to python, but it seems like there should be something...)

",3,14,False,default,,,,,
185,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,15,2mqx4i,wingshore.wordpress.com,Wingshore | What if machines could talk?,https://www.reddit.com/r/MachineLearning/comments/2mqx4i/wingshore_what_if_machines_could_talk/,[deleted],1416377055,,0,0,False,default,,,,,
186,MachineLearning,t5_2r3gv,2014-11-19,2014,11,19,20,2mrgax,self.MachineLearning,What is Multivariate Analysis?,https://www.reddit.com/r/MachineLearning/comments/2mrgax/what_is_multivariate_analysis/,doublebyte1,1416396157,"Multivariate analysis focuses on the results of observations of many different variables for a number of objects. In this sense most machine learning methods lie within this group; I cannot imagine performing regression, without having more than one variable at a time.
However, what about methods such as clustering and anomaly Detection? Although we could easily extend the concepts to a high-dimensional space, and perform a multi-dimensional clustering (for instance SOM), I believe most clustering and anomaly applications work for a single-variable. In that particular case, could they be consider univariate methods?",8,0,False,self,,,,,
187,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,1,2ms9mu,self.MachineLearning,Top Preference distance metric,https://www.reddit.com/r/MachineLearning/comments/2ms9mu/top_preference_distance_metric/,pwoolf,1416416287,"I'm working on a project where I have multiple users who have each ranked a long list of items to identify their top 5.  If I focus on one user (the focus user), what is a good metric for finding other users who likely have the most similar preferences.


For example, imagine that the focus user has expressed that their top 5 preferences are:

[3, 7, 2, 11, 322]

In this order (3 being the most preferred)

User 2 has preferences:
 
[2, 7, 103, 13, 3]

User 3 has preferences:

[7, 3, 86, 44, 322]

I'm trying to find a metric that will allow me to see which users have the closest preferences to the focus user.   Because the goal of the analysis is to find the most favored preferences, ideally there should be a weighting toward the top.  i.e. if the test user most prefers item 3, then that scores more highly than if the test user also prefers item 322 in position 5.  

Ideally too it should be fuzzy, so if a test user prefers item 3 in position 2, while the focus user prefers item 3 in position 1, this should count for something.

Simple methods of counting overlapping sets between the target and test user work for a first approximation, but these don't account for the relative rankings nor do they give additional priority to the higher ranked items.

This problem becomes worse when I have a longer list of preferences, say the top 50 items.  I really care if two users have a common top 5, but don't care so much if they have an identical ordering of their 40th-50th list--these are much less important. 

Any ideas?
",9,5,False,self,,,,,
188,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,2,2msbry,blogs.starcio.com,How Artificial Intelligence Will Solve IoT's Big Data Challeges,https://www.reddit.com/r/MachineLearning/comments/2msbry/how_artificial_intelligence_will_solve_iots_big/,nyike,1416417361,,0,0,False,http://b.thumbs.redditmedia.com/aWUgoicAK3_v491kP-iDRTqbFt7xXV6UE0ynUSjXYZo.jpg,,,,,
189,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,4,2mstuc,wingshore.wordpress.com,Linear Regression in One Variable- Gradient Descent Contd.,https://www.reddit.com/r/MachineLearning/comments/2mstuc/linear_regression_in_one_variable_gradient/,[deleted],1416426356,,3,0,False,default,,,,,
190,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,5,2mswlz,youtube.com,Introducing H2O Flow,https://www.reddit.com/r/MachineLearning/comments/2mswlz/introducing_h2o_flow/,chrisamichaels,1416427746,,0,0,False,default,,,,,
191,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,6,2mt67p,businesswire.com,H2O.ai Announces Availability of New Flow and Play Products at H2O World | Business Wire,https://www.reddit.com/r/MachineLearning/comments/2mt67p/h2oai_announces_availability_of_new_flow_and_play/,chrisamichaels,1416432506,,0,0,False,default,,,,,
192,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,19,2mv5lp,wingshore.wordpress.com,Gradient Descent For Linear Regression,https://www.reddit.com/r/MachineLearning/comments/2mv5lp/gradient_descent_for_linear_regression/,[deleted],1416478776,,3,0,False,default,,,,,
193,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,19,2mv68t,wingshore.wordpress.com,Wingshore | What if machines could talk?,https://www.reddit.com/r/MachineLearning/comments/2mv68t/wingshore_what_if_machines_could_talk/,[deleted],1416479475,,3,0,False,default,,,,,
194,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,19,2mv6rt,wingshore.wordpress.com,Linear Regression with One Variable -Model Representation!,https://www.reddit.com/r/MachineLearning/comments/2mv6rt/linear_regression_with_one_variable_model/,[deleted],1416480104,,1,0,False,default,,,,,
195,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,21,2mvcrr,twitter.com,Wingshore (@WingshoreML),https://www.reddit.com/r/MachineLearning/comments/2mvcrr/wingshore_wingshoreml/,[deleted],1416486391,,0,0,False,default,,,,,
196,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,21,2mvdgd,self.MachineLearning,The perfect bend of Machine Learning data can initiate a greater future. I feel humans and Machines can co-exists!,https://www.reddit.com/r/MachineLearning/comments/2mvdgd/the_perfect_bend_of_machine_learning_data_can/,[deleted],1416487138,,0,1,False,default,,,,,
197,MachineLearning,t5_2r3gv,2014-11-20,2014,11,20,22,2mvj0s,cs.stanford.edu,Deep Visual-Semantic Alignments for Generating Image Descriptions,https://www.reddit.com/r/MachineLearning/comments/2mvj0s/deep_visualsemantic_alignments_for_generating/,vrld,1416491674,,12,135,False,http://b.thumbs.redditmedia.com/18MmI2Y3ckpi1oS4r4ekpdjsDl9saZJXiLPTIE_f3wg.jpg,,,,,
198,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,0,2mvqr9,self.MachineLearning,Persistent Contrastive Divergence and Dropout,https://www.reddit.com/r/MachineLearning/comments/2mvqr9/persistent_contrastive_divergence_and_dropout/,jostmey,1416496723,"Does anyone know how to correctly use Dropout with Persistent Contrastive Divergence (PCD) for a Boltzmann machine? Is Dropout supposed to be used when the Persistent States are updated? Or should the weights be scaled down according to the Dropout probability each time the Persistent States are passed through the neural network.

Thanks!",2,1,False,self,,,,,
199,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,1,2mvxl2,blog.algorithmia.com,Machine Learning Showdown: Apache Mahout vs Weka,https://www.reddit.com/r/MachineLearning/comments/2mvxl2/machine_learning_showdown_apache_mahout_vs_weka/,moriara,1416500532,,3,0,False,http://b.thumbs.redditmedia.com/TKkK9v7JMpwDC2wLOjBbIsU52kDH5Jsdyd9XYSqOkwc.jpg,,,,,
200,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,1,2mw1vq,ncbi.nlm.nih.gov,Emotion Detection in Suicide Notes using Maximum Entropy Classification [pdf],https://www.reddit.com/r/MachineLearning/comments/2mw1vq/emotion_detection_in_suicide_notes_using_maximum/,jonathan881,1416502720,,3,6,False,default,,,,,
201,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,3,2mweai,machinelearningmastery.com,Understand Your Problem With Exploratory Data Analysis,https://www.reddit.com/r/MachineLearning/comments/2mweai/understand_your_problem_with_exploratory_data/,jasonb,1416508962,,0,0,False,http://a.thumbs.redditmedia.com/BduHILUOjuWxfEBLHrI5QCDzadV76Wq64JK95XMiet0.jpg,,,,,
202,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,4,2mwgz3,self.MachineLearning,How to make use of weakly predictive features when the predictive model appears to be better without them.,https://www.reddit.com/r/MachineLearning/comments/2mwgz3/how_to_make_use_of_weakly_predictive_features/,[deleted],1416510310,"I have an interesting problem where if I remove half of the features in my feature set (i.e. go from 40 to 20), I get a significantly more accurate model (a random forest) than I would if I kept those features in.  I get a 40% reduction in Log loss and close to a factor 3 improvement in AUC loss by doing this. This seems odd however because the features I removed have some predictive value not accounted for by other features (unless there are very strange correlations I am unaware of).  My question is, how can I make use of these features? Are random forests bad at making use of weakly predictive features? Here are my current ideas for how I can tackle the problem: I could just ignore the weak attributes with a higher probability when making decision tree splits.  I could try building a separate model that includes the weak attributes and blend that with my current model.  Any ideas, comments, or criticisms will be greatly appreciated.",0,1,False,default,,,,,
203,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,5,2mwshx,self.MachineLearning,How can I make use of weakly predictive features when the predictive model appears to be better without them?,https://www.reddit.com/r/MachineLearning/comments/2mwshx/how_can_i_make_use_of_weakly_predictive_features/,AlexTHawk,1416516138,"
I have an interesting problem where if I remove half of the features in my feature set (i.e. go from 40 to 20), I get a significantly more accurate model (a random forest) than I would if I kept those features in. I get a 40% reduction in Log loss and close to a factor 3 improvement in AUC loss by doing this. This seems odd however because the features I removed have some predictive value not accounted for by other features (unless there are very strange correlations I am unaware of). My question is, how can I make use of these features? Are random forests bad at making use of weakly predictive features? Here are my current ideas for how I can tackle the problem: I could just ignore the weak attributes with a higher probability when making decision tree splits. I could try building a separate model that includes the weak attributes and blend that with my current model. Any ideas, comments, or criticisms will be greatly appreciated.
",3,1,False,self,,,,,
204,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,5,2mwttq,self.MachineLearning,Using PyBrain after training a network,https://www.reddit.com/r/MachineLearning/comments/2mwttq/using_pybrain_after_training_a_network/,anonymouse72,1416516745,"*(Not sure whether I should post this here or on /r/statistics, so I'm trying here.)*

I'm using [PyBrain](http://pybrain.org/) to create a neural network. I'm still pretty new to neural networks and their concepts. I've so far only run train() over the network, as trainUntilConvergence() is taking an incredibly long time (which is to be expected, it's a decently-sized dataset). My question is probably pretty simple--**how do I utilize the network after it has been trained?** I'm only training the network over 80% of the data, and I believe my next step is to actually test the network on the remaining data, but I'm not sure where to go from here, either statistically or technically regarding functions within PyBrain itself.

The specifics are available in my [previous post](https://www.reddit.com/r/statistics/comments/2m3u7x/coding_a_neural_network/) (especially in the comments) on /r/statistics, if you'd like.",9,2,False,self,,,,,
205,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,6,2mx2e6,gigaom.com,Deep learning might help you get an ultrasound at Walgreens,https://www.reddit.com/r/MachineLearning/comments/2mx2e6/deep_learning_might_help_you_get_an_ultrasound_at/,agrman,1416520639,,0,0,False,http://b.thumbs.redditmedia.com/OMLd7a9bAejZgYuWJ0r-UogLwbUl6ldWVpX1FHY539E.jpg,,,,,
206,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,10,2mxqat,self.MachineLearning,Are there any open source implementations of sentence generator using neural networks?,https://www.reddit.com/r/MachineLearning/comments/2mxqat/are_there_any_open_source_implementations_of/,m3wm3wm3wm,1416532353,"Recently there have been attempts in using Convolutional and Recursive Neural Networks in generating sentences in natural language. Eg:

http://nal.co/papers/Kalchbrenner_DCNN_ACL14

http://cs.stanford.edu/people/karpathy/deepimagesent/ (This one says code coming soon, but it never does. Academic pages are not the most reliable ones.)

Anyone aware of an open source implementation for these?
",3,2,False,self,,,,,
207,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,11,2my1xx,github.com,"Mocha.jl, a Deep Learning Library for Julia inspired by Caffe",https://www.reddit.com/r/MachineLearning/comments/2my1xx/mochajl_a_deep_learning_library_for_julia/,jfsantos,1416538436,,3,30,False,http://b.thumbs.redditmedia.com/DBZvmaceqkLcqphj4S3-QYAfl7mJyvyWeOE3DUAK4tE.jpg,,,,,
208,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,16,2myqy3,opendata.cern.ch,CERN Open Data Portal,https://www.reddit.com/r/MachineLearning/comments/2myqy3/cern_open_data_portal/,vamc19,1416554185,,4,33,False,http://b.thumbs.redditmedia.com/KJjUTwQhSSS8QbhYFtzdM_G-xnA9H98yFnY52W6ghzQ.jpg,,,,,
209,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,18,2mz0dd,nuit-blanche.blogspot.com,Provable Bounds for Learning Some Deep Representations (x-post: r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/2mz0dd/provable_bounds_for_learning_some_deep/,compsens,1416562659,,2,1,False,http://b.thumbs.redditmedia.com/MVv_HNXHL7XFB9v-31K9kUB4PS-zSUbUDFF484Fz_oo.jpg,,,,,
210,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,18,2mz13s,velos0.ltt.mech.ntua.gr,Artificial Neural Networks for the Solution of Inverse Problems,https://www.reddit.com/r/MachineLearning/comments/2mz13s/artificial_neural_networks_for_the_solution_of/,Sergiointelnics,1416563472,,0,0,False,default,,,,,
211,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,18,2mz187,github.com,High performance blackbox stochastic optimization C++11 library with Python bindings and support for surrogate models -- Suggestions and feedback on hard ML problems are very much welcome,https://www.reddit.com/r/MachineLearning/comments/2mz187/high_performance_blackbox_stochastic_optimization/,pilooch,1416563611,,0,20,False,http://b.thumbs.redditmedia.com/FHejrilv5lO9ZUWwnZkVmYfTgThmODJ2SF9q_fayYTs.jpg,,,,,
212,MachineLearning,t5_2r3gv,2014-11-21,2014,11,21,23,2mzha2,self.MachineLearning,Identifying multiconvex functions,https://www.reddit.com/r/MachineLearning/comments/2mzha2/identifying_multiconvex_functions/,invaluable,1416579329,"I am working with a function that I suspect is multiconvex. Does anyone know how I can identify whether it is multiconvex or not? I cannot check the second derivatives with respect to each variable analytically; is it a good idea to estimate the second derivatives numerically?

Further, do you have any good sources for reading up on the properties of multiconvex functions?",12,6,False,self,,,,,
213,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,0,2mzpz9,self.MachineLearning,Industrial multivariate time series analysis recommendations,https://www.reddit.com/r/MachineLearning/comments/2mzpz9/industrial_multivariate_time_series_analysis/,MLnewbie,1416584675,"Hi r/machinelearning,

I have a problem that I am looking at where there are 50 or so sensors that are recording data every minute. One of the sensors is the variable of interest (subcool, a temperature difference), 6 of the sensors relate to variables that can be changed by an operator in order to keep the variable of interest in control (steam, choke, gas rates), and the rest of the sensors monitor the environment (temperatures, pressures). The goal is to better predict and control the variable of interest. The time lag between the changing of operator controlled variables and the variable of interest can vary depending on the state of the environment. The time to action right now is usually hourly, and an out of control event could take a day or two to be brought back in control.

Does anyone have any suggestions for how to attack this as a machine learning problem? What kind of preprocessing will I need to do? I have seen HMMs suggested in a few places, or the creation of time independent features using sliding time windows.
Im fairly new to this field, so any suggestions will help. Thanks!
 


Some public reference documents that I found on the subject:

[Real-Time Optimization of SAGD Operations](http://www.weatherford.com/dn/WFT239492)

[Cointerpretation of Flow Rate-Pressure-Temperature Data from Permanent Downhole Gauges](http://cs229.stanford.edu/proj2013/LiTian-CointerpretationOfFlowRatePressureTemperatureDataFromPermanentDownholeGauges.docx.pdf)
",2,2,False,self,,,,,
214,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,2,2mzz5c,self.MachineLearning,LIBSVM (nonlinear regression with e-svr using linear kernel),https://www.reddit.com/r/MachineLearning/comments/2mzz5c/libsvm_nonlinear_regression_with_esvr_using/,__null__,1416589751,"In what cases is libsvm supposed to returned [nan] as the predicted values of nonlinear regression  (with e-svr using linear kernel)?

Is there a faq available ?",8,5,False,self,,,,,
215,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,3,2n05ac,self.MachineLearning,Is ImageNet still useful?,https://www.reddit.com/r/MachineLearning/comments/2n05ac/is_imagenet_still_useful/,rmlrn,1416592914,"So I saw a talk by Matt Zeiler of Clarifai, ""Visualizing and Understanding Deep Neural Networks"", which showed some nice visualizations of what the deep layers of convnets are firing on.

According to him, the neurons in the deepest layers act as ""object recognizers"", corresponding to semantic classes.

One thing he showed/said particularly stood out to me, a ""human face"" filter which appeared in the deep layers, despite faces not being a class in the ImageNet labels. He said that this is because it is useful in distinguising the class ""Neck Brace"".

I wonder if this is a problem with the method of training these networks, and the ImageNet dataset in particular: the annotations are ambigous and often arguably incorrect, so the networks may be learning concepts which are only associated with the labels by statistical correlations of this particular dataset, and might generalize poorly to real world images.

I'm sure that the big companies have developed better annotation methods and training sets, but academic efforts still seemed focused on this benchmark. Has progress reached a point where this may be counterproductive.",1,1,False,self,,,,,
216,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,3,2n05xg,nuit-blanche.blogspot.com,CKN: Convolutional Kernel Networks implementation (x-post r/CompressiveSensing ),https://www.reddit.com/r/MachineLearning/comments/2n05xg/ckn_convolutional_kernel_networks_implementation/,compsens,1416593225,,0,17,False,http://b.thumbs.redditmedia.com/sYFcIDWzfXzcFC7Lb6Ecm3zyqQAylVPmL6ecknkVlms.jpg,,,,,
217,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,7,2n1036,self.MachineLearning,Basic Question from NLP Example,https://www.reddit.com/r/MachineLearning/comments/2n1036/basic_question_from_nlp_example/,kylecares,1416607989,"Hi All,

I'm teaching myself the basics of NLP in Python with the assistance of NLTK.

I have been teaching myself from the example here (http://andybromberg.com/sentiment-analysis-python/) with source code here (https://github.com/abromberg/sentiment_analysis_python/blob/master/sentiment_analysis.py).  There were several problems with the example being outdated, such as their use f the print function without the parends.  I have fixed that.

For some reason, in the example the code:

def find_best_words(word_scores, number):
    best_vals = sorted(word_scores.iteritems(), key=lambda (w, s): s, reverse=True)[:number]
    best_words = set([w for w, s in best_vals])
    return best_words

Has a syntax error saying:

best_vals = sorted(word_scores.iteritems(), key=lambda(w, s): s, reverse=True)[:number]
                                                          ^
SyntaxError: invalid syntax

Any ideas?

Thanks!",6,1,False,self,,,,,
218,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,7,2n120e,geoffboeing.com,Clustering to Reduce Spatial Data Set Size with Python: a Battle of the Algorithms,https://www.reddit.com/r/MachineLearning/comments/2n120e/clustering_to_reduce_spatial_data_set_size_with/,gboeing,1416609008,,1,1,False,http://a.thumbs.redditmedia.com/aow4sX-gIsfAMra-o57n4QNTyyFsC5LwK4ybDNxEer8.jpg,,,,,
219,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,10,2n1ixh,github.com,Brushfire: Distributed decision tree ensemble learning in Scala,https://www.reddit.com/r/MachineLearning/comments/2n1ixh/brushfire_distributed_decision_tree_ensemble/,hrb1979,1416619139,,0,15,False,http://b.thumbs.redditmedia.com/bF76SO5CnhNQqN-hl3-_1HmCU78867wBXICHdgbQ9sE.jpg,,,,,
220,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,14,2n21ug,renasmakina.com,For Many Kind Of Production Machinery Click the Link Below,https://www.reddit.com/r/MachineLearning/comments/2n21ug/for_many_kind_of_production_machinery_click_the/,downahar,1416632546,,0,0,False,default,,,,,
221,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,22,2n2thq,self.MachineLearning,[Question] Sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/2n2thq/question_sentiment_analysis/,__learningCS,1416662625,"What are the different approaches to do sentiment analysis, I found dictionary-based approach, Can you name the other different approaches, or list some of the resources that I should look at (papers, books, tutorials, .. ), I am interested in analyzing the political topics.",7,2,False,self,,,,,
222,MachineLearning,t5_2r3gv,2014-11-22,2014,11,22,22,2n2vjx,self.MachineLearning,[Question] content based news ranking,https://www.reddit.com/r/MachineLearning/comments/2n2vjx/question_content_based_news_ranking/,semmi,1416664678,"what is the current state of the art for news personalization taking into account news content _and_ freshness (not just collaborative filtering)? 
I.e. what is the  best thing reddit could do that is not doing? 

It seems years ago there were people trying to implement this for reddit but it never happened in the end. 

",1,0,False,self,,,,,
223,MachineLearning,t5_2r3gv,2014-11-23,2014,11,23,2,2n3bvs,self.MachineLearning,Need advice about Recurrent neural networks,https://www.reddit.com/r/MachineLearning/comments/2n3bvs/need_advice_about_recurrent_neural_networks/,toanvh,1416676709,"Hi guys,
I just started learning RNNs, and got confused. I don't know exactly which appropriate articles should I begin to read. Can anyone give me some advice?
Thank a lot! :)",13,11,False,self,,,,,
224,MachineLearning,t5_2r3gv,2014-11-23,2014,11,23,9,2n4fu8,iamtrask.github.io,Apache Spark-GPU Cluster Dev in a Notebook,https://www.reddit.com/r/MachineLearning/comments/2n4fu8/apache_sparkgpu_cluster_dev_in_a_notebook/,iamtrask,1416701091,,2,20,False,http://b.thumbs.redditmedia.com/eHGTQnCy-4IS6zqkMS9BCW21agjtpUuSHjaq-PC1aYI.jpg,,,,,
225,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,3,2n6kuo,self.MachineLearning,Libsvm Hyperplane to point distance,https://www.reddit.com/r/MachineLearning/comments/2n6kuo/libsvm_hyperplane_to_point_distance/,[deleted],1416766056,I am using Libsvm in Matlab and wish to see the values between the hyperplane and points to make my own confidence scoring.  I cannot find any info on how to do this anywhere online.  Any help/insights to how to get these values would be greatly appreciated.  (they are definitely calculated during calculating p values with the '-b' flag),1,1,False,default,,,,,
226,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,4,2n6txf,self.MachineLearning,[Question] RBF with kmeans help,https://www.reddit.com/r/MachineLearning/comments/2n6txf/question_rbf_with_kmeans_help/,[deleted],1416771354,"I am implementing an RBF algorithm using k-means to set my gaussian means, and every article I've read tells me that it's good practice to make your variance equal among all your RBFs... Doesn't that interfere with the k-means algorithm? Sorry I can't quite wrap my head around this..",3,1,False,default,,,,,
227,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,6,2n7865,reddit.com,[Journal Club] week 48 / 2014 voting thread,https://www.reddit.com/r/MachineLearning/comments/2n7865/journal_club_week_48_2014_voting_thread/,BeatLeJuce,1416778796,,0,4,False,default,,,,,
228,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,6,2n788l,reddit.com,[Journal Club] How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation,https://www.reddit.com/r/MachineLearning/comments/2n788l/journal_club_how_autoencoders_could_provide/,BeatLeJuce,1416778827,,0,5,False,default,,,,,
229,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,7,2n7amq,blogs.wsj.com,Baidus Andrew Ng on Deep Learning and Innovation in Silicon Valley,https://www.reddit.com/r/MachineLearning/comments/2n7amq/baidus_andrew_ng_on_deep_learning_and_innovation/,hourwithoutaname,1416780216,,5,37,False,http://b.thumbs.redditmedia.com/fh78G5dgaAwtgmySOgD0HSPjMyba2deKdoOoNYFvgGE.jpg,,,,,
230,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,7,2n7b9e,machinelearningmastery.com,Evaluate Yourself As a Data Scientist,https://www.reddit.com/r/MachineLearning/comments/2n7b9e/evaluate_yourself_as_a_data_scientist/,jasonb,1416780594,,1,18,False,http://b.thumbs.redditmedia.com/NHbiSn4rc4qMKVXot8KRQCJCukF2lTihObROMFDBMhI.jpg,,,,,
231,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,9,2n7nm6,iamtrask.github.io,Word2Vec Algorithm on 7 Harry Potter Books,https://www.reddit.com/r/MachineLearning/comments/2n7nm6/word2vec_algorithm_on_7_harry_potter_books/,iamtrask,1416787563,,4,7,False,default,,,,,
232,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,10,2n7vyk,wired.com,The Three Breakthroughs that Have Finally Unleashed AI on the World,https://www.reddit.com/r/MachineLearning/comments/2n7vyk/the_three_breakthroughs_that_have_finally/,[deleted],1416792276,,1,0,False,default,,,,,
233,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,12,2n87yg,self.MachineLearning,Varying Data Granularity in data set,https://www.reddit.com/r/MachineLearning/comments/2n87yg/varying_data_granularity_in_data_set/,someusername1234,1416799350,"This is a simple question, but for some reason it is throwing me off.  By ""granularity"" I mean level of the data.  For example, say in the classic example of spam classification you have data at the email level (IP address of sender, email extension of sender, time of day sent etc..) and data at the individual word level.  

So, the prediction you want to make is at the email level, but you have data at the word granularity for email.  So, at the email level, do you view each possible word as a categorical variable that takes on {0,1} if its present in a given email? So, say you wanted to take a shot at modeling with logistic regression, does this leave with like 100,000 coded variables (if your dictionary has 100,000 words) for each email?  


How does this model stay tractable? Especially if the email level data is significant?  Would that data get overshadowed by fitting 100,000 dummy variables?



I'm more asking about this scenario in general, I know that I could just google methods of email classification.",11,2,False,self,,,,,
234,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,13,2n8c1z,instagender.herokuapp.com,Gender Detection on Instagram using Feature Stacking,https://www.reddit.com/r/MachineLearning/comments/2n8c1z/gender_detection_on_instagram_using_feature/,brcoburn,1416801828,,13,4,False,default,,,,,
235,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,17,2n8y4b,facebook.com,"Tengzhou Sanzhong Machinery Manufacture Co.,Ltd-Please like our company page,thank you",https://www.reddit.com/r/MachineLearning/comments/2n8y4b/tengzhou_sanzhong_machinery_manufacture/,[deleted],1416818536,,1,0,False,default,,,,,
236,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,17,2n8ynf,arxiv.org,A Bayes Consistent 1-Nearest Neighbor Estimator,https://www.reddit.com/r/MachineLearning/comments/2n8ynf/a_bayes_consistent_1nearest_neighbor_estimator/,TheCaterpillar,1416819086,,0,1,False,default,,,,,
237,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,19,2n94t2,self.MachineLearning,Ask ML: Transforming large dataset for data analysis and then applying a machine learning technique.,https://www.reddit.com/r/MachineLearning/comments/2n94t2/ask_ml_transforming_large_dataset_for_data/,the1ullneverse,1416825861,"Hi, so I have come to this subreddit for all of your guys expertise in Machine learning. I am tasked with accessing a large dataset, transforming it ready for data analysis and then applying a machine learning technique of my choice to perform either a classification, association, numerical prediction or clustering task. 

So, what I would like to know, is if any of you guys know of any good pieces of software to do any of the above, and any good resources to help me do this. I am not asking for you guys to do the work for me, just to point me in the right direction hopefully  as I need to evaluate what I did and why in a report! 
Thanks!!",3,1,False,self,,,,,
238,MachineLearning,t5_2r3gv,2014-11-24,2014,11,24,22,2n9e3j,2015.recsyschallenge.com,"2015 Recommender Systems Challenge - Given a sequence of clicks performed by a user, predict whether the user is going to buy something or not",https://www.reddit.com/r/MachineLearning/comments/2n9e3j/2015_recommender_systems_challenge_given_a/,recsys,1416835127,,12,24,False,http://b.thumbs.redditmedia.com/zIFhtp8cNGI5uYFyR3fNOKWo4nDCC9fF3UNoT7QFf2g.jpg,,,,,
239,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,1,2n9t5h,blogs.wsj.com,Artificial Intelligence Company Sentient Emerges From Stealth,https://www.reddit.com/r/MachineLearning/comments/2n9t5h/artificial_intelligence_company_sentient_emerges/,mrprint,1416845002,,0,0,False,http://b.thumbs.redditmedia.com/Ij3asVvz35PBb-9M5PxwvzSiUrdYHJLvwNTBnE36qkI.jpg,,,,,
240,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,1,2n9ted,youtube.com,Computers Can Read Emotion Better Than Humans,https://www.reddit.com/r/MachineLearning/comments/2n9ted/computers_can_read_emotion_better_than_humans/,Sergiointelnics,1416845127,,0,0,False,http://b.thumbs.redditmedia.com/suPgZagcq0qrF27LYUYbMe2WDkIekUsBbAu7ITGheJU.jpg,,,,,
241,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,1,2n9ui9,eugenezhulenev.com,Stock Price Prediction With Big Data and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2n9ui9/stock_price_prediction_with_big_data_and_machine/,ezhulenev,1416845754,,0,1,False,http://b.thumbs.redditmedia.com/wgWmClOuxyqKc8XKLJJDDASHXeif7tMrzkVGkDr7CbE.jpg,,,,,
242,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,4,2nagis,self.MachineLearning,User Interface Development for Creating Training Datasets?,https://www.reddit.com/r/MachineLearning/comments/2nagis/user_interface_development_for_creating_training/,[deleted],1416856814,"Does any have any best practices or guidance on the best way to create user interfaces for operation users to create the training/input data sets for ML model development? 

Ideally, I will be working with tens (or hundreds) of thousands of trained data samples, and want a way to have our internal experts assign the correct values to the training data. 

I know that, in the past, I have read a blog about using Amazon Mechanical Turk to 'crowdsource' training data, however I need to create interfaces for internal use the same way. The people that have used MT, did you create the UI for them to assign to the samples?

Is the interface as simple as just paging through the list of most needed training points, and create web-forms to post the the db? ",2,0,False,default,,,,,
243,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,6,2nazbi,numenta.org,Introducing NuPIC Studio,https://www.reddit.com/r/MachineLearning/comments/2nazbi/introducing_nupic_studio/,numenta,1416865684,,3,21,False,http://b.thumbs.redditmedia.com/HTijY0ZHQaji753vDXfEmYzdiTsNDBNKvWyq9ucTa6w.jpg,,,,,
244,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,7,2nb0uu,self.MachineLearning,How machine learning is implemented in real life,https://www.reddit.com/r/MachineLearning/comments/2nb0uu/how_machine_learning_is_implemented_in_real_life/,[deleted],1416866435,"Hi I'd like to say sorry in advance for my lack of knowledge not only in this particular area but also generally in everything and I'm afraid this will be a really dumb question, but I couldn't get the answer from any searches on google nor in here so here I go.. (if there's a better subreddit for this kind of question, please tell me so)

I guess companies like Amazon or Netflix use the machine learning techniques in order to recommend new movies/products the user is likely to be interested etc.

The web part would be pretty straightforward like.. when a user buys a product, the database will be modified such that e.g. the corresponding User object now has an entry to the corresponding Product object etc. 

We'd have thousands and millions of data like this and we'd be able to predict what the user would like to buy and display in the Recommended section on the web using some machine learning techniques. 

So exactly how is this done? We'd have separate servers dedicated in periodically running the algorithms that pull off this immense data from database, compute, and modify each of the corresponding User object so that it now contains the new computed data for its Recommended field? 

I remember using MathLab in the online course but I don't think it's suitable for this kind of job, it's more like an analysis tool. What technology would they use to run all ML algo's and for scaling as well?",0,1,False,default,,,,,
245,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,7,2nb3nu,self.MachineLearning,How is machine learning being used in real applications?,https://www.reddit.com/r/MachineLearning/comments/2nb3nu/how_is_machine_learning_being_used_in_real/,crowsplay,1416867800,"Hi I'd like to say sorry in advance for my lack of knowledge not only in this particular area but also generally in everything and I'm afraid this will be a really dumb question, but I couldn't get the answer from any searches on google nor in here so here I go.. (if there's a better subreddit for this kind of question, please tell me so)

I guess companies like Amazon or Netflix use the machine learning techniques in order to recommend new movies/products the user is likely to be interested etc.

The web part would be pretty straightforward like.. when a user buys a product, the database will be modified such that e.g. the corresponding User object now has an entry to the corresponding Product object etc.
We'd have thousands and millions of data like this and we'd be able to predict what the user would like to buy and display in the Recommended section on the web using some machine learning techniques.


So exactly how is this done? We'd have separate servers dedicated in periodically running the algorithms that pull off this immense data from database, compute, and modify each of the corresponding User object so that it now contains the new computed data for its Recommended field?


I remember using MathLab in the online course but I don't think it's suitable for this kind of job, it's more like an analysis tool. What technology would they use to run all ML algo's and for scaling as well?

**EDIT**: I was more curious about *how* ML is being used, rather than *where* it's being used.. but all the responses were very insightful! Thanks",13,7,False,self,,,,,
246,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,8,2nbd61,self.MachineLearning,Is Andrew Ng's coursera course worth it for me?,https://www.reddit.com/r/MachineLearning/comments/2nbd61/is_andrew_ngs_coursera_course_worth_it_for_me/,greatluck,1416872572,"I signed up for the most recent course and to be honest after the first couple weeks I hadn't had time to keep up with it.  I found the math more time consuming than I expected.  Also the class uses Octave... I currently use SAS (relatively new; about a year) I have recently started to learn R and I'm not sure I want to dive into another new language.  

So I guess my question is, is it worth it for me to suck it up and get through this class the next time it comes around or is my time and energy better spent self learning through text books?


Edit: Thanks for the replies.  In short, I'll pick up where I left off, not worrying about missed deadlines.",14,3,False,self,,,,,
247,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,9,2nbknx,apps.facebook.com,Liquid FM: a research project to apply liquid voting to music recommendation,https://www.reddit.com/r/MachineLearning/comments/2nbknx/liquid_fm_a_research_project_to_apply_liquid/,_ch3m,1416876644,,1,1,False,default,,,,,
248,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,12,2nc2bb,iamtrask.github.io,Distributing a Fully Connected Neural Network Across a Cluster (A Novel Approach),https://www.reddit.com/r/MachineLearning/comments/2nc2bb/distributing_a_fully_connected_neural_network/,iamtrask,1416886441,,9,16,False,default,,,,,
249,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,17,2ncs1q,self.MachineLearning,Scikit learn handling a huge database,https://www.reddit.com/r/MachineLearning/comments/2ncs1q/scikit_learn_handling_a_huge_database/,stickypens,1416904252,I am new to using scikit. I have a huge dataset which is stored in a database. Dump file is around 8gb. I have a laptop with 6 gigs of ram. How can I load files from the database?,17,7,False,self,,,,,
250,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,22,2nd8vx,youtu.be,Prediction in city planning: The case of bike share,https://www.reddit.com/r/MachineLearning/comments/2nd8vx/prediction_in_city_planning_the_case_of_bike_share/,proxyformyrealname,1416920420,,0,5,False,http://b.thumbs.redditmedia.com/jm-j5yrV6YLgmrrwkEwnGW3IidlM8dGAqw5_q2ybEUQ.jpg,,,,,
251,MachineLearning,t5_2r3gv,2014-11-25,2014,11,25,22,2nddg1,papers.nips.cc,NIPS 2014 Proceedings,https://www.reddit.com/r/MachineLearning/comments/2nddg1/nips_2014_proceedings/,compsens,1416923810,,7,31,False,default,,,,,
252,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,2,2ne0q7,self.MachineLearning,[Question] Social and Demographic Analysis ?,https://www.reddit.com/r/MachineLearning/comments/2ne0q7/question_social_and_demographic_analysis/,_learningCS,1416936609,"- What are the ""relevant"" metrics (eg:Average-age, average-education, average salary .. ) that I should use to compare between states in US?. 
- Is there any project or paper that I should check ? ",1,0,False,self,,,,,
253,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,3,2ne9hf,holidaybullshit.com,Funny example of how (intentionally) reckless statistics can be misleading,https://www.reddit.com/r/MachineLearning/comments/2ne9hf/funny_example_of_how_intentionally_reckless/,twelfthnight,1416940996,,3,0,False,default,,,,,
254,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,4,2nee39,self.MachineLearning,[Question] Trend Analysis,https://www.reddit.com/r/MachineLearning/comments/2nee39/question_trend_analysis/,def_Questions,1416943084,"- What are the most used approaches to design trend analysis engine ? 
- Would you please recommend some papers to read ? ",0,0,False,self,,,,,
255,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,4,2nehvl,self.MachineLearning,Use of Boltzmann machines,https://www.reddit.com/r/MachineLearning/comments/2nehvl/use_of_boltzmann_machines/,tutututututudu,1416944818,"Hi!
I would like to try to code a Boltzmann machine and try it out on some example. Buut as I google I cannot find much examples of uses of Boltzmann machines (I always found only the restricted one). The only problem I found was travelling salesman.
Can you give me some more examples of problems where Boltzmann machines can be applied?

Thank you!
(also sorry if my english is not perfect)",3,2,False,self,,,,,
256,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,8,2nfaby,self.MachineLearning,"can i use the code for image recognition with audio, is it cross compatible since they both use an array of numbers?",https://www.reddit.com/r/MachineLearning/comments/2nfaby/can_i_use_the_code_for_image_recognition_with/,perpetual_virginity,1416958718,,7,0,False,self,,,,,
257,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,9,2nffni,facebook.com,Vladimir Vapnik has joined Facebook AI Research,https://www.reddit.com/r/MachineLearning/comments/2nffni/vladimir_vapnik_has_joined_facebook_ai_research/,vkhuc,1416961659,,19,52,False,http://a.thumbs.redditmedia.com/MuyQTsP-opaiF8taZmKtHsUIl6F6ONv67eZ1rzEP8p4.jpg,,,,,
258,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,13,2ng58z,self.MachineLearning,Illustrating power-law properties with Chinese Restaurant Process (PYP),https://www.reddit.com/r/MachineLearning/comments/2ng58z/illustrating_powerlaw_properties_with_chinese/,koormoosh,1416976809,"I often read that Pitman-Yor Process has power-law properties. Let's say I am interested in modelling English word's distribution (which follows power-law). Using CRP metaphor, words come and get assigned to tables using CRP probabilities. Now I want to draw samples from this CRP to show that it actually captures power-law properties. How would I conduct such sampling?

What I thought initially was to treat it as sampling from a multinomial distribution defined by the seating arrangements (tables, #ofCustomers). But this doesn't seem to be correct.

Any idea?",3,1,False,self,,,,,
259,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,15,2ngh5j,self.MachineLearning,Feature Extraction from Kirzhevsky net in Theano/Pylearn?,https://www.reddit.com/r/MachineLearning/comments/2ngh5j/feature_extraction_from_kirzhevsky_net_in/,BeijingChina,1416984959,"Hi,
People have been using libraries like caffe etc. where they can easily extract features from these large networks. Is there some similar wrapper to do this in a Theano/Pylearn framework. I agree we could always get the features from the other libraries, and funnel them through to any code we desire, but just wanted advice if anyone has come up with wrappers/ standard ways of doing this.

Thanks",13,3,False,self,,,,,
260,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,16,2ngl6t,reddit.com,This would be a neat application of machine learning: An app that can identify birds by their song,https://www.reddit.com/r/MachineLearning/comments/2ngl6t/this_would_be_a_neat_application_of_machine/,MaunaLoona,1416988451,,0,7,False,default,,,,,
261,MachineLearning,t5_2r3gv,2014-11-26,2014,11,26,23,2nh8vo,bitbucket.org,Recursive AutoEncoder using theano. Deep Learning beginner here. Requesting to please review code. (as in paper http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection ),https://www.reddit.com/r/MachineLearning/comments/2nh8vo/recursive_autoencoder_using_theano_deep_learning/,[deleted],1417011382,,0,0,False,default,,,,,
262,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,0,2nhecj,iro.umontreal.ca,Zero-bias autoencoders implemented by Roland Memisevic,https://www.reddit.com/r/MachineLearning/comments/2nhecj/zerobias_autoencoders_implemented_by_roland/,galapag0,1417014757,,0,7,False,default,,,,,
263,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,1,2nhk4c,dataelixir.com,"Data Elixir, Issue 12: Data in the sciences, neural nets, data viz at TED",https://www.reddit.com/r/MachineLearning/comments/2nhk4c/data_elixir_issue_12_data_in_the_sciences_neural/,lonriesberg,1417018013,,3,0,False,default,,,,,
264,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,2,2nhti3,self.MachineLearning,Theano for symbolic differentiation in general,https://www.reddit.com/r/MachineLearning/comments/2nhti3/theano_for_symbolic_differentiation_in_general/,speechMachine,1417023466,"As a Theano n00b, I'd like to take the liberty to ask if people are aware of projects where Theano has been used just for its symbolic differentiation capabilities, though not necessarily for deep learning? Pointers to Github repos/or IPython notebooks of this nature would be extremely useful. Also, out of curiosity how does the symbolic differentation work under the hood? Is it that Theano is computing a numerical gradient by perturbing the parameters with respect to which the gradient needs to be taken with a certain small value of epsilon?",7,3,False,self,,,,,
265,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,6,2nimdr,self.MachineLearning,Machine Learning libraries for C++?,https://www.reddit.com/r/MachineLearning/comments/2nimdr/machine_learning_libraries_for_c/,ankitsablok89,1417037996,I am a newbie to machine learning and I would like to code up some of its algorithms on sample data sets that I have with me but preferably in C++ but I haven't been able to find a library that provides a good support for machine learning in C++ something sort of similar to NumPy and SciPy in Python. Can someone suggest me some C++ libraries that help me do so along with some tutorial links that are useful to learn those libraries?,24,30,False,self,,,,,
266,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,9,2nj4ce,self.MachineLearning,Is RapidMiner any good?,https://www.reddit.com/r/MachineLearning/comments/2nj4ce/is_rapidminer_any_good/,DeLuhmer,1417047863,Just looking for some opinions regarding its worth. ,5,7,False,self,,,,,
267,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,10,2njd0l,aka.ms,"Microsoft's Machine Learning technology covered in CIO mag, WIRED, KDnuggets &amp; PCWorld in the past week",https://www.reddit.com/r/MachineLearning/comments/2njd0l/microsofts_machine_learning_technology_covered_in/,MLBlogTeam,1417053369,,0,1,False,default,,,,,
268,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,14,2njxqp,arxiv.org,Zero-bias autoencoders and the benefits of co-adapting features,https://www.reddit.com/r/MachineLearning/comments/2njxqp/zerobias_autoencoders_and_the_benefits_of/,[deleted],1417066754,,0,1,False,default,,,,,
269,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,19,2nkgpa,software.ac.uk,Creating a future for autonomous robotics at the University of Birmingham,https://www.reddit.com/r/MachineLearning/comments/2nkgpa/creating_a_future_for_autonomous_robotics_at_the/,5pin05auru5,1417083458,,0,4,False,http://b.thumbs.redditmedia.com/OO0wNcHqR5fH4n7Q4tKb-sHIykCpWg4Fjvr-UedTfiU.jpg,,,,,
270,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,20,2nkjdo,wired.co.uk,"They're looking for human volunteers to help them spot the higgs boson. Surely, this is what machine learning excels at?",https://www.reddit.com/r/MachineLearning/comments/2nkjdo/theyre_looking_for_human_volunteers_to_help_them/,MaunaLoona,1417086267,,9,13,False,http://b.thumbs.redditmedia.com/6pZisOh9GQbjSIixh3hrUVPwZbr44NR6HBq40qfPdkI.jpg,,,,,
271,MachineLearning,t5_2r3gv,2014-11-27,2014,11,27,22,2nkquu,arxiv.org,Zero-bias autoencoders and the benefits of co-adapting features,https://www.reddit.com/r/MachineLearning/comments/2nkquu/zerobias_autoencoders_and_the_benefits_of/,budhdub,1417094093,,4,13,False,default,,,,,
272,MachineLearning,t5_2r3gv,2014-11-28,2014,11,28,1,2nl5dr,self.MachineLearning,Image pre processing for emotion classification,https://www.reddit.com/r/MachineLearning/comments/2nl5dr/image_pre_processing_for_emotion_classification/,UannaFF,1417105155,"Hi! i want to do some emotion recognition from pictures, i am using the JAFFE database of Japanese  models expressing different emotions in each picture. I want to work a genetic algorithm(GABIL) and multi layer neural network on the problem to compare them afterwards, but i am having troubles getting the information necessary from the pictures to pass to the algorithms. What i have done til now is pass the pictures to grayscale and what i am passing to the algorithms is the shade of each pixel(it's a 32x44 image) but i haven't got any luck yet, i was trying to simplify the approach, but i think maybe it's necessary some feature recognition to get the regions of the face that matters. Can someone  pass a link to a good approach on this or give me some thoughts on the matter? i would really appreciate it.",5,1,False,self,,,,,
273,MachineLearning,t5_2r3gv,2014-11-28,2014,11,28,16,2nnfo8,slideshare.net,10 lessons from Netflix from building ML Systems,https://www.reddit.com/r/MachineLearning/comments/2nnfo8/10_lessons_from_netflix_from_building_ml_systems/,cast42,1417160806,,1,4,False,http://b.thumbs.redditmedia.com/-6vmO7IN-5xRevUCUAR2j5QyHuTb0S6TWcZ7RneDUYc.jpg,,,,,
274,MachineLearning,t5_2r3gv,2014-11-28,2014,11,28,21,2nnwnr,polykraftmachines.wordpress.com,Different Types of Industrial Drilling Machine,https://www.reddit.com/r/MachineLearning/comments/2nnwnr/different_types_of_industrial_drilling_machine/,polykraftmachines,1417179023,,0,1,False,default,,,,,
275,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,0,2noalc,self.MachineLearning,Text classification - looking for approach suggestions,https://www.reddit.com/r/MachineLearning/comments/2noalc/text_classification_looking_for_approach/,fiedzia,1417190311,"I'm trying classify short texts, using nltk and scikit-learn, but I am not sure yet how exactly approach it, and I am looking for advice.
A particular text may belong to more then one class, or it may not belong to any.
The dataset I have is about 100k items, with relatively small amount of items per category (thousands in few cases, hundreds in many, far less in most). 
For a given cIass I can easily generate samples of items that should be there, but I am not sure what about counter examples (if I need them).
So far I am experimenting with naive Bayes classification, where I train classificator using known sample items and random selection of known items that don't belong to this class, doing this separately for each class. As a result classification works well for things that are good match, but generates lot of false positives. Is there a better way of doing this?",7,1,False,self,,,,,
276,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,1,2nodva,technologyreview.com,Machine-Learning Algorithm Ranks the World's Most Notable Authors,https://www.reddit.com/r/MachineLearning/comments/2nodva/machinelearning_algorithm_ranks_the_worlds_most/,futureisdata,1417192360,,0,1,False,http://b.thumbs.redditmedia.com/TgCr1HKqIzQo6et0gM6qqf0oFudkorMD5uUCkRGnWLU.jpg,,,,,
277,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,2,2nogn4,inbits.com,Mathematics of Hierarchical Temporal Memory,https://www.reddit.com/r/MachineLearning/comments/2nogn4/mathematics_of_hierarchical_temporal_memory/,fergbyrne,1417194114,,14,25,False,http://b.thumbs.redditmedia.com/aTMHgwYDdB6gKJxwweGQOnHl_fAnebZEuQhx7lbPBtY.jpg,,,,,
278,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,3,2nooho,self.MachineLearning,ASK: Spatial data (Latitude and Longitude) as features,https://www.reddit.com/r/MachineLearning/comments/2nooho/ask_spatial_data_latitude_and_longitude_as/,perone,1417198756,"I want to use latitude and longitude as a feature for models like SVM or Logistic Regression (both for classification). Which is the most common approach to use latitude and longitude values as features? I have tried to use decimal representation, I've also tried to scale it (for SVM) and the results were the same as without the latitude/longitude. These features are supposed to be significant in the performance of the model (according to my specific dataset of course).",11,0,False,self,,,,,
279,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,5,2np08w,self.MachineLearning,[Question] K-fold cross validation with SVM,https://www.reddit.com/r/MachineLearning/comments/2np08w/question_kfold_cross_validation_with_svm/,fanmepurple,1417205448,"I am looking at [this](http://stackoverflow.com/questions/3070789/example-of-10-fold-svm-classification-in-matlab/3071938#3071938) link and I am wondering if someone could explain to me what the point of testing with the same C and sigma values is every k iteration? Are you just making sure that the training data were all consistent between one another? 

What is the proper way of using k-fold cross validation when trying to find optimal C and sigma values?",2,2,False,self,,,,,
280,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,5,2np2je,arxiv.org,Random feedback weights support learning in deep neural networks,https://www.reddit.com/r/MachineLearning/comments/2np2je/random_feedback_weights_support_learning_in_deep/,zdwiel,1417206710,,13,10,False,default,,,,,
281,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,6,2npa6d,cs.tau.ac.il,Videos from the Deep Learning Master Class at Tel Aviv University are now Live,https://www.reddit.com/r/MachineLearning/comments/2npa6d/videos_from_the_deep_learning_master_class_at_tel/,vikkamath,1417211195,,10,40,False,http://a.thumbs.redditmedia.com/PDViJG4_-WV_qmyiczKR60X5wqrh03hvr9jqsa-8rt4.jpg,,,,,
282,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,6,2npaz0,self.MachineLearning,EE student (and electrician) interested in using human social and psychological patterns to design software and hardware,https://www.reddit.com/r/MachineLearning/comments/2npaz0/ee_student_and_electrician_interested_in_using/,[deleted],1417211663,"I'm a junior EE student and an apprentice electrician.

I really want to be on a team with psychologists and biologists and other professionals with the purpose of analyzing human and animal behavior and learning patterns, and then applying those models to develop software and hardware.

Specifically: I go out and socialize a lot. I am -INCREDIBLY- socially intelligent and aware. I think I could distill the process and patterns needed to identify various human emotion based on body language and facial expressions. I go out every night and see this happen all around me all the time. I am very aware of it, and often quantify what exactly is happening as it happens. Given the necessary time and resources, I really think I can simplify and explain human emotion (How do I identify it) to a computer, and subsequently teach a computer to mimic human emotion.

I have no idea how to start getting into this field. This is definitely what I want to do. How do I make it happen?",6,0,False,default,,,,,
283,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,11,2npzoh,datasciencecentral.com,Implementing a Distributed Deep Learning Network over Spark,https://www.reddit.com/r/MachineLearning/comments/2npzoh/implementing_a_distributed_deep_learning_network/,vincentg64,1417227298,,1,0,False,http://b.thumbs.redditmedia.com/gevb8r0-9VwnSevRjaTgrZ0jCCZLOW3hHX6YOjU6E6M.jpg,,,,,
284,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,12,2nq5mj,self.MachineLearning,"Input : Node Graph, Output : Float",https://www.reddit.com/r/MachineLearning/comments/2nq5mj/input_node_graph_output_float/,fawar,1417231416,"I have some knowledge of machine learning, but I don't know of any solution related to the following problem.

Having as input a graph of X nodes (x isnt fixed)

Evaluate with a Neural net/svm or anything that is trainable if the graph is good (supervised).

Is there any techniques that would allow me to model that problem?",14,2,False,self,,,,,
285,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,17,2nqtfc,self.MachineLearning,Respecting Structure in Deep Learning,https://www.reddit.com/r/MachineLearning/comments/2nqtfc/respecting_structure_in_deep_learning/,[deleted],1417250832,"So I watched this very good introduction to deep learning: https://www.youtube.com/watch?v=S75EdAcXHKk

And at the end the last thing he says is how it is important to respect the structure in your data which is why you need multiple layers.

So since a letter or number symbol is 2 dimensional, does that mean that you would use 2 layers or dimensions?  Or would it be better to use 3 dimensions since we humans live in a 3D world, and thus the Arabic numeral system was probably designed by trial and error to be somehow simpler to write in our 3D world.

Does adding layers always increase the accuracy, or is there an ideal number of layers to use for each type of task?",2,2,False,default,,,,,
286,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,18,2nqvi9,papers.nips.cc,How Transferable are Features in Deep Networks?,https://www.reddit.com/r/MachineLearning/comments/2nqvi9/how_transferable_are_features_in_deep_networks/,madisonmay,1417253311,,1,8,False,default,,,,,
287,MachineLearning,t5_2r3gv,2014-11-29,2014,11,29,19,2nqzlf,reddit.com,Magically updated statML arXiv subreddit,https://www.reddit.com/r/MachineLearning/comments/2nqzlf/magically_updated_statml_arxiv_subreddit/,arXibot,1417258491,,3,11,False,default,,,,,
288,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,1,2nrivr,vision.stanford.edu,CS231n: Convolutional Neural Networks for Visual Recognition,https://www.reddit.com/r/MachineLearning/comments/2nrivr/cs231n_convolutional_neural_networks_for_visual/,madisonmay,1417277229,,15,88,False,http://b.thumbs.redditmedia.com/9eNNJTXxCBCi4SXbVnkArW66UymmE1DwimculsNjjvo.jpg,,,,,
289,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,3,2nrtgm,self.MachineLearning,Do you use stream reasoning to predict data?,https://www.reddit.com/r/MachineLearning/comments/2nrtgm/do_you_use_stream_reasoning_to_predict_data/,Regentag,1417284109,"Hello guys,

I read about [stream reasoning](http://streamreasoning.org/), which I find quite interesting. It seems to be a really young field in science.

Basically, you have a knowledge base and try to reason, derive new knowledge, on a stream of incoming events. Therefore, complex events are derived from simpler events by means of deductive rules.

One engine for example is [ETALIS](https://code.google.com/p/etalis/), which seems to be quite famous.

Has anybody of you used stream reasoning before? Could this be used to predict some events? 

I appreciate your answers!",6,0,False,self,,,,,
290,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,3,2nrwt3,self.MachineLearning,PCA vs ICA vs NNMF,https://www.reddit.com/r/MachineLearning/comments/2nrwt3/pca_vs_ica_vs_nnmf/,georgeo,1417286082,I've have good results pre-processing data with PCA for ANNs whether of not I need dimension reduction. Has anyone achieved real world gains using Independent Component Analysis or Non-Negative Matrix Factorization? Are they worth the extra computation required. I realize this is subjective but any feedback would be useful.,1,3,False,self,,,,,
291,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,4,2ns2oe,self.MachineLearning,Unsupervised Image recognition example (possibly google),https://www.reddit.com/r/MachineLearning/comments/2ns2oe/unsupervised_image_recognition_example_possibly/,asdf989,1417289599,"I've lost something in the internet. 

I'm trying to find a link/reference to an article showing the results of an unsupervised computer vision effort. 

Amongst the description of the project was a large 6000x6000 image of tons of thumbnails where the similar thumbnails were placed next to each other. The image flowed from cars-&gt;boats -&gt; canoes -&gt; camping -&gt; trees-&gt;flowers-&gt;bugs on flowers and so on.

It 'might' have been done by google.

Thanks for the help.",4,0,False,self,,,,,
292,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,4,2ns52j,papers.nips.cc,Generative Adversarial Nets,https://www.reddit.com/r/MachineLearning/comments/2ns52j/generative_adversarial_nets/,madisonmay,1417291027,,19,2,False,default,,,,,
293,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,5,2nsa5h,self.MachineLearning,"Best algorithm for unsupervised learning, layer by layer, of continuous features?",https://www.reddit.com/r/MachineLearning/comments/2nsa5h/best_algorithm_for_unsupervised_learning_layer_by/,alvarogarred,1417294128,"Hi, I'm new in ML (and English is not my first language, sorry for mistakes). I know and understand (more or less) Restricted Boltzmann Machines, and I would like to stack some layers to do deep belief networks. But my training samples aren't binary values but continuous values, and (if I'm not totally lost) the original formulation of RBM is for discrete samples values.

I've seen some RBM algorithms to work with continuous values but were quite old (http://pdf.aminer.org/000/271/523/a_continuous_restricted_boltzmann_machine_with_a_hardware_amenable_learning.pdf). Is there any ""standarized"" and ""modern"" (and simple if possible but probably that's asking too much) way to learn layer by layer with unsupervised algorithm from continuous information?

Thank you.",3,0,False,self,,,,,
294,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,6,2nscm6,arxiv.org,Learning to Execute,https://www.reddit.com/r/MachineLearning/comments/2nscm6/learning_to_execute/,madisonmay,1417295589,,0,1,False,default,,,,,
295,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,10,2nt2rm,github.com,Evolutionary Intelligent Life Demo,https://www.reddit.com/r/MachineLearning/comments/2nt2rm/evolutionary_intelligent_life_demo/,cazala,1417311843,,1,0,False,http://a.thumbs.redditmedia.com/em1RcgFHZl7g758nCT4JejUsgJ3kLWlsD-ydrbrBvG4.jpg,,,,,
296,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,17,2ntxgt,self.MachineLearning,How can I start with Machine Learning,https://www.reddit.com/r/MachineLearning/comments/2ntxgt/how_can_i_start_with_machine_learning/,soulslicer0,1417335636,"Good day. I've a 3rd year com. eng student going to my 4th next year. I have been sort of focusing/specializing in the computer vision area and signal processing area and am familiar with some of the mathematics and software libraries available for these areas. 

One think I'd like to do this December holidays is to get into ML. But looking at this subreddit, it looks like there are countless things to look at! Looking at the openCV ML Libraries is also rather limiting to just knowing how to apply these. Can I know a good course/book/roadmap to getting into this field?",19,16,False,self,,,,,
297,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,20,2nu5vf,self.MachineLearning,How to store millions of images for data analysis?,https://www.reddit.com/r/MachineLearning/comments/2nu5vf/how_to_store_millions_of_images_for_data_analysis/,mosquit0,1417346387,"Hi. I am looking for suggestions how to store ~10 million images. Each image has different size and has heterogenous information attached (basically a dozen key value pairs). They are rather small rarely more than 100kb.

These images will go through some transformation pipeline which will end with Convolutional Neural Network.

I was thinking about mongodb since the image meta data will be natively kept as documents and image as binary data fields.",16,7,False,self,,,,,
298,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,21,2nu860,self.MachineLearning,"Any resources on metaheuristics applied to general problem solving, e.g. in business?",https://www.reddit.com/r/MachineLearning/comments/2nu860/any_resources_on_metaheuristics_applied_to/,bubaonaruba,1417349383,"I've been trying to find publications / blog posts / info on the application of metaheuristics to general problem solving, (in the business domain, but not necessarily so). E.g. organizing business decision process similarly to SA, GA, Tabu Search, Ant Colony or other metas. I'd be grateful for any pointers.",2,1,False,self,,,,,
299,MachineLearning,t5_2r3gv,2014-11-30,2014,11,30,23,2nudxa,self.MachineLearning,"NLP keyword extraction, bayes therom. Where do I start?",https://www.reddit.com/r/MachineLearning/comments/2nudxa/nlp_keyword_extraction_bayes_therom_where_do_i/,Brightparker,1417356152,"First of all I would like to apologise as I'm new to programing( java) 

I'm working on a side project where I'm extracting ""meaning"" from medical records. This would involve keyword extraction and text summarization. 

How would I apply bayes therom to something like this to develop a text processing technique? 

I know how bayes work in a statical sence but I don't know how to apply it to text processing. 

And how does a program ""Learn"" from bayes?",2,1,False,self,,,,,
