,sbr,sbr_id,date,year,month,hour,day,id,domain,title,full_link,author,created,selftext,num_comments,score,over_18,thumbnail,media_provider,media_thumbnail,media_title,media_description,media_url
0,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,10,30zx3k,phys.org,"For the first time, physicists have performed ML on a photonic quantum computer",https://www.reddit.com/r/MachineLearning/comments/30zx3k/for_the_first_time_physicists_have_performed_ml/,[deleted],1427850515,,0,1,False,default,,,,,
1,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,14,310vcu,homepages.inf.ed.ac.uk,Tutorials on Machine Learning,https://www.reddit.com/r/MachineLearning/comments/310vcu/tutorials_on_machine_learning/,john_philip,1427867368,,0,11,False,default,,,,,
2,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,15,311058,self.MachineLearning,N-Dimensional Neural Nets?,https://www.reddit.com/r/MachineLearning/comments/311058/ndimensional_neural_nets/,dsocma,1427870514,"In deep learning applied to pictures, the input is 2-dimensional.  Would you use a different type of neural network, if the input is 3D?  For example, if you applied deep learning to 3d laser scan data, or 3d models, would you want to design a network where the neurons would be points in a 3d grid, and then they feed up to smaller and smaller cubes, each cube representing larger areas, just like the way it happens in regular deep learning with pictures, except they are square instead of cube.

I think this could be revolutionary if applied to certain things, and if it was extended to N-dimensions and especially if you added a time dimension.

Has this already been thought of and its just too computation intensive?",6,2,False,self,,,,,
3,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,15,3111hm,self.MachineLearning,"Something ""deeply wrong with deep learning""?",https://www.reddit.com/r/MachineLearning/comments/3111hm/something_deeply_wrong_with_deep_learning/,dsocma,1427871441,"In the 6th episode of the Talking Machines podcast at around 23:00 Geoffrey Hinton says:

&gt;""Today you can publish any paper you want on deep learning, but if you say: 'Theres something deeply wrong with deep learning and we need to change the way we are doing things, its impossible to get them published'""

what is he referring to?",48,38,False,self,,,,,
4,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,16,3112si,self.MachineLearning,Andrew Ng will be doing an AMA in /r/MachineLearning on April 14 9AM PST,https://www.reddit.com/r/MachineLearning/comments/3112si/andrew_ng_will_be_doing_an_ama_in/,olaf_nij,1427872363,"I'm happy to announce Chief Scientist at Baidu Research/Coursera Co-Founder/Stanford Professor Andrew Ng will being making an appearance in /r/MachineLearning on April 14 9AM PST for an AMA.

A thread will be created before the official AMA time for those who won't be able to attend.

....No, this is not an April Fools",35,318,False,self,,,,,
5,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,17,3119u7,devblogs.nvidia.com,cuDNN v2: Higher Performance for Deep Learning on GPUs,https://www.reddit.com/r/MachineLearning/comments/3119u7/cudnn_v2_higher_performance_for_deep_learning_on/,harrism,1427878088,,9,5,False,http://b.thumbs.redditmedia.com/yEhQ3xSkeTJPynr14D9CdcJIH7CkRrBT2GCfaneP8sQ.jpg,,,,,
6,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,18,311bhg,self.MachineLearning,"Issue with extended kalman filter, is my data simply too noisy?",https://www.reddit.com/r/MachineLearning/comments/311bhg/issue_with_extended_kalman_filter_is_my_data/,jabza_,1427879493,"I've been learning about extended kalman filters (with regards to SLAM), and have finally finished an implementation in python.

It runs, and adds new landmarks to the covariance matrix fine. However, on moving for the first time, all the landmark positions change drastically (out the map bounds). Until they are redetected as new landmarks. Which of course is fatal. My odometry is noisy, around 1cm deviation for every 10cm travelled.

I'm *certain* I've implemented something wrong (possibly the jacobians), or maybe missed a step completely? Here's the filter: http://pastebin.com/SEFLRQiH

I'd appreciate it if someone could sanity check what I've got so far, perhaps suggest areas to investigate further. Thanks!


",0,3,False,self,,,,,
7,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,18,311buj,nextbigfuture.com,Entanglement-Based Machine Learning on a Photonic Quantum Computer in principle and if scaled would show exponential speed up,https://www.reddit.com/r/MachineLearning/comments/311buj/entanglementbased_machine_learning_on_a_photonic/,earleanperchinski,1427879832,,1,4,False,http://b.thumbs.redditmedia.com/T-LEl-z6vj3iWcsz4kR8dkNeveFo_qkzrB7BKWsdTaw.jpg,,,,,
8,MachineLearning,t5_2r3gv,2015-4-1,2015,4,1,22,311x68,self.MachineLearning,Graduate project question,https://www.reddit.com/r/MachineLearning/comments/311x68/graduate_project_question/,[deleted],1427894378,"I'm working on a proposal for a grad school project in my ML class and what I would like to do is to analyze Twitter data and do information extraction to detect in real-time the status of events that may or may not happen - currently I'm thinking concerts, but that might be subject to change. So for example, if it's done in a supervised fashion, I'd look at tweets mentioning ""Band ABC"" and seek to classify them as either 1) the event is happening, 2) the event is not happening/cancelled, 3) not related to the event's status (which will be most of them, causing a large class imbalance, so it seems that I'd need to score by F1 rather than precision).

I thought I would use the Twitter NLP package here https://github.com/aritter/twitter_nlp to perform NER/POS tagging on the tweets. The author of the toolkit also created a tool that extracts events from Twitter and details his approach in this paper http://aritter.github.io/rt080-ritter.pdf.

Reading through the paper, it would appear that my task is somewhat similar, but in my task the domain would be closed rather than open, which should make things easier. However, as far as the actual classification of tweets, I'm looking for some advice: should I use a supervised approach like that author uses as his baseline? The author seems to use an unsupervised method with Bayesian inference to assign probabilities to event categories and it does significantly better than the supervised baseline. My original thought was something like a simple supervised bag of words model, but I'd appreciate any feedback.",0,2,False,default,,,,,
9,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,0,312c40,self.MachineLearning,UPDATE: Andrew Ng and Adam Coates will be doing an AMA in /r/MachineLearning on April 14 9AM PST,https://www.reddit.com/r/MachineLearning/comments/312c40/update_andrew_ng_and_adam_coates_will_be_doing_an/,olaf_nij,1427901034,"I'm happy to announce Chief Scientist at Baidu Research/Coursera Co-Founder/Stanford Professor Andrew Ng and Director of Baidu Researchs Silicon Valley AI Lab Adam Coates will being making an appearance in /r/MachineLearning on April 14 9AM PST for an AMA.

We've decided to experiment with AMAs with more than one guest. 

A thread will be created before the official AMA time for those who won't be able to attend.

I'm keeping the old thread around because some of the comments are pretty good and it's technically not untrue.

....No, this is not an April Fools",146,93,False,self,,,,,
10,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,0,312h42,self.MachineLearning,How does input data structure and weights initilization affects neural network performance? #noob #MNIST,https://www.reddit.com/r/MachineLearning/comments/312h42/how_does_input_data_structure_and_weights/,Tom-Demijohn,1427903008,"I have 2 MNIST datasets. 
* First one represents images where each pixels is represented by float32 ranging between 0..1. 
* Second dataset represents images with int64 where each pixel takes value between 0..255.

I initilize weigths and biases with ""standard normal"" distribuition (mean 0, std 1)

So on first dataset I do very well, and on the second one I'm horribly wrong.
So I assume that initialization is fine and the issue is with ""large"" input values. Is that right? Or given how input data is structured, there is always proper way to initialize network?

Thanks",5,0,False,self,,,,,
11,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,0,312hl0,arxiv.org,Towards Shockingly Easy Structured Classification: A Search-based Probabilistic Online Learning Framework (w/ code),https://www.reddit.com/r/MachineLearning/comments/312hl0/towards_shockingly_easy_structured_classification/,improbabble,1427903189,,6,9,False,default,,,,,
12,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,1,312qrt,blogs.technet.com,Criteo 1TB click prediction dataset - one of the largest public ML datasets - now on AzureML,https://www.reddit.com/r/MachineLearning/comments/312qrt/criteo_1tb_click_prediction_dataset_one_of_the/,MLBlogTeam,1427906593,,0,1,False,default,,,,,
13,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,1,312s5v,blog.urx.com,The Science of Crawl: Improving on PageRank,https://www.reddit.com/r/MachineLearning/comments/312s5v/the_science_of_crawl_improving_on_pagerank/,jisaacso,1427907180,,0,1,False,http://a.thumbs.redditmedia.com/AtmJ_-2Wir1DDWHchL8lxc2MiZ4SLl1RWqeIRllihO4.jpg,,,,,
14,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,3,3134c6,cse.wustl.edu,Marginalized dAE (seems to be a clear improvement over dAE in training speed and feature quality) - PDF,https://www.reddit.com/r/MachineLearning/comments/3134c6/marginalized_dae_seems_to_be_a_clear_improvement/,[deleted],1427911230,,7,4,False,default,,,,,
15,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,3,3136wk,mechlab-engineering.de,Predicting inner-city parking space occupancy [OC],https://www.reddit.com/r/MachineLearning/comments/3136wk/predicting_innercity_parking_space_occupancy_oc/,balzer82,1427912064,,4,2,False,http://b.thumbs.redditmedia.com/Vt-act8QU8_FYZ4RVGCBXlhjjlLQvO6j6XSarE4SaXk.jpg,,,,,
16,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,3,313dv8,datasciencecentral.com,"Predicting Flights Delay Using Supervised Learning, Logistic Regression",https://www.reddit.com/r/MachineLearning/comments/313dv8/predicting_flights_delay_using_supervised/,vincentg64,1427914399,,5,0,False,http://b.thumbs.redditmedia.com/OafNwPZv9M61rgD3N8pWHoDgLVYF127mTanBqvygJUw.jpg,,,,,
17,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,6,313z9z,rhiever.github.io,Machine Learning Maps Out the Best American Roadtrip Possible,https://www.reddit.com/r/MachineLearning/comments/313z9z/machine_learning_maps_out_the_best_american/,KeponeFactory,1427922174,,4,0,False,default,,,,,
18,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,10,314zc6,youtube.com,Richard Socher's Deep Learning for NLP course video,https://www.reddit.com/r/MachineLearning/comments/314zc6/richard_sochers_deep_learning_for_nlp_course_video/,evc123,1427936929,,15,61,False,http://b.thumbs.redditmedia.com/JXz727FXPE4dztE4M90FA6_OEaoX0wSYH5xpz2Wht2Q.jpg,,,,,
19,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,13,315m8b,tech.zalando.com,You Too Can Save Loads of Money: The Details of the Bayesian Model,https://www.reddit.com/r/MachineLearning/comments/315m8b/you_too_can_save_loads_of_money_the_details_of/,__Joker,1427948115,,1,28,False,http://b.thumbs.redditmedia.com/2eugJq-mVNvagAG1B3T8JfZ1YVEF8lfbbvcyjxxLW5k.jpg,,,,,
20,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,21,316la6,self.MachineLearning,How can huge bias be handled in dependent variable,https://www.reddit.com/r/MachineLearning/comments/316la6/how_can_huge_bias_be_handled_in_dependent_variable/,dileep31,1427976265,I am working on a classification problem where the dependent variable is whether a user converts or not. The conversion rate is around 5% and so my sample is heavily skewed by users who don't convert. Using a ctree I could get three segments where the 5% users got equally distributed. But I want a better model where majority of these 5% fall in one segment. Can I apply some treatment on the dv to achieve this?,0,1,False,self,,,,,
21,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,21,316myb,self.MachineLearning,[Question] Difference between Cellular Neural Networks and Convolutional Neural Networks.,https://www.reddit.com/r/MachineLearning/comments/316myb/question_difference_between_cellular_neural/,Tom-Demijohn,1427977396,Both of them are called CNNs. Anyone has used Cellular NNs and can compare both? ,5,4,False,self,,,,,
22,MachineLearning,t5_2r3gv,2015-4-2,2015,4,2,23,3174pp,machinetoolsemart.com,Products Details - MachineToolseMart.com Electronics,https://www.reddit.com/r/MachineLearning/comments/3174pp/products_details_machinetoolsemartcom_electronics/,saravananvrms,1427986757,,3,0,False,default,,,,,
23,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,1,317exn,blogs.technet.com,"Building models on the very large NYC taxi dataset using ""Learning with Counts""",https://www.reddit.com/r/MachineLearning/comments/317exn/building_models_on_the_very_large_nyc_taxi/,MLBlogTeam,1427991309,,0,1,False,http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg,,,,,
24,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,2,317oyn,radar.oreilly.com,A real-time processing &amp; analytics revival,https://www.reddit.com/r/MachineLearning/comments/317oyn/a_realtime_processing_analytics_revival/,gradientflow,1427995664,,0,0,False,http://b.thumbs.redditmedia.com/JikmiMOkEfOdIPCj3j5IO0OcNh2sAQXsm3XiOCJMzng.jpg,,,,,
25,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,6,318j77,self.MachineLearning,RBM with given connections,https://www.reddit.com/r/MachineLearning/comments/318j77/rbm_with_given_connections/,bioMatrix,1428009120,"I would like to create a restricted Boltzmann machine where the hidden layer has known interpretation.  As such, I want the connections between the hidden layer and visible layer to be non-zero only where I specify.  This is in contrast to most RBMs I see, where there are connections between all hidden nodes and all visible nodes.  My question is, is it reasonable to just train with normal contrastive divergence, but constrain to non-zero updates to the connection matrix only where I want the connections?  Or does that break the likelihood calculation?",3,1,False,self,,,,,
26,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,10,319g89,self.MachineLearning,Language-agnostic text analytics libraries and APIs,https://www.reddit.com/r/MachineLearning/comments/319g89/languageagnostic_text_analytics_libraries_and_apis/,[deleted],1428025045,"I do text analytics and light NLP in Python and R. 

I work on non-European ancient languages for which tools are basically  non-existent. Stemming, lemmatization, word segmentation are all unsolved problems. For some of these languages we don't even have electronic dictionaries.

What tools, libraries and APIs do you suggest to do text analytics on these languages?",4,11,False,default,,,,,
27,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,11,319lvz,nytimes.com,Learning to See Data,https://www.reddit.com/r/MachineLearning/comments/319lvz/learning_to_see_data/,kunjaan,1428027958,,0,2,False,http://a.thumbs.redditmedia.com/7m_OZssRmOGlsCJYlsTpeYzhNzLcr0maIDgpfhM-fh0.jpg,,,,,
28,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,15,31a9qs,datasciencecentral.com,6 Cloud Based Machine Learning Services,https://www.reddit.com/r/MachineLearning/comments/31a9qs/6_cloud_based_machine_learning_services/,urinec,1428042870,,1,3,False,http://b.thumbs.redditmedia.com/qCjg426v--hynbbALCn_QA7dRCwW5pNvsoDnM4htm5s.jpg,,,,,
29,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,16,31adpj,self.MachineLearning,"Why is backpropagation in neural networks a distinct concept from ""compute the gradient""?",https://www.reddit.com/r/MachineLearning/comments/31adpj/why_is_backpropagation_in_neural_networks_a/,Orborde,1428046279,"In neural networks, backpropagation is the name used to describe the algorithm used to compute the gradient of the error function with respect to the network weights. But the backpropagation ""algorithm"" seems to be exactly equivalent to working through the equations of the network to compute the gradient the same way you would any other system of equations.

Why does backpropagation have a special name?",12,4,False,self,,,,,
30,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,16,31af4x,self.MachineLearning,I am a new comer here for a wheat flour machine,https://www.reddit.com/r/MachineLearning/comments/31af4x/i_am_a_new_comer_here_for_a_wheat_flour_machine/,wheatflourmachine,1428047620,,0,0,False,default,,,,,
31,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,17,31ag14,self.MachineLearning,Backpropagation in Neural Network after testing on an image.,https://www.reddit.com/r/MachineLearning/comments/31ag14/backpropagation_in_neural_network_after_testing/,imanishshah,1428048500,"I have a trained neural network for pedestrian detection. Now, suppose I test it on an image. And I show the box around the pedestrian on that image. Suppose, a human being makes some changes to the bounding box around the pedestrian, (Please assume that the changes are always correct) then can we backpropagate those changes in the neural network? will it be a correct thing to do?",2,0,False,self,,,,,
32,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,18,31akeq,english.cri.cn,Chinese Physicists Show How Quantum Computers Can Boost Machine Learning,https://www.reddit.com/r/MachineLearning/comments/31akeq/chinese_physicists_show_how_quantum_computers_can/,mosesvacarro,1428053075,,0,0,False,http://b.thumbs.redditmedia.com/FySRM1oEFitXiiW0a2tk-BDHHmkt61U8DooTlNtwbBA.jpg,,,,,
33,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,19,31ankv,github.com,Tree models with Scikit-Learn Great learners with little assumptions from Gilles Louppe #Pydata (Slides in pdf),https://www.reddit.com/r/MachineLearning/comments/31ankv/tree_models_with_scikitlearn_great_learners_with/,cast42,1428056169,,24,23,False,http://b.thumbs.redditmedia.com/xih34ZyC8p78g4Aj0P4fToYQIh-NrXxqeHAd4oEtVRw.jpg,,,,,
34,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,19,31aq0g,self.MachineLearning,what methods exist to reduce the number of free parameters for fully connected layers of in deep nets during training?,https://www.reddit.com/r/MachineLearning/comments/31aq0g/what_methods_exist_to_reduce_the_number_of_free/,donnaprima,1428058559,"
I have seen couple of works where it is shown that there is redundancy in the learned parameters of deep nets. But as I understand it, these are for analysis of *already learned* networks. 
The fully connected nets dominate the amount of parameters in deep nets, so I wanted to know if there exist methods which actually reduce the free parameters of these layers, (conv to full, and full to full) during *learning*? ",11,2,False,self,,,,,
35,MachineLearning,t5_2r3gv,2015-4-3,2015,4,3,23,31b6x8,self.MachineLearning,Gradient clipping RNNs,https://www.reddit.com/r/MachineLearning/comments/31b6x8/gradient_clipping_rnns/,spurious_recollectio,1428070311,"I've seen some improvement in training plain RNNs on long time series (my LSTM implementation seems is considerably slower) using orthogonal initialization and gradient clipping but after training for many epochs I've found that my RNNs start doing worse on both the training and validation set.  My guess is that this is because the biases in the network are becoming much larger than the weights though this may not be the real root of the problem.

In implementing gradient clipping I'm dividing any parameter (weight or bias) by its norm once the latter hits a certain threshold, so e.g. if dw is a derivative:

    if |dw| &gt; threshold:
        dw = threshold * dw/|dw|

The problem here is how |dw| is defined.  I'm defining it as sum(abs(dw)) where the sum is over the components of dw but this means components of larger matrices/vectors will be pushed to much smaller values than those of smaller matrices/vectors.  In particular biases will have much larger derivs than weights and this is effectively like giving them a much higher learning rate.  This is why I believe my network is becoming bias driven but I might be wrong.

I don't really understand the rational behind using this particular norm for gradient clipping (rather than e.g. the average value of the components) but it seems to be what others do so I'd like to ask if anyone can explain the rationalization or tell me what I'm doing wrong.",26,4,False,self,,,,,
36,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,0,31bec1,self.MachineLearning,Have anyone here had experience with Morphological Neural Networks?,https://www.reddit.com/r/MachineLearning/comments/31bec1/have_anyone_here_had_experience_with/,transhumanist_,1428074079,My professor just told me to have a look on this subject so I would love if someone gave me a brief explanation of what it is and what it does :D I have plenty experience with MLPs,1,3,False,self,,,,,
37,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,0,31bfl3,ustream.tv,GPU Technology Conference 2015 - Dr. Andrew Ng,https://www.reddit.com/r/MachineLearning/comments/31bfl3/gpu_technology_conference_2015_dr_andrew_ng/,vikashkodati,1428074678,,0,9,False,http://b.thumbs.redditmedia.com/Hi-3Yo97qtbI4mqki0wZ5pilnmYCpvogQ7pB3tIe_nU.jpg,,,,,
38,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,1,31bo6m,arxiv.org,A Probabilistic Theory of Deep Learning [interesting new directions],https://www.reddit.com/r/MachineLearning/comments/31bo6m/a_probabilistic_theory_of_deep_learning/,neuromorphics,1428078455,,7,16,False,default,,,,,
39,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,1,31bsid,twitter.com,Machine Learning at American Express: Benefits and Requirements,https://www.reddit.com/r/MachineLearning/comments/31bsid/machine_learning_at_american_express_benefits_and/,metalfreak101,1428080306,,1,0,False,http://b.thumbs.redditmedia.com/1rtbuMC6-1OjaVMe6Lr85kDNomXDI9k-SNFEeDcMZ0Y.jpg,,,,,
40,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,4,31cga3,partiallyderivative.com,Partially Derivative Episode 19: Morning Drinking Edition,https://www.reddit.com/r/MachineLearning/comments/31cga3/partially_derivative_episode_19_morning_drinking/,chrisalbon,1428090616,,0,27,False,http://a.thumbs.redditmedia.com/rfWYrGkTZ6dXaIUAmqggbSHdRaXEiOoWeJETxTzWZV0.jpg,,,,,
41,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,4,31cgup,learningwithdata.wordpress.com,Intro to Bayes Theorem with Continuous Prior,https://www.reddit.com/r/MachineLearning/comments/31cgup/intro_to_bayes_theorem_with_continuous_prior/,syrios12,1428090904,,0,7,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
42,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,6,31cpi4,youtube.com,Music Recommendations at Scale with Spark - Christopher Johnson (Spotify),https://www.reddit.com/r/MachineLearning/comments/31cpi4/music_recommendations_at_scale_with_spark/,kunjaan,1428094824,,3,11,False,http://b.thumbs.redditmedia.com/wiozNAFFHLxgqDCBYu3tdCj8ViHBd0ql-qr4tx8cLZE.jpg,,,,,
43,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,6,31cpkx,self.MachineLearning,A Probabilistic Theory of Deep learning,https://www.reddit.com/r/MachineLearning/comments/31cpkx/a_probabilistic_theory_of_deep_learning/,Fusionnex,1428094859,"[This](http://arxiv.org/abs/1504.00641) is quite a long paper, but it is extremely broad in scope. If the authors claims are true, it seems like this will be foundational material for machine learning, especially in the field of deep learning. I am interested to see what others think of the work.",1,8,False,self,,,,,
44,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,6,31csdw,gputechconf.com,Sessions and Tracks at GPU Technology Conference 2015,https://www.reddit.com/r/MachineLearning/comments/31csdw/sessions_and_tracks_at_gpu_technology_conference/,Tom-Demijohn,1428096161,,0,4,False,http://a.thumbs.redditmedia.com/UOHUIidZOAABIOAvWeFcYJkutuo4PDomxIlGNBoZna8.jpg,,,,,
45,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,10,31dki6,youtu.be,Neural Modularity Helps Organisms Evolve to Learn New Skills without Forgetting Old Skills,https://www.reddit.com/r/MachineLearning/comments/31dki6/neural_modularity_helps_organisms_evolve_to_learn/,Jallafsen,1428111089,,10,22,False,http://a.thumbs.redditmedia.com/_Lm7QmLU8AY9YnVnVI3cd_EZ1eN3uP3e0Hlcz0ym2I4.jpg,,,,,
46,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,15,31ebv4,self.MachineLearning,Any floating ideas on using deep learning for automatic attribute generation?,https://www.reddit.com/r/MachineLearning/comments/31ebv4/any_floating_ideas_on_using_deep_learning_for/,-Ulkurz-,1428129222,"I was wondering if there are any theories or publications on using deep learning to automatically generate attributes in a target table (build from multiple relational database), which can be used to build a model for predictions.

However, I'm not interested in improving the classifier accuracy but would rather like to concentrate on the automatic attribute generation for knowledge discovery. ",1,0,False,self,,,,,
47,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,16,31eepl,otoro.net,recurrent net learns to play 'neural slime volleyball' in javascript. can you beat them?,https://www.reddit.com/r/MachineLearning/comments/31eepl/recurrent_net_learns_to_play_neural_slime/,hardmaru,1428131742,,31,25,False,http://a.thumbs.redditmedia.com/2vb4OKPgwY6GF96an8GfKsGatefCV7T4H6NW-uXESo8.jpg,,,,,
48,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,19,31eqco,yuviral.com,Astonishing Mind Twisting Optical Illusion Paintings,https://www.reddit.com/r/MachineLearning/comments/31eqco/astonishing_mind_twisting_optical_illusion/,Alexandra_Perkins912,1428144108,,0,1,False,default,,,,,
49,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,19,31eqg2,self.MachineLearning,Best processor for a server running machine learning algorithms.,https://www.reddit.com/r/MachineLearning/comments/31eqg2/best_processor_for_a_server_running_machine/,sharmilas1wa,1428144208,"I'm into applying machine learning for large text corpuses.  Of late I was trying to run word2vec and my current system failed miserably.  So I plan to setup a proper server for machine learning that I can experiment with.  I will be working on text corpuses that range in size between 100 Gb to 1Tb. 

What configuration would you recommend for the server machine? 

What is the size and type of problems that you solve and what config do you use? 

 I plan to go with AMD processors instead of Intel as they are cheaper.  Are AMDs good alternatives or will it come back to bite me?  
I'm thinking of using a 16 Gb RAM. Will that be good enough?  

Does SSD make a difference?  If so, which brand do you recommend?  Are there differences in performance of SSDs based on brands?",18,0,False,self,,,,,
50,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,22,31eyzo,arxiv.org,Gradient-based Hyperparameter Optimization through Reversible Learning,https://www.reddit.com/r/MachineLearning/comments/31eyzo/gradientbased_hyperparameter_optimization_through/,Derpscientist,1428152404,,5,31,False,default,,,,,
51,MachineLearning,t5_2r3gv,2015-4-4,2015,4,4,22,31f3n4,dl.acm.org,Group recommendation: semantics and efficiency [PDF],https://www.reddit.com/r/MachineLearning/comments/31f3n4/group_recommendation_semantics_and_efficiency_pdf/,kunjaan,1428155902,,1,1,False,http://b.thumbs.redditmedia.com/SaGNygmpKSQdXbYjoffGPm9YFweGuhL7pnB5M5mZFmo.jpg,,,,,
52,MachineLearning,t5_2r3gv,2015-4-5,2015,4,5,1,31fk7i,self.MachineLearning,Converting target indices to one-hot-vector,https://www.reddit.com/r/MachineLearning/comments/31fk7i/converting_target_indices_to_onehotvector/,sidsig,1428165537,"Is there an efficient way of converting a list of integer target values to a one-hot matrix in python/numpy? I was looking for a solution but couldn't find an obvious one. I know this is an ML subreddit, but considering how common this task is, its probably still relevant here! 

Eg: n_labels = 3, target_vector = [0,2,1]
desired result: [[1,0,0],[0,0,1],[0,1,0]]

The list of target vectors can be huge, therefore a for loop would take forever. I am wondering if there are any cool numpy specific tricks that can be used to make the solution more efficient. ",11,2,False,self,,,,,
53,MachineLearning,t5_2r3gv,2015-4-5,2015,4,5,3,31g05y,self.MachineLearning,Fast Artificial Neural Networks Library,https://www.reddit.com/r/MachineLearning/comments/31g05y/fast_artificial_neural_networks_library/,Muirbequ,1428173871,"Has anyone had any experience using this library? I am currently using the python bindings at it seems that the networks it generates have the same training and testing error rates which makes me feel that it is configured incorrectly.

Currently I am loading the training and testing data and then training and testing every epoch. I'm iterating through all training data, getting MSE, and then doing the same for testing, and then continuing to the next training epoch. Both MSE's end up very similar for all iterations.",3,0,False,self,,,,,
54,MachineLearning,t5_2r3gv,2015-4-5,2015,4,5,4,31g5sk,self.MachineLearning,What's the latest in deep learning?,https://www.reddit.com/r/MachineLearning/comments/31g5sk/whats_the_latest_in_deep_learning/,rudyl313,1428176777,It's been around a year since I was following deep neutral nets. Hinton had decided that rectified linear activations with dropout and without any pretraining (stacked rbms) was the way to go. Any updates since then?,12,5,False,self,,,,,
55,MachineLearning,t5_2r3gv,2015-4-5,2015,4,5,5,31gc0j,github.com,DeepCL: convnets with OpenCL. Cmd line and Python. Runs on Windows and Linux.,https://www.reddit.com/r/MachineLearning/comments/31gc0j/deepcl_convnets_with_opencl_cmd_line_and_python/,Foxtr0t,1428179939,,6,54,False,http://b.thumbs.redditmedia.com/AXqcnd4COPESiT6dNYv5e0NC5q9RufHd3jRiAMMIC8A.jpg,,,,,
56,MachineLearning,t5_2r3gv,2015-4-5,2015,4,5,9,31h0nz,github.com,An efficient LSTM language model implemented in Caffe,https://www.reddit.com/r/MachineLearning/comments/31h0nz/an_efficient_lstm_language_model_implemented_in/,singularai,1428193178,,3,13,False,http://b.thumbs.redditmedia.com/ljAIkHHaxPz7oQ9uzelBG-SZ3KtEwqfVbuyJAWQkg_s.jpg,,,,,
57,MachineLearning,t5_2r3gv,2015-4-5,2015,4,5,11,31hfbx,self.MachineLearning,Neuromorphic chips and machine learning (discussion),https://www.reddit.com/r/MachineLearning/comments/31hfbx/neuromorphic_chips_and_machine_learning_discussion/,dsocma,1428201796,"I haven't heard much about [IBM's Neurosynaptic chips](http://research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml#fbid=vaEDlmFkFty) lately.  When I first heard of it, I thought it would be a huge game changer, and I still do, but I am wondering what other people think about neuromorphic chips and machine learning/AI.",6,2,False,self,,,,,
58,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,1,31j4ok,self.MachineLearning,Incremental training,https://www.reddit.com/r/MachineLearning/comments/31j4ok/incremental_training/,original-blackfire,1428251003,"I am new to ML...

Until now i saw that every usage of ALS was actually processing all the data , which means that the time will always get worst ( assuming no limit on the training data )

Is there a way to incrementally train ? ( e.g. used previously computed model/predictions)

Thanks",3,2,False,self,,,,,
59,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,2,31j8z9,knowm.org,Knowm.org | The Adaptive Power Problem,https://www.reddit.com/r/MachineLearning/comments/31j8z9/knowmorg_the_adaptive_power_problem/,Thistleknot,1428253242,,12,6,False,http://a.thumbs.redditmedia.com/-GYUROACMfoCB5G_23rnX7XduOVTn7kA_qyLp6pxfD8.jpg,,,,,
60,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,2,31jbiy,self.MachineLearning,"""Fill in the blanks"" algorithm",https://www.reddit.com/r/MachineLearning/comments/31jbiy/fill_in_the_blanks_algorithm/,[deleted],1428254574,...,12,0,False,default,,,,,
61,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,7,31k97k,self.MachineLearning,Typical to get call-back rate around 1% for data scientist roles?,https://www.reddit.com/r/MachineLearning/comments/31k97k/typical_to_get_callback_rate_around_1_for_data/,[deleted],1428271944,.,7,1,False,default,,,,,
62,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,8,31khhs,google-engtools.blogspot.com.ar,Bug Prediction at Google (2011),https://www.reddit.com/r/MachineLearning/comments/31khhs/bug_prediction_at_google_2011/,galapag0,1428276281,,3,40,False,http://b.thumbs.redditmedia.com/EbPGXJ_gr2H87eRmqqPXHL3b-RIeEiXb3QfIRaxL7Ms.jpg,,,,,
63,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,9,31kq13,docs.google.com,"Machine Learning Workshop with WEKA, by Baris Yuksel (slides)",https://www.reddit.com/r/MachineLearning/comments/31kq13/machine_learning_workshop_with_weka_by_baris/,mrdrozdov,1428280729,,0,0,False,http://a.thumbs.redditmedia.com/_Vc3Hzm21OzMN0tscdP-19zbGSiMJVDpoDwsRaK77f0.jpg,,,,,
64,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,18,31m0q1,self.MachineLearning,"Decided to teach myself Machine Learning, here's the curriculum I'm currently finishing up",https://www.reddit.com/r/MachineLearning/comments/31m0q1/decided_to_teach_myself_machine_learning_heres/,dieplstks,1428312131,"http://developingdipples.blogspot.com/2015/04/my-curriculum.html

Any suggestions for future courses/books would be awesome. Would also like any advice on finding a first position in the field, my work history is kinda lacking (I spent a few years in Europe playing poker before coming back to the states early last year). ",9,0,False,self,,,,,
65,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,20,31m7jl,self.MachineLearning,[Update] Video of my introductory machine learning seminar,https://www.reddit.com/r/MachineLearning/comments/31m7jl/update_video_of_my_introductory_machine_learning/,Prooffread3r,1428318651,"A few months ago, [I asked /r/MachineLearning for feedback](http://www.reddit.com/r/MachineLearning/comments/2umm75/feedback_on_my_intro_to_machine_learning/) on a presentation I was putting together for an introductory seminar on machine learning for students at McGill University.

The comments were quite helpful, and I finally gave the presentation last week, so I wanted to share the final screencast: [Introduction to data analysis using Machine Learning](https://www.youtube.com/watch?v=U4IYsLgNgoY).",0,5,False,self,,,,,
66,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,20,31m850,self.MachineLearning,identifying lead sentence in a paragraph,https://www.reddit.com/r/MachineLearning/comments/31m850/identifying_lead_sentence_in_a_paragraph/,new2machinelearning,1428319211,"Hi folks .. i am trying to ID the lead sentence in a para and hence trying to interpret the subject of the entire para. Though some heuristics help (like looking for determiners in the POS tagged data like ""The gear train"" and ""a beautifully crafted frame"" ) i would like to train a classifier. I am planning to use a few 100 sentences from different articles and after POS tagging them (so for instance the training data would look like ""JJ-NN-NNS-VBG-PRP$ .."" -&gt; ""Lead"") and am hoping i would be able to ID the lead sentence from a para. Of course the presumption is that when people are writing a para, the first sentence would contain the main idea about the para (an NN / NNP) and the rest of the sentences give further details. Could any of the good folks here, please point out major flaws with this approach / point me to something that's better ? googling hasn't helped (maybe, i am not writing the correct search query). Any help will be deeply appreciated

Regards,
vikram",2,0,False,self,,,,,
67,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,20,31mavq,self.MachineLearning,Which license should I use for the Machine Learning framework?,https://www.reddit.com/r/MachineLearning/comments/31mavq/which_license_should_i_use_for_the_machine/,datumbox,1428321500,"Hi guys,

Few months ago, I have open-sourced the Datumbox Machine Learning framework under GPLv3. Back then some of you commented that the particular [license is too restrictive](https://www.reddit.com/r/MachineLearning/comments/2jon5b/new_opensource_machine_learning_framework_written/). 

Given that the new version of the framework will soon be released soon, I would like to get your input on which license I should choose this time. Apache, MIT, GPL, something else?

**UPDATE:** Thank you for all of your comments and suggestions. We'll use an Apache license for the next version! :)",9,1,False,self,,,,,
68,MachineLearning,t5_2r3gv,2015-4-6,2015,4,6,23,31mrmn,self.MachineLearning,[x-post /r/mlquestions] Subsequence Identification in Time-Series Data?,https://www.reddit.com/r/MachineLearning/comments/31mrmn/xpost_rmlquestions_subsequence_identification_in/,hammerheadquark,1428331523,"EDIT:

Thanks everyone for the replies. This discussion was very helpful and informative for me.

*****

Hi /r/MachineLearning,

I posted this on /r/MLQuestions first because that seemed like the right place, but it doesn't seem too active over there.

I'm researching a problem that's a bit out of my comfort zone and I'm not sure where to start.

## The Problem

I'm dealing with pressure readings during package transport. I'm trying to identify when the package takes off and lands during shipment (it's sitting on the tarmac most of the time). This may happen multiple times per shipment. The way I see it, I'm trying to identify occurrences of a similarly-shaped subsequence for each shipment (something akin to finding instances of a particular word in dialogue).

Here are a few details about the data:

- Each instance of the data is a time series representing a single shipment.

- The data is unlabeled (I don't know which points belong to flight vs. tarmac).

- The pressure readings are recorded at regular intervals.

## My First Guess at How to Approach

This paper:

http://www.sciencedirect.com/science/article/pii/S0950705111000815

describes a method for generating a template for time series data using Dynamic Time Warping (DTW). I could hand-pick examples of the subsequence I'm looking for, create a template, then for each shipment, test every subsequence for closeness in the DTW-sense to my template. I'd pick the best ones that don't overlap or something.

## My Questions

My approach seems super slow. I figure this problem has to be similar enough to something else, but I'm not sure what. I'm not even sure of the name for my problem.

I'm hoping to find some relevant reading material or just some general advice (e.g. another problem similar enough that I could build from it).

Thanks in advance!",28,6,False,self,,,,,
69,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,0,31mv94,blogs.technet.com,Microsoft officially closed the acquisition of Revolution Analytics today,https://www.reddit.com/r/MachineLearning/comments/31mv94/microsoft_officially_closed_the_acquisition_of/,MLBlogTeam,1428333306,,0,1,False,default,,,,,
70,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,2,31ne12,memkite.com,Update with 57 papers to Deeplearning.University Bibliography,https://www.reddit.com/r/MachineLearning/comments/31ne12/update_with_57_papers_to_deeplearninguniversity/,atveit,1428341750,,0,0,False,http://b.thumbs.redditmedia.com/YJiLRsmLfh5Hr7HRYeU1sV7gFJAELNmBs_dRssUmoXw.jpg,,,,,
71,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,3,31nkur,arxiv.org,A Theory of Feature Learning,https://www.reddit.com/r/MachineLearning/comments/31nkur/a_theory_of_feature_learning/,[deleted],1428344695,,5,19,False,default,,,,,
72,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,3,31nlgq,self.MachineLearning,Help with feature selection for supervised learning classification?,https://www.reddit.com/r/MachineLearning/comments/31nlgq/help_with_feature_selection_for_supervised/,Silkut_,1428344965,"Hi all, I'm not sure if this is the right place to ask; I'm a complete newbie to machine learning and while I have some sci-kit classifiers ""working"" on my dataset I'm not sure if I'm using them correctly. I'm doing supervised learning with a hand labeled training set.

The problem is: each item in my data set is a dictionary with approx. 80 keys that are either text, boolean, or integers that I want to use as features. I have about 40,000 items and have hand labeled about 800 of them. Am I meant to select, for example, only boolean features to use, or only integers? Do I need to normalize the features (remove mean + scale to unit variance)? I'm currently not even going to attempt analysis of the text yet so it may be worth not even giving those features to the classifier. Would it be dumb to just try various permutations/combinations of features of the same type (ints)? It could also be that I'm approaching my dataset completely wrong... it's shaped like this:

[ [a, b, c, ...], [a, b, c, ...], [a, b, c, ...], ...] 

Essentially what I hope to achieve is a binary classification of each item in the dataset, basically just ""Good"" or ""Bad"" according to what I've hand labeled. I read that some classifiers work better on different data types, like Bernoulli Naive Bayes, and K Nearest Neighbors works when the ""decision boundary is very irregular"".

Ultimately I want a comparison of classifier accuracy across several different algorithms, in addition to hopefully isolating one that is actually accurate for classifying my data...

Any insight is appreciated, or if there's a better place to ask!",1,0,False,self,,,,,
73,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,4,31ntrt,self.MachineLearning,Neural network backpropagation help,https://www.reddit.com/r/MachineLearning/comments/31ntrt/neural_network_backpropagation_help/,Mierzen,1428348500,"Hi!

Could you please help me with a neural network?

If I have an arbitrary dataset:


Input 1| Input 2 | Expected Output 2 | Expected Output 2 | Actual output 1 | Actual output 2
---|---|---|---|---|---
0.1| 0.2 | 1 | 2 | 2 | 4
0.4| 0.5| 4 | 5 | 8 | 10

Let's say I have x hidden layers with different numbers of neurons and different types of activation functions each.

When running backpropagation (especially iRprop+), when do I update the weights?  Do I update them after calculating each line from the dataset?

I've read that batch learning is often not as efficent as ""on-line"" training. That means that it is better to update the weights after each line, right?

And do I understand it correctly:  an epoch is when you have looped through each line in the input dataset?  If so, that would mean that in one epoch, the weights will be updated twice?


Then, were does the total network error^1 come into play?


[1: [image](http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/error.jpg). From [here](http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture13.html).]


---

_tl;dr_:
Please help help me understand how backprop works",8,0,False,self,,,,,
74,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,5,31o5av,arxiv.org,Beyond Short Snippets: Deep Networks for Video Classification,https://www.reddit.com/r/MachineLearning/comments/31o5av/beyond_short_snippets_deep_networks_for_video/,tabacof,1428353459,,0,9,False,default,,,,,
75,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,6,31oded,self.MachineLearning,Face database for training head-tracking/detection,https://www.reddit.com/r/MachineLearning/comments/31oded/face_database_for_training_headtrackingdetection/,fawar,1428357021,"Hey there,

I'm looking for a good face databse on which I could train an AI so that it could identify/detect faces at a 0.75-8 accuracy?


Which one would be the easiest to use and most ""used""?

(IE something like MNIST)",1,0,False,self,,,,,
76,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,9,31p1iu,aisecurity.org,"AI Security, Ch. 3: ""Abstractions and Implementations""",https://www.reddit.com/r/MachineLearning/comments/31p1iu/ai_security_ch_3_abstractions_and_implementations/,[deleted],1428368359,,0,0,False,default,,,,,
77,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,10,31p3xg,self.MachineLearning,Minibatching prevents neural networks from learning on nonseparable data?,https://www.reddit.com/r/MachineLearning/comments/31p3xg/minibatching_prevents_neural_networks_from/,rcwll,1428369524,"A few weeks ago at work, we got our hands on an interesting data set that we -- for various reasons -- had to fit a deep(ish) neural network to as part of a multi-class problem.  The one notable feature about this data was that it wasn't separable: there were several points where the features were identical but with different labels.

When we did straightforward SGD it fit great, we got down to the Bayes error rate for the test data, and everyone was happy.  But the overhead of shuttling data to and from the GPU accounted for a significant portion of our training time.  The obvious solution was minibatching, which we implemented, only to watch our accuracy absolutely collapse.  The model basically never got past the ""choose the most frequent class label"" heuristic, no matter what ridiculous things we did to the network or what sort of update rule we used (SGD, NAG, Adadelta...).  If we dropped the minibatch size low enough (single digits) then it eventually got better, but doing 1 example at a time gave us the best performance as far as both accuracy wall-clock time to train, even with the overhead of moving the data around.

The network itself (when fit by SGD) was nothing special, 3 hidden layers all with leaky rectified linear units, and a softmax output; pretty much anything we tried seemed to work ok as long as we had 3 or more layers.  

Has anyone run across this sort of thing before?  Any idea what might be behind it?  Our initial thought was that having identical points with differing labels in a single minibatch might be somehow killing the gradients, but even when we experimented with making sure we only had a single version of a given point in any given minibatch, we saw exactly the same thing. 

If it's not obvious by this point, none of us are really neural network experts, so it's quite possible that this is well known and totally obvious, but a few afternoons of googling around hasn't turned anything up for us. 

Any ideas?",8,8,False,self,,,,,
78,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,15,31pxot,cashewroasting.com,"cashew roasting,OZSTAR,cashew nut machine",https://www.reddit.com/r/MachineLearning/comments/31pxot/cashew_roastingozstarcashew_nut_machine/,emrahcoskun,1428386631,,0,1,False,default,,,,,
79,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,15,31pydk,blog.shriphani.com,The Smallest Eigenvectors of a Graph Laplacian,https://www.reddit.com/r/MachineLearning/comments/31pydk/the_smallest_eigenvectors_of_a_graph_laplacian/,[deleted],1428387168,,1,1,False,default,,,,,
80,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,15,31pz1q,blog.shriphani.com,The Smallest Eigenvalues of a Graph Laplacian,https://www.reddit.com/r/MachineLearning/comments/31pz1q/the_smallest_eigenvalues_of_a_graph_laplacian/,shriphani,1428387668,,0,16,False,default,,,,,
81,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,15,31pzqe,fnl.es,An efficient online sequence tagger resource for GATE,https://www.reddit.com/r/MachineLearning/comments/31pzqe/an_efficient_online_sequence_tagger_resource_for/,fnl,1428388211,,0,2,False,default,,,,,
82,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,15,31q1bv,automatedtrader.net,"Best of the Blogs - AI, machine learning, data mining, and big data",https://www.reddit.com/r/MachineLearning/comments/31q1bv/best_of_the_blogs_ai_machine_learning_data_mining/,CruzHou,1428389506,,0,0,False,http://b.thumbs.redditmedia.com/EAc3oQnStl03wd8RLbCb8kYdhbd7_UNWEj6rFzBhqaA.jpg,,,,,
83,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,16,31q3uk,schoolofpensandpapers.weebly.com,Why It Is Important To Get A Crane Operator Training,https://www.reddit.com/r/MachineLearning/comments/31q3uk/why_it_is_important_to_get_a_crane_operator/,sommersjessica80,1428391613,,1,1,False,default,,,,,
84,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,16,31q55q,nuit-blanche.blogspot.com,A Probabilistic Theory of Deep Learning (x-post: r/Compressivesensing),https://www.reddit.com/r/MachineLearning/comments/31q55q/a_probabilistic_theory_of_deep_learning_xpost/,compsens,1428392821,,2,31,False,http://a.thumbs.redditmedia.com/htHzF63CLtfr4XrowvidDY3NGOu8NCvgPYsDlsEDoo4.jpg,,,,,
85,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,17,31q690,self.MachineLearning,(How) Should I bin categorical features to reduce dimensionality after one hot encoding?,https://www.reddit.com/r/MachineLearning/comments/31q690/how_should_i_bin_categorical_features_to_reduce/,dstryhard,1428393897,"Let's assume we have a (0/1) binary classification problem at hand and one of the categorical features can take N possible values. Feature hashing might be the only option if we had N ~1 million. One hot encoding would be ok for small N, but this seems like a fairly bad idea to me if N were around 1000.

A naive approach I can think of would be to:

* compute the average label for data corresponding to each category. 
* replace that categorical variable with a tier, derived from this average label. (for example, all categories with the average label between 0.4 and 0.5 would be assigned Tier 4)

This essentially places each category into a bin that can be one hot encoded to produce vectors of substantially smaller dimension. Is there an intelligent way of performing such a binning operation? (I found http://www.statsoft.com/textbook/optimal-binning but this seems like a sufficiently ubiquitous problem that there should be more I can read on the subject) Any links or keywords to search for would help. 

(sorry for the long winded question, I couldn't find a better way of phrasing it...)",1,0,False,self,,,,,
86,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,22,31qwrs,self.MachineLearning,"[Question] With L1/L2 Regularization in a neural network, why are the weights regularized, but not the biases?",https://www.reddit.com/r/MachineLearning/comments/31qwrs/question_with_l1l2_regularization_in_a_neural/,TheAlienDude,1428414400,"With L1/L2 Regularization in a neural network, why are the weights regularized, but not the biases?

This question came to me after seeing some of the biases on my output layer go to extreme values all the way from -30 to +30 (and keeps increasing, probably to infinity?)

Is there a way to stop the biases from growing too large/overfitting?",9,11,False,self,,,,,
87,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,22,31qxre,blogs.msdn.com,Machine Learning Tools Powerhouse with Azure RemoteApp,https://www.reddit.com/r/MachineLearning/comments/31qxre/machine_learning_tools_powerhouse_with_azure/,tredkar,1428414887,,0,0,False,http://b.thumbs.redditmedia.com/Nl9mLZtMETPocGCmpWmfKG-FxHb5zCXxnw7z_JgxurM.jpg,,,,,
88,MachineLearning,t5_2r3gv,2015-4-7,2015,4,7,23,31r0lr,nbviewer.ipython.org,Introduction to Pandas (Slides from Pydata 2015),https://www.reddit.com/r/MachineLearning/comments/31r0lr/introduction_to_pandas_slides_from_pydata_2015/,cast42,1428416298,,9,6,False,http://b.thumbs.redditmedia.com/c1jL5K9YKdZF3_r4giXQI9rq25COlYr1rxRbIIu2g4Y.jpg,,,,,
89,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,0,31r8gr,innsbigdata.wordpress.com,Towards an Era of Cognitive Computing: 10 Questions to Brenda Dietrich,https://www.reddit.com/r/MachineLearning/comments/31r8gr/towards_an_era_of_cognitive_computing_10/,scardax88,1428419963,,0,0,False,http://b.thumbs.redditmedia.com/AxT5Xk6oOOYrT7olnr2CIAcGst2brr55g-lkGhkJ9mk.jpg,,,,,
90,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,0,31rdbc,self.MachineLearning,Has there ever been an attempt to create a nondeterministic RNN? (or perhaps a CTRNN),https://www.reddit.com/r/MachineLearning/comments/31rdbc/has_there_ever_been_an_attempt_to_create_a/,[deleted],1428422105,"I have done some work with stochastic processes, enough so to know the power of an RNG. Has it ever been used in neural networks with some effectiveness?



If anyone is interested in what a stochastic approach can accomplish you can check this: https://github.com/eschkufz/stoke-release  (this is not mine)",2,1,False,default,,,,,
91,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,1,31rinf,arxiv.org,[1504.00941] A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,https://www.reddit.com/r/MachineLearning/comments/31rinf/150400941_a_simple_way_to_initialize_recurrent/,genneth,1428424382,,15,25,False,default,,,,,
92,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,1,31rjf1,blogs.technet.com,Preview of PyCon in Montreal this week,https://www.reddit.com/r/MachineLearning/comments/31rjf1/preview_of_pycon_in_montreal_this_week/,MLBlogTeam,1428424713,,0,1,False,default,,,,,
93,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,4,31s6hh,startup.ml,Machine Learning for Internet of Things (IoT) - avoiding extensive feature engineering,https://www.reddit.com/r/MachineLearning/comments/31s6hh/machine_learning_for_internet_of_things_iot/,arshakn,1428434345,,0,2,False,http://b.thumbs.redditmedia.com/oMFs0Rori27qFcIaNqib3Q0n15CTaMLsQbNL2Cbk9EE.jpg,,,,,
94,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,6,31sosb,self.MachineLearning,Can you recommend a reading order sorted by difficulty?,https://www.reddit.com/r/MachineLearning/comments/31sosb/can_you_recommend_a_reading_order_sorted_by/,Nixonite,1428442067,"Hello everyone,

Before I start, I'd like to say that I've read the FAQ.

Anyway I noticed that it's hard to finish one book and immediately start another one because there's not really a nice continuity for difficulty level. For example, I've currently read the following:

* Introduction to Statistical Learning - Tibshirani
* Mastering Machine Learning with Scikit-Learn

I'm currently reading:

* All of Statistics - Wasserman
* Machine Learning: The Art and Science of Algorithms that Make Sense of Data - Flach

So far it's not overwhelming. 

I tried to read ""Elements of Statistical Learning"" but it's too difficult without having first read the All of Statistics book. I was wondering if there are some other in-between books that you all might recommend that go between my ""currently reading"" books and the ""Elements of Statistical Learning"" book. I'm also wondering the same for the Bishop book on Pattern Recognition and the Murphy book...

Actually I'm **really** hoping that someone here might post a really long list of books in order of difficulty that can be read one after another without getting lost. I'm a math major so my professors in the math department have some good input on this, but I would like to hear answers from reddit too. 

Thanks for your time and input.",1,0,False,self,,,,,
95,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,7,31su4w,self.MachineLearning,"Out of curiosity, do you work in a open space office?",https://www.reddit.com/r/MachineLearning/comments/31su4w/out_of_curiosity_do_you_work_in_a_open_space/,[deleted],1428444354,"All the cool places like facebook, twitter, ect seems to have open-plan offices. I personally would never be able to work while having 50 people around me with lots of noise and zero privacy (it's unbearable for aspies like me). I don't really understand why it's so trendy right now. I have a office right now that I share with 2 people and that's perfect for me. It's not a job in machine learning though and I heard that all the cool companies have open space office now, hence why I ask.",0,1,False,default,,,,,
96,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,8,31t8ca,self.MachineLearning,[Question] Deep Learning workflow,https://www.reddit.com/r/MachineLearning/comments/31t8ca/question_deep_learning_workflow/,uhustick2002,1428450826,"Hi,

What's your workflow in deep learning model development esp. hyperparameter tuning for convnet?

I just started in this field and so far this is my workflow:

1. start with several small convnets, each with different filter size and strides, e.g. 11x11 (alexnet), 7x7 (Overfeat), 3x3 (OxfordNet)

2. gradually add depth and/or width (filter bank)

3. from the best model (accuracy- and runtime-wise), insert dropout layers (because it takes longer to converge and likely(?) to yield better result)


I would love to hear what's your take on this regard.

Thanks!

ps: apologies if this has been asked before, and please kindly point me to the right direction :)",10,8,False,self,,,,,
97,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,12,31tzi9,self.MachineLearning,Documentation for Mahout,https://www.reddit.com/r/MachineLearning/comments/31tzi9/documentation_for_mahout/,uforeader,1428463622,"I'm very new to Mahout, and am having difficulty finding a good source of documentation and tutorials. Anyone have experience with this book? http://www.amazon.com/Mahout-Action-Sean-Owen/dp/1935182684/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1428463435&amp;sr=1-1&amp;keywords=mahout

More specifically, I'm trying to 1) create a NxM matrix of user ratings of items (where N=row/users and M=columns/items) and 2) use RowSimilarity to calculate the cosine between all the users' vectors.

Any insight you can provide would be very helpful!",2,2,False,self,,,,,
98,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,12,31u091,self.MachineLearning,Do I need a degree to do ML?,https://www.reddit.com/r/MachineLearning/comments/31u091/do_i_need_a_degree_to_do_ml/,[deleted],1428463994,"Hello,

I'm at a juncture in my life, and would appreciate some advice.

I dropped out in high school (early in grade 11), and since then I've been working as a software developer. I'm now in my late 20s.

I'm reasonably proficient, I've done desktop as well as web programming, know multiple programming languages, concurrency, and fairly low level stuff.

I've also done some small projects involving ML, such as a simple OCR system and a toy project with NLP. 

I seriously think that ML is what I want to spend the rest of my life doing. Specifically, I'm interested in NLP, such as data mining / text extraction.

I recently read a survey on Stackoverflow, where they said that almost all machine language programmers have a maths degree / are doing a PhD.

That makes me wonder if I'm seriously getting ahead of myself, and if I should go back to school and get a degree before I'll have any hope of being able to do anything substantial in ML.

Just how much maths knowledge do I need for ML? Do I need to have a PhD or Masters degree, or can i just read a few books on my own, over a few months, and know 90% of the maths that I'll need, for doing big things with ML / NLP?",7,2,False,default,,,,,
99,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,16,31uknk,appleinsider.com,Apple tech applies machine learning to 3D mapping for accurate in-air gesture recognition,https://www.reddit.com/r/MachineLearning/comments/31uknk/apple_tech_applies_machine_learning_to_3d_mapping/,caribelrose,1428477097,,0,15,False,http://b.thumbs.redditmedia.com/TMStzGki_BfBxhN_Vh7tOIiEpWaeUyqJDNTkJAWXeGA.jpg,,,,,
100,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,18,31uruz,self.MachineLearning,[Newbie Question] I am training a neural network where my loss saturates very quickly. How do i diagnose this?,https://www.reddit.com/r/MachineLearning/comments/31uruz/newbie_question_i_am_training_a_neural_network/,millerduvall,1428483771,"I have an experiment where i am trying to match histogram like features in the final layer of my NN. I have found the error decreases very less before the loss function becomes constant. I have made a list of possible causes, but dont understand what to prioritize while diagnosing:

1. less data
2. wrong activation+initialization
3. larger /smaller architecture
4. data preprocessing

Can any experienced hand help me out about how they go about in general to fix a problem where their net is not learning much?",16,5,False,self,,,,,
101,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,21,31v6ro,aria42.com,Numerical Optimization: Understanding L-BFGS,https://www.reddit.com/r/MachineLearning/comments/31v6ro/numerical_optimization_understanding_lbfgs/,thvasilo,1428496041,,15,57,False,http://b.thumbs.redditmedia.com/Mt-orNx2EjIvEfCP8BZ6r5QVwbRDXGthC_ceqzTc_vA.jpg,,,,,
102,MachineLearning,t5_2r3gv,2015-4-8,2015,4,8,22,31vck9,face.cbs.dtu.dk,Face it!: What does your face say?,https://www.reddit.com/r/MachineLearning/comments/31vck9/face_it_what_does_your_face_say/,galapag0,1428499391,,4,5,False,http://b.thumbs.redditmedia.com/iEma1TwOkJFKP6-dtlMkd2EtT95LR5Ae-1afuqs8MkU.jpg,,,,,
103,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,1,31vxh7,blogs.technet.com,Try out the new text analytics service in the Azure marketplace,https://www.reddit.com/r/MachineLearning/comments/31vxh7/try_out_the_new_text_analytics_service_in_the/,MLBlogTeam,1428509051,,0,1,False,default,,,,,
104,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,1,31w1cy,blog.bigml.com,PAPIs Connect: Europes First Machine Learning Event for Decision Makers,https://www.reddit.com/r/MachineLearning/comments/31w1cy/papis_connect_europes_first_machine_learning/,czuriaga,1428510664,,0,1,False,default,,,,,
105,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,3,31wea3,self.MachineLearning,Fat-tailed data and SVM,https://www.reddit.com/r/MachineLearning/comments/31wea3/fattailed_data_and_svm/,TerryN12,1428516180,"Does SVM perform poorly when fat-tailed data with outliers is used? What are some things that could be done to improve learning with such data? Does the choice of kernel and/or kernel parameter change?

For dealing with outliers, I was thinking of transforming the data such that values +/- 2 standard deviations are assumed to be outliers and would be replaced with the closest marginal values. (then transform the data into the range [-0.9, 0.9])

Assuming fat-tails are undesireable with SVM, what are some transformations that could be considered to help alleviate the fat-tails?

(I'm using LIBSVM with the Matlab interface)",2,0,False,self,,,,,
106,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,3,31wfzo,arxiv.org,"(NYU, Facebook AI Research) Weakly Supervised Memory Networks",https://www.reddit.com/r/MachineLearning/comments/31wfzo/nyu_facebook_ai_research_weakly_supervised_memory/,[deleted],1428516893,,0,11,False,default,,,,,
107,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,4,31wmnt,self.MachineLearning,Deep recursive network for information retrieval?,https://www.reddit.com/r/MachineLearning/comments/31wmnt/deep_recursive_network_for_information_retrieval/,[deleted],1428519661,"Hi Guys
I am reasonably new to NPL. If I want to use recursive network for information retrieval with the help of word2vec/Glove to create my word representation, how would the pipeline look?",1,0,False,default,,,,,
108,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,5,31wx4m,github.com,ipython notebooks for advanced scikit_learn tutorial by O. Grisel #PyCon2015,https://www.reddit.com/r/MachineLearning/comments/31wx4m/ipython_notebooks_for_advanced_scikit_learn/,cast42,1428524025,,0,15,False,http://b.thumbs.redditmedia.com/SczJcNu6IgEBuDjo43FH3DaO6yCe1AhbqhvK6VmrwrI.jpg,,,,,
109,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,5,31wxkk,self.MachineLearning,I got into Aalto for Masters in Machine Learning,https://www.reddit.com/r/MachineLearning/comments/31wxkk/i_got_into_aalto_for_masters_in_machine_learning/,ganessh,1428524217,"I got an admit form Aalto for machine learning course. I found from online that this course is very good in Aalto. If someone has more info, can you please shed some light?",2,0,False,self,,,,,
110,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,8,31xme7,self.MachineLearning,Would a double major in computer science and statistics be beneficial in my case?,https://www.reddit.com/r/MachineLearning/comments/31xme7/would_a_double_major_in_computer_science_and/,Indusco,1428535097,"Come next September, I will be attending James Madison University in Virginia. Thanks to the AP courses I took in high school, I will be able to skip at the very least 16 credits. These credits will allow me to get a double major in Stats and CompSci without putting me over 120 credits in college, meaning that I won't need to take more than 5 classes per semester. It does however mean, that I will not have time for electives and I will have a very difficult course load come junior and senior year. I'm hard working and I enjoy challenges, but I wouldn't want to waste so much time if the double major wouldn't be beneficial. Given the above information, would you recommend getting a double major in the two fields or simply getting one and doing something else like research with my extra time? I still plan on getting internships over the summers, considering the fact that I won't need to take summer courses to get the double major. Also, I might choose to get ahead by taking a few summer credits  in my freshman year if I fail in finding an internship just in order to relieve some pressure in the coming years. Thanks for any help!",9,4,False,self,,,,,
111,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,9,31xrri,xzh.me,"Error in Text Understanding from Scratch Paper - Zhang, LeCun",https://www.reddit.com/r/MachineLearning/comments/31xrri/error_in_text_understanding_from_scratch_paper/,gwulfs,1428537632,,10,28,False,default,,,,,
112,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,9,31xufa,self.MachineLearning,Markov Chain Audio Generation,https://www.reddit.com/r/MachineLearning/comments/31xufa/markov_chain_audio_generation/,chrico031,1428538929,"I've played around with Markov Chains recently by building various text generators, and am wondering if anyone has any good resources on using Markov Chains to generate audio.  Say, if I were to feed the model a MIDI library as the training ""data"" as opposed to a text corpus....

Any resources I should look into?

Edit: I should add my initial thoughts on it are to convert the audio files to arrays in Python using the scikits.audiolab library, then using that to train the Markov Chain, then when it creates a new array from that re-run it through the audiolab library and output as an audio file. Does that make sense?",18,20,False,self,,,,,
113,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,12,31yj71,self.MachineLearning,Apache Mahout 0.10 Release,https://www.reddit.com/r/MachineLearning/comments/31yj71/apache_mahout_010_release/,akm_r,1428551146,"Hi /r/MachineLearning, we @ApacheMahout are cutting a release this week and would love to hear your questions and comments.

This release has some major changes from 0.9, including the new Spark and H2O back-ends, the new matrix math DSL, and a whole bunch of cleanup and bug fixes.

Let us know what we can tell you about it; we're also collaborating with the @ASFbigtop team to build out some more flexible deployment options and build more ML tools and applications.",12,10,False,self,,,,,
114,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,13,31ypqa,self.MachineLearning,What is the effect of RAM speed on a server's performance for text processing and machine learning?,https://www.reddit.com/r/MachineLearning/comments/31ypqa/what_is_the_effect_of_ram_speed_on_a_servers/,sharmilas1wa,1428554916,"Hi I asked a question regarding server setup on machine learning subreddit and I got some great advice from you all.  http://www.reddit.com/r/MachineLearning/comments/31eqg2/best_processor_for_a_server_running_machine/

Now I am stuck on something else.   I have the option of getting a 2400Mhz RAM (in which case I have to overclock the CPU) or get the 1600Mhz RAM inherently supported by the processor. 

 Does RAM speed affect the text processing speed (reading through several GBs of data)?  If it will give only a boost of 10% or so, I would prefer going with the lower RAM speed. If the speed up is significant I would prefer the higher one.  

Does RAM speed affect the speed of Neural Nets?  What speed do you use?

I tried googling around and found some links and this excellent review of RAM speeds http://www.anandtech.com/show/6372/memory-performance-16gb-ddr31333-to-ddr32400-on-ivy-bridge-igp-with-gskill .   But all of them are geared towards gaming and not programming.",4,1,False,self,,,,,
115,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,14,31yuuf,self.MachineLearning,Looking for Google paper about lack of datasets in the literature with labeling error.,https://www.reddit.com/r/MachineLearning/comments/31yuuf/looking_for_google_paper_about_lack_of_datasets/,bschu,1428558195,Sorry I don't remember any of the title. I am looking for a paper published by Google researchers about how machine learning literature uses mostly datasets without any labeling error. The conclusion was basically that the literature is highly skewed towards these datasets instead of more realistic datasets.,0,1,False,self,,,,,
116,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,15,31yyqi,self.MachineLearning,Sentiment by term in a document set,https://www.reddit.com/r/MachineLearning/comments/31yyqi/sentiment_by_term_in_a_document_set/,MajorDeeganz,1428561048,https://algorithmia.com/algorithms/nlp/SentimentByTerm,0,0,False,self,,,,,
117,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,16,31z1di,self.MachineLearning,"Doubt in this Linear Regression tutorial: When the predicted Y values are lesser than their previous ones for X=3 and X=5, why is it mentioned that X and Y are having a postive relationship??",https://www.reddit.com/r/MachineLearning/comments/31z1di/doubt_in_this_linear_regression_tutorial_when_the/,krishnakanthreadsit,1428563099,"In [this](http://onlinestatbook.com/2/regression/intro.html) tutorial, if you see just above Table 1, the following text is written.

*The example data in Table 1 are plotted in Figure 1. You can see that there is a positive relationship between X and Y. If you were going to predict Y from X, the higher the value of X, the higher your prediction of Y. *

But the predicted Y values have decreased for two X values, (3 and 5). Then why was it mentioned as ""the higher the value of X, the higher your prediction of Y"" in the above paragraph??? ",5,0,False,self,,,,,
118,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,16,31z2lt,arxiv.org,An Empirical Evaluation of Deep Learning on Highway Driving,https://www.reddit.com/r/MachineLearning/comments/31z2lt/an_empirical_evaluation_of_deep_learning_on/,iori42,1428564068,,0,3,False,default,,,,,
119,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,16,31z53z,self.MachineLearning,"I posed a question about Machine Learning and Artificial Intelligence, but my friends and I couldn't find an answer.",https://www.reddit.com/r/MachineLearning/comments/31z53z/i_posed_a_question_about_machine_learning_and/,1playerpiano,1428566275,"We have machine learning algorithms that help computers sort through images and classify them all on their own. We have algorithms that allow computers to identify objects and describe them uniquely using combinations of phrases and words it encountered in the past...

My question was this: is there any research / projects / experiments being done to implement machine learning for something like math? Is it even possible? If it is possible, it seems reasonable that within the next decade or so we could have machines that start off with a basic 'understanding' of math and learn how to do more complex arithmetic. You could start by teaching a machine the concepts of addition, subtraction, multiplication, division, and so forth, and expand, eventually teaching the machine complex math like differential geometry and linear algebra. I couldn't find anything definitive on the subject, so I thought I'd ask here.",12,2,False,self,,,,,
120,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,17,31z64e,self.MachineLearning,Input variance normalization,https://www.reddit.com/r/MachineLearning/comments/31z64e/input_variance_normalization/,wesolyromek,1428567190,"Is there any intuitive way to understand why reducing training dataset variance (i.e. to 1) can result in a faster gradient descent convergence during neural network learning? I can't even find any papers covering why can it be beneficial for the algorithm.
Could someone please help me?",4,3,False,self,,,,,
121,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,17,31z799,self.MachineLearning,The best parameters for the Stacked Denoising Autoencoders,https://www.reddit.com/r/MachineLearning/comments/31z799/the_best_parameters_for_the_stacked_denoising/,hnizdja2,1428568240,"Dear Redditors,

I'm currently experimenting with the SDAE parameters. My goal is to create latent variables for text documents using AA. I would like to ask you if you have any experiences with parameterizing the Stacked (denoising) AA which you can share with others - what is the best activation function, number of layers, etc. I know that it is strongly dependent on the particular task, nevertheless maybe there is some general knowledge about this field. 

Do you know about some comparisons (benchmarks) of different parameters? It would be the best - thanks in advance!

I'm using [theanets](https://github.com/lmjohns3/theanets) library for the experiments.",2,2,False,self,,,,,
122,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,20,31zlr2,blog.kaggle.com,New video series: Intro to machine learning with scikit-learn (Kaggle),https://www.reddit.com/r/MachineLearning/comments/31zlr2/new_video_series_intro_to_machine_learning_with/,Deterministic-Chaos,1428580604,,1,39,False,http://a.thumbs.redditmedia.com/7sxj0uk-hg6JSh5zsfkjialMB5mdJSwxva39qaKgj-4.jpg,,,,,
123,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,22,31zxve,thetalkingmachines.com,Spinning Programming Plates and Creative Algorithms - Talking Machines,https://www.reddit.com/r/MachineLearning/comments/31zxve/spinning_programming_plates_and_creative/,dustintran,1428587716,,0,30,False,http://b.thumbs.redditmedia.com/Bf48OF_9xBhrpE_TAleNaUaYage-tdtm5eViIjbDI4Q.jpg,,,,,
124,MachineLearning,t5_2r3gv,2015-4-9,2015,4,9,23,32054u,radar.oreilly.com,Building big data systems in academia and industry,https://www.reddit.com/r/MachineLearning/comments/32054u/building_big_data_systems_in_academia_and_industry/,gradientflow,1428591166,,0,1,False,http://b.thumbs.redditmedia.com/JikmiMOkEfOdIPCj3j5IO0OcNh2sAQXsm3XiOCJMzng.jpg,,,,,
125,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,0,3208jr,quantopian.com,Stock trading algorithm generates 224.7% return over 2.5 years using news and blog NLP sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/3208jr/stock_trading_algorithm_generates_2247_return/,bostonrowerguy,1428592737,,0,1,False,default,,,,,
126,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,0,320977,reddit.com,"The weekly discussion thread on /r/DailyProgrammer is about machine learning this week. If you have any expertise to share or cool things to talk about, please pay us a visit!",https://www.reddit.com/r/MachineLearning/comments/320977/the_weekly_discussion_thread_on_rdailyprogrammer/,Elite6809,1428593014,,0,9,False,http://a.thumbs.redditmedia.com/PDQadCzYX_x1bU3KrYuhTptu6eDdOVVagFG6q_Afyb4.jpg,,,,,
127,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,1,320eew,blogs.technet.com,"Free ready-to-use templates for online fraud detection, retail forecasting &amp; text classification",https://www.reddit.com/r/MachineLearning/comments/320eew/free_readytouse_templates_for_online_fraud/,MLBlogTeam,1428595273,,0,1,False,default,,,,,
128,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,3,320xdc,aws.amazon.com,Introducing Amazon Machine Learning  Make Data-Driven Decisions at Scale,https://www.reddit.com/r/MachineLearning/comments/320xdc/introducing_amazon_machine_learning_make/,somnophobiac,1428603307,,22,86,False,http://b.thumbs.redditmedia.com/qbO4ZnEk2uP1i7gSFwdSuVnAk8LyKb7GvNPtToWGr2g.jpg,,,,,
129,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,6,321qtg,technologyreview.com,What is IBM looking for in Numenta?,https://www.reddit.com/r/MachineLearning/comments/321qtg/what_is_ibm_looking_for_in_numenta/,nested_dreams,1428616014,,47,17,False,http://b.thumbs.redditmedia.com/CK6xpeCQie_RLbtt-c0UtBJXX1OCbt0KPXNEn6mOPUM.jpg,,,,,
130,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,10,322jjc,phdata.io,Exploring Spark MLlib,https://www.reddit.com/r/MachineLearning/comments/322jjc/exploring_spark_mllib/,G2Addict,1428629851,,0,5,False,default,,,,,
131,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,13,323549,ausy.tu-darmstadt.de,Tutorials on Reinforcement Learning,https://www.reddit.com/r/MachineLearning/comments/323549/tutorials_on_reinforcement_learning/,[deleted],1428641514,,0,1,False,default,,,,,
132,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,14,323av2,blog.shakirm.com,A Statistical View of Deep Learning (III): Memory and Kernels,https://www.reddit.com/r/MachineLearning/comments/323av2/a_statistical_view_of_deep_learning_iii_memory/,iori42,1428645450,,6,13,False,http://b.thumbs.redditmedia.com/UQt9coo-FRE3MYBwRNn_zeiYTdXgX2JZUQmMd47_AVM.jpg,,,,,
133,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,15,323fpo,self.MachineLearning,[Question] Signal Variance Issues in networks with rectifier units and a softmax layer.,https://www.reddit.com/r/MachineLearning/comments/323fpo/question_signal_variance_issues_in_networks_with/,donnaprima,1428649143,"Hi,
The ReLU nonlinearites being unbounded may increase the variance of their outputs as training proceeds. This may be problematic if the the final layer is a softmax/sigmoid, which might become saturated. What are the best practices for working in this setting?",7,1,False,self,,,,,
134,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,18,323pid,wheat-flour-machine.com,A good wheat flour milling can do you a good job when processing the wheat,https://www.reddit.com/r/MachineLearning/comments/323pid/a_good_wheat_flour_milling_can_do_you_a_good_job/,wheatflourmachine8,1428657913,,2,0,False,default,,,,,
135,MachineLearning,t5_2r3gv,2015-4-10,2015,4,10,20,323xx8,open.blogs.nytimes.com,Extracting Structured Data From Recipes Using Conditional Random Fields,https://www.reddit.com/r/MachineLearning/comments/323xx8/extracting_structured_data_from_recipes_using/,iori42,1428665388,,0,58,False,http://a.thumbs.redditmedia.com/tcXY1dBPh1U2qj-fFLynjEYzLF11ZwtadppLv611Vu4.jpg,,,,,
136,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,0,324o6t,arxiv.org,Invariant backpropagation algorithm,https://www.reddit.com/r/MachineLearning/comments/324o6t/invariant_backpropagation_algorithm/,sdemyanov,1428680053,,17,13,False,default,,,,,
137,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,1,324tyx,self.MachineLearning,Question about anomaly detection,https://www.reddit.com/r/MachineLearning/comments/324tyx/question_about_anomaly_detection/,akiortagem,1428682631,"Hey /r/MachineLearning, I'm planning to do anomaly detection as my thesis. I'm using real-life data to do it, so there is no label available, hence I'm going on the unsupervised approach. One of the main goal of the thesis is to use ANN as a method and I'm planning to use SOM to do that. Do you think this is a good approach? Or is there any other better ANN algorithm to do anomaly detection?

Oh and also, if anyone had any good references on anomaly detection I would greatly appreciate it if you could share it with me.

Please help me /r/MachineLearning",5,3,False,self,,,,,
138,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,1,324y5z,blogs.technet.com,Free webinar next Tue on ready-to-use ML solutions in Azure Marketplace - no data science knowledge required,https://www.reddit.com/r/MachineLearning/comments/324y5z/free_webinar_next_tue_on_readytouse_ml_solutions/,MLBlogTeam,1428684567,,0,1,False,default,,,,,
139,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,3,3257u5,self.MachineLearning,Need Helping using NN to Generalize to Solve the Checkerboard Problem,https://www.reddit.com/r/MachineLearning/comments/3257u5/need_helping_using_nn_to_generalize_to_solve_the/,deeayecee,1428688906,"Thanks to [this post by](http://www.reddit.com/r/MachineLearning/comments/31ankv/tree_models_with_scikitlearn_great_learners_with/) /u/cast42on a PyData presentation by /u/glouppe I got a bug in my mind on capacity and generalization. After reading [this critique of decision trees,](http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+al-decisiontrees-2010.pdf) I can see Bengio's point: decision trees are definite victims of the ""curse of dimensionality"" for problems like the checkerboard.

I was pretty curious to see this in action. I generated an arbitrary checkerboard -- 10x10 for simplicity, coloring each square with a rule like bool(X1 % 2 == 0) ^ bool(X2 % 2 == 1)). For the time being, I plan to not be too evil, restricting the feature space to integers, maintaining class and point balance, etc. I'd like to develop a model that could generalize well to this problem, performing well under the following:

* Have full training data coverage on the 10x10 checkerboard and predict points correctly within that space.
* Have limited training data coverage on the 10x10 and still predict points correctly. For example, I'd like to drop out all (5, 5) from the training sample, but still predict (5, 5) correctly.
* Either of the above training scenarios, but correctly predict a point from outside the board (i.e. (11, 11)).

Without any sort of treatment of the feature space, 1 is easy for most classifiers, given correct parameters and a sufficient data sample (i.e. k in KNN or min_leaf_Samples in DT matching the number of samples per square), 2 is something KNN or DT will get wrong, and 3 is a scenario dictated by luck -- a DT will get (10, 11) wrong but (10, 12) right. I suspect you could get some luck in a highly dimensional checkerboard with Random Forest, but I suspect most of your ""success"" there would be by luck.

After working through the mental gymnastics a bit, I understand Bengio's critique. Now, I was hoping to pull some NN code off-the-shelf, train it on some checkerboard data and see how well it did in 2&amp;3. My intuition is that a sufficiently complex architecture should do well on this, but my problem is that I'm a bit of a rookie when it comes to NN and I was hoping you, /r/MachineLearning redditors, could help me out. Since I like Python, I started using theanets. Like I said, but I'm pretty naive to NN and theanets. I tried following the [theanets XOR example](https://github.com/lmjohns3/theanets/blob/master/examples/xor-classifier.py), even modifying to longer and wider architectures (2-20-20-20-20-1) for regression and (2-20-20-20-20-2) for classification, but I'm not getting very good results on any scenario -- 1, 2, or 3.

Have I missed the crux of the generalization point? Am I using theanets/NNs wrong? Thanks in advance for any help!",4,3,False,self,,,,,
140,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,3,325euc,self.MachineLearning,Tips/Tricks/Intuition to obtain Convolutional Neural Network Layer 1 filter convergence (smooth and orthogonal looking filters)?,https://www.reddit.com/r/MachineLearning/comments/325euc/tipstricksintuition_to_obtain_convolutional/,zZJollyGreenZz,1428692079,"Hello,

Does anyone have any good tips/tricks/intuition about how to get a Convolutional Neural Network to converge to smooth/orthogonal first layer filters?

I have about 25 classes with ~200 grayscale images of each class, which I have split into train/test (80/20 split) and I am achieving a high accuracy rate on the test set using a ConvNet.  The problem is that when I look at the first layer filters, they look really noisy, not smooth and orthogonal like many papers suggest that good ConvNet convergence should produce.  I've played with dropout as a method of regularization at various stages in the ConvNet, but I can't seem to achieve the smooth orthogonal first layer filters.

The network consists of standard input layer -&gt; conv -&gt; relu -&gt; pool -&gt; drop, combinations, similar to ideas listed in the cs231n class.

I have looked through a lot of papers on ConvNets and watched a lot of online ConvNet classes and lectures.  Nothing I have tried has seemed to produce nice first layer filters.  Also, I just haven't gotten a good intuition about what to try or what to do when I see the first layer filters are not converging to nice smooth/orthogonal filters.

As a bit of background, I have taken the Andrew Ng Coursera Machine Learning course, completed a few of the UFLDL tutorials (auto-encoders), read many papers, completed lots of tutorials and implemented several 2-4 layer neural network and convolutional neural network examples using Theano and Caffe.  Some example datasets were ones I thought up, some were published ones like MNIST etc..

Things I am about to try...
1.) So far I have not preprocessed the data.  Subtracting the mean is one of the next things I was going to try...
2.) Downloading the TinyImageNet images or CIFAR-10 / CIFAR-100, or maybe even MNIST, training a published ConvNet that achieves high accuracy and also produces the nice filters and then tweaking the network until it gets noisy filters.  This would help me understand what leads to noisy filters and what leads to smooth orthogonal filters.
",16,5,False,self,,,,,
141,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,5,325tpg,people.csail.mit.edu,"Turning Big data into tiny data: Constant-size coresets for k-means, PCA and projective clustering",https://www.reddit.com/r/MachineLearning/comments/325tpg/turning_big_data_into_tiny_data_constantsize/,muktabh,1428698860,,1,28,False,default,,,,,
142,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,6,325vq5,self.MachineLearning,How to learn neuromorphic engineering?,https://www.reddit.com/r/MachineLearning/comments/325vq5/how_to_learn_neuromorphic_engineering/,nlpkid,1428699789,How do I learn neuromorphic engineering with little background in electronics.,22,0,False,self,,,,,
143,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,6,325zgm,arxiv.org,End-to-End Training of Deep Visuomotor Policies,https://www.reddit.com/r/MachineLearning/comments/325zgm/endtoend_training_of_deep_visuomotor_policies/,True-Creek,1428701544,,2,7,False,default,,,,,
144,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,6,3262wl,mapr.com,An Inside Look at the Components of a Recommendation Engine | MapR,https://www.reddit.com/r/MachineLearning/comments/3262wl/an_inside_look_at_the_components_of_a/,caroljmcdonald,1428703129,,0,2,False,http://b.thumbs.redditmedia.com/kwsCfgYHybrVizmmW3LV5XxXzibrrRyQzFrFdhRH0QM.jpg,,,,,
145,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,7,326499,self.MachineLearning,Q about NNs and embeddings,https://www.reddit.com/r/MachineLearning/comments/326499/q_about_nns_and_embeddings/,test3545,1428703773,"Lets pretrain convnet on ImageNet.

Lets throw away classifier and lets get embedding(hash) for each input image using activations of the last layer.

We could check euclidian distances from a given hash to hashes of known classes. If distance is too big, we set aside that hash.

Later on we could separate those unclassified hashes as new classes*, as long as ImageNet features allow to separate them. 

* Notice that ""new"" classes could overlap with old classes. 

So hashes of flamingos would go to new flamingo class, but images of dogs whose hashes were far away from dog[i] hashes would be classified as dog[i+1] class

Am I inventing something like RBF net based classifier? 

One way or another, advantage seems to be easiness of adding new classes or braking one class into multiple ones (going from identifying dogs to identifying dog breeds etc).
Plus model would know when it is looking at something new and thus know it could not recognise image.
 
And idea is trivial to implement. Why nobody seems to be using this?",5,2,False,self,,,,,
146,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,8,326dsu,blog1.unanimous.ai,"""Social Swarming"" merges human and computer intelligence.",https://www.reddit.com/r/MachineLearning/comments/326dsu/social_swarming_merges_human_and_computer/,[deleted],1428708659,,1,1,False,default,,,,,
147,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,17,327o2t,self.MachineLearning,Question about uniqueness of weights in neural network,https://www.reddit.com/r/MachineLearning/comments/327o2t/question_about_uniqueness_of_weights_in_neural/,osazuwa,1428739478,"When training a neural network by backpropagation, the weights are not unique - each time I redo the fit, I get a different set of weights. However the optimization function (sum squared error in my case) is convex, which means there is one global minima.  So I suppose this minima has several equivalent sets of weights that map to it.   What is the name of this phenomenon and where can I learn more about it?",6,6,False,self,,,,,
148,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,19,327vcb,self.MachineLearning,Does someone can send me Cuda ConvNet trained layers of 2012 Alex Krixhevsky net?,https://www.reddit.com/r/MachineLearning/comments/327vcb/does_someone_can_send_me_cuda_convnet_trained/,[deleted],1428747467,"I need to work on his net, but since the training is very long i'm looking for someone who can send me well-trained checkpoints. Anyone?",0,1,False,default,,,,,
149,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,19,327vxr,self.MachineLearning,"I need the parameters of Alex Krixhevsky 2012 net, can someone send them to me?",https://www.reddit.com/r/MachineLearning/comments/327vxr/i_need_the_parameters_of_alex_krixhevsky_2012_net/,[deleted],1428748091,"I need to work on his net, but since the training is very long i'm looking for someone who can send me well-trained cuda convnet checkpoints. Anyone?",4,0,False,default,,,,,
150,MachineLearning,t5_2r3gv,2015-4-11,2015,4,11,21,3282n9,datasciencetech.institute,"Why is Big Data so painful to deal with? Don't forget size also means ""combinatorics""!",https://www.reddit.com/r/MachineLearning/comments/3282n9/why_is_big_data_so_painful_to_deal_with_dont/,datasciencetech,1428754699,,0,1,False,default,,,,,
151,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,1,328pg1,self.MachineLearning,How do you think that computer vision will impact our lives in the next decade?,https://www.reddit.com/r/MachineLearning/comments/328pg1/how_do_you_think_that_computer_vision_will_impact/,SottileP,1428768927,,31,13,False,self,,,,,
152,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,10,32ahg8,self.MachineLearning,Neural Network Learning Curve Question,https://www.reddit.com/r/MachineLearning/comments/32ahg8/neural_network_learning_curve_question/,jekyllstein,1428803025,"TLDR:  You can see my neural network learning curves here: http://imgur.com/0CL6LVY.  Which regularization term would you pick given that the test error actually drops below the training error at some point.  See implementation details below.

I just trained a neural network with 33 input units, 10 hidden units, and a single output unit.  The hidden units each use a sigmoid activation function and the final output is just a linear combination of those.  No activation function is used for the output because the goal is regression rather than classification.

I trained the network using backpropagation and some annealing at the end to find a better local minima.  I used a training set of 7800 reserving 2600 for testing the error and 2600 for cross-validation.  The error term I minimized was the mean squared error of the network output with the actual output.

I trained the network in several passes using different l2 regularization constants and I plotted the learning curves here:   http://imgur.com/0CL6LVY.  As expected the training error grows with the regularization term and the training/cv error shrinks.  

My question is, there is a point when the training error and test error cross and then the testing error becomes and stays smaller than the training error.  In examples I've seen there is an ideal regularization term that minimizes the test set error but it still lies above the training error.  What would cause the test set error to become so low?  Also, which regularization term would you pick?  The one where the test/training error cross or a larger term that further minimized the test set error.  Thanks.",12,8,False,self,,,,,
153,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,12,32asn7,datasciencecentral.com,New Batch of Machine Learning Resources and Articles,https://www.reddit.com/r/MachineLearning/comments/32asn7/new_batch_of_machine_learning_resources_and/,urinec,1428809349,,0,0,False,http://b.thumbs.redditmedia.com/VEdCMEiuPVDj4gZauhzTU_LdPm1f6oJNYc-6ccH3ShA.jpg,,,,,
154,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,14,32b26g,aws.amazon.com,Amazon Machine Learning  Make Data-Driven Decisions at Scale (amazon.com),https://www.reddit.com/r/MachineLearning/comments/32b26g/amazon_machine_learning_make_datadriven_decisions/,euri10,1428815892,,1,5,False,http://b.thumbs.redditmedia.com/qbO4ZnEk2uP1i7gSFwdSuVnAk8LyKb7GvNPtToWGr2g.jpg,,,,,
155,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,16,32bbvl,stats.stackexchange.com,data visualization - What's wrong with my derivation on stochastic neighbor embedding?,https://www.reddit.com/r/MachineLearning/comments/32bbvl/data_visualization_whats_wrong_with_my_derivation/,[deleted],1428824339,,0,2,False,default,,,,,
156,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,17,32beiq,self.MachineLearning,Does the hidden layer before the softmax layer need a bias neuron?,https://www.reddit.com/r/MachineLearning/comments/32beiq/does_the_hidden_layer_before_the_softmax_layer/,captcompile,1428827082,"Is it necessary for the hidden layer before the softmax layer to have a bias neuron?

I'm assuming that the answer is a yes, because for K classes, the softmax layer can be seen as K logistic regressions that must satisfy the constraint that their outputs must behave as probabilities?",6,2,False,self,,,,,
157,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,19,32blqx,fotiad.is,A comparison of open source tools for sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/32blqx/a_comparison_of_open_source_tools_for_sentiment/,sfotiadis,1428835405,,7,54,False,http://b.thumbs.redditmedia.com/FtXqxnTn9hAhvZFKrAX5F3CPE5ZON_FMooUFZbLkhpo.jpg,,,,,
158,MachineLearning,t5_2r3gv,2015-4-12,2015,4,12,23,32c1we,moalquraishi.wordpress.com,The state of probabilistic programming,https://www.reddit.com/r/MachineLearning/comments/32c1we/the_state_of_probabilistic_programming/,SuperFX,1428849099,,0,27,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
159,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,0,32canp,self.MachineLearning,How do you fit neural network with multiple outputs?,https://www.reddit.com/r/MachineLearning/comments/32canp/how_do_you_fit_neural_network_with_multiple/,osazuwa,1428854192,"This is a pretty basic question, but I'm not embarrassed about it so there.

This [great Youtube tutorial](https://www.youtube.com/watch?v=GlcnxUlrtek) taught me how to fit a neural network with [one output (image)](http://i.imgur.com/MBIYXR1.png)

To apply back-propagation, you first find the Jacobian of the loss function with respect to the weights, [like this](http://i.imgur.com/8okG94d.png)

Here Y is the output of the hidden layer.  In this example the output Y is continuous, so the loss function is squared error loss.

My question is this, how do you extend this to more than one output? [For example](http://neuroph.sourceforge.net/tutorials/images/MLP.jpg)

My initial thought was just to add the sum of squared error loss for each output node, conditional on them being on the same scale, such like [this](http://i.imgur.com/DKVxtC1.png)

However, my friend said this is wrong, something about ""cross terms that combine in the hidden layers.""

What is the right way to do it?",8,0,False,self,,,,,
160,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,4,32cynp,self.MachineLearning,How do people come up with new architectures?,https://www.reddit.com/r/MachineLearning/comments/32cynp/how_do_people_come_up_with_new_architectures/,[deleted],1428866323,"There are a lot of new and interesting architectures in neural networks, but I don't understand how they come up with them.

I am currently playing around with a problem, and used the relevant types of architectures that should work (feed-forward, recurrent), but they don't! I have spent a while analyzing the weights and have a pretty good idea of how data is represented in the weights and what the NN is trying to do and what it should be doing. It has become clear that I need a different architecture to deal with this problem and have to come up with one given what I have learned so far. But I am confused as to how people do this usually!

I guess one way is to tinker with modularity (splitting parts of the network or restricting certain weights to be equal to each other which is how convolutional networks work), and that has helped a bit.

Are there good frameworks or general tips to NN-researchers who need to alter their architectures to fit a specific problem?",1,0,False,default,,,,,
161,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,5,32d7yq,self.MachineLearning,Available technologies for predictions..?,https://www.reddit.com/r/MachineLearning/comments/32d7yq/available_technologies_for_predictions/,myke113,1428870894,"What technologies are currently available for predictions..?  As in, can predict when events are most likely to occur based on established patterns.

For instance, predicting from a call history who you're likely to call.",1,0,False,self,,,,,
162,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,5,32dah3,self.MachineLearning,Anyone know of any semi-supervised learning tools in R or Python?,https://www.reddit.com/r/MachineLearning/comments/32dah3/anyone_know_of_any_semisupervised_learning_tools/,[deleted],1428872134,"I have a dataset that is partially labeled and I wanted to explore semi-supervised techniques for classification, but I could only find one sklearn tool to do this, and I assume there is much more out there. Does anyone know of any packages for semi-supervised learning? It would be cool if I could try some semi-supervised random forest or svm classifiers on my data. Am I going to have to implement them on my own?",2,8,False,default,,,,,
163,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,6,32dc4k,self.MachineLearning,Binary classifier into three classes?,https://www.reddit.com/r/MachineLearning/comments/32dc4k/binary_classifier_into_three_classes/,SuperGore,1428872930,"Hello fellow redditors. I am a student taking Machine Learning course and one of the questions of my assessment is:

''Explain how a binary classifier (i.e. two-class classifier) can be extended to classify patterns belonging to three classes. A diagram may help. ''

Now my question is the following, isn't the extension just a combination of binary classifiers? I can't understand the question itself if someone could explain me I would appreciate it!

(Sorry if there are any spelling mistakes)
Thanks!",9,4,False,self,,,,,
164,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,6,32de6a,arxiv.org,Robust exponential binary pattern storage in Little-Hopfield networks [2012],https://www.reddit.com/r/MachineLearning/comments/32de6a/robust_exponential_binary_pattern_storage_in/,locster,1428873963,,1,5,False,default,,,,,
165,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,6,32dhu9,self.MachineLearning,how to do a Time series prediction?,https://www.reddit.com/r/MachineLearning/comments/32dhu9/how_to_do_a_time_series_prediction/,nickbomtempo,1428875750,"I have an input given by my supervisor of a project to test me, I have 50 numbers, my algorithm needs with 3 numbers predict the number 4, until finishing the entry, for example with index 1, 2, 3 guess the number in index 4 and in the index 2 , 3, 4 guess 5, does anyone know how to do this?",15,0,False,self,,,,,
166,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,8,32dphe,robusttechhouse.com,Amazon Machine Learning Review,https://www.reddit.com/r/MachineLearning/comments/32dphe/amazon_machine_learning_review/,mattrobust,1428879667,,1,1,False,http://b.thumbs.redditmedia.com/DV9--CePAwk7PywO8qeahy4UGbeLu1p0Zk3GzASmxEs.jpg,,,,,
167,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,8,32dsum,self.MachineLearning,UFLDL Softmax Regression,https://www.reddit.com/r/MachineLearning/comments/32dsum/ufldl_softmax_regression/,[deleted],1428881474,"I'm doing the UFLDL tutorials in this link: http://deeplearning.stanford.edu/tutorial/ and I've done the linear and logistic questions, now I'm up to the softmax regression and having trouble getting it to work. This is my current code

    for i = 1:m
        for k = 1:(num_classes-1)
            h = (exp(theta'*X(:,i)))./(exp(theta(:,k)' * X(:,i)));
            if (y(i) == k)
                g = g - (X(:,i) * (1 - h));
                f = f - log(h);
            else
                g = g - (X(:,i) * (0 - h));
            end
        end
    end

If someone could point me in the right direction it would help a lot.",0,0,False,default,,,,,
168,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,8,32dv5e,self.MachineLearning,Question: Why does adding additional variables decrease my SVM classifier accuracy?,https://www.reddit.com/r/MachineLearning/comments/32dv5e/question_why_does_adding_additional_variables/,jons_work_account,1428882736,"I'm trying to train an SVM classifier (libsvm) and I am seeing a peculiar problem. 

Each of my data points consists of 16 parameters (all continuous). When I perform a 5 fold cross validation with all of the data, but just 2 of the 16 parameters, the accuracy of the classifier is higher than when I do the same cross validation with all 16 of the parameters. What could cause this behavior? The only thing I can think of is that the data is completely mixed in 1+ parameters and these parameters are introducing noise into the data.",12,0,False,self,,,,,
169,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,9,32dwwq,self.MachineLearning,Image processing: how can we determine the quality of a model when the most interesting features are the smallest?,https://www.reddit.com/r/MachineLearning/comments/32dwwq/image_processing_how_can_we_determine_the_quality/,greatm31,1428883683,"My question is about how we score generative models against data. Generative models not only explain the data, but can be used to simulate new data.

Example: Say we are trying to determine a model of a particular plant based on an image of it. A ""model"" here is a structural blueprint: the size, orientation, and details of all its components (stem, leaves, flowers). To build such a model one could use a library of flower components and, *in silico*, simulate many possible plant combinations and check to see how well our model matches the image.

The question is, **what scoring function do we use to compare the model to the image**? A simple one is to project our model into 2D, calculate pixel values and compute a chi-square based on the pixel-wise agreement to the data image. However, while this kind of shape matching will guide us to a good global plant shape, it will become less helpful the closer we get to the right answer. That's because the interesting distinguishing features of a plant are very small - particular leaf or flower patterns, texture on the stems, etc. These features don't affect very many pixels but they're extremely important.

Are there **multi-scale** alternatives to this kind of scoring function? That is, it should treat features on multiple scales as equally important. Thanks for your help!",0,0,False,self,,,,,
170,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,9,32dzam,self.MachineLearning,Fixing evolutionary machine learning,https://www.reddit.com/r/MachineLearning/comments/32dzam/fixing_evolutionary_machine_learning/,DemiPixel,1428884915,"So I've had an interest in evolution machine learning and I've been working on a ""real time"" one where, instead of having ""generations"", the characters asexually reproduce with mutations.

I have lots of little... ""robots"" going around eating food. There is always enough food (so I make sure that this is a robot vs. robot rather than robot vs. environment project). There is a 20% chance of a ""bad food"" appearing. Food increases life span and red dots kill you immediately.

Robots are a set of neural networks, inputs from its two eyes (RGB, health, and I tried adding a ""temporary variable"" which I've removed) to the outputs (Turn left, turn right (both mean go straight), change its own RGB, and set its ""temp var"" which, again I've removed).

Firstly, I use to have it that eating food increased your life and declined the amount of time to have a ""baby"" but I have now removed that so the countdown is always the same.

But they kept going extinct from the bad food

I added a feature so that bad food would turn into good food after 15 seconds so that the ""last ones living"" would still have a chance. But they kept on going extinct.

So lastly, just recently, I added a feature that when one dies, if there are less than 10 robots on screen, it WILL reproduce (guaranteed) like before. Now I've just found that ""dumber"" robots sometimes take over. I've tried playing with the mutation setting and such and I am just stuck on how to get the best robot.

I'm hoping to expand this to a ""predator prey"" experiement as well where robots have a ""spike"" that give them food if they hit another robot (drains health) and lose food if they have their spike out at no robot.

So how do I make sure that they don't go extinct and I get the best selection? How could I expand it so they don't just die off or get stuck being dumb?

[Simulation](http://demipixel.net/evolution/)

*You'll probably want to run it on ""fast"". If you pause and click on a robot, you can see their neural network. Mouse over a node to see its connections. The two lines sticking out are their eyes and the color on either side of their body show what color they're seeing.*",1,0,False,self,,,,,
171,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,10,32e36x,self.MachineLearning,TIL feedforward ANNs are stacks of logistic regressions,https://www.reddit.com/r/MachineLearning/comments/32e36x/til_feedforward_anns_are_stacks_of_logistic/,brockl33,1428886973,"I just realized that in sigmoidal ANNs, an individual neuron is a multivariate logistic regression model which has form:

y = sigmoid(B0 + B1x1 + B2x2, ... + Bnxn)

And a feedforward net is a stack of multiple logistic models.

Did I miss something? Why has it taken so long for me to learn this (besides the fact that I am dumb)?",24,15,False,self,,,,,
172,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,18,32f9og,self.MachineLearning,ELI5 Factorization Machines,https://www.reddit.com/r/MachineLearning/comments/32f9og/eli5_factorization_machines/,HelmsmanRobertson,1428916096,"I've looked around this subreddit for material on Factorization Machines, and was chagrined to only find [this link](http://www.csie.ntu.edu.tw/~r01922136/libffm/). Does anyone have any pointers to introductory material, OR have an explanation, themselves, that motivates Factorization Machines?

*edit for grammar*",7,0,False,self,,,,,
173,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,18,32f9w2,self.MachineLearning,CNN stride size question,https://www.reddit.com/r/MachineLearning/comments/32f9w2/cnn_stride_size_question/,RapidLeaper,1428916306,"Hey, everyone! I'm new here but have read quite a bit into neural networks and am extremely interested in CNNs.
Just some quick questions I've been wondering about and haven't found much on.

1) What are some good tips to the choosing of the stride size? Are there any general rules, i.e. a smaller/larger stride size is better?

* It would make sense, to me, that a small window with a small stride length would be better at capturing the finer details, but would come with an increased computational complexity and the risk of not being general enough, while a large window with a larger stride length might not capture enough information. 


2) Could varying the stride length (randomly) increase generality?

* The input has to all the be size dimensions, right? But if there was a way to randomly vary the step size while maintaining the dimensions of the input. I feel that might have a positive impact on the generality of the information extracted. 

I'm definitely not too sure; these may be half/quarter baked ideas. Maybe I have no idea what I'm talking about. Any sort of input would be greatly appreciated. 

Thanks for your time!",15,1,False,self,,,,,
174,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,18,32fbzx,self.MachineLearning,How to use ML for game strategy?,https://www.reddit.com/r/MachineLearning/comments/32fbzx/how_to_use_ml_for_game_strategy/,teluthen,1428918231,"Hi all,
I'm interessted in ML, and i discovered a nice game https://robotgame.net/ which consists in coding python robots which fight vs other's robot.

Actually, even if this can be thoughts as ""Artificial Intelligence"" best robots are more about strategy and game comprehension.

I would like to work with ML to make a robot which would learn how to play instead of me teaching him.

I'm quite new to ML and does not have a strong background, what would you suggest in term of algorithms or any resources (reading, papers, ideas) in order to achieve it (or at least a nice way to go)?

Informations about rules/API o this game:
https://robotgame.net/api

Thx all",4,2,False,self,,,,,
175,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,18,32fc3b,stackoverflow.com,distant supervision: how to connect named entities to freebase (KB) relations,https://www.reddit.com/r/MachineLearning/comments/32fc3b/distant_supervision_how_to_connect_named_entities/,matthias_anglicus,1428918319,,0,2,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
176,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,19,32fecu,self.MachineLearning,What are your thoughts on Kurzweil?.,https://www.reddit.com/r/MachineLearning/comments/32fecu/what_are_your_thoughts_on_kurzweil/,Vortex_Gator,1428920490,"Specifically, regarding his ideas on How to Create a Mind using Hidden Markov Models?, and do you think his work at Google will amount to anything, or do you think it was a PR move on Googles side?.",79,20,False,self,,,,,
177,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,19,32ffpl,ai-maker.com,"How ""dangerous"" is it to optimise a highly peaked objective function with a gradient-based method?",https://www.reddit.com/r/MachineLearning/comments/32ffpl/how_dangerous_is_it_to_optimise_a_highly_peaked/,ai_maker,1428921759,,4,0,False,http://b.thumbs.redditmedia.com/zlNcuqBbVVe5j2aXlwhToh_tipbCpE7M5M42deDpwXA.jpg,,,,,
178,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,20,32fk5j,self.MachineLearning,Are there any algorithms for supervised classification with event log data?,https://www.reddit.com/r/MachineLearning/comments/32fk5j/are_there_any_algorithms_for_supervised/,[deleted],1428925588,"I have timestamped event log data, so my samples are like:

{eventA (time1), eventB (time2)} -&gt; 1    

{eventA (time3)} -&gt; 0    

etc.

Are there any algorithms to deal with this? As most time series classification methods (i.e. HMMs, etc.) assume that you have a continuous time series rather than a collection of instantaneous events at different times.

I really want an interpretable classifier so I can assign importance to ""eventA"", ""eventB"", etc.",7,6,False,default,,,,,
179,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,21,32fm5q,self.MachineLearning,Question about Autoencoder (AE),https://www.reddit.com/r/MachineLearning/comments/32fm5q/question_about_autoencoder_ae/,[deleted],1428927031,"Hi everyone.
I am relatively new to AE. I know that it is used to reconstruct the input and try to minimize the loss function. I have done some classification problems with feed-forward ANN and now I am wondering how I can apply AE in these kind of problems. If not, then what should AE be used for? Any help would be appreciated. Thanks in advance!",9,2,False,default,,,,,
180,MachineLearning,t5_2r3gv,2015-4-13,2015,4,13,22,32fvb8,self.MachineLearning,Converting transactional data into a frequent-pattern-mining dataset,https://www.reddit.com/r/MachineLearning/comments/32fvb8/converting_transactional_data_into_a/,iktl,1428932539,"I apologize if I misuse any terminology here, but I'm looking to run a market basket analysis on a transaction dataset using Spark. Almost every resource I can find in both books and online start midway through the process when you already have a dataset of the format:

Transaction | Items
---------|----------
1|Milk, Eggs, Ham
2|Milk, Eggs
3|Bacon, Eggs

Alternately, others deal with each item as a binary vector:

Transaction|Milk|Eggs|Ham|Bacon
---------|----------|---------|----------|----------
1|1|1|0|1
2|1|1|0|1
3|0|0|0|1

However, I have a more conventional transaction format dataset:

Transaction|Item
---------|----------
1|Milk
1|Eggs
1|Ham
2|Milk
2|Eggs
3|Bacon

I've found some incredibly inefficient ways to do a conversion of my format to the binary vectors using pandas iterating over transaction groups and get_dummies to make columns of each item within a transaction. On a small dataset, I'd be fine with this. However, my data is millions of records with a 100+ item pool so it feels computationally inefficient to do it this way.

Is there a shortcut method, distributed or otherwise, that would assist me in this? ",2,1,False,self,,,,,
181,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,2,32gkmj,self.MachineLearning,[dataset] ImageNet LSVRC-2010 #AlexNet #ConvNet,https://www.reddit.com/r/MachineLearning/comments/32gkmj/dataset_imagenet_lsvrc2010_alexnet_convnet/,Tom-Demijohn,1428944447,"Does someone have access to this dataset? I've send request to competition host via ImagNet competition website, but didn't receive back any message. 

EDIT: I finally got access. (Waiting time ~3  work days)",2,0,False,self,,,,,
182,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,3,32gx2k,databricks.com,for MLlib users: Deep Dive into Apache Spark SQLs Catalyst Optimizer,https://www.reddit.com/r/MachineLearning/comments/32gx2k/for_mllib_users_deep_dive_into_apache_spark_sqls/,gradientflow,1428950061,,0,1,False,http://b.thumbs.redditmedia.com/QJO9IK-btzEijlZ2Nxw31NMAnz7jfDuKqGZgVS67-XI.jpg,,,,,
183,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,3,32gzw0,self.MachineLearning,learning sequences of events,https://www.reddit.com/r/MachineLearning/comments/32gzw0/learning_sequences_of_events/,WJLeinberger,1428951283,,1,0,False,default,,,,,
184,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,3,32h0ez,self.MachineLearning,Why is PCA an unsupervised learning algorithm?,https://www.reddit.com/r/MachineLearning/comments/32h0ez/why_is_pca_an_unsupervised_learning_algorithm/,baleezo,1428951525,,2,0,False,default,,,,,
185,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h0o7,self.MachineLearning,Regarding face recognition,https://www.reddit.com/r/MachineLearning/comments/32h0o7/regarding_face_recognition/,[deleted],1428951630,,0,1,False,default,,,,,
186,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h16s,self.MachineLearning,Next Course on Advanced Algorithm,https://www.reddit.com/r/MachineLearning/comments/32h16s/next_course_on_advanced_algorithm/,mron0210,1428951865,,1,0,False,default,,,,,
187,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h1hv,self.MachineLearning,Research in Machine learning,https://www.reddit.com/r/MachineLearning/comments/32h1hv/research_in_machine_learning/,vineet22,1428952006,,2,0,False,default,,,,,
188,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h4u2,self.MachineLearning,Machine Learning in Helathcare,https://www.reddit.com/r/MachineLearning/comments/32h4u2/machine_learning_in_helathcare/,merdirafiei,1428953461,"Are you aware of any real-world ML applications in healthcare? To the best of my knowledge, the current literature seems to suggest that healthcare data is not reliable enough to produce accurate training sets! Thank You.",4,0,False,self,,,,,
189,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h4yl,self.MachineLearning,Question to Reddit AMA for Prof. Ng,https://www.reddit.com/r/MachineLearning/comments/32h4yl/question_to_reddit_ama_for_prof_ng/,dima0reddit,1428953523,,2,0,False,default,,,,,
190,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h5fi,self.MachineLearning,Best Tools for High-Dimensional Generative Modeling,https://www.reddit.com/r/MachineLearning/comments/32h5fi/best_tools_for_highdimensional_generative_modeling/,alexmlamb,1428953717,"Hello, 

I've read quite a few papers that aim to do generative modeling on high-dimensional random variables (like images).  I'm also interested in generative modeling of other structures like time series or hierarchical time series.  

Variational Autoencoder - Seems to get good results in practice.  No consistency guarantee.  May not be appropriate if one actually wants a calibrated model (i.e. P(Y &lt; y) is asymptotically correct).  

Mixture of Gaussians - Could learn a mixture of multivariate gaussians.  Could scale to high dimensional spaces by requiring each gaussian to have a diagonal covariance matrix.  Should be consistent, but difficult to optimize and very limited in the class of distributions it can model.  

NADE - Fits an RNN over the variables.  Makes most sense if the variables are from a time series or have a natural ordering.  

Generative Stochastic Networks.  

Generative Adversarial Networks - Should be consistent, but some have had trouble getting them to work for RNNs.  In my experience they were somewhat hard to optimize, though it could have been due to an error on my end.  

At least from what I've seen, it seems like we have yet to find a solution that's:

1.  Easy to optimize (but not necessarily convex)

2.  Able to model distributions with a large number of modes.  

3.  Consistent

Does anyone have a view on which models have the most promise for eventually achieving these properties?  Are there are any important algorithms that I've left out?  ",16,15,False,self,,,,,
191,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h7a9,self.MachineLearning,outcomes from teaching thru coursera,https://www.reddit.com/r/MachineLearning/comments/32h7a9/outcomes_from_teaching_thru_coursera/,hgoldberg,1428954499,,1,0,False,default,,,,,
192,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,4,32h84v,self.MachineLearning,Need help with cardiac MRI image segmenting project,https://www.reddit.com/r/MachineLearning/comments/32h84v/need_help_with_cardiac_mri_image_segmenting/,Malt08,1428954851,"Hi, I was wondering if anyone knew of a database with a decent number of cardiac MRI images that also includes a variety of extracted feature vectors. I'm doing a project in my pattern recognition class where I'm going to apply a SVM to data using matlab, and I've found quite a few cardiac MRI databases with just the images, but unfortunately we've covered next to nothing on how to actually extract features from an image. Either a database with some features already extracted, or somewhere to get me started on easily extracting some features myself would be really helpful!

edit: I'm looking specifically for cardiac MRI images where I can see the left ventricle",1,1,False,self,,,,,
193,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,5,32hbt0,datacamp.com,Free Interactive data science coding tutorials,https://www.reddit.com/r/MachineLearning/comments/32hbt0/free_interactive_data_science_coding_tutorials/,martijnT,1428956414,,0,4,False,http://b.thumbs.redditmedia.com/kQCF07aAAM-lWIorO7t1gtiu7XlKWNC2E0HRA2qe6yo.jpg,,,,,
194,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,5,32hfs0,self.MachineLearning,"value of ""m"" too low",https://www.reddit.com/r/MachineLearning/comments/32hfs0/value_of_m_too_low/,baruchatta,1428958129,,1,0,False,default,,,,,
195,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,7,32hpw4,self.MachineLearning,Convolution Neural Network,https://www.reddit.com/r/MachineLearning/comments/32hpw4/convolution_neural_network/,salehalis,1428962623,,3,0,False,default,,,,,
196,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,7,32hvba,self.MachineLearning,about Coursera and Baidu,https://www.reddit.com/r/MachineLearning/comments/32hvba/about_coursera_and_baidu/,[deleted],1428965008,"Hello Prof. Andrew first of all thank you a lot.
I just completed your course and soon a couple of my friends will IA.
We are MS students currently and we are really getting excited about Science and start to realize how cool it is.
I have couple of questions:
1.Instead of spending our time by doing PhD we want to join you in Baidu so that we can learn and apply faster. In order to meet the requirements on what should we focus on?
2.After taking your ML class  successfuly what topic do you recommend for MS students so they can publish paper within 1-1.5 years?
Thank you again May God bless you perfect health!",0,1,False,default,,,,,
197,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,8,32i1jy,self.MachineLearning,CNN &amp; RNN,https://www.reddit.com/r/MachineLearning/comments/32i1jy/cnn_rnn/,balajivenu,1428967761,"I would like to implement the hand written digit recognition using convolution neural networks. Do you think this will decrease the number of layers that i will need to recognise the digits? 

Also, additionally I would like to know what is a RNN and how different is it from a usual feed forward network.",1,0,False,self,,,,,
198,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,10,32ihpe,self.MachineLearning,AMA Andrew Ng and Adam Coates,https://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/,andrewyng,1428975502,"Dr. Andrew Ng is Chief Scientist at Baidu. He leads Baidu Research, which includes the Silicon Valley AI Lab, the Institute of Deep Learning and the Big Data Lab. The organization brings together global research talent to work on fundamental technologies in areas such as image recognition and image-based search, speech recognition, and semantic intelligence. In addition to his role at Baidu, Dr. Ng is a faculty member in Stanford University's Computer Science Department, and Chairman of Coursera, an online education platform (MOOC) that he co-founded. Dr. Ng holds degrees from Carnegie Mellon University, MIT and the University of California, Berkeley.
________________________________________

Dr. Adam Coates is Director of Baidu Research's Silicon Valley AI Lab. He received his PhD in 2012 from Stanford University and subsequently was a post-doctoral researcher at Stanford. His thesis work investigated issues in the development of deep learning methods, particularly the success of large neural networks trained from large datasets. He also led the development of large scale deep learning methods using distributed clusters and GPUs. At Stanford, his team trained artificial neural networks with billions of connections using techniques for high performance computing systems.",277,449,False,self,,,,,
199,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,11,32inon,self.MachineLearning,Anyone know of open-source or pre-trained CNN based speech recognition libs? Or FOSS speech datasets?,https://www.reddit.com/r/MachineLearning/comments/32inon/anyone_know_of_opensource_or_pretrained_cnn_based/,omniron,1428978424,"I have a project I want to do, but am limited by availability of good speech recognition based on CNN. Suggestions welcome.",3,1,False,self,,,,,
200,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,12,32iwu4,self.MachineLearning,Working examples for MDP and Game Theory,https://www.reddit.com/r/MachineLearning/comments/32iwu4/working_examples_for_mdp_and_game_theory/,kaniska_mandal,1428983320,It would be great if we can get links to some working examples for MDP and Game Theory.,0,0,False,self,,,,,
201,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,13,32iyt9,self.MachineLearning,[Question] Comparison between Softmax and Sigmoid for classification output.,https://www.reddit.com/r/MachineLearning/comments/32iyt9/question_comparison_between_softmax_and_sigmoid/,jungenfreud,1428984430,"Hi,
In terms of optimization/overfitting is there anythiing to choose from between Softmax and Sigmoid layers for the output of a classification net?

Are there any known behaviours like ease of optimization/ or lesser tendency towards overfitting between these?",10,2,False,self,,,,,
202,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,13,32j1d0,self.MachineLearning,Machine learning for time series forecast,https://www.reddit.com/r/MachineLearning/comments/32j1d0/machine_learning_for_time_series_forecast/,0fcy0,1428985924,,0,0,False,default,,,,,
203,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,14,32j5wf,self.MachineLearning,Need guidance,https://www.reddit.com/r/MachineLearning/comments/32j5wf/need_guidance/,rockingstar130,1428988878,,0,0,False,default,,,,,
204,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,14,32j67x,self.MachineLearning,Maching learning applications in retail and data analytics,https://www.reddit.com/r/MachineLearning/comments/32j67x/maching_learning_applications_in_retail_and_data/,egmacep,1428989078,"I'm dealing with description and prediction of consumer behavior. We operate a website for local deals (mainly non-food products: washing machines, cell phones, PCs, tables, chairs and the like).

We are currently facing 2 problems and I'd like your advice about how to deal with them: 
1. Automatic product classification: 
We automatically acquire all kind of products from retailers (thousands per day) and we need to allocate a category of each of them (categories like Smartphones, appliances, men shoes, etc.). We manage around 200 categories and each product must drop in only one of those and the right one. 

We cannot create a list of all products in the world and use to classify products (not feasible because there are a lot of different products and new ones appear all the time). 
 
What would be the approach, method and algorithms to use to create a fully automatic product classification system when you have a lot of categories? 

2. Consumer behavior prediction:
We're working to predict, with a high rate of success, what products the consumers will look for in the near future (next week for instance).

What approach to use? What method and algorithm? I don't really have any clue about it. 

Thanks Andrew and I really enjoyed ML and your lectures :-))",2,0,False,self,,,,,
205,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,15,32ja88,self.MachineLearning,Fuzzy Logic and Machine Learning,https://www.reddit.com/r/MachineLearning/comments/32ja88/fuzzy_logic_and_machine_learning/,zevenozon,1428992171,"I wonder why fuzzy logic is not covered in machine learning courses.  It has a huge advantage over most other machine learning techniques in that rules obtained from 'experts' can easily be incorporated and used with those obtained using supervised learning, etc.  We have used it successfully to solve problems in extractive metallurgy and business.  I would like to hear your opinions on that.",9,0,False,self,,,,,
206,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,16,32jhbu,self.MachineLearning,GDS network to solve temporal CSP Example please.,https://www.reddit.com/r/MachineLearning/comments/32jhbu/gds_network_to_solve_temporal_csp_example_please/,[deleted],1428998041,,0,0,False,default,,,,,
207,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,17,32jjgc,self.MachineLearning,What to do next.,https://www.reddit.com/r/MachineLearning/comments/32jjgc/what_to_do_next/,[deleted],1429000128,,1,0,False,default,,,,,
208,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,18,32jmyj,self.MachineLearning,Machine learning and robotics,https://www.reddit.com/r/MachineLearning/comments/32jmyj/machine_learning_and_robotics/,YYun,1429003407,,1,0,False,default,,,,,
209,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,18,32joew,self.MachineLearning,topic analysis: how to extract documents to analyse from corpus,https://www.reddit.com/r/MachineLearning/comments/32joew/topic_analysis_how_to_extract_documents_to/,choopchooptroop,1429004785,"Say I have a large corpus of a given size, yet I know that I only want to analyse certain documents. For example having the whole archive of a newspaper, but only wanting to analyse articles about a certain topic. What would be the best way to extract those articles? 

DiMaggio et. al. [2013] seems to have used a keyword search and then proceeded to check the results of the search manually. Why not use a topic model to identify the articles?",2,0,False,self,,,,,
210,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,18,32jozr,self.MachineLearning,[MODS][Serious] Please do something about this plague of Coursera idiots,https://www.reddit.com/r/MachineLearning/comments/32jozr/modsserious_please_do_something_about_this_plague/,[deleted],1429005260,"Post the AMA thread earlier, use subreddit CSS to hide the 'Submit text post' button, something...",0,1,False,default,,,,,
211,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,19,32jr8b,medium.com,Political Analysis: Building a classification model to predict the speaker.,https://www.reddit.com/r/MachineLearning/comments/32jr8b/political_analysis_building_a_classification/,Scallyb,1429007164,,1,4,False,http://b.thumbs.redditmedia.com/4UGrsASsdK4zIcz5nGO0oar8RvkVSTamyfeIJRAK-nM.jpg,,,,,
212,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,20,32jvoq,sawetwipes.com,wet wipes machine turkey,https://www.reddit.com/r/MachineLearning/comments/32jvoq/wet_wipes_machine_turkey/,Eazyshoping4u,1429010850,,1,1,False,default,,,,,
213,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,21,32k00r,self.MachineLearning,Why is PCA an unsupervised learning algorithm?,https://www.reddit.com/r/MachineLearning/comments/32k00r/why_is_pca_an_unsupervised_learning_algorithm/,baleezo,1429013985,"Dear Prof. Ng,
The PCA is said to be a learning algorithm. Is that because PCA learned how to project a high dimension example to a low dimension vector space to get a new low dimension example(By Using Ureduce)?
BR,
Baleezo",2,0,False,self,,,,,
214,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,21,32k0au,self.MachineLearning,Do recommender systems like we have learned in the classroom suffer from self-fulfilling prophecy ???,https://www.reddit.com/r/MachineLearning/comments/32k0au/do_recommender_systems_like_we_have_learned_in/,RuudW,1429014170,,5,0,False,self,,,,,
215,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,22,32k76m,self.MachineLearning,A few questions for Dr. Ng,https://www.reddit.com/r/MachineLearning/comments/32k76m/a_few_questions_for_dr_ng/,ssaran2014,1429018297,"The ML course on Coursera was excellent! Thanks you!

1. I'm trying to use the methods that you went through such as tuning alpha, lambda for regressions. But I'm trying to do it in R. Do you have any recommended R packages to look into. 

2. You had mentioned at the start of the course that you've taught the same or similar course in R. Is that available to us? Mainly, I'm trying to use the techniques taught and implement them in R, if possible.

3. Post course, what are your thoughts on how best to improve our skill sets? For example, competitions on Kaggle, or some other way?

4. Any thoughts on how to showcase our skill sets to future employers?

5. What is the cutting edge in ML today? Or, in other words, what are the kind of problems that you are working on solving?

Thanks
",2,0,False,self,,,,,
216,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,22,32k8f1,self.MachineLearning,Question,https://www.reddit.com/r/MachineLearning/comments/32k8f1/question/,Eddie75,1429018981,,2,0,False,default,,,,,
217,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,22,32k9dd,self.MachineLearning,Failures of Information Geometry (?),https://www.reddit.com/r/MachineLearning/comments/32k9dd/failures_of_information_geometry/,InfinityCoffee,1429019465,"I was recently pointed in the direction of the following somewhat polemic article on [the failures of information geometry](http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.4905961).
It seems to argue that from a maximum entropy perspective, information geometry is fundamentally flawed. Personally, the article appears to be a bit vague on multiple points, especially considering the aggressive nature, but my own experience with information geometry is rather elementary so I would love to hear some of your thoughts.

Some thoughts: is relative entropy (KL divergence) and the independence-invariant property really the alpha-omega of inference cost functions as proposed? Does this argument hinge on the application of MaxEnt? Are the examples actually correct on all counts (he skips an awful lot of calculations)? ",6,9,False,self,,,,,
218,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,23,32keqn,arxiv.org,[1504.02824] A Deep Embedding Model for Co-occurrence Learning,https://www.reddit.com/r/MachineLearning/comments/32keqn/150402824_a_deep_embedding_model_for_cooccurrence/,improbabble,1429022057,,1,5,False,default,,,,,
219,MachineLearning,t5_2r3gv,2015-4-14,2015,4,14,23,32kgzz,self.MachineLearning,"Hello prof. Ng,",https://www.reddit.com/r/MachineLearning/comments/32kgzz/hello_prof_ng/,chrjxj,1429023132,,0,0,False,default,,,,,
220,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,0,32ki8v,self.MachineLearning,6-12 Month Timeline to Advance Towards Deep Learning,https://www.reddit.com/r/MachineLearning/comments/32ki8v/612_month_timeline_to_advance_towards_deep/,Power_Berries,1429023704,,0,0,False,default,,,,,
221,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,0,32kis7,jsonpedia.org,"Consume Wikipedia pages complete of templates, tables and lists, as JSON, in seconds.",https://www.reddit.com/r/MachineLearning/comments/32kis7/consume_wikipedia_pages_complete_of_templates/,hardest79,1429023942,,0,2,False,default,,,,,
222,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,0,32kkyz,self.MachineLearning,Outdoor recognition,https://www.reddit.com/r/MachineLearning/comments/32kkyz/outdoor_recognition/,[deleted],1429024941,Really enjoying the machine leaning coursera course so far. Any ideas on approaches for image recognition in outdoor settings with varying light levels? Flora and tree recognition?,0,0,False,default,,,,,
223,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,0,32kln1,self.MachineLearning,Need help with a text filtering algorithm,https://www.reddit.com/r/MachineLearning/comments/32kln1/need_help_with_a_text_filtering_algorithm/,[deleted],1429025240,Any chance anybody has ideas how to automatically filter reddit posts into the correct AMA?,0,1,False,default,,,,,
224,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,0,32kn5k,self.MachineLearning,"Difference between learning types (Supervised.Classification, Unsupervised.Associative)",https://www.reddit.com/r/MachineLearning/comments/32kn5k/difference_between_learning_types/,[deleted],1429025923,Both the techniques require learning that has data coming from previous experience(hence forming Association) and creates any room to identify the new requirement in these cases. Normally Unsupervised Learning doesnt need labeled data but when Association is created it automatically is labeled(hence confusion).,0,1,False,default,,,,,
225,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,1,32ksco,self.MachineLearning,NEW USERS WHO DIDN'T READ THE AMA INSTRUCTIONS,https://www.reddit.com/r/MachineLearning/comments/32ksco/new_users_who_didnt_read_the_ama_instructions/,olaf_nij,1429028137,"Questions go in this thread: http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/

Also, anyone know where this influx of new users is coming from for the AMA?
",4,8,False,self,,,,,
226,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,1,32ktkh,self.MachineLearning,Thanks for your ML course,https://www.reddit.com/r/MachineLearning/comments/32ktkh/thanks_for_your_ml_course/,[deleted],1429028643,"Dear Andrew, Your course is truly amazing. I learnt a lot over the past 2 months. Someone pointed me to the Kaggle site and I am seeing a lot of good works being done in ML. Some of the techniques that was mentioned were not covered in your class, like Decision Tree , Random Forest. Do you have any plans on creating a Part II of the ML class? BTW, the Kaggle competition is great. I would suggest anyone interested in seeing how ML can be applied in real life situation to take a look at it. ",0,1,False,default,,,,,
227,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,1,32kv4r,self.MachineLearning,Next step for ML,https://www.reddit.com/r/MachineLearning/comments/32kv4r/next_step_for_ml/,chen64042,1429029318,,0,1,False,default,,,,,
228,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,1,32kx19,self.MachineLearning,A few questions from student,https://www.reddit.com/r/MachineLearning/comments/32kx19/a_few_questions_from_student/,shtankova,1429030154,"1) What are the biggest mistakes that beginners make? What are the biggest wastes of time in learning process? 
2) What do best ML and DM developers have in common? What skills and qualities do they possess? 
3) Which of these skills and qualitites are not explained in courses and books? 
4) What does the skill progression look like? What order is the best to develop profound knowledge of the field? 

Thank you very much for your answers and for your course! ",2,0,False,self,,,,,
229,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,2,32l1ip,self.MachineLearning,why we use normalization ?,https://www.reddit.com/r/MachineLearning/comments/32l1ip/why_we_use_normalization/,tjredwolf,1429032034,"Thanks for your course on ML, you made it really simple to understand. 
My query is normalizing data is prevalent in ML algorithms, so I want to understand how normalizing data helps ML algorithms.
Also i was trying to understand Maximum Entropy algorithm by reading papers over web, and it was not that helpful for an armature, can you provide us some reading stuff on other classification algorithms.
Thanks",1,0,False,self,,,,,
230,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,3,32lb1k,self.MachineLearning,working for Baidu w/ Andrew Ng,https://www.reddit.com/r/MachineLearning/comments/32lb1k/working_for_baidu_w_andrew_ng/,drm509,1429036021,"Hi Andrew,
What would I have to do tobe hired and worked for you at Baidu? I'm a software engineer (grad of 2010) who knows some ML from various school courses and from your ML course on coursera. Do I have any chance of getting hired or do I need a masters degree. (I saw some of the ML job posts from Baidu on my school's job site)
Thanks!",3,0,False,self,,,,,
231,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,5,32lrfz,self.MachineLearning,Deep convolution neural network,https://www.reddit.com/r/MachineLearning/comments/32lrfz/deep_convolution_neural_network/,salehalis,1429043125,"Hello professor, 
1- I want to implement CNN for object recognition. It is possible to give me high accuracy for small dataset[ e.g ORL face database , 200 images for training] or I need large dataset?
2- I implemented scholastic  Gradient decent in CNN. the error sometimes fluctuates . Is this means there is an error in my application?
",0,0,False,self,,,,,
232,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,6,32m3gc,devblogs.nvidia.com,Get Ready for the Low-Power Image Recognition Challenge with Jetson TK1,https://www.reddit.com/r/MachineLearning/comments/32m3gc/get_ready_for_the_lowpower_image_recognition/,harrism,1429048429,,2,3,False,http://b.thumbs.redditmedia.com/gNTiel8f6q_5Z98dqkZASJiHpjY_hEviSITVirGodxg.jpg,,,,,
233,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,9,32mjra,self.MachineLearning,Rudimental,https://www.reddit.com/r/MachineLearning/comments/32mjra/rudimental/,Manbuag,1429056253,,2,0,False,default,,,,,
234,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,10,32mv8d,waitbutwhy.com,The AI Revolution: Road to Superintelligence - Wait But Why,https://www.reddit.com/r/MachineLearning/comments/32mv8d/the_ai_revolution_road_to_superintelligence_wait/,mattrobust,1429061866,,2,0,False,http://b.thumbs.redditmedia.com/_Lm7ej952UG0U1OFEZVFakbdN2ievXQttWBRukm8SwU.jpg,,,,,
235,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,10,32mx2i,machinelearningmastery.com,Get Paid To Apply Machine Learning: The Ladder Approach To Becoming a Machine Learning Consultant - Machine Learning Mastery,https://www.reddit.com/r/MachineLearning/comments/32mx2i/get_paid_to_apply_machine_learning_the_ladder/,mattrobust,1429062770,,11,15,False,http://b.thumbs.redditmedia.com/wSxfMxE0z0o3LeReqhBBHFRwC08KTEbEV0lTYkYc_ys.jpg,,,,,
236,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,12,32nbf3,self.MachineLearning,Can any ML task be called the equivalent of finding a distance?,https://www.reddit.com/r/MachineLearning/comments/32nbf3/can_any_ml_task_be_called_the_equivalent_of/,rorschach122,1429070184,"Does any ML algo, like SVM or Random Forest or Deep learning, have the interpretation of a distance being computed, between test and train or anything? 

",10,2,False,self,,,,,
237,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,13,32ncnc,self.MachineLearning,"[Question] Tracking the ""size"" of the update during training",https://www.reddit.com/r/MachineLearning/comments/32ncnc/question_tracking_the_size_of_the_update_during/,BeijingChina,1429070856,"Hi,
rather than simply looking at the train/test error curves, could some information be taken from looking at the norm of the last update? This coul let us know about the petering out/acceleration of derivatives, and (I am guessing) inform the changing of the learning rate?

I have seen a ""cache"" being used for RMS-prop and AdaGRAD,  is the purpose of these similar to what I describe?",2,2,False,self,,,,,
238,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,15,32npiu,theregister.co.uk,MIT shows off machine-learning script to make CREEPY HEADS,https://www.reddit.com/r/MachineLearning/comments/32npiu/mit_shows_off_machinelearning_script_to_make/,jamarpaluck,1429079622,,0,1,False,http://b.thumbs.redditmedia.com/wUlSpz78iSmnMwZOECsIDb3o7_m0S1lYXmOrR5OzXfE.jpg,,,,,
239,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,15,32npnk,siliconangle.com,Amazon Machine Learning marks a sea change in the way organizations handle data,https://www.reddit.com/r/MachineLearning/comments/32npnk/amazon_machine_learning_marks_a_sea_change_in_the/,jonsabata,1429079739,,0,0,False,http://b.thumbs.redditmedia.com/LvYyYZGSbodyBT8xVpwZhtl2AwQPakmY_U3_YzQExSo.jpg,,,,,
240,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,15,32nqgp,self.MachineLearning,Good WEKA classifier for text classification?,https://www.reddit.com/r/MachineLearning/comments/32nqgp/good_weka_classifier_for_text_classification/,quantumlizard,1429080368,"Hi all,

I'm looking into multiclass classification of blocks of text. For features I am using both the words of the text (unigrams and bigrams) and metadata (word count etc).

I am using WEKA and have experimented with J48 and NaivaBayesMultinomial. The decision tree takes a really long time when the number of features is high (since the algorithm is exponential in the number of features), while I read somewhere that Naive Bayes might be a good starting point but it doesn't do too well when classifying text, despite its quick running time.

So the question would be, if you guys have had experience classifying text with WEKA, what would be a good classifier that doesn't take a million years to build? I am training on a dataset of about 300k instances and 200k features.",6,0,False,self,,,,,
241,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,16,32nshb,self.MachineLearning,Looking for a project,https://www.reddit.com/r/MachineLearning/comments/32nshb/looking_for_a_project/,no_porner,1429081965,"I am looking for a completed or partially completed project for object detection in a video stream.
Please help.",1,0,False,self,,,,,
242,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,18,32o2yz,nerds.airbnb.com,How Airbnb uses machine learning to detect host preferences,https://www.reddit.com/r/MachineLearning/comments/32o2yz/how_airbnb_uses_machine_learning_to_detect_host/,priteshs,1429091742,,10,54,False,http://b.thumbs.redditmedia.com/s7-0NYg8tSazkvYY9JKTk7_VSTFjE23vab8HCsDSOnk.jpg,,,,,
243,MachineLearning,t5_2r3gv,2015-4-15,2015,4,15,22,32olql,nuit-blanche.blogspot.com,"Slides and Streaming info: Paris Machine Learning Meetup #8, Season 2: Deep Learning and more...(First two talks in English, Yoshua Bengio, Sander Dieleman/Ira Korshunova)",https://www.reddit.com/r/MachineLearning/comments/32olql/slides_and_streaming_info_paris_machine_learning/,compsens,1429105201,,0,7,False,http://b.thumbs.redditmedia.com/OSM-1jXkHsQDS_KBNGVKv-gbcEc09UzmkgvGJW25zsw.jpg,,,,,
244,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,1,32p4nm,blogs.technet.com,A conceptual map to help you successfully navigate the cloud data science process,https://www.reddit.com/r/MachineLearning/comments/32p4nm/a_conceptual_map_to_help_you_successfully/,MLBlogTeam,1429113975,,0,1,False,default,,,,,
245,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,3,32ppdc,self.MachineLearning,Distributed sparse matrix multiplication Library in java?,https://www.reddit.com/r/MachineLearning/comments/32ppdc/distributed_sparse_matrix_multiplication_library/,AlexTHawk,1429123128,"Could somebody please refer me to a java library that does distributed matrix multiplication of sparse matrices?  My team is building a web-scale co-occurrence matrix, and we have strong preference for doing everything in Java.",4,5,False,self,,,,,
246,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,3,32pr3s,self.MachineLearning,"How to Automating 3D Connectome Brain Mapping Using ML, ANN, AI, etc Techniques.",https://www.reddit.com/r/MachineLearning/comments/32pr3s/how_to_automating_3d_connectome_brain_mapping/,[deleted],1429123942,"Are you familiar with the crowd-sourcing attempts to create a 3D connectome model of the human brain [1-4]? EyeWire [2-3] is an interactive visualization software that allows a use to looks 2D cross-section of a brain to allow the crowd-sourced users to reconstruct the connection of each neuron. Currently, the connectomic reconstruction of the inner plexiform layer in the mouse retina has been performed using this method [5]. But this reconstruction only had the volume of a 'grain of sand' and it was not in a mouse animal model.

Questions:
*1)* How could the jobs of the +150,000 ""gamers"" using to EyeWire software be replaced by machine-learning and artificial neural networks methods? Using crowd-sourcing was clever, but I don't think it will service for the entire human brain - at least if we want to finish it anytime soon. 
2) What challenges would there be? (I know there will be a lot).
3) What are some similar project that have been done using categorical data (2D pictures) to make a 3D model?
4) Understanding that mapping the entire human brain is an unrealistic project, what would be an appropriate scope to prove that the concept could work? Using the results of the plexiform layer in the mouse retina as a gold standard to compare the A.I.'s results?
5) What other subreddits are appropriate for this post?

Citations:
[1] http://blogs.scientificamerican.com/sa-visual/2014/04/02/how-do-you-visualize-the-brain/
[2] (In this article you may need to skip toward the bottom; there a lot about Amy Robinson in the beginning) http://people.boston.com/amy-robinson/
[3] http://www.scientificamerican.com/citizen-science/eyewire-mit/
[4] 'The Neuroscience Revolution Will Be Crowdsourced' http://blogs.scientificamerican.com/mind-guest-blog/2013/09/11/the-neuroscience-revolution-will-be-crowdsourced/
[5] http://www.nature.com/nature/journal/v500/n7461/full/nature12346.html

EDIT: grammar, spelling, and flow. ",0,1,False,default,,,,,
247,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,4,32pyog,self.MachineLearning,"How to fight unbalanced classes in convolutional neural net, specifically in caffe realization?",https://www.reddit.com/r/MachineLearning/comments/32pyog/how_to_fight_unbalanced_classes_in_convolutional/,Fortyq,1429127317,How make caffe sample balanced batches? Or how do you handle this problem.,2,6,False,self,,,,,
248,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,4,32pzd3,self.MachineLearning,Can machine-learning methods be used to semi/fully automate the 3D connectomic mapping of the brain currently being used by crowd-sourced software like EyeWire?,https://www.reddit.com/r/MachineLearning/comments/32pzd3/can_machinelearning_methods_be_used_to_semifully/,bpine20,1429127622,"BACKGROUND: There has been a crowd-sourcing attempt to re-create a 3D connectomic model of the human brain [1-4]? EyeWire [2-3], an interactive visualization software, allows crowd-sourced users to reconstruct the connection of each neuron by looking at categorical data in the form of 2D cross-section of brain histology (x-y plane) going in the 'z' direction. (The videos in the links below will help if I am not explain well enough)   Currently, the successful connectomic reconstruction of the inner plexiform layer of a mouse retina has been performed using this method [5]. However, only the volume of a ""grain of sand"" was modeled. 

PROPOSED PROJECT GOAL: How could the jobs of the +150,000 crowd-sourced ""gamers"" using the EyeWire software be replaced using machine-learning methods? Using crowd-sourcing was clever, but I don't think it will service for the timely mapping of the entire human brain.

Questions: 
1) What computational methods should I use?  
2) What challenges would there be? (I know there will be a lot). 
3) What are similar projects that have been done using categorical data (2D pictures) to make a 3D model? 
4) Understanding that mapping the entire human brain is an unrealistic project, what would be an appropriate scope to prove that the concept could work? Using the results of the plexiform layer in the mouse retina as a gold standard to compare the A.I.'s results? 
5) What other subreddits are appropriate for this post?

Citations: 
[1] http://blogs.scientificamerican.com/sa-visual/2014/04/02/how-do-you-visualize-the-brain/ 
[2] (In this article you may need to skip toward the bottom; there a lot about Amy Robinson in the beginning) http://people.boston.com/amy-robinson/ 
[3] http://www.scientificamerican.com/citizen-science/eyewire-mit/ 
[4] 'The Neuroscience Revolution Will Be Crowdsourced' http://blogs.scientificamerican.com/mind-guest-blog/2013/09/11/the-neuroscience-revolution-will-be-crowdsourced/ 
[5] http://www.nature.com/nature/journal/v500/n7461/full/nature12346.html

EDIT: spelling, grammar, sentence flow",3,0,False,self,,,,,
249,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,5,32q0me,self.MachineLearning,Seeking advice on how a solution similar to this might be implemented? [video and description inside],https://www.reddit.com/r/MachineLearning/comments/32q0me/seeking_advice_on_how_a_solution_similar_to_this/,tyrael71,1429128183,"https://www.youtube.com/watch?v=dqbhPcef7Do
I'm interested in the first 2 use cases. To give you a bit of background, I'm a computer science student interested in computer vision. I can code in Java/C++/Python and decided to ask from a solution-first perspective. I'm looking to build a prototype that can achieve a similar functionality, but am having trouble understanding where to start.

Would a deep-learning approach be optimal here? Where would you suggest I start?

Edit: To make the question a bit more clear. I do not want to use their sdk, but learn to do it on my own. I'm interested in learning about the object in specific details, shape, text, brand, color, etc. And while a camera is filming it (and automatically recognizing it), I can then color the context around it and make it pressable. Any materials or advice would be appreciated.
",8,3,False,self,,,,,
250,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,5,32q74a,self.MachineLearning,Does anyone know if Deep Belief Networks have been used successfully with Reinforcement learning?,https://www.reddit.com/r/MachineLearning/comments/32q74a/does_anyone_know_if_deep_belief_networks_have/,deepbasu007,1429131032,,5,5,False,self,,,,,
251,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,6,32q9ud,self.MachineLearning,Real applications of probabilistic programming,https://www.reddit.com/r/MachineLearning/comments/32q9ud/real_applications_of_probabilistic_programming/,PoddyOne,1429132219,"So I've been reading a bit about probabilistic programming. Seems like a cool idea, and the languages look like a lot of fun to play with.

However, I'm having a hard time understanding what the real applications are? Sure you can quickly build something like a probabilistic graphical model, but this would also be possible with something like PYMC (and you probably already know Python).

So can someone explain to me why I ought to be excited beyond just 'this looks like a bit of fun'?",4,5,False,self,,,,,
252,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,10,32r67r,robusttechhouse.com,Microsoft Azure Machine Learning Review [We think it is good!],https://www.reddit.com/r/MachineLearning/comments/32r67r/microsoft_azure_machine_learning_review_we_think/,mattrobust,1429147765,,2,1,False,http://b.thumbs.redditmedia.com/LpBo-QcfSZqot-PPl_ubbzFykjBSr0-8M9tCI2jcamc.jpg,,,,,
253,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,12,32rjwb,self.MachineLearning,Experience publishing live data for public consumption?,https://www.reddit.com/r/MachineLearning/comments/32rjwb/experience_publishing_live_data_for_public/,joelathome,1429154640,"I'm in the planning stages of a large data collection experiment, in which I am going to be outfitting my home with power monitoring equipment and recording all of the power usage data to a database. I'm very interested in making the data public, but I don't know if there is a preferred way to store the data so that it is easy for other researchers to access. My instinct tells me to just write everything to a PostgreSQL database, since that's what I have experience with, and do a nightly database dump and make that available. Is there any preferred storage type that would make it easier for other researchers to use my data?

Additionally, I'm planning on only storing an outlet ID, the current power draw, and a timestamp. I should also be able to include a description of what is plugged in (iPad, computer, TV, blender), though I'm not sure how to go about doing that yet. Is there anything else that would be beneficial to include?

I'm planning to use the data myself to test a hypothesis about power usage fingerprinting, and I figured the data could be used by others as well. ",0,0,False,self,,,,,
254,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,13,32roqb,self.MachineLearning,Are there any rules of thumb for selecting the number of hidden layers and neurons per layer in a deep net?,https://www.reddit.com/r/MachineLearning/comments/32roqb/are_there_any_rules_of_thumb_for_selecting_the/,rudyl313,1429157335,,23,23,False,self,,,,,
255,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,13,32rpbv,self.MachineLearning,Made a site where you can post cool machine learning papers,https://www.reddit.com/r/MachineLearning/comments/32rpbv/made_a_site_where_you_can_post_cool_machine/,wenqinYe,1429157713,"As you can tell from the title, I created a website called www.trynibble.com to allow people to post cool machine learning links that they have found. I noticed that people liked to aggregate machine learning papers on github repos, and I thought that it would be much better if there was just a site where you could post all the links to.
",5,0,False,self,,,,,
256,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,18,32sdop,self.MachineLearning,Format for training HMM with MFCCs,https://www.reddit.com/r/MachineLearning/comments/32sdop/format_for_training_hmm_with_mfccs/,obsoletelearner,1429176791,"I'm using the [HMM Toolbox][1] for matlab, I'm trying to implement an HMM to recognise syllables, I have extracted the MFCC feature vectors for the syllable each is of format 13*X where X is the number of frames in the training vector, I'm trying to train the HMM with these features. My training vector is a sequence of MFCCs for each word which features which i'm using to train


    for i=1:length(mfcc),  
        [LL, prior1, transmat1, mu1, Sigma1, mixmat1] = 
        mhmm_em(mfcc{i}, prior1, transmat1, mu1, Sigma1, mixmat1, 'max_iter', 3);
    end

When i use the above code my loglikelihood goes to -Inf after 2 iterations, 
I have tried everything using spherical/diagonal covariance, but it just doesn't work. 

This is the code for intialization

   
    Q = 3;             % Number of states (left to right)
    O = 13;             %output symbols
    mix = 2;

    cov_type = 'full'; %the covariance type that is chosen as ullfor gaussians.
    prior1 = normalise(rand(Q,1));
    transmat1 = mk_stochastic(rand(Q,Q));
    temp = cell2mat(mfcc);
    [mu1, Sigma1] = mixgauss_init(Q*mix, temp, cov_type, 'kmeans'); %K-Means initialization
    mu1 = reshape(mu1, [O Q mix]);
    Sigma1 = reshape(Sigma1, [O O Q mix]);
    mixmat1 = mk_stochastic(rand(Q,mix));

How do fix this? Any help would be great i've been stuck here for days.

Heres the output for one syllable

    A
    iteration 1, loglik = -452.902377
    iteration 2, loglik = -95.861965
    iteration 3, loglik = -53.762907
    iteration 1, loglik = -Inf
    iteration 2, loglik = -Inf
    iteration 3, loglik = -Inf 

  [1]: http://www.cs.ubc.ca/~murphyk/Software/HMM/hmm_usage.html
",3,1,False,self,,,,,
257,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,19,32shg6,self.MachineLearning,Question about Fractional Max-Pooling,https://www.reddit.com/r/MachineLearning/comments/32shg6/question_about_fractional_maxpooling/,RapidLeaper,1429180323,"I asked a question here a few days ago (thanks for all the responses, btw!) and got pointed to this paper by Ben Graham: http://arxiv.org/pdf/1412.6071.pdf

I've read the paper and pretty much get what it's on about, but there is one bit I'm not so clear on. It doesn't seem explained in the paper, not that I've been able to find at least.

It don't understand how the limit on the N^out is determined. 

From what I understand, as stated in the paper, a standard 2x2, non-overlapping pooling region, reduces the input by a factor of two. N^in / N^out = 2. 

He wishes to slow the 'rate of decay.' He wants to slow it to ~sqrt(2). He formally defines the method to generate the pooling regions. All that I get (I think).

Now, I'm a bit lost with the examples of the patterns for the case N^in = 25 and N^out = 18. Copied from the paper they are:

    -Randomly Gen'd-    | -Pseudorandomly Gen'd-

211112112211112122   |   112112121121211212

111222121121112121   |   212112121121121211

121122112111211212   |   211211212112121121


I understand that the sum of each row is supposed to equal N^in (25), but only use N^out numbers(18). I just don't understand how this is ensured.

**TL;DR**: How does he ensure the total for each row is 25 with 18 numbers. ",6,3,False,self,,,,,
258,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,20,32sl1x,onlineuniversityoffinland.com,Why social presence in online learning is important,https://www.reddit.com/r/MachineLearning/comments/32sl1x/why_social_presence_in_online_learning_is/,afrank05,1429183301,,0,0,False,default,,,,,
259,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,21,32so4j,ijcai15.org,Repeat Buyers Prediction Competition [IJCAI 15],https://www.reddit.com/r/MachineLearning/comments/32so4j/repeat_buyers_prediction_competition_ijcai_15/,galapag0,1429185627,,5,4,False,http://b.thumbs.redditmedia.com/IFXywWaSQ_xbENWYiduE7BPYgiikvEO6mEwgFbc_jSw.jpg,,,,,
260,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,22,32suqg,self.MachineLearning,Can you recommend to me a good book about Deep Learning,https://www.reddit.com/r/MachineLearning/comments/32suqg/can_you_recommend_to_me_a_good_book_about_deep/,[deleted],1429189771,I am familiar with Machine Learning and Neural Networks concepts. I am searching for some good books about Deep Learning.,0,0,False,default,,,,,
261,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,22,32swjt,self.MachineLearning,Any advice on good books about Deep Learning?,https://www.reddit.com/r/MachineLearning/comments/32swjt/any_advice_on_good_books_about_deep_learning/,Ewybe,1429190754,"I am familiar with Machine Learning and Neural Networks concepts. I am searching for some good books about Deep Learning.
",5,3,False,self,,,,,
262,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,22,32szue,self.MachineLearning,[Question] How to handle large (10GB) datasets?,https://www.reddit.com/r/MachineLearning/comments/32szue/question_how_to_handle_large_10gb_datasets/,midnight1247,1429192489,"I'm currently self-learning machine learning and statistics. I'm on a project to analyze League Of Legends game data, using Python. Main objective is to learn ML and data analysis libs for Python.

I've stored every match in a separate text file, containing the match info from the Riot API. My dataset contains 350.000, with 11GB size total. Match info is a dict with more dicts and lists inside.

Match structure reference: https://developer.riotgames.com/api/methods#!/967/3313

Ok, so now, how should I handle that? I can't simply load all that info on memory (8GB) and I've came with different possible solutions.

One friend told me to store the data in SQLite, but I don't know exactly how I can link the database with scikit.learn later. I could try some form of lazy loading, accessing files on demand and only storing in memory a fraction of the entire dataset (This sounds extremely slow). I've think too of converting JSON to many CSVs, each one containing only the features I want to use (For example, I think I won't need post-game info for some pre-game analytics).",7,5,False,self,,,,,
263,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,23,32t2ju,inference.vc,A new Favourite Machine Learning Paper: Autoencoders as Probabilistic Models,https://www.reddit.com/r/MachineLearning/comments/32t2ju/a_new_favourite_machine_learning_paper/,fhuszar,1429193904,,12,77,False,http://b.thumbs.redditmedia.com/rODnsesbdadhOMuA1w-mpSCJOeTzLstboLLkVHkfFfs.jpg,,,,,
264,MachineLearning,t5_2r3gv,2015-4-16,2015,4,16,23,32t61f,self.MachineLearning,R script as a data collection tool - how does this work?,https://www.reddit.com/r/MachineLearning/comments/32t61f/r_script_as_a_data_collection_tool_how_does_this/,01062426050,1429195629,"Hello!

I'm not very sure whether this is a corresponding sub, but so far this looks the most relevant! 

I recently heard about R script (R-script) as a data collection tool. I want to know more about this and wondering if you guys can help me out here. here's a little background.

 I've been working with a voting records database of a decision making body. By looking at the voting records, I'm hoping to get a better understanding of political dynamics within that body. 

As a coding/programming illiterate, i could only come up a manual way to set up a database, where i type each voting sheets into an excel worksheet. 

This was a hideous job that took me days. One day, I came across this professor's dataverse where he set up a same time of database,  but 20 times larger. I got in touch with him and asked how he managed to collect that much data. The answer was simple: R-script.

As i said, I don't have much knowledge on statistic, coding or data collection. I spent last couple of weeks figuring out what this R is and has not much clue what i've been reading or practicing online. 

Can anyone explain to me how data collection works using R? Preferably, like you'd do to a five years old. 

Many thanks in advance! 

 ",6,0,False,self,,,,,
265,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,0,32tdyo,nbviewer.ipython.org,Beer and Data Science - Topic Modeling in Multi-Aspect Reviews,https://www.reddit.com/r/MachineLearning/comments/32tdyo/beer_and_data_science_topic_modeling_in/,njchessboy,1429199200,,0,4,False,default,,,,,
266,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,1,32tgvw,self.MachineLearning,"Has anyone been able to reproduce the results in ""A Simple Way to Initialize Recurrent Networks of Rectified Linear Units""?",https://www.reddit.com/r/MachineLearning/comments/32tgvw/has_anyone_been_able_to_reproduce_the_results_in/,feedthecreed,1429200475,"I haven't been able to get noticeable performance gains published in Hinton's most recent paper:  http://arxiv.org/abs/1504.00941 for any of my own text prediction tasks.

Has this result been verified by anyone else?",11,2,False,self,,,,,
267,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,1,32ti13,self.MachineLearning,Two Python deep learning Theano-based toolkits you haven't heard of,https://www.reddit.com/r/MachineLearning/comments/32ti13/two_python_deep_learning_theanobased_toolkits_you/,Foxtr0t,1429200964,"This - surprisingly - turned out to be probably the most popular tweet from @fastml_extra so far, so, just in case:

PDNN
http://www.cs.cmu.edu/~ymiao/pdnntk.html

OpenDeep
http://www.opendeep.org/v0.0.6/docs",25,11,False,self,,,,,
268,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,3,32u0j7,self.MachineLearning,Training a mini-Convnet. My learning curve starts on a Plataeu. Need help understanding why,https://www.reddit.com/r/MachineLearning/comments/32u0j7/training_a_miniconvnet_my_learning_curve_starts/,deep_learner,1429208898,"My [learning curve](https://drive.google.com/file/d/0B-vJfhWswxvdc20xa1ozajQyV0E/view?usp=sharing) starts off slowly and then starts to show some action. 

Can I do anything to break this early malaise (more/less momentum/learning rate/ fancy stuff)? 
",8,0,False,self,,,,,
269,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,4,32u8dr,datasciencecentral.com,"400 Professional Resources - Covering data science, R, Python, machine learning, data mining...",https://www.reddit.com/r/MachineLearning/comments/32u8dr/400_professional_resources_covering_data_science/,urinec,1429212253,,0,1,False,default,,,,,
270,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,4,32ucy3,self.MachineLearning,ArkReddit: Is anyone doing serious research using python3?,https://www.reddit.com/r/MachineLearning/comments/32ucy3/arkreddit_is_anyone_doing_serious_research_using/,postit,1429214280,"Python community is urging developers to begin migrate to python3 so they can start to depreciate version 2.  

We currently have twenty-something projects using python2 scipy, scikit-learn and pandas: And we are not planning upgrade anything for now. 

Are you considering this upgrade? 
Are you currently using python3?

",17,4,False,self,,,,,
271,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,6,32ulv1,github.com,Reproducible Experiment Platform for machine learning,https://www.reddit.com/r/MachineLearning/comments/32ulv1/reproducible_experiment_platform_for_machine/,[deleted],1429218243,,0,1,False,default,,,,,
272,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,6,32ups6,github.com,oxnn - Oxford NN Library,https://www.reddit.com/r/MachineLearning/comments/32ups6/oxnn_oxford_nn_library/,egrefen,1429220070,,2,16,False,http://a.thumbs.redditmedia.com/AFwOAv4ujxuDxbIYORV1NNrFAi1iQI1FylwsG8jae84.jpg,,,,,
273,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,6,32us9d,blog.bigml.com,Apple ResearchKit and HealthKit Meets Machine Learning,https://www.reddit.com/r/MachineLearning/comments/32us9d/apple_researchkit_and_healthkit_meets_machine/,atakante,1429221199,,1,5,False,http://a.thumbs.redditmedia.com/ZqMUBla4xDW8tCoBnYhnE4s7zcAwm_nGWpt9OvbyAq4.jpg,,,,,
274,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,11,32vlpm,imgur.com,Funny Chatbot,https://www.reddit.com/r/MachineLearning/comments/32vlpm/funny_chatbot/,wenqinYe,1429236207,,0,0,False,http://b.thumbs.redditmedia.com/wwQQo02r_okir1CtIE4uqYIZ51W3hXOuR7dHOTwfFdw.jpg,,,,,
275,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,11,32vnjo,arxiv.org,A Group Theoretic Perspective on Unsupervised Deep Learning,https://www.reddit.com/r/MachineLearning/comments/32vnjo/a_group_theoretic_perspective_on_unsupervised/,SuperFX,1429237165,,9,12,False,default,,,,,
276,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,15,32w9kd,self.MachineLearning,"DQN for Chess, Go",https://www.reddit.com/r/MachineLearning/comments/32w9kd/dqn_for_chess_go/,[deleted],1429250754,I'm considering trying to train a DQN to play Chess and Go. Is there any reason why you suspect this will be a waste of time? It seems that the game board grid should be well-suited for a convnet. [This paper](http://arxiv.org/pdf/1412.6564v1.pdf) used a CNN to evaluate Go moves in a supervised setting. I wonder if this would work with the sparse error information in the reinforcement learning setting. ,7,0,False,self,,,,,
277,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,16,32wegc,slideshare.net,What is lathe machine?,https://www.reddit.com/r/MachineLearning/comments/32wegc/what_is_lathe_machine/,jkmachinetools,1429254722,,0,1,False,default,,,,,
278,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,17,32wio1,stackoverflow.com,wikidata: from english names to id values for variable entities,https://www.reddit.com/r/MachineLearning/comments/32wio1/wikidata_from_english_names_to_id_values_for/,h1395010,1429258672,,1,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
279,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,17,32wjsl,self.MachineLearning,Gin Rummy Neuroevolution in Python,https://www.reddit.com/r/MachineLearning/comments/32wjsl/gin_rummy_neuroevolution_in_python/,rickthegrower,1429259810,"Hi Guys!

This is a pet project I've been working on for some time that I hope someone here can get something out of. We have here a living, breathing Gin Rummy simulation using neuroevolution! It's a work in progress, but like they say, release early and update often!

http://rickgorman.github.io/neural

At generation 45000, we're starting to see some ""decent"" games played. Decent meaning that the players are discarding cards that generally make their hands better. Not always, but for a first AI project, I'm stoked at the results.

For anyone out there who enjoys python and Gin Rummy,

Cheers!

(p.s. I'd love any feedback or feature requests.)",10,19,False,self,,,,,
280,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,17,32wku7,self.MachineLearning,(Dynamic) Bayesian Networks / Directed acyclical graphs: continuous vs. discrete nodes?,https://www.reddit.com/r/MachineLearning/comments/32wku7/dynamic_bayesian_networks_directed_acyclical/,FadeToBack,1429260769,"The question in short: can anyone point me to resources which discusses or even compares what the advantages/disadvantages are of either using continuous data directly or discretizing it before use?

I'm currently involved in a project which will use bayesian networks (both dynamic and not) to try to predict human behavior, i.e. of cyclists. The input data is from different kinds of sensors, all continuous. Parameter learning and inference will be used, no structure learning to begin with (network structure is designed by experts).
While researching, I found many sources which explain how to use discrete and continuous data, but I've not come across any advice under which circumstances each approach might be better.
So far I've checked out the books by K. Murphy (2012), Neapolitan (2003) and Russel/Norvig (2010) and an overview paper by Daly/Shen/Aitken (""Learning Bayesian networks: approaches and issues"", 2011).

Any pointers?",4,4,False,self,,,,,
281,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,18,32wmes,self.MachineLearning,Benefit of Unsupervised learning related to Supervised learning,https://www.reddit.com/r/MachineLearning/comments/32wmes/benefit_of_unsupervised_learning_related_to/,ivop,1429262243,"Want to start the following discussion and hear your opinion:

Edit for keeping the true discussion: Generative models model the full joint distribution P(X,Y) where X is the input and Y is the label data, discriminative models - only the conditional distribution P(Y|X), and are hence a subset of the generative problem: P(X,Y) = P(Y|X)P(X). A bigger problem will require a bigger model which in turn will require more data to train. If we face a SL problem with labeled data x_SL, it is beneficial to apply an additional generative loss to a possibly bigger dataset x_UL, only if x_UL/x_SL is bigger than the ratio of model complexity between log P(Y|X) and log P(X). Intuitively, if we have a large image for X with 100K pixels and only 100 labels we will need several magnitudes of more unlabeled than labeled data to justify the use of a generative model. 


Original statement for reference: Unsupervised learning (UL) models the full joint distribution P(X,Y) where X is the input and Y is the label data. Supervised learning (SL) models only the conditional distribution P(Y|X) and is hence a subset of the UL problem: P(X,Y) = P(Y|X)P(X). A bigger problem will require a bigger model which in turn will require more data to train. If we face a SL problem with data x_SL, it is beneficial to apply UL to a possibly bigger dataset x_UL, only if x_UL/x_SL is bigger than the ratio of model complexity between log P(Y|X) and log P(X). Intuitively, if we have a large image for X with 100K pixels and only 100 labels we will need several magnitudes of more UL data than SL data to make any UL learning beneficial. ",16,7,False,self,,,,,
282,MachineLearning,t5_2r3gv,2015-4-17,2015,4,17,21,32wxi1,self.MachineLearning,How are the feature maps in CNNs learned?,https://www.reddit.com/r/MachineLearning/comments/32wxi1/how_are_the_feature_maps_in_cnns_learned/,ddofer,1429272148,"N00b question I know, but I just can't ""get"" it. 
In the initial stage of a convolutional neural network, as part of the feature extraction, instead of using ""predefined"" convolutions (such as applying:
 [1,0      or   [ 0 , 0 , 0              ,   or others (""blur"", ""sharpen"" etc')
 0,1]              0 , 1, 0
                     0 , 0 ,0 ] 

, instead the CNN learns the filters (for a given input and feature-map size, e.g. 2x2 or 5x5..) .  (Then subsequent layers have pooling, different feature maps, etc').

What I don't get at all is how the feature maps are created/generated and learned. I've looked over a few tutorials, but the understandable ones focus on what a convolution is, while the denser ones are a bit over my head (e.g. Theano and Tensors). 

Thanks! ",11,1,False,self,,,,,
283,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,0,32xmit,mapr.com,Machine Learning at American Express: Challenges and Outcomes,https://www.reddit.com/r/MachineLearning/comments/32xmit/machine_learning_at_american_express_challenges/,KeponeFactory,1429285432,,1,6,False,http://a.thumbs.redditmedia.com/YG_yZfyp9l9mZUPUa4LDBgR-pHmL8wLbtPwN8Cbbaw8.jpg,,,,,
284,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,1,32xrrz,memkite.com,Deeplearning.University  Bibliographies from Lisa Labs (Yoshua Bengios lab),https://www.reddit.com/r/MachineLearning/comments/32xrrz/deeplearninguniversity_bibliographies_from_lisa/,atveit,1429287790,,0,0,False,http://b.thumbs.redditmedia.com/dUvjPHn2TyBNeL_TIC6uu1j_-Qs9WbB58VE_29FqwRE.jpg,,,,,
285,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,2,32xy73,partiallyderivative.com,Partially Derivative Ep. 20: Don't Let Baseball Ruin Statistics!,https://www.reddit.com/r/MachineLearning/comments/32xy73/partially_derivative_ep_20_dont_let_baseball_ruin/,chrisalbon,1429290699,,13,16,False,http://a.thumbs.redditmedia.com/rfWYrGkTZ6dXaIUAmqggbSHdRaXEiOoWeJETxTzWZV0.jpg,,,,,
286,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,3,32y617,self.MachineLearning,Assisted writing,https://www.reddit.com/r/MachineLearning/comments/32y617/assisted_writing/,minghuiyu,1429294163,"Hi there,

I am interested in assisted writing backed by machine learning. The purpose is that average people, with the help of assisted writing, can write as good as professional editors in Economist, NY Times, etc.

Any research (or product) in this area?

Thank you!",7,1,False,self,,,,,
287,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,4,32yenz,self.MachineLearning,Few questions,https://www.reddit.com/r/MachineLearning/comments/32yenz/few_questions/,[deleted],1429298040,"Hello!

I am really new to this subject and have some programming background (Java primarily). I have been following a few tutorials such as this [one]( https://www.youtube.com/watch?v=AleGZ9dkfPs&amp;index=2&amp;list=PLQVvvaa0QuDd0flgGphKCej-9jp-QdzZ3) and I find this very interesting. Have been playing with python and sklearn over the last few days. I will be doing that over the next few weeks but ultimately I want to do a small project so that I stay motivated and pursue this. One way to do this is to come up with a fun project (if i have a goal i will work until i solve it). I am thinking of putting a camera out the window and figuring out a way to count cars passing on the street in both directions. My questions are: What tools should I be getting familiar with? What algorithms should I get familiar with? How could I approach this problem? 

This is some info I have found so far that I am reviewing:

https://www.mathworks.com/help/vision/examples/detecting-cars-using-gaussian-mixture-models.html?prodcode=VP&amp;language=en

http://scikit-learn.org/stable/modules/mixture.html",0,0,False,default,,,,,
288,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,4,32yghi,blog.monkeylearn.com,Analyzing news headlines across the globe with Kimono and MonkeyLearn,https://www.reddit.com/r/MachineLearning/comments/32yghi/analyzing_news_headlines_across_the_globe_with/,wildcodegowrong,1429298887,,0,0,False,default,,,,,
289,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,4,32yjjw,arxiv.org,No more meta-parameter tuning in unsupervised sparse feature learning,https://www.reddit.com/r/MachineLearning/comments/32yjjw/no_more_metaparameter_tuning_in_unsupervised/,[deleted],1429300326,,3,5,False,default,,,,,
290,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,6,32yvu1,civisanalytics.com,The Two Cultures of People Science,https://www.reddit.com/r/MachineLearning/comments/32yvu1/the_two_cultures_of_people_science/,[deleted],1429306190,,2,3,False,default,,,,,
291,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,7,32z16w,sumsar.net,The Non-parametric Bootstrap as a Bayesian Model,https://www.reddit.com/r/MachineLearning/comments/32z16w/the_nonparametric_bootstrap_as_a_bayesian_model/,rasmusab,1429309493,,4,39,False,http://b.thumbs.redditmedia.com/vkK8e0c7L6YLtSTo9qS6817LC6dOlsDfj8n9YfISULc.jpg,,,,,
292,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,9,32zd63,self.MachineLearning,Ask ML: Topic Modelling for Tree Structured Documents?,https://www.reddit.com/r/MachineLearning/comments/32zd63/ask_ml_topic_modelling_for_tree_structured/,tinyProton,1429316140,"Hi /r/machineLearning,

I have a huge dataset of tree-structured documents (e.g. each document is a family tree of a specific person). I want to apply a topic modelling algorithm, such as Latent Dirichlet Allocation, on this dataset. Does anyone know a paper/algorithm that address this kind of problems?  ",1,3,False,self,,,,,
293,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,12,32zv3x,self.MachineLearning,Automated generation of classical Chinese poetry?,https://www.reddit.com/r/MachineLearning/comments/32zv3x/automated_generation_of_classical_chinese_poetry/,minghuiyu,1429327267,"I am interested in automated generation of classical Chinese poetry. Compared with other literature forms, classical Chinese poetry is short and has strong regulations (patterns).

My objective is to generate semantically correct &amp; creative classical Chinese poetry that average users cannot tell if it is from a human writer or from a machine.

I have no background in machine learning and is counting on self-taught to learn it. Hopefully in a few years, I can make some progress.
",4,7,False,self,,,,,
294,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,17,330hpz,self.MachineLearning,Advancing the Simulation to help Agents Evolve better,https://www.reddit.com/r/MachineLearning/comments/330hpz/advancing_the_simulation_to_help_agents_evolve/,BinaryAlgorithm,1429345885,"As a dabbler/hobbyist in machine learning for the last 20 years, I have been trying to find novel ways to explore complex emergent behaviors (as opposed to solving a specific domain problem like vision). My experimental setup is usually a sandbox with the following elements:

- Simulated 2D environment, sometimes with tile/grid movement and sometimes with ""free"" rotation and movement or physics
- A series of Agents who can perform actions in the environment and sense the environment; typically I use various types of neural networks to drive them, but I have tried state machine-like processing units too
- Selection criteria based on the environment. It can be direct (ie, eat for energy and then clone yourself or ""die"" from lack of food) or indirect (fixed lifespan, fitness criteria from performed actions, simulation picks the winners).
- Sometimes genetic algorithms and breeding and sometimes replication with mutation.

Now this is all very interesting, and you can get Ant or Swarm behaviors out of it, but I want to try for more complex behaviors like social interaction because I believe that more complexity will yield more interesting results. Admittedly, the limitations so far may be more from the simulation (ie, my inability to imagine challenges to simulate) than anything.

Found a few things today that got me thinking:
- http://www.necsi.edu/visual/systems.html , 6 visuals about complexity. 
- http://edgeoforder.org/Inv2.html , in physical terms, complexity seems to arise from the interface of chaotic and orderly regions; in simulation terms, the Agents and their Environment (""energy balance"" perhaps)
- http://rspb.royalsocietypublishing.org/content/280/1755/20122863 , adding a connection cost to neural networks leads to modular networks (which in turn appear more powerful and adaptable). I want to try a cluster design sparsely connected.
- http://shinyverse.org/larryy/Yaeger.ALife3.pdf , a good rundown of PolyWorld, one of the more interesting sandbox/neural network simulations
- http://qrng.anu.edu.au/index.php , perhaps using quantum generated random numbers when randomizing simulation environment changes and other things, put a little of the physical world into the simulation?

Some other patterns I thought were interesting (or don't have the article):
- Networks can tend to seize (too much activity) or die (too little activity); our brains modulate this I think with tonal signals (""brainwaves"") and I wonder if this kind of ""keep-alive"" signal can help a neural network? Particularly of the spiking variety.
- Topography of an ANN is critical to its function, but I have no way of knowing what will work in advance, so I need a good method to evolve the structure and weights. I'm not yet sure how to encode the actual structure to its genes (when using GA's) but I normally randomly add/remove/adjust nodes and links and their parameters during mutation. It seems critical for some recurrence to happen (independent of inputs).
- Most of my ""evolution"" happens at the mutation stage, but this is also mostly random (or feels so). I have yet to find a good way to make the ANN's weights evolve ""at run-time"" to learn during a single life cycle. I feel that is a major impediment to making smarter Agents.
- One thing I have not seen done, that I think was vital to our own social evolution, is the advancement of communication; some simulations have a basic Agent-Agent communication (let's call it ""Verbal""), but I've not seen the ability of Agents to drop ""artifacts"" (ie, writing) that span generations and time. It's possible to give Agents a leg up on using something like that; in the same way they can output a neuron high to ""move"" and the simulation deals with the action without the Agent having to learn what movement ""means"", we can ""drop an artifact"" with some data and a time-stamp, and later another Agent can ""pick it up"" or read it, knowing how long has passed and what data it contains. This could lead to novel behaviors.
- The highest complexity seems to come from cooperation over competition, but both are necessary. The Environment pushes survival (and this evolution), but cooperation is the way to make more complex networks (ie, Agent communication = networks of networks working together).
- Because I don't program GPUs, I have to limit my simulation processing on my PC; one way that seems good is to penalize excessive computation if it does not result in more intelligent Agents (complexity for the sake of complexity is not always useful). I usually internalize this as an ""energy cost"" of some kind on the organism for larger ANN structures. However, modular ANN is one solution because it's not fully interconnected so it's not like O(N^2) which limits my nodes and connections much more. Not all connections are ""useful"" and brains after all have a cost in real life to be complex.
- I think part of our intelligence comes from our ability to store inputs in a way that we can feed them back through our pre-processing centers later (dreams, daydreams). The brain is active in the absence of inputs. So, one of my concerns is helping the Agents be able to save inputs for recall (I guess this can be done directly in memory? But how to recall it as a synthetic input without interfering with actual input? and how to make that useful?).

There are a number of challenges, but besides how to design an Agent's body and brain (which are important too) one of things I run into is: How do I make the simulation demanding enough to force evolution of interesting behaviors, but not so complex that all Agents stall? I'm of the assumption that if we want a behavior we need the sim to reward it; so if we want them to ""talk and write"" we first provide the function then reward them for taking the action. However, that in itself doesn't create the advanced behavior associated with communication or memory.

In terms of the sorts of ""built-in"" nodes (sensory and action), I wanted to include some kind of recall function where if certain outputs or regions are activated, then stored inputs get set on secondary input nodes (so it's like shadow inputs, but not always active). But, we need to store and recall inputs from a certain *time* and I'm not sure how.

In other words, not just ""instinctual"" behaviors but planning requires a sense of time/memory. Any ideas?",6,2,False,self,,,,,
295,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,18,330j8w,self.MachineLearning,Neural Nets and Card Games,https://www.reddit.com/r/MachineLearning/comments/330j8w/neural_nets_and_card_games/,[deleted],1429347643,"Hey guys. Yesterday I posted my [Gin Rummy Neuroevolution](http://rickgorman.github.io/neural) project (shameless plug). I spent some time today going through a sample game played with the AI, and I have some questions about how best to structure the input layer. Skip the next paragraph or read on for some domain knowledge.

In Gin, you have a 10-card hand and you're trying to make melds and sets out of your cards. A meld is 3-5 cards of the same suit in sequence (i.e. 3d,4d,5d), while a set is 3-4 cards of the same rank (i.e 3d,3c,3s or 6c,6h,6d,6s). Players take turns drawing from either the deck or the top of the discard pile, then choosing a card to discard. When they discard, if they have 10 or fewer deadwood (unmatched cards) they can discard face-down and declare a ""knock"". With exactly 0 deadwood, they can ""knock gin"" and receive a 25-point bonus for the hand. 

My concern has to do with the AI discarding cards that are already paired up. I've seen it do this a number of times, and while I could be naive and say ""it hasn't evolved those smarts yet,"" I wonder if it will be able to evolve those smarts at all.

I'd love some feedback as to how best to structure the input layer such that the weights form meaningful abstractions around things like sets and melds.

My input layer is structured like so:

* There is one Perceptron per card in the hand (so 11 total). The hand is unsorted and the sequence of cards may change at any time.
* There is one Perceptron per card in the discard pile (so 30 total -- the game ends if only two cards are left in the draw pile: 10+10+30+2 = 52)
* (unimplemented) There is one Perceptron per card for the opponent's hand. In the case that the opponent knocks improperly, their hand is played face-up.

I have one hidden layer, about the same size as the input, and an output layer like so [with options]:

* First action of the hand [draw, pickup-from-discard-pile]
* Second action of the hand [discard, knock, knock-gin]
* Index of card for second action [0 through 10]
* Accept an improper knock? [yes, no]

All that out, here's my concern: assume the player's hand is randomly shuffled after each move. How can the weights possibly balance for this? Presumably, the optimal weights for the input layer that deal with the player's hand will all be the same. Can the hidden layer accommodate this? Do I need a second or third hidden layer?

I've considered reworking the player's hand to act as 52 Perceptrons, each of which is a yes/no answering the question, ""Is card X in my hand?"" My gut tells me not to do this, as if I'm giving the AI too much knowledge about the problem domain. I want the AI to remain as expert knowledge-free as possible.

Thank you for reading my question. Your input is greatly appreciated. ",0,1,False,default,,,,,
296,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,18,330jyx,self.MachineLearning,How to best feed a card game hand into a neural network,https://www.reddit.com/r/MachineLearning/comments/330jyx/how_to_best_feed_a_card_game_hand_into_a_neural/,rickthegrower,1429348385,"Hey guys. Yesterday I posted my [Gin Rummy Neuroevolution](http://rickgorman.github.io/neural) project (shameless plug). I spent some time today going through a sample game played with the AI, and I have some questions about how best to structure the input layer. Skip the next paragraph or read on for some domain knowledge.

In Gin, you have a 10-card hand and you're trying to make melds and sets out of your cards. A meld is 3-5 cards of the same suit in sequence (i.e. 3d,4d,5d), while a set is 3-4 cards of the same rank (i.e 3d,3c,3s or 6c,6h,6d,6s). Players take turns drawing from either the deck or the top of the discard pile, then choosing a card to discard. When they discard, if they have 10 or fewer deadwood (unmatched cards) they can discard face-down and declare a ""knock"". With exactly 0 deadwood, they can ""knock gin"" and receive a 25-point bonus for the hand.

My concern has to do with the AI discarding cards that are already paired up. I've seen it do this a number of times, and while I could be naive and say ""it hasn't evolved those smarts yet,"" I wonder if it will be able to evolve those smarts at all.

I'd love some feedback as to how best to structure the input layer such that the weights form meaningful abstractions around things like sets and melds.
My input layer is structured like so:

* There is one Perceptron per card in the hand (so 11 total). The hand is unsorted and the sequence of cards may change at any time.
* There is one Perceptron per card in the discard pile (so 30 total -- the game ends if only two cards are left in the draw pile: 10+10+30+2 = 52)
* (unimplemented) There is one Perceptron per card for the opponent's hand. In the case that the opponent knocks improperly, their hand is played face-up.

I have one hidden layer, about the same size as the input, and an output layer like so [with options]:

* First action of the hand [draw, pickup-from-discard-pile]
* Second action of the hand [discard, knock, knock-gin]
* Index of card for second action [0 through 10]
* Accept an improper knock? [yes, no]

All that out, here's my concern: assume the player's hand is randomly shuffled after each move. How can the weights possibly balance for this? Presumably, the optimal weights for the input layer that deal with the player's hand will all be the same. Can the hidden layer accommodate this? Do I need a second or third hidden layer?

I've considered reworking the player's hand to act as 52 Perceptrons, each of which is a yes/no answering the question, ""Is card X in my hand?"" My gut tells me not to do this, as if I'm giving the AI too much knowledge about the problem domain. I want the AI to remain as expert knowledge-free as possible.

Thank you for reading my question. Your input is greatly appreciated.",5,8,False,self,,,,,
297,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,19,330n65,hameddaily.blogspot.com,Robust regression with random sampling,https://www.reddit.com/r/MachineLearning/comments/330n65/robust_regression_with_random_sampling/,mhfirooz,1429352108,,0,2,False,http://b.thumbs.redditmedia.com/pIjrddPeIpMKZGP_YMO2skGSSb1RqDI1KvbaLt11afI.jpg,,,,,
298,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,22,33106g,self.MachineLearning,Dynamics of nonlinear neural networks: dynamical systems perspective,https://www.reddit.com/r/MachineLearning/comments/33106g/dynamics_of_nonlinear_neural_networks_dynamical/,ML_welldoitlive,1429364120,"There was a LOT ( and I really mean a LOT..many folks new to ML/neural nets don't realize how much) work done in 80s/90s/early 2000s in neural networks. Quite a bit of it focussed on analysing how feedforward or recurrent neural nets learn: by converting the backprop into a differential equation (by assuming small learning step), and then apply ODE/Dynamical systems theory, to study evolution of weight vectors in (pseudo) time.

I am looking for find references for feedforward neural networks (deep or not) which use nonlinear activation functions. I was able to find a complete reference to the theory in case of LINEAR activation function, but not too much in the nonlinear case. I KNOW that the nonlinear case is much too difficult, but I am just trying to see what WAS actually done.

Thanks a lot.

Edit: The linear learning survey is : 
""Learning in linear neural networks: a survey"" by PF Baldi.",16,8,False,self,,,,,
299,MachineLearning,t5_2r3gv,2015-4-18,2015,4,18,23,3315oq,self.MachineLearning,Could a Bayesian probability value be considered the same as or similar to a distance function in topological data analysis?,https://www.reddit.com/r/MachineLearning/comments/3315oq/could_a_bayesian_probability_value_be_considered/,sleepicat,1429367843,"I'm wondering if Bayesian probability would still be needed if all of your data were already connected and evaluated in a topological graph.  

I don't know all of the mathematical ins-and-outs, and am new to Bayesian methods and topology, but if someone could shed some light on this, that would be great.",11,5,False,self,,,,,
300,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,0,3318sv,self.MachineLearning,Suggestions to get started with Machine Learning with Python?,https://www.reddit.com/r/MachineLearning/comments/3318sv/suggestions_to_get_started_with_machine_learning/,[deleted],1429369801,"I'm at late beginner / intermediate levels with Python. I know enough to be able to search around for an answer and use it, at any rate. I learned Django after that, and can make, once again, intermediate level web apps.

I want to start learning machine learning, specifically in order to implement it in projects I'm making. My interest specifically lies in NLP and linguistic analysis; I have no interest in the statistical aspects of machine learning. As I said, the language I'd like to use is Python, since I want it to tie in with my Django based apps.

What would be the best approach for me? Where do I start? I checked out Andrew Ng's Machine Learning course, but it uses MatLib. Should I continue with it? ",24,33,False,default,,,,,
301,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,0,331ceb,datasciencecentral.com,New Batch of Machine Learning Resources and Articles - April 18,https://www.reddit.com/r/MachineLearning/comments/331ceb/new_batch_of_machine_learning_resources_and/,vincentg64,1429371987,,1,0,False,default,,,,,
302,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,5,332557,self.MachineLearning,What is the potential value of having software engineering experience for someone who wants to work in the field of business intelligence? Should I take a substantially lower wage to gain the experience? (Xpost from r/jobs),https://www.reddit.com/r/MachineLearning/comments/332557/what_is_the_potential_value_of_having_software/,spiderkiddid,1429387706,"So I originally made [this post](http://www.reddit.com/r/jobs/comments/331yjv/what_is_the_potential_value_of_having_software/) in /r/jobs but I thought this sub might be able to provide some insight as well. 
    
So to give you a little background on myself: I have 3.5 years of work experience as a SQL reporting analyst and data analyst. I worked 2.5 years as a data analyst/reporting analyst for an IT department of a major corporation, six months as an industry survey analyst, 3 months in a start up as a reporting analyst and now 6 months for an online retailer as a SQL Reporting analyst and data analyst. I have also been attending an online Masters program in applied statistics and machine learning.

While my true passion lies in data analysis and business intelligence, I have also slowly been building up my coding skills, first with SQL, SAS, r and Excel VBA, then Python and now more recently C#. I believe that mixing data analysis as well as programming knowledge will make for a potent and profitable combination. Due to my excellent performance at my current job and having expressed an interest in becoming a programmer, I was informed that I could interview for the role of ""Software Developer"" where I would be working on building the processes and databases to mange all of the data in our company's data environment. I interviewed and was given an offer yesterday that I have a week to decide on. The only problem is that this job pays considerably lower that what I could be making in a data scientist/business intelligence job once I get my Masters, which will be by the end of this summer.

So here is my question: is getting experience as a software developer on back end systems something that could be worth it in the long run for a career in business intelligence? ",3,0,False,self,,,,,
303,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,5,33285g,self.MachineLearning,What is the result of training a Maximal Margin Classifier in non separable classes.,https://www.reddit.com/r/MachineLearning/comments/33285g/what_is_the_result_of_training_a_maximal_margin/,sfotiadis,1429389305,"I'm referring to the plain vanilla MMC. Is this optimisation solvable?

Equivalently for the support vector classifier: what if the number of point on the wrong side of the hyperplane are more than the tuning parameter C?",1,3,False,self,,,,,
304,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,9,332xs6,self.MachineLearning,Is there a better way to represent text sequence input in a recurrent network?,https://www.reddit.com/r/MachineLearning/comments/332xs6/is_there_a_better_way_to_represent_text_sequence/,feedthecreed,1429404000,"One-hot word vectors seems like an unnecessarily expensive way to represent a sequence when your vocabulary is fairly high. If you have 40,000 words in your vocabulary, you'll have to work with 40,000 dimensional vectors. On the other hand, single character input sequences have a dimensional w.r.t the number of characters. But this would cause much longer term dependencies between words in a sentence. Is there a representation of text sequences that can give the best of both worlds?",20,16,False,self,,,,,
305,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,14,333oiz,viesearch.com,Al Madina Eshop-Best Machinery Trader,https://www.reddit.com/r/MachineLearning/comments/333oiz/al_madina_eshopbest_machinery_trader/,almadinaeshop,1429421526,,0,0,False,default,,,,,
306,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,15,333r9w,self.MachineLearning,Any tutorials where i can learn theano.?,https://www.reddit.com/r/MachineLearning/comments/333r9w/any_tutorials_where_i_can_learn_theano/,tushar1408,1429423729,,10,0,False,self,,,,,
307,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,18,3341ka,lrec-conf.org,'Some of my best friends are Linguists': neat 2004 retrospective on 70s-80s NLP with lots of quotable quotes,https://www.reddit.com/r/MachineLearning/comments/3341ka/some_of_my_best_friends_are_linguists_neat_2004/,bluecoffee,1429434509,,1,13,False,default,,,,,
308,MachineLearning,t5_2r3gv,2015-4-19,2015,4,19,20,3349m8,self.MachineLearning,Question about feature extraction via dimensionality reduction,https://www.reddit.com/r/MachineLearning/comments/3349m8/question_about_feature_extraction_via/,vytah,1429443579,"I have data on how *m* people rated *n* movies on the scale from 1 to 1, both *m* and *n* are in thousands. I want to extract 1020 traits and assign them to both users and movies, so the dot product (traits of user X)(traits of movie Y) will be a good approximation of (score user X would assign to movie Y).

So the first idea is to treat each movie's scores as vectors in *m*-dimensional space and reduce it to a lower number of dimensions. The problem is: not all users rate all the movies. In fact, the data matrix is only 2025% full.

My question is: how should I proceed? Is this even a good approach, or should I use something else?

My second question: assume I fill the unknown positions with zeroes, then use reduced dimensions to replace those zeroes with results of the prediction, and then iterate the process: will this converge, and if it does, will it converge on anything meaningful?",2,4,False,self,,,,,
309,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,1,334xpz,self.MachineLearning,[Survey] How did you learn about ML?,https://www.reddit.com/r/MachineLearning/comments/334xpz/survey_how_did_you_learn_about_ml/,[deleted],1429460234,"Hi everyone!  My name is Vik, and I'm working on a site that has some free machine learning tutorials.  If you have a couple of minutes to do this survey (linked below), it will help me develop the site.  

The anonymized results will also be released to the community to help design better educational experiences.  I also may use some of the results in an upcoming talk.

[https://vikparuchuri.typeform.com/to/syayHn](https://vikparuchuri.typeform.com/to/syayHn)",0,0,False,default,,,,,
310,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,4,335nrq,self.MachineLearning,What are the current state-of-the-art approaches for information retrieval from text-based product reviews?,https://www.reddit.com/r/MachineLearning/comments/335nrq/what_are_the_current_stateoftheart_approaches_for/,[deleted],1429473588,"I am just starting out with NLP and I am having a hard time ascertaining the current state of the field (particularly w/ respect to this domain, product reviews). Having some training in linguistics and a lot of bioinformatics experience, most of the resources I am finding are either introductory or make me worried that i am reinventing the wheel. 

If anyone with experience in the area could point to some papers or texts (or python libraries) that are current or considered important, i would greatly appreciate it. Thanks.",3,4,False,default,,,,,
311,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,6,335vo3,self.MachineLearning,"What is the best Machine Learning Textbook out there, which has good directions into Deep Learning?",https://www.reddit.com/r/MachineLearning/comments/335vo3/what_is_the_best_machine_learning_textbook_out/,transhumanist_,1429477623,,27,55,False,self,,,,,
312,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,6,33617d,self.MachineLearning,What interesting things could I do with 1.25 million tweets about movies?,https://www.reddit.com/r/MachineLearning/comments/33617d/what_interesting_things_could_i_do_with_125/,neyuh,1429480407,"For my bachelor thesis I have to find patterns in tweets about different movies. 

I now have tweets of about 15 movies with each about 80,000 tweets. I save everything the Twitter API gives me about the tweets.

These are the things I already did:

* Sentiment analysis of the text using a naive bayes classifier trained on movie reviews.
* Applied K-Means clustering to geo location of tweets.
* Made a graph of percentage of positive tweets per day.
* Comparison of sentiment before/after the release of the movie.
* Applied linear regression to a graph with on the x-axis the percentage of positive tweets before release of a movie and on the y-axis the percentage of positive tweets after release.
* Applied linear regression to a graph with on the x-axis the percentage of tweets of a movie classified as positive and on the y-axis the IMDb score of that movie.

Do you have other ideas about things that I can do with the data?

(Sorry if my grammar was bad, English isn't my mother language)",11,0,False,self,,,,,
313,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,9,336ivb,self.MachineLearning,Interesting philosophical question,https://www.reddit.com/r/MachineLearning/comments/336ivb/interesting_philosophical_question/,f4gb4lls,1429489792,"If one were to perform a machine learning algorithm entirely by hand, is it still machine learning?",12,0,False,self,,,,,
314,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,10,336nrt,self.MachineLearning,Computing Mean and Variance of Non integrable in Expectation Propagation,https://www.reddit.com/r/MachineLearning/comments/336nrt/computing_mean_and_variance_of_non_integrable_in/,[deleted],1429492443,"I am trying to compute the mean and variance of a likelihood function in order to create an update rule using the expectation propagation algorithm. 

If I have understood the literature correctly, this is usually done by minimizing the KL Divergence such that
argmin(KL(p(x)q'(x)||q(x)(q'(x)). In other words, we minimize our approximating distribution, q(x) for some factor p(x) with in terms of the mean and variance of q(x) and q'(x) is our overall posterior approximation divided by q(x).

The issue I'm having is that my likelihood function p(x) is unintegrable and is in the form: exp(-exp(||d - x||)). Has anyone had any experience computing similar functions?",0,1,False,default,,,,,
315,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,10,336oud,self.MachineLearning,Computing the Mean and Variance of Non Integrable Functions for Expectation Propagation,https://www.reddit.com/r/MachineLearning/comments/336oud/computing_the_mean_and_variance_of_non_integrable/,lol__wut,1429493048,"Hi all, 
a super specific question but I'm not sure where else to turn.

I am trying to compute the mean and variance of a likelihood function in order to create an update rule using the expectation propagation algorithm. 

If I have understood the literature correctly, this is usually done by minimizing the KL Divergence such that
argmin(KL(p(x)q'(x)||q(x)(q'(x)). In other words, we minimize our approximating distribution, q(x) for some factor p(x) in terms of the mean and variance of q(x) and q'(x) is our overall posterior approximation divided by q(x).

The issue I'm having is that my likelihood function p(x) is unintegrable and is in the form: exp(-exp(||d - x||)). Has anyone had any experience computing similar functions?",4,0,False,self,,,,,
316,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,11,336ss0,self.MachineLearning,Predicting the demise of The Button,https://www.reddit.com/r/MachineLearning/comments/336ss0/predicting_the_demise_of_the_button/,Balootwo,1429495311,"Hi All, 

After all the buzz I only started caring about /r/TheButton today. I'm interested in it from a statistical perspective.  Suppose I'm interested in the final distribution of timer duration as a function of time.  How might I predict the heatmap of this distribution given real time as the (x), timer duration as the (y), and probability as the color? -I'm fairly new to machine learning, so this might be a good 'intro dataset' to help me figure the whole thing out. ",4,0,False,self,,,,,
317,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,11,336yx5,self.MachineLearning,"As someone coming with a huge background with statistics but little background in programming, where do I start?",https://www.reddit.com/r/MachineLearning/comments/336yx5/as_someone_coming_with_a_huge_background_with/,[deleted],1429498707,"I got to attend a machine learning course not long ago, and everyone around me had various competencies. Most were programmers with some statistical background. I am a statistician with no programming background. I was able to use my statistical knowledge techniques though to create the best performing model in the workshop. I left realizing that people around me were making hundreds of thousands of dollars and me, a grad student, came home to my 14k paycheck with a wife and kid.

I know that I lack programming knowledge - I looked up many of the jobs that utilize my skill set and I am overqualified for most of the. Except on one level... My programming skills are nil. I can create incredible models - I have one model that saved my university 30 million in student retention, and that was BEFORE I learned how to use some of the machine learning GUIs. 

I can do better, and I want to. What programming language should I learn that complements machine learning well, and where should I start?",3,0,False,default,,,,,
318,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,13,3377vx,self.MachineLearning,Are there any historically nontechnical companies doing great work in ML?,https://www.reddit.com/r/MachineLearning/comments/3377vx/are_there_any_historically_nontechnical_companies/,dis_my_name,1429504183,Just curious to see if anybody knew of some nontechnical companies doing cool things with data. Does anybody have experience working at one?,5,1,False,self,,,,,
319,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,14,337f9g,cs.stanford.edu,"REINFORCEjs: Reinforcement Learning in Javascript, by Andrej Karpathy",https://www.reddit.com/r/MachineLearning/comments/337f9g/reinforcejs_reinforcement_learning_in_javascript/,iori42,1429509329,,14,34,False,http://a.thumbs.redditmedia.com/EaxnJxI7UIjMxG1_isy0HflP-kcFXC5M4hZRIe5zU40.jpg,,,,,
320,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,22,338brs,self.MachineLearning,Foam Machine: The Answer To All Your Party Needs,https://www.reddit.com/r/MachineLearning/comments/338brs/foam_machine_the_answer_to_all_your_party_needs/,samroborts,1429536384,,0,1,False,default,,,,,
321,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,23,338lfs,petewarden.com,Why GEMM is at the heart of deep learning,https://www.reddit.com/r/MachineLearning/comments/338lfs/why_gemm_is_at_the_heart_of_deep_learning/,iori42,1429541453,,22,47,False,http://b.thumbs.redditmedia.com/CaUdUtImsazleQg9XBH7UZx3QGyhir1zfZlDMsBCsuY.jpg,,,,,
322,MachineLearning,t5_2r3gv,2015-4-20,2015,4,20,23,338me0,self.MachineLearning,Bee Algorithm,https://www.reddit.com/r/MachineLearning/comments/338me0/bee_algorithm/,Gritika,1429541900,"Hi
I was a part of your Machiine Learning course in coursera. You are an amazing teacher, very clear in your concepts. 
I have been given a project on ""Character Recognition using Bee Algorithm"". Could you please guide me. ",1,0,False,self,,,,,
323,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,0,338sqx,self.MachineLearning,"Hierarchical Softmax, why is it faster?",https://www.reddit.com/r/MachineLearning/comments/338sqx/hierarchical_softmax_why_is_it_faster/,AlcaDotS,1429544788,"I'm currently preparing for my thesis in which I want to build language models with neural nets. I've run into the notion of hierarchical softmax in the word2vec paper 'Efficient Estimation of Word Representation in Vector Space' by Mikolov et al. (2013) and also in 'A Scalable Hierarchical Distributed Language Model' by Mnih &amp; Hinton (2007) in which a speedup of 200x is noted over the ['normal' softmax](http://en.wikipedia.org/wiki/Softmax_function). I have trouble understanding them. I'll write down how I think it works and what this means in terms of processing complexity. Hopefully you can correct me where I'm wrong.

I understand that there is a short/fast path to each specific word (which is my class type) in the tree. I learned in [Hinton's coursera class](https://www.youtube.com/watch?v=yqsI-X40OBY) that cross entropy is the 'right' cost function for training a softmax and the error derivative of the softmax is y(i)-t(i). So although the path to a specific word is short, this means that all paths to all words must be evaluated.

Mnih &amp; Hinton use a feature vector for each node to decide which branch to take. For a fully connected tree the number of nodes has the same complexity (grows as fast) as the number of leaves (minus 1) for a full tree. Assuming that the size of the feature vector is the same as the last layer before the softmax then the number of parameters learned for the tree is the same as a fully connected layer to a softmax.

So finding the probabilities of all nodes starts with taking the dot product of the previous layer with each of the decision vectors (1 matrix multiplication). Then for each node the logistic function must be calculated and the product must be forwarded in the tree to find the probabilities of the leaves.

However, according to my math this takes almost exactly as many calculations as the normal softmax.
The normal softmax starts with a matrix multiplication for the fully connected layer below, which is almost as large as the matrix multiplication for the tree. Then the actual softmax is calculated, which is an element-wise exponentiation (which is also necessary for the logistic function) and then a sum (which can be parallelized efficiently in a pairwise tree) to find the normalization term.

So what part(s) don't I understand correctly? It seems like they have the same computational complexity during training.",6,4,False,self,,,,,
324,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,1,338wnf,blogs.technet.com,Free eBook from Microsoft Press on Azure Machine Learning,https://www.reddit.com/r/MachineLearning/comments/338wnf/free_ebook_from_microsoft_press_on_azure_machine/,MLBlogTeam,1429546463,,0,1,False,default,,,,,
325,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,2,3393ht,boston.ml,"Next.ML: Workshops &amp; Case Studies in Cambridge, MA",https://www.reddit.com/r/MachineLearning/comments/3393ht/nextml_workshops_case_studies_in_cambridge_ma/,gwulfs,1429549425,,0,5,False,http://a.thumbs.redditmedia.com/Ilc5i4qhESjnm7pXgRTROD4wbfActGc5xNyLmeqAPR8.jpg,,,,,
326,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,5,339yuc,self.MachineLearning,What good is machine learning? What are you guys doing with it?,https://www.reddit.com/r/MachineLearning/comments/339yuc/what_good_is_machine_learning_what_are_you_guys/,dsschnau,1429562727,Its a naive question because clearly machine learning is good for something or there wouldn't be such a big community around it. So what kind of things are y'all using it for? What makes it so interesting?,2,0,False,self,,,,,
327,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,10,33az62,cireneikual.wordpress.com,A Tutorial on Sparse Distributed Representations (Sparse Codes),https://www.reddit.com/r/MachineLearning/comments/33az62/a_tutorial_on_sparse_distributed_representations/,CireNeikual,1429579576,,8,16,False,http://b.thumbs.redditmedia.com/PqqJoeqZTRuJHLYGtMJXXqVOqfGO2Tcw5vP-JOybxBI.jpg,,,,,
328,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,11,33b6hv,tjo-en.hatenablog.com,Machine learning for package users with R (3): Support Vector Machine,https://www.reddit.com/r/MachineLearning/comments/33b6hv/machine_learning_for_package_users_with_r_3/,TJO_datasci,1429583136,,0,1,False,default,,,,,
329,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,13,33blwr,redwood.berkeley.edu,Minimum Probability Flow Learning,https://www.reddit.com/r/MachineLearning/comments/33blwr/minimum_probability_flow_learning/,downtownslim,1429591850,,4,15,False,default,,,,,
330,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,18,33c5jf,self.MachineLearning,Question about ROC Curve,https://www.reddit.com/r/MachineLearning/comments/33c5jf/question_about_roc_curve/,p1mps,1429608500,"Hi /r/MachineLearning,

I have a noob question about ROC Curve. I know that the ROC Curve analysis evaluates the performance of classifiers, plotting their TFr and FPr on the axis. However, in some papers when the ROC Curve is used to evaluate one classifier performance, the Curve is not straight, how this is possible? I mean, the TFr and FPr how can change if the dataset doesn't change?",6,0,False,self,,,,,
331,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,19,33c808,information-age.com,Is machine learning about to go mainstream?,https://www.reddit.com/r/MachineLearning/comments/33c808/is_machine_learning_about_to_go_mainstream/,mosesvacarro,1429610798,,1,0,False,http://b.thumbs.redditmedia.com/5RK2KYrAkLgPaX9LiM7Zp3DDiXONpnv_out9nosQDcE.jpg,,,,,
332,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,20,33cd4l,self.MachineLearning,Clarification about softmax regression,https://www.reddit.com/r/MachineLearning/comments/33cd4l/clarification_about_softmax_regression/,captcompile,1429615371,"I've been looking at UFLDL's tutorial on [softmax regression](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression). At the equation where they present the gradient of the cost function, should there be an additional summation over all the different classes, i.e. values of j? ",1,0,False,self,,,,,
333,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,21,33ckbv,self.MachineLearning,I need help with Sentimental analysis news.,https://www.reddit.com/r/MachineLearning/comments/33ckbv/i_need_help_with_sentimental_analysis_news/,nickbomtempo,1429620564,"I have one project, to analyse feeling in the news. I don't know where to begin, if anyone know something about the subject, Where I should start? Any paper you can indicate? Any technique? Any summary? 

I am so sorry to take your time.

Anyways, thanks for everything. ",6,0,False,self,,,,,
334,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,22,33cq4d,nbviewer.ipython.org,PyCon 2015 Scikit-Learn Tutorial Index,https://www.reddit.com/r/MachineLearning/comments/33cq4d/pycon_2015_scikitlearn_tutorial_index/,cast42,1429623925,,0,1,False,default,,,,,
335,MachineLearning,t5_2r3gv,2015-4-21,2015,4,21,23,33cx7b,medium.com,I haven't seen a lot of talk about podcasts on here. Here's a list of ML podcasts I enjoy. Hope it's helpful.,https://www.reddit.com/r/MachineLearning/comments/33cx7b/i_havent_seen_a_lot_of_talk_about_podcasts_on/,mattf514,1429627409,,10,62,False,default,,,,,
336,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,0,33d6xz,devquixote.com,Some practical experiences bringing a machine learning feature to our product,https://www.reddit.com/r/MachineLearning/comments/33d6xz/some_practical_experiences_bringing_a_machine/,devquixote,1429631895,,9,0,False,http://a.thumbs.redditmedia.com/yznJebzsyGSvV_E4YGkI2_LT0drRW4cdTjcIJpw9of8.jpg,,,,,
337,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,2,33dio5,efavdb.com,The mean shift clustering algorithm,https://www.reddit.com/r/MachineLearning/comments/33dio5/the_mean_shift_clustering_algorithm/,efavdb,1429636964,,0,22,False,http://b.thumbs.redditmedia.com/9VvXmWWTT3kE34Zw1_nGELemkIxWBx6QwnaI1ydIQ1c.jpg,,,,,
338,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,3,33dr43,blog.francoismaillet.com,Epic NHL goal celebration hack: real-time machine learning and Philips hue light show,https://www.reddit.com/r/MachineLearning/comments/33dr43/epic_nhl_goal_celebration_hack_realtime_machine/,ddcarnage,1429640596,,2,80,False,http://b.thumbs.redditmedia.com/-OIRCTWgRCPnhCfaec1El3c5BDaSXNqNpdwcT9eYHNs.jpg,,,,,
339,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,3,33dt6u,datanami.com,AMEX Adopts Machine Learning to Crunch More Data,https://www.reddit.com/r/MachineLearning/comments/33dt6u/amex_adopts_machine_learning_to_crunch_more_data/,caroljmcdonald,1429641543,,0,0,False,http://b.thumbs.redditmedia.com/_qm74K7fcnIm3V_VElIosr7EaKErg2ebhaTL_lTDqwY.jpg,,,,,
340,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,4,33dygb,self.MachineLearning,We (Custora) are hiring: Looking for data scientist researchers and engineers,https://www.reddit.com/r/MachineLearning/comments/33dygb/we_custora_are_hiring_looking_for_data_scientist/,dtelad11,1429643832,,0,0,False,default,,,,,
341,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,6,33eglq,self.MachineLearning,Python help? json/csv /pandas,https://www.reddit.com/r/MachineLearning/comments/33eglq/python_help_jsoncsv_pandas/,python_J,1429651680,"Hello - I am new to this field.
I am hoping to do some summary statistics (for starters!) on the Yelp Dataset Challenge using Python. I am struggling to even start however :-)
Is it best to convert these json files to csv first, or should i be able to work with json on the fly.
I'm struggling to understand. Apologies if this is a very basic questions. I've only dealt with flat csv files before this.
",7,0,False,self,,,,,
342,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,9,33f035,self.MachineLearning,Applying Deep Learning on Large(~10 million) Sparse high-dimensional(~1000 dimension) real valued medical data,https://www.reddit.com/r/MachineLearning/comments/33f035/applying_deep_learning_on_large10_million_sparse/,aashus18,1429660858,"I would like to explore the use of Deep Learning for classification task - on the above dataset. Most of the literature that I reviewed deals with application speech, images and text. 
Can anyone please point me in the right direction - and if at all deep learning is the right path to follow in this scenario.",16,6,False,self,,,,,
343,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,14,33fz5a,millenniumindustry.blogspot.com,Millennium work: Fabrication classes,https://www.reddit.com/r/MachineLearning/comments/33fz5a/millennium_work_fabrication_classes/,pakvoter,1429679674,,0,1,False,default,,,,,
344,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,19,33gmso,vitalflux.com,Machine Learning Research in Top 10 US Universities,https://www.reddit.com/r/MachineLearning/comments/33gmso/machine_learning_research_in_top_10_us/,krish14,1429700227,,4,0,False,http://b.thumbs.redditmedia.com/45vgQktYwfNJ_VMEGjpmPsjr6Z9kw3z5J_e2kqSLDSg.jpg,,,,,
345,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,20,33gnzp,self.MachineLearning,Turing machine rules,https://www.reddit.com/r/MachineLearning/comments/33gnzp/turing_machine_rules/,alanturing123,1429701261,"Does anyone know the turing machine rules for - 

multiple add - 
and a copy function? 

",0,0,False,self,,,,,
346,MachineLearning,t5_2r3gv,2015-4-22,2015,4,22,22,33gzlb,quantamagazine.org,Artificial Intelligence Aligned With Human Values | Q&amp;A With Stuart Russell | Quanta Magazine,https://www.reddit.com/r/MachineLearning/comments/33gzlb/artificial_intelligence_aligned_with_human_values/,Bekdorf5v,1429708956,,14,21,False,http://b.thumbs.redditmedia.com/7uftz8q1nkje1wk5I6EbQPNR1-gTs2C6zqgpa7zhtHs.jpg,,,,,
347,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,0,33hdbk,self.MachineLearning,How to overcome overfitting in the validation set?,https://www.reddit.com/r/MachineLearning/comments/33hdbk/how_to_overcome_overfitting_in_the_validation_set/,gsmafra,1429715607,"When running an algorithm for training a system it is common to consider a lot of models and using the validation set for selecting one of them. In my case I am running a mini-batch gradient descent and validating it at each epoch. It runs for hundreds of epochs, sometimes one or two thousands.

Considering also that there is an hyperparameter search, I may do this dozens of times and at the end I finish up with a number of models in the order of tens of thousands.

[This](http://i.imgur.com/gM8Gii3.jpg) is a scatter plot of some of my validation accuracies againt test accuracy. Validation in x axis, test in y. Test is higher because there is a further treatment with majority voting in a group of elements

I found only [this article](http://robotics.stanford.edu/~ang/papers/cv-final.pdf) by Ng in 1997 that states the problem and gives some tips to solve it. So my question is, are there more recent and established works on it, or are there other more simple approaches?",19,2,False,self,,,,,
348,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,0,33hgi4,rapidminer.com,RapidMiner is looking for Technical Paper submissions to be published at their upcoming modern analytics conference,https://www.reddit.com/r/MachineLearning/comments/33hgi4/rapidminer_is_looking_for_technical_paper/,[deleted],1429717005,,0,0,False,default,,,,,
349,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,0,33hi90,self.MachineLearning,Dropout/FMP/DropConnect idea,https://www.reddit.com/r/MachineLearning/comments/33hi90/dropoutfmpdropconnect_idea/,mypasswordis543210,1429717793,"NN trained with dropout could be viewed as an assembly of multiple nets. 

But what if we take network trained with dropout and apply dropout at test time, but if particular instance of network misclassify example(while most instances produce correct classification), we mark those non-dropped units as suspicious. And then we figure out what units contribute to misclassifications the most on many test examples. Question: if we drop those units from network permanently - will this improve overall performance? 

Same could be applied to fractional max pooling(so we exclude some regions from being pooled) or dropconnect(connections in this case).",9,1,False,self,,,,,
350,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,0,33hj7y,self.MachineLearning,Classifiers performance,https://www.reddit.com/r/MachineLearning/comments/33hj7y/classifiers_performance/,pollausen85,1429718249,"Hi everyone,

I'm a newbie in the machine learning and pattern recognition field. For a course for my PhD I have to analyse a real dataset. I chose to analyse the Wisconsin breast dataset. I'm using different classifiers: linear discriminant, quadratic discriminant, parzen and a non linear support vector. What I saw is if I use all the features, the non linear support vector classifier gets the worse results than the others; whereas, if I apply a feature extraction or a feature selection, the support vector classifier gets the better results. As results I mean the average classification error calculated with a cross-validation.
The my possible explanation is that in when I use all the features, the two dataset are distinctly separate in the features space and the non-linear classifier introduce an overfitting; while when I apply the feature selection / extraction is able to overcome better the overlap between the two classes. For you, could it be a correct explanation?

Thank you. ",4,0,False,self,,,,,
351,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,1,33hkoe,self.MachineLearning,I need help of you and your computers to perform smulation tests,https://www.reddit.com/r/MachineLearning/comments/33hkoe/i_need_help_of_you_and_your_computers_to_perform/,Zolden,1429718853,"Hi guys, I'm working on an universal genetic system, that would allow to generate AI for simple objects operating in some environment. So, instead of coding the AI, wi will have simply configure a simulation, and AI will be created evolutionary.

I've recently posted the [results](http://www.reddit.com/r/gamedev/comments/31idqb/im_building_evolution_based_ai_toolkit_to/) of my first experiment.

As you can see, there's some nice progress happened already. The first version of genetical system proved its ability to create a simple control system, but it need improvements and optimization, to gain the ability to leave local optimums for better choices, to find good solutions faster, to have better chances to find the best solution for the task.

So, what I do these days is change the system, run the simulation and decide if the change is good or not. I need to try different math of on genes level, different mechanics on genome level - how the mutations happen, and also different variants of fitness function on simulation level, to lead the evolution in the correct direction.

The problem is: every simulation takes too much time. About 10 hours per simulation to say how good a variant.

I have many ideas to try. And it's hard to predict, which would work better. So, there's no slowly pursuing a glimpse of better choice, it's brutal force way of trying everything. I run 2 tests a day, and it's a realy slow progress.

So, I though, what if I ask people, if they want to run a simulation of two for me, and then upload an image with the results. [Example of image](http://i.imgur.com/iw2Pz2w.png). It's a graph of fitness function of the evolving species.

Benefits for you is to watch some evolution on your computer. Though, I must say, that in most cases the results won't be any exciting, because good solutions are rare, I try new approaches, to see if there some good ways to drive the evolution.

**How I think this may work.**

Pretty simple.

You post ""I could help"". I send you .exe file (for windows). You run it for 2-3K generations (5 - 10 hours). Then you make a screenshot of the graph and send it back to me.

If there will be at least 20-30 people who agree to help, I could make all the tests in a few days. It would be realy helpful. (Or not, because so far my actual solution works better than the new ones I try).

Also note: right now I'm talking about only one experiment. 

There are wolves, rabbits, bears, food for wolves and rabbits, also wolves can shoot projectiles to kill wolves, rabbits and bears. Wolves and rabbits evolve.

But it's only one simple experment. I have about 10 more, untill I start the ultimate one, the one I started all this for. Those simple preliminary experiments supposed to help me to poish the genome system. And you can help me with this.

If this work, and I'll find ways improve the genome, we may see some exciting new experiments much sooner.",9,0,False,self,,,,,
352,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,1,33hncb,info.microsoft.com,Free webinar on text classification next week,https://www.reddit.com/r/MachineLearning/comments/33hncb/free_webinar_on_text_classification_next_week/,MLBlogTeam,1429720007,,0,1,False,default,,,,,
353,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,1,33holq,mechlab-engineering.de,"Realtime Motion Activity Classification (walking, bike, running, sitting) with Acceleration and Rotationrate Sensors [incl. Video]",https://www.reddit.com/r/MachineLearning/comments/33holq/realtime_motion_activity_classification_walking/,balzer82,1429720554,,0,0,False,http://b.thumbs.redditmedia.com/4-Zxik3If7ktFL41_1-xKAL_ZRmWAKlkKZT4Ch7pfpY.jpg,,,,,
354,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,1,33hq5a,arxiv.org,[1504.04788] Compressing Neural Networks with the Hashing Trick,https://www.reddit.com/r/MachineLearning/comments/33hq5a/150404788_compressing_neural_networks_with_the/,[deleted],1429721249,,15,35,False,default,,,,,
355,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,2,33hurq,datasciencecentral.com,"""How do I learn machine learning?"" A collection of resources and advice",https://www.reddit.com/r/MachineLearning/comments/33hurq/how_do_i_learn_machine_learning_a_collection_of/,rhiever,1429723266,,0,1,False,default,,,,,
356,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,2,33hv5i,self.MachineLearning,IBM Watson + Machine Learning Algorithms = Finding A Job,https://www.reddit.com/r/MachineLearning/comments/33hv5i/ibm_watson_machine_learning_algorithms_finding_a/,Bardia_UnitesUs,1429723442,"Hi Guys, My name is Bardia Nikpourian and I am the CTO of **UnitesUs.com**. I wrote this Reddit to introduce ourselves to the Reddit community and showcase how we are using IBM Watson to take the work out of finding work.

UnitesUs.com is a employment platform utilizing big data and cognitive computing to connect employers and job seekers with one another on the basis of psychometric profiling, company cultural fit and all the core competencies that are required to successfully fulfill a position. We utilize our proprietary machine learning algorithms in conjunction with cognitive computing ( IBM Watson) to take the work out of finding work.

Check out the images below for some examples on how we have used IBM Watson to make finding a job easier than ever.

Check out our demo video to understand what we do, and how we do it:
[About UnitesUs.com Video](https://www.youtube.com/watch?v=y0o9isDWDpU)

How your unique personality sets you apart from other job seekers to fit into the right company culture for you:
[UnitesUs Psychometric Calculator ](http://imgur.com/a/1PImw)


I will be more than happy to answer any questions you guys may have on our use of this technology and suggested improvements from the community are always appreciated. 

Bardia, CTO UnitesUs.Com
www.UnitesUs.com
",7,0,False,self,,,,,
357,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,2,33hwwt,self.MachineLearning,KKTTolerance setting for SMO solver in fitcsvm() in Matlab?,https://www.reddit.com/r/MachineLearning/comments/33hwwt/kkttolerance_setting_for_smo_solver_in_fitcsvm_in/,neurone214,1429724199,"Just wondering if anyone has any experience with optimizing the KKTTolerance parameter when using the SMO solver for fitcsvm(). I get a noticeable improvement in cross validation accuracy when I optimize, but my optimization is already a fairly slow process so I'd like to speed it up if I can.  In general, my optimization (using patternsearch()) runs much more quickly when I set constraints on the parameters; right now I'm using random initial conditions between 0 and 1 with the lower and upper limits set to [0 5]. Anyone know what a reasonable upper limit might be? I literally just guessed at this based on allowing a few models without constraints to optimize, but I'm hoping someone could provide some insight based on their knowledge of this parameter or pratical experience. ",0,0,False,self,,,,,
358,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,3,33i5k1,self.MachineLearning,How to get last 5 to 10 percent in classification machine learning task?,https://www.reddit.com/r/MachineLearning/comments/33i5k1/how_to_get_last_5_to_10_percent_in_classification/,matlab484,1429727953,"What's your guys method to do this? I feel like its pretty reasonable to get to 90% accuracy, but getting the last part is really hard. I'm training a classifier using AlexNet and got to 90 but am having trouble getting further. Do you gather more data, augment the data like rotations, change the algorithm?

There's a really good article on this topic here: http://cdixon.org/2015/02/01/the-ai-startup-idea-maze/ but I wanted to hear what everyone else does",16,2,False,self,,,,,
359,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,6,33iq36,self.MachineLearning,"Face Recognition on Faces ""In the Wild"" with &lt;1000 training examples",https://www.reddit.com/r/MachineLearning/comments/33iq36/face_recognition_on_faces_in_the_wild_with_1000/,alexmlamb,1429736876,"Hello,

I'm building a face recognition system for faces that are ""in the wild"". They are somewhat similar to the Labeled Faces In the Wild dataset, except that the lighting is a lot harder and some of the faces are in the profile view. In general the poses vary quite a lot. One other unusual aspect of the problem is that I will probably have between 500 - 10000 training examples of about 10 different people. Another unusual aspect of the problem is that I'm willing to settle for low recall with high precision. For example, I'd be okay if I could only classify 50% of the faces if I can classify that set with &gt;90% accuracy.

Here are the approaches that I'm considering / prototyping:

-Fisherfaces.

-Sort the instances by pose (just frontal / profile in the simplest case), and build a separate fisherfaces model for each pose. A more sophisticated variant could learn a pose clustering and then use a separate fisherfaces model for each pose.

-Define a distance metric using standard features like SIFT and then do KNN (already tried this and it's not so good, like 10% recall at 90% precision).

-Using a set of standard features like Overfeat (which is mostly trained on non-faces) and then train a shallow classifier (like logistic regression or rf) on top of it.",1,6,False,self,,,,,
360,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,7,33j4mm,quantopian.com,Accern applies advance machine-learning techniques on over 20 million web sources for algo trading.,https://www.reddit.com/r/MachineLearning/comments/33j4mm/accern_applies_advance_machinelearning_techniques/,kumesh,1429743203,,3,4,False,default,,,,,
361,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,8,33ja51,wired.com,Computers That Know How You Feel Will Soon Be Everywhere,https://www.reddit.com/r/MachineLearning/comments/33ja51/computers_that_know_how_you_feel_will_soon_be/,ahamino,1429745871,,2,4,False,http://a.thumbs.redditmedia.com/DR6nYiuG8F8w0N95wN_m6oO-DWu2zmoxzLoH6kcwgL0.jpg,,,,,
362,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,9,33jj3u,affectiva.com,SDK for tracking Facial Expressions and Human Emotion,https://www.reddit.com/r/MachineLearning/comments/33jj3u/sdk_for_tracking_facial_expressions_and_human/,ahamino,1429750430,,1,6,False,http://b.thumbs.redditmedia.com/4IU9kffNAc8I9t7WfJ157T3usa0W-LnBbNFUzFjLZFc.jpg,,,,,
363,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,15,33kipa,youtu.be,How one could do something like this using deep learning? Any ideas?,https://www.reddit.com/r/MachineLearning/comments/33kipa/how_one_could_do_something_like_this_using_deep/,test3545,1429771940,,11,4,False,http://b.thumbs.redditmedia.com/SeIPijpGpkbo23nYSoUB3fWGZhvqza9mcz57G-4GrDA.jpg,,,,,
364,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,17,33koap,bits.blogs.nytimes.com,PredPol start-up: Crime prediction with machine learning,https://www.reddit.com/r/MachineLearning/comments/33koap/predpol_startup_crime_prediction_with_machine/,pgay,1429776946,,2,1,False,http://b.thumbs.redditmedia.com/MAHzvz2oAuYdozBlvXG_S9jNiSbUjqfMVkTaVh1aIes.jpg,,,,,
365,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,18,33kr7a,drive.google.com,Aetherial Symbols - A (seemingly) new talk by Geoff Hinton,https://www.reddit.com/r/MachineLearning/comments/33kr7a/aetherial_symbols_a_seemingly_new_talk_by_geoff/,vikkamath,1429779869,,25,68,False,http://b.thumbs.redditmedia.com/YuQo0u1CS9IzfyGOHdTESfDPGHEyOPO3oUfwNdi5lbs.jpg,,,,,
366,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,19,33kyil,information-age.com,Best goal celebration ever? Hockey fan creates epic light show in his living room with machine-learning technology,https://www.reddit.com/r/MachineLearning/comments/33kyil/best_goal_celebration_ever_hockey_fan_creates/,techCIO,1429786742,,0,2,False,http://b.thumbs.redditmedia.com/Be4sjVtcyIRnP_TpziqyHd9nQLUiTBcrGvfB6U2Qd7c.jpg,,,,,
367,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,20,33kz69,self.MachineLearning,"Developing deep learning algorithms: languages, libraries, toolkits?",https://www.reddit.com/r/MachineLearning/comments/33kz69/developing_deep_learning_algorithms_languages/,petrux,1429787294,"Hi guys. I am starting writing some code for my PhD project. Basically, I have to train different deep networks (recursive, recurrent, LSTM). I'm taking a look to several repositories on github, trying to decide which language/libraries I will use. The **first** problem is that **I don't have any GPU on my laptop** and I wanted to practice a bit on it (maybe with small toy-examples) in order to get confident. Keeping in account this issue, what would you suggest?

 * Theano
 * Matlab/Octave with some libraries (if existing)
 * Scala (or even Java) with some libraries (if existing)

I also find something called `Torch` but it requires a GPU and you have to program in LUA, which I don't know at all. 
Suggestions? References? Thanks!",6,1,False,self,,,,,
368,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,21,33l39o,youtube.com,"Genetic Algorithm being applied to construct more efficient ""police sketches"", abandoning feature-by-feature strategy historically used by forensic artists in favor of a more holistic ""does it look like them"" approach (x-post /r/futurology)",https://www.reddit.com/r/MachineLearning/comments/33l39o/genetic_algorithm_being_applied_to_construct_more/,shaggorama,1429790456,,5,39,False,http://b.thumbs.redditmedia.com/UiRvZdlSVzgpjAATAZv11gli5XeFcZK3kSfYOd7hy6U.jpg,,,,,
369,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,21,33l7if,yahoolabs.tumblr.com,Machine Learning for (Smart) Dummies,https://www.reddit.com/r/MachineLearning/comments/33l7if/machine_learning_for_smart_dummies/,srkiboy83,1429793312,,3,7,False,http://b.thumbs.redditmedia.com/86wYKK0EKmXAI-Hoe7JM7dSFqdzrzwMaexq_ngzjl3U.jpg,,,,,
370,MachineLearning,t5_2r3gv,2015-4-23,2015,4,23,22,33lad0,hunch.net,Randomized experimentation,https://www.reddit.com/r/MachineLearning/comments/33lad0/randomized_experimentation/,seabass,1429795027,,3,2,False,default,,,,,
371,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,0,33ln3s,blog.yhathq.com,Rodeo: A data science IDE for Python,https://www.reddit.com/r/MachineLearning/comments/33ln3s/rodeo_a_data_science_ide_for_python/,theglamp,1429801457,,7,30,False,default,,,,,
372,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,0,33lo69,thetalkingmachines.com,Starting Simple and Machine Learning in Meds,https://www.reddit.com/r/MachineLearning/comments/33lo69/starting_simple_and_machine_learning_in_meds/,vkhuc,1429801954,,0,16,False,http://b.thumbs.redditmedia.com/Bf48OF_9xBhrpE_TAleNaUaYage-tdtm5eViIjbDI4Q.jpg,,,,,
373,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,1,33lvsq,blogs.technet.com,How to Build Your Own Custom R Modules in Azure ML,https://www.reddit.com/r/MachineLearning/comments/33lvsq/how_to_build_your_own_custom_r_modules_in_azure_ml/,MLBlogTeam,1429805397,,0,1,False,default,,,,,
374,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,1,33lxfz,self.MachineLearning,Need some direction with ELMs,https://www.reddit.com/r/MachineLearning/comments/33lxfz/need_some_direction_with_elms/,xonbugeja,1429806137,"Hi, 

Currently a college student working on my final year project for which i'm required to get a decent working knowledge of Extreme Learning Machine's for predicting seizures from EEG data. 

Was wondering if there are any kind souls here that could perhaps point me to some useful resources on the subject? Or perhaps some concepts i should focus on heavily to properly grasp this king of ML system? 

Unfortunately my course, computer engineering, hasn't focused that much on ML, (one credit in 3 years, which wasn't actually ML, lecturer just kindly mentioned some basics) 

I've had some very simple experience coding very simple ML algos (logistic regression &amp; PNN for human detection in photos).

Any and all help is appreciated. Thanks :)

Tl;dr very basic ML background, wanting to get into ELM, any suggested material or advice in general? Tnx",3,1,False,self,,,,,
375,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,1,33m2e4,youtu.be,"Deep Networks Overview, CMU 2015, Alex Smola",https://www.reddit.com/r/MachineLearning/comments/33m2e4/deep_networks_overview_cmu_2015_alex_smola/,[deleted],1429808304,,0,1,False,default,,,,,
376,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,2,33m4yg,radar.oreilly.com,"Michael Stack on HBase past, present, and future",https://www.reddit.com/r/MachineLearning/comments/33m4yg/michael_stack_on_hbase_past_present_and_future/,gradientflow,1429809411,,0,2,False,default,,,,,
377,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,3,33mbwr,self.MachineLearning,What is a good dataset where Deep Belief Network gives good result?,https://www.reddit.com/r/MachineLearning/comments/33mbwr/what_is_a_good_dataset_where_deep_belief_network/,mkarki2,1429812488,I am looking for some other image datasets besides MNIST and its variants. ,3,2,False,self,,,,,
378,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,4,33ml4x,self.MachineLearning,information extraction with limited labeled data,https://www.reddit.com/r/MachineLearning/comments/33ml4x/information_extraction_with_limited_labeled_data/,spurious_recollectio,1429816510,"I'm trying to build an information extraction system to extract e.g. relation triples from a corpus of specialized text.  I've tried a variety of tools but have run into issues with each.  I have some restrictions because this project has a commercial aspect so requires a compatible license and also I'm working in python so would prefer a python interface.

Attempts so far:

- open ie, ollie, reverb -- seem to perform ok but they're implemented in java and have a rather restrictive license.

- nltk - requires writing manual regex's to extract relations and is a bit laborious.

- mitie - seems like it might be able to work but would require training data as the existing binary relation extractors don't match the kinds of relations I'm interested in.

I would appreciate any other suggestions that might work.  I can imagine generated a few thousand labelled examples by hand (bootstrapping from some poor labeler like a regex parser) but I'm not sure what kind of a model can be trained with that limited number of examples.

If I had lots of labelled examples I might imagine using a decoder-encoder RNN to generate relation triples, etc... but I imagine that would take a huge amount of labelled data.  Does anyone know of training something like this (or a similar model) using some kind of auto-encoding and then a relatively small dataset for fine-tuning (e.g. 2-3 thousand examples).",0,1,False,self,,,,,
379,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,4,33mlu7,self.MachineLearning,"Lots of unlabeled data, few labeled data, what to do?",https://www.reddit.com/r/MachineLearning/comments/33mlu7/lots_of_unlabeled_data_few_labeled_data_what_to_do/,regularized,1429816804,I have a lot of unlabeled data but a few labeled data. Stacked denoising autoencoder is the first thing that came to my mind. What are the recent developments in the area of autoencoders? What other methods can I try(I have already tried RBM)?,7,3,False,self,,,,,
380,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,6,33n5ke,nuit-blanche.blogspot.fr,AutoML Challenge: Python Notebooks for Round 1 and more...,https://www.reddit.com/r/MachineLearning/comments/33n5ke/automl_challenge_python_notebooks_for_round_1_and/,compsens,1429825585,,0,1,False,http://a.thumbs.redditmedia.com/t0EOxxucm97RkW1mYN4XPgWOIOB3Q3HyJXxi-8Ii078.jpg,,,,,
381,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,6,33n77s,imgur.com,Android App: Nipple Detection using Convolutional Neural Network. Results. [NSFW],https://www.reddit.com/r/MachineLearning/comments/33n77s/android_app_nipple_detection_using_convolutional/,deepPurpleHaze,1429826353,,191,670,True,nsfw,,,,,
382,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,15,33op9l,self.MachineLearning,Undergrad Major?,https://www.reddit.com/r/MachineLearning/comments/33op9l/undergrad_major/,jklegit,1429857206,"Hi, I'm a community college student who's going to be transferring to a 4 year unversity. My coursework is fairly limited- 2-3 programming classes (stuff upto data structures), and math upto Linear Algebra/Calc III (self-studying linear algebra). I'm a bit torn on my undergrad major- either CS or Physics. I'm interested computational problems in physics- astrophysics and biophysics, and it seems that there's a fair amount of data mining stuff in astrophysics. On the other hand, I'm interested in theoretical parts of CS, but the CS major has more core courses in systems (OS, networking, etc.). It seems that the physics major would be more mathematically rigorous. If any people who studied these subjects could provide their perspective that would awesome. Lastly, I noticed that there are a decent amount of CS professors who studied physics, and that its easier to go from physics to CS than from CS to physics. ",9,0,False,self,,,,,
383,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,19,33p5cy,self.MachineLearning,I'm searching a simple training tool for images,https://www.reddit.com/r/MachineLearning/comments/33p5cy/im_searching_a_simple_training_tool_for_images/,fimari,1429872232,For tagging training and testing - with a GUI for lazy people. Is something out there?,5,1,False,self,,,,,
384,MachineLearning,t5_2r3gv,2015-4-24,2015,4,24,21,33pcwo,blogs.technet.com,Quick recap of Pycon 2015 at Montreal earlier this month,https://www.reddit.com/r/MachineLearning/comments/33pcwo/quick_recap_of_pycon_2015_at_montreal_earlier/,MLBlogTeam,1429878044,,0,1,False,default,,,,,
385,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,0,33pvx9,sites.google.com,Deep Learning Workshop @ ICML'15: paper submission deadline approaching (May 1)!,https://www.reddit.com/r/MachineLearning/comments/33pvx9/deep_learning_workshop_icml15_paper_submission/,dpkingma,1429888178,,0,0,False,default,,,,,
386,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,2,33qhi2,self.MachineLearning,Unsupervised learning provides insights on human hand tools?,https://www.reddit.com/r/MachineLearning/comments/33qhi2/unsupervised_learning_provides_insights_on_human/,dani_dg,1429897926,"I hope this is the right subreddit! Apologies if it isn't...

I am trying to find a link or some information on a study I read about several years ago. From what I recall, Google had allowed it's Google X ""brain"" supercomputer to scan the internet using unsupervised learning techniques. One result of this work was that virtually all human hand-tools since the dawn of human history have been created and oriented at a specific ""advantageous"" angle. 

I've tried like crazy to find a source for this story, which I heard about (probably on reddit) a few years ago. Now that I am learning more about machine learning, I am very interested in re-visiting it. 

Does anyone else recall this work? 

Edit: *now, not nos",3,0,False,self,,,,,
387,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,3,33qkpf,bicorner.com,Amazon dumbs down machine learning for the rest of us,https://www.reddit.com/r/MachineLearning/comments/33qkpf/amazon_dumbs_down_machine_learning_for_the_rest/,derrickmartins,1429899293,,0,0,False,default,,,,,
388,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,4,33qx80,technologyreview.com,An Algorithm Set To Revolutionize 3-D Protein Structure Discovery,https://www.reddit.com/r/MachineLearning/comments/33qx80/an_algorithm_set_to_revolutionize_3d_protein/,cavedave,1429905062,,4,39,False,http://a.thumbs.redditmedia.com/dkyVT0RY3NhWZLPR7BfBVR0NQQZBt0tzedZoO308Su0.jpg,,,,,
389,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,7,33rglo,self.MachineLearning,MBA walks into a bar,https://www.reddit.com/r/MachineLearning/comments/33rglo/mba_walks_into_a_bar/,Foxtr0t,1429914711,"An MBA walks into a machine learning bar.

""Here's a project, we need this and that. The budget is $5k. I want to talk to the manager. Do you have any reviews for this bar? [gets one] I'm not gonna read it anyway. Give me a timetable. OK, first train me a convnet on this dataset, for $300. [gets a counteroffer] But what if it doesn't work? I don't want to waste money. Try to understand my side.""

Gets a polite explanation how things work with grown-ups, walks out.",2,0,False,self,,,,,
390,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,8,33rjum,news.ycombinator.com,Hacker News classification using Machine Learning with MonkeyLearn,https://www.reddit.com/r/MachineLearning/comments/33rjum/hacker_news_classification_using_machine_learning/,rgarreta,1429916473,,0,1,False,default,,,,,
391,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,8,33rmig,self.MachineLearning,Need a conceptual understanding of EM algorithm accelerator,https://www.reddit.com/r/MachineLearning/comments/33rmig/need_a_conceptual_understanding_of_em_algorithm/,_fysikz,1429917958,"I'm learning both the EM algorithm and a quasi-Newton accelerator (QN1, which uses Broyden's update/the secant method, and is described [here](http://www.quretec.com/u/vilo/edu/2003-04/DM_seminar_2003_II/ver1/P10/artiklid/JamshidianJennrich.pdf)). I'm using multivariate Gaussian mixtures for my data.

I'm trying to understand how to implement the accelerator, and would greatly appreciate it if someone could help me with some very basic, conceptual questions. The accelerator is a ""pure"" one, and is supposedly ""very easy"" to implement.

1. Do I need to re-derive the expressions for new parameters in terms of old parameters to use the accelerator, or can I use the same expressions derived for the EM algorithm for GMM?

2. If I don't need to derive anything new, then how does the QN1 expression work? It doesn't seem to rely on probability distributions, as the EM algorithm expressions do. It just seems to take in an initial value and spit out a new one- sounds too good to be true.

Thanks in advance for any tips anyone can give me.",1,0,False,self,,,,,
392,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,12,33sa4h,self.MachineLearning,Face detection,https://www.reddit.com/r/MachineLearning/comments/33sa4h/face_detection/,varun_invent,1429932056,Everyone uses the good old viola-jones face detector to localize faces in images. I am just curious to know whether there are any other methods which are better than viola-jones for real time face detection(localization).,8,0,False,self,,,,,
393,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,16,33st87,twitch.tv,CMU's Claudico No Limit Holdem AI is playing heads up against top top poker pros. Streamed on Twitch,https://www.reddit.com/r/MachineLearning/comments/33st87/cmus_claudico_no_limit_holdem_ai_is_playing_heads/,MantisPrayingMantis,1429946816,,18,42,False,http://b.thumbs.redditmedia.com/ZWxK8lxcMCadPj3q5fOCyqU83v-v7RM82VFk4-faQBc.jpg,,,,,
394,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,19,33t2dd,blog.davidvassallo.me,First Steps in applying machine learning to InfoSec  WEKA,https://www.reddit.com/r/MachineLearning/comments/33t2dd/first_steps_in_applying_machine_learning_to/,J3diMindTricks,1429956399,,0,0,False,http://b.thumbs.redditmedia.com/WgzwS1zCA7owNB3z_GcK9vETUcrVYimBSnCn9_tZG_U.jpg,,,,,
395,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,19,33t53t,zx.rs,Markov Composer - Using machine learning and a Markov chain to compose music,https://www.reddit.com/r/MachineLearning/comments/33t53t/markov_composer_using_machine_learning_and_a/,galapag0,1429959371,,10,15,False,default,,,,,
396,MachineLearning,t5_2r3gv,2015-4-25,2015,4,25,20,33t62j,self.MachineLearning,Machine learning in computer networks,https://www.reddit.com/r/MachineLearning/comments/33t62j/machine_learning_in_computer_networks/,ash_rrr,1429960328,"Hello everyone, I'm an Undergraduate junior in computer science, I have to choose a project to work on in my final year. I was wondering if there are any projects/problems in Computer networks area where Machine learning/Data mining could be of use. If yes, just give a brief outline of the problem and its current state. Thank you!",0,0,False,self,,,,,
397,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,1,33u00b,self.MachineLearning,Comparing errors between regression and regression with a kernel,https://www.reddit.com/r/MachineLearning/comments/33u00b/comparing_errors_between_regression_and/,Linkazzz,1429980554,"Greetings, r/Machinelearning,

I'm working on something and I've a question regarding how to compare average squared errors on different models. I basically need to compare errors between two models: a plain vanilla regression and regression with a polynomial kernel. How do I go about doing that? 

If the models were, say, regression and a neural network, I'd just normalise the data before learning and compare the errors at face value, but should I do the same after applying the kernel (i.e. just get the weight vector and compute avg. squared error)? Does normalising the data and then normalising the data after the kernel make sense? Any advice is appreciated.",0,2,False,self,,,,,
398,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,2,33u41f,bbcomp.ini.rub.de,Black box optimization competition,https://www.reddit.com/r/MachineLearning/comments/33u41f/black_box_optimization_competition/,galapag0,1429982592,,17,34,False,http://b.thumbs.redditmedia.com/B85J0CJ1iyM2-O9duQ1S8hyMaJqG4MAJCG43eSh0zjc.jpg,,,,,
399,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,5,33upde,self.MachineLearning,Help with functions for Turing machine emulator,https://www.reddit.com/r/MachineLearning/comments/33upde/help_with_functions_for_turing_machine_emulator/,unwhi908,1429993470,"So I have to provide my program with some inputs like so: addTransition(currentState, currentBit, newState, newBit, direction).
currentBit is the character we are looking at in the string and direction is where the read/write head of the turing machine is supposed to move. For example, here is an implementation of x+y in turing machine.
 a.addTape(""1111101111110"");
		   a.addTransition(0,'1',0,'1',1);
		   a.addTransition(0,'0',1,'0',1);
		   a.addTransition(1,'0',3,'0',0);
		   a.addTransition(1,'1',2,'0',-1);
		   a.addTransition(2,'0',0,'1',1);
		   a.addTransition(2,'1',4,'1',0);
		   finalStates.add(3);
If the state reaches the finalState, the program halts. I was wondering if someone could help me with the functions for 2n and n^2",1,0,False,self,,,,,
400,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,5,33us7h,joyofdata.de,GPU Powered DeepLearning with NVIDIA DIGITS on EC2,https://www.reddit.com/r/MachineLearning/comments/33us7h/gpu_powered_deeplearning_with_nvidia_digits_on_ec2/,joyofdata,1429994924,,0,1,False,default,,,,,
401,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,5,33ussj,self.MachineLearning,Complexity behind the set of mathematical operators in our universe,https://www.reddit.com/r/MachineLearning/comments/33ussj/complexity_behind_the_set_of_mathematical/,ceasar_rsa,1429995221,"Hello! (and sorry if a similar question already has been asked)
In one of your speeches you talk about complexity and computation of pseudo-randomness (as an example you mentioned the decimal expension of pi which can be computed by a very short program).
When i look at such a number, i usually think ""Where does all this randomness come from?"" If we can calculate a complex series of number by a short program, doesn't this imply that the set of instruction we make use of in such a programm has a high inherent complexity? Of course, every instruction for its own appears simple, but the combination of these instructions create such effects like pi that look complex at the first glance.

In other words: Does this imply that the set of instructions used to run (for example) our universe already encoded all pseudo-randomness we observe?

Assumed, i want want to build an algorithm (and the required mathematics for this algorithm) from scratch to run a nice universe capable of generating intelligent entities, must i be aware of this?",0,0,False,self,,,,,
402,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,5,33uthy,github.com,A cooperative ML contest: predict review scores,https://www.reddit.com/r/MachineLearning/comments/33uthy/a_cooperative_ml_contest_predict_review_scores/,vikparuchuri,1429995594,,0,1,False,http://b.thumbs.redditmedia.com/t7iAZuawu7Y6Bivz2SLCSXMBZasqbALmqa3yo6vyvOo.jpg,,,,,
403,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,6,33uu9c,self.MachineLearning,Recurrent Decoder in Theano?,https://www.reddit.com/r/MachineLearning/comments/33uu9c/recurrent_decoder_in_theano/,wolet,1429996019,"Hello,

does anyone know any implementation of recurrent decoder in Theano?

[Groundhog](https://github.com/lisa-groundhog/GroundHog/) has it but it's supercomplicated. I plan to use it to generate text after training a Language Model.",2,4,False,self,,,,,
404,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,7,33v62w,self.MachineLearning,"Does scikit offer any classifier based on rule induction such as PRISM, CN2, MODLEM, etc?",https://www.reddit.com/r/MachineLearning/comments/33v62w/does_scikit_offer_any_classifier_based_on_rule/,Fireblend,1430002498,"I'm interested in using scikit (actually, astroML, but it uses scikit for the actual machine learning) to create various classifiers and compare their classification accuracy. I'm particularly interested in using a classifier based on rule-induction, rather than a decision tree or something like that. I'm wondering if there is one available on scikit that you guys might know of. I know Weka offers [MODLEM](http://list.waikato.ac.nz/pipermail/wekalist/2013-October/059309.html), for example.

If not, how feasible would it be to code one myself, as a contribution to Scikit? I'm a pretty experienced programmer, but I'm not that familiar with machine learning so I'd rather avoid it if I could. 

Thanks in advance!",1,0,False,self,,,,,
405,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,8,33va0i,self.MachineLearning,Radio stations audiences dataset,https://www.reddit.com/r/MachineLearning/comments/33va0i/radio_stations_audiences_dataset/,Xixiduro,1430004769,"I all.

I can't find any data about radios audiences.
I need the data to build a classifier to find usefull patterns for a project.

I already search but and i can't find anything! If you can help me or know a repository with that kind of stuffs i will be grateful.

Thanks",3,1,False,self,,,,,
406,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,10,33vkhw,self.MachineLearning,Proper way to do gradient clipping in recurrent networks?,https://www.reddit.com/r/MachineLearning/comments/33vkhw/proper_way_to_do_gradient_clipping_in_recurrent/,feedthecreed,1430010951,"I've seen a few implementations that seem to consistently calculate the norm of the gradient w.r.t to all parameters concatenated. 

Examples:

https://github.com/fchollet/keras/blob/master/keras/optimizers.py#L22

    norm = T.sqrt(sum([T.sum(g**2) for g in grads]))
https://github.com/kastnerkyle/minet/blob/master/minet/net.py#L245

    grad_norm = T.sqrt(sum(map(lambda x: T.sqr(x).sum(), gparams)))


This seems rather strange to me as it seems like you would want to calculate the norms of each parameter set (ex: recurrent weights, input weights, biases) separately.",14,7,False,self,,,,,
407,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,10,33vo2a,datascience.stackexchange.com,Gradient descent for word2vec skip gram with negative sampling,https://www.reddit.com/r/MachineLearning/comments/33vo2a/gradient_descent_for_word2vec_skip_gram_with/,sagarjp,1430013206,,2,0,False,http://b.thumbs.redditmedia.com/DgkI__UkgxwMY41b_9KM0m0RF-vPkIw6QKnpV7VAYkU.jpg,,,,,
408,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,19,33wqet,self.MachineLearning,What are some important problems in the field of machine learning?,https://www.reddit.com/r/MachineLearning/comments/33wqet/what_are_some_important_problems_in_the_field_of/,onewugtwowugs,1430044666,"Inspired by Richard Hamming in his talk [You and your research](http://www.cs.virginia.edu/~robins/YouAndYourResearch.html), where he talks about the necessity of recognizing what problems are important within your field of research:

""If you do not work on an important problem, it's unlikely you'll do important work. It's perfectly obvious. Great scientists have thought through, in a careful way, a number of important problems in their field, and they keep an eye on wondering how to attack them. Let me warn you, `important problem' must be phrased carefully. The three outstanding problems in physics, in a certain sense, were never worked on while I was at Bell Labs. By important I mean guaranteed a Nobel Prize and any sum of money you want to mention. We didn't work on (1) time travel, (2) teleportation, and (3) antigravity. They are not important problems because we do not have an attack. It's not the consequence that makes a problem important, it is that you have a reasonable attack. That is what makes a problem important. When I say that most scientists don't work on important problems, I mean it in that sense. The average scientist, so far as I can make out, spends almost all his time working on problems which he believes will not be important and he also doesn't believe that they will lead to important problems.

The whole thing is very much worth reading. Now, of course every researcher considers their work important enough for paying attention to, but there are arguably some problems that are more important than others. What is your take on this?",34,14,False,self,,,,,
409,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,19,33wqye,heurolabs.com,Fighting Malaria with Technology,https://www.reddit.com/r/MachineLearning/comments/33wqye/fighting_malaria_with_technology/,visof,1430045218,,1,6,False,http://a.thumbs.redditmedia.com/QdnJl7b3T4ldaFHm_jsuY7wFk47lekrNEuhaxOZ6660.jpg,,,,,
410,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,22,33x1r6,self.MachineLearning,What kind of machine learning could I implement for this particular problem?,https://www.reddit.com/r/MachineLearning/comments/33x1r6/what_kind_of_machine_learning_could_i_implement/,sir_fucks_a_lot,1430054885,"Hi all,

So for my final year project at school I've been tasked with building an Android app that tells users where their phone is in the house in the absence of GPS  based on the sensor readings the phone is giving out.

For example, if it's day time and the photo sensor is showing a very high reading your phone must be next to a window! If the pressure sensor is indicated you are at an elevated altitude then you must be upstairs. 

So you get the idea, using the sensors to give you a rough idea of where you phone is in the house.

Now the way to get the highest marks in this project will be to use some clever machine learning. My exposure to ML is very basic I did a module in my first year and well I haven't given it much thought since. 

My question to you folks is, what kind of machine learning should I be looking at implementing to pull something like this off? What sort of algorithms/techniques should I be looking at? Neural Networks? Regression? To create a program which can take in environmental variables and give the user an approximate idea of what part of the house their device is in.

Thanks
 ",16,16,False,self,,,,,
411,MachineLearning,t5_2r3gv,2015-4-26,2015,4,26,22,33x3mb,datasciencetech.institute,Data ScienceTech Institute Keynote Speakers will materialise Machine Learning applications!,https://www.reddit.com/r/MachineLearning/comments/33x3mb/data_sciencetech_institute_keynote_speakers_will/,datasciencetech,1430056260,,0,1,False,default,,,,,
412,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,2,33xqop,self.MachineLearning,What kind of machine learning could I implement for this object recognition problem?,https://www.reddit.com/r/MachineLearning/comments/33xqop/what_kind_of_machine_learning_could_i_implement/,banana_code,1430069214,"For my final project I need to implement a machine learning algorithm to identify one kind of vehicle in an image taken from the air, like this one: http://media.komonews.com/images/150324_sseattle_shooting_lg.jpg

The machine needs to count the vehicles or answer questions like ""Is there a red car?"" If yes ""Where is it?""

We get to choose one of the two problems.

I think the first one needs a Regression Machine Learning System and the second one  needs a Classification Machine Learning System, but i dont know which one is easier to implement for a begginer in machine learning.

What do you suggest?

And i would love to know what is the approach for this imagen understaing process.

Thanks!


",3,0,False,self,,,,,
413,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,7,33ytur,self.MachineLearning,How many ML types of algorithms exist? (I don't know if it is the right word),https://www.reddit.com/r/MachineLearning/comments/33ytur/how_many_ml_types_of_algorithms_exist_i_dont_know/,jhrs21,1430088482,"I know that exists clustering, classification, regression, etc... But which other exists and for what kind of problems are good?

Thanks!",2,0,False,self,,,,,
414,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,9,33z4pg,self.MachineLearning,"Product Data, Machine Learning, and me.",https://www.reddit.com/r/MachineLearning/comments/33z4pg/product_data_machine_learning_and_me/,Banality_Of_Seeking,1430094131,"I work in a field where I handle and have access to mass quantities of product data, ti/hi quantities descriptions, dimensional information, nutrient info, allergens, hazmat such as MSDS sheets, marketing such as images, webpages, pdf's so on and so forth..All part of the GDSN(Global Data Synchronization network) where suppliers communicate there information to distributors.

The Problem I have is where you are a huge distributor of products both soft and hard. is how to differentiate between the data set to group the items. I have developed a system to separate out some data, but was wondering if there could a simpler way to analyze the data into categories.

I have read about Bag of words, and think this might be the place to start for me, are there any really basic resources that explain this concept in detail, with examples?


",3,0,False,self,,,,,
415,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,12,33zo14,self.MachineLearning,Boosting a linear model?,https://www.reddit.com/r/MachineLearning/comments/33zo14/boosting_a_linear_model/,[deleted],1430104533,"Hi everyone,

I'm in an undergrad level machine learning class at the moment. Our text book says boosting is a method than can be applied to many statistical methods to improve performance, learn from the previous iteration, and start to fix problem areas over time. The concept of boosting is introduced in the random forests/trees chapter of the text, but I'm curious about another application.

Is it possible to boost a simple linear method or simple time series? If so, could you please help me understand what I'd need to learn to help make this happen or direct me to some resources? If not, could you please describe in what contexts boosting makes the most sense or why it wouldn't work for linear models?

Thanks in advance! I'm having a hard time finding resources online that are accessible to someone with a basic level of knowledge of machine learning",10,0,False,default,,,,,
416,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,12,33zpzk,self.MachineLearning,Database or time series more important for ML?,https://www.reddit.com/r/MachineLearning/comments/33zpzk/database_or_time_series_more_important_for_ml/,LaVieEnRoux,1430105686,"Long story short, I have to decide between taking a database management course and a time series course in my final undergrad year. In your own experience, which of these would be the most integral for going into a Master's in ML?",7,0,False,self,,,,,
417,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,14,340261,self.MachineLearning,Binary MNIST in literature,https://www.reddit.com/r/MachineLearning/comments/340261/binary_mnist_in_literature/,ciolaamotore,1430113640,"I'd like to find resources about a binarized version of MNIST, i. e.  one with image pixels being only black or white not gray scaled.  I could simply create one myself by applying a not informative threshold, however I'd like to know whether and how it has found a place in literature. Which are, for instance, the best performers on this version of the dataset?",5,0,False,self,,,,,
418,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,15,3407ou,self.MachineLearning,Simple classification problem-Need help,https://www.reddit.com/r/MachineLearning/comments/3407ou/simple_classification_problemneed_help/,feyn_struct,1430117985,"Hi, I need to classify around a million comments(text) into around 500 predetermined classes. I'm fairly new to this. I wanted to know which algo would be the best and fastest(not compromising too much on accuracy). Also, It would be great if you guys directed me toward a site which would help me do this.",9,0,False,self,,,,,
419,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,16,340auu,self.MachineLearning,Deep learning based Face detection tutorial?,https://www.reddit.com/r/MachineLearning/comments/340auu/deep_learning_based_face_detection_tutorial/,rishok,1430120810,"Hi guys

I was wondering if there exit a Deep learning based Face detection tutorial? Feeling inspired by the models of DeepFace and faceNet, i am trying to develop (webcam) face detector using convolutional neural networks (with alignment technique).

",9,13,False,self,,,,,
420,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,20,340ox3,stackoverflow.com,Need help with multiclass classification with growing number of classes,https://www.reddit.com/r/MachineLearning/comments/340ox3/need_help_with_multiclass_classification_with/,stiggpwnz,1430133920,,4,0,False,http://b.thumbs.redditmedia.com/IHFTGdD4Sly4xGviFH0i9qavZz9ELnmBmcahqZ6rTCE.jpg,,,,,
421,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,22,340y86,self.MachineLearning,Is it possible to incorporate word embedding features also in CRF base sequence labeling task ?,https://www.reddit.com/r/MachineLearning/comments/340y86/is_it_possible_to_incorporate_word_embedding/,sunilcsit,1430140117,,3,1,False,self,,,,,
422,MachineLearning,t5_2r3gv,2015-4-27,2015,4,27,23,3418k6,self.MachineLearning,Classification of high-dimensional data with many classes,https://www.reddit.com/r/MachineLearning/comments/3418k6/classification_of_highdimensional_data_with_many/,XalosXandrez,1430145621,"Hello all,

I was just wondering - what are the commonly used strategies for something like a 10k+ label class problem, where the data points have many (million) dimensions? 
K-nearest neighbour-like methods wont work because of the dimensionality. 
Training a 10k-way SVM will probably just fail.

Note: This is just to spawn an academic discussion about the problem. I am not really working on any such problem at the moment, but hope to pursue something like this in the near future.",4,1,False,self,,,,,
423,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,0,341ez6,self.MachineLearning,"Classification of Coral Reef Species. Handed cool project, but need some advice.",https://www.reddit.com/r/MachineLearning/comments/341ez6/classification_of_coral_reef_species_handed_cool/,Rackcityfoo,1430148624,"Hello!

I'm currently a Master's student that was handed an interesting machine learning/computer vision project. I was wondering if you guys could offer some guidance as I am relatively new to the machine learning/computer vision world. My machine learning skills go just about as deep as Andrew Ngs awesome class.

The end goal is to classify each coral species as seen in this album: http://imgur.com/a/fVEkg.

So far I have attempted a Tree Bagging Algorithm that does extremely well in that area, but does not generalize well to other parts of the reef.

The features that I feed the algorithm are:


1. Red Pixel Value (Passed through a median filter)
2. Green Pixel Value (Passed through a median filter)
3. Blue Pixel Value (Passed through a median filter)
4. Standard Deviation of the Red pixels in a 15x15 neighborhood
5. Standard Deviation of the Blue pixels in a 41x41 neighborhood
6. Standard Deviation of the Green pixels in a 61x61 neighborhood

The hope is that the STD filter will grab the texture of each of the different species. 

The next thing that I am starting to look into is convolutional neural networks. Any other suggestions or suggestions of things I should read up on? I also have time allocations on a super computer, so I can scale up as much as I need to.

Thanks for any help you could offer.

P.S. This subreddit is awesome. Ive been lurking here for a while and there is a lot of great content.
",11,3,False,self,,,,,
424,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,0,341hth,self.MachineLearning,Real numbers are a distraction and they are dangerously infectious. If you allow yourself to be infected by them you could end up with probabilities. -- Geoffrey Hinton,https://www.reddit.com/r/MachineLearning/comments/341hth/real_numbers_are_a_distraction_and_they_are/,glassackwards,1430149883,From Geoff's [slides posted last week](http://www.reddit.com/r/MachineLearning/comments/33kr7a/aetherial_symbols_a_seemingly_new_talk_by_geoff/) [Slide 2],9,7,False,self,,,,,
425,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,1,341kxn,youtube.com,Artificial Intelligence and Atari - Hill Climbing Algorithms with Fishing Derby,https://www.reddit.com/r/MachineLearning/comments/341kxn/artificial_intelligence_and_atari_hill_climbing/,rhiever,1430151234,,0,1,False,http://b.thumbs.redditmedia.com/kbORw0FRzCIL3rQs1-JAlY-TcC6K88CFFpOppPB9YKw.jpg,,,,,
426,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,1,341qk9,cloudacademy.com,Amazon Machine Learning: use cases and a real example in Python,https://www.reddit.com/r/MachineLearning/comments/341qk9/amazon_machine_learning_use_cases_and_a_real/,alexcasalboni,1430153728,,12,74,False,http://b.thumbs.redditmedia.com/GzOrg-om_Yg74liPuBcxZZNx0FOJ2mrSDKdmPpIyjYk.jpg,,,,,
427,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,2,341xjy,self.MachineLearning,Computer Vision study group?,https://www.reddit.com/r/MachineLearning/comments/341xjy/computer_vision_study_group/,matlab484,1430156685,"Does anyone want to make one together, like a Slack group? There's so many cool applications to build using computer vision, but its also hard to get very good accuracy with these. I've been working with Caffe http://caffe.berkeleyvision.org/ for half a year now and still run into things I don't know. It'd be cool to have a group that learns together and bounces ideas off eachother

Edit: Great! I'll pm everyone who wants to join about the slack channel",18,8,False,self,,,,,
428,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,3,3424kq,self.MachineLearning,What is MaxEnt (maximum entropy) species distribution modelling in everyday English?,https://www.reddit.com/r/MachineLearning/comments/3424kq/what_is_maxent_maximum_entropy_species/,RumbutterMcSquash,1430159677,I'm doing a species modeling project with African lions and can't quite get my head around exactly how the MaxEnt algorithm works. The studies by Phillips and even the statistical explanations by Elith go over my head and assume the reader is already quite advanced.  Is there a higher-level explanation (besides it creates a probability surface) that still captures how MaxEnt creates its output? Thank you.,2,4,False,self,,,,,
429,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,4,342912,self.MachineLearning,Four month summer break before starting an Applied Statistics master's. What should I be doing?,https://www.reddit.com/r/MachineLearning/comments/342912/four_month_summer_break_before_starting_an/,coffeecoffeecoffeee,1430161491,"I just got my BS in Math and Statistics from a decently-ranked public university.  Next year I'll be doing a one-year master's program at Carnegie Mellon.  I don't have any CS credentials but took some classes, and as a result I know Python, R, Matlab, SAS, SQL, and a decent amount of Java.  I'm not that experienced with OOP and don't really like Java, but I figure I just need to get more used to it.  With regard to math and stats I have deep theoretical knowledge but very little applied because of my university.  So I can explain a ton about mathematical statistics, probability, linear algebra, regression, etc., but have very little experience building models on real data.  I feel like this is the opposite of most people looking to get more involved in machine learning, who seem to have a lot of practical and programming experience but not nearly as much statistical theory.  My master's program will rectify this a lot because it's heavily applied.

I have virtually no projects to speak of but also want to get better at programming.  What should I be doing this summer if I eventually want a programmer/analyst ""data science"" role once I get my master's?  I was thinking about teaching myself data structures and algorithms in C++, or possibly functional programming (I want to learn some kind of Lisp for the hell of it).  I'm also curious about learning Hadoop or Spark but I don't know where I'd get the kind of computing power I'd need to actually learn them.

Right now I don't have anything lined up for the summer after I applied for a ton of internships and positions, but I might be doing paid research at Carnegie Mellon doing some heavy statistics simulations. (I've had one internship and it was only tangentially related to my field and I was literally the only statistics person there).  If that doesn't pan out, what kinds of things should I spend my summer working on?  Thank you!",8,0,False,self,,,,,
430,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,6,342rzv,datasciencecentral.com,How NoSQL Fundamentally Changed Machine Learning,https://www.reddit.com/r/MachineLearning/comments/342rzv/how_nosql_fundamentally_changed_machine_learning/,urinec,1430169600,,1,0,False,http://b.thumbs.redditmedia.com/rMovKQ25SEnqmsVALvER_A0SH4cCSbReC8e5dYa2R1I.jpg,,,,,
431,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,6,342tng,self.MachineLearning,"Can you mine for ""facts"" in texts?",https://www.reddit.com/r/MachineLearning/comments/342tng/can_you_mine_for_facts_in_texts/,ishi86,1430170265,"Let's say you are mining political articles. Can a machine extract only chunks of text to be used as quotes such as ""The Congress approved Obama's law"" or ""120 people died since 2013"" ?",7,1,False,self,,,,,
432,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,8,343daf,arxiv.org,On the Expressive Efficiency of Sum Product Networks,https://www.reddit.com/r/MachineLearning/comments/343daf/on_the_expressive_efficiency_of_sum_product/,downtownslim,1430179094,,0,2,False,default,,,,,
433,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,9,343hb5,self.MachineLearning,standard notation for neural nets?,https://www.reddit.com/r/MachineLearning/comments/343hb5/standard_notation_for_neural_nets/,brockl33,1430180965,"Just wondering what kind of notation everyone uses with ANNs. Is the notation that Andrew Ng uses most common?

http://ufldl.stanford.edu/wiki/index.php/Sparse_Autoencoder_Notation_Summary",3,0,False,self,,,,,
434,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,9,343ifv,self.MachineLearning,Where to go for answers?,https://www.reddit.com/r/MachineLearning/comments/343ifv/where_to_go_for_answers/,[deleted],1430181511,"My posts requesting best practices or improvements in specific scenarios have gone mostly without helpful answers.  Half the posted questions in this sub and other ML subs with no comments or no helpful comments.

What's next for answers?

- Hire freelancers but where can I find someone more competent than me who will freelance for like 3-10 hours and go into detail answering questions?
- Post to stackoverflow (same situation regarding paucity of answers)
- Just steam roll ahead with what I think is best..
",0,0,False,default,,,,,
435,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,11,343zv4,self.MachineLearning,Market segmentation in R/SQL/Python,https://www.reddit.com/r/MachineLearning/comments/343zv4/market_segmentation_in_rsqlpython/,starkiller1990,1430189920,"I need to try and do a segmentation analysis, identify clusters of customers who can be targetted for a energy company. Anyone have any experience with this in R or SQL or python? any links or insigts would be appreciated.
I'm assuming some form of clustert anylsis would be needed but are there other things to be done?",2,0,False,self,,,,,
436,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,13,344bgi,github.com,k-modes package for clustering categorical variables,https://www.reddit.com/r/MachineLearning/comments/344bgi/kmodes_package_for_clustering_categorical/,[deleted],1430196381,,0,1,False,default,,,,,
437,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,13,344cm4,github.com,k-modes package for clustering categorical variables,https://www.reddit.com/r/MachineLearning/comments/344cm4/kmodes_package_for_clustering_categorical/,[deleted],1430197101,,0,1,False,default,,,,,
438,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,14,344drq,github.com,k-modes clustering package for categorical clustering (Python),https://www.reddit.com/r/MachineLearning/comments/344drq/kmodes_clustering_package_for_categorical/,NYDreamer,1430197846,,0,4,False,http://b.thumbs.redditmedia.com/W_2hqSaqKRcGghqMK2snDdtFf6TZQMRT1x9QoKfdPuk.jpg,,,,,
439,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,15,344iqj,fastml.com,The emperor's new clothes: distributed machine learning,https://www.reddit.com/r/MachineLearning/comments/344iqj/the_emperors_new_clothes_distributed_machine/,iori42,1430201324,,4,4,False,http://b.thumbs.redditmedia.com/3Q7XMkmlNwYvbya5CKn1iHoQxWH7D1wmKNpJa3uuZ9s.jpg,,,,,
440,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,17,344ti6,nuit-blanche.blogspot.fr,ICML Workshop on Machine Learning Open Source Software 2015: Open Ecosystems,https://www.reddit.com/r/MachineLearning/comments/344ti6/icml_workshop_on_machine_learning_open_source/,compsens,1430210446,,0,3,False,http://b.thumbs.redditmedia.com/hHnJs-swuz4yoD3eRPqP7kXrpy7Gl0hyYNljLfqA5uk.jpg,,,,,
441,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,18,344xn9,self.MachineLearning,Ideas for machine learning algorithm for balancing in a 2d-environment,https://www.reddit.com/r/MachineLearning/comments/344xn9/ideas_for_machine_learning_algorithm_for/,yolandasquatpump,1430214238,"Hey all!

Me and my group at uni are working on developing a Java program where a simple body is supposed to teach itself how to balance using machine learning. The end goal is hopefully to be able to do something similar to this: http://rednuht.org/genetic_walkers/

We already have a pretty functional demo for physics simulation using dyn4j and are working on tidying up the code. The next step is choosing an approach and algorithm to use.

The ideas is that the balancer will receive a reinforcement depending on how succesful it is at balancing. It is able to act by applying an impulse to the left or to the right. Were not really sure where to go from here.. Do any of you have any ideas on which approach to use.. maybe bayesian network or hidden markov models?

Thank you,
Freddy",5,2,False,self,,,,,
442,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,20,3454i6,github.com,Our Python deep neural network library is closer to release: pylearn2 wrapper compatible with scikit-learn. Feedback welcome!,https://www.reddit.com/r/MachineLearning/comments/3454i6/our_python_deep_neural_network_library_is_closer/,alexjc,1430220115,,24,158,False,http://b.thumbs.redditmedia.com/W7KkYliwV2ceFpO3FzdVLDG0aphk0zio8I8LtXAftwo.jpg,,,,,
443,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,21,345cy3,self.MachineLearning,Conditional Logit model,https://www.reddit.com/r/MachineLearning/comments/345cy3/conditional_logit_model/,tjorriemorrie,1430225735,"I'm trying to read up on models, and saw this:

""The conditional logit model has been the most widely used statistical classification model due to its ability to account for both independent variables measuring a horses potential and within-race competition.""

and then:

""However, lately new machine learning algorithms have been proposed, e.g. Lessmann et al. (2007) use the least-square support vector regression""

Can anyone elaborate on these models for situations like horse-racing and golf; are SVR now mostly used? I'm trying to look into models where your competition influence your probability and what is currently most popular now.",1,0,False,self,,,,,
444,MachineLearning,t5_2r3gv,2015-4-28,2015,4,28,22,345gmd,machinalis.com,Short story on scaling an #NLP problem without using a ton of hardware.,https://www.reddit.com/r/MachineLearning/comments/345gmd/short_story_on_scaling_an_nlp_problem_without/,copybin,1430227801,,0,1,False,http://b.thumbs.redditmedia.com/UIjtq3ofS0QJzZ3xWRBCgEcziAMkLyGxlEcdB8-VZ1A.jpg,,,,,
445,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,1,3460w4,kdnuggets.com,The Myth of Model Interpretability,https://www.reddit.com/r/MachineLearning/comments/3460w4/the_myth_of_model_interpretability/,iori42,1430237301,,6,7,False,http://b.thumbs.redditmedia.com/YoqbEdmYdvhQmuBXsB5vQJPuOKbxr-h0IdL6o2k4Mxc.jpg,,,,,
446,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,1,3463by,self.MachineLearning,Machine Learning With Functional Programming,https://www.reddit.com/r/MachineLearning/comments/3463by/machine_learning_with_functional_programming/,uncountableB,1430238358,"Hey guys! I just started going through CS 229 from Stanford, and am in the process of learning Haskell for my own enjoyment. I was just wondering if Haskell, or another functional programming language, is good for machine learning in general.

Anyone have experience with this? I always hear Python and R is the language to go to for this type of stuff, but something like Haskell, Scala, or F# appeals to me. Also, if anyone happens to use functional programming languages in industry, I'd also love to hear about that, too! Thanks!
",14,11,False,self,,,,,
447,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,4,346rf3,self.MachineLearning,"[R] Have a corpus of contracts, need to predict who the parties to the contract are",https://www.reddit.com/r/MachineLearning/comments/346rf3/r_have_a_corpus_of_contracts_need_to_predict_who/,[deleted],1430248688,"I have the agreements in html format in a folder on my hard drive. They have random file names, but the parties to the contract are generally listed on the first couple lines or so. The parties are usually written like ""An agreement between PartyA and PartyB,"" but occasionally PartyA and PartyB are switched and occasionally the parties are listed further down in the document.

I would like to get R to predict who the parties to the contract are, but I'm not sure about how to do this. I'm fairly familiar with R and programming, but very new to NLP. What should I be looking for here? Is there a concept I can Google or something?",0,0,False,default,,,,,
448,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,4,346t8x,arxiv.org,[1412.2302] Theano-based Large-Scale Visual Recognition with Multiple GPUs,https://www.reddit.com/r/MachineLearning/comments/346t8x/14122302_theanobased_largescale_visual/,[deleted],1430249464,,7,5,False,default,,,,,
449,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,5,3472pu,randalolson.com,Artificial Intelligence has crushed all human records in 2048. Heres how the AI pulled it off.,https://www.reddit.com/r/MachineLearning/comments/3472pu/artificial_intelligence_has_crushed_all_human/,[deleted],1430253475,,7,0,False,default,,,,,
450,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,6,3478m1,self.MachineLearning,[Discussion] Theano vs Torch7,https://www.reddit.com/r/MachineLearning/comments/3478m1/discussion_theano_vs_torch7/,[deleted],1430255992,"Few people have extensive experience with both Theano and Torch7, and even fewer bothered to share their opinion on the relative merits of the two frameworks, but /u/nicholas-leonard wrote [this on Google+](https://plus.google.com/+YannLeCunPhD/posts/iJV2tJgpF16) :

&gt; Hi I am from the LISA Lab. I used pylearn2/Theano for a year before switching to Torch7 nine months ago. So why Torch7 vs Theano?
&gt;
&gt; Torch7 has lots of documentation. And for neural networks, it has much more than Theano (see https://github.com/torch/nn). If you need to optimize a non-standard piece of code (which is a commonplace requirement for research), then Torch7 is for you. It makes it much easier to code components in C/Cuda.
&gt;
&gt; In Theano you have to interface with python's complicated C API hidden in the strings of a huge compiler. Theano is a C/CUDA compiler, which makes it suitable for optimizing a computation graph and performing automatic gradient differentiation. Torch7 on the other hand is not a compiler, so you do not need to think symbolically. This also means that you can code complex computation graphs without needing to wait 5 minutes for it to compile (imagine debugging).
&gt;
&gt; Pylearn2 adds stuff to Theano like ready-to-use datasets, higher-level models and unsupervised learning. However, using Pylearn2 isn't easy. Its very different from the mainstream programming you would have learned in school. You always have to think symbolically, but then there are tons of exceptions, which further complicate things. For the year that I used Pylearn2, I loved and got to know it very well. But I eventually got tired of wrestling with the constant changes to the master branch, spending hours going around the code to find how to implement what seemed like a simple extension for my research.
&gt;
&gt; I wanted to get back to non-symbolic programming.  It is much easier to navigate and understand. So I switched to Torch7. I haven't stopped contributing to Torch7 ever since. Even working on my own Pylearn2-like alternative: https://github.com/nicholas-leonard/dp (pylearn2 had some good ideas, like datasets, early-stopping, plugins, which Torch7 doesn't provide out of the box), and some experimental CUDA extensions: https://github.com/nicholas-leonard/cunnx.

What do you think? (Upvote, if you want to see an active discussion -- I earn no karma, as it's a self-post)",48,31,False,self,,,,,
451,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,6,347an4,code.madbits.com,Machine Learning with Torch7 (A set of 6 tutorials),https://www.reddit.com/r/MachineLearning/comments/347an4/machine_learning_with_torch7_a_set_of_6_tutorials/,[deleted],1430256861,,1,1,False,default,,,,,
452,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,10,3482jx,self.MachineLearning,"I'm an extremely disciplined 18 year old who will be going to college this fall to get a double major in computer science and statistics and later a masters in ML. I want to make &gt;$500,000 per year after 10 years of working; what would be the best way of doing this?",https://www.reddit.com/r/MachineLearning/comments/3482jx/im_an_extremely_disciplined_18_year_old_who_will/,[deleted],1430270248,"By what can I do, I mean what field should I go into and what specific things should I study/do while in college. I say 500k because I feel as if an amount this high would satisfy me and allow me to live comfortably while saving a lot of money and investing a lot. 

I also really like stock trading (I have 8,000 saved and in the stock market and have been investing for 4 years), would something like Wall Street be a good place to work at given my goal and the fact that being very busy is okay with me?

Edit: The reason why I say 10 years is because I believe that if one sets firm goals and constantly works towards them, they are likely to be achieved",12,0,False,default,,,,,
453,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,10,3483bp,venturefizz.com,Cortex: Artificial Intelligence for Social Media Marketing Delivered by Boston Startup | VentureFizz,https://www.reddit.com/r/MachineLearning/comments/3483bp/cortex_artificial_intelligence_for_social_media/,venturefizz,1430270605,,1,1,False,default,,,,,
454,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,10,3485kx,self.MachineLearning,"Help Me, I'm an Idiot",https://www.reddit.com/r/MachineLearning/comments/3485kx/help_me_im_an_idiot/,IdiotDataScientist,1430271636,"I quit my job today as a data scientist (gave notice) because I felt I wasn't being challenged. I got a Masters in statistics a few years ago and I learned a lot of methods but didn't focus much on machine learning.

Now, it seems clear to me after spending several years working in the ""data"" field that I need to learn these things so I can remain relevant 5 years from now. 

Please, give me paths and resources to go after (I'm good paying for things, I wouldn't leave a job without feeling financially OK).

Thank you kindly",4,0,False,self,,,,,
455,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,12,348jc0,bits.blogs.nytimes.com,Less Noise but More Money in Data Science,https://www.reddit.com/r/MachineLearning/comments/348jc0/less_noise_but_more_money_in_data_science/,iidealized,1430278391,,0,1,False,default,,,,,
456,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,13,348o27,medium.com,Lets build a modern Hadoop,https://www.reddit.com/r/MachineLearning/comments/348o27/lets_build_a_modern_hadoop/,wanderlust516,1430281051,,8,16,False,http://b.thumbs.redditmedia.com/_cA8zp-XfMRSFUqENcxb2TdcLo96rep8Xqfp2mKmXkM.jpg,,,,,
457,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,13,348p4q,blog.ablelending.com,Able - Collaborative Small Business Lending,https://www.reddit.com/r/MachineLearning/comments/348p4q/able_collaborative_small_business_lending/,wanderlust516,1430281727,,1,0,False,http://b.thumbs.redditmedia.com/75lAZefnpH-rMiIaHIS4qNyQb3B-XKZI6W_sdcC5M9E.jpg,,,,,
458,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,13,348rpw,leanpub.com,R Programming for by Roger D. Peng [Leanpub PDF/iPad/Kindle],https://www.reddit.com/r/MachineLearning/comments/348rpw/r_programming_for_by_roger_d_peng_leanpub/,wanderlust516,1430283441,,4,4,False,http://b.thumbs.redditmedia.com/xj1Z72EZ05k3vEI2G28nLYmf53On6MD3H3fciGdX7_k.jpg,,,,,
459,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,15,3491fa,benfrederickson.com,Building a People Who Like This Also Like Feature,https://www.reddit.com/r/MachineLearning/comments/3491fa/building_a_people_who_like_this_also_like_feature/,wanderlust516,1430290587,,3,43,False,default,,,,,
460,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,16,34945m,intelligentjava.wordpress.com,Implementing decision tree with java 8,https://www.reddit.com/r/MachineLearning/comments/34945m/implementing_decision_tree_with_java_8/,keptavista,1430292886,,0,4,False,http://b.thumbs.redditmedia.com/cYqha0vUwTZX4NwVjlANkt44N2Y6tN3OHO0Dnnp3jPE.jpg,,,,,
461,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,17,3496fk,self.MachineLearning,Network datasets with numeric attributes,https://www.reddit.com/r/MachineLearning/comments/3496fk/network_datasets_with_numeric_attributes/,ciolaamotore,1430294956,"Are there some freely available network datasets (i.e. datasets representing networks of interactions like social nets, co-citation, etc, like DBLP, Us-Patents and so on) for which some non categoric information is attached to the nodes?

",2,1,False,self,,,,,
462,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,17,3496vm,profresearchreports.com,"Global and Chinese Light tower Industry, 2009-2019- Market Trends, Share, Size, Research Report, Trends and Forecast",https://www.reddit.com/r/MachineLearning/comments/3496vm/global_and_chinese_light_tower_industry_20092019/,profresearchreports,1430295374,,0,1,False,default,,,,,
463,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,19,349f6m,self.MachineLearning,Which are suitable machine learning techniques/models for (semi) automatic data entry?,https://www.reddit.com/r/MachineLearning/comments/349f6m/which_are_suitable_machine_learning/,dtosato,1430303208,,2,1,False,self,,,,,
464,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,19,349f6r,visual.cs.ucl.ac.uk,Using computers to teach humans visual classification tasks,https://www.reddit.com/r/MachineLearning/comments/349f6r/using_computers_to_teach_humans_visual/,macaodha,1430303210,,6,1,False,default,,,,,
465,MachineLearning,t5_2r3gv,2015-4-29,2015,4,29,23,34a1ge,radar.oreilly.com,More tools for managing and reproducing complex data projects,https://www.reddit.com/r/MachineLearning/comments/34a1ge/more_tools_for_managing_and_reproducing_complex/,gradientflow,1430317283,,0,0,False,http://b.thumbs.redditmedia.com/JikmiMOkEfOdIPCj3j5IO0OcNh2sAQXsm3XiOCJMzng.jpg,,,,,
466,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,1,34aieg,arxiv.org,Deep Karaoke: Extracting Vocals from Musical Mixtures Using a Convolutional Deep Neural Network,https://www.reddit.com/r/MachineLearning/comments/34aieg/deep_karaoke_extracting_vocals_from_musical/,iori42,1430325139,,18,14,False,default,,,,,
467,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,2,34apcl,mlconf.com,"MLconf Seattle is this Friday with talks from Quora, Microsoft, Facebook, Databricks, Netflix and more. We will be live streaming the event as well.",https://www.reddit.com/r/MachineLearning/comments/34apcl/mlconf_seattle_is_this_friday_with_talks_from/,shonburton,1430328119,,8,42,False,http://b.thumbs.redditmedia.com/9jo7XCsedhlQ8dotThvPfJ9FncMQW7GS3qKpw9QYAeo.jpg,,,,,
468,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,3,34b1kr,coursera.org,Recommender Systems | Coursera,https://www.reddit.com/r/MachineLearning/comments/34b1kr/recommender_systems_coursera/,wanderlust516,1430333375,,5,29,False,default,,,,,
469,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,4,34b4m2,bootphon.blogspot.fr,Deconstructing AI-Complete Question-Answering,https://www.reddit.com/r/MachineLearning/comments/34b4m2/deconstructing_aicomplete_questionanswering/,bunchoffruits,1430334697,,0,3,False,default,,,,,
470,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,7,34bxkc,github.com,FaceBook / iTorch,https://www.reddit.com/r/MachineLearning/comments/34bxkc/facebook_itorch/,[deleted],1430347148,,5,5,False,http://a.thumbs.redditmedia.com/9Msb6KH07bHjB9E0bnMmmLlQO0x3UGrL3e-D4wdBb00.jpg,,,,,
471,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,9,34cdwi,self.MachineLearning,K-Means/Medoid: Use original data or distance matrix?,https://www.reddit.com/r/MachineLearning/comments/34cdwi/kmeansmedoid_use_original_data_or_distance_matrix/,[deleted],1430354994,"I am a little confused because I see examples where people feed in original data to K medoid, whereas other people feed in the distance matrix that relates every data point with every other data point. When do you use which?",2,2,False,default,,,,,
472,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,14,34dbv5,nxn.se,Improper applications of PCA on multimodal data,https://www.reddit.com/r/MachineLearning/comments/34dbv5/improper_applications_of_pca_on_multimodal_data/,iori42,1430373573,,13,17,False,http://b.thumbs.redditmedia.com/DYCFQlMaa80Gmd7ixvJr1_yhFhUo306OzX_i7UPXDRQ.jpg,,,,,
473,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,18,34dq9w,tradegateway.com,Machine Tools Accessories,https://www.reddit.com/r/MachineLearning/comments/34dq9w/machine_tools_accessories/,Md-Malik,1430385225,,0,1,False,default,,,,,
474,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,19,34dufl,self.MachineLearning,ICML 2015 accepted papers?,https://www.reddit.com/r/MachineLearning/comments/34dufl/icml_2015_accepted_papers/,sidsig,1430388912,"Hi everyone, do you know if a list of accepted papers has been published? Really excited to have a look at what's been going on :)",4,22,False,self,,,,,
475,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,21,34e5ba,learningwithdata.wordpress.com,Tutorial on Logistic Regression and Optimization in Python,https://www.reddit.com/r/MachineLearning/comments/34e5ba/tutorial_on_logistic_regression_and_optimization/,syrios12,1430396831,,0,1,False,http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg,,,,,
476,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,22,34e9ib,self.MachineLearning,Ideas for contour-segmentation of leaves.,https://www.reddit.com/r/MachineLearning/comments/34e9ib/ideas_for_contoursegmentation_of_leaves/,Gosvig,1430399112,"I am doing a project on plant species recognition. The first step in this project is to segment the leaf from the image. All images are ""scan-likes"" meaning that the background is somewhat white and consistent.

At the moment I have tried using Otzu's method, and feeding that to OpenCV's find contour function. It is giving me mixed results and some of the errors can be seen here [Segmentation](http://imgur.com/a/M6PzM)
The errors come from shadows and the background being to dark at some places.

I have also tried thresholding the leaf-color but of course not all leaves have the same color, and therefore this method doesn't work.


Does anyone have some good ideas, or have tried something similar and found something to work??",8,2,False,self,,,,,
477,MachineLearning,t5_2r3gv,2015-4-30,2015,4,30,23,34elae,re-work.co,"'Deep Learning with Structure' - new Q&amp;A with Charlie Tang, U of Toronto, working under Geoff Hinton &amp; Ruslan Salakhutdinov",https://www.reddit.com/r/MachineLearning/comments/34elae/deep_learning_with_structure_new_qa_with_charlie/,reworksophie,1430404911,,0,1,False,http://b.thumbs.redditmedia.com/uXjuBdSLKKRQk49MvzcGOySWgTBWOmUp5x40bVnKvrY.jpg,,,,,
