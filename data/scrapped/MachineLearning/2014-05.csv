,date,year,month,day,hour,id,title,full_link,author,created,selftext,num_comments,score
0,2014-5-1,2014,5,1,10,24f1q5,Deep Learning Stanford tutorial forbidden,https://www.reddit.com/r/MachineLearning/comments/24f1q5/deep_learning_stanford_tutorial_forbidden/,[deleted],1398906086,"Hi, the unsupervised feature learning/deep learning tutorial at http://ufldl.stanford.edu is returning 403 Forbidden, would thank a fix or a url for a mirror.",3,0
1,2014-5-1,2014,5,1,13,24fhts,Topics of Machine Learning and Deep Learning in Finance,https://www.reddit.com/r/MachineLearning/comments/24fhts/topics_of_machine_learning_and_deep_learning_in/,jungletroll37,1398917054,"Hi guys, I'm studying Economics and Finance with a focus on statistics and finance. I recently took a Machine Learning course and found it really interesting, particularly, so now I'm thinking of writing my thesis on machine learning used in finance. 

Do any of you have any topic suggestions or papers that you can recommend?",7,3
2,2014-5-1,2014,5,1,14,24fn8j,Data Driven Energy Efficiency in Buildings,https://www.reddit.com/r/MachineLearning/comments/24fn8j/data_driven_energy_efficiency_in_buildings/,nipun_batra,1398921382,,0,0
3,2014-5-1,2014,5,1,15,24fs9u,Best,https://www.reddit.com/r/MachineLearning/comments/24fs9u/best/,[deleted],1398926511,,0,0
4,2014-5-1,2014,5,1,16,24ft6t,Recommended way to convert a probability distribution to a feature vector?,https://www.reddit.com/r/MachineLearning/comments/24ft6t/recommended_way_to_convert_a_probability/,[deleted],1398927618,"I splitted a number of short texts into its sentences and ran Stanford's CoreNLP sentiment analysis on each one. This discards the words and works with the POS tree only. As a result, I have a probability distribution of a given part-of-speech tree being in one out of five possible sentiment classes (from very negative to very positive). Now I want to build a feature vector for the texts.

My first approach was to add five elements to the feature vector for each unique tree in the output of the sentiment classifier, then just populate it with the values of the probability distributions. Does this make sense? Wouldn't I be somehow losing the representativity of the sentiment magnitude?

Thanks!",0,1
5,2014-5-1,2014,5,1,16,24fux6,Finding Maximum Dot (or Inner) Product,https://www.reddit.com/r/MachineLearning/comments/24fux6/finding_maximum_dot_or_inner_product/,shawntan,1398929827,,4,3
6,2014-5-1,2014,5,1,18,24g08n,A Neuroevolution Approach to General Atari Game Playing,https://www.reddit.com/r/MachineLearning/comments/24g08n/a_neuroevolution_approach_to_general_atari_game/,datumbox,1398937861,,0,5
7,2014-5-1,2014,5,1,19,24g241,Clortex Pre-Alpha Now Public,https://www.reddit.com/r/MachineLearning/comments/24g241/clortex_prealpha_now_public/,fergbyrne,1398940469,,3,0
8,2014-5-1,2014,5,1,20,24g4zp,Python for Finance,https://www.reddit.com/r/MachineLearning/comments/24g4zp/python_for_finance/,[deleted],1398944035,,1,0
9,2014-5-2,2014,5,2,0,24gn5i,Converting categorical data into numbers with Pandas and Scikit-learn,https://www.reddit.com/r/MachineLearning/comments/24gn5i/converting_categorical_data_into_numbers_with/,Foxtr0t,1398958407,,3,15
10,2014-5-2,2014,5,2,2,24gwiy,Library for out-of-the-box (Deep) Learning on Images?,https://www.reddit.com/r/MachineLearning/comments/24gwiy/library_for_outofthebox_deep_learning_on_images/,[deleted],1398964043,"I've got a set of images, each with an output label.

What libraries are out there that will let me load in my images and do classification? I mean libraries that automatically handle extracting features like color histogram, or SIFT/SURF, etc.?

Additionally, any libraries like this that do deep learning on images out of the box?

",7,2
11,2014-5-2,2014,5,2,7,24hruh,Couple of Simple questions about Artificial Neural Networks,https://www.reddit.com/r/MachineLearning/comments/24hruh/couple_of_simple_questions_about_artificial/,Korvv,1398982813,"I've been reading Negnevitsky's 'Artificial Intelligence - A guide to Intelligent Systems'. 

I'm trying to get my head around ANN's but one thing is thing is bothering me (I think lack of real-world examples is making it hard for me to understand).

* Is there an output node for **every possible outcome**? Does this have to be implemented into the NN **before training**?

* Unsupervised Learning is the means of modifying the weights of a NN without specifying the desired output for any input patterns. Can someone tell me a simple real world application of this? 

Thank you, I'd appreciate any help.",3,1
12,2014-5-2,2014,5,2,8,24hyv9,How big is the largest strongly connected component of the web?,https://www.reddit.com/r/MachineLearning/comments/24hyv9/how_big_is_the_largest_strongly_connected/,disconnectedLUT,1398987462,"I'm not sure if this is the right place to ask, so tell me if you think there is a better subreddit

According to this link: http://www9.org/w9cdrom/160/160.html the main SCC of the web was over 50M pages in 1999. Is there any source that tracks this currently? 

Would scaling 50M by some growth factor provide a reasonable estimate? I'm not sure if this is a valid thing to do on SCC because the number of interconnected websites is not necessarily proportional to the total amount of information on the web... ",2,5
13,2014-5-2,2014,5,2,8,24i0ne,"Researchers develop a computer application that generates gibberish that one of the major robo-graders, IntelliMetric, has consistently scored above the 90th percentile overall",https://www.reddit.com/r/MachineLearning/comments/24i0ne/researchers_develop_a_computer_application_that/,terremoto,1398988681,,3,36
14,2014-5-3,2014,5,3,0,24josn,What do Machine Learning Engineers for companies actually do?,https://www.reddit.com/r/MachineLearning/comments/24josn/what_do_machine_learning_engineers_for_companies/,agator2,1399043604,"ML algorithms come prepaged in R and Python. 

I am wondering what are the specific tasks, responsibilities and methods used for these positions. 

Specifics on the programming that is done would be great!",27,36
15,2014-5-3,2014,5,3,0,24js8b,Machine Learning Process - Aureus,https://www.reddit.com/r/MachineLearning/comments/24js8b/machine_learning_process_aureus/,hiteshpawar1429,1399045779,,1,0
16,2014-5-3,2014,5,3,2,24k0o3,Anyone have a tutorial link for building a basic classifier?,https://www.reddit.com/r/MachineLearning/comments/24k0o3/anyone_have_a_tutorial_link_for_building_a_basic/,icutyouwithmyknife,1399050996,"Hi,

I'm a newbie in machine learning field. I'm generally interested as how to get started by classifying text.

Edit: Let me give an example. Suppose i have a month's of openstack mailing list data. 

1. I want to be able to say with high confidence that a particular user is interested in 'X' component and what is the probability of this guy asking a question on the same question?

2. How do i detect a question in the message body? If someone asks a legitimate question in a mail thread, how do i detect it and label it as a question or an answer? This would be a classification problem, i suppose.

I know these are very hard questions which are unsolvable. But having a basic system in place to make the prediction even with 50% accuracy would be amazing.

I feel if i know how to solve these questions to an extent, that would be great because there's endless things one can achieve by having a basic knowledge of ML. 
",2,0
17,2014-5-3,2014,5,3,15,24luid,yahmm: Yet Another Hidden Markov Model (xpost from r/python),https://www.reddit.com/r/MachineLearning/comments/24luid/yahmm_yet_another_hidden_markov_model_xpost_from/,ants_rock,1399100068,"Hello all!

I'm happy to present Yet Another Hidden Markov Model (yahmm), a new package written by a friend and myself in Python, attempting to make the usage of HMMs easy, while also being comprehensive in scope and written in Cython for speed.

Features:

* Build your graph node by node and edge by edge, instead of using matrix format (but you can still use matrix format if you'd like!)
* States are not limited to a single distribution type. Not only can different states in the same model have different distributions, but a single state can be an arbitrary weighted mixture of any distributions you'd like, or just be silent.
* Normal, Exponential, Uniform Gamma, Inverse-Gamma, Discrete, and Lambda distributions implemented, as well as Gaussian, Uniform, and Triangular Kernel Densities (and of course, Mixtures), and a simple way to define your own distributions.
* Implements forward, backward, forward-backward, viterbi, all in O( states * edges ) time, instead of doing full-graph computations, for significant speedups.
* Both Baum-Welch and Viterbi training implemented, allowing the possibility of tied-states.
* Auto-normalization of edge weights to sum to 1
* Options to simplify the graph structure by merging silent states with a single probability 1 out edge
* Writing and reading of models to allow for time-intensive training followed by human-readable storage for future use
* Sampling from the model

Check out the github repo here: https://github.com/jmschrei/yahmm

Get it quickly with pip, just use ""pip install yahmm"".

Please check it out, and let us know what you think! Comments always appreciated.

Requirements:

* numpy &gt;= 1.8.0
* scipy &gt;= 0.13.3
* networkx &gt;= 1.8.1
* matplotlib &gt;= 1.3.1",8,17
18,2014-5-3,2014,5,3,20,24m670,Bayesian versus frequentist??,https://www.reddit.com/r/MachineLearning/comments/24m670/bayesian_versus_frequentist/,Lubbadubdub,1399117726,"For me, when the parameters are fixed (deterministic) but unknown, it is easy to interpret. 
However in a Bayesian approach, say p(x=1)=p_1 (unknown). The parameter p_1 is stochastic, which has a probability distribution p(p_1). Then is this p(p_1) deterministic??? And why???
Of course I accept the idea... I mean, somewhere you gotta make assumptions. But the idea of assuming something to be deterministic or stochastic (just because...) somehow bugs me... Anyone got the same feelings?    ",6,3
19,2014-5-3,2014,5,3,21,24m707,Stephen Hawking reckons that Artificial Intelligence could spell the end for humans (we're looking at you IBM/Apple/Google),https://www.reddit.com/r/MachineLearning/comments/24m707/stephen_hawking_reckons_that_artificial/,eyoop,1399118844,,0,0
20,2014-5-3,2014,5,3,21,24m7wp,Deep learning for NLP,https://www.reddit.com/r/MachineLearning/comments/24m7wp/deep_learning_for_nlp/,binge_learner,1399120040,"Hi everyone !
I am working right now on a project for applyind deep learning techniques to sentiment analysis, and text classification in general, do you guys have any links, book recommendations or videos that can help me get started ? 
I just started working for an NLP company as a Data Scientist btw, and my math and CS backgrounds are pretty strong ( MS in statistics + MS in optimization and OR ).
Thanks a lot !!!",19,26
21,2014-5-3,2014,5,3,22,24ma17,What are some literature/resource on extracting author name from a web page?,https://www.reddit.com/r/MachineLearning/comments/24ma17/what_are_some_literatureresource_on_extracting/,quantisan,1399122576,,3,0
22,2014-5-4,2014,5,4,1,24mnzk,Some questions I need answered about ML. Help me maybe?,https://www.reddit.com/r/MachineLearning/comments/24mnzk/some_questions_i_need_answered_about_ml_help_me/,[deleted],1399134626,"1. Is Bayes Classifier the best? why?
2. Is it enough to have conditional density fy|x to estimate regression func m?
3. No. of support vectors in LS-SVM if data is linearly separable?",4,0
23,2014-5-4,2014,5,4,6,24nbp2,Stranger in a strange land: a Big Data programmer meets the HPC community.,https://www.reddit.com/r/MachineLearning/comments/24nbp2/stranger_in_a_strange_land_a_big_data_programmer/,qkdhfjdjdhd,1399151599,,1,7
24,2014-5-4,2014,5,4,6,24nckh,Intuition for Simulated Annealing.,https://www.reddit.com/r/MachineLearning/comments/24nckh/intuition_for_simulated_annealing/,qkdhfjdjdhd,1399152215,,15,36
25,2014-5-4,2014,5,4,7,24nis6,"I understand the intuition behind the kernel trick, but still can't grasp why it doesn't need to be explicitly calculated. Could someone enlighten me?",https://www.reddit.com/r/MachineLearning/comments/24nis6/i_understand_the_intuition_behind_the_kernel/,iRoygbiv,1399156726,Thanks!,5,3
26,2014-5-4,2014,5,4,7,24njiw,"Calculating point distance to hyperplane, LibSVM?",https://www.reddit.com/r/MachineLearning/comments/24njiw/calculating_point_distance_to_hyperplane_libsvm/,[deleted],1399157294,"I wasn't sure if anyone had run into an instance when they needed to know the distance from a point to the hyperplane. In the FAQ for for LibSVM they write the following:

Q: How do I get the distance between a point and the hyperplane? 


The distance is |decision_value| / |w|. We have |w|^2 = w^Tw = alpha^T Q alpha = 2*(dual_obj + sum alpha_i). Thus in svm.cpp please find the place where we calculate the dual objective value (i.e., the subroutine Solve()) and add a statement to print w^Tw.

My issue is, I'm using MATLAB. And so I have no idea how to alter the C files and then recompile, or whatever, to use in MATLAB. Wasn't sure if anyone has overcome this issue...Thanks!
",0,0
27,2014-5-4,2014,5,4,14,24obq1,bayesian fantasy football,https://www.reddit.com/r/MachineLearning/comments/24obq1/bayesian_fantasy_football/,bayesff,1399181139,"http://www.bayesff.com/bayesian101/

http://www.bayesff.com/historical/

I created this site for last year's fantasy football season and wanted to pass it along, mostly in the hope that you guys find it interesting, but also to see if anyone might have an interest in working to help improve the model for next season.

I don't really have any formal background in this type of analysis (to be honest I'm not 100% certain this falls under the purvey of 'machine learning', though I think it does), but I was frustrated with my in-season rankings and this seemed like a natural way to think about the problem.

I did the analysis in R/JAGS (and the data processing in Stata), but am working now on learning Python in the off-season.

Again, my expertise is likely lacking compared to most people on this sub, but I have a bunch of data, code for my first cut at the problem, and am excited about working it more/open to collaboraters, so please feel free to get in touch.  I'm also very interested in coming up with tools to make the info more dynamic/easier for users to understand.  I'm working on learning how to code some of those tools up myself, but this is a fun side-project to a (non ML/developer oriented full-time job), so it's a bit slow going.

Cheers,

Nate",9,27
28,2014-5-5,2014,5,5,3,24pjln,"Literature on ""pure"" supervised feature learning?",https://www.reddit.com/r/MachineLearning/comments/24pjln/literature_on_pure_supervised_feature_learning/,dhammack,1399227677,"I'm interested in literature on supervised feature learning, specifically in an environment when labels are always available. I see that there are a lot of methods for semi-supervised feature learning, but I couldn't find anything for purely supervised feature learning. 

The only method I am aware of is to use the hidden layer activations from a neural net or svm trained to predict the targets from the data. Are there any other important methods out there that I'm missing? 

To me it seems like this is an under-appreciated problem; any nontrivial supervised learning task should involve supervised feature learning as a first step. ",7,7
29,2014-5-5,2014,5,5,11,24qss2,"what`s the meaning of primitive in ""gradient descent primitive""?",https://www.reddit.com/r/MachineLearning/comments/24qss2/whats_the_meaning_of_primitive_in_gradient/,phoenixbai,1399257934,"Hi all,

I am reading the doc of spark (http://spark.apache.org/docs/0.9.0/mllib-guide.html#gradient-descent-primitive). 
I am trying to translate the doc into Chinese, and there it talks about gradient descent primitive, and  but i am not quite sure what it mean by primitive?

I know gradient descent, but I am not sure what he is referring to by gradient descent primitive? does it mean the most basic form of gradient descent?

please enlighten me. thank you in advance.",4,7
30,2014-5-5,2014,5,5,12,24qx6p,How to regularize / stabilize neural network with sparse inputs?,https://www.reddit.com/r/MachineLearning/comments/24qx6p/how_to_regularize_stabilize_neural_network_with/,alexmlamb,1399261056,"Hello, 

Suppose we have the first layer of a neural network h = activation(W*x).  In general, I would regularize this part of the network by applying a small amount of dropout to x, constraining the row-norm of the weight matrix W, and applying a small amount of weight decay to W.  

However, I'm not sure that this is still the best strategy if x is very sparse.  Suppose in the most extreme case that x is a single categorical variables with tens of thousands of possible values.  Also it is a practical necessity that the runtime of the training algorithm is proportional to the number of non-zero elements in x rather than the total size of x.  

1.  Dropout on the inputs.  This is a sparse operation if implemented correctly.  However, I am somewhat concerned that it will be too strong and noisy as a regularizer for sparse categorical features (since by definition the values of a sparse categorical feature are not positively correlated, whereas features like adjacent pixels in an image are highly positively correlated).  

2.  Weight constraints.  Neither Computing the row-norm of the weight matrix nor dividing the weights by the norm are sparse operations.  Also, there are a lot of researchers who work on sparse linear models, and I've never seen any of them use weight constraints.  

3.  Weight decay.  If one adds an L1/L2 penalty to the cost term, then the gradients and the update are not sparse.  Alternatively one could have a penalty which only applies when the corresponding x term is non-zero.  

Any ideas / experience on what sorts of methods work well?  ",5,5
31,2014-5-5,2014,5,5,15,24r74e,What's going on with Hebbian learning these days?,https://www.reddit.com/r/MachineLearning/comments/24r74e/whats_going_on_with_hebbian_learning_these_days/,T_hank,1399269613,"I'm not very familiar with the ins and outs of hebbian learning, but it seems it was pretty important in the earlier days of neural networks, Oja showed it could be used to get PCA from linear networks.
Is it pretty much excluded from researcher's menu's these days, or have there been some newer exciting developments with respect to it?",11,11
32,2014-5-5,2014,5,5,15,24r8vp,Screen Printing,https://www.reddit.com/r/MachineLearning/comments/24r8vp/screen_printing/,aplmachineryindia,1399271520,,0,0
33,2014-5-5,2014,5,5,16,24rby9,Kato Cranes - PDF,https://www.reddit.com/r/MachineLearning/comments/24rby9/kato_cranes_pdf/,jessicperson,1399275251,,0,1
34,2014-5-5,2014,5,5,19,24rk90,Buy screen printing machines,https://www.reddit.com/r/MachineLearning/comments/24rk90/buy_screen_printing_machines/,aplmachineryindia,1399286510,,0,0
35,2014-5-5,2014,5,5,20,24rl2z,Overview of Cluster Analysis and Dirichlet Process Mixture Models,https://www.reddit.com/r/MachineLearning/comments/24rl2z/overview_of_cluster_analysis_and_dirichlet/,datumbox,1399287620,,2,24
36,2014-5-5,2014,5,5,22,24rtg9,What's the deal with integer-valued data?,https://www.reddit.com/r/MachineLearning/comments/24rtg9/whats_the_deal_with_integervalued_data/,[deleted],1399296024,"This question concerns models that accept feature vectors which are purely integers: in what ways do e.g. count vectors screw up models which are traditionally defined on the reals? 

[Collins et. al. 2002](https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/Readings/collins02generalization.pdf) generalized Principal Component Analysis to non-Gaussian noise models. In the article they go on to briefly state that ""[t]he Poisson is better suited to integer data, and the Bernoulli to binary data"", but nowhere have I found a deeper explanation for this. 
 
It sounds perfectly reasonable, but I'm having trouble getting what this has to do with PCA since it uses a least squares loss function, which in my mind minimizes the reconstruction error regardless of the domain of your input. 
 
Any insights would be greatly appreciated! ",2,0
37,2014-5-6,2014,5,6,1,24s9yx,LSTM derivs for backprop through time,https://www.reddit.com/r/MachineLearning/comments/24s9yx/lstm_derivs_for_backprop_through_time/,purpleladydragons,1399307042,"Hi guys, I'm struggling with implementing a long short-term memory network. I have the forward pass done, but I'm having trouble deriving the activation functions in order to get the error terms because I suck at math. The original LSTM paper uses a combination of truncated BPTT and RTRL but the paper I'm trying to follow claims to use BPTT only (sidenote: does calculating the full gradient imply not updating the weights at every timestep?). If someone could walk me through how to calculate the derivative of the cell I'd greatly appreciate it.

TLDR: How do I calculate a LSTM cell's derivatives? ",2,1
38,2014-5-6,2014,5,6,1,24sc5n,Data Mining vs. Machine Learning?,https://www.reddit.com/r/MachineLearning/comments/24sc5n/data_mining_vs_machine_learning/,Caesarr,1399308364,"Hi guys,

I'm starting a PhD in Data Mining, and have mostly been equating it with Machine Learning so far until I found this quote by Kevin Murphy:

&gt; Such models often have better predictive accuracy than association rules, although they may be less interpretible. This is typical of the difference between data mining and machine learning: in data mining, there is more emphasis on interpretible models, whereas in machine learning, there is more emphasis on accurate models.

But do you guys see this difference in practice (particularly in academia)? Do people use measures of interestingness rather than straight prediction accuracy? Is time and space complexity less of a concern? Maybe data mining research focuses less on ""Big Data"" and uses more ""medium data""? Do people really ""data mine"" images or text data, or is it mostly just standard databases?

Does DM have much of a presence in ML conferences? I know about ICDM, but what about others?

Basically I'm just after **any general impressions people might have about the academic difference between DM and ML** :)

Grasping the big picture of my research area seems pretty elusive...

Sorry about the ramble,

Thanks guys!",11,4
39,2014-5-6,2014,5,6,6,24t628,Profiting from Machine Learning in the NBA Draft,https://www.reddit.com/r/MachineLearning/comments/24t628/profiting_from_machine_learning_in_the_nba_draft/,cavedave,1399325854,,4,11
40,2014-5-6,2014,5,6,8,24tj56,The Eects of Backtest Overtting on Out-of-Sample Performance.,https://www.reddit.com/r/MachineLearning/comments/24tj56/the_eects_of_backtest_overtting_on_outofsample/,qkdhfjdjdhd,1399334008,,0,2
41,2014-5-6,2014,5,6,10,24tutw,Julia-based MCMC model predicts the Eurovision Song Contest,https://www.reddit.com/r/MachineLearning/comments/24tutw/juliabased_mcmc_model_predicts_the_eurovision/,mewo2,1399341498,,0,8
42,2014-5-6,2014,5,6,11,24tw75,Outlier Detection in Time Series Signals,https://www.reddit.com/r/MachineLearning/comments/24tw75/outlier_detection_in_time_series_signals/,bugra,1399342323,,4,18
43,2014-5-6,2014,5,6,13,24u7up,Yann LeCun will be doing an AMA in /r/MachineLearning on May 15 4PM EST,https://www.reddit.com/r/MachineLearning/comments/24u7up/yann_lecun_will_be_doing_an_ama_in/,olaf_nij,1399350728,"I'm happy to announce Director of AI Research at Facebook/NYU Professor Yann LeCun will be stopping by /r/MachineLearning on May 15 4:00-6:00 PM EST for an AMA.

Based on the success of the last AMA, a thread will be created before the official AMA time for those who won't be able to attend.",5,150
44,2014-5-6,2014,5,6,16,24uh8y,ISSUU - Yanmar Excavators by Machines4u,https://www.reddit.com/r/MachineLearning/comments/24uh8y/issuu_yanmar_excavators_by_machines4u/,jessicperson,1399359998,,0,1
45,2014-5-6,2014,5,6,23,24v3rw,Are there standard libraries for motion classification from wearables data,https://www.reddit.com/r/MachineLearning/comments/24v3rw/are_there_standard_libraries_for_motion/,abhijaydatta,1399384840,"I am a novice in the sensors space and doing some procurement activities for my firm. Does anyone know whether there are research organizations, academia, etc who are building standard libraries that can correctly classify ""motion types"" from streaming sensor data generated from wearables tech.",7,1
46,2014-5-7,2014,5,7,0,24v9wp,What are good ways to find articles in statistics or machine learning on a given topic?,https://www.reddit.com/r/MachineLearning/comments/24v9wp/what_are_good_ways_to_find_articles_in_statistics/,Kodiologist,1399388914,"In my home field, psychology, one typically searches PsycINFO, and perhaps also MEDLINE and other miscellaneous social-science databases (e.g., IDEAS for economics). What are good abstracts databases for statistics and machine learning (my loyalties between the two are pretty much split, although I'm leaning towards the latter currently)? Does one typically search the ACM Digital Library and IEEE Xplore separately, or are there larger databases that include both? Is Google Scholar sufficiently comprehensive for these fields to be useful? Are statistics journals archived on JSTOR also indexed elsewhere?",5,3
47,2014-5-7,2014,5,7,0,24vdis,Cost-sensitive ensemble of binary classifiers,https://www.reddit.com/r/MachineLearning/comments/24vdis/costsensitive_ensemble_of_binary_classifiers/,De_Lille_D2,1399391173,"(Note: I have posted [here](http://www.reddit.com/r/AskStatistics/comments/23o9k2/classifier_aggragator_using_costs/) and [here](http://www.reddit.com/r/AskStatistics/comments/24ujwn/ensemble_of_classifiers_with_costs/) before, but I'm still looking for more information).

I'm looking for an supervised learning ensemble algorithm that can combine individual classifiers, when there is a cost associated with each classifier. Ideally, this algorithm should produce a decision tree of classifiers (nodes are classifiers and each output corresponds to an edge), so that accuracy and cost are balanced.

Specifically, my goal is to classify Android applications as malware or not (2 classes). For this, I have several malware detection methods (classifiers) that each give a 1 or 0 output. Each method has an execution time cost.
I want to organize these methods into a decision tree, so that there is a trade-of between accuracy of classification and total cost.

Do you know of any algorithms that can do this? Where can I find more information on this sort of thing? I doubt this type of ensemble does not yet exist, because medical tests face the same problem (balancing classification accuracy and costs).

[Here](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2004_LingYWZ04.pdf)'s an example that is pretty close to what I need.

EDIT - see my last comment for the papers I found.",20,1
48,2014-5-7,2014,5,7,6,24wbw4,We're hiring an ML/NLP fanatic,https://www.reddit.com/r/MachineLearning/comments/24wbw4/were_hiring_an_mlnlp_fanatic/,gridspaceinc,1399411380,"I'm from [Gridspace](http://www.gridspace.com), and we're looking to hire someone for the below position, ideally with some background in modern machine learning methods, statistics, or natural language processing. You should also be a strong programmer.

*Requirements*

 * B.S./M.S./PhD in Computer Science or Electrical Engineering
 * Demonstrated ability to deliver in startup environment
 
*Preferred Skills*

 * Natural Language Processing
 * iOS/Andriod Development
 * Digital Signal Processing
 * Web Development
 * Machine Learning
 * Rapid Prototyping
 * PCB Design

If you're interested, send me an orange-red or email okay@gridspace.com.

Thanks guys, and let me know if you have any questions about the position or the company.",15,0
49,2014-5-7,2014,5,7,8,24wpej,Training set selection when you have near infinite examples,https://www.reddit.com/r/MachineLearning/comments/24wpej/training_set_selection_when_you_have_near/,is4junk,1399419939,"If you are working with unlimited data sets you could use for training, is there some research on what examples to choose to optimize learning?
For example, lets say you grab 10k random examples and train it - would it be worthwhile to look at examples it did poorly on in the CV set and try to find examples similar to those in your unlimited data set for another round of training?  ",15,5
50,2014-5-7,2014,5,7,10,24x0le,Sentiment Analysis Labeled Dataset?,https://www.reddit.com/r/MachineLearning/comments/24x0le/sentiment_analysis_labeled_dataset/,kristopolous,1399427512,"I'm looking specifically for ""rudeness"" or ""politeness"" labels.  There's many on positivity and negativity.  I'm willing to pay for it too.  Anyone know of any?",22,3
51,2014-5-7,2014,5,7,11,24x6c9,Can machine learning help me on my problem?,https://www.reddit.com/r/MachineLearning/comments/24x6c9/can_machine_learning_help_me_on_my_problem/,SrPeixinho,1399431352,"I have to write a function, `reduce`, that operates on trees and return trees. For example:

    reduce (B (P (F (P (B A) A)) (O (B (B A))))) = (B (P (B A) (B A)))
    reduce (F (P (O (B (B A))) (B A))) = (B A)
    reduce (P A (O (B (B (D A))))) = (P A A)
    reduce (App (Lam (B (Var 0))) (B (B A))) = (B (B (B A)))

And so on. To be specific, reduce is supposed to normalise a program in a language similar to the Simply Typed Lambda Calculus. [Here](http://o7.no/1s14yS1) is part of its definition, which is still incomplete / getting wrong results in some cases.

My problem is: getting the whole definition of ""reduce"" correctly is really tricky. There are too many variables, recursion points and things to track that it is very tricky for a human to do. But I can easily generate an infinite amount of correct input/output pairs (all I have to do is write a few test programs and what they should output!)

So, the question: is there any way Machine Learning can help me with this problem, somehow? Are there ways to throw a lot of computing power to find the correct implementation of `reduce`?",2,0
52,2014-5-7,2014,5,7,16,24xplr,Construction Equipment by Liebherr,https://www.reddit.com/r/MachineLearning/comments/24xplr/construction_equipment_by_liebherr/,jessicperson,1399448905,,0,1
53,2014-5-7,2014,5,7,18,24xu9h,Several Machines that Play Vital Role in Industrial Development,https://www.reddit.com/r/MachineLearning/comments/24xu9h/several_machines_that_play_vital_role_in/,marcosesteban1,1399455361,,0,1
54,2014-5-7,2014,5,7,20,24xz4d,Supervised sequence regression,https://www.reddit.com/r/MachineLearning/comments/24xz4d/supervised_sequence_regression/,zombiecalypse,1399461773,"Hi, I'm relatively new to machine learning and have been wondering on the following problem for a few weeks: 

What kind of algorithms can help me to find a estimator for

f: A^* --&gt; B^*

that is a regression for sequences, where we don't have labels for parts of the sequence and the sequences don't need to be equally big.

So the training set would look like this

* ""aaba"" -&gt; ""zyy""
* ""bab"" -&gt; ""yzzy""
* ...

Examples where this problem would pop up would be automatic translation, finding the pronunciation of an unknown word,  ....

Thanks! ",8,3
55,2014-5-8,2014,5,8,2,24ywyc,Does PCA preserve the order in data?,https://www.reddit.com/r/MachineLearning/comments/24ywyc/does_pca_preserve_the_order_in_data/,vmirjalily,1399485548,"So, I have a set of features, and want to perform PCA and use the first principal component for furthur analysis. Currently, all my features come from some physical energy functions, so the domain knowledge tells me that the lower energies are better (although there is so much noise in energy values). 

So, my question is whether or not after reducing the dimensions to 1st PCA, the data points still have the same physical meanings? 

I mean for example if data points that are lower than mean, correspond to the the same points lower than mean in the original set? My guess is that it probably depends on the sign of eigen-values, but I want to make sure",13,5
56,2014-5-8,2014,5,8,9,24zxqa,real time neural network bipeds for games?,https://www.reddit.com/r/MachineLearning/comments/24zxqa/real_time_neural_network_bipeds_for_games/,punkouter,1399507262,"Has anyone seen a resource for this sort of thing ? I think it will replace canned animation of bipeds for games in the future but I have trouble finding a central place where people talk about this.. just research papers and random youtube videos like

https://www.youtube.com/watch?v=rZlbl6itXSI",9,1
57,2014-5-8,2014,5,8,12,250eb5,Does your Big Data have Structure? Will it therefore have value?,https://www.reddit.com/r/MachineLearning/comments/250eb5/does_your_big_data_have_structure_will_it/,jmaccuish,1399518118,,0,0
58,2014-5-8,2014,5,8,14,250rd1,"Creating ""An Idiot's Guide to Deep Learning"" - any additions/suggestions to help non-data scientists?",https://www.reddit.com/r/MachineLearning/comments/250rd1/creating_an_idiots_guide_to_deep_learning_any/,rustyoldrake,1399528645,,30,44
59,2014-5-8,2014,5,8,18,2511u9,Need Help On Code: Linear Autoencoders to simulate PCA on 3D Gaussian (in Theano),https://www.reddit.com/r/MachineLearning/comments/2511u9/need_help_on_code_linear_autoencoders_to_simulate/,[deleted],1399541050,,1,0
60,2014-5-9,2014,5,9,3,2529qr,Released v0.02 of my GPU-accelerated deep-learning framework Hebel. Now with Windows support.,https://www.reddit.com/r/MachineLearning/comments/2529qr/released_v002_of_my_gpuaccelerated_deeplearning/,hannes_brt,1399574630,"Get Hebel from Github: https://github.com/hannes-brt/hebel

Hebel Changelog
===============

Version 0.02
------------

05-08-2014

* Windows compatibility (Thanks to @Wainberg)
* CUDA 4.x is no longer supported, please upgrade to CUDA 5 or CUDA 6
* All initialization is now handled through `hebel.init()`. No need to
  initialize PyCUDA separately anymore.
* `LogisticLayer` has been renamed to `SoftmaxLayer`. `LogisticLayer`
  now does binary classification while `SoftmaxLayer` is for
  multiclass classification.
* Framework for cross-validation.
* When `ProgressMonitor` has `save_interval=None`, then only the
  currently best model is serialized. If it is a positive integer,
  then regular snapshots of the model are stored with that frequency.

Version 0.01
------------

01-01-2014

* Removed dependency on scikits.cuda (this should make Hebel
  compatible with Windows, but I couldn't test that yet)

* Serious speed-ups by avoiding freeing and reallocating memory for
  temporary objects. Previously, many temporary gpuarrays were
  reallocated in every single minibatch and then discarded, which was
  very inefficient. By using persistent objects for temporary objects
  across minibatches and some other improvements such as doing more
  computations in-place, a roughly 2x speed-up could be realised.
",15,23
61,2014-5-9,2014,5,9,9,253676,Response to Hawking: AI Helps Us Understand a World We've Endangered,https://www.reddit.com/r/MachineLearning/comments/253676/response_to_hawking_ai_helps_us_understand_a/,vonnik,1399594742,,1,0
62,2014-5-9,2014,5,9,13,253t2v,A Mechanism for Layer 4 Sensorimotor Prediction in Hawkins' Cortical Learning Algorithm,https://www.reddit.com/r/MachineLearning/comments/253t2v/a_mechanism_for_layer_4_sensorimotor_prediction/,fergbyrne,1399611141,,0,0
63,2014-5-9,2014,5,9,18,254770,Benefits Of Volvo Hybrid Technology - machines4u's soup,https://www.reddit.com/r/MachineLearning/comments/254770/benefits_of_volvo_hybrid_technology_machines4us/,jessicperson,1399627735,,0,1
64,2014-5-9,2014,5,9,20,254c1g,"""Decoding"" weights W' in a linear autoencoder the transpose of the neuron weight matrix?",https://www.reddit.com/r/MachineLearning/comments/254c1g/decoding_weights_w_in_a_linear_autoencoder_the/,in_the_fresh,1399634269,"I'm reading Ng et al.'s paper on [Deep learning with COTS HPC systems](http://www.stanford.edu/~acoates/papers/CoatesHuvalWangWuNgCatanzaro_icml2013.pdf)
 and came across something I don't intuitively understand: when constructing a linear filter layer in a greedy fashion (i.e. via constructing autoencoders for each layer), they use the transpose of the linear layer's weight matrix, W', as the ""decoding matrix."" Check it out in section 3 of the paper, it is present in the first optimization problem they describe. 

I understand the theory behind autoencoders, but can anyone describe intuitively how they are able to get away with using W' instead of a different weight matrix? It seems to me that it constrains W'*W to be close to a multiple of the identity matrix, but I can't visualize what changes this soft constraint introduces into the learned parameters. Thanks in advance!",18,4
65,2014-5-9,2014,5,9,21,254i44,One Free* Machine Learning Specialist,https://www.reddit.com/r/MachineLearning/comments/254i44/one_free_machine_learning_specialist/,[deleted],1399640341,,5,0
66,2014-5-10,2014,5,10,1,2551lp,Help us crowdsource robot design: the crowd builds; the machines learn (x-post from /r/robotics),https://www.reddit.com/r/MachineLearning/comments/2551lp/help_us_crowdsource_robot_design_the_crowd_builds/,DrJosh,1399653670,,3,20
67,2014-5-10,2014,5,10,2,25570t,Machine learning\predictive analysis in enterprise scenarios,https://www.reddit.com/r/MachineLearning/comments/25570t/machine_learningpredictive_analysis_in_enterprise/,110011001100,1399656970,"Could you point me to any whitepapers\blogs\etc giving examples of machine learning and predictive analysis in large enterprise scenarios?


Reason I need it:  I work as a junior dev in IT for a company that provides global tech support and have been told to implement *something* (anything) where I can show the use of Machine learning. It can be absurd to any level,does not need to have business value, but should hit the right keywords...  Looking for some inspiration",2,2
68,2014-5-10,2014,5,10,3,255ass,Is this formulati on of sarsa lambda correct?,https://www.reddit.com/r/MachineLearning/comments/255ass/is_this_formulati_on_of_sarsa_lambda_correct/,Aumanidol,1399659322,"I'm implementing a sarsa lambda algorithm in matlab.

http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node77.html

I use the first equations to define the algorithm.

But reading the pseudo I notice that something is strange.

In the pseudo they do e(s,a)=e(s,a)+1 for the current state and current action, but then there's the for in wich they multiply each e(s,a) for gamma*lamda.

The problem is that the current couple shoul get gamma*lambda*e(s,a)+1 but doing that way we have instead of the 1 gamma*lambda.

Am I right? Do you have another pseudo for the sarsa lambda?
",3,0
69,2014-5-10,2014,5,10,5,255nmz,2014 Spring Hackathon Outcome,https://www.reddit.com/r/MachineLearning/comments/255nmz/2014_spring_hackathon_outcome/,[deleted],1399667288,,0,1
70,2014-5-10,2014,5,10,5,255npf,NuPIC 2014 Spring Hackathon Outcome,https://www.reddit.com/r/MachineLearning/comments/255npf/nupic_2014_spring_hackathon_outcome/,numenta,1399667343,,10,1
71,2014-5-10,2014,5,10,7,255y42,MNIST Power/Time metrics (or other popular benchmarks),https://www.reddit.com/r/MachineLearning/comments/255y42/mnist_powertime_metrics_or_other_popular/,010011000111,1399674128,"I have previously asked this forum about metrics other than classification performance. These metrics are usually unstated or incomplete in papers, and the opinion of some is that they don't matter much--an assessment I don't particularly agree with. I was wondering if anybody would be willing to state some numbers from their own experience. MNIST is an example, but any other popular benchmark would also work. For example:

training time, hardware specs including power consumption if possible,general algorithm or algorithm class, test error, test time, etc

Any feedback would be very much appreciated, even if its just a guesstimate. Thanks so much for any help.
",14,1
72,2014-5-10,2014,5,10,17,25715s,i'm so stupid!,https://www.reddit.com/r/MachineLearning/comments/25715s/im_so_stupid/,Chicksub,1399709066,"like really i'm fucking stupid. I have a great job but it doesn't need domain knowledge. Well not much. And once I meet people with technical expertise or in specialized fields, i remember how useless I am.

How do i get some knowledge on natural language processing like those PC programs that can answer questions. How do I start riding the Data wave. Is it a side hobby worth pursuing or will i jut reinforce my stupidity and never make it to anywhere meaningful?????

:D:D:D:D:D:D:D::D not spam. just hyper pissed.",10,0
73,2014-5-11,2014,5,11,5,258blo,Where does the prior p(y) come from when implementing Gaussian Discriminant Analysis?,https://www.reddit.com/r/MachineLearning/comments/258blo/where_does_the_prior_py_come_from_when/,axsauze,1399753381,"I've been trying to work out for a while what the value of the prior p(y;) should initially be when solving a Gaussian Discriminant Analysis problem. I am aware that this is ""the probability of y (the hypotheses) before we know the data"" parameterized by  - how would i go on to get the values for this?

To explain what I mean, I am aware that when we maximize [this log likelihood formula.](https://coursera-forum-screenshots.s3.amazonaws.com/8e/4b0680d87d11e394bccd9b85c50978/Screen-Shot-2014-05-10-at-8.59.05-PM.png)

we obtain the Maximum Likelihood parameters which are given by [these definitions.](https://coursera-forum-screenshots.s3.amazonaws.com/b7/ae4550d87d11e386ea71c201692ca6/Screen-Shot-2014-05-10-at-9.00.10-PM.png)

and with these parameters (mu and sigma), we can obtain p(x|y). However, my question is, what is the value for p(y;)? Is it just ? Or what formula is used to compute the probability of y parameterized by ? 

Thank you very much for your help :) Best wishes!



**Edit 1**: After some research and very good answers in several forums it was concluded that, as the name implies, the prior is drawn from any information that we have of the system that would allow us to make prior judgement. Several approaches we may take include:

* Initially give all priors the value of 1 (saying that all probabilities are as likely to happen)
* Use a Beta Distribution for the priors
* Use incremental updating where the posterior becomes the prior (i.e. http://imgur.com/pdNuYhY)",2,2
74,2014-5-11,2014,5,11,6,258j5j,I would like to develop an app that authenticates designer handbags. Suggestions?,https://www.reddit.com/r/MachineLearning/comments/258j5j/i_would_like_to_develop_an_app_that_authenticates/,jacksorob,1399758934,"I have minimal Machine Learning experience. I would just like to get an idea of where I should begin.
 The basic idea is that the user would take specific photos with their phone which would target a certain area. This might be the stitching, logo, monogram, serial numbers, etc. After running them against a database of other authentic handbags of similar models, a decision can be determined of either fake or real. Any help would be appreciated. Thanks.",3,0
75,2014-5-12,2014,5,12,1,25abde,Robust Regression and Outlier Detection via Gaussian Processes,https://www.reddit.com/r/MachineLearning/comments/25abde/robust_regression_and_outlier_detection_via/,bugra,1399824146,,12,12
76,2014-5-12,2014,5,12,4,25asig,Neural Network in VHDL (hardware),https://www.reddit.com/r/MachineLearning/comments/25asig/neural_network_in_vhdl_hardware/,albntomat0,1399836179,,10,29
77,2014-5-12,2014,5,12,4,25ausr,Illustration of principal component analysis (PCA),https://www.reddit.com/r/MachineLearning/comments/25ausr/illustration_of_principal_component_analysis_pca/,joyofdata_de,1399837629,,6,0
78,2014-5-12,2014,5,12,7,25bbb3,How much would you pay for preprocessed parallel text data for nlp research? (X-Post Linguistics),https://www.reddit.com/r/MachineLearning/comments/25bbb3/how_much_would_you_pay_for_preprocessed_parallel/,double_snap,1399849131,"I've been working on a personal project, and as a side effect I've been making a lot of parallel text corpuses in language pairs that are hard to find (Vietnamese/Portuguese, Yoruba/Japanese).

&gt; In many ways, progress in natural language research is driven by the availability of data. This is particularly true for the field of statistical machine translation, which thrives on the emergence of large quantities of parallel text: text paired with its translation into a second language.

[Europarl: A parallel Corpus for Statistical Machine Translation](http://mt-archive.info/MTS-2005-Koehn.pdf)

I know that all of this data has some value and I'm considering setting up a site to sell it instead of letting it just sit on my hard drive. The problem is that I don't really know how to price something like this. 

If you work in research that could use data like this, how much would your lab/company pay for a license?",19,0
79,2014-5-12,2014,5,12,13,25c4df,"Priors, beliefs and coin tosses: Animations, Interactive widgets",https://www.reddit.com/r/MachineLearning/comments/25c4df/priors_beliefs_and_coin_tosses_animations/,nipun_batra,1399870627,,0,3
80,2014-5-12,2014,5,12,16,25cdg7,Suggested John Deere Backhoes - Machinery &amp; Equipment Part Online,https://www.reddit.com/r/MachineLearning/comments/25cdg7/suggested_john_deere_backhoes_machinery_equipment/,jessicperson,1399880467,,0,1
81,2014-5-12,2014,5,12,19,25ck55,Finite Mixture Model based on Dirichlet Distribution,https://www.reddit.com/r/MachineLearning/comments/25ck55/finite_mixture_model_based_on_dirichlet/,datumbox,1399889499,,0,6
82,2014-5-12,2014,5,12,22,25cvjf,Top 10 Biggest Machines,https://www.reddit.com/r/MachineLearning/comments/25cvjf/top_10_biggest_machines/,bnagle1,1399901581,,1,1
83,2014-5-12,2014,5,12,22,25cxls,Online NLP with linear classifiers,https://www.reddit.com/r/MachineLearning/comments/25cxls/online_nlp_with_linear_classifiers/,afireohno,1399903114,,0,11
84,2014-5-13,2014,5,13,0,25d6ku,Finding linear classifier using Perceptron Learning,https://www.reddit.com/r/MachineLearning/comments/25d6ku/finding_linear_classifier_using_perceptron/,[deleted],1399909200,"Hey there. I want to write a matlab function to find an equation of a linear classifier for 2 separable sets of points. I have got 2 files:

**run.m - script that runs the function and plots the result**


    x_1 = [3, 3, 2, 4, 5];
    y_1 = [3, 4, 5, 2, 2];
    x_2 = [6, 7, 5, 9, 8];
    y_2 = [3, 3, 4, 2, 5];

    target_array = [0 0 0 0 0 1 1 1 1 1];

    [ func ] = classify_perceptron([x_1 x_2; y_1 y_2], target_array);
    x = -2:10;
    y = arrayfun(func, x);

    plot(x_1, y_1, 'o', x_2, y_2, 'X', x, y);
    axis([-2, 10, -2, 10]);


**classify_perceptron.m - m file with the function**


    function [ func ] = classify_perceptron( points, target )
        % points - matrix of x,y coordinates
        % target - array of expected results
        % func - function handler which appropriately classifies a point
        %        given by x, y arguments supplied to this function

        target_arr = target;
        weights = rand(1, 2);
        translation = rand();
    
        for i=1:size(points, 2)
            flag = true;
            while flag
                result = weights * points(:, i) + translation;
                y = result &gt; 0;
                e = target_arr(1, i) - y;
            
                if e ~= 0
                    weights = weights + (e * points(:, i))';
                    translation = translation + e;
                else
                    flag = false;
                end
            end
        end
    
        func = @(x)(-(translation + (weights(1, 1) * x)) / weights(1, 2));

        return
    end

The problem is that I don't know where I am making the mistake that leads to incorrect result. It looks like the slope of the line is right, however translation should be a bit bigger. I would be really thankful for pointing me in the right direction.",0,1
85,2014-5-13,2014,5,13,2,25dkaw,"if I have an exact model of the enviroment, can I say that the greedy policy is very close to the optimal and it should be better than TD methods?",https://www.reddit.com/r/MachineLearning/comments/25dkaw/if_i_have_an_exact_model_of_the_enviroment_can_i/,Aumanidol,1399917421,"with exact model I mean that I know exactly the revenues and the transiction probabilities between the states,wich are in a finite number.


with should be better I mean in an event of relatively short number of steps,a few thousands in my case.


I tried some simulations and I get as result that the TD methods are better only when the actual distribution of the event is different from the one I expected building the greedy. Is my time orizont too short (taking it to milions of iteration doesn't help) or a result like that is expected?


Reading about the n-armed bandit I figured out that maybe no matter how long my orizont could be the greedy would be better or just a bit worse than the Q learning and friends,am I right?

Thanks for your opinion and help",3,4
86,2014-5-13,2014,5,13,2,25dkgc,Mirroring Your Twitter Persona with Intelligence,https://www.reddit.com/r/MachineLearning/comments/25dkgc/mirroring_your_twitter_persona_with_intelligence/,primaryobjects,1399917518,,2,10
87,2014-5-13,2014,5,13,10,25evt3,Machine Learning A Cappella - Overfitting Thriller!,https://www.reddit.com/r/MachineLearning/comments/25evt3/machine_learning_a_cappella_overfitting_thriller/,cafebeen,1399945694,,22,79
88,2014-5-13,2014,5,13,16,25fj8o,The Curse of Dimensionality in classification,https://www.reddit.com/r/MachineLearning/comments/25fj8o/the_curse_of_dimensionality_in_classification/,esurior,1399964965,,5,12
89,2014-5-13,2014,5,13,19,25ftco,"WebGL Accelerated Neural Network + Source Code, White Paper &amp; Fiddle",https://www.reddit.com/r/MachineLearning/comments/25ftco/webgl_accelerated_neural_network_source_code/,[deleted],1399978147,,7,8
90,2014-5-13,2014,5,13,22,25g1po,"I am trying to learn basic AI, my stats is ~nil but I know algorithms. I took my first stab with a factor analysis algorithm, can anyone tell me if I am close or way off and/or how to make it better?",https://www.reddit.com/r/MachineLearning/comments/25g1po/i_am_trying_to_learn_basic_ai_my_stats_is_nil_but/,[deleted],1399986651,,2,0
91,2014-5-14,2014,5,14,4,25h4ok,SVM classification of MNIST digit dataset,https://www.reddit.com/r/MachineLearning/comments/25h4ok/svm_classification_of_mnist_digit_dataset/,ComplexColor,1400010673,"For fun, I decided to tackle the MNIST digit dataset. My first ideas involved KMean clustering for feature evaluation and SVM with RBF kernel for classification. Given the nature of the dataset - almost binary images of digits (very few shades of gray), I didn't bother with normalization - not knowing at the time, this will be a huge problem. 
After several failures trying to improve classification with hyper parameter optimization I figured something was wrong. SVM was going nowhere - it degenerated to a constant single class function, while simple linear classifier were doing much better. I decided to remove KMeans for the moment and focus on SVM. Only after normalizing the data (subtract the mean and divide with standard deviation of each feature), did I get meaningfull results.

This was a bit puzzling to me. As I understand it, SVM with RBF should at the worst of generalizations acts as convoluted a KNearestNeigbours algorithm. The data also had a very well defined range, mostly just pixel on - pixel off. How can normalization have such a drastic affect?

TLDR; why does the MNIST digit data have to be normalized to get meaningfull classification with SVM?


  ",4,2
92,2014-5-14,2014,5,14,9,25hvj7,"Ben Goertzel, a leading figure on artificial general intelligence, has written a very interesting and thoughtful review of pro-AGI novel ""The Transhumanist Wager"" (and it's philosophical controversies) in H+ Magazine",https://www.reddit.com/r/MachineLearning/comments/25hvj7/ben_goertzel_a_leading_figure_on_artificial/,IstvanZoltan,1400026946,,0,1
93,2014-5-14,2014,5,14,9,25hy75,Neuronal Synchrony in Complex-Valued Deep Networks,https://www.reddit.com/r/MachineLearning/comments/25hy75/neuronal_synchrony_in_complexvalued_deep_networks/,captcompile,1400028676,,0,9
94,2014-5-14,2014,5,14,13,25iioe,Question: How to recognize data in CSVs,https://www.reddit.com/r/MachineLearning/comments/25iioe/question_how_to_recognize_data_in_csvs/,macarthy,1400043189,,0,1
95,2014-5-14,2014,5,14,14,25ijdi,Question: How to recognize data in CSVs,https://www.reddit.com/r/MachineLearning/comments/25ijdi/question_how_to_recognize_data_in_csvs/,macarthy,1400043797,"I have thousands of CSVs that contain data from a specific industry. 

If I open the CSV in and look at it, I can usually tell what the data consists of, and I would like to build a system that can automatically recognize what the columns are.

For example I know a column with these values 

SB000588  
SB002309  
SB000522  

a type of document the sales use (SB followed by digits.)

In some columns I might have 

58-08-2

12002-61-8

27039-77-6

7785-23-1

7783-89-3


which are CAS numbers,  which can be recognized by the format, a checksum and also just by looking it up in a CAS DB 

Other column might have a string ""Complete"", ""In Complete"".
Basically I would like to figure out these columns and build a system that can learn new items.

I feel there would be several steps in the process, scoring the column as a certain type. 

I'm a ML newbie, so can anyone point me in the right direction? 

EDIT : to all those suggesting regex,  I guess I didn't explain that I would like to train the recognizer on data also, so a user can say this column is this sort of thing, and the system can make judgements about that sort of data in the future.  I'm looking at this work http://ashleygriffiths.co.uk/dataset-tagging/


Thanks ! 
",8,3
96,2014-5-14,2014,5,14,16,25iql3,Impact of Dimensionality on Data in Pictures,https://www.reddit.com/r/MachineLearning/comments/25iql3/impact_of_dimensionality_on_data_in_pictures/,joyofdata_de,1400051013,,2,19
97,2014-5-14,2014,5,14,16,25iqs9,Basic Parts Of Hydraulic Crane Trucks,https://www.reddit.com/r/MachineLearning/comments/25iqs9/basic_parts_of_hydraulic_crane_trucks/,jessicperson,1400051260,,0,1
98,2014-5-14,2014,5,14,16,25iqvp,"Portable Baggage Scanner,Parcel Security Inspection Equipment",https://www.reddit.com/r/MachineLearning/comments/25iqvp/portable_baggage_scannerparcel_security/,emily0214,1400051393,,0,0
99,2014-5-14,2014,5,14,20,25j1ja,"""Democratizing Data Assets: Learning From Data, Big and Small"" - An article by Kirk Borne on Unsupervised Machine Learning.",https://www.reddit.com/r/MachineLearning/comments/25j1ja/democratizing_data_assets_learning_from_data_big/,[deleted],1400065308,,0,7
100,2014-5-14,2014,5,14,22,25j9rn,"Slides for tonight's Paris Machine Learning Meetup #11: Learning What Is It Good For ? SPARFA, Learning to Interact, Action recognition with CNNs and Prediction APIs",https://www.reddit.com/r/MachineLearning/comments/25j9rn/slides_for_tonights_paris_machine_learning_meetup/,compsens,1400073487,,0,8
101,2014-5-15,2014,5,15,1,25jt4d,Which hyperparameters are important in classification trees?,https://www.reddit.com/r/MachineLearning/comments/25jt4d/which_hyperparameters_are_important_in/,goddammitbutters,1400086165,"I am trying to tune a CART classifier, and am overwhelmed at the hyperparameter optimization. There are just so many, and I don't know which ones to optimize. You have at least these parameters:

* maximum depth of tree
* minimum number of observations in a node to attempt another split
* minimum number of observations in any terminal leaf node
* minimum factor by which lack of fit has to be decreased to execute the next split

I don't think I can do all of them, this is  just too expensive on CPU time even for smaller grids.

Is there a consensus which hyperparameters to optimize in classification trees?",2,0
102,2014-5-15,2014,5,15,10,25l9ca,How to auto generate programming language models?,https://www.reddit.com/r/MachineLearning/comments/25l9ca/how_to_auto_generate_programming_language_models/,falcon_shark,1400117987,"Let's say I have 1000's of CSS files, javascript files, HTML files, JSON files e.t.c

Can machine learning help me auto-generate structure models of the files? The grammar is much simpler than english.

Can I use then use the models to parse abstract syntax trees, make predictions? or make an online code editor that runs an AI engine on the back that auto-fixes semicolon mistakes, suggests refactors, auto-formats the code.

How would I go about such a project without having to explicitly write a parser for every language?",3,2
103,2014-5-15,2014,5,15,13,25lnbt,AMA: Yann LeCun,https://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/,ylecun,1400127936,"My name is [Yann LeCun](http://en.wikipedia.org/wiki/Yann_LeCun). I am the Director of Facebook AI Research and a [professor at New York University](http://yann.lecun.com). 

Much of my research has been focused on deep learning, convolutional nets, and related topics.

I joined Facebook in December to build and lead a research organization focused on AI. Our goal is to make significant advances in AI. I have answered some questions about Facebook AI Research (FAIR) in several press articles: [Daily Beast](http://www.thedailybeast.com/articles/2013/12/17/facebook-s-robot-philosopher-king.html), [KDnuggets](http://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab.html), [Wired](http://www.wired.com/2013/12/facebook-yann-lecun-qa/).

Until I joined Facebook, I was the founding director of NYU's [Center for Data Science](http://cds.nyu.edu).

I will be answering questions *Thursday 5/15 between 4:00 and 7:00 PM Eastern Time*. 

I am creating this thread in advance so people can post questions ahead of time. I will be announcing this AMA on my [Facebook](https://www.facebook.com/yann.lecun) and [Google+](https://plus.google.com/+YannLeCunPhD/posts) feeds for verification.",287,377
104,2014-5-15,2014,5,15,15,25lwiw,Available Wall-mounted Slewing Jib Cranes Online - Machinery &amp; Equipment Part Online,https://www.reddit.com/r/MachineLearning/comments/25lwiw/available_wallmounted_slewing_jib_cranes_online/,jessicperson,1400137131,,0,1
105,2014-5-15,2014,5,15,23,25mjqc,Neural networks and a dive into Julia,https://www.reddit.com/r/MachineLearning/comments/25mjqc/neural_networks_and_a_dive_into_julia/,hernamesbarbara,1400162920,,1,6
106,2014-5-16,2014,5,16,1,25mxzw,Is there any way to rigorously validate a clustering method?,https://www.reddit.com/r/MachineLearning/comments/25mxzw/is_there_any_way_to_rigorously_validate_a/,lawrencechernin,1400171889,"One can make visualizations of the clusters, plot the F statistic, etc, but they generally always improve as more clusters are included (increase k).

Should clustering always be just considered exploratory, and if so what would you do next?",6,3
107,2014-5-16,2014,5,16,4,25nhkx,"Learning Irreducible, Invariant Image Representations (via Commutative Lie Groups)",https://www.reddit.com/r/MachineLearning/comments/25nhkx/learning_irreducible_invariant_image/,[deleted],1400183330,,0,1
108,2014-5-16,2014,5,16,6,25ntwg,Play Where's Waldo with OpenCV and Python,https://www.reddit.com/r/MachineLearning/comments/25ntwg/play_wheres_waldo_with_opencv_and_python/,jasonb,1400190578,,3,0
109,2014-5-16,2014,5,16,8,25o2ge,[OC] I made a basic Recommendation Engine for Reddit. Would love to hear your feedback about the implementation.,https://www.reddit.com/r/MachineLearning/comments/25o2ge/oc_i_made_a_basic_recommendation_engine_for/,manueslapera,1400196116,"I made a recommendation engine for Reddit. It allows redditors to discover new subreddits based on which subreddits they are subscribed to.

You can check it out [here](http://www.findasub.manugarri.com/)

Steps I followed:

    1. Find the top 1,400 subreddits by number of users (those subs that have more than 10k subscribers).

    2. for each of those subs, get comments until there is a userbase of 250k redditors

    3. For each subredditor, parse from their profile page all comments and posts they have made.

    4. For each sub among those 1,4K , compute their similarity with the rest using the [Jaccard Similarity coefficient](http://en.wikipedia.org/wiki/Jaccard_index).

    5.Create slick website :)

    6. When a user searches for a redditor, the app checks if that user exists on the database, if not, it pulls the user subs following the logic explained on step 3.

    7.Once the list of all subs that redditor is subscribed to is retrieved, sum the similarities of those subs with every other sub on the database.

    8. Return the top subs based on the total sum of similarities calculated on 7.


Some people have suggested that I could get better information about a user by using the Reddit Api. Even though by implementing this method I could get all the subs a user is subscribed to (instead of those the user has commented or posted on), that would retrieve also the default subreddits that user has not unsubscribed from, therefore increasing the noise on the recommendation algorithm.

What do you guys think of the implementation?
",15,16
110,2014-5-16,2014,5,16,14,25oy1e,How to choose Neural Net Architecture,https://www.reddit.com/r/MachineLearning/comments/25oy1e/how_to_choose_neural_net_architecture/,Livesinthefuture,1400219930,"Hello,

I'm reasonably new to Machine Learning and Neural Networks. I have a system at the moment which uses a Multi-layer Perceptron with Momentum Back Propagation as the learning rule.

A brief description of the task: Produce a ranked list of the top ten books to order in next year, based on things like number of loans, genre popularity etc.

My question is how do I know what the best architecture to use for this problem is? Also is there a name for this kind of problem that would give me better search results.

Thanks",4,0
111,2014-5-16,2014,5,16,15,25p0gc,Searching for software that does Preparing/Cleaning/Engineering data,https://www.reddit.com/r/MachineLearning/comments/25p0gc/searching_for_software_that_does/,cheeeech,1400222459,"Dear fellow machine learner,

I am looking for a tool, may it be SaaS, On-Premises or Open Source  that will allow me to visualize, manipulate data easily.

The features I'm mostly interested in:
Show distribution of variables
Show visualizations between two variables or more (maybe adding conditions e.g. (where var&lt;1)
Simple imputation of missing values
Modifications of numeric/categorical data e.g. (if var&lt;0, var==0)
Easily create new features by splitting/concatenating columns
Find impurities in variables
Being able to concatenate by row or by column
Any other quick matric / dataframe operations...

I found this tool: dataiku that does most of these things but it's 299 /mo.

I will appreciate any recommendation that even responds partially to the features I mention above.

Thanks!

cheeeech",5,0
112,2014-5-16,2014,5,16,18,25p8p8,"Human: Siri, do you have any children? Siri: Only biological entities have children... so far.",https://www.reddit.com/r/MachineLearning/comments/25p8p8/human_siri_do_you_have_any_children_siri_only/,sulleymanjaro,1400233648,,0,1
113,2014-5-16,2014,5,16,18,25p915,"Good models for handling missing data, sharing information",https://www.reddit.com/r/MachineLearning/comments/25p915/good_models_for_handling_missing_data_sharing/,[deleted],1400234162,"For example, if I have some data, where for some samples I have full whole genome sequencing, and for some only SNPs data (or for another example, totalRNASeq and the old RNASeq) which I expect there to be some sort of shared information between.

Is there a way of learning some weak information sharing from the few samples that have both (and how their other variables are correlated too), to maximise predictive power on test cases where there might be only a subset of data available (as is the case in the training set too)?",16,4
114,2014-5-16,2014,5,16,19,25pb7e,Construction Machinery Volvo Excavator EC460C Moving Trench Box on Vimeo,https://www.reddit.com/r/MachineLearning/comments/25pb7e/construction_machinery_volvo_excavator_ec460c/,jessicperson,1400237223,,0,1
115,2014-5-16,2014,5,16,20,25pc4c,15 Free eBooks On Machine Learning!,https://www.reddit.com/r/MachineLearning/comments/25pc4c/15_free_ebooks_on_machine_learning/,Engineer_14,1400238385,,1,27
116,2014-5-17,2014,5,17,1,25pz0t,Help! I need some ideas for machine learning projects.,https://www.reddit.com/r/MachineLearning/comments/25pz0t/help_i_need_some_ideas_for_machine_learning/,badgradstudent,1400256691,"I am a grad student that needs to submit a proposal for my master's thesis in less than a month. I am out of ideas for projects after repeatedly getting shot down. Does anyone have any possible ideas for projects (and what data sets would go well with the idea)?

I will have at least six months to work on the actual project, very little funding / outside resources, and already have knowledge of most of the commonly used algorithms.",5,0
117,2014-5-17,2014,5,17,1,25q2d3,Theory of Convex Optimization for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/25q2d3/theory_of_convex_optimization_for_machine_learning/,compsens,1400258750,,1,16
118,2014-5-17,2014,5,17,1,25q2k5,Brief History of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/25q2k5/brief_history_of_machine_learning/,erogol,1400258889,,0,1
119,2014-5-17,2014,5,17,1,25q2nk,Gradient Based Learning Algorithms vs Global Optimization Learning Algorithms for Neural Networks,https://www.reddit.com/r/MachineLearning/comments/25q2nk/gradient_based_learning_algorithms_vs_global/,sealturkey,1400258946,"First, is posting questions like this accepted in this sub? I usually post these type of questions to Stack Exchange sites, but the few sites which accept Neural Net questions (StackOverflow, CrossValidated,and CogSci) are not specifically for machine learning and hence often get down voted.

Anyway my post:

Neural Networks are usually trained using a gradient based learning algorithm, such as the back propagation algorithm or some variant of it, but can you use global optimization algorithms, such as the Genetic Algorithm, Nelder-Mead Polytope Algorithm, and Particle Swarm Optimisation to train the network?

Since training a neural network all boils down to minimizing a multi-variable cost function, I would assume that it should be easy to do this using a global optimization method, but I have tried to do it myself and I'm getting very bad results.

The GA algorithm reduces my cost function from around 250 (when it is input with random synaptic weights), to only around 170. The Nelder-Mead Algorithm would apparently take years, and I have not yet tried PSO as there is no inbuilt MATLAB function for it.

So is it just accepted that gradient based algorithms are the most suitable for training a Neural Network? If so can someone please point me towards a source of this information? This will be very helpful as I could reference the source in my project to explain why I gave up trying to use global optimization methods to train the network.

Thanks!",13,10
120,2014-5-17,2014,5,17,3,25qd6i,Good intermediate-advanced machine learning course online?,https://www.reddit.com/r/MachineLearning/comments/25qd6i/good_intermediateadvanced_machine_learning_course/,iamiamwhoami,1400265453,"So I did Andrew Ng's course. I'm looking for something more advanced now. The one's on Udacity seem interesting, but they don't have any problem sets. Any suggestions?",5,3
121,2014-5-17,2014,5,17,4,25qhm3,Andrew Ng takes job running Baidu's AI lab,https://www.reddit.com/r/MachineLearning/comments/25qhm3/andrew_ng_takes_job_running_baidus_ai_lab/,Megatron_McLargeHuge,1400268171,,23,90
122,2014-5-17,2014,5,17,6,25qslu,Does it make economic sense to use FPGA for machine learning as opposed to GPUs?,https://www.reddit.com/r/MachineLearning/comments/25qslu/does_it_make_economic_sense_to_use_fpga_for/,SunnyJapan,1400275032,"And if yes, why is everyone using GPUs then? Is there a big difference in speed between GPUs and FPGAs?",5,2
123,2014-5-17,2014,5,17,9,25r6qx,"Having trouble with WEKA - ""train and test set are not compatible"" - how to resolve the fact that attributes in training and testing data are different?",https://www.reddit.com/r/MachineLearning/comments/25r6qx/having_trouble_with_weka_train_and_test_set_are/,ChocolateCorgi,1400284858,"Hi ML redditors!

I have a large, unlabelled dataset of tweets with a certain hashtag, and I want to use supervised learning to label the data based on sentiment (buy, hold, sell). I want to employ Naive Bayes to categorize sentiment on the tweets (the dataset is rather large, and I don't want to have to manually categorize 50000 tweets), and I've converted the training set (manually categorized set of ~500 tweets) to a bag of words model. 

I've built the training model in WEKA, but I'm getting the error ""train and test set are not compatible"" when I try to use the training model to classify the rest of the data. I believe the problem is because I don't have the exact same attributes in the two sets. That is, my training model has a different bag of words than my complete test set, but I'm not sure how to resolve that other than combing through my full data set and remove all words that are not in the training set.

Forgive me if I'm conceptualizing the problem incorrectly or if there's something obvious I'm overlooking, as I don't have a background in ML. Sorry in advance for my newbishness!

Thanks!",8,5
124,2014-5-17,2014,5,17,14,25rtms,"Pi, monte carlo and interactive widgets",https://www.reddit.com/r/MachineLearning/comments/25rtms/pi_monte_carlo_and_interactive_widgets/,nipun_batra,1400303980,,0,1
125,2014-5-17,2014,5,17,14,25rugw,Python Implementation of Convolutional Neural Network,https://www.reddit.com/r/MachineLearning/comments/25rugw/python_implementation_of_convolutional_neural/,[deleted],1400304871,,11,19
126,2014-5-17,2014,5,17,16,25s1kp,Denoising with least squares,https://www.reddit.com/r/MachineLearning/comments/25s1kp/denoising_with_least_squares/,nipun_batra,1400313450,,8,11
127,2014-5-17,2014,5,17,20,25s96l,Having problem grasping the theory behind Neural Networks,https://www.reddit.com/r/MachineLearning/comments/25s96l/having_problem_grasping_the_theory_behind_neural/,[deleted],1400324651,"I'm learning about Neural Networks, specifically Backpropagation neural networks. The problem is, I don't really grasp the idea behind it, my questions are the following:
- Why are we using a Cost Function?
- What's the main advantage of using Backpropagation?
- Main purpose of Gradient Descent algorithm? I know the equation and I know that it's used to minimize the cost function. But what is benefiting us to learning/training the neural network?

I know that the weights and biases changes.

Sorry for these questions, I have been trying to learn more about machine learning and especially neural networks for the past days but I'm not able to understand the use case of Gradient Descent mainly in the idea of ""learning"". 

I'm hoping for a simple logical explanation to why neural network are what they currently are in the sense that why are these equations and algorithms are helping in the learning process.",1,1
128,2014-5-17,2014,5,17,23,25skyr,"I'm havng trouble understanding why SGD, RMSProp, and LBFGS have trouble converging on a solution to this problem (data included)",https://www.reddit.com/r/MachineLearning/comments/25skyr/im_havng_trouble_understanding_why_sgd_rmsprop/,eubarch,1400338254,"Here's a dropbox link to a simple data set:

https://dl.dropboxusercontent.com/u/106825941/IPData.tar.gz


It's about 84,000 examples polled from an cart-and-pendulum simulation.  The columns are:

    position  theta  velocity  angular-velocity force-applied-to-cart value

...Where ""value"" is a simple objective function whose inputs are all taken from the first four columns (x, theta, v, w).  All inputs are scaled such that their mean is 0 and they range more or less within [-3, 3].  The output is scaled such that the mean is 0.5, and all values fall within the interval [0.2, 0.8].  


A 5-25-1 feedforward network tasked with learning the value function and trained in Matlab can converge on an almost perfect solution with Levenberg-Marquardt or Scaled Conjugate Gradient descent very quickly.  However, using very similar network architecture in my own code (one difference being that my output neuron is a sigmoid, while Matlab's output neuron is linear) SGD and RMSprop fail to converge to a good answer.  I've tried minibatches with SGD, using the entire dataset per epoch, and lots of different learning rates and learning rate decay values.  I've spent a similar amount of time tweaking hyperparameters with RMSProp.  


RISO's LBFGS implementation also fails with this dataset, although I haven't put as much time into playing with it.  



I see three possibilities:


1.)  A bug in my code.  This is of course the thing I've been most suspicious of, but I'm begining to doubt this is the cause.  My code passes [this test](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization), and successfully extracts gabor shapes from natural images when used for autoencoding.  It also passes simpler tests, like learning XOR^* .



2.) Something about this dataset is particularly difficult for stochastic methods.  This seems unlikely; if you put together a scatter plot of value-vs-theta-vs-omega, you can see that it's a rather simple structure.



3.) SGD and RMSProp are incredibly sensitive to hyperparameter values, or perhaps weight initialization, and I've just been setting them wrong.  Right now I'm initializing weights with a uniform random variable -1&lt;R&lt;1: w = R * sqrt(6/(N+M+2)), where N and M are the incident layer sizes.  I've also tried simply assigning the weights small uniform random numbers.  



I'm hoping someone can give me some insight into why my SGD and RMSProp are failing here.  This should be an easy problem, but I can't find anything to point to that's demonstrably wrong.  




*: In regards to XOR, my code also seems to be very senstve to hyperparamters when solving this.  A 2-3-1 network needs over 10000 iterations to converge, and won't do so if the batch size is anything other than 1.  Starting froma  configuration that converges, and reducing the learning rate by a decade and also increasing the number of training iterations by a decade does not result in a network that also converges.  This seems wrong.",7,4
129,2014-5-18,2014,5,18,11,25u1h3,Entropy and Perplexity on Image and Text,https://www.reddit.com/r/MachineLearning/comments/25u1h3/entropy_and_perplexity_on_image_and_text/,bugra,1400378727,,0,11
130,2014-5-19,2014,5,19,3,25vmyh,Improving Sparse Data Support with scikit-learn,https://www.reddit.com/r/MachineLearning/comments/25vmyh/improving_sparse_data_support_with_scikitlearn/,[deleted],1400437972,,0,1
131,2014-5-19,2014,5,19,4,25vprg,What is the general technique that one extract an image of a person from various backgrounds.,https://www.reddit.com/r/MachineLearning/comments/25vprg/what_is_the_general_technique_that_one_extract_an/,fishingTigers,1400439934,"I am learning optimization theory right now, and we covered how to create different facial expression by finding the rotational and translational vectors of a static background image (basically the L2 norm of a minimization of Rx + r - y), now, although that is interesting, that made be wonder how you would solve the problem with different backgrounds, or with the same person in an image but with different centering. 

Specifically, how would I capture the person in the image in the first place, a pixel by pixel Rotational and translational movement would not work here, I was thinking of doing a low rank approximation if the person is at least most part of the image, but what if the person is a small part of the image?

Any advice would be appreciated",7,5
132,2014-5-19,2014,5,19,8,25wbgx,Basics of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/25wbgx/basics_of_machine_learning/,[deleted],1400454555,,0,1
133,2014-5-19,2014,5,19,8,25wce2,Basics of Machine Learning,https://www.reddit.com/r/MachineLearning/comments/25wce2/basics_of_machine_learning/,doubleColJustified,1400455219,,1,29
134,2014-5-19,2014,5,19,8,25wcgj,Improving Sparse Data Support in scikit-learn,https://www.reddit.com/r/MachineLearning/comments/25wcgj/improving_sparse_data_support_in_scikitlearn/,ha258,1400455272,,5,5
135,2014-5-19,2014,5,19,9,25wg1k,I am learning by doing...could use a little guidance,https://www.reddit.com/r/MachineLearning/comments/25wg1k/i_am_learning_by_doingcould_use_a_little_guidance/,[deleted],1400457909,.,0,1
136,2014-5-19,2014,5,19,12,25wv0a,"What are the most exciting potential applications of Machine Learning, Deep Learning and Natural Language Processing?",https://www.reddit.com/r/MachineLearning/comments/25wv0a/what_are_the_most_exciting_potential_applications/,feconroses,1400469055,,0,0
137,2014-5-19,2014,5,19,18,25xhl7,Laser Grader,https://www.reddit.com/r/MachineLearning/comments/25xhl7/laser_grader/,jessicperson,1400492627,,0,1
138,2014-5-19,2014,5,19,20,25xnac,A new approach (very) to Natural Language Understanding,https://www.reddit.com/r/MachineLearning/comments/25xnac/a_new_approach_very_to_natural_language/,bethcarey,1400499896,,0,3
139,2014-5-19,2014,5,19,22,25xrsu,"I know some Econometrics, and I'm now getting into Machine Learning. Found this interesting",https://www.reddit.com/r/MachineLearning/comments/25xrsu/i_know_some_econometrics_and_im_now_getting_into/,MissLola_,1400504484,,3,0
140,2014-5-19,2014,5,19,22,25xved,The People Who Would Teach Machines to Learn,https://www.reddit.com/r/MachineLearning/comments/25xved/the_people_who_would_teach_machines_to_learn/,ciskothekid,1400507269,,3,19
141,2014-5-20,2014,5,20,0,25y3yp,"Any good MOOCs on reinforcement learning? If not, what about the math?",https://www.reddit.com/r/MachineLearning/comments/25y3yp/any_good_moocs_on_reinforcement_learning_if_not/,c0wpig,1400513200,"I have an intermediate-advanced background in supervised and unsupervised learning, but I know next to nothing about reinforcement learning. I'm looking for a good MOOC, but the only one I've been able to find is going to start at some undetermined time in the future:
https://www.udacity.com/course/ud820

If no such MOOCs exist, are there any good courses on the underlying math? I know that markov processes and stochastic methods are at the core of a lot of RL techniques, but I haven't been able to find anything searching for those keywords, and tbh I don't really know what I'm looking for.

edit: Barring either of these, what's a good learning resource? Academic papers written by people who have no intention of making their thoughts understandable to anyone outside their bubble world do not qualify.",8,9
142,2014-5-20,2014,5,20,1,25ybq2,Machine Learning Tutorial: High Performance Text Processing,https://www.reddit.com/r/MachineLearning/comments/25ybq2/machine_learning_tutorial_high_performance_text/,[deleted],1400517985,,0,0
143,2014-5-20,2014,5,20,5,25yyzu,A Primer on Deep Learning,https://www.reddit.com/r/MachineLearning/comments/25yyzu/a_primer_on_deep_learning/,datacurator,1400531980,,9,49
144,2014-5-20,2014,5,20,7,25zcgi,The U.S. Securities and Exchange Commission is hiring data scientists,https://www.reddit.com/r/MachineLearning/comments/25zcgi/the_us_securities_and_exchange_commission_is/,jeferyan,1400540062,,0,2
145,2014-5-20,2014,5,20,8,25zhmm,[Update] I made some changes to the Reddit Recommendation Engine,https://www.reddit.com/r/MachineLearning/comments/25zhmm/update_i_made_some_changes_to_the_reddit/,manueslapera,1400543361,"Hi all!

I launched last week a recommendation Engine for Reddit. Thanks to the feedback received, I switched the app from allowing anyone to search recommendations for a user (worse quality of recommendations) to using oauth to see which subreddits the user is subscribed to.

The permission only allows to get the name and the subscriptions, and it should provide much more accurate recommendations.

In case you forgot the link, you can check the recommender [here](http://findasub.manugarri.com/)",5,6
146,2014-5-20,2014,5,20,10,25ztf2,When to call it quits and accept that you can't build a good model for the given data/problem?,https://www.reddit.com/r/MachineLearning/comments/25ztf2/when_to_call_it_quits_and_accept_that_you_cant/,Omega037,1400551012,"One situation that I sometimes find myself facing is when I have tried several different approaches to build a model, yet none perform much better than random.  

How do you decide to stop with your attempts and just assume that either the data is lacking the necessary information content (e.g., predicting cancer recurrence from music choice) or the problem is too complex to solve (e.g., learning full genotype to phenotype mapping)?",3,10
147,2014-5-20,2014,5,20,16,260gjj,Best way of handling missing feature values in neural networks,https://www.reddit.com/r/MachineLearning/comments/260gjj/best_way_of_handling_missing_feature_values_in/,alexmlamb,1400569843,"What is the best way of handling features which are missing at random?  In my case, the feature value is always present in the training set and sometimes missing in the set set.  

Here are a few possible solutions: 
1.  Extend the input dropout mask to cover the missing features.  In practice this amounts to giving the missing feature a value of zero and for each instance multiplying the weights by number_features_total / number_features_not_missing.  On the plus side, this method is relatively simple, but it lacks interpretability.  
2.  Data imputation.  This is a messy approach, but it is relatively easy to verify that it's producing reasonable results.  
3.  Use 1-of-k encoding for all features.  

Does anyone have any experience or opinion regarding which methods works best?  ",3,5
148,2014-5-20,2014,5,20,16,260hfa,Grader Product Catalog,https://www.reddit.com/r/MachineLearning/comments/260hfa/grader_product_catalog/,jessicperson,1400570945,,0,1
149,2014-5-20,2014,5,20,18,260mer,Noobie Question: How to quantify words for a neural network,https://www.reddit.com/r/MachineLearning/comments/260mer/noobie_question_how_to_quantify_words_for_a/,Imoa,1400577349,"So I am trying to create a program that takes movie scripts as inputs into my neural network and attempts to classify the genre of the movie. I think I understand the math I need, but I am struggling with the first step:

Given a list of every word occurring in a movie script (and number of hits), how do I quantify this list so that I can multiply each input by its weight?

Please help!",8,4
150,2014-5-20,2014,5,20,20,260tcp,The Dirichlet Process the Chinese Restaurant Process and other representations,https://www.reddit.com/r/MachineLearning/comments/260tcp/the_dirichlet_process_the_chinese_restaurant/,datumbox,1400585954,,0,11
151,2014-5-20,2014,5,20,23,2614h2,"Out of curiosity, do people tend to be accepting of mental illness in machine learning or data science jobs?",https://www.reddit.com/r/MachineLearning/comments/2614h2/out_of_curiosity_do_people_tend_to_be_accepting/,leviathanxs,1400595003,"Currently, I'm working part time as a statistician in a group of psychologists and psychiatrists studying children mental health. Given that I work for people in mental health and that I'm very good and efficient at what I do, they have no problem with me being bipolar. It also helps that the work hours are very flexible. If I'm feeling extremely depressed, I can generally just come another day unless we have a big deadline coming.

I'm soon finishing my master and after that, I intend to stay at that job for about 1-3 years to accumulate some money and gain experience. But after, I'd like to change track a bit and go more into the data science / machine learning field rather than biostatistics. I'm pretty sure that the environment must be quite different in comparison to a mental health or general medical research setting. So I'd like to know if you have generally seen good treatment or not of people with mental illness at work?",2,5
152,2014-5-21,2014,5,21,0,261enp,"Multi-task learning, neural network classifier",https://www.reddit.com/r/MachineLearning/comments/261enp/multitask_learning_neural_network_classifier/,aanchan,1400601383,"Hello,

I came across some interesting work from 2011 for the task of object recognition. The work is titled,  ""Sharing features between objects and their attributes"" - Hwang, Sha, Grauman CVPR 2011. Here is the link:www.cs.utexas.edu/~grauman/papers/hwang_CVPR2011.pdf. 

The objective function in that work from the perspective of multi-task learning (crudely) is to learn a common set of parameters of a classifier that reduces the classification error rate jointly on all the said tasks. The authors in that work use the SVM as a classifier. The objective function has a collective sparsity regularization term as well, which I understand enforces a so-called joint sparsity structure across the set of ""shared"" classifier parameters. The intuition is that a subspace of parameters is learned that is common to all of the said tasks.

My question is that, if I were to use a neural network as a classifier with a multi-task objective function, what form would that term that enforces the common sparsity take? How different is it from the usual L2-norm-the weight decay term that is generally employed with neural networks? 

Any insights would be appreciated.",3,2
153,2014-5-21,2014,5,21,1,261hfz,What book would you suggest for learning about different types of classifies (I specifically want to know more about maximum entropy classification)?,https://www.reddit.com/r/MachineLearning/comments/261hfz/what_book_would_you_suggest_for_learning_about/,double_snap,1400603040,,6,0
154,2014-5-21,2014,5,21,1,261i6t,Does anyone remember a browser game where you had to push a button and if the machine predicted correctly it moved one?,https://www.reddit.com/r/MachineLearning/comments/261i6t/does_anyone_remember_a_browser_game_where_you_had/,CarminHue,1400603512,"It was a fairly simple game, you could test the machine's learning capabilities by pushing up/down buttons. If the machine predicted correctly what you would push, it moved, if not, you moved. The path was like 40 step long. 

It learnt how you think and it would correctly predict after a few moves.  

One of my friends says that he consistently beats the NYTimes rock-paper-scissors so I want to show him this. :)


Edit: So does anyone know the site it is on? :)",11,18
155,2014-5-21,2014,5,21,2,261qkx,"Learning Irreducible, Invariant Image Representations (via Commutative Lie Groups)",https://www.reddit.com/r/MachineLearning/comments/261qkx/learning_irreducible_invariant_image/,goblin_got_game,1400608510,,0,2
156,2014-5-21,2014,5,21,5,2626uz,"What are the best University departments and research centres in Eastern Europe, for ML and AI?",https://www.reddit.com/r/MachineLearning/comments/2626uz/what_are_the_best_university_departments_and/,Algorithm_1,1400617829,"EDIT: The context of the question is that I'm looking for potential departments to engage with for the purpose of seeking collaborators and contributors (and possibly prospective future employees) for an ML/AI related start-up. I'm interested in engaging with both users and developers of ML technology.

I hold a PhD in this field myself. I'm from the UK. I'm seeking outside opinion first off as I'm not very familiar with the Eastern Europe in this respect - except for the fact that I have recently encountered a handful of very smart and keen ML-oriented developers from the region.",2,12
157,2014-5-21,2014,5,21,16,263pt1,Cluster Analysis for time series,https://www.reddit.com/r/MachineLearning/comments/263pt1/cluster_analysis_for_time_series/,jonthawk,1400657682,"I have about two months of station-level hourly price data for gasoline in the Midwest. Different gas stations seem to follow dramatically different pricing patterns (in terms of deviation from the wholesale price.)

I would like to run a clustering algorithm to group the stations' price time series into these distinct patterns. Since it's the dynamics of the time series I care about, not the actual values, I don't think something like k-means will give me what I want.

Any suggestions would be appreciated! ",8,5
158,2014-5-21,2014,5,21,17,263rrm,Algorithmic Tagging of HackerNews (or any other site),https://www.reddit.com/r/MachineLearning/comments/263rrm/algorithmic_tagging_of_hackernews_or_any_other/,MajorDeeganz,1400660415,,2,11
159,2014-5-21,2014,5,21,17,263t8f,Relation between feature learning and feature selection,https://www.reddit.com/r/MachineLearning/comments/263t8f/relation_between_feature_learning_and_feature/,lucktroy,1400662351,"From wiki, I found the definitions of feature learning and feature selection. 
Feature learning is a newer word (aka representation learning). 
Feature selection is used to *learn* a subset of original features; feature learning is used to learn a transformation of ""raw"" inputs to a representation. 
They have a similar objective, but different ways. 

I'd like to know what is the relation between feature learning (representation learning) and feature selection? 
Maybe they both can be known as a kind of representation learning. 

Links:

Feature learning: http://en.wikipedia.org/wiki/Feature_learning

Feature selection: http://en.wikipedia.org/wiki/Feature_selection",8,6
160,2014-5-21,2014,5,21,18,263tx8,Mining Equipment for Sale - PDF,https://www.reddit.com/r/MachineLearning/comments/263tx8/mining_equipment_for_sale_pdf/,jessicperson,1400663286,,0,1
161,2014-5-21,2014,5,21,23,264cs1, Gvenlii,https://www.reddit.com/r/MachineLearning/comments/264cs1/i_gvenlii/,nedimnamo,1400681766,,3,0
162,2014-5-22,2014,5,22,5,265dxy,How To Tell King Penguins VS Dogs Dressed Up As King Penguins,https://www.reddit.com/r/MachineLearning/comments/265dxy/how_to_tell_king_penguins_vs_dogs_dressed_up_as/,[deleted],1400703913,,3,13
163,2014-5-22,2014,5,22,16,266y74,Scrapers,https://www.reddit.com/r/MachineLearning/comments/266y74/scrapers/,jessicperson,1400744765,,0,1
164,2014-5-22,2014,5,22,18,2673is,Python code for Stacked Denoising Autoencoders for unsupervised learning?,https://www.reddit.com/r/MachineLearning/comments/2673is/python_code_for_stacked_denoising_autoencoders/,[deleted],1400751852,"All the code I can find, for example from [the DeepLearning tutorials using Theano](http://deeplearning.net/tutorial/SdA.html) and from [PyLearn2](http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/stacked_autoencoders/stacked_autoencoders.ipynb) - uses SdAs as a means for supervised classification, and so the fine-tuning code is used with labelled data.

However, I just want to use it for unsupervised learning (like the faces example in [this paper/letter](http://www.cs.toronto.edu/~hinton/science.pdf) ), so I want the fine-tuning part to just minimise the reconstruction error across the whole network, is there Python code available anywhere for this? (As modifying the Theano-based code is a bit intimidating!).

This [image](http://mrlabs.org/wp-content/uploads/2013/06/Denoising-AutoEncoder1.png) is a good example of what I want to do. I tried it with just a single hidden layer autoencoder but I found it just learnt the identity function (so all the information was in the weights).",7,2
165,2014-5-22,2014,5,22,18,2673tu,Speech Recognition: obtaining semantic results from a XML grammar,https://www.reddit.com/r/MachineLearning/comments/2673tu/speech_recognition_obtaining_semantic_results/,MainframePT,1400752245,"I'm currently developing a Speech Recognition application and I'm having some problems when obtaining the semantic results. Here's a small sample of my grammar:

      &lt;rule id=""rootRule""&gt;
      &lt;one-of&gt;
        &lt;item&gt; &lt;ruleref uri=""#CA01b""/&gt; &lt;tag&gt; out.AgePerson=rules.latest(); &lt;/tag&gt; &lt;/item&gt;
        &lt;item&gt; &lt;ruleref uri=""#CA02"" /&gt; &lt;tag&gt; out.RoomInformation=rules.latest(); &lt;/tag&gt; &lt;/item&gt;
      &lt;/one-of&gt;
      &lt;/rule&gt;


      &lt;!-- CA01b: Ask person age --&gt;
      &lt;rule id=""CA01b""&gt;
      &lt;one-of&gt;
          &lt;item&gt; what is your age &lt;/item&gt;
      &lt;/one-of&gt;
      &lt;ruleref uri=""#numbers""/&gt;
      &lt;/rule&gt;

      &lt;!-- CA02: Ask for room information --&gt;
      &lt;rule id=""CA02""&gt;
      &lt;one-of&gt;
          &lt;item&gt; where is room &lt;/item&gt;
      &lt;/one-of&gt;
      &lt;ruleref uri=""#numbers""/&gt;
      &lt;/rule&gt;

    &lt;!-- Numbers --&gt;
      &lt;rule id=""numbers""&gt;
    &lt;item&gt;
        &lt;one-of&gt;
            &lt;item&gt; four &lt;tag&gt;out=""4"";&lt;/tag&gt;&lt;/item&gt;
            &lt;item&gt; five&lt;tag&gt;out=""5"";&lt;/tag&gt;&lt;/item&gt;
            &lt;item&gt; six &lt;tag&gt;out=""6"";&lt;/tag&gt;&lt;/item&gt;
            &lt;item&gt; seven &lt;tag&gt;out=""7"";&lt;/tag&gt;&lt;/item&gt;
                    &lt;/one-of&gt;
          &lt;/item&gt;
      &lt;/rule&gt;

When posed with the following speech:

    1) Where is room 4
    2) My age is 4

I wish to obtain the semantic results:

    1) AgePerson=4
    2) RoomInformation=4

In the application code, within the SpeechRecognitionRecognized event method, I have the following code:

#region Speech Recognized
    // Try to recognize speech, return semantic and write to a file the result obtained
    void spRecEng_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)
    {


        if (e.Result.Confidence &lt; ConfidenceThreshold)
        {
            //engine is not confident about result =&gt; ignore
            return;
        }
        result = e.Result.Text;
        confidence = e.Result.Confidence;
        semantics = e.Result.Semantics.Value.ToString();


        semanticRep.Text = semantics;
        spoken.Text = result;
        confBox.Text = confidence.ToString();

        WriteXML(result, confidence, semantics, recognized);
    }
    #endregion

The string ""semantics"" contains the semantic results, this particular line of code was previously written with an older (less sophisticated) version of the grammar which with the new grammar shown above obviously no longer works and I've been having problems setting it up right. Any sort of help will be greatly appreciated!",0,0
166,2014-5-22,2014,5,22,20,2677in,Our Tips For Graders - machines4u's soup,https://www.reddit.com/r/MachineLearning/comments/2677in/our_tips_for_graders_machines4us_soup/,jessicperson,1400756858,,0,1
167,2014-5-22,2014,5,22,20,2679yw,How do i make myself more qualified for a masters program in machine learning?,https://www.reddit.com/r/MachineLearning/comments/2679yw/how_do_i_make_myself_more_qualified_for_a_masters/,Subcompact_Astronaut,1400759514,"I am currently an undergrad getting a math b.s and I have little programming experience. Been looking up masters programs in the U.S and Europe and not sure if I would qualify. How do i make myself a better candidate? People already in masters programs in this field, how did you do it?",14,2
168,2014-5-22,2014,5,22,22,267htf,"My notes from May 21 presentation in Boulder by Ted Dunning on Mahout 1.0. It's not just a port from Map/Reduce to Spark, it seems to subsume Spark itself. Alternative to MLbase/MLI/MLlib.",https://www.reddit.com/r/MachineLearning/comments/267htf/my_notes_from_may_21_presentation_in_boulder_by/,michaelmalak,1400766051,,0,1
169,2014-5-22,2014,5,22,23,267n87,What is the unique value of IBM Watson developer environment,https://www.reddit.com/r/MachineLearning/comments/267n87/what_is_the_unique_value_of_ibm_watson_developer/,abhijaydatta,1400769604,"Hoepfully I will get an answer here. There are a few ISVs who are part of the IBM Watson developer network who are trying to sell their services to our analytics group. Do not know much about IBM Watson (apart from ""Jeopardy!"" and all that).

Can someone list out what are the unique values that IBM Watson environment brings against standard machine learning languages like Python, weka, R etc which our analytics groups have access to and can build their own algorithms? In short, what more I can do if I have access to IBM Watson development environment?",10,10
170,2014-5-23,2014,5,23,1,267zuv,Explanation of the Regression Plot in the Matlab Neural Network Toolbox,https://www.reddit.com/r/MachineLearning/comments/267zuv/explanation_of_the_regression_plot_in_the_matlab/,[deleted],1400776962,"What does the Regression Plot in the Matlab Neural Network Toolbox show? I thought I understood it when I looked at a univariate regression plot, but I've just plotted one for multivariate regression, and it makes no sense to me.

My Neural Network takes in 24 inputs, and gives 3 outputs. The 24 inputs are 24 different angles, and the 3 outputs are the (x,y,z) positions in 3d space. I expect that the function mapping the two is highly nonlinear, but I'm not sure.

[Here](http://imgur.com/MtmRsI0) is the regression plot, and [here](http://imgur.com/hNHGo25) is the performance plot if that helps.

What does the regression plot show? I do not understand this graph at all. Surely it cant plot the function because that would require a plot in 27 dimensions (24 inputs + 3 outputs). What do the 4 graphs represent? To me it looks like it is saying that the function is linear, could this be true?",1,0
171,2014-5-23,2014,5,23,2,2681za,Quantum Machine Learning - Google talk,https://www.reddit.com/r/MachineLearning/comments/2681za/quantum_machine_learning_google_talk/,Feribg,1400778141,,0,0
172,2014-5-23,2014,5,23,3,2688ny,"New to ML, decided to try my hand applying it to quadcopter stability! Write-up includes an interactive app running a pre-trained NN. [xpost r/multicopter]",https://www.reddit.com/r/MachineLearning/comments/2688ny/new_to_ml_decided_to_try_my_hand_applying_it_to/,virialthm,1400781937,,5,29
173,2014-5-23,2014,5,23,4,268fo7,Machine Learning A Cappella - Overfitting Thriller!,https://www.reddit.com/r/MachineLearning/comments/268fo7/machine_learning_a_cappella_overfitting_thriller/,shreesh13,1400786013,,0,0
174,2014-5-23,2014,5,23,6,268uyj,The worst place to be stung by a bee,https://www.reddit.com/r/MachineLearning/comments/268uyj/the_worst_place_to_be_stung_by_a_bee/,[deleted],1400794914,,1,0
175,2014-5-23,2014,5,23,12,269nrj,"Tamr, a startup that one of the mods is part of came out of stealth mode!",https://www.reddit.com/r/MachineLearning/comments/269nrj/tamr_a_startup_that_one_of_the_mods_is_part_of/,kunjaan,1400814072,"Use machine learning to solve large scale data curation and integration problems. 

Here's a short introductory video: https://www.youtube.com/watch?v=7YpLQJZR0E8

Research paper: https://cs.uwaterloo.ca/~ilyas/papers/StonebrakerCIDR2013.pdf

Official site: http://www.tamr.com/

Congrats Kanak. Best of luck to the Tamr team.
",4,11
176,2014-5-23,2014,5,23,12,269pcn,Single Speaker Speech Recognition using GMM-HMM,https://www.reddit.com/r/MachineLearning/comments/269pcn/single_speaker_speech_recognition_using_gmmhmm/,kkastner,1400815250,,2,14
177,2014-5-23,2014,5,23,22,26anyp,What Machine Learning Research related workshops are happening this summer?,https://www.reddit.com/r/MachineLearning/comments/26anyp/what_machine_learning_research_related_workshops/,justAnotherEngineer,1400851463,,5,7
178,2014-5-24,2014,5,24,0,26aytz,It works like the brain. So?,https://www.reddit.com/r/MachineLearning/comments/26aytz/it_works_like_the_brain_so/,[deleted],1400858995,,0,1
179,2014-5-24,2014,5,24,2,26b819,"*Please* ignore the article itself and focus on the comments section, which is a rather adult conversation on the differences between R and Python (X-posted from /r/rstats)",https://www.reddit.com/r/MachineLearning/comments/26b819/please_ignore_the_article_itself_and_focus_on_the/,thatguydr,1400864903,,3,5
180,2014-5-24,2014,5,24,2,26b9cm,Using machine learning to study the health of the Coral reef.,https://www.reddit.com/r/MachineLearning/comments/26b9cm/using_machine_learning_to_study_the_health_of_the/,SubMachineGhost,1400865742,,2,15
181,2014-5-24,2014,5,24,4,26blbk,* LEVAN: Algorithm Learning Everything about Anything,https://www.reddit.com/r/MachineLearning/comments/26blbk/levan_algorithm_learning_everything_about_anything/,[deleted],1400873227,,0,1
182,2014-5-24,2014,5,24,4,26blmd,LEVAN: Algorithm Learning Everything about Anything,https://www.reddit.com/r/MachineLearning/comments/26blmd/levan_algorithm_learning_everything_about_anything/,tomaskazemekas,1400873410,,5,21
183,2014-5-24,2014,5,24,6,26bw0a,Can anyone tell me anything about Diffeo?,https://www.reddit.com/r/MachineLearning/comments/26bw0a/can_anyone_tell_me_anything_about_diffeo/,polyguo,1400880120,[Here](http://diffeo.com/) is a link to the site. It's meant to capture many data sources and use ML to group relevant data. Does anyone know anything about this entity-based groupings? any of the research behind it?,0,0
184,2014-5-24,2014,5,24,9,26cbjx,I'm looking for a website that gave you a list of the prerequisites for learning a topic.,https://www.reddit.com/r/MachineLearning/comments/26cbjx/im_looking_for_a_website_that_gave_you_a_list_of/,why_do_you_exist,1400891278,"On the site, you could enter in Markov Chains and it would show a bunch of nodes of things that needed to be learned before you could understand that topic. I saw it on here once before. Does anyone know what I'm talking about? ",2,16
185,2014-5-24,2014,5,24,10,26cfzx,Introductory Tutorial on Nonlinear Curve Fitting for Machine Learning,https://www.reddit.com/r/MachineLearning/comments/26cfzx/introductory_tutorial_on_nonlinear_curve_fitting/,Alrecenk,1400894923,,3,0
186,2014-5-24,2014,5,24,13,26cr0w,Another foundation patent in the area of machine learning,https://www.reddit.com/r/MachineLearning/comments/26cr0w/another_foundation_patent_in_the_area_of_machine/,NeuroCode1,1400904376,,8,0
187,2014-5-24,2014,5,24,18,26d7f4,Dynamic clustering of streams of text data,https://www.reddit.com/r/MachineLearning/comments/26d7f4/dynamic_clustering_of_streams_of_text_data/,binge_learner,1400924549,"Hi guys,
I am working right now on Machine Learning problems for social media analysis. And one of the problems I'm currently facing is how to cluster streams of social media data ( Tweets and Facebook posts mostly ) but in a dynamic manner, as in new clusters form, others grow and other disappear based on the content of the stream ( and eventually visualizing the whole thing as a timeline ).
So my question is : is there any work that's been done in the subject ( just to have a starting point ) ? And what do you think are the state of the art techniques for text clustering and topic detection ?
Thanks you.",3,3
188,2014-5-24,2014,5,24,22,26dfz4,Advances in Neural Information Processing,https://www.reddit.com/r/MachineLearning/comments/26dfz4/advances_in_neural_information_processing/,theophrastzunz,1400936558,"Hi, I'm a computational neuroscientist and I'm getting more interested in the shared area of computational statistics, ML and neuroscience.

From my perspective, one of the big journals in this niche are Advances in Neural information Processing and Neural Computation.

How are they seen by the ML crowd?",7,1
189,2014-5-25,2014,5,25,1,26dvnm,Data Mining Reddit Posts Reveals How to Ask For a Favor--And Get it | MIT Technology Review,https://www.reddit.com/r/MachineLearning/comments/26dvnm/data_mining_reddit_posts_reveals_how_to_ask_for_a/,SamStringTheory,1400950200,,8,40
190,2014-5-25,2014,5,25,2,26dyqe,Machine Learning - Application question (Beginner),https://www.reddit.com/r/MachineLearning/comments/26dyqe/machine_learning_application_question_beginner/,whatabih,1400952513,"Hello! 

Like many of you I'm trying to better understand the subject and had a question (which I'm sure many of you will find .... trivial)

I'm trying to implement all the models and concepts I've learned with scikit learn. I randomly picked a problem that has 'US State' as one of the inputs. I'd like to keep it simple and try to solve the problem using linear regression. My Q is - how do I represent the states numerically in my input? If I normalize and randomly give them values -2.5 to 2.5 am I not already enforcing a 'preference' if I'm solving this problem with a linear model?

Edit: To clarify - I have seen some stuff where each state is represented as its own feature. I'm wondering if there was a way that won't impose greater requirements on input data!

Thanks for any input you may have",5,0
191,2014-5-25,2014,5,25,6,26ei69,Correlations between time series.,https://www.reddit.com/r/MachineLearning/comments/26ei69/correlations_between_time_series/,qkdhfjdjdhd,1400967161,,0,23
192,2014-5-25,2014,5,25,14,26fds8,Using Genetic Algorithms to Break Things,https://www.reddit.com/r/MachineLearning/comments/26fds8/using_genetic_algorithms_to_break_things/,n1ghtw1sh,1400994406,,3,0
193,2014-5-25,2014,5,25,16,26fl5l,Meet the algorithm that can learn everything about anything,https://www.reddit.com/r/MachineLearning/comments/26fl5l/meet_the_algorithm_that_can_learn_everything/,mikkom,1401003097,,14,63
194,2014-5-25,2014,5,25,21,26fy3q,What does a neural network actually do?,https://www.reddit.com/r/MachineLearning/comments/26fy3q/what_does_a_neural_network_actually_do/,SuperFX,1401022752,,2,30
195,2014-5-26,2014,5,26,9,26hfwl,Can anyone help with a simple neural net I am trying to code?,https://www.reddit.com/r/MachineLearning/comments/26hfwl/can_anyone_help_with_a_simple_neural_net_i_am/,davidai,1401064733,"I have a neural net with two input units (and a bias) connected to four hidden units, with each input unit (including the bias) connected to all the hidden units. Then there is one output unit. All units are logistic. I have coded the net in MATLAB (will translate to python on Tuesday) and I am trying to get it to learn using back propagation, but I can't seem to get the training quite right. I want it to learn XOR, ""exclusive or.""

I'm not completely sure this is the right subreddit for my question or that my question is appropriate but can someone take a look at my code and tell me what I'm doing wrong?

If someone is willing I can post it here or just PM it.

My main problem is that I can train the net with one of the cases ((0,1) for example would should return 1 or true) and then when i run the net with any of the cases ((0,1),(1,0),(0,0),(1,1)), ALL cases are giving me a value close to 1. 

If I train all cases one after the other, then I just get consistently smaller output values approaching zero. Then the net outputs a value close to zero for any value. 

Not exactly sure what I'm doing wrong.",8,1
196,2014-5-26,2014,5,26,16,26i8jx,How do you handle input vectors of a variable length?,https://www.reddit.com/r/MachineLearning/comments/26i8jx/how_do_you_handle_input_vectors_of_a_variable/,mlnoob,1401089755,"Hi r/MachineLearning , noob here:

I was wondering how to structure a neural net where the inputs are of an undetermined size.

For example, if I wanted to train on images of different dimensions, or if I wanted to do NLP of wordpress posts, how can I deal with posts of different lengths?

All of the examples of NN's I have seen have fixed length input vectors, which obviously makes things easier. But how would you deal with real world data that seems less constrained?",21,7
197,2014-5-26,2014,5,26,17,26ia3b,[Repost] E-book: Predicting movie ratings and recommender systems,https://www.reddit.com/r/MachineLearning/comments/26ia3b/repost_ebook_predicting_movie_ratings_and/,saboteur2,1401091546,,0,0
198,2014-5-26,2014,5,26,18,26iebo,Data Mining Reddit Posts Reveals How to Ask For a Favor--And Get it,https://www.reddit.com/r/MachineLearning/comments/26iebo/data_mining_reddit_posts_reveals_how_to_ask_for_a/,pica_foices,1401097100,,0,0
199,2014-5-26,2014,5,26,18,26ief4,Cat Dozers - PDF,https://www.reddit.com/r/MachineLearning/comments/26ief4/cat_dozers_pdf/,jessicperson,1401097246,,0,1
200,2014-5-26,2014,5,26,20,26iis3,A bank of filter to detect image primitives,https://www.reddit.com/r/MachineLearning/comments/26iis3/a_bank_of_filter_to_detect_image_primitives/,Felix-Neko,1401103363,"I have to train a convolution neural network to classify images. It has a set of N correlation filters (7x7 pixels each) as its first layer. This filters should detect simple primitives like differently tilted lines, corners, etc.

I initialize all weights of my network with gaussian noise with different amplitude (depends on layer). I can see that my first layer filters evolve from random noise to more reasonable detecting filters. For example, I saw there edge detectors and something else.

Also I have seen that first layer trains most slowly. To accelerate its training, I want to initialize it with a set of ready-made primitive detecting filters. Then the network can change then as it wants.

Cannot you advice me a set of such filters?",4,1
201,2014-5-26,2014,5,26,23,26irr0,Paragraph Vector - a step up from word2vec,https://www.reddit.com/r/MachineLearning/comments/26irr0/paragraph_vector_a_step_up_from_word2vec/,Foxtr0t,1401112819,"Word2vec is about vector representation for words. Now Le and Mikolov propose something a lot more interesting: **fixed length vector representation of documents**. Hope that someone publishes the code.

http://arxiv.org/abs/1405.4053",17,49
202,2014-5-26,2014,5,26,23,26iu1w,Simple Python script to find most dominant colors in an image using k-means.,https://www.reddit.com/r/MachineLearning/comments/26iu1w/simple_python_script_to_find_most_dominant_colors/,zionsrogue,1401114742,,0,17
203,2014-5-26,2014,5,26,23,26ivuw,Neural Net outputting same thing each time.,https://www.reddit.com/r/MachineLearning/comments/26ivuw/neural_net_outputting_same_thing_each_time/,[deleted],1401116193,"This the first NN I have built from scratch and I have simplified quite a bit, but it each time I run it the output is almost always the same. I am assuming with randomized weights and biases I should get random output with the data, right? Steps I have already taken:

1) Normalized my data 0-1 then -1 to 1, then -2 to 2. 

2) Changed my weight and bias initialization from numpy's randn (normal) to rand.

3) Take the bias out of the equation during feed forward. 

4) Used different training sets and validation sets.

5) Played with different hidden layer sizes and learning rates.

I now have it where I just randomly fill my weights and biases and feed forward without training and it still will give me all the same numbers. Below is a bit of code I have been using. 

Initializing random weights and bias.

    self.hidden_weight = numpy.random.randn(hidden_size, input_size) 
    
    self.hidden_bias = numpy.random.randn(hidden_size, 1)
    
    self.output_weight = numpy.random.randn(output_size, hidden_size)
    
    self.output_bias = numpy.random.randn(output_size, 1)    

Feed Forward

    activation_input = data_vector
        
    z_hidden = numpy.dot(self.hidden_weight, activation_input) + self.hidden_bias
    activation_hidden = sigmoid(z_hidden)
        
    z_output = numpy.dot(self.output_weight, activation_hidden) + self.output_bias
    activation_output = sigmoid(z_output)
        
    list1 = numpy.array(activation_output).tolist()
    list2 = numpy.array(expected).tolist()
    print list1.index(max(list1)), list2.index(max(list2)) 

That print statement always gives the same output value for list1. The odd thing is the value changes if I don't use a seed or shuffle the data before I split it into training, validation, test sets.
        ",7,0
204,2014-5-27,2014,5,27,12,26ksap,Implementing kernel PCA in numpy for a non-linear data,https://www.reddit.com/r/MachineLearning/comments/26ksap/implementing_kernel_pca_in_numpy_for_a_nonlinear/,vmirjalily,1401161809,,2,11
205,2014-5-27,2014,5,27,13,26kuqx,Opinions on Open-Source ML software,https://www.reddit.com/r/MachineLearning/comments/26kuqx/opinions_on_opensource_ml_software/,kristopolous,1401163716,"wikipedia has this list http://en.wikipedia.org/wiki/Machine_learning#Open-Source_Software and being the objective thing wikipedia is, doesn't really state what's good or not.

So I turn to you!

What are people's opinions of these various technologies?",10,1
206,2014-5-27,2014,5,27,13,26kxug,Deep learning from the bottom up.,https://www.reddit.com/r/MachineLearning/comments/26kxug/deep_learning_from_the_bottom_up/,qkdhfjdjdhd,1401166265,,0,9
207,2014-5-27,2014,5,27,15,26l4gc,What can we automatically learn from the satellite communication logs of flight MH370?,https://www.reddit.com/r/MachineLearning/comments/26l4gc/what_can_we_automatically_learn_from_the/,galapag0,1401172330,,0,0
208,2014-5-27,2014,5,27,16,26l7hh,The Suggested Way to Choose a Grader Blade - machines4u's soup,https://www.reddit.com/r/MachineLearning/comments/26l7hh/the_suggested_way_to_choose_a_grader_blade/,jessicperson,1401175774,,0,1
209,2014-5-27,2014,5,27,18,26lce7,The Unique Process of Grading Excavation,https://www.reddit.com/r/MachineLearning/comments/26lce7/the_unique_process_of_grading_excavation/,jessicperson,1401182206,,0,1
210,2014-5-27,2014,5,27,20,26lheo,ISSUU - Cat Backhoes by Machines4u,https://www.reddit.com/r/MachineLearning/comments/26lheo/issuu_cat_backhoes_by_machines4u/,jessicperson,1401188801,,0,1
211,2014-5-27,2014,5,27,23,26ltge,Good 2d dataset for clustering?,https://www.reddit.com/r/MachineLearning/comments/26ltge/good_2d_dataset_for_clustering/,admiral_snark,1401199468,"I want to create a demonstration of k-means clustering.  Something mindlessly simple, with two features so that it can be easily visualized.

Are there any good datasets for this?  Looking for something pretty general so audience can see &amp; verify the results (eg - iris petals are a little too specialized)",8,0
212,2014-5-28,2014,5,28,0,26m16j,The Flaw Lurking In Every Deep Neural Net,https://www.reddit.com/r/MachineLearning/comments/26m16j/the_flaw_lurking_in_every_deep_neural_net/,slowmantic,1401204431,,68,109
213,2014-5-28,2014,5,28,2,26mbxv,How are the spatial and temporal relationships between data points (pixels/voxels) usually represented in a modern ANN?,https://www.reddit.com/r/MachineLearning/comments/26mbxv/how_are_the_spatial_and_temporal_relationships/,SpaceWizard,1401210866,"I ask because I saw some code where the workhorse function appeared to be taking an image as a vector instead of a matrix. Are ANNs ever explicitly coded to know that pixel x is a neighbor of pixel y or does that just get sorted out with the learning algorithm? I could see how it wouldn't matter in the end if all the images are the same dims, but knowing about neighboring pixels seems like it could speed things up.",4,0
214,2014-5-28,2014,5,28,13,26o34u,Realtime predictive analytics using scikit-learn &amp; RabbitMQ - PyCon 2014,https://www.reddit.com/r/MachineLearning/comments/26o34u/realtime_predictive_analytics_using_scikitlearn/,Feribg,1401249941,,1,11
215,2014-5-28,2014,5,28,16,26og8q,Excavators,https://www.reddit.com/r/MachineLearning/comments/26og8q/excavators/,jessicperson,1401262743,,0,1
216,2014-5-28,2014,5,28,18,26ol02,Best Fixed CPAP Machine in Australia,https://www.reddit.com/r/MachineLearning/comments/26ol02/best_fixed_cpap_machine_in_australia/,taylorswift1234,1401269291,,0,1
217,2014-5-28,2014,5,28,19,26oo47,Google Using Machine Learning to Boost Efficiency in Data Centres,https://www.reddit.com/r/MachineLearning/comments/26oo47/google_using_machine_learning_to_boost_efficiency/,dataconomy,1401273288,,7,21
218,2014-5-28,2014,5,28,20,26os4c,machines4u's uploaded images,https://www.reddit.com/r/MachineLearning/comments/26os4c/machines4us_uploaded_images/,jessicperson,1401277856,,0,1
219,2014-5-28,2014,5,28,21,26ovtt,non parametric mixture model,https://www.reddit.com/r/MachineLearning/comments/26ovtt/non_parametric_mixture_model/,Walden1234,1401281178,"

I have a question about clusters that I am contemplating to treat with a non parametric mixture approach (I think).

The problem is the following:

I am working on the explanation of human comportment. Each row of my database contains : 1) the id of someone 2) some parameters of the environment X (exemple : the temperature, the wind etc.) 3) a binary variable Y representing the person's reaction to the parameters (exemple : get sick or does not get sick because of the weather). My idea (based on intuition and not on data) is that we can gather people in a finite number of groups so that in a group, people have the same reaction to temperature (some are easily sick, others are never sick...). In a given group, more formally, the law of Y conditionnaly to the parameters X is the same.

I have no idea of the law of Y conditionnaly to X. For the parameters X, I can do some hypothesis if necessary.

I would like to create some cluster of people ""having more or less"" the same reaction to parameter. Besides, I would like to predict the reaction of a given person to a given value of the parameters (even if this event has never happened in the database).

It seems to me that we can treat the problem like a non parametric mixture model. Indeed, as I don't have hypothesis on the conditionnal law of Y, I think I will have to create it with kernels method for instance. I have found this paper : http://biometrics.cse.msu.edu/Publications/Clustering/MallapragadaJinJain_NonparametricMixtureModels_SSSPR10.pdf Besides, it seems to me that, in this case, each row of observation (X_i, Y_i) is not a simple realization of some random variable but X_i is a realization of a random variable and Y_i is a realization of a random variable conditionnaly to X_i. I don't know if it makes a difference.

Precision : I have around 100.000 rows , the vector X_i has some components discrete and others continue.

So, I am wondering :

    is my approach correct ?
    would you advise me another point of view for this problem

I would be very interested in any references about it.

Don't hesitate to make me reformulate the problem statement.

Thank you very much in advance.
",0,3
220,2014-5-28,2014,5,28,22,26ox1c,Twitter Archives?,https://www.reddit.com/r/MachineLearning/comments/26ox1c/twitter_archives/,[deleted],1401282194,Can anyone suggest a good place to get twitter archives (paid or free)? I've been playing around with the Twitter API but the tweets I get only go back a week or so. The data I'm going to use with the tweets is (relatively) old.,2,0
221,2014-5-29,2014,5,29,2,26pkrq,IBM Branches Out Into Condiments,https://www.reddit.com/r/MachineLearning/comments/26pkrq/ibm_branches_out_into_condiments/,[deleted],1401297160,,4,2
222,2014-5-29,2014,5,29,2,26pmfc,Clustering heterogeneous data - a question about relevant methods.,https://www.reddit.com/r/MachineLearning/comments/26pmfc/clustering_heterogeneous_data_a_question_about/,smashstacker,1401298152,"Hi everyone, 

I've never posted here before but I thought this would be a good place to look for some guidance on a problem we're facing in our research group. Basically, we are looking to perform clustering on a dataset that has heterogeneous feature vectors. For simplicity's sake, imagine the features are ""color"", ""quantity"", and ""pattern"" - a mixture of nominal and ratio values. Furthermore, they can be missing. (As an additional note, we do not know the underlying struture/quantity of the clusters at all - however, they can be verified by a human). 

Now, we are aware of some distance function solutions for feature vectors of this type so we have a few different modules that calculate distance functions differently. However, we cannot find many clustering algorithms that are useful to us. Many clustering algorithms seem to rely on being able to find a mean of some collection of feature vectors, which clearly does not mean anything for our data set (what's the mean of 'red' and 'blue', for example). Furthermore, we have the missingness issue to contend with - but that's less of a problem at the moment. 

Mostly, I was just wondering if anyone could suggest some clustering methods or distance functions that might be well-suited for our problem. I've read quite a lot of papers and two books (not exactly exhausted my resources but I think I've got a good lay of the land) but I'm still sure there must be something fitting for us out there. For full disclosure, my background is CS and physics but I am doing research in a CS department.   

Thanks!",1,3
223,2014-5-29,2014,5,29,2,26pos8,Overview of Matrix Regularization Techniques,https://www.reddit.com/r/MachineLearning/comments/26pos8/overview_of_matrix_regularization_techniques/,iidealized,1401299486,,0,8
224,2014-5-29,2014,5,29,2,26ppfo,Trying to improve on a backpropagation network by implementing momentum.,https://www.reddit.com/r/MachineLearning/comments/26ppfo/trying_to_improve_on_a_backpropagation_network_by/,davidai,1401299870,"I asked a question recently regarding the right way to code a backpropagation network with three input units, four hidden units, and one output unit (all units logistic).

I got this working and then changed it (required by prof) to a 3 input unit, 2 hidden unit, 1 output unit network with backprop. Now I need to implement momentum to make the code better. 

I have the following functions, but when I run them, I get a terrible total cost. Can anyone tell me what I'm doing wrong?

The code is written in python and implements the numpy module

    def backProp4(a,b,g,w1,w2,dw,dv):
    #a - learning rate
    #b - see conditionals below
    #w1 - the initial matrix of input-hidden unit weights
    #w2 - the initial matrix of hidden-output unit weights
    
    #four training cases
    XX=numpy.asarray([[1.0,0.0,0.0],[1.0,0.0,1.0],[1.0,1.0,0.0],[1.0,1.0,1.0]])
    X=XX[(b-1),:]
    X=res1(X)
    
    #calculating the input of each hidden unit
    #Zin contains the total input in each of the four hidden units
    
    
    Z = numpy.dot(X,w1)
    Z=Z[0]
    Zin = numpy.random.rand(1,2)
    Zin=Zin[0]
    for i in range(2):
        Zin[i]=Z[i]

    
    
    #sigm(Z) is vector-wise application of the function y = 1./(1+exp(-x))
    #input of the output unit Yin and final output Yout are calculated
    Zout = numpy.transpose(sigmv(Z))
    Yin = numpy.dot(Zout,w2)
    Yout = sigm(Yin)
    
    #t is the matrix containing boolean values, for X1, output value should
    #be 0 (false) and so on. This is consistent with ""exclusive or""
    t = numpy.random.rand(4,)
    t[0] = 0
    t[1] = 1
    t[2] = 1
    t[3] = 0
    
    
    #here the backprop alogrithm is started
    #sigp is the derivative of the sigmoid function, y=sigm(x)*(1-sigm(x))
    delw=numpy.random.randn(2,1)
    d = (t[(b-1)] - Yout)*sigp(Yin)
    for i in range(2):
        delw[i] = -g*d+a*dw[i]
    
    
    #calculating change in the input-hidden unit weights
    delv=numpy.random.randn(3,2)
    for i in range(3):
        for j in range(2):
            delv[i,j]=-g*d+a*dv[i,j]
        
    
    
    #updating hidden-output unit weights
    for i in range(2):
        w2[i] = w2[i]+delw[i]
    
    
    #updating input-hidden unit weights
    for i in range(3):
        for j in range(2):
            w1[i,j]=w1[i,j]+delv[i,j]
        
    
    
    W1=w1
    W2=w2
    
    z=numpy.dot(X,W1)
    a=numpy.dot((numpy.transpose(sigmv(z[0]))),W2)
    c=sigm(a)
   
    cost=(1.0/2.0)*(c-t[(b-1)])**2
    
    return (w1,w2,c,cost,delw,delv)
  

    def res1(x):
  
    n = x.shape
    y = numpy.random.rand(n[0],1)
    for i in range(n[0]):
        y[i]=x[i]
        
    return numpy.transpose(y)

this is the function by which I run the previous function

    import numpy

    def epoch4(max_epoch):

    W1 = numpy.random.randn(3,2)
    W2 = numpy.random.randn(2,1)
    

    for epoch in range(1,(max_epoch+1)):
        total_cost = 0
    
        for idx in range(1,5):
            f=backProp2(0.5,idx,W1,W2)
            delw=f[4]
            delv=f[5]
            n=backProp4(0.5,idx,0.5, W1, W2,delw,delv)
            W1=n[0]
            W2=n[1]
            #delw=n[4]
            #delv=n[5]
            c=n[2]
            #total_cost = total_cost + cost
            print c
        
        print (total_cost/4)
",0,1
225,2014-5-29,2014,5,29,4,26pzim,Current state of the art in named entity recognition (NER)?,https://www.reddit.com/r/MachineLearning/comments/26pzim/current_state_of_the_art_in_named_entity/,garnixx,1401305595,"Hi,

years ago I used to follow the results in the field of Named Entity Recognition (i.e. identifying references to People, Places, Companies, and other proper nouns in documents) but I got pulled toward other interests and I feel that my knowledge of the field is a bit out of date.  If there are any experts in this field here, I would be very grateful to hear your opinions about what the current state of the art in this field is, and any exciting new directions that you could see on the horizon.  I would be especially interested in hearing about any progress toward commercial application.",18,16
226,2014-5-29,2014,5,29,8,26qlzt,How to use theta during the Stochastic Gradient Descent?,https://www.reddit.com/r/MachineLearning/comments/26qlzt/how_to_use_theta_during_the_stochastic_gradient/,rainicy,1401318311,"I have a question about updating the theta during the Stochastic GD. I have two ways to update theta:
1) Use the previous theta, to get all the hypotheses for all samples, and then update the theta by each sample. Like:
    hypothese = np.dot(X, theta)
    for i in range(0, m):
	theta = theta + alpha * (y[i] - hypothese[i]) * X[i]
2) Another way: during the scan the samples, update the hypothese[i] using the latest theta. Like:
    for i in range(0, m):
	hypothese[i] = np.dot(X[i], theta)
	theta = theta + alpha * (y[i] - hypothese[i]) * X[i]

So my questions: which way is wrong? And why?
Thank you~",0,1
227,2014-5-29,2014,5,29,11,26r2yy,Free SAS software for higher education : SAS University Edition,https://www.reddit.com/r/MachineLearning/comments/26r2yy/free_sas_software_for_higher_education_sas/,johnt1234,1401329423,,1,1
228,2014-5-29,2014,5,29,16,26rp67,Volvo Construction Equipment - PDF,https://www.reddit.com/r/MachineLearning/comments/26rp67/volvo_construction_equipment_pdf/,jessicperson,1401348379,,0,1
229,2014-5-29,2014,5,29,23,26scsp,Need help to find a tool that i can use to annotate textual data for sentiment analysis,https://www.reddit.com/r/MachineLearning/comments/26scsp/need_help_to_find_a_tool_that_i_can_use_to/,amdorra,1401372861,"I need a tool to annotate textual data for sentiment analysis.

This tool should give the ability to do the following

- upload text posts to be annotated
- be able to give a score or just (negative, positive or neutral) sentiment to the entire text post.
- be able to highlight some words inside the text posts and give it a score.
- be able to download the reports provided by this tool.",1,0
230,2014-5-29,2014,5,29,23,26sdpb,Looking for further reading on hierarchical clustering,https://www.reddit.com/r/MachineLearning/comments/26sdpb/looking_for_further_reading_on_hierarchical/,[deleted],1401373485,"Hi everyone,

I'm working on a presentation on hierarchical clustering for my computer vision class. Can anybody recommend some reading on this topic? 

I'm looking to discuss advantages and disadvantages of minimum-/ maximum- as well as average- linkage in particular and I find most sources focus on k-means clustering instead.

I appreciate any help or advise.",3,0
231,2014-5-29,2014,5,29,23,26sdq8,Neural Network Too Slow,https://www.reddit.com/r/MachineLearning/comments/26sdq8/neural_network_too_slow/,purpleladydragons,1401373501,"Hi guys, I wrote a recurrent network using long short-term memory cells and a mixture density output layer. There's 3 input units, 900 LSTM cells, and 121 output nodes. So, there's a lot of calculations to do. I'm trying to train the network on handwriting data, which contains paragraphs of handwritten data. Unfortunately, my code will take about 8 hours just to do back propagation on a single stroke/letter. I don't know if my code is just that poorly written or if this is the speed that I should expect. Any tips on how I can speed my code up?",15,5
232,2014-5-30,2014,5,30,4,26t6i3,Measuring Performance when Scaling the Number of Classes,https://www.reddit.com/r/MachineLearning/comments/26t6i3/measuring_performance_when_scaling_the_number_of/,ChronicElectronic,1401390738,"I've run a bunch of text classification experiments. In these experiments I varied the number of classes involved. I've grouped them into experiments of 2, 4, and 8 classes.

I'm wondering what the best metric is to determine how well my approach scales to more classes. Does it even make sense to try to compare the experiments?

In a sense I'm looking for something similar to Big O analysis for algorithm performance as a factor of input size.

Thanks.",2,4
233,2014-5-30,2014,5,30,8,26tv1y,"Google Research, MS Research, Pandora &amp; Databricks are presenting at The Graphlab Conference on 7/21 in SF. $40 Discount link.",https://www.reddit.com/r/MachineLearning/comments/26tv1y/google_research_ms_research_pandora_databricks/,shonburton,1401405286,,3,14
234,2014-5-30,2014,5,30,13,26uk1o,"Training Highly Multiclass Classifiers - by {mayagupta, bengio, jweston}@google",https://www.reddit.com/r/MachineLearning/comments/26uk1o/training_highly_multiclass_classifiers_by/,rrenaud,1401422567,,4,5
235,2014-5-30,2014,5,30,14,26upry,"Buy Security Door,Explosive Detection System From Eastimage",https://www.reddit.com/r/MachineLearning/comments/26upry/buy_security_doorexplosive_detection_system_from/,emily0214,1401427533,,0,0
236,2014-5-30,2014,5,30,16,26uw6u,Important Pieces Of Equipment In The Construction Industry - Machinery &amp; Equipment Part Online,https://www.reddit.com/r/MachineLearning/comments/26uw6u/important_pieces_of_equipment_in_the_construction/,jessicperson,1401434187,,0,1
237,2014-5-30,2014,5,30,17,26uzm5,More than 250 million global events are now in the cloud for anyone to analyze,https://www.reddit.com/r/MachineLearning/comments/26uzm5/more_than_250_million_global_events_are_now_in/,abashinyan,1401438703,,12,27
238,2014-5-30,2014,5,30,20,26v7o0,Machine learning and data mining blog,https://www.reddit.com/r/MachineLearning/comments/26v7o0/machine_learning_and_data_mining_blog/,gfoxvh,1401449258,,2,0
239,2014-5-30,2014,5,30,20,26v7qn,"Anyone else think Intel's new singing, beer-serving robot's more than a little creepy?",https://www.reddit.com/r/MachineLearning/comments/26v7qn/anyone_else_think_intels_new_singing_beerserving/,strmbrg,1401449344,,0,1
240,2014-5-31,2014,5,31,0,26vrt2,How to Tell Someones Age When All You Know Is Her Name,https://www.reddit.com/r/MachineLearning/comments/26vrt2/how_to_tell_someones_age_when_all_you_know_is_her/,meandtree,1401464668,,5,0
241,2014-5-31,2014,5,31,1,26vw4v,How to optimise multi-layer neural network architecture using the genetic algorithm in MATLAB,https://www.reddit.com/r/MachineLearning/comments/26vw4v/how_to_optimise_multilayer_neural_network/,blue7777,1401467333,"Can someone please provide me with a very brief summary of how to optimise multi-layer feedforward neural network architecture using the genetic algorithm? i.e, optimise the number of neurons and layers.

I just need somewhere to get started. I've had a go at doing it myself, but I think I used a bad method as it gave poor results.

Thanks!",14,0
242,2014-5-31,2014,5,31,3,26w5hi,Sentiment analysis with Vowpal Wabbit,https://www.reddit.com/r/MachineLearning/comments/26w5hi/sentiment_analysis_with_vowpal_wabbit/,teamnano,1401473062,"Similar to the [MLWave](http://mlwave.com/movie-review-sentiment-analysis-with-vowpal-wabbit/) article, [I just put up an educational post](http://www.mltoolkit.com/rotten-tomatoes-sentiment-analysis-with-excelpython-and-vowpal-wabbit/) detailing how to make a submission to the Rotten Tomatoes Sentiment Analysis competition using Python (or Excel, :shudder:) and Vowpal Wabbit. I tried some different things and ended up tweaking the VW parameters a bit to get a lower average loss, but I haven't bothered submitting the final predictions to the competition since I was more interested in the education component of the process. 
Feel free to use it/abuse it/provide feedback however you see fit.",1,5
243,2014-5-31,2014,5,31,4,26wdds,Spark Release 1.0.0,https://www.reddit.com/r/MachineLearning/comments/26wdds/spark_release_100/,turnersr,1401477901,,1,29
244,2014-5-31,2014,5,31,8,26x06e,Machine Learning Study Group - Thread #1,https://www.reddit.com/r/MachineLearning/comments/26x06e/machine_learning_study_group_thread_1/,rovingr,1401492144,"So I posted a while back ago about a machine learning study group. Sorry for the delay in following up on that, things have been somewhat hectic as I am relocating for a new job. Just as a reminder, the target audience for this study group is people who have finished one of the MOOCs that provide an intro to machine learning, are familiar with basic programming and statistics concepts, and are interested in gaining a deeper understanding of machine learning. 

This week, we'll be discussing Domingo's 'A Few Useful Things to Know about Machine Learning' article, which I thought was pretty interesting. It gives a nice overview of a number of general lessons that are useful to keep in mind when solving machine learning problems. Here is a link:

http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf

Additionally, I am currently working on the Bike Sharing Demand competition on Kaggle, and if anyone else would like to work on a team with me, they are welcome to. 

If you have any ideas for interesting articles or topics that we could focus on in future study groups, feel free to post here and/or message me. 


Edit: Here's a link to the blog, I'll be both creating a new reddit thread and a new post on the blog whenever I post a new article.

datasciencetalk.blogspot.com",17,45
245,2014-5-31,2014,5,31,17,26xz9v,Questions from Deep Sparse Rectifier Networks,https://www.reddit.com/r/MachineLearning/comments/26xz9v/questions_from_deep_sparse_rectifier_networks/,T_hank,1401523294,"I was reading this paper by xavier glorot, and had some questions about how to implement his methods

http://eprints.pascal-network.org/archive/00008596/01/glorot11a.pdf


1.He says in the section in Experiments that he chose to flip half the unit activations to numerically equalize the layer activation.  This is in a layerwise pretraining frame work, so i wanted to confirm that what this means is that whenever we are learning a layer, half of its units are relu units and the rest are negative relu?


2.Also the reconstruction functions seemed to have been problematic for him, and he tries out many methods. One of these is scaling the activations of the hidden layer to 0,1 range and then using sigmoids for the reconstruction.

a. How do we normalize the activations in 0 to 1, is it across a minibatch, or is it across the hidden layer?

b. suppose i call my visible layer v, I learn hidden layer 1, h1, and then want to learn the second hidden layer h2.

So going from v to h1, I use ReLU, but coming down from h2 to h1, I use sigmoid? i.e. the up and down activations of each hidden layer are different?",1,2
246,2014-5-31,2014,5,31,22,26yesm,Machine learning in R better than in scikit-learn ?,https://www.reddit.com/r/MachineLearning/comments/26yesm/machine_learning_in_r_better_than_in_scikitlearn/,binge_learner,1401543673,"Hi everyone,
I just got into kaggle competitions this weekend, and like everyone I started with the titanic challenge. First I used scikit-learn's randomForests ( along with other classifiers that performed worse ) for my first submissions, I did quite a lot of feature engineering with pandas before that, but I never managed to break the 77% accurarcy wall. Then I switched to R ( but still wrangling data with pandas ) and by using the party package ( and even rpart ), I got much better scores ( pas the 80% bar ). I used even the exact same numbers of trees in scikit-learn and R, but still got much better results in R. ( I repeated the experience multiple times to be sure )
How can it be ?",24,15
247,2014-5-31,2014,5,31,22,26yfgz,Coursera: Machine Learning,https://www.reddit.com/r/MachineLearning/comments/26yfgz/coursera_machine_learning/,kalden31,1401544361,,12,32
